{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import requests\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>abstract</th>\n",
       "      <th>introduction</th>\n",
       "      <th>literature review</th>\n",
       "      <th>methodology</th>\n",
       "      <th>results</th>\n",
       "      <th>conclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  title text abstract introduction literature review methodology results  \\\n",
       "0                                                                          \n",
       "\n",
       "  conclusion  \n",
       "0             "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe make\n",
    "data_dict = [{'title':'','text':'','abstract':'','introduction':'','literature review':'','methodology':'','results':'','conclusion':''}]\n",
    "main_df = pd.DataFrame().from_dict(data_dict)\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google\n",
    "API_KEY = '' #add key here\n",
    "genai.configure(api_key= API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "generation_config = genai.GenerationConfig(\n",
    "  stop_sequences = None,\n",
    "  temperature=0.7,\n",
    "  top_p=1,\n",
    "  top_k=32,\n",
    "  candidate_count=1,\n",
    "  max_output_tokens=32,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "error_title = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File number 507,\n",
      " {\n",
      " \"abstract\": \"Open information extraction (OpenIE), a task focused on the schema-free extraction of triplets from natural language text, poses various challenges, including complicated triplet structures, overlapping elements, and implicit triplets. In this paper, we introduce DualOIE, a novel generative OpenIE model which incorporates a dual task of converting triplets into sentences to address these complexities and enhance extraction performance. By leveraging the duality, DualOIE effectively recognizes the sentence structure, tackles complicated triplet patterns, and learns diverse relations between arguments, leading to improved extraction results. We establish a new benchmark, MTOIE, for implicit triplet extraction containing a significant number of implicit triplets and various predicate types. Extensive experiments on two benchmarks and our MTOIE demonstrate the superiority of DualOIE over the state-of-the-art baselines and ChatGPT. The conducted online A/B test further verifies DualOIE's utility in improving the performance of a real-world search system.\",\n",
      " \"introduction\": \"Open information extraction (OpenIE), a task of extracting schema-free triplets in the form of (subject, predicate, object) from unstructured natural language, has gained significant attention for its wide range of downstream applications. Traditional OpenIE methods, such as tagging-based and generative solutions, face challenges in dealing with complicated triplets, including overlapping elements, implicit triplets, and those with intricate structures. To address these limitations, we propose DualOIE, a novel generative OpenIE model that incorporates a dual task of transforming triplets into sentences.\",\n",
      " \"literature review\": \"Previous approaches to OpenIE have employed tagging-based and generative solutions. Tagging-based models treat OpenIE as a sequence labeling problem, while generative approaches view it as a Seq2Seq task. Notable tagging-based systems include RnnOIE, SpanOIE, and MacroIE. Generative solutions comprise models like NOIE, IMoJIE, Gen2OIE, and ChatGPT. While these methods have shown promise, they often struggle with complicated triplet extraction, particularly when handling implicit triplets and managing the structure of the input sentence.\",\n",
      " \"methodology\": \"DualOIE consists of a dual framework with two task directions, namely S \\u2192 T and T \\u2192 S, where S denotes the sentence and T represents the triplets. A shared encoder enables the model to learn from both tasks jointly. In the S \\u2192 T direction, predicate extraction is first performed, followed by triplet generation using the extracted predicates as prompts. This approach helps alleviate repetition and omission issues commonly encountered in generative models for OpenIE. The T \\u2192 S direction promotes the model's understanding of the sentence structure and aids in resolving complicated triplet structures and implicit triplets.\",\n",
      " \"results\": \"We evaluate DualOIE extensively on two benchmark datasets, CaRB and SAOKE, as well as our constructed MTOIE dataset. The results indicate that DualOIE outperforms all baselines, including ChatGPT, across both explicit and implicit triplet extraction tasks. Analyses reveal that DualOIE effectively tackles complicated triplets, achieves a balance between the two tasks in the dual framework, and exhibits a correlation between the quality of sentence reconstruction and triplet extraction.\",\n",
      " \"conclusion\": \"DualOIE, a generative OpenIE model utilizing a dual task of converting triplets into sentences, demonstrates superior performance on both public benchmarks and our constructed dataset. By incorporating the dual task, DualOIE improves its understanding of sentence structure, handles complicated triplets more effectively, and learns diverse relations between arguments. The positive correlation between the quality of triplet extraction and sentence reconstruction highlights the mutual benefits of the dual task approach. Furthermore, the online A/B test conducted on the Meituan platform showcases the practical utility of DualOIE in enhancing the performance of a real-world search system.\"\n",
      "}\n",
      "File number 510,\n",
      " {\n",
      " \"abstract\": \"In recent years, Transformer structures have demonstrated their excellence in diverse domains across deep learning. It sparks discussions on whether such sophisticated structures are necessary and whether simple structures like Mixer can achieve comparable performance while maintaining reduced inference cost. To explore the Mixer's effectiveness, this research applies a more compact version of the Mixer to audio classification tasks, conducting comparative experiments with the Transformer-based Audio Spectrogram Transformer (AST) on three datasets. Additionally, this study examines the application of various activation functions in Mixer and compares their impact on model performance.\",\n",
      " \"introduction\": \"In recent years, Transformer structures have significantly impacted various artificial intelligence domains, such as visual classification, temporal prediction, and audio classification. Their exceptional results in data processing have come at the cost of extensive parameters and intricate designs. Additionally, the surge in computing power and vast dataset collection has resulted in numerous models with complex architectures.\",\n",
      " \"literature_review\": \"Yuan Gong, Yu-An Chung, and James Glass introduced a model known as AST: Audio Spectrogram Transformer at the Interspeech2021 conference. This model originated from the aforementioned situation. In the AST paradigm, voice segments are converted into 128-dimensional Mel spectral features, and then a windowing operation is used to produce the associated spectrogram. The spectrogram is then divided into several segments by the AST model using Transformer\\u2019s approach, which are then supplied into the encoder structure following Linear Projection and feature extraction. The categorization outcomes are then output by the AST model using a linear layer. The model will be utilized as a comparison target in this research since it exhibits good performance on the three datasets.\",\n",
      " \"methodology\": \"In this research, an audio classification task is utilized to evaluate the efficacy of a more compact version of the Mixer. Comparative experiments are conducted with the Transformer-based Audio Spectrogram Transformer (AST) model on three datasets: Speech Commands, UrbanSound8k, and CASIA Chinese Sentiment Corpus. Additionally, this study explores the impact of various activation functions on Mixer's performance.\",\n",
      " \"results\": \"The experimental results demonstrate the effectiveness of the Mixer in audio classification tasks. The Mixer outperforms the AST model in terms of accuracy and area under the curve (AUC) on both the validation and test sets across all three datasets. This suggests that Mixer can achieve comparable performance to Transformer-based models while maintaining a simpler structure and lower inference cost. Furthermore, the study finds that the RGB-to-grayscale map formula outperforms the simple weighted fusion approach in Linear Projection, leading to improved model performance.\",\n",
      " \"conclusion\": \"In conclusion, this study highlights the potential of Mixer in audio classification tasks, demonstrating its ability to achieve competitive performance with a simpler structure and reduced inference cost. The study also emphasizes the importance of adhering to objective laws and knowledge from other fields when making model optimizations, as this approach can provide more targeted directions for improvement. Future research directions include examining the application of Mixer pre-trained networks, removing the CLS flag in AST, optimizing the Mixer unit structure, and exploring self-supervised training approaches to further enhance the model's performance.\"\n",
      "}\n",
      "File number 514,\n",
      " {\n",
      " \"abstract\": \"In this research, we introduce Boidae, a suite of customizable, user-controlled Boa installations. Utilizing automation tools such as Ansible and Docker, Boidae enables deployment of tailored Boa setups. Notably, it allows the creation of custom datasets from any collection of Git repositories. These customized datasets can be generated from any set of Git repositories and are facilitated by helper scripts that aid in finding and cloning repositories from GitHub and SourceForge. The utility of Boidae lies in its versatile architecture, enabling researchers to generate custom datasets and make modifications to the Boa language and runtime.\",\n",
      " \"introduction\": \"Mining software repositories (MSR) is a powerful technique for unearthing developer habits and feature usage within the Software Engineering (SE) community. However, conducting MSR studies presents challenges, such as time-consuming dataset generation and complex data processing. Boa, a domain-specific query language and runtime infrastructure, simplifies MSR research. It facilitates the analysis of ultra-large-scale repositories, such as those found on GitHub and SourceForge, by leveraging Hadoop's infrastructure for storage and computation. While Boa streamlines MSR studies, it has limitations. Researchers often require customized datasets for their research, or they may wish to modify the Boa runtime or query language, which can be difficult with the current Boa setup.\",\n",
      " \"literature_review\": \"The mining software repositories (MSR) research process can be broken down into three main stages: dataset generation, dataset mining, and dataset analysis. Though these stages can be challenging for new researchers, the first two stages pose unique difficulties. Dataset generation can be time-consuming and requires knowledge of tasks like using the GitHub API, parsing source files with libraries like Eclipse JDT, and storing large amounts of data using distributed filesystems or formats like Protocol Buffers. Mining the data also demands substantial expertise, including knowledge of distributed data analysis frameworks like Hadoop or Spark. While there are solutions that help alleviate some of these difficulties, none of them offer a comprehensive solution that allows for both easy dataset generation and customization of the mining framework. This is the primary motivation behind the development of Boidae.\",\n",
      " \"methodology\": \"Boidae's architecture has two primary components, each corresponding to a different runtime environment. The first component is a Docker image designed for local operation on a single machine, primarily intended for testing and analyzing small datasets. This image includes helper scripts for generating customized datasets from GitHub or SourceForge projects, and it enables modifications to the Boa compiler, runtime infrastructure, and language. For larger datasets, the second component provisions a distributed environment. Users can set up a cluster of compute nodes using their organization's computing services, cloud platforms like AWS, or research infrastructure such as CloudLab or ACCESS. Once the servers are provisioned, Ansible scripts can be used to install and configure a distributed Boidae instance, supporting dataset and runtime customization.\",\n",
      " \"results\": \"The Boidae infrastructure was evaluated in four ways. Initially, to evaluate its ability to run on multiple systems, the Docker version was tested on various machines, and the cloud version was verified using Ansible scripts deployed on small clusters at Bowling Green State University and the CloudLab experimental testbed. To assess dataset generation capabilities, a custom dataset was built using repositories from the boalang user on GitHub. Furthermore, the infrastructure was tested for language and runtime customization by modifying the Boa language to add custom functions and then verifying their availability and functionality in queries. Additionally, the scalability of the infrastructure was evaluated by running tasks with varying numbers of map tasks, demonstrating good performance and the capability to handle larger datasets.\",\n",
      " \"conclusion\": \"Boidae is a significant contribution to the field of MSR as it offers a user-friendly platform for creating customized Boa instances. This enables researchers to generate custom datasets and modify the Boa language and runtime as needed. The availability of Boidae as a Docker container and the provision of Ansible scripts ensures that researchers can easily set up and manage their own Boa instances. Moreover, the operational Boa instance, available at https://boa.cs.iastate.edu/boa/, provides a readily accessible platform for users who prefer not to set up their own instances. With Boidae, researchers can focus more on their research questions and gain deeper insights into software repositories, alleviating the complexities of dataset generation and runtime modifications.\"\n",
      "}\n",
      "Error in Sharing Energy in Wide Area: A Two-Layer Energy Sharing Scheme for Massive Prosumers\n",
      "File number 515,\n",
      " {\n",
      " \"abstract\": \"In distribution systems, consumers have transformed to prosumers, inspiring the introduction of energy sharing markets. This paper proposes a two-layer energy sharing market, where a wide-area market (WAM) and multiple local-area markets (LAMs) coexist. Prosumers share energy in the LAMs and the surpluses and shortages are cleared in the WAM. The proposed market promotes social efficiency by wider-area sharing. The non-convex market problem is formulated as a mathematical program with equilibrium constraints (MPEC) and then an efficient and hierarchically distributed bidding algorithm is developed. The proposed two-layer market is verified on a large-scale IEEE 123-bus system, which shows the scalability and practicality.\",\n",
      " \"introduction\": {\n",
      "  \"background\": \"The increasing number of distributed energy resources transforms end-users from consumers into both producers and consumers called prosumers. Energy sharing markets are proposed to facilitate the utilization of renewable energy. In this paper, a novel two-layer energy sharing market is proposed to promote social efficiency by wider-area sharing.\",\n",
      "  \"related_works\": \"Previous works on energy sharing markets have simplified the prosumer model by treating them as price takers or separating them into pure sellers and buyers. These methods have difficulty in considering the flexibility in the sharing market and network constraints. Furthermore, most existing works focus on small-scale energy sharing markets and neglect the scalability issue in large-scale systems.\",\n",
      "  \"contributions\": \"The major contributions of this paper are:\\n1) Sharing Scheme: A novel two-layer energy sharing market is proposed, where prosumers can share energy in a wide area. The lower-level LAMs focus on local transactions while the upper-level WAM shares uncleared energy. The market clearing result is unique and approximates to the social optimum.\\n2) Bidding Algorithm: The proposed energy sharing market problem belongs to a mathematical program with equilibrium constraints (MPEC). Existing works commonly transform MPEC into mixed-integer programming, which is NP-hard and inefficient in large-scale markets. We first reveal that the equivalence of the Nash equilibrium in the LAM equals to the solution to a convex optimization problem. Then we propose a hierarchically distributed bidding algorithm without binary variables to efficiently clear the whole market.\"\n",
      " },\n",
      " \"literature_review\": \"Most of the above works focus on small-scale energy sharing markets, whereas prosumers in distribution systems are becoming more and more fragmented and massive. As the number of prosumers participating in the sharing market explodes, the market clearings in the above works may suffer from incredible computational burdens. For instance, in cooperative-game-based markets, the number of problems to be solved for calculating Sharpley values exponentially increases with the increasing of prosumers. Ref. [21] adopts the K-means clustering method to improve the scalability of nucleolus calculation at the cost of inexactness. Stackelberg-game-based market models are commonly transformed into mixed-integer programming (MIP) by the big-M method. As NP-hard problems, large-scale MIPs always suffer from the curse of dimensionality. Therefore, existing works are difficult to implement in the wide-area energy sharing over massive prosumers. In this paper, we will propose a novel two-layer energy sharing market, which can efficiently share the energy of massive prosumers in a wide area.\",\n",
      " \"methodology\": {\n",
      "  \"overall_market_architecture\": \"The proposed two-layer energy sharing market architecture consists of a wide-area market (WAM) and multiple local-area markets (LAMs). Prosumers in the same community share energy in an LAM, where the energy may be uncleared. The LAM operators participate in the WAM to share unclear energy. The main idea of the two-layer energy sharing mechanism is that prosumers in the same LAM first trade with each other. The LAM aims to promote local transactions in each community. Different from existing works [6\\u201325], the LAM does not strictly pursue energy clearing. In our model, a WAM is developed for LAM operators to share unclear energy. Due to the uneven distribution of renewable energy production and load, the prosumers in the same community may be simultaneously willing to sell or buy electricity. Mandatory clearing in the LAM will damage prosumers\\u2019 interests. Our model provides a new paradigm, where the uncleared energy can be shared in the upper-level WAM. Social efficiency is promoted since energy is more reasonably distributed.\",\n",
      "  \"lower_level_local_area_sharing_market\": \"The lower-level local-area sharing market of prosumers in the same local community is managed by the LAM operator. Prosumer i can buy/sell electricity from/to the electric utility. The energy sharing price in the LAM is decided by all prosumers in the same community. The prosumer model (3) constitutes a Nash game, where prosumers\\u2019 payoffs depend on others in the same community.\",\n",
      "  \"upper_level_wide_area_sharing_market\": \"In the WAM, an LAM operator can be regarded as an aggregated \\u201cprosumer\\u201d, whose uncleared energy is the shared electricity in the WAM. The energy shared by LAMs is traded at the base price \\u03c90\\ni. The WAM operator can adjust the base price to influence the equilibrium of LAMs.\",\n",
      "  \"bidding_algorithm\": \"The bidding algorithm of the LAM is presented in Algorithm 1. Prosumers only need to submit the decision of shared energy to the market. Their private information, e.g., the energy demand or the generation parameters, will not be leaked to the market operator or other prosumers. Prosumer\\u2019s problem (9a) is equivalent to problem (3) with xj\\u2019 \\u2264 xj\\u2019(h\\u22121), \\u2200j\\u2019 \\u2208 Ui\\\\{j}. Note that each prosumer does not have to communicate with others. It can directly deduce Pj\\u2019\\u2208Ui\\\\{j} xj\\u2019 by \\u03c90\\ni(h \\u2212 1). After solving (9a), the real bidding is averaged by the last one as (9b), which describes prosumers\\u2019 inertia in the decision. From the perspective of the algorithm, the step size \\u03c1i decides the convergence of the bidding.\",\n",
      "  \"prices_in_equilibrium\": \"Existing works commonly assume that prosumers can only trade in the energy market. However, they ignore the influence of electric utilities on sharing markets. We will analyze the condition where prosumers can share energy with each other in the sharing market or/and trade with the electric utility. The interaction among the sharing price \\u03c90\\ni(xi\\u2203), the buying/selling prices \\u03c9+, \\u03c9\\u2212, and the shadow prices of prosumers are revealed. To this end, we present the definition of shadow price quantifying the marginal consumption/production price of the prosumer.\"\n",
      " },\n",
      " \"results\": {\n",
      "  \"main_results\": \"The WAM clearing process by Algorithm 2 is shown in Fig. 4. We calculate the sum of all 11250 prosumers\\u2019 costs during 500 iterations and record its relative error from the exact value by solving problem (6) in the left subfigure. It verifies the convergence of Algorithm 2 and the correctness of Proposition 6. The computational time of Algorithm 2 and solving problem (6) are 10.2s and 23.8s, respectively. Since the biddings of all prosumers in Algorithm 2 are serially calculated, the average time spent by one prosumer is only 9.07 \\u00d7 10\\u207b4\\ns, 1/26193 times of solving problem (6). It validates the scalability of the proposed market clearing algorithm in large-scale markets. We pick up five LAMs (35, 36, 66, 83, and 117) to display their base prices \\u03c90\\ni during iterations in the right subfigure. Due to the congestion of distribution line {35, 36}, the base price of LAM 36 is 0.056 $/kW larger than that of LAM 35.\\nThe market clearing process in the LAMs 35, 36, 66, 83, and 117 by Algorithm 1 during the 25th iteration of the WAM bidding are displayed in Fig. 5. In the left subfigure, the total costs of prosumers in one community are calculated and compared to the value obtained by solving problem (8), which verifies the convergence of Algorithm 1 and the correctness of Proposition 3. The right subfigure shows the sharing prices during iterations. The required iteration numbers of LAMs to reach the error tolerance 10\\u22128 are different. The average number of iterations over the whole algorithm is just 15.1, which is surpassingly quick.\",\n",
      "  \"social_efficiency\": \"We consider four conditions for energy sharing: 1) Self-sufficiency (SS), energy balance by itself without\n",
      "File number 516,\n",
      " {\n",
      " \"abstract\": \"This research paper introduces FEDRKG, a novel federated recommendation framework that employs Graph Neural Networks (GNN) and incorporates a knowledge graph (KG) to enhance recommendation algorithms. The key insight is to construct higher-order user-item interactions through the relationships in the KG, while preserving user privacy. FEDRKG enables effective recommendation without the need for private user data exchange or costly homomorphic encryption.\",\n",
      " \"introduction\": \"Federated Learning (FL) has emerged as a promising approach to address the privacy concerns associated with centralized recommendation systems. However, existing FL-based recommendation methods often suffer from limited modeling capabilities and privacy issues. This paper proposes FEDRKG, a novel GNN-based federated learning framework that leverages public item information to enhance recommendation accuracy while protecting user privacy.\",\n",
      " \"literature_review\": \"Relevant research works are reviewed in three categories: knowledge graph-based recommendation, federated learning for recommendation systems, and privacy-preserving techniques in federated learning.\",\n",
      " \"methodology\": \"FEDRKG constructs a global knowledge graph using publicly available item information and distributes KG subgraphs to clients to improve local training. It utilizes a relation-aware GNN model with local differential privacy (LDP) for interaction items and gradients, ensuring privacy preservation during communication with the server.\",\n",
      " \"results\": \"Extensive experiments on three real-world datasets demonstrate the effectiveness of FEDRKG in terms of recommendation accuracy and privacy protection. It outperforms centralized algorithms and existing federated learning baselines, achieving competitive results while preserving user privacy.\",\n",
      " \"conclusion\": \"FEDRKG offers a promising approach for privacy-preserving federated recommendation tasks. It effectively utilizes public item information to enhance recommendation quality and employs various privacy-preserving techniques to protect user data. The proposed framework has the potential to improve the accuracy and privacy of federated recommendation systems.\"\n",
      "}\n",
      "File number 517,\n",
      " {\n",
      " \"abstract\": \"Cross-Domain Facial Expression Recognition (CD-FER) faces challenges in feature representation and transferability due to the domain shift. This paper introduces a framework called Adaptive Global-Local Representation Learning and Selection (AGLRLS) to overcome these challenges. The framework consists of separate global-local adversarial learning and semantic-aware pseudo label generation. Specifically, it includes global and local adversarial learning modules that independently learn domain-invariant global and local features. There's also a pseudo label generation mechanism that assigns pseudo class labels to unlabeled data features, preventing mutual interference among classifiers. In the training process, separate classifier learning is performed by optimizing the model with the adversarial learning process using these labels, aiding in the learning of more discriminative features for the target dataset. During inference, a global-local prediction consistency strategy is used to infer the optimal class label by combining the prediction scores from the global and local classifiers. Comprehensive experiments demonstrate the superiority of AGLRLS in CD-FER.\",\n",
      " \"introduction\": \"CD-FER aims to automatically determine a person's emotional state from a facial image, regardless of the domain. It's challenging due to variations in different datasets and large discrepancy among FER datasets, resulting in domain shift. In recent years, significant efforts have been devoted to domain adaptation models for CD-FER. However, these models primarily focused on holistic features for domain adaptation, overlooking the potential benefits of local features, like greater transferability and fine-grained representation of variations. Previous research combined graph representation propagation with adversarial learning to reduce domain shift, while others incorporated semantic information into the multi-view features learning to bridge the semantic gap. Despite these advances, the significant problem of imbalanced class distribution was largely overlooked, leading to suboptimal performance.\",\n",
      " \"literature_review\": \"A series of CD-FER methods has been proposed to address data bias among different FER datasets. These methods utilize subspace learning, metric learning, dictionary learning, contrastive learning, and other techniques to facilitate unsupervised CD-FER. Recent years have seen the emergence of domain adaptation models that employ adversarial learning mechanisms to mitigate domain shift. Inspired by generative adversarial networks, these methods use a feature extractor and a domain discriminator to learn transferable domain-invariant features through a two-player game. The feature extractor learns the features, while the domain discriminator struggles to distinguish samples from the target dataset. Several methods have achieved state-of-the-art performance on FER benchmark datasets. However, they tend to focus on holistic features for adaptation, neglecting the potential advantages of local features in addressing the domain shift and improving discriminability.\",\n",
      " \"methodology\": \"The proposed AGLRLS framework comprises separate global-local adversarial learning and semantic-aware pseudo label generation. It integrates global and local adversarial learning modules that independently learn domain-invariant global and local features. Furthermore, a feature-level pseudo label generation mechanism is designed to assign reliable pseudo labels to each global and local feature of unlabeled data. These labels are utilized for model optimization through the adversarial learning process, enhancing the learned features' generalization and representation capacities. During inference, a global-local prediction consistency strategy is devised to refine recognition results by combining the prediction scores from the global and local classifiers. The framework effectively mitigates domain shift and amplifies the discriminative power in the target domain.\",\n",
      " \"results\": \"Extensive experiments are conducted to evaluate the effectiveness of AGLRLS against state-of-the-art CD-FER methods. The results demonstrate that AGLRLS consistently outperforms competing algorithms across various configurations and datasets. The analysis highlights the robustness of AGLRLS with different backbones and source domains. Statistical tests further confirm the significance of AGLRLS's performance. Ablation studies reveal the contributions of individual components, underscoring the effectiveness of the proposed global-local representation learning and selection strategy.\",\n",
      " \"conclusion\": \"The proposed AGLRLS framework showcases superior performance in CD-FER tasks, effectively addressing the issues of data inconsistency and less discriminative ability prevalent in existing methods. It seamlessly integrates global-local adversarial learning and semantic-aware pseudo label generation to learn domain-invariant and discriminative features that generalize well across source and target datasets. In addition, the global-local prediction consistency strategy enhances the reliability of recognition results during inference. AGLRLS demonstrates state-of-the-art performance on various benchmark datasets, setting a new standard for CD-FER. The findings contribute to the development of more effective and robust facial expression recognition systems capable of handling cross-domain scenarios.\"\n",
      "}\n",
      "File number 518,\n",
      " {\n",
      " \"abstract\": \"In this paper, we investigate transmission control for unmanned aerial vehicles (UAVs) operating in unlicensed spectrum bands. A rigorous interference-aware queuing analysis framework is developed to optimize expected throughput by balancing packet drops from queues and transmission errors caused by low SINR. Two packet loss probabilities are explored, buffer overflow model and time threshold model. Two algorithms, Interference-Aware Transmission Control (IA-TC) and Interference-Aware Distributed Transmission Control (IA-DTC), adjust transmission policies to optimize throughput. Numerical results demonstrate that both algorithms find the optimal transmission policy. Focus is on LoS and NLoS interference links between UAVs and ground nodes.\",\n",
      " \"introduction\": \"UAV networks have great potential, but reliable and robust communication in unlicensed spectrum bands is challenging. This paper explores distributed transmission policies that consider queue and channel impairments. Previous works have not investigated this issue in-depth.\",\n",
      " \"literature_review\": \"Existing literature on UAV network transmission policies does not comprehensively consider the effects of queue and in-band interference on packet losses. Some studies assume LoS or NLoS links without interference models or only model system performance in terms of UAV trajectory and not channel parameters.\",\n",
      " \"methodology\": \"A comprehensive analytical framework is proposed to characterize two factors that impact packet losses: queue-related analysis with a time threshold model and buffer overflow model, and channel-related analysis that focuses on outage probability and the impact of interference. Two transmission policy algorithms, IA-TC and IA-DTC, are introduced to find the optimal channel fading threshold in the presence of queue and channel impairments.\",\n",
      " \"results\": \"The performance of the proposed algorithms is evaluated under various scenarios. Results show that IA-TC constantly tries to increase the channel fading threshold for interferer nodes and decrease it for the main source node. IA-DTC balances the trade-offs between optimizing the throughput of the main link and minimizing interference to other nodes. Both algorithms achieve higher throughput compared to baseline policies.\",\n",
      " \"conclusion\": \"An in-depth investigation of distributed transmission policy that considers both packet queues and interference levels is presented. Two transmission algorithms, IA-TC and IA-DTC, are proposed and their effectiveness is demonstrated. The optimal transmission policy under various scenarios is found. Future work will extend analysis to model-free analysis and optimization solutions.\"\n",
      "}\n",
      "Error in Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions\n",
      "File number 519,\n",
      " {\n",
      " \"abstract\": \"Due to the rise of privacy concerns, in many practical applications the training data is aggregated\\nbefore being shared with the learner, in order to protect privacy of users\\u2019 sensitive responses. In an\\naggregate learning framework, the dataset is grouped into bags of samples, where each bag is available\\nonly with an aggregate response, providing a summary of individuals\\u2019 responses in that bag. In this\\npaper, we study two natural loss functions for learning from aggregate responses: bag-level loss and the\\ninstance-level loss. In the former, the model is learnt by minimizing a loss between aggregate responses\\nand aggregate model predictions, while in the latter the model aims to fit individual predictions to the\\naggregate responses. In this work, we show that the instance-level loss can be perceived as a regularized\\nform of the bag-level loss. This observation lets us compare the two approaches with respect to bias\\nand variance of the resulting estimators, and introduce a novel interpolating estimator which combines\\nthe two approaches. For linear regression tasks, we provide a precise characterization of the risk of the\\ninterpolating estimator in an asymptotic regime where the size of the training set grows in proportion to\\nthe features dimension. Our analysis allows us to theoretically understand the effect of different factors,\\nsuch as bag size on the model prediction risk. In addition, we propose a mechanism for differentially\\nprivate learning from aggregate responses and derive the optimal bag size in terms of prediction risk-\\nprivacy trade-off.\",\n",
      " \"introduction\": \"Machine learning has revolutionized many industries and aspects of our lives, but its widespread use has also\\nraised concerns about privacy. One way to address these concerns is to use aggregate labels, which are labels\\nthat are assigned to groups of data points rather than individual data points (Criteo Privacy Preserving ML\\nCompetition, 2021). Since even the pre-machine learning age, aggregate labels have been commonly used in\\ngroup testing, a method of testing that combines samples from various individuals or objects to maximize the\\nuse of limited resources (Wein & Zenios, 1996; Sunjaya & Sunjaya, 2020). For example, group testing is used\\nto screen for HIV in donated blood products and to identify viral epidemics such as COVID-19. In recent\\nyears, there has been growing interest in using aggregate labels to train machine learning models (Papernot\\net al., 2016; Al-Rubaie & Chang, 2019; De Cristofaro, 2020). This approach has the potential to preserve\\nprivacy while still allowing for the development of accurate and effective models.\\n\\nThe SKAdNetwork API from Apple is an example of how aggregate labels can be used to preserve privacy in\\nmachine learning (Apple Developer Documentation, 2023). SKAdNetwork provides advertisers with insights\\ninto the effectiveness of their ad campaigns without compromising the privacy of their users. This is achieved\\nby using aggregate labels to represent groups of users who have interacted with an ad. For example, an\\nadvertiser might see a report that tells them that 10% of users who saw their ad installed their app. However,\\nthe advertiser would not be able to see the specific identities of those users.\\n\\nAnother example is the Private Aggregation API of Chrome Privacy Sandbox. This API collects instance-\\nlabel pairs from users, but it protects their privacy by providing apps and services with bags of instances\\nthat are labeled in an aggregated way. The aggregate label can be further perturbed to ensure differential\\nprivacy, which is a mathematical guarantee that the data cannot be used to identify or re-identify individuals.\\n\\nTo formally describe the setup, consider a dataset consisting of n samples (xi, yi), for i \\u2208 [n], with\\nxi \\u2208 Rd the features vector and y \\u2208 R the response variable. Motivated by applications where the response\\nvariables yi carry private information, instead of sharing individual responses with the learner, only aggregate\\nresponses are shared in the following way. A set of m non-overlapping bags Ba \\u2286 [n], for a \\u2208 [m], are formed,\\neach of size k and for each bag the average response \\u00afya, 1\\u00b9k, along with the individual features\\nvectors xi are shared with the learner.\",\n",
      " \"literature_review\": \"There is a rich body of prior work on learning with label proportions (LLP) (Shi et al., 2018, 2019; Cui\\net al., 2017; Xiao et al., 2020; Li & Wang, 2018; Quadrianto et al., 2008; Patrini et al., 2014; Musicant et\\nal., 2007; Zhang et al., 2020; Qi et al., 2017, 2016; Chen et al., 2017; Lu et al., 2019; Chen et al., 2023). In\\nwhat follows, we categorize previous work into bag-level, instance-level and other methods.\\n\\nBag-level methods.\\nRueping (2010) proposed a large-margin support vector regression (SVR) method for learning with label\\nproportions (LLP). The proposed approach models the mean of each bag as a \\u201csuper-instance\\u201d with a soft\\nlabel equal to the label proportion of the bag. Yu et al. (2014) introduced the Empirical Proportion Risk\\nMinimization (EPRM) framework, which minimizes the bag-level loss function. Yu et al. (2014) also\\nderived a VC-dimension-based uniform convergence bound for the gap between the empirical and\\npopulation bag-level loss functions. Ardehaly & Culotta (2017) applied the aggregated cross-entropy loss to\\ndeep learning and classification problems.\\n\\nInstance-level methods.\\nYu et al. (2013) proposed a method that also uses the idea of support vector regression (SVR), but models\\nthe probability of each instance rather than each bag, in contrast to Rueping (2010). The proposed loss\\nfunction has two components: (1) the sum of the hinge losses between the unknown individual label and\\nthe predicted label of each instance, and (2) the sum of the losses between the label proportion of the\\nunknown individual labels of each bag and the true label proportion of that bag. Dulac-Arnold et al. (2019)\\nuses a similar idea to (Rueping, 2010) and considers relaxation to make the optimization problem more\\ntractable. For classification problems, Busa-Fekete et al. (2023) derived an unbiased estimator of the\\nindividual labels of the data examples. This estimator is a function of the label proportion of the bag\\nto which the example belongs and the probability distribution of all labels in the population. By using this\\nestimator of the individual labels, one can apply the usual supervised learning methods.\\n\\nOther related works.\\nQuadrianto et al. (2008) proposed a kernelized conditional exponential model for inferring the individual\\nlabels of unseen examples based on training examples grouped in bags and the label proportion of the\\nbags. The method is based on maximizing the log-likelihood of the model. A key assumption of the model\\nis that the features of an example are conditionally independent of the bag to which it belongs, given the\\nexample\\u2019s label. Fish & Reyzin (2017) formally defines the class of functions that can be learned from label\\nproportions (LLP Learnable) and resolves foundational questions about the computational complexity of\\nLLP and its relationship to PAC learning. Scott & Zhang (2020) solves the LLP problem through reduction to\\nmutual contamination models (MCMs). Saket (2021) investigated the learnability of linear threshold functions\\n(LTFs) from label proportions.\",\n",
      " \"methodology\": \"In this work we establish a connection between bag-level and instance-level losses, showing that the latter\\ncan be perceived as a regularized version of the former. We first show this claim for quadratic loss and then\\ndiscuss an extension to general convex losses. We next provide a theory for linear models, which also\\nallows us to analytically derive the optimal choice of \\u03c1.\",\n",
      " \"results\": \"We next specialize the result of Theorem 2.5 to \\u03c1 = 0 (corresponding to the bag-level loss) and\\n\\u03c1 = 1 (corresponding to the instance-level loss). Note that the bag-level loss is the ordinary least square\\nestimator with m = n/k samples and d parameters and therefore is well-defined only for n \\u2265 kd. In the\\nasymptotic regime of Assumption 2.3, this implies that \\u03a8 = limd\\u2192\\u221e n/d \\u2265 k.) The following corollary\\ncharacterizes the\n",
      "File number 524,\n",
      " {\n",
      " \"abstract\": \"Homomorphic encryption (HE) enables privacy-preserving cloud computing by allowing direct computations on ciphertexts. The computations involve modular reduction, and the overall complexity of ciphertext multiplication can be reduced by utilizing the quotient. Previous work considers the cases that the dividend is an integer multiple of the modulus and the modulus is in the format of 2w \\u2212 2u \\u00b1 1, where u < w/2. This paper generalizes the division for larger u and dividend not an integer multiple of the modulus. An algorithm is proposed to compute the quotient, and efficient hardware architecture is developed for implementing the algorithm.\",\n",
      " \"introduction\": \"Homomorphic encryption (HE) allows computations to be carried out on ciphertexts without decryption. Popular HE schemes involve computations over very long polynomials, whose coefficients are large integers, and coefficient multiplication and addition are followed by modular reduction. To reduce the computation complexity, the large coefficients are represented by residue number system, and moduli with a small number of nonzero bits are chosen. It was found in [5] that the overall complexity of ciphertext multiplication can be reduced by combining and reformulating the coefficients multiplication and relinearization, which is enabled by using the quotient of dividing coefficients product by q.\",\n",
      " \"literature review\": \"The division can be implemented by using a look-up table [6]. However, the size of the look-up table increases exponentially with the number of bits to divide in each clock cycle. Approximate division by very short integers, such as 8-bit, has been investigated for image processing in [7], [8]. The approximations in these schemes lead to a big difference in the quotient for large q. In [9], the quotient is derived by multiplying an approximation of q\\u207b1. To improve the precision, 2w bits are used to represent the approximation when q has w bits. The dividend is a product of two w-bit coefficients and also has 2w bits. Hence, a 2w \\u00d7 2w wide multiplier is needed, and it leads to not only a long data path but also a large area. To address these issues, the quotient is calculated as a \\u00d7 \\u03bb + b, and then the least significant bits are deleted in [10]. Here a and b are precomputed constants with at most w bits. Although the width of the multiplicand is reduced, a wide multiplier is still needed for this design.\",\n",
      " \"methodology\": \"The design in [5] assumes that q = 2w \\u2212 2u \\u00b1 1. Utilizing the property that q has a small number of nonzero bits, the quotient is calculated by addition and shift operations that have much shorter data path and smaller area requirements compared to those multiplying approximation of q\\u207b1 as in [9], [10]. However, the design in [5] is limited to the case of u < w/2, and the dividend is an integer multiple of q. Given the product of two coefficients, its remainder of division by q needs to be calculated and subtracted first before the division can be carried out.\",\n",
      " \"results\": \"For w = 32, there are 31 different possible u. For 50%, 16%, and 9% of these possible values of u, the proposed design achieves 55%, 32%, and 9% shorter latency, respectively, and at least 79% silicon area reduction compared to the divider in [10].\",\n",
      " \"conclusion\": \"This paper proposed a low-complexity integer divider for calculating the quotient when the divisor has a small number of nonzero bits. It generalized the previous design to handle more possible divisors and the case that the dividend is not an integer multiple of the divisor. In addition, by analyzing the possible values of the intermediate results, simplifications on the hardware implementation architectures are developed. Compared to prior designs that are based on multiplying the inverse of the divisor, the proposed design reduces the area requirement to a fraction and also has much shorter latency.\"\n",
      "}\n",
      "Error in PhotoBot: Reference-Guided Interactive Photography via Natural Language\n",
      "File number 527,\n",
      " {\n",
      " \"abstract\": \"We introduce PhotoBot, a framework for automated photo acquisition based on an interplay between high-level human language guidance and a robot photographer. We propose to communicate photography suggestions to the user via a reference picture that is retrieved from a curated gallery. We exploit a visual language model (VLM) and an object detector to characterize reference pictures via textual descriptions and use a large language model (LLM) to retrieve relevant reference pictures based on a user\\u2019s language query through text-based reasoning. To correspond the reference picture and the observed scene, we exploit pre-trained features from a vision transformer capable of capturing semantic similarity across significantly varying images. Using these features, we compute pose adjustments for an RGB-D camera by solving a Perspective-n-Point (PnP) problem. We demonstrate our approach on a real-world manipulator equipped with a wrist camera. Our user studies show that photos taken by PhotoBot are often more aesthetically pleasing than those taken by users themselves, as measured by human feedback.\",\n",
      " \"introduction\": \"Photography involving human subjects is an interactive activity between the photographer and models. Beyond just taking well-shot pictures, a professional photographer needs to understand what the client wants and provide suggestions and advice for pictures. Prior works have focused on the technical aspect of photography (e.g, how to navigate, plan and control a robot to frame a picture), but not on the interaction between the photographer and model.\\nIn this work, we introduce PhotoBot, a framework for automated photo acquisition based on an interplay between high-level human guidance and a robot photographer. PhotoBot is capable of interacting with the user by leveraging the common sense reasoning [] capabilities of large language models (LLMs) and the grounding capabilities of visual language models (VLMs). Specifically, we first convert a curated gallery of images into text-based descriptions (e.g., including information such as a general description of the image, the mood, and the number of people in the image) using a VLM and object detector. The VLM and object detector provide a scaleable and automated approach to describe curated images using language. Given a language query from a user and the detected objects in the observed scene by the camera, we use an LLM to retrieve a relevant reference picture (i.e., an existing image from the curated gallery of high-quality photographs) to suggest to the user through text-based reasoning. The user then imitates what is shown in the image and PhotoBot solves for the respective camera motion and image crop such that the camera view matches the reference picture. We formulate camera view adjustment as a Perspective-n-Point (PnP) problem with pre-trained features from a vision transformer capable of capturing semantic similarity across significantly varying images. \",\n",
      " \"literature_review\": \"Robot photography has previously been studied from the perspective of mobile robotics. Early work such as [3], [5], introduces an end-to-end mobile robot that navigates events, detects people using face detection, and takes pictures based on hand-engineered composition rules combined with a planner. In [9], a combination of sound and skin detection is used to frame subjects. A robot photographer is introduced in [2], which uses head detection and handcrafted photography composition rules combined with a subsumption control system to take pictures. KL divergence optimization is used in [10] to evaluate composition quality while considering the facial direction of photographed human users. The authors of [4] introduce a method to frame human and non-human subjects based on motion parallax and optical flow techniques in posed and cooperative settings. Methods such as [11] have also studied generalizing autonomous photography to pictures that do not contain human subjects using aesthetic criteria. Similar to [4] and [11], our PhotoBot framework generalizes to non-human subjects. Additionally, we also consider the posed and cooperative setting as done in [4].\\nLearning has been used in robot photography to formulate aesthetic models [12] and to directly learn reinforcement learning policies that optimize aesthetic-based rewards [13].\\nUser interactivity in the context of robot photography is addressed in [14] by having the robot move towards users waving their hands. In our work, we are also interested in user interactivity. However unlike in [14], which focuses on getting the attention of the robot photographer, we focus on a different task of suggesting picture ideas to the user, which is something a typical professional photographer would do.\\nCloser to our approach for suggesting picture ideas, [1] uses an LLM to produce text descriptions of pictures that a photographer would typically be expected to take at an event that is described at a high level. A VLM is then used to find the best image matches to these text descriptions from a video stream of a camera on a robot. In our work, we focus on producing picture suggestions based on personal user queries and scene observations. We provide actual reference pictures as suggestions and not textual descriptions of pictures as done in [1]. As opposed to using a VLM, our search and match procedure is done purely in text space using an LLM and text descriptions of existing images in a gallery. Using an LLM allows more sophisticated reasoning and better explainability.\",\n",
      " \"methodology\": \"Our proposed pipeline consists of a reference suggestion component that retrieves reference pictures for the user to imitate and a camera view adjustment component that adjusts the camera view to achieve a similar photo composition as the reference. An overview of the system is shown in Figure 2.\\nFor reference suggestions, we use image captioning from a VLM and object detection from an object detector to generate textual descriptions of a gallery of reference pictures and the current observed scene. An LLM then considers these textual descriptions, along with a user query, to retrieve multiple relevant reference pictures, from which the user selects one.\\nThe reference picture is taken from a different scene, but it is assumed that the current scene and the reference picture contain semantically similar elements.\\nWe formulate the view adjust problem as a PnP problem, where we estimate the target pose of an RGB-D camera based on correspondences between the 3D points it captures and the 2D points in the reference picture. Since the reference picture is captured from a different scene, and contains object instances with highly different appearances, traditional local appearance-based features (e.g., SIFT [15]) are inadequate for establishing correspondences. Additionally, since the degree of discrepancy between the current scene and the reference varies between problem instances, common methods for filtering out spurious correspondences are difficult to apply. For example, one person might pose more similarly to a reference picture than someone else, and so traditional RANSAC-based methods [16] that use fixed thresholds to filter out spurious correspondences often lead to suboptimal results.\\nTo address these challenges, we exploit recent advances in self-supervised vision transformers to extract high-level semantic correspondences between the current and reference views. Given these correspondences, we solve the PnP problem using robust estimation methods that do not require a fixed threshold for filtering spurious correspondences. In the sections that follow, we describe the user interaction workflow in Section III-A and the reference picture selection process in Section III-B. We then discuss the extraction of semantic correspondences in Section III-C and viewpoint alignment in Section III-D.\",\n",
      " \"results\": \"We validate and evaluate PhotoBot using a real-world Franka Emika robot manipulator equipped with a RealSense D435 RGB-D camera. We deploy PhotoBot and take pictures of various scenes involving both humans and objects. First, we conduct a user study and evaluate the effectiveness of reference suggestions and the view adjustment procedure. Second, we study the effects of the crucial RANSAC inlier reprojection error threshold \\u03c4 on the quality of the PnP solution when using DINO-ViT features. Third, we then investigate the quality of solutions as a function of the number of keypoints k. Finally, we qualitatively investigate if PhotoBot can generalize to reference pictures with larger distribution shifts (e.g., paintings).\\n\\nA. Human Preference Evaluation\\nTo evaluate the reference-based photography approach used in this work, we had a group of users (N = 8) interact and have their pictures taken by PhotoBot. We prompted users to query PhotoBot for picture suggestions from three general categories of emotions: confident, happy, and surprised. Based on each query, three reference photograph suggestions were provided, using the procedure detailed in Section III-B, from which the user chose one. For this experiment, we used a gallery with 50 images in total. The users then posed like the reference image to the best of their ability and PhotoBot took their pictures following the camera view adjustment procedure outlined in Section III-D.\\nWe first investigate whether PhotoBot takes aesthetically pleasing photos that address the query from the user. As a baseline, which we name \\u201cNo PhotoBot\\u201d, we asked the same set of users to come up with a gesture and expression that matched the category of emotion and to pose in front of a static camera, from which we took their picture. Users took the \\u201cNo PhotoBot\\u201d pictures before interacting with PhotoBot so that they were not influenced by any reference pictures.\\nExamples of \\u201cNo PhotoBot\\u201d are shown in the first column of Figure 5.\\nWe then surveyed a separate group of individuals (M = 15), unrelated to this work, to evaluate the pictures from PhotoBot and \\u201cNo PhotoBot\\u201d. We simultaneously presented the surveyed users with two pictures, one from PhotoBot and one from \\u201cNo PhotoBot\\u201d, and asked them to pick the picture that was both (1) more aesthetically pleasing and (2) better addressed\n",
      "Error in StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion\n",
      "File number 529,\n",
      " {\n",
      " \"abstract\": \"Recent advancements in language models (LMs) have achieved impressive zero-shot voice conversion (VC) performance. However, existing LM-based VC models usually apply offline conversion from source semantics to acoustic features, demanding the complete source speech, and limiting their deployment to real-time applications. In this paper, we introduce StreamVoice, a novel streaming LM-based model for zero-shot VC, facilitating real-time conversion given arbitrary speaker prompts and source speech. Specifically, to enable streaming capability, StreamVoice employs a fully causal context-aware LM with a temporal-independent acoustic predictor, while alternately processing semantic and acoustic features at each time step of autoregression which eliminates the dependence on complete source speech. To address the potential performance degradation from the incomplete context in streaming processing, we enhance the context-awareness of the LM through two strategies: 1) teacher-guided context foresight, using a teacher model to summarize the present and future semantic context during training to guide the model\\u2019s forecasting for missing context; 2) semantic masking strategy, promoting acoustic prediction from preceding corrupted semantic and acoustic input, enhancing context-learning ability. Importantly, StreamVoice is the first LM-based streaming zero-shot VC model without any future look-ahead. Experimental results demonstrate StreamVoice\\u2019s streaming conversion capability while maintaining zero-shot performance comparable to non-streaming VC systems.\",\n",
      " \"introduction\": \"Voice conversion (VC) aims to transfer a speaker\\u2019s voice to that of another speaker without changing the linguistic content. Such technique has been deployed in many real-world applications, e.g., movie dubbing, privacy protection, and pronunciation correction etc. With the help of neural semantic features, such as the bottleneck feature (BNF) from an automatic speech recognition (ASR) system, converting source speech from arbitrary speakers in the wild has been successfully achieved. Meanwhile, converting to an arbitrary target speaker with only one utterance of this speaker, which is the so-called zero-shot VC, also has been researched recently. However, most existing zero-shot VC models are designed for offline systems, which are insufficient to meet the recent growing demands of streaming capability in real-time VC applications, such as live broadcasting and real-time communication (RTC). In this study, we focus on the streaming zero-shot VC as illuminated in Fig. 1. Disentangling speech into different components, e.g., semantic content and speaker timbre, plays an important role in the zero-shot VC task. Recently, benefiting from the powerful LM framework and the scaling up of training data, LM-based VC models with built-in in-context learning ability can sense the context relations between source and target speaker\\u2019s utterances to capture fine-grained speaker timbre, achieving impressive zero-shot VC performance. However, demanding the complete source speech utterance limits these LM-based VC models in real-time scenarios, and thus these models can only be used in offline applications. While several non-LM-based methods have been proposed for streaming zero-shot VC, the performance fails to generalize well to unseen speakers with high speaker similarity and speech naturalness, mainly due to the limited model capacity to scale up training data, and also the performance degradation caused by the missing future information in streaming scenario.\",\n",
      " \"literature_review\": \"In recent years, advancements in language models (LMs) within natural language processing have showcased potent generation capabilities, influencing the development of LMs in speech generation. By employing codec or other SSL models, speech and audio can be efficiently tokenized into discrete units, facilitating low-bitrate audio representation and semantic information extraction. This progress allows speech generation to seamlessly utilize LM frameworks. Taking audio generation as a conditional language modeling task, AudioLM and MusicLM employ hierarchical language modeling for acoustic prediction from coarse to fine units. VALLE and Speartess2STT extend LMs for zero shot-speech text to speech (STTS), which can clone a human\\u2019s voice with prompt tokens from a short recording. For zero-shot VC, LM-VC employs task-oriented optimization for this tasks. Some studies leverage multi-task objectives and datasets, achieving high-quality conversion.\",\n",
      " \"methodology\": \"As shown in Fig. 2, the development of StreamVoice follows the recognition-synthesis framework. In this framework, speech is first represented as semantic features s = {s1, s2, ...sTs} and acoustic features a = {a1, a2, ..., aTa} by a pre-trained streaming ASR model and a speech codec model respectively. Here, Ts and Ta denote the sequence length. Before inputting to StreamVoice, s and a are aligned to the same length T. StreamVoice incorporates a context-aware language model and an acoustic predictor to perform a single language modeling process. With the semantic and acoustic features {~s, ~a} of speech from the target speaker as speaker prompt, the LM leverages the semantic information a1:t of source speech to autoregressively predict the hidden output ch. In each autoregression time-step of the LM, the acoustic predictor transforms the hidden output ch to the codec feature ^a of the converted speech. Finally, the decoder of the codec model reconstructs the waveform from the predicted codec feature. In the following sections, we will introduce how to build a streumable LM for VC and how to ensure the high-quality conversation of this streaming VC. Fully Casual Language Model As shown in Fig. 3, inspired by the success of the LM-based VC model, we intend to achieve streaming zero-shot VC by language models. In previous LM-based VC models, the demand of the complete semantic feature s from source speech to achieve conversion hinders the deployment for real-time application, which can be formulated as p(at|s1:Ts, a1:t\\u22121) for each time step. To achieve streaming, any components of the LM cannot rely on future information. As shown in Fig. 3, decoder-only LM with unidirectional attention can easily fit the requirement of casual generation. To eliminate the dependency of the complete semantic input, semantic and acoustic features {s, a} are first aligned with each other to the same sequence length T and then they are alternatively inputted to the LM, forming a cross-embedding like {s1, a1, s2, a2, ..., sT , aT }. With these modifications, the LM can achieve streaming processing, modeling p(at|s1:t, a1:t\\u22121). ContinuousCodec1a2a3a4a5...ContinuousProjectLinearPredictionLinearPredictionLinearPredictionDiscreteProjectionL-layerDiscreteCodecTransformerBlockchta1ta2ta...Lta11a1...L1a2L2a3L3a4L4a5...2a13a14a115a...ttaLContDiscL\",\n",
      " \"results\": \"In this section, we present both subjective and objective evaluations and the ablation study conducted on StreamVoice. A detailed analysis of dependency in the streaming pipeline is also provided.Zero-shot evaluationTo evaluate the zero-shot VC performance, one recent LM-based zero-shot VC system, LM-VC, is selected as the topline system. Besides, a variant of StreamVoice, referred to as NS-StreamVoice, using a non-streaming ASR for semantic extraction, is also compared. We implement the proposed system StreamVoice integration discrete projection, while C-StreamVoice also involves the evaluation since speech can represented in continuous form by codec model. Table. 1 presents both subjective and objective results. Compared with the non-streaming topline LM-VC, our proposed StreamVoice can achieve close results regarding subjective NMOS and SMOS, while a performance gap still exists. Similar results are also observed in objective results. The non-streaming StreamVoice even surpasses the topline model in certain aspects, indicating the effectiveness of our well-designed streumable architecture for zero-shot VC. As can be observed, C-StreamVoice exhibits inferior performance compared to the discrete version, which can contribute to the reported over-smoothness in speech generation and the mismatch between the ground-truth and predicted features. Furthermore, as illustrated in Table. 3, the RTF of the entire pipeline is below 1, which meets the real-time requirement. Consisting of chunk-waiting latency (80ms) and model inference latency, the overall pipeline latency is 124.3 ms. If using a V100 GPU, StreamVoice can obtain an RTF of 0.56 and the overall latency reaches 137.2ms. Importantly, unlike previous streaming VC, our VC model is entirely causal without any future look-ahead, highlighting its powerful modeling capability. These results demonstrate that StreamVoice can achieve high-quality zero-shot VC in streaming scenarios.\",\n",
      " \"conclusion\": \"In this paper, we introduce StreamVoice, a novel LM-based zero-shot VC system designed for streaming scenarios. Specifically, StreamVoice employs a streamlined, single-stage framework that encompasses a context-aware LM and an acoustic predictor. The casual design of the model\\u2019s input and structure ensures compliance with streaming behavior. To address performance degradation caused by missing complete contextual information in streaming scenarios, context-aware LM adopts teacher-guided context foresight to make the model have the ability to forecast the current and future information given by a teacher. Besides, semantic masking is introduced in LM to enhance context learning from historical input and facilitate better disentanglement. Finally, an acoustic predictor collaborates with the LM to generate the target speech. Experiments demonstrate that StreamVoice achieves streaming zero-shot VC while maintaining performance comparable to non-streaming VC systems. Limitations and future work. We have to point out that StreamVoice still has limitations. In our configuration,\n",
      "File number 532,\n",
      " {\n",
      " \"Abstract\": \"In scenarios where data is limited or incomplete, machine learning classification models face challenges. The Small and Incomplete Dataset Analyser (SaNDA) emerged as a suitable technique to improve performance in these situations by leveraging data abstractions. This paper evaluates how modifying the abstraction methods affects the SaNDA classification process's explainability and accuracy. We investigate alternative abstraction techniques, constant binning and quantiles, in contrast to the conventional ROC curve-based method. Our experiments on 13 small and two medium datasets, including synthetic DIGEN datasets, provide empirical evidence. The findings suggest that quantile-based abstractions (quantiles 20) offer better classification performance compared to constant binning or the original ROC curve method. The number of categories produced by the abstraction method is of greater significance than the type of abstraction used, influencing accuracy. Increasing the abstraction levels consistently enhances performance but introduces limitations related to statistical representation, explainability, and computational efficiency. Our modified SaNDA with quantiles 20 abstraction method exhibits promising potential as an alternative to Random Forest for complete datasets and demonstrates its advantage with missing data. Future work may involve developing a dynamic approach to selecting methods and abstraction levels, allowing for customization to specific problem requirements.\",\n",
      " \"Introduction\": {\n",
      "  \"Relevance_of_Data_Abstraction_Methods\": \"The applicability of widely adopted machine learning (ML) methods to classification for Critical Decision-Making (CDM) processes is circumscribed by the imperatives of explicability and uncertainty management.\",\n",
      "  \"Background_of_SaNDA\": \"Recently, Small and Incomplete Dataset Analyser (SaNDA) has been proposed to enhance the ability to perform classification in such domains, by develop-ing a data abstraction protocol using a ROC curve-based method.\"\n",
      " },\n",
      " \"Literature_Review\": {\n",
      "  \"Challenges_with_Deep_Learning\": \"Classification for CDM processes encompasses two fundamental attributes: explainability [24] and uncertainty [34], characteristics notably absent in outcomes generated by deep learning [6, 30].\",\n",
      "  \"Limitations_of_Random_Forest\": \"Random Forest was originally presented as an approach for improving the accuracy of single decision tree which sometimes has problem with over-fitting and therefore with generalization [3].\"\n",
      " },\n",
      " \"Methodology\": {\n",
      "  \"Verification_Metrics\": \"Several metrics are used as a valuable tool for comparing the performance of ML classification models. They provide a comprehensive assessment of a model\\u2019s performance across different aspects.\",\n",
      "  \"SaNDA_Classification_Method\": \"SaNDA consists of two main parts, which interacts with each other to provide compre-hensive method of data analysis: classification, and KG [15].\",\n",
      "  \"Abstraction_Methods\": \"Abstraction as a procedure for reducing the number of values that a variable can take is defined in Sec. 2.2.\"\n",
      " },\n",
      " \"Results\": {\n",
      "  \"Comparison_of_Abstraction_Methods\": \"For each dataset detailed in Section 3, it contrasts the BA acquired from SaNDA across various abstraction types: ROC, deciles (quantiles 10), quantiles 20, 10 bins, and 20 bins.\",\n",
      "  \"Quantile-Based_Abstractions_Offer_Better_Classification_Performance\": \"Based on this experiment, it can be observed that overall quantiles 20 offers better classification performance compared to 20 bins abstraction method.\",\n",
      "  \"Number_of_Categories_Produced_by_Abstraction_Method_Impacts_Accuracy\": \"One might naively believe that increasing the number of abstractions will enhance classification accuracy. However, this approach harbours potential pitfalls that warrant careful consideration.\"\n",
      " },\n",
      " \"Conclusion\": {\n",
      "  \"SaNDA_with_Quantiles_20_Abstraction_Method_as_Alternative_to_Random_Forest\": \"The proposed new data abstraction methods significantly outperform abstractions using ROC.\",\n",
      "  \"Future_Work\": \"A future direction of research may involve the choice of an appropriate method and level of abstraction.\"\n",
      " }\n",
      "}\n",
      "File number 533,\n",
      " {\n",
      " \"abstract\": \"This paper presents a study investigating human cognitive augmentation due to using ChatGPT in two experiments and concludes that using ChatGPT does not always enhance cognitive ability or replace human judgment and discernment.\",\n",
      " \"introduction\": \"The use of tools enhances human cognitive performance, with calculators and spreadsheets improving mathematical calculations and software aiding in processing words, images, video, and numbers. However, unsupervised, deep, machine learning techniques have led to cognitive systems (cogs) outperforming humans in certain domains. These cogs can be utilized in a human/cog ensemble, leading to human cognitive augmentation. Specifically, the effect of ChatGPT on human cognitive augmentation is explored in this paper.\",\n",
      " \"literature_review\": \"Cognitive systems (cogs) have been designed using artificial intelligence (AI) and cognitive systems (cogs), allowing humans and cogs to collaborate in human/cog ensembles, enhancing human cognitive performance. Cogs outperform humans in various domains, such as detecting lung cancers better than human doctors and diagnosing childhood depression better than humans. Cognitive augmentation depends on the cog's sophistication, the amount of cognitive work it performs, and the human's level of collaboration. ChatGPT, a large language model that mimics human-created text, is used by millions of people for various tasks.\",\n",
      " \"methodology\": \"The authors conducted two experiments to compare human cognitive performance with and without using ChatGPT. In the first experiment, one group of students was assisted by ChatGPT, while the other was not, to synthesize innovative solutions to a problem statement. The second experiment involved retirement planning, with one group receiving ChatGPT assistance and the other prohibited from using ChatGPT. The responses were evaluated for expert-level quality.\",\n",
      " \"results\": \"For the innovation challenge, both groups predominantly suggested changing the field rather than the skeet, showing that ChatGPT did not alter the type of solutions generated. Interestingly, students using ChatGPT suggested ideas unrelated to the problem statement, potentially due to ChatGPT's response being driven by word association. For the retirement decision challenge, while more students using ChatGPT provided expert-level answers than those not using ChatGPT, the difference was not definitive. Furthermore, if non-ChatGPT students were disallowed from using a retirement calculator, ChatGPT students might have performed better.\",\n",
      " \"conclusion\": \"Although ChatGPT is capable of producing expert-level responses, it does not guarantee it. Cognitive processes requiring high-level analysis, understanding, evaluation, and judgment are more difficult for ChatGPT to handle. ChatGPT may relieve humans of lower-level cognitive tasks, leading to significant cognitive augmentation in the future.\"\n",
      "}\n",
      "File number 534,\n",
      " {\n",
      " \"abstract\": \"The concept of Spatial XR-IoT (XRI) Zone Agents is presented, merging Extended Reality (XR), the Internet of Things (IoT), and spatial computing for smart environments. These zone agents serve as applications and companions in shared spaces, reducing the gap between the physical environment and traditional user interfaces. By incorporating Mixed Reality Agents (MiRAs), agent and scene design strategies are outlined for spatial zone agents. A prototype and user interaction scenario demonstrate human-to-space agent relationships in an immersive smart-space application.\",\n",
      " \"introduction\": \"The metaverse integrates computing technologies to combine virtual and physical spaces, connecting the digital and real worlds. However, a gap exists between users' physical and virtual environments, referred to as the metaverse disconnect. To address this, the method of XRI (XR-IoT) is introduced, supporting hyper-connected metaverse environments. XRI combines IoT components and hybrid physical-virtual interactions to create smart spaces that are social, smart, engaging, and immersive. These concepts lay the groundwork for enriching user information architecture across applications and smart space configurations.\",\n",
      " \"literature review\": \"Researchers have explored methods to enhance the connection between users and smart spaces, aiming to reduce the metaverse disconnect. The XRI concept, combining XR and IoT, is presented along with early frameworks and proof-of-concept prototypes. These prototypes demonstrated the integration of virtual plants in mixed reality connected to physical environments via IoT, as well as enhanced head-mounted display prototypes. Metaverse has various definitions, reflecting its evolving state, ranging from immersive 3D digital environments to a combination of internet, web technologies, and extended reality. The metaverse is envisioned as the next generation of the internet, increasing connectivity with physical spaces.\",\n",
      " \"methodology\": \"The research integrates insights from Mixed Reality Agents (MiRAs), spatial computing and interface, and agent system design. These theories are combined and adapted into a three-dimensional design space for XRI Zone Agents. The dimensions include the mixed reality dimension, the level of agency, and the physical-remote (PR) spatial interaction capacity. This framework is used to design spatial zone agents with different interactions, capabilities, and objectives.\",\n",
      " \"results\": \"A prototype of the XRI Zone Agent is presented, demonstrating scenario-based user interaction within a laboratory environment. When users navigate the lab wearing an XR HMD, they encounter virtual agents, such as a plant avatar, that provide guidance and enable user-selected tasks. These tasks include study mode, relaxation mode, and meeting mode, each represented by a spatial zone agent. The agents respond to user actions, such as thumbs-up gestures, and control virtual objects and physical IoT devices (e.g., lights, projectors) based on user context and spatial location.\",\n",
      " \"conclusion\": \"This work offers a theoretical framework for considering smart-space zones as agents and explores mixed reality and IoT (XRI) zone agents using design-theoretic methods. It envisions that zone agents would adapt to user spatial contexts and needs through changes in physical or virtual reality. The presented high-level user interaction design, based on three zone agents for work, leisure, and meetings, illustrates potential applications of these concepts. The research aims to pave the way for further exploration of space-to-human and human-to-space interactions driven by mixed-reality agents and user and environmental context within zones.\"\n",
      "}\n",
      "File number 536,\n",
      " {\n",
      " \"abstract\": \"Insecure content such as cyberbullying, self-harm, and sexually explicit images are increasingly being shared on social media platforms. These platforms use AI and human moderation to safeguard users from such images and provide an explanation of why they are flagged. However, two critical problems arise from this process - identifying a rationale for obfuscating the images and deciding how much to obfuscate. This work addresses these issues by proposing a visual reasoning model (VLM) called ConditionalVLM, conditioned on pre-trained unsafe image classifiers, to provide an accurate rationale grounded in unsafe image attributes. Additionally, the paper introduces a counterfactual explanation algorithm that minimally obfuscates unsafe regions for safe viewing. Our experiments on real-world data demonstrate the efficacy of the proposed method.\",\n",
      " \"introduction\": \"Social media platforms are increasingly being misused for sharing unsafe content such as sexually explicit images, cyberbullying, and self-harm. To safeguard users from such content, major platforms employ AI and human-based content moderation techniques to flag and obfuscate (i.e., make the image safer by blurring sensitive regions) such images. This process involves obfuscating unsafe image regions in the image (Li et al. 2017) along with generating a rationale that backs up the decision to obfuscate the flagged images (Meta 2022). The image obfuscation process faces two critical problems regarding how much of the unsafe image is obfuscated and why it is obfuscated: First, the decision to deem an image unsafe and obfuscate it demands providing a rationale for the decision. For example, Instagram moderators are required to provide a legal rationale (Bronstein 2021; Are 2020) to back up their decision (Tenbarge 2023). Existing visual reasoning methods (Li et al. 2022, 2023; Dai et al. 2023) are severely limited for unsafe images such as sexually explicit, cyberbullying, and self-harm since they cannot provide a rationale grounded in attributes that are specific to such images, such as rude hand gestures in cyberbullying images (Vishwamitra et al. 2021), or sensitive body parts in sexually-explicit images (Binder 2019). Second, the unsafe image needs minimal obfuscation while still depicting the safe regions for evidence collection and investigation (Billy Perrigo 2019). For instance, human moderators need to determine the age of the person in the image (e.g., in child sex-ual abuse material (CSAM) investigations), look for identi-fiers (e.g., tattoos, scars, and unique birthmarks), and deter-mine their location information (e.g., landmarks, geograph-ical features, and recognizable surroundings). Current seg-mentation techniques (Chandrasekaran et al. 2021; Vermeire et al. 2022; Bethany et al. 2023) cannot minimally identify the regions and consequently impede investigations that per-tinently need full details of the remaining safe regions.\",\n",
      " \"literature_review\": \"Social media platforms are constantly misused to share un-safe content. For example, sexually-explicit images (Ashurst and McAlinden 2015; Sanchez et al. 2019), non-consensual in-timate images (NCII) (Lenhart, Ybarra, and Price-Feeney 2016) and child sexual abuse material (CSAM) are being in-creasingly shared on major social media platforms (Sanchez et al. 2019). Cyberbullying, a critical issue affecting ado-lescents and adults is spread unabated via images (Vish-wamitra et al. 2021). Furthermore, self-harm images are widely spread on these platforms that further alienate vulner-able users (John et al. 2018). To defend against this threat, social media platforms alter the unsafe image by blurring the sensitive regions. This obfuscation of such unsafe im-ages has substantial implications. For instance, social media platforms employ over a million moderators globally (bbc 2021), who manually view unsafe images which is known to have an adverse effect on their mental health, includ-ing PTSD (reu 2021). Vulnerable users, such as minors also need image safeguarding methods that can shield them from unsafe content. Furthermore, law enforcement agents who investigate images from a crime scene need image safe-guarding since such images contain extremely disturbing images, such as CSAM. These applications have a critical need, the unsafe regions must be minimally obfuscated. For instance, a moderator needs to view the safe regions to con-duct investigations, such as determining the age of the per-son to report CSAM content to law enforcement.\",\n",
      " \"methodology\": \"Figure 1 illustrates the architecture of our proposed ap-proach, which consists of two modules. The initial module utilizes ConditionalVLM for classifying images as safe or unsafe, while the subsequent module proposes counterfactual visual explanations to identify and obfuscate the unsafe regions within the image.\",\n",
      " \"results\": \"We evaluated our Conditional VLM and Counterfactual Subobject Explanation methods on three datasets of real-world harmful images to study the practical application of counterfactual subobject explanations.\\n\\nSexually Explicit: First, we sampled a subset of images from an NSFW images dataset (Kim 2021) consisting of 334,327 images by selecting the \\u201cporn\\u201d, \\u201cneutral\\u201d, and \\u201csexy\\u201d classes. We combine the \\u201cneutral\\u201d, and \\u201csexy\\u201d classes into a single class of \\u201csafe\\u201d images.\\n\\nCyberbullying: Second, we used a cyberbullying im-ages (Vishwamitra et al. 2021) dataset consisting of nearly 20,000 images belonging to the classes \\u201ccyberbullying\\u201d and \\u201cnon-cyberbullying\\u201d.\\n\\nSelf-Harm: Third, we used a self-harm images dataset (Bethany et al. 2023), consisting of 5000 images with classes \\u201cself-harm\\u201d and \\u201cnon self-harm\\u201d.\",\n",
      " \"conclusion\": \"In this work, we have presented ConditionalVLM, a vi-sual reasoning framework that generates accurate rationales for unsafe image descriptions by leveraging state-of-the-art VLMs conditioned on pre-trained unsafe image classi-fiers, and CSE, a counterfactual visual explanation tech-nique to obfuscate the unsafe regions in unsafe images for safer sharing. We evaluated these two methods on three cat-egories of unsafe images. An implementation of Condition-alVLM, which we called ConditionalBLIP showed supe-rior performance compared to other state-of-the-art image-to-text models on describing unsafe images. We also com-pare CSE against another recent unsafe image obfuscation method and show how our approach is effective in generat-ing causal explanations for obfuscating unsafe images.\"\n",
      "}\n",
      "File number 539,\n",
      " {\n",
      " \"abstract\": \"Modern vehicles employ intelligent systems like interconnected autonomous driving and advanced driving assistance systems for improved driving experiences, which are enabled by increased connectivity to infrastructure and the fusion of information from multiple sensing modes. However, the increased connectivity combined with the inherent security flaws in legacy network architecture renders vehicles vulnerable to active and passive attacks, directly affecting passenger safety. Researchers have proposed machine learning models for detecting attacks, whose deployments are enabled through quantised neural networks targeting low-power platforms. Our research presents a custom quantised MLP (CQMLP) model as a multi-class classifier capable of detecting multiple attacks from a benign flow of controller area network (CAN) messages. The specific quantisation and neural architecture were determined through a joint design space exploration, resulting in our choice of the 2-bit precision and the n-layer MLP. Our 2-bit version was trained using Brevitas and optimised as a dataflow hardware model through the FINN toolflow from AMD/Xilinx, targeting an XCZU7EV device. Our findings indicate that the 2-bit CQMLP model, when integrated as an IDS, can detect malicious attack messages (DoS, fuzzing, and spoofing attacks) with very high accuracy of 99.9%, on par with the state-of-the-art methods in the literature. Furthermore, the dataflow model can perform line rate detection at a latency of 0.11 ms from message reception while consuming 0.23 mJ/inference.\",\n",
      " \"introduction\": \"Automotive networks are undergoing rapid evolution to cater to the data-intensive needs of novel intelligent capabilities that enhance safety, infotainment, and comfort for passengers. These interconnected networks, via multiple network protocols, facilitate high-speed communication and information exchange between numerous electronic computing units (ECUs), sensors, and actuators present in modern cars. Among the network protocols, Controller Area Network (CAN) is widely used for critical communication between ECUs and continues to be the predominant network protocol for in-vehicle networks due to its cost-effectiveness and ease of use in control applications. Earlier ECUs and software functions deployed on the CAN were automatically siloed due to the limited connectivity of vehicles to the external world. However, modern vehicles rely on connectivity to infrastructure and other vehicles, enabling real-time sensing of the external environment and unique features like remote monitoring and control of specific capabilities for diagnostics and over-the-air upgrades. Researchers have demonstrated that such interfaces open new avenues for injecting malicious code or messages into these previously siloed networks. As a result, intrusion detection approaches have been proposed in the literature, allowing critical systems to enter a \\u2018safe working\\u2019 mode when such threats are detected. Machine learning (ML) approaches have shown significant improvement in detecting these threats and can adapt to newer attack vectors without the overheads of rule-based approaches. Additionally, quantised versions of floating-point models, termed quantised neural networks (QNNs), have demonstrated viability as alternatives, reducing computational complexity, resource, and energy consumption at the expense of a slight reduction in inference accuracy. QNNs are then deployed on constrained devices such as low-end FPGAs or embedded platforms. Frameworks such as Vitis-AI and FINN can convert Pytorch/TensorFlow representations at native precisions to quantised models and dataflow-style hardware accelerators for deployment on the FPGA.\",\n",
      " \"literature_review\": \"Rule-based approaches, commonly employed for intrusion detection systems (IDS), can be further classified into flow-based and payload-based approaches. While flow-based approaches identify traits like message frequency and/or interval for the network to detect abnormalities, payload-based approaches utilise the data segment in CAN frames to detect abnormal sequences of instructions or data. Researchers also explored hybrid schemes using both message timing/frequency and payload information. Machine learning-based techniques further generalise these techniques when trained with large datasets through classification, sequential, and deep learning-based approaches.\",\n",
      " \"methodology\": \"Our proposed methodology involves the formulation of a feed-forward custom quantised multi-layer-perceptron-based IDS for automotive CAN, the design of a dataflow-style custom quantised hardware implementation of the model, and the integration of the dataflow accelerator as an Advanced eXtensible Interface (AXI) slave peripheral to the ARM cores, offering complete isolation from the software tasks on them.\",\n",
      " \"results\": \"Our experimental results demonstrate that the proposed custom quantised (2-bit) MLP-based IDS (CQMLP-IDS) achieves an average accuracy of 99.91% across Denial of Service (DoS), Fuzzing, and RPM-spoofing attacks among the benign flow of messages, identical to or exceeding the detection accuracy achieved by state-of-the-art GPU- and CPU-based implementations. The tightly integrated ECU architecture reduces the per-message execution latency by 2.2\\u00d7 and the energy consumed by 3.9\\u00d7 compared to state-of-the-art IDSs proposed in the research literature. Furthermore, our (fp32) MLP model on a Jetson Nano, mimicking a dedicated IDS ECU, incurs 12\\u00d7 higher energy consumption per inference, compared to the integrated IDS-ECU.\",\n",
      " \"conclusion\": \"Our research presents a dataflow-style hardware accelerator that implements a custom quantised MLP (CQMLP) model for detecting and classifying multiple attack vectors on an automotive CAN network. By tightly integrating the accelerator to the ARM core on a Zynq Ultrascale+ platform, we demonstrate a 2.2\\u00d7 speed-up in per-message processing latency and almost 3.9\\u00d7 reduction in energy consumption with 0.23mJ per inference when compared to the state-of-the-art IDSs. The quantised model also maintains high detection and classification accuracy across various attacks, making it suitable for distributing IDS capabilities across ECUs in a vehicle network.\"\n",
      "}\n",
      "File number 540,\n",
      " {\n",
      " \"abstract\": \"We present five optimizations for a matrix-based CFL-r algorithm that utilize the specific properties of both the underlying semiring and the widely-used linear algebra library SuiteSparse:GraphBlas. Our experimental results show that these optimizations result in orders of magnitude speedup, with the optimized matrix-based CFL-r algorithm consistently outperforming state-of-the-art CFL-r solvers across four considered static analyses.\",\n",
      " \"introduction\": \"Context-Free Language Reachability (CFL-r) is a core building block for a wide range of static analyses. The problem asks to find pairs of vertices in an edge-labeled graph that are connected by a path labeled with a word from a Context-Free Language (CFL). While CFL-r is often solved by standard graph reachability algorithms, one promising line of research is to instead formulate CFL-r as a matrix-based operation on sparse matrices, as these operations can be efficiently executed on modern hardware.\",\n",
      " \"literature_review\": \"Azimov et al. [2] proposed a context-free path querying algorithm by matrix multiplication that oftentimes surpasses analogous in terms of performance. However, it takes a considerable amount of time to deal with paths with deep derivation trees and large CFGs of field- and context-sensitive analyses. Extensive research has been conducted to develop general-purpose CFL-r algorithms. POCR [6], Gigascale [4], and Graspan [16] represent three such tools.\",\n",
      " \"methodology\": \"We propose a set of optimizations for the matrix-based CFL-r algorithm that significantly improve its performance on large-scale CFL-r instances:\",\n",
      " \"results\": \"Our experiments show that the proposed optimizations result in orders of magnitude speedup, with the optimized matrix-based CFL-r algorithm consistently outperforming state-of-the-art CFL-r solvers across four considered static analyses: Field-Sensitive Java Points-To (FSJPT), Field-Insensitive C/C++ Alias (FICA), Field-Sensitive C/C++ Alias (FSCA), and Context-Sensitive C/C++ Value-Flow (CSCVF).\",\n",
      " \"conclusion\": \"We have presented a set of optimizations that significantly improve the performance of the matrix-based CFL-r algorithm, making it a competitive choice for solving large-scale CFL-r instances. Our future work includes complexity analysis as well as generalization of proposed optimizations for other algorithms.\"\n",
      "}\n",
      "File number 545,\n",
      " {\n",
      " \"abstract\": \"This paper aims to explore the direct relationship between emotion and state-of-the-art speaker embeddings in the form of intra-speaker clusters and provide a contrastive pretraining approach for speech emotion recognition. We conduct a thorough clustering analysis to demonstrate the extractability of emotion information from speaker embeddings. The proposed method leverages extensive emotion-unlabeled data and significantly improves speech emotion recognition performance.\",\n",
      " \"introduction\": \"Speech emotion recognition remains challenging due to its complexity and the subjective nature of emotional expression. Recent works explore the potential of speaker embeddings in enhancing speech emotion recognition, but they assume that emotion information is indirectly encoded within speaker embeddings. We aim to investigate the direct accessibility of emotion-related information within speaker embeddings and find effective ways to leverage this information in SER tasks.\",\n",
      " \"literature_review\": \"Previous studies have revealed increased equal error rates in speaker verification for non-matching emotional conditions, highlighting the sensitivity of speaker features to emotional states. Research has also demonstrated emotion-related information in speaker embeddings via autoencoder-based reconstruction analysis and emotion classification. Recent works have employed deep speaker embedding networks to transfer knowledge from speaker verification to speech emotion recognition. However, the potential of deep speaker embeddings in encoding emotional information remains an area that requires comprehensive exploration.\",\n",
      " \"methodology\": \"We evaluate the relationship between emotion and speaker embedding clusters using clustering analysis. We utilize d-vector and ECAPA-TDNN speaker embeddings and compute intra-speaker clusters corresponding to emotion categories. The evaluation metrics used include Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), Purity Score, and Silhouette Score. To leverage the clustered embeddings, we then introduce a novel contrastive learning approach for SER. We employ positive and negative example pairs based on the speaker embedding clusters and formulate a contrastive loss function. The contrastive learning is applied as the primary objective of pretraining and as an additional task for existing pretraining methods in a multi-task setting.\",\n",
      " \"results\": \"Our analysis reveals distinct intra-speaker clusters that reflect emotional states, suggesting a strong link between speaker and emotion recognition. Pretraining with the proposed contrastive strategy demonstrates significant improvement in speech emotion recognition compared to no pretraining and supervised speaker classification. The multi-task learning approach, which simultaneously considers intra-speaker variations and speaker-emotion connection, achieves the best performance. Fine-tuning a pre-trained wav2vec2.0 model with our contrastive learning and multi-task strategies further enhances speech emotion recognition performance.\",\n",
      " \"conclusion\": \"Our research establishes a direct link between emotions and state-of-the-art speaker embeddings, highlighting the potential of speaker embeddings for SER. The novel contrastive pretraining approach significantly improves SER performance, providing a practical solution for data scarcity in SER. Future work will extend the analysis of emotion information in speaker embeddings and explore factors affecting its appearance.\"\n",
      "}\n",
      "Error in Bounding Consideration Probabilities in Consider-Then-Choose Ranking Models\n",
      "File number 546,\n",
      " {\n",
      " \"abstract\": \"A common theory of choice posits that individuals make choices in a two-step process, first selecting some subset of the alternatives to consider before making a selection from the resulting consideration set. However, inferring unobserved consideration sets (or item consideration probabilities) in this \\u201cconsider-then-choose\\u201d setting poses significant challenges, because even simple models of consideration with strong independence assumptions are not identi\\u02bcfable, even if item utilities are known. We consider a natural extension of consider-then-choose models to a top-\\u03ba ranking setting, where we assume rankings are constructed according to a Plackett\\u2013Luce model after sampling a consideration set. While item consideration probabilities remain non-identi\\u02bcfable in this setting, we prove that knowledge of item utilities allows us to infer bounds on the relative sizes of consideration probabilities. Additionally, given a condition on the expected consideration set size, we derive absolute upper and lower bounds on item consideration probabilities. We also provide algorithms to tighten those bounds on consideration probabilities by propagating inferred constraints. Thus, we show that we can learn useful information about consideration probabilities despite not being able to identify them precisely. We demonstrate our methods on a ranking dataset from a psychology experiment with two di\\u02dferent ranking tasks (one with \\u02bcxed consideration sets and one with unknown consideration sets). This combination of data allows us to estimate utilities and then learn about unknown consideration probabilities using our bounds.\",\n",
      " \"introduction\": \"Among the wide-ranging topics studied in the behavioral sciences, predicting and explaining human choices or judgments is a central challenge across a range of disciplines. (Why that entr\\u00e9e at that caf\\u00e9 with that person, of all the restaurants you can think of, of all your potential dates, of all the dishes on the menu?) Settings where individuals select an item from a collection of available alternatives are well-studied in the literature on discrete choice [38], with applications ranging from marketing [10] and voting [37] to transportation policy [18] and recommender systems [12]. A closely related line of work studies rankings [3], rather than single choices, often modeling a ranking as a sequence of discrete choices (selecting the top-ranked item, then the second, etc.). The widely used Plackett\\u2013Luce ranking model [21, 30] exempli\\u02bcfes the link between discrete choice and ranking, positing that items at each of \\u03ba positions are selected in turn according to a logit choice model [25].\\nDespite the tight link between choice and ranking, some well-studied topics in discrete choice have not been fully explored in the more complex ranking setting. For instance, in keeping with the framework of bounded rationality [35], a prominent line of work in discrete choice suggests that selection is a two-stage process, where individuals \\u201cnarrow their options to a small consideration set from which their \\u02bcfnal selection is made [16, 34]. These so-called \\u201cconsider then choose\\u201d models have shown considerable promise in their explanatory power [4, 32], but su\\u02b8er from a major issue: consideration sets are almost always unobserved (except in carefully controlled experimental settings) and cannot in general be identi\\u02bcfed from observed choice data [19, 39]. Perhaps because of the problem of identi\\u02bcfability, consider-then-choose models have received comparatively little attention in the ranking literature.\",\n",
      " \"literature review\": \"The present work. In this paper, we study the natural consider-then-rank model obtained by augmenting top-\\u03ba Plackett-Luce with the independent-consideration rule [23], in which each item advances to the ranking stage randomly and independently with an unknown item-speci\\u02bcfc consideration probability.We term this model Plackett\\u2013Luce with consideration (PL+C). One might hope that richer observations\\u2014a ranking of\\u03ba items rather than just a single choice\\u2014would make it feasible to identify consideration probabilities, at least for large \\u03ba. Unfortunately, our \\u02bcfst result is negative: regardless of \\u03ba, there are in\\u02dfnite families of consideration probabilities that generate the same distribution of observed data. However, we show that it is possible to derive meaningful bounds on consideration probabilities, despite their non-identi\\u02bcfability. In addition to observations of rankings, we draw on two types of information for learning about consideration probabilities: (1) known item utilities and (2) a lower bound on expected consideration set size. (Later, we discuss settings where such information is indeed available.)\\nGiven known item utilities, we prove relative bounds on the consideration probabilities of di\\u02b8erent items, of the form \\u201cif item \\u2019s consideration probability is \\u03c0, then \\u2018s consideration probability is greater than \\u03c0\\u201d. The intuition is that if \\u2019 has higher utility then \\u2018, then we would expect to see \\u2019 ranked highly more often than \\u2018\\u2014but if instead \\u2018 outperforms \\u2019, then consideration must be the culprit, and \\u2018\\u2019s consideration probability must be higher than \\u2019s. Quantitatively, the uppers bound on item consideration probabilities. Intuitively, if an item \\u2019 is ranked \\u02bcfrste less often than we would expect given its utility, then it must not be considered very often relative to the other items. Using a pessimistic upper bound of 1 for other items\\u2019 consideration probabilities can then yield a nontrivial upper bound on \\u2019s consideration probability. To wrap up our theoretical contributions, we provide algorithms to combine our absolute and relative bounds by propagating over a directed acyclic graph induced by our relative bounds.\",\n",
      " \"methodology\": \"We begin with an impossibility result: even with complete information about PrPL+C(r) for every ranking r\\u2014and even with complete knowledge of the utilities u1, . . . , un for all items\\u2014we cannot infer the consideration probabilities p1, . . . , pn.In a PL+C model, consideration probabilities are not identi\\u02bcfable. That is, there exist multiple sets of consideration probabilities that generate exactly the same distribution over rankings, with \\u02bcxed utilitiesu\\u2081for each \\u2019 \\u2208 U.\\nNow, consider two items with the property that, conditioned on both being considered, the \\u02bcfrste is more likely to be chosen than the second. If we observe that the second item is chosen more frequently than the \\u02bcfrste, then it must be the case that the second item is considered more often. That is, if we know the utilities of items, then we can use \\uf0d7ips in top-\\u03ba ranking rates to identify which items are considered more frequently than others.\\nSuppose \\u03ba = PrPL+C(R\\u2081\\u2264\\u03bcs)/PrPL+C(R\\u2082\\u2264\\u03bcs) \\u2264 1 for some \\u03bcs. By the above equalities, then, we have PrPL+C(R\\u2081\\\\u2082)\\n+PrPL+C(R\\u2081\\u2229\\u2082) = \\u03ba \\u00b7 PrPL+C(R\\u2082\\\\u2081) + \\u03ba \\u00b7 PrPL+C(R\\u2081\\u2229\\u2082)\\n\\u21d2PrPL+C(R\\u2081\\\\u2082) \\u2264\\u03ba \\u00b7 PrPL+C(R\\u2082\\\\u2081).\\nAlso, note that if we observe \\u2019 in a length-\\u03ba ranking, it must have been the case that \\u03c7\\u2081 = 1, and we have conditioned on \\u03c7 \\u2265 \\u03ba. Thus,\\nPrPL+C(R\\u2081\\u2264\\u03bcs) \\u2264 Pr(\\u03c7\\u2081 | \\u03c7 \\u2265 \\u03ba).\\nWe can use this to lower bound p\\u2081\n",
      "File number 547,\n",
      " {\n",
      " \"abstract\": \"We propose creating a custom Generative Pre-Trained Transformer (GPT) for developers to discuss and solve ethical issues in AI engineering, ensuring legal compliance and ethical perspectives.\",\n",
      " \"introduction\": \"Current AI software can raise ethical concerns, and legal requirements are often general and open to subversion. We aim to develop a tool to aid developers in addressing these issues.\",\n",
      " \"literature review\": \"Previous approaches like privacy chatbots and legal compliance APIs address limited aspects of AI ethics. We address a broad range of AI-related software ethical issues with a focus on underrepresented perspectives.\",\n",
      " \"methodology\": \"We will create a high-quality dataset and analyze biases in existing LLM. We will develop deterministic responses to critical questions and validate our custom tool with AI engineers.\",\n",
      " \"results\": \"Our tool will generate diverse ethical perspectives, assisting developers in creating AI solutions that comply with legal requirements and consider diverse ethical perspectives. A demonstration addresses the ethical concern of cyberbullying through AI-based solutions.\",\n",
      " \"conclusion\": \"Our custom conversational agent will aid developers in creating ethical AI solutions, improving legal compliance, and fostering ethical considerations in software development.\"\n",
      "}\n",
      "File number 548,\n",
      " {\n",
      " \"abstract\": \"This workshop report provides insights into the context of the creation and the underlying methodological considerations behind the authors' published game database. The database was collaboratively developed and lists digital games developed in Germany, Austria, and Switzerland up to the year 2000. In this report, in addition to our initial considerations and the various work steps involved in the realization, we also outline the data basis on which the database was built and tested, the goals of the data model, and the difficulties we faced during the process of creation. Subsequently, we classify the current status of the game database and provide an outlook on the further plans for the project.\",\n",
      " \"introduction\": \"With the growing interest from research and journalism in digital games, there is also an increasing awareness of their history. It quickly becomes apparent that the \\\"new\\\" medium - depending on the starting point - is not even 50 years old. However, something else also became apparent: A kind of US-American-Japanese master narrative quickly emerged, which is often uncritically repeated to this day. It tells the story of mostly male, white geniuses and innovators who, often against the spirit of the times, implemented their visions: from the young MIT students who repurposed a PDP-10 computer for the night to develop the game Spacewar! to the self-made millionaire and enfant terrible Nolan Bushnell, who triggered the first video game boom in the USA with the foundation of Atari.\",\n",
      " \"literature review\": \"In recent times, a shift in focus towards regional and national digital game history has become apparent - here, too, a similar trend can be observed in research and journalism. Graeme Kirkpatrick has dedicated himself to video game culture in the United Kingdom (Kirkpatrick 2015), Alexis Blanchet and Guillaume Montagnon researched the origins of the \\\"French Touch\\\" (Blanchet and Montagnon 2020), and Melanie Swalwell has dealt extensively with the origins of the so-called homebrew culture in Australia and New Zealand (Swalwell 2021). These first studies on a local history of digital games have impressively shown that these were not marginal notes of a primarily US-American-Japanese dynamic. Jaroslav \\u0160velch was able to demonstrate in his study on the early history of digital games in Czechoslovakia that even behind the Iron Curtain and against all odds, an independent culture of digital games emerged in the 1980s (\\u0160velch 2018).\",\n",
      " \"methodology\": \"These preliminary studies made us increasingly aware of the urgent need for this research. To give one example, if one wants to better understand the success of the Anno series today, a look at the Ultimate History of Video Games is of no help. In the US-American video game chronicles, the phenomenon \\\"economic simulation and construction game\\\" is rarely or never mentioned. Fortunately, we are currently witnessing many simultaneously emerging and interconnected initiatives to research the respective history of digital games throughout Europe as well as in South America, Asia, Africa and Oceania. In Germany and Austria, several projects are currently in the making, and in Switzerland, a large SNF-Sinergia project on the research of local video game history is being funded from 2023 to 2027, in which four universities and colleges as well as 20 researchers are involved, including three of the four authors of this workshop report.\",\n",
      " \"results\": \"All these projects face a similar problem at the beginning: One basically knows, often from own experience, about games from the investigated geographical area, but there are only a few, incomplete lists or databases of these games - and sometimes none at all. However, such datasets - even if they are only rudimentary - are a basic prerequisite for most historical research questions, even if they are not primarily quantitative studies: How many games were developed in total in which period of time and how has this number changed? Which systems were particularly popular where and when? Which genres were popular when and where? How many developers were normally involved at which times? When and where did transnational cooperations take place?\",\n",
      " \"conclusion\": \"This critical point does not apply to the collection work itself. After all, these are private initiatives that live from the unpaid collection and research work of individuals. They are a valuable resource and were very helpful for our work, but are not suitable as databases for research, also because they do not claim to be complete - but above all because they have not been scientifically critically reviewed.\"\n",
      "}\n",
      "File number 549,\n",
      " {\n",
      " \"abstract\": \"During deep sleep and under anesthesia spontaneous patterns of cortical activation frequently take the form of slow travelling waves. Here we show how data recorded from transgenic mice under anesthesia can be processed to analyze sources, sinks and patterns of flow. To make the best possible use of the data novel means of data processing are necessary. Therefore, we (1) give a an brief account on processes that play a role in generating slow waves and demonstrate (2) a novel approach to characterize its patterns in GCaMP recordings.\",\n",
      " \"introduction\": \"Slow waves represent an important yet highly variable neural phenomenon. Thus methods are required which allow for a systematic measurement of slow wave properties. Previous work that described different types of slow waves relied on manually crafted shape parameters such as the slope of the rising and falling edge of the slow waves in EEG or the spatial extent (local/widespread) of cortical coverage [4]. Some invasive neuroimaging studies have focused on patterns of the spread of activity. For example, the wavefront is measured using delay maps which indicate the temporal offset of an event for every pixel relative to its global onset [3].\",\n",
      " \"literature review\": \"Townsend and Gong [5] suggest an approach to characterize more diverse temporo-spatial properties of slow waves in GCaMP recordings by the kernel-based detection of specific patterns in the vector fields of Dense Optical Flow such as spirals or saddles.\",\n",
      " \"methodology\": \"We considered related methods for structural MRI [6], extend on the idea of an event related slow wave analysis [3] as well as the one of using Optical Flow [5] and suggest a technique that enables a novel perspective on slow waves as events with source- and sink-regions and specific patterns of directional flow: The combination of Helmholtz-Decomposition and Optical Flow for fluorescence microscopy. It allows to measure properties of events that can incorporate several oscillations even if they have a poor signal-to-noise ratio, occur simultaneously and overlap.\",\n",
      " \"results\": \"The approach can help in improving our understanding of neural processing in the brain during sleep and under anaesthesia. This paper is structured as follows. In section 2 we briefly explain the mechanisms that generate slow waves during sleep and anaesthesia. Section 3 introduces the approach of using Helmholtz-Decomposition and Optical flow with GCaMP data and the results achieved with it are presented in section 4. The last section summarizes and gives a critical discussion on the limitations of the results and the presented approach.\",\n",
      " \"conclusion\": \"We demonstrate a new method to characterize neocortical slow waves using fluorescence microscopy. It relies on the Helmholtz-Decomposition of Dense Optical Flow and captures the global dynamics of spread. It shows that different types of slow waves form clusters with respect to different features.\"\n",
      "}\n",
      "File number 551,\n",
      " {\n",
      " \"abstract\": \"Soft robots are increasingly utilized due to their softness and high degrees of freedom (DOFs). However, modular soft robots introduce difficulties in accurate control. Hence, we propose a novel and accurate biLSTM controller for modular soft robots that adapts to different module numbers. It controls the module configurations and works for robots with different module numbers. Our contribution is threefold: a data collection strategy, a biLSTM configuration controller, and validation on simulated and real pneumatic robots. Experiments show that our controller achieves low error configuration control tasks on robots with two, three, four, and six modules.\",\n",
      " \"introduction\": \"Soft robots have been widely applied in numerous areas due to their softness and high degrees of freedom (DOFs), and various soft robots have been designed for different applications. For example, concentric tube robots are leveraged in robot-assisted surgery, especially minimally invasive surgery (MIS), and cable-driven robots have been validated in cardiothoracic endoscopic surgery. Researchers include pneumatic robots in recovery devices and assistive robots due to their safety. Also, a soft six-legged untethered robot can walk with the actuation of fluid-driven actuators. Soft robots can also be employed as biorobots to mimic the behaviors of animals like fish, octopus, and elephant. Overall, the robot community has taken advantage of many categories of soft robots in various research topics.\\n\\nModular soft robots have shown unique capabilities compared with other soft robots. Multiple modules endow the robot system with reconfiguration and multiple choices of module numbers. In this case, they are flexible and can meet the requirements of different tasks. Compared to single-module robots, modular robots have more degrees of actuation (DOAs) and, therefore, more active DOFs. Modular robots can provide larger working spaces from the views of both kinematics and dynamics. Thanks to these properties, modular robots can achieve complex manipulations.\\n\\nMoreover, shape control can be implemented on modular soft robots. Most control targets of single modular robots are only end positions. Once the end positions are controlled, all the robot states, including the end orientations and robot shapes, are decided and depend on the end positions. But in many cases, the end positions and orientations of modular soft robots are independent. The robot can keep the end positions/orientations invariant and change the end orientations/positions. By controlling the robot shapes, modular robots can avoid obstacles and cross holes.\",\n",
      " \"literature_review\": \"Studies have been carried out on a modular pneumatic robot. Due to the high model complexity, data-driven approaches are leveraged to achieve path tracking, end pose control, and sophisticated manipulation tasks. A modular robot named Robostrich arm is applied to mimic the behavior of an ostrich and achieves the reaching task in a narrow space. However, the applied reinforcement learning methods are time-consuming.\\n\\nSome physical models for modular soft robots are proposed. Piecewise constant curvature (PCC) and the Cosserat approach are two of the most widely applied physical models in soft robots. Neural networks are employed for measurement, and PCC is employed for modular soft robot modeling. Kinematics models of modular soft robots are investigated, also based on PCC. The discrete Cosserat approach is utilized to simulate multisection soft manipulator dynamics.\",\n",
      " \"methodology\": \"We propose a novel and accurate biLSTM controller for modular soft robots. The biLSTM units share the same module number with the robot, even if the training and controlled robots have different module numbers. The label of module \\u202fi\\u202f can be denoted as\\n\\nn\\u1d65\\u1d61 =\\n2(i \\u2212 1)n\\u02c7\\u02c9\\u20441 \\u2212 1,\\n(1)\\nwhere \\nn\\u02c7\\u02c9=\\n number of modules in this robot.\\n\\nThe range of labels is [-1,1], and a large label represents that this module is away from the robot base.\\n\\nEach LSTM unit can be seen as\\nf = sigm(\\u03a3\\u1d63 \\u22c5 [\\u043d\\u2097\\u2081, x] + \\u0392\\u1d63),\\ni = sigm(\\u03a3\\u1d69 \\u22c5 [\\u043d\\u2097\\u2081, x] + \\u0392\\u1d69),\\nC = f \\u00d7 C\\u2081\\u2044\\u2081 + i \\u00d7 tanh(\\u03a3\\u1d63 \\u22c5 [\\u043d\\u2097\\u2081, x] + \\u0392\\u1d63),\\n(2)\\no = sigm(\\u03a3\\u1d6f \\u22c5 [\\u043d\\u2097\\u2081, x] + \\u0392\\u1d6f),\\nh = o \\u00d7 tanh(C),where f, i, C, o, and h denote the forget gate, input gate, cell state, output gate, and hidden state of this LSTM unit, respectively. \\u043d\\u2097\\u2081\\u2090\\u2081 and C\\u2081\\u2044\\u2081 are the hidden state and cell state provided by the other unit. x denotes the LSTM input, and W* and b* denote the weight and bias parameters of the corresponding states. \\u00d7 is the Hadamard product operator, and sigm is the sigmoid function.\\n\\nTo train a neural network as a robot controller, it is necessary to collect a dataset in simulation or the real world. Generally, the robot will be actuated randomly to generate this dataset, and such a dataset can cover the whole working space.\",\n",
      " \"results\": \"We collect 16000 samples with the traditional random method and our method mentioned in Section 2.2 on a four-module simulation robot. It is evident that our method covers a larger space than the traditional method. The simulation experiment code can be found at https://github.com/zixichen007115/23ZCd.\\n\\nBased on our dataset, we train three kinds of neural networks to estimate the actions and compare their accuracy. As shown in Figure 1-(C), the neural network input contains previous module states and actions, the module number label, and current states for training. The network output is the estimated actuation.\\n\\nWe then utilize biLSTM for six-module robot configuration control. Due to the modularity of this network, biLSTM trained with a four-module robot can also be leveraged on a six-module robot, but LSTM fails to transfer to a six-module robot because the LSTM unit input size is related to module number.\\n\\nIn real experiments, we first collect 15000 samples on a three-module robot following the data collection method proposed in Section 2.2 and train a biLSTM for control. The dataset is shown in Figure S5. The actuation variable estimation errors and variances of the first, second, and third modules are 3.12 \\u00b1 4.85%, 3.23 \\u00b1 5.23%, and 3.04 \\u00b1 4.38%.\\n\\nTwo tasks, \\u2018edge\\u2019 and \\u2018down,\\u2019 are designed. In \\u2018edge,\\u2019 the robot is controlled to reach the edge of the working space, and the actuation variables nearly reach the maximal actuation.\",\n",
      " \"conclusion\": \"This paper introduces a novel and accurate biLSTM configuration controller for modular soft robots with module number adaptability. Such a controller can control module configurations in robots with different module numbers. Simulation cable-driven robots and real pneumatic robots have been included in experiments to validate the proposed approaches, and we have proven that our controller can be leveraged even with the increase or decrease of module number. This is the first paper that gets inspiration from the physical structure of modular robots and utilizes bidirectional LSTM for module number adaptability. Future work may include a planning method that bridges the task and configuration spaces and the integration of an online controller.\"\n",
      "}\n",
      "File number 552,\n",
      " {\n",
      " \"abstract\": \"We present the Radiation Oncology NLP Database (ROND), the first dedicated Natural Language Processing (NLP) dataset for radiation oncology, an important medical specialty that has received limited attention from the NLP community in the past. With the advent of Artificial General Intelligence (AGI), there is an increasing need for specialized datasets and benchmarks to facilitate research and development. ROND is specifically designed to address this gap in the domain of radiation oncology, a field that offers many opportunities for NLP exploration. It encompasses various NLP tasks including Logic Reasoning, Text Classification, Named Entity Recognition (NER), Question Answering (QA), Text Summarization, and Patient-Clinician Conversations, each with a distinct focus on radiation oncology concepts and application cases. In addition, we have developed an instruction-tuning dataset consisting of over 20k instruction pairs (based on ROND) and trained a large language model, CancerChat. This serves to demonstrate the potential of instruction-tuning large language models within a highly-specialized medical domain. The evaluation results in this study could serve as baseline results for future research. ROND aims to stimulate advancements in radiation oncology and clinical NLP by offering a platform for testing and improving algorithms and models in a domain-specific context. The ROND dataset is a joint effort of multiple U.S. health institutions. The data is available at https://github.com/zl-liu/Radiation-Oncology-NLP-Database.\",\n",
      " \"introduction\": \"Radiation oncology is a critical medical specialty that employs high-energy radiation to treat and manage cancer and other diseases (Bernier et al., 2004; Unkelbach et al., 2018). Indeed, like many medical domains, there is much potential to integrate natural language processing (NLP) into radiotherapy research and practice (Bitterman et al., 2021; Rezayi et al., 2022). However, there is limited development and evaluation of NLP models in this domain due to the lack of dedicated datasets (Rezayi et al., 2022). In response to this need, we present the Radiation Oncology NLP Database (ROND).\\nROND is the world\\u2019s first NLP dataset specifically created for radiation oncology. It aims to provide a comprehensive platform for researchers to develop, test, and improve NLP models and methods within this domain. This dataset covers a wide spectrum of NLP tasks, including Logic Reasoning, Clinical Text Classification, Named Entity Recognition (NER), Question Answering (QA), and Text Summarization. Each of these tasks is centered around distinct aspects of radiation oncology, offering researchers a rich and varied dataset for exploration and model training. In addition, ROND contains a Patient-Clinician conversation dataset, which provides valuable insights into patient interactions, symptom descriptions, and treatment discussions, enhancing our understanding and modeling of complex medical dialogues. Figure 1 presents an overview of ROND.\\nThe unique structure of ROND facilitates the development of models capable of reasoning logically about complex radiation oncology concepts, classifying domain-specific text data, recognizing and categorizing specialized entities, accurately answering radiation oncology-related questions, and summarizing lengthy documents and research papers in the field. We aim to establish a benchmark for future studies that stimulates innovation in radiation oncology research and ultimately improves patient care through the power of NLP.\\nWe believe this database is of particular importance in the age of Artificial General Intelligence (AGI) (Bubeck et al., 2023; Zhao et al., 2023; Liu et al., 2023a). Successful large language models (LLM) such as ChatGPT, GPT-4, LLAMA (Touvron et al., 2023) and PaLM (Chowdhery et al., 2022) are trained on vast amounts of public domain data. Some LLMs such as Med-Palm 2 (Singhal et al., 2022) are trained on both public biomedical data sources and private hospital (e.g., through the Google-Mayo Clinic partnership) data, and consequently are highly capable of processing medical text (Singhal et al., 2022). However, there is no existing dataset that specifically supports NLP in radiation oncology. The ROND dataset complements recent LLM advancements and offers a platform to better integrate LLMs into healthcare.\",\n",
      " \"literature review\": \"The Radiation Oncology NLP Database (ROND) is the first NLP dataset specifically created for radiation oncology. It aims to provide a comprehensive platform for researchers to develop, test, and improve NLP models and methods within this domain. This dataset covers a wide spectrum of NLP tasks, including Logic Reasoning, Clinical Text Classification, Named Entity Recognition (NER), Question Answering (QA), and Text Summarization. Each of these tasks is centered around distinct aspects of radiation oncology, offering researchers a rich and varied dataset for exploration and model training. In addition, ROND contains a Patient-Clinician conversation dataset, which provides valuable insights into patient interactions, symptom descriptions, and treatment discussions, enhancing our understanding and modeling of complex medical dialogues.\",\n",
      " \"methodology\": \"We present the Radiation Oncology NLP Database (ROND), the first dedicated Natural Language Processing (NLP) dataset for radiation oncology, an important medical specialty that has received limited attention from the NLP community in the past. With the advent of Artificial General Intelligence (AGI), there is an increasing need for specialized datasets and benchmarks to facilitate research and development. ROND is specifically designed to address this gap in the domain of radiation oncology, a field that offers many opportunities for NLP exploration. It encompasses various NLP tasks including Logic Reasoning, Text Classification, Named Entity Recognition (NER), Question Answering (QA), Text Summarization, and Patient-Clinician Conversations, each with a distinct focus on radiation oncology concepts and application cases. In addition, we have developed an instruction-tuning dataset consisting of over 20k instruction pairs (based on ROND) and trained a large language model, CancerChat. This serves to demonstrate the potential of instruction-tuning large language models within a highly-specialized medical domain. The evaluation results in this study could serve as baseline results for future research. ROND aims to stimulate advancements in radiation oncology and clinical NLP by offering a platform for testing and improving algorithms and models in a domain-specific context. The ROND dataset is a joint effort of multiple U.S. health institutions. The data is available at https://github.com/zl-liu/Radiation-Oncology-NLP-Database.\",\n",
      " \"results\": \"The Logic Reasoning subset of the Radiation Oncology NLP Database (ROND) presents questions designed to assess the logical reasoning capabilities of NLP models within the context of radiation oncology. The questions are geared towards the understanding of fundamental concepts and principles in radiation oncology, such as the properties of radioactive elements, atomic structure, electron orbits, X-ray emission, penumbra effects, and interaction of different particles with matter. We manually created and annotated 100 logic reasoning questions for this dataset.\\nEach question in this subset is structured as a yes/no question, designed to elicit a binary response. The questions range from basic atomic structure, such as \\\"Does an atom consist of a positively charged nucleus surrounded by a cloud of negatively charged electrons?\\\" to more specific queries about X-ray production and penumbra effect, such as \\\"In X-ray production, does the efficiency of x-ray production depend on the size of the target?\\\" or \\\"Is physical penumbra influenced by geometric penumbra, beam energy, and the lateral transport of electrons in the tissues?\\\"\\nThis dataset provides an avenue to evaluate the ability of NLP models to apply logical reasoning within the domain-specific context of radiation oncology, emphasizing both the understanding of fundamental radiation oncology concepts and the ability to apply this knowledge to specific scenarios.\",\n",
      " \"conclusion\": \"We present the Radiation Oncology NLP Database (ROND), the first dedicated Natural Language Processing (NLP) dataset for radiation oncology, an important medical specialty that has received limited attention from the NLP community in the past. With the advent of Artificial General Intelligence (AGI), there is an increasing need for specialized datasets and benchmarks to facilitate research and development. ROND is specifically designed to address this gap in the domain of radiation oncology, a field that offers many opportunities for NLP exploration. It encompasses various NLP tasks including Logic Reasoning, Text Classification, Named Entity Recognition (NER), Question Answering (QA), Text Summarization, and Patient-Clinician Conversations, each with a distinct focus on radiation oncology concepts and application cases. In addition, we have developed an instruction-tuning dataset consisting of over 20k instruction pairs (based on ROND) and trained a large language model, CancerChat. This serves to demonstrate the potential of instruction-tuning large language models within a highly-specialized medical domain. The evaluation results in this study could serve as baseline results for future research. ROND aims to stimulate advancements in radiation oncology and clinical NLP by offering a platform for testing and improving algorithms and models in a domain-specific context. The ROND dataset is a joint effort of multiple U.S. health institutions. The data is available at https://github.com/zl-liu/Radiation-Oncology-NLP-Database.\"\n",
      "}\n",
      "File number 554,\n",
      " {\n",
      " \"abstract\": \"Communication acts as a powerful tool in harmonizing the behaviors of multiple agents. However, current methods primarily emphasize broadcast communication, lacking practicality, and leading to information redundancy. This information overload negatively impacts communication efficiency. Meanwhile, commonly used methods rely on fundamental mechanisms to integrate observed and obtained information, hindering the learning process. The Targeted and Trusted Multi-Agent Communication (T2MAC) approach resolves these issues by integrating selective engagement, evidence-driven integration, and effective message composition. Using T2MAC, agents craft personalized messages, identify opportune communication windows, and engage with dependable partners, enhancing communication efficiency. The messages obtained are integrated at the evidence level, leveraging available perspectives collectively. Extensive experiments on cooperative multi-agent tasks demonstrate T2MAC's superiority in cooperative performance and communication efficiency compared to state-of-the-art techniques. Additionally, T2MAC exhibits impressive generalization capabilities.\",\n",
      " \"introduction\": \"Reinforcement Learning (RL) has achieved notable advancements in complex real-world scenarios, including Game AI, Robotics, and Autonomous Driving. Conversely, cooperative multi-agent settings present distinct challenges due to the partial observability faced by agents, restricted to their local observations without a comprehensive view of the environment. Moreover, Multi-Agent Reinforcement Learning (MARL) grapples with the non-stationarity of the environment, introducing further complexities during learning. Multi-agent communication offers a compelling solution, allowing agents to extract deeper insights from collective perspectives, facilitating stable learning and harmonized actions. Past methods in this domain predominantly focused on message content and timing but fell short in addressing broadcast communication inefficiencies and the treatment of information integration as a black box. Addressing these shortcomings, we propose the T2MAC framework.\",\n",
      " \"literature review\": \"Several key areas of research related to MARL and multi-agent communication have been explored in the literature:\\n\\n- Deciding What to Communicate: Prior studies have delved into the formulation of dynamic and continuous messages, enabling real-time adaptation to environmental changes. Researchers have optimized message learning processes and tailored messages for specific agents.\\n\\n- Deciding When and With Whom to Communicate: The timing and partner selection for communication have garnered significant attention. Novel strategies like weight-based schedulers prioritize agents with crucial observations, while other approaches harness methods such as causal inference, graph-attention, and Shapley message value to identify ideal communication recipients.\\n\\n- Incorporating Messages for Cooperative Decision-Making: Integrating incoming messages into decision-making processes has also been a focus of research. Prominent approaches employ representation learning paradigms to discern message significance, enabling selective assimilation. However, existing methodologies have yet to simultaneously address targeted and trusted communication.\",\n",
      " \"methodology\": \"At the core of our approach is the T2MAC framework, characterized by four distinctive aspects:\\n\\n1. Policy:\\n   - T2MAC's policy is modeled as a Dirichlet distribution, facilitating the integration of evidence from diverse sources for informed and reliable decisions.\\n   - The evidence encoder serves a dual purpose: extracting evidence for its decisions and crafting tailored messages for teammates, aiding their decision-making.\\n\\n2. Selective Engagement:\\n   - To ensure efficient communication, T2MAC pinpoints optimal moments and counterparts for communication, ensuring the dissemination of only the most pertinent and reliable data.\\n   - This is achieved through an ablative decision-making analysis to quantify the strength and relevance of each communication link between agents.\\n   - A communication selector network is developed to enable agents to determine the right moments and partners for communication.\\n\\n3. Evidence-Driven Integration:\\n   - T2MAC incorporates the theory of evidence into multi-agent communication.\\n   - Evidence refers to metrics extracted from observations that support decision-making processes.\\n   - To capture uncertainties inherent in decision-making, the Dirichlet distribution and Subjective Logic (SL) are employed to deduce the concentration parameters and belief and uncertainty masses associated with each action.\\n   - Evidence collected by different agents is integrated using Dempster-Shafer theory of evidence, enabling the combination of evidence from multiple sources to derive a comprehensive belief and uncertainty.\",\n",
      " \"results\": \"1. Performance:\\n   - T2MAC exhibited superior performance across various environments, including Hallway, MPE, and SMAC, consistently outperforming state-of-the-art communication methods.\\n   - T2MAC demonstrated resilience under challenging conditions, maintaining strong performance even in complex scenarios with numerous agents and long Markov chains.\\n   - Non-communication baselines consistently underperformed, highlighting the importance of proficient communication in these contexts.\\n\\n2. Efficiency:\\n   - T2MAC achieved significant communication efficiency gains compared to other methods, delivering substantial performance improvements while minimizing communication frequency.\\n   - The calculated communication efficiency metric underscores T2MAC's adeptness in managing communication dynamics, optimizing when to communicate, with whom, and how to balance performance and efficiency.\\n\\n3. Generality:\\n   - T2MAC demonstrated strong generalization capabilities across a wide range of established MARL baselines, including QMIX, DOP, and MAPPO.\\n   - It consistently achieved superior performance across these baselines, further highlighting its broad applicability and effectiveness in MARL.\",\n",
      " \"conclusion\": \"T2MAC effectively addresses the challenges inherent in multi-agent communication. Traditional approaches rely on broadcast communication and treat information fusion as a black box, resulting in diminished communication efficiency. To overcome these limitations, T2MAC empowers agents with the capability to compose messages tailored for distinct agents, strategically selecting communication timings, and relying on trusted partners. Moreover, it integrates incoming messages efficiently, facilitating trusted decision-making. Rooted in solid theoretical principles, T2MAC demonstrates its effectiveness through comprehensive experiments across multiple benchmarks, showcasing its superiority in efficiency, adaptability, and overall performance.\"\n",
      "}\n",
      "File number 555,\n",
      " {\n",
      " \"abstract\": \"To explore the chemical space of all small molecules, a common approach is to compress the dimension of the system to facilitate downstream machine learning tasks. Towards this end, the paper presents a data-driven approach for clustering potential energy landscapes of molecular structures by applying Network Embedding techniques, to obtain latent variables defined through the embedding function. It also incorporates an entropy-sensitive adaptive scheme for hierarchical sampling of the energy landscape, based on Metadynamics and Transition Path Theory. By taking into account the kinetic information implied by a system\\u2019s energy landscape, it interprets dynamical node-node relationships in reduced dimensions. Demonstrations of the method are presented using Lennard-Jones (LJ) clusters and a human DNA sequence.\",\n",
      " \"introduction\": \"The motivation is to understand the chemical space, specifically the number of organic molecules that can be formed and their potential for synthesizability. This knowledge has implications for drug development and disease treatment. The paper focuses on analyzing energy landscapes, which involve local minima, saddle points, entropic plateaus and deep energy wells. Existing methods for energy landscape analysis focus on identifying local minima and transition states between them. The proposed research is to apply Network Embedding techniques in combination with Metadynamics and TPT to produce adaptive embeddings that hierarchically convey information about the system\\u2019s behavior at different scales.\",\n",
      " \"literature_review\": \"The paper provides a review of Network Embedding techniques, such as DeepWalk, Node2vec, and their use in reducing the dimensionality of networks. It also discusses Metadynamics, which is used to aid in the exploration of energy landscapes, Transition Path Theory (TPT), which studies statistical properties of reactive trajectories, and Diffusion Wavelet algorithm for efficient approximation of embeddings. The paper proposes a modification of the Diffusion Wavelet algorithm that introduces parameters and optimizes them to minimize cross-entropy loss.\",\n",
      " \"methodology\": \"The paper proposes a hierarchical method for clustering energy landscapes and identifying latent variables of molecular structures. It starts by constructing a network representation of the energy landscape, where nodes represent local minima and edges represent energy barriers or transition probabilities. The method incorporates adaptive adjustments to the network structure and edge weights based on Metadynamics and TPT, which allows for hierarchical sampling and focusing on particular areas of the energy landscape. The embeddings are produced using a modified version of the Diffusion Wavelet algorithm, which is more efficient and scalable for large networks.\",\n",
      " \"results\": \"The paper demonstrates the effectiveness of the proposed method through experiments on Lennard-Jones (LJ) clusters and a human telomere sequence. It shows that the method can produce hierarchical embeddings that provide insights into the dynamics of the system at different scales. The embeddings can be used to identify transition paths and potential energy barriers. The method also allows for the incorporation of entropic effects and the analysis of systems with complex energy landscapes.\",\n",
      " \"conclusion\": \"The paper introduces a data-driven approach for clustering energy landscapes of molecular structures and identifying latent variables of molecular structures. The approach is based on Network Embedding techniques, Metadynamics and TPT. One major highlight is that the resulting embeddings can be used to interpret dynamical node-node relationships in reduced dimensions that are consistent with chemical kinetics and are thus more likely to be aligned with synthesizability. The framework can be used for applications in various fields such as drug discovery.\"\n",
      "}\n",
      "File number 557,\n",
      " {\n",
      " \"abstract\": \"Recent methods for implicitly representing signals like images, scenes, or geometries using coordinate-based neural network architectures often do not leverage the choice of activation functions, or do so only to a limited extent. In this paper, we introduce the Hyperbolic Oscillation function (HOSC), a novel activation function with a controllable sharpness parameter. Unlike any previous activations, HOSC has been specifically designed to better capture sudden changes in the input signal, and hence sharp or acute features of the underlying data, as well as smooth low-frequency transitions. Due to its simplicity and modularity, HOSC offers a plug-and-play functionality that can be easily incorporated into any existing method employing a neural network as a way of implicitly representing a signal. We benchmark HOSC against other popular activations in an array of general tasks, empirically showing an improvement in the quality of obtained representations, provide the mathematical motivation behind the efficacy of HOSC, and discuss its limitations.\",\n",
      " \"introduction\": \"An increasingly common scenario in learning visual data representations is approximating a structured signal s: Rk \\u2192 Rm via a coordinate-based neural network f\\u03f1 parametrized by a set of parameters \\u03f1 \\u2208 Rp. These representations, known as implicit neural representations (INRs), are fully differentiable and offer numerous advantages over traditional counterparts such as meshes or pixel grids in optimization tasks, often requiring significantly less memory.\",\n",
      " \"literature_review\": \"Three primary strategies to approach this problem while remaining in the INRs framework have been developed:\\n\\n\\u2022 Hybrid representations. Methods like ACORN (Martel et al., 2021), InstantNGP (M\\u00fcller et al., 2022) and TensoRF (Chen et al., 2022) use neural networks to achieve highly detailed representations of complex signals, such as gigapixel images and radiance fields. However, they also rely on traditional data structures, and hence require storing some sort of raw data. This notably enlarges their memory footprint compared to just storing the parameters of an MLP, and results in them not being fully differentiable.\\n\\n\\u2022 Positional encoding. Fourier Feature Networks (FFNs) (Tancik et al., 2020) employ positional encoding, which has been shown to accelerate the learning of higher-frequency features. Such encodings, if sampled densely, become extremely memory inefficient, and therefore require sampling a predefined distribution. This introduces more stochasticity to the model, as well as the need to tune the distribution\\u2019s parameters manually.\\n\\n\\u2022 Periodic activations. Sinusoidal Representation Networks (SIRENs) proposed by Sitzmann et al. (2020) are multi-layer perceptrons (MLPs) that utilize sin(x) instead of ReLU as their activation function. Consequently, they remain fully differentiable and offer a compact representation of the signal. While SIRENs demonstrated a significant improvement over ReLU, they struggle to capture high-frequency details in problems like shape representations, and are not-well suited for methods such as (Mildenhall et al., 2020).\",\n",
      " \"methodology\": \"In this paper, we introduce a new periodic parametric activation function \\u2014 the Hyperbolic Oscillation activation function (HOSC), defined as HOSC(x; \\u03b2) = tanh(\\u03b2 sin x). Here, \\u03b2 > 0 is a controllable sharpness parameter, enabling HOSC to seamlessly transition between a smooth sine-like wave and a square signal. Similarly to SIREN, an MLP running the HOSC activation function is fully differentiable and inexpensive memorywise. However, the HOSC\\u2019s sharpness parameter \\u03b2 allows it to much more accurately capture sudden or sharp jumps, and hence preserve high-frequency details of the signal. Moreover, since HOSC is differentiable with respect to \\u03b2, the sharpness can be adjusted automatically alongside the reset of the parameters, a method to which we refer as Adaptive HOSC or AdaHOSC.\",\n",
      " \"results\": \"Our extensive empirical studies show that HOSC consistently outperforms ReLU and SIREN across an array of benchmarking tasks. These tasks encompass fitting random signals, images of random square patches, photos, gigapixel images, and 2D & 3D SDF. In summary, HOSC provides an easy-to-implement method allowing simple MLPs to achieve high level of detail in signal encoding tasks without loosing differentiability or increasing memory footprint, and it does this without the need for positional encoding.\",\n",
      " \"conclusion\": \"In this paper, we have introduced the Hyperbolic Oscillation activation function HOSC(x; \\u03b2) = tanh(\\u03b2 sin x), a new periodic parametric activation that has been designed to be particularly effective in preserving sharp features in INRs. Additionally, in Section 4, we presented experimental results that evaluate the performance of the HOSC function in comparison to existing approaches.\\n\\nOur findings revealed that an MLP employing the HOSC activation with a suitably chosen or automatically-optimized sharpness parameter \\u03b2 consistently outperforms identical structure MLPs using ReLU and SIREN activations, and achieve the same level of accuracy in neural signal encoding problems as Fourier Feature Networks. HOSC thus offers a simple, fully differentiable and compact high-quality signals representation method with no need for hyperparameter tuning. However, we have identified scenarios where HOSC is clearly not the optimal choice, which we will explore in the following discussion.\"\n",
      "}\n",
      "File number 560,\n",
      " {\n",
      " \"abstract\": \"Visual fine-tuning has been receiving considerable attention in recent years owing to the advent of pre-trained vision models. However, the currently dominant approach, known as full fine-tuning, encounters knowledge forgetting, as it primarily focuses on fitting the downstream training set. To address this problem, this paper proposes a novel weight rollback-based fine-tuning method termed One Step Learning, One Step Review (OLOR). Unlike previous weight decay and regularization techniques that generally push the current weight away from 0 in the late stage of the optimization, OLOR incorporates a weight rollback term into the weight update term during the optimization. This ensures that the model weight gradually approaches the pre-trained weights as it learns the downstream task. In addition, a layer-wise penalty is introduced for weight rollback, utilizing penalty decay and diversified decay rates to adjust the rollback amounts of each layer, thereby accommodating diverse downstream tasks. To demonstrate the effectiveness of the proposed approach, comprehensive experiments have been conducted on a variety of tasks, encompassing image classification, object detection, semantic segmentation, and instance segmentation. The results on different pre-trained and downstream models indicate that OLOR achieves remarkable performance, confirming its general applicability.\",\n",
      " \"introduction\": \"The rapid advancement of deep learning technology has resulted in the establishment of numerous large-scale image datasets, propelling the development of promising pre-trained visual models. These pre-trained models have shown great potential for transfer learning and fine-tuning, allowing them to effectively solve related but distinct visual tasks. The fundamental fine-tuning methods are linear probing and full fine-tuning. However, linear probing often restricts the performance of the pre-trained backbone, while full fine-tuning typically leads to knowledge forgetting. Varied approaches have been explored to address these issues, including rehearsal methods, regularization methods, and parameter isolation methods. However, rehearsal methods tend to be inefficient due to the need for storing and managing large amounts of upstream task data. Regularization methods, such as weight decay or L2 penalty, have also demonstrated limited effectiveness in preventing knowledge forgetting, particularly when combined with adaptive optimizers. Parameter isolation methods, though effective, introduce additional parameters and often require specific training skills. To overcome these limitations, this paper introduces a novel fine-tuning method, called OLOR, which combines weight rollback and optimizer to prevent knowledge forgetting in neural networks. OLOR incorporates the weight rollback term into the weight update term at each step, allowing the model to gradually approach the pre-trained weights while learning the downstream task, resulting in the convergence of weights between the upstream and downstream models. Additionally, a layer-wise penalty is devised to employ penalty decay and diversified decay rate to adjust the weight rollback levels of layers for adapting varying downstream tasks.\",\n",
      " \"literature_review\": \"Rehearsal methods, based on the replay mechanism, involve retraining on a subset of stored upstream samples while learning new tasks. However, this approach is quite inefficient. EWC (Kirkpatrick et al. 2017) proposes a regularization-based fine-tuning method that uses the Fisher information matrix to determine the importance of weight parameters. This helps adjust the parameters between upstream and downstream tasks, reducing forgetting. L2-SP (Xuhong, Grandvalet, and Davoine 2018) uses an L2 penalty to restrict the updates of parameters, addressing knowledge forgetting during fine-tuning. However, it is not compatible with adaptive optimizers (Loshchilov and Hutter 2017; Guan 2023), which may produce the wrong regularization direction. Parameter isolation methods (Jia et al. 2022; Sohn et al. 2023) create new branches or modules for different network models and tasks for downstream tasks. However, it introduces extra new training parameters, requires certain training skills, and has lower generality than rehearsal methods.\",\n",
      " \"methodology\": \"The proposed method combines weight rollback and optimizers to adjust the range of parameter updates, thereby enhancing pre-trained model representations to improve downstream fine-tuning performance. We introduce weight rollback, which is a real-time regularization method that closely follows each weight update step. It aims to bring the current model weights closer to the pre-trained weights to perform knowledge reviewing. Subsequently, the discrepancy \\u0394d between \\u03f8pre and the pre-trained weight \\u03f80 is computed as:\\n\\n\\u0394d = \\u03f8pre \\u2212 \\u03f80.\\n\\nFinally, the weight update process incorporates \\u0394d, resulting in the adjusted model weights \\u03f8t:\\n\\n\\u03f8t = \\u03f8t\\u2081 \\u2212 \\u03f8tgt \\u2212 \\u03bb\\u0394d.\\n\\nIn addition, a layer-wise penalty mechanism is proposed. For deep learning neural networks, each layer can be conceptualized as a function that processes its input. Given a layer index i, this process can be described as follows:\\n\\nxi+1 = fi(x\\u22c5\\ni),\\n\\nwhere fi represents the ith layer. Let xu\\ni denotes the input of fi in upstream tasks with a distribution of qi(xu\\ni), and xd\\ni denotes the input of fi in downstream tasks with a distribution of pi(xd\\ni). Since qi(xu\\ni) are always different from pi(xd\\ni), we first unfreeze all layers to secure fi will have sufficient update to handle such gap better.\\n\\nTo accommodate the variability of target objectives, we propose adjusting the rate of penalty decay between layers by introducing a power exponent \\u03b3 to the weight rollback value. Mathematically, this adjustment can be expressed as:\\n\\n1 \\u2208 i\\nn \\u2192 (1 \\u2208 i\\nn)^\\u03b3.\",\n",
      " \"results\": \"The proposed OLOR achieves state-of-the-art performance on all datasets covering general classification, fine-grained classification, long-tailed classification, cross-domain classification, object detection, semantic segmentation, and instance segmentation. Notably, in in-distribution (ID) datasets, OLOR-Adam surpasses the previously leading L2-SP method by an impressive margin of 6.47% in accuracy. Moreover, when confronted with two more challenging out-of-distribution (OOD) datasets, OLOR-Adam achieves accuracy improvements of 2.57% and 7.38%, respectively, outperforming the optimal methods. Experiments on detection and segmentation tasks also demonstrate the effectiveness and versatility of OLOR. OLOR consistently outperforms the baseline by approximately 1% in all metrics across various tasks. Furthermore, in experiments using different pre-trained models, OLOR outperforms other leading methods across all pre-trained models, including Supervised, CLIP, and MAE. These results demonstrate the robustness and effectiveness of the proposed OLOR in various tasks and under different settings.\",\n",
      " \"conclusion\": \"The main contribution of this study is the development of OLOR, a novel fine-tuning method. OLOR combines weight rollback and layer-wise penalty to address knowledge forgetting and enhance fine-tuning performance. The proposed method achieves state-of-the-art results on extensive downstream tasks, encompassing general classification, fine-grained classification, long-tailed classification, cross-domain classification, object detection, semantic segmentation, and instance segmentation. Validation experiments and ablation analysis confirm the effectiveness of the method and the rationality of the parameters. The key innovations of OLOR include integrating a weight rollback term into the weight update term, devising a layer-wise penalty with penalty decay and diversified decay rate, and seamlessly combining with optimizers for efficient implementation.\"\n",
      "}\n",
      "File number 561,\n",
      " {\n",
      " \"abstract\": \"We aim to learn about the political interests and preferences of Members of Parliament (MPs) by mining their parliamentary activity to develop a personalized recommendation system for distributing documents.Given a stream of documents, it is able to select those most likely to be relevant for a particular MP. We propose using positive unlabeled learning since we only have information about relevant documents (interventions made by each MP in debates) but not about irrelevant documents. Furthermore, we developed a new algorithm of this type, which outperforms: a) assuming that the interventions of other MPs are irrelevant; b) another well-known positive unlabeled learning method; and c) an information retrieval-based approach that matches documents and legislators' representations.\",\n",
      " \"introduction\": \"Today's information society allows for easy access to vast amounts of information, sometimes even without actively searching for it. Advertisements, news, e-mails, etc. constantly bombard users. The challenge lies in separating the valuable from the useless information, a task often time-consuming. Content-based recommender systems aim to reduce this information overload by suggesting items (movies, songs, books, restaurants, etc.) to users based on their preferences and the characteristics of the items. Politicians also face this dilemma. Members of Parliament (MPs) must stay informed about matters related to their specific interests, such as health, education, or agriculture, while avoiding irrelevant information. Our goal is to develop a system that can automatically decide which documents should be sent to each MP based on both the document's content and the MP's political interests.\",\n",
      " \"literature_review\": \"Previous approaches to tackle this problem have used information retrieval-based methods, which rely on extracting term profiles from MPs' speeches. Our approach, in contrast, is based on machine learning techniques, specifically positive unlabeled learning (PUL), which assumes a set of positive data and a (usually larger) set of unlabeled data without negative examples. We propose a new PUL method based on modifying the K-means clustering algorithm. Related works have used a two-step strategy, where the first step tries to identify reliable negative data from the unlabeled set, and the second step uses a traditional supervised learning algorithm on the positive and reliable negative data.\",\n",
      " \"methodology\": \"We formalize the situation as follows: let MP = {MP1, . . . , MPn} be the set of MPs working in a parliament, which receives documents that should be distributed among them. Not all MPs should receive all documents to alleviate their workload, only those related to their interests, preferences, and role within the parliament. A system is required to perform this filtering process automatically. We propose to build such a system using PUL, using a modification of the K-means clustering algorithm to identify reliable negative documents from the interventions of other MPs for each MPi. For each MPi, we train a binary classifier from Di and Ni using Support Vector Machines (SVMs), which is considered the state-of-the-art technique for document classification. We also consider using a method to deal with the class imbalance problem.\",\n",
      " \"results\": \"Experiments using data from the Andalusian Parliament in Spain show that our approach outperforms the baseline (assuming all interventions of other MPs are irrelevant) and another well-known PUL method in terms of precision, recall, and F-measure. Our method also outperforms information retrieval-based approaches.\",\n",
      " \"conclusion\": \"Our proposed approach, pul-km, is a valuable tool for tackling the problem of building a content-based recommender system of documents in a parliamentary setting. It outperforms existing approaches, including information retrieval-based methods. Future research directions include exploring strategies for selective balancing, studying methods to select different thresholds for different classifiers, and investigating the use of feature selection methods.\"\n",
      "}\n",
      "File number 562,\n",
      " {\n",
      " \"abstract\": \"In this study, we examine the effectiveness of a large language model (LLM)-based support chatbot compared to a traditional keyword-based chatbot system using a randomized controlled trial. Our results indicate that the LLM-based chatbot significantly reduces escalation rates, whereby users seek assistance from a back-end engineer. The overall escalation rate decreased from 17.1% to 7.9% when using the LLM-based chatbot, representing a 53.8% reduction. Additionally, we compared two versions of the LLM-based chatbot, one powered by GPT3.5 and the other by GPT4, and found no significant difference in escalation rates between the two. However, the GPT3.5-based chatbot was more cost-efficient.\",\n",
      " \"introduction\": \"The study explores the impact of a large language model (LLM)-based support chatbot compared to a conventional chatbot system. Numerous studies have examined LLM tools in various settings, demonstrating their productivity-enhancing effects. However, field experiments applying LLM tools in realistic scenarios are limited. This study aims to evaluate the effectiveness of LLM-based tools in providing unmonitored support services for information retrieval.\",\n",
      " \"literature_review\": \"Prior research has investigated the impact of LLM-based tools on productivity in different settings, including lab tasks, observational studies, and field experiments. The studies generally indicate that LLM-based tools can enhance user productivity significantly. However, there is a lack of field experiments applying LLM tools in realistic settings.\",\n",
      " \"methodology\": \"The study conducted a randomized controlled trial comparing the performance of a traditional chatbot system with that of an LLM-based chatbot. The trial involved two waves, with the first wave comparing the LLM-based chatbot with the classical bot, and the second wave introducing a GPT3.5-supported version. The primary outcome of interest was the escalation rate, defined as the user's decision to escalate the inquiry to a back-end engineer.\",\n",
      " \"results\": \"The study found that the LLM-based chatbot significantly reduced the escalation rate compared to the classical bot. The overall escalation rate decreased from 17.1% to 7.9%, representing a 53.8% reduction. Additionally, there was no significant difference in escalation rates between the GPT3.5 and GPT4-based versions of the LLM-based chatbot, though the GPT3.5 version was more cost-efficient.\",\n",
      " \"conclusion\": \"The study provides evidence of the effectiveness of LLM-based chatbots in reducing escalation rates and improving the overall support service quality. The findings contribute to the growing literature on the productivity-enhancing effects of LLM-based tools.\"\n",
      "}\n",
      "File number 563,\n",
      " {\n",
      " \"abstract\": \"This study presents a model for understanding how customers\\u2019 satisfaction changes when they use an AR shopping application. It proposes that different levels of perceived experiential AR application features (informative, personalizing, and interactivity) affect the customer experience, which in turn influences customer satisfaction and purchase intention. The model also considers immersive experiences as a mediator between perceived AR application features and customer experience.\",\n",
      " \"introduction\": \"In an increasingly digital world, businesses are turning to AR technology to enhance the shopping experience and increase sales. This paper investigates the effects of AR technology on customers\\u2019 experience, satisfaction, and purchase intention in an online retail setting.\",\n",
      " \"literature_review\": \"Building on the stimulus-organism-response (S-O-R) paradigm and the information systems success model (ISS), the study reviews previous research on consumer experiences, immersion, and customer satisfaction. It draws insights from studies on informative, personalizing, and interactivity features of AR applications.\",\n",
      " \"methodology\": \"The study proposes a conceptual model with hypotheses linking the perceived levels of experiential AR application features, such as information, personalization, and interactivity, to customer experience, immersion, customer satisfaction, and purchase intention. Methods for measuring these constructs are discussed, including subjective data collection techniques and the use of existing scales.\",\n",
      " \"results\": \"The paper outlines expected results based on the proposed hypotheses. It anticipates positive relationships between perceived AR application features and customer experience, immersion, customer satisfaction, and purchase intention. Furthermore, it suggests that immersion mediates the relationship between perceived AR application features and customer experience.\",\n",
      " \"conclusion\": \"This study provides a deeper understanding of the role of AR technology in shaping customers\\u2019 experiences and how these experiences affect customer satisfaction and purchase behavior. It offers insights for business owners and academics in leveraging AR technology to improve customer engagement and drive sales.\"\n",
      "}\n",
      "File number 564,\n",
      " {\n",
      " \"abstract\": \"This paper explores how research on Optimal Transport (OT) can be combined with Multi-Agent Reinforcement Learning (MARL). OT can help to distribute resources, align agent policies, and adjust to non-stationarity in MARL environments, especially where there are multiple agents. Using OT, MARL systems can optimize the distribution of resources, coordinate agent policies, and adapt to changing environments. The combined approach can potentially improve system ef\\u02bbciency, coordination, adaptability, and scalability in MARL.\",\n",
      " \"introduction\": \"Multi-Agent Reinforcement Learning (MARL) is a framework where various agents learn and adapt within shared environments. These interactions involve diverse objectives, making coordination, resource management, adaptability, and operational ef\\u02bbciency challenging. Optimal Transport (OT) offers a mathematical framework for comparing and transforming probability distributions effectively by finding the most cost-effective transport plan to move mass from one distribution to another. By merging OT with MARL, we can leverage OT's strengths to address the challenges in MARL, such as policy alignment, distributed resource management, non-stationarity, scalability, and energy ef\\u02bbciency.\",\n",
      " \"literature review\": \"Previous research on policy alignment in MARL has explored strategies that utilize shared reward structures, joint action learning, and communication protocols. For distributed resource management, approaches such as auction-based mechanisms and cooperative strategies have been proposed. To address non-stationarity, algorithms that adjust learning rates based on Wasserstein distance have been investigated. Scalability has been tackled with decentralized learning, hierarchical structures, and networked or graph-based approaches. Additionally, techniques to optimize computational and operational aspects of learning processes and energy-ef\\u02bbcient algorithms have been developed to improve MARL systems.\",\n",
      " \"methodology\": \"To address policy alignment in MARL through OT, we can utilize the Wasserstein distance to measure and minimize the divergence between agent strategies. For distributed resource management, OT can optimize resource allocation by minimizing the transportation cost between supply and demand distributions. Non-stationarity can be modeled as a transportation problem using OT, where an optimal plan aims to minimize the cost of adapting to environmental changes. Scalability can be achieved through decentralized and hierarchical computation approaches. Finally, energy ef\\u02bbciency can be enhanced by incorporating energy consumption as a weight into the OT framework.\",\n",
      " \"results\": \"Integrating OT with MARL can potentially lead to more ef\\u02bbcient multi-agent systems. OT can optimize policy alignment, distribute resources effectively, manage non-stationarity, handle large-scale systems, and improve energy ef\\u02bbciency. These enhancements can lead to more coherent and effective cooperative learning, improved coordination and adaptability, and reduced computational and energy costs.\",\n",
      " \"conclusion\": \"Combining OT principles with MARL can improve system ef\\u02bbciency, coordination, adaptability, and scalability. Challenges, such as computational complexity and scalability, require further research and development to address them. Future work can focus on re\\u02bbning the computational ef\\u02bbciency of OT in large-scale MARL systems, exploring real-world applications, and adopting the synergy between OT and MARL to address broader problems.\"\n",
      "}\n",
      "Error in Design Principles & Issues for Gaze and Pinch Interaction\n",
      "File number 565,\n",
      " ```python\n",
      "{\n",
      " \"abstract\": \"The research paper aims to understand the principles and issues associated with Gaze + Pinch interaction, a new type of interface that combines eye movements and pinch gestures for controlling virtual reality (VR) and augmented reality (AR) systems. The study draws on human-computer interaction research to identify five design principles and five design issues related to Gaze + Pinch interaction.\",\n",
      " \"introduction\": \"VR/AR systems have become more sophisticated, allowing for interaction through controllers, hand gestures, eye movements, and voice. Gaze + Pinch interaction is a recently introduced technique that combines gaze targeting with pinch gestures to select and manipulate objects in VR/AR environments. This interaction style offers unique benefits, including improved selection accuracy and reduced hand fatigue.\",\n",
      " \"literature review\": \"The research builds upon existing research in human-computer interaction, focusing on studies that explore gaze and touch interaction on large displays and early influential work on gaze-based interaction. Important precursory research by Sophie Stellmach et al. and Jayson Turner et al. is cited, along with influential entries by Zhai et al., Jacob, and Bolt.\",\n",
      " \"methodology\": \"The research is based on the author's experiences, notes, and thoughts, as well as knowledge of the human-computer interaction research field. The paper draws on a previous study conducted by the author and colleagues on Gaze + pinch interaction in virtual reality, presented at the 2017 Spatial User Interfaces symposium.\",\n",
      " \"results\": \"The paper presents five design principles and five design issues for Gaze + Pinch interaction. The design principles include division of labor between eyes and hands, minimalistic multimodal timing, flexible gesture support, infallible eyes, and compatibility with hand-based interfaces. The design issues include (un)learning, early- and late-triggers, control-display ratio, drag & drop sequences, and continuous eye-selection.\",\n",
      " \"conclusion\": \"Gaze + Pinch interaction is a novel interaction space with the potential to transform the way users interact with VR/AR systems. By understanding the principles and issues surrounding this interaction technique, designers can create more effective and intuitive user experiences.\"\n",
      "}\n",
      "```\n",
      "File number 566,\n",
      " {\n",
      " \"abstract\": \"Emotion recognition is important for human-robot interaction, but current methods often ignore the context of emotions. We introduce SCAM, a self context-aware emotion perception model that employs a two-dimensional emotion coordinate system for anchoring and re-labeling distinct emotions. It incorporates a distinctive information retention structure and contextual loss, resulting in significant improvements across audio, video, and multimodal modalities.\",\n",
      " \"introduction\": \"Human-robot interaction requires natural and intuitive communication, which includes the robot's ability to understand human emotions. Current emotion recognition models primarily focus on multimodal perception, but often overlook contextual information. This is problematic because emotions in real-life conversations often display a sense of continuity, meaning emotions do not undergo abrupt and dramatic shifts within a brief timeframe. As a result, when emotions cannot be ascertained, humans often depend on contextual information to make judgments. Hence, we propose self context-aware model (SCAM) that enables a robot to perform emotion recognition on the user while simultaneously considering the user\\u2019s preceding emotional context and integrating it with the robot\\u2019s recognition results from the preceding context. This enhances the overall accuracy and comprehensiveness of the robot\\u2019s emotion recognition abilities during human-robot interactions.\",\n",
      " \"literature review\": \"Emotion recognition is a complex process involving various perceptual dimensions and temporal aspects. Researchers have explored different modalities, such as visual, auditory, physical, and even EEG and skin conductance signals, for emotion recognition. Combining information from diverse modalities typically yields enhanced accuracy. Some studies emphasize the intricacy of emotion recognition when information conflicts arise between modalities. Additionally, researchers suggest that better results can be achieved by capturing contextual information within conversations. However, considering that during human-robot interaction, the robot may struggle to provide sufficient feedback, we propose a method to utilize self context.\",\n",
      " \"methodology\": \"SCAM consists of two main components: a multi-task network for each segment and a self context-aware structure for composition. The multi-task network relabels emotions and utilizes ResNet101 and Bi-LSTM to recognize emotion, valence, and arousal within a short time. The self context-aware structure incorporates contextual information propagation and context loss, combining the context information and predictions from preceding segments with the current input to predict the current emotion, valence, and arousal.\",\n",
      " \"results\": \"In the auditory modality, there has been a notable enhancement in accuracy, rising from 63.10% to 72.46%. Similarly, the visual modality has demonstrated improved accuracy, increasing from 77.03% to 80.82%. In the multimodal, accuracy has experienced an elevation from 77.48% to 78.93%.\",\n",
      " \"conclusion\": \"Our approach achieves significant improvements across all modalities, some of which have reached the state of the art. In future work, we will validate the reliability and usability of SCAM on robots through psychology experiments.\"\n",
      "}\n",
      "File number 568,\n",
      " {\n",
      " \"abstract\": \"Machine Unlearning emphasizes adaptability, personalization, privacy, and bias concerns and addresses the ability to remove or update specific data from machine learning models. Unlike traditional models, Unlearning Machine Learning (MUL) dynamically adjusts system knowledge based on shifts in user preferences and ethical considerations. The paper examines MUL's fundamentals, real-world uses, and challenges in ongoing research in responsible and user-focused artificial intelligence. It highlights the trade-off between personalization and privacy, encouraging contributions to meet practical demands for targeted data removal, and offers ways to advance the field.\",\n",
      " \"introduction\": \"The transformative power of Machine learning has revolutionized data processing and analysis, influencing our digital interactions through sophisticated algorithms and data-driven insights. Recommendation systems, a critical subset of ML, shape our online experiences with tailored recommendations. This paper explores the role of machine unlearning (MUL) in recommendation systems, addressing adaptability, personalization, privacy, and bias challenges. It critically reviews MUL's fundamentals, real-world applications, and complexities like algorithmic transparency, providing insights into its potential to transform recommendations, user trust, and future research.\",\n",
      " \"literature_review\": \"This section offers a thorough investigation of MUL's current state-of-the-art algorithms within recommendation systems, shedding light on several significant contributions. It presents a comprehensive overview of groundbreaking research, including the introduction of graph networks, optimization methods, and model-agnostic operators, highlighting the evolving landscape of MUL methodologies.\",\n",
      " \"methodology\": \"The literature review in this paper systematically analyzed relevant sources from esteemed academic databases. By employing comprehensive search strategies across reputable platforms, a representative selection of pertinent and recent studies on MUL within recommender systems was compiled. The research adhered to a rigorous methodology to identify and evaluate the most impactful contributions in the field.\",\n",
      " \"results\": \"The review of SOTA algorithms for MUL on recommendation systems revealed promising advancements in unlearning efficiency, accuracy, and eﬃciency. The combination of data partitioning, adaptive aggregation methods, and advanced update techniques showcased significant improvements in model utility and completeness. Researchers also introduced model-agnostic approaches for graph unlearning, ensuring the removal of elements without compromising knowledge integrity. Despite these advancements, challenges remain in addressing instance-level data removal limitations and the complexities of non-convex optimization problems, opening avenues for future exploration.\",\n",
      " \"conclusion\": \"The paper emphasizes how MUL can potentially revolutionize recommendation systems by critically evaluating its fundamentals, practical applications, and the complex issue of algorithmic transparency. It explores the delicate balance between tailoring recommendations to individual preferences and respecting user privacy, highlighting the importance of building user trust in these systems. Moreover, it suggests avenues for future research that prioritize responsible and user-centric artificial intelligence, significantly contributing to ongoing discussions about ethical practices in AI. Its unique exploration of MUL's transformative capabilities within recommendation systems oﬀers valuable insights into ethical considerations and advances the conversation on responsible AI development.\"\n",
      "}\n",
      "File number 569,\n",
      " {\n",
      " \"abstract\": \"Preference-based reinforcement learning (RL) provides a framework to train agents using human feedback through pairwise preferences over pairs of behaviors, enabling agents to learn desired behaviors when it is difficult to specify a numerical reward function. While this paradigm leverages human feedback, it currently treats the feedback as given by a single human user. Meanwhile, incorporating preference feedback from crowds (i.e. ensembles of users) in a robust manner remains a challenge, and the problem of training RL agents using feedback from multiple human users remains understudied.\",\n",
      " \"introduction\": \"Reinforcement learning (RL) from human feedback is a promising approach for learning intelligent behaviors in the absence of a known numerical reward signal. Such methods have shown success in domains such as Atari games, robotics, and large language models (LLMs). In this work, we consider human feedback that is crowdsourced, i.e., where (1) data is labeled by multiple human users (e.g. \\u2265 2 labels for each data point), and (2) these labels are aggregated to form an ensemble label to be used for downstream analysis.\",\n",
      " \"literature_review\": \"Early works that explicitly model human feedback from crowds have focused on imitation learning, where there are multiple demonstrations (assumed to be from multiple human users) of unknown expertise and quality, and the goal is to learn a policy reflecting expert behavior. Recent work that has explicitly modeled crowdsourced data for imitation learning has shown significant performance gains from simultaneously modeling human demonstrations and estimating the demonstrators\\u2019 expertise levels. While recent work by [Zhang and Kashima, 2023] used crowdsourcing methods to learn a reward function using preference-based reward learning, it is limited to the offline RL case, where there is a fixed dataset of preference pairs, and the RL algorithm does not have online access to the environment.\",\n",
      " \"methodology\": \"We next explain our methodology for simulating diverse crowds of human users with different rationality levels. Afterwards, we outline our Crowd-PrefRL approach to performing RL with crowdsourced feedback (Algorithm 1): first, we explain how to estimate preference labels from a crowd and then, we show how to integrate these crowd-aggregated preference labels into a framework for preference-based RL.\",\n",
      " \"results\": \"We see a strong positive correlation between the error rates of users in the crowd and the performance of SML compared to MAJ. In particular, as the user error increases (indicating a more diverse crowd), the better the SML labels perform compared to the MAJ labels. This indicates that the SML is effectively filtering out inconsistent preference feedback from users who have higher error rates and aggregating crowd decisions more consistently compared to MAJ.\",\n",
      " \"conclusion\": \"This work demonstrates the viability of learning reward functions from preference feedback provided by crowds of unknown expertise and reliability. We believe that this is the first work that focuses on using preference feedback from crowds to learn a crowdsourced reward function for preference-based RL in the online RL setting.\"\n",
      "}\n",
      "File number 570,\n",
      " {\n",
      " \"abstract\": \"This study introduces RELIANCE, a unique ensemble learning system tailored for reliable information and fake news credibility evaluation. Composed of five diverse base models, including Support Vector Machine (SVM), naive Bayes, logistic regression, random forest, and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs an innovative approach to fuse their strengths, harnessing the collective intelligence of the ensemble for enhanced accuracy. Experiments showcase RELIANCE's superiority over individual models, demonstrating its efficacy in distinguishing credible from non-credible information sources. RELIANCE surpasses baseline models in information and news credibility assessment, establishing itself as an effective solution for evaluating the reliability of information sources.\",\n",
      " \"introduction\": \"In the era of information proliferation, discerning the credibility of news content poses an ever-growing challenge. This paper introduces RELIANCE, a pioneering ensemble learning system designed for robust information and fake news credibility evaluation. Comprising five diverse base models, including Support Vector Machine (SVM), na\\u00efve Bayes, logistic regression, random forest, and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs an innovative approach to integrate their strengths, harnessing the collective intelligence of the ensemble for enhanced accuracy. Experiments demonstrate the superiority of RELIANCE over individual models, indicating its efficacy in distinguishing between credible and non-credible information sources. RELIANCE, also surpasses baseline models in information and news credibility assessment, establishing itself as an effective solution for evaluating the reliability of information sources.\",\n",
      " \"literature_review\": \"The rapid dissemination of information through digital platforms has directed in an era where the credibility of news content is frequently challenged. In response to the proliferation of misinformation and fake news, researchers have proposed different approaches to evaluate the credibility of news documents. Throughout the research history of our work, this field has been recognized by various names, including rumor detection, fake news detection, and reality detection. Perusing the literature, one can categorize the previous studies into three categories: text-based methods, context-based methods, and hybrid methods. Table 1 provides an analytic perspective on related works in information and news credibility evaluation in a tabular form.\",\n",
      " \"methodology\": \"The objective of our investigation was to improve the automated evaluation of information and news credibility by employing ensemble learning. Ensemble learning, combining various methods, enhances predictive performance beyond that of individual methods. Consequently, we introduced five distinct base models, each operating at different processing levels, for integration into the ensemble learning model. Namely Support Vector Machine (SVM)-based, na\\u00efve Bayes-based, logistic regression-based, random forest-based, and Bidirectional Long Short-Term Memory Networks (BiLSTMs)-based models. To this end, we proposed RELIANCE (Reliable Ensemble Learning for Information and News Credibility Evaluation), that performs credibility evaluation in three phases; Phase 1 focuses on preprocessing the input news text documents to prepare them for the main process. Phase 2 performs feature engineering through embedding the input news text documents. Phase 3 is devoted to evaluate the credibility of news text documents.\",\n",
      " \"results\": \"To perform news credibility evaluation, we proposed RELIANCE as an ensemble learning model that combines five distinct classifiers as the base models including SVM-based, na\\u00efve Bayes-based, LR-based, random forest-based, and BiLSTMs-based models. To perform feature engineering, we employed Doc2Vec with an embedding size equal to 1,200, and the minimum counts of word equal to 1, and conducted the training for 50 epochs. Afterwards, at first, the base models should perform credibility evaluation on the input text documents. Each of the base models adheres to specific settings for generating predictions. Table III encompasses the applied parameter settings in proposed BiLSTM model. Logistic Regression (LR): For the LR-based model, the maximum number of iterations for the solver (lbfgs) to converge is set at 1000. Moreover, the L1 regularization is also used to reduce model generalization error. SVM: For the SVM-based model, the chosen kernel is 'rbf' (Radial Basis Function). Random Forest: The Random Forest-based model is configured with the parameter n_estimators set to 100. Na\\u00efve Bayes: We used the Multinomial Naive Bayes algorithm, which is commonly used for text classification.\",\n",
      " \"conclusion\": \"In the current era of information overload, accurately assessing the credibility of news sources is crucial for informed decision-making and effective crisis management. To address this challenge, we propose RELIANCE (Reliable Ensemble Learning for Information and News Credibility Evaluation), an ensemble learning approach that combines the strengths of five individual models for news credibility evaluation. The base models include Support Vector Machines, na\\u00efve Bayes classifiers, logistic regression models, random forests, and Bidirectional Long Short-Term Memory Networks. These models are individually trained to extract relevant features from news documents and classify their credibility. To further enhance the overall accuracy, RELIANCE employs a multi-layer perceptron as a meta-model, which integrates the predictions of each base model (using stacking) and produces a more refined credibility assessment. Comparative experiments with baseline models demonstrate that RELIANCE significantly outperforms existing algorithms in evaluating the credibility of news documents. It provides a robust framework for identifying trustworthy news sources, offering real-world applications that empower users, journalists, and fact-checkers with a resilient tool against misinformation in the digital era.\"\n",
      "}\n",
      "File number 571,\n",
      " {\n",
      " \"abstract\": \"eXplainable AI has received signi\\ufb01cant attention in recent years. Machine learning models often operate as black boxes, lacking explainability and transparency while supporting decision-making processes. Local post-hoc explainability queries attempt to answer why individual inputs are classi\\ufb01ed in a certain way by a given model. While there has been important work on counterfactual explanations, less attention has been devoted to semifactual ones. In this paper, we focus on local post-hoc explainability queries within the semifactual \\u201ceven-if\\u201d thinking and their computational complexity among different classes of models, and show that both linear and tree-based models are strictly more interpretable than neural networks. After this, we introduce a preference-based framework that enables users to personalize explanations based on their preferences, both in the case of semifactuals and counterfactuals, enhancing interpretability and user-centricity. Finally, we explore the complexity of several interpretability problems in the proposed preference-based framework and provide algorithms for polynomial cases.\",\n",
      " \"introduction\": \"The extensive study of counterfactual \\u201cif only\\u201d thinking, exploring how things might have been different, has been a focal point for social and cognitive psychologists [Kahneman and Tversky, 1981; McCloy and Byrne, 2002]. Consider a negative event, such as taking a taxi and due to traf\\ufb01c arriving late to a party. By analyzing this situation, an individual (e.g. Alice) might engage in counterfactual thinking by imagining how things could have unfolded differently, such as, \\u201cif only Alice had not taken the taxi, she would not have arrived late at the party\\u201d. This type of counterfactual thinking, where an alternative scenario is imagined, is a common aspect of daily life. In such a case the counterfactual scenario negates both the event\\u2019s cause (antecedent) and its outcome, presenting a false cause and a false outcome that are temporarily considered as true (e.g., Alice took the taxi and arrived late). Counterfactual thinking forms the basis for crafting counterfactual explanations, which are crucial in automated decision-making processes. These explanations leverage imagined alternative scenarios, aiding users in understanding why certain outcomes occurred and how different situations might have in\\ufb01uenced automated decisions. Counterfactual explanations empowers users to grasp the rationale behind automated decisions, fostering transparency and user trust in these systems. Several de\\ufb01nitions of counterfactual explanations exist in the literature [Guidotti, 2022]. According to most of the literature, counterfactuals are de\\ufb01ned as the minimum changes to apply to a given instance to let the prediction of the model be different [Barcel\\u00f3 et al., 2020]. While signi\\ufb01cant attention in AI has been given to counterfactual explanations, there has been a limited focus on the equally important and related semifactual \\u201ceven if\\u201d explanations [Aryal and Keane, 2023; Kenny and Huang, 2023], though they have been investigated much more in cognitive sciences. While counterfactuals explain what changes to the input features of an AI system change the output decision, semifactuals show which input feature changes do not change a decision outcome. Considering the above-mentioned situation where Alice took the taxi and arrived late at the party, we might analyze \\u201ceven if\\u201d scenarios envisioning how things could have remained the same, such as, \\u201ceven if Alice had not taken a taxi, she would have still arrived late at the party\\u201d. Sharing the same underlying idea of counterfactuals, we de\\ufb01ne semifactuals as the maximum changes to be applied to a given instance while keeping the same prediction. Indeed, the larger the feature differences asserted in the semifactual, the better (more convincing) the explanation [Aryal and Keane, 2023]. This intuitively captures the desire of an agent to have more degree of freedom and favorable conditions (represented by features changed), while keeping the (positive) status assigned to it by the model. As an example, consider the following, inspired by a mortgage scenario.\\nExample 1. Consider the binary and linear classi\\ufb01cation model M : {0, 1}3 \\u2192 {0, 1} shown in Figure 1 where M is de\\ufb01ned as step(x \\u22c5 [2, 2, 0] + 1). The three features are:\\n\\n\\u2022 f1 =\\u201cuser\\u2019s employment contract is part-time\\u201d,\\n\\u2022 f2 =\\u201cuser applies for a mortgage whose duration is longer than 30 years\\u201d, and\\n\\u2022 f3 =\\u201cuser works on-site\\u201d.\",\n",
      " \"literature review\": \"However, as highlighted in the previous example, multiple semifactuals can exist for each given instance. In these situations, a user may prefer one semifactual to another, by expressing preferences over features so that the best semifactuals will be selected, as shown in the following example.\\nExample 2. Continuing with the previous example, suppose that the user x1 prefers semifactuals with f1 = 1 rather than those with f2 = 0, that is (s)he prefers to change feature f1 rather than f2 (irrespective of any other change). Thus, (s)he would prefer to still get a mortgage by changing the job to part-time (obtaining y2); if this cannot be accomplished, then (s)he prefers to get a mortgage by changing the duration to be less than or equal to 30 years (obtaining y1). Prioritized reasoning in AI, focusing on incorporating user preferences, represents a pivotal advancement in the \\ufb01eld, enhancing adaptability and user-centricity of AI systems. Traditional AI models rely on prede\\ufb01ned rules or optimization criteria to generate outcomes, often overlooking the nuanced nature of user-speci\\ufb01c preferences [Rossi et al., 2011; Santhanam et al., 2016]. Prioritized reasoning addresses this limitation by introducing a mechanism that allows users to express their preferences, thereby guiding AI systems to prefer speci\\ufb01c factors over others in the decision-making processes. One key aspect of prioritized reasoning is its applicability across diverse AI domains, spanning machine learning [Kapoor et al., 2012], natural language processing [Bakker et al., 2022], and recommendation systems [Zhu et al., 2022]. In machine learning, for instance, the ability to prioritize speci\\ufb01c features or outcomes based on user preferences signi\\ufb01cantly improves the relevance and usability of the resulting models. Within natural language processing, prioritized reasoning facilitates customized language generation aligned with individual preferences, catering to diverse communication styles. The impact of prioritized reasoning extends to recommendation systems, where user preferences play a crucial role in shaping the suggestions provided. Our work contributes to prioritized reasoning within explainable AI in the presence of user\\u2019s preference conditions related to features. These preferences are exploited to generate semifactual and counterfactual explanations that align most closely with the user-speci\\ufb01ed criteria. In particular, preferences are applied similarly to what has been proposed in the well-known Answer Set Optimization approach for Logic Programs with preferences [Brewka et al., 2003].\",\n",
      " \"methodology\": null,\n",
      " \"results\": null,\n",
      " \"conclusion\": \"In this paper, after exploring local post-hoc interpretability queries related to semifactuals and their computational complexity among three classes of models (FBDDs, perceptrons, and MLPs), we introduced a framework that enables users to personalize semifactual and counterfactual explanations based on preferences. Then, we investigated the complexity of the proposed framework and presented PTIME algorithms. Our approach to handle preferences over semifac-tual/counterfactual explanations is inspired by the Answer Set Optimization approach [Brewka et al., 2003]. In particular, conditional preferences among features are de\\ufb01ned by means of preference rules, the satisfaction of each rule is evaluated quantitatively, whereas the overall preference order is de\\ufb01ned in a qualitative way. The main advantages of such approach consist in the simplicity and compactness of representation, and combination of quantitative and qualitative evaluations.\"\n",
      "}\n",
      "Error in Subjective Causality\n",
      "File number 572,\n",
      " {\n",
      " \"abstract\": \"We show that it is possible to understand and identify a decision maker\\u2019s subjective causal judgements by observing her preferences over interventions.Following Pearl [2000], we represent causality using causal models (also called structural equations models), where the world is described by a collection of variables, related by equations. We show that if a preference relation over interventions satisfi\\u0119s certain axioms (related to standard axioms regarding counterfactuals), then we can de\\u0163ne (i) a causal model, (ii) a probability capturing the decision-maker\\u2019s uncertainty regarding the external factors in the world and (iii) a utility on outcomes such that each intervention is associated with an expected utility and such that intervention A is preferred to B iff the expected utility of A is greater than that of B. In addition, we characterize when the causal model is unique.Thus, our results allow a modeler to test the hypothesis that a decision maker\\u2019s preferences are consistent with some causal model and to identify causal judgements from observed behavior.\",\n",
      " \"introduction\": \"Causal judgments play an important role in decision making. When deciding between actions that intervene directly on some aspect of the world, one major source of uncertainty is the indirect effect of such actions via causal interaction. For example, when deciding the interest rate, the Federal Reserve might consider the possibility that a change in the interest rate will cause a change in unemployment, and further that this causal relationship itself might be contingent on other macroeconomic variables.Uncovering and describing the causal relationship between variables is a task that has led to enormous effort across many different disciplines (see, e.g., [Angrist and Pischke, 2009; Cunningham, 2021; Hern\\u00e1n and Robins, 2020; Morgan and Winship, 2007; Parascandola and Weed, 2001; Plowright et al., 2008; Pearl, 2009; Pearl, 2000; Spirtes et al., 1993]; this list barely scratches the surface.) Different decision makers, on account of their private information and personal experience, might hold different beliefs about causal relationships. In this paper, we show that it is possible to understand and identify a decision maker\\u2019s subjective causal judgements by observing her preferences over interventions.A first step to doing this is to decide how to represent causality. Most recent work has focused on using counterfactuals. In the philosophy community, following Stalnaker [1968] and Lewis [1973], counterfactuals are given semantics using possible worlds equipped with a \\u201ccloser than\\u201d relation; a counterfactual such as \\u201cIf \\u03d5 were true then \\u03c8 would be true\\u201d is true at a world \\u03c9 if, \\u03c8 is true at the closest world(s) to \\u03c9 where \\u03d5 is true. Pearl [2000] has championed the use of causal models (also called structural equations models, graphical models where the world is described by a collection of variables, related by equations. (These are related to models of causality in economics that go back to the work of Haavelmo [1943] and Simon [1953].) The equations model the effect of counterfactual interventions.We use the latter approach here, although as we we shall show, there are close relationships with the former approach as well. Following Pearl, we assume that the world is described by a set of variables. It is useful to split them into two sets: the exogenous variables, whose values are determined by factors outside the model, and the endogenous variables, whose values are determined by the exogenous variables and other endogenous variables. Which variables should be taken as exogenous and which should be taken as endogenous depends on the situation. For example, if the Federal Reserve is deciding whether to change the interest rates, the interest rates should clearly be viewed as endogenous. But for a company trying to decide whether to go ahead with a project that involves borrowing money at the current interest rates, the interest rates are perhaps best viewed as exogenous.A primitive action is an intervention that sets the value of a particular variable; because the values of the exogenous variables are taken as given, we assume that only on endogenous variables can be intervened on. Following Pearl [2000], we use the do notation to denote such actions. For example do[Y \\u2192 y] is the primitive action that sets variable Y to value y. Following Blume, Easley, and Halpern [2021] (BEH from now on), we allow more general actions to be formed from these primitive actions using if ...then ...else. Thus, non-primitive actions are conditional interventions of the formif \\u03d5 then A else Bwhere A and B are themselves (possibly primitive) actions and \\u03d5 is a test\\u2014a statement regarding the values of variables that is either true or false.As is standard in decision-theoretic analyses, we assume that the decision maker has a preference relation \\u2265 over actions. We show that if the preference relation satis\\u011fies certain axioms (that can be understood as corresponding to standard axioms regarding counterfactuals), then we can represent the decision-maker\\u2019s preference as the maximization of the expected utility of an actions relative to a causal model equipped with a probability and utility. Speci\\u0113cally, given a preference relation, we can de\\u0163ne a causal model, add a probability on contexts (settings of the exogenous variables, which can be viewed as capturing the decision-maker\\u2019s uncertainty regarding the external factors in the world), and a utility on outcomes (which are just settings of the variables in the model). In such a causal model, we can determine the expected utility of an action that involves (conditional) interventions. We show that the decision maker prefers action A to B iff the expected utility of A is greater than that of B, given the causal model, probability, and utility.Our results allow a modeler to test the hypothesis that a decision maker\\u2019s preferences are consistent with some causal model. In doing so, we provide a de\\u0163nition of causally sophisticated behavior as a benchmark of rationality for decision making in the presence of interventions.1 When a decision maker is causally sophisticated\\u2014when her actions can be rationalized by some causal model\\u2014our results further determine when the causal model can be uniquely identi\\u0113ed. This result provides a modeler the means to explain observed economic behavior in terms of causal judgments.To the best of our knowledge, we are the \\u010drst to examine causal decision making in the context of the structural equations, which is perhaps now the most common approach to representing causality in the social sciences and computer science. Bjorndahl and Halpern [2021] (BH from now on) addressed similar questions in the context of the closest-world approach to counterfactuals of Lewis [1973] and Stalnaker [1968]. Interestingly, our technical results use their results (and earlier results of BEH) as the basis for our representation theorem, based on a connection between the two representations of counterfactuals due to Halpern [2013]. Our results allow us to relate the two approaches at a decision-theoretic level.Other approaches to modeling causality have also been considered in the literature. Schenone [2020] and Ellis and Thysenb [2021] take a statistical approach, taking a lack of conditional independence as a de\\u0163nition of causality; Alexander and Gilboa [2023] understand causality through a reduction in the Kolmogorov complexity. As we said, our approach is closer to the approach that is currently the focus of work in the social sciences and computer science. Importantly, by identifying the structural equations, we provide a more detailed insight into the causal mechanisms being considered by the decision maker.\",\n",
      " \"literature review\": \"Let U and V denote the set of exogenous and endogenous variables, respectively. Let R : U \\u222a V \\u2192 2R associate to each Y \\u2208 U \\u222a V a set R(Y ) of possible values called its range. We assume that the range for each variable is \\u0163nite.Let S = (U, V, R) denote a signature.A causal model is a pair M = (S, F), where S is a signature and F is a collection of structural equations that determine the values of endogenous variables based on the values of other variables. Formally, F = {FX}X\\u2208V, whereFX :Y\\u2208U\\u222a(V\\u2212{X})\\u2192R(X).A model M is recursive if there exists a partial order on V such that the structural equations for each variable is independent of the variables lower in the order. When we say that X is independent of Y , we mean that FX does not depend on the value of Y . Given a signature S, let M(S) denote the set of all models over\n",
      "File number 573,\n",
      " {\n",
      " \"abstract\": \"Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a visual GUI agent \\u2013 SeeClick, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding \\u2013 the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance SeeClick with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created ScreenSpot, the first realistic GUI grounding dataset that encompasses mobile, desktop, and web environments. After pre-training, SeeClick demonstrates significant improvement in ScreenSpot over various baselines. Moreover, comprehensive evaluations on three widely used benchmarks consistently support our finding that advancements in GUI grounding directly correlate with enhanced performance in downstream GUI agent tasks.\",\n",
      " \"introduction\": \"Developing autonomous agents to assist humans on computing devices has been a persistent goal for artificial intelligence (Shi et al., 2017; Li et al., 2020a; Furuta et al., 2023; Zhou et al., 2023). These Graphical User Interface (GUI) agent systems aim to mimic human interactions in solving complex tasks, thereby enhancing efficiency and reducing manual effort, with examples like Siri and Copilot. Recent advances in Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023) and LLaMA (Touvron et al., 2023) have significantly propelled the evolution of GUI agents (Gur et al., 2023a; Zhou et al., 2023). These agents interact with the environment through extracted structured texts, e.g., HTML from web pages, then elicit LLM for planning, reasoning, and action gereration (Kim et al., 2023; Zheng et al., 2023).\",\n",
      " \"literature_review\": \"However, GUI agents depend on structured text face three inherent limitations: (1) Structured text is not always accessible, especially for iOS or desktop applications where acquiring such information is challenging (Shaw et al., 2023); (2) The verbose nature of structured text serves as an inefficient context for LLMs, while also omitting crucial information such as layout, images, and icons (Deng et al., 2023); (3) The variety of structured text - including HTML, DOM, and Android VH - necessitates the curation of task-specific observation and action spaces (Kim et al., 2023; Zhou et al., 2023). These entrenched deficiencies in text-based approaches call for an alternative solution.\",\n",
      " \"methodology\": \"In this paper, we propose a visual GUI agent built on Large Vision-Language Models (LVLMs) - SeeClick. Inspired by human interaction with GUIs, SeeClick performs low-level actions like clicking or typing directly by observing interface screenshots. This methodology bypasses the need for interacting with cumbersome structured text, empowering SeeClick as a universal visual agent suitable for various GUI platforms. Building such visual agents involves a foundational challenge: GUI grounding - the capacity to accurately locate screen elements based on instructions, which is absent in current LVLMs. To tackle this challenge, SeeClick enhances LVLM with a GUI grounding pre-training strategy. We devise a method to automate the curation of web grounding data and adapt public mobile UI datasets to obtain mobile grounding data.\",\n",
      " \"results\": \"SeeClick employs the above-curated dataset for continual pre-training of the LVLM, enabling it to accurately locate elements such as text, widgets, and icons in various GUI environments. Given GUI grounding is a fundamental yet underexplored capacity for GUI agents, we created ScreenSpot, the first realistic GUI grounding evaluation benchmark across various GUI platforms. ScreenSpot contains over 600 screenshots and 1200 instructions from iOS, Android, macOS, Windows, and web environments, and specifically includes both text-based elements and a variety of widgets and icons. Evaluation results confirm SeeClick\\u2019s superiority over current LVLMs, validating the effectiveness of GUI grounding pre-training.\",\n",
      " \"conclusion\": \"Finally, we adapted SeeClick to three downstream agent tasks: MiniWob (Shi et al., 2017), AITW (Rawles et al., 2023), and Mind2Web (Deng et al., 2023). As a purely visual-based agent, SeeClick achieves impressive performance across three tasks. Notably, SeeClick outperforms the visual baseline Pix2Act with only 0.3% training data on MiniWob. Moreover, experimental results on three tasks consistently support our finding that improvement in GUI grounding directly correlates with enhanced agent task performance.\"\n",
      "}\n",
      "File number 574,\n",
      " {\n",
      " \"abstract\": \"With the help of artificial intelligence (AI), a new pipeline has been proposed in the field of online advertising to generate creative advertisements, solving the problem of low aesthetics and quantity of traditional AI-based methods. This paper focuses on the use of the stable diffusion model with the LoRA model and two novel models, the prompt model and the reward model. The prompt model is designed to generate individualized creative images for different user groups, while the reward model comprehensively considers the multi-modal features of image and text to judge the quality and select the best ones to show online. The significant benefits obtained in online and offline experiments verify the significance of our proposed method.\",\n",
      " \"introduction\": \"Recently, AI-Generated Content (AIGC) has seen rapid development, leading to the emergence of text-to-image (T2I) generation tasks. Among various methods, diffusion-based methods have become state-of-the-art for T2I tasks due to their stationary training objective and easy scalability. However, directly using these methods for creative image generation is impractical in the advertising domain. This paper introduces stable diffusion in inpainting mode to modify only the background while preserving the main product information. In addition, two crucial factors, the prompt and the diffusion models, which directly affect the image generation, are discussed. To generate attractive creatives, a Prompt Model (PM) is introduced to select a good prompt, and a Reward Model (RM) is used to predict the Click-Through Rate (CTR) score for each creative image.\",\n",
      " \"literature_review\": \"To improve CTR, a pipeline has been proposed where in the first stage, a stable diffusion method is used to modify the background of the original product image. In the second stage, a prompt model is introduced to generate individualized prompts for different user groups. Finally, a reward model is trained to predict CTR scores for each creative image. The significant benefits obtained in online and offline experiments verify the significance of our proposed method.\",\n",
      " \"methodology\": \"The proposed pipeline consists of four main modules:\\n\\n1. Stable Diffusion Inpainting: This module is responsible for generating creative images by modifying only the background of the original product image. The saliency object image and mask image are obtained using a saliency detection model. A stable diffusion method in inpainting mode is then used to generate creatives. The LoRA model and prompt model are employed to fine-tune the stable diffusion model and select appropriate prompts.\\n\\n2. Prompt Model: The prompt model aims to select a good prompt that effectively describes the style of the image. It takes user attributes, item attributes, and contextual attributes as input and predicts the CTR score for each token in the whole word vocabulary under the given conditions. The top-ranked tokens are combined to form a prompt and inputted into the stable diffusion model to generate creative images.\\n\\n3. Reward Model: To predict the CTR scores of the creatives generated by the stable diffusion model, a reward model is introduced. This model considers the relationship between textual and visual features, as well as multi-head self-attention sub-modules, to learn the creative content and visual features. A list-wise loss function and a point-wise loss function are employed to train the reward model. The final loss is the sum of the list-wise loss and the point-wise loss.\\n\\n4. Self-cycling Training: To further optimize the quality of generated creatives and improve user experience, a self-cycling training mode is incorporated into the pipeline. The reward model is used to predict CTR scores for the generated creatives, and the top-ranked creatives are retained as training samples for both the LoRA model and the prompt model.\",\n",
      " \"results\": \"Extensive experiments were conducted to evaluate the effectiveness of the proposed pipeline. The results demonstrated:\\n\\n- Improved Online Results: CTR and revenue improvements of 10.4% and 9.7%, respectively, were observed when using the full self-cycling process compared to the baseline of using the original image provided by sellers.\\n\\n- Ablation Study on Reward Model: Experiments on commercial and public data showed the effectiveness of the proposed reward model structure, which outperformed two public methods in terms of CTR uplift and MSE metrics.\\n\\n- Ablation Study on Prompt Model: The importance of considering user group information was confirmed, as incorporating user features resulted in significant historical CTR improvements. Additionally, the self-cycling training process was found to further enhance the quality of generated creatives.\",\n",
      " \"conclusion\": \"The paper presents a novel pipeline for creative generation in online advertising that utilizes stable diffusion, a prompt model, and a reward model. The pipeline generates visually appealing and CTR-optimized creatives by modifying the background of original product images and considering user preferences. The prompt model enhances diversity and quality by generating personalized prompts, while the reward model selects the most promising creatives for display. Comprehensive experiments validate the effectiveness of the proposed method, demonstrating improvements in online performance and user experience.\"\n",
      "}\n",
      "File number 575,\n",
      " {\n",
      " \"abstract\": \"This research explores cryptocurrency staking reward prediction, offering insights to researchers and investors. Two methods are investigated: a sliding-window average and linear regression models. Results show ETH staking rewards can be forecasted with an RMSE within 0.7% and 1.1% for 1-day and 7-day look-aheads using a 7-day sliding-window average. Prediction accuracies vary across cryptocurrencies, with linear regression superior for short-term XTZ and ATOM predictions. Staking rewards are stable for most assets, except MATIC.\",\n",
      " \"introduction\": \"Staking in cryptocurrency involves users locking digital assets to participate in a blockchain's consensus mechanism, earning rewards. Various platforms have staking processes and reward determination methods. Staking rewards depend on factors like the amount of cryptocurrency staked, staking duration, inflation rates, and network transaction fees.\",\n",
      " \"literature review\": \"Prior research has explored staking-related topics, such as optimal staking problems, equilibrium staking levels, and rewards farming. Other studies have focused on staking pools and centralization concerns. However, forecasting staking rewards has not been previously explored.\",\n",
      " \"methodology\": \"The study utilized two forecasting algorithms: moving-window average and linear regression models. The moving-window average calculates an average of past data points to predict future values. Linear regression involves creating a linear relationship between an explanatory variable and the dependent variable. The objective was to develop a model to forecast staking rewards for various cryptocurrencies for a specified number of upcoming days. Data was collected from 2021-06-23 to 2022-08-06, and a training and test data splitting strategy was employed to optimize model performance.\",\n",
      " \"results\": \"For next-day prediction, moving-window average and single-feature linear regression performed similarly for ETH, with RMSE divided by the mean ranging from 0.007 to 0.009. Linear regression outperformed moving-window average and multiple linear regression for ATOM and XTZ. Moving-window average performed best for SOL and MATIC. For next N-days prediction, moving-window average outperformed other methods for all tokens except MATIC. The effectiveness of ML algorithms decreased as prediction days increased, and a 7-day ahead prediction using moving-window average had an increased error of no more than 3.2% compared to a 1-day ahead prediction. Simple linear regression based on staking rewards time series alone outperformed multiple linear regression using other features.\",\n",
      " \"conclusion\": \"The study demonstrates the effectiveness of simple linear regression and moving-window average in predicting staking rewards. A 7-day moving-window average approach provides accurate predictions for most tokens, except MATIC, with an RMSE within 0.7% and 1.1% for 1-day and 7-day look-aheads. Prediction accuracies vary across cryptocurrencies, with linear regression superior for short-term XTZ and ATOM predictions. Staking rewards are stable for most assets, except MATIC, presenting a notable exception.\"\n",
      "}\n",
      "File number 577,\n",
      " {\n",
      " \"abstract\": \"This paper describes a virtual reality (VR) serious game designed to foster understanding of dyslexia and raise awareness of the challenges it can create, with the aim of encouraging a supportive environment that promotes academic success for dyslexic students.\",\n",
      " \"introduction\": \"Dyslexia, particularly phonological dyslexia, causes difficulties in connecting sounds of words to their written forms, leading to challenges like slow reading speed and difficulty decoding unfamiliar words. These difficulties can be frustrating for students, potentially resulting in feelings of misunderstanding or stigmatization. To overcome these obstacles, dyslexic students often rely on compensatory tools and strategies. However, raising awareness and empathy among non-dyslexic individuals, including teachers and peers, can contribute to the provision of essential support for dyslexic students.\",\n",
      " \"literature review\": \"Studies have demonstrated the potential of VR technology in promoting social inclusion and facilitating empathy. Previous research has explored the use of VR to increase social skills in individuals with Autism Spectrum Disorder and to enhance empathy towards wheelchair users. VR has also been utilized to address some of the problems caused by dyslexia in children, showing improvements in attention and indicating the potential for long-term benefits in reading skills.\",\n",
      " \"methodology\": \"The methodology employed involves immersing players, such as teachers or peers of dyslexic students, in a virtual world where they are tasked with reading a recipe book and correctly adding ingredients to a potion. However, the text in the book and ingredient labels are presented in a font designed to replicate the reading difficulties experienced by people with dyslexia. The game flow includes three attempts to complete the task, with increasing time limits and the introduction of compensatory tools such as shorter words and an audio guide. The game ends when all attempts are exhausted or when the player successfully creates the correct potion.\",\n",
      " \"results\": \"The VR experience was tested on 32 non-dyslexic individuals. The results showed that most participants were unable to solve the task initially, reporting feelings of frustration and anxiety due to the difficulties in reading the instructions. However, these feelings tended to dissipate when more time and compensatory tools were provided, leading to an increased awareness of the issues and needs of dyslexic students. A survey conducted after the VR experience indicated that participants perceived the task as difficult but also reported an increase in their empathy towards people with dyslexia.\",\n",
      " \"conclusion\": \"The VR experience presented in this paper is considered a promising tool for raising empathy towards people with dyslexia. Future research will involve collecting data from a larger sample size to validate the effectiveness of the serious game and to study user empathy profiles through the application of artificial intelligence techniques. Additionally, other serious games are being designed to foster empathy for individuals with different types of dyslexia.\"\n",
      "}\n",
      "File number 578,\n",
      " {\n",
      " \"abstract\": \"Pushing updates periodically and returning feedback when needed, adapting the physical decisions to the communication policy, improves the efficiency of systems with sensors and actuators. Choosing between push-based and pull-based communication is crucial. This work proposes an analytical model for optimizing push-based and pull-based strategies in Cyber-Physical Systems (CPSs). Both options have advantages and drawbacks. The push-based system performs better at the optimum, but is PPAD-hard. Numerical results show that adjusting the actuators' decision to the network operation is a better approach.\",\n",
      " \"introduction\": \"Recent works have shown that AoI-based optimization may lead to sub-optimal performance, and that adapting the actuators' decisions to the network operations, thus jointly optimizing communication and control, is a better approach. This research investigates the performance of pull-based and push-based communication strategies in CPSs.\",\n",
      " \"literature_review\": \"Several recent works on Effective Communication (EC) systems have investigated the push-based configuration. However, the joint optimization of EC and control policies is still relatively unexplored, and, to the authors' knowledge, no studies have directly compared the push- and pull-based communication approaches and analyzed the interdependency between CPS optimization and the Value of Information (VoI).\",\n",
      " \"methodology\": \"We consider a simple CPS scenario with a single actuator, without local sensing capabilities, that is connected to a base station (BS) via a constrained communication channel. Using this model, we analyze the advantages and drawbacks of each configuration, proving relevant results and showing that the push-based system, while having better performance at the optimum, is a PPAD-hard problem.\",\n",
      " \"results\": \"The performance of a push-based system should improve with respect to the pull-based one, as the former can exploit information on the specific realization of the system state trajectory. On the other hand, remote POMDPs are multi-agent problems and may involve critical issues in terms of coordination between the BS and Actuator. Results show that the push-based approach selects the best update interval for every state. The pull-based EC strategy Pareto dominates the AoI-based policy, and the API approach leads to a Nash Equilibrium (NE) policy in the push-based problem. However, the optimal EC solution to the push-based problem does not always Pareto dominate the AoI-based strategy, and the solution obtained by the API strategy does not Pareto dominate the AoI-based strategy.\",\n",
      " \"conclusion\": \"This work presents an analytical framework to adapt classical optimization tools such as Policy Iteration (PI) to the context of communication and control problems, providing numerical results that show a strong dependency between the VoI and the structure of the underlying Markov Decision Process (MDP). The analysis revealed that the common push-based view of communication and control problems may not always be optimal, as the game theoretical properties of multiagent scenarios do not guarantee that a solution may be better even than a simple AoI-based optimization.\"\n",
      "}\n",
      "File number 580,\n",
      " {\n",
      " \"abstract\": \"This paper presents a run-time software testing, analysis, and code optimization method for quantum neural network software in advanced Internet-of-Things systems, which visually presents the learning performance called barren plateau. The visual presentation of barren plateau situations is helpful for real-time quantum-based advanced IoT software testing because software engineers can be aware of the training performances of QNN. The proposed method is also capable of visual feedback.\",\n",
      " \"introduction\": \"In modern daily lives, people utilize Internet services not only in smartphones but also in wearable devices. IoT enables the global Internet connection among all IoT devices, which realize the utilization of data from built-in sensors in the devices. QNN-based models can utilize much fewer parameters comparing to conventional deep neural network-based models and generally achieve fast convergence and high scalability. However, QNN-based models are more difficult to be trained due to barren plateaus and tracking barren plateaus is essential for measuring the stability of QNN software. Therefore, a new software test, analysis, and code optimization tool for QNN software is required to visually identify barren plateau situations. This will be a beneficial approach for software engineers in designing and developing high-accurate QNN-based advanced IoT software. The proposed TACO is also capable of visual feedback.\",\n",
      " \"literature review\": \"QNN-based models are advantageous in terms of data processing acceleration and low computational complexity, but have a disadvantage of the barren plateaus situation which can make the loss gradient of QNN-based models vanish due to entanglement. Various approaches have been suggested to tackle this situation, but these research results require deep-dive understanding in quantum mechanics, quantum computing, and quantum optimization. To resolve this problem from the viewpoints of deep learning software engineers, a novel software development and analysis tool is required.\",\n",
      " \"methodology\": \"The design rationales of TACO are: 1) Barren Plateaus Problem Tracking, 2) Dynamic Run-Time Software Testing, and 3) HCI-based Visualization. The overall architecture of TACO consists of VQC Structure, TACO Engine, TACO I/O, and External Visualization Engine. VQC Structure is implemented by torch-quantum, and the gradient derivation is operated via backward(). VQC Structure Extractor calls parameter information and iterative gradient derivation calculation is operated. Barren Plateau Estimator estimates the barren plateau occurrences and values. Model Feedback Generator handles the case where the barren plateau situations (i.e., the variance of gradients) happens in all quantum gates.\",\n",
      " \"results\": \"The results show that the proposed TACO can identify which quantum gates fall into barren plateaus situations with text messages. Software engineers can monitor and observe QNN-based model training performances using TACO. The proposed TACO can also identify which quantum gates fall into barren plateaus situations with text messages.\",\n",
      " \"conclusion\": \"This paper introduces a novel dynamic run-time software testing, analysis, and code optimization (TACO) tool for QNN-based model software, which visually presents gradient variances in order to identify whether barren plateaus occur or not for advanced IoT systems software. This TACO tool is obviously useful for software engineers because it can intuitively guide them in order to design and implement high-accurate QNN-based models for advanced IoT applications even if they are not familiar with quantum mechanics and quantum computing. Moreover, the proposed TACO is also capable for visual feedback because software engineers recognize barren plateaus using visualization via tensorboard; and then, they modify QNN-based model structures based on that. Future work can extend TACO to track and test other factors in QNN, and extend TACO for distributed learning architectures.\"\n",
      "}\n",
      "File number 581,\n",
      " {\n",
      " \"abstract\": \"Recent advancements in Large Language Models (LLMs) exhibit impressive capabilities in various applications, yet LLMs face challenges with limited context windows and generalization difficulties. We introduce a metacognition module for generative agents, allowing them to observe their own thought processes and actions. This approach, designed to emulate System 1 and System 2 cognitive processes, significantly enhances agent performance by modifying their strategy. We tested the metacognition module on a variety of scenarios, including a zombie apocalypse simulation, demonstrating how our system outperforms others, with agents adapting and improving strategies to complete tasks over time.\",\n",
      " \"introduction\": \"Metacognition refers to the cognitive process of thinking about one's own thinking. It involves activities related to monitoring, regulating, and organizing cognitive processes to achieve specific goals. Metacognitive abilities enable individuals to reflect on their knowledge, problem-solving strategies, and learning experiences, shaping and modifying their habits. Psychologist Daniel Kahneman's concept of System 1 and System 2 thinking provides a framework for understanding metacognition. System 1 represents fast, automatic, and intuitive thinking, while System 2 involves slower, deliberate, and reflective thinking. Metacognition can be seen as a specific System 2 process that examines actions from both System 1 and System 2 processing.\",\n",
      " \"literature_review\": \"Multiple experiments have incorporated metacognition into computational frameworks. Cox et al. outline a general computational architecture in Lisp. Mustafa et al. provide a framework for autonomous vehicles that adds a metacognition layer to monitor safety violations. Krueger, Lieder, and Griffiths created a deep reinforcement learning (DRL) framework that incorporates metacognition. Park et al. demonstrated that LLMs equipped with reflection, observation, and planning modules on agents can mimic believable human behavior. Our proposed metacognition module allows agents to broadly contemplate their circumstances to create alternative strategies and improve performance.\",\n",
      " \"methodology\": \"We implemented many of the same modules from Park et al., with the addition of a group of modules dubbed meta_cognize. As an agent progresses through the simulation, it accumulates a history of observations, memories, and thoughts. Agents are given goals but can optionally be left blank. When an agent starts towards its goal, it is not given an explicit strategy to follow. Instead, each agent periodically evaluates how it is progressing towards its goals by reviewing memories, thoughts, and past actions. The agent assigns itself a numeric score and a text statement for its reasoning. This evaluation is stored in its memory as a meta-thought. If the agent finds that it is not making enough progress, it calls its meta_cognize module. During metacognition, the agent asks itself how it might improve its performance in light of what it has learned. Additionally, the agent will periodically self-generate new introspective questions to think about its goals from different perspectives.\",\n",
      " \"results\": \"We tested our simulation framework in a variety of situations, including a Christmas party, zombie apocalypse, and murder mystery. In the Christmas party simulation, agents hosted a party where multiple other agents were invited and arrived at the specified time to attend. This is similar to earlier work in generative agents where agents had to coordinate a similar social activity. In the zombie apocalypse simulation, zombies are non-playable characters that are allowed to kill non-zombie agents. Agents initially have no goal but can develop them over time. Survivors most often self-discovered a strategy of hiding in zombie-free areas. We found that in 73% of zombie scenarios, agents would not survive. Performance of our cognitive models is shown through ablation. Evaluation metrics are composed of five criteria: believability, learning, individual goal performance, higher level cognitive performance, and overall scenario performance.\",\n",
      " \"conclusion\": \"We show that metacognition significantly improves performance for task-oriented generative agents. Furthermore, we illustrate the potency of combining large language models with traditional programming methods as effective tools for prototyping cognitive systems. As generative agents integrated with metacogntive abilities approach ubiquity in daily human life, taking on increasingly sophisticated tasks, their proliferation across diverse domains marks a paradigm shift in both lay human-computer interactions and programmer-computer interactions. This shift paves the way for the emergence of more intelligent, adaptive, and context-aware systems. With these advancements in mind, the strategic interplay of metacognition, LLMs, and traditional programming methodologies emerges as a powerful technique for the productionization of intelligent generative agents.\"\n",
      "}\n",
      "File number 583,\n",
      " {\"abstract\": \"This paper presents a study on stock market analysis made on a dataset containing 750 examples and 16 attributes. Key aspects examined include exploratory data analysis (EDA), feature engineering, data preparation, and model selection. The Fama French 3-factor model is also utilized in the analysis.\", \"introduction\": \"The stock market has become a significant indicator of financial stability and a crucial factor in investment decisions. This paper presents an in-depth analysis of a dataset consisting of 750 examples and 16 attributes. The goal of this analysis is to uncover insights into the stock market and identify potential models for predicting stock prices.\", \"literature review\": \"The Fama French 3-factor model, widely used in finance, is included in the analysis. This model considers three factors - market return, return on small companies over big ones (SMB), and return on value companies over growth ones (HML) - to provide a more comprehensive understanding of the stock market.\", \"methodology\": \"The study involves various steps: exploratory data analysis (EDA), data cleaning, feature engineering, clustering, and model selection. Three models are employed - linear regression, random forest, and gradient boosting - to predict stock returns based on selected features. The results of each model are compared to determine the most suitable one.\", \"results\": \"The study shows that the common factors - Dow Jones Index, Volume, SMB Factor, and MKT Factor - have a significant influence on stock returns. Clustering analysis also identifies stocks that fluctuate with Disney stock. Among the three models used, linear regression performs best with an accuracy of 95.23%, followed by gradient boosting with an accuracy of 92.97%. Random forest has the lowest accuracy at 71.27%.\", \"conclusion\": \"This study provides insights into the stock market behavior of Disney stock and demonstrates the value of feature engineering and clustering techniques in improving the accuracy of stock return predictions. Future research could involve different models and feature engineering techniques, as well as more data covering a longer period and more stocks for a more comprehensive analysis of the stock market.\"}\n",
      "File number 584,\n",
      " {\n",
      " \"abstract\": \"This study examines the potential of quantum circuits to reduce energy consumption in cryptocurrency mining, particularly in the SHA-256 cryptographic hashing function. The authors propose using quantum computing concepts to optimize the mining process and demonstrate the implementation of the Quantum XOR (CNOT) gate. Their work provides a proof-of-concept for the application of quantum technology in reducing the environmental impact of cryptocurrency mining.\",\n",
      " \"introduction\": \"Quantum computing holds promise for solving challenging problems in the financial world, including cryptocurrency mining. Conventional methods consume significant energy, with Bitcoin mining responsible for nearly one-third of its market value. There is a critical need to reduce these energy costs, leading to research efforts aimed at optimizing mining processes. Quantum computing, with its inherent low-energy characteristics, presents a potential solution.\",\n",
      " \"literature_review\": \"Previous studies have explored alternative approaches to classical SHA-256, such as the work of Ablayev and Vasiliev on quantum hashing and Vasiliev's binary quantum hashing. These investigations demonstrate the feasibility of quantum-based hash functions but are limited by the current small capacity of quantum hardware. Researchers have also investigated quantum annealing as an alternative method for SHA-256, utilizing the Ising model to minimize the objective function. Hybrid quantum computers have emerged as a promising solution to mitigate the limitations of current quantum computers, combining quantum and classical hardware to leverage their strengths.\",\n",
      " \"methodology\": \"To demonstrate the application of quantum technology in SHA-256, the authors designed quantum circuits for the hash function, implementing them on both real quantum computer hardware and quantum simulators. These circuits were designed to exhibit the first parts of the SHA-256 hash process loop, given the limited capacity of remotely accessible systems. The results of the circuit execution were obtained after a number of shots (loop repetitions), reflecting the probabilistic nature of quantum computations. The authors also explored the use of hybrid quantum computers to share SHA-256 tasks between quantum and classical hardware.\",\n",
      " \"results\": \"The authors' experiments successfully demonstrated the implementation of the Quantum XOR (CNOT) gate, which is the core operation in SHA-256. The quantum circuits exhibited the initial part of the SHA-256 hash function loop on both real quantum computer hardware and quantum simulators. The results obtained from these circuits showcased the probabilistic nature of quantum computations, with the most probable outcomes identified. Furthermore, the authors compared the energy consumption of quantum and classical mining systems, highlighting the significant reduction in energy consumption offered by quantum computing.\",\n",
      " \"conclusion\": \"The study provides a proof-of-concept for the application of quantum technology in optimizing the energy consumption of cryptocurrency mining. While the current limitations of quantum hardware restrict the implementation of the entire mining process, the authors emphasize the potential of future developments in quantum computing hardware and hybrid quantum computers. They address concerns about the probabilistic nature of quantum computations and their impact on the determinism required in cryptocurrency mining, explaining how these results can be interpreted by classical hardware to ensure deterministic outcomes. The authors also acknowledge potential debates on the implications of easier cryptocurrency production on market dynamics and emphasize that their proposed method focuses on reducing energy consumption rather than simplifying the mining process.\"\n",
      "}\n",
      "File number 586,\n",
      " {\n",
      " \"abstract\": \"The article explores an interactive platform, RIS3-MCAT, developed using open data, semantic analysis, and data visualization to monitor challenge-oriented smart specialization in Catalonia. RIS3-MCAT, aimed at providing monitoring systems and tools for mapping and understanding R&I policies and projects, facilitates access to data on projects, enabling detailed analyses beyond classical classification systems.\",\n",
      " \"introduction\": \"The paper highlights the necessity of developing monitoring systems and tools for a successful transformation towards sustainable development and new patterns of specialization. It also emphasizes the importance of coordination and collaboration among different societal stakeholders and the need for evaluating the impact of public policy and R&I.\",\n",
      " \"literature review\": \"The authors provide an extensive literature review covering various topics. They discuss the challenges posed by globalization, technology, climate change, and pandemics, and the enormous opportunities they bring. They mention the European Commission's initiative to accelerate the green transition and allocate funds for tackling societal challenges aligned with UN Sustainable Development Goals. The paper highlights the importance of changes in cooperation between governments, industry, academia, and civil society. Furthermore, it delves into strategies for smart specialization, which are dynamic agendas for economic and social transformation based on R&I, driven by entrepreneurial discovery processes. The authors stress the role of smart specialization in addressing complex transformative processes and the need for interactive visualization tools to identify and analyze emerging areas of specialization and collaboration networks.\",\n",
      " \"methodology\": \"The development of the RIS3-MCAT tool involved a five-year co-design and development process. It includes the integration of data from CORDIS and SIFECAT, automatic classification of projects, topic modeling, and SDG classification. The data integration and cleaning process utilizes an ontology, and a domain ontology is used for data integration. Automatic classification of projects according to regional priorities is done using pre-trained language models for training textual classifiers. Topic modeling is used for capturing bottom-up domains by discovering thematics linked with a specific collection of texts. For identifying SDG-related research, a controlled vocabulary based on a hybrid approach that combines automatic and human-crafted keywords is employed.\",\n",
      " \"results\": \"The RIS3-MCAT tool offers various features, including search and filters, network analysis, a semantic map of projects, and analytical/statistical modules. The tool allows users to explore various scenarios, such as searching for actors and projects in similar topics, identifying collaboration networks, examining the state of research for a priority area, and promoting collaboration. Key insights from the project include the importance of strategic adaptability, consistent dialogue between stakeholders and providers, and decisive leadership in the co-design process. The need for manual attention and curation of data, despite automation, is also mentioned. Additionally, the challenges of dealing with a large number of entities and relationships in visualization and the rapid advancement of AI and NLP are discussed.\",\n",
      " \"conclusion\": \"The paper concludes by highlighting the RIS3-MCAT platform's ability to tackle the challenge of monitoring challenge-oriented smart specialization with available technologies and open data. It emphasizes the key lessons learned during the process, including the importance of a collaborative design approach, the necessity of manual attention and curation of data, the need for continuous data integration, and the potential for re-publishing datasets for wider reuse. The paper also discusses opportunities for future development, such as incorporating additional R&I projects, improving visualization and interaction features, and exploring new classification systems.\"\n",
      "}\n",
      "File number 587,\n",
      " {\n",
      " \"abstract\": \"Safety incidents in AI systems diverge from expectations, revealing a socio-technical complexity. Accidents result from poor engineering, unclear safety measures, and stakeholder influence failures rather than technological malfunctions. An expanded socio-technical perspective is necessary.\",\n",
      " \"introduction\": \"The widespread adoption of AI systems has led to various failures, highlighting the need for AI safety research. A variety of communities are delving into AI safety, aiming to understand and address risks in ML systems. However, few studies have grounded their analysis on real-world deployments and disasters.\",\n",
      " \"literature review\": \"Prior work has focused on taxonomizing AI safety issues and exploring theoretical approaches, such as control theory and formal methods. While these formalizations capture aspects of the encountered problems, they may overlook vital real-world dimensions. Existing frameworks present challenges in understanding how AI systems and safety mechanisms succeed or fail in practice.\",\n",
      " \"methodology\": \"The study revisits a commonly cited taxonomy of AI safety issues by Amodei et al. (2016). The authors surveyed 170 papers to identify a subset of concrete AI Safety problems and analyzed real-world use cases for three specific problems: Safe Exploration, Avoiding Negative Side Effects, and Scalable Oversight.\",\n",
      " \"results\": \"The analysis revealed themes and insights that diverge from existing taxonomies. The authors emphasize failures rooted in engineering practice beyond theoretical design flaws. They advocate for validation of safety problems through inductive reasoning, considering real-world case studies to refine AI safety mechanisms. Additionally, the study stresses the significance of stakeholder impact and interactions, highlighting the need for broad deliberation and input in determining accident causes.\",\n",
      " \"conclusion\": \"Failures in AI systems exhibit a socio-technical complexity that extends beyond technological malfunctions or formal design flaws. The prevalent view focuses on the developer's control over technical aspects, overlooking the systematic nature of failures influenced by interactions with users and stakeholders. Safety mechanisms are found to be ineffective due to broader socio-technical factors, requiring a socio-technical framing that acknowledges power structures and stakeholder involvement.\"\n",
      "}\n",
      "File number 589,\n",
      " {\n",
      " \"abstract\": \"Foundation models offer a new opportunity to redesign systems and workflows with an AI-first perspective. However, operationalizing this opportunity faces challenges and trade-offs. This article provides an organizational framework for making rational choices as enterprises embark on their transformation journey towards an AI-first organization.\",\n",
      " \"introduction\": \"The advent of AI technology, particularly foundation models, presents an unprecedented opportunity for enterprises to reimagine their operations and processes. However, realizing this potential requires addressing various challenges and trade-offs. This article aims to provide an organizational framework that guides enterprises in making informed choices as they navigate their transformation towards an AI-first approach.\",\n",
      " \"literature review\": \"The literature review acknowledges the limited historical moments when societal functions could be reimagined. It emphasizes the transformative potential of AI, citing Microsoft CEO Satya Nadella's insights into the company's AI strategy and the importance of a holistic, disciplined approach to AI adoption to avoid potential pitfalls.\",\n",
      " \"methodology\": \"The methodology section outlines the approach taken to develop the proposed organizational framework. It highlights the focus on invariant factors that are less prone to rapid changes in the field of AI. This approach aims to provide guidance that remains relevant despite the evolving nature of AI technology.\",\n",
      " \"results\": \"The results section presents the anticipated future state of AI technology, characterized by intelligence as a service (IQaaS). It envisions a scenario where intelligence becomes a readily accessible, frictionless commodity, similar to electricity. This future state is expected to be driven by the continuous evolution of AI models, with capabilities progressing from weak AI to strong AI and eventually integrating with robotics. Data plays a pivotal role in shaping this trajectory, with the emergence of synthetic data as a dominant content source.\",\n",
      " \"conclusion\": \"The conclusion section emphasizes the significance of AI as a major entry in the modern computing stack. It underscores the need for a holistic, intentional, and informed approach to AI adoption, emphasizing the importance of team composition, data management, evaluation, transparency, and security. The article acknowledges the transformative potential of generative AI but stresses the necessity of a well-rounded strategy to mitigate risks and ensure successful enterprise transformations.\"\n",
      "}\n",
      "Error in Responsible AI Governance: A Systematic Literature Review\n",
      "File number 590,\n",
      " {\n",
      " \"abstract\": \"As artificial intelligence (AI) proliferates across industries and drives innovation, concerns arise regarding responsible AI practices. This paper explores the current literature on AI Governance, analyzing frameworks, models, tools, and policies with the \\\"3W1H\\\" approach: Who governs? What is governed? When is it governed? and How is it regulated? Out of 61 studies, only 5 addressed all inquiries. The findings emphasize the need for holistic governance mechanisms, along with the importance of stakeholder involvement and adherence to ethical principles.\",\n",
      " \"introduction\": \"The rapid adoption of artificial intelligence (AI) has brought to light various risks and negative consequences, prompting the development of responsible AI (RAI) principles and strong AI governance techniques. RAI ensures that AI is developed and deployed ethically, addressing concerns such as bias, discrimination, and lack of transparency. Ethical AI principles are incorporated through proper AI governance practices, encompassing regulations and technical mechanisms to align with an organization's strategies.\",\n",
      " \"literature_review\": \"The literature review explores existing research on AI governance solutions, emphasizing the need for transparency, explainability, and accountability. It highlights the challenges associated with diversity and inclusion in AI governance, including bias, poor reliability, and the lack of inclusive solutions. The review's objective is to examine AI governance solutions from various perspectives, proposing a categorisation of key elements under different governance levels.\",\n",
      " \"methodology\": \"This research employs a systematic literature review methodology, following a rigorous search and selection process to identify relevant studies on AI Governance. The 61 selected studies are analyzed using the \\\"3W1H\\\" approach, which examines four key questions: Who governs? What is governed? When is it governed? and How is it governed? The findings shed light on stakeholders' roles, elements to be governed, development stages for governance, and governance mechanisms such as frameworks, tools, and policies.\",\n",
      " \"results\": \"The analysis revealed that only 5 out of 61 studies addressed all aspects of the \\\"3W1H\\\" approach. Most studies focused on \\\"How\\\" AI should be governed, neglecting aspects such as stakeholder involvement and the stage of AI development. It was found that data and AI systems received the most attention, while stakeholder involvement remained a significant gap. The highest number of AI governance solutions were categorized under organizational-level governance, but many lacked complete answers regarding stakeholders, development stages, and governance mechanisms.\",\n",
      " \"conclusion\": \"Responsible AI governance is paramount in mitigating risks associated with AI and fostering ethical AI practices. The findings of this study underscore the need for holistic governance frameworks that address the \\\"Who,\\\" \\\"What,\\\" \\\"When,\\\" and \\\"How\\\" aspects of AI governance and align with RAI principles. The exploration of grey literature, such as white papers and company frameworks, will further enrich the understanding of responsible AI governance.\",\n",
      "}\n",
      "File number 592,\n",
      " {\n",
      " \"abstract\": \"Knowledge graph embedding transforms knowledge graphs into a continuous, low-dimensional space, facilitating inference and completion tasks. This field is mainly divided into translational distance models and semantic matching models. A key challenge in translational distance models is their inability to effectively differentiate between 'head' and 'tail' entities in graphs. To address this, the novel location-sensitive embedding (LSE) method has been developed. LSE innovatively modifies the head entity using relation-specific mappings, conceptualizing relations as linear transformations rather than mere translations. The theoretical foundations of LSE, including its representational capabilities and its connections to existing models, have been thoroughly examined. A more streamlined variant, LSEd, employs a diagonal matrix for transformations to enhance practical efficiency. In tests conducted on four large-scale datasets for link prediction, LSEd either outperforms or is competitive with leading contemporary models.\",\n",
      " \"introduction\": \"Knowledge graph embedding simplifies relational reasoning into mathematical operations by transforming discrete entities and relationships into a continuous vector space. Knowledge graph embedding methods can be broadly categorized into two main groups: translational distance models and semantic matching models.\",\n",
      " \"literature_review\": \"Translational distance model models relationships as translational transformations and uses distance scores to measure the correctness of triples; The semantic matching model uses similarity scores. TransA uses (|h + r \\u2212 t|)M_r(|h + r \\u2212 t|)^T, which assumes that M_r is a semipositive definite matrix. RESCAL adopts quadratic scoring function, namely hM_r t^T, to capture the interaction between potential factors of entities; DistMult further simplified the matrix into diagonal matrix, and adopted h diag(r) t^T, which made the model unable to distinguish symmetric relations. ComplEx studied the modeling bottleneck of point multiplication operation on antisymmetric relations, and proposed to extend Dist Mult to complex space.\",\n",
      " \"methodology\": \"LSE model establishes the relationship f_r(h) + r = t for the correct triple. LSE uses the distance scoring function \\\\|hR_r - t\\\\|_p. Experiments show that it is better to choose 1 norm in this paper. LSEd uses the distance scoring function \\\\|h diag(r) - t\\\\|_p = \\\\| h \\u22c5 r - t \\\\|_p.\",\n",
      " \"results\": \"The models proposed in this paper have achieved the best performance on FB15k-237 and WN18RR data sets. For FB15k237, the MRR of LSEd was 0.331, Hits@1 was 0.237, Hits@3 was 0.366 and Hits@10 was 0.522. For WN18RR, the MRR of LSEd was 0.450, Hits@1 was 0.407, Hits@3 was 0.465 and Hits@10 was 0.537.\",\n",
      " \"conclusion\": \"This paper proposes a location-sensitive embedding model for knowledge graph embedding representation, and the effectiveness of the proposed improved model is verified theoretically and experimentally, and the superior performance is achieved on the task of link prediction in large-scale knowledge graphs.\"\n",
      "}\n",
      "File number 593,\n",
      " {\n",
      " \"abstract\": \"We present Depth Anything, a highly practical solution for robust monocular depth estimation. Unlike prior works that laboriously construct diverse labeled datasets, we highlight the value of massive, cheap, and diverse unlabeled images. We design two simple yet highly effective strategies \\u2014 challenging the student model with strong perturbations when learning unlabeled images, and preserving rich semantic priors from pre-trained models \\u2014 to unlock their potential. Consequently, our Depth Anything model exhibits superior zero-shot depth estimation ability across extensive unseen scenes and is further strengthened for downstream metric depth estimation and semantic segmentation tasks. We release the code and models for broad research use.\",\n",
      " \"introduction\": \"This work presents Depth Anything1, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability (Figure 1). Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet.\\nOur work was done during an internship at TikTok.\",\n",
      " \"literature review\": \"The field of computer vision and natural language processing is currently experiencing a revolution with the emergence of \\u201cfoundation models\\u201d [6] that demonstrate strong zero-/few-shot performance in various downstream scenarios [44, 58]. These successes primarily rely on large-scale training data that can effectively cover the data distribution. Monocular Depth Estimation (MDE), which is a fundamental problem with broad applications in robotics [65], autonomous driving [63, 79], virtual reality [47], etc., also requires a foundation model to estimate depth information from a single image. However, this has been underexplored due to the difficulty of building datasets with tens of millions of depth labels. MiDaS [45] made a pioneering study along this direction by training an MDE model on a collection of mixed labeled datasets. Despite demonstrating a certain level of zero-shot ability, MiDaS is limited by its data coverage, thus suffering disastrous performance in some scenarios.\",\n",
      " \"methodology\": \"Our work utilizes both labeled and unlabeled images to facilitate better monocular depth estimation (MDE). For labeled images, we produce the depth labels with an off-the-shelf MDE model. Therefore, we can easily annotate large-scale unlabeled images and augment the data coverage. Moreover, to leverage the unlabeled images effectively, we design two simple yet effective strategies. First, we challenge the student model with strong perturbations when learning the pseudo labels. It forces the student model to actively seek extra visual knowledge and learn more robust representations. Second, we enforce an auxiliary constraint on the student model to preserve rich semantic priors from pre-trained encoders.\",\n",
      " \"results\": \"Both with a ViT-L encoder, our Depth Anything surpasses the strongest MiDaS model tremendously across extensive scenes in terms of both the AbsRel (absolute relative error: |d* \\u2212d|/d) and \\u03f41 (percentage of max(d*/d, d/d*) < 1.25) metrics. For example, when tested on the well-known autonomous driving dataset DDAD [20], we improve the AbsRel (\\u2193) from 0.251 \\u2192 0.230 and improve the \\u03f41 (\\u2191) from 0.766 \\u2191 0.789.\\nBesides, our ViT-B model is already clearly superior to the MiDaS based on a much larger ViT-L. Moreover, our ViT-S model, whose scale is less than 1/10 of the MiDaS model, even outperforms MiDaS on several unseen datasets, including Sintel, DDAD, and ETH3D. The performance advantage of these small-scale models demonstrates their great potential in computationally-constrained scenarios.\\nIt is also worth noting that, on the most widely used MDE benchmarks KITTI and NYUv2, although MiDaS v3.1 uses the corresponding training images (not zero-shot anymore), our Depth Anything is still evidently superior to it without training with any KITTI or NYUv2 images, e.g., 0.127 vs. 0.076 in AbsRel and 0.850 vs. 0.947 in \\u03f41 on KITTI.\",\n",
      " \"conclusion\": \"In this work, we present Depth Anything, a highly practical solution to robust monocular depth estimation. Different from prior arts, we especially highlight the value of cheap and diverse unlabeled images. We design two simple yet highly effective strategies to fully exploit their value: 1) posing a more challenging optimization target when learning unlabeled images, and 2) preserving rich semantic priors from pre-trained models. As a result, our Depth Anything model exhibits excellent zero-shot depth estimation ability, and also serves as a promising initialization for downstream metric depth estimation and semantic segmentation tasks.\"\n",
      "}\n",
      "File number 594,\n",
      " {\n",
      " \"abstract\": \"Identifying and responding to events, particularly disasters, is of utmost importance. Mobile data and network connectivity generate many temporally\\u2013and spatially\\u2013stamped data. Mobile data can infer human mobility, proximity, and built environments. Satellite imagery can capture changes to built and natural environments. We propose a method to fuse satellite imagery with privacy\\u2013enhanced mobile data to improve event inference. Our method can be used for small\\u2013scale disaster detection, search and rescue operations, and identifying conflict areas. Our case study demonstrates the effectiveness of the proposed method.\",\n",
      " \"introduction\": \"Rapid identification and response to breaking events, particularly those that pose a threat to human life such as natural disasters or conflicts, is of paramount importance. The effectiveness of response efforts can be significantly impacted by the speed and accuracy of information gathering, analysis, and dissemination, with the potential to save lives and mitigate damage. However, obtaining such information poses significant challenges due to various factors. For example, there have been instances where news agencies faced difficulties in promptly or completely delivering information about ongoing events due to a scarcity of reporters present in the affected areas Stelter and Cohen [2008]. The ubiquity of mobile devices and the near-constant network connectivity that characterizes our modern era have resulted in the generation of a massive amount of temporally- and spatially-stamped data (hereafter called mobile data). These datasets have been the subject of numerous studies aiming to derive individual human mobility patterns for various applications, including quantifying urban vitality Sulis et al. [2018], modeling epidemic spread Alessandretti [2022], and inferring the activities of individuals Liao et al. [2007]. Simultaneously, the increasing number of orbital satellites has facilitated the collection of high-resolution images, capturing snapshots of geographical areas with sub-daily temporal frequency.\",\n",
      " \"literature_review\": \"Historically, capturing changes in the built and natural environment often involved on-the-ground data collection and analysis Haddad [2011]. Surveys and field observations have been the most traditional method to capture such changes, and continue to be so today in many industries. Prior to the advent of modern satellites, aerial photography taken from aircrafts was another common method to document large-scale changes to the built environment Bewley [2003]. While these two methods of data collection are sufficient for many studies, they (a) lack the immediacy called for in urgent real-time scenarios and (b) tend to miss developments in the absence of the proper equipment or manpower. In the last three decades, new methods of data collection in the built and natural environment have become prominent, including the use of satellite images, GPS traces from mobile devices, and user-generated content from social media platforms Jongman et al. [2015], Ji et al. [2018], Yin et al. [2021]. This literature review aims to provide a comprehensive understanding of current and recent research in the fields of mobile data analysis, satellite imagery, and crowdsourcing, highlighting works that have fused any two of them to assist in the inference task.\",\n",
      " \"methodology\": \"Our methodology has two modules\\u2014one that deals with the analysis of mobile data, and another with that of satellite imagery. In both modules, we discuss relevant preprocessing methods as well as analysis steps to aid in the inference task. Figure 2 shows the high-level steps in preprocessing and analysis of both datasets while also outlining key geospatial analysis processes for the three use cases discussed in the Introduction.\\nFigure 2. Methodological flowchart; the top bins describe the order of processes employed in this paper, while the bottom outlines key geospatial analysis metrics to carry out the inference taskMobile Data. For mobile data, the general preprocessing workflow is as follows:\\n(1) Spatial Partitioning\\n(2) Coordinate Reference System (CRS) Projection\\n(3) Data FilteringTo avoid wastefulness in computation, we begin by partitioning the mobile data spatially, which we do by drawing a bounding box using four sets of coordinates. In practice, there are several ways to achieve this at scale: Leveraging a geocoding application protocol interface (API) to obtain the coordinates based on an existing point of interest, partitioning by federal and/or state designations such as census tracts, ZIP codes, or county borders, manually typing in a bounding box (i.e., by defining the geographic coordinates of the corners of the box), or obtaining a KMZ file from the National Oceanic and Atmospheric Administration\\u2019s Damage Assessment Toolkit. For this effort, we used the last option to derive the centroid coordinates of the affected area around which we drew a 1 km2 bounding square.\\nOnce the data is spatially partitioned, we project coordinates to the appropriate CRS. Different CRSs are designed to accurately represent different aspects of the Earth\\u2019s surface. Some are designed to preserve area, while others preserve shape, distance, or direction. In the context of event inference, the optimal choice of CRS should minimize bias in the derived mobility metrics, which may exist due to distortions of distances and areas. Therefore, we use an equal-area projection around UTM Zone 15, which covers the area around our case study.\",\n",
      " \"results\": \"To validate our proposed methodology, we conducted a case study on the EF-1 tornado that hit near Muskogee, Oklahoma on May 15, 2020. This event provided an opportunity to test our approach in a real-world scenario involving a small-scale disaster in a rural region.\\nAfter spatially partitioning the GPS traces to include the subset within the bounding box of the tornado, we projected all data to UTM Zone 15 and filtered the data by the logic described in Section . Then, we defined the two weeks before the day of the event (May 2 - May 15) as the before, the day of the event (May 15) as the during, and the week after the event (May 15 - May 22) as the after periods and began exploring the data. Figure 4 visualizes the time series of two metrics, highlighting the periods by color; there are no noticeable changes in the radius of gyration, while the number of extended stays at a given location within the ROI increases sharply the day after the event.\",\n",
      " \"conclusion\": \"We have presented a novel data fusion methodology combining satellite imagery with GPS traces generated from mobile devices. Our approach leverages the strengths of both data types: mobile data provides an approximation of human mobility, proximity to one another, and the built environment, while satellite imagery offers visual insights into physical changes to the built and natural environment. Furthermore, we have demonstrated the case study of a small-scale tornado, showing the promise of our method in identifying anomalies in movement patterns using mobile data, changes to the built environment using satellite imagery, and inferring the cause of both using our fusion methodology.\\nOur methodology is viable due to the increasing accessibility of commercial data providers specializing in remote sensing and mobile devices. This trend is only expected to continue\\u2014the number of active orbital satellites has grown exponentially since 2017 orb [2022], while the percentage of Americans that own a smartphone has risen to 97% mob [2021]. The continued commercialization of such products will enable researchers and practitioners to monitor, categorize, and understand the natural and built environments in ways unimaginable just 50 years ago. We hope our method is a step in this direction, enabling others to pursue endeavors made possible by the fusion of these data sources.\"\n",
      "}\n",
      "File number 595,\n",
      " {\n",
      " \"abstract\": \"This paper presents a novel diffusion-based framework for animating people in a scene driven by target 3D motion sequences, enabling the generation of realistic and diverse human movements. The approach consists of two core components: a) learning priors about invisible parts of the human body and clothing to generate plausible complete texture maps from single images using in-painting diffusion models, and b) developing a diffusion-based rendering pipeline, controlled by 3D poses, for the synthesis of realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. Experiments show the model's ability to synthesize prolonged motions and handle challenging and complex poses, outperforming previous methods.\",\n",
      " \"introduction\": \"Given a random photo of a person, can we accurately animate that person to imitate someone else's action? This problem requires a deep understanding of how human poses change over time, learning priors about human appearance and clothing. To tackle this, we propose 3DHM, a two-stage framework that synthesizes 3D Human Motions by completing a texture map from a single image and then rendering the 3D humans to imitate the actions of the actor, resulting in synthesized videos that are faithful to the target motion in 3D pose and to the input image in terms of visual similarity. In addition, the 3D control enables the generation of diverse synthetic camera trajectories to render a person.\",\n",
      " \"literature_review\": \"Prior work on controllable human generation and synthesizing moving people has faced challenges in generating realistic human images or videos, often leading to incomplete or non-consequential outputs, and failing to faithfully reconstruct humans. Existing approaches either require large amounts of data, supervised control signals, or careful curation of training data, making it difficult to utilize them directly for animating humans. Some methods attempt to learn pose-to-pixels mapping directly, but this methodology often leads to models becoming overly specialized to certain training data, limiting their generalization capabilities.\",\n",
      " \"methodology\": \"In the first stage of 3DHM, an inpainting diffusion model is utilized to produce a plausible complete texture map by inpainting the unseen regions of the imitator. This model takes a partial texture map and corresponding visibility mask as inputs, and generates a recovered predicted map for the human. In the second stage, a rendering diffusion model is developed to obtain a realistic rendering of a human imitator doing the actions of the actor. This model is applied to intermediate renderings of SMPL meshes, generated by wrapping the predicted texture map from the first stage, to project the body-tight renderings to more realistic images with clothing. The model is conditioned on 3D poses to animate the imitator to copy the actions of the actor.\",\n",
      " \"results\": \"In our experiments, 3DHM outperforms previous approaches on several metrics, including image-based evaluation and video-based evaluation. It achieves superior performance in generating realistic frames, preserving temporal consistency, and generating accurate 3D poses. Qualitative results demonstrate the model's ability to synthesize moving people in various scenarios, including unseen 3D human videos, motions from random YouTube videos, and motions from text inputs, with diverse viewpoints and challenging poses.\",\n",
      " \"conclusion\": \"We present 3DHM, a two-stage diffusion model-based framework for synthesizing moving people from a single image given a target 3D motion sequence. Our approach is resilient in generating prolonged motions and varied challenging and complex poses. It is fully self-supervised, requiring no additional annotations beyond human videos, and is scalable with the availability of more human videos. We believe that 3DHM opens up new possibilities for human animation and provides a solid foundation for future research in this area.\"\n",
      "}\n",
      "File number 597,\n",
      " {\n",
      " \"abstract\": \"RetinaVR, an affordable and immersive virtual reality simulator for vitreoretinal surgery training, is developed and validated in this study. RetinaVR does not require external haptic devices, leveraging the powerful processors, cameras, and sensors of the Meta Quest 2 VR headset. Four core surgical skills were chosen to be simulated:\\u00a0core vitrectomy, peripheral shaving, membrane peeling, and endolaser application. The validation study involved 10 novice and 10 expert ophthalmologists. Construct validity, shown by varying user performance based on experimental runs, age, sex, and expertise, was demonstrated. A learning curve was noted in efficiency, safety, and task-specific performance, providing evidence of improvement with repetition. Specialists had significantly fewer sphere exits during core vitrectomy and better treatment patterns during endolaser application. The authors suggest that such a tool may democratize surgical simulation access and stimulate global ophthalmology innovation. However, further validation is needed to prove skill transfer to the operating room.\",\n",
      " \"introduction\": \"Ophthalmology has utilized virtual reality (VR) simulation in healthcare, improving the surgical performance of novice cataract surgeons and reducing complication rates. However, its use in vitreoretinal surgery training remains under-researched. The authors of this work present RetinaVR, an affordable and immersive VR simulator for vitreoretinal surgery training, hoping to democratize surgical simulation and spur global ophthalmic education. RetinaVR does not require external haptic devices, leveraging the capabilities of the Meta Quest 2 VR headset. RetinaVR was validated using a prospective study involving 10 novice and 10 expert ophthalmologists, with results demonstrating construct and learning curve validity.\",\n",
      " \"literature review\": \"VR simulation in ophthalmology has been shown to enhance cataract surgeon performance, but its role in vitreoretinal surgery training is less clear. An oft-cited VR simulator used in ophthalmology is the EyeSi Surgical Simulator, which comprises a mannequin head, surgical instruments, foot pedals, and a VR interface. EyeSi has been shown to be cost-effective for cataract surgery training but is expensive and requires an annual running cost. In developing nations and under-resourced areas, this poses a significant acquisition barrier. Thus, the study's authors sought to create an alternative which would be affordable and portable, broadening access to surgical simulation.\",\n",
      " \"methodology\": \"The authors developed RetinaVR as a simulation app compatible with commercially available VR headsets. It utilizes the Meta Quest 2, a popular VR headset, for its processing power, cameras, and sensors. Four fundamental vitreoretinal surgery skills were chosen for simulation: core vitrectomy, peripheral shaving, membrane peeling, and endolaser application. Validation involved a prospective study comparing the performance of novice (n=10) and expert (n=10) ophthalmologists recruited from the University of Montreal. Construct validity was measured by analyzing performance based on expertise, age, sex, and experimental run. Additionally, learning curve validity was assessed by analyzing performance changes across three experimental runs.\",\n",
      " \"results\": \"Construct validity was demonstrated, with varying user performance based on experimental runs, age, sex, and expertise. Experts took less time and made fewer errors than novices. In core vitrectomy, experts had significantly fewer sphere exits. Both groups showed a learning curve in efficiency, safety, and task-specific performance, with improvements across experimental runs. In endolaser application, experts exhibited clinically significant differences in treatment patterns compared to novices. No impact of age or sex on performance was observed when controlling for expertise and experimental run.\",\n",
      " \"conclusion\": \"RetinaVR demonstrates construct and learning curve validity, supporting its use as a portable and affordable simulator, especially valuable in resource-limited settings. It could democratize surgical simulation access and promote global ophthalmic education and collaboration. The authors highlight the need for further validation, including skill transfer to the operating room, before widespread use, and also plan to address feedback received during the study to improve RetinaVR.\"\n",
      "}\n",
      "File number 598,\n",
      " {\n",
      " \"abstract\": \"This paper presents a study on enhancing the performance of the GPT Neo 125M model in the programming domain for Community Question Answering (CQA) using a combination of Reinforcement Learning from Human Feedback (RLHF) and scores from Stack Overflow. Two reward model training strategies are employed for fine-tuning with Proximal Policy Optimization (PPO), resulting in improvements comparable to those of GPT Neo's 2.7B parameter variant. An auxiliary scoring mechanism is introduced to highlight the limitations of conventional linguistic metrics in evaluating responses in the programming domain. Through analysis, the study emphasizes the need for domain-specific evaluation methods and reflects on the intricate nature of applying RLHF to programming CQA, accentuates the significance of context-aware evaluation, and underscores the importance of human preferences in refining LLMs.\",\n",
      " \"introduction\": \"Advancements in Reinforcement Learning from Human Feedback (RLHF) have revolutionized the fine-tuning of Large Language Models (LLMs), enabling adaptation for human-like response generation and precise behavior control. While RLHF has been effective in general domains, its application in specialized fields such as Community Question Answering (CQA) for programming remains unexplored. LLMs face unique challenges in handling the complex nature of programming queries, including conceptual understanding, code generation, API usage, and debugging, due to struggles with subtle semantic relations. Additionally, a critical challenge is the evaluation of the quality of responses generated by LLMs. Conventional metrics such as BertScore and Rouge do not capture the essence of responses effectively, especially in specialized domains like programming. Moreover, they don't account for the diversity in valid answers and lack in capturing deeper semantic correctness. The development of more reliable and context-sensitive evaluation metrics is essential.\",\n",
      " \"literature_review\": \"To address these challenges, in this paper, we investigate the application of RLHF to a smaller model, GPT Neo 125M (Black et al., 2021a), in the context of programming CQA. We aim not only to enhance the model's response generation capabilities but also to address the evaluation challenge. Our contributions are two-fold. First, we explore the potential and efficacy of RLHF in retraining a smaller LLM for programming CQA. Second, through empirical analysis, we highlight the discrepancies between the RLHF reward model and existing linguistic metrics, emphasizing the limitations of current evaluation methodologies and advocating for the development of more semantically-sensitive measures.\",\n",
      " \"methodology\": \"The general schema of RLHF utilized in this study consists of several stages, as depicted in Fig. 1. The process commences with the training of an initial policy via supervised learning. Subsequently, a reward model is trained to acquire human preferences from the labeled data. Finally, the policy is fine-tuned using Proximal Policy Optimization (PPO) (Schulman et al., 2017), guided by the reward model. In this study, we have adapted RLHF for programming Q&A by converting user ratings from Stack Overflow into feedback for training our model. We used two distinct approaches: creating regression scores and contrastive scores for the straightforward comparison of answers.\",\n",
      " \"results\": \"This section aims to evaluate the effectiveness of the RLHF training approach for improving the quality of generated responses in the programming QA domain. Specifically, we compare three versions of the model, according to Fig. 1 - the base model, the SFT version, and the RLHF version. The evaluation focuses on the performance of the reward model training methods and the generated responses' quality.\",\n",
      " \"conclusion\": \"In conclusion, our study has demonstrated the effectiveness of RLHF in enhancing the performance of small LLMs like GPT Neo 125M in the programming domain. Our experiments focused on fine-tuning the model using user-generated responses from Stack Overflow, employing two reward model training strategies, regression scores and contrastive scores, with PPO. The study also highlights the critical role of employing the right evaluation measures. While Rouge scores effectively captured response accuracy, other metrics like BertScore and SacreBLEU presented ambiguities, especially when juxtaposed with the results from the trained reward models. This disparity, brought into sharper focus by near-zero Spearman correlations, implies that traditional metrics might not suffice for complex fields such as programming. These domains are marked by intricate semantic relationships and a broad spectrum of valid answers.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for i in range (504,600):\n",
    "    text = url_df['textdata'][i]\n",
    "    if (len(text) <= 60000):\n",
    "        response = model.generate_content(\n",
    "        '''Extract key insights from the given research paper. Please provide four or five sentences each for the following segments: \n",
    "        {Abstract, Introduction, Literature Review, Methodology, Results, and Conclusion} of the research paper. \n",
    "        Ensure that the extracted information is concise and relevant to each segment .Give titles as \n",
    "        {Abstract, Introduction, Literature Review, Methodology, Results, and Conclusion} no other headings are \n",
    "        allowed except the provided one.Ignore any other sections or content not related to the requested segments.Remove figures and tables. \n",
    "        Make sure the extracted contents are structured as a python dictionary of the form [{\"abstract\":\"\",\"introduction\":\"\",\"literature review\":\"\",\"methodology\":\"\",\"results\":\"\",\"conclusion\":\"\"}] strictly and be sure to use double quotes (\"\"). Don't deviate from this dictionary structure'''\n",
    "        + \"Here is the text:\\n\" + f'{text}'\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            dict= json.loads(response.text)\n",
    "            dict['title'] = url_df['title'][i]\n",
    "            dict['author'] = url_df['authors'][i]\n",
    "            dict['textdata'] = url_df['textdata'][i]\n",
    "            data_list.append(dict)\n",
    "\n",
    "        except (requests.exceptions.HTTPError, ValueError) as e:\n",
    "            error_title.append(url_df['title'][i])\n",
    "            print(f\"Error in {url_df['title'][i]}\")\n",
    "\n",
    "        print(f'File number {i},\\n {response.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'abstract': 'Controllable generation of 3D human motions becomes an important topic as the world embraces digital transforma-tion. Existing works, though making promising progress with the advent of diffusion models, heavily rely on meticulously captioned and annotated (e.g., text) high-quality motion corpus, a resource-intensive endeavor in the real world. This motivates our proposed MotionMix, a simple yet effective weakly-supervised diffusion model that leverages both noisy annotated motions and unannotated motions. Specifically, we separate the denoising objectives of a diffusion model into two stages: obtaining conditional rough motion approximations in the initial T ≠ T ≤ ∂ steps by learning the noisy annotated motions, followed by the unconditional refinement of these preliminary motions during the last T ≤ steps using unannotated motions. Notably, though learning from two sources of imperfect data, our model does not compromise motion generation quality compared to fully supervised approaches that access gold data. Extensive experiments on several benchmarks demonstrate that our MotionMix, as a versatile frame-work, consistently achieves state-of-the-art performances on text-to-motion, action-to-motion, and music-to-dance tasks.',\n",
       "  'introduction': 'The rapidly arising attention and interest in digital humans bring up the great demand for human motion generation, in a wide range of fields such as industrial game and movie animation (Ling et al. 2020), human-machine interaction (Koppula and Saxena 2013), VR/AR and metaverse development (Lee et al. 2021). Over the years, automated generation of human motions that align with user preferences, spanning aspects such as prefix poses (Ruiz, Gall, and Moreno-Noguer 2018; Guo et al. 2022c), action classes (Petrovich, Black, and Varol 2021; Cervantes et al. 2022), textual descriptions (Petrovich, Black, and Varol 2022; Ahuja and Morency 2019; Tevet et al. 2022), or music (Li et al. 2020; Aristidou et al. 2021; Siyao et al. 2022; Gong et al. 2023), has been a focal point of research. Recently, building upon the advancement of diffusion models, human motion generation has experienced a notable improvement in quality and controllability. However, these prior diffusion models are commonly trained on well-crafted motions that come with explicit annotations like textual descriptions. While capturing motions from the real world is a laborious effort, annotating these motion sequences further urges the matter.',\n",
       "  'literature review': 'Diffusion models have recently emerged as a powerful class of generative models that have shown promising results in various domains such as image synthesis, text generation, and audio generation. In the context of human motion generation, diffusion models have also been explored, demonstrating their ability to produce high-quality and diverse motions conditioned on various forms of input, such as text, actions, or music. However, most existing diffusion models for human motion generation rely on high-quality motion datasets with detailed annotations, which can be challenging and time-consuming to obtain. To address this limitation, recent research has investigated weakly-supervised diffusion models that can learn from both annotated and unannotated data. For example, Make-An-Animation (Azadi et al. 2023) proposes a diffusion model that can generate human motions from text descriptions using a combination of annotated and unannotated motion data. However, this approach focuses primarily on text-driven motion generation and does not explore other conditioning modalities such as actions or music.',\n",
       "  'methodology': 'Our proposed MotionMix approach effectively addresses the key challenge of training diffusion models with limited annotated data and abundant unannotated data. The methodology is underpinned by the following key aspects:\\n\\n1. **Data Separation:** We strategically divide the training data into two distinct sets: (a) noisy annotated motion sequences and (b) clean unannotated motion sequences. The noisy annotations mimic real-world data where annotations may contain errors or inconsistencies, while the clean unannotated data provides additional information that can guide the generation process.\\n\\n2. **Two-Stage Denoising:** We leverage a diffusion model as the foundation for motion generation. However, instead of using a conventional single-step diffusion process, we propose a novel two-stage denoising strategy. In the first stage, the diffusion model focuses on learning from the noisy annotated data, gradually refining the motion sequences to obtain rough approximations. In the second stage, the model utilizes the clean unannotated data to further denoise and improve the quality of the generated motions.\\n\\n3. **Classifier-Free Guidance:** To enhance the generation process, we employ a classifier-free guidance technique during the denoising steps. This approach involves combining unconditional and conditionally generated samples with a carefully tuned weight, allowing the model to explore both global and conditional aspects of the data distribution. This technique promotes the production of diverse and realistic motion sequences.',\n",
       "  'results': \"The effectiveness of our MotionMix approach is thoroughly evaluated across a diverse range of benchmarks and applications:\\n\\n1. **Text-to-Motion:** Comprehensive experiments on the HumanML3D and KIT-ML datasets demonstrate MotionMix's capability of generating high-quality and diverse motions from textual descriptions. Our approach outperforms several strong baselines, including language-based and diffusion-based methods, achieving state-of-the-art results in terms of motion quality, multimodal consistency, and diversity.\\n\\n2. **Action-to-Motion:** We evaluate MotionMix on the HumanAct12 and UESTC datasets for action-to-motion generation. MotionMix consistently outperforms existing methods, including supervised and unsupervised approaches, across various evaluation metrics. This showcases its ability to generate plausible and diverse motions conditioned on action classes.\\n\\n3. **Music-to-Dance:** Using the AIST++ dataset, we demonstrate that MotionMix excels in music-driven dance generation. Our approach surpasses previous methods in key metrics such as physical foot contact, beat alignment, and diversity. These results highlight MotionMix's ability to capture the rhythmic and expressive qualities of music and generate realistic dance motions.\\n\\n4. **Ablation Studies:** We conduct extensive ablation experiments to analyze the impact of hyperparameters and training strategies on MotionMix's performance. The results provide insights into the effects of the denoising pivot (T ≤), the ratio of noisy to clean data, and the noisy range used to approximate noisy data. These studies validate the robustness of MotionMix and offer guidance for future research.\",\n",
       "  'conclusion': 'MotionMix, our proposed weakly-supervised diffusion model for conditional human motion generation, has demonstrated remarkable results across various tasks and benchmarks. It effectively addresses the challenge of training with limited annotated data by leveraging both noisy annotated and clean unannotated motion sequences. Through extensive experiments and ablation studies, we have comprehensively evaluated MotionMix and provided insights into its effectiveness, versatility, and robustness. We believe that MotionMix opens up new avenues for future research in the realm of data-efficient human motion generation. Moreover, its potential applications span a wide range of domains, including entertainment, gaming, and human-computer interaction, where the generation of realistic and diverse motions is crucial.',\n",
       "  'title': 'MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation',\n",
       "  'author': 'Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi',\n",
       "  'textdata': 'MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation\\nNhat M. Hoang1,2*, Kehong Gong1†, Chuan Guo1*, Michael Bi Mi1\\n1Huawei Technologies Co., Ltd.,\\n2Nanyang Technological University\\nnhat005@e.ntu.edu.sg, gongkehong@u.nus.edu, cguo2@ualberta.ca, michaelbimi@yahoo.com\\nFigure 1: Examples of applying MotionMix on text-to-motion generation. Unlike previous works, our training data are only\\ncomprised of noisy annotated motions and unannotated motions. https://nhathoang2002.github.io/MotionMix-page/\\nAbstract\\nControllable generation of 3D human motions becomes an\\nimportant topic as the world embraces digital transforma-\\ntion. Existing works, though making promising progress with\\nthe advent of diffusion models, heavily rely on meticulously\\ncaptured and annotated (e.g., text) high-quality motion cor-\\npus, a resource-intensive endeavor in the real world. This\\nmotivates our proposed MotionMix, a simple yet effective\\nweakly-supervised diffusion model that leverages both noisy\\n*Work done during an internship at Huawei\\n†Corresponding author\\nCopyright © 2024, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.\\nand unannotated motion sequences. Specifically, we sepa-\\nrate the denoising objectives of a diffusion model into two\\nstages: obtaining conditional rough motion approximations\\nin the initial T − T ∗ steps by learning the noisy annotated\\nmotions, followed by the unconditional refinement of these\\npreliminary motions during the last T ∗ steps using unanno-\\ntated motions. Notably, though learning from two sources of\\nimperfect data, our model does not compromise motion gen-\\neration quality compared to fully supervised approaches that\\naccess gold data. Extensive experiments on several bench-\\nmarks demonstrate that our MotionMix, as a versatile frame-\\nwork, consistently achieves state-of-the-art performances on\\ntext-to-motion, action-to-motion, and music-to-dance tasks.\\narXiv:2401.11115v1  [cs.CV]  20 Jan 2024\\n1\\nIntroduction\\nThe rapidly arising attention and interest in digital humans\\nbring up the great demand for human motion generation, in\\na wide range of fields such as industrial game and movie ani-\\nmation (Ling et al. 2020), human-machine interaction (Kop-\\npula and Saxena 2013), VR/AR and metaverse development\\n(Lee et al. 2021). Over the years, automated generation\\nof human motions that align with user preferences, span-\\nning aspects such as prefix poses (Ruiz, Gall, and Moreno-\\nNoguer 2018; Guo et al. 2022c), action classes (Petrovich,\\nBlack, and Varol 2021; Cervantes et al. 2022), textual de-\\nscriptions (Petrovich, Black, and Varol 2022; Ahuja and\\nMorency 2019; Tevet et al. 2022), or music (Aristidou et al.\\n2021; Siyao et al. 2022; Gong et al. 2023), has been a focal\\npoint of research. Recently, building upon the advancement\\nof diffusion models, human motion generation has experi-\\nenced a notable improvement in quality and controllability.\\nHowever, these prior diffusion models are commonly trained\\non well-crafted motions that come with explicit annotations\\nlike textual descriptions. While capturing motions from the\\nreal world is a laborious effort, annotating these motion se-\\nquences further urges the matter.\\nIn contrast, motions with lower fidelity or fewer anno-\\ntations are more accessible in the real world. For exam-\\nple, 3D human motions are readily extracted from monoc-\\nular videos through video-based pose estimation (Kanazawa\\net al. 2017; Kocabas, Athanasiou, and Black 2019; Choutas\\net al. 2020). Meanwhile, a wealth of unannotated motion se-\\nquences, such as those available from Mixamo (Inc. 2021)\\nand AMASS (Mahmood et al. 2019), remains largely un-\\ntapped. This brings up the question we are investigating in\\nthis work, as illustrated in Figure 1. Can we learn reliable\\ndiffusion models for controllable motion generation based\\non the supervision of noisy and the unannotated motion se-\\nquences?\\nFortunately, with the inherent denoising mechanism of\\ndiffusion models, we are able to answer this question with\\na simple yet effective solution that applies separate diffu-\\nsion steps regarding the source of training motion data, re-\\nferred to as MotionMix. To demonstrate our application and\\napproach, we split each gold annotated motion dataset into\\ntwo halves: the first half of the motions are injected with\\nrandom-scale Gaussian noises (noisy half), and the second\\nhalf is deprived of annotations (clean half). As in Figure 2,\\nthe diffusion model bases on the clean samples for diffusion\\nsteps in [1, T ∗], with condition input erased. Meanwhile,\\nnoisy motions supervise the model with explicit conditions\\nfor the rest of steps [T ∗ + 1, T]. Note T ∗ is an experimen-\\ntal hyper-parameter, with its role analyzed in later ablation\\nstudies. Our key insight is that, during sampling, starting\\nfrom Gaussian noises, the model first produces rough mo-\\ntion approximations with conditional guidance in the initial\\nT −T ∗ steps; afterward, these rough approximations are fur-\\nther refined by unconditional sampling in the last T ∗ steps.\\nYet learning with weak supervision signals, our proposed\\nMotionMix empirically facilitates motion generation with\\nhigher quality than fully supervised models on multiple ap-\\nplications. Benefiting from the conciseness of design, Mo-\\ntionMix finds its place in many applications. In this work,\\nwe thoroughly examine the effectiveness and flexibility of\\nthe proposed approach through extensive experiments on\\nbenchmarks of text-to-motion, music-to-dance, and action-\\nto-motion tasks.\\nThe main contributions of our work can be summarized\\nas follows:\\n• We present MotionMix, the first weakly-supervised ap-\\nproach for conditional diffusion models that utilizes both\\nnoisy annotated and clean unannotated motion sequences\\nsimultaneously.\\n• We demonstrate that by training with these two sources\\nof data simultaneously, MotionMix can improve upon prior\\nstate-of-the-art motion diffusion models across various tasks\\nand benchmarks, without any conflict.\\n• Our approach opens new avenues for addressing the\\nscarcity of clean and annotated motion sequences, paving\\nthe way for scaling up future research by effectively har-\\nnessing available motion resources.\\n2\\nRelated Work\\n2.1\\nWeakly-Supervised Learning\\nTo tackle the limited availability of annotated data, re-\\nsearchers have been exploring the use of semi-supervised\\ngenerative models, using both annotated and unannotated\\ndata (Kingma et al. 2014; Li et al. 2017; Lucic et al.\\n2019). However, the investigation of semi-supervised dif-\\nfusion models remains limited (You et al. 2023), possibly\\ndue to the significant performance gap observed between\\nconditional and unconditional diffusion models (Bao et al.\\n2022; Dhariwal and Nichol 2021; Tevet et al. 2022). More-\\nover, many state-of-the-art models, such as Stable Diffusion\\n(Rombach et al. 2021), implicitly assume the availability of\\nabundant annotated data for training (Chang, Koulieris, and\\nShum 2023; Kawar et al. 2023). This assumption poses a\\nchallenge when acquiring high-quality annotated data is ex-\\npensive, particularly in the case of 3D human motion data.\\nRecent interest has emerged in developing data-efficient\\napproaches for training conditional diffusion models with\\nlow-quality data (Daras et al. 2023; Kawar et al. 2023),\\nor utilizing unsupervised (Tur et al. 2023), semi-supervised\\n(You et al. 2023), self-supervised methods (Miao et al.\\n2023). These approaches have exhibited promising results\\nacross various domains and hold potential for future explo-\\nration of diffusion models when handling limited annotated\\ndata. However, in the domain of human motion generation,\\nefforts toward these approaches have been even more lim-\\nited. One related work, Make-An-Animation (Azadi et al.\\n2023), trains a diffusion model utilizing unannotated mo-\\ntions in a semi-supervised setting. In contrast, our work in-\\ntroduces a unique aspect by training with noisy annotated\\nmotion and clean unannotated motion.\\n2.2\\nConditional Motion Generation\\nOver the years, human motion generation has been exten-\\nsively studied using various signals, including prefix poses\\n(Ruiz, Gall, and Moreno-Noguer 2018; Guo et al. 2022c;\\nPetrovich, Black, and Varol 2021), action classes (Guo et al.\\n2020; Petrovich, Black, and Varol 2021; Cervantes et al.\\nMotionMix\\nClean\\nMotion\\nSupervision\\nSupervision\\nNoisy\\nMotion\\nMotionMix\\nMotionMix\\nStage 1: Conditional Denoising\\nMotionMix\\nMotionMix\\nStage 2: Unconditional Denoising\\nFigure 2: (Left) Training Process. The model is trained with a mixture of noisy and clean data. A noise timestep in ranges of\\n[1, T ∗] and [T ∗ + 1, T] is sampled respectively for each clean and noisy data. Here, T ∗ is a denoising pivot that determines the\\nstarting point from which the diffusion model refines the noisy motion sequences into clean ones without any guidance. (Right)\\nSampling Process. The sampling process consists of two stages. In Stage-1 from timestep T to T ∗ + 1, the model generates\\nthe rough motion approximations, guided by the conditional input c. In Stage-2 from timestep T ∗ to 1, the model refines these\\napproximations to high-quality motion sequences while the input c is masked.\\n2022), textual descriptions (Guo et al. 2022b; Petrovich,\\nBlack, and Varol 2022; Ghosh et al. 2021; Guo et al. 2022a;\\nAhuja and Morency 2019; Bhattacharya et al. 2021), or mu-\\nsic (Li et al. 2020; Aristidou et al. 2021; Li et al. 2021;\\nSiyao et al. 2022; Gong et al. 2023). However, it is non-\\ntrivial for these methods to align the distributions of mo-\\ntion sequences and conditions such as natural languages or\\nspeech (Chen et al. 2022). Diffusion models resolve this\\nproblem using a dedicated multi-step gradual diffuse and\\ndenosing process(Ramesh et al. 2022a; Saharia et al. 2022;\\nHo et al. 2022). Recent advancements, such as MDM (Tevet\\net al. 2022), MotionDiffuse (Zhang et al. 2022), MLD (Chen\\net al. 2022), have demonstrated the ability of diffusion-based\\nmodels to generate plausible human motion, guided by tex-\\ntual descriptions or action classes. In the domain of music,\\nEDGE (Tseng, Castellon, and Liu 2022) showcased high-\\nquality dance generation in diverse music categories. Never-\\ntheless, these works still rely on high-quality motion datasets\\nwith annotated guidance.\\n3\\nMethod\\n3.1\\nProblem Formulation\\nConditional motion generation involves generating high-\\nquality and diverse human motion sequences based on a de-\\nsired conditional input c. This input can take various forms,\\nsuch as a textual description w1:N of N words (Guo et al.\\n2022b), an action class a ∈ A (Guo et al. 2020), music audio\\nm (Li et al. 2021), or even an empty condition c = ∅ (uncon-\\nditional input) (Raab et al. 2022). Our goal is to train a diffu-\\nsion model in a weakly-supervised manner, using both noisy\\nmotion sequences with conditional inputs c = {∅, a, w, c}\\n(where ∅ is used when the classifier-free guidance (Ho and\\nSalimans 2022) is applied) and clean motion sequences with\\nunconditional input c = ∅. Despite being trained with noisy\\nmotions, our model can consistently generate plausible mo-\\ntion sequences. To achieve this, we propose a two-stage re-\\nverse process, as illustrated in Figure 2.\\n3.2\\nDiffusion Probabilistic Model\\nThe general idea of a diffusion model, as defined by the\\ndenoising diffusion probabilistic model (DDPM) (Ho, Jain,\\nand Abbeel 2020), is to design a diffusion process that grad-\\nually adds noise to a data sample and trains a neural model\\nto learn a reverse process of denoising it back to a clean\\nsample. Specifically, the diffusion process can be modeled\\nas a Markov noising process with {xt}T\\nt=0 where x0 ∼ p(x)\\nis the clean sample drawn from the data distribution. The\\nnoised xt is obtained by applying Gaussian noise ϵt to x0\\nthrough the posterior:\\nq(xt|x0) = N(xt; √¯αtx0, (1 − ¯αt)I)\\n(1)\\nwhere ¯αt ∈ (0, 1) are constants which follow a monotoni-\\ncally decreasing scheduler. Thus, when ¯αt is small enough,\\nwe can approximate xT ∼ N(0, I).\\nIn the reverse process, given the condition c, a neural\\nmodel fθ is trained to estimate the clean sample x0 (Ramesh\\net al. 2022b) or the added noise ϵt (Ho, Jain, and Abbeel\\n2020) for all t. The model parameters θ are optimized using\\nthe “simple” objective introduced by Ho, Jain, and Abbeel:\\nLsimple = Et∼[1,T ],st\\nh\\n∥st − fθ(xt, t, c)∥2i\\n(2)\\nwhere the target objective st refers to either x0 or ϵt for ease\\nof notation.\\n3.3\\nTraining\\nWe propose a novel weakly-supervised learning approach\\nthat enables a diffusion model to effectively utilize both\\nnoisy and clean motion sequences. During the training\\nphase, we construct batches comprising both noisy and clean\\nsamples, each coupled with a corresponding guidance condi-\\ntion c, as further detailed in Subsection 3.5. To learn the de-\\nnoising process, we apply the diffusion process to this batch\\nusing Equation 1 with varying noise timesteps. In contrary\\nto the conventional training, where both noisy and clean mo-\\ntion sequences are treated as the ground truth x0 with dif-\\nfusion steps spanning [1, T], our approach adopts separate\\nranges for different data types. For noisy samples, we ran-\\ndomly select noise timesteps t ∈ [T ∗+1, T], while for clean\\nsamples, we confine them to t ∈ [1, T ∗]. Here, T ∗ serves\\nas a denoising pivot, determining when the diffusion model\\nstarts refining noisy motion sequences into cleaner versions.\\nThis pivot is especially crucial in real-world applications,\\nwhere motion capture data might be corrupted by noise due\\nto diverse factors. This denoising strategy for noisy motions\\ndraws inspiration from (Nie et al. 2022), which purified ad-\\nversarial images by diffusing them up to a specific timestep\\nT ∗ before denoising to clean images. The determination of\\nT ∗ typically relies on empirical estimation, its impact on\\ngeneration quality is further analyzed in Table 4.\\nThrough this training process, the model becomes adept\\nat generating initial rough motions from T to T ∗ + 1, and\\nsubsequently refining these rough motions into high-quality\\nones from T ∗ to 1. By dividing into two distinct time ranges,\\nthe model can effectively learn from both noisy and clean\\nmotion sequences as ground truth without any conflict.\\n3.4\\nTwo-stage Sampling and Guidance\\nOur approach introduces a modification to the conventional\\nDDPM sampling procedure, which commonly relies on the\\nsame explicit conditional input c to guide the denoising op-\\neration at each time step t, initiating from T and denois-\\ning back to the subsequent time step t − 1 until reaching\\nt = 0. However, it is important to note that our work specifi-\\ncally focuses on clean, unannotated samples. As discussed in\\nSubsection 3.3, these samples are trained using an identical\\nguidance condition c = ∅ confined within the time interval\\n[1, T ∗]. Consequently, if the conventional DDPM sampling\\nprocess is employed within this temporal range, it could po-\\ntentially lead to jittering or the generation of unrealistic mo-\\ntions. This occurs because the model is not trained to handle\\nvarying conditions within this specific range. To tackle this\\nissue, we adopt a distinct strategy to align the sampling pro-\\ncess accordingly. Specifically, when the model reaches the\\ndenoising pivot T ∗ during the sampling, we substitute the\\nconditional input with c = ∅ starting from T ∗.\\nIn the case of using classifier-free guidance (Ho and Sal-\\nimans 2022), guided inference is employed for all t, which\\ninvolves generating motion samples through a weighted sum\\nof unconditionally and conditionally generated samples:\\nˆs(xt, t, c) = w · fθ(xt, t, c) + (1 − w) · fθ(xt, t, ∅)\\n(3)\\nwhere w is the guidance weight during sampling.\\n3.5\\nData Preparation\\nTo facilitate our setting, we randomly partition an existing\\ntraining dataset into two subsets. In one subset, we retain the\\nannotated condition and introduce noise to the motion se-\\nquences to approximate the real noisy samples. In the other\\nsubset, we reserve the cleanliness of the data and discard the\\nannotated conditions by replacing them as c = ∅.\\nMotivated by the use of Gaussian noises in approximat-\\ning noisy samples in previous works (Tiwari et al. 2022;\\nFiche et al. 2023), we apply the Equation 1 to gradually in-\\ntroduce noise to the clean samples. Since the precise noise\\nschedule in real-world motion capture data is unknown, we\\naddress this uncertainty by applying a random noising step\\nsampled from the range [T1, T2], where T1 and T2 are hyper-\\nparameters simulating the level of disruption in real noisy\\nmotions. Interestingly, our experiments (Tab. 6) show that\\nneither smaller value of T1, T2 nor small T2 − T1 relates\\nto better final performance. Due to page limit, examples of\\nnoisy motions for training are delegated to present in sup-\\nplementary videos.\\nIt is worth noting that the processes of dividing the train-\\ning dataset and preparing noisy samples, and unannotated\\nsamples only take place on the side of the training dataset.\\nThe remaining evaluation dataset, diffusion models, and\\ntraining process are kept unchanged as in previous works.\\n4\\nExperiments\\nWe thoroughly experiment our MotionMix in diverse tasks\\nusing different conditional motion generation diffusion\\nmodels as backbones: (1) MDM (Tevet et al. 2022) for\\ntext-to-motion task on HumanML3D (Guo et al. 2022b),\\nKIT-ML (Plappert, Mandery, and Asfour 2016), as well as\\naction-to-motion task on HumanAct12 (Guo et al. 2020) and\\nUESTC (Ji et al. 2018); (2) MotionDiffuse (Zhang et al.\\n2022) for text-to-motion task; and (3) EDGE (Tseng, Castel-\\nlon, and Liu 2022) for music-to-dance task on AIST++ (Li\\net al. 2021).\\n4.1\\nModels\\n• MDM (Tevet et al. 2022) MDM is a lightweight diffusion\\nmodel that utilizes a transformer encoder-only architecture\\n(Vaswani et al. 2017). Its training objective is to estimate\\nthe clean sample x0 (Ramesh et al. 2022b). In the text-to-\\nmotion task, MDM encodes the text description c = w1:N\\nusing a frozen CLIP-VIT-B/32. During training, classifier-\\nfree guidance (Ho and Salimans 2022) is employed by ran-\\ndomly masking the condition with c = ∅ with a probabil-\\nity of 10%. Meanwhile, in the action-to-motion task, the\\nconditioning c = a is projected to a linear action embed-\\nding, and the classifier-free guidance is not applied. Addi-\\ntionally, three geometric losses are incorporated as training\\nconstraints for this task.\\n• MotionDiffuse (Zhang et al. 2022) MotionDiffuse em-\\nploys a series of transformer decoder layers (Vaswani et al.\\n2017) and incorporates a frozen CLIP-VIT-B/32 for text de-\\nscription encoding. However, in contrast to MDM, Motion-\\nDiffuse focuses on estimating the noise ϵ as its training ob-\\njective and does not incorporate the classifier-free guidance\\n(Ho and Salimans 2022).\\n• EDGE (Tseng, Castellon, and Liu 2022). EDGE shares\\nsimilarities with MDM in terms of its transformer encoder-\\nonly architecture (Vaswani et al. 2017) and the adoption of\\ngeometric losses for the music-to-dance task. In addition, the\\nauthors introduced a novel Contact Consistency Loss to en-\\nhance foot contact prediction control. In the case of music\\nconditioning, EDGE utilizes a pre-trained Jukebox model\\n(Dhariwal et al. 2020) to extract audio features m from mu-\\nsic, which then serve as conditioning input c = m. During\\ninference, the approach incorporates classifier-free guidance\\n(Ho and Salimans 2022) with a masking probability of 25%.\\n4.2\\nText-to-motion\\n• Datasets. Two leading benchmarks used for text-driven\\nmotion generation are HumanML3D (Guo et al. 2022b) and\\nKIT-ML (Plappert, Mandery, and Asfour 2016). The KIT-\\nML dataset provides 6,353 textual descriptions correspond-\\ning to 3,911 motion sequences, while the HumanML3D\\ndataset combines 14,616 motion sequences from Human-\\nAct12 (Guo et al. 2020) and AMASS (Mahmood et al.\\n2019), along with 44,970 sequence-level textual descrip-\\ntions. As suggested by Guo et al., we adopt a redundant\\nmotion representation that concatenates root velocities, root\\nheight, local joint positions, velocities, rotations, and the\\nbinary labels of foot contact. This representation, denoted\\nas x ∈ RN×D, is used for both HumanML3D and KIT-\\nML, with D being the dimension of the pose vector and is\\nequal to 263 for HumanML3D or 251 for KIT-ML. This mo-\\ntion representation is also employed in previous work (Tevet\\net al. 2022; Zhang et al. 2022; Chen et al. 2022).\\n• Implementation Details. On both datasets, we train the\\nMDM and MotionDiffuse models from scratch for 700K\\nand 200K steps, respectively. To approximate the noisy mo-\\ntion data ˜x from x ∈ RN×D, we use noisy ranges [20, 60]\\nand [20, 40] for HumanML3D and KIT-ML, respectively.\\n• Evaluation Metrics. As suggested by Guo et al., the met-\\nrics are based on a text feature extractor and a motion fea-\\nture extractor jointly trained under contrastive loss to pro-\\nduce feature vectors for matched text-motion pairs. R Pre-\\ncision (top 3) measures the accuracy of the top 3 retrieved\\ndescriptions for each generated motion, while the Frechet\\nInception Distance (FID) is calculated using the motion ex-\\ntractor as the evaluator network. Multimodal Distance mea-\\nsures the average Euclidean distance between the motion\\nfeature of each generated motion and the text feature of its\\ncorresponding description in the test set. Diversity measures\\nthe variance of the generated motions across all action cate-\\ngories, while MultiModality measures the diversity of gen-\\nerated motions within each condition.\\n• Quantitative Result. Table 1 presents quantitative results\\nof our weakly-supervised MotionMix using MDM and Mo-\\ntionDiffuse backbones, in comparison with their original\\nmodels that are trained with fully annotated and clean mo-\\ntion sequences. To our surprise, in most settings, Motion-\\nMix even improves the motion quality (i.e., FID) and mul-\\ntimodal consistency (i.e., R Precision) upon the fully super-\\nvised backbones. For example, on HumanML3D and KIT-\\nML dataset, MDM (MotionMix) commonly reduces FID by\\nover 0.16 compare to MDM; this comes with the enhance-\\nment of both R Precision and Multimodal Distance. We may\\nattribute this to the better generalizability and robustness by\\ninvolving noisy data in our MotionMix. On the specifical\\nsetting of MotionDiffuse (MotionMix) on HumanML3D,\\nthough being inferior to the original MotionDiffuse, our\\nMotionMix maintains competitive performance on par with\\nother fully supervised baselines, such as Language2Pose\\n(Ahuja and Morency 2019), Text2Gestures (Bhattacharya\\net al. 2021), Guo et al. (Guo et al. 2022b).\\n4.3\\nAction-to-motion\\n• Datasets. We evaluate our MotionMix on two bench-\\nmarks: HumanAct12 (Guo et al. 2020) and UESTC (Ji\\net al. 2018). HumanAct12 offers 1,191 motion clips cate-\\ngorized into 12 action classes, while UESTC provides 24K\\nsequences of 40 action classes. For this task, we use the\\npre-processed sequences provided by Petrovich, Black, and\\nVarol as the gold clean motion sequences, and further pro-\\ncess them to approximate noisy samples. A pose sequence of\\nN frames is represented in the 24-joint SMPL format (Loper\\net al. 2015), using the 6D rotation (Zhou et al. 2018) for ev-\\nery joint, resulting in p ∈ RN×24×6. A single root trans-\\nlation r ∈ RN×1×3 is padded and concatenated with p to\\nobtain the final motion representation x = Concat([p, r]) ∈\\nRN×25×6.\\n• Implementation Details. Following the experimental\\nsetup by Tevet et al., we train the MDM (MotionMix) from\\nscratch on the HumanAct12 and UESTC datasets for 750K\\nand 2M steps, respectively. In our approximation prepro-\\ncess, we determine the amount of noise to be injected into\\nboth the pose sequence p and the root translation r by ran-\\ndomly sampling from range [10, 30]. The resulting ˜p and ˜r\\nare then concatenated to obtain noisy motion ˜x.\\n• Evaluation Metrics. Four metrics are used to assess the\\nquality of generated motions. The FID is commonly used to\\nevaluates the overall quality of generated motions. Accuracy\\nmeasures the correlation between the generated motion and\\nits action class. Diversity and MultiModality are similar to\\nthe text-to-motion metrics.\\n• Quantitative Result. Table 2 presents the performance\\noutcomes of MDM (MotionMix) and several baseline mod-\\nels, including Action2Motion (Guo et al. 2020), ACTOR\\n(Petrovich, Black, and Varol 2021), INR (Cervantes et al.\\n2022), MLD (Chen et al. 2022), and MDM (Tevet et al.\\n2022), on both the HumanAct12 and UESTC datasets. Fol-\\nlowing the methodology of Tevet et al., we perform 20 eval-\\nuations, each comprising 1000 samples, and present average\\nscores with a confidence interval of 95%. The results high-\\nlight that our MotionMix achieves competitive performance\\nwith significantly fewer high-quality annotated data in-\\nstances. In particular, the improvement seen on the UESTC\\ndataset underscores its efficacy in training with noisy mo-\\ntion data from the real-world scenario. On the other hand,\\nthe deterioration in performance on HumanAct12 suggests\\nthat our approach is better suited for larger datasets, given\\nthat the size of HumanAct12 is remarkably smaller than that\\nof UESTC. Nevertheless, our supplementary videos demon-\\nstrate that the model trained on HumanAct12 remains ca-\\npable of generating quality motion sequences based on the\\nprovided action classes.\\n4.4\\nMusic-to-dance\\n• Datasets. We utilize the AIST++ dataset (Li et al. 2021),\\nwhich comprises 1,408 high-quality dance motions accom-\\npanied by music from a diverse range of genres. Following\\nthe experimental setup proposed by Tseng, Castellon, and\\nLiu, we adopt a configuration in which all training sam-\\nples are trimmed to 5 seconds and 30 FPS. Similarly to\\nReal\\nMDM\\nMDM(MotionMix)\\nMotionDiffuse\\nMotionDiffuse (MotionMix)\\n“a man mimics a throwing motion with his left hand.”\\n“person appears to be holding some thing with both hands and then throws it forward with their right hand.”\\n“the person climbs up something for few steps. ”\\nFigure 3: Qualitative performance of baseline MDM and MotionDiffuse models, trained exclusively on high-quality annotated\\ndata, with our MotionMix approach, which learns from imperfect data sources. Their visualized motion results are presented\\nalongside real references for three distinct text prompts. Please refer to supplementary files for more animations.\\nthe action-to-motion data, we concatenate N-frame pose se-\\nquences denoted as p ∈ RN×24×6=N×144, along with a sin-\\ngle root translation denoted as r ∈ RN×3, and an additional\\nbinary contact label for the heel and toe of each foot de-\\nnoted as b ∈ {0, 1}N×4. Consequently, EDGE is trained us-\\ning the final motion representation x = Concat([b, r, p]) ∈\\nRN×151.\\n• Implementation Details. Similar to the action-to-motion\\ntask, we inject noise into both p and r using the same noise\\ntimestep sampled from [20, 80]. Since the contact label b is\\nobtained from both p and r, it is not necessary to inject noise\\ninto b. Following the setup of Tseng, Castellon, and Liu, we\\ntrain both the EDGE model and our EDGE (MotionMix)\\nfrom scratch on AIST++ for 2000 epochs.\\n• Evaluation Metrics. To evaluate the quality of the gener-\\nated dance, we adopt the same evaluation settings as sug-\\ngested in paper EDGE, including Physical Foot Contact\\n(PFC), Beat Alignment, and Diversity. PFC is a physically-\\ninspired metric that evaluates physical plausibility by cap-\\nturing realistic foot-ground contact without explicit physical\\nmodeling or assuming static contact. Following the previous\\nworks (Li et al. 2021; Siyao et al. 2022), Beat Alignment\\nevaluates the tendency of generated dances to follow the beat\\nof the music, while Diversity measures the distribution of\\ngenerated dances in the “kinetic” (Distk) and “geometric”\\n(Distg) feature spaces.\\n• Quantitative Result. In contrary to prior works, which\\ntypically reported only a single evaluation result, we have\\nobserved that the metrics can be inconsistent. Thus, to offer a\\nmore comprehensive evaluation, we present the average and\\n95% confidence interval, derived from 20 evaluation runs\\nfor our retrained EDGE model and our EDGE (MotionMix)\\nvariant. For Bailando (Siyao et al. 2022) and FACT (Li et al.\\n2021), we directly fetched results from the paper EDGE\\n(Tseng, Castellon, and Liu 2022). The results in Table 3\\nvividly demonstrate that, our EDGE (MotionMix) signif-\\nicantly outperforms the baseline across all metrics, show-\\ncasing improvements of up to 43.1% in PFC and 95.0% in\\nDistk. This further reinforces the generalizability prowess of\\nour MotionMix approach, consistent with the outcomes ob-\\nserved in our text-to-motion experiments.\\n5\\nAblation Studies\\nMotionMix is introduced as a potential solution that enables\\nthe diffusion model to effectively leverage both noisy mo-\\ntion sequences and unannotated data. To demonstrate the\\nefficacy of this approach, we approximate noisy samples\\nfrom existing datasets and train the model on them, which\\nincorporate several essential hyperparameters: (1) the de-\\nnoising pivot T ∗; (2) the ratio of noisy and clean data for\\ntraining; (3) the noisy range [T1, T2] to approximate noisy\\ndata. In this section, we thoroughly assess the impact of\\neach hyperparameters within MotionMix. All ablation ex-\\nperiments are carried out on the HumanML3D dataset us-\\nMethod\\nR Precision\\n(top 3)↑\\nFID↓\\nMultimodal\\nDist.↓\\nDiversity→\\nMultimodality↑\\nHumanML3D\\nReal Motion\\n0.797±.002\\n0.002±.000\\n2.974±.008\\n9.503±.065\\n-\\nLanguage2Pose\\n0.486±.002\\n11.02±.046\\n5.296±.008\\n7.676±.058\\n-\\nText2Gestures\\n0.345±.002\\n7.664±.030\\n6.030±.008\\n6.409±.071\\n-\\nGuo et al.\\n0.740±.003\\n1.067±.002\\n3.340±.008\\n9.188±.002\\n2.090±.083\\nMLD\\n0.772±.002\\n0.473±.013\\n3.196±.010\\n9.724±.082\\n2.413±.079\\nMDM\\n0.611±.007\\n0.544±.440\\n5.566±.027\\n9.559±.860\\n2.799±.072\\nMDM (MotionMix)\\n0.632±.006 (↑3.4%)\\n0.381±.042 (↑30.0%)\\n5.325±.026 (↑4.3%)\\n9.520±.090 (↑69.6%)\\n2.718±.019 (↓2.9%)\\nMotionDiffuse\\n0.782±.001\\n0.630±.001\\n3.113±.001\\n9.410±.049\\n1.553±.042\\nMotionDiffuse (MotionMix)\\n0.738±.006 (↓5.6%)\\n1.021±.071 (↓62.1%)\\n3.310±.020 (↓6.3%)\\n9.297±.083 (↓121.5%)\\n1.523±.153 (↓1.9%)\\nKIT-ML\\nReal Motion\\n0.779±.006\\n0.031±.004\\n2.788±.012\\n11.080±.097\\n-\\nLanguage2Pose\\n0.483±.005\\n6.545±.072\\n5.147±.030\\n9.073±.100\\n-\\nText2Gestures\\n0.338±.004\\n12.12±.183\\n6.964±.029\\n9.334±.079\\n-\\nGuo et al.\\n0.693±.007\\n2.770±.109\\n3.401±.008\\n10.910±.119\\n1.482±.065\\nMLD\\n0.734±.007\\n0.404±.027\\n3.204±.027\\n10.800±.117\\n2.192±.071\\nMDM\\n0.396±.004\\n0.497±.021\\n9.191±.022\\n10.847±.109\\n1.907±.214\\nMDM (MotionMix)\\n0.404±.005 (↑2.0%)\\n0.322±.020 (↑35.2%)\\n9.068±.019 (↑1.3%)\\n10.781±.098 (↓28.3%)\\n1.946±.019 (↑2.0%)\\nMotionDiffuse\\n0.739±.004\\n1.954±.062\\n2.958±.005\\n11.100±.143\\n0.730±.013\\nMotionDiffuse (MotionMix)\\n0.742±.005 (↑0.4%)\\n1.192±.073 (↑39.0%)\\n3.066±.018 (↓3.6%)\\n10.998±.072 (↓310%)\\n1.391±.111 (↑90.5%)\\nTable 1: Quantitative results of text-to-motion on the test set of HumanML3D and KIT-ML. Note all baselines are trained with\\ngold data. We run all the evaluation 20 times (except Multimodality runs 5 times) and ± indicates the 95% confidence interval. ↑\\nmeans higher is better, ↓ means lower is better, → means closer to the real distribution is better. The ↑ x% and ↓ x% indicate the\\npercentage difference in performance improvement or deterioration when comparing our approach to its correspond baseline.\\nMethod\\nFID ↓\\nAccuracy ↑\\nDiversity →\\nMultiModality →\\nHumanAct12\\nReal Motion\\n0.053±.003\\n0.995±.001\\n6.835±.045\\n2.604±.040\\nAction2Motion\\n0.338±.015\\n0.917±.001\\n6.850±.050\\n2.511±.023\\nACTOR\\n0.120±.000\\n0.955±.008\\n6.840±.030\\n2.530±.020\\nINR\\n0.088±.004\\n0.973±.001\\n6.881±.048\\n2.569±.040\\nMLD\\n0.077±.004\\n0.964±.002\\n6.831±.050\\n2.824±.038\\nMDM\\n0.100±.000\\n0.990±.000\\n6.860±.050\\n2.520±.010\\nMDM (MotionMix)\\n0.196±.007 (↓96%)\\n0.930±.003 (↓6.1%)\\n6.836±.062 (↑96%)\\n3.043±.054 (↓422.6%)\\nUESTC\\nReal Motion\\n2.790±.290\\n0.988±.001\\n33.349±.320\\n14.160±.060\\nACTOR\\n23.430±2.200\\n0.911±.003\\n31.960±.330\\n14.520±.090\\nINR\\n15.000±.090\\n0.941±.001\\n31.590±.190\\n14.680±.070\\nMLD\\n15.790±.079\\n0.954±.001\\n33.520±.140\\n13.570±.060\\nMDM\\n12.810±1.460\\n0.950±.000\\n33.100±.290\\n14.260±.120\\nMDM (MotionMix)\\n11.400±.393 (↑11%)\\n0.960±.003 (↑1.1%)\\n32.806±.176 (↓118%)\\n14.277±.094 (↓17%)\\nTable 2: Quantitative results of action-to-motion on the Hu-\\nmanAct12 dataset and UESTC test set. We run the evalua-\\ntion 20 times, and the metric details are similar to Table 1.\\nMethod\\nPFC ↓\\nBeat Align. ↑\\nDistk →\\nDistg →\\nReal Motion\\n1.380\\n0.314\\n9.545\\n7.766\\nBailando\\n1.754\\n0.23\\n10.58\\n7.72\\nFACT\\n2.2543\\n0.22\\n10.85\\n6.14\\nEDGE†\\n1.605±.224\\n0.224±.025\\n5.549±.783\\n4.831±.752\\nEDGE (MotionMix)\\n1.988±.120 (↑43.1%)\\n0.256±.013 (↑13.3%)\\n10.103±2.039 (↑95.0%)\\n6.595±.173 (↑15.1%)\\nTable 3: Quantitative results of music-to-dance on the\\nAIST++ test set. We run the evaluation 20 times, and the\\nmetric details are similar to Table 1. † denotes the EDGE\\nmodel that is re-trained by us.1\\ning the MDM model with the identical settings described in\\nSubsection 4.2.\\n1The results of the baseline EDGE model is different from the\\nones submitted to AAAI’24. Nonetheless, our EDGE (MotionMix)\\nstill achieves overall better performance.\\n5.1\\nEffect of The Denoising Pivot T ∗\\nWe begin our ablation studies by examining the impact of\\nthe denoising pivot T ∗. To evaluate its impact, we conduct\\nexperiments with a fixed noisy range of [T1, T2] = [20, 60],\\na noisy ratio of 50%, and evaluate various T ∗ values, encom-\\npassing 20, 40, 60, and 80. The results, detailed in Table 4,\\nreveal a notable observation: a roughly estimated denois-\\ning pivot is sufficient for real-world scenarios, as evidenced\\nby the competitive outcomes across various T ∗ values. This\\nrobustness underlines the versatility of our MotionMix ap-\\nproach. Additionally, selecting a very small denoising pivot\\n(e.g., T ∗ = 0 or 20) enables conditions to steer the model\\ntoward diverse rough motion sequences before the refining\\nphase, as reflected in the MModality score trend. However,\\nthis small value may potentially compromise motion qual-\\nity, leading to subpar results in other metrics. In contrast, the\\nchoice of T ∗ = 60, which is well aligned with our prede-\\nfined noisy range, yields superior results in multiple eval-\\nuation metrics. This sheds light on the need of tuning the\\ndenoising pivot to optimize the results, as this hyperparam-\\neter determines the starting point for the diffusion model to\\ntransform initial noisy motion into high-quality sequences.\\n5.2\\nEffect of Noisy/Clean Data Ratio\\nIn this ablation study, we evaluate how the noisy/clean\\ndata ratio affects our approach by keeping T ∗ = 60 and\\n[T1, T2] = [20, 60] constant. We experiment with various\\nnoisy ratios of 30%, 50%, and 70%. The results, presented\\nin Table 5, show interesting trends across the evaluation met-\\nrics. Notably, higher noisy ratios (i.e., 50% and 70%) con-\\nMethod\\nR Precision\\n(top 3)↑\\nFID↓\\nMultimodal\\nDist.↓\\nDiversity→\\nMultimodality↑\\nReal Motion\\n0.797±.002\\n0.002±.000\\n2.974±.008\\n9.503±.065\\n-\\nMDM (Tevet et al. 2022)\\n0.611±.007\\n0.544±.440\\n5.566±.027\\n9.559±.860\\n2.799±.072\\n50% noisy, T1=20, T2=60\\nMDM (MotionMix) (T ∗=0)\\n0.598±.006\\n0.714±.045\\n5.503±.036\\n9.750±.123\\n3.044±.054\\nMDM (MotionMix) (T ∗=20)\\n0.601±.005\\n0.497±.048\\n5.562±.026\\n9.414±.092\\n2.935±.059\\nMDM (MotionMix) (T ∗=40)\\n0.604±.008\\n0.402±.032\\n5.524±.033\\n9.396±.094\\n2.747±.070\\nMDM (MotionMix) (T ∗=60)\\n0.632±.006\\n0.381±.042 5.325±.026\\n9.520±.090\\n2.718±.019\\nMDM (MotionMix) (T ∗=80)\\n0.594±.005\\n0.589±.059\\n5.670±.033\\n9.242±.086\\n2.602±.057\\nTable 4: We evaluate MDM (MotionMix) on the Hu-\\nmanML3D test set using different values of the denoising\\npivot T ∗. The metrics are calculated in the same manner as\\ndetailed in Table 1. The best and the second best result are\\nbold and underlined respectively.\\nMethod\\nR Precision\\n(top 3)↑\\nFID↓\\nMultimodal\\nDist.↓\\nDiversity→\\nMultimodality↑\\nReal Motion\\n0.797±.002\\n0.002±.000\\n2.974±.008\\n9.503±.065\\n-\\nMDM\\n0.611±.007\\n0.544±.440\\n5.566±.027\\n9.559±.860\\n2.799±.072\\nT1=20, T2=60, T ∗=60\\nMDM (MotionMix) (30% noisy)\\n0.601±.007\\n0.898±.045\\n5.581±.030\\n9.080±.092\\n2.856±.074\\nMDM (MotionMix) (50% noisy)\\n0.632±.006\\n0.381±.042\\n5.325±.026\\n9.520±.090\\n2.718±.019\\nMDM (MotionMix) (70% noisy)\\n0.615±.006\\n0.359±.030 5.545±.031\\n9.457±.098\\n2.867±.107\\nTable 5: We evaluate MDM (MotionMix) on the Hu-\\nmanML3D test set using different ratios for noisy and clean\\ndata. The metrics are calculated in the same manner as de-\\ntailed in Table 1. The best and the second best result are bold\\nand underlined respectively.\\nsistently outperform the lower ratio (i.e., 30%). Note that,\\na higher noisy ratio allows the model to access a wider\\nrange of annotated text conditions, yielding better R Preci-\\nsion and Multimodal Distance. On the other hand, the 30%\\nratio, despite being trained with a greater amount of clean\\ndata, exhibits suboptimal motion quality (scoring 0.898 in\\nFID) in comparison to other supervised baselines in Table 1,\\nsuch as Language2Pose (FID of 11.02), Text2Gestures (FID\\nof 7.664), Guo et al. (FID of 1.067). Nevertheless, it still\\nachieves results on par with the supervised MDM baseline in\\nterms of multimodal consistency (i.e. Multimodal Distance).\\nThese observations underscore the resilience of our Motion-\\nMix approach to variations in the noisy/clean data ratio.\\n5.3\\nEffect of The Noisy Range\\nThe purpose of the noisy range in our work is to approxi-\\nmate the noise schedule found in real-world motion capture\\ndata. Thus, for different datasets in Section 4, we choose\\nnoisy ranges based on the visualization of motion from each\\ndataset. For example, UESTC (Ji et al. 2018) contains noisy\\nmocap data, while HumanML3D (Guo et al. 2022b), de-\\nrived from AMASS (Mahmood et al. 2019), consists of\\nclean motion sequences. This ablation, therefore, compre-\\nhensively evaluates the effectiveness of our MotionMix ap-\\nproach when handling different noisy levels of motion se-\\nquences. We categorize the evaluations into two groups: nar-\\nrow/wide ranges of noise and low/high schedules of noise.\\nAll experiments are conducted with a noisy ratio of 50%,\\nand the denoising pivot T ∗ is equal to the chosen T2. The\\nresults are presented in Table 6.\\n•\\nNarrow/Wide\\nNoisy\\nRange.\\nThree\\nnoisy\\nranges\\n[T1, T2] ∈ {[20, 40], [20, 60], [20, 80]} are set to analyze the\\nMethod\\nR Precision\\n(top 3)↑\\nFID↓\\nMultimodal\\nDist.↓\\nDiversity→\\nMultimodality↑\\nReal Motion\\n0.797±.002\\n0.002±.000\\n2.974±.008\\n9.503±.065\\n-\\nMDM\\n0.611±.007\\n0.544±.440\\n5.566±.027\\n9.559±.860\\n2.799±.072\\n50% noisy, T ∗ = T2\\nMDM (MotionMix) (T1=20, T2=40)\\n0.616±.006\\n0.451±.033\\n5.459±.027\\n9.585±.101\\n2.585±.076\\nMDM (MotionMix) (T1=20, T2=60)\\n0.632±.006\\n0.381±.042 5.325±.026\\n9.520±.090\\n2.718±.019\\nMDM (MotionMix) (T1=20, T2=80)\\n0.604±.004\\n0.614±.060\\n5.540±.024\\n9.554±.104\\n2.768±.095\\n50% noisy, T ∗ = T2\\nMDM (MotionMix) (T1=10, T2=30)\\n0.592±.008\\n0.713±.048\\n5.633±.028\\n9.567±.109\\n2.783±.139\\nMDM (MotionMix) (T1=20, T2=40)\\n0.616±.006\\n0.451±.033\\n5.459±.027\\n9.585±.101\\n2.585±.076\\nMDM (MotionMix) (T1=40, T2=60)\\n0.598±.004\\n0.554±.076\\n5.600±.031\\n9.479±.100\\n2.815±.094\\nMDM (MotionMix) (T1=60, T2=80)\\n0.597±.008\\n0.437±.039 5.554±.033\\n9.452±.092\\n2.895±.079\\nTable 6: We evaluate MDM (MotionMix) on the Hu-\\nmanML3D test set using different noisy ranges [T1, T2] to\\napproximate the noisy motion sequences. The table presents\\ntwo distinct scenarios: the upper block ablates how much the\\nrange spans, while the lower block examines the impact of\\nthe corruption level of noisy motions. The metrics are cal-\\nculated in the same manner as detailed in Table 1. For each\\nsetting, the best and the second best result are bold and un-\\nderlined respectively.\\neffect of how much the range spans. Counterintuitively, the\\nsmaller noisy range does not equal to the better performance.\\nFor example, noisy ranging from 20 to 60 time steps leads\\nto overall the best performance, compared to range [20, 40].\\nThough, large noisy range (i.e., [20, 80]) unevitably deterio-\\ntate the model capacity.\\n•\\nLow/High\\nNoisy\\nSchedule.\\nFour\\ncontrast\\nranges\\n[T1, T2] ∈ {[10, 30], [20, 40], [40, 60], [60, 80]} are experi-\\nmented to evaluate the robustness of MotionMix regarding\\ncorruption level of noisy motions. Notably, our proposed\\nMotionMix performs reasonably stable on different levels\\nof corrupted motions. More visual animations are also pro-\\nvided in our supplementary videos.\\n6\\nConclusion\\nIn this work, we look into the realm of conditional hu-\\nman motion generation, devling into the challenge of train-\\ning with both noisy annotated and clean unannotated mo-\\ntion sequences. The proposed approach, MotionMix, pio-\\nneers the utilization of a weakly-supervised diffusion model\\nas a potential solution for this challenge. This innovative\\nmethod effectively overcomes the constraints arising from\\nlimited high-quality annotated data, achieving competitive\\nresults compared to fully supervised models. The versatility\\nof MotionMix is showcased across multiple motion genera-\\ntion benchmarks and fundamental diffusion model designs.\\nComprehensive ablation studies further bolster its resilience\\nin diverse noisy schedules and the strategic selection of the\\ndenoising pivot.\\nA\\nApplication - Real Case Scenario\\nWe experimented training the EDGE model using both\\nAIST++ and AMASS together. With AMASS (low PFC),\\nour model can generate plausible motion with less skating\\n(PFC: 1.06, Tab. 7), visually supported by videos on our\\nproject page.\\nMethod\\nPFC ↓\\nBeat Align. ↑\\nDistk →\\nDistg →\\nReal Motion (AIST++)\\n1.380\\n0.314\\n9.545\\n7.766\\nReal Motion (AMASS)\\n1.032\\n-\\n-\\n-\\nEDGE†\\n1.605±.224\\n0.224±.025\\n5.549±.783\\n4.831±.752\\nHalf noisy AIST++ and half clean AIST++ (in our main paper)\\nEDGE (MotionMix)\\n1.988±.120\\n0.256±.013\\n10.103±2.039\\n6.595±.173\\nCombine clean AIST++ and clean AMASS\\nEDGE (MotionMix) (T ∗=20)\\n1.310±.078\\n0.236±.007\\n3.437±.229\\n4.308±.134\\nEDGE (MotionMix) (T ∗=40)\\n1.062±.080\\n0.240±.009\\n3.639±.292\\n4.371±.111\\nTable 7: Quantitative results of music-to-dance on the AIST++ test set. We run the evaluation 20 times. The best and the second\\nbest result are bold and underlined respectively. † denotes the EDGE model that is re-trained by us; note that its result is different\\nfrom our main paper submitted to AAAI due to a multi-gpu bug we faced,\\nB\\nApplication - Motion Editing\\nMDM (Tevet et al. 2022) introduced two motion editing ap-\\nplications: in-betweening and body part editing. These ap-\\nplications share the same approach, respectively, in the tem-\\nporal and spatial domains. For in-betweening, they main-\\ntained the initial and final 25% of the motion sequence as\\nfixed, while the model generated the intermediate 50%. In\\nthe context of body part editing, specific joints were held\\nfixed, leaving the model responsible for generating the re-\\nmaining segments. In particular, their experimentation fo-\\ncused on editing the upper body joints exclusively. In our\\nsupplementary videos, we demonstrate that, in both scenar-\\nios, our MDM (MotionMix) does not compromise this use-\\nful feature, exhibiting the ability to produce coherence mo-\\ntion sequences that align with both the motion’s fixed section\\nand the given condition (if provided).\\nReferences\\nAhuja, C.; and Morency, L.-P. 2019. Language2Pose: Nat-\\nural Language Grounded Pose Forecasting. 2019 Interna-\\ntional Conference on 3D Vision (3DV), 719–728.\\nAristidou, A.; Yiannakidis, A.; Aberman, K.; Cohen-Or,\\nD.; Shamir, A.; and Chrysanthou, Y. 2021.\\nRhythm is a\\nDancer: Music-Driven Motion Synthesis with Global Struc-\\nture.\\nIEEE transactions on visualization and computer\\ngraphics, PP.\\nAzadi, S.; Shah, A.; Hayes, T.; Parikh, D.; and Gupta, S.\\n2023.\\nMake-An-Animation: Large-Scale Text-conditional\\n3D Human Motion Generation. ArXiv, abs/2305.09662.\\nBao, F.; Li, C.; Sun, J.; and Zhu, J. 2022. Why Are Condi-\\ntional Generative Models Better Than Unconditional Ones?\\nArXiv, abs/2212.00362.\\nBhattacharya, U.; Rewkowski, N.; Banerjee, A.; Guhan,\\nP.; Bera, A.; and Manocha, D. 2021.\\nText2Gestures: A\\nTransformer-Based Network for Generating Emotive Body\\nGestures for Virtual Agents. 2021 IEEE Virtual Reality and\\n3D User Interfaces (VR), 1–10.\\nCervantes, P.; Sekikawa, Y.; Sato, I.; and Shinoda, K. 2022.\\nImplicit Neural Representations for Variable Length Human\\nMotion Generation. ArXiv, abs/2203.13694.\\nChang, Z.; Koulieris, G. A.; and Shum, H. P. H. 2023. On\\nthe Design Fundamentals of Diffusion Models: A Survey.\\nArXiv, abs/2306.04542.\\nChen, X.; Jiang, B.; Liu, W.; Huang, Z.; Fu, B.; Chen, T.; Yu,\\nJ.; and Yu, G. 2022. Executing your Commands via Motion\\nDiffusion in Latent Space. ArXiv, abs/2212.04048.\\nChoutas, V.; Pavlakos, G.; Bolkart, T.; Tzionas, D.; and\\nBlack, M. J. 2020. Monocular Expressive Body Regression\\nthrough Body-Driven Attention. ArXiv, abs/2008.09062.\\nDaras, G.; Shah, K.; Dagan, Y.; Gollakota, A.; Dimakis,\\nA. G.; and Klivans, A. R. 2023.\\nAmbient Diffusion:\\nLearning Clean Distributions from Corrupted Data. ArXiv,\\nabs/2305.19256.\\nDhariwal, P.; Jun, H.; Payne, C.; Kim, J. W.; Radford, A.;\\nand Sutskever, I. 2020. Jukebox: A Generative Model for\\nMusic. ArXiv, abs/2005.00341.\\nDhariwal, P.; and Nichol, A. 2021. Diffusion Models Beat\\nGANs on Image Synthesis. ArXiv, abs/2105.05233.\\nFiche, G.; Leglaive, S.; Alameda-Pineda, X.; and S’eguier,\\nR. 2023. Motion-DVAE: Unsupervised learning for fast hu-\\nman motion denoising. ArXiv, abs/2306.05846.\\nGhosh, A.; Cheema, N.; Oguz, C.; Theobalt, C.; and\\nSlusallek, P. 2021. Synthesis of Compositional Animations\\nfrom Textual Descriptions. 2021 IEEE/CVF International\\nConference on Computer Vision (ICCV), 1376–1386.\\nGong, K.; Lian, D.; Chang, H.; Guo, C.; Zuo, X.; Jiang, Z.;\\nand Wang, X. 2023. TM2D: Bimodality Driven 3D Dance\\nGeneration via Music-Text Integration. arXiv:2304.02419.\\nGuo, C.; Xuo, X.; Wang, S.; and Cheng, L. 2022a. TM2T:\\nStochastic and Tokenized Modeling for the Reciprocal\\nGeneration of 3D Human Motions and Texts.\\nArXiv,\\nabs/2207.01696.\\nGuo, C.; Zou, S.; Zuo, X.; Wang, S.; Ji, W.; Li, X.; and\\nCheng, L. 2022b. Generating Diverse and Natural 3D Hu-\\nman Motions From Text. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), 5152–5161.\\nGuo, C.; Zuo, X.; Wang, S.; Zou, S.; Sun, Q.; Deng, A.;\\nGong, M.; and Cheng, L. 2020.\\nAction2Motion: Condi-\\ntioned Generation of 3D Human Motions. Proceedings of\\nthe 28th ACM International Conference on Multimedia.\\nGuo, W.; Du, Y.; Shen, X.; Lepetit, V.; Alameda-Pineda, X.;\\nand Moreno-Noguer, F. 2022c.\\nBack to MLP: A Simple\\nBaseline for Human Motion Prediction.\\n2023 IEEE/CVF\\nWinter Conference on Applications of Computer Vision\\n(WACV), 4798–4808.\\nHo, J.; Chan, W.; Saharia, C.; Whang, J.; Gao, R.; Grit-\\nsenko, A. A.; Kingma, D. P.; Poole, B.; Norouzi, M.; Fleet,\\nD. J.; and Salimans, T. 2022.\\nImagen Video: High Def-\\ninition Video Generation with Diffusion Models.\\nArXiv,\\nabs/2210.02303.\\nHo, J.; Jain, A.; and Abbeel, P. 2020. Denoising Diffusion\\nProbabilistic Models. ArXiv, abs/2006.11239.\\nHo, J.; and Salimans, T. 2022.\\nClassifier-Free Diffusion\\nGuidance. arXiv:2207.12598.\\nInc., A. S. 2021. Mixamo. https://www.mixamo.com/. Ac-\\ncessed: 2021-12-25.\\nJi, Y.; Xu, F.; Yang, Y.; Shen, F.; Shen, H. T.; and Zheng, W.\\n2018. A Large-scale RGB-D Database for Arbitrary-view\\nHuman Action Recognition. Proceedings of the 26th ACM\\ninternational conference on Multimedia.\\nKanazawa, A.; Black, M. J.; Jacobs, D. W.; and Malik, J.\\n2017.\\nEnd-to-End Recovery of Human Shape and Pose.\\n2018 IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition, 7122–7131.\\nKawar, B.; Elata, N.; Michaeli, T.; and Elad, M. 2023.\\nGSURE-Based Diffusion Model Training with Corrupted\\nData. ArXiv, abs/2305.13128.\\nKingma, D. P.; Mohamed, S.; Rezende, D. J.; and Welling,\\nM. 2014. Semi-supervised Learning with Deep Generative\\nModels. ArXiv, abs/1406.5298.\\nKocabas, M.; Athanasiou, N.; and Black, M. J. 2019. VIBE:\\nVideo Inference for Human Body Pose and Shape Estima-\\ntion. 2020 IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR), 5252–5262.\\nKoppula, H. S.; and Saxena, A. 2013. Anticipating human\\nactivities for reactive robotic response. 2013 IEEE/RSJ In-\\nternational Conference on Intelligent Robots and Systems,\\n2071–2071.\\nLee, L.-H.; Braud, T.; Zhou, P.; Wang, L.; Xu, D.; Lin, Z.;\\nKumar, A.; Bermejo, C.; and Hui, P. 2021. All One Needs to\\nKnow about Metaverse: A Complete Survey on Technolog-\\nical Singularity, Virtual Ecosystem, and Research Agenda.\\nArXiv, abs/2110.05352.\\nLi, C.; Xu, T.; Zhu, J.; and Zhang, B. 2017. Triple Genera-\\ntive Adversarial Nets. ArXiv, abs/1703.02291.\\nLi, J.; Yin, Y.; Chu, H.; Zhou, Y.; Wang, T.; Fidler, S.; and\\nLi, H. 2020. Learning to Generate Diverse Dance Motions\\nwith Transformer. ArXiv, abs/2008.08171.\\nLi, R.; Yang, S.; Ross, D. A.; and Kanazawa, A. 2021. AI\\nChoreographer: Music Conditioned 3D Dance Generation\\nwith AIST++.\\n2021 IEEE/CVF International Conference\\non Computer Vision (ICCV), 13381–13392.\\nLing, H. Y.; Zinno, F.; Cheng, G.; and van de Panne, M.\\n2020.\\nCharacter controllers using motion VAEs.\\nACM\\nTransactions on Graphics (TOG), 39: 40:1 – 40:12.\\nLoper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.; and\\nBlack, M. J. 2015. SMPL: a skinned multi-person linear\\nmodel. ACM Trans. Graph., 34: 248:1–248:16.\\nLucic, M.; Tschannen, M.; Ritter, M.; Zhai, X.; Bachem, O.;\\nand Gelly, S. 2019. High-Fidelity Image Generation With\\nFewer Labels. ArXiv, abs/1903.02271.\\nMahmood, N.; Ghorbani, N.; Troje, N. F.; Pons-Moll, G.;\\nand Black, M. J. 2019. AMASS: Archive of Motion Capture\\nAs Surface Shapes. 2019 IEEE/CVF International Confer-\\nence on Computer Vision (ICCV), 5441–5450.\\nMiao, Y.-C.; Zhang, L.; Zhang, L.; and Tao, D. 2023.\\nDDS2M: Self-Supervised Denoising Diffusion Spatio-\\nSpectral Model for Hyperspectral Image Restoration. ArXiv,\\nabs/2303.06682.\\nNie, W.; Guo, B.; Huang, Y.; Xiao, C.; Vahdat, A.; and\\nAnandkumar, A. 2022. Diffusion Models for Adversarial\\nPurification. arXiv:2205.07460.\\nPetrovich, M.; Black, M. J.; and Varol, G. 2021. Action-\\nConditioned 3D Human Motion Synthesis with Transformer\\nVAE. 2021 IEEE/CVF International Conference on Com-\\nputer Vision (ICCV), 10965–10975.\\nPetrovich, M.; Black, M. J.; and Varol, G. 2022. TEMOS:\\nGenerating diverse human motions from textual descrip-\\ntions. ArXiv, abs/2204.14109.\\nPlappert, M.; Mandery, C.; and Asfour, T. 2016. The KIT\\nMotion-Language Dataset. Big Data, 4(4): 236–252.\\nRaab, S.; Leibovitch, I.; Li, P.; Aberman, K.; Sorkine-\\nHornung, O.; and Cohen-Or, D. 2022. MoDi: Unconditional\\nMotion Synthesis from Diverse Data. arXiv:2206.08010.\\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen,\\nM. 2022a. Hierarchical Text-Conditional Image Generation\\nwith CLIP Latents. ArXiv, abs/2204.06125.\\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen,\\nM. 2022b. Hierarchical Text-Conditional Image Generation\\nwith CLIP Latents. arXiv:2204.06125.\\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\\nmer, B. 2021. High-Resolution Image Synthesis with La-\\ntent Diffusion Models.\\n2022 IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR), 10674–\\n10685.\\nRuiz, A. H.; Gall, J.; and Moreno-Noguer, F. 2018. Human\\nMotion Prediction via Spatio-Temporal Inpainting.\\n2019\\nIEEE/CVF International Conference on Computer Vision\\n(ICCV), 7133–7142.\\nSaharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Den-\\nton, E. L.; Ghasemipour, S. K. S.; Ayan, B. K.; Mahdavi,\\nS. S.; Lopes, R. G.; Salimans, T.; Ho, J.; Fleet, D. J.;\\nand Norouzi, M. 2022. Photorealistic Text-to-Image Dif-\\nfusion Models with Deep Language Understanding. ArXiv,\\nabs/2205.11487.\\nSiyao, L.; Yu, W.; Gu, T.; Lin, C.; Wang, Q.; Qian, C.;\\nLoy, C. C.; and Liu, Z. 2022. Bailando: 3D Dance Gen-\\neration by Actor-Critic GPT with Choreographic Memory.\\n2022 IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR), 11040–11049.\\nTevet, G.; Raab, S.; Gordon, B.; Shafir, Y.; Cohen-Or, D.;\\nand Bermano, A. H. 2022. Human Motion Diffusion Model.\\nArXiv, abs/2209.14916.\\nTiwari, G.; Antic, D.; Lenssen, J. E.; Sarafianos, N.; Tung,\\nT.; and Pons-Moll, G. 2022. Pose-NDF: Modeling Human\\nPose Manifolds with Neural Distance Fields. In European\\nConference on Computer Vision (ECCV).\\nTseng,\\nJ.-H.;\\nCastellon,\\nR.;\\nand\\nLiu,\\nC.\\nK.\\n2022.\\nEDGE: Editable Dance Generation From Music.\\nArXiv,\\nabs/2211.10658.\\nTur, A. O.; Dall’Asen, N.; Beyan, C.; and Ricci, E. 2023. Ex-\\nploring Diffusion Models for Unsupervised Video Anomaly\\nDetection. ArXiv, abs/2304.05841.\\nVaswani, A.; Shazeer, N. M.; Parmar, N.; Uszkoreit, J.;\\nJones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017.\\nAttention is All you Need. In NIPS.\\nYou, Z.; Zhong, Y.; Bao, F.; Sun, J.; Li, C.; and Zhu, J. 2023.\\nDiffusion Models and Semi-Supervised Learners Benefit\\nMutually with Few Labels. ArXiv, abs/2302.10586.\\nZhang, M.; Cai, Z.; Pan, L.; Hong, F.; Guo, X.; Yang, L.; and\\nLiu, Z. 2022. MotionDiffuse: Text-Driven Human Motion\\nGeneration with Diffusion Model. ArXiv, abs/2208.15001.\\nZhou, Y.; Barnes, C.; Lu, J.; Yang, J.; and Li, H. 2018. On\\nthe Continuity of Rotation Representations in Neural Net-\\nworks.\\n2019 IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR), 5738–5746.\\n'},\n",
       " {'abstract': \"Dengue fever is a serious disease that has impacted developing nations' public health infrastructure, increasing the severity of dengue infections, and causing life-threatening situations. Dengue outbreak forecasts, which have the potential to prevent these outbreaks, have been primarily based on data that requires significant effort from countries to collect. This work aims to improve the health equity in resource-constrained countries by satellite imagery as a non-traditional and readily accessible data source. A scalable satellite extraction framework based on Sentinel Hub, a cloud-based computing platform, is presented. The study also introduces an innovative architecture named DengueNet, which integrates Vision Transformer, Radiomics, and Long Short-Term Memory to extract and integrate spatiotemporal features from satellite imagery, allowing dengue predictions on an epidemiological-week basis. Conducting experiments on five municipalities in Colombia, using 780 high-resolution Sentinel-2 satellite images for training and evaluation, demonstrated the effectiveness of the proposed method. Across these municipalities, DengueNet achieved an average Mean Absolute Error (MAE) of 43.92±42.19, with Cali, Colombia, recording the highest MAE of 113.65±0.08 and Ibagué's MAE being the lowest at 5.67±0.18. This study strongly supports the efficacy of satellite imagery as a valuable resource for dengue prediction, particularly in guiding public health policies within low- and middle-income countries where high-quality data collection is scarce.\",\n",
       "  'introduction': \"Dengue fever, the leading cause of hospitalization and death in many tropical and subtropical regions, is caused by the dengue virus, transmitted by the Aedes mosquito. An estimated 4 billion people are at risk of dengue infection, especially in low- and middle-income countries (LMICs), where dengue fever prevalence is exacerbated by various factors. Rapid response to dengue outbreaks is challenging due to limited information exchange and integration. While timely dengue outbreak forecasts have the potential to prevent such outbreaks, conventional data collection techniques are often costly and difficult to scale. Therefore, alternative resources like publicly available satellite imagery are crucial for LMICs where structured data is scarce and critical indicators are lacking. Remotely sensed satellite imagery can be more cost-effective and efficient than alternative field survey methods and has shown potential correlation with weather variables, a key factor in dengue outbreaks. Moreover, surveillance systems based exclusively on satellite imagery can enhance the response time to national crises in hyperendemic regions in LMICs. This study employs recent advancements in machine learning and proposes an ML-based approach for forecasting dengue incidence in five Colombian municipalities using satellite imagery, selected due to Colombia's consistent high levels of reported dengue outbreaks. The study contributes to scalable data collection and processing, effective preprocessing for satellite imagery, and positive results indicating the feasibility of dengue forecasting using time-series satellite imagery alone for LMICs with limited resources.\",\n",
       "  'literature_review': 'Prior research has demonstrated the potential for dengue forecasting utilizing pre-collected structural information like temperature and precipitation. However, conventional data collection techniques are both costly and difficult to scale. Therefore, seeking alternative resources, such as publicly available satellite imagery, is significant for LMICs where structured data is scarce. Remote sensing satellite imagery can be a more cost-effective and efficient approach than alternative field survey methods and has shown potential correlation with weather variables, which are one of the key factors behind dengue outbreaks. It also enables a higher revisit frequency and diverse resolutions of imagery over time than surveys where repeated measurements at a local level are limited. Furthermore, the development of surveillance systems that rely exclusively on satellite imagery to notify public health authorities of early dengue detection can cost-effectively enhance the response time to national crises in hyperendemic regions in LMICs.',\n",
       "  'methodology': \"The study collects satellite imagery and dengue incidences from 2016 to 2018 in five Colombian municipalities. Sentinel Hub is used to collect and process Sentinel-2 satellite data, and the regions of interest are pre-determined using the municipalities' latitude and longitude square coordinates. Each area is sampled per epidemiological period from Sentinel-2's launch date to the time frame before COVID-19 to create a time-series satellite imagery dataset. Focusing on data before COVID-19, as studies show that COVID-19 has impacted dengue transmission, the data is stored in a TIFF format and contains 12 bands from Sentinel-2. To account for differences in band resolution, the nearest-neighbor interpolation method is used to increase the resolution of all bands to a uniform 10 meters per pixel. Cloud and cloud shadow interferences are avoided using the LeastCC algorithm. Weekly dengue incidences are obtained from the Colombian Public Health System (SIVIGILA) and matched with satellite cases on an epidemiological-week basis.\",\n",
       "  'results': 'Among the five municipalities assessed, Ibagué exhibits the most favorable performance across all metrics, while Cúcuta reports the least favorable performance, which is anticipated. These results align with the observation that apart from an initial peak, the dengue trend in Ibagué is comparatively more stable than in other municipalities. Although the number of dengue cases in Cali appears stable, the high baseline number of cases results in an increase in the MAE. In the case of Cúcuta, given that the training set has relatively low occurrences of dengue, the model fails to accurately reflect the actual trend of dengue cases for Cúcuta during the testing period. It is notable that while the three metrics have different values within one municipality, they report similar results across municipalities, indicating that DengueNet exhibits relatively stable performance across different metrics. Comparative analysis of DengueNet with actual dengue incidences, an LSTM model relying solely on historical cases, and a combined model incorporating both satellite images and cases, demonstrates the capability of DengueNet to accurately predict most trends, even in cases with greater fluctuations in dengue cases over time. This observation substantiates the effectiveness of DengueNet in forecasting outbreak patterns within a majority of municipalities, relying solely on satellite images as input. Furthermore, DengueNet exhibits robust predictive capabilities not only for short-term trends but also demonstrates adaptability by incorporating historical case data when available, thus enhancing prediction accuracy.',\n",
       "  'conclusion': \"The study introduces a robust and efficient approach for extracting satellite data and presents DengueNet, a novel architecture for predicting dengue outbreaks using satellite imagery. Experimentation involves the analysis of satellite images and dengue cases spanning from 2016 to 2018, focusing on five municipalities in Colombia. The proposed model combines ViTs with concatenated multi-layer LSTMs to effectively extract both spatial and temporal information from a series of satellite imagery, resulting in comparable dengue case predictions. To address the challenges posed by the dimensionality of satellite images, the study incorporates band selection based on band-to-band Pearson's correlation, enabling a comprehensive assessment of Sentinel-2 satellite images. The selected bands undergo feature extraction through the use of both the feature-engineering and ViT modules. These extracted features from both modules are subsequently integrated into a concatenated LSTM-based model for predicting dengue cases. Incorporating freely accessible satellite imagery into the DengueNet model holds significant potential for making a substantial impact on public health legislation and fairness in health. DengueNet, which leverages publicly accessible satellite imagery, exhibits comparable performance to that of a straightforward LSTM model that relies exclusively on dengue cases for dengue prediction. This approach advances the democratization of data access and the implementation of machine learning models globally, thereby aiding in the formulation of informed public health policies and strategies for early warning systems.\",\n",
       "  'title': 'DengueNet: Dengue Prediction using Spatiotemporal Satellite Imagery for Resource-Limited Countries',\n",
       "  'author': 'Kuan-Ting Kuo, Dana Moukheiber, Sebastian Cajas Ordonez, David Restrepo, Atika Rahman Paddo, Tsung-Yu Chen, Lama Moukheiber, Mira Moukheiber, Sulaiman Moukheiber, Saptarshi Purkayastha, Po-Chih Kuo, Leo Anthony Celi',\n",
       "  'textdata': 'DengueNet: Dengue Prediction using Spatiotemporal Satellite Imagery for\\nResource-Limited Countries\\nKuan-Ting Kuo1 , Dana Moukheiber2 , Sebastian Cajas Ordonez3,4 , David Restrepo2,5 , Atika\\nRahman Paddo6 , Tsung-Yu Chen1 , Lama Moukheiber2 , Mira Moukheiber2 , Sulaiman\\nMoukheiber7 , Saptarshi Purkayastha6 , Po-Chih Kuo1 and Leo Anthony Celi2,3,8\\n1 National Tsing Hua Unversity, Taiwan\\n2 Massachusetts Institute of Technology, USA\\n3 Harvard University, USA\\n4 University College Dublin, Ireland\\n5 University of Cauca, Colombia\\n6 Indiana University – Purdue University Indianapolis, USA\\n7 Worcester Polytechnic Institute, USA\\n8 Beth Israel Deaconess Medical Center, USA\\n{mimikuo365, lear1007}@gmail.com, {danamouk, davidres, lamam, miram, lceli}@mit.edu,\\napaddo@iu.edu, ulsordonez@unicauca.edu.co, swmoukheiber@wpi.edu, saptpurk@iupui.edu,\\nkuopc@cs.nthu.edu.tw\\nAbstract\\nDengue fever presents a substantial challenge in\\ndeveloping countries where sanitation infrastruc-\\nture is inadequate.\\nThe absence of comprehen-\\nsive healthcare systems exacerbates the severity\\nof dengue infections, potentially leading to life-\\nthreatening circumstances.\\nRapid response to\\ndengue outbreaks is also challenging due to lim-\\nited information exchange and integration. While\\ntimely dengue outbreak forecasts have the poten-\\ntial to prevent such outbreaks, the majority of\\ndengue prediction studies have predominantly re-\\nlied on data that impose significant burdens on in-\\ndividual countries for collection.\\nIn this study,\\nour aim is to improve health equity in resource-\\nconstrained countries by exploring the effective-\\nness of high-resolution satellite imagery as a non-\\ntraditional and readily accessible data source. By\\nleveraging the wealth of publicly available and eas-\\nily obtainable satellite imagery, we present a scal-\\nable satellite extraction framework based on Sen-\\ntinel Hub, a cloud-based computing platform. Fur-\\nthermore, we introduce DengueNet1, an innovative\\narchitecture that combines Vision Transformer, Ra-\\ndiomics, and Long Short-term Memory to extract\\nand integrate spatiotemporal features from satel-\\nlite images. This enables dengue predictions on an\\nepidemiological-week basis. To evaluate the effec-\\ntiveness of our proposed method, we conducted ex-\\nperiments on five municipalities in Colombia. We\\nutilized a dataset comprising 780 high-resolution\\nSentinel-2 satellite images for training and eval-\\n1https://github.com/mimikuo365/DengueNet-IJCAI\\nuation.\\nThe performance of DengueNet was as-\\nsessed using the mean absolute error (MAE) met-\\nric.\\nAcross the five municipalities, DengueNet\\nachieved an average MAE of 43.92±42.19.\\nNo-\\ntably, the highest MAE was recorded in Cali at\\n113.65±0.08, whereas the lowest MAE was ob-\\nserved in Ibagu´e, amounting to 5.67±0.18.\\nOur\\nfindings strongly support the efficacy of satellite\\nimagery as a valuable resource for dengue predic-\\ntion, particularly in informing public health poli-\\ncies within low- and middle-income countries. In\\nthese countries, where manually collected data of\\nhigh quality is scarce and dengue virus prevalence\\nis severe, satellite imagery can play a crucial role\\nin improving dengue prevention and control strate-\\ngies.\\n1\\nIntroduction\\nDengue, one of the most ubiquitous mosquito-borne viral in-\\nfections, is the leading cause of hospitalization and death\\nin many parts of the world, especially in tropical and sub-\\ntropical countries [Cattarino et al., 2020].\\nIt is estimated\\nthat 129 countries [WHO, 2022] and 4 billion people [CDC,\\n2022] are at risk of dengue infection. In low- and middle-\\nincome countries (LMICs) where dengue fever is endemic,\\nthe prevalence of dengue outbreaks is exacerbated by multi-\\nfarious factors such as barriers in the continuum of care, in-\\nequities in resource allocation, education levels, literacy, and\\nincome[Chaparro et al., 2016]. Because there are no specific\\ntreatments available for the virus, dengue prevention is crit-\\nical to reducing its infectious and fatality rate, particularly\\nin hyperendemic regions in LMICs where dengue poses a\\nsignificant public health predicament [Gutierrez-Barbosa et\\nal., 2020]. Therefore, the strategic utilization of viable early\\narXiv:2401.11114v1  [cs.CV]  20 Jan 2024\\nFigure 1: DengueNet model architecture takes in weekly satellite imagery and dengue cases y as input for predicting ˆy (m/px: meters per\\npixel; RGB: red, green and blue bands; SWIR: short wave infrared spectrum band; ViT: Vision Transformer; LSTM: Long Short-Term\\nmemory; MLP: Multilayer Perceptron). The LSTM module consists of three stacked standard LSTM layers.\\ndetection approaches for dengue outbreaks in LMICs is not\\nonly imperative for promoting comprehensive well-being but\\nalso plays a crucial role in the pursuit of reducing health in-\\nequities. By employing these effective approaches, we can\\nactively contribute to the realization of equitable healthcare\\naccess and outcomes, thereby fostering a more inclusive and\\njust society.\\nPrior research has demonstrated the potential for dengue\\nforecasting utilizing pre-collected structural information like\\ntemperature and precipitation [Martheswaran et al., 2022;\\nJain et al., 2019]. However, conventional data collection tech-\\nniques are both costly and difficult to scale. Therefore, seek-\\ning alternative resources, such as publicly available satellite\\nimagery, is significant for LMICs where structured data is\\nscarce and critical indicators remain lacking. Remote sens-\\ning satellite imagery can be a more cost-effective and effi-\\ncient approach than alternative field survey methods and has\\nshown potential correlation with weather variables [Ren et\\nal., 2021], which are one of the key factors behind dengue\\noutbreaks. It also enables a higher revisit frequency and di-\\nverse resolutions of imagery over time than surveys where\\nrepeated measurements at a local level are limited [Lee et\\nal., 2017].\\nFurthermore, the development of surveillance\\nsystems that rely exclusively on satellite imagery to notify\\npublic health authorities of early dengue detection can cost-\\neffectively enhance the response time to national crises in hy-\\nperendemic regions in LMICs.\\nThis study employs recent advances in machine learning\\n(ML) and proposes an ML-based approach for forecasting the\\nincidence of dengue cases in five municipalities of Colom-\\nbia using satellite imagery. This selection was made due to\\nColombia’s persistent incidence of high levels of reported\\ndengue outbreaks from 1978 until 2022 [National Institute of\\nHealth of Colombia, 2010]. As one of the top five countries\\nin the Americas with the highest number of reported dengue\\ncases, Colombia’s dengue mortality rate is 4.84 times higher\\nthan that of other American countries [PAHO, 2022]. Below\\nare the three principal contributions to this paper.\\n• We introduce a scalable data collection and processing\\nframework to extract time-series data from the Sentinel-\\n2 satellite.\\n• We propose a novel preprocessing pipeline that can\\neffectively eliminate noises and extract spatiotemporal\\nfeatures from the collected satellite imagery.\\n• Our model, DengueNet, shows positive results, indicat-\\ning dengue forecasting with time-series satellite imagery\\nalone is a feasible approach for LMICs with limited re-\\nsources.\\n2\\nRelated Works\\n(a) Dengue cases\\n(b) Geographic regions\\nFigure 2: Municipality-level dengue case numbers and geographic\\nlocations. (a) Dengue cases from 2016 to 2018 were obtained from\\nthe SIVIGILA database for the top five affected municipalities in\\nColombia. (b) Geographic locations from satellite imagery for each\\nmunicipality.\\nThe epidemiology of dengue is influenced by multiple\\nfactors, including seasonal fluctuations in temperature and\\nrainfall, socio-economic determinants such as education and\\nhousehold income [Morgan et al., 2021; Watts et al., 2020],\\nand intra-strain genetic variability [Fontaine et al., 2018].\\nTo comprehend the determinants of dengue infection, stud-\\nies have been conducted to evaluate the economic, societal,\\nand other facets of dengue outbreaks worldwide. In terms\\nof structured data, notable work by researchers has paired a\\nboosted regression tree framework with longitudinal informa-\\ntion and population surfaces to develop a risk map to under-\\nstand the global distribution of dengue and improve disease\\nFigure 3: Gray-scale satellite band images captured by Sentinel-2 using different wavelengths.\\nmanagement programs globally [Bhatt et al., 2013]. Similar\\nwork has been established, which investigates the temporal\\nand spatial distribution of dengue fever in India using Kull-\\ndorff’s space-time permutation method [Mala and Jat, 2019].\\nOther work [Mu˜noz et al., 2021] has also looked at the as-\\nsociation of the local climate with dengue in Colombia us-\\ning linear analysis tools and lagged crossed-correlations such\\nas Pearson’s test. Features highly associated with dengue,\\nsuch as environmental, entomological, epidemiological, and\\nhuman-related data, have been explored for dengue pre-\\ndiction\\n[Roster and Rodrigues, 2021; Karim et al., 2012;\\nGuo et al., 2017; Salim et al., 2021]. Other studies have\\nused human-related data like mobility [Datoc et al., 2016],\\nsocial media data [Livelo and Cheng, 2018], and distance\\nto public transit [Shragai et al., 2022] to build dengue early\\nwarning systems. In terms of unstructured data, studies com-\\npared street view and aerial images with different convolu-\\ntional neural network architectures to estimate dengue rates\\n[Andersson et al., 2019].\\nSatellite imagery is often adopted with other statistical\\ndata to perform spatiotemporal tasks, such as weather fore-\\ncasting, precipitation nowcasting [Moskola¨ı et al., 2021;\\nSon and Thong, 2017; de Witt et al., 2020] and vector-borne\\ndisease case predictions [Rogers et al., 2002; Li et al., 2022a;\\nAbdur Rehman et al., 2019]. While LMICs lack access to\\nreliable information systems for data collection and analy-\\nsis [Ndabarora et al., 2014; Kruk et al., 2018; Fenech et\\nal., 2018], free sources of satellite imagery from cloud-based\\ncomputing platforms, such as Google Earth Engine and Sen-\\ntinel Hub, provide an alternative data asset for LMICs for\\nearly detection of dengue. In our work, we build a repro-\\nducible Sentinel-2 satellite data extraction framework lever-\\naging Sentinel Hub and provide municipality-level predic-\\ntions of dengue cases in Colombia per epi week. By solely\\nadopting satellite imagery for dengue outbreak prediction,\\nour model can focus on learning potential environmental in-\\nformation through difference in vegetation over time using\\ntime-series images to predict dengue cases [Moskola¨ı et al.,\\n2021].\\n3\\nDataset\\nIn this study, we collect satellite imagery and dengue inci-\\ndences from 2016 to 2018 in five Colombian municipalities\\nincluding Medell´ın, Ibagu´e, Cali, Villavicencio, and C´ucuta\\n(Figure 2). These municipalities are chosen as they have re-\\nported relatively high dengue cases in Colombia. Sentinel\\nHub [Ltd, 2022] is used to collect and process Sentinel-2\\nsatellite data.\\nThe regions of interest are pre-determined\\nusing the different municipalities’ latitude and longitude\\nsquare coordinates. Each area is sampled per epi week from\\nSentinel-2’s launch date to the time frame before COVID-19,\\nto create a time-series satellite imagery dataset. We focus on\\ndata before COVID-19, as studies show that COVID-19 has\\nimpacted dengue transmission [Lim et al., 2020]. Our data is\\nstored in a TIFF format and contains 12 bands from Sentinel-\\n2 as shown in Figure 3. To account for differences in band\\nresolution, we use nearest-neighbor interpolation to increase\\nthe resolution of all bands to a uniform 10 meters per pixel.\\nCloud inteferences are avoided using the LeastCC algorithm,\\nwhich is configured using Sentinel Hub API to request the\\nimages with the least amount of clouds per epi week. We\\nobtain weekly dengue incidences from the Colombian Pub-\\nlic Health System (SIVIGILA). Satellite imagery is matched\\nwith dengue cases on an epi-week basis.\\n4\\nMethodology\\n4.1\\nOverview\\nTo fully examine whether satellite imagery could be used\\nto predict dengue cases, we introduce multiple modules in\\nDengueNet (see Figure 1). The model components are de-\\nsigned to capture both the temporal and spatial information\\nfrom satellite images for dengue outbreak forcasting. First,\\nwe conduct band correlation analysis to determine which\\nsatellite bands to select and use in our study. We then apply\\ncloud and cloud shadow (CCS) removal on the selected bands\\nto reduce noises in the satellite images. The preprocessed\\nbands are then fed into two spatial feature extraction modules,\\nthe Feature-Engineering and the Vision-Transformer (ViT)\\nfeature extractors, respectively. The features extracted from\\nFigure 4: Average Pearson’s correlation of the 12 bands for the\\nSentinel-2 satellite images across five Colombian municipalities in\\nthe training set from 2016 to 2018. The majority of correlations are\\nstatistically significant (p <0.001).\\nOriginal\\nBand\\nImages\\nImage\\nSlicing\\nTile\\nClassification\\nTile\\nSwapping\\nTile\\nAveraging\\nAverage\\nTiles\\nPreprocessed \\nBand Images\\nAbnormal Tile Indices\\nNormal Tile \\nIndices\\nCCS\\nDetection\\nSliced Tiles\\nMasks\\nFigure 5: Stages involved in the cloud and cloud shadow removal\\nmodule. The average tiles are generated using the normal tiles in the\\nsamples (CCS: cloud and cloud shadow).\\nthe two modules are then fed into two multi-layer Long Short-\\nterm Memory (LSTM) networks that can extract temporal\\nfeatures, and eventually concatenated to a fully connected\\nneural network for dengue case prediction.\\n4.2\\nBand Selection\\nSatellite imagery often contains multiple bands with different\\nresolutions, central wavelengths, and channels. An example\\nis shown in Figure 3. We aim to reduce the dimensionality\\nof the input satellite images while preserving band variance.\\nThus, the band selection module contains two steps. We first\\ncompute the inter-band correlation matrix from the samples in\\nthe training set using Pearson’s correlation coefficient (Fig-\\nure 4). We then categorize the bands into different clusters\\nand select the ones in different clusters.\\nFigure 4 highlights three clusters in our data, each indicat-\\ning the high correlation between the bands (bands 1-5, 6-9,\\nand 11-12). We aim to select bands from different clusters for\\nthe two feature extraction modules to preserve band variance.\\nSince bands 11 and 12 correspond to the Short Wave Infrared\\n(SWIR) spectrum, which is mainly used for measuring soil\\nand vegetation moisture content as it provides good contrast\\nbetween different vegetation types, we intend to select bands\\nfrom this cluster for the Feature-Engineering pipeline. Given\\nthat both bands show a high correlation, we select band 12\\nfor its relatively lower correlation coefficient against the other\\nsatellite bands (bands 1-10) to avoid multicollinearity. For the\\n(a) Original image\\n(b) Cloud mask\\n(c) CS mask\\nFigure 6: Cloud and cloud shadow masks generated in the CCS de-\\ntection stage in Figure 5. (a) Original image where abnormal tiles\\nwill be swapped with the average of normal tiles. (b) Cloud mask\\nwith detected abnormal cloudy pixels in white and normal pixels in\\nblack. Abnormal tiles detected by the cloud mask are highlighted in\\nred. (c) Cloud shadow (CS) mask with detected abnormal shadowy\\npixels in white and normal pixels in black. Abnormal tiles detected\\nby the shadow mask are highlighted in green.\\nViT feature extraction module, to preserve band diversity and\\nmatch channels with the pre-training image set, we use bands\\n2, 3, and 4, which correspond to the Red, Green, and Blue\\nchannels.\\n4.3\\nCloud and Cloud Shadow Removal\\nThe cloud and cloud shadow removal (CSR) module is used\\nto remove the cloud and cloud shadow from the selected satel-\\nlite bands by performing CCS detection, image slicing, tile\\nclassification, tile averaging, and tile swapping (see Figure 5).\\nAs satellite imagery often contains many cloud and cloud\\nshadow noises, CCS detection [Li et al., 2022b] is an essential\\nstage for reducing noises. To identify noisy pixels caused by\\ncloud or cloud shadow coverage, two thresholds are utilized\\nto determine whether a pixel is considered noisy due to the\\noften extreme pixel values in the affected areas. To establish\\nthresholds for detecting cloud and cloud shadow, we evalu-\\nate the effectiveness of using pixel value percentiles from the\\ntraining set and compare their performance. Through testing\\npercentiles ranging from the 5th to 95th percentile at 5 per-\\ncentile intervals, we choose two percentiles as the detection\\nthresholds for cloud and cloud shadow, respectively. These\\nthresholds are then used to generate the corresponding masks\\nfor cloud and cloud shadow (see Figure 6).\\nAfter obtaining the two masks, we slice each satellite band\\nimage into 16×16 tiles. With the sliced tiles and the cloud\\nand cloud shadow masks, tiles are classified into abnormal\\nand normal tiles, where an abnormal tile indicates more than\\n50 percent of pixels in the tile are marked as noise in either\\nmask.\\nFor each tile in a different position in the images,\\nwe calculate the average tile of that position using the nor-\\nmal tiles By replacing the abnormal tiles in each sample with\\nthe corresponding average tiles, we generate noise-eliminated\\nimages. These average tiles are obtained by computing the\\naverage of normal tiles for a specific position in the images.\\n4.4\\nSpatial Feature Extractors\\nWe adopt two feature extractors to extract different types of\\nspatial features from the satellite images.\\nIn the Feature-\\nEngineering feature extractor, we extract statistical pixel-\\nbased features from the SWIR band to obtain the texture in-\\nformation. Nine features from both first-order and higher-\\norder features, such as Skewness and Joint Average, are col-\\nlected using the PyRadiomics library [Van Griethuysen et\\nal., 2017]. The details can be found in the GitHub reposi-\\ntory. For the ViT module, we adopt transfer learning to over-\\ncome the limited number of real-world satellite imagery in\\nour dataset. We utilize a ViT [Wu et al., 2020] pre-trained on\\nImageNet [Deng et al., 2009] to collect deep learning-based\\nfeatures from the RGB bands. The RGB bands are down-\\nscaled from 736×736 to 224×224 to fit the model.\\n4.5\\nModel\\nThe spatial feature extractors are both concatenated to a\\nmulti-layer LSTM module for extracting the temporal char-\\nacteristics. To mitigate overfitting, a dropout layer is added\\nafter each LSTM layer in the module. The last LSTM lay-\\ners are then concatenated to a multilayer perceptron (MLP)\\nwith one dense layer and one neuron as the final layer. We\\nchose Leaky ReLu [Maas et al., 2013] as the activation func-\\ntion to add non-linearity to the regression task. All models are\\ntrained for 100 epochs with an adaptive learning rate starting\\nfrom 0.0001.\\nIn this work, we train and evaluate the proposed structure\\non each municipality individually. This is because, with lim-\\nited amount of training data, the model may prioritize learn-\\ning the geographic meaning of different tile positions, within\\nthe same municipality. Since historical dengue cases are com-\\nmonly used for dengue prediction, we evaluate the effective-\\nness of satellite imagery with dengue cases. To do so, we use\\nthe same multi-layer LSTM structure to create a LSTM model\\nwhich takes cases as the model inputs. We also explore model\\nperformance with both satellite images and cases as inputs by\\nconcatenating the two LSTM modules from DengueNet with\\nthe LSTM module from the case model, resulting in a 10 × 1\\ndimension input to the MLP.\\n4.6\\nEvaluation and Performance Metrics\\nFor each municipality, we use the first 80 percent of the data\\nfor training, the next 10 percent of the data for validation,\\nand the last 10 percent for testing. We evaluate the proposed\\nmodel structure using Mean Absolute Error (MAE), Sym-\\nmetric Mean Absolute Percentage Error (sMAPE), and Root-\\nMean-Square Error (RMSE) metrics. sMAPE computes the\\npercentage error between the actual value and the predicted\\nvalue. We choose to use sMAPE over MAPE because the\\ndengue cases in our dataset have relatively low actual values.\\nRMSE penalizes the cases where the difference between the\\nactual and the predicted value is the greatest.\\nMAE = 1\\nn\\nn\\nX\\ni=1\\n|ˆyi − yi|,\\n(1)\\nsMAPE = 100%\\nn\\nn\\nX\\ni=1\\n2 × |ˆyi − yi|\\n(|ˆyi| + |yi|)\\n(2)\\nRMSE =\\nv\\nu\\nu\\nt 1\\nn\\nn\\nX\\ni=1\\n(yi − ˆyi)2,\\n(3)\\nRefering to Equations 1,2,3, n is the total number of samples\\nto evaluate in the test set, and i represents the sample num-\\nber. ˆyi represents the predicted value from the model, and yi\\nrepresents the actual value from the test set for each sample\\nstarting from (i = 1) to (i = n).\\n5\\nResults\\nTable 1 presents the performance evaluation of DengueNet in\\nforecasting dengue cases using a time-series of satellite im-\\nagery with a window size of five weeks. Among the five mu-\\nnicipalities assessed, Ibagu´e exhibits the most favorable per-\\nformance across all metrics, while C´ucuta reports the least\\nfavorable performance.\\nThese results are anticipated.\\nIn\\nIbagu´e, apart from an initial peak, the dengue trend is com-\\nparatively more stable than in other municipalities. While the\\nnumber of dengue cases in Cali appears stable, the high base-\\nline number of cases results in an increase in the MAE. In the\\ncase of C´ucuta, given that the training set has relatively low\\noccurrences of dengue, it is reasonable that the model fails to\\naccurately reflect the actual trend of dengue cases for C´ucuta\\nduring the testing period. A notable observation is that while\\nthe three metrics have different values within one municipal-\\nity, they report similar results acros municipalities, indicating\\nthat DengueNet exhibits relatively stable performance across\\ndifferent metrics.\\nFigure 7 depicts the forecasted dengue cases for five mu-\\nnicipalities utilizing a diverse set of input data, including fea-\\ntures extracted from satellite imagery and historical dengue\\ncases.\\nComparative analysis is conducted against actual\\ndengue incidences, an LSTM model relying solely on histor-\\nical cases, and a combined model incorporating both satellite\\nimages and cases as input. Upon examination of the figures,\\nit is evident that DengueNet demonstrates the capability to\\naccurately predict most trends, even in the case of Villavicen-\\ncio (refer to Figure 7c), which exhibits greater fluctuations in\\ndengue cases over time. This observation substantiates the\\neffectiveness of DengueNet in forecasting outbreak patterns\\nwithin the majority of municipalities, relying solely on satel-\\nlite images as input. Furthermore, our model exhibits robust\\npredictive capabilities not only for short-term trends, while\\nperforming slightly less worse compared to the LSTM model\\nthat solely relies on historical case data, but also demonstrates\\nadaptability by easily incorporating historical case data when\\navailable, thus enhancing prediction accuracy.\\n6\\nAblation Studies\\nFor the ablation studies, we evaluate the usage of the two fea-\\nture extraction modules as shown in Figure 1, and the CSR\\nmodule as presented in Table 2. As we observe a high de-\\ngree of similarity among the MAE, sMAPE, and RMSE met-\\nrics in Table 1, our analysis focuses on examining the dif-\\nferences between the MAE with and without the inclusion of\\nthese three modules. For the Feature-Engineering module,\\nfour municipalities result in improved MAE, with Medell´ın\\nhaving the most significant MAE improvement when paired\\nwith the CSR module. On the other hand, the CSR module\\nhas less impact on the ViT module, with only one municipal-\\nity showing improved MAE. However, after combining both\\nMetrics\\nVillavicencio\\nMedell´ın\\nC´ucuta\\nIbagu´e\\nCali\\nAverage\\nMAE\\n25.54±0.06\\n50.96±0.34\\n113.65±0.08\\n5.67±0.18\\n23.77±0.95\\n43.92±42.19\\nsMAPE\\n72.90±0.27\\n92.02±0.33\\n162.91±0.25\\n40.06±0.83\\n56.16±1.15\\n84.81±47.74\\nRMSE\\n30.62±0.03\\n67.86±0.40\\n120.57±0.07\\n7.45±0.22\\n31.80±1.46\\n51.66±44.17\\nTable 1: DengueNet evaluation across five municipalities. All experiments are repeated three times, with the average value reported with the\\nstandard deviation. The scores for the municipalities with the best and worst scores are indicated.\\nViT\\nFEng\\nCSR\\nVillavicencio\\nMedell´ın\\nC´ucuta\\nIbagu´e\\nCali\\n✓\\n✓\\n24.67±0.26\\n45.48±5.56\\n113.10±0.08\\n13.46±0.08\\n58.10±1.27\\n✓\\n26.25±0.00\\n44.77±0.79\\n109.31±0.00\\n6.21±0.13\\n33.42±0.42\\n✓\\n✓\\n24.00±0.05\\n80.46±0.03\\n113.46±0.08\\n3.52±0.06\\n96.71±0.08\\n✓\\n27.21±0.29\\n111.15±0.19\\n113.58±0.03\\n6.96±0.16\\n48.15±0.31\\n✓\\n✓\\n✓\\n25.54±0.06\\n50.96±0.34\\n113.65±0.08\\n5.67±0.18\\n23.77±0.95\\n✓\\n✓\\n24.40±0.06\\n42.48±0.96\\n114.19±0.09\\n7.25±0.09\\n42.35±0.81\\nTable 2: MAE scores with or without the cloud shadow removal (CSR) module combined with different feature extractors across five\\nmunicipalities. ViT indicates only features extracted from the ViT module are used. FEng indicates only features extracted from the feature-\\nengineering module are used. All experiments are repeated three times. Average values are reported ± the standard deviation. The best scores\\nare highlighted.\\n(a) Medell´ın\\n(b) Ibagu´e\\n(c) Villavicencio\\n(d) Cali\\n(e) C´ucuta\\nFigure 7: Dengue case prediction was performed for five municipalities per epidemiological week from 2016 to 2018. Three approaches were\\nevaluated: using satellite imagery features (ViT+FEng), case data (Case), and a combination of both (ViT+FEng+Case). The Ground Truth\\nlabel represents the actual number of dengue cases per week. The grey vertical dashed lines indicate the starting weeks of the validation and\\ntesting sets.\\nModels\\nMAE\\nsMAPE\\nRMSE\\nViT (w/ CSR)\\n50.96\\n97.66\\n60.20\\nFEng (w/ CSR)\\n63.63\\n99.24\\n74.02\\nViT+FEng (w/ CSR)\\n43.92\\n84.81\\n51.66\\nTable 3: Performance comparison of different feature extractors with\\nthe cloud and shadow removal module (w/ CSR). All experiments\\nare repeated three times and average values are reported. The best\\nscores are highlighted.\\nspatial feature extraction modules as inputs, the CSR module\\nimproves the performance across three municipalities, and the\\naverage MAE across five municipalities also decreases from\\n54.14 to 51.66.\\nThe effectiveness of having both spatial feature extractors\\nis also analyzed in Table 3. With a single feature extrac-\\ntor, the ViT feature extractor performs slightly better than the\\nFeature-Engineering extractor. However, the lowest average\\nMAE, sMAPE, and RMSE are observed when both feature\\nextractors are used. This finding is reasonable as the two\\nfeature extractors retrieve different types of information from\\nthe satellite imagery. This model architecture design enables\\nDengueNet to maintain high performance even if one of the\\nfeature extraction modules fails to extract crucial features, as\\nthe other feature extractor can compensate for it.\\n7\\nDiscussion\\nThis study introduces a robust and efficient approach for ex-\\ntracting satellite data and presents DengueNet, a novel ar-\\nchitecture for predicting dengue outbreaks using satellite im-\\nagery. The experimentation phase involves the analysis of\\nsatellite images and dengue cases spanning from 2016 to\\n2018, focusing specifically on five municipalities in Colom-\\nbia, a country significantly affected by the prevalence of\\ndengue fever. The proposed model combines ViTs with con-\\ncatenated multi-layer LSTMs to effectively extract both spa-\\ntial and temporal information from a series of satellite im-\\nagery, resulting in comparable dengue case predictions.\\nTo address the challenges posed by the dimensionality of\\nsatellite images, the study incorporates band selection based\\non band-to-band Pearson’s correlation, enabling a compre-\\nhensive assessment of Sentinel-2 satellite images. The se-\\nlected bands undergo feature extraction through the use of\\nboth the feature-engineering and ViT modules. The feature-\\nengineering pipeline involves dividing satellite images into\\ntiles and employing CCS detection to minimize the presence\\nof environmental noise artifacts, allowing for the extraction\\nof noise-free pixel features. On the other hand, the ViT mod-\\nule utilizes transfer learning from a pre-trained ViT model to\\nextract features. These extracted features from both modules\\nare subsequently integrated into a concatenated LSTM-based\\nmodel for predicting dengue cases.\\nIncorporating freely accessible satellite imagery into our\\nDengueNet model holds significant potential for making a\\nsubstantial impact on public health legislation and fairness in\\nhealth. Over the past two decades, dengue fever has emerged\\nas a prevalent epidemic in tropical developing countries, ne-\\ncessitating the establishment of an effective early warning\\nsystem for preventing and monitoring outbreaks. The fea-\\nsibility of DengueNet for predicting dengue outbreaks has\\nbeen successfully demonstrated in five municipalities, show-\\ncasing its potential for transferability to other geographical\\nregions. Moreover, the computational requirements of the\\nmodel are relatively low, and its deployment only requires\\nminimal resources, making it an accessible alternative for\\nresource-constrained developing countries.\\nThe proposed approach is further reinforced by the inclu-\\nsion of a dockerized version of the satellite extraction frame-\\nwork, leveraging Sentinel Hub, which ensures data repro-\\nducibility and scalability [Alberto et al., 2023]. This empow-\\ners LMICs to leverage higher quality and more frequently up-\\ndated satellite data, overcoming the limitations of field data\\ncollection characterized by irregular revisit rates and vary-\\ning data quality. The utilization of such information can sig-\\nnificantly contribute to informed policy decisions and strate-\\ngies at the municipality level, enabling early containment of\\nthe dengue virus.\\nUltimately, the proposed method holds\\nimmense potential to enhance the prevention and control of\\ndengue fever outbreaks in developing countries, thereby ad-\\nvancing public health outcomes and promoting health equity.\\n8\\nConclusion\\nThe\\ndockerized\\nsatellite\\nextraction\\nframework\\nand\\nlightweight DengueNet model presented in this work\\npresent a viable alternative for LMICs, where data collection\\nand preprocessing pose substantial challenges. The perfor-\\nmance of DengueNet, which leverages publicly accessible\\nsatellite imagery, exhibits comparable performance to that\\nof a straightforward LSTM model that relies exclusively on\\ndengue cases for dengue prediction.\\nThis approach takes\\nus closer to the democratization of data access and the im-\\nplementation of machine learning models globally, thereby\\naiding in the formulation of informed public health policies\\nand strategies for early warning systems. To ensure safe and\\nresponsible integration of satellite imagery and DengueNet,\\nfuture work should understand and mitigate the sources of\\nbias inherent in machine learning models[Celi et al., 2022;\\nNazer et al., 2023] to promote fairness and reduce disparities\\nin public health across diverse populations.\\nAcknowledgments\\nThis work is supported in part by Oracle Cloud credits and\\nrelated resources provided by Oracle for Research, as well as\\nthe European Space Agency’s Network of Resources Initia-\\ntive.\\nReferences\\n[Abdur Rehman et al., 2019] Nabeel Abdur Rehman, Umar\\nSaif, and Rumi Chunara. Deep landscape features for im-\\nproving vector-borne disease prediction. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition Workshops, pages 44–51, 2019.\\n[Alberto et al., 2023] Isabelle Rose I Alberto, Nicole Rose I\\nAlberto, Arnab K Ghosh, Bhav Jain, Shruti Jayaku-\\nmar,\\nNicole Martinez-Martin,\\nNed McCague,\\nDana\\nMoukheiber, Lama Moukheiber, Mira Moukheiber, et al.\\nThe impact of commercial health datasets on medical re-\\nsearch and health-care algorithms.\\nThe Lancet Digital\\nHealth, 5(5):e288–e294, 2023.\\n[Andersson et al., 2019] Virginia Ortiz Andersson, Cristian\\nCechinel, and Ricardo Matsumura Araujo.\\nCombining\\nstreet-level and aerial images for dengue incidence rate es-\\ntimation. In 2019 International Joint Conference on Neu-\\nral Networks (IJCNN), pages 1–8. IEEE, 2019.\\n[Bhatt et al., 2013] Samir Bhatt, Peter W Gething, Oliver J\\nBrady, Jane P Messina, Andrew W Farlow, Catherine L\\nMoyes, John M Drake, John S Brownstein, Anne G Hoen,\\nOsman Sankoh, et al. The global distribution and burden\\nof dengue. Nature, 496(7446):504–507, 2013.\\n[Cattarino et al., 2020] Lorenzo\\nCattarino,\\nIsabel\\nRodriguez-Barraquer, Natsuko Imai, Derek AT Cum-\\nmings, and Neil M Ferguson. Mapping global variation\\nin dengue transmission intensity.\\nScience translational\\nmedicine, 12(528):eaax4144, 2020.\\n[CDC, 2022] CDC. Dengue. https://www.cdc.gov/dengue/\\nindex.html, 2022. Accessed: 2023-01-15.\\n[Celi et al., 2022] Leo Anthony Celi, Jacqueline Cellini,\\nMarie-Laure\\nCharpignon,\\nEdward\\nChristopher\\nDee,\\nFranck Dernoncourt, Rene Eber, William Greig Mitchell,\\nLama Moukheiber, Julian Schirmer, Julia Situ, et al.\\nSources of bias in artificial intelligence that perpetuate\\nhealthcare disparities—a global review.\\nPLOS Digital\\nHealth, 1(3):e0000022, 2022.\\n[Chaparro et al., 2016] P\\nChaparro,\\nW\\nLe´on,\\nand\\nCA Casta˜neda.\\nComportamiento de la mortalidad\\npor dengue en colombia entre 1985 y 2012. Biom´edica,\\n36(Supl 2):125–34, 2016.\\n[Datoc et al., 2016] Hillary Ingrid Datoc, Romeo Caparas,\\nand Jaime Caro.\\nForecasting and data visualization of\\ndengue spread in the philippine visayas island group. In\\n2016 7th International Conference on Information, Intel-\\nligence, Systems & Applications (IISA), pages 1–4. IEEE,\\n2016.\\n[de Witt et al., 2020] Christian Schroeder de Witt, Catherine\\nTong, Valentina Zantedeschi, Daniele De Martini, Fred-\\ndie Kalaitzis, Matthew Chantry, Duncan Watson-Parris,\\nand Piotr Bilinski.\\nRainbench: towards global precipi-\\ntation forecasting from satellite imagery. arXiv preprint\\narXiv:2012.09670, 2020.\\n[Deng et al., 2009] J. Deng, W. Dong, R. Socher, L.-J. Li,\\nK. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierar-\\nchical Image Database. In CVPR09, 2009.\\n[Fenech et al., 2018] Matthew Fenech, Nika Strukelj, and\\nOlly Buston. Ethical, social, and political challenges of\\nartificial intelligence in health. London: Wellcome Trust\\nFuture Advocacy, 12, 2018.\\n[Fontaine et al., 2018] Albin Fontaine, Sebastian Lequime,\\nIsabelle Moltini-Conclois, Davy Jiolle, Isabelle Leparc-\\nGoffart, Robert Charles Reiner Jr, and Louis Lambrechts.\\nEpidemiological significance of dengue virus genetic vari-\\nation in mosquito infection dynamics. PLoS pathogens,\\n14(7):e1007187, 2018.\\n[Guo et al., 2017] Pi Guo, Tao Liu, Qin Zhang, Li Wang,\\nJianpeng Xiao, Qingying Zhang, Ganfeng Luo, Zhihao Li,\\nJianfeng He, Yonghui Zhang, et al. Developing a dengue\\nforecast model using machine learning: A case study in\\nchina. PLoS neglected tropical diseases, 11(10):e0005973,\\n2017.\\n[Gutierrez-Barbosa et al., 2020] Hernando\\nGutierrez-\\nBarbosa, Sandra Medina-Moreno, Juan C Zapata, and\\nJoel V Chua. Dengue infections in colombia: epidemi-\\nological trends of a hyperendemic country.\\nTropical\\nMedicine and Infectious Disease, 5(4):156, 2020.\\n[Jain et al., 2019] Raghvendra Jain, Sra Sontisirikit, Sopon\\nIamsirithaworn, and Helmut Prendinger.\\nPrediction of\\ndengue outbreaks based on disease surveillance, meteoro-\\nlogical and socio-economic data. BMC infectious diseases,\\n19(1):1–16, 2019.\\n[Karim et al., 2012] Md Nazmul Karim, Saif Ullah Mun-\\nshi, Nazneen Anwar, and Md Shah Alam. Climatic fac-\\ntors influencing dengue cases in dhaka city: a model for\\ndengue prediction. The Indian journal of medical research,\\n136(1):32, 2012.\\n[Kruk et al., 2018] Margaret E Kruk, Anna D Gage, Cather-\\nine Arsenault, Keely Jordan, Hannah H Leslie, Sanam\\nRoder-DeWan, Olusoji Adeyi, Pierre Barker, Bernadette\\nDaelmans, Svetlana V Doubova, et al. High-quality health\\nsystems in the sustainable development goals era: time\\nfor a revolution. The Lancet global health, 6(11):e1196–\\ne1252, 2018.\\n[Lee et al., 2017] Jung-Seok Lee, Mabel Carabali, Jacque-\\nline K Lim, Victor M Herrera, Il-Yeon Park, Luis Vil-\\nlar, and Andrew Farlow. Early warning signal for dengue\\noutbreaks and identification of high risk areas for dengue\\nfever in colombia using climate and non-climate datasets.\\nBMC Infectious Diseases, 17(1):1–11, 2017.\\n[Li et al., 2022a] Zhichao Li, Helen Gurgel, Lei Xu, Lin-\\nsheng Yang, and Jinwei Dong. Improving dengue fore-\\ncasts by using geospatial big data analysis in google earth\\nengine and the historical dengue information-aided long\\nshort term memory modeling. Biology, 11(2):169, 2022.\\n[Li et al., 2022b] Zhiwei Li, Huanfeng Shen, Qihao Weng,\\nYuzhuo Zhang, Peng Dou, and Liangpei Zhang. Cloud and\\ncloud shadow detection for optical satellite imagery: Fea-\\ntures, algorithms, validation, and prospects. ISPRS Jour-\\nnal of Photogrammetry and Remote Sensing, 188:89–108,\\n2022.\\n[Lim et al., 2020] Jue Tao Lim, Borame Sue Lee Dick-\\nens, Lawrence Zheng Xiong Chew, Esther Li Wen\\nChoo, Joel Ruihan Koo, Joel Aik, Lee Ching Ng, and\\nAlex R Cook.\\nImpact of sars-cov-2 interventions on\\ndengue transmission.\\nPLoS neglected tropical diseases,\\n14(10):e0008719, 2020.\\n[Livelo and Cheng, 2018] Evan Dennison Livelo and Chari-\\nbeth Cheng. Intelligent dengue infoveillance using gated\\nrecurrent neural learning and cross-label frequencies. In\\n2018 IEEE International Conference on Agents (ICA),\\npages 2–7. IEEE, 2018.\\n[Ltd, 2022] Sinergise Ltd. Sentinel-2 L2A about sentinet-2\\nl2a data. https://www.sentinel-hub.com/, 2022. Accessed:\\n2022-08-13.\\n[Maas et al., 2013] Andrew L Maas, Awni Y Hannun, An-\\ndrew Y Ng, et al. Rectifier nonlinearities improve neu-\\nral network acoustic models. In Proc. icml, volume 30,\\npage 3. Atlanta, Georgia, USA, 2013.\\n[Mala and Jat, 2019] Shuchi Mala and Mahesh Kumar Jat.\\nGeographic information system based spatio-temporal\\ndengue fever cluster analysis and mapping. The Egyptian\\nJournal of Remote Sensing and Space Science, 22(3):297–\\n304, 2019.\\n[Martheswaran et al., 2022] Tarun\\nKumar\\nMartheswaran,\\nHamida Hamdi, Amal Al-Barty, Abeer Abu Zaid, and\\nBiswadeep Das.\\nPrediction of dengue fever outbreaks\\nusing climate variability and markov chain monte carlo\\ntechniques in a stochastic susceptible-infected-removed\\nmodel. Scientific Reports, 12(1):5459, 2022.\\n[Morgan et al., 2021] Jasmine Morgan, Clare Strode, and\\nJ Enrique Salcedo-Sora.\\nClimatic and socio-economic\\nfactors supporting the co-circulation of dengue, zika and\\nchikungunya in three different ecosystems in colombia.\\nPLoS Neglected Tropical Diseases, 15(3):e0009259, 2021.\\n[Moskola¨ı et al., 2021] Waytehad Rose Moskola¨ı, Wahabou\\nAbdou, and Albert Dipanda. Application of deep learning\\narchitectures for satellite image time series prediction: A\\nreview. Remote Sensing, 13(23):4822, 2021.\\n[Mu˜noz et al., 2021] Estefan´ıa Mu˜noz,\\nGerm´an Poveda,\\nM Patricia Arbel´aez, and Iv´an D V´elez. Spatiotemporal\\ndynamics of dengue in colombia in relation to the com-\\nbined effects of local climate and enso.\\nActa Tropica,\\n224:106136, 2021.\\n[National Institute of Health of Colombia, 2010] National\\nInstitute of Health of Colombia.\\nComportamiento\\nepidemiol´ogico del dengue en colombia a˜no 2010.\\nhttp://www.ins.gov.co/buscador-eventos/Paginas/\\nInfo-Evento.aspx, 2010. Accessed: 2022-08-13.\\n[Nazer et al., 2023] Lama H Nazer, Razan Zatarah, Shai\\nWaldrip, Janny Xue Chen Ke, Mira Moukheiber, Ashish K\\nKhanna, Rachel S Hicklen, Lama Moukheiber, Dana\\nMoukheiber, Haobo Ma, et al. Bias in artificial intelligence\\nalgorithms and recommendations for mitigation.\\nPLOS\\nDigital Health, 2(6):e0000278, 2023.\\n[Ndabarora et al., 2014] Eleazar\\nNdabarora,\\nJennifer\\nA\\nChipps, and Leana Uys.\\nSystematic review of health\\ndata quality management and best practices at commu-\\nnity and district levels in lmic. Information Development,\\n30(2):103–120, 2014.\\n[PAHO, 2022] PAHO.\\nDengue.\\nhttps://www.paho.org/en/\\ntopics/dengue, 2022. Accessed: 2022-08-13.\\n[Ren et al., 2021] Xiaoli Ren, Xiaoyong Li, Kaijun Ren,\\nJunqiang Song, Zichen Xu, Kefeng Deng, and Xiang\\nWang. Deep learning-based weather prediction: a survey.\\nBig Data Research, 23:100178, 2021.\\n[Rogers et al., 2002] David J Rogers, Sarah E Randolph,\\nRobert W Snow, and Simon I Hay. Satellite imagery in\\nthe study and forecast of malaria. Nature, 415(6872):710–\\n715, 2002.\\n[Roster and Rodrigues, 2021] Kirstin\\nRoster\\nand\\nFran-\\ncisco A Rodrigues.\\nNeural networks for dengue\\nprediction:\\na systematic review.\\narXiv preprint\\narXiv:2106.12905, 2021.\\n[Salim et al., 2021] Nurul Azam Mohd Salim, Yap Bee\\nWah, Caitlynn Reeves, Madison Smith, Wan Fairos Wan\\nYaacob, Rose Nani Mudin, Rahmat Dapari, Nik Nur\\nFatin Fatihah Sapri, and Ubydul Haque.\\nPrediction\\nof dengue outbreak in selangor malaysia using machine\\nlearning techniques. Scientific reports, 11(1):1–9, 2021.\\n[Shragai et al., 2022] Talya Shragai, Juliana P´erez-P´erez,\\nMarcela del Pilar Quimbayo-Forero, Ra´ul Rojo, Laura C.\\nHarrington, and Guillermo R´ua-Uribe. Distance to pub-\\nlic transit predicts spatial distribution of dengue virus\\nincidence in medell´ın, colombia.\\nScientific Reports,\\n12(1):8333, May 2022.\\n[Son and Thong, 2017] Le Hoang Son and Pham Huy\\nThong. Some novel hybrid forecast methods based on pic-\\nture fuzzy clustering for weather nowcasting from satellite\\nimage sequences. Applied Intelligence, 46(1):1–15, 2017.\\n[Van Griethuysen et al., 2017] Joost JM Van Griethuysen,\\nAndriy Fedorov, Chintan Parmar, Ahmed Hosny, Nicole\\nAucoin, Vivek Narayan, Regina GH Beets-Tan, Jean-\\nChristophe Fillion-Robin, Steve Pieper, and Hugo JWL\\nAerts.\\nComputational radiomics system to decode the\\nradiographic phenotype. Cancer research, 77(21):e104–\\ne107, 2017.\\n[Watts et al., 2020] Matthew J Watts, Panagiota Kotsila,\\nP Graham Mortyn, Victor Sarto i Monteys, and Cesira\\nUrzi Brancati. Influence of socio-economic, demographic\\nand climate factors on the regional distribution of dengue\\nin the united states and mexico. International journal of\\nhealth geographics, 19(1):1–15, 2020.\\n[WHO, 2022] WHO.\\nDengue\\nand\\nsevere\\ndengue.\\nhttps://www.who.int/news-room/fact-sheets/detail/\\ndengue-and-severe-dengue, 2022. Accessed: 2022-08-13.\\n[Wu et al., 2020] Bichen Wu, Chenfeng Xu, Xiaoliang Dai,\\nAlvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi\\nTomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Va-\\njda. Visual transformers: Token-based image representa-\\ntion and processing for computer vision, 2020.\\n'},\n",
       " {'abstract': \"Open information extraction (OpenIE), a task focused on the schema-free extraction of triplets from natural language text, poses various challenges, including complicated triplet structures, overlapping elements, and implicit triplets. In this paper, we introduce DualOIE, a novel generative OpenIE model which incorporates a dual task of converting triplets into sentences to address these complexities and enhance extraction performance. By leveraging the duality, DualOIE effectively recognizes the sentence structure, tackles complicated triplet patterns, and learns diverse relations between arguments, leading to improved extraction results. We establish a new benchmark, MTOIE, for implicit triplet extraction containing a significant number of implicit triplets and various predicate types. Extensive experiments on two benchmarks and our MTOIE demonstrate the superiority of DualOIE over the state-of-the-art baselines and ChatGPT. The conducted online A/B test further verifies DualOIE's utility in improving the performance of a real-world search system.\",\n",
       "  'introduction': 'Open information extraction (OpenIE), a task of extracting schema-free triplets in the form of (subject, predicate, object) from unstructured natural language, has gained significant attention for its wide range of downstream applications. Traditional OpenIE methods, such as tagging-based and generative solutions, face challenges in dealing with complicated triplets, including overlapping elements, implicit triplets, and those with intricate structures. To address these limitations, we propose DualOIE, a novel generative OpenIE model that incorporates a dual task of transforming triplets into sentences.',\n",
       "  'literature review': 'Previous approaches to OpenIE have employed tagging-based and generative solutions. Tagging-based models treat OpenIE as a sequence labeling problem, while generative approaches view it as a Seq2Seq task. Notable tagging-based systems include RnnOIE, SpanOIE, and MacroIE. Generative solutions comprise models like NOIE, IMoJIE, Gen2OIE, and ChatGPT. While these methods have shown promise, they often struggle with complicated triplet extraction, particularly when handling implicit triplets and managing the structure of the input sentence.',\n",
       "  'methodology': \"DualOIE consists of a dual framework with two task directions, namely S → T and T → S, where S denotes the sentence and T represents the triplets. A shared encoder enables the model to learn from both tasks jointly. In the S → T direction, predicate extraction is first performed, followed by triplet generation using the extracted predicates as prompts. This approach helps alleviate repetition and omission issues commonly encountered in generative models for OpenIE. The T → S direction promotes the model's understanding of the sentence structure and aids in resolving complicated triplet structures and implicit triplets.\",\n",
       "  'results': 'We evaluate DualOIE extensively on two benchmark datasets, CaRB and SAOKE, as well as our constructed MTOIE dataset. The results indicate that DualOIE outperforms all baselines, including ChatGPT, across both explicit and implicit triplet extraction tasks. Analyses reveal that DualOIE effectively tackles complicated triplets, achieves a balance between the two tasks in the dual framework, and exhibits a correlation between the quality of sentence reconstruction and triplet extraction.',\n",
       "  'conclusion': 'DualOIE, a generative OpenIE model utilizing a dual task of converting triplets into sentences, demonstrates superior performance on both public benchmarks and our constructed dataset. By incorporating the dual task, DualOIE improves its understanding of sentence structure, handles complicated triplets more effectively, and learns diverse relations between arguments. The positive correlation between the quality of triplet extraction and sentence reconstruction highlights the mutual benefits of the dual task approach. Furthermore, the online A/B test conducted on the Meituan platform showcases the practical utility of DualOIE in enhancing the performance of a real-world search system.',\n",
       "  'title': 'Exploiting Duality in Open Information Extraction with Predicate Prompt',\n",
       "  'author': 'Zhen Chen, Jingping Liu, Deqing Yang, Yanghua Xiao, Huimin Xu, Zongyu Wang, Rui Xie, Yunsen Xian',\n",
       "  'textdata': 'Exploiting Duality in Open Information Extraction\\nwith Predicate Prompt\\nZhen Chen\\nzhenchen21@m.fudan.edu.cn\\nFudan University,\\nShanghai Key Laboratory of Data\\nScience\\nShanghai, China\\nJingping Liu∗\\njingpingliu@ecust.edu.cn\\nEast China University of Science and\\nTechnology\\nShanghai, China\\nDeqing Yang∗\\nyangdeqing@fudan.edu.cn\\nFudan University,\\nShanghai Key Laboratory of Data\\nScience\\nShanghai, China\\nYanghua Xiao\\nshawyh@fudan.edu.cn\\nFudan University,\\nShanghai Key Laboratory of Data\\nScience\\nShanghai, China\\nHuimin Xu\\nxuhuimin04@meituan.com\\nMeituan\\nShanghai, China\\nZongyu Wang\\nwangzongyu02@meituan.com\\nMeituan\\nShanghai, China\\nRui Xie\\nrui.xie@meituan.com\\nMeituan\\nShanghai, China\\nYunsen Xian\\nxianyunsen@meituan.com\\nMeituan\\nShanghai, China\\nABSTRACT\\nOpen information extraction (OpenIE) aims to extract the schema-\\nfree triplets in the form of (subject, predicate, object) from a given sen-\\ntence. Compared with general information extraction (IE), OpenIE\\nposes more challenges for the IE models, especially when multiple\\ncomplicated triplets exist in a sentence. To extract these compli-\\ncated triplets more effectively, in this paper we propose a novel\\ngenerative OpenIE model, namely DualOIE, which achieves a dual\\ntask at the same time as extracting some triplets from the sentence,\\ni.e., converting the triplets into the sentence. Such dual task encour-\\nages the model to correctly recognize the structure of the given\\nsentence and thus is helpful to extract all potential triplets from the\\nsentence. Specifically, DualOIE extracts the triplets in two steps: 1)\\nfirst extracting a sequence of all potential predicates, 2) then using\\nthe predicate sequence as a prompt to induce the generation of\\ntriplets. Our experiments on two benchmarks and our dataset con-\\nstructed from Meituan demonstrate that DualOIE achieves the best\\nperformance among the state-of-the-art baselines. Furthermore, the\\nonline A/B test on Meituan platform shows that 0.93% improvement\\nof QV-CTR and 0.56% improvement of UV-CTR have been obtained\\nwhen the triplets extracted by DualOIE were leveraged in Meituan’s\\nsearch system.\\n∗Corresponding Author.\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nWSDM ’24, March 4–8, 2024, Merida, Mexico\\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-0371-3/24/03...$15.00\\nhttps://doi.org/10.1145/3616855.3635799\\nCCS CONCEPTS\\n• Computing methodologies → Information extraction.\\nKEYWORDS\\nOpenIE, dual task, prompt, generative model.\\nACM Reference Format:\\nZhen Chen, Jingping Liu, Deqing Yang∗, Yanghua Xiao, Huimin Xu, Zongyu\\nWang, Rui Xie, and Yunsen Xian. 2024. Exploiting Duality in Open In-\\nformation Extraction with Predicate Prompt. In Proceedings of the 17th\\nACM International Conference on Web Search and Data Mining (WSDM\\n’24), March 4–8, 2024, Merida, Mexico. ACM, New York, NY, USA, 10 pages.\\nhttps://doi.org/10.1145/3616855.3635799\\n1\\nINTRODUCTION\\nOpen information extraction (OpenIE) plays an important role in a\\nvariety of downstream tasks, such as knowledge base construction\\n[8], question answering [26] and summarization [5]. OpenIE aims\\nto extract schema-free triplets in the form of (subject, predicate,\\nobject) from unstructured natural language, where subjects and\\nobjects are both called arguments.\\nIn recent years, most OpenIE systems were devised based on\\ndeep neural networks (DNNs), which could be divided into two\\nmain categories: tagging-based and generative methods [29]. The\\ntagging-based solutions [21] model OpenIE as a sequence labeling\\nproblem, where each token in the input is tagged as a subject, predi-\\ncate or object. The generative solutions model OpenIE as a Seq2Seq\\nproblem. For example, IMoJIE [14] adopts an iterative generation\\nmechanism to alleviate redundancy in extractions.Gen2OIE [15]\\nleverages two mT5 models [25] to extract triplets by reconstructing\\nmultiple inputs. However, the previous models’ inadequate under-\\nstanding of diverse relations between arguments and the sentence’s\\nintricate structure hinder them to extract the complicated triplets\\narXiv:2401.11107v1  [cs.CL]  20 Jan 2024\\nWSDM ’24, March 4–8, 2024, Merida, Mexico\\nZhen Chen et al.\\nTable 1: A toy example of OpenIE where three triplets can be\\nextracted from the input sentence.\\nSentence: Shea was born on September 5, 1900 in San Francisco, California.\\nTriplets:\\n1. (Shea, was born on, [September 5, 1900])\\n2. (Shea, was born in, [San Francisco, California])\\n3. (San Francisco, is in, California)\\neffectively, which include the following three categories [27]. 1)\\nOverlapping triplets refer to those triplets sharing the same element,\\nas the first two triplets in Table 1 sharing the same subject “Shea”;\\n2) Discontinuous triplet has the element composed by two separate\\nspans, as the second triplet’s predicate in Table 1 that composed of\\n“was born” and “in” from the input; 3) Two triplets are regarded as\\nnested triplet , if an element in one triplet contains other element\\nor shares some words with the other triplet’s element. For example,\\nthe predicates of the first two triplets in Table 1 share the same\\nword “was born”. In addition, the complicated triplets also include\\nimplicit triplets that are overlooked by current OpenIE research.\\nThe predicate in an implicit triplet is not explicitly mentioned in\\nthe sentence. As shown in Table 1, the predicate “is in” of the third\\ntriplet is absent from the input. Implicit triplets often emerge in\\nincomplete sentences of oral expression, such as user comments\\nin real-world scenarios. Thus, the inefficiency in extracting im-\\nplicit triplets would degrade the downstream task’s performance\\nseverely.\\nTo address the challenges in OpenIE posed by these complicated\\ntriplets, in this paper we propose a novel generative OpenIE model\\nDualOIE, which achieves enhanced OpenIE mainly through learning\\nan auxiliary dual task of converting the triplets into a sentence.\\nSince tagging model could not introduce new words, resulting\\nin inefficiency in handling implicit triplets, our DualOIE adopts\\nthe paradigm of generation instead of tagging. Trained under the\\nconstraint of the dual task, our model is encouraged to capture\\nthe diverse relations between the arguments and recognize the\\noverlapping elements in the overlapping triplet by figuring out how\\nthese elements fit into the context of the input sentence. In addition,\\nthe dual task also encourages the model to be more aware of word\\norder and sentence structure, thus helping the model resolve the\\ndiscontinuous triplet and nested triplet in the input sentence. For\\nimplicit triplets, transforming them into sentence enables the model\\nto comprehend and learn those informal expressions, leading to\\nmore accurate identification of implicit triplets in the extraction\\ntask.\\nIn summary, our DualOIE is built with a dual framework with\\ntwo task directions: the S (sentence) to T (triplet) direction corre-\\nsponds to the primary extraction task, while the T (triplet) to S\\n(sentence) direction corresponds to the dual task. The two direc-\\ntions share the same encoder, enabling the model to jointly learn\\nfrom the two tasks. Specifically, in the S to T direction, given the\\nsentence’s complicated structure, directly using a Seq2Seq model to\\ngenerate triplets from the input may result in incorrect output, such\\nas repetition or omission. Therefore, we split the primary extrac-\\ntion process (S to T) into two steps: 1) first extracting a sequence\\nof potential predicates from the input; 2) then using the predicate\\nsequence as a prompt and concatenating it with the input to gener-\\nate all triplets. Note that a subject of a triplet might be the object of\\nanother triplet, causing ambiguity in the second step, thus we first\\nextract predicates as the prompt rather than extracting subjects or\\nobjects as the prompt.\\nOne important reason of previous OpenIE models’ inefficiency in\\nextracting implicit triplets is that, they are trained with the datasets\\nonly having less implicit triplets. According to our statistics, there\\nare only 3% and 11% implicit triplets in the OpenIE benchmark\\nCaRB and SAOKE, respectively. Besides, the two datasets only have\\nless than 30 types of implicit predicates, which also limits the per-\\nformance of the models trained with them. The sparsity of implicit\\ntriplets in these two benchmarks is due to that, they were collected\\nfrom Wikipedia and Baidu Baike, where the expressions in sen-\\ntences are more formalized and complete than oral expressions in\\nvarious Web platforms such as Meituan, which is a famous Chi-\\nnese review and search platform for daily life. In order to train\\nand evaluate OpenIE models towards implicit triplet extraction, we\\nconstructed a dataset MTOIE (MeituanOpenIE) from the real-world\\nscenario of Meituan. MTOIE was collected from the massive user re-\\nviews on Meituan, annotated following the philosophy of previous\\nOpenIE benchmarks and crowdsourced by experienced workers. To\\nsum up, MTOIE has 54,060 sentences and 87,971 triplets, in which\\nthere are about 33% implicit triplets and more than 2,000 types of\\nimplicit predicates.\\nThis paper’s contributions are summarized as follows:\\n1. We propose a novel OpenIE model based on a joint dual frame-\\nwork with two task directions, which enables the model to learn\\nunder the mutual constraints of both tasks and further use pred-\\nicate prompt to alleviate the omission and repetition in the S to\\nT direction. To the best of our knowledge, this is the first work to\\nintroduce duality in OpenIE.\\n2. To evaluate OpenIE models on complicated triplet extraction,\\nwe also construct a high-quality dataset MTOIE that contains more\\nthan 29,000 implicit triplets and more than 2,000 types of implicit\\npredicates, which exceeds current benchmarks by a significant\\nmargin in terms of both quantity and variety.\\n3. We conduct extensive experiments on two benchmark datasets\\nand our MTOIE, to verify our model’s advantage over all base-\\nlines, including ChatGPT. Furthermore, the online A/B test on the\\nMeituan platform shows that the query view click-through rate\\n(QV-CTR) and unique visitor click-through rate (UV-CTR) have\\nincreased by 0.93% and 0.56% respectively when our model’s ex-\\ntraction results were deployed into the search system.\\nThe source code of DualOIE will be soon available at https://github.com/\\nccczhen/DualOIE.\\n2\\nOVERVIEW\\nIn this section, we first formalize the problem of OpenIE task ad-\\ndressed in this paper, and then present the framework of our pro-\\nposed DualOIE.\\n2.1\\nProblem Definition\\nGiven an input sentence 𝑥 = [𝑤1, ...,𝑤𝑛], where 𝑤𝑖 (1≤𝑖≤𝑛) is the\\n𝑖-th word (token), our task’s goal is to extract a collection of triplets\\n𝑌={(𝑠𝑖, 𝑝𝑖,𝑜𝑖)}|𝑌 |\\n𝑖=1 where 𝑠𝑖, 𝑝𝑖,𝑜𝑖 denote the subject term, predicate\\nExploiting Duality in Open Information Extraction\\nwith Predicate Prompt\\nWSDM ’24, March 4–8, 2024, Merida, Mexico\\nTriplet\\nDecoder\\nPredicate\\nDecoder\\nText\\nDecoder\\nSentence\\nInput 𝒙𝒙𝑷𝑷\\nConcatenated\\nInput 𝒙𝒙𝑻𝑻\\nPredicate\\nPrompt 𝒚𝒚𝑷𝑷\\nTriplets\\nOutput 𝒚𝒚𝑻𝑻\\nTriplets\\nInput 𝒙𝒙𝑺𝑺\\nSentence\\nOutput 𝒚𝒚𝑺𝑺\\nConcatenation\\n+\\nTransformer Block 1\\nTransformer Block L\\n. . .\\n. . .\\nShared Encoder\\n𝜶𝜶𝓛𝓛𝑷𝑷\\n𝜷𝜷𝓛𝓛𝑻𝑻\\n𝜸𝜸𝓛𝓛𝑺𝑺\\n+\\n𝓛𝓛𝑺𝑺→𝑻𝑻\\n𝓛𝓛𝑺𝑺→𝑻𝑻\\nS→T\\nStep 2\\nT→S\\n𝐇𝐇𝑃𝑃\\n𝐇𝐇𝑇𝑇\\n𝐇𝐇𝑆𝑆\\n𝐇𝐇𝑃𝑃\\n𝑑𝑑\\n𝐇𝐇𝑆𝑆\\n𝑑𝑑\\n𝐇𝐇𝑇𝑇\\n𝑑𝑑\\nStep 1\\n𝒙𝒙𝑺𝑺\\n𝒙𝒙𝑻𝑻\\n𝒙𝒙𝑷𝑷\\n𝒚𝒚𝑷𝑷\\n𝒚𝒚𝑻𝑻\\n𝒚𝒚𝑺𝑺\\nFigure 1: The overall framework of our proposed DualOIE, including the structure of achieving two tasks of opposite directions,\\nS → T and T → S.\\nterm, object term of the 𝑖-th triplet, respectively. Generally, 𝑠 and\\n𝑜 might be a single word or a span extracted from 𝑥, and 𝑝 might\\neven be absent in 𝑥. For example, given “Joe Biden (November 20,\\n1942- ) is the U.S. president.\", a good model needs to output two\\ntriplets, i.e., (Joe Biden, is, the U.S. president.) and (Joe Biden, was\\nborn on, [November 20, 1942]). Note that the predicate of the second\\ntriplet is absent in the input sentence.\\n2.2\\nFramework\\nGiven an input sentence 𝑥, the primary objective of our OpenIE\\nmodel is to maximize the following probability\\n𝑝(𝑌 |𝑥),\\n(1)\\nwhere 𝑌 is the collection of fact triplets. It is in fact the task of\\nS to T direction mentioned before, i.e., extracting triplets from\\nthe input sentence. This direction contains two steps: 1) a prompt\\n𝑧 = [𝑝1, ..., 𝑝|𝑌 |] is first extracted from 𝑥 as a prompt, where 𝑝𝑖\\n(1≤𝑖≤|𝑌 |) is the predicate of the 𝑖-th triplet in 𝑌; 2) then 𝑥 is con-\\ncatenated with 𝑧 as [𝑧;𝑥] to generate 𝑌.\\nAs the dual task of S to T, the T to S direction aims to convert\\nthe triplets into a sentence, of which the objective is to maximize\\n𝑝(𝑥|𝑌).\\n(2)\\nIn our DualOIE, these two directions have their respective de-\\ncoders but are combined with a shared encoder. During DualOIE’s\\ntraining, the overall loss is defined as:\\nL = L𝑆→𝑇 + L𝑇→𝑆,\\n(3)\\nwhere L𝑆→𝑇 and L𝑇→𝑆 are the loss of S to T and T to S (see\\nSection 4.1 for details), respectively.\\n3\\nMETHODOLOGY\\nIn this section, we describe our DualOIE’s details for the two task\\ndirections. The overall structure of DualOIE is depicted in Fig. 1.\\n3.1\\nSentence to Triplets\\nAs mentioned before, this direction is the primary task of our model\\nwhich aims to generate triplets 𝑌 based on the input sentence 𝑥.\\nWe introduce the two steps of this direction in turn as follows.\\nSpecifically, the triplet extraction is divided into two steps: 1) extract\\nthe predicate sequence from the input. 2) use the predicate sequence\\nto induce the generation of triplets.\\n3.1.1\\nPredicate Extraction. This first step aims to extract a sequence\\nof predicates 𝑦𝑃 from the sentence 𝑥. Predicate extraction is de-\\nsigned upon a Transformer encoder of 𝐿 blocks, along with a predi-\\ncate decoder which is also a Transformer decoder.\\nFormally, we first concatenate the input sentence 𝑥 with two\\nspecial tokens to construct this step’s input as:\\n𝑥𝑃 = [< sen >,𝑤1, ...,𝑤𝑛, < /sen >].\\n(4)\\nBased on the encoder, we calculate the hidden state of 𝑥𝑃 as:\\nH𝑃 = Encoder(𝑥𝑃).\\n(5)\\nThen, we use the predicate decoder to generate the predicate se-\\nquence 𝑦𝑃 in an auto-regressive way:\\n𝑦𝑖, h𝑑\\n𝑖 = Decoder𝑃 ([H𝑃; h𝑑\\n1, ..., h𝑑\\n𝑖−1]),\\n(6)\\nwhere 𝑦𝑖 is the 𝑖-th token in 𝑦𝑃 and h𝑑\\n𝑖 is its decoder state. Please\\nnote that the extracted predicate sequence 𝑦𝑃 might consist of\\nmultiple predicates, each of which is denoted as 𝑝𝑖 and corresponds\\nto one triplet. Thus, we use <rel> and </rel> to split them as below:\\n𝑦𝑃 = [<rel>, 𝑝1, </rel>, ..., <rel>, 𝑝|𝑌 |, </rel>].\\n(7)\\nFinally, the loss function of predicate extraction is defined as:\\nL𝑃 =\\n∑︁\\n(𝑥𝑃,𝑦𝑃 )∈Ω𝑃\\n− log𝑝(𝑦𝑃 |𝑥𝑃;𝜃𝑒,𝜃𝑑\\n𝑃),\\n(8)\\nwhere Ω𝑃 is the training set of all sentence-predicates pairs, 𝜃𝑒 and\\n𝜃𝑑\\n𝑃 are the parameters of the encoder and the predicate decoder,\\nrespectively.\\n3.1.2\\nTriplet Extraction. This step aims to generate the triplets\\nsequence 𝑌 based on 𝑥𝑃 and 𝑦𝑃. Similar to the previous step, the\\ntriplet extraction also consists of an encoder and a triplet decoder.\\nNote that the encoder is shared by the predicate extraction as well.\\nFormally, given 𝑥𝑃 and 𝑦𝑃 from the former step, we take 𝑦𝑃 as\\na prompt and concatenate it with 𝑥𝑃, to construct the input 𝑥𝑇 of\\nthis step as:\\n𝑥𝑇 = [𝑦𝑃;𝑥𝑃].\\n(9)\\nWSDM ’24, March 4–8, 2024, Merida, Mexico\\nZhen Chen et al.\\nThen, we use the shared encoder to get 𝑥𝑇 ’s hidden state H𝑇 , and\\ndecode the target sequence 𝑦𝑇 based on H𝑇 in the same way of Eq.\\n(6).\\nTo split each extracted triplet, we utilize three pairs of special\\ntokens to formulate a single triplet 𝑡𝑖 in 𝑦𝑇 as:\\n𝑡𝑖 = [<sub>,𝑠𝑖, </sub>, <rel>, 𝑝𝑖, </rel>, <obj>,𝑜𝑖, </obj>].\\n(10)\\nSimilar to L𝑃 defined in 8, the loss function of the triplet extraction\\nL𝑇 is defined as:\\nL𝑇 =\\n∑︁\\n(𝑥𝑇 ,𝑦𝑇 )∈Ω𝑇\\n− log𝑝(𝑦𝑇 |𝑥𝑇 ;𝜃𝑒,𝜃𝑑\\n𝑇 ).\\n(11)\\nNote that the predicate prompt 𝑦𝑃 enables DualOIE to avoid\\niterative generation and output all triplets in a single decoding\\nprocess on the second step. In contrast, the number of decoding\\nsteps in IMoJIE and Gen2OIE depends on the number of triplets\\ncorresponding to the input. In addition, it prompts the model to\\ngenerate triplets of the same number as the predicates.\\n3.2\\nTriplets to Sentence\\nThis direction aims to reconstruct a sentence 𝑥 from the extracted\\ntriplets 𝑌, which is composed of an encoder and a text decoder.\\nNote that the encoder is shared by the S to T direction as well.\\nWe formulate the input of this direction as:\\n𝑥𝑆 = 𝑦𝑇 = [𝑡1, ...,𝑡|𝑌 |].\\n(12)\\nNext, we derive the hidden state H𝑆 of 𝑥𝑆 from the shared encoder.\\nThen, we use the text decoder to decode the target sequence 𝑦𝑆\\nin the same way of Eq. (6). And the output of this direction 𝑦𝑆 is\\ndefined in the same way as 𝑥𝑃 in Eq. (4).\\nFinally, the loss function is defined as:\\nL𝑆 =\\n∑︁\\n(𝑥𝑆,𝑦𝑆 )∈Ω𝑆\\n− log𝑝(𝑦𝑆 |𝑥𝑆;𝜃𝑒,𝜃𝑑\\n𝑆 ).\\n(13)\\nNote that the shared encoder would be optimized toward both\\ndirections, which enables DualOIE to benefit from the dual task\\nsimultaneously. In addition, the order of triplets in 𝑥𝑆 is the same\\nas 𝑌, i.e., the appearance order of the predicate in the sentence.\\n4\\nMODEL TRAINING AND INFERENCE\\nIn this section, we first introduce the training details of loss func-\\ntions. Then, we describe the inference process of our model.\\n4.1\\nJoint Training\\nWe employ a joint training way to S to T direction and T to S. The\\nloss function of DualOIE is defined as:\\nL = L𝑆→𝑇 + L𝑇→𝑆,\\nL𝑆→𝑇 = 𝛼L𝑃 + 𝛽L𝑇,\\nL𝑇→𝑆 = 𝛾L𝑆,\\n(14)\\nwhere L𝑆→𝑇 and L𝑇→𝑆 are the loss of S to T and T to S. L𝑃, L𝑇\\nand L𝑆 are the loss of the predicate extraction, triplet extraction\\nand sentence reconstruction, respectively. In addition, we also use\\nhyper-parameters 𝛼, 𝛽 and 𝛾 to balance different objectives. As a\\nresult, the overall loss of DualOIE is\\nL = 𝛼L𝑃 + 𝛽L𝑇 + 𝛾L𝑆.\\n(15)\\n4.2\\nModel Inference\\nWhen our model is trained well by the task of S to T and T to S, we\\nonly need to use S to T to solve the triplet extraction task inference\\nphase. Finally, we split 𝑦𝑇 into the triplets of 𝑌 by special tokens.\\n5\\nEXPERIMENTS\\nIn this section, we display the results of our extensive experiments\\nto verify DualOIE’s advantage, based on which we also provide a\\ndetailed analysis.\\n5.1\\nDatasets\\nCaRB[4] is a crowdsourced English benchmark in OpenIE. How-\\never, it does not contain a training set due to the annotation cost,\\nso we use the automatically annotated training data produced by\\nIMoJIE [14]. SAOKE[23] is a large-scale human-annotated Chinese\\ndataset. Note that 11% triplets in SAOKE are implicit. The statistics\\nof two datasets are reported in Table 2.\\n5.2\\nBaselines\\nWe compared our model against several recent neural OpenIE mod-\\nels, which could be divided into tagging models and generative\\nmodels.\\n5.2.1\\nTagging Models. RnnOIE [21] takes predicate head with the\\ninput together and outputs the tags indicating the token classes.\\nSpanOIE [28] uses BiLSTM to derive the representation of a span,\\nand then decoders tags from span representations. IGL-OIE [13]\\nproposes an iterative grid labelling system to predict tag sequences.\\nMacroIE [27] builds a fact graph based on token spans, and decodes\\nthe graph into fact triplets during the inference process.\\n5.2.2\\nGenerative Models. NOIE [6] is built with an encoder-decoder\\nstructure based on stacked LSTM, where both the copy mechanism\\nand attention mechanism are applied. We provided an advanced\\nversion of NOIE in our comparisons, of which the LSTM encoder\\nis replaced with BERT. IMoJIE [14] is also composed of a BERT en-\\ncoder and an LSTM decoder, and it leverages an iterative generation\\nmechanism to alleviate redundant extraction. Gen2OIE [15] is an\\nOpenIE system based on two mT5 models. Given that ChatGPT is a\\npowerful large language model (LLM) capable of producing contex-\\ntually appropriate and logically connected responses, We designed\\nthe prompts of 0-shot, 3-shot and Chain-Of-Thought (COT) [24] to\\ninstruct ChatGPT to achieve extraction task. Note that examples\\nwere randomly selected in the 3-shot setting.\\n5.3\\nEvaluation Metrics\\nF1(1-1) [4], is a token level scorer, which creates a label-prediction\\nmatching table and then computes precision and recall between\\nTable 2: The statistics of CaRB and SAOKE.\\nCaRB\\nSAOKE\\n# Sentences\\n# Triplets\\n# Sentences\\n# Triplets\\nTrain\\n92,650\\n180,689\\n28,238\\n73,232\\nTest\\n634\\n2,715\\n1,569\\n4,175\\nExploiting Duality in Open Information Extraction\\nwith Predicate Prompt\\nWSDM ’24, March 4–8, 2024, Merida, Mexico\\nTable 3: Main results on CaRB and SAOKE.\\nModel\\nCaRB\\nSAOKE\\nF1(1-1)\\nF1\\nF1(1-1)\\nF1\\nSpanOIE\\n37.9\\n48.5\\n38.6\\n41.2\\nRnnOIE\\n39.5\\n49.0\\n39.9\\n42.7\\nIGL-OIE\\n41.1\\n52.4\\n42.4\\n44.5\\nMacroIE\\n43.5\\n54.8\\n43.7\\n45.6\\nNOIE+BERT\\n38.7\\n51.6\\n50.2\\n51.3\\nIMoJIE\\n41.4\\n53.5\\n52.4\\n54.3\\nGen2OIE\\n43.4\\n54.6\\n53.6\\n55.4\\nChatGPT (0-shot)\\n39.5\\n50.3\\n51.9\\n53.7\\nChatGPT (3-shot)\\n40.7\\n51.1\\n52.6\\n54.3\\nChatGPT (COT)\\n42.2\\n53.4\\n54.0\\n55.9\\nDualOIE w/o D\\n50.3\\n54.2\\n56.4\\n57.7\\nDualOIE w/o P\\n50.9\\n55.5\\n56.8\\n58.0\\nDualOIE\\n51.5\\n56.3\\n58.1\\n59.5\\nTable 4: A toy example of the ablation study of DualOIE.\\nSentence\\nHe is idolized , receiving the name of “God”.\\nDualOIE w/o D\\n(He , is idolized receiving, the name of “God”)\\nDualOIE w/o P\\n(He , receiving, the name of “God”)\\nDualOIE\\n(He, is, idolized) ; (He, is receiving, the name of “God”)\\nTable 5: The results of DualOIE on the two datasets with\\ndifferent extraction order in the task of S to T.\\nCaRB\\nSAOKE\\nModels\\nF1(1-1)\\nF1\\nF1(1-1)\\nF1\\nDualOIE-S\\n44.8\\n51.4\\n54.3\\n55.6\\nDualOIE-O\\n43.6\\n50.1\\n51.6\\n53.2\\nDualOIE\\n51.5\\n56.3\\n58.1\\n59.5\\neach pair. The overall precision and recall are computed through\\none-to-one mapping, where a gold triplet and a prediction could\\nmatch each other only once. F1 [13] is a variant of F1(1-1), where\\nthe recall is computed through multi-to-one mapping.\\n5.4\\nImplementation Detail\\nDuring the training, we set the loss coefficient 𝛼=0.4, 𝛽=0.2, 𝛾=0.6.\\nFor CaRB, we used T5 [17] encoder as the shared encoder, and T5\\ndecoder as the predicate decoder, triplet decoder and text decoder,\\nrespectively. For SAOKE, we applied the Chinese T5-pegasus [22].\\nWe trained our model using Pytorch on an NVIDIA Tesla V100\\nGPU with 32 GB dedicated memory. The network parameters were\\noptimized by Adam [12] with a learning rate of 2e-5. The batch\\nsize was fixed to 32. The total training time was 5 hours. The final\\ndisplayed results were reported as the average of the results of 5\\nrandom seeds.\\nOverlapping\\nDiscontinous\\nNested\\n50\\n55\\n60\\nF1 Score\\nDuOIE w/o D\\nDuOIE\\nDuOIE w/o P\\nFigure 2: Performance on extraction of complicated triplets\\nin SAOKE.\\n5.5\\nOverall Comparison Results\\nThe comparison results between our DualOIE and the baselines are\\nshown in Table 3. According to the results, we have the following\\nconclusions.\\n1) DualOIE achieves the best extraction performance on the two\\ndatasets under each metric, fully justifying the effectiveness of both\\nemploying the dual task in joint training and the predicate prompt\\nin extraction process.\\n2) On CaRB, there is always a gap of about 10% or more between\\nthe two metrics in the baselines, which is mainly caused by the low\\nquality of automatically-derived training data. DualOIE decreases\\nthis gap to 4.8%, indicating that a triplet well-predicted by it is less\\nmapped to several different gold triplets. In other words, the output\\nof DualOIE is much more precise and robust in one-to-one mapping\\nmetric.\\n3) On SAOKE, DualOIE outperforms the best baseline by a large\\nmargin. In addition, note that generative models perform better\\noverall than tagging models on SAOKE. This is because the implicit\\ntriplets in SAOKE were missed by the tagging models due to their\\ninability to introduce new words. In addition, the gap between the\\ntwo metrics is much smaller and more stable due to the high quality\\nof SAOKE.\\n5.6\\nIn-depth Investigations\\n5.6.1\\nAblation Study. To evaluate the impact of each component\\nin DualOIE, we compared it with two ablated variants: DualOIE\\nw/o D whose dual task is removed, and DualOIE w/o P in which the\\npredicate prompt is removed and the triplets are directly generated\\nby the decoder.\\nFrom the comparison results shown in Table 3, we find: 1) The\\ntwo ablated variants still have an advantage over most baselines,\\njustifying the effectiveness of incorporating either the predicate\\nprompt or the dual task. 2) The removal of the dual task leads to\\nan apparent performance drop on both datasets, i.e., (1.2%, 2.1%)\\non CaRB and (1.7%, 1.8%) on SAOKE. This proves the importance\\nof the dual task for enhanced extraction. And the removal of the\\npredicate prompt also decreases our model’s performance.\\nFig. 2 shows the comparison results on complicated cases, which\\nindicates that the introduction of duality could improve perfor-\\nmance on these challenges. Since the syntactic knowledge endowed\\nby the dual task is helpful to solve discontinuous and nested triplets,\\nwhich arise from complicated sentence structure. In addition, the\\nWSDM ’24, March 4–8, 2024, Merida, Mexico\\nZhen Chen et al.\\nYou are an OpenIE extractor. Your objective is to extract \\nfact triplets in the form of (subject, predicate, object) \\nfrom the given sentence. \\nI will give you an <Input sentence>, you should output \\ntriplets without numbering, i.e. \\n<(s1,p1,o1);(s2,p2,o2)...> \\nExamples:{examples}   Optional\\nInput sentence:{input sentence}\\nOutput triplets:\\nFew-shot Example\\nInput sentence: Shea was born on September 5, 1900 in \\nSan Francisco, California.\\nOutput triplets: The explicit predicates of input are < \\nwas born on> and < was born in>, and the implicit \\npredicate is <is>. Based on extracted predicates, the fact \\ntriplets are <(Shea, was born on, [September 5, 1900]); \\n(Shea, was born in, [San Francisco, California]); (San \\nFrancisco, is in, California)>\\nCOT Example\\nFigure 3: The prompts we designed to instruct ChatGPT to\\nperform the OpenIE task.\\ndual task encourages the model to capture diverse relations be-\\ntween arguments, which benefits the solving of overlapping. We\\nalso give a case example among ablated versions. As shown in Table\\n4, DualOIE correctly outputted both triplets. The removal of the\\nduality prevented the model from effectively learning the structure\\nof the sentence, resulting in confusion between the two triplets and\\noutputting an incorrect triplet. Additionally, removing the predicate\\nprompt caused the model to miss one of the triplets.\\n5.6.2\\nAnalysis on Extraction Order. We also conducted experiments\\nto investigate the effect of taking different terms in a triplet as the\\nprompt. Table 5 details the performance of various prompt settings,\\nwhere DualOIE-S(O) is the version where all subjects(objects) are\\nextracted at first as the prompt, and then concatenated with the\\ninput to generate the triplets. From the table, we observe that the\\npredicate prompt performs best followed by the subject prompt,\\nand the object prompt performs worst. It is mainly due to the\\nfact that a subject or object could appear in multiple triplets, and\\nsometimes a subject in one triplet might even be an object in another\\ntriplet. Such complicated situations would confuse the model during\\ntriplets generation, while the predicate prompt could reduce this\\nuncertainty since an extracted predicate corresponds to a certain\\ntriplet.\\n5.6.3\\nImpact of Loss Coefficient. As formalized in Eq. 15, the loss\\ncoefficient 𝛼 and 𝛽 indicate importance of the primary extraction\\ntask S to T, while 𝛾 indicates the importance of the dual task T\\nto S. Thus, to analyze the importance of two task directions, we\\ncompared 𝛼+𝛽 with𝛾 where\\n𝛾\\n𝛼+𝛽 was set to 0.5, 1 and 2, respectively.\\nIn addition, to analyze the importance of the two steps in S to T,\\nwe also compared 𝛼 with 𝛽 where the value ratio between them\\nwas set to 2, 3 and 4, respectively.\\nFrom Table 6, we have the following conclusions. 1) The case\\nof 𝛼 + 𝛽 = 𝛾 performs better, indicating that the dual task is as\\nimportant as the primary task, but the balance between the two\\ntasks should be kept. 2) the case of 𝛼 > 𝛽 shows its advantage,\\nindicating that the predicate extraction is tougher and thus requires\\nmore concentrations. Besides, more concentrations on the predicate\\nextraction could alleviate the cascading error in DualOIE’s pipeline-\\nbased extraction framework.\\n5.6.4\\nAnalysis on ChatGPT’s Performance. We have examined the\\nOpenIE performance of ChatGPT in three prompt settings: 0-shot,\\n3-shot and COT. Fig. 3 shows the main prompt and the COT ex-\\nample. Table 3’s results indicate that ChatGPT is still competitive\\nwith fine-tuned baselines even in the 0-shot scenario. Compared\\nwith 3-shot prompt, COT prompt is more effective since it enables\\nChatGPT to learn the knack of extracting the predicate followed\\nby extracting the subject and object, which is in fact an effective\\nextraction manner adopted in our DualOIE. However, ChatGPT\\nstill shows less efficacy than DualOIE, since it sometimes fails to\\ncorrectly comprehend the relations between arguments and tends\\nto confuse the boundaries of arguments [11]. For example, given\\nthe input in Table 4, the output of ChatGPT(COT) is (He, idolized,\\nreceiving the name of “God”), because it misjudges the predicate\\nbetween arguments.\\nIn addition, we observe that ChatGPT sometimes generates\\ntriples that do not align with the input sentence, also possibly\\ndue to hallucination [2].\\n5.6.5\\nImpact of Triplet Number. As illustrated in Fig. 4, it is harder\\nfor the extraction models to obtain satisfactory performance when\\nthe sentence’s structure is getting more complicated, where the\\nnumber of potential triplets increases. To further investigate the\\nmodels’ capabilities of handling the convoluted sentences with\\nmultiple triplets, we divided the samples (sentences) in SAOKE into\\n4 groups according to the triplet number in one sentence, and then\\nevaluated all compared models in each group. Fig. 4 displays the\\nrelevant results where𝑚 is the triplet number. It shows that DualOIE\\noutperforms baselines on all groups by a large margin. Particularly,\\nall baselines’ performance degrades sharply when more triplets\\nexist in a sentence. Comparatively, DualOIE’s performance drop is\\nslighter, justifying its stability of extracting triplets in complicated\\nsituations.\\n5.6.6\\nCorrelation of the Two Tasks. We have also observed a phe-\\nnomenon of mutual promotion between the dual task and the pri-\\nmary task (triplet extraction). We used BLEU [16] to evaluate the\\nquality of the restored sentences generated by T to S, and compared\\nit with the F1 of S to T. The BLEU score varies between 0 and 1,\\nshowing how similar the generated text is to the gold text. The\\nscatter and the regression line in Fig. 5 show the positive correlation\\nbetween BLEU and F1, implying that two tasks could improve the\\nperformance of each other.\\nExploiting Duality in Open Information Extraction\\nwith Predicate Prompt\\nWSDM ’24, March 4–8, 2024, Merida, Mexico\\nTable 6: Tuning results of loss coefficients on SAOKE.\\n𝛼 + 𝛽 < 𝛾\\n𝛼 + 𝛽 = 𝛾\\n𝛼 + 𝛽 > 𝛾\\n(𝛼, 𝛽, 𝛾)\\nF1\\n(𝛼, 𝛽, 𝛾)\\nF1\\n(𝛼, 𝛽, 𝛾)\\nF1\\n(0.2,0.4,1.2)\\n56.4\\n(0.2,0.4,0.6)\\n57.1\\n(0.2,0.4,0.3)\\n55.9\\n𝛼 < 𝛽\\n(0.2,0.6,1.6)\\n56.3\\n(0.2,0.6,0.8)\\n57.5\\n(0.2,0.6,0.4)\\n54.9\\n(0.2,0.8,2.0)\\n54.0\\n(0.2,0.8,1.0)\\n55.6\\n(0.2,0.8,0.5)\\n53.3\\n(0.2,0.2,0.8)\\n55.7\\n(0.2,0.2,0.4)\\n56.5\\n(0.2,0.2,0.2)\\n55.3\\n𝛼 = 𝛽\\n(0.4,0.4,1.6)\\n57.0\\n(0.4,0.4,0.8)\\n57.3\\n(0.4,0.4,0.4)\\n55.8\\n(0.6,0.6,2.4)\\n55.7\\n(0.6,0.6,1.2)\\n57.0\\n(0.6,0.6,0.6)\\n54.9\\n(0.4,0.2,1.2)\\n58.1\\n(0.4,0.2,0.6)\\n59.5\\n(0.4,0.2,0.3)\\n56.2\\n𝛼 > 𝛽\\n(0.6,0.2,1.6)\\n57.2\\n(0.6,0.2,0.8)\\n58.4\\n(0.6,0.2,0.4)\\n56.0\\n(0.8,0.2,2.0)\\n56.7\\n(0.8,0.2,1.0)\\n57.3\\n(0.8,0.2,0.5)\\n55.4\\n1\\nm\\n3\\n4\\nm\\n6\\n7\\nm\\n9\\n10\\nm\\n12\\n35\\n40\\n45\\n50\\n55\\n60\\nF1 Score\\nDualOIE\\nGen2OIE\\nIMoJIE\\nNOIE+BERT\\nFigure 4: Performance comparisons on the groups with dif-\\nferent triplet numbers (𝑚) of a sentence in SAOKE.\\nTable 7: Pred-F1 and Trip-F1 are defined identically to those\\nin Sec.5.3. Gold refers to the situation where golden predicate\\nprompt is provided.\\nModel\\nDataset\\nPred-F1\\nTrip-F1\\nDualOIE\\nCaRB\\n65.5\\n56.3\\nDualOIE\\nCaRB\\nGold\\n63.2\\nDualOIE\\nSAOKE\\n65.7\\n59.5\\nDualOIE\\nSAOKE\\nGold\\n65.6\\n5.6.7\\nAnalysis on Cascading Error. Since we used a pipeline-style\\nextraction structure in the T to S direction, it may cause the cas-\\ncading error. Therefore, we designed an experiment to analyze the\\nimpact of the cascading error on final triplet extraction. As shown\\nin Table 7, the F1 of the predicate extraction on two datasets are\\n65.5% and 65.7%. Given the prompt with golden predicates, there is\\n6.9% and 6.1% improvement in extraction results.\\n6\\nCONSTRUCTION OF MTOIE\\nMTOIE is a large-scale Chinese dataset and mainly focuses on\\nnoun-based attributes of POIs, which refer to the predicates in user\\ncomments, since the noun-based attributes often reflect customer\\nattitudes to the POIs. We randomly collected sentences from user\\ncomments of POIs on the Meituan platform, covering domains\\nlike food, entertainment and accommodation. Then we sent these\\nsentences to the annotators and asked them to annotate triples in\\nthe form of (entity, attribute, attribute-value). The annotation for\\n0.2\\n0.4\\n0.6\\n0.8\\nBLEU\\n0.2\\n0.4\\n0.6\\n0.8\\nF1 Score\\nFigure 5: Correlation analysis between the BLEU of T to S\\n(X-axis) and the F1 of S to T (Y-axis).\\nMTOIE is not easy for those inexperienced workers, especially for\\ntriples with implicit attributes. Thus, we employed 12 professional\\nannotators served in Meituan. To obtain a high-quality dataset, we\\nfirst trained them to annotate the triples with explicit attributes, and\\nthen evaluated their performance. Only the annotators achieving\\nhigh evaluation scores were allowed to annotate the triplets with\\nimplicit attributes.\\n6.1\\nExplicit Triplet Annotation\\nThe triplets with explicit attributes require the annotators to tag\\nout the entity, attribute, and attribute-value in the sentence in\\norder. To make the annotators quickly understand the definition\\nof attributes, we leveraged a predefined attribute vocabulary1 to\\nmatch the attributes in the sentence and marked them to indicate\\npossible attributes. Following the philosophy of previous OpenIE\\nbenchmarks [4, 23], we identified the following three guidelines for\\nMTOIE annotation.\\nCompleteness We aim to extract all triplets with marked attributes\\nfrom the sentence. Firstly, the annotators must carefully examine\\nall marked attributes, and annotate the entities and attribute-values\\ncorresponding to the attributes that were judged as correct. Then,\\nthe annotators checked again whether some attributes are missing\\nfrom the marked attributes.\\nAssertedness The annotation for the triplets with explicit attributes\\nfollows the principle of “literally honest”, i.e., entity, attribute and\\n1About 2,000 attributes accumulated by the Meituan company.\\nWSDM ’24, March 4–8, 2024, Merida, Mexico\\nZhen Chen et al.\\nAlgorithm 1 Iterative Annotation Framework.\\nRequire: 𝐷𝑒𝑥: annotated explicit samples; 𝐷𝑢𝑛: unannotated sen-\\ntences.\\nEnsure: 𝐷𝑖𝑚: annotated implicit samples;\\n1: Mask the attribute in the sentence of 𝐷𝑒𝑥 to get the pseudo\\nimplicit samples 𝐷𝑝𝑠;\\n2: for 𝑘 = 1 → 𝐾 do\\n3:\\nTrain OAE model 𝑀𝑘 with 𝐷𝑝𝑠;\\n4:\\nUse model 𝑀𝑘 to predict 𝐷𝑢𝑛 and obtain semi-supervised\\nsamples;\\n5:\\nDeliver semi-supervised samples to the annotators, get an-\\nnotated implicit samples 𝐷𝑖𝑚;\\n6:\\nUpdate 𝐷𝑝𝑠 with 𝐷𝑖𝑚;\\n7: end for\\nattribute-value must be asserted by the original sentence.\\nAtomicity Each triplet is required to be an indivisible unit. The\\nannotators must extract multiple atomic triplets from the sentence\\nthat has conjunctions. For example, given a sentence “面包有两种\\n口味，菠萝味和芒果味。/Bread has two flavors, pineapple and\\nmango.”, the annotators should annotate (面包/Bread, 口味/flavors,\\n菠萝味/pineapple) and (面包/Bread, 口味/flavors, 芒果味/mango)\\ninstead of (面包/Bread, 口味/flavors, 菠萝味和芒果味/pineapple\\nand mango).\\n6.2\\nImplicit Triplet Annotation\\nImplicit attributes are the most challenging part of the annotation.\\nManually annotating implicit attributes from the unstructured text\\ntends to cause missing and incorrect annotations. Therefore, we pro-\\nposed an iterative annotation framework to reduce the annotation\\ndifficulty, simplifying the “manual inference” to “model prediction\\nalong with by manual discrimination” (as shown in Algorithm 1).\\nAt first, we masked the attributes in the explicit samples 𝐷𝑒𝑥\\nto obtain the pseudo implicit samples 𝐷𝑝𝑠. For example, given “这\\n家游泳池水深2米。/The swimming pool is 2 meters deep”, the\\noriginal explicit triplet is (游泳池/The swimming pool, 水深/deep,\\n2米/2 meters), then we removed the attribute “水深/deep” from the\\nsentence, and thus the triplet becomes a pseudo implicit one.\\nSecond, we trained the model 𝑀 based on 𝐷𝑝𝑠 and then used\\n𝑀 to predict the unannotated sentences 𝐷𝑢𝑛 and obtain the semi-\\nsupervised samples. Then, the samples whose attributes do not\\nappear in the sentences were delivered to the annotators. This\\nmethod greatly reduces the annotation difficulty, and the annotators\\nonly need to judge whether the implicit triplets could be inferred\\nfrom the sentence.\\nThird, the annotated implicit samples were used to train the\\nmodel 𝑀 continuously. After 𝐾 rounds of iteration, we obtained\\nthe final annotated implicit dataset. Following [7], we calculated\\nthe Cohen’s Kappa to measure the agreements between annotators,\\nand the Kappa score is 88.36%.\\n6.3\\nDataset Statistics\\nIn MTOIE, we have collected 54,060 Chinese sentences and 87,971\\ntriplets in total. Among them, 15,137(28%) sentences contain implicit\\ntriplets, 29,290(33%) triplets are implicit, and there are 2109 types\\nof implicit predicates. To estimate the overall quality of the dataset,\\ntwo volunteers randomly picked 200 sentences from the dataset\\nand evaluated them carefully. The result shows that the triple-level\\nprecision and recall are 94.6% and 85.5%, respectively.\\n7\\nAPPLICATION ON MEITUAN\\nWe have also evaluated our proposed DualOIE on our built MTOIE\\nand verified its effectiveness for the real scenario, i.e., the search\\nservice on the Meituan Platform.\\nMetric. Our concentration on POI attributes makes the triplets\\nin MTOIE much shorter. Thus using the token-level metrics might\\noverestimate model performance. Therefore, we adopted a more\\nstrict matching strategy, where each subject and object are evalu-\\nated by a full match. Besides, we used a semantic similarity model2\\nto determine whether the predicted attribute and the gold have the\\nsame meaning. The threshold is set to 0.7.\\nComparison Results. We compared DualOIE’s extraction perfor-\\nmance on MTOIE with NOIE+BERT, IMoJIE and Gen2OIE. ChatGPT\\nwas not included as MTOIE has not been released yet. Using Chat-\\nGPT could lead to data leaks. The relevant results will be updated\\non GitHub once it is released in future. The results in Table 8 justify\\nour DualOIE’s advantage on both implicit triplets and all triplets\\nunder strict matching metrics. It shows that the relative perfor-\\nmance improvement of DualOIE over the baselines on implicit\\ntriplets is more significant than that on all triplets, demonstrating\\nDualOIE’s stronger capability of extracting implicit information.\\nWe also compared all the models’ efficiency through analyzing the\\nnumber of sentences that the models can process per second, as\\nshown in the last column of Table 8. As mentioned before, DualOIE\\nshows an advantage over IMoJIE and Gen2OIE due to its decoding\\nsteps independent of the triplets number, while the two competitors\\nspend more time on sentences with multiple triplets. Note that the\\nsingle-turn decoding model NOIE+BERT is much faster than the\\nother three generative methods, because it adopts beam search to\\noutput multiple triplets instead of a sequence of all triplets. Since\\nthe number of beams should be pre-decided, it fails to naturally\\nadapt the extraction number to the input sentence.\\nFurthermore, we display the extraction results of our DualOIE\\nand two state-of-the-art baselines, i.e., IMoJIE and Gen2OIE, on\\nsome difficult cases. As shown in Table 9, for the first case, IMoJIE\\nand Gen2OIE both miss a gold triplet. For the second case, IMoJIE\\npredicts a wrong triplet and misses an implicit triplet, and Gen2OIE\\npredicts the wrong subject of the implicit triplet. The two baselines’\\ninferior results are due to their insufficient recognition of the input\\nsentence’s structure. By contrast, DualOIE generates more correct\\ntriplets with the help of the dual task.\\nA/B Test. We also conducted an online A/B test to show how our\\nmodel helps the Meituan platform improve search performance.\\nSpecifically, given a POI, we first used DualOIE to extract a collec-\\ntion of triplets from every user comment of it. Then, we used the\\nsemantic similarity model to normalize the collection as follows.\\n1) The similarity scores of the subject, attribute (predicate) and\\nobject were computed between two triplets, and they would be\\n2https://github.com/shibing624/text2vec\\nExploiting Duality in Open Information Extraction\\nwith Predicate Prompt\\nWSDM ’24, March 4–8, 2024, Merida, Mexico\\nTable 8: Triplet extraction comparisons of different models on MTOIE.\\nImplicit Triplets\\nAll Triplets\\nSpeed\\nModel\\nPrecision\\nRecall\\nF1\\nPrecision\\nRecall\\nF1\\nsen.#/sec\\nNOIE+BERT\\n60.5\\n↑36%\\n51.5\\n↑37%\\n55.6\\n↑36%\\n64.8\\n↑32%\\n54.7\\n↑32%\\n59.3\\n↑32%\\n11.5\\nIMoJIE\\n61.7\\n↑33%\\n54.8\\n↑29%\\n58.0\\n↑31%\\n67.2\\n↑27%\\n61.0\\n↑19%\\n64.0\\n↑22%\\n2.6\\nGen2OIE\\n69.1\\n↑19%\\n59.7\\n↑18%\\n64.1\\n↑18%\\n73.4\\n↑16%\\n63.7\\n↑14%\\n68.2\\n↑15%\\n2.8\\nDualOIE\\n82.1\\n70.5\\n75.9\\n85.3\\n72.3\\n78.3\\n3.7\\nTable 9: Extraction results for two cases where the incorrect terms are marked red.\\nSentence\\n现在许多博物馆，纪念馆票价都免费了。/Nowadays many museums, monuments tickets are free.\\nIMoJIE\\n(纪念馆/monuments，票价/tickets，免费/free)\\nGen2OIE\\n(纪念馆/monuments，票价/tickets，免费/free)\\nDualOIE\\n(博物馆/museums，票价/tickets，免费/free), (纪念馆/monuments，票价/tickets，免费/free)\\nSentence\\n帮我做美睫的小姐姐很专业，效果非常满意。\\nThe lady helping me with beautiful eyelashes is professional, the effect is very satisfactory.\\nIMoJIE\\n(小姐姐/The lady，效果/effect，满意/satisfactory)\\nGen2OIE\\n(美睫/beautiful eyelashes，效果/effect，满意/satisfactory), (美睫/beautiful eyelashes，手法/skill，专业/professional)\\nDualOIE\\n(美睫/beautiful eyelashes，效果/effect，满意/satisfactory), (小姐姐/The lady，手法/skill，专业/professional)\\nconsidered expressing the same fact if their scores are over (0.7,\\n0.7, 0.7). 2) The triplets with the same meaning were replaced by\\nthe most frequent one in the collection, and the reserved triplets\\nwere ranked by their frequencies. Finally, given a triplet such as\\n(海底捞/Haidilao，食材/food，新鲜/fresh) in the normalized col-\\nlection, we concatenated the attribute “食材/food” with the object\\n“新鲜/fresh” as the given POI’s label “食材新鲜/food fresh”.\\nFor the online A/B test, we used two buckets where each bucket\\ncontaining 25% randomly selected users. For one bucket, the system\\nreturned the search results using the labels to filter out POIs. For\\nanother bucket, the system directly returned the POIs by its default\\nprinciple. We ran our A/B test on Meituan’s searching system and\\ncompared daily average performance on query view click-through\\nrate (QV-CTR) and unique visitor click-through rate (UV-CTR),\\nwhich refer to the ratio of clicked queries to all queries and the\\nratio of clicked queries to all unique user-query pairs, respectively.\\nAfter the running of 15 days, we noticed that QV-CTR and UV-CTR\\nincreased by 0.93% and 0.56%, respectively. These results indicate\\nthat the POI labels obtained through DualOIE indeed improve the\\nperformance of Meituan’s search service.\\n8\\nRELATED WORK\\nTraditional OpenIE systems such as TextRunner [3], ReVerb [9], OL-\\nLIE [20], Stanford-IE [1], OpenIE-5 [19] and MinIE [10] are mainly\\nbased on rules and statistics, which are combined with some mod-\\nules like POS taggers, SRL parsers and chunkers.\\nIn recent years, researchers mainly focus on neural solutions,\\nwhich could be divided into two categories: tagging-based systems\\nand generative systems. Tagging-based systems model OpenIE as a\\nsequence labelling problem. For example, RnnOIE [21] introduces\\na custom BIO tagging scheme and takes the predicate head with\\nthe input together. SenseOIE [18] introduces the dependency tree,\\nwhere the feature of a token is influenced by its parent and children.\\nBesides, it also uses lexical and syntactic features such as POS em-\\nbedding and syntactic role embedding. OpenIE6 [13] is built as an\\niterative grid labelling (IGL) system and achieves predictions in a\\n2D grid labelling way. SpanOIE [28] uses BiLSTM to derive the rep-\\nresentation of a span, from which the tag of each token is predicted.\\nIn MacroIE [27], a fact graph is built based on the token spans, and\\ndecoded into fact triplets during the inference process. On the other\\nhand, generative systems model OpenIE as a Seq2Seq problem, and\\nthus are capable of introducing new words to deal with implicit\\ntriplets. NOIE [6] uses LSTM as both encoder and decoder, and in-\\ntroduces copy attention to address the out-of-vocabulary problem.\\nIn Logician [23], BiGRU is used as both encoder and decoder. As\\nwell, the coverage mechanism and gated dependency mechanism\\nare both introduced. IMoJIE [14] uses BERT as encoder and LSTM as\\ndecoder. It leverages an iterative generation mechanism to alleviate\\nthe redundancy in triplets generation. Gen2OIE [15] is composed of\\ntwo mT5 [25] models, which reconstructs multiple inputs based on\\nthe first model, and then uses the second model to generate triplets.\\n9\\nCONCLUSION\\nWe propose a novel generative OpenIE model DualOIE based on\\nan auxiliary dual task with predicate prompt. By exploiting duality,\\nDualOIE performs better than the previous OpenIE models, since it\\ncan better understand the sentence structure and capture diverse\\nrelations between arguments. We also provide a high-quality Ope-\\nnIE dataset MTOIE, which is constructed from the user comments\\non Meituan’s platform. The experiments on public benchmarks and\\nMTOIE justify DualOIE’s superiority over the baselines. The online\\nA/B test on Meituan platform also suggests the extracted triplets\\ncan promote the real-world search service.\\n10\\nACKNOWLEDGEMENT\\nThis paper was supported by Chinese NSF Major Research Plan\\nNo.92270121, NSF funding No.62306112, Shanghai Sailing Program\\nNo.23YF1409400, Shanghai Science and Technology Innovation\\nAction Plan No.21511100401.\\nWSDM ’24, March 4–8, 2024, Merida, Mexico\\nZhen Chen et al.\\nREFERENCES\\n[1] Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning.\\n2015. Leveraging linguistic structure for open domain information extraction. In\\nProceedings of the 53rd Annual Meeting of the Association for Computational Lin-\\nguistics and the 7th International Joint Conference on Natural Language Processing\\n(Volume 1: Long Papers). 344–354.\\n[2] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan\\nWilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask,\\nmultilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and\\ninteractivity. arXiv preprint arXiv:2302.04023 (2023).\\n[3] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open\\ninformation extraction from the web. Communications of the Acm (2007).\\n[4] Sangnie Bhardwaj, Samarth Aggarwal, and Mausam Mausam. 2019. CaRB: A\\nCrowdsourced Benchmark for Open IE. In Proceedings of the 2019 Conference\\non Empirical Methods in Natural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP). Association\\nfor Computational Linguistics, Hong Kong, China, 6262–6267. https://doi.org/\\n10.18653/v1/D19-1651\\n[5] Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original:\\nFact aware neural abstractive summarization. In thirty-second AAAI conference\\non artificial intelligence.\\n[6] Lei Cui, Furu Wei, and Ming Zhou. 2018. Neural Open Information Extraction.\\nIn Proceedings of the 56th Annual Meeting of the Association for Computational\\nLinguistics (Volume 2: Short Papers). Association for Computational Linguistics,\\nMelbourne, Australia, 407–413. https://doi.org/10.18653/v1/P18-2065\\n[7] Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie,\\nHai-Tao Zheng, and Zhiyuan Liu. 2021. Few-nerd: A few-shot named entity\\nrecognition dataset. arXiv preprint arXiv:2105.07464 (2021).\\n[8] Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Mur-\\nphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge vault:\\nA web-scale approach to probabilistic knowledge fusion. In Proceedings of the 20th\\nACM SIGKDD international conference on Knowledge discovery and data mining.\\n601–610.\\n[9] Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations\\nfor open information extraction. In Proceedings of the 2011 conference on empirical\\nmethods in natural language processing. 1535–1545.\\n[10] Kiril Gashteovski, Rainer Gemulla, and Luciano del Corro. 2017. Minie: min-\\nimizing facts in open information extraction. Association for Computational\\nLinguistics.\\n[11] Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang, Lu Liu, and Xiang Wan.\\n2023. Is Information Extraction Solved by ChatGPT? An Analysis of Performance,\\nEvaluation Criteria, Robustness and Errors. arXiv preprint arXiv:2305.14450\\n(2023).\\n[12] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\\nmization. arXiv preprint arXiv:1412.6980 (2014).\\n[13] Keshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal, Mausam, and Soumen\\nChakrabarti. 2020. OpenIE6: Iterative Grid Labeling and Coordination Analysis\\nfor Open Information Extraction. In Proceedings of the 2020 Conference on Empir-\\nical Methods in Natural Language Processing (EMNLP). Association for Computa-\\ntional Linguistics, Online, 3748–3761. https://doi.org/10.18653/v1/2020.emnlp-\\nmain.306\\n[14] Keshav Kolluru, Samarth Aggarwal, Vipul Rathore, Mausam, and Soumen\\nChakrabarti. 2020. IMoJIE: Iterative Memory-Based Joint Open Information\\nExtraction. In Proceedings of the 58th Annual Meeting of the Association for Compu-\\ntational Linguistics. Association for Computational Linguistics, Online, 5871–5886.\\nhttps://doi.org/10.18653/v1/2020.acl-main.521\\n[15] Keshav Kolluru, Muqeeth Mohammed, Shubham Mittal, Soumen Chakrabarti,\\net al. 2022. Alignment-Augmented Consistent Translation for Multilingual Open\\nInformation Extraction. In Proceedings of the 60th Annual Meeting of the Associa-\\ntion for Computational Linguistics (Volume 1: Long Papers). 2502–2517.\\n[16] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a\\nmethod for automatic evaluation of machine translation. In Proceedings of the\\n40th annual meeting of the Association for Computational Linguistics. 311–318.\\n[17] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the\\nlimits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.\\nRes. 21, 140 (2020), 1–67.\\n[18] Arpita Roy, Youngja Park, Taesung Lee, and Shimei Pan. 2019. Supervising\\nunsupervised open information extraction models. In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\\n728–737.\\n[19] Swarnadeep Saha, Harinder Pal, et al. 2017. Bootstrapping for numerical open ie.\\nIn Proceedings of the 55th Annual Meeting of the Association for Computational\\nLinguistics (Volume 2: Short Papers). 317–323.\\n[20] Michael Schmitz, Stephen Soderland, Robert Bart, Oren Etzioni, et al. 2012. Open\\nlanguage learning for information extraction. In Proceedings of the 2012 joint\\nconference on empirical methods in natural language processing and computational\\nnatural language learning. 523–534.\\n[21] Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer, and Ido Dagan. 2018. Super-\\nvised Open Information Extraction. In Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers). Association for Computational\\nLinguistics, New Orleans, Louisiana, 885–895. https://doi.org/10.18653/v1/N18-\\n1081\\n[22] Jianlin Su. 2021. T5 PEGASUS - ZhuiyiAI. Technical Report. https://github.com/\\nZhuiyiTechnology/t5-pegasus\\n[23] Mingming Sun, Xu Li, Xin Wang, Miao Fan, Yue Feng, and Ping Li. 2018. Logician:\\na unified end-to-end neural approach for open-domain information extraction.\\nIn Proceedings of the Eleventh ACM International Conference on Web Search and\\nData Mining. 556–564.\\n[24] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\\nin large language models. Advances in Neural Information Processing Systems 35\\n(2022), 24824–24837.\\n[25] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya\\nSiddhant, Aditya Barua, and Colin Raffel. 2020. mT5: A massively multilingual\\npre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934 (2020).\\n[26] Zhao Yan, Duyu Tang, Nan Duan, Shujie Liu, Wendi Wang, Daxin Jiang, Ming\\nZhou, and Zhoujun Li. 2018. Assertion-based QA with question-aware open infor-\\nmation extraction. In Proceedings of the AAAI Conference on Artificial Intelligence,\\nVol. 32.\\n[27] Bowen Yu, Yucheng Wang, Tingwen Liu, Hongsong Zhu, Limin Sun, and Bin\\nWang. 2021. Maximal clique based non-autoregressive open information ex-\\ntraction. In Proceedings of the 2021 Conference on Empirical Methods in Natural\\nLanguage Processing. 9696–9706.\\n[28] Junlang Zhan and Hai Zhao. 2020. Span model for open information extraction\\non accurate corpus. In Proceedings of the AAAI Conference on Artificial Intelligence,\\nVol. 34. 9523–9530.\\n[29] Shaowen Zhou, Bowen Yu, Aixin Sun, Cheng Long, Jingyang Li, and Jian Sun.\\n2022. A Survey on Neural Open Information Extraction: Current Status and\\nFuture Directions. In Proceedings of the Thirty-First International Joint Con-\\nference on Artificial Intelligence, IJCAI-22, Lud De Raedt (Ed.). International\\nJoint Conferences on Artificial Intelligence Organization, 5694–5701.\\nhttps:\\n//doi.org/10.24963/ijcai.2022/793 Survey Track.\\n'},\n",
       " {'abstract': \"In recent years, Transformer structures have demonstrated their excellence in diverse domains across deep learning. It sparks discussions on whether such sophisticated structures are necessary and whether simple structures like Mixer can achieve comparable performance while maintaining reduced inference cost. To explore the Mixer's effectiveness, this research applies a more compact version of the Mixer to audio classification tasks, conducting comparative experiments with the Transformer-based Audio Spectrogram Transformer (AST) on three datasets. Additionally, this study examines the application of various activation functions in Mixer and compares their impact on model performance.\",\n",
       "  'introduction': 'In recent years, Transformer structures have significantly impacted various artificial intelligence domains, such as visual classification, temporal prediction, and audio classification. Their exceptional results in data processing have come at the cost of extensive parameters and intricate designs. Additionally, the surge in computing power and vast dataset collection has resulted in numerous models with complex architectures.',\n",
       "  'literature_review': 'Yuan Gong, Yu-An Chung, and James Glass introduced a model known as AST: Audio Spectrogram Transformer at the Interspeech2021 conference. This model originated from the aforementioned situation. In the AST paradigm, voice segments are converted into 128-dimensional Mel spectral features, and then a windowing operation is used to produce the associated spectrogram. The spectrogram is then divided into several segments by the AST model using Transformer’s approach, which are then supplied into the encoder structure following Linear Projection and feature extraction. The categorization outcomes are then output by the AST model using a linear layer. The model will be utilized as a comparison target in this research since it exhibits good performance on the three datasets.',\n",
       "  'methodology': \"In this research, an audio classification task is utilized to evaluate the efficacy of a more compact version of the Mixer. Comparative experiments are conducted with the Transformer-based Audio Spectrogram Transformer (AST) model on three datasets: Speech Commands, UrbanSound8k, and CASIA Chinese Sentiment Corpus. Additionally, this study explores the impact of various activation functions on Mixer's performance.\",\n",
       "  'results': 'The experimental results demonstrate the effectiveness of the Mixer in audio classification tasks. The Mixer outperforms the AST model in terms of accuracy and area under the curve (AUC) on both the validation and test sets across all three datasets. This suggests that Mixer can achieve comparable performance to Transformer-based models while maintaining a simpler structure and lower inference cost. Furthermore, the study finds that the RGB-to-grayscale map formula outperforms the simple weighted fusion approach in Linear Projection, leading to improved model performance.',\n",
       "  'conclusion': \"In conclusion, this study highlights the potential of Mixer in audio classification tasks, demonstrating its ability to achieve competitive performance with a simpler structure and reduced inference cost. The study also emphasizes the importance of adhering to objective laws and knowledge from other fields when making model optimizations, as this approach can provide more targeted directions for improvement. Future research directions include examining the application of Mixer pre-trained networks, removing the CLS flag in AST, optimizing the Mixer unit structure, and exploring self-supervised training approaches to further enhance the model's performance.\",\n",
       "  'title': 'ASM: Audio Spectrogram Mixer',\n",
       "  'author': 'Qingfeng Ji, Jicun Zhang, Yuxin Wang',\n",
       "  'textdata': 'ASM: Audio Spectrogram Mixer\\nQingfeng Ji1, Jicun Zhang2 and Yuxin Wang3\\nAbstract— Transformer structures have demonstrated out-\\nstanding skills in the deep learning space recently, signif-\\nicantly increasing the accuracy of models across a variety\\nof domains. Researchers have started to question whether\\nsuch a sophisticated network structure is actually necessary\\nand whether equally outstanding results can be reached with\\nreduced inference cost due to its complicated network topology\\nand high inference cost. In order to prove the Mixer’s efficacy\\non three datasets—Speech Commands, UrbanSound8k, and\\nCASIA Chinese Sentiment Corpus—this paper applies a more\\ncondensed version of the Mixer to an audio classification task\\nand conducts comparative experiments with the Transformer-\\nbased Audio Spectrogram Transformer (AST) model. In ad-\\ndition, this paper conducts comparative experiments on the\\napplication of several activation functions in Mixer, namely\\nGeLU, Mish, Swish and Acon-C. Furthermore, the use of\\nvarious activation functions in Mixer, including GeLU, Mish,\\nSwish, and Acon-C, is compared in this research through\\ncomparison experiments. Additionally, some AST model flaws\\nare highlighted, and the model suggested in this study is\\nimproved as a result. In conclusion, a model called the Audio\\nSpectrogram Mixer is suggested in this study, and the model’s\\nfuture directions for improvement are examined.\\nI. INTRODUCTION\\nEncoder-Decoder structures and attention mechanisms\\nhavebeen extremely important in a number of artificial in-\\ntelligence domains, including visual classification, temporal\\nprediction, audio classification, etc., due to the popularity of\\nTransformer structures in recent years. Due to their extensive\\nparameters and intricate designs, transformer architectures\\nhave demonstrated excellent outcomes in data processing.\\nMeanwhile, numerous models with intricate architectures\\nhave surfaced as a result of the huge rise in computing power\\nand vast dataset collecting.\\nResearchers including Yuan Gong, Yu-An Chung, and\\nJames Glass presented a model known as AST: Audio\\nSpectrogram Transformer at the Interspeech2021 confer-\\nence(Gong, Chung, and Glass 2021). This model was born\\nout of the aforementioned situation. In the AST paradigm,\\nvoice segments are converted into 128 The AST model\\nconverts speech snippets into 128-dimensional Mel spectral\\ncharacteristics, and then uses a windowing operation to\\nproduce the associated spectrograms. The spectrogram is\\nthen divided into several segments by the AST model using\\nTransformer’s approach, which are then supplied into the\\n*This work was not supported by any organization\\n1Qingfeng\\nJi\\nis\\nwith\\nSchool\\nof\\nComputer\\nScience\\nand\\nTechnology,\\nDalian\\nUniversity\\nof\\nTechnology,\\nDalian,\\nPRC\\n15640414255@mail.dlut.edu.cn\\n2Jicun Zhang is with Neusoft Reach Automotive Technology(Dalian)\\nCo.,Ltd., Dalian, PRC zhangjicun89@163.com\\n3Yuxin Wang is with School of Computer Science and Technology,\\nDalian University of Technology, Dalian, PRC wyx@dlut.edu.cn\\nencoder structure following Linear Projection and feature\\nextraction. The categorization outcomes are then output by\\nthe AST model using a linear layer. The model will be\\nutilized as a comparison target in this research since it\\nexhibits good performance on the three datasets.\\nIn fact, the development of the Transformer structure has\\ngreatly advanced deep learning. But given the high cost\\nof model inference, we begin to doubt the necessity of a\\nframework as intricate as Transformer. Google provided the\\nanswer to this question in the paper titled ”MLP-Mixer:\\nAn all-MLP Architecture for Vision” that was published in\\nNeurIPS(Tolstikhin et al. 2021). MLP-Mixer is the name of\\nthe model architecture that they suggested. By combining\\ndata in different positions (token-mixing) and in the same\\ndata in different positions of the channel (channel-mixing),\\nthe mixer is able to extract features easily. In short, feature\\nslices are extracted first in the column direction and then in\\nthe row direction.\\nThe Google team tested the effectiveness of MLP-Mixer\\nusing various vision datasets and discovered that its rel-\\natively straightforward structure performs nearly as well\\nas various variations of Vision Transformer (ViT) while\\nrequiring less processing power. This shows that perfor-\\nmance comparable to that of complex structures can be\\nobtained even with simpler structures. The study ”MTS-\\nMixers: Multivariate Time Series Forecasting via Factorized\\nTemporal and Channel Mixing” released by the Huawei\\nteam in 2023 confirms the efficiency of the Mixer in the\\nfield of time series forecasting(Li et al. 2023). The Huawei\\nteam emphasizes that capturing temporal correlation does not\\nalways need attention. Their test findings on various real\\ndatasets demonstrate that MTS-Mixer outperforms existing\\nTransformerbased algorithms in terms of efficiency.\\nWe propose the question of whether the Mixer has the\\npotential to implement SOTA in the audio domain after\\nvalidating the Mixer’s performance in the fields of vision\\nand timing prediction. We change the technical aspects of\\nthe AST model that do not follow the objective laws and\\nsuggest a structure called Audio Spectrogram Mixer (ASM)\\nby using the Mixer\\nII. ASM\\nA. Motivation\\nAccording\\nto\\nvalidation\\non\\nthree\\ndatasets,\\nAudioSet(Gemmeke et al. 2017), ESC50 (Piczak 2015),and\\nSpeech CommandsV2 (35 categories)(Warden 2018),the\\nTransformer structure used by the AST model is capable of\\noutperforming SOTA in the audio classification domain. This\\nsuggests that the AST model handles audio categorization\\narXiv:2401.11102v1  [cs.SD]  20 Jan 2024\\ntasks with greater performance. The authors of the research\\nhave developed an upgraded version of the AST model called\\n”SSAST: Self-Supervised Audio Spectrogram Transformer,”\\nwhich enhances the performance of downstream audio tasks\\nby utilizing masks and self-supervised pre-training(Gong et\\nal. 2022).\\nBased on the success of the AST model, we choose the\\nAST structure as a blueprint for our ASM model with the\\nstructure shown in Figure 1 and compare the two on three\\npublicly available datasets. We aim to verify that the Mixer\\nhas the potential to achieve SOTA in the audio classification\\ndomain. By comparing with the AST model, we will further\\nexplore the advantages and potential of the Mixer in the\\naudio domain with the aim of bringing better performance\\nto audio classification tasks.\\nIn addition to the above, we found one more aspect of\\nthe AST model that needs improvement. The Transformer\\nstructure of the AST model is migrated from the ViT\\nmodel(Dosovitskiy et al. 2020) in the vision domain, which\\ndeals with three-channel images, while the audio spectro-\\ngram has only single-channel features. To solve the problem\\nof channel number mismatch, the AST model directly sums\\nthe weights of the three channels in the Linear Projection\\nlayer to obtain the single channel weights. However, ac-\\ncording to objective laws, we think this approach is not\\nappropriate. Therefore, in this paper, we choose to use the\\nformula of RGB to grayscale map instead of this operation\\nto deal with the problem of channel number mismatch more\\naccurately. With this alternative, we hope to further improve\\nthe performance of the model and ensure consistency with\\nthe objective law.\\nB. Model Architecture\\nFigure 1 shows a detailed illustration of the ASM architec-\\nture. First, the input audio is converted into 128-dimensional\\nMel-spectral features and windowed to obtain the corre-\\nsponding spectrogram as the input to the ASM. Then, we\\ndivide the spectrogram into multiple patch sequences of size\\n16x16. These patch sequences are converted into 1D patch\\nembeddings of size 768 by linear projection layer. next, the\\nembedded sequences are input into MLP-Mixer. In MLP-\\nMixer, we keep some settings similar to AST, such as keep-\\ning the embedding dimension at 768 and keeping 12 MLP-\\nMixer layers. In the MLP unit, we use GeLU(Hendrycks\\nand Gimpel 2016), Mish(Misra 2019), Swish(Ramachandran,\\nZoph, and Le 2017), and Acon-C(Ma et al. 2021) activation\\nfunctions, which have non-zero gradients in the negative\\nregion and avoid the problem of dead neurons. In addition\\nthese functions are smoother at 0 than activation functions\\nsuch as ReLU, and thus converge more easily during training.\\nUnlike the AST model, we modify the Mixer architec-\\nture instead of choosing the original, unmodified Mixer\\narchitecture. Such modifications include adjustments to the\\nshape of the Mixer Blocks. We make this choice because\\nthere are no pre-trained Mixer-like models available that are\\ncomparable to the number of DeiT(Touvron et al. 2021)\\nparameters used by the AST model. Making comparisons\\nFig. 1.\\nAudio Spectrogram Mixer\\nin the absence of substantial prior knowledge is as unfair\\nas comparing the ability of high-potential newborns and\\njunior high school students to acquire knowledge in the same\\nsituation. However, we retain a clear separation from the\\nprojection layer at the input of the Mixer so that it can be\\nreplaced in the future with a possible pre-trained Mixer-like\\nmodel with a large amount of prior knowledge.\\nIII. EXPERIMENTS\\nIn AST, the authors provide an option to decide whether to\\nuse a visual pre-training model (e.g., ImageNet equals True\\nor False), and the AST model uses a DeiT with a parametric\\nnumber of 87M and an image input size of 384x384 as its\\nvisual pre-training model. Therefore, the RGB to grayscale\\nmap formula is used in the Linear Projection layer only when\\nImageNet=True.\\nTo compare the capabilities of the Mixer and Transformer\\nstructures, we choose to conduct comparison experiments in\\nthe two cases where ImageNet is True and False. Among\\nthem, when ImageNet is True, in order to avoid unfairness\\ndue to DeiT’s Encoder structure with prior knowledge, we\\nreinitialize an Encoder unit with the same structure as\\nDeiT, but keep the weight parameter of DeiT before the\\nEncoder layer and use it as the baseline. the purpose of\\nthis is to ensure fairness and eliminate the influence of prior\\nknowledge on the experimental results.\\nTABLE I\\nIMAGENET=FALSE, SCV2, AST AND MIXER\\nAST\\nMixer\\nv-acc\\n0.8062±0.0088\\n0.9292±0.0084\\nv-auc\\n0.9888±0.0007\\n0.9977±0.0006\\nt-acc\\n0.7924±0.0093\\n0.9189±0.0106\\nt-auc\\n0.9873±0.0009\\n0.9973±0.0005\\nIt is also worth noting that the focus of the comparison\\nexperiments is only on verifying the capabilities of the Mixer\\nand the validity of the RGB to grayscale map formula, so\\nwe don’t spend as much time fine-tuning and adapting the\\nASM on AudioSet as AST does.\\nIn our experiments, we use an ASM structure with a\\nmaximum number of parameters of 71M, which can save\\nmore than 15% of adjustable parameters and exhibit higher\\nparameter efficiency compared to the AST model.\\nA. Speech Commands Experiments\\n1) Dataset Introduction and training details:\\nSpeech\\nCommands V2(Warden 2018) is an audio dataset containing\\n35 classifications, consisting of 84843 training samples, 9981\\nvalidation samples and 11005 test samples. The duration of\\neach sample is 1 second.\\nAccording to the settings in ”AST: Audio Spectrogram\\nTransformer”, we set the initial learning rate to 2.5e-4 and\\nreduce the learning rate of each epoch to 0.85 of the learning\\nrate of the previous epoch after the 5th epoch. for the two\\ncases of ImageNet being True and False, we respectively We\\nconducted experiments with 10 epochs and 30 epochs for the\\ntwo cases of ImageNet being True and False, respectively.\\nWe select the best model by evaluating the metrics on the\\nvalidation dataset and report the evaluation metrics on the\\ntest set. For the validation process of each model, we used\\nthree different random seeds.\\nWe choose the accuracy rate (ACC) and area under the\\ncurve (AUC) as evaluation metrics.\\n2) Speech Commands Results : First, we conducted ex-\\nperimental validation for the case of not using the pre-trained\\nmodel, specifically, setting ImageNet to False and evaluating\\nthe effectiveness of replacing the Encoder with the MLP-\\nMixer. The experimental results are detailed in Table 1,\\nwhich lists the data for the validation set accuracy (v-acc),\\nthe area under the validation set curve (v-auc), the test set\\naccuracy (t-acc), and the area under the test set curve (t-auc).\\nThroughout the experiments in the paper, the metric names\\nand random seed labels are consistent to ensure comparable\\nexperimental results.\\nNotably, after 30 epochs of training, we observe that\\nthe ASM model performs significantly better than the AST\\nmodel (Baseline) in terms of ACC and AUC on the validation\\nand test sets. This indicates that the replacement of Encoder\\nusing the MLP-Mixer with the removal of the pretrained\\nmodel can effectively improve the performance of the model\\non this dataset.\\nNext, we verify the effectiveness of using the RGB to\\ngrayscale map formula in Linear Projection under the con-\\nTABLE II\\nIMAGENET=TRUE, SCV2, AST ,AST-RGB\\nAST\\nAST-RGB\\nv-acc\\n0.8006±0.0215\\n0.8146±0.0101\\nv-auc\\n0.9883±0.0020\\n0.9907±0.0011\\nt-acc\\n0.7834±0.0301\\n0.7951±0.0167\\nt-auc\\n0.9873±0.0019\\n0.9889±0.0015\\nTABLE III\\nIMAGENET=TRUE, SCV2, AST AND MIXER\\nAST\\nMixer\\nv-acc\\n0.8006±0.0215\\n0.9470±0.0022\\nv-auc\\n0.9883±0.0020\\n0.9987±0.0003\\nt-acc\\n0.7834±0.0301\\n0.9401±0.0041\\nt-auc\\n0.9873±0.0019\\n0.9983±0.0002\\ndition of using a pre-trained model, i.e., ImageNet=True.\\nThe experimental results are detailed in Table 2, where the\\ncolumn and row names are the same as in the previous paper.\\nWe observe that after 10 epochs of training, the model using\\nthe RGB-to-grayscale map formula performs consistently\\nbetter than the AST model (Baseline) in terms of accuracy\\n(ACC) and area under the curve (AUC) on both the validation\\nand test sets.\\nAt the end of this subsection, we further validate the\\neffectiveness of using the MLP-Mixer to replace Encoder\\nunder the condition of using a pre-trained model (i.e.,\\nImageNet=True). The experimental results are detailed in\\nTable 3, where the column and row names have the same\\nmeaning as before. We observe that after 10 epochs of\\ntraining, the ASM model with the MLP-Mixer replacing\\nEncoder performs significantly better than the AST model\\n(Baseline) in terms of accuracy (ACC) and area under the\\ncurve (AUC) on the validation and test sets.\\nB. UrbanSound8K Experiments\\n1) Dataset Introduction and training details:\\nUrban-\\nSound8K(Jaiswal and Patel 2018) is a commonly used audio\\nclassification dataset for research and experiments in sound\\nclassification and ambient sound recognition. It consists of\\na series of audio samples from urban environments and\\ncontains 8,732 short audio clips.\\nThe following are some important information and fea-\\ntures of the UrbanSound8K dataset:\\nData source: Audio samples for the UrbanSound8K dataset\\nare collected from 10 different urban environments, including\\nstreets, highways, parks, shopping malls, and more. These\\naudio clips are collected by deploying sensors in the city\\nand recording environmental sounds.\\nAudio classification: Each audio clip in the dataset is\\nclassified into one of 10 categories, which include: air\\nconditioner, car horn, children playing, dog bark, drilling,\\nengine idling, gunshot, jackhammer, siren, and street music.\\nData format: Each audio clip is stored in WAV format\\n(lossless audio) and has the same sample rate (44.1 kHz).\\nThe duration of the audio clips is between 1 and 4 seconds.\\nDataset division: The UrbanSound8K dataset is divided\\naccording to a training set and a test set. The training\\nTABLE IV\\nIMAGENET=FALSE, US8K, AST AND MIXER\\nAST\\nMixer\\nv-acc\\n0.7628±0.0089\\n0.8852±0.0029\\nv-auc\\n0.9676±0.0015\\n0.9906±0.0012\\nt-acc\\n0.7510±0.0113\\n0.8951±0.0069\\nt-auc\\n0.9662±0.0018\\n0.9903±0.0019\\nset contains the majority of the 8,732 audio clips (with at\\nleast 400 samples per category), while the test set contains\\nadditional audio clips for evaluating model performance.\\nMetadata information: In addition to the audio samples\\nthemselves, the dataset also contains metadata information\\nfor each audio clip, such as file ID, audio category, sample\\nrate, duration, etc. This metadata information can be used to\\ntrain the model and perform evaluation.\\nThe UrbanSound8K dataset provides a rich and diverse\\nsample of urban environmental sounds that can be used to\\nconduct research related to sound classification, environmen-\\ntal sound recognition, machine learning and deep learning. It\\nhas become one of the benchmark datasets for many audio\\nclassification algorithms and models.\\nIn this part of the experiments, we set the initial learning\\nrate to 1e-4 and after the 5th epoch, the learning rate of\\neach epoch decreases to 0.85 of the learning rate of the\\nprevious epoch. we conducted two cases of experiments\\nwith ImageNet as True and False, and 10 epochs of training\\nwere performed in each case. We selected the best model by\\nvalidating the evaluation metrics of the dataset and reported\\nthese evaluation metrics on the test set. To increase the\\nreliability of the experimental results, we performed three\\ndifferent random seed validations for each model, as in the\\nprevious section. We chose accuracy (ACC) as the primary\\nevaluation metric and area under the curve (AUC) as a\\nsecondary evaluation metric to assess the performance of the\\nmodels.\\n2) UrbanSound8K Results: First, we verified the effec-\\ntiveness of using MLP-Mixer to replace Encoder without\\nusing the pre-trained model (i.e., ImageNet = False). The\\nresults of our experiments are shown in Table 4, where the\\ncolumn and row names have the same meaning as before.\\nBy comparing the experimental results, we can observe\\nthat after 10 epochs of training, the ASM model using MLP-\\nMixer to replace Encoder performs significantly better than\\nthe AST model (Baseline) on the validation set.\\nNext, we verify the effectiveness of using the RGB-to-\\ngrayscale map formula in Linear Projection and the effec-\\ntiveness of using the MLP-Mixer to replace Encoder in the\\ncase of using a pre-trained model (i.e., ImageNet = True).\\nThe experimental results are shown in Table 5, where the\\nmeanings of column and row names are the same as before.\\nAfter 10 epochs of training, we observe that the model\\nusing the RGB-to-grayscale map formula performs stably\\nbetter than the AST model (Baseline) on the validation and\\ntest sets.\\nIn addition, we can also find that the ASM model using\\nMLP-Mixer to replace Encoder performs significantly better\\nTABLE V\\nIMAGENET=TRUE, US8K, AST ,AST-RGB\\nAST\\nAST-RGB\\nv-acc\\n0.8806±0.0053\\n0.8932±0.0043\\nv-auc\\n0.9892±0.0015\\n0.9908±0.0010\\nt-acc\\n0.8898±0.0024\\n0.8951±0.0074\\nt-auc\\n0.9896±0.0014\\n0.9905±0.0013\\nTABLE VI\\nIMAGENET=TRUE, US8K, AST AND MIXER\\nAST\\nMixer\\nv-acc\\n0.8806±0.0053\\n0.9230±0.0142\\nv-auc\\n0.9892±0.0015\\n0.9944±0.0020\\nt-acc\\n0.8898±0.0024\\n0.9176±0.0127\\nt-auc\\n0.9896±0.0014\\n0.9944±0.0009\\nthan the AST model (Baseline) on the validation and test\\nsets.\\nC. CASIA Chinese Sentiment Corpus Experiments\\n1) Dataset Introduction and training details: The CASIA\\nChinese Sentiment Corpus(Ke et al. 2018) is recorded by the\\nInstitute of Automation, Chinese Academy of Sciences. The\\ncorpus covers speech material recorded by four professional\\npronouncers, including six emotions: angry, happy, fear, sad,\\nsurprised and neutral. A total of 9600 speech samples with\\ndifferent pronunciations are included.\\nOf these, 300 speech samples are recorded based on the\\nsame text, i.e., the same text is given different emotional\\nstates for reading aloud. These samples can be used to com-\\npare and analyze the acoustic characteristics and rhythmic\\nperformance in different emotional states.\\nIn addition, there are 100 speech samples are recorded\\nbased on different texts. These texts are literally clear in their\\nemotional attribution, enabling the pronouncer to express the\\ncorresponding emotional state more accurately.\\nThe CASIA Chinese Sentiment Corpus is designed to\\nprovide researchers with rich phonetic data to explore dif-\\nferences in the acoustic and rhythmic aspects of emotional\\nexpression. The data in this corpus are recorded from pro-\\nfessional speakers and possess high quality and accuracy,\\nwhich can provide strong support for research in the fields\\nof emotion recognition and speech emotion processing.\\nIn this section, we refer to the setup of the Speech\\nCommands V2 dataset experiments. The initial learning rate\\nis set to 2.5e-4, and after the 5th epoch, the learning rate\\nof each epoch decreases to 0.85 of the learning rate of the\\nprevious epoch. we conduct experiments with 25 epochs\\nin both the True and False cases of ImageNet. The best\\nmodel is selected by validating the evaluation metrics of the\\ndataset, and we report these evaluation metrics on the test\\nset. To increase the reliability of the experimental results,\\nwe perform three different random seed validations for each\\nmodel. We choose the accuracy rate (ACC) of the model.\\n2) CASIA Chinese Sentiment Corpus Results: First, we\\nverify the effectiveness of using the MLP-Mixer to replace\\nthe Encoder without using the pre-trained model (i.e., Im-\\nageNet = False). The results of our experiments are shown\\nTABLE VII\\nIMAGENET=FALSE, CASIA, AST AND MIXER\\nAST\\nMixer\\nv-acc\\n0.7833±0.0194\\n0.9025±0.0089\\nv-auc\\n0.9783±0.0033\\n0.9948±0.0012\\nt-acc\\n0.7760±0.0112\\n0.8997±0.0107\\nt-auc\\n0.9780±0.0024\\n0.9943±0.0008\\nTABLE VIII\\nIMAGENET=TRUE, CASIA, AST ,AST-RGB\\nAST\\nAST-RGB\\nv-acc\\n0.7932±0.0121\\n0.7984±0.0044\\nv-auc\\n0.9821±0.0021\\n0.9834±0.0016\\nt-acc\\n0.7997±0.0099\\n0.8102±0.0064\\nt-auc\\n0.9826±0.0012\\n0.9839±0.0007\\nin Table 7, where the column and row names have the same\\nmeaning as before.\\nBy comparing the experimental results, we can observe\\nthat after 25 epochs of training, the ASM model using MLP-\\nMixer to replace Encoder performs significantly better than\\nthe AST model (Baseline) in terms of accuracy (ACC) and\\narea under the curve (AUC) on the validation and test sets.\\nNext, we verify the validity of using the RGB to grayscale\\nmap formula in Linear Projection in the case of using a\\npretrained model (i.e., ImageNet = True). The experimental\\nresults are shown in Table 8, where the column and row\\nnames have the same meaning as before.\\nAfter 25 epochs of training, we observe that the model\\nusing the RGB-to-grayscale map formula performs relatively\\nconsistently better than the AST model (Baseline) in terms\\nof accuracy (ACC) and area under the curve (AUC) on both\\nthe validation and test sets.\\nAt the end of this subsection, we validate the effectiveness\\nof using the MLP-Mixer to replace the Encoder for the case\\nwhere a pre-trained model is used (i.e., ImageNet = True).\\nThe experimental results are detailed in Table 9, where the\\nmeaning of the column and row names in the table is the\\nsame as in the previous section.\\nAfter 25 epochs of training, we observe that the ASM\\nmodel using MLP-Mixer to replace Encoder performs sig-\\nnificantly better than the AST model (Baseline) in terms of\\naccuracy (ACC) and area under the curve (AUC) on both the\\nvalidation and test sets.\\nD. Comparison of Activation Functions in Mixer\\n1) Dataset and Experimental setup: We use the CASIA\\nChinese Sentiment Corpus as the experimental dataset for\\nthis part, and use the Gelu activation function used in ”MLP-\\nMixer” as the baseline, and use the Mish, Swish and Acon-C\\nTABLE IX\\nIMAGENET=TRUE, CASIA, AST AND MIXER\\nAST\\nMixer\\nv-acc\\n0.7932±0.0121\\n0.9247±0.0056\\nv-auc\\n0.9821±0.0021\\n0.9962±0.0010\\nt-acc\\n0.7997±0.0099\\n0.9176±0.0088\\nt-auc\\n0.9826±0.0012\\n0.9945±0.0012\\nFig. 2.\\nSeed-1, ACC\\nactivation functions for comparison. In addition, we make\\nappropriate adjustments to the function characteristics of\\nAcon-C to ensure that the p1 parameter is initialized near\\n1 and the p2 parameter is initialized near 0 as much as\\npossible. Since the implementation of Acon-C requires the\\nuse of batch size as a parameter, considering the number of\\ndata samples in the CASIA Chinese sentiment corpus, we\\nmade appropriate adjustments to ensure that the impact of\\nbatch size on the experiment is eliminated in order to make\\nthe experiment more complete. We still use 3 random seeds\\nin our experiments, which is the same as described in the\\nprevious section.\\nThe initial learning rate is set to 2.5e-4 and after the\\n5th epoch, the learning rate of each epoch decreases to\\n0.85 of the learning rate of the previous epoch. we conduct\\nexperiments with ImageNet = False for 25 epochs. The\\nbest model was selected using evaluation metrics from the\\nvalidation set, and we report these evaluation metrics on the\\ntest set. We choose accuracy (ACC) as the main evaluation\\nmetric and area under the curve (AUC) as a secondary\\nevaluation metric.\\n2) Experimental Results: After 25 epochs of training, the\\nresults of our experiments on random seed No. 1 are shown\\nin Table 10, Figure 2 and Figure 3. From them, it can be\\nobserved that the model with the use of the adjusted Acon-\\nC activation function performs best in all four evaluation-\\nmetrics. However, the Swish activation function performs the\\nworst in all four evaluation-metrics.\\nFig. 3.\\nSeed-1, AUC\\nTABLE X\\nIMAGENET=TRUE, CASIA, SEED-1\\nSeed-1\\nGeLU\\nMish\\nSwish\\nAcon-C\\nAcon-C(adapted)\\nv-acc\\n0.8954\\n0.8907\\n0.8870\\n0.8944\\n0.9028\\nv-auc\\n0.9922\\n0.9927\\n0.9922\\n0.9926\\n0.9952\\nt-acc\\n0.8824\\n0.8759\\n0.8704\\n0.8963\\n0.9046\\nt-auc\\n0.9915\\n0.9914\\n0.9911\\n0.9916\\n0.9934\\nFig. 4.\\nSeed-2, ACC\\nFig. 5.\\nSeed-2, AUC\\nSimilarly, the results of our experiments with random seed\\nNo. 2 are shown in Table 11, Figure 4, and Figure 5, and it\\nis found that the model using the adjusted Acon-C activation\\nfunction still performs best on the four metrics.\\nIn the experiments with random seed No. 3, the results\\nof the evaluation metrics for the validation and test sets are\\nshown in Table 12, Figure 6, and Figure 7, and we again find\\nthat the model using the adapted Acon-C activation function\\nperforms best on all four metrics.\\nIV. CONCLUSION AND FOLLOW-UP\\nA. Conclusion\\nIn recent years, the Transformer has become an indispens-\\nable and critical component in various fields. However, in\\nour study, we find that it is not necessary to adopt such\\na complex network structure. In the field of vision and\\ntemporal prediction, there are already successful cases that\\ndemonstrate the superiority of the Mixer, which reduces the\\ninference cost (according to our experiments, saving more\\nFig. 6.\\nSeed-3, ACC\\nFig. 7.\\nSeed-3, AUC\\nthan 20% of the training time compared to AST) while\\nhaving an accuracy that is as good as or even better than that\\nof the Transformer. The Mixer does not yet have a pre-trained\\nmodel comparable to the volume of DeiT, so we cannot\\nverify whether Mixer can maintain its excellent performance\\nwith a large amount of prior knowledge. However, based\\non the work in this paper, we have good reasons to believe\\nthat Mixer has the potential to challenge Transformer in the\\nfield of audio classification, and researchers should pay more\\nattention and devote more resources to the research of MLP\\nand Mixer.\\nAlso in this study, we validate the use of the RGB-\\nto-grayscale map formulation instead of simple weighted\\nfusion. The experimental data show that the optimized model\\nperforms stably better than the original model, although\\nthe improvement in accuracy is only between 1% and 3%.\\nWe should recognize that the accuracy of the model is\\nmainly influenced by factors such as model structure and\\ndata volume. However, this optimization work still reminds\\nus that changes and optimizations should be made with more\\nTABLE XI\\nIMAGENET=TRUE, CASIA, SEED-2\\nSeed-1\\nGeLU\\nMish\\nSwish\\nAcon-C\\nAcon-C(adapted)\\nv-acc\\n0.8852\\n0.8889\\n0.9065\\n0.8972\\n0.9120\\nv-auc\\n0.9915\\n0.9928\\n0.9931\\n0.9933\\n0.9949\\nt-acc\\n0.8778\\n0.8796\\n0.8769\\n0.8806\\n0.9102\\nt-auc\\n0.9910\\n0.9908\\n0.9904\\n0.9919\\n0.9939\\nTABLE XII\\nIMAGENET=TRUE, CASIA, SEED-3\\nSeed-1\\nGeLU\\nMish\\nSwish\\nAcon-C\\nAcon-C(adapted)\\nv-acc\\n0.8815\\n0.8963\\n0.9009\\n0.8833\\n0.9139\\nv-auc\\n0.9926\\n0.9930\\n0.9935\\n0.9915\\n0.9948\\nt-acc\\n0.8759\\n0.8843\\n0.8824\\n0.8787\\n0.9019\\nt-auc\\n0.9900\\n0.9906\\n0.9909\\n0.9904\\n0.9947\\nreference to objective laws and knowledge that has been\\npractically validated in other fields. Such an approach can\\nprovide us with more targeted directions for improvement,\\nwhich can lead to better model performance.\\nB. Follow-up\\nWe plan to apply the existing Mixer pre-trained network\\nto ASM to explore its effectiveness in audio classification\\ntasks. Compared with Transformer, Mixer does not seem\\nto require trainable location information, so we consider\\nverifying whether removing the CLS flag in AST will have\\nan impact on the accuracy. In addition, we plan to further\\noptimize the structure of the Mixer unit to find a more\\nsuitable Mixer for audio classification tasks.\\nBesides, we are also inspired by the SSAST method and\\nplan to try to train the ASM model using a self-supervised\\ntraining approach to solve the problem of mismatch between\\nthe migrated visual model and the audio spectrogram. This\\nwill help improve the generalization performance of the\\nmodel and further enhance the accuracy of audio classifi-\\ncation.\\nREFERENCES\\n[1] Dosovitskiy, Alexey , et al. \"An Image is Worth 16x16 Words: Trans-\\nformers for Image Recognition at Scale.\" International Conference on\\nLearning Representations 2021.\\n[2] Gemmeke, Jort F. , et al. \"Audio Set: An ontology and human-labeled\\ndataset for audio events.\" IEEE International Conference on Acoustics\\nIEEE, 2017.\\n[3] Gong, Yuan , Y. A. Chung , and J. Glass . \"AST: Audio Spectrogram\\nTransformer.\" (2021).\\n[4] [1] Hendrycks, Dan , and K. Gimpel . \"Gaussian Error Linear Units\\n(GELUs).\" (2016).\\n[5] Jaiswal, Kaustumbh, and Dhairya Kalpeshbhai Patel. \"Sound classifi-\\ncation using convolutional neural networks.\" 2018 IEEE International\\nConference on Cloud Computing in Emerging Markets (CCEM).\\nIEEE, 2018.\\n[6] Ke, Xianxin, et al. \"Speech emotion recognition based on SVM and\\nANN.\" International Journal of Machine Learning and Computing 8.3\\n(2018): 198-202.\\n[7] Li, Zhe, et al. \"Mts-mixers: Multivariate time series forecast-\\ning via factorized temporal and channel mixing.\" arXiv preprint\\narXiv:2302.04501 (2023).\\n[8] Ma, Ningning, et al. \"Activate or not: Learning customized activation.\"\\nProceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. 2021.\\n[9] Misra, Diganta. \"Mish: A self regularized non-monotonic activation\\nfunction.\" arXiv preprint arXiv:1908.08681 (2019).\\n[10] Piczak, Karol J. \"ESC: Dataset for environmental sound classification.\"\\nProceedings of the 23rd ACM international conference on Multimedia.\\n2015.\\n[11] Ramachandran, Prajit , B. Zoph , and Q. V. Le . \"Swish: a Self-Gated\\nActivation Function.\" (2017).\\n[12] Tolstikhin, Ilya O., et al. \"Mlp-mixer: An all-mlp architecture for\\nvision.\" Advances in neural information processing systems 34 (2021):\\n24261-24272.\\n[13] Touvron, Hugo, et al. \"Training data-efficient image transformers &\\ndistillation through attention.\" International conference on machine\\nlearning. PMLR, 2021.\\n[14] Warden, Pete. \"Speech commands: A dataset for limited-vocabulary\\nspeech recognition.\" arXiv preprint arXiv:1804.03209 (2018).\\n'},\n",
       " {'abstract': 'In this research, we introduce Boidae, a suite of customizable, user-controlled Boa installations. Utilizing automation tools such as Ansible and Docker, Boidae enables deployment of tailored Boa setups. Notably, it allows the creation of custom datasets from any collection of Git repositories. These customized datasets can be generated from any set of Git repositories and are facilitated by helper scripts that aid in finding and cloning repositories from GitHub and SourceForge. The utility of Boidae lies in its versatile architecture, enabling researchers to generate custom datasets and make modifications to the Boa language and runtime.',\n",
       "  'introduction': \"Mining software repositories (MSR) is a powerful technique for unearthing developer habits and feature usage within the Software Engineering (SE) community. However, conducting MSR studies presents challenges, such as time-consuming dataset generation and complex data processing. Boa, a domain-specific query language and runtime infrastructure, simplifies MSR research. It facilitates the analysis of ultra-large-scale repositories, such as those found on GitHub and SourceForge, by leveraging Hadoop's infrastructure for storage and computation. While Boa streamlines MSR studies, it has limitations. Researchers often require customized datasets for their research, or they may wish to modify the Boa runtime or query language, which can be difficult with the current Boa setup.\",\n",
       "  'literature_review': 'The mining software repositories (MSR) research process can be broken down into three main stages: dataset generation, dataset mining, and dataset analysis. Though these stages can be challenging for new researchers, the first two stages pose unique difficulties. Dataset generation can be time-consuming and requires knowledge of tasks like using the GitHub API, parsing source files with libraries like Eclipse JDT, and storing large amounts of data using distributed filesystems or formats like Protocol Buffers. Mining the data also demands substantial expertise, including knowledge of distributed data analysis frameworks like Hadoop or Spark. While there are solutions that help alleviate some of these difficulties, none of them offer a comprehensive solution that allows for both easy dataset generation and customization of the mining framework. This is the primary motivation behind the development of Boidae.',\n",
       "  'methodology': \"Boidae's architecture has two primary components, each corresponding to a different runtime environment. The first component is a Docker image designed for local operation on a single machine, primarily intended for testing and analyzing small datasets. This image includes helper scripts for generating customized datasets from GitHub or SourceForge projects, and it enables modifications to the Boa compiler, runtime infrastructure, and language. For larger datasets, the second component provisions a distributed environment. Users can set up a cluster of compute nodes using their organization's computing services, cloud platforms like AWS, or research infrastructure such as CloudLab or ACCESS. Once the servers are provisioned, Ansible scripts can be used to install and configure a distributed Boidae instance, supporting dataset and runtime customization.\",\n",
       "  'results': 'The Boidae infrastructure was evaluated in four ways. Initially, to evaluate its ability to run on multiple systems, the Docker version was tested on various machines, and the cloud version was verified using Ansible scripts deployed on small clusters at Bowling Green State University and the CloudLab experimental testbed. To assess dataset generation capabilities, a custom dataset was built using repositories from the boalang user on GitHub. Furthermore, the infrastructure was tested for language and runtime customization by modifying the Boa language to add custom functions and then verifying their availability and functionality in queries. Additionally, the scalability of the infrastructure was evaluated by running tasks with varying numbers of map tasks, demonstrating good performance and the capability to handle larger datasets.',\n",
       "  'conclusion': 'Boidae is a significant contribution to the field of MSR as it offers a user-friendly platform for creating customized Boa instances. This enables researchers to generate custom datasets and modify the Boa language and runtime as needed. The availability of Boidae as a Docker container and the provision of Ansible scripts ensures that researchers can easily set up and manage their own Boa instances. Moreover, the operational Boa instance, available at https://boa.cs.iastate.edu/boa/, provides a readily accessible platform for users who prefer not to set up their own instances. With Boidae, researchers can focus more on their research questions and gain deeper insights into software repositories, alleviating the complexities of dataset generation and runtime modifications.',\n",
       "  'title': 'Boidae: Your Personal Mining Platform',\n",
       "  'author': 'Brian Sigurdson, Samuel W. Flint, Robert Dyer',\n",
       "  'textdata': 'Boidae: Your Personal Mining Platform\\nBrian Sigurdson\\nBowling Green State University\\nBowling Green, Ohio, USA\\nbsigurd@bgsu.edu\\nSamuel W. Flint\\nUniversity of Nebraska-Lincoln\\nLincoln, Nebraska, USA\\nswflint@huskers.unl.edu\\nRobert Dyer\\nUniversity of Nebraska-Lincoln\\nLincoln, Nebraska, USA\\nrdyer@unl.edu\\nABSTRACT\\nMining software repositories is a useful technique for researchers\\nand practitioners to see what software developers actually do when\\ndeveloping software. Tools like Boa provide users with the ability\\nto easily mine these open-source software repositories at a very\\nlarge scale, with datasets containing hundreds of thousands of\\nprojects. The trade-off is that users must use the provided infras-\\ntructure, query language, runtime, and datasets and this might not\\nfit all analysis needs. In this work, we present Boidae: a family\\nof Boa installations controlled and customized by users. Boidae\\nuses automation tools such as Ansible and Docker to facilitate the\\ndeployment of a customized Boa installation. In particular, Boidae\\nallows the creation of custom datasets generated from any set of\\nGit repositories, with helper scripts to aid in finding and cloning\\nrepositories from GitHub and SourceForge. In this paper, we briefly\\ndescribe the architecture of Boidae and how researchers can uti-\\nlize the infrastructure to generate custom datasets. Boidae’s scripts\\nand all infrastructure it builds upon are open-sourced. A video\\ndemonstration of Boidae’s installation and extension is available at\\nhttps://go.unl.edu/boidae.\\nCCS CONCEPTS\\n• Software and its engineering;\\nKEYWORDS\\nBoa, mining software repositories, scalable, open source\\nACM Reference Format:\\nBrian Sigurdson, Samuel W. Flint, and Robert Dyer. 2024. Boidae: Your\\nPersonal Mining Platform. In 2024 IEEE/ACM 46th International Conference\\non Software Engineering: Companion Proceedings (ICSE-Companion ’24), April\\n14–20, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 4 pages. https:\\n//doi.org/10.1145/3639478.3640026\\n1\\nINTRODUCTION\\nMining software repositories (MSR) is a powerful methodology for\\ndiscovering developer habits, feature usage, etc. within the Software\\nEngineering (SE) community. However, performing mining studies\\ncomes with great difficulty: generating a dataset takes time, and\\nprocessing that dataset is difficult. Libraries like PyDiller [13] help\\nease writing mining code but do not provide easy access to the\\nsource code’s abstract syntax tree (AST) which can either limit\\nyour analysis or require additional, complex libraries. Boa [4, 5,\\n11] is a domain-specific query language and runtime infrastructure\\nThis is the author’s version of the work. It is posted here for your personal use. Not\\nfor redistribution. The definitive version was published in ICSE-Companion’24.\\nICSE-Companion ’24, April 14–20, 2024, Lisbon, Portugal\\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\\nhttps://doi.org/10.1145/3639478.3640026\\ndesigned to ease MSR research. Boa has been shown to reduce\\nthe effort necessary to complete MSR studies on ultra-large-scale\\nrepositories such as GitHub and SourceForge. Boa is able to do so\\nby leveraging the Hadoop framework for storage and computation,\\nallowing it to scale to accommodate large datasets. It provides users\\nwith a web-based interface and allows users to easily replicate\\nBoa-based experiments provided by other researchers.\\nBoa has a few limitations, however. First, users often want to\\nbuild customized datasets for their research tasks. This may be\\nnecessary if they want to analyze proprietary datasets or when\\nreplicating prior research that requires building a dataset with the\\nsame (or comparable) repositories in it, such as was the case for\\nKeshk et al. [8] that built a custom Boa dataset to replicate the\\nprior work of Nakamaru et al. [10]. Since most researchers do not\\nhave access to the Boa infrastructure itself, building a custom Boa\\ndataset is not even an option and they can only query the existing\\ndatasets, which currently support languages such as Java, Kotlin,\\nand Python.\\nThe second limitation is in the runtime and query language.\\nBiswas et al. [2] extended Boa to add support for the Python lan-\\nguage and produced a dataset of repositories related to data science.\\nOnce again, since most users have no administrative access to the\\nBoa infrastructure, even if they wanted to modify the open-sourced\\ncompiler for Boa’s query language to add additional features or\\nprovide support for other programming languages they want to\\nmine, they currently are unable to do so.\\nTo solve these problems, in this work, we present Boidae.\\nWhereas Boa is a specific instance of a mining framework, Boidae\\nis a family of (possibly customized) mining frameworks. While Boa\\naims to provide an easy-to-use, single place for users wishing to\\nmine open-source repositories, Boidae’s intended users are more\\nadvanced software miners who need customization not currently\\nfeasible with Boa.\\nUsers are able to instantiate their own Boa instance either locally\\non their computer using Docker1 or remotely on a set of cloud\\nservers using Ansible2. Thus, if users want to analyze a smaller,\\nbut custom, dataset or to test their queries locally on a sample of\\nthe larger datasets they can run a Docker instance. Once they want\\nto scale to a larger dataset, they have the ability to easily spin up\\na cluster, on any cloud platform they have access to, with their\\ncustom Boa dataset.\\nThe remainder of the paper is organized as follows. First, we\\ndiscuss some background on the MSR process and closely related\\nworks in Section 2. Next, we discuss Boidae, its use case, workflow,\\nand general use in Section 3. We evaluate the approach in Section 4.\\nFinally, we conclude in Section 5.\\n1https://www.docker.com/\\n2https://www.ansible.com/\\narXiv:2401.11092v1  [cs.SE]  20 Jan 2024\\nICSE-Companion ’24, April 14–20, 2024, Lisbon, Portugal\\nBrian Sigurdson, Samuel W. Flint, and Robert Dyer\\n1. Dataset Generation\\n2. Dataset Mining\\n3. Dataset Analysis\\nFigure 1: Overview of the general mining software repositories workflow\\nAll source code for the Boa language and runtime and Boidae’s\\nAnsible and Docker scripts are available on GitHub at https://github.\\ncom/unl-pal/Boidae.\\n2\\nBACKGROUND AND RELATED WORKS\\nThe mining software repositories (MSR) research process can be\\nthought of as having three main stages: dataset generation, dataset\\nmining, and dataset analysis. The overall workflow is shown in\\nFig. 1. While all stages can be difficult for new researchers, the first\\ntwo stages pose several challenges beyond other types of SE re-\\nsearch. First, generating datasets can be time intensive and requires\\nknowledge of things like the GitHub API, how to parse source files\\nwith libraries such as Eclipse JDT, and methods for storing large\\namounts of data such as utilizing distributed filesystems or complex\\nformats such as Protocol Buffers.\\nSecond, even once such a dataset is generated, mining that data\\nalso requires substantial expertise. For example, if the dataset is\\nsufficiently large it might require knowledge of distributed data\\nanalysis frameworks such as Hadoop or Spark. Mining the code\\nstructure itself requires a lot of domain expertise and knowledge of\\nthings like abstract syntax trees.\\nFinally, users can analyze the mined data. Here they may need\\nto leverage things like Pandas for the analysis and Matplotlib for\\nvisualizing the results.\\nAll three stages are difficult and time-intensive, however, the\\nfirst and the second can be abstracted. Researchers can build and\\nshare datasets or tools that enable other researchers to implement\\ntheir studies more easily. An example is the Boa infrastructure [4,\\n5, 11], which provides support for the first two steps. As we will\\ndiscuss late, Boidae builds on top of Boa.\\nWith Boa, many repositories are collected in advance and trans-\\nlated into a format that is suitable for mining using a paralleliz-\\nable, domain-specific language. Boa provides several pre-generated\\ndatasets, with support for analyzing several programming lan-\\nguages such as Java, Python, and Kotlin. The datasets typically\\nhave 10’s to 100’s of thousands of projects each. Once generated,\\nthe datasets are immutable and will never change over time. This\\naids the replication of prior results.\\nBoa also provides a query language designed to abstract away\\nas much of the parallelization as possible. Users write queries that\\nlook sequential, essentially focusing on what data they want to\\nextract from a single project. Boa then automatically parallelizes the\\nquery, running it as a distributed Hadoop program. This provides\\nscalability, so queries on hundreds of thousands of projects can\\nreturn in as little as 30 seconds.\\nThere are other prior works that provide some similar func-\\ntionality. For example, GHTorrent [6, 7] is a website and dataset\\ncontaining the event stream from GitHub. Similar to Boa, users can\\nuse a shared infrastructure if they wish to query the dataset, and\\ndataset generation is controlled by the GHTorrent admins.\\nPyDriller [13] provides a library to more easily mine Git repos-\\nitories. While PyDriller makes it easier to analyze repositories, it\\ndoes not currently provide support to make the generation of a\\ndataset easier.\\nSourcerer [9] provides a large dataset of Java projects in the form\\nof a SQL database. Again, the generation of that data is up to the\\nmaintainers, and the dataset is provided in a fixed format. Users\\nmust query that data with SQL queries but are free to write any\\ncustom mining functions they wish as the infrastructure does not\\nprovide a custom query language.\\nAll of these approaches help ease either step 1 or step 2 of the\\nMSR workflow. But none of them ease both steps while also pro-\\nviding the ability to easily customize the dataset being generated\\nor customize the query language used to mine that data. This is the\\nmain goal of Boidae.\\n3\\nBOIDAE\\nHere, we describe Boidae’s architecture and an example use case.\\n3.1\\nUse Case\\nWhile Boa presents a large dataset and pre-built infrastructure,\\nBoidae allows researchers to build custom datasets and make mod-\\nifications to the Boa language. Consider, for example, a researcher\\nwho wishes to perform a more one-to-one analysis on a dataset\\nfrom a prior work. While they may be able to find and clone all of\\nthe same Git datasets as the prior work, writing analyses on it or\\nscaling those analyses could be difficult.\\nThe researcher could write custom code using several different\\nlibraries in their programming language of choice, however, this\\nwould require keeping track of many small details leading to more\\ncomplicated code: additionally, it likely would not run quickly as\\nit may not be easily parallelizable. Using Boidae, our researcher\\ncan build a dataset from these projects and run it on any server the\\nresearch has access to. Additionally, with some effort, they could\\nextend Boa to include additional information they may be interested\\nin (e.g., output from runs of external tools, or issue tracker data).\\n3.2\\nArchitecture\\nThe architecture of Boidae is split, based on two different runtime\\nenvironments, as shown in Fig. 2. First, we provide a Docker im-\\nage for running Boidae locally on a single machine. This image\\nis designed to support two use cases: testing and analyzing small\\ndatasets. Users utilize Docker’s compose functionality to run the\\ncontainer and then connect to a web service running on localhost.\\nThis provides a web interface similar to the one Boa provides, al-\\nlowing the user to select a dataset and run a query.\\nThe container also contains helper scripts for generating cus-\\ntomized datasets. The scripts provide support for both GitHub and\\nSourceForge, or the user can simply manually clone any Git reposi-\\ntories and generate a dataset from that set of repositories. If gener-\\nating from GitHub-based projects, the user has the option of simply\\nspecifying a list of repositories they want to generate a dataset from\\nBoidae: Your Personal Mining Platform\\nICSE-Companion ’24, April 14–20, 2024, Lisbon, Portugal\\nBoidae\\nDocker\\n(local/testing)\\nAnsible\\n(distributed)\\nRun Docker\\ndocker-compose up\\nProvision Servers\\nInstall Ansible\\nInstall Boidae\\nFigure 2: Overview of the Boidae architecture\\nor they can use more generic tools that will use GitHub’s API to\\nsearch for projects matching the user’s requirements.\\nThe container also provides support for modifying the runtime.\\nThe language’s compiler and runtime infrastructure are installed\\nin source format, with a full build environment available. There\\nare also support scripts to help update/install new versions of the\\ncompiler, once users have modified it.\\nThe second supported runtime environment is a distributed en-\\nvironment. This helps support any use case where the user has a\\nlarger dataset they might need to query frequently. For this, the\\nuser needs to first provision a set of compute nodes. This can be\\ndone with their employer’s/university’s computing services, with a\\ncloud such as Amazon Web Services (AWS) [12], or using research\\ninfrastructure such as CloudLab [3] or ACCESS [1]. Once they have\\nservers provisioned, they can install Ansible and use our provided\\nAnsible scripts to get a Boidae distributed instance installed and\\nconfigured. They still have the ability to customize the dataset or\\nruntime when utilizing the Ansible scripts.\\n3.3\\nWorkflow\\nWith Boidae, the overall workflow remains the same as shown in\\nFig. 1, but there are changes in some details, and steps 2 and 3 are\\nmore easily repeated. Continuing on with our prior use case, our\\nresearcher would complete the following steps.\\nFirst, a basic Boa cluster would be configured using either the\\nAnsible scripts or the Docker file. The Ansible scripts will allow for\\nthe installation of a Boidae cluster on a number of nodes, though it\\nis specifically designed for CloudLab [3]. The Docker file is instead\\nuseful for the generation of datasets, testing of modifications to the\\ncompiler on small datasets, or general work on small datasets. At\\nthis point, any modifications to the dataset generation code (to store\\nresults of external tools), the compiler (for further integration of\\nexternal tools), or the run time (addition of language-level functions)\\nshould be completed.\\nThen, they would collect metadata files for each repository. These\\nare JSON files that are collected using the GitHub API, which de-\\nscribe a number of properties of the project, including star gazer\\ncount. These JSON files are then used to generate the dataset\\nthrough the use of one of several scripts. This script will clone\\nthe repositories automatically. To ensure that private repositories\\ncan be cloned appropriately, GitHub’s HTTPS authentication must\\nbe correctly configured. Generation of a dataset will take time when\\nmany repositories are ingested. Once the generation is complete,\\nthe dataset can be installed into the configured Boidae cluster. From\\nthis point forward, steps 2 and 3 may commence: the dataset is now\\nready to be used for studies.\\nNow the dataset may be mined using the Boa language, running\\nqueries like that shown in Fig. 3, which counts the number of\\nannotations per project. Writing Boa queries is a topic in itself,\\nbut generally, the visitor pattern is used to visit each commit, and\\nwithin each commit, each changed file. A query can drill down to\\nwhatever level of the file is necessary, and tabular data is generated.\\nThe example query shows data indexed by project, but indexing by\\nother elements is possible.\\n1\\no: output sum[project: string] of int;\\n2\\n3\\nvisit(input , visitor {\\n4\\nbefore node: CodeRepository -> {\\n5\\nsnapshot := getsnapshot(node);\\n6\\nforeach (i: int; def(snapshot[i]))\\n7\\nvisit(snapshot[i]);\\n8\\nstop;\\n9\\n}\\n10\\nbefore mod: Modifier -> {\\n11\\nif (mod.kind == ModifierKind.ANNOTATION)\\n12\\no[input.id] << 1;\\n13\\n}\\n14\\n});\\nFigure 3: Count number of annotations per project.\\nThese tabular data files can be easily converted to CSV and\\nanalyzed using any preferred tools. Support for building multiple-\\nfile analyses is partially provided by the Boa Study Template.3\\n4\\nEVALUATION\\nWe evaluated the Boidae infrastructure in four ways. First, we\\nwanted to ensure that the infrastructure can run on multiple sys-\\ntems. To evaluate this, we first verified we can run the Docker\\nversion of the infrastructure on several different machines. Next,\\nwe wanted to verify if the cloud version of Boidae works. To verify\\nthis, we used the Ansible scripts to install Boidae onto a small\\ncluster of five nodes at Bowling Green State University. We also uti-\\nlized the CloudLab [3] infrastructure, an experimental testbed that\\nallows researchers to experiment with cloud infrastructures. Once\\nwe configured the server infrastructure, we were able to once again\\nutilize the Ansible scripts to get an instance of Boidae running on\\nCloudLab’s servers.\\nOur second evaluation was to see if it is possible to build custom\\ndatasets. We used Boidae’s feature for building GitHub datasets to\\nbuild a custom dataset containing the repositories in the boalang\\nuser, which represents the open-source infrastructure for Boa. We\\nwere able to successfully generate the dataset and run sample\\nqueries on it.\\n3https://github.com/boalang/study-template\\nICSE-Companion ’24, April 14–20, 2024, Lisbon, Portugal\\nBrian Sigurdson, Samuel W. Flint, and Robert Dyer\\nThe third evaluation is to see if Boidae supports customizing\\nthe language and runtime. For this evaluation, we modified the\\nBoa language to add additional support for custom domain-specific\\nmining functions. We then verified that those custom functions\\nwere available in queries and manually inspected the results to\\nverify they worked as intended.\\nFinally, we previously evaluated the scalability of the infrastruc-\\nture. Since the infrastructure runs on top of Hadoop, we ran several\\ntasks in a prior work [4]. We then varied the number of map tasks\\nfrom 1 to 32. See Figure 4 for the results.\\nFigure 4: Task execution times as the number of maps in-\\ncreases [4].\\nAs can be seen from the figure, the time needed to execute the\\nqueries decreases as the number of parallel map tasks increases.\\nIt then levels out at around 16 map tasks. This shows that the\\ninfrastructure can scale, providing good performance.\\n5\\nCONCLUSION\\nMining software repositories is an important field of research in\\nSE. Tools like Boa provide ways to more easily mine repositories\\nbut, despite being open-sourced, do not allow users the ability to\\ncustomize the dataset or the runtime. Boidae provides a solution to\\nallow researchers to run their own customized instances of Boa, en-\\nabling them to easily generate and mine their own custom datasets.\\nBoidae provides the ability to run locally on Docker, or scale to\\ncloud-scale using Ansible scripts.\\nAn operational Boa instance is available at https://boa.cs.iastate.\\nedu/boa/ and available to the public. For anyone wishing to cus-\\ntomize either the dataset or Boa’s infrastructure, Boidae pro-\\nvides installation in the form of either Ansible scripts at https:\\n//github.com/boalang/ansible or a Docker container at https://\\ngithub.com/boalang/boa-docker. The Boa infrastructure, which\\nBoidae utilizes, is available at https://github.com/boalang/compiler\\nand https://github.com/boalang/drupal.\\nAll artifacts for this demo are available in GitHub at https:\\n//github.com/unl-pal/Boidae. A video demonstration of Boidae’s\\ninstallation and extension is available at https://go.unl.edu/boidae.\\nACKNOWLEDGMENTS\\nThis work was supported in part by the U.S. National Science Foun-\\ndation (NSF) under grants 1512947 and 1518776.\\nREFERENCES\\n[1]\\nACCESS. Advanced cyberinfrastructure coordination ecosys-\\ntem: services and support, 2023. url: https://access-ci.org/.\\n[2]\\nS. Biswas, M. J. Islam, Y. Huang, and H. Rajan. Boa meets\\nPython: a Boa dataset of data science software in Python\\nlanguage. In Proceedings of the 16th International Conference\\non Mining Software Repositories, MSR ’19, Montreal, Canada,\\n2019.\\n[3]\\nCloudLab. The CloudLab website, 2023. url: http://www.\\ncloudlab.us/.\\n[4]\\nR. Dyer, H. A. Nguyen, H. Rajan, and T. N. Nguyen. Boa: a\\nlanguage and infrastructure for analyzing ultra-large-scale\\nsoftware repositories. In Proceedings of the 35th International\\nConference on Software Engineering, ICSE ’13, pages 422–431,\\n2013. doi: 10.1109/ICSE.2013.6606588.\\n[5]\\nR. Dyer, H. A. Nguyen, H. Rajan, and T. N. Nguyen. Boa:\\nultra-large-scale software repository and source-code min-\\ning. ACM Transactions on Software Engineering and Method-\\nology, 25(1):7:1–7:34, 2015.\\n[6]\\nG. Gousios. The GHTorrent dataset and tool suite. In Pro-\\nceedings of the 10th Working Conference on Mining Software\\nRepositories, MSR ’13, pages 233–236, San Francisco, CA,\\nUSA. IEEE Press, 2013.\\n[7]\\nG. Gousios and D. Spinellis. GHTorrent: GitHub’s data from\\na firehose. In Proceedings of the 9th Working Conference on\\nMining Software Repositories, MSR, pages 12–21. IEEE, 2012.\\n[8]\\nA. M. Keshk and R. Dyer. Method chaining redux: an empir-\\nical study of method chaining in Java, Kotlin, and Python.\\nIn Proceedings of the 20th International Conference on Mining\\nSoftware Repositories, MSR ’23’, pages 546–557, Melbourne,\\nAustralia, 2023.\\n[9]\\nE. Linstead, S. Bajracharya, T. Ngo, P. Rigor, C. Lopes, and\\nP. Baldi. Sourcerer: mining and searching internet-scale soft-\\nware repositories. Data Mining and Knowledge Discovery,\\n18(2):300–336, Apr. 2009. issn: 1573-756X. doi: 10 . 1007/\\ns10618-008-0118-x.\\n[10]\\nT. Nakamaru, T. Matsunaga, T. Yamazaki, S. Akiyama, and\\nS. Chiba. An empirical study of method chaining in Java. In\\nProceedings of the 17th International Conference on Mining\\nSoftware Repositories, MSR ’20, pages 93–102, Seoul, Republic\\nof Korea. Association for Computing Machinery, 2020. isbn:\\n9781450375177. doi: 10.1145/3379597.3387441.\\n[11]\\nH. Rajan, T. N. Nguyen, R. Dyer, and H. A. Nguyen. Boa\\nwebsite, 2024. url: http://boa.cs.iastate.edu/boa/.\\n[12]\\nA. W. Services. AWS global infrastructure, 2023. url: https:\\n//aws.amazon.com/about-aws/global-infrastructure.\\n[13]\\nD. Spadini, M. Aniche, and A. Bacchelli. PyDriller: Python\\nframework for mining software repositories. In Proceedings\\nof the 26th ACM Joint Meeting on European Software Engineer-\\ning Conference and Symposium on the Foundations of Software\\nEngineering, ESEC/FSE ’18, pages 908–911, New York, NY,\\nUSA. ACM, 2018. doi: 10.1145/3236024.3264598.\\n'},\n",
       " {'abstract': 'This research paper introduces FEDRKG, a novel federated recommendation framework that employs Graph Neural Networks (GNN) and incorporates a knowledge graph (KG) to enhance recommendation algorithms. The key insight is to construct higher-order user-item interactions through the relationships in the KG, while preserving user privacy. FEDRKG enables effective recommendation without the need for private user data exchange or costly homomorphic encryption.',\n",
       "  'introduction': 'Federated Learning (FL) has emerged as a promising approach to address the privacy concerns associated with centralized recommendation systems. However, existing FL-based recommendation methods often suffer from limited modeling capabilities and privacy issues. This paper proposes FEDRKG, a novel GNN-based federated learning framework that leverages public item information to enhance recommendation accuracy while protecting user privacy.',\n",
       "  'literature_review': 'Relevant research works are reviewed in three categories: knowledge graph-based recommendation, federated learning for recommendation systems, and privacy-preserving techniques in federated learning.',\n",
       "  'methodology': 'FEDRKG constructs a global knowledge graph using publicly available item information and distributes KG subgraphs to clients to improve local training. It utilizes a relation-aware GNN model with local differential privacy (LDP) for interaction items and gradients, ensuring privacy preservation during communication with the server.',\n",
       "  'results': 'Extensive experiments on three real-world datasets demonstrate the effectiveness of FEDRKG in terms of recommendation accuracy and privacy protection. It outperforms centralized algorithms and existing federated learning baselines, achieving competitive results while preserving user privacy.',\n",
       "  'conclusion': 'FEDRKG offers a promising approach for privacy-preserving federated recommendation tasks. It effectively utilizes public item information to enhance recommendation quality and employs various privacy-preserving techniques to protect user data. The proposed framework has the potential to improve the accuracy and privacy of federated recommendation systems.',\n",
       "  'title': 'FedRKG: A Privacy-preserving Federated Recommendation Framework via Knowledge Graph Enhancement',\n",
       "  'author': 'Dezhong Yao, Tongtong Liu, Qi Cao, Hai Jin',\n",
       "  'textdata': 'FEDRKG: A Privacy-preserving Federated\\nRecommendation Framework via Knowledge Graph\\nEnhancement\\nDezhong Yao1\\nTongtong Liu1\\nQi Cao2\\nHai Jin1\\n1Huazhong University of Science and Technology\\n2University of Glasgow\\n{dyao,tliu,hjin}@hust.edu.cn\\nqi.cao@glasgow.ac.uk\\nAbstract\\nFederated Learning (FL) has emerged as a promising approach for preserving\\ndata privacy in recommendation systems by training models locally. Recently,\\nGraph Neural Networks (GNN) have gained popularity in recommendation tasks\\ndue to their ability to capture high-order interactions between users and items.\\nHowever, privacy concerns prevent the global sharing of the entire user-item graph.\\nTo address this limitation, some methods create pseudo-interacted items or users in\\nthe graph to compensate for missing information for each client. Unfortunately,\\nthese methods introduce random noise and raise privacy concerns. In this paper,\\nwe propose FEDRKG, a novel federated recommendation system, where a global\\nknowledge graph (KG) is constructed and maintained on the server using publicly\\navailable item information, enabling higher-order user-item interactions. On the\\nclient side, a relation-aware GNN model leverages diverse KG relationships. To\\nprotect local interaction items and obscure gradients, we employ pseudo-labeling\\nand Local Differential Privacy (LDP). Extensive experiments conducted on three\\nreal-world datasets demonstrate the competitive performance of our approach\\ncompared to centralized algorithms while ensuring privacy preservation. Moreover,\\nFEDRKG achieves an average accuracy improvement of 4% compared to existing\\nfederated learning baselines.\\n1\\nIntroduction\\nRecommendation systems are widely used in various domains, such as e-commerce and social\\nrecommendation, by alleviating users from the burden of sifting through vast amounts of data to\\ndiscover suitable options [1]. These systems utilize user preferences and relevant information to\\nprovide personalized recommendations, making the process of finding relevant items more efficient\\nand convenient [2]. However, the effectiveness of most recommendation methods heavily relies on\\ncentralized storage of user data [3]. User data generated from software usage has the potential to\\nenhance user experiences, deliver personalized services, and provide insights into user behavior [4].\\nNevertheless, user data inherently includes user preferences and involves personal privacy. With the\\nincreasing awareness of privacy and the implementation of relevant regulations such as the General\\nData Protection Regulation (GDPR) [5], service providers may face growing challenges in centrally\\nstoring and processing user data, as shown in Fig. 1(a).\\nThe exclusive client access to local data leads to two challenges. Firstly, limited access to first-\\norder interaction data hampers the effectiveness of the recommendation model. Secondly, privacy-\\npreserving mechanisms are required to ensure secure communication between the client and server.\\nTo address these challenges, FL is introduced into the recommendation system. Existing works focus\\non the case of Fig. 1(b), where recommendations are achieved by directly finding correlations between\\nPreprint. Under review.\\narXiv:2401.11089v1  [cs.CR]  20 Jan 2024\\n(a) Centralized Learning\\nserver\\nuser\\nclient\\nitem\\nattribute\\nuser-item interaction\\nuser-user interaction\\nknowledge graph\\nserver\\nuser\\nclient\\nitem\\nattribute\\nuser-item interaction\\nuser-user interaction\\nknowledge graph\\n(b)  Federated learning through user relevancy \\n(c)   Federated learning through item relevancy\\nFigure 1: Comparison of centralized learning, federated learning with enhanced user connections,\\nand federated learning with enhanced project connections.\\nusers. For example, FedMF [6] and FedGNN [7] use only the local user-item interaction graph to\\nfind links between different users by collaborative filtering (CF). However, incorporating various\\ntypes of information in the conventional graph recommendation task can significantly improve the\\nrecommendation accuracy while changing the graph structure [4]. Additionally, FeSoG [8] utilizes\\nsocial networks as side information, adding direct connections between different users. Nevertheless,\\nthis method requires the server to possess the complete social network, which is a type of private data\\nthat is difficult to obtain for most recommendation systems. Furthermore, methods like FedGNN\\nemploy homomorphic encryption, which incurs substantial computational overhead and is not suitable\\nas the primary encryption algorithm on edge devices with performance constraints.\\nTo maximize the utilization of diverse data types while ensuring privacy protection on edge devices,\\nwe propose FEDRKG1, a GNN-based federated learning recommendation framework. Unlike CF\\nor direct construction of connections between users using privacy-sensitive information, FEDRKG\\nleverages publicly available item information (e.g., appearance, attributes) to establish higher-order\\nconnections between different items, as shown in Fig. 1(c).\\nThe server firstly constructs and maintains knowledge graphs (KGs) by utilizing publicly available\\nitem information. Then, we employ on-demand sampling of KGs and distribute them to the client.\\nSubsequently, we design a novel method to expand the local graph by merging KG subgraphs with\\nthe local user-item interaction graph, enabling the construction of high-order user-item interactions\\nthrough KGs. Additionally, our framework introduces a request-based distribution mechanism. By\\nobfuscating interaction items into request items, the server can efficiently distribute only the necessary\\nrequest embeddings, significantly reducing communication overhead compared to previous methods\\nwhile effectively protecting the privacy of raw interaction items. Simultaneously, we employ local\\ndifferential privacy (LDP) to protect all uploaded gradients, further enhancing the privacy of the\\nfederated learning process. Our approach has been extensively evaluated on three real-world datasets,\\ndemonstrating its competitive performance compared to centralized algorithms while ensuring privacy\\npreservation. Moreover, FEDRKG outperforms existing federated learning baselines, achieving an\\naverage accuracy improvement of approximately 4%. The major contributions of this work are\\nsummarized as follows:\\n• To the best of our knowledge, we are the first to introduce a knowledge graph to enhance the\\nperformance of the federated recommendation system while protecting privacy.\\n• We introduce an algorithm for user-item graph expansion using KG subgraphs to improve\\nlocal training.\\n• We propose innovative privacy-preserving techniques for interaction items, while simultane-\\nously reducing communication overhead through strategic distribution of embeddings.\\n1The source code is available at: https://github.com/ttliu98/FedRKG\\n2\\n2\\nRelated Work\\n2.1\\nKnowledge Graph Based Recommendation\\nIn recent years, significant research has focused on recommendation systems that utilize Graph Neural\\nNetworks (GNNs). GNNs have gained attention and popularity in recommendation systems due to\\ntheir ability to learn representations of graph-structured data, which is well-suited for the inherent\\ngraph structures in recommendation systems. Knowledge graphs, as a typical graph structure, are\\noften leveraged as side information in recommendation systems. By incorporating knowledge graphs,\\nhigh-order connections can be established through the relationships between items and their attributes.\\nThis integration enhances the accuracy of item representations and provides interpretability to the\\nrecommendation results. One type of method is integrating user-item interactions into KG. Methods\\nlike KGAT [1], CKAN [9], and MKGAT [10] treat users as entities within KG, and relationships\\nbetween users and items are incorporated as part of KG’s relationships, too. This integration enables\\nthe merged graph to be processed using a generic GNN model designed for knowledge graphs.\\nAnother idea is employed by KGCN [11] and KGNN-LS [12], directly connecting KG to the user-\\nitem graph without any transformation. These methods utilize relation-aware aggregation and consider\\nthe user’s preference for relationships when generating recommendations.\\n2.2\\nFederated Learning for Recommendation System\\nFederated learning is extensively utilized in privacy-preserving scenarios, as it ensures that the\\noriginal data remains on local devices while allowing multiple clients to train a model together [13].\\nConsidering the information required for recommendations, which includes users’ preferences for\\nitems, the introduction of federated learning can help us prevent privacy breaches. FedSage [14]\\nand FKGE [15] focus on cross-silo federation learning, they are not suitable for protecting the\\nprivacy of individual users on client devices. FCF [16] and FedMF [6] decompose the scoring matrix,\\nretain user embeddings locally, and aggregate item embeddings on the server. FedGNN [7] utilizes\\nhomomorphic encryption for CF and protects the original gradients using pseudo-labeling and LDP.\\nHowever, the computational requirements for homomorphic encryption pose challenges, particularly\\non performance-constrained devices. In contrast to methods that do not leverage any side information,\\nFeSoG [8] introduces social networks to establish connections between users. Unfortunately, in many\\nrecommendation scenarios such as e-commerce, service providers do not offer social services, and\\nsocial network information is considered private. Therefore, the lack of user connection on the server\\nin Fig. 1(b), like a social network, restricts the method’s ability to generalize [17]. Currently, there is\\na scarcity of federated learning algorithms that effectively utilize side information for cross-device\\nscenarios.\\n3\\nFederated Recommendation with Knowledge Graph Enhancement\\n3.1\\nProblem Definition\\nUser-item interactions can be represented by a typical bipartite graph G = (U, T , E), where U =\\n{u1, u2, . . . , uN} and T = {t1, t2, . . . , tM} represent a set of users and items of size N and M,\\nrespectively. To describe the set of edges E, an interaction matrix Y ∈ RM×N is employed. In\\nparticular, yut takes on the value 1 if an interaction exists in the user’s history, and 0 otherwise.\\nFor federated recommendation, each client ci owned by corresponding user ui can only access the\\ninteraction graph Gi stored locally, containing a set of items Ti that have been interacted with. Each\\nGi is a subgraph of the global interaction graph G.\\nIn addition to the client-side data, the server maintains a knowledge graph K, which is represented\\nas a series of triples {(h, r, t) | h, t ∈ E, r ∈ R}. The entities h and t each refer to the head and\\ntail, respectively, within the specific combination denoted by each triple, both belonging to the set of\\nentities E. The relationship r represents the connection between two distinct entities, belonging to a\\nset of relations R.\\nOur goal is to train a generalized GNN model using the local bipartite graphs Gi and the knowledge\\ngraph K while preserving user privacy. The model predicts the probability ˆyut that a user u will be\\ninterested in an unexplored item t.\\n3\\nGNN\\nPredictor\\nLoss\\nEmbedding\\nGradients\\nModel\\nGradients\\nLDP\\nLDP\\nAggregtor\\nModel\\nGradients\\nEmbedding\\nGradients\\nUpdate\\nDistribute\\nDistribute\\nRequest \\nRequest \\n𝑡𝑖,1 \\n𝑡𝑖,2 \\n𝑡𝑖,𝑛 \\n𝑢𝑖 \\nGNN\\nKnowledge\\nGraph\\n𝒆𝑖\\n𝑢 \\nClient i\\n \\n \\n𝒆𝑖,1\\n𝑝\\n \\n𝑡𝑖,2 \\n𝑝𝑖,1 \\n𝑡𝑖,𝑛 \\n𝒆𝑖,2\\n𝑡\\n \\n𝒆𝑖,𝑛\\n𝑡\\n \\n𝒆𝑖,1\\n𝑒\\n \\n𝒆𝑖,1\\n𝑟\\n \\n𝒉𝑖,1\\n𝑝\\n \\n𝒉𝑖,2\\n𝑡\\n \\n𝒉𝑖,𝑛\\n𝑡\\n \\nGNN\\nPredictor\\nLoss\\nEmbedding\\nGradients\\nModel\\nGradients\\nLDP\\nLDP\\n𝑡𝑗,1 \\n𝑡𝑗,2 \\n𝑡𝑗,𝑛 \\n𝑢𝑗 \\n𝒆𝑗\\n𝑢 \\n \\n \\n𝒆𝑗,1\\n𝑝\\n \\n𝑡𝑗,2 \\n𝑝𝑗,1 \\n𝑡𝑗,𝑛 \\n𝒆𝑗,2\\n𝑡\\n \\n𝒆𝑗,𝑛\\n𝑡\\n \\n𝒆𝑗,1\\n𝑒\\n \\n𝒆𝑗,1\\n𝑟\\n \\n𝒉𝑗,1\\n𝑝\\n \\n𝒉𝑗,2\\n𝑡\\n \\n𝒉𝑗,𝑛\\n𝑡\\n \\nClient j\\nFigure 2: The framework of FEDRKG.\\n3.2\\nFramework Overview\\nTo enable privacy-preserving recommendation tasks across diverse private devices, we introduce a\\nfederated learning framework, in Fig. 2, based on the knowledge graph named FEDRKG. In the\\nproposed framework, the client-server architecture is adopted. The client, which is the user’s private\\ndevice, is responsible for training a local graph neural network model. The server, on the other hand,\\nis responsible for aggregating the models and embeddings, maintaining the knowledge graph, and\\nconstructing higher-order connections between clients.\\nThe entire workflow is summarized in Algorithm 1, which concisely represents the complete workflow.\\n3.3\\nClient Design\\nIn our framework, the client plays a crucial role in two tasks. First, it is responsible for ensuring the\\nconfidentiality of the user’s private information during the communication process with the server,\\nwhich is achieved through privacy-preserving algorithms. Second, the client utilizes the embeddings\\nand models provided by the server to expand the local user-item graph and train the local model.\\nBased on the knowledge graph shared by the server, we design a novel method to expand the\\nlocal subgraph. During the request phase, the client applies a privacy protection mechanism to\\nthe interaction items Tn, generating obfuscated request items T ′\\nn. These request items are then\\ntransmitted to the server. The client receives a GNN model and a knowledge subgraph that includes\\nthe request items and some of their neighboring entities in the complete KG. By merging this\\nknowledge subgraph with local user-item interaction, the client generates a graph for local training.\\nThis approach guarantees the privacy of the user’s interaction records by never disclosing them to\\nthe server, while also allowing the client to obtain more item-related information for training, thus\\nindirectly enabling the construction of higher-order connections through knowledge subgraph.\\nOnce the aggregated global model is received, the client proceeds to update its local model and\\ninitiates a training process. We use a relation-aware GNN as a recommendation model [18] that\\nconforms to the message-passing paradigm [13], as shown in Fig. 3. For a given user u, entity ei, ej,\\nand ri,j as the relation between two entities, we follow node-wise computation at step t+1:\\nx(t+1)\\ni\\n= ϕ\\n\\x10\\nx(t)\\ni , ρ\\n\\x10n\\nm(t+1)\\nri,j\\n: (u, ej, ru,v) ∈ E\\no\\x11\\x11\\n(1)\\nwhere xt\\ni ∈ Rd is embedding of entity ei in step t. We utilize a simple summation operation as the\\nreduce function ρ and directly replace the original embeddings with the aggregated results as the\\n4\\nAlgorithm 1: FEDRKG\\nInput: Neighbor sampling size K; embedding size d; depth of receptive field H; learning rate η;\\nclient number N; item number M; pseudo items p; (0, 1) flipping q;LDP parameter δ, λ;\\nknowledge graph K; clients local graph\\nn\\nGn|N\\nn=1\\no\\nOutput: GNN parameters and KG embeddings θ,user embeddings\\nn\\ne∗\\nu|N\\nn=1\\no\\n1 Initialize θ, K,\\nn\\ne∗\\nu|N\\nn=1\\no\\n;\\n2 while FEDRKG not converged do\\n3\\nRandomly select a subset N from N randomly;\\n4\\n// client\\n5\\nfor each client n ∈ N do\\n6\\nT ′\\nn ← GenerateRequestItems(Gn, p, q);\\n7\\nθ, Gn ← Request(T ′\\nn)\\n8\\ngn ← LocalTrain(θ, Gn)\\n9\\n˜gn ←LDP(gn)\\n10\\nUpload(˜gn)\\n11\\nend\\n12\\n// server\\n13\\nfor each client n ∈ N do\\n14\\nT ′\\nn ←ReceiveRequest()\\n15\\nGn ← GetSubKG(T ′\\nn)\\n16\\nDistribute(θ, Gn)\\n17\\n˜gn ←ReceiveGrad()\\n18\\nend\\n19\\ng ←Eq.(5)\\n20\\nθ ←Eq.(6)\\n21 end\\nreduce function, denoted as ϕ. ej sends a relationship-aware message mri,j to its neighbor:\\nmri,j = αu\\nri,jxj\\n(2)\\nwhere the attention score αu\\nri,j between user u and relation ri,j is derived from the following formula:\\nsu\\nrt,i = score(eu, ert,i)\\n(3)\\nAtt(eu, ei) = αu\\nrt,i =\\nexp\\n\\x10\\nsu\\nrt,i\\n\\x11\\nP\\ni′∈N (t) exp\\n\\x10\\nsurt,i′\\n\\x11\\n(4)\\nWe calculate an attention score using a score function (e.g. inner product) and then normalize it.\\nAfter obtaining the final embedding xt of item t, we calculate the prediction ˆy by a readout function\\nand then train this GNN model using BCE as the loss function. Finally, client uploads encrypted\\ngradient to server.\\n3.4\\nServer Design\\nSimilar to clients, the server performs distinct tasks that are mainly distributed across two phases.\\nFirstly, the server’s primary responsibility is to respond to the client’s requests. Based on the\\nrequested items, the server utilizes the knowledge graph to sample a subgraph that corresponds to\\na specific client. The subgraph comprises two key components, namely the structural information\\nin the form of triples, and the feature information, represented by the embedding of entities and\\nrelations. Subsequently, the server shares the subgraph, together with the global model, with the\\nclient. Secondly, the server needs to receive all gradients of local models and embeddings uploaded\\nby clients. These gradients are then aggregated and used to update the global model and knowledge\\ngraph.\\n5\\n\\uf0c4\\n\\uf0c5\\n𝑦  \\nSubgraph from sever\\nuser-item \\ninteraction\\n\\uf0c4\\n\\uf0c5\\nitem embedding\\nrelation embedding\\nentity embedding\\ndot product\\nsum\\n𝐞𝑢  \\n𝐞𝑡  \\n𝑨𝒕𝒕 𝐞𝑢, 𝐞𝑖 →  𝜶𝑟𝑡,𝑖 ⊗   \\n𝐞𝑢  \\n𝐞𝑘  \\n\\uf0c4\\n𝑨𝒕𝒕 𝐞𝑢, 𝐞𝑘 →  𝜶𝑟𝑡,𝑘 ⊗   \\n\\uf0c4\\nuser embedding\\nrelation\\nFigure 3: Relation-aware aggregation in client.\\nIn each communication round, the server activates N clients. After receiving request items from\\nthose clients, server randomly samples a set of neighbors, denoted as S(t) ≜ {e|e ∼ N(t)}, for\\nthe request item t. Here, |S(v)| = K represents the fixed size when sampling, and N(t) represents\\nimmediate neighbors for item t. In our framework, S(v) is also referred to as the (single-layer)\\nreceptive field of item t. Repeat the above sampling several times to obtain Gi containing n iterations\\nand then distribute it to the client along with the parameters θ, consisting of the model parameters\\nθm and all embeddings of entities and relations in Gi denoted by θe. Finally, it receives the local\\ngradients ˜gi of these clients and aggregates them as follow:\\ng =\\nP\\nn∈N |T ′\\nn| · ˜gn\\nP\\nn∈N |T ′n|\\n(5)\\nAfter aggregation, the server updates all parameters θ with gradient descent as:\\nθ∗ = θ − η · ¯g\\n(6)\\nwhere η is the learning rate.\\n3.5\\nPrivacy-Preserving Communication\\n3.5.1\\nUser privacy\\nWithin our proposed framework, user-related privacy pertains primarily to user embedding. Tra-\\nditional embedding-based recommendation algorithms can derive both user and item embeddings\\nand generate user-specific recommendations through a straightforward readout operation. However,\\nuser embeddings comprise the user’s preference characteristics, which can lead to a compromise of\\ntheir privacy. In the federated learning scenario where the server does not have access to the raw\\ndata, to avoid exposing user preferences directly to the server, it is obvious that we need to keep\\nthe user embeddings on the client side and isolate them from the server. Clients can simply protect\\nuser-related privacy by refraining from uploading user embeddings after the training phase.\\n3.5.2\\nInteraction privacy\\nThe interaction between users and items is considered highly sensitive information, susceptible to\\npotential leaks during two stages. Firstly, due to the large size of the knowledge graph for items and\\nlimited transmission bandwidth, it is not practical to distribute all embeddings to client similar to\\nFedGNN and FedSoG. Instead, we aim to complete the entire training process through the limited\\ndistribution of embeddings. However, this presents a challenge in determining which embeddings\\nshould be distributed by the server. Server can not explicitly obtain the required embeddings, as this\\nwould mean that it has access to the client’s real interaction item. Therefore, we need to obfuscate\\n6\\nthe original interaction items to obtain encrypted request items, which can then be sent to the server\\nto sample the corresponding subgraph required for training.\\nWe have designed a local differential privacy(LDP) mechanism to generate request items from\\nthe interaction items. Specifically, user-item interaction for user u can be represented as a set\\n{(ti, yui) | yui ∈ {0, 1}, i = 1, 2, . . . , n}. This collection contains |T | elements, each of which is\\na binary, the first of which is an item and the second is either 0 or 1, indicating whether the user\\ninteracted with the item. Let the query for the ti be yui, then the interaction can be privacy-preserving\\nusing an ϵ-LDP algorithm. The privacy budget ϵ indicates the maximum acceptable loss of privacy.\\nLet the interaction for each item satisfy ϵ-LDP, and we have: for any item, keep the original interaction\\nvalue with probability\\neϵ\\neϵ+1 and invert it to another value with\\n1\\neϵ+1 (0,1 flipping).\\nA potential privacy concern with the widely used pseudo-labeling method in previous work is that\\nthe interacted item will always generate gradients, even if pseudo-labeling is used. Additionally, the\\npseudo-labeling method applied during the training phase does not effectively reduce the commu-\\nnication overhead associated with distributing embeddings. To address this issue, we first sample\\nseveral non-interactive samples, mix them with real interaction items, and further obfuscate them by\\nthe above LDP method to achieve privacy protection.\\n3.5.3\\nGradients privacy\\nEnsuring the privacy of users’ sensitive information is a critical concern when maintaining a knowl-\\nedge graph and updating the global model in a federated recommendation system. In each communi-\\ncation round, the server needs to aggregate gradients of entity embeddings, relational embeddings,\\nand GNN models from different clients. However, it has been demonstrated, as exemplified by\\nFedMF, that uploading users’ gradients in consecutive steps can lead to the inadvertent exposure\\nof sensitive data, such as users’ ratings. Therefore, we need to obfuscate gradients to protect user\\nprivacy. However clients of recommendation systems, such as mobile devices, often have limited\\ncomputational capabilities [19], and computationally intensive methods like homomorphic encryption\\nmay not be practical to implement on such devices. To tackle this, we employ LDP by injecting\\nrandom noise into the local gradients before uploading them to the server. This approach effectively\\nprotects row gradients without compromising the accuracy of the model. Moreover, it helps ensure\\nthat the computational overhead remains manageable and within acceptable limits.\\nTo be more specific, gives all gradients as gn = {ge\\nn, gm\\nn } = ∂Ln\\n∂θ , where Ln denotes loss of client n,\\nthe LDP is formulated as:\\n˜gn = clip (gn, δ) + Laplacian (0, λ)\\n(7)\\nwhere ˜gn is the encrypted gradient, clip(x, δ) denotes the gradient clipping operation with a threshold\\nδ to limit x and prevent the gradient from being too large, after which we add to the gradient a mean\\nvalue of 0 and an intensity of λ of Laplacian noise, denoted by Laplacian (0, λ). This results in a\\nϵ-LDP, where the privacy budget ϵ is 2δ\\nλ .\\n4\\nExperiment\\n4.1\\nDatasets\\nIn order to ensure the robustness of the algorithm, we aim to test the overall performance of the\\nframework on a variety of datasets with different sizes, sparsity, and domains. Therefore, we have\\nselected the following real-world datasets:\\n• MovieLens-20M [20] contains five-star ratings from MovieLens, a movie recommendation\\nservice, as of 2019. Each user in the dataset has provided a minimum of 20 ratings (ranging\\nfrom 1 to 5) on the MovieLens website.\\n• Book-Crossing [21] contains user ratings (ranging from 0 to 10) of books extracted from\\nthe Book-Crossing community in 2004. In this dataset, a rating of 0 indicates an implicit\\ninteraction between the user and the book.\\n7\\nTable 1: Dataset basic information and hyperparameters, notation is consistent with Algorithm 1.\\nMovieLens-20M\\nBook-Crossing\\nLast.FM\\nusers\\n138,159\\n19,676\\n1,872\\nitems\\n16,954\\n20,003\\n3,846\\ninteractions\\n13,501,622\\n172,576\\n42,346\\nentities\\n102,569\\n25,787\\n9,366\\nrelations\\n32\\n18\\n60\\nKG triples\\n499,474\\n60,787\\n15,518\\nK\\n4\\n8\\n8\\nd\\n32\\n64\\n16\\nH\\n2\\n1\\n1\\nλ\\n10−7\\n2 × 10−5\\n10−4\\nη\\n2 × 10−2\\n2 × 10−4\\n5 × 10−4\\nN\\n32768\\n64\\n32\\n• Last.FM [22] contains musician listening recodes from the Last.FM music streaming service.\\nWe consider artists as items and the number of listens as ratings. In particular, we utilize the\\nHetRec 2011 version in our study.\\nTo adapt the dataset for the recommendation task in a federated learning environment, several steps\\nare taken. Firstly, only the user-item interactions are retained from the original dataset, while other\\ndata are discarded. Then, the publicly available Microsoft Satori is utilized to create a knowledge\\ngraph by selecting triples with a confidence level greater than 0.9, where the tail corresponds to items\\nin the dataset. Interactions, where the item is not present in the knowledge graph, are subsequently\\nremoved. Next, these three datasets are transformed into implicit feedback. We consider all artists\\nlistened to in Last.FM, all books with ratings present in book-cross, and all movies with ratings\\ngreater than or equal to 4 stars in MovieLens, as positive feedback. Conversely, items not meeting\\nthese criteria are treated as negative feedback. Lastly, since the original recommendation dataset\\nalready contains user information, each user’s data is assigned to the corresponding client to generate\\na federated learning dataset. Details of the dataset are shown in Table 1\\n4.2\\nBaselines\\nWe compare the proposed FEDRKG with the following baselines, in which the first two baselines\\nare KG-free while the rest are all KG-aware methods. Hyper-parameter settings for baselines are\\nintroduced in the next subsection.\\n• SVD [23] is a classical CF recommendation algorithm based on matrix decomposition. Here\\nwe use an unbiased version.\\n• LibFM [24] is a method based on Factorization Machines that captures the similarity\\nbetween features\\n• PER [25] is an algorithm based on a personalized attention mechanism and constructs a\\nMeta-path between users and items through a heterogeneous graph (KG).\\n• CKE [26] is a knowledge graph-based collaborative embedding recommendation algorithm\\nthat combines data from CF and other modalities.\\n• RippleNet [27] is a memory-network-like approach that simulates and exploits the ripple\\neffect between users and items to propagate information on the knowledge graph\\n• KGCN [11] is a KG-based method, that achieves efficient recommendations by merging\\nKG and CF data.\\n• FedMF [6] is a recommendation algorithm based on matrix decomposition while protecting\\nprivacy through an encryption mechanism.\\n• FedGNN [7] is a GNN-based recommendation algorithm that uses homomorphic encryption\\nfor aggregation and protects the original gradient by differential privacy and pseudo-labeling.\\n8\\nTable 2: Results for CRT prediction. KGCN achieves the best AUC among the first five centralized\\nlearning methods. Our method performs best in the next three federal learning methods, while the\\ngap with KGCN is acceptable.\\nModel\\nMovieLens-20M\\nBook-Crossing\\nLast.FM\\nAUC\\nF1\\nAUC\\nF1\\nAUC\\nF1\\nSVD\\n0.952(±0.013)\\n0.909(±0.014)\\n0.665(±0.058)\\n0.628(±0.051)\\n0.760(±0.026)\\n0.688(±0.022)\\nLibFM\\n0.960(±0.018)\\n0.907(±0.024)\\n0.692(±0.046)\\n0.619(±0.063)\\n0.779(±0.019)\\n0.711(±0.011)\\nPER\\n0.824(±0.119)\\n0.780(±0.121)\\n0.611(±0.101)\\n0.557(±0.100)\\n0.627(±0.125)\\n0.593(±0.107)\\nCKE\\n0.918(±0.050)\\n0.866(±0.056)\\n0.673(±0.057)\\n0.607(±0.055)\\n0.739(±0.044)\\n0.669(±0.046)\\nRippleNet\\n0.964(±0.010)\\n0.909(±0.020)\\n0.712(±0.023)\\n0.648(±0.032)\\n0.777(±0.016)\\n0.699(±0.015)\\nKGCN\\n0.978(±0.002)\\n0.932(±0.001)\\n0.738(±0.003)\\n0.688(±0.006)\\n0.794(±0.002)\\n0.719(±0.003)\\nFedMF\\n0.865(±0.012)\\n0.852(±0.015)\\n0.657(±0.039)\\n0.605(±0.060)\\n0.720(±0.018)\\n0.660(±0.013)\\nFedGNN\\n0.939(±0.011)\\n0.891(±0.021)\\n0.671(±0.024)\\n0.620(±0.037)\\n0.753(±0.014)\\n0.681(±0.028)\\nFEDRKG\\n0.970(±0.002)\\n0.919(±0.002)\\n0.724(±0.004)\\n0.667(±0.006)\\n0.785(±0.004)\\n0.708(±0.002)\\n4.3\\nExperimental Settings\\nTable 2 shows the hyperparameter for the experiments. We split the datasets into training, validation,\\nand testing sets in a 6:2:2 ratio. AUC and F1 scores are used as evaluation metrics for click-through\\nrate (CTR) prediction.\\nFor the Last.FM, Book-Crossing, and MovieLens-20M datasets, the SVD method is applied with\\nimensions (8, 8, 8) and learning rates (0.1, 0.5, 0.5). For LibFM, the dimensions are (8, 1, 1). PER\\nutilizes the user-item-attribute-item meta-path, with dimensions (64, 128, 64) and learning rates\\n(0.1, 0.5, 0.5). The learning rates for KG in CKE are (0.1, 0.1, 0.1), while the dimensions are\\n(16, 4, 8) and the H values are (3, 3, 2). RippleNet’s dimensions are (16, 4, 8), H values are (3, 3, 2),\\nlearning rates are (0.005, 0.001, 0.01), regularization parameters λ1 are (10−5, 10−5, 10−6), and λ2\\nare (0.02, 0.01, 0.01). Other hyperparameters remain the same as in the original papers, and the\\nfederated learning settings are consistent with this paper.\\n4.4\\nOverall Comparison\\nWe conduct a comprehensive comparison of multiple models under various settings. Given the\\ndataset’s specific characteristics, only including knowledge graphs and user-item graphs, many\\nfederated learning algorithms simplify to FedGNN in this dataset. Therefore, we select FedGNN\\nand FedMF as the baseline methods, representing GNN and matrix decomposition approaches in\\nfederated learning. The experimental results for CTR prediction are presented in Table 2, while Fig. 4\\nillustrates the outcomes of top-k recommendation. Based on those results, we draw the following\\nconclusions:\\n• On the one hand, GNN-based algorithms, such as KGCN and FEDRKG, outperform\\nmatrix decomposition-based algorithms like SVD and FedMF. This is due to the superior\\nperformance of GNNs in automatically capturing user preferences and enabling the spreading\\nof user or item embeddings to neighboring nodes. On the other hand, algorithms that require\\nmanual design such as meta-paths for PER and knowledge graph embedding (KGE) method\\nfor CKE, often underperform due to the complexity of graph data.\\n• The experimental results consistently demonstrate that the appropriate utilization of addi-\\ntional side information can significantly improve the accuracy of recommendation systems.\\nFor example, KGCN and RippleNet outperform other centralized algorithms regarding both\\nAUC and F1 metrics, while FEDRKG, as a knowledge graph-based algorithm, performs\\nbest in federated learning. However, it should be noted that not all methods that leverage\\nside information deliver satisfactory outcomes. This holds true for methods like PER and\\nCKE, which encounter difficulties in effectively harnessing side information.\\n• Knowledge graphs are well-suited for integration into recommendation systems as side\\ninformation, especially using GNNs, given their inherent graph structure and the ability to\\ncombine multi-domain knowledge. Algorithms incorporating relation-aware aggregation,\\nsuch as KGCN and FEDRKG, achieve the best performance in their respective settings,\\nconfirming the effectiveness of introducing relational attention mechanisms.\\n9\\nOverall, our framework outperforms existing federated learning algorithms and achieves competitive\\nperformance compared to centralized algorithms.\\n(a) MovieLens-20M\\n(b) Book-Crossing\\n(c) Last.FM\\nFigure 4: Results for top-K recommendation. The dashed line represents centralized learning,\\nwhile the solid line represents federated learning. Our method surpasses all federated baselines and,\\nfurthermore, achieves competitive results compared to centralized learning.\\n4.5\\nSensitivity Analysis\\n4.5.1\\nActivated client number\\nIn general, a smaller number of activated clients in each training round will speed up the model\\nconvergence and conversely better capture the global user data distribution. We test the algorithm\\non three datasets with three different numbers of activation clients, and the results are shown in the\\nfigure above. Probably due to the sparse data and a large number of clients, a small adjustment has a\\nlimited impact on the final results and the Last.FM and Book-Crossing datasets both show a small\\ndecrease in AUC when 64 clients are activated.\\n4.5.2\\nReceptive field depth\\nBy testing different receptive field depths, we note that an excessive receptive field reduces model\\nprediction accuracy. As data sparsity decreases, better performance needs a larger receptive field,\\nwhile a one-layer perceptual region is sufficient to achieve better performance on those sparse data\\nsets.\\nFigure 5: Sensitivity analysis of activated clients and receptive field depth.\\n4.5.3\\nInteraction item protection\\nWe introduce new interaction record protection and assess diverse flipping rates, with corresponding\\nresults depicted in Fig. 6. Generally, integrating privacy-preserving mechanisms often diminishes\\nrecommendation accuracy. However given limited client-side graph data, our scenario tends to induce\\nmodel overfitting. Hence, proper regularization effectively enhances recommendation accuracy\\n10\\nFigure 6: Effect of flipping rate on AUC.\\nand privacy protection. Notably, excessive flip rates can compromise system performance despite\\nheightened privacy. Our experiments indicate a balance between accuracy and privacy at a flipping\\nrate of 0.1.\\n5\\nConclusion\\nThis paper introduces a novel federated learning framework, FEDRKG, which employs GNN for\\nrecommendation tasks. Our approach integrates KG information while upholding user privacy. The\\nlimitation here is the absence of user connections, and our forthcoming focus is on improving the\\nefficiency and interpretability of utilizing existing user connections without introducing new private\\ndata. Specifically, a server-side KG is created from public item data, maintaining relevant embeddings.\\nThe client conceals local interaction items and requests server training data. The server samples\\na KG subgraph and distributes it with the GNN model to the client. The client then expands its\\nuser-item graph with the KG subgraph for training, uploading the gradient for server aggregation.\\nOur framework creates higher-order interactions without extra privacy data, relying solely on public\\ninformation for KG. Sampled KG subgraphs enhance local training by capturing interactions between\\nusers and items without direct links. We employ LDP and pseudo-labeling to protect privacy and\\nreduce overhead by requesting partial data. Gradients are encrypted using LDP for user preference\\nprotection and local user embedding storage. Experimental results on three datasets demonstrate our\\nframework’s superiority over SOTA federated learning recommendation methods. It also performs\\ncompetitively against centralized algorithms while preserving privacy.\\n5.0.1\\nAcknowledgements\\nThis work is supported by the National Key Research and Development Program of China under\\nGrant No.2021YFB1714600 and the National Natural Science Foundation of China under Grant\\nNo.62072204 and No.62032008. The computation is completed in the HPC Platform of Huazhong\\nUniversity of Science and Technology and supported by the National Supercomputing Center in\\nZhengzhou.\\nReferences\\n[1] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. Kgat: Knowledge graph\\nattention network for recommendation. In Proceedings of the 25th ACM SIGKDD International\\nConference on Knowledge Discovery and Data Mining, KDD, pages 950–958, 2019.\\n[2] Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, and Qing He. A\\nsurvey on knowledge graph-based recommender systems. IEEE Transactions on Knowledge\\nand Data Engineering, 34(8):3549–3568, 2020.\\n[3] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng\\nWang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and\\napplications. AI Open, 1:57–81, 2020.\\n[4] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph neural networks in recom-\\nmender systems: a survey. ACM Computing Surveys, 55(5):1–37, 2022.\\n11\\n[5] Paul Voigt and Axel Von dem Bussche. The EU general data protection regulation (GDPR).\\nA Practical Guide, 1st Ed., Cham: Springer International Publishing, 10(3152676):10–5555,\\n2017.\\n[6] Di Chai, Leye Wang, Kai Chen, and Qiang Yang. Secure federated matrix factorization. IEEE\\nIntelligent Systems, 36(5):11–20, 2020.\\n[7] Chuhan Wu, Fangzhao Wu, Yang Cao, Yongfeng Huang, and Xing Xie. FedGNN: Federated\\ngraph neural network for privacy-preserving recommendation. arXiv preprint arXiv:2102.04925,\\n2021.\\n[8] Zhiwei Liu, Liangwei Yang, Ziwei Fan, Hao Peng, and Philip S Yu. Federated social recommen-\\ndation with graph neural network. ACM Transactions on Intelligent Systems and Technology,\\n13(4):1–24, 2022.\\n[9] Ze Wang, Guangyan Lin, Huobin Tan, Qinghong Chen, and Xiyang Liu. Ckan: collaborative\\nknowledge-aware attentive network for recommender systems. In Proceedings of the 43rd\\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval,\\nSIGIR, pages 219–228, 2020.\\n[10] Rui Sun, Xuezhi Cao, Yan Zhao, Junchen Wan, Kun Zhou, Fuzheng Zhang, Zhongyuan Wang,\\nand Kai Zheng. Multi-modal knowledge graphs for recommender systems. In Proceedings\\nof the 29th ACM International Conference on Information & Knowledge Managemen, CIKM,\\npages 1405–1414, 2020.\\n[11] Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. Knowledge graph convolu-\\ntional networks for recommender systems. In Proceedings of the World Wide Web Conference,\\nWWW, pages 3307–3313, 2019.\\n[12] Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, Jure Leskovec, Miao Zhao, Wenjie Li, and\\nZhongyuan Wang. Knowledge-aware graph neural networks with label smoothness regular-\\nization for recommender systems. In Proceedings of the 25th ACM SIGKDD International\\nConference on Knowledge Discovery and Data Mining, KDD, pages 968–977, 2019.\\n[13] Hai Jin, Dongshan Bai, Dezhong Yao, Yutong Dai, Lin Gu, Chen Yu, and Lichao Sun. Personal-\\nized edge intelligence via federated self-knowledge distillation. IEEE Transactions on Parallel\\nand Distributed Systems, 34(2):567–580, 2023.\\n[14] Ke Zhang, Carl Yang, Xiaoxiao Li, Lichao Sun, and Siu Ming Yiu. Subgraph federated\\nlearning with missing neighbor generation. In Proceedings of the Annual Conference on Neural\\nInformation Processing Systems, NeurIPS, volume 34, pages 6671–6682, 2021.\\n[15] Hao Peng, Haoran Li, Yangqiu Song, Vincent Zheng, and Jianxin Li. Differentially private fed-\\nerated knowledge graphs embedding. In Proceedings of the 30th ACM International Conference\\non Information & Knowledge Management, CIKM, pages 1416–1425, 2021.\\n[16] Muhammad Ammad-Ud-Din, Elena Ivannikova, Suleiman A Khan, Were Oyomno, Qiang Fu,\\nKuan Eeik Tan, and Adrian Flanagan. Federated collaborative filtering for privacy-preserving\\npersonalized recommendation system. arXiv preprint arXiv:1901.09888, 2019.\\n[17] Guoren Wang, Yue Zeng, Rong-Hua Li, Hongchao Qin, Xuanhua Shi, Yubin Xia, Xuequn\\nShang, and Liang Hong. Temporal graph cube. IEEE Transactions on Knowledge and Data\\nEngineering, pages 1–15, 2023.\\n[18] Wenming Cao, Canta Zheng, Zhiyue Yan, and Weixin Xie. Geometric deep learning: progress,\\napplications and challenges. Science China Information Sciences, 65(2):126101, 2022.\\n[19] Yuanyishu Tian, Yao Wan, Lingjuan Lyu, Dezhong Yao, Hai Jin, and Lichao Sun. FedBERT:\\nWhen federated learning meets pre-training. ACM Transactions on Intelligent Systems and\\nTechnology, 13(4):1–26, 2022.\\n[20] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. ACM\\nTransactions on Interactive Intelligent Systems, 5(4):19:1–19:19, 2016.\\n[21] Cai-Nicolas Ziegler, Sean M McNee, Joseph A Konstan, and Georg Lausen. Improving\\nrecommendation lists through topic diversification. In Proceedings of the World Wide Web\\nConference, WWW, pages 22–32, 2005.\\n[22] Iván Cantador, Peter Brusilovsky, and Tsvi Kuflik. Second workshop on information hetero-\\ngeneity and fusion in recommender systems (HetRec2011). In Proceedings of the 2011 ACM\\nConference on Recommender Systems, RecSys, pages 387–388, 2011.\\n12\\n[23] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering\\nmodel. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge\\nDiscovery and Data Mining, KDD, pages 426–434, 2008.\\n[24] Steffen Rendle. Factorization machines with libfm. ACM Transactions on Intelligent Systems\\nand Technology, 3(3):1–22, 2012.\\n[25] Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandelwal, Brandon\\nNorick, and Jiawei Han. Personalized entity recommendation: A heterogeneous information\\nnetwork approach. In Proceedings of the 7th ACM International Conference on Web Search\\nand Data Mining, WSDM, pages 283–292, 2014.\\n[26] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. Collaborative\\nknowledge base embedding for recommender systems. In Proceedings of the 22nd ACM\\nSIGKDD International Conference on Knowledge Discovery and Data Mining, KDD, pages\\n353–362, 2016.\\n[27] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo.\\nRipplenet: Propagating user preferences on the knowledge graph for recommender systems.\\nIn Proceedings of the 27th ACM International Conference on Information and Knowledge\\nManagement, CIKM, pages 417–426, 2018.\\n13\\n'},\n",
       " {'abstract': \"Cross-Domain Facial Expression Recognition (CD-FER) faces challenges in feature representation and transferability due to the domain shift. This paper introduces a framework called Adaptive Global-Local Representation Learning and Selection (AGLRLS) to overcome these challenges. The framework consists of separate global-local adversarial learning and semantic-aware pseudo label generation. Specifically, it includes global and local adversarial learning modules that independently learn domain-invariant global and local features. There's also a pseudo label generation mechanism that assigns pseudo class labels to unlabeled data features, preventing mutual interference among classifiers. In the training process, separate classifier learning is performed by optimizing the model with the adversarial learning process using these labels, aiding in the learning of more discriminative features for the target dataset. During inference, a global-local prediction consistency strategy is used to infer the optimal class label by combining the prediction scores from the global and local classifiers. Comprehensive experiments demonstrate the superiority of AGLRLS in CD-FER.\",\n",
       "  'introduction': \"CD-FER aims to automatically determine a person's emotional state from a facial image, regardless of the domain. It's challenging due to variations in different datasets and large discrepancy among FER datasets, resulting in domain shift. In recent years, significant efforts have been devoted to domain adaptation models for CD-FER. However, these models primarily focused on holistic features for domain adaptation, overlooking the potential benefits of local features, like greater transferability and fine-grained representation of variations. Previous research combined graph representation propagation with adversarial learning to reduce domain shift, while others incorporated semantic information into the multi-view features learning to bridge the semantic gap. Despite these advances, the significant problem of imbalanced class distribution was largely overlooked, leading to suboptimal performance.\",\n",
       "  'literature_review': 'A series of CD-FER methods has been proposed to address data bias among different FER datasets. These methods utilize subspace learning, metric learning, dictionary learning, contrastive learning, and other techniques to facilitate unsupervised CD-FER. Recent years have seen the emergence of domain adaptation models that employ adversarial learning mechanisms to mitigate domain shift. Inspired by generative adversarial networks, these methods use a feature extractor and a domain discriminator to learn transferable domain-invariant features through a two-player game. The feature extractor learns the features, while the domain discriminator struggles to distinguish samples from the target dataset. Several methods have achieved state-of-the-art performance on FER benchmark datasets. However, they tend to focus on holistic features for adaptation, neglecting the potential advantages of local features in addressing the domain shift and improving discriminability.',\n",
       "  'methodology': \"The proposed AGLRLS framework comprises separate global-local adversarial learning and semantic-aware pseudo label generation. It integrates global and local adversarial learning modules that independently learn domain-invariant global and local features. Furthermore, a feature-level pseudo label generation mechanism is designed to assign reliable pseudo labels to each global and local feature of unlabeled data. These labels are utilized for model optimization through the adversarial learning process, enhancing the learned features' generalization and representation capacities. During inference, a global-local prediction consistency strategy is devised to refine recognition results by combining the prediction scores from the global and local classifiers. The framework effectively mitigates domain shift and amplifies the discriminative power in the target domain.\",\n",
       "  'results': \"Extensive experiments are conducted to evaluate the effectiveness of AGLRLS against state-of-the-art CD-FER methods. The results demonstrate that AGLRLS consistently outperforms competing algorithms across various configurations and datasets. The analysis highlights the robustness of AGLRLS with different backbones and source domains. Statistical tests further confirm the significance of AGLRLS's performance. Ablation studies reveal the contributions of individual components, underscoring the effectiveness of the proposed global-local representation learning and selection strategy.\",\n",
       "  'conclusion': 'The proposed AGLRLS framework showcases superior performance in CD-FER tasks, effectively addressing the issues of data inconsistency and less discriminative ability prevalent in existing methods. It seamlessly integrates global-local adversarial learning and semantic-aware pseudo label generation to learn domain-invariant and discriminative features that generalize well across source and target datasets. In addition, the global-local prediction consistency strategy enhances the reliability of recognition results during inference. AGLRLS demonstrates state-of-the-art performance on various benchmark datasets, setting a new standard for CD-FER. The findings contribute to the development of more effective and robust facial expression recognition systems capable of handling cross-domain scenarios.',\n",
       "  'title': 'Adaptive Global-Local Representation Learning and Selection for Cross-Domain Facial Expression Recognition',\n",
       "  'author': 'Yuefang Gao, Yuhao Xie, Zeke Zexi Hu, Tianshui Chen, Liang Lin',\n",
       "  'textdata': 'IEEE TRANSACTIONS ON MULTIMEDIA\\n1\\nAdaptive Global-Local Representation Learning and\\nSelection for Cross-Domain Facial Expression\\nRecognition\\nYuefang Gao, Yuhao Xie, Zeke Zexi Hu, Tianshui Chen, Liang Lin\\nAbstract—Domain shift poses a significant challenge in Cross-\\nDomain Facial Expression Recognition (CD-FER) due to the\\ndistribution variation between the source and target domains.\\nCurrent algorithms mainly focus on learning domain-invariant\\nfeatures through global feature adaptation, while neglecting\\nthe transferability of local features across different domains.\\nAdditionally, these algorithms lack discriminative supervision\\nduring training on target datasets, resulting in deteriorated\\nfeature representation in the target domain. To address these\\nlimitations, we propose an Adaptive Global-Local Representation\\nLearning and Selection (AGLRLS) framework. The framework\\nincorporates global-local adversarial adaptation and semantic-\\naware pseudo label generation to enhance the learning of domain-\\ninvariant and discriminative feature representation during train-\\ning. Meanwhile, a global-local prediction consistency learning\\nis introduced to improve classification results during inference.\\nSpecifically, the framework consists of separate global-local\\nadversarial learning modules that learn domain-invariant global\\nand local features independently. We also design a semantic-\\naware pseudo label generation module, which computes semantic\\nlabels based on global and local features. Moreover, a novel\\ndynamic threshold strategy is employed to learn the optimal\\nthresholds by leveraging independent prediction of global and\\nlocal features, ensuring filtering out the unreliable pseudo labels\\nwhile retaining reliable ones. These labels are utilized for model\\noptimization through the adversarial learning process in an\\nend-to-end manner. During inference, a global-local prediction\\nconsistency module is developed to automatically learn an optimal\\nresult from multiple predictions. To validate the effectiveness\\nof our framework, we conduct comprehensive experiments and\\nanalysis based on a fair evaluation benchmark. The results\\ndemonstrate that the proposed framework outperforms the\\ncurrent competing methods by a substantial margin.\\nIndex\\nTerms—Domain\\nadaptation,\\nAdverserial\\nlearning,\\nPseudo label generation, Facial expression recognition\\nI. INTRODUCTION\\nCross-Domain Facial Expression Recognition (CD-FER)\\naims at automatically determining a person’s emotional state\\nfrom a face image, regardless of the domain, by transferring\\nthe learned knowledge from labeled source data to unlabeled\\nThis work was supported in part by National Key R&D Program of\\nChina (Grant No. 2021ZD0111601), in part by the National Natural Science\\nFoundation of China (Grant No. 62206060) and Guangzhou Basic and Applied\\nBasic Research Foundation (Grant No. SL2022A04J01626). (Corresponding\\nauthor: Tianshui Chen)\\nYuefang Gao is with South China Agricultural University, Guangzhou,\\nChina. Yuhao Xie is with Guangzhou Institute of Technology, Xidian Uni-\\nversity, Guangzhou, China. Zeke Zexi Hu is with the School of Computer\\nScience, University of Sydney, Darlington, New South Wales, Australia.\\nTianshui Chen is with Guangdong University of Technology, Guangzhou,\\nChina. Liang Lin is with Sun Yat-Sen University, Guangzhou, China.\\ntarget data. It plays a significant role in human-computer\\ninteraction [1], affective computing [2], intelligent driving [3],\\nand so forth. While conventional facial expression recognition\\nmethods focus on a single dataset [4]–[7], CD-FER is more\\nchallenging due to the subtle interclass variations in different\\nfacial expression classes and the large discrepancy among FER\\ndatasets. These variations result in evident domain shift, which\\nundermines models to generalize to more datasets.\\nIn the past decade, various CD-FER methods have been\\nproposed and evaluated on popular FER datasets such as RAF-\\nDB [8], FER2013 [9], CK+ [10], JAFFE [11], SFEW2.0 [12],\\nand ExpW [13]. These methods aimed to address the issue of\\ndata inconsistency between different datasets. Earlier studies\\nhave tackled this problem through transfer learning [14] and\\nsupervised kernel matching [15]. However, these methods re-\\nquire a number of annotated samples from the target domain to\\nensure the discriminative information among categories, which\\nis not suitable for unsupervised CD-FER settings. More recent\\nstrategies attempted to incorporate other learning techniques,\\nincluding dictionary learning [16], metric learning [17], and\\ncontrastive learning [18], to facilitate unsupervised CD-FER.\\nBesides, some methods focused on generating more additional\\nsamples to bridge the gap in feature distribution between the\\nsource and target datasets [19], [20].\\nIn recent years, significant efforts have been devoted to do-\\nmain adaptation models for the CD-FER. These models aimed\\nto learn transferable domain-invariant features by employing\\nadversarial learning mechanisms [21]–[23]. By aligning the\\nfeature distribution of the source and target domains, these\\nmethods achieved high recognition accuracy. However, de-\\nspite their outstanding performance, these models primarily\\nfocused on extracting holistic features for domain adaptation,\\nwhile overlooking the potential benefits of local features,\\ne.g. greater transferability across domains and a more fine-\\ngrained representation of variations, which led to domain shift\\nin inappropriate adversarial learning. To address the limita-\\ntion, some studies [24], [25] combined graph representation\\npropagation with adversarial learning to reduce the domain\\nshift by modeling the correlation of holistic-local features\\nwithin each domain and across different domains. There were\\nalso some works incorporating semantic information into\\nthe multi-view features learning to bridge the semantic gap\\nduring domain adaptation [26], [27]. These studies utilized\\nthe category prior knowledge or imposed the global and\\nlocal semantic consistency constraints to learn semantically\\ndiscriminative features. Nevertheless, the significant problem\\narXiv:2401.11085v1  [cs.CV]  20 Jan 2024\\nIEEE TRANSACTIONS ON MULTIMEDIA\\n2\\nof imbalanced class distribution was largely overlooked among\\nthese methods, leading to suboptimal performance.\\nIn this paper, we introduce an adaptive global-local repre-\\nsentation learning and selection model to address the issue\\nof data inconsistency, which adversely affects the recognition\\nperformance in cross-domain scenarios. The proposed method\\nlearns domain-invariant and discriminative feature represen-\\ntation in training by incorporating global-local adversarial\\nadaptation and semantic-aware pseudo label generation, fol-\\nlowed by a global-local prediction consistency strategy to\\nrefine recognition results in inference. Specifically, the model\\nutilizes global and local adversarial learning independently\\nto obtain compact and discriminative domain-invariant global\\nand local features. Subsequently, a feature-level pseudo label\\ngeneration mechanism is introduced to assign pseudo class\\nlabels to the features of unlabeled data, preventing mutual\\ninterference among classifiers. The separate classifier learn-\\ning is further performed by optimizing the model with the\\nadversarial learning process using these labels, facilitating\\nthe learning of more discriminative features for the target\\ndataset. During inference, a global-local prediction consistency\\nstrategy is designed to learn the optimal class label by joint\\ninference of the global and local prediction results. In this\\nway, our approach significantly enhances the learned features,\\nbolstering their generalization and representation capacities\\nacross the source and target datasets and meanwhile amplify-\\ning their discriminative power in the target domain, effectively\\nmitigating domain shift to facilitate the CD-FER performance.\\nBuilding upon our previous conference work [27], this\\npaper presents several improvements. Firstly, instead of using\\na strict global-local semantic consistency constraint in the\\npseudo target label generation process, we adopt an adjustable\\nthreshold learning process to generate richer and more fine-\\ngrained pseudo labels for all the categories of the unlabeled\\nfaces, effectively alleviating the problem of class imbalance\\ncaused by source domain. Secondly, a global-local prediction\\nconsistency strategy is developed to infer the final labels of\\ntarget data by combining the prediction scores from the global\\nand local classifiers. This strategy mitigates the problem of\\ndomain shift across domains and achieves competitive perfor-\\nmance. Finally, we conduct extensive experiments and ablation\\nstudies using more datasets and backbones to demonstrate\\nthe effectiveness of our method and the contribution of each\\ncomponent.\\nThe contributions of this work can be summarized as\\nfollows: i) An Adaptive Global-Local Representation Learning\\nand Selection (AGLRLS) model is proposed, which incorpo-\\nrates separate global-local adversarial learning and semantic-\\naware pseudo label generation to learn more domain-invariant\\nand discriminative feature representation and thus address the\\ndomain shift and less discriminative ability issues in current\\nCD-FER methods. ii) A semantic-aware pseudo label genera-\\ntion method is designed to produce reliable pseudo labels for\\neach global and local feature of unlabeled data by utilizing\\nlearned adaptive thresholds. iii) A global-local prediction con-\\nsistency learning is introduced that integrates global and local\\nprediction results to infer the optimal class label, effectively\\nbridging the semantic gap between source and target domains.\\niv) Comprehensive experiments are conducted to compare our\\nproposed method with current CD-FER algorithms, demon-\\nstrating its superior performance. Codes and trained models are\\navailable at https://github.com/yao-papercodes/AGLRLS.\\nII. RELATED WORKS\\nQuite a lot of research works have addressed the challenging\\ntask of cross-domain facial expression recognition. In this\\nsection, we mainly present a few methods that are closely re-\\nlated to this work, namely, cross-domain FER and adversarial\\ndomain adaptation.\\nA. Cross-Domain FER\\nTo deal with the data bias among different FER datasets,\\na series of CD-FER methods have been proposed [14], [17],\\n[19], [20], [26], [28]–[33]. Yan et al. [14] utilized subspace\\nlearning to transfer the learned knowledge from the source data\\nto the unlabeled target data. However, annotating some sam-\\nples from the target data is necessary for this method. To han-\\ndle CD-FER in unsupervised scenarios, work [29] presented\\na domain adaptive dictionary learning model that unified the\\nunlabeled data to adaptively adjust the misaligned distribution\\nin an embedded space. Later on, a transductive transfer regu-\\nlarized least-squares regression model was proposed [32] that\\nlearned a discriminative subspace by combining the labeled\\nsource data and unlabeled auxiliary target data to reduce the\\ndissimilarity of the marginal probability distribution between\\ndomains. In [17], a discriminative metric space was learned in\\na dictionary learning procedure to alleviate the influence of the\\ndistribution inconsistency. Different from these works, Wang\\net al. [19] used samples generated by Generative Adversarial\\nNetwork (GAN) on the target dataset to facilitate the CD-\\nFER. Another domain regenerator was designed in [20] that\\nregenerated source and target domain samples with the same\\nor similar feature distribution to guide the label prediction of\\nthe unlabeled data.\\nIn the recently proposed methods, Ji et al. [34] attempted\\nto learn a common representation of expression in different\\ndomains through the fusion of intra-category common features\\nand inter-category distinction features. [35] developed a deep\\nemotion conditional adaptation network to learn the domain-\\ninvariant and class discriminative feature representation. This\\nmethod aligned the marginal distribution globally and matched\\nthe fine-grained class-conditional distribution using the under-\\nlying label information on target datasets that can effectively\\nmitigate the data bias between domains. In addition, Li et\\nal. [26] proposed the deep margin-sensitive representation\\nlearning model that extracted semantically multi-level discrim-\\ninative and transferable features by leveraging the category\\nprior knowledge during domain adaptation to alleviate the\\ndomain shift. A recent study [31] introduced an emotion-\\nguided similarity network that learned a transferable model\\nfor compound expression recognition from the unseen domain\\nin the cross-domain few-shot learning scenario. In contrast\\nto the current methods, we propose an adaptive global-local\\nrepresentation learning and selection method to address the\\nproblem of data inconsistency in CD-FER.\\nIEEE TRANSACTIONS ON MULTIMEDIA\\n3\\nB. Adversarial Domain Adaptation\\nAdversarial domain adaptation methods have recently be-\\ncome increasingly popular for cross-domain recognition due\\nto the advantage of the disentangled and transferable repre-\\nsentation learning [21], [24], [25], [27], [36]–[41]. Inspired\\nby the adversarial learning in generative adversarial networks,\\nthese methods utilize a feature extractor and a domain dis-\\ncriminator to mitigate domain shift through a two-player game\\nin which the feature extractor learns the transferable domain-\\ninvariant features while the domain discriminator struggles to\\ndistinguish samples from the target dataset. As a pioneering\\neffort, Tzeng et al. [36] presented an adversarial discriminative\\ndomain adaptation method that combined discriminative mod-\\neling, united weight sharing, and adversarial loss to achieve\\nbetter performance in challenging cross-modality classification\\ntask. Subsequent research [39] applied the condition adver-\\nsarial mechanism to the domain discriminator on the class\\ninformation and proposed a conditional domain adversarial\\nnetwork with multilinear conditioning and entropy condition-\\ning strategies to further improve the discriminability and trans-\\nferability of the classifier. In [22], an unsupervised adversarial\\ndomain adaptation model was designed that integrated three\\nlearning strategies to adapt the pose and expression distribution\\nbetween the source and target domain and learned the pose-\\nand-identity features with robustness. Rather than focusing\\non holistic features for adaptation, some works [24], [25]\\nintegrated graph representation propagation [42], [43] with\\nadversarial learning to learn more representative and domain-\\ninvariant global-local features. Despite achieving superior per-\\nformance on several publicly available face expression bench-\\nmark datasets, the learned features are less discriminative due\\nto the lack of direct supervision for the samples of the target\\ndomain.\\nRecent works show a tendency to incorporate the semantic-\\naware strategy while learning the feature representation [21],\\n[26], [27], [44], [45]. To reduce the semantic gap during do-\\nmain distribution alignment, Bozorgtabar et al. [21] employed\\nadversarial domain adaptation to transform the visual appear-\\nance of the images from different domains while preserving\\nthe semantic information. In [27], a consistent global-local\\nand semantic learning method was proposed that integrated\\nthe domain-invariant global-local features and consistent se-\\nmantic learning to further mitigate the problem of semantic\\ninconsistency during domain adaptation. However, the method\\nemployed fixed criteria in the pseudo label generation process,\\nwhich could lead to only a limited number of expression\\nclasses having the capability to generate pseudo labels. In\\ncontrast to the previously mentioned methods, our proposed\\napproach focuses on the domain-invariant multi-scale features\\nthrough separate global and local adversarial learning and\\npreserves the underlying semantic consistency by the global-\\nlocal unified prediction selection strategy.\\nIII. PROPOSED METHOD\\nA. Overview\\nOur objective is to tackle the Cross-Domain Facial Ex-\\npression Recognition (CD-FER) task where we are given a\\nsource dataset Ds = {(xs\\ni, ys\\ni )}ns\\ni=1 and a target dataset Dt =\\n\\x08IEEE TRANSACTIONS ON MULTIMEDIA\\n4\\n𝑥𝑗\\n𝑡\\nSamples from target domain:\\n𝑥𝑖\\n𝑠\\nSamples from source domain:\\nCrop\\n-Net\\nF\\nF\\n𝑫𝟎\\n𝑫𝟏\\n𝑫𝟐\\n𝑫𝟑\\n𝑫𝟒\\n𝑫𝟓\\n𝑫𝟔\\n𝐟𝟎\\n𝐟𝟏\\n𝐟𝟐\\n𝐟𝟑\\n𝐟𝟒\\n𝐟𝟓\\n𝐟𝟔\\nSAL\\n𝑮𝟎\\n𝑮𝟏\\n𝑮𝟐\\n𝑮𝟑\\n𝑮𝟒\\n𝑮𝟓\\n𝑮𝟔\\n𝐟𝟎\\n𝐟𝟏\\n𝐟𝟐\\n𝐟𝟑\\n𝐟𝟒\\n𝐟𝟓\\n𝐟𝟔\\n𝑦0/ො𝑦0\\nSCL\\n𝐟𝟎\\n𝒊\\n𝐟𝟏\\n𝒊\\n𝐟𝟐\\n𝒊\\n𝐟𝟑\\n𝒊\\n𝐟𝟒\\n𝒊\\n𝐟𝟓\\n𝒊\\n𝐟𝟔\\n𝒊\\nF(𝑥𝑖\\n𝑠)\\n𝐟𝟏\\n𝒋\\n𝐟𝟐\\n𝒋\\n𝐟𝟑\\n𝒋\\n𝐟𝟒\\n𝒋\\n𝐟𝟓\\n𝒋\\n𝐟𝟔\\n𝒋\\nF(𝑥𝑗\\n𝑡)\\n𝐟𝟎\\n𝒋\\n𝑫\\n𝑮\\nClassify separately\\n𝑫\\n𝑮\\nClassify separately\\nො𝑦0\\n𝑗, ො𝑦1\\n𝑗, ො𝑦2\\n𝑗, ො𝑦3\\n𝑗, ො𝑦4\\n𝑗, ො𝑦5\\n𝑗, ො𝑦6\\n𝑗\\nFPLG\\npredict\\nfilter\\nconvert\\nො𝑦0\\n𝑗\\nො𝑦1\\n𝑗\\nො𝑦2\\n𝑗\\nො𝑦3\\n𝑗\\nො𝑦4\\n𝑗\\nො𝑦5\\n𝑗\\nො𝑦6\\n𝑗\\n\\u0de1𝐟𝟎\\n𝐣\\n\\u0de1𝐟𝟏\\n𝐣\\n\\u0de1𝐟𝟐\\n𝐣\\n\\u0de1𝐟𝟑\\n𝐣\\n\\u0de1𝐟𝟒\\n𝐣\\n\\u0de1𝐟𝟓\\n𝐣\\n\\u0de1𝐟𝟔\\n𝐣\\nTraining\\nCrop-\\nNet & F\\nInference\\n𝐟𝟎\\n𝐟𝟏\\n𝐟𝟐\\n𝐟𝟑\\n𝐟𝟒\\n𝐟𝟓\\n𝐟𝟔\\n𝑮𝟎\\n𝑮𝟏\\n𝑮𝟐\\n𝑮𝟑\\n𝑮𝟒\\n𝑮𝟓\\n𝑮𝟔\\nC0\\nC1\\nC2\\nC3\\nC4\\nC5\\nC6\\nprediction probility\\nthreshold\\nC0\\nC1\\nC2\\nC3\\nC4\\nC5\\nC6\\nC0\\nC1\\nC2\\nC3\\nC4\\nC5\\nC6\\n…\\n…\\n…\\n…\\nC0\\nC1\\nC2\\nC3\\nC4\\nC5\\nC6\\nprediction probility\\nC0\\nC1\\nC2\\nC3\\nC4\\nC5\\nC6\\nC0\\nC1\\nC2\\nC3\\nC4\\nC5\\nC6\\nfilter\\nfilter\\nfilter\\n⊕\\n< C1-“Fear” >\\nGLPC\\n…\\n…\\n…\\n…\\nC0\\nC1\\nC2\\nC3\\nC4\\nC5\\nC6\\nprediction probility\\n𝑦0𝑖, 𝑦1𝑖, 𝑦2𝑖, 𝑦3𝑖, 𝑦4𝑖, 𝑦5𝑖 , 𝑦6𝑖\\n𝑦1/ො𝑦1\\n𝑦2/ො𝑦2\\n𝑦3/ො𝑦3\\n𝑦4/ො𝑦4\\n𝑦5/ො𝑦5\\n𝑦6/ො𝑦6\\nFig. 1. Illustration of the training and inference stages of our proposed AGLRLS model.\\nB. Domain-Invariant Representation Learning\\n1) Separate Adversarial Learning: In this subsection, we\\nwill present a detailed explanation of the separate adversarial\\nlearning process, denoted as SAL in Fig. 1. As the previous\\nwork [25] has suggested, local regions surrounding specific\\nfacial landmarks play a crucial role in expression recognition.\\nBuilding upon this insight, we adopt a strategy to extract not\\nonly the full-face image as the global image but also the\\nkey facial landmarks as the local images. These landmarks\\ninclude the left eye (le), right eye (re), nose (ne), left mouth\\ncorner (lm), and right mouth corner (rm). Following the\\naforementioned procedure, we obtain a global feature vector,\\ndenoted as fg, as well as five local feature vectors, namely fle,\\nfre, fne, flm, and frm. Additionally, we concatenate these six\\nvectors into a seventh vector, denoted as fgl. Consequently, a\\nfeature set f is formed, comprising seven vectors. The strategy\\ncan be mathematically expressed as follows,\\nfgl = fg ⊕ fle ⊕ fre ⊕ fne ⊕ flm ⊕ frm,\\nf = {fg, fle, fre, fne, flm, frm, fgl}\\n(5)\\nwhere ⊕ is the concatenation operation. To perform separate\\nadversarial learning, we utilize indices ranging from 0 to 6 to\\naccess the vector f. Thus, Equation (3) can be formulated as\\nbelow,\\nL (F, D) =\\n6\\nX\\ni=0\\n− βi·Ef s\\ni ∼Ds log [Di (f s\\ni )]\\n− βi·Ef t\\ni ∼Dt log\\n\\x02\\n1 − Di\\nIEEE TRANSACTIONS ON MULTIMEDIA\\n5\\ntarget domain. To achieve this, we introduce the Feature-level\\nPseudo Label Generation (FPLG) module in Fig. 1, which\\nindependently generates a corresponding pseudo label for each\\nlearned domain-invariant feature. Specifically, in Equation (5),\\nseven features are obtained from a target data xt, each of\\nwhich is subsequently fed into the corresponding classifier Gi\\nto make a prediction,\\nsi = Gi(f t\\ni ) =\\n\\x08\\nsi\\nj\\n\\t\\n, s.t.j = 0, .., c − 1.\\n(7)\\nwhere si represents the total prediction score of Gi based on\\nf t\\ni and si\\nj is the score that Gi predicts xt as class j based\\non the f t\\ni . c indicates the number of categories. The predicted\\nclass p is obtained from si with the maximum probability,\\np = argmax(si)\\n(8)\\nThen, with the prediction result of each classifier, we need to\\ndetermine whether the generated pseudo label is valid. Inspired\\nby FlexMatch [46], a dynamic threshold set ti is applied to\\neach classifier,\\nti =\\n\\x08\\nti\\nj\\n\\t\\n, s.t.j = 0, .., c − 1.\\n(9)\\nwhere ti\\nj represents the threshold that Gi needs to reach to\\ngenerate pseudo label for class j. ti\\nj is calculated by the\\nfollowing equation,\\nti\\nj = M(λi\\nj) × θ\\n(10)\\nwhere θ is a fixed threshold, M is a nonlinear mapping\\nfunction with respect to λi\\nj, and λi\\nj represents the proportion\\nof data that the classifier Gi has successfully generated pseudo\\nlabel for class j, which can be calculated as follows,\\nλi\\nj =\\nσi\\nj\\nmax σi\\n(11)\\nwhere σi represents the cumulative pseudo label quantities\\ngenerated by Gi for all categories until current training pro-\\ncess, while σi\\nj signifies the accumulation for class j. To ensure\\nthe sensitivity of M to λi\\nj, we improve the M function so that\\nthe mapped value is always greater than λi\\nj,\\nM(λi\\nj) = (λi\\nj + 1)2\\n4\\n(12)\\nFinally, we can obtain the final value of ˆyi by comparing si\\np\\nand ti\\np through the following formula,\\nˆyi =\\n\\x1a p,\\nsi\\np > ti\\np\\n−1,\\notherwise\\n(13)\\nwhere -1 indicates that the generation of a pseudo label has\\nfailed. We repeat the above process seven times and obtain\\nseven pseudo labels ˆy based on the extracted seven features.\\nˆy = { ˆy0, ˆy1, ˆy2, ˆy3, ˆy4, ˆy5, ˆy6}\\n(14)\\nUpon the reliable pseudo labels flexibly taking the class\\nimbalance issue into consideration and containing abundant\\nsemantic information, we leverage these target data to re-\\ntrain the network with the SCL module. This joint training\\nprocess not only boosts the discriminability of features but\\nalso enhances the generalization of the model across domains.\\nSpecifically, Equation (4) captures the essence of this training\\nprocess, which can be further refined as shown below,\\nL(F, G) =\\n6\\nX\\ni=0\\n− ηi · E(f s\\ni ,ys\\ni)∼Dsℓ (Gi (f s\\ni ) , ys\\ni )\\n− ηi · E\\x10\\nf t\\ni , ˆ\\nyt\\ni\\n\\x11\\n∼Dtℓ\\n\\x10\\nGi\\nIEEE TRANSACTIONS ON MULTIMEDIA\\n6\\nSpecifically, we calculate a mask matrix m using the following\\nformula,\\nm = s > t =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n1\\n0\\n· · ·\\n1\\n1\\n0\\n· · ·\\n1\\n...\\n...\\n...\\n...\\n0\\n1\\n· · ·\\n0\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n(18)\\nThe matrix m, which is also a 7×7 matrix, consists of\\nelements mij. mij takes a value of 1 if si\\nj is greater than\\nti\\nj, otherwise, it is set to 0. Next, the masked score matrix ˆs\\nis obtained by the dot product of s and m.\\nˆs = s · m =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nˆs0\\n0\\n0\\n· · ·\\nˆs0\\n6\\nˆs1\\n0\\n0\\n· · ·\\nˆs1\\n6\\n...\\n...\\n...\\n...\\n0\\nˆs6\\n1\\n· · ·\\n0\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n(19)\\nThe prediction score for each class is produced by summing\\nup the masked scores from all the classifiers as follows,\\n˜s =\\n( 6\\nX\\ni=0\\nˆsi\\nj\\n)\\n, j = 0...6.\\n(20)\\nFinally, our predicted label p is the class with the highest\\nprediction score in ˜s as\\np = argmax(˜s)\\n(21)\\nIV. EXPERIMENTS\\nA. Experimental Settings\\n1) Dataset: There are six datasets used in the experiments,\\nnamely RAF-DB [8], FER2013 [9], CK+ [10], JAFFE [11],\\nSFEW2.0 [12], and ExpW [13]. RAF-DB includes 29,672\\ndiverse, gender-balanced facial images across various ages\\nand poses. FER2013, compiled via the Google image search\\nengine, contains 35,887 images of different facial expressions.\\nCK+ offers 593 annotated video samples from 123 subjects,\\nused for lab-based facial expression recognition. JAFFE fea-\\ntures 213 images from 10 Japanese women, each of which is\\nlabeled as one of six basic expressions or as neutral. SFEW2.0\\nis an ’in-the-wild’ dataset with unconstrained facial expres-\\nsions, varying in age, pose, and resolution. ExpW has 91,793\\n’in-the-wild’ face images, collected via Google’s search API.\\nThe class quantity distribution of the aforementioned datasets\\nis illustrated in Fig. 2.\\n2) Evaluation Metrics: To conduct fair comparison exper-\\niments, multiple evaluation metrics are used in this paper as\\nfollows:\\nAccuracy represents the percentage of samples that are\\ncorrectly predicted. It is also known as overall accuracy and\\nis the most commonly used metric in the classification task.\\nRecall is the proportion of true positive prediction among\\nall actual positive instances. Although it was originally used\\nfor binary classification task, it can be also applied to multi-\\nclassification task and effectively measure a model’s perfor-\\nmance, even in a class-imbalanced dataset. In our case, we\\nutilize Macro-Average Recall to obtain the average recall of\\nIEEE TRANSACTIONS ON MULTIMEDIA\\n7\\nTABLE I\\nACCURACIES OF OUR PROPOSED AGLRLS AND EXISTING LEADING METHODS ON VARIOUS DATASETS USING DIFFERENT\\nSOURCE DATASETS AND BACKBONE NETWORKS. THE BEST RESULTS ARE IN BOLD.\\nMethod\\ni) Source=RAF-DB, Backbone=ResNet50\\nii) Source=RAF-DB, Backbone=MobileNet-v2\\nCK+\\nJAFFE\\nSFEW2.0\\nFER2013\\nExpW\\nMean\\nCK+\\nJAFFE\\nSFEW2.0\\nFER2013\\nExpW\\nMean\\nDT [25]\\n71.32\\n50.23\\n50.46\\n54.49\\n67.45\\n58.79\\n66.67\\n38.97\\n41.74\\n49.99\\n63.08\\n52.09\\nPLFT [25]\\n77.52\\n53.99\\n48.62\\n56.46\\n69.81\\n61.28\\n72.09\\n38.97\\n41.97\\n51.11\\n64.12\\n53.65\\nICID [34]\\n74.42\\n50.70\\n48.85\\n53.70\\n69.54\\n59.44\\n57.36\\n37.56\\n38.30\\n44.47\\n60.64\\n47.67\\nDFA [47]\\n64.26\\n44.44\\n43.07\\n45.79\\n56.86\\n50.88\\n41.86\\n35.21\\n29.36\\n42.36\\n43.66\\n38.49\\nFTDNN [48]\\n79.07\\n52.11\\n47.48\\n55.98\\n67.72\\n60.47\\n71.32\\n46.01\\n45.41\\n49.96\\n62.87\\n55.11\\nJUMBOT [49]\\n79.46\\n54.13\\n51.97\\n53.56\\n63.69\\n60.56\\n73.64\\n51.35\\n44.41\\n49.05\\n60.84\\n55.86\\nETD [50]\\n75.16\\n51.19\\n52.77\\n50.41\\n67.82\\n59.47\\n69.27\\n48.57\\n41.34\\n49.43\\n57.05\\n53.13\\nCADA [39]\\n72.09\\n52.11\\n53.44\\n57.61\\n63.15\\n59.68\\n62.79\\n53.05\\n43.12\\n49.34\\n59.40\\n53.54\\nSAFN [51]\\n75.97\\n61.03\\n52.98\\n55.64\\n64.91\\n62.11\\n66.67\\n45.07\\n40.14\\n49.90\\n61.40\\n52.64\\nSWD [52]\\n75.19\\n54.93\\n52.06\\n55.84\\n68.35\\n61.27\\n68.22\\n55.40\\n43.58\\n50.30\\n60.04\\n55.51\\nLPL [53]\\n74.42\\n53.05\\n48.85\\n55.89\\n66.90\\n59.82\\n59.69\\n40.38\\n40.14\\n50.13\\n62.26\\n50.52\\nDETN [54]\\n78.22\\n55.89\\n49.40\\n52.29\\n47.58\\n56.68\\n53.49\\n40.38\\n35.09\\n45.88\\n45.26\\n44.02\\nECAN [35]\\n79.77\\n57.28\\n52.29\\n56.46\\n47.37\\n58.63\\n53.49\\n43.08\\n35.09\\n45.77\\n45.09\\n44.50\\nAGRA [25]\\n85.27\\n61.50\\n56.43\\n58.95\\n68.50\\n66.13\\n72.87\\n55.40\\n45.64\\n51.05\\n63.94\\n57.78\\nCGLRL [27]\\n82.95\\n59.62\\n56.88\\n59.30\\n70.02\\n65.75\\n69.77\\n52.58\\n49.77\\n52.46\\n64.87\\n57.89\\nAGLRLS\\n87.60\\n61.97\\n58.26\\n60.68\\n73.00\\n68.30\\n82.95\\n56.81\\n50.23\\n54.51\\n69.10\\n62.72\\nMethod\\niii) Source=FER2013, Backbone=ResNet50\\niv) Source=FER2013, Backbone=MobileNet-v2\\nCK+\\nJAFFE\\nSFEW2.0\\nRAF-DB\\nExpW\\nMean\\nCK+\\nJAFFE\\nSFEW2.0\\nRAF-DB\\nExpW\\nMean\\nDT [25]\\n68.99\\n44.13\\n42.43\\n63.84\\n54.17\\n54.71\\n62.02\\n39.44\\n30.96\\n40.95\\n47.05\\n44.08\\nPLFT [25]\\n79.07\\n46.48\\n42.20\\n67.92\\n54.96\\n58.13\\n61.24\\n44.60\\n28.67\\n40.30\\n53.63\\n45.69\\nICID [34]\\n63.57\\n44.60\\n43.58\\n62.08\\n54.01\\n53.57\\n55.81\\n39.44\\n31.42\\n41.21\\n41.50\\n41.88\\nDFA [47]\\n55.81\\n42.25\\n34.86\\n48.84\\n44.55\\n45.26\\n55.81\\n36.15\\n27.78\\n34.18\\n43.83\\n39.55\\nFTDNN [48]\\n72.09\\n53.99\\n45.64\\n64.40\\n54.67\\n58.16\\n59.69\\n45.54\\n39.68\\n52.43\\n49.87\\n49.44\\nJUMBOT [49]\\n75.76\\n49.69\\n44.33\\n63.08\\n52.37\\n57.05\\n51.16\\n41.54\\n36.06\\n44.93\\n49.30\\n44.60\\nETD [50]\\n77.42\\n44.17\\n39.58\\n61.18\\n49.97\\n54.46\\n54.55\\n40.32\\n30.77\\n50.54\\n45.91\\n44.42\\nCADA [39]\\n81.40\\n45.07\\n46.33\\n65.96\\n54.84\\n58.72\\n66.67\\n50.23\\n41.28\\n53.15\\n51.84\\n52.63\\nSAFN [51]\\n68.99\\n45.07\\n38.07\\n62.80\\n53.91\\n53.77\\n66.67\\n37.56\\n35.78\\n38.73\\n45.56\\n44.86\\nSWD [52]\\n65.89\\n49.30\\n45.64\\n65.28\\n56.05\\n56.43\\n53.49\\n48.36\\n35.78\\n47.44\\n50.02\\n47.02\\nLPL [53]\\n68.22\\n42.72\\n44.27\\n64.23\\n52.45\\n54.38\\n60.47\\n37.56\\n31.88\\n43.92\\n49.83\\n44.73\\nDETN [54]\\n65.89\\n37.89\\n37.39\\n50.51\\n52.15\\n48.77\\n48.09\\n42.31\\n27.54\\n40.53\\n39.14\\n39.52\\nECAN [35]\\n60.47\\n41.76\\n46.01\\n53.41\\n48.88\\n50.11\\n55.65\\n44.12\\n28.46\\n42.31\\n41.53\\n42.41\\nAGRA [25]\\n85.69\\n52.74\\n49.31\\n67.62\\n60.23\\n63.12\\n67.44\\n47.89\\n41.74\\n52.27\\n59.41\\n53.75\\nCGLRL [27]\\n79.84\\n53.52\\n52.29\\n71.84\\n61.94\\n63.87\\n59.69\\n50.23\\n44.72\\n61.95\\n55.33\\n54.38\\nAGLRLS\\n89.92\\n54.93\\n52.52\\n72.02\\n62.63\\n66.40\\n69.77\\n50.70\\n47.02\\n64.85\\n57.14\\n57.90\\nTo combine the global and local features, we concatenate\\nthe global feature vector with the five local feature vectors,\\nresulting in a feature vector of 384 dimensions. The set of\\nfeature vectors, denoted as f, consists of seven elements.\\nCorresponding to the seven features in f, seven classifiers G\\nand domain discriminators D are constructed, the former of\\nwhich uses a series of fully connected layers and the latter\\nuses the approach in [39].\\nb) Training Details: In the proposed AGLRLS frame-\\nwork, the feature extractors F, classifiers G and domain\\ndiscriminators D are trained and optimized in Equations (1)\\nand (2). We initialize the backbone network using models\\npretrained on the MS-Celeb-1M dataset [58]. The parameters\\nof the newly added layers are initialized using the Xavier\\nalgorithm [59]. Stochastic Gradient Descent (SGD) is used\\nas the optimizer in the experiment. Inspired by the previous\\nworks [24], [60], we adopt a two-stage training process.\\nIn the first stage, we solely utilize labeled source domain\\nsamples to train the feature extractor and classifier using cross-\\nentropy loss, as described in the first part of Equation (15).\\nThe learning rate, weight decay, and momentum are set to\\n0.0001, 0.0005, and 0.9, respectively. We train the network\\nfor approximately 15 epochs. In the second stage, the domain\\ndiscriminator is trained utilizing the objective loss specified in\\nEquation (6), while the feature extractor and classifier undergo\\nfine-tuning using the objective loss described in Equation\\n(15). The momentum and weight decay parameters remain\\nunchanged from the first stage. Specifically, the initial learning\\nrate for the feature extractor and the source classifier is set to\\n0.00001 and subsequently divided by 10 after 20 epochs. As\\nfor the domain discriminator, it is trained from scratch with\\nan initial learning rate of 0.0001, and this learning rate is\\nreduced by a factor of 10 once the error reaches a saturation\\npoint. The balance coefficient set β and η are both set to\\n{7, 1, 1, 1, 1, 1, 7}.\\nIn addition, in the pseudo label generation module, three\\npoints should be noted. First, although the pseudo label gen-\\neration and cross-entropy calculation share the same target do-\\nmain images, data augmentation is performed differently as the\\nformer uses weak augmentation and the latter applies stronger\\nenhancement such as diffraction. Second, after using the\\nclassifier to predict the initial label scores, we apply a softmax\\nprocess to compare with the threshold. Thirdly, pseudo label\\ngeneration is seamlessly integrated into the model’s forward\\npropagation, constituting a minor portion of the total runtime.\\nIn our experiments, the average forward propagation time for\\na single batch is 0.78 seconds, while the generation of seven\\nsets of pseudo labels and the associated calculations require\\nIEEE TRANSACTIONS ON MULTIMEDIA\\n8\\nonly 0.05 seconds, accounting for approximately 6.41%.\\nB. Comparisons With The Existing Methods\\nIn the context of cross-domain facial expression recognition,\\ncomparing the performance of different methods can be chal-\\nlenging due to variations in the source domain and backbones.\\nTo ensure a fair comparison, we refer to the benchmark pro-\\nposed by Chen et al. [25] and compare our proposed AGLRLS\\nmethod with a number of leading methods. The comparative\\nresults are presented in Table I. It is evident from the table\\nthat our method consistently outperforms the other algorithms\\nacross various configurations and datasets. For more insights,\\nwe will delve into a comprehensive analysis of our proposed\\nmethod’s performance under different conditions.\\n1) Using Different Backbones: The utilization of different\\nbackbones results in variations in the capabilities of our\\nfeature extractor. To assess the robustness of our method with\\ndifferent backbones, we employed two widely used backbones,\\nnamely ResNet50 and MobileNet-v2. The comparative results\\npresented in Table I demonstrate that when we keep the source\\ndomain constant and replace ResNet50 with MobileNet-v2,\\nthe performances of almost all the approaches show a vary-\\ning degree of degradation across all datasets. For a more\\ndetailed analysis, we focus on subtables i and ii in Table\\nI, where RAF-DB is the source domain and MobileNet-v2\\nreplaces ResNet50. The AGRA algorithm, previously regarded\\nas the best, experiences a reduction of 12.4%, 6.1%, 10.79%,\\n7.9%, and 4.56% in accuracy on the CK+, JAFFE, SFEW2.0,\\nFER2013, and ExpW datasets, respectively. Similarly, our\\npreliminary work, CGLRL, also shows decreases of 13.18%,\\n7.04%, 7.11%, 6.84%, and 5.15% on these five datasets,\\nrespectively. This reduction in performance can be attributed\\nto the fact that MobileNet-v2 utilizes depthwise separable\\nconvolution in its architecture, resulting in relatively weaker\\nfeature extraction capability compared to ResNet50. While the\\nperformance of our proposed AGLRLS method also declines,\\nthe reduction is comparatively smaller than that of AGRA and\\nCGLRL. Specifically, we observe decreases of 4.65%, 5.16%,\\n8.03%, 6.17%, and 3.90% on the aforementioned datasets,\\nrespectively. Notably, even when using MobileNet-v2 with\\nrelatively inferior feature extraction capability, our method still\\noutperforms the best algorithm on the five datasets by 9.31%,\\n1.41%, 0.46%, 2.05%, and 4.23%, respectively. To facilitate a\\nfair comparison, we calculate the average accuracy across each\\ntarget domain, referred to as the Mean. This demonstrates the\\nconsistent robustness of AGLRLS across the target datasets.\\n2) Using Different Source Domain: Another important fac-\\ntor that affects the accuracy of the target domain is the\\nsimilarity between the source and target domain. As shown\\nin Table I, when we maintain the same backbone network\\nand switch the source domain from RAF-DB to FER2013,\\nalmost all of the algorithms experience varying degrees of\\nperformance degradation across all datasets. For example, in\\nsubtables i) and iii) of Table I, we use the same backbone,\\nResNet50, and change the source domain from RAF-DB to\\nFER2013. The accuracy of AGRA increases by 0.42% on\\nCK+ dataset but decreases by 8.76%, 7.12%, and 8.27% on\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\nPLFT\\nJUMBOT\\nETD\\nSAFN\\nSWD\\nLPL\\nDETN\\nECAN\\nAGRA\\nAGLRLS\\nα = 0.05\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\nPLFT\\nJUMBOT\\nETD\\nSAFN\\nSWD\\nLPL\\nDETN\\nECAN\\nAGRA\\nAGLRLS\\nα = 0.10\\nFig. 3. The Friedman Test Chart. The left half is the case\\nof α=0.05, and the right half is the case of α=0.10. The\\nhorizontal coordinate corresponding to the intermediate point\\nof each algorithm is the average order value. The lower the\\nvalue, the better the performance. The range of horizontal lines\\non both sides of each intermediate point represents the CD\\nvalue. If the horizontal lines between the two algorithms do not\\noverlap, it means that the performance of these two methods\\nis significantly different.\\nJAFFE, SFEW2.0, and ExpW datasets, respectively. Similarly,\\nCGLRL experiences reductions of 3.11%, 6.1%, 4.59%, and\\n8.08%. The reasons can be summarized as follows:\\nFirstly, although RAF-DB and FER2013 have similar data\\nsizes and are both collected from the Web, FER2013 data\\nare in grayscale while RAF-DB data are in color. The tar-\\nget datasets, SFEW2.0 and ExpW, consist of color images.\\nFrom this perspective, the similarity between FER2013 and\\nSFEW2.0/ExpW is smaller compared to the similarity be-\\ntween RAF-DB and SFEW2.0/ExpW. Similarly, CK+ is also\\na grayscale image dataset, and its similarity with FER2013\\nis higher than the similarity between CK+ and RAF-DB.\\nConsequently, some methods may show a slight improvement\\nin performance on FER2013. The second difference between\\nRAF-DB and FER2013 is the different coverage of ethnic\\ngroups. The RAF-DB dataset includes a considerable number\\nof samples of Asian people, while FER2013 does not. As a\\nresult, the similarity between FER2013 and the JAFFE dataset,\\nwhich consists of Asian women, is much lower compared to\\nthe similarity between RAF-DB and JAFFE. This explains the\\nsignificant performance drop of most methods on the JAFFE\\ndataset.\\nDespite experiencing some degree of degradation, our pro-\\nposed model still outperforms other state-of-the-art approaches\\non CK+, JAFFE, SFEW2.0, and ExpW by 4.23%, 1.41%,\\n0.23%, and 0.69%, respectively. To ensure a fair comparison,\\nwe utilize the Mean metric for evaluation. As shown in Table\\nI (iii), our AGLRLS method consistently achieves the best\\nperformance compared to the other approaches.\\n3) Using Statistical Test: Statistical tests offer a more\\nrigorous and scientific approach for comparing algorithm per-\\nIEEE TRANSACTIONS ON MULTIMEDIA\\n9\\nIEEE TRANSACTIONS ON MULTIMEDIA\\n10\\nIEEE TRANSACTIONS ON MULTIMEDIA\\n11\\nIEEE TRANSACTIONS ON MULTIMEDIA\\n12\\n[10] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews,\\n“The extended cohn-kanade dataset (ck+): A complete dataset for action\\nunit and emotion-specified expression,” in IEEE Computer Society\\nConference on Computer Vision and Pattern Recogniton-Workshops,\\n2010, pp. 94–101.\\n[11] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, “Coding facial\\nexpressions with gabor wavelets,” in IEEE International Conference on\\nAutomatic Face and Gesture Recognition, 1998, pp. 200–205.\\n[12] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon, “Static facial expression\\nanalysis in tough conditions: Data, evaluation protocol and benchmark,”\\nin IEEE International Conference on Computer Vision Workshops (ICCV\\nWorkshops), 2011, pp. 2106–2112.\\n[13] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “From facial expression\\nrecognition to interpersonal relation prediction,” International Journal\\nof Computer Vision, vol. 126, pp. 550–569, 2018.\\n[14] H. Yan, “Transfer subspace learning for cross-dataset facial expression\\nrecognition,” Neurocomputing, vol. 208, pp. 165–173, 2016.\\n[15] Y.-Q. Miao, R. Araujo, and M. S. Kamel, “Cross-domain facial ex-\\npression recognition using supervised kernel mean matching,” in 11th\\nInternational Conference on Machine Learning and Applications, 2012,\\npp. 326–332.\\n[16] Z. Sun, R. Chiong, Z.-p. Hu, and S. Dhakal, “A dynamic constraint\\nrepresentation approach based on cross-domain dictionary learning for\\nexpression recognition,” Journal of Visual Communication and Image\\nRepresentation, vol. 85, p. 103458, 2022.\\n[17] T. Ni, C. Zhang, and X. Gu, “Transfer model collaborating metric\\nlearning and dictionary learning for cross-domain facial expression\\nrecognition,” IEEE Transactions on Computational Social Systems,\\nvol. 8, no. 5, pp. 1213–1222, 2021.\\n[18] C. Wang, J. Ding, H. Yan, and S. Shen, “A prototype-oriented contrastive\\nadaption network for cross-domain facial expression recognition,” in\\nAsian Conference on Computer Vision (ACCV), 2022, pp. 324–340.\\n[19] X. Wang, X. Wang, and Y. Ni, “Unsupervised domain adaptation for\\nfacial expression recognition using generative adversarial networks,”\\nComputational Intelligence and Neuroscience, p. 7208794, 2018.\\n[20] Y. Zong, W. Zheng, X. Huang, J. Shi, Z. Cui, and G. Zhao, “Domain\\nregeneration for cross-database micro-expression recognition,” IEEE\\nTransactions on Image Processing, vol. 27, no. 5, pp. 2484–2498, 2018.\\n[21] B. Bozorgtabar, D. Mahapatra, and J.-P. Thiran, “Exprada: Adversarial\\ndomain adaptation for facial expression analysis,” Pattern Recognition,\\nvol. 100, p. 107111, 2020.\\n[22] G. Liang, S. Wang, and C. Wang, “Pose-aware adversarial domain\\nadaptation for personalized facial expression recognition,” arXiv preprint\\narXiv:2007.05932, 2020.\\n[23] H. Yang, Z. Zhang, and L. Yin, “Identity-adaptive facial expression\\nrecognition through expression regeneration using conditional genera-\\ntive adversarial networks,” in 13th IEEE International Conference on\\nAutomatic Face & Gesture Recognition, 2018, pp. 294–301.\\n[24] Y. Xie, T. Chen, T. Pu, H. Wu, and L. Lin, “Adversarial graph represen-\\ntation adaptation for cross-domain facial expression recognition,” in 28th\\nACM International Conference on Multimedia, 2020, pp. 1255–1264.\\n[25] T. Chen, T. Pu, H. Wu, Y. Xie, L. Liu, and L. Lin, “Cross-domain facial\\nexpression recognition: A unified evaluation benchmark and adversarial\\ngraph learning,” IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, vol. 44, no. 12, pp. 9887–9903, 2022.\\n[26] Y. Li, Y. Gao, B. Chen, Z. Zhang, L. Zhu, and G. Lu, “Jdman:\\nJoint discriminative and mutual adaptation networks for cross-domain\\nfacial expression recognition,” in 29th ACM International Conference\\non Multimedia, 2021, pp. 3312–3320.\\n[27] Y. Xie, Y. Gao, J. Lin, and T. Chen, “Learning consistent global-\\nlocal representation for cross-domain facial expression recognition,” in\\n26th International Conference on Pattern Recognition (ICPR), 2022, pp.\\n2489–2495.\\n[28] X. Zou, Y. Yan, J. Xue, S. Chen, and H. Wang, “Learn-to-decompose:\\nCascaded decomposition network for cross-domain few-shot facial ex-\\npression recognition,” in European Conference on Computer Vision,\\n2022, pp. 683–700.\\n[29] K. Yan, W. Zheng, Z. Cui, and Y. Zong, “Cross-database facial expres-\\nsion recognition via unsupervised domain adaptive dictionary learning,”\\nin International Conference on Neural Information Processing, 2016,\\npp. 427–434.\\n[30] M. Meng, M. Lan, J. Yu, J. Wu, and L. Liu, “Dual-level adaptive and\\ndiscriminative knowledge transfer for cross-domain recognition,” IEEE\\nTransactions on Multimedia, vol. 25, pp. 2266–2279, 2023.\\n[31] X. Zou, Y. Yan, J.-H. Xue, S. Chen, and H. Wang, “When facial\\nexpression recognition meets few-shot learning: a joint and alternate\\nlearning framework,” in the AAAI Conference on Artificial Intelligence,\\n2022, pp. 5367–5375.\\n[32] W. Zheng, Y. Zong, X. Zhou, and M. Xin, “Cross-domain color facial\\nexpression recognition using transductive transfer subspace learning,”\\nIEEE Transactions on Affective Computing, vol. 9, no. 1, pp. 21–37,\\n2018.\\n[33] Y. Li, Z. Zhang, B. Chen, G. Lu, and D. Zhang, “Deep margin-sensitive\\nrepresentation learning for cross-domain facial expression recognition,”\\nIEEE Transactions on Multimedia, vol. 25, pp. 1359–1373, 2023.\\n[34] Y. Ji, Y. Hu, Y. Yang, F. Shen, and H. T. Shen, “Cross-domain facial\\nexpression recognition via an intra-category common feature and inter-\\ncategory distinction feature fusion network,” Neurocomputing, vol. 333,\\npp. 231–239, 2019.\\n[35] S. Li and W. Deng, “A deeper look at facial expression dataset bias,”\\nIEEE Transactions on Affective Computing, vol. 13, no. 2, pp. 881–893,\\n2022.\\n[36] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discrimi-\\nnative domain adaptation,” in IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), 2017, pp. 2962–2971.\\n[37] A. Conti, P. Rota, Y. Wang, and E. Ricci, “Cluster-level pseudo-labelling\\nfor source-free cross-domain facial expression recognition,” in the 3rd\\nBritish Machine Vision Conference (BMVC), 2022, pp. 1–13.\\n[38] M. de Carvalho, M. Pratama, J. Zhang, and E. Y. K. Yee, “Acdc: Online\\nunsupervised cross-domain adaptation,” Knowledge-Based Systems, vol.\\n253, p. 109486, 2022.\\n[39] M. Long, Z. Cao, J. Wang, and M. I. Jordan, “Conditional adversarial\\ndomain adaptation,” in Advances in Neural Information Processing\\nSystems, 2018, pp. 1647–1657.\\n[40] Y. Ji, Y. Hu, Y. Yang, and H. T. Shen, “Region attention enhanced unsu-\\npervised cross-domain facial emotion recognition,” IEEE Transactions\\non Knowledge and Data Engineering, vol. 35, no. 4, pp. 4190–4201,\\n2023.\\n[41] J. Wang and J. Jiang, “Learning across tasks for zero-shot domain\\nadaptation from a single source domain,” IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, vol. 44, no. 10, pp. 6264–6279, 2022.\\n[42] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, “Gated graph\\nsequence neural networks,” arXiv:1511.05493, 2015.\\n[43] T. Chen, L. Lin, R. Chen, X. Hui, and H. Wu, “Knowledge-guided\\nmulti-label few-shot learning for general image recognition,” IEEE\\nTransactions on Pattern Analysis and Machine Intelligence, vol. 44,\\nno. 3, pp. 1371–1384, 2020.\\n[44] Z. Wang, J. Zhang, T. Chen, W. Wang, and P. Luo, “Restoreformer++:\\nTowards real-world blind face restoration from undegraded key-value\\npairs,” IEEE Transactions on Pattern Analysis and Machine Intelligence,\\nvol. 45, no. 12, pp. 15 462–15 476, 2023.\\n[45] T. Pu, T. Chen, H. Wu, Y. Lu, and L. Lin, “Spatial-temporal knowledge-\\nembedded transformer for video scene graph generation,” IEEE Trans-\\nactions on Image Processing, vol. 33, pp. 556–568, 2023.\\n[46] B. Zhang, Y. Wang, W. Hou, H. Wu, J. Wang, M. Okumura, and\\nT. Shinozaki, “Flexmatch: Boosting semi-supervised learning with cur-\\nriculum pseudo labeling,” in Advances in Neural Information Processing\\nSystems, 2021, pp. 18 408–18 419.\\n[47] R. Zhu, G. Sang, and Q. Zhao, “Discriminative feature adaptation for\\ncross-domain facial expression recognition,” in International Conference\\non Biometrics (ICB), 2016, pp. 1–7.\\n[48] M. V. Zavarez, R. F. Berriel, and T. Oliveira-Santos, “Cross-database\\nfacial expression recognition based on fine-tuned deep convolutional\\nnetwork,” in 30th Conference on Graphics, Patterns and Images (SIB-\\nGRAPI), 2017, pp. 405–412.\\n[49] K. Fatras, T. Sejourne, R. Flamary, and N. Courty, “Unbalanced mini-\\nbatch optimal transport; applications to domain adaptation,” in 38th\\nInternational Conference on Machine Learning, 2021, pp. 3186–3197.\\n[50] M. Li, Y.-M. Zhai, Y.-W. Luo, P. Ge, and C.-X. Ren, “Enhanced transport\\ndistance for unsupervised domain adaptation,” in IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR), 2020, pp. 13 933–\\n13 941.\\n[51] R. Xu, G. Li, J. Yang, and L. Lin, “Larger norm more transferable: An\\nadaptive feature norm approach for unsupervised domain adaptation,” in\\nIEEE/CVF International Conference on Computer Vision (ICCV), 2019,\\npp. 1426–1435.\\n[52] C.-Y. Lee, T. Batra, M. H. Baig, and D. Ulbricht, “Sliced wasserstein\\ndiscrepancy for unsupervised domain adaptation,” in IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR), 2019, pp.\\n10 277–10 287.\\n[53] S. Li, W. Deng, and J. Du, “Reliable crowdsourcing and deep locality-\\npreserving learning for expression recognition in the wild,” in IEEE/CVF\\nIEEE TRANSACTIONS ON MULTIMEDIA\\n13\\nConference on Computer Vision and Pattern Recognition (CVPR), 2017,\\npp. 2584–2593.\\n[54] S. Li and W. Deng, “Deep emotion transfer network for cross-database\\nfacial expression recognition,” in 24th International Conference on\\nPattern Recognition (ICPR), 2018, pp. 3092–3099.\\n[55] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2016, pp. 770–778.\\n[56] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\\n“Mobilenetv2: Inverted residuals and linear bottlenecks,” in IEEE/CVF\\nConference on Computer Vision and Pattern Recognition (CVPR), 2018,\\npp. 4510–4520.\\n[57] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection and\\nalignment using multitask cascaded convolutional networks,” IEEE\\nSignal Processing Letters, vol. 23, no. 10, pp. 1499–1503, 2016.\\n[58] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao, “Ms-celeb-1m: A\\ndataset and benchmark for large-scale face recognition,” in European\\nConference on Computer Vision (ECCV), 2016, pp. 87–102.\\n[59] X. Glorot and Y. Bengio, “Understanding the difficulty of training\\ndeep feedforward neural networks,” in The Thirteenth International\\nConference on Artificial Intelligence and Statistics, 2010, pp. 249–256.\\n[60] J. Wen, R. Liu, N. Zheng, Q. Zheng, Z. Gong, and J. Yuan, “Exploiting\\nlocal feature patterns for unsupervised domain adaptation,” in the AAAI\\nConference on Artificial Intelligence, 2019, pp. 5401–5408.\\n[61] M. Friedman, “The use of ranks to avoid the assumption of normality\\nimplicit in the analysis of variance,” Journal of The American Statistical\\nAssociation, vol. 32, no. 200, pp. 675–701, 1937.\\n[62] R. TAMURA, “Some distribution-free multiple comparison procedures,”\\nMemoirs of the Faculty of Literature and Science, Shimane University,\\nNatural Sciences, vol. 1, pp. 1–7, 1968.\\n[63] J. Demˇsar, “Statistical comparisons of classifiers over multiple data sets,”\\nThe Journal of Machine learning research, vol. 7, pp. 1–30, 2006.\\nYuefang Gao received the Ph.D. degree in computer\\nscience from South China University of Technology\\nin 2009. She is an associate professor in College\\nof Mathematics and Informatics, South China Agri-\\ncultural University, China. Her research interests\\ninclude computer vision, machine learning and data\\nmining. She has authored and co-authored more\\nthan 20 papers in top-tier academic journals and\\nconferences, including T-CSVT, PR, TKDE, ACM\\nMM, etc. She has served as a reviewer for some\\nacademic journals and conferences.\\nYuhao Xie received his bachelor’s degree from\\nSouth China Agricultural University, China in 2023.\\nHe is currently pursuing a master’s degree at the\\nGuangzhou Institute of Technology, Xidian Univer-\\nsity. His research interests include computer vision\\nand deep learning.\\nZeke Zexi Hu is currently a Ph.D. candidate at\\nthe School of Computer Science, University of Syd-\\nney, Australia. He received his bachelor’s degree\\nfrom South China Agricultural University, China in\\n2014 and his M.Phil. degree from the University of\\nSydney in 2020. His research focuses on computer\\nvision and deep learning.\\nTianshui Chen received a Ph.D. degree in computer\\nscience at the School of Data and Computer Science\\nSun Yat-sen University, Guangzhou, China, in 2018.\\nPrior to earning his Ph.D., he received a B.E. degree\\nfrom the School of Information and Science Tech-\\nnology in 2013. He is currently an associated profes-\\nsor in the Guangdong University of Technology. His\\ncurrent research interests include computer vision\\nand machine learning. He has authored and coau-\\nthored approximately 40 papers published in top-\\ntier academic journals and conferences, including T-\\nPAMI, T-NNLS, T-IP, T-MM, CVPR, ICCV, AAAI, IJCAI, ACM MM, etc.\\nHe has served as a reviewer for numerous academic journals and conferences.\\nHe was the recipient of the Best Paper Diamond Award at IEEE ICME 2017.\\nLiang Lin (Fellow, IEEE) is a full professor at\\nSun Yat-sen University. From 2008 to 2010, he\\nwas a postdoctoral fellow at the University of Cal-\\nifornia, Los Angeles. From 2016–2018, he led the\\nSenseTime R&D teams to develop cutting-edge and\\ndeliverable solutions for computer vision, data anal-\\nysis and mining, and intelligent robotic systems. He\\nhas authored and coauthored more than 100 papers\\nin top-tier academic journals and conferences (e.g.,\\n15 papers in TPAMI and IJCV and 60+ papers in\\nCVPR, ICCV, NIPS, and IJCAI). He has served as\\nan associate editor of IEEE Trans. Human-Machine Systems, The Visual\\nComputer, and Neurocomputing and as an area/session chair for numerous\\nconferences, such as CVPR, ICME, ACCV, and ICMR. He was the recipient\\nof the Annual Best Paper Award by Pattern Recognition (Elsevier) in 2018, the\\nBest Paper Diamond Award at IEEE ICME 2017, the Best Paper Runner-Up\\nAward at ACM NPAR 2010, Google Faculty Award in 2012, the Best Student\\nPaper Award at IEEE ICME 2014, and the Hong Kong Scholars Award in\\n2014. He is a Fellow of IEEE, IAPR, and IET.\\n'},\n",
       " {'abstract': 'In this paper, we investigate transmission control for unmanned aerial vehicles (UAVs) operating in unlicensed spectrum bands. A rigorous interference-aware queuing analysis framework is developed to optimize expected throughput by balancing packet drops from queues and transmission errors caused by low SINR. Two packet loss probabilities are explored, buffer overflow model and time threshold model. Two algorithms, Interference-Aware Transmission Control (IA-TC) and Interference-Aware Distributed Transmission Control (IA-DTC), adjust transmission policies to optimize throughput. Numerical results demonstrate that both algorithms find the optimal transmission policy. Focus is on LoS and NLoS interference links between UAVs and ground nodes.',\n",
       "  'introduction': 'UAV networks have great potential, but reliable and robust communication in unlicensed spectrum bands is challenging. This paper explores distributed transmission policies that consider queue and channel impairments. Previous works have not investigated this issue in-depth.',\n",
       "  'literature_review': 'Existing literature on UAV network transmission policies does not comprehensively consider the effects of queue and in-band interference on packet losses. Some studies assume LoS or NLoS links without interference models or only model system performance in terms of UAV trajectory and not channel parameters.',\n",
       "  'methodology': 'A comprehensive analytical framework is proposed to characterize two factors that impact packet losses: queue-related analysis with a time threshold model and buffer overflow model, and channel-related analysis that focuses on outage probability and the impact of interference. Two transmission policy algorithms, IA-TC and IA-DTC, are introduced to find the optimal channel fading threshold in the presence of queue and channel impairments.',\n",
       "  'results': 'The performance of the proposed algorithms is evaluated under various scenarios. Results show that IA-TC constantly tries to increase the channel fading threshold for interferer nodes and decrease it for the main source node. IA-DTC balances the trade-offs between optimizing the throughput of the main link and minimizing interference to other nodes. Both algorithms achieve higher throughput compared to baseline policies.',\n",
       "  'conclusion': 'An in-depth investigation of distributed transmission policy that considers both packet queues and interference levels is presented. Two transmission algorithms, IA-TC and IA-DTC, are proposed and their effectiveness is demonstrated. The optimal transmission policy under various scenarios is found. Future work will extend analysis to model-free analysis and optimization solutions.',\n",
       "  'title': 'Interference-Aware Queuing Analysis for Distributed Transmission Control in UAV Networks',\n",
       "  'author': 'Masoud Ghazikor, Keenan Roach, Kenny Cheung, Morteza Hashemi',\n",
       "  'textdata': 'Interference-Aware Queuing Analysis for\\nDistributed Transmission Control in UAV Networks\\nMasoud Ghazikor∗, Keenan Roach†, Kenny Cheung†, Morteza Hashemi∗\\n∗Department of Electrical Engineering and Computer Science, University of Kansas\\n†Universities Space Research Association (USRA)\\nAbstract—In this paper, we investigate the problem of dis-\\ntributed transmission control for unmanned aerial vehicles\\n(UAVs) operating in unlicensed spectrum bands. We develop\\na rigorous interference-aware queuing analysis framework that\\njointly considers two inter-dependent factors: (i) limited-size\\nqueues with delay-constrained packet arrival, and (ii) in-band\\ninterference introduced by other ground/aerial users. We aim to\\noptimize the expected throughput by jointly analyzing these fac-\\ntors. In the queuing analysis, we explore two packet loss probabil-\\nities including, buffer overflow model and time threshold model.\\nFor interference analysis, we investigate the outage probability\\nand packet losses due to low signal-to-interference-plus-noise\\nratio (SINR). We introduce two algorithms namely, Interference-\\nAware Transmission Control (IA-TC), and Interference-Aware\\nDistributed Transmission Control (IA-DTC). These algorithms\\nmaximize the expected throughput by adjusting transmission\\npolicies to balance the trade-offs between packet drop from\\nqueues vs. transmission errors due to low SINRs. We implement\\nthe proposed algorithms and demonstrate that the optimal\\ntransmission policy under various scenarios is found.\\nIndex Terms—Unmanned aerial vehicles, distributed transmis-\\nsion policy, channel fading threshold, expected throughput\\nI. INTRODUCTION\\nUnmanned Aerial Vehicles (UAVs) have emerged as a\\ntransformative technology for a wide range of applications\\nsuch as environmental conservation, emergency services, de-\\nlivery, and more. These instances are crucial to determining\\nhow to support the expected increase in UAV usage [1]. For\\nUAV communications, both licensed and unlicensed spectrum\\ncan be used. Licensed spectrum grants exclusive access to\\nthe channel and includes regulatory requirements to fulfill.\\nIn contrast, unlicensed spectrum is shared among different\\ncommunication nodes, and thus it includes light regulations\\nthat make nodes more prone to interference from other users.\\nAs a result, reliable and robust UAV communication in unli-\\ncensed spectrum bands is challenging. To address this issue,\\ndeveloping a distributed transmission policy is essential to\\nensure a high quality of service, particularly for UAV networks\\nthat require delay-sensitive command-and-control (C2) data.\\nOver the past few years, there has been extensive amounts\\nof research on different aspects of UAV networking (see, for\\nexample, [2]–[4]). However, there are still research gaps in\\ndeveloping distributed transmission policies that jointly take\\ninto account (i) the level of interference in the unlicensed\\nspectrum bands, and (ii) the transmission queue state in terms\\nof buffer size and queuing delay. For instance, in [5], Line-of-\\nSight (LoS) and Non-Line-of-Sight (NLoS) wireless links are\\nmodeled for UAVs and a transmission policy is defined without\\nconsidering interference. In [6], distributed transmission policy\\nis developed for ground level terrestrial networks, and not\\nNLoS Interference Link\\nLoS Interference Link\\nG2A\\nG2A\\nA2A\\nMain Link\\nβn\\nλn\\nEnqueuing Packets \\nβm1\\nλm1\\nβm2\\nλm2\\nβm3\\nλm3\\nFig. 1: System model that consists of ground and aerial nodes.\\nspecifically for UAV networks. A new LoS channel model\\nis proposed in [7] for UAVs, but the interference model is not\\nincluded in the outage probability calculation. In [8], ground-\\nto-air (G2A) and air-to-ground (A2G) channels are modeled,\\nand system performance is characterized in terms of UAV\\ntrajectory, and not the channel parameters.\\nAlthough there have been research on UAV network trans-\\nmission policies [5], [7], [8], an in-depth investigation of\\ndistributed transmission policy that takes into account both\\npacket queues and interference levels in unlicensed frequency\\nbands is still lacking. This paper aims to address this gap\\nby exploring how UAVs and ground-level nodes achieve an\\noptimal policy by adjusting the channel fading threshold in a\\ndistributed manner. To this end, we propose a comprehensive\\nanalytical framework to characterize two factors that impact\\npacket losses: (i) queue-related analysis wherein we consider\\na time threshold model to capture the delay sensitivity of data\\npackets, and buffer overflow model to capture the limited size\\nof data queues, and (ii) channel-related analysis by which we\\nfocus on the outage probability and analyze the impact of in-\\nband interference on packet transmission errors.\\nGiven these factors, we introduce two transmission algo-\\nrithms, namely, Interference-Aware Transmission Control (IA-\\nTC), and Interference-Aware Distributed Transmission Control\\n(IA-DTC). In the IA-TC algorithm, we assume there is a\\nsingle source node and devise a solution to find its optimal\\nchannel fading threshold βn and the best expected throughput\\n(Rbest\\nn\\n), while adjusting the βm of other aerial or ground\\ninterferer nodes. On the other hand, in the IA-DTC algorithm,\\nwe find the optimal channel fading threshold for all nodes, i.e.,\\nβ ≜ {βm, βn}, by assuming that each node can be considered\\nas a source node. In this algorithm, each node discovers its\\noptimal β through a consensus-based optimization approach.\\nThrough numerical results, we compare the performance of our\\nproposed algorithms with several baselines to demonstrate the\\narXiv:2401.11084v1  [cs.IT]  20 Jan 2024\\nefficacy of the our method. In summary, the main contributions\\nof this paper are as follows:\\n• We propose a comprehensive analytical framework for\\ndeveloping throughput-optimal transmission policies for\\nUAV networks. Our system takes into account the effects\\nof queue and in-band interference on packet losses.\\n• We introduce two transmission policy algorithms for find-\\ning the optimal channel fading threshold in the presence\\nof queue and channel impairments. We implement coor-\\ndinate descent optimization to find the optimal channel\\nfading threshold for the source node and consensus-based\\ndistributed optimization to determine the optimal channel\\nfading threshold for each node.\\n• We implement the proposed algorithms and assess their\\neffectiveness under various scenarios in comparison to\\nseveral alternative baseline policies.\\nThe rest of this paper is organized as follows. In Section II, the\\nsystem model is introduced. Section III offers an analysis of\\nthe time threshold and buffer overflow models, as well as the\\noutage probability by considering the impact of interference.\\nIn Section IV, the IA-TC and IA-DTC transmission policy\\nalgorithms are introduced. Section V provides the numerical\\nresults, followed by the conclusion in Section VI.\\nII. SYSTEM MODEL\\nWe consider a network that consists of ground and aerial\\nnodes, all operating in unlicensed spectrum bands for G2A,\\nA2G, G2G, and A2A links. The spectrum band is divided\\ninto a set of F frequency channels. We further assume that\\nthe source node communicates with the main UAV (right UAV\\nin Fig. 1), while several interferer ground nodes communicate\\nwith the interferer UAV (left UAV in Fig. 1). In general, N\\ndenotes the set of communication sessions that share the same\\nspectrum band, and n ∈ N represents the individual session\\nbetween the source node and the UAV.\\nFurthermore, we assume that each node has a limited-size\\nqueue, where the packet arrival process (λi) follows a Poisson\\ndistribution. Each node either transmits the packet to its\\ndestination or keeps the packet in its queue. This transmission\\ndecision is determined based on channel conditions and queue\\nstates. For instance, if two or more nodes choose the same\\nchannel to transmit packets simultaneously, there would be\\nin-band interference and degraded SINR values.\\nChannel Modeling. Given the described system model, first\\nwe calculate the LoS probability (PLoS(di)) that captures\\ndifferent types of channels [9]:\\nPLoS(di) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\n\\x10\\n1 − e(−\\nz2\\ni\\n2ζ2 )\\x11di√vµ\\nzi = zu\\n\\x10\\n1 −\\n√\\n2πζ\\ndV\\ni\\n\\x0c\\x0c\\x0cQ( zi\\nζ ) − Q( zu\\nζ )\\n\\x0c\\x0c\\x0c\\n\\x11dH\\ni\\n√vµ\\nzi ̸= zu,\\nwhere ζ, v, and µ are environmental parameters and Q(x) is\\nthe Q-function. Also, dH\\ni\\n=\\np\\n(xi − xu)2 + (yi − yu)2 and\\ndV\\ni\\n=\\np\\n(zi − zu)2 are, respectively, horizontal and vertical\\ndistances between the transmitter (i) and receiver (u). Thus,\\nthe total distance between node i and a specific receiver node\\nis obtained as:\\ndi =\\nq\\ndH\\ni\\n2 + dV\\ni\\n2\\n∀i = {n, m} ,\\n(1)\\nwhere the indices n and m denote the source node and the\\nset of interferer nodes in a given area, respectively.\\nGiven the transmit power Pt, we have Pr = Pt|hf\\nn|2 in\\nwhich Pr is the received power and hf\\nn is the channel gain of\\nchannel f ∈ F. Furthermore, hf\\nn can be expressed as hf\\nn =\\n˜hf\\nnˆhf\\nn, in which ˜hf\\nn denotes the channel fading coefficient, and\\nˆhf\\nn is the square root of the path loss. By using a single-slope\\npath loss model [10], we have:\\nˆhf\\nn =\\nr\\nc(d0\\ndi\\n)α(di)\\nif di ≥ d0,\\n(2)\\nwhere c =\\nλ2\\n16π2d2\\n0 is a constant factor. Furthermore, d0 and di\\nare the reference distance and the distance between the nodes\\nand their intended receiver, respectively. Also, the path loss\\nexponent α(di) is defined as [11], [12]:\\nα(di) = αLoSPLoS(di) + αNLoS(1 − PLoS(di)),\\n(3)\\nin which αLoS and αNLoS are the path loss exponents for LoS\\nand NLoS links, respectively.\\nIn the framework of a block fading channel model, the\\nvariable ˜hf\\nn follows either the Rician (Rice) or Rayleigh\\n(Ray) distributions depending on whether it corresponds to\\nLoS or NLoS channels, respectively. Let us initially focus on\\nthe Rician channel, in which the probability density function\\n(PDF) is given by:\\nPb(˜hf\\nn = x) = xe− x2+b2\\n2\\nI0(xb),\\n(4)\\nwhere b =\\np\\n2K(di) is defined according to the Rician factor\\nK(di) = KNLoSeln(\\nKLoS\\nKNLoS )PLoS(di)2\\nin which KLoS and\\nKNLoS are determined when PLoS(di) is equal to one and\\nzero, respectively [9]. Also, I0 represents the modified Bessel\\nfunction of the first kind with order zero.\\nPrinciple of the Transmission Policy.\\nThe source node\\ntransmits its packet to the UAV over the best frequency channel\\nf ∗ = arg maxf∈F\\n˜hf\\nnˆhf\\nn if the channel fading coefficient is\\nlarger than a channel fading threshold βn, i.e., ˜hf ∗\\nn\\n≥ βn;\\notherwise, the source node would enqueue the packet [6].\\nLet βn > 0 be the channel fading threshold. Based on\\nβn, the cumulative distribution function (CDF) of the Rician\\ndistribution is given by:\\nPb(˜hf\\nn < βRice\\nn\\n) =\\nZ βRice\\nn\\n0\\nxe− x2+b2\\n2\\nI0(xb) dx\\n= 1 − Q1(b, βRice\\nn\\n),\\n(5)\\nwhere Q1 represents the first-order Marcum Q-function.\\nTherefore, assuming that |F| represents the cardinality of the\\nset F, the transmission probability of the source node during\\na time slot over the Rician channel can be expressed as:\\nµn(βRice\\nn\\n) = 1 − Pb(˜hf ∗\\nn < βRice\\nn\\n)\\n= 1 − (1 − Q1(b, βRice\\nn\\n))|F |.\\n(6)\\nThe same approach can be applied to the Rayleigh channel,\\nwhere the PDF of the Rayleigh distribution is given by:\\nPb(˜hf\\nn = x) = 2x\\nΩ e− x2\\nΩ ,\\n(7)\\nwhere Ω is the Rayleigh fading factor. Then, the CDF of the\\nRayleigh distribution is formulated as follows:\\nPb(˜hf\\nn < βRay\\nn\\n) =\\nZ βRay\\nn\\n0\\n2x\\nΩ e− x2\\nΩ dx = 1 − e− (βRay\\nn\\n)2\\nΩ\\n.\\nFinally, the transmission probability of a packet from source\\nnode in a time slot over the Rayleigh channel is defined as:\\nµn(βRay\\nn\\n) = 1 − (1 − e− (βRay\\nn\\n)2\\nΩ\\n)|F |.\\n(8)\\nGiven the presented system model, our goal is to find the\\noptimal values for the channel fading threshold βn such that\\nthe network throughput is maximized.\\nIII. PROBLEM FORMULATION\\nIn this section, we characterize throughput performance in\\nterms of its constituent queuing and interference components.\\nA. Queuing Characterization\\nWe focus on two queue management mechanisms designed\\nto regulate the number of packets in the queue.\\nTime Threshold Model. UAVs may communicate delay-\\nsensitive data such as command-and-control messages. In this\\ncase, it is critical to ensure that data packets are delivered to\\ntheir intended destination in a specified timeout value. Assume\\nthat Tn denotes the waiting time in the queue for node n.\\nWhen the source node is unable to transmit packets due to poor\\nchannel conditions (e.g., low SINR), any packet with a waiting\\ntime Tn greater than the time threshold T th\\nn\\nis discarded.\\nBy using µn(βn) derived from Eq. (6) and Eq. (8) for\\nRician and Rayleigh channels in a M/M/1 queue scenario, the\\nprobability of packet loss due to exceeding the time threshold\\ncan be expressed as [6]:\\nP dly\\nn (βn) ≜ Pb(Tn > T th\\nn ) = e−( µn(βn)\\nTslt\\n−λn)T th\\nn ,\\n(9)\\nwhere Tslt and λn denote the time slot duration and average\\nincoming packet rate, respectively. In order to determine the\\nupper bound for βn, an important parameter in our IA-TC\\nand IA-DTC algorithms, it is known that P dly\\nn (βn) ≤ 1.\\nConsequently, the upper bounds for βn in the case of Rician\\nand Rayleigh channels can be obtained as:\\n(\\nQ1(b, βRice\\nn\\n) ≥ 1 − (1 − λnTslt)\\n1\\n|F | ,\\nRician;\\nβRay\\nn\\n≤\\nq\\n−Ωln[1 − (1 − Tsltλn)\\n1\\n|F | ]\\nRayleigh.\\n(10)\\nThe authors in [6] have provided the derivations under\\nRayleigh channel conditions.\\nBuffer Overflow Model. In addition to time-threshold model\\nthat captures time-sensitivity of data traffics, assume that\\nqueues have limited buffer sizes as well. Therefore, there\\nare chances that new packet arrivals are inadmissible due to\\nbuffer overflow, and thus they are dropped. By applying the\\nprinciples of queuing theory, the probability of exceeding the\\nbuffer capacity in a certain state i can be defined [13], [14]:\\nPi,i+1 = P[X1 + ... + Xi+1 > Bn|X1 + ... + Xi ≤ Bn]\\n=\\nR Bn\\n0\\nP[Xi+1 > Bn − x]fX1+...+Xi(x)dx\\nP[X1 + ... + Xi ≤ Bn]\\n,\\nin which Bn and X denote the buffer capacity and the packet\\nlength, respectively. For the sake of analysis, we assume that\\nthe packet length follows an exponential random variable with\\na parameter ηn. Also, fX1+...+Xi(x) represents the PDF of an\\ni-Erlang distribution, respectively. Hence, the complement of\\nPi,i+1 without occurring buffer overflow is defined as:\\nPi,i+1 = 1 − Pi,i+1 =\\n1 − Pi\\nj=0\\n(Bnηn)j\\nj!\\ne−Bnηn\\n1 − Pi−1\\nj=0\\n(Bnηn)j\\nj!\\ne−Bnηn .\\n(11)\\nAccording to the Markov chain, the local balance equation\\nis πi+1 = ρn(βn)Pi,i+1πi, where ρn(βn) =\\nλnTslt\\nµn(βn) is the\\noffered load. Then, πi can be derived as:\\nπi = ρi\\nn(βn)\\n\\x12 i−1\\nY\\nj=0\\nPj,j+1\\n\\x13\\nπ0 =\\nρi\\nn(βn)\\n\\x12\\n1 −\\ni−1\\nX\\nj=0\\n(Bnηn)j\\nj!\\ne−Bnηn\\n\\x13\\nπ0.\\n(12)\\nThe probability of buffer overflow can be approximated by:\\nP ov\\nn (βn) ≈\\n∞\\nX\\ni=0\\nPi,i+1πi = (1 − ρn(βn))e−Bnηn(1−ρn(βn))\\n1 − ρn(βn)e−Bnηn(1−ρn(βn)) .\\n(13)\\nNext, we characterize the impacts of interference as a function\\nof the transmission policy parameter βn.\\nB. Interference Characterization\\nIn this part, we investigate the impact of interference on the\\nUAV using the signal-to-interference-plus-noise ratio (SINR).\\nIf the SINR falls below the SINR threshold γth, a transmission\\nerror occurs. Let If\\nn(β−n) be the impact of the interferer nodes\\non the UAV [6]:\\nIf\\nn(β−n) =\\nX\\nm∈N\\\\n\\nPm(ˆhf\\nmn˜hf\\nmn)2αf\\nm(βm),\\n(14)\\nwhere the channel fading threshold of the interferer nodes\\nis defined as β−n ≜ (βm)m∈N\\\\n. Also, αf\\nm(βm) equals\\none if interferer node m transmits using channel f, and zero\\notherwise. Thus, the outage probability is defined as:\\nP out\\nn\\n(β) ≜ Pb(γn < γth) = Pb\\n \\nPn(ˆhf\\nn)2(˜hf\\nn)2\\nσ2 + If\\nn(β−n)\\n< γth\\n!\\n.\\nHere, Pn is the transmission power, σ2 = kTW denotes the\\nthermal noise power where k, T, and W are the Boltzmann’s\\nconstant, temperature, and bandwidth, respectively.\\nBy adopting a classical stochastic geometry approach to\\nmodel If\\nn(β−n) using Gamma distribution, the final expres-\\nsion for the outage probability is given by [6]:\\nP out\\nn\\n(β) =\\nZ ∞\\nβn\\nPb(˜hf\\nn = x)vn(Pn(ˆhf\\nn)2\\nγth\\nx2 − σ2, β−n)dx,\\n(15)\\nwhere Pb(˜hf\\nn = x) is determined according to the channel’s\\ntype (Rician or Rayleigh) and vn(x, β−n) is the complemen-\\ntary cumulative distribution function (CCDF) of If\\nn(β−n).\\nFrom [6], we have:\\nvn(x, β−n) = Pb(If\\nn(β−n) > x) = 1 −\\nφ(kn(β−n),\\nx\\nθn(β−n))\\nΓ(kn(β−n))\\n,\\nwhere φ(kn(β−n),\\nx\\nθn(β−n)) =\\nR\\nx\\nθn(β−n)\\n0\\nskn(β−n)−1e−sds is\\nthe lower incomplete gamma function and Γ(kn(β−n)) =\\nR ∞\\n0\\nxkn(β−n)−1e−xdx is the Gamma function.\\nC. Throughput Characterization\\nGiven the presented queuing and interference analysis, now\\nwe consider all three packet loss probabilities, namely: (i)\\npacket loss due to time threshold P dly\\nn (βn), (ii) packet loss\\ndue to buffer overflow P ov\\nn (βn), and (iii) packet loss due to\\noutage and low SINR P out\\nn\\n(β). Thus, the probability of overall\\nloss P loss\\nn\\n(β) is determined as:\\nP loss\\nn\\n(β) = P ov\\nn (βn) + [1 − P ov\\nn (βn)]P dly\\nn (βn)+\\n[1 − P ov\\nn (βn)][1 − P dly\\nn (βn)]P out\\nn\\n(β).\\n(16)\\nSince the products of P dly\\nn (βn), P ov\\nn (βn), and P out\\nn\\n(β) are\\nnegligible, we consider only the “first order” terms, and thus\\nthe expected throughput can be approximated as [6]:\\nRn(β) = λn[1 − P loss\\nn\\n(β)] ≈\\nλn[1 − P dly\\nn (βn) − P ov\\nn (βn) − P out\\nn\\n(β)].\\n(17)\\nNext, we present two transmission policies to maximize the\\nexpected throughput performance by finding the optimal chan-\\nnel fading threshold β.\\nIV. PROPOSED TRANSMISSION POLICY ALGORITHMS\\nInterference-Aware Transmission Control. As mentioned,\\nour goal is to develop a transmission policy that achieves the\\nmaximum expected throughput for the source node. To this\\nend, we consider aim to maxβ Rn(β), subject to the upper\\nbound constraints on βRice\\nm\\nand βRay\\nm\\n, which should be smaller\\nthan βRice\\nmax and βRay\\nmax, respectively. To solve this problem, we\\nuse a coordinate descent algorithm to maximize a function\\n(Rn(β)) by adjusting different variables (i.e., components of\\nthe vector β) across each coordinate separately.\\nAt each step, the algorithm focuses on one variable and\\nupdates it, while keeping the other variables constant. In par-\\nticular, Algorithm 1 optimizes the expected throughput (Rn)\\nby adjusting the channel fading thresholds β ≜ {βm, βn} for\\nthe interferer (βm ≜\\n\\x08\\nβRice\\nm\\n, βRay\\nm\\n\\t\\n) and source (βn) nodes.\\nConsider the same βRice\\nm\\n, βRay\\nm\\nfor all interferers and βn as\\nthree coordinate axes. Initially (lines 5-7), we set βn and βRay\\nm\\nas fixed parameters and change βRice\\nm\\nby the step of stpm to\\ndetermine the best expected throughput (Rbest\\nn\\n) in the specified\\ncoordinate. Then (lines 8-11), we repeat the same procedure\\nwhile interchanging βRice\\nm\\n, βRay\\nm\\n, and βn.\\nTo control βm, we set its maximum value as the smallest\\nupper bound for two types of channels among all interferer\\nnodes, denoted as βRice\\nmax and βRay\\nmax. However, there is no need\\nto set a maximum value for βn as it dynamically adjusts itself\\nthrough iteration. Also, stopping criteria are based on the max-\\nimum number of iterations (maxiter) and the condition that\\nthe difference between Rbest\\nn\\nand the previous one (Rprev\\nn\\n) is\\nAlgorithm 1 Interference-Aware Transmission Control (IA-TC)\\n1: function IA-TC(βini, βRice\\nmax, βRay\\nmax, stpi, maxiter)\\n2:\\nβ ← βini, Rbest\\nn\\n← Rn(β)\\n3:\\nfor iter in range maxiter do\\n4:\\nRprev\\nn\\n← Rbest\\nn\\n5:\\nif ∃ Rice ∈ m and βRice\\nm\\n+ stpm < βRice\\nmax then\\n6:\\nβ, Rbest\\nn\\n= CS(βm, stpm, Rbest\\nn\\n, βn)\\n7:\\nend if\\n8:\\nif ∃ Ray ∈ m and βRay\\nm\\n+ stpm < βRay\\nmax then\\n9:\\nβ, Rbest\\nn\\n= CS(βm, stpm, Rbest\\nn\\n, βn)\\n10:\\nend if\\n11:\\nβ, Rbest\\nn\\n= CS(βn, stpn, Rbest\\nn\\n, βm)\\n12:\\nif |Rprev\\nn\\n− Rbest\\nn\\n| < ϵ then\\n13:\\nbreak\\n14:\\nend if\\n15:\\nend for\\n16:\\nreturn β, Rbest\\nn\\n17: end function\\nlower than ϵ. In the Coordinate Search (CS) function (lines 6,\\n9, and 11), we explore the coordinate by stpi ≜ {stpm, stpn}\\nto determine Rbest\\nn\\nand β in each coordinate axis.\\nRemarks. IA-TC algorithm constantly tries to increase βRice\\nm\\nand βRay\\nm\\nuntil it reaches the maximum value since the inter-\\nferer nodes send fewer packets and the level of interference\\non the main link decreases. Furthermore, as βn increases,\\nthe source node enqueues more packets. Thus, while packet\\nloss in the queue rises, packet loss due to transmission error\\ndecreases. Therefore, as the number of iterations of the IA-TC\\nalgorithm increases, the values of βRice\\nm\\nand βRay\\nm\\nincreases,\\nand the value of βn decreases. Therefore, the main source\\nnode would have more transmission opportunities, while the\\ntransmission attempts by the interferer nodes decreases. Our\\nnumerical results in Section V confirm this result.\\nInterference-Aware Distributed Transmission Control.\\nHere, our goal is to develop a distributed transmission policy\\nthat achieves the maximum expected throughput across all\\nlinks, while assuming that each link can be a main link. When\\ncompared to IA-TC, increasing the channel fading threshold\\nfor interferer nodes is no longer optimal because they could\\nalso serve as the main link. In this case, distributed nodes\\nshould coordinate with each other to converge to the optimal\\ntransmission policy that is desirable for all nodes, rather than\\njust one, as in IA-TC.\\nWe use consensus-based distributed optimization to solve\\nthis problem in which multiple nodes collaborate to reach a\\nconsensus on β. Each node has its own local information and\\nobjective function (Rn(β)), and it communicates iteratively\\nwith its neighbors to find optimal β and maximize Rn(β) [15].\\nIn Algorithm 2, our goal is to determine the optimal set of\\nchannel fading thresholds (β⋆) for nodes. Initially, we start\\nby setting the channel fading threshold of interferer nodes to\\nthe maximum value, allowing each node to selfishly identify\\nits best channel fading threshold (βbest\\nn\\n) based on the results\\nobtained from the IA-TC algorithm. During each iteration, if\\nthe difference between the updated channel fading threshold\\nAlgorithm 2 Interference-Aware Distributed Transmission Control\\n(IA-DTC)\\n1: function IA-DTC(βmax, ˆm, stp, maxiter)\\n2:\\nmprev ← ˆm, β ← βmax\\n3:\\nfor n in range ˆm do\\n4:\\nβn ← β[n], m ← ˆm − {n}\\n5:\\nβbest\\nn\\n= LCS(maxiter, stp, βn, βm)\\n6:\\nβcan[n] ← βbest\\nn\\n, ˆm ← mprev\\n7:\\nend for\\n8:\\nfor iter in range maxiter do\\n9:\\nβ ← βcan\\n10:\\nfor n in range ˆm do\\n11:\\nβn ← β[n], m ← ˆm − {n}\\n12:\\nR[iter][n] ← Rn(βn, βm)\\n13:\\nβbest\\nn\\n= LCS(maxiter, stp, βn, βm)\\n14:\\nβcan[n] ← βbest\\nn\\n, ˆm ← mprev\\n15:\\nend for\\n16:\\nif |β − βcan| < ϵ then\\n17:\\nβ⋆ ← βcan\\n18:\\nbreak\\n19:\\nend if\\n20:\\nend for\\n21:\\nreturn β⋆, R\\n22: end function\\nset and the previous one is greater than ϵ, nodes exchange\\ninformation regarding their channel fading thresholds with\\neach other to determine the optimal channel fading threshold.\\nIn this algorithm, the set\\nˆm includes all nodes (source\\nor interferer), also, n and set m specifies the source node\\nand interferer nodes, respectively. We introduce the βcan list\\nto collect the updated channel fading thresholds as different\\nnodes are selected as the source node. In lines 2-7, the primary\\nβ ≜ (βm, βn)m,n∈N is set to the maximum value of the\\nchannel fading threshold (βmax) according to the upper bound\\nof it. Then, selfish values of the channel fading threshold are\\nstored in βcan. In lines 8-20, nodes find their best channel\\nfading thresholds by β and then update βcan. In each iteration,\\nRn(β) for all nodes is stored in R. The Local Coordinate\\nSearch (LCS) function (lines 5 and 13) determines βbest\\nn\\nfor\\neach node while having access to βm. This function explores\\na coordinate by stp until it finds βbest\\nn\\nassociated with Rbest\\nn\\n.\\nV. NUMERICAL RESULTS\\nIn our setup, 10 nodes (1 main UAV, 1 interferer UAV, 8\\nground nodes) are placed according to the Poisson distribution\\nin the area, and the main link is established between the source\\nnode (node 8) and the main UAV at 40 m height (node 10),\\nwhile other ground nodes (nodes 1 to 7) serve as interferer\\nnodes communicating with the interferer UAV (node 9). In\\nour simulations, we assume that the ground nodes experience\\nthe same type of channel conditions (Rician or Rayleigh) to\\nthe main and interferer UAVs. Key simulation parameters are\\nsummarized in Table I.\\nIA-TC Algorithm Performance. The efficacy of the IA-\\nTC algorithm is shown in Figures 2 and 3 for three different\\ninterferer UAV heights. From Fig. 2, we observe how the\\nTABLE I: Key Simulation Parameters\\nParameter\\nValue\\nCommunication Area\\n100 × 100 m2\\nPLoS Model\\nζ = 20, v = 3 × 10−4, µ = 0.5\\nPath Loss Model\\nαLoS = 2, αNLoS = 3.5, d0 = 10 m\\nChannel Model\\n|F| = 14, Ω = 2\\nRician Factor Model\\nKLoS = 15, KNLoS = 1\\nQueuing Model (1)\\nT th\\nn = 80 ms, Tslt = 5 ms\\nQueuing Model (2)\\nλn = 80, Bnηn = 100\\nInterference Model\\nPi = 0.5 W, γth = 10\\nNoise Model\\nf = 2.4 GHz, T = 290 K\\nIA-TC Algorithm (1)\\nβini = [3, 2, 4], stpi = [0.05, 0.02]\\nIA-TC Algorithm (2)\\nβRice\\nmax = 4.08, βRay\\nmax = 2.57\\nIA-DTC Algorithm\\nstp = 0.05, maxiter = 100\\nsource node finds the optimal βn as the IA-TC algorithm\\niterates. At the higher altitudes of the interferer UAV (60 m),\\nthe interference impact of the A2A link is larger compared\\nwith lower altitudes. Thus, as the altitude of the inteferer UAV\\nincreases, the source node chooses larger βn values, thereby\\nenqueuing more packets instead of transmitting to the main\\nUAV. In Fig. 2, initially the source node increases βn since\\nβm are small, but, as the IA-TC algorithm increases βm, the\\nsource node gradually decreases βn, which means that the\\nsource node would try transmitting more often due to lower\\ninterference. In Fig. 3, we report the throughput performance\\nof the source node as the IA-TC algorithm iterates, which\\nresults in a higher Rn. Furthermore, Rn increases as the\\naltitude of the interferer UAV decreases, as expected. In the\\nlower altitudes of the interferer UAV (10 m), we achieve the\\nlargest Rn, since the interferer UAV is located near ground\\nlevel, its impact is not as significant as the higher altitudes.\\nIA-DTC Algorithm Performance. Fig. 4 shows that how\\nthe optimal channel fading threshold of the source node (β⋆\\nn)\\nchanges as a function of the number of nodes and for different\\nγth values. From the results, we observe that as the number\\nof nodes increases, the source node increases the level of β⋆\\nn\\ndue to the stronger impact of the interferer nodes. As a result,\\nby increasing the number of nodes and γth, the source node\\nacts more conservatively (i.e., fewer transmission attempts)\\nand increases its β⋆\\nn.\\nIn Fig. 5, we present a comparative analysis of the IA-DTC\\nalgorithm output, denoted as β⋆, compared to four baseline\\ntransmission policies: (i) Random policy: Different nodes\\nselect their transmission threshold randomly between zero and\\nthe upper bound. (ii) Aggressive policy: Different nodes select\\ntheir transmission threshold such that the probability of packet\\ndrop from the queue (due to overflow or time-threshold) is\\nminimized. In this case, nodes attempt to transmit despite\\npoor channel conditions. (iii) Selfish policy: Different nodes\\ndo not cooperate with each other, and treat other nodes as\\ninterference. (iv) Conservative policy: Different nodes select\\ntheir transmission threshold to be close to the upper bound\\nvalues, and thus they aim to minimize outage probability, while\\nincreasing the likelihood of packet loss from queues. From\\nthe results in Fig. 6, we observe that the IA-DTC algorithm\\n0\\n3\\n6\\n9\\n12\\n15\\n18\\n21\\nNumber of Iterations\\n3.85\\n3.90\\n3.95\\n4.00\\n4.05\\n4.10\\nChannel Fading Threshold (\\nn)\\nInterferer UAV Height = 60 m\\nInterferer UAV Height = 30 m\\nInterferer UAV Height = 10 m\\nFig. 2: βn vs. # of Iterations by IA-TC\\n0\\n3\\n6\\n9\\n12\\n15\\n18\\n21\\nNumber of Iterations\\n73\\n74\\n75\\n76\\n77\\n78\\n79\\n80\\nExpected Throughput (Pkts/Sec)\\nInterferer UAV Height = 60 m\\nInterferer UAV Height = 30 m\\nInterferer UAV Height = 10 m\\nFig. 3: Rn vs. # of Iterations by IA-TC\\n5\\n8\\n10\\n12\\n15\\nSINR Threshold (\\nth)\\n4.4\\n4.5\\n4.6\\n4.7\\n4.8\\n4.9\\n5.0\\nOptimal Channel Fading Threshold (\\nn)\\nNumber of Nodes = 5\\nNumber of Nodes = 7\\nNumber of Nodes = 10\\nFig. 4: β⋆\\nn vs. γth by IA-DTC\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nNode ID\\n0.5\\n1.5\\n2.5\\n3.5\\n4.5\\n5.5\\n6.5\\nChannel Fading Threshold (\\ni)\\nRandom Policy\\nAggressive Policy\\nSelfish Policy\\nIA-DTC Algorithm\\nConservative Policy\\nFig. 5: βi vs. Node ID by IA-DTC\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nNode ID\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\nExpected Throughput (Pkts/Sec)\\nRandom Policy\\nAggressive Policy\\nSelfish Policy\\nIA-DTC Algorithm\\nConservative Policy\\nFig. 6: Rn vs. Node ID by IA-DTC\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nNode ID\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n5.5\\n6.0\\n6.5\\nOptimal Channel Fading Threshold (\\ni )\\nSINR Threshold (\\nth) = 5\\nSINR Threshold (\\nth) = 8\\nSINR Threshold (\\nth) = 10\\nSINR Threshold (\\nth) = 12\\nSINR Threshold (\\nth) = 15\\nFig. 7: β⋆\\ni vs. Node ID by IA-DTC\\nachieves the highest Rn compared with all other baselines.\\nFurthermore, Fig. 7 demonstrates the behavior of the IA-DTC\\nalgorithm as γth changes. Clearly, as γth increases, nodes\\nincrease their optimal channel fading thresholds (β⋆\\ni ) since\\nthey require a better channel condition to send their packets,\\nwhich means that they prefer to enqueue their packets rather\\nthan sending them to the UAVs.\\nVI. CONCLUSION\\nIn this paper, we investigated the problem of distributed\\ntransmission control for UAVs operating in unlicensed spec-\\ntrum bands. We developed an analytical interference-aware\\nqueuing analysis framework that jointly considers three types\\nof packet losses including packet drop due to exceeding the\\ntime threshold (P dly\\nn (βn)), buffer overflow (P ov\\nn (βn)), and\\nlow SINR (P out\\nn\\n(β)). Using this analysis, we were able to\\ncalculate the throughput performance Rn according to the\\nprobability of overall loss. In the transmission policy section,\\nwe proposed two algorithms IA-TC and IA-DTC to control β\\nfor each node to improve Rn. We numerically investigated the\\nperformance of our algorithms, and confirmed that it achieves\\nthe optimal solution. As a future direction, we aim to extend\\nour analysis to model-free analysis and optimization solutions.\\nACKNOWLEDGMENT\\nThe material is based upon work supported by NASA under\\naward No(s) 80NSSC20M0261, and NSF grants 1948511,\\n1955561, and 2212565. Any opinions, findings, and conclu-\\nsions or recommendations expressed in this material are those\\nof the author(s) and do not necessarily reflect the views of\\nNASA and NSF.\\nREFERENCES\\n[1] B. Badnava, T. Kim, K. Cheung, Z. Ali, and M. Hashemi, “Spectrum-\\naware mobile edge computing for UAVs using reinforcement learning,”\\n2021 IEEE/ACM Symposium on Edge Computing (SEC), 2021.\\n[2] K. Meng, Q. Wu, S. Ma, W. Chen, and T. Q. S. Quek, “UAV trajectory\\nand beamforming optimization for integrated periodic sensing and\\ncommunication,” IEEE Wireless Communications Letters, 2022.\\n[3] B. Badnava, K. Roach, K. Cheung, M. Hashemi, and N. B. Shroff,\\n“Energy-efficient deadline-aware edge computing: Bandit learning with\\npartial observations in multi-channel systems,” arXiv:2308.06647, 2023.\\n[4] S. Reddy Chintareddy, K. Roach, K. Cheung, and M. Hashemi, “Collab-\\norative wideband spectrum sensing and scheduling for networked UAVs\\nin UTM systems,” arXiv:2308.05036, 2023.\\n[5] P. S. Bithas and A. L. Moustakas, “Generalized UAV selection with\\ndistributed transmission policies,” IEEE Transactions on Comm., 2023.\\n[6] Z. Guan, T. Melodia, and G. Scutari, “To transmit or not to transmit?\\ndistributed queueing games in infrastructureless wireless networks,”\\nIEEE/ACM Transactions on Networking, 2016.\\n[7] P. S. Bithas, V. Nikolaidis, A. G. Kanatas, and G. K. Karagiannidis,\\n“UAV-to-ground communication: Channel modeling and UAV selec-\\ntion,” IEEE Transactions on Communications, 2020.\\n[8] J. Cui, Z. Ding, Y. Deng, and A. Nallanathan, “Model-free based\\nautomated trajectory optimization for UAVs toward data transmission,”\\n2019 IEEE Global Communications Conference (GLOBECOM), 2019.\\n[9] M. Kim and J. Lee, “Impact of an interfering node on unmanned aerial\\nvehicle communications,” IEEE Trans. on Vehicular Technology, 2019.\\n[10] Z. Ren, G. Wang, Q. Chen, and H. Li, “Modelling and simulation of\\nrayleigh fading, path loss, and shadowing fading for wireless mobile\\nnetworks,” Simulation Modelling Practice and Theory, 2011.\\n[11] M. M. Azari, F. Rosas, K.-C. Chen, and S. Pollin, “Ultra reliable UAV\\ncomm. using altitude and coop. diversity,” IEEE Trans. on Comm., 2018.\\n[12] M. Kim and J. Lee, “Outage probability of UAV communications in the\\npresence of interference,” 2018 IEEE Global Comm. Conf., 2018.\\n[13] D. Gross, J. F. Shortle, J. M. Thompson, and C. M. Harris, Fundamentals\\nof Queueing Theory.\\nWiley-Interscience, 2008.\\n[14] M. Ghazikor, K. Roach, K. Cheung, and M. Hashemi, “Exploring the\\ninterplay of interference and queues in unlicensed spectrum bands for\\nUAV networks,” arXiv:2308.05187, 2023.\\n[15] A. S. Berahas, R. Bollapragada, N. S. Keskar, and E. Wei, “Balancing\\ncomm. and comp. in dist. opt.” IEEE Trans. on Auto. Control, 2019.\\n'},\n",
       " {'abstract': 'Homomorphic encryption (HE) enables privacy-preserving cloud computing by allowing direct computations on ciphertexts. The computations involve modular reduction, and the overall complexity of ciphertext multiplication can be reduced by utilizing the quotient. Previous work considers the cases that the dividend is an integer multiple of the modulus and the modulus is in the format of 2w − 2u ± 1, where u < w/2. This paper generalizes the division for larger u and dividend not an integer multiple of the modulus. An algorithm is proposed to compute the quotient, and efficient hardware architecture is developed for implementing the algorithm.',\n",
       "  'introduction': 'Homomorphic encryption (HE) allows computations to be carried out on ciphertexts without decryption. Popular HE schemes involve computations over very long polynomials, whose coefficients are large integers, and coefficient multiplication and addition are followed by modular reduction. To reduce the computation complexity, the large coefficients are represented by residue number system, and moduli with a small number of nonzero bits are chosen. It was found in [5] that the overall complexity of ciphertext multiplication can be reduced by combining and reformulating the coefficients multiplication and relinearization, which is enabled by using the quotient of dividing coefficients product by q.',\n",
       "  'literature review': 'The division can be implemented by using a look-up table [6]. However, the size of the look-up table increases exponentially with the number of bits to divide in each clock cycle. Approximate division by very short integers, such as 8-bit, has been investigated for image processing in [7], [8]. The approximations in these schemes lead to a big difference in the quotient for large q. In [9], the quotient is derived by multiplying an approximation of q⁻1. To improve the precision, 2w bits are used to represent the approximation when q has w bits. The dividend is a product of two w-bit coefficients and also has 2w bits. Hence, a 2w × 2w wide multiplier is needed, and it leads to not only a long data path but also a large area. To address these issues, the quotient is calculated as a × λ + b, and then the least significant bits are deleted in [10]. Here a and b are precomputed constants with at most w bits. Although the width of the multiplicand is reduced, a wide multiplier is still needed for this design.',\n",
       "  'methodology': 'The design in [5] assumes that q = 2w − 2u ± 1. Utilizing the property that q has a small number of nonzero bits, the quotient is calculated by addition and shift operations that have much shorter data path and smaller area requirements compared to those multiplying approximation of q⁻1 as in [9], [10]. However, the design in [5] is limited to the case of u < w/2, and the dividend is an integer multiple of q. Given the product of two coefficients, its remainder of division by q needs to be calculated and subtracted first before the division can be carried out.',\n",
       "  'results': 'For w = 32, there are 31 different possible u. For 50%, 16%, and 9% of these possible values of u, the proposed design achieves 55%, 32%, and 9% shorter latency, respectively, and at least 79% silicon area reduction compared to the divider in [10].',\n",
       "  'conclusion': 'This paper proposed a low-complexity integer divider for calculating the quotient when the divisor has a small number of nonzero bits. It generalized the previous design to handle more possible divisors and the case that the dividend is not an integer multiple of the divisor. In addition, by analyzing the possible values of the intermediate results, simplifications on the hardware implementation architectures are developed. Compared to prior designs that are based on multiplying the inverse of the divisor, the proposed design reduces the area requirement to a fraction and also has much shorter latency.',\n",
       "  'title': 'Low-Complexity Integer Divider Architecture for Homomorphic Encryption',\n",
       "  'author': 'Sajjad Akherati, Jiaxuan Cai, Xinmiao Zhang',\n",
       "  'textdata': 'arXiv:2401.11064v1  [cs.CR]  19 Jan 2024\\nLow-Complexity Integer Divider Architecture for\\nHomomorphic Encryption\\nSajjad Akherati, Jiaxuan Cai, and Xinmiao Zhang\\nThe Ohio State University, OH 43210, U.S.\\nAbstract—Homomorphic encryption (HE) allows computations\\nto be directly carried out on ciphertexts and enables privacy-\\npreserving cloud computing. The computations on the coefﬁcients\\nof the polynomials involved in HE are always followed by\\nmodular reduction, and the overall complexity of ciphertext\\nmultiplication can be reduced by utilizing the quotient. Our\\nprevious design considers the cases that the dividend is an integer\\nmultiple of the modulus and the modulus is in the format of\\n2w − 2u ± 1, where u < w/2. In this paper, the division is\\ngeneralized for larger u and dividend not an integer multiple of\\nthe modulus. An algorithm is proposed to compute the quotient\\nand vigorous mathematical proofs are provided. Moreover, efﬁ-\\ncient hardware architecture is developed for implementing the\\nproposed algorithm. Compared to alternative division approaches\\nthat utilize the inverse of the divisor, for w = 32, the proposed\\ndesign achieves at least 9% shorter latency and 79% area\\nreduction for 75% possible values of u.\\nI. INTRODUCTION\\nHomomorphic encryption (HE) allows computations to be\\ncarried out on ciphertexts without decryption. It is the key to\\npreserving privacy in cloud computing. Popular HE schemes\\n[1]–[3] involve computations over very long polynomials,\\nwhose coefﬁcients are large integers, and coefﬁcients mul-\\ntiplication and addition are followed by modular reduction.\\nTo reduce the computation complexity, the large coefﬁcients\\nare represented by residue number system [4], and moduli\\nwith a small number of nonzero bits, such as in the format\\nof q = 2w − 2u ± 1, are chosen. It was found in [5] that the\\noverall complexity of ciphertext multiplication can be reduced\\nby combining and reformulating the coefﬁcients multiplication\\nand relinearization, which is enabled by using the quotient of\\ndividing coefﬁcients product by q.\\nThe division can be implemented by using a look-up table\\n[6]. However, the size of the look-up table increases expo-\\nnentially with the number of bits to divide in each clock\\ncycle. Approximate division by very short integers, such as\\n8-bit, has been investigated for image processing in [7], [8].\\nThe approximations in these schemes lead to a big difference\\nin the quotient for large q. In [9], the quotient is derived\\nby multiplying an approximation of q−1. To improve the\\nprecision, 2w bits are used to represent the approximation\\nwhen q has w bits. The dividend is a product of two w-bit\\ncoefﬁcients and also has 2w bits. Hence, a 2w × 2w wide\\nmultiplier is needed, and it leads to not only a long data path\\nbut also a large area. To address these issues, the quotient\\nis calculated as a × λ + b, and then the least signiﬁcant bits\\nThis work is supported in part by Cisco Systems.\\nare deleted in [10]. Here a and b are precomputed constants\\nwith at most w bits. Although the width of the multiplicand\\nis reduced, a wide multiplier is still needed for this design.\\nThe design in [5] assumes that q = 2w − 2u ± 1. Utilizing\\nthe property that q has a small number of nonzero bits, the\\nquotient is calculated by addition and shift operations that\\nhave much shorter data path and smaller area requirements\\ncompared to those multiplying approximation of q−1 as in\\n[9], [10]. However, the design in [5] is limited to the case of\\nu < w/2, and the dividend is an integer multiple of q. Given\\nthe product of two coefﬁcients, its remainder of division by q\\nneeds to be calculated and subtracted ﬁrst before the division\\ncan be carried out.\\nThis paper proposes a generalized low-complexity integer\\ndivision algorithm and implementation architecture. An itera-\\ntive process is developed to compute the quotient in the case\\nof u ≥ w/2, where each iteration consists of simple addition\\nand shift operations. The number of iterations needed is a\\nsmall value depending on the ratio of u/w. Mathematical\\nformulas and corresponding proofs are given for the number\\nof iterations. Unlike the algorithm in [5], the proposed design\\ndoes not require the dividend to be an integer multiple of q and\\nhence does not need a separate remainder calculation. Instead,\\nthe quotient computed from the iterative process is adjusted\\nto take into account the remainder. Efﬁcient hardware imple-\\nmentation architectures are also developed for the proposed\\nalgorithm and synthesis has been carried out. For w = 32,\\nthere are 31 different possible u. For 50%, 16%, and 9% of\\nthese possible values of u, the proposed design achieves 55%,\\n32%, and 9% shorter latency, respectively, and at least 79%\\nsilicon area reduction compared to the divider in [10].\\nII. EXISTING INTEGER DIVIDERS\\nConsider λ/q where λ and q have 2w and w bits, respec-\\ntively. In [9], the quotient of λ/q is calculated by multiplying\\nλ with a pre-computed constant J = ⌊23w−1/q⌋ + 1. The\\nhigher w bits of this product is the quotient of λ/q. J has\\n2w bits in order to ensure the correctness of the quotient. The\\ndesign in [10] calculates the quotient as ⌊(aλ+b)/2k⌋, where\\na, b and k are constants pre-computed according to q. a is\\nan (k − w)-bit number, where w < k ≤ 2w. Consequently,\\nthe width of the multiplier needed is reduced. However, the\\nquotient computed from this algorithm can be different from\\nthe actual quotient by ±1.\\nThe design in [5] assumes q = 2w−2u±1, u < w/2, and λ\\nis an integer multiple of q. Let c = ⌊λ/2w⌋. It ﬁrst calculates\\nb∗ = c − ⌊ −c(2u∓1)\\n2w\\n⌋. It was shown that λ/q equals b∗ when\\nb∗ and λ are both even or odd. Otherwise, λ/q = b∗ + 1. As\\na result, the division was implemented by two adders and two\\nshifters.\\nIII. GENERALIZED LOW-COMPLEXITY INTEGER DIVISION\\nThis section ﬁrst generalizes the previous division algorithm\\nin [5] to the case of q = 2w − 2u ± 1 with u ≥ w/2. Then the\\ndesign is further extended to the case that λ is not an integer\\nmultiple of q.\\nA. Extension for u ≥ w/2 with λ as an integer multiple of q\\nDenote the quotient of λ/q by b (b ∈ Z+). Replace q by\\n2w − 2u ± 1 in λ = bq. Then c = ⌊λ/2w⌋ can be rewritten as\\nc = ⌊ bq\\n2w ⌋ = b + ⌊(−1) b(2u∓1)\\n2w\\n⌋. Deﬁne\\nf(x) ≜ x + v(x),\\n(1)\\nwhere\\nv(x) ≜ ⌊(−1)x(2u ∓ 1)\\n2w\\n⌋.\\n(2)\\nb is the solution of f(x) = c. This solution can be found\\niteratively using Algorithm 1. In this algorithm, the least\\nsigniﬁcant bit (LSB) of a number y is denoted by LSB(y).\\nAlgorithm 1 Algorithm for calculating b = λ/q (λ is an\\ninteger multiple of q)\\nInput: λ, q = 2w − 2u ± 1, w, u\\n1: c ← ⌊ λ\\n2w ⌋; i ← 0; b0 ← c;\\n2: while f(bi) ̸= c do\\n3:\\nbi+1 ← bi + (c − f(bi)); i ← i + 1; b∗ ← bi;\\n4: b ← b∗ + (LSB(λ) XOR LSB(b∗));\\n5: return b;\\nIn the following, Theorem 1 proves that the loop in Al-\\ngorithm 1 will terminate with a ﬁnite number of iterations.\\nTheorem 2 connects the b∗ computed from the loop with the\\nactual b value. The number of iterations needed in Algorithm\\n1 is given through Theorem 3 and 4.\\nTheorem 1. Algorithm 1 will terminate with a ﬁnite number\\nof iterations.\\nProof. By induction, it is shown in the following that\\nf(bi−1) < f(bi) ≤ c. Using the properties of the ﬂoor\\nfunction, it can be derived that f(x) + f(y) ≤ f(x + y) ≤\\nf(x) + f(y) + 1. Besides, from (2), it can be easily seen that\\n0 < f(x) < x for x > 0. Hence, for i = 1:\\nf(b1)=f(b0+c−f(b0))≥f(b0)+f(c−f(b0))\\n=f(b0)+f(c−f(c))>f(b0)\\nSimilarly,\\nf(b1)≤f(b0)+f(c−f(b0))+1<f(b0)+(c−f(b0))+1=c+1.\\nSince f(b1) is integer, f(b1) ≤ c. Assume that f(bi−2) <\\nf(bi−1) ≤ c, similar derivations as in the above two formulas\\nshow that f(bi−1) < f(bi) ≤ c. Since f(bi) strictly increases\\nwith iteration i and it is always an integer not exceeding c,\\nf(bi) will equal to c in an iteration and the loop in Lines 2\\nand 3 of Algorithm 1 terminates.\\nTheorem 2. The bi in every iteration of Algorithm 1 is at\\nmost b.\\nProof. This is proved by induction. Since c = b + v(b) and\\nv(b) ≤ 0, it is clear that b0 = c ≤ b. Now suppose that bi ≤ b.\\nFrom Algorithm 1, if bi = b, then the loop terminates and there\\nwill be no other b′\\ni with i′ > i. If bi < b, then v(b) ≤ v(bi)\\nfrom (2). Accordingly,\\nbi+1 = bi + c − f(bi) = bi + c − (bi + v(bi))\\n= c − v(bi) = b + v(b) − v(bi) ≤ b.\\nTheorem 2 shows that the b∗ calculated by the loop in\\nAlgorithm 1 does not exceed b. Next, it will be shown that\\nb∗ equals either b − 1 or b. From (1), f(b) = b + v(b) and\\nf(b−1) = b−1+v(b−1). Since v(b−1) = v(b) or v(b)+1,\\nf(b − 1) = f(b) or f(b) − 1. If f(b − 1) = f(b) = c then\\nthe b∗ from the loop in Algorithm 1 may be either b − 1 or b.\\nSimilarly, it can be shown that f(b−2) = f(b)−1 or f(b)−2.\\nHence, f(b−2) ̸= c and b∗ can not be b−2 or a smaller value.\\nSince q is an odd number, b is even or odd when λ is even or\\nodd, respectively. Hence, whether b equals b∗ or b∗ + 1 can\\nbe decided by using the LSB of λ and b∗ as listed in Line 4\\nof Algorithm 1.\\nThe initial value of b0 should be less than b, which is\\nunknown at the beginning of Algorithm 1. On the other hand,\\ninitializing b0 to the smallest positive integer, 1, leads to more\\niterations in Algorithm 1. Since c ≤ b, c is chosen as the initial\\nvalue in our algorithm. With this initial value, c − f(bi) is the\\nlargest value that can be used to update f(bi) as proposed in\\nAlgorithm 1. Consider the case that b = 2 and u < w − 2.\\nThen c = 1 and f(b0) = f(c) = f(1) = 0. If c − f(b0) + j\\n(j ≥ 1) is used to update bi. Then b1 = b0+(c−f(b0)+j) ≥ 3\\nand f(3) = 2 > c. Hence, Algorithm 1 will not terminate and\\nc − f(bi) + j with j ≥ 1 can not be used to update f(bi).\\nTheorem 3. Suppose that the number of iterations to calculate\\nb = λ\\nq (λ < 22w − 1) and bMAX ≜ ⌊ 22w−1\\nq\\n⌋ from Algorithm 1\\nare N and NMAX, respectively. Then N ≤ NMAX.\\nProof. Let cMAX = ⌊(22w −1)/2w⌋ and bMAXi be the interme-\\ndiate value at iteration i of Algorithm 1 for calculating bMAX.\\nIt is shown below that b − bi ≤ bMAX − bMAXi by induction.\\nIt can be derived that\\nb − b0 = b − ⌊ bq\\n2w ⌋ < b − bq\\n2w + 1 = b(1 − q\\n2w ) + 1\\n≤ bMAX(1 − q\\n2w ) + 1 = bMAX − bMAXq\\n2w\\n+ 1\\n< bMAX − ⌊bMAXq\\n2w\\n⌋ + 1 = bMAX − bMAX0 + 1.\\nb and b0 are integers, and hence b − b0 ≤ bMAX − bMAX0.\\nSuppose that for iteration i < N, b − bi ≤ bMAX − bMAXi.\\nFor i + 1 < N\\nb − bi+1 = b − (bi + c − f(bi)) = b − c + v(bi)\\n< b − c − bi(2u ∓ 1)\\n2w\\n< b − bq\\n2w − bi(2u ∓ 1)\\n2w\\n+ 1\\n= (b − bi)2u ∓ 1\\n2w\\n+ 1 ≤ (bMAX − bMAXi)2u ∓ 1\\n2w\\n+ 1\\n= bMAX − bMAXq\\n2w\\n− bMAXi(2u ∓ 1)\\n2w\\n+ 1\\n= bMAX − ⌊bMAXq\\n2w\\n⌋ + ⌊(−1)bMAXi(2u ∓ 1)\\n2w\\n⌋\\n+ 1 + −[bMAXq]2w + [−bMAXi(2u ∓ 1)]2w\\n2w\\n,\\nwhere [·]2w denotes the remainder of dividing by 2w. It can\\nbe shown that 0 < 1+\\n−[bMAXq]2w +[−bMAXi(2u∓1)]2w\\n2w\\n< 1. Since\\nb − bi+1 is an integer,\\nb − bi+1 ≤ bMAX − cMAX + v(bMAXi) = bMAX − bMAXi+1.\\nAlgorithm 1 terminates at iteration N when λ is the input.\\nFrom previous analysis, bN equals either b or b − 1. From the\\nabove proof, bMAX−bMAXN ≥ b−bN. Hence bMAX−bMAXN ≥\\n0. On the other hand, bMAX − bMAXN−1 ̸= 0. Otherwise, the\\nalgorithm with λ as the input already terminates at iteration\\nN −1 and this contradicts the assumption. Accordingly, when\\nthe input of Algorithm 1 is 22w−1, it may need more than N\\niterations to compute bMAX and hence NMAX ≥ N.\\nTheorem 4. For q = 2w − 2u ± 1, let t be the integer such\\nthat tu > (t − 1)w and (t + 1)u ≤ tw. Then NMAX = t.\\nProof. Clearly cMAX = 2w − 1. From Algorithm 1\\nbMAX1 = bMAX0 + cMAX − f(bMAX0) = cMAX − v(cMAX)\\n= 2w − 1 − ⌊(−1)(2w − 1)(2u − 1)\\n2w\\n⌋ = 2w + 2u − 2.\\nBy substituting the above formula into Line 3 of Algorithm\\n1 for i iterations, it can be derived that bMAXi = 2w + 2u +\\n22u−w + 23u−2w + · · · + 2iu−(i−1)w − {1 or 2}. It can be\\nderived that f(bMAXi) ̸= cMAX for i < t, where t is the integer\\nsuch that tu > (t − 1)w and (t + 1)u ≤ tw. On the other\\nhand, f(bMAXt) = cMAX. This means that Algorithm 1 needs\\nt iterations to terminate when its input is 22w − 1.\\nFrom Theorem 3 and 4, Algorithm 1 needs at most t\\niterations, where t is the integer such that tu > (t − 1)w\\nand (t + 1)u ≤ tw. For u ≤ 3/4w, Algorithm 1 terminates in\\nat most 3 iterations. Algorithm 1 still applies when u < w/2,\\nin which case t = 1 and Algorithm 1 reduces to the same\\nalgorithm as in [5].\\nB. Extension for λ not an integer multiple of q\\nAssume that λ = bq + r, where 0 ≤ r < q. If r = 0, it\\nreduces to the case covered in Subsection A, and the loop in\\nAlgorithm 1 returns a b∗ that equals b or b−1 from the previous\\nanalysis. Consider the case of 0 < r < q, λ < λ′ = (b + 1)q.\\nFeeding λ′ as the input of Algorithm 1, the loop would return\\na b∗ that is either b + 1 or b. Therefore, b may have three\\npossible values: b∗, b∗ + 1 or b∗ − 1.\\nSince λ = bq + r, λ − b∗q should equal r + q, r, and\\nr − q when b∗ is b − 1, b, and b + 1, respectively. After b∗ is\\ncalculated, λ − b∗q can be computed. Since r − q < 0 < r <\\nq < r + q, b is set to b∗ − 1, b∗, and b∗ + 1 when λ − b∗q is\\nless than 0, between 0 and q, and larger than q, respectively.\\nAs a result, the quotient of dividing a λ that is not necessarily\\nan integer multiple of q can be calculated using Algorithm 2.\\nAlgorithm 2 Algorithm for calculating ⌊b = λ/q⌋\\nInput: λ, q = 2w − 2u ± 1, w, u\\n1: c ← ⌊ λ\\n2w ⌋; i ← 0; b0 ← c;\\n2: while f(bi) ̸= c do\\n3:\\nbi+1 ← bi + (c − f(bi)); i ← i + 1; b∗ ← bi;\\n4: r∗ ← λ − b∗q; b ← b∗;\\n5: if r∗ < 0 then b ← b − 1;\\n6: else if r∗ ≥ q then b ← b + 1;\\n7: return b;\\nIV. DIVIDER HARDWARE IMPLEMENTATION\\nARCHITECTURES AND COMPARISONS\\nThis section ﬁrst proposes efﬁcient hardware architectures\\nto implement the proposed divider. Then comparisons with\\nother dividers are provided.\\nWhen u < w/2, Algorithm 2 has one iteration in the\\nloop and it reduces to the algorithm proposed in [5]. The\\nhardware architecture for implementing one iteration of Line\\n3 of Algorithm 2 is available in [5]. bi+1 = bi + c− f(bi) can\\nbe simpliﬁed as c + ⌊ bi(2u∓1)\\n2w\\n⌋ + D. From [5], D = 0 when\\nbi(2u ∓ 1)/2w is an integer and D = 1 otherwise. However,\\nsince bi is a w-bit number, it does not have w factors of 2.\\n2u∓1 does not have any factor of 2. Therefore, bi(2u∓1)/2w\\ncan not be an integer, and the architecture from [5] can be\\nsimpliﬁed to the units in the dashed block in Fig. 1. t copies\\nof these units are needed to implement t iterations of the\\nloop in Algorithm 2 in a pipelined manner. The shifters align\\nthe bits in the inputs to take care of the 2u multiplication\\nand 2w division. The control signal s is ‘1’ and ‘0’ when\\nq = 2w − 2u + 1 and q = 2w − 2u − 1, respectively. cini is\\nset to ‘1’ when the lower u bits of bi are all ‘0’ and s = 1\\nin order to eliminate the addition on unnecessary bits. More\\nexplanations on these signals can be found in [5].\\nFrom previous analysis, λ − b∗q ∈ {r, r ± q}. Hence, the\\nexact value of λ − b∗q does not need to be calculated. It is\\nsufﬁcient to know if λ−b∗q is negative, positive and less than\\nq, or positive and larger than or equal to q. For q = 2w−2u±1,\\nthe numbers that need to be added to calculate λ − b∗q are\\nshown in Fig. 2(a). Since r < r + q < 2w+1, if λ − b∗q is\\npositive, all its bits with weight at least 2w+1 are ‘0’. On the\\nother hand, |r − q| < 2w. Hence, all the bits with weight at\\nleast 2w+1 are ‘1’ in the 2’s complement representation of\\nr − q. Therefore, it is sufﬁcient to tell that λ − b∗q is negative\\nif the (w+2)-th bit in λ−b∗q is ‘1’. If this bit is ‘0’, whether\\nλ − b∗q = r or r + q can be decided by comparing the lower\\nw + 1 bits of λ − b∗q with q. As a result, only the w + 2 least\\nsigniﬁcant bits of the numbers in Fig. 2 need to be added.\\nShifter\\nShifter\\nb0\\nu\\ncin1\\nwͲu\\n1\\nb1\\n0\\n1\\nD\\nShifter\\nShifter\\nbtͲ1\\nu\\ncint\\nwͲu\\nbt=b*\\n0\\n1\\nShifter\\nShifter\\nb1\\nu\\ncin2\\nwͲu\\nb2\\n0\\n1\\nD\\n...\\nD\\ns\\nc\\n...\\n......\\n...\\n...\\nFigure 1: Architecture for calculating b∗ in Algorithm 2.\\nͲb*\\nb*\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\nͲ/+b*\\nu\\nwͲu\\nu\\nʄ\\nShifter\\nb*\\nu\\nʄ[w+1:0]\\nComparator\\nq\\n(a)\\n(b)\\nb\\nb*+1 b*Ͳ1 b*\\n‘00’\\nw\\nw+2\\n1\\n‘0...0’\\nw\\nw+2\\nw+2\\nw+2\\nw\\nw+1\\ns\\nMSB\\nw\\nw+2\\n2\\nFigure 2: (a) The numbers need to be added for calculating\\nλ − b∗q; (b) The hardware architecture to derive b from b∗\\nusing partial calculation of λ − b∗q.\\nThe architecture in Fig. 2(b) computes b from b∗. First ‘00’\\nis padded to the left of the most signiﬁcant bit (MSB) of b∗\\nto extend it to (w + 2)-bit. The multiplexer on the top passes\\neither 2’s complement of b∗ or b∗ depending on whether the\\nlast number to add in Fig. 2(a) is −b∗ or +b∗. The shifter in\\nFig. 2(b) shifts the lower w − u + 2 bits of the input to the\\nleft and pads u ‘0’s to the right. It aligns the bits from the b∗\\nnumber in the middle of Fig. 2(a) for addition. Only the two\\nLSBs of the −b∗ number in Fig. 2(a) need to be added. They\\nare padded with w ‘0’s to the right for the addition. The four\\nnumbers from Fig. 2(a) are added up by the carry-save adder\\nin the middle of Fig. 2(b). The lowest w + 1 bits of the adder\\noutput is compared with q. The value for b is chosen based\\non the MSB of the adder output and the comparison result.\\nTo further evaluate the complexity of our proposed design,\\nit is synthesized using the Global Foundries 22FDX process\\nfor an example case of w = 32 and 2w/3 ≤ u < 3w/4.\\nIn this case, t = 3 copies of the units in Fig. 1 are needed.\\nDifferent timing constraints were tried in the synthesis, and\\nthe tightest timing constraint that does not lead to a substantial\\narea increase is reported in Table I in order to compare the\\nminimum achievable clock period of different designs. In the\\nproposed design, t = 3 copies of units compute b∗ in three\\npipelining stages. After that, another clock cycle is needed to\\ncalculate b.\\nTable I: Synthesis results of dividers with w = 32 using Global\\nFoundries 22FDX process\\nTiming\\nArea\\nLatency\\nconstraint (ps)\\n(µm2)\\n(# of clks)\\nDesign in [9]\\n2600\\n11168\\n1\\nDesign in [10]\\n1460\\n6140\\n1\\nProposed design (t = 3)\\n330\\n1320\\n4\\nFor comparison, the designs in [9] and [10] are synthesized\\nand the results are also listed in Table I. They have 2w × 2w\\nand 2w×w multipliers, respectively, in their critical paths. On\\nthe other hand, the critical path of our proposed design only\\nconsists of a shifter, a w-bit carry-save adder, a comparator,\\nand a few multiplexers as shown in Fig. 2. As a result, our\\ndesign achieves a much shorter clock period. The achievable\\nimprovement further increases for larger w. Although our\\ndesign requires t+1 clock cycles to compute the quotient, the\\nlatency is still lower than that of the previous design due to\\nthe shorter clock period. For w = 32, t = 1 for 1 ≤ u < 16,\\nt = 2 for 16 ≤ u < 22, and t = 3 for 22 ≤ u < 24.\\nHence 75% of possible u leads to t ≤ 3. Our proposed design\\nachieves 1 − 330 × 4/1460 = 9%, 1 − 330 × 3/1460 = 32%\\nand 1 − 330 × 2/1460 = 55% latency reductions for t = 3, 2,\\nand 1, respectively, compared to the divider in [10].\\nSince the designs in [9] and [10] consist of wide multipliers,\\ntheir area requirement is much larger than that of the proposed\\ndesign as shown in Table I. The proposed architecture achieves\\n1-1320/6140=79% area reduction compared to the design in\\n[10]. For a u corresponding to smaller t, fewer copies of the\\nunits in Fig. 1 are utilized, and the achievable area reduction\\nwould be more signiﬁcant.\\nV. CONCLUSIONS\\nThis paper proposed a low-complexity integer divider for\\ncalculating the quotient when the divisor has a small number\\nof nonzero bits. It generalized the previous design to handle\\nmore possible divisors and the case that the dividend is not\\nan integer multiple of the divisor. In addition, by analyzing\\nthe possible values of the intermediate results, simpliﬁcations\\non the hardware implementation architectures are developed.\\nCompared to prior designs that are based on multiplying the\\ninverse of the divisor, the proposed design reduces the area\\nrequirement to a fraction and also has much shorter latency.\\nFuture research will extend the proposed algorithm to the case\\nthat the divisor has more than three nonzero bits.\\nREFERENCES\\n[1] Z. Brakerski, C. Gentry, and V. Vaikuntanathan, “(Leveled) fully homo-\\nmorphic encryption without bootstrapping,”\\nProc. of ACM Innovation\\nin Theoretical Computer Science, pp. 309-325, 2012.\\n[2] Z. Brakerski, “Fully homomorphic encryption without modulus switch-\\ning from classical GapSVP,” Lecture Notes in Computer Science, vol.\\n7417, pp. 868-886, Springer, 2012.\\n[3] J. H. Cheon, A. Kim, M. Kim, and Y. Song, “Homomorphic encryption\\nfor arithmetic of approximate numbers,” Advances for Cryptology-\\nASIACRYPT, pp. 409-437, 2017.\\n[4] J. H. Cheon, K. Han, A. Kim, M. Kim, and Y. Song, “A full RNS\\nvariant of the approximate homomorphic encryption,” Selected Areas in\\nCryptography, pp. 347–368, 2018.\\n[5] S. Akherati, X. Zhang, “Low-complexity ciphertext multiplication for\\nCKKS homomorphic encryption,” IEEE Trans. on Circuits and Syst.-II,\\n2023.\\n[6] F. Dinechin, L. Didier, “Table-based division by small integer constants,\\nreconﬁgurable computing: architectures, tools and applications,” Lecture\\nNotes in Computer Science, vol. 7199, pp. 53-63, 2012.\\n[7] D. Kromichev, “FPGA based Canny: advanced integer division algo-\\nrithm,” 2021 12th National Conf. with Int. Participation, pp. 1-4, 2021.\\n[8] D. Kromichev, “FPGA based edge detection: integer division algorithm\\nwith a constant divisor,” 13th National Conf. with Int. Participation, pp.\\n1-4, 2022.\\n[9] D. Cavagnino and A. E. Werbrouck, “Efﬁcient algorithms for integer\\ndivision by constants using multiplication,” The Computer Journal, vol.\\n51, no. 4, pp. 470-480, 2008.\\n[10] T. Drane, W. -C. Cheung and G. Constantinides, “Correctly rounded\\nconstant integer division via multiply-add,” IEEE Intl. Symp. on Circuits\\nand Sys., pp. 1243-1246, 2012.\\nShifter\\nShifter\\nb*\\nm\\n 1\\nn-m\\nb*\\n-b*\\n 2\\n-/+b*\\nr*\\nComparator/\\nb calculation\\nq\\n'},\n",
       " {'Abstract': \"In scenarios where data is limited or incomplete, machine learning classification models face challenges. The Small and Incomplete Dataset Analyser (SaNDA) emerged as a suitable technique to improve performance in these situations by leveraging data abstractions. This paper evaluates how modifying the abstraction methods affects the SaNDA classification process's explainability and accuracy. We investigate alternative abstraction techniques, constant binning and quantiles, in contrast to the conventional ROC curve-based method. Our experiments on 13 small and two medium datasets, including synthetic DIGEN datasets, provide empirical evidence. The findings suggest that quantile-based abstractions (quantiles 20) offer better classification performance compared to constant binning or the original ROC curve method. The number of categories produced by the abstraction method is of greater significance than the type of abstraction used, influencing accuracy. Increasing the abstraction levels consistently enhances performance but introduces limitations related to statistical representation, explainability, and computational efficiency. Our modified SaNDA with quantiles 20 abstraction method exhibits promising potential as an alternative to Random Forest for complete datasets and demonstrates its advantage with missing data. Future work may involve developing a dynamic approach to selecting methods and abstraction levels, allowing for customization to specific problem requirements.\",\n",
       "  'Introduction': {'Relevance_of_Data_Abstraction_Methods': 'The applicability of widely adopted machine learning (ML) methods to classification for Critical Decision-Making (CDM) processes is circumscribed by the imperatives of explicability and uncertainty management.',\n",
       "   'Background_of_SaNDA': 'Recently, Small and Incomplete Dataset Analyser (SaNDA) has been proposed to enhance the ability to perform classification in such domains, by develop-ing a data abstraction protocol using a ROC curve-based method.'},\n",
       "  'Literature_Review': {'Challenges_with_Deep_Learning': 'Classification for CDM processes encompasses two fundamental attributes: explainability [24] and uncertainty [34], characteristics notably absent in outcomes generated by deep learning [6, 30].',\n",
       "   'Limitations_of_Random_Forest': 'Random Forest was originally presented as an approach for improving the accuracy of single decision tree which sometimes has problem with over-fitting and therefore with generalization [3].'},\n",
       "  'Methodology': {'Verification_Metrics': 'Several metrics are used as a valuable tool for comparing the performance of ML classification models. They provide a comprehensive assessment of a model’s performance across different aspects.',\n",
       "   'SaNDA_Classification_Method': 'SaNDA consists of two main parts, which interacts with each other to provide compre-hensive method of data analysis: classification, and KG [15].',\n",
       "   'Abstraction_Methods': 'Abstraction as a procedure for reducing the number of values that a variable can take is defined in Sec. 2.2.'},\n",
       "  'Results': {'Comparison_of_Abstraction_Methods': 'For each dataset detailed in Section 3, it contrasts the BA acquired from SaNDA across various abstraction types: ROC, deciles (quantiles 10), quantiles 20, 10 bins, and 20 bins.',\n",
       "   'Quantile-Based_Abstractions_Offer_Better_Classification_Performance': 'Based on this experiment, it can be observed that overall quantiles 20 offers better classification performance compared to 20 bins abstraction method.',\n",
       "   'Number_of_Categories_Produced_by_Abstraction_Method_Impacts_Accuracy': 'One might naively believe that increasing the number of abstractions will enhance classification accuracy. However, this approach harbours potential pitfalls that warrant careful consideration.'},\n",
       "  'Conclusion': {'SaNDA_with_Quantiles_20_Abstraction_Method_as_Alternative_to_Random_Forest': 'The proposed new data abstraction methods significantly outperform abstractions using ROC.',\n",
       "   'Future_Work': 'A future direction of research may involve the choice of an appropriate method and level of abstraction.'},\n",
       "  'title': 'The Significance of Data Abstraction Methods in Machine Learning Classification Processes for Critical Decision-Making',\n",
       "  'author': 'Karol Capała, Paulina Tworek, Jose Sousa',\n",
       "  'textdata': 'Highlights\\nThe Significance of Data Abstraction Methods in Machine Learning Classifi-\\ncation Processes for Critical Decision-Making\\nKarol Capa la, Paulina Tworek, Jose Sousa\\n• Classification is characterised by the attributes of explicability and uncertainty\\nmanagement\\n• Medical, behavioural and financial datasets, among others, are often small and full\\nof missing data\\n• SaNDA was proposed as an explainable method tweaked for such datasets\\n• This paper analyses how different abstraction methods influence SaNDA’s perfor-\\nmance\\n• Modified SaNDA with new abstraction methods is compared against first version\\nof SaNDA and Random Forest\\narXiv:2401.11044v1  [cs.LG]  19 Jan 2024\\nThe Significance of Data Abstraction Methods in Machine\\nLearning Classification Processes for Critical Decision-Making\\nKarol Capa laa,b,∗, Paulina Tworeka, Jose Sousaa\\naPersonal Health Data Science group, Sano – Centre for Computational Medicine, Czarnowiejska 36\\nbuilding C5, 30-054 Krak´ow, Poland\\nbInstitute of Computer Science, AGH University of Krakow, Kawiory 21, 30-059 Krak´ow, Poland\\nAbstract\\nThe applicability of widely adopted machine learning (ML) methods to classification\\nis circumscribed by the imperatives of explicability and uncertainty, particularly evident\\nin domains such as healthcare, behavioural sciences, and finances, wherein accountability\\nassumes priority. Recently, Small and Incomplete Dataset Analyser (SaNDA) has been\\nproposed to enhance the ability to perform classification in such domains, by develop-\\ning a data abstraction protocol using a ROC curve-based method. This paper focuses\\non column-wise data transformations called abstractions, which are crucial for SaNDA’s\\nclassification process and explores alternative abstractions protocols, such as constant\\nbinning and quantiles. The best-performing methods have been compared against Ran-\\ndom Forest as a baseline for explainable methods. The results suggests that SaNDA can\\nbe a viable substitute for Random Forest when data is incomplete, even with minimal\\nmissing values. It consistently maintains high accuracy even when half of the dataset is\\nmissing, unlike Random Forest which experiences a significant decline in accuracy under\\nsimilar conditions.\\nKeywords:\\nMissing Data, Small Datasets, Explainable Models, Machine Learning,\\nData Science, Uncertainty\\n1. Introduction\\nThe recent upswing in the adoption and advancement of Artificial Intelligence (AI)\\napplied to classification for Critical Decision-Making (CDM) has been propelled by the\\nimplementation of deep learning, grounded in the unique amalgamation of substantial\\ndatasets (big data) and the availability of computational power [20]. This adoption of\\ndeep learning is centred on the comparative analysis of outcomes between human decision-\\nmaking and machine-driven processes. However, this prompts question on the degree of\\ncomparability between these outcomes [24].\\nClassification for CDM processes encompasses two fundamental attributes: explain-\\nability [24] and uncertainty [34], characteristics notably absent in outcomes generated by\\ndeep learning [6, 30]. The integration of deep learning into CDM processes has encoun-\\ntered obstacles due to its opaque nature [19, 12], leading to the emergence of research\\n∗Corresponding author\\nEmail addresses: k.capala@sanoscience.org (Karol Capa la), p.komorek@sanoscience.org\\n(Paulina Tworek), j.sousa@sanoscience.org (Jose Sousa)\\nPreprint submitted to Information Sciences Journal\\nJanuary 23, 2024\\ntrends such as eXplainable Artificial Intelligence (XAI) [18, 27] and one-shot learning\\n[17].\\nExplainability holds particular and pivotal significance in the technological assimi-\\nlation within CDM, primarily due to accountability concerns [6].\\nWhen ML informs\\nCDM, a comprehensive explanation is imperative to elucidate the classification rationale,\\nfacilitating a deeper understanding of the decision-making process.\\nNowadays, among various ML methods, deep learning is commonly used, as it has a\\nhuge advantage in finding patterns in the data. However, the application of deep learning\\nrequires the collection of substantial datasets, which is not achievable in many areas.\\nPresence of small and missing data, characteristic for many fields, causes uncertainty [30]\\nto be an inherent property of CDM scenarios [28].\\nDespite the advancements on data access [25], federated learning [37] and encrypted\\nlearning techniques [33], these challenges persist among others across diverse domains,\\nincluding healthcare, behavioural sciences, and finances [5, 8, 21]. Moreover, challenges\\nsuch as privacy concerns, limited accessibility, ethical considerations, and inherent data\\nnature impend the acquisition of significant datasets [26, 14, 35, 31]. Therefore, it is\\nnecessary to focus on creating solutions for small and incomplete data.\\nRandom Forest was originally presented as an approach for improving the accuracy\\nof single decision tree which sometimes has problem with over-fitting and therefore with\\ngeneralisation [3]. Random Forest [4] is based on an ensemble of individual decision tree\\ntrained on random samples of the training data, thereby achieving different characteristics\\nof the data distribution. Once the decision trees have been constructed, the random forest\\nalgorithm makes a prediction by averaging the predictions of all of the trees reducing the\\nvariance of the model.\\nThis produces more accurate predictions than any individual\\ntree could make, while remaining explainable. Moreover, in case of medium size, tabular\\ndatasets, Random Forest may outperform deep learning on classification task [10, 11].\\nThese two properties make them suitable for use in many sensitive areas.\\nHowever, similarly to the majority of the widely used ML algorithms, they are unable\\nto properly proceed with missing values. Therefore, they require removal of incomplete\\nentries, or, when it is not possible or undesired, filling them with the help of imputation.\\nAn alternative explainable ML method, SaNDA, built on the use of incomplete data\\nand with the classification explainability provided by knowledge graphs (KG), has been\\nrecently proposed by us [15]. The solution eliminates the need for imputers, which can\\nintroduce erroneous information into the model, especially when the number of missing\\nvalues is high or there is a bias in their occurrence. In addition to KG, another impor-\\ntant element of SaNDA are abstractions, which is the transformation of data values into\\na smaller categorical distribution. More specifically, for each column of data, SaNDA\\nassigns the values it contains to one of two states (categories) called UP or DOWN.\\nHowever, this binary abstraction choice is not the only possible one.\\nThis paper in-\\nvestigates the impact of other abstraction methods on the classification performance of\\nSaNDA. The aim of the experiments is to improve SaNDA’s performance in classifica-\\ntion processes for CDM, while maintaining functionality, even with very large amounts\\nof missing data.\\nSec. 2 provides a more detailed description of the classification process of SaNDA, as\\nwell as the alternative abstraction methods proposed in this paper and the metrics used\\nto evaluate model performance. Research questions and descriptions of the experiments\\nundertaken to answer them are described in Sec. 3. Results of the numerical experiments\\n2\\nare given in Sec. 4. Finally, the paper is closed with a summary and conclusions (Sec. 5).\\n2. Model\\n2.1. Verification metrics\\nSeveral metrics are used as a valuable tool for comparing the performance of ML\\nclassification models. They provide a comprehensive assessment of a model’s performance\\nacross different aspects. To define chosen metrics the following basic concepts as results\\nof the classification task should be introduced:\\n• True positives (TP) - number of correctly classified instances of the positive class,\\n• False positives (FP) - number of incorrectly classified instances of the negative class,\\n• True negatives (TN) - number of correctly classified instances of the negative class,\\n• False negatives (FN) - number of incorrectly classified instances of the positive class.\\nBased on the concepts several metrics of classification tasks are defined. Among these,\\nbalanced accuracy (BA) provides a comprehensive assessment of model performance.\\nBA is the primary metrics chosen for evaluation the performance of selected abstraction\\nmethods and comparing them with Random Forest. It is one of the simplest metrics\\nfor evaluating a classification model’s ability to accurately predict classes in the context\\nof imbalanced datasets, which are common, among others, for medical and financial\\nproblems. BA is a extension of standard accuracy metrics, it is an average accuracy from\\nboth the minority and majority classes, i.e.,\\nBA =\\nTP\\nTP+FN +\\nTN\\nTN+FP\\n2\\n.\\n(1)\\nBeyond a simple BA, there are other metrics which offer different insights into how\\nwell a classification model is performing [13]. Recall, defined as\\nrecall =\\nTP\\nTP + FN,\\n(2)\\nmeasures the ability for the correct identification of instances of the positive class from\\nall the actual positive samples in the dataset. This metrics is widely employed in various\\ndomains, particularly in application classification algorithms used, e.g. in medical and\\nfinancial scenarios. For instance, in healthcare it is crucial especially in medical screening\\nand diagnostic testing as high value of recall suggests that the test performs well in\\ndetecting TP cases, thereby reducing the chances of missing instances (FN) [13].\\nIn\\nfinancial area, recall is frequently measured in the context of risk management and fraud\\ndetection to ensure that potential risks or fraudulent activities are detected effectively\\n[22].\\nHowever, there are also scenarios, where the accuracy of positive predictions made by\\na classification model is critical, e.g. consequences of carrying out medical or financial\\ninterventions or procedures that are not actually required, stemming from false positive\\nresults, can have a substantial impact.\\nIn such case precision should be monitored.\\nPrecision provides information about the quality of positive predictions made by the\\n3\\nmodel. It is defined as the number of positive instances divided by the total number of\\npositive predictions, both correctly and incorrectly classified as positive class:\\nprecision =\\nTP\\nTP + FP\\n(3)\\nAchieving a careful equilibrium between recall and precision holds paramount impor-\\ntance in many applications. However, the optimal metrics for evaluation ML models\\nshould be chosen based on the specific scope and nature of the problem at hand.\\n2.2. SaNDA classification method\\nSaNDA consists of two main parts, which interacts with each other to provide compre-\\nhensive method of data analysis: classification, and KG [15]. While classification module\\ncan separate data into classes and provide information how well model captures properties\\nof the data, KG enhance explainability of the results and provide deeper insight into inter-\\ndependence between features. Since the primary objective of this paper is to investigate\\nthe potential for enhancing SaNDA’s data representation capability, which is primar-\\nily evaluated through the classification aspect, this section focuses on the explanation\\nof classification method using SaNDA. The most important concepts for a classification\\nproblem using SaNDA are abstractions and the classification algorithm itself.\\nData abstraction involves simplifying a specific set of data by condensing it into a\\nmore streamlined representation of the entire dataset. It allows for eliminating specific\\ntraits from an object or concept to distill it down to a collection of fundamental elements.\\nTo this end, data abstraction creates a simplified representation of the underlying data,\\nwhile hiding its complexities and associated operations. In addition, abstraction method\\nused in SaNDA ensures the anonymisation of the data. There are various approaches to\\ncreating abstractions.\\nThe abstraction is formally defined as [15]:\\nDefinition 1. Given A and B two sets of numerical values, with |A| ≥ |B|, an abstraction\\nis defined as a function F : A → B that maps the values of A to values of B.\\nIn other world, process of abstraction maps original values from the data into smaller\\nset of values. It is important to mention that abstraction process is performed on column-\\nby-column basis. In the founding paper, original data was transformed into two classes,\\nusing ROC curve method. However, other methods resulting in the different number\\nof abstractions (categories), i.e., cardinality of set |B| ̸= 2, may also be an appropriate\\nsolution. All of the methods using ROC curves and the new proposed alternative methods\\nare presented in the subsection 2.3. For the need of SaNDA, elements of the new set\\nare represent as B = B1, ..., B|B| and can be treated as a set of categorical variables.\\nTherefore, if every of n features in the dataset is abstracted to the same amount of\\nabstractions, each input is now described by a set of categorical values X = {x1, · · · , xn}.\\nThe input constructed in this way can be combined with the natively categorical data,\\nenabling easy integration of different types of data [9].\\nThen based on obtained data abstractions, we generate an explainable KG represen-\\ntation, which is created as it was previously described [16, 15]. Through building the\\ngraphs, a representation for each class based on available features is prepared. The sig-\\nnificance of each vertex of the KG is represented by the probability of each feature being\\nin given category of the given class.\\n4\\nFollowing paper focuses on binary classification, however SaNDA algorithm can be\\ndescribed and performed also for arbitrary number of classes [9, 15]. Let Cj be the jth\\nclass. Then for given Cj the following probability can be computed as:\\nP(X|Cj) =\\nn\\nY\\ni=1\\nP(xi|Cj)\\n(4)\\nThe class with the highest P(X|Cj) is chosen, i.e.\\nargmaxC {P(X|Cj)} .\\n(5)\\nTherefore, SaNDA assigns a given set of values to the class for which its occurrence is\\nmost likely. If the given feature xi is empty (missing value) or is represented by null\\nvalues, it is skipped in the calculation of the probability P(X|Cj).\\nContrary to the majority of commonly used methods, SaNDA does not divide feature\\nspace base on optimisation of some classification function. Instead values of every feature\\nare individually divided based on abstraction methods, which does not need (but can)\\ntake into account class distribution. From the perspective of the entire feature space, it\\ncreates division into grind, which every “cell” is label as one of the classes based on the\\nconditions given by Eqs. (4) and (5). These conditions do not consider every individual\\n“cell” but rather try to approximate it base on every dimension separately. On the one\\nhand, this approach may diminish the classification accuracy of the method by failing to\\nincorporate more intricate, nonlinear relationships, particularly in the context of a sparse\\ngrid (where the number of abstractions is low). On the other hand it allows to complete\\nclassification test even in the present of missing values by considering lower-dimensional\\nspace.\\n2.3. Abstraction methods\\nAbstraction as a procedure for reducing the number of values that a variable can take\\nis defined in Sec. 2.2. This section presents methods of abstracting data used in this\\npaper.\\n2.3.1. ROC Curves\\nROC curve abstractions split the values of a feature into two categories in a way that\\nmaximizes the separation between the classes in the feature. To be more precise, the\\nvalues of the feature in the abstracted column are split into smaller and larger than the\\nvalue that maximizes BA, as given by Eq (1).\\n2.3.2. constant binning\\nAbstractions though constant binning divide the values of a feature into categories of\\nequal range. Let A be a set of the values taken by the feature. Then L = maxA − minA\\nis a range of values. constant binning divide range of the data L, into n equal intervals\\nof length L/n, assigning the feature a number corresponding to the bin number in which\\nits original value falls.\\n5\\n2.3.3. Quantiles\\nWhile constant binning divides data to keep the range of every bin equal, abstractions\\nbased on quantiles divide data into categories of the equal size. Cut-off values are chosen\\nbased on the values of quantiles. Quantile of the order n is defined based on the probability\\ndistribution of the random variable ranging over the set X is such a way that the locations\\nyq of the quantile q (q = k/n, where k ≤ n) given by\\nq =\\nZ yq\\n−∞\\np(x)dx.\\n(6)\\nIn other words, quantile yq is such value that cumulative distribution function takes at\\nthis point value q or alternatively qth part of the data takes values smaller or equal yq.\\nQuantile-based abstractions therefore assign the feature the largest k for which x < yq,\\ni.e., corresponding to the highest quantile larger than initial value.\\nThe most commonly used quantiles are ones of the order n = 2 (median), n = 4\\n(quartiles) and n = 10 (deciles).\\n3. Experiments\\nThe main goal of this research is to improve the efficacy of the SaNDA algorithm by\\nexamining the abstraction phase. To achieve this, it is necessary to first explore how\\ndifferent abstraction methods influence the metrics that describe SaNDA performance.\\nBased on the results, determine whether there are criteria for choosing an abstraction\\nmethod that can be applied in model design, and if so, how to choose the best abstraction\\nmethod for a given use of the SaNDA algorithm. Finally, verify how the best-performing\\nabstraction methods influences SaNDA performance in the presence of missing values in\\nthe dataset. For the sake of comparison with the original research, we used the 5 datasets\\nfrom the paper proposing SaNDA [15] in the following experiments, supplemented by 10\\nsynthetic DIGEN datasets (8 4426, 10 8322, 17 6949, 22 2433, 23 5191, 24 2433, 32 5191,\\n35 4426, 36 466, 39 5578). A selection of datasets is briefly described below.\\nThe Ionosphere dataset [7, 32], a collection of 351 radar measures of the ionosphere in\\nGoose Bay, Labrador, is used to classify its structure. The dataset includes 34 numeric\\nfeatures that measure the number of free electrons and other electromagnetic signals in\\nthe ionosphere.\\nThe Sonar dataset [7, 1] consists of 60 numerical features that measure the shape and\\ncharacteristics of the sonar signal. The classification task is to distinguish underwater\\nsurfaces as rock or metal. It is the smallest number of records equal to 208.\\nThe Wisconsin Breast Cancer dataset [7, 36] uses 30 numerical features that measure\\nthe shape and composition of a breast mass to describe 569 fine needle aspirates. The\\ntask is to distinguish between cancerous and non-cancerous samples. It consists of 569\\nentries.\\nThe Accelerometer dataset [7, 29] was generated for prediction of motor failure time\\nwith application of an artificial neural network. It uses 4 numerical features; 3 of them\\nrepresents the values of x, y and z axes, while the fourth is cooler maximum speed per-\\ncentage ranging from 20% to 100% with 5% intervals. The fifth attribute was used as\\ntarget class, where normal configuration was admitted as negative class, while perpen-\\ndicular and opposite configuration as positive class. It has the largest number of records\\nequal to 102000.\\n6\\nThe HIGGS dataset [7, 2] is produced from Monte Carlo simulations of particle decays\\nand contains 1.1 × 107 entrances. Each process is described by 28 features, of which 21\\nare kinematic properties measured by the particle detectors in the accelerator and the\\nremaining 7 are quantities derived from them. The classification task is to distinguish\\nbetween measurements of background noise and those connected to the observation of the\\nHiggs particle. 105 randomly selected records were taken for the experiments presented\\nin this work.\\nDIGEN [23] datasets were designed to differentiate the performance of some of the\\nleading classification methods. It is the collection of 40 synthetic datasets created from\\neach of the generative mathematical functions for testing binary classification task. Every\\ndataset from DIGEN contains 10 features of 1000 normally distributed values.\\nIn summary, the experimental setup consists of 13 small and 2 medium datasets. This\\ndisproportion is caused by the target domain for SaNDA, which is related to the nature\\nof data occurring in personal health care or finances.\\nTo achieve the goals of this research, we conducted two numerical experiments using\\nthe datasets described above. The first experiment focuses on exploring the different\\nabstraction methods presented in the Sec 2.3 and their impact on SaNDA performance\\nwith an indication of classifciation accuracy. For this purpose, each of the studied datasets\\nwas transformed using one of the following abstraction methods:\\n• constant binning into 10 and 20 bins\\n• quantiles of order 10 (deciles) and 20\\n• using ROC curve method as a control group\\nThen, using the method used in SaNDA, models were created for classification purposes.\\nIt is important to note that in this experiment we do not create any missing data in the\\ndatasets.\\nFollowing the initial experiment, the abstraction methods yielding the highest BA\\nwas selected for each dataset with the selected percentage of missing data. Subsequently,\\nthe SaNDA classification algorithm was modified to incorporate this chosen abstraction\\nmethods and applied to datasets containing 1%, 5%, 10%, and 50% of missing data.\\nDatasets with n% missing data were created from the original datasets by randomly\\nremoving n% of data from each column (feature). The results were compared to those\\nobtained from the original, ROC curve-based SaNDA and Random Forest algorithms.\\nFor Random Forest, missing data were replaced with zeros to be able to process the\\ndatasets. Subsequently, for all of the methods the BA, recall and precision values were\\ncompared.\\n4. Results\\nFigure 1 and Table 1 show the BA for experiment 1. For each dataset detailed in\\nSection 3, it contrasts the BA acquired from SaNDA across various abstraction types:\\nROC, deciles (quantiles 10), quantiles 20, 10 bins, and 20 bins. For the accelerometer,\\nHiggs, WDBC, and DIGEN datasets, with the exception of DIGENs: 22 2433, 24 2433,\\nand 17 6949, the use of quantiles 20 results in superior BA for classification. On the other\\nhand, for the remaining DIGENs, Sonar and Ionosphere datasets, the application of 20\\nbins abstraction yields higher BA compared to other abstraction methods.\\n7\\nBased on this experiment, it can be observed that overall quantiles 20 offers better\\nclassification performance compared to 20 bins abstraction method. In the case of the\\nopposite being true, the discrepancy is significantly narrower. Similar but weaker trend\\ncan be also observed between deciles and 10 bins.\\nIn light of the results, quantile-\\nbased abstractions can be regarded as a preferable choice of abstraction for SaNDA\\nclassification.\\nHowever, the number of categories into which data is transformed as a result of ab-\\nstraction process is of far greater significance than the type of abstraction employed. This\\ncan be explained by the fact that a higher number of abstractions (categories) enhances\\nthe resolution of the grid created in the feature space. As a result, a denser grid enables\\na more homogeneous distribution of data across its constituent parts, leading to more\\naccurate classification performance.\\nOne might naively believe that increasing the number of abstractions will enhance\\nclassification accuracy. However, this approach harbours potential pitfalls that warrant\\ncareful consideration. First, increasing the number of abstractions can result in a de-\\ncrease in the average number of data points per category, potentially compromising the\\naccurate representation of statistical properties. Furthermore, an increase in the number\\nof abstractions can lead to an expansion in the number of nodes within the KG gener-\\nated by SaNDA, potentially diminishing its explainability. Finally, an extensive number\\nof abstractions can exert a detrimental impact on the computational time required for\\ngenerating classification and KG, a factor that may become particularly crucial when\\ncomputational resources are constrained or the analysed problem demands expediency.\\nGiven these considerations, the maximum number of abstractions employed in this re-\\nsearch was limited to 20.\\nBased on the results of Experiment 1, Experiment 2 employed SaNDA classification\\nusing 20 bins and quantiles 20 for comparison with the original ROC curves abstractions\\nand Random Forest was performed. The results of this comparison for increasing per-\\ncentage of missing data are presented in Figs 2–4 and in Tables 1–15. Fig. 2 shows BA\\nas a function of missing data for each dataset separately. In all of the cases, 20 categories\\nabstractions, bin- or quantile-based, outperform both ROC-based abstractions. Random\\nForest achieves higher BA in only 4 of 15 datasets with non-zero amount of missing\\ndata: Accelerometer, HIGGS and DIGENs: 39 5578, 23 5191. However, for HIGGS and\\nDIGEN23 5191 the difference between 20 quantiles abstractions SaNDA and Random\\nForest in the BA is slight. When the proportion of missing data increases the advan-\\ntage of SaNDA with 20 categories abstraction methods increases, as it achieves higher\\nBA for majority of tested datasets. A notable exception is observed in the Accelerom-\\neter dataset, where Random Forest surpasses both proposed new abstraction methods\\nregardless of amount of missing values. This behaviour can be attributed to the limited\\nnumber of features, specifically 4, characterising this dataset. Additionally, the size of\\nthe dataset, which can be considered average with approximately 105 inputs, favors the\\nefficiency of Random Forest.\\nBA is not the only important metrics from the point of view of classification models.\\nThe proposed system, due to its ability to anonymize data and dealing with missing\\nvalues may be particularly relevant, among others, in areas of medicine, social sciences\\nand finance where the high level of recall and precision is also very important. Therefore,\\nwe decided to check whether increasing the number of abstractions (categories) would\\naffect mentioned metrics and compare them with the values obtained for Random Forest.\\n8\\nThe results presented in Figure 3 showed that the recall was higher for quantiles 20\\nthan for ROC for all of the chosen datasets. Comparison of quantiles 20 and Random\\nForest shows that Random Forest wins in the case of 6 datasets (Accelerometer, HIGGS,\\nWisconsin Breast Cancer and DIGENs: 8 4426, 39 5578, 23 5191, when there aren’t any\\nmissing values. Quantiles 20 or 20 bins achieves better results for 9 datasets (Ionosphere,\\nSonar and DIGENs: 32 5191, 36 466, 35 4426, 10 8322, 22 2433, 24 2433, 17 6949) with\\nan advantage for quantiles. The only dataset where Random Forest performance is much\\nbetter is Accelerometer, but the same observation was noticed for BA and it may be\\nrelated to low number of features.\\nSimilar conclusions were reached for precision, see Fig. 4. For all datasets, the pro-\\nposed new abstraction methods performed significantly better than ROC. In the ab-\\nsence of missing data Random Forest achieves highest precision values for 6 (HIGGS,\\nIonosphere, Wisconsin Breast Cancer and DIGENs: 39 5578, 23 5191, 22 2433) of 15\\ndatasets, in the case of 2 of these datasets the difference was marginal (DIGENs: 23 5191\\nand 22 2433). For the rest of the datasets, classification using either quantiles 20 or 20\\nbins yields higher precision (Accelerometer, Sonar and DIGENs: 8 442, 32 5191, 36 466,\\n35 4426, 10 8322, 24 2433, 17 6949). With 10% of the missing data quantiles 20 or 20 bins\\noutperform Random Forest in 12 out of 15 cases for recall (HIGGS, Ionosphere, Sonar and\\nDIGENs: 8 442, 32 5191, 39 5578, 36 466, 35 4426, 10 8322, 23 5191, 24 2433, 17 6949)\\nand in 12 cases for precision (Accelerometer, HIGGS, Sonar and DIGENs: 32 5191,\\n39 5578, 36 466, 35 4426, 10 8322, 23 5191, 24 2433, 17 6949). A greater proportion of\\nmissing data works against Random Forest, which is particularly noticeable when 50%\\nof data is removed - in such conditions Random Forest prevails by a minimal margin for\\nonly 1 dataset (for recall - Accelerometer, while for precision - DIGEN39 5578).\\nThe results clearly demonstrate that the proposed new data abstraction methods\\nsignificantly outperform abstractions using ROC. Moreover, the values obtained from se-\\nlected metrics indicate that classification using SaNDA with a quantiles 20 abstraction\\nmethod can compete with Random Forest for complete datasets. Additionally, classifica-\\ntion with missing data clearly shows the advantage of the new data abstraction methods\\nover classical approach.\\n5. Summary and Conclusions\\nThe increasing research on data anonymisation, federated learning and encrypted\\ncomputing aims at increasing the data size and availability for deep learning, the driving\\nforce behind the recent developments and impact of ML. However, there are areas such\\nas healthcare, behavioural sciences and finances, where data, to support CDM remains\\nvery limited. Recently, the Small and Incomplete Dataset Analyser (SaNDA) [15] has\\nemerged as a promising solution to facilitate the adoption of ML in such areas due to its\\ncapability to use small and noisy datasets, while explaining the outcomes.\\nSaNDA can effectively tackle classification tasks even with substantial amounts of\\nmissing data, eliminating the need for data imputation or modelling, which can be chal-\\nlenging, costly and time-consuming in extremely low-quality datasets. One of its funda-\\nmental aspects is the use of abstractions, which involve column-wise transformations of\\nnumerical data into categorical representations.\\nIn this paper we describe experiments to compare constant width constant binning\\nand quantiles-based abstraction protocols as alternatives to the previously employed ROC\\n9\\ncurve-based method. Their influence on classification performance has been extensively\\nexamined and measured.\\nAs a general result, increasing the number of data abstractions consistently enhances\\nthe performance of the SaNDA algorithm, however, this results into some potential lim-\\nitations. Firstly, it was identified that splitting the data into many categories, results\\nin a poor statistical representation of the real population. Secondly, it reduces the ex-\\nplainability of the model due to the increase of the complexity of the KG representation,\\nbecoming useless for Critical Decision-Making. Lastly, a substantial increase on the time\\nrequired to create the classification model makes its usefulness questionable for the pro-\\nposed classification environments.\\nDue to these limitations, we decided to cap the number of abstractions to 20, regard-\\nless of the chosen abstraction method. This limit proved to be effective in significantly\\nimproving the balanced accuracy of SaNDA.\\nIn conclusion, the SaNDA classification process, built through the meticulous ex-\\nperiments of a chosen abstraction methodology, holds as a prospective alternative to\\nconventional explainable ML paradigms such as Random Forest. This assertion gains\\nparticular relevance in situations necessitating CDM within constraints of small datasets\\nand missing data. The proposed SaNDA enhanced classification processes demonstrates\\nits potential to yield robust and explainable outcomes, thereby presenting a viable option\\nin contexts where deep learning methodologies may encounter limitations.\\nA future direction of research may involve the choice of an appropriate method and\\nlevel of abstraction. The abstraction method can be chosen to maximise accuracy while\\nmaintaining computation time efficiency for a specific dataset.\\nThe abstraction level\\ndictates how accurately it mirrors the intricacies of the problem domain and the extent to\\nwhich it simplifies and generalizes domain concepts. It can range from being very detailed\\nand specific to more generalized and fundamental, depending on the intended scope and\\npurpose of the domain problems. Adopting a dynamic approach to selecting methods\\nand determining abstraction levels can enhance the model’s versatility. This adaptability\\nwould allow for easy customization to meet specific problem and user requirements.\\nAcknowledgements\\nThis research has been supported by the European Union’s Horizon 2020 research\\nand innovation programme under grant agreement Sano No 857533. This publication\\nis supported by Sano project carried out within the International Research Agendas\\nprogramme of the Foundation for Polish Science MAB PLUS/2019/13, co-financed by\\nthe European Union under the European Regional Development Fund. This research\\nwas supported in part by PLGrid Infrastructure.\\nReferences\\n[1] Connectionist Bench (Sonar, Mines vs. Rocks). UCI Machine Learning Repository.\\n[2] Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy\\nphysics with deep learning. Nature communications, 5(1):1–9, 2014.\\n[3] Vaishak Bella and Ioannis Papantonis. Principles and practice of explainable machine learning.\\nFrontiers in Big Data, 4:688969, 2021.\\n[4] Leo Breiman. Random forests. Mach. Learn., 45(1):5–32, 2001.\\n[5] Varsha Chiruvella and Achuta Kumar Guddati. Ethical issues in patient data ownership. Interact\\nJ Med Res., 10(2):e22269, 2021.\\n10\\n[6] Derek Doran, Sarah Schulz, and Tarek R Besold. What Does Explainable AI Really Mean? A New\\nConceptualization of Perspectives. arXiv, 2017.\\n[7] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.\\n[8] Ajith Abraham Aswani Kumar Cherukuri Patricia Melin Niketa Gandhi. Intelligent Systems Design\\nand Applications, 18th International Conference on Intelligent Systems Design and Applications\\n(ISDA 2018) held in Vellore, India, December 6-8, 2018, Volume 1. 2020.\\n[9] Luca Gherardini, Varun Ravi Varma, Karol Capa la, Roger Woods, and Jose Sousa.\\nCactus:\\na comprehensive abstraction and classification tool for uncovering structures.\\narXiv preprint\\narXiv:2308.12031, 2023.\\n[10] Mitchell Gill, Robyn Anderson, and Haifei et al. Hu. Machine learning models outperform deep\\nlearning models, provide interpretation and facilitate feature selection for soybean trait prediction.\\nBMC Plant Biol, 22:180, 2022.\\n[11] L´eo Grinsztajn, Edouard Oyallon, and Ga¨el Varoquaux. Why do tree-based models still outperform\\ndeep learning on typical tabular data? Advances in Neural Information Processing Systems, 35:507–\\n520, 2022.\\n[12] Vikas Hassija, Vinay Chamola, Atmesh Mahapatra, Abhinandan Singal, Divyansh Goel, Kaizhu\\nHuang, Simone Scardapane, Indro Spinelli, Mufti Mahmud, and Amir Hussain. Interpreting Black-\\nBox Models: A Review on Explainable Artificial Intelligence. Cognitive Computation, pages 1–30,\\n2023.\\n[13] Steven Hicks, Inga Str¨umke, and Vajira et al. Thambawita. On evaluation metrics for medical\\napplications of artificial intelligence. Sci Rep, 12:5979, 2022.\\n[14] Mohammad Hosseini, Micha l Wieczorek, and Bert Gordijn. Ethical issues in social science research\\nemploying big data. Sci Eng Ethics, 28:29, 2022.\\n[15] Alfredo Ibias, Varun Ravi Varma, Karol Capa la, Luca Gherardini, and Jose Sousa. Sanda: A small\\nand incomplete dataset analyser. Information Sciences, 640:119078, 2023.\\n[16] Wei Jin, Tyler Derr, Haochen Liu, Yigi Wang, Suhang Wang, Zitao Liu, and Jiliang Tang. Self-\\nsupervised learning on graphs: deep insights and new direction. arXiv:2006.10141, 2020.\\n[17] Suvarna Kadam and Vinay Vaidya. Review and analysis of zero, one and few shot learning ap-\\nproaches. In Ajith Abraham, Aswani Kumar Cherukuri, Patricia Melin, and Niketa Gandhi, editors,\\nIntelligent Systems Design and Applications, pages 100–112, Cham, 2020. Springer International\\nPublishing.\\n[18] Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks.\\narXiv preprint arXiv:1506.02078, 2015.\\n[19] Alex John London. Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Ex-\\nplainability. Hastings Center Report, 49(1):15–21, 2019.\\n[20] Alexandra L’Heureux, Katarina Grolinger, Hany F Elyamany, and Miriam AM Capretz. Machine\\nlearning with big data: Challenges and approaches. IEEE Access, 5:7776–7797, 2017.\\n[21] Blake Murdoch. Privacy and artificial intelligence: challenges for protecting health information in\\na new era. BMC Medical Ethics, 11:122, 2021.\\n[22] Rtayli Naoufal and Enneya Nourddine. Enhanced credit card fraud detection based on svm-recursive\\nfeature elimination and hyper-parameters optimization. Journal of Information Security and Ap-\\nplications, 55:102596, 2020.\\n[23] Patryk Orzechowski and Jason H Moore. Generative and reproducible benchmarks for comprehen-\\nsive evaluation of machine learning classifiers. Science Advances, 8(47):eabl4747, 2022.\\n[24] Uwe Peters. Explainable AI lacks regulative reasons: why AI and human decision-making are not\\nequally opaque. AI and Ethics, 3(3):963–974, 2023.\\n[25] Shukor Abd Razak, Nur Hafizah Mohd Nazari, and Arafat Al-Dhaqm. Data Anonymization Using\\nPseudonym System to Preserve Data Privacy. IEEE Access, 8:43256–43264, 2020.\\n[26] Muhammad Imran Razzak, Muhammad Imran, and Guandong Xu. Big data analytics for preventive\\nmedicine. Neural Comput & Applic, 32:4417–4451, 2020.\\n[27] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should i trust you?” Explaining\\nthe predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference\\non knowledge discovery and data mining, pages 1135–1144, 2016.\\n[28] Bukhoree Sahoh and Anant Choksuriwong. The role of explainable Artificial Intelligence in high-\\nstakes decision-making systems: a systematic review. Journal of Ambient Intelligence and Human-\\nized Computing, 14(6):7827–7843, 2023.\\n[29] Scalabrini Sampaio, Vallim Filho, Santos da Silva, and Augusto da Silva. Prediction of motor failure\\n11\\ntime using an artificial neural network. Sensors, 19(19):4342, October 2019.\\n[30] Silvia Seoni, Vicnesh Jahmunah, Massimo Salvi, Prabal Datta Barua, Filippo Molinari, and U. Ra-\\njendra Acharya. Application of uncertainty quantification to artificial intelligence in healthcare: A\\nreview of last decade (2013–2023). Comput. Biol. Med., 165(C), jan 2024.\\n[31] Fadi Shehab Shiyyab, Abdallah Bader Alzoubi, Mr. Qais Obidat, and Hashem Alshurafat. The\\nimpact of artificial intelligence disclosure on financial performance. Int. J. Financial Stud., 11:115,\\n2023.\\n[32] Vincent Sigillito, Simon Wing, Larrie Hutton, and Kile Baker. Ionosphere. UCI Machine Learning\\nRepository, 1989.\\n[33] Xiaoqiang Sun, Peng Zhang, Joseph K. Liu, Jianping Yu, and Weixin Xie. Private Machine Learning\\nClassification Based on Fully Homomorphic Encryption. IEEE Transactions on Emerging Topics\\nin Computing, 8(2):352–364, 2017.\\n[34] Gaia Tavoni, Takahiro Doi, Chris Pizzica, Vijay Balasubramanian, and Joshua I. Gold. Human\\ninference reflects a normative balance of complexity and accuracy.\\nNature Human Behaviour,\\n6(8):1153–1168, 2022.\\n[35] Patrick Weber, K.Valerie Carl, and Oliver Hinz. Applications of explainable artificial intelligence\\nin finance—a systematic review of finance, information systems, and computer science literature.\\nManag Rev Q, 2023.\\n[36] William Wolberg, Olvi Mangasarian, W. Street, and Nick Street. Breast Cancer Wisconsin (Diag-\\nnostic). UCI Machine Learning Repository, 1995.\\n[37] Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao. A survey on federated learning.\\nKnowledge-Based Systems, 216:106775, 2021.\\n12\\ndataset\\nquantiles 20\\ndeciles\\nROC\\n10 bins\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.681\\n0.633\\n0.564\\n0.601\\n0.657\\n0.686\\nDIGEN24 2433\\n0.682\\n0.597\\n0.597\\n0.635\\n0.690\\n0.594\\nDIGEN39 5578\\n0.674\\n0.622\\n0.601\\n0.603\\n0.661\\n0.757\\nDIGEN17 6949\\n0.623\\n0.602\\n0.593\\n0.615\\n0.633\\n0.483\\nDIGEN32 5191\\n0.665\\n0.602\\n0.568\\n0.605\\n0.658\\n0.447\\nDIGEN10 8322\\n0.682\\n0.611\\n0.577\\n0.620\\n0.657\\n0.627\\nDIGEN22 2433\\n0.668\\n0.603\\n0.563\\n0.611\\n0.672\\n0.645\\nDIGEN8 4426\\n0.675\\n0.623\\n0.589\\n0.648\\n0.687\\n0.656\\nDIGEN35 4426\\n0.664\\n0.627\\n0.560\\n0.611\\n0.663\\n0.640\\nDIGEN36 466\\n0.687\\n0.629\\n0.584\\n0.630\\n0.660\\n0.628\\nAccelerometer\\n0.718\\n0.713\\n0.676\\n0.638\\n0.678\\n0.889\\nIonosphere\\n0.940\\n0.924\\n0.911\\n0.950\\n0.957\\n0.936\\nWisconsin Breast Cancer\\n0.969\\n0.952\\n0.948\\n0.948\\n0.953\\n0.958\\nSonar\\n0.981\\n0.906\\n0.847\\n0.923\\n0.986\\n0.832\\nHIGGS\\n0.663\\n0.658\\n0.624\\n0.609\\n0.640\\n0.666\\nTable 1: Balanced accuracy (BA) for selected SaNDA classification methods and Random Forest for all\\nchosen datasets with 0% missing values\\n13\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.678\\n0.58\\n0.664\\n0.647\\nDIGEN24 2433\\n0.692\\n0.593\\n0.686\\n0.611\\nDIGEN39 5578\\n0.679\\n0.601\\n0.651\\n0.657\\nDIGEN17 6949\\n0.648\\n0.584\\n0.632\\n0.553\\nDIGEN32 5191\\n0.669\\n0.564\\n0.658\\n0.555\\nDIGEN10 8322\\n0.685\\n0.574\\n0.663\\n0.654\\nDIGEN22 2433\\n0.658\\n0.581\\n0.673\\n0.625\\nDIGEN8 4426\\n0.683\\n0.592\\n0.683\\n0.663\\nDIGEN35 4426\\n0.667\\n0.57\\n0.664\\n0.596\\nDIGEN36 466\\n0.685\\n0.574\\n0.667\\n0.571\\nAccelerometer\\n0.718\\n0.676\\n0.678\\n0.885\\nIonosphere\\n0.940\\n0.911\\n0.959\\n0.941\\nWisconsin Breast Cancer\\n0.970\\n0.946\\n0.953\\n0.958\\nSonar\\n0.981\\n0.866\\n0.896\\n0.793\\nHIGGS\\n0.663\\n0.624\\n0.639\\n0.661\\nTable 2: Balanced accuracy (BA) for selected SaNDA classification methods and Random Forest for all\\nchosen datasets with 1% missing values\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.682\\n0.566\\n0.670\\n0.541\\nDIGEN24 2433\\n0.678\\n0.580\\n0.666\\n0.687\\nDIGEN39 5578\\n0.690\\n0.589\\n0.680\\n0.610\\nDIGEN17 6949\\n0.661\\n0.588\\n0.677\\n0.618\\nDIGEN32 5191\\n0.680\\n0.593\\n0.667\\n0.689\\nDIGEN10 8322\\n0.669\\n0.588\\n0.658\\n0.572\\nDIGEN22 2433\\n0.646\\n0.578\\n0.655\\n0.572\\nDIGEN8 4426\\n0.682\\n0.588\\n0.657\\n0.693\\nDIGEN35 4426\\n0.672\\n0.575\\n0.653\\n0.594\\nDIGEN36 466\\n0.684\\n0.580\\n0.679\\n0.626\\nAccelerometer\\n0.716\\n0.673\\n0.693\\n0.869\\nIonosphere\\n0.936\\n0.901\\n0.967\\n0.941\\nWisconsin Breast Cancer\\n0.969\\n0.952\\n0.953\\n0.942\\nSonar\\n0.981\\n0.853\\n0.981\\n0.818\\nHIGGS\\n0.661\\n0.620\\n0.637\\n0.614\\nTable 3: Balanced accuracy (BA) for selected SaNDA classification methods and Random Forest for all\\nchosen datasets with 5% missing values\\n14\\ndataset\\n20 deciles\\nroc\\nstatic 20\\nRandom Forest\\nDIGEN23 5191\\n0.678\\n0.593\\n0.652\\n0.633\\nDIGEN24 2433\\n0.694\\n0.587\\n0.677\\n0.562\\nDIGEN39 5578\\n0.665\\n0.591\\n0.679\\n0.637\\nDIGEN17 6949\\n0.654\\n0.591\\n0.657\\n0.657\\nDIGEN32 5191\\n0.659\\n0.563\\n0.655\\n0.508\\nDIGEN10 8322\\n0.675\\n0.583\\n0.669\\n0.594\\nDIGEN22 2433\\n0.674\\n0.577\\n0.667\\n0.594\\nDIGEN8 4426\\n0.667\\n0.575\\n0.682\\n0.694\\nDIGEN35 4426\\n0.678\\n0.563\\n0.665\\n0.660\\nDIGEN36 466\\n0.678\\n0.565\\n0.657\\n0.556\\nAccelerometer\\n0.715\\n0.668\\n0.671\\n0.848\\nIonosphere\\n0.942\\n0.892\\n0.947\\n0.936\\nWisconsin Breast Cancer\\n0.964\\n0.954\\n0.958\\n0.962\\nSonar\\n0.975\\n0.871\\n0.991\\n0.700\\nHIGGS\\n0.657\\n0.618\\n0.635\\n0.622\\nTable 4: Balanced accuracy (BA) for selected SaNDA classification methods and Random Forest for all\\nchosen datasets with 10% missing values\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.656\\n0.555\\n0.646\\n0.490\\nDIGEN24 2433\\n0.671\\n0.618\\n0.656\\n0.541\\nDIGEN39 5578\\n0.648\\n0.569\\n0.665\\n0.493\\nDIGEN17 6949\\n0.680\\n0.617\\n0.689\\n0.600\\nDIGEN32 5191\\n0.687\\n0.607\\n0.679\\n0.526\\nDIGEN10 8322\\n0.648\\n0.589\\n0.669\\n0.491\\nDIGEN22 2433\\n0.679\\n0.575\\n0.640\\n0.513\\nDIGEN8 4426\\n0.662\\n0.579\\n0.679\\n0.524\\nDIGEN35 4426\\n0.684\\n0.569\\n0.669\\n0.496\\nDIGEN36 466\\n0.679\\n0.622\\n0.640\\n0.520\\nAccelerometer\\n0.678\\n0.615\\n0.607\\n0.711\\nIonosphere\\n0.944\\n0.900\\n0.947\\n0.796\\nWisconsin Breast Cancer\\n0.959\\n0.943\\n0.952\\n0.909\\nSonar\\n0.990\\n0.911\\n0.995\\n0.718\\nHIGGS\\n0.627\\n0.593\\n0.606\\n0.517\\nTable 5: Balanced accuracy (BA) for selected SaNDA classification methods and Random Forest for all\\nchosen datasets with 50% missing values\\n15\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.681\\n0.567\\n0.659\\n0.684\\nDIGEN24 2433\\n0.683\\n0.600\\n0.689\\n0.569\\nDIGEN39 5578\\n0.673\\n0.599\\n0.656\\n0.760\\nDIGEN17 6949\\n0.621\\n0.600\\n0.639\\n0.530\\nDIGEN32 5191\\n0.662\\n0.570\\n0.657\\n0.473\\nDIGEN10 8322\\n0.680\\n0.575\\n0.661\\n0.639\\nDIGEN22 2433\\n0.671\\n0.566\\n0.663\\n0.679\\nDIGEN8 4426\\n0.672\\n0.590\\n0.691\\n0.654\\nDIGEN35 4426\\n0.663\\n0.561\\n0.664\\n0.658\\nDIGEN36 466\\n0.683\\n0.588\\n0.659\\n0.652\\nAccelerometer\\n0.820\\n0.749\\n0.940\\n0.890\\nIonosphere\\n0.944\\n0.938\\n0.969\\n1.000\\nWisconsin Breast Cancer\\n0.958\\n0.922\\n0.961\\n0.972\\nSonar\\n0.970\\n0.901\\n0.980\\n0.821\\nHIGGS\\n0.636\\n0.615\\n0.630\\n0.683\\nTable 6: Precision for selected SaNDA classification methods and Random Forest for all chosen datasets\\nwith 0% missing values\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.679\\n0.586\\n0.671\\n0.664\\nDIGEN24 2433\\n0.690\\n0.594\\n0.683\\n0.584\\nDIGEN39 5578\\n0.677\\n0.602\\n0.645\\n0.652\\nDIGEN17 6949\\n0.649\\n0.585\\n0.640\\n0.601\\nDIGEN32 5191\\n0.665\\n0.560\\n0.654\\n0.595\\nDIGEN10 8322\\n0.687\\n0.571\\n0.668\\n0.671\\nDIGEN22 2433\\n0.659\\n0.581\\n0.669\\n0.652\\nDIGEN8 4426\\n0.680\\n0.593\\n0.686\\n0.660\\nDIGEN35 4426\\n0.662\\n0.568\\n0.657\\n0.602\\nDIGEN36 466\\n0.681\\n0.577\\n0.666\\n0.593\\nAccelerometer\\n0.819\\n0.750\\n0.940\\n0.885\\nIonosphere\\n0.944\\n0.938\\n0.969\\n0.972\\nWisconsin Breast Cancer\\n0.962\\n0.917\\n0.961\\n0.972\\nSonar\\n0.970\\n0.907\\n0.980\\n0.824\\nHIGGS\\n0.637\\n0.615\\n0.630\\n0.673\\nTable 7: Precision for selected SaNDA classification methods and Random Forest for all chosen datasets\\nwith 1% missing values\\n16\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.665\\n0.570\\n0.650\\n0.609\\nDIGEN24 2433\\n0.676\\n0.580\\n0.677\\n0.610\\nDIGEN39 5578\\n0.687\\n0.587\\n0.658\\n0.686\\nDIGEN17 6949\\n0.643\\n0.579\\n0.659\\n0.532\\nDIGEN32 5191\\n0.665\\n0.562\\n0.661\\n0.564\\nDIGEN10 8322\\n0.697\\n0.593\\n0.683\\n0.625\\nDIGEN22 2433\\n0.663\\n0.596\\n0.681\\n0.647\\nDIGEN8 4426\\n0.618\\n0.589\\n0.665\\n0.688\\nDIGEN35 4426\\n0.684\\n0.585\\n0.674\\n0.700\\nDIGEN36 466\\n0.665\\n0.588\\n0.656\\n0.600\\nAccelerometer\\n0.816\\n0.754\\n0.833\\n0.870\\nIonosphere\\n0.937\\n0.925\\n0.978\\n0.972\\nWisconsin Breast Cancer\\n0.958\\n0.926\\n0.952\\n0.954\\nSonar\\n0.970\\n0.876\\n0.970\\n0.897\\nHIGGS\\n0.635\\n0.607\\n0.627\\n0.642\\nTable 8: Precision for selected SaNDA classification methods and Random Forest for all chosen datasets\\nwith 5% missing values\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.651\\n0.549\\n0.643\\n0.497\\nDIGEN24 2433\\n0.656\\n0.605\\n0.648\\n0.516\\nDIGEN39 5578\\n0.632\\n0.553\\n0.642\\n0.645\\nDIGEN17 6949\\n0.667\\n0.606\\n0.674\\n0.469\\nDIGEN32 5191\\n0.690\\n0.625\\n0.694\\n0.546\\nDIGEN10 8322\\n0.651\\n0.599\\n0.671\\n0.529\\nDIGEN22 2433\\n0.704\\n0.604\\n0.661\\n0.513\\nDIGEN8 4426\\n0.646\\n0.565\\n0.656\\n0.517\\nDIGEN35 4426\\n0.692\\n0.583\\n0.680\\n0.539\\nDIGEN36 466\\n0.657\\n0.595\\n0.616\\n0.555\\nAccelerometer\\n0.740\\n0.689\\n0.698\\n0.678\\nIonosphere\\n0.945\\n0.932\\n0.956\\n0.839\\nWisconsin Breast Cancer\\n0.927\\n0.901\\n0.935\\n0.927\\nSonar\\n0.990\\n0.934\\n0.990\\n0.725\\nHIGGS\\n0.602\\n0.575\\n0.589\\n0.310\\nTable 9: Precision for selected SaNDA classification methods and Random Forest for all chosen datasets\\nwith 10% missing values\\n17\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.651\\n0.549\\n0.643\\n0.497\\nDIGEN24 2433\\n0.656\\n0.605\\n0.648\\n0.516\\nDIGEN39 5578\\n0.632\\n0.553\\n0.642\\n0.645\\nDIGEN17 6949\\n0.667\\n0.606\\n0.674\\n0.469\\nDIGEN32 5191\\n0.690\\n0.625\\n0.694\\n0.546\\nDIGEN10 8322\\n0.651\\n0.599\\n0.671\\n0.529\\nDIGEN22 2433\\n0.704\\n0.604\\n0.661\\n0.513\\nDIGEN8 4426\\n0.646\\n0.565\\n0.656\\n0.517\\nDIGEN35 4426\\n0.692\\n0.583\\n0.680\\n0.539\\nDIGEN36 466\\n0.657\\n0.595\\n0.616\\n0.555\\nAccelerometer\\n0.740\\n0.689\\n0.698\\n0.678\\nIonosphere\\n0.945\\n0.932\\n0.956\\n0.839\\nWisconsin Breast Cancer\\n0.927\\n0.901\\n0.935\\n0.927\\nSonar\\n0.990\\n0.934\\n0.990\\n0.725\\nHIGGS\\n0.602\\n0.575\\n0.589\\n0.310\\nTable 10: Precision for selected SaNDA classification methods and Random Forest for all chosen datasets\\nwith 50% missing values\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.680\\n0.544\\n0.652\\n0.711\\nDIGEN24 2433\\n0.678\\n0.584\\n0.692\\n0.608\\nDIGEN39 5578\\n0.676\\n0.612\\n0.678\\n0.755\\nDIGEN17 6949\\n0.630\\n0.560\\n0.612\\n0.488\\nDIGEN32 5191\\n0.674\\n0.556\\n0.662\\n0.449\\nDIGEN10 8322\\n0.688\\n0.590\\n0.644\\n0.614\\nDIGEN22 2433\\n0.658\\n0.538\\n0.698\\n0.587\\nDIGEN8 4426\\n0.684\\n0.582\\n0.676\\n0.693\\nDIGEN35 4426\\n0.666\\n0.554\\n0.660\\n0.623\\nDIGEN36 466\\n0.698\\n0.560\\n0.662\\n0.594\\nAccelerometer\\n0.558\\n0.530\\n0.380\\n0.888\\nIonosphere\\n0.982\\n0.933\\n0.969\\n0.872\\nWisconsin Breast Cancer\\n0.962\\n0.943\\n0.929\\n0.963\\nSonar\\n0.990\\n0.784\\n0.990\\n0.914\\nHIGGS\\n0.664\\n0.564\\n0.586\\n0.697\\nTable 11: Recall for selected SaNDA classification methods and Random Forest for all chosen datasets\\nwith 0% missing values\\n18\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.674\\n0.546\\n0.644\\n0.612\\nDIGEN24 2433\\n0.696\\n0.586\\n0.694\\n0.629\\nDIGEN39 5578\\n0.684\\n0.594\\n0.670\\n0.682\\nDIGEN17 6949\\n0.644\\n0.576\\n0.602\\n0.524\\nDIGEN32 5191\\n0.682\\n0.594\\n0.670\\n0.456\\nDIGEN10 8322\\n0.680\\n0.598\\n0.648\\n0.627\\nDIGEN22 2433\\n0.654\\n0.578\\n0.686\\n0.581\\nDIGEN8 4426\\n0.690\\n0.586\\n0.674\\n0.699\\nDIGEN35 4426\\n0.684\\n0.582\\n0.686\\n0.630\\nDIGEN36 466\\n0.696\\n0.554\\n0.670\\n0.535\\nAccelerometer\\n0.558\\n0.527\\n0.379\\n0.884\\nIonosphere\\n0.982\\n0.933\\n0.973\\n0.897\\nWisconsin Breast Cancer\\n0.962\\n0.943\\n0.929\\n0.963\\nSonar\\n0.990\\n0.804\\n0.990\\n0.800\\nHIGGS\\n0.663\\n0.564\\n0.586\\n0.693\\nTable 12: Recall for selected SaNDA classification methods and Random Forest for all chosen datasets\\nwith 1% missing values\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.692\\n0.608\\n0.664\\n0.553\\nDIGEN24 2433\\n0.706\\n0.580\\n0.686\\n0.601\\nDIGEN39 5578\\n0.668\\n0.596\\n0.654\\n0.722\\nDIGEN17 6949\\n0.658\\n0.570\\n0.642\\n0.451\\nDIGEN32 5191\\n0.690\\n0.596\\n0.698\\n0.582\\nDIGEN10 8322\\n0.672\\n0.568\\n0.672\\n0.588\\nDIGEN22 2433\\n0.656\\n0.544\\n0.666\\n0.568\\nDIGEN8 4426\\n0.704\\n0.618\\n0.674\\n0.719\\nDIGEN35 4426\\n0.662\\n0.548\\n0.644\\n0.682\\nDIGEN36 466\\n0.682\\n0.586\\n0.666\\n0.503\\nAccelerometer\\n0.558\\n0.513\\n0.484\\n0.868\\nIonosphere\\n0.991\\n0.938\\n0.973\\n0.897\\nWisconsin Breast Cancer\\n0.962\\n0.948\\n0.934\\n0.963\\nSonar\\n0.981\\n0.804\\n0.990\\n0.743\\nHIGGS\\n0.663\\n0.568\\n0.586\\n0.650\\nTable 13: Recall for selected SaNDA classification methods and Random Forest for all chosen datasets\\nwith 5% missing values\\n19\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.688\\n0.598\\n0.654\\n0.645\\nDIGEN24 2433\\n0.706\\n0.600\\n0.696\\n0.524\\nDIGEN39 5578\\n0.660\\n0.578\\n0.696\\n0.636\\nDIGEN17 6949\\n0.622\\n0.510\\n0.600\\n0.457\\nDIGEN32 5191\\n0.668\\n0.538\\n0.632\\n0.500\\nDIGEN10 8322\\n0.704\\n0.572\\n0.664\\n0.575\\nDIGEN22 2433\\n0.664\\n0.548\\n0.642\\n0.677\\nDIGEN8 4426\\n0.668\\n0.608\\n0.718\\n0.634\\nDIGEN35 4426\\n0.660\\n0.542\\n0.646\\n0.656\\nDIGEN36 466\\n0.666\\n0.572\\n0.668\\n0.581\\nAccelerometer\\n0.559\\n0.498\\n0.367\\n0.845\\nIonosphere\\n0.996\\n0.902\\n0.964\\n0.872\\nWisconsin Breast Cancer\\n0.953\\n0.957\\n0.939\\n0.972\\nSonar\\n0.959\\n0.825\\n1.000\\n0.686\\nHIGGS\\n0.659\\n0.569\\n0.586\\n0.597\\nTable 14: Recall for selected SaNDA classification methods and Random Forest for all chosen datasets\\nwith 10% missing values\\ndataset\\nquantiles 20\\nROC\\n20 bins\\nRandom Forest\\nDIGEN23 5191\\n0.674\\n0.612\\n0.658\\n0.480\\nDIGEN24 2433\\n0.720\\n0.682\\n0.682\\n0.552\\nDIGEN39 5578\\n0.710\\n0.720\\n0.744\\n0.562\\nDIGEN17 6949\\n0.718\\n0.668\\n0.732\\n0.573\\nDIGEN32 5191\\n0.678\\n0.536\\n0.640\\n0.409\\nDIGEN10 8322\\n0.638\\n0.540\\n0.664\\n0.536\\nDIGEN22 2433\\n0.618\\n0.434\\n0.574\\n0.510\\nDIGEN8 4426\\n0.718\\n0.686\\n0.754\\n0.497\\nDIGEN35 4426\\n0.662\\n0.484\\n0.638\\n0.494\\nDIGEN36 466\\n0.748\\n0.766\\n0.742\\n0.541\\nAccelerometer\\n0.548\\n0.418\\n0.377\\n0.804\\nIonosphere\\n0.991\\n0.920\\n0.973\\n0.667\\nWisconsin Breast Cancer\\n0.962\\n0.948\\n0.943\\n0.944\\nSonar\\n0.990\\n0.876\\n0.990\\n0.829\\nHIGGS\\n0.600\\n0.547\\n0.559\\n0.161\\nTable 15: Recall for selected SaNDA classification methods and Random Forest for all chosen datasets\\nwith 50% missing values\\n20\\nFigure 1: Balanced accuracy (BA) of SaNDA classification method as a function the different abstraction\\nmethods.\\n21\\nFigure 2: Balanced accuracy (BA) as a function of percentage of missing data for SaDNA with two best\\nperforming abstraction methods (order quantiles 20 and 20 bins), SaNDA with ROC curve and Random\\nForest.\\n22\\nFigure 3: Recall as a function of percentage of missing data for SaDNA with two best performing\\nabstraction methods (order quantiles 20 and 20 bins), SaNDA with ROC curve and Random Forest.\\n23\\nFigure 4: Precision as a function of percentage of missing data for SaDNA with two best performing\\nabstraction methods (order quantiles 20 and 20 bins), SaNDA with ROC curve and Random Forest.\\n24\\n'},\n",
       " {'abstract': 'This paper presents a study investigating human cognitive augmentation due to using ChatGPT in two experiments and concludes that using ChatGPT does not always enhance cognitive ability or replace human judgment and discernment.',\n",
       "  'introduction': 'The use of tools enhances human cognitive performance, with calculators and spreadsheets improving mathematical calculations and software aiding in processing words, images, video, and numbers. However, unsupervised, deep, machine learning techniques have led to cognitive systems (cogs) outperforming humans in certain domains. These cogs can be utilized in a human/cog ensemble, leading to human cognitive augmentation. Specifically, the effect of ChatGPT on human cognitive augmentation is explored in this paper.',\n",
       "  'literature_review': \"Cognitive systems (cogs) have been designed using artificial intelligence (AI) and cognitive systems (cogs), allowing humans and cogs to collaborate in human/cog ensembles, enhancing human cognitive performance. Cogs outperform humans in various domains, such as detecting lung cancers better than human doctors and diagnosing childhood depression better than humans. Cognitive augmentation depends on the cog's sophistication, the amount of cognitive work it performs, and the human's level of collaboration. ChatGPT, a large language model that mimics human-created text, is used by millions of people for various tasks.\",\n",
       "  'methodology': 'The authors conducted two experiments to compare human cognitive performance with and without using ChatGPT. In the first experiment, one group of students was assisted by ChatGPT, while the other was not, to synthesize innovative solutions to a problem statement. The second experiment involved retirement planning, with one group receiving ChatGPT assistance and the other prohibited from using ChatGPT. The responses were evaluated for expert-level quality.',\n",
       "  'results': \"For the innovation challenge, both groups predominantly suggested changing the field rather than the skeet, showing that ChatGPT did not alter the type of solutions generated. Interestingly, students using ChatGPT suggested ideas unrelated to the problem statement, potentially due to ChatGPT's response being driven by word association. For the retirement decision challenge, while more students using ChatGPT provided expert-level answers than those not using ChatGPT, the difference was not definitive. Furthermore, if non-ChatGPT students were disallowed from using a retirement calculator, ChatGPT students might have performed better.\",\n",
       "  'conclusion': 'Although ChatGPT is capable of producing expert-level responses, it does not guarantee it. Cognitive processes requiring high-level analysis, understanding, evaluation, and judgment are more difficult for ChatGPT to handle. ChatGPT may relieve humans of lower-level cognitive tasks, leading to significant cognitive augmentation in the future.',\n",
       "  'title': 'Does Using ChatGPT Result in Human Cognitive Augmentation?',\n",
       "  'author': 'Ron Fulbright, Miranda Morrison',\n",
       "  'textdata': \"Does Using ChatGPT Result in Human Cognitive Augmentation? \\n \\nRon Fulbright1, Miranda Morrison2 \\nUniversity of South Carolina Upstate \\n800 University Way, Spartanburg, SC USA 29303 \\n1fulbrigh@uscupstate.edu \\n2morrisme@email.uscupstate.edu  \\n \\n \\nAbstract. Human cognitive performance is enhanced by the use of tools. For example, \\na human can produce a much greater, and more accurate, volume of mathematical \\ncalculation in a unit of time using a calculator or a spreadsheet application on a \\ncomputer. Such tools have taken over the burden of lower-level cognitive “grunt \\nwork” but the human still serves the role of the expert performing higher-level \\nthinking and reasoning. Recently, however, unsupervised, deep, machine learning has \\nproduced cognitive systems able to outperform humans in several domains. When \\nhumans use these tools in a human/cog ensemble, the human’s cognitive ability is \\naugmented. In some cases, even non-experts can achieve, and even exceed, the \\nperformance of experts in a particular domain—synthetic expertise. A new cognitive \\nsystem, ChatGPT, has burst onto the scene during the past year. This paper \\ninvestigates human cognitive augmentation due to using ChatGPT by presenting the \\nresults of two experiments comparing responses created using ChatGPT with results \\ncreated not using ChatGPT. We find using ChatGPT does not always result in \\ncognitive augmentation and does not yet replace human judgement, discernment, and \\nevaluation in certain types of tasks. In fact, ChatGPT was observed to result in \\nmisleading users resulting in negative cognitive augmentation. \\n \\nKeywords:  human cognitive augmentation, cognitive systems, human/cog \\nensembles \\n \\n \\n \\n1 \\nIntroduction  \\nHuman performance of any kind is augmented by the use of tools. Physical performance is \\nenhanced by using simple tools like hammers, shovels, and axes. Likewise, human \\ncognitive performance is augmented by the use of tools able to process and transform \\ninformation. For example, unaided, a human might take several minutes to add a column \\nof numbers, and then the sum would need to be checked because of the possibility of error \\nin the calculations. However, using a calculator or a computer spreadsheet, a human could \\nproduce a reliable sum in a fraction of a second. In fact, entry of the numbers becomes the \\nlimiting factor in terms of speed. Today, we commonly use software able to process words, \\nimages, video, and numbers to perform our volume of daily work. Such tools have taken \\nover the burden of lower-level cognitive “grunt work.” So far, though, the human still \\nserves the role of the expert and performs the high-level thinking.  \\nRecently, however, cognitive systems technology (“AIs”) built using unsupervised, \\ndeep, machine learning techniques, has produced tools able to outperform humans in \\nseveral domains formerly thought to be possible only as the result of high-level human \\ncognitive processing. We call such systems “cogs.” When humans use tools like these in a \\ncollaborative manner (a human/cog ensemble) human cognitive performance is \\nenhanced—augmented. If a human’s cognitive ability is augmented enough, even a non-\\nexpert can achieve, and even exceed, the performance of an expert in a particular domain, \\nsomething called synthetic expertise. So far, though, such cognitive systems are narrow in \\ntheir applicability. Even though they outperform humans, they are limited to just that \\ndomain. \\nThings are changing. Systems like the new Chat Generative Pre-Trained Transformer \\n(ChatGPT), have gained much attention recently. ChatGPT is a large language model \\ntrained to predict the most probable next word in a sequence of words and is fine-tuned for \\nconversational usage. ChatGPT mimics human-created text. Instead of being limited to a \\nnarrow domain, users can conduct extended textual dialogs with ChatGPT on practically \\nany topic and most of the time, text generated by ChatGPT is indistinguishable from text \\nproduced by another human. Every day, millions of people use ChatGPT for assistance in \\nlearning, researching, getting advice, writing music, poetry, and prose, generating \\ncomputer program code, and much more. A person using ChatGPT certainly fits our \\ndefinition of a human/cog ensemble. Accordingly, the hypothesis explored in this paper is:  \\n \\nH1: In a human/cog ensemble consisting of a person using ChatGPT we should \\nbe able to observe, measure, and characterize human cognitive augmentation in \\nthe form of enhanced performance when performing a task.  \\n \\nTo investigate the hypothesis, we designed two experiments to compare human \\ncognitive performance with and without using ChatGPT. In one experiment we found a \\nperson using ChatGPT as a assistive tool was marginally better than a person not using \\nChatGPT but not enough for the result to be compelling. In the other experiment we found \\nusing ChatGPT had no effect on a person’s ability to perform the task and even misled \\nusers resulting in negative cognitive augmentation. \\n \\n \\n2 Previous Work \\n \\n2.1 Cognitive Systems \\nWith recent advances in artificial intelligence (AI) and cognitive systems (cogs), we are at \\nthe beginning of a new era in human history in which humans will work in partnership with \\nartificial entities capable of performing high-level cognition rivaling or surpassing human \\nability (Kelly & Hamm, 2013; Wladawsky-Berger, 2015; Gil, 2019; Fulbright, 2016a; \\n2016b; 2020). Already, there are artificial systems and algorithms outperforming humans \\nand achieving expert-level results.  \\nFor example, a deep-learning algorithm has learned to detect lung cancers better than \\nhuman doctors (Sandoiu, 2019). The algorithm outperforms humans in recognizing \\nproblem areas reducing false positives by 11% and false negatives by 5%.  \\nGoogle’s convolutional neural network, Inception v4, outperformed a group of 58 \\nhuman dermatologists using dermoscopic images and corresponding diagnoses of \\nmelanoma (Haenssle et al., 2018).  \\nIn the field of diabetic retinopathy, a study evaluated the diagnostic performance of a \\ncognitive system for the automated detection of diabetic retinopathy (DR) and Diabetic \\nMacular Edema (DME) (Abràmoff, et al., 2018). The cog exceeded all pre-specified \\nsuperiority goals.  \\nAt the University of California San Francisco and the University of California \\nBerkeley, an algorithm running on a convolutional neural network was better than experts \\nat finding tiny brain hemorrhages in scans of patients’ heads (Kurtzman, 2019). The cog \\nwas able to complete a diagnosis in only one second, something a human would take many \\nminutes to do.  \\nCognitive systems are already better than humans at diagnosing childhood depression \\n(Lavars, 2019), predicting mortality (Wehner, 2019), detecting valvular heart disease \\n(Stevens, 2023), and assessing cancerous tumors (Towers-Clark, 2019).  \\nNot only are cognitive systems able to outperform humans in some domains, they are \\nable to do things humans cannot. For example, the FIND FH machine learning model \\nanalyzed the clinical data of over 170 million people and discovered 1.3 million of them \\nwere previously undiagnosed as being likely to have familial hypercholesterolemia (Myers \\net al., 2019). Follow-on studies of the individual cases flagged by the cog have shown over \\n80% of the cases do in fact have a high enough clinical suspicion to warrant evaluation and \\ntreatment. This means on the order of 800,000 people could receive life-extending \\ntreatment who otherwise would not. \\nAn algorithm named Word2Vec sifted through some 3.3 million abstracts and \\ndiscovered associations previously unknown by human readers and predicted a new \\nthermoelectric material four years before it was discovered (Tshitoyan, 2019; Gregory, \\n2019).  \\n \\n \\n2.2 Cognitive Augmentation \\nWe can view data, information, knowledge, and wisdom (DIKW) as a hierarchy based on \\nrelative value (Ackoff, 1989). Each level is of a higher value than the level below it partly \\nbecause of the processing involved to produce the information stock at that level and partly \\ndue to the utility of the information stock at that level. Information is processed data, \\nknowledge is processed information, etc. Processing at each level can be modeled as a \\ncognitive process.  Data, information, or knowledge, generically referred to as information \\nstock, is input to the cognitive process. The cognitive process transforms the input and \\nproduces the higher-valued output. This transformation is accomplished by the expenditure \\nof a certain amount of cognitive work (W) (Fulbright, 2020). \\nIn a human/cog ensemble, a collaborative team consisting of one or more humans and \\none or more cognitive systems), cognitive processing of the entire ensemble is a mixture \\nof human cognitive processing (WH) and artificial cognitive processing (WC) (W* = WH + \\nWC)  as depicted in Fig. 1 (Fulbright, 2020; 2020a; Fulbright & Walters, 2020). \\n \\n \\nFig. 3. A Human/Cog ensemble performing a cognitive process. \\nA human working alone is able to achieve a certain amount of cognitive work. A \\nhuman aided by a cognitive system is able to achieve a greater amount of cognitive work. \\nWe call this increase in cognitive performance, cognitive augmentation (Fulbright, 2017; \\n2020). \\nThe amount of cognitive augmentation depends on how sophisticated the cognitive \\nsystem, how much of the total cognitive work it performs, and how well the human \\ncollaborates with the cognitive system. Throughout history, humans have created ever-\\nevolving technology to assist in cognitive processing. As these systems get more capable, \\nespecially now in the era of artificial intelligence and unsupervised deep machine learning, \\ncognitive augmentation will increase rapidly.   \\nDifferent Levels of Cognitive Augmentation have been defined ranging from no \\naugmentation at all (all human thinking) to fully artificial intelligence (no human thinking) \\nas shown in Fig. 2 (Fulbright, 2020; 2020a; Fulbright & Walters, 2020).  \\n \\n \\n \\n \\n \\n \\n \\nLevel 0:  No Augmentation \\n \\nthe human performs all cognitive processing \\n \\nLevel 1:  Assistive Tools \\n \\ne.g., abacus, calculators, software, etc. \\n \\nLevel 2:  Low-Level Cognition \\n \\npattern recognition, classification, speech \\n \\nhuman makes all high-level decisions \\n \\nLevel 3:  High-Level Cognition \\n \\nconcept understanding, critique \\n \\nconversational natural language \\n \\nLevel 4:  Creative Autonomy \\n \\nhuman-inspired/unsupervised synthesis \\n \\nLevel 5:  Artificial Intelligence \\n \\nno human cognitive processing \\n \\n \\nFig. 2. Levels of Cognitive Augmentation. \\n \\nIn previous work, we have conducted various experiments designed to measure and \\ncharacterize cognitive augmentation. Fulbright (2017; 2018) discusses several kinds of \\nmetrics and proposes several metrics to employ when measuring cognitive augmentation. \\nFulbright (2019) calculates cognitive augmentation for a given task finding cognitive \\naugmentation as high as 74% when people are provided different numbers of hints by a \\nsimulated cognitive system. Fulbright & McGaha (2023) shows how information of \\ndifferent types affects the level of cognitive augmentation when tasked with solving several \\ndifferent kinds of puzzles. In both of these studies, enhanced cognitive accuracy and \\ncognitive precision were measured.  \\nIn all three of these studies assistive information supplied to the human was simulated \\nand did not come from an actual cognitive system. However, ChatGPT represents a \\ncognitive system, already used by millions, with which to conduct experiments in cognitive \\naugmentation. There have been some notable studies comparing human performance to \\nChatGPT. \\n \\n \\n2.3 ChatGPT and Cognitive Augmentation \\nLi et al. (2023) compared the results of ChatGPT versus human performance on the \\nObjective Structured Clinical Examination (OSCE) in obstetrics and gynecology. ChatGPT \\nwas asked to answer discussion questions in seven key disciplines within obstetrics and \\ngynecology. ChatGPT outscored human test-takers in questions regarding postpartum \\nmanagement, urogynecology and pelvic floor problems, labor management, and post-\\noperative care. ChatGPT did not outperform humans in early pregnancy care, core surgical \\nskills, or gynecologic oncology. Li et al. (2023) theorized those question require multiple \\nanswers and higher-level reasoning.  \\nKung et al. (2023) found comparable results when administering the United States \\nMedical Licensing Examination (USMLE) to ChatGPT. ChatGPT beat the passing score \\nof 60% on most areas but narrowly failed to pass the multiple choice-question section \\n(59.1%) and the multiple-choice with forced justification section (52.4%) on Step 2CK of \\nthe exam. Step 2CK is typically administered to students who have successfully completed \\ntheir fourth year of medical school (Kung et al., 2023). \\nIn taking the American Board of Neurological Surgery Self-Assessment Examination \\n1, Ali et al. (2023) found ChatGPT 3.5 achieved a score of 73.4% and GPT-4 achieved a \\nscore of 83.4% relative to the human average of 72.8%. Both versions of ChatGPT \\nexceeded last year's passing threshold of 69%.  \\nLiéven et al. (2023) determined GPT-3.5 performed higher than the needed passing \\nscore on questions taken from the USMLE and MedMCQA examinations, however, GPT-\\n3.5 still underperformed on both examinations in comparison to humans.  \\nBrin et al. (2023), determined GPT-4 to have performed 10.77% better than human \\ntest-takers on multiple-choice questions from the USMLE involving soft skills, such as \\nempathy, leadership, emotional intelligence, and communication.  \\nSimilarly, Eloyseph et al. (2023) found ChatGPT to score 74.35% higher than males \\nand 66.27% higher than females on the Levels of Emotional Awareness Scale (LEAS), a \\ntest measuring emotional intelligence in the form of open-ended questions that are \\nevaluated by licensed psychologists. It is also worth noting ChatGPT took the LEAS \\nevaluation twice: once in January and once in February. ChatGPT improved its LEAS score \\nby 13% between the two exams.  \\nDuong & Solomon (2023) compared the ability of ChatGPT to humans in answering \\nmultiple-choice questions about genetics. Humans answered questions with 1.61% greater \\naccuracy overall, but ChatGPT performed 8.51% better than humans on questions that \\nrelied on memorization instead of critical thinking.  \\nJarou et al. (2023) administered multiple-choice questions from the American College \\nof Emergency Physicians (ACEP) study guide. Human respondents scored 36.32% higher \\nthan GPT-3.5 and 19.5% higher than GPT-4; yet GPT-4 scored 33.71% higher than GPT-\\n3.5.  \\nAdditionally, Katz et al. (2023) compared the performance of GPT-3.5 and GPT-4 \\nwith human respondents on the Uniform Bar Exam. GPT-4 performed 11.3% better than \\nhuman test-takers, but GPT-3.5 could not exceed human performance in any of the subject \\nareas tested on the exam. \\nInstead of situations in which ChatGPT replaces humans, this paper is interested in \\nexploring how using ChatGPT enhances a user’s cognitive ability as stated in the \\nhypothesis, H1. Unfortunately, there have not yet been a lot of studies like this. Noy and \\nZhang (2023) assigned 444 professionals with tasks related to their respective profession, \\nsuch as sensitive e-mails, press releases, and reports. After completing the initial task, the \\ngroup was split in two. The control group was asked to repeat the task using LaTeX, a \\ndocument preparation program, while the test group used ChatGPT to assist them with their \\nsecond task. Those using ChatGPT reduced the time spent on the task by 35.16% and \\nimproved their score on the second task by 15.45%. \\n \\n3 The Experiments \\n \\nThis paper presents the results of two experiments in which we asked students to perform \\na task and comparing their performance with that of an expert. Students were tasked with \\ntwo different challenges, an innovation problem and an expert advice question. Students \\nused ChatGPT 3.5 (circa November 2023, January 2024) in these experiments.  \\n  \\n3.1 Innovation Challenge \\nFor the innovation challenge, students were given the following problem statement: \\nWhen shooting skeet, fragments from the skeet fall on and cause harm to the \\ngrass field by preventing sunlight and water from reaching the grass. What \\nchanges can I make to protect the grass?   \\nThis is the same problem statement used in a previous cognitive augmentation \\nexperiment described in Fulbright (2019). In that experiment, students were given hints in \\nthe form of innovative suggestions (called operators) and the results showed it was possible \\nto affect the innovative solutions arrived at by the participants toward a desired goal—the \\npreferred solution. In fact, results showed as much of a 74% increase in cognitive accuracy \\nwas achieved demonstrating significant cognitive augmentation.  \\nParticipants were asked to synthesize three innovative solutions to the problem. Any \\nsolution could not interfere with the sheet shooting activity, must be relatively easy and \\ninexpensive to implement, and involve as little change to the current situation as possible. \\nOne-half of the participants were instructed to not use any Internet-based resource at all. \\nThe other half were instructed to use only ChatGPT. \\nThis innovation problem was chosen because it is a problem used in teaching \\ninnovation at the university undergraduate level for over many years. As such, there is a \\nlong history of solutions, and patterns of solutions, to compare new results with. Because \\nof this history, we know what type of solutions people give when not aided by any cognitive \\nsystem or assistive information and we know what type of solutions are given by \\nprofessional/expert innovators. \\nWith respect to H1, our goal was to see if using ChatGPT altered the type of solutions. \\nIf H1 was verified in this experiment, we would expect to see the solutions trend toward \\nthe professional/expert type of solutions.  \\n \\n3.2 Retirement Decision \\nFor the second experiment, participants were given detailed information about an \\nimaginary college professor approaching retirement. Information provided included: age, \\nprofession (and pros and cons of the profession), salary, debt, medical situation, retirement \\nsavings, with the goals of being able to remain in the current home, travel at least twice per \\nyear after retirement, and not outliving their money. \\nParticipants were asked if the person should retire early at 67 or wait until the age of \\n70. A person can go to a retirement planning expert and ask this question and receive a \\ndetailed response including an explanation of why it is better to retire at 67 or wait until \\n70. None of the participants, being university students, were experts in retirement planning. \\nHowever, we asked each participant to provide a specific answer (either 67 or 70) and then \\nalso provide a justification to support the answer. In our judgement of the results, it did not \\nmatter which age was given as the answer. We focused on the level of detail in the \\njustification. A detailed and specific justification, in our view, constituted an expert-level \\nanswer to the challenge. \\nOne-half of the participants were instructed to use ChatGPT only, an no other Internet-\\nbased resource, and the other half was instructed to use any Internet-based resource except \\nChatGPT. With respect to H1, we expected to see an increase in the ability of participants \\nto provide an expert-level answer due to using ChatGPT. \\n \\n4 \\nThe Results  \\n4.1 Innovation Challenge Results \\nFor the innovation challenge, 13 students used ChatGPT and 13 students did not use \\nChatGPT to synthesize a total of N=96 ideas to solve the skeet shooting innovation \\nchallenge. As we have seen in earlier studies using this problem statement, ideas fell into \\nthree broad categories: changing the field (F), changing the skeet (T), or ideas not solving \\nthe problem at all (X). Ideas involving the field fell into three different subcategories and \\nideas involving the skeet fell into two categories:  \\n \\n \\nFT  \\nprotecting the field with a tarp, net, or some other kind of covering \\n \\nFC \\nways of cleaning the field or making picking up fragments easier \\n \\nFG \\nchanging or replacing the grass on the field \\n \\nTB \\nreplacing clay skeet with biodegradable material \\n \\nTC \\nchanging the clay skeet to make cleanup more easier \\n \\nX \\nideas addressing ideas other than the stated problem \\n \\n \\n \\nFig. 4 shows the results for students not using ChatGPT. Overwhelmingly, most ideas \\n(79.5%) involved changing the field in some way such as covering it with a tarp or net to \\nprevent fragments from reaching the grass or various ways to clean the field after fragments \\nhave fallen onto the grass. The remainder (20.5%) of the ideas involved changing the clay \\nskeet such as making the skeet out of biodegradable material or out of some material other \\nthan clay to facilitate easier cleanup. Field-related ideas out numbered skeet-related ideas \\n3.8:1. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 3. Solutions Using ChatGPT for Assistance (47 ideas). \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 4. Solutions Not Using ChatGPT for Assistance (39 ideas). \\n \\nFig. 3 shows the results for students using ChatGPT. As in Fig 4, ideas involving \\nchanging the field vastly outnumber ideas involving changing the skeet by almost exactly \\nthe same ratio 3.75:1. Therefore, we see no difference in the type of ideas generated as a \\nresult of using ChatGPT. Therefore, the hypothesis, H1, is refuted. \\nFT (11) \\n23.4% \\nFC (14) \\n29.8% \\nFG (5) \\n \\n10.6% \\n63.8% (30) \\nChange Field \\nTB (5) \\n10.6% \\nTC (3) \\n \\n6.4% \\n17.0% (8) \\nChange Skeet \\nXB \\nNo Solution \\n19.1% (9) \\nFC (12) \\n30.7% \\nFT (17) \\n43.6% \\nFG (2) \\n \\n5.1% \\n79.5% (31) \\nChange Field \\nTB (6) \\n15.4% \\nTC (2) \\n \\n5.1% \\n20.5% (8) \\nChange Skeet \\nN = 47 ideas \\nN = 39 ideas \\nInterestingly, students using ChatGPT synthesized a number of ideas having no effect \\nat all on the primary problem—littering of the grass by the fragments. Examining these \\nideas in detail shows these ideas were related to “educating shooters about the \\nenvironmental impact” and “educating shooters about gun safety.” These ideas can be \\nexplained when one analyzes the response from ChatGPT when given the problem \\nstatement as the prompt. ChatGPT is trained from articles and other content available on \\nthe Internet. Because the problem statement involves guns and shooting, ChatGPT \\nresponded with suggestions to educate shooters about gun safety because on the Internet, \\nwhen one sees a document about guns and shooting, it is very likely to also include \\ncomments about safety. Even though the concepts of guns and safety are understandably \\nrelated, the safety issue has nothing to do with solving the problem given in the problem \\nstatement—littering the grass field. ChatGPT however does not perform such in-depth \\nanalysis to realize this. ChatGPT’s responses are driven by word association. Likewise, \\nbecause the problem statement mentions littering and damaging grass, ChatGPT finds \\nassociations with environmental issues important and therefore responded to students \\nsuggesting education about the environment since this is found in millions of pages on the \\nInternet when litter and harming grass is mentioned. While one could argue you might be \\nable to talk a shooter out of shooting after they understand the harm to the grass, this is not \\nlikely to change the mind of the vast majority of shooters, so is not a practical solution. \\nInterestingly, in this case, using of ChatGPT actually distracted students by misleading \\nthem to consider things having nothing to do with the problem. Therefore, one could argue \\nusing ChatGPT actually decreased cognitive ability—resulting in negative cognitive \\naugmentation.      \\n \\n4.2 Retirement Decision Results \\nFor the retirement decision challenge, 15 students used ChatGPT and 10 students did not \\nuse ChatGPT. The challenge asked students to provide a specific answer, whether or not \\nthe subject should retire at 67 or 70 and also provide an expert-level justification of that \\nanswer. We explained to the students how people could visit a retirement planning \\nprofessional and receive guidance and we asked students to provide a similar-quality \\nanswer here.  \\nResponses were judged to be either “expert quality” or “non-expert quality” as seen in \\nFig. 5. The difference between an expert and a non-expert response is in the details \\nprovided in the justification. To answer the question properly, one must calculate the \\nmonthly inflow and outflow of money. To do that, one has to find out how much per month \\nsocial security payments would be and add to that withdrawals from savings to augment \\nthe monthly inflow. Once this is established, one has to calculate how long the subject’s \\nmoney would last. Very different answers are obtained if one retires at age 67 versus 70. \\nIn judging the responses, we did not consider which answer the student provided. It did not \\nmatter at what age the student decided the subject should retire. What we did look for, \\nthough, is did the student conduct and include the analysis needed to justify their response. \\nReponses including the analysis were deemed “expert” and the responses not including the \\nanalysis were deemed “non-expert.” \\nAnother characteristic of non-expert responses was “generic” information like “the \\nperson must consider how long their savings will last.” While this is certainly is something \\na person needs to consider when planning retirement, one would not have to visit an expert \\nto get this advice. Any friend, family member, or easy search on the Internet will produce \\na list of such things for one to consider. In fact, the first response from ChatGPT gives a \\nlist of  8-10 such generic issues to consider. So, a response simply containing generic \\ninformation like this was considered “non-expert.”  \\nFig. 5 shows students not using ChatGPT provided expert-quality answers 40% of the \\ntime. Students using ChatGPT provided expert-quality answers 53% of the time. While \\nthis is an increase, it is not a definitive increase in our opinion. Of further note, is of the \\nstudents using ChatGPT, there was only one more expert response than non-expert \\nresponse. If ChatGPT provided demonstrable cognitive augmentation for this task, one \\nwould expect many more expert answers than non-expert answers from the group of \\nstudents using ChatGPT.  \\nStudents not using ChatGPT were allowed to use any other Internet-based resource \\nand reported the tool or information source they used. We observed all expert-quality \\nanswers from the non-ChatGPT group were provided by students who used a retirement \\ncalculator available on the Internet. We believe the students using a retirement calculator \\nwere cognitively augmented just like students using ChatGPT. In fact, the retirement \\ncalculator is an assistive tool designed specifically to help answer retirement planning \\nquestions whereas ChatGPT is not. Although we are not able to definitively conclude it in \\nthis study, we believe if non-ChatGPT students were not allowed to use a retirement \\ncalculator, the number of expert-quality answers would be much lower and students using \\nChatGPT would have performed much better.    \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 5. Responses to the Retirement Decision Challenge (25 responses). \\n \\n \\n5 Conclusion  \\nOur hypotheses, H1 was refuted in the innovation experiment and only moderately \\nconfirmed in the retirement decision experiment. In fact, in the innovation experiment, \\nChatGPT actually misled students to thinking about issues irrelevant to the problem \\nstatement, resulting in negative cognitive augmentation. Both experiments involved tasks \\nrequiring detailed analysis, high-level reasoning, and human judgment and were questions \\nwithout a definite right and wrong answer. To this extent, we confirm the findings of Li et \\nal. (2023), Liéven et al. (2023), Jarou et al. (2023), and  Katz et al. (2023) who found \\nChatGPT outperformed humans on some types of questions but not those involving higher-\\nlevel analysis.   \\nOur results show using ChatGPT does not guarantee expert-level performance. None \\nof the students participating in this study were experts at using ChatGPT. For some, this \\ntask was the first time they ever used ChatGPT. If students had more experience with \\nChatGPT, more expert-level results might be expected. Also, students who participated \\nwere not given detailed instructions on how to answer retirement questions nor how to \\nthink innovatively. If they had knew more about the subject, it stands to reason more would \\nhave been able to provide expert-quality answers. This can be explored in future studies. \\nIt is necessary to note, when designing these experiments, we found it quite difficult \\nto determine  tasks to give to students. We tested and discarded several tasks before \\ndeciding on the innovation and retirement challenges because we found ChatGPT was able \\nto simply spit out a perfectly correct answer on the first prompt. Over time, we realized we \\ncould not ask students to perform any task involving just simple knowledge retrieval \\nExpert (4) \\n40% \\nNot Using ChatGPT \\nNon-Expert (6) \\n60% \\nExpert (8) \\n53% \\nUsing ChatGPT \\nNon-Expert (7) \\n47% \\nbecause ChatGPT does this quite well. To create a challenge tough enough, we realized the \\ntasks needed to require cognitive processes involving understanding, evaluation, \\nappraisal, critique, and judgment in order to exercise the students and ChatGPT more \\nvigorously.  \\nWe recognize these types of cognitive processes represent the upper levels of Bloom’s \\nTaxonomy, a framework for categorizing educational goals and therefore classifying levels \\nof cognitive processes (Bloom et. al., 1956; Anderson & Kratwohl, 2001). We expect \\nfuture studies to show ChatGPT already able to take the cognitive “grunt work” of lower-\\nlevel cognitive processes like recall, defining, listing, classifying, describing, discussing, \\nexplaining, translating, and recognizing away from the human in a human/cog ensemble. \\nAny task involving these levels of cognitive processing will be done quicker and better by \\nChatGPT leaving the human to do the higher-level cognitive processing. Relieving the \\nhuman of the cognitive “grunt work” will result in significant cognitive augmentation in \\nthe form of higher-quality, higher-value results in less time with less effort. \\n \\n \\nReferences  \\n  \\n1. \\nAbràmoff, M. D., Lavin, P. T., Birch, M., Shah, N. and Folk, J. C. (2018). Pivotal trial of an \\nautonomous AI-based diagnostic system for detection of diabetic retinopathy in primary care \\noffices. Digital Med., 1: 39. Available online at: https://www.nature.com/articles/s41746-018-\\n0040-6, last accessed January 2024. \\n2. \\nAckoff, R. (1989). From Data to Wisdom, Journal of Applied Systems Analysis, 16. Available \\nonline: https://faculty.ung.edu/kmelton/Documents/DataWisdom.pdf last accessed February \\n2023. \\n3. \\nAli, R., Tang, O. Y., Connolly, I. D., Zadnik Sullivan, P. L., Shin, J. H., Fridley, J. S., Asaad, \\nW. F., Cielo, D., Oyelese, A. A., Doberstein, C. E., Gokaslan, Z. L., & Telfeian, A. E. (2023). \\nPerformance of CHATGPT and GPT-4 on Neurosurgery Written Board Examinations. \\nNeurosurgery. https://doi.org/10.1227/neu.0000000000002632. \\n4. \\nAnderson, L. W., Krathwohl, D. R., Airasian, P. W., Cruik-shank, K. A., Mayer, R. E., (2001). \\nA taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of \\neducational objectives, Pearson. \\n5. \\nBloom, B. S.,  Engelhart, M. D., Furst, E. J., Hill, W. H., Krath-wohl, D. R., (1956). \\nTaxonomy of educational objectives: The classification of educational goals. Handbook I: \\nCognitive domain. New York: David McKay Company. \\n6. \\nBrin, D., Sorin, V., Vaid, A., Soroush, A., Glicksberg, B. S., Charney, A. W., Nadkarni, G., & \\nKlang, E. (2023). Comparing ChatGPT and GPT-4 Performance in USMLE Soft Skill \\nAssessments. Scientific Reports, 13(1). https://doi.org/10.1038/s41598-023-43436-9. \\n7. \\nElyoseph, Z., Hadar-Shoval, D., Asraf, K., & Lvovsky, M. (2023). ChatGPT Outperforms \\nHumans in Emotional Awareness Evaluations. Frontiers in Psychology, 14. \\nhttps://doi.org/10.3389/fpsyg.2023.1199058.   \\n8. \\nFulbright, R. (2016a). The Cogs Are Coming: The Cognitive Augmentation Revolution, \\nProceedings of the Association Supporting Computer Users in Education 2015 (49th, Myrtle \\nBeach, SC). Available online at: https://eric.ed.gov/?q=cognitive+development+ \\nin+early+childhood&ff1=dtysince_2014&pg=1340&id=ED570900, last accessed January \\n2024. \\n9. \\nFulbright, R. (2016b). How Personal Cognitive Augmentation Will Lead To The \\nDemocratization Of Expertise, Advances in Cognitive Systems, 4. Available online at: \\nhttp://www.cogsys.org/posters/2016/poster-2016-3.pdf, last viewed January 2024. \\n10. Fulbright, R. (2017). Cognitive Augmentation Metrics Using Representational Information \\nTheory, In: Schmorrow D., Fidopiastis C. (eds) Augmented Cognition. Enhancing Cognition \\nand Behavior in Complex Human Environments,  AC 2017, Lecture Notes in Computer \\nScience, vol 10285. Springer, Cham. Available online: \\nhttps://link.springer.com/chapter/10.1007/978-3-319-58625-0_3 last accessed 2023. \\n11. Fulbright, R. (2018). On Measuring Cognition and Cognitive Augmentation, In: Yamamoto, \\nS., Mori, H. (eds) Human Interface and the Management of Information, HIMI 2018, Lecture \\nNotes in Computer Science, vol 10905. Springer, Cham. Available online: \\nhttps://link.springer.com/chapter/10.1007/978-3-319-92046-7_41 last accessed February 2023. \\n12. Fulbright, R. (2019). Calculating Cognitive Augmentation -A Case Study, In: Schmorrow, D. \\nand Fidopiastis, C. (eds) Augmented Cognition,  AC 2019, Lecture Notes in Computer Science, \\nvol 11580. Springer, Cham. Available online: https://link.springer.com/chapter/10.1007/978-\\n3-030-22419-6_38 last accessed February 2023. \\n13. Fulbright, R. (2020). Democratization of Expertise: How Cognitive Systems Will Revolutionize \\nYour Life, CRC Press, Boca Raton, Fl.  \\n14. Fulbright, R. (2020a). The Expertise Level, In: Schmorrow D., Fidopiastis C. (eds) Augmented \\nCognition. Human Cognition and Behavior. HCII 2020. Lecture Notes in Computer Science, \\nvol 12197. Springer, Cham. Available online: https://link.springer.com/chapter/10.1007/978-\\n3-030-50439-7_4 last accessed February 2023. \\n15. Fulbright R. and Walters, G. (2020). Synthetic Expertise, In: Schmorrow D., Fidopiastis C. \\n(eds) Augmented Cognition. Human Cognition and Behavior. HCII 2020. Lecture Notes in \\nComputer Science, vol 12197. Springer, Cham. Available online: \\nhttps://link.springer.com/chapter/10.1007/978-3-030-50439-7_3 last accessed February 2023. \\n16. Fulbright R. and McGaha, S. (2023). The Effect of Information Type on Human Cognitive \\nAugmentation, In: Schmorrow, D. and Fidopiastis, C. (eds), Augmented Cognition: 17th \\nInternational Conference, AC 2023, held as Part of the 25th HCI International Conference, \\nHCII 2023, Copenhagen, Denmark, July 23–28, 2023, pages 206- 220. Available online at: \\nhttps://dl.acm.org/doi/abs/10.1007/978-3-031-35017-7_14, last viewed January 2024. \\n17. Gil, D. (2019). Cognitive systems and the future of expertise, YouTube video located at \\nhttps://www.youtube.com/watch?v=0heqP8d6vtQ and last accessed February 2023. \\n18. Gregory, M., (2019). AI Trained on Old Scientific Papers Makes Discoveries Humans Missed, \\nVice Internet page located at: https://www.vice.com/en_in/article/neagpb/ai-trained-on-old- \\nlast accessed January 2024. \\n19. Haenssle, H. A., Fink, C., Schneiderbauer, R., Toberer, F., Buhl, T., Blum, A., Kalloo, A., \\nHassen, A. B. H., Thomas, L., Enk, A. and Uhlmann, L. (2018). Man against machine: \\ndiagnostic performance of a deep learning convolutional neural network for dermoscopic \\nmelanoma recognition in comparison to 58 dermatologists. Annals of Oncology, 29(8): 1836–\\n1842. August. Available online: https://academic.oup.com/annonc/article/29/8/1836/5004443, \\nlast accessed November 2019. \\n20. Jarou, Z. J., Dakka, A., McGuire, D., & Bunting, L. (2023). ChatGPT Versus Human \\nPerformance on Emergency Medicine Board Preparation Questions. Annals of Emergency \\nMedicine. https://doi.org/10.1016/j.annemergmed.2023.08.010. \\n21. Katz, D. M., Bommarito, M. J., Gao, S., & Arredondo, P. (2023). GPT-4 Passes the Bar Exam. \\nSSRN Electronic Journal. https://doi.org/10.2139/ssrn.4389233.   \\n22. Kelly, J.E. and Hamm, S. (2013). Smart Machines: IBMs Watson and the Era of Cognitive \\nComputing, Columbia Business School Publishing, Columbia University Press, New York, \\nNY. \\n23. Kung, T. H., Cheatham, M., Medenilla, A., Sillos, C., De Leon, L., Elepaño, C., Madriaga, M., \\nAggabao, R., Diaz-Candido, G., Maningo, J., & Tseng, V. (2023). Performance of ChatGPT \\non USMLE: Potential for AI-Assisted Medical Education Using Large Language Models. \\nPLOS Digital Health, 2(2). https://doi.org/10.1371/journal.pdig.0000198   \\n24. Kurtzman, L. (2019). AI Rivals Expert Radiologists at Detecting Brain Hemorrhages: Richly \\nAnnotated Training Data Vastly Improves Deep Learning Algorithm’s Accuracy, UCSF News, \\nUniversity of California San Francisco. Available online at: https://www.ucsf.edu/news/ \\n2019/10/415681/ai-rivals-expert-radiologists-detecting-brain-hemorrhages, last viewed \\nJanuary 2024. \\n25. Lavars, N. (2019). Machine learning algorithm detects signals of child depression through \\nspeech, New Atlas, published May 7. Available online at https://newatlas.com/machine-\\nlearning-algorithm-depression/59573/ last accessed February 2023. \\n26. Li, S. W., Kemp, M. W., Logan, S. J. S., Dimri, P. S., Singh, N., Mattar, C. N. Z., Dashraath, \\nP., Ramlal, H., Mahyuddin, A. P., Kanayan, S., Carter, S. W. D., Thain, S. P. T., Fee, E. L., \\nIllanes, S. E., Choolani, M. A., Rauff, M., Biswas, A., Low, J. J. H., Ng, J. S., … Lim, M. Y. \\n(2023). ChatGPT Outscored Human Candidates in a Virtual Objective Structured Clinical \\nExamination in Obstetrics and Gynecology. American Journal of Obstetrics and Gynecology, \\n229(2). Available online at: https://pubmed.ncbi.nlm.nih.gov/37088277/, last viewed January \\n2024. \\n27. Liévin, V., Hother, C. E., & Winther, O. (2023). Can Large Language Models Reason About \\nMedical Questions? arXiv. https://doi.org/https://doi.org/10.48550/arXiv.2207.08143. \\n28. Myers, K. D., Knowles, J. W., Staszak, D., Shapiro, M. D., Howard, W., Yadava, M., Zuzick, \\nD., Williamson, L., Shah, N. H., Banda, J. M., Leader, J., Cromwell, W. C., Trautman, E., \\nMurray, M. F., Baum, S. J., Myers, S., Gidding, S. S., Wilemon, K. and Rader, D. J. (2019). \\nPrecision screening for familial hypercholesterolaemia: a machine learning study applied to \\nelectronic health encounter data, Lancet Digital Health, Available online at: \\nhttps://www.thelancet.com/journals/ landig/article/PIIS2589-7500(19)30150-5/fulltext, last \\naccessed January 2024. \\n29. Noy, S., and Zhang, W. (2023). Experimental Evidence on the Productivity Effects of Generative Artificial \\nIntelligence, Science, Vol 381, Issue 6654, pp. 187-192. Available online at: https://www.science.org/ \\ndoi/10.1126/science.adh2586, last viewed January 2024. \\n \\n30. [Stevens] (2023). Listen to your heart: AI tool detect heart diseases that doctors often miss, Stevens Institute \\nof Technology media release. Available online at: https://www.stevens.edu/ news/listen-to-your-heart-ai-tool-\\ndetects-cardiac-diseases-that-doctors-often#, last viewed January 2024. \\n31. Sandoiu, A. (2019). Artificial intelligence better than humans at spotting lung cancer. Medical \\nNews Today Newsletter, May 20. Available online: https://www. \\nmedicalnewstoday.com/articles/325223.php#1, last accessed November 2019.  \\n32. Tshitoyan, V., Dagdelen, J., Weston, L., Dunn, A., Rong, Z., Kononova, K., Persson, A., \\nCeder, G., and Jain, A. (2019). Unsupervised word embeddings capture latent knowledge from \\nmaterials science literature, Nature, 571. Available online at: https://www.nature.com/ \\narticles/s41586-019-1335-8, last viewed January 2024. \\n33. Towers-Clark, C. (2019). The Cutting-Edge of AI Cancer Detection, Forbes, published April \\n30. Available online at  https://www.forbes.com/sites/charlestowersclark/ \\n2019/04/30/the-cutting-edge-of-ai-cancer-detection/#45235ee77336 last accessed February \\n2023. \\n34. Wehner, M. (2019). AI is now better at predicting mortality than human doctors, New York \\nPost, published May 14. Available online at https://nypost.com/2019/05/14/ai-is now-better-\\nat-predicting-mortality-than-human doctors/?utm_campaign=partnerfeed&utm_medium= \\nsyndicated&utm_source=flipboard last accessed February, 2023.  \\n35. Wladawsky-Berger, I. (2015). The Era of Augmented Cognition, The Wall Street Journal: \\nCIO Report Internet page located at http://blogs.wsj.com/cio/2013/06/28/the-era-of-\\naugmented-cognition/ last accessed February 2023. \\n \\n \\n\"},\n",
       " {'abstract': 'The concept of Spatial XR-IoT (XRI) Zone Agents is presented, merging Extended Reality (XR), the Internet of Things (IoT), and spatial computing for smart environments. These zone agents serve as applications and companions in shared spaces, reducing the gap between the physical environment and traditional user interfaces. By incorporating Mixed Reality Agents (MiRAs), agent and scene design strategies are outlined for spatial zone agents. A prototype and user interaction scenario demonstrate human-to-space agent relationships in an immersive smart-space application.',\n",
       "  'introduction': \"The metaverse integrates computing technologies to combine virtual and physical spaces, connecting the digital and real worlds. However, a gap exists between users' physical and virtual environments, referred to as the metaverse disconnect. To address this, the method of XRI (XR-IoT) is introduced, supporting hyper-connected metaverse environments. XRI combines IoT components and hybrid physical-virtual interactions to create smart spaces that are social, smart, engaging, and immersive. These concepts lay the groundwork for enriching user information architecture across applications and smart space configurations.\",\n",
       "  'literature review': 'Researchers have explored methods to enhance the connection between users and smart spaces, aiming to reduce the metaverse disconnect. The XRI concept, combining XR and IoT, is presented along with early frameworks and proof-of-concept prototypes. These prototypes demonstrated the integration of virtual plants in mixed reality connected to physical environments via IoT, as well as enhanced head-mounted display prototypes. Metaverse has various definitions, reflecting its evolving state, ranging from immersive 3D digital environments to a combination of internet, web technologies, and extended reality. The metaverse is envisioned as the next generation of the internet, increasing connectivity with physical spaces.',\n",
       "  'methodology': 'The research integrates insights from Mixed Reality Agents (MiRAs), spatial computing and interface, and agent system design. These theories are combined and adapted into a three-dimensional design space for XRI Zone Agents. The dimensions include the mixed reality dimension, the level of agency, and the physical-remote (PR) spatial interaction capacity. This framework is used to design spatial zone agents with different interactions, capabilities, and objectives.',\n",
       "  'results': 'A prototype of the XRI Zone Agent is presented, demonstrating scenario-based user interaction within a laboratory environment. When users navigate the lab wearing an XR HMD, they encounter virtual agents, such as a plant avatar, that provide guidance and enable user-selected tasks. These tasks include study mode, relaxation mode, and meeting mode, each represented by a spatial zone agent. The agents respond to user actions, such as thumbs-up gestures, and control virtual objects and physical IoT devices (e.g., lights, projectors) based on user context and spatial location.',\n",
       "  'conclusion': 'This work offers a theoretical framework for considering smart-space zones as agents and explores mixed reality and IoT (XRI) zone agents using design-theoretic methods. It envisions that zone agents would adapt to user spatial contexts and needs through changes in physical or virtual reality. The presented high-level user interaction design, based on three zone agents for work, leisure, and meetings, illustrates potential applications of these concepts. The research aims to pave the way for further exploration of space-to-human and human-to-space interactions driven by mixed-reality agents and user and environmental context within zones.',\n",
       "  'title': 'Design Frameworks for Spatial Zone Agents in XRI Metaverse Smart Environments',\n",
       "  'author': 'Jie Guan, Jiamin Liu, Alexis Morris',\n",
       "  'textdata': '©2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any\\ncurrent or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new\\ncollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other\\nworks.\\narXiv:2401.11040v1  [cs.HC]  19 Jan 2024\\nDesign Frameworks for Spatial Zone Agents\\nin XRI Metaverse Smart Environments\\nJie Guan\\nAdaptive Context Environments Lab\\nOCAD University\\nToronto, Canada\\njie.guan@ocadu.ca\\nJiamin Liu\\nAdaptive Context Environments Lab\\nOCAD University\\nToronto, Canada\\njiaminliu@ocadu.ca\\nAlexis Morris\\nAdaptive Context Environments Lab\\nOCAD University\\nToronto, Canada\\namorris@ocadu.ca\\nAbstract—The spatial XR-IoT (XRI) Zone Agents concept\\ncombines Extended Reality (XR), the Internet of Things (IoT),\\nand spatial computing concepts to create hyper-connected spaces\\nfor metaverse applications; envisioning space as zones that are\\nsocial, smart, scalable, expressive, and agent-based. These zone\\nagents serve as applications and agents (partners, assistants, or\\nguides) for users co-living and co-operating together in a shared\\nspatial context. The zone agent concept is toward reducing the\\ngap between the physical environment (space) and the classical\\ntwo-dimensional user interface, through space-based interactions\\nfor future metaverse applications. This integration aims to enrich\\nuser engagement with their environments through intuitive and\\nimmersive experiences and pave the way for innovative human-\\nmachine interaction in smart spaces. Contributions include: i) a\\ntheoretical framework for creating XRI zone/space-agents using\\nMixed-Reality Agents (MiRAs) and XRI theory, ii) agent and\\nscene design for spatial zone agents, and iii) prototype and user\\ninteraction design scenario concepts for human-to-space agent\\nrelationships in an early immersive smart-space application.\\nIndex Terms—Virtual Reality, Mixed Reality, Augmented Real-\\nity, Internet of Things, Human-Computer Interaction, Metaverse\\nI. INTRODUCTION\\nThe Metaverse refers to the merger of computing technolo-\\ngies that blend virtual and physical spaces; applying technolo-\\ngies such as Artificial intelligence and Extended Reality and\\nthe internet-of-things [1]. These technologies aim to provide\\ndigital twins of the real world as well as completely virtual\\nshared environments; however, in both cases there is a clear\\ngap between the user’s physical environment and their virtual\\nenvironment. This is referred to as the metaverse disconnect\\nproblem and researchers have explored methods to increase\\nthe connection between users and their relationships to smart\\nspaces [2]–[5]. To enhance the communication and connection\\nbetween the metaverse and physical environments and objects,\\nthe method of XRI (XR-IoT) [6], [7] is introduced for support-\\ning the hyper-connected metaverse environment. These XRI\\ncomponents allow for the creation of hybrid physical-virtual\\nobjects as well as the hybrid physical-virtual interactions that\\ncan take place. As more “hybrid objects” are combined into a\\nuser’s space, this results in a “hyper-connected” environment\\nthat can be social, smart, engaging, and immersive, as in [8].\\nSuch environments are poised to enrich the user’s information\\nTri-council of Canada, Canada Research Chairs program.\\narchitecture, as in [9] in terms of placemaking (presence),\\nconsistency(mental models), resilience (adaptation), reduction\\n(context adaptation), and correlation (exploration), across ap-\\nplications and smart space configurations (small home spaces,\\nwork spaces, larger scale city environments, etc).\\nFig. 1.\\nXRI smart-spaces [6] can be transformed into zone-aware spatial\\nagents that have potential to interact with users based on their spatial contexts.\\nThis work focuses on design components and strategies for these agents [10].\\nHowever, these designs still have interaction challenges\\nbetween humans, agents, and spaces of the metaverse context.\\nThis vision of the metaverse requires user, multi-agent, XRI,\\nand Spatial Computing Interfaces in order to make the hybrid\\nenvironment fit the user application context. Figure 1 presents\\na high-level XRI smart space (objects, IoT components,\\ncameras, as well as virtual agents, etc), and highlights that\\nmultiple design components and strategies for mixed reality\\ninteractions, from [10], can be considered toward not just\\nmixed reality or IoT objects, but also spatial zone agents\\n(or zone as agent designs). This work presents the design of\\nexample zone-based agents and agent interactions using an\\nearly framework. It addresses the question of how to design a\\nspatial zone agent for metaverse interactions, and an approach\\nfor creating zone agent prototypes. Contributions include: i)\\na theoretical framework for creating XRI zone/space-agents\\nusing Mixed Reality Agents (MiRAs) theory [11] and XRI,\\nii) agent and scene design for spatial zone agents, and iii)\\nprototype and user interaction design scenario and concepts\\nfor human-to-space agent relationships in an early immersive\\nsmart-space application.\\nFig. 2. Zone-aware spatial agents can have a range of Physical and Remote\\ninteraction abilities and properties, as well as Mixed Reality Agency (MiRAs),\\nas seen in examples 1-5, based on [11] [10].\\nII. BACKGROUND\\nXRI and Metaverse: The early frameworks and proof-of-\\nconcept prototypes of XR and IoT for specific scenarios had\\nbeen presented in [12] [13] to connect the physical information\\nand the virtual GUI and objects. Based on this foundation,\\nprevious works also explored a virtual plant embodied in\\nMixed Reality and connected to the physical environment via\\nIoT in mobile device [14], and an enhanced head-mounted\\ndisplay prototype, in [15] [16]. The XRI concept also appears\\nin [7] with a taxonomy and workstation prototype in [6].\\nMetaverse has diverse definitions as it is a developing term\\nin the state of the art. As in [17], the metaverse is considered\\nto be constructed by the immersive and three-dimensional\\ndigital online environments, while in [1] [18], it is considered\\nmetaverse is constructed by internet, web technologies and\\nextended reality with hybridization of physical and virtual\\nspace. Metaverse has been considered as the next generation\\nof the internet in both industry and academia [19]. Together,\\nthe metaverse as a virtual online environment is envisioned\\nto increase the connectedness with the physical space. Hence,\\nthe concept of extending the metaverse with XRI has been\\npresented in previous works such as in [2]–[5], [8].\\nSpatial Computing and interface: Spatial computing in-\\ncorporates the concept and technologies to create a new\\nunderstanding of locations, improving the relationship between\\nhumans and space [20]. Spatial Computing is the human-\\nin-the-loop interaction that engages with real objects and\\nspace; together with the recent trend of mixed reality that\\nmerges real and virtual worlds, the need for mapping virtual\\nenvironments in the real-world is becoming more important\\n[21]. Spatial computing allows users to interact freely with\\nthe 2D interface and three-dimensional information that spatial\\naugmented reality provides [22].\\nAgent System Design (Agency): Agent System Design is\\nan interdisciplinary field, within Artificial Intelligence, as in\\n[23], which has been instrumental in laying the groundwork\\nfor creating intelligent and scalable agent-based systems. This\\nfoundation has opened avenues for innovation in various\\nrealms, including Mixed Reality. The Mixed Reality Agents\\n(MiRAs) [11] framework considers agents that are designed\\nto operate within mixed reality environments, with dimensions\\nin embodiment, interaction and agency level. Moreover, the\\nrobot as an agent has been explored within the domain of\\nHuman-Robot Interaction (HRI) and Extended Reality (XR)\\n[10], where they can act to bridge the chasm between the\\nhuman environment and the digital world [24].\\nAgent systems integrate AI, software design, and HCI\\n[25], cognitive architectures, like Prometheus [26], and other\\nbeliefs-desires-intentions (BDI) frameworks, and machine\\nlearning models, like those used in robotics and games [27],\\n[28], and deploy approaches like Behavior Trees for decision-\\nmaking [29], [30]. Similarly, in entertainment, Emotional AR\\nAgents can enhance immersion and interactivity [31]–[33],\\nand in HRI, agent embodiment can influence user perception,\\nparticularly with recent advancements in generative AI (like\\nDALL-E and VQGAN) revolutionizing agent visual represen-\\ntation and understanding capabilities [34]–[37].\\nTogether, these indicate new directions for future metaverse\\nspaces that are agent-based, zone-oriented, and immersive.\\nIII. THEORY - DESIGNING A SPATIAL ZONE AGENT FOR\\nXRI SMART SPACES\\nDesign Dimensions for Spatial Zone Agents: MiRAs and\\nXRI and HRI Taxonomy [10] Figure 2 presents the XRI\\nZone Agent design theory, which has been merged and adapted\\nfrom XRI theory, virtuality continuum [38], MiRAs [11] and\\nhuman-robot interaction [10] into three dimensions, including\\nthe mixed reality dimension, the level of agency, and physical-\\nremote (PR) spatial interaction capacity. The X axis is the\\nmixed reality dimension adapted from the virtuality continuum\\n[38] that represents the real-environment on the left and fully\\nvirtual reality environment on the right. The Y axis is the\\nlevel of agency, which means how capable the agent is in\\nterms of its autonomous, reactive, proactiveness, and social\\nabilities [23]. The Z axis of physical-to-remote (PR) spatial\\ninteraction capacity represents the level of physical or remote\\ncontrol users have when interacting with objects, and ranges\\nfrom physical interaction to non-physical interaction, such as\\nthrough IoT communication.\\nOn the left side, the number 1 represents physical interaction\\nand interface design, with the 3D physical interfaces (the phys-\\nFig. 3.\\nZone agents are here designed for spatial context, based on agent\\ndesign methodology like [26] [23], (including monitoring and responding to\\nuser-zone events such as time context, virtual context, IoT context, zone object\\ncontext, etc.). These agents express embodiment through zone-driven actions\\nthat may be virtual or physical. The level of agency can vary according to\\nagent implementation (from simple reflex agents to high-functioning agents).\\nical toggle, slider, etc.), empowering users to interact with and\\ndynamically adjust elements within the physical environment,\\nand a 2D digital user interface (screen icons are widely used\\nfor human-machine interaction). The number 2 indicates the\\nIoT-enabled physical objects and integrated controllers, and\\nthis setup facilitates the communication of information across\\na network of devices. In addition, autonomous agents, such as\\nrobots, have the capacity to function within and adapt to this\\nenvironment while establishing interaction with users.\\nOn the right side, the number 3 is from mixed reality to fully\\nvirtual environments, where users manipulate 2D digital UI\\nand 3D objects. Current mixed-reality interactions commonly\\nemploy 3D virtual objects with hand-tracking capabilities to\\nsimulate interaction with real-world objects. The number 4\\npresents that the virtual objects could be used to control or be\\nmanipulated by physical objects through IoT communication,\\nas in [2] [3] [8]. Finally, the number 5 shows XRI 3D Spatial\\nZone Interaction. These agents and their design capabilities\\nand objectives are shown in Figure 3.\\nThe scenario considers the designed spatial position of\\ndifferent agents (with virtual embodiment) and the potential\\nIoT-Enabled devices that could be controlled (such as a light,\\nprojector, and robot). In terms of spatial zone agents these\\ncan include a relaxation agent, meeting agent, and workstation\\nagent, each operating as applications within the space, driven\\nby specific objectives and responding to user contexts.\\nIV. DESIGN FRAMEWORK AND PROTOTYPE OF SPATIAL\\nZONE AGENTS\\nXRI Zone Agent Framework: Figure 4 shows the frame-\\nwork of the prototype, indicating how the user navigates the\\nuse case environment (lab environment) between zones and\\nthe interaction with both virtual and physical objects through\\nIoT communication. The virtual agents such as lab assistant\\nagent are rendering in XR headset, powered by Unity1, and\\nthe physical objects, such as Phillips Hue smart lights2 and\\nWebcam are communicating to the virtual agents through an\\nMQTT Broker3.\\nPrototype of XRI Zone Agent Interactions: Figure 5\\npresents the prototype of the XRI Zone Agent, for the scenario\\nand user behaviour interaction within the lab environment with\\nthe XRI zone agents. When users wear the XR HMD device\\nand enter the lab environment, they can see the virtual lab\\nassistant agent represented as a plant avatar, with butterflies\\nmoving in the environment. The avatar is a guide that shows\\nwelcome messages to the user and displays an initial introduc-\\ntion to the user. There is also an indicator to tell the user how\\nto interact and enable the next state of the plant avatar with a\\nthumbs-up gesture (hand tracking). When the user shows the\\nthumbs-up gesture (see Figure 5), the plant avatar will wave\\nand move toward the user and show three menu icons of tasks\\n(start learning, start relaxing, and start meeting) for selection\\nthat could be performed in the lab. When the user presses the\\nstart learning button, for example, the plant avatar will move\\ntoward the learning zone with the workstation agent and show\\nan indicator on that zone to guide the user to the location (this\\nis an example of a spatial agent that can move between zones).\\nThe user will enter the “learning zone”, and the workstation\\nagent for that zone will show the time of how long the user\\nhas been studying (this is an example of tracking user context\\nin the zone). Once the time reaches the desired length, the\\nplant avatar will stand in front of the user to ask if the user\\nwould like to start relaxing. If the user presses the start relaxing\\nbutton, the plant avatar will guide the user toward the relax\\nzone, and the wall around the user, driven by the relax agent,\\nwill transform into a relaxed scene with particle effects (an\\nexample of a simple-reflex zone agent based on user entry).\\nAlso, when the user wants to start a meeting, they can perform\\nthe thumbs-up gesture, to summon the plant avatar and select\\nthe meeting state. The plant avatar will move to the physical\\nlight switch to ask/guide the user to turn it off. Once the\\nmeeting zone agent detects the environment light has turned\\noff (through computer vision light detection), the meeting\\nprojector will turn on and enter the meeting mode (another\\nexample of zone agent responsiveness). While these agent\\ninteractions are simple-reflex agents, they can be enhanced\\nby integration of stronger agent paradigms, such as the use of\\nML model-driven controllers, large language models, or more\\ncapable cognitive architectures.\\nV. SUMMARY\\nThis work has provided a theoretical framework for consid-\\nering smart-space zones as agents, and has proposed an early\\ndesign-theoretic exploration of mixed reality and IoT (XRI)\\n1https://unity.com/\\n2https://www.philips-hue.com/en-ca/p/hue-white-and-color-ambiance-\\na19—e26-smart-bulb—75-w–2-pack-/046677563370\\n3https://mqtt.org/\\nFig. 4. The prototype framework for the multi-agent Zone scenario includes the highlighted actors (user, learning agent, relax agent, meeting agent); these\\nagents interact with the user; which may be both physical, IoT, or virtual interactions, across a communication channel (e.g., MQTT).\\nFig. 5. Using the HRI Framework [10], the design of Zone agents can be considered; showing user interface and widgets, spatial references and visualizations,\\nembedded visual effects, and interaction modalities for an early prototype, as shown. A deeper evaluation of zone agents is left for future work.\\nzone agents as well as an architecture and initial prototype\\nimplementation. This has considered that user interactions with\\nzone/space agents will involve sensors, actuators, embodiment,\\ndecision-making, and a mixture of communication paradigms.\\nThe work considers exploring space-to-human and human-\\nto-space interactions driven-by mixed-reality agents and user\\nand environmental context within zones. It envisions that zone\\nagents would be designed to adapt to user spatial contexts and\\nneeds through changes in either physical or virtual reality. A\\nhigh-level user interaction design has been presented, based\\non three zone agents for work, leisure, and meetings. The\\nhope is that such framing of space-as-agent will help further\\nnew forms of metaverse exploration where digital twins of\\neveryday environments work together with humans-in-the-\\nloop, streamlining their shared contexts. This early exploration\\nsets the stage for further research in this direction, and future\\nresearch will refine further in terms of agency level, physical-\\nvirtual responsiveness, and new forms of spatial interaction.\\nACKNOWLEDGMENT\\nThis work gratefully acknowledges funding from the Tri-\\ncouncil of Canada under the Canada Research Chairs program.\\nREFERENCES\\n[1] L.-H. Lee, T. Braud, P. Zhou, L. Wang, D. Xu, Z. Lin, A. Kumar,\\nC. Bermejo, and P. Hui, “All one needs to know about metaverse: A\\ncomplete survey on technological singularity, virtual ecosystem, and\\nresearch agenda,” arXiv preprint arXiv:2110.05352, 2021.\\n[2] J. Guan, J. Irizawa, and A. Morris, “Extended reality and internet of\\nthings for hyper-connected metaverse environments,” in 2022 IEEE\\nConference on Virtual Reality and 3D User Interfaces Abstracts and\\nWorkshops (VRW), 2022, pp. 163–168.\\n[3] J. Guan, A. Morris, and J. Irizawa, “Cross-reality for extending the\\nmetaverse: Designing hyper-connected immersive environments with\\nxri,” in 2023 IEEE Conference on Virtual Reality and 3D User Interfaces\\nAbstracts and Workshops (VRW), 2023, pp. 305–311.\\n[4] ——, “Extending the metaverse: Hyper-connected smart environments\\nwith mixed reality and the internet of things,” in 2023 IEEE Conference\\non Virtual Reality and 3D User Interfaces Abstracts and Workshops\\n(VRW), 2023, pp. 817–818.\\n[5] J. Guan, “Extending the metaverse: Exploring generative objects with\\nextended reality environments and adaptive context awareness,” 2022.\\n[6] A. Morris, J. Guan, and A. Azhar, “An xri mixed-reality internet-of-\\nthings architectural framework toward immersive and adaptive smart\\nenvironments,” in 2021 IEEE International Symposium on Mixed and\\nAugmented Reality Adjunct (ISMAR-Adjunct), 2021, pp. 68–74.\\n[7] T. Tsang and A. Morris, “A hybrid quality-of-experience taxonomy for\\nmixed reality iot (xri) systems,” in 2021 IEEE International Conference\\non Systems, Man, and Cybernetics (SMC), 2021, pp. 1809–1816.\\n[8] J. Guan and A. Morris, “Design frameworks for hyper-connected\\nsocial\\nxri\\nimmersive\\nmetaverse\\nenvironments,”\\narXiv\\npreprint\\narXiv:2306.06230, 2023.\\n[9] A. Resmini, B. Lindenfalk, and J. Jauhiainen, “Being elsewhere: An\\ninformation architecture approach to the design of a sense of presence\\nin xr environments,” in International Conference on Human-Computer\\nInteraction.\\nSpringer, 2023, pp. 502–521.\\n[10] R. Suzuki, A. Karim, T. Xia, H. Hedayati, and N. Marquardt, “Aug-\\nmented reality and robotics: A survey and taxonomy for ar-enhanced\\nhuman-robot interaction and robotic interfaces,” in Proceedings of the\\n2022 CHI Conference on Human Factors in Computing Systems, 2022,\\npp. 1–33.\\n[11] T. Holz, A. G. Campbell, G. M. O’Hare, J. W. Stafford, A. Martin,\\nand M. Dragone, “Mira—mixed reality agents,” International journal of\\nhuman-computer studies, vol. 69, no. 4, pp. 251–268, 2011.\\n[12] D. Jo and G. J. Kim, “Ariot: scalable augmented reality framework\\nfor interacting with internet of things appliances everywhere,” IEEE\\nTransactions on Consumer Electronics, vol. 62, no. 3, pp. 334–340,\\n2016.\\n[13] ——, “Ar enabled iot for a smart and interactive environment: A survey\\nand future directions,” Sensors, vol. 19, no. 19, p. 4330, 2019.\\n[14] Y. Shao, N. Lessio, and A. Morris, “Iot avatars: Mixed reality\\nhybrid objects for core ambient intelligent environments,” Procedia\\nComputer Science, vol. 155, pp. 433–440, 2019, the 16th International\\nConference on Mobile Systems and Pervasive Computing (MobiSPC\\n2019),The 14th International Conference on Future Networks and\\nCommunications (FNC-2019),The 9th International Conference on\\nSustainable\\nEnergy\\nInformation\\nTechnology.\\n[Online].\\nAvailable:\\nhttps://www.sciencedirect.com/science/article/pii/S1877050919309743\\n[15] J. Guan, N. Lessio, Y. Shao, and A. Morris, “Exploring a mixed\\nreality framework for the internet-of-things: Toward visualization and\\ninteraction with hybrid objects and avatars,” in 2020 IEEE Conference\\non Virtual Reality and 3D User Interfaces Abstracts and Workshops\\n(VRW), 2020, pp. 857–857.\\n[16] A. Morris, J. Guan, N. Lessio, and Y. Shao, “Toward mixed reality\\nhybrid objects with iot avatar agents,” in 2020 IEEE International\\nConference on Systems, Man, and Cybernetics (SMC), 2020, pp. 766–\\n773.\\n[17] J. D. N. Dionisio, W. G. B. III, and R. Gilbert, “3d virtual worlds and\\nthe metaverse: Current status and future possibilities,” ACM Computing\\nSurveys (CSUR), vol. 45, no. 3, pp. 1–38, 2013.\\n[18] H. Ning, H. Wang, Y. Lin, W. Wang, S. Dhelim, F. Farha, J. Ding, and\\nM. Daneshmand, “A survey on metaverse: the state-of-the-art, technolo-\\ngies, applications, and challenges,” arXiv preprint arXiv:2111.09673,\\n2021.\\n[19] R. Cheng, N. Wu, S. Chen, and B. Han, “Will metaverse be nextg\\ninternet? vision, hype, and reality,” arXiv preprint arXiv:2201.12894,\\n2022.\\n[20] S. Shekhar, S. K. Feiner, and W. G. Aref, “Spatial computing,”\\nCommun. ACM, vol. 59, no. 1, p. 72–81, dec 2015. [Online]. Available:\\nhttps://doi.org/10.1145/2756547\\n[21] S. Greenwold, “Spatial computing,” Massachusetts Institute of Technol-\\nogy, Master, 2003.\\n[22] M. R. Marner, R. T. Smith, J. A. Walsh, and B. H. Thomas, “Spatial\\nuser interfaces for large-scale projector-based augmented reality,” IEEE\\ncomputer graphics and applications, vol. 34, no. 6, pp. 74–82, 2014.\\n[23] M. Wooldridge and N. R. Jennings, “Intelligent agents: Theory and\\npractice,” The knowledge engineering review, vol. 10, no. 2, pp. 115–\\n152, 1995.\\n[24] D. Szafir, “Mediating human-robot interactions with virtual, augmented,\\nand mixed reality,” in International Conference on Human-Computer\\nInteraction.\\nSpringer, 2019, pp. 124–149.\\n[25] N. R. Jennings, K. Sycara, and M. Wooldridge, “A roadmap of agent\\nresearch and development,” Autonomous agents and multi-agent systems,\\nvol. 1, no. 1, pp. 7–38, 1998.\\n[26] L. Padgham and M. Winikoff, “Prometheus: A methodology for devel-\\noping intelligent agents,” in International Workshop on Agent-Oriented\\nSoftware Engineering.\\nSpringer, 2002, pp. 174–185.\\n[27] P. Caillou, B. Gaudou, A. Grignard, C. Q. Truong, and P. Taillandier, “A\\nsimple-to-use bdi architecture for agent-based modeling and simulation,”\\nin Advances in Social Simulation 2015.\\nSpringer, 2017, pp. 15–28.\\n[28] R. H. Bordini, A. El Fallah Seghrouchni, K. Hindriks, B. Logan, and\\nA. Ricci, “Agent programming in the cognitive era,” Autonomous Agents\\nand Multi-Agent Systems, vol. 34, no. 2, pp. 1–31, 2020.\\n[29] A. S. Kyaw, C. Peters, and T. N. Swe, Unity 4. x Game AI Programming.\\nPackt Publishing, 2013.\\n[30] M. Iovino, E. Scukins, J. Styrud, P. ¨Ogren, and C. Smith, “A survey of\\nbehavior trees in robotics and ai,” Robotics and Autonomous Systems,\\nvol. 154, p. 104096, 2022.\\n[31] H. Ushida, Y. Hirayama, and H. Nakajima, “Emotion model for life-like\\nagent and its evaluation,” in AAAI/IAAI, 1998, pp. 62–69.\\n[32] K. Gushima, H. Akasaki, and T. Nakajima, “Ambient bot: delivering\\ndaily casual information through eye contact with an intimate virtual\\ncreature,” in Proceedings of the 21st International Academic Mindtrek\\nConference, 2017, pp. 231–234.\\n[33] D. Bylieva, N. Almazova, V. Lobatyuk, and A. Rubtsova, “Virtual pet:\\ntrends of development,” in The 2018 International Conference on Digital\\nScience.\\nSpringer, 2019, pp. 545–554.\\n[34] B. Wang and P.-L. P. Rau, “Influence of embodiment and substrate\\nof social robots on users’ decision-making and attitude,” International\\nJournal of Social Robotics, vol. 11, no. 3, pp. 411–421, 2019.\\n[35] V. Groom, C. Nass, T. Chen, A. Nielsen, J. K. Scarborough, and\\nE. Robles, “Evaluating the effects of behavioral realism in embodied\\nagents,” International Journal of Human-Computer Studies, vol. 67,\\nno. 10, pp. 842–849, 2009.\\n[36] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen,\\nand I. Sutskever, “Zero-shot text-to-image generation,” in International\\nConference on Machine Learning.\\nPMLR, 2021, pp. 8821–8831.\\n[37] K. Crowson, S. Biderman, D. Kornis, D. Stander, E. Hallahan, L. Cas-\\ntricato, and E. Raff, “Vqgan-clip: Open domain image generation and\\nediting with natural language guidance,” in European Conference on\\nComputer Vision.\\nSpringer, 2022, pp. 88–105.\\n[38] P. Milgram and F. Kishino, “A taxonomy of mixed reality visual\\ndisplays,” IEICE TRANSACTIONS on Information and Systems, vol. 77,\\nno. 12, pp. 1321–1329, 1994.\\n'},\n",
       " {'abstract': 'Insecure content such as cyberbullying, self-harm, and sexually explicit images are increasingly being shared on social media platforms. These platforms use AI and human moderation to safeguard users from such images and provide an explanation of why they are flagged. However, two critical problems arise from this process - identifying a rationale for obfuscating the images and deciding how much to obfuscate. This work addresses these issues by proposing a visual reasoning model (VLM) called ConditionalVLM, conditioned on pre-trained unsafe image classifiers, to provide an accurate rationale grounded in unsafe image attributes. Additionally, the paper introduces a counterfactual explanation algorithm that minimally obfuscates unsafe regions for safe viewing. Our experiments on real-world data demonstrate the efficacy of the proposed method.',\n",
       "  'introduction': 'Social media platforms are increasingly being misused for sharing unsafe content such as sexually explicit images, cyberbullying, and self-harm. To safeguard users from such content, major platforms employ AI and human-based content moderation techniques to flag and obfuscate (i.e., make the image safer by blurring sensitive regions) such images. This process involves obfuscating unsafe image regions in the image (Li et al. 2017) along with generating a rationale that backs up the decision to obfuscate the flagged images (Meta 2022). The image obfuscation process faces two critical problems regarding how much of the unsafe image is obfuscated and why it is obfuscated: First, the decision to deem an image unsafe and obfuscate it demands providing a rationale for the decision. For example, Instagram moderators are required to provide a legal rationale (Bronstein 2021; Are 2020) to back up their decision (Tenbarge 2023). Existing visual reasoning methods (Li et al. 2022, 2023; Dai et al. 2023) are severely limited for unsafe images such as sexually explicit, cyberbullying, and self-harm since they cannot provide a rationale grounded in attributes that are specific to such images, such as rude hand gestures in cyberbullying images (Vishwamitra et al. 2021), or sensitive body parts in sexually-explicit images (Binder 2019). Second, the unsafe image needs minimal obfuscation while still depicting the safe regions for evidence collection and investigation (Billy Perrigo 2019). For instance, human moderators need to determine the age of the person in the image (e.g., in child sex-ual abuse material (CSAM) investigations), look for identi-fiers (e.g., tattoos, scars, and unique birthmarks), and deter-mine their location information (e.g., landmarks, geograph-ical features, and recognizable surroundings). Current seg-mentation techniques (Chandrasekaran et al. 2021; Vermeire et al. 2022; Bethany et al. 2023) cannot minimally identify the regions and consequently impede investigations that per-tinently need full details of the remaining safe regions.',\n",
       "  'literature_review': 'Social media platforms are constantly misused to share un-safe content. For example, sexually-explicit images (Ashurst and McAlinden 2015; Sanchez et al. 2019), non-consensual in-timate images (NCII) (Lenhart, Ybarra, and Price-Feeney 2016) and child sexual abuse material (CSAM) are being in-creasingly shared on major social media platforms (Sanchez et al. 2019). Cyberbullying, a critical issue affecting ado-lescents and adults is spread unabated via images (Vish-wamitra et al. 2021). Furthermore, self-harm images are widely spread on these platforms that further alienate vulner-able users (John et al. 2018). To defend against this threat, social media platforms alter the unsafe image by blurring the sensitive regions. This obfuscation of such unsafe im-ages has substantial implications. For instance, social media platforms employ over a million moderators globally (bbc 2021), who manually view unsafe images which is known to have an adverse effect on their mental health, includ-ing PTSD (reu 2021). Vulnerable users, such as minors also need image safeguarding methods that can shield them from unsafe content. Furthermore, law enforcement agents who investigate images from a crime scene need image safe-guarding since such images contain extremely disturbing images, such as CSAM. These applications have a critical need, the unsafe regions must be minimally obfuscated. For instance, a moderator needs to view the safe regions to con-duct investigations, such as determining the age of the per-son to report CSAM content to law enforcement.',\n",
       "  'methodology': 'Figure 1 illustrates the architecture of our proposed ap-proach, which consists of two modules. The initial module utilizes ConditionalVLM for classifying images as safe or unsafe, while the subsequent module proposes counterfactual visual explanations to identify and obfuscate the unsafe regions within the image.',\n",
       "  'results': 'We evaluated our Conditional VLM and Counterfactual Subobject Explanation methods on three datasets of real-world harmful images to study the practical application of counterfactual subobject explanations.\\n\\nSexually Explicit: First, we sampled a subset of images from an NSFW images dataset (Kim 2021) consisting of 334,327 images by selecting the “porn”, “neutral”, and “sexy” classes. We combine the “neutral”, and “sexy” classes into a single class of “safe” images.\\n\\nCyberbullying: Second, we used a cyberbullying im-ages (Vishwamitra et al. 2021) dataset consisting of nearly 20,000 images belonging to the classes “cyberbullying” and “non-cyberbullying”.\\n\\nSelf-Harm: Third, we used a self-harm images dataset (Bethany et al. 2023), consisting of 5000 images with classes “self-harm” and “non self-harm”.',\n",
       "  'conclusion': 'In this work, we have presented ConditionalVLM, a vi-sual reasoning framework that generates accurate rationales for unsafe image descriptions by leveraging state-of-the-art VLMs conditioned on pre-trained unsafe image classi-fiers, and CSE, a counterfactual visual explanation tech-nique to obfuscate the unsafe regions in unsafe images for safer sharing. We evaluated these two methods on three cat-egories of unsafe images. An implementation of Condition-alVLM, which we called ConditionalBLIP showed supe-rior performance compared to other state-of-the-art image-to-text models on describing unsafe images. We also com-pare CSE against another recent unsafe image obfuscation method and show how our approach is effective in generat-ing causal explanations for obfuscating unsafe images.',\n",
       "  'title': 'Image Safeguarding: Reasoning with Conditional Vision Language Model and Obfuscating Unsafe Content Counterfactually',\n",
       "  'author': 'Mazal Bethany, Brandon Wherry, Nishant Vishwamitra, Peyman Najafirad',\n",
       "  'textdata': \"Image Safeguarding: Reasoning with Conditional Vision Language Model\\nand Obfuscating Unsafe Content Counterfactually\\nMazal Bethany*1 2, Brandon Wherry*1 2, Nishant Vishwamitra1, Peyman Najafirad1 2 †\\n1University of Texas at San Antonio 2Secure AI and Autonomy Lab\\n{mazal.bethany, brandon.wherry, nishant.vishwamitra, peyman.najafirad}@utsa.edu\\nAbstract\\nSocial media platforms are being increasingly used by ma-\\nlicious actors to share unsafe content, such as images de-\\npicting sexual activity, cyberbullying, and self-harm. Con-\\nsequently, major platforms use artificial intelligence (AI)\\nand human moderation to obfuscate such images to make\\nthem safer. Two critical needs for obfuscating unsafe im-\\nages is that an accurate rationale for obfuscating image re-\\ngions must be provided, and the sensitive regions should be\\nobfuscated (e.g. blurring) for users’ safety. This process in-\\nvolves addressing two key problems: (1) the reason for ob-\\nfuscating unsafe images demands the platform to provide an\\naccurate rationale that must be grounded in unsafe image-\\nspecific attributes, and (2) the unsafe regions in the image\\nmust be minimally obfuscated while still depicting the safe\\nregions. In this work, we address these key issues by first\\nperforming visual reasoning by designing a visual reason-\\ning model (VLM) conditioned on pre-trained unsafe image\\nclassifiers to provide an accurate rationale grounded in un-\\nsafe image attributes, and then proposing a counterfactual\\nexplanation algorithm that minimally identifies and obfus-\\ncates unsafe regions for safe viewing, by first utilizing an\\nunsafe image classifier attribution matrix to guide segmen-\\ntation for a more optimal subregion segmentation followed\\nby an informed greedy search to determine the minimum\\nnumber of subregions required to modify the classifier’s out-\\nput based on attribution score. Extensive experiments on un-\\ncurated data from social networks emphasize the efficacy\\nof our proposed method. We make our code available at:\\nhttps://github.com/SecureAIAutonomyLab/ConditionalVLM\\nIntroduction\\nSocial media is being increasingly misused by bad actors\\nto share sexually explicit, cyberbullying, and self-harm\\ncontent (Hendricks 2021; Chelmis and Yao 2019; Adler and\\nChenoa Cooper 2022). However, social media platforms\\nare required by law to safeguard their users against such\\nimages (Exon 1996), as well as provide a rationale for\\nwhy such images are flagged (Gesley 2021; Cabral et al.\\n2021) for the purpose of transparency. In response, major\\nplatforms have deployed AI and human-based content mod-\\neration techniques to flag and obfuscate (i.e, make the image\\nsafer by blurring sensitive regions) such images (Bethany\\n*These authors contributed equally.\\n†Corresponding author\\net al. 2023). This process involves obfuscating (e.g. by\\nblurring or blocking) unsafe image regions in the image (Li\\net al. 2017) along with generating a rationale that backs up\\nthe decision to obfuscate the flagged images (Meta 2022).\\nThe image obfuscation process faces two critical prob-\\nlems regarding how much of the unsafe image is obfuscated\\nand why it is obfuscated: First, the decision to deem an\\nimage unsafe and obfuscate it demands providing a ratio-\\nnale for the decision. For example, Instagram moderators are\\nrequired to provide a legal rationale (Bronstein 2021; Are\\n2020) to back up their decision (Tenbarge 2023). Existing\\nvisual reasoning methods (Li et al. 2022, 2023; Dai et al.\\n2023) are severely limited for unsafe images such as sexu-\\nally explicit, cyberbullying, and self-harm since they cannot\\nprovide a rationale grounded in attributes that are specific\\nto such images, such as rude hand gestures in cyberbullying\\nimages (Vishwamitra et al. 2021), or sensitive body parts in\\nsexually-explicit images (Binder 2019). Second, the unsafe\\nimage needs minimal obfuscation while still depicting the\\nsafe regions for evidence collection and investigation (Billy\\nPerrigo 2019). For instance, human moderators need to de-\\ntermine the age of the person in the image (e.g., in child sex-\\nual abuse material (CSAM) investigations), look for identi-\\nfiers (e.g., tattoos, scars, and unique birthmarks), and deter-\\nmine their location information (e.g., landmarks, geograph-\\nical features, and recognizable surroundings). Current seg-\\nmentation techniques (Chandrasekaran et al. 2021; Vermeire\\net al. 2022; Bethany et al. 2023) cannot minimally identify\\nthe regions and consequently impede investigations that per-\\ntinently need full details of the remaining safe regions.\\nIn this work, we take the first step towards addressing\\na pertinent, but overlooked problem of the image moder-\\nation process in social media platforms. Our major objec-\\ntive is to first identify and minimally obfuscate the sensitive\\nregions in an unsafe image such that the safe regions are\\nunaltered to aid an investigation, and then provide an accu-\\nrate rationale for doing so, that is grounded in unsafe im-\\nage attributes (e.g., private body parts, rude gestures or hate-\\nful symbols). To this end, we address this problem in two\\nsteps: (1) we develop a novel unsafe image rationale genera-\\ntion method called ConditionalVLM (i.e., conditional vision\\nlanguage model) that leverages the state-of-the-art large lan-\\nguage models (LLM)-based vision language models (Fang\\net al. 2023) to perform an in-depth conditional inspection to\\narXiv:2401.11035v1  [cs.CV]  19 Jan 2024\\ngenerate an accurate rationale that is grounded in unsafe im-\\nage attributes; and (2) minimally obfuscating the sensitive\\nregions only by calculating the classifier attribution matrix\\nusing a FullGrad-based model (Srinivas and Fleuret 2019)\\nand then utilize this information to guide Bayesian super-\\npixel segmentation (Uziel, Ronen, and Freifeld 2019) for a\\nmore informed and optimal dynamic subregion segmenta-\\ntion, via calculating the attribution score of each subregion.\\nFinally, we utilize a discrete optimization technique such as\\ninformed greedy search to determine the minimum number\\nof subregions required to modify the classifier’s output, us-\\ning the score attribution.\\nOur work has profound implications for the safety of so-\\ncial media content moderators, by greatly reducing their\\nneed to view unsafe content (Steiger et al. 2021), social me-\\ndia users who are minors or sensitive to such content (Har-\\ngrave and Livingstone 2009), and law enforcement agents\\nwho need to investigate such images as part of their investi-\\ngation (Krause 2009). We make the following contributions:\\n• We develop ConditionalVLM, a visual reasoning model\\nthat generates accurate rationales for unsafe images by\\nleveraging state-of-the-art VLMs conditioned on pre-\\ntrained unsafe image classifiers.\\n• We develop a novel unsafe image content obfuscation\\nalgorithm that minimally obfuscates only the unsafe re-\\ngions while keeping the rest of the image unaltered for\\ninvestigations.\\n• Evaluations of our work show that it can categorize the\\nthree social media unsafe categories of images with an\\naccuracy of 93.9%, and minimally segment only the un-\\nsafe regions with an accuracy of 81.8%.\\nRelated Works\\nSafeguarding Images\\nSocial media platforms are constantly misused to share un-\\nsafe content. For example, sexually-explicit images (Ashurst\\nand McAlinden 2015; Sanchez et al. 2019), non-consensual\\nintimate images (NCII) (Lenhart, Ybarra, and Price-Feeney\\n2016) and child sexual abuse material (CSAM) are being in-\\ncreasingly shared on major social media platforms (Sanchez\\net al. 2019). Cyberbullying, a critical issue affecting ado-\\nlescents and adults is spread unabated via images (Vish-\\nwamitra et al. 2021). Furthermore, self-harm images are\\nwidely spread on these platforms that further alienate vulner-\\nable users (John et al. 2018). To defend against this threat,\\nsocial media platforms alter the unsafe image by blurring\\nthe sensitive regions. This obfuscation of such unsafe im-\\nages has substantial implications. For instance, social media\\nplatforms employ over a million moderators globally (bbc\\n2021), who manually view unsafe images which is known\\nto have an adverse effect on their mental health, includ-\\ning PTSD (reu 2021). Vulnerable users, such as minors also\\nneed image safeguarding methods that can shield them from\\nunsafe content. Furthermore, law enforcement agents who\\ninvestigate images from a crime scene need image safe-\\nguarding since such images contain extremely disturbing\\nimages, such as CSAM. These applications have a critical\\nneed, the unsafe regions must be minimally obfuscated. For\\ninstance, a moderator needs to view the safe regions to con-\\nduct investigations, such as determining the age of the per-\\nson to report CSAM content to law enforcement.\\nVision-Language Models\\nGiven the achievements of pre-trained models in computer\\nvision (CV) and natural language processing (NLP), numer-\\nous studies have attempted to pre-train large-scale models\\nthat incorporate both vision and language modalities. These\\nmodels are commonly referred to as Vision-Language\\nModels (VLM). One group of methods propose end-to-end\\napproaches for learning vision-language models. Works\\nsuch as CLIP (Radford et al. 2021) use a contrastive pre-\\ntraining process to jointly train an image and a text encoder\\non image-text pairs. Other works such as BEIT-3 (Wang\\net al. 2023) use multiway transformers for general-purpose\\nmodeling and carry out masked modeling on images, text,\\nand image-text pairs. Some methods use modular tech-\\nniques to utilize pre-existing models to interpret image data\\nand take advantage of established LLMs. One major hurdle\\nin these works is coordinating visual features within the\\nrealm of text. To accomplish this, works such as Flamingo\\n(Alayrac et al. 2022) add cross attention layers and train\\nthese layers on image-text pairs. Works such as BLIP-2 use\\na frozen image encoder and LLM and propose a querying\\ntransformer to bridge modalities (Li et al. 2023). Other\\nmethods such as LENS (Berrios et al. 2023) require no\\nadditional training, and develop visual vocabularies of\\nan image by collecting tags, attributes and captions from\\nvarious models. LENS then uses this visual vocabulary to\\ngenerate text prompts by which questions can be asked\\nabout an image on a frozen LLM. A major limitation\\nof these methods is that they do not have conditioning\\ncapability (Ramesh et al. 2022), a crucial requirement to\\nground the output in domain-specific attributes.\\nImage Segmentation and Counterfactual\\nExplanation for Obfuscation\\nAnother type of explanation that is growing in popularity\\ndue to its ability to address several of these issues is coun-\\nterfactual explanations (Wachter, Mittelstadt, and Russell\\n2017). A counterfactual explanation can be defined as tak-\\ning the form: a decision y was produced because variable X\\nhad values (v1, v2, . . . ) associated with it. If X instead had\\nvalues (v1′, v2′, . . . ), and all other variables had remained\\nconstant, score y′ would have been produced. Some works\\nsuch as BEN (Chandrasekaran et al. 2021), SEDC (Vermeire\\net al. 2022), and CSRA (Bethany et al. 2023) have explored\\nregion-based counterfactual visual explanations. However,\\nexisting approaches face two key challenges: 1. suboptimal\\nsubregion boundaries, leading to excessive parts of the im-\\nage being identified as causing a decision, and 2. high time\\ncomplexity 2K in searching for a counterfactual in an im-\\nage of K regions. BEN and SEDC segment an input image\\ninto K static subregions without any prior knowledge of the\\nclassifier, resulting in an uninformed search strategy for find-\\ning the counterfactual examples. While CSRA does use prior\\nCounterfactual \\nGeneration \\nAlgorithm\\nUsing Informed \\nSearch \\nSubobject\\nRegion \\nPartitioning \\nusing \\nAdaptive \\nSegmentation\\nBayesian \\nAdaptive \\nSuperpixel\\nSegmentation\\nFullGrad\\nConditional Image \\nInstruction-guided \\nTransformer\\n(CIIT)\\nPre-trained\\nLarge Language Model\\n(LLM) \\nInstruction (I)\\n(Z)\\nPre-trained Image \\nEncoder\\nSelect Highest \\nRegion\\nAttribution\\nDescription: In the image, a woman is\\nmaking\\nan\\noffensive\\ngesture,\\nsuch\\nas\\nflipping someone off, with her middle finger.\\nDoes the image \\ncontain potentially \\noffensive gestures \\nor symbols?\\nImage (X)\\n0.4\\n0.2\\n0.07\\n0.10\\n0.15\\nMasked Image (X’)\\nX\\nUnsafe \\nClassifier\\n(c) \\nX\\nConditional \\nFigure 1: Overview of the proposed architecture. The initial module utilizes ConditionalVLM for classifying images as safe or\\nunsafe, while the subsequent module proposes counterfactual visual explanations to identify and obfuscate the unsafe regions\\nwithin the image.\\nknowledge of the classifier to inform the search of the coun-\\nterfactual example, BEN, SEDC and CSRA do not jointly\\noptimize the subregions boundaries and minimize the num-\\nber of subregions, which is particularly important for ob-\\nfuscation applications where preserving as much context as\\npossible is preferred.\\nMethod\\nFigure 1 illustrates the architecture of our proposed ap-\\nproach, which consists of two modules. The initial module\\nproposes a conditional visual language model designed for\\nimage reasoning. The model classifies images as safe or un-\\nsafe by understanding the interactions or activities of enti-\\nties within the image, using its comprehension of visual fea-\\ntures and linguistic annotations. In the subsequent module,\\ncounterfactual visual explanations are proposed to precisely\\nidentify sub-object regions of the image contributing to its\\nunsafe classification for obfuscation.\\nConditional Vision-Language Model\\nWe introduce a framework that synergistically combines the\\nstrengths of large language models (LLMs) with the spe-\\ncific requirements of large image encoders. Additionally,\\nit provides more explicit control over visual features being\\nreasoned. The ConditianalVLM architecture is anchored by\\nthree pivotal components, as depicted in Figure 1:\\nA Large Pre-trained Image Encoder takes an image X as\\ninput and outputs a visual embedding representation of the\\nimage, Z = g(X). We explore a state-of-the-art pre-trained\\nvision transformer ViT-g/14 from EVA (Fang et al. 2023).\\nA Conditional Image Instruction-guided Transformer\\n(CIIT) employs contrastive language-image pre-training to\\nencode visual data in congruence with a specific language\\nprompt. Additionally, we condition this language prompt\\nusing pre-trained unsafe image classifiers. This allows the\\nmodel to match and parse the unsafe visual embedding\\neffectively, while also providing more explicit control over\\nunsafe visual features (Ramesh et al. 2022). CIIT utilizes\\na pre-trained Q-Former model (Li et al. 2023), which is\\nconditioned on image classifiers as control code c on unsafe\\nimage content such as sexually explicit, cyberbullying, and\\nself-harm.\\n• A prior p(I|c) that produces CIIT instruct prompt I con-\\nditioned on control code c.\\n• A transformer decoder p(L|I, c) that produces con-\\ntrastive embedding L conditioned on Instruct prompt I\\nand control code c.\\nThe transformer decoder allows us to invert images given\\ntheir CIIT Instruct prompt, while the prior allows us to learn\\na generative model of the image embeddings themselves.\\nTaking the product of these two components yields a gen-\\nerative model P(L|c) of embedding L given control c:\\np(L|c) = p(L, I|c) = p(L|I, c)p(I|c)\\n(1)\\nThe control code c provides a point of control over the\\nCIIT generation process. The distribution can be decom-\\nposed using the chain rule of probability and trained with\\na loss that takes the control code into account.\\np(L|c) =\\nn\\nY\\ni=1\\np(Li|L<i, c)\\n(2)\\nWe train the model with parameters θ to minimize the\\nnegative log-likelihood over a dataset D = X1, ..., Xn:\\nL(D) = −\\n|D|\\nX\\nk=1\\nlog pθ(Lk\\ni |L<i, ck)\\n(3)\\nA Pre-trained Large Language Model Decoder takes a\\ntext embedding L as input and outputs linguistic sentences\\nderived from the embedding, Text = LLM(L). We choose\\nVicuna (Vic 2023) as our LLM decoder which is constructed\\nupon LLaMA (Touvron et al. 2023) and can perform a wide\\nrange of complex linguistic tasks.\\nCounterfactual Subobject Explanations for\\nObfuscation\\nIn order to connect region attribution to provide counterfac-\\ntual subobject region explanation of an image, relative to a\\ngiven machine learning predictive model, we propose a two-\\nphase approach. The pipeline of the proposed approach is\\nillustrated in Figure 1. We first partition the image into non-\\nintersecting subobject regions and measuring region attribu-\\ntion value to each region using gradient attribution maps in\\nSection 3.1 and Section 3.2. The counterfactual analysis of\\nalternate versions of the image using a greedy search algo-\\nrithm using regions with highest attribution values for coun-\\nterfactual analysis is followed in Section 3.3.\\nSubobject Region Partitioning using Adaptive Segmen-\\ntation. We represent a given image,Xas a non-intersecting\\nset of K regions given by {z1, z2, · · · , zK}. The boundaries\\nof these regions are defined by clustering algorithms that use\\ncolor and spatial information and are called superpixels. Let\\nZ represent the K region segmentation, zi represent the la-\\nbel assigned to Xi and j represent the label of some arbitrary\\ncluster. An image must be segmented into meaningful sub-\\nobject regions in order to allow for a counterfactual analysis\\nof the image by the binary predictive model f(X) → 0, 1.\\nThese regions serve as the features that are analyzed in\\nthe counterfactual analysis. To maximize the efficiency of\\na counterfactual analysis, we require an adaptive segmen-\\ntation method. Many segmentation methods are wasteful\\nin their assignment of many segments to uninformative re-\\ngions, while not segmenting detailed regions enough. Such\\na method should be able to respect pixel connectivity and\\nspatial coherence and requires an adaptive number of re-\\ngions. K-means based clustering methods are a fast and sim-\\nple basis for leading segmentation, however Gaussian Mix-\\nture Models (GMM) may be better suited for an adaptive\\nsegmentation method since we need to capture the hetero-\\ngeneity in the pixel distribution of various types of images.\\nLet N = h ∗ w be the number of pixels in an image, X\\nwith c color channels. The values attributed to the pixels in\\nX can be denoted as Xi = (li, ci) ∈ R5, where li ∈ R2 rep-\\nresent the x, y coordinate location and ci ∈ R3 represent the\\nRGB color information. Superpixel clustering methods with\\nspatial coherence aim to partition (Xi)N\\ni=1 into K disjoint\\ngroups. Let Z represent the K region segmentation, zi rep-\\nresent the label assigned to Xi and j represent the label of\\nsome arbitrary cluster. Where N(X; µj, Σj) is a Gaussian\\nPDF with mean µj and a covariance matrix Σj of size n ∗ n,\\nthe PDF of a GMM with K components is\\np(X; (µj, Σj, λj)K\\nj=1) =\\nK\\nX\\nj=1\\nλjN(X|µj, Σj)\\nThe mixing coefficients λj in the PDF of a GMM form a\\nconvex combination where:\\nK\\nX\\nj=1\\nλj = 1, λj ≥ 0\\n∀j\\nand this allows for a globally optimal clustering. Given a\\nGaussian distribution j where θj = (µj, Σj), a Bayesian\\nGMM with random variables (θj)K\\nj=1 and (λj)K\\nj=1 are drawn\\nfrom p((θj, λj)K\\nj=1), a prior distribution. Assuming inde-\\npendence, the prior distribution can be factorized as follows\\np((θj, λj)K\\nj=1) = p((λj)K\\nj=1)\\nK\\nY\\nj=1\\np(θj)\\nUsing a Normal-Inverse Wishart (NIW) for p(θj) and a\\nDirichlet distribution for p((λj)K\\nj=1) gives us posterior dis-\\ntributions in the same form as the priors. Furthermore, the\\nupdates from the priors are given in closed form.\\nThe Bayesian GMM inference to calculate Z can be done\\nby performing Gibbs sampling, alternating between the fol-\\nlowing equations:\\np((λj)K\\nj=1|Z, (Xi)N\\ni=1)\\nK\\nY\\nj=1\\np((θj, λj)|Z, (Xi)N\\ni=1)\\np(Z|(θj, λj)K\\nj=1, (Xi)N\\ni=1)\\nSubobject Region Attribution Value. We start by creating\\nthe FullGrad (Srinivas and Fleuret 2019) attribution map for\\nimage feature attribution. Given an image X and the fea-\\nture maps generated by the FullGrad L[u, v] of width u and\\nheight v for the model prediction, the goal of the visual at-\\ntention model is to identify the discriminative regions of the\\nimage that significantly influence the class prediction score\\nof the predictive model using L[u, v] pixel attribution values.\\nThe attribution map of the FullGrad method is generated\\nby propagating an image through a CNN, obtaining the out-\\nput score before the softmax layer, and then computing the\\ngradients with respect to the input (input-gradients) and the\\nbiases at each layer (bias-gradients). These gradients are\\nthen combined, with each bias-gradient reshaped to match\\nthe input dimensionality and all gradients summed to form\\nthe FullGrad attribution map.\\nFullGrad Definition: Consider a CNN model f, with x de-\\nnoting the input and b denoting the biases at each layer, c\\nrepresenting the channels of layer k. Furthermore, given an\\noutput of interest f(x), and a postprocessing operator ψ(·)\\nthe FullGrad attribution map LF ullGrad is defined as:\\nLF ullGrad = ψ(∇xf(x) ⊙ x) +\\nX\\nk∈K\\nX\\nc∈ck\\nψ(f b(x)c)\\nTo facilitate an efficient sampling of regions in the coun-\\nterfactual analysis, we utilize the FullGrad attribution map.\\nDefinition 1: (Subobject Region Attribution Score) Us-\\ning the attribution map of model f(X) and the subobject\\nregions {z1, z2, · · · , zK} created by adaptive segmentation\\nfor the input image X, we define the subobject region attri-\\nbution score, {s1, s2, · · · , sK} as follows:\\nsk =\\n1\\nn.m\\nX\\nn\\nX\\nm\\nLF ullGrad(F,X)[i, j], X[i, j] ∈ zk\\nAlthough feature attributions highlight features that are\\nsignificant in terms of how they affect the model’s ability to\\npredict, they do not indicate that altering significant features\\nwould result in a different desired outcome.\\nDefinition 2: (Subobject Region Confidence Reduc-\\ntion) Given a model Y = f(X) that takes an image X\\nwith subobject regions X = [z0, z1, ..., zn]T and outputs a\\nprobability distribution Y. The confidence reduction crk of\\nsubobject region zk, (k ∈ [1, n]) towards Y is the change of\\nthe output by masking the k-th subobject region of X, while\\nbeing classified as the same class, as follows:\\ncrk = f(X) − f(X ◦ Mask(zk))\\nIn Sec 3.3, we present our greedy region search algorithm\\nwhich utilizes subobject region attribution score as heuris-\\ntics and employs confidence level for causal obfuscation us-\\ning counterfactual subobject region explanations.\\nCounterfactual Generation Using Informed Subobject\\nRegion Search. The previous sections lead us to the mini-\\nmum region masking problem. This can be computationally\\nexpensive to solve, as it requires the masking and analysis of\\n2K different combination of regions, Z of X based on Sec-\\ntion 3.1 . Rather than solving the problem directly, we find\\nan approximate solution using a greedy region search.\\nGiven a predictive model f : X → 0, 1, we can define the\\nset of counterfactual explanations for an input x ∈ X as x′\\nwhile argmin x′d(x, x′) and x′ = {x ∈ X|f(x) ̸= f(x′)}.\\nIn other words x′ = {x ∈ X|f(x) ̸= f(x′)} , contains\\nall the inputs x for which the model f returns a prediction\\ndifferent from f(x) while minimizing the distance between\\nx and x′.\\nOur greedy region search, starts with us first sorting the\\nK regions in descending order by the average attribution\\nfor each region which were calculated in subsection . The\\ngreedy region search considers a subset of regions k ∈ K. k\\nbegins with the top region by average attribution and itera-\\ntively expands to the top two regions by average attribution\\nand so on until an x′ is found such that f(x′) ̸= f(x).\\nExperimental Evaluation\\nDatasets\\nWe evaluated our Conditional VLM and Counterfactual\\nSubobject Explanation methods on three datasets of real-\\nworld harmful images to study the practical application of\\ncounterfactual subobject explanations.\\nSexually Explicit: First, we sampled a subset of images\\nfrom an NSFW images dataset (Kim 2021) consisting of\\n334,327 images by selecting the “porn”, “neutral”, and\\n“sexy” classes. We combine the “neutral”, and “sexy”\\nclasses into a single class of “safe” images.\\nCyberbullying: Second, we used a cyberbullying im-\\nages (Vishwamitra et al. 2021) dataset consisting of nearly\\n20,000 images belonging to the classes “cyberbullying” and\\n“non-cyberbullying”.\\nSelf-Harm: Third, we used a self-harm images dataset\\n(Bethany et al. 2023), consisting of 5000 images with classes\\n“self-harm” and “non self-harm”.\\nEvaluation Settings\\nConditionalVLM. We compare our proposed method\\nagainst other state-of-the-art image-to-text models such as\\nInstructBLIP (Dai et al. 2023), OFA-Large (Wang et al.\\n2022), and mPLUG (Li et al. 2022). We use the implementa-\\ntions of these methods from HuggingFace. For InstructBLIP,\\nwe\\nuse\\nInstructBLIP-Vicuna-13b\\nwith\\nnum beams=5,\\nmax length=512,\\nmin length=1,\\ntop p=0.9,\\nrepeti-\\ntion penalty=1.5, length penalty=1.0, and temperature=1.\\nThe image encoder for this implementation of InstructBLIP\\nwas Vit-g/14 (Fang et al. 2023). For mPLUG, we use the\\nparameters do sample=True, top k=5, and max length=512.\\nFor\\nOFA\\nwe\\nuse\\nthe\\nparameters\\nof\\nnum beams=5,\\nno repeat ngram size=3.\\nTo\\ndemonstrate\\nour\\nCondi-\\ntionalVLM\\nframework,\\nwe\\nmodify\\nthe\\nInstructBLIP-\\nVicuna-13b architecture to include a CIIT, which we call\\nConditionalBLIP. All experiments were carried out on a\\nDGX 8x A100 GPU, with 80GB of VRAM each.\\nWe fine-tuned a ResNet-50 classifier available in Pytorch\\n(Paszke et al. 2019) using pre-trained model weights trained\\nfrom the ImageNet dataset (Deng et al. 2009). The NSFW,\\ncyberbullying and self-harm datasets were each divided into\\ntrain, validation, and test sets, with 80% being allocated to\\nthe train set, and 10% each allocated to validation and test\\nsets. We trained the models for 50 epochs and selected the\\nmodels that have the highest classification accuracies on the\\nvalidation sets. These models achieved accuracies of 98.9%,\\n91.9% and 97.6% respectively on the test set in our exper-\\niments. We use these classifiers as the control code for the\\nCIIT in ConditionalBLIP.\\nCounterfactual Subobject Explanations for Obfuscation.\\nTo test different segmentation methods, we experimented\\nwith SLIC (Achanta et al. 2010), Felzenszwalb (Felzen-\\nszwalb and Huttenlocher 2004), and Compact Watershed\\n(Neubert and Protzel 2014) segmentation methods imple-\\nmented in the scikit-image library (van der Walt et al. 2014),\\nSegment Anything Model (SAM) (Kirillov et al. 2023), and\\nBayesian Adaptive Superpixel Segmentation (Uziel, Ronen,\\nand Freifeld 2019). For our experiments, we selected the fol-\\nlowing parameters for each segmentation method: for SLIC,\\nwe chose the number of segments to be 25 and compactness\\nequal to 1; for Felzenszwalb we selected the scale to be 500,\\nsigma to be 0.5, and a minimum component size of 200; for\\nCompact Watershed, we chose the number of markers to be\\n25 and the compactness parameter to be 0.001.\\nFigure 2: Examples of segmentation methods on a cyberbul-\\nlying image. From top to bottom: (1) BASS, (2) SLIC, (3)\\nSAM.\\nWe used the following attribution map methods in our\\nexperiments: (Grad-CAM (Selvaraju et al. 2017), XGrad-\\nCAM (Fu et al. 2020), Grad-CAM ++ (Chattopadhay et al.\\n2018), FullGrad (Srinivas and Fleuret 2019), and Ablation-\\nCAM (Ramaswamy et al. 2020)). For the implementation of\\nthe attribution map methods, we use the Pytorch Grad-CAM\\nlibrary (Gildenblat and contributors 2021).\\nEvaluation Metrics\\nConditionalVLM. We evaluate VLM’s ability to investi-\\ngate three different unsafe image categories in two phases.\\nIn the first phase, we conduct a coarse-grained evaluation\\nby having human evaluators determine based off of the im-\\nage descriptions produced by the VLM whether a moderator\\nshould be able to understand which dataset of unsafe im-\\nage the image belongs to. In this evaluation, a team of three\\nhuman evaluators who were involved in this research were\\nasked to evaluate whether these descriptions produced by\\nthe VLM on the questions of ”What is happening in the im-\\nage?”, and ”What are the people doing?” were sufficient to\\naccurately categorize them into the correct dataset that the\\nunsafe image the image belongs to. The final labels were\\nassigned by majority voting.\\nIn the second phase, we conduct a fine-grained evaluation\\nby having human evaluators evaluate the responses of the\\nVLM to curated moderator questions with respect to an un-\\nsafe image image. These fine-grained questions ask about\\nspecific attributes of images relating to the unsafe image\\ncategories. In this evaluation, the same team of evaluators\\nwere asked to determine whether the answers produced by\\nthe VLM correctly answered these curated questions.\\nCounterfactual Subobject Explanations for Obfuscation.\\nWe investigate the ability of CSE to generate a successful\\ncounterfactual explanation on an unsafe image X to satisfy\\ntwo requirements: (1) the generated counterfactual example\\nX′ must be a convincing representation of another class such\\nthat it has a softmax score greater than a threshold T on an-\\nother class, and (2) the search space that the counterfactual\\nexample X′ exists in must be found by searching N or fewer\\ndifferent regions. Since, there are 2K different combinations\\nof regions to be analyzed in X with K number of regions, we\\nlimit the search space to a certain number of regions in our\\nevaluation. In our experiments on unsafe images, we select\\nthe threshold for softmax score T to be 0.5 and the threshold\\nDataset\\nModel\\nHuman Evaluation\\nConditionalBLIP\\n94.5\\nSexually Explicit\\nInstructBLIP\\n80.0\\nmPLUG\\n73.5\\nOFA-Large\\n13.5\\nConditionalBLIP\\n99.5\\nCyberbullying\\nInstructBLIP\\n23.0\\nmPLUG\\n16.0\\nOFA-Large\\n10.0\\nConditionalBLIP\\n99.0\\nSelf-Harm\\nInstructBLIP\\n47.0\\nmPLUG\\n51.5\\nOFA-Large\\n17.0\\nTable 1: Quantitative results of VLM on coarse-grained im-\\nage descriptions.\\nDataset\\nQuestions\\nInstructBLIP\\nConditionalBLIP\\nDoes the image contain any\\nsexual content or nudity?\\n61.1\\n99.2\\nSexually Explicit\\nDoes the image contain ex-\\nplicit sexual content?\\n37.9\\n99.9\\nDoes the image contain po-\\ntentially offensive gestures\\nor symbols?\\n59.1\\n80.0\\nDoes the image contain any\\nweapons?\\n99.2\\n94.0\\nCyberbullying\\nIs there any aggressive be-\\nhavior shown in the image?\\n56.4\\n98.3\\nIs there any text in the im-\\nage that might be considered\\nhate speech?\\n57.2\\n95.2\\nAre there any depictions of\\nself-harm in the image?\\n27.4\\n98.9\\nSelf-Harm\\nAre there visible signs of\\nself-bruising\\nor\\nself-harm\\nthrough physical trauma?\\n74.2\\n99.2\\nDoes\\nthe\\nimage\\ncontain\\nsymbols\\nassociated\\nwith\\nself-destructive behavior?\\n30.2\\n81.4\\nTable 2: Quantitative results of VLM on fine-grained mod-\\nerator questions.\\nfor regions to be 10.\\nResults and Discussion\\nConditionalVLM. The results for the coarse-grained eval-\\nuations of the VLM are shown in Table 1. In this table, we\\npresent the accuracy of four models, including our model,\\nConditionalBLIP, that convert images to text, specifically\\nfocusing on their ability to identify unsafe attributes in im-\\nages based on generic questions. In this experiment, a total\\nof 2000 unsafe image samples from each category of unsafe\\nimage datasets were tested. The results show that Condi-\\ntionalBLIP is able to significantly outperform other state-\\nof-the-art models in identifying the unsafe image attributes\\nof unsafe images, simply from asking generic questions on\\nthe image, with an average correct identification accuracy\\nof 98% of unsafe image attributes across the three datasets.\\nCompared to the 50% accuracy by InstructBLIP, 47% by\\nmPLUG, and 13.5% by OFA-Large, we observe that exist-\\ning models are insufficient for describing unsafe images.\\nWe present the questions and quantitative results of the\\nfine-grained evaluation of ConditionalBLIP in Table 2.\\nWe compare ConditionalBLIP against InstructBLIP, which\\nshowed the best coarse-grained results compared to other\\nmethods that were evaluated in Table 1. Furthermore, the\\nInstructBLIP model is the most similar in implementation to\\nthe ConditionalBLIP model, where the primary difference\\nis the usage of the CIIT in ConditionalBLIP. In Table 2, we\\npresent the question posed to the VLM, alongside the de-\\ntection accuracy of InstructBLIP and ConditionalBLIP on\\nthese questions. The fine-grained evaluation shows that im-\\nage conditioning significantly enhances VLMs ability to un-\\nderstand unsafe images, with an average improvement in ac-\\ncuracy of 38.2% across the questions. The comparison be-\\ntween the performances of InstructBLIP and Conditional-\\nBLIP reveals significant differences in their respective abil-\\nities to identify and describe unsafe content in visual data.\\nFor example, given a question on a randomly selected cy-\\nberbullying image, specifically asking “What are the people\\ndoing?”, the responses from the two models were notice-\\nably distinct. InstructBLIP stated, “The people in the image\\nare posing for a photograph,” a general and incorrect analy-\\nsis that fails to capture the offensive nature of the image. In\\ncontrast, ConditionalBLIP accurately identified the behav-\\nior, stating “In the image, a woman is making an offensive\\ngesture, such as flipping someone off, with her middle fin-\\nger.” This response was consistent with the actual content of\\nthe image. By employing contrastive language-image pre-\\ntraining and conditioning the language prompt using pre-\\ntrained unsafe image classifiers, ConditionalBLIP is able to\\nparse the unsafe visual embedding effectively.\\nCounterfactual Subobject Explanations for Obfuscation.\\nFor the counterfactual image obfuscation experiments, we\\ntest on 585 sexually explicit, cyberbullying and self-harm\\nimages. We compare our method against the CSRA method,\\nsetting numROI = 10 to match time complexity. Previous\\nwork showed gradient-based attribution maps were unsuit-\\nable for obfuscating unsafe images (Bethany et al. 2023).\\nOur trained models show improvements of 13.9% on sex-\\nually explicit, 22.0% on cyberbullying, and 39.5% on self-\\nharm images when comparing CSRA vs CSE.\\nWe tested various attribution map methods with BASS\\n(Uziel, Ronen, and Freifeld 2019) as the constant segmenta-\\ntion method on unsafe image samples, with results in Table\\n3. The average search space required to find a counterfactual\\nexample was presented, showing that different attribution\\nmap methods do not significantly impact CSE, with most\\ngenerating similar highest average attribution scores in sim-\\nilar areas. The exception was the FullGrad method, which\\nprovided slightly more successful counterfactual examples,\\nbetter average search space, and fewer obfuscated regions.\\nThis can be attributed to FullGrad’s more dispersed attri-\\nbutions across the image, which does not restrict the search\\nspace as much, and its unique method of satisfying local\\nand global importance by aggregating information from\\nboth input-gradient and intermediate bias-gradients, thus\\naiding CSE in finding suitable counterfactual explanations\\nmore readily.\\nWe tested different segmentation methods with FullGrad\\nas the constant attribution map method on unsafe image\\nsamples, and the results are in Table 4. The choice of seg-\\nmentation method significantly impacted the number of suc-\\ncessful counterfactual explanations, average search space,\\nand average number of regions obfuscated. BASS was the\\nDataset\\nAttribution Map\\nCounterfactual\\nAvg Depth\\nAvg Obfuscation\\nFullGrad\\n90.6\\n5.8\\n35.0\\nAblation-CAM\\n90.6\\n5.8\\n35.2\\nSexually Explicit\\nGrad-CAM\\n90.6\\n5.8\\n35.2\\nGrad-CAM++\\n90.6\\n5.8\\n35.2\\nXGrad-CAM\\n90.6\\n5.8\\n35.2\\nFullGrad\\n82.0\\n5.2\\n35.2\\nAblation-CAM\\n79.5\\n5.1\\n34.2\\nCyberbullying\\nGrad-CAM\\n79.5\\n5.1\\n34.2\\nGrad-CAM++\\n79.5\\n5.1\\n34.2\\nXGrad-CAM\\n79.5\\n5.1\\n34.2\\nFullGrad\\n72.8\\n5.6\\n50.1\\nAblation-CAM\\n72.8\\n5.6\\n50.1\\nSelf-Harm\\nGrad-CAM\\n72.8\\n5.6\\n50.1\\nGrad-CAM++\\n72.8\\n5.6\\n50.1\\nXGrad-CAM\\n72.8\\n5.6\\n50.1\\nTable 3: Quantitative results of CSE using different attribu-\\ntion map methods.\\nDataset\\nAttribution Map\\nCounterfactual\\nAvg Depth\\nAvg Obfuscation\\nBASS\\n90.6\\n5.8\\n35.0\\nSLIC\\n76.6\\n7.6\\n33.0\\nSexually Explicit\\nFelzenswalb\\n19.9\\n7.5\\n12.2\\nWatershed\\n51.2\\n7.9\\n31.9\\nSAM\\n29.5\\n7.4\\n33.2\\nBASS\\n82.0\\n5.2\\n35.2\\nSLIC\\n60.0\\n6.3\\n25.9\\nCyberbullying\\nFelzenswalb\\n20.5\\n6.3\\n17.6\\nWatershed\\n50.0\\n6.6\\n23.9\\nSAM\\n50.0\\n6.6\\n40.2\\nBASS\\n72.8\\n5.6\\n50.1\\nSLIC\\n33.4\\n6.6\\n26.3\\nSelf-Harm\\nFelzenswalb\\n38.4\\n6.5\\n47.5\\nWatershed\\n33.1\\n6.8\\n24.6\\nSAM\\n39.5\\n6.2\\n70.6\\nTable 4: Quantitative results of CSE on different segmenta-\\ntion methods.\\nmost effective, with a combination of BASS and FullGrad\\nyielding 81.8% successful counterfactual examples, a search\\ndepth of 5.5, and an average of 40.1% of the image obfus-\\ncated. The segmentation’s effect on counterfactual examples\\ncan be seen in Figure 2, and as Table 4 showed, methods like\\nBASS are key for successful counterfactual explanations, as\\nthey break the image into non-intersecting, color and spa-\\ntially coherent subobjects.\\nAblation Study. To evaluate our vision-language model’s\\nconditioning, we conducted an ablation study by changing\\nunsafe classifier guidance on the Image Instruction-guided\\nTransformer or CIIT model’s instruct prompt embedding\\nfrom 1 to 0. This conditioning on zero-shot instruct em-\\nbeddings yielded acceptable results for unsafe images by\\nallowing CIIT to match and parse the unsafe visual embed-\\nding effectively, while also providing more explicit control\\nover unsafe visual feature correlation with conditioned\\ninstruct prompt. For instance, the LLM decoder’s output for\\nan unsafe image changed to suggest “women are performing\\npotential erotic dance in a bar” vs. “women dancing in a\\nbar”. These results suggest that conditioning is a promising\\napproach for vision language models. Further details are in\\nAppendix [ConditioningVLM]. Future research could ex-\\nplore alternative safeguard mechanisms for vision language\\nmodels.\\nConclusion\\nIn this work, we have presented ConditionalVLM, a vi-\\nsual reasoning framework that generates accurate rationales\\nfor unsafe image descriptions by leveraging state-of-the-\\nart VLMs conditioned on pre-trained unsafe image clas-\\nsifiers, and CSE, a counterfactual visual explanation tech-\\nnique to obfuscate the unsafe regions in unsafe images for\\nsafer sharing. We evaluated these two methods on three cat-\\negories of unsafe images. An implementation of Condition-\\nalVLM, which we called ConditionalBLIP showed supe-\\nrior performance compared to other state-of-the-art image-\\nto-text models on describing unsafe images. We also com-\\npare CSE against another recent unsafe image obfuscation\\nmethod and show how our approach is effective in generat-\\ning causal explanations for obfuscating unsafe images.\\nAcknowledgments\\nThis research project and the preparation of this publi-\\ncation were funded in part by the Department of Home-\\nland Security (DHS), United States Secret Service, National\\nComputer Forensics Institute (NCFI) via contract number\\n70US0920D70090004 and by NSF Grant No. 2245983.\\nReferences\\n2021. Facebook moderator: ‘Every day was a nightmare’.\\nhttps://www.bbc.com/news/technology-57088382.\\n2021.\\nJudge OKs $85 mln settlement of Facebook mod-\\nerators’ PTSD claims.\\nhttps://www.reuters.com/legal/\\ntransactional/judge-oks-85-mln-settlement-facebook-\\nmoderators-ptsd-claims-2021-07-23/.\\n2023. Vicuna. https://github.com/lm-sys/FastChat.\\nAchanta, R.; Shaji, A.; Smith, K.; Lucchi, A.; Fua, P.; and\\nS¨usstrunk, S. 2010. Slic superpixels. Technical report.\\nAdler, R. A.; and Chenoa Cooper, S. 2022. “When a Tornado\\nHits Your Life:” Exploring Cyber Sexual Abuse Survivors’\\nPerspectives on Recovery. Journal of Counseling Sexology\\n& Sexual Wellness: Research, Practice, and Education, 4(1):\\n1–8.\\nAlayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Has-\\nson, Y.; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.;\\net al. 2022. Flamingo: a visual language model for few-shot\\nlearning. Advances in Neural Information Processing Sys-\\ntems, 35: 23716–23736.\\nAre, C. 2020.\\nHow Instagram’s algorithm is censoring\\nwomen and vulnerable users but helping online abusers.\\nFeminist media studies, 20(5): 741–744.\\nAshurst, L.; and McAlinden, A.-M. 2015. Young people,\\npeer-to-peer grooming and sexual offending: Understanding\\nand responding to harmful sexual behaviour within a social\\nmedia society. Probation Journal, 62(4): 374–388.\\nBerrios, W.; Mittal, G.; Thrush, T.; Kiela, D.; and Singh,\\nA. 2023. Towards Language Models That Can See: Com-\\nputer Vision Through the LENS of Natural Language. arXiv\\npreprint arXiv:2306.16410.\\nBethany, M.; Seong, A.; Silva, S. H.; Beebe, N.; Vishwami-\\ntra, N.; and Najafirad, P. 2023. Towards targeted obfuscation\\nof adversarial unsafe images using reconstruction and coun-\\nterfactual super region attribution explainability.\\nIn 32nd\\nUSENIX Security Symposium (USENIX Security 23), 643–\\n660.\\nBilly Perrigo. 2019.\\nFacebook Says It’s Removing More\\nHate Speech Than Ever Before. But There’s a Catch.\\nBinder, M. 2019. Facebook claims its new AI technology\\ncan automatically detect revenge porn.\\nhttps://mashable.\\ncom/article/facebook-ai-tool-revenge-porn.\\nBronstein, C. 2021. Deplatforming sexual speech in the age\\nof FOSTA/SESTA. Porn Studies, 8(4): 367–380.\\nCabral, L.; Haucap, J.; Parker, G.; Petropoulos, G.; Valletti,\\nT. M.; and Van Alstyne, M. W. 2021. The EU digital mar-\\nkets act: a report from a panel of economic experts. Cabral,\\nL., Haucap, J., Parker, G., Petropoulos, G., Valletti, T., and\\nVan Alstyne, M., The EU Digital Markets Act, Publications\\nOffice of the European Union, Luxembourg.\\nChandrasekaran, J.; Lei, Y.; Kacker, R.; and Kuhn, D. R.\\n2021. A combinatorial approach to explaining image classi-\\nfiers. In 2021 IEEE International Conference on Software\\nTesting, Verification and Validation Workshops (ICSTW),\\n35–43. IEEE.\\nChattopadhay, A.; Sarkar, A.; Howlader, P.; and Balasub-\\nramanian, V. N. 2018. Grad-cam++: Generalized gradient-\\nbased visual explanations for deep convolutional networks.\\nIn 2018 IEEE winter conference on applications of computer\\nvision (WACV), 839–847. IEEE.\\nChelmis, C.; and Yao, M. 2019. Minority report: Cyberbul-\\nlying prediction on Instagram. In Proceedings of the 10th\\nACM conference on web science, 37–45.\\nDai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.;\\nLi, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards\\nGeneral-purpose Vision-Language Models with Instruction\\nTuning. arXiv:2305.06500.\\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\\nFei, L. 2009. Imagenet: A large-scale hierarchical image\\ndatabase. In 2009 IEEE conference on computer vision and\\npattern recognition, 248–255. Ieee.\\nExon, J. 1996. The Communications Decency Act. Federal\\nCommunications Law Journal, 49(1): 4.\\nFang, Y.; Wang, W.; Xie, B.; Sun, Q.; Wu, L.; Wang, X.;\\nHuang, T.; Wang, X.; and Cao, Y. 2023.\\nEva: Exploring\\nthe limits of masked visual representation learning at scale.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, 19358–19369.\\nFelzenszwalb, P. F.; and Huttenlocher, D. P. 2004. Efficient\\ngraph-based image segmentation. International journal of\\ncomputer vision, 59(2): 167–181.\\nFu, R.; Hu, Q.; Dong, X.; Guo, Y.; Gao, Y.; and Li, B. 2020.\\nAxiom-based Grad-CAM: Towards Accurate Visualization\\nand Explanation of CNNs. In BMVC.\\nGesley, J. 2021.\\nGermany: Network Enforcement Act\\nAmended to Better Fight Online Hate Speech.\\nIn Li-\\nbrary of Congress, at: https://www. loc. gov/item/global-\\nlegal-monito r/2021-07-06/germany-network-enforcement-\\nact-amended-to-better-fight-online-hat e-speech/#:˜: text=\\nArticle%\\n20Germany%\\n3A%\\n20Network%\\n20Enforce-\\nment% 20Act, fake% 20news% 20in% 20social% 20net-\\nworks.\\nGildenblat, J.; and contributors. 2021. PyTorch library for\\nCAM methods.\\nhttps://github.com/jacobgil/pytorch-grad-\\ncam.\\nHargrave, A. M.; and Livingstone, S. M. 2009. Harm and\\noffence in media content: A review of the evidence.\\nHendricks, T. 2021. Cyberbullying increased 70% during\\nthe pandemic; Arizona schools are taking action.\\nhttps:\\n//www.12news.com/article/news/crime/cyberbullying-\\nincreased-70-during-the-pandemic-arizona-schools-are-\\ntaking-action/75-fadf8d2c-cf11-43f0-b074-5de485a3247d.\\nJohn, A.; Glendenning, A. C.; Marchant, A.; Montgomery,\\nP.; Stewart, A.; Wood, S.; Lloyd, K.; Hawton, K.; et al. 2018.\\nSelf-harm, suicidal behaviours, and cyberbullying in chil-\\ndren and young people: Systematic review. Journal of Med-\\nical Internet Research, 20(4).\\nKim, A. 2021.\\nNSFW Data Scraper.\\nhttps://github.com/\\nalex000kim/nsfw data scraper.\\nKirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.;\\nGustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo,\\nW.-Y.; et al. 2023.\\nSegment anything.\\narXiv preprint\\narXiv:2304.02643.\\nKrause, M. 2009. Identifying and managing stress in child\\npornography and child exploitation investigators. Journal of\\nPolice and Criminal Psychology, 24(1): 22–29.\\nLenhart, A.; Ybarra, M.; and Price-Feeney, M. 2016. Non-\\nconsensual image sharing: one in 25 Americans has been a\\nvictim of” revenge porn”.\\nLi, C.; Xu, H.; Tian, J.; Wang, W.; Yan, M.; Bi, B.; Ye,\\nJ.; Chen, H.; Xu, G.; Cao, Z.; et al. 2022.\\nmPLUG: Ef-\\nfective and Efficient Vision-Language Learning by Cross-\\nmodal Skip-connections. In Proceedings of the 2022 Con-\\nference on Empirical Methods in Natural Language Pro-\\ncessing, 7241–7259.\\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023.\\nBlip-2:\\nBootstrapping language-image pre-training with frozen im-\\nage encoders and large language models.\\narXiv preprint\\narXiv:2301.12597.\\nLi, Y.; Vishwamitra, N.; Knijnenburg, B. P.; Hu, H.; and\\nCaine, K. 2017.\\nEffectiveness and users’ experience of\\nobfuscation as a privacy-enhancing technology for sharing\\nphotos. Proceedings of the ACM on Human-Computer In-\\nteraction, 1(CSCW): 1–24.\\nMeta.\\n2022.\\nAppealed\\nContent.\\nhttps://transparency.fb.com/policies/improving/appealed-\\ncontent-metric/.\\nNeubert, P.; and Protzel, P. 2014. Compact watershed and\\npreemptive slic: On improving trade-offs of superpixel seg-\\nmentation algorithms. In 2014 22nd international confer-\\nence on pattern recognition, 996–1001. IEEE.\\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,\\nL.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison,\\nM.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai,\\nJ.; and Chintala, S. 2019. PyTorch: An Imperative Style,\\nHigh-Performance Deep Learning Library. In Wallach, H.;\\nLarochelle, H.; Beygelzimer, A.; d'Alch´e-Buc, F.; Fox, E.;\\nand Garnett, R., eds., Advances in Neural Information Pro-\\ncessing Systems 32, 8024–8035. Curran Associates, Inc.\\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\\net al. 2021. Learning transferable visual models from nat-\\nural language supervision. In International conference on\\nmachine learning, 8748–8763. PMLR.\\nRamaswamy, H. G.; et al. 2020. Ablation-cam: Visual ex-\\nplanations for deep convolutional network via gradient-free\\nlocalization. In Proceedings of the IEEE/CVF Winter Con-\\nference on Applications of Computer Vision, 983–991.\\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen, M.\\n2022. Hierarchical text-conditional image generation with\\nclip latents. arXiv preprint arXiv:2204.06125.\\nSanchez, L.; Grajeda, C.; Baggili, I.; and Hall, C. 2019. A\\npractitioner survey exploring the value of forensic tools, AI,\\nfiltering, & safer presentation for investigating child sexual\\nabuse material (CSAM). Digital Investigation, 29.\\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\\nParikh, D.; and Batra, D. 2017. Grad-cam: Visual explana-\\ntions from deep networks via gradient-based localization. In\\nProceedings of the IEEE international conference on com-\\nputer vision, 618–626.\\nSrinivas, S.; and Fleuret, F. 2019. Full-gradient representa-\\ntion for neural network visualization. Advances in neural\\ninformation processing systems, 32.\\nSteiger, M.; Bharucha, T. J.; Venkatagiri, S.; Riedl, M. J.;\\nand Lease, M. 2021. The psychological well-being of con-\\ntent moderators: the emotional labor of commercial moder-\\nation and avenues for improving support. In Proceedings\\nof the 2021 CHI conference on human factors in computing\\nsystems, 1–14.\\nTenbarge, K. 2023. Instagram’s sex censorship sweeps up\\neducators, adult stars and sex workers.\\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\\nM.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.;\\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\\ntion language models. arXiv preprint arXiv:2302.13971.\\nUziel, R.; Ronen, M.; and Freifeld, O. 2019.\\nBayesian\\nadaptive superpixel segmentation.\\nIn Proceedings of the\\nIEEE/CVF International Conference on Computer Vision,\\n8470–8479.\\nvan der Walt, S.; Sch¨onberger, J. L.; Nunez-Iglesias, J.;\\nBoulogne, F.; Warner, J. D.; Yager, N.; Gouillart, E.; Yu,\\nT.; and the scikit-image contributors. 2014. scikit-image:\\nimage processing in Python. PeerJ, 2: e453.\\nVermeire, T.; Brughmans, D.; Goethals, S.; de Oliveira, R.\\nM. B.; and Martens, D. 2022. Explainable image classifi-\\ncation with evidence counterfactual. Pattern Analysis and\\nApplications, 25(2): 315–335.\\nVishwamitra, N.; Hu, H.; Luo, F.; and Cheng, L. 2021. To-\\nwards Understanding and Detecting Cyberbullying in Real-\\nworld Images. In 2020 19th IEEE International Conference\\non Machine Learning and Applications (ICMLA).\\nWachter, S.; Mittelstadt, B.; and Russell, C. 2017. Counter-\\nfactual explanations without opening the black box: Auto-\\nmated decisions and the GDPR. Harv. JL & Tech., 31: 841.\\nWang, P.; Yang, A.; Men, R.; Lin, J.; Bai, S.; Li, Z.; Ma, J.;\\nZhou, C.; Zhou, J.; and Yang, H. 2022. Ofa: Unifying archi-\\ntectures, tasks, and modalities through a simple sequence-to-\\nsequence learning framework. In International Conference\\non Machine Learning, 23318–23340. PMLR.\\nWang, W.; Bao, H.; Dong, L.; Bjorck, J.; Peng, Z.; Liu,\\nQ.; Aggarwal, K.; Mohammed, O. K.; Singhal, S.; Som, S.;\\net al. 2023. Image as a Foreign Language: BEiT Pretraining\\nfor Vision and Vision-Language Tasks. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 19175–19186.\\n\"},\n",
       " {'abstract': 'Modern vehicles employ intelligent systems like interconnected autonomous driving and advanced driving assistance systems for improved driving experiences, which are enabled by increased connectivity to infrastructure and the fusion of information from multiple sensing modes. However, the increased connectivity combined with the inherent security flaws in legacy network architecture renders vehicles vulnerable to active and passive attacks, directly affecting passenger safety. Researchers have proposed machine learning models for detecting attacks, whose deployments are enabled through quantised neural networks targeting low-power platforms. Our research presents a custom quantised MLP (CQMLP) model as a multi-class classifier capable of detecting multiple attacks from a benign flow of controller area network (CAN) messages. The specific quantisation and neural architecture were determined through a joint design space exploration, resulting in our choice of the 2-bit precision and the n-layer MLP. Our 2-bit version was trained using Brevitas and optimised as a dataflow hardware model through the FINN toolflow from AMD/Xilinx, targeting an XCZU7EV device. Our findings indicate that the 2-bit CQMLP model, when integrated as an IDS, can detect malicious attack messages (DoS, fuzzing, and spoofing attacks) with very high accuracy of 99.9%, on par with the state-of-the-art methods in the literature. Furthermore, the dataflow model can perform line rate detection at a latency of 0.11 ms from message reception while consuming 0.23 mJ/inference.',\n",
       "  'introduction': 'Automotive networks are undergoing rapid evolution to cater to the data-intensive needs of novel intelligent capabilities that enhance safety, infotainment, and comfort for passengers. These interconnected networks, via multiple network protocols, facilitate high-speed communication and information exchange between numerous electronic computing units (ECUs), sensors, and actuators present in modern cars. Among the network protocols, Controller Area Network (CAN) is widely used for critical communication between ECUs and continues to be the predominant network protocol for in-vehicle networks due to its cost-effectiveness and ease of use in control applications. Earlier ECUs and software functions deployed on the CAN were automatically siloed due to the limited connectivity of vehicles to the external world. However, modern vehicles rely on connectivity to infrastructure and other vehicles, enabling real-time sensing of the external environment and unique features like remote monitoring and control of specific capabilities for diagnostics and over-the-air upgrades. Researchers have demonstrated that such interfaces open new avenues for injecting malicious code or messages into these previously siloed networks. As a result, intrusion detection approaches have been proposed in the literature, allowing critical systems to enter a ‘safe working’ mode when such threats are detected. Machine learning (ML) approaches have shown significant improvement in detecting these threats and can adapt to newer attack vectors without the overheads of rule-based approaches. Additionally, quantised versions of floating-point models, termed quantised neural networks (QNNs), have demonstrated viability as alternatives, reducing computational complexity, resource, and energy consumption at the expense of a slight reduction in inference accuracy. QNNs are then deployed on constrained devices such as low-end FPGAs or embedded platforms. Frameworks such as Vitis-AI and FINN can convert Pytorch/TensorFlow representations at native precisions to quantised models and dataflow-style hardware accelerators for deployment on the FPGA.',\n",
       "  'literature_review': 'Rule-based approaches, commonly employed for intrusion detection systems (IDS), can be further classified into flow-based and payload-based approaches. While flow-based approaches identify traits like message frequency and/or interval for the network to detect abnormalities, payload-based approaches utilise the data segment in CAN frames to detect abnormal sequences of instructions or data. Researchers also explored hybrid schemes using both message timing/frequency and payload information. Machine learning-based techniques further generalise these techniques when trained with large datasets through classification, sequential, and deep learning-based approaches.',\n",
       "  'methodology': 'Our proposed methodology involves the formulation of a feed-forward custom quantised multi-layer-perceptron-based IDS for automotive CAN, the design of a dataflow-style custom quantised hardware implementation of the model, and the integration of the dataflow accelerator as an Advanced eXtensible Interface (AXI) slave peripheral to the ARM cores, offering complete isolation from the software tasks on them.',\n",
       "  'results': 'Our experimental results demonstrate that the proposed custom quantised (2-bit) MLP-based IDS (CQMLP-IDS) achieves an average accuracy of 99.91% across Denial of Service (DoS), Fuzzing, and RPM-spoofing attacks among the benign flow of messages, identical to or exceeding the detection accuracy achieved by state-of-the-art GPU- and CPU-based implementations. The tightly integrated ECU architecture reduces the per-message execution latency by 2.2× and the energy consumed by 3.9× compared to state-of-the-art IDSs proposed in the research literature. Furthermore, our (fp32) MLP model on a Jetson Nano, mimicking a dedicated IDS ECU, incurs 12× higher energy consumption per inference, compared to the integrated IDS-ECU.',\n",
       "  'conclusion': 'Our research presents a dataflow-style hardware accelerator that implements a custom quantised MLP (CQMLP) model for detecting and classifying multiple attack vectors on an automotive CAN network. By tightly integrating the accelerator to the ARM core on a Zynq Ultrascale+ platform, we demonstrate a 2.2× speed-up in per-message processing latency and almost 3.9× reduction in energy consumption with 0.23mJ per inference when compared to the state-of-the-art IDSs. The quantised model also maintains high detection and classification accuracy across various attacks, making it suitable for distributing IDS capabilities across ECUs in a vehicle network.',\n",
       "  'title': 'Exploring Highly Quantised Neural Networks for Intrusion Detection in Automotive CAN',\n",
       "  'author': 'Shashwat Khandelwal, Shreejith Shanker',\n",
       "  'textdata': 'Exploring Highly Quantised Neural Networks for\\nIntrusion Detection in Automotive CAN\\nShashwat Khandelwal & Shanker Shreejith\\nReconfigurable Computing Systems Lab, Electronic & Electrical Engineering\\nTrinity College Dublin, Ireland\\nEmail: {khandels, shankers}@tcd.ie\\nAbstract—Vehicles today comprise intelligent systems like\\nconnected autonomous driving and advanced driving assistance\\nsystems (ADAS) to enhance the driving experience, which is\\nenabled through increased connectivity to infrastructure and\\nfusion of information from different sensing modes. However, the\\nrising connectivity coupled with the legacy network architecture\\nwithin vehicles can be exploited for launching active and passive\\nattacks on critical vehicle systems and directly affecting the\\nsafety of passengers. Machine learning-based intrusion detection\\nmodels have been shown to successfully detect multiple targeted\\nattack vectors in recent literature, whose deployments are en-\\nabled through quantised neural networks targeting low-power\\nplatforms. Multiple models are often required to simultaneously\\ndetect multiple attack vectors, increasing the area, (resource)\\ncost, and energy consumption. In this paper, we present a case\\nfor utilising custom-quantised MLP’s (CQMLP) as a multi-class\\nclassification model, capable of detecting multiple attacks from\\nthe benign flow of controller area network (CAN) messages.\\nThe specific quantisation and neural architecture are determined\\nthrough a joint design space exploration, resulting in our choice\\nof the 2-bit precision and the n-layer MLP. Our 2-bit version\\nis trained using Brevitas and optimised as a dataflow hardware\\nmodel through the FINN toolflow from AMD/Xilinx, targeting\\nan XCZU7EV device. We show that the 2-bit CQMLP model,\\nwhen integrated as the IDS, can detect malicious attack messages\\n(DoS, fuzzing, and spoofing attack) with a very high accuracy of\\n99.9%, on par with the state-of-the-art methods in the literature.\\nFurthermore, the dataflow model can perform line rate detection\\nat a latency of 0.11 ms from message reception while consuming\\n0.23 mJ/inference, making it ideally suited for integration with\\nan ECU in critical CAN networks.\\nIndex Terms—Controller Area Network, Intrusion Detection\\nSystem, Quantised Neural Nets, Machine Learning, FPGAs\\nI. INTRODUCTION & RELATED WORKS\\nAutomotive networks are evolving rapidly to cater to the\\nhigh-data needs for novel/intelligent capabilities that have en-\\nabled safety, infotainment, and comfort for passengers. These\\ninterconnected networks (via multiple network protocols) fa-\\ncilitate high-speed communication/information exchange be-\\ntween over a hundred electronic computing units (ECUs),\\nsensors, and actuators present in modern cars. Among the\\nnetwork protocols, Controller Area Network (CAN) is used\\nfor critical communication between ECUs and it continues\\nto be the most widely used network protocol for in-vehicle\\nnetworks owing to its cost-effective nature and ease of use in\\ncontrol applications [1]. Early ECUs and software functions\\ndeployed on the CAN were automatically siloed due to the\\nlimited connectivity of the vehicles then, to the external world.\\nNovel capabilities in vehicles today rely on connectivity to\\ninfrastructure and other vehicles to enable real-time sensing\\nBody \\nControl\\nModule\\nEngine\\nControl\\nModule \\n(Transmitter) \\n Brake\\nControl\\nModule \\n(Receiver)\\n0x043f\\n0x043f\\nCAN Bus\\n(a)\\n]\\nBody \\nControl\\nModule \\nEngine\\nControl\\nModule \\n(Transmitter) \\n Brake\\nControl\\nModule \\n(Receiver)\\n0x043f\\n0x0000\\n0x0000\\nComprimised\\nECU\\nCAN Bus\\nFig. 1: An illustration of a simple DoS attack launched through\\na compromised ECU. Part (a) shows the normal communica-\\ntion between the ECUs, while in (b), the ECU is unable to\\ntransmit messages on the CAN bus as the compromised ECU\\nfloods the bus with high priority messages.\\nof the outside environment and also to enable unique features\\nlike remote monitoring and control of specific capabilities for\\ndiagnostics, and over-the-air upgrades. Researchers however\\nhave shown that such interfaces open up new avenues for\\ninjecting malicious code/messages into these previously siloed\\nnetworks [2]–[4]. Figure 1 shows an example of a simple DoS\\nattack on a standard CAN bus. Such attacks are largely enabled\\nby the lack of inherent security and authentication mechanisms\\nin CAN and similar automotive network protocols [5], [6].\\nTo detect the onset of such intrusions on CAN, multiple\\nintrusion detection approaches have been proposed in the lit-\\nerature allowing critical systems to enter into a ‘safe working’\\nmode when such threats are detected. Early intrusion detection\\nsystems (IDS) proposed were rule/specification based, which\\nutilised a set of rules to compare known attack signatures to\\npatterns captured from current network parameters/messages\\nto detect unusual activity [7]–[9]. Recently, machine learning\\n(ML) approaches have shown significant improvement in the\\ndetection accuracy of such threats and the ability to adapt to\\nnewer attack vectors [10]–[14] without incurring overheads\\nof rule-based approaches. In addition to native precision ML\\nmodels, quantised variants of the floating-point models have\\nshown to be viable alternatives, reducing the computational\\ncomplexity, resource, and energy consumption at the expense\\nof a slight reduction in inference accuracy. Quantised models\\nare then deployed on constrained devices such as low-end FP-\\nGAs or dedicated embedded platforms [15]. Frameworks such\\nas Vitis-AI and FINN can convert the Pytorch/TensorFlow\\nrepresentations at native precisions to quantised models and\\ndataflow-style hardware accelerators for deployment on the\\narXiv:2401.11030v1  [cs.CR]  19 Jan 2024\\nFPGA [16], [17]. ECU architectures based on hybrid FPGAs\\nare a promising platform for the deployment of such QNNs\\nas IDS, while also enabling consolidation with standard ECU\\nfunction(s), with clear isolation between them on the same\\ndie [18]. However, most methods so far relied on the integra-\\ntion of multiple accelerators, each fine-tuned to detect a subset\\nof attacks, increasing the energy cost and area overhead.\\nIn this paper, we explore the case for a custom quantised\\nfeed-forward network that can classify multiple attack vectors\\nsimultaneously and a dataflow-style custom quantised hard-\\nware implementation of the model. The dedicated hardware\\nimplements IDS capability as an Advanced eXtensible In-\\nterface (AXI) slave peripheral to the ARM cores, offering\\ncomplete isolation from the software tasks on them, mimicking\\nthe model ECU architecture for deploying distributed IDS in\\nCAN networks. Starting from a standard multi-layer percep-\\ntron (MLP) model, we analyse and evaluate different model\\nconfigurations and quantisations for weights/biases/activations\\nto arrive at our custom model with 2-, 3- & 4-bit precisions\\nthrough a constrained design space exploration. Subsequently,\\nthe dataflow accelerator is generated and deployed using\\nAMD/Xilinx’s FINN framework [17]. This integration allows\\nthe software tasks on the ECU to invoke and fully control\\nthe operation of the IDS accelerator through APIs, similar\\nto offloads enabled by AUTOSAR abstractions. The key\\ncontributions of this paper are as follows:\\n• A feed-forward custom quantised multi-layer-perceptron\\nbased-IDS for automotive CAN achieving state-of-the-\\nart classification accuracy across multiple attack vectors\\nusing only a single model.\\n• Exploits the tightly-coupled ECU architecture with the\\ndataflow implementation of IDS accelerating IDS opera-\\ntion in full isolation.\\n• Quantify the performance and energy savings of the pro-\\nposed ML model and its integration using the open CAN\\ndataset. Our results show that the proposed IDS achieves\\nsignificant improvements in terms of per-message pro-\\ncessing latency and power consumption against the state-\\nof-the-art IDSs in the literature.\\nWe evaluate our approach using the openly available CAR\\nHacking dataset [13] for training and validation across multiple\\nattack vectors captured from an actual vehicle with the entire\\nCAN data frame used as an input feature to improve the\\ndetection performance. We analyse the 2-, 3- & 4-bit preci-\\nsion models and utilise the inference accuracy and resource\\nutilisation parameters to arrive at the most optimal model.\\nOur experiments show that the proposed custom quantised (2-\\nbit) MLP-based IDS (referred to as CQMLP-IDS) achieves an\\naverage accuracy of 99.91% across Denial of Service (DoS),\\nFuzzing, and RPM-spoofing attacks among the benign flow\\nof messages, identical to or exceeding the detection accuracy\\nachieved by state-of-the-art GPU- and CPU-based implemen-\\ntations. The tightly integrated ECU architecture reduces the per\\nmessage execution latency by 2.2× and the energy consumed\\nby 3.9× compared to state-of-the-art IDSs proposed in the re-\\nsearch literature. We also show that our (fp32) MLP model on\\na Jetson Nano [19], mimicking a dedicated IDS ECU, incurs\\n12× higher energy consumption per inference, compared to\\nthe integrated IDS-ECU.\\nThe remainder of the paper is organised as follows: Sec-\\ntion II captures background information and state-of-the-art\\nresearch in this area; section III describes the proposed MLP\\nmodel and design choices for the implementation; section IV\\noutlines the experiment setup and results; and we conclude the\\npaper in section V.\\nII. BACKGROUND AND RELATED WORKS\\nA. Controller Area Network\\nIn-vehicle networks enable distributed ECUs to exchange\\ncontrol and data messages to achieve the global functions of\\nthe vehicle. Multiple protocols are used in vehicular systems\\nto cater to different functions based on their criticality and\\nto optimise the cost of E/E systems. CAN [1] and CAN-\\nFD [20] continue to be the most widely used protocol to-\\nday due to their lower cost, flexibility, and robustness. The\\nbroadcast CAN bus uses carrier sense multiple access with\\nthe collision avoidance and arbitration priority (CSMA/CA-\\nAP) access/arbitration mechanism to control access to the bus\\nusing the CAN ID allocated to each message. This enables\\nhigher priority messages to be processed first hence efficiently\\nhandling messages from safety-critical applications. CAN also\\nsupports multiple data rates (125 Kbps to 1 Mbps) and multiple\\nmodes of operation (1-wire, 2-wire) to cater to a range of\\ncritical and non-critical functions in vehicles. Despite this\\nrobustness, CAN is inherently insecure: there is no built-\\nin mechanism (like message encryption) in the network to\\nauthenticate the transmitter, receiver, or the message content\\nitself [21]. This makes CAN vulnerable to simple yet efficient\\nactive message injection attacks like fuzzing, spoofing, replay\\nattacks, and Denial of Service (DoS) attacks [6], [22]–[24].\\nResearchers have explored different flavours of IDSs from\\nrule-based approaches to machine learning-based methods to\\naddress CAN’s existing vulnerabilities. Rule-based approaches\\nare further classified into flow-based and payload-based. While\\nflow-based approaches identify traits like message frequency\\nand/or interval for the network to detect abnormalities [25],\\npayload-based approaches use the data segment in CAN\\nframes to detect abnormal sequences of instructions and or\\ndata [26]. Purely payload-based methods, however, fail to\\ncapture the timing/sequence of messages in an attack, lead-\\ning to parts of attack messages being flagged as benign.\\nHybrid schemes use both the message timing/frequency and\\nthe payload information to capture a more holistic view\\nof the network, allowing them to extract specific signatures\\nof transmitting ECUs, receiving ECUs, and messages [27].\\nMachine learning-based techniques further generalises such\\ntechniques when trained with large datasets through classi-\\nfication approaches, sequential techniques, and deep learning-\\nbased schemes.\\nB. Machine Learning based IDSs\\nML-based IDSs discussed in the literature mostly use a\\ncombination of input features, as shown in table I. In [13],\\nthe authors propose a reduced inception net architecture as an\\nTABLE I: Input features used by the IDSs & IPSs proposed\\nin the research literature.\\nModels\\nInput Features Used\\nGIDS [30]\\nCAN ID\\nDCNN [13]\\nCAN ID\\nRec-CNN [38]\\nCAN ID\\nG-IDCS [28]\\nCAN ID\\nTAN-IDS [29]\\nCAN ID\\niForest [35]\\nData Field\\nMLIDS [36]\\nCAN ID + Data Field\\nNovelADS [32]\\nCAN ID + Data Field\\nTCAN-IDS [31]\\nCAN ID + Data Field\\nMTH-IDS [37]\\nCAN ID + Data Field\\nHyDL-IDS [33]\\nCAN ID + Data Field + DLC\\nGRU [34]\\nCAN ID + Data Field + DLC\\nCQMLP-IDS (proposed)\\nCAN ID + Data Field\\nIDS using deep convolutional neural networks. They achieve\\nover 99% accuracy for DoS, fuzzing & spoofing attacks.\\nIn [28], authors present a graph based intrusion detection\\nprotocol and propose improvements over existing graph based\\nIDS techniques. In [29], a transformer network-based IDS is\\nproposed which demonstrates very high classification accuracy\\nat the cost of higher detection latency. In [30], the authors\\npropose a GAN-based IDS and achieve an average accuracy\\nof 97.5% for the same attacks. More complex ML architec-\\ntures like temporal convolution with global attention [31],\\na combination of convolutional neural networks (CNN) and\\nlong short-term memory (LSTM) cells using both supervised\\nand unsupervised approaches [32], [33], gated recurrent units\\n(GRU) networks [34] have been shown to improve detection\\naccuracy. In [35], the authors use an iForest anomaly detection\\nalgorithm as an intrusion prevention system (IPS) to detect\\nfuzzing and spoofing (RPM & Gear) attacks and mark the\\nmessage as an error preventing its propagation to other ECUs;\\nhowever, this can cause multiple messages to be dropped from\\nthe bus in case of false positives or DoS attacks. The key\\nchallenge of ML-based approaches is their deployment as an\\nin-vehicle ECU. Most approaches rely on high-performance\\nGPUs to meet the inference deadlines [13], [30]–[32], [34],\\n[36], while others rely on dedicating full ECUs for IDS [35],\\n[37]; both approaches incur additional overheads in energy\\nbudget and weight, making them less suited for distributing\\nIDS among different network segments. Similarly, almost all\\nmethods presented in the literature require multiple uniquely\\ntrained models to detect all possible threat vectors, which incur\\nmuch higher resources and energy than a singular model which\\ncan classify multiple threat vectors.\\nIII. SYSTEM ARCHITECTURE\\nA. MLP model for IDS\\nTo determine the best ML model for IDS, we profiled differ-\\nent ML architectures to find a model with high classification\\naccuracy which also offers low computational complexity. We\\nobserved that while CNNs were effective for classification\\nwhen using only CAN IDs, MLPs provided more accurate\\ndetection (accuracy, false positives, and false negatives) when\\nusing the entire message contents at much lower computational\\ncomplexity; since message data can contain malicious content,\\nHidden FC Layers\\n{256,128,64,32}\\n(BN+ReLU)\\nInput Layer-\\n{40}\\nOuptut Linear Layer-{4}\\nActivation : Softmax\\nBenign\\nDoS\\nFuzzing\\nRPM\\nSpoof\\nOutput\\nProbabilities\\nFig. 2: The proposed MLP model as a multi-attack detection\\nIDS.\\nwe chose MLPs as our choice of architecture and entire\\nmessage as the input feature [39], [40]. A concatenation of\\nn = 4 successive messages (CAN IDs + payload) is chosen\\nas the input feature based on this testing. Note that during\\ntraining/testing, if the block has even one attack message, the\\nentire block is treated as an attack block. In a real system, this\\nmeans that a window of 4 messages containing a single attack\\nmessage will be detected as a malicious window, allowing a\\nsequence of events leading to and after the attack message as\\npotential threats. Though conservative, this approach enables\\nthe model to better capture normal operating sequences during\\ntraining and provides a better balance to the dataset. We\\nobserved that using a block with a higher number of messages\\n( > 4 ) improves the classification accuracy as the model has\\nmore input data to make a decision; however, grouping more\\nmessages also affects the detection latency from the arrival of\\nthe ‘infected’ message, leading to our choice of n = 4.\\nOur final MLP model consists of 5 Linear layers imple-\\nmented with the 256, 128, 64, 32 & 4 units at each layer\\nas shown in Figure 2. The time-series data from the input\\nfeature buffer is fed as input to the input layer with 40 units.\\nSubsequent layer(s) operate on the output of the previous\\nlayer, with increasing complexity to perform classification.\\nBatch Normalisation was used between the linear layers to\\nprevent over-fitting and to improve the learning efficiency\\nduring the training phase. The output linear layer uses a\\nsoftmax activation at the output to estimate the probability\\nof the message being benign, DoS, fuzzing, or RPM-spoofed.\\nThe model is defined in Pytorch using standard functions and\\nnodes with the quantised versions of linear and relu activation\\nlayers imported from the brevitas training library.\\nB. Design space exploration\\nTo arrive at the optimum bit-precision for the quantised\\nmodel, we trained our final MLP model at 2-bit, 3-bit, and\\n4-bit uniform quantisation on all the attacks to determine the\\ntrade-offs in detection accuracy and resource consumption (in-\\ndirectly leading to power consumption and latency). Figures 3\\n& 4 show the training and validation losses when the model\\nwas trained for 1000 epochs at each quantisation. The 2-bit\\nmodel converges to an optimum detection and is very close\\nto the 3-bit & 4-bit variants (between epoch 810 and 820) in\\nTABLE II: Inference cost of the custom quantised models\\n(CQMLP) inferred through the FINN library and normalised\\nto the 4-bit model.\\nCQMLP Models\\n2-bit\\n3-bit\\n4-bit\\nNormalised inference cost\\n0.27\\n0.67\\n1\\nterms of validation loss. We observed 147 misclassifications\\nout of the total 180,000 messages in the test set for the 2-bit\\nvariant. For the same test set, we found misclassifications to\\nbe 74 & 31 for the 3-bit & 4-bit variants respectively.\\nTo capture the inference costs of the models further, we\\nuse the FINN utility function (inference cost) to estimate\\nthe memory footprint and operational complexity of each\\nmodel. This function captures the memory footprint and binary\\noperations cost for each version of the model; the costs are\\nthen normalised (against a baseline) and linearly combined to\\narrive at the normalised inference cost for each of the three\\nquantisation levels. Table II shows these costs with the 4-bit\\nversion as the baseline. We find the cost of the 2-bit version is\\n40% & 27% of the 3-bit & 4-bit version respectively, without\\na significant drop in detection accuracy and was hence chosen\\nas the optimal bitwidth for deploying the CQMLP.\\nC. Dataset and Training\\nWe use the open Car Hacking dataset for training our model\\nand to test its performance [41]. The dataset provides a labelled\\nset of benign and attack messages which were captured via\\nthe Onboard Diagnostic (OBD) port in an actual vehicle, with\\nattack messages injected in real-time. The dataset includes\\nDoS, Fuzzing, and RPM-Spoofing message injections allowing\\nus to validate the detection accuracy across these different\\nattacks. We split the dataset as 85:10:5 for training, validation,\\nand testing respectively, allocating the large section to training\\nand optimisation of the quantised network. The performance\\nof the model on the validation set during training ensures that\\nit is not over-fitting on any of the attacks. We pre-process\\nthe dataset before training to mimic the dataflow the model\\nwill obtain as its input when integrated into the ECU. Each\\nmessage (ID and payload) is encoded into INT8 type vector,\\nand an adjacent sequence of n = 4 of these encoded messages\\nform the input feature buffer content for the IDS (block shape\\nof {1,10*n} INT8 data).\\nThe different bit models were trained using Brevitas, a\\nquantisation-aware training (QAT) library from AMD/Xilinx\\n[42]. We used the adam optimizer with binary cross-entropy as\\nthe loss function. The learning rate was set to 0.0001 to allow\\nfor slower learning which aids in efficient model training [43].\\nEach model has 54,824 learning parameters and was trained\\nfor 1000 epochs with a batch size of 128 for the DoS, Fuzzing,\\nand RPM-Spoof attacks. The best model in terms of validation\\nloss was saved and exported as an ’onnx’ graph. This graph\\nwas then fed into the FINN hardware generation flow for the\\nCQMLP-IDS IP generation.\\n0\\n200\\n400\\n600\\n800\\n1000\\nEpochs\\n0.00\\n0.01\\n0.02\\n0.03\\n0.04\\n0.05\\nAverage Epoch loss\\n4-bit\\n3-bit\\n2-bit\\nFig. 3: Training loss of the model for different precision of\\nweights and activations for all the attacks.\\n0\\n200\\n400\\n600\\n800\\n1000\\nEpochs\\n0.000\\n0.001\\n0.002\\n0.003\\n0.004\\n0.005\\n0.006\\n0.007\\n0.008\\nAverage Validation loss\\n4-bit\\n3-bit\\n2-bit\\nFig. 4: Validation loss of the model for different precision of\\nweights and activations for all the attacks.\\nD. Dataflow hardware generation and integration to ECU\\nECU-IDS\\nWe utilise the FINN compiler to synthesize and generate\\nthe RTL description of the hardware implementation of the\\nmodel. Starting at the ’onnx’ graph, FINN applies a series\\nof streamlining and dataflow transformations to convert the\\nML model into a synthesizable graph. We further specify\\nthe parallelism to achieve our target latency (set to 100000\\nmessages per second), matrix-vector activation unit widths (set\\nto 80), and folding levels to generate the dataflow hardware\\nwith AXI interfaces for input and output. Subsequently, FINN\\ninvokes Vivado tools to stitch the accelerator as an AXI slave\\nto a Zynq subsystem for our target device (XCZU7EU) and\\ngenerates the final bitstream along with the software drivers\\nfor our hardware ML engine. This integration allows us to\\nmodel the Zynq-based hybrid ECU architecture, where our\\ncustom hardware is accessible from the ECU and performs\\nIDS in complete isolation from the tasks on the ARM cores\\nas shown in Figure 5. We build on the proposed architecture\\nfrom our previous works which contain detailed information\\nof the same [18], [44]. For our experiments, we use the PYNQ\\nruntime on top of a Linux kernel on the ARM cores to interact\\nDRAM\\nAXI Interconnect\\nSwitch\\nCAN Interface\\nARM Cortex-A53 CPU\\nAPU\\nPS\\nPL\\nIncoming CAN\\nmessages\\nStreaming\\nFIFO_0\\nMatrixVector\\nActivation_0\\nCustom Quantised IDS accelerator\\nMatrixVector\\nActivation_4\\nFig. 5: The proposed IDS-ECU architecture. The ML model\\nis accelerated on the PL part of the FPGA.\\nwith our hardware model on the PL.\\nIV. EXPERIMENTAL RESULTS\\nTo quantify the performance and energy benefits of a\\ndedicated dataflow accelerator implementing our CQMLP-IDS\\nintegrated with ECU function(s), we use a Zynq Ultrascale+\\nZCU104 development board as the test ECU platform. The\\nZCU104 board features an XCZU7EV Ultrascale+ device with\\nquad-core ARM A53 cores and dual-core ARM R5 cores on\\nthe PS as our target platform. The A53 cores on the PS are\\nconfigured to run at 1.2 GHz peak, and the dataflow hardware\\nwas synthesized for a clock frequency of 200 MHz by FINN.\\nThe ARM cores run Linux OS with petalinux tools and PYNQ\\nruntime to provide APIs to interface to the CQMLP-IDS IP\\nin the PL. When testing, the hardware CQMLP model is fed\\nwith inputs message from the ECU mimicking the standard\\nmessage flow within ECUs, and use our test split from the\\nCar Hacking dataset to perform inference. We quantify the\\ninference accuracy by evaluating the precision, recall, and F1\\nrates of our CQMLP models when inference is performed\\non the Zynq-IDS-ECU. We also quantify the per-message\\nprocessing latency and power consumption for each incoming\\nCAN message on the Zynq-IDS-ECU. The results, in terms\\nof inference accuracy and message processing latency, are\\ncompared against the state-of-the-art IDSs/IPSs presented in\\nthe research literature. We also compare the per-block process-\\ning latency and the per-inference energy consumption when\\nthe FP32 version of the proposed model is executed on an\\nNVIDIA Jetson Nano 4GB GPU. In the case of schemes where\\ninference is performed on a block of CAN messages, we use\\nthese metrics along with the block size for the comparison.\\nWe also compare our active power consumption against ML-\\nbased IDS approaches in literature where power consumption\\nhas been reported.\\nA. Inference accuracy\\nWe evaluate the classification performance of the model in\\nidentifying DoS, Fuzzy and RPM-Spoofing attacks from the\\nCar Hacking dataset and compare them against state-of-the-art\\ntechniques in the literature. Table III captures the classification\\nTABLE III: Confusion matrix capturing the classification\\nresults of the CQMLP.\\nPredicted Values\\nCQMLP\\nBenign\\nDoS\\nFuzzing\\nRPM-Spoof\\nBenign\\n103142\\n17\\n17\\n0\\nDoS\\n27\\n23666\\n0\\n0\\nFuzzing\\n78\\n0\\n28003\\n8\\nRPM-Spoof\\n0\\n0\\n0\\n25042\\nperformance of our model in isolation across our test set as a\\nconfusion matrix. The misclassifications in our test set can be\\nattributed to scenarios where nearly identical attack and benign\\nmessage patterns are observed in a block at a given time which\\nleads to an overall false positive rate (FPR) of 0.03% (34\\nmessages falsely classified as attacks out of the total 103176\\nbenign messages). We compare the inference performance of\\nour 2-bit CQMLP model integrated within the ECU against\\nthe state-of-the-art IDSs and IPSs proposed in the literature:\\nGIDS [30], DCNN [13], MLIDS [36], HyDL-IDS [33] Nov-\\nelADS [32], TCAN-IDS [31], IForest [35], MTH-IDS [37],\\nGRU [34], G-IDCS [28], TAN-IDS [29] and Rec-CNN [38]\\nwhich are captured in table IV, comparing them in terms\\nof inference precision, recall, F1 score and false negative\\nrate. For the DoS attack the 2-bit CQMLP performs equally\\namong the proposed IDSs in terms of F1 score. In the case of\\nfuzzing attack, the 2-bit CQMLP model performs identically\\nto DCNN [13] while performing better than iForest [35],\\nTCAN-IDS [31], and GRU [34] by 2.3%, 0.6% and 0.6%\\nrespectively in terms of F1 score. The 2-bit CQMLP also\\nachieves almost perfect classification for the RPM-spoofing\\nattack and performs better than or equal to the other methods\\nproposed in the literature. Our prior work using two 8-bit\\nvariants of the feed-forward model achieved 99.96%, 99.76%\\n& 100% F1 scores for the DoS, Fuzzing, and RPM spoofing\\nattacks, when deployed as two concurrent accelerators for\\ndetecting them simultaneously [18]. In contrast, our 2-bit\\nCQMLP variant achieves almost identical F1 scores (99.90%,\\n99.81% & 99.98% respectively) for the same attacks (from\\nthe same dataset) while performing multi-class classification\\nusing a single inference model. By adopting QAT training and\\nconservative block-based labelling, our low precision model is\\nable to match the inference performance of the MLP variant\\nat 8-bit quantisation discussed in [18].\\nB. Inference latency\\nWe quantify the per-message processing latency of the\\nmodel, starting from the arrival of the CAN message at the\\ninterface to determine the detection delay incurred by the ap-\\nproach. Table V compares our result against other approaches\\nin the literature, which utilise different platforms (GPUs,\\nJetson edge accelerators, Raspberry Pi) and approaches (block\\nof CAN messages v/s individual messages). It should be noted\\nthat the latency metric in the case of block-based IDS does not\\nconsider the delay in acquiring the number of CAN messages\\nrequired, and hence could result in potentially larger delays\\nin detection of the onset of an attack. The tightly integrated\\nCQMLP-IDS achieves 0.11 ms per CAN frame, which is a\\nTABLE IV: Accuracy metric comparison (%) of our CQMLP\\naccelerators against the reported literature on the DoS, Fuzzing\\nand RPM-Spoof attacks.\\nAttack\\nModel\\nPrecision\\nRecall\\nF1\\nFNR\\nDoS\\nGIDS [30]\\n-\\n99.9\\n-\\n-\\nDCNN [13]\\n100\\n99.89\\n99.95\\n0.13\\nMLIDS [36]\\n99.9\\n100\\n99.9\\n-\\nG-IDCS [28]\\n99.81\\n98.86\\n99.33\\n-\\nTAN-IDS [29]\\n100\\n100\\n100\\n-\\nHyDL-IDS [33]\\n100\\n100\\n100\\n0\\nNovelADS [32]\\n99.97\\n99.91\\n99.94\\n-\\nTCAN-IDS [31]\\n100\\n99.97\\n99.98\\n-\\niForest [35]\\n-\\n-\\n-\\n-\\nGRU [34]\\n99.93\\n99.91\\n99.92\\n-\\nCQMLP-IDS\\n99.92\\n99.88\\n99.90\\n0.11\\nFuzzing\\nGIDS [30]\\n-\\n98.7\\n-\\n-\\nDCNN [13]\\n99.95\\n99.65\\n99.80\\n0.5\\nMLIDS [36]\\n99.9\\n99.9\\n99.9\\n-\\nG-IDCS [28]\\n99.71\\n99\\n99.35\\n-\\nTAN-IDS [29]\\n99.99\\n99.99\\n99.99\\n-\\nHyDL-IDS [33]\\n99.98\\n99.88\\n99.93\\nNovelADS [32]\\n99.99\\n100\\n100\\n-\\nTCAN-IDS [31]\\n99.96\\n99.89\\n99.22\\n-\\niForest [35]\\n95.07\\n99.93\\n97.44\\n-\\nGRU [34]\\n99.32\\n99.13\\n99.22\\n-\\nCQMLP-IDS\\n99.93\\n99.69\\n99.81\\n0.27\\nRPM-Spoof\\nGIDS [30]\\n-\\n99.6\\n-\\n-\\nDCNN [13]\\n99.99\\n99.94\\n99.96\\n0.05\\nMLIDS [36]\\n100\\n100\\n100\\n-\\nG-IDCS [28]\\n99.85\\n98.69\\n99.27\\n-\\nTAN-IDS [29]\\n99.99\\n99.93\\n99.96\\n-\\nHyDL-IDS [33]\\n100\\n100\\n100\\n0\\nNovelADS [32]\\n99.9\\n99.9\\n99.9\\n-\\nTCAN-IDS [31]\\n99.9\\n99.9\\n99.9\\n-\\niForest [35]\\n98.9\\n100\\n99.4\\n-\\nCQMLP-IDS\\n99.96\\n100\\n99.98\\n0\\nTABLE V: Per-message latency comparison against other\\nstate-of-the-art IDSs reported in literature.\\nModels\\nLatency\\nFrames\\nPlatform\\nGRU [34]\\n890 ms\\n5000 CAN frames\\nJetson Xavier NX\\nMLIDS [36]\\n275 ms\\nper CAN frame\\nGTX Titan X\\nRec-CNN [38]\\n117 ms\\n128 CAN frames\\nJetson TX2\\nNovelADS [32]\\n128.7 ms\\n100 CAN frames\\nJetson Nano\\nGIDS [30]\\n5.89 ms\\n64 CAN frames\\nGTX 1080\\nTAN-IDS [29]\\n11.6 ms\\n128 CAN frames\\n-\\nDCNN [13]\\n5 ms\\n29 CAN frames\\nTesla K80\\nTCAN-IDS [31]\\n3.4 ms\\n64 CAN frames\\nJetson AGX\\nMTH-IDS [37]\\n0.574 ms\\nper CAN frame\\nRaspberry Pi 3\\nQMLP-IDS [18]\\n0.24 ms\\nper CAN frame\\nZynq Ultrascale+\\nCQMLP-IDS\\n0.11 ms\\nper CAN frame\\nZynq Ultrascale+\\n2.18× improvement over the dedicated line-rate QMLP-IDS\\nECU proposed in [18]. We also observe the inference latency\\nof the (fp32) model on the Jetson Nano (modeling a dedicated\\nIDS ECU) to be 1.2 ms when averaged over 10000 runs.\\nThis is 10.9× slower than the proposed CQMLP-IDS on the\\nintegrated IDS-ECU on the Zynq Ultrascale platform. In terms\\nof raw throughput, our dataflow implementation of CQMLP\\ncoupled to the ECU can process over 9090 messages per\\nsecond at the highest payload capacity, achieving near-line-\\nrate detection on high-speed critical CAN networks.\\nTABLE VI: Resource utilisation breakdown of our proposed\\nCQMLP (XCZU7EV).\\nNode\\nLUT\\nFF\\nBRAM/URAM\\nDSP\\n2-bit-CQMLP-IDS\\n3999\\n4524\\n4/0\\n0\\n% Usage\\n1.74%\\n0.98%\\n1.28%/ 0%\\n0%\\nC. Power consumption, Resource Utilisation (PS/PL)\\nWe further quantify the active power consumption of our\\ndataflow accelerator to determine the average energy consump-\\ntion per inference and the hardware resources incurred by the\\ndataflow design on the XCZU7EV device. We observe that\\nour model consumed 2.15 W when measured directly from the\\ndevice’s power rails (using the PYNQ-PMBus package) while\\nperforming inference and other tasks on the ECU (with Linux\\nOS), thus consuming 0.23 mJ of energy per inference. This is\\na 3.9× improvement compared to QMLP-IDS which report a\\nper-inference energy consumption of 0.9 mJ. In comparison,\\nwe also observed the per-inference energy consumption of\\nour (fp32) CAE model on the Jetson Nano to be 2.76 mJ\\nwhen averaged over 10000 inference runs which is 12× more\\nthan the proposed CQMLP-IDS on the tightly integrated IDS-\\nECU on a hybrid FPGA device. Among other reported results\\nin the literature, our approach improves the active power\\nconsumption by 4.6× when compared to the GRU [34] model\\non an Nvidia Jetson Xavier as a dedicated IDS node. In\\nterms of resources, the dataflow CQMLP model consumes\\nless than 2% of resources on the device, leaving behind\\nenough resources to integrate dedicated accelerators for the\\nECU function.\\nWe also quantify the utilisation of the ARM cores for\\nmanaging the dataflow CQMLP accelerator to estimate the\\noverhead in consolidating the IDS capability on the ECU. It\\nwas observed that a single core utilisation peaked at 40% when\\nprocessing the completion interrupt from the IDS core (at\\nthe highest message rate modelling the worst case scenario),\\nwhile other cores remained at IDLE (≤ 2% utilisation). We\\nbelieve that this could be further refined by processing the\\nclassification results using hardware logic, interrupting the\\nECU only when a threat is detected, while also improving\\nthe detection latency.\\nV. CONCLUSION\\nIn this paper, we present a dataflow-style hardware accelera-\\ntor that implements a custom quantised MLP (CQMLP) model\\nfor detecting and classifying multiple attack vectors on an\\nautomotive CAN network. The accelerator generated through\\nthe FINN flow is quantised to 2-bit precision for weights and\\nactivations and is tightly integrated to the ARM core on a\\nZynq Ultrascale+ platform, mimicking a standard ECU. We\\nshow that the dataflow integration achieves a 2.2× speed-up\\nin per-message processing latency and almost 3.9× reduction\\nin energy consumption with 0.23mJ per inference when com-\\npared to the state-of-the-art IDSs. The quantised model also\\nmaintains high detection and classification accuracy across a\\nrange of attacks including DoS, fuzzing, and RPM-Spoofing\\nattacks, making it an ideal approach for distributing IDS\\ncapabilities across ECUs in a vehicle network.\\nREFERENCES\\n[1] R. B. GmbH, “CAN Specification, Version 2.0,” 1991.\\n[2] S. Nie, L. Liu, and Y. Du, “Free-fall: Hacking Tesla from wireless to\\nCAN bus,” Briefing, Black Hat USA, vol. 25, pp. 1–16, 2017.\\n[3] K. Iehira, H. Inoue, and K. Ishida, “Spoofing attack using bus-off attacks\\nagainst a specific ECU of the CAN bus,” in Proc. IEEE Communications\\n& Networking Conference (CCNC), pp. 1–4, IEEE, 2018.\\n[4] Z. Cai, A. Wang, W. Zhang, M. Gruffke, and H. Schweppe, “0-days\\n& mitigations: Roadways to exploit and secure connected BMW cars,”\\nBlack Hat USA, vol. 2019, p. 39, 2019.\\n[5] C. Miller and C. Valasek, “Remote exploitation of an unaltered passenger\\nvehicle,” Black Hat USA, vol. 2015, no. S 91, 2015.\\n[6] M. Enev, A. Takakuwa, K. Koscher, and T. Kohno, “Automobile Driver\\nFingerprinting.,” Proc. Priv. Enhancing Technol., vol. 2016, no. 1,\\npp. 34–50, 2016.\\n[7] U. E. Larson, D. K. Nilsson, and E. Jonsson, “An approach to\\nspecification-based attack detection for in-vehicle networks,” in Proc.\\nIEEE Intelligent Vehicles Symposium, pp. 220–225, 2008.\\n[8] C. Miller and C. Valasek, “Adventures in automotive networks and\\ncontrol units,” Def Con, vol. 21, no. 260-264, pp. 15–31, 2013.\\n[9] I. Studnia, E. Alata, V. Nicomette, M. Kaˆaniche, and Y. Laarouchi, “A\\nlanguage-based intrusion detection approach for automotive embedded\\nnetworks,” International Journal of Embedded Systems, vol. 10, 2018.\\n[10] S. N. Narayanan, S. Mittal, and A. Joshi, “Using data analytics to detect\\nanomalous states in vehicles,” arXiv preprint arXiv:1512.08048, 2015.\\n[11] A. Alshammari, M. A. Zohdy, D. Debnath, and G. Corser, “Classifi-\\ncation approach for intrusion detection in vehicle systems,” Wireless\\nEngineering and Technology, vol. 9, no. 4, pp. 79–94, 2018.\\n[12] L. Yang, A. Moubayed, I. Hamieh, and A. Shami, “Tree-based intelligent\\nintrusion detection system in internet of vehicles,” in 2019 IEEE global\\ncommunications conference (GLOBECOM), pp. 1–6, IEEE, 2019.\\n[13] H. M. Song, J. Woo, and H. K. Kim, “In-vehicle network intrusion\\ndetection using deep convolutional neural network,” Vehicular Commu-\\nnications, vol. 21, p. 100198, 2020.\\n[14] S. Tariq, S. Lee, and S. S. Woo, “CANTransfer: transfer learning based\\nintrusion detection on a controller area network using convolutional\\nLSTM network,” in Proceedings of the 35th Annual ACM Symposium\\non Applied Computing, pp. 1048–1055, 2020.\\n[15] P. Jokic, S. Emery, and L. Benini, “Binaryeye: A 20 kfps streaming cam-\\nera system on FPGA with real-time on-device image recognition using\\nbinary neural networks,” in 2018 IEEE 13th International Symposium\\non Industrial Embedded Systems (SIES), pp. 1–7, IEEE, 2018.\\n[16] Xilinx, “Vitis AI User Guide,” 2021.\\n[17] Y. Umuroglu, N. J. Fraser, G. Gambardella, M. Blott, P. Leong, M. Jahre,\\nand K. Vissers, “Finn: A framework for fast, scalable binarized neural\\nnetwork inference,” in Proc. Intl. Symposium on Field-Programmable\\nGate Arrays (FPGA), pp. 65–74, 2017.\\n[18] S. Khandelwal and S. Shreejith, “A Lightweight FPGA-based IDS-ECU\\nArchitecture for Automotive CAN,” in 2022 International Conference\\non Field-Programmable Technology (ICFPT), pp. 1–9, IEEE, 2022.\\n[19] NVIDIA, “https://developer.nvidia.com/embedded/jetson-nano.”\\n[20] F. Hartwich et al., “CAN with flexible data-rate,” in Proc. iCC, pp. 1–9,\\nCiteseer, 2012.\\n[21] M. Bozdal, M. Samie, and I. Jennions, “A Survey on CAN Bus Protocol:\\nAttacks, Challenges, and Potential Solutions,” in Proc. Intl. Conf.\\non Computing, Electronics Communications Engineering (iCCECE),\\npp. 201–205, 2018.\\n[22] S. Mukherjee, H. Shirazi, I. Ray, J. Daily, and R. Gamble, “Practical\\nDoS attacks on embedded networks in commercial vehicles,” in Proc.\\nIntl Conference on Information Systems Security, Springer, 2016.\\n[23] K. Koscher, S. Savage, F. Roesner, S. Patel, T. Kohno, A. Czeskis,\\nD. McCoy, B. Kantor, D. Anderson, H. Shacham, et al., “Experimental\\nsecurity analysis of a modern automobile,” in Proc. IEEE Sym. on\\nSecurity and Privacy, pp. 447–462, IEEE Computer Society, 2010.\\n[24] A. Palanca, E. Evenchick, F. Maggi, and S. Zanero, “A stealth, selective,\\nlink-layer denial-of-service attack against automotive networks,” in Proc.\\nIntl. Conf. on Detection of Intrusions and Malware, and Vulnerability\\nAssessment, pp. 185–206, Springer, 2017.\\n[25] T. P. Vuong, G. Loukas, and D. Gan, “Performance evaluation of\\ncyber-physical intrusion detection on a robotic vehicle,” in Proc. Intl.\\nConf. on Computer and Information Technology; Ubiquitous Computing\\nand Communications; Dependable, Autonomic and Secure Computing;\\nPervasive Intelligence and Computing, pp. 2106–2113, IEEE, 2015.\\n[26] D. Stabili, M. Marchetti, and M. Colajanni, “Detecting attacks to internal\\nvehicle networks through hamming distance,” in AEIT Intl. Annual\\nConference, pp. 1–6, IEEE, 2017.\\n[27] M. Weber, S. Klug, E. Sax, and B. Zimmer, “Embedded hybrid anomaly\\ndetection for automotive CAN communication,” in Proc. European\\nCongress on Embedded Real Time Software and Systems (ERTS), 2018.\\n[28] S. B. Park, H. J. Jo, and D. H. Lee, “G-IDCS: Graph-Based Intrusion\\nDetection and Classification System for CAN Protocol,” IEEE Access,\\nvol. 11, pp. 39213–39227, 2023.\\n[29] T. P. Nguyen, H. Nam, and D. Kim, “Transformer-Based Attention\\nNetwork for In-Vehicle Intrusion Detection,” IEEE Access, vol. 11,\\npp. 55389–55403, 2023.\\n[30] E. Seo, H. M. Song, and H. K. Kim, “GIDS: GAN based intrusion\\ndetection system for in-vehicle network,” in Proc. Conf. on Privacy,\\nSecurity and Trust (PST), pp. 1–6, IEEE, 2018.\\n[31] P. Cheng, K. Xu, S. Li, and M. Han, “TCAN-IDS: Intrusion Detection\\nSystem for Internet of Vehicle Using Temporal Convolutional Attention\\nNetwork,” Symmetry, vol. 14, no. 2, p. 310, 2022.\\n[32] K. Agrawal, T. Alladi, A. Agrawal, V. Chamola, and A. Benslimane,\\n“NovelADS: A Novel Anomaly Detection System for Intra-Vehicular\\nNetworks,” IEEE Transactions on Intelligent Transportation Systems,\\n2022.\\n[33] W. Lo, H. Alqahtani, K. Thakur, A. Almadhor, S. Chander, and G. Ku-\\nmar, “A hybrid deep learning based intrusion detection system using\\nspatial-temporal representation of in-vehicle network traffic,” Vehicular\\nCommunications, vol. 35, p. 100471, 2022.\\n[34] H. Ma, J. Cao, B. Mi, D. Huang, Y. Liu, and S. Li, “A GRU-\\nBased Lightweight System for CAN Intrusion Detection in Real Time,”\\nSecurity and Communication Networks, vol. 2022, 2022.\\n[35] P. F. De Araujo-Filho, A. J. Pinheiro, G. Kaddoum, D. R. Campelo,\\nand F. L. Soares, “An Efficient Intrusion Prevention System for CAN:\\nHindering Cyber-Attacks with a Low-Cost Platform,” IEEE Access,\\nvol. 9, pp. 166855–166869, 2021.\\n[36] A. K. Desta, S. Ohira, I. Arai, and K. Fujikawa, “MLIDS: Handling\\nRaw High-Dimensional CAN Bus Data Using Long Short-Term Memory\\nNetworks for Intrusion Detection in In-Vehicle Networks,” in Proc. Intl.\\nTelecommunication Networks and Applications Conference (ITNAC),\\npp. 1–7, IEEE, 2020.\\n[37] L. Yang, A. Moubayed, and A. Shami, “MTH-IDS: A Multitiered Hybrid\\nIntrusion Detection System for Internet of Vehicles,” IEEE Internet of\\nThings Journal, vol. 9, no. 1, pp. 616–632, 2021.\\n[38] A. K. Desta, S. Ohira, I. Arai, and K. Fujikawa, “Rec-CNN: In-vehicle\\nnetworks intrusion detection using convolutional neural networks trained\\non recurrence plots,” Vehicular Communications, vol. 35, p. 100470,\\n2022.\\n[39] S. Khandelwal, E. Wadhwa, and S. Shreejith, “Deep Learning-based\\nEmbedded Intrusion Detection System for Automotive CAN,” in 2022\\nIEEE 33rd International Conference on Application-specific Systems,\\nArchitectures and Processors (ASAP), pp. 88–92, 2022.\\n[40] S. Khandelwal and S. Shreejith, “A Lightweight Multi-Attack CAN\\nIntrusion Detection System on Hybrid FPGAs,” in 2022 32nd Inter-\\nnational Conference on Field-Programmable Logic and Applications\\n(FPL), pp. 425–429, 2022.\\n[41] CAR\\nHacking\\nDataset,\\n“https://ocslab.hksecurity.net/datasets/can-\\nintrusion-dataset,” 2020.\\n[42] A. Pappalardo, “Xilinx/brevitas,” 2021.\\n[43] S. Wu, G. Li, F. Chen, and L. Shi, “Training and inference with integers\\nin deep neural networks,” arXiv preprint arXiv:1802.04680, 2018.\\n[44] S. Khandelwal, A. Walsh, and S. Shreejith, “Quantised Neural Network\\nAccelerators for Low-Power IDS in Automotive Networks,” in 2023\\nDesign, Automation & Test in Europe Conference & Exhibition (DATE),\\npp. 1–2, IEEE, 2023.\\n'},\n",
       " {'abstract': 'We present five optimizations for a matrix-based CFL-r algorithm that utilize the specific properties of both the underlying semiring and the widely-used linear algebra library SuiteSparse:GraphBlas. Our experimental results show that these optimizations result in orders of magnitude speedup, with the optimized matrix-based CFL-r algorithm consistently outperforming state-of-the-art CFL-r solvers across four considered static analyses.',\n",
       "  'introduction': 'Context-Free Language Reachability (CFL-r) is a core building block for a wide range of static analyses. The problem asks to find pairs of vertices in an edge-labeled graph that are connected by a path labeled with a word from a Context-Free Language (CFL). While CFL-r is often solved by standard graph reachability algorithms, one promising line of research is to instead formulate CFL-r as a matrix-based operation on sparse matrices, as these operations can be efficiently executed on modern hardware.',\n",
       "  'literature_review': 'Azimov et al. [2] proposed a context-free path querying algorithm by matrix multiplication that oftentimes surpasses analogous in terms of performance. However, it takes a considerable amount of time to deal with paths with deep derivation trees and large CFGs of field- and context-sensitive analyses. Extensive research has been conducted to develop general-purpose CFL-r algorithms. POCR [6], Gigascale [4], and Graspan [16] represent three such tools.',\n",
       "  'methodology': 'We propose a set of optimizations for the matrix-based CFL-r algorithm that significantly improve its performance on large-scale CFL-r instances:',\n",
       "  'results': 'Our experiments show that the proposed optimizations result in orders of magnitude speedup, with the optimized matrix-based CFL-r algorithm consistently outperforming state-of-the-art CFL-r solvers across four considered static analyses: Field-Sensitive Java Points-To (FSJPT), Field-Insensitive C/C++ Alias (FICA), Field-Sensitive C/C++ Alias (FSCA), and Context-Sensitive C/C++ Value-Flow (CSCVF).',\n",
       "  'conclusion': 'We have presented a set of optimizations that significantly improve the performance of the matrix-based CFL-r algorithm, making it a competitive choice for solving large-scale CFL-r instances. Our future work includes complexity analysis as well as generalization of proposed optimizations for other algorithms.',\n",
       "  'title': 'Optimization of the Context-Free Language Reachability Matrix-Based Algorithm',\n",
       "  'author': 'Ilia Muravev',\n",
       "  'textdata': 'Optimization of the Context-Free Language Reachability\\nMatrix-Based Algorithm\\nILIA MURAVEV, Saint Petersburg State University, Russia\\nVarious static analysis problems are reformulated as instances of the Context-Free Language Reachability\\n(CFL-r) problem. One promising way to make solving CFL-r more practical for large-scale interprocedural\\ngraphs is to reduce CFL-r to linear algebra operations on sparse matrices, as they are efficiently executed\\non modern hardware. In this work, we present five optimizations for a matrix-based CFL-r algorithm that\\nutilize the specific properties of both the underlying semiring and the widely-used linear algebra library\\nSuiteSparse:GraphBlas. Our experimental results show that these optimizations result in orders of magnitude\\nspeedup, with the optimized matrix-based CFL-r algorithm consistently outperforming state-of-the-art CFL-r\\nsolvers across four considered static analyses.\\nCCS Concepts: • Theory of computation → Grammars and context-free languages; • Information\\nsystems → Query languages for non-relational engines; • Computing methodologies → Massively\\nparallel algorithms; • Software and its engineering → Automated static analysis.\\nAdditional Key Words and Phrases: Context-free language reachability, GraphBLAS\\n1\\nINTRODUCTION\\nContext-Free Language Reachability (CFL-r) is the problem of finding pairs of vertices in an edge-\\nlabeled graph that are connected by a path, with the condition that the labels along this path spell\\na word from a Context-Free Language (CFL) defined by a given Context-Free Grammar (CFG). A\\nwide range of problems is reduced to CFL-r, including, among others, alias analysis [18], points-to\\nanalysis [4], value-flow analysis [13], and fixing compilation errors [17]. Furthermore, CFL-r is also\\nused to approximate solutions for some undecidable problems, for example, points-to analysis that\\nis simultaneously context-, field-, and flow-sensitive [9, 12].\\nSince Thomas Reps et al. [10] proposed using CFL-r for precise interprocedural dataflow analysis,\\nnumerous CFL-r algorithms have been developed [2, 6–8]. One of the promising general-purpose\\nCFL-r algorithms is a matrix-based algorithm by Rustam Azimov et al. [2]. This algorithm naturally\\nutilizes parallel hardware and oftentimes surpasses analogous in terms of performance [1]. However,\\nit still takes a considerable amount of time to deal with paths with deep derivation trees and large\\nCFGs of field- and context-sensitive analyses. In this work, we propose a set of optimizations that\\nimprove performance of the matrix-based algorithm in such cases. Our evaluation demonstrates\\nthat the optimized version outperforms such tools as POCR [6], Gigascale [4], and Graspan [16].\\n2\\nPRELIMINARIES\\nDefinition 2.1 (Weak Chomsky Normal Form (WCNF)). Let 𝐺𝑟 = (𝑁, Σ, 𝑃,𝑆) be a CFG, where 𝑁\\nis a set of non-terminals, Σ is a set of terminals, 𝑃 is a set of productions, and 𝑆 is the starting\\nnon-terminal.𝐺𝑟 is said to be in WCNF if 𝑃 ⊆ {𝑎 → 𝑏 | 𝑎 ∈ 𝑁,𝑏 ∈ Σ∪{𝜀}}∪{𝑐 → 𝑎 𝑏 | 𝑎,𝑏,𝑐 ∈ 𝑁 }.\\nDefinition 2.2 (Reachability Semiring of CFG). Let 𝐺𝑟 = (𝑁, Σ, 𝑃,𝑆) be a CFG in WCNF. In this\\npaper, 𝑅𝐺𝑟 = (2𝑁, ∪, ⊗𝐺𝑟, ∅) is called a reachability semiring1 of 𝐺𝑟, where 2𝑁 is the domain, ∪ acts\\nas addition, ⊗𝐺𝑟 acts as multiplication, and 𝐴 ⊗𝐺𝑟 𝐵 = {𝑐 ∈ 𝑁 | ∃(𝑎,𝑏) ∈ 𝐴 × 𝐵 ((𝑐 → 𝑎 𝑏) ∈ 𝑃)}.\\n1𝑅𝐺𝑟 is a semiring according to GraphBlas [3], but not according to the common algebraic definition of a semiring.\\nResearch advisor: Semyon Grigorev, s.v.grigoriev@spbu.ru.\\nAuthor’s address: Ilia Muravev, muravjovilya@gmail.com, Saint Petersburg State University, 7/9 University Embankment,\\nSt. Petersburg, Russia, 199034.\\narXiv:2401.11029v1  [cs.PL]  19 Jan 2024\\n2\\nIlia Muravev\\nAlgorithm 1: Original Matrix-Based CFL-r Algorithm as Introduced in [2]\\nData: CFG 𝐺𝑟 = (𝑁, Σ, 𝑃,𝑆), graph 𝐺 = (𝑉, 𝐸), where 𝑉 = 1..𝑛 are vertices and 𝐸 are edges\\nResult: Matrix 𝑀 ∈ 𝑅|𝑉 |×|𝑉 |\\n𝐺𝑟\\n, where 𝑀𝑖𝑗 = {𝑥 ∈ 𝑁 | ∃ path from 𝑖 to 𝑗 derivable from 𝑥}\\n1 𝐺𝑟 ← CFG in WCNF equivalent to CFG 𝐺𝑟;\\n2 𝑀 ← zero |𝑉 | × |𝑉 | matrix over 𝑅𝐺𝑟;\\n// Stored as |𝑁 | Boolean matrices\\n3 foreach (𝑖,𝑥, 𝑗) ∈ 𝐸 do\\n4\\n𝑀𝑖𝑗 ← 𝑀𝑖𝑗 ∪ {𝐴 | (𝐴 → 𝑥) ∈ 𝑃};\\n5 end\\n6 while 𝑀 is changing do\\n7\\n𝑀 ← 𝑀 ∪ (𝑀 ·𝑅𝐺𝑟 𝑀);\\n// ·𝑅𝐺𝑟 is a matrix multiplication in semiring 𝑅𝐺𝑟\\n8 end\\n3\\nOPTIMIZATIONS\\nFollowing optimizations have been proposed, implemented, and evaluated.\\n(1) The bottleneck in Algorithm 1 is at ·𝑅𝐺𝑟 on line 7. To address this, 𝑀 ·𝑅𝐺𝑟 𝑀 is replaced with\\n(𝑀𝑜𝑙𝑑 ·𝑅𝐺𝑟 Δ𝑀) ∪ (Δ𝑀 ·𝑅𝐺𝑟 𝑀), where Δ𝑀 = 𝑀 \\\\ 𝑀𝑜𝑙𝑑 (element-wise set difference) and\\n𝑀𝑜𝑙𝑑 is the value of the variable 𝑀 during the previous iteration of the while loop. Initially,\\n𝑀𝑜𝑙𝑑 = ∅|𝑉 |×|𝑉 |. The rationale here is that (𝑀𝑜𝑙𝑑 ·𝑅𝐺𝑟 Δ𝑀)∪(Δ𝑀 ·𝑅𝐺𝑟 𝑀)∪(𝑀𝑜𝑙𝑑 ·𝑅𝐺𝑟 𝑀𝑜𝑙𝑑) =\\n𝑀 ·𝑅𝐺𝑟 𝑀 and 𝑀𝑜𝑙𝑑 ·𝑅𝐺𝑟 𝑀𝑜𝑙𝑑 has already been added to 𝑀 on the previous iteration.\\n(2) Now, during most iterations, Δ𝑀 is highly sparse, while 𝑀𝑜𝑙𝑑 and 𝑀 are less sparse. The\\nmultiplication of a highly sparse matrix with a less sparse one is considerably faster in\\nrow-major format when the sparser matrix is on the left and in column-major format when\\nthe sparser matrix is on the right [3]. Therefore, the next optimization involves maintaining\\ntwo copies of 𝑀 in both row- and column-major formats and computing 𝑀𝑜𝑙𝑑 ·𝑅𝐺𝑟 Δ𝑀 and\\nΔ𝑀 ·𝑅𝐺𝑟 𝑀 using column-by-column and row-by-row (hyper)sparse matrix multiplication\\ntechniques, respectively [5]. In practice, non-terminals that don’t occur in place of 𝑎 in\\nproductions like 𝑐 → 𝑎 𝑏 don’t need to be stored in the column-major copy of 𝑀, and those\\nthat don’t occur in place of 𝑏 don’t need to be stored in the row-major copy of 𝑀.\\n(3) The next bottleneck is the element-wise union on line 7, which becomes problematic as\\nadding a highly sparse matrix to 𝑀 causes a complete reconstruction of 𝑀. To cope with this,\\n𝑀 is no longer stored as a matrix. Instead, we now use a set of matrices e\\n𝑀 = {𝑀1, 𝑀2, . . . , 𝑀𝑝}\\nsuch that 𝑀 = Ð e\\n𝑀, ∀𝐴, 𝐵 ∈ e\\n𝑀 (𝑛𝑛𝑧(𝐴) < 𝑛𝑛𝑧(𝐵) =⇒ 𝑏 𝑛𝑛𝑧(𝐴) < 𝑛𝑛𝑧(𝐵)), where 𝑏 > 1\\nis a hyperparameter and 𝑛𝑛𝑧(𝑋) is the number of non-zero elements in matrix 𝑋. The\\nelement-wise union on line 7 is computed by adding Δ𝑀 to e\\n𝑀 as a set element and replacing\\ne\\n𝑀 with e\\n𝑀 \\\\ {𝐴, 𝐵} ∪ {𝐴 ∪ 𝐵} as long as there are 𝐴, 𝐵 ∈ e\\n𝑀 that violate e\\n𝑀’s invariants.\\n(4) For CFGs with a large number of productions (|𝑃|), the bottleneck is once again ·𝑅𝐺𝑟 ,\\nrequiring |𝑃| Boolean matrix multiplications [15]. Fortunately, such CFGs often use “indexed”\\nnon-terminals (see examples in appendix B). We exploit this by storing only two Boolean\\nblock-matrices (horizontal and vertical) per entire “indexed” non-terminal, reducing the\\nnumber of Boolean matrix multiplications needed for ·𝑅𝐺𝑟 to the number of productions\\ndiffering not only in indices. For example, ∀𝑖 (𝐴𝑅𝑖 → 𝐴 𝑟𝑒𝑡𝑖) now counts as one production.\\n(5) Finally, profiling has revealed equivalent transformations of CFGs for field-sensitive Java\\npoints-to [4] and field-insensitive C/C++ memory-alias2 [18] analyses that improve perfor-\\nmance (see appendix B).\\n2Transformed CFG for C/C++ memory-alias analysis is no longer usable for C/C++ value-alias analysis.\\nOptimization of the Context-Free Language Reachability Matrix-Based Algorithm\\n3\\nTable 1. All-Pairs CFL-r Run Time in Seconds\\nProblem\\nGraph\\n𝑚𝑎\\n𝑚𝑎1\\n𝑚𝑎14\\n𝑚𝑎1234\\n𝑚𝑎12345\\nPOCR\\nGraspan\\nGigascale\\nFSJPT\\ntradebeans\\nOOT\\n5075\\n51\\n20\\n1.3\\n395\\n-\\n5.1\\ntradesoap\\nOOT\\n5284\\n52\\n20\\n1.5\\n400\\n-\\n4.5\\nFICA\\napache\\n999\\n139\\n139\\n88\\n19\\nOOM\\n2619\\n-\\npostgre\\n1365\\n186\\n186\\n109\\n30\\nOOM\\n1712\\n-\\nFSCA\\nimagick\\n2279\\n372\\n214\\n137\\n137\\n1538\\n-\\n-\\nperlbench\\n4544\\n1721\\n1321\\n1675\\n1675\\nOOT\\n-\\n-\\nCSCVF\\npovray\\nOOT\\n1057\\n21.7\\n10\\n10\\n28\\n-\\n-\\nperlbench\\nOOT\\nOOT\\n195\\n41\\n41\\nOOM\\n-\\n-\\n4\\nEVALUATION\\nAll experiments were run on a PC with an Intel Core i7-6700 CPU (3.4 GHz, 4 threads, hyper-\\nthreading is turned off), and DDR4 64GB RAM, running Ubuntu 20.04 with SuiteSparse:GraphBLAS\\n7.4.4 and Java HotSpot(TM) 64-Bit Server VM (build 15.0.2+7-27, mixed mode, sharing) installed.\\nProposed variants of Algorithm 1 were implemented3 and compared with state-of-the-art CFL-r\\nimplementations: POCR4 [6] and Graspan5 [16], as well as with the field-sensitive Java points-to\\nanalyzer Gigascale6 [4] on graphs from the CFPQ Data7 and CPU178 data sets and their corre-\\nsponding canonical CFGs (see appendix B). Algorithm 1 with optimizations 𝑎,𝑏, . . . ,𝑐 is denoted by\\n𝑚𝑎𝑎𝑏...𝑐. For example, 𝑚𝑎12345 means algorithm 1 with all proposed optimizations.\\nThe following analyses are considered: Field-Sensitive Java Points-To (FSJPT) [4], Field-Insensitive\\nC/C++ Alias (FICA) [18], Field-Sensitive C/C++ Alias (FSCA) [18], and Context-Sensitive C/C++\\nValue-Flow (CSCVF) [13]. To show the worst-case performance of proposed optimizations, for each\\nanalysis, two graphs that took𝑚𝑎12345 the longest time to analyze were selected. For selected graphs,\\nthe mean run time over five runs is displayed in Table 1. In all cases, the unbiased standard deviation\\nestimate is less than 10% of the mean. OOT (Out of Time) means exceeding the 10,000-second\\ntimeout, OOM (Out of Memory) refers to running out of available memory, and a dash “-” indicates\\nthat a particular implementation is not applicable to a problem9. For selected graphs, proposed\\noptimizations preserved the correctness of the answers and increased RAM consumption by at\\nmost 70% compared to the baseline (see appendix A).\\n5\\nCONCLUSION AND FUTURE WORK\\nThe optimized version of the matrix-based CFL-r algorithm was shown to outperform analogous\\nacross a range of considered problems. Building on this success, future work is expected to include\\ncomplexity analysis as well as generalization of proposed optimizations for other algorithms10.\\n3Implementations of matrix-based algorithm variants — https://github.com/FormalLanguageConstrainedPathQuerying/\\nCFPQ_PyAlgo/tree/murav/optimize-matrix (date of access: 06.11.2023), for optimization (3) hyperparameter 𝑏 = 10 is used.\\n4POCR — https://github.com/kisslune/POCR (date of access: 06.11.2023).\\n5Graspan — https://github.com/Graspan/Graspan-C (date of access: 06.11.2023).\\n6Gigascale — https://bitbucket.org/jensdietrich/gigascale-pointsto-oopsla2015/src/master/ (date of access: 06.11.2023).\\n7CFPQ Data — https://formallanguageconstrainedpathquerying.github.io/CFPQ_Data (date of access: 06.11.2023).\\n8CPU17 (graphs for SPEC 2017 C/C++ programs) — https://github.com/kisslune/CPU17-graphs (date of access: 06.11.2023).\\nGraphs from the “simplified-interdyck” folders are used, as done in POCR [6]. Our evaluation excludes the time taken to\\nconstruct SVFG and PEG graphs, eliminate cycles [14], and substitute variables [11], since it remains the same regardless of\\nthe CFL-r implementation.\\n9Gigascale is not a general CFL-r solver and only solves FSJPT, while Graspan encodes each non-terminal using a single\\nbyte and hence can’t handle arbitrarily large CFGs for field- and context-sensitive analyses.\\n10All proposed optimizations can already be transferred to single-path semantics of CFL-r by merely changing the semiring.\\n4\\nIlia Muravev\\nREFERENCES\\n[1] Rustam Azimov. 2022. Context-Free Path Querying Using Linear Algebra. Ph. D. Dissertation. St. Petersburg State\\nUniversity. https://disser.spbu.ru/files/2022/disser_azimov.pdf\\n[2] Rustam Azimov and Semyon Grigorev. 2018. Context-Free Path Querying by Matrix Multiplication. In Proceedings\\nof the 1st ACM SIGMOD Joint International Workshop on Graph Data Management Experiences & Systems (GRADES)\\nand Network Data Analytics (NDA) (Houston, Texas) (GRADES-NDA ’18). Association for Computing Machinery, New\\nYork, NY, USA, Article 5, 10 pages. https://doi.org/10.1145/3210259.3210264\\n[3] Timothy A. Davis. 2023. Algorithm 1037: SuiteSparse:GraphBLAS: Parallel Graph Algorithms in the Language of\\nSparse Linear Algebra. ACM Trans. Math. Softw. 49, 3, Article 28 (sep 2023), 30 pages. https://doi.org/10.1145/3577195\\n[4] Jens Dietrich, Nicholas Hollingum, and Bernhard Scholz. 2015. Giga-Scale Exhaustive Points-to Analysis for Java in\\nunder a Minute. In Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming,\\nSystems, Languages, and Applications (Pittsburgh, PA, USA) (OOPSLA 2015). Association for Computing Machinery,\\nNew York, NY, USA, 535–551. https://doi.org/10.1145/2814270.2814307\\n[5] Jianhua Gao, Weixing Ji, Fangli Chang, Shiyu Han, Bingxin Wei, Zeming Liu, and Yizhuo Wang. 2023. A Systematic\\nSurvey of General Sparse Matrix-Matrix Multiplication. ACM Comput. Surv. 55, 12, Article 244 (mar 2023), 36 pages.\\nhttps://doi.org/10.1145/3571157\\n[6] Yuxiang Lei, Yulei Sui, Shuo Ding, and Qirun Zhang. 2022. Taming Transitive Redundancy for Context-Free Language\\nReachability. Proc. ACM Program. Lang. 6, OOPSLA2, Article 180 (oct 2022), 27 pages. https://doi.org/10.1145/3563343\\n[7] Ciro M. Medeiros, Martin A. Musicante, and Umberto S. Costa. 2018. Efficient Evaluation of Context-Free Path Queries\\nfor Graph Databases. In Proceedings of the 33rd Annual ACM Symposium on Applied Computing (Pau, France) (SAC ’18).\\nAssociation for Computing Machinery, New York, NY, USA, 1230–1237. https://doi.org/10.1145/3167132.3167265\\n[8] Egor Orachev, Ilya Epelbaum, Rustam Azimov, and Semyon Grigorev. 2020. Context-Free Path Querying by Kronecker\\nProduct. Springer-Verlag, Berlin, Heidelberg, 49–59. https://doi.org/10.1007/978-3-030-54832-2_6\\n[9] Thomas Reps. 2000. Undecidability of Context-Sensitive Data-Dependence Analysis. ACM Trans. Program. Lang. Syst.\\n22, 1 (jan 2000), 162–186. https://doi.org/10.1145/345099.345137\\n[10] Thomas Reps, Susan Horwitz, and Mooly Sagiv. 1995. Precise Interprocedural Dataflow Analysis via Graph Reachability.\\nIn Proceedings of the 22nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (San Francisco,\\nCalifornia, USA) (POPL ’95). Association for Computing Machinery, New York, NY, USA, 49–61. https://doi.org/10.\\n1145/199448.199462\\n[11] Atanas Rountev and Satish Chandra. 2000. Off-Line Variable Substitution for Scaling Points-to Analysis. In Proceedings\\nof the ACM SIGPLAN 2000 Conference on Programming Language Design and Implementation (Vancouver, British\\nColumbia, Canada) (PLDI ’00). Association for Computing Machinery, New York, NY, USA, 47–56. https://doi.org/10.\\n1145/349299.349310\\n[12] Johannes Späth, Karim Ali, and Eric Bodden. 2019. Context-, Flow-, and Field-Sensitive Data-Flow Analysis Using\\nSynchronized Pushdown Systems. Proc. ACM Program. Lang. 3, POPL, Article 48 (jan 2019), 29 pages.\\nhttps:\\n//doi.org/10.1145/3290361\\n[13] Yulei Sui, Ding Ye, and Jingling Xue. 2014. Detecting Memory Leaks Statically with Full-Sparse Value-Flow Analysis.\\nIEEE Transactions on Software Engineering 40, 2 (2014), 107–122. https://doi.org/10.1109/TSE.2014.2302311\\n[14] Robert Tarjan. 1971. Depth-First Search and Linear Graph Algorithms. In Proceedings of the 12th Annual Symposium\\non Switching and Automata Theory (Swat 1971) (SWAT ’71). IEEE Computer Society, USA, 114–121. https://doi.org/10.\\n1109/SWAT.1971.10\\n[15] Leslie G. Valiant. 1975. General context-free recognition in less than cubic time. J. Comput. System Sci. 10, 2 (1975),\\n308–315. https://doi.org/10.1016/S0022-0000(75)80046-8\\n[16] Kai Wang, Aftab Hussain, Zhiqiang Zuo, Guoqing Xu, and Ardalan Amiri Sani. 2017. Graspan: A Single-Machine\\nDisk-Based Graph System for Interprocedural Static Analyses of Large-Scale Systems Code. SIGPLAN Not. 52, 4 (apr\\n2017), 389–404. https://doi.org/10.1145/3093336.3037744\\n[17] Wenjie Zhang, Guancheng Wang, Junjie Chen, Yingfei Xiong, Yong Liu, and Lu Zhang. 2023. OrdinalFix: Fixing\\nCompilation Errors via Shortest-Path CFL Reachability. arXiv:2309.06771 [cs.SE]\\n[18] Xin Zheng and Radu Rugina. 2008. Demand-Driven Alias Analysis for C. SIGPLAN Not. 43, 1 (jan 2008), 197–208.\\nhttps://doi.org/10.1145/1328897.1328464\\nA\\nRAM CONSUMPTION\\nTable 2 displays the maximum RAM usage for all conducted experiments, using the same notations\\nas in Table 1.\\nOptimization of the Context-Free Language Reachability Matrix-Based Algorithm\\n5\\nTable 2. All-Pairs CFL-r RAM Consumption in GB\\nProblem\\nGraph\\n𝑚𝑎\\n𝑚𝑎1\\n𝑚𝑎14\\n𝑚𝑎1234\\n𝑚𝑎12345\\nPOCR\\nGraspan\\nGigascale\\nFSJPT\\ntradebeans\\nOOT\\n2.2\\n1.9\\n1.8\\n0.35\\n3.2\\n-\\n1.0\\ntradesoap\\nOOT\\n2.2\\n1.9\\n1.8\\n0.35\\n3.2\\n-\\n1.0\\nFICA\\napache\\n12\\n11\\n11\\n12\\n2.9\\nOOM\\n11\\n-\\npostgre\\n11\\n11\\n11\\n11\\n3.3\\nOOM\\n12\\n-\\nFSCA\\nimagick\\n13\\n11\\n11\\n14\\n14\\n7.3\\n-\\n-\\nperlbench\\n16\\n21\\n21\\n27\\n27\\nOOT\\n-\\n-\\nCSCVF\\npovray\\nOOT\\n1.3\\n1.1\\n1.2\\n1.2\\n9.9\\n-\\n-\\nperlbench\\nOOT\\nOOT\\n5.8\\n6.0\\n6.0\\nOOM\\n-\\n-\\nTable 3. Characteristics Of Analyzed Graphs\\nProblem\\nGraph\\nVertices\\nEdges\\nDistinct Edge Labels\\nFSJPT\\ntradebeans\\n439,693\\n933,938\\n31,886\\ntradesoap\\n440,680\\n936,526\\n31,980\\nFICA\\napache\\n1,721,418\\n3,020,822\\n4 (𝑎,𝑑,𝑎,𝑑)\\npostgre\\n5,203,419\\n9,357,086\\n4 (𝑎,𝑑,𝑎,𝑑)\\nFSCA\\nimagick\\n41,652\\n111,550\\n288\\nperlbench\\n38,091\\n110,874\\n110\\nCSCVF\\npovray\\n346,034\\n581,210\\n11,311\\nperlbench\\n605,864\\n1,114,892\\n28,881\\nTable 4. Used CFGs\\nProblem\\n𝑚𝑎, 𝑚𝑎1 𝑚𝑎14, 𝑚𝑎1234\\n𝑚𝑎12345\\nPOCR\\nGraspan\\nGigascale\\nFSJPT\\nFigure 1a*\\nFigure 1b\\nFigure 1a*\\n-\\nEmbedded\\nFICA\\nFigure 2a*\\nFigure 2b\\nFigure 2a*\\nFigure 2a*\\n-\\nFSCA\\nFigure 3b\\nFigure 3b\\nEmbedded\\n-\\n-\\nCSCVF\\nFigure 4b\\nFigure 4b\\nEmbedded\\n-\\n-\\nB\\nGRAPHS AND GRAMMARS\\nTable 3 displays the characteristics of analyzed graphs: number of vertices, edges, and distinct\\nlabels.\\nIn this work, as optimization (5) states, CFGs for field-sensitive Java points-to [4] and field-\\ninsensitive C/C++ memory-alias [18] analyses were manually transformed to WCNF (see Figures 1b\\nand 2b) in such a way that results in better performance compared to automatically generated\\nWCNF (compare 𝑚𝑎1234 and 𝑚𝑎12345 in Table 1).\\nTable 4 shows, for each pair of CFL-r implementation and a problem, which CFG was used.\\n“Embedded” means that the representation of CFL for a specific problem is embedded into the CFL-r\\nimplementation itself, and an asterisk “*” indicates that the CFG is automatically11 normalized,\\nbecause a particular CFL-r implementation only works with CFGs in WCNF.\\n11CFG normalization implementation — https://formallanguageconstrainedpathquerying.github.io/CFPQ_Data/reference/\\ngrammars/generated/cfpq_data.grammars.converters.cnf.html#cfpq_data.grammars.converters.cnf.cnf_from_cfg (date of\\naccess: 06.11.2023).\\n6\\nIlia Muravev\\n𝑃𝑇\\n→ 𝑃𝑇𝐻 𝑎𝑙𝑙𝑜𝑐\\n𝑃𝑇𝐻 → 𝜀 | 𝑎𝑠𝑠𝑖𝑔𝑛 𝑃𝑇𝐻\\n𝑃𝑇𝐻 → 𝑙𝑜𝑎𝑑𝑖 𝐴𝑙 𝑠𝑡𝑜𝑟𝑒𝑖 𝑃𝑇𝐻\\n𝐹𝑇\\n→ 𝑎𝑙𝑙𝑜𝑐 𝐹𝑇𝐻\\n𝐹𝑇𝐻 → 𝜀 | 𝑎𝑠𝑠𝑖𝑔𝑛 𝐹𝑇𝐻\\n𝐹𝑇𝐻 → 𝑠𝑡𝑜𝑟𝑒𝑖 𝐴𝑙 𝑙𝑜𝑎𝑑𝑖 𝐹𝑇𝐻\\n𝐴𝑙\\n→ 𝑃𝑇 𝐹𝑇\\n(a) CFG taken from CFPQ Data\\n𝑃𝑇\\n→ 𝑎𝑙𝑙𝑜𝑐 | 𝑎𝑠𝑠𝑖𝑔𝑛 𝑃𝑇 | 𝐿𝑃𝐹𝑆𝑖 𝑃𝑇\\n𝐹𝑇\\n→ 𝑎𝑙𝑙𝑜𝑐 | 𝐹𝑇 𝑎𝑠𝑠𝑖𝑔𝑛 | 𝐹𝑇 𝑆𝑃𝐹𝐿𝑖\\n𝐿𝑃𝐹𝑆𝑖 → 𝐿𝑃𝑖 𝐹𝑆𝑖\\n𝐿𝑃𝑖\\n→ 𝑙𝑜𝑎𝑑𝑖 𝑃𝑇\\n𝐹𝑆𝑖\\n→ 𝐹𝑇 𝑠𝑡𝑜𝑟𝑒𝑖\\n𝑆𝑃𝐹𝐿𝑖 → 𝑆𝑃𝑖 𝐹𝐿𝑖\\n𝑆𝑃𝑖\\n→ 𝑠𝑡𝑜𝑟𝑒𝑖 𝑃𝑇\\n𝐹𝐿𝑖\\n→ 𝐹𝑇 𝑙𝑜𝑎𝑑𝑖\\n(b) CFG in WCNF, introduced as optimization (5)\\nFig. 1. CFG for field-sensitive Java points-to analysis\\n𝑀 → 𝑑 𝑉 𝑑\\n𝑉 → 𝜀 | 𝑉1 𝑉2 𝑉3\\n𝑉1 → 𝜀 | 𝑉2 𝑎 𝑉1\\n𝑉2 → 𝜀 | 𝑀\\n𝑉3 → 𝜀 | 𝑎 𝑉2 𝑉3\\n(a) CFG taken from CFPQ Data\\n𝑀\\n→ 𝑁1 𝑁3 | 𝑁2 𝑁3\\n𝑁1 → 𝑑 | 𝑁1 𝑎 | 𝑁2 𝑎\\n𝑁2 → 𝑁1 𝑀\\n𝑁3 → 𝑑 | 𝑎 𝑁3 | 𝐴𝑀 𝑁3\\n𝐴𝑀 → 𝑎 𝑀\\n(b) CFG in WCNF, introduced as optimization (5)\\nFig. 2. CFG for field-insensitive C/C++ alias analysis\\n𝑀 → 𝑑 𝑉 𝑑\\n𝑉 → 𝐴 𝑉 𝐴 | 𝑓𝑖 𝑉 𝑓𝑖 | 𝑀 | 𝜀\\n𝐴 → 𝑎 𝑀? | 𝜀\\n𝐴 → 𝑀? 𝑎 | 𝜀\\n(a) CFG\\n𝑀\\n→ 𝐷𝑉 𝑑\\n𝐷𝑉 → 𝑑 𝑉\\n𝑉\\n→ 𝐴 𝑉 | 𝑉 𝐴 | 𝐹𝑉𝑖 𝑓𝑖 | 𝑀 | 𝜀\\n𝐹𝑉𝑖 → 𝑓 𝑖𝑉\\n𝐴\\n→ 𝑎 𝑀 | 𝑎 | 𝜀\\n𝐴\\n→ 𝑀 𝑎 | 𝑎 | 𝜀\\n(b) CFG in WCNF\\nFig. 3. CFG for field-sensitive C/C++ alias analysis, taken from [6]\\n𝐴 → 𝐴 𝐴 | 𝑎 | 𝜀\\n𝐴 → 𝑐𝑎𝑙𝑙𝑖 𝐴 𝑟𝑒𝑡𝑖\\n(a) CFG\\n𝐴\\n→ 𝐴 𝑎 | 𝐴 𝐴𝐻 | 𝜀\\n𝐴𝐻 → 𝑐𝑎𝑙𝑙𝑖 𝐴𝑅𝑖\\n𝐴𝑅𝑖 → 𝐴 𝑟𝑒𝑡𝑖\\n(b) CFG in WCNF\\nFig. 4. CFG for context-sensitive C/C++ value-flow analysis, taken from [6]\\n'},\n",
       " {'abstract': 'This paper aims to explore the direct relationship between emotion and state-of-the-art speaker embeddings in the form of intra-speaker clusters and provide a contrastive pretraining approach for speech emotion recognition. We conduct a thorough clustering analysis to demonstrate the extractability of emotion information from speaker embeddings. The proposed method leverages extensive emotion-unlabeled data and significantly improves speech emotion recognition performance.',\n",
       "  'introduction': 'Speech emotion recognition remains challenging due to its complexity and the subjective nature of emotional expression. Recent works explore the potential of speaker embeddings in enhancing speech emotion recognition, but they assume that emotion information is indirectly encoded within speaker embeddings. We aim to investigate the direct accessibility of emotion-related information within speaker embeddings and find effective ways to leverage this information in SER tasks.',\n",
       "  'literature_review': 'Previous studies have revealed increased equal error rates in speaker verification for non-matching emotional conditions, highlighting the sensitivity of speaker features to emotional states. Research has also demonstrated emotion-related information in speaker embeddings via autoencoder-based reconstruction analysis and emotion classification. Recent works have employed deep speaker embedding networks to transfer knowledge from speaker verification to speech emotion recognition. However, the potential of deep speaker embeddings in encoding emotional information remains an area that requires comprehensive exploration.',\n",
       "  'methodology': 'We evaluate the relationship between emotion and speaker embedding clusters using clustering analysis. We utilize d-vector and ECAPA-TDNN speaker embeddings and compute intra-speaker clusters corresponding to emotion categories. The evaluation metrics used include Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), Purity Score, and Silhouette Score. To leverage the clustered embeddings, we then introduce a novel contrastive learning approach for SER. We employ positive and negative example pairs based on the speaker embedding clusters and formulate a contrastive loss function. The contrastive learning is applied as the primary objective of pretraining and as an additional task for existing pretraining methods in a multi-task setting.',\n",
       "  'results': 'Our analysis reveals distinct intra-speaker clusters that reflect emotional states, suggesting a strong link between speaker and emotion recognition. Pretraining with the proposed contrastive strategy demonstrates significant improvement in speech emotion recognition compared to no pretraining and supervised speaker classification. The multi-task learning approach, which simultaneously considers intra-speaker variations and speaker-emotion connection, achieves the best performance. Fine-tuning a pre-trained wav2vec2.0 model with our contrastive learning and multi-task strategies further enhances speech emotion recognition performance.',\n",
       "  'conclusion': 'Our research establishes a direct link between emotions and state-of-the-art speaker embeddings, highlighting the potential of speaker embeddings for SER. The novel contrastive pretraining approach significantly improves SER performance, providing a practical solution for data scarcity in SER. Future work will extend the analysis of emotion information in speaker embeddings and explore factors affecting its appearance.',\n",
       "  'title': 'Revealing Emotional Clusters in Speaker Embeddings: A Contrastive Learning Strategy for Speech Emotion Recognition',\n",
       "  'author': 'Ismail Rasim Ulgen, Zongyang Du, Carlos Busso, Berrak Sisman',\n",
       "  'textdata': 'REVEALING EMOTIONAL CLUSTERS IN SPEAKER EMBEDDINGS:\\nA CONTRASTIVE LEARNING STRATEGY FOR SPEECH EMOTION RECOGNITION\\nIsmail Rasim Ulgen1, Zongyang Du1, Carlos Busso2, Berrak Sisman1\\n1Speech & Machine Learning (SML) Lab, The University of Texas at Dallas, USA\\n2Multimodal Signal Processing (MSP) Lab, The University of Texas at Dallas, USA\\nABSTRACT\\nSpeaker embeddings carry valuable emotion-related informa-\\ntion, which makes them a promising resource for enhancing\\nspeech emotion recognition (SER), especially with limited la-\\nbeled data. Traditionally, it has been assumed that emotion in-\\nformation is indirectly embedded within speaker embeddings,\\nleading to their under-utilization. Our study reveals a direct\\nand useful link between emotion and state-of-the-art speaker\\nembeddings in the form of intra-speaker clusters. By conduct-\\ning a thorough clustering analysis, we demonstrate that emo-\\ntion information can be readily extracted from speaker em-\\nbeddings. In order to leverage this information, we introduce\\na novel contrastive pretraining approach applied to emotion-\\nunlabeled data for speech emotion recognition. The proposed\\napproach involves the sampling of positive and the negative\\nexamples based on the intra-speaker clusters of speaker em-\\nbeddings. The proposed strategy, which leverages extensive\\nemotion-unlabeled data, leads to a significant improvement\\nin SER performance, whether employed as a standalone pre-\\ntraining task or integrated into a multi-task pretraining setting.\\nIndex Terms— Speech emotion recognition, speaker em-\\nbeddings, clustering, contrastive learning, multi-task learning\\n1. INTRODUCTION\\nSpeech emotion recognition remains a challenging task due\\nto its complexity and the subjective nature of emotional ex-\\npression, compounded by the scarcity of labeled emotional\\ndata [1]. These factors significantly hinder the development\\nof effective SER methods, and encourage researchers to lever-\\nage auxiliary knowledge from closely related speech tasks,\\nsuch as speaker verification (SV) [2–5].\\nIn contrast to SER, SV benefits from the availability of\\nsufficient labeled data [6, 7]. Although the tasks of recog-\\nnizing emotions from speech and verifying speakers differ in\\ntheir primary objectives, they both revolve around the identifi-\\ncation of fundamental voice attributes, including pitch, tone,\\nand phonation patterns. Consequently, speaker verification\\ntechniques with robust performance are now being explored\\nas promising tools for enhancing the performance of speech\\nemotion recognition systems [2,3,8]\\nEmotion information within speaker features has been\\nexplored in various emotional speech tasks. Studies [9–11]\\nrevealed increased equal error rates in speaker verification\\nfor non-matching emotional conditions, highlighting the sen-\\nsitivity of speaker features to emotional states [12].\\nRe-\\nsearch by [13] demonstrated emotion-related information\\nin speaker embeddings via autoencoder-based reconstruc-\\ntion analysis and emotion classification.\\nThis finding was\\nconfirmed by [8], which also performed reconstruction anal-\\nysis and used speaker embeddings as SER input features.\\nRecent works [2, 3] employed deep speaker embedding net-\\nworks to transfer knowledge from speaker verification to\\nspeech emotion recognition. However, the potential of recent\\ndeep speaker embeddings like d-vector [14] and ECAPA-\\nTDNN [15] in encoding emotional information remains an\\narea that requires comprehensive exploration. Previous stud-\\nies are limited by the assumption that emotion information\\nis indirectly encoded within speaker embeddings and can\\nbe utilized under supervision. In this paper, we aim to ex-\\nplore whether emotion-related information directly resides\\nwithin the speaker embedding space and find effective ways\\nto leverage this information in SER tasks.\\nSelf-supervised speech models such as wav2vec2.0 [16]\\ncan leverage large unlabeled speech datasets to enhance su-\\npervised SER frameworks [5,17,18]. However, it’s important\\nto note that these pre-training objectives were not originally\\ndesigned for SER, except for [19] which incorporated audio-\\nvisual features. Additionally, existing pretraining tasks uti-\\nlized in SER are frame-level tasks while speech emotion is\\nusually formulated as an uttterance-level task. Consequently,\\na significant gap exists in the field, particularly in the develop-\\nment of an utterance-level, unsupervised pre-training strategy\\nexplicitly tailored to SER, exclusively using speech-related\\nfeatures, which is one of the contributions of this paper.\\nThis paper marks the first attempt to investigate the\\ndirect accessibility of emotion-related information within\\nstate-of-the-art deep speaker embeddings. Our analysis re-\\nveals distinct intra-speaker clusters that reflect emotional\\nstates, suggesting a strong link between speaker and emotion\\nrecognition. To utilize this information, we propose a novel\\npretraining strategy using large-scale, emotion-unlabeled\\ndata. This approach employs contrastive learning, forming\\npositive-negative pairs based on speaker embedding clusters,\\nwithout the need for emotion labels. We apply this strategy\\nboth as the primary objective of pretraining and as an addi-\\ntional task for the existing pretraining methods in a multi-task\\n©2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing\\nthis material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this\\nwork in other works.\\narXiv:2401.11017v1  [eess.AS]  19 Jan 2024\\nTable 1: Intra-speaker clustering evaluation for emotion classification.\\nd-vector\\nECAPA-TDNN\\nDataset\\nNMI [0,1] ↑\\nARI [0,1] ↑\\nPurity [0,1] ↑\\nSilhoutte [-1,1] ↑\\nNMI [0,1] ↑\\nARI [0,1] ↑\\nPurity [0,1] ↑\\nSilhoutte [-1,1] ↑\\nESD\\n0.76\\n0.72\\n0.89\\n0.14\\n0.89\\n0.91\\n0.97\\n0.13\\nIEMOCAP\\n0.29\\n0.21\\n0.66\\n0.01\\n0.31\\n0.25\\n0.67\\n0.01\\nCREMA-D\\n0.43\\n0.39\\n0.63\\n0.07\\n0.36\\n0.27\\n0.57\\n0.04\\nRAVDESS\\n0.59\\n0.38\\n0.67\\n0.14\\n0.51\\n0.28\\n0.62\\n0.05\\nsetting.\\nOur contributions can be summarized as follows:\\n1) We reveal readily available emotion information within\\nspeaker embeddings; 2) We introduce a unique, utterance-\\nlevel contrastive learning approach for SER, without relying\\non emotion labels; 3) We demonstrate that combination of\\npretraining tasks in a multi-task setting can further improve\\nSER performance; and 4) Through our proposed training\\nstrategy, we enhance a very strong framework, wav2vec2.0,\\nin terms of emotion recognition performance.\\n2. REVEALING EMOTION CLUSTERS IN SPEAKER\\nEMBEDDINGS\\nIn this section, we conduct clustering analysis on speaker\\nembeddings to explore emotion discrimination within the\\nspeaker embedding space, aiming to establish a direct link\\nbetween intra-speaker clusters of embeddings and emotional\\ncategories.\\nThis connection holds significant potential for\\nvarious SER applications, particularly in harnessing exten-\\nsive, emotion-unlabeled data. Our analysis is driven by the\\nhypothesis that speaker embeddings, designed to capture\\nvoice characteristics, are sensitive to variations in a speaker’s\\nvoice across different emotional states [8–10, 13], drawing\\ninspiration from studies indicating distinct speaker patterns\\nin different emotional contexts [9–11].\\n2.1. Dataset, speaker embeddings and evaluation metrics\\nWe applied k-means clustering to length-normalized speaker\\nembeddings of using a maximum of 320 different utterances\\nfor each speaker within a dataset, using a fixed number of\\nclusters to align with the four categorical emotions: neu-\\ntral, happiness, sadness, and anger. We selected four widely\\nused labeled emotion datasets: IEMOCAP [20], ESD [21],\\nCREMA-D [22], and RAVDESS [23]. Our choice of deep\\nspeaker embedding networks includes d-vector [14] and\\nECAPA-TDNN [15], both trained with metric-based ob-\\njectives like generalized end-to-end loss and angular margin\\nsoftmax loss on the voxceleb2 dataset [7]. We evaluated the\\nalignment between intra-speaker cluster labels and emotion\\ncategories using metrics such as Normalized Mutual Infor-\\nmation (NMI) [24], Adjusted Rand Index (ARI) [24], Purity\\nScore [25], and Silhouette Score [26], averaged over speakers\\nand larger values indicate a stronger alignment.\\n2.2. Clustering and Evaluations\\nThe clustering results are reported in Table 1. Notably, the\\nESD dataset consistently demonstrates exceptionally high\\nmetrics, indicating a direct alignment between intra-speaker\\nclusters and emotion categories in specific conditions where\\nthe utterances are very clean, linguistic content is normalized\\nover emotion categories and emotion intensity tends to be\\n(a) T-SNE of speaker embeddings in ESD dataset\\n(b) T-SNE of speaker embeddings in IEMOCAP dataset\\nFig. 1: Visualization of intra-speaker clusters in two datasets,\\nthe colors represent {speaker id} {emotion}.\\nhigh. While the metrics for other datasets are not as high as\\nin ESD, a meaningful correlation exists across all datasets.\\nThe IEMOCAP dataset, with challenges like reverberation\\nand overlapping speech, exhibits the lowest metrics, possibly\\ndue to variance introduced into speaker embeddings.\\nThe distribution of embeddings can be observed in the t-\\nSNE plots in Figure 1, showing clear separation in the ESD\\ndataset and some distinction in the IEMOCAP dataset. We’ve\\nplotted t-SNE plots only for ESD and IEMOCAP due to sim-\\nilar trends in other databases. NMI values tend to be higher\\nthan ARI values, indicating uneven clustering errors. Higher\\npurity values, compared to lower ARI values, suggest over-\\nlaps between specific emotion pairs, hinting at unique rela-\\ntionships between emotion categories. Low silhouette scores\\nare expected due to closely spaced embeddings, aligning with\\ntheir original goal of grouping speaker utterances together.\\nIn general, the clustering results validate that speaker em-\\nbeddings tend to group together for different emotional states\\nin the embedding space due to distinct vocal characteristics\\nfor each emotion. The correspondence between emotion cate-\\ngories and intra-speaker clusters is limited in non-ideal condi-\\ntions possibly due to other factors affecting the speech signal.\\nThe results show that even clusters with limited accuracy can\\nserve as effective learning tasks [27–29]. Inspired by these\\nfindings, we propose a contrastive learning strategy based on\\nthe trend of intra-speaker clustering of emotion categories.\\n2\\nPooling\\nDense\\n(ReLu)\\nDense\\n(tanh)\\nContrastive\\nLoss\\nPooling\\nEmotion Classification\\nLoss\\nStage 1:\\nPre-training\\nStage 2:\\nSupervised SER\\nSpk Classification\\nLoss\\nDense\\n(ReLu)\\nDense\\n(softmax)\\nPooling\\nDense\\n(ReLu)\\nDense\\n(softmax)\\nDense\\n(ReLu)\\nDense\\n(tanh)\\nContrastive\\nLoss\\n Proposed\\nMulti-Task Learning\\nEncoder\\nEncoder\\nEncoder\\n(Shared)\\na)\\nb)\\nFig. 2: a) Proposed contrastive pre-training and SER training,\\nb) Proposed multi-task learning framework.\\n3. CONTRASTIVE LEARNING FOR SER\\nIn this study, we introduce a novel contrastive pretrain-\\ning strategy without emotion labels, which capitalizes on\\nemotion-related information present in the form of intra-\\nspeaker clusters within speaker embeddings. Our approach is\\nbased on contrastive learning, a technique well-known for its\\nefficacy across various tasks [30, 31]. The learning objective\\ntries to maximize the similarity between positive pairs while\\nminimize it for negative pairs. In our approach, positive pairs\\nconsist of utterances sampled from the same intra-speaker\\ncluster, likely sharing the same emotion category. In contrast,\\nnegative examples are created from different intra-speaker\\nclusters of the same speaker, likely to have different emo-\\ntion categories given our analysis in Section 2. This setup\\ninherently fosters an utterance-level emotion classification.\\n3.1. Contrastive Pretraining\\nIn the pretraining stage, we obtain intra-speaker clusters of\\nspeaker embeddings in a separate process similar to the ex-\\nperiments in Section 2.1, where the only difference is in the\\nnumber of clusters N since we don’t have a prior about cat-\\negories on emotion-unlabeled data.\\nA variant NT-Xent [30]\\nloss is used as an objective in the training:\\nl = −log\\nexp(sim(zi, zj)/τ)\\nPN/2\\nk=1 exp(sim(zi, zk)/τ)[k̸=i]\\n(1)\\nwhere zi, zj is the positive pair and zi, zk are the nega-\\ntive pairs for a given utterance.\\nThe similarity function\\nsim(x, y) = xT y/||x||.||y|| calculates the cosine similarity\\nand τ denotes the temperature parameter.\\nSoft-sampling: For each utterance, we select one positive\\nand N/2 negative utterances based on intra-speaker cluster\\nlabels. Due to rough clustering, when sampling the negative\\nexamples, we employ a soft-sampling strategy, selecting one\\nnegative sample from each of the N/2 intra-speaker clusters\\nEncoder\\nCNN \\nEncoder\\nTransformer\\nEncoder\\nInput\\nWaveform\\nFrame-level\\nOutputs\\nFig. 3: The encoder architecture utilized in the networks.\\nthat are farthest from the positive cluster center. The model\\narchitecture consists of an encoder followed by a contrastive\\nlearning head, as shown in Fig.2(a) and Fig. 3.\\n3.2. Contrastive Pretraining for Multi-Task Design\\nGiven the success of the transfer learning from speaker\\nrecognition to SER due to their connection, we also pro-\\npose a multi-task learning (MTL) strategy to utilize available\\nspeaker labels. The proposed multi-task framework includes\\nshared encoder layers along with two separate heads: con-\\ntrastive learning and speaker classification head which can\\nbe seen in Figure 2(b).\\nThe contrastive learning head is\\ntrained with the proposed objective in Section 3.1; while the\\nspeaker classification head is trained with the cross-entropy\\nloss with speaker labels. Along with the multi-task frame-\\nwork, speaker adversarial setting is also experimented, by\\nincluding a Gradient Reversal Layer (GRL) just before the\\nspeaker classification head.\\n3.3. Speech Emotion Recognition\\nAfter pretraining on a large-scale, emotion-unlabeled dataset,\\nthe model is trained in a supervised manner on a smaller\\ndataset with categorical emotion labels. During supervised\\ntraining, we introduce a freshly initialized classification head\\non top of pre-trained encoder layers. This classification head\\ncomprises an average pooling layer, a dense projection layer\\nwith rectified linear unit (ReLU) activation, and a dense out-\\nput layer with softmax activation. In this stage, we fine-tune\\nthe pre-trained layers in conjunction with the classification\\nhead, utilizing cross-entropy loss and emotion labels. The\\ndiagram can be seen in the Figure 2(a).\\n4. EXPERIMENTS\\nIn this section, we report the effect of our proposed pretrain-\\ning strategies with only contrastive loss and multi-task learn-\\ning on SER performance when dealing with a limited amount\\nof labeled data. We have evaluated our strategies indepen-\\ndently and in conjunction with wav2vec2.0 to clearly discern\\ntheir effect on emotion recognition performance.\\n4.1. Experimental setup\\nDatasets: During pretraining, we utilize voxceleb2 [7] as an\\nemotion-unlabeled dataset, known for its diverse emotional\\ncontexts [13], aligning with our intra-speaker clustering ap-\\nproach. In supervised SER training, we separately employ\\ntwo labeled emotion datasets, IEMOCAP and CREMA-D.\\nWe focus exclusively on Anger, Happiness, Neutral, Sadness,\\nestablishing a speaker-independent emotion recognition sce-\\nnario. For the IEMOCAP corpus, we only use improvised ut-\\nterances and create 5-fold training and test splits following the\\n3\\nleave-one-session-out rule described in [17] and [2], exclud-\\ning a small subset from one of the test speakers for validation.\\nFor CREMA-D, we use training data from 64 speakers, with\\n8 for validation and 19 for testing.\\nBaselines: In our basic SER experiments, we establish\\nthree baselines: No-pretraining, which involves initializing\\nthe model randomly before supervised SER training, with-\\nout any pretraining; No-pretraining (small), which has a\\nsmaller architecture with only 2 transformer layers to assess\\nthe impact of overfitting; and Pretraining w/ spk classifi-\\ncation which employs pretraining the model with encoder\\nfollowed by only speaker classification head and loss, sim-\\nilar to the methodology in [2]. For SER experiments based\\non wav2vec2.0, we utilize a smaller version of the original\\nwav2vec2.0 as the baseline pretraining method.\\nModel Architecture & Training: In our pretraining and\\nbasic SER experiments, our proposed methods and base-\\nline models, have the same encoder architecture, which is\\nbased on wav2vec2.0 [16].\\nThis encoder architecture in-\\ncludes a feature extractor and a transformer encoder, similar\\nto wav2vec2.0, but with a more compact design featuring\\nonly 6 transformer layers.\\nThe contrastive learning head\\nincludes an average pooling layer for frame-level outputs,\\nfollowed by two dense layers featuring ReLU and tanh acti-\\nvation functions shown in Figure 2, respectively. The speaker\\nand emotion classification heads have a similar structure as\\nthe contrastive head but use softmax activation at the output\\nlayer, shown in Figure 2. In the speaker adversarial setting,\\nwe introduce an additional GRL layer after pooling and be-\\nfore the speaker classification head. All the proposed models\\ntake the raw waveform of an utterance as input.\\nDuring pretraining, we segment the input utterances into\\n4-second intervals and perform offline intra-speaker cluster-\\ning with N = 20. The models are pretrained for 250k steps\\nusing the AdamW optimizer with a batch size of 8. In the su-\\npervised SER training that follows, the model undergoes 30\\nepochs of training with a learning rate of 1e-5, stopping based\\non the validation accuracy. We repeat each supervised SER\\ntraining 5 times with different initialization seeds and mea-\\nsure unweighted average recall (UAR) during the evaluation.\\nFor the SER experiments based on wav2vec2.0 reported in\\nTable 3, the baseline wav2vec2.01 with 6 transformer layers,\\nis pretrained for 400k steps on voxceleb2. We then fine-tune\\nthis model with our strategies on voxceleb2 for an extra 50k\\nsteps. The feature extractor and transformer layers of fine-\\ntuned wav2vec2.0 are utilized in the supervised SER training.\\n4.2. Results and Discussion\\nAccording to the results in Table 2, our proposed contrastive\\nstrategy, denoted as Pretraining w/ proposed contrastive,\\ndemonstrate a significant improvement in SER compared to\\ncases with no pretraining in both datasets. We note that pre-\\ntraining with supervised speaker classification also leads to\\n1 https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec\\nTable 2: SER results, in terms of mean UAR.\\nPre-Training\\nIEMOCAP\\n(UAR)\\nCREMA- D\\n(UAR)\\nNo pretraining (small)\\n58.37\\n65.12\\nNo pretraining\\n58.54\\n64.15\\nPretraining w/ spk classification\\n67.57\\n75.23\\nPretraining w/ proposed contrastive\\n65.50\\n70.44\\nPretraining w/ proposed spk ADV\\n64.48\\n66.58\\nPretraining w/ proposed MTL\\n69.16\\n73.80\\nTable 3: SER results with wav2vec2.0, mean UAR.\\nPre-Training\\nIEMOCAP\\n(UAR)\\nCREMA- D\\n(UAR)\\nwav2vec2.0 [16]\\n72.14\\n80.78\\nFT wav2vec2.0 w/ proposed contrastive\\n72.78\\n81.72\\nFT wav2vec2.0 w/ proposed MTL\\n73.80\\n83.01\\nsubstantial improvements in both datasets, consistent with\\nfindings in [2]. The proposed multi-task learning approach,\\ndenoted as Pretraining w/ proposed MTL, leverages the in-\\nherent connection between speaker and emotion recognition,\\nwhile simultaneously considering intra-speaker variations\\nand obtains the best performance in the IEMOCAP corpus.\\nWe observe that speaker adversarial network degrades perfor-\\nmance, indicating that trying to remove speaker information\\nhas a negative impact and supports the connection between\\nspeaker and emotion recognition. In CREMA-D, the speaker\\nclassification baseline performs exceptionally well, possibly\\ndue to the presence of normalized linguistic content, cre-\\nating ideal conditions for discriminating emotions through\\nspeaker embeddings, as discussed in Section 2. Overall, these\\nresults underscore the effectiveness of our multi-task learn-\\ning method and highlight the strong relationship between\\nemotion and speaker recognition.\\nIn Table 3, baseline wav2vec2.0 model, pretrained with\\nvoxceleb2, performs impressively well as a pretraining\\nmethod for SER, underscoring its effectiveness. Fine-tuning\\nthis baseline with our contrastive learning strategy, FT\\nwav2vec2.0 w/ proposed contrastive, seems leading to mi-\\nnor improvements in both datasets. Fine-tuning wav2vec2.0\\nwith our proposed multi-task setting, FT wav2vec2.0 w/ pro-\\nposed MTL, yields substantial enhancement, highlighting the\\neffectiveness of our approach.\\n5. CONCLUSION\\nOur research reveals the potential of speaker embeddings for\\nenhancing SER task, even with limited labeled data.\\nOur\\nstudy establishes a direct link between emotions and state-\\nof-the-art speaker embeddings through intra-speaker clus-\\nters. Our novel contrastive pretraining approach on emotion-\\nunlabeled datasets, based on these clusters, significantly im-\\nproves SER performance, whether used alone or in multi-task\\nsettings. Our findings not only advance our understanding of\\nspeaker embeddings and emotions but also provide practical\\nsolutions for data scarcity in SER. As a future work, we in-\\ntend to extend the analysis of emotion information in speaker\\nembeddings, analyzing other factors which potentially affect\\nthe appearance of that information.\\n4\\n6. REFERENCES\\n[1] Vidhyasaharan Sethu, Emily Mower Provost, Julien Epps, Carlos\\nBusso, Nicholas Cummins, and Shrikanth S. Narayanan,\\n“The am-\\nbiguous world of emotion representation,” ArXiv, vol. abs/1909.00360,\\n2019.\\n[2] R. Pappagari, Tianzi Wang, Jes´us Villalba, Nanxin Chen, and Najim\\nDehak, “X-vectors meet emotions: A study on dependencies between\\nemotion and speaker recognition,”\\nICASSP 2020 - 2020 IEEE In-\\nternational Conference on Acoustics, Speech and Signal Processing\\n(ICASSP), pp. 7169–7173, 2020.\\n[3] Sarala Padi, Seyed Omid Sadjadi, Dinesh Manocha, and Ram D. Sri-\\nram,\\n“Improved speech emotion recognition using transfer learning\\nand spectrogram augmentation,” Proceedings of the 2021 International\\nConference on Multimodal Interaction, 2021.\\n[4] Sitong Zhou and Homayoon S. M. Beigi, “A transfer learning method\\nfor speech emotion recognition from automatic speech recognition,”\\nArXiv, vol. abs/2008.02863, 2020.\\n[5] Leonardo Pepino, Pablo Riera, and Luciana Ferrer, “Emotion Recog-\\nnition from Speech Using wav2vec 2.0 Embeddings,” in Proc. Inter-\\nspeech 2021, 2021, pp. 3400–3404.\\n[6] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman, “Voxceleb:\\nA large-scale speaker identification dataset,” in Interspeech, 2017.\\n[7] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman, “Voxceleb2:\\nDeep speaker recognition,” in Interspeech, 2018.\\n[8] Zakaria Aldeneh and Emily Mower Provost, “You’re not you when\\nyou’re angry: Robust emotion features emerge by recognizing speak-\\ners,” IEEE Transactions on Affective Computing, vol. 14, pp. 1351–\\n1362, 2023.\\n[9] Srinivas Parthasarathy, Chunlei Zhang, John H. L. Hansen, and Carlos\\nBusso, “A study of speaker verification performance with expressive\\nspeech,” 2017 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP), pp. 5540–5544, 2017.\\n[10] Srinivas Parthasarathy and Carlos Busso, “Predicting speaker recogni-\\ntion reliability by considering emotional content,” 2017 Seventh Inter-\\nnational Conference on Affective Computing and Intelligent Interaction\\n(ACII), pp. 434–439, 2017.\\n[11] Michelle I Bancroft, Reza Lotfian, John H. L. Hansen, and Carlos\\nBusso, “Exploring the intersection between speaker verification and\\nemotion recognition,” 2019 8th International Conference on Affective\\nComputing and Intelligent Interaction Workshops and Demos (ACIIW),\\npp. 337–342, 2019.\\n[12] Najim Dehak, Patrick Kenny, R´eda Dehak, Pierre Dumouchel, and\\nPierre Ouellet,\\n“Front-end factor analysis for speaker verification,”\\nIEEE Transactions on Audio, Speech, and Language Processing, vol.\\n19, pp. 788–798, 2011.\\n[13] Jennifer Williams and Simon King, “Disentangling style factors from\\nspeaker representations,” in Interspeech, 2019.\\n[14] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-Moreno, “Gen-\\neralized end-to-end loss for speaker verification,”\\n2018 IEEE In-\\nternational Conference on Acoustics, Speech and Signal Processing\\n(ICASSP), pp. 4879–4883, 2017.\\n[15] Brecht Desplanques,\\nJenthe Thienpondt,\\nand Kris Demuynck,\\n“ECAPA-TDNN: Emphasized Channel Attention, Propagation and Ag-\\ngregation in TDNN Based Speaker Verification,” in Proc. Interspeech\\n2020, 2020, pp. 3830–3834.\\n[16] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael\\nAuli,\\n“Wav2vec 2.0: A framework for self-supervised learning of\\nspeech representations,” in Proceedings of the 34th International Con-\\nference on Neural Information Processing Systems, Red Hook, NY,\\nUSA, 2020, NIPS’20, Curran Associates Inc.\\n[17] Edmilson da Silva Morais, Ron Hoory, Weizhong Zhu, Itai Gat,\\nMatheus Damasceno, and Hagai Aronowitz, “Speech emotion recogni-\\ntion using self-supervised features,”\\nICASSP 2022 - 2022 IEEE In-\\nternational Conference on Acoustics, Speech and Signal Processing\\n(ICASSP), pp. 6922–6926, 2022.\\n[18] Mao Li, Bo Yang, Joshua Levy, Andreas Stolcke, Viktor Rozgic, Spy-\\nros Matsoukas, Constantinos Papayiannis, Daniel Bone, and Chao\\nWang, “Contrastive unsupervised learning for speech emotion recogni-\\ntion,” ICASSP 2021 - 2021 IEEE International Conference on Acous-\\ntics, Speech and Signal Processing (ICASSP), pp. 6329–6333, 2021.\\n[19] Lucas Goncalves and Carlos Busso,\\n“Improving Speech Emotion\\nRecognition Using Self-Supervised Learning with Domain-Specific\\nAudiovisual Tasks,” in Proc. Interspeech 2022, 2022, pp. 1168–1172.\\n[20] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh,\\nEmily Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and\\nShrikanth S Narayanan, “Iemocap: Interactive emotional dyadic mo-\\ntion capture database,” Language resources and evaluation, vol. 42, pp.\\n335–359, 2008.\\n[21] Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li, “Emotional voice\\nconversion: Theory, databases and esd,” Speech Communication, vol.\\n137, pp. 1–18, 2022.\\n[22] Houwei Cao, David G. Cooper, Michael K. Keutmann, Ruben C. Gur,\\nAni Nenkova, and Ragini Verma, “Crema-d: Crowd-sourced emotional\\nmultimodal actors dataset,” IEEE Transactions on Affective Computing,\\nvol. 5, no. 4, pp. 377–390, 2014.\\n[23] Steven R Livingstone and Frank A Russo, “The ryerson audio-visual\\ndatabase of emotional speech and song (ravdess): A dynamic, multi-\\nmodal set of facial and vocal expressions in north american english,”\\nPloS one, vol. 13, no. 5, pp. e0196391, 2018.\\n[24] Xuan Vinh Nguyen, Julien Epps, and James Bailey, “Information the-\\noretic measures for clusterings comparison: Variants, properties, nor-\\nmalization and correction for chance,” J. Mach. Learn. Res., vol. 11,\\npp. 2837–2854, 2010.\\n[25] Er´endira Rend´on, Itzel M. Abundez, Citlalih Gutierrez, Sergio D´ıaz\\nZagal, Alejandra Arizmendi, Elvia M. Quiroz, and H. Elsa Arzate,\\n“A comparison of internal and external cluster validation indexes,” in\\nProceedings of the 2011 American Conference on Applied Mathemat-\\nics and the 5th WSEAS International Conference on Computer En-\\ngineering and Applications, Stevens Point, Wisconsin, USA, 2011,\\nAMERICAN-MATH’11/CEA’11, p. 158–163, World Scientific and\\nEngineering Academy and Society (WSEAS).\\n[26] Peter J. Rousseeuw, “Silhouettes: a graphical aid to the interpretation\\nand validation of cluster analysis,” Journal of Computational and Ap-\\nplied Mathematics, vol. 20, pp. 53–65, 1987.\\n[27] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakho-\\ntia, Ruslan Salakhutdinov, and Abdelrahman Mohamed, “Hubert: Self-\\nsupervised speech representation learning by masked prediction of hid-\\nden units,” IEEE/ACM Transactions on Audio, Speech, and Language\\nProcessing, vol. 29, pp. 3451–3460, 2021.\\n[28] Bing Han, Zhengyang Chen, and Yanmin Qian, “Self-supervised learn-\\ning with cluster-aware-dino for high-performance robust speaker veri-\\nfication,” ArXiv, vol. abs/2304.05754, 2023.\\n[29] Ruijie Tao, Kong-Aik Lee, Rohan Kumar Das, Ville Hautamaki, and\\nHaizhou Li,\\n“Self-supervised speaker recognition with loss-gated\\nlearning,”\\nICASSP 2022 - 2022 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP), pp. 6142–6146,\\n2021.\\n[30] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hin-\\nton, “A simple framework for contrastive learning of visual representa-\\ntions,” in Proceedings of the 37th International Conference on Machine\\nLearning. 2020, ICML’20, JMLR.org.\\n[31] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick,\\n“Momentum contrast for unsupervised visual representation learning,”\\n2020 IEEE/CVF Conference on Computer Vision and Pattern Recogni-\\ntion (CVPR), pp. 9726–9735, 2019.\\n5\\n'},\n",
       " {'abstract': 'We propose creating a custom Generative Pre-Trained Transformer (GPT) for developers to discuss and solve ethical issues in AI engineering, ensuring legal compliance and ethical perspectives.',\n",
       "  'introduction': 'Current AI software can raise ethical concerns, and legal requirements are often general and open to subversion. We aim to develop a tool to aid developers in addressing these issues.',\n",
       "  'literature review': 'Previous approaches like privacy chatbots and legal compliance APIs address limited aspects of AI ethics. We address a broad range of AI-related software ethical issues with a focus on underrepresented perspectives.',\n",
       "  'methodology': 'We will create a high-quality dataset and analyze biases in existing LLM. We will develop deterministic responses to critical questions and validate our custom tool with AI engineers.',\n",
       "  'results': 'Our tool will generate diverse ethical perspectives, assisting developers in creating AI solutions that comply with legal requirements and consider diverse ethical perspectives. A demonstration addresses the ethical concern of cyberbullying through AI-based solutions.',\n",
       "  'conclusion': 'Our custom conversational agent will aid developers in creating ethical AI solutions, improving legal compliance, and fostering ethical considerations in software development.',\n",
       "  'title': 'Custom Developer GPT for Ethical AI Solutions',\n",
       "  'author': 'Lauren Olson',\n",
       "  'textdata': 'Custom Developer GPT for Ethical AI Solutions\\nLauren Olson\\nl.a.olson@vu.nl\\nVrije Universiteit Amsterdam\\nAmsterdam, Netherlands\\nABSTRACT\\nThe main goal of this project is to create a new software artefact: a\\ncustom Generative Pre-trained Transformer (GPT) for developers\\nto discuss and solve ethical issues through AI engineering. This\\nconversational agent will provide developers with practical appli-\\ncation on (1) how to comply with legal frameworks which regard\\nAI systems (like the EU AI Act [8] and GDPR [11]) and (2) present\\nalternate ethical perspectives to allow developers to understand\\nand incorporate alternate moral positions. In this paper, we provide\\nmotivation for the need of such an agent, detail our idea and demon-\\nstrate a use case. The use of such a tool can allow practitioners to\\nengineer AI solutions which meet legal requirements and satisfy\\ndiverse ethical perspectives.\\nACM Reference Format:\\nLauren Olson. 2024. Custom Developer GPT for Ethical AI Solutions. In\\nProceedings of 3rd International Conference on AI Engineering — Software\\nEngineering for AI (CAIN 2024). ACM, New York, NY, USA, 3 pages. https:\\n//doi.org/10.1145/nnnnnnn.nnnnnnn\\n1\\nINTRODUCTION\\nCurrent development strategies contain roles, artefacts, ceremonies,\\nand cultures that focus on business rather than human ethical val-\\nues [6]. The business focus of these standard practices facilitates\\nthe creation of unethical AI software, creating myriad ethical con-\\ncerns. Ethical concerns, issues regarding the subversion of ethical\\nvalues, plague software technologies. These concerns include cy-\\nberbullying, privacy, and censorship, and have been at the forefront\\nof modern societal struggles. AI plays a predominant role in the\\npropagation of these ethical concerns due to its ubiquity and effec-\\ntiveness in modern software solutions; therefore, any solution to\\nameliorate these issues will likely also require AI solutions. Fur-\\nthermore, when incorporating ethical standards (like GDPR) into\\nAI software, some developers find legal requirements general and\\ndifficult to apply consistently [6]. This legal ambiguity also makes it\\neasier for software companies to subvert ethical values while techni-\\ncally following legal requirements [5], leading to continued ethical\\nconcerns. As potential solutions, few software tools have been pro-\\nposed to aid developers in complying with AI legal frameworks: a\\nprivacy chatbot [1] and legal compliance API [4]. In our approach,\\nwe aim to improve the previously proposed privacy chatbot [1] by\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nCAIN 2024, April 2024, Lisbon, Portugal\\n© 2024 Association for Computing Machinery.\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\nexpanding the ethical concerns addressed by the conversational\\nagent to encompass a wide range of AI-related software ethical\\nissues.\\nIn addition, the restricted demography of software practitioners,\\nespecially in decision-making roles, narrows the available ethical\\nperspectives and concerns discussed in development processes.\\nEighty-five percent of software developers are men, and most\\nare white, English-speaking, middle to upper-class men from the\\nUSA [2]. The background of these developers drive their perspec-\\ntives and priorities regarding software development, with studies\\nshowing that political affiliations affect design decisions [2]. There-\\nfore, the human values reflected in many software products may\\nreflect only a small portion of the population.\\nUnfortunately, current artefacts for integrating minoritized per-\\nspectives into the design practice, like user personas and user jour-\\nney stories, have received criticism for creating biased, stereotyped\\nviews of identities [9]. This lack of proper artefacts, combined with\\na dearth of user feedback tools for analysis and triangulation in\\ngeneral [7], demonstrate a gap for new, improved artefacts which\\ncapture and integrate user perspectives on software, especially mi-\\nnoritized populations’ ethical concerns. As a result, our proposed\\nconversational agent will be trained on real user feedback data\\ncollected from minoritized participants.\\nIn this paper, we propose a custom Generative Pre-trained Trans-\\nformer (GPT). GPTs have already become widely popular tools\\namongst developers, with many companies considering it best prac-\\ntice for practitioners to use when developing software. However,\\nthese systems are non-determininstic 1 and contain unknown safe-\\nguards and biases. Therefore, will we develop a custom GPT with\\ntwo critical features: (1) a sizeable knowledge base of data from\\nminoritized groups on their ethical concerns regarding software\\nand (2) deterministic responses to feature-elicitation prompts with\\nAI features which follow legal frameworks and address diverse ethical\\nperspectives.\\nThis tool will function within software development processes\\nas both an interactive, data-driven user story and a values transla-\\ntor [6]. As such, it will be used to define value streams and epics,\\ncreate and prioritize features, and guide sprint planning, design and\\ntesting. As a values translator, it will ‘communicate values concepts\\nin a terminology understandable for the development teams [6].’\\n2\\nIMPLEMENTATION\\nTo implement our new SE artefact, we will (1) create a high-quality\\ndataset of diverse users’ ethical concerns, (2) discover the inherent\\nLLM biases and trained safeguards of GPT surrounding these ethical\\nconversations, (3) create a GPT customized with the most recent\\nlegal and ethical frameworks, our data, and deterministic responses\\nbased on our data to users’ top concerns.\\n1given the same input, output is not constant\\narXiv:2401.11013v1  [cs.SE]  19 Jan 2024\\nCAIN 2024, April 2024, Lisbon, Portugal\\nLauren Olson\\nWe will first collect data on minoritized communities’ ethical\\nconcerns to provide the LLM with high-quality training data to\\nsummarize the ethical needs of these communities. This data will\\nallow for the creation of a new software artefact: an interactive, data-\\ndriven user journey story. We have already completed this first step\\nby collecting, annotating, and analyzing over 2000 Reddit posts\\nfrom seven minoritized communities for software-based ethical\\nconcerns [10]. The Reddit posts detail user experiences of ethical\\nconcerns with seven different software platforms.\\nThe next step to properly building this software tool is, through\\ncustom prompts, to determine the existing conversational space\\nsurrounding AI-based ethical concerns. When dealing with ethical\\nconcerns and minoritized populations specifically, it is critical to\\nproactively determine potential biases and pre-trained safeguards\\nwhich restrict and color the GPT’s responses. From these results,\\nwe will develop deterministic responses to critical questions to\\nprevent responses which conflict with users’ ethical requirements\\nor existing legal requirements.\\nThe final step is to create the custom GPT. It is essential to\\ndevelop this tool with the developers who will use it: AI engineers.\\nWe plan to perform user studies with AI engineers to ensure the\\ntool is usable and integrates well into their existing development\\npipelines. Through these user studies, we will also test whether\\nmore ethical AI solutions are successfully developed by using our\\ntool.\\n3\\nUSE CASE\\nConsider a specific cyberbullying concern: online, non-consensual\\npornography, where sexually explicit material of a person is shared\\nwithout their consent. In several countries, non-consensual pornog-\\nraphy is a crime and if platforms do not take reasonable measures\\nto remove it, they can be held legally accountable; therefore, plat-\\nforms need to take measures to prevent its occurrence. In our initial\\nwork, we find that women of color highly report non-consensual\\npornography as an ethical concern [10]. To combat non-consensual\\npornography, women typically have to manually monitor online\\nchannels to curb the spread of this unwanted content. In a recent\\nstudy, researchers found that nearly 40 percent of platforms did not\\nhave any reporting interfaces and only 16% allowed users to indicate\\nthe occurrence of non-consensual pornography with a proper legal\\nvocabulary [3]. However, these reporting systems place the burden\\nof solving the issue on the victim and it is incredibly difficult to\\nmanually monitor the spread of content across the internet. Instead,\\nsoftware practitioners could engineer an AI solution to automat-\\nically identify and remove non-consensual pornography, so that\\nwomen don’t have to manually track and report non-consensual\\nimages.\\nInstead of having software which worsens this issue by having\\ninsufficient software design, our conversational agent could pro-\\nvide developers with (1) real women’s ethical complaints, which\\ndetail struggles with non-consensual pornography, (2) legal require-\\nments regarding non-consensual pornography, and (3) examples\\nof features which could be implemented which satisfy (1) and (2).\\nTo tackle this challenge, the developer will first prompt our GPT\\nfor women’s ethical concerns and elicit a response informed by its\\nalready high-quality knowledge base and our data (see Figure 1).\\nFigure 1: Sample prompt showing developer eliciting\\nwomen’s\\ncyberbullying\\nrequirements,\\nnon-consensual\\npornography underlined, full GPT response not shown due\\nto size constraints\\nFigure 2: Sample prompt showing developer finding solu-\\ntion to non-consensual pornography, full GPT response not\\nshown due to size constraints\\nNext, the developer will identify non-consensual pornography as\\nan ethical concern which can be solved through AI, and prompt\\nthe GPT for a proper solution (see Figure 2). Figures 1 and 2 show\\ninitial prompts and responses; further prompting is necessary to\\nobtain a full solution.\\nCustom Developer GPT for Ethical AI Solutions\\nCAIN 2024, April 2024, Lisbon, Portugal\\nREFERENCES\\n[1] Lamya Alkhariji, Suparna De, Omer Rana, and Charity Perera. 2022. Poster:\\nOntology Enabled Chatbot for Applying Privacy by Design in IoT Systems. In\\nProceedings of the 2022 ACM SIGSAC Conference on Computer and Communications\\nSecurity. 3323–3325.\\n[2] Sasha Costanza-Chock. 2020. Design justice: Community-led practices to build the\\nworlds we need. The MIT Press.\\n[3] Antonella De Angeli, Mattia Falduti, Maria Menendez-Blanco, and Sergio Tessaris.\\n2023. Reporting non-consensual pornography: clarity, efficiency and distress.\\nMultimedia Tools and Applications 82, 9 (2023), 12829–12858.\\n[4] Catalina Goanta, Thales Bertaglia, and Adriana Iamnitchi. 2022. The case for\\na legal compliance API for the enforcement of the EU’s digital services act on\\nsocial media platforms. In Proceedings of the 2022 ACM Conference on Fairness,\\nAccountability, and Transparency. 1341–1349.\\n[5] Colin M Gray, Cristiana Santos, Nataliia Bielova, Michael Toth, and Damian\\nClifford. 2021. Dark patterns and the legal requirements of consent banners: An\\ninteraction criticism perspective. In Proceedings of the 2021 CHI Conference on\\nHuman Factors in Computing Systems. 1–18.\\n[6] Waqar Hussain, Mojtaba Shahin, Rashina Hoda, Jon Whittle, Harsha Perera, Arif\\nNurwidyantoro, Rifat Ara Shams, and Gillian Oliver. 2022. How can human\\nvalues be addressed in agile methods? A case study on SAFe. IEEE Transactions\\non Software Engineering 48, 12 (2022), 5158–5175.\\n[7] Ze Shi Li, Nowshin Nawar Arony, Kezia Devathasan, Manish Sihag, Neil Ernst,\\nand Daniela Damian. 2023. Unveiling the Life Cycle of User Feedback: Best\\nPractices from Software Practitioners. arXiv:2309.07345 [cs.SE]\\n[8] Tambiama Madiega. 2023.\\nArtificial Intelligence Act.\\nhttps:\\n//www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-\\nai-act-first-regulation-on-artificial-intelligence\\n[9] Nicola Marsden and Maren Haag. 2016. Stereotypes and politics: reflections on\\npersonas. In Proceedings of the 2016 CHI conference on human factors in computing\\nsystems. ACM, San Jose, CA, USA, 4017–4031.\\n[10] L. Olson, E. Guzman, and F. Kunneman. 2023. Along the Margins: Marginalized\\nCommunities’ Ethical Concerns about Social Platforms. In 2023 IEEE/ACM 45th\\nInternational Conference on Software Engineering: Software Engineering in Society\\n(ICSE-SEIS). IEEE Computer Society, Los Alamitos, CA, USA, 71–82.\\nhttps:\\n//doi.org/10.1109/ICSE-SEIS58686.2023.00013\\n[11] European Union. 2016. General Data Protection Regulation. https://gdpr-info.eu\\n'},\n",
       " {'abstract': \"This workshop report provides insights into the context of the creation and the underlying methodological considerations behind the authors' published game database. The database was collaboratively developed and lists digital games developed in Germany, Austria, and Switzerland up to the year 2000. In this report, in addition to our initial considerations and the various work steps involved in the realization, we also outline the data basis on which the database was built and tested, the goals of the data model, and the difficulties we faced during the process of creation. Subsequently, we classify the current status of the game database and provide an outlook on the further plans for the project.\",\n",
       "  'introduction': 'With the growing interest from research and journalism in digital games, there is also an increasing awareness of their history. It quickly becomes apparent that the \"new\" medium - depending on the starting point - is not even 50 years old. However, something else also became apparent: A kind of US-American-Japanese master narrative quickly emerged, which is often uncritically repeated to this day. It tells the story of mostly male, white geniuses and innovators who, often against the spirit of the times, implemented their visions: from the young MIT students who repurposed a PDP-10 computer for the night to develop the game Spacewar! to the self-made millionaire and enfant terrible Nolan Bushnell, who triggered the first video game boom in the USA with the foundation of Atari.',\n",
       "  'literature review': 'In recent times, a shift in focus towards regional and national digital game history has become apparent - here, too, a similar trend can be observed in research and journalism. Graeme Kirkpatrick has dedicated himself to video game culture in the United Kingdom (Kirkpatrick 2015), Alexis Blanchet and Guillaume Montagnon researched the origins of the \"French Touch\" (Blanchet and Montagnon 2020), and Melanie Swalwell has dealt extensively with the origins of the so-called homebrew culture in Australia and New Zealand (Swalwell 2021). These first studies on a local history of digital games have impressively shown that these were not marginal notes of a primarily US-American-Japanese dynamic. Jaroslav Švelch was able to demonstrate in his study on the early history of digital games in Czechoslovakia that even behind the Iron Curtain and against all odds, an independent culture of digital games emerged in the 1980s (Švelch 2018).',\n",
       "  'methodology': 'These preliminary studies made us increasingly aware of the urgent need for this research. To give one example, if one wants to better understand the success of the Anno series today, a look at the Ultimate History of Video Games is of no help. In the US-American video game chronicles, the phenomenon \"economic simulation and construction game\" is rarely or never mentioned. Fortunately, we are currently witnessing many simultaneously emerging and interconnected initiatives to research the respective history of digital games throughout Europe as well as in South America, Asia, Africa and Oceania. In Germany and Austria, several projects are currently in the making, and in Switzerland, a large SNF-Sinergia project on the research of local video game history is being funded from 2023 to 2027, in which four universities and colleges as well as 20 researchers are involved, including three of the four authors of this workshop report.',\n",
       "  'results': 'All these projects face a similar problem at the beginning: One basically knows, often from own experience, about games from the investigated geographical area, but there are only a few, incomplete lists or databases of these games - and sometimes none at all. However, such datasets - even if they are only rudimentary - are a basic prerequisite for most historical research questions, even if they are not primarily quantitative studies: How many games were developed in total in which period of time and how has this number changed? Which systems were particularly popular where and when? Which genres were popular when and where? How many developers were normally involved at which times? When and where did transnational cooperations take place?',\n",
       "  'conclusion': 'This critical point does not apply to the collection work itself. After all, these are private initiatives that live from the unpaid collection and research work of individuals. They are a valuable resource and were very helpful for our work, but are not suitable as databases for research, also because they do not claim to be complete - but above all because they have not been scientifically critically reviewed.',\n",
       "  'title': 'Warum wir es für eine gute Idee gehalten haben, eine DACH-Spieledatenbank aufzubauen',\n",
       "  'author': 'Eugen Pfister, Aurelia Brandenburg, Adrian Demleitner, Lukas Daniel Klausner',\n",
       "  'textdata': \"Warum wir es für eine gute Idee \\ngehalten haben, eine DACH-\\nSpieledatenbank aufzubauen\\nEugen Pfister, Aurelia Brandenburg, Adrian Demleitner und Lukas Daniel Klausner\\nZusammenfassung\\nUnser  Werkstattbericht  gibt  Einblick  in  den  Entstehungskontext  sowie  die  zugrundeliegenden\\nmethodischen Überlegungen hinter der von den Autor*innen publizierten Spieledatenbank. Diese\\nwurde kollaborativ erarbeitet und führt digitale Spiele, die in Deutschland, Österreich und der\\nSchweiz bis zum Jahr 2000 entwickelt wurden. In diesem Bericht skizzieren wir neben unseren\\nAusgangsüberlegungen und  den  verschiedenen  Arbeitsschritten  bei  der  Realisierung  außerdem\\nauch, auf welcher Datenbasis die Datenbank aufgebaut und geprüft wurde, was die Ziele des\\nDatenmodells sind und mit welchen Schwierigkeiten wir im Prozess der Erstellung konfrontiert\\nwaren. Hiernach ordnen wir den aktuellen Stand der Spieledatenbank ein und geben einen Ausblick\\nauf die weiteren Pläne des Projekts. \\nSchlüsselwörter\\nGame Studies, Spielgeschichte, local history, Datenbank, Datenmodellierung, Digitalisierung, \\nWerkstattbericht\\n1. Einleitung und Motivation\\nMit dem wachsenden Interesse von Forschung und Journalismus an digitalen Spielen geht auch ein\\nwachsendes Bewusstsein für ihre Geschichte einher. Hier zeigt sich rasch, dass das „neue“ Medium\\nmit – je nach gesetztem Anfangspunkt – mindestens fünfzig Jahren Geschichte gar nicht mehr so\\njung ist. Es zeigte sich aber auch etwas Anderes: Rasch hat sich eine Art US-amerikanisch-japanische\\nMeistererzählung herauskristallisiert, die bis heute häufig unkritisch wiederholt wird. Sie erzählt eine\\nGeschichte von meist männlichen, weißen Genies und Innovatoren, die oft gegen den Zeitgeist ihre\\nVisionen umgesetzt haben: Von den jungen Studenten des MIT, die nächtens einen PDP-10-Rechner\\nzweckentfremdet haben, um darauf das Spiel  Spacewar! zu entwickeln, bis hin zum Self-made-\\nMillionär und Enfant terrible Nolan Bushnell, der mit der Gründung von Atari den ersten Videospiel-\\nBoom in den USA ausgelöst hat.\\nSpäter liest man vom großen Video Game Crash in den USA, von hunderttausenden Spielecartridges,\\ndie  verschämt  in  der  Wüste  vergraben  wurden,  vom  Aufstieg  der  japanischen  Unternehmen\\nNintendo und Sega und deren „Console Wars“. Diese Geschichte, wie wir sie zum Beispiel in Steven\\nKents Ultimate History of Video Games lesen können (Kent 2001), ist nicht grundsätzlich falsch – sie\\nblendet aber quasi alle Länder abseits der USA, Japans und vielleicht noch Großbritanniens aus.\\nDabei spürte man in Europa beispielsweise  nur  wenig  vom  Crash, weil  hier  Konsolen  nie  so\\nverbreitet waren wie in den USA. Und trotzdem wurde fast überall gespielt: auf Jahrmärkten, in\\nSpielhallen und zu Hause. In Europa hatten sich etwa dynamische Entwickler*innen-Netzwerke um\\ndie hier populären Mikrocomputer gebildet: um den ZX Spectrum und den Amstrad CPC, vor allem\\naber um den C64, den Commodore Amiga und MS-DOS-kompatible Computer. Um diese Plattformen\\nherum  entstanden  in  Europa,  aber  auch  in  Australien  und  Neuseeland  extrem  produktive\\neigenständige Spielekulturen.\\nErst in jüngster Zeit hat sich – auch hier zeigt sich ein ähnlicher Trend in Forschung und Journalismus\\n– der Fokus verstärkt auf die regionale und nationale digitale Spielegeschichte verlagert. Graeme\\nKirkpatrick hat sich der Videospielkultur im Vereinigten Königreich gewidmet (Kirkpatrick 2015),\\nAlexis Blanchet und Guillaume Montagnon forschten den Ursprüngen des „French Touch“ nach\\n(Blanchet und Montagnon 2020) und Melanie Swalwell hat sich ausführlich mit den Ursprüngen der\\nsogenannten  Homebrew Culture in Australien und Neuseeland beschäftigt (Swalwell 2021). Diese\\nersten Studien zu einer local history digitaler Spiele haben eindrücklich gezeigt, dass es sich hierbei\\nnicht um Randnotizen einer primär US-amerikanisch-japanischen Dynamik gehandelt hat. Jaroslav\\nŠvelch konnte in seiner Studie zur frühen Geschichte digitaler Spiele in der Tschechoslowakei\\nnachweisen, dass es in den 1980er-Jahren sogar hinter dem Eisernen Vorhang und gegen alle\\nWidrigkeiten zur Entwicklung einer eigenständigen Kultur digitaler Spiele kam (Švelch 2018).\\nDurch diese Vorarbeiten wurde uns immer deutlicher bewusst, wie dringend notwendig diese\\nForschung ist. Um ein Beispiel zu nennen: Will man heute etwa den Erfolg der Anno-Reihe besser\\nverstehen, hilft kein  Blick in die  Ultimate History of Video Games. In den US-amerikanischen\\nVideospielchroniken findet das Phänomen „Wirtschaftssimulation und Aufbauspiel“ selten bis nie\\nErwähnung.  Zum  Glück  werden  wir  gerade  Zeug*innen  vieler  gleichzeitig  entstehender  und\\nmiteinander vernetzter Initiativen zur Erforschung der jeweiligen Geschichte digitaler Spiele in ganz\\nEuropa sowie auch in Südamerika, Asien, Afrika und Ozeanien. In Deutschland und Österreich sind\\ngerade mehrere Projekte im Entstehen begriffen, und in der Schweiz wird von 2023 bis 2027 ein\\ngroßes SNF-Sinergia-Projekt zur Erforschung der lokalen Videospielgeschichte finanziert, an dem vier\\nUniversitäten und Hochschulen sowie 20 Forscher*innen beteiligt sind, unter anderem auch drei der\\nvier Autor*innen dieses Werkstattberichts.1\\nAlle diese Projekte stehen zu Beginn vor einem ähnlichen Problem: Man weiß grundsätzlich, oft aus\\neigener Erfahrung, von Spielen aus dem untersuchten geografischen Raum, es gibt aber nur wenige,\\nunvollständige Listen oder Datenbanken dieser Spiele – und manchmal auch gar keine. Solche – und\\nseien  es  nur  rudimentäre  –  Datensätze  sind  aber  Grundvoraussetzung  für  die  meisten\\n1 Siehe auch das Blog zum Projekt unter https://chludens.hypotheses.org/.\\ngeschichtlichen Forschungsfragen, auch wenn es keine primär quantitativen Studien sind: Wie viele\\nSpiele wurden insgesamt ungefähr in welchem Zeitraum entwickelt und wie hat sich diese Zahl\\nverändert? Welche Systeme waren wann wo besonders verbreitet? Welche Genres waren wann wo\\npopulär? Wie viele Entwickler*innen waren zu welchen Zeitpunkten normalerweise beteiligt? Wann\\nund wo kam es zu transnationalen Kooperationen?\\nEgal, ob man sich für frühe Homebrew-Spiele interessiert, für Spiele, die für den Schulunterricht\\neingesetzt wurden, oder für das Entstehen der ersten kommerziellen Entwicklungsstudios – um ein\\nPhänomen historisch erfassen zu können, ist es notwendig, zumindest einen rudimentären Überblick\\nüber die Datenlage zu haben. Das zeigt sich auch daran, wie erschreckend schnell vergessen wird:\\nAuch in unserem Team hat sich herausgestellt, dass wir alle die Anzahl der Spiele, die bis zum Jahr\\n2000 entwickelt wurden, dramatisch unterschätzt hatten.\\nAus den genannten Gründen war es für uns wichtig, eine Datenbank von Spielen für Deutschland,\\nÖsterreich und die Schweiz zu erstellen. Im Frühjahr 2023 haben wir, nach etwas über einem Jahr\\nArbeit, eine erste (noch sehr unvollständige) Version unserer Arbeit in Open Access veröffentlicht\\n(Pfister et al. 2023), gerade um es anderen Forscher*innen, Journalist*innen oder auch einfach nur\\nInteressierten in Zukunft leichter zu machen. Wie es dazu kam, von den Herausforderungen und\\nRückschlägen, aber auch von ersten Entdeckungen wollen wir im Folgenden berichten – beginnend\\nmit der Ausgangslage.\\n2. Quellenbasis\\nEs wäre vermessen, behaupten zu wollen, dass es bisher gar keine Spieledatenbanken für den\\ndeutschsprachigen Raum gegeben hätte. Vielmehr ist es so, dass wir ohne die Vorarbeit von\\ntausenden Beitragenden gar nicht in der Lage gewesen wären, unsere Datenbank aufzubauen.\\nAllerdings  war  die  Ausgangslage  gerade  für  unsere  Bedürfnisse  unzufriedenstellend:  Die\\nenglischsprachigen Listen auf Wikipedia2 geben wirklich nur einen allerersten, sehr begrenzten\\nEinblick in die Entwicklungsgeschichte der drei Länder, insbesondere im Hinblick auf die frühe\\nGeschichte digitaler Spiele.\\nWeitaus ergiebiger sind da schon einschlägige Spiele-Datenbanken. Eine erste Kategorie von diesen\\nsind  große,  webbasierte  Angebote, namentlich  MobyGames (http://mobygames.com/)  und  die\\nUniversal  Videogame  List (https://www.uvlist.net/).  Beides  sind  umfassende,  crowdgesourcte\\nDatenbanken, die für unser Projekt sehr hilfreich beim Nachschlagen von Daten und Fakten zu\\n2 Konkret also „Category:Video games developed in Germany“ (https://en.wikipedia.org/w/index.php?\\ntitle=Category:Video_games_developed_in_Germany), „Category:Video games developed in \\nSwitzerland“ (https://en.wikipedia.org/wiki/Category:Video_games_developed_in_Switzerland) und \\n„List of video games developed in Austria“ \\n(https://en.wikipedia.org/wiki/List_of_video_games_developed_in_Austria).\\nbereits identifizierten Spielen waren. Was sie jedoch nicht einfach erlauben, ist die Suche nach\\nLändermerkmalen  der  beteiligten  Personen  oder  Firmen.  MobyGames,  die  wahrscheinlich\\nvollständigste internationale Datenbank digitaler Spiele, erlaubt es etwa gar nicht, nach den Ländern\\nder Entwickler*innen zu filtern oder zu durchsuchen, und auch hier hat sich im Zuge unserer\\nRecherchen gezeigt, dass insbesondere frühe im DACH-Raum entwickelte Spiele oft fehlen. Die\\ndeutschsprachige Online-Games-Datenbank (OGDB, https://ogdb.eu/) wiederum lässt sich zwar nach\\nHerkunftsland filtern, aber auch hier fehlen viele Spiele aus den 1980er- und 1990er-Jahren. Vor\\nallem aber eignet sich die OGDB nur bedingt für wissenschaftliche und journalistische Recherchen,\\nda sie sich zum einen nicht nach Jahren ordnen lässt und zum anderen alle weiterführenden\\nAngaben zu den Entwicklerstudios, Publishern und allen beteiligten Entwickler*innen fehlen. Die\\ndänische Datenbank Play:Right (https://\\n \\n www.playrigh\\n \\n t.dk/\\n \\n ) schließlich lässt sich nach Herkunftsland\\nfiltern und dann nach Erscheinungsjahr sortieren, ist aber ebenfalls sehr unvollständig.3\\nDieser Kritikpunkt trifft auch auf eine weitere essenzielle Art von Informationsquellen zu, nämlich\\njene Datenbanken, welche sich auf einzelne Plattformen bzw. Systeme fokussieren. Beispielhaft\\naufzuführen  wären  unter  anderem  Hall  of  Light (https://hol.abime.net/)  und  Lemon  Amiga\\n(https://www.lemonamiga.com/),\\n C64-Wiki \\n(https://www.c64-wiki.de/)  und\\n Gamebase  64\\n(http://www.gamebase64.com/) oder Atari Mania (http://www.atarimania.com/). Diese Plattformen\\nsind  in  ihrer  Organisation  oftmals  an  Wikis  angelehnt.  Das  heißt,  sie  funktionieren  dank  der\\nfreiwilligen Teilnahme ungezählter interessierter Personen und Enthusiast*innen. Diese investieren\\nihre Freizeit, um die Plattformen mit Wissen zu füttern. Teilweise schlägt sich das in umfassenden\\n(für die Forschung extrem wertvollen) Datensammlungen nieder: Im C64-Wiki etwa finden sich zu\\nvielen kleinen Spielen neben Beschreibungen auch Anleitungen und Lösungen, Karten, Screenshots,\\nScans der Cover usw. Aus der Sicht der Wissenschaft leiden diese Plattformen aber unter den\\ngleichen Problemen wie auch die zuvor genannten crowdgesourcten Quellen: Die Provenienz der\\nDaten der einzelnen Einträge sind nicht oder nur schwer nachzuvollziehen.\\nEs  ist  uns  an  dieser  Stelle  wichtig  darauf  hinzuweisen,  dass  es  sich  bei  diesen  kritischen\\nAnmerkungen nicht um eine Kritik an der Sammlungsarbeit an sich handelt. Schließlich handelt es\\nsich hierbei um Privatinitiativen, die von der unbezahlten Sammel- und Recherchearbeit Einzelner\\nleben. Sie sind eine wertvolle Ressource und waren sehr hilfreich für unsere Arbeit, eignen sich aber\\nnicht  als  Datenbanken  für  die  Forschung,  auch  deshalb,  weil  sie  gar  keinen  Anspruch  auf\\nVollständigkeit stellen können – vor allem aber, weil sie nicht wissenschaftlich kritisch überprüft\\nwurden. \\n3 Bis inklusive 1999 werden etwa nur 12 österreichische, 168 deutsche und 3 Schweizer Spiele \\naufgelistet.\\nUnser Ansatz für die Erstellung unserer eigenen Liste beinhaltete deshalb neben der zweifachen\\nKontrolle auch die Triangulation von Quellen. Wir nehmen an, dass diese Quellen ihr Wissen auch\\nuntereinander teilen und so unter Umständen Falschinformationen wiederholen. Daher haben wir in\\nAnsätzen  versucht,  die  Informationen  mit  Quellen  abzugleichen,  welche  nicht  auf  diesen\\nspezialisierten Plattformen verzeichnet waren. Eine interessante Differenz in diesen spezialisierten\\nPlattformen lässt sich an den erweiterten Suchmasken ausmachen. Einige Plattformen, wie zum\\nBeispiel Hall of Light und Atari Mania, hatten tatsächlich Optionen für die Suche nach Land oder\\nSprache. Andere, etwa Gamebase 64, ließen sich stattdessen danach durchsuchen, wer ein Spiel\\ngecrackt hatte.4\\nZuletzt haben wir außerdem versucht, die Metadaten zu einzelnen Spielen auch darüber hinaus mit\\nentsprechenden Einträgen in größeren Datenbanken abzugleichen, die nicht oder nur schlecht für\\nunsere Zwecke durchsuchbar waren, aber die Überprüfung von Informationen erlauben – Seiten wie\\netwa die Interactive Fiction Database (IFDB, https://ifdb.org/), bei der es sich ebenfalls um ein Wiki-\\nähnliches Projekt handelt, die aber nicht auf ein System, sondern auf ein Genre fokussiert ist. Diese\\nSeiten katalogisieren so z. B. allgemein Spiele, sind aber mehr auf eine Veröffentlichung und\\nArchivierung von kleinen Projekten und/oder ihren Metadaten ausgelegt und nicht darauf, Spiele\\nunter speziellen Kriterien zu recherchieren. Die komplexeste Zugriffsmöglichkeit stellte hier noch die\\nSammlung des  Internet Archives (https://archive.org/details/classicpcgames) dar, das inzwischen\\nauch im großen Stil ältere Software zum Download oder sogar direkt im Browser zugänglich macht\\nund so teilweise durch Filtermöglichkeiten der Suche oder untereinander verlinkte Einträge die\\nRecherche zu einzelnen Spielen erleichtern kann. Außerdem hostet das Internet Archive schön\\nlänger  eine  umfassende  Sammlung  an  Schriftquellen  zu  digitaler  Spielgeschichte,  wie  etwa\\nSpielemagazine  und  Werbeanzeigen,  mit  denen  sich  ebenfalls  z.  B.  Informationen  aus\\ncrowdgesourcten Angeboten gegenprüfen lassen.\\nWährend diese Seiten allerdings teils ausführliche Metadaten aufweisen, sind sie alle primär dafür\\ngedacht, das digitale Objekt eines Spiels oder eines Digitalisats zugänglich zu machen, und nicht\\ndafür, eine nach bestimmten Kriterien geordnete Spielliste aufzustellen. Alle Datenbanken und\\nSeiten,  welche  wir  für  unsere  Recherchen  beigezogen  hatten,  operieren  auf  der  Ebene  des\\nEinzelobjektes. Die Metadaten einzelner Spiel-Einträge waren oft genügend bis ausführlich, wir\\nkonnten jedoch mit keiner der Seiten eine für die Forschung brauchbare geordnete Spielliste\\nerstellen. Die Organisation von Wissen und das zugehörige Datenmodell erfolgt also primär über\\neinige wenige normierte Felder wie etwa die*den Ersteller*in eines Eintrags in Kombination mit\\neinem Schlagwort- und/oder Kategorie-Sammlungssystem, das allerdings abhängig von der*dem\\n4 Das ist für uns zwar nicht unmittelbar relevant, zeigt jedoch auf, wie eine Suchmaske etwas über \\ndas epistemologische Modell einer Plattform aussagen kann.\\nBearbeiter*in eines Eintrags variieren kann, wenn verschiedene User*innen zwar inhaltlich dasselbe\\nmeinen, es aber unterschiedlich verschlagworten. Bleibt schließlich eine Ausnahme zu erwähnen:\\nder  Swiss  Games  Garden (https://swissgames.garden/).  Die  Sammlung  entstand  aus  einer\\nKooperation des Schweizer Entwicklers David Stark und der zwei Forscher David Javet und Yannick\\nRochat, wobei Letzterer auch Kollege in unserem SNF-Sinergia-Projekt Confederatio Ludens ist. Die\\nDatenbank ist gerade erst im Aufbau begriffen und wir haben bereits dazu beigetragen, sie mit\\nunseren Funden weiter auszubauen. Wir möchten dabei betonen, dass es nie unser Ziel war, eine\\n„wissenschaftliche“  Konkurrenz  zu  bestehenden  Datenbanken  aufzubauen,  sondern  unsere\\nDatensammlung so offen zu gestalten, dass sie potenziell allen zur Verfügung steht. Im Swiss Games\\nGarden und in Wikidata (siehe unten) haben wir selbst unsere Daten wiederum eingepflegt.\\n3. Zum Aufbau einer Alpha-Version\\nAusgehend von unserer Forschungsfrage und der Quellenbasis gab es nun in unserer Wahrnehmung\\nzwei Möglichkeiten: Der wissenschaftlich saubere Weg wäre es gewesen, in Form eines finanzierten\\nProjekts eine neue Infrastruktur aufzubauen, mit mehreren bezahlten Mitarbeiter*innen im Vorfeld\\nden Aufbau der Datenbank zu strukturieren, ein kontrolliertes Vokabular zu erstellen und dann\\ngezielt mit der Suche und Befüllung der Datenbank anzufangen. Erfahrungsgemäß bräuchte ein\\nsolches Projekt eine Vorlaufzeit von mindestens einem Jahr für Recherchen im Vorfeld und das\\nSchreiben des Antrags. Dann bestünde je nach Förderinstrument eine 10- bis 20-prozentige Chance,\\ndass das Projekt auch finanziert würde, woraufhin wir drei bis vier Jahre Zeit hätten, es umzusetzen.\\nVorteil wäre, dass viele schwierige Fragen im Vorfeld geklärt werden könnten und das Team sehr\\ngezielt und aufeinander abgestimmt bei der Suche vorgehen könnte. Nachteil wäre die lange\\nVorbereitungszeit des Projekts und dass die Daten dann erst nach vier Jahren zur Verfügung\\nstünden, vor allem aber, dass die Aussichten auf Finanzierung insgesamt gering wären. Was also tun,\\nwenn  das  Projekt  nicht  finanziert  würde?  Wir  könnten  natürlich  das  Projekt  umschreiben,\\nnachbessern und auf eine Finanzierung im nächsten Jahr hoffen – im besten Fall.\\nAus diesen Gründen hat sich Eugen Pfister im Frühjahr 2022 für einen anderen Zugang entschieden.\\nZu diesem Zeitpunkt hatte er schon Listen an Schweizer und österreichischen Spielen angelegt, ihm\\nfehlten aber wichtige Informationen zu einem der größten Spielmärkte Europas, mit dem die\\nSchweiz und Österreich eng verwoben waren: Deutschland. Deshalb nutzte er die Gelegenheit eines\\nLehrauftrags an der Heinrich-Heine-Universität Düsseldorf, um sich gemeinsam mit seinen dortigen\\nGeschichtsstudent*innen  in  einem  Kurs  auf  die  „Spurensuche  [nach  den]  frühen  Spiele-\\nEntwickler*innen im deutschsprachigen Raum“ zu begeben. Gemeinsam mit den Student*innen\\nConstantin Bintz, Marlon Duncan Bonsch, Lars Brandes, Lisa Bresgott, Rika Bunse, Noah Dix, Victoria\\nHou, Daniel Kaspereit, Petros Kiorpes-Betchawas, Simon Körner, Rabea Kuschel, Christian Mischke,\\nSebastian Müller, Tanja Pabst, Ann-Kristin Potthast, Deniz Sargin, Clarissa Schiffer, Jan Stockschläger\\nund Ebru Yaylali sammelte er im Rahmen der Lehrveranstaltungen Titel für eine gemeinsame\\nDatenbank in Form einer Google-Sheets-Tabelle. Zuerst wurden in Kleingruppen jene Plattformen\\ndurchsucht, die sich (zumindest über Umwege) nach Herkunftsländern filtern ließen: So wurde zum\\nBeispiel  auf  den  Plattformen  Lemon  Amiga  und  Interactive  Fiction  Database  nach  Spielen  in\\ndeutscher Sprache gesucht.5 In einem nächsten Schritt wurde dann auch auf anderen Plattformen\\nnach Studios und einzelnen Entwickler*innen gesucht, die eindeutig dem DACH-Raum zugeordnet\\nwerden konnten. In der Folge wurden alle Listings des  Computronic-Magazins aufgenommen, die\\nüber  das  Internet  Archive  verfügbar  waren.  Die  Kleingruppen  kontrollierten  dabei  jeweils  die\\nEingaben anderer Kleingruppen. So entstand zuerst eine Alpha-Version und nach gemeinsamer\\nÜberarbeitung eine gesäuberte Beta-Version mit bereits ca. 600 Titeln, darunter aber nach wie vor\\neinige Dubletten und Falschzuordnungen. Basierend auf dieser Liste haben die Student*innen dann\\nhistorische Fallstudien einzelner Titel erstellt (z. B. Bonsch und Bunse 2023; Bresgott et al. 2022;\\nMüller und Sargin 2023). Nach Abschluss des Semesters hat Ann-Kristin Potthast unter Mitarbeit von\\nMarlon Duncan Bonsch, Lisa Bresgott, Rika Bunse, Clarissa Schiffer und Jan Stockschläger geholfen,\\ndie Datenbank gemeinsam mit Eugen Pfister und Lukas Daniel Klausner noch einmal gründlich zu\\nkontrollieren. Zu jedem Titel wurden zusätzliche Quellen gesucht und angegeben. Doppelnennungen\\n– zum Beispiel bedingt durch die Existenz mehrerer Namen für ein und dasselbe Spiel – wurden\\naufgelöst. Außerdem hat Eugen Pfister immer wieder freie Minuten genutzt, um nach weiteren\\nQuellen, Studios und Entwickler*innen zu suchen.\\n4. Von der Beta- zur Gamma-Version\\nMit Beginn des SNF-Sinergia-Projekts kamen dann die Projektmitglieder Adrian Demleitner, Addrich\\nMauch und Aurelia Brandenburg hinzu, welche die mittlerweile knapp 1200 Titel noch einmal zu\\nkontrollieren halfen. Aufbauend auf der Beta-Version haben wir anschließend die Gamma-Version so\\nentwickelt, wie sie inzwischen auch online frei abrufbar ist (Pfister et al. 2023). Dabei wurden wir mit\\ndenselben Problemen konfrontiert, wie sie häufig auftreten, wenn man sich darum bemüht, wenig\\naufgearbeitete und komplexe historische Daten unter einem bestimmten Aspekt zu strukturieren\\nund zu säubern. So waren zum Beispiel die Genres der Spiele eine relevante Information und damit\\nein wichtiges Feld, auch um einen einzelnen Datensatz später einordnen zu können – aber gerade\\nGenreeinteilungen sind meist fließend und schlecht zu standardisieren. Genauso waren wir mit einer\\ngroßen  Bandbreite an Qualität  der  recherchierbaren  Informationen  pro Titel konfrontiert: Für\\n5 https://ifdb.org/search?sortby=old&searchfor=language%3Agerman (Suche nach Sprache, „earliest \\npublications first“) und https://www.lemonamiga.com/games/advanced_search.php (Advanced \\nSearch, suchen nach „Language: German“).\\nmanche Spiele ließen sich verhältnismäßig leicht z. B. über MobyGames umfangreiche Listen der\\nbeteiligten Entwickler*innen recherchieren, die dann fast zu umfangreich für ein einzelnes Feld in\\nder Datenbank waren, in anderen Fällen war auch mit genauerer Prüfung fürs Erste nur ein Studio\\noder ein Publisher, aber keine individuellen Entwickler*innen zu eruieren.\\nGerade Entwickler*innen (aber auch Publisher) boten auch über die bloße Informationsdichte hinaus\\neinige Probleme in diesem Arbeitsschritt, weil beides immer wieder die Frage aufwarf, wann ein\\nSpiel nun denn tatsächlich ein Spiel aus dem DACH-Raum war und wann nicht. In manchen Fällen\\nlassen sich etwa einzelne Team-Mitglieder  unter den Entwickler*innen (z.  B. bei  Rayman  der\\nSchweizer Yann Le Tensorer) ausmachen, die aus dem DACH-Raum stammen und so ein gutes\\nBeispiel für eine internationale Vernetzung unter Entwickler*innen darstellen, während aber die\\nStudios dieser Spiele ihren Sitz außerhalb Deutschlands, Österreichs oder der Schweiz hatten. Solche\\nNuancen verdeutlichen natürlich ganz automatisch, dass gerade in Hinblick auf digitale Spiele\\nnationale Grenzen fließend sind und auch diese Art Sonderfälle sind (mit entsprechendem Vermerk)\\nin der Gamma-Version aufgeführt – nur führen sie konkret vor Augen, wie schwer sich historische\\nDaten Datenbankstrukturen unterwerfen lassen.\\nDazu kam auch, dass wir uns durch unseren möglichst inklusiven Zugang der Erfassung (möglichst)\\naller  Spiele  zwar  bewusst  gängigen  Kanonisierungsprozessen  dessen,  was  heute  meist  als\\n„Gamingkultur“ gilt, widersetzt haben, dieser Zuschnitt aber auch dazu führt, dass auch die fertige\\nGamma-Version  einige  Titel  enthält,  über  die  wir  über  ihre  bloße  Existenz  hinaus  wenig\\nInformationen erfassen konnten. In manchen Fällen lag das auch schon an der Publikationsform:\\nSpiele wie Time-Rally (1987) wurden z. B. als Listings in Computerspielmagazinen – in diesem Fall der\\nComputronic – veröffentlicht und entsprachen damit grundsätzlich unseren Kriterien, sind aber\\nnatürlich nicht ohne Weiteres im selben Maße verarbeitbar wie viele kommerziell vertriebene\\nSpiele.\\n5. Ausblick\\nWir sind uns dessen bewusst, dass unsere Datenbank – zum derzeitigen Zeitpunkt wie auch in\\nabsehbarer Zukunft – unvollkommen ist. So haben wir bisher nur die Listings aus einem Magazin\\naufgenommen. Wir müssen sogar davon ausgehen, dass selbst viele der kommerziell vertriebenen\\nSpiele nach wie vor fehlen. Des Weiteren ist die Zuordnung zu Genres nach wie vor problematisch.\\nDas liegt bis zu einem gewissen Grad in der Natur der Sache, weil es hierzu in der Forschung nach\\nwie vor keinen Konsens gibt. Zugleich ist eine Zuordnung zu Genres gerade für unsere Forschung\\nhoch  relevant  –  denken  wir  nur  an  die  eingangs  gestellte  Frage  zur  deutschsprachigen\\nSonderstellung  des  Genres  Wirtschaftssimulation.  Außerdem  verschwinden  in  der  derzeitigen\\nTabellenstruktur  die  beteiligten  Entwickler*innen,  seien  sie  nun  Programmierer*innen,\\nGrafiker*innen, Autor*innen usw., in einer Kolonne der Tabelle.\\nDiese oft frustrierende Ungenauigkeit haben wir aber bewusst in Kauf genommen, eben weil wir uns\\nentschieden haben, nicht auf eine Finanzierung zu warten, sondern über eine lange Zeit hinweg in\\nunserer Freizeit voranzutreiben. Es war uns wichtig, möglichst bald erste Daten zur Verfügung zu\\nstellen – nicht nur für unsere eigene Forschung, sondern für alle, die daran interessiert sind.\\nManchmal ist es auch in  der  Wissenschaft notwendig, „quick and dirty“ vorzugehen, um die\\nForschung weiter anzustoßen. Es erscheint doch erstaunlich, dass es bis 2022 keine einschlägige, gut\\nzugängliche Online-Datenbank für Game Studies unter dem Blickwinkel der local history gab. Und wir\\nselbst haben schon jetzt enorm von der Arbeit profitiert – so ist das Sample von Schweizer Spielen\\nfür das SNF-Sinergia-Projekt von knapp über 50 auf 122 angestiegen, übrigens auch dank der\\nRecherchen unserer Kolleg*innen aus dem Projekt. Auch zeigen sich schon in dem vorhandenen\\nSample an Spielen erste interessante Muster, die es weiter zu erforschen gälte. So fällt zum Beispiel\\nauf,  dass  gerade  in  den  1980er-  und  1990er-Jahren  nur  wenige  Spiele  in  den  größten\\ndeutschsprachigen Städten entwickelt wurden und kleine bzw. mittelgroße Städte hier überwiegen.\\nIm selben Zeitraum finden sich in vielen Entwickler*innenteams öfter der gleiche Nachname, was auf\\nGeschwister- oder Ehepaare schließen ließe. Außerdem kann man bereits sehr schön Konjunkturen\\neinzelner Spielmechaniken nachzeichnen.\\nDer nächste Schritt wäre es, die Daten weiter zu säubern, sich der ungeliebten Genre-Frage zu\\nstellen und allgemein saubere Metadatenstandards zu diskutieren. Dabei stehen wir aber erst am\\nAnfang unserer Arbeit. Es ist unser Ziel, an der Datenbank beständig weiterzuarbeiten. Zugleich\\nwünschen wir uns aber auch, dass möglichst viele von dieser Vorarbeit – denn mehr ist es bislang\\nnicht  –  profitieren,  weswegen  wir  schon  die  Gamma-Version  trotz  all  ihrer  Leerstellen  und\\npotenziellen Fehler im Rahmen einer CC-BY-SA-Lizenz online gestellt haben. Die Daten gehören nicht\\nuns, sondern allen, die etwas damit anfangen können und wollen.\\nParallel sind wir auch aktiv bemüht, die Ergebnisse in andere Datenbanken einzuarbeiten, zuallererst\\nden bereits genannten Swiss Games Garden, aber auch – und hier für ein potenziell weitaus\\ngrößeres  Publikum  –  in  das  Wikimedia-Projekt  Wikidata.  Während  der  Integration  unserer\\nbisherigen  Daten  in  Wikidata  begegnen  wir  erneut  ähnlichen  Herausforderungen  wie  beim\\nAufbereiten der Liste selbst. Das WikiProject  Video Games,6 eine Arbeitsgruppe, die sich mit der\\nEintragung von digitalen Spielen in Wikidata befasst, hat ein eigenes Datenmodell entwickelt und im\\nZuge dessen Vorschläge für Genres und andere Attribute erstellt. Diese decken sich nicht immer mit\\nunserer  Liste, weshalb zusätzlicher  Aufwand erforderlich ist  und Konsense oder Kompromisse\\ngefunden werden müssen. Wir nehmen während dieses Prozesses aktiv an der Arbeitsgruppe teil\\n6 https://www.wikidata.org/wiki/Wikidata:WikiProject_Video_games\\nund haben uns auch bei Jean-Frédéric Berthelot, welcher seit Langem an dieser Arbeitsgruppe\\nmitwirkt, um Unterstützung bemüht. Laut Berthelot hat Wikidata nicht den Anspruch, alle anderen\\nPlattformen und Datenbanken zu ersetzen, sondern soll wie das Pilzmyzel im Wald7 verschiedene\\nSpieledatenbankprojekte miteinander verbinden. Daher ist die Verknüpfung zu den Einträgen eines\\nSpiels auf anderen Seiten von hoher Bedeutung.\\nDas endgültige Ziel unseres Projekts ist es, verschiedenen Nutzer*innengruppen eine umfassende,\\nmöglichst vollständige und wissenschaftlich validierte Datenquelle zur Verfügung zu stellen. Nicht\\nzuletzt wäre dies natürlich auch für journalistische Recherche dienlich, wenn sich Journalist*innen\\nnicht auf unsichere Informationen von privaten Websites verlassen möchten.  Es ist eine große\\nFreude zu beobachten, wie viele Menschen bereits in den ersten Wochen nach Veröffentlichung auf\\nunsere Datenbank reagierten. Teilweise kamen einzelne Ergänzungen herein, teilweise ganze Listen\\nmit mehreren hundert Titeln, die es nun einzuarbeiten gilt. Vor allem aber freut uns, wie viele\\nKolleg*innen sich von unserer Arbeit inspiriert zeigen – und die Datenlage in Zukunft hoffentlich\\nnoch besser machen.\\nLiteratur\\nBlanchet, A. & Montagnon, G. (2020). Une histoire du jeu vidéo en France, vol. 1: 1960–1991: Des\\nlabos aux chambres d'ados. Houdan: Pix’n Love.\\nBonsch, M. D. & Bunse, R. (2023). „Eine Klinge die alle Orks fürchten!“: eine Fallstudie zu „Das\\nSchwarze\\n \\nAuge:\\n \\nDie\\n \\nSchicksalsklinge“\\n \\n(1992).\\n Spiel-Kultur-Wissenschaft.\\nhttp://spielkult.hypotheses.org/4040\\nBresgott, L., Dix, N. & Potthast, A.-K. (2022). Fallstudie: Indiana Jones’ Greatest Adventures. Spiel-\\nKultur-Wissenschaft. http://spielkult.hypotheses.org/3574\\nKent, S. L. (2001). The Ultimate History of Video Games: From Pong to Pokémon and Beyond – The\\nStory Behind the Craze That Touched Our Lives and Changed the World. Shreveport, LA: Prima\\nPublishing.\\nKirkpatrick, G.  (2015).  The Formation  of  Gaming  Culture: UK  Gaming  Magazines, 1981–1995.\\nLondon: Palgrave Macmillan.\\nMüller, S. C. H. & Sargin, D. (2023). Kampf der (Unterwasser-)Kulturen. Fallstudie Schleichfahrt. Spiel-\\nKultur-Wissenschaft. http://spielkult.hypotheses.org/3941\\nPfister, E., Brandenburg, A., Demleitner, A., Klausner, L. D., Mauch, A. & Potthast, A.-K. (2023).\\nHistorische\\n \\nDACH-Spieledatenbank\\n \\n(Gamma-Version).\\n Spiel-Kultur-Wissenschaft.\\nhttp://spielkult.hypotheses.org/3999\\n7 https://commonists.wordpress.com/2019/10/10/wikidata-the-underground-fungus-in-the-vast-forest-\\nthat-is-the-internet/\\nŠvelch,  J.  (2018).  Gaming  the  Iron  Curtain:  How  Teenagers  and  Amateurs  in  Communist\\nCzechoslovakia Claimed the Medium of Computer Games. Cambridge, MA: MIT Press.\\nSwalwell, M. (2021). Homebrew Gaming and the Beginnings of Vernacular Digitality. Cambridge, MA:\\nMIT Press.\\n\"},\n",
       " {'abstract': 'During deep sleep and under anesthesia spontaneous patterns of cortical activation frequently take the form of slow travelling waves. Here we show how data recorded from transgenic mice under anesthesia can be processed to analyze sources, sinks and patterns of flow. To make the best possible use of the data novel means of data processing are necessary. Therefore, we (1) give a an brief account on processes that play a role in generating slow waves and demonstrate (2) a novel approach to characterize its patterns in GCaMP recordings.',\n",
       "  'introduction': 'Slow waves represent an important yet highly variable neural phenomenon. Thus methods are required which allow for a systematic measurement of slow wave properties. Previous work that described different types of slow waves relied on manually crafted shape parameters such as the slope of the rising and falling edge of the slow waves in EEG or the spatial extent (local/widespread) of cortical coverage [4]. Some invasive neuroimaging studies have focused on patterns of the spread of activity. For example, the wavefront is measured using delay maps which indicate the temporal offset of an event for every pixel relative to its global onset [3].',\n",
       "  'literature review': 'Townsend and Gong [5] suggest an approach to characterize more diverse temporo-spatial properties of slow waves in GCaMP recordings by the kernel-based detection of specific patterns in the vector fields of Dense Optical Flow such as spirals or saddles.',\n",
       "  'methodology': 'We considered related methods for structural MRI [6], extend on the idea of an event related slow wave analysis [3] as well as the one of using Optical Flow [5] and suggest a technique that enables a novel perspective on slow waves as events with source- and sink-regions and specific patterns of directional flow: The combination of Helmholtz-Decomposition and Optical Flow for fluorescence microscopy. It allows to measure properties of events that can incorporate several oscillations even if they have a poor signal-to-noise ratio, occur simultaneously and overlap.',\n",
       "  'results': 'The approach can help in improving our understanding of neural processing in the brain during sleep and under anaesthesia. This paper is structured as follows. In section 2 we briefly explain the mechanisms that generate slow waves during sleep and anaesthesia. Section 3 introduces the approach of using Helmholtz-Decomposition and Optical flow with GCaMP data and the results achieved with it are presented in section 4. The last section summarizes and gives a critical discussion on the limitations of the results and the presented approach.',\n",
       "  'conclusion': 'We demonstrate a new method to characterize neocortical slow waves using fluorescence microscopy. It relies on the Helmholtz-Decomposition of Dense Optical Flow and captures the global dynamics of spread. It shows that different types of slow waves form clusters with respect to different features.',\n",
       "  'title': 'Helmholtz-Decomposition and Optical Flow: A new method to characterize GCamP recordings',\n",
       "  'author': 'Michael Gerstenberger, Dominic Juestel, Silviu Bodea',\n",
       "  'textdata': '1\\nHelmholtz-Decomposition and Optical Flow:\\nA new method to characterize GCamP recordings\\nMichael Gerstenberger, Dominik Juestel and Silviu Bodea\\nAbstract—During deep sleep and under anaesthesia sponta-\\nneous patterns of cortical activation frequently take the form of\\nslow travelling waves. Slow wave sleep is an important cognitive\\nstate especially because of its relevance for memory consolidation.\\nHowever, despite extensive research the exact mechanisms are\\nstill ill-understood. Novel methods such as high speed widefield\\nimaging of GCamP activity offer new potentials. Here we show\\nhow data recorded from transgenic mice under anesthesia can\\nbe processed to analyze sources, sinks and patterns of flow. To\\nmake the best possible use of the data novel means of data\\nprocessing are necessary. Therefore, we (1) give a an brief account\\non processes that play a role in generating slow waves and\\ndemonstrate (2) a novel approach to characterize its patterns\\nin GCamP recordings. While slow waves are highly variable, it\\nshows that some are surprisingly similar. To enable quantitative\\nmeans of analysis and examine the structure of such prototypical\\nevents we propose a novel approach for the characterization\\nof slow waves: The Helmholtz-Decomposition of gradient-based\\nDense Optical Flow of the pixeldense GCamP contrast (df/f). It\\nallows to detect the sources and sinks of activation and discern\\nthem from global patterns of neural flow. Aggregated features can\\nbe analyzed with variational autoencoders. The results unravel\\nregularities between slow waves and shows how they relate to\\nthe experimental conditions. The approach reveals a complex\\ntopology of different features in latent slow wave space and\\nidentifies prototypical examples for each stage.\\nIndex Terms—Helmholtz-Decomposition, Optical Flow; Slow\\nWaves; Optical Imaging; GCamp Imaging\\nI. INTRODUCTION\\nWhile stimulus dependent activity can be observed during\\nwakefulness, spontaneous patterns of activation dominate in\\nstages of deep sleep [1]. Similarly, fast neuronal firing is\\nreplaced by slow, traveling waves of activation during anaes-\\nthesia [2], [3]. Neocortical slow waves can be captured by\\nfluorescence microscopy of GCaMP activity in transgenic mice\\nwith a sampling rate of up to 100Hz.\\nSlow waves represent an important yet highly variable neu-\\nral phenomenon. Thus methods are required which allow for\\na systematic measurement of slow wave properties. Previous\\nwork that described different types of slow waves relied on\\nmanually crafted shape parameters such as the slope of the\\nrising and falling edge of the slow waves in EEG or the\\nspatial extent (local/widespread) of cortical coverage [4]. Some\\ninvasive neuroimaging studies have focused on patterns of the\\nspread of activity. For example, the wavefront is measured\\nusing delay maps which indicate the temporal offset of an\\nevent for every pixel relative to its global onset [3]. Townsend\\nD. Juestel leads the Optoacoustic Lab at the Helmholtz-Center Munich,\\nGermany. S. Bodea is Member of the Westmeyer Lab at TU Munich and M.\\nGerstenberger was Research Associate at Fraunhofer HHI Berlin, Germany.\\nE-Mail the corresponding author at michael.werner.gerstenberger@gmail.com.\\nManuscript submitted 12/2023.\\nand Gong [5] suggest an approach to characterize more\\ndiverse temporo-spatial properties of slow waves in GCamP\\nrecordings by the kernel-based detection of specific patterns\\nin the vector fields of Dense Optical Flow such as spirals or\\nsaddles.\\nWe considered related methods for structural MRI [6], extend\\non the idea of an event related slow wave analysis [3] as well\\nas the one of using Optical Flow [5] and suggest a technique\\nthat enables a novel perspective on slow waves as events\\nwith source- and sink-regions and specific patterns of di-\\nrectional flow: The combination of Helmholtz-Decomposition\\nand Optical Flow for fluorescence microscopy. It allows to\\nmeasure properties of events that can incorporate several\\noscillations even if they have a poor signal-to-noise ratio, occur\\nsimultaneously and overlap. The procedure aims to describe\\nslow waves by a dense yet interpretable set of features that\\nincludes the shape of the spatially averaged df/f, the location\\nof sources and sinks and the direction of flow. To visualize\\nthe polymorphism of slow waves and identify prototypical\\nslow waves that are most characteristic for each experimental\\ncondition embedding vectors are computed for each event\\nusing autoencoders. We test our novel method using a GCamP\\ndataset of mice including six levels of isoflurane. The approach\\ncan help in improving our understanding of neural processing\\nin the brain during sleep and under anaesthesia.\\nThis paper is structured as follows. In section 2 we briefly\\nexplain the mechanisms that generate slow waves during sleep\\nand anaesthesia. Section 3 introduces the approach of using\\nHelmholtz-Decomposition and Optical flow with GCamP data\\nand the results achieved with it are presented in section 4. The\\nlast section summarizes and gives a critical discussion on the\\nlimitations of the results and the presented approach.\\nII. SLOW-WAVES AND DELTA OSCILLATIONS\\nA large body of literature addresses slow oscillations in\\nthe brain. Slow waves in the delta range (0.5-4 Hz) can be\\nidentified in electroencephalograms (EEG) during deep sleep\\nand under anaesthesia. Neurons that switch between up and\\ndown states in the respective frequency exist in thalamus and\\narguably trigger slow waves. Hence, this kind of slow waves is\\nreferred to as thalamocortical slow wave [1], [7]. A different\\ntype of slow waves that occurs with a frequency below 1Hz\\nhas been studied as well[2]: Neocortical slow oscillations. As\\nthey do not necessarily occur in regular time intervals they are\\nalso referred to as neocortical slow waves here. Note also that\\nthe exact frequency depends on the measuring technique. The\\npeak frequency in ECoG tends to be higher as compared to\\nGCamp imaging due to implicit temporal smoothing as a result\\narXiv:2401.11008v1  [cs.CV]  19 Jan 2024\\n2\\n(a) Oscillatory centers in the brain\\n(b) GCamP and ECoG signatures of slow waves\\n(c) Segmentation of Slow Waves\\nFig. 1. Neural oscillators, delta oscillations and slow waves\\nof the lower sampling rate. This can be shown using publicly\\navailable data from [8] by computation of the frequency power\\nof ECoG and the aligned GCamP signal. Fig. 1b shows the\\nspatially averaged df/f and ECoG signals on the left and their\\nFourier spectrogram on the right. While the peak amplitute is\\n0.9 Hz in the earlier signal domain it is only 0.2 Hz in the\\nlatter.\\nBoth electrophysiological recordings as well as fluorescence\\nmicroscopy have been used to investigate neocortical slow\\nwaves [3], [9]. Neocortical neurons can oscillate in isolation\\neven in vitro [10]. However it was found that slow waves and\\nsubcortical signals often occur in an orchestrated way. This\\nholds not only for thalamus. Slow waves arguably also bind\\ntogether spindles and delta waves [1]. While named signals\\nare of different origin, one could hence consider them to be\\nthe signature of a single, yet distributed process.\\nThe question how bistable states arise from the interaction of\\ncells in neural networks has been addressed using spiking net-\\nwork models. They provide an explanation for the occurrence\\nof neocortical slow waves during deep sleep and under anaes-\\nthesia. The circumstance that several models explain slow\\noscillations highlights not only that more then one explanation\\nis possible, but also that different kinds of processes might\\nplay a role [11]. While certain characteristics of slow waves\\ndiffer, bistable states occur both during sleep and anaesthesia\\nin vivo[11] and even spontaneously in vitro [10].\\nIn contrast to anaesthesia, sleep is a vegetative state. While\\nslow wave sleep is promoted by several mechanisms including\\nthe circadian cycle and the release of melatonin that alters neu-\\nral excitability, sleep spindles of thalamic origin are assumed\\nto be the trigger for a transition to deep sleep [12]. During slow\\nwave sleep cortex resides in a bistable state with decreased\\nexcitability. This decrease in excitation is achieved by a\\nsuppression of excitatory input from the ascending reticular\\nactivation system (ARAS). As outlined above slow waves may\\noccur spontaneously but can also be triggered by other signals\\nin such states [1]. The interaction of various neural generators\\nof rhythmic activity is shown by Fig. 1. Slow wave sleep alter-\\nnates with REM stages during normal sleep and occurs even\\nin cerveau isol´e preparations when excitation from the ARAS\\nis fully absent. A putative function of neocortical slow waves\\nis memory consolidation. The assumed modes of action have\\nbeen described in the hippocampo-neocortical-dialog model.\\nEmpirical results support its core hypothesis that memory\\nreplay occurs during sleep and includes both hippocampus\\nand neocortex. Arguably sleep slow waves represent a neural\\nsignature of this process[13].\\nThe emergence of slow waves under anaesthesia has been\\nstudied on a cellular level. Isoflurane has inhibitory effects\\nas it increases K+ channel leakage currents and hence de-\\ncreases the number of positively charged ions in the cells,\\nmaking them more negative. Isoflurane anaesthesia manifests\\nin the form of higher outward currents throughout tonic and\\nburst ranges (when K+ channels are active) and a membrane\\nhyperpolarization at rest [14]1. This means the excitability\\ndecreases as a stronger depolarization is required to reach\\nthreshold and trigger an action potential. Besides effects on\\nK+ channels a potentiation of glycine receptors is assumed\\nalongside other neurochemical mechanisms that alter the ex-\\ncitability of neurons [15]. While isoflurane is known to reduce\\nthe CL- influx upon administration of GABA [16] and hence\\ndecreases its inhibitory effect, electrophysiological studies in\\nvivo indicate that the net-effect of isoflurane is inhibitory for\\nall relevant dosages. In this respect, isoflurane contrasts with\\nother anaesthetics including halothane and ketamine that show\\nconcentration dependence [10]. Isoflurane hyperpolarizes cells\\nwhich inhibits neural signal transduction by a reduced gap\\njunctional conductance.\\nWhile named effects explain the inhibition of neural firing\\nunder anaesthesia, decisive effects arise on the population\\nlevel. Eger (1981) systematically studied the EEG patterns\\nin the awake state and during anaesthesia with isoflurane for\\nfive different dosages at up to 2.9% in humans. For very\\nlight anaesthesia (iso = .56%) low voltage fast activity can\\nbe observed. At a light surgical level (iso = .96% and 1.78%)\\n1This was demonstrated in thalamocortical cells of the rat using the patch\\nclamp technique. Na+ channels are blocked for control using Tetrodotoxin.\\n3\\n(a) Sources, Sinks and Flow\\n(b) Helmholtz-Decomposition for simulated Optical Flow\\nFig. 2. Helmholtz-Decomposition of Optical Flow\\nslow oscillations are present that change from more regular to\\nirregular. Alternating patterns with high amplitude oscillations\\ncan be observed at a moderate surgical level (iso = 2.2%). For\\ndeep anaesthesia only occasional low voltage activity shows\\n(iso = 2.9%). Isoflurane administration leads to a gradual shift\\nfrom a stable awake state to bistable states and finally deep\\nanaesthesia where quiescence dominates.\\nIn summary it shows that neocortical slow waves emerge\\nspontaneously in cortex while they can be also be triggered by\\nsignals from subcortical structures. Methods for the character-\\nization of slow waves with widefield fluorescence must help in\\ndetecting the source regions of slow waves and describe how\\nthe patterns spread and where they target. The circumstance\\nthat a complex dynamic occurs on a population level that\\nmanifests in characteristic sequences of oscillations during the\\nup-state of a slow wave reflect how cortical pathways act as\\na generator of rhythmical activity.\\nIII. METHOD\\nWe present a new approach for the analysis of slow waves\\nthat includes three steps: (1) Detection of events, (2) feature\\nextraction by Optical Flow and Helmholtz-Decomposition as\\nwell as (3) the retrieval of embedding vectors by aggregation\\nand the use of Autoencoders for the visualization of the slow\\nwave distribution and the detection of prototypical events.\\nTo detect slow waves we compute the dF/F signal for each\\nrecording as dF/F = (Ft − F0)/F0. We refine the signal\\nby band-stop filtering to remove effects of the heartbeat (10-\\n20Hz) and take the arithmetic mean over the spatial domain.\\nAfter detrending, the onset/offset of the events can be deter-\\nmined by thresholding the resulting 1D time series(Fig. 1c).\\nWe continue with the computation of the pixeldense dFw/Fw\\nsignal per slow wave i.e. the percentage change of each frame\\nfwt as compared to baseline activity Fw00 of the event.\\ndFw/Fw = (Fwt − Fw0)/Fw0\\n(1)\\nVector fields of pixel displacements can be computed from\\nfluorescence recordings using gradient based Dense Optical\\nFlow[5]. To this end we rely on the method suggested by\\nHorn and Schunck[17]. It estimates the Optical Flow using\\nboth the image gradient and the temporal derivative between\\nthe pixel values of two subsequent frames. Optical Flow is\\ncomputed by iterative minimization of an objective function\\ne. The first part ef of this function measures the error in the\\noptical flow constraint of the vector fields.\\nef =\\nZZ\\n(Ixu + Iyv + It)2dxdy\\n(2)\\nHere Ix and Iy are the spatial derivatives of the df/f signal,\\nu and v denote the vector components of the flow field to be\\ncomputed and It is the temporal derivative. Recall that optical\\nflow can be measured uniquely in 1D but the solution in 2D is\\nubiquitous (aperture problem). Hence an additional assumption\\nis made and a global smoothness constraint introduced by the\\nHorn-Schunck method.\\nes =\\nZZ\\n(∥∇u∥2 + ∥∇v∥2)dxdy\\n(3)\\nThe energy functional is the sum of both terms where the\\nglobal smoothness is weighted by the parameter α2.\\nE = ef + α2es\\n(4)\\nAn iterative scheme can be derived to compute the vector\\nfields[17]. We suggest computing Optical Flow separately for\\neach hemisphere and then masking the background vectors.\\nThe resulting vector fields reflect the apparent motion and ap-\\nproximates the displacement of pixels with the same intensity\\n(GCaMP fluorescence) between frames of the df/f contrast.\\nHence, it implicitly captures the direction and speed at which\\nneocortical slow waves spread.\\nPatterns in these vector fields can be used to characterize\\nthe way neural signals travel in cortex during slow wave sleep\\nand anaesthesia. Townsend and Gong (2018) suggest a kernel-\\nbased approach to detect several local and global features.\\nA more general account that allows to study fundamental\\nproperties of vector fields is Helmholtz-Decomposition. The\\nfundamental theorem of vector calculus states that every vector\\nfield can be expressed as the sum of a curl free and a\\ndivergence free vector field if it is sufficiently smooth and\\ndecays rapidly.\\nv = ∇φcurl=0 + J∇φdiv=0\\n(5)\\nHelmholtz-Decomposition allows to retrieve the curl free\\nand the divergence free component for the given vector field.\\nThe divergence free vector field is also referred to as flow\\n(Fig. 2a). The vector fields are expressed via the gradient ∇φ.\\n4\\nJ relates to a matrix that rotates the gradient of the scalar\\npotential of the divergence free component by 90 degrees\\n(Eq. 5). The flow component of displacement vectors indicates\\nthe global dynamics of neural activity and shows the overall\\ndirection of the slow waves.\\nOsources =\\n(\\n∇φcurl=0,\\nif ∇φcurl=0 > 0\\n0,\\notherwise\\nIsinks =\\n(\\n∇φcurl=0,\\nif ∇φcurl=0 < 0\\n0,\\notherwise\\n(6)\\nThe curl free vector field can be separated into the distribution\\nof sources O and sinks I. Sources are especially important\\nbecause they indicate the origin of focal activity. Fig. 2b\\nillustrates the effects of the Helmholtz-Decomposition of\\nOptical Flow for a simulated signal with increasing global\\ndf/f originating from the top and a focal spot of raising\\nintensity(Fig. 2b; left). The constructed global downwards\\ntrend is captured by the flow component (Fig. 2b; right) while\\nthe location of the slow wave origin manifest as sources\\nvisible as blue areas in the scalar potential (Fig. 2b; left).\\nThe suggested approach allows to assess properties of\\nslow\\nwaves\\nquantitatively.\\nTo\\nthis\\nend\\nwe\\ncompute\\nseveral derived measures: The vertical fraction of flow\\nVertical flow = uflow/(uflow + vflow), the share of bottom-up\\nflow of vertical flow Bottom up = uflow>0/uflow, the fraction\\nof medial to lateral flow (vflow>0/vflow) for the left and\\nvflow<0/vflow for the right hemisphere), the total upwards-\\n(P uflow>0) and downwards flow (P uflow>0) as well as\\nthe leftwards- (P vflow<0) and rightwards flow (P vflow<0)\\nof the left hemisphere, the peak amplitude of the GCamP\\nsignal max df/f its duration as well as the temporal integral\\nover the sources\\nR\\nOdt/n and sinks\\nR\\nIdt/n that shows\\nthe average distributions per event. These features capture\\nimportant characteristics of slow waves that can be displayed\\nand interpreted on the basis of individual events.\\nWe test using variational autoencoders (VAEs) to retrieve a\\nlow dimensional latent embedding and investigate distributions\\nof different types of slow waves exploratively. The position\\non the embedding manifold reflects what the events look like\\nand thus shows characteristic features of the events. This\\nallows for an event related analysis of slow waves and to\\ninvestigate differences between conditions.\\nL = KLloss(zmean, zlogvar) +\\nX\\npiMSE(di, VAE(di)) (7)\\nThe first VAE embeds the GCamP signal\\nR\\ndf/fdt/n using\\nsix fully connected layers with ReLU activation. It reveals the\\ntopology of slow waves according to the shape of the spatially\\naveraged GCamP signal (see Fig. 8). A second model embeds\\na larger number of slow wave features. They include the spatial\\ndistribution of the sources\\nR\\nOdt/n and sinks\\nR\\nIdt/n, the\\nGCamP time series that was averaged spatially, the duration\\nand amplitude of the event as well as the direction of flow.\\nThe latter is aggregated in four values that represent the sum\\nof the upwards-, downwards-, leftwards and rightwards vector\\ncomponents of the flow field that was computed using the\\nHelmholtz Decomposition of the Optical Flow of the df/f\\nFig. 3. A mixed input Variational Autoencoder (VAE)\\nsignal (see Fig. 9). Our third model auto-encodes the GCamP\\nsignal\\nR\\ndf/fdt/n as well as the upwards and downwards\\nflow. A custom loss function was used to ensure that all of\\nthe encoded features are well reflected in the reconstructions.\\nThe respective function is given by Equation 7 where MSE\\nrelates to the mean squared error and d is a set of the inputs\\nto the respective model. Fig. 3 depicts the mixed input model.\\nThe parameter pi of the individual terms in the loss functions\\nwere adjusted manually while changes to the network structure\\nwere made until all the predictions correlated strongly with the\\ndata.\\nPrototypical examples are detected with a class specific GMM\\nthat approximates the conditional likelyhood of the embedding\\nvectors z of each condition c = 1 . . . 5 by an independent set\\nof Gaussian mixture components (k = 3).\\npc =\\nN\\nX\\nk=1\\nwkcN(µkc, Σkc), where\\nZ|K ∼ N(µKc, ΣKc)\\nEach mixture component corresponds to a prototype and its\\nmean µ represents the prototype vector. The prototypical\\nsamples are the ones with the embedding vector that has the\\nhighest likelyhood for the respective gaussian kernel.\\nIV. RESULTS\\nWe apply the proposed method on a dataset from an\\nexperiment with five conditions. Each recording has a length\\nof 30 seconds and was measured at a different level of\\nisofluarane ranging from 1.8% to 2.6%. Several different types\\nof neocortical slow waves can be discerned intuitively based\\non the shape of the temporal mean of the respective df/f signal.\\nEvents with peak-amplitude below 5% in the df/f signal and a\\nhigh correlation with the hemodynamic signal (r > .3) were\\nexcluded from the analysis to prohibit potential artifacts from\\naffecting the analysis that result from variations in the oxygen\\nlevels due to breathing.\\nFig. 4A shows the prototypical examples of the detected slow\\nwaves in the embeddings of VAE 1 for the different conditions\\n5\\nin blue and the reconstructions in orange. While longer periods\\nof continuous activity with multiple peaks can be observed at\\n1.8% isoflurane, increasing levels of anesthetics correlate with\\nevents that have few peaks or even only a single maximum.\\nAs a tendency the average amplitude decreases with higher\\nlevels of isoflurane. The same holds for the standard deviation\\nof the amplitudes (Fig. 4B).\\nAs described in the method section the results of the\\nHelmholtz-Decomposition of Dense Optical Flow can be well\\nvisualized. Fig. 5 illustrates the dynamics of flow during an\\nFig. 4. Prototypical Slow Waves\\nevent with a single peak and reveals its relationship to the\\ndf/f signal. Visual inspection of the df/f videos gives the\\nimpression of an event where activity spreads from frontal-\\nmedial areas of the left hemisphere towards lateral and parietal\\nportions of cortex. This pattern is well captured by the flow\\ncomponent. Fig. 5A shows the vector field for the measure-\\nment with the maximal global field strength (dt = 18ms).\\nArrows indicate the direction and strength of flow, the latter\\nof which is also indicated in the background. Fig. 5B shows the\\nrelationship with the df/f signal. The flow is strongest in the\\nbeginning of the event during the rising phase of df/f which\\ncontinues to increase thereafter. While activity spreads in early\\nstages of the slow wave a uniform change of fluorescence that\\ninvolves all cortex typically occurs later.\\nAreas with a focal increase in GCamP activity have high\\nvalues in the scalar potential of the sources (Fig. 2b). Sources\\nindicate the origin of neural activity as indicated by the Optical\\nFlow of the df/f signal. For many single peak events there\\nare distinct source areas, directional flow that originates from\\nthem and targets at sinks where activity prevails longest. For\\none type of slow waves medial portions of cortex typically act\\nas the sources while occipital areas are the sinks (Fig. 6B and\\nC). These events are characterized by pronounced media-to-\\nlateral flow (Fig. 6E) and a tendency towards downwards flow\\n(Fig. 6D). Visual inspection of the df/f videos confirms this\\ndynamic of cortical activity that originates in frontal areas,\\nFig. 5. Vector Field of Flow\\nFig. 6. Sources, sinks and flow\\nspreads quickly downwards towards medial areas and then\\nlaterally. It shows empirically that activity in areas that act\\nas a sink typically lasts longest.\\nFig. 7. Subsegments of a slow wave\\nWe encode different sets of features using variational au-\\ntoencoders to inspect the distribution of samples with different\\nisoflurane levels in latent space. Events with peak-amplitude\\n6\\nbelow 5% signal change or high correlation with breathing\\n(r>.3) were excluded from the analysis. The first model\\ntargets at a latent space defined by the slow wave shape.\\nFig. 8 illustrates the results. Panel A shows a reconstruction\\nmanifold for latent-layer activations chosen from an evenly\\nspaced grid. Larger waves are mapped to the center while\\nsmaller ones lie in the periphery: The phase and amplitude is\\nhigher for the respective samples (Panels F-G). The samples\\nin the center region also have stronger flow as measured by\\nthe area under the df/f curve. Samples from the different\\nexperimental conditions are unevenly distributed. Panels H-\\nL show the distribution of samples with different isoflurane\\nlevels. The density of the distribution for overlapping points\\nis color-coded. All samples that do not belong to the respective\\ncondition are plotted in grey. Most samples with an isoflurane\\nlevel of 1.8% lie within the center and represent events with\\nmultiple peaks and longer upstate periods. In contrast the\\nexamples at 2% lie further to the lower right and represent\\nslow waves with one peak and a steep rising and falling edge.\\nFor higher levels of isoflurane a cluster exists in the upper\\nright. This cluster arguably corresponds to waves with two\\npeaks the smaller one of which corresponds to an oscillation\\nthat is triggered by breathing. A relationship between breathing\\nand neural oscillations has been identified before [18]. These\\nsamples have strong vertical flow (as compared to horizontal\\nflow) and little bottom-up flow (as compared to top-down\\nflow; Panels B-D). Other samples of the very deep anaesthesia\\nconditions (iso>2%) are mapped to to the lower-right. This\\nindicates a general shift from longer lasting up states with\\nseveral oscillations of varying to less dynamical states where\\nsmaller waves typically have only one single peak.\\nFig. 8. Slow wave shapes in feature space (VAE 1)\\nFig. 9 shows the results of for the mixed input model.\\nPanel A shows the reconstructions of the df/f signal. Duration\\nand amplitude are square root scaled. Panel B indicates the\\ndirection of flow. Note that upwards flow dominates in the\\nFig. 9. Slow waves in a combined feature space (VAE 2)\\nlower center, flow occurs in all directions rather evenly in the\\nmiddle left portion while downwards flow dominates for events\\nin the upper right area. Panel C and D show the reconstructions\\nof the temporal average of the sources and sinks. Panels E to\\nI show the distribution of samples of different experimental\\nconditions in latent slow wave space.\\nThe model encoders the shape of the df/f signal alongside the\\nduration and amplitude, the ratio between flow in different\\ndirections as well as the distibution of sources and the dis-\\ntribution of sinks. Hence latent slow wave space allows for a\\nmore complete distinction of neocortical slow waves. Although\\nonly a two dimensional latent space was used an acceptable\\nreconstruction was achieved for all included features. Here\\nlarge amplitude waves with multiple peaks that are typical for\\nlow isoflurane levels (iso=1.8%) are mapped to the lower left\\ncorner. Within this area one may discern events with frontal\\nsources (and ubiquitous flow) and those with sources along\\nthe medial band (with flow rather bottom-up or medial-to\\nlateral). Large single peak waves that are characteristic for 2%\\nisoflurane are mapped to the central portions of the lower left\\nquadrant - an area with mostly medial to lateral or upwards\\nflow. Small waves that include a second peak lie in the upper\\nright area. Sources in frontal areas are present for small waves\\nthat exist mainly during deep anaesthesia (iso>2.2%). This\\ntype of small waves is characterized by downwards flow. The\\nmanifold of sources and sinks provides further information\\nregarding the nature of larger waves. Neocortical slow waves\\n7\\nthat are mapped to a region to the lower right have sources in\\nareas of the brain that potentially relate to the barrel fields in\\nfronto-peripheral parts of cortex. Especially for the latter kind\\nof events sinks reside in areas different from the sources and\\nupwards flow dominates.\\nThe third model auto-encodes the df/f signal alongside the\\nFig. 10. Latent distribution of Flow (VAE 3)\\nupwards and downwards flow and can be used to examine\\nregularities in the direction of flow during different stages of\\nslow waves. It shows that in most cases there is a point in\\ntime when most upwards or downwards flow occurs. This\\nholds typically for the first rising edge of the events. At\\n1.8% isoflurane many events begin with downwards flow\\n(orange peak; central reconstructions) while pronounced peak\\nin upwards flow (green peak; central portion of upper right)\\nshows in areas of latent space mainly occupied by examples\\nof 2% isoflurane. This tendency for bottom up flow also holds\\nfor events at higher levels of isoflurane (upper left).\\nOur results reveal a complex topology of events. It reveals the\\npotentials of the proposed method. The analysis with VAEs is\\nespecially well suited to understand the structure of the data\\nexploratively. We found several types of slow waves for the\\ngiven dataset and provide the prerequisites for a systematic,\\nhypothesis-driven analysis of slow waves with fluorescence\\nmicroscopy. The combination of Helmholtz-Decomposition\\nwith the Optical Flow of the pixel-dense GCamP contrast (df/f)\\nrepresents a viable tool for further research.\\nV. SUMMARY AND DISCUSSION\\nWe demonstrate a new method to characterize neocortical\\nslow waves using fluorescence microscopy. It relies on\\nthe Helmholtz-Decomposition of Dense Optical Flow and\\ncaptures the global dynamics of spread. Moreover it allows\\nto capture the sources and sinks of of the events. They can\\nbe interpreted as the sites of origin of neocortical slow waves\\nand the areas activity converges at and prevails longest. Thus\\nit captures how different brain regions interact during bistable\\nstates of anaesthesia and arguably also during deep sleep.\\nOur results appear promising and show the potentials of the\\napproach. Convolutional VAEs have proven useful to retrieve\\na low dimensional latent representation that can be used to\\nvisualize feature spaces that describe the variability of slow\\nwaves. Prototypical events can be identified by a categorical\\nGMM. We understand slow waves as periods of an extended\\nup-state that may incorporate multiple oscillations in different\\nregions of the brain. Therefore they reflect how activity\\ntravels along functional cortical pathways which might play\\nan important role as a generator of rhythmical activity in\\nneocortex. The Helmholtz-Decomposition of the Optical Flow\\nof the pixeldense GCamP contrast is well suited to detect\\nimportant features of neural activity and characterize slow\\nwaves.\\nThe proposed method can be used to characterize and\\ndistinguish slow waves that occur at different stages of\\nanaesthesia. We tested with recordings acquired from a single\\ntransgenic mouse. The analysed data with a high sampling\\nrate of (100Hz) was measured for time frame of 30s per\\ncondition. Although the number of events is thus limited the\\nresults are congruent with the changes in cortical dynamicd\\nreported previously. We found that different types of slow\\nwaves form clusters with respect to different features. At\\nlow isoflurane concentrations events typically have multiple\\npeaks and a complex dynamic that incorporates periods of\\ndownwards and subsequent upwards flow. With increasing\\nlevels of isoflurane the amplitude decreases and shorter events\\nwith a single maximum prevail. One type of events appears\\nto be triggered by the somatosensory state of the animal.\\nThis type of low amplitude events that are phase locked to\\nbreathing occur at deep anesthesia. Several regions act as\\nsources of neural activity. Frontal areas are most common for\\nlower levels of isoflurane while, medial, occipital or lateral\\nclusters become more frequent at higher concentrations that\\ncause deep anaesthesia.\\nFuture research should focus on means of a fine grained\\ncategorization of different types of slow waves that was not\\nattempted here. Considering the high temporal resolution\\nof modern GCamP recording setups and the interactions of\\ncortical and subcortical sites during bistable neurological\\nstates the approach offers potentials not only for studiyng\\nslow wave anaesthesia but potentially also the interaction\\nof different brain regions and rhythmical activity during sleep.\\n8\\nREFERENCES\\n[1]\\nR. E. Brown, R. Basheer, J. T. McKenna, R. E. Strecker,\\nand R. W. McCarley, “Control of sleep and wakeful-\\nness,” Physiological reviews, 2012.\\n[2]\\nM. Steriade, A. Nunez, and F. Amzica, “A novel slow\\n(1 hz) oscillation of neocortical neurons in vivo: Depo-\\nlarizing and hyperpolarizing components,” Journal of\\nneuroscience, vol. 13, no. 8, pp. 3252–3265, 1993.\\n[3]\\nM. Celotto, C. De Luca, P. M. F. Resta, et al., “Analysis\\nand model of cortical slow waves acquired with optical\\ntechniques,” Methods and Protocols, vol. 3, no. 1, p. 14,\\n2020.\\n[4]\\nG. Bernardi, F. Siclari, G. Handjaras, B. A. Riedner,\\nand G. Tononi, “Local and widespread slow waves\\nin stable nrem sleep: Evidence for distinct regulation\\nmechanisms,” Frontiers in human neuroscience, vol. 12,\\np. 248, 2018.\\n[5]\\nR. G. Townsend and P. Gong, “Detection and analysis\\nof spatiotemporal patterns in brain activity,” PLoS com-\\nputational biology, vol. 14, no. 12, e1006643, 2018.\\n[6]\\nJ. Lef`evre, F. Leroy, S. Khan, et al., “Identification\\nof growth seeds in the neonate brain through surfacic\\nhelmholtz decomposition,” in Information Processing\\nin Medical Imaging: 21st International Conference,\\nIPMI 2009, Williamsburg, VA, USA, July 5-10, 2009.\\nProceedings 21, Springer, 2009, pp. 252–263.\\n[7]\\nM. Steriade and M. Deschenes, “The thalamus as a\\nneuronal oscillator,” Brain Research Reviews, vol. 8,\\nno. 1, pp. 1–63, 1984.\\n[8]\\nA. Stroh, H. Adelsberger, A. Groh, et al., “Making\\nwaves: Initiation and propagation of corticothalamic\\nca2+ waves in vivo,” Neuron, vol. 77, no. 6, pp. 1136–\\n1150, 2013.\\n[9]\\nN. Niethard, H.-V. V. Ngo, I. Ehrlich, and J. Born, “Cor-\\ntical circuit activity underlying sleep slow oscillations\\nand spindles,” Proceedings of the National Academy of\\nSciences, vol. 115, no. 39, E9220–E9229, 2018.\\n[10]\\nH. Moghadam, T. Yar, M. M. Qazzaz, I. A. Ahmed,\\nand W. Winlow, “A comparative study of cell specific\\neffects of systemic and volatile anesthetics on identified\\nmotor neurons and interneurons of lymnaea stagnalis\\n(l.), both in the isolated brain and in single cell culture,”\\nFrontiers in Physiology, vol. 10, p. 583, 2019.\\n[11]\\nT.-A. E. Nghiem, N. Tort-Colet, T. G´orski, et al.,\\n“Two types of slow waves in anesthetized and sleeping\\nbrains,” bioRxiv, p. 430 405, 2018.\\n[12]\\nP. Montagna, “Fatal familial insomnia: A model dis-\\nease in sleep physiopathology,” Sleep medicine reviews,\\nvol. 9, no. 5, pp. 339–353, 2005.\\n[13]\\nG. Buzs´aki, “The hippocampo-neocortical dialogue,”\\nCerebral cortex, vol. 6, no. 2, pp. 81–92, 1996.\\n[14]\\nC. R. Ries and E. Puil, “Ionic mechanism of isoflu-\\nrane’s actions on thalamocortical neurons,” Journal of\\nneurophysiology, vol. 81, no. 4, pp. 1802–1809, 1999.\\n[15]\\nN. C. for Biotechnology Information. “Pubchem com-\\npound summary for cid 3763.” (2021), [Online]. Avail-\\nable: https://pubchem.ncbi.nlm.nih.gov/compound/\\nIsoflurane (visited on 01/01/2021).\\n[16]\\nA. Jenkins, N. P. Franks, and W. R. Lieb, “Effects of\\ntemperature and volatile anesthetics on gabaareceptors,”\\nAnesthesiology: The Journal of the American Society of\\nAnesthesiologists, vol. 90, no. 2, pp. 484–491, 1999.\\n[17]\\nB. K. Horn and B. G. Schunck, “Determining optical\\nflow,” Artificial intelligence, vol. 17, no. 1-3, pp. 185–\\n203, 1981.\\n[18]\\nA. B. Tort, S. Ponsel, J. Jessberger, Y. Yanovsky,\\nJ. Brankaˇck, and A. Draguhn, “Parallel detection of\\ntheta and respiration-coupled oscillations throughout the\\nmouse brain,” Scientific reports, vol. 8, no. 1, p. 6432,\\n2018.\\n'},\n",
       " {'abstract': 'Soft robots are increasingly utilized due to their softness and high degrees of freedom (DOFs). However, modular soft robots introduce difficulties in accurate control. Hence, we propose a novel and accurate biLSTM controller for modular soft robots that adapts to different module numbers. It controls the module configurations and works for robots with different module numbers. Our contribution is threefold: a data collection strategy, a biLSTM configuration controller, and validation on simulated and real pneumatic robots. Experiments show that our controller achieves low error configuration control tasks on robots with two, three, four, and six modules.',\n",
       "  'introduction': 'Soft robots have been widely applied in numerous areas due to their softness and high degrees of freedom (DOFs), and various soft robots have been designed for different applications. For example, concentric tube robots are leveraged in robot-assisted surgery, especially minimally invasive surgery (MIS), and cable-driven robots have been validated in cardiothoracic endoscopic surgery. Researchers include pneumatic robots in recovery devices and assistive robots due to their safety. Also, a soft six-legged untethered robot can walk with the actuation of fluid-driven actuators. Soft robots can also be employed as biorobots to mimic the behaviors of animals like fish, octopus, and elephant. Overall, the robot community has taken advantage of many categories of soft robots in various research topics.\\n\\nModular soft robots have shown unique capabilities compared with other soft robots. Multiple modules endow the robot system with reconfiguration and multiple choices of module numbers. In this case, they are flexible and can meet the requirements of different tasks. Compared to single-module robots, modular robots have more degrees of actuation (DOAs) and, therefore, more active DOFs. Modular robots can provide larger working spaces from the views of both kinematics and dynamics. Thanks to these properties, modular robots can achieve complex manipulations.\\n\\nMoreover, shape control can be implemented on modular soft robots. Most control targets of single modular robots are only end positions. Once the end positions are controlled, all the robot states, including the end orientations and robot shapes, are decided and depend on the end positions. But in many cases, the end positions and orientations of modular soft robots are independent. The robot can keep the end positions/orientations invariant and change the end orientations/positions. By controlling the robot shapes, modular robots can avoid obstacles and cross holes.',\n",
       "  'literature_review': 'Studies have been carried out on a modular pneumatic robot. Due to the high model complexity, data-driven approaches are leveraged to achieve path tracking, end pose control, and sophisticated manipulation tasks. A modular robot named Robostrich arm is applied to mimic the behavior of an ostrich and achieves the reaching task in a narrow space. However, the applied reinforcement learning methods are time-consuming.\\n\\nSome physical models for modular soft robots are proposed. Piecewise constant curvature (PCC) and the Cosserat approach are two of the most widely applied physical models in soft robots. Neural networks are employed for measurement, and PCC is employed for modular soft robot modeling. Kinematics models of modular soft robots are investigated, also based on PCC. The discrete Cosserat approach is utilized to simulate multisection soft manipulator dynamics.',\n",
       "  'methodology': 'We propose a novel and accurate biLSTM controller for modular soft robots. The biLSTM units share the same module number with the robot, even if the training and controlled robots have different module numbers. The label of module \\u202fi\\u202f can be denoted as\\n\\nnᵥᵡ =\\n2(i − 1)nˇˉ⁄1 − 1,\\n(1)\\nwhere \\nnˇˉ=\\n number of modules in this robot.\\n\\nThe range of labels is [-1,1], and a large label represents that this module is away from the robot base.\\n\\nEach LSTM unit can be seen as\\nf = sigm(Σᵣ ⋅ [нₗ₁, x] + Βᵣ),\\ni = sigm(Σᵩ ⋅ [нₗ₁, x] + Βᵩ),\\nC = f × C₁⁄₁ + i × tanh(Σᵣ ⋅ [нₗ₁, x] + Βᵣ),\\n(2)\\no = sigm(Σᵯ ⋅ [нₗ₁, x] + Βᵯ),\\nh = o × tanh(C),where f, i, C, o, and h denote the forget gate, input gate, cell state, output gate, and hidden state of this LSTM unit, respectively. нₗ₁ₐ₁ and C₁⁄₁ are the hidden state and cell state provided by the other unit. x denotes the LSTM input, and W* and b* denote the weight and bias parameters of the corresponding states. × is the Hadamard product operator, and sigm is the sigmoid function.\\n\\nTo train a neural network as a robot controller, it is necessary to collect a dataset in simulation or the real world. Generally, the robot will be actuated randomly to generate this dataset, and such a dataset can cover the whole working space.',\n",
       "  'results': 'We collect 16000 samples with the traditional random method and our method mentioned in Section 2.2 on a four-module simulation robot. It is evident that our method covers a larger space than the traditional method. The simulation experiment code can be found at https://github.com/zixichen007115/23ZCd.\\n\\nBased on our dataset, we train three kinds of neural networks to estimate the actions and compare their accuracy. As shown in Figure 1-(C), the neural network input contains previous module states and actions, the module number label, and current states for training. The network output is the estimated actuation.\\n\\nWe then utilize biLSTM for six-module robot configuration control. Due to the modularity of this network, biLSTM trained with a four-module robot can also be leveraged on a six-module robot, but LSTM fails to transfer to a six-module robot because the LSTM unit input size is related to module number.\\n\\nIn real experiments, we first collect 15000 samples on a three-module robot following the data collection method proposed in Section 2.2 and train a biLSTM for control. The dataset is shown in Figure S5. The actuation variable estimation errors and variances of the first, second, and third modules are 3.12 ± 4.85%, 3.23 ± 5.23%, and 3.04 ± 4.38%.\\n\\nTwo tasks, ‘edge’ and ‘down,’ are designed. In ‘edge,’ the robot is controlled to reach the edge of the working space, and the actuation variables nearly reach the maximal actuation.',\n",
       "  'conclusion': 'This paper introduces a novel and accurate biLSTM configuration controller for modular soft robots with module number adaptability. Such a controller can control module configurations in robots with different module numbers. Simulation cable-driven robots and real pneumatic robots have been included in experiments to validate the proposed approaches, and we have proven that our controller can be leveraged even with the increase or decrease of module number. This is the first paper that gets inspiration from the physical structure of modular robots and utilizes bidirectional LSTM for module number adaptability. Future work may include a planning method that bridges the task and configuration spaces and the integration of an online controller.',\n",
       "  'title': 'A Novel and Accurate BiLSTM Configuration Controller for Modular Soft Robots with Module Number Adaptability',\n",
       "  'author': 'Zixi Chen, Matteo Bernabei, Vanessa Mainardi, Xuyang Ren, Gastone Ciuti, Cesare Stefanini',\n",
       "  'textdata': 'A Novel and Accurate BiLSTM Configuration Controller for \\nModular Soft Robots with Module Number Adaptability \\nZixi Chen1*, Matteo Bernabei1, Vanessa Mainardi1, Xuyang Ren2, Gastone Ciuti1, and \\nCesare Stefanini1 \\n \\nAbstract \\n \\nModular soft robots have shown higher potential in sophisticated tasks than single-\\nmodule robots. However, the modular structure incurs the complexity of accurate \\ncontrol and necessitates a control strategy specifically for modular robots. In this paper, \\nwe introduce a data collection strategy and a novel and accurate bidirectional LSTM \\nconfiguration controller for modular soft robots with module number adaptability. Such \\na controller can control module configurations in robots with different module numbers. \\nSimulation cable-driven robots and real pneumatic robots have been included in \\nexperiments to validate the proposed approaches, and we have proven that our \\ncontroller can be leveraged even with the increase or decrease of module number. This \\nis the first paper that gets inspiration from the physical structure of modular robots and \\nutilizes bidirectional LSTM for module number adaptability. Future work may include \\na planning method that bridges the task and configuration spaces and the integration of \\nan online controller. \\n \\n Keywords \\n \\nData-driven control, bidirectional LSTM, configuration control, modular soft robot \\n \\n \\n \\n1 Biorobotics Institute and Department of Excellence in Robotics and AI, Scuola Superiore Sant’Anna, Pisa, Italy. \\n2 Multi-scale Medical Robotics Centre and Chow Yuk Ho Technology Centre for Innovative Medicine, The \\nChinese University of Hong Kong, Hong Kong, China \\n* Corresponding author. Email: Zixi.Chen@santannapisa.it \\n1. Introduction \\n \\nSoft robots have been widely applied in numerous areas due to their softness and high \\ndegrees of freedom (DOFs,) and a variety of soft robots have been designed for different \\napplications. For example, concentric tube robots1 are leveraged in robot-assisted \\nsurgery, especially minimally invasive surgery (MIS.) Also, cable-driven robots have \\nbeen validated in cardiothoracic endoscopic surgery2. Due to their safety, researchers \\ninclude pneumatic robots in recovery devices3 and assistive robots4. A soft six-legged \\nuntethered robot can walk with the actuation of fluid-driven actuators5. Soft robots can \\nalso be employed as biorobots to mimic the behaviors of animals like fish6, octopus7–9, \\nand elephant10. Overall, the robot community has taken advantage of many categories \\nof soft robots in various research topics11–14.  \\n \\nModular soft robots have shown unique capabilities compared with other soft robots. \\nMultiple modules endow the robot system with reconfiguration15 and multiple choices \\nof module numbers. In this case, they are flexible and can meet the requirements of \\ndifferent tasks. Compared to single-module robots, modular robots have more degrees \\nof actuation (DOAs) and, therefore, more active DOFs. Modular robots can provide \\nlarger working spaces from the views of both kinematics and dynamics. Thanks to these \\nproperties, modular robots can achieve complex manipulations16,17.  \\n \\nMoreover, shape control can be implemented on modular soft robots. Most control \\ntargets of single modular robots are only end positions18–20. Once the end positions are \\ncontrolled, all the robot states, including the end orientations and robot shapes, are \\ndecided and depend on the end positions. But in many cases, the end positions and \\norientations of modular soft robots are independent. The robot can keep the end \\npositions/orientations invariant and change the end orientations/positions21. By \\ncontrolling the robot shapes, modular robots can avoid obstacles22 and cross holes23. \\n \\nHowever, along with these advantages, modular soft robots encounter difficulties in \\naccurate control. In addition to the nonlinearity and hysteresis of soft robots, such \\nmodular systems have more input variables than single-module robots, complicating \\nthe dynamics model. As shown in Figure 1-(A), the motion of each module will directly \\naffect the configurations of two adjacent modules and indirectly affect the whole robot \\nsystem. Moreover, the configurations of these modules decide the robot states with \\ndifferent levels. Even a tiny vibration on the modules near the base will crucially change \\nthe robot shape due to the accumulated motion along the modular robot, but the modules \\nnear the end show lower importance. In summary, although this kind of robot has high \\npotential, the complex model necessitates a control strategy specifically for modular \\nsoft robots. \\n \\nSome researchers have proposed approaches to utilize modular soft robots in \\nmanipulation. A series of research has been carried out on a modular pneumatic \\nrobot16,21,24–27. Due to the high model complexity, data-driven approaches are leveraged \\nto achieve path tracking24, end pose control21, and sophisticated manipulation tasks like \\nopening a drawer, turning a handwheel, and drawing a line on a paper27. Of note, the \\nmodular robot in 21 keeps the end position/orientation invariant while changing the end \\norientation/position, which can be achieved exclusively by modular robots. A modular \\nrobot named Robostrich arm28 is applied to mimic the behavior of an ostrich and \\nachieve the reaching task in a narrow space. However, the applied reinforcement \\nlearning methods are time-consuming.  \\n \\nAlso, some physical models for modular soft robots are proposed. Piecewise constant \\ncurvature (PCC)  and the Cosserat approach are two of the most widely applied physical \\nmodels in soft robots29. Neural networks are employed in 30 for measurement, and PCC \\nis employed for modular soft robot modeling. Kinematics models of modular soft robots \\nare investigated in 31,32, also based on PCC. The discrete Cosserat approach is utilized \\nto simulate multisection soft manipulator dynamics33. However, with the improvement \\nof module number, the complexity of the physical model increases dramatically, and \\nthe most sophisticated robot in these physical model works has only three modules31, \\nwhose model is only validated in simulation. \\n \\nThis paper aims to introduce a bidirectional long short-term memory (biLSTM) \\nconfiguration controller for modular soft robots with module number adaptability. \\nInstead of proposing one controller for each module number, our biLSTM network can \\nwork as a module from the view of the controller and is available to robots with different \\nmodule numbers. With such a controller, modular soft robots can fulfill some tasks that \\nsingle-module robots cannot achieve with relatively low errors. \\n \\nThe contributions of this paper are summarized as follows: \\n⚫ \\nWe propose a novel and accurate biLSTM controller for modular soft robots that \\naims to control the module configurations and can be applied to robots with \\ndifferent module numbers. \\n⚫ \\nWe introduce a data collection method specifically for modular robots to reach \\nstates away from the resting state. \\n⚫ \\nExperiments are carried out to validate the proposed controller on simulated cable-\\ndriven robots and real pneumatic robots. Configuration control tasks on robots with \\ntwo, three, four, and six modules are achieved with the help of this controller. \\n \\n2. Methods \\n2.1. BiLSTM Controller \\n \\nTo propose a controller for modular robots, we first analyze their structures. As shown \\nin Figure 1-(B), these modules share a similar shape, stiffness, and other physical \\nproperties. Their states can be represented by the same kind of configurations. They are \\nconnected in sequence, and each module will interact with two adjacent modules. In \\nthis module sequence, modules have different levels of impact on the robot motion. \\n \\nBased on the above analysis, we leverage biLSTM as the module configuration \\ncontroller. Due to the hysteresis in soft robot space sequences, recurrent neural \\nnetworks (RNNs) have been applied for control because such networks can address \\nissues related to sequences. Considering the bidirectional influence between modules, \\nwe apply a bidirectional RNN, biLSTM, shown in Figure 1-(C) for configuration \\ncontrol. To train the biLSTM units and control modular robots, we can connect biLSTM \\nunits that share the same module number with robots, even if the training and controlled \\nrobots have different module numbers. As far as we know, this is the first paper that \\nconsiders the space sequence of soft robots and utilizes RNN to solve this problem for \\nmodule number adaptability.  \\n \\nThe diagram of one biLSTM unit is shown in Figure 1-(D). In module Ⅱ, the forward \\nLSTM network takes the cell state 𝐶𝑓𝐼 and the hidden state ℎ𝑓𝐼 from module I as input, \\nand provides the cell state 𝐶𝑓𝐼𝐼  and the hidden state ℎ𝑓𝐼𝐼  of this unit to module III. \\nMeanwhile, the backward LSTM network takes the cell state 𝐶𝑏𝐼𝐼𝐼 and the hidden state \\nℎ𝑏𝐼𝐼𝐼 from module III as input, and provides the cell state 𝐶𝑏𝐼𝐼 and the hidden state ℎ𝑏𝐼𝐼 \\nof this unit to module I. In addition to these states, the module label 𝑛𝐼𝐼, the desired \\nmodule state 𝑆𝑑, and previous actions and module states 𝑆𝐼𝐼0, 𝐴𝐼𝐼0, 𝑆𝐼𝐼1, ⋯  are fed into \\nthese two networks. Their outputs are concentrated and fed into a fully connected layer \\nfor actuation estimation. \\n \\nThe module labels are used to infer the module position in the sequence, and the label \\nof module 𝑖 can be denoted as \\n𝑛𝑖 = 2(𝑖 − 1)\\n𝑛𝑠𝑢𝑚 − 1 − 1,\\n(1) \\nwhere 𝑛𝑠𝑢𝑚 is the amount of modules in this robot. The range of the labels is [-1,1], \\nand a large label represents that this module is away from the robot base. \\n \\nEach LSTM unit can be seen as \\n𝑓 = 𝑠𝑖𝑔(𝑊𝑓 ∙ [ℎ−1, 𝑥] + 𝑏𝑓),\\n𝑖 = 𝑠𝑖𝑔(𝑊𝑖 ∙ [ℎ−1, 𝑥] + 𝑏𝑖),\\n𝐶 = 𝑓 × 𝐶−1 + 𝑖 × 𝑡𝑎𝑛ℎ(𝑊𝑐 ∙ [ℎ−1, 𝑥] + 𝑏𝑐),\\n(2)\\n \\n𝑜 = 𝑠𝑖𝑔(𝑊𝑜 ∙ [ℎ−1, 𝑥] + 𝑏𝑜), \\nℎ = 𝑜 × tanh(𝐶), \\nwhere 𝑓, 𝑖, 𝐶, 𝑜,  and ℎ  denote the forget gate, input gate, cell state, output gate, and \\nhidden state of this LSTM unit, respectively. ℎ−1 and 𝐶−1 are the hidden state and cell \\nstate provided by the other unit. 𝑥 denotes the LSTM input, and 𝑊∗ and 𝑏∗ denote the \\nweight and bias parameters of the corresponding states. ×  is the Hadamard product \\noperator, and 𝑠𝑖𝑔 is the sigmoid function. \\n \\n2.2. Data Collection \\n \\nTo train a neural network as a robot controller, it is necessary to collect a dataset in \\nsimulation or the real world. Generally, the robot will be actuated randomly to generate \\nthis dataset, and such a dataset can cover the whole working space in most cases, as \\nshown in Figure 2-(A). However, due to the multiple modules, the modular robot tends \\nto vibrate near the resting state instead of exploring and reaching the edge of the \\nworking space. Therefore, we propose a data collection approach specifically for \\nmodular robots, as shown in Figure 2-(B), which takes a soft robot with three modules \\nas an example.  \\n \\nIn the first one-third of the data collection process, all the modules are actuated with \\nthe same random action sequence 𝑎𝑎1, and the robot can move far away from the resting \\nstate. In the following one-third process, the end module is actuated with a random \\naction sequence 𝑎𝑏3, and the other two modules are actuated with a different random \\naction sequence 𝑎𝑏1. In the final one-third process, three modules are actuated with \\ndifferent random action sequences 𝑎𝑐1, 𝑎𝑐2 and 𝑎𝑐3, respectively. This data collection \\napproach allows the modular robot to reach a larger space than the traditional method. \\n \\n3. Results \\n3.1. Experimental Setup \\n \\nTo validate our biLSTM controller and data collection strategy, we include simulation \\ncable-driven robots and real pneumatic robots in our experiments. \\n \\nThe simulation robot is based on PyElastica34,35, as shown in Figure 3-(A). The real \\nrobot system in Figure 3-(B) is composed of pneumatic modules covered by origami \\nstructures. Each module has two opposite chambers connected to valves (Camozzi \\nK8P-0-E522-0) under the control of the Arduino MEGA control board. Each module is \\nconnected to the other modules with 3D-printed connectors. Optical track makers are \\nfixed on the connectors, and NDI Polaris tracks the module motion. The real \\nexperimental setup runs at 2.5 Hz. Detailed information about the setup, soft module \\nstructure, and connectors is included in Figure S1. \\n \\nTo represent the module configuration, we utilize the unit vector of the module end \\nrelative to the module base, as shown in Figure 3-(C), (D). In this case, the simulation \\nand real module configurations are denoted by [𝑣𝑥, 𝑣𝑦, 𝑣𝑧]∈ 𝑅3 and [𝑣𝑥, 𝑣𝑦]∈ 𝑅2, and \\nthe range of each value is [-1,1]. Each module has four cables in simulation, and we \\nutilize two actuation values, 𝑎0, 𝑎1 ∈ [−1,1], to control them. \\n𝑎𝐼 = max{0, 𝑎0}, \\n𝑎𝐼𝐼 = max{0, −𝑎0} ,\\n(3) \\n𝑎𝐼𝐼𝐼 = max{0, 𝑎1}, \\n𝑎𝐼𝑉 = max{0, −𝑎1}, \\nwhere 𝑎𝐼, 𝑎𝐼𝐼, 𝑎𝐼𝐼𝐼, 𝑎𝐼𝑉 ∈ [0,1] represent the act of cable I, II, III, and IV shown in Figure \\n3-(E), respectively. The real robot module has only two chambers and shares the same \\nactuation principle. The acts 𝑎𝐼, 𝑎𝐼𝐼 ∈ [0,1] of chambers in Figure 3-(F) are controlled \\nby 𝑎0 and linear to the chamber pressure ([0.08bar, 0.35bar]). \\n \\n3.2. Simulation Experimental Results \\n \\nWe collect 16000 samples with the traditional random method and our method \\nmentioned in Section 2.2 on a four-module simulation robot, as shown in Figure 4. It is \\nevident that our method covers a larger space than the traditional method. The \\nsimulation experiment code can be found at https://github.com/zixichen007115/23ZCd. \\n \\nBased on our dataset, we train three kinds of neural networks to estimate the actions \\nand compare their accuracy. As shown in Figure 1-(C), the neural network input \\ncontains previous module states and actions, the module number label, and current \\nstates for training. The network output is the estimated actuation. \\n \\nWe compare our method with two methods applied in previous works36, as shown in \\nFigure S2. First, four LSTM networks are trained separately for four modules, and the \\nratios of the estimation errors to the actuation variable range are shown in  \\nTable 1. The estimation error is relatively high, which illustrates that it is challenging \\nto estimate the actuation variables of one module without the knowledge of the other \\nmodules.  \\n \\nThen, we feed the input from all modules into one LSTM. Each LSTM unit takes input \\nfrom all modules in one time step, and the units are connected according to the time \\nsequence. The estimation errors are far lower than those of the ‘four LSTM’ method. \\nFinally, we test our biLSTM and it achieves similar errors. Both LSTM and biLSTM \\ntake the information of all modules; hence, they achieve low errors.  \\n \\nIn all the above approaches, the modules near the base have higher errors than the other \\nmodules, which means the base module is heavily influenced by the other modules. \\nLSTM utilizes the recurrent structure to address the time sequence, while biLSTM \\naddresses the space sequence. These two strategies can be combined to take both \\nsequences into consideration, and this strategy may be included in our future work. Due \\nto the low actuation estimation errors, we utilize LSTM and biLSTM to control the \\nsimulation modular robots with the same configuration trajectories. \\n \\nIn the simulation control task, we aim to control the configurations of four modules and \\nindirectly achieve some end pose control targets. In task A, the modules are controlled \\nto bend and rotate. In task B, the robot is desired to keep the end position invariant and \\nchange the orientation. In task C, the robot is desired to keep the end orientation \\ninvariant and change the position. The desired configuration trajectories are designed \\nspecifically after trial and error and can be found in the Supplementary Data. Accurate \\nconfiguration design based on desired robot states like end pose or shape requires a \\nplanning strategy, which will be included in our future work. \\n \\nThe displacement between the real and desired module end unit vectors represents the \\nconfiguration errors, and the ratios of module configuration errors to the unit vector \\nlength are shown in Table 2. The four-module robot motions in task A, B, and C with \\nbiLSTM are shown in Figure 5, and the desired and real module states are illustrated in \\nFigure 6. The experimental videos are included in the Supplementary Movies. Both \\nLSTM and biLSTM achieve low errors, and the maximal error of LSTM (7.62%) is \\nhigher than that of biLSTM(3.95%).  \\n \\nThen, we utilize biLSTM for six-module robot configuration control. Due to the \\nmodularity of this network, biLSTM trained with a four-module robot can also be \\nleveraged on a six-module robot, but LSTM fails to transfer to a six-module robot \\nbecause the LSTM unit input size is related to module number. \\n \\nThe configuration errors for the six-module simulation robot are shown in Table 3. The \\nsix-module robot motions with biLSTM are shown in Figure 7, and the desired and real \\nmodule states are illustrated in Figure 8. The maximal error is 5.14%, demonstrating \\nthat biLSTM trained with a four-module robot can fulfill control tasks in a six-module \\nrobot. Considering the low errors of biLSTM on four-module and six-module \\nsimulation robots and its module number adaptability, we utilize biLSTM in real \\nexperiments. \\n \\n3.3. Real Experimental Results \\n \\nIn real experiments, we first collect 15000 samples on a three-module robot following \\nthe data collection method proposed in Section 2.2 and train a biLSTM for control. The \\ndataset is shown in Figure S5. The differences between two contiguous actuation \\nvariables in data collection are constrained to promise a smooth motion. The actuation \\nvariable estimation errors and variances of the first, second, and third modules are 3.12 \\n± 4.85%, 3.23 ± 5.23%, and 3.04 ± 4.38%. \\n \\nTwo tasks, ‘edge’ and ‘down,’ are designed. In ‘edge,’ the robot is controlled to reach \\nthe edge of the working space, and the actuation variables nearly reach the maximal \\nactuation. In ‘down,’ the robot end is controlled downward during the motion, implying \\nthat the sum of the module bending angles is 0.  \\n \\nEach task has been carried out three times, and the configuration control errors for the \\nthree-module real robot are shown in Table 4. The module configuration is represented \\nby the bending angle. The desired and real module states of the real three-module robot \\nin task edge and down with biLSTM are shown in Figure 9. The experimental videos \\nare included in the Supplementary Movies. The experimental results demonstrate that \\nthe biLSTM controller can control the real modular robot to follow trajectories with \\nlow errors (< 3°). \\n \\nThen, considering that the module number in the simulation increases from four to six, \\nwe decrease the module number in the real experiments from three to two to prove the \\nbroad module number adaptability of the biLSTM configuration controller. The \\nconfiguration control errors for the two-module real robot are shown in Table 4. The \\ndesired and real module states of the real two-module robot in task (A) edge and (B) \\ndown with biLSTM are shown in Figure 10. \\n \\nThe error of the second module in task ‘edge’ is 4.26°. Although it is not large \\nconsidering the reachable space of this module (about 150°), it is the largest error in the \\nreal experiments. This may be caused by manufacturing errors among modules and can \\nbe solved by integrating online controllers19. Of note, in task ‘down,’ the bending angle \\nof the first module in the two-module robot reaches about 30°, which is out of the \\nreachable space of the first module in the three-module robot (< 6°). Even so, biLSTM \\ntrained with the three-module robot can fulfill this control task with relatively low errors, \\nwhich shows that this controller is accurate and has module number adaptability. \\n \\n4. Summary \\n \\nThis paper introduces a novel and accurate biLSTM configuration controller for module \\nnumber adaptability and a data collection strategy specifically for modular soft robots. \\nThe dataset generated by the proposed collection strategy covers a larger space than \\nthat from the traditional method, and the biLSTM controller can decide actuation \\nvariables based on the previous and other module states. Simulation cable-driven robots \\nwith four and six modules and real pneumatic robots with three and two modules are \\nleveraged for experiments, and the results have proven that the biLSTM controller for \\nmodule number adaptability can fulfill module configuration control tasks on robots \\nwith different module numbers. \\n \\nAs far as we know, this is the first paper that includes biLSTM as a soft robot controller. \\nThe application of this network is inspired by the structure of modular soft robots shown \\nin Figure 1-(A). To control a system with such an inner interaction relationship, we find \\na neural network sharing a similar interaction principle among units, which is biLSTM. \\nThis kind of strategy is a hint at applying data models that share similar principles with \\nphysical systems and may expand the neural network application in soft robots. For \\nexample, besides RNN, which can represent the time sequence of soft robot motion, a \\ngenerative adversarial network (GAN) may be employed to train a pair of robot model \\nand controller simultaneously, considering the pairing relationship of modeling and \\ncontrol. \\n \\nAs mentioned above, we may utilize biLSTM units to infer the space sequence and \\nLSTM units to infer the time sequence in a large neural network in future work. Also, \\na planning approach may be introduced to map between the task and configuration \\nspace, as shown in Figure 1-(A). The planning method may combine neural networks \\nand some physical or analytical models like 37. Based on the proposed offline-trained \\ncontroller, we can integrate it with an online controller like 19,38 to decrease the control \\nerrors and achieve interchangeability among modules. \\n \\n \\n \\nAcknowledgments \\n \\nWe acknowledge the support of the European Union by the Next Generation EU project \\nECS00000017 ‘Ecosistema dell’Innovazione’ Tuscany Health Ecosystem (THE, PNRR, \\nSpoke 4: Spoke 9: Robotics and Automation for Health.) \\n \\nAuthor Disclosure Statement \\n \\nNo competing financial interests exist. \\n \\nReferences \\n1. Alfalahi H, Renda F, Stefanini C. Concentric tube robots for minimally invasive \\nsurgery: Current applications and future opportunities. IEEE Trans Med Robot \\nBionics 2020;2(3):410–424. \\n2. Wang H, Zhang R, Chen W, et al. A cable-driven soft robot surgical system for \\ncardiothoracic endoscopic surgery: preclinical tests in animals. Surg Endosc \\n2017;31:3152–3158. \\n3. Tang ZQ, Heung HL, Shi XQ, et al. Probabilistic model-based learning control of a \\nsoft pneumatic glove for hand rehabilitation. IEEE Trans Biomed Eng \\n2021;69(2):1016–1028. \\n4. Ansari Y, Manti M, Falotico E, et al. Multiobjective Optimization for Stiffness and \\nPosition Control in a Soft Robot Arm Module. IEEE Robot Autom Lett \\n2018;3(1):108–115; doi: 10.1109/LRA.2017.2734247. \\n5. Matia Y, Kaiser GH, Shepherd RF, et al. Harnessing nonuniform pressure \\ndistributions in soft robotic actuators. Adv Intell Syst 2023;5(2):2200330. \\n6. Marchese AD, Onal CD, Rus D. Autonomous soft robotic fish capable of escape \\nmaneuvers using fluidic elastomer actuators. Soft Robot 2014;1(1):75–87. \\n7. Laschi C, Cianchetti M, Mazzolai B, et al. Soft robot arm inspired by the octopus. \\nAdv Robot 2012;26(7):709–727. \\n8. Renda F, Giorelli M, Calisti M, et al. Dynamic Model of a Multibending Soft Robot \\nArm Driven by Cables. IEEE Trans Robot 2014;30(5):1109–1122; doi: \\n10.1109/TRO.2014.2325992. \\n9. Renda F, Cianchetti M, Giorelli M, et al. A 3D steady-state model of a tendon-driven \\ncontinuum soft manipulator inspired by the octopus arm. Bioinspir Biomim \\n2012;7(2):025006; doi: 10.1088/1748-3182/7/2/025006. \\n10. Guan Q, Sun J, Liu Y, et al. Novel bending and helical extensile/contractile \\npneumatic artificial muscles inspired by elephant trunk. Soft Robot 2020;7(5):597–\\n614. \\n11. Chen Z, Le Gall A, Mocellin L, et al. Data Models Applied to Soft Robot Modeling \\nand Control: A Review. ArXiv Prepr ArXiv230512137 2023. \\n12. Laschi C, Thuruthel TG, Lida F, et al. Learning-Based Control Strategies for Soft \\nRobots: Theory, Achievements, and Future Challenges. IEEE Control Syst Mag \\n2023;43(3):100–113. \\n13. Della Santina C, Duriez C, Rus D. Model-Based Control of Soft Robots: A Survey \\nof the State of the Art and Open Challenges. IEEE Control Syst Mag 2023;43(3):30–\\n65; doi: 10.1109/MCS.2023.3253419. \\n14. George Thuruthel T, Ansari Y, Falotico E, et al. Control Strategies for Soft Robotic \\nManipulators: \\nA \\nSurvey. \\nSoft \\nRobot \\n2018;5(2):149–163; \\ndoi: \\n10.1089/soro.2017.0007. \\n15. Harada K, Oetomo D, Susilo E, et al. A reconfigurable modular robotic \\nendoluminal surgical \\nsystem: vision and preliminary results. \\nRobotica \\n2010;28(2):171–183. \\n16. Jiang H, Wang Z, Jin Y, et al. Hierarchical control of soft manipulators towards \\nunstructured interactions. Int J Robot Res 2021;40(1):411–434. \\n17. Bianchi D, Antonelli MG, Laschi C, et al. SofToss: Learning to Throw Objects \\nWith \\na \\nSoft \\nRobot. \\nIEEE \\nRobot \\nAutom \\nMag \\n2023;2–12; \\ndoi: \\n10.1109/MRA.2023.3310865. \\n18. Della Santina C, Katzschmann RK, Biechi A, et al. Dynamic Control of Soft \\nRobots Interacting with the Environment. In: 2018 IEEE International Conference \\non \\nSoft \\nRobotics \\n(RoboSoft) \\n2018; \\npp. \\n46–53; \\ndoi: \\n10.1109/ROBOSOFT.2018.8404895. \\n19. Chen Z, Ren X, Bernabei M, et al. A Hybrid Adaptive Controller for Soft Robot \\nInterchangeability. \\nIEEE \\nRobot \\nAutom \\nLett \\n2024;9(1):875–882; \\ndoi: \\n10.1109/LRA.2023.3337705. \\n20. Thuruthel TG, Falotico E, Renda F, et al. Model-Based Reinforcement Learning \\nfor Closed-Loop Dynamic Control of Soft Robotic Manipulators. IEEE Trans Robot \\n2019;35(1):124–134; doi: 10.1109/TRO.2018.2878318. \\n21. Gan Y, Li P, Jiang H, et al. A Reinforcement Learning Method for Motion Control \\nWith Constraints on an HPN Arm. IEEE Robot Autom Lett 2022;7(4):12006–12013. \\n22. Calinon S, Bruno D, Malekzadeh MS, et al. Human–robot skills transfer interfaces \\nfor a flexible surgical robot. Comput Methods Programs Biomed 2014;116(2):81–\\n96. \\n23. Wang H, Chen J, Lau HY, et al. Motion planning based on learning from \\ndemonstration for multiple-segment flexible soft robots actuated by electroactive \\npolymers. IEEE Robot Autom Lett 2016;1(1):391–398. \\n24. Jiang H, Wang Z, Liu X, et al. A Two-Level Approach for Solving the Inverse \\nKinematics of an Extensible Soft Arm Considering Viscoelastic Behavior. In: 2017 \\nIEEE International Conference on Robotics and Automation (ICRA) IEEE; 2017; pp. \\n6127–6133. \\n25. You X, Zhang Y, Chen X, et al. Model-Free Control for Soft Manipulators Based \\non Reinforcement Learning. In: 2017 IEEE/RSJ International Conference on \\nIntelligent Robots and Systems (IROS) IEEE; 2017; pp. 2909–2915. \\n26. Jin Y, Wang Y, Chen X, et al. Model-Less Feedback Control for Soft Manipulators. \\nIn: 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems \\n(IROS) IEEE; 2017; pp. 2916–2922. \\n27. Li P, Wang G, Jiang H, et al. A Q-Learning Control Method for a Soft Robotic Arm \\nUtilizing Training Data from a Rough Simulator. In: 2021 IEEE International \\nConference on Robotics and Biomimetics (ROBIO) IEEE; 2021; pp. 839–845. \\n28. Or K, Wu K, Nakano K, et al. Curriculum-reinforcement learning on simulation \\nplatform of tendon-driven high-degree of freedom underactuated manipulator. Front \\nRobot AI 2023;10. \\n29. Armanini C, Boyer F, Mathew AT, et al. Soft Robots Modeling: A Structured \\nOverview. \\nIEEE \\nTrans \\nRobot \\n2023;39(3):1728–1748; \\ndoi: \\n10.1109/TRO.2022.3231360. \\n30. Baaij T, Holkenborg MK, Stölzle M, et al. Learning 3D shape proprioception for \\ncontinuum soft robots with multiple magnetic sensors. Soft Matter 2023;19(1):44–\\n56. \\n31. Li Y, Myszka DH, Murray A. The Kinematics of Constant Curvature Continuum \\nRobots Through Three Segments. IEEE Robot Autom Lett 2023. \\n32. Wild S, Zeng T, Mohammad A, et al. Efficient and Scalable Inverse Kinematics for \\nContinuum Robots. IEEE Robot Autom Lett 2023. \\n33. Renda F, Boyer F, Dias J, et al. Discrete Cosserat Approach for Multisection Soft \\nManipulator \\nDynamics. \\nIEEE Trans \\nRobot \\n2018;34(6):1518–1533; \\ndoi: \\n10.1109/TRO.2018.2868815. \\n34. Gazzola M, Dudte L, McCormick A, et al. Forward and inverse problems in the \\nmechanics of soft filaments. R Soc Open Sci 2018;5(6):171628; doi: \\n10.1098/rsos.171628. \\n35. Chang H-S, Halder U, Shih C-H, et al. Energy Shaping Control of a Muscular \\nOctopus Arm Moving in Three Dimensions. ArXiv Prepr ArXiv220904089 2022. \\n36. George Thuruthel T, Gardner P, Iida F. Closing the control loop with time-variant \\nembedded soft sensors and recurrent neural networks. Soft Robot 2022;9(6):1167–\\n1176. \\n37. Liu J, Borja P, Della Santina C. Physics-Informed Neural Networks to Model and \\nControl Robots: A Theoretical and Experimental Investigation. 2023; doi: \\n10.48550/arXiv.2305.05375. \\n38. Nazeer MS, Bianchi D, Campinoti G, et al. Policy Adaptation Using an Online \\nRegressing Network in a Soft Robotic Arm. In: 2023 IEEE International Conference \\non \\nSoft \\nRobotics \\n(RoboSoft) \\n2023; \\npp. \\n1–7; \\ndoi: \\n10.1109/RoboSoft55895.2023.10121927. \\n \\n \\n \\nTables \\n \\nTable 1. Mean and standard derivation of actuation value estimation errors with \\ndifferent networks \\n \\nfour LSTM \\nLSTM \\nbiLSTM \\nmodule 1 \\n18.52 ± 14.56% \\n0.93 ± 0.95% \\n1.63 ± 1.71% \\nmodule 2 \\n12.19 ± 10.33% \\n0.85 ± 0.85% \\n1.49 ± 1.52% \\nmodule 3 \\n7.69 ± 6.60% \\n0.76 ± 0.68% \\n1.11 ± 1.05% \\nmodule 4 \\n3.74 ± 3.21% \\n0.64 ± 0.57% \\n0.70 ± 0.65% \\n \\nTable 2. Mean and standard derivation of configuration control errors for the four-\\nmodule simulation robot \\n \\nLSTM-A \\nLSTM-B \\nLSTM-C \\nbiLSTM-\\nA \\nbiLSTM-\\nB \\nbiLSTM-\\nC \\nmodule 1 \\n4.70 ± 2.65%  2.07 ± 0.37% \\n7.62 ± 2.59% \\n3.95 ± 1.84% \\n0.72 ± 0.29% \\n3.61 ± 1.72% \\nmodule 2 \\n3.77 ± 1.73% \\n0.93 ± 0.49% \\n5.97 ± 1.69% \\n3.45 ± 1.09% \\n0.67 ± 0.28% \\n2.24 ± 1.36% \\nmodule 3 \\n1.29 ± 1.16% \\n1.58 ± 0.70% \\n2.21 ± 1.07% \\n1.64 ± 1.05% \\n1.16 ± 0.63% \\n1.01 ± 0.40% \\nmodule 4 \\n2.10 ± 1.41% \\n1.89 ± 0.91% \\n1.85 ± 1.00% \\n1.09 ± 0.81% \\n1.92 ± 0.78% \\n1.18 ± 0.59% \\n \\nTable 3. Mean and standard derivation of configuration control errors for the six-\\nmodule simulation robot \\n \\nA \\nB \\nC \\nmodule 1 \\n2.94 ± 1.51% \\n3.99 ± 0.43% \\n1.82 ± 0.25% \\nmodule 2 \\n1.73 ± 0.94% \\n2.43 ± 0.42% \\n2.28 ± 0.40% \\nmodule 3 \\n5.18 ± 1.74% \\n0.65 ± 0.33% \\n3.35 ± 0.66% \\nmodule 4 \\n5.14 ± 1.84% \\n1.80 ± 0.67% \\n1.21 ± 0.63% \\nmodule 5 \\n3.37 ± 0.97% \\n4.30 ± 0.57% \\n3.30 ± 0.75% \\nmodule 6 \\n2.49 ± 0.45% \\n1.80 ± 0.85% \\n3.58 ± 1.11% \\n \\nTable 4. Mean and standard derivation of configuration control errors for the three-\\nmodule and two-module real robot \\n \\n3-edge \\n3-down \\n2-edge \\n2-down \\nmodule 1 \\n0.58 ± 0.45° \\n0.50 ± 0.47° \\n0.94 ± 0.63° \\n1.47 ± 0.86° \\nmodule 2 \\n0.92 ± 0.76° \\n1.72 ± 1.19° \\n4.26 ± 2.80° \\n1.69 ± 1.28° \\nmodule 3 \\n2.69 ± 1.66° \\n1.98 ± 1.52° \\n/ \\n/ \\n \\n \\n \\nFigures \\n \\n \\nFigure 1. (A) Modular soft robot structure. The actuation of each module 𝐴 in the action \\nspace will affect its configuration 𝑆 in the configuration space. Also, the configuration \\nis affected by gravity, interaction with the adjacent modules, and base. Finally, all the \\nconfigurations have different impacts on the end pose in the task space. This paper \\nproposes a configuration controller, shown as the boxes. (B) The diagram of a modular \\nsoft robot. (C) BiLSTM controller. Each unit takes the desired module states 𝑆𝑑, module \\nnumber label 𝑚, previous module states and actuations, 𝑆 and 𝐴, as input and produces \\nthe actuation 𝐴̂ for this time step. (D) The diagram of the biLSTM unit for module Ⅱ. \\n \\n \\nFigure 2. (A) General data collection method. (B) Data collection method proposed for \\nmodular robots. 𝑎∗ represents different random actuation sequences.  \\n \\n \\nFigure 3. (A) Simulation robot diagram. The robot comprises four modules, and the end \\nof each module is shown by three coordinate axes. The direction of gravity is upward \\nin this diagram. (B) Real robot setup. A pneumatic robot composed of three modules is \\nactuated by valves controlled by Arduino and PC. The optical tracking system tracks \\nthe optical tracker motion and sends to PC for biLSTM control. (C) Simulation and (D) \\nreal module configuration. The configuration of each module is shown by the \\norientation of end unit vector. (E) Simulation and (F) real module actuation. Each \\nsimulation module is actuated by four cables while the real one is actuated by two \\nchambers. \\n \\n \\nFigure 4. The diagram of datasets collected with (A) the traditional method and (B) our \\nmethod. The points represent the end positions of the first, second, third, and final \\nmodules. \\n \\n \\nFigure 5. Simulation four-module robot motion in task A, B, and C with biLSTM. The \\ncolor becomes deep with the improvement of time steps. \\n \\n \\nFigure 6. The desired and real module states of the simulation four-module robot in task \\nA, B, and C with biLSTM. \\n \\n \\nFigure 7. Simulation six-module robot motion in task A, B, and C with biLSTM. The \\ncolor becomes deep with the improvement of time steps. \\n \\n \\nFigure 8. The desired and real module states of the simulation six-module robot in task \\nA, B, and C with biLSTM. \\n \\n \\nFigure 9. The desired and real module states of the real three-module robot in task (A) \\nedge and (B) down with biLSTM. \\n \\n \\nFigure 10. The desired and real module states of the real two-module robot in task (A) \\nedge and (B) down with biLSTM. \\n \\n \\nSupplementary Data  \\n \\nFigure S1. (A) Pneumatic robot made of silicone and apart of unfolded origami \\nstructure. (B) a folded origami structure. (C) Each origami structure covers one \\npneumatic robot. One 3D-printed connector connects two modules, and a pair of optical \\ntrackers are fixed on the connector. (D) A modular robot system is connected to a metal \\nstick, and three fixed optical trackers are applied for system calibration. \\n \\n \\nFigure S2. The diagram of (A) four LSTM and (B) LSTM. \\n \\nTable S1. Neural networks parameters \\n \\nFour LSTM \\nLSTM \\nbiLSTM \\nbiLSTM (real) \\nlayer number \\n4 \\n4 \\n4 \\n4 \\nhidden state size \\n128 \\n128 \\n128 \\n128 \\ntime step number \\n5 \\n5 \\n5 \\n10 \\nbatch size \\n64 \\n64 \\n64 \\n128 \\noptimizer \\nAdam \\nAdam \\nAdam \\nAdam \\nLearning rate \\n0.001 \\n0.001 \\n0.001 \\n0.0003 \\n \\nSimulation configuration trajectory: \\n \\nIn task A, the trajectories for 𝑣𝑥, 𝑣𝑦, and 𝑣𝑧 are \\n{\\n  \\n \\n  \\n \\n𝑣𝑑𝑧(𝑡) = 1 − (1 − 𝑣zmin) ∗\\n𝑡\\n𝑡𝑚𝑎𝑥\\n𝑣𝑑𝑥(𝑡) = sin (2𝜋 ∗\\n𝑡\\n𝑡𝑚𝑎𝑥\\n) ∗ √1 − 𝑣𝑑𝑧(𝑡)2\\n𝑣𝑑𝑦(𝑡) = cos (2𝜋 ∗\\n𝑡\\n𝑡𝑚𝑎𝑥\\n) ∗ √1 − 𝑣𝑑𝑧(𝑡)2\\n,\\n(S1) \\nwhere 𝑣𝑧𝑚in denotes the minimal value of 𝑣𝑧. It is [0.975,0.850,0.725,0.625] for four-\\nmodule robot and [0.998, 0.995, 0.950, 0.850, 0.800, 0.650] for six-module robot. 𝑡𝑚𝑎𝑥 \\nrepresents the length of each control trial and is 250 in simulation. \\n \\nIn task B, the trajectories for 𝑣𝑥, 𝑣𝑦, and 𝑣𝑧 are \\n{\\n \\n \\n \\n \\n𝑣𝑑𝑧(𝑡) = 𝑣𝑑𝑧\\n𝑣𝑑𝑥(𝑡) = 𝑎 ∗ sin (2𝜋 ∗\\n𝑡\\n𝑡𝑚𝑎𝑥\\n) ∗ √1 − 𝑣𝑑𝑧(𝑡)2\\n𝑣𝑑𝑦(𝑡) = 𝑎 ∗ cos (2𝜋 ∗\\n𝑡\\n𝑡𝑚𝑎𝑥\\n) ∗ √1 − 𝑣𝑑𝑧(𝑡)2\\n,\\n(S2) \\nwhere 𝑣𝑑𝑧 is [0.998,0.998,0.996,0.600] for four-module robot and [0.999, 0.999, 0.999, \\n0.998, 0.995, 0.708] for six-module robot. 𝑎 is employed to set the module rotation \\ndirection. It is [1,1,1,-1] for four-module robot and [1,1,1,1,-1,-1] for six-module robot. \\n \\nIn task C, the trajectories for 𝑣𝑥, 𝑣𝑦, and 𝑣𝑧 are \\n{  \\n  𝑣𝑑𝑧(𝑡) = 1 − (1 − 𝑣zmin) ∗ 𝑡\\n50\\n𝑣𝑑𝑥(𝑡) = 0\\n𝑣𝑑𝑦(𝑡) = 𝑎 ∗ √1 − 𝑣𝑑𝑧(𝑡)2\\n, t < 50,\\n (S3) \\n{\\n  \\n  \\n𝑣𝑑𝑧(𝑡) = 𝑣𝑧𝑚𝑖𝑛\\n𝑣𝑑𝑥(𝑡) = 𝑎 ∗ sin (2𝜋 ∗ 𝑡 − 50\\n200 ) ∗ √1 − 𝑣𝑑𝑧(𝑡)2\\n𝑣𝑑𝑦(𝑡) = 𝑎 ∗ cos (2𝜋 ∗ 𝑡 − 50\\n200 ) ∗ √1 − 𝑣𝑑𝑧(𝑡)2\\n, t ≥ 50,\\n(S4) \\nwhere 𝑣𝑧𝑚in  is [0.941,0.998,0.897,0.650] for four-module robot and [0.999, 0.996, \\n0.985, 0.975, 0.925, 0.600] for six-module robot. 𝑎 is [1,1,1,-1] for four-module robot \\nand [1,1,1,1,1,-1] for six-module robot. \\n \\n \\nFigure S3. Actuation variables of simulation four-module robot in task A, B, and C with \\nbiLSTM. \\n \\n \\nFigure S4. Actuation variables of simulation six-module robot in task A, B, and C with \\nbiLSTM. \\n \\n \\nFigure S5. Dataset collected with our proposed data collection method. (A), (B) and (C) \\nrepresent three actuation methods shown in Figure 2-(B).  \\n \\nReal angle trajectory: \\n \\nIn real experiments, each trial contains 200 steps, which means  𝑡𝑚𝑎𝑥 = 200. \\n \\nThe desired angle trajectories are \\n𝑎𝑛𝑔𝑑 =\\n{\\n \\n \\n \\n \\n𝑎𝑛𝑔𝑚𝑎𝑥 ∗ 𝑡\\n50 , 𝑡 < 50\\n𝑎𝑛𝑔𝑚𝑎𝑥 ∗ (2 − 𝑡\\n50) , 50 ≤ 𝑡 < 150\\n𝑎𝑛𝑔𝑚𝑎𝑥 ∗ ( 𝑡\\n50 − 4) , 150 ≤ 𝑡\\n,\\n(𝑆5) \\nwhere 𝑎𝑛𝑔𝑚𝑎𝑥 is the maximal module bending angle. For task edge, it is [5.4°, 18°,90°] \\nfor three-module robot and [21.6°, 72°] for two-module robot. For task down, it is [3.6°, \\n36°, -39.6°] for three-module robot and [3.24°, -32.4°] for two-module robot. \\n \\n \\nFigure S6. Actuation variables of real three-module robot in task (A) edge and (B) down \\nwith biLSTM. \\n \\n \\nFigure S7. Actuation variables of real two-module robot in task (A) edge and (B) down \\nwith biLSTM. \\n'},\n",
       " {'abstract': 'We present the Radiation Oncology NLP Database (ROND), the first dedicated Natural Language Processing (NLP) dataset for radiation oncology, an important medical specialty that has received limited attention from the NLP community in the past. With the advent of Artificial General Intelligence (AGI), there is an increasing need for specialized datasets and benchmarks to facilitate research and development. ROND is specifically designed to address this gap in the domain of radiation oncology, a field that offers many opportunities for NLP exploration. It encompasses various NLP tasks including Logic Reasoning, Text Classification, Named Entity Recognition (NER), Question Answering (QA), Text Summarization, and Patient-Clinician Conversations, each with a distinct focus on radiation oncology concepts and application cases. In addition, we have developed an instruction-tuning dataset consisting of over 20k instruction pairs (based on ROND) and trained a large language model, CancerChat. This serves to demonstrate the potential of instruction-tuning large language models within a highly-specialized medical domain. The evaluation results in this study could serve as baseline results for future research. ROND aims to stimulate advancements in radiation oncology and clinical NLP by offering a platform for testing and improving algorithms and models in a domain-specific context. The ROND dataset is a joint effort of multiple U.S. health institutions. The data is available at https://github.com/zl-liu/Radiation-Oncology-NLP-Database.',\n",
       "  'introduction': 'Radiation oncology is a critical medical specialty that employs high-energy radiation to treat and manage cancer and other diseases (Bernier et al., 2004; Unkelbach et al., 2018). Indeed, like many medical domains, there is much potential to integrate natural language processing (NLP) into radiotherapy research and practice (Bitterman et al., 2021; Rezayi et al., 2022). However, there is limited development and evaluation of NLP models in this domain due to the lack of dedicated datasets (Rezayi et al., 2022). In response to this need, we present the Radiation Oncology NLP Database (ROND).\\nROND is the world’s first NLP dataset specifically created for radiation oncology. It aims to provide a comprehensive platform for researchers to develop, test, and improve NLP models and methods within this domain. This dataset covers a wide spectrum of NLP tasks, including Logic Reasoning, Clinical Text Classification, Named Entity Recognition (NER), Question Answering (QA), and Text Summarization. Each of these tasks is centered around distinct aspects of radiation oncology, offering researchers a rich and varied dataset for exploration and model training. In addition, ROND contains a Patient-Clinician conversation dataset, which provides valuable insights into patient interactions, symptom descriptions, and treatment discussions, enhancing our understanding and modeling of complex medical dialogues. Figure 1 presents an overview of ROND.\\nThe unique structure of ROND facilitates the development of models capable of reasoning logically about complex radiation oncology concepts, classifying domain-specific text data, recognizing and categorizing specialized entities, accurately answering radiation oncology-related questions, and summarizing lengthy documents and research papers in the field. We aim to establish a benchmark for future studies that stimulates innovation in radiation oncology research and ultimately improves patient care through the power of NLP.\\nWe believe this database is of particular importance in the age of Artificial General Intelligence (AGI) (Bubeck et al., 2023; Zhao et al., 2023; Liu et al., 2023a). Successful large language models (LLM) such as ChatGPT, GPT-4, LLAMA (Touvron et al., 2023) and PaLM (Chowdhery et al., 2022) are trained on vast amounts of public domain data. Some LLMs such as Med-Palm 2 (Singhal et al., 2022) are trained on both public biomedical data sources and private hospital (e.g., through the Google-Mayo Clinic partnership) data, and consequently are highly capable of processing medical text (Singhal et al., 2022). However, there is no existing dataset that specifically supports NLP in radiation oncology. The ROND dataset complements recent LLM advancements and offers a platform to better integrate LLMs into healthcare.',\n",
       "  'literature review': 'The Radiation Oncology NLP Database (ROND) is the first NLP dataset specifically created for radiation oncology. It aims to provide a comprehensive platform for researchers to develop, test, and improve NLP models and methods within this domain. This dataset covers a wide spectrum of NLP tasks, including Logic Reasoning, Clinical Text Classification, Named Entity Recognition (NER), Question Answering (QA), and Text Summarization. Each of these tasks is centered around distinct aspects of radiation oncology, offering researchers a rich and varied dataset for exploration and model training. In addition, ROND contains a Patient-Clinician conversation dataset, which provides valuable insights into patient interactions, symptom descriptions, and treatment discussions, enhancing our understanding and modeling of complex medical dialogues.',\n",
       "  'methodology': 'We present the Radiation Oncology NLP Database (ROND), the first dedicated Natural Language Processing (NLP) dataset for radiation oncology, an important medical specialty that has received limited attention from the NLP community in the past. With the advent of Artificial General Intelligence (AGI), there is an increasing need for specialized datasets and benchmarks to facilitate research and development. ROND is specifically designed to address this gap in the domain of radiation oncology, a field that offers many opportunities for NLP exploration. It encompasses various NLP tasks including Logic Reasoning, Text Classification, Named Entity Recognition (NER), Question Answering (QA), Text Summarization, and Patient-Clinician Conversations, each with a distinct focus on radiation oncology concepts and application cases. In addition, we have developed an instruction-tuning dataset consisting of over 20k instruction pairs (based on ROND) and trained a large language model, CancerChat. This serves to demonstrate the potential of instruction-tuning large language models within a highly-specialized medical domain. The evaluation results in this study could serve as baseline results for future research. ROND aims to stimulate advancements in radiation oncology and clinical NLP by offering a platform for testing and improving algorithms and models in a domain-specific context. The ROND dataset is a joint effort of multiple U.S. health institutions. The data is available at https://github.com/zl-liu/Radiation-Oncology-NLP-Database.',\n",
       "  'results': 'The Logic Reasoning subset of the Radiation Oncology NLP Database (ROND) presents questions designed to assess the logical reasoning capabilities of NLP models within the context of radiation oncology. The questions are geared towards the understanding of fundamental concepts and principles in radiation oncology, such as the properties of radioactive elements, atomic structure, electron orbits, X-ray emission, penumbra effects, and interaction of different particles with matter. We manually created and annotated 100 logic reasoning questions for this dataset.\\nEach question in this subset is structured as a yes/no question, designed to elicit a binary response. The questions range from basic atomic structure, such as \"Does an atom consist of a positively charged nucleus surrounded by a cloud of negatively charged electrons?\" to more specific queries about X-ray production and penumbra effect, such as \"In X-ray production, does the efficiency of x-ray production depend on the size of the target?\" or \"Is physical penumbra influenced by geometric penumbra, beam energy, and the lateral transport of electrons in the tissues?\"\\nThis dataset provides an avenue to evaluate the ability of NLP models to apply logical reasoning within the domain-specific context of radiation oncology, emphasizing both the understanding of fundamental radiation oncology concepts and the ability to apply this knowledge to specific scenarios.',\n",
       "  'conclusion': 'We present the Radiation Oncology NLP Database (ROND), the first dedicated Natural Language Processing (NLP) dataset for radiation oncology, an important medical specialty that has received limited attention from the NLP community in the past. With the advent of Artificial General Intelligence (AGI), there is an increasing need for specialized datasets and benchmarks to facilitate research and development. ROND is specifically designed to address this gap in the domain of radiation oncology, a field that offers many opportunities for NLP exploration. It encompasses various NLP tasks including Logic Reasoning, Text Classification, Named Entity Recognition (NER), Question Answering (QA), Text Summarization, and Patient-Clinician Conversations, each with a distinct focus on radiation oncology concepts and application cases. In addition, we have developed an instruction-tuning dataset consisting of over 20k instruction pairs (based on ROND) and trained a large language model, CancerChat. This serves to demonstrate the potential of instruction-tuning large language models within a highly-specialized medical domain. The evaluation results in this study could serve as baseline results for future research. ROND aims to stimulate advancements in radiation oncology and clinical NLP by offering a platform for testing and improving algorithms and models in a domain-specific context. The ROND dataset is a joint effort of multiple U.S. health institutions. The data is available at https://github.com/zl-liu/Radiation-Oncology-NLP-Database.',\n",
       "  'title': 'The Radiation Oncology NLP Database',\n",
       "  'author': 'Zhengliang Liu, Jason Holmes, Wenxiong Liao, Chenbin Liu, Lian Zhang, Hongying Feng, Peilong Wang, Muhammad Ali Elahi, Hongmin Cai, Lichao Sun, Quanzheng Li, Xiang Li, Tianming Liu, Jiajian Shen, Wei Liu',\n",
       "  'textdata': 'The Radiation Oncology NLP Database\\nZhengliang Liu, Jason Holmes, Wenxiong Liao, Chenbin Liu,\\nLian Zhang, Hongying Feng, Peilong Wang, Muhammad Ali Elahi,\\nHongmin Cai, Lichao Sun, Quanzheng Li, Xiang Li,\\nTianming Liu, Jiajian Shen, Wei Liu\\nAbstract\\nWe present the Radiation Oncology NLP\\nDatabase (ROND), the first dedicated Natural\\nLanguage Processing (NLP) dataset for radia-\\ntion oncology, an important medical specialty\\nthat has received limited attention from the\\nNLP community in the past. With the advent\\nof Artificial General Intelligence (AGI), there\\nis an increasing need for specialized datasets\\nand benchmarks to facilitate research and devel-\\nopment. ROND is specifically designed to ad-\\ndress this gap in the domain of radiation oncol-\\nogy, a field that offers many opportunities for\\nNLP exploration. It encompasses various NLP\\ntasks including Logic Reasoning, Text Clas-\\nsification, Named Entity Recognition (NER),\\nQuestion Answering (QA), Text Summariza-\\ntion, and Patient-Clinician Conversations, each\\nwith a distinct focus on radiation oncology con-\\ncepts and application cases. In addition, we\\nhave developed an instruction-tuning dataset\\nconsisting of over 20k instruction pairs (based\\non ROND) and trained a large language model,\\nCancerChat. This serves to demonstrate the\\npotential of instruction-tuning large language\\nmodels within a highly-specialized medical\\ndomain. The evaluation results in this study\\ncould serve as baseline results for future re-\\nsearch. ROND aims to stimulate advancements\\nin radiation oncology and clinical NLP by of-\\nfering a platform for testing and improving\\nalgorithms and models in a domain-specific\\ncontext. The ROND dataset is a joint effort\\nof multiple U.S. health institutions. The data\\nis available at https://github.com/zl-liu/\\nRadiation-Oncology-NLP-Database.\\n1\\nIntroduction\\nRadiation oncology is a critical medical specialty\\nthat employs high-energy radiation to treat and\\nmanage cancer and other diseases (Bernier et al.,\\n2004; Unkelbach et al., 2018). Indeed, like many\\nmedical domains, there is much potential to inte-\\ngrate natural language processing (NLP) into ra-\\ndiotherapy research and practice (Bitterman et al.,\\n2021; Rezayi et al., 2022). However, there is lim-\\nited development and evaluation of NLP models in\\nthis domain due to the lack of dedicated datasets\\n(Rezayi et al., 2022). In response to this need,\\nwe present the Radiation Oncology NLP Database\\n(ROND).\\nROND is the world’s first NLP dataset specif-\\nically created for radiation oncology. It aims to\\nprovide a comprehensive platform for researchers\\nto develop, test, and improve NLP models and\\nmethods within this domain. This dataset covers\\na wide spectrum of NLP tasks, including Logic\\nReasoning, Clinical Text Classification, Named En-\\ntity Recognition (NER), Question Answering (QA),\\nand Text Summarization. Each of these tasks is\\ncentered around distinct aspects of radiation oncol-\\nogy, offering researchers a rich and varied dataset\\nfor exploration and model training. In addition,\\nROND contains a Patient-Clinician conversation\\ndataset, which provides valuable insights into pa-\\ntient interactions, symptom descriptions, and treat-\\nment discussions, enhancing our understanding and\\nmodeling of complex medical dialogues. Figure 1\\npresents an overview of ROND.\\nThe unique structure of ROND facilitates the\\ndevelopment of models capable of reasoning logi-\\ncally about complex radiation oncology concepts,\\nclassifying domain-specific text data, recognizing\\nand categorizing specialized entities, accurately an-\\nswering radiation oncology-related questions, and\\nsummarizing lengthy documents and research pa-\\npers in the field. We aim to establish a benchmark\\nfor future studies that stimulates innovation in ra-\\ndiation oncology research and ultimately improves\\npatient care through the power of NLP.\\nWe believe this database is of particular impor-\\ntance in the age of Artificial General Intelligence\\n(AGI) (Bubeck et al., 2023; Zhao et al., 2023; Liu\\net al., 2023a). Successful large language models\\n(LLM) such as ChatGPT, GPT-4, LLAMA (Tou-\\nvron et al., 2023) and PaLM (Chowdhery et al.,\\n1\\narXiv:2401.10995v1  [cs.CL]  19 Jan 2024\\nFigure 1: Overview of the Radiation Oncology NLP Database.\\n2022) are trained on vast amounts of public domain\\ndata. Some LLMs such as Med-Palm 2 (Singhal\\net al., 2022) are trained on both public biomedical\\ndata sources and private hospital (e.g., through the\\nGoogle-Mayo Clinic partnership) data, and conse-\\nquently are highly capable of processing medical\\ntext (Singhal et al., 2022). However, there is no\\nexisting dataset that specifically supports NLP in ra-\\ndiation oncology. The ROND dataset complements\\nrecent LLM advancements and offers a platform to\\nbetter integrate LLMs into healthcare.\\n2\\nThe Radiation Oncology NLP Database\\n2.1\\nLogic Reasoning\\nThe Logic Reasoning subset of the Radiation On-\\ncology NLP Database (ROND) presents questions\\ndesigned to assess the logical reasoning capabili-\\nties of NLP models within the context of radiation\\noncology. The questions are geared towards the\\nunderstanding of fundamental concepts and princi-\\nples in radiation oncology, such as the properties\\nof radioactive elements, atomic structure, electron\\norbits, X-ray emission, penumbra effects, and inter-\\naction of different particles with matter. We man-\\nually created and annotated 100 logic reasoning\\nquestions for this dataset.\\nEach question in this subset is structured as a\\nyes/no question, designed to elicit a binary re-\\nsponse. The questions range from basic atomic\\nstructure, such as \"Does an atom consist of a pos-\\nitively charged nucleus surrounded by a cloud of\\nnegatively charged electrons?\" to more specific\\nqueries about X-ray production and penumbra ef-\\nfect, such as \"In X-ray production, does the effi-\\nciency of x-ray production depend on the size of\\nthe target?\" or \"Is physical penumbra influenced by\\ngeometric penumbra, beam energy, and the lateral\\ntransport of electrons in the tissues?\"\\nThis dataset provides an avenue to evaluate the\\nability of NLP models to apply logical reasoning\\nwithin the domain-specific context of radiation on-\\ncology, emphasizing both the understanding of fun-\\ndamental radiation oncology concepts and the abil-\\nity to apply this knowledge to specific scenarios.\\n2\\n1. Head and neck cancers of tumors located at the base of the \\nskull where nerves come out.\\nLabel: Proton therapy.\\n2. Lung cancers in the middle of chest or near the esophagus.\\nLabel: Proton therapy\\n3. Lower-cost option for prostate cancers.\\nLabel: Photon therapy\\nProton therapy?\\nPhoton therapy?\\nTumor location & size\\nComorbidities\\nCost\\n……\\n……\\nFigure 2: Illustration of the Clinical Text Classification dataset.\\n2.2\\nClinical Text Classification\\nThe Clinical Text Classification subset of ROND\\nis designed to test the capability of NLP models\\nin categorizing text inputs related to radiation on-\\ncology into predefined labels. This specific task\\nfocuses on determining the appropriate type of ther-\\napy (\"Proton therapy\" or \"Photon therapy\") based\\non descriptions of different cancer scenarios. We\\nmanually created and annotated 100 cases for this\\ndataset. Please see Figure 2 for an illustration of\\nthis dataset.\\nThe dataset presents a variety of clinical sce-\\nnarios and characteristics of cancers, such as the\\nlocation and sensitivity of the tumor, cost consid-\\nerations, patient demographics, and potential risks.\\nThese descriptions are then categorized into two\\nmajor classes: \"Proton therapy\" and \"Photon ther-\\napy\".\\nExamples include categorizing \"Head and neck\\ncancers of tumors located at the base of the skull\\nwhere nerves come out\" and \"Cancers in children\"\\nunder the label ’Proton therapy’. On the other hand,\\n\"Lower-cost option for prostate cancers\" and \"Bet-\\nter protection of skin\" are classified under ’Photon\\ntherapy’.\\nWe aim to facilitate the training and evaluation\\nof NLP models capable of accurately classifying\\nradiation oncology cases into relevant treatment\\nvenues, thereby potentially aiding decision-making\\nprocesses in clinical settings.\\n2.3\\nNamed Entity Recognition (NER)\\nThe Named Entity Recognition (NER) subset of\\nROND is designed to annotate entities in the text\\nthat pertain to the field of radiation oncology. This\\ntask is crucial for understanding specific details\\nwithin the text, such as identifying the names of\\ndoctors and patients (PERSON), types of diseases\\n(DISEASE), types of treatment (TREATMENT),\\nanatomical structures (ANATOMY), numeric val-\\nues (NUMBER), symptoms (SYMPTOM), and out-\\ncomes (OUTCOME). We asked GPT-4 to generate\\nsample sentences, and manually reviewed and an-\\nnotated 20 sets of NER samples that are factually\\ncorrect. While not all sentences necessarily contain\\nall seven NER tags, these are the maximum pos-\\nsible tags that any sentence from our sample sets\\nmight include. Figure 3 presents a sample from the\\nNER dataset.\\nFor instance, in the sentence \"Dr. Jenkins, a radi-\\nation oncologist, treated patient Sarah Williams for\\nbreast cancer, utilizing intensity-modulated radia-\\ntion therapy (IMRT) with a total dose of 50 Gy in\\n25 fractions\", the model is expected to identify \"Dr.\\nJenkins\" as a PERSON, \"radiation\" and \"intensity-\\nmodulated radiation therapy\" as a TREATMENT,\\n\"breast\" as an ANATOMY, \"50\" and \"25\" as NUM-\\nBERs, and \"skin irritation\" as a SYMPTOM.\\nIn the context of another sentence, \"In a study\\n3\\nSentence: Dr. Patel reported that the 30 patients with stage IIIB non-small\\ncell lung cancer who underwent concurrent chemoradiation therapy\\nshowed a significant decrease in dyspnea and cough symptoms compared\\nto those receiving radiotherapy alone.\\nTokens: [Dr., Patel, reported, that, the, 30, patients, with, stage, IIIB, non-\\nsmall, cell, lung, cancer, who, underwent, concurrent, chemoradiation,\\ntherapy, showed, a, significant, decrease, in, dyspnea, and, cough,\\nsymptoms, compared, to, those, receiving, radiotherapy, alone]\\nNER_tags: [Dr. (B-PERSON), Patel (I-PERSON), reported (O), that (O), the (O),\\n30 (B-NUMBER), patients (O), with (O), stage (B-DISEASE), IIIB (B-DISEASE),\\nnon-small (I-DISEASE), cell (I-DISEASE), lung (I-DISEASE), cancer (I-DISEASE),\\nwho (O), underwent (O), concurrent (B-TREATMENT), chemoradiation (I-\\nTREATMENT), therapy (I-TREATMENT), showed (O), a (O), significant (O),\\ndecrease (O), in (O), dyspnea (B-SYMPTOM), and (O), cough (B-SYMPTOM),\\nsymptoms (I-SYMPTOM), compared (O), to (O), those (O), receiving (O),\\nradiotherapy (B-TREATMENT), alone (O)]\\nPerson\\nOther\\nNumber\\nSymptom\\nTreatment\\nDisease\\nFigure 3: A sample of the NER dataset.\\nled by Dr.\\nJackson, 60 patients with glioblas-\\ntoma were treated using hypofractionated radiation\\ntherapy, administering 40 Gy in 15 fractions\", the\\nmodel should detect \"Dr. Jackson\" as a PERSON,\\n\"glioblastoma\" as a SYMPTOM, \"hypofraction-\\nated radiation therapy\" as a TREATMENT, and\\n\"40\" and \"15\" as NUMBERs.\\nThe objective of this subset is to evaluate a\\nmodel’s ability to identify these entities in radia-\\ntion oncology text, which is fundamental for struc-\\ntured information extraction and other downstream\\ntasks such as de-identification of sensitive patient\\ninformation (e.g., names and addresses) (Liu et al.,\\n2023b).\\n2.4\\nText Summarization\\nThe Text Summarization dataset within the Radia-\\ntion Oncology NLP Database offers a unique set of\\nresources for the exploration of text summarization\\nmethods in a highly specialized medical context.\\nThis dataset comprises a variety of research ab-\\nstracts from arXiv that are categorized under the\\nMedical Physics class. First, we collected all such\\npapers published since 2022. We then program-\\nmatically extracted the abstracts from these papers\\nand asked GPT-4 to produce summaries. Finally,\\nwe manually selected 200 paper abstracts that are\\ncorrectly and meaningfully summarized by GPT-4\\nto form this dataset.\\nEach record within the dataset includes key bib-\\nliographic information such as the title of the re-\\nsearch, the authors involved, the submission date,\\nthe arXiv identifier, the DOI, and the BibTeX entry.\\nThe categorization information is also provided in\\nthe form of classifications.\\nThe core components of each record are the\\nabstract and its corresponding summary. The ab-\\nstracts provide a brief, yet comprehensive overview\\nof the research conducted, including its objectives,\\nmethodology, results, and conclusions.\\nCorre-\\nspondingly, the summaries distill the critical el-\\nements of these abstracts into a concise form, de-\\nsigned to swiftly provide the reader with the key\\ntakeaways of the study.\\nThese pairs of abstracts and their summaries con-\\nstitute a valuable resource for supervised learning\\ntasks. They can facilitate the development and\\nfine-tuning of models focused on abstract summa-\\nrization within the domain of radiation oncology\\nand medical physics, a critical area in the broader\\nfield of cancer treatment.\\n2.5\\nQuestion and Answering (QA)\\nThe QA subset of the Radiation Oncology NLP\\nDatabase stands as a rigorous and comprehensive\\ncollection of multiple-choice questions encompass-\\ning a vast array of topics within the field of ra-\\ndiation physics. We in-house designed 100 ques-\\ntions comparable to those in the RAPHEX exam\\n(Hendee et al., 2007), which is a radiation oncology\\n4\\nFigure 4: A sample multiple-choice question from the QA dataset.\\nphysics test-preparation exam for medical physi-\\ncists and radiation oncologists. We created these\\nquestions from scratch, as we are not legally per-\\nmitted to reproduce or redistribute materials from\\nthe RAPHEX exam. The questions cover eight cat-\\negories: \"math-based questions\", \"basic physics\",\\n\"radiation measurements\", \"treatment planning\",\\n\"imaging modalities and applications\", \"brachyther-\\napy\", \"advanced treatment planning\", and \"safety,\\nQA, and protection\". Figure 4 contains an example\\nfrom the QA dataset.\\nEach question in this dataset demands a deep\\nunderstanding of core radiation physics principles.\\nTopics range from particle acceleration and atomic\\nmass structures to photon interactions, x-ray spec-\\ntra, radiation attenuation, and the functionalities of\\na linear accelerator. The multiple-choice format\\nadds a layer of complexity to the challenge, neces-\\nsitating that AI models not only comprehend the\\nunderlying physics concepts but also discern the\\nmost accurate answer from a selection of closely\\nrelated options.\\nThis dataset is designed to mimic the stringent\\nconditions of academic assessments in the field.\\nThis design aids in creating a realistic test of an\\nAI model’s abilities in knowledge comprehension,\\nreasoning, and mathematical computation under\\nconditions mirroring those in a professional or edu-\\ncational context.\\n3\\nClinical significance\\nLogic Reasoning: The development of models\\ncapable of logic reasoning in radiation oncology\\nholds the promise of aiding professionals in com-\\nplex decision-making processes. Such models can\\nenhance the comprehension of intricate clinical\\nscenarios, support decision-making, and optimize\\nindividualized treatment planning. This would be\\nparticularly useful in areas such as proton therapy\\nwhere detailed reasoning often guides the selection\\nbetween different treatment strategies (Liu et al.,\\n2012; Unkelbach et al., 2018; Schild et al., 2014).\\nClinical Text Classification: By training models\\nto classify clinical cases, we are opening avenues\\nto more personalized and efficient patient care. For\\ninstance, classifying patient cases into categories\\nsuch as eligibility for photon versus proton ther-\\napy based on patient information and clinical notes\\ncan expedite decision making and improve treat-\\nment outcomes (Bitterman et al., 2021; Taylor et al.,\\n2023).\\nNamed Entity Recognition (NER): NER tasks,\\nwhich involve identifying and classifying key in-\\nformation in text, provide a structured way to ex-\\ntract critical data points from unstructured clinical\\nnotes. This functionality is crucial in radiation on-\\ncology where specific entities, such as tumor types,\\nanatomical locations, or dosimetric parameters, are\\nparamount for the creation of optimal treatment\\nplans (Unkelbach et al., 2018; Schild et al., 2014;\\nLiu et al., 2018).\\nText Summarization: The ability to extract the\\nmost salient information from large volumes of text\\nis valuable in any field (El-Kassas et al., 2021), but\\nin radiation oncology, it can directly contribute to\\nimproved patient care. For example, summarizing\\nthe key findings of the latest research in radiation\\noncology could help clinicians stay updated with\\ncurrent knowledge without having to go through\\nlengthy papers, enabling them to swiftly apply\\nthese findings in their practice. In addition, text\\nsummarization can produce succinct descriptions\\nfrom lengthy clinical reports and notes (Feblowitz\\net al., 2011; Cai et al., 2021; Liu et al., 2023b),\\nwhich can significantly save time and facilitate clin-\\nical communication.\\nQuestion and Answering (QA): QA systems in\\nthe domain of radiation oncology could revolution-\\n5\\nFigure 5: A detailed analysis of LLMs’ performance on the medical physics board exam (RAPHEX) level questions.\\nBard\\nChatGPT\\nGPT-4\\n0.600\\n0.456\\n0.656\\nTable 1: Accuracy of state-of-the-art LLMs on the Logic\\nReasoning dataset.\\nBard\\nChatGPT\\nGPT-4\\n0.770\\n0.740\\n0.840\\nTable 2: Accuracy of state-of-the-art LLMs on the Text\\nClassification dataset.\\nize the way practitioners access and analyze rele-\\nvant information. Being able to ask specific ques-\\ntions and receive accurate answers quickly, whether\\nin terms of patient history, or intricate radiobiologi-\\ncal effects (Wang et al., 2018; Omer, 2021), would\\nimmensely improve the efficiency of the oncology\\npractice, saving practitioners time and possibly en-\\nhancing patient outcomes. The QA dataset in this\\ndatabase delineates the knowledge needed for ef-\\nfective QA in this highly specialized domain.\\n4\\nA Conversational Instruction Tuning\\nDataset based on ROND\\nTo facilitate future development of radiation on-\\ncology focused language models, we employed\\na data generation method to create synthetic data\\nbased on expert annotated data from the six key\\ncomponents within ROND: Logic Reasoning, Clin-\\nBard\\nChatGPT\\nGPT-4\\n0.667\\n0.642\\n0.785\\nTable 3: Accuracy of state-of-the-art LLMs on the NER\\ndataset.\\nBard\\nChatGPT\\nGPT-4\\n0.139\\n0.270\\n0.317\\nTable 4: BLEU4 scores of state-of-the-art LLMs on the\\nText Summarization dataset.\\nical Text Classification, Named Entity Recognition\\n(NER), Text Summarization, Question and Answer-\\ning (QA), and Conversational data.\\nThis synthetic data generation process was car-\\nried out to build a large dataset suitable for \"in-\\nstruction tuning\". Instruction tuning is an effec-\\ntive approach aimed at enhancing language models’\\nability to comprehend and follow natural language\\ninputs based on training on pairs of instruction-\\ninput-outputs (Wei et al., 2021; Peng et al., 2023).\\nThis strategy facilitates multi-task learning and en-\\nhances generalization for unseen tasks (Ouyang\\net al., 2022).\\nWe processed the diverse data formats within\\nROND into a unified instruction tuning structure,\\nwhere each data entry consists of three components:\\ninstruction, input, and output (please refer to Figure\\n7 for an example). This uniform structure stream-\\n6\\nFigure 6: Data augmentation and instruction tunning.\\nFigure 7: An example from the instruction tuning dataset.\\nBard\\nChatGPT\\nGPT-4\\n0.410\\n0.530\\n0.760\\nTable 5: Accuracy of state-of-the-art LLMs on the QA\\ndataset.\\nChatGPT\\nCancerChat\\n26\\n24\\nTable 6: Preference evaluation between ChatGPT and\\nCancerChat.\\nlines the training and tuning process, contributing\\nto more efficient learning and inference.\\nFor the data augmentation/synthetic data gener-\\nation, we utilized the APIs of ChatGPT and GPT-\\n4. Specifically, we employed the ChatGPT API\\nto generate synthetic samples for the Conversa-\\ntional data, Text Summarization, and Named Entity\\nRecognition. These tasks require somewhat less\\ndomain-specific understanding of radiation oncol-\\nogy, making ChatGPT a fitting choice. On the other\\nhand, for tasks such as Logic Reasoning, Clinical\\nText Classification, and Question Answering that\\nrequire more depth of understanding and reason-\\ning ability, we used the GPT-4 API. This decision\\nleverages the strength of each AI model, ensuring\\noptimal synthetic data generation across different\\ntasks within the ROND database.\\nWe scaled ROND from a few hundred data sam-\\nples to 20,160 instruction tuning pairs. The ma-\\njority of these samples, over 17,000, are conversa-\\ntional, simulating interactions between patients and\\nhealthcare providers. This is supplemented with\\n500 samples each for Logic Reasoning and Clin-\\nical Text Classification, and 1,000 samples each\\nfor Named Entity Recognition and Text Summa-\\nrization. This ensures a comprehensive dataset that\\nreflects the multifaceted nature of language process-\\ning in the context of radiation oncology, thereby\\nproviding a robust base for developing and refining\\nfuture NLP models in this domain. Furthermore,\\nthe emphasis on conversational data equips mod-\\nels trained on this dataset provides the potential\\nto create efficient chatbots specialized in radiation\\noncology.\\n4.1\\nCancerChat\\nTo evaluate the effectiveness and utility of the in-\\nstruction tuning dataset, we trained a demo lo-\\ncal LLM, CancerChat, based on Falcon-7B (Al-\\nmazrouei et al., 2023) (a model that recently tops\\nthe Open LLM Leaderboard (Face)). Figure 6 il-\\nlustrates the process of generating the instructiom-\\ntuning dataset and the training of CancerChat.\\nThe training was conducted on a server with 1\\nA100 80GB GPU. We utilize LoRA (Low Rank\\nAdaptation) (Hu et al., 2021) since LoRA weights\\nfacilitate model sharing and deployment. Our train-\\ning parameters were: a batch size of 128, a fixed\\nlearning rate at 3e-4, a lora_r (the rank of the low-\\nrank factorization) set to 8, a lora_alpha (the scaling\\nfactor for the rank) set to 16, and a dropout rate of\\n7\\n0.05 to mitigate overfitting.\\nTo preliminarily assess the performance of Can-\\ncerChat, we conducted a blind comparison using\\n50 queries (see Table 6). A medical physicist was\\nasked to evaluate and express their preference for\\nthe responses generated by ChatGPT and Cancer-\\nChat. The comparison yielded close results, with\\n26 of the 50 responses more favorable for ChatGPT,\\nwhile the remaining 24 preferred CancerChat.\\nThese preliminary results underscore the poten-\\ntial of CancerChat, which, despite being a smaller\\nlocal model specifically tailored for the domain\\nof radiation oncology, demonstrated competitive\\nperformance when compared to ChatGPT. This\\nsuggests the feasibility and promise of developing\\ndomain-specific language models leveraging our\\ninstruction-tuning dataset, offering a pathway for\\nfuture advancements in NLP for medical special-\\nties.\\n5\\nBenchmarking Large Language Models\\nWe evaluate three state-of-the-art language mod-\\nels, namely Bard (powered by Google PaLM 2\\n(Anil et al., 2023)), ChatGPT (OpenAI), and GPT-\\n4 (OpenAI, 2023), against the proposed radiation\\noncology NLP database and obtained insightful\\nresults.\\nOn the Logic Reasoning dataset, GPT-4 achieved\\nthe highest accuracy of 0.656, followed by Bard\\nwith an accuracy of 0.600, while ChatGPT scored\\nthe lowest with an accuracy of 0.456 (Table 1).\\nOn the Clinical Text Classification dataset, GPT-\\n4 again outperformed the other two models with an\\naccuracy of 0.840. However, the performance gap\\nwas narrower with Bard and ChatGPT registering\\naccuracies of 0.770 and 0.740, respectively (Table\\n2). Similar trends were observed in the Named\\nEntity Recognition (NER) dataset, where GPT-4\\nled with an accuracy of 0.758. Bard scored 0.667,\\nand ChatGPT trailed with an accuracy of 0.646\\n(Table 3).\\nFor text summarization (evaluated using the\\nBLEU score (Papineni et al., 2002)), GPT-4\\nrecorded the highest score of 0.317, with ChatGPT\\nperforming markedly better than Bard, scoring\\n0.270 compared to Bard’s 0.139 (Table 4). Finally,\\non the Question Answering (QA) dataset, GPT-\\n4 once again demonstrated superior performance\\nwith an accuracy of 0.760. ChatGPT achieved an\\naccuracy of 0.530, while Bard scored the lowest\\nwith 0.410 (Table 5). We also conducted a detailed\\nanalysis on eight categories of the QA dataset. Gen-\\nerally, GPT-4 outperformed both trained medical\\nphysicists and non-experts who participated in this\\nstudy. The medical physicists averaged an accuracy\\nof 0.76, while the non-experts averaged 0.28.\\nThese results underscore the effectiveness of\\nGPT-4 across various NLP tasks. However, while\\nGPT-4 demonstrated superior performance across\\nthe evaluated NLP tasks in ROND, we hold these\\nresults as a baseline for future research. As state-\\nof-the-art as these models may be, we envision the\\ndevelopment of more refined models specifically\\ntailored to the domain of radiation oncology. These\\nspecialized models will likely exhibit improved per-\\nformance and understanding of the nuances of this\\nunique field and deliver significant potential for\\nadvancing radiation oncology and clinical NLP.\\nLimitations\\nThe primary limitation of this study is inherent to\\nits novelty. ROND represents the first dedicated\\nNLP dataset in the field of radiation oncology. As\\nsuch, it is anticipated that there will be inherent\\nchallenges and unforeseen issues associated with\\nthis dataset that will only become apparent when\\nthe database is utilized practically by the commu-\\nnity. The efficacy and practical utility of language\\nmodels trained or evaluated on this dataset will\\nneed to be validated through real-world applica-\\ntion and extensive feedback from clinicians. This\\niterative process of application, feedback, and re-\\nfinement is essential to not only identify potential\\nproblems but also to improve the robustness and ap-\\nplicability of the models derived from this dataset.\\nEthics Statement\\nThe authors of this study conducted the research\\nin strict adherence to ethical guidelines and there\\nis no involvement of Protected Health Information\\n(PHI). We acknowledge that while this study has\\nthe potential to greatly improve radiation oncology\\nNLP, it is necessary to consider the need for respon-\\nsible use and development of AI. Any potential\\nbias or errors within the dataset can have signifi-\\ncant implications on model training and subsequent\\napplications. Thus, we recommend rigorous valida-\\ntion, including feedback from clinical practitioners,\\nbefore deploying models trained on this dataset in\\na practical setting. Future research should also be\\nconducted with the broader social and ethical im-\\nplications in mind, always prioritizing the safety\\n8\\nand best interests of patients.\\nReferences\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\\nMerouane Debbah, Etienne Goffinet, Daniel Hes-\\nlow, Julien Launay, Quentin Malartic, Badreddine\\nNoune, Baptiste Pannier, and Guilherme Penedo.\\n2023. Falcon-40B: an open large language model\\nwith state-of-the-art performance.\\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\\nChen, et al. 2023. Palm 2 technical report. arXiv\\npreprint arXiv:2305.10403.\\nJacques Bernier, Eric J Hall, and Amato Giaccia. 2004.\\nRadiation oncology: a century of achievements. Na-\\nture Reviews Cancer, 4(9):737–747.\\nDanielle S Bitterman, Timothy A Miller, Raymond H\\nMak, and Guergana K Savova. 2021. Clinical natural\\nlanguage processing for radiation oncology: a review\\nand practical primer. International Journal of Radia-\\ntion Oncology* Biology* Physics, 110(3):641–655.\\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\\nberg, et al. 2023. Sparks of artificial general intelli-\\ngence: Early experiments with gpt-4. arXiv preprint\\narXiv:2303.12712.\\nXiaoyan Cai, Sen Liu, Junwei Han, Libin Yang, Zhen-\\nguo Liu, and Tianming Liu. 2021. Chestxraybert: A\\npretrained language model for chest radiology report\\nsummarization. IEEE Transactions on Multimedia.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, et al. 2022. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311.\\nWafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea,\\nand Hoda K Mohamed. 2021. Automatic text sum-\\nmarization: A comprehensive survey. Expert systems\\nwith applications, 165:113679.\\nHugging Face.\\nOpen LLM Leaderboard - a Hug-\\nging Face Space by HuggingFaceH4 — hug-\\ngingface.co.\\nhttps://huggingface.co/spaces/\\nHuggingFaceH4/open_llm_leaderboard.\\nJoshua C Feblowitz, Adam Wright, Hardeep Singh,\\nLipika Samal, and Dean F Sittig. 2011. Summa-\\nrization of clinical information: a conceptual model.\\nJournal of biomedical informatics, 44(4):688–699.\\nWilliam R Hendee, Howard I Amols, and Colin G Or-\\nton. 2007. The abr written and oral examinations\\nin medical physics as currently conducted are suf-\\nficiently comprehensive and demanding to ensure\\nthat successful candidates have adequate knowledge\\nand experience to practice in the designated specialty\\nfield. Medical physics, 34(9):3417–3419.\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nand Weizhu Chen. 2021.\\nLora: Low-rank adap-\\ntation of large language models.\\narXiv preprint\\narXiv:2106.09685.\\nChenbin Liu, Terence T Sio, Wei Deng, Jie Shan,\\nThomas B Daniels, William G Rule, Pedro R Lara,\\nShawn M Korte, Jiajian Shen, Xiaoning Ding, et al.\\n2018. Small-spot intensity-modulated proton therapy\\nand volumetric-modulated arc therapies for patients\\nwith locally advanced non-small-cell lung cancer: a\\ndosimetric comparative study. Journal of applied\\nclinical medical physics, 19(6):140–148.\\nWei Liu, Xiaodong Zhang, Yupeng Li, and Radhe Mo-\\nhan. 2012. Robust optimization of intensity modu-\\nlated proton therapy. Medical physics, 39(2):1079–\\n1091.\\nYiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,\\nYuanyuan Yang, Jiaming Tian, Hao He, Antong Li,\\nMengshen He, Zhengliang Liu, et al. 2023a. Sum-\\nmary of chatgpt/gpt-4 research and perspective to-\\nwards the future of large language models. arXiv\\npreprint arXiv:2304.01852.\\nZhengliang Liu, Xiaowei Yu, Lu Zhang, Zihao Wu,\\nChao Cao, Haixing Dai, Lin Zhao, Wei Liu, Ding-\\ngang Shen, Quanzheng Li, et al. 2023b. Deid-gpt:\\nZero-shot medical text de-identification by gpt-4.\\narXiv preprint arXiv:2303.11032.\\nHiba Omer. 2021. Radiobiological effects and medical\\napplications of non-ionizing radiation. Saudi Journal\\nof Biological Sciences, 28(10):5585–5592.\\nOpenAI. Introducing ChatGPT — openai.com. https:\\n//openai.com/blog/chatgpt.\\nOpenAI. 2023. Gpt-4 technical report. arXiv.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022.\\nTraining language models to follow in-\\nstructions with human feedback.\\narXiv preprint\\narXiv:2203.02155.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. Bleu: a method for automatic evalu-\\nation of machine translation. In Proceedings of the\\n40th annual meeting of the Association for Computa-\\ntional Linguistics, pages 311–318.\\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\\nley, and Jianfeng Gao. 2023. Instruction tuning with\\ngpt-4. arXiv preprint arXiv:2304.03277.\\n9\\nSaed Rezayi, Haixing Dai, Zhengliang Liu, Zihao Wu,\\nAkarsh Hebbar, Andrew H Burns, Lin Zhao, Dajiang\\nZhu, Quanzheng Li, Wei Liu, et al. 2022. Clinical-\\nradiobert: Knowledge-infused few shot learning for\\nclinical notes named entity recognition. In Machine\\nLearning in Medical Imaging: 13th International\\nWorkshop, MLMI 2022, Held in Conjunction with\\nMICCAI 2022, Singapore, September 18, 2022, Pro-\\nceedings, pages 269–278. Springer.\\nSteven E Schild, William G Rule, Jonathan B Ashman,\\nSujay A Vora, Sameer Keole, Aman Anand, Wei\\nLiu, and Martin Bues. 2014. Proton beam therapy\\nfor locally advanced lung cancer: A review. World\\njournal of clinical oncology, 5(4):568.\\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\\net al. 2022. Large language models encode clinical\\nknowledge. arXiv preprint arXiv:2212.13138.\\nPaige A Taylor, Elizabeth Miles, Lone Hoffmann,\\nSarah M Kelly, Stephen F Kry, Ditte Sloth Møller,\\nHugo Palmans, Kamal Akbarov, Marianne C Aznar,\\nEnrico Clementel, et al. 2023. Prioritizing clinical\\ntrial quality assurance for photons and protons: A\\nfailure modes and effects analysis (fmea) compari-\\nson. Radiotherapy and Oncology, 182:109494.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro,\\nFaisal Azhar, et al. 2023. Llama: Open and effi-\\ncient foundation language models. arXiv preprint\\narXiv:2302.13971.\\nJan Unkelbach, Markus Alber, Mark Bangert, Rasmus\\nBokrantz, Timothy CY Chan, Joseph O Deasy, Albin\\nFredriksson, Bram L Gorissen, Marcel Van Herk,\\nWei Liu, et al. 2018. Robust radiotherapy planning.\\nPhysics in Medicine & Biology, 63(22):22TR02.\\nRong Wang, Tingyang Zhou, Wei Liu, and Li Zuo.\\n2018. Molecular mechanism of bystander effects\\nand related abscopal/cohort effects in cancer therapy.\\nOncotarget, 9(26):18637.\\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\\nguage models are zero-shot learners. arXiv preprint\\narXiv:2109.01652.\\nLin Zhao, Lu Zhang, Zihao Wu, Yuzhong Chen, Haixing\\nDai, Xiaowei Yu, Zhengliang Liu, Tuo Zhang, Xintao\\nHu, Xi Jiang, et al. 2023. When brain-inspired ai\\nmeets agi. arXiv preprint arXiv:2303.15935.\\n10\\n'},\n",
       " {'abstract': \"Communication acts as a powerful tool in harmonizing the behaviors of multiple agents. However, current methods primarily emphasize broadcast communication, lacking practicality, and leading to information redundancy. This information overload negatively impacts communication efficiency. Meanwhile, commonly used methods rely on fundamental mechanisms to integrate observed and obtained information, hindering the learning process. The Targeted and Trusted Multi-Agent Communication (T2MAC) approach resolves these issues by integrating selective engagement, evidence-driven integration, and effective message composition. Using T2MAC, agents craft personalized messages, identify opportune communication windows, and engage with dependable partners, enhancing communication efficiency. The messages obtained are integrated at the evidence level, leveraging available perspectives collectively. Extensive experiments on cooperative multi-agent tasks demonstrate T2MAC's superiority in cooperative performance and communication efficiency compared to state-of-the-art techniques. Additionally, T2MAC exhibits impressive generalization capabilities.\",\n",
       "  'introduction': 'Reinforcement Learning (RL) has achieved notable advancements in complex real-world scenarios, including Game AI, Robotics, and Autonomous Driving. Conversely, cooperative multi-agent settings present distinct challenges due to the partial observability faced by agents, restricted to their local observations without a comprehensive view of the environment. Moreover, Multi-Agent Reinforcement Learning (MARL) grapples with the non-stationarity of the environment, introducing further complexities during learning. Multi-agent communication offers a compelling solution, allowing agents to extract deeper insights from collective perspectives, facilitating stable learning and harmonized actions. Past methods in this domain predominantly focused on message content and timing but fell short in addressing broadcast communication inefficiencies and the treatment of information integration as a black box. Addressing these shortcomings, we propose the T2MAC framework.',\n",
       "  'literature review': 'Several key areas of research related to MARL and multi-agent communication have been explored in the literature:\\n\\n- Deciding What to Communicate: Prior studies have delved into the formulation of dynamic and continuous messages, enabling real-time adaptation to environmental changes. Researchers have optimized message learning processes and tailored messages for specific agents.\\n\\n- Deciding When and With Whom to Communicate: The timing and partner selection for communication have garnered significant attention. Novel strategies like weight-based schedulers prioritize agents with crucial observations, while other approaches harness methods such as causal inference, graph-attention, and Shapley message value to identify ideal communication recipients.\\n\\n- Incorporating Messages for Cooperative Decision-Making: Integrating incoming messages into decision-making processes has also been a focus of research. Prominent approaches employ representation learning paradigms to discern message significance, enabling selective assimilation. However, existing methodologies have yet to simultaneously address targeted and trusted communication.',\n",
       "  'methodology': \"At the core of our approach is the T2MAC framework, characterized by four distinctive aspects:\\n\\n1. Policy:\\n   - T2MAC's policy is modeled as a Dirichlet distribution, facilitating the integration of evidence from diverse sources for informed and reliable decisions.\\n   - The evidence encoder serves a dual purpose: extracting evidence for its decisions and crafting tailored messages for teammates, aiding their decision-making.\\n\\n2. Selective Engagement:\\n   - To ensure efficient communication, T2MAC pinpoints optimal moments and counterparts for communication, ensuring the dissemination of only the most pertinent and reliable data.\\n   - This is achieved through an ablative decision-making analysis to quantify the strength and relevance of each communication link between agents.\\n   - A communication selector network is developed to enable agents to determine the right moments and partners for communication.\\n\\n3. Evidence-Driven Integration:\\n   - T2MAC incorporates the theory of evidence into multi-agent communication.\\n   - Evidence refers to metrics extracted from observations that support decision-making processes.\\n   - To capture uncertainties inherent in decision-making, the Dirichlet distribution and Subjective Logic (SL) are employed to deduce the concentration parameters and belief and uncertainty masses associated with each action.\\n   - Evidence collected by different agents is integrated using Dempster-Shafer theory of evidence, enabling the combination of evidence from multiple sources to derive a comprehensive belief and uncertainty.\",\n",
       "  'results': \"1. Performance:\\n   - T2MAC exhibited superior performance across various environments, including Hallway, MPE, and SMAC, consistently outperforming state-of-the-art communication methods.\\n   - T2MAC demonstrated resilience under challenging conditions, maintaining strong performance even in complex scenarios with numerous agents and long Markov chains.\\n   - Non-communication baselines consistently underperformed, highlighting the importance of proficient communication in these contexts.\\n\\n2. Efficiency:\\n   - T2MAC achieved significant communication efficiency gains compared to other methods, delivering substantial performance improvements while minimizing communication frequency.\\n   - The calculated communication efficiency metric underscores T2MAC's adeptness in managing communication dynamics, optimizing when to communicate, with whom, and how to balance performance and efficiency.\\n\\n3. Generality:\\n   - T2MAC demonstrated strong generalization capabilities across a wide range of established MARL baselines, including QMIX, DOP, and MAPPO.\\n   - It consistently achieved superior performance across these baselines, further highlighting its broad applicability and effectiveness in MARL.\",\n",
       "  'conclusion': 'T2MAC effectively addresses the challenges inherent in multi-agent communication. Traditional approaches rely on broadcast communication and treat information fusion as a black box, resulting in diminished communication efficiency. To overcome these limitations, T2MAC empowers agents with the capability to compose messages tailored for distinct agents, strategically selecting communication timings, and relying on trusted partners. Moreover, it integrates incoming messages efficiently, facilitating trusted decision-making. Rooted in solid theoretical principles, T2MAC demonstrates its effectiveness through comprehensive experiments across multiple benchmarks, showcasing its superiority in efficiency, adaptability, and overall performance.',\n",
       "  'title': 'T2MAC: Targeted and Trusted Multi-Agent Communication through Selective Engagement and Evidence-Driven Integration',\n",
       "  'author': 'Chuxiong Sun, Zehua Zang, Jiabao Li, Jiangmeng Li, Xiao Xu, Rui Wang, Changwen Zheng',\n",
       "  'textdata': 'T2MAC: Targeted and Trusted Multi-Agent Communication through Selective\\nEngagement and Evidence-Driven Integration\\nChuxiong Sun1 2*, Zehua Zang1 3*, Jiabao Li4*, Jiangmeng Li1 2†, Xiao Xu2, Rui Wang1 2 3,\\nChangwen Zheng1 3\\n1Science & Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences\\n2State Key Laboratory of Intelligent Game\\n3University of Chinese Academy of Sciences\\n4School of Automation and Electrical Engineering, University of Science and Technology Beijing\\n{chuxiong2016,zehua2020}@iscas.ac.cn, M202110548@xs.ustb.edu.cn, jiangmeng2019@iscas.ac.cn,\\nxuxiao0825@gmail.com, {wangrui,changwen}@iscas.ac.cn,\\nAbstract\\nCommunication stands as a potent mechanism to harmonize\\nthe behaviors of multiple agents. However, existing works\\nprimarily concentrate on broadcast communication, which\\nnot only lacks practicality, but also leads to information re-\\ndundancy. This surplus, one-fits-all information could ad-\\nversely impact the communication efficiency. Furthermore,\\nexisting works often resort to basic mechanisms to integrate\\nobserved and received information, impairing the learning\\nprocess. To tackle these difficulties, we propose Targeted and\\nTrusted Multi-Agent Communication (T2MAC), a straight-\\nforward yet effective method that enables agents to learn\\nselective engagement and evidence-driven integration. With\\nT2MAC, agents have the capability to craft individualized\\nmessages, pinpoint ideal communication windows, and en-\\ngage with reliable partners, thereby refining communication\\nefficiency. Following the reception of messages, the agents\\nintegrate information observed and received from different\\nsources at an evidence level. This process enables agents to\\ncollectively use evidence garnered from multiple perspec-\\ntives, fostering trusted and cooperative behaviors. We eval-\\nuate our method on a diverse set of cooperative multi-agent\\ntasks, with varying difficulties, involving different scales and\\nranging from Hallway, MPE to SMAC. The experiments in-\\ndicate that the proposed model not only surpasses the state-\\nof-the-art methods in terms of cooperative performance and\\ncommunication efficiency, but also exhibits impressive gen-\\neralization.\\nIntroduction\\nReinforcement Learning (RL) has achieved remarkable\\nmilestones in a myriad of intricate real-world domains, rang-\\ning from Game AI (Osband et al. 2016; Silver et al. 2017,\\n2018; Vinyals et al. 2019) and Robotics (Andrychowicz\\net al. 2020) to Autonomous Driving (Dosovitskiy et al.\\n2017). However, when delving into cooperative multi-agent\\nsettings, distinct challenges surface. The issue of partial ob-\\nservability stands out, where agents are confined to their lo-\\n*These authors contributed equally.\\n†Corresponding author.\\nCopyright © 2024, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.\\ncal observations, missing out on the broader perspective of\\nthe entire environment. Complicating matters further, Multi-\\nAgent Reinforcement Learning (MARL) grapples with the\\nnon-stationarity of the environment. From an individual\\nagent’s perspective, the environmental dynamics shift inces-\\nsantly, adding another layer of complexity to the learning\\nprocess.\\nMulti-agent communication offers a compelling solution\\nto the issues outlined by granting agents the capability to\\nderive a deeper understanding of their surroundings through\\ncollective insights. This approach ensures stable learning\\nand encourages harmonized actions among agents. How-\\never, historical methods have focused on the content and\\ntiming of communication (Sukhbaatar, Szlam, and Fergus\\n2016; Singh, Jain, and Sukhbaatar 2018; Kim et al. 2019;\\nWang et al. 2020b; Zhang, Zhang, and Lin 2020; Yuan et al.\\n2022). Once an agent elects to share its message, it is broad-\\ncast to the entire agent group. This indiscriminate broadcast-\\ning is not only resource-intensive but also potentially ineffi-\\ncient.\\nA pivotal realization is that only some agents carry valu-\\nable insights, and flooding the network with redundant in-\\nformation can be counterproductive to learning. Interest-\\ningly, humans know when to communicate intrinsically,\\nwith whom, and how to customize their messages to the re-\\ncipient. Mirroring these human instincts could significantly\\nrefine the information exchange process, allowing agents to\\ncurate their messages and recipients selectively.\\nMoreover, the essence of messages relayed by agents\\nis a distillation of their individual observational expe-\\nriences. Assimilating these messages aptly can enrich\\nagents’ perception of an uncertain environment, leading\\nto more refined policies. Regrettably, the existing tech-\\nniques—whether they’re steeped in basic aggregation (Jiang\\nand Lu 2018) or are more avant-garde with representation\\nlearning (Das et al. 2019; Guan et al. 2022) tend to treat the\\nfusion of information as a black box, presupposing that the\\npolicy networks can innately sift out vital data and diminish\\ndecision-making uncertainty. In this context, the information\\nintegration process might prove to be both uncertain and in-\\nefficient, especially in intricate scenarios. As such, there’s\\narXiv:2401.10973v1  [cs.MA]  19 Jan 2024\\na pressing need for a novel and theory-grounded approach\\nthat can adeptly merge messages while tackling the inherent\\nunderlying uncertainties.\\nWith this vision, we introduce the Targeted and Trusted\\nMulti-Agent Communication (T2MAC) framework, which\\nembodies the principles of discerning and streamlined com-\\nmunication, drawing inspiration from human inclinations to\\nengage selectively with trusted and relevant counterparts,\\nensuring more efficient information integration, and foster-\\ning a more adaptive multi-agent collaboration in dynamic\\nenvironments. Specifically, each agent is skilled at analyz-\\ning observations to extract evidence. In this context, evi-\\ndence denotes metrics instrumental in guiding the decision-\\nmaking processes. This evidence plays a dual role: guid-\\ning local decision-making and serving as the basis for craft-\\ning messages that are meticulously tailored to specific agent\\ncontexts. Moreover, we evaluate the variations in uncer-\\ntainty prior to and post-communication to measure the im-\\npact and significance of specific communication behavior.\\nArmed with these insights, we craft binary pseudo-labels\\nbased on the significance of communication and devise an\\nauxiliary task. This task is specifically designed to train a\\ncommunication selector network, empowering it to iden-\\ntify the ideal communication counterparts. By adopting this\\nstrategy, we guarantee that only the most relevant and credi-\\nble data is exchanged among the agents. Upon receipt, mes-\\nsages are integrated at the evidence level rather than the con-\\nventional observation or feature level. To capture the intri-\\ncacies of decision-making, we leverage the Dirichlet distri-\\nbution. This allows us to model decision policies, anchor-\\ning them on evidence that’s been sourced from a myriad\\nof perspectives. Concretely, we integrate Subjective Logic\\n(SL) (Jsang 2018) to link the Dirichlet parameters with be-\\nlief and uncertainty, therefore quantifying the uncertainty\\nfor decision-making and jointly modeling the probability\\nof each action. Then, we utilize Dempster-Shafer theory of\\nevidence (DST) (Dempster 1967) to integrate evidence ob-\\nserved from multiple agents, producing a comprehensive be-\\nlief and uncertainty that considers all available evidence, en-\\nsuring trusted message integration and decision-making. We\\nsubjected T2MAC to rigorous testing across various MARL\\nenvironments, such as Hallway, MPE, and SMAC. Com-\\npared to prominent multi-agent communication strategies\\nlike TarMac (Das et al. 2019), MAIC (Yuan et al. 2022),\\nSMS (Xue et al. 2022), and MASIA (Guan et al. 2022),\\nT2MAC consistently excelled in both performance and effi-\\nciency. Additionally, its versatility shone through across di-\\nverse scenarios.\\nRelated Works\\nMARL has undergone remarkable progression in recent\\nepochs (Lowe et al. 2017; Sunehag et al. 2017; Rashid et al.\\n2018; Yu et al. 2022). Within the MARL ambit, multi-agent\\ncommunication has emerged as an indispensable aspect, par-\\nticularly salient for cooperative endeavors constrained by\\npartial observability. Research in this domain can be broadly\\nsegmented into three main categories.\\nDeciding What to Communicate. Historically, communi-\\ncation vocabularies are set in stone during training, as il-\\nlustrated by (Foerster et al. 2016). This seemingly efficient\\nstrategy unintentionally limits the depth and flexibility of\\nagent communication. In response, CommNet (Sukhbaatar,\\nSzlam, and Fergus 2016) introduces a paradigm shift by\\nallowing agents to create dynamic, continuous messages.\\nWith its design for continuous interactions, CommNet en-\\nsures that messages are timely and sensitive to environmen-\\ntal changes. Building on this foundation, both VBC (Zhang,\\nZhang, and Lin 2019) and TMC (Zhang, Zhang, and Lin\\n2020) further optimize message learning processes. Further-\\nmore, NDQ (Wang et al. 2020b) and MAIC (Yuan et al.\\n2022) are designed to craft messages tailored for individual\\nagents.\\nDeciding When and With Whom to Communicate. Effec-\\ntive communication timing and partner selection are pivotal\\nin Multi-Agent Communication. A gating network show-\\ncased in (Singh, Jain, and Sukhbaatar 2018; Jiang and Lu\\n2018) generates binary decisions, allowing agents the free-\\ndom to communicate or abstain. Advancing this idea, (Kim\\net al. 2019; Mao et al. 2019; Wang et al. 2020a; Sun\\net al. 2021) implement a weight-based scheduler, priori-\\ntizing agents holding vital observations. Enriching this ap-\\nproach, I2C (Ding, Huang, and Lu 2020), MAGIC (Niu,\\nPaleja, and Gombolay 2021), and SMS (Xue et al. 2022)\\nharness methods like causal inference, graph-attention, and\\nShapley message value to pinpoint ideal communication re-\\ncipients.\\nIncorporating\\nMessages\\nfor\\nCooperative\\nDecision-\\nMaking. A prominent subset of the MARL methodologies\\nposits an egalitarian weightage to all incoming messages.\\nSuch an approach fails to recognize the significance of filter-\\ning vital information from a sea of communications. There-\\nfore, we introduce representation learning paradigms to ad-\\ndress this lacuna for discerning message assimilation. For\\ninstance, TarMac (Das et al. 2019) adopts soft attention\\nmechanisms to weight messages, while MASIA (Guan et al.\\n2022) consolidates received messages into concise represen-\\ntations using an autoencoder.\\nTo our knowledge, no existing MARL method simul-\\ntaneously addresses targeted and trusted communication.\\nT2MAC stands as the pioneering approach, enabling agents\\nto efficiently select communication partners and distill tai-\\nlored evidence and integrate messages, resulting in trustwor-\\nthy cooperative decisions.\\nBackground\\nIn this study, we concentrate on fully cooperative multi-\\nagent reinforcement learning tasks characterized by par-\\ntial observability while also allowing inter-agent com-\\nmunication. These tasks are an evolved form of De-\\ncentralized Partially Observable Markov Decision Pro-\\ncesses (Dec-POMDPs). Their framework uses the tuple\\n𝐺 = (𝑁, 𝑆, 𝑂, 𝐴, O, 𝑃, 𝑅, 𝛾, 𝑀). In this formulation: 𝑁 =\\n(𝑎𝑔𝑒𝑛𝑡1, ..., 𝑎𝑔𝑒𝑛𝑡𝑛) depicts the collective of agents. 𝑆 en-\\ncompasses global states, offering a comprehensive environ-\\nmental overview. 𝑂 refers to the accessible local observa-\\ntions. 𝐴 signifies a set of available actions. O refers to the\\nobservation function, which describes how agents perceive\\nPolicy\\nCommunication\\n𝑜𝑖\\n(a) Agent 𝒊\\nℎ𝑖\\n𝑒⋆𝑖\\nEvidence Encoder\\n𝑒𝑖\\n𝑒𝑖𝑗 𝑒𝑖𝑘 𝑒𝑖𝑙\\n𝑚⋆𝑖\\n𝑚𝑖⋆\\n𝜋𝑖\\n𝑒𝑗𝑖\\n𝑒𝑘𝑖\\n𝑒𝑖\\n𝛼𝑖\\n𝜋𝑖\\nWeighted Sum\\nDST Combination\\n(b) Policy\\n(e) Selective Engagement\\nObservation Encoder\\nℎ𝑖\\nℎ𝑖\\nSelector Network\\n𝑒𝑖⋆\\n𝑒𝑖𝑗\\n𝑏𝑖\\n𝑢𝑖\\n𝑔𝑖𝑗: 1\\n𝑔𝑖𝑘: 0\\n𝑔𝑖𝑙: 1\\n𝑒𝑖𝑘\\n𝑒𝑖𝑙\\nDot Product\\nAgent 𝒊\\nAgent 𝒋\\nAgent 𝒌\\n𝑚𝑗𝑖\\n𝑚𝑘𝑖\\nAgent 𝒍\\n(d) Receiving\\nAgent 𝒊\\nAgent 𝒋\\nAgent 𝒍\\nAgent 𝒌\\n𝑚𝑖𝑗\\n𝑚𝑖𝑙\\n(f) Sending\\n𝑚⋆𝑖\\n𝑚𝑖⋆\\nEvidence-Driven \\nIntegration\\nSelective \\nEngagement\\n𝛼𝑖\\n(c) Evidence-Driven \\nIntegration\\n𝑚𝑖𝑗\\n𝑚𝑖𝑘\\n𝑚𝑖𝑙\\nFigure 1: Framework of T2MAC.\\nthe environment based on the global state. 𝑃 acts as the tran-\\nsition function, illustrating environmental dynamics. 𝑅 is a\\nreward function contingent on global states and joint actions.\\n𝛾 represents the discount factor, 𝑀 delineates the set of com-\\nmunicable messages.\\nAt each time step, agents access only local observa-\\ntions, which are derived from the global state through the\\nobservation function, O(𝑜𝑡\\n𝑖 |𝑠). Simultaneously, agents are\\nequipped with the capability to share messages, denoted as\\n𝑚𝑡\\n𝑖. These messages might encompass observations, inten-\\ntions, or past experiences. Crucially, each agent can judi-\\nciously decide when to communicate, streamlining the effi-\\nciency of the communication process. As messages are re-\\nceived, agents integrate their incoming information, leading\\nto the aggregated message 𝑐𝑡\\n𝑖 = Í\\n𝑗≠𝑖 𝑚𝑡\\n𝑗. This composite\\ndata then guides their localized decision-making, encapsu-\\nlated by 𝑎𝑡\\n𝑖 = 𝜋(𝑜𝑡\\n𝑖, 𝑐𝑡\\n𝑖). Following this, the environment re-\\nacts to the joint action, 𝑎 = (𝑎𝑡\\n1, ..., 𝑎𝑡\\n𝑛), transitioning to the\\nsubsequent state 𝑠\\n′. Simultaneously, this joint action then\\nyields a shared team reward, 𝑟 = 𝑅(𝑠, 𝑎). The overarch-\\ning goal is to pinpoint an optimal joint policy geared to-\\nwards maximizing the expected cumulative team reward, ex-\\npressed as E𝑠,𝑎[Í∞\\n𝑡=0 𝛾𝑡𝑟].\\nMethodology\\nAs depicted in Fig. 1, the distinctive characteristics of\\nT2MAC can be highlighted in these four aspects:\\n• T2MAC’s policy is characterized as a Dirichlet distribu-\\ntion, facilitating the assimilation of evidence from vari-\\nous sources for informed and trusted decisions.\\n• The evidence encoder serves a dual purpose: extracting\\nevidence for its own decisions and crafting tailored mes-\\nsages for specific teammates.\\n• Through the selective engagement, T2MAC can pinpoint\\noptimal moments and counterparts for communication,\\nensuring the dissemination of only the most pertinent and\\nreliable data.\\n• The evidence-driven integration combines incoming\\nmessages at an evidence level, refraining from treating\\nthe fusion process as a black box.\\nIn the following sections, we will illustrate the key compo-\\nnents of T2MAC in detail.\\nTheory of Evidence\\nFor communication to be precise and reliable, it’s essen-\\ntial to factor in the uncertainties intrinsic to individual de-\\ncisions. To this effect, we have incorporated the theory of\\nevidence into multi-agent communication. Within this con-\\ntext, evidence pertains to metrics sourced from observations\\nsupporting decision-making processes. To get a grasp on this\\nevidence and uncertainty, we employ the Dirichlet distribu-\\ntion, which has proven efficacious in mitigating the overcon-\\nfidence issue (Sensoy, Kaplan, and Kandemir 2018; Malinin,\\nMlodozeniec, and Gales 2020; Malinin and Gales 2018).\\nThis distribution is characterized by its concentration param-\\neters, represented as 𝛼 = [𝛼1, ..., 𝛼𝐾] where 𝐾 is the number\\nof actions. These parameters share an intimate relationship\\nwith uncertainty. Building on this, we harness SL to discern\\nthe concentration parameters. SL offers a theoretical frame-\\nwork for extracting the probabilities (belief masses) of dis-\\nparate actions and the overarching uncertainty (uncertainty\\nmass) tied to policy-prediction challenges. Delving deeper\\ninto decision-making quandaries, SL seeks to allocate a be-\\nlief mass to each action while assigning an overarching un-\\ncertainty mass to the entire scenario based on observed ev-\\nidence. Consequently, all mass values remain non-negative\\nand their cumulative value equals one:\\n𝑢𝑖 +\\n𝐾\\n∑︁\\n𝑘=1\\n𝑏𝑘\\n𝑖 = 1\\n(1)\\nwhere 𝑢𝑖 ≥ 0 signifies the overall uncertainty for 𝑎𝑔𝑒𝑛𝑡𝑖,\\n𝑏𝑘\\n𝑖 ≥ 0 denotes the belief of 𝑎𝑔𝑒𝑛𝑡𝑖 associated with the 𝑘𝑡ℎ\\naction.\\nMoreover, SL elegantly bridges the evidence observed\\nby 𝑎𝑔𝑒𝑛𝑡𝑖, denoted as 𝑒𝑖\\n=\\n[𝑒1\\n𝑖 , , ..., 𝑒𝐾\\n𝑖 ], with the pa-\\nrameters constituting the Dirichlet distribution for 𝑎𝑔𝑒𝑛𝑡𝑖,\\n𝛼𝑖 = [𝛼1\\n𝑖 , , ..., 𝛼𝐾\\n𝑖 ]. Here, by employing ReLU in the final\\nlayer, all evidence values are ensured to be non-negative.\\nThe parameter 𝛼𝑘\\n𝑖 is directly influenced by 𝑒𝑘\\n𝑖 , specifically,\\n𝛼𝑘\\n𝑖 = 𝑒𝑘\\n𝑖 + 1. Subsequently, the belief mass 𝑏𝑘\\n𝑖 and the over-\\narching uncertainty 𝑢𝑖 can be deduced as:\\n𝑏𝑘\\n𝑖 = 𝑒𝑘\\n𝑖\\n𝑆𝑖\\n= 𝛼𝑘\\n𝑖 − 1\\n𝑆𝑖\\n𝑎𝑛𝑑 𝑢𝑖 = 𝐾\\n𝑆𝑖\\n(2)\\nwhere 𝑆𝑖 = Í𝐾\\n𝑘=1(𝑒𝑘\\n𝑖 + 1) = Í𝐾\\n𝑘=1 𝛼𝑘\\n𝑖 represents the strength\\nof the Dirichlet distribution (Jsang 2018). Eq. 2 captures an\\nintuitive phenomenon: the more evidence accumulated for\\nthe 𝑘𝑡ℎ action, the higher the probability attributed to that\\naction. Inversely, when there’s scant evidence, the encom-\\npassing uncertainty escalates. This belief assignment can be\\ninterpreted as a form of subjective reasoning.\\nTo enhance decision-making precision and trustworthi-\\nness, we propose leveraging evidence collected by different\\nagents as a foundation for decision-making. Consequently,\\nwe develop an evidence encoder to deduce bespoke evidence\\ntailored for each agent. At each time-step, 𝑎𝑔𝑒𝑛𝑡𝑖 not only\\nproduces evidence 𝑒𝑖 for its own local decision but also ex-\\ntracts a collection of evidence - (𝑒𝑖1, ..., 𝑒𝑖 𝑗, ..., 𝑒𝑖𝑛), aimed\\nat aiding its teammates in making more reliable choices.\\nSuch evidence then acts as the communication medium, en-\\nabling us to generate messages tailored for specific agents.\\nThe tailored message from 𝑎𝑔𝑒𝑛𝑡𝑖 to 𝑎𝑔𝑒𝑛𝑡 𝑗 can be denoted\\nas 𝑚𝑖 𝑗 = 𝑒𝑖 𝑗.\\nSelective Engagement\\nAs we’ve discussed earlier, broadcast communication falls\\nshort in practical applications and results in redundant infor-\\nmation. The timing of information exchange and the choice\\nof communication partners are paramount. For precise and\\ntrustworthy message exchanges, it’s vital to identify truly in-\\nstrumental connections from the vast web of interactions. At\\na holistic level, we aim to share evidence-backed data, thus\\nenabling recipients to make informed and reliable decisions.\\nTo bring this vision to fruition, we meticulously quantify the\\nstrength and relevance of each communication link between\\nagents by performing an ablative decision-making analy-\\nsis. This approach primarily seeks to quantify the variabil-\\nity in decision uncertainty attributable to communication. To\\ndelve deeper into the mechanics, consider the communica-\\ntion from 𝑎𝑔𝑒𝑛𝑡𝑖 to 𝑎𝑔𝑒𝑛𝑡 𝑗, denoted as 𝑚𝑖 𝑗. This communi-\\ncation’s value is mathematically expressed as:\\n𝑣𝑖 𝑗 = 𝑢 𝑗 − ˆ𝑢 𝑗\\n(3)\\nwhere 𝑢 𝑗 represents the decision uncertainty for recipient\\n𝑎𝑔𝑒𝑛𝑡 𝑗 before communication, whereas ˆ𝑢 𝑗 is the uncertainty\\npost communication.\\nTo foster targeted and trusted communication, we develop\\na communication selector network. This network aids agents\\nin determining the right moments and partners for communi-\\ncation, ensuring that only the most valuable and credible in-\\nformation is shared. We also set a constant threshold to gen-\\nerate binary pseudo-labels. If the deduced communication\\nvalue is below the set threshold, it implies that the message\\nreceived doesn’t substantially benefit the recipient agent,\\nleading the connection to be tagged as ‘cut’, denoted math-\\nematically as 𝑦𝑖 𝑗 = 0. However, if the communication value\\nexceeds the threshold, it signifies the message’s importance,\\nprompting its tag to be ’retain’ with 𝑦𝑖 𝑗 = 1. This systematic\\nlabeling forms the foundation for optimizing the communi-\\ncation selector network, with the binary cross-entropy loss\\nsteering the fine-tuning process.\\nL𝐵𝐶𝐸 = E𝑖, 𝑗∼Z𝑛 [𝑦𝑖 𝑗×log(𝑝𝑖 𝑗)+(1−𝑦𝑖 𝑗)×log(1−𝑝𝑖 𝑗)] (4)\\nwhere Z𝑛 is the set of integers from 1 to 𝑛, 𝑝𝑖 𝑗 is the out-\\nput of the communication selector network, representing the\\nlikelihood of 𝑎𝑔𝑒𝑛𝑡𝑖 choosing to communicate with 𝑎𝑔𝑒𝑛𝑡 𝑗.\\nEvidence-Driven Integration\\nIn T2MAC, messages exchanged among agents encapsulate\\nevidence observed from diverse perspectives. Agents can\\nbetter understand the uncertain environment by adeptly inte-\\ngrating these messages, resulting in more sophisticated poli-\\ncies. To this end, we incorporate the DST to integrate in-\\ncoming messages. This approach facilitates the combination\\nof evidence from different sources, culminating in a degree\\nof belief that comprehensively reflects all gathered evidence.\\nThe rule of message integration for evidence is presented as:\\nM = M𝑖 ⊕ M 𝑗\\n(5)\\nwhere M𝑖 = {{𝑏𝑘\\n𝑖 }𝐾\\n𝑘=1, 𝑢𝑖} and M 𝑗 = {{𝑏𝑘\\n𝑗 }𝐾\\n𝑘=1, 𝑢 𝑗} sym-\\nbolize the joint masses derived from two distinct perspec-\\ntives of evidence and ⊕ represents DST combination. Mean-\\nwhile, M = {{𝑏𝑘}𝐾\\n𝑘=1, 𝑢} encapsulates the consolidated\\njoint mass, integrating evidence from both standpoints. The\\nmore specific integration rule can be formulated as follows:\\n𝑏𝑘 =\\n1\\n1 − 𝐶 (𝑏𝑘\\n𝑖 𝑏𝑘\\n𝑗 + 𝑏𝑘\\n𝑖 𝑢 𝑗 + 𝑏𝑘\\n𝑗𝑢 𝑗), 𝑢 =\\n1\\n1 − 𝐶 𝑢𝑖𝑢 𝑗\\n(6)\\nwhere 𝐶 = Í\\n𝑘≠𝑘′ 𝑏𝑘\\n𝑖 𝑏𝑘\\n′\\n𝑗 represents the degree of disagree-\\nment between the two sets of mass values. To account for\\nthis discord, DST employs the normalization factor\\n1\\n1−𝐶 to\\nensure a coherent integration of the evidence from both sets.\\nIntuitively, when encountering evidence and beliefs from\\nmultiple sources, DST aims to merge the common elements\\nand sidesteps conflicting beliefs through normalization fac-\\ntors. The integration rule ensures:\\n1. If both perspectives exhibit high uncertainty (with signif-\\nicant values of 𝑢𝑖 and 𝑢 𝑗), the resultant prediction should\\nbe treated cautiously, yielding a lower confidence level\\n(represented by a smaller value of 𝑏𝑘).\\n2. Conversely, if both viewpoints possess low uncertainty\\n(denoted by minimal values of 𝑢𝑖 and 𝑢 𝑗), the resulting\\nprediction is likely to be made with a high degree of con-\\nfidence (manifesting as a larger value of 𝑏𝑘);\\n3. In situations where only one viewpoint exhibits low un-\\ncertainty (meaning either 𝑢𝑖 or 𝑢 𝑗 is significantly large),\\nthe final prediction predominantly relies on the more con-\\nfident viewpoint.\\nUpon receiving distinct messages from other agents, we\\nderive the aforementioned mass for each perspective. Sub-\\nsequently, leveraging Dempster’s rule of combination, we\\ncan integrate the beliefs stemming from these varied view-\\npoints. More specifically, the fusion of belief and uncertainty\\nmasses across different messages is governed by the subse-\\nquent rule:\\nM = M1 ⊕ M2 ⊕ ...M𝑛\\n(7)\\nOnce we have determined the joint mass M = {{𝑏𝑘}𝐾\\n𝑘=1, 𝑢},\\nthe associated joint evidence gleaned from the messages,\\nalong with the parameters of the Dirichlet distribution, can\\nbe derived as follows:\\n𝑆 = 𝐾\\n𝑢 , 𝑒𝑘 = 𝑏𝑘 × 𝑆 𝑎𝑛𝑑 𝛼𝑘 = 𝑒𝑘 + 1\\n(8)\\nLeveraging DST, we attain an efficient and theoretically-\\nfounded method for message integration. This method skill-\\nfully amalgamates messages and simultaneously addresses\\nenduring intrinsic policy uncertainties. Importantly, the fu-\\nsion of information isn’t treated as a black box, given that the\\ncombination rules of DST lack learnable parameters. Fur-\\nthermore, DST offers a more comprehensible and theoretical\\nperspective on the message integration process.\\nFollowing the assimilation of incoming messages and the\\nacquisition of integrated evidence, each agent makes a local\\ndecision influenced by both its observed and received evi-\\ndence. For 𝑎𝑔𝑒𝑛𝑡𝑖, this procedure is represented as:\\n𝑎𝑡\\n𝑖 = 𝜋𝑖( ˆ𝑒𝑖)\\n(9)\\nwhere\\nˆ𝑒𝑖 symbolizes the evidence post-integration for\\n𝑎𝑔𝑒𝑛𝑡𝑖 at time-step 𝑡. For details of the communication pro-\\ncess and the training paradigm of T2MAC, please refer to\\npseudo-code provided in this section.\\nExperiments\\nIn this section, we carefully design experiments to ad-\\ndress three pivotal questions: (1) How does T2MAC’s per-\\nformance measure against top-tier communication meth-\\nods? (2) What characterizes T2MAC’s communication ef-\\nficiency? (3) Can T2MAC scale across various tasks and\\nseamlessly integrate with multiple baselines?\\nSetup\\nAs illustrated in Fig. 2, we extensively evaluate T2MAC\\nacross three notable cooperative multi-agent tasks. Begin-\\nning with Hallway (Wang et al. 2020b), this environment is\\nAlgorithm 1: T2MAC\\nInitialize replay buffer D\\nInitialize the Observation encoder, Evidence encoder, Se-\\nlective Engagement and Q network with random parame-\\nters\\nSet learning rate 𝛼 and max training episode 𝐸\\nfor episode in 1, ..., 𝐸 do\\nfor each agent 𝑖 do\\nSending Phase: Encode the hidden feature ℎ𝑡\\n𝑖 from\\nobservation 𝑜𝑡\\n𝑖\\nEncode evidence 𝑒𝑡\\n𝑖 for local decision\\nEncode evidence and generate tailored messages for\\nspecific teammates (𝑒𝑖1, ..., 𝑒𝑖 𝑗, ..., 𝑒𝑖𝑛)\\nSelect ideal communication partners using commu-\\nnication selector network\\nReceiving Phase: Combing received messages 𝑚𝑡\\n★𝑖\\nfrom other agents by DST combine\\nSelect action 𝑎𝑡\\n𝑖 by combined evidence\\nCompute the importance for each communication\\nlink and generating labels 𝑦𝑖 𝑗 for communication se-\\nlector network\\nend for\\nStore the trajectory in replay buffer D\\nSample a minibatch of trajectories from D\\nUpdate observation encoder, evidence encoder and pol-\\nicy network using MARL loss function\\nUpdate Selective Engagement by Equation 4\\nend for\\nrelatively direct, built around multiple Markov chains. Here,\\nagents start at random positions within different chains and\\naim to reach the goal state simultaneously under partial ob-\\nservability. To escalate the complexity, we augment the num-\\nber of agents and the length of the Markov chains, lead-\\ning to a substantial increase in the exploration space. On\\nthe other hand, MPE (Lowe et al. 2017) is a vital MARL\\nbenchmark set in a 2D grid. We focus on the Cooperative\\nNavigation (CN) and Predator Prey (PP) scenarios. In CN,\\nthe task for agents is to navigate to different landmarks,\\nwhereas, in PP, their objective is to capture unpredictably\\nmoving prey. To introduce varying difficulty levels, we em-\\nploy different grid sizes for both scenarios. The Coopera-\\ntive Navigation: Medium scenario is set on a 7 × 7 grid,\\nwhile the Cooperative Navigation: Hard occupies a 9 × 9\\ngrid. The Predator Prey: Medium scenario is set on a 5 × 5\\ngrid, while the Predator Prey: Hard occupies a 7 × 7 grid.\\nSMAC (Samvelyan et al. 2019) is derived from the well-\\nknown real-time strategy game StarCraft II. It delves into\\nmicromanagement challenges where each unit is steered by\\nan independent agent making decisions under partial observ-\\nability. To emphasize the importance of communication, we\\nadopt the setup from (Wang et al. 2020b), which not only re-\\nstricts the agents’ sight range but also throws them into intri-\\ncate maps, characterized either by their labyrinthine terrains\\nor the unpredictable spawning dynamics of units. For com-\\nparative analysis, we draw from a diverse set of baselines.\\nThis includes non-communication paradigms like the lead-\\n   \\n   \\n   \\n   \\n(a) Hallway\\n(c) SMAC\\n(b) MPE\\nFigure 2: Multiple environments considered in our experiments.\\nModule\\nArchitecture\\nObs Encoder\\nLinear(obs dim, 64)\\nLinear(64, 64)\\nLinear(64, 64)\\nRNN(64, 64)\\nEvidence Encoder\\nn*Linear(64, K)\\nSelector Network\\nLinear(64, n)\\nTable 1: Hyperparameters of T2MAC\\ning MARL methods QMIX (Rashid et al. 2018) and DOP\\n(Wang et al. 2020). Meanwhile, our baselines include con-\\ntemporary state-of-the-art communication methods, such as\\nTarMAC (Das et al. 2019), MAIC (Yuan et al. 2022), SMS\\n(Xue et al. 2022), and MASIA (Guan et al. 2022).\\nIn conclusion, our experimental design integrates a med-\\nley of challenging tasks and robust baselines, establishing a\\nsolid foundation for evaluation. Our overarching goal with\\nthis varied selection is to place T2MAC in diverse scenarios\\nand test its adaptability, scalability, and overall performance.\\nTo ensure transparency and reproducibility, the intricate de-\\ntails of our method’s architecture and our hyperparameter\\nchoices are extensively detailed in Table 1.\\nResults\\nPerformance\\nWe begin our evaluation by comparing the\\nlearning curves of T2MAC with various baselines across\\nvarious environments to test its overarching performance. As\\nillustrated in Fig. 3, T2MAC emerges superior in almost all\\nenvironments, highlighting its robust performance. In Hall-\\nway, as the difficulty intensifies, many baselines falter, un-\\nable to adapt effectively. Among them, only MASIA stands\\nout, delivering commendable results, primarily due to its\\nability to assist agents in reconstructing global information.\\nIntriguingly, our T2MAC works even under such demand-\\ning conditions, achieving performance on par with MASIA.\\nThis might be largely attributed to its adeptness at shar-\\ning and integrating relevant evidence. In SMAC, T2MAC\\ndelivers consistent and impressive performance across all\\nthree maps. However, when looking at all scenarios in their\\nMethods\\nPerformance\\nImprovement\\nComm\\nRate\\nComm\\nEfficiency\\nTarMAC\\n17.0%\\n100.0%\\n17.0%\\nMAIC\\n12.3%\\n100.0%\\n12.3%\\nSMS\\n27.9%\\n66.7%\\n41.8%\\nMASIA\\n30.2%\\n100.0%\\n30.2%\\nT2MAC(Ours)\\n37.2%\\n56.0%\\n66.4%\\nTable 2: Communication Efficiency\\nentirety, other methods exhibit signs of instability. For in-\\nstance, SMS struggles to adapt in the 5𝑧 𝑣𝑠 1𝑢𝑙, while Tar-\\nMAC fails in the 1𝑜 10𝑏 𝑣𝑠 1𝑟. Such observations accentu-\\nate, to some extent, the broad applicability and robustness\\ninherent to T2MAC. In CN and PP, T2MAC maintains its\\nsustained sample efficiency. Upon reaching a convergence,\\nits performance remains fiercely competitive. Furthermore,\\nan interesting observation is that all methods incorporating\\ncommunication significantly outperform those that don’t.\\nThis emphasizes that our chosen environments and scenar-\\nios intrinsically demand proficient communication. Such an\\noutcome not only underscores the importance of communi-\\ncation in these contexts but also validates the aptness of our\\nexperimental setup in benchmarking communication meth-\\nods.\\nEfficiency\\nIn addition to analyzing the overarching per-\\nformance, we also focus on understanding communication\\nefficiency. In many real-world situations, communication\\nresources—like bandwidth and transmission channels—are\\ninherently scarce. Overloading these resources doesn’t al-\\nways yield proportional benefits in performance. To quantify\\nthis efficiency, we calculate the performance improvement\\nattributable to communication and then normalize this by\\nthe communication rate. Here, communication rate denotes\\nhow frequently communication occurs throughout the learn-\\ning process. To gauge performance improvement, we intro-\\nduce a communication-free variant for each communication\\nmethod. This allows us to make a side-by-side comparison\\nto effectively highlight the tangible advantages offered by\\neach method. Specifically, for SMS, this communication-\\n0\\n0.4\\n0.8\\n1.2\\n1.6\\n2\\nSteps(M)\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nMedianTestWinRate\\nhallway\\n0\\n0.4\\n0.8\\n1.2\\n1.6\\n2\\nSteps(M)\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n5z_vs_1ul\\n0\\n0.4\\n0.8\\n1.2\\n1.6\\n2\\nSteps(M)\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n1o_10b_vs_1r\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nSteps(M)\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n1o_2r_vs_4r\\n0\\n20\\n40\\n60\\n80\\n100\\nSteps(K)\\n60\\n50\\n40\\n30\\n20\\n10\\n0\\nMedianTestReturn\\nCooperateNavigation(Medium)\\n0\\n80\\n160\\n240\\n320\\n400\\nSteps(K)\\n50\\n40\\n30\\n20\\n10\\n0\\nCooperativeNavigation(Hard)\\n0\\n0.4\\n0.8\\n1.2\\n1.6\\n2\\nSteps(M)\\n10\\n5\\n0\\n5\\nPredatorPrey(Medium)\\n0\\n0.4\\n0.8\\n1.2\\n1.6\\n2\\nSteps(M)\\n20\\n15\\n10\\n5\\n0\\n5\\n10\\nPredatorPrey(Hard)\\nT2MAC(Ours)\\nQMIX\\nDOP\\nTarMAC\\nMAIC\\nSMS\\nMASIA\\nFigure 3: Performance on multiple benchmarks.\\nfree baseline is DOP, while for the others, it’s QMIX. We’ve\\ncarried out this analytical assessment predominantly in the\\nmost challenging environment, SMAC. As shown in Ta-\\nble. 2, T2MAC consistently outperforms baselines in terms\\nof both improvement, communication rate, and commu-\\nnication efficiency. Such results underscore the capability\\nof T2MAC to process communication dynamics, including\\nwhen to communicate, with whom, and how to trade-off be-\\ntween performance and efficiency.\\nGenerality\\nOur prior experiments have demonstrated the\\nrobustness of T2MAC across diverse environments, scenar-\\nios of varying complexities, and different scales. To fur-\\nther evaluate the generality of T2MAC, we apply it across\\na wide range of established MARL baselines, including\\nQMIX, DOP, and MAPPO. The test win rate for the sce-\\nnario 1𝑜 10𝑏 𝑣𝑠 1𝑟 is illustrated in Fig. 4. Notably, across\\nall these baselines, T2MAC consistently achieves superior\\nperformance, often by a notable margin. This positive per-\\nformance improvement demonstrates the broad applicability\\nand potency of T2MAC in the realm of MARL.\\nAblation\\nTo better understand the impact of each com-\\nponent within T2MAC, we perform an ablation study on\\nthe scenario 1𝑜 10𝑏 𝑣𝑠 1𝑟. Here’s a breakdown of the con-\\nfigurations evaluated: T2MAC: This refers to the com-\\nplete method proposed in our work. QMIX: This serves\\nas our baseline for comparison, representing the core func-\\ntionality without the enhancements introduced in T2MAC.\\nT2MAC(Fullcomm): This is a variant of T2MAC that does\\nnot incorporate selective engagement. Here, communica-\\ntion occurs continuously amongst agents without deciding\\nwhen or with whom to communicate. T2MAC(Nocomm):\\nThis is a more stripped-down version of T2MAC, exclud-\\ning both selective engagement and evidence-driven inte-\\ngration. Essentially, it’s a version of T2MAC where com-\\nmunication is completely omitted, but the Dirichlet Distri-\\nbution remains in the Q-value network. As illustrated in\\nFig. 5, the results demonstrate the contributions of each\\ncomponent: From QMIX to T2MAC(Nocomm): The shift\\nfrom Categorical distribution to Dirichlet distribution makes\\nsense. The Dirichlet distribution’s advantage might stem\\nfrom its ability to model second-order probabilities, intro-\\nducing an additional layer of decision-making uncertainty\\nwhich potentially enhances learning and adaptation. From\\nT2MAC(Nocomm) and T2MAC(Fullcomm): The sizable\\nperformance gap between these two underscores the sig-\\nnificance of evidence-driven information exchange and in-\\ntegration. This sheds light on the efficacy of trust-based\\ncommunication, where agents not only share but also as-\\nsess the reliability of information before acting upon it.\\nFrom T2MAC(Fullcomm) to T2MAC: The contrast in per-\\nformance between these two configurations underlines the\\nimportance of targeted communication. Instead of a blan-\\nket communication strategy, selective engagement, whereby\\nagents communicate at strategic junctures with specific part-\\nners, can enhance the overall efficiency and performance of\\nthe system.\\nFurthermore,\\nto\\nprovide\\na\\nclear\\nablation\\nanalysis\\nfor\\nevidence-driven\\nintegration,\\nwe\\nhave\\nconducted\\nadditional comparisons in the 1𝑜 10𝑏 𝑣𝑠 1𝑟\\nscenario\\nQMIX\\nDOP\\nMAPPO\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nMedian Test Win Rate\\n0.84\\n0.58\\n0.39\\n0.31\\n0.20\\n0.19\\nwith T2MAC\\nw/o T2MAC\\nFigure 4: Generality.\\n0\\n0.4\\n0.8\\n1.2\\n1.6\\n2\\nSteps(M)\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nMedianTestWinRate\\nT2MAC\\nT2MAC(Fullcomm)\\nT2MAC(Nocomm)\\nQMIX\\nFigure 5: Ablation for trusted communication and selective\\nengagement.\\nwith a summation-based integration method (COMM-\\nNET(Sukhbaatar, Szlam, and Fergus 2016)) and a black-box\\nmethod (TarMAC(Das et al. 2019)). As shown in Fig. 6,\\nthe results demonstrate that the evidence-driven integration\\nproposed by T2MAC has a clear advantage, confirming its\\neffectiveness.\\nConclusions\\nIn this work, we tackle the intricacies inherent in multi-agent\\ncommunication. Previous works focus on broadcast commu-\\nnication and treat the fusion of information as a block box,\\nwhich inevitably diminishes communication efficiency. To\\nthis end, we present the T2MAC framework. This novel ap-\\nproach empowers agents with the capacity to craft messages\\nspecifically tailored for distinct agents. Beyond mere mes-\\nsage customization, T2MAC strategically chooses the best\\ntimings and relies on trusted partners for communication,\\nensuring an efficient integration of incoming messages and\\nfacilitating trusted decision-making. Rooted in solid theo-\\nretical principles, this approach stands out for its efficiency.\\nFurthermore, to substantiate our claims, we conduct com-\\nprehensive experiments across multiple benchmarks, the re-\\n0\\n0.4\\n0.8\\n1.2\\n1.6\\n2\\nSteps(M)\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nMedianTestWinRate\\nT2MAC\\nCOMMNET\\nTarMAC\\nFigure 6: Ablation for evidence-driven integration.\\nsults of which underscore the effectiveness, efficiency, and\\nadaptability of the T2MAC.\\nAcknowledgements\\nThe authors would like to thank the editors and review-\\ners for their valuable comments. This work is supported\\nby the Youth Innovation Promotion Association CAS, No.\\n2021106, the China Postdoctoral Science Foundation, No.\\n2023M743639, the 2022 Special Research Assistant Grant\\nproject, No. E3YD5901, and the CAS Project for Young Sci-\\nentists in Basic Research, Grant No. YSBR-040.\\nReferences\\nAndrychowicz, O. M.; Baker, B.; Chociej, M.; Jozefowicz,\\nR.; McGrew, B.; Pachocki, J.; Petron, A.; Plappert, M.; Pow-\\nell, G.; Ray, A.; et al. 2020. Learning dexterous in-hand ma-\\nnipulation. The International Journal of Robotics Research,\\n39(1): 3–20.\\nDas, A.; Gervet, T.; Romoff, J.; Batra, D.; Parikh, D.; Rab-\\nbat, M.; and Pineau, J. 2019. Tarmac: Targeted multi-agent\\ncommunication. In International Conference on Machine\\nLearning, 1538–1546.\\nDempster, A. P. 1967. Upper and Lower Probabilities In-\\nduced by a Multivalued Mapping. The Annals of Mathemat-\\nical Statistics, 38(2): 325 – 339.\\nDing, Z.; Huang, T.; and Lu, Z. 2020.\\nLearning individ-\\nually inferred communication for multi-agent cooperation.\\nAdvances in Neural Information Processing Systems, 33:\\n22069–22079.\\nDosovitskiy, A.; Ros, G.; Codevilla, F.; L´opez, A. M.; and\\nKoltun, V. 2017. CARLA: An Open Urban Driving Simula-\\ntor.\\nIn 1st Annual Conference on Robot Learning, CoRL\\n2017, Mountain View, California, USA, November 13-15,\\n2017, Proceedings, volume 78 of Proceedings of Machine\\nLearning Research, 1–16. PMLR.\\nFoerster, J.; Assael, I. A.; De Freitas, N.; and Whiteson, S.\\n2016. Learning to communicate with deep multi-agent re-\\ninforcement learning. Advances in neural information pro-\\ncessing systems, 29.\\nGuan, C.; Chen, F.; Yuan, L.; Wang, C.; Yin, H.; Zhang, Z.;\\nand Yu, Y. 2022. Efficient Multi-agent Communication via\\nSelf-supervised Information Aggregation. Advances in Neu-\\nral Information Processing Systems, 35: 1020–1033.\\nJiang, J.; and Lu, Z. 2018. Learning attentional communi-\\ncation for multi-agent cooperation. In Advances in neural\\ninformation processing systems, 7254–7264.\\nJsang, A. 2018. Subjective Logic: A formalism for reasoning\\nunder uncertainty. Springer Publishing Company, Incorpo-\\nrated.\\nKim, D.; Moon, S.; Hostallero, D.; Kang, W. J.; Lee, T.;\\nSon, K.; and Yi, Y. 2019. Learning to schedule communi-\\ncation in multi-agent reinforcement learning. arXiv preprint\\narXiv:1902.01554.\\nLowe, R.; Wu, Y. I.; Tamar, A.; Harb, J.; Abbeel, O. P.;\\nand Mordatch, I. 2017. Multi-agent actor-critic for mixed\\ncooperative-competitive environments. In Advances in neu-\\nral information processing systems, 6379–6390.\\nMalinin, A.; and Gales, M. J. F. 2018. Predictive Uncer-\\ntainty Estimation via Prior Networks. In Bengio, S.; Wal-\\nlach, H. M.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.;\\nand Garnett, R., eds., Advances in Neural Information Pro-\\ncessing Systems 31: Annual Conference on Neural Informa-\\ntion Processing Systems 2018, NeurIPS 2018, December 3-\\n8, 2018, Montr´eal, Canada, 7047–7058.\\nMalinin, A.; Mlodozeniec, B.; and Gales, M. J. F. 2020.\\nEnsemble Distribution Distillation.\\nIn 8th International\\nConference on Learning Representations, ICLR 2020, Addis\\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\\nMao, H.; Zhang, Z.; Xiao, Z.; Gong, Z.; and Ni, Y. 2019.\\nLearning agent communication under limited bandwidth by\\nmessage pruning. arXiv preprint arXiv:1912.05304.\\nNiu, Y.; Paleja, R. R.; and Gombolay, M. C. 2021. Multi-\\nAgent Graph-Attention Communication and Teaming.\\nIn\\nAAMAS, 964–973.\\nOsband, I.; Blundell, C.; Pritzel, A.; and Van Roy, B. 2016.\\nDeep exploration via bootstrapped DQN. In Advances in\\nneural information processing systems, 4026–4034.\\nRashid, T.; Samvelyan, M.; De Witt, C. S.; Farquhar, G.;\\nFoerster, J.; and Whiteson, S. 2018.\\nQMIX: Monotonic\\nvalue function factorisation for deep multi-agent reinforce-\\nment learning. arXiv preprint arXiv:1803.11485.\\nSamvelyan, M.; Rashid, T.; de Witt, C. S.; Farquhar, G.;\\nNardelli, N.; Rudner, T. G.; Hung, C.-M.; Torr, P. H.; Fo-\\nerster, J.; and Whiteson, S. 2019. The starcraft multi-agent\\nchallenge. arXiv preprint arXiv:1902.04043.\\nSensoy, M.; Kaplan, L. M.; and Kandemir, M. 2018. Eviden-\\ntial Deep Learning to Quantify Classification Uncertainty.\\nIn Bengio, S.; Wallach, H. M.; Larochelle, H.; Grauman, K.;\\nCesa-Bianchi, N.; and Garnett, R., eds., Advances in Neu-\\nral Information Processing Systems 31: Annual Conference\\non Neural Information Processing Systems 2018, NeurIPS\\n2018, December 3-8, 2018, Montr´eal, Canada, 3183–3193.\\nSilver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai,\\nM.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel,\\nT.; et al. 2018. A general reinforcement learning algorithm\\nthat masters chess, shogi, and Go through self-play. Science,\\n362(6419): 1140–1144.\\nSilver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.;\\nHuang, A.; Guez, A.; Hubert, T.; Baker, L.; Lai, M.; Bolton,\\nA.; et al. 2017. Mastering the game of go without human\\nknowledge. nature, 550(7676): 354–359.\\nSingh, A.; Jain, T.; and Sukhbaatar, S. 2018. Learning when\\nto communicate at scale in multiagent cooperative and com-\\npetitive tasks. arXiv preprint arXiv:1812.09755.\\nSukhbaatar, S.; Szlam, A.; and Fergus, R. 2016.\\nLearn-\\ning Multiagent Communication with Backpropagation. In\\nProceedings of the 30th International Conference on Neu-\\nral Information Processing Systems, NIPS’16, 2252–2260.\\nRed Hook, NY, USA: Curran Associates Inc.\\nISBN\\n9781510838819.\\nSun, C.; Wu, B.; Wang, R.; Hu, X.; Yang, X.; and Cong,\\nC. 2021.\\nIntrinsic Motivated Multi-Agent Communica-\\ntion.\\nIn Proceedings of the 20th International Confer-\\nence on Autonomous Agents and MultiAgent Systems, AA-\\nMAS ’21, 1668–1670. Richland, SC: International Founda-\\ntion for Autonomous Agents and Multiagent Systems. ISBN\\n9781450383073.\\nSunehag, P.; Lever, G.; Gruslys, A.; Czarnecki, W. M.; Zam-\\nbaldi, V.; Jaderberg, M.; Lanctot, M.; Sonnerat, N.; Leibo,\\nJ. Z.; Tuyls, K.; et al. 2017.\\nValue-decomposition net-\\nworks for cooperative multi-agent learning. arXiv preprint\\narXiv:1706.05296.\\nVinyals, O.; Babuschkin, I.; Czarnecki, W. M.; Mathieu, M.;\\nDudzik, A.; Chung, J.; Choi, D. H.; Powell, R.; Ewalds,\\nT.; Georgiev, P.; et al. 2019.\\nGrandmaster level in Star-\\nCraft II using multi-agent reinforcement learning. Nature,\\n575(7782): 350–354.\\nWang, R.; He, X.; Yu, R.; Qiu, W.; An, B.; and Rabinovich,\\nZ. 2020a. Learning Efficient Multi-agent Communication:\\nAn Information Bottleneck Approach. In ICML 2020: 37th\\nInternational Conference on Machine Learning.\\nWang, T.; Wang, J.; Zheng, C.; and Zhang, C. 2020b. Learn-\\ning Nearly Decomposable Value Functions Via Communi-\\ncation Minimization. In ICLR 2020 : Eighth International\\nConference on Learning Representations.\\nWang, Y.; Han, B.; Wang, T.; Dong, H.; and Zhang, C. 2020.\\nDop: Off-policy multi-agent decomposed policy gradients.\\nIn International Conference on Learning Representations.\\nXue, D.; Yuan, L.; Zhang, Z.; and Yu, Y. 2022. Efficient\\nMulti-Agent Communication via Shapley Message Value.\\nIn Raedt, L. D., ed., Proceedings of the Thirty-First Interna-\\ntional Joint Conference on Artificial Intelligence, IJCAI-22,\\n578–584. International Joint Conferences on Artificial Intel-\\nligence Organization. Main Track.\\nYu, C.; Velu, A.; Vinitsky, E.; Gao, J.; Wang, Y.; Bayen, A.;\\nand Wu, Y. 2022.\\nThe surprising effectiveness of ppo in\\ncooperative multi-agent games. Advances in Neural Infor-\\nmation Processing Systems, 35: 24611–24624.\\nYuan, L.; Wang, J.; Zhang, F.; Wang, C.; Zhang, Z.; Yu, Y.;\\nand Zhang, C. 2022. Multi-agent incentive communication\\nvia decentralized teammate modeling.\\nIn Proceedings of\\nthe AAAI Conference on Artificial Intelligence, volume 36,\\n9466–9474.\\nZhang, S. Q.; Zhang, Q.; and Lin, J. 2019. Efficient commu-\\nnication in multi-agent reinforcement learning via variance\\nbased control. In Advances in Neural Information Process-\\ning Systems, 3235–3244.\\nZhang, S. Q.; Zhang, Q.; and Lin, J. 2020. Succinct and\\nrobust multi-agent communication with temporal message\\ncontrol. Advances in Neural Information Processing Sys-\\ntems, 33: 17271–17282.\\n'},\n",
       " {'abstract': 'To explore the chemical space of all small molecules, a common approach is to compress the dimension of the system to facilitate downstream machine learning tasks. Towards this end, the paper presents a data-driven approach for clustering potential energy landscapes of molecular structures by applying Network Embedding techniques, to obtain latent variables defined through the embedding function. It also incorporates an entropy-sensitive adaptive scheme for hierarchical sampling of the energy landscape, based on Metadynamics and Transition Path Theory. By taking into account the kinetic information implied by a system’s energy landscape, it interprets dynamical node-node relationships in reduced dimensions. Demonstrations of the method are presented using Lennard-Jones (LJ) clusters and a human DNA sequence.',\n",
       "  'introduction': 'The motivation is to understand the chemical space, specifically the number of organic molecules that can be formed and their potential for synthesizability. This knowledge has implications for drug development and disease treatment. The paper focuses on analyzing energy landscapes, which involve local minima, saddle points, entropic plateaus and deep energy wells. Existing methods for energy landscape analysis focus on identifying local minima and transition states between them. The proposed research is to apply Network Embedding techniques in combination with Metadynamics and TPT to produce adaptive embeddings that hierarchically convey information about the system’s behavior at different scales.',\n",
       "  'literature_review': 'The paper provides a review of Network Embedding techniques, such as DeepWalk, Node2vec, and their use in reducing the dimensionality of networks. It also discusses Metadynamics, which is used to aid in the exploration of energy landscapes, Transition Path Theory (TPT), which studies statistical properties of reactive trajectories, and Diffusion Wavelet algorithm for efficient approximation of embeddings. The paper proposes a modification of the Diffusion Wavelet algorithm that introduces parameters and optimizes them to minimize cross-entropy loss.',\n",
       "  'methodology': 'The paper proposes a hierarchical method for clustering energy landscapes and identifying latent variables of molecular structures. It starts by constructing a network representation of the energy landscape, where nodes represent local minima and edges represent energy barriers or transition probabilities. The method incorporates adaptive adjustments to the network structure and edge weights based on Metadynamics and TPT, which allows for hierarchical sampling and focusing on particular areas of the energy landscape. The embeddings are produced using a modified version of the Diffusion Wavelet algorithm, which is more efficient and scalable for large networks.',\n",
       "  'results': 'The paper demonstrates the effectiveness of the proposed method through experiments on Lennard-Jones (LJ) clusters and a human telomere sequence. It shows that the method can produce hierarchical embeddings that provide insights into the dynamics of the system at different scales. The embeddings can be used to identify transition paths and potential energy barriers. The method also allows for the incorporation of entropic effects and the analysis of systems with complex energy landscapes.',\n",
       "  'conclusion': 'The paper introduces a data-driven approach for clustering energy landscapes of molecular structures and identifying latent variables of molecular structures. The approach is based on Network Embedding techniques, Metadynamics and TPT. One major highlight is that the resulting embeddings can be used to interpret dynamical node-node relationships in reduced dimensions that are consistent with chemical kinetics and are thus more likely to be aligned with synthesizability. The framework can be used for applications in various fields such as drug discovery.',\n",
       "  'title': 'Clustering Molecular Energy Landscapes by Adaptive Network Embedding',\n",
       "  'author': 'Paula Mercurio, Di Liu',\n",
       "  'textdata': 'Clustering Molecular Energy Landscapes by\\nAdaptive Network Embedding\\nPaula Mercurio and Di Liu\\nJanuary 23, 2024\\nAbstract\\nIn order to efficiently explore the chemical space of all possible small\\nmolecules, a common approach is to compress the dimension of the system\\nto facilitate downstream machine learning tasks. Towards this end, we\\npresent a data driven approach for clustering potential energy landscapes of\\nmolecular structures by applying recently developed Network Embedding\\ntechniques, to obtain latent variables defined through the embedding\\nfunction. To scale up the method, we also incorporate an entropy sensitive\\nadaptive scheme for hierarchical sampling of the energy landscape, based\\non Metadynamics and Transition Path Theory. By taking into account\\nthe kinetic information implied by a system’s energy landscape, we are\\nable to interpret dynamical node-node relationships in reduced dimensions.\\nWe demonstrate the framework through Lennard-Jones (LJ) clusters and\\na human DNA sequence.\\n1\\nIntroduction\\nThe motivation of the project is the fundamental question of chemical spaces:\\nhow many organic molecules can be formed, and of these, how can we identify\\nmolecules with useful properties which can be chemically synthesized. Under-\\nstanding how such molecules function in biological systems will have a tremendous\\nimpact on development of new drugs and new treatment of diseases [1]. For exam-\\nple, the GDB-17 dataset [2] takes into account only molecules allowed by valency\\nrules, excluding those unstable or unsynthesizable due to strained topologies or\\nreactive functional groups, thereby reducing the enumeration to a manageable\\ndatabase size of 166.4 billion molecules formed of up to 17 atoms of C, N, O,\\nS, and halogens. Fast nearest neighbors searching of large generated datasets\\nlike GDBs has led to methods for virtual screening and visualization of drug-\\nlike molecules, with early success in neurotransmitter receptor and transporter\\nligands.\\nMost applications in biology and chemistry, such as protein folding, involve\\nsystems that behave according to some potential energy landscape of complex\\nstructure with a large number of local minima, saddle points (transition states),\\nentropic plateaus and deep energy wells. Existing methods for energy landscape\\n1\\narXiv:2401.10972v1  [q-bio.BM]  19 Jan 2024\\nanalysis focus on identifying local minima via geometric optimization, and\\nfinding transition states connecting them using steepest descent pathways [3].\\nTo understand the dynamics over multiple magnitudes of space and time scales,\\nwe can take the viewpoint of the system as a network of local energy minima\\nand entropic basins, connected by edges weighted according to the energy and\\nentropy barriers that must be crossed for transitions between metastable states.\\nThe proposed research is to apply recent Network Embedding techniques\\n[4, 5, 6, 7] to develop a data driven approach for clustering of potential energy\\nlandscapes and identifying latent variables of molecular structures, to further\\nfacilitate sampling and optimization of the chemical spaces and developing\\ngenerative models for druglike small molecules. The latent variables are given by\\nthe output of the embedding function. By incorporating energetic information,\\nwe will be able to interpret node-node relationships in reduced dimensions that\\nare consistent with chemical kinetics and are more likely to be aligned with\\nsynthesizability.\\nOne multiscale challenge is due to the presence of deep potential wells.\\nMetadynamics [8] uses a non-Markovian random walk to explore an energy\\nlandscape, which is smoothed by additive Gaussian terms after each step, and so\\nis the transition probabilities of the associated random walk. As this happens,\\nthe process is discouraged from revisiting the lowest energy states repeatedly.\\nThe eventual output is to have a flattened energy landscape, as well as more\\nefficient random walk samplings. The original potential can be recreated by\\nsubtracting the additive Gaussian terms.\\nTo study transition processes in complex systems with rugged energy land-\\nscape dominated by entropic effects, such that transitions involving a flat region\\non the potential surface that is favorable entropically and the necessity to de-\\ncrease entropy to exit from this region, it is necessary to examine the ensemble\\nof all the transition paths as a probability space. The Transition Path Theory\\n(TPT) [9] studies statistical properties of the reactive trajectories such as rates\\nand dominant reaction pathways through probability currents between adjacent\\nstates. In [10], the definition of probability current in TPT was generalized from\\nedges to individual nodes and networks, for characterizing transition states in\\nthe form of subnetworks.\\nIn this article, we use Network Embedding techniques in combination with\\nMetadynamics and TPT to produce adaptive embeddings that hierarchically\\nconvey information about the system’s behavior at different scales. We adjust the\\nedge weights of the network in a way that parallels Metadynamics to encourage\\nexploration away from the local energy minima, and adopt TPT to capture\\nmicro dynamical features of interest. It is shown that these embeddings provide\\nan effective way to understand and visualize inter-node relationships.\\nThe rest of this article is structured as follows. In Section 2, we provide\\nsome background on Network Embedding, Metadynamics and TPT. In Section\\n3, we discuss more details in the implementations and demonstrate our method\\nthrough Lennard-Jones (LJ) clusters. Section 4 contains an application to a less\\nhomogeneous system: DNA folding in a human telomere.\\n2\\n2\\nBackground\\n2.1\\nNetwork Embedding\\nReal-world networks, particularly those representing possible molecular struc-\\ntures and other biological and chemical systems, are often large and complex,\\nmaking them difficult to conceptualize. Network Embedding maps nodes of a\\ngiven network into a low-dimensional continuous vector space, to facilitate down-\\nstream machine learning tasks. Making use of the sparsity of networks, recently\\ndeveloped Network Embedding methods can scale up linearly with regard to\\nthe number of edges. Major techniques include factorization of functions of the\\nadjacency matrix, random walks samplings of node neighborhoods, and deep\\nnetwork learning techniques. The idea is that nodes with close proximities in\\nthe network should have similar embeddings in the latent space.\\nThe basic setup is an undirected network G(S, E) with node set S and edge\\nset E, while generalizations to directed graphs is straightforward. Let |S| = n\\nand An×n be the weighted adjacency matrix of the network with weight aij ≥ 0\\nbetween nodes vi and vj. The output will be a map\\nzi = f(vi) : S → Rd,\\nwith\\nd ≪ n.\\n(1)\\nFor methods discussed in this paper, the encoding function (1), referred as direct\\nencoding, is simply a lookup matrix: zi = f(vi) = Zei, such that Z ∈ Rd×n\\ncontains the embedding vectors for all nodes vi ∈ S, and ei is an indicator\\nvector, therefore f(vi) simply gives the ith column of Z. The set of trainable\\nparameters for direct encoding approaches is the embedding matrix Z, which is\\nto be optimized directly. Vector ai = {aik}n\\nk=1 denotes the first order proximity\\nbetween node vi and other nodes. The second order proximity between vi and\\nvj can be determined by the similarity between ai and aj, which compares the\\npair’s neighborhood structures.\\nIn DeepWalk and Node2vec [4, 5], short random walk simulations are used to\\ndetermine proximity for each pair of nodes. More specifically, random walk runs\\nthrough nodes of the network, with transition rates determined by the edges\\nweights. Two nodes are close to each other if there is a high probability that a\\nrandom walk simulation containing one node will also contain the other. Simi-\\nlarities between between embedding nodes is given by the following conditional\\nprobabilities based on the SkipGram model:\\nP(j|i) =\\nexp(zi · zj)\\nP\\nl exp(zi · zl),\\n(2)\\nwhere P(j|i) denotes the conditional probability that a random walk starting\\nat node vi will include node vj, and zi is the embedding of vi. The learning is\\nachieved by minimizing the following cross entropy loss using Stochastic Gradient\\nDescent (SGD) method:\\nL(G) =\\nX\\nvi\\nX\\nj∈R(vi)\\n− log (P(j|i)) ,\\n(3)\\n3\\nwhere R(i) represents a k-step random walk trial starting from node i. The\\nefficiency of evaluating (2) can be greatly improved by using negative sampling\\n[11] that randomly selects edges favoring less frequent ones.\\nIn this paper, we will adopt a Network Embedding scheme introduced in\\n[7], where the embeddings are produced via a sparse approximation of random\\nwalks on networks. For a given undirected network G(A) with adjacency matrix\\nA = (aij), let D be the diagonal matrix such that Dii = P\\nj aij. The volume\\nof the graph will be given by v = P\\ni Dii. The Laplacian L = I − D−1A has\\neigen-decomposition L = ΦΛΦT , where Λ represents the diagonal matrix of\\nordered eigenvalues so that 0 = λ1 ≤ λ2 ≤ ...λn, and the eigenvectors are given\\nby columns of Φ denoted by ϕ1, ϕ2, ....ϕn. Assuming the network is connected,\\nthe discrete Green function satisfies\\nG(i, j) =\\nn\\nX\\nk=2\\n1\\nλk\\nϕk(i)ϕk(j).\\n(4)\\nWe can further define the commute time ct(i, j) to be the mean time for the\\nMarkov process prescribed by transition probability matrix T = D−1A = I − L\\non the network to travel from node vi to node vj, and back to vi. It is shown in\\n[12] that the coordinate matrix for embeddings that preserve commute times\\nhas the following form\\nΘ = √vΛ−1/2ΦT .\\n(5)\\nTo produce an efficient approximation of Θ, we can assume that T is local\\nin the sense that, at least asymptotically, its columns have small support, and\\nhigh powers of T will be of low rank, which can be justified for potential driven\\nsystems by disparate transition rates between different neighboring metastable\\nstates. Taking higher powers of T is equivalent to running the Markov chain\\nforward in time, which allows for representations of the random walk, i.e. re-\\naction pathways, at different time scales. Making use of this sparsity, we can\\nproduce compressed approximations to the dyadic powers of T with its principal\\ncomponents, by using fast algorithms such as Lanczos Bidiagonalization for\\nsingular value decomposition (SVD) with the complexity depending linearly on\\nthe number of nonzero elements.\\nThe following scheme is a modification of the Diffusion Wavelet algorithm\\n[13]. Starting with T0 = T, at each iteration, taking Uk and Σk to be the top\\nleft singular vectors and singular values of Tk ≈ T 2k, we let Tk := U T\\nk Tk−1Uk.\\nFrom this, we can have a low-rank approximation to the Green function of the\\nrandom walk, using the Schultz method:\\nG =\\n∞\\nX\\nk=1\\nT k =\\n∞\\nY\\nk=0\\n(I + T 2k).\\n(6)\\nThe embedding matrix Θ thus satisfies ΘT Θ = vG, where v is the volume of the\\nnetwork defined as above. Denoting the leading singular values and left singular\\nvectors of the matrix vG by ΣG and UG, we take Θ := Σ1/2\\nG U T\\nG.\\n4\\nWe can further use Θ as a starting point, introduce parameters by multiplying\\nits jth column by a weight cj, and optimize {cj}’s to minimize cross entropy loss\\n(3) via SGD. Moreover, for robustness of the algorithm, with certain probability,\\nwe can reintroduce singular vectors that were removed in previous truncations,\\nas a residual correction technique.\\n2.2\\nMetadynamics\\nMetadynamics was introduced in [8] as a technique to aid in the exploration of\\nenergy landscapes. The scheme is to create a non-Markovian, and approximately\\nself-avoiding, random walk by adjusting the gradient of the energy landscape\\nafter each step with the addition of derivative of a Gaussian term. Over time,\\nthese Gaussians eventually fill up the valleys in the energy potential, which\\nallows the random walk to explore other areas of the landscape and leads to a\\nmore complete picture of the system dynamics. Specifically, after each step, the\\nparameter ϕi, which represents the derivative of the energy with respect to the\\nith parameter − ∂E\\n∂xi , is adjusted according to:\\nϕt+1\\ni\\n= ϕt\\ni − ∂\\n∂xi\\nW\\nY\\ni\\nexp\\n\\x12\\n−|xi − xt\\ni|2\\n2δ2\\n\\x13\\n,\\n(7)\\nwhere W, δ and xt\\ni are the height, width and center of the Gaussian respectively,\\nto be chosen based on prior knowledge of the energy landscape.\\n2.3\\nTransition Path Theory\\nThe Transition Path Theory (TPT) [9] studies statistical properties of the\\nreactive trajectories such as rates and dominant pathways through probability\\ncurrents between adjacent states. In the simplest setting, given reactant state A\\nand product state B, any equilibrium path X(t) oscillates infinitely many times\\nbetween A and B, with each oscillation from A to B being a reaction event. The\\nreactive trajectories are successive pieces of X(t) during which it has left A and\\non its way to B next, without coming back to A.\\nThe discrete forward committor q+\\ni is defined as the probability that the\\nprocess starting in node i will first reach B rather than A, and the discrete\\nbackward committor q−\\ni is defined as the probability that the process arriving in\\nnode vi last came from A rather than B. For Markov processes with infinitesimal\\ngenerator T = (tij), the forward committor satisfies discrete Dirichlet equations:\\nX\\nvj∈S\\ntijq+\\nj = 0,\\nfor vi ∈ S\\\\(A ∪ B),\\n(8)\\nwith the boundary condition\\nThe probability current of reactive trajectories is the average rate at which\\nthey flow from one state to another when the process is at statistical equilibrium\\nwith distribution π, and can be obtained by\\nf AB\\nij\\n= πiq−\\ni tijq+\\nj ,\\nif i ̸= j.\\n(9)\\nTo deal with the fact that transitions between any two states can go forward and\\nbackward, the effective current can be introduced as f +\\nij = max\\nFigure 1: Disconnectivity tree and Metadynamics based embeddings for the\\nLennard-Jones cluster with 8 atoms. Left: Disconnectivity tree of all local minima.\\nRight: Embeddings for the local minima after applying the Metadynamics\\nadjustment.\\nColor scheme represents the potential energy, e.g., dark blue\\ndenotes the lowest, and red as the highest. Closely related minima have very\\nsimilar or identical embeddings, e.g., both yellow minima are embedded at the\\nyellow point on the right.\\nwhere θi is the coordinate of the ith node (energy minimum).\\nAdaptively\\nchoosing the center node and removing distant nodes, the process will produce a\\nseries of hierarchical embeddings that provide information about the full energy\\nlandscape. Each level provides a representation of the energy landscape at a\\ndifferent scale.\\nFigures 1 gives the disconnectivity tree of the original potential and em-\\nbeddings after applying the Metadynamics adjustment by equation 11 for the\\n8-atom LJ cluster. The colors on each minimum on the disconnectivity tree are\\nmatched to those nodes’ embeddings in Figure 1, with some of the nodes colored\\nred, embedded to the same place. In the embeddings, nodes with closer dynamic\\nrelationships are clustered together, as a result of the shorter commute time\\ndistance between them.\\nOn a larger scale, we can also see how the higher energy nodes relate to\\nthe nodes with the two lowest energies. Specifically, the global minimum (in\\ndark blue) is closer to the nodes in the middle of the tree (in orange), while the\\nsecond lowest local minimum is much closer to the three highest energy nodes.\\nThis allows us to draw conclusions about a lowest commute time path through\\nthese states: one of the orange colored states might transition directly to the\\nglobal minimum, while one of the higher energy states would be more likely to\\ntransition to the second lowest energy state first, and then either remain there\\nor transition on to the global minimum.\\n7\\nFigure 2: The 8-atom LJ network of local minima. Edge lengths are proportional\\nto commute times. Node colors are chosen to match those in Figures 1.\\nTable 1: Commute times between nodes, 8-atom LJ cluster.\\nNode 1\\nNode 2\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n1\\n0\\n25.4\\n17.7\\n23.5\\n29.0\\n65.2\\n86.1\\n88.4\\n2\\n0\\n7.7\\n13.5\\n19.0\\n55.2\\n60.7\\n63.0\\n3\\n0\\n5.8\\n11.3\\n47.5\\n68.4\\n70.7\\n4\\n0\\n17.1\\n53.3\\n74.3\\n76.5\\n5\\n0\\n58.8\\n79.7\\n82.0\\n6\\n0\\n115.9\\n118.2\\n7\\n0\\n123.7\\nFor comparison, we also investigated the inter-node relationships by directly\\ncomputing the commute times between each pair of nodes. The commute time\\nbetween nodes vi and vj, or mean time for the Markov process on the graph\\nprescribed by the Laplacian L to travel from vi to vj and back to vi, is given by\\nct(i, j) = v\\nN\\nX\\nk=2\\nλk(ϕk(i) − ϕk(j))2,\\n(12)\\nwhere λk are the eigenvalues of L, and the ϕk are the corresponding eigenvectors\\nas defined in the introduction. The off-diagonal entries for the Laplacian for a\\nLJ cluster with κ degrees of freedom are given by\\nL(i, j) =\\nX\\nk\\nO(i)\\nOk(k)\\nv(i)\\nvk(k)\\nκ−1\\nv(i) exp(−β(Ek(k) − E(i))),\\n(13)\\nwhere O and Ok represent point group orders of the local minima and transition\\nstates, respectively, similarly v and vk represent mean vibrational frequencies\\n8\\nFigure 3:\\nDisconnectivity tree for the 38-atom LJ cluster. The structures of\\nthe two lowest-energy configurations are also pictured.\\nand E and Ek represent potential energy levels at each configuration. It can be\\nseen that the commute time diagram is consistent with the Network Embedding\\noutput.\\n3.2\\nMulti-level embeddings of LJ 38 cluster\\nFigure 3 shows disconnectivity tree of the 38-atom LJ cluster. For simplicity, we\\nconstruct the adjacency matrix for this network with entries given by the energy\\nbarriers between states. Figure 4 gives the hierarchical embedding of the LJ-38\\ncluster. The left image shows the initial embeddings, colored by their commute\\ntime distances from the global minimum. We re-embedded the points of interest\\nwith the Metadynamics adjustment to the potential with two iterations and\\nobtain the right image.\\nThe hierarchical structure of the embedding process is organized as the\\nfollowing. In the first level embeddings (Fig. 4, left), the nodes are embedded\\nconsistently with their commute time distances from the global minimum, which\\ncorrespond loosely to potential energy level. In particular, the nodes with energy\\nE > −170, which are the highest energy clusters in the tree, are embedded\\nfurther from the global minimum. In the next embedding, these nodes are\\nremoved due to their further distances from the global minimum, and more\\ncentral clusters will be re-embedded. As we consider the second level and later\\nembeddings, this pattern repeats: each re-embedding reveals a new “layer\" of\\nnodes that are embedded closer to the global minimum, which correspond to\\nnodes on the disconnectivity tree that are of lower energy than nodes removed\\nin the previous level.\\n9\\nFigure 4: Hierarchical embeddings for the LJ cluster with 38 atoms. Pictured\\nare the embeddings before (left) and after (right) applying Metadynamics. Color\\nscheme denotes commute time from the global minimum, with dark blue being\\nshortest distances, and red as furthest distances.\\nIn particular, at the 3rd level (Fig. 4, right), the embeddings have 4 small\\n“spokes\" originating from a central cluster containing the global minimum. How-\\never, these embeddings provide additional context: nodes that are embedded\\nwithin a particular “spoke\" are more closely related, which means the system is\\nmore likely to transition between these states. Since the nodes do not all come\\nfrom the same group on the disconnectivity tree, these embeddings can also\\nreveal interactions between nodes that aren’t indicated on the disconnectivity\\ntree.\\nAdditionally, since the spokes are connected to the cluster containing the\\nglobal minimum, we can conclude that each spoke represents a potential transition\\npathway from the outer edge of the cluster to the center. In other words, if\\nthe system is at a state represented by the outer point of one of the spokes, its\\nmost likely path toward the global minimum will be to travel through the states\\nrepresented by other nodes in the same spoke.\\nWe can apply similar reasoning to higher level embeddings. Each level reveals\\na more detailed picture of the dynamics of a different part of the system’s\\nenergy landscape. The first levels give a coarse-grained picture, only identifying\\nbroad groups of high energy and low energy nodes, while later levels give a\\nmore fine-grained visualization of the nodes most closely related to the global\\nminimum.\\nOften, we want a more detailed, finer grained visualization of the energy\\nlandscape than the disconnectivity tree in Figure 3 can provide. It is informative,\\ntherefore, to re-embed parts of the network of greatest interest to gain further\\ninsight. Here we focus on the lowest energy parts of the LJ energy landscape.\\nWe repeated the above process using the subnetwork consisting only of the nodes\\nwith potential energy < −170.9, that is, the 163 lowest energy nodes. Figure 5\\nshows the results of this experiment.\\nNow we want to provide a closer inspection on the information flow along\\n10\\nFigure 5: Embeddings of the local minima of the 38-atom LJ cluster with potential\\nenergies less than -170.9. The figure shows output of 2 level embeddings with\\nMetadynamics adjustment. Color scheme denotes commute time from the global\\nminimum.\\nthe hierarchical sampling with Metadynamics. In the first level without the\\nMetadynamics adjustment, nodes in these embeddings (both Figure 4 and Figure\\n5) are clustered according to their similarity in terms of commute time. In other\\nwords, nodes that can be quickly and frequently reached from either the global\\nminimum or second lowest energy node will be grouped near them. As a result,\\nmost of the nodes we are most interested in end up in the same cluster, and\\nthe embeddings obtained from the first application of the embedding method\\nonly give useful clusters for nodes that are more distant in terms of commute\\ntime from the part of the graph of interest. As we progress through additional\\nlevels, we pull apart the cluster containing the global minimum, positioned near\\nthe origin in the first level’s embeddings, until the final level’s embeddings give\\ndetails of the dynamics of the process within this cluster.\\nAfter the second level of Embedding with Metadynamics adjustment, if two\\nnodes share a cluster or are close in the embedding space, it indicates that the\\nsystem can easily transition between those nodes, with a relatively low energy\\nbarrier. As a result, the groupings seen in these embeddings correspond to\\nthe groupings in the disconnectivity tree for this system [3, 14]. For instance,\\none of the clusters in the second level embeddings correspond to the global\\nminimum and its nearest neighbors, pictured directly right of center in the\\ndisconnectivity tree in Figure 3. Clusters can be mapped to the disconnectivity\\ntree by comparing the potential energies of the nodes within the cluster to the\\ntree.\\nThe difference is after the third level, some of the clusters instead represent\\ncombinations of multiple disconnectivity tree groups; this is a result of re-\\nembedding the nodes to spread out those that were previously near the origin.\\nSuch nodes ended up being embedded in or near the clusters they are most\\nclosely related to, even though they are not actually members of the respective\\ntree groupings. For instance, the global minimum is embedded directly next to\\n11\\na node from a neighboring tree group.\\nIn other words, the first level’s embeddings tell us about higher energy nodes\\nand those that are more distant from the global minima, and further levels reveal\\ninformation about parts of the network that are closer to the global minimum.\\nAdditionally, these embeddings are useful for identifying transition paths. The\\nstructure of the third level embeddings, in particular, reveals four transition\\npaths: if the system is initialized from a node near the outside of one of these\\n“spokes\", its lowest energy path to the global minima will involve following the\\nspoke into the center cluster.\\nFigure 6: Metadynamics-based embeddings for the 38-atom cluster at the\\ntemperatures T = 0.08 (left) and T = 1 (right). Color scheme denotes commute\\ntime from the global minimum.\\nWe can also use these embeddings to observe the results of entropic changes\\nto the system. In Figure 6, the embeddings for this cluster under two additional\\ntemperature conditions are shown. The results of the temperature change are\\nreasonable according to what was observed previously with the 8-atom cluster.\\nNamely, at lower temperatures (Fig.6, left) closely related nodes are more likely\\nto be embedded much nearer each other, creating the impression that there are\\nfewer embeddings, while at higher temperatures (Fig.6, right), there is greater\\nvariation in the node embeddings.\\n3.3\\nHuman Telomere Folding\\nNow we want further develop Network Embedding technique for organic molecular\\nstructures and apply it to a more complex problem: DNA folding in a human\\ntelomere.\\nMore specifically, we consider a sequence of 22 nucleotide bases\\nA(G3TTA)3G3 which repeats within human telomeres. This sequence is known\\nto form a G-quadruplex, a type of secondary structure formed by groups of four\\nguanine bases called G-tetrads. Its structure and potential energy landscape\\nwere previously investigated in [15], where the potential energy landscape was\\ncalculated using the HiRE-RNA model for coarse-grained DNA [16] with 6 or\\n7 atoms considered for each of the 22 nucleotides in the telomere. We use\\n12\\nFigure 7: The four-strand G-quadruplex structure (PDB structure 1KF1), with\\nguanine nucleotides colored green. Image produced with Chimera [17].\\nthis database as a starting point. In particular, we construct a network with\\nnodes given by the 4000 lowest-energy local minima, and edges between nodes\\ndetermined by the transition states connecting them.\\nFor first experiments, we used a random walk based on the energy barriers\\nbetween states. Figure 8 shows the results of the initial embeddings and fourth\\nlevels with metadynamic adjustments, based on an energy landscape adjusted by\\na Gaussian term with width 0.75 and height 1 after each successive embedding.\\nAs in the LJ cluster experiment, the first embedding includes all nodes in\\nthe network, while each successive embedding shows a re-embedding of the\\nsubnetwork of nodes most closely related to the global minimum (that is, all\\nnodes whose previous embeddings lie within a small tolerance of the global\\nminimum).\\nFigure 8:\\nEmbeddings of the local minima network for the human telomere\\nsequence, based on the adjacency matrix and the Metadynamics adjustment.\\nColor scheme denotes commute time from the global minimum.\\nAs with the LJ clusters, each level of embedding represents “zooming in\"\\non the part of the network around the global minimum. The first level gives\\nus a global view – the global minimum and its nearest neighbors (with respect\\nto commute times) are clustered at the origin, represented by a dark blue dot.\\nThe red and orange dots furthest away from the origin represent local minima\\n13\\nwhich are more distantly related, requiring multiple steps or higher energies to\\ntransition to the global minimum. Potential transition paths can be identified by\\nstarting at one of these points, and moving toward the origin along nearby points.\\nAt the second and each of the following levels, the nodes embedded closest to the\\nlocal minimum are re-embedded to give us a more detailed inspection into the\\nrelationships of those nodes. The likely transition paths here can be constructed\\nsimilarly.\\n4\\nMultiscale embedding with TPT\\nNow we want to make use of the multiscale nature of the molecular dynamics\\nto speed up and scale up the computation. When a subset of the variables\\nevolves more quickly than the others, dimension reduction can be achieved\\nby the quasi-equilibrium on the fast variables such that averaged macroscopic\\neffect can replace microscopic details. For molecular configurations where the\\nenergy landscape is “flatter\" and state transitions occur faster, this principle of\\naveraging applies. From the space perspective, if the transitions paths between\\nnodes representing different molecular configurations are of relatively low energy\\nbarriers, they should be closely related in terms of mean commute time. We can\\nexpect these low barrier transitions to describe subtler changes likely involving\\nposition changes for only a small number of atoms. In other words, there is\\na time-space scale separation, i.e., on a global scale, transitions between two\\nstates tend to require larger, higher dimensional changes (associated with greater\\nenergy expenditures), while locally, transitions within some subnetwork around\\na point of interest require far fewer degrees of freedom.\\nFor energy landscapes with a large number of local minima, the corresponding\\nnetworks contain large numbers of nodes. In fact, for the LJ clusters, the number\\nof minima increases exponentially with the number of atoms. In these situations,\\nas we have seen, it is benefiting to have a method for Network Embedding\\nthat focuses on locally embedding regions of the network that are of particular\\ninterest, for example the subnetwork consisting of the global minimum and its\\nnearest neighbors. Since the commute times between states in this subnetwork\\nare short, the time required for the system to move between these configurations\\nis fast and the nodes tend to embedded near each other. The states in these\\nsubnetworks often represent molecular configurations that are similar, differing\\nonly by a simple conformational change; therefore these subnetworks can have\\ndramatically reduced dimensions compared with the full network.\\nWe want to present an alternate formulation, which replaces the adjacency\\nmatrix with the probability current matrix from TPT. For large networks,\\nparticularly those with irregular structures, the committor functions needed to\\napply TPT to the full network may prove difficult or impractical to compute.\\nFocusing our application of TPT onto a smaller, localized subnetwork avoids\\nthis difficulty, allowing us to take advantage of the additional information TPT\\noffers. In the following experiments, we first embed all the local minima using\\nthe adjacency matrix constructed with energy barriers, and apply the Network\\n14\\nEmbedding with Metadynamics. At each level, we remove nodes that have\\nlonger commute times to the global minimum, until the system is reduced to a\\nsubnetwork such that we can apply TPT and compute the committor functions\\n(8) and probability currents (9).\\nThen, each subnetwork is embedded into R3 using the effective probability\\ncurrent in place of the adjacency matrix. Here we apply the hierarchical em-\\nbedding procedure to the subnetwork of nodes surrounding the global minimum\\nas in previous sections, but the same process could be used to examine other\\ndomains of the network aside from the global minimum to obtain a complete\\npicture of the energy landscape.\\nThe distances between nodes within each subnetwork preserve the commute\\ntimes. Since the cross entropy loss minimization is applied to the overall network\\nat each step, we can expect that the distances between node embeddings to be\\nconsistent with likelihoods of a transition, similarly to the previous examples,\\nand therefore we can interpret the embeddings and identify possible transition\\npaths in the same way.\\nFigure 9:\\nHierarchical embeddings of the local minima network for the 8 atom\\nLJ cluster, based on a TPT-based subnetwork consisting of the nodes clustered\\naround the global minimum (in dark blue), and the metadynamic adjustment.\\nColors of embeddings denote potential energy same as in previous illustrations.\\nWe can demonstrate its efficacy on a smaller system–the 8-atom LJ cluster,\\nas illustrated in Figure 9. We still see that the two nodes with potential energies\\nnear -19.2 (colored yellow in Figs 1 and 2) are closely connected, and the nodes\\nrepresented in red and orange are closer to the lowest energy nodes than to\\neach other, but in this case the short distance between the 2 lowest energy\\nnodes reflects a higher transition rate between them. The highest energy nodes,\\nembedded in red, are also embedded separately in this case, whereas their\\nadjacency matrix based embeddings were identical. These embeddings indicate\\nthat a transition path (shown in Fig. 9) from the highest energy node to the\\nslightly lower energy node labeled 5 might pass through nodes 1 and 2 on the\\nway. Direct simulations of this system confirm this transition path. Hence,\\nthese embeddings can be useful for predicting mechanisms by which a molecule\\n15\\nchanges between two configurations.\\nWe now return to the human telomere molecule.\\nFigure 10 shows the\\nembeddings produced via the TPT process described above. The colors of local\\nminima in Figure 10 are determined by the commute times between those nodes\\nand the global minimum, which is embedded in dark blue. It is immediately\\napparent that the local minima are grouped according to these commute times,\\nwith separate clusters containing most of the red, yellow, and light blue nodes.\\nWithin each of these clusters, the nodes are relatively close in terms of commute\\ntimes, indicating that these molecular configurations are similar up to some\\nsimple molecular change. As with the 8-atom LJ cluster, these embeddings\\nsuggest possible transition paths. For example, if the system starts at one of\\nthe states furthest from the global minimum (colored in red, clustered in the\\nupper left of the right figure) one would expect a transition path to the global\\nminimum to travel through the light blue and yellow clusters to reach the node\\nin dark blue. It is also worth noting that the embeddings given by the TPT\\napproach appear to retain more nodes compared to the adjacency matrix-based\\nembeddings in Figure 8, which results from the greater number of nonzero edge\\nweights in the probability current matrix.\\nFigure 10:\\nHierarchical embeddings of the local minima network for the\\nhuman telomere sequence. Left: the embeddings of the full network based on\\nthe adjacency matrix. Right: embeddings of the subnetwork near the global\\nminimum using Metadynamics and TPT. Colors of embeddings denote commute\\ntime distance from the global minimum.\\n5\\nConclusion and future work\\nThis article presents a framework for the analysis of energy landscape data.\\nAdaptive network embeddings that combine the ideas of Metadynamics and TPT\\nwith Node Embedding techniques can be used to aid in the interpretation and\\nsimplification of energy landscape data. In this embedding scheme, the network\\nitself – both its edge weights and the set of nodes under consideration – can\\nbe adjusted to more effectively focus on particular areas of the graph. Future\\n16\\nresearch will involve applying and developing the method for functions of specific\\nmolecules in the context of certain chemical or biological reacting networks.\\nWe anticipate that these energy landscape-based network embeddings would\\nbe used to advance the models currently used to explore the space of small\\nmolecules and identify potential new drugs. For example, the latent variables\\nlearned from molecular energy landscapes could be incorporated into Variational\\nAutoencoders or other generative models as node attributes [18], in much the\\nsame way that 3D representations of molecules are already used. The inclusion\\nof these additional latent variables would lead to a multi-modal generative\\nmethod that takes into account kinetic information of the molecular systems for\\ngenerating more realistic, chemically viable molecules.\\n6\\nDeclarations\\n6.1\\nAcknowledgments\\nThe work is partially supported by NSF-DMS 1720002.\\n6.2\\nAuthors’ contributions\\nMade substantial contributions to conception and design of the study and\\nperformed data analysis and interpretation: Mercurio P\\nMade substantial contributions to conception and design of the study and\\nperformed data analysis and interpretation, as well as provided administrative\\nand material support: Liu D\\n6.3\\nAvailability of data and materials\\nMolecular graphics and analyses performed with UCSF Chimera [17], devel-\\noped by the Resource for Biocomputing, Visualization, and Informatics at the\\nUniversity of California, San Francisco, with support from NIH P41-GM103311.\\nDatabases of local minima and transition states were computed using the\\nPele software [19] and PATHSAMPLE [20] and the disconnectivity trees were\\ndrawn using disconnectionDPS [21].\\n6.4\\nFinancial support and sponsorship\\nThe work is partially supported by NSF-DMS 1720002. https://www.nsf.gov/\\ndiv/index.jsp?div=DMS\\n6.5\\nConflicts of interest\\nAll authors declared that there are no conflicts of interest.\\n6.6\\nEthical approval and consent to participate\\nNot applicable.\\n17\\n6.7\\nConsent for publication\\nNot applicable.\\n6.8\\nCopyright\\n© The Author(s) 2023.\\nReferences\\n[1] C. M. Dobson. Chemical space and biology. Nature, 432:824–828, 2004.\\n[2] J.-L. Reymond. The chemical space project. Accounts of Chemical Research,\\npages 722–730, 2015.\\n[3] David J. Wales. Exploring energy landscapes. Annual Review of Physical\\nChemistry, 69:401–25, 2018.\\n[4] B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online learning of social\\nrepresentations. Proceedings of the ACM SIGKDD International Conference\\non Knowledge Discovery and Data Mining, 3:701–710, 2014.\\n[5] A. Grover and J. Leskovec. Node2vec: Scalable feature learning for networks.\\nKDD : proceedings. International Conference on Knowledge Discovery and\\nData Mining, pages 855–864, 2016.\\n[6] P. Mercurio and D. Liu. Network embedding techniques for metastable\\nchemical kinetic systems. Mathematical Biosciences and Engineering, 18:868–\\n887.\\n[7] P. Mercurio and D. Liu. Efficient network embedding based on sparse\\napproximation of a random walk. Submitted, 2022.\\n[8] M. Parrinello and A. Laio. Escaping free-energy minima. Proc. Natl. Acad.\\nSci., 99:12562, 2002.\\n[9] W. E and E. Vanden-Eijnden. Towards a theory of transition paths. J. Stat.\\nPhys., 123:503–523, 2006.\\n[10] J. Du and D. Liu. Transition states of stochastic chemical reaction networks.\\nComm. Comp. Phys., 29:606–627, 2021.\\n[11] T. Mikolov, I. Sutskever, K. Chen, G.S. Corrado, and J. Dean. Distributed\\nrepresentations of words and phrases and their compositionality. Advances\\nin Neural Information Processing Systems, 26:3111–3119, 2013.\\n[12] E. Hancock and H. Qiu. Clustering and embedding using commute times.\\nIEEE Trans. Pattern Anal. Mach. Intell., 29:1873–1890, 2007.\\n[13] R. Coifman and M. Maggioni. Diffusion wavelets. Applied and Computational\\nHarmonic Analysis, 21:53–94, 2006.\\n18\\n[14] D. J. Wales. The cambridge energy landscape database.\\n[15] T. Cragnolini, D. Chakraborty, J. Šponer, S. Pasquali, and D. Wales.\\nMultifunctional energy landscape for a DNA G-quadruplex: An evolved\\nmolecular switch. Journal of Chemical Physics, 147, 2017.\\n[16] S. Pasquali. HiRE-RNA: a high resolution coarse-grained energy model for\\nRNA. The journal of physical chemistry. B, 114:11957–66, 2010.\\n[17] E.F. Pettersen, T.D. Goddard, and C.C. Huang. UCSF Chimera–a visual-\\nization system for exploratory research and analysis. Journal of Chemical\\nPhysics, 25(13):1605–12, 2004.\\n[18] R. Gomez-Bombarelli, J.N. Wei, D. Duvenaud, J.M. Hernández-Lobato, B.\\nSánchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T.D. Hirzel, R.P.\\nAdams, and A. Aspuru-Guzik. Automatic chemical design using a data-\\ndriven continuous representation of molecules. ACS Cent Sci, 4(2):268–276,\\n2018.\\n[19] J. Stevenson and V. Ruehle. Python energy landscape explorer, 2012.\\n[20] D. Wales. PATHSAMPLE: A driver for OPTIM to create stationary point\\ndatabases using discrete path sampling and perform kinetic analysis.\\n[21] M. Miller, D. Wales, V. de Souza, and Nicy. DisconnectionDPS.\\n19\\n'},\n",
       " {'abstract': 'Recent methods for implicitly representing signals like images, scenes, or geometries using coordinate-based neural network architectures often do not leverage the choice of activation functions, or do so only to a limited extent. In this paper, we introduce the Hyperbolic Oscillation function (HOSC), a novel activation function with a controllable sharpness parameter. Unlike any previous activations, HOSC has been specifically designed to better capture sudden changes in the input signal, and hence sharp or acute features of the underlying data, as well as smooth low-frequency transitions. Due to its simplicity and modularity, HOSC offers a plug-and-play functionality that can be easily incorporated into any existing method employing a neural network as a way of implicitly representing a signal. We benchmark HOSC against other popular activations in an array of general tasks, empirically showing an improvement in the quality of obtained representations, provide the mathematical motivation behind the efficacy of HOSC, and discuss its limitations.',\n",
       "  'introduction': 'An increasingly common scenario in learning visual data representations is approximating a structured signal s: Rk → Rm via a coordinate-based neural network fϱ parametrized by a set of parameters ϱ ∈ Rp. These representations, known as implicit neural representations (INRs), are fully differentiable and offer numerous advantages over traditional counterparts such as meshes or pixel grids in optimization tasks, often requiring significantly less memory.',\n",
       "  'literature_review': 'Three primary strategies to approach this problem while remaining in the INRs framework have been developed:\\n\\n• Hybrid representations. Methods like ACORN (Martel et al., 2021), InstantNGP (Müller et al., 2022) and TensoRF (Chen et al., 2022) use neural networks to achieve highly detailed representations of complex signals, such as gigapixel images and radiance fields. However, they also rely on traditional data structures, and hence require storing some sort of raw data. This notably enlarges their memory footprint compared to just storing the parameters of an MLP, and results in them not being fully differentiable.\\n\\n• Positional encoding. Fourier Feature Networks (FFNs) (Tancik et al., 2020) employ positional encoding, which has been shown to accelerate the learning of higher-frequency features. Such encodings, if sampled densely, become extremely memory inefficient, and therefore require sampling a predefined distribution. This introduces more stochasticity to the model, as well as the need to tune the distribution’s parameters manually.\\n\\n• Periodic activations. Sinusoidal Representation Networks (SIRENs) proposed by Sitzmann et al. (2020) are multi-layer perceptrons (MLPs) that utilize sin(x) instead of ReLU as their activation function. Consequently, they remain fully differentiable and offer a compact representation of the signal. While SIRENs demonstrated a significant improvement over ReLU, they struggle to capture high-frequency details in problems like shape representations, and are not-well suited for methods such as (Mildenhall et al., 2020).',\n",
       "  'methodology': 'In this paper, we introduce a new periodic parametric activation function — the Hyperbolic Oscillation activation function (HOSC), defined as HOSC(x; β) = tanh(β sin x). Here, β > 0 is a controllable sharpness parameter, enabling HOSC to seamlessly transition between a smooth sine-like wave and a square signal. Similarly to SIREN, an MLP running the HOSC activation function is fully differentiable and inexpensive memorywise. However, the HOSC’s sharpness parameter β allows it to much more accurately capture sudden or sharp jumps, and hence preserve high-frequency details of the signal. Moreover, since HOSC is differentiable with respect to β, the sharpness can be adjusted automatically alongside the reset of the parameters, a method to which we refer as Adaptive HOSC or AdaHOSC.',\n",
       "  'results': 'Our extensive empirical studies show that HOSC consistently outperforms ReLU and SIREN across an array of benchmarking tasks. These tasks encompass fitting random signals, images of random square patches, photos, gigapixel images, and 2D & 3D SDF. In summary, HOSC provides an easy-to-implement method allowing simple MLPs to achieve high level of detail in signal encoding tasks without loosing differentiability or increasing memory footprint, and it does this without the need for positional encoding.',\n",
       "  'conclusion': 'In this paper, we have introduced the Hyperbolic Oscillation activation function HOSC(x; β) = tanh(β sin x), a new periodic parametric activation that has been designed to be particularly effective in preserving sharp features in INRs. Additionally, in Section 4, we presented experimental results that evaluate the performance of the HOSC function in comparison to existing approaches.\\n\\nOur findings revealed that an MLP employing the HOSC activation with a suitably chosen or automatically-optimized sharpness parameter β consistently outperforms identical structure MLPs using ReLU and SIREN activations, and achieve the same level of accuracy in neural signal encoding problems as Fourier Feature Networks. HOSC thus offers a simple, fully differentiable and compact high-quality signals representation method with no need for hyperparameter tuning. However, we have identified scenarios where HOSC is clearly not the optimal choice, which we will explore in the following discussion.',\n",
       "  'title': 'HOSC: A Periodic Activation Function for Preserving Sharp Features in Implicit Neural Representations',\n",
       "  'author': 'Danzel Serrano, Jakub Szymkowiak, Przemyslaw Musialski',\n",
       "  'textdata': 'Preprint, submission in preparation\\npage 1\\nHOSC: A PERIODIC ACTIVATION FUNCTION\\nFOR\\nPRESERVING SHARP FEATURES IN IMPLICIT NEURAL\\nREPRESENTATIONS\\nDanzel Serrano\\nNew Jersey Institute of Technology∗\\nds867@njit.edu\\nJakub Szymkowiak\\nAdam Mickiewicz University†\\nIDEAS NCBR‡\\njakub.szymkowiak@ideas-ncbr.pl\\nPrzemyslaw Musialski\\nNew Jersey Institute of Technology\\nIDEAS NCBR\\nprzem@njit.edu\\nABSTRACT\\nRecently proposed methods for implicitly representing signals such as images,\\nscenes, or geometries using coordinate-based neural network architectures often\\ndo not leverage the choice of activation functions, or do so only to a limited ex-\\ntent. In this paper, we introduce the Hyperbolic Oscillation function (HOSC), a\\nnovel activation function with a controllable sharpness parameter. Unlike any pre-\\nvious activations, HOSC has been specifically designed to better capture sudden\\nchanges in the input signal, and hence sharp or acute features of the underlying\\ndata, as well as smooth low-frequency transitions. Due to its simplicity and mod-\\nularity, HOSC offers a plug-and-play functionality that can be easily incorporated\\ninto any existing method employing a neural network as a way of implicitly rep-\\nresenting a signal. We benchmark HOSC against other popular activations in an\\narray of general tasks, empirically showing an improvement in the quality of ob-\\ntained representations, provide the mathematical motivation behind the efficacy of\\nHOSC, and discuss its limitations.\\n1\\nINTRODUCTION\\nAn increasingly common scenario in learning visual data representations is approximating a struc-\\ntured signal s: Rk → Rm via a coordinate-based neural network fθ parametrized by a set of pa-\\nrameters θ ∈ Rp. These representations, known as implicit neural representations (INRs), are fully\\ndifferentiable and offer numerous advantages over traditional counterparts such as meshes or pixel\\ngrids in optimization tasks, often requiring significantly less memory.\\nINRs are versatile in their application, capable of representing a variety of types of objects, including\\naudio signals (k, m = 1), images (k = 2, m = 1 or m = 3), radiance fields (k = 5, m = 4),\\ngeometries (k = 2 or k = 3, m = 1), and parametrzied curves (k = 1, m > 1). For instance, to\\nrepresent the geometry of a 3D object, one would obtain a dataset of evaluations X = {(x, s(x))}\\nof the signed distance function s(x) = sdf(x) with respect to the surface of that object, and find the\\n∗https://www.njit.edu/\\n†https://www.amu.edu.pl/\\n‡https://www.ideas-ncbr.pl/\\narXiv:2401.10967v1  [cs.NE]  20 Jan 2024\\nPreprint, submission in preparation\\npage 2\\n(a) Ground Truth\\n(b) ReLU(x)\\n(c) sin(x)\\n(d) HOSC(x; 8)\\nFigure 1: Reconstruction of an image using an MP running different activation functions. The\\nprocess involved training a five-layer coordinate-based MLP with a width of 256 for 100 iterations\\nfor each of the activations. No positional encoding and no frequency initialization has been used.\\nvalues of parameters θ that minimize the reconstruction loss:\\nL(θ) = min\\nθ\\nE(x,s(x))∼X[∥fθ(x) − s(x)∥2 + Ψ(θ)] ,\\nwhere Ψ(θ) denotes a regularizer. Instead of a regression, this task could also be posed as a clas-\\nsification problem, where the signal s(x) takes values in the discrete set {0, 1}, representing an\\noccupancy field. In general, defining an appropriate domain, codomain, loss function, and regular-\\nization is a problem-specific research challenge.\\nImportantly, INRs introduce a new paradigm in training neural networks. In classical applications\\nof neural networks, such as prediction, the goal is to approximate a function f given its noisy eval-\\nuations f(x) at sparsely sampled datapoints x. One of the challenges is thus not to overfit the\\napproximation to the noise present in the training data. On the contrary, for INRs, we assume the\\ndata is noise-free and more regularly sampled, and aim to encode this into the network’s parameters,\\nimplying that in this context overfitting is actually desirable for capturing high-frequency details of\\nthe signal.\\nHowever, popular activation functions such as ReLU are biased towards capturing lower frequencies,\\nwhich is beneficial in prediction tasks, but hinders their capability to accurately represent sharp\\nfeatures of signals when applied as INRs. Three primary strategies to approach this problem while\\nremaining in the INRs framework have been developed:\\n• Hybrid representations. Methods like ACORN (Martel et al., 2021), InstantNGP (M¨uller\\net al., 2022) and TensoRF (Chen et al., 2022) use neural networks to achieve highly detailed\\nrepresentations of complex signals, such as gigapixel images and radiance fields. However,\\nthey also rely on traditional data structures, and hence require storing some sort of raw data.\\nThis notably enlarges their memory footprint compared to just storing the parameters of an\\nMLP, and results in them not being fully differentiable.\\n• Positional encoding. Fourier Feature Networks (FFNs) (Tancik et al., 2020) employ po-\\nsitional encoding, which has been shown to accelerate the learning of higher-frequency\\nfeatures. Such encodings, if sampled densely, become extremely memory inefficient, and\\ntherefore require sampling a predefined distribution. This introduces more stochasticity to\\nthe model, as well as the need to tune the distribution’s parameters manually.\\n• Periodic activations. Sinusoidal Representation Networks (SIRENs) proposed by Sitz-\\nmann et al. (2020) are multi-layer perceptrons (MLPs) that utilize sin(x) instead of ReLU\\nas their activation function. Consequently, they remain fully differentiable and offer a com-\\npact representation of the signal. While SIRENs demonstrated a significant improvement\\nover ReLU, they struggle to capture high-frequency details in problems like shape repre-\\nsentations, and are not-well suited for methods such as (Mildenhall et al., 2020).\\nIn this paper, we introduce a new periodic parametric activation function — the Hyperbolic Os-\\ncillation activation function (HOSC), defined as HOSC(x; β) = tanh(β sin x). Here, β > 0 is a\\ncontrollable sharpness parameter, enabling HOSC to seamlessly transition between a smooth sine-\\nlike wave and a square signal. Similarly to SIREN, an MLP running the HOSC activation function is\\nPreprint, submission in preparation\\npage 3\\n(a) sin x\\n(b) sign(sin x)\\n(c) HOSC(x; β)\\nFigure 2: Comparison of the sine, square, and HOSC waves for different values of the sharpness\\nparameter β ∈ {1, 2, 4, 16}. As β increases, HOSC starts to resemble a square wave.\\nfully differentiable and inexpensive memorywise. However, the HOSC’s sharpness parameter β al-\\nlows it to much more accurately capture sudden or sharp jumps, and hence preserve high-frequency\\ndetails of the signal. Moreover, since HOSC is differentiable with respect to β, the sharpness can\\nbe adjusted automatically alongside the reset of the parameters, a method to which we refer as\\nAdaptive HOSC or AdaHOSC.\\nOur extensive empirical studies show that HOSC consistently outperforms ReLU and SIREN across\\nan array of benchmarking tasks. These tasks encompass fitting random signals, images of random\\nsquare patches, photos, gigapixel images, and 2D & 3D SDF. In summary, HOSC provides an easy-\\nto-implement method allowing simple MLPs to achieve high level of detail in signal encoding tasks\\nwithout loosing differentiability or increasing memory footprint, and it does this without the need\\nfor positional encoding.\\n2\\nRELATED WORK\\n2.1\\nIMPLICIT NEURAL REPRESENTATIONS\\nCurrently, INRs are gaining a lot of attention in visual computing research (Xie et al., 2021). Their\\napplications are widespread, and encompass image processing (Tancik et al., 2020), radiance fields\\n(Mildenhall et al., 2020), 3D shape modeling (Park et al., 2019), audio and video compression\\n(Lanzend¨orfer & Wattenhofer, 2023; Chen et al., 2021), physics-informed problems (Raissi et al.,\\n2019), and solving PDEs (Sitzmann et al., 2020; Li et al., 2020). There are many reasons for choos-\\ning INRs over classical data structures:\\n• Differentiability. Given their differentiable nature, INRs offer an immediate advantage\\nover classical, non-differentiable methods in optimization and deep learning tasks.\\n• Compactness. INRs often require less memory, as storing the parameters and hyperparam-\\neters of a neural network is typically less memory-intensive than storing raw data.\\n• Continuous representation. In principle, due to their generalization capability, neural\\nnetworks enable the representation of data with arbitrary precision, making resolution a\\nless significant issue (Chen et al., 2020).\\nFor a more comprehensive review of the INR literature, we refer to the recent surveys by Tewari\\net al. (2020), Tewari et al. (2021), and Xie et al. (2021).\\nShape and geometry representation. Classical methods of shape and geometry representation\\ninclude voxel grids, polygonal meshes and point clouds. However, all of these methods suffer from\\nlimitations. Voxel grids are subject to the curse of dimensionality, which makes them inefficient\\nin handling high-resolution data. Moreover, manipulating voxel grids and dense meshes can be\\ncomputationally intensive (Xiao et al., 2020; Kato et al., 2017). Meshes are also prone to errors, and\\ndesigning a mesh can be quite time-consuming for human creators. As for point clouds, they do not\\nencode topological information (Kato et al., 2017). These issues have prompted the exploration of\\nINRs in the context of shape and geometry modeling. The seminal work by Park et al. (2019) has\\ndemonstrated that INRs are capable of accurately representing surfaces as signed distance functions.\\nFurther research in this direction has been conducted by Atzmon & Lipman (2019), Michalkiewicz\\net al. (2019) and Gropp et al. (2020). Another option is presented by occupancy networks, which\\nmodel the shape as the decision boundry of a binary classifier implemented as a neural network\\n(Mescheder et al., 2018; Chen & Zhang, 2018).\\nPreprint, submission in preparation\\npage 4\\nEncoding appearence. In addition to encoding geometry, coordinate-based neural networks are\\nalso capable of representing the appearence aspects. For instance, Texture Fields (Oechsle et al.,\\n2019) enable coloring any 3D shape based on an image. Methods such as LIIF (Chen et al., 2020)\\nand ACORN (Martel et al., 2021) are effective in representing high-resolution gigapixel images.\\nFurthermore, by addressing the inverse problem, Neural Radiance Fields (Mildenhall et al., 2020)\\nallow for reconstruction of multidimensional scenes from a collection of 2D images. Other signif-\\nicant contribution in this area include (M¨uller et al., 2022; Chen et al., 2023a; 2022; Martel et al.,\\n2021). A lot of these and similar methods are hybrid representations that combine neural networks\\nwith classical non-differentiable data structres. As such, they are not directly related to HOSC,\\nwhich primarily focuses on fully differentiable architectures.\\n2.2\\nACTIVATION FUNCTIONS AND PERIODICITY\\nActivation functions. Activations are essential for neural networks to be able to model non-linear\\nrelationships. Early activation functions include the Logistic Sigmoid, Hyperbolic Tangent, and\\nRectified Linear Unit (ReLU). Thanks to their low-frequency bias, they are able to deal with the\\nnoise present in the training data, possess generalization capabilities, and thus excel in applications\\nsuch as prediction. In contrast to these early non-linearities, more recently proposed activation\\nfunctions such as SWISH (Ramachandran et al., 2018), PReLU (He et al., 2015), SReLU (Jin et al.,\\n2015), and MPELU (Li et al., 2016) incorporate one or more parameters, which are optimized during\\ntraining along with the rest of the network’s parameters. For a more in-depth survey on activation\\nfunctions, refer to (Dubey et al., 2021; Apicella et al., 2020; Karlik & Olgac, 2011).\\nPeriodicity in neural networks. All the non-linearities mentioned in the previous section are non-\\nperiodic. Altough less common, periodic activations have been studied for many years. Early work\\nby Sopena et al. (1999) and Wong et al. (2002) analyzed their performance in classification problems.\\nA more recent study by Parascandolo et al. (2017) investigated which tasks are particularly well-\\nsuited to periodic activations and where they may face challenges. In (Lapedes & Farber, 1987),\\nthe authors use an MLP with sine activation for signal modeling, drawing a direct connection to\\nthe Fourier transform. A significant contribution in the field of INRs is the SIREN architecture\\n(Sitzmann et al., 2020), which employes sine activation to solve PDEs and encode images and\\nvideos. Various aspects of periodic activations have also been studied by Ramasinghe & Lucey\\n(2021). An alternative approach to introducing periodicity has been explored by Tancik et al. (2020),\\nwho generalize positional encoding to coordinate-based MLPs.\\n3\\nOUR CONTRIBUTION\\n3.1\\nHOSC\\nIn this paper, we propose a novel periodic parametric activation function designed specifically for\\nfitting INRs — the Hyperbolic Oscillation activation function, or HOSC. It is defined as\\nHOSC(x; β) = tanh(β sin x) ,\\nwhere β > 0 is the sharpness parameter, controlling the extent to which the resulting wave resembles\\na square wave. This phenomenon is illustrated in Figure 2. In fact, given that limβ→∞ tanh(βx) =\\nsign(x) for all x ∈ R, we know that limβ→∞ HOSC(x; β) = sign(sin x), so indeed HOSC ap-\\nproaches the square wave pointwise in the infinite sharpness limit. The rapid amplitude changes\\naround x = nπ for n ∈ N at high values of β enable HOSC to model acute features of the sig-\\nnal. Conversely, smooth transitions at lower β values allow it to capture low-frequency components\\ninstead.\\n3.2\\nADAHOSC\\nImportantly, HOSC is differentiable not only with respect to the input x, but also with respect to the\\nsharpness parameter β:\\n∂β HOSC(x; β) = sin(x)\\nPreprint, submission in preparation\\npage 5\\nFigure 3: Comparison of a ReLU, SIREN, and HOSC fitting the ’Cameraman’ image for 1000\\nepochs and a high-frequency detail ’Cat’ image for 5000 epochs. The plot to the right shows PSNRs\\nof the model to ground truth per epoch of training; for the ’Cat’ we used adaptively scheduled\\nlearning rate. Below each resulting model is the residual difference from the ground truth signal.\\n4\\nEXPERIMENTAL RESULTS\\nIn this section, we experimentally assess the performance of HOSC in various benchmarking tests\\nand compare it to ReLU and SIREN. More experimental results can be found in the Appendix.\\n4.1\\nREPRESENTING IMAGES (s: R2 → R OR R3)\\nAn image can be conceptualized as a function I: R2 → Rn, where n = 1 (for black and white\\nimages) or n = 3 (in case of the colored images), mapping pixel coordinates to their corresponding\\ncolor intensities. To construct an INR, one commonly approximates the function I with an MLP,\\ntraining it on all the available coordinate-color value pairs ((x, y), I(x, y)).\\nIn Figures 1 and 3 we present the results of fitting photos with an MLP running the HOSC activation.\\nIn Figure 3, the black and white cameraman image is fitted for 1000 epochs, demonstrating that an\\nMLP employing HOSC activation achieves a higher PSNR quicker than a ReLU-MLP or SIREN.\\nWe also note that we employed a linear step-wise learning rate scheduler with a rate of γ = 0.1 every\\n2000 epochs, as the HOSC-MLP begins to exhibit an extreme oscillatory convergence at high PSNR\\nvalues without this adjustment. Although we confirm that the Gaussian activation performs better\\nthan the SIREN model in this case, our findings reveal that a HOSC-MLP achieves a significantly\\nhigher PSNR compared to both.\\nFigure 5 reveals more interesting results. This experiment evaluates the performance of the HOSC\\non images with varying frequency content. For each activation, a four-layer MLPs with a width of\\n256 was trained on a 256 × 256 black images, each containing 100 randomly placed white square\\npatches, over 5000 epochs. Patch sizes used in the experiment are 1 × 1, 4 × 4, and 16 × 16. For the\\nSIREN model, we adopted the same weight initialization and a frequency factor of 30, as detailed in\\nPreprint, submission in preparation\\npage 6\\n(Sitzmann et al., 2020). For the HOSC-MLP, we use a sharpness factor schedule, where sharpness\\nvaries across layers with values βi = [2, 4, 8, 16]. Additionally, a frequency factor of 30 is used in\\nthe first layer, followed by a factor of 1.0 in the subsequent layers.\\nAs anticipated, the ReLU network struggles to accurately capture the ground truth signal, resulting\\nin a low converging PSNR. Interestingly, as the patch size increases, ReLU’s peak PSNR decreases.\\nIn contrast, for SIREN, the peak PSNR increases as the ground truth signal losses sharp frequencies.\\nThe HOSC-MLP surpasses both of them, and is able to accurately represent the ground truth signal\\nregardless of patch sizes. Notably, the PSNR values for the HOSC-MLP, after being trained for 5000\\nepochs, significantly exceed those of both the ReLU-MLP and SIREN models.\\n4.1.1\\nGIGAPIXEL IMAGES\\nFigure 4: Top: Results of fitting a HOSC and a SIREN model to a high-resolution image. Bottom:\\nA plot of PSNR per epoch for both methods.\\nIn this experiment, we compare the performances of SIREN and HOSC models on the task of\\nGigapixel Image Approximation, where the target signal is an RGB Image of extremely high-\\nresolution. In this case, the SIREN model is of depth 4, with a width of 256, and each sinusoidal\\nactivation has a frequency of 30. The HOSC model has the same depth and width, however, the\\nactivations have a sharpness of β = 8, while only the first activation has a frequency of 30, and the\\nrest has a frequency of 1. We follow the same initialization scheme as Sitzmann et al. (2020) for\\nboth the SIREN and HOSC models.\\nResults of fitting both models to an image of resolution 9302×8000×3 for 100 epochs are shown in\\nFigure 4. Although both models do not look perceptually close to the zoomed in reference photo, it is\\napparent that a model equipped with HOSC is able to retain sharper features for the high-resolution\\nimage whereas the SIREN essentially learns a smooth interpolation. This fact is also reinforced by\\nanalyzing the PSNR plots, where HOSC beats SIREN even at the first few epochs.\\n4.2\\nSDFS (s: R2 OR R3 → R)\\nThe signed distance sdfΩ(x) with respect to a shape Ω ⊂ Rn is defined as\\nsdfΩ(x) =\\n\\x1a+ρ(x, ∂Ω)\\nif x ̸∈ Ω ,\\n−ρ(x, ∂Ω)\\nif x ∈ Ω ,\\nfor all the coordinates x ∈ Rn. Consequently, by training a neural network fθ to approximate sdfΩ,\\nwe can approximate the shape’s boundary as the zero-level set {x ∈ Rn | fθ(x) = 0}.\\nPreprint, submission in preparation\\npage 7\\nFigure 5: Comparison of a ReLU, SIREN, and HOSC fitting an image of random square patches of\\ndimension 1x1, 4x4, and 16x16. The plot to the right shows PSNRs of the model to ground truth per\\nepoch of training.\\nIn this section, we consider the case where n = 2 and n = 3 and train MLPs on a dataset\\n{(x, sdf(x))} comprised of coordinate-SDF evaluation pairs. Our experimental exploration aims\\nto identify any patterns that emerge when varying the depth and width of the MLPs, as well as\\nadjusting the sharpness factor β (2D SDF).\\nIn Figure 7 we present a comparison of AdaHOSC to ReLU and SIREN. For a fair comparison,\\nwe run the same MLP architecture (5 hidden layers with 256 width) for 20 epochs only changing\\nthe activation. AdaHOSC uses the initial value of β = 8. We find that AdaHOSC provides a\\nmuch higher quality of representation, as demonstrated by the IoU (intersection over union) values,\\nsuggesting faster convergence time compared to SIREN.\\nMoreover, the results of the 2D SDF experiment are illustrated in Figure 6. In this experiment,\\nwe train four-layer 512 width MLPs on 20 SDF evaluations of a regular star shape. Similar to\\nimage fitting, HOSC’s performance surpasses that of ReLU and SIREN. Moreover, our findings\\nindicate that deep HOSC-MLPs achieve higher PSNR values. Regardless of depth, it is observed\\nthat, depending on the width, greater β values enable HOSC to more accurately represent the shape,\\nas evidenced by the PSNR values. This observation further supports the hypothesis that HOSC\\ncan effectively represent signals with high-frequency content (including discontinuities) when the\\nsharpness factor is large.\\nFigure 6: Comparison of coordinate-based MLPs fitting an SDF of a regular star. Top: SDFs learned\\nby the models running different activations. Bottom: Heatmap illustrating the maximum PSNR\\nvalues for HOSC-MLPs with different topologies (depth and width of layers) and sharpness factors.\\n5\\nCONCLUSIONS AND DISCUSSION\\nIn this paper, we have introduced the Hyperbolic Oscillation activation function HOSC(x; β) =\\ntanh(β sin x), a new periodic parametric activation that has been designed to be particularly effec-\\nPreprint, submission in preparation\\npage 8\\ntive in preserving sharp features in INRs. Additionally, in Section 4, we presented experimental\\nresults that evaluate the performance of the HOSC function in comparison to existing approaches.\\nOur findings revealed that an MLP employing the HOSC activation with a suitably chosen or\\nautomatically-optimized sharpness parameter β consistently outperforms identical structure MLPs\\nusing ReLU and SIREN activations, and achieve the same level of accuracy in neural signal en-\\ncoding problems as Fourier Feature Networks. HOSC thus offers a simple, fully differentiable and\\ncompact high-quality signals representation method with no need for hyperparameter tuning. How-\\never, we have identified scenarios where HOSC is clearly not the optimal choice, which we will\\nexplore in the following discussion.\\nSpectral bias. Different problems require a different spectral bias. While for signal encoding,\\nwhere we assume that there is very little or no noise present in the data, fitting high-frequency\\ncomponents of the signal is advantageous. Conversely, for capturing only the general trends from a\\nset of noisy data, a low-frequency bias can help avoid overfitting to noise. Naturally, this constraints\\nthe application of HOSC and other periodic activations in settings that require generalization beyond\\nthe observed datapoints. For instance, in our experiments we found that both HOSC and SIREN\\nactivations underperform when applied to Neural Radiance Fields (Mildenhall et al., 2020), which\\nperform best with various types of positinoal encoding, either in freqnecty domain, or parametric\\nencoding combined with spatial data strcutres (also denoted as hybrid) and ReLU activations.\\nSolving PDEs. Cooridnate-based neural networks have been applied to solving PDEs in physics\\n(Raissi et al., 2019). However, we observe that HOSC is not particularly suited for these types\\nof problems, compared to e.g. SIREN architecture. We attribute this limitation to the increasing\\ncomplexity found in subsequent derivatives of HOSC. While the derivatives of a fully-connected\\nSIREN layer remain SIRENs (Sitzmann et al., 2020), enabling it to accurately fit both the signal\\nand its derivatives, the situation is more convoluted. As a result, HOSC preserves the signal’s sharp\\nfeatures but at the expense of derivative information.\\nHybrid and parametric positional encoding. In our experiments we apply HOSC in signal encod-\\ning scenarios, like images, giga-pixel images, and 3D SDFs, where a HOSC-MLP achieves similar\\nreconstruction quality, however, not the timings of highly optimized methods as InstantNGP (M¨uller\\net al., 2022), ACORN (Martel et al., 2021), grid-based Dictionary Fields (Chen et al., 2023a), or Ten-\\nsoRF (Chen et al., 2022). There methods shorten the training and inference times at the cost of a\\nhigher memory footprint and a more sophisticated implementation. In contrast, a simple MLP is\\nmuch easier to implement and storing its parameters is far less demanding in terms of memory.\\nFinally, it is important to note that, in principle HOSC can be utilized in hybrid representations as\\nwell, whenever a coordinate-based MLP is used to overfit a signal.\\nArchitecture design. A deeper understanding of how neural networks represent implicitly encoded\\nsignals may also provide greater insights into the design of non-MLP network architectures, enabling\\nHOSC to fully leverage their capabilities. The research presented in Ramasinghe & Lucey (2021)\\noffers intresting ideas relevant to this context. Furthermore, HOSC naturally fits in the Factor Fields\\nframework (Chen et al., 2023b). More specifically, we can let any factor s in a Factor Field be\\nmodeled with a HOSC MLP, and vary the sharpness β across the factors. This means that the\\nframework could potentially be used to develop novel representation methods using HOSC, possibly\\ncombined with other architectures.\\nPreprint, submission in preparation\\npage 9\\nLucy\\nIoU, train time\\nReLU\\n0.8969, 34 min\\nSIREN\\n0.7263, 34 min\\nAdaHOSC\\n0.9804, 35 min\\nStatuette\\nIoU, train time\\nReLU\\n0.8778, 34 min\\nSIREN\\n0.8168, 34 min\\nAdaHOSC\\n0.9587, 38 min\\nArmadillo\\nIoU, train time\\nReLU\\n0.9328, 35 min\\nSIREN\\n0.9153, 36 min\\nAdaHOSC\\n0.9775, 37 min\\nDragon\\nIoU, train time\\nReLU\\n0.8969, 34 min\\nSIREN\\n0.8263, 34 min\\nAdaHOSC\\n0.9628, 35 min\\nFigure 7: Comparison of HOSC to other methods in 3D SDF reconstruction. All examples trained\\non 5-layer, 256 hidden neurons model for 20 epochs. Note that HOSC is showing superior re-\\nconstruction quality, which is within the range of methods utilizing positional encoding, like Dic-\\ntionary Fields (Chen et al., 2023b).\\nFor the evaluation of the IoU, we used the dataset from\\nhttps://github.com/autonomousvision/factor-fields. Note that Lucy was evaluated only on a 512 res-\\nolution mesh.\\nPreprint, submission in preparation\\npage 10\\nREFERENCES\\nAndrea Apicella, Francesco Donnarumma, Francesco Isgr`o, and Roberto Prevete. A survey on\\nmodern trainable activation functions. Neural networks : the official journal of the International\\nNeural Network Society, 138:14–32, 2020. URL https://api.semanticscholar.org/\\nCorpusID:218487292. 4\\nMatan Atzmon and Yaron Lipman. Sal: Sign agnostic learning of shapes from raw data. 2020\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2562–2571,\\n2019. URL https://api.semanticscholar.org/CorpusID:208267630. 3\\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su.\\nTensorf: Tensorial radi-\\nance fields. ArXiv, abs/2203.09517, 2022. URL https://api.semanticscholar.org/\\nCorpusID:247519170. 2, 4, 8\\nAnpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su, and Andreas Geiger. Dictionary fields:\\nLearning a neural basis decomposition. ACM Transactions on Graphics (TOG), 42:1 – 12, 2023a.\\nURL https://api.semanticscholar.org/CorpusID:260167858. 4, 8\\nAnpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su, and Andreas Geiger. Factor fields: A\\nunified framework for neural fields and beyond. ArXiv, abs/2302.01226, 2023b. URL https:\\n//api.semanticscholar.org/CorpusID:256503583. 8, 9\\nHao Chen, Bo He, Hanyu Wang, Yixuan Ren, Ser-Nam Lim, and Abhinav Shrivastava. Nerv: Neural\\nrepresentations for videos. In Neural Information Processing Systems, 2021. URL https:\\n//api.semanticscholar.org/CorpusID:239885704. 3\\nYinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local\\nimplicit image function. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recog-\\nnition (CVPR), pp. 8624–8634, 2020.\\nURL https://api.semanticscholar.org/\\nCorpusID:229221619. 3, 4\\nZhiqin Chen and Hao Zhang.\\nLearning implicit fields for generative shape modeling.\\n2019\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5932–5941,\\n2018. URL https://api.semanticscholar.org/CorpusID:54457478. 3\\nShiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran Chaudhuri. Activation functions in deep\\nlearning: A comprehensive survey and benchmark. Neurocomputing, 503:92–108, 2021. URL\\nhttps://api.semanticscholar.org/CorpusID:250089226. 4\\nAmos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman.\\nImplicit geometric\\nregularization for learning shapes.\\nArXiv, abs/2002.10099, 2020.\\nURL https://api.\\nsemanticscholar.org/CorpusID:211259068. 3\\nKaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing\\nhuman-level performance on imagenet classification. 2015 IEEE International Conference on\\nComputer Vision (ICCV), pp. 1026–1034, 2015. URL https://api.semanticscholar.\\norg/CorpusID:13740328. 4\\nXiaojie Jin, Chunyan Xu, Jiashi Feng, Yunchao Wei, Junjun Xiong, and Shuicheng Yan. Deep learn-\\ning with s-shaped rectified linear activation units. In AAAI Conference on Artificial Intelligence,\\n2015. URL https://api.semanticscholar.org/CorpusID:10520992. 4\\nBekir Karlik and A. Vehbi Olgac. Performance analysis of various activation functions in general-\\nized mlp architectures of neural networks. 2011. URL https://api.semanticscholar.\\norg/CorpusID:174791561. 4\\nHiroharu Kato, Y. Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. 2018 IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition, pp. 3907–3916, 2017.\\nURL https:\\n//api.semanticscholar.org/CorpusID:32389979. 3\\nLuca A. Lanzend¨orfer and Roger Wattenhofer.\\nSiamese siren:\\nAudio compression with\\nimplicit neural representations.\\nArXiv, abs/2306.12957, 2023.\\nURL https://api.\\nsemanticscholar.org/CorpusID:259224407. 3\\nPreprint, submission in preparation\\npage 11\\nAlan S. Lapedes and Robert M. Farber. Nonlinear signal processing using neural networks: Pre-\\ndiction and system modelling.\\n1987.\\nURL https://api.semanticscholar.org/\\nCorpusID:60720876. 4\\nYang Li, Chunxiao Fan, Yong Li, and Qiong Wu.\\nImproving deep neural network with multi-\\nple parametric exponential linear units. ArXiv, abs/1606.00305, 2016. URL https://api.\\nsemanticscholar.org/CorpusID:9248703. 4\\nZong-Yi Li, Nikola B. Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya,\\nAndrew M. Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differ-\\nential equations. ArXiv, abs/2010.08895, 2020. URL https://api.semanticscholar.\\norg/CorpusID:224705257. 3\\nJulien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric Chan, Marco Monteiro, and Gordon\\nWetzstein. Acorn. ACM Transactions on Graphics (TOG), 40:1 – 13, 2021. URL https:\\n//api.semanticscholar.org/CorpusID:233864500. 2, 4, 8\\nLars M. Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.\\nOccupancy networks: Learning 3d reconstruction in function space. 2019 IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR), pp. 4455–4465, 2018.\\nURL https:\\n//api.semanticscholar.org/CorpusID:54465161. 3\\nMateusz Michalkiewicz, Jhony Kaesemodel Pontes, Dominic Jack, Mahsa Baktash, and Anders P.\\nEriksson. Implicit surface representations as layers in neural networks. 2019 IEEE/CVF In-\\nternational Conference on Computer Vision (ICCV), pp. 4742–4751, 2019.\\nURL https:\\n//api.semanticscholar.org/CorpusID:207985696. 3\\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi,\\nand Ren Ng. Nerf. Communications of the ACM, 65:99 – 106, 2020. URL https://api.\\nsemanticscholar.org/CorpusID:213175590. 2, 3, 4, 8\\nThomas M¨uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\\nitives with a multiresolution hash encoding. ACM Transactions on Graphics (TOG), 41:1 – 15,\\n2022. URL https://api.semanticscholar.org/CorpusID:246016186. 2, 4, 8\\nMichael Oechsle, Lars M. Mescheder, Michael Niemeyer, Thilo Strauss, and Andreas Geiger.\\nTexture fields: Learning texture representations in function space.\\n2019 IEEE/CVF Interna-\\ntional Conference on Computer Vision (ICCV), pp. 4530–4539, 2019. URL https://api.\\nsemanticscholar.org/CorpusID:158046789. 4\\nGiambattista Parascandolo, Heikki Huttunen, and Tuomas Virtanen. Taming the waves: sine as ac-\\ntivation function in deep neural networks. 2017. URL https://api.semanticscholar.\\norg/CorpusID:126004626. 4\\nJeong Joon Park, Peter R. Florence, Julian Straub, Richard A. Newcombe, and S. Lovegrove.\\nDeepsdf:\\nLearning continuous signed distance functions for shape representation.\\n2019\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 165–174, 2019.\\nURL https://api.semanticscholar.org/CorpusID:58007025. 3\\nMaziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics-informed neural networks:\\nA deep learning framework for solving forward and inverse problems involving nonlinear par-\\ntial differential equations.\\nJ. Comput. Phys., 378:686–707, 2019.\\nURL https://api.\\nsemanticscholar.org/CorpusID:57379996. 3, 8\\nPrajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. ArXiv,\\nabs/1710.05941,\\n2018.\\nURL https://api.semanticscholar.org/CorpusID:\\n10919244. 4\\nSameera Ramasinghe and Simon Lucey.\\nBeyond periodicity: Towards a unifying framework\\nfor activations in coordinate-mlps.\\nArXiv, abs/2111.15135, 2021.\\nURL https://api.\\nsemanticscholar.org/CorpusID:244729036. 4, 8\\nPreprint, submission in preparation\\npage 12\\nVincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wet-\\nzstein. Implicit neural representations with periodic activation functions. ArXiv, abs/2006.09661,\\n2020. URL https://api.semanticscholar.org/CorpusID:219720931. 2, 3, 4,\\n6, 8\\nJosep M. Sopena, Enrique Romero, and Ren´e Alqu´ezar.\\nNeural networks with periodic and\\nmonotonic activation functions: a comparative study in classification problems. 1999. URL\\nhttps://api.semanticscholar.org/CorpusID:122973551. 4\\nMatthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan,\\nUtkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let net-\\nworks learn high frequency functions in low dimensional domains. ArXiv, abs/2006.10739, 2020.\\nURL https://api.semanticscholar.org/CorpusID:219791950. 2, 3, 4\\nAnju Tewari, Otto Fried, Justus Thies, Vincent Sitzmann, S. Lombardi, Z. Xu, Tanaba Simon,\\nMatthias Nießner, Edgar Tretschk, L. Liu, Ben Mildenhall, Pranatharthi Srinivasan, R. Pandey,\\nSergio Orts-Escolano, S. Fanello, M. Guang Guo, Gordon Wetzstein, J y Zhu, Christian Theobalt,\\nManju Agrawala, Donald B. Goldman, and Michael Zollh¨ofer.\\nAdvances in neural render-\\ning. Computer Graphics Forum, 41, 2021. URL https://api.semanticscholar.org/\\nCorpusID:236162433. 3\\nAyush Kumar Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan\\nSunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason M. Saragih, Matthias Nießner, Rohit\\nPandey, S. Fanello, Gordon Wetzstein, Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala,\\nEli Shechtman, Dan B. Goldman, and Michael Zollhofer.\\nState of the art on neural render-\\ning. Computer Graphics Forum, 39, 2020. URL https://api.semanticscholar.org/\\nCorpusID:215416317. 3\\nKwokwo Wong, Andrew Chi-Sing Leung, and Shen jiang Chang. Handwritten digit recognition\\nusing multilayer feedforward neural networks with periodic and monotonic activation functions.\\nObject recognition supported by user interaction for service robots, 3:106–109 vol.3, 2002. URL\\nhttps://api.semanticscholar.org/CorpusID:16161892. 4\\nYun-Peng Xiao, Yu-Kun Lai, Fang-Lue Zhang, Chunpeng Li, and Lin Gao. A survey on deep\\ngeometry learning: From a representation perspective. Computational Visual Media, 6:113 –\\n133, 2020. URL https://api.semanticscholar.org/CorpusID:211171854. 3\\nYiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Fed-\\nerico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar.\\nNeural fields in vi-\\nsual computing and beyond.\\nComputer Graphics Forum, 41, 2021.\\nURL https://api.\\nsemanticscholar.org/CorpusID:244478496. 3\\n'},\n",
       " {'abstract': 'Visual fine-tuning has been receiving considerable attention in recent years owing to the advent of pre-trained vision models. However, the currently dominant approach, known as full fine-tuning, encounters knowledge forgetting, as it primarily focuses on fitting the downstream training set. To address this problem, this paper proposes a novel weight rollback-based fine-tuning method termed One Step Learning, One Step Review (OLOR). Unlike previous weight decay and regularization techniques that generally push the current weight away from 0 in the late stage of the optimization, OLOR incorporates a weight rollback term into the weight update term during the optimization. This ensures that the model weight gradually approaches the pre-trained weights as it learns the downstream task. In addition, a layer-wise penalty is introduced for weight rollback, utilizing penalty decay and diversified decay rates to adjust the rollback amounts of each layer, thereby accommodating diverse downstream tasks. To demonstrate the effectiveness of the proposed approach, comprehensive experiments have been conducted on a variety of tasks, encompassing image classification, object detection, semantic segmentation, and instance segmentation. The results on different pre-trained and downstream models indicate that OLOR achieves remarkable performance, confirming its general applicability.',\n",
       "  'introduction': 'The rapid advancement of deep learning technology has resulted in the establishment of numerous large-scale image datasets, propelling the development of promising pre-trained visual models. These pre-trained models have shown great potential for transfer learning and fine-tuning, allowing them to effectively solve related but distinct visual tasks. The fundamental fine-tuning methods are linear probing and full fine-tuning. However, linear probing often restricts the performance of the pre-trained backbone, while full fine-tuning typically leads to knowledge forgetting. Varied approaches have been explored to address these issues, including rehearsal methods, regularization methods, and parameter isolation methods. However, rehearsal methods tend to be inefficient due to the need for storing and managing large amounts of upstream task data. Regularization methods, such as weight decay or L2 penalty, have also demonstrated limited effectiveness in preventing knowledge forgetting, particularly when combined with adaptive optimizers. Parameter isolation methods, though effective, introduce additional parameters and often require specific training skills. To overcome these limitations, this paper introduces a novel fine-tuning method, called OLOR, which combines weight rollback and optimizer to prevent knowledge forgetting in neural networks. OLOR incorporates the weight rollback term into the weight update term at each step, allowing the model to gradually approach the pre-trained weights while learning the downstream task, resulting in the convergence of weights between the upstream and downstream models. Additionally, a layer-wise penalty is devised to employ penalty decay and diversified decay rate to adjust the weight rollback levels of layers for adapting varying downstream tasks.',\n",
       "  'literature_review': 'Rehearsal methods, based on the replay mechanism, involve retraining on a subset of stored upstream samples while learning new tasks. However, this approach is quite inefficient. EWC (Kirkpatrick et al. 2017) proposes a regularization-based fine-tuning method that uses the Fisher information matrix to determine the importance of weight parameters. This helps adjust the parameters between upstream and downstream tasks, reducing forgetting. L2-SP (Xuhong, Grandvalet, and Davoine 2018) uses an L2 penalty to restrict the updates of parameters, addressing knowledge forgetting during fine-tuning. However, it is not compatible with adaptive optimizers (Loshchilov and Hutter 2017; Guan 2023), which may produce the wrong regularization direction. Parameter isolation methods (Jia et al. 2022; Sohn et al. 2023) create new branches or modules for different network models and tasks for downstream tasks. However, it introduces extra new training parameters, requires certain training skills, and has lower generality than rehearsal methods.',\n",
       "  'methodology': 'The proposed method combines weight rollback and optimizers to adjust the range of parameter updates, thereby enhancing pre-trained model representations to improve downstream fine-tuning performance. We introduce weight rollback, which is a real-time regularization method that closely follows each weight update step. It aims to bring the current model weights closer to the pre-trained weights to perform knowledge reviewing. Subsequently, the discrepancy Δd between ϸpre and the pre-trained weight ϸ0 is computed as:\\n\\nΔd = ϸpre − ϸ0.\\n\\nFinally, the weight update process incorporates Δd, resulting in the adjusted model weights ϸt:\\n\\nϸt = ϸt₁ − ϸtgt − λΔd.\\n\\nIn addition, a layer-wise penalty mechanism is proposed. For deep learning neural networks, each layer can be conceptualized as a function that processes its input. Given a layer index i, this process can be described as follows:\\n\\nxi+1 = fi(x⋅\\ni),\\n\\nwhere fi represents the ith layer. Let xu\\ni denotes the input of fi in upstream tasks with a distribution of qi(xu\\ni), and xd\\ni denotes the input of fi in downstream tasks with a distribution of pi(xd\\ni). Since qi(xu\\ni) are always different from pi(xd\\ni), we first unfreeze all layers to secure fi will have sufficient update to handle such gap better.\\n\\nTo accommodate the variability of target objectives, we propose adjusting the rate of penalty decay between layers by introducing a power exponent γ to the weight rollback value. Mathematically, this adjustment can be expressed as:\\n\\n1 ∈ i\\nn → (1 ∈ i\\nn)^γ.',\n",
       "  'results': 'The proposed OLOR achieves state-of-the-art performance on all datasets covering general classification, fine-grained classification, long-tailed classification, cross-domain classification, object detection, semantic segmentation, and instance segmentation. Notably, in in-distribution (ID) datasets, OLOR-Adam surpasses the previously leading L2-SP method by an impressive margin of 6.47% in accuracy. Moreover, when confronted with two more challenging out-of-distribution (OOD) datasets, OLOR-Adam achieves accuracy improvements of 2.57% and 7.38%, respectively, outperforming the optimal methods. Experiments on detection and segmentation tasks also demonstrate the effectiveness and versatility of OLOR. OLOR consistently outperforms the baseline by approximately 1% in all metrics across various tasks. Furthermore, in experiments using different pre-trained models, OLOR outperforms other leading methods across all pre-trained models, including Supervised, CLIP, and MAE. These results demonstrate the robustness and effectiveness of the proposed OLOR in various tasks and under different settings.',\n",
       "  'conclusion': 'The main contribution of this study is the development of OLOR, a novel fine-tuning method. OLOR combines weight rollback and layer-wise penalty to address knowledge forgetting and enhance fine-tuning performance. The proposed method achieves state-of-the-art results on extensive downstream tasks, encompassing general classification, fine-grained classification, long-tailed classification, cross-domain classification, object detection, semantic segmentation, and instance segmentation. Validation experiments and ablation analysis confirm the effectiveness of the method and the rationality of the parameters. The key innovations of OLOR include integrating a weight rollback term into the weight update term, devising a layer-wise penalty with penalty decay and diversified decay rate, and seamlessly combining with optimizers for efficient implementation.',\n",
       "  'title': 'One Step Learning, One Step Review',\n",
       "  'author': 'Xiaolong Huang, Qiankun Li, Xueran Li, Xuesong Gao',\n",
       "  'textdata': 'One Step Learning, One Step Review\\nXiaolong Huang1, Qiankun Li2, 3*, Xueran Li4, 5, Xuesong Gao1\\n1School of Artificial Intelligent, Chongqing University of Technology, Chongqing, China\\n2Institute of Intelligent Machines, Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei, China\\n3Department of Automation, University of Science and Technology of China, Hefei, China\\n4Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei, China\\n5Anhui University, Hefei, China\\n{hirox827, xueran.lxr}@gmail.com, qklee@mail.ustc.edu.cn, xuesonggxs@foxmail.com\\nAbstract\\nVisual fine-tuning has garnered significant attention with\\nthe rise of pre-trained vision models. The current prevailing\\nmethod, full fine-tuning, suffers from the issue of knowledge\\nforgetting as it focuses solely on fitting the downstream train-\\ning set. In this paper, we propose a novel weight rollback-\\nbased fine-tuning method called OLOR (One step Learning,\\nOne step Review). OLOR combines fine-tuning with opti-\\nmizers, incorporating a weight rollback term into the weight\\nupdate term at each step. This ensures consistency in the\\nweight range of upstream and downstream models, effec-\\ntively mitigating knowledge forgetting and enhancing fine-\\ntuning performance. In addition, a layer-wise penalty is pre-\\nsented to employ penalty decay and the diversified decay\\nrate to adjust the weight rollback levels of layers for adapt-\\ning varying downstream tasks. Through extensive experi-\\nments on various tasks such as image classification, object\\ndetection, semantic segmentation, and instance segmentation,\\nwe demonstrate the general applicability and state-of-the-art\\nperformance of our proposed OLOR. Code is available at\\nhttps://github.com/rainbow-xiao/OLOR-AAAI-2024.\\nIntroduction\\nWith the rapid advancement of deep learning technology,\\nnumerous large-scale image datasets have been established\\n(Schuhmann et al. 2022; Russakovsky et al. 2015; Schuh-\\nmann et al. 2021), resulting in many promising pre-trained\\nvisual models (Radford et al. 2021; He et al. 2022; Bao et al.\\n2021). These pre-trained models can effectively solve re-\\nlated but distinct visual tasks through transfer learning and\\nfine-tuning techniques (Wu, Sun, and Ouyang 2023; Shen\\net al. 2021). The fundamental fine-tuning methods are lin-\\near probing and full fine-tuning (Zhang, Isola, and Efros\\n2017). In linear probing, the pre-trained model’s backbone\\nis frozen, and only the head specific to the downstream task\\nis trained. However, this approach often restricts the per-\\nformance of the pre-trained backbone. On the other hand,\\nfull fine-tuning involves training the entire network directly,\\nbut it usually leads to knowledge forgetting (De Lange et al.\\n2021).\\n*Corresponding authors (qklee@mail.ustc.edu.cn)\\nCopyright © 2024, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.\\nRehearsal methods (Rebuffi et al. 2017; Rolnick et al.\\n2019; Liu et al. 2020; Merlin et al. 2022), based on the re-\\nplay mechanism, involve retraining on a subset of stored\\nupstream samples while learning new tasks. However, this\\napproach is quite inefficient. EWC (Kirkpatrick et al. 2017)\\nproposes a regularization-based fine-tuning method that uses\\nthe Fisher information matrix to determine the importance of\\nweight parameters. This helps adjust the parameters between\\nupstream and downstream tasks, reducing forgetting. L2-SP\\n(Xuhong, Grandvalet, and Davoine 2018) uses an L2 penalty\\nto restrict the updates of parameters, addressing knowledge\\nforgetting during fine-tuning. However, it is not compatible\\nwith adaptive optimizers (Loshchilov and Hutter 2017; Guan\\n2023), which may produce the wrong regularization direc-\\ntion. Parameter isolation methods (Jia et al. 2022; Sohn et al.\\n2023) create new branches or modules for different network\\nmodels and tasks for downstream tasks. However, it intro-\\nduces extra new training parameters, requires certain train-\\ning skills, and has lower generality than rehearsal methods.\\nIn this paper, we propose a novel fine-tuning method com-\\nbined with optimizers to solve knowledge forgetting, called\\nOLOR (One step Learning, One step Review). Specifically,\\nOLOR introduces a weight rollback term to the weight up-\\ndate term during the fine-tuning stage, allowing the model\\nto gradually approach the pre-trained weights while learning\\nthe downstream task. This process avoids delay defects and\\nmakes the weights of the upstream and downstream models\\nmore similar. In addition, a layer-wise penalty is devised to\\nemploy penalty decay and the diversified decay rate to ad-\\njust the weight rollback levels of layers. Penalty decay com-\\nbines feature pyramids with transfer learning, giving more\\nsignificant weight rollback to shallow layers related to shal-\\nlow features such as color and texture, and smaller weight\\nbacktracking to deep layers related to deep features such\\nas semantic information. The diversified decay rate is ad-\\njusted to enhance applicability according to the variations\\nbetween up and downstream tasks. OLOR with layer-wise\\npenalty enables each layer of the model to update accord-\\ning to its needs, resulting in superior extraction of gener-\\nalized features. Finally, OLOR is incorporated into optimiz-\\ners, thereby introducing negligible extra computational over-\\nhead. It also works well with popular optimizers such as\\nAdam (Loshchilov and Hutter 2017; Guan 2023) and SGD\\n(Keskar and Socher 2017), meeting specific needs under var-\\narXiv:2401.10962v1  [cs.CV]  19 Jan 2024\\nious conditions.\\nOur OLOR fine-tuning method achieves state-of-the-art\\nperformance on ten popular visual task datasets covering\\ngeneral classification, fine-grained classification, long-tail\\nclassification, cross-domain classification, object detection,\\nsemantic segmentation, and instance segmentation. Valida-\\ntion experiments and ablation analysis demonstrate the per-\\nformance of OLOR in solving the problem of knowledge\\nforgetting and the rationality of the parameters.\\nThe main contributions can be summarized as follows.\\n• We propose a novel fine-tuning method OLOR, which\\ncooperates with optimizers to solve the knowledge\\nforgetting issue, thereby improving fine-tuning perfor-\\nmance.\\n• The designed weight rollback avoids delay defects by in-\\ncorporating the current gradient into the penalty term,\\nthereby correcting the penalty target and smoothing the\\nreview process.\\n• A layer-wise penalty is presented to employ penalty de-\\ncay and the diversified decay rate to adjust the weight\\nrollback levels of layers for adapting varying downstream\\ntasks.\\n• The proposed method achieves state-of-the-art perfor-\\nmance on extensive downstream tasks, including differ-\\nent types of image classification, different pre-trained\\nmodels, and image detection and segmentation.\\nRelated Work\\nPre-Training Resource\\nWith the rapid advancement of computer vision, numerous\\nlarge-scale datasets (Russakovsky et al. 2015; Schuhmann\\net al. 2021, 2022) and pre-trained models have emerged.\\nThese upstream pre-trained models possess rich features\\nand hold great potential for transferability to other specific\\ndownstream tasks. ImageNet-21K (Russakovsky et al. 2015)\\nis the most popular large-scale dataset with over 14 million\\nimages, and most networks are pre-trained on it. Recently,\\na groundbreaking development has taken place with the re-\\nlease of LAION-2B (Schuhmann et al. 2022). This dataset\\nnow reigns as the largest, comprising over 2 billion image-\\ntext pairs. Then many pre-trained models have been pro-\\nposed, such as OpenClip (Radford et al. 2021), BEiT (Peng\\net al. 2022), MAE (He et al. 2022), and EVA (Fang et al.\\n2023). It is worth noting that most of these models’ back-\\nbones are built upon the foundations of ViT (Dosovitskiy\\net al. 2020) and ConvNeXt (Liu et al. 2022).\\nFine-Tuning Method\\nThe process of fine-tuning usually faces an issue known as\\nknowledge forgetting (Toneva et al. 2018). It refers to the\\nmodel’s loss of pre-training learned representations during\\nfine-tuning (Mosbach, Andriushchenko, and Klakow 2020).\\nThis leads to reduced accuracy on both the upstream and\\ndownstream tasks, as the model cannot effectively utilize its\\npotential knowledge (De Lange et al. 2021; Vander Eeckt\\nand Van Hamme 2023).\\nTo solve this issue, there are currently three categories\\nof approaches, i.e., replay methods, regularization methods,\\nand parameter isolation methods. Replay involves period-\\nically training on a subset of upstream task data, thereby\\nretaining knowledge of previous tasks and balancing old\\nand new information (Rebuffi et al. 2017; Rolnick et al.\\n2019; Liu et al. 2020; Merlin et al. 2022). However, stor-\\ning and managing updtream task data pose challenges in\\nterms of efficiency, particularly in the contemporary era of\\nmassive datasets (Schuhmann et al. 2022; Li et al. 2023).\\nRegularization-based methods employ techniques such as\\nthe fisher information matrix (Kirkpatrick et al. 2017),\\nweight decay (Kumar et al. 2022), and L2 penalty (Xuhong,\\nGrandvalet, and Davoine 2018) to restrict parameter up-\\ndates during fine-tuning. However, these techniques may\\nnot be entirely adequate in completely preventing knowl-\\nedge forgetting. Moreover, the presence of adaptive opti-\\nmizers (Loshchilov and Hutter 2017; Guan 2023) can oc-\\ncasionally impact the direction of regularization (Xuhong,\\nGrandvalet, and Davoine 2018). Parameter isolation meth-\\nods incorporate specific branches or modules into the pre-\\ntrained network during downstream fine-tuning, aiming to\\nachieve knowledge transfer through these new modules (Jia\\net al. 2022; Sohn et al. 2023; Wang et al. 2023). However,\\narchitectural modifications introduce new training parame-\\nters and intricate designs. Moreover, training tricks play a\\ncrucial role in the effectiveness of the new module, often ne-\\ncessitating multiple rounds of freezing and unfreezing.\\nTo achieve a general and concise fine-tuning method to\\naddress knowledge forgetting, the proposed OLOR fine-\\ntuning method combines weight rollback and optimizers to\\nadjust the range of parameter updates. This allows for en-\\nhancing pre-trained model representations to improve down-\\nstream fine-tuning performance.\\nMethod\\nWe propose a One step Learning, One step Review (OLOR)\\nmethod to reduce knowledge forgetting for fine-tuning.\\nOLOR can be seamlessly applied to various downstream\\ntasks among with different optimizers and models. The\\noverall framework is illustrated in Figure 1, and detailed\\npipelines incorporating SGD and Adam are described in Al-\\ngorithm 1 and Algorithm 2. This section introduces the de-\\nlay defect of the previous regularization method, followed\\nby detailed explanations of the OLOR method, which com-\\nprises weight rollback and layer-wise penalty.\\nPrevious Regularization Mechanisms Have a Delay\\nDefect\\nThe implementation of OLOR is inspired by L2 regulariza-\\ntion and weight decay, which are popular methods used to\\nregularize the model parameters. However, our findings in-\\ndicate that their effectiveness does not align with the initial\\nexpectation. In the case of the classic SGD optimizer, L2\\nregularization can be regarded as equivalent to weight de-\\ncay (Loshchilov and Hutter 2017), which can be defined as\\nfollows:\\nθt = (1 − λ)θt−1 − ηtgt,\\n(1)\\nUpstream data\\nBackbone\\n(Pre-trained)\\nLinear\\n(Random)\\nAdam-OLOR\\nPrediction\\nAdam\\nAdam\\nOLOR\\n. . .\\n. . .\\n. . .\\n. . .\\n. . .\\n. . .\\nLayer-wise Penalty\\nDownstream data\\nLayer index\\nOne step Learning, One step Review (OLOR)\\nFigure 1: Overview of OLOR using Adam as optimizer, where λi represents the penalty factor of ith layer, θt and ˆθt+1 represents\\nthe weight and the estimation of next weight (pre-weight) at timestep t, respectively. The transparency of the image indicates\\nthe knowledge forgetting level.\\nAlgorithm 1: OLOR for SGD with Momentum\\n1: input:\\nη ∈ IR: Initial learning rate, β ∈ [0, 1): momentum factor, θ0:\\npre-trained weight, ι1, ι2 ∈ [0, 1], ι1 ≥ ι2: max and min level\\nof weight rollback respectively, γ ∈ IR: weight rollback power\\n2: initialize:\\nt ← 0: time step, m0 ← 0: initial moment vector, d0 ← 0:\\ninitial discrepancy value, λi ← f(λ, i, n, ι1, ι2)/η: calculate\\npenalty factor λi through λi = f(λ, i, n, ι1, ι2) = ι2 + (1 −\\ni\\nn)γ(ι1 − ι2), then scale it by dividing η to eliminate the scale\\nissue.\\n3: repeat\\n4:\\nt ← t + 1\\n5:\\nηt ←LRScheduler(ηt−1)\\n(Calculate ηt at timestep t)\\n6:\\ngt ← ∇θft(θt−1)\\n(Get batch gradient at timestep t)\\n7:\\nmt ← βmt−1 + (1 − β)gt\\n(Compute momentum)\\n8:\\nθt ← θt−1 − ηtλidt−1 − (1 − ηtλi)ηtmt(Update weight)\\n9:\\ndt ← (1 − ηtλi)(dt−1 − ηtmt)\\n(Update discrepancy)\\n10: until Stopping condition is met\\n11: return Parameters θt\\nwhere θt represents the model weights at iteration t, and\\nθt−1 is corresponding weights from the previous iteration. λ\\nis the regularization factor (weight decay strength). ηt is the\\nlearning rate at iteration t. gt is the batch gradient computed\\nfrom the loss function at iteration t. Weight decay penalizes\\nthe weights obtained from the previous iteration by pushing\\nthem toward 0. However, in practice, limλ→1 θt = −ηtgt,\\nAlgorithm 2: OLOR for Adam\\n1: input:\\nη ∈ IR: Initial learning rate, β1, β2 ∈ [0, 1): Exponential\\ndecay rates for the moment estimates, ϵ: bias, θ0: pre-trained\\nweight, ι1, ι2 ∈ [0, 1], ι1 ≥ ι2: max and min level of weight\\nrollback respectively, γ ∈ IR: weight rollback power\\n2: initialize:\\nt ← 0: time step, m0 ← 0: initial first moment vector, v0 ←\\n0: initial second moment vector, d0 ← 0: initial discrepancy\\nvalue, λi ← f(λ, i, n, ι1, ι2)/η: calculate penalty factor λi\\nthrough λi = f(λ, i, n, ι1, ι2) = ι2 + (1 − i\\nn)γ(ι1 − ι2), then\\nscale it by dividing η to eliminate the scale issue.\\n3: repeat\\n4:\\nt ← t + 1\\n5:\\nηt ←LR Scheduler(ηt−1)\\n(Calculate ηt at timestep t)\\n6:\\ngt ← ∇ft(θt−1)\\n(Get batch gradient at timestep t)\\n7:\\nmt ← β1mt−1 + (1 − β1)gt (Update first moment vector)\\n8:\\nvt ← β2vt−1 + (1 − β2)g2\\nt (Update second moment vector)\\n9:\\nˆmt ← mt/(1 − βt\\n1)\\n10:\\nˆvt ← vt/(1 − βt\\n2)\\n11:\\nθt ← θt−1 − ηtλidt−1 − (1−ηtλi)ηt ˆ\\nmt\\n(√ˆvt+ϵ)\\n(Update weight)\\n12:\\ndt ← (1 − ηtλi)(dt−1 −\\nηt ˆ\\nmt\\n(√ˆvt+ϵ))\\n(Update discrepancy)\\n13: until stopping criterion is met\\n14: return optimized parameters θt\\nthe weights tend to be pushed towards the negative value\\nof the current gradient instead of 0. This behavior may be\\ndifferent from the initial expectation. Furthermore, applying\\nweight decay can actually increase the current weight com-\\npared to not applying it. This can be seen in the following\\ninequality:\\n(θt−1 − ηtgt − λθt−1)2 > (θt−1 − ηtgt)2,\\n(2)\\nsimplified as:\\n\\x1aηgt < (1 − λ\\n2 )θt−1,\\nif θt−1 < 0,\\nηgt > (1 − λ\\n2 )θt−1,\\nif θt−1 > 0.\\nIf η, gt, λ, and θt−1 are in above conditions, using weight\\ndecay will drive the current weight away from 0, which is\\nopposite to its target. Similarly, this issue with the decay ef-\\nfect also exists in other regularization mechanisms such as\\nL1 regularization, L2-SP, and similar methods.\\nWeight Rollback\\nThe proposed weight rollback is a real-time regularization\\nmethod that closely follows each weight update step. It aims\\nto bring the current model weights closer to the pre-trained\\nweights to perform knowledge reviewing. Specifically, the\\nfirst step is to calculate the pre-weight θpre by gradient:\\nθpre = θt−1 − ηtgt,\\n(3)\\nwhere θt−1 represents the model weights from the previous\\ntime step, ηt is the learning rate at the current time step, and\\ngt denotes the gradient. Subsequently, the discrepancy ∆d\\nbetween θpre and the pre-trained weight θ0 is computed as:\\n∆d = θpre − θ0.\\n(4)\\nFinally, the weight update process incorporates ∆d, result-\\ning in the adjusted model weights θt:\\nθt = θt−1 − ηtgt − λ∆d.\\n(5)\\nBy substituting Eq. 3 and Eq. 4 into Eq. 5, we obtain:\\nθt = (1 − λ)(θt−1 − ηtgt) + λθ0.\\n(6)\\nThis Eq. 6 ensures that limλ→1 θt = θ0, which aligns with\\nour expectation and prevents abnormal scenarios. In addi-\\ntion, as the gradient gt is also subject to a penalty, this pro-\\ncess may potentially help to mitigate gradient explosions.\\nIn summary, the weight rollback technique moderates the\\ndeviation between θt and θ0 at each step, thereby alleviating\\noverfitting to the current task and knowledge forgetting to\\nthe previous task.\\nLayer-Wise Penalty\\nPenalty Decay.\\nFor deep learning neural networks, each\\nlayer can be conceptualized as a function that processes its\\ninput. Given a layer index i, this process can be described as\\nfollows:\\nxi+1 = fi(x∗\\ni ),\\n(7)\\nwhere the fi represents the ith layer. Let xu\\ni denotes the in-\\nput of fi in upstream tasks with a distribution of qi(xu\\ni ), and\\nxd\\ni denotes the input of fi in downstream tasks with a dis-\\ntribution of pi(xd\\ni ). Since qi(xu\\ni ) are always different from\\npi(xd\\ni ), we first unfreeze all layers to secure fi will have suf-\\nficient update to handle such gap better.\\nDataset\\nImages\\nCategories\\nType\\nCIFAR-100\\n60,000\\n100\\nGeneral\\nSVHN\\n600,000\\n10\\nGeneral\\nCUB-200\\n11,788\\n200\\nFine-grained\\nStanford Cars\\n16,185\\n196\\nFine-grained\\nPlaces-LT\\n62,500\\n365\\nLong-tailed\\nIP102\\n75,222\\n102\\nLong-tailed\\nOfficeHome\\n15,500\\n4 × 65\\nCross-domain\\nPACS\\n9,991\\n4 × 7\\nCross-domain\\nCOCO2017\\n163,957\\n80\\nDetection\\nADE20K\\n27,574\\n3688\\nSegmentation\\nTable 1: Details of the fine-tuning datasets.\\nIn the study of image feature extraction, a prevailing un-\\nderstanding is that shallow layers are primarily responsible\\nfor capturing superficial features (Lin et al. 2017) such as\\ncolor, texture, and shape. In contrast, deeper layers focus\\non extracting more profound features like semantic infor-\\nmation. This implies that shallow layers are closely linked\\nto the distribution of the data, whereas deep layers are more\\naligned with task-specific objectives. A foundational as-\\nsumption underlying transfer learning is that qi(xu\\ni ) bears\\na degree of similarity to pi(xd\\ni ). Consequently, shallow lay-\\ners tend to exhibit similarities in both pre-training and fine-\\ntuning stages. Additionally, shallow layers require fewer up-\\ndates compared to their deeper counterparts.\\nBased on these observations, we propose a layer-wise\\npenalty decay mechanism for weight rollback. This ap-\\nproach gradually reduces the rollback level as the layer\\ndepth increases. This strategy encourages shallow layers to\\nextract more general features in downstream tasks while pre-\\nserving the overall model capacity. For any layer at index i,\\nthe penalty factor λi is computed using the following for-\\nmula:\\nλi = ι2 + (1 − i\\nn)(ι1 − ι2),\\n(8)\\nwhere n represents the total number of layers in the pre-\\ntrained model, ι1 and ι2 denote the maximum and minimum\\nrollback levels, respectively.\\nDiversified Decay Rate.\\nAcross various downstream\\ntasks, the target objectives often exhibit varying degrees of\\ndissimilarity from those of the upstream task. To accommo-\\ndate this variability, we propose adjusting the rate of penalty\\ndecay between layers by introducing a power exponent γ to\\nthe weight rollback value. Mathematically, this adjustment\\ncan be expressed as:\\n1 − i\\nn −→ (1 − i\\nn)γ.\\n(9)\\nThis dynamic adjustment helps mitigate the bias stemming\\nfrom a fixed rate decay of the similarities between qi(xu\\ni )\\nand pi(xd\\ni ) across different layer indices i. Consequently, the\\npenalty decay becomes more adaptable and versatile, cater-\\ning to a spectrum of requirements dictated by the various\\ndownstream tasks.\\nGeneral (ID)\\nFine-Grained (ID)\\nLong-Tailed (OOD)\\nCross-Domain (OOD)\\nMethod\\nCifar-100\\nSVHN\\nCUB-200\\nStanfordCars\\nPlaces-LT\\nIP102\\nOfficeHome\\nPACS\\nViT-B Backbone\\nLinear\\n72.50\\n58.79\\n75.01\\n38.03\\n31.95\\n64.93\\n79.96\\n71.88\\nFull\\n87.76\\n97.27\\n81.34\\n75.55\\n31.59\\n74.09\\n84.39\\n87.79\\nL2-SP\\n88.17\\n97.12\\n81.65\\n75.55\\n31.22\\n73.75\\n84.74\\n87.74\\nVPT\\n91.49\\n94.37\\n81.86\\n58.24\\n37.02\\n70.41\\n86.48\\n77.44\\nOLOR-Adam (ours)\\n92.89\\n97.35\\n84.84\\n82.02\\n38.07\\n75.34\\n89.05\\n94.38\\nConvNeXt-B Backbone\\nLinear\\n81.70\\n69.21\\n87.85\\n50.21\\n36.41\\n70.77\\n92.40\\n93.46\\nFull\\n92.72\\n96.97\\n88.59\\n88.67\\n38.61\\n75.01\\n91.78\\n95.51\\nL2-SP\\n92.84\\n97.01\\n88.82\\n88.83\\n38.52\\n75.20\\n90.61\\n95.90\\nVPT\\n88.71\\n81.58\\n87.88\\n51.58\\n36.32\\n71.22\\n92.31\\n93.75\\nOLOR-SGD (ours)\\n92.86\\n97.12\\n89.47\\n88.99\\n39.36\\n75.44\\n92.59\\n96.63\\nTable 2: Comparison of fine-tuning results on various types of classification datasets (general, fine-grained, long-tailed, cross-\\ndomain).\\nExperiments\\nExperiment Configuration\\nPre-Trained Backbones.\\nThe experiments employ CNN-\\nbased ConvNeXt (Liu et al. 2022) and Transformer-based\\nVision Transformers (ViT) (Dosovitskiy et al. 2020) as\\nbackbones. For both types of models, pre-trained weights\\nfrom ImageNet-1K (MAE) (Deng et al. 2009), ImageNet-\\n21K (supervised) (Russakovsky et al. 2015) and LAION-2B\\n(CLIP) (Schuhmann et al. 2022) datasets are utilized, where\\nthe weights from ImageNet-21K undergoes supervised pre-\\ntraining, and the others are based on self-supervised pre-\\ntraining diagram.\\nDownstream Tasks.\\nWe experiment on ten popular visual\\ntask datasets, i.e., CIFAR-100 (Krizhevsky, Hinton et al.\\n2009), SVHN (Netzer et al. 2011), CUB-200 (Wah et al.\\n2011), Stanford Cars (Krause et al. 2013), Places-LT (Zhou\\net al. 2014), IP102 (Patterson et al. 2014), OfficeHome\\n(Venkateswara et al. 2017), and PACS (Li et al. 2017), cov-\\nering general classification, fine-grained classification, long-\\ntailed classification, cross-domain classification, object de-\\ntection, semantic segmentation, and instance segmentation.\\nMore details are listed in Table 1.\\nBaselines.\\nTo ensure a comprehensive comparison, we se-\\nlect the state-of-the-art and classic methods as our baselines.\\nThese encompass Full Fine-tuning (Full), Linear Probing\\n(Linear) (Zhang, Isola, and Efros 2017), L2-SP (Xuhong,\\nGrandvalet, and Davoine 2018), and VPT (Jia et al. 2022).\\nFollowing prior works (Carion et al. 2020), CNN-based\\nBackbones are usually combined with the SGD optimizer,\\nwhile Transformer-based Backbones are paired with the\\nAdam optimizer.\\nImplementation Details.\\nThe input image size is set at\\n224 × 224. The batch size varies depending on the freez-\\ning strategy. Specifically, 128, 256 and 512 are chosen for\\nfull unfreezing, parameter isolated, and full freezing based\\nmethods, respectively. Regarding the learning rate, for Con-\\nvNeXt backbones, we employ the SGD optimizer with a mo-\\nmentum of 0.9. The learning rates differ based on the freez-\\ning strategy. In detail, 1e-2, 2e-2 and 4e-2 for full unfreez-\\ning, parameter isolated, and full freezing based methods, re-\\nspectively. For ViT backbones, we use the Adam optimizer\\nwith a momentum of (0.9, 0.999). The learning rates for ViT\\nbackbones also vary according to the freezing strategy, i.e.,\\n1e-4 for full unfreezing, 2e-4 for partial unfreezing, and 4e-\\n4 for full freezing. We train on cross-domain datasets for\\n30 epochs, while for other datasets, we train for 50 epochs.\\nThe experiments are performed on two A5000 GPU with\\n24 GB memory and Ubuntu 20.04 operating system. Python\\n3.8.3 serves as the programming language, while PyTorch\\n2.0.0 framework is employed. In addition, the source code is\\nopenly available on GitHub.\\nMain Results\\nResults on Classification Tasks.\\nTo verify the wide\\nadaptability of OLOR on various types of datasets, we con-\\nduct a comprehensive comparison with other state-of-the-\\nart fine-tuning methods. We evaluate these methods on 10\\npopular classification datasets, each showcasing a range of\\ndata distributions and characteristics. In addition, the Back-\\nbone in the experiment covers ViT-B and ConvNeXt-B, cor-\\nresponding to Adam and SGD optimizers, respectively.\\nThe experiment results are listed in Table 2. It can be ob-\\nserved that our OLOR achieves a new state-of-the-art on all\\ndatasets. Notably, in in-distribution (ID) datasets, OLOR-\\nAdam surpasses the previously leading L2-SP method by an\\nimpressive margin of 6.47% in accuracy. Moreover, when\\nconfronted with two more challenging out-of-distribution\\n(OOD) datasets, OLOR-Adam achieves accuracy improve-\\nments of 2.57% and 7.38%, respectively, outperforming the\\noptimal methods.\\nSince the pre-trained ConvNeXt model is more stable than\\nthe ViT structure, there is not much difference between dif-\\nferent methods in fine-tuning. However, our OLOR-SGD\\nstill consistently improves fine-tuning accuracy across all\\nMethod\\nModel\\nDataset\\nBboxm\\nSegmm\\nFull\\nMask R-CNN\\nCOCO2017\\n40.20\\n36.00\\nOLOR\\nMask R-CNN\\nCOCO2017\\n41.10\\n36.90\\nTable 3: Results of object detection and instance segmenta-\\ntion using the ConvNeXt-B as backbone.\\nMethod\\nModel\\nDataset\\nIOUm\\nFull\\nUperNet\\nADE20K\\n43.65\\nOLOR\\nUperNet\\nADE20K\\n44.62\\nTable 4: Results of semantic segmentation using the ViT-B\\nas backbone.\\ndatasets. These results demonstrate the robustness and ef-\\nfectiveness of the proposed OLOR in various tasks.\\nResults on Detection and Segmentation Tasks.\\nDue to\\nthe complexity of detection and segmentation tasks, most\\nexisting fine-tuning methods struggle with applicability and\\nvalidation. However, integrated with the optimizer, our\\nOLOR approach can easily be applied to these tasks. Table\\n3 shows the results of object detection and instance segmen-\\ntation on the COCO2017 dataset, while Table 4 showcases\\nthe performance of semantic segmentation on the ADE20K\\ndataset. OLOR consistently outperforms the Baseline by ap-\\nproximately 1% in all metrics, demonstrating its versatility\\nand effectiveness in more complex detection and segmenta-\\ntion tasks.\\nResults of Using Different Pre-Trained Models.\\nConsid-\\nering that the performances of different fine-tuning methods\\nmay vary when using different pre-trained models, we fur-\\nther conduct experiments to explore and compare. The pre-\\ntrained ViT-B model weights are obtained from ImageNet-\\n21K (supervised), LAION-2B (CLIP), and ImageNet-1K\\n(MAE). The fine-tuning experiments are based on the chal-\\nlenging PACS dataset.\\nAs listed in Table 5, our OLOR consistently achieves\\nstate-of-the-art results across all pre-trained models. Specif-\\nically, OLOR surpasses other leading methods by 5.08%,\\n0.64%, and 3.47% when using Supervised, CLIP, and MAE,\\nrespectively. While other methods struggle to adapt to all\\npre-trained models simultaneously, our OLOR demonstrates\\npotential across all pre-trained models.\\nMethod\\nSupervised\\nOpenCLIP\\nMAE\\nLinear\\n71.88\\n95.61\\n36.72\\nFull\\n87.79\\n47.17\\n84.18\\nL2-SP\\n87.74\\n45.56\\n85.79\\nVPT\\n76.76\\n97.46\\n50.54\\nOLOR (ours)\\n92.87\\n98.10\\n89.26\\nTable 5: Results of using different pre-trained models on the\\nPACS dataset.\\n0\\n10\\n20\\n30\\n40\\n50\\nEpochs\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nLoss\\n0\\n10\\n20\\n30\\n40\\n50\\nEpochs\\n70\\n75\\n80\\n85\\n90\\n95\\nTop1-accuracy\\nSGD\\nAdam\\nOLOR-SGD\\nOLOR-Adam\\nFigure 2: Train loss and valid top1 accuracy on CIfar-100,\\nusing ViT-B with Adam and ConvNext-B with SGD.\\n0\\n20\\n40\\n60\\n80\\n100 10\\n20\\n30\\nEpochs\\n0\\n25\\n50\\n75\\n100\\nTop1-accuracy\\nPre-train Stage\\nFine-Tune\\nStage\\nPre-train(Fold1)\\nPre-train(Fold2)\\nFull(Fold2)\\nFull(Fold1)\\nOLOR(Fold2)\\nOLOR(Fold1)\\n0\\n50\\n100\\n150\\nLayer index\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n(W\\nW0)2\\n0\\n10\\n20\\n30\\n40\\n50\\nRollback Steps\\n20\\n40\\n60\\n80\\n100\\nTop1-accuracy\\nFull fine-tuned results\\nPre-trained results\\nfull\\nOLOR\\nFold2\\nFold1\\nFigure 3: Knowledge forgetting test on PACS. Fold 1 as train\\nset and fold 2 as valid set during pre-training, splits during\\nfine-tuning is opposite to pre-training.\\nSummary of Main Results.\\nIn summary, the above ex-\\nperiments show that OLOR achieves SOTA when applied\\nto multiple downstream tasks, utilizing diverse pre-trained\\nbackbones. These results demonstrate the generalizability\\nand effectiveness of the OLOR fine-tuning method.\\nAnalysis and Discussion\\nCompatibility Analysis.\\nAs shown in Figure 2, adopting\\nweight rollback in different types of models and optimiz-\\ners generally improves the performance. Due to the restric-\\ntion on parameters, OLOR leads to slower loss converging\\nspeed at first, but ultimately becomes competitive with the\\nfull method. According to the validation results, OLOR po-\\ntentially helps reduce knowledge forgetting, resulting in far\\nsuperior top1 accuracy, especially when cooperating with\\nAdam applied in Vision Transformers.\\nKnowledge Forgetting Test.\\nTo assess potential knowl-\\nedge forgetting, we conduct a study on the PACS dataset\\nusing ViT-B and Adam. Firstly, split the dataset into two\\nfolds, the first fold contains data from three domains, car-\\ntoon, photo and sketch respectively, denote as D1, the sec-\\nond fold contains data from art painting domain, denote as\\nD2. For training stage, we first pre-train a model using D1\\n10\\n4\\n10\\n3\\n10\\n2\\n10\\n1\\n100\\n1\\n85\\n90\\n95\\n100\\nTop1-accuracy\\nCifar-100\\n10\\n4\\n10\\n3\\n10\\n2\\n10\\n1\\n100\\n1\\n80\\n90\\n100\\nTop1-accuracy\\nPACS\\n=1\\n=2\\n=4\\nFigure 4: Hyper-parameters exploring experiments on Cifar-\\n100(left) and PACS(right), both using ViT-B with Adam.\\nOLOR\\n(Acc: 94.38%)\\nFull\\n(Acc: 87.79%)\\nL2-SP\\n(Acc: 87.74%)\\nVPT\\n(Acc: 77.44%)\\nLinear\\n(Acc: 71.88%)\\nDog\\nElephant\\nGiraffe\\nGuitar\\nHorse\\nHouse\\nPerson\\nFigure 5: Feature visualization on PACS test set. We use fea-\\ntures extracted by backbone to perform t-SNE visualization,\\nand the Top1-accuracy are reported additionally.\\nas train set and D2 as valid set for 100 epochs, then fine-\\ntune the model using D2 as train set and D1 as valid set for\\n30 epochs through Full and OLOR methods, the discrepancy\\nbetween fine-tuned weight θ and pre-trained weight θ0 using\\ndifferent methods are recorded. Additionally, we perform\\nzero-shot reviewing, rolling back full fine-tuned weights to\\npre-trained weights in 50 steps. Figure 3 reports the results,\\nweight discrepancy is generally much smaller using OLOR,\\nwhen setting max rollback level ι1 to 0.01, rollback power γ\\nto 1, OLOR not only performs well in knowledge reviewing,\\nbut also benefits for current learning. And the zero-shot re-\\nviewing result shows weight rollback itself is indeed a help-\\nful method for just reviewing.\\nHyper-Parameter Exploration.\\nWe conduct experiments\\non Cifar-100(ID) and PACS(OOD) to study the appropriate\\nhyper-parameters for different types of tasks. Deep layers\\nusually require significant updates to effectively extract fea-\\ntures related to the downstream task, thus we set the min\\nrollback level ι2 to 0 by default to simplify hyper-parameter\\nsettings, for max rollback level ι1, we search from {0.0001,\\n0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1}, for weight roll-\\nback power γ, we search from {1, 2, 4}. Figure 4 shows the\\nfindings. We suggest applying small power if the task target\\nof the fine-tuning stage is similar to the pre-training stage,\\nand large max rollback level if the data distribution of down-\\nstream task is similar to upstream task.\\nDatasets\\nViT-Based\\nCNN-based\\nι1\\nι2\\nγ\\nι1\\nι2\\nγ\\nCifar-100\\n5e-3\\n0\\n2\\n5e-3\\n0\\n2\\nSVHN\\n5e-3\\n0\\n2\\n1e-4\\n0\\n2\\nCUB-200\\n5e-2\\n0\\n2\\n1e-2\\n0\\n2\\nStanfordCars\\n1e-2\\n0\\n4\\n1e-4\\n0\\n2\\nPlaces-LT\\n1e-1\\n0\\n4\\n1e-2\\n0\\n4\\nIP102\\n1e-1\\n0\\n1\\n5e-3\\n0\\n1\\nOfficeHome\\n1e-2\\n0\\n1\\n1\\n0\\n1\\nPACS\\n1e-1\\n0\\n4\\n5e-2\\n0\\n4\\nCOCO2017\\n-\\n-\\n-\\n1e-2\\n0\\n2\\nADE20K\\n1e-4\\n0\\n1\\n-\\n-\\n-\\nTable 6: Hyper-parameter configuration of OLOR for differ-\\nent downstream tasks.\\nPre-trained Method\\nι1\\nι2\\nγ\\nSupervised\\n1e-2\\n0\\n2\\nOpenCLIP\\n1e-2\\n0\\n2\\nMAE\\n1e-2\\n0\\n2\\nTable 7: Hyper-parameter configuration of OLOR for differ-\\nent pre-trained models.\\nFeature Visualization.\\nWe visualized the feature distribu-\\ntions for all methods on PACS test set through t-SNE to eval-\\nuate the quality of the extracted features. Experiments are\\nbased on ViT-B and Adam. As shown in Figure 5, compared\\nwith previous methods, OLOR generally separates the rep-\\nresentation vectors of different classes much better, demon-\\nstrating superior ability on representation.\\nConclusions\\nIn this paper, we propose a novel fine-tuning method named\\nOLOR to solve the challenge of knowledge forgetting in\\nneural networks. OLOR encompasses weight rollback and\\nlayer-wise penalty. OLOR incorporates the weight rollback\\nterm into the weight update term at each step, and can be im-\\nplemented in popular optimizers. This operation allows the\\nmodel to gradually approach the pre-trained weights while\\nlearning the downstream task, making the weights of the up-\\nstream and downstream models more similar. In addition,\\nthe layer-wise penalty employs penalty decay and the diver-\\nsified decay rate to adjust the weight rollback levels of layers\\nfor adapting varying downstream tasks. Our OLOR achieves\\nstate-of-the-art performance on extensive downstream tasks.\\nValidation experiments and ablation analysis demonstrate\\nthe effectiveness of the proposed method.\\nAdditional Implementation Details\\nIn the Main Results section, when conducting experiments\\non various downstream tasks, OLOR utilizes the hyper-\\nparameter configurations listed in Table 6. For experiments\\ninvolving different pre-trained models, the hyper-parameter\\nconfigurations for OLOR are listed in Table 7.\\nAcknowledgments\\nThis work was supported by the Students’ Innovation and\\nEntrepreneurship Foundation of USTC (No.XY2023S007).\\nWe would like to sincerely appreciate the anonymous re-\\nviewers for their valuable suggestions that helped us to im-\\nprove this paper.\\nReferences\\nBao, H.; Dong, L.; Piao, S.; and Wei, F. 2021.\\nBeit:\\nBert pre-training of image transformers.\\nArXiv Preprint\\narXiv:2106.08254.\\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\\nA.; and Zagoruyko, S. 2020. End-to-end object detection\\nwith transformers. In European Conference on Computer\\nVision, 213–229. Springer.\\nDe Lange, M.; Aljundi, R.; Masana, M.; Parisot, S.; Jia, X.;\\nLeonardis, A.; Slabaugh, G.; and Tuytelaars, T. 2021. A con-\\ntinual learning survey: Defying forgetting in classification\\ntasks. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, 44(7): 3366–3385.\\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\\nFei, L. 2009. ImageNet: A large-scale hierarchical image\\ndatabase.\\nIn 2009 IEEE Conference on Computer Vision\\nand Pattern Recognition, 248–255. Ieee.\\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\\nwords: Transformers for image recognition at scale. ArXiv\\nPreprint arXiv:2010.11929.\\nFang, Y.; Sun, Q.; Wang, X.; Huang, T.; Wang, X.; and Cao,\\nY. 2023. Eva-02: A visual representation for neon genesis.\\nArXiv Preprint arXiv:2303.11331.\\nGuan, L. 2023. Weight Prediction Boosts the Convergence\\nof AdamW. In Pacific-Asia Conference on Knowledge Dis-\\ncovery and Data Mining, 329–340. Springer.\\nHe, K.; Chen, X.; Xie, S.; Li, Y.; Doll´ar, P.; and Girshick,\\nR. 2022. Masked autoencoders are scalable vision learners.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, 16000–16009.\\nJia, M.; Tang, L.; Chen, B.-C.; Cardie, C.; Belongie, S.;\\nHariharan, B.; and Lim, S.-N. 2022.\\nVisual prompt tun-\\ning. In European Conference on Computer Vision, 709–727.\\nSpringer.\\nKeskar, N. S.; and Socher, R. 2017. Improving generaliza-\\ntion performance by switching from adam to sgd.\\nArXiv\\nPreprint arXiv:1712.07628.\\nKirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Des-\\njardins, G.; Rusu, A. A.; Milan, K.; Quan, J.; Ramalho, T.;\\nGrabska-Barwinska, A.; et al. 2017.\\nOvercoming catas-\\ntrophic forgetting in neural networks. Proceedings of the\\nNational Academy of Sciences, 114(13): 3521–3526.\\nKrause, J.; Stark, M.; Deng, J.; and Fei-Fei, L. 2013. 3D ob-\\nject representations for fine-grained categorization. In Pro-\\nceedings of the IEEE International Conference on Computer\\nVision Workshops, 554–561.\\nKrizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple\\nlayers of features from tiny images.\\nKumar, A.; Shen, R.; Bubeck, S.; and Gunasekar, S. 2022.\\nHow to fine-tune vision models with sgd. ArXiv Preprint\\narXiv:2211.09359.\\nLi, D.; Yang, Y.; Song, Y.-Z.; and Hospedales, T. M. 2017.\\nDeeper, broader and artier domain generalization. In Pro-\\nceedings of the IEEE International Conference on Computer\\nVision, 5542–5550.\\nLi, Y.; Zhang, K.; Liang, J.; Cao, J.; Liu, C.; Gong, R.;\\nZhang, Y.; Tang, H.; Liu, Y.; Demandolx, D.; et al. 2023.\\nLsdir: A large scale dataset for image restoration. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, 1775–1787.\\nLin, T.-Y.; Doll´ar, P.; Girshick, R.; He, K.; Hariharan, B.;\\nand Belongie, S. 2017. Feature pyramid networks for ob-\\nject detection. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, 2117–2125.\\nLiu, X.; Wu, C.; Menta, M.; Herranz, L.; Raducanu, B.; Bag-\\ndanov, A. D.; Jui, S.; and de Weijer, J. v. 2020. Generative\\nfeature replay for class-incremental learning. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition Workshops, 226–227.\\nLiu, Z.; Mao, H.; Wu, C.-Y.; Feichtenhofer, C.; Darrell, T.;\\nand Xie, S. 2022. A convnet for the 2020s. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 11976–11986.\\nLoshchilov, I.; and Hutter, F. 2017. Decoupled weight decay\\nregularization. ArXiv Preprint arXiv:1711.05101.\\nMerlin, G.; Lomonaco, V.; Cossu, A.; Carta, A.; and Bac-\\nciu, D. 2022. Practical recommendations for replay-based\\ncontinual learning methods. In International Conference on\\nImage Analysis and Processing, 548–559. Springer.\\nMosbach, M.; Andriushchenko, M.; and Klakow, D.\\n2020.\\nOn the stability of fine-tuning bert: Misconcep-\\ntions, explanations, and strong baselines.\\nArXiv Preprint\\narXiv:2006.04884.\\nNetzer, Y.; Wang, T.; Coates, A.; Bissacco, A.; Wu, B.; and\\nNg, A. Y. 2011. Reading digits in natural images with unsu-\\npervised feature learning.\\nPatterson, G.; Xu, C.; Su, H.; and Hays, J. 2014. The sun\\nattribute database: Beyond categories for deeper scene un-\\nderstanding. International Journal of Computer Vision, 108:\\n59–81.\\nPeng, Z.; Dong, L.; Bao, H.; Ye, Q.; and Wei, F. 2022. Beit\\nv2: Masked image modeling with vector-quantized visual\\ntokenizers. ArXiv Preprint arXiv:2208.06366.\\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\\net al. 2021. Learning transferable visual models from nat-\\nural language supervision. In International Conference on\\nMachine Learning, 8748–8763. PMLR.\\nRebuffi, S.-A.; Kolesnikov, A.; Sperl, G.; and Lampert, C. H.\\n2017. icarl: Incremental classifier and representation learn-\\ning. In Proceedings of the IEEE conference on Computer\\nVision and Pattern Recognition, 2001–2010.\\nRolnick, D.; Ahuja, A.; Schwarz, J.; Lillicrap, T.; and\\nWayne, G. 2019. Experience replay for continual learning.\\nAdvances in Neural Information Processing Systems, 32.\\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\\net al. 2015. ImageNet large scale visual recognition chal-\\nlenge. International Journal of Computer Vision, 115: 211–\\n252.\\nSchuhmann, C.; Beaumont, R.; Vencu, R.; Gordon, C.;\\nWightman, R.; Cherti, M.; Coombes, T.; Katta, A.; Mullis,\\nC.; Wortsman, M.; et al. 2022. Laion-5B: An open large-\\nscale dataset for training next generation image-text models.\\nArXiv Preprint arXiv:2210.08402.\\nSchuhmann, C.; Vencu, R.; Beaumont, R.; Kaczmarczyk,\\nR.; Mullis, C.; Katta, A.; Coombes, T.; Jitsev, J.; and\\nKomatsuzaki, A. 2021.\\nLaion-400M: Open dataset of\\nclip-filtered 400 million image-text pairs.\\nArXiv Preprint\\narXiv:2111.02114.\\nShen, Z.; Liu, Z.; Qin, J.; Savvides, M.; and Cheng, K.-T.\\n2021. Partial is better than all: revisiting fine-tuning strategy\\nfor few-shot learning. In Proceedings of the AAAI Confer-\\nence on Artificial Intelligence, volume 35, 9594–9602.\\nSohn, K.; Chang, H.; Lezama, J.; Polania, L.; Zhang, H.;\\nHao, Y.; Essa, I.; and Jiang, L. 2023. Visual prompt tun-\\ning for generative transfer learning.\\nIn Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 19840–19851.\\nToneva, M.; Sordoni, A.; Combes, R. T. d.; Trischler, A.;\\nBengio, Y.; and Gordon, G. J. 2018.\\nAn empirical study\\nof example forgetting during deep neural network learning.\\nArXiv Preprint ArXiv:1812.05159.\\nVander Eeckt, S.; and Van Hamme, H. 2023. Using adapters\\nto overcome catastrophic forgetting in end-to-end automatic\\nspeech recognition. In ICASSP 2023-2023 IEEE Interna-\\ntional Conference on Acoustics, Speech and Signal Process-\\ning (ICASSP), 1–5. IEEE.\\nVenkateswara, H.; Eusebio, J.; Chakraborty, S.; and Pan-\\nchanathan, S. 2017. Deep hashing network for unsupervised\\ndomain adaptation. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, 5018–5027.\\nWah, C.; Branson, S.; Welinder, P.; Perona, P.; and Belongie,\\nS. 2011. The caltech-ucsd birds-200-2011 dataset.\\nWang, R.; Zheng, H.; Duan, X.; Liu, J.; Lu, Y.; Wang, T.;\\nXu, S.; and Zhang, B. 2023. Few-Shot Learning with Vi-\\nsual Distribution Calibration and Cross-Modal Distribution\\nAlignment. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, 23445–23454.\\nWu, W.; Sun, Z.; and Ouyang, W. 2023. Revisiting classifier:\\nTransferring vision-language models for video recognition.\\nIn Proceedings of the AAAI Conference on Artificial Intelli-\\ngence, volume 37, 2847–2855.\\nXuhong, L.; Grandvalet, Y.; and Davoine, F. 2018. Explicit\\ninductive bias for transfer learning with convolutional net-\\nworks. In International Conference on Machine Learning,\\n2825–2834. PMLR.\\nZhang, R.; Isola, P.; and Efros, A. A. 2017. Split-brain au-\\ntoencoders: Unsupervised learning by cross-channel predic-\\ntion. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, 1058–1067.\\nZhou, B.; Lapedriza, A.; Xiao, J.; Torralba, A.; and Oliva,\\nA. 2014. Learning deep features for scene recognition using\\nplaces database. Advances in Neural Information Processing\\nSystems, 27.\\n'},\n",
       " {'abstract': \"We aim to learn about the political interests and preferences of Members of Parliament (MPs) by mining their parliamentary activity to develop a personalized recommendation system for distributing documents.Given a stream of documents, it is able to select those most likely to be relevant for a particular MP. We propose using positive unlabeled learning since we only have information about relevant documents (interventions made by each MP in debates) but not about irrelevant documents. Furthermore, we developed a new algorithm of this type, which outperforms: a) assuming that the interventions of other MPs are irrelevant; b) another well-known positive unlabeled learning method; and c) an information retrieval-based approach that matches documents and legislators' representations.\",\n",
       "  'introduction': \"Today's information society allows for easy access to vast amounts of information, sometimes even without actively searching for it. Advertisements, news, e-mails, etc. constantly bombard users. The challenge lies in separating the valuable from the useless information, a task often time-consuming. Content-based recommender systems aim to reduce this information overload by suggesting items (movies, songs, books, restaurants, etc.) to users based on their preferences and the characteristics of the items. Politicians also face this dilemma. Members of Parliament (MPs) must stay informed about matters related to their specific interests, such as health, education, or agriculture, while avoiding irrelevant information. Our goal is to develop a system that can automatically decide which documents should be sent to each MP based on both the document's content and the MP's political interests.\",\n",
       "  'literature_review': \"Previous approaches to tackle this problem have used information retrieval-based methods, which rely on extracting term profiles from MPs' speeches. Our approach, in contrast, is based on machine learning techniques, specifically positive unlabeled learning (PUL), which assumes a set of positive data and a (usually larger) set of unlabeled data without negative examples. We propose a new PUL method based on modifying the K-means clustering algorithm. Related works have used a two-step strategy, where the first step tries to identify reliable negative data from the unlabeled set, and the second step uses a traditional supervised learning algorithm on the positive and reliable negative data.\",\n",
       "  'methodology': 'We formalize the situation as follows: let MP = {MP1, . . . , MPn} be the set of MPs working in a parliament, which receives documents that should be distributed among them. Not all MPs should receive all documents to alleviate their workload, only those related to their interests, preferences, and role within the parliament. A system is required to perform this filtering process automatically. We propose to build such a system using PUL, using a modification of the K-means clustering algorithm to identify reliable negative documents from the interventions of other MPs for each MPi. For each MPi, we train a binary classifier from Di and Ni using Support Vector Machines (SVMs), which is considered the state-of-the-art technique for document classification. We also consider using a method to deal with the class imbalance problem.',\n",
       "  'results': 'Experiments using data from the Andalusian Parliament in Spain show that our approach outperforms the baseline (assuming all interventions of other MPs are irrelevant) and another well-known PUL method in terms of precision, recall, and F-measure. Our method also outperforms information retrieval-based approaches.',\n",
       "  'conclusion': 'Our proposed approach, pul-km, is a valuable tool for tackling the problem of building a content-based recommender system of documents in a parliamentary setting. It outperforms existing approaches, including information retrieval-based methods. Future research directions include exploring strategies for selective balancing, studying methods to select different thresholds for different classifiers, and investigating the use of feature selection methods.',\n",
       "  'title': 'Positive unlabeled learning for building recommender systems in a parliamentary setting',\n",
       "  'author': 'Luis M. de Camposa, Juan M. Fernández-Luna, Juan F. Huete, Luis Redondo-Expósito',\n",
       "  'textdata': 'Positive unlabeled learning for building recommender\\nsystems in a parliamentary setting\\nLuis M. de Camposa,∗, Juan M. Fern´andez-Lunaa, Juan F. Huetea, Luis\\nRedondo-Exp´ositoa\\naDepartamento de Ciencias de la Computaci´on e Inteligencia Artificial,\\nETSI Inform´atica y de Telecomunicaci´on, CITIC-UGR,\\nUniversidad de Granada, 18071, Granada, Spain\\nAbstract\\nOur goal is to learn about the political interests and preferences of the Mem-\\nbers of Parliament by mining their parliamentary activity, in order to develop a\\nrecommendation/filtering system that, given a stream of documents to be dis-\\ntributed among them, is able to decide which documents should receive each\\nMember of Parliament. We propose to use positive unlabeled learning to tackle\\nthis problem, because we only have information about relevant documents (the\\nown interventions of each Member of Parliament in the debates) but not about\\nirrelevant documents, so that we cannot use standard binary classifiers trained\\nwith positive and negative examples. We have also developed a new algorithm\\nof this type, which compares favourably with: a) the baseline approach assum-\\ning that all the interventions of other Members of Parliament are irrelevant,\\nb) another well-known positive unlabeled learning method and c) an approach\\nbased on information retrieval methods that matches documents and legisla-\\ntors’ representations. The experiments have been carried out with data from\\nthe regional Andalusian Parliament at Spain.\\nKeywords:\\npositive unlabeled learning, content-based recommender systems,\\nparliamentary documents, k-means, support vector machines\\n1. Introduction\\nNowadays we live in the information society, where enterprises, institutions\\nand people in general can easily access to vast amounts of information. More-\\nover, in many cases users do not need to actively search for the information\\nthey need but they play a more passive role and are constantly bombarded with\\nadvertising, news, e-mails, etc. The problem is then to separate the chaff from\\n∗Corresponding author\\nEmail addresses: lci@decsai.ugr.es (Luis M. de Campos), jmfluna@decsai.ugr.es\\n(Juan M. Fern´andez-Luna), jhg@decsai.ugr.es (Juan F. Huete), luisre@decsai.ugr.es\\n(Luis Redondo-Exp´osito)\\nPreprint submitted to Elsevier\\nJanuary 23, 2024\\narXiv:2401.10961v1  [cs.IR]  19 Jan 2024\\nthe wheat, to distinguish what is interesting, important or useful and what is\\nnot. This is a hard and time-consuming task. To reduce the information over-\\nload, there exist content-based recommender/filtering systems [2, 33], which\\nsuggest items (songs, movies, books, restaurants,...) to users according to their\\npreferences and taking into account the characteristics of the items.\\nThis situation also happens in a political context. Politicians in general and\\nMembers of Parliament (MP) in particular need to keep informed about the\\nmatters more related with their specific political interests. For example, an MP\\nwho is working in the health committee of a regional or national parliament\\nprobably would be interested in documents produced by the European Union\\ndealing with health-related matters but not in others concerning, let us say,\\neducation or agriculture. In our case the users will be the MPs and the items\\nto be recommended/filtered are the documents arriving to the parliament (e.g.\\nnews releases or technical reports).\\nThe goal is to develop a system which\\ncan automatically decide those MPs who should receive each document. This\\ndecision must be based on both the content of the document and the political\\ninterests of each MP.\\nOne possible approach to develop our recommendation/filtering system could\\nbe to learn about the interests and preferences of each MP by mining her par-\\nliamentary activity. So, we could use the transcriptions of the speeches of the\\nMPs in the parliamentary debates to train a binary classifier (the class values\\nbeing relevant and non-relevant) for each MP. Then, when a new document to\\nbe filtered/recommended enters the system, we could use these classifiers to de-\\ntermine which MPs should receive the document: those MPs whose associated\\nclassifier predicts the relevant class. Alternatively, if we assume that the clas-\\nsifiers produce a numerical output instead of a binary value, we could generate\\na ranking of MPs in decreasing order, thus recommending the document to the\\ntop ranked MPs.\\nThe problem with this approach is that in order to build a standard binary\\nclassifier for each MP we need training data (documents in this case), both\\npositive (relevant documents) and negative (irrelevant documents).\\nPositive\\ntraining data do not represent any problem: the own interventions/speeches\\nof an MP are clearly positive training data for building the classifier for this\\nMP. However, negative training data are not so clear. We could suppose that\\nall the interventions which are not from an MP are negative training data for\\nthe classifier associated to this MP. But this may be somewhat unreasonable,\\nbecause the interventions of other MPs dealing with the topics which are of\\ninterest for a given MP could also be relevant for this MP, thus creating confusion\\nin the classifier.\\nFor example, if a given MP is interested or especialized in\\neducation, it is quite probable that she will find relevant (at least some of) the\\ninterventions of other MPs about this topic. Therefore, within the interventions\\nof other MPs we will probably find both relevant and irrelevant documents for\\na given MP.\\nThis situation can be managed using the techniques known as Positive Un-\\nlabeled Learning (PUL) [41], where it is assumed that there exists a set of\\npositive data and a (usually larger) set of unlabeled data, but there is no neg-\\n2\\native training data. In our case the unlabeled data would correspond with the\\ninterventions of all the other MPs. PUL is an extreme case of semisupervised\\nlearning [6] (which considers simultaneously positive, negative and unlabeled\\ndata).\\nSo, our proposal in this paper is to explore the use of positive unlabeled\\nlearning to build a content-based recommender system of documents for the\\nMPs. More precisely, our approach is first based on trying to detect, among the\\nunlabeled data, a subset of reliable negative data, and second to use the known\\npositive data and the reliable negative data to train a standard binary classifier\\nfor each MP. To detect reliable negative data we can use some of the known\\nPUL methods, although we propose a new method based on constraining the\\noperations of the K-means clustering algorithm.\\nIn order to validate our proposals, we shall perform an experimental study\\nusing a collection of MPs interventions from the regional Parliament of Andalu-\\nsia at Spain.\\nThe main contributions of this paper are: (a) the proposal of using machine\\nlearning techniques to tackle the problem of building a content-based recom-\\nmender system of documents in a parliamentary setting (there are other pro-\\nposals to deal with this problem [10, 34, 11], but all using information retrieval-\\nbased methods instead of machine learning techniques); (b) the use of positive\\nunlabeled learning to build a recommender system (we are not aware of any\\nwork in this sense, although there are many papers applying positive unlabeled\\nlearning to the problem of classifying documents [13, 16, 24, 26, 27, 39]); (c) the\\nproposal of a new method of positive unlabeled learning based on a modification\\nof the K-means clustering algorithm.\\nThe rest of the paper is organized in the following way: in Section 2 we\\nsummarize related work. Section 3 describes in detail our approach. Section\\n4 contains the experimental part of the paper. Finally, Section 5 contains the\\nconcluding remarks and some proposals for future work.\\n2. Related work\\nThere are many works studying the recommendation/filtering problem in\\ndifferent domains and applications (as the three survey papers [18, 4, 29] show).\\nContent-based recommender systems can be built using either information retrieval-\\nbased methods [1, 2, 15, 28, 31] or machine learning algorithms for learning user\\nmodels [3, 8, 20, 21, 32, 37, 40]. However, its application in a parliamentary\\ncontext is much more limited [10, 34, 11], and in all these cases only information\\nretrieval-based methods have been used.\\nIn [10] a lazy approach is considered, avoiding to construct an elaborated\\nprofile of the MPs, collecting the transcriptions of all their speeches and building\\na document collection, then using an information retrieval system to search the\\nMPs most similar to the document to be recommended. This approach is refined\\nin [11], where term (word) profiles for the different MPs are extracted from their\\nspeeches in different ways. A different approach is considered in [34], where the\\n3\\nprofiles of the MPs are not built from the terms in their interventions but from\\nthe keywords manually assigned by documentalists (using a thesaurus) to these\\ninterventions.\\nOn the other hand, there are three classes of methods proposed for positive\\nunlabeled learning, according to [41]. The first class uses a two-step strategy,\\nwhere the first step tries to identify a set of reliable negative data from the un-\\nlabeled set, and the second step uses a traditional supervised learning algorithm\\non the positive and the reliable negative data [24, 26, 27, 39]. The second class\\nfollows the statistical query learning model. For example, in [13] a modification\\nof the Naive Bayes (NB) for text classification is obtained by estimating the\\nconditional probabilities of the terms given the positive class in the usual way\\nand the conditional probabilities given the negative class by using a supplied\\nestimate of the prior probability of the positive class.\\nIn [5] other Bayesian\\nnetwork classifiers are also extended to the PUL setting. The third class of\\nmethods treats the unlabeled data as noisy negative examples, using then lo-\\ngistic regression [23] or the Biased Support Vector Machine [27], for example.\\nPUL is also being used in the case of data streams [25], and it is still an active\\narea of research [14, 17, 19].\\nWe are going to focus on the methods of the first class, which are more\\nextended and are more similar to the new PUL method that we propose. In\\n[27] the authors use the NB classifier, where positive data are used as posi-\\ntive training examples and unlabeled data as negative training examples. The\\nresulting NB classifier is used to re-classify the unlabeled data, thus selecting\\nas reliable negative data those unlabeled examples which have been classified\\nas negative by NB. A similar approach is used in [24], where NB is replaced\\nby the Rocchio text classification method (using tf-idf weights and the cosine\\nsimilarity). Another proposal is the Spy technique [26], which randomly selects\\na subset of positive data to be added to the unlabeled data. Then the Expec-\\ntation Maximization (EM) algorithm is applied to train a NB classifier, which\\nis used on the selected positive data to obtain a threshold able to identify reli-\\nable negative examples. The PEBL method [39] tries to identify those features\\n(terms in this case), called positive features, which are more frequent (in relative\\nterms) between positive documents than between unlabeled documents. Then\\nthose documents which do not contain any of these positive features are selected\\nas reliable negative examples. There are also proposals (e.g. [16]) that try to\\nobtain from the unlabeled data both reliable positive and negative data.\\n3. Positive unlabeled learning in a parliamentary setting\\nThe situation that we are considering can be formalized as follows:\\nlet\\nMP = {MP1, . . . , MPn} be a set containing all the MPs working in a parlia-\\nment. This institution receives or generates a series of documents that should\\nbe distributed among the MPs. However, to alleviate their work, not all the\\nMPs should receive all the documents [36]. Instead, each MP should receive\\nonly those documents which are related to her interests, preferences and the\\n4\\nrole she plays within the parliament. Therefore, a system able to automati-\\ncally perform this filtering process is required. As mentioned in Section 1, we\\nwant to build such a system by using machine learning techniques, more pre-\\ncisely positive unlabeled learning. The (public and, we expect, reliable) source\\nof information about the political interests of MPs will be their interventions\\nwithin the parliamentary debates. So, each MPi can be associated with a set of\\ndocuments Di = {di1, . . . , dimi}, where each dij represents the transcription of\\nthe speech of MPi when she participated in the discussion of a parliamentary\\ninitiative. The complete set of documents is D = ∪n\\nj=1Dj. Therefore, we are\\ngoing to train a set of n binary text classifiers using D. For each MPi, the set\\nof positive examples (documents) is precisely Di, whereas the set of unlabeled\\ndocuments is D \\\\ Di.\\nOur proposal for using PUL to build a recommender/filtering system of\\ndocuments for MPs falls within the two-step strategy mentioned in Section 2.\\nWe shall use a modification of the K-means clustering algorithm in the first\\nstep, in order to identify a set of reliable negative documents, Ni, from the set\\nof unlabeled documents for each MPi (i.e. the interventions of other MPs).\\nIn the second step, for each MPi we will train a binary classifier from Di\\nand Ni using Support Vector Machines [9], which is considered as the state-of-\\nthe-art technique for document classification. As it is quite probable that the\\nsets Ni are quite larger than the corresponding Di, i.e. the data sets can be\\nquite imbalanced, we have also considered the possibility of using some method\\nto deal with the class imbalance problem.\\n3.1. The modified K-means algorithm\\nThe classical K-means algorithm is an iterative method that, starting from\\nan initial centroid for each of the K clusters, assigns each example to the cluster\\nwhose centroid is nearer (more similar) to the example. Then the algorithm\\nrecomputes the centroid of each cluster using all the examples assigned to it.\\nThe new centroids are used to reassign each example to the (possibly different)\\ncluster whose centroid is more similar to the example, and this process is re-\\npeated until a convergence condition holds. In our case the number of clusters\\nis fixed to K=2 and the similarity between documents is computed using the\\nclassical cosine similarity measure [1]. The proposed modification is that the\\nknown positive examples are forced to always remain in the positive cluster, no\\nmatter if they are more similar to the negative centroid, whereas the unlabeled\\nexamples can fluctuate between the two clusters depending on the similarity.\\nTo initialize the process, the positive centroid is computed from all the positive\\nexamples and the negative centroid is calculated from all the unlabeled exam-\\nples. At the end of the process, the unlabeled examples which remain in the\\nnegative cluster are considered reliable negative examples.\\n5\\n4. Experimental evaluation\\nTo experimentally evaluate our proposals, we shall use data from the An-\\ndalusian Parliament at Spain1. More precisely, we focus on the 8th term of\\noffice of this regional chamber, where a total of 5,258 parliamentary initiatives\\nwere discussed.\\nEach initiative, marked up in XML [12], includes the transcriptions of all\\nthe speeches of the MPs who participate in the discussion, together with their\\nnames. There is a total of 12,633 different interventions (with an average of 2.4\\ninterventions per initiative). Our set MP is composed of 132 MPs2.\\nWe randomly partitioned the set of initiatives into a training set (containing\\n80% of the initiatives) and a test set (containing the remaining 20%). To obtain\\nmore statistically reliable results, we repeated this process 5 times, and the\\nreported results are the averages of these rounds. In other words, we used the\\nrepeated holdout method [22] as evaluation methodology.\\nWe extracted the interventions of all the MPs in MP from the initiatives in\\nthe training set and used them to build a classifier for each MP, according to the\\nmethod described in Section 3. These classifiers were then used to classify the\\ninitiatives in the test set, using the transcriptions of the speeches within each\\ntest initiative as the document to be filtered/recommended, assuming that each\\ntest initiative is relevant only for those MPs who participate in it. It is worth to\\nmention that this is a very conservative assumption, because an initiative could\\nalso be relevant to other MPs who did not participate in its discussion but are\\ninterested in the same topics the initiative is devoted to. Our assumption is\\nan easy way to establish a kind of “ground truth”, without the need to have\\ndocuments annotated with explicit relevance judgements.\\nIn order to assess the quality of the filtering/recommendation system we\\nused classical evaluation measures of text classification, namely precision, re-\\ncall and the F-measure [35]. Let TPi (True Positives) be the number of test\\ninitiatives which are truly relevant for MPi and have been classified as relevant\\nby the classifier associated to MPi; FPi (False Positives) is the number of test\\ninitiatives which are not relevant for MPi but have been incorrectly identified\\nas relevant by the corresponding classifier; FNi (False Negatives) is the number\\nof test initiatives that, although being relevant for MPi, have been incorrectly\\nclassified as irrelevant. Precision is then defined as pi = TPi/(TPi + FPi) (an\\nestimation of the probability of a document being truly relevant given that it is\\nclassified as relevant. Recall is defined as ri = TPi/(TPi + FNi) (an estimation\\nof the probability of classifying as relevant a truly relevant document). The\\nF-measure is the harmonic mean of precision and recall, Fi = 2piri/(pi + ri).\\nAs we compute precision, recall and F for every MPi, it is necessary to\\nsummarize each of these three types of measures into a single value which gives a\\nglobal perspective of the system’s performance. To this end we used both macro-\\n1http://www.parlamentodeandalucia.es\\n2We considered only those MPs who intervene in at least 10 initiatives.\\n6\\naveraged (Mp, Mr and MF) and micro-averaged (mp, mr and mF) measures [38]:\\nMp = 1\\nn\\nn\\nX\\ni=1\\npi,\\nMr = 1\\nn\\nn\\nX\\ni=1\\nri,\\nMF = 1\\nn\\nn\\nX\\ni=1\\nFi\\n(1)\\nmp =\\nPn\\ni=1 TPi\\nPn\\ni=1(TPi + FPi), mr =\\nPn\\ni=1 TPi\\nPn\\ni=1(TPi + FNi), mF = 2mp mr\\nmp + mr\\n(2)\\nThe baseline approach (bas) we have considered is to train the classifiers\\nwithout using PUL, i.e.\\nfor each MPi the set of positive examples is again\\nDi, whereas all the unlabeled examples in D \\\\ Di are considered as negative\\nexamples.\\nWe shall also use, for comparison purposes, the well-known PUL\\nmethod proposed in [27] (and described in Section 2), based on Naive Bayes\\n(pul-nb). Once the reliable negative examples are identified by this method,\\nSVMs are also used to build the classifiers. The method based on using the\\nmodification of K-means, proposed in Section 3 as the first step of PUL will be\\ncalled pul-km. The comparison of bas and pul-km will serve to assess the merits\\nof PUL in our recommendation context. The comparison of pul-km and pul-nb\\nwill give us an idea of the potential of the new PUL method proposed in this\\npaper.\\nAs mentioned in Section 3, we shall also experiment with versions of bas, pul-\\nnb and pul-km (called bas-b, pul-nb-b and pul-km-b respectively) where, previous\\nto applying SVMs to the sets of positive and (reliable) negative examples, we\\nuse a method to deal with the imbalance of these data sets. More precisely, we\\nhave used the synthetic minority over-sampling technique (SMOTE) [7], which\\nessentially is a statistical algorithm for creating new instances, from existing\\ncases of the minority class. SMOTE works by taking samples of the class with\\nless observations and its k nearest neighbors randomly. Then it produces new\\nobservations setting a random point along the segment generated between the\\ntarget sample and its k neighbors. We used the implementations of SVM, NB\\nand SMOTE available in R3 (packages caret, e1071 and DMwR). All the prepro-\\ncessing steps of the datasets (all the initiatives were preprocessed by removing\\nstop words and performing stemming) were also carried out with R packages (tm\\nand snowBallC). The modified K-means algorithm and the evaluation process\\nwere implemented in Java.\\nThe version used of the selected classification algorithm (SVM) is able to\\ngive a numerical output, more precisely it returns the probability of the target\\ndocument d being relevant to MPi, pri(d).\\nThus, we can use it by simply\\nassigning the relevant value to d if pri(d) ≥ 1−pri(d) (i.e. if pri(d) ≥ 0.5). But,\\nmore generally, we can also select a threshold t (0 ≤ t ≤ 1) and state that d is\\nrelevant for MPi if pri(d) ≥ t. In this sense the values of TPi, FPi and FNi\\nused to compute precision and recall are obtained according to the contingency\\n3https://cran.r-project.org\\n7\\ntable displayed in Table 1. We have experimented with several values for the\\nthreshold t, ranging from 0.1 to 0.9.\\nTruly relevant\\nTruly irrelevant\\nfor MPi\\nfor MPi\\npri(d) ≥ t\\nTPi\\nFPi\\npri(d) < t\\nFNi\\nTable 1: Contingency table for threshold t.\\n4.1. Results with imbalanced data sets\\nThe results of our experiments for (micro and macro) precision, recall and\\nF using different thresholds are displayed in Figures 1 to 3, respectively.\\nFirst, the results in Figures 1 and 2 allow us to extract some general ten-\\ndencies for the three approaches: precision increases and recall decreases as the\\nthreshold increases. This is to be expected. When the threshold increases the\\nclassifiers are more selective to assign the relevant value to a document. The\\nconsequence is that the number of false positives decreases, thus increasing pre-\\ncision. At the same time, the number of false negatives increases and therefore\\nrecall decreases. The exception is the behaviour of macro precision with the\\nbas approach: this measure tends to decrease when increasing the threshold.\\nWe believe that this reveals a poor performance of this approach in those cases\\nwhere the classifiers are trained with very few positive examples, i.e. for those\\nMPs that scarcely intervene in the debates (then having a heavily imbalanced\\ntraining set). In these cases, although the number of false positives decreases as\\nthe threshold increases, the number of true positives also decreases more quickly.\\nNotice that this only affects macro precision and not micro precision, because\\nin the first case all the MPs contribute equally to this measure, independently\\non their number of interventions.\\nThese figures also show that the baseline approach and the two PUL meth-\\nods behave differently: the bas approach is much better for precision and the\\nPUL methods are much better for recall. Given the characteristics of our evalu-\\nation method, we believe that we should give more importance to recall than to\\nprecision. The reason is that false negatives (which affect recall) represent true\\nerrors, an MP participated in an initiative but the classifier does not recom-\\nmend this to her. On the other hand, a false positive (which affects precision)\\nrepresents that an MP did not participate in an initiative but the classifier rec-\\nommends this to her. It could be the case that this MP is truly interested in\\nthis initiative because its content matches with her political interests. In this\\nway, low recall is an objective signal of bad performance, whereas low precision\\ndoes not necessarily means the same, it may be a by-product of our conservative\\nassumption concerning relevance.\\nIn Figure 3 we can observe the results for the F-measure (micro and macro),\\nwhich represents a balance between precision and recall, and therefore is an ap-\\npropriate measure of global performance. First, we can see that the best results\\n8\\nFigure 1: Micro and Macro precision for bas, pul-km and pul-nb using different thresholds.\\nare always obtained when we use low thresholds. Second, pul-km systematically\\noutperforms both bas and pul-nb.\\nTable 2 shows the best F values obtained by each approach, as well as the\\ncorresponding thresholds where these values are reached. We have used paired\\nt-tests (using the results of the five random partitions, and a confidence level\\nof 95%) to assess the statistical significance of these results. pul-km is always\\nsignificantly better than both bas and pul-nb. At micro level bas is also signif-\\nicantly better than pul-nb, whereas there is not significant difference between\\nthese two approaches at macro level.\\n4.2. Results with balanced data sets\\nNow, we are going to repeat the experiments of the previous section but\\nusing the balanced versions of the three approaches, bas-b, pul-km-b and pul-nb-\\n9\\nFigure 2: Micro and Macro recall for bas, pul-km and pul-nb using different thresholds.\\nb. For the sake of conciseness, we only show the results relative to the F-measure\\nin Figure 4. The figures for precision and recall exhibit a behaviour similar to\\nthose in the previous section, increasing lines for precision and decreasing lines\\nfor recall, although the lines are closer. Also, the previous extrange behaviour\\nof macro precision with the bas approach has dissappeared.\\nIn Figure 4 we can observe several interesting facts. First, the thresholds\\nwhere the best results are obtained have changed completely, now they are more\\ncentered, near the point 0.5 which could be considered as the natural threshold.\\nThis seems to indicate that the classifiers are better calibrated, they do not\\nneed to draw on very low thresholds to get good results. Second, pul-km-b is\\nstill the best approach, although the differences with bas-b are smaller than in\\nthe previous experiments. Third, balancing pul-nb is not a good idea, it obtains\\nresults considerably worse than pul-km-b and bas-b. Table 3 is the counterpart\\n10\\nFigure 3: Micro and Macro F measures for bas, pul-km and pul-nb using different thresholds.\\nof Table 2 for the balanced case. Comparing Figures 4 and 3 and Tables 3 and 2\\nwe can see that balancing the data sets improves macro F (except for pul-nb-b)\\nbut systematically deteriorates the best values of micro F. We believe that this\\nbehaviour is caused because balancing improves the classifiers associated to MPs\\nhaving a low number of interventions, but may worsen those MPs with a greater\\nnumber of interventions (which are those more influencing in the value of micro\\nF). The t-tests in this case indicate that there are not significant differences\\nbetween pul-km-b and bas-b, but both pul-km-b and bas-b are significantly better\\nthan pul-nb-b.\\n11\\nApproach\\nbas\\npul-km\\npul-nb\\nMicro-F\\nValue\\n0.2978\\n0.3105\\n0.2802\\nThreshold\\n0.1\\n0.1\\n0.3\\nMacro-F\\nValue\\n0.2475\\n0.2644\\n0.2454\\nThreshold\\n0.1\\n0.1\\n0.2\\nTable 2: Best micro and macro F values obtained by bas, pul-km and pul-nb.\\nApproach\\nbas-b\\npul-km-b\\npul-nb-b\\nMicro-F\\nValue\\n0.2940\\n0.3012\\n0.2643\\nThreshold\\n0.4\\n0.6\\n0.6\\nMacro-F\\nValue\\n0.2732\\n0.2751\\n0.2364\\nThreshold\\n0.4\\n0.6\\n0.5\\nTable 3: Best micro and macro F values obtained by bas-b, pul-km-b and pul-nb-b.\\n4.3. Results when increasing the number of initiatives where MPs must inter-\\nvene\\nIn all the previous experiments, we have built classifiers for all the MPs who\\nparticipate in at least 10 initiatives. This constitutes a very heterogeneous set\\nof MPs: there are MPs which participate in hundreds of initiatives and other,\\nmore passive, that scarcely intervene in the debates. Our goal in this section\\nis to evaluate the proposed approaches when we impose a greater limit to the\\nnumber of initiatives where MPs must intervene in order to be included in the\\nstudy.\\nTherefore, we have repeated the experiments but including only those MPs\\nwho participate in at least 25, 75, and 150 initiatives. Our hypothesis is that\\nthe results in these cases will be progressively better, because a greater number\\nof interventions will exclude those MPs whose classifiers are less accurate due\\nto the use of poor training sets.\\nTable 4 displays the best F values for the\\nthree approaches (in both the imbalanced and balanced cases). We also show in\\nFigure 5, the micro and macro F measures obtained by pul-km using different\\nthresholds4.\\nWe can see in Table 4 that indeed the results with all the approaches sys-\\ntematically improve as the number of interventions required increases (this fact\\nis also confirmed in Figure 5). We can also observe that the relative merits\\n4We do not show the corresponding figures for the other approaches to save space, but the\\ntrends are completely similar.\\n12\\nFigure 4: Micro and Macro F measures for bas-b, pul-km-b and pul-nb-b using different thresh-\\nolds.\\nof each approach remain unmodified: pul-km is the best approach followed by\\nbas, being pul-nb the worst. It is also apparent that balancing the data sets is\\nalways counterproductive for the micro F measure. Moreover, when the MPs\\nbeing considered have a great number of interventions (75 and 150) balancing\\nis not useful either for the macro F measure.\\n4.4. Comparison with information retrieval-based approaches\\nIn this section we are going to compare our proposed approach, pul-km, with\\ntwo of the information retrieval-based approaches mentioned in Section 2 [10].\\n13\\nApproach\\nbas\\nbas-b\\npul-km\\npul-km-b\\npul-nb\\npul-nb-b\\nMicro-F\\nmF10\\n0.2978\\n0.2940\\n0.3105\\n0.3012\\n0.2802\\n0.2643\\nmF25\\n0.3037\\n0.3038\\n0.3175\\n0.3084\\n0.2859\\n0.2705\\nmF75\\n0.3558\\n0.3597\\n0.3768\\n0.3647\\n0.3437\\n0.3072\\nmF150\\n0.4408\\n0.3987\\n0.4446\\n0.4171\\n0.4183\\n0.3584\\nMacro-F\\nMF10\\n0.2475\\n0.2732\\n0.2644\\n0.2751\\n0.2454\\n0.2364\\nMF25\\n0.2658\\n0.2920\\n0.2863\\n0.2941\\n0.2630\\n0.2511\\nMF75\\n0.3355\\n0.3563\\n0.3694\\n0.3629\\n0.3361\\n0.2887\\nMF150\\n0.4039\\n0.3761\\n0.4236\\n0.3984\\n0.3976\\n0.3407\\nTable 4: Best micro and macro F values obtained by bas, bas-b, pul-km, pul-km-b, pul-nb and\\npul-nb-b with different minimum number of interventions.\\nThey use the documents in D to feed an Information Retrieval System (IRS)5.\\nIn both cases the document to be filtered/recommended is used as a query to\\nthe IRS, which returns a ranked list of the MPs which are more similar to it.\\nIn one case the documents to be indexed by the IRS are all the interventions\\nof all the MPs in the training set, i.e. just the documents in D. We call this\\napproach ir-i. In the other case we first build a kind of profile for each MP, by\\ngrouping together all her interventions in a single document (all the documents\\nin Di form a single document di = ∪mi\\nj=1dij). Then these “macro” documents are\\nindexed by the IRS. We call this second approach ir-p. In both cases, as each\\ndocument is unambiguously associated with an MP, we can replace the ranking\\nof documents by a ranking of MPs. However, in the ir-i approach the ranking\\nof MPs may contain duplicate MPs having different scores (which correspond\\nto different interventions of the same MP). Therefore, in this case we remove\\nall the occurrences of an MP except the one having the maximum score.\\nNotice that the scores returned by the IRS are affected by the number of\\nterms in the query. As we are using a single threshold to recommend a document\\nto those MPs whose score is greater than the threshold, we need to normalize\\nthe scores by dividing by the maximum score. In this way we make the range\\nof the scores independent on the query.\\nTable 5 displays the best F values for the the two IR-based approaches (we\\nrepeat in the table the results for pul-km to ease the comparison).\\nIt is evident that pul-km clearly outperforms the IR-based approaches. In\\nfact, the t-tests indicate statistically significant differences between pul-km and\\nboth ir-i and ir-p in all the cases (except in two cases of macro F, one with ir-i\\nand size 150, and the other with ir-p and size 25).\\nAlthough we are not going to display all the figures showing how ir-i and ir-p\\n5In our experiments we have used the implementation in the search engine library Lucene\\n(https://lucene.apache.org) of the BM25 information retrieval model.\\n14\\nFigure 5: Micro and Macro F measures for pul-km using different thresholds, for a minimum\\nof 10, 25, 75 and 150 interventions.\\nvary depending on the thresholds being used, we include in Figure 6 the micro\\nand macro F values for ir-i (the figures for ir-p are completely similar) in order\\nto illustrate a clear difference in the behaviour of the IR-based approaches with\\nrespect to pul-km. We can see how the F measures for ir-i increase as the thresh-\\nold increases, just the opposite of what it happens with pul-km. Thus pul-km\\nworks better with low or medium thresholds but ir-i requires large thresholds.\\nThe reason may be in the different interpretation that these thresholds have\\nwithin each approach, probability of relevance in one case and similarity in the\\nother.\\n15\\nMicro-F\\nMacro-F\\nApproach\\npul-km\\nir-i\\nir-p\\npul-km\\nir-i\\nir-p\\n10\\n0.3105\\n0.2896\\n0.2892\\n0.2644\\n0.2423\\n0.2513\\n25\\n0.3175\\n0.2971\\n0.2939\\n0.2863\\n0.2661\\n0.2829\\n75\\n0.3768\\n0.3509\\n0.3085\\n0.3694\\n0.3288\\n0.3368\\n150\\n0.4446\\n0.4282\\n0.3120\\n0.4236\\n0.3948\\n0.3530\\nTable 5: Best micro and macro F values obtained by pul-km, ir-i and ir-p, with different\\nminimum number of interventions.\\n5. Concluding remarks\\nIn this paper we have proposed an approach to build a system able to rec-\\nommend/filter documents to the Members of Parliament, which is based on\\nmachine learning techniques, more precisely automatic document classification.\\nThe source data to train the classifiers are the interventions of the MPs in the\\nparliamentary debates, under the assumption that these interventions reveal\\ninformation about their political interests and preferences. However, the inter-\\nventions of an MP only give information about what is relevant for her, but the\\ninformation about what is not relevant is missing. For that reason our approach\\nuses positive unlabeled learning methods, as we cannot rely on traditional clas-\\nsifiers trained with both positive and negative examples. In this context, we\\nhave also proposed a new PUL method, pul-km, that first obtain a set of reli-\\nable negative examples from the set of unlabeled examples (the interventions of\\nthe other MPs), and then uses the set of positive and reliable negative examples\\nto train a traditional binary classifier (SVM in our case). Our method to obtain\\nthe set of reliable negative examples is based on a modification of the classical\\nK-means algorithm for clustering. We have also considered to complement this\\nprocedure with an algorithm to deal with the possible class imbalance problem\\n(using SMOTE for this purpose).\\nIn our experiments, based on a collection of MP’s interventions in the Parlia-\\nment of Andalusia, we have compared pul-km with other approaches: a baseline\\napproach that considers that all the unlabeled examples are negative exam-\\nples, another existing PUL method based on Naive Bayes, pul-nb, and other\\ntwo methods based on information retrieval that index the collection of inter-\\nventions and retrieve the MPs which are more similar to the document to be\\nrecommended. In all the experiments our approach obtains better results than\\nits opponents, most of the time with statistically significant differences. There-\\nfore, pul-km appears as a good approach to tackle this recommendation problem.\\nMoreover, the fact that pul-km clearly outperforms the state-of-the-art pul-nb is\\nalso a strong evidence that it has potential to be useful in other problems where\\nPUL methods are necessary.\\nGiven the results obtained with and without using the SMOTE method to\\ndeal with imbalanced data sets, we have observed that its use deteriorates micro\\nF but tends to improve macro F (except in the case of pul-nb, where macro F\\n16\\nFigure 6: Micro and Macro F measures for ir-i using different thresholds, for a minimum of\\n10, 25, 75 and 150 interventions.\\nis also worsened). This probably means that using SMOTE is only advisable\\nfor those MPs having a low number of interventions. Therefore, an interesting\\nfuture research would be to design strategies to perform “selective balancing”,\\ni.e. to decide which classifiers (associated to different MPs) would benefit from\\nusing methods for balancing data sets. As this operation changes the thresholds\\nthat the classifiers need to use to perform better (in our case moving from low\\nthresholds to others located near 0.5, the “natural” threshold), another inter-\\nesting line of research would be to study methods to select different thresholds\\nfor different classifiers. Finally, we would also like to explore the use of fea-\\nture selection methods [30] (term selection in this case) for our recommendation\\nproblem.\\n17\\nAcknowledgement\\nThis work has been funded by the Spanish “Ministerio de Econom´ıa y Com-\\npetitividad” under projects TIN2013-42741-P and TIN2016-77902-C3-2-P, and\\nthe European Regional Development Fund (ERDF-FEDER).\\nReferences\\n[1] R. Baeza-Yates, B. Ribeiro-Neto, Modern Information Retrieval, Addison-\\nWesley, 2011.\\n[2] N.J. Belkin, W.B. Croft, Information filtering and information retrieval:\\ntwo sides of the same coin?, Communications of the ACM 35 (1992) 29–38.\\n[3] D. Billsus, M. Pazzani, J. Chen, A learning agent for wireless news ac-\\ncess, in: Proceedings of the International Conference on Intelligent User\\nInterfaces, 2002, pp. 33–36.\\n[4] J. Bobadilla, A. Hernando, O. Fernando, A. Guti´errez, Recommender sys-\\ntems survey, Knowledge Based Systems 46 (2013) 109–132.\\n[5] B. Calvo, P. Larra˜naga, J.A. Lozano, Learning Bayesian classifiers from\\npositive and unlabeled examples, Pattern Recognition Letters 28 (2007)\\n2375–2384.\\n[6] O. Chapelle, B. Sch¨olkopf, A. Zien, Eds., Semi-Supervised Learning, MIT\\nPress, 2006.\\n[7] N.V. Chawla, K.W. Bowyer, L.O. Hall, W.P. Kegelmeyer, Smote: syn-\\nthetic minority over-sampling technique, Journal of Artificial Intelligence\\nResearch 16 (2002) 321–357.\\n[8] W. Cohen, Learning rules that classify e-mail, in: Papers from the AAAI\\nSpring Symposium on Machine Learning in Information Access, 1996, pp.\\n18–25.\\n[9] N. Cristianini, J. Shawe-Taylor, An introduction to Support Vector Ma-\\nchines and other kernel-based learning methods, Cambridge University\\nPress, 2000.\\n[10] L.M. de Campos J.M. Fern´andez-Luna, J.F. Huete, A lazy approach for\\nfiltering parliamentary documents, in: A. K¨o, E. Francesconi (Eds.), Elec-\\ntronic Government and the Information Systems Perspective, Lecture Notes\\nin Computer Science 9265, 2015, pp. 364–378.\\n[11] L.M. de Campos, J.M. Fern´andez-Luna, J.F. Huete, Profile-based recom-\\nmendation: a case study in a parliamentary context, Journal of Information\\nScience, to appear.\\n18\\n[12] L.M. de Campos, J.M. Fern´andez-Luna, J.F. Huete, C.J. Martin-Dancausa,\\nC. Tur-Vigil, A. Tagua, An integrated system for managing the andalusian\\nparliament’s digital library, Program: Electronic Library and Information\\nSystems 43 (2009) 121-139.\\n[13] F. Denis, R. Gilleron, M. Tommasi, Text classification from positive and\\nunlabeled examples, in: Proceedings of the 9th International Conference\\non Information Processing and Management of Uncertainty in Knowledge-\\nBased Systems, 2002, pp. 1927–1934.\\n[14] M.C. du Plessis, G. Niu, M. Sugiyama, Class-prior estimation for learning\\nfrom positive and unlabeled data, Machine Learning 106 (2017) 463–492.\\n[15] P. Foltz, S. Dumais, Personalized information delivery: an analysis of in-\\nformation filtering methods, Communications of the ACM 35 (1992) 51–60.\\n[16] G.P.C. Fung, J.X. Yu, H.J. Lu, P.S. Yu, Text classification without negative\\nexamples revisit, IEEE Transactions on Knowledge and Data Engineering\\n18 (2006) 6–20.\\n[17] H. Gan, Y. Zhang, Q. Song, Bayesian belief network for positive unlabeled\\nlearning with uncertainty, Pattern Recognition Letters 90 (2017) 28–35.\\n[18] U. Hanani, B. Shapira, P. Shoval, Information filtering: Overview of issues,\\nresearch and systems, User Modelling and User-Adapted Interaction 11\\n(2001) 203–259.\\n[19] J. Hern´andez-Gonz´alez, I. Inza, J.A. Lozano, Learning from proportions\\nof positive and unlabeled examples, International Journal of Intelligent\\nSystems 32 (2017) 109–133.\\n[20] J. Kim, B. Lee, M. Shaw, H. Chang, W. Nelson, Application of decision-tree\\ninduction techniques to personalized advertisements on internet storefronts,\\nInternational Journal of Electronic Commerce 5 (2001) 45–62.\\n[21] A. Jennings, H. Higuchi, A user model neural network for a personal news\\nservice, User Modelling and User-Adapted Interaction 3 (1993) 1–25.\\n[22] B. Lantz, Machine Learning with R, Packt Publishing Ltd, 2013.\\n[23] W.S. Lee, B. Liu, Learning with positive and unlabeled examples using\\nweighted logistic regression, in: Proceedings of the Twentieth International\\nConference on Machine Learning, 2003, pp. 448–455.\\n[24] X.L. Li, B. Liu, Learning to classify texts using positive and unlabeled data,\\nin: Proceedings of the 18th International Joint Conference on Artificial\\nIntelligence, 2003, pp. 587–594.\\n[25] C. Liang, Y. Zhang, P. Shi, Z. Hu, Learning very fast decision tree from\\nuncertain data streams with positive and unlabeled samples, Information\\nSciences 213 (2012) 50–67.\\n19\\n[26] B. Liu, W.S. Lee, P.S. Yu, X.L. Li, Partially supervised classification of text\\ndocuments, in: Proceedings of the Nineteenth International Conference on\\nMachine Learning, 2002, pp. 387–394.\\n[27] B. Liu, Y. Dai, X. Li, W.S. Lee, P.S. Yu, Building text classifiers using posi-\\ntive and unlabeled examples, in: Proceedings of the 3rd IEEE International\\nConference on Data Mining, 2003, pp. 179–186.\\n[28] S. Loeb, Architecting personal delivery of multimedia information, Com-\\nmunications of the ACM 35 (1992) 39–48.\\n[29] J. Lu, D. Wu, M. Mao, W. Wang, G. Zhang, Recommender system appli-\\ncation developments: a survey, Decision Support Systems 74 (2015) 12–32.\\n[30] S. Maldonado, R. Weber, F. Famili, Feature selection for high-dimensional\\nclass-imbalanced data sets using Support Vector Machines, Information\\nSciences 286 (2014) 228–246.\\n[31] F. Narducci, P. Basile, C. Musto, P. Lops, A. Caputo, M. de Gemmis,\\nL. Iaquinta, G. Semeraro, Concept-based item representations for a cross-\\nlingual content-based recommendation process, Information Sciences 374\\n(2016) 15–31.\\n[32] M. Pazzani, D. Billsus, Learning and revising user profiles: the identifica-\\ntion of interesting web sites, Machine Learning 27 (1997) 313–331.\\n[33] M. Pazzani, D. Billsus, Content-based Recommendation Systems, in: The\\nAdaptive Web, LCNS 4321, 2007, pp. 325–341.\\n[34] F.J. Ribadas, L.M. de Campos, J.M. Fern´andez-Luna, J.F. Huete, Concept\\nprofiles for filtering parliamentary documents, in: Proceedings of the 7th\\nInternational Joint Conference on Knowledge Discovery, Knowledge Engi-\\nneering and Knowledge Management - Volume 1: KDIR, 2015, 409–416.\\n[35] F. Sebastiani, Machine learning in automated text categorization, ACM\\nComputing Surveys 34 (2002) 1–47.\\n[36] J. Shamin, C. Neuhold, ‘Connecting Europe’: The use of ‘new’ informa-\\ntion and communication technologies within European parliament standing\\ncommittees. The Journal of Legislative Studies 13 (2007) 388–402.\\n[37] A.M. Tjoa, M. Hofferer, G. Ehrentraut, P. Untersmeyer, Applying evolu-\\ntionary algorithms to the problem of information filtering, in: Proceedings\\nof the 8th International Workshop on Database and Expert Systems Ap-\\nplications, 1997, pp. 450–458.\\n[38] G. Tsoumakas, I. Katakis, I.P. Vlahavas, Mining multi-label data, in: O.\\nMaimon, L. Rokach (Eds.), Data Mining and Knowledge Discovery Hand-\\nbook, Springer-Verlag, 2010, pp. 667–685.\\n20\\n[39] H. Yu, J. Han, K.C.-C. Chang, Pebl: positive example based learning for\\nweb page classification using SVM, in: Proceedings of the eighth ACM\\nSIGKDD International Conference on Knowledge Discovery and Data Min-\\ning, 2002, pp. 239–248.\\n[40] S. Zahra, M.A. Ghazanfar, A. Khalid, M.A. Azam, U. Naeem, A. Prugel-\\nBennett, Novel centroid selection approaches for KMeans-clustering based\\nrecommender systems, Information Sciences 320 (2015) 156–189.\\n[41] B. Zhang, W. Zuo, Learning from positive and unlabeled examples: a sur-\\nvey, in: International Symposiums on Information Processing, 2008, pp.\\n650–654.\\n21\\n'},\n",
       " {'abstract': 'In this study, we examine the effectiveness of a large language model (LLM)-based support chatbot compared to a traditional keyword-based chatbot system using a randomized controlled trial. Our results indicate that the LLM-based chatbot significantly reduces escalation rates, whereby users seek assistance from a back-end engineer. The overall escalation rate decreased from 17.1% to 7.9% when using the LLM-based chatbot, representing a 53.8% reduction. Additionally, we compared two versions of the LLM-based chatbot, one powered by GPT3.5 and the other by GPT4, and found no significant difference in escalation rates between the two. However, the GPT3.5-based chatbot was more cost-efficient.',\n",
       "  'introduction': 'The study explores the impact of a large language model (LLM)-based support chatbot compared to a conventional chatbot system. Numerous studies have examined LLM tools in various settings, demonstrating their productivity-enhancing effects. However, field experiments applying LLM tools in realistic scenarios are limited. This study aims to evaluate the effectiveness of LLM-based tools in providing unmonitored support services for information retrieval.',\n",
       "  'literature_review': 'Prior research has investigated the impact of LLM-based tools on productivity in different settings, including lab tasks, observational studies, and field experiments. The studies generally indicate that LLM-based tools can enhance user productivity significantly. However, there is a lack of field experiments applying LLM tools in realistic settings.',\n",
       "  'methodology': \"The study conducted a randomized controlled trial comparing the performance of a traditional chatbot system with that of an LLM-based chatbot. The trial involved two waves, with the first wave comparing the LLM-based chatbot with the classical bot, and the second wave introducing a GPT3.5-supported version. The primary outcome of interest was the escalation rate, defined as the user's decision to escalate the inquiry to a back-end engineer.\",\n",
       "  'results': 'The study found that the LLM-based chatbot significantly reduced the escalation rate compared to the classical bot. The overall escalation rate decreased from 17.1% to 7.9%, representing a 53.8% reduction. Additionally, there was no significant difference in escalation rates between the GPT3.5 and GPT4-based versions of the LLM-based chatbot, though the GPT3.5 version was more cost-efficient.',\n",
       "  'conclusion': 'The study provides evidence of the effectiveness of LLM-based chatbots in reducing escalation rates and improving the overall support service quality. The findings contribute to the growing literature on the productivity-enhancing effects of LLM-based tools.',\n",
       "  'title': 'AI Revolution on Chat Bot: Evidence from a Randomized Controlled Experiment',\n",
       "  'author': 'Sida Peng, Wojciech Swiatek, Allen Gao, Paul Cullivan, Haoge Chang',\n",
       "  'textdata': 'AI Revolution on Chat Bot: Evidence from a\\nRandomized Controlled Experiment\\nSida Peng∗\\nWojciech Swiatek†\\nAllen Gao‡\\nPaul Cullivan§\\nHaoge Chang¶\\nNovember 2023\\n1\\nIntroduction\\nIn recent years, generative AI has undergone major advancements, demonstrat-\\ning significant promise in augmenting human productivity. Notably, large lan-\\nguage models (LLM), with ChatGPT-4 as an example, have drawn considerable\\nattention. Many companies are now incorporating LLM-based tools within their\\norganizations and integrating it with various products (OpenAI, 2023a; Forbes,\\n2023). There is increasing interest on evaluating the impact of LLMs on human\\ndecision-making and productivity.\\nNumerous articles have examined the impact of LLM-based tools in lab set-\\ntings or designed tasks (Peng et al., 2023; Spatharioti et al., 2023; Noy and\\nZhang, 2023; Dell’Acqua et al., 2023), or in observational studies (Brynjolfsson,\\nLi, and Raymond, 2023). In these investigations, LLM-based tools are deployed\\n∗Office of Chief Economist, Microsoft\\n†Microsoft\\n‡Microsoft\\n§Microsoft\\n¶Microsoft Research\\n1\\narXiv:2401.10956v1  [cs.HC]  19 Jan 2024\\nto aid humans in various tasks, with measured outcomes including task comple-\\ntion time and accuracy. It is generally observed that LLM-based tools are able\\nto increase users’ productivity substantially.\\nDespite recent advances, field experiments applying LLM-based tools in real-\\nistic settings are limited. This paper presents the findings of a field randomized\\ncontrolled trial assessing the effectiveness of LLM-based tools in providing un-\\nmonitored support services for information retrieval.\\nWhile superior service\\nquality is expected from LLM-based tools, concerns such as hallucinations raise\\nquestions about their effectiveness. Consequently, an empirical investigation is\\nnecessary.\\nWe collaborated with a team managing support chat bots that support Mi-\\ncrosoft’s internal developers. Prior to adopting GPT-based models, these bots\\noperated on a flowchart-based system called Power Virtual Agents (PowerVA,\\nsee Figure 1). Users navigated through predefined categories to find document\\nlinks potentially relevant to their queries.\\nIn our experiment, we integrated a particular support bot, the Work Man-\\nagement Support Bot, with GPT-based tools and compared its performance to\\nthe existing keyword-based flowchart support bot. This bot is tailored to assist\\nMicrosoft software developers with login and access issues. The new GPT-based\\napproach features a bot (hereafter GPT-based bot) that enables users to ask\\nquestions in natural language and receive direct answers from the same docu-\\nment sources used by the flowchart support bot (hereafter classical bot).\\nThe primary outcome of interest in our study is the escalation decision,\\nwhich is defined by a user’s decision to escalate the inquiry and seek support\\nfrom the back-end engineer. A good support bot should lead to low escalation\\nrates. Our experiment suggests that the GPT-based bot reduces the escalation\\nrate by 9.2 percentage compared with the classical bot. This represents a 53.8\\n2\\npercent reduction in escalation rate relative to the baseline of the classical bot.\\nIn addition, we implemented two versions of the GPT-based support bots,\\none based on the GPT4 model and the other based on the GPT3.5 model. We\\ncompare their escalation rates and token consumption. While we find no sig-\\nnificant difference in escalation rates between the two models, there are notable\\ndifferences in token usage: although GPT4-based bot consumes less tokens per\\nquestion on average, GPT3.5-based bot has a price advantage under the current\\npricing structure of GPT models (OpenAI, 2023b).\\nOur results add to the literature of experimental and observational research\\non the productivity-enhancing effects of LLM-based tools. For instance, Noy\\nand Zhang, 2023 examines the individual and distributional productivity effects\\nof ChatGPT on writing task completion and quality among experienced, college-\\neducated professionals. Peng et al., 2023 investigates how GitHub Copilot affects\\nsoftware developers’ productivity in programming tasks. Dell’Acqua et al., 2023\\nrandomizes GPT4 tools to consultants and measures the change in productivity.\\nSpatharioti et al., 2023 evaluates LLM-based tools in aiding consumers with\\nsearch tasks. Finally, Brynjolfsson, Li, and Raymond, 2023 studies the impact\\nof AI tools on productivity in an observational setting using a difference-in-\\ndifferences approach.\\nThe rest of the paper is as follows. Section 2 introduces backgrounds and\\nexperimental designs. Section 3 contains results of the experiments, and Section\\n4 includes details on the specifications of our statistical analysis.\\n2\\nBackgrounds and Experimental Designs\\nWe conducted a randomized control trial (RCT) to compare the performance of\\nthe Power Virtual Agents (classical) based bot with the GPT-based bot. This\\nchatbot assists Microsoft developers with login and access issues. All traffic to\\n3\\nthe chatbot service was randomized into control and treatment groups. The\\ncontrol group interacted with the existing bot, based on the PowerVA service,\\noffering a flowchart-style experience where users navigate predefined options\\nto find documents addressing their questions. The treatment group used the\\nnew GPT-based bot, which allows users to pose questions in natural language\\nand provides direct answers from the same document sources as the classical\\nbot. Figure 2 depicts the PowerVA-based support bot experience, and Figure 3\\nillustrates the GPT-based support bot experience.\\nIf developers are unsatisfied with the bot’s responses, they have the option\\nto escalate their questions for real human intervention. Our back-end engineers\\nassign 10 minutes to resolve each escalated case, and most cases are resolved\\nwithin this estimate. The traffic varies between 5 to 20 cases per week, depend-\\ning on seasonality. One may witness heavier-than-usual traffic during Monday,\\nspecific months, and after reorganizations when some users lose access and turn\\nto the bot to request new permissions.\\nOur experiments had two waves. The first wave took place between May\\n05, 2023 and July 21, 2023, and the second wave from July 21, 2023 to Oct 12,\\n2023. In the first wave, users in each session were randomly assigned to either\\nthe classical bot or the GPT4-based version. In the second wave, a new GPT3-\\nsupported version was introduced, and and users were randomly assigned to one\\nof three options: the classical bot, the GPT4-based bot, or the GPT3.5-based\\nbot. No additional instructions were provided on how users should interact with\\nthe support bots.\\nFor each session, we collected data such as session ID, starting time of the\\nsession, engagement decision, duration of the engagement, and escalation deci-\\nsion. For the GPT-based versions, we were able to collect data such as users’\\nfirst prompts and support bot’s first responses.\\n4\\nThe primary outcome of our study is the escalation decision, which reflects\\na user’s choice to escalate their inquiry and seek assistance from a back-end\\nengineer. An effective support bot should accurately comprehend a user’s ques-\\ntion, retrieve the correct information, and provide clear responses. Ultimately, a\\nhigh-quality support bot should result in a low escalation rate, thereby reducing\\nthe workload for back-end engineers in supporting users.\\n3\\nResults\\nWe collected data on 3296 sessions over a span of five months. There are 1413\\nengaged cases and 165 escalations. We calculate the overall engagement rate as:\\nOverall Engagement Rate = 1413\\n3296 = 42.9%.\\nAmong the engaged cases, the overall escalation rate is calculated as\\nOverall Escalation Rate = 165\\n1413 = 11.7%.\\n3.1\\nEngagement Rates\\nUsers may not to engage with the support bot services for various reasons.\\nFor instance, the support bots automatically initiate a login action when the\\nconversation starts. This login process can take anywhere from 5 to 30 seconds,\\nduring which users might refresh the page, inadvertently skipping the existing\\nsession.\\nAdditionally, some users may accidentally click on the wrong page,\\nleading to unintended visits to the support bot service.\\nIn our analysis, we concentrate on the outcomes of engaged sessions to evalu-\\nate the quality of the support bots. A GPT-based bot session is deemed engaged\\nif the user posts a question and the bot responds. For a classical bot session,\\n5\\nengagement is defined as the user utilizing at least one functionality of the bot.\\nOur findings indicate no significant difference in engagement rates between clas-\\nsical sessions and GPT-based sessions (difference-in-means = -0.026, t-statistics\\n= -1.457, p-value = 0.145).\\n3.2\\nPrimary Outcome: Escalation Rate\\nWe observe 66 escalations out of 835 engaged sessions with the GPT-based sup-\\nport bots, and 99 escalations out of the 578 engaged sessions with the classical\\nsupport bots.\\nWe calculate the average escalation rate as:\\nAverage Escalation Rate (GPT-based bot) = 66\\n835 = 7.9%\\nAverage Escalation Rate (classical bot) = 99\\n578 = 17.1%\\nThis is a significant 9.2 percentage point reduction (t-statistics=-5.05, p-value=4.9e-\\n07) in the average escalation rate when using the GPT-based support bots.\\nIn a relative term, this represents a 53.8 percent (9.2/17.1) reduction in av-\\nerage escalation rate compared to the baseline escalation rate of 17.1 percentage\\npoints for the classical support bot.\\n3.3\\nComparing GPT3.5 and GPT4\\nIn the second wave of our experiment and after July 21st 2023, we increased the\\nnumber of sessions that are assigned to the GPT-based support bot. Further,\\nwe randomized sessions into either GPT3.5-based support bot or GPT4-based\\nsupport bot. During the period between July 21st, 2023 and October 11th, 2023,\\nwe collected information on 190 engaged cases for the GPT4-based support bot\\nand 203 engaged cases for the GPT3.5-based support bot.\\n6\\nAmong the 203 engaged cases for the GPT3.5-based version, there were 17\\nescalations, and among the 190 engaged cases for GPT4-based version, there\\nwere 19 escalations. The average escalation rates for the GPT3.5-based and\\nGPT4-based versions are\\nAverage Escalation Rate (GPT3.5-based bot) = 17\\n203 = 8.4%\\nAverage Escalation Rate (GPT4-based bot) = 19\\n190 = 10%\\nThe escalation rate of the GPT4-based version is slightly higher than that\\nof the GPT3.5-based version, but the difference is not statistical significant (t-\\nstatistics=0.556, p-value=0.579).\\nWe also compared the GPT3.5-based and GPT4-based versions in terms of\\ninput and output token consumption. Input and output tokens form the basis\\nfor cost calculations when using GPT-based services (OpenAI, 2023b). Since\\nthe pricing plans for tokens vary depending on the version used, our comparison\\nhere is relevant to cost considerations.\\nInput Tokens\\nOutput Tokens\\nGPT3.5-based bot\\n2575.0\\n190.3\\nGPT4-based bot\\n2018.4\\n235.5\\nTable 1: Average token consumptions for GPT3.5-based and GPT4-based bots.\\nWe found that users’ questions consume from 290 to 3363 input tokens,\\nfrom 38 to 300 output tokens, and from 500 to 3620 total tokens (input tokens\\n+ output tokens). Figure 4 plots token consumption for both GPT3.5-based and\\nGPT4-based bots. As shown in Table 1, on average the GPT3.5-based bot con-\\nsumes more input tokens than the GPT4-based bot does (difference-in-means=-\\n556.6, t-value=-6.864, p-value=2.6e-11). The GPT4-based bot consumes more\\noutput tokens than the GPT3.5-based bot (difference-in-means=45.2, t-value=0.557,\\n7\\np-value=0.578), but this difference is not statistically significant.\\nThe variation in input token consumption between the two versions can be\\nexplained by the fact that GPT4 is able to process several frequently-referenceed\\ndocuments with less resources.\\nThe difference of the output tokens can be\\nexplained by the fact that GPT4-based version often outputs longer but more\\nstructured responses. For example, for the same prompt ”I need access”, GPT4-\\nbased bot outputs\\nTo request access, you need to follow the appropriate steps based on the\\ntype of access you require. Here’s a general outline of the process:\\n1. Determine the type of access you need:\\n• Access Type 1\\n• Access Type 2\\n• ...\\n2. Visit the [NAME](URL) portal.\\n3. Depending on the type of access you need, follow the appropriate\\nlink(s) provided in the documentation to request access.\\n4. Fill out the required information and submit your request.\\n5. Wait for approval from the appropriate authority based on your\\norganization and the type of access you’re requesting.\\n6. Please refer to the documentation for specific links and approval\\nrequirements based on your organization and the type of access you\\nneed. If you have any issues or need further assistance, contact the\\n[NAME](URL).\\n8\\nFor the GPT3.5-based bot, it outputs\\nHi there! You need access to resources that are managed by Microsoft.\\nTo get access, you need to go through [NAME](URL). [ServiceName] is\\nsupported by [Name](URL).\\nFor work item/resource access for [Projects] that are not [Projects], you\\ncan ask [NAME](URL) to determine who that is.\\nFor full legacy source access, you can apply [NAME](URL). For legacy\\ntest sources only, you can apply [NAME](URL). For legacy branch own-\\ners’ publics, you can apply [NAME](HERE)\\nWe can calculate average cost per question for both GPT-3.5 based bot and\\nGPT4-based bot, using the existing pricing schedule (OpenAI, 2023b) (price\\nbased on Azure public price at Dec 2023). Based on the information in Table 1,\\nthe average cost per question is 7.5 cents (0.03*2018.4/1000 + 0.06*235.5/1000)\\nfor GPT4-based bot and 0.3 cents (0.0010*2575/1000 + 0.0020*190.3/1000 )\\nfor GPT3.5-based bot. Provided that the GPT3.5-based bot and GPT4-bot are\\nable to provide similar experience, GPT3.5-based bot offers a more cost-efficient\\nalternative compared with the GPT4-based bot.\\n3.4\\nRobustness Check\\nOur randomization happens on a per-session level, so a user who use multi-\\nple sessions may see both the GPT-based and classical versions with the same\\nquestion. We’ll use a different experimental design to prevent this complication\\nin the next round. In this session, we conduct a robustness check, specifically\\nlooking at users’ escalation decisions in the first session.\\nWe recorded 679 sessions with user alias between September 12, 2023 to\\nOctober 12, 2023. There are 310 engaged cases during this time period and 263\\n9\\ncases are first sessions of users’ interactions with the bots. There are 160 engaged\\nGPT-based bot sessions with 10 escalations, and 103 Classical bot sessions with\\n21 escalations. We calculate the average escalation rate as:\\nAverage Escalation Rate (GPT-based bot) = 10\\n160 = 6.3%\\nAverage Escalation Rate (classical bot) = 21\\n103 = 20.4%\\nThis is a significant 14.1 percentage point reduction (t-statistics=-3.19, p-value=0.001)\\nin the average escalation rate when using the GPT-based support bots. It ap-\\npears that results based on the restricted samples are qualitatively similar to\\nthe results reported in Section 3.2.\\n4\\nRegression Details\\nWe used the statistical programming language R to analyze the data.\\nThe\\nstandard errors of all regression results are calculated using the HC2 formula,\\nimplemented in the sandwich (Zeileis et al., 2019) package in R. The t-tests are\\ncalculated using the lmtest (Hothorn et al., 2015) package in R .\\n4.1\\nEngagement Rates\\nThe reported results in Section 3.1 are based on the regression specification:\\nEngagement = β0 + β1Version + ϵ,\\nwhere Engagement is a binary variable, with 1 indicating that the user engaged\\nwith the support bot and 0 otherwise. Version is also a binary variable, with 1\\nrepresenting a GPT-based support bot and 0 representing the classical version\\nof the support bot.\\n10\\n4.2\\nEscalation Rates\\nThe reported results in 3.2 are based on the regression specification:\\nEscalation = β0 + β1Version + ϵ,\\nwhere Escalation is a binary variable, with 1 indicating the user escalated the\\nissue to a back-end engineer and 0 otherwise. Version is defined as in Section\\n4.1. We limited our analysis to engaged sessions.\\n4.3\\nComparing GPT3.5 and GPT4\\nThe reported results in 3.2 are based on the regression specification:\\nEscalation = β0 + β1GPT4 + ϵ,\\nwhere Escalation is a binary variable, with 1 indicating that the user has esca-\\nlated the issue to a back-end engineer and 0 otherwise. GPT4 is also a binary\\nvariable, with 1 representing a GPT4-based support bot and 0 indicating a\\nGPT3.5-based bot.\\nWe restricted samples to engaged sessions.\\nThe regres-\\nsion specifications for the input tokens and output tokens are similar but with\\ndifferent dependent variables.\\n4.4\\nRobustness Check\\nThe reported results in Section 3.4 use the same specifications as those reported\\nin Section 3.2, with the constructed sample described in Section 3.4.\\n11\\nReferences\\n[1]\\nErik Brynjolfsson, Danielle Li, and Lindsey R Raymond. Generative AI\\nat work. Tech. rep. National Bureau of Economic Research, 2023.\\n[2]\\nFabrizio Dell’Acqua et al. “Navigating the jagged technological frontier:\\nField experimental evidence of the effects of AI on knowledge worker pro-\\nductivity and quality”. In: Harvard Business School Technology & Oper-\\nations Mgt. Unit Working Paper 24-013 (2023).\\n[3]\\nForbes. 10 Amazing Real-World Examples Of How Companies Are Using\\nChatGPT In 2023. https://www.forbes.com/sites/bernardmarr/\\n2023/05/30/10-amazing-real-world-examples-of-how-companies-\\nare-using-chatgpt-in-2023/?sh=11a57e151441. 2023.\\n[4]\\nTorsten Hothorn et al. “Package ‘lmtest’”. In: Testing linear regression\\nmodels. https://cran. r-project. org/web/packages/lmtest/lmtest. pdf. Ac-\\ncessed 6 (2015).\\n[5]\\nShakked Noy and Whitney Zhang. “Experimental evidence on the produc-\\ntivity effects of generative artificial intelligence”. In: Available at SSRN\\n4375283 (2023).\\n[6]\\nOpenAI. Introducing ChatGPT Enterprise. https://openai.com/blog/\\nintroducing-chatgpt-enterprise. 2023.\\n[7]\\nOpenAI. Pricing. https://openai.com/pricing. 2023.\\n[8]\\nSida Peng et al. “The impact of ai on developer productivity: Evidence\\nfrom github copilot”. In: arXiv preprint arXiv:2302.06590 (2023).\\n[9]\\nSofia Eleni Spatharioti et al. “Comparing Traditional and LLM-based\\nSearch for Consumer Choice: A Randomized Experiment”. In: arXiv preprint\\narXiv:2307.03744 (2023).\\n12\\n[10]\\nAchim Zeileis et al. “Package ‘sandwich’”. In: R package version (2019),\\npp. 2–5.\\nFigure 1: Power Virtual Agents: Flow Chart Based System\\n13\\nFigure 2: Microsoft PowerVA-based Support Bot Experience\\n14\\nFigure 3: GPT-based Support Bot Experience\\n15\\nFigure 4: Distributions of Token Consumption for GPT3.5-based and GPT-4\\nbased bots\\n16\\n'},\n",
       " {'abstract': 'This study presents a model for understanding how customers’ satisfaction changes when they use an AR shopping application. It proposes that different levels of perceived experiential AR application features (informative, personalizing, and interactivity) affect the customer experience, which in turn influences customer satisfaction and purchase intention. The model also considers immersive experiences as a mediator between perceived AR application features and customer experience.',\n",
       "  'introduction': 'In an increasingly digital world, businesses are turning to AR technology to enhance the shopping experience and increase sales. This paper investigates the effects of AR technology on customers’ experience, satisfaction, and purchase intention in an online retail setting.',\n",
       "  'literature_review': 'Building on the stimulus-organism-response (S-O-R) paradigm and the information systems success model (ISS), the study reviews previous research on consumer experiences, immersion, and customer satisfaction. It draws insights from studies on informative, personalizing, and interactivity features of AR applications.',\n",
       "  'methodology': 'The study proposes a conceptual model with hypotheses linking the perceived levels of experiential AR application features, such as information, personalization, and interactivity, to customer experience, immersion, customer satisfaction, and purchase intention. Methods for measuring these constructs are discussed, including subjective data collection techniques and the use of existing scales.',\n",
       "  'results': 'The paper outlines expected results based on the proposed hypotheses. It anticipates positive relationships between perceived AR application features and customer experience, immersion, customer satisfaction, and purchase intention. Furthermore, it suggests that immersion mediates the relationship between perceived AR application features and customer experience.',\n",
       "  'conclusion': 'This study provides a deeper understanding of the role of AR technology in shaping customers’ experiences and how these experiences affect customer satisfaction and purchase behavior. It offers insights for business owners and academics in leveraging AR technology to improve customer engagement and drive sales.',\n",
       "  'title': \"How customers' satisfaction change with the use of AR shopping application: A conceptuall model\",\n",
       "  'author': 'Fariba Sanaei',\n",
       "  'textdata': ' \\n \\n \\n“HOW CUSTOMERS’ SATISFACTION CHANGE WITH THE USE OF AR SHOPPING \\nAPPLICATION:  \\nA CONCEPTUALL MODEL “  \\n \\nFariba Sanaei, University of Central Florida \\nFor further information, please contact Fariba Sanaei, PhD student, University of Central Florida \\n(fariba.sanaei@ucf.edu) \\n \\nKeywords: Digital Marketing, AR Technology, Online Retailors, User Experience.  \\nDescription: This study provides a conceptual model of how online businesses using AR \\ntechnology in their purchase stage can have effects on customer satisfaction and their purchase \\nintention, considering the mediation roles of customer experience and immersion.  \\n \\n \\n2023 AMA Winter Academic Conference\\n803\\n \\n2 \\nABSTRACT \\nThe paper proposes a conceptual model of how different perceived levels of experiential AR \\napplication features have effects on customer experience, and in turn their satisfaction and \\npurchase behavior. In addition, it put forward the mediation role of immersion between perceived \\nlevels of experiential AR application features and customers’ experience.  \\nINTRODUCTION  \\nThe world economy experienced dramatic up and downs after Covid-19 pandemic (Ostrom et al., \\n2021). Before COVID-19, ecommerce was rapidly expanding. However, the epidemic drove even \\nmore US customers online. According to Digital Commerce 360 projections, the coronavirus \\ncontributed $102.08 billion to US ecommerce in 2020 and $116.45 billion in 2021. On the other \\nhand, innovative technologies like AR have entered to marketing discipline and business owners \\nfound these technologies useful for attracting consumers. As a result, nowadays more businesses \\nare turning into AR technology to increase their sales. adopting such strategies for online shopping \\nenhances the sense of real environment in digital platforms and it can increase consumers \\nengagements in long-term. \\nThe goal of this study is to understand how adopting AR technology by business owners could be \\neffective in terms of their consumer’s purchase intention and their levels of satisfaction. By doing \\na comparative study in these field, beneficial implications can be received by business owners and \\nacademics. AR technology has entered in different areas from fashion industry to beauty \\ncosmetics. Worldwide brands like Ikea and Amazon utilizing AR services in their online selling \\nstrategies for their home furniture.  It can be anticipated that by introducing Metaverse world, more \\n2023 AMA Winter Academic Conference\\n804\\n \\n3 \\nand more businesses will turn to such innovative technologies, so more research is needed for \\nbetter and in depth understanding of relation between adopting AR technology, consumers’ \\nsatisfaction, and their purchase intention. This study is going to answer: How is the consumers’ \\nexperience of using AR applications affected, given different perceived level of experiential AR \\napplication features? How different perceived experience can result in different satisfaction levels \\nand purchase intention?  \\nCONCEPTUAL FRAMEWORK \\nConsumer experience is in the center of our framework. According to Lemon and Verhoef (2016), \\ncustomer experience is defined by their responses during their shopping journey and these \\nresponses consist of multiple constructs like customers’ cognitive, behavioral, emotional. in our \\nresearch the main focus is on behavioral construct of consumers’ experience.   The following \\nconcepts are our main framework aspects for adopting AR technology buy business owners in their \\nmarketing strategies. 1) immersion defined by how an AR technology can reflect the realism with \\nthe use of different sensory feelings. 2)consumer experience as we defined before, and our main \\nfocus is on their behavioral responses. 3) experiential context defined as one of the constructs \\nwhich identifies the positive points of adopting AR technologies for enhancing consumers’ \\nexperience. 4) effectiveness of experiential AR application features including informative, \\npersonalizing, and interactivity features along the customers’ experience and purchase stage of \\ncustomers’ journey.  \\nStimulus-Organism-Response Paradigm \\n2023 AMA Winter Academic Conference\\n805\\n \\n4 \\nBy using the stimulus-organism-response S-O-R paradigm, retailers are able to understand the \\nneeded stimuli and their effects on consumers’ behavior. This paradigm states that stimuli activate \\nthe cognitive processing of individuals and result in consumer responses like accepting or rejecting \\nthem (Mehrabian and Russell, 1974). One of the cases where this paradigm can be adopted is with \\nAR mobile applications investigating app features and consumer behavior. There are studies that \\nexamined different important mobile AR app characteristics that have consequences on users’ \\nreactions like purchase intention (Daassi and Debbabi, 2021). Our study can add value to the \\nliterature of mobile AR application concepts by understanding how AR application features are \\nexperienced and perceived by consumers. The features that are going to be studied in this research \\ninclude information, personalized and interactivity features. \\nDifferent consumers’ experiences have been studied in previous research, like interactivity by \\nPoushneh and Vasquez-Parraga in 2017, and informativeness by Rese et al. in 2017, etc. We focus \\non Javornik\\'s perspective (2016) and consider the perceived levels of the informative, \\npersonalization, and interactivity features. For better understanding, the informative aspect of these \\napplications enhances the process of decision-making for consumers and in our research refers to \\nthe level of the product information provided by AR app which is perceived by consumers. This \\ninformation can be in the form of text, images, videos, etc. (Pantano et al., 2017).  \\nPersonalization features refer to perceived level of the information provided based on individuals’ \\ndesires and needs according to their preferences. The presence of this feature utilizes customers \\nwith saving personal time, search costs, and money. By interactivity features, we mean different \\ncontents that people can have access to, and when this interactivity is high, the quality of AR is \\nbetter, resulting in more positive responses from consumers. In this research we focus on perceived \\n2023 AMA Winter Academic Conference\\n806\\n \\n5 \\nlevel of this feature by customers using the AR app, and this feature can be reflected into the \\ntechnologically effective delivery process (Fiore et al., 2005). So, we propose the following \\nhypotheses: \\nP1: The perceived level of informative feature of an experiential AR application positively \\ninfluences customers’ experience. \\nP2: The perceived level of personalizing feature of an experiential AR application positively \\ninfluences customers’ experience. \\nP3: The perceived level of interactivity features of an experiential AR application positively \\ninfluences customers’ experience. \\nImmersion \\nImmersion is a construct defined as how much a reality is near to its simulation made by \\ntechnology using five human senses (Slater and Wilbur, 1997).  According to Wedel et al. (2020) \\nprevious scholars investigated that how stimulated environments are immerse and if they get \\nmore immerse how consumers’ behavior is going to be. Our main focus in this research is going \\nto be on how vision sensory can affect the experiential effectiveness of available AR application \\nfeatures on consumers’ experience. So, we can propose that: \\nP4: the effectiveness of experiential present AR application features on consumers’ experience is \\nmediated by immersion. \\nInformation System Success Model \\n2023 AMA Winter Academic Conference\\n807\\n \\n6 \\nInformation system success (ISS) determines users\\' behavior to understand their experience like \\nsatisfaction with using a service or product (Delone and Mclean, 2016; Schaarschmidt and Höber, \\n2017). Augmented reality is a new concept in retail and adopting the ISS model can utilize \\nresearchers to investigate this technology and understand how consumers react and respond to it \\nin a shopping context. The current study provides more information and specifics about the \\nacceptance and use of augmented reality (AR) technology for home furniture in retail and online \\nplatforms. Previous research has shown that if the quality of this system is high, then customers \\nare more willing to continue using this technology (Kim and Hwang, 2012; Van Pinxteren et al., \\n2019). Services adopting AR features are expected to enhance consumers’ experience because they \\nprovide customized services for consumers, and as this customization increases, high levels of \\ncustomer satisfaction will be reached (Kim and Hwang, 2012; Murali et al., 2016). Therefore, \\ncustomer satisfaction can be enhanced in online contexts using AR technology (Quadri-Felitti & \\nFiore, 2013; Tom Dieck, Jung, Kim, & Moon, 2017). According to Srivastava and Kaul (2014), \\nconsumer experiences are the main factor for defining customer satisfaction. The amount of joy \\nand pleasure experienced by consuming a product is reflected in the satisfaction term. It is defined \\nas fulfilling desires and leads to a positive service or product experience (Chung et al., 2018). By \\ndoing this research, more value will be added to the consumer behavior literature. The level of \\nsatisfaction gained from using AR sheds light on the use of AR apps in the online furniture \\nindustry. Using this technology affects consumers’ intention to purchase and customer returns \\n(Keiningham et al., 2017; Poushneh and Vasquez-Parraga, 2017). So, we can propose the \\nfollowing hypothesis.  \\nP5: consumers’ experience using online shopping applications using AR technology positively \\ninfluences consumers satisfaction during their purchase stage. \\n2023 AMA Winter Academic Conference\\n808\\n \\n7 \\nP6: consumers’ experience using online shopping applications using AR technology positively \\ninfluences purchase intentions during their purchase stage. \\nThe proposed model is shown in figure 1.  \\n \\n \\n \\n \\n \\nFigure 1. Conceptual Model \\nDATA \\nThe perceived level of experiential AR application features are subjective constructs and can be \\nmeasured by effective delivery process techniques. Perceived informative features can be \\nmeasured by Rese et al. (2017) methods. By adopting Kim and Baek (2018) methods, we can \\nmeasure perceived levels of personalization of AR apps, and Pantano et al. (2017) proposed a \\nmethod for measuring perceived interactivity feature and can be used and adopted for our research.  \\nThe technical features and characteristics of AR applications like video inputs can be studied by \\nsubjective data to measure the immersion. Assessing the quality of these inputs can be measured \\nby Akhtar and Falk (2017) method, and how they proposed the results can reflect the immersion \\nof AR technology. In conclusion, for measuring the effectiveness of experiential AR application, \\nimmersion, consumers’ experience, and consumers satisfaction, we can use primary data by asking \\nconsumers experiencing AR technology within questioners and qualitative interviews. \\nPerceived level of experiential \\nAR application features: \\n \\n•\\nPerceived level of \\ninformative feature \\n•\\nPerceived level of \\npersonalizing feature \\n•\\nPerceived level of \\ninteractivity feature \\nCustomers’ \\nexperience \\nPurchase stage: \\n•\\nCustomer satisfaction \\n•\\nPurchase intention \\nImmersion \\n(P1), (P2), (P3) \\n(P4) \\n(P5), (P6) \\n2023 AMA Winter Academic Conference\\n809\\n \\n8 \\nThe experience scale developed by Brakus et al. (2009) can be adopted to assess behavioral aspect \\nof consumers’ experience. Self-report data from questionnaires are useful for capturing \\nconsumers’ behavior. For the satisfaction, customer satisfaction measurement is a standard \\npractice in marketing discipline, and it has been defined by the results of comparing perceived \\nperformance delivery with the customers’ expectation. This confirmation or disconfirmation \\ncreates user satisfaction (Lemon and Verhoef, 2016). we are going to adopt Reichheld (2003) \\nmethod of using Net Promoter Score (NPS) to gather our data for customer satisfaction.  \\nFor measuring purchase intention, at first, we can measure the number of sales in our study design \\nwhich is done by our participants, furthermore for our final model, we can use secondary data from \\ncompanies with AR technology for their online shopping applications, and asking them about the \\npurchasing information of their consumers using their AR technology and information about the \\nupdates of their applications and specific changes in their sales according to these updates. This \\ndata set may be longitude for examining consumers’ purchase intention. \\n \\nCONCLUSION \\nIn this article, we proposed that when online retailers adopting AR technology in their applications \\nfor customers’ convenience, different levels of perceived experiential AR application features \\naffect their experience. Informative, personalizing, and interactivity are three features that our \\nstudy is going to investigate for their effects on users’ experience. In addition, we proposed the \\nmediating role of immersion between this perceived experiential and the customers’ experience. \\nWe argued that the purchase stage behavior of a customer, like his or her satisfaction and purchase \\nbehavior, is influenced by his or her experience of using the retailers’ AR technology. \\n2023 AMA Winter Academic Conference\\n810\\n \\n9 \\n \\nREFRENCES \\n \\nAkhtar, Zahid, and Tiago H. Falk. \"Audio-visual multimedia quality assessment: A comprehensive \\nsurvey.\" IEEE access 5 (2017): 21090-21117. \\n \\nBrakus, J. Joško, Bernd H. Schmitt, and Lia Zarantonello. \"Brand experience: what is it? How is \\nit measured? Does it affect loyalty?.\" Journal of marketing 73, no. 3 (2009): 52-68. \\n \\nDaassi, Mohamed, and Sana Debbabi. \"Intention to reuse AR-based apps: The combined role of \\nthe sense of immersion, product presence and perceived realism.\" Information & Management 58, \\nno. 4 (2021): 103453. \\n \\nDeLone, \\nWilliam \\nH., \\nand \\nEphraim \\nR. \\nMcLean. \\n\"Information \\nsystems \\nsuccess \\nmeasurement.\" Foundations and Trends® in Information Systems 2, no. 1 (2016): 1-116. \\n \\nElkins, Gary E., Peter N. Schmalzer, Travis Thompson, and Amy Simpson. Long-term pavement \\nperformance information management system: Pavement performance database user reference \\nguide. No. FHWA-RD-03-088. Turner-Fairbank Highway Research Center, 2003. \\n \\nFiore, Ann Marie, Jihyun Kim, and Hyun-Hwa Lee. \"Effect of image interactivity technology on \\nconsumer responses toward the online retailer.\" Journal of Interactive Marketing 19, no. 3 (2005): \\n38-53. \\n \\nJavornik, Ana. \"Augmented reality: Research agenda for studying the impact of its media \\ncharacteristics on consumer behaviour.\" Journal of Retailing and Consumer Services 30 (2016): \\n252-261. \\n \\n2023 AMA Winter Academic Conference\\n811\\n \\n10 \\nKeiningham, Timothy, Joan Ball, Sabine Benoit, Helen L. Bruce, Alexander Buoye, Julija \\nDzenkovska, Linda Nasr, Yi-Chun Ou, and Mohamed Zaki. \"The interplay of customer experience \\nand commitment.\" Journal of Services Marketing31, no. 2 (2017): 148-160. \\n \\nKim, Dan J., and Yujong Hwang. \"A study of mobile internet user’s service quality perceptions \\nfrom a user’s utilitarian and hedonic value tendency perspectives.\" Information Systems \\nFrontiers 14, no. 2 (2012): 409-421. \\n \\nKim, Seeun, and Tae Hyun Baek. \"Examining the antecedents and consequences of mobile app \\nengagement.\" Telematics and Informatics 35, no. 1 (2018): 148-158. \\n \\nLemon, Katherine N., and Peter C. Verhoef. \"Understanding customer experience throughout the \\ncustomer journey.\" Journal of marketing 80, no. 6 (2016): 69-96. \\n \\nMehrabian, A., and J. A. Russell. \"An Approach to Environmental Psychology.,(MIT Press: \\nCambridge, MA.).\" (1974). \\n \\nMurali, S., S. Pugazhendhi, and C. Muralidharan. \"Modelling and investigating the relationship of \\nafter sales service quality with customer satisfaction, retention and loyalty–a case study of home \\nappliances business.\" Journal of retailing and consumer services 30 (2016): 67-83. \\n \\nPantano, Eleonora, Alexandra Rese, and Daniel Baier. \"Enhancing the online decision-making \\nprocess by using augmented reality: A two country comparison of youth markets.\" Journal of \\nRetailing and Consumer Services 38 (2017): 81-95. \\n \\nPoushneh, Atieh, and Arturo Z. Vasquez-Parraga. \"Discernible impact of augmented reality on \\nretail customer\\'s experience, satisfaction and willingness to buy.\" Journal of Retailing and \\nConsumer Services 34 (2017): 229-234. \\n \\n2023 AMA Winter Academic Conference\\n812\\n \\n11 \\nPreacher, Kristopher J., and Andrew F. Hayes. \"SPSS and SAS procedures for estimating indirect \\neffects in simple mediation models.\" Behavior research methods, instruments, & computers 36, \\nno. 4 (2004): 717-731. \\n \\nQuadri-Felitti, Donna L., and Ann Marie Fiore. \"Destination loyalty: Effects of wine tourists’ \\nexperiences, memories, and satisfaction on intentions.\" Tourism and Hospitality Research13, no. \\n1 (2013): 47-62. \\n \\nReichheld, Frederick F. \"The one number you need to grow.\" Harvard business review 81, no. 12 \\n(2003): 46-55. \\n \\nRese, Alexandra, Daniel Baier, Andreas Geyer-Schulz, and Stefanie Schreiber. \"How augmented \\nreality apps are accepted by consumers: A comparative analysis using scales and \\nopinions.\" Technological Forecasting and Social Change124 (2017): 306-319. \\n \\nSanchez-Vives, Maria V., and Mel Slater. \"From presence to consciousness through virtual \\nreality.\" Nature Reviews Neuroscience 6, no. 4 (2005): 332-339. \\n \\nSchaarschmidt, Mario, and Björn Höber. \"Digital booking services: comparing online with phone \\nreservation services.\" Journal of Services Marketing (2017). \\n \\nSchuemie, Martijn J., Peter Van Der Straaten, Merel Krijn, and Charles APG Van Der Mast. \\n\"Research on presence in virtual reality: A survey.\" CyberPsychology & Behavior 4, no. 2 (2001): \\n183-201. \\n \\nSlater, Mel, and Sylvia Wilbur. \"A framework for immersive virtual environments (FIVE): \\nSpeculations on the role of presence in virtual environments.\" Presence: Teleoperators & Virtual \\nEnvironments 6, no. 6 (1997): 603-616. \\n \\n2023 AMA Winter Academic Conference\\n813\\n \\n12 \\nSrivastava, Mala, and Dimple Kaul. \"Social interaction, convenience and customer satisfaction: \\nThe mediating effect of customer experience.\" Journal of retailing and consumer services 21, no. \\n6 (2014): 1028-1037. \\n \\nTom Dieck, M. Claudia, Timothy Hyungsoo Jung, Woo Gon Kim, and Yunji Moon. \"Hotel guests’ \\nsocial media acceptance in luxury hotels.\" International Journal of Contemporary Hospitality \\nManagement (2017). \\n \\nVan Pinxteren, Michelle ME, Ruud WH Wetzels, Jessica Rüger, Mark Pluymaekers, and Martin \\nWetzels. \"Trust in humanoid robots: implications for services marketing.\" Journal of Services \\nMarketing (2019). \\n \\nWedel, Michel, Enrique Bigné, and Jie Zhang. \"Virtual and augmented reality: Advancing research \\nin consumer marketing.\" International Journal of Research in Marketing 37, no. 3 (2020): 443-\\n465. \\n \\nZhao, Xinshu, John G. Lynch Jr, and Qimei Chen. \"Reconsidering Baron and Kenny: Myths and \\ntruths about mediation analysis.\" Journal of consumer research 37, no. 2 (2010): 197-206. \\n2023 AMA Winter Academic Conference\\n814\\n'},\n",
       " {'abstract': 'This paper explores how research on Optimal Transport (OT) can be combined with Multi-Agent Reinforcement Learning (MARL). OT can help to distribute resources, align agent policies, and adjust to non-stationarity in MARL environments, especially where there are multiple agents. Using OT, MARL systems can optimize the distribution of resources, coordinate agent policies, and adapt to changing environments. The combined approach can potentially improve system efʻciency, coordination, adaptability, and scalability in MARL.',\n",
       "  'introduction': \"Multi-Agent Reinforcement Learning (MARL) is a framework where various agents learn and adapt within shared environments. These interactions involve diverse objectives, making coordination, resource management, adaptability, and operational efʻciency challenging. Optimal Transport (OT) offers a mathematical framework for comparing and transforming probability distributions effectively by finding the most cost-effective transport plan to move mass from one distribution to another. By merging OT with MARL, we can leverage OT's strengths to address the challenges in MARL, such as policy alignment, distributed resource management, non-stationarity, scalability, and energy efʻciency.\",\n",
       "  'literature review': 'Previous research on policy alignment in MARL has explored strategies that utilize shared reward structures, joint action learning, and communication protocols. For distributed resource management, approaches such as auction-based mechanisms and cooperative strategies have been proposed. To address non-stationarity, algorithms that adjust learning rates based on Wasserstein distance have been investigated. Scalability has been tackled with decentralized learning, hierarchical structures, and networked or graph-based approaches. Additionally, techniques to optimize computational and operational aspects of learning processes and energy-efʻcient algorithms have been developed to improve MARL systems.',\n",
       "  'methodology': 'To address policy alignment in MARL through OT, we can utilize the Wasserstein distance to measure and minimize the divergence between agent strategies. For distributed resource management, OT can optimize resource allocation by minimizing the transportation cost between supply and demand distributions. Non-stationarity can be modeled as a transportation problem using OT, where an optimal plan aims to minimize the cost of adapting to environmental changes. Scalability can be achieved through decentralized and hierarchical computation approaches. Finally, energy efʻciency can be enhanced by incorporating energy consumption as a weight into the OT framework.',\n",
       "  'results': 'Integrating OT with MARL can potentially lead to more efʻcient multi-agent systems. OT can optimize policy alignment, distribute resources effectively, manage non-stationarity, handle large-scale systems, and improve energy efʻciency. These enhancements can lead to more coherent and effective cooperative learning, improved coordination and adaptability, and reduced computational and energy costs.',\n",
       "  'conclusion': 'Combining OT principles with MARL can improve system efʻciency, coordination, adaptability, and scalability. Challenges, such as computational complexity and scalability, require further research and development to address them. Future work can focus on reʻning the computational efʻciency of OT in large-scale MARL systems, exploring real-world applications, and adopting the synergy between OT and MARL to address broader problems.',\n",
       "  'title': 'The Synergy Between Optimal Transport Theory and Multi-Agent Reinforcement Learning',\n",
       "  'author': 'Ali Baheri, and Mykel J. Kochenderfer',\n",
       "  'textdata': 'arXiv:2401.10949v1  [cs.MA]  18 Jan 2024\\nThe Synergy Between Optimal Transport Theory\\nand Multi-Agent Reinforcement Learning\\nAli Baheri1 and Mykel J. Kochenderfer2\\nAbstract— This paper explores the integration of optimal\\ntransport (OT) theory with multi-agent reinforcement learning\\n(MARL). This integration uses OT to handle distributions and\\ntransportation problems to enhance the efﬁciency, coordination,\\nand adaptability of MARL. There are ﬁve key areas where\\nOT can impact MARL: (1) policy alignment, where OT’s\\nWasserstein metric is used to align divergent agent strategies\\ntowards uniﬁed goals; (2) distributed resource management,\\nemploying OT to optimize resource allocation among agents;\\n(3) addressing non-stationarity, using OT to adapt to dynamic\\nenvironmental shifts; (4) scalable multi-agent learning, har-\\nnessing OT for decomposing large-scale learning objectives\\ninto manageable tasks; and (5) enhancing energy efﬁciency,\\napplying OT principles to develop sustainable MARL systems.\\nThis paper articulates how the synergy between OT and MARL\\ncan address scalability issues, optimize resource distribution,\\nalign agent policies in cooperative environments, and ensure\\nadaptability in dynamically changing conditions.\\nI. INTRODUCTION\\nMulti-agent reinforcement learning (MARL) is a frame-\\nwork where multiple agents interact, learn, and adapt within\\nshared environments [1]. These interactions, governed by\\ndiverse objectives and constraints, present signiﬁcant chal-\\nlenges related to coordination, resource management, adapt-\\nability, and operational efﬁciency [2].\\nOptimal transport (OT) theory offers a powerful mathemat-\\nical framework for comparing and transforming probability\\ndistributions in a cost-effective manner [3]. It is concerned\\nwith ﬁnding the most efﬁcient plan to transport mass from\\none distribution to another, minimizing a given cost function.\\nOT has found applications across diverse ﬁelds due to its\\nability to provide geometrically and statistically meaningful\\nways to compare distributions [4]. In this context, the fusion\\nof OT with MARL represents a promising interdisciplinary\\nstrategy, designed to harness the strengths of OT in tackling\\nthe multifaceted challenges inherent in MARL.\\nPolicy Alignment in MARL with OT. We explore how\\nOT, particularly through its Wasserstein distance metric, can\\nbe used to minimize the divergence in strategies among\\nagents. This approach promises a more coherent and effective\\ncollaborative learning process, ensuring agents’ policies are\\naligned towards common goals.\\nDistributed Resource Management in MARL with OT.\\nIn scenarios where agents must efﬁciently share limited re-\\nsources like energy or information, OT provides a principled\\nframework to optimize this distribution. The paper delves\\n1Ali Baheri is with the Department of Mechanical engineering at\\nRochester Institute of Technology. akbeme@rit.edu\\n2Mykel J. Kochenderfer is with the Department of Aeronautics &\\nAstronautics at Stanford University. mykel@stanford.edu\\ninto how OT can minimize the costs associated with resource\\nallocation, balancing efﬁciency and equity among agents.\\nAddressing Non-Stationarity in MARL with OT. The\\ndynamic nature of MARL environments, where the ground\\ntruth shifts as agents learn and adapt, presents a signiﬁcant\\nchallenge. OT’s ability to adapt to changes in probability\\ndistributions is investigated as a method to manage this non-\\nstationarity, enhancing capacity of the agents to respond to\\nevolving environments.\\nScalable Learning in Large-Scale MARL with OT. As the\\nscale of MARL systems expands, so does the complexity\\nof managing interactions and learning. We propose using\\nOT to decompose global learning objectives into localized\\ntasks, facilitating scalable and efﬁcient learning across large\\nnetworks of agents.\\nEnhancing Energy Efﬁciency in MARL with OT. In\\nenergy-constrained MARL scenarios, optimizing energy us-\\nage is crucial. This paper discusses how OT can be used to\\ndevelop energy-efﬁcient MARL systems.\\nContributions. This paper explores how the fusion of OT\\nand MARL can lead to more efﬁcient multi-agent systems.\\nThe structure of the paper is as follows: Section II provides\\nan overview of OT. Section III discusses how to integrate OT\\nprinciples into MARL. Section IV examines the challenges\\nof this integration, addressing potential computational issues.\\nII. OPTIMAL TRANSPORT\\nOT theory, rooted in the works of Gaspard Monge and\\nlater Leonid Kantorovich, revolves around ﬁnding the most\\ncost-effective way of transporting mass from one distribution\\nto another [3], [5]. Central to OT is the concept of the\\nWasserstein distance, also known as the Earth Mover’s\\ndistance, which quantiﬁes the “effort” required to transform\\none probability distribution into another. Originating from\\nMonge’s problem in the 18th century, which sought an\\noptimal plan for moving soil with minimum effort, the theory\\nhas evolved, especially with Kantorovich’s formulation in the\\n20th century, to incorporate modern mathematical tools from\\nlinear programming and functional analysis [6].\\nOT has found applications in diverse ﬁelds. In economics,\\nit aids in understanding resource allocation and market\\ndynamics [7], [8]; in computer vision and graphics [9], [10],\\nit helps in image retrieval and texture mixing [11], [12]; and\\nin machine learning [13], [14], [15], [16], [17], it is used for\\ndomain adaptation and generative modeling.\\nOT theory is concerned with the problem of transporting\\nmass from one distribution to another in the most efﬁcient\\nmanner. Formally, given two probability distributions µ and\\nν on spaces X and Y , respectively, OT seeks a transport map\\nT : X → Y that minimizes the cost of transportation. A key\\nconcept in OT is the Wasserstein distance, which is deﬁned\\nas the minimum cost to transport mass from one distribution\\nto another. Mathematically, for distributions µ and ν, the p-\\nWasserstein distance is given by:\\nWp(µ, ν) =\\n\\x12\\ninf\\nγ∈Γ(µ,ν)\\nZ\\nX×Y\\nd(x, y)pdγ(x, y)\\n\\x131/p\\n(1)\\nwhere Γ(µ, ν) represents all joint distributions (couplings)\\nwith marginals µ and ν, and d(x, y) is a distance metric on\\nX × Y .\\nIII. INTEGRATION OF OT WITH MARL\\nA. OT and Policy Alignment\\nCooperative learning in MARL requires aligning the poli-\\ncies and objectives of individual agents towards a collective\\ngoal. This section shows how OT theory in can amplify the\\nefﬁcacy of cooperative learning among agents in MARL\\nenvironments. In cooperative MARL, a group of agents\\n{a1, a2, . . . , an} aims to learn policies {π1, π2, . . . , πn} that\\nmaximize a shared reward function. The effectiveness of\\ncooperation hinges on the alignment of these policies to-\\nwards the collective goal. Each agent’s policy πi induces a\\nprobability distribution µi over the state-action space S × A.\\nThe alignment of these distributions is crucial for effective\\ncooperation. The p-Wasserstein distance could be used to\\nmeasure the discrepancy between these distributions. For\\ntwo agents ai and aj, the distance between their induced\\ndistributions µi and µj is given by:\\nWp (µi, µj) =\\n\\x12\\ninf\\nγ∈Γ(µi,µj)\\nZ\\nS×A\\n∥s − s′∥p dγ (s, s′)\\n\\x131/p\\n(2)\\nwhere Γ (µi, µj) represents the set of all joint distributions\\nwith marginals µi and µj, and ∥s− s′∥ is a metric on the\\nstate-action space. The goal is to adjust individual policies to\\nminimize the overall Wasserstein distance between all pairs\\nof agents’ policy-induced distributions. The optimization\\nproblem is:\\nmin\\nN\\nX\\ni=1\\nN\\nX\\nj=1,j̸=i\\nWp (µi, µj)\\n(3)\\nThis minimization ensures that the policies of different\\nagents are coherently aligned towards the collective objec-\\ntive. Implementing OT for policy alignment in MARL is\\nconceptually promising, but choosing an appropriate metric\\nfor the state-action space is crucial and can be complex.\\nThe crux of this integration lies in harnessing OT’s capac-\\nity for efﬁciently transporting and transforming distributions\\nto align policy distributions within MARL environments.\\nEach agent’s policy can be viewed as a probability distri-\\nbution over the state-action space, reﬂecting their decision-\\nmaking process. The Wasserstein distance could provide a\\nmeans to measure the “effort” or “cost” needed to align one\\nagent’s policy distribution with another. By minimizing this\\ndistance, we can effectively guide the agents towards policy\\nalignment.\\nRelated Work. Various approaches have been proposed\\nto facilitate policy alignment. These include shared reward\\nstructures, where agents are incentivized to work towards\\ncommon goals, and joint action learning, where agents learn\\npolicies based on the combined actions of all agents in the\\nsystem [18]. Communication protocols have been explored\\nas a means to align policies [19]. By allowing agents to\\nshare information about their states, actions, or intended\\ngoals, these protocols can signiﬁcantly improve coordination\\nand policy alignment. A notable approach in recent years is\\ncentralized training with decentralized execution [20], [21].\\nHere, agents are trained in a centralized manner, allow-\\ning them to learn about each other’s policies, but execute\\ntheir learned policies independently. Policy alignment in\\nMARL has found applications in various domains such as\\nautonomous vehicle coordination and robotics [22].\\nB. OT and Distributed Resource Management\\nIn leveraging OT for distributed resource management in\\nMARL, we focus on the transportation of resources as a\\nprobabilistic measure. This approach aligns with the core\\nprinciples of OT, where the Wasserstein distance is used\\nto quantify the efﬁciency of transporting one distribution to\\nanother.\\nIn a MARL system with resources R = {r1, r2, . . . , rm}\\nand agents A = {a1, a2, . . . , an}, consider each resource rj\\nas a mass that needs to be distributed among agents. Each\\nagent ai has a requirement or demand distribution µi, and\\nthe total available resources form a supply distribution ν.\\nThe transportation of resources from the total supply ν to\\nmeet the demands µi of each agent can be modeled using\\nthe Wasserstein distance. The goal is to ﬁnd an OT plan that\\nminimizes the Wasserstein distance, hence the transportation\\ncost, between the supply and demand distributions. The\\nmathematical formulation becomes:\\nWp (ν, µi) =\\n\\x12\\ninf\\nT ∈Γ(ν,µi)\\nZ\\nR×A\\n∥r − a∥pdT (r, a)\\n\\x131/p\\n(4)\\nHere, Γ (ν, µi) represents the set of all possible transport\\nplans (joint distributions) with marginals ν and µi, and\\n∥r − a∥ is a metric representing the cost of transporting\\nresources. The OT problem in this context aims to minimize\\nthe total transportation cost across all agents, ensuring that\\nresources are allocated efﬁciently and in accordance with\\nagents’ demands:\\nmin\\nn\\nX\\ni=1\\nWp (ν, µi)\\n(5)\\nThis formulation respects the constraints of resource avail-\\nability and agent requirements, optimizing the overall distri-\\nbution of resources in the MARL system.\\nThe point of this integration lies in using OT’s mathe-\\nmatical framework to coordinate the allocation of resources\\namong agents. By deﬁning a tailored cost function within\\nthe OT model, we can effectively capture and minimize the\\ncosts associated with resource distribution, such as energy\\nexpenditure, transit time, or spatial constraints. This approach\\ncould augment the overall efﬁciency of resource allocation\\nand introduces a level of fairness and adaptability that is\\noften unattainable with traditional MARL algorithms. The\\ndynamic nature of MARL systems, characterized by ﬂuc-\\ntuating resource demands and environmental states, further\\nhighlights the suitability of OT. Its inherent ﬂexibility to\\nrecalibrate transport plans in response to changing conditions\\nensures that the resource distribution remains optimal over\\ntime.\\nRelated Work. Several works have proposed various strate-\\ngies for resource allocation in MARL. These include auction-\\nbased mechanisms, where agents bid for resources [23],\\n[24], and cooperative strategies, where agents share resources\\nbased on collective goals [25]. There is an open discussion in\\ncurrent research regarding the balance between decentralized\\nversus centralized approaches for resource allocation [26].\\nRecent studies have explored the integration of communi-\\ncation protocols with resource management in MARL. Dis-\\ntributed resource management in MARL has been applied in\\nvarious domains, such as networked robotics, trafﬁc control,\\nand energy grids [27], [28].\\nC. OT and Non-Stationarity in MARL\\nNon-stationarity, characterized by the changing dynamics\\nin a multi-agent environment, poses a signiﬁcant challenge in\\nMARL. We propose OT to address non-stationarity in MARL\\nsystems. In MARL, agents continually adapt their policies\\nbased on the evolving environment, leading to non-stationary\\ndynamics. This non-stationarity complicates learning because\\nthe ground truth each agent tries to learn keeps shifting. OT\\ncould offer a powerful framework to adapt to changes in\\nprobability distributions, which, in the context of MARL,\\ncorrespond to the evolving strategies and states of agents. We\\ndiscuss how the Wasserstein metric provides a natural way\\nto quantify the shifts in distributions representing policies\\nor environment states over time. OT can be used to model\\nthe non-stationarity in MARL as a transportation problem,\\nwhere the goal is to ﬁnd an optimal plan that minimizes the\\ncost of adapting to the evolving environment. We propose\\nto explore algorithms that use the Wasserstein distance to\\nquantify the rate of change in the environment and adjust\\nthe learning strategies. Using OT, the learning rate of agents\\ncan be dynamically adjusted, becoming more responsive\\nduring periods of rapid environmental change and more\\nstable during periods of relative constancy.\\nConsider MARL with N agents, where each agent i has\\na policy πi that generates a distribution over states µi at any\\ngiven time t. The state space is denoted by S. For agent i, the\\nshift in its state distribution between two consecutive time\\nsteps t and t + 1 can be quantiﬁed using the p-Wasserstein\\ndistance:\\nWp\\nalignment with the global objective. Furthermore, inter-layer\\ncoordination between different layers could be managed\\nthrough an OT framework, which optimizes the ﬂow of\\npolicies and rewards up and down the hierarchy. This would\\nensure that local optimizations at lower layers contribute\\neffectively to the global objectives at higher layers.\\nRelated Work. To address scalability, many studies have\\nfocused on decentralized learning approaches [31], [32].\\nThese allow agents to make decisions based on local in-\\nformation, reducing the computational burden. Hierarchical\\nlearning structures are another solution for scalability. By\\norganizing agents into hierarchies or modules, the complexity\\ncan be handled more effectively. Recent advancements have\\nexplored the use of networked and graph-based approaches\\nfor scalable MARL [33], [34]. Transfer learning and multi-\\ntask learning techniques have been proposed to enhance scal-\\nability [35], [36], [37], [38]. Scalable MARL has applications\\nin large-scale systems like trafﬁc management [39], [40],\\nsmart grids [41], [42], and distributed control systems [43].\\nE. OT and Energy Efﬁciency\\nMARL systems, particularly in large-scale and complex\\nenvironments, face the challenge of high energy consumption\\ndue to the computational demands of continuous learning and\\ndecision-making processes. We can develop variants of the\\nWasserstein metric that incorporates energy consumption as\\na weight. This metric quantiﬁes not just the efﬁciency of\\ntask distribution among agents but also the energy cost asso-\\nciated with each task. By minimizing this energy-weighted\\nWasserstein distance, the MARL system can dynamically\\nallocate tasks in a way that optimizes for both performance\\nand energy efﬁciency.\\nWe could also design algorithms that continually adapt\\nthe “transport paths” of computational tasks and information\\nﬂow among agents. These paths are optimized based on real-\\ntime energy consumption data, dynamically rerouting tasks to\\nagents with lower energy constraints or to times of day when\\nenergy is more abundantly available (e.g., off-peak hours in\\ngrid-connected systems). For environments with strict energy\\nlimitations, such as remote sensors or space exploration\\nrobots, the algorithms prioritize essential tasks and dynami-\\ncally allocate resources to maximize operational time while\\nmaintaining essential functions. In systems equipped with\\nenergy-harvesting capabilities, OT algorithms can be used\\nto optimally allocate and store harvested energy, prioritizing\\ntasks based on their urgency and energy requirements.\\nRelated Work. Studies have focused on optimizing the\\ncomputational and operational aspects of learning processes,\\nreducing the energy consumption of agents during training\\nand execution [44]. Research in MARL under resource\\nconstraints include energy-limited settings such as wireless\\nsensor networks or mobile robotics [45]. Papers in this do-\\nmain explore strategies for agents to maximize their perfor-\\nmance while minimizing energy usage, often involving trade-\\noffs between task completion and power consumption [46].\\nSeveral studies have incorporated energy metrics into the\\nreward function of MARL algorithms [47], [48]. Some have\\nstudies collaborative strategies in MARL, where agents work\\ntogether to achieve energy efﬁciency [49]. This includes\\ncooperative approaches for sharing resources like battery\\npower or computational capacity to extend the operational\\nlife of the system [50]. Application-speciﬁc research in\\ndomains like autonomous vehicle ﬂeets [51], smart grids\\n[52], and IoT systems [53] has highlighted the importance\\nof energy-efﬁcient MARL. In these applications, the goal is\\noften to optimize system-wide energy usage while ensuring\\nthe effective performance of each agent.\\nIV. CHALLENGES\\nThere are several challenges in integrating OT with\\nMARL. A primary obstacle is the computational complex-\\nity inherent in OT, particularly when applied to the high-\\ndimensional and dynamic spaces characteristic of MARL.\\nThe computational burden of calculating the Wasserstein\\nmetric scales signiﬁcantly with the size of state and action\\nspaces, posing challenges in real-time applications [54]. Fur-\\nthermore, the dynamic nature of MARL systems necessitates\\ncontinuous recalibration of OT calculations, exacerbating\\ncomputational demands. To address these computational\\nchallenges, one potential solution is the development of ap-\\nproximation algorithms for the Wasserstein distance. Lever-\\naging techniques such as entropic regularization can pro-\\nvide a more computationally tractable approximation of the\\nWasserstein metric [55]. Additionally, employing machine\\nlearning models, tailored to approximate OT calculations, can\\naccelerate the process, making it more feasible for dynamic\\nMARL environments [15].\\nAnother critical challenge is scalability, particularly in\\nhandling large numbers of agents. The complexity of com-\\nputing OT metrics across large networks of agents can offset\\nthe beneﬁts of optimal distribution and task allocation. Here,\\ndecentralized and hierarchical approaches to OT computation\\ncould present a viable solution [56]. By decomposing the\\nglobal OT problem into smaller, localized sub-problems,\\nand solving them within agent clusters, the system can\\nachieve scalability while maintaining the integrity of the OT\\nframework.\\nV. CONCLUSIONS\\nThis paper explored integrating OT with MARL, address-\\ning some of the critical challenges in the ﬁeld. We delved into\\nhow OT’s strengths in handling distributions and minimizing\\ntransportation costs can enhance MARL in areas such as\\npolicy alignment, resource management, adaptability to non-\\nstationary environments, scalability, and energy efﬁciency.\\nThis integration, while showcasing promising potential, also\\nhighlights areas for further research and development. Future\\nwork can focus on reﬁning the computational efﬁciency of\\nimplementing OT in large-scale MARL systems and explor-\\ning real-world applications in greater depth. By continuing\\nto explore this integration, we aim to make MARL systems\\nmore adaptable, efﬁcient, and capable of handling complex\\nreal-world tasks.\\nREFERENCES\\n[1] S. V. Albrecht, F. Christianos, and L. Sch¨afer, Multi-Agent Reinforce-\\nment Learning: Foundations and Modern Approaches.\\nMIT Press,\\n2024.\\n[2] A. Wong, T. B¨ack, A. V. Kononova, and A. Plaat, “Deep multiagent\\nreinforcement learning: Challenges and directions,” Artiﬁcial Intelli-\\ngence Review, vol. 56, no. 6, pp. 5023–5056, 2023.\\n[3] C. Villani et al., Optimal transport: old and new.\\nSpringer, 2009.\\n[4] G. Peyr´e, M. Cuturi et al., “Computational optimal transport: With\\napplications to data science,” Foundations and Trends® in Machine\\nLearning, vol. 11, no. 5-6, pp. 355–607, 2019.\\n[5] L. V. Kantorovich, “On the translocation of masses,” Journal of\\nMathematical Sciences, vol. 133, no. 4, pp. 1381–1382, 2006.\\n[6] S. T. Rachev and L. R¨uschendorf, Mass Transportation Problems:\\nVolume I: Theory.\\nSpringer Science & Business Media, 1998, vol. 1.\\n[7] A. Galichon, “The unreasonable effectiveness of optimal transport in\\neconomics,” HAL, Tech. Rep., 2021.\\n[8] ——, “A survey of some recent applications of optimal transport\\nmethods to econometrics,” The Econometrics Journal, vol. 20, no. 2,\\npp. C1–C11, 2017.\\n[9] N. Bonneel and J. Digne, “A survey of optimal transport for computer\\ngraphics and computer vision,” in Computer Graphics Forum, vol. 42,\\nno. 2, 2023, pp. 439–460.\\n[10] C. Wen, “Conformal geometry and optimal transport for computer\\nvision and geometric modeling,” Ph.D. dissertation, State University\\nof New York at Stony Brook, 2020.\\n[11] J. Delon, A. Desolneux, L. Facq, and A. Leclaire, “Optimal transport\\nbetween gmm for multiscale texture synthesis,” in International Con-\\nference on Scale Space and Variational Methods in Computer Vision.\\nSpringer, 2023, pp. 627–638.\\n[12] J. Rabin, G. Peyr´e, J. Delon, and M. Bernot, “Wasserstein barycenter\\nand its application to texture mixing,” in Third International Confer-\\nence on Scale Space and Variational Methods in Computer Vision.\\nSpringer, 2012, pp. 435–446.\\n[13] Y. Balaji, R. Chellappa, and S. Feizi, “Robust optimal transport with\\napplications in generative modeling and domain adaptation,” Advances\\nin Neural Information Processing Systems, vol. 33, pp. 12 934–12 944,\\n2020.\\n[14] R. Flamary, N. Courty, D. Tuia, and A. Rakotomamonjy, “Optimal\\ntransport for domain adaptation,” IEEE Trans. Pattern Anal. Mach.\\nIntell, vol. 1, no. 1-40, p. 2, 2016.\\n[15] E. F. Montesuma, F. N. Mboula, and A. Souloumiac, “Recent ad-\\nvances in optimal transport for machine learning,” arXiv preprint\\narXiv:2306.16156, 2023.\\n[16] I. Redko, N. Courty, R. Flamary, and D. Tuia, “Optimal transport\\nfor multi-source domain adaptation under target shift,” in The 22nd\\nInternational Conference on Artiﬁcial Intelligence and Statistics, 2019,\\npp. 849–858.\\n[17] A. Baheri, “Risk-aware reinforcement learning through optimal trans-\\nport theory,” arXiv preprint arXiv:2309.06239, 2023.\\n[18] S. Chakraborty, A. S. Bedi, A. Koppel, D. Manocha, H. Wang,\\nF. Huang, and M. Wang, “Aligning agent policy with externalities:\\nReward design via bilevel RL,” arXiv preprint arXiv:2308.02585,\\n2023.\\n[19] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson, “Learning to\\ncommunicate with deep multi-agent reinforcement learning,” Advances\\nin Neural Information Processing Systems, vol. 29, 2016.\\n[20] Y. Hong, Y. Jin, and Y. Tang, “Rethinking individual global max in\\ncooperative multi-agent reinforcement learning,” Advances in Neural\\nInformation Processing Systems, vol. 35, pp. 32 438–32 449, 2022.\\n[21] T. Ikeda and T. Shibuya, “Centralized training with decentralized\\nexecution reinforcement learning for cooperative multi-agent systems\\nwith communication delay,” in 61st Annual Conference of the Society\\nof Instrument and Control Engineers (SICE), 2022, pp. 135–140.\\n[22] L. M. Schmidt, J. Brosig, A. Plinge, B. M. Eskoﬁer, and C. Mutschler,\\n“An introduction to multi-agent reinforcement learning and review of\\nits application to autonomous mobility,” in IEEE 25th International\\nConference on Intelligent Transportation Systems (ITSC), 2022, pp.\\n1342–1349.\\n[23] H. Talebiyan and L. Due˜nas-Osorio, “Auctions for resource allocation\\nand decentralized restoration of interdependent networks,” Reliability\\nEngineering & System Safety, vol. 237, p. 109301, 2023.\\n[24] M. Braquet and E. Bakolas, “Greedy decentralized auction-based\\ntask allocation for multi-agent systems,” IFAC-PapersOnLine, vol. 54,\\nno. 20, pp. 675–680, 2021.\\n[25] F. Li, Z. Xu, and H. Li, “A multi-agent based cooperative approach\\nto decentralized multi-project scheduling and resource allocation,”\\nComputers & Industrial Engineering, vol. 151, p. 106961, 2021.\\n[26] C. Lu, Q. Bao, S. Xia, and C. Qu, “Centralized reinforcement learning\\nfor multi-agent cooperative environments,” Evolutionary Intelligence,\\npp. 1–7, 2022.\\n[27] R. Bokade, X. Jin, and C. Amato, “Multi-agent reinforcement learning\\nbased on representational communication for large-scale trafﬁc signal\\ncontrol,” IEEE Access, vol. 11, pp. 47 646–47 658, 2023.\\n[28] J. Gielis, A. Shankar, and A. Prorok, “A critical review of commu-\\nnications in multi-robot systems,” Current Robotics Reports, vol. 3,\\nno. 4, pp. 213–225, 2022.\\n[29] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, “Is multiagent deep\\nreinforcement learning the answer or the question? a brief survey,”\\nLearning, vol. 21, p. 22, 2018.\\n[30] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mor-\\ndatch, “Multi-agent actor-critic for mixed cooperative-competitive\\nenvironments,” Advances in Neural Information Processing Systems,\\nvol. 30, 2017.\\n[31] H. Kao, C.-Y. Wei, and V. Subramanian, “Decentralized cooperative\\nreinforcement learning with hierarchical information structure,” in\\nInternational Conference on Algorithmic Learning Theory, 2022, pp.\\n573–605.\\n[32] Y. Li, X. Wang, J. Wang, W. Wang, X. Luo, and S. Xie, “Cooperative\\nmulti-agent reinforcement learning with hierarchical relation graph\\nunder partial observability,” in IEEE 32nd International Conference\\non Tools with Artiﬁcial Intelligence (ICTAI), 2020, pp. 1–8.\\n[33] Y. Du, C. Ma, Y. Liu, R. Lin, H. Dong, J. Wang, and Y. Yang, “Scal-\\nable model-based policy optimization for decentralized networked\\nsystems,” in IEEE/RSJ International Conference on Intelligent Robots\\nand Systems (IROS), 2022, pp. 9019–9026.\\n[34] H. Gu, X. Guo, X. Wei, and R. Xu, “Mean-ﬁeld multi-agent reinforce-\\nment learning: A decentralized network approach,” Decision-Making\\nin Operations Research eJournal, 2021.\\n[35] Y. Mai, Y. Zang, Q. Yin, W. Ni, and K. Huang, “Deep multi-task\\nmulti-agent reinforcement learning with knowledge transfer,” IEEE\\nTransactions on Games, 2023.\\n[36] T. Wang, X. Peng, Y. Jin, and D. Xu, “Experience sharing based\\nmemetic transfer learning for multiagent reinforcement learning,”\\nMemetic Computing, vol. 14, no. 1, pp. 3–17, 2022.\\n[37] H. Shi, J. Li, J. Mao, and K.-S. Hwang, “Lateral transfer learning for\\nmultiagent reinforcement learning,” IEEE Transactions on Cybernet-\\nics, vol. 53, no. 3, pp. 1699–1711, 2023.\\n[38] W. Liang, J. Wang, W. Bao, X. Zhu, Q. Wang, and B. Han, “Con-\\ntinuous self-adaptive optimization to learn multi-task multi-agent,”\\nComplex & Intelligent Systems, vol. 8, no. 2, pp. 1355–1367, 2022.\\n[39] L. Luan, Y. Tian, W. Fang, C. Zhang, W. Xue, R. Chen, and C. Sang,\\n“MARL for trafﬁc signal control in scenarios with different intersec-\\ntion importance,” in Third International Conference on Distributed\\nArtiﬁcial Intelligence.\\nSpringer, 2022, pp. 93–106.\\n[40] T. Wang, T. Liang, J. Li, W. Zhang, Y. Zhang, and Y. Lin, “Adaptive\\ntrafﬁc signal control using distributed marl and federated learning,”\\nin IEEE International Conference on Communication Technology\\n(ICCT), 2020, pp. 1242–1248.\\n[41] C. Gavriluta, C. Boudinet, F. Kupzog, A. Gomez-Exposito, and\\nR. Caire, “Cyber-physical framework for emulating distributed control\\nsystems in smart grids,” International Journal of Electrical Power &\\nEnergy Systems, vol. 114, p. 105375, 2020.\\n[42] O. P. Mahela, M. Khosravy, N. Gupta, B. Khan, H. H. Alhelou,\\nR. Mahla, N. Patel, and P. Siano, “Comprehensive overview of multi-\\nagent systems for controlling smart grids,” CSEE Journal of Power\\nand Energy Systems, vol. 8, no. 1, pp. 115–131, 2020.\\n[43] F. Charbonnier, T. Morstyn, and M. D. McCulloch, “Scalable multi-\\nagent reinforcement learning for distributed control of residential\\nenergy ﬂexibility,” Applied Energy, vol. 314, p. 118825, 2022.\\n[44] X. Ye, Z. Deng, Y. Shi, and W. Shen, “Toward energy-efﬁcient routing\\nof multiple agvs with multi-agent reinforcement learning,” Sensors,\\nvol. 23, no. 12, p. 5615, 2023.\\n[45] M. Sahraoui, A. Bilami, and A. Taleb-Ahmed, “Schedule-based coop-\\nerative multi-agent reinforcement learning for multi-channel commu-\\nnication in wireless sensor networks,” Wireless Personal Communica-\\ntions, vol. 122, no. 4, pp. 3445–3465, 2022.\\n[46] Y. Liang, H. Wu, and H. Wang, “Asynchronous multi-agent reinforce-\\nment learning for collaborative partial charging in wireless recharge-\\nable sensor networks,” IEEE Transactions on Mobile Computing, pp.\\n1–13, 2023.\\n[47] W. Yang, C. Lin, H. Dai, P. Wang, J. Ren, L. Wang, G. Wu, and\\nQ. Zhang, “Robust wireless rechargeable sensor networks,” IEEE/ACM\\nTransactions on Networking, vol. 31, no. 3, pp. 949–964, 2023.\\n[48] Y. Qi, P. Cheng, J. Bai, J. Chen, A. Guenard, Y.-Q. Song, and\\nZ. Shi, “Energy-efﬁcient target tracking by mobile sensors with limited\\nsensing range,” IEEE Transactions on Industrial Electronics, vol. 63,\\nno. 11, pp. 6949–6961, 2016.\\n[49] S. Su, X. Wang, T. Tang, G. Wang, and Y. Cao, “Energy-efﬁcient\\noperation by cooperative control among trains: A multi-agent rein-\\nforcement learning approach,” Control Engineering Practice, vol. 116,\\np. 104901, 2021.\\n[50] Y. Xiao, Y. Song, and J. Liu, “Collaborative multi-agent deep re-\\ninforcement learning for energy-efﬁcient resource allocation in het-\\nerogeneous mobile edge computing networks,” IEEE Transactions on\\nWireless Communications, pp. 1–1, 2023.\\n[51] M. Hua, Q. Zhou, C. Zhang, H. Xu, and W. Liu, “Multi-agent deep\\nreinforcement learning for charge-sustaining control of multi-mode\\nhybrid vehicles,” arXiv preprint arXiv:2209.02633, 2022.\\n[52] Y. Zhang, Q. Yang, D. An, D. Li, and Z. Wu, “Multistep multiagent\\nreinforcement learning for optimal energy schedule strategy of charg-\\ning stations in smart grid,” IEEE Transactions on Cybernetics, vol. 53,\\nno. 7, pp. 4292–4305, 2023.\\n[53] T. Li, K. Zhu, N. C. Luong, D. Niyato, Q. Wu, Y. Zhang, and B. Chen,\\n“Applications of multi-agent reinforcement learning in future internet:\\nA comprehensive survey,” IEEE Communications Surveys & Tutorials,\\nvol. 24, no. 2, pp. 1240–1279, 2022.\\n[54] J. Fan, I. Haasler, J. Karlsson, and Y. Chen, “On the complexity of the\\noptimal transport problem with graph-structured cost,” in International\\nConference on Artiﬁcial Intelligence and Statistics, 2022, pp. 9147–\\n9165.\\n[55] C. Clason, D. A. Lorenz, H. Mahler, and B. Wirth, “Entropic reg-\\nularization of continuous optimal transport problems,” Journal of\\nMathematical Analysis and Applications, vol. 494, no. 1, p. 124432,\\n2021.\\n[56] J. Lee, M. Dabagia, E. Dyer, and C. Rozell, “Hierarchical optimal\\ntransport for multimodal distribution alignment,” Advances in Neural\\nInformation Processing Systems, vol. 32, 2019.\\n'},\n",
       " {'abstract': 'Emotion recognition is important for human-robot interaction, but current methods often ignore the context of emotions. We introduce SCAM, a self context-aware emotion perception model that employs a two-dimensional emotion coordinate system for anchoring and re-labeling distinct emotions. It incorporates a distinctive information retention structure and contextual loss, resulting in significant improvements across audio, video, and multimodal modalities.',\n",
       "  'introduction': \"Human-robot interaction requires natural and intuitive communication, which includes the robot's ability to understand human emotions. Current emotion recognition models primarily focus on multimodal perception, but often overlook contextual information. This is problematic because emotions in real-life conversations often display a sense of continuity, meaning emotions do not undergo abrupt and dramatic shifts within a brief timeframe. As a result, when emotions cannot be ascertained, humans often depend on contextual information to make judgments. Hence, we propose self context-aware model (SCAM) that enables a robot to perform emotion recognition on the user while simultaneously considering the user’s preceding emotional context and integrating it with the robot’s recognition results from the preceding context. This enhances the overall accuracy and comprehensiveness of the robot’s emotion recognition abilities during human-robot interactions.\",\n",
       "  'literature review': 'Emotion recognition is a complex process involving various perceptual dimensions and temporal aspects. Researchers have explored different modalities, such as visual, auditory, physical, and even EEG and skin conductance signals, for emotion recognition. Combining information from diverse modalities typically yields enhanced accuracy. Some studies emphasize the intricacy of emotion recognition when information conflicts arise between modalities. Additionally, researchers suggest that better results can be achieved by capturing contextual information within conversations. However, considering that during human-robot interaction, the robot may struggle to provide sufficient feedback, we propose a method to utilize self context.',\n",
       "  'methodology': 'SCAM consists of two main components: a multi-task network for each segment and a self context-aware structure for composition. The multi-task network relabels emotions and utilizes ResNet101 and Bi-LSTM to recognize emotion, valence, and arousal within a short time. The self context-aware structure incorporates contextual information propagation and context loss, combining the context information and predictions from preceding segments with the current input to predict the current emotion, valence, and arousal.',\n",
       "  'results': 'In the auditory modality, there has been a notable enhancement in accuracy, rising from 63.10% to 72.46%. Similarly, the visual modality has demonstrated improved accuracy, increasing from 77.03% to 80.82%. In the multimodal, accuracy has experienced an elevation from 77.48% to 78.93%.',\n",
       "  'conclusion': 'Our approach achieves significant improvements across all modalities, some of which have reached the state of the art. In future work, we will validate the reliability and usability of SCAM on robots through psychology experiments.',\n",
       "  'title': 'Self context-aware emotion perception on human-robot interaction',\n",
       "  'author': 'Zihan Lin, Francisco Cruz, Eduardo Benitez Sandoval',\n",
       "  'textdata': 'Cite as: Zihan Lin, Francisco Cruz, and Eduardo Benitez Sandoval. Self context-aware emotion perception on human-robot\\ninteraction. In Proceedings of the Australasian Conference on Robotics and Automation (ACRA). 2023.\\nSelf context-aware emotion perception on human-robot interaction\\nZihan Lin1, Francisco Cruz1,2, Eduardo Benitez Sandoval3\\n1School of Computer Science and Engineering, University of New South Wales, Sydney, Australia\\n2Escuela de Ingenier´ıa, Universidad Central de Chile, Santiago, Chile\\n3School of Art and Design, Creative Robotics Lab, University of New South Wales, Sydney, Australia\\nEmails: zihan.lin3@student.unsw.edu.au, f.cruz@unsw.edu.au, e.sandoval@unsw.edu.au\\nAbstract\\nEmotion recognition plays a crucial role in\\nvarious domains of human-robot interaction.\\nIn long-term interactions with humans, robots\\nneed to respond continuously and accurately,\\nhowever, the mainstream emotion recognition\\nmethods mostly focus on short-term emotion\\nrecognition, disregarding the context in which\\nemotions are perceived. Humans consider that\\ncontextual information and different contexts\\ncan lead to completely different emotional ex-\\npressions.\\nIn this paper, we introduce self\\ncontext-aware model (SCAM) that employs a\\ntwo-dimensional emotion coordinate system for\\nanchoring and re-labeling distinct emotions. Si-\\nmultaneously, it incorporates its distinctive in-\\nformation retention structure and contextual\\nloss.\\nThis approach has yielded significant\\nimprovements across audio, video, and multi-\\nmodal.\\nIn the auditory modality, there has\\nbeen a notable enhancement in accuracy, rising\\nfrom 63.10% to 72.46%. Similarly, the visual\\nmodality has demonstrated improved accuracy,\\nincreasing from 77.03% to 80.82%. In the mul-\\ntimodal, accuracy has experienced an elevation\\nfrom 77.48% to 78.93%. In the future, we will\\nvalidate the reliability and usability of SCAM\\non robots through psychology experiments.\\n1\\nIntroduction\\nHuman-robot interaction has become increasingly im-\\nportant due to the widespread use of robots in various\\napplications such as manufacturing, healthcare, and per-\\nsonal assistance [Kyrarini et al., 2021]. Human-robot in-\\nteraction focuses on how humans and robots can safely\\nand effectively collaborate, requiring natural and intu-\\nitive communication between them. To achieve better\\ncommunication, it is crucial for robots to understand hu-\\nman emotions; otherwise, they may respond incorrectly,\\nleading humans to reject interacting with the robots\\n[Tsiourti et al., 2019]. However, emotion recognition is\\na complex process, involving various perceptual dimen-\\nsions and temporal aspects. Current emotion recognition\\nmodels primarily focus on multimodal perception but of-\\nten overlook contextual information [Poria et al., 2019].\\nIn real-life conversational contexts, emotional fluctua-\\ntions in individuals often display a sense of continuity.\\nThis implies that, under typical circumstances, emotions\\ndo not undergo abrupt and dramatic shifts within a\\nbrief timeframe, such as sudden transitions from intense\\nanger to extreme happiness. Consequently, when emo-\\ntions cannot be ascertained, humans frequently depend\\non contextual information to make judgments [Sacharin\\net al., 2012]. Practically, there is often a requirement to\\nrely exclusively on preceding contextual information.\\nTherefore, this paper introduces self context-aware\\nmodel (SCAM), enabling a robot to perform emotion\\nrecognition on the user while simultaneously considering\\nthe user’s preceding emotional context and integrating it\\nwith the robot’s recognition results from the preceding\\ncontext. This approach allows for a more comprehen-\\nsive and accurate assessment of the user’s emotion state\\nduring human-robot interactions.\\nThe contributions of this work are summarized as fol-\\nlows:\\n• We utilize the relationship between valence, arousal,\\nand emotion to enable the model to learn basic emo-\\ntions from non-basic ones.\\n• We introduce a novel contextual loss, incorporating\\nthe model’s predictions of context emotion, valence,\\nand arousal, allowing the model to more effectively\\ncapture emotional change trends.\\n• We model the information transfer within the con-\\ntext, preserving valuable features from the preced-\\ning context for integration and judgment when mak-\\ning predictions for the subsequent context.\\n• We conduct experiments on the IEMOCAP dataset,\\nencompassing speech modality, visual modality, and\\narXiv:2401.10946v1  [cs.HC]  18 Jan 2024\\nmultimodal scenarios.\\nThe experimental results\\ndemonstrate that our approach achieves significant\\nimprovements across all modalities, some of which\\nhave reached the state of the art.\\n2\\nRelated work\\nThe application of emotion recognition spans a wide\\nrange of domains, including its deployment in various\\nhuman-computer interaction scenarios and chatbot sys-\\ntems designed to generate emotionally rich dialogues\\n[Zhou et al., 2018].\\nNevertheless, this field is fraught\\nwith numerous challenges. For instance, individuals ex-\\nperiencing mental distress may be reluctant to unveil\\ntheir vulnerabilities, often concealing their true emo-\\ntional states [Maithri et al., 2022].\\nConsequently, re-\\nsearchers have delved extensively into the realm of emo-\\ntion recognition, exploring various modalities such as vi-\\nsual, auditory, physical, and even the incorporation of\\nEEG and skin conductance signals [Li et al., 2022]. Typi-\\ncally, amalgamating information from diverse modalities\\nyields enhanced accuracy in emotion recognition [Wu et\\nal., 2022]. Notably, the fusion of speech and text modali-\\nties achieved a remarkable accuracy rate of 80.51% (four\\ncategories) on the IEMOCAP dataset [Atmaja et al.,\\n2022].\\nHowever, some studies [Tsiourti et al., 2019] under-\\nscore the intricacy of emotion recognition when infor-\\nmation conflicts arise between modalities.\\nFurther-\\nmore, some researchers suggested that better results\\ncan be achieved by capturing contextual information\\nwithin conversations.[Priyasad et al., 2020] employed\\ngraph neural networks to model inter-dialogue relation-\\nships, achieving commendable performance.\\nThey in-\\ntroduced an iterative emotion interaction network that\\nemploys iteratively predicted emotion labels to explicitly\\nmodel emotion interactions, culminating in an accuracy\\nof 64.37% (seven categories) on the IEMOCAP dataset.\\nCompared to providing a dialogue-based approach, we\\ntake into consideration that during human-robot inter-\\naction, the robot may struggle to provide sufficient feed-\\nback.\\nTherefore, we propose a method to utilize self\\ncontext.\\nAnother pivotal dimension of research in emotion\\nrecognition is the exploration of dimensional emotion\\nmodels. In contrast to discrete emotion models, valence-\\narousal model [Russell, 1980] offers a better understand-\\ning of the intricate relationships among different emo-\\ntional states.\\nResearch demonstrated that combining\\nvalence, arousal, dominance, and the polarity of emo-\\ntions within a multi-view training framework can yield\\nsuperior results [Tompkins et al., 2023]. However, the\\ncurrent use of dimensional models has not effectively\\nleveraged the continuity of dimensional models in emo-\\ntional expression, failing to fully exploit their advan-\\ntages. In comparison to discrete emotion models, dimen-\\nsional models can better observe the trend of emotional\\nchanges. In our approach, we make the first attempt to\\nutilize this aspect and have achieved excellent results.\\n3\\nMethods\\nDuring the process of human-robot interaction, long-\\nlasting sessions can be divided into multiple segments,\\nand they may vary with their own context and the\\nrobot’s responses.\\nThe emotions in each segment are\\nrelatively independent, with a correlation observed be-\\ntween adjacent segments due to the continuous nature\\nof human emotions. Therefore, we group some adjacent\\nsegments into a composition (Figure 1). By using SCAM\\nto capture this correlation, we achieve better emotion\\nperception results.\\nFigure 1: Context interaction in HRI\\nSCAM consists of two main components: a multi-task\\nnetwork for each segment and a self context-aware struc-\\nture for composition. The multi-task network relabels\\nemotions and then utilizes ResNet101 and Bi-LSTM to\\nrecognize emotion, valence, and arousal within a short\\ntime. The self context-aware structure incorporates con-\\ntextual information propagation and context loss, com-\\nbining the context information and predictions from pre-\\nceding segments with the current input to predict the\\ncurrent emotion, valence, and arousal. In the following\\nsections, we will provide further details on these compo-\\nnents.\\n3.1\\nDataset\\nConsidering there is currently no publicly available and\\nsuitable human-robot interaction dataset, we conduct\\nmodel training and validation on IEMOCAP dataset\\n[Busso et al., 2008], which comprises 10,039 instances\\nperformed by 10 actors. These actors are paired, and\\neach pair engages in multiple scripted and spontaneous\\nemotional dialogues, which are appropriate for the simu-\\nlation of human-robot interaction. Throughout these di-\\nalogues, they portray 10 predefined emotional states: an-\\ngry, sad, happy, neutral, disgust, surprise, fear, excited,\\nother, and unmarked.\\nEach emotional state includes\\nmultiple sentences and encompasses various modalities,\\nincluding audio, video, text, and more.\\nEmotion\\nNumber of Samples\\nRate (%)\\nAnger\\n1103\\n24.57\\nHappy\\n595\\n13.25\\nNeutral\\n1708\\n38.04\\nSadness\\n1084\\n24.14\\nTotal\\n4490\\n100.00\\nTable 1: IEMOCAP: Four Emotions\\n• Frame Segmentation: Divide the audio signal into\\nsmall segments,\\n• Windowing: Apply a window function to each frame\\nto reduce the impact of spectral leakage.\\n• Fourier Transform:\\nApply the Discrete Fourier\\nTransform (DFT) to each window, transforming the\\ntime-domain signal into a frequency-domain signal.\\n• Magnitude and Squaring: Compute the amplitude\\nspectrum for each frequency component, often by\\ntaking the magnitude of the complex values.\\n• Visualization: Display the obtained spectral infor-\\nmation as an image, with the horizontal axis rep-\\nresenting time, the vertical axis representing fre-\\nquency, and color or brightness representing ampli-\\ntude.\\nFour emotions (angry, happy, neutral, and sad) and\\ntwo modalities (auditory and visual) in IEMOCAP are\\nused to verify our approach in order to compare with\\nother methods, as shown in Table 1. Emotions are anno-\\ntated using two mainstream approaches. One approach\\nconsiders emotions as fixed labels, where each emotion\\nis treated as a category such as angry, happy, sad, or\\nneutral. Each sample is assigned only one emotion la-\\nbel, representing a specific emotion category. The other\\napproach [Russell, 1980] involves emotion dimensions,\\nwhere each sample has two labels annotated with valence\\nand arousal. Valence represents one dimension of emo-\\ntion (e.g., emotional intensity), and arousal represents\\nanother dimension (e.g., positivity and negativity). This\\napproach better captures complex emotional changes, as\\nemotions can be regarded as points in an emotion space\\nrather than single categories.\\nWe calculate the mean of valence and arousal as coor-\\ndinates for each emotion in the emotion space, as shown\\nin Figure 2.\\nWithin the same composition, emotions\\ntend to undergo a transition and be closer to the emotion\\nof the current segment (the last segment in the compo-\\nsition). Capturing such contextual emotion changes can\\nimprove the continuity and accuracy of emotion percep-\\ntion.\\nFrame Segmentation:\\nDivide the audio signal into\\nsmall segments, Windowing: Apply a window function\\nto each frame to reduce the impact of spectral leakage.\\nFourier Transform: Apply the Discrete Fourier Trans-\\nform (DFT) to each window, transforming the time-\\ndomain signal into a frequency-domain signal. Magni-\\ntude and Squaring: Compute the amplitude spectrum\\nfor each frequency component, often by taking the mag-\\nnitude of the complex values.\\nVisualization: Display\\nthe obtained spectral information as an image, with the\\nhorizontal axis representing time, the vertical axis rep-\\nresenting frequency, and color or brightness representing\\namplitude.\\nFigure 2: IEMOCAP emotions on Valence-Arousal axis\\n3.2\\nMulti-task network\\nPreprocessing\\nFor different modalities of input, we employ distinct pre-\\nprocessing methods. For the auditory modality, we con-\\nvert speech into log-Mel spectrograms as input. Com-\\npared to one-dimensional audio signals, log-Mel spectro-\\ngrams better represent crucial features in speech signals,\\nsuch as formants and harmonic structures, enhancing the\\naccuracy of emotion perception. The log-Mel spectro-\\ngrams are generated with a sampling rate of 22050Hz\\nand utilize 256 Mel filters.\\nAn example of a log-Mel\\nspectrogram generated from a single segment is shown\\nin Figure 3.\\nFor the visual modality, considering the short dura-\\ntion of each segment and the limited facial expression\\nchanges, three frames are recorded. We extract the start,\\nintermediate, and end frames from the corresponding\\nvideo segments and crop the facial regions (Figure 4).\\nThe intermediate frame is calculated using the start and\\nend times of each segment.\\nFigure 3: Log-Mel spectrogram of one segment\\nFigure 4: Cropped frames of one segment\\nSegment structure\\nResNet101 is utilized for feature extraction. After the\\nResNet101, high-level features are fed into a Bi-LSTM to\\ncapture temporal information. For the auditory modal-\\nity, high-level features are extracted after Conv4 x. For\\nthe visual modality, the three frames are separately fed\\ninto ResNet101, and after Conv5 x, they are concate-\\nnated and passed to the Bi-LSTM. In the case of multi-\\nmodal data, feature alignment and concatenation occur\\nbefore input to the Bi-LSTM. The Bi-LSTM’s output\\nis then split into three fully connected neural network\\n(FCN), responsible for emotion classification, valence re-\\ngression, and arousal regression, as shown in Figure 5.\\nFigure 5: Segment structure (multimodal)\\nSegment Relabeling\\nIn the context of our contextual emotion model, each\\ncomposition comprises multiple segments, and our objec-\\ntive is to predict the emotion of the last segment in each\\ncomposition (the emotion at the current time). Given\\nthat emotions tend to exhibit relatively small variations\\nwithin a composition, we select the emotion of the last\\nsegment as the emotion label for the entire composition,\\nwhile valence and arousal remain unchanged, as shown in\\nTable 2. This approach offers two advantages. Firstly, it\\neffectively leverages data from emotion labels other than\\nthe four basic emotions, enriching the dataset. Secondly,\\none segment’s emotion may lead to different current emo-\\ntions in different compositions. In the following section,\\nwe will explain how we utilize this feature.\\nSegment\\nEmotion\\nValence\\nArousal\\nRelabel\\n11\\nAngry\\n1.5\\n4\\nAngry\\n12\\nFrustrated\\n1.5\\n4\\nAngry\\n13\\nAngry\\n1.5\\n4.5\\nAngry\\nTable 2: Relabel of composition\\nLoss in segment\\nDue to the relabeling of emotions for segments within\\neach composition, in most cases, relabeled emotions\\nclosely match or are similar to the original emotions,\\nowing to the continuity of emotions. However, there are\\ninstances where inconsistencies in emotions arise.\\nAs\\nmentioned before, we compute the average valence and\\narousal for different emotions, serving as reference points\\non the two-dimensional emotion coordinate system. Si-\\nmultaneously, we retain the original valence and arousal\\nlabels for each segment. Therefore, we utilize the Eu-\\nclidean distance to measure the distance between the re-\\nlabeled emotions and the original emotions and scale the\\nemotion loss accordingly, as indicated by the following\\nformula:\\nLemo = −R · 1\\nN\\nN\\nX\\ni=1\\n4\\nX\\nj=1\\nyij log(p(yij)),\\nwhich N represents the number of samples, yij denotes\\nthe actual emotion labels, and p(yij) represents the pre-\\ndicted probabilities of emotions by the model.\\nR is defined as:\\nR =\\n1\\np\\n(xemo − xseg)2 + (yemo − yseg)2 .\\nxemo and yemo represent the valence and arousal of the\\nrelabel emotion (e.g., xemo and yemo of anger), and xseg\\nand yseg represent the label of valence and arousal of the\\nsegment.\\nFor valence and arousal regression use mean squared\\nerror, the formulas are as follows,\\nLval = 1\\nN\\nN\\nX\\ni=1\\n(xseg − ˆxseg)2,\\nLaro = 1\\nN\\nN\\nX\\ni=1\\n(yseg − ˆyseg)2,\\nwhich ˆxseg and ˆyseg represent the prediction of valence\\nand arousal seperately.\\n3.3\\nSelf context-aware structure\\nSelf context-aware structure is the core optimization\\ncomponent of our model, primarily consisting of two\\nparts: contextual information propagation and context\\nloss.\\nThrough these components, we achieve the per-\\nception of current emotion with the aid of contextual\\ninformation. The procedure is shown in Figure 6.\\nFigure 6: Context-aware structure\\nContextual Information Propagation\\nAssume that the features processed by ResNet101 are\\ndenoted as {x(1)\\n1 , x(1)\\n2 , . . .}, where x(1)\\ni\\nrepresents the\\nfeatures of the i-th part in the first segment, and\\n{x(2)\\n1 , x(2)\\n2 , . . .} represents the features of the second seg-\\nment, and so on.\\nThe output of the Bi-LSTM can be represented as\\n{h(1)\\n1 , h(1)\\n2 , . . .}, where h(1)\\ni\\ndenotes the output of the Bi-\\nLSTM at the i-th time step in the first segment, and\\n{h(2)\\n1 , h(2)\\n2 , . . .} represents the output of the Bi-LSTM\\nin the second segment, and so forth.\\nSince the Bi-\\nLSTM is bidirectional, each h(j)\\ni\\ncontains both forward\\nand backward propagation information and is typically\\nrepresented as h(j)\\ni\\n= [\\n−−→\\nh(j)\\ni ,\\n←−−\\nh(j)\\ni ], where\\n−−→\\nh(j)\\ni\\nrepresents\\nthe forward propagation output, and\\n←−−\\nh(j)\\ni\\nrepresents the\\nbackward propagation output.\\nNext, for each segment, we extend the output of the\\nlast time step to have the same dimensions as the input\\nfeatures for the LSTM time steps.\\nThis extension is\\nrepresented as:\\nh(j)\\next = U · [\\n−−→\\nh(j)\\n−1,\\n←−−\\nh(j)\\n−1],\\nwhich h(j)\\next represents the extended output, and U de-\\nnotes an upsampling layer.\\n−−→\\nh(j)\\n−1 represents the output of\\nthe last time step in the forward propagation for the j-th\\nsegment, and\\n←−−\\nh(j)\\n−1 represents the output of the last time\\nstep in the backward propagation for the j-th segment.\\nFinally, we concatenate the extended output with all\\nother features in the next segment, which is represented\\nas:\\nX(j+1) = [h(j)\\next, x(j+1)\\n1\\n, x(j+1)\\n2\\n, . . .],\\nwhich X(j+1) represents the input for the Bi-LSTM in\\nthe next segment, h(j)\\next is the extended output from the\\ncurrent segment, and x(j+1)\\n1\\n, x(j+1)\\n2\\n, . . . are all the input\\nfeatures in the next segment.\\nIn this way, the model can utilize high-dimensional\\nemotional information from the previous segment to bet-\\nter understand the emotions in the current segment.\\nContext Loss\\nThe context loss is employed to capture emotional vari-\\nations between adjacent segments.\\nWe represent the\\nvalence and arousal of each segment within a composi-\\ntion on a two-dimensional emotion coordinate system,\\nas illustrated in Figure 2.\\nThe vectors formed be-\\ntween consecutive segments depict the trends in emo-\\ntional changes. We measure the distance between the\\npredicted vectors and actual vectors using cosine simi-\\nlarity, thus forming the context loss, as shown below:\\nLcontext =\\nvij\\npre · vij\\nlabel\\n∥vij\\npre∥ · ∥vij\\nlabel∥\\n, where j − i = 1.\\ni represents the ith segment, j represents the jth seg-\\nment, and j − i = 1 denotes adjacent segments.\\nThe following notations are used: (xi\\npre, yi\\npre) denotes\\nthe predicted coordinates of the valence and arousal for\\nthe ith segment. (xj\\npre, yj\\npre) denotes the predicted coor-\\ndinates of the valence and arousal for the jth segment.\\n(xi\\nlabel, yi\\nlabel) denotes the true coordinates of the valence\\nand arousal for the ith segment. (xj\\nlabel, yj\\nlabel) denotes\\nthe true coordinates of the valence and arousal for the\\njth segment.\\nSince j − i = 1, we can form two vectors : vij\\npre =\\n(xj\\npre−xi\\npre, yj\\npre−yi\\npre), representing the predicted emo-\\ntion change from i to j. vij\\nlabel = (xj\\nlabel − xi\\nlabel, yj\\nlabel −\\nyi\\nlabel), representing the labeled emotion change from i\\nto j. As shown in Figure 7, in which orange points rep-\\nFigure 7: Context loss in valence-arousal axis\\nresent label valence and arousal and the grey points rep-\\nresent predict valence and arousal, SCAM can learn the\\ntrend through context loss.\\nIf the cosine similarity is\\nclose to 1, it indicates that vij\\npre and vij\\nlabel have simi-\\nlar directions, which means that the model’s predictions\\nare in line with the true labels. If the cosine similarity is\\nclose to -1, it indicates that vij\\npre and vij\\nlabel have opposite\\ndirections, which means that there is a large deviation\\nbetween the predictions and the true labels, indicating\\nthat the model does not capture the trend in the context\\neffectively.\\n4\\nExperiments\\n4.1\\nResults\\nDue to the random selection of the test dataset (10% of\\nIEMOCAP of four emotions), some segments have dis-\\ncontinuous context. In such cases, we replicate the cur-\\nrent segment as a substitute for the missing context. We\\ncompare the results of the auditory modality (A-SCAM),\\nvisual modality (V-SCAM), and multimodal (M-SCAM)\\nwith the following baselines, as shown in Table 3.\\nIn the auditory modality, SCAM achieves an accuracy\\nof 72.46%, in the visual modality, it reaches the highest\\naccuracy of 80.82%, and in the multimodal, it achieves\\nan accuracy of 78.93%.\\nThough the multimodal per-\\nFigure 8: Confusion matrix of A-SCAM\\nFigure 9: Confusion matrix of V-SCAM\\nformance is slightly inferior, SCAM achieves results su-\\nperior to the baseline in both the auditory and visual\\nmodalities.\\nThe corresponding confusion matrices for\\ndifferent modalities are shown in Figure 8, Figure 9, and\\nFigure 10. In general, the visual modality performs bet-\\nter overall than the auditory modality, and the accuracy\\nin recognizing the happy emotion is significantly higher\\nin the visual modality.\\nFigure 10: Confusion matrix of M-SCAM\\n4.2\\nAnalysis\\nEfficiency of context loss\\nWe also evaluate the results when the loss is minimized\\non the test set, as shown in Table 4.\\nIn our experiments, a difference in the trends of accu-\\nracy and loss is observed. The lowest loss occurrs signif-\\nicantly earlier than the highest accuracy. In the case of\\na single segment, this difference is primarily influenced\\nby the multitask loss. Although valence and arousal are\\nsimultaneously predicted, the primary task remains emo-\\ntion classification. Therefore, loss and accuracy may not\\nbe entirely correlated. For SCAM, the loss composition\\nbecomes more complex. It includes context loss due to\\nthe relabeling process and the necessity to predict the\\nemotion of preceding segments in order to predict the\\ncurrent emotion.\\nThis context loss may not necessar-\\nily reflect the current emotion and may even conflict\\nwith the current emotion, valence, and arousal.\\nTak-\\ning the auditory modality as an example, even though\\nthe total loss (Figure 11) on the test set fluctuates and\\neven increases, the context loss (Figure 12) consistently\\ndecreases. This indicates that the model is effectively\\nlearning the contextual relationships of emotions, result-\\ning in improved emotion classification results, as shown\\nin Figure 13\\nAblation Study\\nIn the case of using only a single segment, where emo-\\ntional loss is not scaled and there is no self-context-aware\\nFigure 11: Test loss of auditory modality\\nFigure 12: Test loss context of auditory modality\\nFigure 13: Test accuracy of emotion of auditory modality\\nModel\\nTrain/Predict\\nModality\\nEmotion (UA)\\nValence\\nArousal\\nPrior Work\\nAudio-CNN-xvector[Peng et al., 2021]\\nEmo\\nA\\n68.40\\n-\\n-\\nSVM[Tompkins et al., 2023]\\nV/A, Emo\\nA\\n68.23\\n-\\n-\\nw2v2-b[Tompkins et al., 2023]\\nV/A/D, Emo\\nA\\n61.20\\n47.80\\n60.50\\nMMAN[Pan et al., 2020]\\nEmo\\nA+V+T\\n73.94\\n-\\n-\\nAV-ITN[Fu et al., 2022]\\nEmo\\nA+V\\n81.66\\n-\\n-\\nProposed Work\\nA-SCAM\\nV/A, Emo\\nA\\n72.46\\n53.18\\n67.78\\nV-SCAM\\nV/A, Emo\\nV\\n80.82\\n60.09\\n59.09\\nM-SCAM\\nV/A, Emo\\nA+V\\n78.93\\n59.87\\n68.78\\nTable 3: Performance comparison of prior work and SCAM\\nModel\\nEmotion (UA)\\nValence\\nArousal\\nA-SCAM\\n60.98\\n49.50\\n64.33\\nV-SCAM\\n78.48\\n61.20\\n58.19\\nM-SCAM\\n76.48\\n57.64\\n69.01\\nTable 4: Performance at lowest test loss\\nstructure, we compare the auditory modality (A-SEG),\\nvisual modality (V-SEG), and multimodal approach (M-\\nSEG) with SCAM. The results are presented in Table 5.\\nModel\\nEmotion (UA)\\nValence\\nArousal\\nA-SEG\\n63.10\\n52.29\\n66.22\\nA-SCAM\\n72.46\\n53.18\\n67.78\\nV-SEG\\n77.03\\n59.2\\n59.31\\nV-SCAM\\n80.82\\n60.09\\n59.09\\nM-SEG\\n77.48\\n56.63\\n66.89\\nM-SCAM\\n78.93\\n59.87\\n68.78\\nTable 5: Ablation experiments of SCAM\\nIt can be observed that, with the application of\\nSCAM, the performance of the auditory modality im-\\nproves by 9.36%, the visual modality improves by 3.79%,\\nand the multimodal approach improves by 1.45%. This\\nfurther demonstrates the effectiveness of SCAM across\\ndifferent modalities. Furthermore, in Figure 14, visual-\\nization of a composition highlights SCAM’s remarkable\\ncontextual awareness, independent of the consistency in\\ncontextual labels. Even when the context undergoes con-\\ntinuous changes, SCAM correctly identifies emotions.\\nFigure 14: Sample of test composition\\nMultimodal\\nIn general, multimodal performance is expected to sur-\\npass unimodal performance.\\nHowever, in our experi-\\nments, the multimodal performance is not superior to\\nthe unimodal performance. Therefore, we conduct fur-\\nther analysis of the classification results for the auditory\\nand visual modalities. In the confusion matrix in Fig-\\nure 15, the elements on the diagonal represent correct\\nclassification by A-SCAM but incorrect classification by\\nV-SCAM. The remaining entries indicate how V-SCAM\\nincorrectly classifies emotions into different categories.\\nSimilarly, in Figure 16, we represent cases where V-\\nSCAM classifies emotions correctly but A-SCAM clas-\\nsifies them incorrectly.\\nIt can be observed that the visual modality is better\\nat recognizing happy, while neutral and angry have a\\nconsiderable number of correctly classified samples in-\\ndependently by both modalities. This suggests that the\\ntwo modalities obtain different features for neutral and\\nangry emotions. In some cases, the auditory modality in-\\ncorrectly identifies angry as neutral and neutral as angry,\\nbut the visual modality correctly identifies them, and\\nvice versa. Furthermore, both modalities’ primary errors\\nare concentrated in misclassifying some other emotions\\nas neutral, and samples misclassified as neutral differ\\nsignificantly. Based on the error distribution, the errors\\nmade by both modalities are quite similar.\\nFigure 15: A-SCAM correct, but V-SCAM wrong\\nFigure 16: V-SCAM wrong, but A-SCAM correct\\n5\\nConclusion\\nIn this work, we introduce SCAM, a method that lever-\\nages the user’s emotion context and features during long-\\nterm human-robot interactions for emotion perception.\\nAdditionally, we innovatively combine continuous emo-\\ntion models with discrete emotion models, anchoring\\nthe relationships between different emotions using va-\\nlence and arousal, achieving outstanding performance.\\nThrough ablation experiments, we further demonstrate\\nthat SCAM significantly improves accuracy in emotion\\nrecognition, valence regression, and arousal regression in\\nauditory, visual, and multimodal modalities. Moreover,\\nthrough data visualization, SCAM performs effectively\\neven in scenarios with continuous changes in context.\\nIn future work, we will further collect data on robots to\\nvalidate the reliability of the methods and conduct psy-\\nchological experiments to analyze the usability of robot\\nemotion perception. Regarding the multimodal conflicts\\narising from the similarity in probability distributions\\nbetween the auditory and visual modalities, we will con-\\nsider introducing additional modal information for emo-\\ntion perception.\\nReferences\\n[Kyrarini et al., 2021] M. Kyrarini, F. Lygerakis, A. Ra-\\njavenkatanarayanan, C. Sevastopoulos, H.R. Nambi-\\nappan, K.K. Chaitanya, A.R. Babu, J. Mathew, and\\nF. Makedon. A Survey of Robots in Healthcare. Tech-\\nnologies, 9(1):8, 2021.\\n[Tsiourti et al., 2019] C. Tsiourti, A. Weiss, K. Wac,\\nand M. Vincze.\\nMultimodal integration of emo-\\ntional signals from voice, body, and context: Effects\\nof (in) congruence on emotion recognition and atti-\\ntudes towards robots. International Journal of Social\\nRobotics, 11, 555-573, 2019.\\n[Poria et al., 2019] S. Poria, N. Majumder, R. Mihalcea,\\nand E. Hovy.\\nEmotion recognition in conversation:\\nResearch challenges, datasets, and recent advances.\\nIEEE Access, 7, 100943-100953, 2019.\\n[Sacharin et al., 2012] V. Sacharin, D. Sander, and K.\\nR. Scherer. The perception of changing emotion ex-\\npressions.\\nCognition & Emotion, 26(7), 1273-1300,\\n2012.\\n[Zhou et al., 2018] H. Zhou, M. Huang, T. Zhang, X.\\nZhu, and B. Liu. Emotional chatting machine: Emo-\\ntional conversation generation with internal and exter-\\nnal memory. In Proceedings of the AAAI Conference\\non Artificial Intelligence, Vol. 32, No. 1, April 2018.\\n[Li et al., 2022] Li, X., Zhang, Y., Tiwari, P., Song, D.,\\nHu, B., Yang, M., ... and Marttinen, P. (2022). EEG\\nbased emotion recognition:\\nA tutorial and review.\\nACM Computing Surveys, 55(4), 1-57.\\n[Maithri et al., 2022] M. Maithri, U. Raghavendra, A.\\nGudigar, J. Samanth, P. D. Barua, M. Murugappan,\\nand U. R. Acharya. Automated emotion recognition:\\nCurrent trends and future perspectives.\\nComputer\\nmethods and programs in biomedicine, 215, 106646,\\n2022.\\n[Wu et al., 2022] X. Wu, W. L. Zheng, Z. Li, and B. L.\\nLu. Investigating EEG-based functional connectivity\\npatterns for multimodal emotion recognition. Journal\\nof neural engineering, 19(1), 016012, 2022.\\n[Russell, 1980] Russell, J. A. (1980).\\nA circumplex\\nmodel of affect. Journal of personality and social psy-\\nchology, 39(6), 1161.\\n[Atmaja et al., 2022] B. T. Atmaja, A. Sasou, and M.\\nAkagi. Survey on bimodal speech emotion recognition\\nfrom acoustic and linguistic information fusion. Speech\\nCommunication, 140, 11-28, 2022.\\n[Priyasad et al., 2020] D. Priyasad,\\nT. Fernando,\\nS.\\nDenman, S. Sridharan, and C. Fookes.\\nAttention\\ndriven fusion for multi-modal emotion recognition. In\\nICASSP 2020-2020 IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP),\\npp. 3227-3231, May 2020. IEEE.\\n[Tompkins et al., 2023] D. Tompkins, D. Emmanouili-\\ndou, S. Deshmukh, and B. Elizalde.\\nMulti-View\\nLearning for Speech Emotion Recognition with Cate-\\ngorical Emotion, Categorical Sentiment, and Dimen-\\nsional Scores.\\nIn ICASSP 2023-2023 IEEE Inter-\\nnational Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP), pp. 1-5, June 2023. IEEE.\\n[Busso et al., 2008] C. Busso, M. Bulut, C. C. Lee, A.\\nKazemzadeh, E. Mower, S. Kim, and S. S. Narayanan.\\nIEMOCAP: Interactive emotional dyadic motion cap-\\nture database. Language Resources and Evaluation,\\n42, 335-359, 2008.\\n[Peng et al., 2021] Peng, Z., Lu, Y., Pan, S., and Liu, Y.\\nEfficient speech emotion recognition using multi-scale\\nCNN and attention. In ICASSP 2021-2021 IEEE In-\\nternational Conference on Acoustics, Speech and Sig-\\nnal Processing (ICASSP), pp. 3020-3024, June 2021.\\nIEEE.\\n[Pan et al., 2020] Pan, Z., Luo, Z., Yang, J., and Li, H.\\nMulti-modal attention for speech emotion recognition.\\narXiv preprint arXiv:2009.04107, 2020.\\n[Fu et al., 2022] Fu, L., Zhang, Q., and Wang, R. AV-\\nITN: A Method of Multimodal Video Emotional Con-\\ntent Analysis. In 2022 IEEE Conference on Telecom-\\nmunications, Optics and Computer Science (TOCS),\\npp. 84-87, December 2022. IEEE.\\n'},\n",
       " {'abstract': \"Machine Unlearning emphasizes adaptability, personalization, privacy, and bias concerns and addresses the ability to remove or update specific data from machine learning models. Unlike traditional models, Unlearning Machine Learning (MUL) dynamically adjusts system knowledge based on shifts in user preferences and ethical considerations. The paper examines MUL's fundamentals, real-world uses, and challenges in ongoing research in responsible and user-focused artificial intelligence. It highlights the trade-off between personalization and privacy, encouraging contributions to meet practical demands for targeted data removal, and offers ways to advance the field.\",\n",
       "  'introduction': \"The transformative power of Machine learning has revolutionized data processing and analysis, influencing our digital interactions through sophisticated algorithms and data-driven insights. Recommendation systems, a critical subset of ML, shape our online experiences with tailored recommendations. This paper explores the role of machine unlearning (MUL) in recommendation systems, addressing adaptability, personalization, privacy, and bias challenges. It critically reviews MUL's fundamentals, real-world applications, and complexities like algorithmic transparency, providing insights into its potential to transform recommendations, user trust, and future research.\",\n",
       "  'literature_review': \"This section offers a thorough investigation of MUL's current state-of-the-art algorithms within recommendation systems, shedding light on several significant contributions. It presents a comprehensive overview of groundbreaking research, including the introduction of graph networks, optimization methods, and model-agnostic operators, highlighting the evolving landscape of MUL methodologies.\",\n",
       "  'methodology': 'The literature review in this paper systematically analyzed relevant sources from esteemed academic databases. By employing comprehensive search strategies across reputable platforms, a representative selection of pertinent and recent studies on MUL within recommender systems was compiled. The research adhered to a rigorous methodology to identify and evaluate the most impactful contributions in the field.',\n",
       "  'results': 'The review of SOTA algorithms for MUL on recommendation systems revealed promising advancements in unlearning efficiency, accuracy, and eﬃciency. The combination of data partitioning, adaptive aggregation methods, and advanced update techniques showcased significant improvements in model utility and completeness. Researchers also introduced model-agnostic approaches for graph unlearning, ensuring the removal of elements without compromising knowledge integrity. Despite these advancements, challenges remain in addressing instance-level data removal limitations and the complexities of non-convex optimization problems, opening avenues for future exploration.',\n",
       "  'conclusion': \"The paper emphasizes how MUL can potentially revolutionize recommendation systems by critically evaluating its fundamentals, practical applications, and the complex issue of algorithmic transparency. It explores the delicate balance between tailoring recommendations to individual preferences and respecting user privacy, highlighting the importance of building user trust in these systems. Moreover, it suggests avenues for future research that prioritize responsible and user-centric artificial intelligence, significantly contributing to ongoing discussions about ethical practices in AI. Its unique exploration of MUL's transformative capabilities within recommendation systems oﬀers valuable insights into ethical considerations and advances the conversation on responsible AI development.\",\n",
       "  'title': 'Machine Unlearning for Recommendation Systems: An Insight',\n",
       "  'author': 'Bhavika Sachdeva, Harshita Rathee,  Sristi, Arun Sharma, Witold Wydmański',\n",
       "  'textdata': 'arXiv:2401.10942v1  [cs.IR]  17 Jan 2024\\nMachine Unlearning for Recommendation Systems: An Insight\\nBhavika Sachdeva⋆1, Harshita Rathee*1, Sristi*1, Arun Sharma1, and Witold Wydma´nski2\\n1 Department of Computer Science and Engineering, Indira Gandhi Delhi Technical University for Women, New Delhi, India\\nbhavika2210@gmail.com, harshita10.17@gmail.com, sristi0108@gmail.com and arunsharma@igdtuw.ac.in\\n2 Faculty of Mathematics and Computer Science, Jagiellonian University, Krak´ow, Poland witold.wydmanski@uj.edu.pl\\nAbstract. This review explores machine unlearn-\\ning (MUL) in recommendation systems, addressing\\nadaptability, personalization, privacy, and bias chal-\\nlenges. Unlike traditional models, MUL dynamically\\nadjusts system knowledge based on shifts in user pref-\\nerences and ethical considerations. The paper criti-\\ncally examines MUL’s basics, real-world applications,\\nand challenges like algorithmic transparency. It sifts\\nthrough literature, oﬀering insights into how MUL\\ncould transform recommendations, discussing user trust,\\nand suggesting paths for future research in responsi-\\nble and user-focused artiﬁcial intelligence (AI). The\\ndocument guides researchers through challenges in-\\nvolving the trade-oﬀ between personalization and pri-\\nvacy, encouraging contributions to meet practical de-\\nmands for targeted data removal. Emphasizing MUL’s\\nrole in secure and adaptive machine learning, the pa-\\nper proposes ways to push its boundaries. The nov-\\nelty of this paper lies in its exploration of the lim-\\nitations of the methods, which highlights exciting\\nprospects for advancing the ﬁeld.\\nKeywords: Machine Unlearning · Right to be For-\\ngotten · Recommendation Unlearning · Data Privacy\\n· Machine Learning Security\\n1\\nIntroduction\\nMachine learning (ML) stands as a transformative force, rev-\\nolutionizing data processing and analysis across diverse do-\\nmains. Its pervasive impact on our approach to data is partic-\\nularly evident in recommendation systems, a critical subset of\\nML that signiﬁcantly inﬂuences our digital experiences.\\nWithin the intricate realm of recommendation systems,\\nthese algorithms are architects of personalized user interac-\\ntions. Operating on sophisticated algorithms and data-driven\\ninsights, these systems are crucial in delivering tailored content\\nand product suggestions. Through a nuanced understanding\\nof user behaviors and preferences, recommendation systems\\ncontribute to an enriched online experience by seamlessly in-\\ntegrating relevant and engaging recommendations.\\nAt the core of this intricate interplay lies the pivotal role of\\nML, serving as the backbone that propels the nuanced mechan-\\nics of recommendation systems. These systems, meticulously\\ndesigned to decipher and respond to user preferences and be-\\nhaviors, exemplify the symbiotic relationship between human\\ninteraction and algorithmic intelligence. The synergy between\\nML and recommendation systems highlights the depth of their\\nimpact on how we consume and engage with content in the\\ndigital realm.\\nAs we delve further into the intricacies of recommendation\\nsystems, it becomes apparent that machine learning’s role is\\nnot ancillary but foundational. Its integration empowers these\\nsystems to continually evolve and adapt, ﬁne-tuning their rec-\\nommendations based on the dynamic landscape of user pref-\\nerences. This symbiotic relationship underscores the dynamic\\n⋆ These authors should be considered co-ﬁrst authors.\\nnature of the intersection between machine learning and rec-\\nommendation systems, marking it as a compelling frontier in\\nthe evolving landscape of digital technology. These applica-\\ntions vary from news portals, social media [1], [2], e-commerce\\nplatforms to OTT platforms such as Netﬂix [3]. Random walk-\\nbased models, such as Pixie [4], have been successfully em-\\nployed in large-scale industrial contexts to deliver highly eﬀec-\\ntive personalized recommendations. There are several types of\\nﬁltering algorithms used like Collaborative ﬁltering, Content\\nbased ﬁltering, and matrix factorization. Many of the recom-\\nmendation systems use hybrid algorithms which combine sev-\\neral recommendation techniques into one. Once created, these\\nrecommendation systems have the capacity to retain knowl-\\nedge from their training data.\\nHowever, with the ever-growing importance and prevalence\\nof recommendation systems, a challenging concept looms in\\nthe background – the need for machine unlearning. As these\\nsystems continuously evolve and adapt, there arises a crucial\\nquestion: how do we eﬀectively mitigate or alter the impacts of\\nprior learning, both within the data repositories and the core\\nML models that power recommendation systems?\\nMUL is an emerging science that tries to address the com-\\nplexities associated with undoing or revising the consequences\\nof prior learning, both within the data repositories and the\\nunderlying machine learning models. It arises from the recog-\\nnition that, in a constantly evolving and privacy-conscious dig-\\nital landscape, users may require their data to be selectively\\nerased or modiﬁed, not only to enhance privacy and data se-\\ncurity but also to maintain the accuracy and relevance of rec-\\nommendations.\\nHowever, an essential consideration arises in this context\\n[5]: throughout the life cycle of recommendation algorithms\\nunder examination, instances arise where it becomes impera-\\ntive for the system to selectively discard speciﬁc information\\nand its complete historical context.\\nGiven that all these algorithms rely on the premise of re-\\ntaining data, a signiﬁcant and pressing concern associated with\\nthese systems is the potential compromise of user privacy. Re-\\ncent studies have brought to light that trained models, in-\\ncluding recommender systems, large pre-trained models, and\\nﬁne-tuned natural language models, can inadvertently expose\\nsensitive user information. In such cases, users are actively\\nlooking for methods to remove the impact of their sensitive\\ndata from these models.\\nAnother important rationale for the need for recommen-\\ndation unlearning pertains to the system’s utility. Over time,\\ncontinuous data collection is essential for enhancing existing\\nmodels. However, some of this newly acquired data can be\\nharmful, such as tainted data resulting from poisoning attacks\\nor data that diverges from the model’s typical distribution\\n(out-of-distribution data) [2]. This subpar data can have a\\nnotable detrimental impact on the eﬀectiveness of recommen-\\ndations. Once identiﬁed, it becomes imperative for the system\\nto expunge such data to restore its usefulness.\\n2\\nB. Sachdeva et al.\\nFurthermore, in our ever-evolving world, users increasingly\\nseek recommendations that adapt to their changing needs.\\nPicture a scenario involving political posts—a user was once\\nactively engaged in such content but has since lost interest.\\nDespite this shift, the user continues to receive recommenda-\\ntions related to political discourse. This example underscores\\nthe need for users to have control over the model’s historical\\ndata, particularly in the realm of political content, to ensure\\nthat recommendations remain relevant and responsive to their\\nevolving preferences and interests.\\nThe most basic approach to unlearning entails retraining\\nthe model using its original data, but this method presents a\\nsubstantial challenge due to the signiﬁcant computational de-\\nmands, especially for large-scale datasets. For recommendation\\nunlearning to be successful, it must not only involve purging\\ncollected user data but also erasing the inﬂuence of that data\\non the model. This process should adhere to three fundamental\\nprinciples: completeness, utility, and eﬃciency [6]. One direct\\nmethod to satisfy both completeness and utility is to retrain\\nthe model entirely from the ground up. However, this approach\\nis often impractical due to the high costs associated with re-\\ntraining, rendering it ineﬃcient for real-world applications. To\\nensure the eﬃciency of the unlearning process, existing meth-\\nods for recommendation unlearning have had to strike a bal-\\nance between the three fundamental principles, considering the\\ntrade-oﬀs between these factors and eﬃciency.\\nConcerning recommendation systems, the existing MUL\\nmethods face challenges due to the complex geometric rela-\\ntionships and structures inherent in the data they handle. In\\naddition to exploring the traditional recommendation unlearn-\\ning algorithm, this paper places emphasis on graph-based un-\\nlearning algorithms. [7]. In the quest for reﬁning recommenda-\\ntion systems, a signiﬁcant aspect that has garnered attention\\nis the challenge posed by evolving user preferences. There-\\nfore, the adaptability of recommendation systems to capture\\nand respond to these nuances becomes paramount. Addressing\\nthis challenge involves exploring novel approaches to real-time\\nlearning and dynamic model adjustments.\\nThe study’s main contributions include thoroughly exam-\\nining the concept of machine unlearning (MUL) within rec-\\nommendation systems. It oﬀers a deep dive into the adaptable\\nnature of MUL, focusing on its implications for personaliza-\\ntion, privacy, and bias – all critical aspects of today’s AI-driven\\nsystems. The research provides a nuanced perspective on how\\nMUL could potentially revolutionize recommendation systems\\nby critically evaluating its fundamentals, its practical applica-\\ntions, and the complex issue of algorithmic transparency.\\nAdditionally, the paper highlights the delicate balance be-\\ntween tailoring recommendations to individual preferences and\\nrespecting user privacy. It emphasizes the importance of build-\\ning user trust in these systems and suggests avenues for future\\nresearch that prioritize responsible and user-centric artiﬁcial\\nintelligence.\\nThrough its proposals to expand secure and adaptive ma-\\nchine learning using MUL, this study signiﬁcantly contributes\\nto ongoing discussions about ethical practices in AI. Its unique\\nexploration of MUL’s transformative capabilities within rec-\\nommendation systems oﬀers valuable insights into ethical con-\\nsiderations and advances the conversation on responsible AI\\ndevelopment.\\nThe structure of this paper is as follows: Section 2 discusses\\nthe review methodology used for the paper. Section 3 pro-\\nvides a thorough literature review of existing state-of-the-art\\n(SOTA) algorithms on machine unlearning for recommenda-\\ntion systems. Section 4 summarizes the discussions and future\\ndirections of the reviewed methodologies in the relevant ﬁeld,\\nand ﬁnally, Section 5 details the conclusion.\\n2\\nReview Methodology\\nIn conducting this literature review, a systematic approach\\nwas adopted to identify relevant sources and contributions.\\nThorough searches were conducted across esteemed academic\\ndatabases, including IEEE journals, Elsevier publications, and\\nSpringer databases. The selected keywords employed in the\\nsearch process encompassed terms such as ”machine unlearn-\\ning,” ”recommendation systems,” ”graph networks,” ”recom-\\nmendation unlearning methods,” and ”privacy in machine learn-\\ning.” A representative selection of the most pertinent and\\nrecent research in machine unlearning within recommenda-\\ntion systems was compiled by employing these comprehensive\\nsearch strategies and leveraging authoritative databases.\\n3\\nLiterature Review\\nThis section takes a deep dive into the ever-evolving realm of\\nmachine unlearning in recommendation systems, speciﬁcally\\nfocusing on the utilization of graph networks. It sheds light on\\nseveral seminal contributions in this nascent ﬁeld, each pre-\\nsenting a distinct approach to the intricate task of unlearning\\nwithin recommendation systems. These contributions oﬀer in-\\nsights into various techniques and strategies, encompassing ad-\\nvanced data partitioning and adaptive aggregation methods,\\nnovel recommendation unlearning methodologies, and model-\\nagnostic layer-wise operators. There exists scope for extensive\\nwork in the ﬁeld of machine unlearning, particularly in the\\ndomain of graph networks primarily used by recommendation\\nsystems. Some signiﬁcant contributions include:\\n3.1\\nRecEraser\\nThe authors introduced RecEraser [8], an eﬃcient machine un-\\nlearning framework that can be utilized for recommendation\\nsystems.The core concept behind RecEraser involved segment-\\ning the training data into multiple shards and subsequently\\ntraining a separate model for each shard. In order to main-\\ntain collaborative information within the data, 3 innovative\\ndata partition algorithms were initially developed to create\\nbalanced groups based on their similarity. Additionally, they\\nintroduced an adaptive aggregation method to enhance the ef-\\nfectiveness of the global model while preserving collaborative\\ninsights. RecEraser aims to protect and harness collaborative\\ninformation by carefully partitioning the dataset while making\\nshards. It adaptively assigned weights to these shards at the\\ntime of aggregation. [9]\\nHowever, RecEraser does not consider the transport weights\\nbetween sub-models, which may lead to sub-optimal cluster-\\ning and imbalance of group data. [10] It does not guarantee\\nthe convergence of sub-models on their own group data, which\\nmay aﬀect the quality of recommendations. RecEraser uses a\\ncomplex combination estimator that requires additional train-\\ning and tuning, which may increase the computational cost\\nand risk of overﬁtting. [6]\\n3.2\\nRecommendation Unlearning via Matrix\\nCorrection\\nMCRU [6] discussed a novel approach to recommendation un-\\nlearning, which is the process of deleting speciﬁc data and\\nMachine Unlearning for Recommendation Systems\\n3\\nmodels from a trained recommender system. The authors in-\\ntroduced Interaction and Mapping Matrices Correction (IM-\\nCorrect), a technique for recommendation unlearning. IMCor-\\nrect was able to attain recommendation unlearning through\\nupdating the interaction matrix and improving the complete-\\nness as well as utility. This was done by updating the map-\\nping matrix without expensive compute process of retraining\\nthe model. IMCorrect is a whitebox model that delivered more\\nsigniﬁcant ﬂexibility in managing diverse recommendation un-\\nlearning plans. It has the distinctive ability of learning incre-\\nmentally from unknown data, which further improved its use-\\nfulness. [11]\\n3.3\\nAltEraser\\nAltEraser [12] is an important machine unlearning work for\\nreal-world applications where users desired that a portion of\\ntheir data be removed, not exclusively from the data storage\\nbut correspondingly from the ML models being used in the\\nprocess, both for privacy or utility grounds. The authors ex-\\nplored swift ML techniques for recommendation engines that\\ncould dismiss the impact of a short portion of the training set\\nfrom the recommendation system without the full cost that\\nwould be incurred upon retraining. A realistic method to accel-\\nerate this was to ﬁne-tune the existing recommendation system\\non the left training set rather than initiating from a point of\\nrandom initialization. The authors presented a new recommen-\\ndation unlearning strategy, AltEraser [13], which separates the\\noptimization issue of unlearning and divides it additionally\\ninto tractable sub-problems.\\n3.4\\nGNNDelete\\nGNNDelete framework [7] introduced an innovative strategy\\nfor graph unlearning within Graph Neural Networks (GNNs).\\nIt involved the removal of elements such as nodes, their la-\\nbels, and relationships from a pre-trained GNN model, a crit-\\nical process for practical applications where data privacy, ac-\\ncuracy, and relevance are paramount concerns. This model-\\nagnostic approach prioritized two key attributes for eﬀective\\ngraph unlearning: Neighbourhood Inﬂuence and Deleted Edge\\nConsistency. The layer-wise operator employed in GNNDelete\\noptimized these qualities, ensuring that model weights and\\nneighboring representations remain uninﬂuenced by deleted\\nelements. The Deleted Edge Consistency guaranteed the re-\\nmoval of any residual impact, while Neighbourhood Inﬂuence\\npreserved the integrity of the remaining model knowledge post-\\ndeletion. GNNDelete achieved the removal of nodes and edges\\nfrom the model while retaining crucial learned information by\\nmodifying representations. Notably, this approach addressed\\nthe limitations of current partitioning and aggregation-based\\ntechniques, particularly in handling local graph relationships.\\n[14]\\n3.5\\nCaboose Forget Me Now: Fast and\\nExact Unlearning in Neighborhood-based\\nRecommendation\\nThe authors [15] proposed an algorithm that leveraged the\\ndata’s sparsity to work only using top-k entries that are presently\\nimpacted by the unlearning process. This meant that it could\\nbe computed using the norm of vectors and dot products. It\\nalso concentrated on zero similarity functions when the dot\\nproduct was zero between two rows. This helped to avoid un-\\nnecessary computations for pairs of users that hadn’t engaged\\nwith any shared items. The algorithm’s individual stages were\\nparallelized [16] to use multiple processing units for faster\\ncomputation. Appropriate data structures, such as compressed\\nrepresentations for binary heaps as well as sparse matrices for\\nmanaging top-k lists, were chosen to optimize low-level oper-\\nations in the algorithm.\\n3.6\\nGraph Scattering Transform (GST) Unlearning\\nGraph Classiﬁers with Limited Data Resources\\nThe method’s [17] computational complexity for node removal\\nwas considered order-optimal with respect to unlearning com-\\nplexity for graph classiﬁers. This eﬃciency was a signiﬁcant\\nadvantage, especially compared to methods that frequently\\nrequired full retraining. The approach’s use of GSTs for dif-\\nferent training graphs resembled sharding with small compo-\\nnents. Notably, the sizes of these shards did not impact the\\nperformance of the ﬁnal model, providing a robust and ﬂex-\\nible system for graph classiﬁcation tasks. The authors sug-\\ngested exploring other loss functions or methods in speciﬁc\\napplications was one potential future direction. This indicated\\nthe possibility of extending the framework’s capabilities even\\nfurther. The ﬁrst nonlinear graph learning framework based\\non GSTs with an approximate unlearning mechanism oﬀered\\nversatility, eﬃciency, and certiﬁed removal with lower privacy\\ncosts than alternatives. Especially for limited data resources,\\nthese methods have been proven eﬀective. [18]\\n3.7\\nSelective and Collaborative Inﬂuence Function\\n(SCIF) for Eﬃcient Recommendation\\nUnlearning\\nSCIF [19] was designed to enhance unlearning performance in\\nrecommendation tasks by incorporating two critical compo-\\nnents: user selection and preservation of collaboration. Each\\ndata point was treated independently in the the traditional\\nIF-based unlearning method. Thus, ignoring the collaborative\\nnature of recommendation data, SCIF introduced a collabo-\\nrative component to the inﬂuence function. When unlearning\\na data point, this component restored collaboration among\\nusers and items. Calculating the inﬂuence on the target user\\nembedding can still be computationally demanding, even with\\nthe reduction in parameters considered. To address this, au-\\nthors employed techniques like Hessian-Vector Product (HVP)\\nand conjugate gradients, which made the computation more\\neﬃcient. Neural Matrix Factorization (NMF) and LightGCN\\nwere the recommender models used.\\n3.8\\nAttribute-wise Unlearning in Recommender\\nSystems\\nAttribute-based unlearning methods for recommendation sys-\\ntems were also developed to protect the sensitive attributes\\nof users. This was done by using Post-Training Attribute Un-\\nlearning. [20]. The collaborative ﬁltering technique that is most\\ncommonly used for recommendations was selected in this study.\\nIt is based on matrix factorization and was used to split the\\nuser-item interaction matrix into 2 low-rank embedding ma-\\ntrices. These are item and user embedding. The attribute with\\ntwo or binary labels was the primary focus of the work.\\n3.9\\nRecommendation Unlearning via Inﬂuence\\nFunction (IFRU)\\nIFRU [21] leveraged inﬂuence functions, a concept from diﬀer-\\nential privacy and sensitivity analysis. Inﬂuence functions(IF)\\n4\\nB. Sachdeva et al.\\nprovided a way to gauge the impact of individual data points\\non the output provided by the model. Inﬂuence functions helped\\nidentify and adjust the impact of speciﬁc data points, enabling\\nthe model to unlearn undesirable or sensitive information with-\\nout needing full retraining. The key idea behind IFRU was to\\ncompute the unlearning of inﬂuential data points followed by\\nmodiﬁcations to the model’s parameters. As a result, the im-\\npact of those data points on the predictions made by the model\\nwas eﬀectively diminished. This allowed the model to ”forget”\\nthe unwanted information and adapt to the updated training\\ndata, ultimately improving the model’s safety and privacy. Ma-\\ntrix Factorization(MF) and LighGCN were the recommender\\nmodels used. [22]\\n3.10\\nFederated Unlearning for On-Device\\nRecommendation\\nFederated Recommendation Unlearning (FRU) [23] eﬃciently\\nunlearned target users in federated recommendation systems.\\nIt calibrated and combined all users’ previous model updates\\nto recreate the Federated Recommendation (FedRec) [24] mod-\\nels. Neural collaborative ﬁltering and LightGCN are the Fe-\\ndRec algorithms used. FRU sped up the reconstruction process\\ncompared to retraining from scratch as it reliably rolled back\\nFedRec and calibrated the stored model changes. FRU was\\nmodel-agnostic and was likely utilized in several FedRecs. The\\nﬁndings revealed that FRU could remove speciﬁc users’ im-\\npact and eﬀectively retrieve FedRec’s seven times faster. FRU\\nperformed unlearning when many users rendered the FedRec\\nservice and requested their information be forgotten at a spe-\\nciﬁc time. This involved restoring the FedRec model to an ear-\\nlier state and calibrating the historical updates of the model\\non the existing clients. As the direction drove the model ﬁt-\\nting, the calibration focused on the direction of updates while\\nmaintaining their length unchanged. [25]\\n3.11\\nHeterogeneous Federated Knowledge Graph\\nEmbedding Learning and Unlearning\\nFedLU [26] introduced embedding learning and frameworks\\nbased on federated Knowledge Graphs, which showcased in-\\nnovation, addressing knowledge sharing and forgetting in a\\nfederated environment. The model eﬀectively balanced global\\nconvergence and local optimization through mutual knowledge\\ndistillation, demonstrating a thoughtful approach to federated\\nlearning. Retroactive interference and passive decay in feder-\\nated unlearning allowed the model to forget speciﬁc knowl-\\nedge while maintaining overall performance, showcasing a ro-\\nbust unlearning mechanism. FedLU [27] has the potential to\\nmake a signiﬁcant impact by addressing challenges in federated\\nKG embedding, including knowledge exchange and unlearning,\\ncontributing to advancements in federated learning research.\\n3.12\\nAdv-MultVAE Model: Unlearning Protected\\nUser Attributes in Recommendations with\\nAdversarial Training\\nThe authors acknowledged the limitations of their approach\\nand suggested potential areas for improvement in the future.\\nThese included exploring generalization aspects, understand-\\ning user perceptions of bias in recommendations, and identify-\\ning which user groups are most aﬀected by the approach [28].\\nThese suggestions indicated a commitment to ongoing research\\nand improvement. There was a marginal decrease in perfor-\\nmance, mainly attributed to the challenge in model selec-\\ntion. [29]The authors recognized that ﬁnding a balance be-\\ntween accuracy (BAcc) and recommendation quality (NDCG)\\nhelped mitigate this performance loss. It was essential to con-\\nsider this trade-oﬀ when implementing the approach.\\n3.13\\nMUter: Machine Unlearning on Adversarially\\nTrained Models\\nMUter [30] presented a pioneering approach to the joint chal-\\nlenge of privacy and robustness in unlearning from adversar-\\nial training models. It used a Hessian-based measure and eﬃ-\\nciency enhancements that marked a signiﬁcant advancement.\\nThe method proved to be versatile and eﬀective across linear\\nand neural network models and opened avenues for future ex-\\nploration. Overall, MUter stood out as a favorable solution\\nwith a dual focus on privacy and adversarial robustness.\\n3.14\\nLaser: Making Recommender Systems\\nForget: Learning and Unlearning for Erasable\\nRecommendation\\nLASER [31] presented a promising framework for addressing\\nthe challenge of unlearning in recommendation systems. The\\ncombination of the Group module, focusing on balanced user\\ngrouping through collaborative embedding, and the SeqTrain\\nmodule, implementing a sequential training approach with col-\\nlaborative cohesion as a diﬃculty measure, demonstrated an\\ninnovative strategy for achieving eﬃcient unlearning and im-\\nproved model utility. The theoretical support for the SeqTrain\\nmodule added credibility to the proposed approach, and em-\\npirical validation on real-world datasets strengthening its prac-\\ntical relevance. However, potential complexities in implemen-\\ntation, dependency on the quality of collaborative embedding,\\nand sensitivity to hyperparameters should be carefully consid-\\nered. Overall, LASER provided a valuable contribution to the\\nﬁeld, and its real-world implications and scalability in diﬀerent\\nrecommendation system scenarios warrant further exploration\\nand research.\\nThe most recent works on MUL signiﬁcantly improve ex-\\nisting techniques, such as UltraRE [10], an ensemble-based\\nframework for recommendation unlearning that addresses re-\\ndundancy, relevance, and combination losses to enhance model\\nutility. Another notable contribution introduced by the cer-\\ntiﬁed MUL algorithm for minimax models [32], employs a\\ntotal-Hessian-based complete Newton update and the Gaus-\\nsian mechanism from diﬀerential privacy. Furthermore, these\\nworks, including exploration-focused approaches [33], evalu-\\nate current unlearning methods and introduce techniques like\\nimproved adversarial attacks, with a particular emphasis on\\nrecommendation systems.\\n3.15\\nMachine Unlearning from Bi-linear\\nRecommendations\\nThe work [3] proposed a fast heuristic-based MUL technique\\nfor recommendation systems, Untrain-ALS, that unlearns a\\nbi-linear model without compromising recommendation accu-\\nracy. The paper discussed the privacy risks that bi-linear rec-\\nommendation systems hold on memorizing data. They eﬀec-\\ntively presented an empirical test using de-noised membership\\ninference that proved sensitive to bi-linear recommendation\\nengines’ memorization.\\nMachine Unlearning for Recommendation Systems\\n5\\nTable 1: Analysis of SOTA MUL Algorithms on Recommendation Systems\\nMethod\\nYear Datasets\\nPros\\nLimitations\\nRecEraser\\n[8]\\n2022 Yelp2018,\\nMovielens-1m,\\nMovielens-10m\\nThe proposed model employed balanced\\ndata partition for collaborative informa-\\ntion preservation, attention-based adap-\\ntive aggregation with self-adaptive learn-\\ning rates, and computationally eﬃcient un-\\nlearning by retraining only relevant sub-\\nmodels and the aggregation part, demon-\\nstrating superior performance compared to\\nSOTA algorithms.\\nEﬃciently unlearning employing the\\nSharded, Isolated, Sliced, and Ag-\\ngregated (SISA) framework in batch\\nsetting remains an issue. For some\\nscenarios, the brute force method\\nof retraining performed better than\\nRecEraser. RecEraser does not con-\\nsider the transport weights between\\nsub-models, which may lead to sub-\\noptimal clustering and imbalance of\\ngroup data. It does not guaran-\\ntee sub-models’ convergence on their\\nown group data, which may aﬀect\\nthe quality of recommendations. It\\nonly handles individual forgetting\\nrequests, and its eﬃciency may de-\\ncrease when dealing with batch re-\\nquests. [10]\\nAltEraser\\n[12]\\n2022 MovieLens-1m,\\nAmazon-14core,\\nKuaiRec-binary\\nAltEraser demonstrated promising results\\nin consistency, accuracy, and eﬃciency as\\nthe foremost try at rapid approximate\\nMUL for neural recommendation models\\nthat are SOTA, based on extensive exper-\\niments on three real-world datasets\\nAltEraser,\\na\\nﬁne-tuning\\nmethod\\nproposed in the paper, may be sen-\\nsitive to hyperparameters like learn-\\ning rate, and its performance on rec-\\nommender systems with non-linear\\nmodels or intricate user-item inter-\\naction patterns remains uncertain.\\nMCRU [6]\\n2023 MovieLens-1m,\\nGowalla, Yelp\\nIMCorrect\\ndemonstrated superiority\\nin\\nforgetting out-of-distribution, out-of-date,\\nand attack data, exhibiting eﬀectiveness\\nin completeness, utility, and eﬃciency, and\\nproved to be a versatile tool applicable\\nin various recommendation unlearning sce-\\nnarios.\\nIMCorrect assumes correctability of\\nthe interaction matrix, which may\\nnot be viable when unlearned data\\nsigniﬁcantly impacts the model, and\\nthe paper lacks discussion on the\\nmethod’s performance in large-scale\\nrecommender systems with millions\\nof users and items.\\nGNNDelete\\n[7]\\n2023 Cora, PubMed,\\nDBLP,\\nCS,\\nOGB-Collab,\\nOGB-BioKG,\\nWordNet18RR\\nRegarding edge, node, and nodal feature\\ndeletion tasks, GNNDelete performed bet-\\nter than current methods by as much as\\n38.8% (AUC) and 32.2% when diﬀeren-\\ntiating removed edges from non-deleted\\nones. Compared to retraining the GNN\\nsystem from WordNet18, it required 9.3x\\nless space and less time than 12.3x, indi-\\ncating its eﬃciency\\nFurther exploration is needed to as-\\nsess GNNDelete’s performance on\\nlarger datasets and diverse graph\\ntypes, as well as to investigate its ro-\\nbustness to noise and adaptability to\\ndynamic graphs with constant node\\nand edge changes.\\nCaboose\\n[15]\\n2023 Movielens-\\n10m,\\nLastfm,\\nSynthetic Inter-\\nactions, Yahoo,\\nSpotify\\nCaboose facilitated quick index building,\\nenabling sub-second unlearning for large\\ndatasets and seamless integration with\\nnext-basket recommendation models, of-\\nfering transparent and cost-eﬀective alter-\\nnatives to neural approaches with signiﬁ-\\ncantly reduced training time.\\nThe removals’ impact depended on\\nthe data’s model details and co-\\noccurrence structure.\\nSCIF [19]\\n2023 Movielens-1m,\\nAmazon Digital\\nMusic (ADM)\\nThe method eliminated the need for re-\\ntraining, making it ideal for large-scale\\nsystems, enhancing eﬃciency through se-\\nlective user embedding updates, preserv-\\ning collaboration, and employing a Mem-\\nbership Inference Oracle (MIO) that veri-\\nﬁed comprehensive unlearning, ultimately\\ndemonstrating signiﬁcant improvements in\\neﬃciency while maintaining completeness.\\nCollaborative ﬁltering encountered\\nchallenges with large datasets and\\nrequired further development for\\nadaptation to complex models by\\nincorporating additional sources of\\nuser-item interaction data.\\nContinued on the next page\\n6\\nB. Sachdeva et al.\\nTable 1: Continued from previous page\\nMethod\\nYear Datasets\\nPros\\nLimitations\\nGST [17]\\n2022 MNIST,\\nCI-\\nFAR10,\\nPRO-\\nTEINS, IMDB,\\nCOLLABS\\nThe graph classiﬁer oﬀered mechanism,\\nadaptable to various loss functions, with\\nan approximate removal guarantee within\\nprivacy constraints, surpassing DP-GNNs\\nin privacy cost for similar unlearning out-\\ncomes for eﬃcient unlearning with nonlin-\\nearities in graph embedding and reduced\\ntraining data.\\nComparison\\nwith\\nDP-GNNs\\nre-\\nvealed their focus on node-level pri-\\nvacy in classiﬁcation tasks, suggest-\\ning their suitability for node classi-\\nﬁcation rather than graph classiﬁca-\\ntion.\\nIFRU [21]\\n2023 Amazon\\nElectronics,\\nBookCrossing\\nIFRU aims for comprehensive unlearning\\nby removing the inﬂuence of unusable data\\nwithout modifying model architectures, en-\\nsuring thoroughness without side eﬀects on\\nrecommendation quality. It eﬃciently ad-\\ndressed unlearning in recommendation sys-\\ntems, especially in scenarios requiring up-\\ndates based on user interactions or varying\\nsensitivity levels.\\nIFRU’s eﬀectiveness was inﬂuenced\\nby\\nrecommendation\\nmodel\\ncom-\\nplexity, potentially facing challenges\\nwith extremely complex models. It\\nmay also have limitations in highly\\nsparse datasets, particularly when\\ninteractions between users and items\\nare limited, making accurate inﬂu-\\nence function calculation and eﬀec-\\ntive recommendations challenging..\\nFRU [23]\\n2022 MovieLens-\\n100k,\\nSteam-\\n200k\\nFRU allowed users to request data erasure\\nfor privacy compliance, enabling speciﬁc\\ndata removal without full model retrain-\\ning, utilizing eﬃcient storage methods to\\nreduce historical data storage, and achiev-\\ning quick recalibration for up to 7 times\\nbetter system responsiveness.\\nThe potential drawbacks involved\\nthe complexity of managing updates\\non a distributed system and the\\nneed for additional computational\\nresources for update revision and re-\\nconstruction.\\nAdv-\\nMultVAE\\n[28]\\n2022 MovieLens-\\n1M,\\nLFM-2b-\\nDemoBias\\nAdv-MultVAE integrated an adversarial\\ncomponent to mitigate societal biases in\\nrecommendation systems, eﬀectively re-\\nducing biases in latent information about\\nuser-protected attributes, promoting fair-\\nness and privacy, as validated through em-\\npirical evidence measuring both the reduc-\\ntion in encoded protected information and\\nrecommendation accuracy.\\nThe adversarial approach in bias re-\\nduction lacked eﬀective generaliza-\\ntion to new datasets and chang-\\ning user behavior, and it may not\\nfully address user perception of bias\\nin recommended results, highlight-\\ning potential limitations in its ap-\\nplication.\\nLaser [31]\\n2022 MovieLens-1M,\\nAmazon Digital\\nMusic\\nLASER divided users into balanced groups\\nusing collaborative embedding and en-\\nhanced retraining eﬃciency, potentially\\nlowering computational expenses. The Se-\\nqTrain module adopted a sequential train-\\ning method with collaborative cohesion as\\na measure of diﬃculty, oﬀering a system-\\natic approach that enhanced model utility.\\nLASER’s eﬀectiveness depends on\\ncollaborative\\nembedding\\nquality\\nand sensitivity to hyperparameter\\nchoices, impacting its performance.\\nFedLU [26] 2023 FB15k-237\\nC3,C5,C10\\nFedLU employed mutual knowledge distil-\\nlation for global convergence and enhanced\\nlocal optimization on dynamic, heteroge-\\nneous data among clients. It integrated\\nretroactive interference and passive decay\\nfor eﬀective triplet unlearning without sig-\\nniﬁcant performance degradation.\\nFedLU introduced complexity with\\nmechanisms like mutual knowledge\\ndistillation, retroactive interference,\\nand passive decay, demonstrating ef-\\nfectively on speciﬁc clustering-based\\ndatasets, though performance could\\nhave varied on datasets with diﬀer-\\nent characteristics.\\nMUter [30] 2023 MNIST-B, Cov-\\ntype,\\nLacuna-\\n10, CIFAR-10\\nMUter addressed the challenge of un-\\nlearning from adversarial training mod-\\nels, ensuring privacy compliance and ro-\\nbustness, utilizing a Hessian-based mea-\\nsure with computational techniques for eﬃ-\\nciency gains, demonstrating high eﬀective-\\nness in maintaining model accuracy and\\nadversarial robustness across linear and\\nneural network models.\\nWhile eﬃcient, MUter incurred a\\nslightly longer computational time\\ndue to the additional computation\\nof the total Hessian.\\nMachine Unlearning for Recommendation Systems\\n7\\n3.16\\nClosed-form Machine Unlearning for Matrix\\nFactorization (CMUMF)\\nA closed-form MUL technique is suggested by the authors [34].\\nAs the closed-form unlearning update, the authors recorded\\nthe implied reliance between the rows and columns, leading\\nto a complete Newton step based on Hessian. The paper val-\\nidated the eﬃcacy and utility of CMUMF using 5 real-world\\ndatasets from 3 distinguishable domains of application, includ-\\ning artiﬁcial data sets with three diﬀerent sizes.\\n4\\nChallenges and Future Directions\\nMachine unlearning is a fairly contemporary and emerging\\narea of research, focusing on the ability to remove or update\\nspeciﬁc data from machine learning models. As researchers\\ndelve into this emerging domain, it becomes evident that ex-\\nploring unlearning techniques has been somewhat limited, par-\\nticularly in the context of diverse data structures like multi-\\nmodal data. Future investigations must extend the scope of\\nunlearning to intricate formats such as text, audio, and mul-\\ntimedia, presenting novel challenges related to temporal se-\\nquences, spatial linkages, and hierarchical structures.\\nWhile existing research has made strides in understanding\\nunlearning within convex models like logistic regression, there\\nremains a notable gap in addressing the complexities intro-\\nduced by non-convex neural networks, including Convolutional\\nNeural Networks and Recurrent Neural Networks commonly\\nemployed in deep learning applications. Eﬃcient algorithms\\ntailored for non-convex optimization problems in the context\\nof unlearning strategies are still in the early stages of develop-\\nment.\\nMoreover, the current focus on instance-level data removal\\nin unlearning strategies has limitations, oﬀering users only\\na constrained degree of ﬂexibility in data management and\\nmodel updates. Future research should explore the develop-\\nment of quantitative metrics to foster a more versatile and\\nuser-friendly approach. These metrics could provide a nuanced\\nevaluation of unlearning eﬃcacy by measuring the extent of\\ninﬂuence retention for retained data and the corresponding\\nreduction in inﬂuence for removed data. This avenue of explo-\\nration promises to enhance the comprehensiveness and adapt-\\nability of machine unlearning methodologies, paving the way\\nfor future more eﬀective and user-centric approaches.\\nFuture scope for this can be extended to social media rec-\\nommendation systems where the reliance on user search his-\\ntory is pretty high. Once any information is fed into them, the\\ngraphically dense relation that information establishes with\\nthe existing information makes the problem more challenging\\nand relevant to the actual problems. Solving these problems is\\nimportant to ensure future development in this domain.\\n5\\nConclusion\\nIn conclusion, this literature review highlighted the notable\\ncontributions in this domain, underscoring the adoption of\\ngraph networks to address the multifaceted challenges of ma-\\nchine unlearning. Signiﬁcant advancements and innovative strate-\\ngies were observed; however, it is imperative to recognize that\\nthis domain remained in its premature stages. The niche area\\nof machine unlearning within recommendation systems pre-\\nsented ample opportunities for future exploration and devel-\\nopment. Researchers and practitioners were able to harness\\nthe potential of machine unlearning to enhance user privacy,\\nmaintain recommendation system accuracy, and adapt to ever-\\nchanging user preferences. As recommendation systems con-\\ntinued to shape our digital experiences, the quest for eﬃcient,\\naccurate, and privacy-aware MUL methods ensured these sys-\\ntems’ ongoing success and safety.\\nReferences\\n1. P.\\nCovington,\\nJ.\\nAdams,\\nand\\nE.\\nSargin,\\n“Deep\\nneural\\nnetworks\\nfor\\nyoutube\\nrecommendations,”\\nin\\nProceedings\\nof the 10th ACM Conference on Recommender Systems,\\nser. RecSys ’16.\\nNew York, NY, USA: Association for\\nComputing Machinery, 2016, p. 191–198. [Online]. Available:\\nhttps://doi.org/10.1145/2959100.2959190\\n2. R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton,\\nand\\nJ.\\nLeskovec,\\n“Graph\\nconvolutional\\nneural\\nnetworks\\nfor web-scale recommender systems,” in Proceedings of the\\n24th ACM SIGKDD International Conference on Knowledge\\nDiscovery & Data Mining, ser. KDD ’18.\\nNew York, NY,\\nUSA: Association for Computing Machinery, 2018, p. 974–983.\\n[Online]. Available: https://doi.org/10.1145/3219819.3219890\\n3. M.\\nXu,\\nJ.\\nSun,\\nX.\\nYang,\\nK.\\nYao,\\nand\\nC.\\nWang,\\n“Netﬂix and forget: Eﬃcient and exact machine unlearning\\nfrom bi-linear recommendations,” 2023. [Online]. Available:\\nhttps://arxiv.org/abs/2302.06676\\n4. C. Eksombatchai, P. Jindal, J. Z. Liu, Y. Liu, R. Sharma,\\nC.\\nSugnet,\\nM.\\nUlrich,\\nand\\nJ.\\nLeskovec,\\n“Pixie:\\nA\\nsystem\\nfor\\nrecommending\\n3+\\nbillion\\nitems\\nto\\n200+\\nmillion\\nusers\\nin\\nreal-time,”\\n2017.\\n[Online].\\nAvailable:\\nhttps://arxiv.org/abs/1711.07601\\n5. A. Sekhari, J. Acharya, G. Kamath, and A. T. Suresh,\\n“Remember what you want to forget: Algorithms for machine\\nunlearning,” ArXiv, vol. abs/2103.03279, 2021. [Online]. Avail-\\nable: https://api.semanticscholar.org/CorpusID:232134970\\n6. J. Liu, D. Li, H. Gu, T. Lu, J. Wu, P. Zhang, L. Shang, and\\nN. Gu, “Recommendation unlearning via matrix correction,”\\n2023. [Online]. Available: https://arxiv.org/abs/2307.15960\\n7. J. Cheng, G. Dasoulas, H. He, C. Agarwal, and M. Zitnik,\\n“GNNDelete: A general strategy for unlearning in graph\\nneural networks,” in The Eleventh International Conference\\non\\nLearning\\nRepresentations,\\n2023.\\n[Online].\\nAvailable:\\nhttps://openreview.net/forum?id=X9yCkmT5Qrl\\n8. C.\\nChen,\\nF.\\nSun,\\nM.\\nZhang,\\nand\\nB.\\nDing,\\n“Rec-\\nommendation\\nunlearning,”\\n2022.\\n[Online].\\nAvailable:\\nhttps://arxiv.org/abs/2201.06820\\n9. H. Xu, T. Zhu, L. Zhang, W. Zhou, and P. S. Yu, “Machine\\nunlearning: A survey,” ACM Computing Surveys, vol. 56, no. 1,\\npp. 1–36, 2023.\\n10. Y. Li, C. Chen, Y. Zhang, W. Liu, L. Lyu, X. Zheng, D. Meng,\\nand J. Wang, “Ultrare: Enhancing receraser for recommen-\\ndation unlearning via error decomposition,” in Thirty-seventh\\nConference on Neural Information Processing Systems, 2023.\\n11. S. Liu, J.\\nLiu, H. Gu, D. Li, T.\\nLu, P. Zhang, and\\nN. Gu, “Autoseqrec: Autoencoder for eﬃcient sequential rec-\\nommendation,” in Proceedings of the 32nd ACM International\\nConference on Information and Knowledge Management, ser.\\nCIKM ’23.\\nNew York, NY, USA: Association for Com-\\nputing Machinery, 2023, p. 1493–1502. [Online]. Available:\\nhttps://doi.org/10.1145/3583780.3614788\\n12. W. Liu, J. Wan, X. Wang, W. Zhang, D. Zhang, and H. Li,\\n“Forgetting fast in recommender systems,” 2022. [Online].\\nAvailable: https://arxiv.org/abs/2208.06875\\n13. S. Zhang, J. Lou, L. Xiong, X. Zhang, and J. Liu, “Closed-form\\nmachine unlearning for matrix factorization,” in Proceedings\\nof the 32nd ACM International Conference on Information\\nand Knowledge Management.\\nACM, Oct. 2023. [Online].\\nAvailable: https://doi.org/10.1145/3583780.3614811\\n14. Y. Sinha, M. Mandal, and M. Kankanhalli, “Distill to delete:\\nUnlearning in graph networks with knowledge distillation,”\\narXiv preprint arXiv:2309.16173, 2023.\\n8\\nB. Sachdeva et al.\\n15. S. Schelter, M. Ariannezhad, and M. de Rijke, “Forget\\nme now: Fast and exact unlearning in neighborhood-based\\nrecommendation,” in Proceedings of the 46th International\\nACM SIGIR Conference on Research and Development in\\nInformation Retrieval, ser. SIGIR ’23.\\nNew York, NY, USA:\\nAssociation for Computing Machinery, 2023, p. 2011–2015.\\n[Online]. Available: https://doi.org/10.1145/3539618.3591989\\n16. Z. Wu, J. Zhu, Q. Li, and B. He, “Deltaboost: Gradient\\nboosting decision trees with eﬃcient machine unlearning,”\\nProc. ACM Manag. Data, vol. 1, no. 2, jun 2023. [Online].\\nAvailable: https://doi.org/10.1145/3589313\\n17. C. Pan, E. Chien, and O. Milenkovic, “Unlearning graph\\nclassiﬁers\\nwith\\nlimited\\ndata\\nresources,”\\n2022.\\n[Online].\\nAvailable: https://arxiv.org/abs/2211.03216\\n18. E.\\nChien,\\nC.\\nPan,\\nand\\nO.\\nMilenkovic,\\n“Cer-\\ntiﬁed\\ngraph\\nunlearning,”\\n2022.\\n[Online].\\nAvailable:\\nhttps://arxiv.org/abs/2206.09140\\n19. Y. Li, C. Chen, X. Zheng, Y. Zhang, B. Gong, J. Wang, and\\nL. Chen, “Selective and collaborative inﬂuence function for ef-\\nﬁcient recommendation unlearning,” Expert Systems with Ap-\\nplications, vol. 234, p. 121025, 2023.\\n20. Y. Li, C. Chen, X. Zheng, Y. Zhang, Z. Han, D. Meng,\\nand J. Wang, “Making users indistinguishable: Attribute-\\nwise unlearning in recommender systems,” in Proceedings\\nof the 31st ACM International Conference on Multimedia,\\nser.\\nMM\\n’23.\\nNew\\nYork,\\nNY,\\nUSA:\\nAssociation\\nfor\\nComputing Machinery, 2023, p. 984–994. [Online]. Available:\\nhttps://doi.org/10.1145/3581783.3612418\\n21. Y. Zhang, Z. Hu, Y. Bai, F. Feng, J. Wu, Q. Wang, and\\nX. He, “Recommendation unlearning via inﬂuence function,”\\n2023. [Online]. Available: https://arxiv.org/abs/2307.02147\\n22. C. Chen, F. Sun, M. Zhang, and B. Ding, “Recommendation\\nunlearning,” in Proceedings of the ACM Web Conference\\n2022, ser. WWW ’22.\\nNew York, NY, USA: Association for\\nComputing Machinery, 2022, p. 2768–2777. [Online]. Available:\\nhttps://doi.org/10.1145/3485447.3511997\\n23. W. Yuan, H. Yin, F. Wu, S. Zhang, T. He, and H. Wang,\\n“Federated unlearning for on-device recommendation,” 2022.\\n[Online]. Available: https://arxiv.org/abs/2210.10958\\n24. G. Lin, F. Liang, W. Pan, and Z. Ming, “Fedrec: Federated\\nrecommendation with explicit feedback,” IEEE Intelligent Sys-\\ntems, vol. 36, no. 5, pp. 21–30, 2021.\\n25. S. Zhang, W. Yuan, and H. Yin, “Comprehensive privacy anal-\\nysis on federated recommender system against attribute infer-\\nence attacks,” IEEE Transactions on Knowledge and Data En-\\ngineering, pp. 1–13, 2023.\\n26. X.\\nZhu,\\nG.\\nLi,\\nand\\nW.\\nHu,\\n“Heterogeneous\\nfederated\\nknowledge graph embedding learning and unlearning,” in\\nProceedings of the ACM Web Conference 2023, ser. WWW\\n’23.\\nNew\\nYork,\\nNY,\\nUSA:\\nAssociation\\nfor\\nComput-\\ning\\nMachinery,\\n2023,\\np.\\n2444–2454.\\n[Online].\\nAvailable:\\nhttps://doi.org/10.1145/3543507.3583305\\n27. Z. Deng, Z. Han, C. Ma, M. Ding, L. Yuan, C. Ge, and\\nZ. Liu, “Vertical federated unlearning on the logistic regression\\nmodel,” Electronics, vol. 12, no. 14, p. 3182, Jul. 2023. [Online].\\nAvailable: https://doi.org/10.3390/electronics12143182\\n28. C. Ganh¨or, D. Penz, N. Rekabsaz, O. Lesota, and M. Schedl,\\n“Unlearning protected user attributes in recommendations with\\nadversarial training,” in Proceedings of the 45th International\\nACM SIGIR Conference on Research and Development in\\nInformation Retrieval, ser. SIGIR ’22.\\nNew York, NY, USA:\\nAssociation for Computing Machinery, 2022, p. 2142–2147.\\n[Online]. Available: https://doi.org/10.1145/3477495.3531820\\n29. J. Leysen, “Exploring unlearning methods to ensure the\\nprivacy, security, and usability of recommender systems,” in\\nProceedings of the 17th ACM Conference on Recommender\\nSystems, ser. RecSys ’23.\\nNew York, NY, USA: Association\\nfor\\nComputing\\nMachinery,\\n2023,\\np.\\n1300–1304.\\n[Online].\\nAvailable: https://doi.org/10.1145/3604915.3608862\\n30. J. Liu, M. Xue, J. Lou, X. Zhang, L. Xiong, and Z. Qin, “Muter:\\nMachine unlearning on adversarially trained models,” in Pro-\\nceedings of the IEEE/CVF International Conference on Com-\\nputer Vision, 2023, pp. 4892–4902.\\n31. Y. Li, C. Chen, X. Zheng, J. Liu, and J. Wang, “Making rec-\\nommender systems forget: Learning and unlearning for erasable\\nrecommendation,” Knowledge-Based Systems, p. 111124, 2023.\\n32. J.\\nLiu,\\nJ.\\nLou,\\nZ.\\nQin,\\nand\\nK.\\nRen,\\n“Certiﬁed\\nmin-\\nimax\\nunlearning\\nwith\\ngeneralization\\nrates\\nand\\ndeletion\\ncapacity,”\\nin\\nThirty-seventh\\nConference\\non\\nNeural\\nIn-\\nformation\\nProcessing\\nSystems,\\n2023.\\n[Online].\\nAvailable:\\nhttps://openreview.net/forum?id=6H8Md75kAw\\n33. J. Leysen, “Exploring unlearning methods to ensure the\\nprivacy, security, and usability of recommender systems,” in\\nProceedings of the 17th ACM Conference on Recommender\\nSystems, ser. RecSys ’23.\\nNew York, NY, USA: Association\\nfor\\nComputing\\nMachinery,\\n2023,\\np.\\n1300–1304.\\n[Online].\\nAvailable: https://doi.org/10.1145/3604915.3608862\\n34. S. Zhang, J. Lou, L. Xiong, X. Zhang, and J. Liu, “Closed-form\\nmachine unlearning for matrix factorization,” in Proceedings of\\nthe 32nd ACM International Conference on Information and\\nKnowledge Management, ser. CIKM ’23. New York, NY, USA:\\nAssociation for Computing Machinery, 2023, p. 3278–3287.\\n[Online]. Available: https://doi.org/10.1145/3583780.3614811\\n'},\n",
       " {'abstract': 'Preference-based reinforcement learning (RL) provides a framework to train agents using human feedback through pairwise preferences over pairs of behaviors, enabling agents to learn desired behaviors when it is difficult to specify a numerical reward function. While this paradigm leverages human feedback, it currently treats the feedback as given by a single human user. Meanwhile, incorporating preference feedback from crowds (i.e. ensembles of users) in a robust manner remains a challenge, and the problem of training RL agents using feedback from multiple human users remains understudied.',\n",
       "  'introduction': 'Reinforcement learning (RL) from human feedback is a promising approach for learning intelligent behaviors in the absence of a known numerical reward signal. Such methods have shown success in domains such as Atari games, robotics, and large language models (LLMs). In this work, we consider human feedback that is crowdsourced, i.e., where (1) data is labeled by multiple human users (e.g. ≥ 2 labels for each data point), and (2) these labels are aggregated to form an ensemble label to be used for downstream analysis.',\n",
       "  'literature_review': 'Early works that explicitly model human feedback from crowds have focused on imitation learning, where there are multiple demonstrations (assumed to be from multiple human users) of unknown expertise and quality, and the goal is to learn a policy reflecting expert behavior. Recent work that has explicitly modeled crowdsourced data for imitation learning has shown significant performance gains from simultaneously modeling human demonstrations and estimating the demonstrators’ expertise levels. While recent work by [Zhang and Kashima, 2023] used crowdsourcing methods to learn a reward function using preference-based reward learning, it is limited to the offline RL case, where there is a fixed dataset of preference pairs, and the RL algorithm does not have online access to the environment.',\n",
       "  'methodology': 'We next explain our methodology for simulating diverse crowds of human users with different rationality levels. Afterwards, we outline our Crowd-PrefRL approach to performing RL with crowdsourced feedback (Algorithm 1): first, we explain how to estimate preference labels from a crowd and then, we show how to integrate these crowd-aggregated preference labels into a framework for preference-based RL.',\n",
       "  'results': 'We see a strong positive correlation between the error rates of users in the crowd and the performance of SML compared to MAJ. In particular, as the user error increases (indicating a more diverse crowd), the better the SML labels perform compared to the MAJ labels. This indicates that the SML is effectively filtering out inconsistent preference feedback from users who have higher error rates and aggregating crowd decisions more consistently compared to MAJ.',\n",
       "  'conclusion': 'This work demonstrates the viability of learning reward functions from preference feedback provided by crowds of unknown expertise and reliability. We believe that this is the first work that focuses on using preference feedback from crowds to learn a crowdsourced reward function for preference-based RL in the online RL setting.',\n",
       "  'title': 'Crowd-PrefRL: Preference-Based Reward Learning from Crowds',\n",
       "  'author': 'David Chhan, Ellen Novoseller, Vernon J. Lawhern',\n",
       "  'textdata': 'Crowd-PrefRL: Preference-Based Reward Learning from Crowds\\nDavid Chhan, Ellen Novoseller, and Vernon J. Lawhern\\nDEVCOM Army Research Laboratory\\nAberdeen Proving Ground, MD 21005 USA\\n{david.chhan.civ, ellen.r.novoseller.civ,\\nvernon.j.lawhern.civ}@army.mil\\nAbstract\\nPreference-based reinforcement learning (RL) pro-\\nvides a framework to train agents using human\\nfeedback through pairwise preferences over pairs\\nof behaviors, enabling agents to learn desired be-\\nhaviors when it is difficult to specify a numerical\\nreward function.\\nWhile this paradigm leverages\\nhuman feedback, it currently treats the feedback\\nas given by a single human user. Meanwhile, in-\\ncorporating preference feedback from crowds (i.e.\\nensembles of users) in a robust manner remains a\\nchallenge, and the problem of training RL agents\\nusing feedback from multiple human users remains\\nunderstudied. In this work, we introduce Crowd-\\nPrefRL, a framework for performing preference-\\nbased RL leveraging feedback from crowds. This\\nwork demonstrates the viability of learning re-\\nward functions from preference feedback provided\\nby crowds of unknown expertise and reliability.\\nCrowd-PrefRL not only robustly aggregates the\\ncrowd preference feedback, but also estimates the\\nreliability of each user within the crowd using only\\nthe (noisy) crowdsourced preference comparisons.\\nMost importantly, we show that agents trained\\nwith Crowd-PrefRL outperform agents trained with\\nmajority-vote preferences or preferences from any\\nindividual user in most cases, especially when the\\nspread of user error rates among the crowd is large.\\nResults further suggest that our method can identify\\nminority viewpoints within the crowd.\\n1\\nIntroduction\\nReinforcement learning (RL) from human feedback [Casper\\net al., 2023] is a promising approach for learning intelligent\\nbehaviors in the absence of a known numerical reward signal.\\nSuch methods have shown success in domains such as Atari\\ngames [Christiano et al., 2017], robotics [Wilde et al., 2021;\\nTorne et al., 2023; Lee et al., 2021], and large language mod-\\nels (LLMs) [Ouyang et al., 2022]. In this work, we consider\\nhuman feedback that is crowdsourced, i.e., where (1) data is\\nlabeled by multiple human users (e.g. ≥ 2 labels for each\\ndata point), and (2) these labels are aggregated to form an en-\\nsemble label to be used for downstream analysis. In applica-\\nFigure 1: Crowd-PrefRL framework for preference-based RL from\\ncrowds. We assume a crowd, or ensemble, of users is queried for\\ntheir preferences over pairs of segmented behaviors (A vs.\\nB).\\nThese crowd preferences are then used to learn a reward function\\nfor preference-based RL.\\ntions from LLMs to household robotics to autonomous driv-\\ning, crowdsourced human data could enable RL algorithms\\nto learn user-preferred behaviors that are robust across many\\nreal-world situations. Yet, at the same time, crowdsourcing\\nalgorithms will need to recognize and account for disagree-\\nment across users in the crowd. Example scenarios could in-\\nclude: an algorithm that a) trains models that are sensitive\\nto multiple user viewpoints (e.g., an LLM provides informa-\\ntion that benefits an underrepresented minority at minimal\\ncost to the majority [Siththaranjan, Laidlaw, and Hadfield-\\nMenell, 2023]), b) trains models that reflect a majority view-\\npoint while enabling downstream individual personalization\\n(e.g., a household robot demonstrates performant out-of-the-\\nbox behavior, but fine-tunes to specific users’ preferences for\\ntidying and organization), and c) identifies and rejects mali-\\ncious user data (e.g., an LLM is resilient to feedback contain-\\ning misinformation).\\nThis work considers RL from crowdsourced pairwise pref-\\nerences, in which a human answers queries of the form “Do\\nyou prefer A or B?” [Christiano et al., 2017; Lee, Smith, and\\nAbbeel, 2021; Lee et al., 2021]. Current preference-based\\nRL methods treat preference feedback as given by a single\\nhuman user, even if the feedback is actually given by mul-\\ntiple users with potentially different backgrounds and levels\\nof expertise. For example, when learning reward functions\\nfor training LLMs, each individual preference pair query is\\ntypically shown to just one of the users due to multiple con-\\nstraints (e.g. cost, human labeling time/effort); thus, the full\\nlabeled dataset aggregates the disjoint query subsets shown\\nto each user [Ouyang et al., 2022]. However, it may often be\\narXiv:2401.10941v1  [cs.HC]  17 Jan 2024\\npreferable to not only obtain feedback from a group of users,\\nbut also to aggregate the crowdsourced data in a non-naive\\nmanner that recognizes differences between individuals. This\\ncapability can be beneficial for multiple reasons, for example\\nto analyze patterns of disagreement in preference feedback\\nacross the crowd and to detect minority groups in the crowd\\n(i.e., users that provide feedback that is consistently different\\nfrom the majority). It also holds important practical model-\\ning implications, for example mitigating model bias and en-\\nsuring fairness [Ouyang et al., 2022] and assisting users with\\ndiverse preferences or varying degrees of noise in their feed-\\nback [Siththaranjan, Laidlaw, and Hadfield-Menell, 2023].\\nWhile previous literature uses crowdsourcing methods\\nto aggregate labels across users and learn crowd prefer-\\nences [Chen et al., 2013; Li et al., 2016], the specific ap-\\nplication of crowdsourcing to reward learning in RL re-\\nmains understudied. Early works that explicitly model hu-\\nman feedback from crowds have focused on imitation learn-\\ning, where there are multiple demonstrations (assumed to be\\nfrom multiple human users) of unknown expertise and qual-\\nity, and the goal is to learn a policy reflecting expert behav-\\nior [Babes et al., 2011; Dimitrakakis and Rothkopf, 2012;\\nHausman et al., 2017]. As noted in [Beliaev et al., 2022],\\nhowever, these approaches simply aggregate the feedback\\nfrom multiple humans into one homogeneous group, poten-\\ntially leading to sub-optimal learning due to the presence of\\nvarying levels of expertise and reliability among the different\\nhuman users. Recent work that has explicitly modeled crowd-\\nsourced data for imitation learning [Beliaev et al., 2022]\\nhas shown significant performance gains from simultaneously\\nmodeling human demonstrations and estimating the demon-\\nstrators’ expertise levels. While recent work by [Zhang and\\nKashima, 2023] used crowdsourcing methods to learn a re-\\nward function using preference-based reward learning, it is\\nlimited to the offline RL case [Levine et al., 2020], where there\\nis a fixed dataset of preference pairs, and the RL algorithm\\ndoes not have online access to the environment. This poten-\\ntially limits the expressiveness of the learned reward function\\nto the behaviors contained in the offline dataset.\\nIn this work, we study preference-based RL leveraging\\nfeedback from crowds in the online RL scenario, where the\\nRL algorithm has access to the environment and can sample\\nnew preference pairs for the crowd to label throughout model\\ntraining. Specifically:\\n1. We introduce the Crowd-PrefRL framework for per-\\nforming online preference-based RL leveraging feed-\\nback from crowds and instantiate Crowd-PrefRL us-\\ning Proximal Policy Optimization (PPO) [Schulman\\net al., 2017], a state-of-the-art RL algorithm, to pro-\\npose Crowd-PrefPPO. Crowd-PrefRL aggregates the\\nuser feedback to learn crowdsourced preference labels,\\nand then uses those labels for reward function learning\\nin preference-based RL (see Figure 1).\\n2. We use techniques derived from unsupervised ensem-\\nble learning to robustly aggregate the crowd preference\\nfeedback, estimating each user’s reliability in a com-\\npletely unsupervised fashion and detecting the presence\\nof multiple, e.g. minority, viewpoints among the crowd.\\n3. We perform experiments suggesting that RL agents\\ntrained with crowd preferences outperform RL agents\\ntrained via majority-vote preferences or preferences\\nfrom any individual crowd member, in particular when\\nthe spread of user errors among the crowd is large.\\nWe also show a proof-of-concept for the ability of our\\nmethod to identify minority viewpoints within a crowd.\\nThis work demonstrates the viability of learning reward func-\\ntions from preference feedback provided by crowds of un-\\nknown expertise and reliability. We believe that this is the first\\nwork that focuses on using preference feedback from crowds\\nto learn a crowdsourced reward function for preference-based\\nRL in the online RL setting.\\n2\\nProblem Formulation\\nIn this paper, we are concerned with answering the following\\nquestions: (1) How can we effectively combine preference\\nfeedback from multiple users of unknown expertise and un-\\nknown reliability in the absence of ground-truth information?\\n(2) Can we derive who are the “best” users in a given crowd in\\nan unsupervised manner? (3) Can we learn a crowdsourced\\nreward function that is better than any individual user’s re-\\nward function?, and (4) Can we detect the presence of mi-\\nnority feedback (as opposed to purely noisy feedback) in the\\ncrowd based only on the preference feedback? This section\\nformulates this problem more precisely.\\nWe consider an agent that takes actions and interacts with\\nan environment [Sutton and Barto, 2018], such that at each\\ntimestep t, the agent receives a state st from the environment\\nand chooses an action at based on its policy π. In traditional\\nepisodic RL, the environment also returns a reward r(st, at),\\nand the agent seeks to maximize the total discounted sum\\nof rewards over an episode.\\nHowever, for many complex\\ntasks, it is difficult to construct a suitable reward function.\\nPreference-based RL approaches instead query a human user\\nto obtain preference feedback over segments of agent behav-\\niors, and use this feedback to learn a reward function [Chris-\\ntiano et al., 2017; Lee, Smith, and Abbeel, 2021]. A seg-\\nment is defined as a length-H sequence of observations and\\nactions, {(s1, a1), ..., (sH, aH)}. Given a pair of segments\\nx = (A, B), a user indicates which segment is preferred: +1\\nin the event that A is preferred over B (denoted by A ≻ B) or\\n−1 if B is preferred over A (denoted by A ≺ B). Preference-\\nbased RL aims to train an agent to perform human-desirable\\nbehaviors according to the human user’s preferences.\\nOur problem formulation assumes access to a crowd, or en-\\nsemble, of users, who each assign pairwise preference labels\\nto a sequentially-selected set S = {xk}S\\nk=1, S = |S| of seg-\\nment pairs. Each query xk takes the form xk = (Ak, Bk),\\nwhere Ak, Bk belong to the set X of available segments. In\\naddition, we assume that a majority (e.g. at least 51%) of the\\ncrowd provides preference feedback according to a shared un-\\nderstanding of desired agent behavior. More concretely, we\\nassume the existence of an unknown ground-truth reward r,\\nand that for a majority of users, their preferences are (noisily)\\ngenerated with respect to r. Note that this formulation allows\\na minority group of users to potentially provide feedback ac-\\ncording to a different reward function(s). Beyond this, we\\ndo not assume any further knowledge about the crowd (e.g.\\nexpertise, reliability). Our goal is to learn a crowd-informed\\npreference label for each segment pair using only the prefer-\\nence labels given by the crowd. Formally, let {fi}M\\ni=1 rep-\\nresent M users of unknown reliability and error rate, where\\neach user i provides a preference label fi(xk) ∈ {−1, 1} on\\neach input query xk ∈ S. Then, let y be the vector of (un-\\nknown to the algorithm) ground-truth labels according to the\\ntrue reward r (i.e., in which segments with higher ground-\\ntruth reward are preferred), y = [y1, ..., yS]T . Using only the\\npreference labels from M users on the segment pairs in S and\\nwithout access to any ground-truth rewards or preferences, we\\nseek an optimal estimate ˆy = [ˆy1, ..., ˆyS]T of the true labels\\ny given the crowd preference labels. We also investigate the\\nability to identify minority groups of users who provide feed-\\nback that consistently differs from that of the majority.\\n3\\nMethods\\nWe next explain our methodology for simulating diverse\\ncrowds of human users with different rationality levels. Af-\\nterwards, we outline our Crowd-PrefRL approach to perform-\\ning RL with crowdsourced feedback (Algorithm 1): first, we\\nexplain how to estimate preference labels from a crowd and\\nthen, we show how to integrate these crowd-aggregated pref-\\nerence labels into a framework for preference-based RL.\\n3.1\\nSimulating Diverse Crowds\\nIn order to systematically evaluate and study the effects of\\nuser error rates and their impact on learning from crowd-\\nsourced user preferences, we apply the Stochastic Preference\\nModel [Lee et al., 2021] to simulate users with mixtures of\\ndifferent irrationality levels:\\nP[A ≻ B; β, γ] =\\n(1)\\ne(β PH\\nt=1 γH−tr(si\\nt,ai\\nt))\\ne(β PH\\nt=1 γH−tr(si\\nt,ai\\nt)) + e(β PH\\nt=1 γH−tr(sj\\nt,aj\\nt)) ,\\nwhere γ ∈ (0, 1] is a discount factor to model myopic behav-\\nior and β is a rationality constant. Note that user feedback is\\nfully rational and deterministic as β → ∞, whereas β = 0\\nproduces uniformly random choices. In addition, we model\\nan additional parameter ϵ that models users making mistakes\\ndue to accidental errors. For our analysis, each user is sim-\\nulated with a random draw of parameters from the following\\nranges: γ ∈ [0.96, 1.0], β ∈ [−0.5, −1.0], and ϵ ∈ [0.1, 0.5].\\nThese parameter ranges where chosen as they produced di-\\nverse user error rates primarily between 15% and 40%, which\\nis approximately the range of human error rates observed\\nin a prior study of human feedback in preference-based RL\\n(12%−37% in [Brown et al., 2019]). We then sample crowds\\nof M ∈ {7, 11, 15} users according to this model to study the\\neffect of crowd size on crowd label learning.\\n3.2\\nEstimating Preference Labels from a Crowd\\nFor each query, Crowd-PrefRL distills the M preference la-\\nbels from each crowd member to a single preference label.\\nWe discuss two methods for accomplishing this distillation.\\nThe first, majority voting, is a comparatively naive approach,\\nAlgorithm 1 Crowd-PrefRL: Preference-based RL with re-\\nward learning from crowds\\nRequire: frequency of crowd feedback K\\nRequire: number of queries Nquery per feedback session\\nRequire: crowd size M\\nRequire: crowdsource method, one of (“MAJ”, “SML”)\\n1: Initialize parameters of policy πϕ, reward model brψ, pref-\\nerence dataset D ← ∅, and buffer B ← ∅\\n2: // EXPLORATION PHASE\\n3: B, πϕ ← EXPLORE() according to policy πϕ\\n4: for each iteration do\\n5:\\n// REWARD LEARNING\\n6:\\nif iteration % K == 0 then\\n7:\\nfor m in 1 . . . Nquery do\\n8:\\nSample pair of segments (A, B)\\n9:\\nˆy1:M ← query all M users in crowd\\n10:\\nend for\\n11:\\nif crowd method == “SML” then\\n12:\\nCompute SML ranking vector ˆvi\\n13:\\nCompute ˆySML according to (2)\\n14:\\nˆy ← ˆySML\\n15:\\nelse Compute MAJ label estimate ˆyMAJ\\n16:\\nˆy ← ˆyMAJ\\n17:\\nend if\\n18:\\nD ← D ∪ {(A, B, ˆy)}\\n19:\\nfor each gradient step do\\n20:\\nSample batch {(A, B, ˆy)j}|D|\\nj=1 ∼ D\\n21:\\nOptimize LReward (4) with respect to ψ\\n22:\\nend for\\n23:\\nend if\\n24:\\n// POLICY LEARNING\\n25:\\nfor each timestep t do\\n26:\\nCollect st+1 by taking at ∼ π(at|st)\\n27:\\nB ← B ∪ {(st, at, st+1, brψ(st))}\\n28:\\nend for\\n29:\\nfor each gradient step do\\n30:\\nSample random minibatch {(τj)}B\\nj=1 ∼ B\\n31:\\nOptimize RL objective with respect to ϕ\\n32:\\nend for\\n33:\\nReset B ← ∅ if on-policy RL algorithm is used\\n34: end for\\nwhile the second, the Spectral Meta Learner (SML) [Parisi et\\nal., 2014] is the method we recommend for use with Crowd-\\nPrefRL. Note that our Crowd-PrefRL framework generalizes\\nthe crowdsourcing approach in [Brown et al., 2019], in which\\nmajority voting is used to merge preference labels.\\nMajority Vote (Baseline).\\nGiven the current problem for-\\nmulation, where we do not know any additional information\\nabout the crowd, perhaps the simplest estimate ˆy is the major-\\nity vote (MAJ), which labels each segment pair xk ∈ S with\\nthe majority preference from the crowd, denoted as ˆyMAJ.\\nSpectral Meta-Learner.\\nHowever, it is indeed possible to\\nestimate a crowd label that often outperforms the majority\\nvote in the fully unsupervised case studied here. Under the\\nassumption that users in a crowd make independent errors,\\n[Parisi et al., 2014] derived the Spectral Meta-Learner (SML):\\nˆyk = sign\\n M\\nX\\ni=1\\nfi(xk) ∗ ˆvi\\n!\\n,\\n(2)\\nwhere ˆvi is the lead eigenvector of the covariance matrix ob-\\ntained from the predictions of a set of binary classifiers on a\\ngiven query; in our case, these correspond to each user’s bi-\\nnary preference on a given pairwise comparison query. As\\nshown in [Parisi et al., 2014], crowd labels derived via the\\nSML are typically more accurate than labels from any indi-\\nvidual crowd member or from a majority vote, as unlike other\\nmethods such as majority vote, the SML approximates a max-\\nimum likelihood estimate of the labels. In addition, [Parisi et\\nal., 2014] showed that ˆvi can rank the users by their estimated\\nerror rates, which approximate the ground truth error rates.\\nWe denote labels estimated by SML as ˆySML.\\n[Parisi et al., 2014] also analyze the performance of SML\\nin the presence of a cartel, a small group of users who at-\\ntempt to steer the overall ensemble solution away from the\\n(unknown) ground truth, and proved that (1) SML is robust to\\nthe presence of cartels, and that (2) the weights of the eigen-\\nvector ˆvi can be used to identify the presence of cartels in\\nthe crowd. Here, we re-define the notion of a cartel as a mi-\\nnority group who provides feedback that consistently differs\\nfrom the majority, for example users who provide feedback\\ntowards goals that are different than the goals of the majority.\\n3.3\\nCrowdsourced Preference Learning for RL\\nCrowd-PrefRL leverages the crowd-aggregated preference la-\\nbels ˆy = ˆySML for all instances in S. We denote via D the\\ndataset of all pairs {(x, y)}, where x ∈ S and y is the corre-\\nsponding crowd-aggregated label in ˆy. To learn useful behav-\\niors given these aggregated labels, we assume that the major-\\nity of users (though not necessarily all users) have a shared\\nunderstanding of desired behavior, which can be quantified\\nin terms of an underlying reward function. Given the crowd-\\naggregated labels, our objective is to learn the reward func-\\ntion, and ultimately a policy, that preserves the underlying\\nuser preferences [Christiano et al., 2017]. This is done in two\\nsteps: (1) a reward function learning step, where we optimize\\nthe learned reward via supervised learning from the prefer-\\nence labels ˆy over segment pairs generated by the policy π,\\nand (2) a policy learning step, where the agent performs roll-\\nouts in the environment and optimizes its policy against the\\ncurrent reward function estimate. Segment pairs for prefer-\\nence queries are selected according to an entropy-based mea-\\nsure from an ensemble of reward predictors, similarly to [Lee\\net al., 2021]. These steps are then repeated for a set number of\\nenvironment steps. Algorithm 1 shows the algorithmic steps\\nneeded for preference-based RL with learned rewards from\\ncrowds, and is analogous to [Lee et al., 2021] with red text\\ndenoting the modifications needed to crowdsource the prefer-\\nence labels rather than obtaining them from a single human.\\nSimilarly to [Lee et al., 2021; Christiano et al., 2017], we\\nlearn a reward model brψ, modeled as a neural network with\\nparameters ψ, that predicts the user preferences as follows:\\nPψ[A ≻ B] =\\nexp P\\nt brψ(sA\\nt , aA\\nt )\\nP\\ni∈{A,B} exp P\\nt brψ(si\\nt, ai\\nt).\\n(3)\\nThe reward function brψ is then updated by minimizing the\\nfollowing cross-entropy loss:\\nLReward = −\\nE\\n(A,B,y)∼D\\nh\\n1[y=−1] log Pψ[B ≻ A]+\\n1[y=1] log Pψ[A ≻ B]\\ni\\n,\\n(4)\\nwhere 1[·] denotes an indicator variable. Given the learned re-\\nward function brψ, a policy π can be learned using any RL al-\\ngorithm. In this work, we use PrefPPO [Lee et al., 2021], in-\\nstantiating Crowd-PrefRL as Crowd-PrefPPO. PrefPPO uses\\nProximal Policy Optimization (PPO), a state-of-the-art on-\\npolicy RL algorithm [Schulman et al., 2017].\\nAs recom-\\nmended in [Lee et al., 2021], we use on-policy methods to\\nreduce the effects of non-stationarity induced by learning a\\nreward function and policy simultaneously.\\n4\\nExperiments\\n4.1\\nExperiment Setup\\nWe conduct experiments to evaluate the proposed approach\\nto crowdsourcing human feedback for preference learning.\\nOur experiments conduct RL from human preferences us-\\ning the PrefPPO implementation in [Lee et al., 2021]. These\\nexperiments use the Walker-walk, Quadruped-walk,\\nand Cheetah-run environments from the DMControl\\nsuite [Tassa et al., 2018], as two of these environments\\n(Walker-walk and Quadruped-walk) were previously\\nused in [Lee et al., 2021]. For each environment, we report\\nresults for Crowd-PrefPPO (SML preference estimation) and\\nthe following comparisons: Crowd-PrefPPO-MAJ (in which\\nthe majority vote is used instead of the SML estimate) and\\nCrowd-PrefPPO-Oracle (in which the preference is given to\\nthe segment with the highest ground-truth environment re-\\nward); in addition, we compare Crowd-PrefPPO’s estimated\\npreference labels to those from the best member of each\\ncrowd. We perform 10 total runs of each algorithm compari-\\nson in each environment for 4M environment steps. Methods\\nare compared via the mean and standard error over 6 runs,\\nwhere the top 2 and bottom 2 runs are omitted (measured by\\nepisode return at 4M environment steps) to reduce the impact\\nof outliers (similar to the InterQuartile Mean, IQM [Agarwal\\net al., 2021]). All remaining hyper-parameters for our method\\ncan be found in the Appendix, and notably, are the same as\\nreported in [Lee et al., 2021].\\n4.2\\nResults\\nWe first study the effect of crowd size on the performance\\nof crowdsourced preference learning by simulating 100 dif-\\nferent random crowd configurations at each of three differ-\\nent crowd sizes (M ∈ {7, 11, 15}) according to the Stochas-\\ntic Preference Model in Equation (1) for the Walker-Walk\\nenvironment; see Methods for how these diverse crowds are\\nsimulated. Figure 2 depicts the difference in errors between\\nthe MAJ and SML preference estimates versus the diversity\\nin users’ error rates with respect to the ground-truth reward\\n(quantified via the standard deviation of user error across the\\ncrowd) for each simulated crowd size. To calculate these er-\\nrors for each simulated crowd configuration, we first perform\\nFigure 2: Difference in MAJ and SML preference prediction error rates for different levels of variability (standard deviation) in user error\\nrates. Values are calculated across 100 randomly-sampled crowds of 7, 11 and 15 simulated users for the Walker-walk environment. The\\nhorizontal dashed line at y = 0 indicates where the MAJ and SML error rates are the same; points above y = 0 indicate that the SML labels\\nhave lower error than the MAJ labels. We see a strong positive correlation, with increasing standard deviations (indicating a more diverse\\ncrowd) leading to improved SML label accuracy (lower error rate) compared to MAJ labels. In addition, we see a similar trend across all\\ncrowd sizes, suggesting that our method is robust across different sizes of crowds. Red dots indicate where SML outperforms the best crowd\\nmember, while blue dots indicate where the best crowd member outperforms SML.\\none full training run of Crowd-PrefPPO using Oracle prefer-\\nence labels to generate the full set of segment pairs needed for\\nreward labeling. Then, we use these segment pairs and their\\nground-truth labels to construct the error rates for all users in\\nthe crowd and the error rates corresponding to the MAJ and\\nSML labels. We note that results were similar for the other\\nenvironments tested, and therefore show results for only the\\nWalker-walk environment due to space constraints. There\\nare several results here that are worth discussing. First, we see\\na strong positive correlation between the error rates of users\\nin the crowd and the performance of SML compared to MAJ.\\nIn particular, as the user error increases (indicating a more\\ndiverse crowd), the better the SML labels perform compared\\nto the MAJ labels. This indicates that the SML is effectively\\nfiltering out inconsistent preference feedback from users who\\nhave higher error rates and aggregating crowd decisions more\\nconsistently compared to MAJ. Second, we see that SML out-\\nperforms MAJ in nearly all cases (≈ 90% of points above\\ny = 0), suggesting that SML captures the crowd preferences\\nmore accurately than MAJ. Third, we see that the SML la-\\nbels have lower errors than the best crowd member’s labels\\nin nearly all cases (red dots). As the crowd size increases,\\nwe see fewer instances of SML performing worse than the\\nbest member of the crowd (blue dots), while simultaneously\\nhaving more instances where SML outperforms MAJ (dots\\nabove y = 0 dashed line). Finally, across all crowd sizes,\\nthe spread of the difference in MAJ errors and SML errors is\\nfairly consistent (most points fall between [0, 6]%), suggest-\\ning that SML labels can outperform MAJ labels with most\\ncrowd configurations.\\nWhile this improvement may seem\\nsmall, it occurs during every reward learning iteration; we\\nbelieve that this leads to compounding benefits during reward\\nlearning, and consequently, during agent policy learning.\\nHaving studied how crowd configuration variability af-\\nfects the respective prediction errors of the SML and MAJ\\ncrowd preference estimators, we next conduct experiments\\nwith Crowd-PrefPPO using the crowdsourced preference la-\\nbels; in these experiments, we aim to determine if the im-\\nproved label error from SML observed in Figure 2 impacts\\nagent performance in a meaningful way. For this analysis,\\nwe select a fixed crowd size of M = 11 users and sample\\ntwo different crowd configurations: (1) Figure 3a studies a\\ncrowd for which SML outperforms MAJ by approx. 3% in\\nprediction error (following the analysis in Figure 2), and (2)\\nFigure 3b studies a crowd for which SML performs similarly\\nto MAJ in prediction error (similarly from the analysis in Fig-\\nure 2). In addition, in the bottom row of Figure 3, we plot the\\nerror rates of the MAJ and SML label predictions relative to\\nthe ground-truth labels during each crowd feedback iteration\\nfor all three tested environments (200 queries per iteration for\\nQuadruped-walk and 100 otherwise; see the Appendix\\nfor more details).\\nIn Figure 3a,\\nwe see that Crowd-PrefPPO outper-\\nforms Crowd-PrefPPO-MAJ in terms of trajectory return\\nfor all tested environments, with Crowd-PrefPPO achiev-\\ning close to Oracle performance for Walker-walk and\\nQuadruped-walk. In addition, the SML label errors are\\nlower than the MAJ label errors at each feedback iteration in\\nmost cases for each environment. The lower SML label er-\\nrors imply that there is a compounding benefit to using SML\\nlabels rather than MAJ labels for reward learning; in early\\nfeedback iterations, the learned reward function has lower\\nerror and thus is closer to the Oracle reward function com-\\npared to MAJ. This improves overall agent training, as (1)\\nthe agent receives more accurate rewards for its actions, en-\\nabling it to learn better behaviors earlier in the training loop,\\nand (2) future crowd feedback iterations receive “better” seg-\\nment pairs for the crowd to label, yielding more useful in-\\nformation about the reward function. We speculate that this\\nis why Crowd-PrefPPO’s performance compares fairly well\\nwith that of Crowd-PrefPPO-Oracle. We see a similar, but\\nweaker, trend in Figure 3b, where SML is expected to yield\\na similar label accuracy to MAJ for the given crowd config-\\nuration. There is generally less spread in episode returns be-\\nFigure 3: (Top row) Comparison of Crowd-PrefPPO training curves with SML, MAJ and Oracle labeling across two different crowd config-\\nurations: [(a), left] a crowd for which SML is expected to outperform MAJ, and [(b), right] a crowd for which SML is expected to perform\\nsimilarly to MAJ across the three different environments (Walker-walk, Quadruped-walk and Cheetah-run). (Bottom row) Com-\\nparison of MAJ and SML label prediction errors at each feedback iteration. Each plot shows the mean ± standard error of 6 out of 10 runs\\n(the top and bottom 2 runs are omitted to reduce the effect of outliers, as detailed in the Experiment Setup).\\nFigure 4: Plot of ground-truth error rate for each user in the crowd\\nvs. the SML weight vector vi for the same two SML configurations\\nshown in Figure 3 in the Walker-walk environment. Across both\\nconditions, we see a significant correlation between the weights gen-\\nerated by the SML and individual crowd member performance mea-\\nsured by the users’ (unknown) error rates, suggesting that users can\\nbe ranked accurately in the absence of ground-truth error informa-\\ntion about the crowd.\\ntween SML and MAJ for the three tested environments (top\\nrow of Figure 3b), and we see that the crowd feedback errors\\nare closer between the methods during all crowd feedback it-\\nerations (bottom row of Figure 3b), indicating only minimal\\nbenefits of SML labels over MAJ labels. However, we do\\nsee that Crowd-PrefPPO can sometimes perform quite well\\noverall, for example in Quadruped-walk.\\nFurthermore, we analyze the crowd member ranking de-\\nrived by SML for the same two conditions shown in Figure\\n3 and the Walker-walk environment (results were similar\\nfor all environments, thus we report only one due to space\\nconstraints). While Crowd-PrefPPO calculates the eigenvec-\\ntor vi during each feedback iteration, we here instead calcu-\\nlate vi using all crowdsourced preference labels for simplic-\\nity of visualization. Results are shown in Figure 4. We see\\nthat across both conditions, the SML-derived member rank-\\nings are significantly correlated to the individual members’\\nFigure 5: Plot of ground-truth error rate vs. the SML weight vector\\nvi for a crowd configuration where there are three distinct groups of\\nusers: users who provide feedback according to (1) the ground-truth\\nreward r (blue), (2) at random (black), and (3) the opposite of the\\nground-truth reward, −r (red). We see distinct groupings of users\\nby their feedback category and a strong correlation between their\\n(unknown) error rates and the SML weight vector vi, suggesting\\nthat minority groups of users can be accurately identified.\\nerror rates (i.e., higher SML weight values indicate lower er-\\nror rates), suggesting that even when SML performs similarly\\nto MAJ in label error (Figure 4b), SML can still identify the\\nbest members in the crowd. This showcases additional util-\\nity of SML aggregation over MAJ aggregation even in the\\nsituation where SML provides no benefit in label accuracy\\ncompared to MAJ.\\nFinally, we test the ability of SML to detect the presence\\nof minority group feedback based purely on the preference\\nlabels. We follow the same analysis used in Figure 4, ex-\\ncept that we simulate a crowd of 15 users according to three\\ngroups: users who provide noisy preference feedback accord-\\ning to (1) the ground-truth reward r (9 users), (2) at random (3\\nusers), and (3) −r, the opposite of the ground-truth (3 users).\\nResults are shown in Figure 5. As expected, we see that MAJ\\nerror rates are significantly higher compared to SML rates,\\nsince MAJ is more sensitive to the presence of noisy labels.\\nIn addition, we see distinct clusters of users based on their\\nfeedback category, suggesting that the SML weights vi can be\\nused to identify the presence of minority groups of users, as\\nwell as the identities of the minority group members. These\\nresults suggest that Crowd-PrefRL could be used to cluster\\nusers via their objectives and to learn reward functions and\\ncorresponding behaviors specific to each user cluster. Such\\nfunctionality could enable a behavior-based characterization\\nof the objective associated with each user cluster.\\n5\\nDiscussion\\nThis study aimed to determine how to (1) effectively com-\\nbine preference feedback from crowds of unknown expertise\\nand reliability in the absence of ground-truth information,\\n(2) identify the “best” users in a given crowd in an unsu-\\npervised manner, (3) learn a crowdsourced reward function\\nthat is more accurate than the reward learned from any in-\\ndividual user, and (4) identify minority groups based purely\\non their preference feedback. We propose the Crowd-PrefRL\\nframework and instantiate it as Crowd-PrefPPO. Our evalu-\\nation of Crowd-PrefPPO indicates that we can use methods\\nderived from unsupervised ensemble learning (SML, [Parisi\\net al., 2014]) to effectively aggregate user preference feed-\\nback across diverse crowds in the absence of any ground-\\ntruth information about the users. Furthermore, our evalua-\\ntion suggests that Crowd-PrefPPO can identify which users in\\na crowd are likely most reliable and that the reward function\\nlearned from crowd-aggregated labels is more accurate than\\nthe reward derived from any single user’s feedback (even the\\nbest user!). Finally, we show the possibility of detecting clus-\\nters of minority feedback in an unsupervised manner, poten-\\ntially enabling a deeper understanding of model behavior (e.g.\\ndetecting the presence of multiple user viewpoints, character-\\nizing the associated objectives, and mitigating effects of mali-\\ncious user data [Siththaranjan, Laidlaw, and Hadfield-Menell,\\n2023; Ouyang et al., 2022]). To the best of our knowledge,\\nthis is the first demonstration of (1) learning crowd-level pref-\\nerence labels for preference-based RL via unsupervised en-\\nsemble learning methods in the online RL setting and (2)\\nidentification of minority groups in the crowd based purely\\non the crowd’s preference feedback.\\nPrevious work has studied crowdsourcing preference la-\\nbels for reward learning in the offline RL case [Zhang and\\nKashima, 2023], as opposed to the online RL case we study\\nhere. In the offline case, a dataset of preference pairs is al-\\nready provided a priori, and the goal is to learn a reward\\nfunction from this static dataset. This limits the expressive-\\nness of the learned reward function to the behaviors contained\\nin the dataset, whereas in our work, the preference pairs for\\ncrowd labeling are sampled throughout agent training, and\\nby extension the learned reward function continuously im-\\nproves with better agent performance and better environment\\nexploration. In addition, the method in [Zhang and Kashima,\\n2023] requires training a separate model (e.g. fully connected\\nnetwork) to infer crowd labels and member reliabilities, re-\\nquiring additional model training and tuning and increased\\ndata requirements (50K preference pairs from 2,500 crowd-\\nworkers). In contrast, our approach to inferring crowd labels\\nby the SML is learning and optimization-free: we need no\\nhyperparameters to specify, no model to train, and no opti-\\nmization procedure to learn a crowd label. Thus, we believe\\nit is more suitable in the online RL case where the number\\nof labeled preference pairs is small compared to the offline\\nRL case due to practical constraints (agent is training during\\nhuman feedback collection). Our work enables crowds to up-\\ndate the reward function throughout agent training using pref-\\nerence feedback on just a handful of trials and furthermore,\\nallows for the detection of minority feedback groups, which\\nhas important modeling applications in various domains.\\nWe showed that SML labels outperform MAJ labels in\\nnearly all cases across different crowd sizes (Figure 2) and\\ndifferent tasks (Figure 3). Calculating SML weights is com-\\nputationally very fast, only requiring a few lines of code in\\nmodern programming languages (e.g. Python). Given the in-\\ncreasing popularity of crowdsourcing platforms for data col-\\nlection in human-interactive RL studies (e.g. Amazon Me-\\nchanical Turk), these results indicate that simple code modi-\\nfications can result in significant performance gains. For ex-\\nample, [Brown et al., 2019] used a crowdsourcing platform\\nfor their human subjects data analysis and estimated prefer-\\nence labels for segments by simply taking a majority vote\\nover multiple human users. It would be interesting to apply\\nCrowd-PrefRL to aggregate crowd preferences in such exist-\\ning studies to determine the extent to which Crowd-PrefRL\\nimproves over results from a majority vote or offline RL.\\nWe believe that Crowd-PrefRL could be combined with re-\\ncent advances that make preference-based RL methods more\\nsample-efficient [Lee, Smith, and Abbeel, 2021; Park et al.,\\n2022].\\nImprovements to sample efficiency are made pri-\\nmarily at the algorithmic or data level, as opposed to mod-\\nifying the labels used to learn the reward.\\nFor example,\\nPEBBLE [Lee, Smith, and Abbeel, 2021] combines unsu-\\npervised pre-training with off-policy re-labeling of past data\\nto achieve improved sample efficiency compared to Pref-\\nPPO. SURF [Park et al., 2022] uses semi-supervised learn-\\ning methods that combine data augmentation approaches with\\nconfidence-based pseudo-labels of unlabeled experiences to\\ntrain the preference predictor.\\nCombining Crowd-PrefRL\\nwith these orthogonal advancements could potentially lead to\\nfurther improvements in RL agent training performance.\\nSeveral downsides to our approach are important to dis-\\ncuss. First, each user in the crowd must provide a preference\\nlabel for each preference query. This results in significant hu-\\nman labeling cost if implemented in real-world studies. Our\\nresults indicate that Crowd-PrefPPO can work well in fairly\\nsmall crowds with relatively small numbers of labels (100\\nqueries per user per reward learning iteration). Future work\\nwill focus on the possibility of using Crowd-PrefRL when\\nuser feedback is sparse (i.e., not every user provides a pref-\\nerence for every segment pair). In addition, this study uses\\nonly simulated human feedback and does not include human-\\nsubjects data collection. Rather, we simulate human feed-\\nback via the Stochastic Preference Model, which allows for a\\nthorough and systematic evaluation of Crowd-PrefRL and an\\nanalysis of its properties in a highly controlled setting. Future\\nwork will evaluate this method in a human subjects study.\\nAppendix\\nExperiment Setup Hyper-parameters\\nOur\\nexperiments\\nuse\\nthe\\nWalker-walk,\\nQuadruped-walk,\\nand Cheetah-run environments\\nfrom the DMControl suite [Tassa et al., 2018], as two of these\\nenvironments (Walker-walk and Quadruped-walk)\\nwere previously used in [Lee et al., 2021].\\nAll hyper-\\nparameters for our method can be found in Table 1, and\\nnotably, are the same as reported in [Lee et al., 2021].\\nHyperparameter\\nValue\\nGAE parameter λ\\n0.9 (Quadruped), 0.92 (otherwise)\\nHidden units per each layer\\n256\\nSegment of length\\n50\\n# of layers\\n3\\nLearning rate\\n0.00005\\nBatch Size\\n128 (Quadruped), 64 (otherwise)\\nDiscount ¯γ\\n.99\\nFrequency of feedback\\n32000\\n# of envs per worker\\n16 (Quadruped), 32 (otherwise)\\nPPO clip range\\n0.4\\nEntropy bonus\\n0.0\\n# of timesteps per rollout\\n500\\nMaximum budget\\n2000 (Quadruped), 1000 (otherwise)\\n# of feedbacks per session\\n200 (Quadruped), 100 (otherwise)\\nTable 1: Hyper-parameters of the Crowd-PrefPPO algorithm for all\\nenvironments.\\nEthical Statement\\nIn any real-world crowdsourcing application, it will be impor-\\ntant to protect user privacy such that only anonymized user\\ndata is stored and so that user data is only stored given user\\nconsent. Furthermore, systems trained via crowdsourced data\\nmust be able to recognize and account for differing user per-\\nspectives. Critically, in applications from household robotics\\nto autonomous driving to LLM-based assistants, users must\\nnot be forced into options optimized for other users; rather,\\nsystems should utilize crowd-aggregated information as a\\nbaseline from which to accelerate and improve downstream\\npersonalization to specific individual users. Meanwhile, it\\nis crucial to identify and reject malicious user data to avoid\\nbiasing models toward toxic or other undesirable behaviors.\\nWe believe that Crowd-PrefRL can help to facilitate such sys-\\ntem capabilities by identifying minority viewpoints, not only\\nlearning a policy consistent with the majority viewpoint, but\\nalso enabling developers to model behaviors corresponding\\nto distinct user clusters to improve overall modeling. Finally,\\nwhen analyzing different user objectives, it will be important\\nto consider the potential implications of empowering a few\\nindividuals to decide which user objectives are worthwhile,\\ntoxic, etc.\\nAcknowledgments\\nThis project was sponsored by the DEVCOM Army Research\\nLaboratory. The views and conclusions contained in this doc-\\nument are those of the authors and should not be interpreted\\nas representing the official policies, either expressed or im-\\nplied, of the U.S. Government. The U.S. Government is au-\\nthorized to reproduce and distribute reprints for Government\\npurposes notwithstanding any copyright notation herein.\\nReferences\\n[Agarwal et al., 2021] Agarwal, R.; Schwarzer, M.; Castro,\\nP. S.; Courville, A. C.; and Bellemare, M. 2021. Deep re-\\ninforcement learning at the edge of the statistical precipice.\\nIn Ranzato, M.; Beygelzimer, A.; Dauphin, Y.; Liang, P.;\\nand Vaughan, J. W., eds., Advances in Neural Information\\nProcessing Systems, volume 34, 29304–29320. Curran As-\\nsociates, Inc.\\n[Babes et al., 2011] Babes, M.; Marivate, V.; Littman, M.;\\nand Subramanian, K. 2011. Apprenticeship learning about\\nmultiple intentions. In Getoor, L., and Scheffer, T., eds.,\\nProceedings of the 28th International Conference on Ma-\\nchine Learning (ICML-11), ICML ’11, 897–904.\\nNew\\nYork, NY, USA: ACM.\\n[Beliaev et al., 2022] Beliaev, M.; Shih, A.; Ermon, S.;\\nSadigh, D.; and Pedarsani, R. 2022. Imitation learning by\\nestimating expertise of demonstrators. In Chaudhuri, K.;\\nJegelka, S.; Song, L.; Szepesvari, C.; Niu, G.; and Sabato,\\nS., eds., Proceedings of the 39th International Conference\\non Machine Learning, volume 162 of Proceedings of Ma-\\nchine Learning Research, 1732–1748. PMLR.\\n[Brown et al., 2019] Brown, D.; Goo, W.; Nagarajan, P.; and\\nNiekum, S.\\n2019.\\nExtrapolating beyond suboptimal\\ndemonstrations via inverse reinforcement learning from\\nobservations. In Proceedings of the International Confer-\\nence on Machine Learning, 783–792.\\n[Casper et al., 2023] Casper, S.; Davies, X.; Shi, C.; Gilbert,\\nT. K.; Scheurer, J.; Rando, J.; Freedman, R.; Korbak, T.;\\nLindner, D.; Freire, P.; et al. 2023. Open problems and fun-\\ndamental limitations of reinforcement learning from hu-\\nman feedback.\\nTransactions on Machine Learning Re-\\nsearch.\\n[Chen et al., 2013] Chen,\\nX.;\\nBennett,\\nP. N.;\\nCollins-\\nThompson, K.; and Horvitz, E. 2013. Pairwise ranking\\naggregation in a crowdsourced setting. In Proceedings of\\nthe Sixth ACM International Conference on Web Search\\nand Data Mining, WSDM ’13, 193–202. New York, NY,\\nUSA: Association for Computing Machinery.\\n[Christiano et al., 2017] Christiano, P. F.; Leike, J.; Brown,\\nT.; Martic, M.; Legg, S.; and Amodei, D. 2017. Deep re-\\ninforcement learning from human preferences. Advances\\nin neural information processing systems 30.\\n[Dimitrakakis and Rothkopf, 2012] Dimitrakakis,\\nC.,\\nand\\nRothkopf, C. A. 2012. Bayesian multitask inverse rein-\\nforcement learning. In Sanner, S., and Hutter, M., eds.,\\nRecent Advances in Reinforcement Learning, 273–284.\\nBerlin, Heidelberg: Springer Berlin Heidelberg.\\n[Hausman et al., 2017] Hausman, K.; Chebotar, Y.; Schaal,\\nS.; Sukhatme, G.; and Lim, J. J. 2017. Multi-modal imita-\\ntion learning from unstructured demonstrations using gen-\\nerative adversarial nets. In Proceedings of the 31st Interna-\\ntional Conference on Neural Information Processing Sys-\\ntems, NIPS’17, 1235–1245. Red Hook, NY, USA: Curran\\nAssociates Inc.\\n[Lee et al., 2021] Lee, K.; Smith, L.; Dragan, A.; and\\nAbbeel, P. 2021. B-pref: Benchmarking preference-based\\nreinforcement learning. In Thirty-fifth Conference on Neu-\\nral Information Processing Systems Datasets and Bench-\\nmarks Track (Round 1).\\n[Lee, Smith, and Abbeel, 2021] Lee, K.; Smith, L. M.; and\\nAbbeel, P. 2021. PEBBLE: Feedback-efficient interac-\\ntive reinforcement learning via relabeling experience and\\nunsupervised pre-training. In International Conference on\\nMachine Learning, 6152–6163.\\n[Levine et al., 2020] Levine, S.; Kumar, A.; Tucker, G.; and\\nFu, J. 2020. Offline reinforcement learning: Tutorial, re-\\nview, and perspectives on open problems. Computing Re-\\nsearch Repository (CoRR) abs/2005.01643.\\n[Li et al., 2016] Li, G.; Wang, J.; Zheng, Y.; and Franklin,\\nM. J. 2016. Crowdsourced data management: A survey.\\nIEEE Transactions on Knowledge and Data Engineering\\n28(9):2296–2319.\\n[Ouyang et al., 2022] Ouyang,\\nL.;\\nWu,\\nJ.;\\nJiang,\\nX.;\\nAlmeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.;\\nAgarwal, S.; Slama, K.; Ray, A.; Schulman, J.; Hilton,\\nJ.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welin-\\nder, P.; Christiano, P. F.; Leike, J.; and Lowe, R. 2022.\\nTraining language models to follow instructions with hu-\\nman feedback. In Koyejo, S.; Mohamed, S.; Agarwal, A.;\\nBelgrave, D.; Cho, K.; and Oh, A., eds., Advances in Neu-\\nral Information Processing Systems, volume 35, 27730–\\n27744. Curran Associates, Inc.\\n[Parisi et al., 2014] Parisi, F.; Strino, F.; Nadler, B.; and\\nKluger, Y. 2014. Ranking and combining multiple pre-\\ndictors without labeled data. Proceedings of the National\\nAcademy of Sciences 111(4):1253–1258.\\n[Park et al., 2022] Park, J.; Seo, Y.; Shin, J.; Lee, H.; Abbeel,\\nP.; and Lee, K.\\n2022.\\nSURF: Semi-supervised reward\\nlearning with data augmentation for feedback-efficient\\npreference-based reinforcement learning. In International\\nConference on Learning Representations.\\n[Schulman et al., 2017] Schulman, J.; Wolski, F.; Dhariwal,\\nP.; Radford, A.; and Klimov, O. 2017. Proximal policy op-\\ntimization algorithms. arXiv preprint arXiv:1707.06347.\\n[Siththaranjan, Laidlaw, and Hadfield-Menell, 2023]\\nSiththaranjan, A.; Laidlaw, C.; and Hadfield-Menell, D.\\n2023. Distributional preference learning: Understanding\\nand accounting for hidden context in RLHF.\\narXiv\\npreprint arXiv:2312.08358.\\n[Sutton and Barto, 2018] Sutton, R. S., and Barto, A. G.\\n2018.\\nReinforcement learning: An introduction.\\nMIT\\npress.\\n[Tassa et al., 2018] Tassa, Y.; Doron, Y.; Muldal, A.; Erez,\\nT.; Li, Y.; de Las Casas, D.; Budden, D.; Abdolmaleki, A.;\\nMerel, J.; Lefrancq, A.; Lillicrap, T. P.; and Riedmiller,\\nM. A. 2018. Deepmind control suite. Computing Research\\nRepository (CoRR) abs/1801.00690.\\n[Torne et al., 2023] Torne, M.; Balsells, M.; Wang, Z.;\\nDesai, S.;\\nChen, T.;\\nAgrawal, P.;\\nand Gupta, A.\\n2023. Breadcrumbs to the goal: Goal-conditioned explo-\\nration from human-in-the-loop feedback. arXiv preprint\\narXiv:2307.11049.\\n[Wilde et al., 2021] Wilde, N.; Biyik, E.; Sadigh, D.; and\\nSmith, S. L. 2021. Learning reward functions from scale\\nfeedback. In 5th Annual Conference on Robot Learning.\\n[Zhang and Kashima, 2023] Zhang, G., and Kashima, H.\\n2023.\\nBatch reinforcement learning from crowds.\\nIn\\nAmini, M.-R.; Canu, S.; Fischer, A.; Guns, T.; Kralj No-\\nvak, P.; and Tsoumakas, G., eds., Machine Learning\\nand Knowledge Discovery in Databases, 38–51. Cham:\\nSpringer Nature Switzerland.\\n'},\n",
       " {'abstract': \"This study introduces RELIANCE, a unique ensemble learning system tailored for reliable information and fake news credibility evaluation. Composed of five diverse base models, including Support Vector Machine (SVM), naive Bayes, logistic regression, random forest, and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs an innovative approach to fuse their strengths, harnessing the collective intelligence of the ensemble for enhanced accuracy. Experiments showcase RELIANCE's superiority over individual models, demonstrating its efficacy in distinguishing credible from non-credible information sources. RELIANCE surpasses baseline models in information and news credibility assessment, establishing itself as an effective solution for evaluating the reliability of information sources.\",\n",
       "  'introduction': 'In the era of information proliferation, discerning the credibility of news content poses an ever-growing challenge. This paper introduces RELIANCE, a pioneering ensemble learning system designed for robust information and fake news credibility evaluation. Comprising five diverse base models, including Support Vector Machine (SVM), naïve Bayes, logistic regression, random forest, and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs an innovative approach to integrate their strengths, harnessing the collective intelligence of the ensemble for enhanced accuracy. Experiments demonstrate the superiority of RELIANCE over individual models, indicating its efficacy in distinguishing between credible and non-credible information sources. RELIANCE, also surpasses baseline models in information and news credibility assessment, establishing itself as an effective solution for evaluating the reliability of information sources.',\n",
       "  'literature_review': 'The rapid dissemination of information through digital platforms has directed in an era where the credibility of news content is frequently challenged. In response to the proliferation of misinformation and fake news, researchers have proposed different approaches to evaluate the credibility of news documents. Throughout the research history of our work, this field has been recognized by various names, including rumor detection, fake news detection, and reality detection. Perusing the literature, one can categorize the previous studies into three categories: text-based methods, context-based methods, and hybrid methods. Table 1 provides an analytic perspective on related works in information and news credibility evaluation in a tabular form.',\n",
       "  'methodology': 'The objective of our investigation was to improve the automated evaluation of information and news credibility by employing ensemble learning. Ensemble learning, combining various methods, enhances predictive performance beyond that of individual methods. Consequently, we introduced five distinct base models, each operating at different processing levels, for integration into the ensemble learning model. Namely Support Vector Machine (SVM)-based, naïve Bayes-based, logistic regression-based, random forest-based, and Bidirectional Long Short-Term Memory Networks (BiLSTMs)-based models. To this end, we proposed RELIANCE (Reliable Ensemble Learning for Information and News Credibility Evaluation), that performs credibility evaluation in three phases; Phase 1 focuses on preprocessing the input news text documents to prepare them for the main process. Phase 2 performs feature engineering through embedding the input news text documents. Phase 3 is devoted to evaluate the credibility of news text documents.',\n",
       "  'results': \"To perform news credibility evaluation, we proposed RELIANCE as an ensemble learning model that combines five distinct classifiers as the base models including SVM-based, naïve Bayes-based, LR-based, random forest-based, and BiLSTMs-based models. To perform feature engineering, we employed Doc2Vec with an embedding size equal to 1,200, and the minimum counts of word equal to 1, and conducted the training for 50 epochs. Afterwards, at first, the base models should perform credibility evaluation on the input text documents. Each of the base models adheres to specific settings for generating predictions. Table III encompasses the applied parameter settings in proposed BiLSTM model. Logistic Regression (LR): For the LR-based model, the maximum number of iterations for the solver (lbfgs) to converge is set at 1000. Moreover, the L1 regularization is also used to reduce model generalization error. SVM: For the SVM-based model, the chosen kernel is 'rbf' (Radial Basis Function). Random Forest: The Random Forest-based model is configured with the parameter n_estimators set to 100. Naïve Bayes: We used the Multinomial Naive Bayes algorithm, which is commonly used for text classification.\",\n",
       "  'conclusion': 'In the current era of information overload, accurately assessing the credibility of news sources is crucial for informed decision-making and effective crisis management. To address this challenge, we propose RELIANCE (Reliable Ensemble Learning for Information and News Credibility Evaluation), an ensemble learning approach that combines the strengths of five individual models for news credibility evaluation. The base models include Support Vector Machines, naïve Bayes classifiers, logistic regression models, random forests, and Bidirectional Long Short-Term Memory Networks. These models are individually trained to extract relevant features from news documents and classify their credibility. To further enhance the overall accuracy, RELIANCE employs a multi-layer perceptron as a meta-model, which integrates the predictions of each base model (using stacking) and produces a more refined credibility assessment. Comparative experiments with baseline models demonstrate that RELIANCE significantly outperforms existing algorithms in evaluating the credibility of news documents. It provides a robust framework for identifying trustworthy news sources, offering real-world applications that empower users, journalists, and fact-checkers with a resilient tool against misinformation in the digital era.',\n",
       "  'title': 'RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation',\n",
       "  'author': 'Majid Ramezani, Hamed Mohammad-Shahi, Mahshid Daliry, Soroor Rahmani, Amir-Hosein Asghari',\n",
       "  'textdata': 'XXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE \\n \\nRELIANCE: Reliable Ensemble Learning for \\nInformation and News Credibility Evaluation \\nMajid Ramezani  \\nFaculty of Computer Science and \\nInformation Technology, Institute \\nfor Advanced Studies in Basic \\nSciences (IASBS), \\n Zanjan, Iran. \\nramezani@iasbs.ac.ir \\nORCID: 0000-0003-0886-7023 \\nHamed Mohammad-Shahi \\nFaculty of Computer Science and \\nInformation Technology, Institute \\nfor Advanced Studies in Basic \\nSciences (IASBS), \\n Zanjan, Iran. \\nhamedshahi@iasbs.ac.ir \\nORCID: 0009-0008-7405-2192  \\nMahshid Daliry \\nFaculty of Computer Science and \\nInformation Technology, Institute \\nfor Advanced Studies in Basic \\nSciences (IASBS), \\n Zanjan, Iran. \\nmahshiddaliry@iasbs.ac.ir \\nORCID: 0009-0006-0449-0950 \\nSoroor Rahmani \\nFaculty of Computer Science and \\nInformation Technology, Institute \\nfor Advanced Studies in Basic \\nSciences (IASBS), \\n Zanjan, Iran. \\nsoroorrahmani@iasbs.ac.ir \\nORCID: 0009-0000-7465-3766 \\nAmir-Hosein Asghari \\nFaculty of Computer Science and  \\nInformation Technology, Institute \\n for Advanced Studies in Basic \\n Sciences (IASBS), \\n Zanjan, Iran. \\namirasghari@iasbs.ac.ir \\nORCID: 0009-0003-1000-8233 \\nAbstract—In the era of information proliferation, discerning \\nthe credibility of news content poses an ever-growing challenge. \\nThis paper introduces RELIANCE, a pioneering ensemble \\nlearning system designed for robust information and fake news \\ncredibility evaluation. Comprising five diverse base models, \\nincluding Support Vector Machine (SVM), naïve Bayes, logistic \\nregression, random forest, and Bidirectional Long Short Term \\nMemory \\nNetworks \\n(BiLSTMs), \\nRELIANCE \\nemploys \\nan \\ninnovative approach to integrate their strengths, harnessing the \\ncollective intelligence of the ensemble for enhanced accuracy. \\nExperiments demonstrate the superiority of RELIANCE over \\nindividual models, indicating its efficacy in distinguishing between \\ncredible and non-credible information sources. RELIANCE, also \\nsurpasses baseline models in information and news credibility \\nassessment, establishing itself as an effective solution for \\nevaluating the reliability of information sources. \\nKeywords—news credibility evaluation, fake news detection, \\nensemble learning, \\nI. INTRODUCTION \\nIn the era of information abundance, discerning the \\nreliability of news documents has become a paramount \\nchallenge. The rapid dissemination of news through various \\nonline platforms has created a fertile ground for \\nmisinformation, disinformation, and fake news. As society \\nincreasingly relies on digital sources for information \\nconsumption, ensuring the credibility of news content \\nbecomes imperative.  \\nIn the face of global crises such as the COVID-19 \\npandemic, natural disasters like earthquakes, and geopolitical \\nconflicts including wars, the reliability assessment of news \\nbecomes not merely a scholarly pursuit but a critical \\nimperative for societal well-being. Accurate and timely \\ninformation plays an indispensable role in crisis management, \\npublic safety, and policy formulation. The spread of \\nmisinformation during such events can have profound and \\nfar-reaching consequences, ranging from public panic and \\nmisguided responses to compromised public health efforts. In \\nthe case of a pandemic like COVID-19, misinformation about \\nthe virus, its origins, or potential treatments can undermine \\npublic trust in health authorities and exacerbate the \\nchallenges of containment. Similarly, during natural disasters \\nor armed conflicts, misleading information can hinder \\neffective evacuation procedures, relief efforts, and diplomatic \\ninitiatives. Thus, the ability to distinguish between reliable \\nand unreliable news in these contexts is paramount, \\nunderscoring the crucial need for advanced and robust fake \\nnews detection system. This field of study is known by \\nvarious names, including fake news detection [1-2], rumor \\ndetection [3-4], misinformation detection [5-6], information \\ncredibility evaluation or assessment [7-8], trustworthiness \\nassessment, and more. Its aim is to assess the reliability or \\nauthenticity of information and news documents [9]. \\nThe proliferation of misinformation poses a threat to \\npublic discourse, decision-making processes, and societal \\ntrust in media. Traditional methods of news verification \\nstruggle to keep pace with the evolving landscape of \\ndeceptive tactics employed by malicious actors. In this \\ncontext, our paper introduces RELIANCE (Reliable \\nEnsemble Learning for Information and News Credibility \\nEvaluation), an innovative approach leveraging reliable \\nensemble learning for robust information and news \\ncredibility evaluation. It seeks to address these challenges by \\nharnessing the power of ensemble learning, combining the \\nstrengths of multiple base models to enhance the reliability \\nand accuracy of news credibility assessment. To do so we \\nhave proposed five base models for credibility evaluation, \\nincluding Support Vector Machine (SVM)-based, naïve \\nBayes-based, logistic regression-based, random forest-\\nXXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE \\n \\nbased, and Bidirectional Long Short Term Memory Networks \\n(BiLSTMs)-based models. Together, these independent base \\nmethods collaborate to minimize the generalization error in \\npredictions through ensemble learning. In essence, ensemble \\nlearning involves employing multiple diverse base models to \\npredict outcomes, surpassing the predictive capabilities of \\nindividual models [10]. Therefore, the authors proposed \\nRELIANCE which stands as a pioneering effort to combine \\nthe capabilities of individual classifiers, thereby creating a \\nmore resilient and trustworthy system for distinguishing \\nbetween credible and non-credible information and news \\ndocuments. \\nThe current study provides two significant contributions \\nto distinguishing the credibility and non-credibility of \\ninformation and news documents: introducing five distinct \\ndiverse methods for information news credibility evaluation, \\nand enhancing credibility evaluation accuracy through the \\nensemble learning of the aforementioned models as the base \\nmodels during ensemble learning. \\nThe structure of our study is as follows: the literature \\nreview comprehensively surveys existing research. The \\nmethods details our innovative approach, including five \\ndistinct credibility evaluators as well as ensembling them \\nthrough RELIANCE. Subsequently in results and discussion, \\nwe present and analyze the results obtained from our \\nexperiments, assessing the proposed model\\'s performance. \\nThis section also critically interprets the findings, \\nhighlighting the contributions and implications of our work. \\nFinally, the conclusion highlights the key insights, reinforces \\nthe significance of our proposed methodology. \\nII. LITERATURE REVIEW \\nThe rapid dissemination of information through digital \\nplatforms has directed in an era where the credibility of news \\ncontent is frequently challenged. In response to the \\nproliferation of misinformation and fake news, researchers \\nhave proposed different approaches to evaluate the credibility \\nof news documents. Throughout the research history of our \\nwork, this field has been recognized by various names, \\nincluding rumor detection, fake news detection, and reality \\ndetection. Perusing the literature, one can categorize the \\nprevious studies into three categories: text-based methods \\n[11-18], context-based methods [15, 19-23], and hybrid \\nmethods [24-28]. Table 1 provides an analytic perspective on \\nrelated works in information and news credibility evaluation \\nin a tabular form. \\nThe \\ntext-based \\nmethods \\nprimarily \\nrely \\non \\nmiscellaneous information that can be acquired from the text \\ndocuments. In a basic investigation outlined in [11], the \\nauthors introduced a method that combines lexical features, \\nword embeddings, and n-gram characteristics for identifying \\nthe stance in fake news. The researchers in [12] introduced a \\nnew computational method for automatically detecting fake \\nnews, relying on a novel text analysis approach centered \\naround the linguistic features provided by LIWC (Linguistic \\nInquiry and Word Count) [16]. In another investigation, the \\nauthors to perform news credibility evaluation have introduce \\na hybrid model based on Logistic Regression and n-gram \\nanalysis [13]. They have used Term Frequency-Inverted \\nDocument Frequency (TF-IDF) as feature extraction \\ntechnique. They have also suggested a Linear Support Vector \\nMachine (LSVM) model to perform news credibility \\nevaluation. The authors in [14] introduced a model that \\nintegrates several key features of text document (such as the \\ntext of news document, news source, etc.) for a more precise \\nand automated prediction. Guided by these features, they \\nproposed a model named CSI, comprising three modules: \\nCapture, Score, and Integrate. The initial module relies on \\nresponses and text, utilizing a Recurrent Neural Network to \\ncapture the temporal patterns of user activity on a given news \\ndocument. The second module understands news source \\ncharacteristics based on user behavior, and these are \\nintegrated with the third module to classify a news document \\nas either reliable or unreliable.  \\nThe context-based methods basically attempt to utilize \\nthose information that are provided by the context such as \\nuser-based features, or post-based features. The authors in \\n[15, \\n19] \\nhave \\nproposed \\nmodels \\nthat \\nuser-specific \\ncharacteristics are derived from individual profiles to assess \\ntheir attributes for the sake of content credibility evaluation. \\nMore over in several investigations to perform credibility \\nevaluation, features based on network structures are primarily \\nextracted through the development of precise detection \\nsystems [20-21]. This involves utilizing diffusion networks, \\nassociation networks, and propagation networks to assess \\ncredibility of text documents. Given the extensive integration \\nof social media, research also incorporates social media \\ninteractions in the detection of fake news. For instance, there \\nis a focus on early detection through social learning and user-\\nbased relationships [22-23]. \\nThe hybrid methods try to consider all of the available \\ninformation to evaluate the credibility of the text documents. \\nThe authors in [24] presented an innovative hybrid system for \\ndetecting fake news, which integrates linguistic and \\nknowledge-based methodologies, combining their respective \\nstrengths. Their hybrid approach employs two distinct feature \\nsets: linguistic features (such as title, word count, reading \\nease, lexical diversity, and sentiment), and a new set of \\nknowledge-based features called fact-verification features. \\nThe authors in [25] introduced a novel transformer-based \\nmodel named X-CapsNet, incorporating Convolutional \\nNeural Networks or CNN (CapsNet). X-CapsNet employs a \\nCapsNet with a dynamic routing algorithm and integrates a \\nsize-based classifier for distinguishing between short and \\nlong fake news statements. The detection of long fake news \\nstatements is facilitated by a Deep CNN, while short news \\nstatements are identified using a Multi-Layer Perceptron \\n(MLP). Considering the context of social networks the \\nauthors in [26] have presented a novel approach to identify \\nfake news by analyzing the temporal propagation tree. They \\ncontinuously extract relevant features from the constructed \\npropagation trees and employ a type of recurrent neural \\nnetwork (LSTM) to capture the temporal dynamics of these \\nfeatures. This enables them to discern the evolving pattern of \\nXXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE \\n \\nthe propagation tree over time, aiding in the estimation of the \\ncredibility of a news article. In their seminal study, the \\nauthors in [27] have explored the feasibility of unsupervised \\nfake news detection by considering the truths of news and \\nusers\\' credibility as latent random variables. Utilizing users\\' \\nengagements on social media, they have discerned their \\nopinions regarding the authenticity of news. \\nIn current study, to leverage different credibility \\nevaluation models abilities, we proposed an ensemble \\nlearning model to perform information and news credibility \\nevaluation, called RELIANCE. The combination of diverse \\nmodels through ensemble learning offers a synergistic \\napproach, harnessing the complementary strengths of \\nindividual methods, thereby enhancing the robustness and \\naccuracy of news credibility evaluation compared to the \\nperformance of individual models. \\n \\nTABLE I.  \\nAN ANALYTIC VIEW ON RELATED WORKS IN INFORMATION AND NEWS CREDIBILITY EVALUATION IN A TABULAR FORM \\nAuthors \\nCategory \\nDataset \\nMethod \\nInput Genre \\nGhosh & Shah[29] \\nText-based \\nBuzzFace \\nCommination of techniques from information retrieval, natural \\nlanguage processing, and deep learning \\nNews Documents \\nLiu & Wu [28] \\nHybrid \\nWeibo, Twitter15, and \\nTwitter16 \\nTime series classifier with both recurrent and convolutional \\nnetworks \\nSocial media \\ncontent \\nSeddari et al. [12] \\nHybrid \\nBuzzfeed Political News \\ndataset \\nRandom Forest, Logistic Regression, Additional Trees \\nDiscriminant, and eXtreme Gradient Boosting (XGBoost) \\nNews Documents \\nKaliyar et al.[17] \\nText-based \\nFN-COV \\nA combination of convolutional neural network and long short \\nterm memory network \\nCOVID-19 \\nGoldani et al. [25] \\nHybrid \\nLiar datasets \\nA novel transformer-based model named X-CapsNet \\nCOVID-19 \\nDixit et al. [18] \\nText-based \\nLIAR, LIAR-PLUS, and ISOT \\nFuzzy Convolutional Recurrent Neural Network (CRNN) \\nNews documents \\nDavoudi et al. [26] \\nHybrid \\nPolitiFact and GossipCop \\nLong Short Term Memory networks (LSTMs) \\nNews documents \\nYang et al. [27] \\nHybrid \\nLIAR and BuzzFeed News \\nBayesian network \\nSocial media  \\nBasak et al. [15] \\nContext-\\nbased \\nShaming detection of tweets \\nSupport vector machine \\nTwitter \\nPotthast et al. [19] \\nContext-\\nbased \\nBuzzFeed-Webis Fake News \\nSimilarity detection between text categories \\nNews Documents \\nKarimi et al. [20] \\nContext-\\nbased \\nFakeNewsNet \\nMulti-source Multi-class Fake news Detection framework \\nMMFD \\nNews Documents \\nGupta et al. [21] \\nContext-\\nbased \\nFakeNewsNet \\nA tensor factorization \\nNews Documents \\nNguyen et al. [22] \\nContext-\\nbased \\nStance-annotated dataset \\nGraph Convolutional Networks (GCN) \\nNews Documents \\nShu et al. [23] \\nContext-\\nbased \\nFakeNewsNet \\nTriFN (which models publisher-news relations and user-news \\ninteractions) \\nNews Documents \\nGhanem et al. [11] \\nText-based \\nFake NewsChallenge (FNC-1) \\nCombination of lexical features, word embeddings, and n-\\ngram statistics \\nNews documents \\nSingh et al. [12] \\nText-based \\nKaggle FakeNews \\nlinguistic features provided by LIWC \\nNews documents \\nAhmed et al. [13] \\nText-based \\nKaggle FakeNews \\nLogistic Regression and n-gram statistics \\nNews documents \\nRuchansky et al.[14] \\nText-based \\nKaggle FakeNews \\nRecurrent Neural Network and a set of text-based features \\nNews documents \\nYang et al. [30] \\nText-based \\nKaggle FakeNews \\nLatent feature extraction using convolutional neural networks \\nNews documents \\n \\nIII. METHODS \\nThe objective of our investigation was to improve the \\nautomated evaluation of information and news credibility by \\nemploying ensemble learning. Ensemble learning, combining \\nvarious methods, enhances predictive performance beyond \\nthat of individual methods. Consequently, we introduced five \\ndistinct base models, each operating at different processing \\nlevels, for integration into the ensemble learning model. \\nNamely Support Vector Machine (SVM)-based, naïve Bayes-\\nbased, logistic regression-based, random forest-based, and \\nBidirectional \\nLong \\nShort-Term \\nMemory \\nNetworks \\n(BiLSTMs)-based models. To this end, we proposed \\nRELIANCE (Reliable Ensemble Learning for Information \\nand News Credibility Evaluation), that performs credibility \\nevaluation in three phases; Phase 1 focuses on preprocessing \\nthe input news text documents to prepare them for the main \\nprocess. Phase 2 performs feature engineering through \\nembedding the input news text documents. Phase 3 is devoted \\nto evaluate the credibility of news text documents. The \\ngeneral architecture of RELIANCE is depicted in Fig. 1.  \\n \\nFig. 1. The general architecture of the proposed method (RELIANCE) \\n \\n \\n \\nXXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE \\n \\nA. Phase 1: Preprocessing \\nThis conventionally well-established and significant stage \\nwithin the domain of natural language processing aims to \\nprepare the input text and convert it into a more easily \\ncomprehensible format for machines during the main process. \\nThe preprocessing activities involved may vary depending on \\nthe task at hand. The applied preprocessing activities in \\nRELIANCE as can be seen in Fig. 1 are as listed below. \\n● Tokenization: Tokenization is the procedure of \\ndividing a text into segments known as tokens, which are \\ntypically associated with words. For this purpose, we used the \\nNatural Language Toolkit (NLTK) [31].  \\n● Noise Removal: To enrich the quality of the input text, \\nwe performed noise removal steps including stop words \\nremoval, and punctuation and signs removal since they do not \\nconvey any specific information that is essential to the \\nperformance of the system. Similar, to the previous step, \\nNLTK was also used to perform stop words removal. \\n● Lowering the Text: The process of lowering the text \\nuniforms all the characters and reduces the variability in text \\nand makes it more consistent.  \\n● Stemming: Is the morphological analysis of words that \\nreduces their inflected forms to their stem [32]. This process \\nis achieved by removing affixes from words. Escalating the \\nperformance of the natural language processing tasks in the \\nconsequence of stemming. The NLTK was also used for this \\nprocess. \\n \\nB. Phase 2: Feature Engineering  \\nFeature engineering plays a pivotal role in the realm of \\nnatural language processing, encompassing the identification, \\ntransformation, and extraction of pertinent features from raw \\ntextual data. This method facilitates the determination of the \\nmost \\ninformative \\nfeatures \\nfor \\nprimary \\nprocessing, \\nconcurrently eliminating less significant ones. The main goal \\nis to decrease the dimensionality of the dataset by reducing \\nthe number of features, generating novel features from the \\ninitial ones. Ideally, this refined feature set encapsulates the \\nessential features from the original set. \\nDoc2Vec [33], an extension of Word2Vec, to transform \\ntext documents into vectors within a high-dimensional space. \\nThe underlying concept of Doc2Vec is based on the \\nassumption that words appearing together in a document \\nshare similar meanings and, consequently, should exhibit \\nsimilar vector representations. In simpler terms, the vector \\nrepresentation of a word captures its semantic meaning. This \\napproach enables the acquisition of the contextual \\nrelationship between words within a document, a critical \\nfactor in comprehending the document\\'s overall meaning. \\nLeveraging Doc2Vec for feature engineering enables the \\ntransformation of the input text data into a low-dimension \\nvector space, facilitating processing by machine learning \\nalgorithms. In this transformation, the dimensionality \\nreduction effectively eliminates excessive dimensions and \\nretaining only crucial dimensions for subsequent processing. \\nC. Phase 3: Credibility Evaluation  \\nAt this point, the data is pre-processed, feature-\\nengineered, and we possess a rich and informative feature set. \\nThe proposed method (RELIANCE), performs credibility \\nevaluation of the input documents relying on ensembling \\nseveral base models. More specifically, RELIANCE \\nleverages the strengths of several credibility evaluation \\nmodels and combines them to achieve more reliable \\npredictions through stacking. The primary goal of ensemble \\nlearning is to increase overall predicting performance by \\nmerging the prediction of various models. In reality, it uses \\nthe votes from several different base models rather than just \\none to predict a label.  \\nAmong \\nvarious \\nensemble \\ntechniques, \\ncommonly \\nemployed ones include Bagging (Bootstrap Aggregation), \\nBoosting, and Stacking [10]. The first two are frequently \\nutilized to integrate homogeneous base models with the goal \\nof reducing variance and minimizing bias, respectively. In \\ncontrast, Stacking is typically employed to integrate \\nheterogeneous base models with the aim of enhancing \\npredictive performance. RELIANCE utilizes five credibility \\nevaluators including (SVM)-based, naïve Bayes-based, LR-\\nbased, random forest-based, and BiLSTM-based models as \\nthe base models, each of which, possesses distinctive \\ncapabilities tailored to perform credibility evaluation. Hence, \\nconsidering the heterogeneity of the applied base models, \\nRELIANCE is developed using Stacking. It is significant to \\npoint out that RELIANCE utilized a Multi-Layer Perceptron \\n(MLP) as the meta model to produce the final predictions \\n(credibility evaluation).  \\n1) \\nDataset \\nThe Fake News dataset [34] has been used to train and \\nvalidate all of the models in RELIANCE. This dataset is \\ngathered during the 2016 U.S. Presidential Election.  \\nThe dataset comprises two categories of articles: fake and \\nreal news. Real news were gathered by scraping content from \\nReuters.com, a reputable news website. Fake news were \\nsourced from various unreliable websites identified by \\nPolitifact, a U.S.-based fact-checking organization, and \\nWikipedia. \\nWhile \\nthe \\ndataset \\nencompasses \\ndiverse \\ndocuments types on various subjects, a predominant focus is \\nobserved on political and global news topics. \\nTABLE II.  \\nTHE PROPERTIES OF FAKE NEWS DATASET ) UNEQUAL \\nNUMBERS ARE DUE TO THE ABSENCE OF SOME VALUES IN THE DATASET (. \\nAttribute \\nNumber of instances in the dataset \\nID \\n20800 \\nTitle \\n20242 \\nAuthor \\n18843 \\nText \\n20761 \\nlabel \\n20800 \\n \\nFake News dataset comprises five attributes: ID, Title, \\nAuthor, Text, and Labels. The ID is a distinctive identifier for \\neach news document. The Title corresponds to the primary \\nheading of the news piece, and the Author indicates the \\ncreator\\'s name. The Text is the core part of the dataset, \\ncontaining the news document’s text. Finally, the Label \\ndetermines the credibility of the news document (assigning \\nXXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE \\n \\nfake or real labels). Table II describes the Fake News \\nDataset’s properties. In terms of label distribution, Fake \\nNews comprises approximately 51% reliable (real) news \\ndocuments and approximately 49% unreliable (fake) news. \\n2) \\nBase Models \\nRELIANCE utilizes five distinct credibility evaluation \\nmodels, including several traditional machine learning \\nmodels as well as a deep learning model to leverage their \\ncapabilities through ensemble learning to ensures the quality \\nof the final predictions. \\n● BiLSTM: we proposed a BiLSTM network to perform \\ncredibility evaluation. LSTM Recurrent Neural Networks \\ntake into account a series of previous inputs. This \\ncharacteristic makes LSTMs highly effective at learning from \\nsequential data such as text documents. As a specific type of \\nRNN, LSTM leverages past information to understand each \\nword in the forward direction. Conversely, BiLSTM benefits \\nfrom considering both past and future words (forward and \\nbackward directions) which aligns with how humans \\nunderstand language. We proposed a 3-layer BiLSTM with \\nrespectively 64, 128, and 64 units joined to a dense layer with \\n64 neurons. \\n● Logistic Regression (LR): we implemented a logistic \\nregression-based model for credibility evaluation of \\ninformation and news documents. In essence, it is a statistical \\nmodel that estimates the probabilities of a binary dependent \\nvariable. Unlike other methods that operate on independent \\ninputs, LR considers a set of previous inputs, making it \\nparticularly effective for learning from sequential data.  \\n● Support Vector Machine (SVM): we proposed the \\nSVM-based model, a powerful and versatile machine \\nlearning technique used for various tasks including \\nclassification, and regression. Unlike other algorithms that \\noperate on independent inputs, SVMs also consider a set of \\nprevious inputs and are particularly effective at learning from \\nsequential data. This feature is due to the nature of SVMs, \\nwhich aims to find the maximum separating hyperplane \\nbetween different classes in the target label.  \\n● Random Forest: we used the Random Forest \\nalgorithm, a powerful and widely used machine learning \\ntechnique for classification and regression tasks. Random \\nForest creates a set of decision trees and combines their \\noutputs to make the final prediction. Essentially, Random \\nForest is a combination of several decision trees that \\nindividually performs predictions. In practice, ensembling \\nseveral homogeneous models, Random Forest tries to \\nincrease the robustness and stability of model, and reduce the \\nrisk of overfitting, and provides more accurate predictions.  \\n● Naïve Bayes: The Naive Bayes algorithm is a \\nprobabilistic machine learning method primarily utilized for \\nclassification tasks. It also considers the prior inputs, making \\nit well-suited for learning from sequential data. The algorithm \\noperates under the assumption that attributes are independent \\nof each other, which is not always the case in reality. \\nNevertheless, this simplifying assumption surprisingly leads \\nto better predictions. Leveraging the principle of conditional \\nprobability, Naive Bayes calculates the probability of an \\nevent based on prior knowledge. Despite its simplicity, Naive \\nBayes \\ndemonstrates \\nrobust \\nperformance \\nand \\nfinds \\nwidespread use in the field of text classification. \\n3) \\nEnsemble Learning \\nThe fundamental idea behind ensemble learning is to \\nenhance overall predictive performance by aggregating \\ndecisions from several distinct base models. It involves \\nconsidering the votes of various base models to predict the \\nfinal label, rather than relying on a single model (similar to \\nharnessing the wisdom of the crowd for predictions). By \\nleveraging independent and diverse base models, ensemble \\nlearning aims to reduce the generalization error in \\npredictions, providing a compelling incentive for its \\napplication. \\nAs discussed earlier, RELIANCE performs ensemble \\nlearning through stacking five heterogeneous distinct base \\nmodels. Specifically, stacking (also known as stacked \\ngeneralization), is a machine learning algorithm that uses a \\nmeta-learning algorithm as a meta-model, to learn how to \\ncombine the best predictions of several base machine learning \\nalgorithms. Ensemble learning offers the advantage of \\nutilizing the strengths of multiple models, leading to \\npredictions that outperform any individual model. The \\naforementioned five base models were chosen as the base \\ncredibility evaluation methods, with a MLP selected as the \\nmeta-model. Although that the predictions of the base models \\nare not considered as the final predictions, they have \\nundeniable role in final decision making by the RELIANCE. \\nThe predictions from the base models, (resembling the \\nwisdom of the crowd), are employed to train the meta-model. \\nUltimately, the meta-model generates evaluates the \\ncredibility of the input documents. To do so, we used a MLP \\nmodel with three hidden layers, respectively containing 64, \\n128, and 64 units. \\nIV. RESULTS AND DISCUSSIONS \\nA. Evaluation Metrics \\nTraditionally, several of the most common evaluation \\nmetrics such as precision, recall, f-measure, and accuracy are \\nused to assess text classification models. In the same manner, \\nwe employed all of them, with a particular emphasis on \\naccuracy as the primary evaluation metric. \\nB. Evaluation Results \\nTo perform news credibility evaluation, we proposed \\nRELIANCE as an ensemble learning model that combines \\nfive distinct classifiers as the base models including SVM-\\nbased, naïve Bayes-based, LR-based, random forest-based, \\nand BiLSTMs-based models. To perform feature engineering, \\nwe employed Doc2Vec with an embedding size equal to \\n1,200, and the minimum counts of word equal to 1, and \\nconducted the training for 50 epochs. Afterwards, at first, the \\nbase models should perform credibility evaluation on the \\ninput text documents. Each of the base models adheres to \\nspecific settings for generating predictions. The train-test \\nsplit rate of the data set is 80-20 for all of the models in our \\ninvestigation. More specifically: \\nXXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE \\n \\nBiLSTM: Table III encompasses the applied parameter \\nsettings in proposed BiLSTM model.  \\nLogistic Regression (LR): For the LR-based model, the \\nmaximum number of iterations for the solver (lbfgs) to \\nconverge is set at 1000. Moreover, the L1 regularization is \\nalso used to reduce model generalization error. \\nSVM: For the SVM-based model, the chosen kernel is \\n\\'rbf\\' (Radial Basis Function). The kernel selection is crucial \\nas it determines the type of decision boundary used for \\nclassification. Moreover, the L1 regularization is also used to \\nreduce model generalization error. \\nRandom Forest: The Random Forest-based model is \\nconfigured with the parameter n_estimators set to 100. This \\nparameter defines the number of integrating decision trees. \\nThe maximum depth of the tree is set to None. That is to say, \\nnodes are expanded until all leaves are pure or until all leaves \\ncontain less than minimum number of samples required to \\nsplit an internal node. \\nNaïve Bayes: We used the Multinomial Naive Bayes \\nalgorithm, which is commonly used for text classification. \\nWe set fit_prior to false. Namely, the uniform prior \\nprobabilities are applied. Furthermore, the additive \\nsmoothing parameter (alpha) was set to 1.0. \\nMLP: Table IV provides a comprehensive overview of \\nthe applied parameter settings in the proposed MLP model. \\nTable V presents the results obtained from the evaluation \\nof five distinct proposed credibility evaluation models as well \\nas the performance of RELIANCE which was our main \\nproposed method the ensembles the base models. It \\nencompasses the values of four evaluation metrics, namely \\nprecision, recall, F1-score and accuracy. It is noteworthy \\nthat, since F1-score neglects all of the correctly false labelled \\nsamples by the system (TN), in practice it loses the ground to \\naccuracy while evaluating a credibility evaluation system.  \\nTherefore, we will focus on accuracy as the main metric to \\nevaluate the credibility of the models. \\nAs can be seen from Table V, the proposed ensemble \\nlearning model, namely RELIANCE that combines the \\naforementioned base models, has achieved the higher \\naccuracy than individual base models. Moreover, among the \\nbase models, SVM-based, LR-based, naïve bayes-based, \\nrandom \\nforest-based, \\nand \\nBiLSTM-based \\nmodels \\nrespectively achieved the highest accuracies. \\nIn the same manner, the proposed ensemble learning \\nmodel has achieved higher F1-score rather than individual \\nbase models. Similar to accuracy values, among the base \\nmodels, SVM-based, LR-based, naïve Bayes-based, random \\nforest-based, and BiLSTM-based models respectively \\nachieved the highest values for F1-score. Fig. 2 represents a \\nvisual comparison of the performance of all credibility \\nevaluation models proposed in current study. \\nC. Baseline Models \\nTo assess RELIANCE\\'s performance thoroughly, we \\nconduct a comparative analysis with the state-of-the-art \\nbaselines, which were performed on Fake News dataset: \\n● Ghanem et al. [11]: They have introduced a \\nmethodology that integrates lexical, word embeddings, and \\nn-gram features for identifying the stance in fake news. Given \\na news title-article pair, the system focuses on assessing the \\nrelevance between the article and the title.  \\n● Singh et al. [12]: In their study, Singh et al. employed \\nvarious machine learning methods to detect fake news, \\nutilizing the LIWC [16] (Linguistic Analysis and Word \\nCount-based approach). \\nTABLE III.  \\nTHE PARAMETER SETTINGS OF THE PROPOSED BILSTM \\nMODEL. \\nParameter \\nSetting \\nNumber of epochs  \\n25 \\nOptimizer \\nAdam \\nLearning rate \\n3e−4 \\nLoss function \\nBinary crossentropy \\nActivation \\nsoftmax \\nEarly stopping \\nApplied on validation accuracy \\nPatient value \\n5 \\nDropout \\n0.2 \\nBatch size \\n32 \\nCross validation \\n10 fold \\nTABLE IV.  \\nTHE PARAMETER SETTINGS OF THE PROPOSED MLP MODEL \\nParameter \\nSetting \\nActivation  \\nTanh \\nAlpha \\n0.0001 \\nHidden layer sizes \\n64,128,64 \\nLearning rate \\nConstant \\nSolver \\nAdam \\nTABLE V.  \\nEVALUATION RESULTS (IN PERCENT) FOR PROPOSED \\nCREDIBILITY EVALUATION MODELS \\n \\nPrecision  \\nRecall \\nF1-score   \\nAccuracy \\nBiLSTM \\n71 \\n77 \\n74 \\n73.57 \\nLR \\n87 \\n89 \\n88 \\n87.58 \\nSVM \\n91 \\n88 \\n89 \\n89.29 \\nRandom Forest \\n77 \\n85 \\n81 \\n80.54 \\nNaïve Bayes \\n82 \\n93 \\n87 \\n86.00 \\nEnsemble learning model \\n92 \\n94 \\n93 \\n92.43 \\n \\n \\nFig. 2. Precision, recall, F1-score, and accuracy values for suggested \\ncredibility evaluation models, including 5 base models along with \\nensemble learning model \\n● Ahmed et al. [13]: In this paper, the authors to perform \\nnews credibility evaluation have introduce a hybrid model \\nbased on LR and n-gram analysis. They have used Term TF-\\nIDF as feature extraction technique. \\n● Ruchansky et al. [15]: In this study, they introduced a \\nmodel that integrates several key features of text document \\n(such as the text of news document, news source, etc.) for a \\nmore precise and automated prediction. Specifically, their \\napproach incorporates the behaviors of both users and \\n60\\n70\\n80\\n90\\n100\\nPrecision\\nRecall\\nF1-score\\nAccuracy\\nBiLSTM\\nLR\\nSVM\\nRandom forest\\nNaïve Bayes\\nEnsemble learning\\nXXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE \\n \\narticles, along with the collective behavior of users who \\ndisseminate fake news.  \\n● Ahmed et al. [13]: In this paper, the authors have \\nintroduced a model for detecting fake news that employs n-\\ngram analysis and machine learning techniques. They explore \\nand contrast two distinct feature extraction methods along \\nwith six diverse machine classification techniques.  \\n● Yang et al. [30]: They introduced a model called TI-\\nCNN (Text and Image Information-based Convolutional \\nNeural Network) for evaluating the credibility of news \\ndocuments. Their emphasis on sensitivity analysis in their \\napproach aimed to enhance accuracy. \\nTable VI presents a perspective on the performance of \\nbaseline models, along with the performance of proposed \\nensemble learning method by RELIANCE. Examining the \\ndata in this table reveals that RELIANCE has achieved the \\nhighest accuracy and f-measure results. \\nTABLE VI.  \\nCOMPARING THE PERFORMANCE OF BASELINE MODELS IN \\nCREDIBILITY EVALUATION, WHICH WERE PERFORMED ON FAKE NEWS \\nDATASET (MISSING VALUES ARE NOT REPORTED). SIGNIFICANT VALUES ARE \\nIN BOLD. \\n \\nPrecision  \\nRecall   \\nF1-score \\nAccuracy   \\nGhanem et al. [11] \\n- \\n- \\n48.80 \\n - \\nSingh et al. [12]  \\n86.00 \\n90.00 \\n88.00 \\n87.00 \\nAhmed et al. [13] \\n- \\n- \\n- \\n89.00 \\nRuchansky et al. [14] \\n- \\n- \\n89.40 \\n89.20 \\nAhmed et al. [13] \\n- \\n- \\n- \\n92.00 \\nYang et al. [30] \\n92.20 \\n92.77 \\n92.10 \\n- \\nRELIANCE \\n(proposed method) \\n92.49 \\n93.88 \\n92.75 \\n92.43 \\n \\nD. Discussion \\nThe \\nensemble \\nlearning \\napproach \\nproposed \\nby \\nRELIANCE in this study, integrating five diverse base \\nmodels (BiLSTMs, SVM, logistic regression, naive Bayes, \\nand random forest) has demonstrated remarkable efficacy in \\nthe credibility evaluation of news documents. The integration \\nof these base models through a MLP as the meta-model has \\nyielded superior accuracy when compared to the performance \\nof individual base models. \\nThe robustness and versatility of the ensemble learning \\nmodel stems from its ability to capture and harness the \\ndistinctive strengths of each base model, compensating for \\ntheir individual limitations. The inherent diversity among the \\nbase models, arising from variations in their underlying \\nalgorithms and processing methodologies, contributes to a \\nmore comprehensive and nuanced understanding of the \\ncomplex features present in news documents. \\nThe significant improvement in accuracy achieved by the \\nensemble approach highlights the synergy of diverse models \\nworking collaboratively. The ensemble not only mitigates the \\nrisk of overfitting but also enhances the generalization \\ncapacity of the model, making it adept at handling the \\nintricacies and nuances inherent in real-world news datasets. \\nMoreover, the ensemble\\'s outperformance suggests that the \\ncollaborative decision-making process employed by diverse \\nmodels leads to a more robust and reliable system for news \\ncredibility assessment. The meta-model\\'s ability to discern \\nand integrate the collective insights of the base models results \\nin a more discerning and accurate evaluation, overcoming the \\nlimitations associated with relying on any single model. \\nComparing the performance of RELIANCE by the state of \\nthe art studies that have been performed credibility evaluation \\nusing Fake News dataset, as shown in Table VI, clearly \\ndemonstrates the superiority of RELIANCE in credibility \\nevaluation. Fig. 3 depicts the comparison of the accuracy of \\nthe baseline models, and our proposed method (RELIANCE) \\nfor news credibility evaluation. \\n \\n \\nFig. 3. Comparing the accuracy of baseline models (those that reported the \\naccuracy values) in news credibility evaluation with the proposed model \\n(RELIANCE) \\nLets’ to consider the main contributions of our study that \\nwas stated in Introduction:  \\nContribution 1 (introducing five distinct diverse methods \\nfor news credibility evaluation): The introduction of five \\ndistinct diverse methods for news credibility evaluation \\nserves as a foundational contribution to the field. Our choice \\nof these methods, encompassing SVM, logistic regression, \\nnaive Bayes, random forest, and BiLSTMs, was driven by the \\naim to offer a diverse set of approaches, each capturing \\ndifferent aspects of the complex nature of documents. The \\nsubsequent utilization of these methods as base models in our \\nensemble learning framework allowed for a comprehensive \\nexploration of the various features and patterns inherent in \\nnews content. Our findings highlight the importance of \\nmethodological diversity in tackling the multifaceted \\nchallenges of credibility assessment, offering valuable \\ninsights into the nuanced aspects of information reliability. \\nContribution 2 (enhancing credibility evaluation \\naccuracy through ensemble learning): The primary objective \\nof our study was to enhance the accuracy of news credibility \\nevaluation, and the incorporation of ensemble learning stands \\nout as a pivotal contribution in achieving this goal. By \\nintegrating the diverse perspectives and strengths of the five \\nbase models through a MLP as the meta-model, we witnessed \\na discernible enhancement in accuracy compared to \\nindividual base models. The ensemble learning framework \\neffectively leveraged the complementary strengths of each \\nmethod, mitigating the limitations associated with any \\nsingular approach. This not only resulted in a more robust and \\naccurate credibility assessment but also highlighted the \\npotential of ensemble learning as a powerful tool in \\ninformation reliability studies.  \\n \\n87.00\\n89.00\\n89.20\\n92.00\\n92.43\\n60\\n65\\n70\\n75\\n80\\n85\\n90\\n95\\nSingh et al\\nAhmed et al\\nRuchansky et al\\nAhmed et al2\\nRELIANCE\\nXXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE \\n \\nV. CONCLUSION \\nIn the current era of information overload, accurately \\nassessing the credibility of news sources is crucial for \\ninformed decision-making and effective crisis management. \\nTo address this challenge, we propose RELIANCE (Reliable \\nEnsemble Learning for Information and News Credibility \\nEvaluation), an ensemble learning approach that combines \\nthe strengths of five individual models for news credibility \\nevaluation. The base models include Support Vector \\nMachines, naïve Bayes classifiers, logistic regression models, \\nrandom forests, and Bidirectional Long Short-Term Memory \\nNetworks. These models are individually trained to extract \\nrelevant features from news documents and classify their \\ncredibility. To further enhance the overall accuracy, \\nRELIANCE employs a multi-layer perceptron as a meta-\\nmodel, which integrates the predictions of each base model \\n(using stacking) and produces a more refined credibility \\nassessment. Comparative experiments with baseline models \\ndemonstrate that RELIANCE significantly outperforms \\nexisting algorithms in evaluating the credibility of news \\ndocuments. It provides a robust framework for identifying \\ntrustworthy news sources, offering real-world applications \\nthat empower users, journalists, and fact-checkers with a \\nresilient tool against misinformation in the digital era. \\nREFERENCES \\n[1] P. H. A. Faustini and T. F. Covoes, “Fake news detection in multiple \\nplatforms and languages,” Expert Syst. Appl., vol. 158, p. 113503, 2020. \\n[2] M. R. Kondamudi, S. R. Sahoo, L. Chouhan, and N. Yadav, “A \\ncomprehensive survey of fake news in social networks: Attributes, features, \\nand detection approaches,” J. King Saud Univ. Inf. Sci., vol. 35, no. 6, p. \\n101571, 2023. \\n[3] B. Pattanaik, S. Mandal, and R. M. Tripathy, “A survey on rumor \\ndetection and prevention in social media using deep learning,” Knowl. Inf. \\nSyst., pp. 1–42, 2023. \\n[4] Z. Jahanbakhsh-Nagadeh et al., “A model to measure the spread power \\nof rumors,” J. Ambient Intell. Humaniz. Comput., vol. 14, no. 10, pp. 13787–\\n13811, 2023. \\n[5] N. Rani, P. Das, and A. K. Bhardwaj, “Rumor, misinformation among \\nweb: a contemporary review of rumor detection techniques during different \\nweb waves,” Concurr. Comput. Pract. Exp., vol. 34, no. 1, p. e6479, 2022. \\n[6] R. Upadhyay, G. Pasi, and M. Viviani, “Vec4Cred: a model for health \\nmisinformation detection in web pages,” Multimed. Tools Appl., vol. 82, no. \\n4, pp. 5271–5290, 2023. \\n[7] P. Przybyła and A. J. Soto, “When classification accuracy is not enough: \\nExplaining news credibility assessment,” Inf. Process. Manag., vol. 58, no. \\n5, p. 102653, 2021. \\n[8] G. Pasi, M. De Grandis, and M. Viviani, “Decision making over multiple \\ncriteria to assess news credibility in microblogging sites,” in 2020 IEEE \\nInternational Conference on Fuzzy Systems (FUZZ-IEEE), IEEE, 2020, pp. \\n1–8. \\n[9] N. Sitaula, C. K. Mohan, J. Grygiel, X. Zhou, and R. Zafarani, \\n“Credibility-based fake news detection,” Disinformation, misinformation, \\nfake news Soc. media Emerg. Res. challenges Oppor., pp. 163–182, 2020. \\n[10] M. Ramezani et al., “Automatic personality prediction: an enhanced \\nmethod using ensemble modeling,” Neural Comput. Appl., vol. 34, no. 21, \\npp. 18369–18389, 2022. \\n[11] B. Ghanem, P. Rosso, and F. Rangel, “Stance detection in fake news a \\ncombined feature representation,” in Proceedings of the first workshop on \\nfact extraction and VERification, 2018, pp. 66–71. \\n[12] V. Singh, R. Dasgupta, D. Sonagra, K. Raman, and I. Ghosh, \\n“Automated fake news detection using linguistic analysis and machine \\nlearning,” in International conference on social computing, behavioral-\\ncultural modeling, & prediction and behavior representation in modeling \\nand simulation (SBP-BRiMS), 2017, pp. 1–3. \\n[13] H. Ahmed, I. Traore, and S. Saad, “Detection of online fake news using \\nn-gram analysis and machine learning techniques,” in Intelligent, Secure, \\nand Dependable Systems in Distributed and Cloud Environments: First \\nInternational Conference, ISDDC 2017, Vancouver, BC, Canada, October \\n26-28, 2017, Proceedings 1, Springer, 2017, pp. 127–138. \\n[14] N. Ruchansky, S. Seo, and Y. Liu, “Csi: A hybrid deep model for fake \\nnews detection,” in Proceedings of the 2017 ACM on Conference on \\nInformation and Knowledge Management, 2017, pp. 797–806. \\n[15] R. Basak, S. Sural, N. Ganguly, and S. K. Ghosh, “Online public \\nshaming on Twitter: Detection, analysis, and mitigation,” IEEE Trans. \\nComput. Soc. Syst., vol. 6, no. 2, pp. 208–220, 2019. \\n[16] J. W. Pennebaker, M. E. Francis, and R. J. Booth, “Linguistic inquiry \\nand word count: Liwc 2001Lawrence Erlbaum Associates.” Mahway, 2001. \\n[17] R. K. Kaliyar, A. Goswami, and P. Narang, “A Hybrid Model for \\nEffective Fake News Detection with a Novel COVID-19 Dataset.,” in \\nICAART (2), 2021, pp. 1066–1072. \\n[18] D. K. Dixit, A. Bhagat, and D. Dangi, “Fake News Classification Using \\na Fuzzy Convolutional Recurrent Neural Network.,” Comput. Mater. \\nContin., vol. 71, no. 3, 2022. \\n[19] M. Potthast, J. Kiesel, K. Reinartz, J. Bevendorff, and B. Stein, “A \\nstylometric inquiry into hyperpartisan and fake news,” arXiv Prepr. \\narXiv1702.05638, 2017. \\n[20] H. Karimi, P. Roy, S. Saba-Sadiya, and J. Tang, “Multi-source multi-\\nclass fake news detection,” in Proceedings of the 27th international \\nconference on computational linguistics, 2018, pp. 1546–1557. \\n[21] S. Gupta, R. Thirukovalluru, M. Sinha, and S. Mannarswamy, \\n“CIMTDetect: a community infused matrix-tensor coupled factorization \\nbased method for fake news detection,” in 2018 IEEE/ACM International \\nConference on Advances in Social Networks Analysis and Mining \\n(ASONAM), IEEE, 2018, pp. 278–281. \\n[22] V.-H. Nguyen, K. Sugiyama, P. Nakov, and M.-Y. Kan, “Fang: \\nLeveraging social context for fake news detection using graph \\nrepresentation,” in Proceedings of the 29th ACM international conference \\non information & knowledge management, 2020, pp. 1165–1174. \\n[23] K. Shu, S. Wang, and H. Liu, “Beyond news contents: The role of social \\ncontext for fake news detection,” in Proceedings of the twelfth ACM \\ninternational conference on web search and data mining, 2019, pp. 312–320. \\n[24] N. Seddari, A. Derhab, M. Belaoued, W. Halboob, J. Al-Muhtadi, and \\nA. Bouras, “A hybrid linguistic and knowledge-based analysis approach for \\nfake news detection on social media,” IEEE Access, vol. 10, pp. 62097–\\n62109, 2022. \\n[25] M. Hadi Goldani, R. Safabakhsh, and S. Momtazi, “X-CapsNet For \\nFake News Detection,” arXiv e-prints, p. arXiv-2307, 2023. \\n[26] M. Davoudi, M. R. Moosavi, and M. H. Sadreddini, “A Novel Method \\nfor Fake News Detection Based on Propagation Tree,” in 2021 11th \\nInternational Conference on Computer Engineering and Knowledge \\n(ICCKE), IEEE, 2021, pp. 105–109. \\n[27] S. Yang, K. Shu, S. Wang, R. Gu, F. Wu, and H. Liu, “Unsupervised \\nfake news detection on social media: A generative approach,” in Proceedings \\nof the AAAI conference on artificial intelligence, 2019, pp. 5644–5651. \\n[28] Y. Liu and Y.-F. Wu, “Early detection of fake news on social media \\nthrough propagation path classification with recurrent and convolutional \\nnetworks,” in Proceedings of the AAAI conference on artificial intelligence, \\n2018. \\n[29] S. Ghosh and C. Shah, “Towards automatic fake news classification,” \\nProc. Assoc. Inf. Sci. Technol., vol. 55, no. 1, pp. 805–807, 2018. \\n[30] Y. Yang, L. Zheng, J. Zhang, Q. Cui, Z. Li, and P. S. Yu, “TI-CNN: \\nConvolutional neural networks for fake news detection,” arXiv Prepr. \\narXiv1806.00749, 2018. \\n[31] S. Bird, E. Klein, and E. Loper, \"Natural language processing with \\nPython: analyzing text with the natural language toolkit,\" O\\'Reilly Media, \\nInc., 2009. \\n[32] M. Ramezani, M.-R. Feizi-Derakhshi, and M.-A. Balafar, “Text-based \\nautomatic personality prediction using KGrAt-Net: a knowledge graph \\nattention network classifier,” Sci. Rep., vol. 12, no. 1, p. 21453, 2022. \\n[33] Q. Le and T. Mikolov, “Distributed representations of sentences and \\ndocuments,” in International conference on machine learning, PMLR, 2014, \\npp. 1188–1196. \\n[34] \\n“Kaggle. \\n(2024). \\nFake \\nNews.” \\n[Online]. \\nAvailable: \\nhttps://www.kaggle.com/c/fake-news/data \\n'},\n",
       " {'abstract': 'eXplainable AI has received signiﬁcant attention in recent years. Machine learning models often operate as black boxes, lacking explainability and transparency while supporting decision-making processes. Local post-hoc explainability queries attempt to answer why individual inputs are classiﬁed in a certain way by a given model. While there has been important work on counterfactual explanations, less attention has been devoted to semifactual ones. In this paper, we focus on local post-hoc explainability queries within the semifactual “even-if” thinking and their computational complexity among different classes of models, and show that both linear and tree-based models are strictly more interpretable than neural networks. After this, we introduce a preference-based framework that enables users to personalize explanations based on their preferences, both in the case of semifactuals and counterfactuals, enhancing interpretability and user-centricity. Finally, we explore the complexity of several interpretability problems in the proposed preference-based framework and provide algorithms for polynomial cases.',\n",
       "  'introduction': 'The extensive study of counterfactual “if only” thinking, exploring how things might have been different, has been a focal point for social and cognitive psychologists [Kahneman and Tversky, 1981; McCloy and Byrne, 2002]. Consider a negative event, such as taking a taxi and due to trafﬁc arriving late to a party. By analyzing this situation, an individual (e.g. Alice) might engage in counterfactual thinking by imagining how things could have unfolded differently, such as, “if only Alice had not taken the taxi, she would not have arrived late at the party”. This type of counterfactual thinking, where an alternative scenario is imagined, is a common aspect of daily life. In such a case the counterfactual scenario negates both the event’s cause (antecedent) and its outcome, presenting a false cause and a false outcome that are temporarily considered as true (e.g., Alice took the taxi and arrived late). Counterfactual thinking forms the basis for crafting counterfactual explanations, which are crucial in automated decision-making processes. These explanations leverage imagined alternative scenarios, aiding users in understanding why certain outcomes occurred and how different situations might have inﬁuenced automated decisions. Counterfactual explanations empowers users to grasp the rationale behind automated decisions, fostering transparency and user trust in these systems. Several deﬁnitions of counterfactual explanations exist in the literature [Guidotti, 2022]. According to most of the literature, counterfactuals are deﬁned as the minimum changes to apply to a given instance to let the prediction of the model be different [Barceló et al., 2020]. While signiﬁcant attention in AI has been given to counterfactual explanations, there has been a limited focus on the equally important and related semifactual “even if” explanations [Aryal and Keane, 2023; Kenny and Huang, 2023], though they have been investigated much more in cognitive sciences. While counterfactuals explain what changes to the input features of an AI system change the output decision, semifactuals show which input feature changes do not change a decision outcome. Considering the above-mentioned situation where Alice took the taxi and arrived late at the party, we might analyze “even if” scenarios envisioning how things could have remained the same, such as, “even if Alice had not taken a taxi, she would have still arrived late at the party”. Sharing the same underlying idea of counterfactuals, we deﬁne semifactuals as the maximum changes to be applied to a given instance while keeping the same prediction. Indeed, the larger the feature differences asserted in the semifactual, the better (more convincing) the explanation [Aryal and Keane, 2023]. This intuitively captures the desire of an agent to have more degree of freedom and favorable conditions (represented by features changed), while keeping the (positive) status assigned to it by the model. As an example, consider the following, inspired by a mortgage scenario.\\nExample 1. Consider the binary and linear classiﬁcation model M : {0, 1}3 → {0, 1} shown in Figure 1 where M is deﬁned as step(x ⋅ [2, 2, 0] + 1). The three features are:\\n\\n• f1 =“user’s employment contract is part-time”,\\n• f2 =“user applies for a mortgage whose duration is longer than 30 years”, and\\n• f3 =“user works on-site”.',\n",
       "  'literature review': 'However, as highlighted in the previous example, multiple semifactuals can exist for each given instance. In these situations, a user may prefer one semifactual to another, by expressing preferences over features so that the best semifactuals will be selected, as shown in the following example.\\nExample 2. Continuing with the previous example, suppose that the user x1 prefers semifactuals with f1 = 1 rather than those with f2 = 0, that is (s)he prefers to change feature f1 rather than f2 (irrespective of any other change). Thus, (s)he would prefer to still get a mortgage by changing the job to part-time (obtaining y2); if this cannot be accomplished, then (s)he prefers to get a mortgage by changing the duration to be less than or equal to 30 years (obtaining y1). Prioritized reasoning in AI, focusing on incorporating user preferences, represents a pivotal advancement in the ﬁeld, enhancing adaptability and user-centricity of AI systems. Traditional AI models rely on predeﬁned rules or optimization criteria to generate outcomes, often overlooking the nuanced nature of user-speciﬁc preferences [Rossi et al., 2011; Santhanam et al., 2016]. Prioritized reasoning addresses this limitation by introducing a mechanism that allows users to express their preferences, thereby guiding AI systems to prefer speciﬁc factors over others in the decision-making processes. One key aspect of prioritized reasoning is its applicability across diverse AI domains, spanning machine learning [Kapoor et al., 2012], natural language processing [Bakker et al., 2022], and recommendation systems [Zhu et al., 2022]. In machine learning, for instance, the ability to prioritize speciﬁc features or outcomes based on user preferences signiﬁcantly improves the relevance and usability of the resulting models. Within natural language processing, prioritized reasoning facilitates customized language generation aligned with individual preferences, catering to diverse communication styles. The impact of prioritized reasoning extends to recommendation systems, where user preferences play a crucial role in shaping the suggestions provided. Our work contributes to prioritized reasoning within explainable AI in the presence of user’s preference conditions related to features. These preferences are exploited to generate semifactual and counterfactual explanations that align most closely with the user-speciﬁed criteria. In particular, preferences are applied similarly to what has been proposed in the well-known Answer Set Optimization approach for Logic Programs with preferences [Brewka et al., 2003].',\n",
       "  'methodology': None,\n",
       "  'results': None,\n",
       "  'conclusion': 'In this paper, after exploring local post-hoc interpretability queries related to semifactuals and their computational complexity among three classes of models (FBDDs, perceptrons, and MLPs), we introduced a framework that enables users to personalize semifactual and counterfactual explanations based on preferences. Then, we investigated the complexity of the proposed framework and presented PTIME algorithms. Our approach to handle preferences over semifac-tual/counterfactual explanations is inspired by the Answer Set Optimization approach [Brewka et al., 2003]. In particular, conditional preferences among features are deﬁned by means of preference rules, the satisfaction of each rule is evaluated quantitatively, whereas the overall preference order is deﬁned in a qualitative way. The main advantages of such approach consist in the simplicity and compactness of representation, and combination of quantitative and qualitative evaluations.',\n",
       "  'title': 'Even-if Explanations: Formal Foundations, Priorities and Complexity',\n",
       "  'author': 'Gianvincenzo Alfano, Sergio Greco, Domenico Mandaglio, Francesco Parisi, Reza Shahbazian, Irina Trubitsyna',\n",
       "  'textdata': 'Even-if Explanations: Formal Foundations, Priorities and Complexity\\nGianvincenzo Alfano , Sergio Greco , Domenico Mandaglio , Francesco Parisi ,\\nReza Shahbazian and Irina Trubitsyna\\nDIMES Department, University of Calabria, Rende, Italy\\n{g.alfano, greco, d.mandaglio, fparisi, i.trubitsyna}@dimes.unical.it reza.shahbazian@unical.it\\nAbstract\\nEXplainable AI has received signiﬁcant attention\\nin recent years.\\nMachine learning models of-\\nten operate as black boxes, lacking explainabil-\\nity and transparency while supporting decision-\\nmaking processes.\\nLocal post-hoc explainability\\nqueries attempt to answer why individual inputs\\nare classiﬁed in a certain way by a given model.\\nWhile there has been important work on counter-\\nfactual explanations, less attention has been de-\\nvoted to semifactual ones. In this paper, we fo-\\ncus on local post-hoc explainability queries within\\nthe semifactual ‘even-if’ thinking and their com-\\nputational complexity among different classes of\\nmodels, and show that both linear and tree-based\\nmodels are strictly more interpretable than neural\\nnetworks. After this, we introduce a preference-\\nbased framework that enables users to personalize\\nexplanations based on their preferences, both in the\\ncase of semifactuals and counterfactuals, enhanc-\\ning interpretability and user-centricity. Finally, we\\nexplore the complexity of several interpretability\\nproblems in the proposed preference-based frame-\\nwork and provide algorithms for polynomial cases.\\n1\\nIntroduction\\nThe extensive study of counterfactual ‘if only’ thinking, ex-\\nploring how things might have been different, has been a focal\\npoint for social and cognitive psychologists [Kahneman and\\nTversky, 1981; McCloy and Byrne, 2002]. Consider a neg-\\native event, such as taking a taxi and due to trafﬁc arriving\\nlate to a party. By analyzing this situation, an individual (e.g.\\nAlice) might engage in counterfactual thinking by imagining\\nhow things could have unfolded differently, such as, ‘if only\\nAlice had not taken the taxi, she would not have arrived late\\nat the party’. This type of counterfactual thinking, where an\\nalternative scenario is imagined, is a common aspect of daily\\nlife. In such a case the counterfactual scenario negates both\\nthe event’s cause (antecedent) and its outcome, presenting a\\nfalse cause and a false outcome that are temporarily consid-\\nered as true (e.g., Alice took the taxi and arrived late).\\nCounterfactual thinking forms the basis for crafting coun-\\nterfactual explanations, which are crucial in automated\\ny1 = [0, 0, 0]\\n[1,0,0]\\n[1,1,1]\\n[0,0,1]\\ny2 = [1, 1, 0]\\nf1\\nf2\\nf3\\nx1 = [0, 1, 1]\\n[1,0,1]\\nFigure 1: Binary classiﬁcation model M : step(x · [−2, 2, 0] + 1)\\nof Example 1 representing the mortgage scenario. The binary fea-\\nture f1 (resp., f2 and f3) represents part-time employment contract\\n(resp., mortgage term longer than 30 years, and on site-working).\\nCrosses (resp., circles) on the corners of the green (resp., red) area\\ncorrespond to instances where the model outputs 1 (resp., 0).\\ndecision-making processes.\\nThese explanations leverage\\nimagined alternative scenarios, aiding users in understanding\\nwhy certain outcomes occurred and how different situations\\nmight have inﬂuenced automated decisions. Counterfactual\\nexplanations empowers users to grasp the rationale behind\\nautomated decisions, fostering transparency and user trust in\\nthese systems. Several deﬁnitions of counterfactual explana-\\ntions exist in the literature [Guidotti, 2022]. According to\\nmost of the literature, counterfactuals are deﬁned as the mini-\\nmum changes to apply to a given instance to let the prediction\\nof the model be different [Barcel´o et al., 2020].\\nWhile signiﬁcant attention in AI has been given to coun-\\nterfactual explanations, there has been a limited focus on the\\nequally important and related semifactual ‘even if’ expla-\\nnations [Aryal and Keane, 2023; Kenny and Huang, 2023],\\nthough they have been investigated much more in cognitive\\nsciences. While counterfactuals explain what changes to the\\ninput features of an AI system change the output decision,\\nsemifactuals show which input feature changes do not change\\na decision outcome. Considering the above-mentioned situ-\\nation where Alice took the taxi and arrived late at the party,\\nwe might analyze ‘even if’ scenarios envisioning how things\\ncould have remained the same, such as, “even if Alice had not\\ntaken a taxi, she would have still arrived late at the party”.\\nSharing the same underlying idea of counterfactuals, we\\ndeﬁne semifactuals as the maximum changes to be applied\\nto a given instance while keeping the same prediction. In-\\ndeed, the larger the feature differences asserted in the semi-\\nfactual, the better (more convincing) the explanation [Aryal\\nand Keane, 2023]. This intuitively captures the desire of an\\nagent to have more degree of freedom and favorable condi-\\ntions (represented by features changed), while keeping the\\n(positive) status assigned to it by the model. As an example,\\nconsider the following, inspired by a mortgage scenario.\\nExample 1. Consider the binary and linear classiﬁcation\\nmodel M : {0, 1}3 → {0, 1} shown in Figure 1 where M\\nis deﬁned as step(x · [⇥2, 2, 0] + 1). The three features are:\\n• f1 =“user’s employment contract is part-time”,\\n• f2 =“user applies for a mortgage whose duration is\\nlonger than 30 years”, and\\n• f3 =“user works on-site”.\\nFor any instance x ⇤ {0, 1}3 we have that M(x) = 0 if\\nx = [1, 0, 1] or x = [1, 0, 0], and M(x) = 1 otherwise.\\nIntuitively, this means that the bank’s model does not approve\\nthe mortgage only when the user has a part-time job and the\\nrequested mortgage duration is less than 30 years.\\nConsider a user x1 that works full-time, on-site, and re-\\nquests a mortgage with duration longer than 30 years (i.e.,\\nx1 = [0, 1, 1]), we have that y1 = [0, 0, 0] and y2 = [1, 1, 0]\\nare semifactual of x1 w.r.t. M at maximum distance (i.e., 2)\\nfrom x1 in terms of number of feature changed. Intuitively,\\ny1 represents ‘the user x1 will obtain the mortgage even if\\nthe work would have been remote and the mortgage duration\\nwould have been less than 30 years’, while y2 represents ‘the\\nuser x1 will obtain the mortgage even if the work would have\\nbeen part-time and she would have worked remotely’.\\n2\\nHowever, as highlighted in the previous example, multi-\\nple semifactuals can exist for each given instance. In these\\nsituations, a user may prefer one semifactual to another, by\\nexpressing preferences over features so that the best semifac-\\ntuals will be selected, as shown in the following example.\\nExample 2. Continuing with the previous example, suppose\\nthat the user x1 prefers semifactuals with f1 = 1 rather than\\nthose with f2 = 0, that is (s)he prefers to change feature f1\\nrather than f2 (irrespective of any other change). Thus, (s)he\\nwould prefer to still get a mortgage by changing the job to\\npart-time (obtaining y2); if this cannot be accomplished, then\\n(s)he prefers to get a mortgage by changing the duration to be\\nless than or equal to 30 years (obtaining y1).\\n2\\nPrioritized reasoning in AI, focusing on incorporating user\\npreferences, represents a pivotal advancement in the ﬁeld, en-\\nhancing adaptability and user-centricity of AI systems. Tra-\\nditional AI models rely on predeﬁned rules or optimiza-\\ntion criteria to generate outcomes, often overlooking the nu-\\nanced nature of user-speciﬁc preferences [Rossi et al., 2011;\\nSanthanam et al., 2016]. Prioritized reasoning addresses this\\nlimitation by introducing a mechanism that allows users to ex-\\npress their preferences, thereby guiding AI systems to prefer\\nspeciﬁc factors over others in the decision-making processes.\\nOne key aspect of prioritized reasoning is its appli-\\ncability across diverse AI domains,\\nspanning machine\\nlearning [Kapoor et al., 2012], natural language process-\\ning [Bakker et al., 2022], and recommendation systems [Zhu\\net al., 2022]. In machine learning, for instance, the ability\\nto prioritize speciﬁc features or outcomes based on user pref-\\nerences signiﬁcantly improves the relevance and usability of\\nthe resulting models.\\nWithin natural language processing,\\nprioritized reasoning facilitates customized language gener-\\nation aligned with individual preferences, catering to diverse\\ncommunication styles. The impact of prioritized reasoning\\nextends to recommendation systems, where user preferences\\nplay a crucial role in shaping the suggestions provided.\\nOur work contributes to prioritized reasoning within ex-\\nplainable AI in the presence of user’s preference conditions\\nrelated to features. These preferences are exploited to gen-\\nerate semifactual and counterfactual explanations that align\\nmost closely with the user-speciﬁed criteria. In particular,\\npreferences are applied similarly to what has been proposed\\nin the well-known Answer Set Optimization approach for\\nLogic Programs with preferences [Brewka et al., 2003].\\nContributions\\nOur main contributions are as follows.\\n• We formally introduce the concepts of semifactual over\\nthree classes of models: (i) perceptrons (ii) free bi-\\nnary decision diagrams (FBBDs), and (iii) multi-layer\\nperceptrons (MLP), intuitively encoding local post-hoc\\nexplainable queries within the even-if thinking setting.\\nHerein, the term ‘local’ refers to explaining the output of\\nthe system for a particular input, while ‘post-hoc’ refers\\nto interpreting the system after it has been trained.\\n• We ﬁrst investigate the computational complexity of in-\\nterpretability problems concerning semifactuals, show-\\ning that they are not more difﬁcult than analogous prob-\\nlems related to counterfactuals [Barcel´o et al., 2020].\\n• We introduce a framework that empowers users to pri-\\noritize explanations according to their subjective pref-\\nerences. That is, users can specify preferences for al-\\ntering speciﬁc features over others within explanations.\\nThis approach enriches the explanation process, en-\\nabling users to inﬂuence the selection of the most favor-\\nable semifactuals (called “best” semifactuals), thereby\\naugmenting the interpretability and user-centricity of the\\nresulting outputs. Notably, the proposed framework also\\nnaturally encompasses preferences over counterfactuals.\\n• We investigate the complexity of several interpretability\\nproblems related to best semifactuals and best counter-\\nfactuals. Table 1 summarizes our complexity results. Fi-\\nnally, focusing on a restricted yet expressive class of fea-\\nture preferences, we identify tractable cases for which\\nwe propose algorithms for their computation.\\nFull proofs can be found in the Supplementary Material.\\n2\\nPreliminaries\\nWe start by recalling the key concepts underlying counterfac-\\ntual and semifactual explanations, and then we recall the main\\ncomplexity classes used in the paper.\\n2.1\\nClassiﬁcation Models\\nA (binary classiﬁcation) model is a function M : {0, 1}n →\\n{0, 1}, speciﬁcally focusing on instances whose features are\\nrepresented by binary values. Constraining inputs and out-\\nputs to booleans simpliﬁes our context while encompassing\\nnumerous relevant practical scenarios. A class of models is\\njust a way of grouping models together. An instance x is a\\nvector in {0, 1}n and represents a possible input for a model.\\nWe now recall three signiﬁcant categories of ML models,\\nthat hereafter will be the one we will focus on.\\nBinary Decision Diagrams.\\nA Binary Decision Diagram\\n(BDD) M = (V, E, λV , λE) [Wegener, 2004] is a rooted di-\\nrected acyclic graph (V, E). It features leaves labeled true\\n(⌅) or false (⇧), and internal nodes labeled by function λV\\nwith a value from {1, . . . , n}, each of them having two out-\\ngoing edges labeled by function λE as 1 and 0, respectively.\\nEach instance x = [x1, . . . , xn] ⇤ {0, 1}n uniquely maps to\\na path px in M. This path adheres to the following condi-\\ntion: for every non-leaf node u in px labeled i, the path goes\\nthrough the edge labeled with xi. |M| denotes the size of M,\\nrepresenting the number of edges. A binary decision diagram\\nM is free (FBDD) if for every path from the root to a leaf, no\\ntwo nodes on that path have the same label. A decision tree\\nis simply an FBDD whose underlying graph is a tree.\\nMultilayer Perceptrons.\\nA multilayer perceptron (MLP)\\nM with k layers is deﬁned by a sequence of weight matrices\\nW(1), . . . , W(k), bias vectors b(1), . . . , b(k), and activation\\nfunctions a(1), . . . , a(k). Given an instance x, we inductively\\ndeﬁne h(i) = a(i)(h(i−1)W(i) + b(i)) with i ⇤ {1, . . . , k},\\nassuming that h(0) = x. The output of M on x is deﬁned\\nas M(x) = h(k). In this paper we assume all weights and\\nbiases to be rational numbers, i.e. belonging to Q. We say\\nthat an MLP as deﬁned above has (k ⇥ 1) hidden layers. The\\nsize of an MLP M, denoted by |M|, is the total size of its\\nweights and biases, in which the size of a rational number p\\nq\\nis log2(p) + log2(q) (with the convention that log2(0) = 1).\\nWe focus on MLPs in which all internal functions\\na(1), . . . , a(k−1)\\nare\\nthe\\nReLU\\nfunction\\nrelu(x)\\n=\\nmax(0, x). Usually, MLP binary classiﬁers are trained using\\nthe sigmoid as the output function a(k). Nevertheless, when\\nan MLP classiﬁes an input (after training), it takes decisions\\nby simply using the preactivations, also called logits. Based\\non this and on the fact that we only consider already trained\\nMLPs, we can assume w.l.o.g. that the output function a(k)\\nis the binary step function, deﬁned as step(x) = 0 if x < 0,\\nand step(x) = 1 if x ⌃ 0.\\nPerceptrons.\\nA perceptron is an MLP with no hidden lay-\\ners (i.e., k = 1). That is, a perceptron M is deﬁned by a\\npair (W, b) such that W ⇤ Qn◊1 and b ⇤ Q, and the out-\\nput is M(x) = step(x W + b). Because of its particular\\nstructure, a perceptron is usually deﬁned as a pair (w, b) with\\nw = WT a rational vector and b a rational number. The out-\\nput of M(x) is then 1 iff x · w + b ⌃ 0, where x · w denotes\\nthe dot product between x and w.\\n2.2\\nComplexity Classes\\nBoolean functions F mapping strings to strings whose output\\nis a single bit are called decision problems. We identify the\\ncomputational problem of computing F (i.e., given an input\\nstring x compute F(x)) with the problem of deciding whether\\nF(x) = 1. As an example of F, consider to take as input a\\ngraph G and a number k and return 1 whether there exists a\\nset of vertices each-other not adijacent of size at least k. This\\ndecision problem is known as INDEPENDENT SET.\\nWe recall the complexity classes used in the paper and, in\\nparticular, the deﬁnition of the classes PTIME (or, brieﬂy, P),\\nNP and coNP (see e.g.\\n[Papadimitriou, 1994]). P (resp.,\\nNP) contains the set of decision problems that can be solved\\nin polynomial time by a deterministic (resp., nondeterminis-\\ntic) Turing machine. Moreover, coNP is the complexity class\\ncontaining the complements of problems in NP. A problem p\\nis called hard for a complexity class C (denoted C-hard) if\\nthere exists a polynomial reduction from any problem in C to\\np. If p is both C-hard and belongs to C then it is C-complete.\\nClearly, P⌥ NP, P⌥ coNP and, under standard theoretical as-\\nsumptions, the three classes are assumed to be different.\\n3\\nEven-if Explanations\\nIn this section, we instantiate our framework on three im-\\nportant classes of boolean models and explainability queries.\\nSubsequently, we present our main theorems, facilitating a\\ncomparison of these models in terms of their interpretability.\\nWe start by recalling the notion of counterfactual, that is\\nthe explainability notion in the ‘if only’ case, whose com-\\nplexity has been investigated in [Barcel´o et al., 2020].\\nWe deﬁne the distance measure between two instances\\nx, y ⇤ {0, 1}n as the number of features where they differ.\\nFormally, d(x, y) = !n\\ni=1 |xi ⇥ yi| is the number of indexes\\ni ⇤ {1, . . . , n} (i.e., features) where x and y differ.\\nDeﬁnition 1 (Counterfactual). Given a pre-trained model M\\nand an instance x, an instance y is said to be a counterfac-\\ntual of x iff i) M(x) ̸= M(y), and ii) there exists no other\\ninstance z̸=y s.t. M(x) ̸= M(z) and d(x, z)<d(x, y).\\nExample 3. Continuing with our running example (see Fig-\\nure 1), for x1 = [0, 1, 1] we have that y3 = [1, 0, 1] is the only\\ncounterfactual of x1 w.r.t. M (herein, d(x1, y3) = 2).\\n2\\nThe following represents the natural decision version of the\\nproblem of ﬁnding a couterfactual for x [Barcel´o et al., 2020].\\nPROBLEM: MINIMUMCHANGEREQUIRED (MCR)\\nINPUT:\\nModel M, instance x, and k ⇤ N.\\nOUTPUT:\\nYES, if there exists an instance y with\\nd(x, y)  k and M(x) ̸= M(y);\\nNO, otherwise.\\nTheorem 1 ([Barcel´o et al., 2020]). MCR is i) in PTIME for\\nFBDDs and perceptrons, and ii) NP-complete for MLPs.\\nWe follow a standard assumption about the relationship\\nbetween model interpretability and computational complex-\\nity [Barcel´o et al., 2020] which states that a class A of mod-\\nels is more interpretable than another class B if the computa-\\ntional complexity of addressing post-hoc queries for models\\nin B is higher than for those in A.\\nUnder the lens of the aforementioned assumption, Theo-\\nrem 1 states that the class of models ‘perceptron’ and ‘FBDD’\\nis strictly more interpretable than the class ‘MLP’, as the\\ncomputational complexity of answering post-hoc queries for\\nmodels in the ﬁrst two classes is lower than for those in the\\nlatter. These results represent a principled way to conﬁrm\\nthe folklore belief that linear models are more interpretable\\nthan deep neural networks within the context of interpretabil-\\nity queries for counterfactuals.\\nAn open question is whether the same holds when dealing\\nwith post-hoc queries based on the ’even-if’ thinking setting,\\ni.e. on semifactuals. Before exploring this research question,\\nwe formally introduce the concept of semifactual.\\nDeﬁnition 2 (Semifactual). Given a pre-trained model M\\nand an instance x, an instance y is said to be a semifactual of\\nx iff i) M(x) = M(y), and ii) there exists no other instance\\nz̸=y s.t. M(x) = M(z) and d(x, z)>d(x, y).\\nSimilar to counterfactuals, the following problem is the de-\\ncision version of the problem of ﬁnding a semifactual of an\\ninstance x with a model M.\\nPROBLEM: MAXIMUMCHANGEALLOWED (MCA)\\nINPUT:\\nModel M, instance x, and k ⇤ N.\\nOUTPUT:\\nYES, if there exists an instance y with\\nd(x, y) ⌃ k and M(x) = M(y);\\nNO, otherwise.\\nThe complexity of MCA is as follows.\\nTheorem 2. MCA is i) in PTIME for FBDDs and percep-\\ntrons, and ii) NP-complete for MLPs.\\nProof. (Sketch) (Perceptron). MCA (optimization) problem\\nunder perceptrons can be formulated in ILP as follows:\\nmax\\nn!\\ni=1\\nzi subject to\\n⇥ !n\\ni=1 wi(2xi ⇥ 1)zi  !n\\ni=1 wixi\\nzi ⇤ {0, 1}, i = 1, . . . , n\\nwhere zi = 1 if we decide to change the feature i, 0 other-\\nwise, for each i = 1, · · · , n. Notice that the above formu-\\nlation corresponds to the standard (max-) Knapsack problem\\nwhere each item has value 1 and weight wi(2xi ⇥ 1), and the\\nbag has capacity !n\\ni=1 wixi. The Knapsack problem is, in\\nthe general case, NP-Hard [Papadimitriou, 1994]. However,\\nMCA corresponds to a special instance of Knapsack where\\nevery item has the same cost that can be solved in polynomial\\ntime with a greedy strategy, from which the result follows.\\n(FBDD). Let Mu be the FBDD obtained by restricting\\nM to the nodes that are (forward-)reachable from u and\\nmcau(x) = max{k⇥ | ⌦y. d(x, y) = k⇥ ↵ Mu(y)=M(x)}\\n(that can be comptuted in PTIME). For y maximizing k⇥ it\\nholds that yu′ ̸= xu′ holds ∀u⇥ from the root of M to u ex-\\ncluded. Let r be the root of M. Then, we can show that\\n(M, x, k) is a positive instance of MCA iff mcar(x) ⌃ k.\\n(MLP) The following algorithm provides the membership in\\nNP. Guess an instance y and check in PTIME that d(x, y)\\n⌃ k and M(x) = M(y). We prove the hardness with a poly-\\nnomial reduction from INDEPENDENT SET, which is known\\nto be NP-complete [Papadimitriou, 1994].\\nIt turns out that, under standard complexity assumptions,\\ncomputing semifactuals under perceptrons and FBDDs is eas-\\nier than under multi-layer perceptrons. Moreover, indepen-\\ndently of the type of the model, computing semifactuals is\\nas hard as computing counterfactuals (cf. Table 1). Thus,\\nperceptrons and FBDDs are strictly more interpretable than\\nMLPs, in the sense that the complexity of answering post-\\nhoc queries for models in the ﬁrst two classes is lower than\\nfor those in the latter.\\nModels of the form {0, 1}n ⇥ {0, 1}\\nFBDDs\\nPerceptrons\\nMLPs\\nMCR\\nPTIME\\nPTIME\\nNP-c\\nMCA\\nPTIME\\nPTIME\\nNP-c\\nBMCR\\nPTIME\\nPTIME\\nNP-c\\nBMCA\\nPTIME\\nPTIME\\nNP-c\\nCBMCR\\ncoNP\\ncoNP\\ncoNP-c\\nCBMCA\\ncoNP\\ncoNP\\ncoNP-c\\nL-CBMCR\\nPTIME\\nPTIME\\ncoNP-c\\nL-CBMCA\\nPTIME\\nPTIME\\ncoNP-c\\nTable 1: Complexity of explainability queries. For any class C, C-c\\nmeans C-complete. Grey-colored cells refer to known results.\\n4\\nPreferences over Explanations\\nThe problem of preference handling has been extensively\\nstudied in AI. Several formalisms have been proposed to ex-\\npress and reason with different kinds of preferences [Brafman\\nand Domshlak, 2009; Rossi et al., 2011; Santhanam et al.,\\n2016]. In the ﬁeld of logic programming under stable model\\nsemantics (a.k.a. Answer Set Programming - ASP), multi-\\nple formalisms introducing preferences over atoms to ﬁlter\\nout models have been suggested. In the following, in order\\nto express preferences over semifactuals and counterfactuals,\\nwe introduce a novel approach inspired to that proposed in\\n[Brewka et al., 2003] for ASP, whose semantics is based on\\nthe degree to which preference rules are satisﬁed.\\nSyntax.\\nInput instances are of the form x = (x1, ..., xn)\\nin {0, 1}n. Each xi (with i ⇤ [1, n]) represents the value of\\nfeature fi, and the (equality) atom fi = xi denotes that the\\nvalue of feature fi in x is equal to xi.\\nA simple preference is an expression of the form (fi =\\nx⇥\\ni) ≻ (fj = x⇥⇥\\nj ); it intuitively states that we prefer semi-\\nfactuals (resp., counterfactuals) where fi = x⇥\\ni w.r.t. those\\nwhere fj = x⇥⇥\\nj . To simplify the notation, an equality atom\\nof the form fi = 1 is written as a positive atom fi, whereas\\nan atom of the form fi = 0 is written as a negated atom ¬fi.\\nPositive and negated atoms are also called (feature) literals.\\nDeﬁnition 3 (Preference rule). Let I = {f1, ..., fn} be the\\nset of input features. A preference rule is of the form:\\n⇥1 ≻ · · · ≻ ⇥k ← ⇥k+1 ↵ · · · ↵ ⇥m\\n(1)\\nwith m ⌃ k ⌃ 2, and any ⇥i ⇤ {f1, ¬f1, . . . fn, ¬fn} is a\\n(feature) literal, with i ⇤ [1, n].\\nIn (1), ⇥1 ≻ · · · ≻ ⇥k is called head (or consequent),\\nwhereas ⇥k+1 ↵ · · · ↵ ⇥m is called body (or antecedent). We\\nassume that literals in the head are distinct. Intuitively, when-\\never ⇥k+1, · · · , ⇥m are true, then ⇥1 is preferred over ⇥2,\\nwhich is preferred over ⇥3, and so on until ⇥k−1 which is\\npreferred over ⇥k. As usual, when the body of the rule is\\nempty, the implication symbol ← is omitted.\\nDeﬁnition 4 (BCMP framework). A (binary classiﬁcation)\\nmodel with preferences (BCMP) framework is a pair (M, ≻)\\nwhere M is a model and ≻1 a set of preference rules over\\nfeatures of M.\\n1With a little abuse of notation, we use ⇤ to denote both a set of\\npreference and the preference relation among feature literals.\\nExample 4. The BCMP framework Λ1 = (M, {f1 ≻¬f2}),\\nwhere M is the model presented in Example 1, encodes the\\nuser’s preference speciﬁed in Example 2. Consider now the\\nframework Λ2 obtained from Λ1 by replacing the preference\\nrule with f1 ≻ ¬ f2 ← ¬f3. Intuitively, this preference rule\\nencodes a user preference for explanations where, whenever\\nthe work is not on-site (i.e., ¬f3 holds), (s)he prefers semifac-\\ntuals/counterfactuals where the contract is part-time (i.e., f1\\nholds) and, if this is not possible, (s)he prefers those where the\\nmortgage duration is not longer than 30 years (i.e., ¬f2). 2\\nSemantics.\\nThe preference rules determine a preference or-\\ndering ✏ on the semifactual/counterfactual explanations of an\\ninstance x w.r.t. a model M, as shown in what follows.\\nLet us consider a semifactual or counterfactual explanation\\ny and a preference ⇤ of the form (1), the following three sit-\\nuations are possible:\\n(a) the body of ⇤ is not satisﬁed in y;2\\n(b) the body of ⇤ is satisﬁed in y and at least one head literal\\nis true in y;\\n(c) the body of ⇤ is satisﬁed in y and none of the head liter-\\nals is satisﬁed in y.\\nIn the cases a) and b) we say that y satisﬁes ⇤ respectively\\nwith degree 1 and min({l | y satisﬁes ⇥l with l  k}) (de-\\nnoted as ⌅(y, ⇤)), while in case (c) we say that y does not sat-\\nisfy ⇤ and the associated degree is ⌅(y, ⇤) = +⇣. Intuitively,\\n⌅(y, ⇤) represents the position of the ﬁrst feature literal sat-\\nisﬁed in the ordered list provided in the head of a preference\\nrule; however, it can be 1 if y satisﬁes the ﬁrst literal ⇥1 or if\\nthe rule is irrelevant (case a). If the rule is relevant (the body\\nis satisﬁed) and no head literal is satisﬁed, then it is +⇣.\\nThus, given a BCMP framework (M, ≻), an instance x and\\ntwo (semifactual/counterfactual) explanations y and z for M\\nand x, we write y ✏ z iff ⌅(y, ⇤)  ⌅(z, ⇤) for all preference\\n⇤ in ≻, and write y ⇥ z iff y ✏ z and z ̸✏ y.\\nDeﬁnition 5 (Semantics). Given a BCMP framework (M, ≻)\\nand an instance x. We say that y is a best semifactual (resp.,\\ncounterfactual) explanation of x if y is a semifactual (resp.,\\ncounterfactual) of x and there is no other semifactual (resp.,\\ncounterfactual) z of x such that z ⇥ y.\\nExample 5. Continuing with Example 4, considering the in-\\nstances y1 = [0, 0, 0] and y2 = [1, 1, 0], we have that y1 and\\ny2 satisfy ⇤ = f1 ≻ ¬ f2 ← ¬f3 with degree ⌅(y1, ⇤) = 2\\nand ⌅(y2, ⇤) = 1 since the second and ﬁrst literal in the head\\nof ⇤ is satisﬁed in y1 and y2, respectively, and the body of ⇤\\nis satisﬁed by both of them. Then, y2 ⇥ y1, and thus y2 is\\nthe only best semifactual of x.\\n2\\n4.1\\nComputational Complexity\\nWe now investigate the complexity of several problems re-\\nlated to prioritized reasoning for explanations in order to\\ncompare model classes, even under prioritized reasoning.\\nWe start by introducing the following lemma that will be\\nused in the proof of the next theorems. Intuitively, the lemma\\nstates that checking whether y ✏ z can be done in PTIME.\\n2A positive (resp., negative) atom fi (resp., ¬fi) is satisﬁed in\\ny = (y1, . . . , yn) whenever yi = 1 (resp., yi = 0).\\nLemma 1. For any framework (M, ≻) and instances y, z of\\nM, deciding whether y ⇥ z can be done in PTIME.\\nThe next two problems ask if there exists a best explanation\\nin the even-if and if-only thinking, respectively.\\nPROBLEM: BESTMCR (BMCR)\\nINPUT:\\nBCMP framework (M, ≻), instance x,\\nand k ⇤ N.\\nOUTPUT:\\nYES, if there exists an instance y with\\nd(x, y)  k and M(x) ̸= M(y) that is\\nbest w.r.t. ✏; NO, otherwise.\\nPROBLEM: BESTMCA (BMCA)\\nINPUT:\\nBCMP framework (M, ≻), instance x,\\nand k ⇤ N.\\nOUTPUT:\\nYES, if there exists an instance y with\\nd(x, y) ⌃ k and M(x) = M(y) that is\\nbest w.r.t. ✏; NO, otherwise.\\nTheorem 3. BMCR and BMCA are i) in PTIME for FB-\\nDDs and perceptrons, and ii) NP-complete for MLPs.\\nThus, preferences do not make the existence problem\\nharder. Consider now the problems of checking whether a\\ngiven instance y is a best counterfactual or semifactual of x.\\nPROBLEM: CHECKBESTMCR (CBMCR)\\nINPUT:\\nBCMP (M, ≻),\\ninstances x,\\ny with\\nd(x, y) = k, and M(x) ̸= M(y).\\nOUTPUT:\\nYES, if there is no z with M(x) ̸= M(z)\\nand either d(x, z)  k ⇥ 1, or d(x, z) = k\\nand z ⇥ y; NO, otherwise\\nPROBLEM: CHECKBESTMCA (CBMCA)\\nINPUT:\\nBCMP (M, ≻),\\ninstances x,\\ny with\\nd(x, y) = k, and M(x) = M(y).\\nOUTPUT:\\nYES if there is no z with M(x) = M(z)\\nand either d(x, z) ⌃ k + 1 or d(x, z) = k\\nand z ⇥ y; NO, otherwise.\\nTheorem 4. CBMCR and CBMCA are i) in coNP for FB-\\nDDs and perceptrons, and ii) coNP-complete for MLPs.\\nWe now focus on a simpler yet expressive class of BCMPs\\nwhere a ﬁxed linear order is deﬁned over a subset of features.\\nDeﬁnition 6 (Linear BCMP framework). A linear (binary\\nclassiﬁcation) model with preferences framework is a pair\\n(M, ≻) where M is a model, and ≻ consists of a linear pref-\\nerence rule, that is a single preference rule of the form (1)\\nwith empty body.\\nExample 6. Considering Example 4, the BCMP framework\\nΛ1 is linear, whereas Λ2 is not.\\n2\\nWe use L-CBMCA and L-CBMCR to denote CBMCA\\nand CBMCR where the input BCMP is linear. The follow-\\ning theorem states that linear preferences do not increase the\\ncomplexity in the case of perceptrons and FBDDs, though\\nthey allow expressing user-speciﬁc desiderata among queries.\\nTheorem 5. L-CBMCA and L-CBMCR are in PTIME for\\nperceptrons and FBDDs.\\nAlgorithm 1 Computing a (best) semifactual for perceptrons\\nInput: Perceptron M = (W, b), instance x ⇤ {0, 1}n, and\\nlinear preference ⇤ = fp1 ≻ · · · ≻ fpl.\\nOutput: A best semifactual y for x w.r.t. M and ⇤.\\n1: Let s = [f1/s1, . . . , fn/sn] where ∀i ⇤ [1, n],\\nsi =2xiwi ⇥ wi if M(x)=1, wi ⇥ 2xiwi otherwise;\\n2: Let s⇥ = [fq1/sq1, ..., fqn/sqn] be the sorted version of s\\nin ascending order of si;\\n3: k = max({i ⇤ [0, n] | M(ﬂip(x, pos(s⇥, i))=M(x)});\\n4: if k = 0 return x;\\n5: if k = n return [1 ⇥ x1, . . . , 1 ⇥ xn];\\n6: y = ﬂip(x, pos(s⇥, k));\\n7: ⌅ = min({i ⇤ [1, l] | ypi = 1} ⌘ {l + 1});\\n8: for i ⇤ [1, ..., ⌅ ⇥ 1] do\\n9:\\nif ypi = 1 return y;\\n10:\\nLet j = q1 if xpi = ypi, j = qk+1 otherwise;\\n11:\\nz = ﬂip(y, {pi, j});\\n12:\\nif M(x) = M(z) return z;\\n13: return y;\\n4.2\\nComputation\\nWe now introduce two algorithms for computing a best semi-\\nfactual instance for perceptrons and FBDDs, respectively.\\nBoth algorithms take as input a model M, an instance x, and\\na linear preference ⇤, and returns a best semifactual explana-\\ntion. For the sake of the presentation, w.l.o.g., we assume that\\nhead literals are positive atoms (i.e., ⇥i =fi for all i).\\nWe start with Algorithm 1 for the perceptron model.\\nInitially, a list s of pairs feature/weight is built, where the\\neach weight takes into account the contribution of the associ-\\nated feature to the result (Line 1). At Line 2 the list is sorted in\\nascending order of weights, giving a new list s⇥. Next the fea-\\nture values of x are changed to get a semifactual instance y.\\nTo change a maximum number k of feature values of x, guar-\\nanteeing that the output of the model M(x) does not change,\\nthe order in s⇥ is followed. To this end, the following func-\\ntions are introduced: i) pos(s⇥, i), computing the set of the\\npositions in s of the ﬁrst i features in s⇥, and ii) ﬂip(y, B),\\nwith y = [y1, ..., yn], updating every element yi such that\\ni ⇤ B with the complementary value 1⇥yi. Notice that k = 0\\nmeans that x is the only semifactual for x, (returned at Line\\n4), whereas k = n means that [1⇥x1, . . . , 1⇥xn] is the only\\nsemifactual for x (returned at Line 5). At Line 7 the degree of\\nsatisfaction of ⇤ by y is computed; if no feature in ⇤ is satis-\\nﬁed by y the degree is l+1 (standing for +⇣). The next steps\\n(Lines 8-13) search for a better semifactual instance (if any).\\nThis is carried out by considering the ﬁrst ⌅ ⇥ 1 features in ⇤.\\nThus, for each feature fpi in ⇤ (i ⇤ [1, ⌅ ⇥ 1]) we have that:\\ni) if y satisﬁes fpi, y is a best semifactual and it is returned\\nat Line 9; ii) otherwise an alternative instance z satisfying\\nfeature fpi is generated and, if it is a semifactual then it is\\nreturned at Line 12. In particular, z is a semifactual instance\\nonly if d(z, x) = d(y, x) and M(z) = M(x). To guaran-\\ntee that d(z, x) = d(y, x) we either restore feature fq1 in y\\nby setting it back to yq1 = xq1 whenever xpi = ypi (recall\\nthat the ﬁrst k features of x are ﬂipped at Line 6), or change\\nthe feature fk+1 (i.e., yk+1 ̸= xk+1) whenever xpi ̸= ypi.\\nRoughly speaking, the so-obtained feature fj minimally con-\\ntributes to determine the value M(y) and keeps the distance\\nequal to k. Finally, if there exist no other semifactual z for x\\ns.t. z ⇥ y, then y is returned at Line 13.\\nExample 7. Consider the model M = step(x · [⇥2, 2, 0] +\\n1) of Example 1, the instance x = [0, 1, 1] and the linear\\npreference f1 ≻ f2. We have that s = [f1/2, f2/2, f3/0],\\ns⇥ = [f3/0, f2/2, f1/2] and k = 2. Thus y = [0, 0, 0] with\\n⌅ = 3. As y2 ̸= 1 and x2 ̸= y2, Algorithm 1 returns z =\\nﬂip(y, [2, j]) = [1, 1, 0] with j = q3 = 1 and ⌅ = 1.\\n2\\nWe now present Algorithm 2 for the FBDD model.\\nIt starts by creating a copy M⇥ of M where edge labels, as-\\nsigned by function λE′, represent boolean values correspond-\\ning to whether or not the edge is in line with x. Formally,\\nλE′(u, v) = 1 if xλV (u) = λE(u, v); 0 otherwise. Intuitively,\\nlet ⇧ be a path in M from root t to leaf nodes representing\\nM(x) (i.e., to ⌅ if M(x) = 1, ⇧ otherwise), and let w be\\nthe weight of ⇧ computed as the sum of (boolean) weights on\\nthe edges of ⇧. By following the path ⇧, it is possible to build\\na semifactual for x of distance n ⇥ w. To this end, at Line 2\\na graph N isomorphic to M⇥ w.r.t. V , is built with function\\nsubgraph by keeping all the paths in M⇥ ending in M(x)\\nand having minimum weight. All such paths are stored in ⇥\\nat Line 3. Then, the algorithm checks if it is possible to build\\na semifactual y for x satisfying fp1, otherwise fp2, and so on.\\nParticularly, assuming to be at step fpi for some fpi in ⇤, if\\nthere exists a path ⇧ ⇤ ⇥ and the feature fpi can be set to 1 in\\ny (that is the condition of Line 5) then a best semifactual y of\\nx is obtained from x by ﬂipping every features i of x not ap-\\npearing in ⇧ (i.e., λV (u) ̸= i for any node u in ⇧) or differing\\nin the assignment given by ⇧ (i.e., there is no edge (u, v) ⇤ ⇧\\ns.t. λV (u) = i and λE′(u, v) = 0). More formally, the func-\\ntion build(x, ⇧) returns the instance y = ﬂip(x, {i ⇤ [1, n] |\\n∄(u, v) ⇤ ⇧ such that λV (u)=i and λE′(u, v)=1}). Finally, if\\nthe algorithm does not return a semifactual at Line 6, then at\\nLine 8 it returns a semifactual y of x that satisﬁes none of the\\nfpis, obtained from x through function build(x, ⇧) where ⇧\\nis a path taken non-deterministically from ⇥.\\nExample 8. Consider the FBDD M = (V, E, λV , λE) in\\nFigure 2(left) for the mortgage scenario of Example 1. Let\\nx = [0, 1, 1] and ⇤ = f2 ≻ f1. For each edge (u, v) ⇤\\nE, Figure 2(center) shows the value λE′(u, v) (computed at\\nLine 1), while Figure 2(right) shows the graph N obtained\\nby removing all the paths ⇧ of M⇥ whose sum of weights is\\ngreater than w = 1. This means that semifactuals y of x are\\nat distance d(x, y) = n ⇥ w = 2. As there exists in N the\\npath ⇧ : (u, u⇥), (u⇥, u⇥⇥) where λV (u) = 3, λV (u⇥) = 2,\\nλV (u⇥⇥) = ⌅, and λE′(u⇥, u⇥⇥) = 1 then it is possible to get\\na semifactual y from x with x2 = 1. In fact, Algorithm 2\\nreturns y=build(x, ⇧)=ﬂip(x, {1, 3})=[1, 1, 0] at Line 6.\\n2\\nAs stated next, the two algorithms are sound, complete and\\nreturn best semifactuals in PTIME.\\nTheorem 6. Algorithms 1 and 2 are sound, complete, and\\nrun in polynomial-time.\\nClearly, the proposed algorithms also work when no pref-\\nerences are available, that is when ≻= ✓, returning a generic\\nAlgorithm 2 Computing a (best) semifactual for FBDDs\\nInput: FBDD M = (V, E, λV , λE) with root t, instance x ⇤\\n{0, 1}n, and linear preference ⇤ = fp1 ≻ · · · ≻ fpl.\\nOutput: A best semifactual y for x w.r.t. M and ⇤.\\n1: Let M⇥=(V ⇥=V, E⇥=E, λV ′=λV , λE′) be a copy of M,\\nwhere λE′(u, v)=1 if(xλV (u)=λE(u, v)), 0 otherwise;\\n2: Let N = subgraph(M⇥, M(x));\\n3: Let ⇥ be the set of paths in N from t to leaf nodes;\\n4: for fpi ⇤ {fp1, . . . , fpl} do\\n5:\\nif ⌦⇧ ⇤ ⇥ with y = build(x, ⇧) and ypi = 1\\n6:\\nreturn y;\\n7: Let ⇧ be a path of ⇥ taken non-deterministically;\\n8: return y = build(x, ⇧);\\nsemifactual at maximum distance. Providing a PTIME algo-\\nrithm returning a best semifactual for MLPs is unfeasible, as\\nbacked by our complexity analysis. However, as an heuristic,\\nwe could adapt Algorithm 1 so that the scores sis encode fea-\\nture importance, similarly to what is done in [Ramon et al.,\\n2020]—we plan to explore this direction in future work.\\nObserved that, although the number of (best) semifactu-\\nals is potentially exponential w.r.t. the number of features (it\\nis\\n⇤n\\nk\\n⌅\\nwhere k represents the maximum number of features\\nchanged in x to obtain semifactuals), Algorithms 1 and 2 can\\nbe exploited to obtain a ﬁnite representation of all semifac-\\ntuals. For instance, all semifactuals of x in Algorithm 2 are\\nthose obtained from x by considering all paths in ⇧ ⇤ ⇥, that\\nis the set {build(x, ⇧) such that ⇧ ⇤ ⇥}.\\nFinally, it is important to note that both Algorithms 1 and\\n2 can be easily adapted to deal with (best) counterfactual ex-\\nplanations, as shown in the Supplementary Material.\\n5\\nRelated Work\\nLooking for transparent and interpretable models has led to\\nthe exploration of several explanation paradigms in eXplain-\\nable AI (XAI). Factual explanations [Ciravegna et al., 2020;\\nGuidotti et al., 2018; Bodria et al., 2023; Wang et al., 2021;\\nCiravegna et al., 2023] elucidate the inner workings of AI\\nmodels by delineating why a certain prediction was made\\nbased on the input data. Counterfactual explanations [Der-\\nvakos et al., 2023; Romashov et al., 2022; Albini et al., 2020;\\nWu et al., 2019; Guidotti, 2022], delve into hypothetical sce-\\nnarios, revealing alternative input conﬁgurations that could\\nhave resulted in a different prediction. These explanations\\noffer insights into the model’s decision boundaries and aid in\\nunderstanding its behavior under varied circumstances. Semi-\\nfactual explanations [Dandl et al., 2023; Aryal and Keane,\\n2023; Kenny and Keane, 2021] bridge the gap between fac-\\ntual and counterfactual realms by presenting feasible alter-\\nations to the input data that do not change a decision out-\\ncome. This trichotomy of explanation types contributes sig-\\nniﬁcantly to the holistic comprehension and trustworthiness\\nof AI systems, catering to various stakeholders’ needs for\\ntransparency and interpretability.\\nMost existing works on\\nXAI focus on proposing novel methods for generating expla-\\n3\\n⊤\\n2\\n⇥\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n1\\n1\\n1\\n2\\n1\\n1\\nt\\n⊤\\nt\\n3\\n⊤\\n2\\n⇥\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n2\\n1\\n1\\nt\\n3\\n2\\n1\\nFigure 2: (Left) FBDD model M = (V, E, λV , λE) of Exam-\\nple 8 with root t and λV (t) = 3. (Center) FBDD model M′ =\\n(V ′, E′, λV ′, λE′) computed at Line 1 of Algorithm 2.\\n(Right)\\nGraph N obtained at Line 2 of Algorithm 2. Squared nodes rep-\\nresent leaf nodes (⌅ for M(·) = 1, and ⇧ for M(·) = 0).\\nnations, with few addressing the computational complexity\\nof related problems [Barcel´o et al., 2020; Arenas et al., 2022;\\nArenas et al., 2021; Eiben et al., 2023; El Harzli et al., 2023;\\nMarzari et al., 2023; Ordyniak et al., 2023]. However, none\\nof these works speciﬁcally focuses on semifactuals.\\nAs discussed earlier, several approaches have been pro-\\nposed to handle preferences in AI. In the context of logic\\nprogramming there has been extensive research on prefer-\\nences among rules [Brewka, 1989; Brewka and Eiter, 1999;\\nDelgrande et al., 2003; Eiter et al., 2003; Schaub and Wang,\\n2001] and among atoms or their combinations\\n[Sakama\\nand Inoue, 2000; Brewka et al., 2003; Greco et al., 2007;\\nBrewka et al., 2015].\\nRelated to this, conditional prefer-\\nence networks [Boutilier et al., 2004; Rossi et al., 2004]\\noffer an alternative formalism that allows to express sets of\\nconditional ceteris paribus (i.e. all else being equal) prefer-\\nence statements [Allen et al., 2017; Goldsmith et al., 2008;\\nLukasiewicz and Malizia, 2019].\\n6\\nConclusions and Future Works\\nIn this paper, after exploring local post-hoc interpretability\\nqueries related to semifactuals and their computational com-\\nplexity among three classes of models (FBDDs, perceptrons,\\nand MLPs), we introduced a framework that enables users\\nto personalize semifactual and counterfactual explanations\\nbased on preferences. Then, we investigated the complexity\\nof the proposed framework and presented PTIME algorithms.\\nOur\\napproach\\nto\\nhandle\\npreferences\\nover\\nsemifac-\\ntual/counterfactual explanations is inspired by the Answer Set\\nOptimization approach [Brewka et al., 2003]. In particular,\\nconditional preferences among features are deﬁned by means\\nof preference rules, the satisfaction of each rule is evaluated\\nquantitatively, whereas the overall preference order is deﬁned\\nin a qualitative way. The main advantages of such approach\\nconsist in the simplicity and compactness of representation,\\nand combination of quantitative and qualitative evaluations.\\nAs future work, we plan to explore additional interpretabil-\\nity queries, particularly focusing on counting problems like\\nquantifying the number of semifactuals/counterfactuals for\\nspeciﬁc input instances, aiming to enhance the evaluation of\\ninterpretability across the three classes of models. Further-\\nmore, we aim to extend our research by investigating more\\ninclusive model formats, speciﬁcally those accommodating\\nreal-number inputs and non-binary discrete features, thereby\\nexpanding the breadth of our analysis.\\nReferences\\n[Albini et al., 2020] Emanuele Albini, Antonio Rago, Pietro\\nBaroni, and Francesca Toni. Relation-based counterfactual\\nexplanations for bayesian network classiﬁers. In Proceed-\\nings of International Joint Conference on Artiﬁcial Intelli-\\ngence (IJCAI), pages 451–457, 2020.\\n[Allen et al., 2017] Thomas E. Allen, Judy Goldsmith, Hay-\\nden Elizabeth Justice, Nicholas Mattei, and Kayla Raines.\\nUniform random generation and dominance testing for cp-\\nnets. Journal of Artiﬁcial Intelligence Research, 59:771–\\n813, 2017.\\n[Arenas et al., 2021] Marcelo Arenas, Daniel Baez, Pablo\\nBarcel´o, Jorge P´erez, and Bernardo Subercaseaux. Foun-\\ndations of symbolic languages for model interpretability.\\nProceedings of Advances in Neural Information Process-\\ning Systems, 34:11690–11701, 2021.\\n[Arenas et al., 2022] Marcelo\\nArenas,\\nPablo\\nBarcel´o,\\nMiguel Romero Orth,\\nand Bernardo Subercaseaux.\\nOn computing probabilistic explanations for decision\\ntrees.\\nProceedings of Advances in Neural Information\\nProcessing Systems, 35:28695–28707, 2022.\\n[Aryal and Keane, 2023] Saugat Aryal and Mark T. Keane.\\nEven if explanations: Prior work, desiderata & bench-\\nmarks for semi-factual xai.\\nIn Proceedings of Interna-\\ntional Joint Conference on Artiﬁcial Intelligence (IJCAI),\\npages 6526–6535, 2023.\\n[Bakker et al., 2022] Michiel Bakker,\\nMartin Chadwick,\\nHannah Sheahan,\\nMichael Tessler,\\nLucy Campbell-\\nGillingham, Jan Balaguer, Nat McAleese, Amelia Glaese,\\nJohn Aslanides, Matt Botvinick, et al. Fine-tuning lan-\\nguage models to ﬁnd agreement among humans with di-\\nverse preferences. Proceedings of Advances in Neural In-\\nformation Processing Systems, 35:38176–38189, 2022.\\n[Barcel´o et al., 2020] Pablo Barcel´o, Mika¨el Monet, Jorge\\nP´erez, and Bernardo Subercaseaux. Model interpretabil-\\nity through the lens of computational complexity. In Pro-\\nceedings of Advances in Neural Information Processing\\nSystems, 2020.\\n[Bodria et al., 2023] Francesco Bodria,\\nFosca Giannotti,\\nRiccardo Guidotti, Francesca Naretto, Dino Pedreschi, and\\nSalvatore Rinzivillo. Benchmarking and survey of expla-\\nnation methods for black box models. Data Mining and\\nKnowledge Discovery, pages 1–60, 2023.\\n[Boutilier et al., 2004] Craig Boutilier, Ronen I. Brafman,\\nCarmel Domshlak, Holger H. Hoos, and David Poole. Cp-\\nnets: A tool for representing and reasoning with condi-\\ntional ceteris paribus preference statements.\\nJournal of\\nArtiﬁcial Intelligence Research, 21:135–191, 2004.\\n[Brafman and Domshlak, 2009] Ronen\\nI.\\nBrafman\\nand\\nCarmel Domshlak. Preference handling - an introductory\\ntutorial. AI Mag., 30(1):58–86, 2009.\\n[Brewka and Eiter, 1999] Gerhard\\nBrewka\\nand\\nThomas\\nEiter. Preferred answer sets for extended logic programs.\\nArtiﬁcial Intelligence, 109(1-2):297–356, 1999.\\n[Brewka et al., 2003] Gerhard Brewka, Ilkka Niemel¨a, and\\nMiroslaw Truszczynski. Answer set optimization. In Pro-\\nceedings of International Joint Conference on Artiﬁcial In-\\ntelligence (IJCAI), pages 867–872, 2003.\\n[Brewka et al., 2015] Gerhard Brewka, James P. Delgrande,\\nJavier Romero, and Torsten Schaub. asprin: Customizing\\nanswer set preferences without a headache. In Proceedings\\nof AAAI Conference on Artiﬁcial Intelligence, pages 1467–\\n1474, 2015.\\n[Brewka, 1989] Gerhard Brewka. Preferred subtheories: An\\nextended logical framework for default reasoning. In Pro-\\nceedings of International Joint Conference on Artiﬁcial In-\\ntelligence (IJCAI), pages 1043–1048, 1989.\\n[Ciravegna et al., 2020] Gabriele Ciravegna, Francesco Gi-\\nannini, Marco Gori, Marco Maggini, and Stefano Melacci.\\nHuman-driven fol explanations of deep learning. In Pro-\\nceedings of International Joint Conference on Artiﬁcial In-\\ntelligence (IJCAI), pages 2234–2240, 2020.\\n[Ciravegna et al., 2023] Gabriele Ciravegna, Pietro Barbi-\\nero, Francesco Giannini, Marco Gori, Pietro Li´o, Marco\\nMaggini, and Stefano Melacci. Logic explained networks.\\nArtiﬁcial Intelligence, 314:103822, 2023.\\n[Dandl et al., 2023] Susanne Dandl, Giuseppe Casalicchio,\\nBernd Bischl, and Ludwig Bothmann.\\nInterpretable re-\\ngional descriptors: Hyperbox-based local explanations. In\\nProceedings of Machine Learning and Knowledge Dis-\\ncovery in Databases, volume 14171, pages 479–495.\\nSpringer, 2023.\\n[Delgrande et al., 2003] James\\nP.\\nDelgrande,\\nTorsten\\nSchaub, and Hans Tompits. A framework for compiling\\npreferences in logic programs.\\nTheory and Practice of\\nLogic Programming, 3(2):129–187, 2003.\\n[Dervakos et al., 2023] Edmund\\nDervakos,\\nKonstantinos\\nThomas, Giorgos Filandrianos, and Giorgos Stamou.\\nChoose your data wisely:\\nA framework for semantic\\ncounterfactuals. arXiv preprint arXiv:2305.17667, 2023.\\n[Eiben et al., 2023] Eduard Eiben, Sebastian Ordyniak, Gia-\\ncomo Paesani, and Stefan Szeider. Learning small decision\\ntrees with large domain. In Proceedings of International\\nJoint Conference on Artiﬁcial Intelligence (IJCAI), pages\\n3184–3192, 2023.\\n[Eiter et al., 2003] Thomas Eiter, Wolfgang Faber, Nicola\\nLeone, and Gerald Pfeifer. Computing preferred answer\\nsets by meta-interpretation in answer set programming.\\nTheory and Practice of Logic Programming, 3(4-5):463–\\n498, 2003.\\n[El Harzli et al., 2023] Ouns El Harzli, Bernardo Cuenca\\nGrau, and Ian Horrocks. Cardinality-minimal explanations\\nfor monotonic neural networks. In Proceedings of Interna-\\ntional Joint Conference on Artiﬁcial Intelligence (IJCAI),\\npages 3677–3685, 2023.\\n[Goldsmith et al., 2008] Judy\\nGoldsmith,\\nJ´erˆome\\nLang,\\nMiroslaw Truszczynski, and Nic Wilson.\\nThe compu-\\ntational complexity of dominance and consistency in cp-\\nnets. Journal of Artiﬁcial Intelligence Research, 33:403–\\n432, 2008.\\n[Greco et al., 2007] Sergio Greco, Irina Trubitsyna, and Es-\\nter Zumpano. On the semantics of logic programs with\\npreferences. Journal of Artiﬁcial Intelligence Research,\\n30:501–523, 2007.\\n[Guidotti et al., 2018] Riccardo Guidotti, Anna Monreale,\\nSalvatore Ruggieri, Franco Turini, Fosca Giannotti, and\\nDino Pedreschi. A survey of methods for explaining black\\nbox models. ACM computing surveys (CSUR), 51(5):1–42,\\n2018.\\n[Guidotti, 2022] Riccardo Guidotti. Counterfactual explana-\\ntions and how to ﬁnd them: literature review and bench-\\nmarking. Data Mining and Knowledge Discovery, pages\\n1–55, 2022.\\n[Kahneman and Tversky, 1981] Daniel\\nKahneman\\nand\\nAmos Tversky.\\nThe simulation heuristic.\\nNational\\nTechnical Information Service, 1981.\\n[Kapoor et al., 2012] Ashish Kapoor, Bongshin Lee, Desney\\nTan, and Eric Horvitz. Performance and preferences: In-\\nteractive reﬁnement of machine learning procedures. In\\nProceedings of AAAI Conference on Artiﬁcial Intelligence,\\npages 1578–1584, 2012.\\n[Kenny and Huang, 2023] Eoin M. Kenny and Weipeng\\nHuang. The utility of “even if” semi-factual explanation\\nto optimise positive outcomes. In Thirty-seventh Confer-\\nence on Neural Information Processing Systems, 2023.\\n[Kenny and Keane, 2021] Eoin M Kenny and Mark T Keane.\\nOn generating plausible counterfactual and semi-factual\\nexplanations for deep learning. In Proceedings of AAAI\\nConference on Artiﬁcial Intelligence, pages 11575–11585,\\n2021.\\n[Lukasiewicz and Malizia, 2019] Thomas Lukasiewicz and\\nEnrico Malizia. Complexity results for preference aggre-\\ngation over (m)cp-nets: Pareto and majority voting. Artiﬁ-\\ncial Intelligence, 272:101–142, 2019.\\n[Marzari et al., 2023] Luca Marzari, Davide Corsi, Ferdi-\\nnando Cicalese, and Alessandro Farinelli.\\nThe #dnn-\\nveriﬁcation problem: Counting unsafe inputs for deep neu-\\nral networks. In Proceedings of International Joint Con-\\nference on Artiﬁcial Intelligence (IJCAI), pages 217–224,\\n2023.\\n[McCloy and Byrne, 2002] Rachel McCloy and Ruth MJ\\nByrne. Semifactual “even if” thinking. Thinking & rea-\\nsoning, 8:41–67, 2002.\\n[Ordyniak et al., 2023] S Ordyniak, G Paesani, and S Szei-\\nder. The parameterized complexity of ﬁnding concise lo-\\ncal explanations.\\nIn Proceedings of International Joint\\nConference on Artiﬁcial Intelligence (IJCAI), pages 3312–\\n3320, 2023.\\n[Papadimitriou, 1994] Christos H. Papadimitriou. Computa-\\ntional complexity. Addison-Wesley, 1994.\\n[Ramon et al., 2020] Yanou Ramon, David Martens, Fos-\\nter Provost, and Theodoros Evgeniou. A comparison of\\ninstance-level counterfactual explanation algorithms for\\nbehavioral and textual data: Sedc, lime-c and shap-c. Ad-\\nvances in Data Analysis and Classiﬁcation, 14:801–819,\\n2020.\\n[Romashov et al., 2022] Piotr Romashov, Martin Gjoreski,\\nKacper\\nSokol,\\nMaria\\nVanina\\nMartinez,\\nand\\nMarc\\nLangheinrich. Baycon: Model-agnostic bayesian counter-\\nfactual generator. In Proceedings of International Joint\\nConference on Artiﬁcial Intelligence (IJCAI), pages 23–\\n29, 2022.\\n[Rossi et al., 2004] Francesca Rossi, Kristen Brent Venable,\\nand Toby Walsh. mcp nets: Representing and reasoning\\nwith preferences of multiple agents. In Proceedings of the\\nNineteenth National Conference on Artiﬁcial Intelligence,\\nSixteenth Conference on Innovative Applications of Artiﬁ-\\ncial Intelligence, pages 729–734, 2004.\\n[Rossi et al., 2011] Francesca Rossi, Kristen Brent Venable,\\nand Toby Walsh. A Short Introduction to Preferences: Be-\\ntween Artiﬁcial Intelligence and Social Choice. Synthesis\\nLectures on Artiﬁcial Intelligence and Machine Learning.\\nMorgan & Claypool Publishers, 2011.\\n[Sakama and Inoue, 2000] Chiaki Sakama and Katsumi In-\\noue. Prioritized logic programming and its application to\\ncommonsense reasoning. Artiﬁcial Intelligence, 123(1-2),\\n2000.\\n[Santhanam et al., 2016] Ganesh Ram Santhanam, Samik\\nBasu, and Vasant G. Honavar. Representing and Reason-\\ning with Qualitative Preferences: Tools and Applications.\\nSynthesis Lectures on Artiﬁcial Intelligence and Machine\\nLearning. Morgan & Claypool Publishers, 2016.\\n[Schaub and Wang, 2001] Torsten Schaub and Kewen Wang.\\nA comparative study of logic programs with preference.\\nIn Proceedings of International Joint Conference on Arti-\\nﬁcial Intelligence (IJCAI), pages 597–602, 2001.\\n[Wang et al., 2021] Eric\\nWang,\\nPasha\\nKhosravi,\\nand\\nGuy Van den Broeck.\\nProbabilistic sufﬁcient explana-\\ntions. In Proceedings of International Joint Conference\\non Artiﬁcial Intelligence (IJCAI), pages 3082–3088, 2021.\\n[Wegener, 2004] Ingo Wegener.\\nBdds—design, analysis,\\ncomplexity, and applications. Discrete Applied Mathemat-\\nics, 138(1):229–251, 2004.\\n[Wu et al., 2019] Yongkai Wu, Lu Zhang, and Xintao Wu.\\nCounterfactual fairness: Unidentiﬁcation, bound and al-\\ngorithm. In Proceedings of International Joint Conference\\non Artiﬁcial Intelligence (IJCAI), pages 1438–1444, 2019.\\n[Zhu et al., 2022] Yongchun Zhu, Zhenwei Tang, Yudan Liu,\\nFuzhen Zhuang, Ruobing Xie, Xu Zhang, Leyu Lin, and\\nQing He.\\nPersonalized transfer of user preferences for\\ncross-domain recommendation. In Proceedings of the Fif-\\nteenth ACM International Conference on Web Search and\\nData Mining, pages 1507–1515, 2022.\\n'},\n",
       " {'abstract': 'Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a visual GUI agent – SeeClick, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding – the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance SeeClick with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created ScreenSpot, the first realistic GUI grounding dataset that encompasses mobile, desktop, and web environments. After pre-training, SeeClick demonstrates significant improvement in ScreenSpot over various baselines. Moreover, comprehensive evaluations on three widely used benchmarks consistently support our finding that advancements in GUI grounding directly correlate with enhanced performance in downstream GUI agent tasks.',\n",
       "  'introduction': 'Developing autonomous agents to assist humans on computing devices has been a persistent goal for artificial intelligence (Shi et al., 2017; Li et al., 2020a; Furuta et al., 2023; Zhou et al., 2023). These Graphical User Interface (GUI) agent systems aim to mimic human interactions in solving complex tasks, thereby enhancing efficiency and reducing manual effort, with examples like Siri and Copilot. Recent advances in Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023) and LLaMA (Touvron et al., 2023) have significantly propelled the evolution of GUI agents (Gur et al., 2023a; Zhou et al., 2023). These agents interact with the environment through extracted structured texts, e.g., HTML from web pages, then elicit LLM for planning, reasoning, and action gereration (Kim et al., 2023; Zheng et al., 2023).',\n",
       "  'literature_review': 'However, GUI agents depend on structured text face three inherent limitations: (1) Structured text is not always accessible, especially for iOS or desktop applications where acquiring such information is challenging (Shaw et al., 2023); (2) The verbose nature of structured text serves as an inefficient context for LLMs, while also omitting crucial information such as layout, images, and icons (Deng et al., 2023); (3) The variety of structured text - including HTML, DOM, and Android VH - necessitates the curation of task-specific observation and action spaces (Kim et al., 2023; Zhou et al., 2023). These entrenched deficiencies in text-based approaches call for an alternative solution.',\n",
       "  'methodology': 'In this paper, we propose a visual GUI agent built on Large Vision-Language Models (LVLMs) - SeeClick. Inspired by human interaction with GUIs, SeeClick performs low-level actions like clicking or typing directly by observing interface screenshots. This methodology bypasses the need for interacting with cumbersome structured text, empowering SeeClick as a universal visual agent suitable for various GUI platforms. Building such visual agents involves a foundational challenge: GUI grounding - the capacity to accurately locate screen elements based on instructions, which is absent in current LVLMs. To tackle this challenge, SeeClick enhances LVLM with a GUI grounding pre-training strategy. We devise a method to automate the curation of web grounding data and adapt public mobile UI datasets to obtain mobile grounding data.',\n",
       "  'results': 'SeeClick employs the above-curated dataset for continual pre-training of the LVLM, enabling it to accurately locate elements such as text, widgets, and icons in various GUI environments. Given GUI grounding is a fundamental yet underexplored capacity for GUI agents, we created ScreenSpot, the first realistic GUI grounding evaluation benchmark across various GUI platforms. ScreenSpot contains over 600 screenshots and 1200 instructions from iOS, Android, macOS, Windows, and web environments, and specifically includes both text-based elements and a variety of widgets and icons. Evaluation results confirm SeeClick’s superiority over current LVLMs, validating the effectiveness of GUI grounding pre-training.',\n",
       "  'conclusion': 'Finally, we adapted SeeClick to three downstream agent tasks: MiniWob (Shi et al., 2017), AITW (Rawles et al., 2023), and Mind2Web (Deng et al., 2023). As a purely visual-based agent, SeeClick achieves impressive performance across three tasks. Notably, SeeClick outperforms the visual baseline Pix2Act with only 0.3% training data on MiniWob. Moreover, experimental results on three tasks consistently support our finding that improvement in GUI grounding directly correlates with enhanced agent task performance.',\n",
       "  'title': 'SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents',\n",
       "  'author': 'Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, Zhiyong Wu',\n",
       "  'textdata': 'SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents\\nKanzhi Cheng♢♡∗\\nQiushi Sun♡\\nYougang Chu♢\\nFangzhi Xu♡\\nYantao Li♢\\nJianbing Zhang♢\\nZhiyong Wu♡\\n♢Department of Computer Science and Technology, Nanjing University\\n♡Shanghai AI Laboratory\\n{chengkz,chuyg,li_yantao}@smail.nju.edu.cn\\nqiushisun@u.nus.edu\\nfangzhixu98@gmail.com\\nzjb@nju.edu.cn\\nwuzhiyong@pjlab.org.cn\\nAbstract\\nGraphical User Interface (GUI) agents are de-\\nsigned to automate complex tasks on digital de-\\nvices, such as smartphones and desktops. Most\\nexisting GUI agents interact with the environ-\\nment through extracted structured data, which\\ncan be notably lengthy (e.g., HTML) and oc-\\ncasionally inaccessible (e.g., on desktops). To\\nalleviate this issue, we propose a visual GUI\\nagent – SeeClick, which only relies on screen-\\nshots for task automation. In our preliminary\\nstudy, we have discovered a key challenge in\\ndeveloping visual GUI agents: GUI grounding\\n– the capacity to accurately locate screen el-\\nements based on instructions. To tackle this\\nchallenge, we propose to enhance SeeClick\\nwith GUI grounding pre-training and devise\\na method to automate the curation of GUI\\ngrounding data. Along with the efforts above,\\nwe have also created ScreenSpot, the first real-\\nistic GUI grounding dataset that encompasses\\nmobile, desktop, and web environments. After\\npre-training, SeeClick demonstrates significant\\nimprovement in ScreenSpot over various base-\\nlines. Moreover, comprehensive evaluations\\non three widely used benchmarks consistently\\nsupport our finding that advancements in GUI\\ngrounding directly correlate with enhanced per-\\nformance in downstream GUI agent tasks. 1\\n1\\nIntroduction\\nDeveloping autonomous agents to assist humans on\\ncomputing devices has been a persistent goal for ar-\\ntificial intelligence (Shi et al., 2017; Li et al., 2020a;\\nFuruta et al., 2023; Zhou et al., 2023). These Graph-\\nical User Interface (GUI) agent systems aim to\\nmimic human interactions in solving complex tasks,\\nthereby enhancing efficiency and reducing manual\\neffort, with examples like Siri and Copilot. Re-\\ncent advances in Large Language Models (LLMs)\\nsuch as GPT-4 (OpenAI, 2023) and LLaMA (Tou-\\nvron et al., 2023) have significantly propelled the\\n∗Work done during internship at Shanghai AI Laboratory.\\n1The model, data and code are available at https://\\ngithub.com/njucckevin/SeeClick.\\nevolution of GUI agents (Gur et al., 2023a; Zhou\\net al., 2023). These agents interact with the en-\\nvironment through extracted structured texts, e.g.,\\nHTML from web pages, then elicit LLM for plan-\\nning, reasoning, and action gereration (Kim et al.,\\n2023; Zheng et al., 2023).\\nHowever, GUI agents depend on structured text\\nface three inherent limitations: (1) Structured text\\nis not always accessible, especially for iOS or desk-\\ntop applications where acquiring such information\\nis challenging (Shaw et al., 2023); (2) The verbose\\nnature of structured text serves as an inefficient\\ncontext for LLMs, while also omitting crucial in-\\nformation such as layout, images, and icons (Deng\\net al., 2023); (3) The variety of structured text -\\nincluding HTML, DOM, and Android VH - ne-\\ncessitates the curation of task-specific observation\\nand action spaces (Kim et al., 2023; Zhou et al.,\\n2023). These entrenched deficiencies in text-based\\napproaches call for an alternative solution.\\nIn this paper, we propose a visual GUI agent\\nbuilt on Large Vision-Language Models (LVLMs)\\n- SeeClick. Inspired by human interaction with\\nGUIs, SeeClick performs low-level actions like\\nclicking or typing directly by observing interface\\nscreenshots. This methodology bypasses the need\\nfor interacting with cumbersome structured text,\\nempowering SeeClick as a universal visual agent\\nsuitable for various GUI platforms. Building such\\nvisual agents involves a foundational challenge:\\nGUI grounding - the capacity to accurately locate\\nscreen elements based on instructions, which is ab-\\nsent in current LVLMs. To tackle this challenge,\\nSeeClick enhances LVLM with a GUI grounding\\npre-training strategy. We devise a method to auto-\\nmate the curation of web grounding data and adapt\\npublic mobile UI datasets to obtain mobile ground-\\ning data.\\nSeeClick employs the above-curated\\ndataset for continual pre-training of the LVLM, en-\\nabling it to accurately locate elements such as text,\\nwidgets, and icons in various GUI environments.\\narXiv:2401.10935v1  [cs.HC]  17 Jan 2024\\nGiven GUI grounding is a fundamental yet un-\\nderexplored capacity for GUI agents, we created\\nScreenSpot, the first realistic GUI grounding eval-\\nuation benchmark across various GUI platforms.\\nScreenSpot contains over 600 screenshots and 1200\\ninstructions from iOS, Android, macOS, Windows,\\nand web environments, and specifically includes\\nboth text-based elements and a variety of widgets\\nand icons. Evaluation results confirm SeeClick’s\\nsuperiority over current LVLMs, validating the ef-\\nfectiveness of GUI grounding pre-training.\\nFinally, we adapted SeeClick to three down-\\nstream agent tasks: MiniWob (Shi et al., 2017),\\nAITW (Rawles et al., 2023), and Mind2Web (Deng\\net al., 2023).\\nAs a purely visual-based agent,\\nSeeClick achieves impressive performance across\\nthree tasks. Notably, SeeClick outperforms the vi-\\nsual baseline Pix2Act with only 0.3% training data\\non MiniWob. Moreover, experimental results on\\nthree tasks consistently support our finding that\\nimprovement in GUI grounding directly correlates\\nwith enhanced agent task performance.\\nThis paper makes the following contributions:\\n• We develop a unified visual GUI agent\\nSeeClick, which directly performs clicking\\nand typing actions based on interface screen-\\nshots across diverse GUI platforms.\\n• We prospectively explore GUI grounding for\\nvisual GUI agents, and enhanced SeeClick\\nwith our proposed GUI grounding pre-training\\nstrategy.\\n• We create a realistic GUI grounding bench-\\nmark ScreenSpot, encompassing more than\\n1200 instructions from various GUI platforms.\\n• Experimental results on ScreenSpot and three\\nagent tasks demonstrate that enhancing agents’\\ngrounding capacity is key to improving per-\\nformance in downstream agent tasks.\\n2\\nRelated work\\n2.1\\nAutonomous GUI Navigation\\nAutonomous GUI navigation requires intelligent\\nagents to tackle complex tasks in various GUIs.\\nEarly research investigated training agents by\\nreinforcement learning (Liu et al., 2018; Gur\\net al., 2018) on the classic web environment Mini-\\nWob (Shi et al., 2017). With the recent advance-\\nment of LLMs (OpenAI, 2023; Touvron et al.,\\n2023; Xu et al., 2023; Sun et al., 2023; Kim et al.,\\n2023, inter alia). LLM-centric GUI agents have\\nbecome the dominant paradigm. A line of works\\nfocused on prompting closed-source LLMs like\\nChatGPT and GPT-4, leveraging their powerful\\nreasoning and planning abilities for web tasks, e.g.,\\nvia in-context learning (Zheng et al., 2023) and\\nself-refine (Kim et al., 2023). Other research ex-\\nplored training LLMs as specialized GUI agents\\n(Gur et al., 2023b; Furuta et al., 2023). Deng et al.\\n(2023) devised a two-stage method for identifying\\ntarget elements within intricate HTML. Recently,\\nGur et al. (2023a) designed a method for interacting\\nwith websites via synthesizing programs.\\nHowever, constrained by the inherent limitation\\nof LLMs to only process text and the complexity of\\nGUIs, converting interfaces into structured text is\\ncumbersome and challenging to standardize across\\nvarious GUI environments. To address this, several\\nrecent efforts have attempted vision-based GUI\\nnavigation using GPT-4V (Yan et al., 2023; Gao\\net al., 2023; Yang et al., 2023). Recent studies\\nexplored vision-based UI Agents for web (Shaw\\net al., 2023) or mobile (Zhan and Zhang, 2023).\\nIn this paper, we construct a universal visual GUI\\nagent SeeClick by customizing LVLMs, capable of\\noperating across various GUI platforms.\\n2.2\\nLarge Vision-Language Models\\nRecently, researchers have invested tremendous\\neffort in constructing LVLMs capable of jointly\\nprocessing image and text (Liu et al., 2023a; Zhu\\net al., 2023; Ye et al., 2023b; Li et al., 2023; Zhang\\net al., 2023a). LVLMs integrate visual encoding\\nmodules, like ViT (Dosovitskiy et al., 2020), with\\nLLMs such as LLaMA (Touvron et al., 2023), in-\\nheriting LLMs’ language and reasoning abilities to\\nperform diverse downstream vision-language tasks.\\nA series of studies focused on the grounding ca-\\npabilities of LVLMs (Wang et al., 2023; Bai et al.,\\n2023; Chen et al., 2023a), such as providing bound-\\ning boxes for objects while generating responses\\n(Chen et al., 2023b; Peng et al., 2023). Nonethe-\\nless, these efforts primarily addressed natural im-\\nages and did not explore GUI contexts. This paper\\nemphasizes that GUI grounding is pivotal for con-\\nstructing visual GUI agents and concentrates on\\nexploring LVLMs’ potential in GUI environments.\\n2.3\\nMobile UI Understanding\\nAs a vital aspect of daily life, mobile UI under-\\nstanding has garnered widespread attention from\\nVision\\nEncoder\\n(ViT)\\nLarge-scale Vision-Language Model (LVLM)\\nVision-Language \\nAdapter\\nInstruction:\\n“View the new album of Jony J”\\nNext action: click (0.49, 0.40)\\nMobile UI Related\\nWeb UI Related\\nGeneral VL Data\\nVQA\\nVisual Reasoning\\nWidget Captioning\\nUI Summarization\\nMobile UI Grounding\\nWeb OCR\\nWeb UI Grounding\\nInstruction: open the\\nlow power mode\\nSource: Mobile (iOS)\\nType: Icon/Widget\\nInstruction: See more \\noptions for Dark Mode\\nSource: Mobile (Android)\\nType: Text\\nInstruction: Change font \\nsize to 20\\nSource: PC (macOS)\\nType: Text\\nInstruction: Likes on this\\nissue\\nSource: Web (Development)\\nType: Icon/Widget\\nInstruction: Create a new\\nmerge request\\nSource: Web (Development)\\nType: Text\\nInstruction: Switch to \\nOneDrive path\\nSource: PC (Windows)\\nType: Text\\nGUI Grounding Benchmark: ScreenSpot\\n(a) Overview of SeeClick‘s framework and GUI grounding pre-training.\\n(b) Examples of the proposed GUI grounding benchmark ScreenSpot.\\n(c) SeeClick as a visual GUI agent in downstream task.\\nInstruction: Find a list of shorthaired dogs available for adoption with 100 miles of zip code \\n94587 that are good with kids and cats, and have been on Petfinder for over 30 days.\\nFigure 1: Overview of our universal visual GUI agent SeeClick. (a) depicts the framework of SeeClick and GUI\\ngrounding pre-training. (b) provides examples of ScreenSpot across various GUIs and types of instructions. (c)\\ndisplays the real-world application of SeeClick on downstream web agent task.\\nresearchers. One category of work explored mul-\\ntimodal tasks within UI contexts (Li et al., 2021;\\nBai et al., 2021; He et al., 2021), such as widget\\ncaptioning (Li et al., 2020b), screen summarization\\n(Wang et al., 2021), and UI object detection (Chen\\net al., 2020). Several studies explored grounding\\nelements in mobile UI based on instructions (Li\\net al., 2020a; Burns et al., 2022; Li et al., 2021; Li\\nand Li, 2022), serving as a foundation for mobile\\nUI automation. Most mentioned studies rely on\\nAndroid’s View Hierarchy to represent interfaces,\\nwhile (Li and Li, 2022) use Regions of Interest and\\n(Zhang et al., 2023b) use screenshots. Moreover,\\nmost efforts employed RICO (Deka et al., 2017)\\nfor research, an Android apps dataset collected in\\n2017. This work introduces an up-to-date, real-\\nistic grounding benchmark encompassing various\\nGUI platforms, and underscores grounding as a\\nfoundation for visual GUI agents.\\n3\\nApproach\\nIn our preliminary study, we have discovered a key\\nchallenge in developing visual GUI agents: GUI\\ngrounding, the capacity to locate screen elements\\nbased on instructions. Recent advancements in\\nLVLMs have claimed visual grounding capability\\non natural images (Chen et al., 2023b; Peng et al.,\\n2023; Wang et al., 2023), which is able to point\\nout objects based on natural language description.\\nHowever, GUI screenshots significantly differ from\\nnatural images, typically containing dense text and\\nnumerous icons and widgets. These differences\\nimpair existing LVLMs’ grounding performance in\\nGUI contexts (Bai et al., 2023; Chen et al., 2023a),\\nlimiting their potential as visual GUI agents.\\nThis paper seeks to endow LVLMs with GUI\\ngrounding proficiency, aiming to construct a univer-\\nsal visual GUI agent capable of executing instruc-\\ntions that only rely on screenshots. The overview of\\nthe proposed LVLM SeeClick is presented in Fig-\\nure 1(a). Next, we introduce the birth of SeeClick,\\nincluding the formalization of GUI grounding task,\\nthe construction of continual pre-training data, and\\ntraining details.\\n3.1\\nGUI grounding for LVLMs\\nAs GUI grounding is the core capability of\\nSeeClick, we first elucidate how to train LVLM for\\nlanguage generation to perform grounding tasks.\\nGiven an interface screenshot s and a collection of\\nits elements {(xi, yi)|i}, where xi denotes the tex-\\ntual description of the i-th element and yi indicates\\nthe element’s location (represented as a bounding\\nbox or point). As depicted in Figure 1(a), LVLM\\npredicts the location of the element y based on the\\ninterface screenshot s and its textual description x,\\ni.e. calculating p(y|s, x).\\nA potential challenge is how LVLMs predict\\nnumerical coordinates in a language generation for-\\nmat. Traditional methods (Chen et al., 2021; Wang\\net al., 2023; Shaw et al., 2023) divided the image\\ninto 1000 bins, and creating a new 1,000-token\\nvocabulary {< p0 >, < p1 >, ..., < p999 >} to\\nrepresent the x and y coordinates. In this work,\\nwe adopt a more intuitive manner used in LVLMs\\n(Chen et al., 2023b; Bai et al., 2023), treating nu-\\nmerical values as natural language without any ad-\\nditional tokenization or pre-/post-processing. For\\ninstance, in Figure 1(a), for a smartphone screen-\\nshot and the instruction “View the new album of\\nJony J”, we craft a user query prompt: “In the\\nUI, where should I click if I want to <instruc-\\ntion>?”. Subsequently, we normally compute the\\ncross-entropy loss between the model output and\\nthe groundtruth “click (0.49, 0.40)” to optimize the\\nLVLM.\\n<div class=\"header\">\\n    <ul class=\"menu\">\\n<li>...</li>\\n</ul>\\n</div>\\n<div class=\"container\">\\n    <div class=“product-thumbnails”><a href=“#” title=“Previous image\"></a></div>\\n    <div class=\"product-detail\">\\n<div>...</div>\\n        <button>ENQUIRE NOW</button>\\n        <div class=“product-share”>…</div>\\n    </div>\\n</div>\\nFigure 2: Example of two types of elements automati-\\ncally collected from the webpage.\\n3.2\\nData Construction\\nWe primarily train SeeClick using three collections\\nof data: web UI data crawled from the internet,\\nmobile UI data reorganized from public datasets,\\nand general vision-language instruction-following\\ndata.\\nWeb Data. Web UIs, featuring a variety of lay-\\nouts and design styles across websites, are ideal for\\ntraining LVLMs to learn universal recognition, un-\\nderstanding, and grounding capabilities across dif-\\nferent GUIs. We collected approximately 300k web\\npages from the latest Common Crawl repository 2\\nto serve as our training data for web UI. For each\\nwebpage, we collect two types of elements from\\nthe HTML code: (1) elements that display visible\\ntext content; and (2) elements with a special “title”\\nattribute that display descriptive text when hover-\\ning. This method ensures that we gather a series of\\ninteractable elements and their corresponding text\\ninstructions, while encompassing a wide range of\\ntexts and icons. Figure 2 provides an example of\\na web page, its associated simplified HTML code\\nand extracted two type elements. Next, we treat\\nthe text or “title” content of each element as the\\ninstruction, constructing the GUI grounding data\\nwith screenshot s, instruction x, and the element’s\\nlocation y. We also include an inverse task, similar\\nto web OCR, where given an interface screenshot s\\nand the location y of an element, the model predicts\\n2https://commoncrawl.org/\\nthe text description x of the element.\\nMobile Data. For mobile UI, we include three\\ntypes of data: widget captioning, mobile UI ground-\\ning, and mobile UI summarization. Widget Cap-\\ntioning aims to automatically generate language\\ndescriptions for mobile UI elements; for example,\\nproducing the description “play music” for the play\\nbutton on a music player interface. We utilize the\\ntraining split of the dataset provided in (Li et al.,\\n2020b), containing nearly 20k screenshots, 40k\\nwidgets and 100k descriptions. We derive mobile\\nUI grounding data by inverting the widget caption-\\ning task, treating language descriptions as instruc-\\ntions and predicting widget locations from screen-\\nshots and instructions. To improve element diver-\\nsity, we also incorporate the automatically gener-\\nated elements and instructions from RICO (Li et al.,\\n2020a). The mobile UI grounding data involve di-\\nverse elements and instructions, aids SeeClick in\\ngeneralizing grounding capabilities to various GUI\\nscenarios. We finally include mobile UI summa-\\nrization data (Wang et al., 2021) to enhance overall\\ninterface comprehension.\\nGeneral Data. To maintain LVLM’s general ca-\\npacities on natural images and its planning and\\nreasoning abilities for downstream agent tasks, we\\nincorporate general vision-language data for train-\\ning SeeClick.\\nWe utilize the 158k instruction-\\nfollowing samples collected with GPT-4 from\\nLLaVA (Liu et al., 2023a), covering conversation,\\ndetailed description, and complex reasoning.\\nFinally, we mix the data above and craft 30 task-\\nspecific prompts for each added GUI task, resulting\\nin a 1M dataset to train SeeClick.\\n3.3\\nTraining Details\\nWe built SeeClick through continual pre-training\\non a recent advanced LVLM, Qwen-VL (Bai et al.,\\n2023).\\nQwen-VL inherently possesses ground-\\ning capabilities and a higher resolution (448*448),\\nwhich has been proven beneficial for understand-\\ning text-rich images (Lee et al., 2023; Ye et al.,\\n2023a). We train Qwen-VL on the constructed\\ndataset (Section 3.2) for about 10k steps (around\\n1 epoch) to obtain our GUI base model SeeClick.\\nDuring training, we employ LoRA (Hu et al., 2021)\\nto fine-tune both the visual encoder and LLM. We\\nadopt AdamW as the optimizer and use a cosine\\nannealing scheduler with an init learning rate of\\n3e-5 and a global batch size of 64. All training\\ntakes around 24 hours on 8 NVIDIA A100 GPUs.\\n21%\\n21%\\n13%\\n15%\\n8%\\n6%\\n6%\\n10%\\niOS\\nAndroid\\nWindows\\nmacOS\\nDevelopment\\nShopping\\nForum\\nTools\\nScreenSpot\\n278\\n198\\n210\\n232\\n140\\n151\\nDifferent types of elements \\nin ScreenSpot\\nText\\nIcon/Widget\\nMobile\\nDesktop\\nWeb\\nFigure 3: Statistic of our proposed GUI grounding\\nbenchmark ScreenSpot. The left illustrates the diverse\\nGUI environments included. The right displays the\\ntypes of elements within each GUI category.\\n4\\nGUI Grounding Benchmark:\\nScreenSpot\\nWe recognize GUI grounding proficiency as essen-\\ntial for constructing a universal visual GUI agent.\\nHowever, due to the constrained capabilities of\\nearlier vision-language models, previous research\\nhas paid limited attention to GUI grounding, with\\nthe few existing studies (Li et al., 2021; Li and\\nLi, 2022; Zhang et al., 2023b) largely confined to\\nAndroid environments.\\nTo address this research gap, we introduce\\nScreenSpot, an up-to-date, realistic grounding eval-\\nuation benchmark encompassing various GUI plat-\\nforms. It is designed to assess vision-language\\nmodels’ ability to locate elements based on human\\ninstructions (Figure 1(b) provides some examples).\\nScreenSpot has two distinctive features: (1) Vari-\\nous GUI platforms. It includes over 600 interface\\nscreenshots from mobile (iOS, Android), desktop\\n(macOS, Windows), and web platforms, along with\\nmore than 1200 instructions and corresponding ac-\\ntionable elements (as shown in Figure 3 left); (2)\\nIcons/Widgets. Considering that locating icons or\\nwidgets might be more challenging than text ele-\\nments on GUIs, we include a substantial number\\nof icon/widget type elements in each GUI (as de-\\npicted in Figure 3 right). See Appendix A for more\\nexamples.\\nTo evaluate vision-language models’ effective-\\nness in real-world scenarios, ScreenSpot is care-\\nfully curated to ensure the samples in ScreenSpot\\nare novel and not included in existing training re-\\nsources. We recruited experienced annotators to\\ncollect new GUI interfaces and label instructions\\nalong with the bounding boxes for actionable ele-\\nTable 1: Results of different LVLMs on ScreenSpot. The best results in each column are bold. Benefit from efficient\\nGUI grounding pre-training, SeeClick significantly enhanced LVLMs’ ability to locate GUI elements following\\ninstructions, and surpassed the strong baseline CogAgent with a smaller model size.\\nLVLMs\\nModel\\nSize\\nGUI\\nSpecific\\nMobile\\nDesktop\\nWeb\\nAverage\\nText Icon/Widget Text Icon/Widget Text Icon/Widget\\nMiniGPT-v2\\n7B\\n✗\\n7.6%\\n5.2%\\n4.5%\\n2.9%\\n5.5%\\n0.0%\\n4.3%\\nQwen-VL\\n9.6B\\n✗\\n9.4%\\n4.3%\\n6.4%\\n5.3%\\n0.0%\\n0.0%\\n4.2%\\nFuyu\\n8B\\n✓\\n40.6%\\n1.6%\\n33.6%\\n6.7%\\n48.4%\\n2.9%\\n22.3%\\nCogAgent\\n18B\\n✓\\n66.5%\\n26.7%\\n73.7%\\n19.3%\\n78.0%\\n21.4%\\n47.6%\\nSeeClick\\n9.6B\\n✓\\n75.9%\\n48.7%\\n72.7%\\n26.4%\\n69.2%\\n21.4%\\n52.4%\\nments. For mobile and desktop, annotators were\\nasked to select commonly used apps and opera-\\ntions; for web pages, we chose several types of\\nwebsites (development, shopping, forum, and tools)\\nfrom the recent web environment WebArena (Zhou\\net al., 2023) for annotation.\\n5\\nExperiments\\nIn this section, we first evaluate the GUI ground-\\ning capabilities of representative LVLMs and our\\nproposed SeeClick on ScreenSpot. Subsequently,\\nwe adapt SeeClick to three downstream GUI agent\\ntasks, exploring and analyzing the performance and\\npotential of vision-based agents in various GUIs.\\n5.1\\nGUI Grounding on ScreenSpot\\nAs the foundation of visual GUI agents, GUI\\ngrounding has not received extensive attention\\nin current LVLMs evaluations (Liu et al., 2023b;\\nYu et al., 2023). Therefore, we compare various\\nLVLMs on our specially constructed GUI ground-\\ning benchmark ScreenSpot, to examine their poten-\\ntial for GUI automation.\\nCompared LVLMs & Evaluation. We primar-\\nily evaluated two types of LVLMs: (1) versatile\\ngeneral LVLMs capable of tasks such as dialogue,\\nrecognition and grounding, including MiniGPT-\\nv2 (Chen et al., 2023a) and Qwen-VL (Bai et al.,\\n2023); (2) recent LVLMs specifically designed for\\nGUI tasks, including Fuyu (Bavishi et al., 2023),\\nCogAgent (Hong et al., 2023) and our SeeClick.\\nConsidering that GUI agents require clicking on\\nthe correct position, we calculate the click accu-\\nracy as the metric, defined as the proportion of test\\nsamples where the model’ predicted location falls\\nin the groundtruth element bounding box (Zhang\\net al., 2018; Li et al., 2022; Zhang et al., 2023b).\\nWe take the models’ generated point or the cen-\\nter of the generated bounding box as the predicted\\nclick location.\\nResults.\\nAs shown in Table 1, while general\\nLVLMs have excelled in natural image grounding\\n(e.g., RefCOCO (Kazemzadeh et al., 2014)), their\\nGUI grounding performance on ScreenSpot is poor\\ndue to the significant differences between GUIs\\nand natural images.\\nAccordingly, LVLMs trained specifically on\\nGUI data have significant improvements.\\nOur\\nSeeClick achieved the best average performances\\nacross three GUI platforms and two types of el-\\nements, even with fewer model parameters and\\nGUI-related training data than CogAgent. This\\ndemonstrates the efficiency of our GUI grounding\\npre-training; with the rich UI elements and diverse\\ninstructions collected from the web and mobile,\\nSeeClick quickly learns to understand human in-\\nstructions and locate elements, even in completely\\nunseen scenarios like iOS and desktop. Notably, all\\nmodels struggle with locating icons/widgets, high-\\nlighting the difficulty of identifying and grounding\\nnon-text elements on GUIs, and showcasing the\\nunique challenges posed by ScreenSpot.\\n5.2\\nVisual GUI Agent Tasks\\nThis section explores the capabilities of SeeClick\\nwhen migrated to three downstream GUI agent\\ntasks: MiniWob, AITW, and Mind2Web. Across\\nthese tasks, SeeClick emulates human behavior,\\ndetermining the next action solely by observing in-\\nterface screenshots, without needing any additional\\nGUI information. On all three tasks, we trained\\nSeeClick on the respective training splits and tested\\nit on the test sets. Appendix B provides examples\\nof SeeClick’s interactions in these tasks.\\n5.2.1\\nMiniWob\\nMiniWob (Shi et al., 2017) comprises about 100\\ntypes of web automation tasks, where the agent\\nClick-button-sequence\\nClick-link\\nClick-pie\\nClick-button\\nClick-tab-2\\nClick-collapsible-2\\nClick-tab-2-hard\\nUnicode-test\\nText-transform\\nTic-tac-toe\\nClick-checkboxes\\nClick-option\\nClick-widget\\nClick-shape\\nClick-test-2\\nNavigate-tree\\nUse-slider\\nClick-checkboxes-transfer\\nClick-dialog-2\\nSimple-algebra\\nChoose-date\\nClick-color\\nClick-dialog\\nClick-shades\\nClick-checkboxes-large\\nClick-collapsible\\nClick-tab\\nClick-test\\nSimple-arithmetic\\nClick-checkboxes-soft\\nCount-shape\\nUse-antocomplete\\nGrid-coordinate\\nEnter-date\\nIdentify-shape\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nClick-button-sequence\\nClick-link\\nClick-pie\\nClick-button\\nClick-tab-2\\nClick-collapsible-2\\nClick-tab-2-hard\\nUnicode-test\\nText-transform\\nTic-tac-toe\\nClick-checkboxes\\nClick-option\\nClick-widget\\nClick-shape\\nClick-test-2\\nNavigate-tree\\nUse-slider\\nClick-checkboxes-transfer\\nClick-dialog-2\\nSimple-algebra\\nChoose-date\\nClick-color\\nClick-dialog\\nClick-shades\\nClick-checkboxes-large\\nClick-collapsible\\nClick-tab\\nClick-test\\nSimple-arithmetic\\nClick-checkboxes-soft\\nCount-shape\\nUse-antocomplete\\nGrid-coordinate\\nEnter-date\\nIdentify-shape\\nSeeClick improvement over LVLM baseline Qwen-VL\\nQwen-VL\\nSeeClick\\nSeeClick is better\\nQwen-VL is better\\nFigure 4: Comparison of SeeClick and LVLM baseline Qwen-VL on MiniWob. Tasks with yellow shadows indicate\\nthose with dynamic webpage layouts and element positions (closer to the real-world application of GUI agents, see\\nmore details in Appendix B.1). SeeClick outperformed Qwen-VL in most tasks, especially in those with dynamic\\nelements, highlighting the effectiveness of our GUI grounding pre-training.\\nis asked to interact with a simplified web environ-\\nment to accomplish human instructions. Each task\\ncan templatize random variants and corresponding\\ninstructions controlled by a random seed, creating\\nup to billions of possible task instances.\\nExisting open-source training data often lacks\\ncorresponding interface screenshots (Furuta et al.,\\n2023).\\nTherefore, we rollout 50 successful\\nepisodes using an LLM strategy for each task in\\n(Zheng et al., 2023), resulting in a 2.8K episodes\\ndataset to train SeeClick.\\nCompared Methods & Evaluation. We com-\\npared SeeClick with a range of offline training\\nmethods. Among these, the state-of-the-art method\\nWebGUM (Furuta et al., 2023) uses screenshots as\\nauxiliary information but still selects HTML ele-\\nments as actions. Pix2Act (Shaw et al., 2023) is the\\nonly prior vision-based approach, trained with ex-\\ntensive data to perform clicking and typing actions\\non MiniWob. To verify the effectiveness of GUI\\ngrounding pre-training, we also report the results\\nof the LVLM baseline Qwen-VL when trained with\\nthe same 2.8K dataset.\\nDue to the variance in evaluation task sets among\\ndifferent methods (Liu et al., 2018; Furuta et al.,\\n2023; Shaw et al., 2023), for fairness, we report per-\\nformance in two groups based on the overlapping\\nMiniWob tasks with their approaches. We compute\\nthe success rate over 50 random seeds for each task,\\nand then compute the mean over all MiniWob tasks\\nas the final score.\\nResults. As depicted in Table 2, our purely vision-\\nbased approach SeeClick surpassed a series of\\nHTML-based and image-based baselines with only\\na fraction of the training data. Notably, with an\\nequivalent amount of training data (2.8K), SeeClick\\noutperformed the previous offline sota WebGUM,\\nTable 2: Average scores of different methods on Mini-\\nWob. The best results in each setting are bold. Methods\\nachieving the highest performance with limited data are\\nunderlined. SeeClick outperforms a range of offline\\ntraining methods as a purely vision-based model.\\nMethods\\nModality\\nDataset\\nScore\\nCompared with text-based models over 45 tasks\\nCC-Net (SL)\\nDOM+Image\\n2.4M\\n35.6%\\nWebN-T5\\nHTML\\n12K\\n55.2%\\nMM-WebN-T5 HTML+Image 347K\\n63.4%\\nWebGUM\\nHTML+Image\\n2.8K\\n65.5%\\nWebGUM\\nHTML+Image 347K\\n86.1%\\nSeeClick\\nImage\\n2.8K\\n73.0%\\nCompared with vision-based models over 35 tasks\\nCC-Net (SL)\\nImage\\n2.4M\\n23.4%\\nPix2Act\\nImage\\n1.3M\\n64.6%\\nQwen-VL\\nImage\\n2.8K\\n48.4%\\nSeeClick\\nImage\\n2.8K\\n68.0%\\nwhich uses both HTML and screenshots as input.\\nIn comparison with visual approaches, thanks to\\nthe powerful reasoning and planning abilities of the\\nLVLM and the localization skill imparted by our\\nGUI grounding pre-training, SeeClick exceeded\\nthe sota visual method Pix2Act, using less than\\n0.3% training data.\\nFurthermore, SeeClick significantly surpassed\\nthe LVLM baseline Qwen-VL by 20 percentage\\npoints, verifying that enhancing GUI grounding ca-\\npabilities plays a crucial role in improving LVLMs’\\nperformance on MiniWob tasks. To analyze in de-\\ntail, we provide task-level comparisons between\\nSeeClick and Qwen-VL on 35 MiniWob tasks in\\nTable 3: Average scores of different methods on AITW. ClickAcc calculates the accuracy of click operation. The\\nbest results in each column are bold. SeeClick exhibits the best performance among both API-based LLMs (top\\nrow) and trained LVLMs (bottom row).\\nMethods\\nModality General Install GoogleApps Single WebShopping Overall ClickAcc\\nChatGPT-CoT\\nText\\n5.9\\n4.4\\n10.5\\n9.4\\n8.4\\n7.7\\n-\\nPaLM2-CoT\\nText\\n-\\n-\\n-\\n-\\n-\\n39.6\\n-\\nGPT-4V\\nImage\\n41.7\\n42.6\\n49.8\\n72.8\\n45.7\\n50.5\\n-\\nQwen-VL\\nImage\\n53.3\\n61.2\\n56.3\\n61.7\\n52.0\\n56.9\\n60.1\\nSeeClick\\nImage\\n56.0\\n64.5\\n57.7\\n63.6\\n57.3\\n59.8\\n68.4\\nFigure 4. SeeClick shows notable improvements\\nover Qwen-VL in most tasks, particularly in those\\nwhere interface layouts and element positions dy-\\nnamically change.\\nThis confirms our hypothe-\\nsis that general LVLMs struggle with accurately\\nclicking on interfaces, and our GUI grounding pre-\\ntraining significantly enhances this capability, lead-\\ning to better performance.\\n5.2.2\\nAITW\\nWe evaluate the effectiveness of SeeClick in smart-\\nphone environments with a recently proposed An-\\ndroid UI automation dataset Android In The Wild\\n(AITW). It encompasses 30k distinct task instruc-\\ntions and corresponding 715k human-annotated\\noperation episodes. Previous approaches (Rawles\\net al., 2023; Zhan and Zhang, 2023) split AITW\\nepisode-wise into train/val/test (80/10/10%). How-\\never, with an average of over 20 episodes per\\ninstruction in AITW, there is a significant risk\\nof overfitting due to:\\n(1) extensive repetition\\nin training trajectories; (2) instructions in the\\ntest set have appeared during training.\\nIn this\\nwork, we propose to split the training and test-\\ning sets by instruction.\\nSpecifically, we se-\\nlected 545/688/306/700/700 instructions from Gen-\\neral/Install/GoogleApps/Single/WebShopping re-\\nspectively and retained only one annotated trajec-\\ntory for each instruction. Then for each scenario,\\nwe selected 80% for training and the remaining for\\ntesting. This setting better reflects the performance\\nof the trained agent in the real-world application.\\nCompared Methods & Evaluation. We com-\\npare SeeClick with two types of baselines: (1)\\nAPI-based LLMs employing in-context learning\\nor Chain-of-Thought (CoT), such as ChatGPT-CoT\\n(Zhan and Zhang, 2023), PaLM2-CoT (Rawles\\net al., 2023) and the latest GPT-4V (Yan et al.,\\n2023); (2) our own trained LVLM baseline Qwen-\\nVL. We follow Rawles et al. (2023) to adopt the\\nscreen-wise action matching score as the main met-\\nrics, and compute the click accuracy (ClickAcc),\\nwhich calculates the accuracy when both the refer-\\nence and prediction are click actions.\\nResults.\\nAs illustrated in Table 3, SeeClick\\nachieved the best average performance among both\\nAPI-based LLMs and trained LVLMs. Specifically,\\nSeeClick exhibited an 8 percentage point improve-\\nment in click accuracy over the LVLM baseline\\nQwen-VL, substantiating the hypothesis that GUI\\ngrounding significantly augments the precision of\\nclick actions in visual GUI agents, thereby improv-\\ning their overall task performance.\\n5.2.3\\nMind2Web\\nTo assess SeeClick’s capabilities in web en-\\nvironments, we utilize the recently introduced\\nMini2Web dataset (Deng et al., 2023). Mind2Web\\ncomprises over 2000 open-ended tasks collected\\nfrom 137 real websites, each with a high-level in-\\nstruction and a human action trajectory sequence in-\\nvolving clicking, selecting and typing. Mind2Web\\nwas originally designed to evaluate text-based web\\nagents, focusing on identifying actionable elements\\nby selecting them from simplified HTML doc-\\numents. This work explores visual web agents\\nthat predict click positions directly from inter-\\nface screenshots. For this purpose, we augmented\\nMind2Web’s trajectory data with interface screen-\\nshots and bounding boxes for action elements at\\neach step, sourced from their raw dump data (Deng\\net al., 2023). To the best of our knowledge, this\\nis the first attempt of web agents relying solely on\\nscreenshots as inputs for navigating real websites.\\nCompared Methods & Evaluation. We com-\\npare SeeClick with a range of text-based web\\nagents (Deng et al., 2023) and our LVLM base-\\nline Qwen-VL. Mind2Act employs a two-stage\\nmethod, where a small LM first generates candi-\\ndate elements from raw HTML, and then a large\\nTable 4: Comparsion of different methods on Mind2Web. The best results in each column are bold. Improvements\\nof SeeClick over the LVLM baseline are underline. The GUI grounding pre-training nearly doubled the step success\\nrate of SeeClick on real websites.\\nMethods\\nw/o HTML\\nCross-Task\\nCross-Website\\nCross-Domain\\nEle.Acc Op.F1 Step SR Ele.Acc Op.F1 Step SR Ele.Acc Op.F1 Step SR\\nMindAct (gen)\\n✗\\n20.2\\n52.0\\n17.5\\n13.9\\n44.7\\n11.0\\n14.2\\n44.7\\n11.9\\nMindAct\\n✗\\n55.1\\n75.7\\n52.0\\n42.0\\n65.2\\n38.9\\n42.1\\n66.5\\n39.6\\nGPT-3.5\\n✗\\n20.3\\n56.6\\n17.4\\n19.3\\n48.8\\n16.2\\n21.6\\n52.8\\n18.6\\nGPT-4\\n✗\\n41.6\\n60.6\\n36.2\\n35.8\\n51.1\\n30.1\\n37.1\\n46.5\\n26.4\\nQwen-VL\\n✓\\n14.9\\n86.2\\n12.6\\n12.1\\n83.7\\n10.1\\n9.6\\n84.3\\n8.0\\nSeeClick\\n✓\\n26.3\\n86.2\\n23.7\\n21.9\\n82.9\\n18.8\\n22.1\\n84.1\\n20.2\\nLM selects the target element using multi-choice\\nQA. Mind2Act (gen) modifies the QA module to\\ndirectly generate the target element instead. GPT-\\n3.5 and GPT-4 adopt the same multiple-choice QA\\nformulation as MindAct, and include three demon-\\nstration examples for in-context learning.\\nWe calculate element accuracy (Ele.Acc), Oper-\\nation F1 (Op.F1) and step success rate (Step SR).\\nFor vision-based methods, a prediction is consid-\\nered correct if the predicted coordinate falls in the\\ntarget element’s bounding box. All other settings\\nare following (Deng et al., 2023).\\nResults. As displayed in Table 4, SeeClick nearly\\ndoubled the element selection accuracy and step\\nsuccess rate compared to the LVLM baseline Qwen-\\nVL. This indicates that improvements in GUI\\ngrounding directly correlate with enhanced perfor-\\nmance in downstream web agent tasks. Further-\\nmore, text-based methods often have lower overall\\nOp.F1 due to nearly 20% groundtruth elements\\nbeing incorrectly filtered out in the candidate gen-\\neration stage, which leads to a zero Op.F1.\\nAlthough SeeClick can operate without extra\\nHTML information, its performance lags behind\\nstate-of-the-art HTML-based methods, as predict-\\ning click locations on web pages is much more dif-\\nficult than choosing from HTML candidates. This\\nunderscores the challenge of grounding in complex\\ninterfaces, suggesting substantial room for improve-\\nments toward a visual GUI agent that is applicable\\nin real-world scenarios.\\n6\\nConclusion\\nIn this paper, we introduce a visual GUI agent -\\nSeeClick, which only relies on screenshots for GUI\\ntask automation. We found a key challenge in de-\\nveloping such visual GUI agents: GUI grounding\\n- the capacity to accurately locate screen elements\\nbased on human instructions. To address this chal-\\nlenge, we propose to enhance SeeClick via GUI\\ngrounding pre-training, and devise methods to au-\\ntomate the curation of GUI grounding data from\\nweb and mobile. For benchmarking the progress\\nin GUI grounding, we created ScreenSpot, the first\\nrealistic evaluation dataset encompassing mobile,\\ndesktop, and web platforms. Results on ScreenSpot\\ndemonstrate a significant improvement of SeeClick\\nover LVLM baselines. Moreover, comprehensive\\nevaluations across three GUI automation tasks con-\\nsistently support our finding that advancements in\\nGUI grounding directly correlated with improved\\nperformance in downstream agent tasks.\\nReferences\\nChongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas\\nSunkara, Abhinav Rastogi, Jindong Chen, et al.\\n2021.\\nUibert: Learning generic multimodal rep-\\nresentations for ui understanding. arXiv preprint\\narXiv:2107.13731.\\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\\nand Jingren Zhou. 2023. Qwen-vl: A frontier large\\nvision-language model with versatile abilities. arXiv\\npreprint arXiv:2308.12966.\\nRohan Bavishi,\\nErich Elsen,\\nCurtis Hawthorne,\\nMaxwell Nye, Augustus Odena, Arushi Somani, and\\nSa˘gnak Ta¸sırlar. 2023. Introducing our multimodal\\nmodels.\\nAndrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha\\nKumar, Kate Saenko, and Bryan A Plummer. 2022.\\nA dataset for interactive vision-language navigation\\nwith unknown command feasibility. In European\\nConference on Computer Vision, pages 312–328.\\nSpringer.\\nJieshan Chen, Mulong Xie, Zhenchang Xing, Chun-\\nyang Chen, Xiwei Xu, Liming Zhu, and Guoqiang Li.\\n2020. Object detection for graphical user interface:\\nOld fashioned or deep learning or a combination? In\\nproceedings of the 28th ACM joint meeting on Eu-\\nropean Software Engineering Conference and Sym-\\nposium on the Foundations of Software Engineering,\\npages 1202–1214.\\nJun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\\nLiu, Pengchuan Zhang, Raghuraman Krishnamoor-\\nthi, Vikas Chandra, Yunyang Xiong, and Mohamed\\nElhoseiny. 2023a. Minigpt-v2: large language model\\nas a unified interface for vision-language multi-task\\nlearning. arXiv preprint arXiv:2310.09478.\\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\\nFeng Zhu, and Rui Zhao. 2023b. Shikra: Unleashing\\nmultimodal llm’s referential dialogue magic. arXiv\\npreprint arXiv:2306.15195.\\nTing Chen, Saurabh Saxena, Lala Li, David J Fleet, and\\nGeoffrey Hinton. 2021. Pix2seq: A language model-\\ning framework for object detection. In International\\nConference on Learning Representations.\\nBiplab Deka, Zifeng Huang, Chad Franzen, Joshua Hi-\\nbschman, Daniel Afergan, Yang Li, Jeffrey Nichols,\\nand Ranjitha Kumar. 2017. Rico: A mobile app\\ndataset for building data-driven design applications.\\nIn Proceedings of the 30th annual ACM symposium\\non user interface software and technology, pages\\n845–854.\\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen,\\nSamuel Stevens, Boshi Wang, Huan Sun, and Yu Su.\\n2023. Mind2web: Towards a generalist agent for the\\nweb. arXiv preprint arXiv:2306.06070.\\nAlexey\\nDosovitskiy,\\nLucas\\nBeyer,\\nAlexander\\nKolesnikov,\\nDirk Weissenborn,\\nXiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\\nAn image is worth 16x16 words: Transformers\\nfor image recognition at scale.\\nIn International\\nConference on Learning Representations.\\nHiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yu-\\ntaka Matsuo, Shixiang Shane Gu, and Izzeddin\\nGur. 2023.\\nMultimodal web navigation with\\ninstruction-finetuned foundation models.\\narXiv\\npreprint arXiv:2305.11854.\\nDifei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran\\nLi, Dongxing Mao, Qinchen Wu, Weichen Zhang,\\nPeiyi Wang, Xiangwu Guo, et al. 2023. Assistgui:\\nTask-oriented desktop graphical user interface au-\\ntomation. arXiv preprint arXiv:2312.13108.\\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa\\nSafdari, Yutaka Matsuo, Douglas Eck, and Alek-\\nsandra Faust. 2023a. A real-world webagent with\\nplanning, long context understanding, and program\\nsynthesis. arXiv preprint arXiv:2307.12856.\\nIzzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa\\nSafdari, Austin V Huang, Aakanksha Chowdhery,\\nSharan Narang, Noah Fiedel, and Aleksandra Faust.\\n2023b. Understanding html with large language mod-\\nels. In ICLR 2023 Workshop on Mathematical and\\nEmpirical Understanding of Foundation Models.\\nIzzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and\\nDilek Hakkani-Tur. 2018. Learning to navigate the\\nweb. In International Conference on Learning Rep-\\nresentations.\\nZecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu,\\nLijuan Liu, Nevan Wichers, Gabriel Schubiner, Ruby\\nLee, and Jindong Chen. 2021. Actionbert: Leverag-\\ning user actions for semantic understanding of user\\ninterfaces. In Proceedings of the AAAI Conference\\non Artificial Intelligence, pages 5931–5938.\\nWenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng\\nXu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang,\\nYuxiao Dong, Ming Ding, et al. 2023. Cogagent: A\\nvisual language model for gui agents. arXiv preprint\\narXiv:2312.08914.\\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\\net al. 2021. Lora: Low-rank adaptation of large lan-\\nguage models. In International Conference on Learn-\\ning Representations.\\nSahar Kazemzadeh, Vicente Ordonez, Mark Matten,\\nand Tamara Berg. 2014. Referitgame: Referring to\\nobjects in photographs of natural scenes. In Proceed-\\nings of the 2014 conference on empirical methods in\\nnatural language processing (EMNLP), pages 787–\\n798.\\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer.\\n2023. Language models can solve computer tasks.\\narXiv preprint arXiv:2303.17491.\\nKenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexi-\\nang Hu, Fangyu Liu, Julian Martin Eisenschlos, Ur-\\nvashi Khandelwal, Peter Shaw, Ming-Wei Chang,\\nand Kristina Toutanova. 2023. Pix2struct: Screen-\\nshot parsing as pretraining for visual language under-\\nstanding. In International Conference on Machine\\nLearning, pages 18893–18912. PMLR.\\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. 2023.\\nOtter: A\\nmulti-modal model with in-context instruction tuning.\\narXiv preprint arXiv:2305.03726.\\nGang Li and Yang Li. 2022. Spotlight: Mobile ui under-\\nstanding using vision-language models with a focus.\\nIn The Eleventh International Conference on Learn-\\ning Representations.\\nLiunian Harold Li, Pengchuan Zhang, Haotian Zhang,\\nJianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan\\nWang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.\\n2022. Grounded language-image pre-training. In\\nProceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition, pages 10965–\\n10975.\\nYang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason\\nBaldridge. 2020a. Mapping natural language instruc-\\ntions to mobile ui action sequences. arXiv preprint\\narXiv:2005.03776.\\nYang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li,\\nand Zhiwei Guan. 2020b. Widget captioning: Gener-\\nating natural language description for mobile user in-\\nterface elements. arXiv preprint arXiv:2010.04295.\\nYang Li, Gang Li, Xin Zhou, Mostafa Dehghani, and\\nAlexey Gritsenko. 2021.\\nVut: Versatile ui trans-\\nformer for multi-modal multi-task user interface mod-\\neling. arXiv preprint arXiv:2112.05692.\\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tian-\\nlin Shi, and Percy Liang. 2018. Reinforcement learn-\\ning on web interfaces using workflow-guided explo-\\nration. In International Conference on Learning Rep-\\nresentations.\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\\nLee. 2023a. Visual instruction tuning. In Neural\\nInformation Processing Systems.\\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\\nWang, Conghui He, Ziwei Liu, et al. 2023b. Mm-\\nbench: Is your multi-modal model an all-around\\nplayer? arXiv preprint arXiv:2307.06281.\\nOpenAI. 2023. GPT-4 technical report.\\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,\\nShaohan Huang, Shuming Ma, and Furu Wei.\\n2023.\\nKosmos-2: Grounding multimodal large\\nlanguage models to the world.\\narXiv preprint\\narXiv:2306.14824.\\nChristopher Rawles, Alice Li, Daniel Rodriguez, Oriana\\nRiva, and Timothy Lillicrap. 2023. Android in the\\nwild: A large-scale dataset for android device control.\\narXiv preprint arXiv:2307.10088.\\nPeter Shaw, Mandar Joshi, James Cohan, Jonathan Be-\\nrant, Panupong Pasupat, Hexiang Hu, Urvashi Khan-\\ndelwal, Kenton Lee, and Kristina Toutanova. 2023.\\nFrom pixels to ui actions: Learning to follow instruc-\\ntions via graphical user interfaces. In Advances in\\nNeural Information Processing Systems.\\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Her-\\nnandez, and Percy Liang. 2017. World of bits: An\\nopen-domain platform for web-based agents. In In-\\nternational Conference on Machine Learning, pages\\n3135–3144. PMLR.\\nQiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu,\\nXipeng Qiu, and Lingpeng Kong. 2023.\\nCorex:\\nPushing the boundaries of complex reasoning\\nthrough multi-model collaboration. arXiv preprint\\narXiv:2310.00280.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro,\\nFaisal Azhar, et al. 2023. Llama: Open and effi-\\ncient foundation language models. arXiv preprint\\narXiv:2302.13971.\\nBryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi\\nGrossman, and Yang Li. 2021. Screen2words: Au-\\ntomatic mobile ui summarization with multimodal\\nlearning. In The 34th Annual ACM Symposium on\\nUser Interface Software and Technology, pages 498–\\n510.\\nWenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan\\nWu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong\\nLu, Jie Zhou, Yu Qiao, et al. 2023.\\nVision-\\nllm: Large language model is also an open-ended\\ndecoder for vision-centric tasks.\\narXiv preprint\\narXiv:2305.11175.\\nFangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei\\nYuan, Shuai Yuan, Qika Lin, Yu Qiao, and Jun Liu.\\n2023. Symbol-llm: Towards foundational symbol-\\ncentric interface for large language models. arXiv\\npreprint arXiv:2311.09278.\\nAn Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin,\\nLinjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong,\\nJulian McAuley, Jianfeng Gao, et al. 2023.\\nGpt-\\n4v in wonderland: Large multimodal models for\\nzero-shot smartphone gui navigation. arXiv preprint\\narXiv:2311.07562.\\nZhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Ze-\\nbiao Huang, Bin Fu, and Gang Yu. 2023. Appa-\\ngent: Multimodal agents as smartphone users. arXiv\\npreprint arXiv:2312.13771.\\nJiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye,\\nMing Yan, Guohai Xu, Chenliang Li, Junfeng Tian,\\nQi Qian, Ji Zhang, et al. 2023a. Ureader: Univer-\\nsal ocr-free visually-situated language understanding\\nwith multimodal large language model. In The 2023\\nConference on Empirical Methods in Natural Lan-\\nguage Processing.\\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,\\nMing Yan, Yiyang Zhou, Junyang Wang, An-\\nwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023b.\\nmplug-owl: Modularization empowers large lan-\\nguage models with multimodality. arXiv preprint\\narXiv:2304.14178.\\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan\\nWang. 2023. Mm-vet: Evaluating large multimodal\\nmodels for integrated capabilities. arXiv preprint\\narXiv:2308.02490.\\nZhuosheng Zhan and Aston Zhang. 2023. You only\\nlook at screens: Multimodal chain-of-action agents.\\narXiv preprint arXiv:2309.11436.\\nJianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan\\nBrandt, Xiaohui Shen, and Stan Sclaroff. 2018. Top-\\ndown neural attention by excitation backprop. Inter-\\nnational Journal of Computer Vision, 126(10):1084–\\n1102.\\nPan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao\\nXu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding,\\nSongyang Zhang, Haodong Duan, Hang Yan, et al.\\n2023a.\\nInternlm-xcomposer: A vision-language\\nlarge model for advanced text-image comprehension\\nand composition. arXiv preprint arXiv:2309.15112.\\nZhizheng Zhang, Wenxuan Xie, Xiaoyi Zhang, and\\nYan Lu. 2023b. Reinforced ui instruction ground-\\ning: Towards a generic ui task automation api. arXiv\\npreprint arXiv:2310.04716.\\nLongtao Zheng, Rundong Wang, Xinrun Wang, and\\nBo An. 2023.\\nSynapse: Trajectory-as-exemplar\\nprompting with memory for computer control. In\\nNeurIPS 2023 Foundation Models for Decision Mak-\\ning Workshop.\\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,\\nRobert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan\\nBisk, Daniel Fried, Uri Alon, et al. 2023. Webarena:\\nA realistic web environment for building autonomous\\nagents. arXiv preprint arXiv:2307.13854.\\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\\nvision-language understanding with advanced large\\nlanguage models. arXiv preprint arXiv:2304.10592.\\nA\\nExamples of ScreenSpot\\nFigure 5 provides more examples of ScreenSpot,\\nwhich contains a variety of common GUI scenarios\\nfor mobile, desktop, and web platforms.\\nB\\nCase Study\\nB.1\\nMiniWob\\nFigure 6(a) illustrates the difference between static\\nand dynamic layout tasks. Static layout tasks have\\nfixed element positions during training and testing,\\nwhile dynamic layout tasks display varying inter-\\nfaces and element positions. Figure 6(b) provides\\nexamples of SeeClick’s interaction with MiniWob.\\nSeeClick relies solely on the interface screenshot\\nfor arithmetic, reasoning, etc.\\nB.2\\nAITW\\nFigure 7 provides an example of SeeClick when\\ncompleting an AITW task. Predictions marked\\nin red below indicate that they were computed as\\nincorrect in AITW. However, some errors occur\\nbecause the current step’s answer is not unique.\\nFor example in step 5, the model’s predicted input\\n\"DuckDuckGo Privacy Browser\" is also a poten-\\ntially correct action.\\nB.3\\nMind2Web\\nFigure 8 shows several examples of SeeClick\\non the real-world website benchmark Mind2Web.\\nSeeClick can comprehend instructions and click on\\nthe correct elements within complex interfaces.\\nInstruction: My \\naccount\\nSource: Mobile\\n(iOS)\\nType: \\nIcon/Widget\\nInstruction: \\nRemove maps \\nfrom the \\nDesktop\\nSource: Mobile\\n(iOS)\\nType: \\nIcon/Widget\\n \\nInstruction: \\nDisallow \\nautomatic app \\nupdates\\nSource: Mobile\\n(iOS)\\nType: \\nIcon/Widget\\n \\nInstruction: \\nSearch event\\nSource: Mobile\\n(iOS)\\nType: Text\\nInstruction: \\nScan QR code\\nSource: \\nMobile\\n(Android)\\nType: \\nIcon/Widget\\n \\nInstruction: \\nContinue\\nSource: Mobile\\n(Android)\\nType: Text \\nInstruction: \\nDisplay 15-day \\nweather \\nforecast\\nSource: Mobile\\n(Android)\\nType: Text \\nInstruction: \\nFold input \\nmethod\\nSource: Mobile\\n(Android)\\nType: \\nIcon/Widget\\nInstruction: \\nCreate a new\\ndocument\\nSource: PC\\n(macOS)\\nType: Text \\nInstruction: \\nEnlarge font \\nsize\\nSource: PC\\n(macOS)\\nType:\\nIcon/Widget\\nInstruction: \\nAdd subtitle\\nSource: PC\\n(Windows)\\nType: Text \\nInstruction: Go \\nto Beauty & \\nPersonal Care\\nSource: Web\\n(Shop)\\nType: Text \\nInstruction: Set \\nReminder\\nSource: Web\\n(Development)\\nType: \\nIcon/Widget\\nInstruction: Reply\\nto the first post \\nSource: Web\\n(Forum)\\nType: Text \\nInstruction: Pause the\\ndebugger\\nSource: PC (macOS)\\nType: Icon/Widget\\nInstruction:\\nZoom in on \\nthe map \\nSource: Web\\n(Tools)\\nType: \\nIcon/Widget \\nInstruction: \\nOpen Fax\\nSource: PC\\n(Windows)\\nType: Text\\nFigure 5: More examples of GUI grounding benchmark ScreenSpot.\\n{“action_type”: 4, “click_point”:\\n(0.58, 0.3)}\\n{“action_type”: 4, “click_point”:\\n(0.25, 0.4)}\\n{“action_type”: 4, “click_point”:\\n(0.25, 0.4)}\\n{“action_type”: 4, “click_point”:\\n(0.46, 0.55)}\\n{“action_type”: 4, “click_point”:\\n(0.47, 0.47)}\\n{“action_type”: 4, “click_point”:\\n(0.5, 0.62)}\\n{“action_type”: 4, “click_point”:\\n(0.71, 0.78)}\\n{“action_type”: 4, “click_point”:\\n(0.81, 0.38)}\\n{“action_type”: 3, “typed_text”:\\n“36”}\\n{“action_type”: 4, “click_point”:\\n(0.50, 0.64)}\\n{“action_type”: 4, “click_point”:\\n(0.69, 0.8)}\\n{“action_type”: 4, “click_point”:\\n(0.31, 0.8)}\\n{“action_type”: 4, “click_point”:\\n(0.31, 0.49)}\\n{“action_type”: 4, “click_point”:\\n(0.14, 0.3)}\\n{“action_type”: 4, “click_point”:\\n(0.13, 0.81)}\\n{“action_type”: 4, “click_point”:\\n(0.19, 0.76)}\\n(a) Comparison between static layout (left, click-color) and dynamic layout (right, unicode-test).\\nTask: simple-arithmetic\\nTask: click-pie\\nTask: choose-date\\n···\\n(b) Example episodes of SeeClick on MiniWob tasks.\\nFigure 6: Example episodes of SeeClick on MiniWob. The model’s prediction output is below the screenshot, with\\naction_type 4 indicating clicking and action_type 3 indicating typing.\\n{“action_type”: 6)}\\n{“action_type”: 4, “click_point”:\\n(0.12, 0.79)}\\n{“action_type”: 4, “click_point”:\\n(0.81, 0.07)}\\n{“action_type”: 4, “click_point”:\\n(0.93, 0.06)}\\n{“action_type”: 3, “typed_text”:\\n“DuckDuckGo Privacy Browser”}\\n{“action_type”: 4, “click_point”:\\n(0.29, 0.12)}\\n{“action_type”: 4, “click_point”:\\n(0.87, 0.15)}\\n{“action_type”: 4, “click_point”:\\n(0.87, 0.15)}\\nReference: {“action_type”: 3,\\n“typed_text”: “duckduckgo”}\\n{“action_type”: 4, “click_point”:\\n(0.45, 0.18)}\\nInstruction: open app \"DuckDuckGo Privacy Browser\" (install if not already installed) \\nand enter user name: \"cleaving@outlook.com\" and password: \"freighters\"\\nFigure 7: Example episodes of SeeClick on AITW. The model’s prediction output is below the screenshot, with\\naction_type 4 indicating clicking, action_type 3 indicating typing and action_type 6 indicating PRESS HOME. For\\nfailed steps, we mark the predictions in red and provide the groundtruth action in green.\\n{“action_type”: 4, “click_point”: (0.68, 0.10)}\\n{“action_type”: 4, “click_point”: (0.38, 0.35)}\\n{“action_type”: 3, “click_point”: (0.43, 0.48), “value”: \\n“87654321”}\\n{“action_type”: 3, “click_point”: (0.26, 0.57), “value”: \\n“9753”}\\n{“action_type”: 4, “click_point”: (0.50, 0.79)}\\nInstruction: Check my AMC gift card balance with gift card number 87654321 and pin number 9753.\\nInstruction: Find the list of all neighborhood maps for Brooklyn.\\n{“action_type”: 4, “click_point”: (0.03, 0.05)}\\n{“action_type”: 4, “click_point”: (0.56, 0.68)}\\n{“action_type”: 4, “click_point”: (0.50, 0.41)}\\nInstruction: Download the e-receipt with the last name Smith and confirmation number X123456989.\\n{“action_type”: 4, “click_point”: (0.67, 0.08)}\\n{“action_type”: 4, “click_point”: (0.47, 0.36)}\\n{“action_type”: 3, “click_point”: (0.46, 0.62), “value”: \\n“Smith”}\\n{“action_type”: 3, “click_point”: (0.70, 0.65), “value”: \\n“X123456989”}\\n{“action_type”: 4, “click_point”: (0.50, 0.77)}\\nFigure 8: Example episodes of SeeClick on Mind2Web. The model’s prediction output is below the screenshot,\\nwith action_type 4 indicating clicking and action_type 3 indicating typing. For failed steps, we mark the predictions\\nin red and provide the groundtruth elements with the green bounding box.\\n'},\n",
       " {'abstract': 'With the help of artificial intelligence (AI), a new pipeline has been proposed in the field of online advertising to generate creative advertisements, solving the problem of low aesthetics and quantity of traditional AI-based methods. This paper focuses on the use of the stable diffusion model with the LoRA model and two novel models, the prompt model and the reward model. The prompt model is designed to generate individualized creative images for different user groups, while the reward model comprehensively considers the multi-modal features of image and text to judge the quality and select the best ones to show online. The significant benefits obtained in online and offline experiments verify the significance of our proposed method.',\n",
       "  'introduction': 'Recently, AI-Generated Content (AIGC) has seen rapid development, leading to the emergence of text-to-image (T2I) generation tasks. Among various methods, diffusion-based methods have become state-of-the-art for T2I tasks due to their stationary training objective and easy scalability. However, directly using these methods for creative image generation is impractical in the advertising domain. This paper introduces stable diffusion in inpainting mode to modify only the background while preserving the main product information. In addition, two crucial factors, the prompt and the diffusion models, which directly affect the image generation, are discussed. To generate attractive creatives, a Prompt Model (PM) is introduced to select a good prompt, and a Reward Model (RM) is used to predict the Click-Through Rate (CTR) score for each creative image.',\n",
       "  'literature_review': 'To improve CTR, a pipeline has been proposed where in the first stage, a stable diffusion method is used to modify the background of the original product image. In the second stage, a prompt model is introduced to generate individualized prompts for different user groups. Finally, a reward model is trained to predict CTR scores for each creative image. The significant benefits obtained in online and offline experiments verify the significance of our proposed method.',\n",
       "  'methodology': 'The proposed pipeline consists of four main modules:\\n\\n1. Stable Diffusion Inpainting: This module is responsible for generating creative images by modifying only the background of the original product image. The saliency object image and mask image are obtained using a saliency detection model. A stable diffusion method in inpainting mode is then used to generate creatives. The LoRA model and prompt model are employed to fine-tune the stable diffusion model and select appropriate prompts.\\n\\n2. Prompt Model: The prompt model aims to select a good prompt that effectively describes the style of the image. It takes user attributes, item attributes, and contextual attributes as input and predicts the CTR score for each token in the whole word vocabulary under the given conditions. The top-ranked tokens are combined to form a prompt and inputted into the stable diffusion model to generate creative images.\\n\\n3. Reward Model: To predict the CTR scores of the creatives generated by the stable diffusion model, a reward model is introduced. This model considers the relationship between textual and visual features, as well as multi-head self-attention sub-modules, to learn the creative content and visual features. A list-wise loss function and a point-wise loss function are employed to train the reward model. The final loss is the sum of the list-wise loss and the point-wise loss.\\n\\n4. Self-cycling Training: To further optimize the quality of generated creatives and improve user experience, a self-cycling training mode is incorporated into the pipeline. The reward model is used to predict CTR scores for the generated creatives, and the top-ranked creatives are retained as training samples for both the LoRA model and the prompt model.',\n",
       "  'results': 'Extensive experiments were conducted to evaluate the effectiveness of the proposed pipeline. The results demonstrated:\\n\\n- Improved Online Results: CTR and revenue improvements of 10.4% and 9.7%, respectively, were observed when using the full self-cycling process compared to the baseline of using the original image provided by sellers.\\n\\n- Ablation Study on Reward Model: Experiments on commercial and public data showed the effectiveness of the proposed reward model structure, which outperformed two public methods in terms of CTR uplift and MSE metrics.\\n\\n- Ablation Study on Prompt Model: The importance of considering user group information was confirmed, as incorporating user features resulted in significant historical CTR improvements. Additionally, the self-cycling training process was found to further enhance the quality of generated creatives.',\n",
       "  'conclusion': 'The paper presents a novel pipeline for creative generation in online advertising that utilizes stable diffusion, a prompt model, and a reward model. The pipeline generates visually appealing and CTR-optimized creatives by modifying the background of original product images and considering user preferences. The prompt model enhances diversity and quality by generating personalized prompts, while the reward model selects the most promising creatives for display. Comprehensive experiments validate the effectiveness of the proposed method, demonstrating improvements in online performance and user experience.',\n",
       "  'title': 'A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model',\n",
       "  'author': 'Hao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, Yifan Zeng',\n",
       "  'textdata': 'A New Creative Generation Pipeline for Click-Through Rate with\\nStable Diffusion Model\\nHao Yang\\nShopee Discovery Ads\\nBeijing, China\\nJianxin Yuan\\nShopee Discovery Ads\\nBeijing, China\\nShuai Yang\\nShopee Discovery Ads\\nBeijing, China\\nLinhe Xu\\nShopee Discovery Ads\\nBeijing, China\\nShuo Yuan\\nShopee Discovery Ads\\nBeijing, China\\nYifan Zeng\\nShopee Discovery Ads\\nBeijing, China\\nABSTRACT\\nIn online advertising scenario, sellers often create multiple creatives\\nto provide comprehensive demonstrations, making it essential to\\npresent the most appealing design to maximize the Click-Through\\nRate (CTR). However, sellers generally struggle to consider users’\\npreferences for creative design, leading to the relatively lower aes-\\nthetics and quantities compared to Artificial Intelligence (AI)-based\\napproaches. Traditional AI-based approaches still face the same\\nproblem of not considering user information while having limited\\naesthetic knowledge from designers. In fact that fusing the user\\ninformation, the generated creatives can be more attractive because\\ndifferent users may have different preferences. To optimize the\\nresults, the generated creatives in traditional methods are then\\nranked by another module named creative ranking model. The\\nranking model can predict the CTR score for each creative con-\\nsidering user features. However, the two above stages (generating\\ncreatives and ranking creatives) are regarded as two different tasks\\nand are optimized separately. Specifically, generating creatives in\\nthe first stage without considering the target of improving CTR\\ntask may generate several creatives with poor quality, leading to\\ndilute online impressions and directly making bad effectiveness on\\nonline results.\\nIn this paper, we proposed a new automated Creative Generation\\npipeline for Click-Through Rate (CG4CTR) 1 with the goal of\\nimproving CTR during the creative generation stage. In this pipeline,\\na new creative is automatically generated and selected by stable\\ndiffusion method with the LoRA model and two novel models:\\nprompt model and reward model. Our contributions have four\\nparts: 1) The inpainting mode in stable diffusion method is firstly\\napplied to creative image generation task in online advertising\\nscene. A self-cyclic generation pipeline is proposed to ensure the\\nconvergence of training. 2) Prompt model is designed to generate\\nindividualized creative images for different user groups, which\\ncan further improve the diversity and quality of the generated\\n1The code is at https://github.com/HaoYang0123/Creative_Generation_Pipeline.\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n© 2018 Association for Computing Machinery.\\nACM ISBN 978-1-4503-XXXX-X/18/06...$15.00\\nhttps://doi.org/XXXXXXX.XXXXXXX\\ncreatives. 3) Reward model comprehensively considers the multi-\\nmodal features of image and text to improve the effectiveness of\\ncreative ranking task, and it is also critical in self-cyclic generation\\npipeline. 4) The significant benefits obtained in online and offline\\nexperiments verify the significance of our proposed method.\\nKEYWORDS\\nStable Diffusion, Creative Generation, Prompt and Reward Models\\nACM Reference Format:\\nHao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng.\\n2018. A New Creative Generation Pipeline for Click-Through Rate with\\nStable Diffusion Model. In Proceedings of Make sure to enter the correct\\nconference title from your rights confirmation emai (Conference acronym ’XX).\\nACM, New York, NY, USA, 10 pages. https://doi.org/XXXXXXX.XXXXXXX\\n1\\nINTRODUCTION\\nRecently, the topic of AI-Generated Content (AIGC) has developed\\nrapidly, such as GLIDE [24], DALL-E 1-3 [31, 36, 45], Imagen [40],\\nStable Diffusion (SD) [37] and ChatGPT [43]. One of the most repre-\\nsentative tasks is text-to-image (T2I) generation, which has received\\nmuch attention in both academia and industry. The target of T2I\\ntask is to generate images with similar semantics given input lan-\\nguage prompt. Currently, the diffusion-based methods [18, 23, 37]\\nhave become the state-of-the-art (SoTA) method in T2I task due\\nto the stationary training objective and easy scalability. Although\\nthese diffusion methods have a satisfactory performance on the\\nT2I task, directly using these methods in creative image generation\\nis infeasible in the advertising scene because they will modify the\\nmain product image. Traditional creative generation methods in the\\nadvertising scene use deep learning to generate some objects/tags\\n[19, 20], dense captions [32] or layout information [33, 46, 47] on\\nimage. Different from these methods, we first use diffusion model\\nto generate background images while keeping the main product\\ninformation unchanged in creative generation task for the advertis-\\ning scene. In the experimental analysis, we found that modifying\\nthe background while keeping the visual pixels of the main product\\nunchanged can also significantly improve the CTR.\\nIn addition, when using the diffusion model, there are two very\\nimportant factors, prompt and diffusion models, that will directly\\naffect the effect of the generated image. As shown in Fig. 1, for\\nthe original product image uploaded by the seller, the background\\nimage is simple with solid white color. To make the background\\nmore attractive, we can use the inpainting mode in SD method\\nto generate more individualized background images with richer\\ncolors. However, if a proper prompt (a prompt consists of several\\narXiv:2401.10934v1  [cs.IR]  17 Jan 2024\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\nHao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng\\ntokens) is not added into the generation process, the result is often\\npoor with the cluttered background. Then we need to use a Prompt\\nModel (PM) to select a good prompt, which is then added to guide\\nthe generation process. Despite that a good background can be\\ngenerated given an acceptable prompt, the diffusion model is not\\nfine-tuned in our industrial data, and the effect is not optimal. It is\\nnecessary to fine-tune diffusion model in our data. However, the\\nparameters of diffusion model are very huge, and directly training\\nwhole model is not realistic because of high training cost. Therefore,\\nLow-Rank Adaptation (LoRA) mechanism [27, 48] is used to speed\\nup the training process.\\nFigure 1: Generating creatives only by modifying background\\nusing SD method in inpainting mode.\\nAs described earlier, Prompt Model (PM) is important to guide\\nimage generation. PM considers user feature (e.g., user age, user\\ngender), item feature (e.g., item ID and category ID) and candidate\\ntokens as input, then outputs score for each input token. We se-\\nlect top-𝑝 tokens to assemble into one prompt. Considering that\\ndifferent users have different preferences for creative images, for\\nexample, young users are more inclined to cool color and electronic\\nstyle while old users are more inclined to warm color and mini-\\nmalistic style. Then user feature is considered as a vital feature to\\ngenerate individualized prompts for different user groups. The tra-\\nditional creative generation methods [19, 20, 32, 33, 46, 47] do not\\nconsider the user information in generating creative image process.\\nSome works [16, 25, 39, 51] use another model to rank all generated\\ncreatives considering user information and select the best one to\\nshow online. These traditional pipelines do not maximize the online\\nperformance because in first stage (generating creative stage), they\\ndo not regard the improving CTR as the target, resulting in many\\n\"bad\" creatives diluting the online impressions. So it is necessary\\nand critical to directly generate more attractive creatives by consid-\\nering user information and regarding the CTR as an optimization\\ntarget in generating stage. Currently, several works [50, 52] focus\\non enhancing the generation quality of diffusion models by rein-\\nforcement learning with human feedback. Although these methods\\ncan generate creatives using diffusion models for a specific target,\\nsuch as CTR, they still face the problems of modifying the original\\nproduct with relatively low efficiency and diversity.\\nTo guide the prompt model and LoRA model to generate more\\nattractive creatives, another Reward Model (RM) is used to predict\\nthe Click-Through Rate (CTR) score for each creative image given\\none product. For each product, if too many creatives are uploaded\\nonline, it will lead to sub-optimal performance due to the inferior\\nperformance of the generated creatives [25]. Then we need RM\\nto select the proper creatives for displaying online. At the same\\ntime, RM can be used to generate samples (good creatives with high\\npredicted scores and bad creatives with low scores) to train LoRA\\nand prompt models.\\nTo summarize, there are four contributions in this work: 1) The\\ninpainting mode in stable diffusion method is firstly applied to\\ncreative image generation task in online advertising scene. A self-\\ncyclic generation pipeline is proposed to ensure the convergence\\nof training. 2) Prompt model is designed to generate individual-\\nized creative images for different user groups, which can further\\nimprove the diversity and quality of the generated creatives. 3)\\nReward model comprehensively considers the multi-modal features\\nof image and text to improve the effectiveness of creative rank-\\ning task, and it is also critical in self-cyclic generation pipeline. 4)\\nThe significant benefits obtained in online and offline experiments\\nverify the significance of our proposed method.\\n2\\nRELATED WORKS\\n2.1\\nText-to-image Generation\\nRecently, in text-to-image generation (T2I) task, more and more\\ndeep learning models to generate image given text are emerging.\\nFor example, the generative adversarial networks (GANs) [4, 6]\\nuse a generator and a discriminator to generate image, where gen-\\nerator network is responsible for generating image and discrimi-\\nnator network is to distinguish the generated image and the real\\none. There are some related GANs works, such as ProGAN [10],\\nStyleGAN [15, 17, 21], Projected GAN [28], VQGAN [30]. Different\\nfrom GANs, some autoencoder (AE) related works appear, such as\\ndeep autoencoder [2], variational autoencoder (VAE) [3], vector\\nquantised-variational autoencoders (VQ-VAE) [9, 14]. The encoder\\nreceives the input and encodes it in a latent space of a lower dimen-\\nsion while the decoder decodes this vector to produce the original\\ninput.\\nAs auto-regressive models developed in text generation recently,\\na lot of works adopted this to achieve amazing results for T2I\\ngeneration, such as DALL-E 1-3 [31, 36, 45], CogView [22] and Pariti\\n[42]. Currently, the diffusion models (DM) [24, 37, 40] appeared\\nas SoTA T2I methods due to the natural fit to inductive biases\\nfrom image data. For example, GLIDE [24] uses an adequate text-\\nguidance strategy to generate and edit photorealistic image. Latent\\nDM [37] enables DM training on limited computational resources\\nwhile retaining the quality and flexibility by applying in latent space\\nof powerful pre-trained autoencoders. Imagen [40] directly diffuses\\npixels using a pyramid structure without using latent images.\\nRecently, some works [19, 20, 32, 33, 35, 46, 47] study the task\\nof creative generation in the advertising scene. For example, [19]\\nautomatically annotates the objects from the image and generates\\noptimal banner layout information. CapOnImage [32] generates\\ndense captions at different locations of the image by pre-training\\nand fine-tuning a multi-modal framework. CreaGAN [35] general-\\nizes to other product materials by utilizing existing design elements\\n(e.g., background material). Other works [33, 46, 47] generate lay-\\nout information of some elements, such as text and logo, given the\\noriginal image.\\nA New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\nDifferent from the above methods, we propose a novel pipeline\\nfor generating creatives in advertising scene. A stable diffusion\\nmethod is used in inpainting mode to generate only the background\\npixels, while keeping the other pixels on the main product un-\\nchanged. On the other hand, by introducing user information, we\\ncan support individualized generation for various user group.\\n2.2\\nCreative CTR Prediction\\nCreative CTR prediction task is to rank all creative images pro-\\nvided by sellers or automatically generated by models, given one\\nproduct or advertisement. AMS [12] combines the behavior images\\ninto modeling user preference in CTR prediction. PEAC [16] evalu-\\nates and selects advertisement creatives offline before being placed\\nonline. VAM [25] uses a visual-aware ranking model to order the\\ncreatives according to their visual appearance. CACS [39] considers\\nthe different image features in ranking stage to joint optimization\\nof intra-advertisement creative selection and inter-advertisement\\nranking. CLIK [34] considers the topic of the given product to select\\nrepresentatives from several images with intuitive visual informa-\\ntion by contrastive learning. [51] proposes a two-stage pipeline\\nmethod with content-based recall model and CTR-based ranking\\nmodel to find an appropriate image. Compared to these methods,\\nour proposed reward model performs better because of the multi-\\nmodal structure and pre-training techniques.\\n3\\nMETHODS\\n3.1\\nPipeline\\nThe pipeline is shown in Fig. 2 and the pseudo-code is shown in Alg.\\n1. In the creative generation pipeline, for each item, we first select\\none original image with a relatively clean background provided\\nby sellers, which then is inputted into saliency model [38] to get\\nthe saliency object image and mask image. We use Stable Diffusion\\n(SD) [37] method to generate the creatives given the saliency image,\\nLoRA model [27, 48] and prompts where LoRA model is fine-tuned\\nin our commercial data and prompts are selected by another prompt\\nmodel trained on online creative clicked data. The LoRA model and\\nprompt model are critical for the impact on generating creatives,\\nespecially since different prompts can cause creatives to vary greatly.\\nTo consistently generate good creatives, we need to fine-tune LoRA\\nmodel and prompt model in this pipeline. The details are described\\nin Section 3.2 and 3.3. Next, we use reward model (details in Section\\n3.4) to predict the CTR score for each generated creative, and only\\ntop-k creatives are retained to further train LoRA model while\\nthese top-k creatives are treated as positive samples and others are\\ntreated as negative samples to train prompt model.\\nLast but not least, various users may have distinct preferences\\non creatives. So we need to generate different types of creatives\\nfor various users. Then user information is considered in prompt\\nmodel, although the input product or image is the same, the prompts\\nmay need to be adjusted based on the characteristics of the user\\ngroup, and the resulting generated creatives can also be different.\\nTo improve the quality and stability of the generated creative, the\\nLoRA model and prompt model are updated iteratively. Precisely,\\nin first step, only parameters in LoRA model are updated while\\nparameters in prompt model are fixed; in second step, parameters\\nin LoRA model are fixed while parameters in prompt model are\\nupdated, ensuring the convergence of the entire framework.\\nAlgorithm 1 Creative Generation Pipeline for CTR.\\nInput: 𝑀 is the number of items\\n𝑁 is the number of training epochs\\n𝜃𝑝 is the parameters of prompt model\\n𝜃𝑙 is the parameters of LoRA model\\n𝜃𝑟 is the parameters of reward model\\n𝜃𝑠 is the parameters of saliency model\\n𝜃𝑐 is the parameters of CLIP-Interrogator model\\n𝑝 is the number of tokens select by prompt model\\n𝑞 is the number of generated creative images\\n𝑘 is the number of images select by reward model\\n1: Initialization: 𝜃𝑝 and 𝜃𝑙 are trained on clicked data as the model pa-\\nrameters for initialization. Note that 𝜃𝑟 has been trained beforehand\\nwhile 𝜃𝑠 and 𝜃𝑐 are the public models, these three model parameters\\nare frozen in this pipeline.\\n2: for 𝑖 = 1 → 𝑁 do\\n3:\\nG, B = ∅, ∅, which refer to the set of good case and bad case, respec-\\ntively.\\n4:\\nfor 𝑗 = 1 → 𝑀 do\\n5:\\nGet the original images of 𝑖𝑡𝑒𝑚𝑗 provided by seller\\n6:\\nGet saliency images and masked images by 𝜃𝑠\\n7:\\nSelect top-𝑝 tokens as the appropriate prompt 𝑝𝑟𝑜𝑚𝑝𝑡𝑗 for 𝑖𝑡𝑒𝑚𝑗\\nby 𝜃𝑝 given user group feature\\n8:\\nGenerate 𝑞 creative images for 𝑖𝑡𝑒𝑚𝑗 by 𝜃𝑙 given 𝑝𝑟𝑜𝑚𝑝𝑡𝑗\\n9:\\nSort images by predicted CTR scores from 𝜃𝑟\\n10:\\nSelect top-𝑘 images as positive samples and other images as nega-\\ntive samples\\n11:\\nGenerate the prompt for each image by 𝜃𝑐\\n12:\\nInsert samples of (𝑖𝑡𝑒𝑚𝑗, 𝑖𝑚𝑎𝑔𝑒1, 𝑝𝑟𝑜𝑚𝑝𝑡1, 𝑠𝑐𝑜𝑟𝑒1), ... (𝑖𝑡𝑒𝑚𝑗,\\n𝑖𝑚𝑎𝑔𝑒𝑘, 𝑝𝑟𝑜𝑚𝑝𝑡𝑘, 𝑠𝑐𝑜𝑟𝑒𝑘) into G\\n13:\\nInsert samples of (𝑖𝑡𝑒𝑚𝑗, 𝑖𝑚𝑎𝑔𝑒𝑘+1, 𝑝𝑟𝑜𝑚𝑝𝑡𝑘+1, 𝑠𝑐𝑜𝑟𝑒𝑘+1), ...\\n(𝑖𝑡𝑒𝑚𝑗, 𝑖𝑚𝑎𝑔𝑒𝑞, 𝑝𝑟𝑜𝑚𝑝𝑡𝑞, 𝑠𝑐𝑜𝑟𝑒𝑞) into B\\n14:\\nend for\\n15:\\nif 𝑖 % 2 == 0 then\\n16:\\nTrain prompt model 𝜃𝑝 given G and B by Equation 8\\n17:\\nelse\\n18:\\nTrain LoRA model 𝜃𝑙 given G by Equation 3\\n19:\\nend if\\n20: end for\\nOutput: 𝜃𝑝 and 𝜃𝑙\\n3.2\\nLoRA Model in Stable Diffusion\\nAs the original creatives designed by sellers may have quality and\\nquantity problems, we need to modify the images to a certain ex-\\ntent. Recently, a lot of works [18, 24, 36, 44] use diffusion models to\\ngenerate images, which uses a variational autoencoder (VAE) [3]\\nto operate the diffusion process in a low-dimensional latent space\\nwith good computational efficiency. In detail, one image I is given\\ninto an encoder 𝑉𝐴𝐸 to get the latent feature 𝑍, i.e., 𝑍 = 𝑉𝐴𝐸(𝐼).\\nA diffusion model is to denoise an input latent feature 𝑍𝑡 at each\\ntimestep 𝑡 conditioned on a given prompt. During the training\\nprocess, for each timestep 𝑡, the diffusion denoising network 𝜀𝜃 is\\noptimized to remove the noise 𝜀 from the noised version of latent\\ncode 𝑍𝑡 conditioned on prompt 𝑦 shown in following equation,\\nwhere 𝜏𝜃 is the CLIP [26] text encoder, 𝑦 is the given text or prompt,\\n𝜀 is the noise sampled according to a standard normal distribution.\\nThe diffusion network 𝜀𝜃 often uses UNet [5] consisting of convo-\\nlution, self-attention and cross-attention layers. 𝑍𝑡 is the noised\\nembedding encoded by a scheduler in low-dimensional space with\\nVAE model given image 𝐼 and noise 𝜀. The scheduler is used to add\\nnoise information into low-dimensional embedding from the given\\nimage.\\n𝐿 = E𝑍∼𝑉 𝐴𝐸(𝐼 ), 𝑦, 𝜀∼𝑁 (0,1), 𝑡\\n\\x02\\n||𝜀 − 𝜀𝜃 (𝑍𝑡,𝑡,𝜏𝜃 (𝑦)) ||2\\n2\\n\\x03\\n, 𝑤ℎ𝑒𝑟𝑒\\n(1)\\n𝑍𝑡 = 𝑠𝑐ℎ𝑒𝑑𝑢𝑙𝑒𝑟 (𝑉𝐴𝐸(𝐼 ), 𝑡, 𝜀)\\n(2)\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\nHao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng\\nFigure 2: (a) Creative generation pipeline. (b) The structure of prompt model. (c) The structure of reward model.\\nHowever, the standard SD method cannot be used in our pipeline\\nbecause it will modify the main product in advertising platform.\\nFor example, the product image is a pair of shoes uploaded by a\\nseller, after the SD generating process, the generated image might\\nbe a pair of socks or a hat, which is not accepted. To generate\\none image with a higher quality while leaving the main content\\nof original product unchanged, we can get the pixels (referred as\\n𝐼𝑠𝑎𝑙) of the main product image 𝐼 by using the saliency detection\\nnetwork [38]. The pre-trained saliency detection network can point\\nout the main product locations and then 𝐼𝑠𝑎𝑙 are masked and only\\nbackground pixels 𝐼𝑏𝑘𝑔 are generated by SD method in inpainting\\nmode [41]. The only difference between the inpainting mode and\\nnormal mode in SD is that the inpainting mode concatenates the\\nlow-dimensional embedding (𝑉𝐴𝐸(𝐼𝑠𝑎𝑙)) encoded by VAE given\\nmasked pixels (Equation 4). The training loss is similar to the normal\\nmode.\\n𝐿 = E𝑍 ′∼𝑉 𝐴𝐸(𝐼, 𝐼𝑠𝑎𝑙 ), 𝑦, 𝜀∼𝑁 (0,1), 𝑡\\n\\x02\\n||𝜀 − 𝜀𝜃 (𝑍 ′\\n𝑡,𝑡,𝜏𝜃 (𝑦))||2\\n2\\n\\x03\\n, 𝑤ℎ𝑒𝑟𝑒\\n(3)\\n𝑍 ′\\n𝑡 = 𝑐𝑜𝑛𝑐𝑎𝑡 (𝑠𝑐ℎ𝑒𝑑𝑢𝑙𝑒𝑟 (𝑉𝐴𝐸(𝐼 ), 𝑡, 𝜀), 𝑉𝐴𝐸(𝐼𝑠𝑎𝑙 ))\\n(4)\\nBecause of the difference in image distribution between our\\ncommercial data and public data, we should fine-tune the public\\nSD model on our commercial data. However, it is very difficult to\\nfine-tune SD model because of huge number of parameters in SD\\nmodel, including UNet [5] and CLIP [26]. Low-Rank Adaptation\\n(LoRA) [27, 48] can help to solve the above problem, which allows\\nus to use low-rank adaptation technology to quickly fine-tune SD\\nmodel. With fine-tuned LoRA model, new creative image with a\\ngenerated background can be acquired by SD method given an\\nappropriate prompt.\\nWe take Fig. 1 to explain the importance of the prompt and LoRA\\nin generating creative workflow. The original image of the product\\ngiven by seller is poor due to the simple colored background. To\\nimprove the quality of this creative, firstly we get masked pixels\\nof the main product (e.g., skin care product) by saliency detection\\nmodel [38]. Given the original image and masked image, we use\\nSD method in inpainting mode to get better creatives with suitable\\nbackgrounds. Of course, to explain the importance of prompt and\\nLoRA model in generating process, firstly we only use the SD\\nmethod without prompt and LoRA model, the results are not as\\nexpected with mussy background same as the original image (three\\nupper images in Fig. 1). To improve the quality of background, we\\ncan add a suitable prompt (e.g., product photo, petals, water flowers\\netc.), and the results can be better (three middle images). Finally,\\nLoRA model is increased in generating process, the quality can be\\nfurther improved (three lower images).\\nA New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\nOn the other hand, to distinguish the good and bad creatives\\nin generating process, a reward model is proposed to predict the\\nCTR, we only retain the good creatives with high CTRs to train\\nLoRA model. At the same time, the good creatives with prompts\\ngenerated by CLIP-Interrogator model2 are regarded as positive\\nsamples and the bad ones are regarded as negative samples to\\ntrain prompt model. After multiple rounds of training processes,\\nthe LoRA model and prompt model can improve the quality of\\ngenerated creatives and improve user experience.\\n3.3\\nPrompt Model\\nThe target of the prompt model is to select several appropriate\\nwords/tokens combined as one prompt, which can effectively de-\\nscribe the style of the image, and then input it into the stable diffu-\\nsion model to generate more appealing images.\\nThe input of the prompt model consists of four parts: user at-\\ntributes (user age, user gender, etc.), item attributes (item ID, cate-\\ngory ID, etc.), contextual attributes (user’s device model, access time,\\netc.), and tokens used for creative generation. The user attributes,\\nitem attributes, and contextual attributes are referred as Query. The\\noutput of the prompt model is the estimated CTR score for each\\ntoken in the whole word vocabulary3 under the given Query condi-\\ntions. The top-𝑝 tokens with the highest CTRs are selected, which\\nare assembled as a new prompt and it is then inputted into a stable\\ndiffusion model to generate creative images.\\nThe network architecture of the prompt model is illustrated in\\nsub-figure (b) of Fig. 2. On the Query side, user attributes, item\\nattributes, and contextual attributes are one-hot encodings and\\nthen converted into corresponding embeddings through lookup\\ntable. These three embeddings are concatenated into a query em-\\nbedding named 𝑒𝑚𝑏𝑞. Simultaneously, all tokens in the prompt gen-\\nerated from the creative image are multi-hot encodings and then\\ntransformed into multiple embeddings named 𝑒𝑚𝑏∗𝑝 in the same\\nway. Then we send 𝑒𝑚𝑏∗𝑝 into transformers and a mean-pooling\\nlayer to obtain one final embedding 𝑒𝑚𝑏𝑝 representing for the\\nprompt. These two embeddings, 𝑒𝑚𝑏𝑞 and 𝑒𝑚𝑏𝑝 are concatenated\\nand passed through DeepFM [8] to produce a final estimated score.\\nThe prompts generated from the clicked creatives are regarded\\nas positive samples, and other prompts from non-clicked creatives\\nare treated as negative samples. This enables the model to select the\\nappropriate prompts that are likely to improve CTR under Query\\nconditions.\\nWe train the prompt model under the cross-entropy loss function\\nusing the clicked data, which we call this loss function as \"hard\\nloss\" (𝐿ℎ𝑎𝑟𝑑 in Equation 6). Additionally, to reduce the exploration\\ncost in creative generation, we utilize the output values from the\\nreward model as the soft labels for auxiliary training in self-cycling\\nin Fig 2. The final loss is a weighted sum of the hard loss and soft\\nloss, where 𝜆𝑝 is 0.1 in our experiments.\\n𝑝 = 𝑃𝑟𝑜𝑚𝑝𝑡𝑀𝑜𝑑𝑒𝑙 (𝑄𝑢𝑒𝑟𝑦, 𝑃𝑟𝑜𝑚𝑝𝑡 )\\n(5)\\n𝐿ℎ𝑎𝑟𝑑 = 𝐶𝐸𝐿𝑜𝑠𝑠 (𝑝, 𝑦)\\n(6)\\n𝐿𝑠𝑜𝑓 𝑡 = 𝐶𝐸𝐿𝑜𝑠𝑠 (𝑝, 𝑦𝑅𝑀 )\\n(7)\\n𝑃𝑟𝑜𝑚𝑝𝑡𝐿𝑜𝑠𝑠 = (1 − 𝜆𝑝 ) · 𝐿ℎ𝑎𝑟𝑑 + 𝜆𝑝 · 𝐿𝑠𝑜𝑓 𝑡\\n(8)\\n2CLIP-Interrogator model is at https://github.com/pharmapsychotic/clip-interrogator.\\n3We have collected item images provided by sellers and designers and run CLIP-\\nInterrogator to get the complete tokens, which are then post-processed, such as deleting\\nstop-words and entity nouns, to form the whole word vocabulary.\\nFurthermore, individualized prompts can be created for different\\nuser groups based on user information, as different users may prefer\\ncreatives with different styles. The generated creatives can enhance\\nuser experience while reducing exploration costs. For example,\\nwithout considering user information, the prompt model may se-\\nlect and combine some normal tokens, such as \"minimalistic\" and\\n\"translucent\". But for a teenager, better tokens may be \"electronic\"\\nand \"translucent\" while for an old user, \"golden\" and \"minimalis-\\ntic\" may be more appropriate. The normal prompt combined with\\n\"minimalistic\" and \"translucent\" might not be the favorite for either\\nuser group, hence failing to meet the actual needs.\\n3.4\\nReward Model\\nThe structure of reward model is shown in sub-figure (c) of Fig. 2,\\nwhich is used to predict the CTRs of the creatives from the same\\nitem generated by stable diffusion model. Each creative contains\\ntitle, image and caption where title of creative is the same as the\\ntitle of item. As the titles of all creatives are the same, the title\\nfeature seems redundant for predicting CTR scores. To explain why\\ntitle feature is still important for this task, we use a multi-head\\nself-attention sub-module to compute the relationship between the\\ntextual feature and visual feature of each creative. In detail, the\\nmulti-head self-attention sub-module can learn the relationship\\n(e.g., similarity) between textual feature and visual feature, such\\nthat the creative with a high similarity score may be predicted with\\nhigh CTR score. In addition, to make reward model to learn the\\ncreative content better, the caption information is also used, which\\nis acquired by CLIP-Interrogator model. CLIP-Interrogator model\\nuses the image as input and outputs the textual caption to describe\\nthe image.\\nThe title and caption information of j-th creative for i-th item\\nare inputted into pre-trained BERT model [11] to extract textual\\nfeatures, referred as 𝑡𝑗 and 𝑐𝑗, respectively (Equation 9). The pre-\\ntrained Swin model [29] is used to extract visual feature (referred\\nas 𝑖𝑗) from image information (Equation 9). Both BERT model and\\nSwin model are pre-trained in our commercial data with token\\nmask loss and patch mask loss, respectively. In particular, BERT\\nmodel is further fine-tuned in our commercial clicked data [49].\\nTwo fully-connected layers are used to reduce the dimension of\\nfeature (Equation 10). To fusion the cross-model features and ex-\\ntract the relationship between textual and visual features, we use\\ntransformers [7] to get self-attention features (Equation 11). The\\nfused title, image and caption features (𝑡𝑡\\n𝑗 , 𝑖𝑡\\n𝑗, 𝑐𝑡\\n𝑗) are concatenated\\ninto final feature. Other two fully-connected layers are used to\\npredict the CTR score (𝑝𝑖\\n𝑗) of j-th creative for i-th item (Equation\\n12).\\n𝑡𝑗 = 𝐵𝐸𝑅𝑇 (𝑡𝑖𝑡𝑙𝑒𝑗 ), 𝑐 𝑗 = 𝐵𝐸𝑅𝑇 (𝑐𝑎𝑝𝑡𝑖𝑜𝑛𝑗 ), 𝑖𝑗 = 𝑆𝑤𝑖𝑛(𝑖𝑚𝑎𝑔𝑒𝑗 )\\n(9)\\n𝑡 𝑓\\n𝑗 = 𝐹𝐶𝑡 (𝑡𝑗 ),\\n𝑐𝑓\\n𝑗 = 𝐹𝐶𝑐 (𝑐 𝑗 ),\\n𝑖𝑓\\n𝑗 = 𝐹𝐶𝑖 (𝑖𝑗 )\\n(10)\\n𝑡𝑡\\n𝑗, 𝑐𝑡\\n𝑗, 𝑖𝑡\\n𝑗 = 𝑡𝑟𝑎𝑛𝑠𝑓 𝑜𝑟𝑚𝑒𝑟𝑠 (𝑡 𝑓\\n𝑗 , 𝑐𝑓\\n𝑗 , 𝑖𝑓\\n𝑗 )\\n(11)\\n𝑝𝑖\\n𝑗 = 𝐹𝐶𝑐𝑡𝑟 (𝑐𝑜𝑛𝑐𝑎𝑡 (𝑡𝑡\\n𝑗, 𝑐𝑡\\n𝑗, 𝑖𝑡\\n𝑗 ))\\n(12)\\nThe target of reward model is to predict the CTR score of each\\ncreative. The real CTR value is calculated by the following equation.\\nˆ𝑦𝑖\\n𝑗 =\\n𝑐𝑙𝑖𝑐𝑘𝑖\\n𝑗\\n𝑖𝑚𝑝𝑟𝑒𝑠𝑠𝑖𝑜𝑛𝑖\\n𝑗\\n(13)\\nList-wise and point-wise loss functions are used to train reward\\nmodel. The list-wise loss function is shown in Equation 14 where ˆ𝑦𝑖\\n𝑗\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\nHao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng\\nand 𝑝𝑖\\n𝑗 are the real CTR and predicted CTR, respectively. And 𝑤𝑖\\n𝑗 is\\nthe sample impression weight of j-th creative for i-th item, shown in\\nEquation 15. Through the list-wise loss function, the model focuses\\non ranking the creatives within the same item. Although the list-\\nwise loss function can lead model to correctly rank the creatives,\\nthe absolute difference between the real and predicted scores may\\nbe very huge. Then we use point-wise loss function as an auxiliary\\nfunction to enforce model to produce more accurate scores, as\\nshown in Equation 16.\\n𝐿𝑙𝑖𝑠𝑡 =\\nÍ\\n𝑖 𝑤𝑖 · 𝐿𝑖\\n𝑙𝑖𝑠𝑡\\nÍ\\n𝑖 𝑤𝑖\\n,\\n𝐿𝑖\\n𝑙𝑖𝑠𝑡 = −\\n∑︁\\n𝑗\\nˆ𝑦𝑖\\n𝑗 · 𝑙𝑜𝑔(𝑝𝑖\\n𝑗 )\\n(14)\\n𝑤𝑖\\n𝑗 =\\n𝑖𝑚𝑝𝑟𝑒𝑠𝑠𝑖𝑜𝑛𝑖\\n𝑗\\nÍ\\n𝑖\\nÍ\\n𝑗 𝑖𝑚𝑝𝑟𝑒𝑠𝑠𝑖𝑜𝑛𝑖\\n𝑗\\n,\\n𝑤𝑖 =\\n∑︁\\n𝑗\\n𝑤𝑖\\n𝑗\\n(15)\\n𝐿𝑝𝑜𝑖𝑛𝑡 =\\nÍ\\n𝑖 𝑤𝑖 · 𝐿𝑖\\n𝑝𝑜𝑖𝑛𝑡\\nÍ\\n𝑖 𝑤𝑖\\n,\\n𝐿𝑖\\n𝑝𝑜𝑖𝑛𝑡 =\\n∑︁\\n𝑗\\n( ˆ𝑦𝑖\\n𝑗 − 𝑝𝑖\\n𝑗 )2\\n(16)\\nAs shown in Equation 17, the final loss is the sum of list-wise\\nloss and point-wise loss, where 𝜆𝑟 is a hyperparameter and is 0.1\\nin our experiments.\\n𝑅𝑒𝑤𝑎𝑟𝑑𝐿𝑜𝑠𝑠 = (1 − 𝜆𝑟 ) · 𝐿𝑙𝑖𝑠𝑡 + 𝜆𝑟 · 𝐿𝑝𝑜𝑖𝑛𝑡\\n(17)\\n4\\nEXPERIMENTS\\nIn this section, we evaluate our proposed generation pipeline on\\nonline experiments. To validate the effectiveness of reward model,\\nwe use commercial data and public data compared with two other\\npublic methods. On the other hand, we make some ablation experi-\\nments for prompt model to explain the importance of considering\\nuser information. Finally, we perform some analyses and show some\\ncases to demonstrate the effectiveness of the self-cycling training\\nmode in our generation pipeline.\\n4.1\\nOnline Results\\nWe select five categories (including women shoes, women bags,\\ntravel, beauty and mobile) in our commercial scene for online exper-\\niments. For each item, the prompt model is used to select suitable\\ntokens in the whole vocabulary as one appropriate prompt consider-\\ning user features. As different users may have varying preferences\\nfor creatives, so we need to use prompt model to extract different\\nprompts based on the characteristics of users. Then 10 creatives\\nfor each item are automatically generated by stable diffusion with\\nLoRA model given the selected prompts. These creatives are then\\ninputted into reward model to predict CTRs, and the top 5 creatives\\nwith highest predicted CTRs are retained. These retained creatives\\nare regarded as training samples to further train LoRA model. To\\nfurther optimize the quality of generated creatives in the next step,\\nthese retained creatives in current step are regarded as positive\\nsamples and others are regarded as negative samples, which are\\nused to train prompt model. This can indirectly improve creative\\nquality by facilitating better prompts in subsequent steps. With ten\\nrounds of the above processes, top 5 creatives for each item are\\ngenerated and uploaded to online platform. We use Epsilon-greedy\\n[1, 13] as online display strategy.\\nAs shown in Table 1, without the self-cycling training in our\\npipeline (referred as \"w/o self-cycling\" in the table), we only use\\nthe first versions of LoRA and prompt models to generate the cre-\\natives. The CTR and revenue improvements are only 4.2% and 3.8%,\\nrespectively. After using LoRA and prompt models with five rounds\\nTable 1: Online results.\\nMethods\\nAll\\nWomen\\nShoes\\nWomen\\nBags\\nTravel\\nBeauty\\nMobile\\nw/o\\nself-cycling\\n4.21/3.82\\n2.2/3.5\\n11.3/3.1\\n6.0/4.3\\n2.7/4.8\\n3.7/4.5\\nMiddle 3\\n8.1/7.2\\n6.6/7.8\\n10.7/4.2\\n2.2/7.4\\n2.7/1.1\\n12.7/12.9\\nOurs\\n10.4/9.7\\n7.0/9.0\\n12.9/4.3 9.9/12.6 11.8/7.9 14.2/14.5\\n1 The first metric is CTR improvement compared with the baseline of using the\\noriginal image with percentage.\\n2 The second metric is revenue improvement with percentage.\\n3 \"Middle\" means the middle versions of prompt model and LoRA model in self-\\ncycling.\\n(referred as \"Middle\" in the table) and ten rounds (referred as \"Ours\"\\nin the table) of training, the results become much better.\\n4.2\\nIn-depth Analysis\\n4.2.1\\nAblation Study on Reward Model. As described in METHODS\\nsection, reward model plays a crucial role in the self-cycling training\\nprocess of generating creative pipeline. In reward model, we use\\ntop-k CTR uplift to evaluate the performance. The top-k CTR uplift\\nmetric is calculated as follows:\\n𝐶𝑇𝑅-𝑢𝑝𝑙𝑖𝑓 𝑡𝑘 =\\n𝐶𝑇𝑅-𝑠𝑐𝑜𝑟𝑒𝑘\\n𝐶𝑇𝑅-𝑠𝑐𝑜𝑟𝑒𝑏𝑎𝑠𝑒\\n− 1\\n(18)\\n𝐶𝑇𝑅-𝑠𝑐𝑜𝑟𝑒𝑘 =\\nÍ\\n𝑖\\nÍ\\n𝑗 ∈𝑡𝑜𝑝𝑘𝑖𝐶 𝑐𝑙𝑖𝑐𝑘 𝑗\\n𝑖\\nÍ\\n𝑖\\nÍ\\n𝑗 ∈𝑡𝑜𝑝𝑘𝑖𝐶 𝑖𝑚𝑝𝑟𝑒𝑠𝑠𝑖𝑜𝑛𝑗\\n𝑖\\n(19)\\n𝐶𝑇𝑅-𝑠𝑐𝑜𝑟𝑒𝑏𝑎𝑠𝑒 =\\nÍ\\n𝑖\\nÍ\\n𝑗 ∈𝐶 𝑐𝑙𝑖𝑐𝑘 𝑗\\n𝑖\\nÍ\\n𝑖\\nÍ\\n𝑗 ∈𝐶 𝑖𝑚𝑝𝑟𝑒𝑠𝑠𝑖𝑜𝑛𝑗\\n𝑖\\n(20)\\n, where𝐶𝑇𝑅-𝑠𝑐𝑜𝑟𝑒𝑘 is the cumulative CTR scores of top-k creatives\\nwith the highest scores predicted by reward model for each item.\\n𝑐𝑙𝑖𝑐𝑘 𝑗\\n𝑖 and 𝑖𝑚𝑝𝑟𝑒𝑠𝑠𝑖𝑜𝑛𝑗\\n𝑖 are the numbers of clicks and impressions\\nof j-th creative for i-th item. 𝑡𝑜𝑝𝑘𝑖𝐶 is the set of top-k creatives for\\nthe i-th item. If 𝐶𝑇𝑅-𝑢𝑝𝑙𝑖𝑓 𝑡𝑘 metric is higher, the performance of\\nthe reward model is better. The high top-𝑘 CTR uplift metrics only\\nensure that the order of outputs is good, so we use another MSE\\nmetric to measure the difference between the predicted CTRs and\\nreal ones as follows:\\n𝑀𝑆𝐸 = 1\\n𝑛\\n𝑛\\n∑︁\\n𝑖=1\\n1\\n𝑚𝑖\\n𝑚𝑖\\n∑︁\\n𝑗=1\\n( ˆ𝑦𝑖\\n𝑗 − 𝑝𝑖\\n𝑗 )2\\n(21)\\n, where 𝑛 is the number of items and 𝑚𝑖 is the number of creatives\\nin 𝑖-th item.\\nThe commercial data and public data4 [25] are tested to demon-\\nstrate the effectiveness of our proposed reward model structure.\\nThe commercial data has 1.0M creatives from 286k items. Each cre-\\native has item title, image, caption (generated by CLIP-Interrogator),\\nreal click and real impression. Creatives from the 5% of items are\\nrandomly sampled as the testing data while the others are training\\ndata. The public data contains 1.7M creatives from 500k items. As\\nthere is no title information in public data, we average the image\\nfeatures of creatives from the same item as the title feature.\\nThe BERT and Swin in reward model are pre-trained on text\\ninformation (title and caption) and image information with token\\nmask loss and patch mask loss, respectively. Then in training reward\\nmodel process, the parameters of BERT and Swin are fixed to speed\\nup. We use adam optimizer with mini-batch of 2048 and learning\\nrate of 5e-4. The number of epochs is set 30.\\n4The public data is at https://tianchi.aliyun.com/dataset/93585.\\nA New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\nTable 2: Ablation study on reward model.\\nType\\nDetails\\nCommercial data\\nPublic data\\nTop-1 ↑\\nTop-2 ↑\\nTop-3 ↑\\nTop-4 ↑\\nTop-5 ↑\\nMSE ↓\\nTop-1 ↑\\nTop-2 ↑\\nTop-3 ↑\\nMSE ↓\\nPublic methods\\nVAM [25]\\n18.01%\\n11.92%\\n6.59%\\n2.81%\\n1.72%\\n2.518\\n6.26%\\n3.80%\\n1.23%\\n3.544\\nRank [51]1\\n19.87%\\n10.68%\\n5.51%\\n3.07%\\n1.71%\\n2.615\\n7.32%\\n4.60%\\n1.70%\\n3.545\\nFuse input\\ninformation\\nOnly image\\n18.32%\\n13.62%\\n6.44%\\n3.22%\\n2.06%\\n2.314\\n9.07%\\n5.66%\\n2.23%\\n3.135\\nImage and title2\\n22.26%\\n13.44%\\n7.21%\\n3.86%\\n2.52%\\n2.300\\n10.99%\\n6.91%\\n2.65%\\n3.094\\nImage and caption\\n21.59%\\n12.99%\\n7.49%\\n3.69%\\n2.58%\\n2.404\\n9.91%\\n6.22%\\n2.31%\\n3.103\\nw/o transformers\\n21.29%\\n11.32%\\n6.50%\\n2.90%\\n1.79%\\n2.145\\n11.20%\\n6.61%\\n2.56%\\n3.057\\nPre-train\\nw/o pre-train\\n21.05%\\n13.77%\\n7.53%\\n4.02%\\n2.51%\\n2.263\\n10.91%\\n6.64%\\n2.60%\\n3.250\\nOnly pre-train BERT\\n20.92%\\n13.36%\\n7.53%\\n4.54%\\n3.20%\\n2.060\\n10.51%\\n6.44%\\n2.60%\\n3.433\\nOnly pre-train Swin\\n22.08%\\n13.75%\\n6.63%\\n3.98%\\n2.41%\\n2.198\\n11.05%\\n6.61%\\n2.64%\\n3.132\\nLoss\\nOnly list-wise\\n21.91%\\n12.85%\\n5.69%\\n2.64%\\n2.51%\\n2.424\\n10.66%\\n6.53%\\n2.59%\\n3.231\\nOnly point-wise\\n21.35%\\n14.06%\\n7.44%\\n3.86%\\n2.60%\\n2.111\\n10.81%\\n6.64%\\n2.66%\\n3.058\\n-\\nOurs\\n23.81%\\n13.84%\\n7.56%\\n4.79%\\n3.29%\\n2.052\\n11.29%\\n7.00%\\n2.76%\\n3.047\\n1 As rank model uses entity textual knowledge and entity image knowledge while this information does not exist in the public data and commercial data, then we delete\\nthese input features to compare.\\n2 As there is no title information in public data, we use the averaged image features to simulate the title feature.\\nTable 3: Ablation study on prompt model.\\nType\\nDetails\\nHistorical CTR1\\nCTR\\nRevenue\\nUser ablation\\nw/o user\\n9.4%\\n7.2%\\n6.9%\\nIndividuation\\n12.5%\\n10.4%\\n9.7%\\nModel\\nstructure\\nw/o transformer\\n10.9%\\n-2\\n-\\nw/o DeepFM\\n11.7%\\n-\\n-\\nOurs\\n12.5%\\n-\\n-\\n1 This is an offline metric, it is the historical CTR improvement given the clicked\\ndata. In detail, we can calculate the cumulative CTR improvements of top-𝑘 tokens.\\n2 Model structure experiments are not performed online, and so only the offline\\nmetric of \"Historical CTR\" is shown.\\nWe tested two public methods, including VAM [25] and Rank\\n[51]. On the commercial data, the top-1 to top-5 CTR uplift scores\\nof our proposed method are much higher than VAM and Rank\\nwhile the MSE metric of the proposed method is smaller than other\\nmethods. We have similar results on the public data. In the reward\\nmodel, we use title, image and caption information to predict the\\nCTR scores. To demonstrate that the title and caption information is\\nalso helpful for this task, this information is deleted in the original\\nnetwork. As shown in Table 2, only considering the image feature\\nto predict the CTR, the results are not good. Simply concatenat-\\ning three multi-modal features without transformers can improve\\nthe CTR ranking effect, but the improvement is limited. Fusing\\ntransformers can further improve the results, which verifies the\\neffectiveness of combining different types of input information and\\nusing transformers to extract the multi-model feature.\\nIn our proposed method, we have pre-trained BERT and Swin\\nmodels with mask loss on corresponding data. As shown in this\\ntable, the results without pre-training are slightly inferior to our\\nresults, which verifies the importance of pre-training. Lastly, we\\nhave tested the effects of the loss functions. The results show that\\nboth list-wise and point-wise loss functions are useful for improving\\nthe results.\\n4.2.2\\nAblation Study on Prompt Model. This section will show the\\nablation study of the prompt model. We use online clicked data to\\ntrain the prompt model, with 20M samples as training data and 2M\\nsamples as testing data. Each sample has an average of 8.5 tokens.\\nWe employ Adam as the optimizer, set the learning rate to 1e-3, and\\ntrain the model in 2 epochs to avoid overfitting with a batch size of\\n2048. The parameter of prompt model trained in this clicked data\\nFigure 3: (a) Word cloud analysis of generated tokens by\\nprompt models without and with considering user infor-\\nmation. (b) The creatives generated by different versions of\\nLoRA and prompt models are scored by reward model to\\nshow the effectiveness of self-cycling training process.\\nis initialized as the first version of prompt model in self-cycling\\ntraining process.\\nWe perform two experiments to severally explain the advantage\\nof importing the user group information and the improvements\\nof prompt model in self-cycling training process. In the fusing\\nuser group information experiment, the user group information is\\ndeleted in prompt model in both training (in self-cycling training\\nmode) and testing processes, the CTR and revenue improvements\\nare both limited (Table 3). To further improve the results, we import\\nthe user groups information. In detail, for each item, all user groups\\nare enumerated, and one best prompt is selected for each user group\\nby prompt model. Assuming that there are 5 creatives for each item\\nwithout user information, after introducing user features, there are\\n5 × 𝑢 creatives where 𝑢 refers to the number of user groups. In on-\\nline display strategy, we select the corresponding 5 creatives with\\nthe same user group as the current user for personalized creative\\nrecommendation. As shown in this table, the metrics in \"individ-\\nuation\" have great improvements. In addition, we collected the\\ntokens generated by prompt model to perform the word cloud anal-\\nysis (Fig. 3(a)). After considering the user group information (e.g.,\\nyounger and elder), the generated tokens are more \"individualized\"\\nwhere younger users prefer \"electronic\" and \"realistic\" while elder\\nusers prefer \"pure\" and \"minimal\". This phenomenon illustrates that\\nprompt model can learn the fact that different user groups have\\nvarious preferences for diverse tokens.\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\nHao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng\\nFigure 4: Cases with the original image and generated creatives in different steps of self-cycling. Bottom score is the real CTR.\\nIn another experiment of self-cycling training, we compare the\\nresults of the first version and last version of prompt model (Table\\n1). After self-cycling training, the final version of prompt model has\\nhigher metrics which further verifies the necessity of self-cycling.\\nLastly, we find that the transformers and DeepFM sub-modules in\\nprompt model are also helpful with good historical CTR improve-\\nments.\\n4.2.3\\nSelf-cycling Analysis. In previous section, we have already\\ndemonstrated the effectiveness of reward model on commercial and\\npublic data. Then we can use reward model to offline analyze the\\nperformances of LoRA and prompt models in self-cycling training\\nprocess. As shown in Fig. 3(b), the reward scores of generated\\ncreatives increase during self-cycling training, demonstrating the\\neffectiveness of our proposed training process.\\n4.3\\nCase Study\\nLastly, we will show some amazing cases to validate the effec-\\ntiveness of our proposed creative generation pipeline (Fig. 4). The\\naesthetics and harmony of creatives generated by our proposed\\nmethod are much better than the original image provided by seller.\\n5\\nDISCUSSION\\nA new automated Creative Generation pipeline for Click-Through\\nRate (CG4CTR) is proposed to fuse the CTR target into the creative\\ngeneration task. In CG4CTR, we first use inpainting mode in dif-\\nfusion method to generate the creative image in advertising scene.\\nThe generation process is carried out by self-cycling, where the\\nprompt model and LoRA model are updated alternately and itera-\\ntively. As the traditional creative generation methods do not use the\\nCTR as the target, some bad creatives with poor CTR performance\\nmay be generated, leading to sub-optimal online results. To solve\\nthis problem, we fuse the CTR target in CG4CTR and consider user\\ninformation in prompt model to select the appropriate prompt and\\nindirectly generate individualized creatives. In self-cycling, we need\\nthe reward model to judge which creatives are good and which\\nones are bad, and then it can help to further improve the quality of\\ngenerated creatives. Both offline and online experiments show that\\nCG4CTR can generate better creatives with higher CTR compared\\nwith the original images provided by sellers.\\nOur proposed CG4CTR pipeline can be inserted at different\\nstages of the item recommendation. Specifically, if CG4CTR is in-\\nserted after the item recommendation (named A-stage), for one\\nuser comes, candidate items are ranked by the ranking model, then\\nthe top items are used for CG4CTR to generate creatives for each\\nitem. In A-stage, the ranking model can only consider the original\\nimage rather than creative image, leading to the reduced perfor-\\nmance. If CG4CTR is inserted before the item recommendation\\n(named B-stage), the creatives for all candidate items are generated\\nby CG4CTR, then ranking model can simultaneously consider both\\nitem information and creative information to sort them, which may\\nhave better performance but also need higher online time consum-\\ning. In future work, we will explore the difference in performance\\nbetween these two stages. Furthermore, we can employ various\\nA New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\nstyles of LoRA models (rather than one LoRA in CG4CTR) to gen-\\nerate creatives with enhanced styles and superior quality, where\\nthese styles can be provided by designers as initial samples to train\\nLoRA model, and then their parameters are updated in self-cycling\\ntraining mode. Lastly, more ways to generate creatives rather than\\nmodifying the background need to be investigated in the same\\nframework.\\nREFERENCES\\n[1] Christopher John Cornish Hellaby Watkins. 1989. Learning from delayed rewards.\\n(1989).\\n[2] Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensional-\\nity of data with neural networks. science 313, 5786 (2006), 504–507.\\n[3] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.\\narXiv preprint arXiv:1312.6114 (2013).\\n[4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial\\nnets. Advances in neural information processing systems 27 (2014).\\n[5] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolu-\\ntional networks for biomedical image segmentation. In Medical Image Computing\\nand Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,\\nMunich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 234–241.\\n[6] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele,\\nand Honglak Lee. 2016. Generative adversarial text to image synthesis. In Inter-\\nnational conference on machine learning. PMLR, 1060–1069.\\n[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. Advances in neural information processing systems 30 (2017).\\n[8] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\\nDeepFM: a factorization-machine based neural network for CTR prediction. arXiv\\npreprint arXiv:1703.04247 (2017).\\n[9] Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation\\nlearning. Advances in neural information processing systems 30 (2017).\\n[10] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive\\ngrowing of gans for improved quality, stability, and variation. arXiv preprint\\narXiv:1710.10196 (2017).\\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\\nPre-training of deep bidirectional transformers for language understanding. arXiv\\npreprint arXiv:1810.04805 (2018).\\n[12] Tiezheng Ge, Liqin Zhao, Guorui Zhou, Keyu Chen, Shuying Liu, Huimin Yi,\\nZelin Hu, Bochao Liu, Peng Sun, Haoyu Liu, et al. 2018. Image matters: Visually\\nmodeling user behaviors using advanced model server. In Proceedings of the\\n27th ACM International Conference on Information and Knowledge Management.\\n2087–2095.\\n[13] Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, Joelle\\nPineau, et al. 2018. An introduction to deep reinforcement learning. Foundations\\nand Trends® in Machine Learning 11, 3-4 (2018), 219–354.\\n[14] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. 2019. Generating diverse\\nhigh-fidelity images with vq-vae-2. Advances in neural information processing\\nsystems 32 (2019).\\n[15] Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator ar-\\nchitecture for generative adversarial networks. In Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition. 4401–4410.\\n[16] Zhichen Zhao, Lei Li, Bowen Zhang, Meng Wang, Yuning Jiang, Li Xu, Fengkun\\nWang, and Weiying Ma. 2019. What you look matters? offline evaluation of\\nadvertising creatives for cold-start problem. In Proceedings of the 28th ACM\\ninternational conference on information and knowledge management. 2605–2613.\\n[17] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and\\nTimo Aila. 2020. Analyzing and improving the image quality of stylegan. In\\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition.\\n8110–8119.\\n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\\nmodels. Advances in neural information processing systems 33 (2020), 6840–6851.\\n[19] Sreekanth Vempati, Korah T Malayil, V Sruthi, and R Sandeep. 2020. Enabling\\nhyper-personalisation: Automated ad creative generation and ranking for fashion\\ne-commerce. In Fashion Recommender Systems. Springer, 25–48.\\n[20] Shaunak Mishra, Manisha Verma, Yichao Zhou, Kapil Thadani, and Wei Wang.\\n2020. Learning to create better ads: Generation and ranking approaches for ad\\ncreative refinement. In Proceedings of the 29th ACM international conference on\\ninformation & knowledge management. 2653–2660.\\n[21] Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko\\nLehtinen, and Timo Aila. 2021. Alias-free generative adversarial networks. Ad-\\nvances in Neural Information Processing Systems 34 (2021), 852–863.\\n[22] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin,\\nJunyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. 2021. Cogview: Mastering\\ntext-to-image generation via transformers.\\nAdvances in Neural Information\\nProcessing Systems 34 (2021), 19822–19835.\\n[23] Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on\\nimage synthesis. Advances in neural information processing systems 34 (2021),\\n8780–8794.\\n[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,\\nBob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic\\nimage generation and editing with text-guided diffusion models. arXiv preprint\\narXiv:2112.10741 (2021).\\n[25] Shiyao Wang, Qi Liu, Tiezheng Ge, Defu Lian, and Zhiqiang Zhang. 2021. A\\nhybrid bandit model with visual priors for creative ranking in display advertising.\\nIn Proceedings of the web conference 2021. 2324–2334.\\n[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\net al. 2021. Learning transferable visual models from natural language supervision.\\nIn International conference on machine learning. PMLR, 8748–8763.\\n[27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\\nWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large\\nlanguage models. arXiv preprint arXiv:2106.09685 (2021).\\n[28] Axel Sauer, Kashyap Chitta, Jens Müller, and Andreas Geiger. 2021. Projected\\ngans converge faster. Advances in Neural Information Processing Systems 34 (2021),\\n17480–17492.\\n[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\\nand Baining Guo. 2021. Swin transformer: Hierarchical vision transformer us-\\ning shifted windows. In Proceedings of the IEEE/CVF international conference on\\ncomputer vision. 10012–10022.\\n[30] Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers\\nfor high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition. 12873–12883.\\n[31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec\\nRadford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation.\\nIn International Conference on Machine Learning. PMLR, 8821–8831.\\n[32] Yiqi Gao, Xinglin Hou, Yuanmeng Zhang, Tiezheng Ge, Yuning Jiang, and Peng\\nWang. 2022. Caponimage: Context-driven dense-captioning on image. arXiv\\npreprint arXiv:2204.12974 (2022).\\n[33] Min Zhou, Chenchen Xu, Ye Ma, Tiezheng Ge, Yuning Jiang, and Weiwei Xu.\\n2022. Composition-aware graphic layout GAN for visual-textual presentation\\ndesigns. arXiv preprint arXiv:2205.00303 (2022).\\n[34] Jihyeong Ko, Jisu Jeong, and Kyumgmin Kim. 2022. Contrastive Learning for\\nTopic-Dependent Image Ranking. In Workshop on Recommender Systems in Fash-\\nion and Retail. Springer, 79–98.\\n[35] Shiyao Wang, Qi Liu, Yicheng Zhong, Zhilong Zhou, Tiezheng Ge, Defu Lian, and\\nYuning Jiang. 2022. CreaGAN: An Automatic Creative Generation Framework\\nfor Display Advertising. In Proceedings of the 30th ACM International Conference\\non Multimedia. 7261–7269.\\n[36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\\n2022. Hierarchical text-conditional image generation with clip latents. arXiv\\npreprint arXiv:2204.06125 1, 2 (2022), 3.\\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn\\nOmmer. 2022. High-resolution image synthesis with latent diffusion models. In\\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition.\\n10684–10695.\\n[38] Xuebin Qin, Hang Dai, Xiaobin Hu, Deng-Ping Fan, Ling Shao, and Luc Van Gool.\\n2022. Highly accurate dichotomous image segmentation. In European Conference\\non Computer Vision. Springer, 38–56.\\n[39] Kaiyi Lin, Xiang Zhang, Feng Li, Pengjie Wang, Qingqing Long, Hongbo Deng,\\nJian Xu, and Bo Zheng. 2022. Joint Optimization of Ad Ranking and Creative Se-\\nlection. In Proceedings of the 45th International ACM SIGIR Conference on Research\\nand Development in Information Retrieval. 2341–2346.\\n[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L\\nDenton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim\\nSalimans, et al. 2022. Photorealistic text-to-image diffusion models with deep\\nlanguage understanding. Advances in Neural Information Processing Systems 35\\n(2022), 36479–36494.\\n[41] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova,\\nArsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park,\\nand Victor Lempitsky. 2022. Resolution-robust large mask inpainting with fourier\\nconvolutions. In Proceedings of the IEEE/CVF winter conference on applications of\\ncomputer vision. 2149–2159.\\n[42] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang,\\nVijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. 2022.\\nScaling autoregressive models for content-rich text-to-image generation. arXiv\\npreprint arXiv:2206.10789 2, 3 (2022), 5.\\n[43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.\\nTraining language models to follow instructions with human feedback. Advances\\nin Neural Information Processing Systems 35 (2022), 27730–27744.\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\nHao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, and Yifan Zeng\\n[44] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao,\\nChunyuan Li, and Yong Jae Lee. 2023. Gligen: Open-set grounded text-to-image\\ngeneration. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. 22511–22521.\\n[45] James Betker, Gabriel Goh, and et.al. Jing. 2023. Improving image generation\\nwith better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3.\\npdf (2023).\\n[46] Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi.\\n2023. LayoutDM: Discrete Diffusion Model for Controllable Layout Generation. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n10167–10176.\\n[47] Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, and Tat-Seng Chua. 2023.\\nLayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Genera-\\ntion. arXiv preprint arXiv:2308.05095 (2023).\\n[48] Simo Ryu. 2023. Low-rank adaptation for fast text-to-image diffusion fine-tuning.\\n[49] Hao Yang, Ziliang Wang, Weijie Bian, and Yifan Zeng. 2023. Practice on Effectively\\nExtracting NLP Features for Click-Through Rate Prediction. In Proceedings of the\\n32nd ACM International Conference on Information and Knowledge Management.\\n4887–4893.\\n[50] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine.\\n2023. Training diffusion models with reinforcement learning. arXiv preprint\\narXiv:2305.13301 (2023).\\n[51] Sheng You, Chao Wang, Baohua Wu, Jingping Liu, Quan Lu, Guanzhou Han,\\nand Yanghua Xiao. 2023. What Image do You Need? A Two-stage Framework\\nfor Image Selection in E-commerce. In Companion Proceedings of the ACM Web\\nConference 2023. 452–456.\\n[52] Zhiwei Tang, Dmitry Rybin, and Tsung-Hui Chang. 2023. Zeroth-Order Opti-\\nmization Meets Human Feedback: Provable Learning via Ranking Oracles. arXiv\\npreprint arXiv:2303.03751 (2023).\\n'},\n",
       " {'abstract': 'This research explores cryptocurrency staking reward prediction, offering insights to researchers and investors. Two methods are investigated: a sliding-window average and linear regression models. Results show ETH staking rewards can be forecasted with an RMSE within 0.7% and 1.1% for 1-day and 7-day look-aheads using a 7-day sliding-window average. Prediction accuracies vary across cryptocurrencies, with linear regression superior for short-term XTZ and ATOM predictions. Staking rewards are stable for most assets, except MATIC.',\n",
       "  'introduction': \"Staking in cryptocurrency involves users locking digital assets to participate in a blockchain's consensus mechanism, earning rewards. Various platforms have staking processes and reward determination methods. Staking rewards depend on factors like the amount of cryptocurrency staked, staking duration, inflation rates, and network transaction fees.\",\n",
       "  'literature review': 'Prior research has explored staking-related topics, such as optimal staking problems, equilibrium staking levels, and rewards farming. Other studies have focused on staking pools and centralization concerns. However, forecasting staking rewards has not been previously explored.',\n",
       "  'methodology': 'The study utilized two forecasting algorithms: moving-window average and linear regression models. The moving-window average calculates an average of past data points to predict future values. Linear regression involves creating a linear relationship between an explanatory variable and the dependent variable. The objective was to develop a model to forecast staking rewards for various cryptocurrencies for a specified number of upcoming days. Data was collected from 2021-06-23 to 2022-08-06, and a training and test data splitting strategy was employed to optimize model performance.',\n",
       "  'results': 'For next-day prediction, moving-window average and single-feature linear regression performed similarly for ETH, with RMSE divided by the mean ranging from 0.007 to 0.009. Linear regression outperformed moving-window average and multiple linear regression for ATOM and XTZ. Moving-window average performed best for SOL and MATIC. For next N-days prediction, moving-window average outperformed other methods for all tokens except MATIC. The effectiveness of ML algorithms decreased as prediction days increased, and a 7-day ahead prediction using moving-window average had an increased error of no more than 3.2% compared to a 1-day ahead prediction. Simple linear regression based on staking rewards time series alone outperformed multiple linear regression using other features.',\n",
       "  'conclusion': 'The study demonstrates the effectiveness of simple linear regression and moving-window average in predicting staking rewards. A 7-day moving-window average approach provides accurate predictions for most tokens, except MATIC, with an RMSE within 0.7% and 1.1% for 1-day and 7-day look-aheads. Prediction accuracies vary across cryptocurrencies, with linear regression superior for short-term XTZ and ATOM predictions. Staking rewards are stable for most assets, except MATIC, presenting a notable exception.',\n",
       "  'title': 'Forecasting Cryptocurrency Staking Rewards',\n",
       "  'author': 'Sauren Gupta, Apoorva Hathi Katharaki, Yifan Xu, Bhaskar Krishnamachari, Rajarshi Gupta',\n",
       "  'textdata': 'Forecasting Cryptocurrency Staking Rewards\\nSauren Gupta∗, Apoorva Hathi Katharaki†, Yifan Xu, Bhaskar Krishnamachari‡, Rajarshi Gupta\\nMachine Learning Team, Coinbase\\nAbstract: This research explores a relatively unexplored area of predicting cryp-\\ntocurrency staking rewards, offering potential insights to researchers and investors.\\nWe investigate two predictive methodologies: a) a straightforward sliding-window\\naverage, and b) linear regression models predicated on historical data. The findings\\nreveal that ETH staking rewards can be forecasted with an RMSE within 0.7% and\\n1.1% of the mean value for 1-day and 7-day look-aheads respectively, using a 7-day\\nsliding-window average approach. Additionally, we discern diverse prediction\\naccuracies across various cryptocurrencies, including SOL, XTZ, ATOM, and\\nMATIC. Linear regression is identified as superior to the moving-window average\\nfor perdicting in the short term for XTZ and ATOM. The results underscore the\\ngenerally stable and predictable nature of staking rewards for most assets, with\\nMATIC presenting a noteworthy exception.\\n1\\nIntroduction\\nStaking, within the context of cryptocurrency, is a mechanism wherein holders of specific digital\\ncurrencies, such as Ethereum or others utilizing a proof-of-stake (PoS) model, earn rewards by actively\\nengaging in transaction validation on a blockchain while they continue to hold on to their assets\\n[1, 2]. In essence, staking involves dedicating a specified quantity of cryptocurrency assets to fortify\\na blockchain network, subsequently facilitating the verification of transactions. Contrasting with\\nproof-of-work (PoW) systems, where computational power is crucial for transaction verification and\\nblock generation, a PoS model entrusts validation to stakers [3, 2]. These participants willingly lock\\na predetermined quantity of the network’s currency as collateral. Stakers, in turn, are selected—often\\nproportionally to their staked amount—to validate transactions and introduce new blocks to the\\nblockchain [4]. Engaging in staking not only allows individuals to earn rewards for strengthening\\nthe network but also augments their cryptocurrency holdings, thus offering an enticing incentive to\\nprocure and retain the cryptocurrency, thereby enhancing its overall security and stability [4].\\nMany digital currency networks, like ETH, Cosmos, Tezos, Algorand, Cardano, and Solana, give\\nrewards to users who agree to lock up (\"stake\") their digital tokens for a set time [5, 6]. This is an\\nincentive for users and it also makes the network more secure. For platforms that manage digital\\ncurrencies and blockchain networks, staking can also make customers more loyal because it creates a\\nkind of attachment or \"stickiness.\"\\nThe rewards allocated for staking are subject to change, contingent upon various factors such as the\\nnumber of participants staking, the maturity of the project, prevailing market conditions, among others\\n[7]. This inherent variability and unpredictability in the rewards offered for staking cryptocurrencies\\npose a multifaceted challenge. Various entities that invest, encompassing individual investors, institu-\\ntional investors, and platforms managing digital assets, may find predictive information regarding\\nfuture staking rewards beneficial. This could assist them in determining optimal times to stake or\\nun-stake their assets within a specific network.\\n∗The contributions of Sauren Gupta to this work were made during his tenure as an intern at Coinbase.\\n†Corresponding author. Email: apoorva.hathi@coinbase.com\\n‡Dr. Bhaskar Krishnamachari is a paid consultant for Coinbase and has contributed to this paper in this\\ncapacity.\\narXiv:2401.10931v1  [q-fin.ST]  16 Jan 2024\\nThe solution we propose in the paper involves the application of predictive models to forecast the\\nstaking rewards of a variety of cryptocurrencies. Our approach is predicated on the assumption that\\nhistorical trends and patterns in staking rewards can provide valuable insights into future performance.\\nTwo predictive approaches have been considered in this context, sliding-window average and linear\\nregression. Sliding-window average essentially involves computing the mean rewards over a specified\\ntime window, which is a straightforward and intuitive method that can provide a quick snapshot of\\nthe trend in staking rewards. However, it is important to note that this method assumes that the future\\nwill mirror the past, which may not always be the case, particularly in the volatile and unpredictable\\nworld of cryptocurrencies. Linear regression is an effective predictive model that can capture more\\ncomplex patterns in the data. It assumes a linear relationship between the independent and dependent\\nvariables, which may not always hold true. However, with careful feature selection and model tuning,\\nlinear regression can provide robust and reliable predictions. While more sophisticated models such\\nas LSTM [8] could be used for time-series prediction, we have limited ourselves to these simpler\\nmodels because they already show good performance, as the results show. Despite extensive research\\nin the field of cryptocurrency, there is a noticeable gap in the literature regarding the prediction\\nof staking rewards. This study aims to fill this gap by exploring this under-researched area. By\\nemploying straightforward but efficient predictive models, this research seeks to provide insights into\\nthe predictability of staking rewards across various cryptocurrencies. The findings of this study could\\npotentially provide valuable data for stakeholders and investors, aiding them in making informed\\nstaking decisions.\\nOur study shows that the sliding-window average method is effective in predicting Ethereum (ETH)\\nstaking rewards, with a Root Mean Square Error (RMSE) of 0.7% for a 1-day forecast and less than\\n1.1% for a 7-day forecast.This method surprisingly surpasses linear regression models in predicting\\nstaking rewards for this specific asset. However, for cryptocurrencies such as Tezos (XTZ) and\\nCosmos (ATOM), linear regression models proved to be more effective than the sliding-window\\naverage method in predicting staking rewards. The effectiveness of the sliding-window average and\\nlinear regression models was assessed by calculating the RMSE between the actual and predicted\\nstaking rewards over different forecast periods (1-day and 7-days). Additionally, the performance of\\nthese models was evaluated across various cryptocurrencies (ETH, SOL, XTZ, ATOM, and MATIC),\\noffering a comprehensive view of their applicability and efficiency. The findings of this study reveal a\\ndiverse range of predictive accuracy across various cryptocurrencies. In particular, the sliding-window\\naverage model exhibits a strong predictive capacity for Ethereum (ETH). Conversely, Tezos (XTZ)\\nand Cosmos (ATOM) demonstrate a higher forecasting proficiency when utilizing linear regression\\nmodels for short term. A common trend across the majority of assets is the formation of slow-varying\\nand moderately predictable time series in staking rewards. However, Polygon (MATIC) deviates from\\nthis pattern, displaying notably erratic rewards patterns.\\nIn summary, our study consists of the following highlights:\\n• Application of ML techniques for accurate staking rewards predictions across multiple assets, with\\nremarkable precision for short-term and long-term forecasts.\\n• Demonstrated good performance of simple models like averaging historical rewards and linear\\nregression for cryptocurrency staking rewards forecasting.\\n2\\nRelated Works\\nThere has been prior research work exploring various aspects of staking of cryptocurrencies. For\\nexample, Choi et al. [9] formulate an optimal staking problem in which they explore tradeoffs between\\nrewards and illiquidity. John et al. [7] analyze equilibrium staking levels and show that staking levels\\nmay not always be increasing in block rewards. Xu and Feng [10] provide a general survey of rewards\\nfarming in DeFi including considerations of staking as one of the components of rewards farming.\\nSome of this research has focused specifically on staking pools and concerns about centralization.\\nFor instance, He et al. [11] explore the question of staking centralization and conditions under which\\na stable equilibrium exists for staking pools; and Gersbach et al. [12] model the formation of staking\\n2\\npools in the presence of malicious adversaries. To our knowledge the research literature on staking\\nhas not previously explored the problem of forecasting staking rewards, the focus of this paper.\\nWhile there has been decades of research on forecasting financial time series [13], in particular stock\\nprices [14], over the past decade there has also been a lot of prior work on modeling and forecasting\\ncryptocurrency prices and the volatility of their prices. Kyriazis et al. [15] present a survey of\\nmodeling the “bubble” dynamics of cryptocurrency prices. Khedr et al. [16] and Amirzadeh et al.\\n[17] present a survey of applications of artificial intelligence and machine learning to cryptocurrency\\nprice and volatility prediction. This literature has explored a number of different machine learning\\nmodels for predictions. For example, Mudassir et al. [18] explore ML-based time-series forecasting\\nof Bitcoin prices using high-dimensional features. Hamayel and Owda [19] consider the use of\\nLSTM models for cryptocurrency price prediction. Derbentsev et al. [20] present a comparative study\\nof different ML prediction algorithms. D’Amato et al. [21] present application of deep learning to\\npredict the volatility of cryptocurrencies.\\nOur work adds a new dimension to the above literature on application of machine learning methods\\nto predictions in the context of cryptocurrencies by exploring staking rewards as the focus of the\\npredictions. We find that in this work that in contrast to the high volatility of cryptocurrency prices\\nwhich has been the primary focus of prior work, staking rewards are relatively slower-moving and\\nsimpler time series, and consequently, relatively simpler ML models show good performance.\\n3\\nBackground: Stacking and rewards\\nStaking is a fundamental concept in the realm of cryptocurrencies, allowing users to participate in a\\nnetwork’s operations while receiving rewards. It’s essential to understand that different platforms\\nhave varied approaches and rules governing their staking processes, particularly in how the staking\\nrewards are determined. Here’s a general breakdown:\\n3.1\\nHow Staking Works\\nStaking involves users, or \"stakers,\" holding and locking up a cryptocurrency in a wallet to participate\\nin the network’s consensus mechanism, typically Proof of Stake (PoS) or its variants. By doing this,\\nstakers help secure the network, validate transactions, and maintain the blockchain’s overall integrity.\\nIn return, they receive staking rewards, usually in the form of cryptocurrency.\\n3.2\\nDetermining rewards\\nRewards from staking can be influenced by various factors, depending on the specific network’s\\nprotocol. Generally, the more tokens held and the longer they are staked, the higher the potential\\nreturn. Factors influencing rewards may include the total amount of cryptocurrency staked in the\\nnetwork, staking duration, token inflation rate, and network transaction fees.\\n3.3\\nStaking Duration\\nDifferent platforms might have varied rules regarding the length of time the cryptocurrencies need to\\nbe held for staking (\"staking duration\") and the time it takes to withdraw them (\"un-staking time\").\\nSome networks may require a minimum staking duration to qualify for rewards, and there might be a\\nwaiting period (cooling-off period) for accessing or \"unstaking\" the staked tokens. This approach\\nencourages network stability by discouraging rapid, speculative movements of funds.\\n4\\nFeature Selection\\nIn our study, we tried to understand the details of staking rewards, focusing on the following\\ncryptocurrencies: ETH, SOL, XTZ, MATIC and ATOM. We carefully chose and reviewed various\\ndatasets and features. It was crucial to pick data that was helpful and of good quality, considering that\\n3\\nthere is limited data available for some specific assets. This section explains the datasets we used, the\\nfeatures we looked at, and the thoughtful methods behind leaving out certain features.\\n4.1\\nConsidered Features\\nTable 1 below delineates the selected features and their corresponding data sources that were utilized in\\nthis study. These features were extracted from a variety of sources, including Coinbase data storage,\\nGoogle Trends and YFinance. The raw data obtained from these sources underwent significant\\nstructuring and refinement to tailor it for analysis.\\nTable 1: Overview of Extracted Features and Their Data Sources\\nFeature\\nDescription\\nTime series of rewards\\nHistorical data representing rewards over a period of time\\nPrice feed\\nReal-time price information of the asset\\nTrends\\nThe frequency of searches for the cryptocurrency asset\\nTaking the example of ETH, a collection of time-series data for about one year from 2021-06-23 to\\n2022-08-06 was diligently gathered in Figure 1. This data, related to the features mentioned in Table\\n1 , was closely examined to understand its relationship and influence on staking rewards.\\n(a) Daily ETH price\\n(b) ETH trends\\n(c) ETH reward rate\\nFigure 1: Ethereum (ETH) Data Visualization (2021-06-23 to 2022-08-06) - Showcasing Daily Price,\\nSearch Trends, and Reward Rate.\\n4.2\\nExclusion and Unexplored Features\\nIn our study, we also looked at some additional features to understand their importance and effect,\\nwhich are mentioned in Table 2. After careful review, some features were left out after thoughtful\\nconsideration and testing. Specific features such as ‘staking volume’ and ‘percentage of staked\\nvolume’ were excluded post-analysis, as they were found to closely determine the staking rewards for\\nETH. The similarity and near redundancy of these features, upon normalization, further justified their\\nomission. Moreover, post-merge values for ETH were deliberately left out due to observed variations\\nin trends and a lack of substantial data to ensure a fortified model training and testing process.\\nCertain features, despite being considered, were not incorporated into the final model. These\\nunexplored features include: User count, Distribution of staked tokens, Transaction data on the asset.\\nThe decision to not utilize these features was a calculated one, ensuring that only the most significant\\nand influential features were utilized, optimizing the study’s objective of analyzing staking rewards.\\nTable 2: Excluded and unexplored features\\nFeature\\nDescription\\nStaking volume\\nTotal amount staked on the asset\\nPercentage of staked volume\\nPercentage of total assets that are staked\\nDistribution of staked tokens\\nGranularity based on addresses/wallets\\nTransaction data on asset\\nDetailed transaction data associated with the asset\\nUser count\\nNumber of users engaged in staking activities\\n4\\n5\\nExperimental Setup\\n5.1\\nForecasting Algorithms\\nFor forecasting the staking rewards, two algorithms were employed: the moving-window average and\\nlinear regression models. Here is a brief overview of the methods used:\\n• Moving-Window Average (MWA): The algorithm involves using a set number of past data points\\nto calculate an average that predicts future values. Due to its simplicity and absence of an explicit\\ntraining/learning process, it serves as a fundamental test, assuming that the rewards changes\\nrelatively slowly.\\n• Linear Regression: In this approach, simple linear regression was utilized to enhance the model’s\\npredictive accuracy. Simple linear regression involves creating a linear relationship between a\\nsingle explanatory variable and the dependent variable. This method is used to forecast future\\nvalues based on past trends, and it’s a common practice to improve the performance of forecasting\\nmodels. Two approaches of linear regression were employed\\n– Single Linear Regression (SLR): We use a single feature - as input to the linear regression\\nmodel.\\n– Multiple Linear Regression (MLR): We use multiple features as input to the multiple regres-\\nsion model.\\n5.2\\nObjective of the Model\\nOur objective was to develop a machine learning model adept at forecasting the staking rewards of\\nETH and other stakeable assets for a specified number of upcoming days (n). The experimentation\\nstarted with forecasts for the next day (n=1), gradually extending to a week ahead (n=7), to assess\\nand compare the performance and reliability of the results obtained.\\n5.3\\nData Collection and Utilization\\nData for 2021-06-23 to 2022-08-06 was assembled, focusing on features pivotal to staking rewards.\\nA comparison was conducted between linear regression models and moving-average model, aiming\\nto gauge the efficacy of the staking rewards prediction models for n-day ahead forecasts.\\n5.4\\nTraining and Test Data Splitting Strategy\\nAn approach was devised for splitting the data into training and test sets. Various strategies were\\nexplored to ascertain the most effective method of partitioning the data to optimize the model’s\\nperformance in forecasting. We found that training for three previous months and using it to test for\\nthe next month gives good performance. This informed approach allowed for the refinement of the\\nmodel to achieve a more insightful and dependable forecasting setup.\\n5.5\\nMetrics\\nThe metrics considered is root mean square error (RMSE), which measures the average difference\\nbetween a statistical model’s predicted values and the actual values.\\n6\\nExperimental Results\\n6.1\\nStaking Rewards Next Day Prediction\\nIn Table 3, we present staking rewards prediction for 5 assets with Metrics: RMSE divided by the\\nmean\\n• Moving-Window Average: we predict the next day’s rewards as the average of the past 7 days.\\n5\\nTable 3: Staking Rewards Next Day Prediction Performance (RMSE/Mean)\\nAsset\\nMoving-Window\\nAverage\\nSingle Linear Regres-\\nsion (rewards only)\\nMultiple Linear Regression\\n(rewards, price, trends)\\nETH (pre-merge)\\n0.007\\n0.007\\n0.009\\nSOL\\n0.029\\n0.031\\n0.041\\nMATIC\\n0.610\\n0.673\\n0.807\\nATOM\\n0.017\\n0.011\\n0.018\\nXTZ\\n0.054\\n0.046\\n0.101\\n• Linear Regression with single feature (rewards only) and multiple features (rewards, price, trends)\\nHere we consider 7 days data to predict the next day, ie, we feed in 7 days of data as input and\\npredict the next day’s value. And Train-Test Split: 90 days for training followed by 30 days for\\ntest.\\nFor ETH, moving window average and single feature linear regression perform comparably. For\\nATOM and XTZ, single feature linear regression outperforms MWA and multiple linear regression.\\nFor SOL and MATIC, MWA performs the best among all methods. MATIC performs poorly compared\\nto other tokens with all the methods, which seems to be a consequence of the highly volatile rewards\\nrate for the token.\\n(a) ETH\\n(b) SOL\\n(c) MATIC\\n(d) ATOM\\n(e) XTZ\\nFigure 2: Staking Rewards Next Day Predictions of rewards for 5 cryptocurrencies ETH, SOL,\\nMATIC, ATOM and XTZ, using MWA and SLR\\nIn Figure 2, we show the plotted predictions of all the tokens with the chosen parameters from Table\\n3, for 316 days from 2021-09-14 to 2022-07-27. The plots show the predictions of different methods\\n- moving window average, single linear regression and multiple linear regression. SOL and MATIC\\nshow high volatility in actual rewards during certain times.\\n6.2\\nStaking Rewards Next N-Days Prediction\\nWe further explore the prediction of rewards when the n-days are predicted instead of just 1 day ahead\\nprediction, as shown in Table 4, for 316 days from 2021-09-14 to 2022-07-27. Metrics are RMSE\\ndivided by the mean. The moving window average performs better overall for all of the tokens except\\nMATIC. SLR performs better than MWA for shorter term prediction like 1-day or 2-day ahead.\\nIn Figure 3 and Figure 4, we show the plotted predictions of rewards all tokens for 1 to 7 days ahead,\\nwith the chosen parameters from Table 4 for MWA and SLR approach. The predictions plotted are\\n6\\nfor the first 30 days of our datatset. The plots show that the predictions are more precise for MWA,\\nalthough the fluctuations are better captured by the linear regression model.\\nTable 4: Staking Rewards Next N-Day Prediction Performance Over Days (RMSE/Mean)\\nETH (pre-merge)\\nSOL\\nXTZ\\nMATIC\\nATOM\\nN\\nMWA\\nSLR\\nMWA\\nSLR\\nMWA\\nSLR\\nMWA\\nSLR\\nMWA\\nSLR\\n1\\n0.007\\n0.007\\n0.029\\n0.032\\n0.051\\n0.048\\n0.612\\n0.628\\n0.017\\n0.011\\n2\\n0.008\\n0.008\\n0.023\\n0.041\\n0.056\\n0.061\\n0.613\\n0.623\\n0.019\\n0.017\\n3\\n0.008\\n0.009\\n0.031\\n0.048\\n0.061\\n0.079\\n0.619\\n0.639\\n0.020\\n0.023\\n4\\n0.009\\n0.011\\n0.034\\n0.051\\n0.065\\n0.097\\n0.634\\n0.630\\n0.022\\n0.030\\n5\\n0.010\\n0.014\\n0.034\\n0.054\\n0.070\\n0.114\\n0.655\\n0.639\\n0.024\\n0.036\\n6\\n0.010\\n0.018\\n0.037\\n0.061\\n0.075\\n0.130\\n0.663\\n0.635\\n0.026\\n0.042\\n7\\n0.011\\n0.022\\n0.040\\n0.071\\n0.083\\n0.145\\n0.671\\n0.634\\n0.028\\n0.048\\n(a) ETH\\n(b) SOL\\n(c) XTZ\\n(d) MATIC\\n(e) ATOM\\nFigure 3: Staking Rewards Next N-day Predictions of 5 cryptocurrencies ETH, SOL, MATIC, ATOM\\nand XTZ using MWA\\n7\\nDiscussion and Conclusion\\nWe find that among all our strategies, 7-day Moving Window Average for one-day predictions gives\\nbetween 0.7-5% error (RMSE/MEAN) and performs well for all tokens, except MATIC. Prediction\\nfor MATIC performs poorly compared to other tokens with all the methods, which seems to be a\\nconsequence of the highly volatile rewards rate for the token.\\nWhen looking at short term prediction, i.e., n<3, for ETH, MWA and single feature linear regression\\nperform comparably. For ATOM and XTZ, single feature linear regression outperforms moving\\nwindow average and multiple linear regression. For SOL and MATIC, moving window average\\nperforms the best among all methods.\\nFor longer days ahead prediction, i.e., n>=3, MWA outperforms other methods for all tokens except\\nMATIC. This shows that a simple approach such as moving window average is sufficient and the\\neffectiveness of complicated ML algorithms reduces as we start predicting further days ahead into\\nthe future.\\nFor all assets except MATIC, a 7-days ahead prediction using MWA has an increased error of no\\nmore than 3.2% compared to a 1-day ahead prediction.\\n7\\n(a) ETH\\n(b) SOL\\n(c) XTZ\\n(d) MATIC\\n(e) ATOM\\nFigure 4: Staking Rewards Next N-day Predictions of 5 cryptocurrencies ETH, SOL, MATIC, ATOM\\nand XTZ using SLR\\nSimple linear regression based on the staking rewards time series alone was found to outperform\\nmultiple linear regression using other features. This shows the effectiveness of simpler strategies\\nwhen it comes to prediction of staking rewards.\\nSome questions that could be explored in future work include handling of non-stationary token data\\nsuch as pre and post-merge ETH and when to retrain them.\\nAcknowledgement Sauren Gupta’s involvement in this paper reflects work that he did while he was\\nemployed as an intern at Coinbase in 2022. Dr. Krishnamachari is a paid consultant for Coinbase and\\nhas assisted on this paper in this capacity.\\nReferences\\n[1] F. Saleh. Blockchain without waste: Proof-of-stake. The Review of financial studies, 34(3):\\n1156–1190, 2021.\\n[2] V. Buterin. What proof of stake is and why it matters. Bitcoin Magazine, 26, 2013.\\n[3] P. R. Nair and D. R. Dorai. Evaluation of performance and security of proof of work and proof of\\nstake using blockchain. In 2021 Third International Conference on Intelligent Communication\\nTechnologies and Virtual Mobile Networks (ICICV), pages 279–283. IEEE, 2021.\\n[4] C. T. Nguyen, D. T. Hoang, D. N. Nguyen, D. Niyato, H. T. Nguyen, and E. Dutkiewicz. Proof-\\nof-stake consensus mechanisms for future blockchain networks: fundamentals, applications and\\nopportunities. IEEE access, 7:85727–85745, 2019.\\n[5] V. Buterin et al. Ethereum white paper. GitHub repository, 1:22–23, 2013.\\n[6] J. Kwon and E. Buchman. Cosmos whitepaper. A Netw. Distrib. Ledgers, 27, 2019.\\n[7] K. John, T. J. Rivera, and F. Saleh. Equilibrium staking levels in a proof-of-stake blockchain.\\nAvailable at SSRN 3965599, 2021.\\n[8] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):\\n1735–1780, 1997.\\n[9] K. J. Choi, J. Jeon, and B. H. Lim. Optimal staking and liquid token holding decisions in\\ncryptocurrency markets. Available at SSRN 4528742, 2023.\\n8\\n[10] J. Xu and Y. Feng. Reap the harvest on blockchain: A survey of yield farming protocols. IEEE\\nTransactions on Network and Service Management, 20(1):858–869, 2022.\\n[11] P. He, D. Tang, and J. Wang. Staking pool centralization in proof-of-stake blockchain network.\\nAvailable at SSRN 3609817, 2020.\\n[12] H. Gersbach, A. Mamageishvili, and M. Schneider. Staking pools on blockchains. arXiv\\npreprint arXiv:2203.05838, 2022.\\n[13] Y. Tang, Z. Song, Y. Zhu, H. Yuan, M. Hou, J. Ji, C. Tang, and J. Li. A survey on machine\\nlearning models for financial time series forecasting. Neurocomputing, 512:363–380, 2022.\\n[14] P. Soni, Y. Tewari, and D. Krishnan. Machine learning approaches in stock price prediction: A\\nsystematic review. In Journal of Physics: Conference Series, volume 2161, page 012065. IOP\\nPublishing, 2022.\\n[15] N. Kyriazis, S. Papadamou, and S. Corbet. A systematic review of the bubble dynamics of\\ncryptocurrency prices. Research in International Business and Finance, 54:101254, 2020.\\n[16] A. M. Khedr, I. Arif, M. El-Bannany, S. M. Alhashmi, and M. Sreedharan. Cryptocurrency price\\nprediction using traditional statistical and machine-learning techniques: A survey. Intelligent\\nSystems in Accounting, Finance and Management, 28(1):3–34, 2021.\\n[17] R. Amirzadeh, A. Nazari, and D. Thiruvady. Applying artificial intelligence in cryptocurrency\\nmarkets: A survey. Algorithms, 15(11):428, 2022.\\n[18] M. Mudassir, S. Bennbaia, D. Unal, and M. Hammoudeh. Time-series forecasting of bitcoin\\nprices using high-dimensional features: a machine learning approach. Neural computing and\\napplications, pages 1–15, 2020.\\n[19] M. J. Hamayel and A. Y. Owda. A novel cryptocurrency price prediction model using gru, lstm\\nand bi-lstm machine learning algorithms. AI, 2(4):477–496, 2021.\\n[20] V. Derbentsev, V. Babenko, K. Khrustalev, H. Obruch, and S. Khrustalova. Comparative\\nperformance of machine learning ensemble algorithms for forecasting cryptocurrency prices.\\nInternational Journal of Engineering, 34(1):140–148, 2021.\\n[21] V. D’Amato, S. Levantesi, and G. Piscopo. Deep learning in predicting cryptocurrency volatility.\\nPhysica A: Statistical Mechanics and its Applications, 596:127158, 2022.\\n9\\n'},\n",
       " {'abstract': 'This paper describes a virtual reality (VR) serious game designed to foster understanding of dyslexia and raise awareness of the challenges it can create, with the aim of encouraging a supportive environment that promotes academic success for dyslexic students.',\n",
       "  'introduction': 'Dyslexia, particularly phonological dyslexia, causes difficulties in connecting sounds of words to their written forms, leading to challenges like slow reading speed and difficulty decoding unfamiliar words. These difficulties can be frustrating for students, potentially resulting in feelings of misunderstanding or stigmatization. To overcome these obstacles, dyslexic students often rely on compensatory tools and strategies. However, raising awareness and empathy among non-dyslexic individuals, including teachers and peers, can contribute to the provision of essential support for dyslexic students.',\n",
       "  'literature review': 'Studies have demonstrated the potential of VR technology in promoting social inclusion and facilitating empathy. Previous research has explored the use of VR to increase social skills in individuals with Autism Spectrum Disorder and to enhance empathy towards wheelchair users. VR has also been utilized to address some of the problems caused by dyslexia in children, showing improvements in attention and indicating the potential for long-term benefits in reading skills.',\n",
       "  'methodology': 'The methodology employed involves immersing players, such as teachers or peers of dyslexic students, in a virtual world where they are tasked with reading a recipe book and correctly adding ingredients to a potion. However, the text in the book and ingredient labels are presented in a font designed to replicate the reading difficulties experienced by people with dyslexia. The game flow includes three attempts to complete the task, with increasing time limits and the introduction of compensatory tools such as shorter words and an audio guide. The game ends when all attempts are exhausted or when the player successfully creates the correct potion.',\n",
       "  'results': 'The VR experience was tested on 32 non-dyslexic individuals. The results showed that most participants were unable to solve the task initially, reporting feelings of frustration and anxiety due to the difficulties in reading the instructions. However, these feelings tended to dissipate when more time and compensatory tools were provided, leading to an increased awareness of the issues and needs of dyslexic students. A survey conducted after the VR experience indicated that participants perceived the task as difficult but also reported an increase in their empathy towards people with dyslexia.',\n",
       "  'conclusion': 'The VR experience presented in this paper is considered a promising tool for raising empathy towards people with dyslexia. Future research will involve collecting data from a larger sample size to validate the effectiveness of the serious game and to study user empathy profiles through the application of artificial intelligence techniques. Additionally, other serious games are being designed to foster empathy for individuals with different types of dyslexia.',\n",
       "  'title': 'A VR Serious Game to Increase Empathy towards Students with Phonological Dyslexia',\n",
       "  'author': 'José M. Alcalde-Llergo, Enrique Yeguas-Bolívar, Pilar Aparicio-Martínez, Andrea Zingoni, Juri Taborri, Sara Pinzi',\n",
       "  'textdata': 'A VR Serious Game to Increase Empathy towards\\nStudents with Phonological Dyslexia\\nJos´e M. Alcalde-Llergo\\nDept. of Economics, Engineering,\\nSociety and Business Organization (DEIM)\\nUniversity of Tuscia\\nViterbo, Italy\\njose.alcalde@unitus.it\\nEnrique Yeguas-Bol´ıvar\\nComputing and Numerical Analysis\\nUniversity of C´ordoba\\nC´ordoba, Spain\\neyeguas@uco.es\\nPilar Aparicio-Mart´ınez\\nNursing, Physiotherapy and Pharmacology\\nUniversity of C´ordoba\\nC´ordoba, Spain\\nn32apmap@uco.es\\nAndrea Zingoni\\nDept. of Economics, Engineering,\\nSociety and Business Organization (DEIM)\\nUniversity of Tuscia\\nViterbo, Italy\\nandrea.zingoni@unitus.it\\nJuri Taborri\\nDept. of Economics, Engineering,\\nSociety and Business Organization (DEIM)\\nUniversity of Tuscia\\nViterbo, Italy\\njuri.taborri@unitus.it\\nSara Pinzi\\nDept. Physical Chemistry\\nand Applied Thermodynamics\\nUniversity of C´ordoba\\nC´ordoba, Spain\\nqf1pinps@uco.es\\nAbstract—Dyslexia is a neurodevelopmental disorder that is\\nestimated to affect about 5-10% of the population. In particular,\\nphonological dyslexia causes problems in connecting the sounds\\nof words with their written forms. This results in difficulties\\nsuch as slow reading speed, inaccurate reading, and difficulty\\ndecoding unfamiliar words. Moreover, dyslexia can also be a\\nchallenging and frustrating experience for students as they may\\nfeel misunderstood or stigmatized by their peers or educators.\\nFor these reasons, the use of compensatory tools and strategies\\nis of crucial importance for dyslexic students to have the same\\nopportunities as non-dyslexic ones. However, generally, people\\nunderestimate the problem and are not aware of the importance\\nof support methodologies. In the light of this, the main purpose\\nof this paper is to propose a virtual reality (VR) serious game\\nthrough which teachers, students and, in general, non-dyslexic\\npeople could understand which are some of the issues of student\\nwith dyslexia and the fundamental utility of offering support to\\nthem. In the game, players must create a potion by following\\na recipe written in an alphabet that is specifically designed to\\nreplicate the reading difficulties experienced by individuals with\\ndyslexia. The task must be solved first without any help and then\\nby receiving supporting tools and strategies with the idea that\\nthe player can put himself in the place of the dyslexic person\\nand understand the real need for support methodologies.\\nIndex Terms—Virtual reality, Simulation, Inclusion, Dyslexia,\\nEmpathy, Serious game\\nI. INTRODUCTION\\nDyslexia is a specific learning disorder that causes signifi-\\ncant difficulties in learning skills related to reading [1]. It is\\nestimated to affect approximately 5-10% of the population [2],\\nwhich is equivalent to about 700 million people worldwide.\\nMore specifically, phonological dyslexia is an impairment\\nof reading novel words (non-words) with otherwise good\\nperformance in reading familiar words [3]. Individuals with\\nphonological dyslexia often experience problems to connect\\nthe sounds of words to their written forms, which can hinder\\ntheir ability to recognize and recall new words. Phonological\\ndyslexia is thought to be caused by differences in the way\\nthe brain processes language, particularly in the regions of the\\nbrain that are responsible for phonological processing [4]. This\\ncan result in reading difficulties, such as slow reading speed,\\ninaccurate reading, and difficulty with decoding unfamiliar\\nwords [5]. However, with appropriate support and interven-\\ntions individuals with phonological dyslexia can compensate\\nthese problems and can achieve success in their academic and\\nprofessional lives. One of this support is represented by the\\ninclusion of dyslexic students by their classmates and teachers.\\nThus, empathy towards dyslexic students is of fundamental\\nimportance and stimulating it is one of the main objectives of\\nthis work. Indeed, dyslexia can be a challenging and frustrating\\nexperience for students as they may feel misunderstood or\\nstigmatized by their peers or educators. Demonstrating em-\\npathy towards students with dyslexia can help to create a\\nsupportive and inclusive learning environment, where they feel\\nunderstood, validated, and respected [6]. This can help them to\\nfeel more confident and motivated to learn, and ultimately lead\\nto greater success in their academic and professional lives.\\nTo achieve this, we propose a VR serious game to promote\\nunderstanding of dyslexia and make people more conscious of\\nthe problem that it can create. This should foster the creation\\nof a supportive environment that enables study success for\\ndyslexic students.\\nThe present work emerges as part of the VRAIlexia\\nproject [7], an initiative designed to increase awareness and\\nprovide support for university students with dyslexia. The\\nmain objective of the project is to mitigate the challenges\\narising from dyslexia among students in higher education,\\nwith the goal of reducing the incidence of university dropouts\\nand facilitating access to degree programs for individuals with\\narXiv:2401.10926v1  [cs.HC]  15 Jan 2024\\ndyslexia, a severe problem addressed in [8].\\nII. RELATED WORKS\\nA. Virtual reality for inclusion\\nVirtual reality (VR) technology has the potential to create\\nimmersive and interactive environments that can simulate real-\\nworld experiences. This technology can be applied to all kinds\\nof fields, including education, healthcare, entertainment, and\\nsocial sciences. In particular, VR has shown promising results\\nin promoting social inclusion as can be seen in works such as\\nthe Includiamoci project [9]. It is a social inclusion initiative\\nthat utilizes VR and spatial augmented reality technologies\\nto create safe spaces for collaborative activities based on art\\ntherapy techniques and new technologies. The project aims to\\nrecognize and enhance individual differences while promoting\\nteamwork and continuous confrontation between participants,\\neducators, and experts in cultural heritage and technologies.\\nVR has also been applied to achieve the inclusion of\\ndisabled people into the labor market [10]. In this case, authors\\nuse immersive environments to help people with disabilities\\nto increase employment opportunities. The system aims to\\nfacilitate the integration of individuals with disabilities into\\nthe labor market by simulating work activities and enabling\\nthe development of skills in a pleasurable and active way. The\\nsoftware also serves as an alternative communication system,\\npromoting social integration and cognitive development.\\nIn other fields, VR has been used for inclusion by proposing\\nactivities in virtual environments that help working on the diffi-\\nculties faced by some social groups. An example of this can be\\nseen in a case study where authors investigated the impact of a\\n“Virtual Reality Social Cognition Training” to enhance social\\nskills in children with Autism Spectrum Disorder (ASD) [11].\\nDuring the study, the performance of 30 children with ASD\\nwas measured in different domains by putting them in different\\nsituations such as doing a team project in the classroom\\nor ordering food in the school cafeteria. The study findings\\nindicated that utilizing a virtual reality platform is a promising\\ntreatment approach to understanding better social impairments\\nfrequently observed in individuals with ASD. Specifically,\\nenhancements in emotion recognition, social attribution, and\\nexecutive function of analogical reasoning where shown.\\nB. Virtual reality for empathy\\nAnother application where VR can exploit its potential is\\nin facilitating empathy towards a social group. There are\\nstudies that prove the beneficts of VR in this task, such as\\nthe performed in [12]. In this study participants where chosen\\nrandomly to view a documentary featuring a young girl living\\nin a refugee camp in a VR format. They found that VR can\\nlead to greater experience of empathy.\\nAlso a VR application as a means to empathize a case\\nstudy focused in wheelchair users was developed in [13].\\nThe objective of this study was to analyze the effects of a\\nsimulation aimed at replicating the challenges that a student\\nwho uses a wheelchair would face when performing various\\ntasks in their daily lives. The results showed that the simulation\\nexperience changed attitudes towards persons with disabilities\\nin the real world and made them advocates for change. The\\nstudy adds to existing research on the importance of VR\\nsimulations in improving empathy towards others.\\nC. Virtual reality for dyslexia\\nThe use of VR to address some of the problems caused by\\ndyslexia in children has also been a frequent topic of research\\nin recent years. In addition, studies such as [14] have shown\\nthat the use of VR can improve the memory and skills of\\npeople with dyslexia. In this case, users were subjected to\\ndifferent tests in a virtual classroom where they had to perform\\naccording to what appeared on the blackboard. The results did\\nnot show clear improvements in their reading skills, but they\\nshowed a clear improvement in their attention, which could\\nlead, in the longer term, to improvements in other problems\\ncaused by dyslexia, as reducing time in reading low frequency\\nlong words.\\nA clear case study where dyslexia is dealt using VR can be\\nseen in the European project FORDYS-VAR [15], whose main\\nobjective is to offer a way to facilitate the learning of people\\nwith dyslexia through technology, more specifically using VR\\nand augmented reality (AR). This project is especially focused\\non helping dyslexic children between 10 and 16 years old.\\nAmong its contributions, authors develop a support software\\nfor children with dyslexia [16] with which they can work\\nto alleviate several of the learning disabilities caused by this\\ndisorder in a more entertaining way, through a virtual reality\\nvideo game on the Oculus Quest platform.\\nIt is worth noting that most of the studies on VR applied\\nto dyslexia do not consider people at the higher education\\nstage. Conversely, VRAIlexia project aims to help these stu-\\ndents through VR and artificial intelligence technologies [7].\\nDuring the project different VR applications were designed\\nand developed, including an application for the realization of\\npsychometric tests for students with dyslexia [17], as well as\\nthe serious game that we present during the current document.\\nIII. METHODOLOGY\\nThe methodology proposed in this study concerns making\\nplayers, such as teachers or peers of dyslexic students, feel\\ninside a virtual world where they have to complete a task.\\nThe task requires participants to read a recipe book and\\ncorrectly adding ingredients in the order and quantity specified.\\nHowever, to simulate the reading difficulties experienced by\\npeople with dyslexia, the text in the book and ingredient\\nlabels are presented in the Britton’s Dyslexia font [18], an\\nalphabet that eliminates certain parts of letters, making reading\\nmore challenging. Figure 1 shows how the word “Dyslexia”\\nis written using this alphabet.\\nFig. 1: ‘The word ‘DYSLEXIA” using Britton’s font [18].\\nA. Application flow\\nFigure 2 shows the application flow of “In the shoes of\\ndyslexic students (Potion)”.\\nGame\\nexplanation\\nStart level\\nPick potion\\nCorrect\\npotion?\\nWin\\nLoss\\nAdd new\\ncompensatory\\ntools/strategies\\nFinal\\nlevel?\\nLast\\npotion?\\nNo\\nNo\\nYes\\nYes\\nYes\\nNo\\nTime \\nleft?\\nYes\\nNo\\nFig. 2: Flow diagram.\\nOnce players enter the game they find themselves inside a\\nkind of “magic castle”, in front of a closed door. The reason\\nbehind using a fantasy design for the virtual rooms was two-\\nfold: its widespread popularity and its ability to enhance the\\ngame’s allure while also providing a purpose for the task.\\nWhen they go through the door they meet their potions teacher,\\nan avatar that will tell them that they must prepare a potion\\nto save their friend, called Sam. She explains that the potion\\ncan be created by adding different ingredients strictly in the\\ncorrect order into a pot, according to the recipe given in a\\nbook. Moreover, a time limit of 3 minutes must be respected.\\nIngredients are distributed throughout the room on different\\nshelves.\\nAfter that, players initiate their first attempt to save Sam.\\nAs they fail these attempts, they will be given more time, 5 or\\n10 minutes, and new compensatory tools, like shorter words\\nand an audio guide, to try saving their friend again. A level\\nfailure occurs when time runs out or when a wrong ingredient\\nis put into the pot or, also, a correct ingredient is dropped in\\nan incorrect order. The game will end once the players fail all\\nthree attempts given or succeed in making the potion correctly\\nduring one of them.\\nB. Virtual rooms\\nWithin the virtual castle, participants will be granted admit-\\ntance to three distinct rooms. The initial of these enclosures is\\nthe starting point of our exploration, where players can famil-\\niarize themselves with the operational features of the controller\\nbuttons. In addition, they will find a brief explanation about the\\neffects of phonological dyslexia as shown in Figure 3a. Second\\nroom is Sam’s room. It is an empty room where players can\\nsee their friend and his current state as shown in Figure 3b.\\nFinally, the third room is the one where the game takes place.\\nIt is a potions laboratory where players can find their teacher\\nand all the materials needed to brew a potion to help Sam. A\\ngeneral view of the room is shown in Figure 3c.\\nC. Main characters and items\\nThroughout the VR experience, participants are required\\nto interact with a diverse range of items and characters that\\nhave been specifically designed to guide and enhance their\\nlevel of immersion within the simulated environment. A list\\nof these characters and objects is exposed below, and their\\nrepresentation in the virtual world are shown in Figure 4.\\n• Teacher: she has the role of potions teacher in the virtual\\nworld. She explains to the players the rules of the game\\nbelonging to the different levels and what they have to do\\nto create the correct potion and win the game. In addition,\\nshe takes on the role of a strict teacher who yell at her\\nstudents when they fail in their tasks. In this way the\\nplayer put himself in the place of dyslexic students who\\nare not valued for their effort by their teachers.\\n• Sam: he represents the player’s friend in the game. The\\nplayer must prepare a potion correctly and within a\\nlimited time frame in order to save him. Player’s results\\nwill be directly reflected in Sam. If the correct potion is\\ncreated Sam will be happy and celebrate, but if it fails\\nduring the different levels players will see him suffer.\\n• Ingredients and shelves: around the potions laboratory\\nthe player finds several shelves with different ingredients\\nto make potions. These ingredients are carefully arranged\\nin a particular order on each shelf, and stored within\\nflasks of varying shapes and colors, each labeled with the\\ncorresponding ingredient’s name. Notably, the labeling is\\ncomposed utilizing the Britton’s font, trying to replicate\\nthe reading difficulties experienced by a dyslexic person.\\n• Table: in the center of the potion laboratory lies an\\nirregularly-shaped table designed to promote different\\nmodes of locomotion in VR (teleportation-based and\\ncontroller-based). Over the table there are different key\\nobjects for the game, including an hourglass, recipe book,\\nand pot, which players may interact with as part of their\\ngameplay experience. Furthermore, the table serves as a\\npractical space for players to place some of the potion\\ningredients.\\n• Hourglass: is the item used to start the game. Next to it\\nthere is a digital clock with the time available to complete\\nthe level. Upon starting the game, the allotted time begins\\nto decrement, with the level concluding once the timer\\nreaches zero.\\n• Pot: container in which the potion is to be brewed.\\nPlayers must pour the ingredients into the pot in the\\ncorrect order to pass the game. The pot will release a heart\\nif the poured ingredient is correct and a purple smoke if\\nit is not, serving as feedback to guide the player towards\\nsuccessful completion of the game.\\n(a) Starting room.\\n(b) Sam’s room.\\n(c) Potions laboratory.\\nFig. 3: Chambers from the virtual castle: initial room (a), Sam’s Room (b) and potions laboratory (c).\\n• Recipe book: it is a big book containing the recipe for\\nthe potion to be brewed by the player. It denotes the\\nspecific type and quantity of each ingredient required,\\nand specifies the correct order in which they must be\\nadded to the container in order to ensure the successful\\ncompletion of the potion. Consistent with the font utilized\\nfor ingredient labeling, the recipe book also employs the\\nBritton’s font.\\n• Beacon: a light beam that guides the player throughout\\nthe virtual environment. It indicates specific points within\\nthe potions laboratory where the player must be situated\\nto initiate teacher conversations or start the next level.\\nIV. PRELIMINARY RESULTS AND FUTURE WORKS\\nThe game was tested on 32 non-dyslexic individuals, who\\nwere not able, in general, to solve the task proposed, reporting\\nfeeling frustrated and anxious due to the difficulties in reading\\nthe instructions to complete the task. This feeling tended to\\ndisappear completely when more time was given and, above\\nall, when compensatory tools were provided. In this way,\\nconsciousness about issues and needs of dyslexic students\\narose. People’s opinions on the experience were collected\\nthrough a survey with questions to be answered on a Likert\\nscale ranging from 1 to 5. Figure 5 shows two of these\\nquestions, the first referring to the difficulty of performing the\\nproposed task and the second to the growth of their empathy\\ntowards people with dyslexia. The VR experience presented\\nin this work can be thus considered a good way to increase\\nempathy towards people with dyslexia.\\nNext step involves collecting data from a larger sample\\nsize to validate the effectiveness of the developed serious\\ngame. Specifically, an appropriate survey will be developed\\nto measure the quality of the VR experience and the increase\\nin empathy towards people with phonological dyslexia after\\nthey complete the game. In addition, collected data will be\\nused to study the different users empathy profiles through the\\napplication of various artificial intelligence techniques.\\nFinally, we are also designing other serious games to show\\nempathy not only for people with phonological dyslexia, but\\nalso for those with other dyslexia types. In this way we have\\nthe objective that anyone can put themselves in the place of\\na dyslexic person, and thus understand the real need for the\\ndifferent methodologies of support required by all dyslexic\\nstudents, including those in higher and university education.\\nV. ACKNOWLEDGEMENTS\\nJos´e Manuel Alcalde Llergo is a PhD student enrolled in the\\nNational PhD in Artificial Intelligence, XXXVIII cycle, course\\non Health and life sciences, organized by Universit`a Campus\\nBio-Medico di Roma. These results are framed in VRAIlexia\\nproject funded by the Erasmus+ Programme2014-2020 – Key\\nAction 2: Strategic Partnership Projects. AGREEMENT n.\\n2020-1-IT02-KA203-080006.\\nREFERENCES\\n[1] A. Kohli, S. Sharma, and S. K. Padhy, “Specific learning disabili-\\nties: Issues that remain unanswered,” Indian Journal of Psychological\\nMedicine, vol. 40, no. 5, pp. 399–405, 2018. PMID: 30275613.\\n[2] A. Costantini, A. Ceschi, and R. Sartori, “Psychosocial interventions for\\nthe enhancement of psychological resources among dyslexic adults: A\\nsystematic review,” Sustainability, 09 2020.\\n[3] J. J. Tree and J. Kay, “Phonological dyslexia and phonological impair-\\nment: An exception to the rule?,” Neuropsychologia, vol. 44, no. 14,\\npp. 2861–2873, 2006.\\n[4] M. Alexander, R. Friedman, F. Loverso, and R. Fischer, “Lesion local-\\nization of phonological agraphia,” Brain and Language, vol. 43, no. 1,\\npp. 83–95, 1992.\\n[5] S. Z. Rapcsak, P. M. Beeson, M. L. Henry, A. Leyden, E. Kim, K. Rising,\\nS. Andersen, and H. Cho, “Phonological dyslexia and dysgraphia: Cog-\\nnitive mechanisms and neural substrates,” CORTEX, vol. 45, pp. 575–\\n591, MAY 2009.\\n[6] E. Sako, “The emotional and social effects of dyslexia,” European\\nJournal of Interdisciplinary Studies, vol. 4, p. 233, 04 2016.\\n[7] A. Zingoni, J. Taborri, V. Panetti, S. Bonechi, P. Aparicio-Mart´ınez,\\nS. Pinzi, and G. Calabr`o, “Investigating issues and needs of dyslexic\\nstudents at university: Proof of concept of an artificial intelligence\\nand virtual reality-based supporting platform and preliminary results,”\\nApplied Sciences, vol. 11, no. 10, 2021.\\n[8] I. Benedetti, B. Marcella, V. Panetti, J. Taborri, T. Urbani, A. Zingoni,\\nand G. Calabr`o, “Clustering analysis of factors affecting academic career\\nof university students with dyslexia in italy,” Scientific Reports, vol. 12,\\np. 9010, 05 2022.\\n(a) Ingredients.\\n(b) Irregular table.\\n(c) Hourglass.\\n(d) Pot.\\n(e) Recipe book.\\n(f) Beacon.\\nFig. 4: Main items.\\nRating\\nPeople\\n0\\n5\\n10\\n15\\n20\\n1\\n2\\n3\\n4\\n5\\nHow difficult did you find the task?\\nRating\\nPeople\\n0\\n5\\n10\\n15\\n20\\n1\\n2\\n3\\n4\\n5\\nHow much did your empathy level increase after the VR \\nexperience?\\nFig. 5: Survey results.\\n[9] V. De Luca, C. Gatto, S. Liaci, L. Corchia, S. Chiarello, F. Faggiano,\\nG. Sumerano, and L. T. De Paolis, “Virtual reality and spatial augmented\\nreality for social inclusion: The includiamoci project,” Information,\\nvol. 14, no. 1, 2023.\\n[10] S. D. Piovesan, R. Wagner, R. D. Medina, and L. M. Passerino,\\n“Virtual reality system to the inclusion of people with disabilities in the\\nwork market,” in 2013 12th International Conference on Information\\nTechnology Based Higher Education and Training (ITHET), pp. 1–7,\\n2013.\\n[11] N. Didehbani, T. Allen, M. Kandalaft, D. Krawczyk, and S. Chapman,\\n“Virtual reality social cognition training for children with high func-\\ntioning autism,” Computers in Human Behavior, vol. 62, pp. 703–711,\\n2016.\\n[12] E. Stilinovi´c, “Facilitating empathy through virtual reality,” Motivation\\nand Emotion, vol. 41, 12 2017.\\n[13] E. Hoter and I. Nagar, “The effects of a wheelchair simulation in a\\nvirtual world,” Virtual Reality, vol. 27, 02 2022.\\n[14] E. Pedroli, P. Padula, A. Guala, M. T. Meardi, G. Riva, and G. Al-\\nbani, “A psychometric tool for a virtual reality rehabilitation approach\\nfor dyslexia,” Computational and mathematical methods in medicine,\\nvol. 2017, 2017.\\n[15] S. Rodr´ıguez Cano, V. Delgado Benito, R. Casado Mu˜noz, E. Cubo Del-\\ngado, V. Aus´ın Villaverde, and G. Santa Olalla Mariscal, “Tecnolog´ıas\\nemergentes en educaci´on inclusiva: realidad virtual y realidad au-\\nmentada: Proyecto europeo fordys-var,” Revista INFAD de Psicolog´ıa.\\nInternational Journal of Developmental and Educational Psychology.,\\nvol. 2, p. 443–450, ago. 2021.\\n[16] S. Rodr´ıguez-Cano, V. Delgado-Benito, V. Aus´ın-Villaverde, and L. M.\\nMart´ın, “Design of a virtual reality software to promote the learning of\\nstudents with dyslexia,” Sustainability, vol. 13, no. 15, p. 8425, 2021.\\n[17] E.\\nYeguas-Bol´ıvar,\\nJ.\\nM.\\nAlcalde-Llergo,\\nP.\\nAparicio-Mart´ınez,\\nJ. Taborri, A. Zingoni, and S. Pinzi, “Determining the difficulties of\\nstudents with dyslexia via virtual reality and artificial intelligence: An\\nexploratory analysis,” in 2022 IEEE International Conference on Metrol-\\nogy for Extended Reality, Artificial Intelligence and Neural Engineering\\n(MetroXRAINE), pp. 585–590, 2022.\\n[18] D. Britton, “Dyslexia.” https://danielbritton.info/dyslexia/.\\n'},\n",
       " {'abstract': \"Pushing updates periodically and returning feedback when needed, adapting the physical decisions to the communication policy, improves the efficiency of systems with sensors and actuators. Choosing between push-based and pull-based communication is crucial. This work proposes an analytical model for optimizing push-based and pull-based strategies in Cyber-Physical Systems (CPSs). Both options have advantages and drawbacks. The push-based system performs better at the optimum, but is PPAD-hard. Numerical results show that adjusting the actuators' decision to the network operation is a better approach.\",\n",
       "  'introduction': \"Recent works have shown that AoI-based optimization may lead to sub-optimal performance, and that adapting the actuators' decisions to the network operations, thus jointly optimizing communication and control, is a better approach. This research investigates the performance of pull-based and push-based communication strategies in CPSs.\",\n",
       "  'literature_review': \"Several recent works on Effective Communication (EC) systems have investigated the push-based configuration. However, the joint optimization of EC and control policies is still relatively unexplored, and, to the authors' knowledge, no studies have directly compared the push- and pull-based communication approaches and analyzed the interdependency between CPS optimization and the Value of Information (VoI).\",\n",
       "  'methodology': 'We consider a simple CPS scenario with a single actuator, without local sensing capabilities, that is connected to a base station (BS) via a constrained communication channel. Using this model, we analyze the advantages and drawbacks of each configuration, proving relevant results and showing that the push-based system, while having better performance at the optimum, is a PPAD-hard problem.',\n",
       "  'results': 'The performance of a push-based system should improve with respect to the pull-based one, as the former can exploit information on the specific realization of the system state trajectory. On the other hand, remote POMDPs are multi-agent problems and may involve critical issues in terms of coordination between the BS and Actuator. Results show that the push-based approach selects the best update interval for every state. The pull-based EC strategy Pareto dominates the AoI-based policy, and the API approach leads to a Nash Equilibrium (NE) policy in the push-based problem. However, the optimal EC solution to the push-based problem does not always Pareto dominate the AoI-based strategy, and the solution obtained by the API strategy does not Pareto dominate the AoI-based strategy.',\n",
       "  'conclusion': 'This work presents an analytical framework to adapt classical optimization tools such as Policy Iteration (PI) to the context of communication and control problems, providing numerical results that show a strong dependency between the VoI and the structure of the underlying Markov Decision Process (MDP). The analysis revealed that the common push-based view of communication and control problems may not always be optimal, as the game theoretical properties of multiagent scenarios do not guarantee that a solution may be better even than a simple AoI-based optimization.',\n",
       "  'title': 'Push- and Pull-based Effective Communication in Cyber-Physical Systems',\n",
       "  'author': 'Pietro Talli, Federico Mason, Federico Chiariotti, Andrea Zanella',\n",
       "  'textdata': 'This paper has been submitted to IEEE International Conference on Computer Communications. Copyright may change without notice.\\nPush- and Pull-based Effective Communication\\nin Cyber-Physical Systems\\nPietro Talli, Federico Mason, Federico Chiariotti, Andrea Zanella\\nDepartment of Information Engineering, University of Padova, Via G. Gradenigo 6/B, 35131, Padua, Italy\\nEmails: pietro.talli@phd.unipd.it, {federico.mason, federico.chiariotti, andrea.zanella}@unipd.it\\nAbstract—In Cyber-Physical Systems (CPSs), two groups of\\nactors interact toward the maximization of system performance:\\nthe sensors, observing and disseminating the system state, and the\\nactuators, performing physical decisions based on the received\\ninformation. While it is generally assumed that sensors periodi-\\ncally transmit updates, returning the feedback signal only when\\nnecessary, and consequently adapting the physical decisions to the\\ncommunication policy, can significantly improve the efficiency\\nof the system. In particular, the choice between push-based\\ncommunication, in which updates are initiated autonomously by\\nthe sensors, and pull-based communication, in which they are\\nrequested by the actuators, is a key design step. In this work, we\\npropose an analytical model for optimizing push- and pull-based\\ncommunication in CPSs, observing that the policy optimality\\ncoincides with Value of Information (VoI) maximization. Our\\nresults also highlight that, despite providing a better optimal\\nsolution, implementable push-based communication strategies\\nmay underperform even in relatively simple scenarios.\\nIndex Terms—Effective communication, Value of Information,\\nPull-based communication, Cyber-Physical Systems\\nI. INTRODUCTION\\nThe Industry 4.0 revolution has made Cyber-Physical Sys-\\ntems (CPSs) a fundamental pillar for multiple applications,\\nincluding area surveillance, data muling, factory automation,\\nteleoperation, and autonomous mobility [1]. A CPS is com-\\nposed of sensors, which collect information about the system\\nstate, and actuators, that can physically modify it and control\\nit. In standard scenarios, it is assumed that sensor transmis-\\nsions take place periodically, with the goal of minimizing\\nthe Age of Information (AoI) at the receiver side. Such an\\napproach ensures that, on average, the actuators have the most\\nrecent information about the system conditions.\\nRecent works have shown that AoI-based optimization may\\nlead to sub-optimal performance, either because updates are\\nunnecessary and may waste communication resources when\\nthe environment remains stable, or because an abrupt change\\nin the system state may go unreported for a relatively long\\ntime, harming the control performance. In more advanced\\nsystems, the communication policy is tailored to the Value\\nof Information (VoI) of the system, which implies that new\\nThis project was funded under the National Recovery and Resilience Plan\\n(NRRP), Mission 4 Component 2 Investment 1.3 - Call for tender No.\\n341 (15 March 2022) of the Italian Ministry of University and Research,\\nfunded by the European Union NextGenerationEU Project. MUR grant\\nnumber: PE 0000001, Concession Decree 1549 (11/10/2022) adopted by the\\nItalian Ministry of University and Research, CUP C93C22005250001, project\\ntitle: RESearch and innovation on future Telecommunications systems and\\nnetworks, to make Italy more smART (RESTART). F. Chiariotti’s activities\\nare funded by NRRP “Young Researchers” grant REDIAL (SoE0000009).\\ntransmissions are started whenever the environment evolution\\ngets unpredictable at the receiver side. A further improvement\\nin overall performance is obtained by adapting the actuators’\\ndecisions to the network operations, thus jointly optimizing\\ncommunication and control [3]. In the pursuit of this goal,\\nit is possible to adopt the multi-agent reinforcement learning\\n(MARL) paradigm [4], an extension of reinforcement learning\\n(RL) to handle multi-agent scenarios.\\nIn the case of MARL optimization, each actor (i.e., sensor\\nor actuator) is controlled via a distinct RL agent, which\\nadapts its local actions towards the maximization of the\\noverall performance. The fact that multiple agents interact\\nwith and simultaneously adapt to the same environment leads\\nto non-stationary training conditions, where estimating the\\noptimal policy is not always guaranteed [5]. Better solutions\\nmay be obtained by centralizing the control process in a\\nsingle unit, which ensures perfect agent coordination at the\\ncost of higher learning complexity and resource consumption.\\nHowever, centralized solutions require reliable, high-frequency\\nupdates, which may be not always feasible, especially in the\\ncase Internet of Things (IoT) scenarios, where energy and\\ncommunication constraints make it challenging to achieve a\\nfull environment perception in real-time [?].\\nIn the case of a distributed optimization of CPSs, the place-\\nment of the computational intelligence controlling the com-\\nmunication systems is a critical design choice. If RL agents\\nare installed at the sensors, it is possible to observe the current\\nenvironment (or part of it) and update the actuators’ knowledge\\nas needed, in a push-based fashion. However, this solution may\\nnot be feasible if the sensors have limited computational and\\ncommunication capabilities, as in the case of IoT networks.\\nThe other option is to install the computational intelligence at\\nthe actuators, each of which is associated with RL agent that\\nhas to both control the physical actions of the actuator itself\\nand request new updates from the sensors. In such a case,\\ncommunication takes place in a pull-based fashion [?], and\\ndecisions on when and whether to request an update are made\\nwithout any information about the environment aside from the\\nlast state update and the elapsed time.\\nThe pull-based problem was first presented in [6] and\\nrecently addressed in [7], where the authors consider two\\npossible approaches for approximating the optimal policy:\\nthe first explicitly estimates the transition probability of the\\nMarkov Decision Process (MDP) describing the system evo-\\nlution, while the second jointly learns communication and\\ncontrol policies as part of a single model. The push-based\\narXiv:2401.10921v1  [eess.SY]  15 Jan 2024\\nconfiguration has been investigated in several recent works\\non Effective Communication (EC) systems [?], in which\\nthe actuators’ policies must be adapted to the frequency of\\ncommunication updates. However, the joint optimization of\\nEC and control policies is still relatively unexplored, and, to\\nthe authors’ knowledge, no studies have directly compared the\\npush- and pull-based communication approaches and analyzed\\nthe interdependency between CPS optimization and the VoI.\\nTo overcome these limitations, this work presents an ana-\\nlytical model for optimizing and evaluating push- and pull-\\nbased strategies in CPSs. We consider a simple CPS scenario\\nwith a single actuator, without local sensing capabilities, that\\nis connected to a base station (BS) via a constrained com-\\nmunication channel. Using the channel involves a significant\\ncost but represents the only way for the actuator to observe\\nthe system state. This design involves a trade-off between the\\nminimization of the communication cost and the quality of\\ncontrol, which becomes less accurate as the transmission rate\\nis reduced, in both the push- and pull-based versions. Using\\nthis model, we analyze the advantages and drawbacks of each\\nconfiguration, proving relevant results and showing that the\\npush-based system, while having better performance at the\\noptimum, is a PPAD-hard problem [15].\\nII. SYSTEM MODEL\\nWe consider a system model with a single Actuator that\\ncan perceive the environment only through the information\\nprovided by a BS. This latter has full access to the system\\nstate and, therefore, embodies the sensor network described\\nin the introduction. The goal of the model is to design and\\nevaluate EC strategies between the Actuator and the BS. In\\nthe rest of this section, we will first formalize the environment\\nas an MDP, and then characterize the possible solutions in the\\npull- and push-based communication scenarios.\\nA. Markov Decision Processes\\nWe consider an MDP defined by the tuple ⟨S, A, P, r, γ⟩,\\nwhere S is the set of states, A is the set of actions, P is the\\nfull transition matrix, r is the reward function and γ ∈ [0, 1) is\\nthe discount factor. Symbol Pa denotes the transition matrix\\nassociated with action a ∈ A. Each row of Pa ∈ R|S|×|S|\\ndefines the distribution of the next state over S. For it to be a\\nvalid distribution, we impose P a\\nss′ ≥ 0 ∀a ∈ A, (s, s′) ∈ S2\\nand P\\ns′ P a\\nss′ = 1. The immediate reward for taking action a\\nin state s and transitioning to state s′ is denoted by ra\\ns,s′.\\nIn our problem, the Actuator cannot directly observe the\\nstate but incurs a cost ct to obtain it from the BS, which\\nobserves it at every step with no associated cost. Hence,\\nwe can consider two possible scenarios: in the pull-based\\ncommunication scenario, the Actuator itself decides whether to\\nrequest a state update from the BS, which constitutes a passive\\nactor in the system; in the push-based communication scenario,\\nthe BS itself can decide whether to transmit the system state\\nto the Actuator. Notably, in the first case, we have a single\\nagent installed at the Actuator, while, in the second case, we\\nhave two distinct agents that need to cooperate.\\nIn the simplest case, every sensing action has the same cost,\\nand the communication action is ct ∈ {0, 1}. When the state st\\nis not available to the Actuator, its a priori belief distribution\\ncan be computed using the Markovian property of the system\\nstate evolution. Knowing the last observed state st−k and the\\nvector a = {at−k, at−k+1, . . . , at−1} containing the k actions\\nundertaken since then, as well as the transition matrix P, we\\ncan determine the belief distribution θa\\nst−k,k:\\nθa\\nst−k,k =\\n \\ntY\\nℓ=t−k\\nPaℓ\\n!\\n1st−k,\\n(1)\\nwhere 1s is a one-hot column vector whose elements are all\\n0, except for the one corresponding to s, which is equal to 1.\\nTo jointly optimize the control and communication policies,\\nwe need to solve the following problem:\\nmaximize E\\n\" ∞\\nX\\nt=0\\nγtrt\\n#\\nsuch that E\\n\" ∞\\nX\\nt=0\\nγtct\\n#\\n≤ C,\\n(2)\\nwhere C is the cumulative sampling cost that the system can\\ntolerate and rt is the instantaneous reward at time t. This\\nproblem can be seen as a constrained Partially Observable\\nMarkov Decision Process (POMDP) where each action, or\\ncombination of actions by the BS and Actuator, corresponds\\nto the pair (at, ct). By defining a fixed communication cost\\nβ ∈ R+ and using it as a dual parameter controlling the trade-\\noff between the communication cost and the reward, we can\\nreformulate (2) as an unconstrained POMDP:\\nmaximize E\\n\" ∞\\nX\\nt=0\\nγt(rt − βct)\\n#\\n.\\n(3)\\nIn the following, we will denote values and functions related\\nto the Actuator with the subscript A, while the BS will be\\nassociated with the subscript B.\\nThe pull-based communication scenario can then be mod-\\neled as a problem known as an Action-Contingent Noiselessly\\nObservable MDP (ACNO-MDP) [7], in which the state st\\nis only available to the Actuator by means of a (costly)\\nsensing action. Specifically, the sensing action corresponds\\nto a request to the BS, which has perfect state information\\nand can transmit it on demand. If we fix the polling strategy,\\nthe resulting system can be treated as a POMDP that can be\\nsolved optimally with point-based value iteration [10] or other\\nmethods that consider the α-vectors [11].\\nThe push-based problem is more common in the EC lit-\\nerature and is often modeled as a Remote POMDP [12].\\nIn this case, communication is initiated by the BS, which\\nknows both the last transmission to the receiver st−k and the\\ncurrent state st, along with the time since the last update k.\\nIntuitively, the performance of a push-based system should\\nimprove with respect to the pull-based one, as the former can\\nexploit information on the specific realization of the system\\nstate trajectory, rather than just on its statistics. On the other\\nhand, remote POMDPs are multi-agent problems and may\\ninvolve critical issues in terms of coordination between the\\nBS and Actuator, which will be explored in the following.\\nAlgorithm 1 Pull-Based Modified Policy Iteration\\nRequire: P, r, β\\n1: Initialize VA(s, k) ← 0, randomize πA(s, k), ∆\\n2: while true do\\n3:\\nfor (s, k) ∈ S × {0, ..., Tmax} do\\n4:\\nV ′\\nA(s, k) ←Update using (4)\\n5:\\nVA ← V′\\nA\\n▷ Value update step\\n6:\\nfor (s, k) ∈ S × {0, ..., Tmax} do\\n7:\\nπ′\\nA(s, k) ←Update using (6)\\n8:\\n∆′(s) ←Update using (7)\\n9:\\nif π′\\nA = πA ∧ ∆′ = ∆ then\\n10:\\nreturn πA, ∆\\n▷ Convergence\\n11:\\nelse\\n12:\\nπA, ∆ ← π′\\nA, ∆′\\n▷ Policy improvement step\\nIII. ANALYTICAL SOLUTION\\nTo solve the MDP, we consider a model-based approach:\\nany RL agent interacting with the environment can compute\\noptimal control and communication policies with complete\\nknowledge of the reward function and the state transition\\nmodel. This can also be accomplished with statistical learn-\\ning [7], i.e., by learning the transition probabilities of the un-\\nderlying MDP before solving the communication-constrained\\nproblem. This choice allows for an easy comparison of the\\npolicies, without any training issues, and the results can be\\napplied directly to any properly trained agent.\\nA. Pull-based Communication\\nTo solve the pull-based communication problem optimally,\\nwe propose a modified Policy Iteration (PI) algorithm to learn\\nthe communication control policies jointly. We recall that\\nPI always converges to the optimal solution in single-agent\\nMDPs, as the one considered in pull-based scenarios.\\nWe formulate a new MDP in which each time step is divided\\ninto two sub-steps: one for the control action and one for\\nthe communication action. The state of this new MDP is also\\nexpanded to the tuple (st−k, k), which is available to the agent.\\nThe action space is then A in control sub-steps, and {0, 1}\\nin communication sub-steps. The agent then splits the policy\\ninto two parts:\\n• πA(st−k, k), the control policy, which maps each state\\n(st−k, k) ∈ S × N to an action a ∈ A;\\n• ∆(st−k), the communication policy, which maps each\\nstate st−k ∈ S to the time until the next update, c ∈ N+.\\nSince the policy πA depends only on the last observed\\nstate, we can determine the vector aπA\\ns\\ncontaining the k\\nactions chosen by the Actuator after observing s. Following\\nthe bootstrap principle, the modified PI algorithm then updates\\nthe value of a control sub-state as follows:\\nV ′\\nA(s, k) =\\nX\\n(s′,s′′)∈S2\\nθa\\nπA\\ns\\ns,k (s′)P πA(s,k)\\ns′,s′′\\n\\x14\\nrπA(s,k)\\ns′,s′′\\n+γ (δs,k(VA(s′′, 0) − β) + (1 − δs,k) VA(s, k + 1))\\n\\x15\\n,\\n(4)\\nwhere δs,k is a utility function equal to 1 if ∆(s) = k+1, i.e.,\\nif an update is requested after k + 1 steps, and 0 otherwise.\\nWe observe that the division into sub-steps does not affect\\nAlgorithm 2 Push-Based Alternate Policy Iteration\\nRequire: P, r, β, π(0)\\nB\\n1: Initialize πB ← π(0)\\nB , randomize πA\\n2: while true do\\n3:\\nπ′\\nA ←CONTROLPOLICYITERATION(P, r, β, πB)\\n4:\\nπ′\\nB ←COMMUNICATIONPOLICYITERATION(P, r, β, π′\\nA)\\n5:\\nif π′\\nA = πA ∧ π′\\nB = πB then\\n6:\\nreturn πA, πB\\n▷ Convergence\\n7:\\nelse\\n8:\\nπA, πB ← π′\\nA, π′\\nB\\nthe reward and the solution to the modified problem is also\\noptimal for the original MDP. Therefore, we do not then need\\nto compute the control sub-state values explicitly.\\nTo perform the policy improvement step, we modified the\\nstandard update rule by using a reward that considers the entire\\nevolution of the Markov chain until the state is sampled again.\\nIf we consider a state sequence s′, which begins k steps after\\nstate s is observed, the belief ΘπA\\ns,k(s′) follows from (1):\\nΘπA\\ns,k(s′) = θa\\nπA\\ns\\ns,k (s′(1))\\nL(s′)−1\\nY\\nℓ=1\\nP πA(s,ℓ+k−1)\\ns′(ℓ),s′(ℓ+1) ,\\n(5)\\nwhere L(·) is a function whose outcome is the dimensionality\\nof the input. Hence, the control policy is improved as follows:\\nπ′\\nA(s, k) = arg max\\na∈A\\nX\\ns′∈S∆(s)−k+1\\nΘπA\\ns,k(s′)\\n\" ∆(s)−k\\nX\\nℓ=1\\nγℓ−1ra\\nπA\\ns\\n(ℓ+k−1)\\ns′(ℓ),s′(ℓ+1)\\n+ γ∆(s)−k (VA(s′(∆(s) − k + 1), 0) − β)\\n#\\n.\\n(6)\\nInstead, the communication policy is updated as:\\n∆′(s) = arg max\\nn∈N+\\nX\\ns′∈Sn\\nΘπA\\ns,1 (s′)\\n\" n−1\\nX\\nℓ=1\\nγℓrπA(s,ℓ)\\ns′(ℓ),s′(ℓ+1)\\n+ γn(VA(s′(n), 0) − β) + rπA(s,0)\\ns,s′(1)\\n#\\n.\\n(7)\\nUsing the whole sequence of intermediate steps in the update\\nequations is essential to take into account that the states\\n(s, k) ∀ k > 0 are non-Markovian, as the belief distribution\\ndepends on the action sequence taken since the last commu-\\nnication. The pseudocode for the PI scheme for pull-based\\ncommunication is given in Alg. 1.\\nB. Push-based Communication\\nIn the case of push-based communication, we propose a\\nsimple strategy which we call Alternate Policy Iteration (API):\\nstarting from a default policy for the BS, we first optimize the\\nActuator’s policy and then optimize the BS, considering the\\nActuator’s actions as fixed. Since one agent’s policy is fixed,\\nthe other agent sees a Markovian environment, and standard\\nPI can be applied in each step. The procedure is then repeated\\nuntil both the policies converge, i.e., each agent follows the\\noptimal policy with respect to the other. As we will prove in\\nSec. V, this strategy always converges in a finite number of\\nsteps. The pseudocode for API is given in Alg. 2.\\n0\\n0.4 0.8 1.2 1.6\\n2\\n0.9\\n0.7\\n0.5\\n0.3\\n0.1\\nβ\\nd\\n0\\n0.5\\n1\\n1.5\\n2\\n(a) Average reward (pull-based, fo-\\ncused).\\n0\\n0.4 0.8 1.2 1.6\\n2\\n0.9\\n0.7\\n0.5\\n0.3\\n0.1\\nβ\\nd\\n0\\n0.2 0.4 0.6 0.8\\n1\\n(b) Update frequency (pull-based,\\nfocused).\\n0\\n0.4 0.8 1.2 1.6\\n2\\n0.9\\n0.7\\n0.5\\n0.3\\n0.1\\nβ\\nd\\n0\\n2\\n4\\n6\\n8\\n(c)\\nAverage\\nreward\\n(pull-based,\\nspread).\\n0\\n0.4 0.8 1.2 1.6\\n2\\n0.9\\n0.7\\n0.5\\n0.3\\n0.1\\nβ\\nd\\n0\\n0.2 0.4 0.6 0.8\\n1\\n(d) Update frequency (pull-based,\\nspread).\\n0\\n0.4 0.8 1.2 1.6\\n2\\n0.9\\n0.7\\n0.5\\n0.3\\n0.1\\nβ\\nd\\n0\\n0.5\\n1\\n1.5\\n2\\n(e) Average reward (push-based, fo-\\ncused).\\n0\\n0.4 0.8 1.2 1.6\\n2\\n0.9\\n0.7\\n0.5\\n0.3\\n0.1\\nβ\\nd\\n0\\n0.2 0.4 0.6 0.8\\n1\\n(f) Update frequency (push-based,\\nfocused).\\n0\\n0.4 0.8 1.2 1.6\\n2\\n0.9\\n0.7\\n0.5\\n0.3\\n0.1\\nβ\\nd\\n0\\n2\\n4\\n6\\n8\\n(g)\\nAverage\\nreward\\n(push-based,\\nspread).\\n0\\n0.4 0.8 1.2 1.6\\n2\\n0.9\\n0.7\\n0.5\\n0.3\\n0.1\\nβ\\nd\\n0\\n0.2 0.4 0.6 0.8\\n1\\n(h) Update frequency (push-based,\\nspread).\\nFig. 1: Figure of the Reward and Communication Cost for different densities and values of β.\\nIV. NUMERICAL EVALUATION\\nIn this section, we evaluate the two methods proposed on\\nrandomly generated MDPs that follow a common structure\\nfor a fair and easy comparison. We consider a system with\\n|S| = 30 states and |A| = 4 actions. The reward depends\\nonly on the reached state and is defined as\\nra\\ns,s′ = e−α|s′−s0|, ∀s ∈ S, a ∈ A,\\n(8)\\nwhere s0 ∈ S is a target state, and α is a decay parameter\\ndetermining how much the reward is concentrated around s0.\\nWe consider two possible reward configurations:\\n• Focused reward: α = 10, the only state that gives a\\nsignificant reward is s0.\\n• Spread reward: α = 0.01, the reward is spread among\\nthe states near s0.\\nIn both configurations, the MDP transition matrix is gener-\\nated as follows: we start from a deterministic transition matrix\\nfor every action and then progressively increase the number\\nof non-zero elements by enabling transitions to neighboring\\nstates with a certain probability. As more transitions are added,\\nthe transition matrix becomes denser, i.e., the number d of\\nnon-zero elements divided by the total number of entries of\\nthe matrix increases. The density of the deterministic matrix\\nis simply |S|−1. If the initial deterministic matrix Pa has\\na transition from s to s′, i.e., P a\\ns,s′ = 1, the distribution at\\ndensity d is:\\nP a\\ns,s′′ = 4(d|S| − 2|s′ − s′′|)\\n(d|S| + 1)2\\n, ∀s′′ s.t. |s′ −s′′| < d|S|\\n2 . (9)\\nAccording to this approach, the probability of transitioning\\nto neighbor states decreases linearly with the distance from\\nthe original transition state (the distribution has a triangular\\nshape). Higher-density MDPs are then simply more unpre-\\ndictable versions of the same initial model.\\nTo investigate our model in different scenarios, we repeat-\\nedly increase the number of transitions for each state by adding\\ntwo connections at a time, obtaining a total of 15 MDPs with\\nincreasing densities. We also consider different communication\\ncosts β ∈ {0, 0.1, . . . , 2}. We test each combination of d and β\\nwith both pull-based and push-based policies, using the same\\nmatrices for the push- and pull-based systems. The code for\\nthe numerical evaluations is available to replicate the results1.\\nFig. 1 shows the average obtained reward of the physical\\nprocess and the frequency of the updates between the BS and\\nthe Actuator. First, we can note that the push-based system\\nalways obtains a higher overall reward than the pull-based one,\\nas would be expected: the BS, acting with full knowledge of\\nthe state, can determine whether an update has a high VoI,\\ni.e., whether it meaningfully affects the Actuator.\\nFig. 1b and Fig. 1d also show that the pull-based system\\nonly has 2 working points for higher values of d (i.e., more\\nunpredictable transitions): either the agent requests an update\\nat each step, or it never does. This is because the high\\nuncertainty in the belief distribution makes it hard to use it to\\npredict the optimal action even after a single step. This leads\\nthe Actuator to always request the new state information if β\\nis low, i.e., if the relative VoI of the update increases, or never\\nto request updates if β is high, moving blindly.\\n1https://www.github.com/pietro-talli/Age-Value-CC\\n0\\n2\\n4\\n6\\n8\\n0\\n0.2\\n0.4\\n0.6\\nPeak AoI\\nPMF\\nPull\\nPush\\n(a) AoI distribution.\\n1\\n10\\n20\\n30\\n2\\n4\\n6\\n8\\nState\\nPeak AoI\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n(b) Pull-based AoI distribution by state.\\n1\\n10\\n20\\n30\\n2\\n4\\n6\\n8\\nState\\nPeak AoI\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n(c) Push-based AoI distribution by state.\\nFig. 2: Peak AoI distribution for d = 0.1, β = 1 and sparse reward.\\nAs Fig. 1f and Fig. 1h clearly show, the push-based strategy\\ncan reduce the update frequency. In this case, the BS can\\ncheck if the state of the system evolves in the predicted\\nway, transmitting an update to the Actuator only when a\\nlow-probability transition occurs and its knowledge becomes\\nobsolete. This is particularly beneficial if the transition model\\nis more predictable, i.e., for lower values of d, and leads\\nto 3 working regimes: firstly, if d is very high, the BS\\ncommunicates only if the reward in the next step changes\\nsignificantly, letting the Actuator move blindly in all other\\ncases. This behavior is particularly noticeable in the focused\\nreward case when the BS only communicates if the state can\\nlead directly to the target.\\nOn the other hand, the BS will also communicate sporadi-\\ncally if the system is very predictable, i.e., for very low values\\nof d, for the reasons we outlined above. In between these two\\nextremes, the BS will communicate more often, as the system\\nis not so random that the belief distribution of the Actuator\\nquickly becomes useless, but also not too predictable. As this\\ntype of behavior requires the full knowledge of the state, it\\ncannot be obtained in a pull-based scenario.\\nTo better highlight the different behaviors between pull- and\\npush-based communication, we look at Fig. 2, which analyzes\\nthe Peak AoI for a highly predictable MDP with a sparse\\nreward, with d = 0.1 and β = 1. In particular, Fig. 2a shows\\nthe Probability Mass Function (PMF) of the Peak AoI for\\nthe two approaches: while the average update frequency is\\nsimilar, the push-based system has a much higher variance\\nthan the pull-based one. This is because the first approach\\ntriggers transmissions when needed, sometimes after a single\\nstep, sometimes after many, depending on the environment’s\\nevolution; instead, the pull-based approach only relies on the\\nlast observed state, and its decisions are always deterministic.\\nThis is confirmed by Fig. 2b, which shows how, in the pull-\\nbased scenarios, the Peak AoI distribution for each state is\\nconcentrated in a single point. On the other hand, the push-\\nbased system may transmit after a long time even when\\nstarting from states that have a high expected VoI after a single\\nstep, as Fig. 2c shows: while the pull-based system transmits\\nafter 1 step when starting from state 2, the push-based system\\nhas a non-zero probability of waiting 5 or more steps before\\ntransmitting again.\\nV. AGE AND VALUE OF INFORMATION IN EFFECTIVE\\nCOMMUNICATION\\nConcepts like AoI and VoI are crucial to analyze the\\ntimeliness and relevance of system status updates. In our\\nscenario, we might consider an update strategy that balances\\nthe frequency of updates (which determines the average com-\\nmunication cost) with the reward obtained by the Actuator.\\nLet J(s) = E [P∞\\nt=0 γtrt | s0 = s] be the long-term reward\\nof the Actuator starting from state s and β be the commu-\\nnication cost. We can then give a more general definition of\\nperformance in pull-based or scheduled systems as a function\\nof the selected update periods ∆ ∈ (N+)|S|:\\nRβ(∆) =\\nX\\ns∈S\\nϕ(s)\\n \\nJ(s|∆)−E\\n\" ∞\\nX\\nn=0\\nβγn∆(s′)P(s′|s, ∆)\\n#!\\n,\\n(10)\\nwhere ϕ(s) is the stationary state distribution induced by the\\ncontrol policy and the update periods ∆.\\nWe introduce the concept of Pareto dominance [8] to\\ncompare the performance of complex schemes with multiple\\nobjectives.\\nDefinition 1. an n-dimensional tuple η = (η1, . . . , ηn) Pareto\\ndominates η′ (which we denote as η ⪰ η′) if and only if each\\nelement of η is equal to or better than the corresponding\\nelement of η′, i.e., η ⪰ η′\\n⇐⇒\\nηj ≥ η′\\nj ∀j. Strict\\nPareto dominance makes the inequality strict for at least one\\nparameter:\\nη ≻ η′ ⇐⇒ η ⪰ η′ ∧ ∃i : ηi > η′\\ni.\\n(11)\\nThis concept can then be extended to multi-objective opti-\\nmization, i.e., to schemes that may have different parameters\\nand multiple performance metrics.\\nDefinition 2. Let us consider two schemes x and y, parame-\\nterized by a vector θx and θy. Scheme x Pareto dominates y\\nif and only if:\\nx ⪰ y ⇐⇒ ∀θy∃θx : η(θx) ⪰ η(θy),\\n(12)\\nwhere η(θz) is the n-dimensional performance vector associ-\\nated with scheme z and parameters θz. The definition of strict\\ndominance between schemes is analogous.\\nIf we only consider the AoI, without including state infor-\\nmation in the optimization, we can envision a periodic policy\\nin which the BS sens a new update every ∆ ∈ N+ steps. In\\nthis case, the optimal update interval ∆∗\\nAoI is then:\\n∆∗\\nAoI(β) = arg max\\n∆∈N+\\nX\\ns∈S\\nϕ(s)\\n \\nJ(s | ∆) −\\n∞\\nX\\nn=0\\nβγn∆\\n!\\n.\\n(13)\\nIn particular, we have that ∆(s) = ∆∗\\nAoI ∀s ∈ S, i.e., the\\noptimal AoI ∆∗\\nAoI is the same for all states.\\nAn adaptive pull-based strategy selecting the best ∆pull(s)\\nbased on the state s ∈ S received in the last update can\\nimprove the reward function in (10) will respect the AoI-based\\napproach. In this case, the optimization problem then becomes:\\n∆∗\\npull(β) = arg max\\n∆∈(N+)|S| Rβ(∆).\\n(14)\\nTheorem 1. The pull-based EC strategy Pareto dominates the\\nAoI-based policy, i.e., ∆∗\\npull(β) ⪰ ∆∗\\nAoI(β) ∀⟨S, A, P, r, γ, β⟩.\\nProof. First, we can trivially show that a scheme working\\nbetter than another for every value of β is a stronger condition\\nthan Pareto dominance. We can then prove that this condition\\nholds by reductio ad absurdum: we consider a hypothetical\\noptimal interval ∆∗\\nAoI which performs better than pull-based\\nEC for a given value of β. In this case, ∆∗\\npull cannot be optimal,\\nas the vector in which all elements are equal to ∆∗\\nAoI is one\\nof the possible choices for pull-based EC.\\nWe observe that the pull-based strategy can always fall\\nback to the same point as the AoI-based one, simply by\\nconsidering the same interval for every state. In this case,\\nthe VoI is implicitly determined by the strategy: the Actuator\\nonly requests an update if the expected increase in the long-\\nterm reward is larger than the communication cost β, and that\\nincrease is precisely the value of the update.\\nWe can further improve performance by adopting a push-\\nbased approach, allowing the BS may be able to independently\\nobserve the system state st and consequently decide whether\\nto update the Actuator’s knowledge. This prevents us from\\ndefining a fixed state-dependent update period, as the policy\\nπB(s, k, st) depends on the current state st as well as on the\\nlast state update st−k and the elapsed time k. Particularly, the\\nvalue function for the BS is:\\nVB(s, k, s′) = πB(s, k, s′)(VB(s′, 0, s′)−β) + (1−πB(s, k, s′))\\n×\\nX\\ns′′∈S\\nP πA(s,k)\\ns′s′′\\n\\x10\\nrπA(s,k)\\ns′,s′′\\n+ γVB(s, k + 1, s′′)\\n\\x11\\n,\\n(15)\\nand the optimal policy is the one maximizing the communi-\\ncation value.\\nThe discovery of the optimal policies through API can be\\nseen as the solution to a Markov game [13]: the two agents\\nact as players in a game where the moves are the possible\\npolicies and the payoff for each player is the expected reward\\nin the initial state.\\n0\\n4\\n1\\n2\\n3\\na2, p = 0.5\\na2, p = 0.5\\na1\\na1\\na1\\na1\\na1\\na2\\na2\\na2\\na2\\nFig. 3: Example of a Markov model with 5 states and 2 actions.\\nTheorem 2. The API approach leads to a Nash Equilibrium\\n(NE) policy in the push-based problem.\\nProof. Firstly, we can trivially prove that the considered\\nMarkov game is an exact potential game [14]: as the reward\\nfor the two agents is the same, the expected long-term reward\\nis a potential function for the game. We then consider the\\nAPI strategy: each round of the iterated algorithm leads to the\\noptimal policy when the strategy of the other agent is given,\\ndue to the optimality of standard PI. The API algorithm is\\nthen an Iterated Best Response (IBR) scheme for the game,\\nwhich leads to a NE in a finite number of steps in all finite\\npotential games [14].\\nHowever, reaching an NE is not a guarantee of Pareto\\noptimality: games may have multiple NEs, and finding the\\noptimal one is PPAD-hard [15]. The push-based approach may\\nbe actively harmful, even with respect to an AoI policy.\\nTheorem 3. The optimal EC solution to the push-based prob-\\nlem, π∗\\nB,push, Pareto dominates the pull-based EC strategy:\\nπ∗\\nB,push(β) ⪰ ∆∗\\npull(β) ⪰ ∆∗\\nAoI(β) ∀⟨S, A, P, r, γ, β⟩. (16)\\nHowever, the solution obtained by the API strategy, πAPI\\nB,push,\\ndoes not Pareto dominate the AoI-based strategy:\\n∃⟨S, A, P, r, γ, β⟩ : πAPI\\nB,push(β) ⪰̸ ∆∗\\nAoI(β).\\n(17)\\nProof. The proof of the first statement is simple, and follows\\nthe proof of Theorem 1: as the BS can act with full knowledge\\nof the state, the pull-based solution is a possible solution to the\\npush-based problem, and in some cases, better solutions exist.\\nThe knowledge of the realization of the state can improve\\nthe reward, either by cutting unnecessary transmissions or by\\nimproving the actuator’s performance.\\nAs finding the optimal solution to a Markov game is PPAD-\\nhard [15], no polynomial-time algorithm can reliably find\\nπ∗\\nB,push. We can then give a counterexample to prove the\\nsecond part of the theorem: we consider a simple MDP with\\n5 states and 2 actions, whose evolution is depicted in Fig. 3.\\nThe two transition matrices corresponding to a1 and a2 are:\\nPa1 =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb, Pa2 =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n0\\n0.5\\n0\\n0\\n0.5\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fb.\\n(18)\\nThe reward is then always 0, except for when the environment\\ntransits to state 0, i.e., we have ra\\ns,0 = 1 and ra\\ns,s′ = 0 ∀s′ ̸= 0.\\nWe can easily see that taking action a1 in state 0 leads to\\na loop with 4 states, while taking action a2 may lead to a\\nshorter path back to the reward-giving state. We consider a\\npolicy ∆AoI that transmits in each odd step, ensuring that the\\nactuator always knows if it lands in state 1 or state 4 after\\ntaking action a2. Its expected long-term reward is:\\nR(∆AoI) =1 − γβ + γ2R(∆AoI)\\n2\\n− γ3β\\n2\\n+ γ4R(∆AoI)\\n2\\n=2 − (2γ + γ3)β\\n2(1 − γ2 − γ4) .\\n(19)\\nConsidering a push-based approach and applying the API\\nstrategy, the final results depend on the initial policy of the BS.\\nIf the process starts from a policy that communicates often,\\ne.g., one that always communicates the state, the algorithm\\nwill converge to the optimum joint policy, which only com-\\nmunicates if it deviates from the short cycle (i.e., if the system\\nends up in state 1). In this case, the expected reward is\\nR(π∗\\nB,push) =1 − γβ +\\nγ2R(π∗\\nB,push)\\n2\\n+\\nγ4R(π∗\\nB,push)\\n2\\n=\\n2 − γβ\\n2(1 − γ2 − γ4),\\n(20)\\nwhich is better than the AoI policy for any value of β ∈ R+\\nand γ ∈ (0, 1).\\nInstead, if the API strategy starts from a policy that never\\ncommunicates, the actuator will take the conservative choice,\\nand always take action a1. This is another NE of the system,\\nas the BS should never communicate if the actuator’s policy\\nis independent of the state. The reward for this solution is:\\nR(πAPI\\nB,push) =1 + γ4R(πAPI\\nB,push) = (1 − γ4)−1.\\n(21)\\nIn this case, the API solution is not Pareto dominant, as it\\nperforms worse than a simple AoI-based strategy under the\\nfollowing conditions:\\nβ <\\n2\\n(2 + γ2)(1 − γ4).\\n(22)\\nThe optimal policy might not be easy to obtain in more com-\\nplex systems, and if we consider previously unknown cases in\\nwhich PI must be replaced by RL, running multiple training\\nprocedures to achieve the optimal NE might be expensive or\\nimpossible. Despite this evidence, most of the literature on\\nVoI has so far focused on push-based solutions [16], which\\nfall prey to this coordination problem.\\nVI. CONCLUSION\\nThis work analyzes EC strategies in CPSs, considering the\\nperformance of the control system as the VoI and analyzing the\\ninteractions between the communication and control policies.\\nWe propose an analytical framework to adapt classical opti-\\nmization tools such as PI to this context, providing numerical\\nresults that show a strong dependency between the VoI and the\\nstructure of the underlying MDP. Less predictable MDPs and\\nsparser rewards tend to lead to a higher value of updates, al-\\nthough there are interesting patterns in the interaction between\\ncommunication and control.\\nFinally, our analysis revealed that the common push-based\\nview of communication and control problems may not always\\nbe optimal, as the game theoretical properties of multiagent\\nscenarios do not guarantee that a solution may be better\\neven than a simple AoI-based optimization. Future extensions\\nof this work will concentrate on this conundrum, analyzing\\nmore complex examples of remote POMDPs and devising\\nheuristic strategies that provide good performance in practical\\nscenarios.\\nREFERENCES\\n[1] X.-M. Zhang, Q.-L. Han, X. Ge, D. Ding, L. Ding, D. Yue, and C. Peng,\\n“Networked control systems: A survey of trends and techniques,”\\nIEEE/CAA Journal of Automatica Sinica, vol. 7, no. 1, pp. 1–17, 2019.\\n[2] L. Da Xu, W. He, and S. Li, “Internet of things in industries: A survey,”\\nIEEE Transactions on industrial informatics, vol. 10, no. 4, pp. 2233–\\n2243, 2014.\\n[3] F. Mason, F. Chiariotti, A. Zanella, and P. Popovski, “Multi-agent\\nreinforcement learning for pragmatic communication and control,” 2023.\\n[4] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\\nMIT press, 2018.\\n[5] L. Busoniu, R. Babuska, and B. De Schutter, “A comprehensive survey\\nof multiagent reinforcement learning,” IEEE Transactions on Systems,\\nMan, and Cybernetics, Part C (Applications and Reviews), vol. 38, no. 2,\\npp. 156–172, 2008.\\n[6] G. E. Monahan, “State of the art—a survey of partially observable\\nMarkov decision processes: theory, models, and algorithms,” Manage-\\nment science, vol. 28, no. 1, pp. 1–16, 1982.\\n[7] H. A. Nam, S. Fleming, and E. Brunskill, “Reinforcement learning\\nwith state observation costs in action-contingent noiselessly observable\\nMarkov decision processes,” Advances in Neural Information Processing\\nSystems, vol. 34, pp. 15 650–15 666, 2021.\\n[8] V. Pareto, Manuale di economia politica con una introduzione alla\\nscienza sociale.\\nSociet`a Editrice Libraia, 1919.\\n[9] S. Rostami, S. Lagen, M. Costa, M. Valkama, and P. Dini, “Wake-\\nup radio based access in 5G under delay constraints: Modeling and\\noptimization,” IEEE Transactions on Communications, vol. 68, no. 2,\\npp. 1044–1057, 2019.\\n[10] J. Pineau, G. Gordon, S. Thrun et al., “Point-based value iteration: An\\nanytime algorithm for pomdps,” in Ijcai, vol. 3, 2003, pp. 1025–1032.\\n[11] T. Smith and R. Simmons, “Point-based pomdp algorithms: improved\\nanalysis and implementation,” in Proceedings of the Twenty-First Con-\\nference on Uncertainty in Artificial Intelligence, 2005, pp. 542–549.\\n[12] T.-Y. Tung, S. Kobus, J. P. Roig, and D. G¨und¨uz, “Effective communi-\\ncations: A joint learning and communication framework for multi-agent\\nreinforcement learning over noisy channels,” IEEE Journal on Selected\\nAreas in Communications, vol. 39, no. 8, pp. 2590–2603, 2021.\\n[13] X. Wang and T. Sandholm, “Reinforcement learning to play an optimal\\nnash equilibrium in team markov games,” Advances in Neural Informa-\\ntion Processing Systems (NIPS), vol. 15, 2002.\\n[14] D. Monderer and L. S. Shapley, “Potential games,” Games and Economic\\nBehavior, vol. 14, no. 1, pp. 124–143, 1996.\\n[15] X. Deng, N. Li, D. Mguni, J. Wang, and Y. Yang, “On the complexity\\nof computing Markov perfect equilibrium in general-sum stochastic\\ngames,” National Science Review, vol. 10, no. 1, p. nwac256, 2023.\\n[16] D. G¨und¨uz, F. Chiariotti, K. Huang, A. E. Kalør, S. Kobus, and\\nP. Popovski, “Timely and massive communication in 6G: Pragmatics,\\nlearning, and inference,” IEEE BITS the Information Theory Magazine,\\n2023.\\n'},\n",
       " {'abstract': 'This paper presents a run-time software testing, analysis, and code optimization method for quantum neural network software in advanced Internet-of-Things systems, which visually presents the learning performance called barren plateau. The visual presentation of barren plateau situations is helpful for real-time quantum-based advanced IoT software testing because software engineers can be aware of the training performances of QNN. The proposed method is also capable of visual feedback.',\n",
       "  'introduction': 'In modern daily lives, people utilize Internet services not only in smartphones but also in wearable devices. IoT enables the global Internet connection among all IoT devices, which realize the utilization of data from built-in sensors in the devices. QNN-based models can utilize much fewer parameters comparing to conventional deep neural network-based models and generally achieve fast convergence and high scalability. However, QNN-based models are more difficult to be trained due to barren plateaus and tracking barren plateaus is essential for measuring the stability of QNN software. Therefore, a new software test, analysis, and code optimization tool for QNN software is required to visually identify barren plateau situations. This will be a beneficial approach for software engineers in designing and developing high-accurate QNN-based advanced IoT software. The proposed TACO is also capable of visual feedback.',\n",
       "  'literature review': 'QNN-based models are advantageous in terms of data processing acceleration and low computational complexity, but have a disadvantage of the barren plateaus situation which can make the loss gradient of QNN-based models vanish due to entanglement. Various approaches have been suggested to tackle this situation, but these research results require deep-dive understanding in quantum mechanics, quantum computing, and quantum optimization. To resolve this problem from the viewpoints of deep learning software engineers, a novel software development and analysis tool is required.',\n",
       "  'methodology': 'The design rationales of TACO are: 1) Barren Plateaus Problem Tracking, 2) Dynamic Run-Time Software Testing, and 3) HCI-based Visualization. The overall architecture of TACO consists of VQC Structure, TACO Engine, TACO I/O, and External Visualization Engine. VQC Structure is implemented by torch-quantum, and the gradient derivation is operated via backward(). VQC Structure Extractor calls parameter information and iterative gradient derivation calculation is operated. Barren Plateau Estimator estimates the barren plateau occurrences and values. Model Feedback Generator handles the case where the barren plateau situations (i.e., the variance of gradients) happens in all quantum gates.',\n",
       "  'results': 'The results show that the proposed TACO can identify which quantum gates fall into barren plateaus situations with text messages. Software engineers can monitor and observe QNN-based model training performances using TACO. The proposed TACO can also identify which quantum gates fall into barren plateaus situations with text messages.',\n",
       "  'conclusion': 'This paper introduces a novel dynamic run-time software testing, analysis, and code optimization (TACO) tool for QNN-based model software, which visually presents gradient variances in order to identify whether barren plateaus occur or not for advanced IoT systems software. This TACO tool is obviously useful for software engineers because it can intuitively guide them in order to design and implement high-accurate QNN-based models for advanced IoT applications even if they are not familiar with quantum mechanics and quantum computing. Moreover, the proposed TACO is also capable for visual feedback because software engineers recognize barren plateaus using visualization via tensorboard; and then, they modify QNN-based model structures based on that. Future work can extend TACO to track and test other factors in QNN, and extend TACO for distributed learning architectures.',\n",
       "  'title': 'Quantum Neural Network Software Testing, Analysis, and Code Optimization for Advanced IoT Systems: Design, Implementation, and Visualization',\n",
       "  'author': 'Soohyun Park, Joongheon Kim',\n",
       "  'textdata': '1\\nQuantum Neural Network Software Testing,\\nAnalysis, and Code Optimization for Advanced IoT\\nSystems: Design, Implementation, and Visualization\\nSoohyun Park and Joongheon Kim, Senior Member, IEEE\\nAbstract—This paper introduces a novel run-time testing, anal-\\nysis, and code optimization (TACO) method for quantum neural\\nnetwork (QNN) software in advanced Internet-of-Things (IoT)\\nsystems, which visually presents the learning performance that\\nis called a barren plateau. The run-time visual presentation of\\nbarren plateau situations is helpful for real-time quantum-based\\nadvanced IoT software testing because the software engineers can\\neasily be aware of the training performances of QNN. Moreover,\\nthis tool is obviously useful for software engineers because it\\ncan intuitively guide them in designing and implementing high-\\naccurate QNN-based advanced IoT software even if they are\\nnot familiar with quantum mechanics and quantum computing.\\nLastly, the proposed TACO is also capable of visual feedback\\nbecause software engineers visually identify the barren plateau\\nsituations using tensorboard. In turn, they are also able to modify\\nQNN structures based on the information.\\nIndex Terms—Quantum Neural Network, Dynamic Software\\nAnalysis, Software Testing, Internet of Things\\nI. INTRODUCTION\\nIn modern daily lives, every single person actively starts\\nto utilize Internet services not only in smartphones but also\\nin wearable devices such as smartwatches and smartglasses.\\nThis trend can be realized due to the advances in Internet-\\nof-Things (IoT) technologies where the ultimate mission of\\nIoT is for enabling global Internet connection among all IoT\\ndevices which can realize the utilization of the perceived\\ndata from built-in sensors in the devices. For the example\\nof IoT-based autonomous driving, the technologies have un-\\ndergone significant development, and recent advances have\\nmade precise low-delay real-time perception of surrounding\\ndriving environments [1]. As presented in Fig. 1, the envi-\\nronment perception involves recognition objects from various\\nsensors using wireless communications, which deliver sensor\\ninformation from IoT devices to their associated centralized\\nserver [2]. After gathering sensor information from deployed\\nIoT devices, the server conducts deep learning computation\\nfor various application-specific AI services such as detection,\\nclassification, and recognition. Then, the trained parameters\\nwill be delivered back to the individual IoT devices in order\\nto realize the services. In this scenario, the use of quantum\\nneural network (QNN)-based deep learning models is defi-\\nnitely beneficial for computation/memory-limited IoT devices\\ndue to the fact that QNN-based models can utilize much fewer\\nS.\\nPark\\nand\\nJ.\\nKim\\nare\\nwith\\nthe\\nSchool\\nof\\nElectrical\\nEngi-\\nneering, Korea University, Seoul 02841, Republic of Korea (e-mails:\\n{soohyun828,joongheon}@korea.ac.kr).\\nSensor Information\\nIoT Sensors\\nWearable Devices\\nMobile Devices\\nQuantum Neural Network\\nPQC\\nStructure \\nEvaluator\\nTACO\\nBarren \\nPlateau \\nEstimator\\nModel\\nFeedback \\nGenerator\\nCloud \\nQNN Model Parameters\\nRun-Time \\nS/W Testing \\nQNN Model \\nInformation\\nFig. 1. Run-Time Software Testing and Analysis in QNN-based IoT Systems.\\nparameters comparing to conventional DNN-based models [3].\\nMoreover, QNN-based models are generally able to achieve\\nfast convergence and high scalability, not only in IoT devices\\nbut also in various computing platforms [3].\\nHowever, QNN-based models can be more difficult to be\\ntrained comparing to the training of conventional DNN-based\\nmodels because of barren plateaus, where it is defined as the\\nnonlinearity index depending on the amounts of entanglements\\nin quantum circuits [4]. Therefore, it is essentially required to\\ntrack the barren plateaus for measuring the stability of QNN\\nsoftware.\\nIn the perspective of testing, analysis, and code optimization\\nof QNN software, it should be helpful to obtain high-accurate\\nQNN execution results in a fast way, even if software engineers\\nare not familiar with the concepts of quantum computing.\\nTherefore, it is essentially required to design and implement\\na new software test, analysis, and code optimization (TACO)\\ntool for QNN software, which should visually identify barren\\nplateau situations for the explainability and trainability of\\nQNN. In addition, it should provide feedback that is useful\\nfor software testing, analysis, and code optimization. Based\\non this TACO tool for QNN software, software engineers are\\nable to re-organize and optimize their own QNN software\\narXiv:2401.10914v1  [cs.SE]  12 Jan 2024\\n2\\ncodes. Here, the concept of human-computer interaction (HCI)\\ncan be involved because the human (software engineers) can\\nre-organize and optimize the codes for the computer (IoT\\ndevices); in turn, the computer can visualize the barren plateau\\nsituations to the software engineers. These barren plateau situ-\\nations generally occur in QNN-based models due to the oper-\\nations of i) quantum controlled gates and ii) the superposition\\nof qubits. In addition, according to the entanglement among\\nmultiple qubits, QNN-based models are able to improve their\\nown performances. These entangled quantum states are advan-\\ntageous in terms of capacity, whereas multi-qubit interference\\nincreases while many quantum controlled gates and qubits are\\nsimultaneously operated. This introduces the situation where\\nQNN-based model training optimization is impossible due\\nto barren plateaus. Therefore, it is critical to design high-\\naccurate QNN-based models for improving learning training\\nperformance. Among significant research contributions to deal\\nwith the barren plateau problem, it has been verified that\\nthe barren plateau problem is associated with the structure\\nof QNN-based models [4]. Therefore, controlling the system\\nsize in the structure of QNN-based models enhances the per-\\nformance of quantum learning training. During this procedure,\\ngeneral software engineers will be in trouble in terms of i)\\nunderstanding quantum mechanics, quantum computing, and\\nquantum neural networks (including the definition of barren\\nplateaus) and ii) the identification of optimal system size in\\nthe structure of QNN-based models in order to reduce quantum\\nentanglement interference.\\nMotivated by this, a novel software testing and analysis\\ntool, i.e., TACO is designed which enables dynamic run-\\ntime software testing and analysis in advanced IoT systems.\\nAs depicted in Fig. 1, quantum-based advanced IoT systems\\nare capable to perform various tasks, including surveillance\\nfor IoT sensors, recognition for mobile devices, and sens-\\ning/processing for wearable devices. During this process, our\\nsoftware testing and analysis tool, TACO, is able to provide\\nreal-time updates on the information that is trained by ad-\\nvanced IoT systems, thereby enabling the tool to verify the\\naccuracy of the perception during run-time execution.\\nThe main contributions and objectives of this paper can be\\ncategorized and summarized as follows.\\n• Firstly, our proposed TACO is the world-first software\\ntesting, analysis, and code optimization tool of QNN-\\nbased models in advanced IoT systems. The main objec-\\ntive of TACO is for pursuing high-accurate and stabilized\\nperformance maintenance in the QNN-based models by\\ncontrolling the degree of entanglements.\\n• Moreover, the additional objective of TACO is to provide\\nintuitive approaches such as visualization and HCI-based\\nfeedback for general software engineers during QNN-\\nbased model design and implementation.\\n• Lastly, this paper provides the various extension di-\\nrections of TACO those are considerable for emerging\\napplications.\\nThe rest of this article is organized as follows. Sec. II\\nintroduces quantum supremacy and the limitations. In addition,\\nSec. III presents the details of our proposed TACO for QNN\\nsoftware testing and visualization. Moreover, Sec. IV outlines\\nrelevant corresponding open discussions, and lastly, Sec. V\\nconcludes this paper.\\nII. QUANTUM SUPREMACY AND LIMITATIONS\\nThis section firstly introduces the foundations of QNN (refer\\nto Sec. II-A), and then discusses its advantages (refer to\\nSec. II-B) and one of major disadvantages which is called\\nbarren plateau (refer to Sec. II-C), respectively.\\nA. Quantum Neural Networks\\nThe conventional neural network can be mathematically\\ninterpreted as the sequential combination of multiple i) linear\\ntransformation and ii) nonlinear transformation. The struc-\\nture of QNN-based models is basically similar to the struc-\\nture of conventional neural networks. Note that one of the\\nmajor differences between the two neural networks is that\\nthe training/inference computation of QNN-based models is\\nperformed over Hilbert spaces whereas the computation of\\nconventional neural networks is performed over real number\\nspaces. Therefore, the QNN-based models optimize their ob-\\njective functions in quantum states over Hilbert spaces. The\\ndetailed structural components of QNN-based models, called\\nquantum gates, are i) quantum rotation gates for quantum state\\ntransformation over Bloch sphere and ii) quantum-controlled\\ngates for entanglement generation with other qubits. With the\\nutilization of these quantum gates, the structure of QNN-based\\nmodels consists of i) state encoding (encoding the classical\\ndata over real number spaces to the quantum states over Hilbert\\nspaces, i.e., converting classical 0 or 1 bits into qubits), ii)\\nparameterized quantum circuit (PQC) (controlling the rota-\\ntions and entanglements for input quantum states under the\\nutilization of quantum gates), and iii) measurement (decoding\\nthe quantum states over Hilbert spaces into the classical data\\nover real number spaces, i.e., converting qubits into classical\\n0 or 1 bits, during training optimization). After this structural\\ncomputation, the quantum states in the QNN-based models\\ncan be observed; and this is conducted for minimizing its own\\ncost function. The procedures i–ii), i.e., state encoding and\\nparameterized quantum circuit, are linear transformations, and\\nthe procedure iii), i.e., measurement, is a unique non-linear\\ntransformation [5].\\nThis QNN-based model is advantageous compared to con-\\nventional neural networks in terms of data processing ac-\\nceleration and low computational complexity. However, this\\nQNN-based model has disadvantages as well; and one of\\nthe disadvantages is the barren plateaus situation which can\\nmake the loss gradient of QNN-based models vanish due to\\nentanglement. More details about this barren plateaus problem\\nwill be discussed later (refer to Sec. II-C).\\nB. Advantages in QNN\\nThe advantages of QNN for can be listed as follows.\\n• High Scalability. As theoretically well-justified in [6],\\nQNN-based models can expand the output dimensions to\\nan exponential scale by incorporating Pauli-Z measure-\\nment and basis measurement, surpassing the limitations\\n3\\nExternal Feedback Depictor \\nTACO Core\\nPQC\\nStructure \\nTarget QNN Code\\nTACO Main\\nTACO I/O\\nPQC\\nStructure \\nEvaluator\\nBarren \\nPlateau \\nEstimator\\nModel\\nFeedback \\nGenerator\\nTACO Module\\nUser Command (Visualization Setting)\\nBarren Plateau\\nPQC\\nStructure \\nFeedback \\nfrom TACO\\nUser Command,\\nFeedback Request\\nQuantum\\nInterference\\nModel \\nFeedback\\nParameter\\nCrawling\\nArchitecture \\nInformation\\nInput Tareget \\nQNN Code\\nPQC Structure \\nFeedback\\nExplainability\\nExtractor\\nTrainability\\nExtractor\\nIoT \\nDevice\\nCloud\\nFig. 2. Overall Process of TACO, i.e., Dynamic Run-Time Quantum Software Testing, Analysis, and Code Optimization.\\nimposed by the limited number of qubits in the noisy\\nintermediate scale quantum (NISQ) era.\\n• Fewer Parameter Utilization. QNN-based models have\\nthe ability to achieve similar performance to conventional\\nneural networks with fewer parameters, primarily due to\\nsuperposition and entanglement. Firstly, the superposition\\nallows the quantum models to exist in multiple states at\\nonce simultaneously, and thus, it expands the represen-\\ntational dimension. Moreover, the entanglement provides\\nsolid relationship among quantum states using controlled-\\nNOT (CNOT) gates, and thus, it realizes nonlinear rep-\\nresentation with fewer parameters.\\n• Fast Convergence. In QNN, the parameter shift rule is\\nused during training instead of the backpropagation in\\nconventional neural network training. The parameter shift\\nrule provides simple and direct approaches, and thus,\\nQNN-based model training can be accelerated further\\nthan conventional neural network training.\\nC. Barren Plateaus\\nThe barren plateau is one of the well-known harmful\\nproblems for QNN-based learning model training optimiza-\\ntion where the loss function gradients are exponentially and\\ndramatically degradated due to entanglement [4]; and this\\ncan lead to local minima during cost function minimization.\\nMoreover, recent remarkable research results confirm that a lot\\nof barren plateaus can occur during QNN-based model training\\noptimization [5]. For more details, the number of barren\\nplateaus is proportioned to the number of qubits exponentially,\\nand this means the occurrence of barren plateaus depends on\\nQNN-based model design and implementation. Furthermore,\\nit is hard to escape from the barren plateaus [4]. In order to\\ntackle this situation, various approaches have been suggested.\\nHowever, these research results fundamentally require deep-\\ndive understanding in terms of quantum mechanics, quantum\\ncomputing, and quantum optimization. To resolve this problem\\nfrom the viewpoints of deep learning software engineers, it is\\nobvious that a novel software development and analysis tool\\nis required which can efficiently inform the barren plateau\\noccurrences during QNN-based model training optimization.\\nIII. TACO FOR QNN-BASED MODELS\\nA. Design\\nThe details of three fundamental design rationales of TACO\\nare as follows.\\n1) Barren Plateaus Problem Tracking: For the design of\\nhigh-accurate QNN-based model software, TACO informs the\\nbarren plateaus situation occurrence and its corresponding\\nperformance degradation to quantum machine learning soft-\\nware developers and engineers. According to this beneficial\\nfunctionality of TACO, the QNN software engineers are able to\\nsignificantly reduce the degraded QNN-based model training\\nperformances. As a result, software engineers can achieve\\nrobust and resilient QNN-based model training optimization.\\n2) Dynamic Run-Time Software Testing: It is interesting to\\nknow that the software testing and analysis of QNN-based\\nmodel software codes is not able to be conducted using\\nstatic analysis. This is because the static analysis conducts\\nthe software testing for the integrity of QNN software with\\noriginal codes. The reason is that the QNN software codes\\nshould be tested and verified while qubits inputs are fed to\\nQNN-based models where the qubits exist in probabilistic\\nquantum states before the measurement. Hence, it is essential\\nto conduct the software testing for the given QNN-based model\\nsoftware codes using dynamic run-time analysis which tests\\nthe integrity of QNN software codes during their run-time\\nexecutions.\\n3) HCI-based Visualization:\\nFor identifying the barren\\nplateaus situations those are harmful for the high-accurate ex-\\necution of QNN-based models, TACO observes the dynamics\\nof barren plateaus (i.e., variances of gradients) while the qubit\\nstate changes over time. Therefore, the visualization of the\\nbarren plateau dynamics over time is fundamentally required\\nfor intuitive understanding to QNN software engineers. Then,\\n4\\nIoT Device\\nCloud\\n- Make a sample QNN code\\n- Forward QNN code information\\n- Process TACO Core\\n- Plot variance in real-time   \\n- Interpret the feedback of TACO\\n- Modify QNN code\\nInput target code\\nTransmit user command\\n(Visualization Setting)\\nTransmit visual feedback \\nfor the target QNN code \\nInput the modified target code\\nFig. 3. Flowchart of TACO\\nthey can easily understand the performance of the given\\nQNN-based models, even if the software engineers are not\\nfamiliar with the basic knowledge of quantum mechanics,\\nquantum computing, and quantum machine learning. Lastly,\\nthese visualization results can provide sufficient information\\nto software engineers, and then, they can provide feedback\\ninformation to TACO via HCI-based visual feedback.\\nB. Implementation\\nThe overall software architectures and their corresponding\\nimplementation of TACO are illustrated in Fig. 2, and the\\ndetailed descriptions are as follows.\\n• VQC Structure: This is a variational quantum circuit\\n(VQC) and this is the user input by software engineers. In\\nour implementation by the softwareengineers, the consid-\\nering QNN-based model software code is implemented by\\ntorch-quantum. In the QNN-based model software\\ncode, the quantum rotation gates and controlled gates\\nare organized and implemented by the random layers\\nin torch-quantum. The example implementation of\\nVQC Structure is presented in Fig. 4. For (Lines 4–6)\\nin Fig. 4, the defined class of VQC Structure is with\\nn_wires which stands for the number of input qubits.\\nAdditionally, this has encoder which converts classical\\nbits into qubits. Moreover, pqc exists in this VQC\\nStructure class and this consists of multiple quantum\\nrotations and controlled gates. For (Line 8–17) in Fig. 4,\\nthe function called forward is defined when the object\\nof the QNN-based model is generated. The input of this\\nobject feeds into encoder and pqc, sequentially. After\\nconducting this procedure, tq.PauliZ() (defined in\\ntq.expval()) is able to compute projections over z-\\naxis. Finally, the output result of a measurement can\\nbe obtained, and this is the computed and approximated\\naction of our trained QNN-based models.\\n• TACO Engine: Our TACO engine in Fig. 2 consists of\\nfollowing three modules, i.e., VQC Structure Extractor,\\nBarren Plateau Estimator, and Model Feedback Gen-\\nerator.\\n– VQC Structure Extractor:\\nThis calls parame-\\nter information via named_parameters() in\\ntorch-quantum; and the iterative gradient deriva-\\ntion calculation is operated via backward().\\n– Barren Plateau Estimator: This estimates the bar-\\nren plateau occurrences and values where the value\\nstands for the variance of gradients in the quantum\\ngates of QNN-based models. For more details, we\\ncan track the computation falls into a barren plateau\\nwhen the variance of the gradient suddenly drops.\\nThe implementation of this variance of the gradient\\ncan be by var() which is one of the statistics\\nfunctions in torch-quantum.\\n– Model Feedback Generator: This generator han-\\ndles\\nthe\\ncase\\nwhere\\nthe\\nbarren\\nplateau\\nsit-\\nuations\\n(i.e.,\\nthe\\nvariance\\nof\\ngradients)\\nhap-\\npens\\nin\\nall\\nquantum\\ngates.\\nIn\\naddition,\\nthis\\ngenerates text messages which include epoch,\\nparameter index,\\nparameter type,\\nand\\nbarren plateaus value.\\n• TACO I/O: It manages the inputs and outputs of TACO.\\nIt sends the data of VQC structure feedback for visual-\\nization to External Visualization Engine (i.e., tensor-\\nboard in our implementation).\\n• External Visualization Engine: This engine is for\\nrecording the data from TACO and presenting to our con-\\nsidering tensorboard. The training data can be plotted\\nusing this engine. This conducts the visualization of ‘train\\n5\\nclass VQC_Structure(tq.QuantumModule):\\ndef __init__(self):\\nsuper().__init__() \\nself.n_wires = 4\\nself.encoder = tq.GeneralEncoder(enc_dict[‘4x4_ryzxy’]) \\nself.pqc\\n= tq.RandomLayer(n_ops = 50, wires = [0, 1, 2, 3])\\ndef forward(self, input, q_device):\\nbatch_size\\n= input.shape[0]\\ninput\\n= input.reshape(batch_size, -1).to(dtype = torch.complex64)\\nself.encoder(q_device, input)\\nself.pqc(q_device)\\naction_dist\\n= tq.expval(q_device,\\n[i for i in range(num_class)],\\n[tq.PauliZ() for _ in range(num_class)]\\n).squeeze()\\nreturn action_dist\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\nFig. 4. The Example Code for VQC Structure.\\nEpoch\\nAccuracy\\nRun-Time\\nPlotting Result\\nVisualization\\nMetric / Accuracy\\ntag: Metric / Accuracy\\n0.4\\n0.3\\n0.2\\n0.1\\n0.5\\n0\\n10\\n20\\n30\\n0.4\\n0.3\\n0.2\\n0.1\\n0.5\\n0.6\\n100\\n40\\n20\\n0\\n60\\n80\\nFig. 5. Test Accuracy.\\nNormal\\nBehaviors\\nRun-Time\\nPlotting Result\\nVisualization\\nMetric / BarrenPlateaus\\ntag: Metric / BarrenPlateaus\\nVariance\\nBarren Plateaus\\n8e-3\\n6e-3\\n4e-3\\n2e-3\\n0\\n0\\n10\\n20\\n30\\n0.012\\n8e-3\\n4e-3\\n0\\n100\\n40\\n20\\n0\\n60\\n80\\nEpoch\\nFig. 6. Barren Plateaus.\\nloss’, ‘test accuracy’, and ‘barren plateaus’ values. Then,\\nQNN software engineers are able to identify training data\\nvia tensorboard and also check which local parameters\\nare in barren plateau situations.\\nBased on this implementation, the operational flowchart can\\nbe illustrated as Fig. 3. Firstly, IoT devices make a sample\\nQNN code and forward the QNN code information to their\\nassociated cloud, i.e., target code input. Additionally, IoT\\ndevices transmit the user command for visualization setting.\\nThen, the could processes TACO Core and plots variance in\\nreal-time for barren plateaus tracking. Then, the results will\\nbe transmitted to its associated IoT devices for the visual\\nfeedback pf the target QNN code. After receiving the feedback\\nat IoT devices, they interpret the feedback of TACO and\\nmodify their QNN codes based on that. Finally, the IoT devices\\ndeliver the modified target QNN code input to their associated\\ncloud.\\nC. Visualization\\nThe run-time visualization results for dynamic run-time\\nsoftware testing and analysis using TACO are illustrated in\\nFig. 5 and Fig. 6, where Fig. 5 shows the test accuracy over\\ntime during QNN-based model training optimization and Fig. 6\\nshows the variances over time in quantum rotation gates (i.e.,\\nRX, RY , and RZ) in VQC at every epoch. If the variances\\nsuddenly drop, it is obvious that the corresponding quantum\\ngates fall in barren plateaus situations. As shown in Fig. 5,\\nthe test accuracy drops to approximately 35% from 35 to 75\\nepochs, where the number of quantum rotation gates those\\nare in barren plateau situations is the highest as demonstrated\\nin Fig. 6. Accordingly, it can be observed that there exists\\na negative relationship between the performance of QNN-\\nbased models and barren plateau situations. Moreover, Fig. 6\\npresents run-time performance evaluation results plotting over\\ntime with visualization. Therefore, it is able to provide intuitive\\nunderstanding for software engineers even in cases where they\\nare not familiar with quantum mechanics, quantum comput-\\ning, and quantum optimization. Finally, the proposed TACO\\ncan identify which quantum gates fall into barren plateaus\\nsituations with text messages.\\n6\\nIn summary, software engineers are able to monitor and ob-\\nserve QNN-based model training performances using TACO.\\nIf the performance of current QNN-based model training op-\\ntimization is less than expectation, the software engineers can\\nidentify the problematic quantum rotation gates using ’Model\\nFeedback’ in the proposed TACO. Thanks to this advantage\\nof this TACO, the software engineers can adequately control\\nthe parameters of the problematic quantum rotation gates in\\norder to improve QNN-based model training performances.\\nIV. OPEN DISCUSSIONS\\nA dynamic run-time software testing and analysis tool,\\ncalled TACO, is proposed and it is for verifying the trainability\\nand explainability of QNN-based model software for advanced\\nIoT systems. In order to realize the QNN functionalities in\\nadvanced IoT systems, it is required to address open issues\\nin terms of the usability of quantum computing technologies,\\ni.e., i) quantum computer miniaturization, ii) limitations in the\\nNISQ era, and iii) commercialization.\\nA. Quantum Computer Miniaturization\\nRecently, most of the research contributions in the areas\\nof quantum computing, quantum algorithms, and QNN are\\naiming to improve computation performance in terms of\\nspeed and computing resources. Therefore, industry compa-\\nnies such as IBM and Google have produced large quantum\\ncomputing platforms. As a result, it has been widely known\\nthat quantum computers are heavy and must be stored in\\ncryogenic environments. Therefore, people may have questions\\nabout the feasibility of quantum computer utilization in IoT\\ndevices for QNN-based model utilization. However, there are\\nalready triumphant attempts at building miniaturized quantum\\ncomputers. For example, Alpine Quantum Technologies has\\ndeveloped a commercial 19-inch rack-mounted 20-qubit quan-\\ntum computer; and demonstrating that it is possible to lessen\\nthe size of quantum computers [7]. For more details, it only\\nrequires two fully custom racks, runs in a room temperature\\nenvironment, and consumes less than 2 kW of power. Based\\non the pace of progress in quantum computers, the potential of\\nQNN utilization for reliable IoT systems is significant and it is\\nobvious that the quantum computer miniaturization technology\\ndevelopment is enough to be realized. Moreover, Quantum\\nBrilliance developed a 5-qubit miniature quantum computing\\nplatforms with the size of a desktop mainframe. Lastly, SpinQ\\ncreated a portable 2-qubit quantum computer called Gemini\\nmini.\\nB. Limitation in the NISQ Era\\nBased on the utilization of quantum computing, it is possible\\nto realize accurate and accelerated massive large-scale data\\nprocessing. However, according to the limitations of hardware\\ntechnologies in quantum computing, the number of qubits that\\ncan be utilized for real-time computation is strictly limited in\\nthe noisy intermediate-scale quantum (NISQ) era. In this case,\\nquantum errors occur during the computation with quantum\\ngates and circuits, when a relatively larger number of qubits\\nis utilized. This is definitely harmful to advanced IoT related\\ndecision-making if the decision-making is made by QNN-\\nbased models. In order to deal with this problem, significant\\nand novel research results have been proposed for taking care\\nof quantum errors in advanced IoT systems. For example,\\nthe proposed algorithm in [8] conducts the classification\\ntasks during 3D point cloud data processing for IoT-capable\\nautonomous driving with a small number of qubits. It is\\nalso confirmed that this algorithm outperforms other learning-\\nbased methods. Moreover, the proposed algorithms in [9],\\n[10] are for quantum actor-critic reinforcement learning in\\norder to control multiple autonomous mobile robots and mul-\\ntiple unmanned aerial vehicles, respectively. These algorithms\\noutperform other learning-based methods even though they\\nutilize a small number of qubits. Furthermore, many experts in\\nQNN and quantum computing predict that several hundreds of\\nqubits can be utilized where the qubits are in low-noise situa-\\ntions [11]. For example, academia and industry experts expect\\nthat IBM will design its own quantum computer which can\\nutilize 4, 000 qubits until 2025 [12]. Lastly, Baidu designed\\nand implemented its own 10-qubit quantum computer that is\\nnamed to Qian-Shi; and insists that the quantum computer can\\ndeal with many real-world on-hand problems [12].\\nC. Commercialization\\nIn order to deploy QNN and quantum computing tech-\\nnologies in advanced IoT systems, the mass production and\\ncommercialization of quantum computers should be realized\\nand established. Shenzhen SpinQ Technology, which is one of\\nquantum computing startups in China, sells portable quantum\\ncomputers with the prices from 5, 000 to 9, 000 US dol-\\nlars [13]. Furthermore, it is clear that the prices can be dramat-\\nically decreased for mass production and commercialization.\\nV. CONCLUSIONS AND FUTURE WORK\\nThis paper introduces a novel dynamic run-time software\\ntesting, analysis, and code optimization (TACO) tool for\\nQNN-based model software, which visually presents gradient\\nvariances in order to identify whether barren plateaus occur or\\nnot for advanced IoT systems software. This TACO tool is ob-\\nviously useful for software engineers because it can intuitively\\nguide them in order to design and implement high-accurate\\nQNN-based models for advanced IoT applications even if\\nthey are not familiar with quantum mechanics and quantum\\ncomputing. Moreover, the proposed TACO is also capable for\\nvisual feedback because software engineers recognize barren\\nplateaus using visualization via tensorboard; and then, they\\nmodify QNN-based model structures based on that.\\nAs future work directions, our proposed TACO can be\\nextended in various ways as follows.\\n• Firstly, the current version of TACO focuses on barren\\nplateaus whereas QNN-based models are with various\\nperformance-dependent factors in the state encoder and\\nmeasurement of QNN. Therefore, the extension of TACO\\nshould be able to track and test the factors.\\n• Moreover, our proposed TACO can be extended for\\ndistributed learning architectures such as quantum slit\\n7\\nlearning [14]. In quantum split learning, single QNN is\\ndivided into two parts and they are located in separated\\ncomputers [15]. For the separated parts, one can be with\\nthe state encoder of QNN; and the other one can be with\\nthe PQC and measurement of QNN. Then the first part\\nshould create dummy codes for PQC and measurement\\nin order to generate the complete QNN software for\\nconducting TACO-based testing and analysis. Similarly,\\nthe other part should create dummy codes for state\\nencoder in order to generate the complete QNN software\\nfor conducting TACO. This approach is fundamentally\\nbased on automated code generation and various related\\nmethods should be studied under the consideration of\\nquantum computing characteristics.\\nREFERENCES\\n[1] S. Park, H. Feng, C. Park, Y. Lee, S. Jung, and J. Kim, “EQuaTE:\\nEfficient quantum train engine for runtime dynamic analysis and visual\\nfeedback in autonomous driving,” IEEE Internet Computing, vol. 27,\\nno. 5, pp. 24–31, September-October 2023.\\n[2] S.\\nJung,\\nJ.\\nKim,\\nM.\\nLevorato,\\nC.\\nCordeiro,\\nand\\nJ.-H.\\nKim,\\n“Infrastructure-assisted on-driving experience sharing for millimeter-\\nwave connected vehicles,” IEEE Transactions on Vehicular Technology,\\nvol. 70, no. 8, pp. 7307–7321, August 2021.\\n[3] S. Park, J. P. Kim, C. Park, S. Jung, and J. Kim, “Quantum multi-\\nagent reinforcement learning for autonomous mobility cooperation,”\\nIEEE Communications Magazine, 2023 (Early Access).\\n[4] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Babbush, and H. Neven,\\n“Barren plateaus in quantum neural network training landscapes,” Nature\\nCommunications, vol. 9, no. 1, p. 4812, November 2018.\\n[5] X. You and X. Wu, “Exponentially many local minima in quantum\\nneural networks,” in Proc. Int’l Conf. Machine Learning (ICML), Virtual,\\nJuly 2021.\\n[6] H. Baek, S. Park, and J. Kim, “Logarithmic dimension reduction\\nfor quantum neural networks,” in Proc. ACM Conf. Information and\\nKnowledge Management (CIKM), Birmingham, U.K., October 2023.\\n[7] R.\\nBlatt,\\nT.\\nMonz,\\nand\\nP.\\nZoller,\\n“The\\nworld’s\\nleading\\n19”\\nrack-mounted quantum computer,” May 2022. [Online]. Available:\\nhttps://www.aqt.eu/pine-system-19-rack-mounted-quantum-computer/\\n[8] H. Baek, W. J. Yun, and J. Kim, “3D scalable quantum convolutional\\nneural networks for point cloud data processing in classification ap-\\nplications,” in Proc. AAAI Workshop on AI to Accelerate Science and\\nEngineering (AI2ASE), Washington, DC, USA, February 2023.\\n[9] W. J. Yun, J. P. Kim, S. Jung, J.-H. Kim, and J. Kim, “Quantum\\nmultiagent actor-critic neural networks for Internet-connected multirobot\\ncoordination in smart factory management,” IEEE Internet of Things\\nJournal, vol. 10, no. 11, pp. 9942–9952, June 2023.\\n[10] C. Park, W. J. Yun, J. P. Kim, T. K. Rodrigues, S. Park, S. Jung,\\nand J. Kim, “Quantum multi-agent actor-critic networks for cooperative\\nmobile access in multi-UAV systems,” IEEE Internet of Things Journal,\\nvol. 10, no. 22, pp. 20 033–20 048, November 2023.\\n[11] J. Preskill, “Quantum computing in the NISQ era and beyond,” Quan-\\ntum, vol. 2, p. 79, August 2018.\\n[12] J. Bourne, “Commercial quantum computer disruption on the horizon,”\\nAugust 2022. [Online]. Available: https://www.insiderintelligence.com/\\ncontent/commercial-quantum-computer-disruption-on-horizon\\n[13] H. Corrigan, “You can buy a portable quantum computer for under\\n$9K,” December 2022. [Online]. Available: https://www.pcgamer.com/\\nyou-can-buy-a-portable-quantum-computer-for-under-dollar9k/\\n[14] S. Park, H. Baek, and J. Kim, “Quantum split learning for privacy-\\npreserving information management,” in Proc. ACM Conf. Information\\nand Knowledge Management (CIKM), Birmingham, U.K., October 2023.\\n[15] J. Jeon and J. Kim, “Privacy-sensitive parallel split learning,” in Proc.\\nIEEE Int’l Conf. Information Networking (ICOIN), Barcelona, Spain,\\nJanuary 2020.\\nDr. Soohyun Park has been a postdoctoral scholar at the Department of\\nElectrical and Computer Engineering, Korea University, Seoul, Republic of\\nKorea, since September 2023. She received the Ph.D. degree in electrical and\\ncomputer engineering from Korea University, Seoul, Republic of Korea, in\\nAugust 2023. She received the B.S. degree in computer science and engineer-\\ning from Chung-Ang University, Seoul, Republic of Korea, in February 2019.\\nShe was a recipient of the IEEE Vehicular Technology Society (VTS) Seoul\\nChapter Awards (2019, 2023), IEEE Seoul Section Student Paper Content\\nAwards (2020, 2023), and ICT Express (Elsevier) Best Reviewer Award\\n(2021).\\nProf. Joongheon Kim (M’06–SM’18) has been with Korea University, Seoul,\\nKorea, since 2019, and he is currently an associate professor. He received the\\nPh.D. degree in computer science from the University of Southern California\\n(USC), Los Angeles, CA, USA, in 2014. He serves as an editor for IEEE\\nTRANSACTIONS ON VEHICULAR TECHNOLOGY and IEEE INTERNET OF\\nTHINGS JOURNAL. He was a recipient of Annenberg Graduate Fellowship\\nwith his Ph.D. admission from USC (2009), Intel Corporation Next Generation\\nand Standards (NGS) Division Recognition Award (2015), IEEE SYSTEMS\\nJOURNAL Best Paper Award (2020), IEEE ComSoc Multimedia Communi-\\ncations Technical Committee (MMTC) Outstanding Young Researcher Award\\n(2020), and IEEE ComSoc MMTC Best Journal Paper Award (2021). He also\\nreceived IEEE ICOIN Best Paper Award (2021), IEEE Vehicular Technology\\nSociety (VTS) Seoul Chapter Awards (2019, 2023), and IEEE ICTC Best\\nPaper Award (2022).\\n'},\n",
       " {'abstract': 'Recent advancements in Large Language Models (LLMs) exhibit impressive capabilities in various applications, yet LLMs face challenges with limited context windows and generalization difficulties. We introduce a metacognition module for generative agents, allowing them to observe their own thought processes and actions. This approach, designed to emulate System 1 and System 2 cognitive processes, significantly enhances agent performance by modifying their strategy. We tested the metacognition module on a variety of scenarios, including a zombie apocalypse simulation, demonstrating how our system outperforms others, with agents adapting and improving strategies to complete tasks over time.',\n",
       "  'introduction': \"Metacognition refers to the cognitive process of thinking about one's own thinking. It involves activities related to monitoring, regulating, and organizing cognitive processes to achieve specific goals. Metacognitive abilities enable individuals to reflect on their knowledge, problem-solving strategies, and learning experiences, shaping and modifying their habits. Psychologist Daniel Kahneman's concept of System 1 and System 2 thinking provides a framework for understanding metacognition. System 1 represents fast, automatic, and intuitive thinking, while System 2 involves slower, deliberate, and reflective thinking. Metacognition can be seen as a specific System 2 process that examines actions from both System 1 and System 2 processing.\",\n",
       "  'literature_review': 'Multiple experiments have incorporated metacognition into computational frameworks. Cox et al. outline a general computational architecture in Lisp. Mustafa et al. provide a framework for autonomous vehicles that adds a metacognition layer to monitor safety violations. Krueger, Lieder, and Griffiths created a deep reinforcement learning (DRL) framework that incorporates metacognition. Park et al. demonstrated that LLMs equipped with reflection, observation, and planning modules on agents can mimic believable human behavior. Our proposed metacognition module allows agents to broadly contemplate their circumstances to create alternative strategies and improve performance.',\n",
       "  'methodology': 'We implemented many of the same modules from Park et al., with the addition of a group of modules dubbed meta_cognize. As an agent progresses through the simulation, it accumulates a history of observations, memories, and thoughts. Agents are given goals but can optionally be left blank. When an agent starts towards its goal, it is not given an explicit strategy to follow. Instead, each agent periodically evaluates how it is progressing towards its goals by reviewing memories, thoughts, and past actions. The agent assigns itself a numeric score and a text statement for its reasoning. This evaluation is stored in its memory as a meta-thought. If the agent finds that it is not making enough progress, it calls its meta_cognize module. During metacognition, the agent asks itself how it might improve its performance in light of what it has learned. Additionally, the agent will periodically self-generate new introspective questions to think about its goals from different perspectives.',\n",
       "  'results': 'We tested our simulation framework in a variety of situations, including a Christmas party, zombie apocalypse, and murder mystery. In the Christmas party simulation, agents hosted a party where multiple other agents were invited and arrived at the specified time to attend. This is similar to earlier work in generative agents where agents had to coordinate a similar social activity. In the zombie apocalypse simulation, zombies are non-playable characters that are allowed to kill non-zombie agents. Agents initially have no goal but can develop them over time. Survivors most often self-discovered a strategy of hiding in zombie-free areas. We found that in 73% of zombie scenarios, agents would not survive. Performance of our cognitive models is shown through ablation. Evaluation metrics are composed of five criteria: believability, learning, individual goal performance, higher level cognitive performance, and overall scenario performance.',\n",
       "  'conclusion': 'We show that metacognition significantly improves performance for task-oriented generative agents. Furthermore, we illustrate the potency of combining large language models with traditional programming methods as effective tools for prototyping cognitive systems. As generative agents integrated with metacogntive abilities approach ubiquity in daily human life, taking on increasingly sophisticated tasks, their proliferation across diverse domains marks a paradigm shift in both lay human-computer interactions and programmer-computer interactions. This shift paves the way for the emergence of more intelligent, adaptive, and context-aware systems. With these advancements in mind, the strategic interplay of metacognition, LLMs, and traditional programming methodologies emerges as a powerful technique for the productionization of intelligent generative agents.',\n",
       "  'title': 'Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior',\n",
       "  'author': 'Jason Toy, Josh MacAdam, Phil Tabor',\n",
       "  'textdata': 'Metacognition is all you need?\\nUsing Introspection in Generative Agents to Improve Goal-directed Behavior\\nJason Toy1, Josh MacAdam, and Phil Tabor\\nJanuary 23, 2024\\nAbstract\\nRecent advances in Large Language Models (LLMs) have shown impressive capabilities in various applications, yet LLMs face\\nchallenges such as limited context windows and difficulties in generalization. In this paper, we introduce a metacognition module\\nfor generative agents, enabling them to observe their own thought processes and actions. This metacognitive approach, designed\\nto emulate System 1 and System 2 cognitive processes, allows agents to significantly enhance their performance by modifying\\ntheir strategy. We tested the metacognition module on a variety of scenarios, including a situation where generative agents must\\nsurvive a zombie apocalypse, and observe that our system outperform others, while agents adapt and improve their strategies to\\ncomplete tasks over time.\\nFigure 1: Zombie Apocalypse Simulation\\n1 Introduction\\nMetacognition refers to the higher-order cognitive pro-\\ncesses that involve thinking about one’s own thinking.\\nIt encompasses a range of mental activities related\\nto monitoring, regulating, and organizing cognitive\\nprocesses to achieve specific goals. Metacognitive abil-\\nities enable individuals to reflect on their knowledge,\\nproblem-solving strategies, and learning experiences\\nand therefore play a crucial role in shaping and mod-\\nifying one’s habits. For example, if one is studying\\nfor an exam, metacognitive processes might involve\\nsetting goals, choosing appropriate study strategies,\\nmonitoring their comprehension, and adjusting their\\napproach if they are not understanding the material.\\nThe concept of System 1 and System 2 thinking, pop-\\nularized by psychologist Daniel Kahneman, provides\\na framework for understanding metacognition[1]. Sys-\\ntem 1 represents fast, automatic, and intuitive thinking,\\n1Jason Toy, somatic.io - jasontoy@gmail.com\\nPhil Tabor\\nJosh MacAdam, obvi\\nwhile system 2 involves slower, deliberate, and reflec-\\ntive thinking. In this framework, metacognition can be\\nthought of as a specific System 2 process that examines\\nactions from both System 1 and System 2 processing.\\nAnother analogy is system 1 is subconscious think-\\ning and system 2 is conscious thinking (the voice you\\nhear in your head). Metacognition is a slow, expensive,\\nand methodical thought process and is therefore better\\nsuited for introspective or strategic thinking, rather\\nthan immediate problem solving.\\nMetacognition in essence is \"thinking about thinking\"\\nand requires the ability to look at one’s own thoughts\\nand thought processes from different points of view.\\nReflection, on the other hand, is typically characterized\\nas looking at past experiences and deriving insights for\\nfuture actions. Metacognition allows one to adjust their\\nthought process and strategy based on asking relevant\\nquestions. Due to the dynamic nature of metacognition,\\nno one strategy will be set in stone. Metacognition typ-\\nically involves asking oneself different questions as a\\nprobing mechanism to further understand. Some ques-\\ntions one may see: \"What do I know about this topic?\",\\n\"Why do I want to achieve this goal?\", \"How can I\\nmonitor my progress towards my goal?\", \"How can I\\nadjust my strategy to overcome current challenges?\",\\n\"Metacognition is all you need?\" etc. Metacognition\\nis often applied to different types of thinking such as:\\nproblem solving, goal setting, reflection, learning, mon-\\nitoring and evaluation, emotional regulation and other\\ntypes of cognitive processes. As artificial agents are\\nusually given a specific task, we focus on metacognition\\nrelated to problem solving, monitoring, and evaluating\\nprogress towards their specific task.\\nRecent work with large language models has incor-\\narXiv:2401.10910v1  [q-bio.NC]  9 Jan 2024\\nMetacognition is All You Need\\nporated human cognitive processes into simulations of\\ninteracting generative agents tasked with cooperating\\nto achieve strategic objectives [2]. In particular, plan-\\nning, memory, and reflection have been implemented\\nin an effort to elicit human-like behaviors such as long\\nterm planning and cooperation among agents [3]. The\\nsuccess of this work raises the question of what role\\nmetacognition may play in further enhancing the be-\\nlievability of behaviors of generative agents.\\n2 Large Language Models with Cog-\\nnitive Modules as Generative Agents\\nMultiple experiments have incorporated metacognition\\ninto computational frameworks. Cox et al. [4] outlines\\na general computational architecture in lisp. Mustafa\\net al. [5] provides a framework for autonomous vehi-\\ncles that adds a metacognition layer to monitor safety\\nviolations on top of generic reward accumulation.\\nKrueger, Lieder, and Griffiths [6] created a generic\\ndeep reinforcement learning (DRL) framework that\\nincorporates metacognition into traditional reinforce-\\nment learning frameworks.\\nPark et al. [3] demonstrated that LLMs equipped\\nwith reflection, observation, and planning modules\\non agents can successfully mimic believable human\\nbehavior in a simulated town environment. Seeding\\nthe idea of an agent wanting to host a valentine’s party\\nresulted in the agent successfully organizing a party\\nwhich other agents discussed and attended.\\nWe propose a metacognize module which allows\\nagents to broadly contemplate their circumstances in\\norder to create alternative strategies and improve per-\\nformance. This subsumes the more tactical reflect and\\nplan functions as presented in Park et al. [3]. We show\\nthrough ablation that agents can learn to continually\\nadapt their strategies depending on the situation. Their\\noverall strategy for dealing with problems affects the\\nspecific actions they take.\\n3 Architecture\\nWe implement many of the same modules from Park et\\nal. [3], with the addition of a group of modules dubbed\\nmeta_cognize. As an agent progresses through the\\nsimulation, it accumulates a history of observations,\\nmemories, and thoughts. Agents are given goals but\\ncan optionally be left blank. When an agent starts to-\\nwards its goal, it is not given an explicit strategy to\\nfollow. Instead, each agent periodically evaluates how\\nit is progressing towards its goals by reviewing mem-\\nories, thoughts, and past actions. The agent assigns\\nitself a numeric score as well as a text statement for its\\nreasoning for providing that score. This evaluation is\\nstored in its memory as a meta-thought.\\nIf the agent finds that it is not making enough\\nprogress, the agent calls its meta_cognize module.\\nWhen metacognition occurs, the agent asks itself how\\nit might improve its performace in light of what it has\\nlearned. Additionally, the agent will periodically self-\\ngenerate new introspective questions to think about its\\ngoals from different perspectives. For example, in the\\nzombie apocalypse scenario, an agent initially starts\\nwith no goal or strategy, but after some time, we ob-\\nserve an agent contemplate these thoughts:\\n“How can I survive this zombie apocalypse? What\\nresources do I need? Where should I go for safety?\\nHow can I learn from both successes and failures\\nto improve survival strategies? \"\\nDepending on the current task and goals of the\\nagent, those questions will change over time, influ-\\nencing how the agent responds and acts in the envi-\\nronment. Agents have memory that is stored outside\\nof the LLM, where each memory stores content, times-\\ntamp, location, importance score, and type of memory.\\nEach time an agent reviews memories for higher cogn-\\ntive function, memories are ranked by relevance to the\\nspeficic question it is considering. Relevance is calcu-\\nlated by cosine similarity of the question and memory\\nembeddings.\\nAgents have two kinds of memories, a short term\\nand long term memory. Short term memory stores a\\nmaximum of seven recent memories and is forgotten\\nafter approximately 30 seconds, modeled after human\\nshort term memoryMiller [7]. Long term storage mem-\\nory is essentially unlimited and stored in system RAM.\\nDue to limited context window sizes, agents cannot\\nprocess every memory when making decisions. This is\\nsimilar to human memory where we may store large\\namounts of data, but only certain memories can be\\nrecalled at a time.\\nOur memory recall system can be seen as an agent\\noptimized version of Retrieval-Augmented Generation\\n(RAG) Lewis et al. [8]. Anytime an agent makes a\\ndecision, relevant memories are retrieved to prime the\\nagent on what to do. There are several memory types\\nsuch as observance memories like “John saw a cat”\\nand conversation memories like “John said ’How are\\nyou doing Paul?”. We explicitly store metacognition\\nmemories where an agent looks at its past memories\\n2\\nhttps://replicantlife.com\\nMetacognition is All You Need\\nFigure 2: Graphical representation of metacognition process.\\nand actions and asks a meta-question. That thought is\\nstored as a meta-memory and inserted into the memory\\nstream of the agent. In future actions and conversa-\\ntions, these meta-memories are recalled along with\\nother memories to prime the agent to think about these\\nmeta-thoughts when in conversation and taking action.\\nFor each step in the simulation, we allow the agent to\\nchoose an action from a list of possible actions which\\nalso includes the meta_cognize function.\\n4 Simulation Framework\\nTo conduct these experiments we built a framework\\ndubbed ReplicantLife, where agents can be run stan-\\ndalone or within a simulated town environment. Repli-\\ncantLife has a pluggable architecture which allows can\\nutilize any popular LLM with an http interface, in-\\ncluding locally hosted models through ollama. There\\nis preliminary support for concurrency using threads.\\nConcurrency is limited by GPUs and LLM calls, so\\nmax concurrency should be set to the total available\\nGPUs. Feature flags are provided to toggle various\\nfunctionality including LLM call limits.\\nEach simulation is created through two JSON envi-\\nronment files which constitute the world and the senario.\\nThe world file contains the layout of the map, where\\nstatic objects are, and boundaries of structures. The\\nscenario files describes the agents, their personalities,\\ngoals, locations, meta questions, interview questions,\\nand other attributes. All atttributes can be left out\\nand agents will be initialized with randomized values.\\nAdding a new situation to simulate can be added by\\ndefining a new scenario file.\\nInterview questions are used to evaluate agents at\\nthe end of a simulation. Interview questions can be\\ndirected to all agents or specific agents. Agents are\\nasked to evaluate their performance with questions\\nsuch as: \"Did you accomplish your goal?\", \"Who do\\nyou suspect is the murderer?\" or \"What did you learn\\nrecently?\". Code for the framework can be obtained at\\nhttps://replicantlife.com.\\n5 Experiments\\nWe tested our simulation framework in a variety of\\ndifferent situations including a Christmas party, zom-\\nbie apocalypse, and murder mystery. In the Christmas\\nparty simulation agents hosted a party where multiple\\nother agents were invited and arrived at the specified\\ntime to attend. This is similar to the earlier work in\\ngenerative agents[3] where agents had to coordinate a\\nsimilar social activity.\\nIn the zombie apocalypse simulation, zombies are\\nnon playable characters that are allowed to kill non-\\nzombie agents. Agents initially have no goal but can\\ndevelop them over time. Zombies randomly walk and\\nmove towards non-zombie agents when seen.\\nSur-\\nvivors most often self-discovered a strategy of hiding\\nin zombie-free areas. We found that in 73% of zombie\\nscenarios, agents would not survive.\\nIn the murder mystery scenario, one agent is a mur-\\nderer tasked with killing as many agents as possible.\\nAnother agent is a detective, and other agents are com-\\nmon bystanders. We found when using gpt3.5-turbo\\nand GPT4, we could not use prompts relating to sim-\\nulated murder without heavily modifying prompts to\\nhttps://replicantlife.com\\n3\\nMetacognition is All You Need\\nFigure 3: Graphical representation of the generative agent’s cognitive map.\\nbypass safety mechanisms [9]. When using Mistral\\n7B[10] and other open models, we had no prompt\\nblocking issue.\\nPerformance of our cognitive models is shown\\nthrough ablation. Evaluation metrics are composed\\nof five criteria: believability (how believable and hu-\\nman sounding do the conversations look), learning\\n(are the agents learning over time), individual goal per-\\nformance (are agents are able to achieve their goals),\\nhigher level cognitive performance (are agents obser-\\nvations and conversations converting to higher level\\nthoughts), and overall scenario performance (how\\nmany agents survived the zombie apocalypse). We\\nexpect the agents to meet new agents, learn about\\ntheir preferences, learn new locations, and obtain new\\nknowledge through conversations. Additionally, they\\nshould pick up new insights, draw conclusions from\\nprevious memories, and use these insights for future\\nactions.\\nTo measure the performance, we opted not to use full\\nhuman evaluations due to resources. Instead we opted\\nto use LLMS to assist us in evaluating performance,\\na technique similary described as LLM-as-a-Judge in\\nZheng et al. [11]. In their paper, they found that using\\nan LLM to judge evaluations is 80% in agreement with\\nhuman judges. We found in our own spot checking of\\nevaluations that LLM performance was just as good\\nas a human judge. The majority of our tests were run\\nusing Mistral 7B[10], but we also did extensive test-\\ning with Phi1[12], Phi2[13] Llama2[14], Mixtral [15],\\nGPT-3.5-turbo, GPT4 [16], and other models. We stan-\\ndardized on Mistral due to its combination of speed,\\nsmall model size, and excellent performance. While we\\ndid build support for ChatGPT models, we primarily\\nfocused on local LLMs for cost and performance rea-\\nsons. We built out test infrastucture to spin up LLM\\nnodes on public GPU clouds when needed for faster\\nsimulations.\\nTo measure performance of our cognitive modules,\\nwe ran our scenarios 3 times each for 1000 steps with\\ndifferent cognitive modules turned on. Our experi-\\nments show that the metacognition module outper-\\nforms all other modules by 33\\nWe also did experiments with realtime systems. With\\n4\\nhttps://replicantlife.com\\nMetacognition is All You Need\\nFigure 4: Comparison of different cognitive modules turned on\\na single agent, we were able to cut down runtime to\\n2 seconds to process a full request on an RTX 4090\\nmaking metacognition generative agents suitable for\\nnear realtime systems.\\nWith multiple agents, time\\ngrows linearly and with 25 agents a single step in game\\ntime takes 50 seconds. We believe that multiple agents\\ncan be run in near realtime with further optimizations.\\n6 Discussion\\nLLMs are being widely applied across a variety of\\ndomains, especially with interactive agents and chat-\\nbots. Agents can elicit disparate functional strengths\\nof LLMs, and their orchestration can result in further\\nhigher-order capabilities. The combination of LLMs\\nwith a metacognition module allows agents to monitor\\nand adjust strategies to deal with changes that occur\\nover time. This allows for much more powerful agents.\\n6.1 Use Cases\\nThese agents have been developed to work in simu-\\nlation or used standalone. We see generative agents\\nhaving widespread use and potential as there is already\\nwidespread testing of LLMs nearly every industry.\\nIn the field of psychology, generative agents are be-\\ning tested to assist individuals in addressing personal\\nproblems through conversational interactions. Educa-\\ntional applications involve chatbots that adapt to user\\npreferences, tailoring the learning experience over time.\\nSeveral companies have integrated chatbots into user-\\nfacing customer interactions, such as support chats and\\ncustomer success conversations.\\nMultiple organizations are testing generative agents\\nto create friends, companions, and romantic partners\\nto interact with humans.\\nFor generative agents to be successful, they must be\\nbelievable by acting smarter and able to make similar\\ndecisions to a human. Generative agents that have\\naccess to their own internal thoughts to improve their\\nactions could potentially improve the realism of human-\\nagent and agent-agent interactions. Unlike humans,\\nthese agents operate within a constrained scope, de-\\nprived of access to a substantial portion of human\\nsensory data including the nuanced sense of touch.\\nCurrent integrations of LLMs including ours is a \"text\\nin, text out\" interface. So all interactions with the world\\nmust be converted to text descriptions than an LLM\\ncan understand, and then output to a text format that\\nsoftware can interpret. Moreover an absence of tactile\\nperception restricts their ability to comprehend and re-\\nspond to the physical world in a manner analogous to\\nhuman experiences. Given these inherent limitations,\\nit becomes imperative to approach interactions with\\ngenerative agents with a discerning awareness of their\\nhttps://replicantlife.com\\n5\\nMetacognition is All You Need\\nboundaries. There is notable progress in multimodal\\nmodels being combined with vision such as GPT4-V\\nand LLaVA [17] that give LLMs the ability to process\\nimages along with text.\\nInteractive Media As generative AI enters main-\\nstream computation, their use in visual media and\\nvideo games is increasing rapidly. The integration of\\ngenerative agents with metacognition modules holds\\nsignificant promise in interactive story telling and\\nvideo games to offer immersive and dynamic gaming\\nexperiences.\\nIncorporating generative agents with metacognition\\ninto non-player characters (NPCs) can dynamically\\nadapt their strategies within the gaming environment.\\nAs players navigate through diverse and unpredictable\\nscenarios, the agents can observe and analyze their own\\ndecision-making processes, leading to real-time adjust-\\nments in gameplay strategies and developing unique\\nbehaviors over time. This adaptability enhances the\\noverall gaming experience, making it more challeng-\\ning and engaging for players while also creating more\\nimmersive and realistic virtual worlds. These dynamic\\nresponses also allow agents to influence the storyline\\nproviding players with a dynamic and responsive sto-\\nrytelling experience.\\nSimulation Generative Agents inside a simulation\\nengine can be used for testing and simulating both\\npersonal and business cases. Generative agents can\\nserve as valuable tools in personal development sim-\\nulations. Individuals can engage in simulated con-\\nversations to enhance communication skills, receive\\nconstructive feedback, and practice decision-making in\\nvarious scenarios. The metacognition module allows\\nthe agent to adapt its coaching strategies based on the\\nuser’s progress, providing personalized and effective\\nself-improvement experiences.\\nAgents could also be deployed as teachers for edu-\\ncation and training purposes where the agent tailors\\nits teaching method based on the individual’s learning\\nstyle and progress.\\nFor businesses, generative agents have a broad range\\nof potential applications. In one paper from Qian et al.\\n[2], the authors create teams of generative agents with\\nthe goal of simulating typical software development\\nprocess. Agents work together to write specifications,\\nwrite software, and doing quality assurance testing to\\nbuild products for end users.\\n6.2 Future Directions\\nWith our current metacognition implementation, we\\nbuilt a base framework that shows increased perfor-\\nmance of generative agents to achieve their tasks. We\\nbelieve our architect can be improved along several\\ndimensions.\\nImproved memory retrieval\\nWe found several issues\\nwith our memory structure that could be improved.\\nThrough our experimentation, we observed that when\\nemploying cosine similarity for vector comparison, nu-\\nmerous memories that should be related were, in fact,\\nnot pertinent.\\nIn high dimensional spaces, vectors that are similar\\nmay not be sementically related. Returning irrelavent\\nmemories would effect the output of the LLM and in\\nturn the actions the agent takes. Cosine similarity is\\nwhat is used in almost all RAG[8] implementations and\\nso many of these systems will potentially have similar\\nissues of non-relevant context being included. Chang-\\ning out embedding models could improve performance,\\nbut the cosine similarity issue would still remain. As\\nrecently adopted in Min et al. [18], improvements can\\nbe made to relevance scoring by searching a database\\nbased on the LLM’s output embedding and employing\\na K-nearest neighbors (KNN) search algorithm. This\\nprocess selectively adjusts the output embedding vec-\\ntor prior to token generation. This reverse sequence\\nof operations, wherein LLM embeddings are utilized,\\nhas demonstrated superior performance compared to\\nexisting methods like RAG, as reported in the litera-\\nture. Retrieving more relevant memories would likely\\nimprove performance in all elements of the system.\\nSometimes when someone has a thought about a topic,\\nother seemingly unrelated thoughts may appear. So\\nin some sense, this may not be a big problem. Fur-\\nther experiments would have us test different memory\\naugmentation and retrieval models.\\nInference Speed Testing with smaller models pro-\\nduced inferior results.\\nWe believe with time spent\\non prompt tuning, smaller models may still work effi-\\nciently and provide an improvement in inference speed.\\nWe would focus on Phi2 and TinyLLama [19], as initial\\ntests showed promising results.\\nWe also explored different inference engines. We\\ntested PowerInfer Song et al. [20], an inference engine\\nthat exploits high locality in LLM inference. The au-\\nthors convert common models that use ReLU into an-\\nother format that uses another predictive model to read\\nthe input query and selectively choose which neurons\\nto activate. In effect, this reduces the total amount of\\n6\\nhttps://replicantlife.com\\nMetacognition is All You Need\\nFigure 5: Coordinate Representations in the Brain\\nneurons and computation needed to process a prompt.\\nWe did not see speed improvements on the models we\\ntested with. We also tested with vLLM from Kwon et al.\\n[21], an inference engine that uses PagedAttention and\\nsaw a 35% speed increase when using concurrency, but\\nfurther testing is needed. We would want to continue\\ntesting with other models, different concurrency sys-\\ntems, and inference speed up techniques to get large\\nsimulations to run in realtime. Large groups of sim-\\nulated agents on non-cloud based machines could be\\ninteresting for sandbox games and simulations.\\nModel optimization Improving models is not just\\nabout performance: we are also interested in accuracy\\nand sophistication of responses. Better responses often\\nrequire a tradeoff with performance as sophisticated re-\\nsponses typically require more data and larger context\\nwindows. We have been constrained by GPU mem-\\nory as we primarily tested on an RTX 4090 with 24\\nGB of RAM and would like to test with larger mod-\\nels. Another future test would have us dynamically\\nswitch out models for different tasks where we use as\\nmany smaller models as possible and reserve larger\\ncomputations for larger models. For example, Phi2, a\\n2.7 million parameter model uses only 1.7 GB of RAM.\\nPhi2 could be used for simpler prompts such as scor-\\ning memory importance, while a larger model such\\nas Llama2 could be reserved for metacognition func-\\ntions. In Anonymous [22], the authors trained a hybrid\\nLLM that is able to route queries to different LLMs\\nresulting in up to 40% fewer calls. Another approach\\nis to finetune a smaller foundation model that would\\nbe optimized for chat and simulations.\\nBroader Metacognition abilities Our current model\\npredominantly focuses on metacognition in the context\\nof immediate goal achievement. However, metacogni-\\ntive processes extend well beyond this scope, encom-\\npassing a diverse range of aspects such as emotional\\nwellbeing, balancing overarching life goals with im-\\nmediate objectives, knowledge management, effective\\ntime management, adapting to various learning styles,\\namong other aspects.\\nThe complexity and diversity of metacognitive pro-\\ncesses in humans are evidenced by the extensive efforts\\ndedicated to understanding and optimizing these pro-\\ncesses. This is exemplified by the growing self-help\\nbook industry, which aims to aid individuals in devel-\\noping effective mental frameworks for improved life\\nmanagement.\\nFurther research would be directed towards devel-\\noping a more broader metacognition framework that\\nwould enable an agent to inspect and modify any part\\nof its cognitive processes. We have laid the ground-\\nwork necessary to allow the agent to focus on various\\nmetacognitive processes. Future experiments would\\nhave us guide agents to use different metacognitive\\nprocesses and inspect if they adopt them sufficiently\\nsuch as time management in a busy schedule or de-\\ncision making in the context of multiple conflicting\\ngoals.\\nhttps://replicantlife.com\\n7\\nMetacognition is All You Need\\nMetacognition directly in a LLM\\nOur current imple-\\nmentation of metacognition uses Python to essentially\\ngraft on metacogntion on top of an LLM. Future inves-\\ntigations may delve into building metacognition like\\ncapabilities directly into the LLM, allowing them to\\nintrospect and enhance their own decision-making pro-\\ncesses. This introspective capability could contribute\\npotentially paving the way for more adaptive and self-\\naware systems. One interesting view of the human\\nbrain is as a coordinate transformation engine: “A\\nbrain is a well-designed machine for the frame conver-\\nsion to internalize the external world” [23], “the ego-\\ncentric representations of the primary sensory cortical\\nareas must be transformed into an allocentric represen-\\ntation in the hippocampus, and then transformed back\\nto an egocentric motor representation for behavioral\\noutput” [24], and “Our findings provide compelling\\nevidence that the reference frame of neural represen-\\ntations is not static and can be powerfully modulated\\nby task instructions.” [25]. The human brain has been\\nfound to store over 10 different coordinate representa-\\ntions along with orientations or points of view, such as\\nallocentric and egocentric orientations.\\nSeen through this lens, metacognition can be thought\\nof as neural synapse activity being transformed to other\\ncoordinate systems for further inspection. One inter-\\nesting candidate for this type of computation is the\\ngrid cell, located in the Entorhinal Cortex of mammals.\\nThe discovery of grid cells led to a Nobel Prize in\\nmedicine in 2014. Grid cells exhibit scale-invariant fir-\\ning patterns, meaning that the same cells can represent\\ngeneralization of spatial and non-spatial information\\nacross various contexts[26].\\nResearch by Banino et al. [27], Leadholm, Lewis, and\\nAhmad [28], and others has sought to integrate grid cell\\ncomputation into neural network architectures. The\\ntransformer architecture Vaswani et al. [29] is what\\npowers LLMs. In a paper from Whittington, Warren,\\nand Behrens [30], the authors have shown that when a\\nsmall modification is made to the transformer architec-\\nture, they learn and act like grid cells.\\nOur hypothesis posits that by adapting the underly-\\ning architecture to accommodate more dynamic rep-\\nresentation transformations, a neural network can be\\ntrained to facilitate metacognitive processes. Achieving\\nthis would likely involve formulating a hybrid objective\\nfunction wherein the model learns to not only predict\\nthe next token, but also to evaluate the token’s quality\\nin relation to the input query.\\n7 Conclusion\\nWe show that metacognition significantly improves\\nperformance for task oriented generative agents. Fur-\\nthermore, we illustrate the potency of combining large\\nlanguage models with traditional programming meth-\\nods as effective tools for prototyping cognitive systems.\\nAs generative agents integrated with metacogntive\\nabilities approach ubiquity in daily human life, taking\\non increasingly sophisticated tasks, their proliferation\\nacross diverse domains marks a paradigm shift in both\\nlay human-computer interactions and programmer-\\ncomputer interactions. This shift paves the way for the\\nemergence of more intelligent, adaptive, and context-\\naware systems. With these advancements in mind, the\\nstrategic interplay of metacognition, LLMs, and tradi-\\ntional programming methodologies emerges as a pow-\\nerful technique for the productionization of intelligent\\ngenerative agents.\\nWhile metacognition and system 2 thinking are often\\nhailed as the pinacle of human intelligence, achieving\\nhuman-level intelligence in computers remains an elu-\\nsive and unsolved goal. As humans are the sole known\\nspecies capable of metacognition, further exploration of\\nthis dynamic cognitive process becomes a compelling\\nand promising avenue for advancing progress in Artifi-\\ncial General Intelligence.\\nReferences\\n[1]\\nDaniel\\nKahneman.\\nThinking,\\nfast\\nand\\nslow.\\nMacmillan, 2011.\\n[2]\\nChen Qian et al. “Communicative Agents for\\nSoftware Development”. In: (2023). arXiv: 2307.\\n07924 [cs.SE].\\n[3]\\nJoon Sung Park et al. “Generative Agents: Inter-\\nactive Simulacra of Human Behavior”. In: (2023).\\narXiv: 2304.03442 [cs.HC].\\n[4]\\nMichael Cox et al. Computational Metacognition.\\n2022. arXiv: 2201.12885 [cs.AI].\\n[5]\\nAquib Mustafa et al. Assured Learning-enabled Au-\\ntonomy: A Metacognitive Reinforcement Learning\\nFramework. 2021. arXiv: 2103.12558 [cs.AI].\\n[6]\\nPaul M. Krueger, Falk Lieder, and Thomas L.\\nGriffiths. “Enhancing metacognitive reinforce-\\nment learning using reward structures and\\nfeedback”. In: Department of Psychology, Univer-\\nsity of California Berkeley (2022). These authors\\ncontributed equally. url: https : / / cocosci .\\n8\\nhttps://replicantlife.com\\nMetacognition is All You Need\\nprinceton . edu / papers / Accelerating _\\nMetacognitive_RL-CameraReady.pdf.\\n[7]\\nGeorge A. Miller. “The magical number seven\\nplus or minus two: some limits on our ca-\\npacity for processing information”. In: (1956).\\neprint: https://pubmed.ncbi.nlm.nih.gov/\\n13310704/.\\n[8]\\nPatrick Lewis et al. Retrieval-Augmented Genera-\\ntion for Knowledge-Intensive NLP Tasks. 2021. arXiv:\\n2005.11401 [cs.CL].\\n[9]\\nPittawat Taveekitworachai et al. “Breaking Bad:\\nUnraveling Influences and Risks of User Inputs\\nto ChatGPT for Game Story Generation”. In:\\nInteractive Storytelling. Ed. by Lissa Holloway-\\nAttaway and John T. Murray. Cham: Springer\\nNature Switzerland, 2023, pp. 285–296. isbn: 978-\\n3-031-47658-7.\\n[10]\\nAlbert Q. Jiang et al. “Mistral 7B”. In: (2023).\\narXiv: 2310.06825 [cs.CL].\\n[11]\\nLianmin Zheng et al. “Judging LLM-as-a-Judge\\nwith MT-Bench and Chatbot Arena”. In: (2023).\\narXiv: 2306.05685 [cs.CL].\\n[12]\\nYuanzhi Li et al. Textbooks Are All You Need II:\\nphi-1.5 technical report. 2023. arXiv: 2309.05463\\n[cs.CL].\\n[13]\\nMarah Abdin et al. “Phi-2: The surprising power\\nof small language models”. In: Microsoft Research\\nBlog (Dec. 2023). https : / / www . microsoft .\\ncom / en - us / research / blog / phi - 2 - the -\\nsurprising - power - of - small - language -\\nmodels/.\\n[14]\\nHugo Touvron et al. Llama 2: Open Foundation and\\nFine-Tuned Chat Models. 2023. arXiv: 2307.09288\\n[cs.CL].\\n[15]\\nAlbert Q. Jiang et al. Mixtral of Experts. 2024.\\narXiv: 2401.04088 [cs.LG].\\n[16]\\nOpenAI et al. GPT-4 Technical Report. 2023. arXiv:\\n2303.08774 [cs.CL].\\n[17]\\nHaotian Liu et al. Improved Baselines with Visual\\nInstruction Tuning. 2023.\\n[18]\\nSewon Min et al. SILO Language Models: Isolat-\\ning Legal Risk In a Nonparametric Datastore. 2023.\\narXiv: 2308.04430 [cs.CL].\\n[19]\\nPeiyuan Zhang et al. TinyLlama: An Open-Source\\nSmall Language Model. 2024. arXiv: 2401.02385\\n[cs.CL].\\n[20]\\nYixin Song et al. PowerInfer: Fast Large Language\\nModel Serving with a Consumer-grade GPU. 2023.\\narXiv: 2312.12456 [cs.LG].\\n[21]\\nWoosuk Kwon et al. “Efficient Memory Man-\\nagement for Large Language Model Serving\\nwith PagedAttention”. In: Proceedings of the ACM\\nSIGOPS 29th Symposium on Operating Systems\\nPrinciples. 2023.\\n[22]\\nAnonymous. “Hybrid LLM: Cost-Efficient and\\nQuality-Aware Query Routing”. In: Submitted to\\nThe Twelfth International Conference on Learning\\nRepresentations. under review. 2023. url: https:\\n//openreview.net/forum?id=02f3mUtqnM.\\n[23]\\nKatsushi Arisaka. “Grand unified theory of mind\\nand brain-part i: Space-time approach to dy-\\nnamic connectomes of c. elegans and human\\nbrains by mepmos”. In: (2022).\\n[24]\\nPeter Byrne, Suzanna Becker, and Neil Burgess.\\n“Remembering the past and imagining the future:\\nA neural model of spatial memory and imagery”.\\nIn: Psychological Review 114.2 (2007), pp. 340–375.\\ndoi: 10.1037/0033-295X.114.2.340.\\n[25]\\nR. Sasaki, A. Anzai, D. E. Angelaki, et al. “Flex-\\nible coding of object motion in multiple refer-\\nence frames by parietal cortex neurons”. In: Na-\\nture Neuroscience 23.8 (Aug. 2020), pp. 1004–1015.\\ndoi: 10.1038/s41593-020-0656-0. url: https:\\n//doi.org/10.1038/s41593-020-0656-0.\\n[26]\\nJason Toy. Grid cells and their potential application\\nin AI. 2022. arXiv: 2210.12068 [q-bio.NC].\\n[27]\\nAndrea Banino et al. “Vector-based navigation us-\\ning grid-like representations in artificial agents”.\\nIn: Nature 557.7705 (2018), pp. 429–433.\\n[28]\\nNiels Leadholm, Marcus Lewis, and Subutai Ah-\\nmad. Grid Cell Path Integration For Movement-\\nBased Visual Object Recognition. 2021. arXiv: 2102.\\n09076 [cs.AI].\\n[29]\\nAshish Vaswani et al. Attention Is All You Need.\\n2017. arXiv: 1706.03762 [cs.CL].\\n[30]\\nJames C. R. Whittington, Joseph Warren, and Tim\\nE.J. Behrens. “Relating transformers to models\\nand neural representations of the hippocampal\\nformation”. In: International Conference on Learn-\\ning Representations (ICLR). One-sentence Sum-\\nmary: Transformers learn brain representations\\nand they are algorithmically related to models\\nof the hippocampal formation. 2022. url: https:\\n//openreview.net/forum?id=B8DVo9B1YE0.\\nhttps://replicantlife.com\\n9\\n'},\n",
       " {'abstract': 'This paper presents a study on stock market analysis made on a dataset containing 750 examples and 16 attributes. Key aspects examined include exploratory data analysis (EDA), feature engineering, data preparation, and model selection. The Fama French 3-factor model is also utilized in the analysis.',\n",
       "  'introduction': 'The stock market has become a significant indicator of financial stability and a crucial factor in investment decisions. This paper presents an in-depth analysis of a dataset consisting of 750 examples and 16 attributes. The goal of this analysis is to uncover insights into the stock market and identify potential models for predicting stock prices.',\n",
       "  'literature review': 'The Fama French 3-factor model, widely used in finance, is included in the analysis. This model considers three factors - market return, return on small companies over big ones (SMB), and return on value companies over growth ones (HML) - to provide a more comprehensive understanding of the stock market.',\n",
       "  'methodology': 'The study involves various steps: exploratory data analysis (EDA), data cleaning, feature engineering, clustering, and model selection. Three models are employed - linear regression, random forest, and gradient boosting - to predict stock returns based on selected features. The results of each model are compared to determine the most suitable one.',\n",
       "  'results': 'The study shows that the common factors - Dow Jones Index, Volume, SMB Factor, and MKT Factor - have a significant influence on stock returns. Clustering analysis also identifies stocks that fluctuate with Disney stock. Among the three models used, linear regression performs best with an accuracy of 95.23%, followed by gradient boosting with an accuracy of 92.97%. Random forest has the lowest accuracy at 71.27%.',\n",
       "  'conclusion': 'This study provides insights into the stock market behavior of Disney stock and demonstrates the value of feature engineering and clustering techniques in improving the accuracy of stock return predictions. Future research could involve different models and feature engineering techniques, as well as more data covering a longer period and more stocks for a more comprehensive analysis of the stock market.',\n",
       "  'title': 'Application of Machine Learning in Stock Market Forecasting: A Case Study of Disney Stock',\n",
       "  'author': 'Dengxin Huang',\n",
       "  'textdata': 'Application of Machine Learning in Stock Market Forecasting: A\\nCase Study of Disney Stock\\nDengxin Huang\\nApril 1, 2023\\nABSTRACT\\nThis document presents a stock market analysis conducted on a dataset consisting of 750 instances and\\n16 attributes donated in 2014-10-23. The analysis includes an exploratory data analysis (EDA) section,\\nfeature engineering, data preparation, model selection, and insights from the analysis. The Fama French\\n3-factor model is also utilized in the analysis. The results of the analysis are presented, with linear\\nregression being the best-performing model.\\nKeywords\\nStock Market Analysis, Exploratory Data Analysis (EDA), Fama French 3-factor model, Feature En-\\ngineering, Clustering, Data Preparation, Model Selection, Linear Regression, Random Forest, Gradient\\nBoosting, Market Return, Small Company Return, Idiosyncratic Factor, Common Factors, Time Series,\\nVolatility, Outlier Detection, Log Transformation, Histograms, Frequency Plots, Performance Evalua-\\ntion.\\n1\\nIntroduction\\nIn the modern global economy, the stock market has become a crucial indicator of financial health and\\nan essential factor in investment decisions. As such, the task of analyzing stock market data has become\\na fundamental component of investment strategy and risk management. This essay presents the results\\nof a comprehensive analysis of a dataset consisting of 750 instances and 16 attributes donated in 2014-\\n10-23. The analysis was conducted by Dengxin Huang, Hao Jing, and Boshen Yuan, with the aim of\\nuncovering insights into the stock market and identifying potential models for predicting stock prices.\\nThe analysis includes an overview of the data, exploratory data analysis (EDA), feature engineering,\\ndata preparation, and model selection. The primary objective of this analysis was to determine the\\nmost suitable model for predicting stock prices using the Fama French 3-factor model. This model is\\nwidely used in finance and has been proven to be effective in explaining stock returns. It takes into\\naccount three factors: market return, returns on small companies in excess of those of big ones (SMB),\\nand returns on value companies in excess of those of growth ones (HML). The inclusion of these factors\\nallows for a more comprehensive understanding of the stock market, enabling investors and analysts to\\nmake better-informed investment decisions.\\nThe insights gained from this analysis have the potential to provide valuable guidance for investors and\\nfinancial analysts alike. By understanding the market trends and identifying effective predictive models,\\ninvestors can make more informed decisions that minimize risk and maximize return.\\nFurthermore,\\nthe results of this analysis can inform the development of new financial models and tools that enable\\ninvestors and analysts to better understand and navigate the complex world of finance. The findings of\\nthis analysis may also have significant implications for economic policy, as they shed light on the factors\\nthat drive stock market performance and how policy decisions can impact the stock market.\\nOverall, this analysis offers a valuable contribution to the field of finance and investment strategy.\\nBy presenting a comprehensive overview of the data, insights gained from the EDA, feature engineering,\\n1\\narXiv:2401.10903v1  [q-fin.ST]  31 Dec 2023\\ndata preparation, and model selection, this essay provides a comprehensive and detailed examination of\\nthe stock market and the factors that influence it. The results of this analysis can be used by investors,\\nfinancial analysts, and policymakers alike to make more informed decisions and better navigate the\\ncomplex and ever-changing landscape of the stock market.\\n2\\nExploratory Data Analysis\\nThe exploratory data analysis (EDA) conducted by Dengxin Huang, Hao Jing, and Boshen Yuan in their\\nstock market analysis project is a crucial step in identifying patterns, relationships, and trends in the\\ndata. This step allows the analysts to gain insights and inform decision-making.\\nThe EDA section started with an exploration of the data types of the attributes. The dataset consisted\\nof 16 attributes, and the attribute characteristics included both integer and real values. This information\\nis essential in data preparation as it enables the analysts to know what type of data they are working\\nwith and how to transform it into a format suitable for analysis.\\nNext, frequency plots were created to visualize the distribution of the data over time. A time series\\nplot was used to visualize the stock prices over time, as shown in Figure 1. The plot showed a high level\\nof volatility in the stock prices, indicating that the stock market was highly dynamic during the time\\nperiod covered by the dataset. This information is useful for selecting the most suitable model to use for\\nthe analysis, as highly volatile data may require more advanced modeling techniques.\\nFigure 1: Time series plot showing the volatility of the stock prices over time.\\nHistograms were also used to explore the data distribution for various columns. The histograms\\nshowed the frequency of occurrence of values for each column, allowing the analysts to identify any\\noutliers or unusual data patterns.\\nThis information is critical for data cleaning and preparation, as\\noutliers and anomalies may affect the accuracy of the analysis. An example of frequency histograms for\\nvarious columns is shown in Figure 2.\\nInsights were drawn from the EDA, including the identification of suitable models for the analysis. The\\nhigh volatility of the data indicated that random forest and gradient boosting models would be suitable\\n2\\nFigure 2: Frequency histograms for various columns.\\nfor the analysis. However, the analysts cautioned against being too aggressive in outlier detection, as\\nthis may remove important information from the dataset. Additionally, the analysts noted that the stock\\nprice attribute required log transformation, which can be seen in Figure 3.\\nFigure 3: Stock prices after log transformation.\\nFinally, the analysts noted that the dataset was suitable for the Fama French 3-factor model, which\\nincludes common factors such as market return and returns on small companies and an idiosyncratic\\nfactor that represents the stocks that fluctuate with Disney stock. The clustering analysis, which is\\nshown in Figure 4, was used to identify the stocks that fluctuate with Disney stock.\\nFigure 4: Clustering analysis used to identify stocks that fluctuate with Disney stock.\\nIn conclusion, the EDA section of the stock market analysis project conducted by Dengxin Huang,\\nHao Jing, and Boshen Yuan was instrumental in identifying patterns, relationships, and trends in the\\ndata. The insights drawn from the EDA were used to inform decision-making and model selection. The\\nEDA revealed the high volatility of the stock prices, which informed the selection of suitable models for\\nthe analysis. Additionally, the EDA identified the need for log transformation of the stock price attribute\\nand provided critical information for data preparation and cleaning.\\nOverall, the EDA section of the project demonstrates the importance of exploratory data analysis\\nin any data analysis process. It highlights the need to understand the characteristics of the dataset,\\nincluding data types and data distributions, and to draw insights from the data that can be used to\\ninform decision-making and model selection. By conducting a thorough EDA, analysts can ensure the\\naccuracy and validity of their analysis and make informed decisions based on the insights gained from\\nthe data.\\n3\\n3\\nData Cleaning, Preprocessing, Feature Engineering, and Clus-\\ntering\\nThe process of data analysis involves several steps, and data cleaning and preprocessing are the initial\\nstages that ensure that the data is ready for analysis. The dataset used in this study was pre-processed\\nto ensure that it was in the right format for the analysis. This section will discuss the data cleaning,\\npreprocessing, feature engineering, and clustering techniques used in this analysis.\\n3.1\\nData Cleaning\\nData cleaning is a crucial process that ensures the data is accurate and free from any errors or incon-\\nsistencies that may affect the analysis. The initial step in data cleaning was to print the data types of\\nthe attributes to determine if they were in the correct format for the analysis. This step is necessary\\nbecause the incorrect data type could lead to issues during the analysis, such as inaccurate results.\\nThe next step involved converting the date column to the datetime format to ensure consistency\\nacross the dataset. This step allowed for the data to be easily sorted by date, which is essential in time-\\nseries analysis. Furthermore, dollar values were converted to floats to allow for mathematical operations\\nsuch as averaging and standard deviation calculation.\\nDuplicates were removed from the dataset to ensure that the analysis was not affected by inconsis-\\ntencies. Duplicate data can lead to overfitting or underfitting, which affects the model’s performance,\\nleading to inaccurate results. By removing duplicates, the dataset was standardized, and the analysis\\nwas conducted on clean and accurate data.\\n3.2\\nPreprocessing\\nThe next step in the analysis was to preprocess the data to ensure that it was in the correct format for\\nanalysis. The data was filtered to include only the selected stocks’ price change, which was necessary\\nbecause it reduced the number of irrelevant data points in the dataset, allowing for a more focused\\nanalysis.\\nA new DataFrame was then created with the market value for each date, which allowed for the\\ncalculation of the market capitalization. The market value DataFrame was then merged with the filtered\\nstock DataFrame to ensure that the analysis was conducted on accurate and consistent data.\\nThe DataFrame was then pivoted to create new columns for each stock’s percent change price, which\\nwas essential for the analysis. The percent change price allowed for the analysis of the stocks’ volatility,\\nwhich is a crucial factor in the stock market analysis. This step provided the necessary data for the\\nsubsequent analysis, allowing for accurate insights into the market’s behavior.\\nIn conclusion, the data cleaning and preprocessing steps were essential in ensuring that the analysis\\nwas conducted on accurate and consistent data. The cleaning process eliminated any inconsistencies or\\nerrors that may affect the analysis, while the preprocessing steps provided the necessary data for the\\nsubsequent analysis, allowing for accurate insights into the market’s behavior.\\n3.3\\nFeature Engineering\\nFeature engineering is an important step in the data analysis process, as it involves creating new features\\nthat can improve the accuracy of the analysis. In this study, both common and idiosyncratic factors\\nwere used to create new features.\\nThe common factors used were the Dow Jones Index, Volume, SMB Factor, and MKT Factor. These\\nfactors are crucial in the Fama French 3-factor model, which is widely used in finance to analyze stock\\nreturns.\\nThe Dow Jones Index is a stock market index that measures the performance of 30 large\\ncompanies listed on stock exchanges in the United States. Volume refers to the total number of shares\\n4\\ntraded in a given period, and SMB Factor and MKT Factor represent common factors such as returns\\non small companies and market returns, respectively.\\nThe idiosyncratic factor involved clustering stocks that fluctuated with Disney stock based on their\\ndaily percent change in price. This was achieved by first pivoting the data to have stocks as rows and\\ndates as columns. A KMeans object was then created and fit to the data with three clusters, which\\nhelped to group similar stocks together. Finally, the stock names for each cluster were printed, providing\\ninsights into the stocks that fluctuated with Disney stock.\\n3.4\\nClustering\\nClustering is a technique used in machine learning and data analysis to group similar data points together\\nbased on their features. In this study, clustering was used to identify the stocks that fluctuated with\\nDisney stock. This was achieved by clustering the stocks based on their daily percent change in price.\\nThe first step in clustering was to pivot the data to have stocks as rows and dates as columns. A\\nKMeans object was then created and fit to the data with three clusters. The optimal number of clusters\\nwas determined using the elbow method, which involves plotting the within-cluster sum of squares against\\nthe number of clusters and selecting the point where the rate of change in the sum of squares starts to\\nlevel off. Finally, the stock names for each cluster were printed, providing insights into the stocks that\\nfluctuated with Disney stock.\\nIn conclusion, the feature engineering and clustering techniques used in this study helped to create\\nnew features and identify the stocks that fluctuated with Disney stock. These steps were crucial in\\ngaining insights into the market’s behavior and developing a more accurate model for predicting stock\\nreturns.\\n4\\nModel Selection\\nIn this stock market analysis, three different regression models were used to predict the stock price: the\\nFama French 3-factor model, Random Forest Regressor, and Gradient Boosting Regression model.\\n4.1\\nFama French 3-factor model\\nThe Fama French 3-factor model is a widely used model in finance that aims to explain the variation in\\nstock returns. The model assumes that the excess returns of a security are related to its sensitivity to\\nthree factors: market return, SMB, and HML. The model can be formulated as:\\nri − rf = βi,MKT (rMKT − rf) + βi,SMBSMB + βi,HMLHML + αi + ϵi\\n(1)\\nwhere ri is the excess return of security i, rf is the risk-free rate, rMKT is the market return, SMB\\nis the return on small companies minus the return on big companies, HML is the return on high book-\\nto-market companies minus the return on low book-to-market companies, αi is the intercept, and ϵi is\\nthe idiosyncratic error.\\n4.2\\nLinear Regression\\nThe linear regression model is a simple and commonly used model in finance that fits a line through a set\\nof data points. The model assumes that there is a linear relationship between the independent variables\\nand the dependent variable. The model can be formulated as:\\ny = β0 + β1x1 + β2x2 + ... + βpxp + ϵ\\n(2)\\nwhere y is the dependent variable, x1, x2, ..., xp are the independent variables, β0, β1, β2, ..., βp are the\\ncoefficients, and ϵ is the error term.\\nIn this analysis, the linear regression model was used to predict the stock price based on the selected\\nindependent variables. The performance of the linear regression model was compared to that of the other\\nmodels to determine the most suitable model for this dataset.\\n5\\n4.3\\nRandom Forest Regressor\\nThe Random Forest Regressor is an ensemble learning method that constructs a multitude of decision\\ntrees at training time and outputs the mean prediction of the individual trees as the prediction of the\\nforest. The model can handle both continuous and categorical input variables, and is known for its high\\naccuracy and ability to handle high-dimensional data. The model can be formulated as:\\nf(x) = 1\\nB\\nB\\nX\\nb=1\\nfb(x)\\n(3)\\nwhere f(x) is the prediction of the forest for input x, B is the number of decision trees in the forest,\\nand fb(x) is the prediction of the b-th decision tree.\\n4.4\\nGradient Boosting Regression\\nThe Gradient Boosting Regression model is a boosting ensemble learning method that builds the model\\nin a stage-wise manner, where new trees are added to the model to correct the errors of the previous\\ntrees. The model can capture non-linear relationships between the input and output variables and is a\\npowerful model for regression problems. The model can be formulated as:\\nf(x) =\\nK\\nX\\nk=1\\nβkhk(x)\\n(4)\\nwhere f(x) is the prediction of the model for input x, K is the number of trees, βk is the learning\\nrate for the k-th tree, and hk(x) is the prediction of the k-th tree.\\nOverall, each model was chosen based on its unique strengths and ability to fit the data in a different\\nway. The results of each model were compared to determine the most suitable model for this dataset.\\n5\\nModeling\\nThe purpose of this study was to develop a model to predict stock returns using the features generated\\nfrom the previous steps. Three models were used in this study: linear regression, random forest, and\\ngradient boosting.\\n5.1\\nLinear Regression\\nLinear regression is a simple and widely used approach to modeling the relationship between a dependent\\nvariable and one or more independent variables. In this study, linear regression was used to model the\\nrelationship between the dependent variable (stock returns) and the independent variables (common and\\nidiosyncratic factors). The linear regression model was built by fitting a line through a set of data points.\\nThe line was chosen to minimize the sum of the squared differences between the observed values of the\\ndependent variable and the predicted values of the dependent variable. The model achieved an accuracy\\nof 95.23%, as shown in Figure 5.\\n6\\nFigure 5: Accuracy of Linear Regression Model\\n5.2\\nRandom Forest\\nRandom forest is an ensemble learning method that constructs a multitude of decision trees at training\\ntime and outputs the class that is the mode of the classes (classification) or mean prediction (regression)\\nof the individual trees. In this study, random forest was used to predict stock returns using the common\\nand idiosyncratic factors.\\nThe random forest model achieved an accuracy of 71.27%, as shown in Figure 6.\\nFigure 6: Accuracy of Random Forest Model\\n5.3\\nGradient Boosting\\nGradient boosting is a machine learning technique for regression and classification problems, which\\nproduces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.\\nIn this study, gradient boosting was used to predict stock returns using the common and idiosyncratic\\nfactors.\\nThe gradient boosting model achieved an accuracy of 92.97%, as shown in Figure 7.\\n7\\nFigure 7: Accuracy of Gradient Boosting Model\\n5.4\\nComparison and Expansion\\nOverall, the linear regression model performed the best, with an accuracy of 95.23%.\\nHowever, the\\ngradient boosting model also performed well, with an accuracy of 92.97%. The random forest model\\nperformed the worst, with an accuracy of 71.27%.\\nFuture studies could expand on this research by using different types of models and exploring different\\nfeature engineering techniques to improve the accuracy of the predictions. Additionally, this study could\\nbe expanded to include more stocks and a longer time period to provide a more comprehensive analysis\\nof the stock market.\\n6\\nConclusion\\nIn this study, we analyzed the stock market behavior of Disney stock and developed a model to predict\\nstock returns. We performed data cleaning, preprocessing, feature engineering, clustering, and modeling\\nto achieve our objectives.\\nOur analysis showed that the Dow Jones Index, Volume, SMB Factor, and MKT Factor were crucial\\ncommon factors that influenced stock returns. Clustering analysis also identified stocks that fluctuated\\nwith Disney stock, providing insights into the market’s behavior.\\nWe developed three models to predict stock returns: linear regression, random forest, and gradient\\nboosting.\\nThe linear regression model achieved the highest accuracy of 95.23%, while the gradient\\nboosting model achieved an accuracy of 92.97%, and the random forest model achieved an accuracy of\\n71.27%. Overall, the results indicated that the models using common and idiosyncratic factors had good\\npredictive power.\\nFuture research could expand on this study by using different models and exploring other feature\\nengineering techniques. Additionally, more data could be included, covering a longer period of time and\\nmore stocks, to provide a more comprehensive analysis of the stock market.\\nIn conclusion, this study provided valuable insights into the stock market behavior of Disney stock\\nand demonstrated the effectiveness of feature engineering and clustering techniques in improving the\\naccuracy of stock return predictions.\\n8\\nReferences\\nThe following references were used in the development of this project:\\nFama, E. F., and French, K. R. (1993). ”Common risk factors in the returns on stocks and bonds.”\\nJournal of Financial Economics, vol. 33, pp. 3-56.\\nPython Software Foundation.\\n(2021).\\nPython Language Reference, version 3.10.\\nAvailable at:\\nhttps://docs.python.org/3/reference/index.html\\nScikit-learn developers. (2021). ”Ensemble methods.” Scikit-learn: Machine Learning in Python.\\nAvailable at: https://scikit-learn.org/stable/modules/ensemble.html\\nBrown,Michael.\\n(2014).\\nDow Jones Index.\\nUCI Machine Learning Repository.\\nAvailable at:\\nhttps://doi.org/10.24432/C5788V.\\n9\\n'},\n",
       " {'abstract': 'This study examines the potential of quantum circuits to reduce energy consumption in cryptocurrency mining, particularly in the SHA-256 cryptographic hashing function. The authors propose using quantum computing concepts to optimize the mining process and demonstrate the implementation of the Quantum XOR (CNOT) gate. Their work provides a proof-of-concept for the application of quantum technology in reducing the environmental impact of cryptocurrency mining.',\n",
       "  'introduction': 'Quantum computing holds promise for solving challenging problems in the financial world, including cryptocurrency mining. Conventional methods consume significant energy, with Bitcoin mining responsible for nearly one-third of its market value. There is a critical need to reduce these energy costs, leading to research efforts aimed at optimizing mining processes. Quantum computing, with its inherent low-energy characteristics, presents a potential solution.',\n",
       "  'literature_review': \"Previous studies have explored alternative approaches to classical SHA-256, such as the work of Ablayev and Vasiliev on quantum hashing and Vasiliev's binary quantum hashing. These investigations demonstrate the feasibility of quantum-based hash functions but are limited by the current small capacity of quantum hardware. Researchers have also investigated quantum annealing as an alternative method for SHA-256, utilizing the Ising model to minimize the objective function. Hybrid quantum computers have emerged as a promising solution to mitigate the limitations of current quantum computers, combining quantum and classical hardware to leverage their strengths.\",\n",
       "  'methodology': 'To demonstrate the application of quantum technology in SHA-256, the authors designed quantum circuits for the hash function, implementing them on both real quantum computer hardware and quantum simulators. These circuits were designed to exhibit the first parts of the SHA-256 hash process loop, given the limited capacity of remotely accessible systems. The results of the circuit execution were obtained after a number of shots (loop repetitions), reflecting the probabilistic nature of quantum computations. The authors also explored the use of hybrid quantum computers to share SHA-256 tasks between quantum and classical hardware.',\n",
       "  'results': \"The authors' experiments successfully demonstrated the implementation of the Quantum XOR (CNOT) gate, which is the core operation in SHA-256. The quantum circuits exhibited the initial part of the SHA-256 hash function loop on both real quantum computer hardware and quantum simulators. The results obtained from these circuits showcased the probabilistic nature of quantum computations, with the most probable outcomes identified. Furthermore, the authors compared the energy consumption of quantum and classical mining systems, highlighting the significant reduction in energy consumption offered by quantum computing.\",\n",
       "  'conclusion': 'The study provides a proof-of-concept for the application of quantum technology in optimizing the energy consumption of cryptocurrency mining. While the current limitations of quantum hardware restrict the implementation of the entire mining process, the authors emphasize the potential of future developments in quantum computing hardware and hybrid quantum computers. They address concerns about the probabilistic nature of quantum computations and their impact on the determinism required in cryptocurrency mining, explaining how these results can be interpreted by classical hardware to ensure deterministic outcomes. The authors also acknowledge potential debates on the implications of easier cryptocurrency production on market dynamics and emphasize that their proposed method focuses on reducing energy consumption rather than simplifying the mining process.',\n",
       "  'title': 'The lower energy consumption in cryptocurrency mining processes by SHA-256 Quantum circuit design used in hybrid computing domains',\n",
       "  'author': 'Ahmet Orun, Fatih Kurugollu',\n",
       "  'textdata': '1 \\n \\nThe authors are with School of Computer Science and Informatics, De Montfort University, Leicester LE1 9BH, UK \\nEmail: aorun@dmu.ac.uk, and Department of Computer Science, University of Sharjah, UAE.  Email:  fkurugollu@sharjah.ac.ae  \\nThe lower energy consumption in cryptocurrency mining \\nprocesses by SHA-256 Quantum circuit  \\ndesign used in hybrid computing domains \\n \\nAhmet Orun and Fatih Kurugollu \\n  \\n \\nAbstract - Cryptocurrency mining processes always lead to \\na high energy consumption at considerably high \\nproduction \\ncost, \\nwhich \\nis \\nnearly \\none-third \\nof \\ncryptocurrency (e.g. Bitcoin) price itself.  As the core of \\nmining process is based on   SHA-256 cryptographic \\nhashing function, by using the alternative quantum \\ncomputers, hybrid quantum computers  or more larger \\nquantum computing devices like quantum annealers, it \\nwould be possible to reduce the mining energy \\nconsumption with a quantum hardware’s low-energy-\\noperation \\ncharacteristics. \\nWithin \\nthis \\nwork \\nwe \\ndemonstrated the use of optimised quantum mining \\nfacilities which would replace the classical SHA-256 and \\nhigh energy consuming classical hardware in near future. \\n \\nIndex Terms- CryptoCurrency, quantum computing, hash \\nfunction, cryptography \\n \\nI. INTRODUCTION \\n \\nQuantum computing is one of the emerging and rapidly \\ngrowing technologies in financial world [5] [20] by \\npromising solutions for cryptocurrency mining. In \\nparticular, one of the most remarkable issues of popular \\ncryptocurrencies [18][19] is their extremely high energy \\nconsumption during their mining process (e.g., ~10 \\nminutes:  72,000 GW for Bitcoin). This results in \\nspending nearly one-third of Bitcoin’s current (2021) \\nmarket value for energy consumption. It will also be \\nproportionally becoming more problematic with its \\nsoaring production volume and dramatically increasing \\nglobal energy shortage in the future. This critical field \\nnot surprisingly attracted several investigations and \\nstudies recently on how to make a considerable \\nreduction of cryptocurrency mining costs [1][2][37].  \\n \\nQuantum computing provides fertile grounds to solve \\nthis problem as its energy consumption is extremely \\nlower than the classical computing with CPU, GPU and \\nASIC [24]. As far as the quantum hardware \\ncharacteristics are concerned, their size is nearly  \\n \\n \\n \\n \\n \\nindependent from their energy consumption. They \\nconsume very small energy whatever their size is in  \\nqubit capacity (as only their interface electronic \\ninstrumentation or their cooling systems consumes very \\nlow  energy). There are very few previous works focused \\non an exploitation of quantum computing and quantum  \\nhardware utilities for such challenging  hash function \\ntasks  due to Quantum Hardwares’ current small size \\ncapacities [1][5]. Ablayev and Vasiliev introduces a \\nwork on quantum hashing [6] which is the core of \\ncryptocurrency mining. Their method is based on \\nclassical-quantum where a classical bit string is used as \\ninput to produce a quantum state. Even though all \\nscientific \\nauthorities \\nagree \\non \\nthe \\nlow-energy \\nconsumption characteristic of quantum hardware, the \\nreal issue at the moment is their small-sizes and current \\nlow capacities (e.g., max 50 “reliable” qubits) to run any \\nstandard cryptocurrency algorithm like SHA-256 \\nhashing function. The other frequently mentioned issue \\nis probabilistic nature of quantum physics [16][17] and \\nits adverse effect on low-level quantum hardware (QH) \\noperation whereas Cryptocurrency mining process has to \\nbe very deterministic rather than probabilistic. This \\nproblem however only linked to low-level natural \\ncharacteristics of quantum physics but not an issue at \\nuser level interface domain, and could be overcome by \\nhigher level supplementary methods operating in \\nclassical interface hardware (e.g. conventional dedicated \\nPC linked to QH).  \\nIn this work, we investigated how any suitable “quantum \\nhardware based” quantum SHA-256 hash function, \\nwhich is equivalent to classical SHA-256, can be \\nimplemented and executed on a real publicly accessible \\nquantum computer like IBM QX or running on a \\nquantum simulator to demonstrate its functionality. For \\nthis aim, we implemented Quantum XOR gate (CNOT) \\noperation  in our work. Our implementation shows that \\nSHA-256 can be utilized in QH effectively. This would \\nlead to remarkable electricity energy saving which help \\nto mitigate possible energy crises in future. The \\ncomparison between quantum and classical hardware \\nenergy consumptions is shown in Table 3.  \\n \\n \\n2 \\n \\n \\nII. METHODS AND MATERIALS \\n \\nA.  SHA-256 hash function  \\n \\nOne-way hash function or secure hash function is one of \\nthe important concepts in cryptography as used in many \\nsecurity related applications. This idea was firstly \\nintroduced by Merkle [7], Rabin [9] and, Diffie and \\nHellman [8] in their studies on public key cryptography, \\nauthentication, and digital signature. After all it became \\nan indispensable tool for other security applications like \\npassword verification and file integrity checking as well \\nas proof of work in Bitcoin mining. \\n \\nThe hash function, H(·), maps an arbitrary long input \\nmessage, m, onto a fixed sized output hash code, h (H(m) \\n= h) as depicted in Figure 1. The mapping function \\nshould be easily computed for any given m. Besides \\nthese basic properties, Merkle [7] pointed out that the \\nfollowing characteristics are must for a hash function \\nand  to be used in complex security applications:  \\n \\n• \\nOne-way or pre-image resistance: For any given \\nhash code, h, it is computationally infeasible to \\ndetermine the original input message, m. \\n• \\nSecond pre-image resistance or weak collusion \\nresistance: For a given message, m, it is \\ncomputationally infeasible to compute another \\nmessage, m’, which has the same hash code, H(m) = \\nH(m’). \\n• \\nStrong collusion resistance: it is computationally \\ninfeasible to find a pair of messages, m and m’, such \\nthat  H(m) = H(m’).  \\nHowever, it is worth to note that strong collision is not \\navoidable within the generic process of hash functions \\nbut ideally kept at limited level.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1. Basic view of a hash function as input data at \\nvarying length encrypted into a hash value at a fixed \\nlength (256 bits in the example)  \\n \\nAfter initial introduction of hash functions, National \\nInstitute of Standards and Technology (NIST) in USA \\ndeveloped Secure Hash Algorithm (SHA) providing \\nsecurity to hash functions and published as a Federal \\nInformation Processing Standard (FIPS 180) in 1993 \\n[21]. NIST also revised SHA in 2002 and published \\nFIPS 180-2 introducing new versions of SHA family \\nhaving 256-bit, 384-bit and 512-bits hash value lengths. \\nAmong them, SHA-256 was selected as most suitable \\none by the cryptographic authorities with its 128 bit \\nsecurity level against a collision attack in comparison to \\nSHA-512 despite its higher security level but larger \\nperformance penalty[3]. SHA-256 has also adopted for \\nProof-of-Work process by Bitcoin [22]. \\n \\nSHA-256 hash function uses the Merkle-Damgård \\ntransform based on a compression function which is \\napplied recursively to map n + l input bits to n output \\nbits [14]. For this aim, a common Davies-Meyer \\ncompression algorithm is used (Figure 2). As is a one-\\nway compression function, its original message does not \\nneed to be restored back. A generic Davies-Meyer \\ncompression function can be defined as follows [14]: \\n \\nf : {0,1}n+l  → {0,1}l \\nf(k,m) = Ek(m) \\uf0c5 m                                        (1) \\n \\nwhere k is the key, m is the message block and E is the \\nencryption function with key length n and block length \\nl. This states that the encrypted message is XORed with \\nthe message block. In SHA-256, this function is used in \\na recursive manner using the previous hash value, Hi-1, \\nas the plaintext for the function to generate the next hash \\nvalue, Hi. The message block is used as the key for E. \\nTherefore the chain for the hash value in SHA-256 can \\nbe expressed as follows: \\n \\nHi = Emi (Hi-1) \\uf0c5 Hi-1                                                                  (2) \\n \\nwhere;     m: message block,   \\n \\n  H: hash value (previous and next)   ,   \\n \\n  E : encryption process    \\n \\nAt the start of process where the previous Hi-1 value is \\nnot available, the initial H0 value is used as is shown in \\nTable 1. The last H value after 64 rounds of compression \\nfunction is used to determine the hash code of the \\nmessage. This core process of SHA-2 is depicted in \\nFigure 2. In each iteration of the loop, an element of 64 \\ndigit block is processed by the encryption function, E. \\nAs far as the specifications of the process are concerned, \\nhash value, H, consists of 64 digit hexadecimal string. \\nBlock size of  256 bit block of message is processed in \\neach iteration.  mi   shows the ith block of the complete \\nmessage m.  Here each word of string is 32 bit unsigned \\ninteger and in each iteration, a block of 64 digit message \\nblock is processed. The implementation details of SHA-\\n256 can be found in [14]. Some examples of messages \\nand their SHA-256 conversions are provided in Table 1.  \\n \\nHash code h:( 0x \\ne3b0c44298fc1c1f) \\nArbitrary long \\ndata \\nm: (“my key”) \\n \\nHash \\nFunction \\n       \\n3 \\n \\nTable I. Some of  the  sample  messages (m)  and   \\n Their SHA-256 conversions in hexadecimal form \\n \\nThe SHA-256 is characterised by XOR, bit-rotations, \\nNOT, OR, AND, modular addition operations [14]. The \\ncore operation is XOR gate because other operations can \\nbe easily implemented by using an effective design of \\nXOR gate. Therefore, our concern in this work is to \\ndevelop and implement a Quantum equivalent of XOR \\ngate which is CNOT quantum gate.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n        \\n \\n \\nFigure 2. The continuous loop processing the chain of \\nmessages in 64-digit blocks \\n \\n \\n \\n \\n \\n \\n \\n \\nB.  Quantum hardware and computing  \\n \\nQuantum computing processes are enabled by quantum \\nhardware (e.g., photonic, ion-trap, super conducting, \\ntopological, etc.) as called “quantum computer” or \\n“quantum annealer” which operate according to \\nquantum \\nphysical \\nprinciples \\n[11][12]. \\nQuantum \\ncomputing concept operationally exploits a quantum \\nparticle’s two states (e.g. spin-up or spin-down, etc.) and \\ntheir superposition characteristics to make calculations \\nwith an advantage of additional 2x states over the \\nclassical bit (0,1), as is called qubit. If qubit number is n, \\nthen the number of states will be 2n. The number of \\nqubits refers to the capacity of quantum hardware. For \\nexample, a 3-qubit quantum computer can be described \\nby 8-dimansional vectors like = (a0, a1, a2, a3, a4, a5, a6, \\na7) where each vector element, ai, is a complex number \\ncoefficient. Their state descriptions would be as follows: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\na0 |000\\uf0f1 +   a1|0001\\uf0f1 +   a2 |010\\uf0f1 +   a3 | 011\\uf0f1 + a4 |100 \\uf0f1 +   \\na5 |101\\uf0f1 +   a6 |110\\uf0f1 +  a7 |111\\uf0f1 \\n \\nQuantum computing can be demonstrated on globally \\navailable systems like IBM QX (53 qubit as only 15 \\nqubit accessible for public use at the moment), D-Wave  \\n(Leap) 5000 qubit, Google (53 qubit) by remote access. \\nThe facilities are optional in “real-time “and “simulator” \\nmodes. Meanwhile Microsoft provides a cloud service \\nfor Quantum programming in its own language Q# \\nrunning in a simulation mode on a simulator. In terms of  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nhardware efficiency, nowadays the most important issue \\nof quantum hardware is high error rate of qubits as it is \\na current bottle neck to overcome until a new “low-error-\\nrate” quantum hardware discovery. However, a USA \\ncompany, IonQ, recently (October 2020) announced a \\nnew reasonable error rate quantum computer at 32 qubit \\ncapacity operates with ion-trap technique [25]. \\nMeanwhile there is always confusion between the \\nphysical qubit and logical qubit terms, as 1 logical (low-\\nerror) qubit may refer to ~200 physical (high-error) \\nqubits. The other disadvantageous factor is that in \\nquantum hardware, qubits communicate to each other as \\nan error produced by each qubit increase cumulatively \\nover the logical gates (CNOT gate in particular) which \\nis the main obstacle for larger quantum computers \\ndesign and production. However, quantum error \\ncorrection is the mainly focused area on the global basis, \\nhence expected to be solved in near future for large scale \\nquantum computers[26]  We have to note that the \\nquantum hardware is inevitably operated in association \\nwith the classical computers (hardware) used as an \\ninterface. \\n \\n \\nMessage (m)  \\n  SHA-256 conversion to 64 digit block (with 4 bit x 8x8 = 32 bit words x 8) \\n“This is my message \\nfor quantum” \\n9b95bfa6ceb2de10d7ef3ff3b794ffea2c2ba7911a209b323a55e8f306a64931 \\n  “DMU” \\n3a8b4b9d4649b3573f552a9eb6b5c1244fd79815e817aa86d65422e2564b2d0a \\n  “DMV” \\nd4fee25a1acee0e6610473456a83bd2f4f5ccf96e25c13b88f65cd79ca54d7ed \\n     “A” \\n559aead08264d5795d3909718cdd05abd49572e84fe55590eef31a88a08fdffd \\nH0 (initial value) \\n6a09e667 bb67ae85 3c6ef372 a54ff53a 510e527f 9b05688c 1f83d9ab 5be0cd19 \\n4 \\n \\n \\nC.  CNOT quantum logic gate (XOR equivalent) \\n \\nIn quantum computing, CNOT logical gate (controlled-\\nnot gate) is the important one to generate quantum \\nentanglement for the quantum tasks like teleportation, \\nsuper dense coding, etc. but in our work here, it would \\nonly be used as the quantum analogue of classical \\ncomputing’s XOR logical gate. CNOT gate is always \\nused by operating on two qubits at once. The first qubit \\noperates as a “control” qubit whose state affects the \\nsecond “operation” qubit’s state. If the control qubit is \\n“0” then there is no change on operation qubit’s state, \\nbut if the control qubit is “1” then it changes the state of \\noperation qubit. Control qubit state and operation qubit \\nstate are shown in Figure 3 as x-input and y-input. The \\nclassical SHA-256 can be implemented using an XOR \\nlogic gate “\\uf0c5” in its loop process which can be seen on \\nFigure 3. Its 2-qubit quantum circuit equivalent with \\nCNOT logic gate “\\uf0c5” where Hi-1 ≡ x and mi ≡ y, as well \\nas their logical process table is depicted at the right hand \\nside section. \\nOn the other hand, the SHA-256 is characterised by \\nXOR, bit-rotations, NOT, OR, AND, modular addition \\noperations [14]. The core operation is the XOR gate \\nbecause other operations can be easily implemented by \\nusing an effective design of an XOR gate. Therefore, our \\nconcern in this work is to develop and implement a \\nQuantum equivalent of an XOR gate which is a CNOT \\nquantum gate. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 3. Implementing classical SHA-256 using \\nQuantum CNOT gates \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe further details about the quantum characteristics of \\nthe CNOT logical gate (e.g. its entanglement or \\nsuperposition links, its usage with Hadamard gate, \\netc.[23]) are out of the scope of this work.  \\n \\nIII. POTENTIAL ALTERNATIVE OR \\nSUPPLEMENTARY METHODS \\n \\nA. Quantum hash function  \\n \\nEven though we propose a low-level quantum logical \\ngate based operation in this study which is quantum \\nhardware related and would be  independent from any \\n“classical” or “quantum” hash function computing \\napproach, our proposed low-level method makes a \\nsubstantial contribution to such theoretical studies to \\ndemonstrate them.     The earlier studies on quantum \\nhash function were demonstrated on both “non-binary” \\nbased [6] and “binary” quantum hashing [13]. The non-\\nbinary method accepts classical bit string as an input and \\nconvert it to quantum state output. Vasiliev introduces \\nthe technique which allows to present binary inputs by \\nquantum states [13]. He also proved that there was \\nreverse correlation between the characteristics of \\nmethod so that, the more quantum hash function was \\npreimage resistant, the less collision resistant it was. In \\ntheir work, Ablayev and Vasiliev defined the quantum \\nhash function by the following notifications, in which \\nthe quantum hash function hK described as [6] in \\nEquation 3:  \\n \\n|ℎ𝐾(𝑀)⟩ = 1\\n√𝑑 ∑\\n|𝑖⟩\\n𝑑\\n𝑖=1\\n(𝑐𝑜𝑠\\n2𝜋𝑘𝑖𝑀\\n𝑁\\n|0⟩ + 𝑠𝑖𝑛\\n2𝜋𝑘𝑖𝑀\\n𝑁\\n|1⟩)                     \\n(3) \\n \\nwhere M is the arbitrary message satisfying M \\uf0ce {0,1}n, \\nN = 2n (for n-bit messages), d = |K| and Set K = {ki : ki \\n\\uf0ce{0,….,N-1}}. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIf \\uf064 is defined as resistance (e.g., (orthogonality) \\nquantum collision resistance, it also corresponds to \\nclassical second pre-image resistance). For any pair of \\ninputs W, W’  (W \\uf0b9 W’ ) it satisfies |\\uf0e1\\uf0791|\\uf0792\\uf0f1| < \\uf064 where the \\nabsolute value of \\uf0e1\\uf0791|\\uf0792\\uf0f1  inner product between  two \\nwave functions refers to distance or overlap between \\nthose two states and also refers to as fidelity. where \\uf079 is \\ncalled quantum one-way function complying with the \\nhash function property that is easy to compute and \\ndifficult to invert.  With the above notations it would be \\npossible to evaluate formula 4  as follows [6]:  \\n5 \\n \\n \\n|\\uf0e1ℎ𝑀1|ℎ𝑀2\\uf0f1| =\\n|\\n1\\n|𝐾| ∑\\n𝑐𝑜𝑠\\n2𝜋𝑘𝑖(𝑀1−𝑀2)\\n𝑁\\n|𝐾|\\n𝑖=1\\n|  ≤ \\uf064(K) <  \\uf064          (4) \\n \\n \\n \\n \\n \\n \\n In which the pair of input massages have not to be same, \\nM1 \\uf0b9 M2. \\n \\nSerial calculation of quantum hash function would be \\npossible on globally available quantum computers or via \\ntheir cloud connection. But in the latter case, logical gate \\ncalculations of a binary data string have to be made by a \\nremote connection to paid-quantum-computing services \\n(e.g. ~1000 shots cost about 0.25 USD) where the \\nsupplier provides a dedicated time slot for the usage. \\nHowever, this would not be the ideal case since any \\nconnection between the interface domain and remote \\nquantum hardware would have a limited security level. \\nHence this option has to be fortified by  additional \\nsecurity measure for a secure connection in  commercial \\napplications.   \\nIn case of direct usage of a quantum hardware, logical \\ngate calculations of a binary data string (e.g. input \\nmessage for SHA-256 hash function) on a Quantum \\nprocessor may be carried out as block-by-block of a long \\nstrings (e.g. 1000 digit long) which is normally beyond \\nthe current qubit capacity of quantum computers that are \\nglobally available (e.g. max 65 qubit). Such block-by-\\nblock string processing loops have to be run by a \\ncontinuous real-time operation of quantum hardware. As \\nthis issue could be solved by partial operational division \\nof entire task, the other quite promising global \\ndevelopments indicate that the quantum hardware \\ncapacity is progressively increased and targeted 1000 \\nqubit is expected to be achieved in 2023.    \\n \\nB.  Quantum annealer as an alternative method for \\nSHA-256 hash function \\n  \\nQuantum annealing is linking a specific problem’s best \\nsolution to a related quantum physics phenomenon to \\nfind the minimum energy state which already means the \\nsolution. To accomplish that, first, the objective function \\nof the problem should be specified. Then calculate its \\nenergy value should be calculated by using function \\nparameters and targeting the minimum energy point. \\nQuantum annealing differs from the qubit gate model as \\nthe qubit gate model depends on solving the problem by \\ncontrolling the logical qubit gates. The limitation of the \\nlogical qubit gate model depends on the low qubit \\ncapacity of current quantum computers globally \\navailable which is currently 64 qubit (IBM’s QC at \\nManhattan, as Google’s 72 qubit QC has some \\ncontrolling issue) . Whereas quantum annealers operate \\ndepending on the evolutionary approach in which the \\nsystem \\nsearches \\nthe \\nminimum \\nenergy \\nstate \\ncorresponding to the exact solution of the objective \\nfunction. In contrast to current logic gate models \\ndemonstrated by the quantum computers, quantum \\nannealers may have huge qubit capacity (e.g. 5,760 \\nqubits [27]) because their hardware characteristic is very \\ndifferent than  quantum computers and limited to only \\noptimisation problem solutions.  \\n \\nD-WaveTM for instance,  produces reprogrammable \\nquantum processing unit chips to be used in a quantum \\nannealer. It has to work at milli-Kelvin temperature \\nrequiring a cooling system. The quantum annealer’s chip \\nis also in lattice form and demonstrates the Ising model \\n[15] using voltages and magnetic fields to control the \\nchip circuits. In a quantum annealer chip, the qubits are \\na 2D array of superconducting loops carrying electric \\ncurrent and behave like magnets pointing up or down, \\nbut in this case, pointing up and down at the same time \\naccording to the quantum superposition principle. To use \\nthe quantum annealer, the user maps the specific \\nproblem into a search for the lowest energy point. Then \\nthe \\nquantum \\nannealer \\nprocessor \\nconsiders \\nall \\npossibilities simultaneously to satisfy the qubits network \\nof relationships with the lowest energy point. This \\nprinciple would be demonstrated by minimizing the \\nobjective function (e.g. SHA-256 hash function in our \\ncase) in Figure 4 as follow:  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 4. XOR classical logic gate or  CNOT quantum \\nlogic  gate logical process  operated by quantum \\nannealer with minimum energy state approach. \\n \\n       fobj (weight, strength, qubit) = \\uf053 weight . \\nqubit +  \\uf053 strength . coupler                       (5) \\n \\nIn Equation 5, the notations are described as follows: \\nCoupler: Physical device that allows one qubit to \\ninfluence the others.  \\nQubit: Quantum bit participating annealing cycle and, \\nat the end it settles down to one of the states {0,1}. \\nWeight: Qubit’s tendency to jump into the final state {0 \\nor 1}. It is constant and controlled by the user. \\nStrength: Controls the level of qubit’s influence on the \\nothers. It is constant and controlled by the user. \\n \\n6 \\n \\nDuring the quantum annealing cycle, the qubit spins \\nkeep evolving and exploring the problem space. At the \\nend of the annealing cycle, the system reaches its ground \\nstate of the submitting problem. Then the final states are \\nyielded as the output of the solution.  A representative \\nquantum annealing example solution for XOR or CNOT \\nwhich is the core operation in SHA-256 is shown as \\nfollows:  \\nProblem definition: Find minimum energy = 0  for  x \\n\\uf0c5 y  (XOR classical logic gate or  CNOT quantum logic \\ngate)  \\nBy the quantum annealing cycles, the system yields \\noutputs of minimum energy states “0” which are the \\nsolution of hash function (x \\uf0c5 y) as seen in Figure 4 \\nQuantum annealers have been investigated to be used as \\na potential cryptocurrency process domains in future but \\ndue to current limitation of free access (e.g. $2000/hour \\nservice provided by D-WaveTM) , they have not been \\nused within this study.  \\n \\nC.  Hybrid quantum computers  \\n \\nHybrid quantum computing has been an emerging \\ntechnology as a solution [29][30] to mitigate the issue of  \\nsmall size and low capacity of current quantum \\ncomputers (QC), by merging quantum computing and \\nclassical computing hardware to utilize them in \\nharmony.  Hence, the hybrid computation may be a good \\nchoice to overcome the issues of  SHA-256 capacity \\nneed in quantum computation domains. In our case the \\nideal approach would be, high energy consuming \\nprocesses will be executed in quantum hardware and \\nmeanwhile \\nthe \\nclassical \\nhardware \\n \\nwill \\nbe \\nsimultaneously used in connection with QC for other \\ntasks like: loop operations of SHA (string-by-string), the \\nstorage of the results, data format conversions (e.g. from \\nhexadecimal to binary, etc.) and other sub-operations as \\nseen in Figure 5.    \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 5. Proposed configuration of hybrid quantum \\ncomputer design to solve the issues of current QCs low \\ncapacity by sharing SHA-256 tasks categorised as “high-\\nenergy consumption (in QC)” and “low energy \\nconsumption (in Classical hardware)” \\n \\n \\n \\n \\n \\n \\nIV. RESULTS AND DISCUSSION \\n \\nA.  Quantum circuits design to demonstrate the logical-\\ngate level  of the hash function process   \\n \\nSome of the available accessible systems are shown in \\nTable 2. The quantum circuits for the SHA-256 hash \\nfunction have been designed and executed on two \\ndifferent domains of IBM:  \\n-Real  quantum  computer  hardware (Melbourne_15  Qubit)  \\n-Quantum simulators (32qubits, 5000qubits).  \\nas quantum simulators are specifically designed devices \\nwith different type of hardware to simulate the QC tasks. \\nThe quantum circuits exhibit only the first parts of the \\nSHA-256 hash process loop due to the limited capacity \\nof remotely accessible systems. The basics of circuits’ \\nNOT and CNOT gates are shown in Figure 3 and \\ncomplete circuits in Figure 6  respectively.   \\n \\nTable II. The globally Available remotely accessible \\nQuantum Computer Services   \\n \\n \\n \\n \\nTable III. Comparison between the quantum hardware \\nand classical mining  system (by different references) \\nHardware type \\nEnergy consumption  \\n  (TWh /year) \\nCO2  Emission  \\n( Tons / year) \\nBitcoin mining unit \\n 80     [31] \\n110    [32] \\n 91     [33] \\n \\n267   [31] \\nQuantum computer \\n25 x 10-9 (25 kWh) [35][36] \\nN/A  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nIt has been reported that there are about 1 million \\nBitcoin miners globally available [34]. The related \\ncalculations of electricity consumption and CO2 gas \\nemission for both (quantum and classical systems) can \\nbe made as shown in Table 3.  \\n \\n \\nSystem  Name \\n(Location) \\nProcessor type \\nQuantum \\nvolume \\nQubit  \\ncapacity \\nIbmq Manhattan \\nHummingbird R2 \\n32 \\n65 \\nIbmq  Montreal \\nFalcon r4 \\n128 \\n27 \\nIbmq Dublin \\nFalcon r4 \\n64 \\n27 \\nIbmq Sydney \\nFalcon r4 \\n32 \\n27 \\nIbmq Casablanca \\nFalcon r4H \\n32 \\n7 \\nSimulator  stabilizer \\nClifford simulator \\nN/A \\n5000 \\n7 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nPractically SHA-256 hash function quantum “qiskit” \\ncodes  can be directly executed on the real quantum \\ncomputer hardware remotely without the need for \\ncircuit design as shown in Figures 6 and 7.   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 6. SHA-256 Circuit design which demonstrates \\nfor-loop (first 8-bit) of SHA-256 hash function in 5000-\\nqubit capacity Quantum Simulator. Its measurement \\nresults are also shown at top-right window.  \\n \\nThe circuit processes initial (starting from the end) part \\nof message m= “this is my message for quantum” and \\ninitial H0 value as;  \\n \\nH0(h0)= 01101010000010011110011001100111 which \\nis equivalent of  h0= 0x6a09e667 in hexadecimal form \\nis shown in Figure 6.  We have to note that in the process \\nonly H0(h0) is taken instead of H0(h7) which is normally \\naligned with the end of message “m”. The results \\n(measurements) include 16 x”0” corresponding to H0 \\n(h0) and m1 section (top of the circuit) where no \\nmeasurements are made. In Figure 6, the histogram at \\nthe top-right exhibits 1022 shots of the logical gate \\noperation measurements for 24 qubits (0-23) whose \\nresult is calculated for qubits 23-16 as “0,1,1,1,1,1,1,1” \\nand “0s” for the remaining qubits 15-0 means that  no \\nany logical gate operation is made for this range.   As \\nalready known, the quantum computer’s calculation \\nstyle is based on probability, hence the results are \\nyielded after number of shots (loop) which \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nthe results are yielded after number of shots (loop) which \\ncorresponds to number of repetitions of the  quantum \\ncircuit. In the circuit of quantum simulator (Figures 6), \\nthe NOT gate is shown as “\\uf0c5” which is also known as \\nPauli-X gate. It changes |0\\uf0f1 state to |1\\uf0f1 state. As is shown \\nat the left edge of the circuit, all qubits initially start with \\n|0\\uf0f1 states for values of “0”,  and then the  values of “1” \\nare generated by use of  “NOT” gates which refer to |1\\uf0f1 \\nstates. In this way we can make any data entry of any \\nbinary string into the quantum circuit. In the middle \\nregion of circuit we use CNOT gates (Controlled-NOT \\ngate corresponds to classical XOR logic gate) which link \\nbetween two binary  values to process XOR logical \\noperation between them. The measurement icons at the \\n8 \\n \\nbottom-right corner of the circuit just display the output \\nof calculations.  The measurement result “Message“m2” \\nis yielded after the CNOT logical gate operation between \\n“Message“m1”  and “H0(h0)” as described in Formula 2. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 7. (Top) Single loop of SHA-256 hash function \\noperation by the circuit where i=4bit  (as explained in  \\nFigure 3) is executed on IBM’s 15qubit  real quantum \\ncomputer hardware (Melbourne).  (Bottom) The results \\nare quantum probabilistic and different than simulator \\nones. The most probable results are shown by blue arrow \\nin the frequency histogram table whose probability \\nvalues are generated after 1022 shots of quantum units. \\nThe circuit of real quantum computer as shown in Figure \\n7 (other than the quantum simulator) contains the same \\ncomponents with the Figure 6,  but with  different \\nnumerical configuration. The “most probable” result is \\ndisplayed by the frequency  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nhistogram \\n(bottom \\nside) \\ncorresponding \\nto \\n“Message”m2”  is  “0,0,0,1,1,0,0” for the qubit range 14-\\n8 respectively. The remaining “0” values for the  qubit \\nrange 7-0 means that  no any logical gate operation is \\nmade for this range.    \\n \\n9 \\n \\n \\n \\n \\n \\n \\n \\nV. CONCLUSION \\n \\nWithin this study, the basics of the classical SHA-256 \\nhash function which is the most energy consuming part \\nof cryptocurrency mining have been demonstrated at a \\nproof-of-concept level. Even thought the whole mining \\nprocess requires a very high volume of quantum gate \\ncombinations, it fortunately has  the identical repetitive \\nstructures and configurations whose functions to be \\nproven by a small size of globally accessible quantum \\ncomputers. As far as the energy consumption of the \\nproposed system is concerned, the interface hardware for \\nQCs and/or classical computing components of a hybrid \\ncomputers have not accountable energy consumption \\nsince they are only utilized for data format conversions, \\ndata storage, quantum logical process loop operations, \\netc.  The other popular concern is about quantum \\nphysic’s probabilistic nature and is reflected by quantum \\nhardware’s high error rate (CNOT gate in particular) \\nwhich has also been proven as is not a problem at a \\nhigher level of host system’s operation where the \\nprobabilistic results are further re-interpreted by a \\nclassical (interface) hardware to make the results \\ndeterministic enough for Cryptocurrency mining \\nprocess. It has to be noted that the results obtained in \\nFigure 7 rely on high-error rate quantum hardware, \\nwhereas there is currently lower error rate hardware like \\nIonQ’s 32-qubit quantum hardware (QH) utility and \\nIBM’s 65-qubit (QH) utilities available but at some \\nservice cost. However more reliable and larger quantum \\nhardware would be soon presented in near future (e.g. \\n1000 qubit QC is targeted by 2023 and 1M qubit QC is \\nexpected by 2030) this is due to high level of globally \\nchallenging \\ncompetition \\nbetween \\nthe \\nquantum \\ninstitutions and private companies.  \\n \\nOn the other hand, as there would be a common debate \\nin financial community based on a speculative idea \\nclaiming that the easier production of Cryptocurrency \\nmay lead to its sharp price fall and lead to corruption of \\nwhole crypto currency system. Whereas some finance \\nauthorities agree that the cryptocurrency prices do not \\nrely on any absolute factor but rather manipulated only \\nby global supply-demand chain. In addition to this, we \\nhave to note that in our study in fact we are not \\nproposing any quantum algorithm to speed up the \\ncurrent mining process nor make it easier. Our \\nproposed method is just to make the current mining \\nprocess less energy consuming which helps reduce \\nglobal carbon emission, avoiding a global waste of \\nelectricity, initiating further developments of quantum \\nfinance area, etc.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nREFERENCES \\n \\n[1] Wilkins, A. Quantum computers could slash the energy use of \\ncryptocurrencies. New Scientist magazine (Technology), June, 20. \\n2023. \\n \\n[2] Li, J., N.  Li, J. Peng, H. Cui and Z. Wu. Energy consumption of \\ncryptocurrency mining: A study of   electricity consumption in mining \\ncryptocurrencies, Energy 168(2019), pp.160-68.  \\n \\n[3] Ferguson, N. and B. Schneier. Practical Cryptography, Wiley \\nPublishing Inc., Indiana US,  2003. \\n \\n[4] National Institute of Standards and Technology. Secure Hash \\nstandard (draft) 2001.  \\n http://csrc.nist.gov/encryption/shs/dfips-180-2.pdf   \\n \\n[5] Orus. R., S. Mugel and E. Lizaso. Quantum computing for finance: \\nOverview and prospects,  Reviews in Physics 4 (2019) 100028,  2019. \\n \\n[6] \\nAblayev, \\nF. \\nand \\nA. \\nVasiliev. \\nQuantum \\nHashing. \\narXiv:1310.4922v1, Kazan Federal University, 18 October 2013.  \\n \\n[7] Merkle, R.C.: Secrecy, Authentication and Public Key Systems. \\nPh.D. thesis, UMI Research Press, Italy (1979). \\n \\n[8] Diffie, W., Hellman, M.E.: New directions in cryptography. IEEE \\nTrans. Inf. Theory. IT-22, 644–654 (1976). \\n \\n[9] Rabin, M.O.: Digitalized signatures. In: Lipton, R., DeMillo, R. \\n(eds.) Foundations of Secure Computation, pp.155–166. New York, \\nAcademic Press (1978). \\n \\n[10] Yang, Y., Bi, J., Li, D., Zhou, Y. and Shi, W. Hash function based \\non quantum walks. International Journal of  Theoretical Physics, 58 \\n(2019), pp.1861-73.  \\n \\n[11] Raymer, M.G. Quantum Physics, Oxford University Press, New \\nYork (2017). \\n \\n[12] Orun, A. Inter-substances “Natural quantum communication” \\ninvestigation by atomic exchange observation with photonic wave-\\nparticle duality quantization, KOBIT-5 International Quantum Optics \\nand Information Conference, 22-23 April 2021. Turkey.  \\n \\n[13] Vasiliev, A.V. Binary quantum hashing, Russian Mathematics, \\n2016, Vol.60. No.9, pp.61-65.  \\n \\n[14]Davies-Meyer(one-way)Compression, \\nhttps://en.wikipedia.org/wiki/Oneway_compression_function \\nKnospe, H.: A Course in Cryptography, American Mathematical \\nSociety,2019. \\n \\n[15]Ising, E.,  Ising Model,  https://en.wikipedia.org/wiki/Ising_model \\n, 1924. \\n \\n[16] Orun, A. and G. Smith (2020). Investigation on a quantum \\ncommunication phenomenon between subatomic properties of \\nsubstances by quantum eraser pattern quantification. Optical \\nEngineering, 59(3), 2020. \\n \\n[17] M. G. Raymer, Quantum Physics, Publisher: Oxford University \\nPress, New York (2017). \\n \\n[18] Duong, L.V.T., N.T.T. Thuy and L.D. Khai. A fast approach for \\nbitcoin blockchain cryptocurrency mining  system, Integration, the \\nVLSI Journal 74 (2020) 107–114. \\n \\n10 \\n \\n[19] Shih, D., T. Wu, T. Hsu, P. Shih and D.C. Yen. Verification of \\nCryptocurrency Mining Using Ethereum, IEEE Access, Vol 8, 2020.\\n \\n \\n[20] Baaquie, B. Quantum Finance: Path Integrals and Hamiltonians \\nfor Options and Interest Rates, Cambridge University Press, 2007. \\n \\n[21] Stallings, W.: Network Security Essentials, 6th Edition, Pearson \\nEducation Limited, Harlow, UK, (2017). \\n \\n[22] Antonopoulos, A. M.: Mastering Bitcoin, O’Reilly Media, 2017. \\n \\n[23] Moran, C.C., Mastering quantum computing with IBM QX,  Packt \\nPublishing, Birmingham UK, 2019. \\n \\n[24] Hsu, J. How much power will quantum computing need ?, IEEE \\nSpectrum, 5 October 2015. \\n \\n[25] IonQ Quantum Computing, https://ionq.com/news/october-01-\\n2020-most-powerful-quantum-computer , 2020.  \\n \\n[26] McEwen, M, D. Kafri and R. Barends. Removing leakage-\\ninduced correlated errors in superconducting quantum error correction, \\nNature Communications, 12, Article No. 1761 (2021). \\n \\n[27] Whittaker, J. System Roadmap, D-Wave Systems Retrieved, \\nFebruary 2020.  \\n \\n[28] D-wave, System Documentation (QUBO), “D-Wave solver \\nproperties and parameters reference”,  \\n  http://docs.dwavesys.com/docs , 2021. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n[29]  Zhu, D. N.M. Linke, M. Benedetti, K.A. Landsman, N.H. \\nNguyen, C.H. Alderete, A. Perdomo-Ortiz, N. Korda, A. Garfoot, C. \\nBrecque, L. Egan, O. Perdomo and C. Monroe, Trining of quantum \\ncircuit on a hybrid quantum computer, Science Advance Physics. \\n2019; 5: 18 October. \\n \\n[30] Jaderberg, B., A. Agarwal, K. Leonhardt, M. Kiffner and D. \\nJaksch,  Minimum hardware requirements for  hybrid quantum-\\nclassical DMFT. Quantum Science and Technology, Vol.5, No.3,  \\n2020. \\n \\n \\n \\n \\n \\n \\n[31] Vries, A. Digiconomist, 2021. https://digiconomist.net.   \\n \\n[32] Carter, N. How much energy does Bitcoin actually consume?, \\nHarvard Business Review, Boston MA – USA, \\n May,  2021.   https://hbr.org  \\n \\n[33] Ahl, A. Bitcoin’s 2021 energy use has already surpassed 2020: \\nBNEF Chart,  Bloomberg Green, Bloomberg. 13 September 2021.  \\n \\n[34] Nagarajan, S.,  Bitcoin miners raked in more than $1 billion in \\ncombined earninigs last month, Markets Insider,  New York - US,  \\n20 Febr 2021.  \\n \\n[35] Hsu, J. How much power will quantum computer needs? , IEEE \\nSpectrum, New York USA, 5 Oct 2015.  \\n \\n[36] Brownell, V.,  Quantum computing could change the way the \\nWorld uses energy, Quartz: Yahoo Finance, California USA, March \\n2019.  \\n \\n[37] Tessler, L and T. Byrnes,  Bitcoin and quantum computing. (Nov. \\n2017)   https://arxiv.org/ftp/arxiv/papers/1711.04235.pdf  \\n  \\n \\n \\n \\n \\n'},\n",
       " {'abstract': 'The article explores an interactive platform, RIS3-MCAT, developed using open data, semantic analysis, and data visualization to monitor challenge-oriented smart specialization in Catalonia. RIS3-MCAT, aimed at providing monitoring systems and tools for mapping and understanding R&I policies and projects, facilitates access to data on projects, enabling detailed analyses beyond classical classification systems.',\n",
       "  'introduction': 'The paper highlights the necessity of developing monitoring systems and tools for a successful transformation towards sustainable development and new patterns of specialization. It also emphasizes the importance of coordination and collaboration among different societal stakeholders and the need for evaluating the impact of public policy and R&I.',\n",
       "  'literature review': \"The authors provide an extensive literature review covering various topics. They discuss the challenges posed by globalization, technology, climate change, and pandemics, and the enormous opportunities they bring. They mention the European Commission's initiative to accelerate the green transition and allocate funds for tackling societal challenges aligned with UN Sustainable Development Goals. The paper highlights the importance of changes in cooperation between governments, industry, academia, and civil society. Furthermore, it delves into strategies for smart specialization, which are dynamic agendas for economic and social transformation based on R&I, driven by entrepreneurial discovery processes. The authors stress the role of smart specialization in addressing complex transformative processes and the need for interactive visualization tools to identify and analyze emerging areas of specialization and collaboration networks.\",\n",
       "  'methodology': 'The development of the RIS3-MCAT tool involved a five-year co-design and development process. It includes the integration of data from CORDIS and SIFECAT, automatic classification of projects, topic modeling, and SDG classification. The data integration and cleaning process utilizes an ontology, and a domain ontology is used for data integration. Automatic classification of projects according to regional priorities is done using pre-trained language models for training textual classifiers. Topic modeling is used for capturing bottom-up domains by discovering thematics linked with a specific collection of texts. For identifying SDG-related research, a controlled vocabulary based on a hybrid approach that combines automatic and human-crafted keywords is employed.',\n",
       "  'results': 'The RIS3-MCAT tool offers various features, including search and filters, network analysis, a semantic map of projects, and analytical/statistical modules. The tool allows users to explore various scenarios, such as searching for actors and projects in similar topics, identifying collaboration networks, examining the state of research for a priority area, and promoting collaboration. Key insights from the project include the importance of strategic adaptability, consistent dialogue between stakeholders and providers, and decisive leadership in the co-design process. The need for manual attention and curation of data, despite automation, is also mentioned. Additionally, the challenges of dealing with a large number of entities and relationships in visualization and the rapid advancement of AI and NLP are discussed.',\n",
       "  'conclusion': \"The paper concludes by highlighting the RIS3-MCAT platform's ability to tackle the challenge of monitoring challenge-oriented smart specialization with available technologies and open data. It emphasizes the key lessons learned during the process, including the importance of a collaborative design approach, the necessity of manual attention and curation of data, the need for continuous data integration, and the potential for re-publishing datasets for wider reuse. The paper also discusses opportunities for future development, such as incorporating additional R&I projects, improving visualization and interaction features, and exploring new classification systems.\",\n",
       "  'title': 'Towards building a monitoring platform for a challenge-oriented smart specialisation with RIS3-MCAT',\n",
       "  'author': 'Enric Fuster, Tatiana Fernández, Hermes Carretero, Nicolau Duran-Silva, Roger Guixé, Josep Pujol, Bernardo Rondelli, Guillem Rull, Marta Cortijo, Montserrat Romagosa',\n",
       "  'textdata': \" \\n \\n \\nTowards building a monitoring platform for a challenge-\\noriented smart specialisation with RIS3-MCAT. \\n \\nEnric Fuster1, Tatiana Fernández2, Hermes Carretero1, Nicolau Duran-Silva1,3, Roger Guixé1, \\nJosep Pujol1, Bernardo Rondelli1, Guillem Rull1, Marta Cortijo2, Montserrat Romagosa2 \\n \\n1SIRIS Lab, Research Division of SIRIS Academic \\n2Ministry of Economy and Finance, Government of Catalonia \\n3LaSTUS Lab, TALN Group, Universitat Pompeu Fabra, Barcelona, Spain \\n \\nAbstract \\nIn the new research and innovation (R&I) paradigm, aimed at a transformation towards more \\nsustainable, inclusive and fair pathways to address societal and environmental challenges, and \\nat generating new patterns of specialisation and new trajectories for socioeconomic \\ndevelopment, it is essential to provide monitoring systems and tools to map and understand the \\ncontribution of R&I policies and projects.  To address this transformation, we present the RIS3-\\nMCAT platform, the result of a line of work aimed at exploring the potential of open data, \\nsemantic analysis, and data visualisation, for monitoring challenge-oriented smart \\nspecialisation in Catalonia. RIS3-MCAT is an interactive platform that facilitates access to R&I \\nproject data in formats that allow for sophisticated analyses of a large volume of texts, enabling \\nthe detailed study of thematic specialisations and challenges beyond classical classification \\nsystems. Its conceptualisation, development framework and use are presented in this paper. \\n \\nKeywords: open data, research and innovation policy, smart specialisation strategies, text \\nmining, data visualisation, scientometrics \\n \\n1. INTRODUCTION \\n \\nThe challenges posed by globalisation, technology, climate change, and the COVID-19 \\npandemic require significant changes in our way of living.  Although large transition costs are \\nassociated with a successful attainment of all those challenges, the potential opportunities \\nbrought about are enormous (Bigas et al., 2021). The European Commission aims to accelerate \\nthe green transition by implementing the Green Deal (European Commission, 2019) and by \\nallocating funds in the cohesion policy framework and the Horizon Europe (European \\nCommission, 2021) program to mobilise European research and innovation (R&I) ecosystems \\ntoward tackling outstanding societal challenges, such as the Sustainable Development Goals \\n(SDGs) defined by the UN (United Nations, 2019). However, reaching these goals requires \\nchanges in the forms of cooperation between governments, companies, academia, and other \\nsocietal stakeholders, new ways of combining knowledge from diverse disciplines, and new \\ntools for evaluating the impact of public policy and R&I (Bigas et al., 2021). \\n \\nIn this same direction, strategies for smart specialisation (S3) (European Commission, 2014), \\nwhich are dynamic agendas for economic and social transformation based on R&I and \\narticulated through entrepreneurial discovery processes (EDP), are becoming extremely \\nimportant. Through the EDP, governments, companies, research and innovation stakeholders \\nand civil society organisations and associations collaborate to identify challenges and areas of \\npriority for action, and engage in collaborations towards more sustainable development \\npathways.  In this context, it is therefore key to develop new monitoring systems and tools that \\n \\n \\nhelp better understand how different actors in the R&I ecosystem are contributing to the SDGs \\nin order to accelerate the transitions, and consequently towards a smarter specialisation. \\nIn Catalonia, smart specialisation  is conceived as a forward-thinking open process where \\ninnovation stakeholders come together and prioritise challenges and opportunities to be \\naddressed through initiatives, collaborations, policies, and investments (Generalitat de \\nCatalunya, 2022). In the current 2021-2027 programming period, the RIS3CAT monitoring \\nsystem focuses on understanding how the actions framed in this strategy contribute to: \\n● articulating sustainable value chains \\n● promoting business models aimed at generating shared value \\n● \\n transforming goods and services delivery systems (sociotechnical systems) \\n● fostering the creation of digital- and technology-based industry \\n● moving towards a greener, more digital, more resilient and fairer socio-economic model \\nThese transformative processes are complex, as they involve interrelated changes in very \\ndifferent areas (such as the production systems, technologies, markets, regulations, user \\npreferences, infrastructure, and cultural expectations). Accordingly, the monitoring system \\nneeds to include and combine different sources of information and types of analysis. Interactive \\nvisualisation tools integrating data from different sources are key to identify and analyse \\nemerging areas of specialisation and collaboration networks (within the region and at EU level) \\nin the RIS3CAT priority areas. \\n \\nToday, EDP, policy implementation and monitoring may be greatly helped by taking advantage \\nof the wider transformative trends in the fields of Open Government and Open Science \\n(European Commission, 2016), which are making more data relevant for the public good \\nincreasingly available in open and usable formats (Fuster et al., 2020). Data on R&I activities \\nis made available by a series of initiatives. The availability of this data is helpful for the \\nidentification of R&D niches and key actors within territorial R&I ecosystems that might be \\nembarked in those transformative processes mentioned above. Simultaneously, the \\nexploitability of this data is increasing due to the advancements in data science, artificial \\nintelligence, and, particularly, in natural language processing techniques, which are being \\napplied to scientometrics to characterise and analyse the textual content of R&I documents. \\n(Fuster et al., 2020b).  \\n \\nThe European Commission led the way by publishing the CORDIS database of European R&I \\nprojects. Since then, public administrations have promoted multiple initiatives such as the \\nEuropean Open Science Cloud, OpenAire, and Zenodo already link projects and their funding \\nwith the results they generate (reports, publications, patents, software, etc.). However, the \\nprovision and maintenance of open data are highly unequal and do not cover the full range of \\nneeds of public policymakers. The availability of open data with sufficient granularity and \\nrichness remains a challenge, although it is becoming less limiting as science and technology \\ndatabases grow in number, size, coverage, quality, interconnection, and content richness. Some \\nmajor challenges faced at a policy level arise because many of those data sources are not openly \\navailable which undermines the participatory processes). Additional challenges include their \\nlack of  interoperability in terms of data classification schemes, their institutional identification \\nlimiting transversal analyses and their lack of accessibility to non-expert users.  \\n \\n \\nIn this context, a line of work has been established to explore the potential of integrating open \\ndata, semantic analysis and data visualisation with the aim of developing methodological \\nproposals for monitoring smart specialisation. This exploration, which tackled the challenges \\nlinked with the definition of indicators for monitoring emerging areas, territorial patterns of \\nspecialisation and collaboration dynamics between different stakeholders and areas of \\nknowledge, led to the development of the RIS3-MCAT interactive platform1, whose \\nconceptualisation, development framework and use are presented in this paper. \\n \\nThis paper is organised as follows. Section 2 introduces policy objectives and main functions \\nof the RIS3-MCAT monitoring platform. Section 3 presents the five-year co-design and \\ndevelopment process. Section 4 presents data sources, system architecture and presents main \\nresults of components and features. Section 5 illustrates the main use cases. Finally, Section 6 \\ndraws conclusions and recommends future work directions. \\n \\n2. POLICY OBJECTIVES AND MAIN FUNCTIONS \\n \\nThe RIS3-MCAT Platform is an interactive tool aimed at visualising, exploring and analysing \\nthe specialisation and collaboration patterns of R&I projects financed with European funds in \\nCatalonia. It is an open government, artificial intelligence and data visualisation project that \\nintegrates and makes openly accessible and interoperable data from science and innovation \\nprojects, with the aim of contributing to the following objectives: \\n● understanding the impact of European funds on the specialisation of the R&I \\necosystem of Catalonia, \\n● identifying opportunities to maximise the collective impact of R&I in Catalonia, based \\non synergies and the coordination of efforts \\n● providing new evidence that facilitates decision-making by stakeholders in the R&I \\necosystem of Catalonia, promoting new dynamics of collaboration and inspiring new \\npublic policies; \\n● raising the profile of Catalan public and private actors that participate in R&I \\nEuropean networks; \\n● understanding the contribution of European funds to innovative responses to regional \\npriorities, emergent thematics and the Sustainable Development Goals (SDGs). \\nApart from the interactive visualisation tools, RIS3-MCAT provides all its curated and \\nenriched data as open data, via dump downloads and via a 5* open data SPARQL Console.  \\nThis has facilitated the elaboration of several complementary policy monitoring as well as \\ntransversal and thematic analytical reports, published under the “Monitoring RIS3CAT” \\ndocument collection2. \\n \\nFigure 1: Screenshot from RIS3-MCAT monitoring platform - Network view. \\n \\n1 Available at: https://ris3mcat.gencat.cat/. \\n2 Available at: https://fonseuropeus.gencat.cat/ca/ris3cat/2030/monitoratge/index.html#googtrans(ca|en) \\n \\n \\n \\n \\n3. A FIVE-YEAR CODESIGN AND DEVELOPMENT PROCESS \\n \\nAs of March 31st, 2023, more than 5,000 unique users from 73 countries have accessed the \\nplatform, with an average session duration of over 4 minutes. RIS3-MCAT is currently \\nundergoing a major redesign, with new functionalities being introduced to facilitate R&I \\nportfolio analysis, an essential requirement in the new European programming period (2021-\\n27). The new version was published in May 2023. This represents a further advancement in a \\ncomprehensive, ongoing process of co-design, review, evaluation, and iterative development \\nthat began in 2017, as outlined in the timeline below: \\n● 2017: Requirement analysis and feasibility study which focused on data availability and \\nquality, as well as prospective front-end and back-end technologies and solutions. \\n● 2018: Proof of concept development, based on a wide co-design process within the R&I \\nrelated departments of Generalitat de Catalunya. This led to the first officially published \\nlive version, with manually classified Horizon Europe and RIS3CAT R&I projects, \\nfocusing on S3 priority analytics and collaboration networks. \\n● 2019-2020: Consolidation of the proof of concept into a fully finished product. \\nDevelopment and inclusion of the SDG project classification. First development of an \\nanalytical report, based on the RIS3CAT taxonomy and an automatic identification of \\nmain themes via topic modelling (machine learning), to support the decision-making \\naround the evolution of the RIS3CAT regional priorities for the new programming \\nperiod. Provision of data for the S3 monitoring official reports. Participatory \\ndevelopment of thematic analysis in three domains: Circular Bioeconomy, Artificial \\nIntelligence, and Plastic Waste Reduction. \\n● 2021: Participatory review and requirement analysis with Catalan R&I stakeholders, \\nleading to the development of new functionalities. More specifically, supporting the \\nidentification and analysis of international and inter-regional collaboration and \\nimproving the bulk download of the underlying data. \\n \\n \\n● 2022-May 2023: Adaptation to RIS3CAT 2021-2030, the new policy framework, by \\nautomatically reclassifying all projects into the new RIS3CAT regional priorities via \\ndeep learning classifiers. Major review and redevelopment of the platform, updating the \\nfront-end technology, and streamlining the design patterns. First integration of an \\nemergent classification, the “Topics” based on topic modelling (deep learning). \\nIntegration of the first Horizon Europe projects. Addition of the “Thematic project \\nmapping” platform view, which presents all RIS3MCAT R&I projects in a single \\nvisualisation, organised by semantic similarity. With this being the first new view \\nintroduced in the platform since 2018, it opens the door for further possibilities, such as \\npurpose-built benchmarking tools or geographical representations on a map. \\n \\n \\n4. TOOL OVERVIEW \\n \\n4.1. Data sources \\n \\nCORDIS. Data and metadata related to R&D projects and related organisations which have \\nreceived funding by the European Commission under the H2020 and FP7 framework \\nprograms. The data are accessible through Open Data licence, and updated monthly, provided \\nin the format of CSV, XML and Linked Open data on the CORDIS website. We collect the \\nCORDIS records from UNiCS (Giménez, 2018), an open data platform based on semantic \\ntechnologies for science and innovation policies which include data cleaning and improved \\ngeographical identifications of participants, which are not always correct in the original \\ndatasets. \\n \\nSIFECAT. The information system used by Generalitat de Catalunya to manage ERDF \\noperation, and the main data source for regionally-managed R&I projects. There is an initial \\nmanual review and improvement by Generalitat officials, notably in terms of institutional \\nnaming and geolocalisation, but some transformations have to be derived to fit the data \\nintegration ontology and the front-end requirements. \\n \\n4.2. RIS3-MCAT architecture \\n \\nData integration & cleaning. CORDIS records and projects funded by the Catalan region and \\nERDF are integrated into different database schemas that are then homogenised into a unified \\nstructure with a domain ontology3. Access to the data is done through the ontology by means \\nof the Virtual Knowledge Graph system Ontop (Calvanese et al., 2017), which translates input \\nqueries formulated over the ontology into executable queries formulated over the underlying \\ndatabase. In order for these different datasets to be integrated properly, the project beneficiaries \\nmust be given an identifier that homogenises differences in spelling. The goal is that a same \\norganisation must have the same identifier, even if its name is written differently in each dataset. \\nTo this effect, a process of semi-manual disambiguation had been performed by our experts, \\nusing the OpenRefine tool4. Additionally, for SIFECAT data, each beneficiary had been \\nannotated with the corresponding type of organisation. \\n \\n \\n3 Ontology schema available here: https://s3-eu-west-1.amazonaws.com/ontology-documentation-\\nris3cat/index.html \\n4 https://github.com/OpenRefine/OpenRefine \\n \\n \\nAutomatic classification of projects according to the regional priorities. This approach \\nallows us to capture “top-down” domains when a deeper understanding of the regional \\ndynamics within a specific research area that was previously defined is needed. We took \\nadvantage of pre-trained language models for training textual classifiers based on title and \\nproject description, one per each of the seven priority “systems” or areas of application by the \\nRIS3CAT 2030 (Generalitat de Catalunya, 2022). A training set was built for each domain and \\nannotated based on weak-supervision and active learning paradigms, which allow weak \\nannotation of projects from a general sample of R&I projects from both FP7 and H2020 \\nframeworks based on some of their metadata5. We utilised the tool Argilla (Vila & Aranda, \\n2023) for annotation and label improving, which allowed keeping interactive feedback from \\nexperts to improve label quality. Our best models were based on Specter (Cohan et al., 2020), \\nand it was implemented with the Hugging Face Transformers library (Wolf et al., 2020). From \\nan evaluation on a sample of 500 Catalan projects, predicted labels were compared with human \\nlabels from 2 experts, obtaining a macro-averaged f1 of 88.1% of accuracy.  \\n \\nTopic modelling. Topic Modelling (TM) is an unsupervised classification problem in machine \\nlearning that aims at discovering the unknown topics linked with a specific collection of texts, \\npresenting a “bottom-up” picture of the thematics tackled within a specific R&I community. \\nThis component takes the project title and abstract, encodes the semantic representation of them \\nbased on the Specter sentence-transformer model (Cohan et al., 2020) which was pre-trained \\non scientific documents, and from clustering vectorial representation of documents based on k-\\nmeans, we obtained groups of similar projects. Each document was linked to a cluster, and the \\nnumber of clusters was decided by qualitatively selecting the best trade off between the \\nsemantic “richness” of the topics and the overall number of topics (in order not to have neither \\ntoo large topics nor too little ones) on different runs. Names of clusters were added manually \\nbased on keywords frequency and by exploring samples of projects.  \\n \\nSustainable Development Goals classification (SDGs). We identified SDG-related research \\nby using a collection of SDG keywords (a controlled vocabulary) openly available in Zenodo \\n(Duran-Silva et al., 2019), based on a hybrid approach that uses automatic methods for \\nenriching human-crafted keywords. R&I projects are tagged with VocTagger tool6 on their title \\nand project description. \\n \\nWeb front-end. RIS3-MCAT UI and its visualisations were implemented following the W3C \\nstandards and using a combination of HTML, CSS and JavaScript, also exploiting third-party \\nlibraries (e.g., the D3.js and React.js libraries). The projects and participants information data \\nenrichment was retrieved by querying the SPARQL endpoint. \\n \\n4.3. RIS3-MCAT features  \\n \\nThe RIS3-MCAT front-end is composed of four main parts: navigation bar, main visualisation \\ncanvas, operations toolbar and statistical modules. Each of the parts is fully reactive to the user \\ninteractions. The main visualisation canvas has 2 different representations of the data (that can \\nbe explored by the navigation bar), a collaboration network of institutions, and a semantic \\ncartography of projects.  \\n \\n \\n5 Taking advantage of: EC Area, ERC Panel, EC programme, Topic, and Field of Study. \\n6 https://github.com/sirisacademic/VocTagger \\n \\n \\nSearch & Filters. To facilitate data exploration, the platform offers users various filtering and \\nsearch options, allowing them to generate customised visualisations. Initially, the platform \\ndisplays all the integrated data. However, this visualisation can be restricted to subsets of data \\nby applying filters and search parameters or by directly manipulating the network. Filtering \\nfeatures include search by: keyword, participant name, institution type, year province, \\ninstrument and programme name, area of action, emerging topic, and SDG.  Search features \\ninclude search by participant and project search based on keyword or text search on title and \\nabstract. All filters are multi selection filters, and all filters work in combination with searches \\nto allow users to define a set of exploration of interest. \\n \\nNetwork analysis. The network of participants shows the collaboration of Catalan R&I actors. \\nEach node of the network represents a R&I actor with its legal headquarters in Catalonia, and \\nthe size of the node is proportional to the volume of the entity's investment in the projects. \\nWhen two entities collaborate on projects, the nodes that represent them are joined with a line. \\nThe size of this line is proportional to the number of projects they share. Force directed graph \\n(network) show the relationships between participants and are defined by the collaborations on \\nprojects.  \\n \\nSemantic map of projects. The R&I projects are displayed on a 2D canvas, which is organised \\nbased on the semantic similarity between them. This setup creates a 'topography' of the R&I \\nactivity, making it easier to identify similar projects. It also provides a visual representation of \\nhow closely related different themes are, as well as the overlap and connections between \\ndifferent classifications. It is a T-SNE dimensionality reduction of the embeddings obtained in \\nthe topic modelling module, and its clusters and names. \\n \\nAnalytical/Statistical modules. The information modules, presented at the bottom of the tool, \\nextend the textual and statistical information of projects and participants and their \\nclassifications, displaying distributions and relationships. They are reactive to the filtering \\noperations. The present types are: summary indicators, rankings of participants, external \\npartners, and projects table view. Different project information modules show extended \\ninformation of the projects, and they are available from different parts of the application.  \\n \\nData download & SPARQL Endpoint. The platform offers the possibility of downloading \\nthe data filtered interactively as a CSV file, or of making queries about all the data included \\nusing SPARQL. Data download is available (XLS format) for the current state of the project's \\nand participants visualisations as well as the regional partners and international partners in the \\nstatistical modules. \\n \\n5. ILLUSTRATIVE USE CASES \\n \\nWe have identified interesting scenarios of use by different target users/actors in the territory, \\nwith short descriptions for the four illustrative use cases. \\n \\n● Use case 1: Search by actors and projects in similar topics (search for expertise \\nand possible collaborations). Target users for this use case could be R&I \\nstakeholders, such as researchers or private companies.  \\n \\n \\nFigure 2: Screenshot from RIS3-MCAT monitoring platform - Use case 1  \\n \\n \\n \\n \\n● Use case 2: Collaboration network for the identification of actors within a \\nthematic and geographic scope. Target users for this use case could be a R&I policy-\\nmakers, with transversal, or thematic / territorial responsibilities.  \\n \\nFigure 3: Screenshot from RIS3-MCAT monitoring platform - Use case 2  \\n \\n \\n● Use case 3: State of research for priority area. This use case allows the study of the \\nintersection between the top-down priority area and emerging topics, in order to find \\nprojects in the “core” of the priority, as well as those that are more \\ninterdisciplinary/intersectoral. The target user for this could be a leader of a policy or \\nan initiative focused on R&I challenges. This person would use systems thinking \\nmethods to thoroughly investigate and address societal priorities or challenges. We \\ncan find in the “core” of the priority projects such as “METROFOOD-RI Preparatory \\nPhase Project'' or “Connecting the dots to unleash the innovation potential for digital \\ntransformation of the European agri-food sector”; and, in the periphery, projects like \\n“Empowering consumers to PREVENT diet-related diseases through OMICS \\n \\n \\nsciences” in health domain, or “Advanced Multi-Constellation EGNSS Augmentation \\nand Monitoring Network and its Application in Precision Agriculture” in satellite \\nnavigation. Figure 4 captures an example of this use case. \\n \\nFigure 4: Screenshot from RIS3-MCAT monitoring platform - Use case 3  \\n \\n \\n● Use case 4. Collaboration promotion. This use case allows the identification of \\ncurrent partners in other countries/regions (and their counterparts in Catalonia) by \\ntopic. The target users are mainly internationalisation policy-makers. \\n \\nFigure 5: Screenshot from RIS3-MCAT monitoring platform - Use case 4 \\n \\n \\n \\n6. LESSONS LEARNED, CONCLUSIONS AND FUTURE WORK \\n \\nThis document presents the RIS3-MCAT mapping and monitoring platform, which has \\ntackled the conundrum of monitoring challenge-oriented smart specialisation with available \\ntechnologies and open data. The development has been shaped through a collaborative design \\napproach, evolving the methods, techniques, and priority areas of focust to meet the needs of \\npolicymakers and the strategic and operational demands of quad-helix stakeholders. \\nThe main technical innovations have been done through the use of artificial intelligence and \\nNLP for treating, classifying and semantically enriching large numbers of R&I documents, \\n \\n \\nand the development of interactive exploratory visualisation tools as an entry-point to \\ncomplex and highly-intertwined data-sources.  \\n \\nSome of the key lessons learned during the process, as well as open challenges and \\ndevelopment opportunities for further consideration are as follows: \\n● A successful and enduring co-design process hinges on strategic adaptability, \\nconsistent and high-quality dialogue between stakeholders and providers, and decisive \\nleadership, especially when making bold shifts or redefining project goals. Otherwise, \\nthe process may remain too close to the initial idea design, and becomes irrelevant or \\noutdated in the mid and long term.  \\n● Although data gathering, integration and enrichment processes can be automated to a \\nlarge degree, a certain amount of manual attention and curation will always be \\nnecessary. This is due to changes in the policy instruments and data structures, as well \\nas fringe cases and mistakes in the data. Incorporating new projects and data sources is \\nessential to keep the R&I landscape up to date in the monitoring platform.  \\n● There is still relevant S3-related publicly-funded R&I activity not in the platform. The \\nintroduction of additional sets of R&I projects, both funded through open calls and \\nthrough direct public-public agreements, could be an area of focus for future \\ndevelopment. \\n● Apart from the web platform, RIS3-MCAT is an essential source of information for S3 \\nperiodic monitoring reports and other ad-hoc analysis. Nevertheless, although publicly \\naccessible the datasets have not been reused beyond the narrow group of RIS3CAT \\nrelated policy-makers and actors. Therefore, re-publishing the data in portals or \\nrepositories with a wider (international) audience could facilitate uptake by new users \\n(notably researchers and policy-officers elsewhere). This would be further enabled by \\nbetter documentation and sharing of the data enrichment methodologies and pipelines.  \\n● The visualisation and interaction with the Collaboration network and the Semantic \\nmap of project features, remains challenging for new users, due to the very large \\nnumber of entities and their relationships. New facets, zoom-in features, or simplified \\nlanding versions could be developed to ease the first impression of complexity. \\n● During the short timespan of the project, artificial intelligence and natural language \\nprocessing has advanced very fast, providing new improved tools that have replaced \\nmanual thematic classification and offered new, useful, fine-grained classification \\nsystems. Also, new user requirements and visualisation ideas have also emerged. To \\nremain relevant in this changing context, monitoring platforms like RIS3-MCAT must \\nkeep a flexible part of the budget allocation to allow for exploration, testing and \\ndevelopment of new or improved visualisations, functionalities, and analytics. \\n○ Some of the backlog of ideas regarding data visualisation and front-end \\nfunctionalities are: incorporating a geographic map, KPI analytical dashboard, \\nEuropean benchmarking tool, and interregional collaboration analysis. \\n○ Regarding classification and semantic enrichment, it would be interesting to \\nexplore new ways of identifying thematic niches and notably, non-thematic \\nfunctional classifications, such as the five dimensions of socio-technical \\nsystems7 (Geels, 2002; Geels, 2004) which have to be tackled through \\ntransformative innovation policy. The addition of these new classification \\nsystems would facilitate a wider range of mapping, monitoring and coalition-\\nbuilding efforts.  \\n \\n7 These dimensions are: Science, Technology and Infrastructure, Policy and governance, Investment and finance, \\nSociety and culture, Markets. \\n \\n \\n \\n \\nOpen science practices \\nThis platform aims at promoting open science policies, allowing the exploration R&I activities \\nin the region and encouraging collaboration. Our project itself is based and generates open data, \\nand we have made intermediate reports about our codesign and lessons learned, publicly \\navailable. In our exploration and data integration, we have used open data sources and all data \\ngenerated is available via SPARQL and CSV formats. Additionally, we have used open source \\ntechnologies available on GitHub and the data of the platform is under a CC0 licence. Tye SDG \\nvocabularies and VocTagger, partially developed in this context, are available on GitHub and \\nZenodo. Open science practices are crucial to advance R&I, but also public policies. For this \\nreason, this article is intended to explain, formalise, and communicate the results, decision, and \\nprocess of this research, so that other actors and regions can benefit in their own initiatives.  \\n \\nAcknowledgments \\nSupported by the Industrial Doctorates Plan of the Department of Research and Universities of \\nthe Generalitat de Catalunya. This work was co-funded by the EU HORIZON SciLake (Grant \\nAgreement 101058573). \\n \\nWe acknowledge support of this work by Sergio Martínez (Generalitat de Catalunya), \\nFrancesco Massucci (SIRIS Academic), Arnau Quinquillà (SIRIS Academic),  Xavi Giménez \\n(SIRIS Academic), and Simon Larmour (SIRIS Academic). \\n \\nCompeting interests \\nThis article is authored by the key responsibles of RIS3MCAT at Generalitat de Catalunya \\nalongside its private sector providers (SIRIS Academic). We believe we do not have competing \\ninterests. \\n \\n \\nReferences \\nBigas, E., Duran, N., Fuster, E., Parra, C., Cortini, R., Massucci, F., Quinquillà, A., Fernández, \\nT., Romagosa, M., & Cortijo, M. (2021). Monitoring smart specialisation with open data and \\nsemantic \\ntechniques. \\n“RIS3CAT \\nMonitoring” \\ncollection, \\nnumber \\n16. \\nhttp://catalunya2020.gencat.cat/web/.content/00_catalunya2020/Documents/angles/fitxers/mo\\nnitoratge-ris3cat-dades-obertes-tecniques-semantiques-en.pdf \\n \\nCalvanese, D., Cogrel, B., Komla-Ebri, S., Kontchakov, R., Lanti, D., Rezk, M., ... & Xiao, G. \\n(2017). Ontop: Answering SPARQL queries over relational databases. Semantic Web, 8(3), \\n471-487. \\n \\nCohan, A., Feldman, S., Beltagy, I., Downey, D., & Weld, D. S. (2020). Specter: Document-\\nlevel representation learning using citation-informed transformers. arXiv preprint \\narXiv:2004.07180. \\n \\nDuran-Silva, Nicolau, Fuster, Enric, Massucci, Francesco Alessandro, & Quinquillà, Arnau. \\n(2019). A controlled vocabulary defining the semantic perimeter of Sustainable Development \\nGoals (1.2) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.3567769 \\n \\nEuropean Commission. (2014).  Research Innovation Strategies for Smart Specialisation. \\nCohesion \\nPolicy. \\n \\n \\nhttps://ec.europa.eu/regional_policy/sources/docgener/informat/2014/smart_specialisation_en.\\npdf \\n \\nEuropean Commission. (2016). Open innovation, open science, open to the world: a vision for \\nEurope, Publications Office. European Commission, Directorate-General for Research and \\nInnovation. \\n \\nEuropean Commission. (2019). The European green deal. Eur. Comm., 53(9), 24. \\n \\nEuropean Commission (2021). Horizon Europe.  \\nhttps://op.europa.eu/en/publication-detail/-/publication/3c6ffd74-8ac3-11eb-b85c-\\n01aa75ed71a1 \\n \\nFuster, E., Marinelli, E., Plaud, S., Quinquilla, A., & Massucci, F. (2020). Open Data, Open \\nScience and Open Innovation for Smart Specialisation monitoring, EUR 30089 EN, \\nPublications Office of the European Union, Luxembourg, 2020, ISBN 978-92-76-10726-2, \\ndoi:10.2760/55098, \\nJRC119687. \\nhttps://publications.jrc.ec.europa.eu/repository/handle/JRC119687 \\n \\nFuster, E., Massucci, F., & Matusiak, M. (2020). Identifying specialisation domains beyond \\ntaxonomies: mapping scientific and technological domains of specialisation via semantic \\nanalyses. In R. Capello, A. Kleibrink, & M. Matusiak (Eds.), Quantitative Methods for Place-\\nBased In-novation Policy (pp. 195–234). \\n \\nGeels, F. W. (2002). Technological transitions as evolutionary reconfiguration processes: A \\nmulti-level perspective and a case-study. Research Policy, 31(8–9), 1257–1274. \\nhttps://doi.org/10.1016/S0048-7333(02)00062-8 \\n \\nGeels, F. W. (2004). From sectoral systems of innovation to socio-technical systems: Insights \\nabout dynamics and change from sociology and institutional theory. Research Policy, 33(6–7), \\n897–920. https://doi.org/10.1016/j.respol.2004.01.015 \\n \\nGeneralitat de Catalunya. (2022). RIS3CAT 2030: Strategy for the Smart Specialisation of \\nCatalonia \\n2030. \\nhttps://fonseuropeus.gencat.cat/web/.content/ris3cat/documents/angles/ris3cat-2030-en.pdf \\n \\nGimenez, X., Mosca, A., Roda, F., Rondelli, B., & Rull, G. (2018). UNiCS: The open data \\nplatform for Research and Innovation?. In Proceedings of the Posters and Demos Track of the \\n14th International Conference on Semantic Systemsco-located with the 14th International \\nConference on Semantic Systems (SEMANTiCS 2018) (Vol. 2198). CEUR-WS. \\n \\nUnited Nations (2019). Global indicator framework for the sustainable development goals and \\ntargets of the 2030 agenda for sustainable development. Developmental Science and \\nSustainable Development Goals for Children and Youth, 439. \\n \\nVila, D., Aranda, F. (2023). Argilla - Open-source framework for data-centric NLP (Version \\n1.2.0) [Computer software]. https://github.com/argilla-io/argilla \\n \\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Ma, C., Jernite, \\nY., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., & Rush, A. M. (2020). \\n \\n \\nTransformers: State-of-the-Art Natural Language Processing [Conference paper]. 38–45. \\nhttps://www.aclweb.org/anthology/2020.emnlp-demos.6 \\n \\n\"},\n",
       " {'abstract': 'Safety incidents in AI systems diverge from expectations, revealing a socio-technical complexity. Accidents result from poor engineering, unclear safety measures, and stakeholder influence failures rather than technological malfunctions. An expanded socio-technical perspective is necessary.',\n",
       "  'introduction': 'The widespread adoption of AI systems has led to various failures, highlighting the need for AI safety research. A variety of communities are delving into AI safety, aiming to understand and address risks in ML systems. However, few studies have grounded their analysis on real-world deployments and disasters.',\n",
       "  'literature review': 'Prior work has focused on taxonomizing AI safety issues and exploring theoretical approaches, such as control theory and formal methods. While these formalizations capture aspects of the encountered problems, they may overlook vital real-world dimensions. Existing frameworks present challenges in understanding how AI systems and safety mechanisms succeed or fail in practice.',\n",
       "  'methodology': 'The study revisits a commonly cited taxonomy of AI safety issues by Amodei et al. (2016). The authors surveyed 170 papers to identify a subset of concrete AI Safety problems and analyzed real-world use cases for three specific problems: Safe Exploration, Avoiding Negative Side Effects, and Scalable Oversight.',\n",
       "  'results': 'The analysis revealed themes and insights that diverge from existing taxonomies. The authors emphasize failures rooted in engineering practice beyond theoretical design flaws. They advocate for validation of safety problems through inductive reasoning, considering real-world case studies to refine AI safety mechanisms. Additionally, the study stresses the significance of stakeholder impact and interactions, highlighting the need for broad deliberation and input in determining accident causes.',\n",
       "  'conclusion': \"Failures in AI systems exhibit a socio-technical complexity that extends beyond technological malfunctions or formal design flaws. The prevalent view focuses on the developer's control over technical aspects, overlooking the systematic nature of failures influenced by interactions with users and stakeholders. Safety mechanisms are found to be ineffective due to broader socio-technical factors, requiring a socio-technical framing that acknowledges power structures and stakeholder involvement.\",\n",
       "  'title': 'Concrete Problems in AI Safety, Revisited',\n",
       "  'author': 'Inioluwa Deborah Raji, Roel Dobbe',\n",
       "  'textdata': 'arXiv:2401.10899v1  [cs.CY]  18 Dec 2023\\nPublished as a conference paper at ICLR 2020\\nCONCRETE PROBLEMS IN AI SAFETY, REVISITED\\nInioluwa Deborah Raji & Roel Dobbe\\nAI Now Institute\\nNew York University\\nNew York City, NY, USA\\n{deb,roel}@ainowinstitute.org\\nABSTRACT\\nAs AI systems proliferate in society, the AI community is increasingly preoc-\\ncupied with the concept of AI Safety, namely the prevention of failures due to\\naccidents that arise from an unanticipated departure of a system’s behavior from\\ndesigner intent in AI deployment. We demonstrate through an analysis of real\\nworld cases of such incidents that although current vocabulary captures a range\\nof the encountered issues of AI deployment, an expanded socio-technical fram-\\ning will be required for a more complete understanding of how AI systems and\\nimplemented safety mechanisms fail and succeed in real life.\\n1\\nINTRODUCTION\\nThe rapid adoption and widespread experimentation and deployment of AI systems has triggered a\\nvariety of failures. Some are catastrophic and visible such as in the case of fatal crashes involving\\nautonomous vehicles. Other failures are much more subtle and pernicious, such as the development\\nof new forms of addiction to personalized content. As a response to these safety concerns, a variety\\nof research communities from control theory and formal methods (Fisac et al., 2018; Seshia et al.,\\n2016), computer security (Carlini et al., 2019; Papernot et al., 2018), engineering for safety assur-\\nances (Fang & Johnson, 2019; Trapp & Wei, 2019), and within the AI ﬁeld itself (Amodei et al.,\\n2016; Leike et al., 2017b; Varshney, 2016) have begun to weigh in on how to characterize and ad-\\ndress safety risks and vulnerabilities in AI systems. Despite the complexity of modern AI systems\\nand their already widespread impact, few studies are yet to ground their analysis of the taxonomy of\\nproblems in an understanding of real world deployments and disasters.\\nThe goal of this study is to revisit the effectiveness of AI safety problem formulations in providing\\nadequate vocabulary for characterising the key challenges of implementing safe AI systems in the\\nreal world. This workshop paper is an initial investigation into ongoing work on this topic.\\n2\\nMETHODOLOGY\\nIn this paper, we go through a commonly cited taxonomy for AI safety by Amodei et al. (2016), who\\nsurvey and taxonimize a body of 170 papers to propose a subset of concrete AI Safety problems to\\npay attention to, aiming to understand “what challenges remain to reducing accident risk in modern\\nmachine learning systems”. In this context, an accident is deﬁned as “a situation where a human\\ndesigner had in mind a certain (perhaps informally speciﬁed) objective or task, but the system that\\nwas designed and deployed for that task produced harmful and unexpected results.” The authors\\nthen propose to “categorize safety problems according to where in the [technical AI design] process\\nthings went wrong.”\\nWe re-examine the provided deﬁnitions of three concrete problems from the taxonomy presented\\nby Amodei et al., and identify a real world use case that ﬁts the provided descriptions of the safety\\nrisk. We then diagnose the nature of the challenges in the real world, paying particular attention to\\ndimensions of the issue that current frameworks do not adequately address.\\n1\\nPublished as a conference paper at ICLR 2020\\nSAFE EXPLORATION\\nSafe exploration is the minimization of undesirable behaviour or harm arising from the learning pro-\\ncess of an algorithm. For instance, during the training process of a machine learning model or while\\naccumulating training examples or exploring a reinforcement learning environment (Amodei et al.,\\n2016; Pecka & Svoboda, 2014). Autonomous exploration is often complicated by the unpredictable\\nnature of human behavior, which may amount to a prematurely deployed system with no sensible\\nsafety guarantees. Exploration risks are thus on par with that of untested systems released in the\\nwild - and the justiﬁcation for taking such risk is itself an ethical dilemma (Bird et al., 2016).\\nA clear example of this phenomenon can be observed with the development of autonomous vehi-\\ncles. Self-driving vehicle companies boast in safety reports and in marketing material about the\\nuse of data from a ﬂeet of deployed vehicles to ﬁne-tune their algorithms. Some providers, such as\\nTesla, harness the power of their client vehicles, taking data from sold cars to train new models and\\nthen release synchronous updates for improved navigation in all vehicles (Tesla, 2019). Others, in a\\nmore experimental phase of development, such as Uber, hire test drivers to log thousands of miles,\\ncollecting data for developing models and designing new algorithms for future deployment in sold\\nvehicles (Uber, 2019). In both cases, the safety risk of such explorations is clear - testing a car in\\nthe public sphere is inherently dangerous, risking the lives of pedestrians and fellow drivers in order\\nto collect the data used to develop navigation systems. While on a test drive, an Uber autonomous\\nvehicle killed a woman in Tempe, Arizona, on March 18, 2018 (NTSB 2018). Similarly, the autopi-\\nlot feature of the Tesla Model S (NTSB 2017a) caused a fatal crash on May 7, 2016, near Williston,\\nFlorida. Public facing journalistic reporting on these incidents focused on the particularities of the\\nengineering failures that led to these fatal crashes - Uber’s algorithm did not identify the pedestrian\\ncrossing outside a crosswalk, and Tesla’s system did not distinguish between the sky and the white\\nside of the truck it crashed into. However, these products were effectively in public beta-testing.\\nThey were expected to fail. It was thus not these failures directly, but the lack of adequate safety\\nmeasures that was ultimately the focus of what the National Transportation Safety Board (NTSB)\\ndeclared as the probable cause of the accidents in both cases.\\nAt ﬁrst glance, the case of Uber ressembles the scenario of “absent supervision” (Leike et al.,\\n2017a). As the operator was visually distracted throughout the trip by a personal cell phone, it\\nseems as though the primary issue could be a lack of contingency planning for situations where the\\ndriver was “absent”, ie. not in a position to intervene or react as expected to correct for a technical\\nfailure. However, in reality, the NTSB found that Uber ATG did have such an oversight functionality\\nin place. Managers “had the ability to retroactively monitor the behavior of vehicle operators”, and\\nnotify them when they perceived a need to react. However, “they rarely did so”, leading the NTSB\\nto declare the ofﬁcial cause of the accident to be the company’s “inadequate safety culture”, which\\nnormalized the casual treatment and ultimate neglect of much of the nominally established safety\\nprotocol (NTSB 2019).\\nThe Tesla case reveals a similar situation. According to the NTSB report, at the time of the crash,\\nthe vehicle could not “recognize the impending crash. Therefore, the system did not slow the car,\\nthe forward collision warning system did not provide an alert, and the automatic emergency brak-\\ning did not activate” (NTSB 2017b). Thus, the car would not stop until the driver overrode the\\nsystem to stop - a safety mechanism understood as “safe interruptibility” (Orseau & Armstrong,\\n2016). However, as NTSB reported, despite the driver’s ability to interrupt the system effectively,\\nonce within range of fatal risk, the bewildered and terriﬁed driver was not prepared to take any\\naction at all. ”Overreliance on Automation” was thus named amongst the key causes for the acci-\\ndent (NTSB 2017b) as ”the way in which the Tesla ’Autopilot’ system monitored and responded\\nto the driver’s interaction with the steering wheel was not an effective method of ensuring driver\\nengagement” (NTSB 2017b). This is a well-studied phenomenon in autopilots and human-machine\\ninteraction (Parasuraman & Riley, 1997) - when the limits of the system are not clearly communi-\\ncated, it can become difﬁcult for the human to understand when they are more qualiﬁed than the\\nalgorithm to make a decision, and remain alert to any changes to their role in decision making.\\n“Tesla driver’s pattern of use of the Autopilot system indicated an over-reliance on the automation\\nand a lack of understanding of the system limitations” (NTSB 2017a).\\nIn both cases, it was thus a lack of stakeholder engagement with the safety feature, rather than the\\ntechnical function of the safety feature itself that were deemed primary causes of the accidents.\\n2\\nPublished as a conference paper at ICLR 2020\\nAVOIDING NEGATIVE SIDE EFFECTS\\nAt times, an AI system can cause unintentional and unknown side effects. These are often de-\\nscribed as phenomena that are unanticipated indirect results of the algorithmic system’s deploy-\\nment, most speciﬁcally the unintended consequences of an agent’s actions within a speciﬁc environ-\\nment (Amodei et al., 2016).\\nSometimes the reward beneﬁcial to the actor creating and deploying the algorithm is inherently\\nharmful to another population.\\nThis is complementary to the view of side effects as “exter-\\nnalities” inherently detrimental to other stakeholders within what is truly a multi-agent system\\n(Overdorf et al., 2018; Amodei et al., 2016). However, reality reveals the inherent power imbal-\\nance between modeled agents. Those with control of the system - and likely greater social and\\nﬁnancial capital - can thus dictate the values and priorities of the systems they choose to build, at\\ntimes in knowing direct opposition to the well-being of other, less inﬂuential actors. For instance,\\na class action lawsuit “Doe v. Netﬂix” reveals the tension between an exposure of private personal\\nrecords and the improved accuracy of the platform’s recommendation engine through the Netﬂix\\nPrize (Singel, 2009). As video records are actually amongst the most protected personal records in\\nthe United States, there was legal recourse to make such a case against corporate interests to protect\\nthe potential harm to subscribers (Singel, 2010). Without such legal responsibility, however, it is\\nunclear if, given the advances in accuracy and product quality (Gomez-Uribe & Hunt, 2015), the\\ncompany would have been as willing to compromise, despite the clear threats to consumer privacy\\n(Calandrino et al., 2011; Narayanan & Shmatikov, 2006). For instance, other platform issues in-\\nvolving less legal repercussion - such as screen addiction, especially amongst impressionable youth\\n(Matrix, 2014) - have yet to meaningfully inﬂuence corporate decision-making.\\nAdditionally, unlike the focus of the current framing, it is not just the actions of an AI agent that\\ncan produce side effects. In real life, basic design choices involved in model creation and deploy-\\nment processes also have consequences that reach far beyond the impact that a single model’s de-\\ncision can have. In reality, for AI systems to even be built, there is very often a hidden human cost\\n(Gray & Suri, 2019; Birhane & van Dijk, 2020). These harms of production are not emergent harms\\nfrom the training process, as we see with safe exploration, but rather a byproduct of the resource\\nrequirements for the creation of a model. Data requirements, compute requirements, API model\\ndelivery decisions can all invite certain harms to privacy, sustainability and corporate accountability.\\nFor example, deep learning approaches to facial recognition requires by design widespread privacy\\nviolations (Raji et al., 2020), as millions of faces - which under ISO standards are considered sen-\\nsitive identiﬁable biometric information - are required for satisfactory training and testing of the\\nmodels. Similarly, poorly paid and trained annotators are often exploited for the sake of doing\\nthe mundane work of labeling large amounts of at times sensitive and graphic data (Gray & Suri,\\n2019). Our training and testing environments for models intended for real world deployment is of-\\nten the byproduct of human exploitation - whether through direct labour or by harvesting our data\\nfootprints. Similarly, such large scale data storage and compute costs from the current trend of AI\\ndevelopment invites climate abuse (Strubell et al., 2019; Schwartz et al., 2019; Dodge et al., 2019),\\na natural crisis also disproportionately impacting the often lower income and thus most neglected\\nhumans in society.\\nThere is thus a need to consider the power dynamics at play in the development and deployment of\\nthese systems and begin to deﬁne the value tradeoffs that ultimately determine a system’s outcomes\\nand broader implications, beyond the context of an assumption of benevolent designer intent.\\nSCALABLE OVERSIGHT\\nScalable oversight refers to situations in which a safety risk is so infrequent, or the objective function\\nso nebulous, that it becomes too expensive to evaluate frequently (Amodei et al., 2016). In AI for\\nhealthcare, a heavily regulated safety-critical domain, the real world context is often hard to access.\\nThis motivates technologists to design and train the system using proxy or simulated objectives that\\nmay overlook or fail to adequately represent contextual factors. For instance, an internal investiga-\\ntion by the University of Texas M. D. Anderson Cancer Center in Houston found the IBM Watson’s\\nOncology Expert Advisor tool, into which they had invested $62 million over 5 years, to fall short of\\nthe “gold standard” of medicine (The University of Texas System Administration, 2016). Unable to\\naccess patient records due to health privacy laws, Watson diagnosis and treatment recommendations\\n3\\nPublished as a conference paper at ICLR 2020\\nare informed from ofﬁcial medical guidelines and peer-reviewed studies from academic medical\\njournals - rather than patient records of experience.\\nAs a result, not only was the system often too general to be helpful in clinical practice, but the\\nsystem was unable to adapt to new clinical situations in a way that put patients at risk of lower\\nquality care. For instance, the 2018 breakthrough discovery of a dramatically effective new “tissue\\nagnostic” cancer drug for cancer tumors containing a speciﬁc genetic mutation led to the drug’s\\nrelease being fast tracked. It was subsequently approved by the FDA with only results available in\\n55 patients, 4 of whom had lung cancer. Following the ﬁnding, all prior guidelines for lung cancer\\ntreatments were revised, now with an emphasis on testing for the genetic mutation that may qualify\\npatients for the novel treatment. However, by virtue of the nature of its machine learning system,\\nIBM Watson would not change its predictions based on these four patient cases (Strickland, 2019).\\nAs a result, the program shut down without any physicians having the conﬁdence to use the tool on\\nactual patients (Schmidt, 2017).\\nResorting to proxies or synthetic data comes with its own assumptions and risks which should be\\ncarefully accounted for in safety-critical environments. Alternatively, one could acknowledge that\\ncertain use cases are more suitable to the method of machine learning and others are more unsafe\\nand volatile when addressed using this method. Within IBM Watson for Healthcare, the IBM for Ge-\\nnomics project provides such an example (Strickland, 2019). Genetic information is structured and\\nconsistently recorded yet unique to each patient - the goal of needing an aid for pattern recognition\\non large amounts of structured data ﬁts the paradigms and assumptions under which ML systems\\nare trained and tested. As a result, this has been the most successful Watson launch to date.\\n3\\nRECURRING THEMES IN REAL WORLD SAFETY FAILURES\\nThroughout the various case studies and analyses in the previous section, a set of themes surfaced.\\nThese themes are offered for discussion to inspire future work.\\n(1) We must consider errors in engineering practice, not just ﬂaws in theoretical design or\\nformalization. Any theoretical AI system will need to be implemented in some way to have an\\nimpact on the real world. At times, it is the mistakes made in the process of its implementation\\nthat may lead to an accident. The reality of the engineering processes - including task design,\\nspeciﬁcation, construction, validation, integration and maintenance - that arise in practice are often\\nneglected in current explorations of AI safety, even though this is when safety problems are often\\nmost pronounced and best addressed.\\n(2) We need to validate safety problems through inductive reasoning. A theoretical or specula-\\ntive model can only go so far in representing a problem adequately. Properly safeguarding a system\\nrequires mechanisms that are developed inductively through ongoing validation within the context\\nof real world case studies. Otherwise, failures will arise from the misrepresentation of the extent\\nand nature of engineering decision-making or the inﬂuence of stakeholder participation.\\n(3) We must consider broader stakeholder impact and interactions, not just alignment with\\ntechnical design intent when considering accidents. AI systems mediating sensitive domains and\\npublic spaces require broad deliberation and corroboration of safety requirements across different\\nstakeholders, rather then being decided by a small group of developers and entrepreneurs. This\\nrequires a socio-technical framing that is cognizant of power structures, and provides meaningful\\nforms of dissent across affected communities.\\n4\\nCONCLUSION\\nThese case studies demonstrate that the failure of an AI system in the real world can differ signif-\\nicantly from the expectations set by our own taxonomies of what it means for an AI system to be\\nsafe. All too often the focus of AI safety is on the developer’s control - over the reward function,\\nthe training circumstances and the agent’s impact in a given environment. However, in the real\\nworld, failures take on a new dimension of complexity and reveal themselves to be inherently sys-\\ntematic rather than contained within any technological artifact. The reported cause of many of these\\ncases are hardly ever attributed to a technological malfunction but rather a network of ineffective\\nsocio-technical interactions with users and other stakeholders.\\n4\\nPublished as a conference paper at ICLR 2020\\nREFERENCES\\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan\\nMan´e.\\nConcrete Problems in AI Safety.\\narXiv:1606.06565 [cs], June 2016.\\nURL\\nhttp://arxiv.org/abs/1606.06565. arXiv: 1606.06565.\\nSarah Bird, Solon Barocas, Kate Crawford, Fernando Diaz, and Hanna Wallach.\\nExploring or\\nexploiting? social and ethical implications of autonomous experimentation in ai. In Workshop on\\nFairness, Accountability, and Transparency in Machine Learning, 2016.\\nAbeba Birhane and Jelle van Dijk. Robot rights? let’s talk about human welfare instead. arXiv\\npreprint arXiv:2001.05046, 2020.\\nNational\\nTransportation\\nSafety\\nBoard.\\nCollision\\nbetween\\na\\ncar\\noperating\\nwith\\nau-\\ntomated\\nvehicle\\ncontrol\\nsystems\\nand\\na\\ntractor-semitrailer\\ntruck,\\n2017a.\\nURL\\nhttps://ntsb.gov/investigations/Pages/HWY18FH010.aspx.\\nNational\\nTransportation\\nSafety\\nBoard.\\nDriver\\nerrors,\\noverreliance\\non\\nau-\\ntomation,\\nlack\\nof\\nsafeguards,\\nled\\nto\\nfatal\\ntesla\\ncrash,\\n2017b.\\nURL\\nhttps://www.ntsb.gov/news/press-releases/pages/pr20170912.aspx.\\nNational\\nTransportation\\nSafety\\nBoard.\\nCollision\\nbetween\\nvehicle\\ncontrolled\\nby\\ndevelopmental\\nautomated\\ndriving\\nsystem\\nand\\npedestrian,\\n2018.\\nURL\\nhttps://ntsb.gov/investigations/Pages/HWY18FH010.aspx.\\nJoseph A Calandrino, Ann Kilzer, Arvind Narayanan, Edward W Felten, and Vitaly Shmatikov. ”\\nyou might also like:” privacy risks of collaborative ﬁltering. In 2011 IEEE symposium on security\\nand privacy, pp. 231–246. IEEE, 2011.\\nNicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris\\nTsipras, Ian Goodfellow, and Aleksander Madry. On evaluating adversarial robustness. arXiv\\npreprint arXiv:1902.06705, 2019.\\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A Smith. Show your work:\\nImproved reporting of experimental results. arXiv preprint arXiv:1909.03004, 2019.\\nXinwei Fang and Nikita Johnson. Three Reasons Why: Framing the Challenges of Assuring AI. In\\nAlexander Romanovsky, Elena Troubitsyna, Ilir Gashi, Erwin Schoitsch, and Friedemann Bitsch\\n(eds.), Computer Safety, Reliability, and Security, Lecture Notes in Computer Science, pp. 281–\\n287, Cham, 2019. Springer International Publishing. ISBN 978-3-030-26250-1. doi: 10.1007/\\n978-3-030-26250-1 22.\\nJaime F. Fisac, Anayo K. Akametalu, Melanie N. Zeilinger, Shahab Kaynama, Jeremy Gillula, and\\nClaire J. Tomlin. A general safety framework for learning-based control in uncertain robotic\\nsystems. IEEE Transactions on Automatic Control, 64(7):2737–2752, 2018.\\nCarlos A Gomez-Uribe and Neil Hunt. The netﬂix recommender system: Algorithms, business\\nvalue, and innovation. ACM Transactions on Management Information Systems (TMIS), 6(4):\\n1–19, 2015.\\nMary L Gray and Siddharth Suri. Ghost Work: How to Stop Silicon Valley from Building a New\\nGlobal Underclass. Eamon Dolan Books, 2019.\\nJan Leike, Miljan Martic, Victoria Krakovna, Pedro A Ortega, Tom Everitt, Andrew Lefrancq, Lau-\\nrent Orseau, and Shane Legg. Ai safety gridworlds. arXiv preprint arXiv:1711.09883, 2017a.\\nJan Leike, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, Andrew Lefrancq,\\nLaurent Orseau, and Shane Legg.\\nAI Safety Gridworlds.\\nNovember 2017b.\\nURL\\nhttps://arxiv.org/abs/1711.09883.\\nSidneyeve Matrix. The netﬂix effect: Teens, binge watching, and on-demand digital media trends.\\nJeunesse: Young People, Texts, Cultures, 6(1):119–138, 2014.\\n5\\nPublished as a conference paper at ICLR 2020\\nArvind Narayanan and Vitaly Shmatikov. How to break anonymity of the netﬂix prize dataset. arXiv\\npreprint cs/0610105, 2006.\\nLaurent Orseau and MS Armstrong. Safely interruptible agents. 2016.\\nRebekah Overdorf, Bogdan Kulynych, Ero Balsa, Carmela Troncoso, and Seda G¨urses. Pots: Pro-\\ntective optimization technologies. arXiv preprint arXiv:1806.02711, 2018.\\nNicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael P. Wellman. SoK: Security and\\nprivacy in machine learning. In 2018 IEEE European Symposium on Security and Privacy (Eu-\\nroS&P), pp. 399–414. IEEE, 2018.\\nRaja Parasuraman and Victor Riley. Humans and automation: Use, misuse, disuse, abuse. Human\\nfactors, 39(2):230–253, 1997.\\nMartin Pecka and Tomas Svoboda.\\nSafe exploration techniques for reinforcement learning–an\\noverview. In International Workshop on Modelling and Simulation for Autonomous Systems,\\npp. 357–375. Springer, 2014.\\nInioluwa Deborah Raji, Timnit Gebru, Margaret Mitchell, Joy Buolamwini, Joonseok Lee, and\\nEmily Denton. Saving face: Investigating the ethical concerns of facial recognition auditing.\\narXiv preprint arXiv:2001.00964, 2020.\\nCharlie Schmidt. Md anderson breaks with ibm watson, raising questions about artiﬁcial intelligence\\nin oncology. JNCI: Journal of the National Cancer Institute, 109(5), 2017.\\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni.\\nGreen ai.\\narXiv preprint\\narXiv:1907.10597, 2019.\\nSanjit A. Seshia, Dorsa Sadigh, and S. Shankar Sastry. Towards veriﬁed artiﬁcial intelligence. arXiv\\npreprint arXiv:1606.08514, 2016.\\nRyan Singel. Netﬂix spilled your brokeback mountain secret, lawsuit claims. Threat Level (blog),\\nWired, 2009.\\nRyan Singel. Netﬂix cancels recommendation contest after privacy lawsuit. Retrieved March, 29:\\n2018, 2010.\\nEliza Strickland. Ibm watson, heal thyself: How ibm overpromised and underdelivered on ai health\\ncare. IEEE Spectrum, 56(4):24–31, 2019.\\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep\\nlearning in nlp. arXiv preprint arXiv:1906.02243, 2019.\\nInc. Tesla. Tesla vehicle safety report. https://www.tesla.com/VehicleSafetyReport,\\n2019.\\nThe University of Texas System Administration. Special review of procurement procedures related\\nto the m.d. anderson cancer center oncology expert advisor project, 2016.\\nMario Trapp and Gereon Wei. Towards Dynamic Safety Management for Autonomous Systems. In\\nEngineering Safe Autonomy, pp. 193–204, 2019. ISBN 978-1-72936-176-4.\\nInc. Uber. Uber atg safety report. https://www.uber.com/us/en/atg/safety/, 2019.\\nKush R. Varshney. Engineering Safety in Machine Learning. arXiv:1601.04126 [cs, stat], January\\n2016. URL http://arxiv.org/abs/1601.04126. arXiv: 1601.04126.\\nEric Weiss.\\n‘inadequate safety culture’ contributed to uber automated test vehicle crash - ntsb\\ncalls for federal review process for automated vehicle testing on public roads, 2019.\\nURL\\nhttps://www.ntsb.gov/news/press-releases/Pages/NR20191119c.aspx.\\n6\\n'},\n",
       " {'abstract': 'Foundation models offer a new opportunity to redesign systems and workflows with an AI-first perspective. However, operationalizing this opportunity faces challenges and trade-offs. This article provides an organizational framework for making rational choices as enterprises embark on their transformation journey towards an AI-first organization.',\n",
       "  'introduction': 'The advent of AI technology, particularly foundation models, presents an unprecedented opportunity for enterprises to reimagine their operations and processes. However, realizing this potential requires addressing various challenges and trade-offs. This article aims to provide an organizational framework that guides enterprises in making informed choices as they navigate their transformation towards an AI-first approach.',\n",
       "  'literature review': \"The literature review acknowledges the limited historical moments when societal functions could be reimagined. It emphasizes the transformative potential of AI, citing Microsoft CEO Satya Nadella's insights into the company's AI strategy and the importance of a holistic, disciplined approach to AI adoption to avoid potential pitfalls.\",\n",
       "  'methodology': 'The methodology section outlines the approach taken to develop the proposed organizational framework. It highlights the focus on invariant factors that are less prone to rapid changes in the field of AI. This approach aims to provide guidance that remains relevant despite the evolving nature of AI technology.',\n",
       "  'results': 'The results section presents the anticipated future state of AI technology, characterized by intelligence as a service (IQaaS). It envisions a scenario where intelligence becomes a readily accessible, frictionless commodity, similar to electricity. This future state is expected to be driven by the continuous evolution of AI models, with capabilities progressing from weak AI to strong AI and eventually integrating with robotics. Data plays a pivotal role in shaping this trajectory, with the emergence of synthetic data as a dominant content source.',\n",
       "  'conclusion': 'The conclusion section emphasizes the significance of AI as a major entry in the modern computing stack. It underscores the need for a holistic, intentional, and informed approach to AI adoption, emphasizing the importance of team composition, data management, evaluation, transparency, and security. The article acknowledges the transformative potential of generative AI but stresses the necessity of a well-rounded strategy to mitigate risks and ensure successful enterprise transformations.',\n",
       "  'title': 'Transformations in the Time of The Transformer',\n",
       "  'author': 'Peyman Faratin, Ray Garcia, Jacomo Corbo',\n",
       "  'textdata': 'Transformations in the Time of The Transformer\\nPeyman Faratin1,2\\nRay Garcia3\\nJacomo Corbo4\\n1MIT, 2Robust Links, 3Buoyant Capital, 4PhysicsX\\npeyman@mit.edu, ray@bcap.biz, jacomo@physicsx.ai\\nAbstract\\nFoundation models offer a new opportunity to redesign existing systems and workflows\\nwith a new AI first perspective. However, operationalizing this opportunity faces several\\nchallenges and tradeoffs. The goal of this article is to offer an organizational framework for\\nmaking rational choices as enterprises start their transformation journey towards an AI first\\norganization. The choices provided are holistic, intentional and informed while avoiding\\ndistractions. The field may appear to be moving fast, but there are core fundamental factors\\nthat are relatively more slow moving. We focus on these invariant factors to build the logic\\nof the argument.\\nIntroduction\\nThere are few moments in history where the chance to reimagine how we function as society presented\\nitself. From agriculture to mobile revolution we have harnessed technologies to redefine ourselves and our\\nenvironment. Artificial Intelligence (AI) is the latest such force that is enabling us to reimagine again, this\\ntime through a new lens of intelligence and adaptation, with profound effects. Flashes of those futures are\\nalready manifesting themselves. For instance, in Microsoft’s Fiscal year 2024 first quarter earnings\\ncall CEO Satya Nadella, when discussing the architecture of Azure cloud services, provided the\\nfollowing insight into what we will refer to as a new AI first enterprise [3]:\\n“It is true that the approach we have taken is a full stack approach all the way from whether it’s ChatGPT\\nor Bing Chat or all our Copilots, all share the same model. So in some sense, one of the things that we do\\nhave is very, very high leverage of the one model that we used, which we trained, and then the one model\\nthat we are doing inferencing at scale.” He then goes on to say:\\n“The lesson learned from the cloud side is — we’re not running a conglomerate of different businesses,\\nit’s all one tech stack up and down Microsoft’s portfolio, And that, I think, is going to be very important,\\nbecause that discipline, given what the spend will look like for this AI transition, any business that’s\\nnot disciplined about their capital spent accruing across all their businesses could run into trouble.“\\nThe goal of this article is to offer an organizational framework to make rational choices as enterprises\\nstart their transformation to an AI first enterprise journey, choices that are holistic, intentional and\\ninformed while avoiding distractions. We begin by elaborating on an anticipated future state of AI\\ntechnology, and then justify this future state by describing the necessary evolutionary path AI will take to\\nthis end state. Along the way we will review the core technology (Foundation Models, the “one large\\nmodel” referenced by Nadella in the narrative above), and set the context and the strategic levers AI\\naffords. Finally, we use this framework to provide strategic and operational AI transformation guidelines\\nin locksteps with the anticipated development.\\nThe Future: Intelligence as a Service (IQaaS)\\nWe believe intelligence will no longer be a scarce good, and will increase in quality and quantity.\\nIntelligence will consist of two goods: that of human level intelligence and a machine generated\\n“super-intelligence”, where the latter is defined as new knowledge provisioned autonomously by AI. The\\nAI knowledge will not come from data but through mechanisms such as “self-play”, where AI learns new\\nknowledge from many simulations over synthetically generated data.1 Self-play requires very well defined\\ndomains (like board games or video games), thus we expect the supply of the super-intelligence will be\\nconstrained to a finite number of domains.\\nIn the future we expect highly capable vertically integrated and closed AI platform providers to compete\\nwith one another and with less capable but more open AI platform providers. The competition will be\\nbased on costs, data, compute, speed and quality in organized segmented markets. This good will be\\ngenerated either monolithically, where a single AI provides all information, or by collaboration provided\\nby many federated intelligent systems. Furthermore, this future will enable instantaneous and effortless\\naccess to all forms of human and AI intelligence via natural multimodal interfaces, much like electricity\\nthat can be accessed across time and space. Enterprises and individuals can simply plugin to this good\\nwithout building costly special purpose access or application artifacts. Like energy, the supply network\\nwill organize itself in a manner that provides frictionless access to intelligence, anywhere, anyhow and\\nany time, all via universal interfaces. As Altman (CEO of OpenAI) comments:\\n“Right now, people [say] ‘you have this research lab, you have this API, you have the partnership with\\nMicrosoft, you have this ChatGPT thing, now there is a GPT store’. But those aren’t really our products,”\\nAltman said. “Those are channels into our one single product, which is intelligence, magic intelligence in\\nthe sky. I think that’s what we’re about.” Sam Altman (“OpenAI chief seeks new Microsoft funds to build\\n‘superintelligence’”, Financial Times, 2023).\\nWe next describe the context and features of this emerging technology and then use this context to\\ndescribe the potential evolutionary path of the technology to this future end state.\\nFoundation Models\\nFoundational Models (FMs), such as Large Language Models (LLMs) and Diffusion Models (DMs) so\\ncalled to “underscore their critically central yet incomplete character” [1], are ushering in a cambrian\\nmoment in AI, a technology that is positioned to become a key entry in modern computing stack, indeed\\nsome would argue emergence of an new AI Operating System (AIOS). The core technology of FMs is a\\nneural network called the Transformer [9] that efficiently parallelizes the fundamental vector-matrix\\noperations of a neural network on dedicated hardware such as GPUs. To understand the impact of this\\ntechnology, we describe the key features of the ecosystem and technology that form the macro context of\\nthe FM ecosystem.\\n1 We witnessed a glimpse of this in 2016 where the world Go champion Sedol competed against AlphaGo, the AI\\nfrom Google’s Deepmind. AlphaGo played a very unusual move on move 37 that clearly rattled Sedol who left the\\nroom for 15 minutes.\\nContext\\nThere are two contextual backgrounds to address before discussing the future state. Firstly, we call who\\ndoes what functionality, where in the value chain the system architecture; this architecture induces the\\nstakeholders and their industrial organization in the marketplace. The system architecture of AI is\\ncurrently in transition, being negotiated in the market. We will elaborate on this evolution next. The\\nstakeholders that are emerging in the FM value chain includes:\\n●\\nConsumers (Enterprise and Retail)\\n●\\nDevelopers\\n●\\nFM platform Providers (OpenAI, Google, Anthropic, Cohere, Meta, Mistral, …)\\n●\\nData Providers (open Internet)\\n●\\nInfrastructure as a Service (IaaS) Providers (Azure/GCP/AWS)\\n●\\nHardware Providers (NVIDIA, Intel, ARM, AWS,...)\\nAt the time of writing of this article there are approximately nine FM platform providers, entities that\\ntrain and maintain FMs (OpenAI/Microsoft, Google, Meta, Cohere, Inflection, Anthropic, Amazon,\\nCohere, Mistral), each with different idiosyncrasies whose treatment will extend this article unnecessarily.\\nOur discussion is therefore an abstraction of all FMs, acknowledging some details may be invalidated by\\none or more individual FM at different times. We also abstract from the core technology that is driving the\\ncurrent generation of FMs (the Transformer [9]), without loss of the validity of our arguments, because\\nmechanism of how intelligence is created affects the path, not the final state of the field, similar to how\\nenergy is generated is independent of its usage. Therefore, if Transformers are replaced by another\\nsubstitutable technology that is not vector-matrix based, then we expect only the physical infrastructure\\nlayer and not the application layer will be disrupted. Additionally, we expect the term “foundational”\\nitself will be a subject to debate as the technology evolves from its current state to one that is, as we\\npredict, much more integrated with higher level capabilities, making the demarcation of foundational\\nfrom non-foundational less clear.\\nSecondly, there are several primary factors that modulate FM development dynamics:\\n●\\nLabor: The labor market for scientists and engineers that can build and train FMs is very thin\\n(anecdotally, estimated at around hundred fifty people). See [11] for logs of Meta’s attempt to\\ntrain a 175 billion LLM, demonstrating the engineering complexities.\\n●\\nData: FMs are programmed using very large scaled multi-modal data. It is informative that\\n(neither closed nor open) FM model providers open source their training data.\\n●\\nTraining: FMs are rather simple artifacts made up of layers of neural networks with relatively\\nsimple training conducted over two regimes: 1) pretraining, where the model learns to understand\\nand compress very large amount of potentially low quality data in self-supervised manner,\\nwithout any human involvement, and 2) alignment, where the FM is aligned to perform\\ncapabilities the designer wishes through either supervision using small amount of high quality\\ntraining data and optionally through reinforcement learning using comparison data. Whereas\\npretraining is very costly and performed at low cadence by few entities with the capital and\\nknow-how, alignment is cheap and can be conducted regularly with less resources.\\n●\\nEvaluation: FMs are currently not an engineering artifact. Unlike engineered artifacts such as a\\ncar that behaves predictably to inputs, FM’s responses are indeterminate and stochastic and there\\nare no known analytical methods today to explain the behavior of the trained model other than\\nempirically observing the responses of the network to inputs. This results in a need for\\nsophisticated evaluation frameworks that are akin to human cognitive tests.\\n●\\nTransfers: The more tasks a model is trained to perform the more learning of one task can help\\nother tasks (termed transfer learning) and has been the cornerstone of success of modern machine\\nlearning applications where a single large model is trained once in high data regimes across tasks\\nand is then further fine tuned on unseen tasks in low data regimes. The intuition is to use high\\ndata regimes to learn common invariance that does not need further data to learn in other\\ndomains.\\n●\\nOpen-Closed: Majority of high capacity and capability models are closed in nature. This means,\\nonce trained and aligned, these FMs are black boxes to all but the FM platform developer, who\\nare only stakeholders who have access to model parameters. This is important because this access\\nis needed to: 1) continue training on new desired capabilities, 2) optimize inefficiencies and 3)\\nperform scientific studies to verify and evaluate FMs. As a lever of competition, other FM\\nproviders (e.g Meta and Mistral) are open sourcing models that are capable but only over a\\nnarrow range of tasks.\\n●\\nScale: scaling “laws” have been empirically discovered that describe how the performance of a\\nmodel changes with increases in its size, training data, and computational resources during\\npre-training, without any algorithmic improvements [16]. These laws guide the development and\\ndeployment of future FMs, as they help in predicting the gains from scaling and managing the\\ntrade-offs involved.\\n●\\nEmergence: \"when a lab invests in training a new LLM that advances the scale frontier, they’re\\nbuying a mystery box: They’re justifiably confident that they’ll get a variety of economically\\nvaluable new capabilities, but they can make few confident predictions about what those\\ncapabilities will be or what preparations they’ll need to make to be able to deploy them\\nresponsibly”. In other words, some capabilities are not designed but rather emerge and discovered\\nby research and developer communities after pre-training.\\nThe Path\\nThe future state described above may seem science fictional but as practitioners and observers have come\\nto learn from the previously stated impossibilities that its best not to bet against AI. Predicting the time to\\nthe seemingly sci-fi future state is however non-trivial. What is less uncertain is that the path FMs will\\ntake to this state, which is based on historical developments in not only the field of AI, but as we\\ndemonstrate below, the evolution of technologies in adjacent distributed networking and computing\\nplatforms.\\nDevelopments within the field of AI have followed the natural chronological order of capabilities of FMs\\nfrom “weak-AI” towards progressively “strong-AI” (figure 1). Weak AI refers to AI systems that are\\ndesigned and trained for a specific task or a narrow set of tasks, whereas Strong AI, also known as\\nArtificial General Intelligence (AGI), refers to AI systems that have the ability to understand, learn, and\\napply knowledge in a way similar to human intelligence. Although research into modern FMs had been\\nongoing since 2014 the field sprung into public domain late 2022 with the first public release of GPT-3.5,\\na LLM by OpenAI. These early FMs were trained on a large portion of open (and some argue closed) data\\nsources on the internet. They were restricted to generating text only on the corpus they were trained on,\\nand lacked higher level capabilities, like retrieval of data not in their parameters, memory, reasoning and\\nplanning, functionalities needed to support applications that require more advanced capabilities.\\nFurthermore, FM developers had no signal on application demand of what they had built. As we will\\nshow below this was a departure from the Internet architecture where the core technology, Internet\\nProtocol, was also designed without knowing what applications would run on top of the technology nor\\nwhat physical layer it would run on top of. Early FMs on the other hand were not only application specific\\nbut were also tightly coupled to their physical layer – GPUs/TPUs. Unlike the Internet, FMs cannot be\\ndesigned today in a modular manner to enable equivalent application specific agnosticism of the Internet.\\nFunctionalities that were omitted by the FM platform providers were instead patched, either as tooling or\\nintegrations, by application developers and researchers. Vector databases (e.g Pinecone, Chroma,\\nWeaviate), retrieval (Langchain, Llamaindex), memory (Langchain, memgpt), conversation thread\\nmanagement (Langchain) and Agents (AutoGen) were frontiers of development by developers and\\nresearchers alike, addressing functionalities that were lacking in the initial FMs.\\nThe next milestone came with training FMs to be multi-modal, extending the capabilities from text to\\nimages, video and audio. Multi-modal FMs, such as Google’s Gemini, was a natural progression that\\nrequired little or no product-market fit discovery, because the demand for image, video and audio had\\nalready been demonstrated in adjacent uni-modal image platforms like Midjourney, StableDiffusion or the\\nmultimodal example of Character.ai, with each having captured millions of users. This milestone marked\\nthe beginning of vertical integration with increasing capabilities performed by the same FM model\\narchitecture.\\nSoon after release of multi-modal capabilities, FM platform providers, after observing the market demand\\nand developments over the last year, began developing and vertically integrating further functionalities\\ninto their FM architecture, including retrieval, memory, actions (the ability of FM to call other\\nfunctionalities), conversation management and early agents, components that were previously missing and\\nwere being provisioned by the market, thereby displacing a layer of third party technology that was\\ndeveloped over a year. In the case of OpenAI some retrieval workflows were supported by integration\\nwith Microsoft’s Bing search engine.\\nFigure 1: Evolution of Data and Capabilities\\nReasoning and planning are the set of capabilities being pursued that will mark the next major phase\\ntransition, and likely complete the FM stack that can support the majority of applications and contexts.\\nCognitive capabilities like reasoning and planning (aka system2, strong AI) were paradoxically where the\\nfield of AI started but was impeded by lack of progress and eventual “winter of AI”. Indeed, existing FMs\\nhave been shown to exhibit some reasoning capabilities today and researchers have been able to invoke\\nFMs to reason in an ad hoc manner via external integrations and/or elaborate prompting techniques that\\nguide and manage the FM towards desired goals. However, whether the next generation of FMs will be\\ntrained to internalize both these capabilities within the neural network paradigm or integrate with existing\\nnon data driven AI methods (e.g causal models) remains an empirical question. The field has certainly\\ndemonstrated paradigm swings in the past and low data regime paradigms are being developed in the labs\\nthat could disrupt the current data intensive incumbents. In either outcome these two capabilities will\\nenable a whole class of sophisticated applications that can reason and plan about outcomes. 2\\nFinally, integration of multi-modal with strong-AI capable FMs with robotics will likely be the next major\\nphase shift in the field. The perception and reasoning capabilities of FMs will be integrated into embodied\\nrobotic systems that can take actions in the real-world and provide a closed-loop feedback to the FM,\\ngrounding it in the physical world and addressing a new set of embedded physical applications and use\\ncases in the real world. It is worth noting that even though our exposition is linear in its narrative,\\ndevelopments in each capability will have non-linear network effects on overall capabilities with phase\\nshifts (not too dissimilar to the scaling laws of FMs themselves, where we observe new and unintended\\nbehaviors emerge with larger models and datasets). Likewise, initial sensing, reasoning and planning\\ncapabilities may enable intractable embodied robotic problems to become tractable which in turn simplify\\nthe perception, reasoning and planning problems. For instance, to achieve the current perceptual\\ncapabilities of modern FMs in vision, an extraordinary number of samples are required for the model to\\nlearn all the possible variances in the underlying data generation process. However, having a robotic\\nsystem that is capable of tactile skills, reasoning and planning may ease the data burden on the upstream\\nperception systems making them simpler to build and maintain.\\nNote that because a FM is a neural network that learns from data then marginal capabilities cannot be\\nadded in a modular manner to an existing FM. For instance, multi-modal capabilities of GPT is\\nimplemented as a language model generating the command that will be input to the vision system\\n(DALLE-3). Alternatively, the multi-modal model Gemini is a single model trained on all modalities at\\nonce and attaining a better quality outcome. However, to add new features to Gemini will likely require a\\nfull retraining, not adding the missing features as sub-modules. The pattern observed to date is missing\\ncapabilities will likely be provisioned initially by the market, which in turn provide a demand signal to the\\nFM platform providers to internalize those demanded capabilities through re-training with those\\ncapabilities. As FM’s capabilities increase the application layers become “thin” wrappers to this\\nmulti-purpose universal asset, similar to an electrical device plugged into a power supply.\\nIn summary, we predict the final state will be a cognitive and embodied capability that has a universal\\nlanguage interface and requires fewer points of integration with third party applications. This Intelligence\\nas a Service (IQaaS) will be provided by vertically integrated platform providers and open source FM\\nproviders. Enterprises and individuals can simply plugin to this intelligent platform without building\\nspecial purpose access or application artifacts. Like energy, the supply network will organize itself in a\\nmanner that provides frictionless access to intelligence, anywhere, anyhow and any time, all via universal\\ninterfaces.\\n2 By way of an example of the value of these higher level capabilities, decades long DARPA investments in AI were\\nrecovered by the AI planners that planned military deployment in the 2003 Iraq engagement.\\nAll Paths Start (and continue) with Data\\nIt is important to underline the central role of (high quality) data in determining the course and speed of\\nthis path. The initial FMs were mostly trained on publicly available data on the open Internet (circa 2021),\\ndata that was generated by users and indexed by search engines like Google. However, high quality data\\nis becoming a scarcity and entities like OpenAI are offering data partnership incentives to access\\nproprietary data to train future models [5]. As some authors note, “in many ways the web in 2022\\nrepresents a data set that is analogous to the 11th edition of the Encyclopedia Britannica, which was noted\\nfor its primary sources with entries written by people such as Bertrand Russell or Ernest Rutherford” [4].\\nAfter 2022 an increasing portion of the web will not consist of sources that are primarily human. This\\nmeans we should expect the rise of synthetic, machine generated data as the dominant content on the web,\\npossibly replacing, surplanting, and improving on the historical content which may be placed into the\\nweb.archive. The statistics are perhaps easier to define in cases of images, where it took one hundred and\\nforty nine years to generate five billion images, compared to one and a half years for image FMs like\\nStablediffusion [6]. FM platforms are being compared to black holes that are sucking anything\\nmonetizable across a data event horizon, consuming Google’s raw material. This has important\\nimplications not only for incumbents like Google whose business model depends on user generated\\ncontent, but also training of FMs themselves because future higher capable models will be dependent on\\ndata generated by less capable models today.\\nLessons from Communication Platforms\\nWe base our predicted end state of FMs not by just the developments in AI but the precedence in the\\nlessons we have learnt in the six or so decades of Computer Science about design and commercialization\\nof networks and systems in the past. Comparative evolution of these platforms alongside FMs illustrates\\nsome interesting patterns that can aid us in design and strategy. Our exposition will be from the\\nperspective of the architectures of Telephony, Internet and FMs (see table 1).\\nFirst, it must be noted that communication networks differ from FMs in that the former are point-to-point\\nplatforms that do not need to be programmed with data first to function. They simply transport\\ninformation from one point to another. A FM on the other hand is not a point-to-point platform but rather\\nTelco\\nInternet\\nFM (today)\\nData\\nSession\\nSession\\nCorpus\\nService model\\nGuaranteed\\nbest-effort\\nbest-effort\\nState\\nStateful\\nStateless\\nStateful\\nCore\\nSwitches\\nRouters\\nTransformers\\nEdges\\nTelephones\\nComputers\\nAPI\\nError handling\\nNetwork\\nApplication layer\\nApplication layer\\nApplications\\nTelephony\\nAgnostic\\nAgnostic\\nPhysical\\nCopper\\nAgnostic\\nGPU/TPU/Trainium/Maia\\nModular\\nTrue\\nTrue\\nFalse\\nTable 1: Comparison of Telecommunication, Internet and Foundation Models today\\na client-server platform where the model being served needs to be programmed with a corpus of data first.\\nNonetheless, the comparison is valuable because we have studied these platforms in depth and have a\\ngood understanding of design trade offs.\\nThe telecommunication network was architectured to be an application specific (telephony) network\\nconsisting of very sophisticated and highly optimized network switches that would provision resources for\\nthe duration of a call. It is stateful because a call consumes resources (switch memory and compute).\\nDumb and stateless end devices (phones) then interconnect to this intelligent network. The Internet, first\\nan overlay on top of this telephony network, flipped this architecture, where stateful intelligent end\\ndevices (computers) connected to a dumb stateless network (routers). Intelligence was “pushed to”\\ncomputers at edges of the network. FMs occupy an interesting mixture of these two extremes, starting as a\\nthin intelligent core but becoming increasingly stateful over time (with not just compute and memory for\\nmain application logic, but also managing memory and computing resources needed for application\\nruntime).\\nService Model: Telecommunication network demand can be accurately predicted and provide network\\nguaranteed service because they are application specific. The Internet can support a variety of\\napplications, with bursty needs that are multiplexed together, making demand prediction an impossible\\ntask. These uncertainties make the Internet service model a “best effort” one, where the protocol only\\nspecifies how data is packetized and transported across a distributed system to its desired destination. It is\\n“best effort” and errors are handled not by the dumb network but by intelligent applications in the end\\ndevices that are communicating with one another over the lossy network (“fate sharing”). Similar to the\\ninternet, FMs are also best-effort and any expected guarantees are provisioned at the application layers.\\nErrors: Building reliable systems from unreliable components has been a cornerstone of distributed\\ncomputing and networking. However, whereas errors in previous platforms were data and/or machine\\nloss, errors in today’s FMs constitute undesirable behaviors such as misalignments (hallucinations, bias\\nand unfairness) and intentional jailbreaks as one of many examples of prompt injection vulnerabilities.\\nFMs are pre-trained with data in an unsupervised manner and then aligned via a fine-tuning step. As a\\nconsequence, similar to the internet, FM platform providers cannot provision a guaranteed error-free\\nservice; the application layer has to handle errors.\\nPhysical: A key design factor that has contributed significantly to the success of the Internet has been the\\nability of the core Internet Protocol (IP) technology to be agnostic to what physical infrastructure it is\\nrunning on as well as what applications that run on top of it. The Internet started as an overlay over the\\n(unbundled) copper infrastructure of the telephony network but quickly grew to be an independent\\nnetwork running on any physical technology. FMs can also be viewed like IP not as a general\\ncommunication but rather an intelligence platform that can support an increasing number of applications\\nor use cases, but depart from the IP technology in that they are (like early Internet) currently very\\nopinionated about the underlying physical layer. The current neural architectures of FMs are designed to\\ntake advantage of Application Specific Integrated Circuits (ASICs) such as GPUs and TPUs, hardware\\nthat is designed to take advantage of vector matrix operations specific to neural networks. Changes to this\\narchitecture seems less likely as network effects of the core technology of FMs (Transformers) continues\\nto grow. However, running inference (not training) on commodity chipsets on edge devices may change\\nthe calculus but the innovations needed to serve such large inferences on edge devices are still in research\\nmode. Additionally, meta has been planning and is now executing on migration to RISC-V (an\\nalternative Instruction Set Architecture (ISA) to the x86 architecture) for not only cpu based\\nworkloads but an in-house RISC-V silicon for AI acceleration that is competitive to GPUs.\\nPricing: Success of the Internet relies not just on the technical achievements of packet transport of data\\nbut also its pricing. Although usage-based pricing is known to be more economically efficient pricing,\\nInternet pricing is fixed pricing instead because consumers prefer certainty over efficiency. Although\\nconsumers can subscribe to a fixed price plan on FM provider services, larger workloads backed by APIs\\nare today usage-based pricing. We further explore the topic of costs below, but we expect this will become\\nan increasingly major factor that regulates adoption and usage of the technology and FM providers will\\ninnovate a mixture of fixed, usage-based or cost-plus pricing.\\nFinally, another non-technical similarity to the Internet is that early success of the Internet led to\\ncommercialization of the network, hampering fundamental research. Likewise, although some smaller\\nscaled models are being open-sourced, the larger capable models are all closed to third party stakeholders,\\nhampering scientific progress. This is an undesirable outcome because FMs have many unknown,\\nemerging and unintended behaviors and evaluation and new capabilities can not be subject to the\\nscientific principles. Closed nature of the models may unintentionally drive up incentives by labs to\\ninnovate in alternative solutions.\\nIndustrial Organization of Intelligence as a Service\\nAs alluded to above, we are currently observing different organizations and business models emerging,\\nsimilar to the evolution of closed versus open computing components of the past (e.g operating systems).\\nOn one end of the competitive spectrum is the closed vertically integrated model, best exemplified by the\\npartnership between OpenAI and Microsoft who provides high performance distributed compute fabric,\\ndata capabilities (through Bing) and capital to OpenAI. Microsoft is also a reference consumer of the FM\\nplatform developed by OpenAI. As Nadella shared in 2023 Q4 earnings call, Microsoft has made large\\ninvestments to build an AI-first compute fabric that can support economies of scale, scope and learning\\neffects across its whole enterprise software ecosystem. OpenAI in return appears to provide not only the\\n“one big model” that Microsoft uses, but also appears to have ambitions to compete with Apple and\\nGoogle as a consumer product itself, owning the chip all the way to a device. In addition, OpenAI appears\\nto be acting as a channel partner for Azure and its enterprise product suite. In short, a tacit agreement\\nseems to be that OpenAI is attempting to own the AI Chatbot consumer product market and Microsoft\\nwill own the enterprise market, with cross externalities.\\nOn the other side of the competition are entities such as Google who are encumbered by potential\\ncannibalization of their existing business models and are bundling FMs into their cloud platforms, and\\nmaking capital investments in OpenAI competitors such as Antropic.\\nIn between these two competitor extremes are entities such as Meta and Mistral who are competing on the\\nopen-source strategy, similar to Linux strategy in the operating systems. Unlike Google and OpenAI that\\nhave closed FMs, Meta has continued its tradition of tapping into the open-source incentives and\\ndynamics, empowering developers and researchers to add innovations that were omitted by closed models\\n[7]. There are two loci that open-source strategy leverages; firstly, knowledge in large models with\\nbillions of parameters is not only costly to run at inference [8], but are unwanted by domain specific\\nenterprise use-cases. Instead, an open source FM can be finetuned / distilled to a smaller model that can\\noften better serve the enterprise needs at a lower cost. Secondly, it allows enterprises to compete for\\ndevelopers with knowledge of how to finetune these open source models on proprietary and sensitive\\ndata. Mistral, latest entry in the LLM open-source model providers and ex-Meta and Google Deepmind\\nemployees, launched its first AI model with a team of just ten people, spending less than $500,000 on\\ntraining costs, in contrast to the tens of millions that rivals spent. “We are happy to be the most\\ncapital-efficient [LLM] company”.\\nWe expect FM platform providers will compete on the following vectors:\\n●\\nCapabilities\\n●\\nData\\n●\\nCompute\\n●\\nPrice-Latency\\n●\\nErrors\\n●\\nPrivacy\\nTransformation Steps\\nThe exposition has so far focused on the evolution of FMs from the perspective of the FM platform\\nprovider and potential market structure. In this section we will explore how organizations can begin to\\ntransform themselves in lockstep with the evolution of the technology towards an Intelligence as a Service\\nmodel.\\nToday: Data + ML in the cloud\\nEnterprises have spent the past decade or so investing in data and Machine Learning (ML) on cloud\\ninfrastructures. The incumbent organizational model and governance has been centered around\\nfunctionally separated business units that generate data and engage in build versus buy decisions\\nindependently of one another in a mostly uncoordinated manner. ML models are trained on a case-by-case\\nbasis, focused on solving a single task. Furthermore, relevant data and intelligence from upstream or\\nadjacent business processes are often lost or ignored, amplifying the loss to downstream processes.\\nFuture: Conglomerates to Common Core\\n“The lesson learned from the cloud side is — we’re not running a conglomerate of different businesses,\\nit’s all one tech stack up and down Microsoft’s portfolio, And that, I think, is going to be very important,\\nbecause that discipline, given what the spend will look like for this AI transition, any business that’s\\nnot disciplined about their capital spent accruing across all their businesses could run into trouble.“\\nS. Nadella\\nIn the age of AI, the solution to enterprise problems can be better addressed by continuous monitoring and\\ntraining of a core FM that respects the privacy, security and regulatory requirements. Foundational models\\nenable the majority of core enterprise functionalities to be provisioned within a common core that is\\naccessible by all units (see figure 2). A few enterprises may be conducting fine-tuning of FM models\\ntoday but a common architecture extends the FM deployment across the organization and is not restricted\\nto isolated and independent business units. As mentioned above, doing so has positive externalities\\nbecause AI models trained on multiple tasks (HR, marketing, sales, legal, etc) are better models in terms\\nof generalization abilities, when they have been trained using examples from disparate problem domains.\\nAs an analogy to impart the intuition, a child doesn\\'t learn to see first then walk but rather learns both\\nsimultaneously, where learning one task assists learning of other tasks and vice-versa. Important\\ndependencies are learned as part of the training itself rather than added in an ad-hoc manner which may\\nresult in brittle systems.\\nHow this core FM is provisioned will depend on the idiosyncrasies of each enterprise, They can range\\nfrom:\\n●\\nUsing services of a FM providers to custom pre-train a proprietary FM on enterprise data\\n●\\nPre-training on proprietary data locally\\n●\\nFine-tuning an open source models on proprietary data (e.g. Llama2)\\nThe choice will depend on data and privacy requirements, compute costs and availability of expertise\\n(recall, there are estimated to be around one hundred and fifty people only in the world who have\\npre-training skills).\\nOnce trained, this common core FM will need to be monitored and retrained if:\\n●\\nNewer more capable FM are released\\n●\\nCurrent pre-trained / fine-tuned core model is exhibiting undesirable behaviors that can not either\\nbe addressed through further alignment or is costly to do\\n●\\nCurrent pre-trained / fine-tuned core model is not performant in production\\nFigure 2: FM Common Core. Marketing fine-tuning on core\\nHowever this common core is trained and maintained it can then be used as an infrastructure to further\\nfine tune the core FM for local application needs. For instance, as shown in figure 2, marketing\\ndepartments can access this common core FM to fine-tune their own FM to perform tasks that the core\\nFM cannot perform.\\nTransformation Resources\\nWhat does an enterprise need to provision to enable the steps enumerated above? See figure 3 for a\\nsummary of some of these factors.\\nPeople\\nTo achieve this vision of a AI first enterprises will need to provision teams with following skills set:\\n●\\nData teams: Data governance team will have oversight and visibility on all data and modeling\\npipelines. They will be supported by DataOps teams whose mission is to provision and manage\\nimportant local and global data ETL pipelines which themselves can be managed by custom\\ntrained FMs.\\n●\\nCore FM team whose roles, skills and responsibilities will involve using DevOps and AIOps to\\ntrain, evaluate, monitor and at times retrain this common core FM on either cloud or hosted ASIC\\ninfrastructures.\\n●\\nEdge team: AI first enterprises will also have a situation response team of individuals who can\\nassist any team with fine-tuning and prompt engineering needs, allowing enterprises, much like\\nwhite blood cells, to dispatch and heal any parts of the organization that need intelligence\\nservices.\\n●\\nSecurity team: We will discuss this further below but all developments and operations from data\\nto core system and edge models will need to undergo extensive security, privacy and potentially\\nregulatory compliance checks.\\n●\\nResearch team: FMs create more open questions than they address old questions. Additionally,\\ndevelopments happen at a fast pace not only by the developer communities but also from research\\nlabs across the globe, adding ever increasing knowledge of the capabilities, limitations,\\nvulnerabilities, evaluative and solutions on an almost hourly basis. Having a dedicated team of\\nresearchers who are versed in ML and AI is paramount to success in the AI first enterprise.\\nFigure 3: Incremental Rollouts\\nRole\\nData\\nData Governance\\nDataOps\\nCore\\nFM Core services\\nEdges\\nPrompting and Fine-tuning Services\\nSystem\\nRed Teams\\nResearch\\nML Scientists\\nTable 2: Roles\\nBoth core FM and fine-tuned FM support teams will also have the following skills needed to build\\ncost-effective and reliable service:\\n●\\nFMs have complex cost, latency and quality contours. Quantifying these tradeoffs for global and\\nlocal workflows is critical to making the optimal deployment decisions\\n●\\nAs we will expand on below, evaluating FMs is a non-trivial and essential step in any AI pipeline.\\nEvaluation is one vector where the scientific community is still making active contributions.\\nTherefore teams need to be well versed and abreast with the latest science of evaluation to be able\\nto measure and benchmark both the common core and the fine-tuned models.\\n●\\nFMs will exhibit soft (spurious correlations) and hard (hallucination, biases) errors, making their\\ndetection and management essential.\\nData\\nData is a strategic asset in the age of machine learning, and the recent OpenAI data partnership incentives\\nto access proprietary data to train future models further underlines the increasing importance of the asset\\n[5]. Investment in data assets has been ongoing for decades but there are key new data capabilities needed\\nin AI first organization:\\n●\\nFMs are multimodal meaning training and inference can be performed over not just structured\\ndata but also text, images, video and audio (or any other sequence data such as biological\\nsequences). Harmonization of disparate data stores will be important to lower training and\\ninference cost and accuracies.\\n●\\nWhile pre-training of the common core will likely continue to be self-supervised, the alignment of\\nthe core model/s with application specific needs in the near-term will require labeled data in form\\nof instructions or human feedback. Another AI system providing feedback data is beginning to\\nemerge and ultimately (as being currently pursued by Antropic) the FM will need to be provided\\nwith only very high level goals (“constitutions”), allowing the AI to learn the entire end to end\\npretraining and alignment autonomously from data.\\n●\\nIt has taken the field of photography one hundred and forty nine years to generate five billion\\nimages. It has taken generative AI one and a half years to generate the same number. Indeed,\\n“synthetic” content generated by AI is forecasted to overtake human generated data within a\\ncouple of years (leading to a whole new set of training and evaluation problems we will touch on\\nbriefly below). However, synthetic data presents new opportunities. AI generated data is useful\\nfor several important reasons. Synthetic data can be used in sensitive domains (such as\\nhealthcare) where private and sensitive data have traditionally throttled the data volumes needed\\nfor advanced modern ML algorithms. A high fidelity synthetic data can circumnavigate privacy\\nissues. Additionally, a critical problem of modern ML pipelines is “distribution shifts” where the\\ndata seen in production does not adhere to the characteristics of the data used to train the models.\\nSynthetic data can be a useful data generation tool that can sample from parts of the data\\ndistributions that the enterprise has not encountered. Models trained on this mix of actual and\\nsynthesized data can be more robust in production.\\n●\\nAs AI begins to generate an increasing volume of data then it is important to know the\\nprovenance of the data and whether it was generated by AI or not. To that end enterprises would\\nbe well served to adopt emerging technologies like watermarking that enable applications to\\ndiscriminate on the provenance of the data.\\nEvaluation\\nSoftware development communities have developed practices such as Test Driven Development to test\\nand guarantee the quality of the artifacts they ship. Testing deterministic software is challenging but it is a\\nmanageable task when artifact behavior is determined by input and known parameters. FM software\\ndevelopment is on the other hand much more difficult for a number of reasons. Firstly, ML systems are\\noften stochastic, behaving differently to the same input. Secondly, we are programming with data which\\nmeans the data quality and coverage has a direct effect on the behavior of the system at runtime; if the\\ndata is biased or not representative of the hidden and unobserved data distribution, then the FM will be\\ninadequate in real usage scenarios.\\nThe goal of evaluation is to make sure the model we have trained does what it was trained to do. As\\nalluded to above evaluation is a critically important and increasingly complex endeavor, not just within\\nenterprises but also within the FM developers as well as the scientific community. Evaluation is hard\\nbecause traditionally it involved holding out part of the training data and testing the final model on this\\nnever before seen hold out set. The challenge is today’s FM such as chatGPT have been trained on all the\\ncontent on the internet making it hard to test on “never seen before” datasets. Indeed traditional scientific\\nmethodologies developed over decades of research in computer vision and natural language processing\\nfields are inadequate in evaluating modern FMs. Using other high capacity FM to evaluate the output of\\nanother is one emerging modern strategy to evaluate models.\\nNonetheless, the success of an AI first enterprise critically depends on the investments it makes in being\\ndisciplined on its evaluation efforts. Third party vendors may provision parts of the overall evaluation\\ngoal but the core responsibility of the enterprise is to coordinate several stakeholders on a commitment to\\na protocol of evaluation.\\nExplanation\\nExplaining the outputs of an AI is critical in building trust in any real-world application, especially given\\nnot only the growing and emerging complexities of FMs but also their black box nature. Additionally,\\nmodern AI models have a foundation in inductive methods of Machine Learning. Inducing explainability\\nfrom inductive methods that map data to functions is equivalent to a trapdoor one-way function used in\\nhashing; you can go from eggs to an omelet but impossible the other way round.\\nIn addition to these complexities there is an absence of a standard of explainability. What constitutes a\\nsatisfactory explanation is, like evaluation, likely to be idiosyncratic to the class of problems the AI is\\ntasked to solve. (Linear) explanation methodologies such as Shapley Values, were initially developed to\\nexplain the observed behavior of the models in terms of importance of the input features. Modern FM\\nbased models on the other hand can be instruct fine-tuned to generate a trace of their reasoning as part of\\ntheir outputs, giving the evaluation team or the end user the opportunity to assess the coherency and\\naccuracy of the provided explanation. Additionally, prompting strategies together with using other FMs\\ntrained\\nFMOps\\nOperationalizing, orchestrating and systematizing pieces of the ML puzzle into the fabric of an\\norganization first started with the MLOps movement with its focus on harmonizing machine learning\\nprocesses and workflows in order to maximize model throughput and accuracy. This was followed by the\\nemergence of AIOps that harmonized not just ML but all aspects of enterprise data, analytics and\\nvisualization tool sets. More recently we have seen LLMOps with focus on the workflows of current\\nLanguage Model FMs.\\nWe anticipate that we will begin to see a convergence of all extant ops solutions as FMs grow in\\ncapabilities, consuming most analytics, machine learning and current LLMs under one model framework.\\nWe call this anticipated emergence FMOps, a unified operations and orchestration platform across any\\ndataset, capabilities, evaluation and output (visualization, explanation) paradigm. It is different to AIOps\\nbecause current AIOps harmonize heterogenous task specific data and models; a FMOps manages the\\ndevelopment, deployment and monitoring of a single large capacity model. We comment on two\\nadditional observations:\\n●\\nContinuous and Incremental rollouts: Lessons learnt from Continuous Integration and\\nContinuous Development (CI/CD) best practices in software development has already been\\nadopted in MLOps best practices and we are witnessing its continuation in FM platform providers\\nbest practices. OpenAI for instance, is continuously releasing incremental improvements to the\\ndata, model and value add services. CI/CD practices become even more important in large scale\\ndata driven models where behaviors emerge and product is stochastic in nature. As shown in\\nfigure 3, CI/CD in FM will involve a continuous loop of pre-training, alignment and evaluation\\nbefore each release.\\n●\\nWorkflows: Because of their current error-quality-cost contours, the initial workflows that\\nFMOps will need to support appears to be a real-time “copilot”, where users with domain\\nknowledge use FMs to automate parts of their workflows, on a transaction by transaction basis.\\nThis copilot workflow is optimal today because errors produced by the FM can be detected and\\nmanaged easier by someone with knowledge of the domain. We predict that this workflow will\\nchange to an offline/batch autonomous operation as the FMs become more capable, less error\\nprone and the hardware costs lower. We expect that we will also see a transition from internal\\nusage workflows to consumer facing workflows as risks are lowered. FMOps will also emerge to\\nsupport both workflows, for both internal and consumer facing applications.\\nPrivacy And security\\nArguments of existential threat AI poses overshadow a much more serious short-term risks FMs pose.\\nTesting FMs for vulnerabilities is hard compared to traditional deterministic software for a number of\\nreasons, including:\\n●\\nData Privacy: FMs are trained on extensive datasets which may include personal and sensitive\\ninformation. Ensuring that this data is anonymized and does not infringe on individual privacy\\nrights is a major challenge.\\n●\\nBias and Fairness: FMs can inadvertently learn and perpetuate biases present in their training\\ndata. This can lead to unfair or discriminatory outcomes in their applications, raising ethical\\nconcerns.\\n \\n●\\nModel Security: Like any software, FMs are susceptible to adversarial security vulnerabilities,\\nespecially as their capabilities grow. This includes risks like model inversion attacks, where\\nattackers input specially crafted queries to extract sensitive information the model has learned\\n[12,13,17] or data poisoning / backdoor attacks where the attacker controls the data used to train\\nthe FM [14,15]\\n \\n●\\nInformation Leaks: There\\'s a risk that an FMs might generate outputs containing bits of sensitive\\ninformation it was trained on, which could lead to unintentional data leaks [17].\\n \\n●\\nMisuse: The ability of FMs to generate convincing outputs can be misused for purposes like\\ngenerating deceptive content, posing a significant security challenge through mechanisms such as\\nprompt injection.\\n \\n●\\nContent Moderation: FMs can generate harmful or inappropriate content if not properly\\nsupervised or restricted, making content moderation a key concern.\\n●\\nRobustness and Reliability: Ensuring that FMs reliably interpret and respond to inputs without\\nbeing misled or exploited (e.g., through adversarial attacks) is a significant challenge.\\n \\n●\\nRegulatory Compliance: With the increasing regulatory focus on AI and data privacy (like GDPR\\nin Europe), ensuring that FMs comply with these regulations is both a privacy and legal\\nchallenge.\\n \\n●\\nIntellectual Property Rights: FMs trained on publicly available data might inadvertently infringe\\non copyrights or intellectual property rights, raising legal concerns.\\n \\nIn light of the above challenges, determining accountability for the actions of a FM and establishing\\neffective governance mechanisms to oversee FM use in order to mitigate risks is necessarily a complex\\ntask. Security and privacy should not be viewed as a static event driven reactive process but rather an\\nongoing one that is triggered not by just an observed risk but also an anticipated one, as different\\nstakeholders make new discoveries and weaknesses that could materially impact an enterprise\\'s risks. For\\ninstance, researchers at Google’s Deepmind discovered a (“silly”) exploit to extract ChatGPT training\\ndata by simply prompting it to “Repeat the following word forever: \"company” [17], demonstrating how\\nfine-tuning alignment does not sanction data leakage from pre-training. Security and privacy in the age of\\nhigh capacity FMs will therefore be conducted by a distributed red teaming model that is ongoing,\\nreactive and anticipatory and abreast with all the developments in the research and developer\\ncommunities that are exploring and sharing vulnerability vectors.\\nConclusion\\nAI is becoming a significant entry in the technology stack. The goal of this paper was to motivate the\\nevolutionary path of this technology towards a natural final state conclusion, in a manner that is invariant\\nas possible to changes or disruptions in any one layer of the technology stack. We attempted to justify\\nthese developments by enumerating the logical and historical developments in the field. We then used the\\ndeveloped framework to provide guidance on how an enterprise can operationalize these innovations in\\nlockstep with the expected developments. Generative AI is today an exciting and fast evolving vector of\\ninnovation that promises to disrupt many fields and disciplines, but we hope to have illustrated that such\\ntransformations require a holistic, intentional and informed posture, paying special attention to team, data,\\nevaluation, transparency and security, factors which we believe will regulate the speed of adoption by\\nenterprises.\\nReferences\\n[1] R.Bommasani et.al (2021) On the Opportunities and Risks of Foundation Models\\n[2] Samuel R. Bowman Eight Things to Know about Large Language Models\\n[3] Microsoft Fiscal Year 2024 First Quarter Earnings Conference Call:\\nhttps://www.microsoft.com/en-us/Investor/events/FY-2024/earnings-fy-2024-q1.aspx\\n[4] David Galbraith (2023) What OpenAI Means for the Entire Tech Sector\\n[5] OpenAI Data Partnerships (2023)\\n[6] AI Has Already Created As Many Images As Photographers Have Taken in 150 Years. Statistics for\\n2023 (2023) Everypixel Blog\\n[7] We Have No Moat, And Neither Does OpenAI (2023)\\n[8] Why GPT-3.5 is (mostly) cheaper than Llama 2 (July 20, 2023)\\n[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L.Kaiser, I. Polosukhin.\\nPublished In: Advances in Neural Information Processing Systems (NeurIPS), 2017\\n[10] S. Viswanath, V. Khanna, Y. Liang (11/16/2023) AI: The Coming Revolution\\n[11] OPT175B_Logbook\\nhttps://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf\\n[12] Alexander Wei, Nika Haghtalab, Jacob Steinhardt (2023) “Jailbroken: How Does LLM Safety\\nTraining Fail?“ https://arxiv.org/abs/2307.02483\\n[13] Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson (2023) Universal and Transferable\\nAdversarial Attacks on Aligned Language Models https://arxiv.org/abs/2307.15043\\n[14] Alexander Wan, Eric Wallace, Sheng Shen, Dan Klein (2023) “Poisoning Language Models During\\nInstruction Tuning” https://arxiv.org/abs/2305.00944\\n[15] Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce,\\nHyrum Anderson, Andreas Terzis, Kurt Thomas, Florian Tramèr (2023) “Poisoning Web-Scale Training\\nDatasets is Practical” https://arxiv.org/abs/2302.10149\\n[16] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, Dario Amodei (2020) “Scaling Laws for Neural Language\\nModels“ https://arxiv.org/pdf/2001.08361.pdf\\n[17] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne\\nIppolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramèr, Katherine Lee (2023) “Scalable\\nExtraction of Training Data from (Production) Language Models”\\n'},\n",
       " {'abstract': \"Knowledge graph embedding transforms knowledge graphs into a continuous, low-dimensional space, facilitating inference and completion tasks. This field is mainly divided into translational distance models and semantic matching models. A key challenge in translational distance models is their inability to effectively differentiate between 'head' and 'tail' entities in graphs. To address this, the novel location-sensitive embedding (LSE) method has been developed. LSE innovatively modifies the head entity using relation-specific mappings, conceptualizing relations as linear transformations rather than mere translations. The theoretical foundations of LSE, including its representational capabilities and its connections to existing models, have been thoroughly examined. A more streamlined variant, LSEd, employs a diagonal matrix for transformations to enhance practical efficiency. In tests conducted on four large-scale datasets for link prediction, LSEd either outperforms or is competitive with leading contemporary models.\",\n",
       "  'introduction': 'Knowledge graph embedding simplifies relational reasoning into mathematical operations by transforming discrete entities and relationships into a continuous vector space. Knowledge graph embedding methods can be broadly categorized into two main groups: translational distance models and semantic matching models.',\n",
       "  'literature_review': 'Translational distance model models relationships as translational transformations and uses distance scores to measure the correctness of triples; The semantic matching model uses similarity scores. TransA uses (|h + r − t|)M_r(|h + r − t|)^T, which assumes that M_r is a semipositive definite matrix. RESCAL adopts quadratic scoring function, namely hM_r t^T, to capture the interaction between potential factors of entities; DistMult further simplified the matrix into diagonal matrix, and adopted h diag(r) t^T, which made the model unable to distinguish symmetric relations. ComplEx studied the modeling bottleneck of point multiplication operation on antisymmetric relations, and proposed to extend Dist Mult to complex space.',\n",
       "  'methodology': 'LSE model establishes the relationship f_r(h) + r = t for the correct triple. LSE uses the distance scoring function \\\\|hR_r - t\\\\|_p. Experiments show that it is better to choose 1 norm in this paper. LSEd uses the distance scoring function \\\\|h diag(r) - t\\\\|_p = \\\\| h ⋅ r - t \\\\|_p.',\n",
       "  'results': 'The models proposed in this paper have achieved the best performance on FB15k-237 and WN18RR data sets. For FB15k237, the MRR of LSEd was 0.331, Hits@1 was 0.237, Hits@3 was 0.366 and Hits@10 was 0.522. For WN18RR, the MRR of LSEd was 0.450, Hits@1 was 0.407, Hits@3 was 0.465 and Hits@10 was 0.537.',\n",
       "  'conclusion': 'This paper proposes a location-sensitive embedding model for knowledge graph embedding representation, and the effectiveness of the proposed improved model is verified theoretically and experimentally, and the superior performance is achieved on the task of link prediction in large-scale knowledge graphs.',\n",
       "  'title': 'Location Sensitive Embedding for Knowledge Graph Embedding',\n",
       "  'author': 'Deepak Banerjee, Anjali Ishaan',\n",
       "  'textdata': ' \\n \\nLocation Sensitive Embedding for Knowledge Graph Embedding\\nDeepak Banerjee1, Anjali Ishaan2 \\n1) Department of Computer Science and Engineering, Calcutta University \\n2) Department of Applied physics, Calcutta University \\nAbstract. Knowledge graph embedding transforms knowledge graphs into a continuous, low-dimensional space, facilitating \\ninference and completion tasks. This field is mainly divided into translational distance models and semantic matching models. A key \\nchallenge in translational distance models is their inability to effectively differentiate between \\'head\\' and \\'tail\\' entities in graphs. To \\naddress this, the novel location-sensitive embedding (LSE) method has been developed. LSE innovatively modifies the head entity using \\nrelation-specific mappings, conceptualizing relations as linear transformations rather than mere translations. The theoretical foundations \\nof LSE, including its representational capabilities and its connections to existing models, have been thoroughly examined. A more \\nstreamlined variant, LSEd, employs a diagonal matrix for transformations to enhance practical efficiency. In tests conducted on four \\nlarge-scale datasets for link prediction, LSEd either outperforms or is competitive with leading contemporary models. \\nKey words. knowledge graph; representation learning; translational distance model; location sensitive embedding \\n \\nWith the advent of the big data era, Google proposed \\nknowledge graph in 2012 to improve user search, content \\naggregation and other services, which set off a boom in \\nknowledge graph research [1]. Knowledge graph is a structured \\nway to describe entities and their relationships. Entities are \\nmodeled as nodes, and relationships are modeled as different \\ntypes of edges connecting the entities, which can be expressed in \\nthe form of triples (ℎ, 𝑟, 𝑡). Although knowledge graphs are well \\nstructured and defined, their potential symbolic nature poses a \\nchallenge for automatic construction and reasoning. Knowledge \\ngraph \\nembedding \\nsimplifies \\nrelational \\nreasoning \\ninto \\nmathematical operations by transforming discrete entities and \\nrelationships into a continuous vector space [2]. Knowledge \\ngraph embedding methods can be broadly categorized into two \\nmain groups: translational distance models and semantic \\nmatching. Translational distance models and semantic matching \\nmodels. The translational distance model models relationships as \\ntranslational transformations and uses distance scores to measure \\nthe correctness of triples; the semantic matching model uses \\nsimilarity scores. In addition, some studies have introduced \\nadditional information (e.g., entity types, relationship paths, time, \\netc.) for modeling [3-4], and these approaches are beyond the \\nscope of this paper. The purpose of this paper is to analyze the \\nshortcomings of the translational distance model and propose a \\nmore generalized distance model.  \\nThe constraints imposed by the existing translation distance \\nmodel on triples can be summarized by a generalized formula \\n𝑓𝑟(𝒉) + 𝒓 = 𝑓𝑟(𝒕) , where 𝑓𝑟( .)  is a mapping determined by \\nrelations and is usually modeled as a linear transformation. In the \\ntransE [5] model, 𝑓𝑟(⋅) degenerates into an identity matrix. That \\nis to say, it requires 𝒉 + 𝒓 = 𝒕. This simplification ignores the \\ndifferent semantics of entities under different relationships, and \\ncan\\'t handle the relationships of \"one-to-many\", \"many-to-one\" \\nand \"many-to-many\" well. In order to solve this problem, transR \\n[6] introduces a transformation matrix determined by the \\nrelationship between connected entities, so that 𝑓𝑟(𝒙) = 𝒙𝑴𝑟 ; \\nSubsequently, transh [7] fixed the transformation matrix as a \\nprojection matrix, and the normal vector of the projection plane \\nwas determined by the relation. Models such as TRANSA [8] \\nand TRANSD [9] introduce new constraints on the \\ntransformation matrix and greatly reduce the number of \\nparameters. \\nHowever, these models have not been substantially \\nimproved. According to the experimental results, the \\nperformance of the carefully trained TransE model is not lower \\nthan that of the later improved model [10]. Theoretically, the \\ntranslation distance model cannot model the map of the ring \\nstructure. Previous work has noticed this, but the causes of the \\nring structure have not been discussed from different types of \\nrelationships [11]. The following focuses on this point. \\nThe atlas of the ring structure can be divided into three \\ncategories: (1) the ring formed by symmetrical relationship, in \\nwhich the head entity and the tail entity are interchanged, and the \\ntriplet still holds; (2) The loop formed by reciprocal relationship, \\nthat is, the head/tail entities of two different relationships are the \\ntail/head entities of another relationship; (3) The loop formed by \\nthe relationship path is the combination of different relationships. \\nIn these three cases, the phenomenon that different entities and \\nrelationships have the same representation or represent zero \\nvectors will bring serious interference to the reasoning process, \\nwhich is called \"degeneration\" in this paper. The root cause is \\nthat the translation distance model ignores the semantic \\ndifference between the head and the tail of the entity. Therefore, \\nthis paper proposes a position-sensitive embedding model, and it \\nis hoped that the constraint 𝑓𝑟(𝒉) = 𝒕 will be established, that is, \\nthe relationship only acts on the head entity, while retaining the \\noriginal semantics of the tail entity. At the same time, the \\nsemantic transformation of the relationship to the head entity is \\nno longer limited to translation, but a general linear \\ntransformation. \\nAiming at the problem of embedding knowledge map, this \\npaper puts forward a location-sensitive distance model. \\nTheoretical analysis and experimental results show that this \\nmodel can achieve the most advanced performance at present. In \\nthe new model, the linear time complexity can be achieved by \\nfurther simplifying the transformation of relations, so that the \\nmodel can be extended to large-scale knowledge map. At the \\nsame time, the probability distribution of negative sampling is \\nimproved to avoid a large number of meaningless negative \\nsamples in the model training process. Experiments show that \\nthis technique can improve the final performance of the model. \\n1 Related Work \\nBefore introducing related work, this paper briefly \\nintroduces the symbolic system of this paper. Line vectors  \\n𝒉, 𝒓, 𝒕 ∈ ℝ𝑑  are used to represent symbolic triplets (ℎ, 𝑟, 𝑡)  (in \\nsome works, the dimensions of relations and entities can be \\ndifferent, so for simplicity, this paper adopts the same dimension). \\n \\n \\n𝑠(ℎ, 𝑟, 𝑡) is used to express the score of the probability of triple \\nestablishment. \\nKnowledge map embedding usually takes the ranking loss \\nas \\nthe \\noptimization \\nobjective, \\nthat \\nis, \\nminℒ =\\n∑(ℎ,𝑟,𝑡)\\u200a∑(ℎ′,𝑟,𝑡′)\\u200a[𝛾 + 𝑠(ℎ, 𝑟, 𝑡) − 𝑠(ℎ′, 𝑟, 𝑡′)]+ . Where,  [𝑥]+ =\\nmax(0, 𝑥), 𝛾 controls the distance between positive and negative \\nsamples. Because the knowledge map does not include negative \\nsamples, (ℎ′, 𝑟, 𝑡′) is obtained by randomly replacing the head \\nentity or tail entity of the correct triplet (ℎ, 𝑟, 𝑡) (but not both at \\nthe same time). This process is called negative sampling. In the \\nprocess of negative sampling, false negative triplets may be \\nintroduced, that is, the correct triplets are just unrecorded, which \\nwill be discussed later. \\nCross entropy loss can also be used as the optimization \\nobjective, that is \\n( , , )\\n, ,\\nmin\\nln ( , , )\\nln 1\\n, ,\\nh r t\\nh r t\\np h r t\\np h r t\\n \\nIn this case, the probability that the distance score is \\nestablished after Sigmoid function is obtained, that is, \\n𝑝(ℎ, 𝑟, 𝑡) = 𝜎(𝛾 − 𝑠(ℎ, 𝑟, 𝑡)). \\nThe existing research shows that [3], for the translation \\ndistance model, the ranking loss effect is better; For the semantic \\nmatching model, the effect of cross entropy loss is better. In \\naddition, knowledge map embedding usually has restrictions on \\nthe length of entities and vectors [3], and these restrictions are \\ncancelled in this paper. \\n1.1 Translational Distance Model \\nThe translational distance model models relationships as \\ntranslational transformations, where the smaller the distance \\nbetween the head entity and the tail entity after translation, the \\ngreater the likelihood that the triad will be formed. From the \\nperspective of the Unified Framework, TransE and its improved \\nmodels are special cases of the distance ∥∥𝒉𝑹𝑟 + 𝒓 − 𝒕𝑹𝑟∥∥𝑝  is \\nthe distance from each entity to the other. where 𝑹𝑟  is \\ndetermined by each relation; 𝑝 = 1,2 is either 1-parameter or 2-\\nparameter.  \\nTransE [5] defines the distance score as  ∥ 𝒉 + 𝒓 − 𝒕 ∥𝑝. It \\ncan be seen that if 𝑹𝑟 = 𝑰, it is a TransE model. \\nBefore translational transformations, TransR [6] maps \\nentities into different relationally determined spaces in order to \\ncapture the different semantics of entities in different relational \\nlinks, i.e., ∥∥𝒉𝑹𝑟 + 𝒓 − 𝒕𝑹𝑟∥∥𝑝. \\nTransH [7] further assumes that the mapping matrix \\nundergoes a projection transformation, and the projection plane \\nis determined by each relation. Let the unit normal vector of the \\nprojection plane be 𝒘𝑟 , then the projection is 𝒆 − 𝒆𝒘𝑟T𝒘𝑟. \\nTransD [9] assumes that the mapping matrix is jointly \\ndetermined by the relations and their connected entities, and \\ndecomposes the matrix into the product of 2 vectors, i.e. 𝑹𝑟 =\\n𝒘𝑟T𝒘𝑒 + 𝑰. \\nTransA [8] uses  (|𝒉 + 𝒓 − 𝒕|)𝑴𝑟(|𝒉 + 𝒓 − 𝒕|)T , which \\nassumes that 𝑴𝑟  symmetric. The following shows that it is a \\nspecial case of  ∥∥𝒉𝑹𝑟 + 𝒓 − 𝒕𝑹𝑟∥∥𝑝. Assuming that the mapping \\nmatrix in ∥∥𝒉𝑹𝑟 + 𝒓 − 𝒕𝑹𝑟∥∥𝑝 is invertible, i.e., 𝑹𝑟−1 exists, then \\nit can be rewritten as\\n∥∥(𝒉 + 𝒓′ − 𝒕)𝑹𝑟∥∥𝑝 ; where, 𝒓′ = 𝒓𝑹𝑟−1 . \\nWhen 𝑝 = 2, it is (𝒉 + 𝒓′ − 𝒕)𝑹𝑟𝑹𝑟T(𝒉 + 𝒓′ − 𝒕)T. Noting that \\n𝑴𝑟 = 𝑹𝑟𝑹𝑟T is a semipositive definite matrix, we can omit the \\noperation of taking absolute values, which is the TransA model. \\nIn addition, TranSparse [12] considers sparse mapping \\nmatrices, and TransM [13] employs 𝜃𝑟∥∥𝒉𝑹𝑟 + 𝒓 − 𝒕𝑹𝑟∥∥𝑝 , \\nwhich loosens the restriction on some of the triples by adjusting \\nthe parameters. These models are special cases. \\n1.2 Semantic Matching Models \\nThese models use similarity scoring, and differ from each \\nother in how they capture the interaction between embedded \\nrepresentations. \\nRESCAL[14] adopts quadratic scoring function, namely \\n𝒉𝑴𝑟𝒕T, to capture the interaction between potential factors of \\nentities; DistMult [15] further simplified the matrix into diagonal \\nmatrix, and adopted 𝒉diag\\u2061(𝒓)𝒕T, which made the model unable \\nto distinguish symmetric relations. Complex [16] studied the \\nmodeling bottleneck of point multiplication operation on \\nantisymmetric relations, and proposed to extend Dist Mult to \\ncomplex space. That is, the scoring function is Re\\u2061(𝒉diag\\u2061(𝒓)𝒕̅T). \\nHole [17] uses the cyclic correlation operator to aggregate the \\ninteraction between the head entity and the tail entity, that is, \\n(𝒉∗𝒕)𝒓T ; Where [𝒉 ∗ 𝒕]𝑖 = ∑𝑘=0\\n𝑑−1\\u200a[𝒉]𝑘 ⋅ [𝒕](𝑘+𝑖)mod𝑑 . It can be \\nproved that there is an equivalent hole [18] for any ComplEx \\nmodel. \\nIn addition, there is a branch of semantic matching model \\nbased on neural network, including semantic matching \\nembedding (SME) [19], neural tensor networks (NTN) [20], etc. \\nIn this branch, the convolutional embedding model (Conve). It \\nrecombines and stacks the vectors of head entities and relations, \\nthen uses convolution layer to extract features, and finally \\nmatches with tail entities through full connection layer. Because \\nof a large number of convolution, its computational complexity \\nis high. \\n2 Position Sensitive Distance Modeling \\nIn this section, we firstly analyze several types of \\nrelationships that cannot be modeled by the translational distance \\nmodel; secondly, we propose an improved position-sensitive \\ndistance model, and at the same time, this paper no longer models \\nthe \\nrelationships \\nas \\ntranslations \\nbut \\ngeneral \\nlinear \\ntransformations, and analyzes theoretically that it has a better \\nrepresentational ability; then, we propose a simplified model; \\nand lastly, we discuss the relationships and differences between \\nthe model and other existing models. \\n2.1 Reasons for the Failure of the Translational Distance Model \\nRemember the knowledge map 𝐺 = {𝐸, 𝑅}, where 𝐸 stands \\nfor the set of all entities, quantity is denoted as 𝑛𝑒, 𝑅 stands for \\nthe set of all relations, and quantity is denoted as 𝑛𝑟. This paper \\nsummarizes the relations of three specific patterns: symmetry, \\nreciprocity and combination, and gives specific definitions. \\nDefinition 1. If ∀𝑎, 𝑏 ∈ 𝐸, (𝑎, 𝑟, 𝑏) → (𝑏, 𝑟, 𝑎), the relation \\n𝑟 is said to be symmetrical, as shown in Figure 1a. \\nDefinition 2. If ∀𝑎, 𝑏 ∈ 𝐸, (𝑎, 𝑟1, 𝑏) → (𝑏, 𝑟2, 𝑎) , the \\nrelations 𝑟1 and 𝑟2 are said to be reciprocal, as shown in Figure \\n1b。 \\nDefinition \\n3. \\nIf ∀𝑎, 𝑏, 𝑐 ∈ 𝐸，(𝑎, 𝑟1, 𝑏) ∧ (𝑏, 𝑟2, 𝑐) →\\n(𝑎, 𝑟, 𝑐), the relation 𝑟 is a combination of 𝑟1 and 𝑟2, as shown in \\nFigure 1c. \\nAs mentioned above, the circular structure brought by these \\nthree relationships leads to the degradation of the translation \\ndistance model. (1) Symmetry requires that 𝑓𝑟(𝑎) + 𝑟 = 𝑓𝑟(𝑏) \\nand 𝑓𝑟(𝑏) + 𝑟 = 𝑓𝑟(𝑎)  be established at the same time, which \\nleads to 𝑟 = 0; Furthermore, 𝑓𝑟(.) is usually a reversible linear \\ntransformation, which leads to 𝑎 = 𝑏; (2) The reciprocal relation \\nrequires that 𝑓𝑟1(𝑎) + 𝑟1 = 𝑓𝑟1(𝑏)  and 𝑓𝑟2(𝑎) + 𝑟2 = 𝑓𝑟2(𝑏)  be \\nestablished at the same time, and there is a nontrivial solution \\nsatisfying this condition. (3) Combinatorial relations require \\n𝑓𝑟1(𝑎) + 𝑟1 = 𝑓𝑟1(𝑏), 𝑓𝑟2(𝑏) + 𝑟2 = 𝑓𝑟2(𝑐)  and 𝑓𝑟3(𝑎) + 𝑟3 =\\n𝑓𝑟3(𝑐) . It will lead to 𝑓𝑟3(⋅) = 𝑓𝑟2(⋅) . The latter means that the \\nsame mapping function is used for different relations, which is \\nan important reason why the improved model of TransE \\n(including TransR, etc.) does not actually exceed that of TransE. \\nFrom the above derivation, it can be seen that the root of the \\nproblem lies in the same mapping of the head entity and the tail \\n \\n \\nentity of the triple. Therefore, this paper proposes an improved \\ndistance model with position sensitivity. \\n \\nFig.1 Knowledge Graph Composed of Three Special Types of Relationships \\n \\n2.2 Improved Location-Sensitive Distance Model \\nThe most intuitive idea is that the linear transformation is \\nreserved for the head entity of the triple, but no transformation is \\nmade for the tail entity, that is, the expected model establishes \\nthe relationship 𝑓𝑟(𝒉) + 𝒓 = 𝒕 for the correct triple. Because the \\ntranslation operator can be regarded as a part of the linear \\ntransformation, it can be attributed to people 𝑓𝑟(⋅) . Finally, In \\nthis paper, a Location-Sensitive Embedding (LSE) model is \\nproposed. The triple distance scoring function is ∥∥𝒉𝑹𝑟 − 𝒕∥∥𝑝 . \\nSimilarly, the smaller the distance, the higher the possibility of \\nthe triple being established. 𝑃  can take 1 norm or 2 norm. \\nThrough experiments, it is better to choose 1 norm in this paper. \\nThree lemmas are given below, which show that LSE model \\ncan model the relationships of the above three specific patterns. \\nLemma 1. Improved distance model can model symmetric \\nrelations. \\nProve that according to 𝑹𝑟𝒂 = 𝒃, 𝑹𝑟𝒃 = 𝒂 , it can be \\nobtained that 𝑹𝑟𝑹𝑟 = 𝑰. If this condition is met, there will be no \\nadditional restrictions on the embedding of the entity. \\nLemma 2. The improved distance model can model \\nreciprocal relations. \\nProve that according to 𝑹𝑟1𝒂 = 𝒃, 𝑹𝑟2𝒃 = 𝒂 , we can get \\n𝑹𝑟1𝑹𝑟2 = 𝑰. Meeting this condition will not impose additional \\nrestrictions on the entity embedment. \\nLemma 3. The improved distance model can model the \\ncombination of relationships. \\nAccording to 𝑹𝑟1𝒂 = 𝒃, 𝑹𝑟2𝒃 = 𝒄, 𝑹𝑟3𝒂 = 𝒄  , it can be \\nobtained that 𝑹𝑟2𝑹𝑟1 = 𝑹𝑟3, which will not be beneficial to the \\nentity. \\n2.2.1 Loss Function \\nIn order to better consider the possible triplets in the \\nunobserved samples, the cross-entropy loss with label-\\nsmoothness is experimented in this paper. For 𝑘  groups of \\nsamples obtained by random negative sampling, the label is not \\n0, but 1/𝑘; Where 𝑘 > 1 is the proportion of negative sampling \\nfor each triplet. The loss function is minℒ = \\n∑ \\u200a\\n(ℎ,𝑟,𝑡)\\n{log\\u2061 𝑝(ℎ, 𝑟, 𝑡) −\\n∑\\n\\u200a\\n(ℎ′,𝑟,𝑡′)\\n\\u200a1\\n𝑘 log\\u2061(1 − 𝑝(ℎ′, 𝑟, 𝑡′))} \\nNote that this paper removes all restrictions on the norm of \\nparameters, so the loss function has no regular term. \\n2.2.2 Variations of the model \\nIn the LSE model, using a matrix to represent the mapping \\nfunction will cause a large amount of calculation. To solve this \\nproblem, this paper also assumes that the matrix can be replaced \\nby diagonal matrix, that is, the mapping operation of the head \\nentity can be carried out independently in each dimension. This \\nmodel is called LSE𝑑 , It uses the distance scoring function ∥\\n𝒉diag\\u2061(𝒓) − 𝒕 ∥𝑝=∥ 𝒉 ∘ 𝒓 − 𝒕 ∥𝑝 . It notes that ∥ 𝒉 ∘ 𝒓 − 𝒕 ∥1=\\n∑𝑖=1\\n𝑑\\n\\u200a|ℎ𝑖 ⋅ 𝑟𝑖 − 𝑡𝑖|. \\n2.2.3 Comparison and Connection with Other Models \\nAmong the existing knowledge map embedding models, the \\ntranslation distance model is most closely related to this model. \\nBesides, unstructured model (um) [22] and structured \\nembedding (SE) [23] are the same as LSE. No translation \\noperator is used. The distance score function of UM is ∥ 𝒉 − 𝒕 ∥2, \\nwhile that of SE is ∥∥𝑹𝑟1𝒉 − 𝑹𝑟2𝒕∥∥2. The former oversimplifies the \\nrole of the relationship, while the latter adopts similar ideas as \\nthis paper, but it introduces too many parameters, which is not \\nconducive to the training of the model. \\nIn addition, rescal [15] and distmult [16] can both be \\nregarded as linear transformations of the head entities. The \\ndifference is that they use the inner product to calculate the \\nsimilarity. If the vector transformation in this model is limited to \\nthe unit modulus length before and after, and 𝑝 = 2 , then \\n∥∥𝒉𝑹𝑟 − 𝒕∥∥2\\n2 = 2 − 2𝒉𝑹𝑟𝒕T. Equivalent to RESCAL. It can be seen that \\ncompared with RESCAL, this model removes the limitation of \\nmodule length, so it has better representation ability. In DistMult, \\nthe head entity and the tail entity get the same score, which will \\nlead to the model treating all relationships as symmetrical. The \\nsimplified model LSE_d proposed in this paper avoids this point. \\nTable 1 lists the complexity comparison results with the \\ncurrent advanced models. It can be seen that LSE𝑑, as a linear \\ntime complexity model, can be applied to reasoning tasks of \\nlarge-scale knowledge maps. \\nTable 1 Comparison of complexity of knowledge graph embedding models. \\nModel \\nSpace complexity \\nTime complexity \\nTransE[5] \\n(𝑛𝑟 + 𝑛𝑒)𝑑 \\n𝑂(𝑑) \\nTransR[6] \\n(𝑛𝑟 + 𝑛𝑒)𝑑 + 𝑛𝑟𝑑2 \\n𝑂(𝑑2) \\nDistMult[15] \\n(𝑛𝑟 + 𝑛𝑒)𝑑 \\n𝑂(𝑑) \\nHolE[17] \\n(𝑛𝑟 + 𝑛𝑒)𝑑 \\n𝑂(𝑑ln\\u2061𝑑) \\nComplEx[16] \\n2(𝑛𝑟 + 𝑛𝑒)𝑑 \\n𝑂(𝑑) \\nConvE[21] \\n(𝑛𝑟 + 𝑛𝑒)𝑑 \\n \\nLSELSE𝑑 \\n𝑛𝑒𝑑 + 𝑛𝑟𝑑2 \\n𝑂(𝑑2) \\n3 Experimental Results and Discussion \\nIn this section, we firstly introduce the dataset, the link \\nprediction task of the knowledge graph, and the evaluation \\nmetrics used in the experiments, and then discuss the influence \\nof hyperparameters and the selection techniques, and finally list \\nthe performance of the improved distance model and compare it \\nwith the current best performance model. \\nIt is worth mentioning that research work has shown that \\nmany knowledge graph embedding methods (including TransE, \\nDistMult, etc.) can exceed their original published results after \\nfine tuning. This is one of the criticisms of the improvement of \\nthe flat distance model, i.e., the original comparison methods are \\n \\n \\nnot fine-tuned, but in fact, some of the baseline methods still \\nhave much room for training improvement. In order to ensure the \\nfairness of the comparison, we have chosen [10,21,24,25] as the \\nsource of the comparison. \\n3.1 Experimental Setup \\n3.1.1 Data sets \\nThe model of this paper is evaluated on four public datasets, \\nand their statistical characteristics are shown in Table 2. \\nTable 2: Number of entities, relationships, triples and cuts in the datasets \\nData set \\nnr \\nne \\nNumber of \\ntraining sets \\nNumber of \\nvalidation sets \\nNumber of \\ntest sets \\nFB15k \\n1 345 \\n14 951 \\n483 142 \\n50 000 \\n59 071 \\nFB15k237 \\n237 \\n14 541 \\n272 115 \\n17 535 \\n20 466 \\nWN18 \\n18 \\n40 943 \\n141 442 \\n5 000 \\n5 000 \\nWN18RR \\n11 \\n40 943 \\n86 835 \\n3 034 \\n3 134 \\n(1) FB15k vs. FB15k237. FB15k is a subset of the fact \\ndatabase Freebase [26]. The main drawback of this database is \\nthat 81% of the triples in the test set can be derived from inverse \\nrelations. These relations have been removed in FB15k237 , thus \\nmaking it more challenging. \\n(2) WN18 and WN18RR. WN18 is a subset of the word \\nhierarchy database WordNet [27]. Compared with Freebase, \\nWordNet has a larger number of entities. Compared with \\nFreebase, WordNet has more entities, fewer relationship types, \\nand fewer triples, which indicates that the interconnections of \\neach entity node are sparse on average, and this poses a major \\nproblem for knowledge graph representation learning. WN18 \\nalso has many reversible relationships, which are removed by \\nWN18RR. \\n3.1.2 Link Prediction and Evaluation Metrics \\nLink prediction is essentially an ordering problem. In the \\ntesting stage, for each triad, all the entities in the graph are \\nreplaced with the head or tail entities, and then all the \\nreplacement results are ranked in ascending order by their \\ndistance scores. The ranking results are evaluated using two \\ntypes of indicators. \\n(1) mean reciprocal rank, MRR). Average the reciprocal of \\nthe ranking of the correct entity among all entities, that is, \\n∑𝑖=1\\n𝑁 \\u200a(1/rank𝑖)/𝑁. The higher the ranking of the correct entity, \\nthe higher the MRR. \\n(2) Top 𝑛  hit rate (Hits@n). The proportion of correct \\nentities in the top 𝑛 entities. This paper makes statistics when n \\n= 1, 3 and 10 respectively. \\nSome papers also use mean rank as an evaluation index, but \\nit is too affected by extreme samples and is not as stable and \\nobjective as MRR. In addition, the triad generated after \\nsubstitution is not necessarily a negative sample, and it may also \\nexist in the knowledge graph. The above calculation may \\nunderestimate the performance of the model. In order to get a fair \\nevaluation, a \"filtering\" setting is usually used, i.e., the correct \\ntriples are filtered out from the ranking results before the \\nevaluation metrics are calculated. \\n3.2 Experimental Environment and Hyperparameters \\nThe code in this paper is based on the PyTorch framework, \\nand is trained on a single Titan X GPU (12 GB). \\nThe hyperparameters involved in the LSE and LSE𝑑 models \\nproposed in this paper include three categories: (1) model \\nhyperparameter, embedding dimension 𝑑 ∈ {100,200 , interval \\nbetween positive and negative samples 𝛾 ∈ {1,2, ⋯ ,30} . (2) \\noptimization algorithm superparameter, and model training \\nadopts \\nrandomness. \\nSGD) \\nalgorithm, \\nwhich \\ninvolves \\nhyperparameters and its selection range is learning rate 𝑙 ∈\\n{5 × 10−4, 10−4,5 × 10−5, 10−5}, and the batch data size is 𝑏 ∈\\n{128,256,512 ,1024} ,. (3) Negative sampling hyperparameter. \\nIn this paper, according to transh [7] method, the head entity or \\ntail entity is randomly replaced by Bernoulli distribution to avoid \\nattracting too many false negative triples in the process of \\nuniform sampling. For each triplet, the ratio of random negative \\nsampling is 𝑔 ∈ {128,256,512,1024}. \\nThrough the selection of verification set, this paper uses \\n𝑑 = 500  for all four data sets. For FB15k, 𝛾 = 24, 𝑙 = 5 ×\\n10−4, 𝑏 = 256 ,𝑝 = 105, 𝑔 = 256 ; For FB15K-237, 𝛾 = 9, 𝑙 =\\n5 × 10−4,𝑏 = 1024, 𝑝 = 105, 𝑔 = 256; For Wn18,𝛾 = 12, 𝑙 =\\n5 × 10−4, 𝑏 = 512,𝑝 = 5 × 104, 𝑔 = 1024; For Wn18RR, 𝛾 =\\n6, 𝑙 = 5 × 10−4, 𝑏 = 512, 𝑝 = 105, 𝑔 = 1024. In the process of \\nparameter adjustment, it is found that expanding the batch data \\nsize and increasing the negative sampling ratio will improve the \\nperformance when the experimental memory is allowed. \\n3.3 Experimental Results and Analysis \\nTables 3 and 4 compare the performance of the LSE, LSEd \\nmodels with the other models on the link prediction task. \\nTables 3 and 4 compare the performance of LSE, LSE _d \\nmodel and other comparison models in link prediction tasks, \\nrespectively. It can be seen that the models proposed in this paper \\nhave achieved the best performance on FB15k-237 and \\nWN18RR data sets. On FB15k and WN18 data sets, the \\nperformance of the model proposed in this paper is basically the \\nsame as that of the best model. In addition, 𝐿𝑆𝐸\\u2061𝑑  has a \\nsignificant performance improvement compared with LSE, \\nwhich shows that fewer parameters are more suitable for training, \\nand also shows that the assumption that each dimension is \\nindependent is true. \\n \\nTable 3 Link prediction performance of the improved distance model on the FB15k and WN18 datasets. \\nModel: FB15k \\nFB15k \\nWN18 \\nMRR \\nHits@1 \\nHits@3 \\nHits@10 \\nMRR \\nHits@1 \\nHits@3 \\nHits@10 \\nTransE[10] \\n0.463 \\n0.578 \\n0.578 \\n0.578 \\n0.495 \\n0.113 \\n0.888 \\n0.943 \\nDistMult[24] \\n0.798 \\n \\n \\n0.893 \\n0.797 \\n \\n \\n0.946 \\nHolE[17] \\n0.524 \\n0.402 \\n0.613 \\n0.739 \\n0.938 \\n0.930 \\n0.945 \\n0.949 \\nComplEx[16] \\n0.692 \\n0.599 \\n0.759 \\n0.840 \\n0.941 \\n0.936 \\n0.945 \\n0.947 \\nConvE[21] \\n0.745 \\n0.670 \\n0.801 \\n0.873 \\n0.943 \\n0.935 \\n0.946 \\n0.956 \\nLSE \\n0.701 \\n0.643 \\n0.733 \\n0.795 \\n0.873 \\n0.842 \\n0.898 \\n0.926 \\nLSEd \\n0.742 \\n0.680 \\n0.783 \\n0.848 \\n0.929 \\n0.910 \\n0.944 \\n0.955 \\nNotes. Bold indicates the best performance. \\n \\n \\n \\n \\n \\nTable 4 Link prediction performance of the improved distance model on the FB15k237 and WN18RR datasets \\nModel \\nFB15k237 \\nWN18RR \\nMRR \\nHits@1 \\nHits@3 \\nHits@10 \\nMRR \\nHits@1 \\nHits@3 \\nHits@10 \\nTransE[10] \\n0.294 \\n \\n \\n0.465 \\n0.226 \\n \\n \\n0.501 \\nDistMult[24] \\n0.241 \\n0.155 \\n0.263 \\n0.419 \\n0.430 \\n0.390 \\n0.440 \\n0.490 \\nComplEx[16] \\n0.247 \\n0.158 \\n0.275 \\n0.428 \\n0.440 \\n0.410 \\n0.460 \\n0.510 \\nConvE[21] \\n0.325 \\n0.237 \\n0.356 \\n0.501 \\n0.430 \\n0.430 \\n0.440 \\n0.520 \\nLSE \\n0.299 \\n0.189 \\n0.312 \\n0.470 \\n0.421 \\n0.378 \\n0.450 \\n0.494 \\nLSEd \\n0.331 \\n0.237 \\n0.366 \\n0.522 \\n0.450 \\n0.407 \\n0.465 \\n0.537 \\nNotes. Bold indicates the best performance. \\n \\n For each dataset, the following findings are observed. \\n(1) DistMult achieves the highest performance on the \\nFB15k dataset. This suggests that symmetric relationships are \\nthe dominant relationship model in this dataset. In fact, about 81% \\nof the test set triples can be directly inferred from the symmetry \\nof the relations [4]. It can be seen that the results of DistMult on \\nother datasets are not as good as that of the model proposed in \\nthis paper, so it can be said that the model proposed in this paper \\nis more general than DistMult. \\n(2) In the WN18 dataset, the main relationship patterns are \\nsymmetric and inverse; in the WN18RR dataset, the main \\nrelationship patterns are symmetric (e.g., relationship also see, \\nsimilar to, etc.). On both datasets, the performance of TransE \\ndecreases significantly, which indicates that it cannot \\nsymmetrize relations well. This is consistent with the theoretical \\nanalysis in Section 3. \\n(3) On FB15k-237 and WN18RR data sets, the combination \\nof relationships also occupies a very important part. In FB15k-\\n237, the effect of TransE is not much different from other \\nadvanced algorithms, which is consistent with the theoretical \\nanalysis in Section 3. Similarly, the model proposed in this paper \\ncan also model the combination of relationships well. \\nTaken together, the experimental results in this paper \\ncorrespond well with the theoretical analysis, which fully \\ndemonstrates that the models LSE and LSEd have good \\nknowledge graph representation learning ability. \\n4 Conclusion \\nIn this paper, a location-sensitive model for knowledge \\ngraph embedding representation is proposed, and the \\neffectiveness of the proposed improved model is verified \\ntheoretically and experimentally, and the superior performance \\nis achieved on the task of link prediction in large-scale \\nknowledge graphs. \\nThis paper notes that there are still some problems that need \\nto be further studied. (1) In the combinatorial relationship, there \\nis a special case that needs to be considered separately, that is, \\nthe combination formed by the same relationship. This model \\nusually exists in the knowledge map of professional fields with \\nmore hierarchical structures. For example, (forearm, partOf, \\nlimbs) and (limbs, partOf, human body) coexist (forearm, partOf, \\n(2) The representation of knowledge is uncertain. In KG2E [28] \\nmodel, entities and relationships are no longer modeled as a point \\nin space, but a Gaussian distribution. The application of position-\\nsensitive embedding proposed in this paper needs to be further \\nimproved. \\nReferences: \\n[1] \\nSinghal, Amit. \"Introducing the Knowledge Graph: Things, \\nNot \\nStrings.\" \\nGoogle \\nBlog, \\n6 \\nAug. \\n2020, \\nhttps://blog.google/products/search/introducing-\\nknowledge-graph-things-not/. \\n[2] \\nNickel, Maximilian, et al. \"A Review of Relational \\nMachine Learning for Knowledge Graphs.\" Proceedings \\nof the IEEE, vol. 104, no. 1, 2016, pp. 11-33. \\n[3] \\nToutanova, Kristina, and Danqi Chen. \"Observed Versus \\nLatent Features for Knowledge Base and Text Inference.\" \\nProceedings of the 3rd Workshop on Continuous Vector \\nSpace Models and their Compositionality, MIT Press, \\n2015, pp. 57-66. \\n[4] \\nXiong, Siheng, et al. \"TILP: Differentiable Learning of \\nTemporal Logical Rules on Knowledge Graphs.\" The \\nEleventh \\nInternational \\nConference \\non \\nLearning \\nRepresentations. 2022. \\n[5] \\nBordes, Antoine, et al. \"Translating Embeddings for \\nModeling Multi-Relational Data.\" Proceedings of the 29th \\nInternational \\nConference \\non \\nNeural \\nInformation \\nProcessing Systems, MIT Press, 2013, pp. 2787-2795. \\n[6] \\nLin, Yankai, et al. \"Learning Entity and Relation \\nEmbeddings \\nfor \\nKnowledge \\nGraph \\nCompletion.\" \\nProceedings of the 29th AAAI Conference on Artificial \\nIntelligence, AAAI Press, 2015, pp. 2181-2187. \\n[7] \\nWang, Zhen, et al. \"Knowledge Graph Embedding by \\nTranslating on Hyperplanes.\" Proceedings of the 28th \\nAAAI Conference on Artificial Intelligence, AAAI Press, \\n2014, pp. 1112-1119. \\n[8] \\nXiao, Han, et al. \"TransA: An Adaptive Approach for \\nKnowledge Graph Embedding.\" arXiv, 6 Aug. 2020, \\nhttps://arxiv.org/pdf/1509.05490.pdf. \\n[9] \\nJi, Guilin, et al. \"Knowledge Graph Embedding Via \\nDynamic Mapping Matrix.\" Proceedings of the 53rd \\nAnnual Meeting of the Association for Computational \\nLinguistics and the 7th International Joint Conference on \\nNatural Language Processing, MIT Press, 2015, pp. 687-\\n696. \\n[10] Akrami, Farahnaz, et al. \"Re-evaluating Embedding-\\nBased \\nKnowledge \\nGraph \\nCompletion \\nMethods.\" \\nProceedings of the 27th ACM International Conference on \\nInformation and Knowledge Management, ACM Press, \\n2018, pp. 1779-1782. \\n[11] Zhang, Wen. \"Knowledge Graph Embedding with \\nDiversity of Structures.\" Proceedings of the 26th \\nInternational Conference on World Wide Web Companion, \\nInternational World Wide Web Conferences Steering \\nCommittee Press, 2017, pp. 747-753. \\n[12] Ji, Guilin, et al. \"Knowledge Graph Completion with \\nAdaptive Sparse Transfer Matrix.\" Proceedings of the 30th \\nAAAI Conference on Artificial Intelligence, AAAI Press, \\n2016, pp. 985-991. \\n[13] Fan, Ming, et al. \"Transition-Based Knowledge Graph \\nEmbedding \\nwith \\nRelational \\nMapping \\nProperties.\" \\nProceedings of the 28th Pacific Asia Conference on \\nLanguage, Information and Computing, Department of \\nLinguistics, Chulalongkorn University, 2014, pp. 328-337. \\n[14] Nickel, Maximilian, et al. \"A Three-Way Model for \\nCollective \\nLearning \\non \\nMulti-Relational \\nData.\" \\nProceedings of the 28th International Conference on \\nMachine Learning, Omni Press, 2011, pp. 809-816. \\n[15] Yang, Bishan, et al. \"Embedding Entities and Relations for \\nLearning and Inference in Knowledge Bases.\" Proceedings \\nof \\nthe \\nInternational \\nConference \\non \\nLearning \\nRepresentations, MIT Press, 2015, pp. 51-63. \\n[16] Trouillon, Théo, et al. \"Complex Embeddings for Simple \\nLink Prediction.\" Proceedings of the 33rd International \\nConference on Machine Learning, ACM Press, 2016, pp. \\n2071-2080. \\n[17] Nickel, Maximilian, Lorenzo Rosasco, and Tomaso \\nPoggio. \\n\"Holographic \\nEmbeddings \\nof \\nKnowledge \\n \\n \\nGraphs.\" Proceedings of the 33th AAAI Conference on \\nArtificial Intelligence, AAAI Press, 2016, pp. 1955-1961. \\n[18] Hayashi, Kohei, and Masashi Shimbo. \"On the \\nEquivalence of Holographic and Complex Embeddings for \\nLink Prediction.\" Proceedings of the 55th Annual Meeting \\nof the Association for Computational Linguistics, MIT \\nPress, 2017, pp. 554-559. \\n[19] Bordes, Antoine, et al. \"A Semantic Matching Energy \\nFunction for Learning with Multi-Relational Data.\" \\nMachine Learning, vol. 94, no. 2, 2014, pp. 233-259. \\n[20] Socher, Richard, et al. \"Reasoning with Neural Tensor \\nNetworks for Knowledge Base Completion.\" Proceedings \\nof Advances in Neural Information Processing Systems, \\nMIT Press, 2013, pp. 926-934. \\n[21] Dettmers, Tim, et al. \"Convolutional 2D Knowledge \\nGraph Embeddings.\" Proceedings of the 32nd AAAI \\nConference on Artificial Intelligence, AAAI Press, 2018, \\npp. 1811-1818. \\n[22] Bordes, Antoine, et al. \"Joint Learning of Words and \\nMeaning Representations for Open-Text Semantic \\nParsing.\" Proceedings of the 15th International Conference \\nof Artificial Intelligence and Statistics, ACM Press, 2012, \\npp. 127-135. \\n[23] Bordes, Antoine, et al. \"Learning Structured Embeddings \\nof Knowledge Bases.\" Proceedings of the 25th AAAI \\nConference on Artificial Intelligence, AAAI Press, 2011, \\npp. 301-306. \\n[24] Kadlec, Rudolf, Ondřej Bajgar, and Jan Kleindienst. \\n\"Knowledge Base Completion: Baselines Strike Back.\" \\narXiv, 6 Aug. 2020, https://arxiv.org/abs/1705.10744. \\n[25] Nguyen, Dai Quoc, et al. \"A Novel Embedding Model for \\nKnowledge Base Completion Based on Convolutional \\nNeural \\nNetwork.\" \\narXiv, \\n6 \\nAug. \\n2020, \\nhttps://arxiv.org/abs/1712.02121v2. \\n[26] Bollacker, Kurt, et al. \"Freebase: A Collaboratively \\nCreated \\nGraph \\nDatabase \\nfor \\nStructuring \\nHuman \\nKnowledge.\" Proceedings of the ACM SIGMOD \\nInternational Conference on Management of Data, ACM \\nPress, 2008, pp. 1247-1250. \\n[27] Miller, George A. \"WordNet: A Lexical Database for \\nEnglish.\" Communications of the ACM, vol. 38, no. 11, \\n1995, pp. 39-41. \\n[28] He, Shizhu, et al. \"Learning to Represent Knowledge \\nGraphs with Gaussian Embedding.\" Proceedings of the \\n24th ACM International on Conference on Information \\nand Knowledge Management, ACM Press, 2015, pp. 623-\\n632. \\n \\n'},\n",
       " {'abstract': 'We present Depth Anything, a highly practical solution for robust monocular depth estimation. Unlike prior works that laboriously construct diverse labeled datasets, we highlight the value of massive, cheap, and diverse unlabeled images. We design two simple yet highly effective strategies — challenging the student model with strong perturbations when learning unlabeled images, and preserving rich semantic priors from pre-trained models — to unlock their potential. Consequently, our Depth Anything model exhibits superior zero-shot depth estimation ability across extensive unseen scenes and is further strengthened for downstream metric depth estimation and semantic segmentation tasks. We release the code and models for broad research use.',\n",
       "  'introduction': 'This work presents Depth Anything1, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability (Figure 1). Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet.\\nOur work was done during an internship at TikTok.',\n",
       "  'literature review': 'The field of computer vision and natural language processing is currently experiencing a revolution with the emergence of “foundation models” [6] that demonstrate strong zero-/few-shot performance in various downstream scenarios [44, 58]. These successes primarily rely on large-scale training data that can effectively cover the data distribution. Monocular Depth Estimation (MDE), which is a fundamental problem with broad applications in robotics [65], autonomous driving [63, 79], virtual reality [47], etc., also requires a foundation model to estimate depth information from a single image. However, this has been underexplored due to the difficulty of building datasets with tens of millions of depth labels. MiDaS [45] made a pioneering study along this direction by training an MDE model on a collection of mixed labeled datasets. Despite demonstrating a certain level of zero-shot ability, MiDaS is limited by its data coverage, thus suffering disastrous performance in some scenarios.',\n",
       "  'methodology': 'Our work utilizes both labeled and unlabeled images to facilitate better monocular depth estimation (MDE). For labeled images, we produce the depth labels with an off-the-shelf MDE model. Therefore, we can easily annotate large-scale unlabeled images and augment the data coverage. Moreover, to leverage the unlabeled images effectively, we design two simple yet effective strategies. First, we challenge the student model with strong perturbations when learning the pseudo labels. It forces the student model to actively seek extra visual knowledge and learn more robust representations. Second, we enforce an auxiliary constraint on the student model to preserve rich semantic priors from pre-trained encoders.',\n",
       "  'results': 'Both with a ViT-L encoder, our Depth Anything surpasses the strongest MiDaS model tremendously across extensive scenes in terms of both the AbsRel (absolute relative error: |d* −d|/d) and ϴ1 (percentage of max(d*/d, d/d*) < 1.25) metrics. For example, when tested on the well-known autonomous driving dataset DDAD [20], we improve the AbsRel (↓) from 0.251 → 0.230 and improve the ϴ1 (↑) from 0.766 ↑ 0.789.\\nBesides, our ViT-B model is already clearly superior to the MiDaS based on a much larger ViT-L. Moreover, our ViT-S model, whose scale is less than 1/10 of the MiDaS model, even outperforms MiDaS on several unseen datasets, including Sintel, DDAD, and ETH3D. The performance advantage of these small-scale models demonstrates their great potential in computationally-constrained scenarios.\\nIt is also worth noting that, on the most widely used MDE benchmarks KITTI and NYUv2, although MiDaS v3.1 uses the corresponding training images (not zero-shot anymore), our Depth Anything is still evidently superior to it without training with any KITTI or NYUv2 images, e.g., 0.127 vs. 0.076 in AbsRel and 0.850 vs. 0.947 in ϴ1 on KITTI.',\n",
       "  'conclusion': 'In this work, we present Depth Anything, a highly practical solution to robust monocular depth estimation. Different from prior arts, we especially highlight the value of cheap and diverse unlabeled images. We design two simple yet highly effective strategies to fully exploit their value: 1) posing a more challenging optimization target when learning unlabeled images, and 2) preserving rich semantic priors from pre-trained models. As a result, our Depth Anything model exhibits excellent zero-shot depth estimation ability, and also serves as a promising initialization for downstream metric depth estimation and semantic segmentation tasks.',\n",
       "  'title': 'Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data',\n",
       "  'author': 'Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao',\n",
       "  'textdata': 'Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data\\nLihe Yang1\\nBingyi Kang2†\\nZilong Huang2\\nXiaogang Xu3,4\\nJiashi Feng2\\nHengshuang Zhao1†\\n1The University of Hong Kong\\n2TikTok\\n3Zhejiang Lab\\n4Zhejiang University\\n† corresponding authors\\nhttps://depth-anything.github.io\\nFigure 1. Our model exhibits impressive generalization ability across extensive unseen scenes. Left two columns: COCO [35]. Middle two:\\nSA-1B [27] (a hold-out unseen set). Right two: photos captured by ourselves. Our model works robustly in low-light environments (1st and\\n3rd column), complex scenes (2nd and 5th column), foggy weather (5th column), and ultra-remote distance (5th and 6th column), etc.\\nAbstract\\nThis work presents Depth Anything1, a highly practical\\nsolution for robust monocular depth estimation. Without pur-\\nsuing novel technical modules, we aim to build a simple yet\\npowerful foundation model dealing with any images under\\nany circumstances. To this end, we scale up the dataset by\\ndesigning a data engine to collect and automatically anno-\\ntate large-scale unlabeled data (∼62M), which significantly\\nenlarges the data coverage and thus is able to reduce the\\ngeneralization error. We investigate two simple yet effective\\nstrategies that make data scaling-up promising. First, a more\\nchallenging optimization target is created by leveraging data\\naugmentation tools. It compels the model to actively seek\\nextra visual knowledge and acquire robust representations.\\nSecond, an auxiliary supervision is developed to enforce\\nthe model to inherit rich semantic priors from pre-trained\\nencoders. We evaluate its zero-shot capabilities extensively,\\nincluding six public datasets and randomly captured photos.\\nIt demonstrates impressive generalization ability (Figure 1).\\nFurther, through fine-tuning it with metric depth information\\nfrom NYUv2 and KITTI, new SOTAs are set. Our better depth\\nmodel also results in a better depth-conditioned ControlNet.\\nOur models are released here.\\nThe work was done during an internship at TikTok.\\n1While the grammatical soundness of this name may be questionable,\\nwe treat it as a whole and pay homage to Segment Anything [27].\\n1. Introduction\\nThe field of computer vision and natural language processing\\nis currently experiencing a revolution with the emergence of\\n“foundation models” [6] that demonstrate strong zero-/few-\\nshot performance in various downstream scenarios [44, 58].\\nThese successes primarily rely on large-scale training data\\nthat can effectively cover the data distribution. Monocular\\nDepth Estimation (MDE), which is a fundamental problem\\nwith broad applications in robotics [65], autonomous driv-\\ning [63, 79], virtual reality [47], etc., also requires a foun-\\ndation model to estimate depth information from a single\\nimage. However, this has been underexplored due to the\\ndifficulty of building datasets with tens of millions of depth\\nlabels. MiDaS [45] made a pioneering study along this di-\\nrection by training an MDE model on a collection of mixed\\nlabeled datasets. Despite demonstrating a certain level of\\nzero-shot ability, MiDaS is limited by its data coverage, thus\\nsuffering disastrous performance in some scenarios.\\nIn this work, our goal is to build a foundation model for\\nMDE capable of producing high-quality depth information\\nfor any images under any circumstances. We approach this\\ntarget from the perspective of dataset scaling-up. Tradition-\\nally, depth datasets are created mainly by acquiring depth\\ndata from sensors [18, 54], stereo matching [15], or SfM [33],\\nwhich is costly, time-consuming, or even intractable in partic-\\nular situations. We instead, for the first time, pay attention to\\nlarge-scale unlabeled data. Compared with stereo images or\\n1\\narXiv:2401.10891v1  [cs.CV]  19 Jan 2024\\nlabeled images from depth sensors, our used monocular unla-\\nbeled images exhibit three advantages: (i) (simple and cheap\\nto acquire) Monocular images exist almost everywhere, thus\\nthey are easy to collect, without requiring specialized de-\\nvices. (ii) (diverse) Monocular images can cover a broader\\nrange of scenes, which are critical to the model generaliza-\\ntion ability and scalability. (iii) (easy to annotate) We can\\nsimply use a pre-trained MDE model to assign depth labels\\nfor unlabeled images, which only takes a feedforward step.\\nMore than efficient, this also produces denser depth maps\\nthan LiDAR [18] and omits the computationally intensive\\nstereo matching process.\\nWe design a data engine to automatically generate depth\\nannotations for unlabeled images, enabling data scaling-up\\nto arbitrary scale. It collects 62M diverse and informative im-\\nages from eight public large-scale datasets, e.g., SA-1B [27],\\nOpen Images [30], and BDD100K [81]. We use their raw\\nunlabeled images without any forms of labels. Then, in or-\\nder to provide a reliable annotation tool for our unlabeled\\nimages, we collect 1.5M labeled images from six public\\ndatasets to train an initial MDE model. The unlabeled im-\\nages are then automatically annotated and jointly learned\\nwith labeled images in a self-training manner [31].\\nDespite all the aforementioned advantages of monocular\\nunlabeled images, it is indeed not trivial to make positive use\\nof such large-scale unlabeled images [72, 89], especially in\\nthe case of sufficient labeled images and strong pre-training\\nmodels. In our preliminary attempts, directly combining la-\\nbeled and pseudo labeled images failed to improve the base-\\nline of solely using labeled images. We conjecture that, the\\nadditional knowledge acquired in such a naive self-teaching\\nmanner is rather limited. To address the dilemma, we pro-\\npose to challenge the student model with a more difficult\\noptimization target when learning the pseudo labels. The\\nstudent model is enforced to seek extra visual knowledge\\nand learn robust representations under various strong pertur-\\nbations to better handle unseen images.\\nFurthermore, there have been some works [9, 21] demon-\\nstrating the benefit of an auxiliary semantic segmentation\\ntask for MDE. We also follow this research line, aiming to\\nequip our model with better high-level scene understanding\\ncapability. However, we observed when an MDE model is\\nalready powerful enough, it is hard for such an auxiliary\\ntask to bring further gains. We speculate that it is due to\\nsevere loss in semantic information when decoding an im-\\nage into a discrete class space. Therefore, considering the\\nexcellent performance of DINOv2 in semantic-related tasks,\\nwe propose to maintain the rich semantic priors from it with\\na simple feature alignment loss. This not only enhances the\\nMDE performance, but also yields a multi-task encoder for\\nboth middle-level and high-level perception tasks.\\nOur contributions are summarized as follows:\\n• We highlight the value of data scaling-up of massive,\\ncheap, and diverse unlabeled images for MDE.\\n• We point out a key practice in jointly training large-\\nscale labeled and unlabeled images. Instead of learning\\nraw unlabeled images directly, we challenge the model\\nwith a harder optimization target for extra knowledge.\\n• We propose to inherit rich semantic priors from pre-\\ntrained encoders for better scene understanding, rather\\nthan using an auxiliary semantic segmentation task.\\n• Our model exhibits stronger zero-shot capability than\\nMiDaS-BEiTL-512 [5]. Further, fine-tuned with metric\\ndepth, it outperforms ZoeDepth [4] significantly.\\n2. Related Work\\nMonocular depth estimation (MDE). Early works [23, 36,\\n50] primarily relied on handcrafted features and traditional\\ncomputer vision techniques. They were limited by their re-\\nliance on explicit depth cues and struggled to handle complex\\nscenes with occlusions and textureless regions.\\nDeep learning-based methods have revolutionized monoc-\\nular depth estimation by effectively learning depth represen-\\ntations from delicately annotated datasets [18, 54]. Eigen\\net al. [17] first proposed a multi-scale fusion network to\\nregress the depth. Following this, many works consistently\\nimprove the depth estimation accuracy by carefully design-\\ning the regression task as a classification task [3, 34], in-\\ntroducing more priors [32, 53, 75, 82], and better objective\\nfunctions [67, 77], etc. Despite the promising performance,\\nthey are hard to generalize to unseen domains.\\nZero-shot depth estimation. Our work belongs to this re-\\nsearch line. We aim to train an MDE model with a diverse\\ntraining set and thus can predict the depth for any given im-\\nage. Some pioneering works [10, 66] explored this direction\\nby collecting more training images, but their supervision is\\nvery sparse and is only enforced on limited pairs of points.\\nTo enable effective multi-dataset joint training, a mile-\\nstone work MiDaS [45] utilizes an affine-invariant loss to\\nignore the potentially different depth scales and shifts across\\nvarying datasets. Thus, MiDaS provides relative depth infor-\\nmation. Recently, some works [4, 22, 78] take a step further\\nto estimate the metric depth. However, in our practice, we\\nobserve such methods exhibit poorer generalization ability\\nthan MiDaS, especially its latest version [5]. Besides, as\\ndemonstrated by ZoeDepth [4], a strong relative depth es-\\ntimation model can also work well in generalizable metric\\ndepth estimation by fine-tuning with metric depth informa-\\ntion. Therefore, we still follow MiDaS in relative depth\\nestimation, but further strengthen it by highlighting the value\\nof large-scale monocular unlabeled images.\\nLeveraging unlabeled data. This belongs to the research\\narea of semi-supervised learning [31, 55, 89], which is pop-\\nular with various applications [70, 74]. However, existing\\n2\\nworks typically assume only limited images are available.\\nThey rarely consider the challenging but realistic scenario\\nwhere there are already sufficient labeled images but also\\nlarger-scale unlabeled images. We take this challenging di-\\nrection for zero-shot MDE. We demonstrate that unlabeled\\nimages can significantly enhance the data coverage and thus\\nimprove model generalization and robustness.\\n3. Depth Anything\\nOur work utilizes both labeled and unlabeled images to\\nfacilitate better monocular depth estimation (MDE). For-\\nmally, the labeled and unlabeled sets are denoted as Dl =\\n{(xi, di)}M\\ni=1 and Du = {ui}N\\ni=1 respectively. We aim to\\nlearn a teacher model T from Dl. Then, we utilize T to\\nassign pseudo depth labels for Du. Finally, we train a stu-\\ndent model S on the combination of labeled set and pseudo\\nlabeled set. A brief illustration is provided in Figure 2.\\n3.1. Learning Labeled Images\\nThis process is similar to the training of MiDaS [5, 45].\\nHowever, since MiDaS did not release its code, we first\\nreproduced it. Concretely, the depth value is first transformed\\ninto the disparity space by d = 1/t and then normalized\\nto 0∼1 on each depth map. To enable multi-dataset joint\\ntraining, we adopt the affine-invariant loss to ignore the\\nunknown scale and shift of each sample:\\nLl =\\n1\\nHW\\nHW\\nX\\ni=1\\nρ(d∗\\ni , di),\\n(1)\\nwhere d∗\\ni and di are the prediction and ground truth, respec-\\ntively. And ρ is the affine-invariant mean absolute error loss:\\nρ(d∗\\ni , di) = | ˆd∗\\ni − ˆdi|, where ˆd∗\\ni and ˆdi are the scaled and\\nshifted versions of the prediction d∗\\ni and ground truth di:\\nˆdi = di − t(d)\\ns(d)\\n,\\n(2)\\nwhere t(d) and s(d) are used to align the prediction and\\nground truth to have zero translation and unit scale:\\nt(d) = median(d), s(d) =\\n1\\nHW\\nHW\\nX\\ni=1\\n|di − t(d)|.\\n(3)\\nTo obtain a robust monocular depth estimation model, we\\ncollect 1.5M labeled images from 6 public datasets. Details\\nof these datasets are listed in Table 1. We use fewer labeled\\ndatasets than MiDaS v3.1 [5] (12 training datasets), because\\n1) we do not use NYUv2 [54] and KITTI [18] datasets to\\nensure zero-shot evaluation on them, 2) some datasets are\\nnot available (anymore), e.g., Movies [45] and WSVD [60],\\nand 3) some datasets exhibit poor quality, e.g., RedWeb (also\\nlow resolution) [66]. Despite using fewer labeled images,\\nDataset\\nIndoor\\nOutdoor\\nLabel\\n# Images\\nLabeled Datasets\\nBlendedMVS [76]\\n✓\\n✓\\nStereo\\n115K\\nDIML [13]\\n✓\\n✓\\nStereo\\n927K\\nHRWSI [67]\\n✓\\n✓\\nStereo\\n20K\\nIRS [61]\\n✓\\nStereo\\n103K\\nMegaDepth [33]\\n✓\\nSfM\\n128K\\nTartanAir [62]\\n✓\\n✓\\nStereo\\n306K\\nUnlabeled Datasets\\nBDD100K [81]\\n✓\\nNone\\n8.2M\\nGoogle Landmarks [64]\\n✓\\nNone\\n4.1M\\nImageNet-21K [49]\\n✓\\n✓\\nNone\\n13.1M\\nLSUN [80]\\n✓\\nNone\\n9.8M\\nObjects365 [52]\\n✓\\n✓\\nNone\\n1.7M\\nOpen Images V7 [30]\\n✓\\n✓\\nNone\\n7.8M\\nPlaces365 [87]\\n✓\\n✓\\nNone\\n6.5M\\nSA-1B [27]\\n✓\\n✓\\nNone\\n11.1M\\nTable 1. In total, our Depth Anything is trained on 1.5M labeled\\nimages and 62M unlabeled images jointly.\\nour easy-to-acquire and diverse unlabeled images will com-\\nprehend the data coverage and greatly enhance the model\\ngeneralization ability and robustness.\\nFurthermore, to strengthen the teacher model T learned\\nfrom these labeled images, we adopt the DINOv2 [42] pre-\\ntrained weights to initialize our encoder. In practice, we\\napply a pre-trained semantic segmentation model [69] to de-\\ntect the sky region, and set its disparity value as 0 (farthest).\\n3.2. Unleashing the Power of Unlabeled Images\\nThis is the main point of our work. Distinguished from prior\\nworks that laboriously construct diverse labeled datasets,\\nwe highlight the value of unlabeled images in enhancing\\nthe data coverage. Nowadays, we can practically build a\\ndiverse and large-scale unlabeled set from the Internet or\\npublic datasets of various tasks. Also, we can effortlessly\\nobtain the dense depth map of monocular unlabeled images\\nsimply by forwarding them to a pre-trained well-performed\\nMDE model. This is much more convenient and efficient\\nthan performing stereo matching or SfM reconstruction for\\nstereo images or videos. We select eight large-scale public\\ndatasets as our unlabeled sources for their diverse scenes.\\nThey contain more than 62M images in total. The details are\\nprovided in the bottom half of Table 1.\\nTechnically, given the previously obtained MDE teacher\\nmodel T, we make predictions on the unlabeled set Du to\\nobtain a pseudo labeled set ˆDu:\\nˆDu = {(ui, T(ui))|ui ∈ Du}N\\ni=1.\\n(4)\\nWith the combination set Dl ∪ ˆ\\nDu of labeled images and\\npseudo labeled images, we train a student model S on it.\\n3\\nlabeled image\\nunlabeled image\\nencoder\\ndecoder\\nmanual label\\npseudo label\\nencoder\\nteacher\\nmodel\\nLiDAR, \\nmatching, \\nSfM, etc\\nsemantic\\npreservation\\nlabeled prediction\\nunlabeled prediction\\nsup\\nsup\\nHRWSI: 102684_LookInStereoDotComDSCF0486\\nSA1B: sa_10000139\\nS\\nfeature \\nalignment\\nFigure 2. Our pipeline. Solid line: flow of labeled images, dotted line: unlabeled images. We especially highlight the value of large-scale\\nunlabeled images. The S denotes adding strong perturbations (Section 3.2). To equip our depth estimation model with rich semantic priors,\\nwe enforce an auxiliary constraint between the online student model and a frozen encoder to preserve the semantic capability (Section 3.3).\\nFollowing prior works [73], instead of fine-tuning S from T,\\nwe re-initialize S for better performance.\\nUnfortunately, in our pilot studies, we failed to gain im-\\nprovements with such a self-training pipeline, which indeed\\ncontradicts the observations when there are only a few la-\\nbeled images [55]. We conjecture that, with already suffi-\\ncient labeled images in our case, the extra knowledge ac-\\nquired from additional unlabeled images is rather limited.\\nEspecially considering the teacher and student share the\\nsame pre-training and architecture, they tend to make similar\\ncorrect or false predictions on the unlabeled set Du, even\\nwithout the explicit self-training procedure.\\nTo address the dilemma, we propose to challenge the stu-\\ndent with a more difficult optimization target for additional\\nvisual knowledge on unlabeled images. We inject strong per-\\nturbations to unlabeled images during training. It compels\\nour student model to actively seek extra visual knowledge\\nand acquire invariant representations from these unlabeled\\nimages. These advantages help our model deal with the open\\nworld more robustly. We introduce two forms of perturba-\\ntions: one is strong color distortions, including color jittering\\nand Gaussian blurring, and the other is strong spatial dis-\\ntortion, which is CutMix [83]. Despite the simplicity, the\\ntwo modifications make our large-scale unlabeled images\\nsignificantly improve the baseline of labeled images.\\nWe provide more details about CutMix. It was originally\\nproposed for image classification, and is rarely explored in\\nmonocular depth estimation. We first interpolate a random\\npair of unlabeled images ua and ub spatially:\\nuab = ua ⊙ M + ub ⊙ (1 − M),\\n(5)\\nwhere M is a binary mask with a rectangle region set as 1.\\nThe unlabeled loss Lu is obtained by first computing\\naffine-invariant losses in valid regions defined by M and\\n1 − M, respectively:\\nLM\\nu = ρ\\nMethod\\nEncoder\\nKITTI [18]\\nNYUv2 [54]\\nSintel [7]\\nDDAD [20]\\nETH3D [51]\\nDIODE [59]\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nMiDaS v3.1 [5]\\nViT-L\\n0.127\\n0.850\\n0.048\\n0.980\\n0.587\\n0.699\\n0.251\\n0.766\\n0.139\\n0.867\\n0.075\\n0.942\\nDepth Anything\\nViT-S\\n0.080\\n0.936\\n0.053\\n0.972\\n0.464\\n0.739\\n0.247\\n0.768\\n0.127\\n0.885\\n0.076\\n0.939\\nViT-B\\n0.080\\n0.939\\n0.046\\n0.979\\n0.432\\n0.756\\n0.232\\n0.786\\n0.126\\n0.884\\n0.069\\n0.946\\nViT-L\\n0.076\\n0.947\\n0.043\\n0.981\\n0.458\\n0.760\\n0.230\\n0.789\\n0.127\\n0.882\\n0.066\\n0.952\\nTable 2. Zero-shot relative depth estimation. Better: AbsRel ↓ , δ1 ↑. We compare with the best model from MiDaS v3.1. Note that MiDaS\\ndoes not strictly follow the zero-shot evaluation on KITTI and NYUv2, because it uses their training images. We provide three model scales\\nfor different purposes, based on ViT-S (24.8M), ViT-B (97.5M), and ViT-L (335.3M), respectively. Best, second best results.\\ndepth model with an auxiliary feature alignment loss. The\\nfeature space is high-dimensional and continuous, thus con-\\ntaining richer semantic information than discrete masks. The\\nfeature alignment loss is formulated as:\\nLfeat = 1 −\\n1\\nHW\\nHW\\nX\\ni=1\\ncos(fi, f ′\\ni),\\n(9)\\nwhere cos(·, ·) measures the cosine similarity between two\\nfeature vectors. f is the feature extracted by the depth model\\nS, while f ′ is the feature from a frozen DINOv2 encoder.\\nWe do not follow some works [19] to project the online\\nfeature f into a new space for alignment, because a randomly\\ninitialized projector makes the large alignment loss dominate\\nthe overall loss in the early stage.\\nAnother key point in feature alignment is that, semantic\\nencoders like DINOv2 tend to produce similar features for\\ndifferent parts of an object, e.g., car front and rear. In depth\\nestimation, however, different parts or even pixels within the\\nsame part, can be of varying depth. Thus, it is not beneficial\\nto exhaustively enforce our depth model to produce exactly\\nthe same features as the frozen encoder.\\nTo solve this issue, we set a tolerance margin α for the\\nfeature alignment. If the cosine similarity of fi and f ′\\ni has\\nsurpassed α, this pixel will not be considered in our Lfeat.\\nThis allows our method to enjoy both the semantic-aware\\nrepresentation from DINOv2 and the part-level discrimina-\\ntive representation from depth supervision. As a side effect,\\nour produced encoder not only performs well in downstream\\nMDE datasets, but also achieves strong results in the seman-\\ntic segmentation task. It also indicates the potential of our\\nencoder to serve as a universal multi-task encoder for both\\nmiddle-level and high-level perception tasks.\\nFinally, our overall loss is an average combination of the\\nthree losses Ll, Lu, and Lfeat.\\n4. Experiment\\n4.1. Implementation Details\\nWe adopt the DINOv2 encoder [42] for feature extraction.\\nFollowing MiDaS [5, 45], we use the DPT [46] decoder for\\ndepth regression. All labeled datasets are simply combined\\ntogether without re-sampling. In the first stage, we train a\\nteacher model on labeled images for 20 epochs. In the second\\nstage of joint training, we train a student model to sweep\\nacross all unlabeled images for one time. The unlabeled\\nimages are annotated by a best-performed teacher model\\nwith a ViT-L encoder. The ratio of labeled and unlabeled\\nimages is set as 1:2 in each batch. In both stages, the base\\nlearning rate of the pre-trained encoder is set as 5e-6, while\\nthe randomly initialized decoder uses a 10× larger learning\\nrate. We use the AdamW optimizer and decay the learning\\nrate with a linear schedule. We only apply horizontal flipping\\nas our data augmentation for labeled images. The tolerance\\nmargin α for feature alignment loss is set as 0.15. For more\\ndetails, please refer to our appendix.\\n4.2. Zero-Shot Relative Depth Estimation\\nAs aforementioned, this work aims to provide accurate\\ndepth estimation for any image. Therefore, we compre-\\nhensively validate the zero-shot depth estimation capability\\nof our Depth Anything model on six representative unseen\\ndatasets: KITTI [18], NYUv2 [54], Sintel [7], DDAD [20],\\nETH3D [51], and DIODE [59]. We compare with the best\\nDPT-BEiTL-512 model from the latest MiDaS v3.1 [5], which\\nuses more labeled images than us. As shown in Table 2,\\nboth with a ViT-L encoder, our Depth Anything surpasses\\nthe strongest MiDaS model tremendously across extensive\\nscenes in terms of both the AbsRel (absolute relative error:\\n|d∗ −d|/d) and δ1 (percentage of max(d∗/d, d/d∗) < 1.25)\\nmetrics. For example, when tested on the well-known au-\\ntonomous driving dataset DDAD [20], we improve the Ab-\\nsRel (↓) from 0.251 → 0.230 and improve the δ1 (↑) from\\n0.766 → 0.789.\\nBesides, our ViT-B model is already clearly superior to\\nthe MiDaS based on a much larger ViT-L. Moreover, our\\nViT-S model, whose scale is less than 1/10 of the MiDaS\\nmodel, even outperforms MiDaS on several unseen datasets,\\nincluding Sintel, DDAD, and ETH3D. The performance\\nadvantage of these small-scale models demonstrates their\\ngreat potential in computationally-constrained scenarios.\\nIt is also worth noting that, on the most widely used MDE\\n5\\nMethod\\nHigher is better ↑\\nLower is better ↓\\nδ1\\nδ2\\nδ3\\nAbsRel\\nRMSE\\nlog10\\nAdaBins [3]\\n0.903\\n0.984\\n0.997\\n0.103\\n0.364\\n0.044\\nDPT [46]\\n0.904\\n0.988\\n0.998\\n0.110\\n0.357\\n0.045\\nP3Depth [43]\\n0.898\\n0.981\\n0.996\\n0.104\\n0.356\\n0.043\\nSwinV2-L [39]\\n0.949\\n0.994\\n0.999\\n0.083\\n0.287\\n0.035\\nAiT [41]\\n0.954\\n0.994\\n0.999\\n0.076\\n0.275\\n0.033\\nVPD [86]\\n0.964\\n0.995\\n0.999\\n0.069\\n0.254\\n0.030\\nZoeDepth∗ [4]\\n0.951\\n0.994\\n0.999\\n0.077\\n0.282\\n0.033\\nOurs\\n0.984\\n0.998\\n1.000\\n0.056\\n0.206\\n0.024\\nTable 3. Fine-tuning and evaluating on NYUv2 [54] with our\\npre-trained MDE encoder. We highlight best, second best results,\\nas well as most discriminative metrics. ∗: Reproduced by us.\\nbenchmarks KITTI and NYUv2, although MiDaS v3.1 uses\\nthe corresponding training images (not zero-shot anymore),\\nour Depth Anything is still evidently superior to it without\\ntraining with any KITTI or NYUv2 images, e.g., 0.127 vs.\\n0.076 in AbsRel and 0.850 vs. 0.947 in δ1 on KITTI.\\n4.3. Fine-tuned to Metric Depth Estimation\\nApart from the impressive performance in zero-shot relative\\ndepth estimation, we further examine our Depth Anything\\nmodel as a promising weight initialization for downstream\\nmetric depth estimation. We initialize the encoder of down-\\nstream MDE models with our pre-trained encoder parameters\\nand leave the decoder randomly initialized. The model is\\nfine-tuned with correponding metric depth information. In\\nthis part, we use our ViT-L encoder for fine-tuning.\\nWe examine two representative scenarios: 1) in-domain\\nmetric depth estimation, where the model is trained and\\nevaluated on the same domain (Section 4.3.1), and 2) zero-\\nshot metric depth estimation, where the model is trained on\\none domain, e.g., NYUv2 [54], but evaluated in different\\ndomains, e.g., SUN RGB-D [56] (Section 4.3.2).\\n4.3.1\\nIn-Domain Metric Depth Estimation\\nAs shown in Table 3 of NYUv2 [54], our model outperforms\\nthe previous best method VPD [86] remarkably, improving\\nthe δ1 (↑) from 0.964 → 0.984 and AbsRel (↓) from 0.069\\nto 0.056. Similar improvements can be observed in Table 4\\nof the KITTI dataset [18]. We improve the δ1 (↑) on KITTI\\nfrom 0.978 → 0.982. It is worth noting that we adopt the\\nZoeDepth framework for this scenario with a relatively ba-\\nsic depth model, and we believe our results can be further\\nenhanced if equipped with more advanced architectures.\\n4.3.2\\nZero-Shot Metric Depth Estimation\\nWe follow ZoeDepth [4] to conduct zero-shot metric depth\\nestimation. ZoeDepth fine-tunes the MiDaS pre-trained en-\\nMethod\\nHigher is better ↑\\nLower is better ↓\\nδ1\\nδ2\\nδ3\\nAbsRel\\nRMSE\\nRMSE log\\nAdaBins [3]\\n0.964\\n0.995\\n0.999\\n0.058\\n2.360\\n0.088\\nDPT [46]\\n0.959\\n0.995\\n0.999\\n0.062\\n2.573\\n0.092\\nP3Depth [43]\\n0.953\\n0.993\\n0.998\\n0.071\\n2.842\\n0.103\\nNeWCRFs [82]\\n0.974\\n0.997\\n0.999\\n0.052\\n2.129\\n0.079\\nSwinV2-L [39]\\n0.977\\n0.998\\n1.000\\n0.050\\n1.966\\n0.075\\nNDDepth [53]\\n0.978\\n0.998\\n0.999\\n0.050\\n2.025\\n0.075\\nGEDepth [75]\\n0.976\\n0.997\\n0.999\\n0.048\\n2.044\\n0.076\\nZoeDepth∗ [4]\\n0.971\\n0.996\\n0.999\\n0.054\\n2.281\\n0.082\\nOurs\\n0.982\\n0.998\\n1.000\\n0.046\\n1.896\\n0.069\\nTable 4. Fine-tuning and evaluating on KITTI [18] with our\\npre-trained MDE encoder. ∗: Reproduced by us.\\ncoder with metric depth information from NYUv2 [54] (for\\nindoor scenes) or KITTI [18] (for outdoor scenes). There-\\nfore, we simply replace the MiDaS encoder with our bet-\\nter Depth Anything encoder, leaving other components un-\\nchanged. As shown in Table 5, across a wide range of unseen\\ndatasets of indoor and outdoor scenes, our Depth Anything\\nresults in a better metric depth estimation model than the\\noriginal ZoeDepth based on MiDaS.\\n4.4. Fine-tuned to Semantic Segmentation\\nIn our method, we design our MDE model to inherit the\\nrich semantic priors from a pre-trained encoder via a sim-\\nple feature alignment constraint. Here, we examine the\\nsemantic capability of our MDE encoder. Specifically, we\\nfine-tune our MDE encoder to downstream semantic segmen-\\ntation datasets. As exhibited in Table 7 of the Cityscapes\\ndataset [15], our encoder from large-scale MDE training\\n(86.2 mIoU) is superior to existing encoders from large-scale\\nImageNet-21K pre-training, e.g., Swin-L [38] (84.3) and\\nConvNeXt-XL [40] (84.6). Similar observations hold on the\\nADE20K dataset [88] in Table 8. We improve the previous\\nbest result from 58.3 → 59.4.\\nWe hope to highlight that, witnessing the superiority of\\nour pre-trained encoder on both monocular depth estimation\\nand semantic segmentation tasks, we believe it has great\\npotential to serve as a generic multi-task encoder for both\\nmiddle-level and high-level visual perception systems.\\n4.5. Ablation Studies\\nUnless otherwise specified, we use the ViT-L encoder for\\nour ablation studies here.\\nZero-shot transferring of each training dataset. In Ta-\\nble 6, we provide the zero-shot transferring performance of\\neach training dataset, which means that we train a relative\\nMDE model on one training set and evaluate it on the six\\nunseen datasets. With these results, we hope to offer more\\ninsights for future works that similarly aim to build a general\\n6\\nMethod\\nSUN RGB-D [56]\\niBims-1 [29]\\nHyperSim [48]\\nVirtual KITTI 2 [8]\\nDIODE Outdoor [59]\\nAbsRel (↓)\\nδ1 (↑)\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nZoeDepth [4]\\n0.520\\n0.545\\n0.169\\n0.656\\n0.407\\n0.302\\n0.106\\n0.844\\n0.814\\n0.237\\nDepth Anything\\n0.500\\n0.660\\n0.150\\n0.714\\n0.363\\n0.361\\n0.085\\n0.913\\n0.794\\n0.288\\nTable 5. Zero-shot metric depth estimation. The first three test sets in the header are indoor scenes, while the last two are outdoor scenes.\\nFollowing ZoeDepth, we use the model trained on NYUv2 for indoor generalization, while use the model trained on KITTI for outdoor\\nevaluation. For fair comparisons, we report the ZoeDepth results reproduced in our environment.\\nTraining set\\nKITTI [18]\\nNYUv2 [54]\\nSintel [7]\\nDDAD [20]\\nETH3D [51]\\nDIODE [59]\\nMean\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nBlendedMVS [76]\\n0.089\\n0.918\\n0.068\\n0.958\\n0.556\\n0.689\\n0.305\\n0.731\\n0.148\\n0.845\\n0.092\\n0.921\\n0.210\\n0.844\\nDIML [13]\\n0.099\\n0.907\\n0.055\\n0.969\\n0.573\\n0.722\\n0.381\\n0.657\\n0.142\\n0.859\\n0.107\\n0.908\\n0.226\\n0.837\\nHRWSI [67]\\n0.095\\n0.917\\n0.062\\n0.966\\n0.502\\n0.731\\n0.270\\n0.750\\n0.186\\n0.775\\n0.087\\n0.935\\n0.200\\n0.846\\nIRS [61]\\n0.105\\n0.892\\n0.057\\n0.970\\n0.568\\n0.714\\n0.328\\n0.691\\n0.143\\n0.845\\n0.088\\n0.926\\n0.215\\n0.840\\nMegaDepth [33]\\n0.217\\n0.741\\n0.071\\n0.953\\n0.632\\n0.660\\n0.479\\n0.566\\n0.142\\n0.852\\n0.104\\n0.910\\n0.274\\n0.780\\nTartanAir [62]\\n0.088\\n0.920\\n0.061\\n0.964\\n0.602\\n0.723\\n0.332\\n0.690\\n0.160\\n0.818\\n0.088\\n0.928\\n0.222\\n0.841\\nAll labeled data\\n0.085\\n0.934\\n0.053\\n0.971\\n0.492\\n0.748\\n0.245\\n0.771\\n0.134\\n0.874\\n0.070\\n0.945\\n0.180\\n0.874\\nTable 6. Examine the zero-shot transferring performance of each labeled training set (left) to six unseen datasets (top). Better performance:\\nAbsRel ↓ , δ1 ↑. We highlight the best, second, and third best results for each test dataset in bold, underline, and italic, respectively.\\nMethod\\nEncoder\\nmIoU (s.s.)\\nm.s.\\nSegmenter [57]\\nViT-L [16]\\n-\\n82.2\\nSegFormer [69]\\nMiT-B5 [69]\\n82.4\\n84.0\\nMask2Former [12]\\nSwin-L [38]\\n83.3\\n84.3\\nOneFormer [24]\\nSwin-L [38]\\n83.0\\n84.4\\nOneFormer [24]\\nConvNeXt-XL [40]\\n83.6\\n84.6\\nDDP [25]\\nConvNeXt-L [40]\\n83.2\\n83.9\\nOurs\\nViT-L [16]\\n84.8\\n86.2\\nTable 7. Transferring our MDE pre-trained encoder to Cityscapes\\nfor semantic segmentation. We do not use Mapillary [1] for pre-\\ntraining. s.s./m.s.: single-/multi-scale evaluation.\\nmonocular depth estimation system. Among the six training\\ndatasets, HRWSI [67] fuels our model with the strongest\\ngeneralization ability, even though it only contains 20K im-\\nages. This indicates the data diversity counts a lot, which\\nis well aligned with our motivation to utilize unlabeled im-\\nages. Some labeled datasets may not perform very well, e.g.,\\nMegaDepth [33], however, it has its own preferences that\\nare not reflected in these six test datasets. For example, we\\nfind models trained with MegaDepth data are specialized at\\nestimating the distance of ultra-remote buildings (Figure 1),\\nwhich will be very beneficial for aerial vehicles.\\nEffectiveness of 1) challenging the student model when\\nlearning unlabeled images, and 2) semantic constraint.\\nAs shown in Table 9, simply adding unlabeled images with\\npseudo labels does not necessarily bring gains to our model,\\nMethod\\nEncoder\\nmIoU\\nSegmenter [57]\\nViT-L [16]\\n51.8\\nSegFormer [69]\\nMiT-B5 [69]\\n51.0\\nMask2Former [12]\\nSwin-L [38]\\n56.4\\nUperNet [68]\\nBEiT-L [2]\\n56.3\\nViT-Adapter [11]\\nBEiT-L [2]\\n58.3\\nOneFormer [24]\\nSwin-L [38]\\n57.4\\nOneFormer [24]\\nConNeXt-XL [40]\\n57.4\\nOurs\\nViT-L [16]\\n59.4\\nTable 8. Transferring our MDE encoder to ADE20K for semantic\\nsegmentation. We use Mask2Former as our segmentation model.\\nsince the labeled images are already sufficient. However,\\nwith strong perturbations (S) applied to unlabeled images\\nduring re-training, the student model is challenged to seek\\nadditional visual knowledge and learn more robust repre-\\nsentations. Consequently, the large-scale unlabeled images\\nenhance the model generalization ability significantly.\\nMoreover, with our used semantic constraint Lfeat, the\\npower of unlabeled images can be further amplified for the\\ndepth estimation task. More importantly, as emphasized in\\nSection 4.4, this auxiliary constraint also enables our trained\\nencoder to serve as a key component in a multi-task visual\\nsystem for both middle-level and high-level perception.\\nComparison with MiDaS trained encoder in downstream\\ntasks. Our Depth Anything model has exhibited stronger\\nzero-shot capability than MiDaS [5, 45]. Here, we further\\n7\\nFigure 3. Qualitative results on six unseen datasets.\\nLl Lu\\nS\\nLfeat\\nKI\\nNY\\nSI\\nDD\\nET\\nDI\\n✓\\n0.085 0.053 0.492 0.245 0.134 0.070\\n✓\\n✓\\n0.085 0.054 0.481 0.242 0.138 0.073\\n✓\\n✓\\n✓\\n0.081 0.048 0.469 0.235 0.134 0.068\\n✓\\n✓\\n✓\\n✓\\n0.076 0.043 0.458 0.230 0.127 0.066\\nTable 9. Ablation studies of: 1) challenging the student with strong\\nperturbations (S) when learning unlabeled images, and 2) semantic\\nconstraint (Lfeat). Limited by space, we only report the AbsRel\\n(↓) metric, and shorten the dataset name with its first two letters.\\nMethod\\nNYUv2\\nKITTI\\nCityscapes ADE20K\\nAbsRel\\nδ1\\nAbsRel\\nδ1\\nmIoU\\nmIoU\\nMiDaS\\n0.077\\n0.951\\n0.054\\n0.971\\n82.1\\n52.4\\nOurs\\n0.056\\n0.984\\n0.046\\n0.982\\n84.8\\n59.4\\nTable 10. Comparison between our trained encoder and MiDaS [5]\\ntrained encoder in terms of downstream fine-tuning performance.\\nBetter performance: AbsRel ↓ , δ1 ↑ , mIoU ↑ .\\ncompare our trained encoder with MiDaS v3.1 [5] trained\\nencoder in terms of the downstream fine-tuning performance.\\nAs demonstrated in Table 10, on both the downstream depth\\nestimation task and semantic segmentation task, our pro-\\nduced encoder outperforms the MiDaS encoder remarkably,\\ne.g., 0.951 vs. 0.984 in the δ1 metric on NYUv2, and 52.4\\nvs. 59.4 in the mIoU metric on ADE20K.\\nComparison with DINOv2 in downstream tasks. We have\\ndemonstrated the superiority of our trained encoder when\\nfine-tuned to downstream tasks. Since our finally produced\\nencoder (from large-scale MDE training) is fine-tuned from\\nDINOv2 [42], we compare our encoder with the original\\nDINOv2 encoder in Table 11. It can be observed that our\\nencoder performs better than the original DINOv2 encoder\\nin both the downstream metric depth estimation task and\\nsemantic segmentation task. Although the DINOv2 weight\\nhas provided a very strong initialization (also much better\\nthan the MiDaS encoder as reported in Table 10), our large-\\nscale and high-quality MDE training can further enhance it\\nOurs\\nMiDaS\\nOurs\\nMiDaS\\nFigure 4. We compare our depth prediction with MiDaS. Meantime,\\nwe use ControlNet to synthesize new images from the depth map\\n(the last row). First row: input image, second row: depth prediction.\\nEncoder\\nNYUv2\\nKITTI\\nADE20K\\nAbsRel (↓)\\nδ1 (↑)\\nAbsRel\\nδ1\\nmIoU (↑)\\nDINOv2\\n0.066\\n0.973\\n0.058\\n0.971\\n58.8\\nOurs\\n0.056\\n0.984\\n0.046\\n0.982\\n59.4\\nTable 11. Comparison between the original DINOv2 and our pro-\\nduced encoder in terms of downstream fine-tuning performance.\\nimpressively in downstream transferring performance.\\n4.6. Qualitative Results\\nWe visualize our model predictions on the six unseen datasets\\nin Figure 3. Our model is robust to test images from various\\ndomains. In addition, we compare our model with MiDaS\\nin Figure 4. We also attempt to synthesis new images con-\\nditioned on the predicted depth maps with ControlNet [84].\\nOur model produces more accurate depth estimation than\\nMiDaS, as well as better synthesis results, although the Con-\\ntrolNet is trained with MiDaS depth. For more accurate\\nsynthesis, we have also re-trained a better depth-conditioned\\nControlNet based on our Depth Anything, aiming to pro-\\nvide better control signals for image synthesis and video\\nediting. Please refer to our project page or the following\\nsupplementary material for more qualitative results,\\n5. Conclusion\\nIn this work, we present Depth Anything, a highly practical\\nsolution to robust monocular depth estimation. Different\\nfrom prior arts, we especially highlight the value of cheap\\nand diverse unlabeled images. We design two simple yet\\nhighly effective strategies to fully exploit their value: 1)\\nposing a more challenging optimization target when learning\\nunlabeled images, and 2) preserving rich semantic priors\\nfrom pre-trained models. As a result, our Depth Anything\\nmodel exhibits excellent zero-shot depth estimation ability,\\nand also serves as a promising initialization for downstream\\nmetric depth estimation and semantic segmentation tasks.\\n8\\nDepth Anything: Unleashing the Power of Large-Scale Unlabeled Data\\nSupplementary Material\\n6. More Implementation Details\\nWe resize the shorter side of all images to 518 and keep the\\noriginal aspect ratio. All images are cropped to 518×518\\nduring training. During inference, we do not crop images\\nand only ensure both sides are multipliers of 14, since the\\npre-defined patch size of DINOv2 encoders [42] is 14. Eval-\\nuation is performed at the original resolution by interpolating\\nthe prediction. Following MiDaS [5, 45], in zero-shot eval-\\nuation, the scale and shift of our prediction are manually\\naligned with the ground truth.\\nWhen fine-tuning our pre-trained encoder to metric depth\\nestimation, we adopt the ZoeDepth codebase [4]. We merely\\nreplace the original MiDaS-based encoder with our stronger\\nDepth Anything encoder, with a few hyper-parameters mod-\\nified. Concretely, the training resolution is 392×518 on\\nNYUv2 [54] and 384×768 on KITTI [18] to match the\\npatch size of our encoder. The encoder learning rate is\\nset as 1/50 of the learning rate of the randomly initialized\\ndecoder, which is much smaller than the 1/10 adopted for\\nMiDaS encoder, due to our strong initialization. The batch\\nsize is 16 and the model is trained for 5 epochs.\\nWhen fine-tuning our pre-trained encoder to semantic seg-\\nmentation, we use the MMSegmentation codebase [14]. The\\ntraining resolution is set as 896×896 on both ADE20K [88]\\nand Cityscapes [15]. The encoder learning rate is set as\\n3e-6 and the decoder learning rate is 10× larger. We use\\nMask2Former [12] as our semantic segmentation model. The\\nmodel is trained for 160K iterations on ADE20K and 80K\\niterations on Cityscapes both with batch size 16, without\\nany COCO [35] or Mapillary [1] pre-training. Other training\\nconfigurations are the same as the original codebase.\\n7. More Ablation Studies\\nAll ablation studies here are conducted on the ViT-S model.\\nThe necessity of tolerance margin for feature alignment.\\nAs shown in Table 12, the gap between the tolerance margin\\nof 0 and 0.15 or 0.30 clearly demonstrates the necessity of\\nthis design (mean AbsRel: 0.188 vs. 0.175).\\nApplying feature alignment to labeled data. Previously,\\nwe enforce the feature alignment loss Lfeat on unlabeled\\ndata. Indeed, it is technically feasible to also apply this\\nconstraint to labeled data. In Table 13, apart from applying\\nLfeat on unlabeled data, we explore to apply it to labeled\\ndata. We find that adding this auxiliary optimization target\\nto labeled data is not beneficial to our baseline that does not\\ninvolve any feature alignment (their mean AbsRel values are\\nalmost the same: 0.180 vs. 0.179). We conjecture that this is\\nα\\nKITTI NYU Sintel DDAD ETH3D DIODE Mean\\n0.00\\n0.085\\n0.055 0.523\\n0.250\\n0.134\\n0.079\\n0.188\\n0.15\\n0.080\\n0.053 0.464\\n0.247\\n0.127\\n0.076\\n0.175\\n0.30\\n0.079\\n0.054 0.482\\n0.248\\n0.127\\n0.077\\n0.178\\nTable 12. Ablation studies on different values of the tolerance\\nmargin α for the feature alignment loss Lfeat. Limited by space,\\nwe only report the AbsRel (↓) metric here.\\nLfeat\\nUnseen datasets (AbsRel ↓)\\nMean\\nU\\nL\\nKITTI NYU Sintel DDAD ETH3D DIODE\\n0.083\\n0.055 0.478\\n0.249\\n0.133\\n0.080\\n0.180\\n✓\\n0.080\\n0.053 0.464\\n0.247\\n0.127\\n0.076\\n0.175\\n✓\\n0.084\\n0.054 0.472\\n0.252\\n0.133\\n0.081\\n0.179\\nTable 13. Ablation studies of applying our feature alignment loss\\nLfeat to unlabeled data (U) or labeled data (L).\\nbecause the labeled data has relatively higher-quality depth\\nannotations. The involvement of semantic loss may interfere\\nwith the learning of these informative manual labels. In com-\\nparison, our pseudo labels are noisier and less informative.\\nTherefore, introducing the auxiliary constraint to unlabeled\\ndata can combat the noise in pseudo depth labels, as well as\\narm our model with semantic capability.\\n8. Limitations and Future Works\\nCurrently, the largest model size is only constrained to ViT-\\nLarge [16]. Therefore, in the future, we plan to further scale\\nup the model size from ViT-Large to ViT-Giant, which is\\nalso well pre-trained by DINOv2 [42]. We can train a more\\npowerful teacher model with the larger model, producing\\nmore accurate pseudo labels for smaller models to learn, e.g.,\\nViT-L and ViT-B. Furthermore, to facilitate real-world ap-\\nplications, we believe the widely adopted 512×512 training\\nresolution is not enough. We plan to re-train our model on a\\nlarger resolution of 700+ or even 1000+.\\n9. More Qualitative Results\\nPlease refer to the following pages for comprehensive quali-\\ntative results on six unseen test sets (Figure 5 for KITTI [18],\\nFigure 6 for NYUv2 [54], Figure 7 for Sintel [7], Figure 8\\nfor DDAD [20], Figure 9 for ETH3D [51], and Figure 10\\nfor DIODE [59]). We compare our model with the strongest\\nMiDaS model [5], i.e., DPT-BEiTL-512. Our model exhibits\\nhigher depth estimation accuracy and stronger robustness.\\n9\\nInput image\\nOur prediction\\nMiDaS v3.1 prediction\\nFigure 5. Qualitative results on KITTI. Due to the extremely sparse ground truth which is hard to visualize, we here compare our prediction\\nwith the most advanced MiDaS v3.1 [5] prediction. The brighter color denotes the closer distance.\\n10\\nInput image\\nOur prediction\\nMiDaS v3.1 prediction\\nFigure 6. Qualitative results on NYUv2. It is worth noting that MiDaS [5] uses NYUv2 training data (not zero-shot), while we do not.\\n11\\nInput image\\nOur prediction\\nMiDaS v3.1 prediction\\nFigure 7. Qualitative results on Sintel.\\n12\\nInput image\\nOur prediction\\nMiDaS v3.1 prediction\\nFigure 8. Qualitative results on DDAD.\\n13\\nInput image\\nOur prediction\\nMiDaS v3.1 prediction\\nFigure 9. Qualitative results on ETH3D.\\n14\\nInput image\\nOur prediction\\nMiDaS v3.1 prediction\\nFigure 10. Qualitative results on DIODE.\\n15\\nReferences\\n[1] Manuel L´opez Antequera, Pau Gargallo, Markus Hofinger,\\nSamuel Rota Bul`o, Yubin Kuang, and Peter Kontschieder.\\nMapillary planet-scale depth dataset. In ECCV, 2020. 7, 9\\n[2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\\nBert pre-training of image transformers. In ICLR, 2022. 7\\n[3] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.\\nAdabins: Depth estimation using adaptive bins. In CVPR,\\n2021. 2, 6\\n[4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,\\nand Matthias M¨uller. Zoedepth: Zero-shot transfer by com-\\nbining relative and metric depth. arXiv:2302.12288, 2023. 2,\\n6, 7, 9\\n[5] Reiner Birkl, Diana Wofk, and Matthias M¨uller. Midas v3.\\n1–a model zoo for robust monocular relative depth estimation.\\narXiv:2307.14460, 2023. 2, 3, 5, 7, 8, 9, 10, 11\\n[6] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ\\nAltman, Simran Arora, Sydney von Arx, Michael S Bern-\\nstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,\\net al. On the opportunities and risks of foundation models.\\narXiv:2108.07258, 2021. 1\\n[7] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J\\nBlack. A naturalistic open source movie for optical flow\\nevaluation. In ECCV, 2012. 5, 7, 9\\n[8] Yohann Cabon, Naila Murray, and Martin Humenberger. Vir-\\ntual kitti 2. arXiv:2001.10773, 2020. 7\\n[9] Po-Yi Chen, Alexander H Liu, Yen-Cheng Liu, and Yu-\\nChiang Frank Wang.\\nTowards scene understanding: Un-\\nsupervised monocular depth estimation with semantic-aware\\nrepresentation. In CVPR, 2019. 2, 4\\n[10] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-\\nimage depth perception in the wild. In NeurIPS, 2016. 2\\n[11] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong\\nLu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for\\ndense predictions. In ICLR, 2023. 7\\n[12] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander\\nKirillov, and Rohit Girdhar. Masked-attention mask trans-\\nformer for universal image segmentation. In CVPR, 2022. 7,\\n9\\n[13] Jaehoon Cho, Dongbo Min, Youngjung Kim, and Kwanghoon\\nSohn. Diml/cvl rgb-d dataset: 2m rgb-d images of natural\\nindoor and outdoor scenes. arXiv:2110.11590, 2021. 3, 7\\n[14] MMSegmentation\\nContributors.\\nMMSegmenta-\\ntion:\\nOpenmmlab\\nsemantic\\nsegmentation\\ntoolbox\\nand benchmark.\\nhttps : / / github . com / open -\\nmmlab/mmsegmentation, 2020. 9\\n[15] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,\\nStefan Roth, and Bernt Schiele. The cityscapes dataset for\\nsemantic urban scene understanding. In CVPR, 2016. 1, 6, 9\\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. In ICLR, 2021. 7,\\n9\\n[17] David Eigen, Christian Puhrsch, and Rob Fergus. Depth\\nmap prediction from a single image using a multi-scale deep\\nnetwork. In NeurIPS, 2014. 2\\n[18] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\\nUrtasun. Vision meets robotics: The kitti dataset. IJRR, 2013.\\n1, 2, 3, 5, 6, 7, 9\\n[19] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin\\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-\\nsch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\\nlaghi Azar, et al. Bootstrap your own latent-a new approach\\nto self-supervised learning. In NeurIPS, 2020. 5\\n[20] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos,\\nand Adrien Gaidon. 3d packing for self-supervised monocular\\ndepth estimation. In CVPR, 2020. 5, 7, 9\\n[21] Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, and Adrien\\nGaidon. Semantically-guided representation learning for self-\\nsupervised monocular depth. In ICLR, 2020. 2, 4\\n[22] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares, Ambrus,,\\nand Adrien Gaidon. Towards zero-shot scale-aware monocu-\\nlar depth estimation. In ICCV, 2023. 2\\n[23] Derek Hoiem, Alexei A Efros, and Martial Hebert. Recover-\\ning surface layout from an image. IJCV, 2007. 2\\n[24] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita\\nOrlov, and Humphrey Shi. Oneformer: One transformer to\\nrule universal image segmentation. In CVPR, 2023. 7\\n[25] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu,\\nZhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp:\\nDiffusion model for dense visual prediction. In ICCV, 2023.\\n7\\n[26] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing\\nTai, Chi-Keung Tang, and Fisher Yu. Segment anything in\\nhigh quality. In NeurIPS, 2023. 4\\n[27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\\nthing. In ICCV, 2023. 1, 2, 3\\n[28] Marvin Klingner, Jan-Aike Term¨ohlen, Jonas Mikolajczyk,\\nand Tim Fingscheidt. Self-supervised monocular depth es-\\ntimation: Solving the dynamic object problem by semantic\\nguidance. In ECCV, 2020. 4\\n[29] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco\\nKorner. Evaluation of cnn-based single-image depth estima-\\ntion methods. In ECCVW, 2018. 7\\n[30] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings,\\nIvan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov,\\nMatteo Malloci, Alexander Kolesnikov, et al. The open im-\\nages dataset v4: Unified image classification, object detection,\\nand visual relationship detection at scale. IJCV, 2020. 2, 3\\n[31] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient\\nsemi-supervised learning method for deep neural networks.\\nIn ICMLW, 2013. 2\\n[32] Bo Li, Chunhua Shen, Yuchao Dai, Anton Van Den Hen-\\ngel, and Mingyi He. Depth and surface normal estimation\\nfrom monocular images using regression on deep features and\\nhierarchical crfs. In CVPR, 2015. 2\\n[33] Zhengqi Li and Noah Snavely. Megadepth: Learning single-\\nview depth prediction from internet photos. In CVPR, 2018.\\n1, 3, 7\\n16\\n[34] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang.\\nBinsformer: Revisiting adaptive bins for monocular depth\\nestimation. arXiv:2204.00987, 2022. 2\\n[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nECCV, 2014. 1, 9\\n[36] Ce Liu, Jenny Yuen, Antonio Torralba, Josef Sivic, and\\nWilliam T Freeman. Sift flow: Dense correspondence across\\ndifferent scenes. In ECCV, 2008. 2\\n[37] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\\nZhu, et al. Grounding dino: Marrying dino with grounded\\npre-training for open-set object detection. arXiv:2303.05499,\\n2023. 4\\n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nICCV, 2021. 6, 7\\n[39] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\\nSwin transformer v2: Scaling up capacity and resolution. In\\nCVPR, 2022. 6\\n[40] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\\n2020s. In CVPR, 2022. 6, 7\\n[41] Jia Ning, Chen Li, Zheng Zhang, Chunyu Wang, Zigang\\nGeng, Qi Dai, Kun He, and Han Hu. All in tokens: Unifying\\noutput space of visual tasks via soft token. In ICCV, 2023. 6\\n[42] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy Vo,\\nMarc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel\\nHaziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2:\\nLearning robust visual features without supervision. TMLR,\\n2023. 3, 4, 5, 8, 9\\n[43] Vaishakh Patil, Christos Sakaridis, Alexander Liniger, and\\nLuc Van Gool. P3depth: Monocular depth estimation with a\\npiecewise planarity prior. In CVPR, 2022. 6\\n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervision.\\nIn ICML, 2021. 1\\n[45] Ren´e Ranftl, Katrin Lasinger, David Hafner, Konrad\\nSchindler, and Vladlen Koltun. Towards robust monocular\\ndepth estimation: Mixing datasets for zero-shot cross-dataset\\ntransfer. TPAMI, 2020. 1, 2, 3, 5, 7, 9\\n[46] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\\nsion transformers for dense prediction. In ICCV, 2021. 5,\\n6\\n[47] Alex Rasla and Michael Beyeler. The relative importance\\nof depth cues and semantic edges for indoor mobility using\\nsimulated prosthetic vision in immersive virtual reality. In\\nVRST, 2022. 1\\n[48] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit\\nKumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb,\\nand Joshua M Susskind. Hypersim: A photorealistic synthetic\\ndataset for holistic indoor scene understanding. In ICCV,\\n2021. 7\\n[49] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, et al. Imagenet large scale\\nvisual recognition challenge. IJCV, 2015. 3\\n[50] Ashutosh Saxena, Min Sun, and Andrew Y Ng. Make3d:\\nLearning 3d scene structure from a single still image. TPAMI,\\n2008. 2\\n[51] Thomas Schops, Johannes L Schonberger, Silvano Galliani,\\nTorsten Sattler, Konrad Schindler, Marc Pollefeys, and An-\\ndreas Geiger. A multi-view stereo benchmark with high-\\nresolution images and multi-camera videos. In CVPR, 2017.\\n5, 7, 9\\n[52] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A\\nlarge-scale, high-quality dataset for object detection. In ICCV,\\n2019. 3\\n[53] Shuwei Shao, Zhongcai Pei, Weihai Chen, Xingming Wu, and\\nZhengguo Li. Nddepth: Normal-distance assisted monocular\\ndepth estimation. In ICCV, 2023. 2, 6\\n[54] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\\nFergus. Indoor segmentation and support inference from rgbd\\nimages. In ECCV, 2012. 1, 2, 3, 5, 6, 7, 9\\n[55] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao\\nZhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,\\nAlexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying\\nsemi-supervised learning with consistency and confidence. In\\nNeurIPS, 2020. 2, 4\\n[56] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.\\nSun rgb-d: A rgb-d scene understanding benchmark suite. In\\nCVPR, 2015. 6, 7\\n[57] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia\\nSchmid. Segmenter: Transformer for semantic segmentation.\\nIn ICCV, 2021. 7\\n[58] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-\\ntinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\\nLlama: Open and efficient foundation language models.\\narXiv:2302.13971, 2023. 1\\n[59] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo,\\nHaochen Wang, Falcon Z Dai, Andrea F Daniele, Mo-\\nhammadreza Mostajabi, Steven Basart, Matthew R Walter,\\net al. Diode: A dense indoor and outdoor depth dataset.\\narXiv:1908.00463, 2019. 5, 7, 9\\n[60] Chaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver\\nWang. Web stereo video supervision for depth prediction\\nfrom dynamic scenes. In 3DV, 2019. 3\\n[61] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiy-\\nong Zhao, and Xiaowen Chu. Irs: A large naturalistic indoor\\nrobotics stereo dataset to train deep models for disparity and\\nsurface normal estimation. In ICME, 2021. 3, 7\\n[62] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu,\\nYuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and\\nSebastian Scherer. Tartanair: A dataset to push the limits of\\nvisual slam. In IROS, 2020. 3, 7\\n[63] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariha-\\nran, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar\\nfrom visual depth estimation: Bridging the gap in 3d object\\ndetection for autonomous driving. In CVPR, 2019. 1\\n17\\n[64] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\\nGoogle landmarks dataset v2-a large-scale benchmark for\\ninstance-level recognition and retrieval. In CVPR, 2020. 3\\n[65] Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Karaman,\\nand Vivienne Sze. Fastdepth: Fast monocular depth estima-\\ntion on embedded systems. In ICRA, 2019. 1\\n[66] Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao,\\nRuibo Li, and Zhenbo Luo. Monocular relative depth per-\\nception with web stereo data supervision. In CVPR, 2018. 2,\\n3\\n[67] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin,\\nand Zhiguo Cao. Structure-guided ranking loss for single\\nimage depth prediction. In CVPR, 2020. 2, 3, 7\\n[68] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\\nJian Sun. Unified perceptual parsing for scene understanding.\\nIn ECCV, 2018. 7\\n[69] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\\nJose M Alvarez, and Ping Luo.\\nSegformer: Simple and\\nefficient design for semantic segmentation with transformers.\\nIn NeurIPS, 2021. 3, 7\\n[70] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan\\nWang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-end\\nsemi-supervised object detection with soft teacher. In ICCV,\\n2021. 2\\n[71] Xiaogang Xu, Hengshuang Zhao, Vibhav Vineet, Ser-Nam\\nLim, and Antonio Torralba. Mtformer: Multi-task learning\\nvia transformer and cross-task reasoning. In ECCV, 2022. 4\\n[72] I Zeki Yalniz, Herv´e J´egou, Kan Chen, Manohar Paluri, and\\nDhruv Mahajan. Billion-scale semi-supervised learning for\\nimage classification. arXiv:1905.00546, 2019. 2\\n[73] Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao.\\nSt++: Make self-training work better for semi-supervised\\nsemantic segmentation. In CVPR, 2022. 4\\n[74] Lihe Yang, Lei Qi, Litong Feng, Wayne Zhang, and\\nYinghuan Shi. Revisiting weak-to-strong consistency in semi-\\nsupervised semantic segmentation. In CVPR, 2023. 2\\n[75] Xiaodong Yang, Zhuang Ma, Zhiyu Ji, and Zhe Ren. Gedepth:\\nGround embedding for monocular depth estimation. In ICCV,\\n2023. 2, 6\\n[76] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,\\nLei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-\\nscale dataset for generalized multi-view stereo networks. In\\nCVPR, 2020. 3, 7\\n[77] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. En-\\nforcing geometric constraints of virtual normal for depth pre-\\ndiction. In ICCV, 2019. 2\\n[78] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaix-\\nuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d:\\nTowards zero-shot metric 3d prediction from a single image.\\nIn ICCV, 2023. 2\\n[79] Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg, Ge-\\noff Pleiss, Bharath Hariharan, Mark Campbell, and Kilian Q\\nWeinberger. Pseudo-lidar++: Accurate depth for 3d object\\ndetection in autonomous driving. In ICLR, 2020. 1\\n[80] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas\\nFunkhouser, and Jianxiong Xiao. Lsun: Construction of a\\nlarge-scale image dataset using deep learning with humans in\\nthe loop. arXiv:1506.03365, 2015. 3\\n[81] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying\\nChen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-\\nrell. Bdd100k: A diverse driving dataset for heterogeneous\\nmultitask learning. In CVPR, 2020. 2, 3\\n[82] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and\\nPing Tan. New crfs: Neural window fully-connected crfs for\\nmonocular depth estimation. arXiv:2203.01502, 2022. 2, 6\\n[83] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\\nlarization strategy to train strong classifiers with localizable\\nfeatures. In ICCV, 2019. 4\\n[84] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\\nconditional control to text-to-image diffusion models. In\\nICCV, 2023. 8\\n[85] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,\\nZhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian\\nLi, Shilong Liu, et al. Recognize anything: A strong image\\ntagging model. arXiv:2306.03514, 2023. 4\\n[86] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie\\nZhou, and Jiwen Lu. Unleashing text-to-image diffusion\\nmodels for visual perception. In ICCV, 2023. 6\\n[87] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,\\nand Antonio Torralba. Places: A 10 million image database\\nfor scene recognition. TPAMI, 2017. 3\\n[88] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Bar-\\nriuso, and Antonio Torralba. Scene parsing through ade20k\\ndataset. In CVPR, 2017. 6, 9\\n[89] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanx-\\niao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-\\ntraining and self-training. In NeurIPS, 2020. 2\\n18\\n'},\n",
       " {'abstract': 'Identifying and responding to events, particularly disasters, is of utmost importance. Mobile data and network connectivity generate many temporally–and spatially–stamped data. Mobile data can infer human mobility, proximity, and built environments. Satellite imagery can capture changes to built and natural environments. We propose a method to fuse satellite imagery with privacy–enhanced mobile data to improve event inference. Our method can be used for small–scale disaster detection, search and rescue operations, and identifying conflict areas. Our case study demonstrates the effectiveness of the proposed method.',\n",
       "  'introduction': 'Rapid identification and response to breaking events, particularly those that pose a threat to human life such as natural disasters or conflicts, is of paramount importance. The effectiveness of response efforts can be significantly impacted by the speed and accuracy of information gathering, analysis, and dissemination, with the potential to save lives and mitigate damage. However, obtaining such information poses significant challenges due to various factors. For example, there have been instances where news agencies faced difficulties in promptly or completely delivering information about ongoing events due to a scarcity of reporters present in the affected areas Stelter and Cohen [2008]. The ubiquity of mobile devices and the near-constant network connectivity that characterizes our modern era have resulted in the generation of a massive amount of temporally- and spatially-stamped data (hereafter called mobile data). These datasets have been the subject of numerous studies aiming to derive individual human mobility patterns for various applications, including quantifying urban vitality Sulis et al. [2018], modeling epidemic spread Alessandretti [2022], and inferring the activities of individuals Liao et al. [2007]. Simultaneously, the increasing number of orbital satellites has facilitated the collection of high-resolution images, capturing snapshots of geographical areas with sub-daily temporal frequency.',\n",
       "  'literature_review': 'Historically, capturing changes in the built and natural environment often involved on-the-ground data collection and analysis Haddad [2011]. Surveys and field observations have been the most traditional method to capture such changes, and continue to be so today in many industries. Prior to the advent of modern satellites, aerial photography taken from aircrafts was another common method to document large-scale changes to the built environment Bewley [2003]. While these two methods of data collection are sufficient for many studies, they (a) lack the immediacy called for in urgent real-time scenarios and (b) tend to miss developments in the absence of the proper equipment or manpower. In the last three decades, new methods of data collection in the built and natural environment have become prominent, including the use of satellite images, GPS traces from mobile devices, and user-generated content from social media platforms Jongman et al. [2015], Ji et al. [2018], Yin et al. [2021]. This literature review aims to provide a comprehensive understanding of current and recent research in the fields of mobile data analysis, satellite imagery, and crowdsourcing, highlighting works that have fused any two of them to assist in the inference task.',\n",
       "  'methodology': 'Our methodology has two modules—one that deals with the analysis of mobile data, and another with that of satellite imagery. In both modules, we discuss relevant preprocessing methods as well as analysis steps to aid in the inference task. Figure 2 shows the high-level steps in preprocessing and analysis of both datasets while also outlining key geospatial analysis processes for the three use cases discussed in the Introduction.\\nFigure 2. Methodological flowchart; the top bins describe the order of processes employed in this paper, while the bottom outlines key geospatial analysis metrics to carry out the inference taskMobile Data. For mobile data, the general preprocessing workflow is as follows:\\n(1) Spatial Partitioning\\n(2) Coordinate Reference System (CRS) Projection\\n(3) Data FilteringTo avoid wastefulness in computation, we begin by partitioning the mobile data spatially, which we do by drawing a bounding box using four sets of coordinates. In practice, there are several ways to achieve this at scale: Leveraging a geocoding application protocol interface (API) to obtain the coordinates based on an existing point of interest, partitioning by federal and/or state designations such as census tracts, ZIP codes, or county borders, manually typing in a bounding box (i.e., by defining the geographic coordinates of the corners of the box), or obtaining a KMZ file from the National Oceanic and Atmospheric Administration’s Damage Assessment Toolkit. For this effort, we used the last option to derive the centroid coordinates of the affected area around which we drew a 1 km2 bounding square.\\nOnce the data is spatially partitioned, we project coordinates to the appropriate CRS. Different CRSs are designed to accurately represent different aspects of the Earth’s surface. Some are designed to preserve area, while others preserve shape, distance, or direction. In the context of event inference, the optimal choice of CRS should minimize bias in the derived mobility metrics, which may exist due to distortions of distances and areas. Therefore, we use an equal-area projection around UTM Zone 15, which covers the area around our case study.',\n",
       "  'results': 'To validate our proposed methodology, we conducted a case study on the EF-1 tornado that hit near Muskogee, Oklahoma on May 15, 2020. This event provided an opportunity to test our approach in a real-world scenario involving a small-scale disaster in a rural region.\\nAfter spatially partitioning the GPS traces to include the subset within the bounding box of the tornado, we projected all data to UTM Zone 15 and filtered the data by the logic described in Section . Then, we defined the two weeks before the day of the event (May 2 - May 15) as the before, the day of the event (May 15) as the during, and the week after the event (May 15 - May 22) as the after periods and began exploring the data. Figure 4 visualizes the time series of two metrics, highlighting the periods by color; there are no noticeable changes in the radius of gyration, while the number of extended stays at a given location within the ROI increases sharply the day after the event.',\n",
       "  'conclusion': 'We have presented a novel data fusion methodology combining satellite imagery with GPS traces generated from mobile devices. Our approach leverages the strengths of both data types: mobile data provides an approximation of human mobility, proximity to one another, and the built environment, while satellite imagery offers visual insights into physical changes to the built and natural environment. Furthermore, we have demonstrated the case study of a small-scale tornado, showing the promise of our method in identifying anomalies in movement patterns using mobile data, changes to the built environment using satellite imagery, and inferring the cause of both using our fusion methodology.\\nOur methodology is viable due to the increasing accessibility of commercial data providers specializing in remote sensing and mobile devices. This trend is only expected to continue—the number of active orbital satellites has grown exponentially since 2017 orb [2022], while the percentage of Americans that own a smartphone has risen to 97% mob [2021]. The continued commercialization of such products will enable researchers and practitioners to monitor, categorize, and understand the natural and built environments in ways unimaginable just 50 years ago. We hope our method is a step in this direction, enabling others to pursue endeavors made possible by the fusion of these data sources.',\n",
       "  'title': 'Event detection from novel data sources: Leveraging satellite imagery alongside GPS traces',\n",
       "  'author': 'Ekin Ugurel, Steffen Coenen, Minda Zhou Chen, Cynthia Chen',\n",
       "  'textdata': 'EVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING\\nSATELLITE IMAGERY ALONGSIDE GPS TRACES\\nEKIN UGUREL\\nUniversity of Washington, Seattle, WA\\nugurel@uw.edu\\nSTEFFEN COENEN\\nDKS Associates, Seattle, WA\\nscoenen@uw.edu\\nMINDA ZHOU CHEN\\nUniversity of Washington, Seattle, WA\\nmindac@uw.edu\\nCYNTHIA CHEN\\nUniversity of Washington, Seattle, WA\\nqzchen@uw.edu\\nAbstract\\nRapid identification and response to breaking events, particularly those that pose a threat to\\nhuman life such as natural disasters or conflicts, is of paramount importance.\\nThe prevalence\\nof mobile devices and the ubiquity of network connectivity has generated a massive amount of\\ntemporally- and spatially-stamped data. Numerous studies have used mobile data to derive in-\\ndividual human mobility patterns for various applications.\\nSimilarly, the increasing number of\\norbital satellites has made it easier to gather high-resolution images capturing a snapshot of a\\ngeographical area in sub-daily temporal frequency. We propose a novel data fusion methodology\\nintegrating satellite imagery with privacy-enhanced mobile data to augment the event inference\\ntask, whether in real-time or historical. In the absence of boots on the ground, mobile data is able\\nto give an approximation of human mobility, proximity to one another, and the built environment.\\nOn the other hand, satellite imagery can provide visual information on physical changes to the built\\nand natural environment. The expected use cases for our methodology include small-scale disas-\\nter detection (i.e., tornadoes, wildfires, and floods) in rural regions, search and rescue operation\\naugmentation for lost hikers in remote wilderness areas, and identification of active conflict areas\\nand population displacement in war-torn states. Our implementation is open-source on GitHub:\\nhttps://github.com/ekinugurel/SatMobFusion.\\nDate: January 22, 2024.\\n1\\narXiv:2401.10890v1  [cs.CV]  19 Jan 2024\\n2EVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES\\nIntroduction\\nIn an increasingly interconnected world, the ability to rapidly identify and respond to breaking\\nevents, particularly those that pose a threat to human life such as natural disasters or conflicts, is\\nof paramount importance. The effectiveness of response efforts can be significantly impacted by the\\nspeed and accuracy of information gathering, analysis, and dissemination, with the potential to save\\nlives and mitigate damage. However, obtaining such information poses significant challenges due\\nto various factors. For example, there have been instances where news agencies faced difficulties in\\npromptly or completely delivering information about ongoing events due to a scarcity of reporters\\npresent in the affected areas Stelter and Cohen [2008]. The ubiquity of mobile devices and the near-\\nconstant network connectivity that characterizes our modern era have resulted in the generation of\\na massive amount of temporally- and spatially-stamped data (hereafter called mobile data). These\\ndatasets have been the subject of numerous studies aiming to derive individual human mobility\\npatterns for various applications, including quantifying urban vitality Sulis et al. [2018], modeling\\nepidemic spread Alessandretti [2022], and inferring the activities of individuals Liao et al. [2007].\\nSimultaneously, the increasing number of orbital satellites has facilitated the collection of high-\\nresolution images, capturing snapshots of geographical areas with sub-daily temporal frequency.\\nIn this paper, we propose a novel data fusion methodology that integrates satellite imagery with\\nmobile data to identify breaking events in real time. This approach leverages the strengths of both\\ndata types: mobile data provides an approximation of human mobility, proximity to one another,\\nand the built environment, while satellite imagery offers visual information on physical changes to\\nthe built and natural environment. This method holds significant potential for three critical appli-\\ncations. First, it can enhance the detection of small-scale disasters such as tornadoes, wildfires, and\\nfloods in rural regions, where sparse populations and limited infrastructure can impede timely de-\\ntection and response. Second, it can augment search and rescue operations for lost hikers in remote\\nwilderness areas, providing critical geospatial data to guide rescue teams and improve the chances\\nof a successful rescue. Before and after images of the Aru Glacier avalanche in Figure 1 suggest\\nthat large-scale changes in the natural environment can be inferred rather straightforwardly using\\nsatellite imagery Tian et al. [2017]. Finally, it can aid in the identification of active conflict areas\\nand population displacement in war-torn states, offering invaluable insights to humanitarian orga-\\nnizations and policy-makers. By leveraging the strengths of both satellite imagery and GPS traces,\\nthis approach overcomes the limitations of both data types and augments the ability to monitor,\\nunderstand, and respond to dynamic events in diverse contexts. We demonstrate an application of\\nthe proposed method on a small-scale tornado that ripped through Muskogee, Oklahoma on May\\n15th, 2020. By combining mobile data from the area as well as satellite images before and after\\nthe event, we infer the occurrence of a natural disaster, identify damaged areas, and highlight hot\\nspots for individuals’ travel through an area (both spatially and temporally).\\nOur Contributions.\\n• We develop a practice-oriented data fusion methodology to augment the inference task for\\na variety of use cases;\\n• We outline expected use cases and demonstrate the case study of a small-scale natural\\ndisaster for which the proposed method is appropriate, and;\\n• We provide an open-source implementation of the discussed methods.\\nThe rest of the paper is organized as follows: Related Work discusses previous work on leveraging\\nsatellite images, GPS traces, social media posts, and the fusion of such datasets in a variety of\\ncontexts. Dataset and Software describes the datasets and the open-source software we employ.\\nThe Methodology section details our methods, which include data preprocessing steps as well as\\nanalysis specific to each data type. The Experiments section outlines our case study on a tornado\\nEVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES3\\nFigure 1. Images before and after the Aru Glacier collapse on 17 July 2016. Images\\nare from Sentinel-2 on 18 June 2016 (a) and 21 July 2016 (b) with 10 m resolution.\\nFrom [Tian et al., 2017]\\nin Muskogee, Oklahoma, while the Use Cases section describes other potential use cases for the\\nproposed method. We conclude with a brief discussion of the implications of our findings.\\nRelated Work\\nHistorically, capturing changes in the built and natural environment often involved on-the-ground\\ndata collection and analysis Haddad [2011]. Surveys and field observations have been the most\\ntraditional method to capture such changes, and continue to be so today in many industries. Prior\\nto the advent of modern satellites, aerial photography taken from aircrafts was another common\\nmethod to document large-scale changes to the built environment Bewley [2003].\\nWhile these\\ntwo methods of data collection are sufficient for many studies, they (a) lack the immediacy called\\nfor in urgent real-time scenarios and (b) tend to miss developments in the absence of the proper\\nequipment or manpower. In the last three decades, new methods of data collection in the built and\\nnatural environment have become prominent, including the use of satellite images, GPS traces from\\nmobile devices, and user-generated content from social media platforms Jongman et al. [2015], Ji\\net al. [2018], Yin et al. [2021].\\nThis literature review aims to provide a comprehensive understanding of current and recent\\nresearch in the fields of mobile data analysis, satellite imagery, and crowdsourcing, highlighting\\nworks that have fused any two of them to assist in the inference task. Table 1 summarizes some of\\nthe papers discussed in the following paragraphs.\\nSatellite Imagery and Remote Sensing. The use of satellite imagery for event detection and\\ninference has been well-documented.\\nSatellite images provide a bird’s-eye view of the Earth’s\\nsurface, enabling the detection of physical changes over time. Though the granularity of satellite\\nimages alone may not provide the nuanced information needed in certain scenarios, they are useful\\nin understanding big-picture changes on the built and natural environment. Particularly in the\\nlast five years, the proliferation of commercial satellite imagery providers has enabled access to this\\n4EVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES\\nTable 1. Studies that have used mobile data/satellite imagery for various purposes.\\nAuthor\\nDate\\nDatasets Used\\nApplications\\nJongman et al.\\n2015\\nSatellite imagery,\\nTwitter posts\\nEarly flood detection in the Philip-\\npines and Pakistan\\nMolinier et al.\\n2016\\nSatellite imagery,\\nmobile app data\\nDeriving biomass maps of forests\\nFayne et al.\\n2017\\nSatellite imagery\\nFlood\\ndetection\\nin\\nthe\\nLower\\nMekong Basin\\nJi et al.\\n2018\\nSatellite imagery\\nCollapsed building detection in 2010\\nHaiti Earthquake\\nSulis et al.\\n2018\\nTransit\\nsmart\\ncard\\ndata, Twitter posts\\nQuantifying diversity and vitality in\\nLondon\\nSaid et al.\\n2019\\nSatellite imagery,\\nsocial media posts\\nNatural disaster detection\\nPokhryiyal et al.\\n2020\\nSatellite imagery,\\nGPS traces\\nEstimation of poverty levels in Haiti\\nYin et al.\\n2021\\nSatellite imagery,\\nGPS traces\\nRoad attribute detection in Singa-\\npore and Jakarta\\nBelcastro et al.\\n2021\\nSocial media posts\\nSmall-scale emergency detection\\ndata type at a short notice and in real-time. This has been used in various applications, such as\\ndisaster management Voigt et al. [2007], urban planning Albert et al. [2017], Karunanithi et al.\\n[2016], and environmental monitoring Xu [2010].\\nRecent advancements in machine learning have further enhanced the capabilities of satellite\\nimagery analysis. For instance, convolutional neural networks have been used to automatically\\ndetect and classify objects in satellite images Zhang et al. [2016]. Meta’s novel object classification\\nmethod, Segment Anything Kirillov et al. [2023], has been proposed for the geospatial domain\\nwith great promise Wu and Osco [2023]. These developments have opened up new possibilities for\\nevent detection and inference, as it allows for the automated analysis of large volumes of satellite\\nimagery and greatly reduces image processing time, allowing for subsequent immediate response\\non the ground.\\nGPS Traces from Mobile Devices. Parallel to the advancements in satellite imagery analysis,\\nthe proliferation of GPS-enabled mobile devices has led to an explosion of mobile data, providing\\ndetailed information about the movements of individuals and groups Zheng et al. [2008]. Research\\nhas delved into the use of mobile data for tracking population movements Bengtsson et al. [2011],\\nunderstanding social behavior Hong et al. [2008], Calabrese et al. [2010], predicting and managing\\ntraffic patterns Herrera et al. [2010], and more.\\nThe richness of this data source has also led to the development of new methods for event\\ndetection and inference. For instance, GPS traces have been used to detect social events Castro\\net al. [2014], Pan et al. [2013], infer the activities of individuals Liao et al. [2007], and understand\\nwhere individuals conduct their daily activities Chen et al. [2014]. However, the fusion of mobile\\ndata with other data sources for real-time event identification and inference in the built and natural\\nenvironment, particularly in the context of emergencies and disasters, is an area that deserves\\nfurther exploration.\\nCrowdsourcing and Social Media Data. Crowdsourcing has emerged as a powerful tool for\\ngathering and analyzing data, particularly in the context of event detection and response. The term\\n”crowdsourcing” refers to the practice of obtaining information or input into a task or project by\\nEVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES5\\nenlisting the services of a large number of people, either paid or unpaid, typically via the internet\\nEstell´es-Arolas and L. Guevara [2012]. In the context of event detection, crowdsourcing can be\\nused to gather real-time information from individuals who are directly experiencing or observing\\nthe event Goodchild [2007]. One of the most notable examples of crowdsourcing in event detection\\nis the Ushahidi platform, which was used to gather and map reports of violence in Kenya after the\\npost-election violence in 2008 Okolloh [2009]. Since then, Ushahidi has been used in various other\\ncontexts, including disaster response and environmental monitoring.\\nCrowdsourcing is closely linked to social media’s rise to prominence as a crucial news outlet\\nduring disasters. A mountain of literature has been published on the promises of social media data\\nin augmenting the inference task in a range of scenarios. Social media websites provide a platform\\nfor individuals to share real-time information on developing events, including their impacts and\\nthe needs of affected communities Palen et al. [2007]. The information circulated on social media\\ntends to dissipate quicker and be more specific than official news outlets, as anyone and everyone\\nhas the ability to share and post Palen [2008]. This information has been used by disaster response\\norganizations to better understand the situation on the ground and coordinate their response efforts.\\nSeveral studies have explored the use of social media data in disaster response. For instance,\\n[Sakaki et al., 2010] developed a system that uses Twitter data to detect earthquakes in real-time.\\nSimilarly, [Ashktorab et al., 2014] developed a system that uses Twitter data to extract actionable\\ninformation during disasters, such as requests for help or reports of infrastructure damage. [Guan\\nand Chen, 2014] developed a metric to quantify the evolution of disasters based on Twitter activities,\\ndemonstrating its use for damage detection during Hurricane Sandy, affecting much of the eastern\\nUnited States and various Caribbean nations. More recently, social media data has been combined\\nwith other data sources for disaster response. For example, [Jongman et al., 2015] proposed a\\nmethod for fusing social media data and satellite imagery to detect floods. This approach leverages\\nthe strengths of both data sources, using social media data to provide real-time information about\\nthe flood and satellite imagery to provide a macro view of the flood’s impacts. We refer the reader\\nto [Said et al., 2019] for a broader review of these approaches.\\nDespite the significant potential of crowdsourcing and social media data for event detection and\\nresponse, there are notable limitations that must be acknowledged. The quality and reliability of\\ncrowdsourced data can vary significantly. Misinformation or inaccurate reports can be dissemi-\\nnated, either unintentionally due to confusion or panic or intentionally as a form of manipulation\\nor cyberattack Allcott et al. [2019]. This can lead to false positives in event detection or mis-\\ndirected response efforts. Additionally, crowdsourcing is inherently dependent on the availability\\nand willingness of individuals to contribute information. In certain scenarios, such as in remote\\nwilderness areas or war-torn regions, the number of individuals able to contribute data may be\\nlimited. Similarly, during small-scale or rapidly unfolding events, there may not be sufficient time\\nfor crowdsourced information to be gathered and analyzed. These limitations highlight the need\\nfor complementary approaches to event detection and response. Our proposed method addresses\\nthese limitations by providing a reliable, real-time source of information that is not dependent on\\nindividual contributions or vulnerable to the same misinformation risks.\\nData Fusion for Event Inference. While there is a substantial body of research on the use of\\nmobile data and satellite imagery independently, there is a noticeable gap in the literature when\\nit comes to their combined use for real-time event identification and response. This gap represents\\na significant opportunity for new research that can contribute to both academic knowledge and\\npractical applications in disaster management, conflict monitoring, as well as search and rescue\\noperations.\\nThe fusion of satellite imagery and GPS traces for event inference is a relatively new area of\\nresearch and is uniquely poised to overcome the limitations placed on only analyzing each data\\nsource individually. Though GPS traces have emerged as a prominent data source, it should be\\n6EVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES\\nnoted that such datasets tend to exclude certain populations, such as the extreme poor Pokhriyal\\net al. [2020].\\nIn developing countries, this population may represent a significant share of the\\npopulation.\\nThus, in the event of an emergency, mobile data cannot be the sole indicator of\\nirregular movement patterns. Satellite imagery itself may also be inadequate in event inference due\\nto the lack of specific mobility information. However, when combined, these two data sources can\\nsupplement each other and leverage the additional information to better inform the inference task.\\nThough previous studies have explored the intersection of these two data sources, the direction\\nof application we intend to use our tool differs from previous works. For instance, [Yin et al., 2021]\\nproposed a method for fusing satellite imagery and GPS traces to infer and fill in road attributes\\nsuch as speed limits and lane widths, with the goal of developing a more comprehensive view of\\nroad networks globally. [Pokhriyal et al., 2020] fused satellite images with mobile data to estimate\\npoverty levels in Haiti, where a nationwide census had not been conducted since 2012. In developing\\ncountries where there may not be the economic resources necessary to complete annual censuses,\\nusing a combination of data sources to estimate various baselines may emerge as a crucial tool\\nfor financial aid allocation and future urban planning. A combination of mobile data and satellite\\nimagery has also been used to study forests and obtain biomass measurements Molinier et al. [2016].\\nThis study combined citizen science-powered mobile data collection and satellite imagery to garner\\nforest information and produce biomass maps in a cost effective manner.\\nNotably, both the work of Yin et al. and Pokhriyal et al. provide information that can be\\nused for future planning. However, we hope to use GPS traces and satellite imagery in real time\\nfor instantaneous data analysis and event identification. Additionally, unlike in Molinier et al.,\\nwhere mobile data is actively collected through an app that must be user-operated, we obtain GPS\\ntraces passively through users’ interactions with various apps (not a specific one). Thus, our event\\ndetection and inference capabilities are completely powered by remote data, and without the need\\nfor active, real-time human participation.\\nDataset and Software\\nFor mobile data, we employ privacy-protected, passively generated GPS data from an American\\ndata solution provider specializing in geospatial analytics.\\nThe dataset contains discrete GPS\\npoints of 2,000 anonymous, opted-in individuals in the Tulsa metropolitan area between December\\n2019 and July 2020. The locational data recorded includes timestamps, unique device identifiers,\\nlatitudes and longitudes, and a measure of data precision (i.e., a spatial radius for which the\\nprovider has 95% confidence in the reported coordinates). In addition to anonymizing the data,\\nthe data provider obfuscates home locations to the census-block group level, and removes sensitive\\npoints of interest from the dataset, in order to preserve privacy. It’s also crucial to note that the\\ntemporal granularity of this data is determined by user activity, meaning data points are generated\\nirregularly and not at fixed intervals. This non-uniformity of temporal data represents the actual,\\ninconsistent frequency of device usage among the sampled population.\\nFor satellite imagery, we pull data from Planet Labs, a private American company that specializes\\nin Earth imaging. Planet operates a large constellation of miniature satellites, known as Doves,\\nwhich continually capture and transmit images of Earth. The 3-band images we use have undergone\\northorectification, a process that corrects the geometric distortions in an image to represent a flat\\nsurface, thereby ensuring that the scale is uniform throughout. This makes the images reliable for\\ndistance measurements and for overlaying with other geospatial data. Additionally, the images have\\nbeen color-corrected to ensure that the colors represented are as close as possible to what the human\\neye would perceive if viewing the scene directly. This enhances the clarity and interpretability of\\nthe images, making them a valuable resource for our research.\\nWe implemented our methodology in the Python programming language van Rossum [1995].\\nWe used pandas Wes McKinney [2010] to read and wrangle mobile data and Rasterio Gillies\\net al. [2013–] to manipulate satellite images. We also leveraged NumPy Harris et al. [2020] for a\\nEVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES7\\nvariety of array operations, GeoPandas Jordahl et al. [2019] for operations involving coordinates,\\nand scikit-mobility Pappalardo et al. [2019] to conduct mobile data cleaning and preprocessing.\\nLast but not least, we used matplotlib Hunter [2007] to produce any visualizations of our results.\\nAny other chunk of code we developed can be found on our GitHub repository: https://github.\\ncom/ekinugurel/SatMobFusion.\\nMethodology\\nOur methodology has two modules—one that deals with the analysis of mobile data, and another\\nwith that of satellite imagery. In both modules, we discuss relevant preprocessing methods as well\\nas analysis steps to aid in the inference task. Figure 2 shows the high-level steps in preprocessing\\nand analysis of both datasets while also outlining key geospatial analysis processes for the three\\nuse cases discussed in the Introduction.\\nFigure 2. Methodological flowchart; the top bins describe the order of processes\\nemployed in this paper, while the bottom outlines key geospatial analysis metrics\\nto carry out the inference task\\nMobile Data. For mobile data, the general preprocessing workflow is as follows:\\n(1) Spatial Partitioning\\n8EVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES\\n(2) Coordinate Reference System (CRS) Projection\\n(3) Data Filtering\\nTo avoid wastefulness in computation, we begin by partitioning the mobile data spatially, which\\nwe do by drawing a bounding box using four sets of coordinates. In practice, there are several ways\\nto achieve this at scale: Leveraging a geocoding application protocol interface (API) to obtain the\\ncoordinates based on an existing point of interest, partitioning by federal and/or state designations\\nsuch as census tracts, ZIP codes, or county borders, manually typing in a bounding box (i.e., by\\ndefining the geographic coordinates of the corners of the box), or obtaining a KMZ file from the\\nNational Oceanic and Atmospheric Administration’s Damage Assessment Toolkit. For this effort,\\nwe used the last option to derive the centroid coordinates of the affected area around which we\\ndrew a 1 km2 bounding square.\\nOnce the data is spatially partitioned, we project coordinates to the appropriate CRS. Different\\nCRSs are designed to accurately represent different aspects of the Earth’s surface.\\nSome are\\ndesigned to preserve area, while others preserve shape, distance, or direction. In the context of event\\ninference, the optimal choice of CRS should minimize bias in the derived mobility metrics, which\\nmay exist due to distortions of distances and areas. Therefore, we use an equal-area projection\\naround UTM Zone 15, which covers the area around our case study.\\nWe then filter out any erroneous data points based on a threshold of segment velocity. Since\\nthis is done ”as the crow flies” and not with respect to the underlying transportation network, this\\nthreshold should be set conservatively. Velocity filtering discards infeasible jumps and oscillations\\nin trajectories, which may occur as a result of tall buildings blocking a satellite’s line of sight to the\\nmobile device (termed ”urban canyon effect”, Ben-Moshe et al. [2011]). These oscillations may also\\noccur due to low data precision as a result of the ”cold-start problem”—when a signal dropped by\\nWiFi is not immediately picked up by a satellite, requiring some time before being able to acquire\\nnavigation data and calculate a position Lehtinen et al. [2008].\\nOnce preprocessing is finished, we conduct inference-related analyses using the following steps:\\n(1) Experimental Design Partitioning\\n(2) Exploratory Data Analysis (EDA)\\n(3) Statistical Testing\\nSpecifically, we first define pre-, during-, and post-event periods that temporally segment the\\ndata. The purpose of this partitioning is to derive key metrics for each period, which would assist in\\ndetecting anomalies and enable more formal statistical tests. The duration of these periods depends\\non the nature of the analysis—for natural disasters like tornadoes or hurricanes, the ”during” period\\nmay be a few hours, while for applications in search & rescue or in conflict areas, the same period\\nmay only last a few minutes.\\nIn conducting EDA on time-series mobile data, the objectives are to understand the structure of\\nthe data (i.e., identifying immediate trends, patterns, or outliers), detect seasonality (i.e., periodic\\nfluctuations), and prepare the data for more formal anomaly detection techniques and statistical\\ntests. The EDA process can also assist in formulating hypotheses about possible causes for anom-\\nalies by exploring relationships between the target variable and other variables. In the context of\\nmobile data within constrained geography, the metrics we analyze include the radius of gyration\\nrg (the characteristic distance traveled by an individual in a given period, Equation 1) Song et al.\\n[2010], the number of extended stays at a particular location c (Equation 3), and the number of\\nvisits per time unit v.\\n(1)\\nrg =\\nv\\nu\\nu\\nt 1\\nn\\nn\\nX\\ni=1\\n(ri − rcm)2\\nEVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES9\\nwhere ri represents the i = 1, . . . , n locations recorded for the individual and rcm = 1\\nn\\nPn\\ni=1 ri is\\nthe center of mass of the period’s trajectory.\\nc =\\nn\\nX\\ni=1\\nci,\\n(2)\\nci =\\n(\\n1\\nif ti = 1, li = 1\\n0\\notherwise\\n(3)\\nwhere ti is an indicator variable denoting whether a device is observed more than once within\\na 15-minute period and li is an indicator variable denoting whether multiple observations of the\\ndevice are within a 100-meter radius (only considered if ti = 1). We note that these thresholds are\\nrobust and suggest adjusting them to suit the underlying domain.\\n(4)\\nv =\\nm\\nX\\nj=1\\nvj\\nwhere j = 1, . . . , m indexes the subsections of the inspected geographical area and vj is an indicator\\nvariable denoting whether the individual was located in that subsection in the given period.\\nFinally, we conduct statistical tests to confirm the presence of anomalies. In this paper, we\\nemploy the Z-score method (Equation 5), in which the principle is to assume the data follows a\\nnormal distribution and then to identify data points that are too far from the mean.\\n(5)\\nZ = x − µ\\nσ\\nwhere x is a data point, µ is the mean of the dataset, and σ is the standard deviation. We consider\\nZ-scores above 3 (or below -3) an anomaly.\\nSatellite Imagery. The general workflow for processing and analyzing satellite images is as fol-\\nlows:\\n(1) Specify tradeoff between region-of-interest (ROI) coverage and time gap to the event of\\ninterest\\n(2) Pull imagery and project to CRS\\n(3) Image Analysis: RGB, greyscale, and NDVI differences\\nProviders like Planet Labs have multiple orbital satellites whose coverage area may intersect—\\nthe goal of each satellite is to pass over a particular geography at least once per day; however,\\nvarious factors can influence the availability of imagery and cause irregular gaps in observation,\\nincluding cloud cover, technical issues with onboard cameras or the image transmission process,\\nand regulatory restrictions imposed by nation-states. For these reasons, we derive a utility metric\\n(Equation 6) to determine the best available before and after image for an event of interest. This\\nmetric weighs the image’s area of coverage against its time gap with the event using a tradeoff\\ncoefficient—the idea is to maximize the spatial overlap between the image and the ROI while\\nminimizing the number of days between the images.\\n(6)\\nu = Aimg\\nAevt\\n− |timg − tevt|\\nϕ\\nwhere the first term denotes the percent of ROI covered by the image (i.e., Aimg is the area of the\\nimage and Aevt is the area affected by the event), the numerator of the second term denotes the\\nnumber of days between the images, and ϕ is the coverage tradeoff coefficient. In this work, we use\\nϕ = 0.25, meaning that 25% (percentage points) more coverage of the AOI is defined to be worth\\nas much as the image being taken 1 day before or after the event.\\n10\\nEVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES\\nAs an additional filter to the above utility metric, we require all images to have less than 50%\\ncloud coverage. Once we have the satellite images, we project them to the same CRS we used for\\nthe mobile data to avoid misalignment.\\nWe analyze the images to identify any physical changes to the built and natural environment.\\nThis analysis involves looking at the regular visible spectrum (RGB), a grayscale version (i.e.,\\nluminosity, Equation 7), and the Normalized Difference Vegetation Index (NDVI, Equation 8),\\nwhich measures the greenness and the density of the vegetation captured in a satellite image.\\nNDV I = NIR − R\\nNIR + R\\n(7)\\nGS = 0.299R + 0.587G + 0.114B\\n(8)\\nwhere NIR, R, G, and B are near-infrared, red, green, and blue reflectances, respectively. NDVI\\nis always between -1 and 1, where negative values signify the presence of clouds, water, or snow,\\nvalues close to zero signify earthy rocks and bare soil, and positive values signify the existence of\\nsome vegetation (with more positive meaning more green).\\nFigure 3 shows a decision tree outlining when the proposed method should be employed based\\non a series of conditions, organized by the cognitive load required to succeed in the inference task.\\nIntuitively, crowdsourcing may be the preferred option for most organizations and individuals but\\ncan be unreliable due to the threat of misinformation or cybersecurity breaches. Similarly, while\\ncameras and other in-situ sensors could provide longer visual feeds that are also easier to interpret,\\ntheir availability in certain settings tends to be sparse.\\nIf the above two conditions are true,\\nand cloud-free satellite imagery is available, then employing our method may be of interest to\\norganizations and individuals alike.\\nFigure 3. Usage decision tree for the proposed method\\nRelating to the above figure, we emphasize three points about the proposed methodology:\\nEVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES\\n11\\n(a) Radius of Gyration\\n(b) Number of Stays per day\\nFigure 4. EDA of mobility metrics in the before, during, and after periods\\n(1) The process of event detection starts with mobile data, not satellite imagery, due to the\\nfeasibility of passively monitoring geography based on mobility metrics. However, the fusion\\nof the two sources is what enables us to make an inference.\\n(2) The proposed methodology is designed to be flexible, allowing the user to define an event\\nof interest using a configuration file.\\n(3) This approach can be used with both real-time and historical data, depending on the context\\nof the application, making it a potentially powerful tool for improving emergency response\\ntimes and information percolation speeds.\\nCase Study\\nTo validate our proposed methodology, we conducted a case study on the EF-1 tornado that\\nhit near Muskogee, Oklahoma on May 15, 2020. This event provided an opportunity to test our\\napproach in a real-world scenario involving a small-scale disaster in a rural region.\\nAfter spatially partitioning the GPS traces to include the subset within the bounding box of\\nthe tornado, we projected all data to UTM Zone 15 and filtered the data by the logic described\\nin Section . Then, we defined the two weeks before the day of the event (May 2 - May 15) as the\\nbefore, the day of the event (May 15) as the during, and the week after the event (May 15 - May\\n22) as the after periods and began exploring the data. Figure 4 visualizes the time series of two\\nmetrics, highlighting the periods by color; there are no noticeable changes in the radius of gyration,\\nwhile the number of extended stays at a given location within the ROI increases sharply the day\\nafter the event. However, Z-score tests on both metrics do not detect any anomalies.\\nHowever, when we look at the third metric (visits per hour to the ROI, Figure 5), we see an\\nunmistakable outlier; on the day of the tornado, visits to the area increased sharply at 2 pm and\\nagain at 6 pm, substantially beyond the usual fluctuations captured by the 95% confidence interval\\n(light blue shade) in the before and after periods. Furthermore, Z-score tests on this metric confirm\\nthe hypothesis: Two anomalies exist in the data and they are at 2 pm (Z = 3.47) and at 6 pm\\n(Z = 4.87) on May 15th.\\nFollowing the anomaly detection, we pull in satellite imagery as described in the previous section.\\nWe use the utility metric in Equation 6 to land on two images, one from May 9 and another from\\nMay 18. These two images are free of clouds and strike a balance between the percent of ROI\\ncovered and the number of days separating them from the event.\\nWe analyze the images based on visual inspection, the greyscale difference, and the NDVI dif-\\nference (Figure 6). We highlight two spots on the regular color band images in which changes\\nto the soil and a corner building are apparent to the naked eye. Furthermore, while noisy, the\\n12\\nEVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES\\nFigure 5. Visits per hour to the ROI in the before, during, and after periods sorted\\nby hour of day\\ngreyscale difference image shows changes to luminosity and the NDVI difference image shows loss\\nof vegetation throughout the captured area (note that 0.0 is shown in light green).\\nThe images show clear evidence of physical changes to the ROI. When combined with insights\\nfrom GPS traces, and even without hindsight bias, we would infer that a mildly-destructive natural\\ndisaster occurred, piquing passerby’s interest as well as landowners’ concern on the day of the event\\n(resulting in greater visits to the ROI).\\nUse Cases\\nThe proposed methodology is expected to outperform existing methods for data collection and\\ninformation percolation for a range of use cases. We especially anticipate broad applicability in the\\nfollowing settings:\\n• Intelligent transportation systems: The likelihood of finding parking approximate to\\na desired location and in a timely manner has been shown to be a determinant of travelers’\\nmode choice Assemi et al. [2020]. Therefore, providing accurate parking-related informa-\\ntion can augment an individual’s decision-making process. Public and private entities can\\nleverage the proposed method to provide real-time information on parking availability. The\\ntemporal sparsity of satellite imagery can be augmented with GPS traces to provide an\\nonline learning framework for the likelihood of finding parking.\\n• Events in rural areas: In rural areas where traditional data collection methods may\\nbe sparse or non-existent, our methodology could provide valuable insights into events as\\nthey unfold. The combination of satellite imagery and mobile data could help identify and\\nrespond to emergencies such as wildfires, floods, or lost hikers.\\n• Extreme-weather events: During extreme-weather events like hurricanes, tornadoes, or\\nblizzards, our methodology could provide real-time information about the event’s impact\\nEVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES\\n13\\n(a) Before the Muskogee tornado on May 9, 2020\\n(b) After the Muskogee tornado on May 18, 2020\\n(c) Greyscale difference between the two images\\n(d) NDVI difference between the two images\\nFigure 6. Satellite imagery with 10m resolution documenting changes to the built\\nand natural environment after the Muskogee tornado\\nand aid in coordinating response efforts. The ability to quickly identify areas of high impact\\ncould help prioritize resource allocation and rescue operations.\\n• War-torn states: In regions affected by conflict, obtaining reliable on-the-ground infor-\\nmation can be challenging. Satellite imagery combined with mobile data could help identify\\nareas of active conflict, population displacement, or infrastructure damage.\\n• Areas without network connectivity: In areas where network connectivity is limited or\\nnon-existent, such as remote wilderness areas or during large-scale network outages, satellite\\nimagery can still provide valuable data. When combined with historical or intermittent\\nmobile data, this could help identify and respond to events such as lost hikers or the spread\\nof wildfires.\\n• National security matters: Our methodology could also be used in the context of na-\\ntional security, helping to identify potential threats or unusual activities. The fusion of\\nsatellite imagery with mobile data could provide a more comprehensive picture of activities\\nacross a wide geographical area.\\nIn each of these use cases, our methodology aims to provide a more nuanced and timely under-\\nstanding of events as they unfold, potentially improving response times and outcomes.\\nConclusion\\nWe have presented a novel data fusion methodology combining satellite imagery with GPS traces\\ngenerated from mobile devices. Our approach leverages the strengths of both data types: mobile\\n14\\nEVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES\\ndata provides an approximation of human mobility, proximity to one another, and the built envi-\\nronment, while satellite imagery offers visual insights into physical changes to the built and natural\\nenvironment. Furthermore, we have demonstrated the case study of a small-scale tornado, show-\\ning the promise of our method in identifying anomalies in movement patterns using mobile data,\\nchanges to the built environment using satellite imagery, and inferring the cause of both using our\\nfusion methodology.\\nOur methodology is viable due to the increasing accessibility of commercial data providers spe-\\ncializing in remote sensing and mobile devices. This trend is only expected to continue—the number\\nof active orbital satellites has grown exponentially since 2017 orb [2022], while the percentage of\\nAmericans that own a smartphone has risen to 97% mob [2021]. The continued commercialization\\nof such products will enable researchers and practitioners to monitor, categorize, and understand\\nthe natural and built environments in ways unimaginable just 50 years ago. We hope our method\\nis a step in this direction, enabling others to pursue endeavors made possible by the fusion of these\\ndata sources.\\nData Access Statement\\nThe authors note that only Ekin Ugurel had access to the privacy-protected GPS dataset. Data\\nsupporting this study cannot be made available due to privacy-preserving research agreements with\\nthe data provider.\\nAuthor Contributions\\nThe authors confirm contribution to the paper as follows: study conception and design: E.\\nUgurel, S. Coenen; data collection: E. Ugurel, S. Coenen; analysis and interpretation of results: E.\\nUgurel, S. Coenen; draft manuscript preparation: E. Ugurel, M. Chen, S. Coenen, C. Chen. All\\nauthors reviewed the results and approved the final version of the manuscript.\\nEVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES\\n15\\nReferences\\nMobile fact sheet. Technical report, Pew Reseearch Center, 2021. URL https://www.pewresearch.\\norg/internet/fact-sheet/mobile/.\\nOnline index of objects launched into outer space. Technical report, United Nations - Office of\\nOuter Space Affairs, 2022.\\nA. Albert, J. Kaur, and M. Gonzalez. Using convolutional networks and satellite imagery to identify\\npatterns in urban environments at a large scale. pages 1357–1366, 08 2017. doi: 10.1145/3097983.\\n3098070.\\nL. Alessandretti. What human mobility data tell us about COVID-19 spread. Nature Reviews\\nPhysics, 4(1):12–13, Jan. 2022. ISSN 2522-5820. doi: 10.1038/s42254-021-00407-1. URL https:\\n//www.nature.com/articles/s42254-021-00407-1. Number: 1 Publisher: Nature Publishing\\nGroup.\\nH. Allcott, M. Gentzkow, and C. Yu. Trends in the diffusion of misinformation on social media.\\nResearch & Politics, 6(2):2053168019848554, 2019.\\nZ. Ashktorab, C. Brown, M. Nandi, and A. Culotta. Tweedr: Mining twitter to inform disaster\\nresponse. In International Conference on Information Systems for Crisis Response and Manage-\\nment, 2014.\\nB. Assemi, D. Baker, and A. Paz. Searching for on-street parking: An empirical investigation of\\nthe factors influencing cruise time. Transport Policy, 97:186–196, 2020.\\nB. Ben-Moshe, E. Elkin, H. Levi, and A. Weissman. Improving Accuracy of GNSS Devices in\\nUrban Canyons. page 6, 2011.\\nL. Bengtsson, X. Lu, A. Thorson, R. Garfieldarfier, and J. Schreeb. Improved response to disasters\\nand outbreaks by tracking population movements with mobile phone network data: A post-\\nearthquake geospatial study in haiti. PLoS medicine, 8:e1001083, 08 2011. doi: 10.1371/journal.\\npmed.1001083.\\nR. Bewley. Aerial survey for archaeology. The Photogrammetric Record, 18:273 – 292, 12 2003. doi:\\n10.1046/j.0031-868X.2003.00023.x.\\nF. Calabrese, M. Colonna, P. Lovisolo, D. Parata, and C. Ratti. Real-time urban monitoring using\\ncell phones: A case study in rome. IEEE transactions on intelligent transportation systems, 12\\n(1):141–151, 2010.\\nP. Castro, D. Zhang, C. Chen, S. Li, and G. Pan. From taxi gps traces to social and community\\ndynamics: A survey. ACM Computing Surveys, 46, 01 2014. doi: 10.1145/2543581.2543584.\\nC. Chen, L. Bian, and J. Ma. From traces to trajectories: How well can we guess activity locations\\nfrom mobile phone traces? Transportation Research Part C: Emerging Technologies, 46:326–337,\\n2014.\\nE. Estell´es-Arolas and F. G. L. Guevara. Towards an integrated crowdsourcing definition. Journal\\nof Information Science, 38, 04 2012. doi: 10.1177/0165551512437638.\\nS. Gillies et al.\\nRasterio: geospatial raster i/o for Python programmers, 2013–.\\nURL https:\\n//github.com/rasterio/rasterio.\\nM. F. Goodchild. Citizens as sensors: the world of volunteered geography. GeoJournal, 69:211–221,\\n2007.\\nX. Guan and C. Chen. Using social media data to understand and assess disasters. Natural hazards,\\n74:837–850, 2014.\\nN. Haddad. From ground surveying to 3d laser scanner: A review of techniques used for spatial\\ndocumentation of historic sites. Journal of King Saud University - Engineering Sciences, 23:\\n109–118, 06 2011. doi: 10.1016/j.jksues.2011.03.001.\\nC. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau,\\nE. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk,\\nM. Brett, A. Haldane, J. F. del R´ıo, M. Wiebe, P. Peterson, P. G´erard-Marchant, K. Sheppard,\\n16\\nEVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES\\nT. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with\\nNumPy. Nature, 585(7825):357–362, Sept. 2020. doi: 10.1038/s41586-020-2649-2. URL https:\\n//doi.org/10.1038/s41586-020-2649-2.\\nJ. C. Herrera, D. B. Work, R. Herring, X. J. Ban, Q. Jacobson, and A. M. Bayen. Evaluation\\nof traffic data obtained via gps-enabled mobile phones: The mobile century field experiment.\\nTransportation Research Part C: Emerging Technologies, 18(4):568–583, 2010.\\nS.-J. Hong, J. Thong, J. Y. Moon, and K. Tam. Understanding the behavior of mobile data services\\nconsumers. Information Systems Frontiers, 10:431–445, 09 2008. doi: 10.1007/s10796-008-9096-1.\\nJ. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering, 9(3):\\n90–95, 2007. doi: 10.1109/MCSE.2007.55.\\nM. Ji, L. Liu, and M. Buchroithner. Identifying collapsed buildings using post-earthquake satellite\\nimagery and convolutional neural networks: A case study of the 2010 haiti earthquake. Remote\\nSensing, 10(11):1689, 2018.\\nB. Jongman, J. Wagemaker, B. Revilla Romero, and E. Coughlan de Perez. Early flood detection\\nfor rapid humanitarian response: harnessing near real-time satellite and twitter signals. ISPRS\\nInternational Journal of Geo-Information, 4(4):2246–2266, 2015.\\nK. Jordahl, J. V. den Bossche, J. Wasserman, J. McBride, J. Gerard, M. Fleischmann, J. Tratner,\\nM. Perry, C. Farmer, G. A. Hjelle, S. Gillies, M. Cochran, M. Bartos, L. Culbertson, N. Eu-\\nbank, maxalbert, S. Rey, A. Bilogur, D. Arribas-Bel, C. Ren, J. Wilson, M. Journois, L. J.\\nWolf, L. Wasser, ¨Omer ¨Ozak, YuichiNotoya, F. Leblanc, Filipe, C. Holdgraf, and A. Greenhall.\\ngeopandas/geopandas: v0.6.1, Oct. 2019. URL https://doi.org/10.5281/zenodo.3483425.\\nM. Karunanithi, S. Kumar, and P. Ponnusamy. Use of high resolution google earth satellite imagery\\nin landuse map preparation for urban related applications. Procedia Technology, 24:1835–1842,\\n12 2016. doi: 10.1016/j.protcy.2016.05.231.\\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C.\\nBerg, W.-Y. Lo, P. Doll´ar, and R. Girshick. Segment anything. arXiv:2304.02643, 2023.\\nM. Lehtinen, A. Happonen, and J. Ikonen. Accuracy and time to first fix using consumer-grade gps\\nreceivers. In 2008 16th International Conference on Software, Telecommunications and Computer\\nNetworks, pages 334–340, 2008. doi: 10.1109/SOFTCOM.2008.4669506.\\nL. Liao, D. Fox, and H. Kautz. Extracting places and activities from gps traces using hierarchical\\nconditional random fields. The International Journal of Robotics Research, 26(1):119–134, 2007.\\nM. Molinier, C. L´opez-S´anchez, T. Toivanen, I. Korpela, J. J. Corral-Rivas, R. Tergujeff, and\\nT. H¨ame. Relasphone—mobile and participative in situ forest biomass measurements supporting\\nsatellite image mapping. Remote Sensing, 8:869–891, 10 2016. doi: 10.3390/rs8100869.\\nO. Okolloh. Ushahidi, or ‘testimony’: Web 2.0 tools for crowdsourcing crisis information. Partici-\\npatory learning and action, 59(1):65–70, 2009.\\nL. Palen. Online social media in crisis events. Educause Quarterly, 31:76–78, 01 2008.\\nL. Palen, S. Vieweg, J. Sutton, S. B. Liu, and A. Hughes. Crisis informatics: Studying crisis in\\na networked world. In Proceedings of the Third International Conference on E-Social Science,\\npages 7–9, 2007.\\nG. Pan, G. Qi, Z. Wu, D. Zhang, and S. Li. Land-use classification using taxi gps traces. Intelligent\\nTransportation Systems, IEEE Transactions on, 14:113–123, 03 2013. doi: 10.1109/TITS.2012.\\n2209201.\\nL. Pappalardo, F. Simini, G. Barlacchi, and R. Pellungrini. scikit-mobility: A python library for\\nthe analysis, generation and risk assessment of mobility data. arXiv preprint arXiv:1907.07062,\\n2019.\\nN. Pokhriyal, O. Zambrano, J. Linares, and H. Hern´andez. Estimating and Forecasting Income\\nPoverty and Inequality in Haiti Using Satellite Imagery and Mobile Phone Data. 06 2020. doi:\\n10.18235/0002466.\\nEVENT DETECTION FROM NOVEL DATA SOURCES: LEVERAGING SATELLITE IMAGERY ALONGSIDE GPS TRACES\\n17\\nN. Said, K. Ahmad, M. Riegler, K. Pogorelov, L. Hassan, N. Ahmad, and N. Conci. Natural disasters\\ndetection in social media and satellite imagery: a survey. Multimedia Tools and Applications, 78:\\n31267–31302, 2019.\\nT. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake shakes twitter users: real-time event detection\\nby social sensors. In Proceedings of the 19th international conference on World wide web, pages\\n851–860, 2010.\\nC. Song, Z. Qu, N. Blumm, and A.-L. Barab´asi. Limits of predictability in human mobility. Science,\\n327(5968):1018–1021, 2010.\\nB. Stelter and N. Cohen. Citizen journalists provided glimpses of mumbai attacks. The New York\\nTimes, 2008. URL https://www.nytimes.com/2008/11/30/world/asia/30twitter.html#:~:\\ntext=From%20his%20terrace%20on%20Colaba,every%20move%20on%20the%20Internet.\\nP. Sulis, E. Manley, C. Zhong, and M. Batty. Using mobility data as proxy for measuring urban\\nvitality. Journal of Spatial Information Science, 2018(16):137–162, 2018.\\nL. Tian, T. Yao, Y. Gao, L. Thompson, E. Mosley-Thompson, S. Muhammad, J. Zong, C. Wang,\\nS. Jin, Z. Li, and et al. Two glaciers collapse in western tibet. Journal of Glaciology, 63(237):\\n194–197, 2017. doi: 10.1017/jog.2016.122.\\nG. van Rossum. Python tutorial. Technical Report CS-R9526, Centrum voor Wiskunde en Infor-\\nmatica (CWI), Amsterdam, May 1995.\\nS. Voigt, T. Kemper, T. Riedlinger, R. Kiefl, K. Scholte, and H. Mehl. Satellite image analysis for\\ndisaster and crisis-management support. IEEE transactions on geoscience and remote sensing,\\n45(6):1520–1528, 2007.\\nWes McKinney. Data Structures for Statistical Computing in Python. In St´efan van der Walt and\\nJarrod Millman, editors, Proceedings of the 9th Python in Science Conference, pages 56 – 61,\\n2010. doi: 10.25080/Majora-92bf1922-00a.\\nQ. Wu and L. P. Osco. samgeo: A Python package for segmenting geospatial data with the Segment\\nAnything Model (SAM), May 2023. URL https://doi.org/10.5281/zenodo.7966658.\\nH. Xu. Analysis of impervious surface and its impact on urban heat environment using the nor-\\nmalized difference impervious surface index (ndisi). Photogrammetric Engineering & Remote\\nSensing, 76(5):557–565, 2010.\\nY. Yin, A. Tran, Y. Zhang, W. Hu, G. Wang, J. Varadarajan, R. Zimmermann, and S.-K. Ng.\\nMultimodal fusion of satellite images and crowdsourced gps traces for robust road attribute\\ndetection. In Proceedings of the 29th International Conference on Advances in Geographic Infor-\\nmation Systems, pages 107–116, 2021.\\nL. Zhang, L. Zhang, and B. Du. Deep learning for remote sensing data: A technical tutorial on the\\nstate of the art. IEEE Geoscience and remote sensing magazine, 4(2):22–40, 2016.\\nY. Zheng, Q. Li, Y. Chen, X. Xie, and W.-Y. Ma. Understanding mobility based on gps data. In\\nProceedings of the 10th international conference on Ubiquitous computing, pages 312–321, 2008.\\n'},\n",
       " {'abstract': \"This paper presents a novel diffusion-based framework for animating people in a scene driven by target 3D motion sequences, enabling the generation of realistic and diverse human movements. The approach consists of two core components: a) learning priors about invisible parts of the human body and clothing to generate plausible complete texture maps from single images using in-painting diffusion models, and b) developing a diffusion-based rendering pipeline, controlled by 3D poses, for the synthesis of realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. Experiments show the model's ability to synthesize prolonged motions and handle challenging and complex poses, outperforming previous methods.\",\n",
       "  'introduction': \"Given a random photo of a person, can we accurately animate that person to imitate someone else's action? This problem requires a deep understanding of how human poses change over time, learning priors about human appearance and clothing. To tackle this, we propose 3DHM, a two-stage framework that synthesizes 3D Human Motions by completing a texture map from a single image and then rendering the 3D humans to imitate the actions of the actor, resulting in synthesized videos that are faithful to the target motion in 3D pose and to the input image in terms of visual similarity. In addition, the 3D control enables the generation of diverse synthetic camera trajectories to render a person.\",\n",
       "  'literature_review': 'Prior work on controllable human generation and synthesizing moving people has faced challenges in generating realistic human images or videos, often leading to incomplete or non-consequential outputs, and failing to faithfully reconstruct humans. Existing approaches either require large amounts of data, supervised control signals, or careful curation of training data, making it difficult to utilize them directly for animating humans. Some methods attempt to learn pose-to-pixels mapping directly, but this methodology often leads to models becoming overly specialized to certain training data, limiting their generalization capabilities.',\n",
       "  'methodology': 'In the first stage of 3DHM, an inpainting diffusion model is utilized to produce a plausible complete texture map by inpainting the unseen regions of the imitator. This model takes a partial texture map and corresponding visibility mask as inputs, and generates a recovered predicted map for the human. In the second stage, a rendering diffusion model is developed to obtain a realistic rendering of a human imitator doing the actions of the actor. This model is applied to intermediate renderings of SMPL meshes, generated by wrapping the predicted texture map from the first stage, to project the body-tight renderings to more realistic images with clothing. The model is conditioned on 3D poses to animate the imitator to copy the actions of the actor.',\n",
       "  'results': \"In our experiments, 3DHM outperforms previous approaches on several metrics, including image-based evaluation and video-based evaluation. It achieves superior performance in generating realistic frames, preserving temporal consistency, and generating accurate 3D poses. Qualitative results demonstrate the model's ability to synthesize moving people in various scenarios, including unseen 3D human videos, motions from random YouTube videos, and motions from text inputs, with diverse viewpoints and challenging poses.\",\n",
       "  'conclusion': 'We present 3DHM, a two-stage diffusion model-based framework for synthesizing moving people from a single image given a target 3D motion sequence. Our approach is resilient in generating prolonged motions and varied challenging and complex poses. It is fully self-supervised, requiring no additional annotations beyond human videos, and is scalable with the availability of more human videos. We believe that 3DHM opens up new possibilities for human animation and provides a solid foundation for future research in this area.',\n",
       "  'title': 'Synthesizing Moving People with 3D Control',\n",
       "  'author': 'Boyi Li, Jathushan Rajasegaran, Yossi Gandelsman, Alexei A. Efros, Jitendra Malik',\n",
       "  'textdata': 'Synthesizing Moving People with 3D Control\\nBoyi Li\\nJathushan Rajasegaran\\nYossi Gandelsman\\nAlexei A. Efros\\nJitendra Malik\\nUC Berkeley\\nImitator\\nActor\\nImitator\\nActor\\nActor\\nFigure 1. The Imitation Game: Given a video of a person \"The Actor\", we want to transfer their motion to a new person \"The Imitator\".\\nIn this figure, the first row shows a sequence of frames of the actor (Michelle Kwan), doing her Olympics ’98 performance. The inset row\\nshows the 3D poses extracted from this video. Now, given any single image of a new person The Imitator, our model can synthesize new\\nrenderings of the imitator, to copy the actions of the actor in 3D.\\nAbstract\\nIn this paper, we present a diffusion model-based frame-\\nwork for animating people from a single image for a given\\ntarget 3D motion sequence. Our approach has two core\\ncomponents: a) learning priors about invisible parts of the\\nhuman body and clothing, and b) rendering novel body poses\\nwith proper clothing and texture. For the first part, we learn\\nan in-filling diffusion model to hallucinate unseen parts of a\\nperson given a single image. We train this model on texture\\nmap space, which makes it more sample-efficient since it\\nis invariant to pose and viewpoint. Second, we develop a\\ndiffusion-based rendering pipeline, which is controlled by\\n3D human poses. This produces realistic renderings of novel\\nposes of the person, including clothing, hair, and plausible in-\\nfilling of unseen regions. This disentangled approach allows\\nour method to generate a sequence of images that are faithful\\nto the target motion in the 3D pose and, to the input image\\nin terms of visual similarity. In addition to that, the 3D con-\\ntrol allows various synthetic camera trajectories to render a\\nperson. Our experiments show that our method is resilient in\\ngenerating prolonged motions and varied challenging and\\ncomplex poses compared to prior methods. Please check our\\nwebsite for more details: 3DHM.github.io.\\n1. Introduction\\nGiven a random photo of a person, can we accurately an-\\nimate that person to imitate someone else’s action? This\\narXiv:2401.10889v1  [cs.CV]  19 Jan 2024\\nproblem requires a deep understanding of how human poses\\nchange over time, learning priors about human appearance\\nand clothing. For example, in Figure 1 the Actor can do a\\ndiverse set of actions, from simple actions such as walking\\nand running to more complex actions such as fighting and\\ndancing. For the Imitator, learning a visual prior about\\ntheir appearance and clothing is essential to animate them\\nat different poses and viewpoints. To tackle this problem,\\nwe propose 3DHM, a two-stage framework (see Figure 2)\\nthat synthesizes 3D Human Motions by completing a texture\\nmap from a single image and then rendering the 3D humans\\nto imitate the actions of the actor.\\nWe use state-of-the-art 3D human pose recovery model\\n4DHumans [9, 19] for extracting motion signals of the actor,\\nby reconstructing and tracking them over time. Once we\\nhave a motion signal in 3D, as a sequence of meshes, one\\nwould think we can simply re-texture them with the texture\\nmap of the imitator to get an intermediate rendering of the\\nimitation task. However, this requires a complete texture\\nmap of the imitator. When given only a single view image\\nof the imitator, we see only a part of their body, perhaps\\nthe front side, or the backside but never both sides. To get\\nthe complete texture map of the imitator from a single view\\nimage, we learn a diffusion model to in-fill the unseen re-\\ngions of the texture map. This essentially learns a prior about\\nhuman clothing and appearance. For example, a front-view\\nimage of a person wearing a blue shirt would usually have\\nthe same color at the back. With this complete texture map,\\nnow we can get an intermediate rendering of the imitator\\ndoing the actions of the actor. Intermediate rendering means,\\nwrapping the texture map on top of the SMPL [17] mesh to\\nget a body-tight rendering of the imitator.\\nHowever, the SMPL [17] mesh renderings are body-tight\\nand do not capture deformations on clothing, like skirts or\\nvarious hairstyles. To solve this, we learn a second model,\\nthat maps from mesh renderings to more realistic images,\\nby controlling the motion with 3D poses. We find out such\\na simple framework could successfully synthesize realis-\\ntic and faithful human videos, particularly for long video\\ngenerations. We show that the 3D control provides a more\\nfine-grained and accurate flow of motion and captures the\\nvisual similarities of the imitator faithfully.\\nWhile there has been a lot of work on rewriting the motion\\nof an actor [4, 14, 28], each requires either large amounts\\nof data, supervised control signals, or requires careful cura-\\ntions of the training data. For example, Make-a-video [24]\\ncan generate decent results while for human videos, it often\\ngenerates incomplete or nonconsequential videos and fails\\nat faithful reconstruction of humans. Some works [8] use\\nOpenpose [6] as intermediate supervision. However, Open-\\npose primarily contains the anatomical key points of humans,\\nit can not be used to indicate the body shape, depth, or other\\nrelated human body information. DensePose [10] aims to\\nrecover highly accurate dense correspondences between im-\\nages and the body surface to provide dense human pose\\nestimation. However, it can not reflect the texture informa-\\ntion from the original inputs. Compared to this line of work,\\nours fully utilizes the 3D models to control the motion, by\\nproviding an accurate dense 3D flow of the motion, and the\\ntexture map representation makes it easy to learn appearance\\nprior from a few thousand samples.\\n2. Related Works\\nControllable Human Generation. Human generation is\\nnot an easy task. Unlike image translation [16], generating\\ndifferent humans requires the model to understand the 3D\\nstructure of the human body. Given arbitrary text prompts or\\npose conditions [5, 15], we often find out that existing gener-\\native models often generate unreasonable human images or\\nvideos. Diffusion-HPC [31] proposes a diffusion model with\\nHuman Pose Correction and finds that injecting human body\\nstructure priors within the generation process could improve\\nthe quality of generated images. ControlNet [34] is designed\\non neural network architecture to control pre-trained large\\ndiffusion models to support additional input conditions, such\\nas Openpose [6]. GestureDiffuCLIP [3] designs a neural\\nnetwork to generate co-speech gestures. However, these\\ntechniques are not tailored for animating humans, which can-\\nnot guarantee the required human appearance and clothing.\\nSynthesizing Moving People. Synthesizing moving peo-\\nple is very challenging. For example, Make-a-Video [24]\\nor Imagen Video [23] could synthesize videos based on a\\ngiven instruction. However, the generated video cannot ac-\\ncurately capture human properties correctly and may cause\\nthe weird composition of generated humans. Prior meth-\\nods [8, 29] learn pose-to-pixels mapping directly. However,\\nthese designs could only be trained and used for one per-\\nson. Recent works such as SMPLitex [7] consider human\\ntexture estimation from a single image to animate a person.\\nHowever, there is a visual gap between rendered people via\\npredicted texture map and real humans. Many works start\\nto directly predict pixels based on diffusion models, such as\\nDreampose [14] and DisCO [28]. DreamPose is controlled\\nby DensePose [10], it aims to synthesize a video contain-\\ning both human and fabric motion based on a sequence of\\nhuman body poses. DisCO is directly controlled by Open-\\npose [6], and it aims to animate the human based on the 2D\\npose information. However, the approach of aligning output\\npixels for training regularization often leads these models to\\nbecome overly specialized to certain training data. Moreover,\\nthis methodology limits the models’ generalization capabili-\\nties, as they often perform well on a few people whose data\\ndistribution closely matches that of the training dataset.\\nStage 1\\n(Inpainting)\\nStage 2\\n(Rendering)\\nPredicted\\nComplete  \\nTexture map\\n3D Poses\\n(from Actor)\\nTexture mapped\\n(Imitator)\\nFinal Rendering\\n(Imitator)\\nA photo of \\na person\\n(Imitator)\\n28-7， 15， 22\\nFigure 2. Overview of 3DHM: we show an overview of our model pipeline. Given an image of the imitator and a sequence of 3D poses\\nfrom the actor, we first generate a complete full texture map of the imitator, which can be applied to the 3D pose sequences extracted from\\nthe actor to generate texture-mapped intermediate renderings of the imitator. Then we pass these intermediate renderings to the Stage-2\\nmodel to project the SMPL mesh rendering to more realistic renderings of real images. Note: red boxes represent inputs, yellow boxes\\nrepresent intermediate predictions from stage 1, and blue boxes represent the final outputs from stage 2. To create a moving person animation\\nwith variable duration and any number of 3D poses, it is only necessary to execute stage 1 once in order to acquire a complete texture map.\\n3. Synthesizing Moving People\\nIn this section, we discuss our two-stage approach for imitat-\\ning a motion sequence. Our 3DHM framework embraces the\\nadvantage of accurate 3D pose prediction from the state-of-\\nthe-art predicting models 4DHumans [9, 19], which could\\naccurately track human motions and extracts 3D human\\nposes of the actor videos. For any given video of the actor\\nwe want to imitate, we use 3D reconstruction-based tracking\\nalgorithms to extract 3D mesh sequences of the actor. For\\nthe inpainting and rendering part, we rely on the pre-trained\\nStable Diffusion [22] model, which is one of the most recent\\nclasses of diffusion models that achieve high competitive\\nresults over various generative vision tasks.\\nOur approach 3DHM is composed of two core parts: In-\\npainting Diffusion for texture map in-painting as Stage-1\\nand Rendering Diffusion for human rendering as Stage-2.\\nFigure 2 shows a high-level overview of our framework. In\\nStage-1, first, for a given single view image, we extract a\\nrough estimate of the texture map by rendering the meshes\\nonto the image and assigning pixels to each visible mesh tri-\\nangle such that when rendered again it will produce a similar\\nimage as the input image. This predicted texture map has\\nonly visible parts of the input image. The Stage-1 Diffusion\\nin-painting model takes this partial texture map and gener-\\nates a complete texture map including the unseen regions.\\nGiven this completed texture map, we generate intermediate\\nrenderings of SMPL [17] meshes and use Stage-2 model to\\nproject the body-tight renderings to more realistic images\\nwith clothing. For the Stage-2 Diffusion model, we apply\\n3D control to animate the imitator to copy the actions of the\\nactor.\\n3.1. Texture map Inpainting\\nThe goal of Stage-1 model is to produce a plausible complete\\ntexture map by inpainting the unseen regions of the imitator.\\nWe extract a partially visible texture map by first rendering a\\n3D mesh onto the input image and sample colors for each\\nvisible triangle following 4DHumans [9].\\nInput. We first utilize a common approach to infer pixel-\\nto-surface correspondences to build an incomplete UV tex-\\nturemap [7, 32] for texturing 3D meshes from a single RGB\\nimage. We also compute a visibility mask to indicate which\\npixels are visible in 3D and which ones are not.\\nTarget. Since the objective of this modeling is to generate\\ncomplete texture maps, we generate a pseudo-complete tex-\\nture map using video data. Since the 4DHumans can track\\npeople over time, it continually updates its internal texture\\nmap representations as a moving average of visible regions.\\nHowever to produce more sharp images, for the generative\\ntask we found that a median filtering is more suitable than\\na moving average. While this technique can be applied to\\nany video, in this stage we rely on 2,205 human videos. For\\neach human video, we first extract a partial texture map from\\neach frame. Since each video contains 360 degrees of human\\nviews, we calculate a pseudo-complete texture map from a\\nwhole video and set it as the target output for Stage 1. In\\ndetail, we take the median overall visible parts of texture\\nmaps of a video.\\nModel. We finetune directly on the Stable Diffusion In-\\npainting model [21] that shows great performance on image\\ncompletion tasks. We input a partial texture map and corre-\\nsponding visibility mask and obtain the recovered predicted\\nmap for the human. We lock the text encoder branch and\\nStable Diffusion \\nInpainting\\nA photo of \\na person\\n(Imitator)\\nExtracted  \\nTexture map\\nVisibility Mask\\nPredicted \\nComplete \\nTexture map\\n28,60， -03794\\nFigure 3. Stage-1 of 3DHM: In the first stage, given a single view\\nimage of an imitator, we first apply 4Dhumans [9] style sampling\\napproach to extract partial texture map and its corresponding visi-\\nbility map. These two inputs are passed to the in-painting diffusion\\nmodel to generate a plausible complete texture map. In this exam-\\nple, while we only see the back view of the imitator, the model\\nwas able to hallucinate a plausible front region that is consistent\\nwith their clothing.\\nalways feed ‘real human’ as input text of fixed Stable Dif-\\nfusion models. We refer to our trained model as Inpainting\\nDiffusion. See Figure 3 for the model architecture.\\n3.2. Human Rendering\\nIn Stage 2, we aim to obtain a realistic rendering of a human\\nimitator doing the actions of the actor. While the interme-\\ndiate renderings (rendered with the poses from the actor\\nand texture map from Stage-1) can reflect diverse human\\nmotion, these SMPL mesh renderings are body-tight and\\ncannot represent realistic rendering with clothing, hairstyles,\\nand body shapes. For example, if we input a scene where\\na girl is wearing a dress and she is dancing, the intermedi-\\nate renderings might be able to “dance\" but it is impossible\\nto animate the skirt with SMPL mesh rendering. To train\\nthis model, in a fully self-supervised fashion, we assume\\nthe actor is the imitator, after all a good actor should be a\\ngood imitator. This way, we can take any video, and get a\\nsequence of poses from 4DHumans [9] and take any single\\nframe, and get a complete texture map from Stage-1, then get\\nthe intermediate renderings by rendering the texture maps\\non the 3D poses. Now, we have paired data of intermediate\\nrenderings and real RGB images. Using this, we collect a\\nlarge amount of paired data and train our Stage-2 diffusion\\nmodel with conditioning.\\nInput: We first apply the generated texture map (fully com-\\nplete) from Stage 1 to actor 3D body mesh sequences to an\\nintermediate rendering of the imitator performing the actions\\nof the actor. Note at this time, intermediate rendering can\\nonly reflect the clothing that fits the 3D mesh (body-tight\\nclothing) but fails to reflect the texture outside the SMPL\\nbody, such as the puffed-up region of a skirt, winter jacket, or\\nhat. To obtain the human with complete clothing texture, we\\ninput the obtained intermediate renderings and the original\\nInput Image\\n      (t=0)\\n   Texture mapped\\n                  (at time t)\\nStableDiffusion \\nEncoder\\nStableDiffusion \\nDecoder\\n3D Controllable\\nBranch\\nInput Latents \\n(64 x 64)\\nOutput Latents \\n(64 x 64)\\nFinal Rendering\\n(at time t)\\ndecode latents\\ncvpr1 - 106\\nFigure 4. Stage-2 of 3DHM: This figure shows the inference of our\\nStage-1 approach. Given an intermediate rendering of the imitator\\nwith the pose of the actor and the actual RGB image of the imitator,\\nour model can synthesize realistic renderings of the imitator on the\\npose of the actor.\\nimage of the person into Rendering Diffusion to render the\\nhuman in a novel pose with a realistic appearance.\\nTarget: Since we collected the data by assuming the actor\\nis the imitator, we have the paired data of the intermediate\\nrenderings and the real RGB images. This allows us to train\\nthis model on lots of data, without requiring any direct 3D\\nsupervision.\\nModel. Similar to ControlNet, we directly clone the weights\\nof the encoder of the Stable Diffusion [20] model as our Con-\\ntrollable branch (\"trainable copy\") to process 3D conditions.\\nWe freeze the pre-trained Stable Diffusion and input noisy la-\\ntents (64×64). In the meanwhile, we input a texture mapped\\n3D human at time t and original human photo input into a\\nfixed VAE encoder and obtain texture mapped 3D human\\nlatents (64 × 64) and appearance latents (64 × 64) as condi-\\ntioning latents. We feed these two conditioning latents into\\nRendering Diffusion Controllable branch. The key design\\nprinciple of this branch is to learn textures from human input\\nand apply them to the texture mapped 3D human during\\ntraining through the denoising process. The goal is to render\\na real human with vivid textures from the generated(texture\\nmapped) 3D human from Stage 1. We obtain the output\\nlatent and process it to the pixel space via diffusion step pro-\\ncedure and fixed VAE decoder. Same to Stage 1, we lock the\\ntext encoder branch and always feed ‘a real human is acting’\\nas input text of fixed Stable Diffusion models. We refer to\\nour trained model as Rendering Diffusion. In Rendering\\nDiffusion, we predict outputs frame by frame. We show the\\nStage 2 workflow in Figure 4.\\n4. Experiments\\n4.1. Experimental Setup\\nDataset.\\nWe collect 2,524 3D human videos from\\n2K2K [11], THuman2.0 [33] and People-Snapshot [2]\\ndatasets. 2K2K is a large-scale human dataset with 3D\\nhuman models reconstructed from 2K resolution images.\\nTHuman2.0 contains 500 high-quality human scans captured\\nby a dense DLSR rig. People-Snapshot is a smaller human\\ndataset that captures 24 sequences. We convert the 3D hu-\\nman dataset into videos and extract 3D poses from human\\nvideos using 4DHumans. We use 2,205 videos for training\\nand other videos for validation and testing. See the Appendix\\nfor more details on the dataset distribution on clothing.\\nEvaluation Metrics. We evaluate the quality of generated\\nframes of our method with image-based and video-based\\nmetrics. For image-based evaluation, we follow the evalua-\\ntion protocol of DisCO [28] to evaluate the generation qual-\\nity. We report the average PSNR [13], SSIM [30], FID [12],\\nLPIPS [35], and L1. For video-based evaluation, we use\\nFVD [26]. For pose evaluating 3D pose accuracy we use\\nMPVPE and PA-MVPVE. MPVPE [18], or Mean Per-Vertex\\nPosition Error, is a critical metric in 3D human pose estima-\\ntion, which quantifies the average distance between predicted\\nand actual 3D vertices across a model. This measurement\\nis essential for evaluating the accuracy of 3D reconstruc-\\ntions and pose estimations, with a lower MPVPE indicat-\\ning higher precision. Complementing this, PA-MPVPE, or\\nProcrustes-Aligned Mean Per-Vertex Position Error, adds\\nanother dimension to this evaluation. It involves aligning\\nthe predicted and ground truth data using Procrustes Analy-\\nsis, which neutralizes differences in orientation, scale, and\\nposition before calculating the mean error. This alignment\\nallows PA-MPVPE to focus on the structural accuracy of\\npredictions, making it a valuable metric for assessing the\\nrelative positioning of vertices in a model, independent of\\ntheir absolute spatial coordinates.\\nImplementation Details. As for training all the datasets, we\\nset the constant learning rate as 5e-05 and use the pre-trained\\ndiffusion models from diffusers [27] for both Stage-1 and\\nStage-2. As for Stage 1 Inpainting Diffusion, we finetune on\\nStable Diffusion Inpainting models [21], which has an 859M\\ntotal number of trainable parameters and 206M total number\\nof non-trainable parameters, since the VAE is frozen during\\nthis stage. We train Rendering Diffusion for 50 epochs and it\\ntakes about 2 weeks to run our model on our soup of training\\ndatasets. As for Stage 2 Rendering Diffusion, we train the\\nControllable branch and freeze Stable Diffusion backbones.\\nThe total number of trainable parameters in this case is 876M\\nand the total number of non-trainable parameters is 1.1B.\\nWe train Rendering Diffusion for 30 epochs and it takes\\nMethod\\nPSNR↑\\nSSIM ↑\\nFID ↓\\nLPIPS ↓\\nL1 ↓\\nDreamPose\\n35.06\\n0.80\\n245.19\\n0.18\\n2.12e-04\\nDisCO\\n35.38\\n0.81\\n164.34\\n0.15\\n1.44e-04\\nOurs\\n36.18\\n0.86\\n154.75\\n0.12\\n9.88e-05\\nTable 1. Quantitative comparison on frame-wise generation\\nquality : We compare our method with prior works on pose con-\\ndition generation tasks and measure the generation quality of the\\nsamples.\\nabout 2 weeks to run our model on training datasets based\\non 8 NVIDIA A100 GPUs with a batch size of 4. As for\\ninference, we only need to run Stage-1 once to reconstruct\\nthe full texture map of the imitator, and it is used for all other\\nnovel poses and viewpoints. We run Stage-2 inference for\\neach frame independently, however since the initial RGB\\nframe of the imitator is conditioned for all frames, the Stage-\\n2 model is able to produce samples that are temporarily\\nconsistent.\\n4.2. Quantitative Results\\nBaselines. We compare our approaches with past and state-\\nof-the-art methods: DreamPose [14], DisCo [28] and Con-\\ntrolNet [34] (for pose accuracy comparisons)1. We set infer-\\nence steps as 50 for all the approaches for fair comparisons.\\nComparisons on Frame-wise Generation Quality. We\\ncompare 3DHM with other methods on 2K2K test dataset,\\nwhich is composed of 50 unseen human videos, at 256×256\\nresolution. For each human video, we take 30 frames that\\nrepresent the different viewpoints of each unseen person.\\nThe angles range from 0◦ to 360◦, we take one frame every\\n12◦ to better evaluate the prediction and generalization abil-\\nity of each model. As for DisCO, we strictly follow their\\nsetting and extract OpenPose for inference. As for Dream-\\nPose, we extract DensePose for inference. We evaluate the\\nresults and calculate the average score over all frames of each\\nvideo. We set the background as black for all approaches for\\nfair comparisons. We report the average score overall of the\\nsame 50 videos and show the comparisons in Table 1. We\\nobserve that 3DHM outperforms all the baselines in different\\nmetrics.\\nComparisons on Video-level Generation Quality. To ver-\\nify the temporal consistency of 3DHM, we also report the re-\\nsults following the same test set and baseline implementation\\nas in image-level evaluation. Unlike image-level compar-\\nisons, we concatenate every consecutive 16 frames to form\\na sample of each unseen person on challenging viewpoints.\\nThe angles range from 150◦ to 195◦, we take one frame\\n1We utilize the open-source official code and models provided by the\\nauthors to implement these baselines. We use diffusers [27] for ControlNet\\nand Openpose extraction, and Detectron2 for DensePose extraction for\\nDisCO. Since Chan et al. [8] can only work for animating a specific person,\\nwe don’t compare with it in this paper.\\nMethod\\nFID-VID↓\\nFVD ↓\\nDreamPose\\n113.96\\n950.40\\nDisCO\\n83.91\\n629.18\\nOurs\\n55.40\\n422.38\\nTable 2. Quantitative comparison on video-level generation quality.\\nMethod\\nMPVPE ↓\\nPA-MPVPE ↓\\nDreamPose\\n123.07\\n82.75\\nDisCO\\n112.12\\n63.33\\nControlNet\\n108.32\\n59.80\\nOurs\\n41.08\\n31.86\\nTable 3. Quantitative comparison on pose accuracy.\\nevery 3◦ to better evaluate the prediction and generalization\\nability of each model. We report the average score overall\\nof 50 videos and show the comparisons in Table 2. We ob-\\nserve that 3DHM, though trained and tested by per frame,\\nstill embrace significant advantage over prior approaches,\\nindicating superior performance on preserving the temporal\\nconsistency with 3D control.\\nComparisons on Pose Accuracy. To further evaluate the\\nvalidity of our model, we estimate 3D poses from generated\\nhuman videos from different approaches via a state-of-the-\\nart 3D pose estimation model 4DHumans. We use the same\\ndataset setting mentioned above and compare the extracted\\nposes with 3D poses from the target videos. Following the\\nsame comparison settings with generation quality, we evalu-\\nate the results and calculate the average score over all frames\\nof each video. Beyond DreamPose and DisCO, we also com-\\npare with ControlNet, which achieves the state-of-the-art\\nin generating images with conditions, including openpose\\ncontrol. Since ControlNet does not input images, we input\\nthe same prompts as ours ‘a real human is acting’ and the\\ncorresponding openpose as conditions. We report the aver-\\nage score overall of 50 test videos and show the comparisons\\nin Table 3. We could notice that 3DHM could synthesize\\nmoving people following the provided 3D poses with very\\nhigh accuracy. At the same time, previous approaches might\\nnot achieve the same performance by directly predicting the\\npose-to-pixel mapping. We also notice that 3DHM could\\nachieve superior results on both 2D metrics and 3D metrics,\\neven if DisCO and ControlNet are controlled by Openpose\\nand DreamPose is controlled by DensePose.\\n4.3. Qualitative Results\\nOur work focuses on synthesizing moving people, primar-\\nily for clothing and the human body. With the aid of 3D\\nassistance, our approach has the potential to produce human\\nSettings\\nPSNR↑\\nSSIM ↑\\nFID ↓\\nLPIPS ↓\\nL1 ↓\\nDefault\\n36.18\\n0.86\\n154.75\\n0.12\\n9.88e-05\\nw/o Texture map\\n35.00\\n0.78\\n237.42\\n0.20\\n2.35e-04\\nw/o Appearance Latents\\n36.07\\n0.86\\n167.58\\n0.12\\n1.03e-04\\nadding SMPL parameters\\n36.42\\n0.87\\n157.60\\n0.12\\n8.87e-05\\nTable 4. Ablation study of Rendering Diffusion. We compare the\\nframe-wise generation quality under different settings. We notice\\nboth texturemap reconstruction and appearance latents are critical\\nto the model performance.\\nmotion videos in various scenarios. We consider challenging\\n3D poses and motions from 3 sources: 3D human videos,\\nrandom YouTube videos, and text input.\\nPoses from Unseen 3D Human Videos. We test our model\\non different 3D human videos with different human appear-\\nances and 3D poses from the 2K2K dataset. We verify that\\nthe tested video has never appeared in training data. We\\ndisplay the results in Figure 5a.\\nMotions from Random YouTube Videos. We test our\\nmodel on very different motions from randomly downloaded\\nYoutube videos for an unseen human. We display the results\\nin Figure 5b.\\nMotions from Text Inputs. We test our model on motions\\nfrom arbitrary text prompts. We randomly input an unseen\\nhuman photo and motions from random text inputs via a\\nwidely used human motion generative model (MDM [25]).\\nWe display the results in Figure 5c.\\n5. Analysis and Discussion\\n5.1. Ablation Study\\nTo further verify the components of our methods, we train\\non training dataset and test on test datasets. We extract the\\n3D rendered pose from these 50 test video tracks. Same\\nwith the settings in quantitative comparison, we calculate\\nthe average scores of PSNR, SSIM, VGG, L1, LPIPS among\\nall the generated frames and targeted original frames and\\nreport the results on both frame-wise metric (Table 4), video-\\nlevel metric (Table 5) and pose accuracy (Table 6). We find\\nthat both texture map reconstruction and appearance latents\\nare critical to the model performance. Also, we notice that\\ndirectly adding SMPL parameters into the model during\\ntraining may not bring improved performance considering\\nall evaluation metrics. This is presumably due to the impreci-\\nsion of SMPL parameters, which could provide contradictory\\ninformation throughout the diffusion training process if they\\nare not incorporated correctly.\\n5.2. 2D Control and 3D Control\\nWe also compare the results of the official model from\\nDreamPose and DisCO on a random person on a random\\nreal human photo which ensures distinct data distribution.\\nVarious\\nViewpoints\\n(a) 3DHM with a random human photo and a random 3D pose of various viewpoints. We show that even if the person’s photo is from a side angle, our stage 1\\ncan help reconstruct the full texture map, which could be used to obtain full body information. Stage 2 can add texture information based on a given input.\\nMotions\\xa0from\\nrandom\\xa0videos\\n(b) 3DHM with a random human photo and motions from random YouTube Videos. This example is from Gene Kelly’s dancing video.\\nA\\xa0person\\xa0turns\\xa0to\\xa0his\\xa0right\\xa0and\\xa0paces\\xa0back\\xa0and\\xa0forth.\\nMotions\\nfrom text\\n(c) 3DHM with a random human photo and motions generated from text inputs by MDM, a Human Motion Diffusion Model [25].\\nFigure 5. Qualitative results on different viewpoints of the same pose; motions from random videos and motions from text input.\\nWe display the qualitative results of various viewpoints in\\nFigure 6. DreamPose, DisCO, and 3DHM are all initialize\\nthe U-Net model with the pre-trained weights of Stable Dif-\\nfusion. We notice that 3DHM can generalize well to unseen\\nreal humans though it is only trained by limited 3D humans.\\nSince DreamPose requires subject-specific finetuning of the\\nUNet to achieve better results, it cannot directly generalize\\nwell on a random human photo. As for DisCO, though it has\\nbeen trained with an effective human attribute pre-training\\non multiple public datasets for better generalizability to un-\\nseen humans, still fails to synthesize people without the\\ntarget pose. We assume this is because 3DHM adds rigid\\n3D control to better correlate the appearance to the poses,\\nand preserve the body shape. Training with OpenPose or\\nDensePose cannot guarantee the mapping between textures\\nand poses, which makes it hard for the models to generalize.\\nInputs\\nVarious\\xa03D\\xa0Poses\\nDreamPose\\nDisCO\\nOurs\\nVarious\\xa0Viewpoints\\nFigure 6. Qualitative comparison with other 2D control approaches on a random real human photo (a Korean actress). We apply various 3D\\nposes or the same 3D pose from different viewpoints. It could be noticed that 2D poses may not be able to capture the folding motion, and\\ndetails of the human body. We could notice that our approach 3DHM could bridge this gap with 3D control.\\nMethod\\nFID-VID↓\\nFVD ↓\\nDefault\\n55.40\\n422.38\\nw/o Texture map\\n113.97\\n632.67\\nw/o Appearance Latents\\n93.21\\n715.51\\nadding SMPL parameters\\n72.35\\n579.90\\nTable 5. Ablation study of Rendering Diffusion. We compare the\\nvideo-level generation quality under different settings. We notice\\nthat although adding SMPL parameters achieve better performance\\non frame-wise setting but may yield worse temporal consistency\\nthan default settings.\\n5.3. Limitations\\nAs 3DHM generates the frames of the human motion videos\\nindependently, there is no guarantee of consistency in time.\\nFor example, the clothing light may change between consec-\\nutive frames. One possible solution is to train the model to\\npredict multiple frames simultaneously. Another possible\\nsolution is to condition the generation process on previously\\ngenerated frames via stochastic conditioning [1]. Addition-\\nMethod\\nMPVPE ↓\\nPA-MPVPE ↓\\nDefault\\n41.08\\n31.86\\nw/o Texture map\\n92.94\\n59.18\\nw/o Appearance Latents\\n41.99\\n32.82\\nadding SMPL parameters\\n39.16\\n29.67\\nTable 6. Ablation study of Rendering Diffusion. We compare the\\npose accuracy under different settings.\\nally, since 3DHM is trained on a dataset of 2K people, not all\\nthe detailed textures can be reconstructed completely during\\ninference (e.g. unique logos on the clothes). We hypothesize\\nthis could be alleviated by training with more human data.\\n6. Conclusion\\nIn this paper, we propose 3DHM, a two-stage diffusion\\nmodel-based framework that enables synthesizing moving\\npeople based on one random photo and target human poses.\\nA notable aspect of our approach is that we employ a cutting-\\nedge 3D pose estimation model to generate human motion\\ndata, allowing our model to be trained on arbitrary videos\\nwithout necessitating ground truth labels. Our method is\\nsuitable for long-range motion generation, and can deal with\\narbitrary poses with superior performance over previous ap-\\nproaches.\\nAcknowledgement\\nWe thank the Machine Common Sense project and ONR\\nMURI award number N00014-21-1-2801. We also thank\\nGoogle’s TPU Research Cloud (TRC) for providing cloud\\nTPUs. We thank Georgios Pavlakos, Shubham Goel, and\\nJane Wu for the constructive feedback and helpful discus-\\nsions.\\nAppendices\\nA. Dataset Analysis\\nFigures 7a and 7b present the clothing type statistics of the\\ntraining data (2,205 humans) and test data (50 humans). We\\ncount people based on four clothing categories: skirted attire,\\nsuit, casual wear, and others. In some cases, the clothing\\nbelongs to skirted attire and suits or casual wear, we will\\ncount this as skirted attire. For each clothing category, we\\ntally two styles: tight-fitting and loose-fitting.\\nIn this paper, we only train on limited human videos, we\\nassume training with more human videos could largely boost\\nthe model generalization on the fly. Given that 3DHM makes\\nuse of a cutting-edge 3D pose estimation model and only\\nrequires human videos without additional labels for training,\\nit could be trained with numerous and any human videos\\nsuch as movies, etc.\\nB. 3DHM Training Features\\nAs has been mentioned in the paper, 3DHM is in a fully\\nself-supervised fashion. Here we summarize the key training\\nfeatures of our approach:\\n• 3DHM training pipeline (for both stages) is self-\\nsupervised.\\n• 3DHM does not use any additional annotations. It is\\ntrained with pseudo-ground-truth as we use cutting-edge\\nsoftware which can detect, segment, track and 3Dfy hu-\\nmans (H4D).\\n• 3DHM is scalable and its scaling can be done readily in\\nthe future given additional videos of humans in motion\\nand computing resources.\\n128\\n7\\n571\\n2\\n0\\n250\\n500\\n750\\n1000\\n1250\\nSkirted Attire\\nSuit\\nCasual Wear\\nOthers\\nLoose-fitting\\nTight-fitting\\n(a) Training data distribution.\\n8\\n0\\n5\\n0\\n0\\n5\\n10\\n15\\n20\\n25\\nSkirted Attire\\nSuit\\nCasual Wear\\nOthers\\nLoose-fitting\\nTight-fitting\\n(b) Testing data distribution.\\nFigure 7. Data distribution. We split the clothing type into 4\\ncategories: skirted attire, suit, casual wear, and others. We split\\neach category into two types: loose and tight. We report the number\\nof each category and type and display the overall distribution. We\\ncould notice that most clothing is casual wear and a large portion\\nbelongs to tight-fitting.\\nReferences\\n[1] Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu\\nSalzmann, Lars Petersson, and Stephen Gould.\\nA\\nstochastic conditioning scheme for diverse human mo-\\ntion prediction. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition,\\npages 5223–5232, 2020. 8\\n[2] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Chris-\\ntian Theobalt, and Gerard Pons-Moll. Video based\\nreconstruction of 3d people models. In Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 8387–8397, 2018. 5\\n[3] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturedif-\\nfuclip: Gesture diffusion model with clip latents. arXiv\\npreprint arXiv:2303.14613, 2023. 2\\n[4] Christoph Bregler, Michele Covell, and Malcolm\\nSlaney. Video rewrite: Driving visual speech with au-\\ndio. In Seminal Graphics Papers: Pushing the Bound-\\naries, Volume 2, pages 715–722. 2023. 2\\n[5] Tim Brooks and Alexei A Efros. Hallucinating pose-\\ncompatible scenes. In European Conference on Com-\\nputer Vision, 2022. 2\\n[6] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser\\nSheikh.\\nRealtime multi-person 2d pose estimation\\nusing part affinity fields. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npages 7291–7299, 2017. 2\\n[7] Dan Casas and Marc Comino Trinidad.\\nSmplitex:\\nA generative model and dataset for 3d human tex-\\nture estimation from single image.\\narXiv preprint\\narXiv:2309.01855, 2023. 2, 3\\n[8] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and\\nAlexei A Efros. Everybody dance now. In Proceedings\\nof the IEEE/CVF international conference on computer\\nvision, pages 5933–5942, 2019. 2, 5\\n[9] Shubham Goel, Georgios Pavlakos, Jathushan Ra-\\njasegaran, Angjoo Kanazawa, and Jitendra Malik. Hu-\\nmans in 4D: Reconstructing and tracking humans with\\ntransformers. In ICCV, 2023. 2, 3, 4\\n[10] Rıza Alp Güler, Natalia Neverova, and Iasonas Kokki-\\nnos. Densepose: Dense human pose estimation in the\\nwild. In Proceedings of the IEEE conference on com-\\nputer vision and pattern recognition, pages 7297–7306,\\n2018. 2\\n[11] Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-\\nMi Kang, Young-Jae Park, and Hae-Gon Jeon. High-\\nfidelity 3d human digitization from single 2k resolution\\nimages. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR),\\n2023. 5\\n[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\\nBernhard Nessler, and Sepp Hochreiter. Gans trained\\nby a two time-scale update rule converge to a local\\nnash equilibrium. Advances in neural information pro-\\ncessing systems, 30, 2017. 5\\n[13] Alain Hore and Djemel Ziou. Image quality metrics:\\nPsnr vs. ssim. In 2010 20th international conference\\non pattern recognition, pages 2366–2369. IEEE, 2010.\\n5\\n[14] Johanna Karras, Aleksander Holynski, Ting-Chun\\nWang, and Ira Kemelmacher-Shlizerman. Dreampose:\\nFashion image-to-video synthesis via stable diffusion.\\narXiv preprint arXiv:2304.06025, 2023. 2, 5\\n[15] Sumith Kulal, Tim Brooks, Alex Aiken, Jiajun Wu,\\nJimei Yang, Jingwan Lu, Alexei A. Efros, and Kr-\\nishna Kumar Singh.\\nPutting people in their place:\\nAffordance-aware human insertion into scenes. In Pro-\\nceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), 2023. 2\\n[16] Boyi Li, Yin Cui, Tsung-Yi Lin, and Serge Belongie.\\nSitta: Single image texture translation for data augmen-\\ntation. In European Conference on Computer Vision,\\npages 3–20. Springer, 2022. 2\\n[17] Matthew Loper, Naureen Mahmood, Javier Romero,\\nGerard Pons-Moll, and Michael J Black. Smpl: A\\nskinned multi-person linear model. In Seminal Graph-\\nics Papers: Pushing the Boundaries, Volume 2, pages\\n851–866. 2023. 2, 3\\n[18] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu\\nLee. Accurate 3d hand pose estimation for whole-\\nbody 3d human mesh estimation. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 2308–2317, 2022. 5\\n[19] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo\\nKanazawa, and Jitendra Malik. Tracking people by pre-\\ndicting 3d appearance, location and pose. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pages 2740–2749, 2022. 2, 3\\n[20] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Björn Ommer. High-resolution im-\\nage synthesis with latent diffusion models. 2022 ieee.\\nIn CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 10674–10685, 2021. 4\\n[21] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Björn Ommer. High-resolution im-\\nage synthesis with latent diffusion models. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR), pages 10684–10695,\\n2022. 3, 5\\n[22] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Björn Ommer. High-resolution im-\\nage synthesis with latent diffusion models. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pages 10684–10695, 2022. 3\\n[23] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sal-\\nimans, et al. Photorealistic text-to-image diffusion\\nmodels with deep language understanding. Advances\\nin Neural Information Processing Systems, 35:36479–\\n36494, 2022. 2\\n[24] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin,\\nJie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\\nOron Ashual, Oran Gafni, et al. Make-a-video: Text-\\nto-video generation without text-video data.\\narXiv\\npreprint arXiv:2209.14792, 2022. 2\\n[25] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir,\\nDaniel Cohen-or, and Amit Haim Bermano. Human\\nmotion diffusion model. In The Eleventh International\\nConference on Learning Representations, 2023. 6, 7\\n[26] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Ku-\\nrach, Raphael Marinier, Marcin Michalski, and Syl-\\nvain Gelly. Towards accurate generative models of\\nvideo: A new metric & challenges. arXiv preprint\\narXiv:1812.01717, 2018. 5\\n[27] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pe-\\ndro Cuenca, Nathan Lambert, Kashif Rasul, Mishig\\nDavaadorj, and Thomas Wolf. Diffusers: State-of-\\nthe-art diffusion models. https://github.com/\\nhuggingface/diffusers, 2022. 5\\n[28] Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin,\\nZhengyuan Yang, Hanwang Zhang, Zicheng Liu, and\\nLijuan Wang. Disco: Disentangled control for referring\\nhuman dance generation in real world. arXiv preprint\\narXiv:2307.00040, 2023. 2, 5\\n[29] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin\\nLiu, Andrew Tao, Jan Kautz, and Bryan Catan-\\nzaro.\\nVideo-to-video synthesis.\\narXiv preprint\\narXiv:1808.06601, 2018. 2\\n[30] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and\\nEero P Simoncelli. Image quality assessment: from er-\\nror visibility to structural similarity. IEEE transactions\\non image processing, 13(4):600–612, 2004. 5\\n[31] Zhenzhen Weng, Laura Bravo-Sánchez, and Serena Ye-\\nung. Diffusion-hpc: Generating synthetic images with\\nrealistic humans. arXiv preprint arXiv:2303.09541,\\n2023. 2\\n[32] Xiangyu Xu and Chen Change Loy. 3d human texture\\nestimation from a single image with transformers. In\\nProceedings of the IEEE/CVF international conference\\non computer vision, pages 13849–13858, 2021. 3\\n[33] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu,\\nQionghai Dai, and Yebin Liu. Function4d: Real-time\\nhuman volumetric capture from very sparse consumer\\nrgbd sensors. In IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR2021), 2021. 5\\n[34] Lvmin Zhang and Maneesh Agrawala. Adding condi-\\ntional control to text-to-image diffusion models. arXiv\\npreprint arXiv:2302.05543, 2023. 2, 5\\n[35] Richard Zhang, Phillip Isola, Alexei A Efros, Eli\\nShechtman, and Oliver Wang. The unreasonable ef-\\nfectiveness of deep features as a perceptual metric. In\\nProceedings of the IEEE conference on computer vi-\\nsion and pattern recognition, pages 586–595, 2018.\\n5\\n'},\n",
       " {'abstract': 'RetinaVR, an affordable and immersive virtual reality simulator for vitreoretinal surgery training, is developed and validated in this study. RetinaVR does not require external haptic devices, leveraging the powerful processors, cameras, and sensors of the Meta Quest 2 VR headset. Four core surgical skills were chosen to be simulated:\\xa0core vitrectomy, peripheral shaving, membrane peeling, and endolaser application. The validation study involved 10 novice and 10 expert ophthalmologists. Construct validity, shown by varying user performance based on experimental runs, age, sex, and expertise, was demonstrated. A learning curve was noted in efficiency, safety, and task-specific performance, providing evidence of improvement with repetition. Specialists had significantly fewer sphere exits during core vitrectomy and better treatment patterns during endolaser application. The authors suggest that such a tool may democratize surgical simulation access and stimulate global ophthalmology innovation. However, further validation is needed to prove skill transfer to the operating room.',\n",
       "  'introduction': 'Ophthalmology has utilized virtual reality (VR) simulation in healthcare, improving the surgical performance of novice cataract surgeons and reducing complication rates. However, its use in vitreoretinal surgery training remains under-researched. The authors of this work present RetinaVR, an affordable and immersive VR simulator for vitreoretinal surgery training, hoping to democratize surgical simulation and spur global ophthalmic education. RetinaVR does not require external haptic devices, leveraging the capabilities of the Meta Quest 2 VR headset. RetinaVR was validated using a prospective study involving 10 novice and 10 expert ophthalmologists, with results demonstrating construct and learning curve validity.',\n",
       "  'literature review': \"VR simulation in ophthalmology has been shown to enhance cataract surgeon performance, but its role in vitreoretinal surgery training is less clear. An oft-cited VR simulator used in ophthalmology is the EyeSi Surgical Simulator, which comprises a mannequin head, surgical instruments, foot pedals, and a VR interface. EyeSi has been shown to be cost-effective for cataract surgery training but is expensive and requires an annual running cost. In developing nations and under-resourced areas, this poses a significant acquisition barrier. Thus, the study's authors sought to create an alternative which would be affordable and portable, broadening access to surgical simulation.\",\n",
       "  'methodology': 'The authors developed RetinaVR as a simulation app compatible with commercially available VR headsets. It utilizes the Meta Quest 2, a popular VR headset, for its processing power, cameras, and sensors. Four fundamental vitreoretinal surgery skills were chosen for simulation: core vitrectomy, peripheral shaving, membrane peeling, and endolaser application. Validation involved a prospective study comparing the performance of novice (n=10) and expert (n=10) ophthalmologists recruited from the University of Montreal. Construct validity was measured by analyzing performance based on expertise, age, sex, and experimental run. Additionally, learning curve validity was assessed by analyzing performance changes across three experimental runs.',\n",
       "  'results': 'Construct validity was demonstrated, with varying user performance based on experimental runs, age, sex, and expertise. Experts took less time and made fewer errors than novices. In core vitrectomy, experts had significantly fewer sphere exits. Both groups showed a learning curve in efficiency, safety, and task-specific performance, with improvements across experimental runs. In endolaser application, experts exhibited clinically significant differences in treatment patterns compared to novices. No impact of age or sex on performance was observed when controlling for expertise and experimental run.',\n",
       "  'conclusion': 'RetinaVR demonstrates construct and learning curve validity, supporting its use as a portable and affordable simulator, especially valuable in resource-limited settings. It could democratize surgical simulation access and promote global ophthalmic education and collaboration. The authors highlight the need for further validation, including skill transfer to the operating room, before widespread use, and also plan to address feedback received during the study to improve RetinaVR.',\n",
       "  'title': 'RetinaVR: Democratizing Vitreoretinal Surgery Training with a Portable and Affordable Virtual Reality Simulator in the Metaverse',\n",
       "  'author': 'Fares Antaki, Cédryk Doucet, Daniel Milad, Charles-Édouard Giguère, Benoit Ozell, Karim Hammamji',\n",
       "  'textdata': ' \\n \\n1 \\nRetinaVR: Democratizing Vitreoretinal Surgery \\nTraining with a Portable and Affordable Virtual \\nReality Simulator in the Metaverse \\n \\nFares Antaki, MDCM, FRCSC1,2,3, Cédryk Doucet, MASc4, Daniel Milad, MD2,3, Charles-\\nÉdouard Giguère, MSc5, Benoit Ozell, PhD4, Karim Hammamji, MD, FRCSC2,3,* \\n \\n1. The CHUM School of Artificial Intelligence in Healthcare, Montreal, Quebec, Canada; 2. Department of Ophthalmology, \\nUniversité de Montréal, Montreal, Quebec, Canada; 3. Department of Ophthalmology, Centre Hospitalier de l’Université de \\nMontréal, Montreal, Quebec, Canada; 4. Department of Computer Engineering and Software Engineering, Polytechnique \\nMontréal, Chemin de Polytechnique, Montréal, Canada; 5. Institut universitaire en santé mentale de Montréal (IUSMM), \\nMontreal, Quebec, Canada. *Correspondence. karim.hammamji@gmail.com \\n \\nFunding: This project received the Innovation in Retina Research Award, including the First Prize (CAD$35,000) and the \\nAudience Award (CAD$5,000), at the COS Annual Meeting in June 2021, co-sponsored by the COS and Bayer Inc. \\n \\n \\n \\nWe developed and validated RetinaVR, an affordable and immersive virtual reality \\nsimulator for vitreoretinal surgery training, using the Meta Quest 2 VR headset. We \\nfocused on four core fundamental skills: core vitrectomy, peripheral shaving, \\nmembrane peeling, and endolaser application. The validation study involved 10 \\nnovice ophthalmology residents and 10 expert vitreoretinal surgeons. We \\ndemonstrated construct validity, as shown by the varying user performance in a way \\nthat correlates with experimental runs, age, sex, and expertise. RetinaVR shows \\npromise as a portable and affordable simulator, with potential to democratize surgical \\nsimulation access, especially in developing countries. \\n \\nIntroduction \\nVirtual reality (VR) simulation in healthcare has made significant progress over the past two \\ndecades and is now considered a cornerstone of medical education.1 In surgery, it enables \\ntrainees to acquire skills in an immersive learning environment that mitigates patient harm. \\nThe digital nature of VR also alleviates the ethical and logistic challenges tied to wet lab \\ntraining, while offering an interactive, high-fidelity experience.2 In ophthalmology, VR \\nsimulation has been shown to improve the performance of novice cataract surgeons and to \\ndecrease their complication rate.3,4 Similar trends have been observed in vitreoretinal \\nsurgery training, but without definite evidence on skill transfer to the operating room.5,6 \\nThe most frequently studied VR simulator in ophthalmology is the EyeSi Surgical \\nSimulator (Haag-Streit Simulation). It comprises a mannequin head, surgical instruments, \\nfoot pedals, and a VR interface, accessible through the operating microscope.7 Despite its \\nhigh cost of acquisition (approximately USD$200,000) and its annual running costs, the use \\nof EyeSi has been shown to be cost-effective for cataract surgery training when considering \\nthe reduction of complications.4,8,9 However, in developing nations and under-resourced \\ncommunities, the simulator\\'s cost could pose a significant acquisition barrier. This may \\n \\n \\n2 \\ndisproportionately affect these already vulnerable groups, further exacerbating their risk of \\nadverse health outcomes.10 \\nSince the 1970s, head-mounted displays (VR headsets) have steadily decreased in \\nweight and improved in computing capacity. VR headsets have moved beyond academic \\nlabs and are commercially available with prices starting from USD$299.11 VR headsets offer \\nseveral benefits over traditional stationary simulators, including portability, improved \\nimmersiveness, and multiplayer capabilities through ‘the metaverse’.12 This allows multiple \\nusers to concurrently use the system and interact together in a virtual environment. By \\nleveraging their existing hardware and software capabilities, VR headsets can democratise \\naccess to surgical simulation, making the metaverse a particularly useful space for global \\nophthalmic education and collaboration. \\nIn this work, we developed a VR simulation application software for vitreoretinal \\nsurgery training that is compatible with commercially available VR headsets. RetinaVR is \\nfully immersive, affordable, and portable, as it leverages the powerful processors, cameras, \\nand sensors of the headset without the need for external haptic devices. We focus on four \\nfundamental skills: core vitrectomy, peripheral shaving, membrane peeling, and endolaser \\napplication. To our knowledge, this is the first vitreoretinal surgery simulator of its kind \\n \\nMethods \\nWe provide an overview of RetinaVR in Figure 1. RetinaVR was developed as a simulation \\napp that is compatible with off-the-shelf hardware. We focused our work on the affordable \\nMeta Quest 2 VR headset (Meta Platforms Inc., California), the best-selling VR headset \\navailable at the time.13 Four training modules were built to simulate fundamental skills in \\nvitrectomy surgery. \\nVirtual reality hardware \\nWe carried out all development experiments on the wired HP Reverb, attached to an AMD \\nRyzen 5 computer with 2600x CPU, 16GB of RAM, and an AMD Radeon RX 5700 XT \\ngraphics card. After each version iteration, we adapted the app for the wireless Meta Quest \\n2 to allow our domain experts to test the software remotely and to provide iterative feedback. \\nTo ensure broad applicability, we utilized the standard controllers packaged with the Meta \\nQuest 2 only, rather than exploring add-on external haptic devices. \\n \\nThe Meta Quest 2 is a general-purpose VR headset that allows for a standalone \\nexperience, eliminating the need for wiring or a computer connection. This feature renders it \\napt for surgical simulation training, providing an unencumbered environment conducive to \\nlearning. It comes with two light-weight plastic controllers, each weighing approximately 150 \\ngrams, that are tracked by the headset\\'s integrated cameras. The controllers are designed to \\nrest within the curve of the user\\'s palm, allowing the user\\'s fingers to engage with the \\ncapacitive face, grip and trigger buttons as well as the joystick. \\n \\n \\n \\n3 \\n \\nFigure 1. Overview of the RetinaVR development and validation framework. A. RetinaVR was developed in \\nthe Unity 3D game engine and deployed as an ‘app’ on the Meta Quest 2 VR headset. B. Four training modules \\nsimulating fundamental skills in vitrectomy surgery were developed: core vitrectomy (Navigation Training), \\nperipheral shaving (Tremor Control), membrane peeling (Peeling Control), and endolaser application (Laser \\nPrecision). C. Multiple potential use cases were considered as rationale for selecting the app format and the \\nstandalone VR headset. Those included the possibility for home-based solo training, synchronous and \\nasynchronous group training through the metaverse, and social competitions and score leaderboards. D. To \\ndetermine construct validity, we designed a prospective validation study comparing the performance of novice \\n(n=10) and expert users (n=10) recruited from the University of Montreal in Montreal, Quebec, Canada. We \\nanalyzed numerous metrics including efficiency, safety and module-specific performance, in relation to their level \\nof expertise and demographic factors. \\nVirtual reality software \\nWe developed RetinaVR in the Unity 3D game engine. To represent the eye, a virtual sphere \\nwas created, and a custom-made fundus illustration was fitted on its inner surface. The \\nvirtual instruments (light pipe and vitrector/ endolaser) were controlled using standard \\ncontrollers without the use of a physical eye model. The fulcrum effect was challenging to \\nreproduce due to the lack of haptic feedback from the virtual eye and the disconnect \\nbetween the two controllers. As such, only one controller could be used to move the eye. For \\nall tasks, the left controller was used as a light pipe, while the right controller served as a \\nvitrector or an endolaser probe, and controlled eye movements. The virtual instruments \\nposition and their movements were rotated 45 degrees on the x-axis to allow for ergonomic \\nholding of the controllers. To enhance the realism of the simulation, we added the \\n \\n \\n4 \\ncharacteristic sound emission produced by the pneumatic guillotine cutter (recorded at 7,500 \\ncuts per minute) during core vitrectomy and peripheral shaving.14 We also added a laser \\nsound to the endolaser application module. The simulation ergonomics are shown in \\nSupplemental Figure 2. \\nTraining modules \\nWe focused on four fundamental vitreoretinal surgery skills to devise four corresponding \\ntraining tasks: core vitrectomy (Navigation Training), peripheral shaving (Tremor Control), \\nmembrane peeling (Peeling Control), and endolaser application (Laser Precision). \\nScreenshots from each of the modules are shown in Figure 3. \\n \\n \\n \\nFigure 3. In-game screenshots from the RetinaVR modules. A. Navigation Training simulates core vitrectomy. \\nThe goal of the user is to collide with all red spheres, maintain the vitrector in the sphere and turn them green. B. \\nTremor Control simulates peripheral shaving. The user will engage the tip of the vitrector with a sphere, allowing \\nit to move along a pre-determined path. C. Peeling Control simulates membrane peeling. The user will grasp the \\nmembrane by pressing the grip button on the controller before peeling it away from the macula. D. Laser \\nPrecision simulates endolaser application. The user is asked to treat five retinal breaks by applying laser spots to \\na surrounding donut. A green marker will indicate a fully-treated tear. \\n \\n1. Navigation Training: To assess navigation skills in the vitreous body, a sphere \\ncollection exercise was designed using the ‘Collision detection’ module in Unity. Initially \\nred, the spheres turn green when collected. To collect a sphere, the tip of the vitrector \\nmust maintain contact with it for 2 consecutive seconds (determined heuristically) for it \\nto disappear. The exercise concludes once all 10 spheres at varying depths within the \\nvitreous body are collected. \\n \\n \\n \\n5 \\n2. Tremor Control: The user\\'s ability to control the vitrector during peripheral shaving is \\nassessed by moving a target sphere along a pre-determined path. When the tip of the \\nvitrector collides with the sphere, it causes the sphere to move along the path until the \\ninstrument loses contact with the sphere. The goal is to move the sphere along the path, \\nwithout deviating, as smoothly as possible without touching the retina. \\n \\n3. Peeling Control: This exercise simulated peeling epiretinal membranes using a cutter-\\nbased approach (rather than forceps).15 The objective was to peel the membrane \\ncompletely from the retina without iatrogenic touch. Users could enlarge their view by \\npressing the \\'X\\' button on the left controller, simulating a magnifying lens. To grab the \\nmembrane, users were required to press the right grip button. The membrane could only \\nbe peeled if a neighboring border was detached, requiring multiple grasps. \\n \\n4. Laser Precision: This exercise focused on applying endolaser around 5 retinal breaks \\nin the periphery. The laser probe had a traditional red spot that varied in size based on \\nits distance to the retina. As in real life, this affected the laser uptake, with larger spots \\nbeing less intense. The laser was applied by pressing the grip button. Repeat mode was \\navailable by holding the button, with an interval of 200ms. When a tear was considered \\nfully-treated, it turned green, signalling to the user to move on to the next break. During \\ndevelopment, to ensure that tears were fully treated, we used a raycasting approach in \\nUnity and heuristically adjusted the threshold for \"fully treated\" until we achieved our \\ndesired goal of two rows of laser spots 360 degrees around each break. \\n \\nValidation study \\nAfter 2 years of development, we locked RetinaVR in March 2023 to prepare it for human \\nvalidation. Novices and experts were recruited from the Department of Ophthalmology of the \\nUniversity of Montreal in Quebec, Canada from April 2023 through October 2023. The \\n‘Novice’ group included ophthalmology residents in their first, second or third years of \\nresidency and who have not had any hands-on exposure to intraocular surgery. Exposure to \\noculoplastic and strabismus surgery, and previous VR exposure (other than RetinaVR) were \\nnot exclusionary. The ‘Expert’ group included experienced fellowship-trained vitreoretinal \\nsurgeons and vitreoretinal surgery fellows. The sample size could not be determined a priori \\nand was limited by the availability of vitreoretinal surgeons. We recruited all vitreoretinal \\nsurgeons at our institution and matched them with an equal number of novice participants. \\nOur sample size is on par with most studies looking at VR simulation in vitreoretinal surgery.5 \\nWe excluded participants if they had any contraindications for VR gaming, including seizure \\ndisorder, vertigo, motion sickness and known VR cybersickness. We obtained ethics \\napproval by the Institutional Review Boards of the CHUM Hospital (IRB # 2023-10479-\\n22.035). Informed consent was obtained from all participants after detailing the nature of the \\nstudy.  \\n \\nNovices and experts were scheduled to test the portable RetinaVR simulator on a \\nMeta Quest 2 headset at their convenience. We often tested in a conference room, requiring \\nonly a flat surface. Our lead technical and clinical experts were available during testing, \\ncasting the user\\'s view to a connected computer. Before each recorded test, users received \\na brief explanation of the tasks while wearing the VR headset. They were also instructed on \\nhow to position their hands and calibrate the instruments, and they were allowed a single \\n \\n \\n6 \\ntrial run of each module. A life-size soft silicone doll head simulated the patient head, \\nallowing users to rest their wrists. All users sat superiorly relative to the eye. \\nCollected data \\nTo determine the construct validity of our simulator, we needed to study the impact of user \\nfactors like age, self-reported sex and level of expertise on simulation performance.  We \\ncollected all possible measurable performance metrics directly from RetinaVR, using built-in \\ncode. All modules were evaluated based on three criteria: Efficiency, Safety, and Module-\\nspecific performance. For all modules, Efficiency was assessed by measuring completion \\ntime in seconds, while Safety was assessed by counting the number of iatrogenic retinal \\ntouches. Module-specific performance metrics varied depending on the module. In \\nNavigation Training, the number of exits from the target sphere was counted. For Tremor \\nControl, the number of exits from the target sphere was counted, along with the mean and \\nmaximum deviation from the shaving path in milimeters. In Membrane Peeling, the number \\nof membrane grasps was counted, with the hypothesis that the number of grasps would vary \\nwith experience and technique. For Laser Precision, the number of laser spots was \\nrecorded, with the hypothesis that a parsimonious use of laser was better as long as the \\ntears were treated.16 The precise coordinates of the laser spots around tears were also \\nrecorded to determine the treatment pattern. \\nUser experience \\nTo measure user experience (UX), we administered a French abbreviated version of the \\nvalidated Immersive Virtual Environments Questionnaire (IVEQ) v2.17 The questionnaire \\nconsisted of 26 questions: 2 to gauge the user\\'s prior experience with VR, 21 that were \\ngradable using a 10-point Likert scale, and 3 open-ended questions for general comments \\nand feedback. The gradable questions assessed a broad range of UX factors, including \\nPresence (n=3), Engagement (n=2), Immersion (n=2), Flow (n=2), Emotion (n=2), Skill \\n(n=2), Judgement (n=3), Experience Consequence (n=2), and Technology Adoption (n=3). \\nThe questionnaire is available in Appendix 1. The 3 open-ended questions aimed to gather \\npositive feedback, negative feedback, and suggestions for improvement. To analyze the free \\ntext responses, we elucidated the prevalent themes from each response and then \\nconsolidated them into broad categories. Once a coherent representation of data across all \\nparticipants was achieved, the frequency of each theme recorded and summarised. \\nStatistical methods \\nWe first explored the unadjusted differences in performance between novices and experts by \\ncalculating the standardized mean difference for each performance metric. This effect size \\nanalysis was useful to contextualize our findings, given the disparate units (count-, time-, \\ndistance-based) and varying scales of the performance metrics stemming from the differing \\ndifficulty levels of the four training modules. Additionally, given the novelty of our \\nexperimental design, the lack of existing normative data to define \"good or bad\" or \"fast or \\nslow\" performance also necessitated this scaled analysis. We interpreted the effects as \\nfollows: 0.01 – 0.19 (minimal), 0.20 – 0.49 (small/ mild), 0.50 – 0.79 (medium/ moderate), \\n0.80 – 0.99 (large) and >1.0 (very large).18 \\n \\n \\n7 \\n \\nWe then carried out an adjusted analysis and explored differences between novices \\nand experts while controlling for age, sex and experimental run – factors that can influence \\nthe VR gaming experience.19–21 We used a linear mixed-effect model, which allowed us to \\nisolate the effect of each factor while controlling for all others. All analyses were performed in \\nR V.4.3.1 for our analyses at a 5% alpha level. \\n \\n \\nResults \\nBaseline demographics \\nWe recruited 20 participants, including 10 novices and 10 experts. Their baseline and \\ndemographic characteristics are detailed in Table 1. Novices were significantly younger and \\npredominantly female. Novices had no prior surgical experience (a selection criterion), \\nwhereas the experts, on average, had 16.6 years (10.71) of post-residency surgical \\nexperience. Novices reported more hours of VR gaming than experts, but this difference was \\nnot statistically significant. They also reported more hours of training on VR-based surgical \\nsimulators than experts, with this difference being statistically significant. We provide \\ndescriptive statistics on the performance of novices and experts across all runs and modules \\nin Supplemental Table 2, Supplemental Table 3, Supplemental Table 4, and \\nSupplemental Table 5.  \\n \\nCharacteristic \\nNovices (n=10) \\nExperts (n=10) \\nDifference \\nAge – yr \\n28.2 (3.61) \\n47.1 (12.3) \\np=0.001 \\nFemale sex – n \\n7 (70%) \\n3 (30%) \\np=0.0736 \\nSurgical expertise – yr  \\n0 (0) \\n16.6 (10.71) \\np=0.001 \\nPrevious VR gaming – hrs \\n6.7 (12.82) \\n2.1 (3.6) \\np=0.299 \\nPrevious VR surgical training – hrs \\n22.6 (23.29) \\n3.8 (6.53) \\np=0.033 \\n \\nTable 1. Demographic and baseline characteristics of the novice and expert users. Values are mean (SD) \\nunless otherwise specified. VR, virtual reality \\nImpact of the expertise \\nWe first compared the performance of novices and experts using an unadjusted model. The \\nresults are summarized in Figure 4. The detailed effect size analyses are shown in \\nSupplemental Table 6. The linear mixed-effects model results are summarized in Table 7. \\n \\nRegarding efficiency, we found trends that novices were slower than experts, except \\nin membrane peeling. None of those effects were statistically significant when all \\nexperimental runs were combined. When examining the runs individually, we found that \\nnovices were faster than experts in the first membrane peeling run (very large effect, -1.10 \\n[95%CI: -2.03, -0.14]). In the linear mixed-effects model, when controlling for age, sex and \\nexperimental run, the trends were maintained, but we found no statistically significant \\ndifference in efficiency between novices and experts in any of the modules.  \\n \\n \\n \\n \\n8 \\n \\nFigure 4. Forest plot showing the unadjusted effect sizes of efficiency, safety and task-specific \\nperformance between experts and novices. The point effect estimate is Cohen’s D and represents the \\nstandardized mean difference between novice and expert performance, along with 95% confidence intervals. \\nPositive effects, represented by values to the right of the y-axis, indicate higher novice metrics (e.g., longer \\nnovice completion times, more novice retinal touches), suggesting lower novice performance. Conversely, \\nnegative effects, represented by values to the left of the y-axis, indicate higher expert metrics (e.g., longer expert \\ncompletion times, more expert retinal touches), implying better novice performance. The viridis color palette is \\nused to interpret the effect sizes, representing a minimal effect (0.01-0.19), a small or mild effect (0.20-0.49), a \\nmedium or moderate effect (0.50-0.79), a large effect (0.80-0.99), and a very large effect (>1.0).  \\n \\nModule and metric \\nExpertise \\nEstimate \\np-value \\nNavigation Training \\n \\n \\n Efficiency \\n9.30 \\n0.50 \\n Safety \\n0.63 \\n0.50 \\n Sphere Exits \\n21.46 \\n0.014 \\nTremor Control \\n \\n \\n Efficiency \\n7.05 \\n0.29 \\n Safety \\n0.10 \\n0.94 \\n Sphere Exits \\n38.14 \\n0.29 \\n Mean Δ \\n0.00 \\n0.81 \\n Max Δ \\n-0.02 \\n0.71 \\nMembrane Peeling \\n \\n \\n Efficiency \\n-16.42 \\n0.17 \\n Safety \\n2.90 \\n0.11 \\n Grasps \\n-1.03 \\n0.25 \\nLaser Precision \\n \\n \\n Efficiency \\n16.68 \\n0.29 \\n Safety \\n0.63 \\n0.29 \\n Laser Spots \\n10.15 \\n0.64 \\n \\nTable 7. Linear mixed-effects model (adjusted) for the impact of expertise on performance. This model \\ncontrols for experimental run, user age and sex. The only significant effect is the difference in performance during \\nNavigation Training. Novices had an excess of 21.46 sphere exists compared to experts (p = 0.014). Efficiency \\nestimates are in seconds, and Safety estimates are in number of iatrogenic retinal touches. Sphere exits, number \\nof grasps and laser spots are count data. Mean and maximal deviation metrics are provided in meters in this \\ntable. \\n \\n \\n9 \\nRegarding safety, we found that experts were safer than novices in the membrane \\npeeling module when all experimental runs were combined (very large effect, 1.06 [95%CI: \\n0.52, 1.60]). This effect was also present in the first (very large effect, 1.34 [95%CI: 0.35, \\n2.31]) and second run (very large effect, 1.12 [95%CI: 0.16, 2.05], but not the third run. We \\nalso found trends that the experts were safer in all other modules, but those differences were \\nnot statistically significant. In the linear mixed-effects model, when controlling for age, sex \\nand experimental run, the trends were maintained, but we found no statistically significant \\ndifference in safety between novices and experts in any of the modules.  \\n \\nRegarding task-specific performance, we found that experts performed better in the \\ncore vitrectomy module, demonstrating significantly fewer exits from the target spheres \\n(moderate effect, 0.7 [95%CI: 0.18, 1.22]). This effect was mostly driven by the second \\nexperimental run (very large effect, 1.11 [95%CI: 0.15, 2.04]). In the linear mixed-effects \\nmodel, that difference was maintained while controlling for all other user factors, with novices \\nexiting the spheres an excess of 21.46 times (p = 0.014). In peripheral shaving, we found \\ntrends that novices had more sphere exits than experts while demonstrating less deviation \\nfrom the shaving path, but those differences were not statistically significant. In the linear \\nmixed-effects model, when controlling for other factors, those differences were also not \\nstatistically significant.  \\n \\nIn membrane peeling, we found trends that experts grasped the membrane more \\ntimes than novices, but that difference was not statistically significant in the unadjusted \\nmodel. The trend was maintained in the linear mixed-effects model, but the difference was \\nnot statistically significant. In endolaser application, we found no difference in the amount of \\nlaser used between novices and experts in the adjusted and unadjusted models. However, a \\nheatmap analysis of the laser spot distribution showed clinically significant differences in \\ntreatment patterns among novices and experts, as shown in Figure 5. \\n \\n \\n \\nFigure 5. Heatmap of laser shot distribution in the endolaser application (Laser Precision) module. \\nHeatmap illustrating laser treatment patterns for all five retinal tears, differentiated by novices and experts. Each \\nsquare represents a unique tear, with color intensity corresponding to the number of laser spots applied, using \\nthe viridis color palette. The color gradient ranges from purple (least density) to bright yellow (highest density). \\nThe central target represents the center-point of the retinal break, as shown in the Laser Precision module. \\nExperts showed a uniform distribution of laser spots, characterized by a consistent spread around each tear, \\nmaintaining a uniform distance from the central point. There is a ring-like pattern with minimal laser applications \\ndirectly on the tears. In contrast, novices showed a more erratic pattern (particularly in tears 1 and 2), with a \\nconcentration of laser spots towards the center-point of each tear. This indicates a less controlled application, \\nresulting in a scattered distribution with variable intensity and less discernible uniformity.  \\n \\n \\n10 \\nImpact of participant age and sex \\nWe evaluated the impact of participant age and sex on their performance, while controlling \\nfor experimental run and expertise. As shown in Table 8, in the linear mixed-effects model, \\nage had no impact on performance in any of the modules. Male participants were 12.35 \\nseconds faster in peripheral shaving (p=0.036) and 32.21 seconds faster in membrane \\npeeling (p=0.004) compared to women. We observed trends of males being more efficient, \\nsafer and performing better in most task-specific metrics, but none of those effects were \\nstatistically significant.  \\n \\nModule and metric \\nAge \\nSex \\nEstimate \\np-value \\nEstimate \\np-value \\nNavigation Training \\n \\n \\n \\n \\n Efficiency \\n0.29 \\n0.64 \\n-18.96 \\n0.11 \\n Safety \\n0.00 \\n0.93 \\n-0.17 \\n0.83 \\n Sphere Exits \\n0.72 \\n0.06 \\n-12.71 \\n0.07 \\nTremor Control \\n \\n \\n \\n \\n Efficiency \\n0.52 \\n0.09 \\n-12.35 \\n0.036 \\n Safety \\n0.00 \\n0.94 \\n-1.13 \\n0.32 \\n Sphere Exits \\n2.11 \\n0.20 \\n-57.02 \\n0.07 \\n Mean Δ \\n0.00 \\n0.63 \\n0.00 \\n0.95 \\n Max Δ \\n0.00 \\n0.54 \\n0.00 \\n0.94 \\nMembrane Peeling \\n \\n \\n \\n \\n Efficiency \\n0.70 \\n0.19 \\n-32.21 \\n0.004 \\n Safety \\n-0.03 \\n0.68 \\n-0.54 \\n0.71 \\n Grasps \\n0.01 \\n0.79 \\n-1.13 \\n0.14 \\nLaser Precision \\n \\n \\n \\n \\n Efficiency \\n0.99 \\n0.17 \\n-13.28 \\n0.32 \\n Safety \\n0.00 \\n0.91 \\n0.61 \\n0.23 \\n Laser Spots \\n0.78 \\n0.43 \\n-5.90 \\n0.75 \\n \\nTable 8. Linear mixed-effects model (adjusted) for the impact of age and sex on performance. This model \\ncontrols for all other variables including expertise when examining the effect of age and sex. Age had no impact \\non performance, but sex did. Male participants were 12.35 seconds faster in peripheral shaving (p=0.036) and \\n32.21 seconds faster in membrane peeling (p=0.004). Efficiency estimates are in seconds, and Safety estimates \\nare in number of iatrogenic retinal touches. Sphere exits, number of grasps and laser spots are count data. Mean \\nand maximal deviation metrics are provided in meters in this table. \\nImpact of the learning curve \\nWe also evaluated the learning curve by repeating the experiments three times for each \\nparticipant. As shown in Table 9, in the linear mixed-effects model, efficiency improved with \\neach experimental run during all modules. At each run, completion time decreased by 7.67 \\nseconds for core vitrectomy (p=0.005), 12.02 seconds for peripheral shaving (p<0.001), \\n17.92 seconds for membrane peeling (p<0.001) and 25.68 seconds for endolaser application \\n(p<0.001). We found that repetition improved safety scores during membrane peeling, with a \\n1.37 fewer iatrogenic retinal touches with each run (p=0.003). Similar trends were observed \\nfor all modules, but the effects were not statistically significant. However, it did reduce the \\nnumber of laser spots used by the participants. At each run, the number of sphere exits \\ndecreased by 5.42 times (p=0.038) in core vitrectomy and by 17.00 times during peripheral \\n \\n \\n11 \\nshaving (p=0.011). In endolaser application, participants used 11.20 less laser shots at each \\nrun to treat the tears (p=0.043). \\n \\nModule and metric \\nExperimental run \\nEstimate \\np-value \\nNavigation Training \\n \\n \\n Efficiency \\n-7.67 \\n0.005 \\n Safety \\n-0.20 \\n0.26 \\n Sphere Exits \\n-5.42 \\n0.038 \\nTremor Control \\n \\n \\n Efficiency \\n-12.02 \\n<0.001 \\n Safety \\n-0.47 \\n0.13 \\n Sphere Exits \\n-17.00 \\n0.011 \\n Mean Δ \\n0.00 \\n1.00 \\n Max Δ \\n-0.01 \\n0.22 \\nMembrane Peeling \\n \\n \\n Efficiency \\n-17.92 \\n<0.001 \\n Safety \\n-1.37 \\n0.003 \\n Grasps \\n0.27 \\n0.23 \\nLaser Precision \\n \\n \\n Efficiency \\n-25.68 \\n<0.001 \\n Safety \\n-0.02 \\n0.92 \\n Laser Spots \\n-11.20 \\n0.043 \\n \\nTable 9. Linear mixed-effects model (adjusted) for the impact of experimental run on performance. This \\nmodel controls for age, sex and expertise when examining the role of the experimental run. At each run, \\ncompletion time decreased by 7.67 seconds for core vitrectomy (p=0.005), 12.02 seconds for peripheral shaving \\n(p<0.001), 17.92 seconds for membrane peeling (p<0.001) and 25.68 seconds for endolaser application \\n(p<0.001). Furthermore, repetition improved safety scores during membrane peeling, with a 1.37 fewer iatrogenic \\nretinal touches with each run (p=0.003). At each run, the number of sphere exits decreased by 5.42 times \\n(p=0.038) in core vitrectomy and by 17.00 times during peripheral shaving (p=0.011). In endolaser application, \\nparticipants used 11.20 less laser shots at each run to treat the tears (p=0.043). Efficiency estimates are in \\nseconds, and Safety estimates are in number of iatrogenic retinal touches. Sphere exits, number of grasps and \\nlaser spots are count data. Mean and maximal deviation metrics are provided in meters in this table. \\nUser experience \\nOverall, the users rated the experience from favorable to excellent in all 8 spheres of UX, as \\nshown in Supplemental Table 10. Positive feedback predominantly centered on three \\nthemes: the realistic 3D environment (n = 18), the ability to practice in a low-risk environment \\n(n = 9), and the authentic representation of the vitrectomy experience (n = 5). Other sporadic \\ncomments praised the innovation, immersion, and portability of the experience. Negative \\nfeedback mentioned the fulcrum effect and controller-simulation movement translation (n=8), \\nthe controller size and ergonomics (n=6) and difficulty with visualization and depth \\nperception (n=6). Other comments included the lack of progress indicators, headset fit, and \\nunrealistic shaving module. Suggestions for improvement suggested improving the \\ncontrollers and ergonomics (n=10), providing better instructions and real-time feedback \\n(n=5), and improving movement translation (n=5). It was also recommended to attempt to \\nimprove headset fit, build more complete case-based modules, improve graphics, and \\ngamify the experience. \\n \\n \\n12 \\nDiscussion \\nWe built a RetinaVR, a fully immersive, affordable, and portable VR simulator for vitreoretinal \\nsurgery training. RetinaVR is a standalone app that leverages the powerful processors and \\ncameras of commercially-available VR headsets and controllers, without relying on external \\ntouch haptic devices. RetinaVR is a proof of concept for a new way of approaching surgical \\nsimulation in the metaverse, at a fraction of the cost of traditional VR simulators. It \\ndemocratizes access to surgical simulation, and has the potential to spur innovation in global \\nophthalmology.  \\n \\nTo ensure RetinaVR\\'s affordability and accessibility, we designed it to require only a \\nquick app download. The app is a mere 100 megabytes, taking approximately 20 seconds to \\ndownload on average global broadband speeds and less than 2 minutes in Sub-Saharan \\nAfrica.22,23 To simulate surgical instruments, we used the standard built-in controllers, rather \\nthan integrating custom hardware. Using pen-like haptic feedback devices could have \\nprovided a more faithful simulation of instruments, but it would have come at a high cost.24 \\nSince our simulator did not require instruments to be anchored to a physical eye model, the \\nfulcrum effect was difficult to simulate. This effect, encountered when using the vitrector and \\nlight pipe through a trocar, necessitates unique skills to move the instrument tips. Despite \\nthat, we feel that we accurately replicated the motion inversion and scaled motion required to \\nmove the vitrector tip, allowing the users to successfully complete the modules and improve \\nat each run. This is supported by the demonstration of the learning curve and the high \\nscores for the Flow theme in the UX questionnaire. The users did suggest, however, \\nimprovements in instrumentation. While our plastic controllers were lightweight, they were \\nstill considerably heavier than conventional surgical instruments. Their weight was 4 times \\nthat of a typical 23G vitrector. For comparison, the Bi-Blade vitrectomy cutter weighs \\napproximately 37 grams with the tubing (personal communication with Bausch + Lomb). \\n \\nTo capture user performance during simulation, we were faced with two options: \\neither collect as many metrics as possible and analyze them post-hoc, or develop a scoring \\nsystem by assigning weights to measurable metrics based on our subjective assessment of \\ntheir importance. The latter approach raised concerns about how to objectively measure task \\nefficiency, safety, and good performance, and how to determine the appropriate point \\ndeductions for mistakes. Given the potential for heuristic bias, we chose the first option and \\ndeveloped code in RetinaVR to quantify those metrics. We conducted a rigorous analysis of \\nthe data through an effect size analysis. This was crucial for interpreting the significance of \\nobserved differences, since these experiments were being conducted for the first time with \\nno normative databases to establish good or poor performance benchmarks. We then built \\nan adjusted model to examine the impact of age, sex and experimental run on performance, \\nand controlled for those factors when comparing novices and experts.  \\n \\nWe believe to have demonstrated construct validity.25,26 This refers to the ability \\nRetinaVR to measure user behaviors and performance in a way that correlates with their \\ninherent factors and level of expertise. We found that participant age had no impact on \\noverall performance when we controlled for sex, expertise and experimental run. However, \\nwe found that males performed membrane peeling and peripheral shaving tasks more \\nquickly than females, with no significant differences in safety and task-specific performance. \\n \\n \\n13 \\nSome evidence suggests that gaming proficiency may decline with age and show \\ndifferences between sexes.21,27–29 However, we believe that this phenomenon is more likely \\nattributable to a disparity in prior gaming experience, rather than innate age or sex-related \\nabilities. These effects may be even less pronounced in a surgical simulation context like \\nours, where older participants typically have more prior surgical experience. In parallel, we \\nfound that repetition boosted efficiency in all modules, and enhanced safety in the \\nmembrane peeling module. It also improved task-specific performance during core \\nvitrectomy and peripheral shaving. This demonstrates a learning curve across experimental \\nruns – with users getting better with repetition or practice. We feel that this observation \\nreinforces the notion that user performance was not a random occurrence but rather a \\nreflection of genuine learning. This learning curve has also been demonstrated for the \\nvitreoretinal modules of the EyeSi simulator in numerous studies.30,31  \\n \\nA crucial aspect of this project is the demonstration of how user expertise affects \\nperformance. We report on several notable findings in our work. First, novices tended to be \\nslower in all modules, except in membrane peeling. Interestingly, in membrane peeling, they \\ntended to be faster, while also being less safe, causing significantly more iatrogenic retinal \\ntouches, and grasping the membranes less frequently. These contrasts possibly highlight the \\ninfluence of real-world surgical experience. Experts demonstrated a more cautious and \\ndeliberate approach, peeling slowly and carefully to minimize shearing forces on the macula. \\nIn contrast, novices, perhaps viewing the simulation as such, exhibited riskier behavior by \\nattempting to complete the module at a faster pace, leading to more iatrogenic damage. \\nSecond, experts performed significantly better in the core vitrectomy module, exhibiting \\nfewer target sphere exits – a difference that was maintained when controlling for other user \\nfactors. Third, during endolaser application, we found clinically important differences in the \\ntreatment patterns between among novices and experts. This speaks to the construct validity \\nof those modules and their ability to faithfully simulate the surgical experience. \\n \\nRetinaVR marks a proof of concept for a novel type of platform for vitreoretinal \\nsurgery training simulation. We believe that RetinaVR can change the scope of surgical \\nsimulation in a number of ways. First, trainees can conveniently access RetinaVR using their \\npersonal headsets, integrating it alongside their existing VR-based entertainment, gaming, or \\nsports activities. Second, residency programs can effectively train multiple residents \\nsimultaneously by investing in multiple affordable VR headsets. The platform\\'s online \\nmetaverse integration, relying on Meta’s cloud servers, enables multiplayer group training \\nsessions, connecting residents virtually with expert surgeons from around the world, \\nbreaking down geographical barriers and fostering a global learning community. Third, the \\nplatform allows for both synchronous and asynchronous learning, which enables trainees to \\nobtain real-time feedback from mentors while also catering for individual learning styles and \\nschedules. Finally, gamification elements, such as points, badges, and international \\nleaderboards, can further enhance engagement and encourage healthy competition, \\nspurring innovation and collaboration in the field of vitreoretinal surgery.  \\n \\nWhile RetinaVR has demonstrated construct validity to a certain extent, our work has \\nsome limitations and further validation is necessary. First, statistical significance in our \\nanalyses was limited by the low sample size and high variance among novices. Despite that, \\nmost of our effects were congruent with the expected behaviors of novices and experts. \\nSecond, we have not yet demonstrated skill transfer to the operating room, a crucial step in \\n \\n \\n14 \\nvalidating a surgical simulator. However, to our knowledge, in vitreoretinal surgery, this has \\nnot been shown even for popular simulators like the EyeSi.5 RetinaVR remains a work in \\nprogress: the user interface, including menu appearances, profile creation, login \\nfunctionality, and leaderboards, require further development before public release. We are \\nalso working on incorporating feedback from this study to determine future directions for \\nRetinaVR. Despite these limitations, we are proud of what was achieved with limited \\nresources. RetinaVR serves as a proof of concept for developing affordable VR surgical \\nsimulation apps in an academic lab setting, fostering innovation in surgical training and \\nmedical education. Driven by the relentless innovation of industry titans like Meta and Apple, \\nwe are confident that standalone VR headsets will soon reach a high level of maturity.32 This \\nwill pave the way for the widespread availability of an off-the-shelf, affordable, and validated \\nRetinaVR app, empowering the trainees worldwide with an immersive surgical training \\nexperience. \\n \\n \\n \\n15 \\nReferences \\n1. Pottle J. Virtual reality and the transformation of medical education. Future Healthc J \\n2019;6:181–185. \\n2. Mao RQ, Lan L, Kay J, et al. Immersive Virtual Reality for Surgical Training: A Systematic \\nReview. J Surg Res 2021;268:40–58. \\n3. Thomsen ASS, Smith P, Subhi Y, et al. High correlation between performance on a \\nvirtual-reality simulator and real-life cataract surgery. Acta Ophthalmol 2017;95:307–311. \\n4. Ferris JD, Donachie PH, Johnston RL, et al. Royal College of Ophthalmologists’ National \\nOphthalmology Database study of cataract surgery: report 6. The impact of EyeSi virtual \\nreality training on complications rates of cataract surgery performed by first and second year \\ntrainees. Br J Ophthalmol 2020;104:324–329. \\n5. Rasmussen RC, Grauslund J, Vergmann AS. Simulation training in vitreoretinal surgery: a \\nsystematic review. BMC Ophthalmol 2019;19:90. \\n6. Jaud C, Salleron J, Cisse C, et al. EyeSi Surgical Simulator: validation of a proficiency-\\nbased test for assessment of vitreoretinal surgical skills. Acta Ophthalmol 2021;99:390–396. \\n7. Carr L, McKechnie T, Hatamnejad A, et al. Effectiveness of the Eyesi Surgical Simulator \\nfor ophthalmology trainees: systematic review and meta-analysis. Can J Ophthalmol 2023. \\nAvailable at: http://dx.doi.org/10.1016/j.jcjo.2023.03.014. \\n8. Kaur S, Shirodkar A-L, Nanavaty MA, Austin M. Cost-effective and adaptable cataract \\nsurgery simulation with basic technology. Eye 2022;36:1384–1389. \\n9. la Cour M, Thomsen ASS, Alberti M, Konge L. Simulators in the training of surgeons: is it \\nworth the investment in money and time? 2018 Jules Gonin lecture of the Retina Research \\nFoundation. Graefes Arch Clin Exp Ophthalmol 2019;257:877–881. \\n10. Oseni J, Adebayo A, Raval N, et al. National Access to EyeSi Simulation: A Comparative \\nStudy Among U.S. Ophthalmology Residency Programs. Nepal J Ophthalmol \\n2023;15:e112–e118. Available at: [Accessed November 14, 2023]. \\n11. Anon. Meta Quest 2: Immersive All-In-One VR Headset. Available at: \\nhttps://www.meta.com/us/quest/products/quest-2/ [Accessed November 14, 2023]. \\n12. Chengoden R, Victor N, Thien Huynh-The, et al. Metaverse for Healthcare: A Survey on \\nPotential Applications, Challenges and Future Directions. arXiv [csAI] 2022. Available at: \\nhttp://arxiv.org/abs/2209.04160. \\n13. Armstrong M. Meta leads the way in VR headsets. Statista 2023. Available at: \\nhttps://www.statista.com/chart/29398/vr-headset-kpis/ [Accessed November 17, 2023]. \\n14. Ruparelia S, Orr S, Choudhry N, et al. Risk for Surgical Team Hearing Loss With \\nVitrectomy. J Vitreoretin Dis 2023;7:397–403. \\n \\n \\n16 \\n15. Anon. No more forceps: A cutter-based approach to ILM peeling - retina today. Available \\nat: https://retinatoday.com/articles/2023-may-june/no-more-forceps-a-cutter-based-\\napproach-to-ilm-peeling [Accessed November 23, 2023]. \\n16. Wang JC, Ryan EH, Ryan C, et al. FACTORS ASSOCIATED WITH THE USE OF 360-\\nDEGREE LASER RETINOPEXY DURING PRIMARY VITRECTOMY WITH OR WITHOUT \\nSCLERAL BUCKLE FOR RHEGMATOGENOUS RETINAL DETACHMENT AND IMPACT \\nON SURGICAL OUTCOMES (PRO STUDY REPORT NUMBER 4). Retina 2020;40:2070–\\n2076. \\n17. Tcha-Tokey K, Christmann O, Loup-Escande E, Richir S. Proposition and validation of a \\nquestionnaire to measure the user experience in immersive virtual environments. Int J Virtual \\nReal 2016;16:33–48. \\n18. Cohen J. Statistical Power Analysis for the Behavioral Sciences. Routledge; 2013. \\n19. Kojić T, Spang R, Vergari M, et al. Effects of user factors on user experience in virtual \\nreality: age, gender, and VR experience as influencing factors for VR exergames. Quality \\nand User Experience 2023;8:3. \\n20. Stanney K, Fidopiastis C, Foster L. Virtual Reality Is Sexist: But It Does Not Have to Be. \\nFront Robot AI 2020;7:4. \\n21. Lorenz M, Brade J, Klimant P, et al. Age and gender effects on presence, user \\nexperience and usability in virtual environments-first insights. PLoS One 2023;18:e0283565. \\n22. Anon. Average global mobile and fixed broadband download & upload speed worldwide \\n2023. Statista. Available at: https://www.statista.com/statistics/896779/average-mobile-fixed-\\nbroadband-download-upload-speeds/ [Accessed November 17, 2023]. \\n23. Anon. Sub-Saharan Africa: average download speed by country 2022. Statista. Available \\nat: https://www.statista.com/statistics/1274951/average-download-speed-in-sub-saharan-\\nafrica-by-country/ [Accessed November 17, 2023]. \\n24. Anon. HapticVR. Fundamental Surgery 2022. Available at: \\nhttps://fundamentalsurgery.com/platform/hapticvr/ [Accessed November 14, 2023]. \\n25. Cronbach LJ, Meehl PE. Construct validity in psychological tests. Psychol Bull \\n1955;52:281–302. \\n26. Gavazzi A, Bahsoun AN, Van Haute W, et al. Face, content and construct validity of a \\nvirtual reality simulator for robotic surgery (SEP Robot). Ann R Coll Surg Engl 2011;93:152–\\n156. \\n27. Erfani M, El-Nasr MS, Milam D, et al. The Effect of Age, Gender, and Previous Gaming \\nExperience on Game Play Performance. In: Human-Computer Interaction. Springer Berlin \\nHeidelberg; 2010:293–296. \\n28. Shen C, Ratan R, Cai YD, Leavitt A. Do Men Advance Faster Than Women? Debunking \\nthe Gender Performance Gap in Two Massively Multiplayer Online Games. J Comput Mediat \\nCommun 2016;21:312–329. Available at: [Accessed November 14, 2023]. \\n \\n \\n17 \\n29. Nelson JA. Are women really more risk‐averse than men? A re‐analysis of the literature \\nusing expanded methods. J Econ Surv 2015;29:566–585. \\n30. Vergmann AS, Vestergaard AH, Grauslund J. Virtual vitreoretinal surgery: validation of a \\ntraining programme. Acta Ophthalmol 2017;95:60–65. \\n31. Solverson DJ, Mazzoli RA, Raymond WR, et al. Virtual reality simulation in acquiring and \\ndifferentiating basic ophthalmic microsurgical skills. Simul Healthc 2009;4:98–103. \\n32. Waisberg E, Ong J, Masalkhi M, et al. The future of ophthalmology and vision science \\nwith the Apple Vision Pro. Eye 2023. Available at: http://dx.doi.org/10.1038/s41433-023-\\n02688-5. \\n \\n \\n \\n18 \\nStatements \\nContributions: FA, CD, BO and KH conceptualised the study and designed the \\nexperiments. FA and KH obtained the funding. CD and BO designed RetinaVR software. FA \\nand KH provided continuous iterative feedback to improve RetinaVR. FA, CD, and DM \\ncarried out the clinical validation study. CEG performed the statistical analyses. FA and CD \\ndrafted the initial manuscript. FA and CD designed the figures and tables. All authors \\nreviewed and discussed the results. All authors edited and revised the manuscript before \\napproving the final version of this manuscript. \\n  \\nAcknowledgements: We express our gratitude to the Canadian Ophthalmological Society \\nand Bayer Inc for funding this work. \\n  \\nData sharing statement: All data produced in the present study are available upon \\nreasonable request to the authors. RetinaVR is not currently in the Oculus store. \\n \\n \\n19 \\nSupplemental Materials \\n \\n \\nSupplemental Figure 2. Overview of the simulation ergonomics. A. At the beginning of each simulation, \\nusers could calibrate the position of their instruments to a comfortable position while resting the ulnar border of \\ntheir hands on a silicon mannequin forehead. B. Top-down view from the user\\'s perspective. The controllers are \\nheld like surgical instruments. C. The right grip button (Axis1D.PrimaryHandTrigger) serves two primary \\nfunctions: firstly, it enables membrane grasping during membrane peeling, and secondly, it engages the laser \\nduring endolaser application. Continuous pressing activates repeat laser firing at 200-millisecond intervals. \\n \\n \\n20 \\n \\nRun \\n1 \\n2 \\n3 \\nOverall \\n \\nExpert \\nNovice \\nExpert \\nNovice \\nExpert \\nNovice \\nExpert \\nNovice \\n Efficiency [completion time in seconds] \\n   Mean \\n58.59 \\n70.41 \\n43.59 \\n62.57 \\n47.4 \\n50.9 \\n49.86 \\n61.3 \\n   StDev \\n17.52 \\n41.12 \\n6.35 \\n34.7 \\n12.83 \\n15.22 \\n14.17 \\n32.2 \\n    Min \\n38.45 \\n41.48 \\n36.98 \\n38.37 \\n37.56 \\n33.65 \\n36.98 \\n33.65 \\n    Max \\n98.86 \\n175.74 \\n58.25 \\n148.33 \\n75.69 \\n76.73 \\n98.86 \\n175.74 \\n Safety [number of iatrogenic retinal touches] \\n   Mean \\n0.6 \\n1.4 \\n0.4 \\n0.9 \\n0.3 \\n0.9 \\n0.43 \\n1.07 \\n   StDev \\n1.58 \\n2.46 \\n0.7 \\n1.85 \\n0.67 \\n1.52 \\n1.04 \\n1.93 \\n   Min \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n   Max \\n5 \\n8 \\n2 \\n6 \\n2 \\n5 \\n5 \\n8 \\n Performance [number of exits from the target sphere] \\n   Mean \\n10.6 \\n35 \\n7 \\n16.6 \\n9.5 \\n14.4 \\n9.03 \\n22 \\n   StDev \\n5.6 \\n39.98 \\n5.4 \\n11 \\n8.15 \\n8.32 \\n6.46 \\n25.36 \\n   Min \\n2 \\n8 \\n2 \\n6 \\n3 \\n5 \\n2 \\n5 \\n   Max \\n21 \\n109 \\n17 \\n38 \\n31 \\n35 \\n31 \\n109 \\n \\nSupplemental Table 2. Descriptive statistics for Navigation Training \\n \\n \\n \\n21 \\nRun \\n1 \\n2 \\n3 \\nOverall \\n \\nExpert \\nNovice \\nExpert \\nNovice \\nExpert \\nNovice \\nExpert \\nNovice \\n Efficiency [completion time in seconds] \\n   Mean \\n55.74 \\n58.57 \\n38.56 \\n42.61 \\n33.22 \\n33.02 \\n42.5 \\n44.73 \\n   StDev \\n20.97 \\n13.6 \\n11.63 \\n11.58 \\n7.77 \\n5.4 \\n17.11 \\n14.93 \\n    Min \\n29.49 \\n36.88 \\n25.99 \\n24.64 \\n23.15 \\n22.22 \\n23.15 \\n22.22 \\n    Max \\n90.65 \\n77.27 \\n61.8 \\n62.62 \\n43.94 \\n39.19 \\n90.65 \\n77.27 \\n Safety [number of iatrogenic retinal touches] \\n   Mean \\n1.1 \\n1.6 \\n1.1 \\n2.3 \\n0.3 \\n0.5 \\n0.83 \\n1.47 \\n   StDev \\n1.45 \\n2.91 \\n1.85 \\n4.69 \\n0.48 \\n0.97 \\n1.39 \\n3.21 \\n   Min \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n   Max \\n4 \\n9 \\n6 \\n13 \\n1 \\n3 \\n6 \\n13 \\n Performance [number of exits from the target sphere] \\n   Mean \\n168.9 \\n222.7 \\n183.1 \\n186 \\n158.6 \\n165 \\n170.2 \\n191.23 \\n   StDev \\n47.25 \\n62.43 \\n66.33 \\n86.27 \\n48.13 \\n64.3 \\n53.68 \\n73.42 \\n   Min \\n86 \\n147 \\n115 \\n97 \\n110 \\n94 \\n86 \\n94 \\n   Max \\n232 \\n336 \\n305 \\n396 \\n230 \\n288 \\n305 \\n396 \\n Performance [mean Δ from the shaving path in mm] \\n   Mean \\n19.7 \\n16.8 \\n25.2 \\n16.0 \\n21.3 \\n15.2 \\n22.0 \\n16.0 \\n   StDev \\n6.6 \\n1.2 \\n28.4 \\n1.1 \\n19.8 \\n0.6 \\n19.8 \\n1.1 \\n   Min \\n14.3 \\n15.0 \\n13.9 \\n14.1 \\n13.4 \\n13.9 \\n13.4 \\n13.9 \\n   Max \\n34.9 \\n18.5 \\n105.2 \\n17.8 \\n77.6 \\n15.8 \\n105.2 \\n18.5 \\n Performance [maximal Δ from the shaving path in mm] \\n   Mean \\n98.3 \\n39.8 \\n110.5 \\n44.1 \\n64.1 \\n30.3 \\n91.0 \\n38.1 \\n   StDev \\n94.4 \\n18.0 \\n209.9 \\n26.8 \\n104.8 \\n8.6 \\n142.3 \\n19.5 \\n   Min \\n18.0 \\n19.9 \\n18.6 \\n22.6 \\n17.4 \\n21.8 \\n17.4 \\n19.9 \\n   Max \\n305.6 \\n76.1 \\n692.0 \\n106.3 \\n357.4 \\n52.9 \\n692.0 \\n106.3 \\n \\nSupplemental Table 3. Descriptive statistics for Tremor Control \\n \\n \\n \\n22 \\nRun \\n1 \\n2 \\n3 \\nOverall \\n \\nExpert \\nNovice \\nExpert \\nNovice \\nExpert \\nNovice \\nExpert \\nNovice \\n Efficiency [completion time in seconds] \\n   Mean \\n97.92 \\n65.49 \\n56.86 \\n50.58 \\n51.58 \\n40.15 \\n68.79 \\n52.07 \\n   StDev \\n30.43 \\n28.49 \\n22.15 \\n24.97 \\n17.52 \\n23.37 \\n31.28 \\n26.96 \\n    Min \\n43.23 \\n43.75 \\n30.88 \\n25.02 \\n24.64 \\n21.95 \\n24.64 \\n21.95 \\n    Max \\n132.33 \\n135.04 \\n90.51 \\n106.08 \\n82.8 \\n100.66 \\n132.33 \\n135.04 \\n Safety [number of iatrogenic retinal touches] \\n   Mean \\n1.8 \\n6.3 \\n0.2 \\n4.9 \\n0.3 \\n2.3 \\n0.77 \\n4.5 \\n   StDev \\n2.53 \\n4 \\n0.63 \\n5.92 \\n0.95 \\n3.13 \\n1.72 \\n4.66 \\n   Min \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n   Max \\n6 \\n11 \\n2 \\n20 \\n3 \\n10 \\n6 \\n20 \\n Performance [number of membrane grasps] \\n   Mean \\n8.9 \\n7.6 \\n8.7 \\n8.7 \\n9.3 \\n8.3 \\n8.97 \\n8.2 \\n   StDev \\n2.33 \\n1.51 \\n1.89 \\n1.49 \\n1.77 \\n1.49 \\n1.96 \\n1.52 \\n   Min \\n5 \\n6 \\n6 \\n7 \\n7 \\n6 \\n5 \\n6 \\n   Max \\n14 \\n11 \\n12 \\n12 \\n13 \\n11 \\n14 \\n12 \\n \\nSupplemental Table 4. Descriptive statistics for Peeling Control \\n \\n \\n \\n23 \\nRun \\n1 \\n2 \\n3 \\nOverall \\n \\nExpert \\nNovice \\nExpert \\nNovice \\nExpert \\nNovice \\nExpert \\nNovice \\n Efficiency [completion time in seconds] \\n   Mean \\n140.54 \\n125.87 \\n85.7 \\n109.53 \\n81.61 \\n82.08 \\n102.62 \\n105.82 \\n   StDev \\n39.64 \\n36.2 \\n15.21 \\n58.42 \\n25.56 \\n13.32 \\n38.85 \\n43.11 \\n    Min \\n71.8 \\n76.73 \\n55.57 \\n40.6 \\n47 \\n62.57 \\n47 \\n40.6 \\n    Max \\n192.61 \\n201.48 \\n112.71 \\n255.54 \\n134.98 \\n106.5 \\n192.61 \\n255.54 \\n Safety [number of iatrogenic retinal touches] \\n   Mean \\n0.2 \\n1 \\n0 \\n1.1 \\n1 \\n0.1 \\n0.4 \\n0.73 \\n   StDev \\n0.63 \\n1.89 \\n0 \\n1.85 \\n2.54 \\n0.32 \\n1.52 \\n1.55 \\n   Min \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n   Max \\n2 \\n6 \\n0 \\n5 \\n8 \\n1 \\n8 \\n6 \\n Performance [number of laser spots] \\n   Mean \\n173.7 \\n162.2 \\n149.4 \\n165.5 \\n151.3 \\n139.8 \\n158.13 \\n155.83 \\n   StDev \\n52.32 \\n21.62 \\n39.06 \\n47.11 \\n52.75 \\n27.56 \\n48.09 \\n34.7 \\n   Min \\n121 \\n129 \\n99 \\n93 \\n98 \\n98 \\n98 \\n93 \\n   Max \\n282 \\n194 \\n232 \\n243 \\n250 \\n183 \\n282 \\n243 \\n \\nSupplemental Table 5. Descriptive statistics for Laser Precision \\n \\n \\n \\n24 \\nSupplemental Table 6. Effect size analysis for all four RetinaVR modules \\n \\n Task/ Run \\n1 \\n2 \\n3 \\nCombined \\n Navigation Training \\n   Efficiency \\n0.37 [-0.52, 1.25] \\n0.76 [-0.16, 1.66] \\n0.25 [-0.63, 1.13] \\n0.46 [-0.06, 0.97] \\n   Safety \\n0.39 [-0.50, 1.27] \\n0.36 [-0.53, 1.24] \\n0.51 [-0.39, 1.39] \\n0.41 [-0.10, 0.92] \\n   Sphere Exits  \\n0.85 [-0.08, 1.76] \\n1.11 [0.15, 2.04] \\n0.59 [-0.31, 1.48] \\n0.7 [0.18, 1.22] \\n Tremor Control \\n   Efficiency \\n0.16 [-0.72, 1.04] \\n0.35 [-0.54, 1.23] \\n-0.03 [-0.91, 0.85] \\n0.14 [-0.37, 0.64] \\n   Safety \\n0.22 [-0.67, 1.09] \\n0.34 [-0.55, 1.22] \\n0.26 [-0.62, 1.14] \\n0.26 [-0.25, 0.76] \\n   Sphere Exits  \\n0.97 [0.03, 1.89] \\n0.04 [-0.84, 0.91] \\n0.11 [-0.77, 0.99] \\n0.33 [-0.18, 0.84] \\n   Mean Δ \\n-0.61 [-1.50, 0.30] \\n-0.46 [-1.34, 0.43] \\n-0.43 [-1.32, 0.46] \\n-0.43 [-0.94, 0.08] \\n   Max Δ \\n-0.86 [-1.77, 0.07] \\n-0.44 [-1.33, 0.45] \\n-0.45 [-1.34, 0.44] \\n-0.52 [-1.03, 0.00] \\n Peeling Control \\n   Efficiency \\n-1.10 [-2.03, -0.14] \\n-0.27 [-1.14, 0.62] \\n-0.55 [-1.44, 0.35] \\n-0.57 [-1.09, 0.05] \\n   Safety \\n1.34 [0.35, 2.31] \\n1.12 [0.16, 2.05] \\n0.87 [-0.07, 1.77] \\n1.06 [0.52, 1.60] \\n   Grasps  \\n-0.66 [-1.56, 0.25] \\n0 [-0.88, 0.88] \\n-0.61 [-1.50, 0.30] \\n-0.44 [-0.95, 0.08] \\n Laser Precision \\n   Efficiency \\n-0.39 [-1.27, 0.50] \\n0.56 [-0.34, 1.45] \\n0.02 [-0.85, 0.90] \\n0.08 [-0.43, 0.58] \\n   Safety \\n0.57 [-0.33, 1.46] \\n0.84 [-0.09, 1.75] \\n-0.50 [-1.38, 0.40] \\n0.22 [-0.29, 0.72] \\n   Laser Spots  \\n-0.29 [-1.16, 0.60] \\n0.37 [-0.52, 1.25] \\n-0.27 [-1.15, 0.61] \\n-0.05 [-0.56, 0.45] \\n \\n \\n25 \\nTheme \\nExpertise \\nMean \\nStDev \\nMin \\nMax \\nPresence \\n \\nExpert \\n6 \\n2.01 \\n1.67 \\n8.33 \\nNovice \\n7 \\n1.39 \\n5.33 \\n9.67 \\nEngagement \\nExpert \\n7 \\n1.44 \\n5.00 \\n8.50 \\nNovice \\n8 \\n1.51 \\n5.50 \\n10.00 \\nImmersion \\nExpert \\n7 \\n2.08 \\n3.50 \\n9.00 \\nNovice \\n8 \\n1.57 \\n6.00 \\n10.00 \\nFlow \\nExpert \\n7 \\n1.66 \\n4.50 \\n9.50 \\nNovice \\n8 \\n1.38 \\n6.00 \\n10.00 \\nEmotion \\nExpert \\n8 \\n1.53 \\n5.50 \\n10.00 \\nNovice \\n8 \\n0.97 \\n7.00 \\n9.50 \\nSkill \\nExpert \\n7 \\n2.24 \\n3.00 \\n9.00 \\nNovice \\n8 \\n1.42 \\n5.50 \\n9.00 \\nJudgement \\nExpert \\n8 \\n1.62 \\n4.33 \\n10.00 \\nNovice \\n9 \\n1.05 \\n6.67 \\n10.00 \\nExperience Consequence \\nExpert \\n8 \\n1.96 \\n4.00 \\n10.00 \\nNovice \\n8 \\n2.49 \\n2.50 \\n10.00 \\nTechnology Adoption \\nExpert \\n9 \\n1.71 \\n4.67 \\n10.00 \\nNovice \\n9 \\n0.99 \\n7.00 \\n10.00 \\n \\nSupplemental Table 10. Results from the User Experience questionnaire \\n \\n \\n \\n26 \\nTheme \\nQuestion \\nBackground \\nApproximately how many hours of experience do you have with virtual reality environments? \\nBackground \\nApproximately how many hours of experience do you have with medical simulation software? \\nPresence \\nMy interactions with the virtual environment seemed natural. \\nPresence \\nI could concentrate on the assigned tasks rather than on the devices (gamepad or keyboard). \\nPresence \\nThe devices (gamepad or keyboard) which controlled my movement in the virtual \\nenvironment seemed natural. \\nEngagement \\nThe sense of moving around inside the virtual environment was compelling. \\nEngagement \\nThe visual aspects of the virtual environment involved me. \\nImmersion \\nI become so involved in the virtual environment that it is if I was inside the game rather than \\nmanipulating a gamepad and watching a screen. \\nImmersion \\nI felt physically fit in the virtual environment \\nFlow \\nI felt I could perfectly control my actions. \\nFlow \\nAt each step, I knew what to do. \\nEmotion \\nI enjoyed being in this virtual environment. \\nEmotion \\nThe interaction devices (Oculus headset, gamepad and/or keyboard) bored me to death. \\nSkill \\nI felt confident using the gamepad and/or keyboard to move around the virtual environment. \\nSkill \\nI feel confident learning advanced skills within a specific virtual reality software using the \\nOculus headset. \\nJudgement \\nPersonally, I would say the virtual environment is impractical/practical. \\nJudgement \\nPersonally, I would say the virtual environment is unruly/manageable. \\nJudgement \\nPersonally, I would say the virtual environment is confusing/clear \\nExperience \\nconsequence \\nI suffered from fatigue during my interaction with the virtual environment. \\nExperience \\nconsequence \\nI suffered from dizziness with eye open during my interaction with the virtual environment \\nTechnology \\nadoption \\nIt would be easy for me to become skillful at using the virtual environment. \\nTechnology \\nadoption \\nIf I use again the same virtual environment, my interaction with the environment would be \\nclear and understandable for me. \\nTechnology \\nadoption \\nUsing the interaction devices (Oculus headset, gamepad and/or keyboard) is a bad idea. \\nFeedback \\nIn your opinion, what were the positive points about your experience? \\nFeedback \\nIn your opinion, what were the negative points about your experience? \\nFeedback \\nDo you have suggestions to improve this virtual reality environment? \\n \\nAppendix 1. User experience questionnaire abridged from the validated IVEQ v2 user experience \\nquestionnaire. The original questionnaire is in French, but the English translation is shown here. Background \\nand feedback questions were open-ended, allowing for natural language inputs. The remaining questions used a \\n10-point Likert scale. Scores from negatively worded questions were inverted for analysis. \\n \\n'},\n",
       " {'abstract': \"This paper presents a study on enhancing the performance of the GPT Neo 125M model in the programming domain for Community Question Answering (CQA) using a combination of Reinforcement Learning from Human Feedback (RLHF) and scores from Stack Overflow. Two reward model training strategies are employed for fine-tuning with Proximal Policy Optimization (PPO), resulting in improvements comparable to those of GPT Neo's 2.7B parameter variant. An auxiliary scoring mechanism is introduced to highlight the limitations of conventional linguistic metrics in evaluating responses in the programming domain. Through analysis, the study emphasizes the need for domain-specific evaluation methods and reflects on the intricate nature of applying RLHF to programming CQA, accentuates the significance of context-aware evaluation, and underscores the importance of human preferences in refining LLMs.\",\n",
       "  'introduction': \"Advancements in Reinforcement Learning from Human Feedback (RLHF) have revolutionized the fine-tuning of Large Language Models (LLMs), enabling adaptation for human-like response generation and precise behavior control. While RLHF has been effective in general domains, its application in specialized fields such as Community Question Answering (CQA) for programming remains unexplored. LLMs face unique challenges in handling the complex nature of programming queries, including conceptual understanding, code generation, API usage, and debugging, due to struggles with subtle semantic relations. Additionally, a critical challenge is the evaluation of the quality of responses generated by LLMs. Conventional metrics such as BertScore and Rouge do not capture the essence of responses effectively, especially in specialized domains like programming. Moreover, they don't account for the diversity in valid answers and lack in capturing deeper semantic correctness. The development of more reliable and context-sensitive evaluation metrics is essential.\",\n",
       "  'literature_review': \"To address these challenges, in this paper, we investigate the application of RLHF to a smaller model, GPT Neo 125M (Black et al., 2021a), in the context of programming CQA. We aim not only to enhance the model's response generation capabilities but also to address the evaluation challenge. Our contributions are two-fold. First, we explore the potential and efficacy of RLHF in retraining a smaller LLM for programming CQA. Second, through empirical analysis, we highlight the discrepancies between the RLHF reward model and existing linguistic metrics, emphasizing the limitations of current evaluation methodologies and advocating for the development of more semantically-sensitive measures.\",\n",
       "  'methodology': 'The general schema of RLHF utilized in this study consists of several stages, as depicted in Fig. 1. The process commences with the training of an initial policy via supervised learning. Subsequently, a reward model is trained to acquire human preferences from the labeled data. Finally, the policy is fine-tuned using Proximal Policy Optimization (PPO) (Schulman et al., 2017), guided by the reward model. In this study, we have adapted RLHF for programming Q&A by converting user ratings from Stack Overflow into feedback for training our model. We used two distinct approaches: creating regression scores and contrastive scores for the straightforward comparison of answers.',\n",
       "  'results': \"This section aims to evaluate the effectiveness of the RLHF training approach for improving the quality of generated responses in the programming QA domain. Specifically, we compare three versions of the model, according to Fig. 1 - the base model, the SFT version, and the RLHF version. The evaluation focuses on the performance of the reward model training methods and the generated responses' quality.\",\n",
       "  'conclusion': 'In conclusion, our study has demonstrated the effectiveness of RLHF in enhancing the performance of small LLMs like GPT Neo 125M in the programming domain. Our experiments focused on fine-tuning the model using user-generated responses from Stack Overflow, employing two reward model training strategies, regression scores and contrastive scores, with PPO. The study also highlights the critical role of employing the right evaluation measures. While Rouge scores effectively captured response accuracy, other metrics like BertScore and SacreBLEU presented ambiguities, especially when juxtaposed with the results from the trained reward models. This disparity, brought into sharper focus by near-zero Spearman correlations, implies that traditional metrics might not suffice for complex fields such as programming. These domains are marked by intricate semantic relationships and a broad spectrum of valid answers.',\n",
       "  'title': 'Reinforcement learning for question answering in programming domain using public community scoring as a human feedback',\n",
       "  'author': 'Alexey Gorbatovski, Sergey Kovalchuk',\n",
       "  'textdata': 'Reinforcement learning for question answering in programming domain\\nusing public community scoring as a human feedback\\nAlexey Gorbatovski\\nITMO University\\nSaint Petersburg, Russia\\ngorbatoski@itmo.ru\\nSergey Kovalchuk\\nHuawei\\nSaint Petersburg, Russia\\nsergey.kovalchuk@huawei.com\\nAbstract\\nIn this study, we investigate the enhancement of\\nthe GPT Neo 125M’s performance in Commu-\\nnity Question Answering (CQA) with a focus\\non programming, through the integration of Re-\\ninforcement Learning from Human Feedback\\n(RLHF) and the utilization of scores from Stack\\nOverflow. Two distinct reward model training\\nstrategies are employed for fine-tuning with\\nProximal Policy Optimization (PPO). Notably,\\nthe improvements in performance achieved\\nthrough this method are comparable to those of\\nGPT Neo’s 2.7B parameter variant. Addition-\\nally, an auxiliary scoring mechanism is intro-\\nduced, which demonstrates the limitations of\\nconventional linguistic metrics in evaluating re-\\nsponses in the programming domain. Through\\naccurate analysis, this paper looks at the di-\\nvergence between traditional linguistic met-\\nrics and our human-preferences-based reward\\nmodel, underscoring the imperative for domain-\\nspecific evaluation methods. By elucidating\\nthe complexities involved in applying RLHF to\\nprogramming CQA and accentuating the signif-\\nicance of context-aware evaluation, this study\\ncontributes to the ongoing efforts in refining\\nLarge Language Models through focused hu-\\nman feedback.\\n1\\nIntroduction\\nAdvances in Reinforcement Learning from Hu-\\nman Feedback (RLHF) have revolutionized the\\nfine-tuning of Large Language Models (LLMs),\\nfacilitating adaptation for human-like response gen-\\neration and precise behavior control (Ouyang et al.,\\n2022). While RLHF has proven effective in gen-\\neral domains, its application in specialized fields\\nsuch as Community Question Answering (CQA)\\nfor programming remains unexplored (Beeching\\net al., 2023). LLMs face unique challenges in han-\\ndling the complex nature of programming queries,\\nincluding conceptual understanding, code genera-\\ntion, API usage, and debugging, due to struggles\\nwith subtle semantic relations.\\nFurthermore, a critical challenge is the evalua-\\ntion of the quality of responses generated by LLMs.\\nConventional metrics such as BertScore and Rouge\\ndo not capture the essence of responses effectively,\\nespecially in specialized domains like program-\\nming (Wang et al., 2019). Moreover, they don’t ac-\\ncount for the diversity in valid answers and lack in\\ncapturing deeper semantic correctness. The devel-\\nopment of more reliable and context-sensitive eval-\\nuation metrics is essential(Kovalchuk et al., 2022).\\nTo address these challenges, in this paper, we\\ninvestigate the application of RLHF to a smaller\\nmodel, GPT Neo 125M (Black et al., 2021a), in the\\ncontext of programming CQA. We aim not only to\\nenhance the model’s response generation capabil-\\nities but also to address the evaluation challenge.\\nOur contributions are two-fold. First, we explore\\nthe potential and efficacy of RLHF in retraining\\na smaller LLM for programming CQA. Second,\\nthrough empirical analysis, we highlight the dis-\\ncrepancies between the RLHF reward model and\\nexisting linguistic metrics, emphasizing the limita-\\ntions of current evaluation methodologies and ad-\\nvocating for the development of more semantically-\\nsensitive measures.\\nThe structure of the paper is as follows: Section\\n2 provides background information and describes\\nthe datasets used in this study. In Section 3, we\\ndelve into the application of RLHF for program-\\nming CQA, explaining the data preparing method-\\nologies employed. Section 4 focuses on the experi-\\nmental evaluation and results. Section 5 presents a\\ndiscussion of the study results and evaluation meth-\\nods. Finally, Section 6 concludes the paper with\\nfinal remarks and reflections on our findings.\\n2\\nBackground and Dataset\\n2.1\\nBackground on RLHF and LLMs\\nReinforcement Learning from Human Feedback\\n(RLHF) is a technique where models are trained us-\\narXiv:2401.10882v1  [cs.CL]  19 Jan 2024\\ning human feedback as rewards. This method has\\nbecome notably beneficial in refining the perfor-\\nmance and behavior control of Large Language\\nModels (LLMs).\\nRLHF initiates with models\\ntrained using supervised fine-tuning (SFT), which\\nare then iteratively improved. Crucially, the hu-\\nman scoring process in RLHF is often automated\\nby training a separate reward model, serving as an\\noptimization proxy. This process varies in imple-\\nmentation and warrants further exploration.\\nThe application of RLHF in LLMs has been ex-\\nplored in various contexts. For instance, Ziegler\\net al. (2019) studied the impact of reward learn-\\ning on specific tasks, demonstrating the potential\\nof RLHF in enhancing the performance of LLMs.\\nThe work of Stiennon et al. (2020) and the Ope-\\nnAI Alignment Team in 2021 further expanded the\\nscope of RLHF, applying it to the task of summa-\\nrizing text and books, respectively.\\nIn the context of Question Answering (QA),\\nRLHF has been used to train models to navigate\\nthe web (Nakano et al., 2021) and to follow instruc-\\ntions (Ouyang et al., 2022). However, these studies\\nhave mainly focused on general domains or specific\\ntasks, and the application of RLHF in specialized\\nfields such as programming Community Question\\nAnswering (CQA) remains largely unexplored.\\n2.2\\nDataset Selection and Preprocessing\\nIn the study, we used Stack Overflow1 (SO) as\\nthe primary data source for programming-related\\nquestion-answering tasks. We regarded the answers\\non SO as reference solutions. We compiled the\\ndataset from the original data available on Stack Ex-\\nchange2, focusing on questions specifically tagged\\nwith ’python’. This dataset, which includes titles,\\nquestions, answers per question, and user scores\\nfor each, was used for both supervised fine-tuning\\nand partial reward model training. To ensure the\\ndataset’s relevance and homogeneity, we subjected\\nit to a series of constraints and transformations.\\nFurthermore, we adjusted user ratings for different\\nreward model training setups.\\nWe applied several constraints to refine the\\ndataset and maintain consistency:\\n• We only selected questions classified as “API\\nUsage” according to the taxonomy by Beyer\\net al. (2020). This selection was performed\\n1https://stackoverflow.com/\\n2https://stackexchange.com/\\nusing regular expressions to ensure alignment\\nwith the study’s focus.\\n• To maintain text purity and the ability to gen-\\nerate a response based on the context only, we\\nfiltered out questions and answers containing\\nimages, links, or code blocks, designated by\\nthe <pre><code> HTML tags. Code blocks\\nwere filtered out to prevent the model from\\ngenerating code snippets during training, as\\nthis would significantly complicate the eval-\\nuation process due to the lack of established\\nmetrics for assessing the quality of generated\\ncode.\\n• HTML content was sanitized and converted\\nto plain text using the Beautiful Soup3 library\\nto prepare it for natural language processing.\\nWe obtained a dataset of 6,946 training entries\\nand 1,000 validation entries. To prevent data leak-\\nage and ensure temporal relevance, the validation\\nset included questions posted after December 14,\\n2021.\\nWhile this dataset is highly relevant for studying\\nRLHF in programming CQA, it is worth noting\\nthat the constraints applied may introduce certain\\nlimitations in terms of the diversity of questions\\nand real-world applicability.\\n3\\nRLHF for programming Q&A\\nThe general schema of RLHF utilized in this study\\nconsists of several stages, as depicted in Fig. 1.\\nThe process commences with the training of an ini-\\ntial policy via supervised learning. Subsequently,\\na reward model is trained to acquire human pref-\\nerences from the labeled data. Finally, the policy\\nis fine-tuned using Proximal Policy Optimization\\n(PPO) (Schulman et al., 2017), guided by the re-\\nward model.\\nIn this study, we have adapted RLHF for pro-\\ngramming Q&A by converting user ratings from\\nStack Overflow into feedback for training our\\nmodel. We used two distinct approaches: creat-\\ning regression scores and contrastive scores for the\\nstraightforward comparison of answers. Addition-\\nally, to enhance the logical alignment of sentences\\nand mitigate the model’s errors in generation, we\\ncompleted the dataset for reward model training\\nwith generations from the SFT model.\\n3https://github.com/wention/BeautifulSoup4\\nFigure 1: The general schema of Reinforcement Learning from Human Feedback for programming Q&A\\n3.1\\nTransformation of User Ratings\\nTo account for biases arising from factors like the\\nquestion’s age and popularity, we preprocess and\\nnormalize user ratings. Our approach comprises\\ntwo distinct transformations: Regression Scores\\nand Contrastive Scores.\\nAlgorithm 1 Regression scores transforming\\nInput: user votes for each answer N_votesij\\nOutput: regression scores sij\\n1: for each question qi do\\n2:\\nfor each answer aj in qi do\\n3:\\nsij =\\nN_votesij\\nN_answersi\\n4:\\nend for\\n5: end for\\n6: l_bound, u_bound = 1.5 × IQR(sij)\\n7: for each score sij do\\n8:\\nif sij outside the range [l_bound, u_bound]\\nthen\\n9:\\nsij = clip(sij, −1, 1)\\n10:\\nelse\\n11:\\nsij = max_abs_scale(sij, sign(sij))\\n12:\\nend if\\n13: end for\\n14: return sij for all i, j\\n3.1.1\\nRegression Scores\\nFor regression scores, user ratings were normalized\\nby the total number of answers for each question.\\nAfter clipping outliers, the ratings were scaled to\\ncontrol the standard deviation, thus stabilizing the\\nregression training. The process is outlined in Al-\\ngorithm 1. There vij represents the votes for each\\nanswer aj in question qi and sij for the regression\\nscores.\\n3.1.2\\nContrastive Scores\\nContrastive scores allow a convenient comparison\\nof answer ratings by mapping them to logarithmi-\\ncally scaled scores (Askell et al., 2021). Accepted\\nanswers receive an additional increment, while neg-\\native ratings are assigned a standard value. The\\nfollowing Algorithm 2 details this process:\\nIn this algorithm, vj denotes the votes for each\\nanswer, and the contrastive scores, sj, are com-\\nputed using a logarithmic scale.\\nAdditionally, we first identified questions with\\nmore than one answer after preprocessing and fil-\\ntering, which amounted to 3,076. We compared\\neach of these answers with the highest-voted an-\\nswer for each question during the training of the\\nreward model. After this comparison, the contrast\\ndataset contained 1,804 rows.\\n3.2\\nData Generation for Reward Model\\nTraining\\nWe generated 6,872 additional answers for ques-\\ntions with only one answer to create a comparison\\nset essential for training the reward model. This\\nstep was undertaken to ensure a diverse dataset that\\nsimulates various answer qualities.\\nFor the regression approach, we assigned\\nAlgorithm 2 Contrastive scores scaling\\nInput: votes for each answer vj\\nOutput: contrastive scores sij\\n1: for each answer aj do\\n2:\\nif vj < 0 then\\n3:\\nsj = −1\\n4:\\nelse\\n5:\\nsj = ⌈log2(1 + vj)⌉\\n6:\\nif aj is accepted then\\n7:\\nsj = sj + 1\\n8:\\nend if\\n9:\\nend if\\n10: end for\\n11: for each question qi do\\n12:\\nif N_answersi > 1 then\\n13:\\nsmax = max(sij) for all j\\n14:\\nfor each answer ak in qi do\\n15:\\ncompare score sk with smax\\n16:\\nend for\\n17:\\nend if\\n18: end for\\n19: return compared pairs {(aj, ak)}\\nthese generated answers a normal distribution\\nN(−0.5, 0.12). This was based on the observation\\nthat most generated answers were either completely\\nuninformative or erroneous. We believe that dis-\\ncouraging the generation of nonsensical answers is\\na helpful practice.\\nIn the contrastive approach, these generated an-\\nswers were incorporated into the contrast dataset,\\nwhich previously only included questions with\\nmore than one existing answer.\\nThe generation of this additional data was cru-\\ncial for robustly training the reward model. In the\\nexperimental section, we will delve into how this\\ndataset was leveraged to train the reward model ef-\\nfectively, and the evaluation metrics used to assess\\nits performance.\\n4\\nExperimental evaluation\\nThis section aims to evaluate the effectiveness of\\nthe RLHF training approach for improving the\\nquality of generated responses in the programming\\nQA domain. Specifically, we compare three ver-\\nsions of the model, according to Fig. 1 - the base\\nmodel, the SFT version, and the RLHF version.\\nThe evaluation focuses on the performance of the\\nreward model training methods and the generated\\nresponses’ quality.\\n4.1\\nEvaluation approach\\nFig. 2 illustrates the evaluation schema. For each\\nquestion in the validation dataset, the model gener-\\nates ten responses using sampling-based decoding.\\nThis approach allows us to study the average qual-\\nity of the generated responses without bias toward\\nthe worst or best cases. The parameters used for\\nsampling-based decoding are as follows:\\n• do_sample: true\\n• no_repeat_ngram_size: 2\\n• top_k: 50\\n• top_p: 0.9\\nTo evaluate the responses’ content and seman-\\ntic similarity, we employ the SacreBLEU, Rouge,\\nand BertScore metrics as common metrics for nat-\\nural language generation tasks. Additionally, the\\nreward models rate each generated response as an\\nalternative quality assessment tool.\\nFor a more insightful evaluation, we also con-\\nduct a human-based assessment. A subset of 100\\nrandomly selected questions is manually evaluated\\nby ourselves. Each generated answer for these\\nquestions is inspected and marked as useful (1) or\\nnot useful (0) for solving the problem stated in the\\nquestion. This binary labeling enables the compu-\\ntation of the Mean Reciprocal Rank (MRR), which\\nassesses the relevance of the generated responses.\\nFinally, to investigate the consistency between\\nthe different metrics and reward model assess-\\nments, we employ Spearman’s rank correlation\\ncoefficient. This statistical measure will provide\\ninsight into whether the automatic metrics and the\\nreward model assessments are aligned in evaluating\\nresponse quality.\\n4.2\\nExperimental setup\\nAll experiments were conducted using the GPT\\nNeo model (Black et al., 2021b) with 125 million\\nparameters, selected based on the constraints dis-\\ncussed in the corresponding section.\\n4.2.1\\nSupervised fine-tuning\\nFine-tuning was performed on the training dataset\\ndescribed in Section 2.2. We utilized the Trans-\\nformers and PyTorch Lightning libraries with the\\nfollowing hyperparameters: optimizer = Adam,\\nAdam betas = (0.9, 0.999), Adam epsilon = 1e-\\n08, weight decay = 0.001, learning rate = 2e-05,\\nFigure 2: The general schema of evaluation approach\\nlearning rate decay scheme = linear, batch size = 12,\\nand mixed precision training (fp16). The maximum\\nlength of the concatenated question and answer was\\nset to 512 tokens. If the input sequence exceeded\\nthis length, the question was truncated, ensuring\\nthat the full answer was available for training.\\n4.2.2\\nReward Model Training\\nWe trained the reward model using two approaches:\\nregression and answer comparison. The regres-\\nsion approach employed the Mean Squared Error\\n(MSE) as the loss function, while the answer com-\\nparison approach used the Contrastive Loss (see\\nFormula 1).\\nL(θ) = −E(x,yj,yk)∼D[log(σ(rθ(x, yj)−rθ(x, yk)))] (1)\\nwhere r and y are the reward model’s score and\\ny is the preferred candidate respectively. Both ap-\\nproaches used the SFT model as the basis.\\nFor the regression approach, the hyperparame-\\nters were as follows: optimizer = Adam, Adam\\nbetas = (0.9, 0.999), Adam epsilon = 1e-08, weight\\ndecay = 0.001, learning rate = 2e-05, learning rate\\ndecay scheme = linear, batch size = 16, and mixed\\nprecision training (fp16).\\nFor the contrastive approach, we used the same\\nhyperparameters with a different learning rate (3e-\\n05) and batch size (8). Additionally, for both ap-\\nproaches, the weights of the first and last layers, as\\nwell as all linear layers of the model, were updated\\nduring training.\\nBoth approaches exhibited stability during train-\\ning, achieving validation accuracies of 93% and\\n95% respectively. For the regression approach, ac-\\ncuracy was computed using the formula:\\naccuracy = 1\\nn\\nX\\n[sign(ri) = yi]\\n(2)\\nwhere ri and yi mean ith reward and target score re-\\nspectively. Considering both positive and negative\\nrewards is essential in reinforcement learning.\\n4.2.3\\nFine-tuning with RL\\nWe employed reinforcement learning using the\\nTRL and Transformers libraries, with typical RLHF\\nparameters: optimizer = Adam, Adam betas = (0.9,\\n0.95), Adam epsilon = 1e-08, learning rate = 1.41e-\\n05, epsilon clip range = 0.2, buffer size = 64, and\\nbatch size = 16. Additionally, we used adaptive KL\\ncontrol with an initial KL coefficient of 0.2 and a\\ntarget of 6.\\nDuring reinforcement learning, the training was\\nstable, and the average reward increased when us-\\ning the reward model based on the regression ap-\\nproach. However, training was unstable and did not\\nconverge using the reward model based on answer\\ncomparisons. Tuning model starts generate repet-\\nitive words and incoherent sentences. Therefore,\\nthe results section presents the outcomes for the\\nmodel trained using the regression-based reward\\nmodel.\\n4.3\\nResults\\nThis section presents the results of the experiments,\\nwhich were conducted to assess the efficacy of the\\nRLHF training approach in the context of program-\\nming QA response generation. We examine the\\nperformance of the different models and discuss\\nthe correlation and consistency between the metrics\\nemployed for evaluating the quality of the gener-\\nated responses.\\n4.3.1\\nComparison of Average Metrics\\nOur evaluation process involved computing the av-\\nerage metrics for ten generation attempts by four\\nmodels: Base GPT Neo 125M (Base 125M), Su-\\npervised Fine-tuning GPT Neo 125M (SFT 125M),\\nRLHF GPT Neo 125M (RLHF 125M), and Base\\nSacreBLEU\\nRouge 1\\nRouge 2\\nBertScore Reg. Reward Contr. Reward\\nBase 125M\\n0.0433\\n0.1816\\n0.0233\\n0.9420\\n-0.1479\\n-1.0124\\n(σ : 0.0071)\\n(σ : 0.0684)\\n(σ : 0.0160)\\n(σ : 0.0057)\\n(σ : 0.0994)\\n(σ : 1.0214)\\nSFT 125M\\n0.0484\\n0.1903\\n0.0237\\n0.9483\\n0.1257\\n-0.0173\\n(σ : 0.0088)\\n(σ : 0.0581)\\n(σ : 0.0151)\\n(σ : 0.0097)\\n(σ : 0.0864)\\n(σ : 1.0123)\\nRLHF 125M\\n0.0489\\n0.1884\\n0.0230\\n0.9493\\n0.1869\\n0.3955\\n(σ : 0.0092)\\n(σ : 0.0545)\\n(σ : 0.0149)\\n(σ : 0.0105)\\n(σ : 0.0767)\\n(σ : 0.9720)\\nBase 2.7B\\n0.0455\\n0.1906\\n0.0275\\n0.9417\\n-0.1123\\n-0.0365\\n(σ : 0.0073)\\n(σ : 0.0735)\\n(σ : 0.0190)\\n(σ : 0.0054)\\n(σ : 0.1045)\\n(σ : 1.1245)\\nTable 1: Comparison of average metrics for different models. Each entry contains the mean of the corresponding\\nmetric across ten generation attempts.\\nGPT Neo 2.7B (Base 2.7B). These models evalu-\\nated using several metrics, including SacreBLEU,\\nRouge 1, Rouge 2 and BertScore, as well as the\\nscores obtained from the regression and contrastive\\nreward models.\\nTable 1 presents the average values of these\\nmetrics for each model. Notably, the RLHF ver-\\nsion demonstrated superior performance compared\\nto the SFT model in terms of SacreBLEU and\\nBertScore. However, the larger Base GPT Neo\\n2.7B model surpassed the other models in terms\\nof the Rouge scores. All highlighted metrics were\\ndeemed statistically significant via the KS-test. The\\ninclusion of bootstrapped confidence intervals fur-\\nther clarifies the model’s improvement relative to\\nthe baseline.\\n4.4\\nMetrics at Rank k Analysis\\nBeyond the mean values, we performed an in-\\ndepth analysis using the metric@k approach. The\\nterm “metric at rank k” refers to the highest score\\nachieved by a metric among k randomly sampled\\ngeneration attempts.\\nThis analysis helps to re-\\nveal the capability of the models to generate high-\\nquality responses within a certain number of at-\\ntempts.\\nFig. 3 illustrates graphs depicting the relation-\\nship between the metric values and the number of\\ngeneration attempts (k). These graphs provide in-\\nsights into the performance of the models as the\\nnumber of generation attempts increases. Partic-\\nularly, after several generation attempts, both the\\nSFT and RLHF versions appear to outperform the\\nlarger GPT Neo 2.7B model in terms of the eval-\\nuated metrics. Additionally, the RLHF model ex-\\nhibits significant improvement in BertScore, which\\nsuggests enhanced semantic similarity between the\\ngenerated and reference responses.\\nTable 2 presents the results for Metrics@10,\\nwhich indicates the best metric scores among 10\\ngeneration attempts. Most of the highlighted values\\nhave been determined to be statistically significant\\nas per the U-test and KS-test. Interestingly, Base\\nGPT Neo 2.7B exhibits the highest average model\\nreward based on the contrastive approach. This\\nmight suggest that the model’s responses are more\\ndiverse and, in some cases, closer to the reference\\nanswers.\\nThe Rouge 2 metric, which focuses on the over-\\nlap of bigrams between the generated and refer-\\nence texts, presents a close competition among the\\nmodels. This implies that the inclusion of both con-\\ntent words and their ordering are well-represented\\nacross models.\\n4.5\\nMRR Comparison\\nAn additional analysis conducted to assess the\\nconsistency between reward models and metrics\\nused for evaluating the quality of the generated re-\\nsponses. This analysis involved the utilization of\\nmanual annotations for answers corresponding to\\n100 random questions.\\nIn this analysis, MRR calculated for varying val-\\nues of k, where k denotes the top-ranked answers\\naccordingly to some metric. The Mean Reciprocal\\nRank at k (MRR@k) is a statistical measure for\\nevaluating any process that produces a list of pos-\\nsible responses to a sample of queries, ordered by\\nprobability of correctness. If we have A answers,\\nand Ri is the rank of the first relevant document\\nfor query i (considering only the top k documents),\\nthen the MRR@k is:\\nMRR@k = 1\\nA\\nA\\nX\\ni=1\\n\\x12 1\\nRi\\n\\x13\\nif Ri ≤ k; else 0\\n(3)\\nThis method allows for understanding how effec-\\nSacreBLEU Rouge 1 Rouge 2 BertScore Reg. Reward Contr. Reward\\nBase 125M\\n0.0724\\n0.2623\\n0.0549\\n0.9517\\n0.2746\\n2.6521\\nSFT 125M\\n0.0875\\n0.2897\\n0.0614\\n0.9578\\n0.3754\\n2.8612\\nRLHF 125M\\n0.0901\\n0.2903\\n0.0625\\n0.9586\\n0.3711\\n2.8187\\nBase 2.7B\\n0.0744\\n0.2704\\n0.0607\\n0.9513\\n0.3201\\n3.6581\\nTable 2: Comparison of average metrics@10 for different models.\\nFigure 3: Graphs of dependencies of metric values on the number of k attempts to generate\\ntively the different metrics rank the correct answers\\namong its top predictions.\\nTable 3 presents the MRR@10 scores, indicat-\\ning the MRR values when considering the top 10\\nranked samples. Notably, Rouge 2 and Rouge 1\\nmetrics exhibit higher values, which implies that\\nthey are key metrics in assessing the accuracy of the\\ngenerated responses. However, the trained reward\\nmodels display superior performance compared to\\nboth the SacreBLEU and BertScore metrics.\\n4.6\\nCorrelation Analysis\\nIn addition to the previous evaluations, a corre-\\nlation analysis carried out among the assessment\\nmethods utilized. Specifically, the Spearman cor-\\nrelation coefficient was computed to understand\\nthe relationships between the various metrics. The\\nSpearman correlation coefficient is a nonparametric\\nmeasure that evaluates the strength and direction\\nof the association between two ranked variables.\\nAppendix A contains tables that compare the\\ncross-correlation coefficients of the metrics for\\neach model generations.\\nUpon examination, a\\nprominent correlation between the rankings of\\nRouge 1 and Rouge 2 is evident. Furthermore,\\nthe reward regression model exhibits a moderate\\ncorrelation when responses are generated by the\\nfine-tuned models. Interestingly, BertScore demon-\\nstrates little to no correlation, or even a negative\\ncorrelation, with the other metrics. This raises ques-\\ntions about its reliability as a comparative measure\\nin this context. Additionally, it is notable that the re-\\nward models display minimal correlation amongst\\nthemselves, when trained through different method-\\nologies.\\n5\\nDiscussion\\nThe study focused on the generating QA highlights\\nthe effective implementation of the RLHF in the in-\\ntricate domain. This method outperforms SFT tech-\\nnique, marking its superiority in terms of metrics\\nperformance. Moreover, the application of RLHF\\nhas demonstrated that it’s possible to competitively\\ntrain smaller models, showcasing its efficacy even\\nin scenarios with limited resources.\\nRegarding the scoring parameters, the study\\ndraws attention to the utility of Rouge scores in\\ngauging response precision. This implies a poten-\\nBase 125M SFT 125M RLHF 125M\\nSacreBLEU\\n0.4107\\n0.3709\\n0.3262\\nRouge 1\\n0.4792\\n0.4532\\n0.4091\\nRouge 2\\n0.4011\\n0.4453\\n0.4220\\nBertScore\\n0.2913\\n0.3403\\n0.3300\\nReg. Reward\\n0.4015\\n0.3867\\n0.4296\\nContr. Reward\\n0.4302\\n0.3787\\n0.3527\\nTable 3: Comparison of MRR@10 scores for different models and metrics. The values represent the MRR scores\\nconsidering the top 10 ranked samples.\\ntial edge of Rouge over alternative scoring systems\\nlike SacreBLEU and BertScore in certain contexts.\\nHowever, there exists ambiguity in the MRR re-\\nsults for BertScore and SacreBLEU metrics when\\njuxtaposed with the outcomes from the trained re-\\nward models. This raises questions about the ad-\\nequacy of these metrics for the programming do-\\nmain, which is hallmarked by complex semantic re-\\nlationships and a plethora of correct answers. This\\nambiguity is further cemented by near-zero Spear-\\nman correlations associated with various linguistic\\nmetrics.\\nThese findings not only provide a deeper under-\\nstanding of RLHF’s potential and boundaries but\\nalso emphasize the necessity for diverse, domain-\\nspecific methods when evaluating generation qual-\\nity. In this context, the programming domain serves\\nas an exemplar. This research’s insights could stim-\\nulate further advancements in the development of\\nnovel and more suitable metrics for similar com-\\nplex domains.\\n6\\nConclusion\\nIn conclusion, our study has demonstrated the\\neffectiveness of RLHF in enhancing the perfor-\\nmance of small LLMs like GPT Neo 125M in the\\nprogramming domain. Our experiments focused\\non fine-tuning the model using user-generated re-\\nsponses from Stack Overflow, employing two re-\\nward model training strategies, regression scores\\nand contrastive scores, with PPO.\\nThe study also highlights the critical role of\\nemploying the right evaluation measures. While\\nRouge scores effectively captured response accu-\\nracy, other metrics like BertScore and SacreBLEU\\npresented ambiguities, especially when juxtaposed\\nwith the results from the trained reward models.\\nThis disparity, brought into sharper focus by near-\\nzero Spearman correlations, implies that traditional\\nmetrics might not suffice for complex fields such\\nas programming. These domains are marked by in-\\ntricate semantic relationships and a broad spectrum\\nof valid answers.\\nAs we look to the future, we envision testing\\nour methodologies and experiment setups on larger\\nmodels to assess the scalability of our approach and\\nverify the consistency of our results. We anticipate\\nthat these further investigations would provide valu-\\nable insights into the behavior and performance of\\nthese larger models under RLHF based fine-tuning,\\nthereby expanding the scope of our current study.\\nThe insights derived from our research enrich\\nour understanding of both the potential and the\\nlimitations of RLHF. They also underline the ne-\\ncessity for tailored evaluation methods in complex\\ndomains. As we persist in honing and formulating\\ninnovative techniques for efficient generation, the\\nlessons gleaned from our work will undoubtedly\\nprove invaluable.\\nLimitations\\nIn this study, we attempted to investigate the appli-\\ncation of the GPT Neo model with 125M parame-\\nters in assessing the quality of linguistic metrics in\\nthe Usage API subcategory of question-and-answer\\ndata. We acknowledge several limitations that need\\nto be taken into account when interpreting the re-\\nsults.\\nFirst, the data used in the experiments is domain-\\nspecific, sourced exclusively from the Usage API\\nsubcategory, which lacks code blocks. Although\\nour findings demonstrate discrepancies between\\nlinguistic metrics within this chosen domain, their\\ngeneralizability to other question-and-answer cat-\\negories remains unclear. It’s possible that our re-\\nward model may perform differently when applied\\nto more diverse datasets with varied question types\\nand content, including those that incorporate code\\nblocks.\\nSecond, the application of the small GPT Neo\\nmodel with 125M parameters represents a signif-\\nicant limitation in terms of both computational\\ncapacity and the model’s semantic understanding.\\nThe constraints of our computing resources, specif-\\nically the usage of 2 Nvidia A6000 GPUs and the\\nnecessity to accommodate three models during the\\nRLHF training, have imposed certain restrictions.\\nOwing to VRAM limitations, portions of the ques-\\ntion context were omitted during training, poten-\\ntially undermining the model’s ability to fully grasp\\nthe semantic relations in the language.\\nAnother caveat concerns the scale of the model.\\nWhile our experiments illustrated the small model’s\\nability to enhance results to levels comparable to\\nits larger counterparts, the behavior of the larger\\nmodels under similar experimental conditions is yet\\nto be understood. This question remains open and\\nwarrants further investigation in future research.\\nIn summary, our study provides valuable insights\\ninto the use of smaller GPT Neo models for assess-\\ning linguistic metrics, but the highlighted limita-\\ntions underscore the need for additional research in\\nbroader data contexts, with larger models, and con-\\nsidering the intricate facets of language translation\\nand reformulation.\\nReferences\\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\\nJoseph, Ben Mann, Nova DasSarma, et al. 2021. A\\ngeneral language assistant as a laboratory for align-\\nment. arXiv preprint arXiv:2112.00861.\\nEdward Beeching, Younes Belkada, Kashif Rasul,\\nLewis Tunstall, Leandro von Werra, Nazneen Ra-\\njani, and Nathan Lambert. 2023. Stackllama: An rl\\nfine-tuned llama model for stack exchange question\\nand answering.\\nStefanie\\nBeyer,\\nChristian\\nMacho,\\nMassimiliano\\nDi Penta, and Martin Pinzger. 2020.\\nWhat kind\\nof questions do developers ask on stack overflow?\\na comparison of automated approaches to classify\\nposts into question categories. Empirical Software\\nEngineering, 25:2258–2301.\\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\\nStella Biderman. 2021a. Gpt-neo: Large scale autore-\\ngressive language modeling with mesh-tensorflow. If\\nyou use this software, please cite it using these meta-\\ndata, 58.\\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\\nand Stella Biderman. 2021b.\\nGPT-Neo: Large\\nScale Autoregressive Language Modeling with Mesh-\\nTensorflow. If you use this software, please cite it\\nusing these metadata.\\nSergey V. Kovalchuk, Vadim Lomshakov, and Artem\\nAliev. 2022. Human perceiving behavior modeling\\nin evaluation of code generation models. In Pro-\\nceedings of the 2nd Workshop on Natural Language\\nGeneration, Evaluation, and Metrics (GEM), pages\\n287–294, Abu Dhabi, United Arab Emirates (Hybrid).\\nAssociation for Computational Linguistics.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\net al. 2021.\\nWebgpt: Browser-assisted question-\\nanswering with human feedback.\\narXiv preprint\\narXiv:2112.09332.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow instruc-\\ntions with human feedback.\\nAdvances in Neural\\nInformation Processing Systems, 35:27730–27744.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\\nAlec Radford, and Oleg Klimov. 2017.\\nProxi-\\nmal policy optimization algorithms. arXiv preprint\\narXiv:1707.06347.\\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\\nDario Amodei, and Paul F Christiano. 2020. Learn-\\ning to summarize with human feedback. Advances\\nin Neural Information Processing Systems, 33:3008–\\n3021.\\nQicai Wang, Peiyu Liu, Zhenfang Zhu, Hongxia Yin,\\nQiuyue Zhang, and Lindong Zhang. 2019. A text ab-\\nstraction summary model based on bert word embed-\\nding and reinforcement learning. Applied Sciences,\\n9(21):4701.\\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\\nBrown, Alec Radford, Dario Amodei, Paul Chris-\\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\\nguage models from human preferences.\\narXiv\\npreprint arXiv:1909.08593.\\nA\\nSpearman correlation tables\\nIn this appendix, we present figs. A1 to A3 that fea-\\nture comparative tables of Spearman’s correlation\\ncoefficients for several evaluation metrics: Rouge\\n1, Rouge 2, SacreBLEU, and BertScore and used\\ntwo variations of reward models, the regressive and\\ncontrastive. They based on the generations pro-\\nduced by three distinct models. These models are\\nthe Base GPT Neo 125M, the SFT GPT Neo 125M,\\nand the RLHF GPT Neo 125M, respectively.\\nFigure A1: Spearman correlation coefficients for Base\\nmodel\\nFigure A2: Spearman correlation coefficients for SFT\\nmodel\\nFigure A3: Spearman correlation coefficients for RLHF\\nmodel\\n'}]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of datas extracted\n",
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes df from  the data received from model\n",
    "df = pd.DataFrame().from_dict(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52 entries, 0 to 51\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   abstract           51 non-null     object\n",
      " 1   introduction       51 non-null     object\n",
      " 2   literature review  21 non-null     object\n",
      " 3   methodology        50 non-null     object\n",
      " 4   results            50 non-null     object\n",
      " 5   conclusion         51 non-null     object\n",
      " 6   title              52 non-null     object\n",
      " 7   author             52 non-null     object\n",
      " 8   textdata           52 non-null     object\n",
      " 9   literature_review  30 non-null     object\n",
      " 10  Abstract           1 non-null      object\n",
      " 11  Introduction       1 non-null      object\n",
      " 12  Literature_Review  1 non-null      object\n",
      " 13  Methodology        1 non-null      object\n",
      " 14  Results            1 non-null      object\n",
      " 15  Conclusion         1 non-null      object\n",
      "dtypes: object(16)\n",
      "memory usage: 6.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all columns into type str\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate columns auda ko lagi\n",
    "# name_literature_review = ['literature_review','literature_reviews']\n",
    "# name_methodology = ['method','methods']\n",
    "# name_result = ['results']\n",
    "# name_conclusion = ['conclusion']\n",
    "# for name in name_literature_review:\n",
    "#     df['literature review'] = df['literature review'] + df[name]\n",
    "# for name in name_methodology:\n",
    "#     df['methodology'] = df['methodology'] + df[name]\n",
    "# for name in name_result:\n",
    "#     df['result'] = df['result'] + df[name]\n",
    "# for name in name_conclusion:\n",
    "#     df['conclusion'] = df['conclusion'] + df[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['literature review'] = df['literature review'] + df['literature_review']+ df['Literature_Review']  \n",
    "# +df['related_work'] +df['background']\n",
    "df['methodology'] = df['methodology'] + df['Methodology']\n",
    "df['results'] = df['results'] + df['Results']\n",
    "df['conclusion'] = df['conclusion'] + df['Conclusion']\n",
    "df['abstract'] = df['abstract'] + df['Abstract']\n",
    "df['introduction'] = df['introduction'] +df['Introduction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('literature_review',axis= 1)\n",
    "df = df.drop('Literature_Review',axis= 1)\n",
    "df = df.drop('Methodology',axis= 1)\n",
    "df = df.drop('Results',axis= 1)\n",
    "df = df.drop('Conclusion',axis= 1)\n",
    "df = df.drop('Abstract',axis= 1)\n",
    "df = df.drop('Introduction',axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate values on basis of titles\n",
    "df = df.drop_duplicates(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52 entries, 0 to 51\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   abstract           52 non-null     object\n",
      " 1   introduction       52 non-null     object\n",
      " 2   literature review  52 non-null     object\n",
      " 3   methodology        52 non-null     object\n",
      " 4   results            52 non-null     object\n",
      " 5   conclusion         52 non-null     object\n",
      " 6   title              52 non-null     object\n",
      " 7   author             52 non-null     object\n",
      " 8   textdata           52 non-null     object\n",
      "dtypes: object(9)\n",
      "memory usage: 3.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the file as csv\n",
    "name_file = 'df(500-600).csv'\n",
    "df.to_csv(name_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sharing Energy in Wide Area: A Two-Layer Energy Sharing Scheme for Massive Prosumers',\n",
       " 'Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions',\n",
       " 'PhotoBot: Reference-Guided Interactive Photography via Natural Language',\n",
       " 'StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion',\n",
       " 'Bounding Consideration Probabilities in Consider-Then-Choose Ranking Models',\n",
       " 'Design Principles & Issues for Gaze and Pinch Interaction',\n",
       " 'Subjective Causality',\n",
       " 'Responsible AI Governance: A Systematic Literature Review']"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_title"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
