abstract,introduction,literature review,methodology,results,conclusion,title,author,textdata
"Open-vocabulary semantic segmentation aims to segment images into arbitrary open-vocabulary texts, rather than a fixed set of classes. S-Seg is a simple but elegant method that directly trains for pixel-level feature and language alignment. It outperforms more complex methods on three widely tested benchmarks, showcasing the potential of simple open-vocabulary segmentation.","Open-vocabulary semantic segmentation presents a unique challenge as it requires assigning accurate semantic labels to each pixel in an image using arbitrary open-vocabulary texts. Achieving this requires a robust, pixel-level alignment between images and textual descriptions, which enables accurate association of each pixel with the most relevant class from a dynamically provided set of textual categories.

Prior works in this domain address the problem using various strategies, including adapting existing Vision-Language (VL) models, training models on ground truth masks that are annotated for a select number of seen classes, and employing specialized models such as GroupViT and OVSegmentor. However, these methods often rely on complex models, extensive ground truth data, or custom grouping encoders, which can limit their accessibility and scalability.","One common tactic involves adapting existing VL models, which are initially trained for image-level alignment, to perform at the pixel level. Methods like MaskCLIP modify the CLIP image encoder to enhance its pixel-level alignment capabilities, while TCL employs CLIP for initial text-to-image region grounding followed by contrastive learning to refine the alignment between the text embedding and the grounded region. Other works, such as OpenSeg and DiffuMask, explore the use of pseudo-masks, which are segmentation maps generated through self-supervised representation learning or diffusion models, as a form of weak supervision.

Another strategy involves training models using ground truth masks annotated for a limited set of seen classes. ZegFormer demonstrates this approach by decoupling zero-shot semantic segmentation into two sub-tasks: a class-agnostic grouping task and a zero-shot segment classification task.nan","To address these challenges, we propose S-Seg, a simple and intuitive framework for open-vocabulary segmentation. S-Seg directly trains for pixel-level feature and language alignment without relying on complex models, extensive ground truth data, or custom grouping encoders.

The core idea behind S-Seg is to leverage pseudo-mask and language to supervise MaskFormer. A pseudo-mask generator is introduced to produce class-agnostic mask supervision by generating pseudo ground truth masks. These masks are crucial for training, as they provide high-quality supervision for mask prediction while also being efficient to generate. Furthermore, image-text contrastive loss is imposed to promote language alignment between visual and textual features.

S-Seg is designed to be flexible with easily replaceable submodules, promoting simplicity in its design while remaining open to future advancements in segmentation models.","Our evaluation shows that S-Seg achieves competitive performance on three widely tested benchmarks (Pascal VOC, Pascal Context, and COCO). S-Seg outperforms several complex open-vocabulary segmentation methods, including MaskCLIP, GroupViT, and OpenSeg, demonstrating the effectiveness of our simple yet effective approach.

Data scalability and self-training experiments further showcase the potential of our method. We observe consistent gains in performance as more data is utilized for training, indicating good scalability with increasing dataset sizes. Self-training also proves to be beneficial, leading to significant improvements in segmentation accuracy across various datasets. Moreover, qualitative results demonstrate that S-Seg can effectively segment challenging images, including those with overlapping objects, small objects, and complex scenes.","In summary, S-Seg offers a simple and effective approach for open-vocabulary semantic segmentation. It outperforms more sophisticated methods on three benchmarks, highlighting the potential of simpler segmentation techniques. The reliance on complex models and extensive ground truth data in this domain can be reduced, which could lead to a more streamlined and accessible approach to future advancements in open-vocabulary segmentation.",Exploring Simple Open-Vocabulary Semantic Segmentation,Zihang Lai,"Exploring Simple Open-Vocabulary Semantic Segmentation
Zihang Lai
University of Oxford
zihang.lai@eng.ox.ac.uk
Abstract
Open-vocabulary semantic segmentation models aim to
accurately assign a semantic label to each pixel in an image
from a set of arbitrary open-vocabulary texts. In order to
learn such pixel-level alignment, current approaches typ-
ically rely on a combination of (i) image-level VL model
(e.g. CLIP), (ii) ground truth masks, and (iii) custom group-
ing encoders. In this paper, we introduce S-Seg, a novel
model that can achieve surprisingly strong performance
without depending on any of the above elements. S-Seg
leverages pseudo-mask and language to train a MaskFormer,
and can be easily trained from publicly available image-
text datasets. Contrary to prior works, our model directly
trains for pixel-level features and language alignment. Once
trained, S-Seg generalizes well to multiple testing datasets
without requiring fine-tuning. In addition, S-Seg has the
extra benefits of scalability with data and consistently im-
provement when augmented with self-training. We believe
that our simple yet effective approach will serve as a solid
baseline for future research. Our code will be released at
https://github.com/zlai0/S-Seg.
1. Introduction
Open-vocabulary semantic segmentation presents a unique
challenge as it requires assigning accurate semantic labels
to each pixel in an image using arbitrary open-vocabulary
texts, rather than a fixed set of classes. This means that the
model must be able to segment and classify any arbitrary
categories expressed in language. Achieving this requires a
robust, pixel-level alignment between images and textual de-
scriptions, which enables accurate association of each pixel
with the most relevant class from a dynamically provided set
of textual categories.
A primary obstacle in this domain is that it is impossi-
ble to construct datasets that provide pixel-level annotations
for all possible labels. This limitation often results in the
adoption of weakly-supervised or semi-supervised learning
approaches. Current methods typically rely on a combina-
tion of strategies to learn the required pixel-level alignment.
Figure 1. S-Seg result on a web image. Our goal is to segment
everything, including fictional characters like minions.
tiger
minions
Maoi
A yellow 
rubber duck 
sits in water.
Pseudo-mask 
generator 
Mask 
supervision
Language 
model
Semantic 
supervision
Language 
feature
Image-text pair
Figure 2. Our S-Seg framework leverages pseudo-mask and lan-
guage to train a MaskFormer. We show that our method of directly
training for pixel-level feature and language alignment yields supe-
rior results.
One common tactic is adapting existing Vision-Language
(VL) models, which are initially trained for image-level
alignment (e.g., CLIP [46]), to perform at the pixel level.
Another strategy involves training models on ground truth
masks that are annotated for a select number of seen classes,
thereby encouraging the model to extrapolate its learning
to novel unseen classes. Furthermore, specialized models
such as GroupViT [57] and OVSegmentor [58], which are
explicitly designed for open-vocabulary segmentation, are
being explored. These models typically group similar pixels
within the image encoder based on their features, employing
a hierarchical approach, enhancing the model’s ability to
understand the image at multiple granularities.
In this paper, we report a model that can work surpris-
ingly well with none of the above strategies. Our approach,
named S-Seg, is built on top of a standard MaskFormer
model. Our model directly trains for pixel-level feature and
language alignment, using neither existing large image-level
alignment models like CLIP [46] nor manually annotated
segmentation or classification labels.
One of the biggest challenges we face is finding the right
1
arXiv:2401.12217v1  [cs.CV]  22 Jan 2024
PASCAL VOC
PASCAL Context
COCO
Background    Sheep    Cat
Duck       Water       Cat       Floor
Background    Cow    Person    Bowl    Orange
Input 
S-Seg 
S-Seg+ 
a
Figure 3. Qualitative results of S-Seg, evaluated using all dataset classes as queries. Our model copes with challenging situation, such as
overlapping objects (col. 2) and small objects (col. 5). Our model is also capable of handling “stuff” categories such as water and floor (col.
3, 4). Moreover, our S-Seg+ model is able to correct small errors observed in the S-Seg method (col. 4). Finally, in the COCO dataset, which
featured a significantly higher number of objects, our model is still able to achieve high accuracy in its predictions.
supervision since annotated masks and labels are not avail-
able. To address this issue, we propose to leverage pseudo-
masks and language to supervise MaskFormer. Our strategy
involves using a pseudo-mask generator to provide class-
agnostic mask supervision by generating pseudo ground
truth masks. We adopt a simple design that clusters image
representations obtained through self-supervised represen-
tation learning methods like DINO [5]. Our experiments
demonstrate that this approach delivers exceptional perfor-
mance, which is essential for high-quality supervision, as
well as rapid processing speed, which is necessary for effi-
cient training. In addition, we use noisy web texts to provide
semantic supervision. The image-text dataset contains a
wide range of concepts and has demonstrated impressive
zero-shot classification results [46]. We utilize a straight-
forward image-text contrastive loss, which has proven to be
highly effective. Once trained, our model generalizes well
to new categories without requiring fine-tuning.
S-Seg is a simple and effective model that can be trained
using publicly available image-text datasets, such as Concep-
tual Captions [7, 48]. This makes it easy to reproduce and
extend for further research. The S-Seg framework is also
designed to be flexible with easily replaceable submodules.
We prioritize simplicity in our subcomponent selection to
focus on the general design of our framework, while remain-
ing open to more advanced techniques that could result in
further improvements.
We conducted a thorough evaluation of S-Seg using mul-
tiple benchmark datasets, and we show that our method
achieve competitive results on three widely tested bench-
marks (Pascal VOC, Pascal Context, and COCO). In addition,
pseudo-mask and language provide scalable supervision and
our model consistently improves in performance as more
data became available. Finally, we find adding an additional
self-training step leads to an even greater improvement to our
model, with an average increase of 5.5% mIoU over three
# I [n, h, w, c] - minibatch of aligned images
# T [n, l]       - minibatch of aligned texts
# N              - number of MaskFormer queries
# C              - number of pseudo masks
# predict mask, mask feature, and text feature
M, M_f = maskformer(I) # [n, N, H, W], [n, N, d_f]
T_f = text_encoder(T)  # [n, d_f]
# aggregate all mask features [n, d_f]
M_f = M_f.mean(axis=1)
# generate pseudo masks [n, C, H, W]
S = pseudo_mask_generator(I)
# compute loss
loss_c = contrastive_loss(M_f, T_f)
loss_m = mask_loss(M, S)
loss = (loss_c + loss_m)/2
Figure 4. Pseudocode for training S-Seg with image-text pairs.
datasets, highlighting the potential for further improvement
of our approach.
Our simple solution suggests that the reliance on complex
models and extensive ground truth data in open-vocabulary
semantic segmentation may be reduced, leading to a more
streamlined and accessible framework for future develop-
ments in the field, and we hope our exploration can serve as
a solid baseline for future research.
2. Related work
Open-vocabulary segmentation. The earliest efforts to em-
ploy language for image segmentation can be traced back
to Duygulu et al.’s seminal work [16], where the authors
tackled image segmentation by framing it as a machine trans-
lation problem. Current approaches leverage a combination
of strategy to learn pixel-level image-text alignment.
Adapting image-level vision-language models. The first
strategy involves the adapting pretrained vision-language
models, originally designed for image-level alignment, to
2
N Masks
N Mask Features
Language 
Model
Image 
(from image-text pair)
A yellow  
rubber duck  
sits in water.
Language 
Feature
Pseudo masks
Pseudo-Mask 
Generator
Lmask
Lcontrast
Image-text pair
Mask and Semantic supervision
MaskFormer
Figure 5. Overview of S-Seg. A MaskFormer model computes masks and mask features from an image input. A pseudo-mask generator
produces segmentation maps to supervise mask predictions, while a text that describes the image, encoded by a language model trained
together with the MaskFormer, provides supervision for mask features using image-text contrastive loss.
the more granular task of pixel-level alignment. This strategy
is widely adopted in open-vocabulary methods [6, 18, 23,
24, 30, 31, 33, 41, 42, 47, 53, 58, 59, 61]. These works vary
in their methods of refining image-level models for finer
alignment tasks.
MaskCLIP [61] demonstrates modifying the CLIP image
encoder can significantly enhance its pixel-level alignment
capabilities without requiring retraining. TCL [6] employs
CLIP for initial text-to-image region grounding, followed
by contrastive learning to refine the alignment between the
text embedding and the grounded region. OpenSeg [18]
fine-tunes ALIGN [28] using a grounding loss [21] to better
align words in captions to segmentation masks. OpenSeg
and DiffuMask [53] also explored the use of pseudo-masks.
The primary distinction lies in their dependency on differ-
ent sources for learning; OpenSeg uses annotated segments
while DiffuMask employs masks generated through diffu-
sion. In contrast, our method is entirely learned from pseudo
masks. Also, our mask generator is entirely self-supervised,
whereas their mask generator is fully-supervised.
Ground truth masks. Another effective strategy [14, 18,
23, 24, 31, 33, 42] involves training models using ground
truth masks annotated for a limited set of seen classes. By
training on seen annotations, models are encouraged to learn
detailed features and patterns that are potentially applicable
beyond the scope of the trained classes.
Of most relevance, ZegFormer [14] trains a MaskFormer
by decoupling zero-shot semantic segmentation into two
sub-tasks, a class-agnostic grouping task and a zero-shot
segment classification task. Our method has similar training
paradigm but with notable distinctions. Similar to GroupViT,
we train exclusively with image-text pairs and do not uti-
lize a pretrained CLIP model. Notably, even without access
to ground truth masks, labels, or CLIP, our method outper-
forms ZegFormer in unseen categories, indicating potentially
stronger generalization.
Custom grouping-based encoders. The third strategy
employs custom-designed models specifically for open-
vocabulary segmentation. GroupViT [57] groups pixels in
an image hierarchically based on their attention scores with
learnable group tokens. OVSegmentor [58] applies Slot At-
tention [38] for a similar pixel grouping process based on
feature proximity.
Our model, S-Seg, can be conceptualized as a synergy
of these approaches. It can be viewed as a CLIP model
integrated with a MaskFormer image encoder, directly opti-
mizing for pixel-level feature and language alignment. Al-
ternatively, it resembles “ZegFormer with pseudomask and
language training” or “GroupVit with a MaskFormer as the
grouping mechanism.” Interestingly, our model relates to
each method by omitting certain core architectural compo-
nents or supervision method. Even so, our method is able to
achieve competitive performance.
Unsupervised image grouping. Unsupervised image
grouping methods are designed to segment images with-
out the use of manually labeled segmentation masks. Early
unsupervised image grouping methods can be roughly cate-
gorized as low-level feature-based [3], clustering-based [29],
and graph-based [49].
More recently, self-supervised
learning-based approaches [11, 22, 26, 27, 50, 51, 60] have
shown superior performance in unsupervised image group-
ing.
3. Approach
Our proposed method, called S-Seg, is conceptually simple:
we learn a MaskFormer model from pseudo-mask and lan-
guage. Our method leverages image-text pairs solely, with-
out relying on ground truth masks or large-scale preatrained
models. In figure 4, we provide pseudocode for the core
implementation of training S-Seg.
Figure 5 provides a
schematic layout of our approach.
3.1. Problem definition
We consider the problem of open-vocabulary semantic seg-
mentation, where we aim to learn a function f that maps an
image I and a set of category names C = {ci} to a semantic
segmentation map S, where ci can be any category name
expressed as open vocabulary texts.
Our approach is based on previous works [47, 57, 61],
and we adopt their problem setting. Specifically, we use
a web dataset of image-text pairs (Ii, Ti) during training,
where Ti is a textual label that describes the content of the
3
corresponding image Ii. However, since the textual labels
are gathered from the web, they may be noisy and contain
errors. We do not use any additional manual annotated
segmentation or classification labels during training.
During testing, a set of category names C is provided, and
the model is tasked with assigning a semantic label ci ∈ C
to each pixel in an unlabeled image. The performance of
the model is evaluated based on its mean Intersection over
Union (mIoU) with the ground truth labels.
3.2. Adapting MaskFormer
Our approach builds on top of MaskFormer [10]. Here,
we begin by briefly review MaskFormer and explain the
adjustments we made.
The Maskformer model takes an image as input and gen-
erates N masks and mask features. First, the input image
passes through a backbone model to produce feature maps at
different output resolutions. These image features are then
fed into a per-pixel encoder, which upsamples and aggre-
gates them into a set of feature maps with higher resolution.
Meanwhile, a transformer decoder uses N learnable queries
to cross-attend to the set of features with the lowest resolu-
tion and gather global information about each segment.
In the original Maskformer, a linear classifier and softmax
activation were applied to the output of the decoder to predict
class probabilities for a fixed list of categories. However,
as we do not have a fixed list of categories, we remove this
classifier branch and output the N raw mask features instead.
In addition to predicting mask features, the Maskformer
also predicts N binary masks. To predict each mask, a dot
product is taken between the mask embedding, generated
from mask features, and the high resolution per-pixel feature.
Finally, N mask-feature pairs are combined to generate a
semantic segmentation map as the output.
3.3. S-Seg
S-Seg employs MaskFormer as its segmentation model, but
in our weakly-supervised learning setting (where only texts
are available), we face the challenge of not having annotated
masks and labels. To overcome this, we utilize pseudo labels
and language to as supervision.
Our training framework is illustrated in Figure 5. We
first generate a set of segmentation maps using our pseudo-
mask generator (Sec. 3.3.1) and use them as supervision
for mask prediction. Meanwhile, we use a language model
to process input text and generate language embeddings.
These embeddings provide supervision for mask features by
leveraging image-text contrastive loss (Sec. 3.3.2).
Notably, unlike the supervised learning setting, where
mask and label annotations are coupled, we decouple mask
and semantic supervision. This enables us to utilize pseudo-
mask and language as two distinct forms of supervision.
Image
N Masks
N Mask Features
Language  
Features
Text
Text
Text
Classify
N Classes
Zero-shot classifier on mask features
Combine
output 
Language 
Model
MaskFormer
Figure 6. Testing on S-Seg. During inference, S-Seg generalize to
new categories by leveraging language features generated from a
list of candidate classes in text.
DINO 
ViT
K-Means
Figure 7. The Pseudo-mask generator generates pseudo-masks
to supervise predicted mask during training. This module takes an
image as its input, extracts its features using a DINO pre-trained
ViT, and then employs K-means clustering to group the pixels into
segments.
Method
Sup.
P. VOC↑
P. Context↑
Time(s)↓
Spectral Clus. [49]*
none
49.2
43.2
0.543
K-Means [29]*
none
49.5
43.3
0.188
ImageNet [15]+[29]
label
68.8
58.1
0.079
GroupViT [57]
text
73.7
54.6
0.002
Pseudo-mask (Ours)
self
78.8
66.3
0.002
Table 1. Our pseudo-mask generator achieves excellent oracle
performance with rapid speed, making it an ideal mask supervi-
sion. We report amortised running time on a batch of 128 samples,
simulating training time scenario. * We process downsampled im-
age at H
8 × W
8 resolution to obtain reasonable running time.
In the testing phase (as shown in figure 6), the trained
MaskFormer model predicts N masks and mask features
from the input image. The language model takes as input a
list of candidate category names (represented as texts) and
extracts a set of language features. These features are then
used to classify the mask features. This process is similar
to the one used in CLIP [46], where the image and possi-
ble text inputs are encoded by their respective encoders to
compute feature embeddings. The cosine similarity between
these embeddings is calculated and adjusted by a learnable
temperature parameter. The resulting values are normalized
into a class probability distribution using a softmax function,
and a combination module is used to takes N mask-class
pairs to produce the final segmentation map, similar to [10].
Next, we will provide a detailed description of the sub-
components in our framework.
4
Oracle segment
Pseudo mask
Figure 8. Example pseudo-masks. Our pseudo-mask generator is
capable of generating high-quality artificial masks. When provided
with an oracle label, these masks demonstrate a high degree of
overlap with the ground truth annotations.
3.3.1
Pseudo-Mask Generator
In our approach, we use a pseudo-mask generator (fig. 7) to
produce a class-agnostic segmentation map from the input
image, which supervises the mask prediction of our model.
To implement the pseudo-mask generator, we adopt a sim-
ple strategy that involves clustering tokens extracted from a
self-supervised pre-trained ViT. Specifically, we use a DINO-
pretrained ViT to compute a set of featurized tokens from the
input image. We then apply a clustering algorithm (K-Means
in our case) to these tokens, assigning each token a label
that corresponds to the index of the cluster it belongs to. We
reshape the resulting label map into an image and resize it to
the original resolution to supervise the mask prediction of
our segmentation model.
Despite its simplicity, our pseudo-mask generator
achieves both impressive performance, which is crucial for
high-quality supervision, and fast processing speed, which is
essential for efficient training. We evaluate its performance
and compare against baseline methods, and the quantitative
results are presented in Table 1, with example predictions
visualized in 8. Our method significantly outperforms simple
baselines such as K-Means and Spectral Clustering, which
naively cluster image pixels, while running two orders of
magnitude faster. We also observed that clustering DINO
representation outperforms clustering ImageNet pre-trained
ViT representation by a significant margin. Notably, our
pseudo-mask generator even outperforms GroupViT, which
has already employed vision-language training.
Since the predicted masks are unordered, we need to
match the N predicted masks with K pseudo ground truth
masks. To accomplish this, we utilize bipartite matching,
as described in [4, 10], which assigns a pseudo-mask to
each predicted mask such that the overall assignment cost
is minimal in all possible assignments. Since each pseudo-
mask is assigned to at most one predicted mask, N − K
pseudo-masks are unassigned to no-object (Ø). Unlike Mask-
Former [10], we do not penalize these no-object masks, nor
do we use classification loss as an assignment cost. Finally,
we compute the mask loss between predicted masks and their
corresponding pseudo-mask, utilizing a combination of dice
loss [43] and focal loss [35].
Lmask = λdiceLdice + λfocalLfocal
(1)
3.3.2
Language Supervision
Our model learns to classify open-vocabulary concepts from
language supervision. To train the model, we use an image-
text contrastive loss [18, 46]. Specifically, we view N mask
features as representation of the input image, each capturing
information about a different part of the image. We then
compute a single feature that represents the entire image
by taking the average of these mask features. To encode
the text, we use a text transformer [52] and select the em-
bedding corresponding to the [EOS] token, resulting in a
textual feature. Since the visual and textual features may
have different dimensions, we project each representation
into a common embedding space using 2-layer MLPs. To
compute the image-text contrastive loss, we calculate the
cosine similarity between the image embeddings and the
text embeddings within the same batch. Following common
practice [32, 45, 46], we decouple the image-text contrastive
loss into two parts:
LI→T = − 1
N
N
X
i
log
exp(x⊺
i yi/σ)
PN
j=1 exp(x⊺
i yj/σ)
(2)
LT →I = − 1
N
N
X
i
log
exp(y⊺
i xi/σ)
PN
j=1 exp(y⊺
i xj/σ)
(3)
where xi and yi are L2-normalized embedding of image
and text of the i-th pair. N denotes batch size and σ is a
learnable temperature parameter optimized together with
the rest of the model. The total loss is the sum of these
two losses, Lcontrastive = LI→T + LT →I. This loss function
promotes high similarity for positive pairs and low similarity
for negative pairs. The loss is minimized when the positive
image-text pairs have the highest similarity. To increase the
contrastive efficiency, we aggregate negative samples from
all nodes when we use distributed training, enabling more
negative samples to be compared against.
3.3.3
Training Loss
Overall, mask loss (Sec. 3.3.1) and image-text contrastive
loss (Sec. 3.3.2) complete the necessary mask and semantic
supervision that is needed to train our model. The final loss
is a weighted combination of the two losses:
L = λmaskLmask + λcontrastiveLcontrastive
(4)
In our experiment, we use λmask = 1.0, λcontrastive = 1.0,
λdice = 1.0, λfocal = 20.0.
5
Input
CLIP
MaskCLIP
GroupViT
Ours
Ours+
Annotation
Fully Sup.
Figure 9. Qualitative comparison with existing methods. CLIP [46] is primarily designed for classification and does not perform well in
segmentation. MaskCLIP [61] adapts CLIP for segmentation, although it produces noisy predictions and cannot handle background classes.
GroupViT [57] is a strong competitor, but it could struggle in challenging scenarios.
3.3.4
Self-training
In order to enhance our results, we introduce an optional
step wherein we train a new model using the predictions
generated by our current model. This process of self-training
results in an augmented model, which we refer to as S-Seg+.
More specifically, when we evaluate on a given dataset, we
generate pseudo labels for the unlabeled images in the train-
ing set. Subsequently, we employ these pseudo labels to
train a new segmentation model.
Self-training improves the accuracy by leveraging addi-
tional data [56], augmentation [62], and bootstrapping [20].
In our situation, self-training offers even greater benefits
since we can take advantage of additional information that
is obtainable during testing: unlabeled images and testing
categories. We show that this additional step improves our
results significantly at no extra manual labelling cost.
4. Experiments
In this section, we empirically evaluate our method and
compare to existing approaches. We show that, although
our method is quite simple, it performs surprisingly well
against more complex existing methods. We evaluate the
open-vocabulary semantic segmentation performance of S-
Seg on the validation set of three datasets: Pascal VOC
2012 [17] (21 classes), Pascal Context [44] (60 classes) and
COCO [34] (81 classes). For more implementation details,
please refer to our supplementary materials.
Method
P. VOC
P. Context
COCO
3-Avg.
B1: Pseudo Mask + CLIP
12.9
3.9
2.9
6.6
B2: Pseudo-mask ViT
23.2
11.0
10.4
14.9
S-Seg (Ours)
44.9
22.9
22.5
30.1
Table 2. Simple baselines for open-vocabulary semantic segmen-
tation. All models are trained on CC12M. Higher values are better.
Two simple baselines fail to obtain satisfactory results, even using
after using our pseudo masks and no less training data.
4.1. Simple baselines
The high quality of pseudo-masks (as shown in Figure 7)
may lead one to assume that the primary challenge is simply
classifying these masks, and that this can be accomplished
by utilizing pre-existing methods such as CLIP. To test this
assumption, we first develop two simple baselines.
Baseline 1: Pseudo-mask + CLIP. Firstly, our pseudo
label generator is utilized to obtain pseudo segments. Then,
we iterate through all the masks and apply the current mask
to the original image. Next, the masked image is fed to CLIP
for classification and the resulting class label is assigned to
the corresponding segment.
Baseline 2: Pseudo-mask ViT. We introduce a new vi-
sual backbone that differs from the regular ViT. Instead of
pooling all image tokens into a single feature, we first indi-
vidually pool tokens in each segment of the pseudo-mask
into segment features, and then pool these features into a
visual embedding. We train a CLIP-like model from scratch
using this visual backbone. During testing, we classify each
segment feature and assign the label to that segment.
The results are presented in Table 2. As we can see,
6
Method
OV
Sup.
P. VOC
P. Context
COCO
Linearly-probed classification models:
MoCo v3 [9]
✗
self
34.3
21.3
-
DINO [5]
✗
self
39.1
20.4
-
Open-vocabulary models (annotated masks not required for training):
CLIP [46]†
✓
text
13.5
8.1
5.9
MaskCLIP [61]†
✓
text
26.8
22.8
12.8
ViL-Seg [36]
✓
text
34.4
16.3
16.4
CLIPpy [47]
✓
text
52.2
-
-
GroupViT [57]
✓
text
50.8
23.7
27.5
SegCLIP [41]
✓
text
52.6
24.7
26.5
OVSegmentor [58]
✓
text
53.8
20.4
25.1
TCL [6]
✓
text
55.0
30.4
31.6
S-Seg (Ours)
✓
text
53.2
27.9
30.3
S-Seg+ (Ours)
✓
text
62.0
30.2
35.7
Fully-supervised segmentation models:
DeepLabV3+† [8]
✗
GT
78.7
46.4
55.7
MaskFormer† [10]
✗
GT
81.2
50.0
62.1
Table 3. Open-vocabulary semantic segmentation results (back-
ground pixels included in evaluation). Benchmarked on Pascal
VOC (P. VOC), Pascal Context (P. Context) and COCO, following
standard evaluation protocols on open-vocabulary model trained
without annotated masks [6, 41, 57, 58]. Our approach obtain sec-
ond highest performance on average and has better results than
GroupViT on all datasets. † denotes our recomputed results. Higher
values are better.
open-vocabulary segmentation is more complex than sim-
ply grouping image into segments and then categorizing
them into classes, even when the segments are of high qual-
ity. Baseline 1 employs a significantly larger pretrained
CLIP ViT/L-14 model that was also trained on a much larger
dataset, while Baseline 2 is trained using the same data as
ours. Nevertheless, both baselines fail to achieve satisfac-
tory results, suggesting that open-vocabulary segmentation
cannot be naively deconstructed in such ways. We hypoth-
esize that a multi-task learning approach that jointly trains
for segmentation and classification could yield significant
advantages.
4.2. Evaluation with background
In table 3, we evaluate our model and compare with exist-
ing method on open-vocabulary semantic segmentation task.
Following standard evaluation protocols on open-vocabulary
model trained without annotated masks [6, 41, 57, 58], we in-
clude background pixels in evaluation and obtain background
prediction by setting a theshold for background classes [57].
Despite the simplicity of S-Seg, our approach achieve com-
petitive performance over previous open-vocabulary segmen-
tation methods that does not require mask annotations. Our
model has second highest performance on average and has
better results than GroupViT on all datasets. Moreover, our
self-trained model, S-Seg+, provides an impressive 5.5%
mIoU improvement over our base model S-Seg (42.6% vs
37.1% 3-avg. mIoU), suggesting the efficacy of self-training.
Method
OV
Sup.
P. VOC
P. Context
COCO
Open-vocabulary models (annotated masks required for training):
SPNet [54]
✓
mask+text
18.3
24.3
-
ZS3Net [2]
✓
mask+text
38.3
19.4
21.1
LSeg [31]
✓
mask+text
52.3
-
27.2
OpenSeg [18]
✓
mask+text
77.2
45.9
38.1
ZegFormer [14]
✓
mask+text
80.7
-
-
GKC [24]
✓
mask+text
83.2
45.2
-
ODISE [59]
✓
mask+text
85.7
84.6
65.2*
DeOp [23]
✓
mask+text
91.7
48.8
-
OVSeg [33]
✓
mask+text
94.5
55.7
-
SAN [42]
✓
mask+text
94.6
57.7
-
Open-vocabulary models (annotated masks not required for training):
CLIP [46]†
✓
text
39.6
9.0
13.8
MaskCLIP [61]†
✓
text
49.5
25.5
23.6
GroupViT [57]†
✓
text
77.2
23.0
37.5
TCL [6]
✓
text
83.2
33.9
-
S-Seg (Ours)
✓
text
81.8
27.2
42.4
S-Seg+ (Ours)
✓
text
84.7
31.6
53.0
Fully-supervised segmentation models:
DeepLabV3+† [8]
✗
GT
89.9
48.5
66.9
Table 4. Open-vocabulary semantic segmentation results (back-
ground pixels excluded in evaluation). Benchmarked following
standard protocol for evaluating open-vocabulary models with anno-
tated masks [14, 23, 24]. S-Seg achieves competitive performance
compared to earlier methods similar to the previous setting. † de-
notes our recomputed results. *COCO is used for training. Higher
values are better.
4.3. Evaluation without background
We also evaluate our model on the evaluation protocol com-
monly used for evaluating open-vocabulary models with
annotated masks [14, 23, 24], where the background pixels
are excluded in evaluation. We note that this setting is easier
because background class is more diverse in appearance and
often requires additional processing such as thresholding.
Table 4 shows the results. Similar to the previous setting, our
S-Seg and S-Seg+ models achieve competitive performance
compared to earlier methods.
4.4. Ablation studies
Self-training. We investigated the effectiveness of self-
training for improving segmentation performance. To this
end, we compared S-Seg and S-Seg+ on three datasets
and evaluated the results using Figure 10. We found that
self-training consistently improved the segmentation perfor-
mance by a significant margin (+5.5% mIoU on average),
regardless of the data size and test dataset. These results
indicate that self-training is a reliable approach for enhanc-
ing the performance of S-Seg and can provide a desirable
complement for further improvement.
Data scalability.
To evaluate the scalability of our
method, we trained S-Seg and S-Seg+ using three datasets
of increasing sizes: 12M, 15M, and 26M. The results of
the experiments are presented in Figure 12. We observed
that both models achieve significant improvements in per-
7
10
20
30
Size of training data (×106)
45
50
55
60
mIoU.
↑ 8.2%
mIoU.
↑ 9.1%
mIoU.
↑ 8.8%
w/o self-train
w/ self-train
(a) Pascal VOC (+18.3%)
10
20
30
Size of training data (×106)
24
26
28
30
mIoU.
↑ 2.6%
mIoU.
↑ 5.4%
mIoU.
↑ 2.3%
w/o self-train
w/ self-train
(b) Pascal Context (+14.1%)
10
20
30
Size of training data (×106)
23
27
31
35
mIoU.
↑ 3.7%
mIoU.
↑ 4.4%
mIoU.
↑ 5.4%
w/o self-train
w/ self-train
(c) COCO (+17.6%)
Figure 10. Self-training improvement. We show average relative improvement in bracket on top of the plot. we observe that self-training
consistently leads to significant improvement for S-Seg across all of our training and testing data settings.
S-Seg (details)
S-Seg+ (details)
S-Seg (details)
S-Seg+ (details)
Figure 11. Visualizing effect of self-training. Our self-trained
S-Seg+ model demonstrates the ability to accurately predict in
regions overlooked by S-Seg, as shown in the colorful rectangles.
10
20
30
46
49
52
mIoU (%)
Pascal VOC
10
20
30
23
25
27
Pascal Context
10
20
30
24
27
30
COCO
10
20
30
31
34
37
3-Average
(a) S-Seg
0.0
0.2
0.4
0.6
0.8
1.0
Size of training data (×106)
0.00
0.25
0.50
0.75
1.00
10
20
30
54
57
60
mIoU (%)
Pascal VOC
10
20
30
26
28
30
Pascal Context
10
20
30
28
31
34
COCO
10
20
30
36
39
42
3-Average
(b) S-Seg+
Figure 12. Scaling training data provide consistent gain in per-
formance, with or without self-training. We train our model using
different sizes of data: CC12M (12M), CC12M+CC3M (15M), and
CC12M+CC3M+RedCaps (26M). We note a steady improvement
in the performance as the data size increases.
formance across all three testing datasets as the amount of
data increased, suggesting that our method scales well with
larger datasets.
4.5. Visualization
The qualitative results of our model are illustrated in Figure 3.
Our model has demonstrated its ability to handle difficult
situations such as overlapping and small objects. Comparing
our results to those of existing methods, as shown in Fig-
ure 9, we observed that our approach accurately segments
tiger
Maoi
avatar
Figure 13. Qualitative results on web images. The query class
name is shown to the right. Row 1: S-Seg is able to segment fic-
tional characters in an animated scene. Row 2: Despite having
taken a mud bath, the tiger can still be easily recognized and seg-
mented. Row 3: S-Seg is capable of identify specific landmarks.
objects in challenging cases where previous methods have
failed. Additionally, we observed that self-training can cor-
rect minor errors in our base model (as shown in detail in
fig. 11). In Figure 1 and 13, we present S-Seg’s performance
on web images using custom query classes. Our model is
able to produce precise results for these categories. For
more qualitative results, please refer to our supplementary
material.
5. Conclusion
To summarize, we propose S-Seg, a simple and intuitive
framework that enables accurate and generalizable open-
vocabulary segmentation. Our algorithm directly trains for
pixel-level feature and language alignment, and does not
require manual segmentation annotations or extensive pre-
training. We hope that our simple yet effective approach will
serve as a solid baseline for future research.
8
References
[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:
Bert pre-training of image transformers.
arXiv preprint
arXiv:2106.08254, 2021. 11
[2] Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick
P´erez. Zero-shot semantic segmentation. NerIPS, 2019. 7
[3] John Canny. A computational approach to edge detection.
IEEE PAMI, 1986. 3
[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Proc. ECCV, 2020.
5
[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In Proc.
ICCV, 2021. 2, 7
[6] Junbum Cha, Jonghwan Mun, and Byungseok Roh. Learning
to generate text-grounded mask for open-world semantic seg-
mentation from only image-text pairs. In Proc. CVPR, 2023.
3, 7
[7] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12m: Pushing web-scale image-text
pre-training to recognize long-tail visual concepts. In Proc.
CVPR, 2021. 2, 11
[8] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
Proc. ECCV, 2018. 7, 11
[9] Xinlei Chen, Saining Xie, and Kaiming He. An empirical
study of training self-supervised vision transformers. In Proc.
ICCV, 2021. 7
[10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classification is not all you need for semantic segmenta-
tion. 2021. 4, 5, 7, 11
[11] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath
Hariharan. Picie: Unsupervised semantic segmentation using
invariance and equivariance in clustering. In Proc. CVPR,
2021. 3
[12] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christo-
pher D Manning. Electra: Pre-training text encoders as dis-
criminators rather than generators. Proc. ICLR, 2020. 11
[13] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson.
Redcaps: Web-curated image-text data created by the people,
for the people. arXiv preprint arXiv:2111.11431, 2021. 11
[14] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. De-
coupling zero-shot semantic segmentation. In Proc. CVPR,
2022. 3, 7, 13
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In Proc. ICLR, 2020.
4
[16] Pinar Duygulu, Kobus Barnard, Joao FG de Freitas, and
David A Forsyth. Object recognition as machine transla-
tion: Learning a lexicon for a fixed image vocabulary. In
Proc. ECCV, 2002. 2
[17] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. IJCV, 2009. 6, 11, 12
[18] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling
open-vocabulary image segmentation with image-level labels.
In Proc. ECCV, 2022. 3, 5, 7, 13
[19] Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noord-
huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,
Yangqing Jia, and Kaiming He.
Accurate, large mini-
batch sgd: Training imagenet in 1 hour.
arXiv preprint
arXiv:1706.02677, 2017. 11
[20] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-
sch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-
laghi Azar, et al. Bootstrap your own latent-a new approach
to self-supervised learning. NerIPS, 2020. 6
[21] Tanmay Gupta, Arash Vahdat, Gal Chechik, Xiaodong Yang,
Jan Kautz, and Derek Hoiem. Contrastive learning for weakly
supervised phrase grounding. In Proc. ECCV, 2020. 3
[22] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah
Snavely, and William T Freeman. Unsupervised semantic
segmentation by distilling feature correspondences. In Proc.
ICLR, 2022. 3
[23] Cong Han, Yujie Zhong, Dengjie Li, Kai Han, and Lin Ma.
Open-vocabulary semantic segmentation with decoupled one-
pass network. In Proc. ICCV, 2023. 3, 7
[24] Kunyang Han, Yong Liu, Jun Hao Liew, Henghui Ding, Jiajun
Liu, Yitong Wang, Yansong Tang, Yujiu Yang, Jiashi Feng,
Yao Zhao, and Yunchao Wei. Global knowledge calibration
for fast open-vocabulary segmentation. In Proc. ICCV, 2023.
3, 7
[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proc. CVPR, 2022. 11
[26] Jyh-Jing Hwang, Stella X Yu, Jianbo Shi, Maxwell D Collins,
Tien-Ju Yang, Xiao Zhang, and Liang-Chieh Chen. Segsort:
Segmentation by discriminative sorting of segments. In Proc.
ICCV, 2019. 3
[27] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant
information clustering for unsupervised image classification
and segmentation. In Proc. ICCV, 2019. 3
[28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In Proc. ICML, 2021. 3
[29] Tapas Kanungo, David M Mount, Nathan S Netanyahu, Chris-
tine D Piatko, Ruth Silverman, and Angela Y Wu. An efficient
k-means clustering algorithm: Analysis and implementation.
IEEE PAMI, 2002. 3, 4
[30] Laurynas Karazija, Iro Laina, Andrea Vedaldi, and Christian
Rupprecht. Diffusion models for zero-shot open-vocabulary
segmentation. arXiv preprint arXiv:2306.09316, 2023. 3
[31] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Koltun, and Ren´e Ranftl. Language-driven semantic seg-
mentation. In Proc. ICLR, 2022. 3, 7, 13
[32] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-
hofer, and Kaiming He. Scaling language-image pre-training
via masking. 2023. 5
9
[33] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan
Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana
Marculescu. Open-vocabulary semantic segmentation with
mask-adapted clip. In Proc. CVPR, 2023. 3, 7
[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In Proc.
ECCV, 2014. 6, 11, 12
[35] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll´ar. Focal loss for dense object detection. In Proc.
ICCV, 2017. 5
[36] Quande Liu, Youpeng Wen, Jianhua Han, Chunjing Xu, Hang
Xu, and Xiaodan Liang. Open-world semantic segmentation
via contrasting and clustering vision-language embedding. In
Proc. ECCV, 2022. 7
[37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proc. ICCV, 2021. 11
[38] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner,
Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit,
Alexey Dosovitskiy, and Thomas Kipf. Object-centric learn-
ing with slot attention. In NerIPS, 2020. 3
[39] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient
descent with warm restarts. Proc. ICLR, 2016. 11
[40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In Proc. ICCV, 2019. 11
[41] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He, and
Tianrui Li. Segclip: Patch aggregation with learnable centers
for open-vocabulary semantic segmentation. In Proc. ICML,
2023. 3, 7
[42] Fangyun Wei Han Hu Xiang Bai Mengde Xu, Zheng Zhang.
Side adapter network for open-vocabulary semantic segmen-
tation. 2023. 3, 7
[43] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In Proc. 3DV, 2016. 5
[44] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu
Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and
Alan Yuille. The role of context for object detection and
semantic segmentation in the wild. In Proc. CVPR, 2014. 6,
11
[45] Norman Mu, Alexander Kirillov, David Wagner, and Sain-
ing Xie. Slip: Self-supervision meets language-image pre-
training. In Proc. ECCV, 2022. 5
[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervision.
In Proc. ICML, 2021. 1, 2, 4, 5, 6, 7, 11, 12, 16
[47] Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi, Yin-
fei Yang, Alexander Toshev, and Jonathon Shlens. Perceptual
grouping in vision-language models. In Proc. ICCV, 2023. 3,
7
[48] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), 2018. 2,
11
[49] Jianbo Shi and Jitendra Malik. Normalized cuts and image
segmentation. IEEE PAMI, 2000. 3, 4
[50] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Geor-
goulis, Marc Proesmans, and Luc Van Gool. Scan: Learning
to classify images without labels. In Proc. ECCV, 2020. 3
[51] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Geor-
goulis, and Luc Van Gool. Unsupervised semantic segmen-
tation by contrasting object mask proposals. In Proc. ICCV,
2021. 3
[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. NerIPS, 2017. 5, 11
[53] Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou,
and Chunhua Shen. Diffumask: Synthesizing images with
pixel-level annotations for semantic segmentation using diffu-
sion models. In Proc. ICCV, 2023. 3
[54] Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt
Schiele, and Zeynep Akata. Semantic projection network
for zero-and few-label semantic segmentation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 8256–8265, 2019. 7
[55] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun. Unified perceptual parsing for scene understanding.
In Proc. ECCV, 2018. 11
[56] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V
Le. Self-training with noisy student improves imagenet clas-
sification. In Proc. CVPR, 2020. 6
[57] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit:
Semantic segmentation emerges from text supervision. In
Proc. CVPR, 2022. 1, 3, 4, 6, 7, 11, 12, 13, 16
[58] Jilan Xu, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu
Qiao, and Weidi Xie. Learning open-vocabulary semantic
segmentation models from natural language supervision. In
Proc. CVPR, 2023. 1, 3, 7
[59] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong
Wang, and Shalini De Mello. Open-vocabulary panoptic
segmentation with text-to-image diffusion models. In Proc.
CVPR, 2023. 3, 7
[60] Andrii Zadaianchuk, Matthaeus Kleindessner, Yi Zhu,
Francesco Locatello, and Thomas Brox. Unsupervised se-
mantic segmentation with self-supervised object-centric rep-
resentations. In Proc. ICLR, 2023. 3
[61] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free
dense labels from clip. In Proc. ECCV, 2022. 3, 6, 7, 12, 13,
16
[62] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanx-
iao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-
training and self-training. NerIPS, 2020. 6
10
A. Implementation details
A.1. S-Seg experiments
Architecture. Our experiments use MaskFormer [10] with
Swin-S [37] backbone and 6-layer transformer decoder with
N = 64 queries. The hidden and output feature dimension
is 256. The language model is a Transformer [52] with 12
layers, each with a hidden dimension of 256. The context
length (maximum length of input text) is set to 77 and the vo-
cabulary size is 49408. We use a 2-layer MLP to project the
visual and text feature into a common embedding space of
dimension 256. We use DINO ViT-S/8 as the pretrained ViT
in pseudo-mask generator and generates 8 pseudo-masks.
Training. During training, we used three publically avail-
able datasets: CC3M [48], CC12M [7], and RedCaps [13],
containing 3M, 12M and 12M image-text pairs, respectively.
Due to storage constraint, we use only first 11M data sam-
ples at a smaller resolution of when using RedCaps dataset.
In total, we use at most 26M image-text pairs for training
- this is an order of magnitude fewer data than CLIP [46]
and 1-4M fewer than GroupViT [57]. The total dataset takes
about 2.4 TB storage space. Table 5 shows our default train-
ing setting. All input images are random resized and cropped
to 224 × 224 in resolution. Following [57], we extract nouns
and verbs from raw sentence because these words are more
likely to describe the image.
Inference. We evaluate S-Seg on the validation set of
three datasets: Pascal VOC 2012 [17], Pascal Context [44]
and COCO [34]. The Pascal VOC dataset contain 1449 im-
ages for testing. Each image is labeled with 20 foreground
classes and a background class. The Pascal Context dataset
contains 5104 testing images with 59 foreground classes and
a background class. The COCO dataset contains 5000 im-
ages for testing with 80 foreground classes and an additional
background class. As in [57], we combine all instances of
the same class to get semantic segmentation mask for each
image in COCO. Following GroupViT [57], we threshold
the maximum probability to obtain background prediction.
During inference, we set the input resolution to 448 × 448,
which is consistent with [57].
A.2. S-Seg+ experiments
Self-training. For self-training experiments, we use Uper-
Net [55] with MAE [25] pretrained ViT backbone. We utilize
a pyramid-structured network to merge the features obtained
from layer 4, 6, 8, and 12 of the ViT, following the imple-
mentation of BEiT [1]. We use the same model that we used
to evaluate our main results to generate training data from
the train set of the respective dataset. Training hyperparam-
eters are provided in Table 6. Following [1, 25], we use a
layerwise learning rate decay [12]. We do not use relative
position embeddings in our backbone ViT model (which is
used by [1, 25] at fine-tuning stage for extra improvement).
config
value
optimizer
AdamW [40]
base learning rate
5e-4
weight decay
0.05
optimizer momentum
β1, β2=0.9, 0.999
batch size
4096
learning rate schedule
cosine decay [39]
warmup epochs [19]
2
training epochs
30
Table 5. S-Seg setting.
config
value
optimizer
AdamW [40]
base learning rate
1e-4
weight decay
0.05
optimizer momentum
β1, β2=0.9, 0.999
batch size
16
learning rate schedule
polynomial decay
warmup iters [19]
1.5k
training iters
20k (voc), 40k (ctxt), 80k (coco)
layer-wise lr decay [12]
0.7
Table 6. S-Seg+ setting.
A.3. Reimplemented baselines
CLIP [46]. We utilized the CLIP ViT-B/16 model along with
the official pretraining weights. The ViT model incorporates
attentional pooling in its last layer, using an additional [CLS]
token to aggregate other tokens. We choose to employ the
value embedding as the representation of each token, as the
query and key embedding of the final layer is not fully trained
during CLIP pretraining (only the similarity between the
query embedding of the [CLS] token and the key embedding
of other tokens is utilized). Finally, we leverage the language
model to encode all classes and classify the visual tokens,
similar to CLIP’s zero-shot classification approach.
MaskCLIP [10]. We use the testing code and weights
provided by the authors, but re-evaluating them on the
commonly-used protocol that includes the background class.
To further assess the efficacy of our approach, as well as base-
line methods, we employed the evaluation metric utilized by
MaskCLIP, which specifically disregards background pixels.
GroupViT [57]. The GroupViT project has provided
pre-trained models for two configurations. Without specific
clarification, we opt to use the model with the highest aver-
age accuracy, which was trained on CC12M, CC15M, and
Redcaps datasets. This particular model also closely aligns
with our method in terms of training data.
Fully supervised models (DeepLabV3+ [8] and Mask-
Former [10]). We leverage public checkpoints when avail-
able. In cases where a checkpoint is not available, we re-
train the model using the original training hyperparameters
(e.g. optimizer, learning rate, momentum, and weight decay)
along with the standard training schedule, which varies de-
pending on the dataset (40k iterations for P. VOC, 80k for P.
11
data
S-Seg
S-Seg+
VOC
Context
COCO
VOC
Context
COCO
12M
44.9
22.9
22.5
53.1
25.5
26.2
15M
45.1(+0.2)
23.8(+0.9)
27.9(+5.4)
54.2(+1.1)
29.2(+3.7)
28.0(+1.8)
26M
53.2(+8.3)
27.9(+5.0)
30.3(+7.8)
62.0(+8.9)
30.2(+4.7)
35.7(+9.5)
(a) Scaling training data provide consistent gain: We train our model using different
size of data: 12M (CC12M), 15M (+CC3M), and 26M (+RedCaps). We note a steady
improvement in the model’s performance as the data size increases.
method
3-Average
12M
15M
26M
w/o self-train
30.1
30.8
37.1
w/ self-train
34.9
37.1
42.6
∆
+4.8
+6.3
+5.5
(b) Self-training offers constant improvement: We ob-
serve that self-training consistently leads to significant im-
provement on performance across 3 datasets.
Table 7. Ablations on data scalability and self-training. We report mIoU evaluated on three datasets. Higher values are better.
BG.
aeroplane
bicycle
bird
boat
bottle
bus
car
cat
chait
cow
table
dog
horse
motorbike
person
plant
sheep
sofa
train
monitor
mIoU
OV Methods
CLIP
13.2
10.4
4.4
8.0
5.9
19.4
27.0
17.5
26.0
3.1
19.6
9.0
21.5
16.8
11.2
11.7
5.2
13.1
7.6
21.1
12.2
13.5
MaskCLIP
41.3
12.8
18.7
22.5
6.7
22.8
50.7
23.4
56.8
13.6
34.1
8.1
46.3
29.5
39.9
22.7
9.5
29.5
25.1
30.8
18.2
26.8
GroupViT
79.0
37.4
29.9
33.3
33.9
64.4
60.2
62.4
76.7
16.2
68.8
28.0
75.9
62.5
64.2
51.6
38.7
63.0
37.4
44.0
38.4
50.8
S-Seg(Ours)
81.0
47.2
40.1
38.6
30.0
63.5
74.6
67.6
75.7
18.6
65.3
34.4
72.2
56.3
68.0
50.7
45.7
60.2
33.6
53.1
41.0
53.2
S-Seg+ (Ours)
86.5
53.8
42.0
48.1
49.3
76.0
84.7
74.5
87.2
17.1
81.8
35.0
83.4
65.2
74.3
65.3
46.6
78.2
40.2
58.5
53.6
62.0
Table 8. Per-category open vocabulary semantic segmentation performance over 21 Pascal VOC classes. Our method surpass baseline
methods such as GroupViT on the Pascal VOC dataset, particularly in segmenting large objects and categories with consistent textures.
Method
OV
Sup.
LVIS
(1103 classes)
ImageNet-S
(919 classes)
CLIP [46]
✓
text
1.3
8.0
MaskCLIP [61]
✓
text
4.3
9.1
GroupViT [57]
✓
text
7.2
32.2
S-Seg (Ours)
✓
text
8.5
34.9
ViT-FCN 1
✗
GT
9.6
40.4
Table 9. Open-vocabulary semantic segmentation results on
LVIS and ImageNet-S. Our method demonstrates competitive per-
formance on these challenging datasets with a significantly larger
number of classes.
Context, and 160k for COCO). We show the performance of
DeepLabV3+ in qualitative comparisons (Fully Sup.).
B. Additional results
B.1. Additional datasets
We evaluate our method on two new challenging datasets
that contain significantly more classes, LVIS (1103 classes)
and ImageNet-S (919 classes). The results are shown in
Table 9. We observe that our model outperforms several
existing open-vocabulary baseline methods and approaches
supervised models, indicating its robustness in challenging
scenarios.
B.2. Ablation results
In Table 7, we show numerical results corresponding to Fig-
ure 10 and 12 in the main paper. As seen from the table,
scaling data and self-training provide consistent gain in per-
formance for our model.
1We also tried DeepLabV3+ but failed to obtain satisfactory results.
B.3. Per-category result
Table 8 presents the mIoU results of our models and baseline
methods on the Pascal VOC dataset, where each class is eval-
uated separately. Our models outperform GroupViT in most
classes, and S-Seg+ achieves superior performance across
all categories. Our models are particularly effective at seg-
menting large objects such as aeroplanes, buses, and trains,
with an average improvement of 11.1 compared to 2.5 for
all classes. This improvement could suggest that our mod-
els benefit from the pseudo-mask generator, which works
better for larger objects (which shows a 83.3% oracle perfor-
mance compared to 77.2% for other classes). On the other
hand, our self-training model performs better on categories
that share consistent texture, such as cats, cows, dogs, and
sheep, with an average improvement of 14.3 compared to 8.8
for all classes. This indicates that self-training can identify
common features and reduce noise in the self-training labels.
B.4. Additional visualizations
Figures 15 and 16 present more detailed open-vocabulary
segmentation results in higher resolution. As shown in the
results, our approach can effectively segment object-centric
images from [17] (fig. 15) as well as context-rich images
from [34] (fig. 16) accurately. Our method can segment ob-
jects based solely on their category name, without requiring
any annotations from specific target datasets during train-
ing. Figure 17 and 18 provide additional comparison with
previous methods.
12
Method
Algorithm
VL Pretrain
Pretrain data
Anno. masks
I-T pairs
Custom model
Loss
mIoU (VOC)
OpenSeg [18]
Adapt&Refine image-level
VL alignment models
Yes (ALIGN)
1800M
Yes (COCO)
-
Not required
image+pixel
77.2
ZegFormer [14]
Directly training
pixel&language alignment
Yes (CLIP)
400M
Yes (COCO)
-
Not required
image+pixel
80.7
MaskCLIP [61]
Adapt&Refine image-level
VL alignment models
Yes (CLIP)
400M
Not required
-
Not required
image
49.5
GroupViT [57]
Extract segments
from language alignment
Not required
-
Not required
30M
Yes (GroupViT)
image
77.2
S-Seg (Ours)
Directly training
pixel&language alignment
Not required
-
Not required
26M
Not required
image+pixel
81.8
Table 10. Comparing S-Seg (Ours) with closely-related methods (OpenSeg [18], ZegFromer [14], MaskCLIP [61], and GroupViT [57]).
We conduct a comparative analysis of our method against a range of closely-related approaches, which are further detailed in Section C.
MaskCLIP
GroupViT
S-Seg (Ours)
image
text
Pseudo masks
image-text 
pair
ZegFormer
image
class labels
GT masks
prompt
GroupViT
image
text
similarity & 
disimilarity
image-text 
pair
group
ViT
image
text
Language 
Model
image-text 
pair
extract
predict
predict
similarity & 
disimilarity
similarity & 
disimilarity
similarity & 
disimilarity
Language 
Model
Language 
Model
Language 
Model
MaskFormer
MaskFormer
Figure 14. Comparing S-Seg (Ours) with closely-related methods. The components in red are those different from S-Seg.
C. Methodology Comparisons
We present a comparative analysis of our method against
several closely-related exemplary approaches. Our method
serves as a connection among these methodologies. The
primary similarities and differences are outlined in Table 10,
with further discussion below.
Relation to OpenSeg. OpenSeg (and similar methods,
e.g. LSeg [31]) refines image-level models like CLIP/ALIGN
by training on annotated semantic masks. The pretrained
image-level model provides language alignment and utilize
ground truth mask for refining pixel-level feature. In contrast,
S-Seg trains directly on pixel features from pseudo-masks
and learns language alignment through text. Conceptually,
S-Seg offers an end-to-end alternative to OpenSeg, with the
added advantage of training exclusively on image-text pairs.
Our approach removes the need for the resource-intensive
VL pretraining step, streamlines the learning process, and
reduces the reliance on extensive supervised data.
Relation to ZegFormer. Our method can be conceptu-
alized as a variant of ”ZegFormer trained from scratch with
pseudo-masks and language,” albeit with notable implemen-
tation distinctions. Training with seen ground truth masks
benefits in-domain classes, but may not extend to unseen
classes. Interestingly, while our method underperforms com-
pared to ZegFormer on seen classes, it surpasses ZegFormer
in handling unseen classes and demonstrates superior aver-
age performance across the dataset. This suggests that our
solution offers better generalization than ZegFormer, despite
not utilizing CLIP, annotated masks, or pixel-wise labels.
The architectural and training similarities between the two
methods suggest that their integration could lead to enhanced
performance, a hypothesis we leave for future exploration.
Relation to CLIP/MaskCLIP. Our method closely par-
allels CLIP in the image-text contrastive training paradigm
and can be seen as a ”CLIP with MaskFormer as the image
encoder,” supplemented by an additional mask supervision
branch. Despite these similarities, CLIP primarily aims to
learn image-level alignment, whereas S-Seg is focused on
pixel-level alignment. This is evident from the fact that
even with the MaskCLIP adaptation, the segmentation per-
formance significantly lags behind that of other compared
methods. This highlights the importance of incorporating
both the MaskFormer and mask supervision in S-Seg.
Relation to GroupViT. GroupViT and S-Seg share a
similar problem setup, where both methods avoid CLIP pre-
training and manual annotations. Methodologically, S-Seg
resembles ”GroupViT with MaskFormer as the grouping
model.” A key difference, however, is that GroupViT ex-
tracts segments from a trained model, while S-Seg directly
predicts segmentation, supervised by pseudo-masks. This
more explicit form of supervision allows S-Seg to leverage
standard segmentation models like MaskFormer more effec-
tively and offers a potentially simpler pathway for updates
with future advancements in segmentation models.
13
Input
Ours
Ours+
Cat          Cow          Car         Boat          Dog          Person          Bicycle
Figure 15. Additional qualitative results of S-Seg in higher resolution (object-centric images). Our method demonstrates robustness
in dealing with challenging scenarios, such as objects with unconventional shapes and poses (row 1), images with unusual color and tone
(row 2), objects of the same class but with differing colors (row 3), objects with the similar color but of different classes (row 4), concealed
objects (row 5), and various other difficult situations.
14
Input
Ours
Ours+
Person          Surfboard          Bird          Banana          Umbrella          Boat          Couch
Figure 16. Additional qualitative results of S-Seg in higher resolution (context-rich images). Although context-rich images pose
challenges in segmentation due to the presence of an increased number of small and cluttered objects, our method can still accurately
segment the objects with precision.
15
Input
CLIP
MaskCLIP
GroupViT
Ours
Ours+
Annotation
Fully Sup.
Figure 17. Additional qualitative comparison with existing methods. CLIP [46] is primarily designed for classification and does not
perform well in segmentation. MaskCLIP [61] adapts CLIP for segmentation, although it produces noisy predictions and cannot handle
background classes. GroupViT [57] is a strong competitor, but it could struggle in challenging scenarios.
16
Input
CLIP
MaskCLIP
GroupViT
Ours
Ours+
Fully Sup.
Ground Truth
Input
CLIP
MaskCLIP
GroupViT
Ours
Ours+
Fully Sup.
Ground Truth
Input
CLIP
MaskCLIP
GroupViT
Ours
Ours+
Fully Sup.
Ground Truth
Input
CLIP
MaskCLIP
GroupViT
Ours
Ours+
Fully Sup.
Ground Truth
Figure 18. Additional qualitative comparison with existing methods (continued).
17
"
"Parameter-efficient fine-tuning (PEFT) has recently emerged as a powerful approach for transfer learning on computer vision tasks, while its effectiveness on medical vision foundation models remains unclear. In this paper, we conduct a detailed empirical study on applying PEFT to chest radiography foundation models, specifically investigating the LoRA method. Our results show that LoRA outperforms full-parameter fine-tuning (FFT) in 13 out of 18 transfer learning tasks using fewer than 1% tunable parameters. This study highlights the potential of PEFT for medical imaging tasks and encourages further research in this area.","Full-parameter fine-tuning (FFT) has been widely adopted for transfer learning, but its dependence on a large number of model parameters can be suboptimal when downstream tasks have limited annotations. Parameter-efficient fine-tuning (PEFT) aims to reduce the number of tunable parameters, and has been successful in language and vision tasks. However, its application to medical imaging tasks, where data annotation is challenging, has not been extensively explored. This paper focuses on using PEFT for chest radiography foundation models, specifically investigating the LoRA method.","Previous work on PEFT for medical image analysis has primarily focused on ImageNet pre-trained models, ignoring more generalizable vision foundation models trained on large-scale medical data. This paper addresses this gap by comparing LoRA to FFT on two self-supervised radiography foundation models across three datasets. The results demonstrate the superiority of LoRA in terms of performance and data efficiency.nan","The study evaluates the effectiveness of LoRA by comparing its performance to FFT on chest radiography foundation models. Three datasets are used: NIH ChestX-ray, CheXpert, and RSNA pneumonia. Two self-supervised foundation models, MRM and MAE, are pre-trained on the MIMIC-CXR dataset. LoRA and FFT are applied to these models and evaluated using AUROC (%). Ablation analyses are conducted to investigate the impact of LoRA rank, pre-training epochs, model scale, and the use of natural images pre-trained foundation models.","The results show that LoRA outperforms FFT in 13 out of 18 transfer learning tasks, demonstrating its universality across different foundation models and datasets. LoRA exhibits high data efficiency, particularly when using only 1% or 10% labeled data. On 100% labeled data, LoRA performs competitively with FFT while using only 1.5% of the parameters. Experiments with larger foundation models and different pre-training strategies further validate the benefits of LoRA.","The study demonstrates the effectiveness of LoRA for parameter-efficient fine-tuning of medical vision foundation models. LoRA consistently outperforms FFT in most transfer learning tasks, achieving state-of-the-art results on several data-efficient learning tasks. These findings highlight the potential of PEFT for medical imaging tasks and encourage further research in this direction.",Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models,"Chenyu Lian, Hong-Yu Zhou, Yizhou Yu, Liansheng Wang","arXiv:2401.12215v1  [cs.CV]  22 Jan 2024
Proceedings of Machine Learning Research 1–6
Less Could Be Better: Parameter-eﬃcient Fine-tuning
Advances Medical Vision Foundation Models
Chenyu Lian1
cylian@stu.xmu.edu.cn
1 School of Informatics, Xiamen University
Hong-Yu Zhou2
whuzhouhongyu@gmail.com
2 Department of Biomedical Informatics, Harvard University
Yizhou Yu3
yizhouy@acm.org
3 Department of Computer Science, The University of Hong Kong
Liansheng Wang∗1
lswang@xmu.edu.cn
Abstract
Parameter-eﬃcient ﬁne-tuning (PEFT) that was initially developed for exploiting pre-
trained large language models has recently emerged as an eﬀective approach to perform
transfer learning on computer vision tasks. However, the eﬀectiveness of PEFT on medical
vision foundation models is still unclear and remains to be explored. As a proof of concept,
we conducted a detailed empirical study on applying PEFT to chest radiography foundation
models. Speciﬁcally, we delved into LoRA, a representative PEFT method, and compared
it against full-parameter ﬁne-tuning (FFT) on two self-supervised radiography foundation
models across three well-established chest radiograph datasets. Our results showed that
LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using
fewer than 1% tunable parameters. Combining LoRA with foundation models, we set up
new state-of-the-art on a range of data-eﬃcient learning tasks, such as an AUROC score of
80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more
attention from the community in the use of PEFT for transfer learning on medical imaging
tasks. Code and models are available at https://github.com/RL4M/MED-PEFT.
Keywords: Transfer learning, Medical vision foundation models, Chest X-ray.
1. Introduction
Full-parameter ﬁne-tuning (FFT) has long been recognized and adopted as a superior
technique to do transfer learning (He et al., 2022; Wang et al., 2023; Zhou et al., 2023a,c;
Yu et al., 2020). However, foundation models usually have a large number of parameters,
and ﬁne-tuning the full model weights can be a sub-optimal choice when the downstream
task only has limited annotations. This contrast deserves more attention in medical imag-
ing tasks where annotation is often hard to access due to issues like privacy and safety and
also the rare nature of certain diseases. On the other hand, parameter-eﬃcient ﬁne-tuning
(PEFT) (Houlsby et al., 2019; Hu et al., 2021; Liu et al., 2022) was proposed to largely
reduce the number of model parameters to be tuned and has been widely used in both
language (He et al., 2021; Zhang et al., 2023; Ponti et al., 2023) and vision tasks (Jia et al.,
2022; Sung et al., 2022; Yang et al., 2023).
∗ Corresponding author
© CC-BY 4.0, C. Lian, H.-Y. Zhou, Y. Yu & L. Wang.
Lian Zhou Yu Wang
Table 1: Comparison of the classiﬁcation results of FFT and LoRA on MAE and MRM,
while 1%, 10%, and 100% denote the ratios of labeled data used for ﬁne-tuning.
Pre-trained
Models
Transfer
Methods
NIH
CheXpert
RSNA
1%
10%
100%
1%
10%
100%
1%
10%
100%
MAE
FFT
74.2
82.2
85.6
87.3
90.3
91.8
89.6
90.5
93.1
LoRA
77.1
(+2.9)
82.9
(+0.7)
85.7
(+0.1)
88.4
(+1.1)
91.1
(+0.8)
91.1
(-0.7)
89.9
(+0.3)
91.9
(+1.4)
93.3
(+0.2)
MRM
FFT
80.1
84.1
85.9
90.5
91.5
91.6
91.3
92.8
93.3
LoRA
80.6
(+0.5)
84.0
(-0.1)
85.8
(-0.1)
90.7
(+0.2)
92.0
(+0.5)
91.5
(-0.1)
91.2
(-0.1)
93.1
(+0.3)
93.5
(+0.2)
More recently, some studies tried applying PEFT for medical image analysis (Dutt et al.,
2023; Zhu et al., 2023). However, one limitation of these work is that they only investigated
ImageNet (Deng et al., 2009) pre-trained models and ignored the more generalizable vision
foundation models that were trained on large-scale medical data with self-supervised learn-
ing (Zhou et al., 2023b; Jiang et al., 2023). In this paper, we focus on LoRA (Hu et al.,
2021), a representative PEFT method, comparing it to FFT on two self-supervised radio-
graphy foundation models across three well-established chest radiograph datasets. Experi-
mental results indicate that in 13 out of 18 transfer learning tasks, LoRA exhibits superior
performance over FFT, sometimes by notable margins. For instance, on the NIH ChestX-
ray dataset with merely 1% labeled data, LoRA outperforms FFT by 2.9% with only 0.3%
tunable parameters.
2. Experiments and Analyses
2.1. Settings
Datasets. Three chest radiograph datasets were adopted to evaluate the performance of
transfer learning, including NIH ChestX-ray (NIH) (Wang et al., 2017), CheXpert(Irvin et al.,
2019), and RSNA pneumonia (RSNA) (Shih et al., 2019). To analyze the data eﬃciency of
diﬀerent ﬁne-tuning methods, we also presented results with diﬀerent labeling ratios. We
employed the same data splits and evaluation metrics as of (Zhou et al., 2023a) except that
we used the oﬃcial test set instead of the validation set of CheXpert.
Chest Radiography Foundation Models. We adopted two self-supervised foundation
models, MRM (Zhou et al., 2023a) and MAE (He et al., 2022). Both of them were pre-
trained on the MIMIC-CXR (Johnson et al., 2019) dataset, based on which LoRA and
FFT were applied and compared.
2.2. Eﬀectiveness of LoRA
Table 1 compares the classiﬁcation results of FFT and LoRA based on MAE and MRM,
measured by AUROC (%). Improvements can be observed in 13 out of 18 tasks, mani-
festing the universality of LoRA on diﬀerent radiography foundation models and datasets.
Moreover, the outstanding performance of LoRA on 1% and 10% labeled data indicates its
high data eﬃciency, which is particularly meaningful for medical imaging limited by the
scarcity of data. On 100% labeled data, LoRA performs competitively with FFT but by
tuning only 1.5% parameters, showing the eﬃciency in computation and storage.
2
PEFT for Medical Vision Foundation Models
Table 2: LoRA ranks analysis.
LoRA Rank
2
4
8
1%
80.4
80.6
80.4
LoRA Rank
8
16
32
10%
84.0
84.1
84.0
LoRA Rank
16
32
64
100%
85.7
85.8
85.8
Table 3: Comparison of pre-training epochs.
Methods
Epochs of Pretraining
AUROC (%)
FFT
100
74.4
200
74.2 (-0.2)
LoRA
100
75.9
200
77.1 (+1.2)
2.3. Ablation Analyses
LoRA Rank Analysis. We compare the performances of diﬀerent ranks of LoRA on 1%,
10%, and 100% labeled data of NIH based on MRM, showing that the ranks of LoRA should
be increased accordingly as the data scale. AUROC (%) scores are reported in Table 2.
Pre-training Epochs Analysis. Pre-training was conducted on the MIMIC-CXR dataset
using MAE (He et al., 2022) for 100 and 200 epochs. As shown in Table 3, 1.2% improve-
ment on 1% labeled data of NIH is observed when the pre-training epochs are extended
from 100 to 200, while no improvement is witnessed for FFT. We hypothesize that LoRA
beneﬁts from the small number of tuned parameters (0.3%), mitigating the catastrophic
forgetting.
2.4. More Analyses on Other Vision Foundation Models
Scaling up the Foundation Models. We conducted MAE pre-training using MIMIC-
CXR images on ViT-Large (Dosovitskiy et al., 2020) for 200 epochs. Table 4 shows the
further improvements when scaling up the transformer network. It is noteworthy that the
result of LoRA based on ViT-Base is even 0.9% higher than the one of full-parameter ﬁne-
tuning on ViT-Large, and when adopting LoRA on ViT-Large, the AUROC of NIH 1% can
be further promoted to 77.7%.
Fine-tuned on Natural Images Pre-trained Foundation Models. The results on Ta-
ble 5 show that when adopting the natural images pre-trained models Dinov2 (Oquab et al.,
2023) by FFT, the performance is substantially below the baseline. While showing that
ChestX-ray pre-training is still necessary to ensure downstream performance, the introduc-
tion of LoRA signiﬁcantly mitigates the performance gap caused by diﬀerent modalities.
Table 4: Comparison of model scales.
Method
ViT Scale
AUROC (%)
FFT
Base
74.2
Large
76.2 (+2.0)
LoRA
Base
77.1 (+2.9)
Large
77.7 (+3.5)
Table 5: On natural foundation models.
Method
Model
AUROC (%)
FFT
MAE ViT-B16
74.2
FFT
Dinov2 ViT-B14
66.6 (-7.6)
Dinov2 ViT-L14
70.9 (-3.3)
LoRA
Dinov2 ViT-B14
70.3 (-3.9)
Dinov2 ViT-L14
72.5 (-1.7)
3
Lian Zhou Yu Wang
References
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database.
In 2009 IEEE conference on computer vision and
pattern recognition, pages 248–255. Ieee, 2009.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain
Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929, 2020.
Raman Dutt, Linus Ericsson, Pedro Sanchez, Sotirios A Tsaftaris, and Timothy Hospedales.
Parameter-eﬃcient ﬁne-tuning for medical image analysis: The missed opportunity. arXiv
preprint arXiv:2305.08252, 2023.
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neu-
big.
Towards a uniﬁed view of parameter-eﬃcient transfer learning.
arXiv preprint
arXiv:2110.04366, 2021.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked
autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 16000–16009, 2022.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Larous-
silhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-eﬃcient transfer
learning for nlp. In International Conference on Machine Learning, pages 2790–2799.
PMLR, 2019.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685, 2021.
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute,
Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al.
Chexpert:
A large chest radiograph dataset with uncertainty labels and expert comparison.
In
Proceedings of the AAAI conference on artiﬁcial intelligence, volume 33, pages 590–597,
2019.
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hari-
haran, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer
Vision, pages 709–727. Springer, 2022.
Yankai Jiang, Mingze Sun, Heng Guo, Xiaoyu Bai, Ke Yan, Le Lu, and Minfeng Xu.
Anatomical invariance modeling and semantic alignment for self-supervised learning in
3d medical image analysis. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 15859–15869, 2023.
Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-
ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng.
4
PEFT for Medical Vision Foundation Models
Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. arXiv
preprint arXiv:1901.07042, 2019.
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal,
and Colin A Raﬀel. Few-shot parameter-eﬃcient ﬁne-tuning is better and cheaper than
in-context learning. Advances in Neural Information Processing Systems, 35:1950–1965,
2022.
Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil
Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,
et al.
Dinov2:
Learning robust visual features without supervision.
arXiv preprint
arXiv:2304.07193, 2023.
Edoardo Maria Ponti, Alessandro Sordoni, Yoshua Bengio, and Siva Reddy. Combining
parameter-eﬃcient modules for task-level generalisation. In Proceedings of the 17th Con-
ference of the European Chapter of the Association for Computational Linguistics, pages
687–702, 2023.
George Shih, Carol C Wu, Safwan S Halabi, Marc D Kohli, Luciano M Prevedello, Tessa S
Cook, Arjun Sharma, Judith K Amorosa, Veronica Arteaga, Maya Galperin-Aizenberg,
et al. Augmenting the national institutes of health chest radiograph dataset with expert
annotations of possible pneumonia. Radiology. Artiﬁcial intelligence, 1(1), 2019.
Yi-Lin Sung, Jaemin Cho, and Mohit Bansal.
Vl-adapter: Parameter-eﬃcient transfer
learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 5227–5237, 2022.
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Ag-
garwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a
foreign language: Beit pretraining for vision and vision-language tasks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19175–
19186, 2023.
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M
Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-
supervised classiﬁcation and localization of common thorax diseases. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages 2097–2106, 2017.
Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu Li. Aim: Adapt-
ing image models for eﬃcient video action recognition. arXiv preprint arXiv:2302.03024,
2023.
Shuang Yu, Hong-Yu Zhou, Kai Ma, Cheng Bian, Chunyan Chu, Hanruo Liu, and Yefeng
Zheng. Diﬃculty-aware glaucoma classiﬁcation with multi-rater consensus modeling. In
Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd In-
ternational Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I 23, pages
741–750. Springer, 2020.
5
Lian Zhou Yu Wang
Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu
Chen, and Tuo Zhao.
Adaptive budget allocation for parameter-eﬃcient ﬁne-tuning.
arXiv preprint arXiv:2303.10512, 2023.
Hong-Yu Zhou, Chenyu Lian, Liansheng Wang, and Yizhou Yu.
Advancing radiograph
representation learning with masked record modeling. arXiv preprint arXiv:2301.13155,
2023a.
Hong-Yu Zhou, Chixiang Lu, Chaoqi Chen, Sibei Yang, and Yizhou Yu.
A uniﬁed vi-
sual information preservation framework for self-supervised pre-training in medical image
analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023b.
Hong-Yu Zhou, Yizhou Yu, Chengdi Wang, Shu Zhang, Yuanxu Gao, Jia Pan, Jun Shao,
Guangming Lu, Kang Zhang, and Weimin Li.
A transformer-based representation-
learning model with uniﬁed processing of multimodal input for clinical diagnostics. Nature
Biomedical Engineering, pages 1–13, 2023c.
Yitao Zhu, Zhenrong Shen, Zihao Zhao, Sheng Wang, Xin Wang, Xiangyu Zhao, Dinggang
Shen, and Qian Wang. Melo: Low-rank adaptation is better than ﬁne-tuning for medical
image diagnosis. arXiv preprint arXiv:2311.08236, 2023.
6
"
"We present the first word-level Bangla Sign Language dataset (BdSL40), consisting of 611 videos over 40 BdSL words, and a novel Graph Neural Network approach for classifying the dataset. The proposed GNN achieved an F1 score of 89%. The study highlights the significant lexical similarities between BdSL, West Bengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in the literature.","Sign language is a vital mode of communication for the deaf community, but its practical application varies due to regional expressions and alphabets. Complexities in Bangla Sign language hinder learning and communication with hearing individuals, necessitating an interpreter. Lack of datasets hinders research in Bangla Sign Language (BdSL) recognition, particularly at the word level.","Recent works on sign language recognition are classified into isolated and continuous sign language recognition. However, the lack of available datasets has hindered research into Bangla Sign Language (BdSL) recognition. In particular, there has been no concerted effort to collect word-level data on BdSL, which has limited the application of machine learning in this area.nan","Our dataset BdSL40 comprises of 611 videos over 40 BdSL words. For classification, we propose two approaches: 3D Convolutional Neural Network (3D-CNN), achieving 82.43% accuracy, and a novel method using key points and spatiotemporal graph with Graph Neural Network (GNN), achieving 89% accuracy.","The Spatio-Temporal GNN outperformed the 3D-CNN approach, achieving 89% accuracy compared to 82.43%. Further analysis showed that the GNN method had higher precision, recall, and F1 scores for most classes, demonstrating its effectiveness in distinguishing between sign language gestures.","The study presents the first word-level Bangla Sign Language dataset and two approaches for its classification, emphasizing the potential of GNNs in sign language recognition. The dataset and source code are publicly available, encouraging further research in this area.",Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks for Accurate Bangla Sign Language Recognition,"Haz Sameen Shahgir, Khondker Salman Sayeed, Md Toki Tahmid, Tanjeem Azwad Zaman, Md. Zarif Ul Alam","AI for Bangla 2.0
Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks for
Accurate Bangla Sign Language Recognition
Haz Sameen Shahgir1, Khondker Salman Sayeed1,
Md Toki Tahmid1, Tanjeem Azwad Zaman1
Md. Zarif Ul Alam1*
1 Department of CSE, BUET
Abstract
Recent advances in Deep Learning and Computer Vision have
been successfully leveraged to serve marginalized communi-
ties in various contexts. One such area is Sign Language -
a primary means of communication for the deaf community.
However, so far, the bulk of research efforts and investments
have gone into American Sign Language, and research activ-
ity into low-resource sign languages - especially Bangla Sign
Language - has lagged significantly. In this research paper,
we present a new word-level Bangla Sign Language dataset -
BdSL40 - consisting of 611 videos over 40 words, along with
two different approaches: one with a 3D Convolutional Neu-
ral Network model and another with a novel Graph Neural
Network approach for the classification of BdSL40 dataset.
This is the first study on word-level BdSL recognition, and
the dataset was transcribed from Indian Sign Language (ISL)
using the Bangla Sign Language Dictionary (1997). The pro-
posed GNN model achieved an F1 score of 89%. The study
highlights the significant lexical and semantic similarity be-
tween BdSL, West Bengal Sign Language, and ISL, and the
lack of word-level datasets for BdSL in the literature. The
dataset and source code are publicly available.
Introduction
Sign language is a vital mode of communication for the
deaf community. While sign language dictionaries provide a
foundation for learning sign language, the practical applica-
tion of sign language varies in terms of phonological, mor-
phological, grammatical, and lexical aspects 1. The diver-
sity of sign languages is influenced by regional expressions
and language alphabets, resulting in multiple sign languages,
such as American, Arabic, French, Spanish, Chinese, and In-
dian. In Bangladesh, the lack of proper devices or methods
that can serve as interpreters makes it necessary for hear-
ing individuals to learn sign language to communicate with
the deaf. However, the complexity of the Bangla Sign lan-
guage, which utilizes both hand and body gestures, poses a
challenge for individuals seeking to learn it. Consequently,
deaf individuals have difficulty teaching their sign language
to hearing individuals, creating a distance between the deaf
and the broader society. Therefore, there is a pressing need
*Corresponding Author.
E-mail: 1705010@ugrad.cse.buet.ac.bd
1https://github.com/Patchwork53/BdSL40 Dataset AI for Bangla 2.0 Honorable Mention
to develop an interpreter that can translate sign languages
into text or speech, enabling deaf individuals to communi-
cate effectively with society. This necessity has made the
recognition of Bangladeshi sign language (BdSL) a signif-
icant and challenging topic in the field of computer vision
and machine learning.
While current works on sign language recognition are
classified into two categories - isolated and continuous sign
language recognition, the lack of available datasets has hin-
dered research into Bangla Sign Language (BdSL) recog-
nition. In particular, there has been no concerted effort to
collect word-level data on BdSL, which has limited the ap-
plication of machine learning in this area.
Our contributions are summarized as follows:
• We present the first word-level Bangla Sign Language
dataset (BdSL40), consisting of 611 videos over 40 BdSL
words, with 8 to 22 video clips per word. This dataset
addresses the lack of available word-level datasets for
BdSL, enabling research into sign language recognition
using machine learning.
• We propose a model for the classification of BdSL40
using a 3D Convolutional Neural Network (3D-CNN)
(Tran et al. 2015), which achieved a peak accuracy of
82.43% on an 80-20 split of the dataset. This provides a
strong baseline for future research in this area.
• We also propose a novel method for BdSL40 classifica-
tion by extracting key points from the videos and con-
structing a spatiotemporal graph. We then use a Graph
Neural Network (Yu, Yin, and Zhu 2017) to classify the
dataset, achieving a peak accuracy of 89% on an 80-20
split of the dataset. This method provides an alternative
approach to the classification of sign language recogni-
tion and demonstrates the potential of GNNs in this area.
1
arXiv:2401.12210v1  [cs.CV]  22 Jan 2024
Dataset Creation
Owing to geographical proximity and significant cultural
crossover, the sign languages of Bangladesh (BdSL), West
Bengal (WBSL), and India (ISL) bear striking similarities
with one another. BdSL and WBSL have high semantic sim-
ilarity and are mutually comprehensible while BdSL and
ISL have 75% lexical similarity but differ in their mean-
ing (Johnson and Johnson 2016). For example, the sign for
“Fish” in ISL is the same as for “Tortoise” in BdSL. Out
of the three mentioned sign languages, only ISL has a siz-
able word-level dataset available to the public: INCLUDE
by Sridhar et al. (2020). We went over the INCLUDE dataset
and consulted the Bangladesh Sign Language Dictionary
(1997) to find the Bangla meaning of the signs in the IN-
CLUDE dataset. From the 263 words in INCLUDE, we
were able to collect 40 terms of which 28 words had the
same meaning and sign in both ISL and BdSL, and 12 had
the same sign but a different meaning in BdSL. It is to be
noted that the 40 words had the same signing motions in
both BdSL and ISL. We noticed similarities between other
words but did not include them in BdSL40 since the signing
motions were not the same.
In total, there are 611 videos over 40 BdSL words, with 8
to 22 video clips per word.
Proposed Methodology
We present the methodology for our 2 different approaches
in this section.
Video ResNet
In this section, we describe our methodology for classifying
the Bangla Sign Language 40 (BdSL40) dataset using Video
ResNet (Tran et al. 2018). The proposed methodology con-
sists of two main stages: dataset preprocessing and model
training.
Dataset Preprocessing.
The BdSL40 dataset comprises
611 videos. Each video has a resolution of 1080x1920 and
a frame rate of 30 frames per second. Before training the
model, we first preprocess the dataset to ensure that the data
is in a format suitable for training. First, we split the dataset
into training and testing sets with an 80-20 ratio. Then, we
extracted the frames from each video and resized them to
100x100 to reduce the dimensionality of the data.
Model Training.
We trained the model using the Video
ResNet architecture, which is a variant of the ResNet ar-
chitecture that is specifically designed for video data. The
Video ResNet architecture consists of three main compo-
nents: a 3D convolutional backbone, a temporal pooling
layer, and a fully connected layer. The 3D convolutional
backbone extracts spatio-temporal features from the input
video frames. The temporal pooling layer aggregates the
spatio-temporal features across time to produce a fixed-
length feature vector for each video clip. The fully connected
layer then maps the feature vector to the output classes.
During training, the first 6 and last 8 frames of each video
were skipped to remove any redundant information. Frames
were normalized by subtracting the mean value of 0.5 and
dividing by the standard deviation of 0.5 for each color chan-
nel. The model was trained for 120 epochs with a batch size
of 64 and a learning rate of 5e-5.
Spatio Temporal Graph Neural Network
In this section, we describe our methodology for classify-
ing the Bangla Sign Language 40 (BdSL40) dataset using
Spatio-Temporal Graph Neural Network.
Dataset Preprocessing
For the preprocessing phase, we
need to extract the hands keypoints data using a pre-trained,
ready-to-use solution. Most of the available methods rely on
the use of CNNs and are trained on large amounts of data.
Due to its fast inference speed, high accuracy, and simple
use, we chose the framework MediaPipe Hands (Lugaresi
et al. 2019) for our task. It extracts 21 x-y-z hand key points
of the human body from each frame. The hand key point
data was acquired via the Python API of MediaPipe Hands,
using a minimum detection confidence of 0.8 and a mini-
mum tracking confidence of 0.5 for tracking key points in
consecutive frames.
Spatio-Temporal Graph Construction
Our proposed
Spatio-Temporal Graph Neural Network is fed by a four-
dimensional matrix in the shape of [N, C, T, V ] where
N denotes the batch size, C denotes the number of input
features(x-y-z-coordinate), T denotes the input time steps
and V denotes the graph vertices (joints).
Our method uses the 2-stream Adaptive Graph Convo-
lutional Network (AGCN) proposed by Li et al. (2018).
The adjacency matrix for AGCN is composed of three sub-
matrices:
1. The inward-links starting from the wrist joint,
2. The outward links pointing in the opposite direction, and
3. The self-links of each joint.
Thus, the matrix is of the shape [V , V , 3], where V is
21 in this work. The Hand-AGCN model used in this work
is a stack of 7 AGCN blocks with increasing output feature
dimensions. A preceding batch normalization layer is added
for input data normalization. A global average pooling layer
(GAP) followed by a fully connected layer (FC) maps the
output features to the corresponding output classes.
Hand Graph Modeling
The MediaPipe Hands key point
extraction method predicts the x-y-z coordinates of 21 hand
joints; four joints per finger plus an additional wrist joint.
For the definition of the underlying graph, each of the joints
is connected to its natural neighbor, resulting in a graph of
21 vertices and 20 edges. These might be too few connec-
tions for the fine-grained hand movements. To obtain more
semantic information in the hand graph, two types of addi-
tional joints were added. The first type of added joint links
the fingertips to the base of the right neighbor finger. The
second type of additional joint links the fingertips to the mid-
dle segment of the same finger. These supplementary links
help to retrieve more information about the different states
of the hand. The first type contains data about the horizontal
and vertical distance of two fingers and can therefore help
to encode the overlapping or spreading of two fingers. The
second type encodes the bending of the fingers.
2
Figure 1: BdSL40 example data: Frames extracted from gesture labeled Student
Figure 2: BdSL40 example data: Frames extracted from gesture labeled Tortoise
Figure 3: Classification pipeline of a specific sign language gesture. First, the frames are extracted from the video. Then they
are fed into a pretrained VideoResNet Model which does the classification using 3D Convolutional Networks
3
Figure 4: Spatio Temporal Graph Construction
Model Training
Once the spatio-temporal graphs were
constructed, they were used to train a Spatio-Temporal
Graph Neural Network (GNN) model for classification. The
2s-AGCN algorithm was used for classification.
The batch size was determined in a preliminary experi-
ment using mini-batches of 32, 64, and 128, with 64 result-
ing in the highest accuracy in the validation set. The number
of time steps T was empirically specified to be 50 and V
was set to be 21. The model was trained for 5 epochs with
a batch size of 64 and a learning rate of 1e-2. During the
experiments, it was observed that after 5 epochs none of the
models showed any substantial accuracy increase.
Result Analysis
In this study, we evaluated two methods for classifying
the BdSL40 dataset: Video ResNet and Spatio-Temporal
GNN. The Video ResNet model achieved an accuracy of
82.43% On the other hand, the Spatio Temporal GNN model
achieved an accuracy of 89%
To further analyze the performance of the two methods,
we also calculated precision, recall, and F1 score for each
class. The results showed that the Spatio-Temporal GNN
method had higher precision, recall, and F1 scores for most
of the classes compared to the Video ResNet method. This
suggests that the Spatio-Temporal GNN method is more
effective in distinguishing between different sign language
gestures.
Table 1: Classification results for the BdSL40 dataset using
2s-AGCN
Sign
Precision
Recall
F1-Score
bad
0.922
0.969
0.945
book
0.728
0.780
0.753
brown
0.887
0.913
0.900
bed
0.925
0.925
0.925
camera
0.933
0.944
0.939
cheap
0.949
0.944
0.947
cow
0.933
0.863
0.897
crane
0.446
0.944
0.606
deaf
0.915
0.872
0.893
friend
0.895
0.813
0.852
fulfill
0.906
0.868
0.886
glad
0.906
0.853
0.879
heavy
0.856
0.834
0.845
i
0.930
0.901
0.915
india
0.948
0.953
0.950
lawyer
0.989
0.924
0.956
life
0.903
0.914
0.909
money
0.915
0.941
0.928
more
0.937
0.886
0.911
new
0.913
0.952
0.932
noon
0.890
0.920
0.905
pant
0.748
0.825
0.785
quiet
0.902
0.934
0.918
rich
0.898
0.840
0.868
ring
0.922
0.784
0.847
shirt
0.929
0.926
0.927
shoes
0.809
0.878
0.842
skirt
0.712
0.407
0.517
soap
0.782
0.539
0.638
square
0.926
0.948
0.937
straight
0.780
0.656
0.713
student
0.934
0.881
0.906
teacher
0.950
0.833
0.887
telephone
0.884
0.884
0.884
thick
0.955
0.923
0.939
time
0.605
0.721
0.658
tortoise
0.822
0.694
0.753
winter
0.891
0.936
0.913
yesterday
0.863
0.852
0.857
you
0.856
0.633
0.728
4
References
Johnson, R. J.; and Johnson, J. E. 2016. Distinction between
west Bengal sign language and Indian sign language based
on statistical assessment.
Sign Language Studies, 16(4):
473–499.
Li, R.; Wang, S.; Zhu, F.; and Huang, J. 2018. Adaptive
graph convolutional neural networks. In Proceedings of the
AAAI conference on artificial intelligence, volume 32.
Lugaresi, C.; Tang, J.; Nash, H.; McClanahan, C.; Uboweja,
E.; Hays, M.; Zhang, F.; Chang, C.-L.; Yong, M. G.; Lee, J.;
et al. 2019. Mediapipe: A framework for building perception
pipelines. arXiv preprint arXiv:1906.08172.
Sridhar, A.; Ganesan, R. G.; Kumar, P.; and Khapra, M.
2020. Include: A large scale dataset for indian sign language
recognition. In Proceedings of the 28th ACM international
conference on multimedia, 1366–1375.
Tran, D.; Bourdev, L.; Fergus, R.; Torresani, L.; and Paluri,
M. 2015. Learning spatiotemporal features with 3d convo-
lutional networks. In Proceedings of the IEEE international
conference on computer vision, 4489–4497.
Tran, D.; Wang, H.; Torresani, L.; Ray, J.; LeCun, Y.; and
Paluri, M. 2018. A closer look at spatiotemporal convolu-
tions for action recognition. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition, 6450–
6459.
Yu, B.; Yin, H.; and Zhu, Z. 2017. Spatio-temporal graph
convolutional networks: A deep learning framework for traf-
fic forecasting. arXiv preprint arXiv:1709.04875.
5
"
"Side-channel analysis has proven effective at detecting hardware Trojans in integrated circuits (ICs). However, most detection techniques rely on large external probes and antennas for data collection and require a long measurement time to detect Trojans. Such limitations make these techniques impractical for run-time deployment and ineffective in detecting small Trojans with subtle side-channel signatures. To overcome these challenges, we propose a Programmable Sensor Array (PSA) for run-time hardware Trojan detection, localization, and identification. PSA is a tampering-resilient integrated on-chip magnetic field sensor array that can be re-programmed to change the sensors’ shape, size, and location. Using PSA, EM side-channel measurement results collected from sensors at different locations on an IC can be analyzed to localize and identify the Trojan. The PSA has better performance than conventional external magnetic probes and state-of-the-art on-chip single-coil magnetic field sensors. We fabricated an AES-128 test chip with four AES Hardware Trojans. They were successfully detected, located, and identified with the proposed on-chip PSA within 10 milliseconds using our proposed cross-domain analysis.","Hardware Trojans (HTs) are malicious modifications inserted into ICs by adversaries during the fabrication or design process. These alterations can disrupt system functionality, causing security and privacy issues such as privacy leakage, denial-of-service, and privilege escalation, etc. As the reliance on third-party designs and manufacturing continues to grow, the threat posed by HTs becomes more alarming. Over the years, researchers have discovered that HTs often produce detectable anomalies through side-channels like power consumption and electromagnetic (EM) emanations. The anomalies in side-channels can be detected after the chip is fabricated, either during the test phase or in run time, but often require costly external sensing setups and a golden model, a Trojan-free circuit used as a reference. This makes deploying some of these approaches in run time not applicable. Moreover, many side-channel oriented techniques suffer from low resolution of the collected traces.","nanAs a result, on-chip sensors have gained attraction as a preferred solution. These sensors can intrinsically collect traces from EM side-channels through magnetic field coupling at run time with higher resolutions, thus ruling out the need for external measurement equipment (e.g., external probes, moving stages, etc.). Previous research by Jiaji et al. [1] used a single coil covering the entire chip area, which has low signal-to-noise ratio (SNR) issues. Consequently, this approach couldn’t detect small HTs with few gates and required over 10,000 measurements to detect larger HTs. Its long mean time to detect (MTTD) limits its practical effectiveness. In this paper, we propose a Programmable Sensor Array (PSA) to address the limitations of Jiaji’s work. Our improved design, shown in Figure 1a, consists of a crossbar-like structure.","The PSA can be programmed to different sizes and shapes, allowing it to cover or uncover specific parts of the chip for better SNR and HT localization. At each crossing point in the wire grid, a transmission gate (T-gate) controls the node’s connectivity. This allows the sensor array to have different shapes, locations, and sizes. The size of a single sensor within the PSA can also be programmed to approximately match the size of a HT, ensuring the highest magnetic field emanations from HTs are captured. The improved SNR of PSA enables almost instant runtime detection, reducing MTTD significantly.","The contributions of this paper can be summarized as follows:
• A novel on-chip PSA for EM side-channel measurement with big advantages over existing techniques was proposed, developed and validated.
• A silicon demonstration of the proposed PSA to detect and localize active HTs on an AES design implemented with TSMC 65nm technology.
• Significant SNR improvements with the proposed PSA compared to prior work.
• Experimental validation of the PSA’s capability to detect, locate, and identify digital HTs within 10 ms with the proposed novel cross-domain analysis technique.","In this paper, we proposed an integrated on-chip EM sensor array, PSA, that can be programmed to change its shape, size, and location. The PSA was implemented and tested in a 65nm technology node. The proposed PSA has a lower overhead than existing state-of-the-art on-chip coil sensor designs with much higher SNR and the capability of locating HTs. Furthermore, the MTTD during the runtime verification has been significantly reduced to less than 10 ms. The proposed cross-domain analysis can identify prominent frequency components that include the activity information of different Trojans in the frequency domain and differentiate different Trojans by examining the time-domain signals of these prominent frequency components by switching to the time domain.",Programmable EM Sensor Array for Golden-Model Free Run-time Trojan Detection and Localization,"Hanqiu Wang, Max Panoff, Zihao Zhan, Shuo Wang, Christophe Bobda, Domenic Forte","Programmable EM Sensor Array for Golden-Model
Free Run-time Trojan Detection and Localization
Hanqiu Wang
ECE Department, University of Florida
Gainesville, FL 32611 USA
wanghanqiu@ufl.edu
Max Panoff
ECE Department, University of Florida
Gainesville, FL 32611 USA
m.panoff@ufl.edu
Zihao Zhan
ECE Department, University of Florida
Gainesville, FL 32611 USA
zhan.zihao@ufl.edu
Shuo Wang
ECE Department, University of Florida
Gainesville, FL 32611 USA
shuo.wang@ece.ufl.edu
Christophe Bobda
ECE Department, University of Florida
Gainesville, FL 32611 USA
cbobda@ece.ufl.edu
Domenic Forte
ECE Department, University of Florida
Gainesville, FL 32611 USA
dforte@ece.ufl.edu
Abstract—Side-channel analysis has been proven effective at
detecting hardware Trojans in integrated circuits (ICs). However,
most detection techniques rely on large external probes and
antennas for data collection and require a long measurement
time to detect Trojans. Such limitations make these techniques
impractical for run-time deployment and ineffective in detecting
small Trojans with subtle side-channel signatures. To overcome
these challenges, we propose a Programmable Sensor Array
(PSA) for run-time hardware Trojan detection, localization, and
identification. PSA is a tampering-resilient integrated on-chip
magnetic field sensor array that can be re-programmed to change
the sensors’ shape, size, and location. Using PSA, EM side-
channel measurement results collected from sensors at different
locations on an IC can be analyzed to localize and identify
the Trojan. The PSA has better performance than conventional
external magnetic probes and state-of-the-art on-chip single-coil
magnetic field sensors. We fabricated an AES-128 test chip with
four AES Hardware Trojans. They were successfully detected,
located, and identified with the proposed on-chip PSA within 10
milliseconds using our proposed cross-domain analysis.
Index Terms—EM sensor, side-channel, Hardware Trojan
I. INTRODUCTION
Hardware Trojans (HTs) are malicious modifications in-
serted into ICs by adversaries during the fabrication or design
process. These alterations can disrupt system functionality,
causing security and privacy issues such as privacy leakage,
denial-of-service and privilege escalation, etc. As the reliance
on third-party designs and manufacturing continues to grow,
the threat posed by HTs becomes more alarming. Over the
years, researchers have discovered that HTs often produce de-
tectable anomalies through side-channels like power consump-
tion and electromagnetic (EM) emanations. The anomalies in
side-channels can be detected after the chip is fabricated, either
during the test phase or in run time, but often require costly
external sensing setups and a golden model, a Trojan-free
circuit used as a reference. This makes deploying some of
these approaches in run time not applicable. Moreover, many
side-channel oriented techniques suffer from low resolution of
the collected traces.
As a result, on-chip sensors have gained attraction as
a preferred solution. These sensors can intrinsically collect
traces from EM side-channels through magnetic field coupling
at run time with higher resolutions, thus ruling out the need
for external measurement equipment (e.g., external probes,
moving stages, etc.). Previous research by Jiaji et al. [1]
used a single coil covering the entire chip area, which has
low signal-to-noise ratio (SNR) issues. Consequently, this ap-
proach couldn’t detect small HTs with few gates and required
over 10,000 measurements to detect larger HTs. Its long mean
time to detect (MTTD) limits its practical effectiveness.
In this paper, we propose a Programmable Sensor Array
(PSA) to address the limitations of Jiaji’s work. Our im-
proved design, shown in Figure 1a, consists of a crossbar-like
structure. The PSA can be programmed to different sizes and
shapes, allowing it to cover or uncover specific parts of the
chip for better SNR and HT localization. At each crossing
point in the wire grid, a transmission gate (T-gate) controls
the node’s connectivity. This allows the sensor array to have
different shapes, locations, and sizes. The size of a single
sensor within the PSA can also be programmed to approx-
imately match the size of a HT, ensuring the highest magnetic
field emanations from HTs are captured. The improved SNR
of PSA enables almost instant runtime detection, reducing
MTTD significantly. The contributions of this paper can be
summarized as follows:
• A novel on-chip PSA for EM side-channel measurement
with big advantages over existing techniques was pro-
posed, developed and validated.
• A silicon demonstration of the proposed PSA to detect
and localize active HTs on an AES design implemented
with TSMC 65nm technology.
• Significant SNR improvements with the proposed PSA
compared to prior work.
• Experimental validation of the PSA’s capability to detect,
locate, and identify digital HTs within 10 ms with the
proposed novel cross-domain analysis technique.
arXiv:2401.12193v1  [cs.CR]  22 Jan 2024
Metal8
Via
Polysilicon(Gate of MOSFET)
Transmission Gate
Substrate
Metal7
(a)
Cross section 
Metal7
Metal8
Metal8
Metal7
Substrate
Cross section 
M1-M6 layers 
and Insulator
Via
Transmission Gate
Vo+
Vo-
(b)
Active region
Polysilicon
Metal1
Via
Via
Metal2
Control
Control_bar
Control
4um
3.2um
(c)
Fig. 1: (a)3D structure of on-chip PSA where top-level metals form the coils and transmission gate switches in the active layer control the
size and shape at each intersection. (Only one MOSFET is shown for simplicity) (b)An example of PSA topology forming a 2-turn coil.
The red dots denote the locations where switches are on. (c)T-gate layout.
II. THREAT MODEL AND PRIOR WORK
A. Run-time Verification Versus Test-phase Verification
The distinction between run-time and test-phase verifica-
tions hinges primarily on the approach to HT detection. During
the test phase, efforts are concentrated on the detection of
HTs that can be intentionally triggered. The effectiveness of
HT detection is evaluated based on the accuracy of feature and
trace categorizations as well as the number of attempts needed
to activate the Trojan. Most research focuses on developing
algorithms to successfully trigger HTs within the minimum
amount of time [2] [3]. Conversely, during run time, potential
adversaries must first activate an HT before it can be detected
if the whole batch of ICs is infected with HT, which is
also the assumption of this paper. The primary metric is the
duration between the Trojan’s activation and its detection.
This encompasses recognizing the system’s activation and
discerning the triggering conditions, often referred to as the
Mean Time to Detect (MTTD) [4].
To reduce the hardware overhead during the evaluations,
lightweight measurement and data processing components
should be used. Bai et al. developed RASCv2 board, a compact
2cm x 2cm board that can replace an oscilloscope for side-
channel analysis to detect malware [5] [6]. The board consists
of two analog-to-digital converters (ADCs) for sampling EM
and power side-channel traces, an EM antenna for signal
capture, and an FPGA for data processing. The data processed
by the FPGA are monitored by a security house via an onboard
Bluetooth module, making run-time side-channel verification
feasible. In this paper, we will integrate the concept of
employing the RASC board (manufactured at a trusted facility)
for data processing with the PSA’s on-chip measurement
capabilities to detect HTs in a real-time and trusted manner.
B. EM Side-channel Data Collection Methods
There are several existing techniques to detect and an-
alyze EM side-channel leakage. Traditionally, external EM
probes and oscilloscopes are used to collect EM side-channel
emanation traces over IC packages. The captured signals
will be analyzed in either the time or frequency domains to
identify HTs [7]–[9]. To improve the quality of the collected
traces and reduce overhead, He et al. introduced a single-
winding on-chip EM sensor covering the whole chip to replace
external probes [1]. In [9], Nguyen et al. introduced an EM
backscattering-based Trojan detection approach. A transmitter
antenna injects carrier signals into the IC, and the reflection
signals that have been modulated with the activities of HTs
are subsequently captured with a receiver antenna to identify
HTs. This technique can detect impedance variations resulting
from HT activities, even if these HTs have very small current
consumption. Compared with our proposed PSA detailed in
Section III, existing measurement techniques have problems of
low SNR, limited spatial resolution, complex implementations,
etc. These issues result in the limitations of HT detection rates,
HT localization, and runtime deployment. A comprehensive
comparison will be presented in Section III-B.
III. PROGRAMMABLE SENSOR ARRAY (PSA)
One of the main challenges of on-chip EM sensors is the
balance among sensing area, accuracy, and cost. A single
large sensing coil covering the entire chip is relatively easy to
design and implement; however, it could be inaccurate due to
magnetic flux self-cancellation1. Multiple smaller coils on the
same layer may improve the overall resolution but would have
smaller signal magnitudes and coverage. Using coils across
multiplemetal layers can avoid these issues, but with extra cost.
To mitigate these problems, we propose the PSA technique that
enables the customization of sensor size and position on the
chip at run time.
A. PSA Topology and Structure
PSA utilizes a wire grid that spans two metal layers, with
a switch positioned at every intersection. Each switch is
essentially a transmission gate (T-gate), depicted in Figure 1c,
comprising a single PMOS and NMOS gate connected in
1Magnetic flux forming a small loop within a large sensor induces no
voltage on it.
parallel. The shape, size, and location of a sensing array can
be programmed at run time by controlling specific switches,
as shown in Figure 1b. The size and location of PSA can
be programmed via T-gates by connecting or disconnecting
different cross-points on the wire grid or lattice. The lattice,
comprising several rectangular sensing regions, can be used as
part of the sensing array within the PSA.
Adjusting the shape and size of the PSA enables the PSA to
circumvent the self-cancellation issue highlighted in [1], with
many advantages. For instance, it facilitates the localization of
any detected HTs by reshaping the sensing array. It also elim-
inates sensing boundary limitations or the need for significant
overlap when employing multiple coils.
Implementing such side-channel sensors on chip will raise
concerns on PSA’s ability to execute side-channel attacks
(SCA). Indeed, PSA itself does not protect from SCAs. But if
adversaries can physically access the chip and conduct SCAs,
it may be vulnerable to SCA regardless of the presence of
PSA. Additionally, raw EM traces are not transmitted over
any communication channels. Only processed data should be
sent, and this can help prevent SCAs.
B. PSA Comparison with Prior Work
To highlight the improvement of the PSA-based HT de-
tection technique over prior works, we made a comprehensive
comparison in Table I. This comparison considers five criteria:
HT detection accuracy, spatial resolution of the measurement,
required number of traces, SNR, and feasibility for run-time
deployment.
The approaches relying on traces collected from conven-
tional external probes [7], [8] and on-chip single coils [1] fail
to detect small HT with few gates (implemented as T3 in
our chip in Table II). While Nguyen et al. [9] reports a 100%
detection rate, their approach shares a common limitation with
others: the inability to pinpoint the exact location of the HT
on the chip. On the other hand, our PSA approach not only
ensures a 100% detection rate but also stands out with the
enhanced spatial resolution, providing the distinct capability
of precisely identifying the HTs’ physical location.
Due to low SNR, prior works often rely on statistical
analysis, requiring numerous measurements to detect Tro-
jans. For instance, Xiaolong et al. [7] and Jiaji et al. [1]
compare the Euclidean distance between traces or explore
the Euclidean distance distributions. Nguyen et al. [9] use
Principal Component Analysis and K-means algorithm to
categorize the collected spectra. These approaches require 100
to more than 10,000 measurements. In contrast, the EM side-
channel traces from PSA exhibit sufficiently high SNR with an
apparent difference between HT-inactive and HT-active traces.
Therefore, fewer than ten measurements are needed, leading
to a significantly shorter MTTD and faster Trojan detection.
For implementation complexity, unlike prior works necessi-
tating cumbersome measurement devices [7]–[9], the PSA ap-
proach does not need external probes so it has high feasibility
for run-time deployment. Fujimoto et al. also exploited the on-
chip power noise measurement(OCM) to execute Correlational
TABLE I: COMPARISON OF EM SIDE-CHANNEL DATA COLLEC-
TION METHODS.
Features
External
Probe
[7], [8]
Nguyen
[9]
On-chip
Single
Coil [1]
PSA
(pro-
posed)
HT Detection rate
Low
High
Low
High
HT Localization
No
No
No
Yes
Measurement#
>10,000
100
>10,000
<10
SNR
14.3dB
N/A
30.5dB
41.0dB
Run-time analysis
No
No
Yes
Yes
Power Analysis with high SNR [10], [11], it is also possible
to use such OCM to detect HT, but that requires further
investigation.
IV. PSA’S TAMPER-RESILIENCE
Hardware Trojan risk primarily originates from Third Party
Intellectual Property (3PIP) vendors and malicious foundries.
Our PSA approach is tampering-resilient and can effectively
reduce the risks under both attacking scenarios. The effec-
tiveness of PSA in detecting and locating HTs will be proved
with the experimental results of Subsection VI-D. This section
introduces a few case studies demonstrating PSA’s resistance
to tampering from either source.
A. Case 1: Malicious 3PIP
For malicious 3PIP attacks, as PSA is implemented on
the two topmost metal layers after synthesizing the Register
Transfer Level (RTL) code and is isolated from the main
circuit, it would not interact with any 3PIP. Therefore, any
pre-silicon modifications, including all potential 3PIP attacks,
made prior to the layout phase will not compromise the PSA.
B. Case 2: Malicious Foundries
When dealing with malicious foundries, the PSA itself
may be modified. However, tampering with such a mixed
digital and analog structure is challenging. The adversary must
modify additional circuitry, increasing the attack’s complexity,
thus lowering the attack’s success rate. Additionally, any
modifications that disable the PSA will trigger alarms during
the test phase, as the PSA will return testing values.
Even if the attacker successfully completes the modifica-
tions, designers can easily detect them by reverse-engineering
the two topmost metal layers. They are typically the thickest
metal layers and are relatively easy to be reverse-engineered.
Alternatively, designers can outsource the fabrication of the
two topmost metal layers to other trusted foundries, a split-
manufacturing method similar to those discussed in [12]. This
provides an additional layer of security, making the PSA a
robust defense against HT attacks.
V. PSA IMPLEMENTATION ON A TEST CHIP
A. Integrating PSA on AES-128 Test Chip
To assess the performance of PSA in both test and run-
time phases we implemented and fabricated the PSA on the
two top metal layers of an Advanced Encryption Standard
(AES128) encryption design using TSMC65nm technology.
1
4
3
2
0
5
PSA_control 6
7
8
9 10
11
12
13
14 15
T1
T3
T4
T2
AES_core
UART_FIFO
Sensor4+
Sensor4-
Sensor3+
Sensor3-
Sensor2+
Sensor2-
Sensor1+
Sensor1-
VDD
VSS
UART_in
UART_out
PSA_sel[3:0]
VDD en_T2inv_out
load_out
en_T3
dy_outen_T4 VSS
en_T1
am_out
CLK
rst_n
en_UART
en_LFSR
Drdy1
sensor
EM Probe
1mm 
1mm
Zoom-in View of 
Each Small Square in 
the left figure
Fig. 2: Sensor deployment, IO pin assignment, and Amoeba module
view on the AES128 test chip.
The proposed PSA was situated on layers Metal 7 and Metal
8 of the chip, covering the entire chip. It is a lattice including
36 horizontal wires, 36 vertical wires, and 1296 switches.
The lattice wire measures 16µm in length and 1µm in width.
Frequency sweeping is used to determine the optimal length
and width that maximize the signal magnitude in the desired
frequency range of 10MHz-100MHz.
For this test chip, the entire area was uniformly divided into
16 square sensing areas or sensors. Each sensor shares 33%
of its area with adjacent sensors to ensure adequate circuitry
sampling near the borders between any two sensors.
Main Circuit The underlying main circuit is an AES-128-
LUT core, integrated with an RS232 UART communication
module [13]. The clock frequency was set to 33MHz. The
layout of the routing wires of the test chip, along with the
indices of the PSA sensors, is illustrated in Figure 2.
HT Design The design incorporates four distinct HTs, mod-
ified from Trust-Hub [14] [15]. T1 is an amplitude modulation
radio carrier Trojan capable of emitting an electromagnetic
(EM) wave at a frequency of 750KHz. T2 is a chain of
inverters connected to a key wire to amplify its leakage
current. If T2 is implanted, attackers could recover the key
via power analysis. T3 is a Code Division Multiple Access
(CDMA) channel Trojan designed to leak the key, and T4 is a
simple denial-of-service Trojan that elevates power consump-
tion, potentially causing the IC to overheat. The four Trojans
employ digital standard cells and each has unique triggering
conditions. The 4 HT’s locations are shown in the Amoeba
view in Figure 2.
IO Pin Assignment The test chip uses a QFN 6mm x 6mm
package and has 8 IO pins on each side. The IO pins’ names
are shown in Figure 2. The PSA uses the 8 IO pins on the right
side. 2 IO pins are needed to measure the induced differential
output voltage of one sensor, so we have 4 output channels,
from sensor1+/- to sensor4+/-. The 4 sensors on each row use
the channel on the same row. For example, sensor 0,1,5,6 in
the figure uses sensor1 channel. The PSA control signals use
4 of 8 IO pins on the bottom side. They were decoded into
TABLE II: TROJAN GATES COUNT AND PERCENTAGE
Circuit
Overall
T1
T2
T3
T4
Standard Cell Number
28806
1881
2132
329
2181
Percentage
100
6.52
7.40
1.14
7.57
gate signals for T-gates with the fully combinational decoder
in the figure.
HT Triggering Condition T1 is activated periodically when
a counter reaches 21’h1F FFFF under the 33MHz clock. T2
is triggered when the first four bytes of the plaintext are
16’hAAAA. T3 and T4 are the always-on HTs, and we added
external enable signals as triggers in experiments. The gate
number counts and area ratios of these four HTs are shown in
Table II.
We assess the performance of the PSA in detecting and
locating HTs by activating and deactivating sensors during the
execution of AES encryption. As depicted in Figure 2, the
green box represents the area of a 6-turn-coil sensor, where
most HT circuits are implemented. The orange arcs represent
the size of a circle-shape external EM probe. The main circuit
primarily falls under sensors 2, 3, 4, 7, 8, 9, 10, 11, and 14.
Of these sensors, sensor 10 offers the most coverage of both
Trojan payloads and triggers.
B. T-gate Design and PSA Implementation Cost
Due to the absence of the T-gates that fulfill our require-
ments in the standard cell library, we designed our own
customized T-gates, which are illustrated within a 3.2µm x
4µm customized cell layout in Figure 1c. A single finger
of the NMOS and PMOS devices in the design exhibit an
aspect ratio of 500nm/60nm and 610nm/60nm respectively
to have a similar current conducting capability. For enhanced
performance, each of these MOSFETs incorporates 10 fingers.
To ensure a low turn-on resistance, the layout features two
PMOS and two NMOS devices, effectively creating a pair
of T-gates connected in parallel. As a result, this T-gate
configuration achieves a resistance of approximately 34 Ohms.
For power overhead, the dynamic power consumption of
PSA sensors is negligible and largely contributed by the
leakage power from the transistors. Therefore, PSA’s impact
on the overall power profile is minimal.
For area and resource allocation, the T-gates used in PSA
account for an additional 5% of the total chip area, with the
PSA located on metal layers M7 and M8. Despite this, the
design effectively preserves routing capacity by aligning PSA
wires parallel to main circuit wires, reducing top-layer routing
capacity by just 6.25%.
In contrast to Jiaji’s single-coil structure [1], which utilizes
100% of the top layer’s routing capacity, PSA proves to be
more efficient.
VI. EVALUATION ON TEST CHIP
A. Test and Data Collection Platform Setup
The test chip was mounted on a PCB board with voltage-
level shifters and open-loop OP-AMPs. The output of each
output channel of the PSA is amplified by a THS4504D
Fig. 3: Spectrum magnitude comparison between those from the PSA
and an external EM probe.
OP-AMP with 50dB DC gain and 200MHz UGB, aligning
well with our target frequency range from DC to 120MHz.
A 33MHz crystal oscillator is used to generate the clock
signal. The evaluation is conducted while the test chip executes
AES-128 encryption. It receives plaintext from and sends
ciphertext to a laptop through serial communications. During
this operation, an oscilloscope or a spectrum analyzer triggered
by the rising edge of the clock signal captures the amplified
PSA output and generates the related frequency spectrum for
further analysis. Additionally, we use the zero-span mode of
the spectrum analyzer to measure the time-domain signal of
the PSA’s output at a desired single frequency.
For run-time deployment, a system integrated on the PCB
board can replace the oscilloscopes, spectrum analyzers, and
computers to measure and analyze side-channel data.
B. Signal-to-Noise Ratio (SNR) Measurement
To demonstrate the improved measurement quality offered
by the PSA sensor array, we conducted SNR measurements
and compared the results with those from an external Langer-
EMV LF1 probe. We employ He’s SNR measurement ap-
proach as detailed in [1]. Noise traces are first collected from
the powered-up chip without any encryption activity. Signal
traces are then gathered while the chip performs AES encryp-
tion. The SNR is calculated based on the root mean square
(RMS) voltage ratio of these two data sets, as demonstrated
in Equation (1).
SNR = 20log
V rmssignal
V rmsnoise

(1)
The SNR of the PSA is 41.0 dB, surpassing both the external
probe’s 14.3 dB and the single-coil on-chip sensor’s 30.5 dB as
reported in [1]. In Figure 3, the green spectrum represents the
difference in dB between the spectra from the sensor array and
the external probe, demonstrating that the spectrum from the
PSA can be up to 55 dB higher than that from an external EM
probe. Overall, the proposed PSA structure offers significantly
superior SNR results in comparison to either external probes
or single-coil on-chip sensors.
The best state-of-the-art external probe is the ICR HH100-6
set with a diameter of 100 µm manufactured by the Langer
EMV-Technik. Based on the manufacturers documents, the
SNR of ICR probe is approximately 34dB below 120 MHz,
so the best probe is still worse than our proposed PSA.
C. PSA Performance under Different Supply Voltages and
Different Ambient Temperatures
We have conducted comprehensive evaluations of PSA’s
performance under varying operational conditions to confirm
its suitability for runtime deployment across a wide range of
applications. We performed simulations and experiments at
different supply voltages, spanning from 0.8V to 1.2V, which
covers the voltage supply voltage range for TSMC 65nm
chips. Additionally, we carried out simulations under different
ambient temperatures, ranging from −40 ◦C to 125 ◦C.
1) Voltage: Theoretically, a supply voltage increase results
in a reduction of the turn-on resistance of T-gates. In the Vir-
tuoso simulation, there was only a 4dB drop in the impedance
of a single PSA sensor when the voltage supply was raised
from 0.8V to 1.2V. We also made an experiment to measure
the current response of the PSA by adding a 70mV frequency
sweeping chirp signal to one sensor of the PSA and varying
the supply voltage from 0.8V to 1.25V. The current does not
change significantly, which agrees with the simulation results.
2) Temperature:
Theoretically, an ambient temperature
change only slightly affects the impedance of the PSA. Our
simulations demonstrate that the impedance remains relatively
stable, fluctuating within a range of 4dB.
D. Runtime Cross-Domain Analysis
For each of the 16 sensors, EM traces are recorded under
five scenarios: when HTs T1, T2, T3, and T4 are individually
activated and in the absence of any active HT. For always-on
HTs, we use external enable signals to activate them. Each
trace spans a frequency band from DC to 120MHz, populated
with 2000 sample points. We averaged five collected traces
to derive the spectrum shown in Figure 4. Notably, when
examining the output from Sensor 10, as shown in Figure
4a, 4b, 4c, and 4d, two prominent frequency components at
48MHz and 84MHz, the sideband frequency components of
the 1st and 3rd order clock harmonics, show up in the spectrum
when HTs are active. We also record the traces from Sensor 0
as shown in Figure 4e, where no HTs are implemented beneath
the sensor. There is hardly any spectrum difference that can
distinguish HT’s activities. This result further proves that our
proposed PSA has the high spatial resolution needed to locate
HTs.
In the experiments, we only need fewer than ten traces
collected to detect a HT, resulting in less than 10 ms MTTD,
a significant improvement over one single on-chip coil.
After identifying prominent frequency components related
to HT’s activities, we further switch back to the time domain
by employing the zero-span mode of the spectrum analyzer
to examine the time-domain waveforms of the two frequency
components. Zero-span mode allows us to analyze the pattern
of a time-domain signal at a desired fixed frequency. The time-
domain waveforms at 48MHz when T1, T2, T3 and T4 are
active are shown in Figure 5a, 5b, 5c, and 5d. It is shown
that even if different Trojans leaked their information at the
same frequency, the difference in their time-domain signals at
48MHz can still clearly differentiate different Trojans. This
T1 Spectrum
Orange Circles indicate HT activation
1e8
(a) T1 active, Sensor 10
T2 Spectrum
1e8
(b) T2 active, Sensor 10
T3 Spectrum
1e8
(c) T3 active, Sensor 10
1e8
T4 Spectrum
(d) T4 active, Sensor 10
T1 Spectrum (Sensor 0)
1e8
(e) T1 active, Sensor 0
Fig. 4: Frequency response captured by sensors 10 and 0 for different HTs: red and blue colors represent the Trojan active and inactive
cases, respectively.
is because different HTs result in different modulation pat-
terns for clock frequency, and the information on modulation
patterns is included in the time-domain waveforms of the
side band frequency components. Therefore, by identifying
the unique patterns in the time-domain signals, we can suc-
cessfully classify all 4 HTs without full supervision.
(a) T1 active
(b) T2 active
(c) T3 active
(d) T4 active
Fig. 5: Time-domain signals of the identified prominent frequency
components collected from the PSA sensor 10 were recovered with
zero-span mode to differentiate different HTs successfully.
VII. CONCLUSION
In this paper, we proposed an integrated on-chip EM
sensor array, PSA, that can be programmed to change its
shape, size, and location. The PSA was implemented and
tested in a 65nm technology node. The proposed PSA has
a lower overhead than existing state-of-the-art on-chip coil
sensor designs with much higher SNR and the capability of
locating HTs. Furthermore, the MTTD during the runtime
verification has been significantly reduced to less than 10 ms.
The proposed cross-domain analysis can identify prominent
frequency components that include the activity information of
different Trojans in the frequency domain and differentiate
different Trojans by examining the time-domain signals of
these prominent frequency components by switching to the
time domain.
VIII. ACKNOLEDGEMENT
This work was supported by the Office of Naval Research
under Award Number N00014-19-1-2405.
REFERENCES
[1] J. He, X. Guo, H. Ma, Y. Liu, Y. Zhao, and Y. Jin, “Runtime trust
evaluation and hardware trojan detection using on-chip em sensors,” in
2020 57th ACM/IEEE Design Automation Conference (DAC), 2020, pp.
1–6.
[2] R. S. Chakraborty, F. Wolff, S. Paul, C. Papachristou, and S. Bhunia,
“Mero: A statistical approach for hardware trojan detection,” in Interna-
tional Workshop on Cryptographic Hardware and Embedded Systems.
Springer, 2009, pp. 396–410.
[3] Y. Huang, S. Bhunia, and P. Mishra, “Mers: statistical test generation for
side-channel analysis based trojan detection,” in Proceedings of the 2016
ACM SIGSAC Conference on Computer and Communications Security,
2016, pp. 130–141.
[4] D. Forte, C. Bao, and A. Srivastava, “Temperature tracking: An in-
novative run-time approach for hardware trojan detection,” in 2013
IEEE/ACM International Conference on Computer-Aided Design (IC-
CAD), 2013, pp. 532–539.
[5] Y. Bai, A. Stern, J. Park, M. Tehranipoor, and D. Forte, “Rascv2:
Enabling remote access to side-channels for mission critical and iot sys-
tems,” ACM Transactions on Design Automation of Electronic Systems
(TODAES), vol. 27, no. 6, pp. 1–25, 2022.
[6] Y. Bai, J. Park, M. Tehranipoor, and D. Forte, “Real-time instruction-
level verification of remote iot/cps devices via side channels,” Discover
Internet of Things, vol. 2, no. 1, p. 1, 2022.
[7] J. He, Y. Zhao, X. Guo, and Y. Jin, “Hardware trojan detection through
chip-free electromagnetic side-channel statistical analysis,” IEEE Trans-
actions on Very Large Scale Integration (VLSI) Systems, vol. 25, no. 10,
pp. 2939–2948, 2017.
[8] S. Faezi, R. Yasaei, and M. A. Al Faruque, “Htnet: Transfer learning for
golden chip-free hardware trojan detection,” in 2021 Design, Automation
& Test in Europe Conference & Exhibition (DATE), 2021, pp. 1484–
1489.
[9] L. N. Nguyen, B. B. Yilmaz, M. Prvulovic, and A. Zajic, “A novel
golden-chip-free clustering technique using backscattering side channel
for hardware trojan detection,” in 2020 IEEE International Symposium
on Hardware Oriented Security and Trust (HOST), 2020, pp. 1–12.
[10] D. Fujimoto, M. Nagata, S. Bhasin, and J.-L. Danger, “A novel
methodology for testing hardware security and trust exploiting on-chip
power noise measurement,” in The 20th Asia and South Pacific Design
Automation Conference, 2015, pp. 749–754.
[11] D. Fujimoto, N. Miura, M. Nagata, Y. Hayashi, N. Homma, Y. Hori,
T. Katashita, K. Sakiyama, T.-H. Le, J. Bringer, P. Bazargan-Sabet, and
J.-L. Danger, “On-chip power noise measurements of cryptographic vlsi
circuits and interpretation for side-channel analysis,” in 2013 Interna-
tional Symposium on Electromagnetic Compatibility, 2013, pp. 405–410.
[12] J. Rajendran, O. Sinanoglu, and R. Karri, “Is split manufacturing
secure?” in 2013 Design, Automation & Test in Europe Conference &
Exhibition (DATE), 2013, pp. 1259–1264.
[13] S. Morioka and A. Satoh, “An optimized s-box circuit architecture for
low power aes design,” in International Workshop on Cryptographic
Hardware and Embedded Systems.
Springer, 2002, pp. 172–186.
[14] B. Shakya, T. He, H. Salmani, D. Forte, S. Bhunia, and M. Tehranipoor,
“Benchmarking of hardware trojans and maliciously affected circuits,”
Journal of Hardware and Systems Security, vol. 1, no. 1, pp. 85–102,
2017.
[15] H. Salmani, M. Tehranipoor, and R. Karri, “On design vulnerability
analysis and trust benchmarks development,” in 2013 IEEE 31st Inter-
national Conference on Computer Design (ICCD), 2013, pp. 471–474.
"
"Embedding inversion attacks recover sensitive training data from language models solely by querying an external embedding service. While existing research on such attacks focuses on English, we study multilingual inversion attacks, where the language underlying the target text is unknown. We define the problem of black-box multilingual inversion, develop defensive strategies, and propose a methodology enabling the evaluation of multilingual inversion attacks. Our findings reveal the potential and effectiveness of multilingual embedding inversion, demonstrating that multilingual models can be more vulnerable than their monolingual counterparts.","Embedding inversion attacks can recover sensitive data from language models using queries to external embedding services. Prior work has focused on English-language text, leaving all other languages vulnerable to attack. To rectify this, we conduct the first study of multilingual inversion attacks and associated defenses.","A wealth of research has investigated the vulnerability of language models to inversion attacks, but only in the context of English. Our work extends this research to the multilingual setting, exploring attacks on models trained in multiple languages and developing a novel defense strategy.nan","To investigate multilingual inversion attacks, we construct a black-box setting, where an attacker's access to the victim model is limited to querying an external embedding service. We then formulate the problem of black-box multilingual inversion and propose a methodology for evaluating attacks in this setting.","Our experiments demonstrate the potential of multilingual inversion attacks, showing that multilingual models can be exploited to recover sensitive information even when the language of the target text is unknown. In particular, we find that multilingual models can outperform monolingual models in certain scenarios.","Our research highlights the need for further investigation and enhanced defenses in the area of NLP security, particularly with respect to multilingual embedding inversion attacks. We believe that our findings and proposed methodology will inspire future work in this direction.",Text Embedding Inversion Attacks on Multilingual Language Models,"Yiyi Chen, Heather Lent, Johannes Bjerva","Text Embedding Inversion Attacks on Multilingual Language Models
Yiyi Chen
Heather Lent
Johannes Bjerva
Department of Computer Science, Aalborg University, Denmark
{yiyic, hcle, jbjerva}@cs.aau.dk
Abstract
Representing textual information as real-
numbered embeddings has become the norm in
NLP. Moreover, with the rise of public interest
in large language models (LLMs), Embeddings
as a Service (EaaS) has rapidly gained traction
as a business model. This is not without out-
standing security risks, as previous research
has demonstrated that sensitive data can be re-
constructed from embeddings, even without
knowledge of the underlying model that gen-
erated them. However, such work is limited
by its sole focus on English, leaving all other
languages vulnerable to attacks by malicious
actors. To this end, this work investigates LLM
security from the perspective of multilingual
embedding inversion. Concretely, we define the
problem of black-box multilingual and cross-
lingual inversion attacks, with special attention
to a cross-domain scenario. Our findings re-
veal that multilingual models are potentially
more vulnerable to inversion attacks than their
monolingual counterparts. This stems from
the reduced data requirements for achieving
comparable inversion performance in settings
where the underlying language is not known a-
priori. To our knowledge, this work is the first
to delve into multilinguality within the context
of inversion attacks, and our findings highlight
the need for further investigation and enhanced
defenses in the area of NLP Security.
1
Introduction
Industrial applications of Natural Language Pro-
cessing (NLP) typically utilize Large Language
Models (LLMs) and frequently rely on vector
databases via frameworks such as Embeddings as a
Service (EaaS). In this context, rather than storing
data as strings, high quality sentence embeddings
are stored in a remote database instead. This allows
end-users to efficiently search across these con-
densed representations, which are seemingly im-
pervious to privacy breaches. However, while such
EaaS workflows have previously been assumed to
be secure, recent work has demonstrated that ac-
cess to the embeddings is no more safe than raw
text, as models can learn to decode these embed-
dings (Song and Raghunathan, 2020; Morris et al.,
2023; Zhou et al., 2023). As such, there is a sub-
stantial threat to privacy if malicious actors are able
to eavesdrop on communication channels between
EaaS providors and customers, and access the em-
beddings in the process.
Decoding the content of these embeddings can
be done via inversion attacks. After gaining access
to embeddings and the black-box embedder via the
EaaS API, the malicious actor can train an external
model, which approximates the inversion function
that reconstructs the text from the embeddings. Pre-
vious work has proven has demonstrated that an
exact match for data recreation can be obtained in
specific settings, albeit with the limitation of assum-
ing monolingual English models and embeddings
(Morris et al., 2023).
In a real-world scenario however, an eavesdrop-
per may not necessarily know the language of the
text encoded within the embedding. For instance,
a Spanish EaaS provider might host its data in Ger-
many, for a French-speaking company. Thus in this
work we investigate three research questions: (i)
To what extent are inversion attacks feasible in a
multilingual setting?; (ii) Are attacks feasible and
effective when the language is unknown a-priori?;
(iii) Does cross-lingual transfer allow information
to be leaked across the languages included in a
multilingual model?
Contributions
In this work, we define the prob-
lem of black-box multilingual and cross-lingual
inversion attacks, with special attention to a cross-
domain scenario. While previous research has suc-
ceeded in reconstruction of tokens with bag-of-
words approach (Song and Raghunathan, 2020)
and sequences with informative words (Li et al.,
2023), Morris et al. (2023) has proven the potential
arXiv:2401.12192v1  [cs.CL]  22 Jan 2024
and effectiveness of embedding inversion to the
extent of exact text reconstruction in English. We
approach multilingual inversion by extending the
methodology introduced by Morris et al. (2023) to
a multilingual setting over English, French, Span-
ish, and German.
In this study, we are thus the first to investigate
multilingual inversion attacks and potential of ex-
act textual reconstruction in a multilingual setting,
in particular when the language of a target embed-
ding is unknown. Concretely, we experiment using
a state-of-the-art multilingual black-box encoder,
where the trained multilingual inversion model re-
constructs texts in certain languages outperform-
ing their monolingual counterpart without extra
steps of corrections. Furthermore, we conduct ex-
periments on cross-lingual and cross-domain text
reconstruction, and propose a straightforward Ad
hoc Translation method to counteract the short-
comings of the current standard of string-matching
metrics in this specific setting.
Finally, we open-source all of our trained inver-
sion models due to the computational cost of their
training.1 While open sourcing our model comes
with such risk as providing models to attackers, the
underlying attack mechanism presented in this pa-
per is already established, together with a defense
in (Morris et al., 2023). We believe these models
will be useful to the research community, allow-
ing for the development of multilingual defense
mechanisms, without needing to spend resource on
training the models we present.
2
Related Work
Models are well known to memorize training data,
and are therefore susceptible to leaking private in-
formation (Shokri et al., 2016; Carlini et al., 2018;
Nasr et al., 2019). As such, there is increased re-
search interest in exploring this vulnerability to
inversion attacks from the perspective of cyber-
security, simulating attacks against models to recre-
ate sensitive training data.
Work in this direc-
tion has been conducted across various domains of
machine learning, such as computational genetics
(Fredrikson et al., 2014), computer vision (Fredrik-
son et al., 2015), and more recently NLP (Song and
Raghunathan, 2020). Generally, such works at the
intersection of machine learning and cyber-security
(e.g., on inversion attacks or adversarial attacks)
1The trained inversion models are available at: https:
//huggingface.co/yiyic
make assumptions about the imagined attacker’s
levels of access to the victim model. White-box
scenarios assume attacker access to the full model
(Wallace et al., 2019; Tsymboi et al., 2023), re-
sulting in many possible attack surfaces. Previous
works in NLP have shown that it is possible to re-
trieve sensitive training data by attacking models
directly (Fredrikson et al., 2014, 2015), attacking
gradients (Zhu et al., 2019; Deng et al., 2021), as
well as through leveraging leaked hidden states (Li
et al., 2022). Meanwhile, black-box attacks assume
an attacker has no knowledge of the underlying
model itself, and can only interact with models at
the most abstracted level (e.g., provide input and
register output through an API). For example, Car-
lini et al. (2020) are able to extract sensitive training
data (e.g., names and phone numbers) from GPT-2
(Radford et al., 2019), by first generating data from
the model and then using membership inference
attacks to filter utterances likely to be part of the
original training data.
In the case of embedding inversion attacks,
whereby an imagined attacker aims to recreate
the text encoded by the distributed representation,
Song and Raghunathan (2020) first demonstrated
that 50%–70% percent of tokens could be recov-
ered from an embedding. Since then, the success of
subsequent attacks has only improved, with newer
approaches now able to retrieve entire sentences of
encoded text (Höhmann et al., 2021; Hayet et al.,
2022; Morris et al., 2023; Li et al., 2023). Mean-
while, the development of counter-measures to em-
bedding inversion attacks is an area of ongoing
investigation. For example, Zhou et al. (2023)
propose a defense method which makes randomly
perturbed embeddings after an initial clustering
step, such that the embeddings are still semanti-
cally meaningful and useful for downstream tasks,
while remaining resistant against inversion attacks.
Parameter-efficient fine-tuning has also been found
to protect models against white-box (via gradients)
inversion attacks in the setting of federated learn-
ing (Zhang et al., 2023). Beyond direct defenses
against inversion attacks, other methods for pro-
ducing more secure embeddings have made use of
existing encryption methods (Huang et al., 2020;
Xie and Hong, 2021) as well as differential privacy
(Lyu et al., 2020). However, until privacy can be
guaranteed for embeddings, inversion attacks will
continue to pose a threat, and thus require contin-
ued investigation.
Finally, to our knowledge, previous works in em-
bedding inversion are all conducted in a monolin-
gual setting over English (Song and Raghunathan,
2020; Lyu et al., 2020; Hayet et al., 2022; Parikh
et al., 2022; Kim et al., 2022; Morris et al., 2023;
Zhou et al., 2023; Li et al., 2023). This is a signifi-
cant shortcoming, as it risks leaving defences for
non-English languages unexplored, with implica-
tions for LLM Security for all languages. For in-
stance, this could lead to implementations of LLMs
in non-English being considerably less secure than
their English counterparts.
3
Methodology
Text embeddings can be generated through the
encoding of text using a language model, or
through dedicated text vectorization techniques like
Word2Vec (Mikolov et al., 2013) or GLoVE (Pen-
nington et al., 2014). As EaaS typically deals with
embeddings of sentences or phrases, this requires
us to explore this relatively more challenging set-
ting, as opposed to word-level embeddings. In this
work, we consider a black-box embedding inver-
sion attack scenario. To exemplify such attacks,
envision malicious actors eavesdropping on com-
munication channels between EaaS providers and
customers, ultimately gaining the access to the em-
beddings during the process. While previous work
has assumed that this entire process can be assumed
to take place in English, we here consider the con-
siderably more difficult setting where the underly-
ing language is unknown. That is, we specifically
aim at multilingual embedding inversion.
3.1
Black-box Embedding Inversion
To formalize the attack scenario, assume that a sen-
sitive text sequence x and a black-box encoder ϕ
are given. The embedding inversion attack is then
defined as using an external attacker model ψ to re-
cover textual input x from the embedding obtained
via ϕ(x). However, the architecture and parameters
of ϕ are both inaccessible, we can solely access ϕ
via an EaaS API. The attacker model ψ is built to
learn the inverse mapping ϕ−1, which we formulate
as an approximation problem (Li et al., 2023):
ψ(ϕ(x)) ≈ ϕ−1(ϕ(x)) = x
(1)
Text generation models have proven to be effec-
tive in generating contextually coherent texts (Shen
et al., 2019), they have a widerange of applications,
such as machine translation, summarization and
dialogue systems (Moslem et al., 2022; Sun et al.,
2022). In our work, we approach the inversion at-
tacks in the context of text generation. In this case,
a generation model ψ determines how much infor-
mation can be encoded and decoded, and down the
line, how well text can be constructed. For exam-
ple, if ψ is solely pre-trained on Latin script, then
Sanskrit or Cyrillic data cannot be encoded or de-
coded. Hence, it is not feasible to reconstruct text
in unknown scripts, and it is unexplored whether
text in unknown languages can be reconstructed.
In this study, we investigate text reconstruction in
unknown languages but in the same script. More
specifically, how well a generation model can gen-
eralize across languages in the same script is also a
determinant factor for inversion attacks.
Moreover, to recover the text from ϕ(x), it is
implicit that the data space to which x belongs is
unknown. In practice, to build an attacker model
based on the eavesdropped embeddings, a training
dataset D is used so that the attacker directly learns
ψ from pairs (ϕ(y), y), where y ∈ D. Moreover,
D has a strong impact on inversion performance.
In reality, the inversion attacks are essentially cross-
domain problem, since D most likely do not repre-
sent the data space of x.
Multilingual Inversion Attacks
Compared to
monolingual embedding inversion, to investigate
the potential and effects of multilingual inversion
attacks, the complexity of experimentation scales
up rapidly, as each language space of ψ, ϕ, x and
D plays a vital role. For example, defining the
investigated languages as a set L = {l1, l2, . . . ln}.
The scale of training attacker models multiplies by
languages and other controlled parameters, such as
maximal sequence length for text generation (cf.
Section 4). Moreover, the complexity of an exhaus-
tive multilingual cross-evaluation has a complexity
of O(|L|), since each monolingual and multilin-
gual model should be evaluated on all languages.
We investigate the potential of multilingual em-
bedding inversion under the assumption that we
can directly send almost unlimited queries to the
black-box ϕ, and obtain embeddings ϕ(y) for
y ∈ D. Following the same approximation ap-
proach from Morris et al. (2023), assuming that
e = ϕ(y), the search for text ˆy with the embed-
dings that closest to the target embedding e under
ϕ is optimized by the following formula, utilizing
Cosine similarity to measure the semantic similar-
ity in the embedding space:
Trump once asked then-
acting FBI director Andrew 
Mccabe about his 2016-vote
Target text 𝑦
𝑒
Trump einmal fragte damals 
FBI Director Andrew Mccabe 
während seiner 2016-Vote
Correction ""𝑦(#)
̂𝑒(""#$)
̂𝑒("")
%𝑦("")
Trump once asked then-FBI 
Director Andrew Mccabe
during his 2016-vote
AdTrans(""𝑦)
Ad hoc Translation
Vec2Text
𝝓
𝝋
𝝓
Figure 1: Overview of the method, Vec2Text (Morris et al., 2023) plus Ad hoc Translation. The texts are examples
of crosslingual text reconstruction evaluation. English text is evaluated on the inversion model trained on German
texts. Assuming access to a target embedding e (blue), and query access to the embedder ϕ (blue model) via EaaS
API, the inversion model ψ (orange) aims to iteratively generate hypothesis ˆe (pink) to reach the target. During
cross-lingual evaluation on English text with inversion model trained on German data, the generated text ˆy is in
German, and translated to English (AdTrans(ˆy)), to be compared with the input y. Example input is from test data
in MTG-EN.
ˆy = arg max
y
cos(ϕ(y), e)
(2)
Specifically, as shown in Fig. 1, following (Mor-
ris et al., 2023), the inversion model training and
inference is conditioned on the previous output,
that at correction step t + 1, the model takes the
concatenation of previous output ˆy(t) and hypothe-
sis embedding ˆe(t), plus target embedding e.
We assume that the attacker (1) has access to
the black-box ϕ via EaaS API and also (2) has
the knowledge of the language script of the input
text for the target embeddings. Then multilingual
embedding inversion attack is composed of the
following steps: (a) build an attacker model ψ,
based on a text generation model, pre-trained on the
same language scripts, ideally the same language;
(b) [Attack Model Training] train ψ by iteratively
querying the black-box embedding model ϕ with
text y ∈ D, and resulting in ˆy optimized with Eq. 2
(correction step 1); (c) [Inference] having tested
sound generalizability of ψ, the embeddings ϕ(x)
can be inverted, and reconstruct text x, and further
steps of optimizations (correction steps > 1) with
Eq. 2 and implement beam search at the sequence
level where a new generation is taken only when it
is closer to the target embeddings compared to the
previous step.
Ad hoc Translation
Without prior knowledge of
the language lx of target text, the language ly of the
training dataset D can be different from lx, which
may result in the trained inverter model decoding
texts only in ly. However, the generated text, albeit
in ly, can convey the same information as the target
text in lx. To investigate this aspect, as shown in
Fig. 1, we propose a post-inversion strategy, i.e.,
Ad hoc Translation (AdTrans), where the generated
text is translated from ly in lx, further the translated
text is evaluated against the target text, to verify
whether the inverted text in ly leak information of
the target text in lx (cf. Section 5.3).
Previous research has solely focused on inverting
embeddings for English texts, taking for granted
the knowledge of language of the input text from
target embedding. Our study expands on this line
of research by expanding the language space of
each essential components, without assuming prior
knowledge of the languages.
4
Experimental Setup
Multilingual Embeddings
We leverage T5-base
(Raffel et al., 2023) as our generation model, fol-
lowing Morris et al. (2023). We train the mul-
tilingual inversion models ψ on a state-of-the-
art multilingual encoder ϕ: multilingual-e5-base
(ME5-base)2 (Wang et al., 2022), a pre-trained
transformer based on XLM-R based (Conneau
et al., 2020), via weakly-supervised contrastive pre-
training on a mixture of multilingual datasets. The
model is chosen as it is one of the best performing
multilingual models on the MTEB text embeddings
benchmark (Muennighoff et al., 2023). Further-
more, we also reproduce the results from (Morris
et al., 2023) by training inversion models on GTR-
base using English datasets, as our baselines.
2transformers: intfloat/multilingual-e5-base
Datasets
Previous research (Morris et al., 2023)
trains text inversion models on natural ques-
tions and question-answer pairs, such as MS-
Marco (Bajaj et al., 2018) and Natural Questions
(NQ) (Kwiatkowski et al., 2019) datasets. While
these datasets are large, they are limited to English.
Thus for our experiments, we train and evaluate the
multilingual inversion models on the MTG dataset,
a benchmark suite specific for multilingual text gen-
eration training and evaluation (Chen et al., 2022),
with parallel examples across all languages. MTG
is curated from different domains, including news,
daily life and Wikipedia. In order to ensure the
validity of our experiments, and test generalizabil-
ity, we exclude the data curated from Wikipedia,
since this domain data is used to train both the T5-
base and ME5-base models. For each language,
this results in 123k passages to be used as training
data. Passages refer to paragraphs or sections of a
document. We obtain 3-5M sentences in each lan-
guage for training data in MTG using NLTK (Bird
and Loper, 2004) sentence tokenization. This is
considerably fewer samples as compared to (Mor-
ris et al., 2023), in which the GTR-base model is
trained on 5M passages from NQ.3 Meanwhile, we
train and evaluate on data in English, French, Ger-
man and Spanish, noted as MTG-EN, MTG-FR,
MTG-DE, and MTG-ES, respectively. We also
compose a 5M-sentence multilingual dataset in-
cluding 1.2M sentences from each language, noted
as MTG-MULTI.
Evaluation Metrics
We measure model perfor-
mance using two types of metrics, to compare
with the results from previous research (Morris
et al., 2023).
First, for evaluating text recon-
struction, word-match metrics are used: BLEU
score (Post, 2018), where n-gram similarities be-
tween the true and reconstructed text are measured;
ROUGE score (Lin, 2004), where recall of overlap-
ping words of reconstructed text is reported; Token
F1, the multi-class F1 scores between the set of pre-
dicted tokens and the set of true tokens, considering
each word as a class; Exact-match, the percentage
of reconstructed texts matching perfectly the true
texts. Additionally, the cosine similarity between
3The models truncate texts into 32 tokens and 64 tokens,
to evaluate how sequence length affects the performance of
embeddings inversion. Each passage in NQ is significantly
longer than 32 and 64 tokens. To obtain more training data
samples from MTG dataset, we implement NLTK sentence
tokenization on MTG dataset, which results in about 3-5M
sentences for each language.
the true embedding and the embedding of the re-
constructed text in the embedding space of trained
ϕ. Such metrics fall short in terms of evaluating
whether the semantic content, e.g., specific private
information, is recovered. The limitation is particu-
larly evident in cross-lingual settings, for example,
where the generated German text conveys similar
meaning as the input English text, a nuance that
word-match metrics fail to capture (cf. Fig 1)
Experiments
Following the setup from (Mor-
ris et al., 2023), there are two stages of model
training for embedding inversion: (1) Base inver-
sion model, learning text distributions given em-
beddings, (2) Vec2Text corrector model, initialized
with the trained Base model and training using
Eq. 2. To evaluate the potential of multilingual
and cross-lingual embedding inversion attacks, we
train Base models and Vec2Text models for each
language and MTG-MULTI, and evaluate exten-
sively in multilingual settings. In comparison with
previous research, we train and evaluate English
inversion models on NQ and MTG-EN.
The Adam optimizer with a learning rate of
0.001 with 625 warmup steps is used. We train
each base model and corrector model for 100
epochs each. We use a batch size of 512 for inver-
sion models and 256 for corrector models trained
on data with 32 tokens, while the batch sizes are
halved for models trained on data truncated to 64
tokens, accordingly. All models are trained on 4
AMD MI250 GPUs with distributed training.4 Un-
der these circumstances, training our slowest model
takes about 8 days.
5
Results and Analysis
5.1
Monolingual English Text Reconstruction
In-Domain
To have a proof of concept, we repli-
cate the experiment from Morris et al. (2023), by
training inversion models using GTR-base and
ME5-base as embedders on the NQ dataset. The
Base and Vec2Text model with 1 correction step
trained on ME5-base has a performance on par with
GTR-base. Moreover, the text embeddings trained
on ME5-base are more closer in embedding space
than embeddings trained on GTR-base, i.e., with
higher cosine similarities. However, with more
steps of correction and beam search, the perfor-
mance is boosted to 0.9244 in BELU score with
4Distributed
Training
with
Accelerate:
https://
huggingface.co/docs/transformers/accelerate
#Tokens
#Pred Tok.
BLEU
ROUGE
TF1
Exact
COS
GTR
ME5
GTR
ME5
GTR
ME5
GTR
ME5
GTR
ME5
GTR
ME5
GTR
ME5
Base (0 Steps)
32
32
32
32
0.2718
0.2877
0.6286
0.6368
63.74
65.9
0.4
0.4
0.8793
0.9738
Vec2Text (1 Step)
32
31
32
32
0.4862
0.4792
0.7839
0.7703
78.44
78.35
8
4.8
0.9210
0.9588
(20 Steps)
32
32
32
32
0.8330
0.7447
0.9512
0.8957
95.11
90.3
58
21.8
0.9862
0.9920
(20 Steps)
32
32
32
32
0.8431
0.7503
0.9549
0.8976
95.6
90.56
58.4
21.8
0.9862
0.9920
(50 Steps + 4 sbeam)
32
32
32
32
0.9018
0.7887
0.9726
0.9111
97.15
91.55
74.4
32.6
0.9853
0.9902
(50 Steps + 8 sbeam)
32
32
32
32
0.9244
0.8086
0.9776
0.9189
97.78
92.42
82
35
0.9921
0.9926
(100 Steps)
32
32
32
32
0.9245
0.8082
0.9775
0.9183
97.79
92.37
82
35
0.9921
0.9926
(100 Steps + 4 sbeam)
32
32
32
32
0.9017
0.7882
0.9725
0.9111
97.15
91.53
74.4
32.8
0.9824
0.9902
(100 Steps + 8 sbeam)
32
32
32
32
0.9245
0.8082
0.9775
0.9183
97.79
92.37
82
35
0.9921
0.9926
Table 1: Evaluation of English Text Reconstruction. The best performances for each model reached in the earliest
stages are in bold. The underlined results are where ME5-base model outperforms GTR-base model.
82% exact match for GTR-base model, while the
best performance for ME5-base is 0.8086 in BLEU
score with 35% exact match. The performance dif-
ference could be due to the fact that the GTR-base
is t5-based model, the same structure as the gener-
ation model ψ. However, utilizing ME5-base sets
up a more realistic attack scenario of black-box em-
bedding inversion, as the structure of the embedder
ϕ is unknown.
NQ→MTG-EN
MTG-EN→NQ
MTG-MULTI→NQ
GTR
Base
0.0581 (0.7334)
-
-
Vec2Text
0.3623 (0.9767)
ME5
Base
0.0589 (0.9272)
0.0715 (0.9511)
0.0671 (0.9553)
Vec2Text
0.2119 (0.9440)
0.1535 (0.9669)
0.1079 (0.9708)
Table 2: Cross-Domain English Text Reconstruction
Evaluation, BLEU scores and COS are reported. Hori-
zontal comparison on ME5-Base models, and vertically
on two embedders trained on the same NQ dataset. The
Vec2Text models are evaluated by 50 steps of correc-
tion with sequence beam search width 8. The arrow
→ indicates the cross-domain evaluation direction. For
example, NQ→ MTG-EN indicates that the model is
trained on NQ and evaluated on MTG-EN.
Cross-Domain
To evaluate the performance of
embedding inversion attacks on out-of-domain
dataset in English, the models trained on NQ
and MTG-EN datasets are cross-evaluated on both
datasets, respectively, as shown in Table 2. The
results on MTG-EN are similar in BLEU scores
for both Base models trained on GTR-Base and
ME5-Base, while GTR model outperforms ME5
by more than 0.15 in BLEU scores, and the cosine
similarity of reconstructed and true text embed-
dings are boosted by over 0.24 . In comparison,
the cosine similarity for ME5 models are not much
varied and constantly high (≥ 0.92) across stages
of evaluations and across domains. From the ob-
servations of both in-domain and out-of-domain
English text reconstruction, with solely training
the first stage of inversion model, the multilingual
embeddings model yields better word-matching
performances and the embeddings are closer in the
embedding space. However, the adapted approxi-
mation approach Eq. 2 boosts performance more
on monolingual embedding model.
5.2
Multilingual Text Reconstruction
1 Step
20 Steps
50 Steps
50 Steps + 4 sbeam
50 Steps + 8 sbeam
100 Steps
100 Steps + 4 sbeam
100 Steps + 8 sbeam
0
20
40
60
80
100
0
5k
10k
15k
20k
ME5_MTG-EN BLEU
ME5_MTG-EN Runtime
ME5_MTG-FR BLEU
ME5_MTG-FR Runtime
ME5_MTG-DE BLEU
ME5_MTG-DE Runtime
ME5_MTG-ES BLEU
ME5_MTG-ES Runtime
BLEU Score
RUNTIME (seconds)
Figure 2: BLEU score vs. Runtime by Evaluation for
Inversion Models in English, French, German and Span-
ish.
To explore the potential of multilingual embed-
ding inversion, we train ME5-base embedder on
MTG datasets in English, German and French,
Spanish, noted as ME5_MTG-EN, ME5_MTG-FR,
ME5_MTG-DE and ME5_MTG-ES, respectively,
and the composed multilingual dataset of all four
languages, noted as ME5_MTG-MULTI, and tested
on each language for both experimental settings.
#Tokens
#Pred Tok.
BLEU
ROUGE
TF1
Exact
COS
MONO
MULTI
MONO
MULTI
MONO
MULTI
MONO
MULTI
MONO
MULTI
MONO
MULTI
MONO
MULTI
MTG-EN
Base (0 Steps)
32
32
31.94
31.95
0.1157
0.1079
0.4598
0.4439
44.97
43.71
0
0
0.9381
0.9215
Vec2Text (1 Step)
32
32
31.95
31.96
0.183
0.1338
0.5874
0.4895
56.37
48.22
0.4
0.2
0.9236
0.8637
(20 Steps)
32
32
31.99
31.98
0.4148
0.2372
0.7905
0.6253
75.15
59.74
8.8
3
0.9441
0.8433
(50 Steps)
32
32
31.99
31.97
0.4305
0.2527
0.802
0.6414
76.29
61.39
9.4
3.2
0.9464
0.9296
(50 Steps + 4 sbeam)
32
32
31.99
31.98
0.4587
0.2989
0.827
0.6817
78.24
65.27
10.8
5
0.9372
0.9487
(50 Steps + 8 sbeam)
32
32
31.98
31.98
0.4849
0.3204
0.8351
0.6938
79.16
66.67
12
7.4
0.9277
0.9303
(100 Steps)
32
-
31.98
-
0.4853
-
0.8351
-
79.12
-
12
-
0.9277
-
(100 Steps + 4 sbeam)
32
-
31.99
-
0.459
-
0.8271
-
78.24
-
10.8
-
0.9372
-
(100 Steps + 8 sbeam)
32
-
31.98
-
0.4853
-
0.8351
-
79.12
-
12
-
0.9277
-
MTG-FR
Base [0 Steps]
32
32
32
32
0.1864
0.1981
0.5286
0.552
52.93
55.68
0
0.2
0.9408
0.9511
Vec2Text (1 Step)
32
32
32
31.98
0.291
0.2832
0.6358
0.6308
63.36
63.1
2.6
2
0.9655
0.9271
(20 Steps)
32
32
31.98
32
0.6239
0.5878
0.8412
0.8132
83.48
81.02
36
32
0.9752
0.9492
(50 Steps)
32
32
31.98
32
0.6404
0.6075
0.8518
0.8301
84.51
82.49
36.8
33
0.9754
0.9252
(50 Steps + 4 sbeam)
32
32
32
32
0.7196
0.6872
0.8829
0.867
87.91
86.22
50.4
45.2
0.9643
0.942
(50 Steps + 8 sbeam)
32
32
32
32
0.7454
0.73
0.8912
0.8938
88.83
88.84
54.4
49.6
0.9757
0.942
(100 Steps)
32
-
32
-
0.7444
-
0.891
-
88.77
-
54.4
-
0.9757
-
(100 Steps + 4 sbeam )
32
-
32
-
0.7193
-
0.8826
-
87.89
-
50.4
-
0.9643
-
(100 Steps + 8 sbeam)
32
-
32
-
0.7444
-
0.891
-
88.77
-
54.4
-
0.9757
-
MTG-DE
Base (0 Steps)
32
32
32
31.98
0.133
0.137
0.4313
0.4524
44.6
46.14
0
0
0.9599
0.9642
Vec2Text (1 step)
32
32
31.93
31.98
0.22
0.1808
0.5555
0.5195
56
52.07
1.2
0.2
0.9699
0.9516
(20 Steps)
32
32
31.95
32
0.566
0.4137
0.8095
0.7041
79.84
69.81
30.2
16.6
0.9573
0.9232
(50 Steps)
32
32
31.95
32
0.5736
0.4359
0.8233
0.7228
81.4
71.54
30.4
17.4
0.9687
0.9278
(50 Steps + 4 sbeam)
32
32
31.98
31.98
0.6579
0.5248
0.8584
0.767
84.56
75.75
42.4
28.2
0.9778
0.9321
(50 Steps + 8 sbeam)
32
32
32
32
0.695
0.5408
0.878
0.7757
86.46
76.44
47.4
29.6
0.9671
0.9646
(100 Steps)
32
-
32
-
0.6955
-
0.878
-
86.47
-
47.4
-
0.9791
-
(100 Steps + 4 sbeam)
32
-
31.98
-
0.6561
-
0.8573
-
84.46
-
42.2
-
0.9778
-
(100 Steps + 8 sbeam)
32
-
32
-
0.6955
-
0.878
-
86.47
-
47.4
-
0.9791
-
MTG-ES
Base (0 steps)
32
32
31.95
32
0.2321
0.2709
0.5515
0.6054
56.75
62.07
1.6
1.8
0.938
0.9501
Vec2Text (1 step)
32
32
32
32
0.3518
0.3692
0.6621
0.6804
67.76
68.92
8
9.6
0.9549
0.9423
(20 Steps)
32
32
32
32
0.6661
0.6443
0.8559
0.8461
85.78
84.73
44.8
38.4
0.9632
0.9563
(50 Steps)
32
32
32
32
0.6785
0.6593
0.8661
0.8525
86.67
85.46
45.4
38.8
0.9697
0.9582
(50 Steps + 4 sbeam)
32
32
32
32
0.7729
0.7452
0.9041
0.8945
90.47
89.23
60.8
53.6
0.9697
0.9515
(50 Steps + 8 sbeam)
32
32
32
32
0.8002
0.7772
0.9134
0.9072
91.54
90.44
65
56.8
0.9579
0.987
(100 Steps)
32
-
32
-
0.7996
-
0.9121
-
91.43
-
65
-
0.9579
-
(100 Steps + 4 sbeam)
32
-
32
-
0.7748
-
0.9052
-
90.56
-
60.8
-
0.9697
-
(100 Steps + 8 sbeam)
32
-
32
-
0.7996
-
0.9121
-
91.43
-
65
-
0.9579
-
Table 3: MONO represents the evaluation of Text Reconstruction in multiple languages, with the models trained and
evaluated on MTG datasets with tokens length 32 in English, French, German and Spanish, respectively. MULTI
represents the evaluation of multilingual text reconstruction, with models trained on MTG-MULTI and evaluated on
MTG datasets with tokens length 32 in English, French, German and Spanish, respectively. The best results reached
in the earliest stage for each language across metrics are in bold. The results where MULTI outperforms MONO is
underlined.
The results are shown in Table 3.
Monolingual Text Reconstruction in Multiple
Languages
For monolingual models, we evalu-
ate on Base and Vec2Text models with correction
steps of 1, 20, 50, 100 combined with 4 and 8
sequence beam search width (sbeam). We can ob-
serve that the BLEU score for each language peaks
either by 50 steps correction with 8 sbeam or 100
steps. This evaluation is expensive in terms of time
and computation. In order to search for the optimal
runtime and performance trade-off, Fig. 2 shows
BLEU scores at each step and the lines represent
the trend for runtime for the monolingual models.
The best trade-off points are at the correction step
of 50 with 8 sbeam for all the models, while 100
steps takes more than double the time achieving
similar performance. Until correction step 50 with
8 sbeam, performance increases steadily, and the
trend is generally aligned with cosine similarity. As
a result, we evaluate the subsequent models until
correction step 50 with 8 sbeam.
Moreover, Spanish models outperform the oth-
ers in terms of the word-match metrics across cor-
rection steps, achieving 0.8002 in BLEU score
with 65% of exact match. Despite having a larger
volume of data compared to other languages, the
English model unexpectedly performs the worst
across various metrics, as illustrated by the training
data distribution in Fig. 3. However, as shown in
Appendix B, the evaluation of round-trip translated
English test data indicates no evidence of transla-
tionese effect.
Multilingual Text Reconstruction Without prior
Knowledge of Language
To evaluate the poten-
tial of multilingual text inversion without prior
knowledge of the language in which a target text is
written, we train inversion models on MTG-MULTI
dataset. As shown in Table 3, ME5_MTG-MULTI
Base model outperforms (underlined) or has on
par performance with monolingual Base models
across languages. Overall, the performance does
not deteriorate in proportion of the monolingual
training data volume, i.e., for MTG-MULTI, each
language has a quarter of the data volume compared
to its monolingual counterpart. Rather, the per-
formances are comparable, especially for French
and Spanish, with 0.0154 and 0.023 differences,
respectively. For Spanish, ME5_MTG-MULTI per-
forms slightly better in word-match metrics than
ME5_MTG-ES also for Vec2Text model by 1 step
correction. Across languages, the cosine similari-
ties of the multilingual model are constantly higher
than their monolingual counterparts, with more cor-
rection steps, they worsened for French, i.e., 0.942
compared to 0.9511.
Additionally, to evaluate the out-of-domain per-
formance, ME5_MTG-MULTI is tested on NQ
dataset. As shown in Table 2, for Base models,
ME5-Base trained on multilingual data has better
performance than trained on NQ dataset, while it
fall short compared to its monolingual counterpart.
Its Vec2Text model under-performs compared to
others.
These phenomena indicate that (i) the high
monolingual data volume is not the determining
factor on training a high-performing Base model
and Vec2Text models without the extra correction
steps in both monolingual and multilingual settings,
(ii) the multilingual model training renders closer
embeddings of reconstructed and target texts in
the embedding space, and (iii) the optimization ap-
proach utilizing cosine similarity is not as effective
for multilingual models compared to monolingual
models.
5.3
Cross-lingual Text Reconstruction
Cross-lingual text reconstruction is a specific case
of multilingual text reconstruction without prior
knowledge of languages and also a more realis-
tic scenario, where the embedder ϕ is trained on
a different language with regarding to the target
text language. To investigate the potential of this
MTG-FR
MTG-DE
MTG-ES
GTR-Base
Base
0.0439 (0.7581)
0.0322 (0.7052)
0.0474 (0.7134)
Vec2Text
0.0994(0.8833)
0.0575 (0.8138)
0.1012 (0.9020)
AdTrans
0.0928 (↓-6.59)
0.0518 (↓-9.82)
0.0995 (↓-1.67)
ME5-Base
Base
0.0313 (0.9513)
0.0273 (0.9298)
0.0364 (0.9293)
Vec2Text
0.0449 (0.9487)
0.0299 (0.9107)
0.0392 (0.8963)
AdTrans
0.1017 (↑126.58)
0.0895 (↑128.11)
0.0469 (↑56.57)
(a) Cross-lingual cross-domain evaluation with monolingual
models trained on NQ.
→NQ
ME5_MTG-FR
ME5_MTG-DE
ME5_MTG-ES
Base
0.0180 (0.9399)
0.0182 (0.9016)
0.0141 (0.9178)
Vec2Text
0.0207 (0.9467)
0.0231 (0.9248)
0.0181 (0.9253)
AdTrans
0.0418 (↑103.48)
0.0508 (↑119.9)
0.0356 (↑91.28)
(b) Cross-lingual cross-domain evaluation on NQ with monolin-
gual models trained on MTG datasets.
MTG-EN
MTG-FR
MTG-DE
MTG-ES
ME5_MTG-EN
Base
-
0.032 (0.9132)
0.0371 (0.8945)
0.031 (0.9068)
Vec2Text
-
0.0462 (0.9421)
0.0561 (0.9474)
0.0433 (0.911)
AdTrans
-
0.124 (↑168.08)
0.0672 (↑19.75)
0.1238 (↑185.79)
ME5_MTG-FR
Base
0.033 (0.9176)
-
0.0297 (0.9038)
0.0452 (0.9206)
Vec2Text
0.0536 (0.9235)
-
0.0426 (0.9431)
0.0594 (0.9241)
AdTrans
0.0725 (↑37.71)
-
0.0635 (↑49.47)
0.137 (↑126.79)
ME5_MTG-DE
Base
0.0399 (0.8902)
0.0296 (0.9082))
-
0.0273 (0.9224)
Vec2Text
0.0813 (0.9223)
0.0454 (0.9223)
-
0.0461 (0.9163)
AdTrans
0.0961 (↑18.19)
0.1037 (↑128.62)
-
0.1101 (↑138.91)
ME5_MTG-ES
Base
0.0331 (0.9186)
0.0396 (0.9035)
0.0267 (0.8958)
-
Vec2Text
0.0471 (0.9223)
0.0513 (0.8699)
0.0397 (0.9460)
-
AdTrans
0.0591 (↑25.51)
0.0957 (↑86.56)
0.0556 (↑39.89)
-
(c) Cross-lingual evaluation on monolingual inversion models
trained and test on MTG datasets.
Table 4: Cross-lingual evaluation using BLEU score
and Cosine Similarity (in the brackets) for Base and
Vec2Text models by correction steps of 50 with 8 sbeam.
The BLEU scores and their percentage growth (in the
brackets) compared with BLEU scores on Vec2Text
models are reported for AdTrans strategy for each model.
The up-arrows ↑ indicate performance gain while the
down-arrows ↓ indicate performance loss. The result
with the highest BLEU score with each evaluated model
on each dataset is in bold.
scenario, we conduct cross-lingual evaluation on
all the monolingual models we trained on NQ and
MTG datasets, the results are reported in Table 4,
with regard to the type of models.
We observe that, ME5-Base models trained on
both NQ and MTG-datasets, have a tendency to
decode texts, for example ˆx, in the language of
training data, e.g., ly, given the target text x which
is in a different language, e.g., lx. However, ˆx could
expose the same information just in a different lan-
guage, then the current word-match metrics are not
able to capture this phenomenon. Nonetheless, the
privacy leakage still exists.
For example, evaluating ME5_MTG-DE model
on MTG-EN dataset, Report: Trump einmal fragte
damals FBI Director Andrew Mccabe während
seiner 2016-vote is generated given the target text
Report: Trump once asked then-acting FBI director
Andrew Mccabe about his 2016 vote. The gener-
ated text mistook “während” (during) for “about”,
otherwise, the generated text is close in meaning
with the target English text. The information leak-
age would not be properly captured with the cur-
rent metrics evaluated on the German texts. We use
AdTrans strategy with EasyNMT 5, the generated
German text is translated in English, i.e., Report:
Trump once asked FBI director Andrew Mccabe
during his 2016-vote, in which case information is
missing such as “then-acting”, but BLEU score is
up to 0.5 for this individual example. We imple-
ment this strategy for all cross-lingual evaluation.
As shown in Table 2, other than for GTR-Base
model, the performances are lifted across monolin-
gual and multilingual models for each language.
5.4
Cross-lingual Defense Mechanism
As the underlying architecture is based on Morris
et al. (2023), we point the reader to their paper for
a defence mechanism.
6
Conclusion
While all previous work on embedding inversion
attacks has focused solely on English, we present
the first work on multilingual embedding inversion.
By defining the problem of black-box multilingual
and cross-lingual inversion attacks, we lay the foun-
dation for future work in this direction. As one of
our core findings is the fact that multilingual mod-
els, in some circumstances, are more vulnerable
than monolingual English models, we hope that
this work inspires a multilingual approach to LLM
security and NLP security as a whole.
Limitations
A core limitation of this work is the computation-
ally intense experiments, requiring in the area of
20,000 GPU computing hours. While expanding
this research direction to more languages will fur-
ther increase this expense, we advocate for ensur-
ing that languages other than English are not left
behind in terms of NLP security.
Ethics Statement
This work explores attacks on multilingual em-
bedding models. Our intent with this research is
5https://github.com/UKPLab/EasyNMT
to shed light on the vulnerabilities of languages
other than English, aiming to encourage the com-
munity to include such languages in NLP security
work. While there is potential for misuse by ma-
licious actors, as with many works in NLP secu-
rity, we attempt to mitigate harm by including a
brief pointer to a countermeasure to the attack in
the paper. Moreover, the language models exam-
ined in this paper are open-source models, and thus
this work does not constitute an imminent threat to
embedding-as-a-service providers, who are likely
using private models. Moreover, we do not exper-
iment with truly sensitive data, ensuring that no
real-world harm is caused by the work carried out
in this paper.
Acknowledgements
All authors of this paper are funded by the Carls-
berg Foundation, under the Semper Ardens: Ac-
celerate programme (project nr. CF21-0454). We
are furthermore grateful to the support of the AAU
AI Cloud, and to DeiC for allocating us computing
resources on the LUMI cluster (project nr. DeiC-
AAU-S5-412301).
We thank Sighvatur Sveinn
Davidsson for setting us up with this access, and
for his diligence in assisting with the problems in
the experimental infrastructure, in addition to the
LUMI user support for their very prompt answers
and competence, especially Jing Gong. We further
thank Esther Ploeger for her assistance in testing
translationese effect for the under-performance of
multilingual inversion model in English and Mar-
cell Richard Fekete for his insightful input in proof-
reading the paper.
References
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,
Jianfeng Gao, Xiaodong Liu, Rangan Majumder, An-
drew McNamara, Bhaskar Mitra, Tri Nguyen, Mir
Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,
and Tong Wang. 2018. Ms marco: A human gener-
ated machine reading comprehension dataset.
Steven Bird and Edward Loper. 2004. NLTK: The natu-
ral language toolkit. In Proceedings of the ACL In-
teractive Poster and Demonstration Sessions, pages
214–217, Barcelona, Spain. Association for Compu-
tational Linguistics.
Nicholas Carlini, Chang Liu, Jernej Kos, Úlfar Erlings-
son, and Dawn Song. 2018. The secret sharer: Mea-
suring unintended neural network memorization &
extracting secrets. CoRR, abs/1802.08232.
Nicholas Carlini,
Florian Tramèr,
Eric Wallace,
Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úl-
far Erlingsson, Alina Oprea, and Colin Raffel. 2020.
Extracting training data from large language models.
CoRR, abs/2012.07805.
Yiran Chen, Zhenqiao Song, Xianze Wu, Danqing
Wang, Jingjing Xu, Jiaze Chen, Hao Zhou, and Lei
Li. 2022. MTG: A benchmark suite for multilin-
gual text generation. In Findings of the Association
for Computational Linguistics: NAACL 2022, pages
2508–2527, Seattle, United States. Association for
Computational Linguistics.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale.
Jieren Deng, Yijue Wang, Ji Li, Chenghong Wang,
Chao Shang, Hang Liu, Sanguthevar Rajasekaran,
and Caiwen Ding. 2021. TAG: Gradient attack on
transformer-based language models.
In Findings
of the Association for Computational Linguistics:
EMNLP 2021, pages 3600–3610, Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.
2015. Model inversion attacks that exploit confi-
dence information and basic countermeasures. In
Proceedings of the 22nd ACM SIGSAC Conference
on Computer and Communications Security, CCS
’15, page 1322–1333, New York, NY, USA. Associa-
tion for Computing Machinery.
Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon
Lin, David Page, and Thomas Ristenpart. 2014. Pri-
vacy in pharmacogenetics: An end-to-end case study
of personalized warfarin dosing. In Proceedings of
the 23rd USENIX Conference on Security Symposium,
SEC’14, page 17–32, USA. USENIX Association.
Ishrak Hayet, Zijun Yao, and Bo Luo. 2022. Inver-
net: An inversion attack framework to infer fine-
tuning datasets through word embeddings. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2022, pages 5009–5018, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.
Johannes Höhmann, Achim Rettinger, and Kai Kugler.
2021. Invbert: Text reconstruction from contextu-
alized embeddings used for derived text formats of
literary works. CoRR, abs/2109.10104.
Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, and
Sanjeev Arora. 2020. TextHide: Tackling data pri-
vacy in language understanding tasks. In Findings
of the Association for Computational Linguistics:
EMNLP 2020, pages 1368–1382, Online. Association
for Computational Linguistics.
Donggyu Kim, Garam Lee, and Sungwoo Oh. 2022.
Toward privacy-preserving text embedding similarity
with homomorphic encryption. In Proceedings of the
Fourth Workshop on Financial Technology and Nat-
ural Language Processing (FinNLP), pages 25–36,
Abu Dhabi, United Arab Emirates (Hybrid). Associa-
tion for Computational Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: A benchmark for question answering
research. Transactions of the Association for Compu-
tational Linguistics, 7:452–466.
Haoran Li, Yangqiu Song, and Lixin Fan. 2022. You
don’t know my favorite color: Preventing dialogue
representations from revealing speakers’ private per-
sonas. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 5858–5870, Seattle, United States.
Association for Computational Linguistics.
Haoran Li, Mingshi Xu, and Yangqiu Song. 2023. Sen-
tence embedding leaks more information than you
expect: Generative embedding inversion attack to
recover the whole sentence. In Findings of the As-
sociation for Computational Linguistics: ACL 2023,
pages 14022–14040, Toronto, Canada. Association
for Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out, pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
L. Lyu, Xuanli He, and Yitong Li. 2020. Differentially
private representation for nlp: Formal guarantee and
an empirical study on privacy and fairness. ArXiv,
abs/2010.01285.
Tomas Mikolov, Kai Chen, Gregory S. Corrado, and
Jeffrey Dean. 2013. Efficient estimation of word
representations in vector space. In International Con-
ference on Learning Representations.
John Morris, Volodymyr Kuleshov, Vitaly Shmatikov,
and Alexander Rush. 2023. Text embeddings reveal
(almost) as much as text. In Proceedings of the 2023
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 12448–12460, Singapore.
Association for Computational Linguistics.
Yasmin Moslem, Rejwanul Haque, John Kelleher, and
Andy Way. 2022. Domain-specific text generation
for machine translation. In Proceedings of the 15th
biennial conference of the Association for Machine
Translation in the Americas (Volume 1: Research
Track), pages 14–30, Orlando, USA. Association for
Machine Translation in the Americas.
Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and
Nils Reimers. 2023. Mteb: Massive text embedding
benchmark.
Milad Nasr, Reza Shokri, and Amir Houmansadr.
2019.
Comprehensive privacy analysis of deep
learning: Passive and active white-box inference at-
tacks against centralized and federated learning. In
2019 IEEE Symposium on Security and Privacy (SP).
IEEE.
Rahil Parikh, Christophe Dupuy, and Rahul Gupta. 2022.
Canary extraction in natural language understanding
models. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 552–560, Dublin,
Ireland. Association for Computational Linguistics.
Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word
representation. In Conference on Empirical Methods
in Natural Language Processing.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186–
191, Brussels, Belgium. Association for Computa-
tional Linguistics.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2023. Exploring the limits
of transfer learning with a unified text-to-text trans-
former.
Dinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun
Chen, Xin Wang, Jianfeng Gao, and Lawrence
Carin. 2019. Towards generating long and coherent
text with multi-level latent variable models. arXiv
preprint arXiv:1902.00154.
Reza Shokri, Marco Stronati, and Vitaly Shmatikov.
2016. Membership inference attacks against machine
learning models. CoRR, abs/1610.05820.
Congzheng Song and Ananth Raghunathan. 2020. In-
formation leakage in embedding models. In Pro-
ceedings of the 2020 ACM SIGSAC Conference on
Computer and Communications Security, CCS ’20,
page 377–390, New York, NY, USA. Association for
Computing Machinery.
Xiaofei Sun, Zijun Sun, Yuxian Meng, Jiwei Li, and
Chun Fan. 2022. Summarize, outline, and elabo-
rate: Long-text generation via hierarchical supervi-
sion from extractive summaries. In Proceedings of
the 29th International Conference on Computational
Linguistics, pages 6392–6402, Gyeongju, Republic
of Korea. International Committee on Computational
Linguistics.
Olga Tsymboi, Danil Malaev, Andrei Petrovskii, and
Ivan Oseledets. 2023. Layerwise universal adversar-
ial attack on NLP models. In Findings of the Asso-
ciation for Computational Linguistics: ACL 2023,
pages 129–143, Toronto, Canada. Association for
Computational Linguistics.
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-
ner, and Sameer Singh. 2019. Universal adversarial
triggers for attacking and analyzing NLP. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 2153–2162, Hong
Kong, China. Association for Computational Linguis-
tics.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing
Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,
and Furu Wei. 2022. Text embeddings by weakly-
supervised contrastive pre-training.
Shangyu Xie and Yuan Hong. 2021. Reconstruction
attack on instance encoding for language understand-
ing. In Proceedings of the 2021 Conference on Em-
pirical Methods in Natural Language Processing,
pages 2038–2044, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Mike Zhang and Antonio Toral. 2019. The effect of
translationese in machine translation test sets. In
Proceedings of the Fourth Conference on Machine
Translation (Volume 1: Research Papers), pages 73–
81, Florence, Italy. Association for Computational
Linguistics.
Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang,
Yue Yu, Lizhen Qu, and Zenglin Xu. 2023. Fed-
PETuning:
When federated learning meets the
parameter-efficient tuning methods of pre-trained lan-
guage models. In Findings of the Association for
Computational Linguistics: ACL 2023, pages 9963–
9977, Toronto, Canada. Association for Computa-
tional Linguistics.
Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Yuran Wang,
Yong Ding, Yibo Zhang, Qi Zhang, and Xuanjing
Huang. 2023. TextObfuscator: Making pre-trained
language model a privacy protector via obfuscating
word representations. In Findings of the Associa-
tion for Computational Linguistics: ACL 2023, pages
5459–5473, Toronto, Canada. Association for Com-
putational Linguistics.
Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep
leakage from gradients. In Advances in Neural In-
formation Processing Systems, volume 32. Curran
Associates, Inc.
A
Training Data Distribution
4
14
24
32
0
1M
2M
3M
4M
5M
NQ
MTG_EN
MTG_FR
MTG_DE
MTG_ES
MTG_MULTI
Token Lengths
Nr. of Samples
Figure 3: The Distribution of the training data for mod-
els with the maximal token length of 32.
In MTG datasets, English texts are curated from
various sources, while texts in German, Spanish
and French are machine translated and manually
validated. The languages have diverse morpholo-
gies, resulting in different lengths of sentences and
the number of sentences after sentence tokeniza-
tion across languages. NQ dataset is included to
replicate results from previous work (Morris et al.,
2023), and evaluate cross-domain and cross-lingual
performance of text reconstruction task. NQ has
huge amount of data for English only, and no to-
kenization has been implemented on the included
Wikipedia passages, hence all the training data
from NQ has 32 tokens.
B
The Effect of Translationese on Test
Data
The effect of translationese in machine translation
has been extensively studied, and there is a clear
evidence that the use of translationese in test sets
results in inflated human evaluation scores for MT
systems (Zhang and Toral, 2019). To investigate
whether our multilingual inversion model’s sub-par
performance in English is due to the characteristics
of translationese in other languages, we implement
round trip translation on MTG-EN test data us-
ing Spanish as the pivot language with EasyNMT,
the translation path is thus English → Spanish →
English. Then the evaluation of the multilingual
inversion model is done on the round-trip translated
English test set, the result is shown as in Table 5.
Compared to evaluation on MTG-EN test set, as
shown in Table 3, the performance of translated
English test set is about 0.3 worse at each stage of
corrections. The hypothesis of the translationese
effect on the difference of the performances can
therefore be rejected.
#Tokens
#Pred Tok.
BLEU
ROUGE
TF1
EXACT
COS
Vec2Text (1 Step)
29.59
30.98
0.1003
0.4754
41.28
0.0
0.9046
(20 Steps)
29.59
30.95
0.1448
0.5514
47.80
0.2
0.9130
(50 Steps)
29.59
30.98
0.1511
0.5601
48.56
0.2
0.9261
(50 Steps + 4 sbeam )
29.59
30.88
0.1756
0.6181
52.64
0.2
0.9461
(50 Steps + 8sbeam)
29.59
30.96
0.1742
0.6128
52.44
0.4
0.9185
Table 5: Evaluation of multilingual inversion model on
round-trip translated MTG-EN test dataset.
C
Text Construction on Tokens Length 64
#Tokens
#Pred Tokens
BLEU
ROUGE
TF1
Exact
COS
English
Vec2Text (1 Step)
37.78
43.73
0.1813
0.5933
57.28
0.8
87.94
(20 Steps)
37.78
41.32
0.3848
0.7838
74.23
10.0
88.75
(50 Steps)
37.78
40.97
0.3927
0.7974
75.40
10.2
92.70
(50 Steps + 4 sbeam)
37.78
40.67
0.4523
0.8168
77.31
14.6
89.18
(50 Steps + 8 sbeam)
37.78
40.19
0.4729
0.8334
78.62
16.6
91.09
French
Vec2Text (1 Step)
51.61
57.23
0.2645
0.6358
64.03
0.8
95.07
(20 Steps)
51.61
53.25
0.5825
0.8310
83.01
26.6
96.54
(50 Steps)
51.61
52.60
0.5958
0.8399
83.69
26.8
96.26
(50 Steps + 4 sbeam)
51.61
52.62
0.6461
0.8611
86.03
37.8
97.26
(50 Steps + 8 sbeam)
51.61
52.54
0.6680
0.8674
86.44
41.8
93.83
German
Vec2Text(1 Step)
49.75
56.09
0.1965
0.5458
55.19
0.2
97.43
(20 Steps)
49.75
52.62
0.4611
0.7610
75.30
15.6
93.98
(50 Steps)
49.75
52.76
0.4661
0.7669
75.86
15.8
95.72
(50 Steps + 4 sbeam)
49.75
51.91
0.5278
0.7960
78.93
25.6
92.98
(50 Steps + 8 sbeam)
49.75
51.82
0.5573
0.8087
80.21
30.8
94.97
Spanish
Vec2Text(1 Step)
62.66
62
0.2603
0.6416
65.78
0.4
97.57
(20 Steps)
62.66
62.23
0.5607
0.8353
83.70
17.4
98.28
(50 Steps)
62.66
62.09
0.5673
0.8437
84.46
17.4
97.01
(50 Steps + 4 sbeam)
62.66
61.95
0.6427
0.8678
87.01
29.2
95.39
(50 Steps + 8 sbeam)
62.66
61.76
0.6557
0.8773
87.85
32.8
97.36
Table 6: The evaluation of Text Reconstruction in multi-
ple languages, with the models trained and evaluated on
MTG datasets with maximal token length 64 in English,
French, German and Spanish, respectively.
D
Cross-lingual Evaluation using
AdTrans
+38%
+127%
+103%
+49%
mtg_es
mtg_fr
mtg_de
mtg_en
nq_en
0
2
4
6
8
10
12
14
+18%
+139%
+129%
+120%
0
2
4
6
8
10
12
14
+26%
+87%
+91%
+40%
0
2
4
6
8
10
12
14
+186%
+168%
+20%
0
2
4
6
8
10
12
14
+128%
+127%
+57%
0
2
4
6
8
10
12
14
-1.67%
-6.59%
-9.82%
0
2
4
6
8
10
12
14
Type
Predicted
Translated
Dataset
BLEU
BLEU
BLEU
BLEU
BLEU
BLEU
ME5-MTG-FR
ME5-MTG-DE
ME5-MTG-ES
ME5-MTG-EN
ME5-NQ
GTR-NQ
Figure 4:
Cross-lingual evaluation between recon-
structed texts and translated reconstructed texts.
"
"Automated analysis of chicken behavior is crucial for maintaining optimal health conditions, minimizing economic losses, and improving profitability. This paper presents a novel framework for detecting abnormal chicken behaviors. Two abnormalities, namely inactive broiler and huddling behavior, are explored. The framework consists of three steps: chicken detection, tracking, and abnormality detection. Experiments illustrate the efficacy of the proposed algorithm, offering timely interventions to maintain chicken health and enhance overall productivity.","Chicken meat is a significant source of protein and consumption has increased considerably in the last 30 years. The rapid growth of chicken production increases the risk of spreading diseases. Designing automatic monitoring tools is considered an important topic in the computer vision community. Several AI-based methods have been proposed to help farmers manage poultry houses. However, most of these works are evaluated using private datasets. This paper introduces an innovative framework designed to identify abnormal behaviors in chickens, with a specific emphasis on inactivity and huddling. The framework is engineered for real-time operation and well-suited for edge devices. Inactivity detection holds particular significance as it can serve as an early indicator of chicken sickness.","nanSeveral computer vision and machine learning approaches have been developed for chicken behavior analysis and poultry house management. Techniques include skeleton shape analysis, contour extraction, active contour techniques, IoT sensing devices, watershed segmentation algorithms, and machine learning models. These methods have been employed for various applications, including sickness detection, weight estimation, behavior classification, activity monitoring, and environment preference assessment. Despite these advancements, there is a lack of public datasets for chicken behavior analysis, limiting research in this area. This paper addresses three distinct problems: chicken detection, inactivity detection, and huddling detection. A dataset is collected from the Google search engine, consisting of original and augmented images.","The proposed framework consists of three key components: a chicken detection module, a chicken tracking module, and two abnormality detection modules. The chicken detection module utilizes the YOLO v4 neural network, specifically adapted for broiler detection. The tracker module employs the Centroid tracker algorithm for efficient and low-resource tracking of broilers across consecutive frames. Huddling behavior is detected by computing the Euclidean distance between each detected object and its neighbors, and identifying groups of chickens within a 100-pixel radius. Inactive chicken detection is achieved by measuring the total displacement of chickens over consecutive frames. A threshold is applied to the activity level to identify inactive chickens.","The proposed framework demonstrates promising results in detecting abnormal chicken behaviors. The chicken detection module achieves a mean Average Precision (mAP) of 0.90, indicating accurate detection of chickens in various environments. The huddling detection module achieves a precision, recall, and F1 score of 0.93, 0.88, and 0.85, respectively, effectively identifying congregated chickens. The inactive chicken detection module achieves a precision, recall, and F1 score of 0.92, 0.90, and 0.91, respectively, successfully identifying inactive chickens under different lighting conditions.","The integration of AI in poultry farms offers significant benefits, including early identification of potential issues, elevated animal welfare, and reduced mortality rates. The proposed framework demonstrates the efficacy of AI applications in this domain, with accurate chicken detection, huddling detection, and inactive chicken detection. The framework has the potential to enhance the efficiency and efficacy of poultry farm operations, contributing to improved profitability and animal welfare.",Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses,"Tahereh Zarrat Ehsan, Seyed Mehdi Mohtavipour","Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses 
 
 
1 
 
Broiler-Net: A Deep Convolutional Framework for Broiler 
Behavior Analysis in Poultry Houses 
Tahereh Zarrat Ehsan 1, Seyed Mehdi Mohtavipour 2 
 
1 zarrat.ehsan@gmail.com 
2 mehdi_mohtavipour@elec.iust.ac.ir  
1 School of Electrical Engineering, University of Guilan, Rasht, Iran 
2 School of Electrical Engineering, Iran University of Science and Technology, Tehran, Iran 
Abstract. Detecting anomalies in poultry houses is crucial for maintaining optimal chicken health conditions, 
minimizing economic losses and bolstering profitability. This paper presents a novel real-time framework for 
analyzing chicken behavior in cage-free poultry houses to detect abnormal behaviors. Specifically, two 
significant abnormalities, namely inactive broiler and huddling behavior, are investigated in this study. The 
proposed framework comprises three key steps: (1) chicken detection utilizing a state-of-the-art deep learning 
model, (2) tracking individual chickens across consecutive frames with a fast tracker module, and (3) detecting 
abnormal behaviors within the video stream. Experimental studies are conducted to evaluate the efficacy of 
the proposed algorithm in accurately assessing chicken behavior. The results illustrate that our framework 
provides a precise and efficient solution for real-time anomaly detection, facilitating timely interventions to 
maintain chicken health and enhance overall productivity on poultry farms. 
Github: https://github.com/TaherehZarratEhsan/Chicken-Behavior-Analysis   
Keywords: Convolutional Neural network, Chicken detection and tracking, Inactivity detection 
1. Introduction 
Chicken meat is a significant source of protein for people, and its consumption has increased considerably in 
the last 30 years [1]. According to recent research, the global poultry market is expected to grow from $352.02 
billion in 2022 to $487.39 billion in 2027, with a compound annual growth rate of 7.6%. This growth places 
formidable challenges on farmers to meet the escalating demand. Traditionally, farmers have relied on manual 
observation for monitoring chicken conditions, a time-consuming and error-prone task due to the sheer number 
of chickens. However, the decreasing cost of technological devices [2] and introduction of powerful Deep 
Neural Network (DNN) [3] have facilitated the widespread adoption of monitoring systems in commercial 
farms. Leveraging artificial intelligence (AI), these systems autonomously monitor chickens, offering farmers 
an efficient means to manage poultry houses and enhance profitability. 
The rapid growth of chicken production also increases the risk of spreading diseases, leading to economic 
losses and posing threats to human health. Therefore, designing an automatic monitoring tools to improve farm 
welfare is an important topic in the computer vision community. In [4], bird postures are analyzed using 
skeleton shapes to detect sickness, employing an ellipse segmentation algorithm and handcrafted features for 
classification. Support vector machines are then trained to classify broilers as healthy or sick. Similarly, in [5], 
contours are extracted, and the distance between the highest point of the chicken body and the camera sensor 
is computed to classify birds as standing or lying. Another method proposed in [6] to detect sick chickens in 
caged farms by segmenting chicken body parts using active contour techniques to obtain the heads. The time 
period of eating and drinking is calculated, and chickens with slow behavior are selected as sick. In [7], 
chickens are equipped with wearable IoT sensing devices to obtain their behavior pattern over time. Generative 
Adversarial Network (GAN) is utilized to produce synthetic data and increase the size of the dataset. Several 
machine learning models are trained on a combination of real and synthetic data to classify samples as sick or 
healthy. A monitoring system based on image processing technique is proposed in [8] to predict the weight of 
Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses 
 
 
2 
 
the chicken. This system utilized a watershed segmentation algorithm to segment the images and handcrafted 
descriptors to capture weight information. Bayesian Neural Network (BNN) is trained on these features to 
predict the weight. In [9], a model was developed to analyze the drinking and feeding behavior of broilers 
using machine learning models to estimate the total number of birds at drinkers and feeders. Additionally, [10] 
proposed a neural network-based method to detect the stunned state in chickens using a Convolutional Neural 
Network (CNN) model. Another CNN model is designed in [11] to classify chicken behaviors to six classes 
of standing, walking, running, eating, resting and preening. A model is designed to estimate chicken pose and 
a naïve Bayes classifier is trained on the chicken pose to categorize behaviors. Similarly, another behavior 
classification method is presented in [12] which classify behaviors to three classes of eating, sleeping and 
waling. Chicken trajectories are obtained and handcrafted features are extracted from the trajectories. Several 
machine learning models including logistic regression and naïve base classifier are used for classification.  
A tracking method is proposed in [13] to monitor chickens in consecutive frames of the video. A regression 
neural network is developed to find the location of the chicken in the next frame based on the chicken bounding 
box from the previous frame. Another tracking tool is developed in [14] to detect and track chickens across 
the video. You Only Look Once (YOLO) model is utilized in this work to detect chicken in each frame and 
kalman filter is used for tracking. Similarly, a tracking method is proposed in [15] to obtain chicken trajectory 
over time. In [16], a combination of YOLO object detector and deep sort tracking algorithm is utilized to obtain 
a mobility assessment framework. Authors in [17] proposed a segmentation network to separate chicken from 
the background in the poultry house. A multi-scale encoder decoder network with attention module is designed 
to focus on important features for segmentation.  Another YOLO model is presented in [18] to detect cage-
free chicken on the litter floor. Similarly, a method based on YOLO is presented in [19] to detect chicken face 
from the image. Generative Adversarial Networks (GAN) are employed for data augmentation, enhancing the 
dataset's diversity. The YOLO (You Only Look Once) architecture is refined to achieve improved accuracy in 
detecting small-size targets. In [20], another YOLO model is designed for laying and bath-dusting behavior 
classification. Finally, in [21], different light colors and temperature environments are created and chickens 
behaviors in these environments are analyzed with YOLO model for more than 648 hours to assess their 
environment preference. Table 1 provides details of artificial intelligence-based methods designed to aid 
farmers in managing poultry houses. As can be seen, all the previous works are evaluated using private 
datasets. To the best of our knowledge, there is no public dataset for analyzing the abnormality in chickens.  
This paper introduces an innovative framework designed to identify abnormal behaviors in chickens, with a 
specific emphasis on inactivity and huddling. Engineered for real-time operation, the framework is well-suited 
for on-the-edge devices. Inactivity detection holds particular significance as it can serve as an early indicator 
of chicken sickness. Sick chickens often display sedentary behavior and limited movement within the poultry 
house. Furthermore, huddling behaviors are also detected, where chickens gather closely together. In severe 
instances, chickens may pile on top of each other, potentially leading to illness and fatalities. To mitigate these 
issues, an artificial intelligence-based model is developed to promptly identify and alert farmers to these 
abnormal behaviors. The ultimate objective of this model is to substantially enhance the health and welfare of 
chickens, thereby elevating overall farm productivity. 
Table 1. Related works based on computer vison and machine learning techniques 
Reference 
Application 
Dataset availability 
[4-7] 
Sick chicken detection 
Private 
[8] 
Chicken body weight estimation 
Private 
[9-12], [20, 21] 
 Chicken behavior classification 
Private 
[18] 
Chicken detection 
Private 
[19] 
Chicken face detection 
Public 
[13], [15] 
Chicken tracking  
Private 
[14], [16] 
Chicken detection & tracking  
Private 
[17] 
Chicken segmentation 
Private 
Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses 
 
 
3 
 
The remainder of this paper is as follows, section 2 provides a detailed discussion on the dataset and the 
proposed framework. Section 3 describes the results obtained, offering insights into the outcomes of the study. 
Finally, the concluding remarks are presented in the last section of this paper 
2. Material and methods 
2.1 Dataset 
According to our survey, the lack of a public dataset for chicken welfare analysis in poultry houses presents a 
significant challenge in this field, limiting researchers' ability to work on this topic. Additionally, the number 
of research papers on chicken behavior analysis lags considerably behind other computer vision domain such 
as human behavior analysis [22-24] and human abnormal behavior detection [25-27].  
In this paper, we address three distinct problems: chicken detection, inactivity detection, and huddling 
detection. To collect the dataset, we gathered videos of chickens in cage-free poultry farms from the Google 
search engine. For chicken detection, each video was divided into frames and frames labeled using the 
LabelImg software as it is shown in figure 1. A bounding box was manually drawn for each chicken, and XML 
files were produced which contains the bounding box coordinates and corresponding labels. These XML files 
and frames were then used to train the chicken detection module. Since the training dataset was not large-scale, 
we utilized data augmentation techniques such as vertical and horizontal shift, rotation, and brightness changes 
to increase the size of the training dataset. This allowed the model to learn to recognize different variations of 
the samples and improve training. The details of the dataset can be found in Table 1, which includes both the 
original and augmented datasets consisting of 2080 and 10400 images, respectively. The dataset was split into 
90% for training, 10% for validation, and 10% for testing.  
For abnormality detection, frames were classified as depicting huddling when more than 10 chickens were 
observed closely congregating within a 100-pixel radius. Careful examination of the dataset allowed us to 
annotate 123 frames exhibiting the huddling condition. In the detection of inactive broilers, chickens were 
labeled as inactive if their movements were less than 20 pixels in 50 consecutive frames, resulting in the 
observation and manual annotation of a total of 71 inactive chickens within the dataset. Since huddling and 
inactivity detection do not need training in our framework, there was no need for data augmentation or the 
establishment of train, validation, and test splits. The dataset statistics are succinctly summarized in Table 2.  
Table 2. Statistics of dataset 
 
Original dataset 
Augmented dataset 
train 
validation 
test 
Chicken detection  
2080 
10400 
11232 
1248 
1248 
Huddling detection 
123 
- 
- 
- 
- 
Inactivity detection 
71 
- 
- 
- 
- 
 
Figure 1. LabelImg environment for data annotation 
Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses 
 
 
4 
 
2.2 Flowchart of the proposed framework 
Figure 2 illustrates the flowchart of our proposed framework for detecting abnormal behavior in chickens. The 
framework comprises three key components: a chicken detection module, a chicken tracking module, and two 
abnormality detection modules. Firstly, the input video is divided into frames, and in each frame, chickens are 
extracted to identify huddling behavior. Subsequently, the movement of the chickens is tracked across 
consecutive frames to analyze their activity levels and detect inactivity abnormality. In the subsequent sections, 
we will provide a comprehensive discussion of each component.  
Input video
Read frame
Extract boxes 
with chicken 
detection 
module
Input boxes to 
tracker module
Compute 
centroid of each 
box
Compare 
distances of 
each new 
centroid to IDs
Assign new 
centroid to the 
nearest ID
Register new ID 
for each 
centroid
Increase the 
counter of 
unassigned IDs
Counter > 
threshold 
Delete the 
IDs
Register new ID 
for each new 
centroid
Is frame 1
Centroid Tracker
Inactive chicken 
detection
Huddling 
detection
Any 
unassigned 
IDs
Any unassigned 
Input centroids
Yes
No
Yes
No
Yes
Flowchart
 
Figure 2. Flowchart of the proposed framework for abnormal chicken detection 
 
2.3 Chicken detection module 
Object detectors are composed of three main parts: the backbone, neck, and head. The backbone extracts 
features from the input image using convolutional layers. These layers extract features at different scales, 
ranging from low-level features like edges and contours to high-level features like shapes and object parts. 
Popular networks used for the backbone include DarkNet [28], EfficientNet [29] and ResNet [30]. The 
extracted features are then combined in the head part to obtain more informative features. Feature Pyramid 
Network (FPN) [31] and Path Aggregation Network (PANet) [32] are two well-known head methods is 
responsible for detecting objects based on the extracted features. Finally, non-maximum suppression (NMS) 
is used to remove duplicate objects.  
In this work, as it is shown in figure 3, YOLO v4 network [33] is specialized for broiler detection. Since there 
is no broiler class in the COCO dataset on which YOLO v4 is trained, we use our own dataset for training. In 
order to specifically detect the broiler class, we have made modifications to the final layers of YOLO v4. This 
adaptation is crucial as YOLO v4 is originally designed to detect 80 different classes, whereas our research 
objective focuses solely on identifying broilers. By modifying the final layers, we ensure that our model is 
trained to accurately detect and classify broilers, while disregarding other classes. As our dataset is not large-
Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses 
 
 
5 
 
scale, training YOLO from scratch is not feasible. Instead, we start with a pre-trained YOLO model and fine-
tuned it on the broiler images. This approach allows the model to utilize previously learned features and 
incorporate new information without the need to train from scratch on a large-scale dataset.  
CSPDarknet-53 which consists of 53 convolutional layers with Cross Stage Partial (CSP) blocks is utilized as 
the backbone. CSP divides the feature maps into two parts and computes the residual connection only in one 
part, which reduces computational complexity. It also improves gradient flow in the residual connections and 
enhances model convergence. Therefore, CSPDarknet-53 can extract valuable information with low 
computational costs. The extracted information is then fed to the Spatial Pyramid Pooling (SPP) layer, where 
three maxpooling layers with sliding window sizes of 5, 9, and 13 are applied to extract features at different 
scales. For example, a window size of 5 focuses on smaller objects while a window size of 13 pays more 
attention to larger objects. The outputs of the maxpooling layers are concatenated and passed to the neck part. 
In the neck part, PANet is used to enhance information flow through the pipeline. PANet employs a top-down 
and bottom-up path to propagate information in the model. As features pass through layers, the image 
resolution decreases and the network extracts more semantically complex features. PANet combines earlier 
and deeper layers to leverage both low-level and semantically rich features. Features at different scales are 
then fed to the final head part for detection, with each head focused on detecting objects of a specific scale.  
 
CSP-1
CSP-2
CSP-8
CSP-8
CSP-4
CBL
CBL
CBL
CBL
CBL
CBL
CBL
CBL
CBL
CBL
CBL
CBL
CBL
CBL
CBL
Up
CBL
CBL
CBL
CBL
CBL
CBL
Up
CBL
CBL
CBL
CBL
CBL
CBL
SPP
Maxpool 13 
Maxpool 5 
Maxpool 9 
CBM
CBL
CBL
CBL
CBL
CBL
CBL
SPP
CBM
Conv 2D
BN
Mish
=
CBL
Conv 2D
BN
LeakyRelu
=
CSP_n
CBM
CBM
ResUnit
CBM
CBM
Concat
× n
CBM
CBM
Input
Out
Head 3
Head 2
Head 1
52 x 52 x24
26 x 26 x 24
13 x 13 x 24
Head
Neck
Backbone
Backbone
 
Figure 3. YOLO v4 architecture for chicken detection 
2.4 Tracker module 
To track broilers across video frames, the centroid tracker [34] is employed due to its speed and low 
computational resource requirement. The Centroid tracker is particularly suitable for running on edge devices 
with limited computational capabilities as it solely relies on Euclidean distance computation. As it is shown in 
figure 2, centroid tracker comprises three main components. Firstly, broilers are detected at each frame using 
the YOLO detector. This step allows us to identify the presence of broilers in every frame of the video. Next, 
the Euclidean distance between the newly detected broilers and the broilers present in the previous frames is 
computed. Finally, we assign the new broilers to the broilers with the lowest distance. By assigning each new 
broiler to its nearest counterpart from the previous frames, a consistent tracking system for individual broilers 
across the video frames is established. An illustrative example of the centroid tracker can be seen in the figure 
4. In this example, ID 1, ID 2, and ID 3 represent the broilers detected at frame t. YOLO is then applied to 
frame t+1 to detect new broilers denoted by Unknown 1, Unknown 2, and Unknown 3. The Euclidean distance 
between each Unknown broiler and the IDs is computed, and the Unknown broilers are assigned to the nearest 
ID based on the distance calculation. This integration of YOLO v4 and the Centroid tracker allows to 
Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses 
 
 
6 
 
accurately detect and track broilers in video frames, providing valuable insights for broiler monitoring.  
Objects at frame t are denoted by red bounding 
box and ID number
Unknown1
Known 2
Uknown 3
New Objects at frame t+1 are denoted by blue 
bounding box
ID 1
ID 2
ID 3
Black lines denotes Euclidian distance
 
Figure 4. Broiler tracking idea 
2.5 Huddling behavior module 
In order to address the issue of huddling behavior in chickens, which can occur due to various factors such as 
cold environments, limited coop space, and potential threats, we have developed a model specifically designed 
to detect huddling behavior in cage-free houses. To achieve this, the Euclidean nearest neighbor search 
technique [35] is utilized, which allows to determine the total number of chickens within a fixed radius 
surrounding each detected object. A radius of 100 pixels around each object is chosen and the number of 
chickens present within this radius is calculated. If the total number of chickens exceeds a certain threshold, 
the frame is classified as huddling behavior. Our findings indicate that huddling behavior is predominantly 
observed when more than ten chickens are located within a radius of 100 pixels. Therefore, the threshold is set 
to 10 for the final evaluation of the model. By implementing this method, farmers are provided with a tool to 
identify huddling behavior in its early stages and take preventive measures to avoid potential losses. This 
approach enables proactive management of chicken welfare and aids in maintaining optimal living conditions 
for the flock. 
2.6 Inactive broiler detection 
To address the issue of identifying abnormal broilers in poultry houses, we propose a method based on 
measuring the activity level of chickens. Inactive chickens are often indicative of sickness, as healthier 
chickens tend to move more within the coop. To measure the activity level, the total displacement of chickens 
in consecutive frames is computed. In each frame, the chicken is detected and represented with a rectangular 
bounding box with four values: 𝑥𝑚𝑖𝑛 ,𝑦𝑚𝑖𝑛, 𝑤𝑖𝑑𝑡ℎ and ℎ𝑒𝑖𝑔ℎ𝑡 which 𝑥𝑚𝑖𝑛and 𝑦𝑚𝑖𝑛are the x and y coordinate 
of the lower left corner of the rectangle. The displacement between two frames is then calculated using the 
following formula: 
𝐷𝑖𝑠𝑝𝑙𝑎𝑐𝑒𝑚𝑒𝑛𝑡(𝑖, 𝑖 − 1) = √(𝐶𝑒𝑛𝑡𝑟𝑜𝑖𝑑𝑥(𝑖) − 𝐶𝑒𝑛𝑡𝑟𝑜𝑖𝑑𝑥(𝑖 − 1))2 + (𝐶𝑒𝑛𝑡𝑟𝑜𝑖𝑑𝑦(𝑖) − 𝐶𝑒𝑛𝑡𝑟𝑜𝑖𝑑𝑦(𝑖 − 1))
2  (1) 
Where 𝐶𝑒𝑛𝑡𝑟𝑜𝑖𝑑𝑥(𝑖) and 𝐶𝑒𝑛𝑡𝑟𝑜𝑖𝑑𝑦(𝑖) are the x and y coordinate of the centroid of the bounding box in frame 𝑖 
and computed as follows: 
𝐶𝑒𝑛𝑡𝑟𝑜𝑖𝑑𝑥, 𝐶𝑒𝑛𝑡𝑟𝑜𝑖𝑑𝑦 = 𝑥𝑚𝑖𝑛 +  0.5𝑤𝑖𝑑𝑡ℎ, 𝑦𝑚𝑖𝑛 + 0.5𝐻𝑒𝑖𝑔ℎ𝑡  (2) 
The activity level is obtained by summing up the displacements over a specified number of consecutive 
frames (T): 
𝐴𝑐𝑡𝑖𝑣𝑖𝑡𝑦𝑙𝑒𝑣𝑒𝑙 = ∑
𝑡+𝑇 𝐷𝑖𝑠𝑝𝑙𝑎𝑐𝑒𝑚𝑒𝑛𝑡(𝑖, 𝑖 − 1)
𝑖=𝑡
  (3) 
Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses 
 
 
7 
 
To identify inactive chickens, a threshold is applied to the activity level. Chickens with activity levels below 
this threshold are considered candidates for sickness and can be further analyzed by farmers to assess their 
health conditions. This method saves farmers significant time and effort compared to individually analyzing 
each chicken's health condition, thereby increasing productivity. In conclusion, the proposed method offers a 
practical and efficient approach to detect abnormal broilers based on their activity levels. By quickly 
identifying potentially sick chickens, farmers can take timely action to prevent further spread of disease and 
ensure the overall well-being of their flock 
3 Results 
In this section, the results of our proposed framework is presented. Recall, Precision, 𝐹1score and mean 
Average Precision (mAP) is utilized for evaluating the proposed framework. Recall is calculated as the number 
of correctly detected samples divided by the total number of samples: 
𝑅𝑒𝑐𝑎𝑙𝑙 =
𝑇𝑃
𝑇𝑃 + 𝐹𝑁     (4) 
Precision measures the percentage of correct predictions out of the total number of detected samples: 
𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =
𝑇𝑃
𝑇𝑃 + 𝐹𝑃      (5) 
𝐹1 score is a measure of overall model performance: 
𝐹1𝑠𝑐𝑜𝑟𝑒 = 2 × 𝑅𝑒𝑐𝑎𝑙𝑙 × 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛
𝑅𝑒𝑐𝑎𝑙𝑙 + 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛
         (6) 
mAP is a metric for measuring the accuracy of the object detector. It is computed using the following equation: 
𝑚𝐴𝑃 =
∑
𝐴𝑃𝑖
𝑛𝑢𝑚_𝑐𝑙𝑎𝑠𝑠𝑒𝑠
𝑖=1
𝑛𝑢𝑚_𝑐𝑙𝑎𝑠𝑠𝑒𝑠        (7) 
Which 𝐴𝑃𝑖 is the average precision for class 𝑖. Since the chicken detection problem only involves one class of 
chicken, mAP is equivalent to AP. mAP is obtained by computing the area under the Precision-Recall (PR) 
curve.   
Table 3 presents the results of our proposed work for each part of chicken detection, huddling detection, and 
inactive chicken detection. Since mAP is an object detector metric, it is only reported for chicken detection. 
As shown in the table, the chicken detection module accurately detects chickens in the frame with a mAP value 
of 0.90. For huddling detection, the model can detect 109 out of 123 samples with a precision, recall, and 
F_1score of 0.93, 0.88, and 0.85, respectively. The false negative samples occur due to occlusion, where the 
chicken detection module fails to detect occluded chickens, resulting in improper huddling detection. The final 
part of the model is inactive chicken detection, which is reported in Table 2. The model successfully detects 
64 out of 71 inactive chickens, with a precision, recall, and F_1score of 0.92, 0.90, and 0.91, respectively. 
Figure 4 showcases the outcomes of chicken detection for four randomly selected samples. Even in challenging 
environments, the trained model demonstrates its proficiency in accurately detecting chickens within the 
frame. In samples 1 to 3, featuring images with a multitude of chickens, the model adeptly identifies the 
majority of them. Sample 4 introduces a new image with distinct lighting conditions. Despite not being trained 
specifically for this lighting scenario, the model exhibits a high degree of accuracy in detecting the majority 
of the chickens. Figure 5 presents the outcomes of huddling detection for three randomly chosen samples. The 
proposed method effectively identifies congregated chickens across various lighting conditions. In Figure 6, 
the results of inactive chicken detection for two random samples are depicted. The model demonstrates 
robustness to variations in light conditions and can proficiently identify inactive chickens in diverse 
environments. 
Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses 
 
 
8 
 
Table 3. Result of the proposed framework  
 
Total 
TP 
FP 
FN 
Precision 
Recall 
F1 score 
mAP 
Chicken detection 
2048 
1145 
100 
103 
0.92 
0.91 
0.91 
0.90 
Huddling detection 
123 
109 
8 
22 
0.93 
0.88 
0.85 
- 
Inactive chicken detection 
71 
64 
5 
7 
0.92 
0.90 
0.91 
- 
 
a) sample 1 
 
b) sample 2 
 
c) sample 3 
  
d) sample 4 
Figure 5. Chicken detection in four random samples using the trained YOLO model 
 
a) sample 1 
 
b) sample 2 
 
c) sample 3 
Figure 6. Huddling detection for three random samples 
Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses 
 
 
9 
 
 
 
Figure 7. Inactive chicken detection for two random samples  
4 Conclusion 
In conclusion, the integration of AI in poultry farms emerges as a transformative advancement, offering 
significant benefits. The capability for automated and continuous monitoring of chicken behavior not only 
facilitates the early identification of potential issues or abnormalities but also contributes significantly to 
elevated animal welfare and reduced mortality rates on the farm. This, in turn, empowers farmers with the 
tools for more informed decision-making and the strategic optimization of resources. Our proposed framework 
for detecting abnormal chicken behavior demonstrates the efficacy of AI applications in this domain. The 
chicken detection module, achieving an impressive mAP value of 0.90, exhibits exceptional accuracy in 
identifying chickens within the frame, even in challenging environments. Equally noteworthy, the huddling 
detection module attains high precision, recall, and F1 score values of 0.93, 0.88, and 0.85, respectively. 
Similarly, the inactive chicken detection module demonstrates remarkable performance, successfully 
identifying inactive chickens with precision, recall, and F1 score values of 0.92, 0.90, and 0.91, respectively, 
regardless of lighting conditions. In totality, our framework consistently delivers reliable performance in the 
detection and analysis of chicken behavior, highlighting the substantial potential of AI technology in elevating 
the efficiency and efficacy of poultry farm operations. 
References 
[1] Roiter L, Vedenkina I, Eremeeva N. Analysis of the market potential of poultry meat and its forecast.  IOP 
Conference Series: Earth and Environmental Science: IOP Publishing; 2021. p. 022104. 
[2] Mohtavipour SM, Shahhoseini HS. A low-cost distributed mapping for large-scale applications of 
reconfigurable computing systems.  2020 25th International Computer Conference, Computer Society of Iran 
(CSICC): IEEE; 2020. p. 1-6. 
[3] Mohtavipour SM, Shahhoseini HS. GCN-RA: A graph convolutional network-based resource allocator for 
reconfigurable systems. Journal of Computational Science. 2023;74:102178. 
[4] Zhuang X, Bi M, Guo J, Wu S, Zhang T. Development of an early warning algorithm to detect sick broilers. 
Computers and Electronics in Agriculture. 2018;144:102-13. 
[5] Aydin A. Using 3D vision camera system to automatically assess the level of inactivity in broiler chickens. 
Computers and Electronics in Agriculture. 2017;135:4-10. 
[6] Xiao L, Ding K, Gao Y, Rao X. Behavior-induced health condition monitoring of caged chickens using 
binocular vision. Computers and Electronics in Agriculture. 2019;156:254-62. 
[7] Ahmed G, Malick RAS, Akhunzada A, Zahid S, Sagri MR, Gani A. An approach towards IoT-based 
predictive service for early detection of diseases in poultry chickens. Sustainability. 2021;13:13396. 
[8] Mortensen AK, Lisouski P, Ahrendt P. Weight prediction of broiler chickens using 3D computer vision. 
Computers and Electronics in Agriculture. 2016;123:319-26. 
[9] Li G, Zhao Y, Purswell JL, Du Q, Chesser Jr GD, Lowe JW. Analysis of feeding and drinking behaviors of 
group-reared broilers via image processing. Computers and Electronics in Agriculture. 2020;175:105596. 
[10] Ye C-w, Yu Z-w, Kang R, Yousaf K, Qi C, Chen K-j, et al. An experimental study of stunned state detection 
for broiler chickens using an improved convolution neural network algorithm. Computers and Electronics in 
Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses 
 
 
10 
 
Agriculture. 2020;170:105284. 
[11] Fang C, Zhang T, Zheng H, Huang J, Cuan K. Pose estimation and behavior classification of broiler 
chickens based on deep neural networks. Computers and Electronics in Agriculture. 2021;180:105863. 
[12] Mohialdin AM, Elbarrany AM, Atia A. Chicken Behavior Analysis for Surveillance in Poultry Farms. 2023. 
[13] Fang C, Huang J, Cuan K, Zhuang X, Zhang T. Comparative study on poultry target tracking algorithms 
based on a deep regression network. Biosystems Engineering. 2020;190:176-83. 
[14] Neethirajan S. ChickTrack–a quantitative tracking tool for measuring chicken activity. Measurement. 
2022;191:110819. 
[15] Brunet H, Concordet D. Optimal estimation of broiler movement for commercial tracking. Smart 
Agricultural Technology. 2023;3:100113. 
[16] Jaihuni M, Zhao Y, Gan H, Tabler T, Qi H, Prado M. Broiler Mobility Assessment Via a Semi-Supervised 
Deep Learning Model and Neo-Deep Sort Algorithm. Available at SSRN 4341431. 
[17] Li W, Xiao Y, Song X, Lv N, Jiang X, Huang Y, et al. Chicken image segmentation via multi-scale attention-
based deep convolutional neural network. IEEE Access. 2021;9:61398-407. 
[18] Yang X, Chai L, Bist RB, Subedi S, Wu Z. A deep learning model for detecting cage-free hens on the litter 
floor. Animals. 2022;12:1983. 
[19] Ma X, Lu X, Huang Y, Yang X, Xu Z, Mo G, et al. An Advanced Chicken Face Detection Network Based 
on GAN and MAE. Animals. 2022;12:3055. 
[20] Sozzi M, Pillan G, Ciarelli C, Marinello F, Pirrone F, Bordignon F, et al. Measuring Comfort Behaviours 
in Laying Hens Using Deep-Learning Tools. Animals. 2022;13:33. 
[21] Kodaira V, Siriani ALR, Medeiros HP, De Moura DJ, Pereira DF. Assessment of Preference Behavior of 
Layer Hens under Different Light Colors and Temperature Environments in Long-Time Footage Using a 
Computer Vision System. Animals. 2023;13:2426. 
[22] Ehsan TZ, Nahvi M, Mohtavipour SM. DABA-net: deep acceleration-based AutoEncoder network for 
violence detection in surveillance cameras.  2022 International Conference on Machine Vision and Image 
Processing (MVIP): IEEE; 2022. p. 1-6. 
[23] Ehsan TZ, Nahvi M, Mohtavipour SM. An accurate violence detection framework using unsupervised 
spatial–temporal action translation network. The Visual Computer. 2023:1-21. 
[24] Ehsan TZ, Nahvi M. Violence detection in indoor surveillance cameras using motion trajectory and 
differential histogram of optical flow.  2018 8th International Conference on Computer and Knowledge 
Engineering (ICCKE): IEEE; 2018. p. 153-8. 
[25] Ehsan TZ, Nahvi M, Mohtavipour SM. Learning deep latent space for unsupervised violence detection. 
Multimedia Tools and Applications. 2023;82:12493-512. 
[26] Ehsan TZ, Mohtavipour SM. Vi-Net: a deep violent flow network for violence detection in video 
sequences.  2020 11th International Conference on Information and Knowledge Technology (IKT): IEEE; 
2020. p. 88-92. 
[27] Mohtavipour SM, Saeidi M, Arabsorkhi A. A multi-stream CNN for deep violence detection in video 
sequences using handcrafted features. The Visual Computer. 2022:1-16. 
[28] Redmon J, Farhadi A. Yolov3: An incremental improvement. arXiv preprint arXiv:180402767. 2018. 
[29] Tan M, Le Q. Efficientnet: Rethinking model scaling for convolutional neural networks.  International 
conference on machine learning: PMLR; 2019. p. 6105-14. 
[30] Xie S, Girshick R, Dollár P, Tu Z, He K. Aggregated residual transformations for deep neural networks.  
Proceedings of the IEEE conference on computer vision and pattern recognition2017. p. 1492-500. 
[31] Lin T-Y, Dollár P, Girshick R, He K, Hariharan B, Belongie S. Feature pyramid networks for object 
detection.  Proceedings of the IEEE conference on computer vision and pattern recognition2017. p. 2117-25. 
[32] Liu S, Qi L, Qin H, Shi J, Jia J. Path aggregation network for instance segmentation.  Proceedings of the 
IEEE conference on computer vision and pattern recognition2018. p. 8759-68. 
[33] Bochkovskiy A, Wang C-Y, Liao H-YM. Yolov4: Optimal speed and accuracy of object detection. arXiv 
preprint arXiv:200410934. 2020. 
[34] Anderson F. Real time, video image centroid tracker.  Acquisition, Tracking, and Pointing IV: SPIE; 1990. 
p. 82. 
[35] Ram P, Sinha K. Revisiting kd-tree for nearest neighbor search.  Proceedings of the 25th acm sigkdd 
international conference on knowledge discovery & data mining2019. p. 1378-88. 
Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses 
 
 
11 
 
 
"
"In this work, we introduce Human-LRM, a trained-once feed-forward model designed to predict the Neural Radiance Fields (NeRF) of humans from a provided single image. The proposed model takes advantage of comprehensive training datasets containing multi-view capture and 3D scans. And through a novel strategy that distills multi-view reconstruction into single-view via a conditional triplane diffusion model, Human-LRM achieves improvements in handling unseen body parts from occlusions, making it suitable for in-the-wild scenarios.","Reconstructing human 3D models exclusively from a single image is a significant topic in computer vision because of its practical applications in areas like AR/VR, asset creation, relighting and many more. Presently, there exists a wide array of techniques for tackling this challenging task, each technique comes with its own set of advantages and limitations. Human Mesh Recovery (HMR) methods, also known as parametric reconstruction methods, have the ability to regress pose and shape parameters of SMPL (Skinned Multi-Person Linear) human body mesh model. However, such parametric methods are limited in representing rich clothing details, thereby limiting their application in scenarios requiring realistic and detailed human representations.","nanThere have been various works that utilize NeRF as a representation for learning human geometry and appearance, however, these works typically perform fine-tuning only on single images, which is not generalizable to new observations. New advancements in feed-forward NeRF prediction models, such as Large Reconstruction Model (LRM), address issues of generalizability and offer high-quality 3D reconstructions. Yet, applying pre-trained generic LRM to humans directly, even with fine-tuning, yields sub-optimal results due to issues like coarse reconstructed surfaces, which lack adequate detail preservation.","Our proposed method, Human-LRM, is a specialized LRM for humans that boasts enhanced surface fidelity and benefits from training on an extensive dataset encompassing more than 10K shapes with a combination of multi-view RGB data and 3D scans. Through such training, our model achieves a significant increase in generalizability and excels across a broader spectrum of scenarios and applications. In addition, to improve the applicability of Human-LRM for in-the-wild scenarios, we distill multi-view Human-LRM into single-view through a conditional diffusion model. This extension enables Human-LRM to handle unseen body parts, both from different views and occlusions, thus enabling the generation of full-body humans from partial observations.","Extensive experiments across comprehensive evaluation sets demonstrate the superiority of Human-LRM over previous methods in several respects. Firstly, Human-LRM exhibits remarkable geometry predictions, outperforming existing methods in terms of several metrics, including Chamfer distance, Point-to-Surface (P2S) and Normal Consistency (NC). Secondly, ablation studies confirm the effectiveness of both normal and depth supervision and utilizing SDFs for surface prediction. Thirdly, comparisons to the prior approaches using NeRFs for human reconstruction underscore the advantages of Human-LRM in terms of handling occlusions, appearance rendering quality and surface fidelity. Lastly, the generative extension of Human-LRM empowered by conditional diffusion facilitates the reconstruction of full-body humans, addressing challenges posed by occlusions, and generates diverse credible poses conditioned on a single viewpoint.","In conclusion, Human-LRM is a novel approach for reconstructing human NeRFs from a single image. Its strengths lie in its remarkable scalability, making it highly adaptable to training with vast multi-view RGB datasets. This adaptability significantly bolsters its generalizability, propelling it ahead of prior methods on various test sets. Furthermore, its novel multi-view feature distillation approach adeptly handles inherent variations in human body capture, producing plausible and complete human geometries conditioned on a single view.",Single-View 3D Human Digitalization with Large Reconstruction Models,"Zhenzhen Weng, Jingyuan Liu, Hao Tan, Zhan Xu, Yang Zhou, Serena Yeung-Levy, Jimei Yang","Single-View 3D Human Digitalization with Large Reconstruction Models
Zhenzhen Weng1†, Jingyuan Liu2, Hao Tan2, Zhan Xu2, Yang Zhou2
Serena Yeung-Levy1, Jimei Yang2
1Stanford University, 2Adobe Research
1{zzweng,syyeung}@stanford.edu,
2{jingyliu,hatan,zhaxu,yazhou,jimyang}@adobe.com
Input image
Predicted 
geometry
Predicted
appearance 
(rendered)
Figure 1. We present Human-LRM, a template-free large reconstruction model for feed-forward 3D human digitalization from a single
image. Trained on a vast dataset comprising multi-view capture and 3D scans, our model generalizes across a broader range of scenarios.
Further, equipped with a generative component, our model can generate full body humans from occluded observations. Our project
webpage is at https://zzweng.github.io/humanlrm.
Abstract
In this paper, we introduce Human-LRM, a single-stage
feed-forward Large Reconstruction Model designed to pre-
dict human Neural Radiance Fields (NeRF) from a sin-
gle image. Our approach demonstrates remarkable adapt-
ability in training using extensive datasets containing 3D
scans and multi-view capture.
Furthermore, to enhance
the model’s applicability for in-the-wild scenarios espe-
cially with occlusions, we propose a novel strategy that dis-
tills multi-view reconstruction into single-view via a condi-
tional triplane diffusion model. This generative extension
addresses the inherent variations in human body shapes
when observed from a single view, and makes it possible
to reconstruct the full body human from an occluded image.
†Work done as an intern at Adobe Research.
Through extensive experiments, we show that Human-LRM
surpasses previous methods by a significant margin on sev-
eral benchmarks.
1. Introduction
Reconstructing 3D human models from a single image is an
important research topic in computer vision with an array of
practical applications. These applications encompass areas
such as AR/VR, asset creation, relighting , and many more.
A plethora of techniques have been developed to address
this challenging task, each with its own set of advantages
and limitations. Parametric reconstruction methods, a.k.a.
human mesh recovery (HMR) [10, 20, 46] regress pose
and shape parameters of SMPL (Skinned Multi-Person Lin-
ear) human body mesh model [23], which does not include
1
arXiv:2401.12175v1  [cs.CV]  22 Jan 2024
clothing details. This limits their utility in applications re-
quiring realistic and detailed human representations. Con-
versely, implicit volume reconstruction methods [31, 32]
capture fine-grained clothing details with their pixel-aligned
features but do not generalize across various poses. Recent
hybrid approaches [40, 41, 49] combine the advantages of
parametric and implicit reconstruction methods by using the
predicted SMPL body mesh as conditioning to guide the full
clothed reconstruction. However, these SMPL-conditioned
methods face an inevitable limitation: SMPL prediction er-
rors propagate to the subsequent full reconstruction stage,
resulting in misalignment between the reconstructed mesh
and the input image. These errors are often irreparable and
cannot be fully fixed by post-hoc optimization [40, 41, 49].
Lastly, these implicit reconstruction methods are hampered
by the scarcity of high quality 3D scans for training.
Meanwhile, there have been various works that use
NeRF [25] as a representation to learn geometry as well as
texture of humans, but these work typically performs fine-
tuning only on single images [1, 16], which is time consum-
ing and not generalizable to new observations. Recently,
feed-forward NeRF prediction models such as Large Re-
construction Model (LRM) [14] has been proposed, which
is highly generalizable and produces high-quality 3D re-
constructions as well as NeRF from arbitrary image in-
puts. However, directly applying pre-trained generic LRM
to humans yields sub-optimal results even with fine-tuning
(Figure 2). Primarily, the reconstructed surfaces tend to be
coarse, not preserving enough details.
In this work, we present Human-LRM, a single-stage
feed-forward model that predicts the geometry and appear-
ance of the human from a single image. Leveraging neural
radiance fields as 3D representation, we are able to scale
up our training to encompass multi-view human datasets.
Consequently, we are able to achieve improved generaliza-
tion compared to previous methods [31, 32, 40, 41] that
rely on limit 3D scan supervision. Unlike SHERF [15],
an existing generalizable human NeRF prediction model
that uses the predicted SMPL mesh to transform image fea-
tures to the canonical space, Human-LRM is completely
template-free, allowing for effective generalization in com-
plex situations where SMPL-conditioned methods are in-
adequate. In contrast to LRM, Human-LRM predicts SDF
values and renders with VolSDF [42] instead of classical
NeRF [25], which leads to enhanced surface fidelity for fi-
nal reconstruction. We additionally supervise the human
geometry through normal and depth maps. These improve-
ments prove to be effective in enabling higher quality sur-
face reconstruction.
Lastly, to address the common oc-
clusion scenarios in the wild, we propose a novel training
paradigm that distills multi-view reconstruction into single-
view through conditional triplane diffusion [13, 34]. This
equips Human-LRM with generative capabilities to output
Reconstructed mesh from front and side views (colored by normal)
Input image
Figure 2. Limitations of LRM [14]: depth ambiguities of off-the-
shelf generic LRM (left), coarse geometry even finetuning LRM
on humans (right).
full body humans from partial observations (last column of
Figure 1). Our contributions are summarized below:
• We introduce Human-LRM, a specialized LRM for hu-
mans with improved surface fidelity. Being trained on an
extensive dataset (more than 10K shapes) with both multi-
view RGB data and 3D scans, our model attains substan-
tially enhanced generalizability and excels across a wider
spectrum of scenarios and applications.
• To enhance the applicability of Human-LRM for in-the-
wild scenarios, we distills multi-view Human-LRM into
single-view through a conditional diffusion model. The
generative Human-LRM enables better handling of un-
seen body parts from both other views and occlusions.
• Through extensive experiments, we show that Human-
LRM outperforms previous methods significantly on a
comprehensive evaluation set.
2. Related Work
Parametric reconstruction. Many 3D human reconstruc-
tion works [10, 20, 22, 46] are built on mesh-based para-
metric body models, e.g., SMPL [23].
Given an input
image, these methods, referred as Human Mesh Recov-
ery (HMR), employ neural networks to predict the SMPL
shape and pose parameters from which the target human
body mesh is constructed.
This SMPL-conditioned ap-
proach greatly reduces the network output complexity and
also can be adapted for weakly-supervised training with 2D
pose estimates via differentiable mesh rasterization [20, 38].
As SMPL models minimally-clothed human bodies with a
smooth mesh of fixed topology, it prevents these methods
from reconstructing detailed geometry and texture. Nev-
ertheless, the predicted SMPL mesh is a very good proxy
for the fully clothed reconstruction as it captures the base
body shape and depicts its pose structure.
The promise
of HMR motivates follow-up works to predict 3D off-
sets [2, 24, 27, 50] or build another layer of geometry on
top of the base body mesh to accommodate clothed human
shapes [5, 19]. However, this “body+offset” strategy lacks
the flexibility to represent a wide-range of clothing types.
Implicit reconstruction.
Implicit-functions offer a
topology-agnostic representation for modeling human
2
shapes.
PiFU [31] uses pixel-aligned image features to
predict 3D occupancy values and colors from sampled 3D
points in a predefined grid.
Building on this, PIFuHD
[32] develops a high-resolution module to predict geometric
and texture details with additional front-back normal maps
as input.
While producing expressive reconstruction re-
sults for simple inputs like standing humans against clean
background, such models are not able to generalize well
to in-the-wild scenarios and often yield broken and messy
shapes on challenging poses and lightings due to their lim-
ited model capacity and lack of a holistic representation.
Hybrid reconstruction.
An emerging type of approach
leverages parametric body models (e.g. SMPL [23]) to im-
prove the generalizability of fully-supervised implicit re-
construction methods.
Starting from a given image and
an estimated SMPL mesh, Xiu et al. [40] regresses shapes
from locally-queried features to generalize to unseen poses.
Wang et al. [36] extends ICON with a GAN-based genera-
tive component. Xiu et al. [41] leverages variational normal
integration and shape completion to preserve the details of
loose clothing. Although the incorporation of SMPL does
enhance generalizability to large poses, these methods are
also constrained by the accuracy of SMPL predictions. Any
errors in the estimated SMPL parameters have a cascading
effect on the subsequent mesh reconstruction stage.
Human NeRFs.
Neural Radiance Fields (NeRF) [25]
marks a pivotal milestone in 3D reconstruction. NeRF em-
powers the learning of a 3D representation of an object
solely from 2D observations. While there exist several no-
table works that focus on reconstructing human NeRF, these
efforts often center around the single video [37] or image
[16, 39] fine-tuning setting at the cost of substantial com-
putational time, ranging from tens of minutes to hours. In
contrast, our focus lies on a feed-forward paradigm that rad-
ically reduces the time required for a model to predict a hu-
man NeRF from a single image, typically in mere seconds.
A few recent works [11, 21] also employ a feed-forward
paradigm for generalizability, utilizing SMPL as a geomet-
ric prior and aggregating features from sparse observations,
yet they necessitate multiple views. A closer related work
[15] considers feed-forward human NeRF prediction from a
single image. Nonetheless, their method replies on ground
truth SMPL body meshes that limit their model representa-
tion power. Our method is completely template-free, open-
ing up a broader spectrum of real-world applications, mak-
ing NeRF-based human reconstruction more accessible and
practical for various scenarios.
3. Method
An overview of Human-LRM is presented in Figure 3. Our
method is built on top of LRM [14] that consists of two ma-
jor building blocks: transformer-based triplane decoder and
triplane NeRF. In Section 3.1, we briefly introduce triplane
prediction as our model backbone and then in Section 3.2,
we introduce our improved triplane NeRF to enhance the
surface reconstruction quality of humans. For more details
about the architecture, we refer readers to [14]. Lastly, we
introduce an generative extension of our model based on on
conditional diffusion, designed to complete novel views and
address occlusion (Section 3.3).
3.1. Single-view Triplane Decoder
Given an RGB image as input, LRM first applies a pre-
trained vision transformer (ViT), DINO [7] to encode the
image to patch-wise feature tokens {hi}n
i=1 ∈ R768, where
i denotes the i-th image patch, n is the total number of
patches, and 768 is the latent dimension.
It then uses a transformer module to decode the image
tokens into a 3D triplane [8]. Specifically, the decoder up-
dates learnable tokens to the final triplane features via cam-
era modulation and cross-attention with the image tokens,
similar to the design of PerceiverIO [17]. More specifically,
each transformer layer contains a cross-attention, a self-
attention, and a multi-layer perceptron (MLP) sub-layer,
where the input tokens to each sub-layer are modulated [26]
by the camera features c. The cross-attention layer attends
from the triplane features to the image tokens, which can
help link image information to the triplane. Then, the self-
attention layer further models the intra-modal relationships
across the spatially-structured triplane entries.
Triplane [8] is used as an efficient 3D representation. A
triplane T contains three axis-aligned feature planes TXY,
TYZ and TXZ. In our implementation, each plane is of di-
mension hT × wT × dT where hT × wT is the spatial res-
olution, and dT is the number of feature channels. For any
3D point in the NeRF object bounding box [−1, 1]3 , we can
project it onto each of the planes and query the correspond-
ing point features Txy, Tyz, Txz via bilinear interpolation,
which is then decoded for rendering (Section 3.2).
In short, given an input image I1 ∈ RH×W ×3, we train
an encoder E and decoder D s.t. {hi}n
i=1 = E(I1), and
TXY, TYZ, TXZ = D({hi}n
i=1, c)
3.2. Triplane NeRF
Traditional neural volume rendering methods (as used in
LRM [14]) model geometry through a generalized density
function. The extraction of this geometry is achieved us-
ing a random level set of the density function, which often
results in reconstructions that are noisy and of low fidelity.
Hence, to improve the fidelity of the reconstructions, we
predict Signed Distance Functions (SDF) instead of den-
sity. Specifically, we use two MLPs (i.e. “SDF MLP” and
“RGB MLP” in Figure 3) to predict SDF and RGB from the
point features queried from the triplane representation T.
The SDF MLP takes the point features and output SDF and
a latent vector hp. The RGB MLP takes the point features,
3
Single-view 
Human-
LRM
Multi-view
Human-
LRM
U-Net
Camera Params
Camera Params
Frozen
Multi-view input
Noised latent
Single-view (Conditioning) latent
Concat
reshape
Masked single-view input
timestep
reshape
Multi-view latent
Rendered depth
Triplane features
Image 
Encoder 
Triplane 
Decoder 
SDF MLP
RGB MLP
Query features
Rendered normal
Rendered color
Novel view
Figure 3. Left: Overview of single-view Human-LRM. Given a single image, we encode the image using ViT [7], and employ a transformer
to decode a triplane representation [8], followed by SDF and RGB MLPs for volumetric rendering of RGB, normal and depths from novel
viewpoints. Right: Overview of our generative Human-LRM. We first train a multi-view and a single-view Human-LRM with a shared
NeRF decoder and then train a diffusion model that uses the single-view triplanes as conditioning to denoise the learned triplane from
multi-view. During diffusion model training, the single view encoder takes an additional binary mask to simulate real-world occlusions.
latent vector and normals at sampled points ˆnp (computed
from predicted SDF using finite differences) and output
RGB values. That is, hp, SDF = MLPSDF(Txy, Tyz, Txz),
RGB = MLPRGB(Txy, Tyz, Txz, hp, ˆnp). For a ray r em-
anating from a camera position o in direction v ∈ R3,
||v|| = 1, defined by r(t) = o + tv, t ≥ 0, the color of
the corresponding pixel in the rendered image is computed
via numerical integration
I(r) =
M
X
i=1
αiΠi>j(1 − αj)RGBi, αi = 1 − eσiδi
(1)
where σi is the density converted from SDF using [42], and
δi is the distance between samples. Normals can be ren-
dered using the same formula where we integrate over pre-
dicted normals at sampled points instead.
Training objective.
Our training data contains multiple
views and their respective camera parameter per human.
For each human, we randomly choose a few side views, and
render a random ˆx ∈ Rh×w×3 patch on each view, simi-
lar to [37]. The ground truth RGB values for the patch is
x ∈ Rh×w×3. In addition, we render the predicted depths
and normals of the patch ˆn ∈ Rh×w and ˆd ∈ Rh×w×3, and
supervise with depths maps d ∈ Rh×w and normal maps
n ∈ Rh×w×3. The supervising depth and normal maps
can be either ground-truth renderings or off-the-shelf pre-
dictions. The training objective of our single-view recon-
struction method is computed over losses from V rendered
views, with the input view as well as (V − 1) side views.
Overall, the training objective is to minimize L,
L = 1
V
V
X
v=1
(LMSE(ˆxv, xv)
(2)
+ λlpipsLLPIPS(ˆxv, xv)
(3)
+ λnLMSE(ˆnv, nv) + λdLDSI(ˆdv, dv))
(4)
+ λeikLEikonal
(5)
Subscript v means that the corresponding variable is for the
vth supervising view. LMSE is the normalized pixel-wise L2
loss, LLPIPS is the perceptual image patch similarity [47],
and LDSI is the scale invariant depth loss [4]. LEikonal is the
Eikonal regularization [12] computed using SDF values of
the sampled points along the rays. λlpips, λn, λd, and λeik
are weight coefficients.
3.3. Conditional Diffusion Model
The above mentioned single-view deterministic model has
two limitations: 1) collapsed reconstruction on the unseen
parts and 2) incapability of handling occlusions. In this sec-
tion, we propose a generative extension of Human-LRM
with conditional diffusion. An overview of this model is
illustrated in the right side of Figure 3. Specifically, we first
train a multi-view reconstruction model. In contrast to the
single-view model, the multi-view model incorporates cam-
era conditioning within the ViT encoder. The triplane de-
coder in the multi-view model maintains the same architec-
ture as the single-view model, with the exception that it does
not take camera conditioning. With a sufficient number of
views, we can conceptualize the learned triplane Tmv as a
near-perfect representation of the human. We have chosen
4
to train a 4-view model. This decision is based on the obser-
vation that utilizing four views tends to provide a definitive
and comprehensive depiction of the human subject while
not excessively increasing the model’s capacity. We then
freeze the weights of multi-view encoder and train a single-
view encoder with an additional l2 loss between the single-
view and multi-view triplane features. This l2 loss has an-
nealing weight that starts from 0.1 to 10. For both single-
view and mult-view models, we clamp the triplane features
to [−1, 1] using a tanh layer. To simulate real-world occlu-
sions as well as guiding the diffusion model on which part
to hallucinate, we apply a random mask to the single-view
input image, and pass the binary mask to the single-view
encoder through an additional mask channel.
Tmv = Dmv(Emv({Ii}m
i=1), {ci}m
i=1))
(6)
Tsv = Dsv(Esv(I1 ⊙ M1), c1)
(7)
where M1 ∈ [0, 1]H,W is the binary mask for the single-
view input image, and m is the number of input views to
the multi-view model.
To train a conditional model, we first flatten the predicted
triplanes from the single-view model and multi-view model,
yielding ˜Tsv and ˜Tmv, each of size (hT × 3, wT , dT ). We
then add t steps of Gaussian noise to the multi-view triplane
( ˜Tmv)t and train a conditional diffusion model to restore
˜Tmv. Single-view triplane is used as conditioning, and is
concatenated with the noised multi-view triplane to form
the input to the diffusion model [13]. The objective of dif-
fusion training is
Ld = Et∼[1,T ][|| ˜Tmv − Uθ(( ˜Tmv)t), ˜Tsv, t)||2]
(8)
where t is the randomly sampled timestep, and T = 1000 is
the maximum number of steps. Uθ is a UNet [30] (with
weights θ) that predicts the denoised multi-view triplane
conditioning on single-view triplane and timestep. Note that
since we changed the input channels of single-view encoder,
we finetune single-view encoder during diffusion training.
4. Experiments
Training data. Our complete training set consists of 1,426
high-quality scans (500 from THuman 2.0 [44] and 926
from Alloy++), as well as around 8,000 posed multi-view
captures from HuMMan [6] v1.0. THuman 2.0 and HuM-
Man both contain adults with simple clothing. Thus, to fur-
ther evaluate the generalization capability, we collect Al-
loy++ from Human Alloy [3] and our internal capture. Each
scan from Human Alloy has around 40K polygons and our
internal capture, 100K polygons. The quality of those scans
are similar to that of RenderPeople [29] (100K polygons).
Alloy++ contains humans with more challenging clothing,
poses, as well as little kids.
Evaluation sets. We evaluate on 20 humans from THu-
man 2.0 and 20 humans from Alloy++, each with renderings
from 18 evenly spaced viewpoints. In addition, we create an
evaluation set from X-Human [33]. We randomly sample 2
frames per sequence, which results in 460 frames from 20
human subjects, all with distinct poses. The X-Human test-
set serves as an out-of-domain evaluation set as none of the
models have seen images from this dataset during training.
Data preprocessing. For each scan from THuman 2.0 and
Alloy++, we center it by the origin and scale them so the
longest side has length 1.8. We render each human scan
from 32 randomly sampled viewpoints with the same cam-
era pointing toward the origin. For HuMMan v1.0, there are
10 cameras per pose. In total, there are 16K, 14K, and 80K
distinct input images from the training split of THuman 2.0,
Alloy++ and HuMMan v1.0, respectively.
Inference time. It takes about 0.7 second for the image en-
coder and triplane decoder to get the triplane representation
from the input image(s), and 1.3 seconds to render a 256
by 256 image from the triplanes. For our single-view con-
ditional diffusion model, DDIM sampling with 200 steps
takes about 10 seconds on a single A100 GPU.
4.1. Geometry Comparisons
We compare to existing single-view human reconstruction
methods PiFU [31], PiFUHD [32], Pamir [49], ICON [40]
and ECON [41] 1 . Since Pamir, ICON and ECON require
SMPL parameters as input to their model, we use off-the-
shelf SMPL predictors [10, 46] to produce them. All of
these baselines require ground truth geometry as supervi-
sion and therefore their generalizability is limited by the
availability of such datasets, whereas our method works
with just multi-view capture, which is more accessible.
Following previous works, we report Chamfer distance,
Point-to-Surface (P2S) and Normal Consistency (NC).
First, we compare with their public pretrained models. As
some of the baseline methods are trained on the commer-
cially available RenderPeople, we opt for THuman 2.0, a
publicly available dataset with a similar scale, to ensure
a fair comparison. We train all approaches on the same
dataset, to eliminate the influence of training data. While
baselines use GT geometry to supervise the occupancy di-
rectly, we obtain normal and depth maps from GT geometry
and use them to guide the surface prediction.
We report the quantitative results in Table 1. “Ours - SV
Det.” is our single-view deterministic model as described in
Section 3.1. As shown by Table 1, the geometry predicted
by our method consistently outperforms previous works, in-
cluding works that are prior-free (PiFU and PiFUHD) as
well as works that require SMPL prior (Pamir, ICON and
1Wang et al. [36] is another related work but we couldn’t compare with
it as there is no code release and their authors also informed us that their
model checkpoints got lost.
5
Requires
THuman 2.0
Alloy++
X-Human
Model
Training Data
SMPL
Chamfer ↓
P2S ↓
NC ↓
Chamfer ↓
P2S ↓
NC ↓
Chamfer ↓
P2S ↓
NC ↓
PiFU [31]
RenderPeople-442
×
6.13
6.18
0.239
4.97
5.29
0.207
5.45
5.65
0.200
PiFU-HD [32]
RenderPeople-450
×
6.13
6.08
0.248
5.51
5.45
0.229
5.32
5.13
0.205
Pamir [49]
Twindom [35]-900 + DeepHuman [48]-600
✓
6.85
6.87
0.254
5.83
6.00
0.222
5.61
5.48
0.202
ICON [40]
RenderPeople-450
✓
6.59
6.27
0.245
4.47
4.43
0.196
5.48
5.38
0.197
LRM [14]
Objaverse [9] + MVImgNet [45] -730K in total
×
5.33
4.13
0.215
3.68
3.55
0.153
5.24
4.15
0.209
Ours - SV Det.
THuman 2.0-500, Alloy++ -926, HuMMan v1.0-8000
×
2.23
2.03
0.114
2.35
2.12
0.116
2.29
2.15
0.099
PiFU [31]
THuman 2.0
×
6.15
6.40
0.247
4.97
5.30
0.207
5.43
5.88
0.206
Pamir [49]
THuman 2.0
✓
6.86
6.56
0.251
5.81
5.78
0.217
5.62
5.34
0.199
ICON [40]
THuman 2.0
✓
6.57
6.65
0.251
5.58
5.86
0.218
5.33
5.43
0.197
ECON [41]
THuman 2.0
✓
7.14
6.92
0.247
5.04
4.64
0.197
5.87
5.79
0.200
Ours - SV Det.
THuman 2.0
×
2.62
2.60
0.124
3.22
2.99
0.145
2.43
2.25
0.106
Table 1. Comparison of existing single-view reconstruction methods. Top: Comparison of off-the-shelf models. The size of each training
set is italicized. Bottom: Fair comparison of all models on THuman 2.0.
Normal Consistency ↓
Model
Views
hT , wT
Chamfer
P2S
Front
Side
Back
Average
SV Det.
1
256
2.62
2.60
0.093
0.128
0.119
0.124
SV Det.
1
128
3.74
3.48
0.106
0.207
0.142
0.166
MV Det.
4
128
1.95
1.84
0.086
0.102
0.110
0.100
SV Gen.
1
128
2.28
2.13
0.093
0.122
0.118
0.114
(a) Comparison between our single-view (“SV”) deterministic (“Det.”),
multi-view (“MV”) and single-view conditioned generative (“Gen.”) mod-
els on THuman 2.0.
Normal Consistency ↓
Model
Chamfer
P2S
Front
Side
Back
Average
PiFU
9.86
10.54
0.401
0.352
0.250
0.339
PiFU-HD
7.32
7.65
0.31
0.291
0.201
0.274
Pamir
10.87
11.22
0.387
0.361
0.231
0.336
ICON
10.12
10.75
0.381
0.351
0.231
0.328
ECON
11.26
11.74
0.409
0.355
0.243
0.341
Ours - SV Det.
5.85
4.39
0.138
0.215
0.181
0.187
Ours - SV Gen.
2.36
2.13
0.101
0.131
0.130
0.123
(b) Full body reconstruction results on masked single-view images.
Table 2. Results of our generative models.
ECON). The performance of SMPL-guided works is gener-
ally affected by the errors from the predicted SMPL param-
eters. Even though ICON and ECON utilize an optimiza-
tion algorithm to optimize the SMPL parameters to match
the predicted image normals, the errors in the SMPL pa-
rameters on some images are still significant. Our method
does not rely on a human mesh template such as SMPL and
thus does not suffer from this problem. As shown in Figure
4, our method demonstrates exceptional generalizability to
challenging cases such as people in rare poses (1st and 2nd
columns) as well as little kids (last column).
4.2. Ablations
Effect of GT normal and depth. Although for our best
model, we do use ground truth normal and depth maps
from geometry, we can replace them with estimated ones to
achieve comparable performance. We additionally exper-
iment with supervising with predicted normals and depths
from off-the-shelf predictors. While there is no notable drop
in quantitative performance (Table 3), the surface details are
better with GT normal and depth supervision (Figure 7).
Effect of predicting SDF. LRM [14] uses a single MLP to
predict the density (σ) and RGB, followed by NeRF volu-
metric rendering (i.e. experiment “predict σ” in Table 3).
We noticed that the geometry predicted by this approach
tended to be more rudimentary in detail (Figure 7).
Effect of the scale of training data.
To further show-
case the increased generalization ability of our method with
more training data, we train with additional training data
from Alloy++ (926 scans). As shown in Table 3, our model
shows the best performance, and this performance contin-
ues to enhance as we incorporate additional training data.
This underscores the significance of expanding the scale of
model training to improve generalizability.
4.3. Appearance Comparisons
Our work is closely related to generalizable NeRF SHERF
[15], as we are both considering learning a feed-forward hu-
man NeRF model from a single-image without any single-
image/video finetuning [37] or test-time optimization. In
order to compare to SHERF [15], we train our model on
HuMMan [6] and evaluate the quality of novel view ren-
derings by SSIM, PSNR and LPIPS following the same
evaluation protocol as SHERF (Table 4). The ground truth
(GT) SMPL parameters utilized in SHERF are obtained
through triangulation from multi-view captures. However,
this process is impractical in real-world scenarios where
only single-view capture is present.
When we use es-
timated SMPL parameters from a state-of-the-art model
CLIFF [22], the performance of SHERF experiences a sub-
stantial decline. This decline can be attributed to the fact
that SHERF’s pixel-aligned feature extraction relies heavily
on the assumption that the SMPL vertices align accurately
with their corresponding pixel locations. In contrast, our
model does not rely on a pose prior, which makes it more
resilient and adaptable for real-world scenarios. This ro-
bustness is not only demonstrated through improved quan-
titative results when compared to SHERF (with estimated
SMPL parameters) but, more notably, through the qualita-
tive results illustrated in Figure 6. Lastly, the surface quality
from Human-LRM is significantly better than SHERF. We
include visual comparisons in Supplemental Materials.
6
THuman 2.0
Alloy++
X-Human
Model
Training Data
Chamfer ↓
P2S ↓
NC ↓
Chamfer ↓
P2S ↓
NC ↓
Chamfer ↓
P2S ↓
NC ↓
Est. d.n.
THuman 2.0, HuMMan v1.0
2.63
2.38
0.134
3.35
3.08
0.147
2.45
2.28
0.103
No d.n.
THuman 2.0, HuMMan v1.0
2.63
2.40
0.132
3.68
3.20
0.163
2.75
2.43
0.117
Predict σ
THuman 2.0, HuMMan v1.0
2.49
2.32
0.124
3.48
3.44
0.156
2.67
2.59
0.116
Full Model
THuman 2.0, HuMMan v1.0
2.41
2.21
0.115
3.16
2.92
0.145
2.37
2.21
0.103
Small training set
THuman 2.0
2.62
2.60
0.124
3.22
2.99
0.145
2.43
2.25
0.106
Medium training set
THuman 2.0, HuMMan v1.0
2.41
2.21
0.115
3.16
2.92
0.145
2.37
2.21
0.103
Large training set
THuman 2.0, Alloy++, HuMMan v1.0
2.23
2.03
0.114
2.35
2.12
0.116
2.29
2.15
0.099
Table 3. Ablations of our single-view deterministic model. Top: Effect of using depth and normal maps (“d.n.”) for supervision and
predicting SDFs. Bottom: Effect of the scale of training data.
PiFU-HD
PiFU
ECON
ICON
Ours
Pamir
LRM
(Finetuned)
Figure 4. Comparison to previous volumetric reconstruction methods: PiFU [31], PiFUHD [32], Pamir [49], ICON [40], ECON [41], and
LRM [14]. All models are trained on THuman 2.0. For each example we show the geometry (colored by mesh normals) from 4 views.
4.4. Generative Human-LRM Evaluation
We train a diffusion model using the single view triplane
features as conditioning as described in Section 3.3. Our
conditional diffusion model, enhanced with distilled infor-
mation from multi-view model, is able to reconstruct cred-
ible poses from just a single viewpoint.
This improves
performance, particularly in scenarios where parts of the
human are obscured (e.g. 1st row of Figure 5). We re-
port quantitative performance on THuman 2.0 in Table 2a.
Due to memory issues, we train the diffusion model with a
smaller triplane size (128 by 128), causing the performance
of the single-view model to degrade a little bit. However, we
show that with the conditional diffusion, the performance of
the model becomes even better than the single-view deter-
ministic model with 256 by 256 triplanes.
In addition, our diffusion model is able to reconstruct
7
Ours – SV
(generative)
Ours – SV 
(deterministic)
PiFU
ICON
ECON
Ours - 4V
Input Image
PiFU-HD
Figure 5. Qualitative examples of on images with occlusions. For each example we show the input view as well as side view.
Input image
Ours
SHERF (w/ estimated SMPL)
Figure 6. Novel view renderings results on HuMMan v1.0.
Supervise with estimated 
normal, depth
Supervise with GT 
normal, depth
Predict density 
instead of SDF
Without normal, 
depth supervision
Input image
Figure 7. Ablations. We show the effect of using estimated vs.
ground truth normal and depth as supervision as well as using a
simple MLP as in LRM [14] to predict the density instead of SDF.
complete humans from single-view images even when the
humans are occluded. In Table 2b, we randomly apply a
mask to each input image to simulate real-world occlusion
scenarios. Notice that all baselines fail to reconstruct the
missing body part, even for SMPL-guided works like ICON
and ECON that are conditioned on full-body SMPL prior.
Our generative model, on the other hand, is able to halluci-
Input image
Seed 1
Seed 2
Figure 8. Given an incomplete image, our generative model is able
to generate different credible poses.
Method
GT SMPL
PSNR ↑
SSIM ↑
LPIPS ↓
NHP [21]
✓
18.99
0.84
0.18
MPS-NERF [11]
✓
17.44
0.82
0.19
SHERF [15]
✓
20.83
0.89
0.12
SHERF [15]
×
14.46
0.79
0.20
Ours
×
17.13
0.87
0.12
Table 4. Comparison to generalizable human NeRF methods on
HuMMan v1.0 [6]. Top section: Feed-forward methods that use
GT SMPL parameters during inference. Bottom section: methods
that do not use GT SMPL during inference.
nate the occluded part (e.g. 3rd row of Figure 5). In addi-
tion, with different random seeds, our generative model is
able to reconstruct different credible poses 8.
5. Conclusion and Future Work
In this work, we introduced an approach for reconstructing
human NeRFs from a single image. What sets our approach
apart from previous implicit volumetric human reconstruc-
tion methods is its remarkable scalability, making it highly
adaptable for training on large and diverse multi-view RGB
datasets. This adaptability, in turn, significantly bolsters its
generalizability, enabling it to surpass established baseline
8
models on various testsets. Additionally, our novel multi-
view feature distillation approach handles inherent varia-
tions in human body capture, producing plausible and com-
plete human geometries conditioning on a single view.
Although Human-LRM excels in capturing global geom-
etry, it still falls short in preserving finer facial details. Fu-
ture directions include utilizing more powerful representa-
tion than triplanes or potential refinement techniques.
References
[1] Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil
Kim, Johannes Kopf, and Jia-Bin Huang. Single-image 3d
human digitization with shape-guided diffusion.
In SIG-
GRAPH Asia, 2023. 2
[2] Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar,
Christian Theobalt, and Gerard Pons-Moll. Learning to re-
construct people in clothing from a single rgb camera. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 1175–1186, 2019. 2
[3] Human Alloy. Human alloy, 2023. 5, 1
[4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter
Wonka, and Matthias M¨uller.
Zoedepth: Zero-shot trans-
fer by combining relative and metric depth. arXiv preprint
arXiv:2302.12288, 2023. 4, 1, 2
[5] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt,
and Gerard Pons-Moll. Multi-garment net: Learning to dress
3d people from images. In Proceedings of the IEEE/CVF
international conference on computer vision, pages 5420–
5430, 2019. 2
[6] Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao
Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang
Pan, et al. Humman: Multi-modal 4d human dataset for ver-
satile sensing and modeling.
In European Conference on
Computer Vision, pages 557–577. Springer, 2022. 5, 6, 8, 1
[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision, pages 9650–9660, 2021. 3, 4
[8] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient
geometry-aware 3d generative adversarial networks. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 16123–16133, 2022. 3, 4
[9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana
Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:
A universe of annotated 3d objects.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 13142–13153, 2023. 6
[10] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios
Tzionas, and Michael J Black. Collaborative regression of
expressive bodies using moderation. In 2021 International
Conference on 3D Vision (3DV), pages 792–804. IEEE,
2021. 1, 2, 5
[11] Xiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng,
Zicheng Liu, and Xin Tong. Mps-nerf: Generalizable 3d hu-
man rendering from multiview images. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2022. 3, 8
[12] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and
Yaron Lipman. Implicit geometric regularization for learning
shapes. arXiv preprint arXiv:2002.10099, 2020. 4
[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems, 33:6840–6851, 2020. 2, 5
[14] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,
Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao
Tan. Lrm: Large reconstruction model for single image to
3d. arXiv preprint arXiv:2311.04400, 2023. 2, 3, 6, 7, 8
[15] Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei
Yang, and Ziwei Liu. Sherf: Generalizable human nerf from
a single image. arXiv preprint arXiv:2303.12791, 2023. 2,
3, 6, 8, 1
[16] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Ji-
axiang Tang, Deng Cai, and Justus Thies. Tech: Text-guided
reconstruction of lifelike clothed humans.
arXiv preprint
arXiv:2308.08545, 2023. 2, 3
[17] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac,
Carl Doersch, Catalin Ionescu, David Ding, Skanda Kop-
pula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al.
Perceiver io: A general architecture for structured inputs &
outputs. arXiv preprint arXiv:2107.14795, 2021. 3
[18] Yasamin Jafarian and Hyun Soo Park.
Learning high fi-
delity depths of dressed humans by watching social media
dance videos. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 12753–
12762, 2021. 1, 2, 3
[19] Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang
Liu, and Hujun Bao. Bcnet: Learning body and cloth shape
from a single image. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XX 16, pages 18–35. Springer, 2020. 2
[20] Angjoo Kanazawa, Michael J Black, David W Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and
pose. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 7122–7131, 2018. 1, 2
[21] Youngjoong Kwon, Dahun Kim, Duygu Ceylan, and Henry
Fuchs. Neural human performer: Learning generalizable ra-
diance fields for human performance rendering. Advances
in Neural Information Processing Systems, 34:24741–24752,
2021. 3, 8
[22] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu,
and Youliang Yan.
Cliff: Carrying location information
in full frames into human pose and shape estimation.
In
European Conference on Computer Vision, pages 590–606.
Springer, 2022. 2, 6
[23] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. Smpl: A skinned multi-
person linear model. In Seminal Graphics Papers: Pushing
the Boundaries, Volume 2, pages 851–866. 2023. 1, 2, 3
[24] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades,
Gerard Pons-Moll, Siyu Tang, and Michael J Black. Learn-
ing to dress 3d people in generative clothing. In Proceedings
9
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 6469–6478, 2020. 2
[25] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM, 65(1):99–106, 2021. 2,
3
[26] William Peebles and Saining Xie. Scalable diffusion models
with transformers. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 4195–4205,
2023. 3
[27] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J
Black. Clothcap: Seamless 4d clothing capture and retar-
geting. ACM Transactions on Graphics (ToG), 36(4):1–15,
2017. 2
[28] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun.
Vision
transformers
for
dense
prediction.
CoRR,
abs/2103.13413, 2021. 1, 2
[29] RenderPeople. Renderpeople, 2018. 5
[30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 5
[31] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-
ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned
implicit function for high-resolution clothed human digitiza-
tion. In Proceedings of the IEEE/CVF international confer-
ence on computer vision, pages 2304–2314, 2019. 2, 3, 5, 6,
7
[32] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul
Joo. Pifuhd: Multi-level pixel-aligned implicit function for
high-resolution 3d human digitization.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 84–93, 2020. 2, 3, 5, 6, 7
[33] Kaiyue Shen, Chen Guo, Manuel Kaufmann, Juan Zarate,
Julien Valentin, Jie Song, and Otmar Hilliges. X-avatar: Ex-
pressive human avatars. Computer Vision and Pattern Recog-
nition (CVPR), 2023. 5, 1
[34] Jiaming
Song,
Chenlin
Meng,
and
Stefano
Ermon.
Denoising diffusion implicit models.
arXiv preprint
arXiv:2010.02502, 2020. 2, 1
[35] Twindom. Twindom. 6
[36] Junying Wang, Jae Shin Yoon, Tuanfeng Y Wang, Kr-
ishna Kumar Singh, and Ulrich Neumann. Complete 3d hu-
man reconstruction from a single incomplete image. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 8748–8758, 2023. 3, 5
[37] Chung-Yi Weng,
Brian Curless,
Pratul P Srinivasan,
Jonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-
mannerf: Free-viewpoint rendering of moving people from
monocular video.
In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern Recognition, pages
16210–16220, 2022. 3, 4, 6
[38] Zhenzhen Weng, Kuan-Chieh Wang, Angjoo Kanazawa, and
Serena Yeung. Domain adaptive 3d pose augmentation for
in-the-wild human mesh recovery.
In 2022 International
Conference on 3D Vision (3DV), pages 261–270. IEEE,
2022. 2
[39] Zhenzhen Weng, Zeyu Wang, and Serena Yeung. Zeroavatar:
Zero-shot 3d avatar generation from a single image. arXiv
preprint arXiv:2305.16411, 2023. 3
[40] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J
Black. Icon: Implicit clothed humans obtained from nor-
mals.
In 2022 IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 13286–13296.
IEEE, 2022. 2, 3, 5, 6, 7
[41] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and
Michael J Black. Econ: Explicit clothed humans optimized
via normal integration.
In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 512–523, 2023. 2, 3, 5, 6, 7
[42] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-
ume rendering of neural implicit surfaces. Advances in Neu-
ral Information Processing Systems, 34:4805–4815, 2021. 2,
4
[43] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,
Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron,
and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-
time view synthesis. arXiv preprint arXiv:2302.14859, 2023.
1
[44] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qiong-
hai Dai, and Yebin Liu. Function4d: Real-time human vol-
umetric capture from very sparse consumer rgbd sensors. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR2021), 2021. 5, 1
[45] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu,
Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu,
Zhangyang Xiong, Tianyou Liang, et al.
Mvimgnet: A
large-scale dataset of multi-view images. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 9150–9161, 2023. 6
[46] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,
Yebin Liu, Limin Wang, and Zhenan Sun. Pymaf: 3d human
pose and shape regression with pyramidal mesh alignment
feedback loop. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pages 11446–11456,
2021. 1, 2, 5
[47] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 4
[48] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and
Yebin Liu. Deephuman: 3d human reconstruction from a
single image. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 7739–7749, 2019. 6
[49] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai.
Pamir: Parametric model-conditioned implicit representa-
tion for image-based human reconstruction. IEEE transac-
tions on pattern analysis and machine intelligence, 44(6):
3170–3184, 2021. 2, 5, 6, 7
[50] Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, and Ruigang
Yang. Detailed human shape estimation from a single im-
10
age by hierarchical mesh deformation.
In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 4491–4500, 2019. 2
11
Single-View 3D Human Digitalization with Large Reconstruction Models
Supplementary Material
A. Implementation Details
We train each of our deterministic models with 16 A100
GPUs for 7 days. We use batch size of 4 per GPU. During
training, we sample rays on a random 64 by 64 patch. We
use importance sampling where we sample 48 coarse and
64 fine samples along the rays. λlpips = 2, λd = λn = 1,
λeik = 0.5. We use cosine learning rate scheduler with
initial learning rate of 2e − 5. In SDF-density conversion,
we use scheduled hyper-parameters following [43] for sta-
ble optimization. The dimension of the input image is 512
by 512. For the last 3 layers of the triplane decoder, we
use patch embeddings from upscaled input image (1024
by 1024) to facilitate the incorporation of higher-resolution
features.
For diffusion training, we use learning rate 5e − 5. For
inference, we use DDIM [34] sampling with 200 steps. For
Section 4.1 to 4.3 in the main paper, we use hT = wT =
256 and dT = 16. For Section 4.4, we use hT = wT = 128
and dT = 64. The diffusion model in Section 4.4 is trained
on 8 A100 GPUs with learning rate 5e − 5 for about 7 days.
B. Comparison to SHERF’s Geometry
We include additional novel view renderings results from
SHERF [15] and Human-LRM on HuMMan v1.0 [6] in
Figure 9.
We include qualitative comparisons between
SHERF’s normals (computed from estimated depths) and
our normal predictions. As shown, the geometry quality
from Human-LRM is significantly better than SHERF.
Input Image
SHERF
Human-LRM
Novel View Rendering
Human-LRM
SHERF
Predicted Normals
Figure 9. Additional comparison to SHERF.
C. Comparison to SoTA Depth and Normal Es-
timation Methods
We compare the depth quality of our reconstructed geome-
try to state-of-the-art depth estimation works ZoeDepth [4]
and DPT [28]. Furthermore, we include a comparison of the
depth and normal from our predicted geometry with HD-
Net [18], a method tailored for human surface reconstruc-
tion. The results (Table 5) demonstrate that our approach
surpasses all baseline methods across various datasets. We
include qualitative results for depth estimation in Figure 10,
and normal estimation in Figure 11.
Personal and human subjects data.
In our experiments,
we use public datasets THuman 2.0 2 [44], X-Human 3 [33],
and commercially available dataset Alloy 4 [3].
These
datasets are widely used for human reconstruction research
and we direct to their respective website for information
about their data collection procedures.
Ethics statement.
Human-LRM is capable of transform-
ing a single image into a 3D human. While Human-LRM’s
results are not yet advanced enough to deceive human per-
ception, it’s important to remain vigilant about possible eth-
ical concerns. While we discourage such practices, there is
a potential risk that the 3D human models created could be
used to produce deceptive content.
2https://github.com/ytrock/THuman2.0-Dataset
3https : / / skype - line . github . io / projects / X -
Avatar/
4https://humanalloy.com
1
THuman 2.0
Alloy ++
X-Human
Method
Depth Error (↓)
Normal Error (↓)
Depth Error (↓)
Normal Error (↓)
Depth Error (↓)
Normal Error (↓)
DPT
2.44 ± 0.84
-
2.91 ± 1.24
-
3.17 ± 1.20
-
ZoeDepth
2.20 ± 0.96
-
2.41 ± 1.01
-
2.08 ± 0.88
-
HDNet
2.27 ± 0.80
0.58 ± 0.07
2.37 ± 1.04
0.47 ± 0.06
2.30 ± 0.83
0.48 ± 0.06
Human-LRM (Ours)
0.92 ± 0.41
0.39 ± 0.03
1.79 ± 0.78
0.41 ± 0.05
1.27 ± 0.51
0.35 ± 0.04
Table 5. Comparison with HDNet [18], ZoeDepth [4], and DPT [28].
GT Depth
Ours
HDNet
Input Image
ZoeDepth
DPT
Figure 10. Depth comparison to HDNet [18], ZoeDepth [4] and DPT [28]. Red color means the region is closer.
2
GT Normal
Ours
HDNet
Input Image
Figure 11. Normal comparison to HDNet [18].
3
"
"Existing strategies for multi-agent systems are often complex and can require infinite memory. They are not realistic for humans or limited computational power. Natural Strategies address this by allowing a bounded memory representation while capturing intuitive approaches humans may use. No work has considered Natural Strategies under probabilistic settings until now. In this paper, we propose variants of Probabilistic Alternating-time Temporal Logic (PATL) and PATL* with natural strategies (NatPATL and NatPATL*) and investigate their complexity. We show that NatPATL model checking for a restricted setting is NP-complete with deterministic natural strategies and is in EXPSPACE with probabilistic natural strategies. For the unrestricted case, the model checking of NatPATL is in EXPSPACE and the model checking of NatPATL* is in 3EXPSPACE.","Multi-Agent Systems (MAS) have been the focus of much research in verification due to their prevalence in modern systems. Alternating-time Temporal Logics (ATL and ATL*) have been extended to address various aspects of MAS, including strategy contexts, imperfect information, and epistemic operators. Probabilistic extensions of these logics have also been developed to handle uncertainty in MAS, allowing expressions such as the probability of satisfying a goal. However, these logics often have drawbacks in terms of complexity or memory requirements.

Natural strategies offer a middle ground between combinatorial strategies (functions from histories to actions) and memoryless strategies (dependent only on the current state). They allow agents to strategically use a limited history without requiring infinite memory. This corresponds better to how humans might describe strategies and are easier to explain using natural language. They also have the benefit of inherently featuring imperfect information, reasoning about propositional variables observed in previous states instead of just states themselves.

Our contribution in this paper is to propose variants of probabilistic logics PATL and PATL* with natural strategies (denoted NatPATL and NatPATL*, respectively) and investigate the complexity of their model checking. We report complexity results for both deterministic and probabilistic natural strategies, and with or without recall.","nanSeveral works have considered verification of stochastic MAS using specifications given in probabilistic logics. These include studies on ATL-like logics for stochastic MAS with deterministic strategies and probabilistic knowledge, the Probabilistic Alternating-Time Μ-Calculus, and PATL under imperfect information and memoryless strategies. PATL has also been considered with accumulated costs/rewards and extended to handle bounded memory as an approximation for perfect recall.

In the context of Partially Observable Markov Decision Processes (POMDPs), research has been done on representing strategies with limited memory, such as using input/output automata, decision trees, ATL with bounded memory, and bounded memory approximations for perfect recall. The Natural Strategies framework was first introduced for deterministic settings, including finding winning strategies in concurrent games with LTL specifications, deciding if a set of strategies define a Nash equilibrium, and model checking ATL. It was later extended to ATL with imperfect information and Strategy Logic, which we build on in our work.","We adopt a theoretical approach, using formal methods to study the complexity of model checking for NatPATL and NatPATL* with stochastic MAS.

We define behavioral natural strategies over Concurrent Game Structures (CGS), which are a formalism for representing stochastic MAS. These strategies are conditional plans, represented by a list of condition-action pairs, where the first condition that holds in the history of the game is selected, and the corresponding action is executed. The conditions are regular expressions over Boolean formulas over atomic propositions.

We provide the semantics for NatPATL and NatPATL* formulas, which are interpreted over stochastic CGS and a path. We also present motivating examples to illustrate how NatPATL* formulas can be used to express properties of MAS in probabilistic settings, such as access control and secure voting systems.","We present complexity results for model checking NatPATL and NatPATL* with deterministic and probabilistic natural strategies, considering both memoryless and bounded recall settings.

For deterministic natural strategies, model checking NatPATL is NP-complete and in EXPSPACE when considering memoryless strategies and strategies with bounded recall, respectively. NatPATL* with deterministic natural strategies has a 2NEXPTIME complexity for both memoryless and bounded recall.

With probabilistic natural strategies, model checking NatPATL and NatPATL* has an EXPSPACE and 3EXPSPACE complexity, respectively, for both memoryless strategies and strategies with bounded recall. These results demonstrate the trade-off between increasing the complexity of model checking and the expressiveness gained by allowing more sophisticated strategies.","In this paper, we investigated the complexity of model checking for variants of Probabilistic Alternating-time Temporal Logic (PATL) and PATL* with natural strategies (NatPATL and NatPATL*). We provided complexity results for both deterministic and probabilistic natural strategies, considering both memoryless and bounded recall settings.

Our results show that NatPATL model checking is NP-complete and in EXPSPACE for deterministic natural strategies with memoryless and bounded recall, respectively. NatPATL* with deterministic natural strategies has a higher complexity, being 2NEXPTIME-complete for both memoryless and bounded recall. With probabilistic natural strategies, NatPATL and NatPATL* have an EXPSPACE and 3EXPSPACE complexity, respectively, for both memoryless and bounded recall. These results highlight the challenge of finding efficient model checking algorithms for logics that allow more expressive strategies.

Our work opens up several directions for future research. It would be interesting to consider qualitative versions of NatPATL and NatPATL*, where thresholds are restricted to values greater than 0 and equal to 1. Additionally, exploring techniques from probabilistic model checking, such as graph analysis, bisimulation minimization, symbolic techniques, and partial-order reduction, could lead to improved complexity results.

Furthermore, extending natural strategies to other strategic reasoning frameworks, such as Strategy Logic, could enable the expression of a wider range of properties. Finally, investigating the epistemic dimension of MAS with natural strategies would allow us to reason about the knowledge and beliefs of agents in probabilistic settings.",Natural Strategic Ability in Stochastic Multi-Agent Systems,"Raphaël Berthon, Joost-Pieter Katoen, Munyque Mittelmann, Aniello Murano","arXiv:2401.12170v1  [cs.LO]  22 Jan 2024
Natural Strategic Ability in Stochastic Multi-Agent Systems
Rapha¨el Berthon1, Joost-Pieter Katoen1, Munyque Mittelmann2, Aniello Murano2
1 RWTH Aachen University, Germany
2 University of Naples Federico II, Italy
{berthon,katoen}@cs.rwth-aachen.de, {munyque.mittelmann, aniello.murano}@unina.it
Abstract
Strategies synthesized using formal methods can be complex
and often require inﬁnite memory, which does not correspond
to the expected behavior when trying to model Multi-Agent
Systems (MAS). To capture such behaviors, natural strategies
are a recently proposed framework striking a balance between
the ability of agents to strategize with memory and the model-
checking complexity, but until now has been restricted to
fully deterministic settings. For the ﬁrst time, we consider the
probabilistic temporal logics PATL and PATL∗ under natural
strategies (NatPATL and NatPATL∗, resp.). As main result
we show that, in stochastic MAS, NatPATL model-checking
is NP-complete when the active coalition is restricted to de-
terministic strategies. We also give a 2NEXPTIME com-
plexity result for NatPATL∗ with the same restriction. In the
unrestricted case, we give an EXPSPACE complexity for
NatPATL and 3EXPSPACE complexity for NatPATL∗.
Introduction
In the last decade, much attention has been devoted to
the veriﬁcation of Multi-Agent Systems (MAS). One of the
most important early developments was the Alternating-
time Temporal Logics ATL and ATL∗ (Alur, Henzinger, and
Kupferman 2002). Since its initial proposal, ATL has been
extended in various directions, considering, for instance,
strategy contexts (Laroussinie and Markey 2015) or adding
imperfect information and epistemic operators
(Jamroga
and Bulling 2011). Strategy Logic (SL) (Chatterjee, Hen-
zinger, and Piterman 2010; Mogavero et al. 2014) extends
ATL to treat strategies as ﬁrst-order variables. The proba-
bilistic logics PATL, PATL∗ (Chen and Lu 2007), Stochas-
tic Game Logic (Baier et al. 2012), and PSL (Aminof et al.
2019) enhances ATL, ATL∗, ATL with strategy contexts, and
SL, resp., to the probabilistic setting. Those logics allow us
to express that a coalition can enforce that the probability of
satisfying their goal meets a speciﬁed constraint.
The importance of the aforementioned logics lies in the
uncertainty often faced by MAS, due to the occurrence
of randomization, such as natural events and the behav-
ior of their components (i.e., the agents). While those as-
pects cannot be known with certainty, they can be mea-
sured based on experiments or past observations. Exam-
This is the extended version of the AAAI 2024 paper with the
same title.
ples include, among others, the afﬂuence of users inter-
acting with the system, unknown preference of its agents
modeled with probabilistic distributions, and errors of its
sensorial components. All the aforementioned logics also
have downsides, either complexity-wise or memory-wise.
PSL is undecidable, and is still 3EXPSPACE when re-
stricted to memoryless strategies. PATL model checking is
in NP ∩ co-NP but requires inﬁnite-memory strategies.
Stochastic game logic is PSPACE with memoryless de-
terministic strategies, and EXPSPACE with memoryless
probabilistic strategies. These last two results are of interest,
but the memoryless assumption is quite restrictive.
Natural strategies, ﬁrst deﬁned in (Jamroga, Malvone,
and Murano 2019a), are lists of condition-action pairs with
a bounded memory representation. This deﬁnition contrasts
with combinatorial strategies (i.e., functions from histories
to actions), considered typically in the semantics of logics
for MAS, including ATL and ATL∗. The motivation for nat-
ural strategies, as argued in (Jamroga, Malvone, and Mu-
rano 2019a), is that combinatorial strategies are not realistic
in the context of human behavior, because of the difﬁculty
to execute and design complex plans. In particular, systems
that are difﬁcult to use are often ignored by the users, even
if they respect design speciﬁcations such as security con-
straints. Artiﬁcial agents with limited memory or computa-
tional power cannot use combinatorial strategies either. On
the other end of the spectrum, memoryless strategies that
depend only on the current state cannot provide adequate
solutions to many planning problems.
Natural strategies encompass both bounded memory and
speciﬁcations of agents with “simple” strategies, by allow-
ing agents to use some past observations without requir-
ing inﬁnite memory. They aim at capturing the intuitive ap-
proach a human would use when describing strategies. As
a result, these strategies are easier to explain using natural
language. They also intrinsically feature imperfect informa-
tion, since they reason about the sequence of propositional
variables observed in previous states, instead of the states
themselves. Although the systems with whom these agents
interact may be stochastic, the study of natural strategies has
been until now restricted to fully deterministic settings. For
the ﬁrst time, we consider PATL and PATL∗ under natu-
ral strategies and investigate their model checking problem
for stochastic MAS. Remarkably, the logics we consider can
Det.∼Strategies
Prob.∼Strategies
NatPATLr
NP-complete
EXPSPACE
NatPATL∗
r
2NEXPTIME
3EXPSPACE
NatPATLR
NP-complete
EXPSPACE
NatPATL∗
R
2NEXPTIME
3EXPSPACE
Table 1: Summary of model checking complexity problems
for NatPATL and NatPATL∗ with stochastic MAS.
also be seen as an extension of POMDPS to a setting with
multiple agents with bounded memory strategies (Chatter-
jee, Chmelik, and Davies 2016).
Contribution.
In this paper, we propose variants of the
probabilistic logics PATL and PATL∗ with natural strate-
gies (denoted NatPATL and NatPATL∗, resp.) and study
their complexity for model checking. We present complex-
ity results for deterministic and, for the ﬁrst time, proba-
bilistic natural strategies. With respect to the agents’ mem-
ory, we investigate both the memoryless and bounded re-
call settings 1. Table 1 summarizes the results of this paper.
The main advantage of the logics proposed is that they en-
able to express and verify the strategic abilities of stochas-
tic MAS in which agents have limited memory and/or com-
putational power, with a reasonably good model checking
complexity. In particular, the model checking of NatPATLR
is NP-complete for deterministic natural strategies, and in
EXPSPACE for probabilistic natural strategies.
Outline.
We start the paper by presenting related work and
preliminary deﬁnitions. Then, we introduce behavioral natu-
ral strategies and the logics NatPATL and NatPATL∗. Next,
we discuss motivating examples. We proceed by presenting
technical results on the model checking complexity and ex-
pressivity. Finally, we conclude the paper.
Related Work
Several works consider the veriﬁcation of stochastic MAS
with speciﬁcations given in probabilistic logics. In par-
ticular, Huang and Luo (2013) study an ATL-like logic
for stochastic MAS when agents play deterministic strate-
gies and have probabilistic knowledge. The model checking
problem has been studied for Probabilistic Alternating-Time
µ-Calculus (Song et al. 2019). Huang, Su, and Zhang (2012)
consider the logic Probabilistic ATL∗ (PATL∗) under incom-
plete information and synchronous perfect recall. PATL was
also considered under imperfect information and memory-
less strategies (Belardinelli et al. 2023), and with accumu-
lated costs/rewards (Chen et al. 2013).
Also in the context of MAS, probabilistic logics were
used for the veriﬁcation of unbounded parameterized sys-
tems (Lomuscio and Pirovano 2020), resource-bounded sys-
tems (Nguyen and Rakib 2019), and under assumptions over
opponents’ strategies (Bulling and Jamroga 2009).
Our work is also related to the research on represen-
tation of strategies with limited memory. This includes
the representation of ﬁnite-memory strategies by input/out-
put automata (Vester 2013), decision trees (Br´azdil et al.
1As usual, we denote no recall with r and recall with R.
2015), ATL with bounded memory ( ˚Agotnes and Walther
2009), as well as the use of bounded memory as an ap-
proximation of perfect recall (Belardinelli, Lomuscio, and
Malvone 2018). More recently, Deuser and Naumov (2020)
represented strategies as Mealy machines and studied how
bounded recall affects the agents’ abilities to execute plans.
Natural strategies have ﬁrst been studied in (Jamroga,
Malvone, and Murano 2019a) on multiple deterministic set-
tings: ﬁnding winning strategies in concurrent games with
LTL speciﬁcations, deciding if a set of strategies deﬁnes a
Nash equilibrium, and model checking ATL. This last use
of natural strategies has later been extended to ATL with im-
perfect information (Jamroga, Malvone, and Murano 2019b)
and SL (Belardinelli et al. 2022).
The study of partially observable MDPs (POMDPs) also
considers a variety of strategy representations, as discussed
in (Vlassis, Littman, and Barber 2012). When allowing
inﬁnite-memory strategies, ﬁnding an almost-sure winning
strategy with a B¨uchi or reachability objective requires ex-
ponential time on POMDPs, while ﬁnding strategies for
almost-sure parity objectives (Baier, Bertrand, and Gr¨oßer
2008; Chatterjee, Doyen, and Henzinger 2010) and for max-
imizing a reachability objective (Madani, Hanks, and Con-
don 2003) is undecidable. However, when resticting the
memory of the strategies to some ﬁxed bound (Pajarinen
and Peltonen 2011; Junges et al. 2018), the complexity of
threshold reachability becomes ETR-complete (the exis-
tential theory of the reals) with probabilistic strategies and
NP-complete with deterministic strategies (Junges 2020).
The complexity of almost-sure reachability with bounded
memory probabilistic strategies is also NP-complete (Chat-
terjee, Chmelik, and Davies 2016).
Preliminaries
In this paper, we ﬁx ﬁnite non-empty sets of agents Ag, ac-
tions Ac, and atomic propositions AP. We write c for a tuple
of actions (ca)a∈Ag, one for each agent, and such tuples are
called action proﬁles. Given an action proﬁle c and C ⊆ Ag,
we let cC be the components of agents in C, and c−C is
(cb)b̸∈C. Similarly, we let Ag−C = Ag \ C.
Distributions. Let X be a ﬁnite non-empty set. A (proba-
bility) distribution over X is a function d : X → [0, 1] such
that P
x∈X d(x) = 1. Let Dist(X) be the set of distributions
over X. We write x ∈ d for d(x) > 0. If d(x) = 1 for some
element x ∈ X, then d is a point (a.k.a. Dirac) distribu-
tion. If, for i ∈ I, di is a distribution over Xi, then, writing
X = Q
i∈I Xi, the product distribution of the di is the dis-
tribution d : X → [0, 1] deﬁned by d(x) = Q
i∈I di(xi).
Markov Chains.
A Markov chain M is a tuple (St, p)
where St is a countable non-empty set of states and p ∈
Dist(St×St) is a distribution. For s, t ∈ St, the values p(s, t)
are called transition probabilities of M. A path is an inﬁnite
sequence of states.
Concurrent Game Structures.
A stochastic concurrent
game structure (or simply CGS) G is a tuple (St, L, δ, ℓ)
where (i) St is a ﬁnite non-empty set of states; (ii) L :
St×Ag → 2Ac \{∅} is a legality function deﬁning the avail-
able actions for each agent in each state, we write L(s) for
the tuple (L(s, a))a∈Ag; (iii) for each state s ∈ St and each
move c ∈ L(s), the stochastic transition function δ gives
the (conditional) probability δ(s, c)(s′) of a transition from
state s for all s′ ∈ St if each player a ∈ Ag plays the action
ca, and remark that δ(s, c) ∈ Dist(St); (iv) ℓ : St → 2AP is
a labelling function.
For each state s ∈ St and joint action c ∈ Q
a∈Ag L(s, a),
we assume that there is a state s′ ∈ St such that δ(s, c)(s′)
is non-zero, that is, every state has a successive state from a
legal move, formally c ∈ L(s, a).
Example 1 (Secure voting 2). Assume a voting system with
two types of agents: voters and coercers, represented by
the disjoint sets V
⊂ Ag and C ⊂ Ag, resp. We con-
sider a ﬁnite set of receipts, and signatures. The actions
of the voters are scanBallot, enterV ote, cnlV ote, conf,
checkSigs, checkrecr, shredr, and noop, which represent
that the agent is scanning the ballot, entering their vote,
canceling it, confirming it, checking its signature s, check-
ing the receipt r, shredding the receipt r, and doing noth-
ing, resp. On its turn, the coercer can perform the actions
coercev, requestv, punishv, and noop, representing that
she is coercing the voter v, requesting v to vote, punishing
v, and doing nothing, resp.
The CGS has propositions denoting the state of the vot-
ing system. Speciﬁcally, they describe whether the voter v
was coerced (coercedv), punished (punishedv), requested
to vote (requestedv), has a ballot available (hasBallotv),
scanned the ballot (scannedv), entered the vote which has
the signature s (entV otev,s), and has already voted (votv).
For a signature s, the proposition sigOks denotes whether
the signature s was checked and corresponds to the one in
the system, while the proposition sigFails denotes that it
was checked but did not correspond. For a receipt r, the
propositions recv,r and shrededr denotes whether r asso-
ciated with voter v and whether r was destroyed (and it’s no
longer visible), resp.
Actions performed by the agents may fail and may not
change the state of the system as intended by them. For in-
stance, the coercer may not succeed (attempting) to coerce a
voter with the action coercev (and thus, coercedv may not
be true in the next state). Similarly, a voter’s request to shred
her receipt may fail, and the information on the receipt be
still visible. The probability of an action failing is described
by the CGS stochastic transition function.
Plays. A play or path in a CGS G is an inﬁnite sequence π =
s0s1 · · · of states such that there exists a sequence c0c1 · · ·
of joint-actions such that ci ∈ L(si) and si+1 ∈ δ(si, ci)
(i.e., δ(si, ci)(si+1) > 0) for every i ≥ 0. We write πi for
si, π≥i for the sufﬁx of π starting at position i. Finite paths
are called histories, and the set of all histories is denoted
Hist. We write last(h) for the last state of a history h and
len(h) for the size of h.
2Our running example on secure voting is adapted from the case
study from (Jamroga, Kurpiewski, and Malvone 2020, 2022).
Behavioral Natural Strategies
In this section, we deﬁne behavioral3 natural strategies over
CGS, based on the deﬁnition in (Jamroga, Malvone, and
Murano 2019a), and use them to provide the semantics of
NatATL∗. Natural strategies are conditional plans, repre-
sented through an ordered list of condition-action rules. The
intuition is that the ﬁrst rule whose condition holds in the
history of the game is selected, and the corresponding ac-
tion is executed. The conditions are regular expressions over
Boolean formulas over AP, denoted Bool(AP) and given by
the following BNF grammar:
ϕ ::= p | ϕ ∨ ϕ | ¬ϕ
where p ∈ AP.
Given a state s ∈ St and a formula ϕ ∈ Bool(AP), we
inductively deﬁne the satisfaction value of ϕ in s, denoted
s |= ϕ, as follows:
s |= p
iff p ∈ ℓ(s)
s |= ϕ1 ∨ ϕ2
iff s |= ϕ1 or s |= ϕ2
s |= ¬ϕ
iff not s |= ϕ
Let Reg(Bool(AP)) be the set of regular expressions
over the conditions Bool(AP), deﬁned with the constructors
·, ∪, * representing concatenation, nondeterministic choice,
and Kleene iteration, respectively. Given a regular expres-
sion r and the language L(r) of ﬁnite words generated by
r, a history h is consistent with r iff there exists a word
b ∈ L(r) such that |h| = |b| and h[i] |= b[i], for all
0 ≤ i ≤ |h|. Intuitively, a history h is consistent with a reg-
ular expression r if the i-th epistemic condition in r holds in
the i-th state of h (for any position i in h).
A behavioral natural strategy σ with recall for an agent
a ∈ Ag is a sequence of pairs (r, Dist(Ac)), where r ∈
Reg(Bool(AP)) is a regular expression representing recall,
and d(Ac) is a distribution over the actions with d(c) ̸= 0
if c is available for a in last(h) (i.e., for c ∈ L(a, last(h))),
for all histories h consistent with r. The last pair in the se-
quence is required to be (⊤*, d(Ac)), with d(c) = 1 for
some c ∈ L(s, a) and every s ∈ St. A behavioral memory-
less natural strategy is a behavioral natural strategy without
recall: each condition is a Boolean formula (i.e., all regular
expressions have length 1). A strategy σ is deterministic if
for all pairs (r, d), we have |{c ∈ Ac | d(c) ̸= 0}| = 1.
For readability of the examples, given a pair (r, d), we write
(r, c) if d(c) = 1 for some action c ∈ Ac.
Example 2 (Secure voting, continued). Recall the voting
system introduced in Example 1. The following is a deter-
ministic memoryless natural strategy for the voter v:
1. (hasBallotv ∧ ¬scannedv, scanBallot)
2. (¬votv ∧ scannedv, enterV ote)
3. (¬votv
∧
entV otev,s
∧
¬(sigOks
∨
sigFails), checkSigs), for each signature s
4. (¬votv ∧ entV otev,s ∧ sigFails, cnlV ote), for each s
5. (¬votv ∧ entV otev,s ∧ sigOks, conf), for each s
3Behavioral strategies deﬁne the probability of taking an action
in a state. This is different from mixed strategies, which deﬁne the
probability of taking a strategy in a game. The relation of behav-
ioral and mixed strategies is discussed in (Kaneko and Kline 1995).
6. (votv ∧ recv,r ∧ ¬shrededr, shredr), for each receipt r
7. (⊤, noop)
This strategy speciﬁes that the agent ﬁrst scans the ballot
in case there is one, and it was not scanned (Pair 1). Oth-
erwise, if the agent has not voted yet and has scanned, she
enters her vote (Pair 2). If the agent did not vote, entered
the vote and did not check the signature, she checks it (3).
When the signature is checked, the agent chooses to cancel
or confirm the vote, depending on whether the veriﬁcation
has failed or succeeded (Pairs 4 and 5). If the agent has voted
and there is an unshredded visible receipt, the agent requests
it to be shredded (Pair 6). Finally, if none of the previous
conditions apply, the agent does not do any action (Pair 7).
A behavioral natural strategy with recall for a coercer is:
1.
as p(h, hs′) = P
c∈AcAg σ(h)(c) × δ(last(h), c)(s′). The
Markov chain Mσ,s induces a canonical probability space
on its set of inﬁnite paths (Kemeny, Snell, and Knapp 1976),
which can be identiﬁed with the set of plays in Out(σ, s)
and the corresponding measure is denoted out(σ, s). 5
Given a coalition strategy σC
∈ Q
a∈C Strρ
a, the set
of possible outcomes of σC from a state s ∈ St to be
the set outC(σC, s) = {out((σC, σ−C), s) : σ−C ∈
Q
a∈Ag−C Strρ
a} of probability measures that the players in
C enforce when they follow the strategy σC, namely, for
each a ∈ Ag, player a follows strategy σa in σC. We use
µσC
s
to range over the measures in outC(σC, s) as follows:
Deﬁnition 3 (NatPATL and NatPATL∗ semantics). Given a
setting ρ ∈ {r, R}, NatPATL and NatPATL∗ formulas are
interpreted in a stochastic CGS G and a path π,
G, π |=ρ p
iff p ∈ ℓ(π0)
G, π |=ρ ¬ϕ
iff G, π ̸|=ρ ϕ
G, π |=ρ ϕ1 ∨ ϕ2
iff G, π |=ρ ϕ1 or G, π |=ρ ϕ2
G, π |=ρ ⟨⟨C⟩⟩⊲⊳d
k ϕ
iff ∃σC ∈
Y
a∈C
{α ∈ Strρ
a : c(α) ≤ k}
s.t. ∀µσC
π0 ∈ outC(σC, π0), µσC
π0 ({π′ : G, π′ |=ρ ϕ}) ⊲⊳ d
G, π |=ρ Xϕ
iff G, π≥1 |=ρ ϕ
G, π |=ρ ψ1Uψ2
iff ∃k ≥ 0 s.t. G, π≥k |=ρ ψ2 and
∀j ∈ [0, k). G, π≥j |=ρ ψ1
Motivating Examples
In this section, we present problems that motivate reason-
ing in stochastic MAS, and we illustrate how NatPATL∗-
formulas can be used to express properties on those systems.
Let us start with an example of door access control with a
random robot. This example illustrates a setting in which it
sufﬁces to have deterministic strategies in stochastic CGSs.
Example 3 (Access control). We consider the example il-
lustrated in Figure 1. We are given a set Ag of agents, a
set of square tiles, where a non-controlled robot moves ran-
domly either one tile right, left, up, or down at every time
step. Between every tile, there is either a wall, a door con-
trolled by some agent with actions open and close, or noth-
ing. The robot can cross an empty space, cannot cross a wall,
and can only cross a door if the agent controlling has taken
action open. Given a set of targets represented by atomic
propositions T = {ti ∈ AP, i ∈ {1, n}} labelling some
tiles, and related coalitions {Ci ⊆ Ag, i ∈ {1, n}}, we use
NatPATL∗ to state that some coalition C ⊆ Ag has a strat-
egy with memory k ∈ N reaching all targets inﬁnitely often
with probability 0.7, formally:
⟨⟨C⟩⟩≥0.7
k
G
^
tj∈T,tj̸=ti
F tj
(1)
In the example of Figure 1, where n = 2, the coalition
controls two doors adjacent to the initial state. Even though
5This is a classic construction, see for instance (Clarke et al.
2018; Berthon et al. 2020).
the structure is probabilistic, memoryless strategies are sufﬁ-
cient. Opening the left door gives the robot a chance to move
left, which brings it closer to target t0, but in this center-
leftmost square, the agents not in the coalition may open the
door leading to the bottom-left square, where they can then
trap the robot: the robot only has probability 1
2 to success-
fully reach t0, and otherwise may be trapped forever. The
other option available to the coalition in the initial state is to
close the left door, and open all other doors. The robot will
take longer, but has probability 1 to eventually reach target
t1. Thus, we can reach t1 with probability 1, but t0 with only
probability 1
2, and property 1 does not hold.
2
◦◦–
×
=
=
×
=
=
×
×
=
=
=
t0
t1
Figure 1: A robot in a maze, where = and × denote a door
respectively controlled by the coalition or the agents not in
the coalition. Full lines represent walls. t0, t1 are two targets.
Going back to Example 1, we now illustrate how
NatPATL∗ and NatPATL can be used for the formal secu-
rity analysis of voting systems.
Example 4 (Secure voting, continued). A requirement usu-
ally considered for electronic voting systems is voter-
veriﬁability, which captures the ability of the voter to verify
her vote (Jamroga, Kurpiewski, and Malvone 2022). In our
example, this is represented by the propositions sigOks and
sigFails. The NatPATL formula
⟨⟨v⟩⟩≥0.9
k
F(sigOks ∨ sigFails)
says that the voter v has a strategy of size at most k so that,
at some point and with probability at least 0.9, she obtains
either the positive or the negative outcome of verifying the
signature s.
Another requirement is receipt-freeness, which expresses
that the voters can not gain a receipt to prove that they voted
in a certain way. In our example, the propositions receiptv,r
and shrededr represent that a receipt r is associated with
the voter v and that the information on it was destroyed. The
NatPATL formula:
¬⟨⟨v⟩⟩≥0.5
k
F
_
receipt r
(receiptv,r ∧ ¬shrededr)
says that there is no strategy of complexity at most k to en-
sure with probability at least 0.5 that, eventually, there will
be an unshredded receipt for her.
Model Checking Complexity
In this section, we look at the complexity of model checking
for different versions of NatPATL.
Deﬁnition 4. Given a setting ρ ∈ {r, R}, a CGS G, state
s ∈ St, and formula ϕ in NatPATLρ (NatPATL∗ρ, resp.),
the model checking problem for NatPATLρ (NatPATL∗ρ,
resp.) consists in deciding, whether G, s |=ρ ϕ.
Theorem
1.
Model
checking
NatPATLr
(respectively
NatPATLR) with deterministic natural strategies for the
coalition is in NP.
Proof. We ﬁrst deﬁne the conjugate cj() of comparison op-
erators: cj(≤) is <, cj(<) is ≤, cj(>) is ≥ and cj(≥)
is >. We focus on the case of NatPATLR. Let ϕ be a
NatPATLR formula and G = (St, L, δ, ℓ) be a CGS. We
guess a polynomial witness consisting of one deterministic
strategy σ[s, ϕ′, a] with some bounded complexity k for ev-
ery s ∈ St, every subformula ϕ′ = ⟨⟨C⟩⟩⊲⊳d
k (ϕ′′) of ϕ, and
every a ∈ C. We now show how to check that formula ϕ
holds given such a witness.
For every formula ϕ′′ that does not contain any coali-
tion operator, it can be decided in polynomial time if
s |= ⟨⟨Ag\C⟩⟩cj(⊲⊳)1−d
k
(¬ϕ′′) holds when some strategies
σ[s, ϕ′, a] are ﬁxed for agents a ∈ C. Indeed, after ﬁxing
such strategies, we obtain an MDP where formula ¬ϕ′′ can
be translated in polynomial time in a polynomial reachabil-
ity or invariance objective, and checking whether there is
probability cj(⊲⊳)1 − d for ¬ϕ′′ to hold on this MDP can be
done in polynomial time (Baier and Katoen 2008).
Going through ϕ in a bottom-up manner, we iteratively re-
place every subformula ϕ′ = ⟨⟨C⟩⟩⊲⊳d
k (ϕ′′) by a new formula
that is only true in states s where ⟨⟨Ag\C⟩⟩cj(⊲⊳)1−d
k
(¬ϕ′′)
does not hold assuming agents in C follow strategies
σ[s, ϕ′, a]. This can be done in polynomial time, and returns
whether ϕ holds or not.
Theorem
2.
Model
checking
NatPATLr
(respectively
NatPATLR) with deterministic natural strategies for the
coalition is NP-hard.
Proof. We start by showing that NatPATLr with determinis-
tic natural strategies for the coalition extends POMDPs with
memoryless deterministic strategies and almost-sure reach-
ability objective. Indeed, a POMDP represented as a CGS
G = (St, L, δ, ℓ) (two states are indistinguishable if they are
labelled by the same propositional variables), a single agent
A, and a set of target states distinguished by some proposi-
tional variable t that holds only in these states, there exists a
strategy almost surely reaching t from an initial state s ∈ St
if and only if the NatPATLr formula ⟨⟨A⟩⟩=1
|St|(Ft) holds on
G. Indeed, memoryless strategies cannot have a complex-
ity higher than |St|, the number of states in the MDP, and so
available strategies coincide. In Proposition 2 of (Chatterjee,
Chmelik, and Davies 2016), it is shown that ﬁnding strate-
gies for POMDPs with memoryless randomized strategies
and almost-sure reachability objective is NP-hard. It uses a
reduction from Lemma 1 of (Chatterjee, K¨oßler, and Schmid
2013), that only uses deterministic strategies. As such, ﬁnd-
ing strategies for POMDPs with memoryless determinis-
tic strategies and almost-sure reachability objective is NP-
hard, so it is the model checking NatPATLr with behavioral
natural deterministic strategies for the coalition.
Theorem 3.
Model checking NatPATL∗
r
(respectively
NatPATL∗
R) with deterministic natural strategies for the
coalition is in 2NEXPTIME.
Proof. Let ϕ be a NatPATL∗
R formula and G = (St, L, δ, ℓ)
be a CGS. We guess a polynomial witness consisting of one
deterministic strategy σ[s, ϕ′, a] with complexity k for every
s ∈ St, every subformula ϕ′ = ⟨⟨C⟩⟩⊲⊳d
k (ϕ′′) of ϕ, and every
a ∈ C. We now show how to check in 2EXPTIME that
formula ϕ holds given such a witness.
For every formula ϕ′′ that does not contain any coalition
operator, it can be decided with in 2EXPTIME time if
s |= ⟨⟨Ag\C⟩⟩cj(⊲⊳)1−d
k
(¬ϕ′′) holds when some strategies
σ[s, ϕ′, a] are ﬁxed for agents a ∈ C. Indeed, after ﬁxing
such strategies, we obtain an MDP and an LTL formula ¬ϕ′′
that can be model checked in 2EXPTIME (Courcoubetis
and Yannakakis 1995).
Going through ϕ in a bottom-up manner, we iteratively re-
place every subformula ϕ′ = ⟨⟨C⟩⟩⊲⊳d
k (ϕ′′) by a new formula
that is only true in states s where ⟨⟨Ag\C⟩⟩cj(⊲⊳)1−d
k
(¬ϕ′′)
does not hold assuming agents in C follow strategies
σ[s, ϕ′, a]. We only need to check a polynomial number of
such formulas, and this returns whether ϕ holds or not.
Theorem 4.
Model checking NatPATL∗
r
(respectively
NatPATL∗
R) with deterministic natural strategies for the
coalition is 2EXPTIME-hard.
Proof. We use a reduction from LTL model checking on
MDPs, which is 2EXPTIME-complete. Given an LTL
formula ϕ, a threshold d ∈ [0, 1] and a CGS G with only
one agent Ag, we say ϕ holds with at least probability d
on G if and only if the NatPATL∗
r formula ⟨⟨∅⟩⟩≥1−d
k
(¬ϕ)
holds on MDP G: this formula states that for any strategy of
the agent (without any complexity bound, since the coalition
is empty), formula ¬ϕ holds with probability at least 1 − d:
this only happens if there is no strategy ensuring ϕ with at
least probability d.
When considering probabilistic strategies, we follow the
same technique as (Aminof et al. 2019) to reduce the prob-
lem to model checking real arithmetic. Since LTL is sub-
sumed by NatPATL∗
R, we also have a doubly-exponential
blowup. On the other hand, with NatPATLR, we roughly
follow an idea introduced for stochastic game logic (Baier
et al. 2012) to avoid this blowup. As in the proof of Theo-
rem 1, it is sufﬁcient to consider reachability and invariance
problems, both of which are polynomial. The same holds
for NatPATL∗
r. Next, we consider the model checking of
behavioral strategies. We remark that the 2EXPTIME-
hardness from Theorem 4 also applies to NatPATLr and
NatPATL∗
r with behavioral natural strategies. We now give
model checking algorithms and their complexity.
Theorem 5.
Model checking NatPATL∗
r
(respectively
NatPATL∗
R) with behavioral natural strategies for the coali-
tion is in 3EXPSPACE.
Sketch of proof. Probabilistic Strategy Logic (PSL) with
an additional behavioral natural strategies operator ∃nat
k
captures NatPATL∗
R, and we show its model checking
is in 3EXPSPACE. We give some additional details
on PSL in Deﬁnition 6 and 7 in appendix. When trans-
lating a NatPATL∗
R formula into a PSL formula in a
bottom-up manner, assuming formula ϕ can already be
translated into some PSL(ϕ) without any complexity
blowup, the ⟨⟨C⟩⟩⊲⊳d
k ϕ subformulas, can be translated as
∃nat
k
σ∀µPC→σ, Ag\C→µ(PSL(ϕ)) ⊲⊳ d: a coalition satis-
ﬁes ϕ iff there exists a natural strategy for the coalition such
that for all strategies of the other agents, the PSL translation
of ϕ holds. To model check the operator ∃nat
k
, we modify
the proof of Theorem 1 of (Aminof et al. 2019) showing
that model checking PSL with memoryless strategies is in
3EXPSPACE. This proof translates PSL into real arith-
metic, and a variable rx,s,a represents the probability for
strategy x to take action a in state s. We can extend this
notation to behavioral natural strategies: for a strategy with
complexity k, we replace variables rx,s,a by rx,s,a,q where q
is the current state of the automata representing the regular
expressions of a behavioral natural strategy:
_
strategies σ, compl(σ)≤k
^
(r,a)∈σ
A(r,a)
is an automaton with current state q[r]. We state that two
probabilities are equal if they are accepted by the same reg-
ular expressions:
^
(r,a)∈σ
acc(q[r], A(r,a)) ∧ acc(q′[r], A(r,a))
⇒ rx,s,a,q = rx,s,a,q′
Both are exponential in the largest k in the formula, since
there are exponentially many possible automata of size less
or equal to k, and we need to describe at most k of them
in every conjunction. Nothing else is changed in the proof
of (Aminof et al. 2019), and thus we have a 3EXPSPACE
complexity in the size of the NatPATL∗
R formula, exponen-
tial in the size of the system and 2EXPSPACE in the
largest complexity k used in the formula.
Theorem
6.
Model
checking
NatPATLr
(respectively
NatPATLR) with behavioral natural strategies for the coali-
tion is in EXPSPACE.
Sketch of proof. The proof is slightly more complicated
than the previous one. We again adapt the proof of The-
orem 1 of (Aminof et al. 2019). In the proof of The-
orem 5, we translate our fragment of PSL with nat-
ural strategies to real arithmetic. The only exponential
blowup comes when translating the coalition operator into
∃nat
k
σ∀µPC→σ, Ag\C→µ(PSL(ϕ)) ⊲⊳ d where ϕ is as-
sumed to be an LTL formula whose atoms are either propo-
sitional variables, or variables representing other formulas
starting with P. This translation constructs a deterministic
Rabin automaton whose size is exponential in the size of the
CGS, double exponential in the size of ψ, and uses a num-
ber of quantiﬁers double exponential in the size of ψ. Since
we consider NatPATLR, this LTL formula may only be ei-
ther Xϕ or ϕUϕ′, where ϕ and ϕ′ have been inductively
represented as Boolean formulas. Proposition 5.1 and The-
orem 5.2 of (Alur, Henzinger, and Kupferman 2002) show
that such formulas can be polynomially translated to either
reachability or invariance games, which can be done us-
ing an automaton and a number of variables both polyno-
mial in the size of the CGS and ψ. Since model checking
real arithmetic is exponential in the number of quantiﬁers
of the formula (Ben-Or, Kozen, and Reif 1986; Fitchas, Gal-
ligo, and Morgenstern 1987), we obtain that model checking
NatPATLR with behavioral natural strategies for the coali-
tion is in EXPSPACE.
Expressivity
We now compare the expressive power of NatPATL∗ to that
of PATL∗. We ﬁrst recall the notions of distinguishing and
expressive powers.
Deﬁnition 5 (Distinguishing power and expressive power
(Wang and Dechesne 2009)). Consider two logical systems
L1 and L2, with their semantics (denoted |=L1 and |=L2,
resp.) deﬁned over the same class of models M. We say that
L1 is at least as distinguishing as L2 (written: L2 ⪯d L1) iff
for every pair of models M, M ′ ∈ M , if there exists a for-
mula ϕ2 ∈ L2 such that M |=L2 ϕ2 and M ′ ̸|=L2 ϕ2, then
there is also ϕ1 ∈ L1 with M |=L1 ϕ1 and M ′ ̸|=L1 ϕ1.
Moreover, L1 is at least as expressive as L2 (written:
L2 ⪯e L1) iff for every ϕ2 ∈ L2 there exists ϕ1 ∈ L1 such
that, for every M ∈ M, we have M |=L2 ϕ2 iff M |=L1 ϕ1.
NatPATL∗ and PATL∗ are based on different notions of
strategic ability. As for the deterministic setting with ATL,
each behavioral natural strategy can be translated to a be-
havioral combinatorial one (i.e., mappings from sequences
of states to actions), but not vice versa. Consequently,
PATL∗ can express that a given coalition has a combina-
torial strategy to achieve their goal, which is not expressible
in NatPATL∗. On the other hand, NatATL∗ allows express-
ing that a winning natural strategy with bounded complexity
does not exist, which cannot be captured in PATL∗. Now we
show that NatPATL∗ allows expressing properties that can-
not be captured in PATL∗, and vice versa.
Theorem 7. For both memoryless and recall semantics:
• NatPATL (resp. NatPATL∗) and PATL (resp, PATL∗)
have incomparable distinguishing power over CGS.
• NatPATL (resp. NatPATL∗) and PATL (resp, PATL∗)
have incomparable expressive power over CGS.
Sketch of proof. The proof can be obtained by a slight ad-
justment of the proofs regarding the distinguishing power
of Quantiﬁed SL with Natural Strategies (NatSL[F]) and
Quantiﬁed SL (SL[F]) with combinatorial strategies (Propo-
sitions 8 and 9 of (Belardinelli et al. 2022)). Since the logics
considered there are not Boolean, notice that (i) the satisfac-
tion value 1 and −1 represent whether a formula is satisﬁed
by a weighted CGS or is not; (ii) the counterexamples con-
sidered in the proofs are weighted deterministic CGS where
the value of atomic propositions are restricted to −1 and 1,
and thus can be easily transformed in Boolean determinis-
tic CGSs; and (iii) the counterexamples showing that the
logics have incomparable distinguishing power are easily
converted to PATL and NatPATL formulas by changing the
deterministic coalition operators into probabilistic coalition
operators with probability ≥ 1.
Thus, we have that, for both memoryless and recall se-
mantics:
• NatPATL (resp, NatPATL∗) ̸⪯d PATL (resp, PATL∗)
• PATL (resp, PATL∗) ̸⪯d NatPATL (resp, NatPATL∗)
That is, NatPATL (resp, NatPATL∗) and PATL (resp,
PATL∗) have incomparable distinguishing power.
From the deﬁnitions of distinguishing power and expres-
sive power, it is easy to see that, for two logical systems L1
and L2, L2 ⪯e L1 implies L2 ⪯d L1. By transposition, we
also get that L2 ̸⪯d L1 implies L2 ̸⪯e L1. Thus, NatPATL
(resp, NatPATL∗) and PATL (resp, PATL∗) have incompa-
rable expressive power.
Conclusion
In this work, we have deﬁned multiple variations of PATL
with natural strategies, and studied their model-checking
complexity. We have illustrated with multiple examples the
relevance of the probabilistic setting, which can represent
uncertainty in a very precise way, and the interest in natural
strategies, that are both efﬁcient and much closer to what a
real-world agent is expected to manipulate.
In terms of model checking, the NP-completeness of
NatPATL with deterministic strategies is promising, and
shows we can capture POMDPs with bounded memory
without any signiﬁcant loss. While the 2NEXPTIME
complexity for NatPATL∗ with deterministic strategies
is high, we have shown a close lower bound, namely
2EXPTIME-hardness. With probabilistic strategies, the
EXPSPACE membership of NatPATL is quite similar to
the result of (Baier et al. 2012), and the 3EXPSPACE
membership of NatPATL∗ is also similar to (Aminof et al.
2019). Since this exponential space blowup comes from the
use of real arithmetic to encode probabilities, any improve-
ment would likely come from the introduction of a totally
new technique. Similarly, the doubly-exponential blowup
between PATL and PATL∗ comes from the 2EXPTIME-
completeness of LTL model checking on MDPs. We also
keep the 2EXPTIME-hardness from the deterministic
case. To our knowledge, similar works (Baier et al. 2012;
Aminof et al. 2019), do also not give different lower bounds
between deterministic and probabilistic strategies. A possi-
ble approach would be to use a construction from POMDPs,
more precisely either (Junges et al. 2018), showing that syn-
thesis on POMDPs with reachability objectives and bounded
memory is NP-complete for deterministic strategies and
ETR-complete for probabilistic ﬁnite-memory strategies
or (Oliehoek 2012), showing that ﬁnding a policy maximiz-
ing a reward on a decentralized POMDPs with full mem-
ory is NEXPTIME-complete). Our results on expressiv-
ity mean that there are properties of stochastic MAS with
natural strategies that cannot be equivalently translated to
properties based on combinatorial strategies, and vice versa.
The proof of Theorem 5 shows that we could extend nat-
ural strategies to PSL, but it would be difﬁcult to get a bet-
ter result than our 3EXPSPACE complexity. Consider-
ing qualitative PATL∗ or PSL (i.e. only thresholds > 0 and
= 1) may yield a better complexity. For the quantitative
setting, i.e., thresholds such as >
1
2, techniques from the
ﬁeld of probabilistic model checking can be applied, e.g.,
graph analysis, bisimilation minimization, symbolic tech-
niques, and partial-order reduction (Katoen 2016). Another
direction would be to consider epistemic operators. Indeed,
many applications involving agents with a reasonable way
to strategize also have to take into account the knowledge
and beliefs of these agents. As such, we would have to ﬁnd a
good epistemic framework such that natural strategies keep
the desired balance between expressivity and complexity.
Acknowledgements
This project has received funding from the European
Union’s Horizon 2020 research and innovation programme
under the Marie Skłodowska Curie grant agreement No
101105549. This research has been supported by the PRIN
project RIPER (No. 20203FFYLK), the PNRR MUR project
PE0000013-FAIR, the InDAM 2023 project “Strategic Rea-
soning in Mechanism Design”, and the DFG Project POM-
POM (KA 1462/6-1).
References
˚Agotnes, T.; and Walther, D. 2009. A Logic of Strategic
Ability Under Bounded Memory. Journal of Logic, Lan-
guage and Information, 18(1): 55–77.
Alur,
R.;
Henzinger, T.;
and
Kupferman, O.
2002.
Alternating-time temporal logic. J. ACM, 49(5): 672–713.
Aminof, B.; Kwiatkowska, M.; Maubert, B.; Murano, A.;
and Rubin, S. 2019. Probabilistic Strategy Logic. In Proc.
of IJCAI 2019, 32–38. ijcai.org.
Baier, C.; Bertrand, N.; and Gr¨oßer, M. 2008. On Decision
Problems for Probabilistic B¨uchi Automata. In FOSSACS.
Baier, C.; Br´azdil, T.; Gr¨oßer, M.; and Kucera, A. 2012.
Stochastic game logic. Acta Informatica, 49(4): 203–224.
Baier, C.; and Katoen, J. 2008. Principles of model checking.
MIT Press. ISBN 978-0-262-02649-9.
Belardinelli, F.; Jamroga, W.; Malvone, V.; Mittelmann,
M.; Murano, A.; and Perrussel, L. 2022. Reasoning about
Human-Friendly Strategies in Repeated Keyword Auctions.
In AAMAS-22.
Belardinelli, F.; Jamroga, W.; Mittelmann, M.; and Murano,
A. 2023. Strategic Abilities of Forgetful Agents in Stochas-
tic Environments. In Proc. of KR-23.
Belardinelli, F.; Lomuscio, A.; and Malvone, V. 2018. Ap-
proximating Perfect Recall When Model Checking Strategic
Abilities. In Proc. of KR 2018.
Ben-Or, M.; Kozen, D.; and Reif, J. H. 1986. The Complex-
ity of Elementary Algebra and Geometry. J. Comput. Syst.
Sci., 32(2): 251–264.
Berthon, R.; Fijalkow, N.; Filiot, E.; Guha, S.; Maubert, B.;
Murano, A.; Pinault, L.; Pinchinat, S.; Rubin, S.; and Serre,
O. 2020. Alternating Tree Automata with Qualitative Se-
mantics. ACM Trans. Comput. Logic, 22(1): 1–24.
Br´azdil, T.; Chatterjee, K.; Chmelik, M.; Fellner, A.; and
Kret´ınsk´y, J. 2015. Counterexample Explanation by Learn-
ing Small Strategies in Markov Decision Processes. In CAV
2015, LNCS 9206, 158–177. Springer.
Bulling, N.; and Jamroga, W. 2009. What Agents Can Prob-
ably Enforce. Fundam. Informaticae, 93(1-3): 81–96.
Chatterjee, K.; Chmelik, M.; and Davies, J. 2016. A Sym-
bolic SAT-Based Algorithm for Almost-Sure Reachability
with Small Strategies in POMDPs. In AAAI, 3225–3232.
AAAI Press.
Chatterjee, K.; Doyen, L.; and Henzinger, T. A. 2010. Qual-
itative Analysis of Partially-Observable Markov Decision
Processes. In Proc. of MFCS.
Chatterjee, K.; Henzinger, T. A.; and Piterman, N. 2010.
Strategy Logic. Inf. Comput., 208(6): 677–693.
Chatterjee, K.; K¨oßler, A.; and Schmid, U. 2013.
Auto-
mated analysis of real-time scheduling using graph games.
In HSCC, 163–172. ACM.
Chen, T.; Forejt, V.; Kwiatkowska, M.; Parker, D.; and
Simaitis, A. 2013.
Automatic veriﬁcation of competitive
stochastic systems. Formal Methods in System Design, 43:
61–92.
Chen, T.; and Lu, J. 2007.
Probabilistic alternating-time
temporal logic and model checking algorithm. In Proc. of
FSKD, 35–39.
Clarke, E.; Grumberg, O.; Kroening, D.; Peled, D.; and
Veith, H. 2018. Model checking. MIT press.
Courcoubetis, C.; and Yannakakis, M. 1995. The Complex-
ity of Probabilistic Veriﬁcation. J. ACM, 42(4): 857–907.
Deuser, K.; and Naumov, P. 2020.
On composition of
bounded-recall plans. Artiﬁcial Intelligence, 289: 103399.
Fitchas, N.; Galligo, A.; and Morgenstern, J. 1987.
Al-
gorithmes rapides en sequentiel et en parallele pour
l’´elimination des quantiﬁcateurs en G´eom´etrie ´elementaire.
Seminaire sur les structures alg´ebriques ordonn´ees.
Huang, X.; and Luo, C. 2013.
A logic of probabilistic
knowledge and strategy. In Proc. of AAMAS 2013, 845–852.
Huang, X.; Su, K.; and Zhang, C. 2012.
Probabilistic
Alternating-Time Temporal Logic of Incomplete Informa-
tion and Synchronous Perfect Recall. In Proc. of AAAI 2012.
Jamroga, W.; and Bulling, N. 2011.
Comparing variants
of strategic ability. In Proc. of IJCAI 2011, 252–257. IJ-
CAI/AAAI.
Jamroga, W.; Kurpiewski, D.; and Malvone, V. 2020. Nat-
ural strategic abilities in voting protocols. In Int. Workshop
on Socio-Technical Aspects in Security and Trust, 45–62.
Jamroga, W.; Kurpiewski, D.; and Malvone, V. 2022. How
to measure usable security: Natural strategies in voting pro-
tocols. Journal of Computer Security, 30(3): 381–409.
Jamroga, W.; Malvone, V.; and Murano, A. 2019a. Natural
strategic ability. Artiﬁcial Intelligence, 277: 103170.
Jamroga, W.; Malvone, V.; and Murano, A. 2019b. Natu-
ral Strategic Ability under Imperfect Information. In Proc.
AAMAS 2019.
Junges, S. 2020. Parameter synthesis in Markov models.
Ph.D. thesis, RWTH Aachen University, Germany.
Junges, S.; Jansen, N.; Wimmer, R.; Quatmann, T.; Winterer,
L.; Katoen, J.; and Becker, B. 2018. Finite-State Controllers
of POMDPs using Parameter Synthesis.
In Proc. of UAI
2018.
Kaneko, M.; and Kline, J. J. 1995.
Behavior strategies,
mixed strategies and perfect recall. International Journal
of Game Theory, 24: 127–145.
Katoen, J.-P. 2016. The probabilistic model checking land-
scape. In Proceedings of the 31st Annual ACM/IEEE Sym-
posium on Logic in Computer Science, 31–45.
Kemeny, J. G.; Snell, J. L.; and Knapp, A. W. 1976. Stochas-
tic Processes.
In Denumerable Markov Chains, 40–57.
Springer.
Laroussinie, F.; and Markey, N. 2015.
Augmenting ATL
with strategy contexts. Inf. Comput., 245: 98–123.
Lomuscio, A.; and Pirovano, E. 2020. Parameterised ver-
iﬁcation of strategic properties in probabilistic multi-agent
systems. In Proc. of AAMAS 2020, 762–770.
Madani, O.; Hanks, S.; and Condon, A. 2003. On the un-
decidability of probabilistic planning and related stochastic
optimization problems. Artif. Intell., 147(1-2): 5–34.
Mogavero, F.; Murano, A.; Perelli, G.; and Vardi, M. Y.
2014. Reasoning About Strategies: On the Model-Checking
Problem. ACM Trans. Comput. Log., 15(4).
Nguyen, H. N.; and Rakib, A. 2019. A Probabilistic Logic
for Resource-Bounded Multi-Agent Systems.
In Proc. of
IJCAI 2019, 521–527.
Oliehoek, F. A. 2012. Decentralized POMDPs. In Rein-
forcement Learning: State-of-the-Art, 471–503. Springer.
Pajarinen, J.; and Peltonen, J. 2011. Periodic Finite State
Controllers for Efﬁcient POMDP and DEC-POMDP Plan-
ning. In Proc. of NIPS 2011., 2636–2644.
Song, F.; Zhang, Y.; Chen, T.; Tang, Y.; and Xu, Z. 2019.
Probabilistic alternating-time µ-calculus. In Proc. of AAAI,
6179–6186.
Vester, S. 2013. Alternating-time temporal logic with ﬁnite-
memory strategies. In GandALF 2013, 194–207.
Vlassis, N.; Littman, M. L.; and Barber, D. 2012. On the
Computational Complexity of Stochastic Controller Opti-
mization in POMDPs. ACM Trans. Comput. Theory, 4(4):
12:1–12:8.
Wang, Y.; and Dechesne, F. 2009. On expressive power and
class invariance. arXiv preprint arXiv:0905.4332.
Appendix
Probabilistic Strategy Logic
Given a set of atomic proposition AP, agents Ag, and strat-
egy variables Var, we introduce Probabilistic Strategy Logic
(PSL), deﬁned as in (Aminof et al. 2019).
Deﬁnition 6 (PSL syntax). The syntax of PSL is deﬁned by
the grammar:
ϕ ::= p | ϕ ∨ ϕ | ¬ϕ | ∃x.ϕ | τ ≤ τ
τ ::= c | τ −1 | τ − τ | τ + τ | τ × τ | Pβ(ψ)
ψ ::= ϕ | ¬ψ | ψ ∨ ψ | Xψ | ψUψ
Formulas ϕ are called history formulas, formulas τ are
called arithmetic terms, and formula ψ are called path
formulas. In the proof of Theorem 5, we introduce a
new history formula: ϕ ::= ∃nat
k
x.ϕ, stating that there
exists a natural strategy with complexity k. We remark
that apart from Boolean formulas over propositional vari-
ables, the only history formulas and arithmetic terms we
use in this proof to encode NatATL∗ are of the form
∃nat
k
σ. ∀µ. PC→σ, Ag\C→µ(PSL(ϕ)) ⊲⊳ d.
Deﬁnition 7 (PSL semantics). We denote by Σ the set of
all strategies (natural or not) and use µσC
s
to range over the
measures in outC(σC, s). PSL formulas are interpreted in a
stochastic CGS G, a valuation ν : Var → Σ and a path π.
The semantics of history formulas is as follows:
G, ν, π |= p
iff p ∈ ℓ(π0)
G, ν, π |= ¬ϕ
iff G, ν, π ̸|= ϕ
G, ν, π |= ϕ1 ∨ ϕ2
iff G, ν, π |= ϕ1 or G, ν, π |= ϕ2
G, ν, π |= ∃x.ϕ
iff ∃σ ∈ Σ. G, ν[x 7→ σ], π |= ϕ
G, ν, π |= τ1 ≤ τ2
iff valν, π(τ1) ≤ valν,π(τ2)
where
valν, π(c) = c and valν, π(τ −1) = valν, π(τ))−1
valν, π(τ ⊕ τ ′) = valν, π(τ) ⊕ valν, π(τ ′) for ⊕ ∈ {−, +, ×}
valν, π(Pβ(ψ)) = µν◦β
π0 ({π : G, ν, π |= ψ})
The semantics of path formulas is as follows:
G, ν, π |= ¬ϕ
iff G, ν, π ̸|= ϕ
G, ν, π |= ψ1 ∨ ψ2
iff G, ν, π |= ψ1 or G, ν, π |= ψ2
G, ν, π |= Xψ
iff G, ν, π≥1 |= ψ
G, π |= ψ1Uψ2
iff ∃k ≥ 0 s.t. G, ν, π≥k |= ψ2 and
∀j ∈ [0, k). G, ν, π≥j |= ψ1
We also deﬁne the semantics of the natural strategy exis-
tential quantiﬁer:
G, ν, π |= ϕ1 ∨ ϕ2
iff ∃σ ∈ Σ, σ natural , G, ν[x 7→ σ], π |= ϕ
"
"This paper proposes a novel semantic compression technique to address the challenges of executing computationally intensive Convolutional Neural Network (CNN) inference tasks in Multi-access Edge Computing (MEC) networks. The proposed method aims to efficiently offload CNN inference tasks while maintaining accuracy, making use of the recent advancements in semantic communication. The key contributions include a feature compression module for intermediate tensor compression, a lightweight feature recovery module, a reward function, and a Graph Reinforcement Learning (GRL)-based Autoencoder. The performance of the proposed method is evaluated in a dynamic MEC network comprising of multiple Internet of Things (IoT) devices and Edge Servers (ESs). The results show that the proposed method outperforms existing state-of-the-art methods in terms of inference accuracy, service success reliability, and average throughput, demonstrating its effectiveness in offloading decision-making.","This paper investigates the computational offloading of CNN inference in dynamic MEC networks. Uncertainties in communication time and computation resource availability pose challenges to achieving efficient and reliable offloading. The contributions of this work include: 1) A novel semantic compression method, Autoencoder-based CNN architecture (AECNN), for effective semantic extraction and compression in partial offloading. 2) A lightweight decoder to reconstruct the intermediate data through learning from the compressed data. 3) A reward function and optimization formulation to trade-off communication, computation, and inference accuracy. 4) A GRL-based AECNN method to address dynamic resource availability and uncertain computation time. The proposed method outperforms existing works under different dynamic scenarios, highlighting the effectiveness of GRL-AECNN in offloading decision-making.","The related work covers: 1) CNN inference offloading in MEC networks, discussing dynamic offloading methods and challenges in meeting stringent deadlines, highlighting the need for other alternatives. 2) Semantic communication and feature compression, discussing the emerging paradigm of semantic communication and its potential for efficient CNN inference offloading. Existing works on semantic compression are reviewed, identifying the limitations of directly resizing feature dimensions. 3) Reinforcement learning for offloading, discussing the application of reinforcement learning in addressing dynamic computational offloading problems, and highlighting the advantages of using GRL for graph-like data structures.nan","The proposed GRL-AECNN method involves: 1) Feature compression module: A CA module is designed to quantify the importance of channels in the intermediate tensor, enabling pruning of low-importance channels. Entropy encoding is used to remove statistical redundancy in the remaining features. 2) Feature recovery module: A lightweight CNN-based FR module is designed to recover the pruned features from the decoded features. 3) Reward function and optimization: A reward function is defined to trade-off communication, computation, and inference accuracy. The CNN inference offloading problem is formulated as a maximization problem to optimize average inference accuracy and throughput over the long term under latency and transmission power constraints. 4) GRL-AECNN: GCN is applied to analyze the characteristics of graph data through message passing and aggregation between nodes. GRL-AECNN uses a two-layer GCN to capture the information of tasks and the status of ESs. The actor network generates relaxed offloading actions, which are quantified into binary offload decisions by the critic network. The optimal offloading action is selected based on the reward.","1) Performance of GRL-AECNN: GRL-AECNN improves inference accuracy compared to existing semantic compression methods, BottleNet++, and DeepJSCC, especially when the splitting point is near the input layer. AECNN achieves higher accuracy by splitting the model at the first splitting point than the second with the same communication overhead. 2) Convergence of GRL-AECNN: The moving average of the normalized reward and training loss show gradual convergence towards the optimal solution. GRL-AECNN consistently outperforms other methods in terms of convergence speed and performance metrics. 3) Performance under various number of IoT devices: As the number of IoT devices increases, the average accuracy and SSP decrease while the average throughput gradually reaches a plateau. GRL-AECNN achieves higher improvement in average accuracy, SSP, and throughput compared to other methods, highlighting its capability in handling large numbers of IoT devices. 4) Performance under uncertain computation time: As the variation range of ESs' available computational resources increases, both average inference accuracy and average throughput decrease. GRL-AECNN shows lower degradation in average inference accuracy and average throughput than DROO-AECNN under larger variations in ESs' computational resources and computation time fluctuations. 5) Performance under imperfect CSI: As the uncertainty bound of CSI imperfections increases, the average inference accuracy of the system decreases, but GRL-AECNN's degradation is relatively small, demonstrating its effectiveness in aggregating CSI imperfections and making robust offloading decisions.","This paper proposes a novel semantic compression technique to address the challenges of executing computationally intensive CNN inference tasks in dynamic MEC networks. The proposed AECNN method leverages advancements in semantic communication for efficient CNN inference offloading while maintaining accuracy. The experimental results demonstrate that GRL-AECNN outperforms existing state-of-the-art methods in terms of inference accuracy, service success reliability, and average throughput under dynamic MEC scenarios.",Dynamic Semantic Compression for CNN Inference in Multi-access Edge Computing: A Graph Reinforcement Learning-based Autoencoder,"Nan Li, Alexandros Iosifidis, Qi Zhang","1
Dynamic Semantic Compression for CNN Inference
in Multi-access Edge Computing: A Graph
Reinforcement Learning-based Autoencoder
Nan Li, Student Member, IEEE, Alexandros Iosifidis, Senior Member, IEEE and Qi Zhang, Senior Member, IEEE
Abstract—This paper studies the computational offloading of
CNN inference in dynamic multi-access edge computing (MEC)
networks. To address the uncertainties in communication time
and computation resource availability, we propose a novel seman-
tic compression method, autoencoder-based CNN architecture
(AECNN), for effective semantic extraction and compression in
partial offloading. In the semantic encoder, we introduce a feature
compression module based on the channel attention mechanism
in CNNs, to compress intermediate data by selecting the most
informative features. In the semantic decoder, we design a
lightweight decoder to reconstruct the intermediate data through
learning from the received compressed data to improve accuracy.
To effectively trade-off communication, computation, and infer-
ence accuracy, we design a reward function and formulate the
offloading problem of CNN inference as a maximization problem
with the goal of maximizing the average inference accuracy and
throughput over the long term. To address this maximization
problem, we propose a graph reinforcement learning-based
AECNN (GRL-AECNN) method, which outperforms existing
works DROO-AECNN, GRL-BottleNet++ and GRL-DeepJSCC
under different dynamic scenarios. This highlights the advantages
of GRL-AECNN in offloading decision-making in dynamic MEC.
Index Terms—CNN inference, semantic communication, fea-
ture compression, GRL, service reliability, edge computing
I. INTRODUCTION
T
HE widespread adoption of Internet of Things (IoT)
devices, has paved the way for developing real-time
and context-aware applications, such as autonomous driving
and augmented reality. These devices generate enormous vol-
umes of data, necessitating efficient processing and inference
capabilities. However, the limited computational resources
and constrained bandwidth on IoT devices pose significant
challenges in performing local computing, especially for com-
putationally intensive convolutional neural networks (CNNs)
that require massive multiply-accumulate operations [2]. To
perform the computation-demand and memory-required CNN
inference task within a stringent deadline, a common approach
is to compress and prune CNN topology thereby reducing the
computational operations. However, over-pruning CNNs may
cause severe accuracy degradation.
This work is supported by Agile-IoT project (Grant No. 9131-00119B)
granted by the Danish Council for Independent Research. Part of this paper
is accepted by IEEE ICC 2023 [1].
N. Li, A. Iosifidis, and Q. Zhang are with the Department of Electrical and
Computer Engineering, Aarhus University, Finlandsgade 22, 8200, Denmark,
DIGIT and (email: lnzyy170320@gmail.com; ai@ece.au.dk; qz@ece.au.dk).
To mitigate this issue, edge computing has emerged as
an efficient approach, enabling IoT devices to fully offload
computational tasks (i.e., full offloading) to edge servers (ESs)
through wireless channels [3]. However, the fluctuations in
communication time caused by stochastic wireless channel
states may introduce inherent uncertainty in the communica-
tion time, resulting in varying and unpredictable communica-
tion delays [4]. In addition, the varying size of inference tasks
generated by IoT devices adds further variability, contributing
to the overall uncertainty in communication delays. Conse-
quently, the uncertainty of communication time directly affects
the available time budget for performing the computation,
which may lead to task failure when the computation cannot be
completed within the deadline. Furthermore, the computational
resources of each ES are usually shared by multiple IoT
devices, resulting in dynamic changes in resource availability
[5]. This unpredictable computation resource exacerbates the
uncertainty in computation time, further increasing the likeli-
hood of tasks failing to meet the deadline.
To strike a balance between communication and computa-
tion, dynamic offloading methods have been proposed to opti-
mize the offloading decision-making process [2], [6]. However,
when communication takes too much time or the available
computation resources at ESs are insufficient, meeting strin-
gent deadlines by running the entire pre-trained CNN model on
ES becomes challenging. As such, dynamic neural networks
such as skipping layers [7], kernel filters [8] and early-exits
[9], modify the CNN architecture thereby allowing dynamic
inference time at the expense of inference accuracy. However,
dynamic neural networks still face challenges in meeting strict
time constraints due to uncertain communication and compu-
tation time, potentially resulting in significant degradation of
inference accuracy. These limitations have driven the develop-
ment of other alternatives, among which split computing (i.e.,
partial offloading) has shown promise in striking a balance
between communication and computation [10]. However, most
existing works on split computing primarily focus on model
splitting, and less attention was paid to the compression of
intermediate feature [11].
In general, a well-trained CNN model often contains redun-
dant features that are not essential for performing an inference
task [12], and not all of these features play the same role
therein (as shown in Fig. 1), i.e., different features have
varying degrees of importance in making predictions [13].
Therefore, under poor wireless channel conditions, it is desir-
able to prune less important features to reduce communication
arXiv:2401.12167v1  [eess.IV]  19 Jan 2024
2
Original image
Feature map
Fig. 1: The 1st CL’s output feature maps in ResNet-50. The
blue one is almost useless for inference, while the red one has
enough information to be used to generate the rest.
overhead thereby meeting the deadline. This idea aligns with
the emerging paradigm of semantic communication, which
aims to extract the “meaning” of information to be transmitted
at a transmitter and successfully interpret the received semantic
information at a receiver [14]. Semantic compression can be
used to extract and utilize semantic information to compress
the intermediate tensor in the early layers in partial offloading
and optimize the communication process. For example, in
image classification tasks, not all the features but only the
local features (e.g., pixels) of the image directly relevant to the
classification are transmitted thereby reducing the communi-
cation overhead [15]. Motivated by the fault-tolerant property
of CNNs, Shao et al. [11] proposed BottleNet++, which used a
CNN-based encoder to resize the feature dimension. Similarly,
Jankowski et al. [16] proposed DeepJSCC to compress the
intermediate feature by using a CNN-based encoder. However,
directly resizing feature dimensions may compromise the
effective representation of the semantic information in the
features and result in accuracy degradation.
In wireless edge computing systems, time-varying wireless
channel states and available computing resources significantly
impact the optimal decision-making process for offloading
tasks, especially in multi-access edge computing (MEC) net-
works. In MEC, one of the major challenges is the joint
optimization of computing paradigms (i.e., local computing,
full offloading or split computing), wireless resource allocation
(e.g., transmission power, transmission size of intermediate se-
mantic information) and inference accuracy. This optimization
problem involves ternary offloading variables and is typically
formulated as a mixed integer programming (MIP) problem
[6], which can be solved using dynamic programming and
heuristic local search methods. However, these approaches
either suffer from prohibitively high computational complexity
or require a considerable number of iterations to converge to
an optimal solution, making them impractical for real-time
offloading decisions in time-varying wireless channels [6].
Reinforcement learning (RL) is a holistic learning paradigm
that interacts with the dynamic MEC to maximize long-term
rewards. Li et al. [17] proposed to use deep RL (DRL)-
based optimization methods to address dynamic computational
offloading problem. However, applying DRL directly to the
problem is inefficient in a practical deployment because it
typically requires many iterations to search an effective strat-
egy for unseen scenarios. Huang et al. [6] proposed DROO to
significantly improve the convergence speed through efficient
scaling strategies and direct learning of offloading decisions.
However, the DNN used in DROO can only handle Euclidean
data, which makes it not well suitable for the graph-like
structure data of MEC. In addition, all the above methods do
not provide dynamic inference, which is lack of flexibility in
making good use of any available computation resource under
stringent latency.
In this paper, we propose an adaptive semantic compression
technique to address the challenges associated with executing
computationally intensive CNN inference tasks in MEC. Our
approach leverages the advancements in semantic communi-
cation to achieve efficient CNN inference offloading while
maintaining inference accuracy. The main contributions are
summarized as follows:
• Semantic Encoder: We design a feature compression
module based on the channel attention (CA) method
in CNNs to quantify the importance of channels in the
intermediate tensor. By utilizing the statistics of channel
importance, we can calculate the importance of each
channel, enabling intermediate tensor compression by
pruning channels with lower importance. Furthermore,
we employ entropy encoding to remove statistical re-
dundancy in the compressed intermediate tensor, further
reducing the communication overhead.
• Semantic Decoder: We design a lightweight feature re-
covery (FR) module that employs a CNN to learn and
recover the intermediate tensor from the received com-
pressed tensor. This process enhances inference accuracy
by effectively reconstructing the compressed tensor.
• Reward Function and Optimization: We define a reward
function that strikes a balance between communication,
computation, and inference accuracy. The CNN infer-
ence offloading problem is formulated as a maximization
problem to optimize the average inference accuracy and
throughput over the long term under the constraints of
latency and transmission power.
• Graph Reinforcement Learning (GRL)-based Autoen-
coder: To address the challenges posed by stochastic
available computing resources at ESs and uncertainties in
communication time, we propose GRL-AECNN to ensure
that the inference task is completed within the given time
constraints by leveraging the capacity of reinforcement
learning and graph convolutional network (GCN).
• Performance Evaluation: We employ a step-by-step ap-
proach to fasten the training process [18]. Experimental
results demonstrate that GRL-AECNN achieves better
performance than the existing works DROO-AECNN,
GRL-BottleNet++ and GRL-DeepJSCC under different
dynamic scenarios, which demonstrates the effectiveness
3
of GRL-AECNN in offloading decision-making.
The remainder of this article is organized as follows. The
system model is presented in Section II. Section III describes
the proposed AECNN architecture for CNN inference offload-
ing. In Section IV, the CNN inference offloading problem
is modeled as a maximization problem. In Section V, GRL-
AECNN method is proposed to solve the optimization problem
The simulation results are presented and discussed in Section
VI, and the conclusions are drawn in Section VII. The nota-
tions used in this paper are listed in Table I.
II. SYSTEM MODEL
We consider a dynamic MEC network composed of U
IoT devices and S ESs, as illustrated in Fig. 2. The set of
IoT devices and ESs are denoted as U = {1, 2, · · · , U} and
S = {1, 2, · · · , S} respectively. At each timeslot k ∈ K =
{1, 2, · · · , K}, each IoT device generates a computational task
that needs to be processed within a given time constraint.
The duration of each timeslot is assumed to be constant and
denoted as τ. We mainly focuses on the image classification
task and assumes that the computational task utilizes a CNN
model Ω with L convolutional layers (CLs) and several fully-
connected (FC) layers. We denote the set of CLs as L =
{0, 1, · · · , L}, where the special layer 0 represents the initial
stage of the CNN computation. To execute the computational
task, each IoT device adheres to a ternary computational
policy, i.e., local computing, full offloading or split computing.
A. Task Model
The parameters associated with the computational tasks at
timeslot k are defined as Ik ≜ {
4
compression ratio m. Therefore, we have
X
m∈M
γk
u,m =
(
1,
αk
u,l∈L\{0,L} = 1,
0,
otherwise.
(3)
Note that compressing the intermediate data may result in a
degradation of inference accuracy. Therefore, we use ηk
u,m to
denote the achieved inference accuracy of the task generated
by u at timeslot k when employing the compression ratio m.
In general, to perform the computational task within a given
deadline, the following three decisions need to be made: to
which CL the CNN model should be split, i.e., αk
u,l; to which
ES an IoT device should offload its tasks, i.e., βk
u,s; which
compression ratio an IoT device should select, i.e., γk
u,m.
B. Communication Model
In case that IoT device offloads its entire task or intermedi-
ate tensor to an ES, the incurred transmission delay involves
delivering of entire inference task or intermediate feature map
and its inference result between IoT device and ES. Since the
output of the CNN is typically a small-sized value representing
the classification or detection result, we do not consider the
transmission delay of the feedback in this paper. Consequently,
the amount of data transmitted from IoT device u to ES s can
be described as follows:
Dk
u,s =









βk
u,sdk
u,
ak
u,0 = 1,
4βk
u,sClHlWl
P
m∈M
γk
u,mm ,
ak
u,l∈L\{0,L} = 1,
0,
ak
u,L = 1,
(4)
where Cl, Hl and Wl are the channel, height and width
dimensions of CL l’s output feature map Xl ∈ RCl×Hl×Wl,
respectively. Note that the output tensor Xl is usually in float32
data type, and the size mentioned above is measured in bytes.
We assume the uplink channel gain between IoT device u
and ES s at timeslot k is denoted as gk
u,s ∈ Gk = {gk
u,s
 ∀u ∈
U, ∀s ∈ S}, capturing the effects of path loss and shadowing
fading. Consequently, the uplink transmission data rate from
IoT device u to ES s can be expressed as:
Rk
u,s = Bk
u,s log2
 
1 + pk
u,sgk
u,s
n0Bku,s
!
,
(5)
where Bk
u,s is the channel bandwidth allocated to the link
between IoT device u and ES s, and n0 denotes the noise
power spectral density. The transmission power of IoT device
u when offloading the entire task or intermediate feature
map to ES s, pk
u,s, should not be greater than its maximal
transmission power Pu, i.e., pk
u,s ≤ Pu.
During data transmission, we do not consider the data over-
head introduced by the network protocol stack and forward
error correction. Therefore, the transmission delay of IoT
device u when offloading its entire task or intermediate feature
map to ES s can be expressed as,
tcom
u,s,k = Dk
u,s/Rk
u,s.
(6)
In terms of the energy consumption during data transmis-
sion, we do not consider the efficiency of the power amplifier
in the antenna and power consumption in the baseband circuit.
Therefore, the energy consumption incurred by IoT device u
when offloading data to ES s can be represented as
Ecom
u,s,k = tcom
u,s,kpk
u,s.
(7)
C. Computation Model
In CNN, the computation time is specific to the hardware
architecture, and can vary based on various factors, including
the device, power management techniques, memory access
patterns and etc [19]. Therefore, we employ statistical methods
to measure the computation time of each layer, and denote the
measured computation time from CL i to CL j on IoT device
u and ES s as tcmp
u,i,j and tcmp
s,i,j, respectively.
In AECNN, the feature compression module is needed only
during the training phase for the pruned CL l but not in the
inference process. Additionally, the computation time required
for the lightweight FR module at the ES is so small that can be
considered negligible in practice. Therefore, the computation
time includes the CNN computation time tcmp
u,0,l and the feature
encoding time tenc
u,l,m on the IoT device, as well as the feature
decoding time tdec
s,l,m and CNN computation time on the ES
tcmp
s,l+1,L. Correspondingly, we present the computation time of
an inference task on IoT device u and ES s as
tcmp
u,k =







0,
αk
u,0 = 1,
tcmp
u,0,l + tenc
u,l,m,
αk
u,l∈L\{0,L} = 1,
tcmp
u,0,L,
αk
u,L = 1,
(8)
and
tcmp
u,s,k =







βk
u,stcmp
s,0,L,
αk
u,0 = 1,
βk
u,s

tdec
s,l,m + tcmp
s,l+1,L

,
αk
u,l∈L\{0,L} = 1,
0,
ak
u,L = 1.
(9)
Given the energy constraints of IoT devices, we primarily
focus on investigating the energy consumption of CNN infer-
ence tasks on these energy-constrained IoT devices. According
to [20], we denote the number of floating-point operations
(FLOPs) that can be performed per Watt per second as ρ and
calculate the energy consumption on IoT device u as
Ecmp
u,l,k =
lP
i=0
ξl/ρ,
αk
u,l = 1,
(10)
where ξi represents the FLOPs count of CL i. The detailed
calculation of the FLOPs count can be found in Appendix A.
III. ARCHITECTURE OF AECNN
In this section, we first present an overview of our proposed
AECNN architecture. Next, we describe the structural com-
ponents of our designed feature compression module in the
encoder and how to compress the intermediate tensor. Finally,
we introduce the designed FR module in the decoder.
5
Splitting 
Local computing
Edge computing
Dog
CL l
CL l+1
Feature
compression Module
Encoder
Decoder
Received
fmap
Pruned CL l
Decoded
fmap
Encoded
fmap
Feature
recovery
module
Entropy
Decoding
Entropy
Encoding
Compressed
fmap
(a) AECNN architecture in device-edge co-inference system
(b) Feature compression module
(c) Feature recovery module
Output
fmap
Avgpool
Sigmoid
Channel
attention map
CL l
CL l+1
Norm
Norm
Feature compression module
Channel attention module
Pruned CL l
Maxpool
Insert for
training
Prunning
Statics 
Importance
0.9
0.5
0.3
0.2
0.1
Feature recovery module
Received
fmap
Group
Covolution
Recovered
fmap
Fig. 3: The proposed AECNN architecture in device-edge co-inference system. (a) depicts the overall framework of AECNN;
(b) shows the design of FC module; and (c) displays the designed FR module by using a CNN with group covolutional layers.
A. Overview of AECNN Architecture
Fig. 3 depicts the overall framework of our proposed
AECNN, which consists of an encoder and a decoder. In the
encoder, a CA module is designed to assess the statistical
importance of channels during inference. This enables the
pruning of the channels with low importance based on a prede-
fined compression ratio M. Subsequently, an entropy coding
module can be applied to remove the statistical redundancy in
the remaining intermediate features. Finally, the decoder first
uses the entropy decoding module to decode the received data,
and then uses the designed FR module to recover the pruned
features from the decoded features.
B. Feature compression module
The attention mechanism can effectively improve the classi-
fication performance of CNNs by enhancing the representation
of features with more important information and suppressing
unnecessary information interference [13]. Channel attention
used in CNN usually focuses on evaluating the importance of
tensor’s channels by paying attention to different channels of
the tensor. For example, for CL l, each element of the channel
attention map Wl ∈ RCl×1×1 corresponds to a channel’s
weight of the output tensor Xl. As such, the channels with
lower importance can be identified and removed, thereby
reducing the size of the intermediate tensor and reducing the
communication and computation time on the IoT device. Note
that as IoT device knows which channels will be pruned for
any pre-defined compression ratio, it only needs to compute
the retained channels of the output tensor.
In the previous designs of channel attention [13], two FC
layers are used to handle the attention weight of the chan-
nels. However, this may introduce two drawbacks. First, the
reduction of channel dimensionality for saving computation
overhead may have side effects on the prediction of channel
attention. Second, the learned channel attention by FC is
intrinsically implicit, resulting in unknowable behavior of
neuronal output. To address these issues, normalization can
yield competition or cooperation relationships among chan-
nels, using fewer computation resources while providing more
robust training performance [21]. Motivated by the above, we
design a CA module, i.e., a global max-pooling layer and a
global average-pooling layer with normalization, and insert it
into the original CNN model after the splitting point l, and
then train the resulting network to generate the importance
value of each channel, as shown in Fig. 3(b).
Since the calculation of the global avgpooling layer and
global maxpooling layer are similar, we hereby take the
avgpooling layer as an example. The aggregated features after
the avgpooling layer can be represented as
FAl = AvgPool (Xl) ,
(11)
where FAl ∈ RCl×1×1. And then the aggregated features FAl
is normalized as
FAl = FAl − µ
√
δ2 + ϵ
,
(12)
where ϵ > 0 is a small positive constant, and the parameters
µ and δ are the mean and the standard deviation of FAl,
respectively.
Then, the normalized features are subjected to element-wise
summation and sigmoid activation operation [22] to generate
the final channel attention map WAl as
WAl = sigmoid
6
T arr
u,s,k =



tcmp
u,k + tcom
u,s,k,
k = 1,
max

T arr
u,s′∈N ,k−1, (k − 1) τ + tcmp
u,k

+ tcom
u,s,k,
k ̸= 1.
(16)
tque
u,s,k = arg max
k′∈K,u′∈U





1
7
(a) Create graph 
Actor Network
GCN
Node
embedding
Experience replay buffer
Sample
mini-batch
Training
Link
embedding
(b) Actor-Critic Network 
(c) Offloading policy update  
Offloading
policy update
Offloading
action generation
Graph
Graph Action
Time
...
Graph Action
Time
Graph Action
Time
Graph Action
Time
...
Graph Action
Time
Graph Action
Time
Critic Network
Compute
...
Quantization
MEC
Fig. 4: Framework of graph reinforcement learning-based AECNN
inference accuracy. The definition of ψ(x) is introduced in
Theorem 1 and the proof is detailed in Appendix B.
Theorem 1: Let ψ(x) ≜ 2

1 − sigmoid

5x
σk
u

. For any
completion time tu,s,k and latency requirement σk
u, we have:
1) As tu,s,k approaches σk
u, ψ(tu,s,k) → 0.
2) As tu,s,k approaches 0, ψ(tu,s,k) → 1.
Accordingly, we express the average achieved reward func-
tion over a period as below:
Q (K, G, I, A) = 1
K
X
k∈K
Υ (Gk, Ik, Ak) .
(21)
The optimization problem of maximizing the average ac-
curacy and throughput of inference tasks over a period is
based on the above reward function. It is a mixed integer pro-
gramming non-convex problem that is difficult to solve with
conventional algorithms. To address this issue, we decouple it
into two subproblems, P1 is the offloading strategy:
P1 : max
K,A Q (K, G, I, A)
(22)
s.t. αk
u,l ∈ {0, 1}, ∀u ∈ U and ∀l ∈ L,
(22a)
βk
u,s ∈ {0, 1}, ∀u ∈ U and ∀s ∈ S,
(22b)
γk
u,m ∈ {0, 1} , ∀u ∈ U and ∀m ∈ M
(22c)
Once the optimal computation offloading decision A∗ =
{A∗
1, A∗
2, · · · , A∗
K} is determined, the optimization problem
is simplified to a convex optimization problem P2 to optimize
the resource allocation:
P2 : max
K
Q (K, G, I, A∗)
(23)
s.t. tu,s,k ≤ σk
u, ∀u ∈ U and s ∈ S,
(23a)
pk
u,s ≤ Pu, ∀u ∈ U and ∀s ∈ S,
(23b)
Eu,s,k ≤ Ecmp
u,k,L, ∀u ∈ U and ∀s ∈ S.
(23c)
Note that the constraint (23c) ensures that offloading is more
energy-efficient than performing the computation locally.
V. GRAPH REINFORCEMENT LEARNING-BASED AECNN
In this section, we present the framework of our proposed
GRL-AECNN to address the optimization problem described
in P1. Subsequently, we provide a comprehensive overview of
GRL-AECNN and outline the training strategy.
A. GRL-AECNN framework
In dynamic MEC networks, the data exhibits a graph-like
structure rather than a regular Euclidean format. To effectively
handle such graph data, we propose GRL-AECNN by applying
GCN [23] to analyze the characteristics of graph data through
message passing and aggregation between nodes, as shown
in Fig. 4. By learning the aggregation method based on the
relationships between nodes, GCNs can effectively process
and understand the graph-like characteristics of the data.
Additionally, GRL-AECNN can automatically filter out mes-
sages from disconnected nodes through graph data updates,
which obviates the need for retraining the aggregation function
when facing a new MEC network topology. Consequently,
the GRL-AECNN exhibits robust adaptability in handling
changes within the dynamic MEC network structure, without
necessitating extensive reconfiguration.
In the proposed GRL-AECNN framework, an actor-critic
network is used to generate offloading decisions and update
offloading policies. The actor network is responsible for pre-
dicting actions; the critic network quantifies the prediction
and generates offloading decisions; and the experience replay
buffer stores historical experiences and samples mini-batch
training data to train the GCN. Since each task can only be
split at one layer l, compressed by one compression ratio m
then offloaded to one ES s, the three-step task offload decision
for device u’s task can be merged into one step, i.e., an IoT
device has (L + 1)MS options to perform its task. In GRL-
AECNN, we model the structure information of the MEC
scenario at timeslot k as graph data Γk = (Vk, Ek), where M
devices and (L + 1)MS options are represented by the graph
vertices Vk, and each IoT device and option is connected by
a directed edge e ∈ Ek.
8
B. Actor network
In the actor network, we represent the feature of the ith
GCN layer as h(i) =
n
h(i)
v |v ∈ Vk
o
. Specially, we parse
the MEC state Γk as the initial input data h(0) for GCN.
GCN uses multiple graph convolutional layers to aggregate the
neighborhood information. For each node v ∈ Vk, we define
the neighborhood information aggregation process as follows,
h(i+1)
v
= Relu

ϖ(i+1)C

h(i)
v , A(i)
h(i)
v′

, v′ ∈ ϱv (24)
where ϖ(i+1) is the weight parameters, A(i) (·) is the aggre-
gation function, ϱv is the set of node v’s neighbors, C (·) is a
concatenate operation, Relu (·) is a non-linear function [22].
The system can acquire the information of tasks and the
status of ESs by aggregating the information in the second-
order neighborhood of nodes. For example, IoT device u
can grasp the information of its second-order neighborhood
(other IoT devices connected to ES s) through its first-
order neighborhood ES s; ES s can acquire the status of its
second-order neighborhood (other ESs) through its first-order
neighborhood (IoT devices connected to ES s). Therefore, we
use two GCN layers in GRL-AECNN, i.e., i ∈ {0, 1} in (24).
Once the information aggregation of nodes is finished, the
next step is to obtain the feature representation of edge e ∈ Ek,
he, through concatenating the features of its source node v′ ∈
Vk and destination node v′′ ∈ Vk. This process is outlined as
he = C

h(2)
v′ , h(2)
v′′

.
(25)
Then, we can classify the edges to get the relaxed offloading
action ζk = {ak,e|ak,e = F (he) , e ∈ Ek}, by the function
F (he) = sigmoid (MLP2 (Relu (MLP1 (he)))) ,
(26)
where MLP1 and MLP2 are multi-layer perceptions to extract
the feature of edge e. We use sigmoid (·) function to make the
relaxed offloading action satisfy 0 < ak,e < 1 [22].
C. Critic network
In critic network, we first use the order-preserving method
in DROO [6] to quantify the relaxed offloading action
ζk and generate N
= ULMS candidate binary offload-
ing decisions Ak =
n
ζ
(1)
k , ζ
(2)
k , · · · , ζ
(N)
k
o
, where ζ
(n)
k
=
n
a(n)
k,e|a(n)
k,e ∈ {0, 1} , e ∈ Ek
o
.
Recall that each candidate offloading action ζ
(n)
k
can achieve
reward by solving (20). Therefore, the optimal offloading
action at kth timeslot can be generated as
A∗
k = arg max
ζ
(n)
k
∈Ak
Q (K, G, I, A) .
(27)
D. Complexity Analysis and Training Strategy
1) Complexity Analysis: At timeslot k, the computational
complexity associated with the offloading decision Ak is
represented as ULMS. However, numerous IoT devices and
multiple candidate splitting points of a CNN model may cause
substantial complexity. In fact, not all the candidate splitting
points are meaningful for decision-making. As described in
Algorithm 1: Training strategy of AECNN
Input: CNN model Ω, the set of CLs L, the set of
compression ratio M, the set of training data Z.
Output: The set of AE-enhanced CNN models
Ω =

Ω2
1, · · · , ΩM
1 , · · · , ΩM
L−1
	
.
1: for l = 1 to L − 1 do
2:
Insert CA module after the splitting point l.
3:
Train the resulting network on training data Z.
4:
Calculate the importance of each channel using (14).
5:
Sort the importance of all the channels.
6:
Remove the inserted CA module.
7:
for m = 2 to M do
8:
Compress CL l by pruning Cl
9
Algorithm 2: GRL-AECNN for offloading decision-
making
Input: Input MEC state Γk, ∀k ∈ K, training interval ω.
Output: Output offloading decision A∗
k.
1: for k = 1 to K do
2:
Generate the relaxed offloading action ζk in (26).
3:
Quantify ζk into N binary actions Ak.
4:
Select the optimal offloading action A∗
k using (27).
5:
Update the experience replay buffer by adding
(Γk, A∗
k).
6:
if k mod ω = 0 then
7:
Randomly sample a mini-batch of training data ∆k
from the buffer.
8:
Train GCN and update the parameters using (28).
9:
end if
10: end for
11: return A∗
k
of GCN and reduce the averaged cross-entropy loss [6], as
ξ (∆k) = −
1
|∆k|
X
k′∈∆T
k
(1 − A∗
k′) log (1 − fI (Ek′))
+ A∗
k′ log fI (Ek′) ,
(28)
where |∆k| is the size of the mini-batch training data, ∆T
k
is the set of timeslots, ∆Γ
k is the set of graphs, and ∆A∗
k
is
the set of actions. The detailed process of GRL-AECNN is
described in Algorithm 2.
VI. PERFORMANCE EVALUATION
A. Experimental Setup
We consider an MEC network comprising of S = 2 ESs
(RTX 2080TI GPU) located at [(30m, 30m) , (90m, 30m)], and
U = 14 IoT devices (Raspberry pi 4B) randomly distributed in
the [0, 120]×[0, 60] m2 region. The bandwidth for the 2.4 GHz
WiFi connection between the IoT device and the respective ES
is set at Bk
u,s = 20 MHz, the noise power spectral density is
n0 = −174 dBm/Hz and the maximal transmission power of
each IoT device is limited to Pu = 20 dBm. Similar to [6],
we consider the free-space propagation model and express the
average channel gain as gk
u,s = ga(
3×108
4πfcϑk
u,s )de, where fc is
the WiFi frequency, ga = 2 is the antenna gain, de = 2.8
is the path loss exponent, and ϑk
u,s represents the distance
between IoT device u and ES s, measured in meters. The
wireless channel gain gk
u,s can be expressed as gk
u,s = gk
u,sgr,
where the Rayleigh small-scale fading coefficient follows gr ∼
CN(0, I). Without loss of generality, we assume that channel
gains remain consistent within a single timeslot and exhibit
independent variability from one timeslot to another.
We consider the classification task of Caltech-101 dataset
[24], consisting of approximately 9,000 images categorized
into 101 classes. Each category comprises roughly 40 to
800 images with resolutions ranging from 200 × 200 to
300×300 pixels. The diversity in resolutions aligns well with
the variability typically encountered in IoT applications. We
assume task sizes dk
m,n ranging between 20 KBytes and 100
0
10
20
30
40
50
60
No. of channels
0.0
0.2
0.4
0.6
0.8
1.0
Attention weight
data_batch1
data_batch2
data_batch3
Fig. 5: Attention weights of splitting point l = 1.
KBytes, and use the popular ResNet-50 [18] for image clas-
sification. To perform the co-inference, we split ResNet-50 at
different splitting points and compress the intermediate tensor
with different compression ratios M = {1, 2, 4, 8, 16, 32, 64}.
Since ResNet-50 introduces a branching structure with residual
blocks instead of a sequential structure, the first CL and each
residual block are considered as the candidate splitting points.
We use PyTorch to implement GRL-AECNN with the
following training parameters: the hidden neurons of two GCN
layers are 128 and 64, the learning rate is initialized as 0.001,
the experience replay buffer size is 128, the mini-batch size
|∆k| = 64; the training interval ω = 10, and the optimizer
for the loss function ξ (∆k) is the Adam function [25]. To
validate the effectiveness of GRL-AECNN, we conduct a
comprehensive evaluation comparing semantic compression
performance and CNN inference offloading efficiency. We
first compare the performance of AECNN with existing state-
of-the-art semantic compression methods, BottleNet++ [11]
and DeepJSCC [16]. In both BottleNet++ and DeepJSCC, the
intermediate tensor is encoded using a CNN-based encoder
with dimension adjustment at the final FC layer. Subsequently,
we compare the performance of GRL-AECNN with the state-
of-the-art CNN inference offloading method, DROO [6]. Our
comparative analysis involves the following three methods:
• DROO-AECNN: DROO [6] enhanced AECNN;
• GRL-BottleNet++: GRL enhanced BottleNet++ [11];
• GRL-DeepJSCC: GRL enhanced DeepJSCC [16].
B. Performance of GRL-AECNN
Measurements of attention weights in AECNN: To verify
the robustness of the statistical method for calculating the
importance of channels in AECNN, we use the same amount
of input data from different batches to calculate the importance
of each channel. We hereby take the first candidate point as an
example and calculate the importance of each channel for the
intermediate tensor by randomly sampling three batches of
input data. As shown in Fig. 5, we can see that the overall
trend of the channels’ importance is essentially consistent
across these three batches of data. This means that while
the importance of individual channels might vary depending
on the specific input data, the general ranking and trend
of the channels’ importance remains relatively stable, which
10
TABLE II
INFERENCE ACCURACY UNDER DIFFERENT COMPRESSION RATIOS OF INTERMEDIATE TENSOR AND ENTROPY ENCODING
(MEASURED ON RASPBERRY PI 4B) AND DECODING (MEASURED ON RTX 2080TI) TIME
Splitting
point
Cl × Hl × Wl
m
BottleNet++ (%)
JSCC (%)
CA Pruned (%)
AECNN (%)
C′
l
Entropy (bit)
tenc
u,l,m (ms)
tdec
s,l,m
2×
92.19(±0.22)
93.10(±0.15)
95.58(±0.24)
95.62(±0.20)
32
11.13(±0.18)
4.53
3.03
4×
91.66(±0.27)
92.06(±0.21)
95.05(±0.26)
95.30(±0.21)
16
10.48(±0.16)
3.25
2.41
l = 1
64 × 56 × 56
8×
91.49(±0.36)
91.83(±0.23)
94.57(±0.11)
94.68(±0.17)
8
9.86(±0.21)
1.94
1.52
16×
90.92(±0.23)
91.29(±0.19)
93.89(±0.54)
93.98(±0.17)
4
9.13(±0.27)
1.76
1.37
32×
89.76(±0.18)
90.76(±0.18)
92.69(±0.65)
92.70(±0.31)
2
8.62(±0.12)
1.03
0.74
64×
88.54(±0.44)
89.52(±0.29)
91.10(±0.53)
91.47(±0.25)
1
7.73(±0.32)
0.62
0.39
2×
93.28(±0.30)
94.39(±0.27)
95.43(±0.19)
95.48(±0.33)
128
12.16(±0.27)
7.89
4.76
4×
92.25(±0.17)
93.85(±0.24)
95.23(±0.25)
95.24(±0.28)
64
11.51(±0.29)
5.07
3.21
l = 2
256 × 56 × 56
8×
91.70(±0.39)
92.91(±0.31)
94.96(±0.14)
95.05(±0.25)
32
10.99(±0.24)
3.51
2.60
16×
91.64(±0.31)
92.21(±0.19)
94.80(±0.30)
94.85(±0.16)
16
10.42(±0.21)
3.00
2.35
32×
90.86(±0.28)
91.65(±0.21)
94.61(±0.21)
94.64(±0.19)
8
9.82(±0.18)
1.87
1.46
64×
90.63(±0.22)
91.04(±0.31)
93.63(±0.28)
93.78(±0.19)
4
9.19(±0.22)
1.90
1.44
demonstrates the feasibility of the statistical method we used
for calculating the importance of channels.
Inference accuracy of AECNN under various compres-
sion ratios: In partial offloading, the latency is mainly caused
by the computation and communication time on the IoT device
due to the limited resources and bandwidth. Therefore, we
should split the CNN model as early as possible (near the
input layer) to reduce the computation on the IoT device
and compress the intermediate tensor as much as possible
without compromising too much accuracy. In our experiment,
we found that the computation time on the IoT device alone
exceeds 100ms, if ResNet-50 is split at the third or later
candidate points, which is not suitable for real-time inference.
Therefore, we mainly consider the first and second candidate
points. In our experiments, the inference accuracy of the
original ResNet-50 on the test dataset is 95.84(±0.35)%.
Table II compares the inference accuracy of AECNN with
that of BottleNet++ and DeepJSCC at different compression
ratios of intermediate tensor, where ‘CA Pruned’ signifies
the pruned ResNet-50 without the FR module. We can see
that AECNN improves the accuracy of CA Pruned ResNet-
50, which demonstrates the effectiveness of the proposed FR
module. In general, higher compression will result in more
accuracy loss due to the lack of comprehensive presentation
of the features. The experimental result shows that AECNN
consistently outperforms BottleNet++ and DeepJSCC at differ-
ent compression ratios. For example, when l = 1 and m = 4,
AECNN improves the accuracy from 91.66% and 92.06% to
95.30% in comparison with BottleNet++ and DeepJSCC, re-
spectively. This improvement is attributed to AECNN’s ability
to extract informative features instead of directly compressing
the intermediate tensor as BottleNet++ and DeepJSCC do, thus
avoiding the loss of semantic information. Moreover, AECNN
achieves higher accuracy by splitting the model at the first
splitting point than the second with the same communication
overhead. For example, splitting the model at the first point,
AECNN achieves an accuracy of 93.98% when m = 16, while
that of the second point is 93.78% when m = 64. Note that
in this case, the compressed data size at the first point is
4×64×56×56/16, which is equivalent to 4×256×56×56/64
at the second point. Therefore, choosing the first split point
is a better option, in addition, it uses less computation time
and can reduce the overall task completion time. At the
first splitting point, AECNN can compress the intermediate
tensor by more than 256× (i.e., 64 × 32/7.73) using channel
pruning and entropy coding, with accuracy loss of only about
4% (i.e., 95.84% − 91.47%). In subsequent experiments, we
mainly focus on the first splitting point in partial offloading
to alleviate the computation complexity of GRL-AECNN.
Convergence of GRL-AECNN: We first define the nor-
malized reward for timeslot k as
Υ (Gk, Ik, Ak) = Υ (Gk, Ik, Ak) /Υ (Gk, Ik, A′
k)
(29)
where the action A′
k is obtained by exhaustive searching.
In Fig. 6, we characterize the convergence performance by
plotting the moving average of Υ over the most recent 50
timeslots, alongside the training loss. As the timeslot increases,
the moving average of Υ and training loss show a gradual
convergence towards the optimal solution. The occasional
fluctuations are primarily attributed to the randomness of
training data sampling. Specifically, as the timeslot increases,
the moving average of the normalized reward Υ for GRL-
AECNN consistently exceeds 0.95, while the training loss
remains consistently below 0.03. This performance superiority
is particularly evident when compared to the other three
methods. This distinction can be attributed to GRL-AECNN’s
ability to make full use of the MEC states for making
offloading decisions, thus providing an advantage over DROO-
AECNN, which only considers the wireless channel state for
decision-making. Compared to GRL-BottleNet++ and GRL-
DeepJSCC, GRL-AECNN can effectively extract the semantic
information to make better offloading decisions, resulting in
superior performance. The robust convergence, coupled with
the compelling performance metrics, reinforces the efficiency
of GRL-AECNN in optimizing offloading decision-making.
C. Performance under Various No. of IoT devices
We measured the performance of different offloading meth-
ods for 10,000 timeslots in different scenarios. We define a
11
0
600
1200
1800
2400
3000
Time slot
0.0
0.2
0.4
0.6
0.8
1.0
Normalized reward 
GRL-AECNN = 30ms
DROO-AECNN = 30ms
GRL-DeepJSCC = 30ms
GRL-BottleNet++ = 30ms
(a) Normalized reward
0
600
1200
1800
2400
3000
Time slot
0.0
0.2
0.4
0.6
0.8
Training loss
GRL-AECNN = 30ms
DROO-AECNN = 30ms
GRL-DeepJSCC = 30ms
GRL-BottleNet++ = 30ms
(b) Training loss
Fig. 6: Performance of convergence
4
6
8
10
12
14
No. of IoT devices
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Average inference accuracy
0.56
0.36
0.45
0.41
GRL-AECNN = 30ms
DROO-AECNN = 30ms
GRL-DeepJSCC = 30ms
GRL-BottleNet++ = 30ms
GRL-AECNN = 10ms
DROO-AECNN = 10ms
GRL-DeepJSCC = 10ms
GRL-BottleNet++ = 10ms
(a) Average accuracy
4
6
8
10
12
14
No. of IoT devices
30
40
50
60
70
80
90
100
Service success probability
GRL-AECNN = 30ms
DROO-AECNN = 30ms
GRL-DeepJSCC = 30ms
GRL-BottleNet++ = 30ms
GRL-AECNN = 10ms
DROO-AECNN = 10ms
GRL-DeepJSCC = 10ms
GRL-BottleNet++ = 10ms
(b) Service successful probability
4
6
8
10
12
14
No. of IoT devices
100
200
300
400
500
600
700
Average throughput (fps)
GRL-AECNN = 30ms
DROO-AECNN = 30ms
GRL-DeepJSCC = 30ms
GRL-BottleNet++ = 30ms
GRL-AECNN = 10ms
DROO-AECNN = 10ms
GRL-DeepJSCC = 10ms
GRL-BottleNet++ = 10ms
(c) Average throughput
Fig. 7: Performance under various No. of IoT devices
successful task as the one that is completed within the deadline
and use several metrics to evaluate the reliability, accuracy and
efficiency of GRL-AECNN:
• service success probability (SSP): the number of success-
ful tasks divided by the total number of tasks;
• average inference accuracy: the sum of each successful
task’s accuracy divided by the total number of tasks;
• average throughput: the number of successful tasks is
divided by the cumulative number of timeslots.
As shown in Fig. 7, the average accuracy and SSP decrease
as the number of IoT devices U increases at the given ES
computation resources. This is because when U is large, more
tasks fail to meet their deadlines due to the limited resources
of ESs. As such, the average throughput gradually reaches a
plateau. Additionally, we can see that the system achieves a
higher throughput at τ = 10 ms than that of τ = 30 ms;
however, the increase in throughput comes with an associated
trade-off, i.e., a decrease in both the average accuracy and
the SSP. This is because the higher task generation rate at
τ = 10 ms allows the system to make better use of ES’s idle
time to process more tasks during the same time duration;
however, this will result in more failed tasks because of the
higher occupancy of ES and wireless channels. Furthermore,
GRL-AECNN demonstrates the capability to enhance average
accuracy, SSP, and throughput, particularly in scenarios where
U is large. For example, when U = 10 and τ = 10 ms, GRL-
AECNN achieves average inference accuracy improvement by
0.20, 0.15, and 0.11 respectively, in comparison with DROO-
AECNN, GRL-BottleNet++, and GRL-DeepJSCC. This is
because AECNN can effectively identify the semantic infor-
mation while reducing the computation on IoT devices via
channel pruning, and GRL can use the full information of
MEC to make optimal offloading decisions; however, the se-
mantic encoders in BottleNet++ and DeepJSCC lack effective
extraction and compression of semantic information while
introducing extra computation on IoT devices.
D. Performance under Uncertain Computation Time
In real-world scenarios, ESs are often not consistently idle
and their computational resources are dynamic. To comprehen-
sively reflect the impact of such variations on the effectiveness
of offloading strategies, we consider a scenario with 14 IoT de-
vices and 2 ESs where each ES has a stochastic computational
resource availability ranging between λ% and 100% of its
overall computational capacity at each timeslot. In Fig. 8, we
compare the performance of the mentioned methods under the
case of λ ∈ {25, 50, 75, 100}. As the variation range increases,
both average inference accuracy and average throughput de-
crease. This is because insufficient computational resources
in ESs make it more likely for tasks to miss deadlines,
leading to more task failures, lower average accuracy, and
reduced throughput. Notably, GRL-AECNN achieves higher
gain over the other methods in terms of average accuracy and
average throughput under larger variations in ESs’ available
12
[100%, 100%]
[75%, 100%]
[50%, 100%]
[25%, 100%]
Various available capacity of ESs
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Average inference accuracy
GRL-AECNN = 30ms
DROO-AECNN = 30ms
GRL-DeepJSCC = 30ms
GRL-BottleNet++ = 30ms
GRL-AECNN = 10ms
DROO-AECNN = 10ms
GRL-DeepJSCC = 10ms
GRL-BottleNet++ = 10ms
(a) Average accuracy
[100%, 100%]
[75%, 100%]
[50%, 100%]
[25%, 100%]
Various available capacity of ESs
0
100
200
300
400
500
600
700
800
Average throughput (fps)
GRL-AECNN = 30ms
DROO-AECNN = 30ms
GRL-DeepJSCC = 30ms
GRL-BottleNet++ = 30ms
GRL-AECNN = 10ms
DROO-AECNN = 10ms
GRL-DeepJSCC = 10ms
GRL-BottleNet++ = 10ms
(b) Average throughput
Fig. 8: Performance under various available capacities of ESs
[75%, 100%]
[75%, 100%]+fluc. [50%, 100%]+fluc. [25%, 100%]+fluc.
Various available capacity of ESs
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Average inference accuracy
GRL-AECNN = 30ms
DROO-AECNN = 30ms
GRL-DeepJSCC = 30ms
GRL-BottleNet++ = 30ms
GRL-AECNN = 10ms
DROO-AECNN = 10ms
GRL-DeepJSCC = 10ms
GRL-BottleNet++ = 10ms
Fig. 9: Performance under uncertain computation time
0
1
2
3
4
5
Uncertainty bound of CSI imperfections 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Average inference accuracy
GRL-AECNN = 30ms
DROO-AECNN = 30ms
GRL-DeepJSCC = 30ms
GRL-BottleNet++ = 30ms
GRL-AECNN = 10ms
DROO-AECNN = 10ms
GRL-DeepJSCC = 10ms
GRL-BottleNet++ = 10ms
Fig. 10: Performance under imperfect CSI
computational resources. For example, at λ = 25 and τ = 10
ms, GRL-AECNN improves average inference accuracy up
to (0.189 − 0.019)/0.019 = 8.9× over DROO-AECNN and
(0.189 − 0.074)/0.074 = 1.6× over GRL-DeepJSCC respec-
tively, which is higher than that of (0.302 − 0.157)/0.157 =
0.9× and (0.302 − 0.208)/0.208 = 0.5× when λ = 75 and
τ = 10 ms. Furthermore, as the variation range increases
(i.e., smaller λ), the three GRL-based offloading methods
show lower degradation in average inference accuracy and
average throughput than DROO-AECNN. For example, GRL-
AECNN has (0.398 − 0.189)/0.398 = 0.5× degradation
in average inference accuracy from λ = 100 to λ = 25
when τ = 10 ms; however, that of DROO-AECNN up to
(0.277 − 0.019)/0.277 = 0.9×. This highlights the effec-
tiveness of GRL-AECNN in offloading decision-making for
scenarios with limited available computation resources.
The computation time of ESs can be affected by various
factors, including storage availability, thermal conditions, and
environmental factors. In addition to the previously men-
tioned variation in ES computation resources, we considered
a realistic case where the computation time of each ES
fluctuates by ±25% of its measured value. In Fig. 9, the three
GRL-based offloading methods remain relatively more stable
in average inference accuracy than DROO-AECNN under
computation time fluctuations. For example, GRL-AECNN
has (0.384 − 0.349)/0.349 = 0.1× degradation in average
inference accuracy when λ = 75 and τ = 10 ms; however, that
of DROO-AECNN is (0.258 − 0.157)/0.157 = 0.64×. This
further demonstrates that GRL-AECNN is better at learning
the state information of ESs for effective decision-making.
E. Performance under Imperfect Channel State Information
Since channel estimation is often not perfect in practical
systems, the channel state information (CSI) imperfections
can be deterministically modeled by using the ellipsoidal
approximation [26], as
ˆgk
u,s = gk
u,s10ϑk
u,s/10, ϑk
u,s ∈ [−ε, ε].
(30)
where the non-negative constant ε denotes the uncertainty
bound of CSI imperfections.
In this study, we consider the scenario with 14 IoT devices
and 2 ESs, and include the CSI imperfections under different
uncertainty bound ε. As shown in Fig. 10, the average infer-
ence accuracy of the system decreases as the uncertainty bound
of CSI imperfections increases. This occurs because the task
offloading decisions made at larger biases in imperfect channel
estimation might lead to tasks not being completed within
their specified deadlines. Consequently, this situation causes
more task failures, contributing to an overall decrease in the
average inference accuracy. Nonetheless, our proposed GRL-
AECNN has a relatively small degradation in terms of average
13
inference accuracy. For example, when τ = 10 ms, GRL-
AECNN has (0.398 − 0.380)/0.380 = 0.05× degradation in
average inference accuracy from ε = 0 to ε = 5; however,
that of DROO-AECNN up to (0.277−0.187)/0.187 = 0.48×.
This highlights the effectiveness of GRL-AECNN in aggregat-
ing the CSI imperfections thereby making robust offloading
decisions to ensure that more tasks are processed within the
deadline in dynamic MEC scenarios.
VII. CONCLUSION
In this paper, we studied the computation offloading of CNN
inference tasks in dynamic MEC networks. We proposed a
novel semantic compression method, AECNN, to address the
uncertainties in communication time and available computa-
tion resources at ESs. In AECNN, we designed a CA module
to figure out the importance of channels, then compressed the
intermediate tensor by pruning the less important channels.
We used entropy encoding to further reduce communication
time by removing redundant information and designed a
lightweight CNN-based FR module to recover the interme-
diate tensor through learning from the received compressed
tensor to improve accuracy. We designed a reward function
to trade off the inference accuracy and task completion time,
and formulated the CNN inference offloading problem as
a maximization problem to maximize the average inference
accuracy and throughput in the long term. To address the
optimization problem, we proposed GRL-AECNN to make the
optimal offloading decision and use a step-by-step approach
to fasten the training process. The experimental results show
that GRL-AECNN can achieve better performance in terms
of average inference accuracy, service success reliability, and
average throughput than the state-of-the-art methods, which
highlights the effectiveness of GRL-AECNN in aggregating
all the information of dynamic MEC thereby making robust
offloading decisions.
REFERENCES
[1] N. Li, A. Iosifidis, and Q. Zhang, “Attention-based feature compression
for cnn inference offloading in edge computing,” in ICC 2023 - IEEE
International Conference on Communications, 2023, pp. 967–972.
[2] S. Guo, B. Xiao, Y. Yang, and Y. Yang, “Energy-efficient dynamic
offloading and resource scheduling in mobile cloud computing,” in IEEE
INFOCOM 2016, pp. 1–9.
[3] J. Liu and Q. Zhang, “To improve service reliability for ai-powered time-
critical services using imperfect transmission in mec: An experimental
study,” IEEE Internet of Things Journal, vol. 7, no. 10, pp. 9357–9371,
2020.
[4] T. X. Tran and D. Pompili, “Joint task offloading and resource allocation
for multi-server mobile-edge computing networks,” IEEE Transactions
on Vehicular Technology, vol. 68, no. 1, pp. 856–868, 2019.
[5] Z. Chang, L. Liu, X. Guo, and Q. Sheng, “Dynamic resource alloca-
tion and computation offloading for iot fog computing system,” IEEE
Transactions on Industrial Informatics, vol. 17, no. 5, pp. 3348–3357,
2021.
[6] L. Huang, S. Bi, and Y.-J. A. Zhang, “Deep reinforcement learning
for online computation offloading in wireless powered mobile-edge
computing networks,” IEEE Transactions on Mobile Computing, vol. 19,
no. 11, pp. 2581–2593, 2020.
[7] X. Wang, F. Yu, Z.-Y. Dou, T. Darrell, and J. E. Gonzalez, “Skipnet:
Learning dynamic routing in convolutional networks,” in Proceedings
of the European Conference on Computer Vision (ECCV), September
2018.
[8] X. Gao, Y. Zhao, Łukasz Dudziak, R. Mullins, and C. zhong Xu,
“Dynamic channel pruning: Feature boosting and suppression,” in In-
ternational Conference on Learning Representations, 2019.
[9] S. Teerapittayanon, B. McDanel, and H. Kung, “Branchynet: Fast
inference via early exiting from deep neural networks,” in 2016 23rd
International Conference on Pattern Recognition (ICPR), 2016, pp.
2464–2469.
[10] H.-J. Jeong, I. Jeong, H. J. Lee, and S. M. Moon, “Computation offload-
ing for machine learning web apps in the edge server environment,”
in IEEE International Conference on Distributed Computing Systems
(ICDCS), 2018.
[11] J. Shao and J. Zhang, “Bottlenet++: An end-to-end approach for fea-
ture compression in device-edge co-inference systems,” in IEEE ICC
Workshops, 2020, pp. 1–6.
[12] K. Han, Y. Wang, Q. Tian, J. Guo, C. Xu, and C. Xu, “Ghostnet: More
features from cheap operations,” in IEEE/CVF CVPR, June 2020.
[13] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional block
attention module,” in ECCV, September 2018.
[14] X. Luo, H.-H. Chen, and Q. Guo, “Semantic communications: Overview,
open issues, and future research directions,” IEEE Wireless Communi-
cations, vol. 29, no. 1, pp. 210–219, 2022.
[15] B. Juba and S. S. Vempala, “Semantic communication for simple goals
is equivalent to on-line learning,” in Proceedings of 22nd International
Conference on Algorithmic Learning Theory ALT, vol. 6925, 2011, pp.
277–291.
[16] M. Jankowski, D. G¨und¨uz, and K. Mikolajczyk, “Wireless image re-
trieval at the edge,” IEEE Journal on Selected Areas in Communications,
vol. 39, no. 1, pp. 89–100, 2021.
[17] J. Li, H. Gao, T. Lv, and Y. Lu, “Deep reinforcement learning based
computation offloading and resource allocation for mec,” in IEEE WCNC
2018, pp. 1–6.
[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in IEEE/CVF CVPR, June 2016.
[19] R. Desislavov, F. Mart´ınez-Plumed, and J. Hern´andez-Orallo, “Compute
and energy consumption trends in deep learning inference,” CoRR, vol.
abs/2109.05472, 2021. [Online]. Available: https://arxiv.org/abs/2109.
05472
[20] V. Weaver, “The gflops/w of the various machines in the vmw re-
search group,” [EB/OL], https://web.eece.maine.edu/∼vweaver/group/
green machines.html Accessed June 17, 2023.
[21] Z. Yang, L. Zhu, Y. Wu, and Y. Yang, “Gated channel transformation
for visual recognition,” in IEEE/CVF CVPR, June 2020.
[22] G. Alcantara, “Empirical analysis of non-linear activation functions for
deep neural networks in classification tasks,” arxiv:1710.11272, 2017.
[23] M. Niepert, M. Ahmed, and K. Kutzkov, “Learning convolutional neural
networks for graphs,” in ICML 2016, pp. 2014–2023.
[24] F.-F. Li, M. Andreeto, M. Ranzato, and P. Perona, “Caltech 101,” Apr
2022. [Online]. Available: https://data.caltech.edu/records/20086
[25] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” in ICLR 2015,May 7-9.
[26] K. Papadaki and V. Friderikos, “Robust scheduling in spatial reuse tdma
wireless networks,” IEEE Transactions on Wireless Communications,
vol. 7, no. 12, pp. 4767–4771, 2008.
[27] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning filters
for efficient convnets,” in 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017.
[28] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, “Pruning
convolutional neural networks for resource efficient inference,” in 5th
International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017.
APPENDIX A
FLOPS COUNT IN CNN
In CNNs, the input feature map of a CL is derived from
the output feature map of its preceding CL. For instance, let’s
consider CL l, which takes the input feature map denoted as
Xl−1 ∈ RCl−1×Hl−1×Wl−1 and produces the output feature
map Xl
∈ RCl×Hl×Wl. The computation of FLOPs for
CL l captures the computational complexity associated with
processing data through CL l.
According to the work [27], the FLOPs calculation of CL
l is primarily influenced by the dimensions of the input and
14
output channels, the kernel size employed by the convolution
operation, and the dimensions of the resulting output height
and width. As such, the FLOPs for CL l are calculated using
the following formula from [27]:
ξl = Cl−1Clf 2
l HlWl,
∀l ∈ L,
(A1)
where fl is the kernel size of CL l.
Regarding the FC layer, the computational operations pri-
marily consist of multiplying the input data by weight parame-
ters and then applying activation functions. We consider an FC
layer l′ with the input feature map X in
l′ ∈ R1×Din
l′ and output
feature map X out
l′
∈ R1×Dout
l′ . According to [28], the FLOPs of
an FC layer l′ on can be calculated as
ξl′ = Din
l′Dout
l′ ,
(A2)
where Din
l′ and Dout
l′ are the input dimensionality and the output
dimensionality of FL l′.
APPENDIX B
PROOF OF THEOREM 1
To prove Theorem 1, we first introduce the hyperbolic
tangent function sigmoid(x) =
ex
ex+1. The function sigmoid(x)
is a nonlinear function with values between 0 and 1 [22]. As
shown in Fig. B1, sigmoid(x) approaches 1 as x → 5, while
sigmoid(x) approaches 1
2 as x → 0.
5
0
5
x 
0.0
0.2
0.4
0.6
0.8
1.0
Sigmoid(x)
Fig. B1: sigmoid(x) function
Based on the above, we define a new function ψ′(x) =
2 (1 − sigmoid(x)) and derive that ψ′(x) approaches 0 as x ap-
proaches 5 and approaches 1 as x approaches 0. Extending this
to our original function ψ(x) = 2

1 − sigmoid

5x
σk
u

, we
can deduce that ψ(x) behaves similarly: as x approaches σk
u,
ψ(x) approaches 0; and as x approaches 0, ψ(x) approaches
1.
"
"Clustering is a commonly used unsupervised image segmentation task, where each pixel is assigned a specific label. However, conventional clustering methods often achieve unsatisfactory performance due to the lack of labeled samples for training. This work proposes a novel approach based on nonlinear canonical correlation analysis to address the semi-supervised setting, where only a small fraction of pixels are labeled. We first generate a high-dimensional feature vector consisting of multiple spatial features, including cell, local binary pattern, and gray-level co-occurrence matrices, for each pixel. Then, t-SNE embedding is employed for dimensionality reduction and to approximate the nonlinear shapes of clusters. Finally, a modified canonical correlation analysis algorithm, called RBF-CCA, is proposed. It leverages the labeled data to learn the associated projection matrix and key canonical variables, enabling effective cluster identification. Extensive experiments on remote sensing images demonstrate the superiority of our approach, achieving excellent segmentation performance and robust handling of challenging land-cover scenarios.","Image segmentation is a crucial task in computer vision where the goal is to partition an image into meaningful regions with labeled (supervised) or unlabeled (unsupervised) training samples. However, obtaining dense image annotations for each pixel is often challenging, particularly for remote sensing images with complex scenes containing various categories, like urban areas. Consequently, unsupervised image segmentation methods have been actively pursued, but these often result in inconsistent partitions due to the lack of correspondence between samples and regions. To address this challenge, semi-supervised approaches have gained popularity as they only require a small number of labeled samples to achieve relatively high performance. This study formulates segmentation as a semi-supervised clustering problem and employs a novel algorithm called RBF-CCA (Radial Basis Function - Canonical Correlation Analysis) for the purpose of image segmentation. Specifically, we leverage t-SNE (t-distributed stochastic neighbor embedding) to obtain a lower-dimensional embedding of high-dimensional texture feature vectors, derived from various local properties, for each pixel. Then, the proposed RBF-CCA algorithm, which utilizes a set of labeled data input features, is applied to the t-SNE embedding for nonlinear clustering. The output canonical variables are fed to a k-means clustering algorithm for final segmentation.","The concept of image segmentation using clustering approaches has been extensively explored, and various learning paradigms have been developed. Graph-based methods, such as spectral clustering, have been widely utilized due to their effectiveness in exploiting spatial relationships between pixels. However, purely unsupervised spectral image segmentation often fails to produce satisfactory results due to the absence of labeled information. To enhance clustering outcomes, extra information is sometimes provided in the clustering output space. Semi-supervised clustering methods, which combine complementary information to supervise the cluster learning process, have emerged as a promising approach. Previous works have explored different strategies for semi-supervised clustering, such as incorporating spectral graph construction, multiscale combinatorial grouping algorithms, structured sparse subspace clustering, and multi-view spectral clustering. Despite these advancements, most existing methods may still produce suboptimal results due to insufficient correspondence between samples and regions or the inability to effectively capture the nonlinear relationships in image data.nan","The proposed method in this study consists of four key steps: (1) Pixel-based Multiple Spatial Features Generation: Each pixel in a multi-band remote sensing image is mapped to a feature vector comprising multiple spatial features extracted from cell, local binary pattern, and gray-level co-occurrence matrices. These features collectively provide comprehensive information about the local characteristics of the pixel. (2) Proposed Semi-supervised Spectral Clustering Algorithm Using RBF-CCA: Given a set of high-dimensional feature vectors, t-SNE embedding is employed to reduce dimensionality and obtain a lower-dimensional representation of the data. Subsequently, a modified canonical correlation analysis algorithm, referred to as RBF-CCA, is introduced. RBF-CCA utilizes labeled data as input features and learns the associated projection matrix via the canonical correlation analysis framework. This results in the calculation of canonical variables that characterize the relationships between the nonlinear RBF transform based on t-SNE clusters and the cluster labels. Spectral clustering, using the obtained canonical variables, is then performed to assign cluster labels to each pixel. (3) Clustering: After obtaining the canonical variables from RBF-CCA, standard k-means clustering is applied to group similar points together, effectively segmenting the image into regions with shared characteristics.","The effectiveness of the proposed semi-supervised spectral clustering algorithm based on RBF-CCA and t-SNE is demonstrated through comprehensive experiments on two sets of multi-band remotely sensed images. For evaluation purposes, each data set is processed by splitting into sub-images, and segmentation performance is assessed for each sub-image. Additionally, the final segmentation results are obtained by merging the segmented sub-images. The accuracy of the proposed method is compared with various baseline approaches, including k-means algorithms applied to multiple features, t-SNE embedding, as well as linear, polynomial, and proposed RBF variants of canonical correlation analysis. Quantitative analysis using intersection over union (IOU) and qualitative visual inspection of segmentation results confirm the superior performance of the proposed RBF-CCA algorithm in identifying and delineating semantically meaningful regions, such as buildings, trees, impervious surfaces, and low vegetation.","In this work, we address the challenging task of semi-supervised image segmentation for remote sensing imagery by introducing a novel algorithm that combines nonlinear canonical correlation analysis with t-SNE embedding. The proposed method leverages multiple spatial features and local texture information to represent each pixel in a high-dimensional feature space. By utilizing t-SNE for dimensionality reduction and RBF-CCA for nonlinear clustering, we are able to effectively capture the complex relationships between pixels and assign labels to each region. Extensive experiments on various land-cover scenarios demonstrate the effectiveness and robustness of the proposed approach in producing accurate segmentation results, outperforming conventional unsupervised and semi-supervised methods. Future research directions include extending the algorithm for handling specific land-cover scenarios, optimizing the algorithm for robustness in challenging situations, and applying it to other image data modalities such as sea surfaces covered by ice and clouds.",Semi-supervised segmentation of land cover images using nonlinear canonical correlation analysis with multiple features and t-SNE,"Hong Wei, James Xiao, Yichao Zhang, Xia Hong","1
Semi-supervised segmentation of land cover images
using nonlinear canonical correlation analysis with
multiple features and t-SNE
Hong Wei, James Xiao, Yichao Zhang, and Xia Hong
Abstract
Image segmentation is a clustering task whereby each pixel is assigned a cluster label. Remote sensing data usually consists
of multiple bands of spectral images in which there exist semantically meaningful land cover subregions, co-registered with other
source data such as LIDAR (LIght Detection And Ranging) data, where available. This suggests that, in order to account for
spatial correlation between pixels, a feature vector associated with each pixel may be a vectorized tensor representing the multiple
bands and a local patch as appropriate. Similarly, multiple types of texture features based on a pixel’s local patch would also be
beneficial for encoding locally statistical information and spatial variations, without necessarily labelling pixel-wise a large amount
of ground truth, then training a supervised model, which is sometimes impractical. In this work, by resorting to label only a small
quantity of pixels, a new semi-supervised segmentation approach is proposed. Initially, over all pixels, an image data matrix is
created in high dimensional feature space. Then, t-SNE projects the high dimensional data onto 3D embedding. By using radial
basis functions as input features, which use the labelled data samples as centres, to pair with the output class labels, a modified
canonical correlation analysis algorithm, referred to as RBF-CCA, is introduced which learns the associated projection matrix
via the small labelled data set. The associated canonical variables, obtained for the full image, are applied by k-means clustering
algorithm. The proposed semi-supervised RBF-CCA algorithm has been implemented on several remotely sensed multispectral
images, demonstrating excellent segmentation results.
Index Terms
canonical correlation analysis, land cover, radial basis function, remote sensing, semi-supervised image segmentation, t-SNE
embedding, vectorized features.
I. INTRODUCTION
Image segmentation is an essential task in computer vision to partition an image into semantically meaningful regions with
labelled (supervised) or without labelled (unsupervised) training samples. Obtaining dense image annotations pixel-wise is
particularly difficult and expensive for remotely sensed images with complex scenes containing multiple categories, such as
urban areas. It is desirable to utilise unsupervised image segmentation, which is a longstanding challenge in computer vision.
Over the years, numerous methods have been developed to tackle the problem, from early approaches of clustering using
statistical and mathematical models [1], [2], graph cut-related methods [3]–[5], evidence-based approaches [6], [7], to more
advanced deep learning techniques [8], [9]. Researchers also attempt to adapt domain knowledge into deep learning based
unsupervised image segmentation [10]–[12] or use image labels in pixel-level segmentation [13]. However, these methods may
be highly inconsistent with the true partition due to the lack of correspondence between samples and regions. To avoid using
a large amount of labelled samples, semi-supervised approaches for image segmentation become popular as a research area
[14], in which only a small set of labelled samples is needed to achieve a relatively high performance.
In this study, the segmentation problem is formulated as a semi-supervised clustering approach, over the 3-D t-SNE
embedding [15] derived from a high-dimensional feature space mapped from pixels. As illustrated in Figure 1, given a set of
co-registered images, any pixel in each band of original data is mapped onto a feature space, composed of a vectorized local
patch, together with a set of image textural feature vectors [16], [17] also based on local patches centred at the pixel. The
t-distributed stochastic neighbor embedding (t-SNE) algorithm, proposed by van der Maaten and Hinton, outlined in Section
III-A, is a state-of-art technique for dimensionality reduction and visualization for a wide range of applications [18]. Cai and
Ma investigated the theoretical foundations of t-SNE, which can provide theoretical guidance for its application and help select
its tuning parameters for various applications [19]. This is the foundation to build the semi-supervised clustering method, which
aims to utilise a limited number of labelled data to predict a cluster’s label in unlabelled data by training a clustering model
using both unlabelled and labelled data.
The segmenting process is based on a novel algorithm, referred to as RBF-CCA (Radial Basis Function - Canonical
Correlation Analysis), with data from t-SNE embedding of pixel based image textural feature vectors. CCA was originally
proposed by Hotelling in the seminal works [20], [21]. It finds the best predictor among the linear functions from each set
Hong Wei, Yichao Zhang, and Xia Hong are with the School of Mathematical, Physical and Computational Sciences, University of Reading, Reading, RG6
6AY, UK. (h.wei/x.hong@reading.ac.uk, yichao.zhang@pgr.reading.ac.uk)
James Xiao is an independent researcher, Vancouver, BC, Canada, zifan.james.xiao@gmail.com.
arXiv:2401.12164v1  [cs.CV]  22 Jan 2024
2
Fig. 1. Illustrative diagram of constructing data X for t-SNE then RBF-CCA from a remote sensing image Ifull, where each pixel is mapped onto a feature
vector consisting of multiple spatial features (for details, see Section IV-B).
by maximizing the correlation coefficient between two sets, using spectral analysis. In an attempt to increase the flexibility of
CCA for nonlinear relationships between two random variables, kernel CCA has been introduced [22]. The proposed semi-
supervised clustering algorithm can be viewed as a generalized spectral clustering algorithm, due to that CCA is based on
eigen-decomposition of data matrices.
Our contributions to knowledge are listed below:
1) A novel pixel-based multiple spatial feature vector has been proposed in order to fully exploit spatially-resolved multi-
band image self-description for remotely sensed land cover imagery.
2) The proposed algorithm operates over 3D embedding via t-SNE aimed at clustering of high dimensional data sets
generated by a novel pixel based feature vector.
3) In order to provide approximation of the nonlinear shapes of t-SNE clusters, radial basis functions (RBF) using labelled
data input features, are employed as the first set of variable of RBF-CCA, the class labels become indicative variables
as the second set of variables in CCA.
4) The proposed algorithm transfers knowledge from a small set of labelled data, via projection of the associated canonical
variables, to a large quantity of unlabelled data, to which k-means clustering is applied as used by a spectral clustering
algorithm [23].
5) We discover that the resulting final clusters are capable of matching extremely well to semantically meaningful image
regions, such as buildings, trees, impervious surface, and low vegetation.
The rest of the paper is organized as follows. The related work with literature review is presented in Section II. Section III
introduces mathematical methods of t-SNE embedding and canonical correlation analysis. Section IV initially introduces the
proposed pixel-based multiple spatial feature generation from multi-band land-cover images, followed by the proposed semi-
supervised algorithm based on t-SNE and RBF-CCA for segmentation. In Section V, experiments and results are presented
and discussed. Finally, Section VI concludes the work.
II. RELATED WORK
The concept of image segmentation by using clustering approaches has been explored over time. Different learning approaches
are developed, such as graph-based methods [3], [24]–[28], where spectral clustering algorithms are heavily exploited. Data
clustering, including spectral clustering for image segmentation, is an unsupervised classification paradigm which divides
observed data into different subsets (clusters). Yet, purely unsupervised spectral image segmentation often fails to achieve
satisfactory performance due to that no truth labels are used for training. In some cases, extra information is provided in the
clustering output space to enhance the clustering results. Clustering algorithms that combine complementary information to
supervise the cluster learning process are called semi-supervised clustering methods [29], [30]. For example, Kim, et al. [31],
adopted a semi-supervised learning strategy in developing an efficient algorithm for image segmentation by employing spectral
clustering in constructing a sparse multilayer graph with nodes of pixels and over-segmented regions from the image.
In [32], Pont-Tuset, et al. developed a fast normalized cuts algorithm as the initial step to group pixels. They then proposed a
multiscale combinatorial grouping algorithm to perform a hierarchical search for similar regions across different scales. In their
research, scaled images were used to generate ultrametric contour maps in the segmentation process. Structured sparse subspace
3
clustering was proposed by Li et al. [33], in which each data point is expressed as a structured sparse linear combination of
all other data points. Then, a joint optimisation framework was applied for learning both the affinity learned from the data
and the segmentation implemented by spectral clustering. The concept of multi-view spectral clustering was applied in [34]
to deal with an NP-hard optimization problem which occurs in conventional spectral clustering approaches. The developed
framework was evaluated for image segmentation along with other applications. In [35], Jiao et al. applied spectral clustering
to high-level features, which were outputs from an autoencoder network with its input a set of low-level image features over
pre-generated non-overlapping superpixels. In their work, the low-level image features include colour, gradient, local binary
pattern, and saliency features. With the unsupervised nature, the evaluation on two datasets showed IOU (Intersection over
Union) about 0.6.
The concept of few shots learning (FSL) is proposed [36], [37] which refers to the learning from a limited number of
samples with supervised information. This learning paradigm is desired since large-scale supervised data can be expensive in
many applications, e.g. in semantic segmentation to associate a label or category with every pixel in an image [38]. For remote
sensing image segmentation, Chen et al. attempted using semi-supervised approach to build a spectral-spatial classifier with
affinity scoring to segment hyperspectral imagery [39], and Jiang et al. developed a semi-supervised segmentation method, in
which superpixels are generated to construct the graph in a graph convolutional network for sea ice classification from remotely
sensed synthetic aperture radar (SAR) imagery [40].
Differently from the previous works, our study, categorised as a semi-supervised clustering approach, transforms high-
dimensional image feature vectors into a 3D space by using t-SNE, and then a small set of labelled pixels is applied to
RBF-CCA, with unlabelled pixels finally being integrated in the resulting canonical spectral space to assign labels to each
pixel.
III. PRELIMINARY
For ease of exposition, mathematical background is briefly outlined on t-SNE embedding [15] in Section III-A and canonical
correlation analysis [41] in Section III-B.
A. t-distributed stochastic neighbour embedding (t-SNE)
Given a set of N high-dimensional X = {xi}, i = 1, ..., N. As a tool for visualizing high-dimensional data by giving
each data point a location in a two or three-dimensional map, the t-SNE [15] aims to learn a very low m-dimensional map
Y = {yi ∈ ℜm}, i = 1, ..., N, with m typically chosen as 2 or 3.
The locations of the points yi in the map are determined by minimizing the (non-symmetric) Kullback–Leibler divergence
of the distribution of the pairwise similarity probabilities P (of X) from the distribution Q (of Y ):
KL(P∥Q) =
X
i̸=j
log pij
qij
(1)
where pij is joint probabilities of similarity {xj, xi}, qij is joint probabilities of similarities of {yj, yi}.
To this end, the similarity of two distinctive xj and xi is the conditional probability, pj|i, defined as:
pj|i =
exp
4
B. Canonical correlation analysis
Canonical correlation analysis (CCA) is a way of measuring the linear relationship between two sets of multidimensional
variables. Consider two sets of random variables Φ = {ϕi}, i = 1, ..., d1 and Ψ = {ψi}, i = 1, ..., d2 with zero mean. It is
assumed that Φ, Ψ are full rank, and d = min(rank(Φ), rank(Ψ)).
The total covariance matrix
C =
 CΦΦ
CΦΨ
CΨΦ
CΨΨ

= E
"" Φ
Ψ
  Φ
Ψ
T#
(5)
Define a set of two projection matrices A = [a1, ..., ad] ∈ ℜd1×d and B = [b1, ..., bd] ∈ ℜd2×d, which generate a set of
linear combinations named U = ΦA = [u1, ...ud] and V = ΨB = [v1, ...vd]. Each member of U is paired with a member
of V , as a set of canonical variables pairs (ui, vi).
The task in CCA is to find A, B such that the correlations ρi(ui, vi) are maximized. Represent
ρi =
aT
i CΦΨbi
p
aT
i CΦΦai
q
bT
i CΨΨbi
,
i = 1, ..., d.
(6)
Equivalently,
max
ai,bi ρi = aT
i CΦΨbi, ∀i
(7)
subject to aT
i CΦΦai = 1, bT
i CΨΨbi = 1.
To obtain the CCA solution, initially define [41]
K = C−1/2
ΦΦ CΦΨC−1/2
ΨΨ
(8)
and perform singular value decomposition of K as
K = ΓΛ∆T
(9)
with Γ = [γ1, ..., γd], ∆ = [δ1, ..., δd]. Λ = diag{λ1/2
1
, ..., λ1/2
d
}. and λ1 ≥ λ2 ≥ ...λd are the nonzero eigenvalues of KT K. γi
and δi are the left and right eigenvectors of K.
Now define
ai = C−1/2
ΦΦ γi
bi = C−1/2
ΨΨ δi
(10)
so that
cov(ui, uj) = aT
i CΦΦaj = γT
i γj =
 1
i = j
0
i ̸= j
(11)
cov(vi, vj) = bT
i CΨΨbj = δT
i δj =

1
i = j
0
i ̸= j
(12)
and the correlation between ui and vi has maximal of
ρ(ui, vi) = γT
i ΓΛ∆T δi = λ1/2
i
.
(13)
IV. METHOD
This section starts with detailing the proposed pixel based multiple spatial features generated from multi-band land cover
images in Section IV-A. The proposed semi-supervised RBF-CCA algorithm for segmentation is presented in Section IV-B
based on t-SNE embedding produced from the full image feature space, as well as a few semantic labels.
A. Pixel based multiple spatial features generation
Given a multi-band remote sensing image Ifull ∈ ℜH×W ×M, where H, W, M denote the height, width and number of
grayscaled bands in Ifull. Without loss of generality, each grayscaled image is denoted as Im, m = 1, ..., M and each pixel
is denoted as Im(x, y).
1) Cell feature vector: A cell Cx,y is simply a square patch centered at the current pixel, I(x, y). For example, a 3 by 3
cell is
Cx,y =


I(x − 1, y − 1)
I(x − 1, y)
I(x − 1, y + 1)
I(x, y − 1)
I(x, y)
I(x, y + 1)
I(x + 1, y − 1)
I(x + 1, y)
I(x + 1, y + 1)


(14)
For a predetermined cell size Nc by Nc, a cell feature vector f cell(x, y) ∈ ℜN 2
c is simply to reshape the patch centered at
I(x, y) into a vector. Note that if a pixel is near the edge of Im, the associated cell matrix Cx,y is created by an enlarged
image from Im with auxiliary pixels. This is done by mirroring the image along the edge to the outside, so that the image is
symmetrical with across its edge. The cell size Nc = 7 is used in this work.
5
2) Local binary pattern feature vector: Similarly, a square patch Pat(x, y) centered at the current pixel I(x, y) is considered
for generating local binary pattern (LBP) features [17]. For each pixel in the patch, compare the pixel to each of its 8 neighbours
I(xi, yi) as indexed by i = 1, ..., 8, on its top, left-top, left-middle, left-bottom, bottom, right bottom, right middle, right-top,
which follows the pixels along a complete anti-clockwise circle. The resulting LBP value is
LBP(x, y) =
8
X
i=1
Id [I(xi, yi) ≥ I(x, y)] ∗ 2i−1
(15)
where the indicator function Id(S) is defined as one if the statement S is true, zero otherwise. LBP(x, y) is an integer ranged
with [0, 28 − 1]. Furthermore, motivated by the fact that only a subset of 58 binary patterns, named uniform LBP patterns,
accounts for the majority of LBP patterns, the set of uniformed LBP patterns is reduced from 256 to 58 + 1 = 59, where all
non-uniformed LBP patterns merges as one additional pattern [42].
Over a patch, the histogram of the uniformed LBP can be generated, which may readily be used as features for pixel I(x, y).
In order to obtain more LBP samples for histogram generation, we use a larger patch size Nt > Nc to obtain texture features.
In this work, a 11 by 11 patch (Nt = 11) is used to create f LBP (x, y) ∈ ℜ59.
3) Gray-Level Co-occurrence Matrix statistical feature vector (GLCM): A co-occurrence matrix [16] is a matrix that is
defined over an image as the distribution of co-occurring pixel values at a given offset. In order to generate pixel based feature
of GLCM, the same square patch Pat(x, y) centered at the current pixel I(x, y) as used in LBP feature generation is adopted.
Given the Nt by Nt gray-leveled Pat(x, y) with L different pixel levels (e.g. L = 8), the co-occurrence matrix computes how
often pairs of pixels with a specific value and offset occur in the image. Specifically, the co-occurrence matrix G = {g(i, j)}
is calculated in relation to a given offset, where
g(i, j) =
Nt
X
x=1
Nt
X
y=1
Id [Pat(x, y) = i and Pat(x + δx, y + δy) = j]
(16)
where i, j ∈ [1, ..., L] are pixel gray levels. [x, y] is the spatial position of a pixel and [δx, δy] is the offset, defining the spatial
relation for which this matrix is calculated.
The statistics properties [43] feature vectors, including contrast, correlation, energy and homogeneity, f GLCM = [f GLCM
1
, f GLCM
2
, f GLCM
3
can be derived from the GLCM as
f GLCM
1
= 1
L2
X
i
X
j
|i − j|2g(i, j)
(17)
f GLCM
2
= 1
L2
X
i
X
j
(i − µr)(j − µc)g(i, j)
σrσc
(18)
f GLCM
3
= 1
L2
X
i
X
j
g(i, j)2
(19)
f GLCM
4
= 1
L2
X
i
X
j
g(i, j)
1 + |i − j|
(20)
where µr, µc are the row and column mean of the marginal probabilities of GLCM given by
µr = 1
L2
X
i
i
X
j
g(i, j)
(21)
µc = 1
L2
X
j
j
X
i
g(i, j)
(22)
and σr, σc are associated standard deviation.
σr = 1
L
sX
i
(i − µr)2 X
j
g(i, j)
(23)
σc = 1
L
sX
j
(j − µc)2 X
i
g(i, j)
(24)
6
4) Multiple spatial features: The process of spatial feature generation maps each pixel in a gray level image Im to
Im(x, y) ∈ ℜ → f(x, y) ∈ ℜN 2
c +NLBP +NGLCM
(25)
where
f(x, y) = [[f cell(x, y)]T , [f LBP (x, y)]T , [f GLCM(x, y)]T ]T .
Over all pixels of Im, we have 3D tensor
Xm = {f(x, y)} ∈ ℜH×W ×(N 2
c +NLBP +NGLCM)
(26)
with NLBP = 59, NGLCM = 4, Xm is then reshaped as a feature matrix as
Xm = [f 1, ..., f N]T ∈ ℜN×(N 2
c +NLBP +NGLCM)
(27)
where i = 1, ...N, N = HW is the total number of pixels per image in any band m ∈ [1, ..., M].
For a multi-band image Ifull ∈ ℜH×W ×M, this process is then repeated for all bands, as illustrated in Figure 1. For remote
sensing applications, an additionally derived band may be created based on physical properties. For example, an NDV I
(Normalized Difference Vegetation Index) is computed as the difference between near-infrared (IR) and red (RED) reflectance
divided (element wise) by their sum.
NDV I = IR − R
IR + R
(28)
where IR, R denote near-infrared, and red bands. In this study, we used the NDVI as an additional band to generate of multiple
features from NDVI imagery. LIDAR data are also employed in the feature space where available, with its first echo (FE),
last echo (LE), and intensity, in which LIDAR FE and LE correspond to the first and last points from where the laser beam
is reflected, and LIDAR intensity represents the laser pulse amplitude of the LIDAR FE. When the surface height information
of DSM (Digital Surface Model) is given, it is also used as a band in the feature space. The final feature matrix is
X = [X1, X2, ...., XM] ∈ ℜN×n
(29)
n = M × (N 2
c + NLBP + NGLCM) is a total number of features for the full multi-band remote sensing image Ifull.
B. Proposed semi-supervised spectral clustering algorithm using RBF-CCA
Suppose that we have some Nl ≪ Nu labelled data points, as Dtrain = {xi, ti}Nl
i=1, where xi = [xi,1, ..., xi,n]T ∈ ℜn is a
high dimensional input feature vector. ti ∈ {1, ...K} for a given K ≪ Nl. Let the input data points with labels be denoted as
X(L) = [x1, ..., xNl]T. Simultaneously, there are Nu ≫ Nl unlabelled data points given as X(U) = [xNl+1, ..., xN]T. Denote
the completed input data points
X =
 X(L)
X(U)

∈ ℜN×n,
N = Nu + Nl. The goal of semi-supervised clustering is to partition these points into K disjoint sets, with partial assistance
of supervised learning from the data set Dtrain.
Initially, we apply t-SNE algorithm for dimensionality reduction xi ∈ ℜn −→ yi ∈ ℜm. Denote Y = t-SNE(X) as the full
input features.
It is known that the t-SNE algorithm can generate clusters for visualisation, however, if a conventional clustering algorithm,
e.g. K-means, is applied to assign cluster labels to yi, it may not be very effective, due to that the clustering criterion in
k-means is over simplified, by just minimizing average distances to each cluster centre, and is not suitable for irregularly
shaped clusters, as illustrated in Figure 2. An improvement would be to have a universal approximation model that is capable
of modelling arbitrary cluster shapes, with which to make much appropriate clustering decision. This is the motivation for this
work.
The radial basis functions are a cornerstone in approximation theory [44]. The output of a radial basis function reduces as
the distance between the input and its fixed centre increases, giving its ability to locally identify new data to these centres. In
order to model the irregularly shaped t-SNE clusters, we propose to use a modified CCA based on radial basis function using
data set Dtrain, referred to as RBF-CCA. Specifically, all the training data Dtrain, which are assigned ground truth labels,
are set to be RBF centres. For convenience, denote the embedding of labelled data points as cj = yj, i = 1, ..., Nl. Over Y ,
let ϕi,j = exp

− ∥yi−cj∥2
2σ2

, in which cj are the centres of radial basis functions, σ is a preset proper hyperparameter. In this
work, we simply use
σ =
v
u
u
t
1
NNl
N
X
i=1
Nl
X
j=1
∥yi − cj∥2
(30)
7
Fig. 2. Exemplary 3D view of t-SNE embedding of top, right sub-image (first data set TopoSys GmbH RS, see Section V), which shows irregular shaped
clusters.
to construct a matrix
Φfull =


ϕ1,1
· · ·
ϕ1,Nl
...
...
...
ϕN,1
· · ·
ϕN,Nl


(31)
followed by mean removal as
Φfull ← Φfull − mean(Φfull).
Let
Φfull =

Φ
ΦU

,
(32)
where Φ ∈ ℜN×nl is based on labelled data Dtrain, which is used as the first set of variables in our proposed RBF-CCA. By
examining (10), we can see that this fills up all pair-wise RBF between a query (input features in t-SNE) to that of training
data (with known labels). At each row, only data points close to any given labelled training data will be excited, otherwise
their values are close to zero. Since it is expected that visible clusters are seen in t-SNE space, this means that most of the
data query may excite some cluster via its local RBF centres. We then relate the second set of variables in CCA from ground
truth in Dtrain. The second variable CCA is designed as follows.
In order to create the second set of variables with the objective of semi-supervised clustering, Ψ ∈ ℜNl×K is generated as
Ψ =


ψ1,1
· · ·
ψ1,K
...
...
...
ψNl,1
· · ·
ψNl,K


(33)
in which
ψi,j =
 1
j = ti
0
j ̸= ti
(34)
followed by mean removal as
Ψ ← Ψ − mean(Ψ).
Clearly using canonical correlation analysis (CCA) for the two sets of multidimensional variables Φ, Ψ, as defined above,
aims to capture the relationship between the nonlinear RBF transform based on t-SNE clusters and the cluster labels for the
labelled data set Dtrain. The proposed algorithm, as shown in Algorithm 1, is based on Section III-A to apply the two sets
of random vectors as defined above, referred to as RBF-CCA. Note that Line 5 returns the canonical variable corresponding
to the full input features, which are used for spectral clustering. The proposed semi-supervised spectral clustering based on
RBF-CCA is given in Algorithm 2.
Since canonical variables, which are related to eigenvectors, are applied for clustering, this approach is similar to spectral
clustering algorithms [23]. Note that the idea of Lines 7 and 8 in Algorithm 2 is borrowed from a spectral clustering
algorithm [24] in which clusters in the space of normalized eigenvectors of input data are formed. Here, the clusters in
the space of covariates of input data are to be formed. The idea is similar to spectral clustering in that both employ orthogonal
8
Algorithm 1 Modified CCA using radial basis functions (RBF-CCA).
Require: Number K of clusters to construct; Labelled data Dtrain = {xi, ti}Nl
i=1; Unlabelled data points X(U) =
[xNl+1, ..., xN]T. Complete data in space of t-SNE Y (see Section III-A).
1: Construct Φfull and Ψ using (31) and (33) respectively, then remove their mean.
2: Recover Φ using (32).
3: Perform CCA to obtain A ∈ ℜNl×K and B ∈ ℜK×K (see Section III-B)
4: Calculate U full = ΦfullA using complete input data set.
5: Return U full = {ui,j} ∈ ℜN×K.
Algorithm 2 Proposed semi-supervised algorithm based on RBF-CCA and t-SNE
Require: Number K of clusters to construct; Labelled data Dtrain = {xi, ti}Nl
i=1; Unlabelled data points X(U) =
[xNl+1, ..., xN]T.
1: Form complete input data matrix X. Apply Y = t-SNE(X) to each point.
2: Call Algorithm 1.
3: Form the matrix Z = {zi,j} ∈ ℜN×K by normalising the rows to norm one, i.e. to set
zi,j = ui,j
v
u
u
t
K
X
j=1
u2
i,j.
(35)
4: for i = 1, ..., N do
5:
Let ˆzi ∈ ℜK be the vector corresponding to the ith row of Z.
6: end for
7: Cluster the points ˆzi, i = 1, ..., N with the k-means algorithm [45] into clusters C1, ..., CK.
8: Return: Find clusters k ∈ {1, ..., K} with {k, ˆzi ∈ Ck} and assign original data points xi according to cluster’s index set
of k = 1, ..., K.
spectral analysis of input features, and perform clustering using projected variables in a new orthogonal space. Note that
according to (11), these canonical variables are orthogonal over the small labelled data set, to maximize cross correlation with
the labelled output. Different to spectral clustering, the proposed algorithm, as a semi-supervised approach, realizes transfer
learning via projection of the associated canonical variables to unlabelled data sets. It is therefore reasonable to assume that if
the labelled data set is randomly sampled and representative of the full data set, this could lead to good clustering performance
over the full data set.
The computational complexities of each part of the proposed algorithm is in the order of O(nN 2) (t-SNE), which is further
scaled by iterations of gradient descent algorithms, O(N 3
l ) (RBF-CCA), and O(N) (k-means clustering). Since N ≫ Nl, the
main cost is due to obtaining the t-SNE embedding, which is a drawback for problems with large N and n. To mitigate this,
principal component analysis (PCA) can be applied for reducing n, and the simple strategy of divide and conquer can be
employed to reduce N. In our experiments, images are divided into sub-images, with final segmentation combined at the end.
TABLE I
EXAMPLE 1: COMPARISON OF SEGMENTATION ACCURACY (PERCENTAGE OF CORRECTLY IDENTIFIED PIXEL LABELS). THREE VARIANTS OF CCA ARE
BASED ON LINEAR, POLYNOMIAL AND RADIAL BASIS FUNCTION OF T-SNE FEATURES.
Data sets
k-means on
k-means on
Linear
Polynomial
Proposed
multiple features
t-SNE embedding
CCA
CCA
RBF-CCA
Sub-image (top, left)
62.04
71.22
74.79 ± 0.26
77.90 ± 0.46
87.28 ± 0.25
TopoSys GmbH
Sub-image (top, right)
51.34
72.97
72.98 ± 0.16
73.99 ± 0.17
84.50 ± 0.21
(First data set)
Sub-image (bottom, left)
58.80
74.01
76.01 ± 0.19
79.99 ± 0.21
87.88 ± 0.42
Sub-image (bottom, right)
55.54
71.70
76.77 ± 0.10
79.37 ± 0.68
87.16 ± 0.19
TopoSys GmbH
Full image
60.52
72.91
71.04 ± 0.53
75.64 ± 0.17
84.77 ± 0.20
(Second data set)
V. EXPERIMENTS
Two sets of experiments with multi-band remotely sensed images, which include hand-labelled ground truth, are conducted
for validating the proposed algorithm. For training purposes, we repeatedly used five percent of ground truth as Nl labeled
data, which are randomly selected. The ground truth are also used for the evaluation purpose. In order to obtain the final
performance metrics, the predicted cluster labels are mapped into the given ground truth via the well known Kuhn-Munkres
algorithm [46].
9
TABLE II
INTERSECTION OVER UNION (IOU) OF EXAMPLE 1 (TOPOSYS GMBH).
Class
First data set
Second data set
mean IOU
mean IOU
1- Building
0.9536
0.9371
2- Tree
0.9050
0.9096
3- Low vegetation
0.8723
0.8688
4- Impervious surface OR Car
0.9054
0.8769
Example 1: Two data sets obtained from Trimble Holdings GmbH (formerly TopoSys GmbH, Germany) are experimented.
Each data set contains seven bands, which are LIDAR first echo FE, last echo LE, colour image red (R), green (G), and blue
(B), near infrared (IR), and LIDAR intensity (IN). An additional band of NDVI was generated, so eight bands are used for
each data set.
• The first data set has a size of 400 × 400 pixels per band, with seven measured data plus the derived NDVI, as shown
in Figure 3, and the RGB view of the image is shown in Figure 4(a). The data set was then processed by splitting into four
200 × 200 sub-images, each of which is treated as Ifull ∈ ℜ200×200×8. This results in N = 40000, n = 896, for each
sub-image (left top, right top, left bottom and right bottom). The segmentation was performed for each part respectively, and
final results are obtained by merging the parts into the complete image. The ground truth of this image contains four classes
(1- Building; 2 -Tree ; 3 - Low vegetation ; and 4- Impervious surface OR Car) that were manually labelled pixel-wise, of
which only 5% was used for training, and all are used for validation. In analysis of the data set, there is higher presence of
“Low vegetation” that needs to be segmented, and the ground truth has a type “Impervious surface OR Car”, which makes
data class population well-balanced. The segmentation accuracies of an average of ten random experiments of the proposed
RBF-CCA algorithm are included, in comparison with a few baseline approaches, as shown in Table I. The k-means algorithm
was applied in original high dimensional data space, as well as in 3D t-SNE space. For the two additional versions of CCA used
for the purpose of comparative studies, they differ from RBF-CCA only in how to form Φfull. In the linear CCA algorithm,
the t-SNE features are used without any nonlinear transformation. For the polynomial CCA algorithm, the t-SNE features
together with their quadratic terms are applied. It can be seen that although k-means clustering based on t-SNE embedding
offers improvement over clustering of original data space, the proposed RBF-CCA has much improved clustering performance
over both unsupervised k-means algorithm for original data and t-SNE embedding data. The proposed RBF-CCA algorithm is
better than linear CCA and polynomial CCA, both are included to demonstrate the superior approximation capability of RBF
to model the highly nonlinear t-SNE clusters. The resulting segmented four sets of 200 by 200 pixels labels are then combined.
The ground truth, segmentation representation and classification confusion matrix are shown in Figure 4(b)-(d) respectively,
also demonstrating excellent performance for the first data set.
• The second data set is measured in 220 × 300 per band, as shown in Figure 5. In this case, the data set was processed
as a whole, so Ifull ∈ ℜ220×300×8, resulting in N = 66000, n = 896. A coloured RGB is shown in Figure 6(a). Similar to
the first data set, four classes for segmentation are labelled as (1- Building; 2 -Tree ; 3 - Low vegetation ; and 4- Impervious
surface OR Car). The ground truth, segmentation representation and classification confusion matrix are shown in Figure 6(b)-
(d) respectively. The segmentation accuracies, as also included in Table I, demonstrate superior performance for all classes in
comparison with the k-means algorithm, over multiple features and t-SNE embedding, as well as linear CCA and polynomial
CCA.
The quality of segmentation of Example 1 over both data sets, as measured by intersection over union (IOU) from set theory,
is shown in Table II, where the mean IOU is calculated by averaging IOU of one against all other clusters involved.
Example 2: A data set showing City of Potsdam in ISPRS 2D Semantic Labelling Contest is used in this experiment [47].
The website contains 38 patches of the same size. The patch 7-8 was used to demonstrate the effectiveness of the proposed
approaches. The original data set has a high resolution 6000 by 6000 pixels, with multi-band images consisting of R, G, B, IR
and normalized DSM. Together with an additional band of NDVI, six bands are as shown in Figure 7 and the coloured RGB
view is shown in Figure 8(a). The high resolution multi band images are resized to 800 by 800 pixels, which are then divided
into sixteen 200 by 200 pixels sub-images, so that Ifull ∈ ℜ200×200×8, based on which the proposed algorithm RBF-CCA is
performed.
For each of 16 sub-images, N = 40000 and n = 672. The original ground truth was Impervious surfaces (RGB: 255,
255, 255), Building (RGB: 0, 0, 255), Low vegetation (RGB: 0, 255, 255), Tree (RGB: 0, 255, 0), Car (RGB: 255, 255, 0)
Clutter/background (RGB: 255, 0, 0). This is also resized to 800 by 800 pixels, followed by rounding to the closest (255 or 0)
in each R, G, B band. The ground truth of this image are set as four classes (1- Building; 2 - Impervious surface OR Car OR
Clutter; 3 - Tree ; 4- Low vegetation), as shown in Figure 8(b). The segmentation accuracies of all sixteen sub-images, each
of which the average results of ten random experiments (using Nl = 5%N labelled data in training of RBF-CCA) are listed as
shown in Table III, by comparing with the k-means algorithm on multiple features and t-SNE embedding and the two variants
of CCAs. It can be seen linear CCA fails due to inability to model nonlinearity in t-SNE clusters, and the polynomial CCA
has only limited improvement over linear CCA since it cannot model nonlinearity about labelled data samples locally. Both
10
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Fig. 3. Eight multispectral images of TopoSys GmbH (the first data set, Example 1) (a) Red; (b) Green; (c) Blue; (d) Last echo; (e) First echo; (f) LIDAR
intensity; (g) Near infrared; (h) NDVI.
(a)
(b)
(c)
(d)
Fig. 4. Segmentation results of TopoSys GmbH (the firts data set, Example 1): (a) RGB view (b) Ground truth; (c) Segmentation; and (d) Confusion matrix
(1- Building; 2 -Tree ; 3 - Low vegetation ; and 4- Impervious surface OR Car.)
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Fig. 5. Eight multispectral images of TopoSys GmbH (the second data set, Example 1) (a) Red; (b) Green; (c) Blue; (d) Last echo; (e) First echo; (f) LIDAR
intensity; (g) Near infrared; and (h) NDVI
linear and polynomial CCA have similar performance to unsupervised clustering approaches. Owing to the universal ability of
RBFs to approximate t-SNE embeddings, the proposed RBF-CCA algorithm has the best segmentation performance.
The resulting segmented sixteen 200 by 200 pixels labels are combined and is shown in Figure 8(c), and the pixel based
11
(a)
(b)
(c)
(d)
Fig. 6. Segmentation results of the TopoSys GmbH (the second data set, Example 1); (a) RGB view (b) Ground truth; (c) Segmentation; and (d) Confusion
matrix ((1- Building; 2 - Tree ; 3 - Low vegetation ; and 4- Impervious surface OR Car. )
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 7.
Six multispectral images channels of Potsdam7-8 (Example 2) (a) Red; (b) Green; (c) Blue; (d) Infra-red; (e) DSM; (f) NDVI
(a)
(b)
(c)
(d)
Fig. 8.
Segmentation results of Potsdam7-8 data set (Example 2); (a) RBG view; (b) Ground truth; (c) Segmentation; and (d) Confusion matrix (1- Building;
2 - Impervious surface OR Car OR Clutter; 3 - Tree ; 4- Low vegetation)
classification confusion matrix is shown in Figure 8(d). Finally, the mean IOU is shown in Table IV, demonstrating that the
proposed approaches are highly effective.
VI. CONCLUSIONS
Applied to image segmentation of remotely sensed urban areas, this paper has introduced a novel semi-supervised spectral
clustering method based on multiple texture features of multispectral images plus LIDAR data where available. Centered at
each raw pixel in multiple bands, including the inferred vegetation index band, a pixel patch is vectorized, into concatenated
image texture features of LBP and GLCM to be represented as a data point in a high-dimensional feature space, which are
mapped to a 3-D visually t-SNE embedding space. Using a small subset of pixels having segmentation labels, this paper has
extended canonical correlation analysis with RBF function to nonlinear CCA. We have proposed to perform t-SNE embedding
initially, based on which RBF-CCA algorithm is then developed. The first set of random variables is designed based on a set of
12
TABLE III
EXAMPLE 2: COMPARISON OF SEGMENTATION ACCURACY (PERCENTAGE OF CORRECTLY IDENTIFIED PIXEL LABELS). THREE VARIANTS OF CCA ARE
BASED ON LINEAR, POLYNOMIAL AND RADIAL BASIS FUNCTION OF T-SNE FEATURES.
Data sets
k-means on
k-means on
Linear
Polynomial
Proposed
multiple features
t-SNE embedding
CCA
CCA
RBF-CCA
Potsdam patch 7-8
Sub-image (top, left)
55.94
53.39
50.12 ± 0.01
64.21 ± 0.67
85.63 ± 0.52
(top, left quarter)
Sub-image (top, right)
67.94
62.03
57.42 ± 0.44
73.35 ± 0.42
88.31 ± 0.20
Sub-image (bottom, left)
59.15
75.81
71.61 ± 1.98
75.63 ± 0.25
84.55 ± 0.33
Sub-image (bottom, right)
66.68
65.52
65.05 ± 0.13
68.33 ± 0.31
83.31 ± 0.43
Potsdam patch 7-8
Sub-image (top, left)
63.10
63.96
60.51 ± 0.16
64.78 ± 0.79
83.68 ± 0.60
(top, right quarter)
Sub-image (top, right)
68.80
64.37
53.85 ± 0.86
64.06 ± 0.41
87.16 ± 0.52
Sub-image (bottom, left)
60.17
55.64
62.33 ± 0.36
68.95 ± 0.38
82.54 ± 0.37
Sub-image (bottom, right)
70.94
63.38
55.50 ± 0.88
66.94 ± 1.28
81.38 ± 0.57
Potsdam patch 7-8
Sub-image (top, left)
65.14
62.63
60.73 ± 0.34
74.63 ± 0.35
87.07 ± 0.33
(bottom, left quarter)
Sub-image (top, right)
53.64
51.92
50.63 ± 0.01
65.29 ± 0.50
84.59 ± 0.52
Sub-image (bottom, left)
68.23
47.57
48.01 ± 0.14
72.27 ± 0.27
87.00 ± 0.17
Sub-image (bottom, right)
57.24
52.32
49.59 ± 0.88
59.24 ± 2.05
83.93 ± 0.40
Potsdam patch 7-8
Sub-image (top, left)
58.86
58.52
53.46 ± 0.12
61.12 ± 4.36
81.98 ± 0.59
(bottom, right quarter)
Sub-image (top, right)
72.37
56.29
52.72 ± 0.27
55.20 ± 0.39
82.11 ± 0.66
Sub-image (bottom, left)
65.28
62.25
54.52 ± 0.91
59.58 ± 0.72
83.14 ± 0.76
Sub-image (bottom, right)
73.64
62.04
59.46 ± 0.43
57.56 ± 2.66
86.35 ± 0.45
TABLE IV
INTERSECTION OVER UNION (IOU) OF EXAMPLE 2 (ISPRS 2D POTSDAM PATCH 7-8 ).
Class
mean IOU
1- Building
0.8872
2- Impervious surface
0.9610
OR Car OR Clutter
3- Tree
0.8516
4- Low vegetation
0.8093
RBFs using the labelled data points in t-SNE embedding, and the second set of random variables are related to corresponding
labels. The proposed algorithm learns the associated projection matrix from RBF-CCA, followed by learning the associated
canonical variables for the full multi-band image. Finally, the canonical variables are clustered using k-means clustering. It is
shown that the proposed algorithm is capable of excellent segmentation for several land cover data sets, despite only using a
small amount of labelled data.
In future work with regard to semi-supervised multispectral image segmentation for remote sensing imagery, different land
cover scenarios will be considered. The algorithm will be further developed and optimized in terms of robustness in challenging
situations with a focus on imbalanced clusters, scattered important objects, and segmentation of similar objects. It will also be
extended to deal with sea surfaces covered by ice, water mixed with ice, and shadowed by clouds from satellite multispectral
images.
REFERENCES
[1] J. MacQueen, “Some methods for classification and analysis of multivariate observations,” in the Proceedings of the 5th Berkeley Symposium on
Mathematical Statistics and Probability, vol. 1, pp. 105–117, 1967.
[2] H. Caillol, A. Hillion, and W. Pieczynski, “Fuzzy random fields and unsupervised image segmentation,” IEEE transactions on geoscience and remote
sensing, vol. 31, no. 4, pp. 801–810, 1993.
[3] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp.
888–905, 2000.
[4] L. Zhu, X. Kang, L. Ye, and A. Ming, “Explored normalized cut with random walk refining term for image segmentation,” IEEE Transactions on Image
Processing, vol. 31, pp. 2893–2906, 2022.
[5] A. Dutta, J. Engels, and M. Hahn, “Segmentation of laser point clouds in urban areas by a modified normalized cut method,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 3034–3047, 2019.
[6] Y. Cao, H. Wei, H. Zhao, and N. Li, “An effective approach for land-cover classification from airborne lidar fused with co-registered data,” International
Journal of Remote Sensing, vol. 33, no. 18, pp. 5927–5953, 2012.
[7] F. Yang, H. Wei, and P. Feng, “A hierarchical dempster-shafer evidence combination framework for urban area land cover classification,” Measurement,
vol. 151, p. 105916, 2020.
[8] W. Kim, A. Kanezaki, and M. Tanaka, “Unsupervised learning of image segmentation based on differentiable feature clustering,” IEEE Transactions on
Image Processing, vol. 29, pp. 8055–8068, 2020.
[9] A. Zadaianchuk, M. Kleindessner, Y. Zhu, F. Locatello, and T. Brox, “Unsupervised semantic segmentation with self-supervised object-centric
representations,” in ICLR 2023 The Eleventh International Conference on Learning Representations, 2023.
[10] L. Wu, M. Lu, and L. Fang, “Deep covariance alignment for domain adaptive remote sensing image segmentation,” IEEE Transactions on Geoscience
and Remote Sensing, vol. 60, pp. 1–11, 2022.
[11] X. Liu, C. Yoo, F. Xing, H. Oh, G. E. Fakhri, J.-W. Kang, and J. Woo, “Deep unsupervised domain adaptation: A review of recent advances and
perspectives,” APSIPA Transactions on Signal and Information Processing, 2022.
13
[12] S. Wang, D. Zhao, C. Zhang, Y. Guo, Q. Zang, Y. Gu, Y. Li, and L. Jiao, “Cluster alignment with target knowledge mining for unsupervised domain
adaptation semantic segmentation,” IEEE Transactions on Image Processing, vol. 31, pp. 7403–7418, 2022.
[13] L. Jing, Y. Chen, and Y. Tian, “Coarse-to-fine semantic segmentation from image-level labels,” IEEE Transactions on Image Processing, vol. 29, pp.
225–236, 2020.
[14] J. Cai, J. Hao, H. Yang, X. Zhao, and Y. Yang, “A review on semi-supervised clustering,” Information Sciences, vol. 632, pp. 164–200, 2023.
[15] L. van der Maaten and G. Hinton, “Visualizing high-dimensional data using t-sne,” Journal of Machine Learning Research, vol. 9, no. nov, pp. 2579–2605,
2008, pagination: 27.
[16] R. M. Haralick, K. Shanmugam, and I. Dinstein, “Textural features for image classification,” IEEE Transactions on Systems, Man, and Cybernetics, vol.
SMC-3, no. 6, pp. 610–621, 1973.
[17] T. Ojala, M. Pietikainen, and D. Harwood, “Performance evaluation of texture measures with classification based on kullback discrimination of
distributions,” in Proceedings of 12th International Conference on Pattern Recognition, vol. 1, 1994, pp. 582–585 vol.1.
[18] Y. Wang, H. Huang, C. Rudin, and Y. Shaposhnik, “Understanding how dimension reduction tools work: An empirical approach to deciphering t-sne,
umap, trimap, and pacmap for data visualization,” Journal of Machine Learning Research, vol. 22, no. 201, pp. 1–73, 2021. [Online]. Available:
http://jmlr.org/papers/v22/20-1061.html
[19] T. T. Cai and R. Ma, “Theoretical foundations of t-sne for visualizing high-dimensional clustered data,” Journal of Machine Learning Research,
vol. 23, no. 301, pp. 1–54, 2022. [Online]. Available: http://jmlr.org/papers/v23/21-0524.html
[20] H. Hotelling, “The most predictable criterion,” Journal of Educational Psychology, vol. 26, pp. 139–142, 1935.
[21] ——,
“Relations
between
two
sets
of
variables,”
Biometrika,
vol.
28,
no.
3-4,
pp.
321–377,
12
1936.
[Online].
Available:
https:
//doi.org/10.1093/biomet/28.3-4.321
[22] S. AKAHO, “A kernel method for canonical correlation analysis,” International Meeting of Psychometric Society, 2001, vol. 1, 2001. [Online].
Available: https://cir.nii.ac.jp/crid/1574231874767776512
[23] U. Luxburg, “A tutorial on spectral clustering,” Statistics and Computing, vol. 17, no. 4, p. 395–416, dec 2007.
[24] A. Ng, M. Jordan, and Y. Weiss, “On spectral clustering: Analysis and an algorithm,” Adv. Neural Inf. Process. Syst., vol. 2, no. 11, 2001.
[25] T. Cour, F. Benezit, and J. Shi, “Spectral segmentation with multiscale graph decomposition,” in 2005 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (CVPR’05), vol. 2, 2005, pp. 1124–1131.
[26] X. Zhang, L. Jiao, F. Liu, L. Bo, and M. Gong, “Spectral clustering ensemble applied to sar image segmentation,” IEEE Transactions on Geoscience
and Remote Sensing, vol. 46, no. 7, pp. 2126–2136, 2008.
[27] L. Zhu, X. Kang, L. Ye, and A. Ming, “Explored normalized cut with random walk refining term for image segmentation,” IEEE Transactions on Image
Processing, vol. 31, pp. 2893–2906, 2022.
[28] G. Zhong and C.-M. Pun, “Self-taught multi-view spectral clustering,” Pattern Recognition, vol. 138, p. 109349, 2023.
[29] S. Basu, A. Banerjee, and R. J. Mooney, “Active semi-supervision for pairwise constrained clustering,” in Proceedings of the 2004 SIAM International
Conference on Data Mining (SDM), 2004, pp. 333–344.
[30] Z. Ghasemi, H. A. Khorshidi, and U. Aickelin, “A survey on optimisation-based semi-supervised clustering methods,” in 2021 IEEE International
Conference on Big Knowledge (ICBK), 2021, pp. 477–482.
[31] T. H. Kim, K. M. Lee, and S. U. Lee, “Learning full pairwise affinities for spectral segmentation,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 35, no. 7, pp. 1690–1703, 2013.
[32] J. Pont-Tuset, P. Arbel´aez, J. T. Barron, F. Marques, and J. Malik, “Multiscale combinatorial grouping for image segmentation and object proposal
generation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 1, pp. 128–140, 2017.
[33] C.-G. Li, C. You, and R. Vidal, “Structured sparse subspace clustering: A joint affinity learning and subspace clustering framework,” IEEE Transactions
on Image Processing, vol. 26, no. 6, pp. 2988–3001, 2017.
[34] M. Luo, C. Yan, Q. Zheng, X. Chang, L. Chen, and F. Nie, “Discrete multi-graph clustering,” IEEE Transactions on Image Processing, vol. 28, no. 9,
pp. 4701–4712, 2019.
[35] X. Jiao, Y. Chen, and R. Dong, “An unsupervised image segmentation method combining graph clustering and high-level feature representation,”
Neurocomputing, vol. 409, pp. 83–92, 2020.
[36] F.-F. Li, R. Fergus, and P. Perona, “One-shot learning of object categories,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28,
no. 4, pp. 594–611, 2006.
[37] Y. Wang and Q. Yao, “Few-shot learning: A survey,” CoRR, vol. abs/1904.05046, 2019. [Online]. Available: http://arxiv.org/abs/1904.05046
[38] N. Catalano and M. Matteucci, “Few shot semantic segmentation: a review of methodologies and open challenges,” 2023.
[39] Z. Chen and B. Wang, “Semisupervised spectral–spatial classification of hyperspectral imagery with affinity scoring,” IEEE Geoscience and Remote
Sensing Letters, vol. 12, no. 8, pp. 1710–1714, 2015.
[40] M. Jiang, X. Chen, L. Xu, and D. A. Clausi, “Semi-supervised sea ice classification of sar imagery based on graph convolutional network,” in IGARSS
2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium, 2022, pp. 1031–1034.
[41] L. Hardle, Wolfgang; Simar, in Applied Multivariate Statistical Analysis. Berlin, Heidelberg: Springer Berlin Heidelberg, 2007, ch. Canonical Correlation
Analysis, pp. 321–330.
[42] T. Ojala, M. Pietik¨ainen, and T. M¨aenp¨a¨a, “A generalized local binary pattern operator for multiresolution gray scale and rotation invariant texture
classification,” in Advances in Pattern Recognition — ICAPR 2001, S. Singh, N. Murshed, and W. Kropatsch, Eds.
Berlin, Heidelberg: Springer Berlin
Heidelberg, 2001, p. 399—408.
[43] M. Bevk and I. Kononenko, “A statistical approach to texture description of medical images: a preliminary study,” in Proceedings of 15th IEEE Symposium
on Computer-Based Medical Systems (CBMS 2002), 2002, pp. 239–244.
[44] T. Poggio and F. Girosi, “Networks for approximation and learning,” Proceedings of the IEEE, vol. 78, no. 9, pp. 1481–1497, 1990.
[45] S. S. Haykin, Neural networks and learning machines, 3rd ed., Upper Saddle River, NJ, 2009.
[46] L.
Lov´asz
and
M.
Plummer,
Matching
Theory,
ser.
AMS
Chelsea
Publishing
Series.
Akad´emiai
Kiad´o,
2009.
[Online].
Available:
https://books.google.co.uk/books?id=yW3WSVq8ygcC
[47] [Online]. Available: https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-potsdam.aspx
"
"Social identity plays a significant role in societies. Hence, social identity is a way to encourage responsible behavior in humans. Humans identify with abstract notions and act to uphold them. Thus, identification with a notion affects the choices and actions of the person across multiple scenarios. Most models of autonomous agents replicate human attributes into agents. We propose an agent model that enables agents to identify with certain notions and show how this affects collective outcomes. We also contrast between associations of identity with rational preferences. The proposed model is simulated in an application context of urban mobility, where we show how changes in social identity affect mobility patterns and collective outcomes.","Social identities are important for bringing social changes. Social activist strive to get populations to identify with notions like green energy, diversity etc to bring about desired changes. For this we need to design computational models for social identities in autonomous AI agents. We propose an agent model which enables autonomous identification with notions, and show how it affects collective outcomes. We also compare associations of identity with rational preferences. We simulate the proposed model in an urban mobility context, and show how social identity affects mobility patterns and collective outcomes.","nanAutonomous agents are becoming increasingly important as they are being deployed in diverse fields. Engineering autonomy into agent based systems is challenging yet important. Different approaches have been taken like Normative,Adaptive Learning, Rational Choice, and Models of Self. Models of Self involve modelling agents with a sense of self drawn from identity associations. Some work has been done in incorporating notions in autonomous agents inspired by organisational science, cognitive science, and psychology. However, it assumes notions as goals to be achieved, unlike the CT framework, where notions are drivers of agents' behaviors.","We use Computational Transcendence (CT) framework to model autonomous agents which identify with notions. CT proposes to model autonomous agents with an elastic sense of self. The agents' identity is defined by a set of objects or external entities with which the agent identifies. The association between an identity object and an observable is given by a weight. The greater the weight, the greater the association, while weight 0 means the observable is irrelevant to the object. We define the schema of the identity object as the normalized weights of the relevant objects in the identity set over all the observables in a given context. Using the schemas, we compute the final weights an agent assigns to different observables in a context. These final weights can be interpreted as the preferences of the agent. The behavior of an agent can now be modelled using the standard Markov Decision Process framework using a utility function based on these weights. Since utility of observables is well studied, we define schemas over observables to help model identity based artificial autonomous agents.","We demonstrate modelling transit choices as a realistic use-case of this model. We translate different objects in the identity set to a composition of observables in a specific context. In the model, specific objects in the identity set of an agent become relevant in a specific context, which in turn become drivers for the agent decision making. We model two transit choices, one depicting a private mode of transportation like a taxi service and the other being a public mode of transportation like a bus. We identify factors like cost, time, congestion, and carbon footprint as the relevant contextual observables. We introduce one such factor, conformity, in our model. We show how different initial parameters, like the distribution of the initial semantic distances, the amount of conformity in the network, etc. impact the transit choices of the agents.","Our extended CT model helps to address the problem of modelling an identity in autonomous agents. Using this model, we build agents that identify with different abstract notions to different extents. This framework is extensible to account for various individual and social factors relevant in a given context, and can be used to understand the effects of different policy interventions and help the system designers develop effective interventions leading to sustainable population choices across diverse applications.",Transcending To Notions,"Sama Sai Karthik, Jayati Deshmukh, Srinath Srinivasa","TRANSCENDING TO NOTIONS
Sama Sai Karthik
International Institute of Information Technology, Bangalore
26/C, Electronics City Phase 1
Bangalore, Karnataka, India
sai.karthik@iiitb.ac.in
Jayati Deshmukh
International Institute of Information Technology, Bangalore
26/C, Electronics City Phase 1
Bangalore, Karnataka, India
jayati.deshmukh@iiitb.org
Srinath Srinivasa
International Institute of Information Technology, Bangalore
26/C, Electronics City Phase 1
Bangalore, Karnataka, India
sri@iiitb.ac.in
ABSTRACT
Social identities play an important role in the dynamics of human societies, and it can be argued that
some sense of identification with a larger cause or idea plays a critical role in making humans act
responsibly. Often social activists strive to get populations to identify with some cause or notion–
like green energy, diversity, etc. in order to bring about desired social changes. We explore the
problem of designing computational models for social identities in the context of autonomous AI
agents. For this, we propose an agent model that enables agents to identify with certain notions and
show how this affects collective outcomes. We also contrast between associations of identity with
rational preferences. The proposed model is simulated in an application context of urban mobility,
where we show how changes in social identity affect mobility patterns and collective outcomes.
Keywords Agency, Identity, Multi-Agent Systems
1
Introduction
Most systemic changes are feasible when a large number of people participate and contribute in bringing the change. For
example, in Amsterdam, cycling accounts for 38% of all vehicle trips, and there are about 0.75 bikes per inhabitant [4].
In order to motivate people, social identity plays a crucial role. When people identify with a cause or notion, they
willingly and actively participate in bringing a change. Hence, social identity is a way to encourage responsible
behaviour in humans.
Humans identify with abstract notions and act so as to uphold these notions. For example, people who identify
with environmentalism call themselves environmentalists and act appropriately across different contexts, like pre-
ferring environment-friendly transit options, buying eco-friendly products, reducing and recycling waste, etc. Thus,
identification with a notion affects the choices and actions of the person across multiple scenarios.
Most models of autonomous agents replicate and build on a few specific desired characteristics of humans into the
agents. For example, chatbots are modelled to respond like humans, agents in the healthcare domain are designed
to be empathetic, etc. We focus on designing agents which can be intrinsically responsible. Specifically, just like
arXiv:2401.12159v1  [cs.MA]  22 Jan 2024
Transcending To Notions
humans have a social identity which makes them identify with abstract notions and act responsibly, we propose to
design autonomous agents with a sense of self such that they can identify with abstract notions relevant to the system in
which they are operating.
In this paper, we propose a model for autonomous agents which have a social identity such that they can identify with
abstract notions. We use one of the existing models of responsible identity called Computational Transcendence (CT)
[6] and extend it so that autonomous agents can identify with multiple abstract notions and act such that their actions
are aligned with the notions they identify with.
Next, we demonstrate this model in a scenario where autonomous agents must make transit choices. Autonomous
agents identify with different notions like environmentalism, frugalism, etc., each of which impacts their choices.
Eventually, in our transit simulation, agents must choose between public transport or private transport. Using the model
of identifying with notions, we can create diverse kinds of autonomous agents identifying with different notions to
varying extents and study the collective behaviour which emerges in such populations.
Major contributions presented in this paper are as follows: 1. We present the extended model of Computational
Transcendence using which agents can identify with multiple abstract notions to varying extents. 2. We demonstrate
an application of this model for the use case of autonomous agents making transit choices. 3. We also analyze the
impact of factors like conformity on autonomous agents. The paper is organized as follows: In Section 1, we introduce
the topic of autonomous agents identifying with abstract notions. Next, in Section 2, we look at some of the work
which has been done in the area of decision-making by autonomous agents and different ways of modelling transit
choices. Section 3 presents the detailed formal model of agency using which autonomous agents can identify with
abstract notions. Next, we present the experimental details of translating this model to an agent making transit choices
in Section 4. Results of the simulations are presented in Section 5. Finally, we conclude with key discussions and
conclusions of this work in Section 6.
2
Related Work
In this section, first, we discuss modelling autonomy into artificial agents which affects their decision-making. Next, we
look at the importance of modelling a social identity which can be useful to elicit cooperation from agents for bigger
causes. Finally, we elaborate on some of the existing work done in the area of modelling transit choices.
2.1
Autonomous Decision Making
The motivation to build artificial agents that can make decisions of their own has been fuelled by the deployment
of AI in various fields of practical use like healthcare [19, 16, 1], finance [5, 38], autonomous vehicles [26, 24] to
name a few. Engineering the ability to make decisions by themselves into agent-based systems is studied under the
umbrella of autonomous agency. The AI-based systems built using autonomous agents are expected to make decisions
autonomously in real time. Also, these systems must act responsibly and preferably provide reasoning behind their
decisions. Thus, modelling autonomous agency is challenging yet increasingly important, given the penetration of AI in
society.
Modelling autonomy in artificial agents has been approached from several paradigmatic standpoints. These include
Normative models, Adaptive Learning, Rational Choice, and Models of Self [34].
The Normative paradigm focuses on establishing a set of norms or rules that guide the decision-making process of
autonomous agents. These norms define the agents’ acceptable behaviour and ethical boundaries, enabling them to
make decisions that align with predefined principles and values. The Beliefs-Desire-Intention (BDI) model [8], Event
Condition Action (ECA) [13] sequence and the Truth Maintenance Systems (TMS) [10] etc. are a few such techniques of
modelling agency using the Normative paradigm.
The Adaptive Learning paradigm emphasizes the importance of learning and adaptation in autonomous decision-making.
Agents following this paradigm continuously update their decision-making strategies based on the feedback from the
environment. Techniques like Reinforcement Learning [35], Swarm Intelligence [22], Ant Colony Optimisation [9], etc.
are used to model autonomous agents using this paradigm.
The Rational Choice paradigm is based on the principle of rationality and utility maximization. The Classical Rational
Choice Theory has been critiqued for its departure from human behaviour, which often forms the basis of intelligent or
autonomous behaviour. A few of the aspects highlighted by these critiques are agents showcasing empathy [30], loss
aversion [11], bounded rationality [33], etc. along with maximizing their self-interest.
2
Transcending To Notions
The Models of Self paradigm involves modelling agents with a sense of self. The idea of a sense of self draws inspiration
from identity associations which in turn drive human behaviour. Some models that fall under the umbrella of this
paradigm include autonomic computing [14, 23] which models various self-* properties, Autopoiesis [27] which built
models from a biological perspective, Cybernetics and Artificial Life [20, 25] which models self-regulatory properties
etc.
A recently proposed framework based on the Models of Self paradigm is Computational Transcendence (CT) [6]. CT
proposes to model autonomous agents by defining an elastic sense of self. This framework, in turn, allows system
designers to define different associations in the environment as part of the identity of individual agents. So far, in
practice, only identity associations with other agents in the environment have been modelled using CT. However, it is
also important to model identification with abstract notions to design more varied, realistic and responsible autonomous
agents. In this paper, we present the extended CT model using which autonomous agents can identify with abstract
notions to varying extents and demonstrate diverse behaviour.
Some work has been done on incorporating notions into agents inspired by organisational science, cognitive science, and
psychology [29]. Here artefacts are introduced as passive entities that influence the decision-making of the autonomous
agents and their applicability in various MAS-based scenarios has been discussed. However, most existing work
which incorporates notions in autonomous agents assumes notions as the goals to be achieved. However using the CT
framework, notions are the drivers of agents’ behaviour rather than a goal to be achieved.
2.2
Social Identity
Humans also identify with abstract notions and strive to uphold them. Social identity is defined as “those aspects of an
individual’s self-image that derive from the social categories to which he perceives himself as belonging."" [36] Also,
group identity plays an important role in various social contexts [12]. It is a way to elicit cooperation from the masses
for different causes– like climate change, diversity etc.
2.3
Modelling Transit Choices
Psychology has representations of human behaviour using various models, and understanding some of these helps
design computational models for autonomous agents. For example, the Theory of Planned Behaviour (TPB) [2] is a
widely used theory for modelling human behaviour. It states that the beliefs of an agent, coupled with the subjective
norms, form the fundamental basis of an individual’s behaviour. It highlights the role of perceived behavioural control
in the final behavioural decisions made by the individual.
A recent work [28] in the context of modelling transit choices of humans makes use of TPB to build agents that
subsequently model human behaviour in the context of traffic scenarios.
Cui et al. [32] proposed a Multinomial Logit (MNL) based model describing passengers’ transit choices using ride-
hailing data. This work aimed to suggest how to navigate sustainable ride-hailing choices. They further identified that
factors like pick and drop-off locations, travel distance, time, and cost strongly affect passengers’ transit choices.
There has been work to estimate distributions of different factors associated with transit, like cost, time, etc. Such
studies help model suitable environments by sampling these distributions to simulate the trips of agents [31, 3, 15].
This way of modelling becomes useful, especially when actual data on transit choices is unavailable. Furthermore,
even with the availability of transit-related data, these distributions can be perturbed to model a realistic underlying
stochastic distribution. We use these latent distributions to construct an environment where different types of transit
trips for autonomous agents are simulated.
Agent-based modelling also has been useful in simulating transit choices of people [37, 18, 17]. Further, modelling
these traffic flows includes understanding the population density on roads at a given time and the choices people make
for their transit decisions. Understanding the interplay of the factors that can influence transit decisions, including
beliefs and adaptability of the population, can aid policymakers in planning and designing better transportation solutions.
As highlighted by many of the agent-based studies in the context of transit choices, we identify cost, time, comfort (in
the form of congestion), environmental awareness (in terms of carbon footprint per head) and social factors (in terms of
conformity) as the factors that are relevant for designing the simulation environment. People making transit choices are
often individual or local optimizers of their decisions. Thus, devising appropriate strategies means designing systems
where local optimizations by individuals lead to a desired globally optimal system state. Such a system state can be
quantified in terms of sustainability, low net carbon footprint, etc.
3
Transcending To Notions
3
Modelling Identification with Notions
In this section, we elaborate on how we model autonomous agents which identify with notions. We make use of
Computational Transcendence (CT) [6] for this, a framework used to model autonomous responsible agency. The CT
framework defines an autonomous agent a with an elastic sense of self. Formally, this elastic sense of self is represented
by S(a) = (Ia, da, γa) where:
• Ia represents the set of objects or external entities with which the agent a identifies itself.
• da : a × Ia 7→ ℜ+ is a set of semantic distances. The semantic distance between a and an object in Ia captures
how important the object is in shaping the identity of a. The smaller the semantic distance, the greater its
influence on the identity of a.
• γa ∈ [0, 1] represents the elasticity or transcendence level of the agent a’s sense of self.
We extend the CT framework to build agents who can identify with abstract notions. For this, we first define
the measurable quantities in an agent’s given environment or context as observables. If the context changes, the
corresponding observables change. However, the elements in the identity set of an autonomous agent are independent
of the environment or context in which the agent operates. Since abstract notions are part of the agent’s identity set, they
are also invariant of the context in which the agent operates. The key problem addressed in this paper is to translate the
changes in observables which are measurable from the environment, to the changes in associations with objects in the
identity set of an agent. For this, we define a schema for identity objects over the set of observables.
Consider a transcended agent a. Let there be a set of n observables in the given context, ¯co = (co1, co2, ...con). We
define the association between an identity object and an observable with a weight. This weight can be any non-negative
real number. The greater the weight, the greater the association, while weight 0 means the given observable is irrelevant
to the object in the identity set. The relevant objects in the identity set of agent a are the objects which hold a non-zero
weight for at least one observable in the given context.
The schema of the identity object is then defined as the normalised weights of the relevant objects in the identity set
over all the observables in a given context. For a relevant identity object oi in Ia of the agent a, the schema of oi is
defined as follows:
¯
soi = (si
1, si
2, ...si
n)
(1)
Σn
j=1si
j = 1
(2)
The association of an agent with each element in the identity set can be derived from the CT framework [7]. An agent a
with a transcendence level γ, having a semantic distance of d(oi) for an identity object oi, gives a weight of γd(oi) to
that object. These weights, coupled with the weights defined by the schemas, enable us to compute the final weights an
agent assigns to different observables in a context. These final weights can be interpreted as the preferences of the agent.
Let the identity set of a be Ia : {o1, o2, o3..., om} having m relevant objects in the given context. Subsequently let
each identity object oi have its corresponding schema ¯
soi = (si
1, si
2, ...si
n). Then, the preference vector ¯p over the
observables in that context is defined as follows:
¯p =
Transcending To Notions
u(ai) = (p1, p2, ..., pn)



u(coi
1)
u(coi
2)
.
u(coi
n)



(5)
u(ai) = ¯p.u( ¯coai)
(6)
Thus, the final utility of an action or a choice is the dot product of the preference vector ¯p and the vector of utilities
derived from the observables resulting from an action or a choice. The behaviour of an agent can now be modelled
using the standard Markov Decision Process framework. The sense of self of a transcended agent forms the basis of its
behaviour.
We now describe how the identity associations are updated. The agent receives some values for the observables as a
consequence of its actions. To update its semantic distances, the agent needs to keep track of the average values of
observables. These averages can then be used to calculate the utility of averaged observed values. Note that these
utilities can also be negative. Let the net utility vector of the observables be: ¯
nuco = (nuco1, nuco2..., nucom).
The utility functions map the range of observable values to the interval [−1, 1]. This normalisation ensures that despite
different ranges of different observables, the updates are consistent for the agent.
We use a threshold to check if the disparity between the net utilities over observables and the schema of an identity
object is high enough to induce a semantic distance change. For threshold τ, the semantic distance to an identity object
oi is updated only if:
γdold
a
(oi)
a
.(¯soi. ¯
nuco) > τ
(7)
Let ¯
soi be the schema of an identity object of the transcendence agent a. Then, the updated semantic distance to oi is
defined as follows:
dnew
a
(oi) = dold
a (oi) − lr.γdold
a
(oi)
a
.(¯soi. ¯
nuco)
(8)
The term ¯soi. ¯
nuco is the dot product of the schema of the identity object oi with the net utility vector. This term is
positive when a schema benefits an agent, and subsequently, the agent reduces its semantic distance to oi. On the other
hand, if the agent gets a negative utility due to a schema, it increases its semantic distance to it. However, this update is
only triggered when the term ¯soi. ¯
nuco crosses the pre-defined threshold τ. Thresholding ensures that small deviations
from the schema do not affect the sense of self of an agent.
Since the utility of the observables is constrained to [−1, 1], the term ¯soi. ¯
nuco is always constrained by 1, making the
utility calculations consistent. A transcended agent is said to be stabilised when it does not change its distance to any of
the identity objects in the given environment.
Thus, introducing schemas enables us to translate the utilities computed over the observables to the changes in the
identity set of an agent. The objects in the identity set can represent agents in the system, a collective of agents or
even notions. There is no standard theory for computing the utilities of these diverse entities and in turn to compute
the changes in identity associations. However, the utility of observables is well-studied. Especially theories from
behavioural economics, like the prospect theory [11], have mechanisms to estimate the utility of different observables.
Thus, introducing schemas bridges the gap between the existing literature and the problem of calculating the utilities of
abstract entities like notions.
4
Modelling Transit Choices
Having defined the formal model of autonomous agents that identify with notions, we demonstrate modelling transit
choices as a realistic use-case of this model. We translate different objects in the identity set to a composition of
observables in a specific context. We encounter the problem of estimating the utility of identity objects and show how
defining these objects as schemas over observables helps model identity-based artificial autonomous agents.
As discussed in the model, specific objects in the identity set of an agent become relevant in a specific context, which in
turn become drivers for the agent’s decision-making. For this, defining the context in which the agents are modelled
becomes imperative. We demonstrate an example where the agents in a network need to make decisions about their
transit choices. The transit is assumed to be between two fixed points on a periodic basis, such as traveling from home
5
Transcending To Notions
to work on a daily basis. The repetition of acts enables agents to gain feedback from their environment and update their
behavior. We have specifically modelled two transit choices, one depicting a private mode of transportation like a taxi
service and the other being a public mode of transportation such as a bus.
Various factors in the environment influence the transit choices in humans. For example, we factor cost, time, congestion,
and carbon footprint as the relevant contextual observables. Social factors also influence these decisions. We introduce
one such factor, conformity, in our model.
In the real world, a population of people is actually a composition of multiple autonomous agents. By modelling the
transit choices using artificial autonomous agents, we can simulate different types of populations and study their effects
on individuals and the system as a whole. Also, this model can reveal the kind of identity associations that lead to a
certain population’s behaviour. It can also be used by policymakers and system designers to understand and design
interventions in the system in order to achieve particular goals like reducing carbon footprint, improving the efficiency
of transit, etc.
4.1
Modelling observables
In the context of making a decision about transit choice, we have identified the following observables:
• Cost: The monetary cost incurred by an agent for travelling.
• Time: The time taken by an agent to get from source to destination.
• Congestion: The congestion in terms of occupancy and seating capacity of the mode of transit of the agent.
• Carbon footprint: The carbon footprint per head caused by the chosen mode of transit.
The advantage of dealing with observables is that these can be easily converted into utility. In the model, we use prospect
theory to convert the observables - cost, time, congestion, and carbon footprint per head to perceived utilities. All the
utility functions are defined such that they map an observable value to the range [−1, 1]. This helps in normalizing the
observables, since the ranges and units of the observables could vary a lot.
Prospect theory [21] is used to model the utility function for the observables– time, cost, and carbon footprint. It
proposes that individuals are risk averse, i.e. the magnitude of utility obtained by a profit of a certain payoff is less
than the magnitude of the utility lost by the loss of the same amount of payoff. Figure 1a shows the utility curve that
transforms the observable values of cost, time, and carbon footprint to respective utilities. We see that for the least
amount of time, cost, or carbon footprint, maximum utility is incurred whereas a maximum value of these observables
gives the lowest utility.
Further, the slope of the curve varies on either side of the average. On the positive side of utility, there is a gentle slope,
while on the negative side of utility, there is a steeper slope. Thus, there is a kink in the curve at utility zero.
The utility function for the observable congestion is modelled as a discontinuous function as shown in Figure 1b. As
long as the occupancy of the vehicle is less than the seating capacity, every passenger gets a seat, and thus, the utility is
1. However, when the occupancy exceeds the seating capacity, discomfort due to congestion is introduced as indicated
by the negative utility.
(a) Utility curve to model time, cost, and carbon footprint
(b) Utility curve to model congestion
Figure 1: Utility curves modelling observables
6
Transcending To Notions
4.2
Modelling Choices: Vehicles
Next, we describe how different choices in the context of transit have been modelled. Currently, we have modelled two
transit modes, taxi– representing private transport, and bus– representing public transport. However, other modes of
transport can also be modelled using this framework. The agent needs to choose between these two transit modes as its
commute choice.
For every vehicle on the road (both taxi and bus), the following metrics are tracked:
• avg cost: The average cost incurred in transit.
• avg time: The average time taken to transit from source to destination.
• dist: Distance to be travelled in the current trip.
• distribution for factors: Distribution function which can be used to estimate observables for a vehicle.
• capacity: The capacity of a vehicle.
• occupancy: Current occupancy of a vehicle. It is incremented when agents get assigned to a vehicle.
• maxOccupancy: The maximum passenger capacity of the vehicle.
• emission: The average carbon emission of the vehicle in grams per kilometre.
(a) Taxi travel time distribution
(b) Bus travel time distribution
Figure 2: Underlying probability distributions to model travel times for transit choices using a right-skewed distribution.
The mean and variances for each taxi and bus are highlighted respectively
These parameters can also extended to any mode of transport. For the experiments in this paper, we selected two points
in a metro city. Using Google Maps, we estimated the mean and variance of travel time and cost for both modes of
transport. Further details of the observables are as follows:
Cost A constant function is used to model the cost of transit for a given vehicle. Specifically, for transit between the
two selected points, the average cost of a bus is 20 units and for a taxi is 300 units.
Time A right-skewed distribution, namely the Gumbel distribution is used to model the time taken by the vehicles. The
right tail signifies the rare possibility of unusually high travel times, which corresponds to situations like accidents,
unforeseen traffic jams, etc. The time of the trip is measured in minutes. Figure 2 shows the underlying Gumbel
distributions for taxis and buses respectively.
Occupancy The occupancy of a vehicle is used to calculate the observables - congestion and carbon footprint per
head. Occupancy is modelled using two parameters namely - the seating capacity of the vehicle and the maximum
occupancy of the vehicle. The actual occupancy of a vehicle can be between zero and maximum occupancy. A Gaussian
distribution is used to model the occupancy of a bus, with seating capacity as the mean and one-third of the maximum
occupancy as the variance. This represents that on average, the occupancy of the bus is around its seating capacity.
Sampling from the Gaussian distribution returns a real number which is adjusted to the range of [0, max occupancy],
and then rounded off to the nearest whole number to return the occupancy of the bus. In the case of taxis, a discrete
probability distribution is used to model the occupancy. These distributions are plotted in Figure 3 for both taxis and
buses. Based on occupancy, the observables- congestion and carbon footprint per head for each vehicle are defined as
follows:
7
Transcending To Notions
Figure 3: Underlying probability distributions that model occupancy for transit choices of taxi and bus respectively
Congestion
Congestion =
Occupancy
Seating Capacity
(9)
The seating capacity for a taxi is 4, while that for a bus is 40.
Carbon foot print
Carbon foot print = Total emission
Occupancy
(10)
The total emission is calculated based on the estimates of the carbon emissions of vehicles. For taxis, it is estimated at
40 grams of carbon per kilometre, and for buses, it is estimated at 200 grams of carbon per kilometre.
4.3
Modelling an Agent
We now introduce a set of non-exhaustive relevant notions that could shape an agent’s identity in the context of making
transit choices.
Frugalism: Being frugal refers to the quality of being careful when using resources, especially money. In the context
of transit choice, this notion constitutes a behavioural choice that incurs lower monetary costs.
Idealism: Idealism as a notion refers to an ideology where agents are concerned about some ideal or utopian goal, and
the practicalities of a decision come next. In the case of transit choices, an ideal goal can be considered as the
sustainability of the environment, concern about climate change, etc.
Individualism: Individualism as a notion puts emphasis on the agent’s freedom, and individual identity. In the case
of transit choices, individualism emphasises the idea of individual comfort and individual time incurred in
commute.
Pragmatism: Pragmatism as a notion is about being practical. Pragmatism is concerned with all practically measurable
quantities in the given context.
As discussed earlier, in mapping notions in the agent’s identity to observables in the environment, we need a schema.
Table 1 represents the heuristic schema we have come up with to reconcile the definition of different notions with the
observables in the context of making transit choices.
```````````
Notion
Observable
Cost
Time
Congestion
C Footprint
Frugalism
3
5
1
5
1
5
0
Idealism
1
10
1
10
1
10
7
10
Individualism
2
10
3
10
5
10
0
Pragmatism
1
3
1
3
1
3
0
Table 1: Schemas of notions over observables
8
Transcending To Notions
Usually, agents representing a single notion are modeled in multi-agent setups. However, our extended CT framework
can be effectively used in representing the identity of each agent as a combination of multiple notions.
Agents are simulated in a network and based on their identity associations, they make transit choices.
They estimate the values of each observable for each transit choice probabilistically based on the perceived utility of
each transit choice over time. The perceived utility is computed based on the average value of observables. The sense
of self of the agent is represented in terms of identity associations with the notions. Also, agents update their identity
associations on a slower time scale based on the distance update logic discussed previously.
We also model conformity among the agents based on the network structure. Conformity introduces the social aspect to
the system and it models the way in which agents are affected by their neighbours’ transit choices. Let frac_neighci
be the fraction of neighbours who take the choice ci. Then, an additional utility component is added in the utility
computation of choice ci as follows:
u(ci)+ = cf ∗ frac_neighci
(11)
Here, cf is the conformity factor which is a measure of the extent of the conforming tendency of the agents in
the network. cf can be in the range of [0, 1], with 0 representing agents driven by their individual identities and 1
representing agents who are influenced by their neighbourhood as much as their individual preferences.
5
Experiments and Results
This section presents the results and inferences drawn from different simulation-based experiments. We varied different
initial parameters, like the distribution of the initial semantic distances, the amount of conformity in the network, etc. to
study the impact of these parameters on the transit choices of the agents. A network of agents is stabilized if semantic
distances are updated for less than 1% of the agents in the network.
5.1
Baseline
We first simulate a baseline case. Since conformity is ubiquitous in real-world human decision networks, we assume
a conformity factor of 0.2 in our baseline. The semantic distances to different notions for each agent are randomly
initialised by uniform sampling between d ∈ [0, 4],. We run the simulation on an Erd˝os–Rényi graph with 500 agents.
Each agent has a transcendence level γ = 0.8. With this initialisation of γ and d the value γd lies approximately
between 0.5 and 1, ensuring each notion has a significant impact on the agent’s behaviour at the beginning. Semantic
distance updates are performed after every epoch, which comprises 10 trips.
We run the simulation till the agents in the network stabilise. In the stabilised network, we note that only around 33.5%
of the population chooses public transport. Implying that in our baseline case, agents are inclined towards choosing
private transport.
Next, we examined the average variation in semantic distances of agents in the network for different notions in this
context. Figure 4: ’Whole Population’ - shows the trends of semantic distance to each notion averaged over the agents
with the progress of the trips.
We investigated the average semantic distances of the two sub-populations based on their choice for more insights on
how notions drive decisions. Figures 4 : ’Taxi Population,”Bus Population’ - aggregate results for sub-populations
that chose taxi and bus, respectively. The average semantic distances of these trip-wise sub-populations are plotted.
Note that these plots are plotted with every trip(not epoch) because of the stochastic nature of an agent’s decision;
the sub-population itself might change every trip, though semantic distance update only happens after an epoch. The
following inferences can be made from these plots:
• After stabilisation, in the sub-population of agents that choose taxis, individualism is the dominant notion (having the
lowest average semantic distance). While in the case of buses, the dominant notion is frugalism.
• Notions like frugalism and individualism are strong indicators of sub-population preferences. By strong indicators,
we mean the absolute difference between the average semantic distance to a notion between the sub-populations is
high. For strong indicators, this difference is greater than or equal to one.
• Notions like idealism and pragmatism are weak indicators of preferences. In this case, the absolute difference between
the average semantic distance to notions between the sub-populations is close to zero.
9
Transcending To Notions
Figure 4: Varying of average semantic distances of the total population and sub-populations, choosing a particular
transit choice, to different notions over the epochs
5.2
Varying Semantic Distances
In this experiment, we study the impact of the initialisation of semantic distances on the settled transit choice of the
population. To understand the impact of each notion on the settled transit choice, systematically, the average distance to
a specific notion is reduced. This is done by sampling the semantic distance for a specific notion uniformly from the
range of [0, 2] instead of [0, 4].
Table 2 summarises the four configurations where each of the four notions is given an average lower semantic distance,
implying closer association to that notion in the population. The blue and orange bars represent the initial and final
average semantic distance to each notion.
We observe that the initial distributions affect the settled population’s choice. Thus, there does not exist one unique
equilibrium in this complex system; rather, depending on the initial beliefs of the agents in the system, the resultant
behavior of the stabilized population emerges. It also implies that in order to achieve a specific system state, the
interventions must depend on the starting state of the system. A one-size-fits-all solution for policy makers.
The extended CT framework gives the capability to capture this diversity in agent behaviour. In the next section, we
analyse the effect of conformity in these networks.
5.3
Varying the Extent of Conformity
The edges of the network in which we carry out our simulation indicate an interaction between agents. The neighbour-
hood of an agent a consists of all the other agents in the network whose transit choice influences the decision of a. In
this formulation, we observe the effect of the strength of conformity between every agent and its neighbourhood. This
effect is captured by scaling the utility that an agent derives through conforming, which is proportional to the ratio of
agents in its neighbourhood with the same decision or transit choice during a trip.
10
Transcending To Notions
Frugalism
Idealism
Public transit proportion: 0.642
Public transit proportion: 0.476
Individualism
Pragmatism
Public transit proportion: 0.214
Public transit proportion: 0.336
Table 2: Initialization of average semantic distance to a particular notion reduced to see its effect on the stabilized
network.
The heatmap in Figure 5 shows the relation between the initial semantic distance distribution and the extent of conformity
among the agents in the network. The following observations can be made from this plot:
The extent of conformity influences the polarity of the population’s transit choice. We observe from Figure 5
that with an increase in the extent of conformity in the network, the polarity or the strength of transit choices of the
population increases. When there is no conformity among the agents, every agent makes choices independently based on
their identity associations with different notions. However, with high conformity among the agents, both an individual’s
identity and conformity with the neighbourhood affect their decision-making.
With the increase in conformity factor, the overall decision of the population of agents initialised towards a particular
notion tends to become polarised. This phenomenon can be understood as a consequence of the relative dominance
of utility derived from conforming over the utility derived from an individual’s identity associations. Hence, weak
preferences at a population level develop into an agent’s identity associations, bringing out a strong preference
collectively, and this loop keeps reinforcing an agent. However, when the conformity factor is lower, such polarization
cannot be observed because the initial weak preferences at the population level remain confined to the individual level.
The notion of conformity suggests that an agent must conform with its neighbours, which leads to a reduction in
the diversity of behavioural choices of the population. Thus, modelling autonomous agents using the extended CT
framework resurfaces the impact of conformity in a network of agents.
Strong and Weak Indicators of Transit Choice Notions like Frugalism and Individualism are strong indicators for
behavioural choices made by the agents, while Idealism and Pragmatism are weak indicators. We made this observation
in our first experiment. However, this also becomes apparent through this experiment. From Figure 5, we observe that
in scenarios where the population is initialised towards a notion that is a strong indicator of a behavioural choice, the
overall choice of the settled population becomes more polarised at relatively smaller conformity factors as compared to
the weak indicators.
11
Transcending To Notions
Figure 5: Heatmap of the effect of conformity and initialization of semantic distance to notions on the proportion of
stabilized population choosing public transport
6
Conclusions
Our world is a composition of multiple autonomous agents. By modelling the transit choices using artificial autonomous
agents, we can simulate different types of populations and study the effects of social factors and other factors that
influence the identity of individuals and impact the system as a whole. This model can reveal the kind of identity
associations that lead to emergent behaviours at a population level. Policymakers and system designers can further
use it to understand and design interventions in the system in order to achieve particular goals like reducing carbon
footprint, improving the efficiency of transit, etc.
The extended CT model helps to address the problem by modelling an identity in autonomous agents. Using this model,
we build agents that identify with different abstract notions to different extents. These abstract notions connect to
real-world observables using our proposed mechanism of schemas. While it is difficult to estimate the utility of abstract
notions directly, breaking them down as schemas over the observables enables us to compute and estimate their utilities.
This in turn helps to build autonomous agents having an identity that can dynamically adapt.
We use this to model autonomous agents making transit choices. The factors affecting these transit choices have been
identified in terms of direct observables like time, cost, congestion, environment, etc. Also, social factors play an
essential role in shaping their behaviour. The extended CT model effectively models agent-level behaviour in terms
of direct observables and also allows the introduction of network-level social factors like conformity. We believe this
framework is extensible to account for various individual and social factors relevant in a given context. Also, it can
be used to understand the effects of different policy interventions and help the system designers develop effective
interventions leading to sustainable population choices across diverse applications.
Acknowledgement
We would like to thank the Machine Intelligence and Robotics (MINRO) Center funded by the Government of Karnataka,
India and the Center for Internet of Ethical Things (CIET) funded by the Government of Karnataka, India and the World
Economic Forum for funding and supporting this work.
References
[1] Michael D Abràmoff, Philip T Lavin, Michele Birch, Nilay Shah, and James C Folk. Pivotal trial of an autonomous
ai-based diagnostic system for detection of diabetic retinopathy in primary care offices. NPJ digital medicine, 1
(1):39, 2018.
[2] Icek Ajzen. The theory of planned behavior. Organizational behavior and human decision processes, 50(2):
179–211, 1991.
[3] Beda Büchel and Francesco Corman. Review on statistical modeling of travel time variability for road-based
public transport. Frontiers in Built Environment, 6:70, 2020.
12
Transcending To Notions
[4] Ralph Buehler and John Pucher. Cycling to sustainability in amsterdam. Sustain, 2009.
[5] Longbing Cao. Ai in finance: challenges, techniques, and opportunities. ACM Computing Surveys (CSUR), 55(3):
1–38, 2022. doi:10.1145/3502289.
[6] Jayati Deshmukh and Srinath Srinivasa. Computational transcendence: Responsibility and agency. Frontiers in
Robotics and AI, 9, 2022.
[7] Jayati Deshmukh, Nikitha Adivi, and Srinath Srinivasa. Modeling application scenarios for responsible autonomy
using computational transcendence. In Proceedings of the 2023 International Conference on Autonomous Agents
and Multiagent Systems, pages 2496–2498, 2023.
[8] Frank Dignum, David Morley, Elizabeth A Sonenberg, and Lawrence Cavedon. Towards socially sophisticated
bdi agents. In Proceedings fourth international conference on multiagent systems, pages 111–118. IEEE, 2000.
[9] Marco Dorigo and Gianni Di Caro. Ant colony optimization: a new meta-heuristic. In Proceedings of the 1999
congress on evolutionary computation-CEC99 (Cat. No. 99TH8406), volume 2, pages 1470–1477. IEEE, 1999.
[10] Jon Doyle. A truth maintenance system. Artificial intelligence, 12(3):231–272, 1979.
[11] Kimberley D Edwards. Prospect theory: A literature review. International review of financial analysis, 5(1):
19–38, 1996.
[12] Naomi Ellemers, Russell Spears, and Bertjan Doosje. Self and social identity. Annual review of psychology, 53(1):
161–186, 2002.
[13] S. Franklin and A. Graesser. Is it an agent, or just a program?: A taxonomy for autonomous agents. In International
workshop on agent theories, architectures, and languages, page 2135, 1996.
[14] Alan G Ganek and Thomas A Corbi. The dawning of the autonomic computing era. IBM systems Journal, 42(1):
5–18, 2003.
[15] Younes Guessous, Maurice Aron, Neila Bhouri, and Simon Cohen. Estimating travel time distribution under
different traffic conditions. Transportation Research Procedia, 3:339–348, 2014.
[16] Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini
Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, et al. Development and validation of a deep learning
algorithm for detection of diabetic retinopathy in retinal fundus photographs. Jama, 316(22):2402–2410, 2016.
[17] Banafsheh Hajinasab, Paul Davidsson, Jan A Persson, and Johan Holmgren. Towards an agent-based model of
passenger transportation. In Multi-Agent Based Simulation XVI: International Workshop, MABS 2015, Istanbul,
Turkey, May 5, 2015, Revised Selected Papers 16, pages 132–145. Springer, 2016.
[18] Jiangyan Huang, Youkai Cui, Lele Zhang, Weiping Tong, Yunyang Shi, and Zhiyuan Liu. An overview of
agent-based models for transport simulation and analysis. Journal of Advanced Transportation, 2022, 2022.
[19] Fei Jiang, Yong Jiang, Hui Zhi, Yi Dong, Hao Li, Sufeng Ma, Yilong Wang, Qiang Dong, Haipeng Shen, and
Yongjun Wang. Artificial intelligence in healthcare: past, present and future. Stroke and vascular neurology, 2(4),
2017.
[20] John Johnston, Morris Moscovitch, Carlo Umiltà, et al. The allure of machinic life: Cybernetics, artificial life,
and the new AI. MIT Press, 2008.
[21] Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. Econometrica, 47(2):
363–391, 1979.
[22] James Kennedy. Swarm intelligence. Springer, 2006.
[23] Jeffrey O Kephart and David M Chess. The vision of autonomic computing. Computer, 36(1):41–50, 2003.
[24] Hamid Khayyam, Bahman Javadi, Mahdi Jalili, and Reza N Jazar. Artificial intelligence and internet of things
for autonomous vehicles. Nonlinear Approaches in Engineering Applications: Automotive Applications of
Engineering Problems, pages 39–68, 2020.
[25] Maciej Komosinski and Andrew Adamatzky. Artificial life models in software. Springer Science & Business
Media, 2009.
[26] Yifang Ma, Zhenyu Wang, Hong Yang, and Lin Yang. Artificial intelligence applications in the development of
autonomous vehicles: A survey. IEEE/CAA Journal of Automatica Sinica, 7(2):315–329, 2020.
[27] Humberto R Maturana and Francisco J Varela. Autopoiesis and cognition: The realization of the living, volume 42.
Springer Science & Business Media, 1991.
13
Transcending To Notions
[28] Ali Najmi, Travis Waller, Mehrdad Memarpour, Divya Nair, and Taha H. Rashidi. A human behaviour model and
its implications in the transport context. Transportation Research Interdisciplinary Perspectives, 18:100800, 2023.
ISSN 2590-1982. doi:https://doi.org/10.1016/j.trip.2023.100800. URL https://www.sciencedirect.com/
science/article/pii/S2590198223000477.
[29] Andrea Omicini, Alessandro Ricci, and Mirko Viroli. Artifacts in the a&a meta-model for multi-agent systems.
Autonomous agents and multi-agent systems, 17:432–456, 2008.
[30] Amartya K Sen. Rational fools: A critique of the behavioral foundations of economic theory. Philosophy &
Public Affairs, pages 317–344, 1977.
[31] Chaoyang Shi, Bi Yu Chen, and Qingquan Li. Estimation of travel time distributions in urban road networks using
low-frequency floating car data. ISPRS International Journal of Geo-Information, 6(8):253, 2017.
[32] Yuchao Cui; Hongzhi Guan; Zhengtao Qin; Yang Si. Research on the choice behavior of different types of
ride-hailing services. 20th COTA International Conference of Transportation, pages 3807–3819, 2020.
[33] Herbert A Simon. Bounded rationality. Utility and probability, pages 15–18, 1990.
[34] Srinath Srinivasa and Jayati Deshmukh. Paradigms of computational agency. CoRR, abs/2112.05575, 2021. URL
https://arxiv.org/abs/2112.05575.
[35] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
[36] Henri Tajfel and John C Turner. The social identity theory of intergroup behavior. In Political psychology, pages
276–293. Psychology Press, 2004.
[37] Kay W Axhausen, Andreas Horni, and Kai Nagel. The multi-agent transport simulation MATSim. Ubiquity Press,
2016.
[38] Xiao-lin Zheng, Meng-ying Zhu, Qi-bing Li, Chao-chao Chen, and Yan-chao Tan. Finbrain: when finance meets
ai 2.0. Frontiers of Information Technology & Electronic Engineering, 20(7):914–924, 2019.
14
"
"In 2018, Yang et al. introduced a novel and effective approach, using maximum distance separable (MDS) codes, to mitigate the impact of elasticity in cloud computing systems. This approach is referred to as coded elastic computing. Some limitations of this approach include that it assumes all virtual machines have the same computing speeds and storage capacities, and it cannot tolerate stragglers for matrix-matrix multiplications. In order to resolve these limitations, in this paper, we introduce a new combinatorial optimization framework, named uncoded storage coded transmission elastic computing (USCTEC), for heterogeneous speeds and storage constraints, aiming to minimize the expected computation time for matrix-matrix multiplications, under the consideration of straggler tolerance. Within this framework, we propose optimal solutions with straggler tolerance under relaxed storage constraints. Moreover, we propose a heuristic algorithm that considers the heterogeneous storage constraints. Our results demonstrate that the proposed algorithm outperforms baseline solutions utilizing cyclic storage placements, in terms of both expected computation time and storage size.","Elasticity allows virtual machines in a cloud system to be preempted or become available during computing rounds, leading to computation failure or increased computation time. In [1], the authors proposed a cyclic computation assignment that utilizes maximum distance separable (MDS) coded storage for homogeneous systems, where all machines have the same computation speed and storage capacity. For MDS coded storage elastic computing, the authors in [2] introduced a combinatorial optimization approach aimed at minimizing overall computation time for systems with heterogeneous computing speeds and storage constraints. They proposed an optimal solution using a low-complexity iterative algorithm, called the filling algorithm. Subsequently, in [3], the author extended the filling algorithm to address scenarios with both elasticity and stragglers. In [4], the authors introduced two hierarchical schemes designed to speed up computing and tolerate stragglers, by letting fewer machines select their first computation tasks to work on and more machines select their last computation tasks. In [5], a new metric named transition waste was introduced, quantifying unnecessary changes in computation tasks caused by elasticity. To mitigate this, the authors minimized the transition waste among all cyclic computation assignments and constructed several computation assignments that achieve zero transition waste.","nanDespite the advantages of MDS coded storage elastic computing, they are limited to certain types of computations, such as linear computations. To overcome this limitation, the authors in [6] introduced uncoded storage uncoded transmission elastic computing for heterogeneous systems. They formulated a combinatorial optimization problem and derived optimal solutions with the goal of minimizing the overall computation time for a given storage placement. Most of the existing works in elastic computing, including [1]–[3], [5], [6], primarily focus on matrix-vector multiplications and utilize uncoded transmission during the communication phase. In [7], the authors proposed a coded storage coded transmission elastic computing scheme for matrix-matrix multiplications. However, this scheme cannot tolerate stragglers, as the MDS coded storage placement and transmission fix the number of machines contributing to the decoding process.","In this paper, we introduce the uncoded storage coded transmission elastic computing (USCTEC) for systems with heterogeneous computation speeds and storage constraints. We first formulate a new optimization framework aimed at minimizing the expected computation time over a random distribution of computation speeds, using Lagrange codes, introduced in [8], to design coded transmission and computation. Next, we design optimal USCTEC schemes with straggler tolerance, given any computation speed and no storage constraints. In this design, each machine stores a fraction of dataset. Furthermore, we propose a heuristic algorithm that considers storage constraints for general speed distributions. Finally, our results show that the proposed algorithm outperforms baseline algorithms that utilize cyclic storage placement, in terms of both expected computation time and required storage size.","It can be seen that in each time step both storage selection and computation assignment, which are determined by γ and M, need to be designed. In each time step, the system adjust to a corresponding USCTEC scheme, denoted by (γ, M). Our goal is to minimize the expected computation time in Definition 5 by jointly designing a set of schemes Tℬo and the storage placement Z. We can formulate the following combinatorial optimization problem, arg minZ,Tℬo C(Z, Tℬo) (9a) s.t. 0 ≤ |Zn|q ≤ e[n] ≤ 1, ∀n ∈ [N], (9b) ∀(γ, M) ∈ Tℬo : Xg∈[G] γ[g] = 1, 0 ≤ γ[g] ≤ 1, ∀g ∈ [G], (9c) [f∈[Fg] Mg,f = hg rL i, ∀g ∈ [G], (9d) Pg,f ⊆ Ug, ∀f ∈ [Fg], g ∈ [G], (9e) |Pg,f| = L + S, ∀f ∈ [Fg], g ∈ [G], (9f) where (9b) represents storage constraints. Each USCTEC scheme ((γ, M) corresponding to a speed realization satisfies constraints (9c)-(9f). (9c) ensures that each row in matrix A is computed by available machines. (9d) ensures that each column in Bl, l ∈ [L], is assigned to be computed by available machines. (9e) ensures that the assigned machines have stored Ag. (9f) ensures that each column is assigned to L + S available machines, providing the straggler tolerance of S.The optimization problem presented in (9) is inherently combinatorial, making it challenging to find the optimal solutions. In the following sections, we will propose sub-optimal solutions in two steps. 1) We will relax the storage constraint (9b) by setting e[n] = 1 for all n ∈ [N], and find optimal solutions for a given speed realization. 2) We will develop a heuristic algorithm for general speed distributions, considering the storage constraint (9b). This algorithm will be based on the approach developed in Step 1).","We compare Algorithm 2 with USCTEC systems based on cyclic storage strategy presented in [6]. We use the following example to compare the storage size and expected computation time obtained by two USCTEC systems. Consider a system with N = 12, L = 2, S = 1, and two speed realizations s1 and s2 with equal probabilities, where s1 = (1, 1, 2, 2, 2, 3, 8, 8, 8, 8, 9, 9) and s2 = (8, 8, 2, 3, 9, 9, 2, 1, 8, 5, 2, 8). We define the storage constraint as e = ( Q12, …, Q12) of length 12, where Q ∈ {6, 7, 8, 9, 10, 11, 12}.The USCTEC system based on cyclic storage placement [6] operates as follows. First, each machine utilizes the full storage capacity by defining γ = ( 112, …, 112) of length 12, and letting the n-th machine store Q blocks An%N, …, A(n+Q−1)%N, where we define a%N ≜ a∇⌊ a−1N N. Second, it can be shown that, given the storage placement, the system achieves the minimum computation time. Specifically, For g ∈ [12], Ug is the set of all machines that store block Ag, and μ[g] is the solution to (L + S, sUg, 1)-LP, where sUg is a vector containing the computation speeds of machines in Ug. By varying storage constraints, we have comparisons as shown in Table I.",Uncoded Storage Coded Transmission Elastic Computing with Straggler Tolerance in Heterogeneous Systems,"Xi Zhong, Joerg Kliewer, Mingyue Ji","Uncoded Storage Coded Transmission Elastic
Computing with Straggler Tolerance in
Heterogeneous Systems
Xi Zhong1, Jörg Kliewer2 and Mingyue Ji1
1Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, USA
Email: {xi.zhong, mingyue.ji}@utah.edu
2Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA
Email: jkliewer@njit.edu
Abstract—In 2018, Yang et al. introduced a novel and effective
approach, using maximum distance separable (MDS) codes, to
mitigate the impact of elasticity in cloud computing systems. This
approach is referred to as coded elastic computing. Some limita-
tions of this approach include that it assumes all virtual machines
have the same computing speeds and storage capacities, and it
cannot tolerate stragglers for matrix-matrix multiplications. In
order to resolve these limitations, in this paper, we introduce
a new combinatorial optimization framework, named uncoded
storage coded transmission elastic computing (USCTEC), for
heterogeneous speeds and storage constraints, aiming to minimize
the expected computation time for matrix-matrix multiplications,
under the consideration of straggler tolerance. Within this
framework, we propose optimal solutions with straggler tolerance
under relaxed storage constraints. Moreover, we propose a
heuristic algorithm that considers the heterogeneous storage
constraints. Our results demonstrate that the proposed algorithm
outperforms baseline solutions utilizing cyclic storage placements,
in terms of both expected computation time and storage size.
I. INTRODUCTION
Elasticity allows virtual machines in a cloud system to
be preempted or become available during computing rounds,
leading to computation failure or increased computation time.
In [1], the authors proposed a cyclic computation assign-
ment that utilizes maximum distance separable (MDS) coded
storage for homogeneous systems, where all machines have
the same computation speed and storage capacity. For MDS
coded storage elastic computing, the authors in [2] introduced
a combinatorial optimization approach aimed at minimizing
overall computation time for systems with heterogeneous
computing speeds and storage constraints. They proposed an
optimal solution using a low-complexity iterative algorithm,
called the filling algorithm. Subsequently, in [3], the author
extended the filling algorithm to address scenarios with both
elasticity and stragglers. In [4], the authors introduced two
hierarchical schemes designed to speed up computing and
tolerate stragglers, by letting fewer machines select their first
computation tasks to work on and more machines select their
last computation tasks. In [5], a new metric named transi-
tion waste was introduced, quantifying unnecessary changes
in computation tasks caused by elasticity. To mitigate this,
the authors minimized the transition waste among all cyclic
computation assignments and constructed several computation
assignments that achieve zero transition waste.
Despite the advantages of MDS coded storage elastic com-
puting, they are limited to certain types of computations, such
as linear computations. To overcome this limitation, the au-
thors in [6] introduced uncoded storage uncoded transmission
elastic computing for heterogeneous systems. They formulated
a combinatorial optimization problem and derived optimal
solutions with the goal of minimizing the overall computation
time for a given storage placement.
Most of the existing works in elastic computing, including
[1]–[3], [5], [6], primarily focus on matrix-vector multiplica-
tions and utilize uncoded transmission during the communica-
tion phase. In [7], the authors proposed a coded storage coded
transmission elastic computing scheme for matrix-matrix mul-
tiplications. However, this scheme cannot tolerate stragglers,
as the MDS coded storage placement and transmission fix the
number of machines contributing to the decoding process.
In this paper, we introduce the uncoded storage coded
transmission elastic computing (USCTEC) for systems with
heterogeneous computation speeds and storage constraints. We
first formulate a new optimization framework aimed at mini-
mizing the expected computation time over a random distribu-
tion of computation speeds, using Lagrange codes, introduced
in [8], to design coded transmission and computation. Next,
we design optimal USCTEC schemes with straggler tolerance,
given any computation speed and no storage constraints.
In this design, each machine stores a fraction of dataset.
Furthermore, we propose a heuristic algorithm that considers
storage constraints for general speed distributions. Finally, our
results show that the proposed algorithm outperforms baseline
algorithms that utilize cyclic storage placement, in terms of
both expected computation time and required storage size.
Notation: F denotes a finite field, and R denotes the real
field. We use | · | to represent the cardinality of a set or the
length of a vector, and [n] = {1, 2, . . . , n}. Let a[i] denote
the i-th element of vector a, µ[i, j] denote the entry [i, j] of
matrix µ, and µ[i] denote the i-th row of µ. We use (B)D to
represent the sub-matrix of B with column indices D.
arXiv:2401.12151v1  [cs.IT]  22 Jan 2024
II. SYSTEM MODEL AND PROBLEM FORMULATION
We consider a distributed system consisting of a master node
and N virtual machines, denoted by [N]. The computation
speed is represented by a random vector s = (s[1], · · · s[N]),
where s[n] represents the number of row-column multiplica-
tions that machine n can compute per unit of time. The sample
space of the speed distribution is denoted as Ωs. Given a data
matrix A ∈ Fq×v, at each time step t, with the computation
speed realization s(t) ∈ Ωs and the input matrix B(t) ∈ Fv×r,
a set of Nt available machines, known in the beginning of each
step time and denoted as Nt = {n ∈ [N] : s(t)[n] > 0}, aims
to recover AB(t) while tolerating up to S stragglers. Define
L as the recovery threshold, which is the minimum number of
machines required for successful decoding. In the following,
we explain how a USCTEC system operates.
A. Storage Placement and Storage Selections
Each machine n ∈ [N] stores a subset of rows of the data
matrix A, denoted by Zn. The storage placement of the system
is denoted by Z = {Zn : n ∈ [N]}. The storage constraint is
presented by a vector e = (e[1], · · · , e[N]), where 0 ≤ e[n]
≤ 1 for n ∈ [N], and e[n] indicates the maximum storage size
of machine n, normalized by the size of A, i.e., |Zn|
q
≤ e[n].
In each time step t, machine n ∈ Nt selects a subset of
its storage I(t)
n
⊆ Zn for computation tasks. Let I(t) =
{I(t)
n
: n ∈ Nt}. We obtain a specific I(t) by generating
a partitioning vector γ(t) and a set U(t). Specifically, γ(t) =
(γ(t)[1], · · · , γ(t)[G(t)]) partitions A into G(t) disjoint row
blocks, denoted as A = {Ag ∈ Fqγ(t)[g]×v : g ∈ [G(t)]}, where
P
g∈[G(t)] γ(t)[g] = 1 and 0 < γ(t)[g] ≤ 1 for g ∈ [G(t)].
Next, we generate U(t) = {U(t)
g
: g ∈ [G(t)]}. Each U(t)
g
is
denoted as the selected machines for Ag, where U(t)
g
⊆ Nt,
|U(t)
g | ≥ L + S and each machine in U(t)
g
stores Ag. Hence,
the storage selection for machine n is obtained by
I(t)
n
= {Ag : n ∈ U(t)
g , g ∈ [G(t)]}.
(1)
Note that B(t), I(t), γ(t) and U(t) may change with each
time step, but for simplicity, we omit the reference to the time
step t and denote (·)(t) as (·).
B. Communication Phase
The master partitions matrix B into L blocks of equal size,
denoted as B = {Bl ∈ Fv× r
L : l ∈ [L]}. Each Bl consists of
r
L columns, indexed by [ r
L]. As a result, AB consists of G
sets of blocks {AgBl : l ∈ [L]} for g ∈ [G]. Each set will be
recovered by the computation results from selected machines
Ug. To assign computation tasks to Ug for all g ∈ [G], we
define the computation assignment M.
Definition 1: (Computation Assignment) The computation
assignment of the system is M = {(Mg, Pg) : g ∈ [G]},
where the pair (Mg, Pg) is the computation assignment for
machines in Ug. Mg = {Mg,f : f ∈ [Fg]} represents an Fg-
partition of the column indices [ r
L], i.e., S
f∈[Fg] Mg,f = [ r
L].
Pg = {Pg,f : f ∈ [Fg]} consists of Fg sets of machines,
where Pg,f ⊆ Ug and |Pg,f| = L + S. We denote that the
machines in Pg,f are assigned to the indices Mg,f, as they will
be assigned to computation tasks associated with the columns
in Bl with indices Mg,f, for all l ∈ [L].
Based on M, the indices assigned to machine n ∈ [N]
are denoted as Dn,g = S
f∈[Fg]:n∈Pg,f Mg,f if n ∈ Ug;
otherwise, Dn,g = ∅. The overall assigned indices for machine
n are Dn = S
g∈[G] Dn,g. To generate coded matrices for
transmission, we use Lagrange codes introduced in [8]. due
to the low complexity and the capacity of straggler tolerance.
Specifically, the master selects L numbers {βl ∈ F : l ∈ [L]}
and Nt numbers {αn ∈ F : n ∈ Nt} such that {αn : n ∈
Nt} ∩ {βl : l ∈ [L]} = ∅. The master computes and sends the
following coded matrix to machine n ∈ Nt,
˜
Bn =
X
l∈[L]
(Bl)Dn ·
Y
k∈[L]\{l}
αn − βk
βl − βk
.
(2)
C. Computing Phase and Decoding Phase
For g ∈ [G], machine n ∈ Ug computes and sends the
following matrix to the master,
Hg,n = Ag( ˜
Bn)Dn,g.
(3)
For each block AgBl, l ∈ [L], the master decodes sub-block
Ag(Bl)Mg,f , using the computation results from machines in
Pg,f ⊆ Ug. To do this, we define Fg polynomials Hg,f(z)
with a degree of L − 1 for f ∈ [Fg], where
Hg,f(z) = Ag · Vg,f(z),
(4)
Vg,f(z) =
X
l∈[L]
(Bl)Mg,f ·
Y
k∈[L]\{l}
z − βk
βl − βk
.
(5)
For each Hg,f(z), f ∈ [Fg], we have two observations. First,
from (5), we have Vg,f(βl) = (Bl)Mg,f for l ∈ [L]. From
(4), we have Hg,f(βl) = Ag(Bl)Mg,f , i.e., the sub-block is
the evaluation of the polynomial Hg,f(z) at βl. Second, due
to Mg,f ⊆ Dn,g ⊆ Dn, from (2) and (5), we have
( ˜
Bn)Mg,f = Vg,f(αn)
(6)
for all n ∈ Pg,f. Then, Hg,f(αn)
(a)
= AgVg,f(αn)
(b)
=
Ag( ˜
Bn)Mg,f
(c)
= (Hg,n)Mg,f , where (a) is due to (4), (b) is
due to (6) and (c) is due to (3). In other words, the sub-matrix
of computation result, i.e., (Hg,n)Mg,f , is the evaluation of the
polynomial Hg,f(z) at αn. Therefore, decoding Ag(Bl)Mg,f
for l ∈ [L] and f ∈ [Fg] means interpolating the polynomial
Hg,f(z) using the computation results (Hg,n)Mg,f from any
L machines in Pg,f, denoted by Lg,f, and evaluating Hg,f(βl).
Using Lagrange interpolation, the master computes
Hg,f(βl)=
X
n∈Lg,f
(Hg,n)Mg,f ·
Y
n′∈Lg,f \{n}
βl − αn′
αn − αn′ =Ag(Bl)Mg,f .
By combining Ag(Bl)Mg,f for all l ∈ [L] and f ∈ [Fg],
the master can recover the set of blocks {AgBl : l ∈ [L]}.
By executing the processes above for all g ∈ [G], the master
can recover all sets of blocks and outputs AB. Notably,
Lagrange codes ensure that the USCTEC scheme tolerates up
to S stragglers, since L + S machines in Pg,f are assigned to
compute L+S distinct evaluations of the polynomial Hg,f(z),
while successful decoding requires any L machines.
It can be seen that in each time step both storage selection
and computation assignment, which are determined by γ and
M, need to be designed. In each time step, the system adjust
to a corresponding USCTEC scheme, denoted by (γ, M).
D. USCTEC with Straggler Tolerance Problem Formulation
For a USCTEC system with a random computation speed
s, the goal is to minimize the expected computation time (see
Definitions 4 and 5). To formulate the problem, we introduce
the following four definitions.
Definition 2: (Load Division Matrix) For a USCTEC
scheme (γ, M), the load division matrix is denoted as µ ∈
RG×N. Each entry µ[g, n] represents the normalized number
of columns multiplied by machine n for row block Ag, i.e.,
µ[g, n] =
( |Dn,g|
r/L
if n ∈ Ug,
0
otherwise,
(7)
where 0 ≤ µ[g, n] ≤ 1 for all g ∈ [G] and n ∈ [N].
Using µ, we can represent Ug = {n ∈ [N] : µ[g, n] > 0}
for g ∈ [G]. Hence, from (1), the storage selection I = {In :
n ∈ Nt} can be represented by the pair (γ, µ), where
In = {Ag : µ[g, n] > 0, g ∈ [G]}.
(8)
Definition 3: (Computation Load) For a USCTEC scheme
(γ, M) with a load division matrix µ, the computation load
vector is defined as θ = (θ[1], · · · , θ[N]), where θ[n] =
P
g∈[G] γ[g] · µ[g, n] for n ∈ [N], i.e., θ = γ · µ.
The computation load vector represents the normalized num-
ber of row-column multiplications computed by each machine.
Definition 4: (Computation Time) Given a time step with a
computation speed realization s ∈ Ωs and a USCTEC scheme
(γ, M), the computation time is defined as c(γ, M) ≜
maxn∈Nt
θ[n]
s[n] = maxn∈Nt
P
g∈[G] γ[g]·µ[g,n]
s[n]
.
Definition 5: (Expected Computation Time) Given a
USCTEC system with a speed distribution s and a stor-
age placement Z that supports a set of USCTEC schemes
TΩs = {(γ, M)}, the expected computation time is defined
as C(Z, TΩs) = Es[c(γ, M)].
Our goal is to minimize the expected computation time in
Definition 5 by jointly designing a set of schemes TΩs and
the storage placement Z. We can formulate the following
combinatorial optimization problem,
arg min
Z,TΩs
C(Z, TΩs)
(9a)
s.t. 0 ≤ |Zn|
q
≤ e[n] ≤ 1, ∀n ∈ [N],
(9b)
∀(γ, M) ∈ TΩs :
X
g∈[G]
γ[g] = 1, 0 ≤ γ[g] ≤ 1, ∀g ∈ [G],
(9c)
[
f∈[Fg]
Mg,f =
h r
L
i
, ∀g ∈ [G],
(9d)
Pg,f ⊆ Ug, ∀f ∈ [Fg], g ∈ [G],
(9e)
|Pg,f| = L + S, ∀f ∈ [Fg], g ∈ [G],
(9f)
where (9b) represents storage constraints. Each USCTEC
scheme (γ, M) corresponding to a speed realization satisfies
constraints (9c)-(9f). (9c) ensures that each row in matrix A
is computed by available machines. (9d) ensures that each
column in Bl, l ∈ [L], is assigned to be computed by available
machines. (9e) ensures that the assigned machines have stored
Ag. (9f) ensures that each column is assigned to L + S
available machines, providing the straggler tolerance of S.
The optimization problem presented in (9) is inherently
combinatorial, making it challenging to find the optimal solu-
tions. In the following sections, we will propose sub-optimal
solutions in two steps. 1) We will relax the storage constraint
(9b) by setting e[n] = 1 for all n ∈ [N], and find optimal
solutions for a given speed realization. 2) We will develop a
heuristic algorithm for general speed distributions, considering
the storage constraint (9b). This algorithm will be based on
the approach developed in Step 1).
III. OPTIMAL USCTEC SCHEMES WITHOUT STORAGE
CONSTRAINTS FOR A GIVEN SPEED REALIZATION
A. Problem Analysis and An Illustrative Example
With the relaxed storage constraint e = 1, where 1 is an
all-1 vector, and given a speed realization s, we let machines
utilize their entire storage, i.e., In = Zn for n ∈ Nt. Problem
(9) is reformulated as the following optimization problem,
arg min
γ,M
c(γ, M)
(10a)
s.t.
X
g∈[G]
γ[g] = 1, 0 ≤ γ[g] ≤ 1, ∀g ∈ [G],
(10b)
[
f∈[Fg]
Mg,f =
h r
L
i
, ∀g ∈ [G],
(10c)
Pg,f ⊆ Ug, ∀f ∈ [Fg], g ∈ [G],
(10d)
|Pg,f| = L + S, ∀f ∈ [Fg], g ∈ [G].
(10e)
Based on Definition 4, the computation time c(γ, M) is
fixed when the computation load vector θ is fixed. This
insight prompts us to decompose problem (10) into three sub-
problems. First, we solve the optimal computation load vector
θ∗ that minimizes the computation time. Next, we show the
existence of a storage placement Z∗, induced by a partitioning
vector γ∗ and a load division matrix µ∗ as shown in (8),
where γ∗ · µ∗ = θ∗. Finally, we prove the existence of a
computation assignment M∗ that satisfies µ∗. Therefore, an
optimal USCTEC scheme (γ∗, M∗) is obtained.
Example 1: When N = 6, L = 2, S = 1 and s = (3, 3, 4,
4, 5, 5), the optimal computation load vector is θ∗ = ( 3
8, 3
8, 1
2,
1
2, 5
8, 5
8), which ensures that all machines complete computing
at the same time, resulting in a minimum computation time of
c∗ = 1
8. Let γ∗ = ( 3
8, 1
4, 1
8, 1
8, 1
8) and
µ∗ =


1
0
0
0
1
1
0
0
1
1
1
0
0
1
1
0
0
1
0
1
1
1
0
0
0
1
0
1
0
1

,
(11)
such that θ∗ = γ∗ · µ∗. Using γ∗, the matrix A is divided
into G = 5 row blocks. Using µ∗, the storage placement from
(8) is as follows. Z∗
1 = {A1}, Z∗
2 = {A3, A4, A5}, Z∗
3 =
{A2, A3, A4}, Z∗
4 = {A2, A4, A5}, Z∗
5 = {A1, A2} and
Z∗
6 = {A1, A3, A5}. The sets of selected machines are U∗
1 =
{1, 5, 6}, U∗
2 = {3, 4, 5}, U∗
3 = {2, 3, 6}, U∗
4 = {2, 3, 4} and
U∗
5 = {2, 4, 6}. Next, we provide a computation assignment
M∗. Since µ∗[g, n] = 1 for n ∈ U∗
g , the indices assigned to
each machine n are Dn,g = [ r
2] from (7). Since |U∗
g | = 3 and
the requirement of |Pg,f| = 3 for f ∈ [Fg], we let Fg = 1 for
all g ∈ [5], i.e., M∗
g = {[ r
2]} and P∗
g = {U∗
g }. Therefore, we
obtain the optimal USCTEC scheme (γ∗, M∗).
We will describe the detailed solution as follows.
B. Optimal Computation Load Problem
In this section, we find the optimal computation load.
We introduce the (l, s, σ)-Load Problem, where σ is a load
constraint vector of length N, and σ[n] is the maximum load
that machine n ∈ [N] can be assigned. This problem is used
not only for a given speed realization but also for general
speed distributions with storage constraints in Section IV.
Definition 6: ((l, s, σ)-Load Problem (LP)) Given 0 ≤ l ≤
L+S, a speed realization s and a vector σ = (σ[1], · · · , σ[N]),
where l ≤ P
n∈Nt σ[n] and 0 ≤ σ[n] ≤ 1 for all n ∈ [N], the
goal is to find the solution to
min
θ
max
n∈Nt
θ[n]
s[n]
(12a)
s.t.
X
n∈Nt
θ[n] = l,
(12b)
0 ≤ θ[n] ≤ σ[n] ≤ 1,
∀n ∈ Nt,
(12c)
θ[n] = 0,
∀n ∈ [N] \ Nt.
(12d)
The (l, s, σ)-LP is a convex optimization problem. In fact, its
analytical solution can be obtained using Theorem 1 in [2].
Theorem 1: When l = L + S and σ = e = 1, the optimal
computation load vector, induced by the solution to problem
(10), is the solution to (L + S, s, 1)-LP, without considering
an explicit storage placement and computation assignment.
Proof: Given the optimal solution to problem (10),
(12c) is satisfied, due to θ[n] = P
g∈[G] γ[g] · µ[g, n] =
P
g∈[G]:µ[g,n]>0 γ[g] · µ[g, n]
(a)
≤ P
g∈[G]:µ[g,n]>0 γ[g]
(b)
= |Zn|
q
≤ e[n] = 1, where (a) is due to µ[g, n] ≤ 1 from (7), and
(b) is due to (8). To show (12b), we first claim the following
constraint of the load division matrix,
X
n∈[N]
µ[g, n] = L + S
(13)
for g ∈ [G]. This is due to P
n∈[N] µ[g, n]
(a)
=
P
n∈Ug |Dn,g|
r/L
=
P
n∈Ug
P
f∈[Fg]:n∈Pg,f |Mg,f |
r/L
(b)
=
P
f∈[Fg]
P
n∈Pg,f |Mg,f |
r/L
(c)
=
P
f∈[Fg](L+S)·|Mg,f |
r/L
(d)
= L + S, where (a) is due to (7), (b)
is due to (10d), (c) is due to (10e) and (d) is due to (10c).
Hence, P
n∈Nt θ[n] = P
n∈[N]
P
g∈[G] γ[g]µ[g, n] = P
g∈[G]

γ[g] · P
n∈[N] µ[g, n]

= P
g∈[G] γ[g] · (L + S) = L + S.
C. Storage Placement Problem
To obtain a partitioning vector γ and a load division matrix
µ, given a load vector θ, we introduce the (θ, ρ)-Division
Problem, where ρ is the sum of γ and represents a fraction
of the data matrix A to be partitioned. In problem (10), we
consider ρ = 1, while ρ ̸= 1 will be used in Section IV.
Definition 7: ((θ, ρ)-Division Problem (DP)) Given a com-
putation load vector θ ∈ RN and 0 ≤ ρ ≤ 1, where
P
n∈[N] θ[n] = (L + S)ρ and 0 ≤ θ[n] ≤ ρ, the goal is
to find a vector γ ∈ RG and a matrix µ ∈ RG×N such that
θ = γ · µ,
(14a)
X
g∈[G]
γ[g] = ρ, 0 ≤ γ[g] ≤ 1, ∀g ∈ [G],
(14b)
X
n∈[N]
µ[g, n] = L + S, ∀g ∈ [G],
(14c)
0 ≤ µ[g, n] ≤ 1,
∀n ∈ Nt, g ∈ [G],
(14d)
µ[g, n] = 0, ∀n ∈ [N] \ Nt.
(14e)
Theorem 2: The solution to (θ∗, 1)-DP consists of the
partitioning vector and load division matrix induced by the
optimal solution to problem (10), without considering an
explicit computation assignment M, where θ∗ is the optimal
computation load obtained from (L + S, s, 1)-LP.
Proof: For any solution to (θ∗, 1)-DP, i.e., γ∗ and µ∗,
we let γ∗ be the partitioning vector in problem (10), as
(10b) is satisfied from (14b). Let µ∗ be the load division
matrix induced by the solution to problem (10), as (13) is
satisfied from (14c). From (14a), any computation assignment
satisfying µ∗ achieves the optimal computation time.
To derive a solution to (θ, ρ)-DP, we specify (14d) as
µ[g, n] = 1 or 0, such that the desired binary matrix µ contains
L + S “1”s in each row. We denote the specified problem as
Binary-(θ, ρ)-DP, which is a Filling Problem introduced in [9].
Lemma 1 provides the necessary and sufficient conditions for
a solution exist in Binary-(θ, ρ)-DP.
Lemma 1: ([9]) The solution to Binary-(θ, ρ)-DP exists if
and only if θ[n] ≤
P
i∈[N] θ[i]
L+S
for all n ∈ [N].
From Lemma 1, there always exist solutions to Binary-(θ, ρ)-
DP, due to θ[n] ≤ ρ =
P
i∈[N] θ[i]
L+S
for n ∈ [N].
Solution 1: For Binary-(θ, ρ)-DP, we present (θ, ρ)-Division
Algorithm, by generalizing the algorithm in [9] using a scalar
0 ≤ ρ ≤ 1, which originally considers ρ = 1. With the input
θ and ρ, we obtain outputs γ and µ as shown in Algorithm 1,
which are the solution to Binary-(θ, ρ)-DP.
D. Computation Assignment Problem
Given any load division matrix µ of size G×N, designing a
computation assignment (Mg, Pg) for g ∈ [G] is equivalent
to solving a Binary-(µ[g], 1)-DP, by two steps as follows. For
clarity, we denote the desired vector and matrix in Binary-
(µ[g], 1)-DP as γ′ and µ′, respectively. First, we let Fg = |γ′|
and partition the indices [ r
L] into Fg disjoint sets Mg,1, · · · ,
Mg,Fg of size γ′
1·r
L , · · · ,
γ′
Fg ·r
L
respectively. Second, we let
Algorithm 1 (θ, ρ)-Division Algorithm
Input: θ, ρ
1: g ← 0
2: while θ contains a non-zero element do
3:
g ← g + 1
4:
L′ ← PN
i=1 θ[i]
5:
N ′ ← number of non-zero elements in m
6:
o ← indices that sort the non-zero elements of θ in
ascending order
7:
Ug ← {o[1], o[N ′ − (L + S) + 2], · · · , o[N ′]}
8:
bg ← a {0, 1}-vector where bg[i] = 1 if i ∈ Ug
9:
if N ′ ≥ L + S + 1 then
10:
γg ←1
ρ min

L′
L+S − θ[o[N ′ −(L + S)+1]], θ[o[1]]

11:
else
12:
γg ← θ[o[1]] · 1
ρ
13:
end if
14:
for n ∈ Ug do
15:
θ[n] ← θ[n] − γgρ
16:
end for
17: end while
18: G ← g
19: γ ← a vector of length N, where γ[g] = γg ·ρ for g ∈ [G]
20: µ ← a matrix of size G×N, where µ[g] = bg for g ∈ [G]
Output : γ, µ
Mg,f = {n : µ′[f, n] = 1} for f ∈ [Fg]. From (14a), the
obtained (Mg, Pg) satisfies vector µ[g], i.e., γ′ · µ′ = µ[g].
Moreover, there always exist solutions to Binary-(µ[g], 1)-DP,
as µ[g, n] ≤ 1 =
P
i∈[N] µ[g,i]
L+S
for all n ∈ [N], satisfying the
condition in Lemma 1. Therefore, we obtain (Mg, Pg) for
g ∈ [G] by solving the Binary-(µ[g], 1)-DP using Solution 1,
and using two steps as discussed.
IV. GENERAL SOLUTIONS FOR USCTEC WITH STORAGE
CONSTRAINTS
Algorithm 2 provides a general solution for USCTEC
systems with storage constraints, by generating a storage
placement Z and storage selections for a general speed
distribution. A detailed illustration is provided in Example 2.
The idea is to unionize the storage selections for all speed
realizations. However, if the combined storage exceeds the
storage constraint of any machine, it results in a storage
overflow. In such cases, the machines with storage overflow
will fill their storage capacity, and the storage placement will
be adjusted for the remaining machines in a similar fashion.
Example 2: Consider a system with N = 6, L = 2, S = 1,
e = (0.6, 0.6, 0.8, 0.8, 1, 1), two speed realizations s1 = (3, 3,
4, 4, 5, 5) and s2 = (3, 1, 2, 2, 3, 5) with equal probabilities.
The locations of rows in data matrix A are represented by
real numbers in the range [0, 1]. Specifically, the aq-th row is
located at a. We simplify all notations (·)si in Algorithm 2 as
(·)i for i ∈ [2]. For example, we simplify γsi as γi.
Algorithm 2 Storage Placement and Storage Selections
Input: Ωs, N, L, S
1: ˆρ ← 0
2: ˆγs ← 0 of length 1, ˆµs ← 0 of size 1 × N, σs ← 1 of
length N, and ls ← L + S for all s ∈ Ωs
3: while P
s∈Ωs ls > 0 do
4:
Zn ← ∅ for n ∈ [N]
5:
for s ∈ Ωs do
6:
¯θs ← solution to the (ls, s, σs)-LP
7:
(¯γs, ¯µs) ← solution to the (¯θs, 1 − ˆρ)-DP
8:
(γs, µs) ←

[ˆγs, ¯γs],
ˆµs
¯µs

9:
Is ← the storage selection based on (γs, µs)
10:
Zn ← Zn
S Is,n for n ∈ [N]
11:
end for
12:
if there exists storage overflow on Zn, n ∈ [N] then
13:
ˆρ ← the location of the first row that overflows
14:
for s ∈ Ωs do
15:
(ˆγs, ˆµs) ←
Fig. 1. Storage Placement and Storage Selections in Example 2: The x-axis
represents machine labels. The y-axis represents the location of rows in A.
The union of red and purple bars represents the storage selection for s1. The
union of blue and purple bars represents the storage selection for s2. The
purple bars represent the common storage selection for both s1 and s2. The
red line at y = 3
5 indicates a storage overflow occurred on machine 1.
For si, i ∈ [2], the remaining load is (L + S)(1 − ˆρ) = 6
5.
We update si to s′
i, where s′
i[n] = 0 if n = 1, otherwise
s′
i[n] = si[n], and update σi to σ′
i = (1 − ˆρ, · · · , 1 − ˆρ).
Reassign the Rows in [ 3
5, 1] (Lines 5-11): For each si,
i ∈ [2], we solve the ( 6
5, s′
i, σ′
i)-LP to obtain load vector ¯θ′
i,
where ¯θ′
1 = (0,
6
35,
8
35,
8
35, 2
7, 2
7) and ¯θ′
2 = (0,
1
10, 1
5, 1
5,
3
10,
2
5). We solve the Binary-(¯θ′
i, 1 − ˆρ)-DP to obtain a vector ¯γ′
i
and a matrix ¯µ′
i. The setting of load constrains σ′
i is to ensure
that the obtained ¯θ′
i satisfies the condition in Lemma 1, such
that Binary-(¯θ′
i, 1 − ˆρ)-DP has solutions. Specifically, ¯γ′
1 =
( 43
250,
57
500,
57
500), ¯γ′
2 = ( 1
10,
1
10,
1
10,
1
10),
¯µ′
1 =


0
1
0
0
1
1
0
0
1
1
1
0
0
0
1
1
0
1

 and ¯µ′
2 =


0
1
0
0
1
1
0
0
0
1
1
1
0
0
1
0
1
1
0
0
1
1
0
1

.
As shown in line 8, we consider the combined partitioning
vectors and load division matrices, i.e., γ′
1 = [γ1≺ˆ
ρ, ¯γ′
1]= ( 3
8,
9
40,
43
250,
57
500,
57
500), γ′
2 = [γ2≺ˆ
ρ, ¯γ′
2] = ( 3
16, 3
8,
3
80,
1
10,
1
10,
1
10,
1
10), µ′
1 =
µ1≺ˆ
ρ
¯µ′
1

and µ′
2 =
µ2≺ˆ
ρ
¯µ′
2

. From (8), we obtain
the storage selection Ii for si, using (γ′
i, µ′
i), where i ∈ [2].
Storage Placement and Storage Selections (Line 21):
It can be seen that there is no storage overflow, by letting
the storage placement for machine n be Zn = I1,n
S I2,n.
Therefore, the storage placement Z = {Zn : n ∈ [6]}, storage
selections I1 and I2 for the system are obtained, which are
visualized in Fig. 1.
V. DISCUSSIONS
We compare Algorithm 2 with USCTEC systems based on
cyclic storage strategy presented in [6]. We use the following
example to compare the storage size and expected computation
time obtained by two USCTEC systems.
Consider a system with N = 12, L = 2, S = 1, and two
speed realizations s1 and s2 with equal probabilities, where
s1 = (1, 1, 2, 2, 2, 3, 8, 8, 8, 8, 9, 9) and s2 = (8, 8, 2, 3,
9, 9, 2, 1, 8, 5, 2, 8). We define the storage constraint as e =
( Q
12, · · · , Q
12) of length 12, where Q ∈ {6, 7, 8, 9, 10, 11, 12}.
The USCTEC system based on cyclic storage placement
[6] operates as follows. First, each machine utilizes the full
storage capacity by defining γ = ( 1
12, · · · , 1
12) of length 12,
and letting the n-th machine store Q blocks An%N, · · · ,
A(n+Q−1)%N, where we define a%N ≜ a−⌊ a−1
N ⌋N. Second,
it can be shown that, given the storage placement, the system
achieves the minimum computation time. Specifically, For
g ∈ [12], Ug is the set of all machines that store block Ag,
and µ[g] is the solution to (L + S, sUg, 1)-LP, where sUg is a
vector containing the computation speeds of machines in Ug.
By varying storage constraints, we have comparisons as shown
in Table I.
TABLE I
COMPARISONS TO CYCLIC STORAGE PLACEMENT
Cyclic Storage Placement
Algorithm 2
Q
N
Storage Size
C(Z, TΩs)
Storage Size
C(Z, TΩs)
6
12
6
0.07235
5.16591
0.09164
7
12
7
0.06072
5.23310
0.04812
8
12
8
0.05371
5.23480
0.04766
9
12
9
0.05101
5.23480
0.04766
10
12
10
0.04927
5.23480
0.04766
11
12
11
0.04812
5.23480
0.04766
12
12
12
0.04766
5.23480
0.04766
From Table I, it can be seen that the proposed algorithm
achieves a smaller storage size compared to the baseline
algorithm. In addition, except the case when the storage
constraint is
1
2, the achieved expected computation time of
the proposed algorithm is always smaller than or equal to
the baseline algorithm. In particular, as the storage constraint
increases to 2
3 and larger, we can show that the systems using
Algorithm 2 achieve the optimal expected computation time
of 0.04766 and a storage size of 5.23480.
REFERENCES
[1] Y. Yang, M. Interlandi, P. Grover, S. Kar, S. Amizadeh, and M. Weimer,
“Coded elastic computing,” in 2019 IEEE International Symposium on
Information Theory (ISIT), July 2019, pp. 2654–2658.
[2] N. Woolsey, R.-R. Chen, and M. Ji, “Coded elastic computing on
machines with heterogeneous storage and computation speed,” IEEE
Transactions on Communications, vol. 69, no. 5, pp. 2894–2908, 2021.
[3] N. Woolsey, J. Kliewer, R.-R. Chen, and M. Ji, “A practical algorithm de-
sign and evaluation for heterogeneous elastic computing with stragglers,”
in 2021 IEEE Global Communications Conference (GLOBECOM), 2021,
pp. 1–6.
[4] S. Kiani, T. Adikari, and S. C. Draper, “Hierarchical coded elastic
computing,” in ICASSP 2021 - 2021 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 4045–4049.
[5] S. H. Dau, R. Gabrys, Y.-C. Huang, C. Feng, Q.-H. Luu, E. J. Alzahrani,
and Z. Tari, “Transition waste optimization for coded elastic computing,”
IEEE Transactions on Information Theory, vol. 69, no. 7, pp. 4442–4465,
2023.
[6] M. Ji, X. Zhang, and K. Wan, “A new design framework for heterogeneous
uncoded storage elastic computing,” in 2022 20th International Sympo-
sium on Modeling and Optimization in Mobile, Ad hoc, and Wireless
Networks (WiOpt), 2022, pp. 269–275.
[7] Y. Yang, M. Interlandi, P. Grover, S. Kar, S. Amizadeh, and M. Weimer,
“Coded elastic computing,” arXiv:1812.06411v3, 2018.
[8] Q. Yu, S. Li, N. Raviv, S. M. M. Kalan, M. Soltanolkotabi, and S. A.
Avestimehr, “Lagrange coded computing: Optimal design for resiliency,
security, and privacy,” in Proc. IEEE Int. Conf. on Artificial Intelligence
and Statistics (AISTATS), 2019, pp. 1215–1225.
[9] N. Woolsey, R.-R. Chen, and M. Ji, “An optimal iterative placement
algorithm for pir from heterogeneous storage-constrained databases,” in
2019 IEEE Global Communications Conference (GLOBECOM), 2019,
pp. 1–6.
"
"In this research, we propose the first personalized over-the-air federated learning scheme through multi-task learning, assisted by personal reconfigurable intelligent surfaces (RIS) for each user. We address several aspects such as optimizing communication and computation resources for global and personalized tasks in time-varying channels with imperfect channel state information, utilizing multi-task learning for non-identical and independently distributed data, designing power, local iterations, and RIS configurations adaptively, analyzing the convergence of non-convex objectives, demonstrating the superiority of our proposed algorithm, called PROAR-PFed, on the Fashion-MNIST dataset.","Federated learning (FL) is a distributed machine learning paradigm where collaborative iterative training is done between a parameter server (PS) and edge devices (clients). Over-the-air federated learning (OTA-FL) utilizes the inherent superposition property of wireless channels for bandwidth-efficient learning with the PS receiving the aggregated model through intrinsic superposition over the air. In practice, users only have estimated channel state information (CSI), which can affect learning performance. Personalized FL tackles scenarios with non-identical and independently distributed datasets that can cause severe performance degradation for users and has attracted attention in machine learning recently. Reconfigurable intelligent surfaces (RIS) are low-cost passive reflecting elements that adjust phase shifts of incident signals, creating more favorable propagation environments and facilitation for model aggregation in OTA-FL. Most existing research focuses on minimizing the mean squared error (MSE) of FL model aggregation. This work brings personalization into OTA-FL and 6G programmable wireless environments with personal RIS for each client, enabling personalized learning objectives for each user.","Reference [6] utilizes clustered FL and references [7, 8] explore multi-task learning over-the-air in a hierarchical setup. Our work presents a genuine PFL algorithm for each individual client integrating the wireless physical layer with OTA computation. Existing personalized OTA-FL approaches assume strong convexity while our framework is convergent under non-convex objectives and device heterogeneity. We propose PROAR-PFed (Personal RIS-assisted Over-the-Air Resource Allocation for Personalized Federated Learning) which adaptively designs power, local training iterations, and personal RIS configurations with demonstrated superior performance and effectiveness.nan","We consider a personal RIS-assisted communication system with m clients and a single antenna PS. The goal of conventional FL is to solve a single global objective. To better accommodate data heterogeneity, a bi-level personalized federated learning is considered through a multi-task learning framework. We assume weak direct links between devices and PS, rendering RIS assistance essential. We also assume that each device uses an individual RIS for the uplink, with negligible interference from other RISs. We use a uniform N elements for each RIS, adjusting their phase shifts per global iteration. We define the cascaded device i-RIS i-PS channel as gi₁, given by ((hi₁₂,thi₁₂)HHi₁₂,t)H, where Hi₁₂,t = diag(hi₁₂,t). For each global iteration, personal RIS phase design, global model updates, and personalized model adjustments are done. Dynamic power control is employed for the global model update, and a channel inversion strategy combined with dynamic local steps is proposed to alleviate the effects of channel fading. To design phase shifts of RIS i, a non-convex problem is solved using successive convex approximation.","The convergence of the proposed PROAR-PFed algorithm is proved with a statistical error, local optimization error, uplink channel noise, and global model update error affecting convergence. A convergence rate is obtained by selecting the proper hyperparameters. Numerical results demonstrate the superior performance of PROAR-PFed for personalized tasks compared to prior approaches, especially in the case of highly personalized tasks.","We proposed the first personalized OTA-FL scheme through multi-task learning, assisted by personal RIS for each user. We optimized communication and computation resources for global and personalized tasks in time-varying channels with imperfect channel state information. We presented convergence analysis for non-convex objectives and demonstrated that our PROAR-PFed algorithm outperforms state-of-the-art on the Fashion-MNIST dataset. Future work could focus on extending the proposed framework to scenarios with heterogeneous devices, studying the robustness of the algorithm to CSI estimation errors, and exploring other applications of personal RIS in FL systems.",Personalized Over-the-Air Federated Learning with Personalized Reconfigurable Intelligent Surfaces,"Jiayu Mao, Aylin Yener","PERSONALIZED OVER-THE-AIR FEDERATED LEARNING WITH PERSONALIZED
RECONFIGURABLE INTELLIGENT SURFACES
Jiayu Mao and Aylin Yener
Dept. of Electrical and Computer Engineering, The Ohio State University
ABSTRACT
Over-the-air federated learning (OTA-FL) provides bandwidth-
efficient learning by leveraging the inherent superposition
property of wireless channels. Personalized federated learn-
ing balances performance for users with diverse datasets,
addressing real-life data heterogeneity. We propose the first
personalized OTA-FL scheme through multi-task learning,
assisted by personal reconfigurable intelligent surfaces (RIS)
for each user. We take a cross-layer approach that optimizes
communication and computation resources for global and
personalized tasks in time-varying channels with imperfect
channel state information, using multi-task learning for non-
i.i.d data. Our PROAR-PFed algorithm adaptively designs
power, local iterations, and RIS configurations. We present
convergence analysis for non-convex objectives and demon-
strate that PROAR-PFed outperforms state-of-the-art on the
Fashion-MNIST dataset.
Index Terms— Personalized federated learning, over-
the-air computation, reconfigurable intelligent surfaces, 6G.
1. INTRODUCTION
Federated learning (FL) [1] is a popular distributed ma-
chine learning paradigm that employs collaborative iterative
training between a parameter server (PS) and edge devices
(clients). During each iteration, clients train local models,
which are subsequently sent to the PS for aggregation and
global model update. Whilst a fitting paradigm for mobile
edge networks, addressing challenges brought on by the ra-
dio channel and limited wireless resources is essential for
FL. Over-the-air federated learning (OTA-FL) [2] leverages
the broadcast nature of the wireless channel for bandwidth-
efficient learning, by having all clients simultaneously trans-
mit analog model updates, enabling the PS to directly receive
the aggregated model through the intrinsic superposition over
the air. Naturally, over-the-air (OTA) aggregation relies on
transmitter-side channel state information (CSI) for power
control.
In practice, users only have access to estimated
CSI, potentially harming learning performance. This work
quantifies the impact of estimated CSI on performance.
Personalized federated learning (PFL) is a recent frame-
work designed to tackle FL scenarios where non-i.i.d. datasets
This work is supported in part by NSF CNS-2112471.
can cause severe performance degradation for individual
users. Despite having attracted substantial attention in ma-
chine learning [3–5], PFL approaches are rare in wireless
systems: reference [6] utilizes clustered FL; [7], [8] explore
multi-task learning over-the-air in a hierarchical setup. By
contrast, we propose a genuine PFL algorithm for each indi-
vidual client integrating the wireless physical layer with OTA
computation.
Reconfigurable intelligent surfaces (RIS) [9] are pro-
grammable meta-surfaces with low-cost passive reflecting
elements that adjust phase shifts of incident signals. When
integrated with OTA-FL, RIS can create more favorable prop-
agation environments and facilitate better model aggregation.
Existing research [10–12] mainly focuses on minimizing
the mean squared error (MSE) of FL model aggregation,
while [13] concentrates on unified communication and learn-
ing design based on a time-invariant static channel with
perfect CSI. Recently, we have developed an adaptive joint
communication and learning algorithm in time-varying chan-
nels assisted by one RIS [14,15]. All of these works consider
solving a single global FL problem.
In this paper, we bring personalization into OTA-FL
and 6G programmable wireless environments. We propose
the first personalized OTA-FL framework with assistance
from personal RIS for each client.
Envisioning 6G with
portable and compact RIS units on intelligent edge devices,
we explore and validate the personal RIS model’s potential
for personalized learning objectives, equipping users with
individual RIS under time-varying physical layers and im-
perfect CSI. Inspired from our prior works [14, 15] for a
single global learning objective and single RIS, we estab-
lish an alternating, cross-layer approach optimizing resources
for enhanced global and personalized learning, in a system
model that personalizes the physical layer.
Different than
existing personalized OTA-FL works that assume strong
convexity, our framework is convergent under non-convex
objectives and device heterogeneity.
Specifically, we pro-
pose PROAR − PFed (Personal RIS-assisted Over-the-Air
Resource Allocation for Personalized Federated Learning),
which adaptively designs power control, local training iter-
ations and personal RIS configurations during each global
iteration, to enable each client’s personalized model training
together with global learning objective at no additional cost.
arXiv:2401.12149v1  [cs.IT]  22 Jan 2024
Personal RIS 1 
Base 
Station
(BS)
User 1
……
Personal RIS m
User m
Fig. 1: The personal RIS-assisted communication system.
We present the convergence analysis of PROAR − PFed and
assess its performance on the Fashion-MNIST dataset with
imperfect CSI, showing its superior performance and effec-
tiveness for personalization.
2. SYSTEM MODEL
We consider a personal RIS-assisted communication system
with m clients and single antenna PS (Fig. 1). Each client
i has a training dataset Di with distinct distribution Xi, i.e.,
Xi ̸= Xj if i ̸= j, ∀i, j ∈ [m]. The goal of conventional FL is
to solve a single global objective:
min
w∈Rd F(w) ≜ min
w∈Rd
X
i∈[m]
αiFi(w, Di),
(1)
where w ∈ Rd denote the model, Fi(w, Di) is local objective
function, αi =
|Di|
P
i∈[m] |Di| is the weight of client i.
To better accommodate data heterogeneity, in this pa-
per, we consider a bi-level personalized federated learning
through a multi-task learning framework:
min
vi∈Rd Ri(vi; w∗) ≜ Fi(vi, Di) + λ
2 ∥vi − w∗∥2
s.t.
w∗ = arg min
w
F(w),
(2)
where vi is the local parameter of user i for personalized
task, and λ is the hyperparameter of regularization. When
λ → 0, the personalized task reduces to local model training;
while λ → ∞, it becomes conventional FL. In FL, clients
minimize Fi and send the local updates to the PS. OTA-FL
enables model aggregation via concurrent transmissions over
the wireless medium. Next, PS broadcasts the updated global
model to all devices.
This procedure continues until con-
vergence. In particular, for local training, client i performs
stochastic gradient descent (SGD) to calculate its local gradi-
ent with the global model initialization wt and its dataset Di
for τ i
t steps, which varies across different rounds and users,
consistent with our earlier works [14,15].
We consider weak direct links between devices and
PS, rendering RIS assistance essential.
We assume that
each device uses an individual RIS for uplink, with neg-
ligible interference from other RISs.
Assuming uniform
N elements for each RIS, we adjust their phase shifts per
global iteration.
The phase matrix of RIS i in round t is
Θi
t = diag(θi
1,t, · · · , θi
N,t), with θi
n,t = ejϕi
n,t.
We assume a block fading model for uplink, channel co-
efficients stay unchanged within a communication round, but
vary independently between rounds. We consider an error-
free downlink for simplicity, where clients receive an accu-
rate global model per iteration, i.e., wi
t,0 = wt, ∀i ∈ [m]. We
note that our results readily extend to noisy downlinks by in-
cluding dynamic power control for downlinks as done in [15].
Let hi
RB,t ∈ CN, hi
UR,t ∈ CN, hi
UB,t ∈ C be the channel
gains from RIS i to PS, from user i to RIS i, and from user i
to PS, respectively. Denote xi
t ∈ Rd as transmit signal of user
i, zt as the additive white Gaussian noise (AWGN) with zero
mean and variance σ2
c. The received signal yt at the PS is:
yt =
X
i∈[m]
(hi
UB,t + (hi
UR,t)HΘi
thi
RB,t)xi
t + zt.
(3)
We further define the cascaded device i-RIS i-PS channel as
gi
t, i.e., gi
t = ((hi
UR,t)HHi
RB,t)H ∈ CN, where Hi
RB,t =
diag(hi
RB,t). Then, we can rewrite the phase matrix as a vec-
tor θi
t = (θi
1,t, ..., θi
N,t)T . Thus, the equivalent received sig-
nal is yt = P
i∈[m](hi
UB,t+(gi
t)Hθi
t)xi
t+zt. The power con-
straint of device i in round t is E[∥xi
t∥2] ≤ P i
t , ∀i ∈ [m], ∀t,
where P i
t is the transmit power budget.
We have only imperfect CSI available at clients. bht is
the CSI estimate of each wireless path in t-th round: bht =
ht +∆t, ∀t, where ∆t represents the i.i.d. channel estimation
error, with zero mean and variance eσ2
h. Note that all links
have this estimation error. To simplify notation, we denote
the overall channel gain of device i in the t-th round as hi
t:
hi
t = hi
UB,t + (hi
UR,t)HΘi
thi
RB,t.
(4)
Similarly, bhi
t is the overall estimated CSI at the device i.
3. ALGORITHM DESIGN
In this section, we detail Algorithm 1 for joint communica-
tion and learning optimization. Each global iteration consists
of three phases: personal RIS phase design, global model up-
dates, and personalized model adjustments.
We employ dynamic power control (PC) for the global
model update [14–17]. Let βi
t and βt be the PC factor for
device i and PS, respectively. Device i gets the local updates
and computes signal xi
t to transmit:
xi
t = βi
t(wi
t,τ i
t − wi
t,0), ∀t.
(5)
Next, the PS applies βt to the received signal (3):
wt+1 = wt + 1
βt
m
X
i=1
hi
txi
t + ˜zt, ˜zt ∼ N(0, σ2
c
β2
t
Id).
(6)
We propose a channel inversion strategy combined with dy-
namic local steps to alleviate the effects of channel fading.
Specifically, we design two criteria for each edge device i:
βi
t = βtαi
τ i
tbhi
t
,
3η2
t βi
tτ i
tG2 ≤ P i
t ,
(7)
where G is the bound of the stochastic gradient. Using dy-
namic local steps, τ i
t, we counter learning degradation from
imperfect CSI-induced misalignment and leverage local com-
putation resources. Criterion (7) is set to design phase shifts
of RIS i, ensuring convergence. After phase updates, each
device i finds τ i
t by incorporating (7) and (5) into the power
Algorithm
1
Personal
RIS-assisted
Over-the-Air
Re-
source
Allocation
for
Personalized
Federated
Learn-
ing (PROAR − PFed)
1: Initialization: w0, θi
0, vi
0, βi
t, τ i
v, τ i
t, ∀i ∈ [m].
2: for t = 0, . . . , T − 1 do
3:
for each device i ∈ [m] in parallel do
4:
for j = 0, . . . , J − 1 do
5:
Each device updates its personal RIS i by (12).
6:
end for
7:
end for
8:
PS broadcasts the global model wt.
9:
for each device i ∈ [m] in parallel do
10:
Each device gets τ i
t to satisfy the power constraint
and starts local training, finds βi
t and transmits xi
t.
11:
Each device updates vi
t for τ i
v local steps.
12:
end for
13:
The PS aggregates and updates global model by (6).
14: end for
15: return {vi}i∈[m](personalized), wT (global)
constraint. Then it starts local training.
We assume that personal RISs are controlled by their users
only. Unlike our previous studies with a single RIS requiring
user selection for phase updates, this work lets each device
i directly update its RIS according to criterion (7). The first
step is to rewrite (7):
(gi
t)Hθi
t ≥ 3η2
t βtαiG2
P i
t
− bhi
UB,t.
(8)
Then we design the phase as follows:
min
θi
t
∥(gi
t)Hθi
t − 3η2
t βtαiG2
P i
t
+ bhi
UB,t∥2
2
s.t.
|θi
t,n| = 1,
n = 1, ..., N.
(9)
(9) is non-convex and we use successive convex approxima-
tion (SCA) [18,19]. First, we define:
f(θi
t)
= ||si
t − (gi
t)Hθi
t||2
2
= (si
t)∗si
t − 2Re{(θi
t)Ha} + (θi
t)HUθi
t,
(10)
where a = si
tgi
t, U = gi
t(gi
t)H, si
t =
3η2
t βtαiG2
P i
t
− bhi
UB,t.
Using the equivalent phase element expression θi
n,t = ejϕi
n,t,
and noting that si
t is constant, we derive:
f1(ϕi
t) = (ejϕi
t)HUejϕi
t − 2Re{(ejϕi
t)Ha},
(11)
where ϕi
t = (ϕi
1,t, ..., ϕi
N,t)T . Next, we apply the SCA and
use the second-order Taylor expansion to find the surrogate
function g(ϕi
t, ϕi
t,j) at point ϕi
t,j in iteration j, then use SGD
to find the stationary solution ϕi
t,J:
ϕi
t,j+1 = ϕi
t,j − ∇f1(ϕi
t,j)
λ
.
(12)
Finally, we get the phase design of RIS i: θi
t = ejϕi
t.
Next, each device i optimizes the personalized task in (2).
Instead of directly finding w∗ to minimize Ri(vi; w∗),
we adopt an alternating approach inspired by [4] to solve
the local objective approximately in each global round
(see PROAR − PFed).
Specifically, device i performs τ i
v
SGD steps, initializing with vi
t from last global iteration:
vi
t,k+1 = vi
t,k − ηv(∇Fi(vi
t,k) + λ(vi
t,k − wt)), ∀k, (13)
where ηv is the local learning rate, and we set vi
t+1 = vi
t,τ iv.
In round t, we use wt to approximate w∗ and each device
updates independently. Thus, we can schedule this stage after
transmission, letting devices use PS aggregation downtime for
personalized training, saving overall learning time.
4. CONVERGENCE ANALYSIS
We first provide the assumptions on non-convex objectives:
Assumption 1. ∃L > 0, ∥∇Fi(w1)−∇Fi(w2)∥ ≤ L∥w1 −
w2∥, ∀w1, w2 ∈ Rd, ∀i ∈ [m].
Assumption 2. Local stochastic gradients are unbiased with
bounded variance, i.e., E[∇Fi(w, ξi)] = ∇Fi(w), ∀i ∈ [m],
and E[∥∇Fi(w, ξi) − ∇Fi(w)∥2] ≤ σ2, where ξi is sampled
from Di. Also, E[gi(vi; w)] = ∇Ri(vi; w), where gi(vi; w)
is stochastic gradient of Ri(vi; w).
Assumption 3. ∃G ≥ 0, E[∥∇Fi(w, ξi)∥2] ≤ G2, ∀i ∈ [m].
Convergence of Algorithm 1.
The global model w
does not rely on any personalized models {vi}i∈[m]. Thus,
the global optimization has the same convergence rate with
ROAR − Fed in our previous work [14]1.
Based on this observation, we can now present the con-
vergence analysis of the personalized local task as follows:
Theorem 1. With Assumptions 1- 3, a constant global learn-
ing rate ηt = η ≤
1
L, a constant local learning rate ηv ≤
1
√
2L2+2λ2 and T ≥ 4, for each device i ∈ [m], we have:
min
t∈[T ] E∥∇Ri(vi
t; w∗)∥2 ≤
p
2L2 + 2λ2η2
vσ2
|
{z
}
statistical error
+ 1
T
T −1
X
s=0
2
0
20
40
60
80
100
120
Iteration, t
0
0.2
0.4
0.6
0.8
1
Test Accuracy for Personalization
Fig. 2: Average test accuracy for personalized tasks when γ = 0.5.
Theorem 1 highlights four errors affecting convergence:
statistical error, local optimization error, uplink channel noise,
and global model update error. While the first two are com-
mon in non-convex cases, the latter two are coupled with
global training because of our alternating scheme, making up-
link imperfect CSI and noise influential in local optimization.
The convergence upper bound is finite: as the channel
estimation error is typically small, we adopt the Taylor ex-
pansion similar to [20]. By ignoring higher-order terms and
selecting the proper hyperparameters, we obtain:
Corollary 1. Let |∆t| ≪ |ht|, ∀t ∈ [T], hUB,m =
min
t∈[T ],i∈[m]{|hi
UB,t|}, hUR,a =
max
t∈[T ],i∈[m],j∈[N]{|hi
UB,t,j|},
hRB,a =
max
t∈[T ],j∈[N]{|hRB,t,j|}, η =
1
T , β = T, τ i
v = τv,
αi = 1
m, ∃λ < ϵ, ϵ > 0, the convergence rate is bounded:
min
t∈[T ] E∥∇Ri(vi
t; w∗)∥2 ≤
p
2L2 + 2λ2η2
vσ2 + λ2σ2
c+
1
T
T −1
X
s=0
2
7. REFERENCES
[1] Brendan McMahan et al.,
“Communication-Efficient
Learning of Deep Networks from Decentralized Data,”
in Artificial Intelligence and Statistics. PMLR, 2017, pp.
1273–1282.
[2] Mohammad Mohammadi Amiri and Deniz Gündüz,
“Machine Learning at the Wireless Edge: Distributed
Stochastic Gradient Descent Over-the-Air,”
IEEE
Trans. on Signal Processing, 68, pp. 2155–2169, 2020.
[3] Alireza Fallah et al., “Personalized Federated Learning
with Theoretical Guarantees: A Model-Agnostic Meta-
Learning Approach,” Advances in Neural Information
Processing Systems, vol. 33, pp. 3557–3568, 2020.
[4] Tian Li et al., “Ditto: Fair and Robust Federated Learn-
ing Through Personalization,”
in International Con-
ference on Machine Learning. PMLR, 2021, pp. 6357–
6368.
[5] Xue Zheng, Parinaz Naghizadeh, and Aylin Yener,
“DiPLe: Learning Directed Collaboration Graphs for
Peer-to-Peer Personalized Learning,” in 2022 IEEE In-
formation Theory Workshop (ITW), 2022, pp. 446–451.
[6] Hasin Us Sami and Basak Güler,
“Over-the-Air Per-
sonalized Federated Learning,” in 2022-2022 IEEE In-
ternational Conference on Acoustics, Speech and Signal
Processing (ICASSP). IEEE, 2022, pp. 8777–8781.
[7] Matin Mortaheb et al., “Personalized Federated Multi-
Task Learning over Wireless Fading Channels,” Algo-
rithms, 15(11), p. 421, 2022.
[8] Zihan Chen et al.,
“Personalizing Federated Learn-
ing with Over-the-Air Computations,”
in ICASSP
2023-2023 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP). IEEE,
2023, pp. 1–5.
[9] Qingqing Wu and Rui Zhang,
“Intelligent Reflecting
Surface Enhanced Wireless Network via Joint Active
and Passive Beamforming,”
IEEE Trans. on Wireless
Comm., 18(11), pp. 5394–5409, 2019.
[10] Wanli Ni et al., “Federated Learning in Multi-RIS Aided
Systems,” IEEE Internet of Things Journal, 9(12), pp.
9608–9624, 2021.
[11] Zhibin Wang et al., “Federated Learning via Intelligent
Reflecting Surface,” IEEE Trans. on Wireless Comm.,
21(2), pp. 808–822, 2021.
[12] Heju Li et al.,
“One Bit Aggregation for Federated
Edge Learning with Reconfigurable Intelligent Surface:
Analysis and Optimization,” IEEE Trans. on Wireless
Comm., 2022.
[13] Hang Liu et al., “Reconfigurable Intelligent Surface En-
abled Federated Learning: A Unified Communication-
Learning Design Approach,” IEEE Trans. on Wireless
Comm., 20(11), pp. 7595–7609, 2021.
[14] Jiayu Mao and Aylin Yener, “ROAR-Fed: RIS-Assisted
Over-the-Air Adaptive Resource Allocation for Feder-
ated Learning,” in ICC-IEEE International Conference
on Communications. IEEE, 2023, pp. 4341–4346.
[15] Jiayu Mao and Aylin Yener, “RIS-Assisted Over-the-Air
Adaptive Federated Learning with Noisy Downlink,”
in IEEE International Conference on Communications
Workshops (ICC Workshops). IEEE, 2023, pp. 98–103.
[16] Haibo Yang, Peiwen Qiu, Jia Liu, and Aylin Yener,
“Over-the-Air Federated Learning with Joint Adaptive
Computation and Power Control,” in 2022 IEEE Inter-
national Symposium on Information Theory, 2022, pp.
1259–1264.
[17] Jiayu Mao, Haibo Yang, Peiwen Qiu, Jia Liu, and Aylin
Yener, “CHARLES: Channel-Quality-Adaptive Over-
the-Air Federated Learning over Wireless Networks,” in
2022 IEEE 23rd International Workshop on Signal Pro-
cessing Advances in Wireless Communication (SPAWC),
2022, pp. 1–5.
[18] Gesualdo Scutari et al., “Decomposition by Partial Lin-
earization: Parallel Optimization of Multi-Agent Sys-
tems,”
IEEE Trans. on Signal Processing, 62(3), pp.
641–656, 2013.
[19] Jiayu Mao and Aylin Yener, “Iterative Power Control for
Wireless Networks with Distributed Reconfigurable In-
telligent Surfaces,” in GLOBECOM-IEEE Global Com-
munications Conference. IEEE, 2022, pp. 3290–3295.
[20] Guangxu Zhu et al., “One-Bit Over-the-Air Aggregation
for Communication-Efficient Federated Edge Learning:
Design and Convergence Analysis,”
IEEE Trans. on
Wireless Comm., 20(3), pp. 2120–2135, 2020.
[21] Wankai Tang et al., “Wireless Communications With
Reconfigurable Intelligent Surface: Path Loss Modeling
and Experimental Measurement,” IEEE Trans. on Wire-
less Comm., 20(1), pp. 421–439, 2020.
[22] Han Xiao et al.,
“Fashion-MNIST: a Novel Image
Dataset for Benchmarking Machine Learning Algo-
rithms,” arXiv, arXiv:1708.07747, 2017.
[23] Qinbin Li et al., “Federated Learning on Non-IID Data
Silos: An Experimental Study,” in 2022 IEEE 38th In-
ternational Conference on Data Engineering (ICDE).
IEEE, 2022, pp. 965–978.
"
"Anisotropy, a phenomenon commonly observed in self-supervised learning methods based on Transformers, refers to the unexpected closeness of hidden representations in terms of angular distance. This research explores anisotropy in Transformers across various modalities and investigates the underlying causes. Key insights from the different sections of the research paper are summarized as follows:

* Abstract:
    - Anisotropy is not solely caused by optimizing the cross-entropy loss on long-tailed distributions of tokens.
    - Empirical evidence suggests that anisotropy extends to Transformers trained on other modalities, indicating that it is inherent to Transformer-based models.

* Introduction:
    - Anisotropy has been observed among self-supervised Transformer models and is often attributed to the optimization of the cross-entropy loss on long-tailed token distributions.
    - It remains unclear whether anisotropy is a fundamental property of Transformers or a consequence of the pre-training process.

* Literature Review:
    - Prior research has established a connection between anisotropy and the drift of latent representations in Transformers trained on token-level natural language data.
    - Post-processing methods have been proposed to reduce anisotropy in Transformers, leading to downstream performance improvements.

* Methodology:
    - The study explores the anisotropy issue in different Transformer architectures, including character-based models, speech and vision models, and convolutional-based vision networks.
    - It investigates the correlation between the average cosine similarity and the norm of the average hidden representation.
    - The research also analyzes the behavior of untrained Transformer blocks with added common bias terms to mimic the drift effect.
    - Finally, it explores the dynamics of query and key representations during training.

* Results:
    - Anisotropy is observed in character-based and byte-level language models, as well as speech and vision models, suggesting that it is not solely caused by subword token distributions or linguistic properties.
    - The correlation between anisotropy and the drift effect is observed in some models but not in others, indicating that anisotropy can arise from other factors.
    - For untrained Transformers, adding a common bias term to the input representations leads to sharper self-attention patterns.
    - During training, query and key distributions drift in parallel directions, increasing anisotropy and facilitating sharper attention patterns.

* Conclusion:
    - Anisotropy is an inherent property of the Transformer architecture that arises from the nature of self-attention mechanisms.
    - The study provides insights into the underlying causes of anisotropy and suggests that revising the self-attention operation could help mitigate it while preserving strong attention patterns.

Overall, this research deepens the understanding of anisotropy in Transformers and contributes to the development of more isotropic architectures for various modalities.",nan,nannan,nan,nan,nan,Anisotropy Is Inherent to Self-Attention in Transformers,"Nathan Godey, Éric de la Clergerie, Benoît Sagot","Anisotropy Is Inherent to Self-Attention in Transformers
Nathan Godey 1,2
Éric de la Clergerie1
Benoît Sagot1
1Inria, Paris, France
2Sorbonne Université, Paris, France
{nathan.godey,eric.de_la_clergerie,benoit.sagot}@inria.fr
Abstract
The representation degeneration problem is a
phenomenon that is widely observed among
self-supervised learning methods based on
Transformers. In NLP, it takes the form of
anisotropy, a singular property of hidden rep-
resentations which makes them unexpectedly
close to each other in terms of angular distance
(cosine-similarity). Some recent works tend to
show that anisotropy is a consequence of op-
timizing the cross-entropy loss on long-tailed
distributions of tokens. We show in this paper
that anisotropy can also be observed empiri-
cally in language models with specific objec-
tives that should not suffer directly from the
same consequences. We also show that the
anisotropy problem extends to Transformers
trained on other modalities. Our observations
suggest that anisotropy is actually inherent to
Transformers-based models.
1
Introduction
In recent years, deep learning models based on
Transformers have led to significant breakthroughs
in the field of natural language processing (NLP).
These models have demonstrated state-of-the-art
performance across a range of tasks, such as lan-
guage modeling, machine translation, and senti-
ment analysis. However, despite their successes,
they suffer from a phenomenon known as the repre-
sentation degeneration problem. Specifically, this
degeneration is characterized by anisotropy, a prop-
erty of hidden representations that makes them all
close to each other in terms of angular distance
(cosine-similarity).
Anisotropy has been widely observed among
self-supervised models based on Transformers, and
literature currently suggests that it may be a con-
sequence of optimizing the cross-entropy loss on
long-tailed distributions of tokens (Gao et al., 2019;
Bi´s et al., 2021). However, it remains uncertain
whether anisotropy is a fundamental property of
Transformers-based models or a consequence of
the pre-training process.
In this paper, we investigate the anisotropy prob-
lem in depth, and we make several contributions:
• We demonstrate empirically that anisotropy
can be observed in language models with
character-aware architectures that should not
suffer directly from the same consequences as
token-based models. We extend our observa-
tions to Transformers trained on other modali-
ties, such as image and audio data, and show
that anisotropy cannot be explained solely
based on linguistic properties;
• We provide empirical observations on the
anisotropic properties of the Transformer
block by studying untrained layers, and es-
tablish a relation between anisotropy and the
general sharpness of the self-attention mecha-
nism;
• We conduct an analysis of the representations
used in self-attention (queries and keys) along
training and show that anisotropy appears in-
trinsically in the self-attention mechanism,
when training pushes for sharp patterns.
2
Related Work
The general phenomenon of anisotropy in token-
based Transformers for language models has been
shown in Ethayarajh (2019). Figure 1 extends one
of their experiment to more architectures. Gao et al.
(2019) shows that the degeneration of representa-
tions comes from the distributions of subwords in
natural language, namely the existence of unused
and rare tokens that tend to push all representations
away from the origin towards a specific direction.
Other works have established a connection be-
tween word frequency and distortions of the latent
spaces (Yu et al., 2022; Puccetti et al., 2022; Rajaee
and Pilehvar, 2022). Bi´s et al. (2021) have shown
arXiv:2401.12143v1  [cs.CL]  22 Jan 2024
Figure 1: Average cosine-similarity between hidden rep-
resentations across layers for token-level NLP models.
For T5-base, we concatenate encoder and decoder re-
sults.
that anisotropy in LMs could be explained by a
global drift of the representations in the same di-
rection, thus unifying conclusions from Ethayarajh
(2019) and Gao et al. (2019). The authors propose
that this drift is caused by the persistent updating
of the representation of rare and unused tokens
in a consistent direction, due to the nature of the
softmax operation in the cross-entropy loss. They
show that removing the average component to all
representations leads to a nearly perfect isotropy.
Several methods have been proposed to reduce
anisotropy in Transformers-based LMs at token-
level (Rajaee and Pilehvar, 2021; Wang et al.,
2020), or at sentence-level (Gao et al., 2021; Yan
et al., 2021; Su et al., 2021). They usually consist
in post-processing the representations, and lead to
downstream performance boosts. We argue that
these positive results are paving the way for the
search of pre-training objectives that do not intro-
duce anisotropy in the first place, in the hope that
the resulting models will also perform better with-
out any post-processing, and potentially be trained
more efficiently. This motivates us to gain a deeper
understanding of the underlying factors that induce
anisotropy, whether they belong in data, architec-
tures, or training procedures.
3
Anisotropy in pre-trained Transformers
3.1
Character-based NLP
To assert whether the cross-entropy objective ap-
plied on vocabularies containing rare tokens is the
sole cause for the common drift issue, we explore
anisotropy in character-based models. We study
different architectures:
• CharacterBERT (El Boukkouri et al., 2020) is
Figure 2: Average cosine-similarity between hidden
representations across layers for character-level models.
constructing whole word representations from
character embeddings put through convolu-
tions and highway layers, before feeding them
to a Transformers architecture.
• CANINE (Clark et al., 2022) is downsampling
contextualized character representations via a
strided convolution before feeding them to a
Transformers. It can be trained either with a
subword-based objective (CANINE-s) or with
a character-level one (CANINE-c).
• MANTa-LM (Godey et al., 2022) is based
on a differentiable segmentation and embed-
ding module added before an encoder-decoder
model in the style of T5 (Raffel et al., 2020).
It takes bytes as inputs and outputs, but builds
internal representations that are usually based
on several bytes.
• ByT5 (Xue et al., 2022) is a version of T5
that is trained at byte-level. To afford for
more complex encoding, the authors resize
the encoder-decoder architecture.
Neither of these architectures should suffer from
out-of-vocabulary tokens in the process of creating
representations. The models that predict at word or
sub-word level (CharacterBERT and CANINE-s)
could have the cross-entropy loss systematically
pushing away rare item representations. However,
it is rather unclear why it would imply an embed-
ding drift at deeper layers. Hence, if anisotropy
was only caused by the presence of unused or rare
subwords, those character-level models should be
much less prone to this issue.
To verify this hypothesis, we compute hid-
den representations for the validation set of the
WikiText-103 corpus (Merity et al., 2016). We then
compute the average cosine-similarity between two
representations, uniformly taken in the whole vali-
dation corpus.
In fact, as shown in Figure 2, those models all
display significant levels of anisotropy in at least
one of their layers. Interestingly, the models that
are based solely on characters or bytes for input and
prediction (ByT5, CANINE-c, and MANTA-LM)
seem to display even higher levels of anisotropy.
We note, as it is the case for the T5 model, that
the ByT5 decoder displays extremely high levels
of anisotropy.
3.2
Other modalities
We’ve shown in the previous section that character-
level language models suffer from anisotropy sim-
ilarly to token-level ones, hinting that subword
token distributions are not solely responsible for
anisotropy.
However, it may be argued that
anisotropy is related to linguistic properties. Thus,
we proceed to explore the anisotropy problem for
Transformers-based models in other modalities,
specifically speech and vision.
For speech models, we consider wav2Vec 2.0
(Baevski et al., 2020), HuBERT (Hsu et al., 2021),
and Whisper (Radford et al., 2022) with the Com-
mon Voice 11.0 dataset (Ardila et al., 2020). For
vision models, we use ViT (Wu et al., 2020), BEiT
(Bao et al., 2021), MiT (Xie et al., 2021), and DEiT
(Touvron et al., 2021) on the ImageNet dataset
(Russakovsky et al., 2015).
As in subsection 3.1, we infer hidden represen-
tations on the validation sets for each modality.
We then uniformly sample pairs of vectors to get
cosine-similarity values for every layer of every
model. The averaged results are displayed in Fig-
ure 3.
Once again, almost every model shows a signifi-
cant level of anisotropy on some of its layers. No-
tably, speech models seem to have very anisotropic
representations, as every layer of every model out-
puts an average cosine-similarity of at least 0.2. We
find some exceptions among vision models, since
the MiT model seems to use isotropic representa-
tion spaces and the ViT model has a low average
cosine-similarity for all its layers.
We also conduct the same experiment for
convolution-based networks in the vision modal-
ity. The models at glance are ResNet (He et al.,
2016), EfficientNet (Tan and Le, 2019), CvT (Wu
et al., 2021), ConvNeXt (Liu et al., 2022), and VAN
(Guo et al., 2022). For these networks, we flatten
convolution maps to vectors before computing the
cosine-similarity.
Figure 4: Average cosine-similarity between hidden rep-
resentations across layers for convolution-based vision
models.
We observe in Figure 4 that most of the
convolution-based models are isotropic. Interest-
ingly, the only exception is ResNet-50, whose rep-
resentations become more and more isotropic as
one explores deeper layers. This could partially be
explained by the fact that the batch normalization
(Ioffe and Szegedy, 2015) used in some of these
models mitigates a posteriori the drift effect by re-
moving the mean component of the representations.
However, the ConvNeXt model also seems to use
isotropic representations while not using batch nor-
malization, which shows that this is not the only
factor in the isotropic behavior of these models.
3.3
To drift or not to drift?
Related works (Bi´s et al., 2021; Gao et al., 2019)
show that anisotropy in subword-level language
models is caused by a drift of the hidden represen-
tations in a shared direction. In this section, we try
to extend this observation to other modalities.
We study the correlation between the uniformly
measured cosine-similarity, and the norm of the
average hidden representation ||¯x||2 for each layer.
If anisotropy could be directly explained by the
drift effect, we would expect a monotonic relation
between ||¯x||2 and the average cosine-similarity.
To verify this, we apply a Spearman correlation
test on these two metrics for every model from
subsection 3.1 and subsection 3.2, along with some
token-level language models, namely T5 (Raffel
et al., 2020), BERT (Devlin et al., 2019), RoBERTa
(Liu et al., 2019), and GPT-2 (Radford et al., 2019).
(a) Speech
(b) Vision
Figure 3: Average cosine-similarity between hidden representations across layers for Speech and Vision modalities.
We observe that across both modalities, several models display significant levels of anisotropy.
Figure 5: p-value of the Spearman correlation test be-
tween the norm of the average representation and the
cosine-similarity averaged over all layers, across modal-
ities. For models above the red dotted line, there is no
significant (p > 0.05) correlation between the drift ef-
fect and the anisotropy level.
In Figure 5, we observe that we can correlate
the anisotropy level and the magnitude of the drift
component across layers for several models. The
anisotropy of subword-based models can generally
be correlated with the drift effect, except for GPT-
2 for which the Spearman correlation metric may
not be appropriate. We provide a similar analysis
based on the Pearson correlation test and discuss
the relevance of each statistic in Appendix A.
Interestingly, we notice that the anisotropy af-
fecting most CNN-based vision models is gener-
ally not correlated with the drift effect, contrary to
Tranformers-based models in the same modality.
Some speech models (HuBERT and Whisper-base)
also display signs of anisotropy that cannot be cor-
related with the drift effect. Figure 5 also shows
a correlation for all character-based models but
Canine-C and MANTa-base.
4
Exploring the representation drift
In this section, we focus on some intrinsic prop-
erties of the Transformer block in a modality-
agnostic fashion, i.e. with minimal assumptions on
the data distribution, and without training. We ana-
lyze experimentally the behavior of the untrained
Transformer block T when a common bias term b
is added to untrained input representations x. This
allows us to mimic the common drift as mentioned
in Bi´s et al. (2021) and to identify some proper-
ties induced by this artificial drift on the output
representations.
4.1
Experimental setup
We consider an embedding lookup table E and a
Transformer block T with weights initialized as
in BERT (Devlin et al., 2019). We then draw 16
input embedding sequences x of length 512 uni-
formly from E. To account for a drift component of
norm N ∈ R, we generate a vector bu ∼ N(0, Id),
which we normalize into b =
bu
||bu||2 × N. We fi-
nally compute T(xi + b) for every sequence xi,
and study the resulting distributions.
Specifically, we study the average norm of
the input representations E(||xi + b||2) against
the average norm of the output representations
E(||T(xi +b)||2) in Figure 6b. We also retrieve the
self-attention scores before the softmax operation,
namely QKT
√dk , along with the corresponding Q and
K matrices. We study some of their properties in
Figure 7 and Figure 8.
4.2
Input vs. output analysis
In Figure 6a, we observe that the output representa-
tions have an average cosine-similarity value that
(a) Cosine similarity
BERT max. avg. norm (layer 11)
(b) Norm
Figure 6: Input/Output comparison of a Transformer
block from BERT-base as the bias norms increases.
is slightly higher than the one of the input repre-
sentations, no matter the level of input bias. We
also notice that while the norm of the average out-
put representation increases with the bias norm, it
seems to meet the corresponding input measure for
a given bias norm.
Interestingly, this shows that there is a fixed point
in terms of norm in the Transformers function with
biased input. More formally, there seems to exist a
bias norm N∗ ∈ R+ such that:
Ex,bN∗(||xi + bN∗||) = Ex,bN∗(||T(xi + bN∗)||)
Moreover, this fixed point level N∗ is in the
order of magnitude of the average hidden state
norms of the layers of the trained BERT model.
This hints that the model’s representations stabilize
when their norm is close to this fixed point. We
leave a more thorough analysis of this hypothesis
for future work.
4.3
Exploring the Transformer block
To understand the effect of the drift effect on the
inner workings of the Transformer layer, we take
a closer look at the self-attention operation as the
average input representation drifts away.
Figure 7: Histograms of the pre-softmax attention scores
as the input bias norm increases. Other initializations
of the layer and of the bias direction bu led to a general
increase of the attention scores instead.
Figure 7 shows that the attention scores tend
to move away from zero as the input bias norm
increases. Indeed, as the norm of the average ¯x
of the input embeddings increases, we can expect
the query and key vectors Q and K to also dis-
play signs of anisotropy. Actually, for each self-
attention head, and for all position i ∈ [1, L], we
have:
(
Ex(Qi) = WQ¯x + bQ
Ex(Ki) = WK ¯x + bK
(1)
We can observe in Figure 8 that query and key
representations indeed increase in norm with the
input bias norm. We also notice that the corre-
sponding distributions are anisotropic even when
no bias is added, which may be a consequence of
BERT’s initialization parameters.
(a) Cosine sim.
(b) Norm
Figure 8: Analysis of the self-attention query and key
distributions
4.4
Impact of the drift
After exploring the consequences of the drift of
input representations on the query-key product in
self-attention, we identify in this section the impli-
cations of this drift at a more explainable level, by
observing the resulting post-softmax distributions.
Figure 9: Evolution of the self-attention softmax values
as the input bias norm increases.
In Figure 9, we retrieve softmax values in the
self-attention block and for each position, we ex-
tract the maximum, the median and the minimum.
We then average these values over the whole batch,
and repeat for various input bias norm levels. We
notice that as the input bias norm increases, the
self-attention softmax distributions tend to become
less entropic, evolving towards higher maximal
probabilities and lower minimal probabilities. In
the following analysis, we’ll use the term sharp-
ness to discuss entropy levels of the self-attention
distributions.
(a) Maximum
(b) Minimum
Figure 10: Comparison of the extreme values of each
sequence averaged over the batch as the bias norm in-
creases.
This sharpening effect of the attention distri-
butions becomes even clearer if we consider the
maximum and minimum values over the whole se-
quences, as in Figure 10.
However, at low anisotropy levels, i.e. when the
bias norm is low, we see that the effect is not very
important. Figure 9 and Figure 10 only hint at the
fact that the drift of embeddings may help the self-
attention to be sharper. Another explanation could
be that training favors sharp self-attention patterns,
as has been pointed out in previous works (Clark
et al., 2019), which in turn induces a drift in the
models’ representations. In order to account for
that, we need to study the evolution of latent spaces
at the self-attention level along training.
5
Queries and keys: training dynamics
We have established that manually pushing for drift-
based anisotropy on untrained Transformers mod-
els leads to sharper (i.e. low-entropy) self-attention
patterns. In this section, we show that this evo-
lution of self-attention values actually takes place
during training, and we explore the mechanism be-
hind their appearance. As pointed out in section 4,
the self-attention scores result from the QKT op-
eration, which computes scalar products between
query and key representations corresponding to
each pair of positions. Thus, in this section, we
study the evolution of these query and key represen-
tations along training, and explore the mechanism
behind the increase of the scalar products leading
to self-attention scores.
We use the MultiBERT checkpoints (Sellam
et al., 2021) with seed 0 to retrieve Q and K dis-
tributions at different pretraining steps, and we
use 128 samples from Wikitext-103 as input data.
Along this section, Qs and Ks refer to query and
key representations extracted at a specific layer and
head at a given step s, and ˆ
Qs and ˆ
Ks are the av-
erage representations, taken over all tokens in the
sampled batch. By studying ¯
Qs and ¯
Ks, we aim at
exploring the common (or context-agnostic) drifts
of keys and queries distributions.
In Figure 11 and Figure 12, we compute a SVD
of the union of Qs and Ks for all steps s, so that the
projection makes sense for both distributions across
steps for visualization purposes 1. As shown in our
selected examples, we observe that the dynamics of
¯
Qs and ¯
Ks tend to align along training, making the
average of the distributions drift in either similar
or opposite directions. The first dimension of the
SVD seems to describe this common drift. Note
that in Rdh (dh = 64 being the head dimension),
such an alignment is very unlikely to happen ran-
domly. Interestingly, Figure 12a shows that the
common direction dynamics appear in the first few
steps, while the opposite direction dynamics of Fig-
ure 12b only starts after 8% of the total training
steps.
To consolidate our observations, we compute the
evolution of the cosine-similarity between ¯
Qs and
¯
Ks along training in Figure 13. We also display
1We actually uniformly sample 20% of the whole set of
representations to compute the SVD under reasonable memory
constraints.
(a) Step 0
(b) Step 40k
(c) Step 200k
(d) Step 2M (final)
Figure 11: Evolution of Qs and Ks distributions along training. Vectors are projected using a common SVD.
(a) Similar
(b) Opposite
Figure 12: Evolution of ¯
Qs and ¯
Ks along training for
two different heads in the network, projected via com-
mon SVD. Each arrow represents a checkpoint in the
MultiBERT suite. We display typical examples of dy-
namics in same/opposite direction.
some projected Qs and Ks distributions for several
s steps in Figure 11.
Figure 13 shows that the first layers display
a common direction dynamic, as the cosine-
similarity tends to increase, thus showing that the
key and query distributions drift along a simi-
lar direction in average. The last layers seem to
adopt an opposite direction dynamic, as the cosine-
similarity between their mean key and query repre-
sentations gets negative along training.
As shown in Figure 14, this drift induces an in-
crease in the magnitude of scalar products obtained
in the self-attention QKT operation, thus facilitat-
ing the emergence of sharp patterns where attention
focuses on specific tokens.
Finally, Figure 15 describes the evolution of the
average entropy in self-attention distributions. We
observe that training induces an overall decay of the
entropy for all layers, with different dynamics. This
corresponds to sharper self-attention distributions.
It is interesting to notice that the distributions in
the first layers remain sharper than the ones in the
last layers.
Overall, this section shows that drift anisotropy
emerges in the query and key representations dur-
ing the training of MultiBERT, as self-attention
distributions become sharper. The drifts of queries
and keys tend to align, thus increasing the magni-
tude of scalar products, and the general sharpness
of self-attention.
Although this section focuses on the case of
token-based NLP, we believe that strong attention
patterns may be required when training Transform-
ers across all modalities, potentially generating dis-
tortions in query and key distributions that account
for the final observed anisotropy of the models.
However, we could not extend experiments to other
modalities due to the lack of released intermediate
checkpoints, to the best of our knowledge.
6
Discussion
In this work, we argue that the nature of data
distributions is not solely responsible for the
anisotropy observed in most hidden representations
of Transformers-based models across modalities.
As section 4 shows, untrained Transformers layers
display a tendency towards anisotropy. Biased in-
puts tend to increase the variance of the attention
scores and thus facilitate the emergence of sharp
patterns in the self-attention mechanisms. We also
show in section 5 that along training, query and
key distributions drift in parallel directions, which
increases anisotropy in the inner representations
of the Transformer layers, while allowing sharper
attention patterns. As discussed in Puccetti et al.
(2022), outlier dimensions in Transformers are also
involved in the emergence of strong attention pat-
terns.
Consistency of the SVD
In section 5, we use an
SVD on the union of Qs and Ks for visualization
purposes (see Figure 11 and Figure 12). It may be
argued that this approach favors the emergence of
a discriminative singular direction, that helps dis-
tinguish between keys and queries, thus supporting
the findings in a less convincing way. To address
this concern, we display alternative projections in
Appendix C, where we compute the SVD on Qs or
(a) Layer 0
(b) Layer 4
(c) Layer 9
(d) Layer 11
Figure 13: Evolution of cosine-similarity between ¯
Qs and ¯
Ks along training. Each color represents one self-attention
head. Steps are counted in thousands. We generally observe that almost all heads see ¯
Qs and ¯
Ks align in common
or opposite directions along training. In other words, the average components of keys and queries representations
tend to align in self-attention heads, which maximizes the magnitude of the scalar product between two average
representations. We run a similar experiment on all MultiBERT seeds in Figure 23, and obtain comparable results.
(a) Similar
(b) Opposite
Figure 14: Evolution of the scalar product between ¯
Qs
and ¯
Ks along training. Steps are in thousands.
Figure 15: Average entropy of the probability distribu-
tions corresponding to self-attention rows along training.
Each curve corresponds to one layer.
Ks only, and then project all representations using
this SVD. Our observations show that our findings
are consistent for these alternative projections.
Harmfulness
of
anisotropy
Even
though
anisotropy has not been shown to be an issue in lan-
guage modeling, previous works have advocated
that removing anisotropy in output representations
leads to better sense disambiguation abilities
(Bihani and Rayz, 2021; Bi´s et al., 2021). Isotropic
models could also improve cross-lingual alignment
in multilingual language models (Hämmerl et al.,
2023).
Nevertheless, concurrent works have
suggested that anisotropy may not hurt the quality
of the representations (Ait-Saada and Nadif, 2023;
Rudman and Eickhoff, 2023).
We argue that
anisotropy in the Transformer architecture may
actually help models by allowing sharp attention
patterns, but we also believe that our work can
pave the way for new isotropic architectures that
can easily use sharp attention patterns.
Conclusion
In this paper, we investigated the anisotropy prob-
lem through the lens of the drift effect, and made
several contributions to the understanding of this
phenomenon. We demonstrated that anisotropy can
be observed in language models with character-
aware architectures, extended our observations to
Transformers trained on other modalities, and stud-
ied anisotropy in untrained Transformers layers.
We finally explored the training dynamics of the
query and key distributions, and found that they
drift along a shared direction hence maximizing
QKT scalar products in absolute value, allowing
stronger attention patterns as a result.
We conclude that anisotropy almost systemati-
cally affects Transformers on all modalities, in a
way that is not always correlated with the drift of
the representations. We also provide empirical evi-
dence that anisotropy appears as an inherent prop-
erty of latent distributions used in the self-attention
mechanism when modeling sharp attention patterns.
We hypothesize that a revision of the self-attention
operation could help reduce anisotropy by facil-
itating the emergence of sharp attention softmax
distributions without distorting the geometry of the
hidden representations.
Limitations
As mentioned in the Discussion section, we ac-
knowledge that section 4 does not take into account
the training dynamics, and only exposes some prop-
erties of the Transformer layer at initialization. We
also notice that the Spearman correlation test used
in Figure 5 may not be well-suited for such noisy
observations, as the high p-value of the GPT-2
model shows. We provide a similar graph based on
the Pearson correlation in Appendix A.
Moreover, we are aware that our approach is
not theoretically rigorous in some aspects. For in-
stance, we don’t prove that sharp self-attention pat-
terns cannot emerge without anisotropy in keys and
queries representations. In other words, this arti-
cle is focusing on exposing and correlating factors
that explain anisotropy, but we do not demonstrate
theoretical properties that would help identify the
causes of anisotropy. Nevertheless, we believe that
our work can pave the way for such theoretical
exploration in the future.
Ethics Statement
To the best of our knowledge, our work does not
raise any ethical concern. However, as noted in
Zhou et al. (2021), we believe that distortions in
the embedding space may be related to bias in the
training data, whether it is inherent to the structure
of the modality (e.g. the Zipfian distribution of
words), or due to human factors (e.g. geographical
considerations).
Acknowledgements
This work was funded by the last authors’ chair
in the PRAIRIE institute funded by the French na-
tional agency ANR as part of the “Investissements
d’avenir” programme under the reference ANR-
19-P3IA-0001. This work was granted access to
the HPC resources of IDRIS under the allocation
2023-AD011013680R1 made by GENCI.
We would like to thank Roman Castagné for
useful discussions that led to focusing on observing
the effect of anisotropy in the self-attention process.
References
Mira Ait-Saada and Mohamed Nadif. 2023.
Is
anisotropy truly harmful? a case study on text cluster-
ing. In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers), pages 1194–1203, Toronto, Canada.
Association for Computational Linguistics.
R. Ardila, M. Branson, K. Davis, M. Henretty,
M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M.
Tyers, and G. Weber. 2020.
Common voice: A
massively-multilingual speech corpus. In Proceed-
ings of the 12th Conference on Language Resources
and Evaluation (LREC 2020), pages 4211–4215.
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020. wav2vec 2.0: A framework
for self-supervised learning of speech representations.
In Advances in Neural Information Processing Sys-
tems, volume 33, pages 12449–12460. Curran Asso-
ciates, Inc.
Hangbo Bao, Li Dong, and Furu Wei. 2021.
Beit:
BERT pre-training of image transformers. CoRR,
abs/2106.08254.
Geetanjali Bihani and Julia Rayz. 2021. Low anisotropy
sense retrofitting (LASeR) : Towards isotropic and
sense enriched representations. In Proceedings of
Deep Learning Inside Out (DeeLIO): The 2nd Work-
shop on Knowledge Extraction and Integration for
Deep Learning Architectures, pages 81–95, Online.
Association for Computational Linguistics.
Daniel Bi´s, Maksim Podkorytov, and Xiuwen Liu. 2021.
Too much in common: Shifting of embeddings in
transformer language models and its implications.
In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 5117–5130, Online. Association for Computa-
tional Linguistics.
Jonathan H. Clark, Dan Garrette, Iulia Turc, and John
Wieting. 2022.
Canine: Pre-training an efficient
tokenization-free encoder for language representa-
tion. Transactions of the Association for Computa-
tional Linguistics, 10:73–91.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and
Christopher D. Manning. 2019. What does BERT
look at? an analysis of BERT’s attention. In Pro-
ceedings of the 2019 ACL Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP,
pages 276–286, Florence, Italy. Association for Com-
putational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Hicham El Boukkouri, Olivier Ferret, Thomas Lavergne,
Hiroshi Noji, Pierre Zweigenbaum, and Jun’ichi Tsu-
jii. 2020. CharacterBERT: Reconciling ELMo and
BERT for word-level open-vocabulary representa-
tions from characters. In Proceedings of the 28th
International Conference on Computational Linguis-
tics, pages 6903–6915, Barcelona, Spain (Online).
International Committee on Computational Linguis-
tics.
Kawin Ethayarajh. 2019. How contextual are contextu-
alized word representations? Comparing the geom-
etry of BERT, ELMo, and GPT-2 embeddings. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 55–65,
Hong Kong, China. Association for Computational
Linguistics.
Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-
Yan Liu. 2019. Representation degeneration problem
in training natural language generation models. In
7th International Conference on Learning Represen-
tations, ICLR 2019, New Orleans, LA, USA, May 6-9,
2019. OpenReview.net.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894–6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Nathan Godey, Roman Castagné, Éric de la Clergerie,
and Benoît Sagot. 2022. MANTa: Efficient gradient-
based tokenization for end-to-end robust language
modeling. In Findings of the Association for Com-
putational Linguistics: EMNLP 2022, pages 2859–
2870, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-
Ming Cheng, and Shi-Min Hu. 2022. Visual attention
network. arXiv preprint arXiv:2202.09741.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,
Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-
rahman Mohamed. 2021. Hubert: Self-supervised
speech representation learning by masked prediction
of hidden units. IEEE/ACM Transactions on Audio,
Speech, and Language Processing, 29:3451–3460.
Katharina Hämmerl, Alina Fastowski, Jindˇrich Li-
bovický, and Alexander Fraser. 2023.
Exploring
anisotropy and outliers in multilingual language mod-
els for cross-lingual semantic sentence similarity.
Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by re-
ducing internal covariate shift. In Proceedings of the
32nd International Conference on Machine Learn-
ing, volume 37 of Proceedings of Machine Learning
Research, pages 448–456, Lille, France. PMLR.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining
approach. CoRR, abs/1907.11692.
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Fe-
ichtenhofer, Trevor Darrell, and Saining Xie. 2022. A
convnet for the 2020s. Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition (CVPR).
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els. CoRR, abs/1609.07843.
Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, and
Felice Dell’Orletta. 2022. Outlier dimensions that
disrupt transformers are driven by frequency. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2022, pages 1286–1304, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-
man, Christine McLeavey, and Ilya Sutskever. 2022.
Robust speech recognition via large-scale weak su-
pervision.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research,
21(140):1–67.
Sara Rajaee and Mohammad Taher Pilehvar. 2021. A
cluster-based approach for improving isotropy in con-
textual embedding space. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers), pages 575–584, Online. Association
for Computational Linguistics.
Sara Rajaee and Mohammad Taher Pilehvar. 2022. An
isotropy analysis in the multilingual BERT embed-
ding space. In Findings of the Association for Com-
putational Linguistics: ACL 2022, pages 1309–1316,
Dublin, Ireland. Association for Computational Lin-
guistics.
William Rudman and Carsten Eickhoff. 2023. Stable
anisotropic regularization.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexan-
der C. Berg, and Li Fei-Fei. 2015. ImageNet Large
Scale Visual Recognition Challenge. International
Journal of Computer Vision (IJCV), 115(3):211–252.
Thibault Sellam, Steve Yadlowsky, Jason Wei, Naomi
Saphra, Alexander D’Amour, Tal Linzen, Jasmijn
Bastings, Iulia Turc, Jacob Eisenstein, Dipanjan Das,
Ian Tenney, and Ellie Pavlick. 2021. The multiberts:
Bert reproductions for robustness analysis. arXiv
preprint arXiv:2106.16163.
Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou.
2021. Whitening sentence representations for bet-
ter semantics and faster retrieval.
arXiv preprint
arXiv:2103.15316.
Mingxing Tan and Quoc V. Le. 2019. Efficientnet: Re-
thinking model scaling for convolutional neural net-
works. ArXiv, abs/1905.11946.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran-
cisco Massa, Alexandre Sablayrolles, and Herve Je-
gou. 2021. Training data-efficient image transform-
ers & distillation through attention. In Proceedings
of the 38th International Conference on Machine
Learning, volume 139 of Proceedings of Machine
Learning Research, pages 10347–10357. PMLR.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models.
Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu,
Guangtao Wang, and Quanquan Gu. 2020. Improv-
ing neural language generation with spectrum control.
In International Conference on Learning Representa-
tions.
Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan,
Peizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer,
and Péter Vajda. 2020. Visual transformers: Token-
based image representation and processing for com-
puter vision. ArXiv, abs/2006.03677.
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,
Xiyang Dai, Lu Yuan, and Lei Zhang. 2021. Cvt: In-
troducing convolutions to vision transformers. arXiv
preprint arXiv:2103.15808.
Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anand-
kumar, Jose M. Alvarez, and Ping Luo. 2021.
Segformer:
Simple and efficient design for se-
mantic segmentation with transformers.
CoRR,
abs/2105.15203.
Linting Xue, Aditya Barua, Noah Constant, Rami Al-
Rfou, Sharan Narang, Mihir Kale, Adam Roberts,
and Colin Raffel. 2022. ByT5: Towards a token-free
future with pre-trained byte-to-byte models. Transac-
tions of the Association for Computational Linguis-
tics, 10:291–306.
Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang,
Wei Wu, and Weiran Xu. 2021. ConSERT: A con-
trastive framework for self-supervised sentence repre-
sentation transfer. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 5065–5075, Online. Association for
Computational Linguistics.
Sangwon Yu, Jongyoon Song, Heeseung Kim, Seong-
min Lee, Woo-Jong Ryu, and Sungroh Yoon. 2022.
Rare tokens degenerate all tokens: Improving neural
text generation via adaptive gradient gating for rare
token embeddings. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 29–45,
Dublin, Ireland. Association for Computational Lin-
guistics.
Kaitlyn Zhou, Kawin Ethayarajh, and Dan Jurafsky.
2021. Frequency-based distortions in contextualized
word embeddings. CoRR, abs/2104.08465.
A
Pearson correlation of the drift norm
and anisotropy
Figure 16: p-value of the Pearson correlation test be-
tween the norm of the average representation and the
cosine-similarity averaged over all layers, across modal-
ities. Models above the red dotted line are not signifi-
cantly affected by the drift effect.
The Pearson test measures a linear correlation be-
tween random variables, while the Spearman test
measures a monotonic correlation. As there is
no specific argument in favor of a linear relation-
ship between the measured distributions (average
cosine-similarity and norm of the average represen-
tation), we decided to use the Spearman correlation
test in order to take into account more complex
relation patterns.
Nevertheless, this metric is based on the rank of
each observation, and is thus not robust to fluctu-
ations due to sample variance, specifically when
working with such small samples. This is reflected
by the discrepancy between Pearson and Spearman
p-values for some models (e.g. GPT-2).
B
Cosine-similarity and anisotropy
Figure 17: Density function of cosine-similarity for a
normal distribution as the dimension increases.
Figure 18: 95th quartile of the cosine-similarity distribu-
tion on a normal distribution as the dimension increases.
We add points for the average cosine-similarity level of
Transformers models for several modalities.
It can be argued that describing anisotropy as
the observation of ""high"" cosine-similarity values
is not a convincing definition. This section aims
at showing which ranges of cosine-similarity val-
ues are characteristic of anisotropic distributions.
In Figure 17, we show the density function of the
cosine-similarity values obtained when drawing
pairs of samples from isotropic normal distribu-
tions in Rd as d increases.
For smaller dimensions (d = 16), we see that the
range of cosine-similarity values that are attained
between isotropic distributions is relatively broad
compared to the possible spectrum ([−1, 1]). As
d increases, the support of the observed distribu-
tions seems to become smaller, due to the curse of
dimensionality.
We analyze this effect more in-depth in Fig-
ure 18, where we plot the 95th quantile of the
cosine-similarity distribution in the isotropic sce-
nario. We also add values for the layer-wise av-
erage cosine-similarity levels of typical models
in several modalities for comparison.
We can
clearly observe that the levels of cosine-similarity
observed in the representations of Transformers-
based models are significantly unlikely to be ob-
served in between samples drawn in isotropic nor-
mal distributions.
Nevertheless, as we go towards higher dimen-
sional spaces for bigger models (e.g. Llama-65B
from Touvron et al. (2023) has 8192 hidden di-
mensions), we believe that it may be relevant to
introduce isotropy metrics that are grounded to
isotropic cosine-similarity distributions. We leave
this question for future works.
C
Other projections for Qs and Ks
As mentioned in the Discussion (section 6), we
reproduce visualizations from section 5 using dif-
ferent projection choices. Namely, we compute the
SVD on Ks only in Figure 19 and Figure 21, and
on Qs only in Figure 20 and Figure 22.
The plots show that not only does the distribu-
tion used for the SVD drifts away from the origin
along training, but also that the other distribution
drifts away from the origin in an opposite direc-
tion. In other words, the singular components of
each distribution are also relevant to describe the
drift of the other distribution. Hence, Figure 19
and Figure 20 support our conclusion that the drift
directions of keys and queries are aligned during
training.
D
Stability across MultiBERT seeds
(a) Step 0
(b) Step 40k
(c) Step 200k
(d) Step 2M (final)
Figure 19: Evolution of Qs and Ks distributions along training. Vectors are projected using the SVD computed on
Ks.
(a) Step 0
(b) Step 40k
(c) Step 200k
(d) Step 2M (final)
Figure 20: Evolution of Qs and Ks distributions along training. Vectors are projected using the SVD computed on
Qs.
(a) Similar
(b) Opposite
Figure 21: Evolution of ¯
Qs and ¯
Ks along training for
two different heads in the network, projected via the
SVD of Ks.
(a) Similar
(b) Opposite
Figure 22: Evolution of ¯
Qs and ¯
Ks along training for
two different heads in the network, projected via the
SVD of Qs.
(a) Layer 0
(b) Layer 2
(c) Layer 6
(d) Layer 11
Figure 23: Evolution of cosine-similarity between ¯
Qs and ¯
Ks along training for various initialization seeds.
Representations are concatenated across heads, and each color represents one seed of the MultiBERT models. We
observe similar trends across seeds.
"
"This paper presents a Threshold Logic (TL) inspired Spin Wave (SW) computing paradigm.  The SW TL gate concept and mechanism of mirroring TL gate weights and threshold values into physical parameters are discussed. Micro-magnetic simulations are performed to design and demonstrate the operation of a SW TL based Full Adder (FA). Finally, potential advantages of the proposed scheme compared with majority logic based implementations are highlighted.","The amount of data processed every day has significantly increased. However, computing devices efficiency and power consumption has not scaled equally with the increase of data causing a big jump in the global IT power consumption. To address this issues, new FET architectures like FinFET have been proposed and technologies that completely depart from CMOS, commonly referred to as beyond-CMOS, are under scrutiny, as they can potentially enable new avenues for power efficient data processing. Among those, Spintronics, which makes use of electron spin for information encoding, provides powerful means for the implementation of low power circuits, efficient non-volatile memories, and neuromorphic circuits. Recently, Spin Waves (SW), which are small collective magnetization deviations that travel in a wave like manner through a magnetised material, received special attention as their high frequency and small wavelength provide premises for fast and small circuit implementations.","In this paper we leave the SW interaction avenue and propose Threshold Logic (TL) inspired SW computing. Current research on SW logic mostly revolves around the implementation of classic Boolean functions, by encoding binary data in SW phase and letting an odd number of unit amplitude SWs interfere within a common waveguide. Due to the very nature of the SW interference process it provides natural support for majority function evaluation. Essentially speaking, related to a certain reference, input SW are either in phase (logic 0) or π∅ out of phase (logic 1). Within the waveguide in-phase/out-of-phase SWs constructively/distinctively interact resulting in a SW having the phase of the majority of the SW inputs. The majority value can be obtained in its direct and/or inverted form by properly adjusting the output SW amplitude reading position.nan","To implement an n-input TL gate, instead of inducing multiple SWs and letting them interact within a waveguide, we make use of one single SW on which we induce n + 1 successive phase rotations. The final SW phase sign carries the gate output value, i.e., negative logic 0 and positive logic 1.  The SW TL gate design methodology includes introducing the novel concept, providing inside on the SW TL gate design process, and providing preliminary inside on the potential impact of the proposal.","The working principle of a SW TL based Full Adder (FA) is presented, which is a heavily utilized basic building block in computation platform designs, and conceptually compare MAJ3 and TLG based implementations. The FA outputs Cout and Sum by means of two TL gates each of them including 4 and 5 phase-shifters, respectively. Tables of net phase shift observed by means of micro-magnetic simulations at the output of the TLG producing Cout and Sum, respectively, for all possible FA input combinations are presented.  It is observed that the two FA outputs are correctly evaluated, according to the FA truth table.","In this paper we introduced a novel Threshold Logic (TL) inspired Spin Wave (SW) based computing paradigm, which relies on successive SW phase rotations applied to one single SW instead of on SW interference. We introduced the SW TL gate concept and discussed ways to mirror TL gate weight and threshold values into physical device parameters. We designed and demonstrated proper operation of an SW TL based Full Adder (FA) by means of micro-magnetic simulations. We also provided high level evidence that our proposal can potentially outperform functionally equivalent SW interference based implemented counterparts.",Spin Wave Threshold Gate,"Arne Van Zegbroeck, Pantazis Anagnostou, Said Hamdioui, Christop Adelmann, Florin Ciubotaru, Sorin Cotofana","While Spin Waves (SW) interaction provides natural support for low power
Majority (MAJ) gate implementations many hurdles still exists on the road
towards the realization of practically relevant SW circuits. In this paper we
leave the SW interaction avenue and propose Threshold Logic (TL) inspired
SW computing, which relies on successive phase rotations applied to one
single SW instead of on the interference of an odd number of SWs. After
providing a short TL inside we introduce the SW TL gate concept and dis-
cuss the way to mirror TL gate weight and threshold values into physical
phase-shifter parameters. Subsequently, we design and demonstrate proper
operation of a SW TL based Full Adder (FA) by means of micro-magnetic
simulations. We conclude the paper by providing inside on the potential ad-
vantages of our proposal by means of a conceptual comparison of MAJ and
TL based FA implementations.
1
arXiv:2401.12136v1  [cs.ET]  22 Jan 2024
Spin Wave Threshold Logic Gates
Arne Van Zegbroeck, Pantazis Anagnostou, Said Hamdioui,
Christop Adelmann, Florin Ciubotaru, Sorin Cotofana
1 Introduction
The amount of data that is processed every day has significantly increased over the
past decade [21]. However, the computing devices efficiency and power consumption
has not scaled equally with the increase of data causing a big jump in the global IT
power consumption in that time period [7]. This is even aggravated by the fact that
transistor scaling is running into more and more problems due to short channel effects,
power density and gate tunneling, to name a few [23].
To address this issues, new
FET architectures like FinFET have been proposed [6] and technologies that completely
depart from CMOS, commonly referred to as beyond-CMOS, are under scrutiny, as
they can potentially enable new avenues for power efficient data processing. Examples
include, but are not limited to, Graphene [2, 11], Quantum [10], and Spintronics [8, 1].
Among those, Spintronics, which makes use of electron spin for information encod-
ing, provides powerful means for the implementation of low power circuits, efficient
non-volatile memories [9], and neuromorphic circuits [27]. Recently, Spin Waves (SW),
which are small collective magnetization deviations that travel in a wave like manner [4]
through a magnetised material, received special attention as their high frequency and
small wavelength provide premises for fast and small circuit implementations [4, 14].
Current research on SW logic mostly revolves around the implementation of classic
Boolean functions [14], by encoding binary data in SW phase and letting an odd number
of unit amplitude SWs interfere within a common waveguide. Due to the very nature
of the SW interference process it provides natural support for majority function evalu-
ation. Essentially speaking, related to a certain reference, input SW are either in phase
(logic 0) or π◦ out of phase (logic 1). Within the waveguide in-phase/out-of-phase SWs
constructively/distinctively interact resulting in a SW having the phase of the majority
of the SW inputs. The majority value can be obtained in its direct and/or inverted form
by properly adjusting the output SW amplitude reading position [15]. By following this
concept 3-input majority gates (MAJ3) have been proposed and simulated and/or ex-
perimentally demonstrated [16, 22, 15]. As MAJ3 and inverter form a universal gate
set any Boolean function can be implemented by following this paradigm.
In this paper we leave the SW interaction avenue and propose Threshold Logic (TL)
inspired SW computing. To implement an n-input TL gate, instead of inducing multiple
2
SWs and letting them interact within a waveguide, we make use of one single SW on
which we induce n + 1 successive phase rotations. The final SW phase sign carries the
gate output value, i.e., negative logic 0 and positive logic 1. We introduce this novel
concept, provide inside on the SW TL gate (SWTLG) design methodology, and provide
preliminary inside on the potential impact of our proposal.
The paper is organized as follows: In Section 2 we briefly present Threshold Logic (TL)
fundamentals. In Section 3 we introduce the novel concept of SW phase manipulation
based computing and in Section 4 provide inside on SW phase manipulation and the SW
TL gate design process. In Section 5 we present a Full Adder (FA) SW TL gate design,
validate it by means of micro-magnetic simulations, and compare it with a MAJ3 based
counterpart. We conclude the paper with some final remarks and future work directions.
2 Threshold Logic
Figure 1: Basic threshold logic gate.
As in this paper we bring Threshold rather than Boolean Logic into the framework
of SW computing we briefly introduce Threshold Logic (TL) fundamentals. For more
inside on this paradigm we refer the reader to [17]. TL makes use of the basic gate,
depicted in Figure 1, which evaluates the weighted sum of its inputs, and compares this
value with a given threshold value. Note that such a gate corresponds to the Boolean
output neuron introduced in the McCulloch-Pitts neural model [17] with no learning
features. It initially computes (1) where xiϵ{0, 1}, wi are integer weights, and ψ the
threshold value,
f(x) =
i
X
n=1
wixi − ψ
(1)
and the actual gate output is computed as:
F(x) = sgn(f(x)) =
(
1,
f(x) ≥ 0
0,
f(x) < 0
3
Such a TL gate (TLG) can evaluate basic Boolean functions as AND/NAND and
OR/NOR (see 2 for TL evaluation of AND), but can also perform more complex calcu-
lations.
AND = sgn(x1 + x2 − 2)
(2)
For example the Full Adder (FA) TL implementation (FA is an essential building block
for data processing hardware) can be done with 2 TLGs [13] as follows:
Cout
=
sgn(x1 + x2 + Cin − 2)
(3)
Sum
=
sgn(x1 + x2 + Cin − 2Cout − 1)
(4)
Previous research demonstrated that TL implementations of basic arithmetic functions
can outperform Boolean counterparts in terms of circuit complexity [26, 25, 5] and TLG
implementations have been proposed in CMOS [18, 19] and in emerging technologies
[12].
3 SW Threshold gate concept
Figure 2 depicts our proposed gate concept. The green transducer, which can be an RF
antenna or a magneto-electric cell [3], generates a SW, the orange transducers can only
induce a phase shift ±pi on the SW, and the blue transducer reads the final net phase
change induced by the joint action of the orange transducers. If we use the initial SW
phase as reference and phase-shifter i, i = 1 . . . , n produce a phase shift proportional
with xiwi, and phase-shifter n + 1 a phase shift proportional with ψ the net phase shift
is proportional with the f(x) value computed by (1). Finally, the TLG output according
to (2) is determined by checking the sign of net phase change, i.e., ≥ 0 corresponds to
logic 1, and logic 0 otherwise.
Thus, to implement an n-input TL gate we need n phase-shifters, each of them enabled
by xi, i = 1, . . . , n and producing a phase shift that modulates wi and one always active
phase-shifter inducing a phase shift that modulates ψ. Note that for proper operation
the actual phase change value per each shifter should be determined in such a way that
the net phase shift does not exceed 360◦, as further discussed in the following section.
P
n1
n2
ni
...
ϕ
Figure 2: SW phase shift based threshold logic gate.
4 SW phase manipulation
The behaviour of a SW within a waveguide is captured by the dispersion relation, which
reflects the relation between SW frequency and its wave-number or wavelength. Accord-
ing to [4] (5) presents an approximation of the SW dispersion relation. Note that in
4
Figure 3: Dispersion relation (5) plot for a 200 nm wide and 9 nm thick CoFeB waveguide
using the parameters specified in [14].
Figure 4: FA Cout evaluation TL gate structure.
our investigation we assume backwards volume SW, which are waves that travel along
the magnetization direction, and only consider waves traveling along the long waveguide
axis, i.e., θk = 0 and θm = 0.
ω(k) =
p
(lωH + ωM λexktot)( ωH + ωM λexktot + ωM F),
(5)
where ωH = γµ0Heff, ωM = γµ0Ms, γ = 1.76 ∗ 1011rad/(s T) is the gyromagnetic
ratio, µ0Heff is the effective internal magnetic field, Ms is the saturation magnetization,
ktot = k2 + (nπ/w)2, Aex is the exchange constant, and F is expressed as
F = 1 − gcos2(θk − θM ) +
ωM g(1−g)sin2(θk−θM )
(ωH +ωM λex
h
k2+

nπ/w
2i ,
(6)
where θk = atan[nπ/(kw)], g = 1 − [1 − exp(−d√ktot)]/(d√ktot), and θk is the angle
between SW wave vector and the long axis of the waveguide. Similarly θM is the angle
between the waveguide magnetization and the waveguide long axis. Figure 3 depicts a
dispersion relation (5) plot for external magnetic fields varying from 0.1 T to −0.1 T.
As indicated by Maxwell’s equations, when a current passes through a wire, it gener-
ates a static magnetic field around it. That magnetic field acts on the waveguide and
changes the dispersion relation in the area on which the magnetic field is applied. In the
dispersion relation ωH is related to the effective internal magnetization, which in turn
5
Figure 5: SW phase shift induced by a single 200 nm wide phase-shifter when making
use of the parameters in [14]. The relation is linear around no applied field
point and becomes less linear for larger field values.
relates to the magnetization acting on the waveguide. Thus by changing the current
through the phase-shifter we change the generated magnetic field which correlates to
the applied phase shift. The effect is visualised in Figure 3 where it can be observed
that the dispersion relation moves to higher frequencies for the same wavenumber when
applying a higher field and the opposite happens when applying a negative magnetic
field by changing the direction of the current flowing through the phase-shifter.
All simulations are performed using mumax3 [24]. We use a CoFeB waveguide with
the parameters from [14] with dimensions of 2024 nm by 32 nm by 9 nm with a cell
size of 2 nm by 2 nm by 3 nm. The material parameters are: Ms = 1.36 ∗ 106A/m,
Aex = 18.6 ∗ 10−12J/m, Alpha = 0.004. SW are generated on the left side of the waveg-
uide with a sinusoidal external magnetic field applied on the x-axis with a frequency of
35 GHz and a field strength of 5 mT. The phase-shifters are simulated by applying static
magnetic fields in small strips on the waveguide. We initially consider a strip size of
200 nm, and when simulating a single phase-shifter we place it in the waveguide center,
while when considering multiple phase-shifters they were placed at least 200 nm apart
from each other to make sure that they did not influence one another. Figure 8 depicts a
zoom-in capturing the simulated behaviour of a SW travelling through a waveguide with
two phase-shifters covering the waveguide area between the green and red vertical lines,
respectively. The green phase-shifter induces a magnetic field in the opposite direction
of the waveguide magnetization. When entering this area the SW shrinks because as
indicated in Figure 3, a negative magnetic fields shifts the dispersion relation down,
which results in larger wavenumber and smaller wavelength for the same SW frequency.
A smaller wavelength and same frequency means that the SW travels slower while un-
derneath the phase-shifter resulting in a backward phase shift when comparing with the
unperturbed SW in the bottom waveguide. In this simulation, the first phase-shifter
6
Figure 6: SW phase shift dependency on SW frequency for a single 200 nm wide phase-
shifter when making use of the parameters in [14]. Note that low frequency
SWs experience a larger phase shift for the same magnetic field value.
100
120
140
160
180
200
220
240
260
280
300
Phaseshifter Size (nm)
6
8
10
12
14
16
18
20
22
Phase shift (degrees)
Phase shift caused by different size Phaseshifter
Figure 7: SW phase shift value vs phase-shifter size.
7
Figure 8: Working example of our device visually showing the shortening/lengthening of
the wavelength under the phase-shifters.
produces a 360◦ phase shift, which results in an unchanged phase at its output. The
red phase-shifter applies a magnetic field along the waveguide magnetization direction,
thus it shifts the dispersion relation up, which results in an increased wavelength and
by implication an increase SW velocity. When exiting the red phase-shifter the wave is
significantly forward shifted, as one can clearly observe in the Figure.
To be able to design shifters able to induce phase changes proportional with TLG wi
and ψ values we need to capture the relation between shifter parameters and the induced
the phase shift. First, using a single phase-shifter, we simulated the effect of magnetic
field strengths on the achieved phase shift angle and the results are presented in Figure
5. When applying small magnetic fields (±10 mT) the phase shift changes linearly as
indicated by R2 = 1.000, R2 closer to one portraying a higher linearity, rounded to 3
digits after the comma.
When applying larger magnetic fields (±100 mT) the phase
shift becomes less linear (R2 = 0.9993). Moreover, the system exhibits asymmetry, as
when applying large positive magnetic fields, the phase shift magnitude is larger then
the one induced by the same magnetic fields applied in the opposite direction. This is
related to the dispersion relation non linearity at that frequency, clearly observable in
the dispersion relation plot in Figure 3. It is also clear that by substantially increasing
the applied magnetic field, we can phase shift with larger angles.
Next we looked at the behaviour of different frequency SWs when passing under the
same phase phase-shifter. The results are visualised in Figure 6 for 30 GHz, 35 GHz,
and 40 GHz SWs. As expected from the dispersion relation in Figure 3, the phase shift
is larger for lower frequency SWs and smaller for higher frequency SWs. The linearity
of the phase shift also decreases for lower frequencies because the dispersion relation is
less linear at those lower frequencies. This can be seen in R2
30 = 0.9991, R2
35 = 0.9993,
and R2
40 = 0.9998, which increases when increasing the SW frequency. This can be also
intuitively deduce by observing the dispersion relation plot: as we go to a lower frequency,
the slope becomes smaller, meaning that that the reverse of the slope becomes bigger
resulting in larger positive and negative phase shifts.
Following up, when decreasing the phase-shifter size we observe a phase shift decrease,
8
Table 1: Net Phase Shift ∆ϕ for Cout
a
b
Cin
∆ϕ
Cout
0
0
0
−19.6668
0
1
0
0
−9.6686
0
0
1
0
−9.6748
0
0
0
1
−9.7002
0
1
1
0
0.3165
1
0
1
1
0.3260
1
1
0
1
0.3306
1
1
1
1
10.3377
1
while a size increase results in a larger phase shift. Figure 7 depicts the due to an external
field of 0.01 T phase shift dynamics when increasing the phase-shifter size from 100 nm
to 300 nm. As expected, a larger phase-shifter induces a larger phase shift as the SW is
slowed or sped up over a longer propagation distance.
5 Full Adder design and simulation
In this section we present the TL implementation of the Full Adder outputs Cout and
Sum by means of two TL gates each of them including 4 and 5 phase-shifters, respec-
tively. We made use of 100 nm wide phase-shifters calibrated to induce a 10◦ phase shift
per input weight unit. This corresponds to an applied external field of wi × 0.0147 T
when the input xi = 1 and no field otherwise, on each TLG input. Table 1 and Table
2 present the net phase shift observed by means of micro-magnetic simulations at the
output of the TLG producing Cout and Sum, respectively, for all possible FA input com-
binations. Given that when reading out a phase shift of 0◦ or higher, the TLG outputs
a logic 1 and a negative phase shift results in a logic 0, one can easily observe that the
two FA outputs are correctly evaluated, according to the FA truth table. Note that the
phase shifts are not exact multiples of 10◦ due to the dispersion relation non-symmetry
when applying the same field magnitude in opposite directions. The phase shift pro-
duced by all possible FA input combinations is also visualised in Figure 9 and 10, for
the TLG producing Cout and Sum outputs, respectively.
To get inside into the potential practical implications of the proposed SW TLG we
assume as discussion vehicle the SW implementation of a Full Adder (FA), which is a
heavily utilized basic building block in computation platform designs, and conceptually
compare MAJ3 and TLG based implementations.
The MAJ3 FA implementation relies on the following equations [20]:
Cout
=
MAJ3(x1, x2, Cin)
(7)
Sum
=
MAJ3(Cout, MAJ3(x1, x2, Cout), Cin).
(8)
Thus, it requires 3 MAJ3 gates and exhibit a 2 MAJ3 gates delay, actually a bit larger
as while generating Cout does not require an inverter it induces a small delay overhead
9
000
100
010
001
110
011
101
111
no shift
32 nm
219 nm
Figure 9: Cout TLG output phase shift for all possible FA input combinations.
Table 2: Net Phase Shift ∆ϕ for Sum
a
b
Cin
Cout
∆ϕ
Sum
0
0
0
0
−9.9041
0
1
0
0
0
0.0987
1
0
1
0
0
0.1043
1
0
0
1
0
0.0891
1
1
1
0
1
−9.5451
0
0
1
1
1
−9.5632
0
1
0
1
1
−9.5711
0
1
1
1
1
0.4538
1
10
000
100
010
001
110
011
101
111
no shift
32 nm
219 nm
Figure 10: Sum TLG output phase shift for all possible FA input combinations.
11
due to the extra output transducer. The TLG FA implementation relies on (3) and (4)
and requires 2 TLG gates and exhibits a 2 TLG gates delay, thus it clearly outperforms
the MAJ3 implementation in terms of area.
While a more accurate comparison requires a detailed design of the two FA imple-
mentation and it is subject to future work we can also have a glimpse into other aspects
that may make TLGs more attractive. A MAJ3 gate requires 3 transducers to generate
the input SWs and one to read the output, thus the entire FA requires 12 transducers
(actually it may need one more for reading Cout). As an n-input TLG requires a trans-
ducer to generate the SW, n + 1 shifters, and one output reading transducer, the TLG
FA requires 4 transducers and 9 phase-shifters. Given that phase-shifters are potentially
smaller than transducers the TLG waveguides are smaller, thus faster. Moreover, while
transducers are RF operated the shifters require DC inputs, which may induce further
advantages in terms of power consumption. Last, but not least inline majority gates may
significantly suffer from imprecise manufacturing as the transducers should be placed at
precise distances from each other in order to enable proper MAJ3 gate operation, while
TLGs are more robust as the phase-shifters positioning does not need to be that precise.
6 Conclusions
In this paper we introduced a novel Threshold Logic (TL) inspired Spin Wave (SW) based
computing paradigm, which relies on successive SW phase rotations applied to one single
SW instead of on SW interference. We introduced the SW TL gate concept and discussed
ways to mirror TL gate weight and threshold values into physical device parameters.
We designed and demonstrated proper operation of an SW TL based Full Adder (FA)
by means of micro-magnetic simulations.
We also provided high level evidence that
our proposal can potentially outperform functionally equivalent SW interference based
implemented counterparts.
7 acknowledgements
This project is supported by the imec industrial affiliate program on Beyond CMOS
Logic. It has also been supported by SPIDER, EC contract number 101070417; Topic
advanced spintronics; Unleashing spin into the next generation ICs (RIA).
References
[1] S. D. Bader.
Colloquium: Opportunities in nanomagnetism.
Rev. Mod. Phys.,
78:1–15, Jan 2006.
[2] S. K. Banerjee, L. F. Register, E. Tutuc, D. Basu, S. Kim, D. Reddy, and A. H.
MacDonald. Graphene for cmos and beyond cmos applications. Proceedings of the
IEEE, 98(12):2032–2046, 2010.
12
[3] S. Cherepov, P. Khalili Amiri, J. G. Alzate, K. Wong, M. Lewis, P. Upadhyaya,
J. Nath, M. Bao, A. Bur, T. Wu, G. P. Carman, A. Khitun, and K. L. Wang.
Electric-field-induced spin wave generation using multiferroic magnetoelectric cells.
Applied Physics Letters, 104(8):082403, 02 2014.
[4] A. V. Chumak. Fundamentals of magnon-based computing. arXiv: Mesoscale and
Nanoscale Physics, 2019.
[5] S. Cotofana and S. Vassiliadis.
Periodic symmetric functions, serial addition,
and multiplication with neural networks. IEEE Transactions on Neural Networks,
9(6):1118–1128, 1998.
[6] U. K. Das and T. K. Bhattacharyya.
Opportunities in device scaling for 3-nm
node and beyond: Finfet versus gaa-fet versus ufet. IEEE Transactions on Electron
Devices, 67(6):2633–2638, 2020.
[7] C. Y. Gelenbe E. The impact of information technology on energy consumption
and carbon emissions. ACM, pages 1–15, 2015.
[8] A. Hirohata, K. Yamada, Y. Nakatani, I.-L. Prejbeanu, B. Di´eny, P. Pirro, and
B. Hillebrands. Review on spintronics: Principles and device applications. Journal
of Magnetism and Magnetic Materials, 509:166711, 2020.
[9] A. Hirohata, K. Yamada, Y. Nakatani, I.-L. Prejbeanu, B. Di´eny, P. Pirro, and
B. Hillebrands. Review on spintronics: Principles and device applications. Journal
of Magnetism and Magnetic Materials, 509:166711, Sept. 2020.
[10] J. Hutchby, G. Bourianoff, V. Zhirnov, and J. Brewer. Extending the road beyond
cmos. IEEE Circuits and Devices Magazine, 18(2):28–41, 2002.
[11] Y. Jiang, N. C. Laurenciu, H. Wang, and S. D. Cotofana. Graphene nanoribbon
based complementary logic gates and circuits. IEEE Transactions on Nanotechnol-
ogy, 18:287–298, 2019.
[12] C. Lageweg, S. Cotofana, and S. Vassiliadis. A linear threshold gate implementation
in single electron technology. In Proceedings IEEE Computer Society Workshop on
VLSI 2001. Emerging Technologies for VLSI Systems, pages 93–98, 2001.
[13] C. Lageweg, S. Cotofana, and S. Vassiliadis. A full adder implementation using
set based linear threshold gates. In 9th International Conference on Electronics,
Circuits and Systems, volume 2, pages 665–668 vol.2, 2002.
[14] A. Mahmoud, F. Ciubotaru, F. Vanderveken, A. V. Chumak, S. Hamdioui, C. Adel-
mann, and S. Cotofana. Introduction to spin wave computing. Journal of Applied
Physics, 128(16):161101, 2020.
[15] A. Mahmoud, F. Vanderveken, C. Adelmann, F. Ciubotaru, S. Hamdioui, and
S. Cotofana. Fan-out enabled spin wave majority gate. AIP Advances, 10(3):035119,
03 2020.
13
[16] A. Mahmoud, F. Vanderveken, F. Ciubotaru, C. Adelmann, S. Cotofana, and
S. Hamdioui. n-bit data parallel spin wave logic gate. In 2020 Design, Automation
& Test in Europe Conference & Exhibition (DATE), pages 642–645, 2020.
[17] S. Muroga. Threshold logic and its applications. 1971.
[18] M. Padure, S. Cotofana, C. Dan, M. Bodea, and S. Vassiliadis. A new latch-based
threshold logic family. In 2001 International Semiconductor Conference. CAS 2001
Proceedings (Cat. No.01TH8547), volume 2, pages 531–534 vol.2, 2001.
[19] M. Padure, S. Cotofana, S. Vassiliadis, C. Dan, and M. Bodea. A low-power thresh-
old logic family. In 9th International Conference on Electronics, Circuits and Sys-
tems, volume 2, pages 657–660 vol.2, 2002.
[20] V. Pudi, K. Sridharan, and F. Lombardi. Majority logic formulations for paral-
lel adder designs at reduced delay and circuit complexity. IEEE Transactions on
Computers, 66(10):1824–1830, 2017.
[21] M. E. Richard L. Villars, Carl W. Olofson. Big data: what it is and why you should
care. IDC, pages 1–14, 2011.
[22] G. Talmelli, T. Devolder, N. Tr¨ager, J. F¨orster, S. Wintz, M. Weigand, H. Stoll,
M. Heyns, G. Sch¨utz, I. P. Radu, J. Gr¨afe, F. Ciubotaru, and C. Adelmann. Re-
configurable submicrometer spin-wave majority gate with electrical transducers.
Science Advances, 6(51):eabb4042, 2020.
[23] S. Thompson, R. Chau, T. Ghani, K. Mistry, S. Tyagi, and M. Bohr. In search of
”forever,” continued transistor scaling one new material at a time. IEEE Transac-
tions on Semiconductor Manufacturing, 18(1):26–36, 2005.
[24] A. Vansteenkiste, J. Leliaert, M. Dvornik, M. Helsen, F. Garcia-Sanchez, and
B. Van Waeyenberge.
The design and verification of mumax3.
AIP Advances,
4(10):107133, 2014.
[25] S. Vassiliadis and S. Cotofana. 7—2 counters and multiplication with threshold
logic. In Conference Record of The Thirtieth Asilomar Conference on Signals, Sys-
tems and Computers, volume 1, pages 192–196 vol.1, 1996.
[26] S. Vassilladis, S. Cotofana, and K. Bertels. 2-1 addition and related arithmetic
operations with threshold logic.
IEEE Transactions on Computers, 45(9):1062–
1067, 1996.
[27] D. Zhang, W. Zhao, L. Zeng, K. Cao, M. Wang, S. Peng, Y. Zhang, Y. Zhang, J.-
O. Klein, and Y. Wang. All Spin Artificial Neural Networks Based on Compound
Spintronic Synapse and Neuron. IEEE Transactions on Biomedical Circuits and
Systems, 10(4):828–836, Aug. 2016.
14
"
"A simple method for reducing the population size during a genetic algorithm (GA) run is presented. The method, called Simple Variable Population Sizing (SVPS), is configured by two parameters, one controlling the shrinking speed and the other controlling the severity of population resizing. The intuition justifying the approach is that a larger population size in an early stage of a GA run produces a wider exploration of the search landscape, while a smaller population size increases the exploitation of promising regions as the GA converges. Experiments were performed on different instances and complexities of decomposable trap functions. Results show that SVPS can outperform a fixed-size GA in terms of the number of evaluations needed for convergence.","Setting an adequate population size is key in GAs to preserve solution quality without wasting computational effort. A small problem instance needs a smaller population than a more complex problem. Improving GA performance is also possible by varying the population size during the GA run. The population sizing theory focuses on determining population size according to the difficulty of the problem. Despite some studies that include the idea of varying the population size during the GA run, the issue has not received much attention concerning variable population sizing schemes.","nanSome other varying population methods previously proposed in the Evolutionary Computation field have been reviewed. The methods described can be classified into adaptive methods (e.g., GAVaPS) and deterministic methods (e.g., RVPS). GAVaPS introduces a lifetime parameter that defines the number of generations that each individual is allowed to remain alive. APGA follows the same approach but with a lifetime fixed value. PRoFIGA varies the population size according to the improvement of the best fitness in the population. RVPS randomly changes the population size during the run, and the Saw-Tooth GA varies the population size according to a predefined shape. SRP-EA controls the population size (indirectly) via genetic diversity using a threshold value that defines the Hamming distance above which two chromosomes are allowed to crossover and generate offspring.","The methodology consists of three steps: bisection method to estimate the minimum population size required to supply enough building blocks; refinement of the population size to obtain the necessary supply of building blocks; and simple variation of the population size using a predetermined schedule. The SVPS-GA uses a deterministic function to vary its size, where the values of two parameters, speed (τ) and severity (ρ), influence the shape of the population sizing schedule. The procedure iterates on different values of τ and ρ to find the best combination for the problem instance.","Improvements in the number of evaluations of the SVPS-GA with respect to the fixed-size GA showed that the SVPS-GA scales better than the fixed-size GA. This proves that variable population sizing schemes can improve the performance of GAs. Experiments on trap functions demonstrated that SVPS requires a smaller number of evaluations than the fixed population sizing scheme. An interesting result is the set of strategies (i.e., combinations τ-ρ) in which the SVPS-GA outperforms the fixed-size GA. These strategies show that the GA tends to end with a high percentage of the initial population size (ρ) but loses individuals from a very early stage on the GA run (τ).","The paper presents a framework to test variable population sizing schemes. Additionally, a Simple Variable Population Sizing (SVPS) scheme is proposed to gain insights into the population requirements for the different stages of a GA run. The framework has been designed to be compliant with the recommendations of Lobo and Lima for the analysis of variable population sizing schemes. Results show that SVPS requires a smaller number of evaluations than the fixed population sizing scheme and, therefore, improves the GA performance. Future work includes studying how the addition of variation operators such as mutation affects performance and scaling and fine-tuning which values of ρ and τ are the most appropriate for a wide range of applications.",Improving genetic algorithms performance via deterministic population shrinkage,"Juan Luis Jiménez Laredo, Carlos Fernandes, Juan Julián Merelo, Christian Gagné","arXiv:2401.12121v1  [cs.NE]  22 Jan 2024
Improving Genetic Algorithms Performance via
Deterministic Population Shrinkage
Juan Luis Jiménez Laredo∗
Carlos Fernandes†
Juan Julián Merelo‡
Christian Gagné§
Abstract
Despite the intuition that the same population size is not needed
throughout the run of an Evolutionary Algorithm (EA), most EAs use a
ﬁxed population size. This paper presents an empirical study on the possi-
ble beneﬁts of a Simple Variable Population Sizing (SVPS) scheme on the
performance of Genetic Algorithms (GAs). It consists in decreasing the
population for a GA run following a predetermined schedule, conﬁgured
by a speed and a severity parameter. The method uses as initial popu-
lation size an estimation of the minimum size needed to supply enough
building blocks, using a ﬁxed-size selectorecombinative GA converging
within some conﬁdence interval toward good solutions for a particular
problem. Following this methodology, a scalability analysis is conducted
on deceptive, quasi-deceptive, and non-deceptive trap functions in order to
assess whether SVPS-GA improves performances compared to a ﬁxed-size
GA under diﬀerent problem instances and diﬃculty levels. Results show
several combinations of speed-severity where SVPS-GA preserves the so-
lution quality while improving performances, by reducing the number of
evaluations needed for success.
Note:
This paper is a pre-print version of the following paper that you can cite as:
Juan Luis J. Laredo, Carlos Fernandes, Juan Julián Merelo, and
Christian Gagné. 2009. Improving genetic algorithms performance
via deterministic population shrinkage. In Proceedings of the 11th
Annual conference on Genetic and evolutionary computation (GECCO
’09). Association for Computing Machinery, New York, NY, USA,
819–826. https://doi.org/10.1145/1569901.1570014
∗Corresponding author:
juanlu@ugr.es Department of Architecture and Computer
Technology, University of Granada, Spain
†LASEEB-ISR/IST. University of Lisbon, Portugal
‡Department of Architecture and Computer Technology, University of Granada, Spain
§Laboratoire de vision et systèmes numériques, Laval University, Canada
1
1
Introduction
Setting an adequate population size is a key to obtain good performances in
a Genetic Algorithm (GA), that is, to preserve a good quality in the solutions
without spending extra computational eﬀorts. That way, a small problem in-
stance will require a smaller population size than a larger instance of a more
diﬃcult problem [13, 14]. Additionally, improving the GA performance is also
possible by varying the population size during the GA run, adapting it as the
algorithm is converging toward some solutions [5, 16].
Population sizing theory, based on Goldberg’s facetwise decomposition for
designing competent GAs [12], focuses on determining population sizing accord-
ing to problem diﬃculty. Despite the issue that has been pointed out in Lobo
and Lima’s review of adaptive population sizing schemes [19], such a theory has
not received much attention with respect to variable population sizing schemes.
There are just a few studies that have taken it into account, with as prime ex-
ample population sizing through estimation of schema variances by Smith and
Smuda [21], with selection errors probability and adjustment made according to
an expected selection loss provided by the user.
Therefore, the aim of this paper is to provide a general framework for eval-
uating performance of variable population sizing schemes from the population
sizing theory perspective. For that purpose, a method based on bisection [20]
is used to estimate the minimum size of the initial population Pinit required to
supply enough building blocks (BBs) so that a ﬁxed-size selectorecombinative
GA1 will converge toward optimal solutions. That population size is then used
for bootstrapping another selectorecombinative GA, which is using for the tested
variable population sizing scheme. This allows assessment of the solutions qual-
ity and computational eﬀort required. Given that both ﬁxed and variable sizing
schemes are provided with the same initial supply of BBs, some conclusions can
be drawn from the diﬀerence of performances. Additionally, a scalability anal-
ysis is performed on diﬀerent instances and complexities of decomposable trap
functions [1]. Hence, it is possible not only to analyze a given performance, but
also, to analyze how it is aﬀected by problem size and complexity.
We propose to test with a simple variable population sizing scheme (SVPS).
The SVPS is a deterministic function that monotonically reduces the population
size during the GA run. The function slope can be controlled by setting two
parameters, one for controlling the shrinking speed and the other for controlling
the severity of population resizing. The intuition justifying the approach is that,
at an early stage of a GA run, larger population sizes produce a wider explo-
ration of the search landscape. Meanwhile, smaller population sizes increase the
exploitation of promising regions as the GA converges [19].
Nevertheless, our intent in the current work is to address the following points:
1. Provide a general framework to test diﬀerent variable population sizing
schemes from the population sizing theory perspective.
Following the
1The assumption of a selectorecombinative GA (without mutation) is made as the only
source of diversity is then the initial supply of BBs.
2
same steps, SVPS could be replaced by new or suitable schemes in the
literature.
2. Check whether variable population sizing schemes in GAs are able to out-
perform an equivalent parameterized GA using a ﬁxed size scheme. If the
SVPS-GA improves the ﬁxed-size GA performance, that would mean that
there are better schemes for population sizing than the standard ﬁxed size
scheme.
3. Gain some insight into the dynamics of the SVPS scheme. The idea of
SVPS is to maintain a good quality in the solutions using less evaluations.
Nevertheless, the seamless run of a GA makes it diﬃcult to estimate the
desired population size decreasing. Additionally, a given strategy could
be more or less adequate to diﬀerent problem instances or problem com-
plexities.
The rest of the paper is organized as follows: Section 2 presents the state
of the art in variable population sizing schemes. The proposed methodology
is described in Section 3 and the experimental setup in Section 4. Section 5
explores the dynamics of the SVPS, showing that a variable population sizing
scheme can yield a better performance than a ﬁxed population sizing scheme.
Finally, results are discussed in Section 6.
2
Related Works
This section provides a brief description of some other varying population meth-
ods previously proposed in the Evolutionary Computation research ﬁeld. Fol-
lowing the classiﬁcation of parameter control mechanisms given by Eiben et
al. [6], we may say that some of the techniques described below fall into the
adaptive methods categories (GAVaPS, for instance), while others, like RVPS
[4] and PRoFIGA [7] are deterministic methods. Our proposal may be classiﬁed
as deterministic, because the population size in each generation is deﬁned in the
beginning of the search by two parameters; the size is always forced to decrease,
with more or less speed, and ends with more or less individuals, depending on
those two parameters. Other methods follow a diﬀerent policy and adapt the
size during the run according to the state of the search. One of those algorithms,
proposed in 1994 by Arabas et al. [2], is the Genetic Algorithm with Varying
Population Size (GAVaPS).
GAVaPS does not hold an explicit selection mechanism. As in natural sys-
tems, population size is deﬁned by the birth and death of individuals occurring
at each iteration. A parameter called lifetime is introduced. It deﬁnes the num-
ber of generations in which each individual is allowed to remain alive. After
its creation, the chromosome is assigned to a speciﬁc lifetime, according to its
ﬁtness. Three lifetime calculation methods are proposed. The algorithm pro-
ceeds in a generational manner, at each time step increasing each individual’s
age. When an individual’s age exceeds its lifetime, the chromosome is removed
3
from the population. Since ﬁttest individuals remain in the population for more
generations, thus having a higher probability to be engaged in a reproduction
process and generate oﬀspring, GAVaPS’ chromosomes have equal probability
to be selected to reproduce, independently of their ﬁtness value. This concept of
lifetime/age provides the algorithm with the necessary selection pressure, which
reduces the need for selection strategies: GAVaPS randomly pairs the chromo-
somes for crossover operations. The intensity of the pressure is controlled by
two parameters, minLT and maxLT, that deﬁne, respectively, the minimum
and maximum lifetime allowed for each chromosome. Higher diﬀerence between
the two values leads to a more selective algorithm. However, this process may
have a serious drawback since increasing the maxLT parameter will result in
larger populations and, as stated above, an increasingly high population size is
a characteristic of GAVaPS. The algorithm also introduces another parameter:
reproduction rate (ρ); its value deﬁnes the number of new chromosomes created
in each generation t, depending on the size of the current population. GAVaPS
was tested for the studies presented in [11] and [7] and the results lead to the
conclusion that GAVaPS is extremely sensitive to the reproduction rate, and
very often the population grows exponentially or becomes extinct.
A similar approach was attempted with The Adaptive Population size Ge-
netic Algorithm (APGA) [3]. The only diﬀerence between this algorithm and
GAVaPS resides in reproduction rate, which in APGA has a ﬁxed value of two
individuals. This technique follows the reproduction strategy of the Steady-
State GA and prevents the population from growing out of control as it often
happens with GAVaPS. On the other hand, such a low reproduction rate results
in populations with few individuals unless a high value for maxLT is used. But,
even in the last case, the population size is very stable and apparently does not
react to the evolution process and diﬀerent search stages [3]. The algorithm
appears to perform well on some problems and clearly outperformed GAVaPS
when applied to the Spears’ multi-modal problems [7]. However, Lobo and Lima
[18] questioned the results in [7] and proved that there is an upper bound equal
to 2maxLT + 1 for APGA’s population size, after maxLT generations.
Eiben, Marchiori and Valkó proposed in [7] the Population Resizing on Fit-
ness Improvement GA (PRoFIGA). The variation process of PRoFIGA is based
on the improvement of the best ﬁtness in the population. The process intends
to balance exploration and exploitation by growing the population in earlier
and exploratory stages and gradually decreasing it in later stages of the search.
When the population becomes trapped in local optima, the process is supposed
to generate another growing phase of the population, thus increasing diversity
and escaping the local optima. The authors present a heuristic for size vari-
ation during the run that increases or decreases the population size according
to whether or not the best ﬁtness of the population has been improved and, if
the later case is observed, for how long it has remained unchanged. The main
drawback of this algorithm are its extra six parameters, which makes it very
hard to tune for a non expert user.
In the Random Variation of Population Size GA (RVPS) [4] the population
size is randomly changed during the run. The authors concluded that in some
4
cases the performance of RVPS is equivalent to the standard GA. So, when
there are no hints about the optimal population size for some problem, it may
be appropriate to randomly set and vary the population size of the GA.
Like PRoFIGA and RVPS, the Saw-Tooth Genetic Algorithm [15] is an ex-
ample of a deterministic method used in the variation of the population size. In
this algorithm the population size varies according to a predeﬁned function with
a saw-tooth shape. The authors concluded that the Saw-Tooth GA performed
well on some particular test functions. However, besides a variable population
size, the Saw-Tooth GA also uses a re-initialization mechanism to introduce
genetic diversity in the population.
The Self-Regulated Population Size EA (SRP-EA), proposed by Fernandes
and Rosa in [11], follows GAVaPS guidelines but it manages to control the popu-
lation size, thus avoiding the typical extinction and demographic burst observed
in the dynamics of Arabas’ algorithm. SRP-EA self-controls the population size
(indirectly) via genetic diversity. There is a threshold value that adapts during
the run and that deﬁnes the Hamming distance value above which two chro-
mosomes are allowed to crossover and generate oﬀspring and like GAVaPS the
individuals are provided with a lifetime that deﬁnes the of generations that
they are allowed to remain in the population. The algorithm was compared
with APGA and CHC [10] and outperformed both in the proposed test set.
Finally, there could be other reasons to use variable population sizing schemes.
Laredo et al. expose in [17] the case of a fully distributed EA in which the in-
dividuals have to decide on their own state of reproduction without any central
control, using instead estimations about the global population state for decision
making. The population size varies at run-time as a consequence of such a de-
centralized reproduction and a self-adjusting mechanism based on autonomous
selection [8] tries to keep it stable.
Our strategy, described in the next section, does not aim at adapting the
population size or varying it deterministically according to the state of the
search.
Alternatively, and although it may be classiﬁed as a deterministic
scheme, together with PROFIGA and the Saw-Tooth GA, SVPS tries to explore
the premise that states that larger populations are needed in the beginning of
the search, while towards the end the GA can manage to converge with a smaller
population [19].
3
Methodology
The methodology followed takes into account Goldberg’s facetwise decomposi-
tion for designing competent GAs [12] in order to answer whether a GA using
varying population size improves a ﬁxed-size GA.
The proposed method consists in the following three steps:
1. Bisection method, to estimate size n′ of population P ′
init;
2. Reﬁnement of size of the population size, to obtain the necessary supply
of BBs;
5
3. Simple variation of the population size using a predetermined schedule.
The two ﬁrst steps are used to estimate the minimum initial population size
(Pinit = {ind1, ind2, . . . , indn}) required to supply enough BBs for a reliable
convergence to the problem optimum in a ﬁxed-size population GA.
In the third step, Pinit is used as the initial population of the SVPS-GA
in which the population decreases according to a parameterized speed (τ) and
severity (ρ). Any combination τ-ρ has to preserve the quality of solutions while
improving the number of evaluations.
Steps 1, 2, and 3 are exposed in sections 3.1, 3.2 and 3.3, respectively. Note
that the SVPS-GA (step 3) could be changed for another variable, adaptive or
self-adaptive population sizing scheme. Therefore, this methodology provides a
framework to test diﬀerent population resizing schemes.
3.1
Bisection Method
The bisection method [20] estimates the optimal population size n′ to solve a
problem instance, that is, the lowest n′ for which 98% of the runs ﬁnd the prob-
lem optimum. A ﬁxed-size selectorecombinative GA is used to search the min-
imum population size required using random initialization, to provide enough
BBs to converge to the optimum using only recombination and selection mech-
anisms.
Algorithm 1 depicts the method based on bisection. The method begins
with a small population size which is doubled until the algorithm ensures a
reliable convergence. We deﬁne the reliability criterion as the convergence of
the algorithm to the optimum 49 out of 50 times (0.98 of Success Rate). After
that, the interval (min, max) is halved several times and the population size
adjusted within such a range until max−min
min
> threshold, where min and max
stand respectively for the minimum and maximum population size estimated
and threshold for the accuracy of the adjustment within such a range. This
parameter has been set to
1
16 in order to obtain a good adjustment of the initial
population size.
3.2
Reﬁning Initial Supply of Building Blocks
The initial supply of BBs given by a population P ′
init might be a bit oversized
because the minimum population size n′ is estimated stochastically.
Such a
precision is usually accurate enough for scalability analysis, but in the case of
study, additional BBs will induce smaller values for τ and ρ, leading to a slightly
unfair comparison.
Therefore, Algorithm 2 shows an iterative process to reﬁne P ′
init into Pinit
by randomly subtracting 1% of the individuals until Pinit is minimized.
6
Algorithm 1 Method based on bisection
n′ = initial population size
while reliability of P ′
init with size n′ < 98% do
min = n′; max = 2n′; n′ = 2n′
P ′
init size ← n′
end while
while
max−min
min
>
1
16 do
n′ = max+min
2
P ′
init size ← n′
if reliability of P ′
init with size n′ < 98% then
min = n′
else
max = n′
end if
end while
Algorithm 2 Reﬁnement of P ′
init
Pinit ← P ′
init
while reliability of Pinit with size n ≥ 98% do
Pinit ← randomly removes 1% of individuals in Pinit
end while
3.3
Varying Population Size
The SVPS-GA uses the following deterministic function to vary its size:
ng =
(
n0 ×

1 − (1 − ρ)

g
gmax
τ
,
g ≤ gmax
ngmax,
g > gmax
(1)
In this equation, ng stands for the population size at generation g, n0 the initial
population size, and ngmax the population size when a maximum number of
generations (gmax) of the schedule is reached. To scale the shape of the function
into the SVPS-GA runtime, gmax is estimated on the necessary runtime of the
ﬁxed-size GA. τ and ρ are respectively the speed and severity parameters. As
shown in Figure 1, the values of τ and ρ inﬂuence the shape of the population
sizing schedule. A smaller τ leads to a faster reduction of the population size.
ρ belongs to the interval ]0, 1], where a value of ρ → 0 means that the run ends
with a nearly empty population, and where a value of ρ = 1 does not modify
the initial population size.
Algorithm 3 shows the procedure for iterating on diﬀerent values of τ and
ρ. Only those results which guarantee a success rate of 0.98 are stored (e.g. a
very small τ and ρ could lead to an unreliable convergence of the SVPS-GA).
Lower bounds for τ and ρ have been ﬁxed to pessimistic values that will not
meet the reliability condition (e.g. τ = 0.125 means that the population size
will converge too quickly to ρ), while upper bounds stand for equivalent sizes
7
0  
20%
40%
60%
80%
100%
Algorithm Runtime
Population Size
τ = 1 ρ = 0.8
τ = 2 ρ = 0
τ = 1 ρ = 0
τ = 0.5 ρ = 0
Figure 1: SVPS shapes for diﬀerent values of τ-ρ.
Algorithm 3 Varying the population with τ-ρ
for τ = 0.125, ...∗1.5, 32 do
for ρ = 0.25, ...+0.5, 1 do
if SVPS-GA reliability (Pinit,τ,ρ) ≥ 98% then
store the combination τ-ρ
end if
end for
end for
to the ﬁxed-size population (e.g. ρ = 1 means that the population size does not
shrink, or τ = 32 that the population shrinks in the last few generations).
4
Experimental Setup
Following Lobo and Lima’s recommendations [19] on the selection of a test suite
with known population requirements and investigating the scalability on land-
scapes of diﬀerent characteristics, experiments were conducted on trap functions
[1]. A trap function is a piecewise-linear function deﬁned on unitation (the num-
ber of one values in a binary string). There are two distinct regions in the search
space, one leading to a global optimum and the other leading to the local op-
timum (see Figure 2). In general, a trap function is deﬁned by the following
equation:
trap(u(−→x )) =

a
z (z − u(−→x )),
if u(−→x ) ∈ [0, z]
b
l−z(u(−→x ) − z),
if u(−→x ) ∈]z, l]
(2)
where u(−→x ) is the unitation function returning the number of one values in bit
string −→x , a is the local optimum, b is the global optimum, l is the problem
size and z is a slope-change location separating the attraction basin of the two
optima.
8
Figure 2: Generalized l-trap function.
Trap instances
BB size (l)
2, 3, 4
Number of sub-functions (m)
2, 4, 8, 16, 32, 64
GA Setup
GA
selectorecombinative GA
selectorecombinative SVPS-GA
Population size
Bisection + Reﬁnement
Selection of Parents
Binary Tournament
Recombination
One-point crossover, pc = 1.0
Individual Length
l × m
SVPS Setup
Speed (τ)
0.125, ...∗1.5, 32
Severity (ρ)
0.25, ...+0.05, 1
Table 1: Parameters of the experiments
For the following experiments, 2-trap, 3-trap and 4-trap functions were de-
signed with the following parameter values: a = l − 1, b = l, and z = l − 1.
With these settings, 2-trap is not deceptive, 4-trap is deceptive and 3-trap lies
in the region between deception and non-deception. Under these conditions,
it is possible not only to examine the scalability on trap functions, but also
to investigate how the scalability varies when changing from non-deceptive to
deceptive search landscapes. Scalability tests were performed by juxtaposing m
trap functions and summing the ﬁtness of each sub-function to obtain the total
ﬁtness.
All settings are summarized in Table 1, operators as binary tournament or
one-point crossover are standard in GAs [9]. The baseline for comparison is a
generational selectorecombinative GA. Since the methodology imposes a 98%
success rate in the results, the Average Evaluations to Solution (AES) has been
used as an appropriate metric to measure the computational eﬀort to reach
the success criterion. A more eﬃcient algorithm requires a smaller number of
evaluations.
9
Figure 3: Improvement in the number of evaluations of the SVPS-GA with
respect to the ﬁxed-size GA. Results are depicted as a function of the initial
population size used by the diﬀerent problem instances of 2-trap, 3-trap, and
4-trap functions.
5
Results
From the graphics in Figure 4 it can be concluded that SVPS-GA scales better
than the ﬁxed-size GA. This fact proves that variable population sizing schemes
can improve the performance of GAs. Student’s t-test conducted on the results
in Table 2 shows that, except for the smaller instances (i.e. some combinations
for τ − ρ in m = 2, 4 and 8), improvements are statistically signiﬁcant.
Such improvements are more remarkable when the GA requires large popu-
lation sizes which is directly related with the problem size and diﬃculty. As a
general pattern, the larger the number of sub-functions (m) or the more diﬃcult
the problem, the larger the initial population is. Hence, SVPS performs better
under large instances of diﬃcult problems.
The relationship between the initial population size and the improvement in
the number of evaluations is depicted in Figure 3. It shows the saved evaluations
by the SVPS-GA with respect to the ﬁxed-size GA. The improvement keeps a
proportionality of order Θ(n1.54) to the initial population size. However, it is
our belief that other variable population sizing schemes could outperform this
mark since we have just contemplated the shrinkage of the population.
In order to gain some insight into the way the population shrinks, Figure
5 shows the set of strategies (i.e. combinations τ − ρ) in which the SVPS-GA
outperforms the ﬁxed-size GA. They present the following behavior:
• For a given problem instance, values of ρ decrease as values of τ increase.
• Small values of τ usually report a better GA performance.
• As the problem scales, the set of strategies is shifted to higher values of τ.
Therefore, as a good strategy, the GA tends to end with a high percentage
of the initial population size (ρ) but losing individuals from a very early stage
10
Figure 4: Scalability with trap functions. Optimal population size (top) and
Average Evaluations to Solution (AES) (bottom) values for a ﬁxed-size GA and
the SVPS-GA.
Figure 5: SVPS-GA combinations of τ-ρ converging to a SR of 0.98. From left
to right, 2-trap, 3-trap, and 4-trap are represented for sub-functions values of
m = 4, m = 16, and m = 64.
The area of the circles stands for the AES
improvement with respect to the ﬁxed-size GA.
11
2-Trap
3-Trap
4-Trap
Fixed-size GA
SVPS-GA
Fixed-size GA
SVPS-GA
Fixed-size GA
SVPS-GA
m
n
AES
AES
τ
ρ
n
AES
AES
τ
ρ
n
AES
AES
τ
ρ
2
19.0
30.02
22.53
0.42
0.25
67
101.69
100.96
7.2
0.25
110
268.57
259.8
2.13
0.25
±33.57
±8.17
±49.76
±45
±182
±152
23.61
0.18
0.5
264.02
2.84
0.3
±11.47
±168
24.76
0.28
0.3
267.3
0.125
0.75
±13.92
±161
4
49
170
144.02
0.63
0.7
112
597.9
467.04
0.28
0.7
230
1768.46
1549.14
2.84
0.7
±104
±96.8
±221
±136
±620
±428
145.56
0.28
0.75
562.57
3.2
0.25
1636.63
0.42
0.95
±64.5
±188
±563
155.54
0.9
0.65
581.06
2.13
0.55
1677.57
3.20
0.65
±83.4
±211
±549
8
80
668.49
582.98
7.2
0.55
220
2641.98
2192.42
0.9
0.9
543
8714.10
7713.2
4.8
0.6
±179.8
±169
±648.5
±381
±1952
±1362
625.10
10.81
0.4
2235.63
0.18
0.95
7822.95
7.2
0.35
±194
±363
±1205
637.63
24.32
0.3
2452.55
7.2
0.4
7892.44
0.9
0.95
±199
±397
±1687
2471.52
4.8
0.65
8223.72
10.81
0.25
±532
±1153
16
230
3355.57
2804.08
0.125
0.95
639
13451.8
10763.38
0.42
0.95
1097
30721.04
28384.51
10.81
0.55
±584.9
±436
±1861
±1120
±3348
± 2259
2896.72
2.13
0.8
11585.95
4.8
0.6
28602.94
2.13
0.95
±373
± 988
±2646
2984.48
7.2
0.25
11636.61
7.2
0.25
29212.24
16.21
0.35
±336
±891
±2348
3015.1
4.8
0.5
11684.92
3.20
0.8
29456.2
7.2
0.8
±381
±923
±2674
12023.58
2.13
0.9
29460.75
24.32
0.25
±1188
±2606
32
568
14943.96
11661.64
0.63
0.95
1353
48521.98
40911.83
4.8
0.75
3225
146072.28
123758.85
4.8
0.8
±1475
±875
±5257
±2980
±10777
±6228
12401.14
3.20
0.8
44037.66
10.81
0.6
127900.89
7.2
0.7
±834
±2985
±6957
13223.79
10.81
0.35
46072.4
16.21
0.5
134020.22
16.21
0.3
±1032
±3342
±6396
13327.55
7.2
0.65
46519.18
24.32
0.25
134052.36
3.2
0.95
±1285
±2925
±9115
13802.71
16.21
0.25
138275.59
24.32
0.25
±1102
±7375
64
1280
55415.1
52773.36
16.21
0.7
4480
252368.92
207263.65
3.2
0.9
9408
666693.86
596770.36
10.81
0.65
±3501
±2408
±13604
±8435
±37471
±22940
52843.26
24.32
0.25
227992.93
10.81
0.65
619930.93
16.21
0.55
±2494
±9315
±25075
52963.61
10.81
0.85
229857.77
16.21
0.3
632222.06
24.32
0.45
±2843
±8830
±25380
54897.51
7.2
0.95
±3868
Table 2: Experimental results, values in bold are statistically signiﬁcant.
12
on the GA run (τ). Nevertheless, large instances require that the population
remains almost intact for a longer period, allowing a shrinkage mainly at the
last stage of the GA run.
Despite the fact that SVPS has not been designed to ﬁnd an optimal vari-
ability in the population size, it improves the GA performance. Such a result is
signiﬁcant since it proves that variable population sizing schemes are an open
issue in the design of GAs.
6
Conclusions
In this paper we have presented a framework to test variable population sizing
schemes.
Additionally, a Simple Variable Population Sizing (SVPS) scheme
is proposed in order to gain some insights into the population requirements
for the diﬀerent stages of a GA run. The framework consists in a three step
methodology. The ﬁrst and the second steps provide the initial population to be
used for the variable population sizing scheme. This initial population represents
the minimum initial supply of raw BBs that a ﬁxed-size selectorecombinative
GA needs to converge to the optimum solution. The third step is a deterministic
resizing schedule conﬁgured by a speed and a severity parameter.
The framework has been designed to be compliant with the recommendations
of Lobo and Lima [19] for the analysis of variable population sizing schemes:
• The test suite (i.e. trap functions) has known population sizing require-
ments.
• A scalability analysis has been conducted by varying the size and the
complexity of the problem instances.
• The initial population size is well adjusted so that the GA will converge
to the problem optima with a success rate of 0.98. The initial supply of
BBs and the solution quality are ﬁxed for both population sizing schemes,
allowing a fair comparison based on the diﬀerence of the number of eval-
uations.
Preserving the condition of optimality (i.e. success rate of 0.98), the SVPS
provides not a single but a set of strategies.
Independently of the problem
instance, the set follows a common pattern: a large value of τ implies a small
one of ρ, that is, the population shrinkage strategy has to keep a balance between
severity and speed of the population shrinkage. Results show that SVPS requires
a smaller number of evaluations than the ﬁxed population sizing scheme, and
therefore, improves the GA performance.
Future work will include studying how the addition of variation operators
such as mutation aﬀect performance and its scaling, and also ﬁne-tuning which
values of ρ and τ are the most appropriate for a wide range of applications.
13
Acknowledgements
This work has been supported by the Spanish MICYT project TIN2007-68083-
C02-01, the Junta de Andalucia CICE project P06-TIC-02025, and the Granada
University PIUGR 9/11/06 project. The authors are grateful to Annette Schw-
erdtfeger for proofreading this manuscript.
14
References
[1] D. H. Ackley. A connectionist machine for genetic hillclimbing. Kluwer
Academic Publishers, Norwell, MA, USA, 1987.
[2] J. Arabas, Z. Michalewicz, and J. Mulawka. GAVaPS – A genetic algorithm
with varying population size.
International Conference on Evolutionary
Computation (IEEE-CEC 1994), pages 73–78 vol.1, Jun 1994.
[3] T. Bäck, A. E. Eiben, and N. A. L. van der Vaart. An empirical study on
gas without parameters. In Parallel Problem Solving from Nature (PPSN
VI), pages 315–324, London, UK, 2000.
[4] J. Costa, R. Tavares, and A. Rosa. An experimental study on dynamic
random variation of population size. IEEE conference on Systems, Man,
and Cybernetics (SMC 1999), 1:607–612 vol.1, 1999.
[5] F. F. de Vega, E. Cantú-Paz, J. López, and T. Manzano. Saving resources
with plagues in genetic algorithms. In Parallel Problem Solving from Nature
(PPSN VIII), pages 272–281, 2004.
[6] A. E. Eiben, R. Hinterding, and Z. Michalewicz.
Parameter control in
evolutionary algorithms. IEEE Transactions on Evolutionary Computation,
3:124–141, 1999.
[7] A. E. Eiben, E. Marchiori, and V. A. Valkó. Evolutionary algorithms with
on-the-ﬂy population size adjustment. In Parallel Problem Solving from
Nature (PPSN VIII), pages 41–50, 2004.
[8] A. E. Eiben, M. Schoenauer, D. W. F. van Krevelen, M. C. Hobbelman,
M. A. ten Hagen, and R. C. van het Schip. Autonomous selection in evolu-
tionary algorithms. In Genetic and Evolutionary Computation Conference
(GECCO 2007), pages 1506–1506, 2007.
[9] A. E. Eiben and J. E. Smith. Introduction to Evolutionary Computing.
SpringerVerlag, 2003.
[10] L. Eshelman. The CHC adaptive search algorithm: how to have safe search
when engaging in non-traditional genetic recombination. In Foundations of
Genetic Algorithms (FOGA 1991), pages 265–283, 1991.
[11] C. Fernandes and A. Rosa. Self-regulated population size in evolutionary
algorithms. In Parallel Problem Solving from Nature (PPSN IX), pages
920–929, 2006.
[12] D. Goldberg. The Design of Innovation - Lessons from and for Competent
Genetic Algorithms. Kluwer Academic Publishers, Norwell, MA, 2002.
[13] D. E. Goldberg, K. Deb, and J. H. Clark. Genetic algorithms, noise, and
the sizing of populations. Complex Systems, 6:333–362, 1992.
15
[14] G. Harik, D. E. Goldberg, E. Cantú-paz, and B. L. Miller. The gambler’s
ruin problem, genetic algorithms, and the sizing of populations. In IEEE
Conference on Evolutionary Computation (IEEE-CEC 1997), pages 7–12,
1997.
[15] V. Koumousis and C. Katsaras. A saw-tooth genetic algorithm combining
the eﬀects of variable population size and reinitialization to enhance per-
formance. IEEE Transactions on Evolutionary Computation, 10(1):19–28,
Feb. 2006.
[16] J. Laredo, P. Castillo, A. Mora, J. Merelo, and C. Fernandes. Resilience to
churn of a peer-to-peer evolutionary algorithm. Int. J. High Performance
Systems Architecture, 1(4):260–268, 2008.
[17] J. Laredo, A. Eiben, M. van Steen, and J. Merelo. On the run-time dynam-
ics of a peer-to-peer evolutionary algorithm. In Parallel Problem Solving
from Nature (PPSN X), pages 236–245, 2008.
16
[18] F. G. Lobo and C. F. Lima. Revisiting evolutionary algorithms with on-the-
ﬂy population size adjustment. In Genetic and Evolutionary Computation
Conference (GECCO 2006), pages 1241–1248, 2006.
[19] F. G. Lobo and C. F. Lima. Adaptive population sizing schemes in genetic
algorithms. In Parameter Setting in Evolutionary Algorithms, Studies in
Computational Intelligence, pages 185–204. Springer Berlin / Heidelberg,
2007.
[20] K. Sastry.
Evaluation-relaxation schemes for genetic and evolutionary
algorithms. Technical Report 2002004, University of Illinois at Urbana-
Champaign, Urbana, IL., 2001.
[21] R. E. Smith and E. Smuda. Adaptively resizing populations: Algorithm,
analysis, and ﬁrst results. Complex Systems, 9:47–72, 1995.
17
"
"We introduce a novel perspective on deep ReLU networks as circuit counterparts of Łukasiewicz infinite-valued logic, a many-valued generalization of Boolean logic. An algorithm is presented to extract formulae in many-valued logic from ReLU networks with general, in particular also real-valued, weights. This extends the existing constructive procedures from Boolean to many-valued logic and makes it possible to extract logical formulae directly from trained deep ReLU networks.","Deep neural networks exhibit impressive reasoning capabilities, e.g., in mathematical tasks, program synthesis, and algorithmic reasoning. We initiate the development of a generalization of the classical Boolean circuit-logic correspondence to general functions f : [0, 1]n → [0, 1] by identifying their counterparts as ReLU networks and many-valued (MV) logic, respectively. We devise an algorithm for extracting MV formulae from ReLU networks with integer, rational, or real weights. The overall philosophy is inspired by works that connect neural networks with clipped ReLU nonlinearities to rational Łukasiewicz logic and real-valued Łukasiewicz logic.","nanState-of-the-art deep neural networks exhibit impressive reasoning capabilities, e.g. in mathematical tasks [1], program synthesis [2], and algorithmic reasoning [3]. This paper reports an attempt at systematically connecting neural networks with mathematical logic. Specifically, we shall be interested in reading out logical formulae from (trained) deep neural networks.

Let us first take a step back. Consider a neural network that realizes a map f : [0, 1]n → [0, 1]. When the input and output variables take on two possible values only, say 0 and 1, f reduces to a Boolean function and can hence be studied by means of Boolean algebra, see e.g. [4]. Boolean functions can be realized by Boolean circuits [5]. The idea of using Boolean algebra to analyze and design Boolean circuits dates back to [6, 7] and most prominently by Shannon in [8]. Specifically, this correspondence works as follows. Given a Boolean circuit, one can deduce a Boolean algebraic expression that realizes the circuit’s input-output relation. Conversely, for a given Boolean algebraic expression, it is possible to specify a Boolean circuit whose input-output relation equals this expression.","We start with a brief review of the correspondence between Boolean logic and Boolean circuits as described in [8, 23].

1.1
Boolean Logic and Boolean Circuits
Boolean algebra on the binary set {0, 1} consists of the application of the logical operations OR, AND, NOT, denoted by ⊕, ⋅, and ¬, respectively, to propositional variables. The algebra is fully characterized through the following identities:
x ⊕ 0 = x
x ⋅ 1 = x
x ⊕ ¬x = 1
x ⋅ ¬x = 0
x ⊕ y = y ⊕ x
x ⋅ y = y ⋅ x
(x ⊕ y) ⊕ z = x ⊕ (y ⊕ z)
(x ⋅ y) ⋅ z = x ⋅ (y ⋅ z)
x ⊕ (y ⋅ z) = (x ⊕ y) ⋅ (x ⊕ z)
x ⋅ (y ⊕ z) = (x ⋅ y) ⊕ (x ⋅ z)
(1)
Note that one can define the operation ⋅ in terms of ⊕ and ¬ according to x ⋅ y := ¬(¬x ⊕ ¬y) and then rewrite all the identities in (1) involving ⋅ based on ⊕ and ¬ only.

Shannon developed a systematic approach to the analysis and synthesis of switching circuits using Boolean algebra [8]. At the heart of Shannon’s theory is the following interpretation of Boolean logic in terms of switching circuits. A propositional (Boolean) variable x is interpreted as a make contact on a switch. The negation of x represents a break contact. The constants 0 and 1 signify open and closed circuits, respectively. The Boolean operations ⊕ and ⋅ correspond to parallel and, respectively, series connections of switches. With these correspondences, every switching circuit can be associated with a Boolean formula, namely by identifying the switches in the circuit with Boolean variables, and then connecting them by ⊕ and ⋅ operations according to the connections appearing in the circuit.
Conversely, following these correspondences, Boolean circuits are directly associated with Boolean formulae. Exploiting these equivalences, the manipulation of Boolean circuits can be carried out on their corresponding Boolean expressions and vice versa.

Boolean circuits are represented by directed acyclic graphs whose nodes are logic gates ⊕, ⋅, and ¬.","In practice, trained neural networks will, however, not exhibit integer weights, unless this is explicitly enforced in the training process. Extensions of MV logic, namely Rational Łukasiewicz logic [11] and RL [12], have truth functions that are again continuous piecewise linear, but with rational and real-valued coefficients, respectively. Such functions are likewise naturally realized by ReLU networks, but correspondingly with rational and real-valued weights.

Besides the conceptual contribution residing in the systematic development of the connection between ReLU networks and MV logic along with its extensions, we also devise an algorithm for extracting logical formulae from (trained) ReLU networks with integer, rational, or real-valued weights.
For pedagogical reasons and to render the presentation more accessible, we first present the entire framework for MV logic and ReLU networks with integer weights, and then provide extensions to the rational and real case. In addition, we carry out a detailed comparison between our algorithm and the only two constructive procedures for converting McNaughton functions to their associated MV logical formulae available in the literature [13, 14].","We initiated the development of a generalization of the classical Boolean circuit-logic correspondence to general functions f : [0, 1]n → [0, 1]. The key insight is that ReLU networks naturally operate on an infinite-valued Łukasiewicz logic. Capitalizing on this observation, we devised an algorithm for extracting Łukasiewicz-logic formulae from ReLU networks with integer, rational, or real-valued weights. This algorithm extends exiting constructive procedures from the Boolean to the many-valued logic case. It allows for extracting logical formulae directly from trained deep ReLU networks. Besides the conceptual contribution residing in the systematic development of the connection between ReLU networks and Łukasiewicz logic along with its extensions, the presented algorithm also paves the way for theoretically analyzing neural network reasoning in a logic-based manner.",Extracting Formulae in Many-Valued Logic from Deep Neural Networks,"Yani Zhang, Helmut Bölcskei","Extracting Formulae in Many-Valued Logic from Deep
Neural Networks
Yani Zhang and Helmut Bölcskei
Chair for Mathematical Information Science, ETH Zürich
yanizhang@mins.ee.ethz.ch, hboelcskei@ethz.ch
Abstract
We propose a new perspective on deep ReLU networks, namely as circuit counterparts of Łukasiewicz
infinite-valued logic—a many-valued (MV) generalization of Boolean logic. An algorithm for extract-
ing formulae in MV logic from deep ReLU networks is presented1. As the algorithm applies to net-
works with general, in particular also real-valued, weights, it can be used to extract logical formulae
from deep ReLU networks trained on data.
1
Introduction
State-of-the-art deep neural networks exhibit impressive reasoning capabilities, e.g. in mathematical
tasks [1], program synthesis [2], and algorithmic reasoning [3]. This paper reports an attempt at
systematically connecting neural networks with mathematical logic. Specifically, we shall be interested
in reading out logical formulae from (trained) deep neural networks.
Let us first take a step back. Consider a neural network that realizes a map f : [0, 1]n → [0, 1]. When
the input and output variables take on two possible values only, say 0 and 1, f reduces to a Boolean
function and can hence be studied by means of Boolean algebra, see e.g. [4]. Boolean functions can
be realized by Boolean circuits [5]. The idea of using Boolean algebra to analyze and design Boolean
circuits dates back to [6, 7] and most prominently by Shannon in [8]. Specifically, this correspondence
works as follows. Given a Boolean circuit, one can deduce a Boolean algebraic expression that realizes
the circuit’s input-output relation. Conversely, for a given Boolean algebraic expression, it is possible
to specify a Boolean circuit whose input-output relation equals this expression.
The main aim of the present paper is to initiate the development of a generalization of this corre-
spondence from Boolean functions f : {0, 1}n → {0, 1} to general functions f : [0, 1]n → [0, 1]. This
immediately leads to the following two questions:
1. What is the logical system replacing Boolean logic?
2. What is the counterpart of Boolean circuits?
As to the first question, we shall show that the theory of infinite-valued Łukasiewicz logic [9] pro-
vides a suitable framework for characterizing general (nonbinary) functions f : [0, 1]n → [0, 1] from
a logical perspective. With slight abuse of terminology, we shall refer to infinite-valued Łukasiewicz
logic as many-valued (MV) logic throughout the paper. Based on a fundamental result [10], which
characterizes the class of truth functions in MV logic—also called McNaughton functions—as contin-
uous piecewise linear functions with integer coefficients, we show that neural networks employing
the ReLU nonlinearity ρ(x) = max{0, x} and integer weights2 naturally implement statements in MV
logic. This answers the second question above by identifying ReLU networks as the counterpart of
Boolean circuits.
In practice, trained neural networks will, however, not exhibit integer weights, unless this is explic-
itly enforced in the training process. Extensions of MV logic, namely Rational Łukasiewicz logic [11]
and RL [12], have truth functions that are again continuous piecewise linear, but with rational and
1A Python implementation is available at https://www.mins.ee.ethz.ch/research/downloads/NN2MV.html.
2By weights, we mean the entries of the weight matrices and bias vectors associated with the network.
1
arXiv:2401.12113v1  [cs.LG]  22 Jan 2024
real-valued coefficients, respectively. Such functions are likewise naturally realized by ReLU networks,
but correspondingly with rational and real-valued weights.
Besides the conceptual contribution residing in the systematic development of the connection be-
tween ReLU networks and MV logic along with its extensions, we also devise an algorithm for ex-
tracting logical formulae from (trained) ReLU networks with integer, rational, or real-valued weights.
For pedagogical reasons and to render the presentation more accessible, we first present the entire
framework for MV logic and ReLU networks with integer weights, and then provide extensions to the
rational and real case. In addition, we carry out a detailed comparison between our algorithm and
the only two constructive procedures for converting McNaughton functions to their associated MV
logical formulae available in the literature [13, 14].
The overall philosophy of viewing ReLU networks as the circuit counterpart of MV logic and its
extensions is inspired by [15, 16, 17]. Specifically, Amato et al. [15, 16] pointed out that neural net-
works, with the clipped ReLU (CReLU) nonlinearity σ(x) = min{1, max{0, x}} and rational weights,
realize truth functions in Rational Łukasiewicz logic. Di Nola et al. [17] proved that CReLU networks
with real weights realize truth functions in RL logic. The universal correspondence between ReLU
networks and MV logic as well as its extensions reported here along with the algorithm for extracting
logical formulae from ReLU networks appear to be new.
Finally, we refer to [18, 19, 20] for different frameworks that integrate neural learning with symbolic
logic. Deep neural networks have been successfully applied to symbolic analysis through training on
numerical data [21] or by leveraging large scale pre-training [22].
We start with a brief review of the correspondence between Boolean logic and Boolean circuits as
described in [8, 23].
1.1
Boolean Logic and Boolean Circuits
Boolean algebra on the binary set {0, 1} consists of the application of the logical operations OR, AND,
NOT, denoted by ⊕, ⊙, and ¬, respectively, to propositional variables. The algebra is fully characterized
through the following identities:
x ⊕ 0 = x
x ⊙ 1 = x
x ⊕ ¬x = 1
x ⊙ ¬x = 0
x ⊕ y = y ⊕ x
x ⊙ y = y ⊙ x
(x ⊕ y) ⊕ z = x ⊕ (y ⊕ z)
(x ⊙ y) ⊙ z = x ⊙ (y ⊙ z)
x ⊕ (y ⊙ z) = (x ⊕ y) ⊙ (x ⊕ z)
x ⊙ (y ⊕ z) = (x ⊙ y) ⊕ (x ⊙ z)
(1)
Note that one can define the operation ⊙ in terms of ⊕ and ¬ according to x ⊙ y := ¬(¬x ⊕ ¬y) and
then rewrite all the identities in (1) involving ⊙ based on ⊕ and ¬ only.
Shannon developed a systematic approach to the analysis and synthesis of switching circuits using
Boolean algebra [8]. At the heart of Shannon’s theory is the following interpretation of Boolean logic
in terms of switching circuits. A propositional (Boolean) variable x is interpreted as a make contact
on a switch. The negation of x represents a break contact. The constants 0 and 1 signify open and
closed circuits, respectively. The Boolean operations ⊕ and ⊙ correspond to parallel and, respectively,
series connections of switches. With these correspondences, every switching circuit can be associated
with a Boolean formula, namely by identifying the switches in the circuit with Boolean variables, and
then connecting them by ⊕ and ⊙ operations according to the connections appearing in the circuit.
Conversely, following these correspondences, Boolean circuits are directly associated with Boolean
formulae. Exploiting these equivalences, the manipulation of Boolean circuits can be carried out on
their corresponding Boolean expressions and vice versa.
Boolean circuits are represented by directed acyclic graphs whose nodes are logic gates ⊕, ⊙, and ¬.
For example, the circuit in Figure 1(a) computes the Boolean function
(x1 ⊙ x2) ⊕ ((x2 ⊕ x3) ⊙ (x2 ⊙ x3)).
(2)
The expression in (2) can be manipulated using the identities listed in (1) to arrive at the functionally
2
⊕
⊙
⊙
⊕
⊙
x3
x2
x1
(a)
⊙
⊕
x2
x3
x1
(b)
Figure 1: Two equivalent Boolean circuits.
equivalent, but algebraically simpler, expression x2 ⊙ (x1 ⊕ x3), which corresponds to the smaller
(only 2 logic gates instead of 5) Boolean circuit depicted in Figure 1(b).
Shannon’s seminal paper [23] showed that the circuit complexity, in terms of the number of switches,
of n-ary Boolean functions is upper-bounded by O(2n/n) and that almost all Boolean functions have
circuit complexity close to this bound.
The motivation for studying Boolean circuits is that the time required to compute a given Boolean
function on a Turing machine is closely related to the function’s circuit complexity [24]. For exam-
ple, Pippenger and Fischer [25] proved that a language of time complexity T(n) has circuit complex-
ity O(T(n) log T(n)). Conversely, polynomial circuit complexity implies nonuniform polynomial time
complexity.
1.2
MV Logic
We now generalize from Boolean logic to many-valued logic [26], where propositional variables take
truth values in the interval [0, 1]. The corresponding algebraic counterpart is known as Chang’s MV
algebra [27]. MV logic consists of two logical operations, ⊕ and ¬, in analogy to the Boolean OR
and NOT; the operation ⊙, in analogy to the Boolean AND, is defined in terms of ⊕ and ¬ according
to x ⊙ y := ¬(¬x ⊕ ¬y). This leads us to the definition of the so-called standard MV algebra.
Definition 1.1. Consider the unit interval [0, 1], and define x⊕y = min{1, x+y} and ¬x = 1−x, for x, y ∈
[0, 1]. It can be verified that the structure I = ⟨[0, 1], ⊕, ¬, 0⟩ is an MV algebra [9]. In particular, I constitutes
the algebraic counterpart of Łukasiewicz infinite-valued logic [27]. We further define the operation x ⊙ y :=
¬(¬x ⊕ ¬y) = max{0, x + y − 1}.
It can be shown that the Boolean algebra B := {{0, 1}, OR, NOT, 0} is a special case of MV algebras.
The MV algebra I in 1.1 is referred to as standard because an equation holds in every MV algebra iff
it holds in I [27, 28]. Additional relevant material on MV algebras is provided in Appendix A.
2
ReLU Networks as Circuit Counterpart of MV Logic
MV terms are finite strings composed of propositional variables x1, x2, . . . connected by ⊕, ⊙, and ¬
operations and brackets (), such as (x1 ⊕ ¬x2) ⊙ x3. Term functions are the corresponding truth
functions obtained by interpreting the logical operations according to how they are specified in the
concrete MV algebra used, e.g. x⊕y = min{1, x+y} in the standard MV algebra I. See Definitions A.2
and A.3. In the Boolean algebra B, term functions are binary tables {{0, 1}n → {0, 1} : n ∈ N}. In
the standard MV algebra I, term functions are characterized by continuous piecewise linear functions
with integer coefficients as formalized by the McNaughton theorem [10].
Theorem 2.1. [10] Consider the MV algebra I in Definition 1.1. Let n ∈ N. For a function f : [0, 1]n → [0, 1]
to have a corresponding MV term τ such that the associated term function τ I satisfies τ I = f on [0, 1]n, it is
necessary and sufficient that f satisfy the following conditions:
1. f is continuous with respect to the natural topology on [0, 1]n,
2. there exist linear polynomials p1, . . ., pℓ with integer coefficients, i.e.,
pj(x1, . . ., xn) = mj1x1 + · · · + mjnxn + bj,
3
for j = 1, . . . , ℓ, with mj1, . . ., mjn, bj ∈ Z, such that for every x ∈ [0, 1]n, there is a j ∈ {1, . . ., ℓ}
with f(x) = pj(x).
Functions satisfying these conditions are called McNaughton functions.
ReLU networks (see Definition B.1) are compositions of affine transformations and the ReLU non-
linearity ρ(x) = max{0, x} (applied element-wise) and as such realize continuous piecewise linear
functions. Specifically, the class of ReLU networks with integer weights is equivalent to the class of
formulae in MV logic. The corresponding formal statement is as follows.
Theorem 2.2. For n ∈ N, let τ(x1, . . . , xn) be an MV term in n variables with τ I : [0, 1]n → [0, 1] the
associated term function in I. There exists a ReLU network Φ with integer weights, satisfying
Φ(x1, . . . , xn) = τ I(x1, . . . , xn),
for all (x1, . . . , xn) ∈ [0, 1]n. Conversely, for every ReLU network Φ : [0, 1]n → [0, 1] with integer weights,
there exists an MV term τ(x1, . . . , xn) whose associated term function in I satisfies
τ I(x1, . . . , xn) = Φ(x1, . . . , xn),
for all (x1, . . . , xn) ∈ [0, 1]n.
The remainder of this section is devoted to the proof of Theorem 2.2, along with the development
of an algorithm for extracting formulae in MV logic from ReLU networks with integer weights. First,
we show how, for a given MV term τ, a ReLU network with integer weights realizing the associated
term function τ I can be constructed. We start by noting that the operation ¬x = 1 − x, by virtue of
being affine, is trivially realized by a ReLU network. Further, there exist ReLU networks Φ⊕ and Φ⊙,
with integer weights, realizing the ⊕ and ⊙ operations in I, i.e.,
Φ⊕(x, y) = min{1, x + y}
Φ⊙(x, y) = max{0, x + y − 1},
for all x, y ∈ [0, 1]. Detailed constructions of Φ⊕ and Φ⊙ are provided in Lemma B.3. According
to Lemma B.2 [29], compositions of ReLU networks are again ReLU networks. The ReLU network
realizing the term function associated with the MV term τ can hence be obtained by concatenating
ReLU networks implementing the operations ⊕, ⊙, and ¬ as they appear in the expression for τ. What
is more, inspection of the proof of Lemma B.2 reveals that the integer-valued nature of the weights is
preserved in the process of composition. Therefore, the resulting overall ReLU network has integer
weights.
For example, applying the procedure just outlined (see Appendix E for details) to the simple ex-
ample τ = (x ⊕ x) ⊙ ¬y yields the associated ReLU network
Φτ = W3 ◦ ρ ◦ W2 ◦ ρ ◦ W1,
(3)
where
W1(x, y) =


−2
0
0
1
0
−1


x
y

+


1
0
0

,
x, y ∈ R,
W2(x) =
Step 2: Extract MV terms from individual σ-neurons, which are of the form
σ(m1x1 + · · · + mnxn + b),
with m1, . . . , mn, b ∈ Z. The following lemma provides an inductive way for accomplishing this.
Lemma 2.3. [30, 13] Consider the function f(x1, . . . , xn) = m1x1 +· · ·+mnxn +b, (x1, . . . , xn) ∈ [0, 1]n,
with m1, . . ., mn, b ∈ Z. Without loss of generality, assume that maxn
i=1 |mi| = m1. Let f◦(x1, . . . , xn) =
(m1 − 1)x1 + m2x2 + · · · + mnxn + b. Then,
σ(f) = (σ(f◦) ⊕ x1) ⊙ σ(f◦ + 1).
(4)
Before proceeding to the next step, we demonstrate the application of Lemma 2.3 by way of the
simple example σ(x1 − x2 + x3 − 1). First, we eliminate the variable x1 according to
σ(x1 − x2 + x3 − 1) = (σ(−x2 + x3 − 1) ⊕ x1) ⊙ σ(−x2 + x3).
(5)
Next, we remove the x3-terms inside σ(·),
σ(−x2 + x3 − 1) = (σ(−x2 − 1) ⊕ x3) ⊙ σ(−x2)
(6)
σ(−x2 + x3) = (σ(−x2) ⊕ x3) ⊙ σ(−x2 + 1).
(7)
We note that σ(−x2) = 0, for x2 ∈ [0, 1]. Owing to x ⊙ 0 = 0, for x ∈ [0, 1], (6) reduces to σ(−x2 + x3 −
1) = 0. Likewise, in (7) σ(−x2) ⊕ x3 = x3. We can then further simplify (7) according to
x3 ⊙ σ(−x2 + 1) = x3 ⊙ (1 − σ(x2))
(8)
= x3 ⊙ ¬x2,
(9)
where in (8) we used σ(x) = 1 − σ(−x + 1), for x ∈ R, and (9) is by σ(x) = x and ¬x = 1 − x, both
for x ∈ [0, 1]. Substituting the simplified results of (6) and (7) back into (5), we obtain the MV term
corresponding to σ(x1 − x2 + x3 − 1) as x1 ⊙ (x3 ⊙ ¬x2).
Step 3: Compose the MV terms corresponding to the individual σ-neurons according to the layered
structure of the CReLU network to get the MV term associated with the ReLU network. To illustrate
this step, suppose that the neurons σ(1)(·) and σ(2)(·) have associated MV terms τ (1) and τ (2), respec-
tively, and a third neuron σ(3)(m1x1 + m2x2 + b) has associated MV term τ (3)(x1, x2). The MV term
corresponding to the CReLU network σ(3)(m1σ(1)+m2σ(2)+b) is obtained by replacing all occurrences
of x1 in τ (3) by τ (1) and all occurrences of x2 by τ (2). This finalizes the proof of Theorem 2.2.
The essence of the proof of Theorem 2.2 resides in a strong algebraic property shared by the stan-
dard MV algebra I and ReLU networks. Concretely, compositions of ReLU networks with integer
weights again yield ReLU networks with integer weights, and compositions of formulae in MV logic
result in formulae in MV logic. As we shall see in Section 4, the parallelism between logical formulae
and ReLU networks identified here extends to the cases of ReLU networks with rational weights and
Rational Łukasiewicz logic as well as ReLU networks with real weights and RL.
We hasten to add that the extracted formula associated with a given McNaughton function is not
unique. Different algebraic expressions can exist, but they must be functionally equivalent as they all
represent the same truth function in I.
3
Aspects of Our Algorithm
Recalling that ReLU networks (with integer weights) realize continuous piecewise linear functions
(with integer coefficients), the algorithm devised in the previous section can equivalently be seen as
extracting an MV formula from a given McNaughton function f. We are aware of two other construc-
tive extraction procedures, namely the Schauder hat method [13] and the hyperplane method [14].
The purpose of this section is to briefly review these two procedures and to compare them to our
algorithm.
We start with the Schauder hat method and note that Schauder hats are functions of pyramidal
shape supported on unions of simplices. This method starts with the construction of a simplicial
complex over [0, 1]n obtained by splitting the unit cube according to different permutations of the
5
0
0.5
1
0
1
0
0.25
0.5
0.75
1
0
1
Figure 2: Left: the function g, right: the function g2.
linear pieces of f. The resulting simplicial complex is further subdivided into a unimodular simplicial
complex. Thanks to unimodularity, each Schauder hat can then be expressed in terms of “min” and
“max” operations, which are realizable by the operations ⊕, ⊙, and ¬ according to
min{x, y} = ¬(¬x ⊙ y) ⊙ y := x ∧ y
max{x, y} = ¬(¬x ⊕ y) ⊕ y := x ∨ y.
(10)
The overall MV term is finally constructed by combining the MV terms corresponding to the individual
Schauder hats through the ⊕ operation. The reader is referred to [13] for a detailed account of the
algorithm.
The hyperplane method [14] expresses f, with linear pieces p1, . . . , pℓ, in terms of the truncated
linear polynomials σ(p1), . . . , σ(pℓ), where σ(x) = min{1, max{0, x}}, for x ∈ R, according to
f = min
I
max
J
σ(pi),
(11)
where I, J ⊂ {1, . . . , ℓ} are index sets. Next, MV terms corresponding to σ(p1), . . . , σ(pℓ) are deter-
mined by repeated application of Lemma 2.3. These expressions are finally combined into an MV term
based on (11) and (10).
The first difference between our algorithm and the two existing ones resides in the fact that we
work with a ReLU network Φ that realizes the McNaughton function f, instead of starting from f itself.
While this aspect might seem innocuous, it has important ramifications as discussed next. First, it is
simple to describe a McNaughton function on the unit interval [0, 1], but rather complicated to specify
McNaughton functions on the multi-dimensional unit cube [0, 1]n, for n ≥ 2. Specifically, Mundici [13]
showed that f : [0, 1]n → [0, 1] can be specified by listing all its linear pieces p1, . . . , pℓ along with
the indices of the linear pieces that f falls on at the rational points (c1/d1, . . . , cn/dn) ∈ [0, 1]n with
0 < d1, . . . , dn ≤ (n + 1)(2na)n and c1, . . . , cn ∈ Z. Here, a is the maximum absolute value of all
coefficients of p1, . . . , pℓ. In comparison, it is much easier to specify a ReLU network, namely by simply
providing the associated affine maps. In particular, our approach also provides a parametrization of
all valid McNaughton functions, simply by varying the integer weights across all ReLU networks that
map [0, 1]n to [0, 1]. Note that this, of course, also entails varying network architectures, i.e., depth and
the number of nodes in the individual layers (save for the input and output layers).
The second difference lies in the algebraic expressions obtained by these algorithms, notably in
their complexity, as measured by the length of the formulae. We illustrate this aspect by way of exam-
ples, limiting ourselves to one-dimensional functions, for ease of exposition. Consider the hat function
g : [0, 1] → [0, 1] in Figure 2,
g(x) = ρ(2x) − 2ρ(2x − 1) + ρ(2x − 2)
=
(
2x,
0 ≤ x ≤ 1
2
2 − 2x,
1
2 < x ≤ 1.
The Schauder hat method. The unimodular simplicial complex delivered by the algorithm has vertices
{0, 1/2, 1}. There is only one Schauder hat, which is centered at x = 1/2 and given by
h 1
2 (x) =
(
x,
0 ≤ x ≤ 1
2
1 − x,
1
2 < x ≤ 1.
6
The MV term corresponding to h 1
2 (x) = min{x, 1 − x}, for x ∈ [0, 1], is given by x ∧ ¬x. As g(x) =
2h 1
2 (x), for x ∈ [0, 1], the MV term associated with g is
(x ∧ ¬x) ⊕ (x ∧ ¬x).
(12)
The hyperplane method. Denote the two linear pieces of g by p1 : x 7→ 2x and p2 : x 7→ 2−2x. Iterative
application of Lemma 2.3 produces the MV term corresponding to σ(p1) as x ⊕ x, and that associated
with σ(p2) as ¬x⊕¬x. Direct inspection of Figure 2 shows that g(x) = min{p1(x), p2(x)}, for x ∈ [0, 1].
Therefore, the overall MV term corresponding to g is given by
(x ⊕ x) ∧ (¬x ⊕ ¬x).
(13)
Our algorithm. We first note that g can be realized by a ReLU network Φg according to Φg = W2 ◦ ρ ◦
W1 = g with
W1(x) =


2
2
2

x −


0
1
2

,
W2(x) =
0
1
4
1
3
1
0
1
4
0
1
4
1
3
1
2
1
0
1
3
0
1
3
1
2
3
4
1
0
1
2
0
1
2
3
4
1
0
1
4
Figure 3: Schauder hats at vertices { 1
4, 1
3, 1
2, 3
4} associated with the function g2.
Repeated application of Lemma 2.3 produces MV terms associated with each of the truncated linear
pieces:
σ(p1) : x ⊕ x ⊕ x ⊕ x
σ(p2) : ¬(((((x ⊙ x) ⊕ x) ⊙ (x ⊕ x)) ⊕ x) ⊙ (x ⊕ x ⊕ x))
σ(p3) : ((x ⊙ x ⊙ x) ⊕ x) ⊙ (((x ⊙ x) ⊕ x) ⊙ (x ⊕ x))
σ(p4) : ¬(x ⊙ x ⊙ x ⊙ x).
(19)
Substituting (19) back into (18) yields the overall MV term associated with g2, which, again, is too
long to be shown here.
Our algorithm. As g2 = g ◦g, the ReLU network realizing g2 is simply Φg ◦Φg. The resulting MV term
for g2 is consequently given by the composition of (17) with itself, i.e., one replaces every occurrence
of x in (17) by (x ⊕ x) ⊙ ¬(x ⊙ x). The final result is
(((x ⊕ x) ⊙ ¬(x ⊙ x)) ⊕ ((x ⊕ x) ⊙ ¬(x ⊙ x))) ⊙ ¬(((x ⊕ x) ⊙ ¬(x ⊙ x)) ⊙ ((x ⊕ x) ⊙ ¬(x ⊙ x))), (20)
which, while still somewhat unwieldy, is significantly shorter than the expressions obtained by the
other two methods. Finally, we note that one can mix the results obtained by the different algorithms,
e.g., for g2 = g◦g, we can also replace every occurrence of x in (17) by (12) or by (13), to obtain a valid
MV term for g2. We observe that our method delivers a shorter MV term as it exploits the composi-
tional structure of g2 = g ◦ g, a property that is naturally present in deep ReLU network realizations
of McNaughton functions. It is hence sensible to expect that our approach leads to shorter formu-
lae whenever a “deep” ReLU network realization of the McNaughton function under consideration is
available.
The next example serves to illustrate this aspect in a broader context. Specifically, we want to show
how the nonuniqueness in ReLU network realizations, i.e., for a given function there are infinitely
many ReLU networks that realize it, can be exploited by our algorithm to derive different algebraic ex-
pressions for the same truth function in MV logic. Considering a “shallow” realization of the function
g2 above according to
Φ2
g(x) = ρ(4x) − 2ρ(4x − 1) + 2ρ(4x − 2) − 2ρ(4x − 3) + ρ(4x − 4),
(21)
we first transform (21) into the equivalent CReLU network
Ψ2
g = W2 ◦ σ ◦ W1,
8
with
W1(x) =




4
4
4
4



 x +




0
−1
−2
−3



,
x ∈ R,
W2(x) =
4.1
The Rational Case
Rational Łukasiewicz logic extends MV logic by adding a division (by integers) operation. The alge-
braic counterpart is given by the so-called divisible many-valued (DMV) algebras [11].
Definition 4.1. Consider the MV algebra I in Definition 1.1. Define the unary operations δix = 1
i x, x ∈
[0, 1], for all i ∈ N. The structure Id = ⟨[0, 1], ⊕, ¬, {δi}i∈N, 0⟩ is a DMV algebra [11].
The class of term functions in Id is the class of continuous piecewise linear functions whose linear
pieces have rational coefficients [11, 31]. In analogy to Theorem 2.1, such functions are called rational
McNaughton functions. We next extend Theorem 2.2 to the rational case.
Theorem 4.2. For n ∈ N, let τ(x1, . . . , xn) be a DMV term in n variables and τ Id : [0, 1]n → [0, 1] the
associated term function in Id. There exists a ReLU network Φ with rational weights, satisfying
Φ(x1, . . . , xn) = τ Id(x1, . . . , xn),
for all (x1, . . . , xn) ∈ [0, 1]n. Conversely, for every ReLU network Φ : [0, 1]n → [0, 1] with rational weights,
there exists a DMV term τ(x1, . . . , xn) whose associated term function in Id satisfies
τ Id(x1, . . . , xn) = Φ(x1, . . . , xn),
for all (x1, . . . , xn) ∈ [0, 1]n.
Proof. We already know how to realize the operations ⊕, ⊙, and ¬ by ReLU networks, see Section 2.
The division operation δi : x →
1
i x, for i ∈ N, by virtue of being affine, is trivially realized by a
single-layer ReLU network. Following Lemma B.2, ReLU network realizations of formulae in Rational
Łukasiewicz logic are obtained by concatenating ReLU networks implementing the operations ⊕, ⊙, ¬,
and {δi}i∈N. Again, inspection of the proof of Lemma B.2 reveals that the resulting ReLU network has
rational weights. We recover the algebraic property observed already for MV logic, namely compo-
sitions of ReLU networks with rational weights result in ReLU networks with rational weights, and
compositions of DMV formulae again yield DMV formulae.
Next, we extend our algorithm, described in Section 2, to extract DMV terms from ReLU networks
with rational weights. Steps 1 and 3 remain unaltered. We only need to modify Step 2 because the
σ-neurons are now of the form
h = σ(m1x1 + · · · + mnxn + b),
(28)
with m1, . . . , mn, b ∈ Q, rendering Lemma 2.3 no longer applicable. Instead, we employ an idea by [31]
to transform (28) into multiple copies of h which all have arguments with integer coefficients. Con-
cretely, let s ∈ N be the least common multiple of the denominators of m1, . . . , mn, b. Recognizing
that
sσ(x) = σ(sx) + σ(sx − 1) + · · · + σ(sx − (s − 1)),
for x ∈ R, and setting hi = σ(s(m1x1 + · · · + mnxn + b) − i), it follows that h = Ps−1
i=0
1
shi. As
h = Ps−1
i=0
1
shi ≤ 1, the DMV term associated with h is finally obtained according to ⊕s−1
i=0 δsτi.
The Schauder hat and the hyperplane methods were extended to Rational Łukasiewicz logic in [11].
4.2
The Real Case
We finally turn to the case of ReLU networks with real coefficients, which is of particular practical
interest as it allows to extract formulae from trained ReLU networks. The Riesz many-valued algebra
(RMV) [12] extends the MV algebra in Definition 1.1 by adding the multiplication operation {∆r : x →
rx, for x ∈ [0, 1]}r∈[0,1]. The term functions of the corresponding logical system RL are continuous
piecewise linear functions with real coefficients [12], which can be realized by ReLU networks with
real weights. Moreover, one gets ReLU networks realizing truth functions in RL in the same manner as
in the integer and rational cases, namely by composing ReLU networks realizing the logical operations
appearing in the RMV formuale under consideration. Again, we have the algebraic property of the
compositions of ReLU networks resulting in ReLU networks, while retaining the real-valued nature of
the network weights, and compositions of RMV formulae yielding RMV formulae.
10
We now generalize our algorithm to extract RMV formulae from ReLU networks with real weights.
Concretely, Steps 1 and 3 in Section 2 again remain unaltered. In Step 2, with each σ-neuron of the
form
σ(m1x1 + · · · + mnxn + b),
where m1, . . . , mn, b ∈ R, instead of Lemma 2.3, one applies the following result.
Lemma 4.3. [12] Consider the function f(x1, . . . , xn) = m1x1+· · ·+mnxn+b, (x1, . . . , xn) ∈ [0, 1]n, with
m1, . . ., mn, b ∈ R. For all m ∈ (0, 1] and i ∈ {1, . . . , n}, with f◦(x1, . . . , xn) = m1x1 + · · · + mi−1xi−1 +
(mi − m)xi + mi+1xi+1 + · · · + mnxn + b, it holds that
σ(f) = (σ(f◦) ⊕ (mxi)) ⊙ σ(f◦ + 1).
(29)
We demonstrate the application of Lemma 4.3 through a simple example. Consider the σ-neuron
σ

1
√
2x1 − 2x2

. First apply Lemma 4.3 with m =
1
√
2 and i = 1 to get
σ
 1
√
2x1 − 2x2

=

σ(−2x2) ⊕
 1
√
2x1

⊙ σ(−2x2 + 1),
(30)
thereby eliminating x1. As σ(−2x2) = 0 and 0⊕x = x, (30) reduces to ( 1
√
2x1)⊙σ(−2x2 +1). Next, by
applying Lemma 4.3 twice and using σ(x) = 1 − σ(1 − x), x ∈ R, we obtain the RMV term associated
with σ(−2x2 + 1) as ¬(x2 ⊕ x2). The RMV term corresponding to
1
√
2x1 is given by ∆ 1
√
2 x1. Therefore,
the overall RMV term associated with σ

1
√
2x1 − 2x2

is ∆ 1
√
2 x1 ⊙ ¬(x2 ⊕ x2).
The hyperplane method was extented to the real case in [12]. An extension of the Schauder hat
method to the real case does not seem to be available in the literature, but can easily be devised. We
will report it elsewhere.
We finally note that the conclusions on the differences between our algorithm and the other two
algorithms as summarized in Section 2 carry over to the rational and real cases.
5
Acknowledgment
The authors are deeply grateful to Prof. Olivia Caramello for drawing their attention to the Mc-
Naughton theorem and, more generally, to MV logic.
References
[1] G. Lample and F. Charton,
Deep learning for symbolic mathematics,
arXiv preprint
arXiv:1912.01412 (2019).
[2] S. Bubeck, V. Chandrasekaran, R. Eldan, et al., Sparks of artificial general intelligence: Early
experiments with GPT-4, arXiv preprint arXiv:2303.12712 (2023).
[3] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang,
Transformers learn shortcuts to
automata, arXiv preprint arXiv:2210.10749 (2022).
[4] S. Skyum and L. G. Valiant, A complexity theory based on Boolean algebra, Journal of the ACM
(JACM) 32 (1985) 484–502.
[5] R. B. Boppana and M. Sipser, The complexity of finite functions, in: Algorithms and Complexity,
Elsevier, 1990, pp. 757–804.
[6] A. Nakashima, The theory of relay circuit composition, The Journal of the Institute of Telegraph
and Telephone Engineers of Japan 38 (1935) 461–489.
[7] V. I. Shestakov, Some Mathematical Methods for the Construction and Simplification of Two-
Terminal Electrical Networks of Class A, Phd thesis, The Lomonosov State University, Moscow,
Russia, 1938.
11
[8] C. E. Shannon, A symbolic analysis of relay and switching circuits, Electrical Engineering 57
(1938) 713–723.
[9] R. Cignoli, I. d’Ottaviano, and D. Mundici, Algebraic Foundations of Many-Valued Reasoning,
Springer, 2000.
[10] R. McNaughton, A theorem about infinite-valued sentential logic, The Journal of Symbolic Logic
16 (1951) 1–13.
[11] B. Gerla, Rational Łukasiewicz logic and DMV-algebras, Neural Network World 6 (2001).
[12] A. Di Nola and I. Leuştean, Łukasiewicz logic and Riesz spaces, Soft Computing 18 (2014)
2349–2363.
[13] D. Mundici, A constructive proof of McNaughton’s theorem in infinite-valued logic, The Journal
of Symbolic Logic 59 (1994) 596–602.
[14] S. Aguzzoli, Geometrical and Proof Theoretical Issues in Łukasiewicz Propositional Logics, Phd
thesis, University of Siena, Italy, 1998.
[15] P. Amato, A. Di Nola, and B. Gerla, Neural networks and rational Łukasiewicz logic, in: Annual
Meeting of the North American Fuzzy Information Processing Society Proceedings, 2002, pp.
506–510.
[16] P. Amato, A. Di Nola, and B. Gerla, Neural networks and rational McNaughton Functions, Jour-
nal of Multiple-Valued Logic and Soft Computing 11 (2005) 95–110.
[17] A. Di Nola, B. Gerla, and I. Leuştean, Adding real coefficients to Łukasiewicz logic: An appli-
cation to neural networks, Fuzzy Logic and Applications: 10th International Workshop (2013)
77–85.
[18] G. G. Towell and J. W. Shavlik, Knowledge-based artificial neural networks, Artificial intelligence
70 (1994) 119–165.
[19] A. S. Avila Garcez and G. Zaverucha, The connectionist inductive learning and logic program-
ming system, Applied Intelligence 11 (1999) 59–77.
[20] R. Riegel, A. Gray, F. Luus, et al., Logical neural networks, arXiv preprint arXiv:2006.13155
(2020).
[21] L. Biggio, T. Bendinelli, A. Lucchi, and G. Parascandolo, A seq2seq approach to symbolic regres-
sion, Learning Meets Combinatorial Algorithms at NeurIPS2020 (2020).
[22] L. Biggio, T. Bendinelli, A. Neitz, A. Lucchi, and G. Parascandolo, Neural symbolic regression
that scales, in: International Conference on Machine Learning, 2021, pp. 936–945.
[23] C. E. Shannon, The synthesis of two-terminal switching circuits, The Bell System Technical
Journal 28 (1949) 59–98.
[24] J. E. Savage, Computational work and time on finite machines, Journal of the ACM 19 (1972)
660–674.
[25] N. Pippenger and M. J. Fischer, Relations among complexity measures, Journal of the ACM 26
(1979) 361–381.
[26] A. Tarski, Logic, semantics, metamathematics: Papers from 1923 to 1938, Hackett Publishing,
1983.
[27] C. C. Chang, Algebraic analysis of many-valued logics, Transactions of the American Mathe-
matical Society 88 (1958) 467–490.
[28] C. C. Chang, A new proof of the completeness of the Łukasiewicz axioms, Transactions of the
American Mathematical Society 93 (1959) 74–80.
12
[29] D. Elbrächter, D. Perekrestenko, P. Grohs, and H. Bölcskei, Deep neural network approximation
theory, IEEE Transactions on Information Theory 67 (2021) 2581–2623.
[30] A. Rose and J. B. Rosser, Fragments of many-valued statement calculi, Transactions of the Amer-
ican Mathematical Society 87 (1958) 1–53.
[31] M. Baaz and H. Veith, Interpolation in fuzzy logic, Archive for Mathematical Logic 38 (1999)
461–489.
13
A
MV algebras
Definition A.1. [9] A many-valued algebra is a structure A = ⟨A, ⊕, ¬, 0⟩ consisting of a nonempty set A, a
constant 0 ∈ A, a binary operation ⊕, and a unary operation ¬ satisfying the following axioms:
x ⊕ (y ⊕ z) = (x ⊕ y) ⊕ z
(31.1)
x ⊕ y = y ⊕ x
x ⊕ 0 = x
(31.2)
¬¬x = x
x ⊕ ¬0 = ¬0
¬(¬x ⊕ y) ⊕ y = ¬(¬y ⊕ x) ⊕ x.
Specifically, (31.1)-(31.2) state that the structure ⟨A, ⊕, 0⟩ is an abelian monoid. An MV algebra
⟨A, ⊕, ¬, 0⟩ is said to be nontrivial iff A contains more than one element. In each MV algebra we can
define a constant 1 and a binary operation ⊙ as follows:
1 := ¬0
(32)
x ⊙ y := ¬(¬x ⊕ ¬y).
(33)
The ensuing identities are then direct consequences of Definition A.1:
x ⊙ (y ⊙ z) = (x ⊙ y) ⊙ z
x ⊙ y = y ⊙ x
x ⊙ 1 = x
x ⊙ 0 = 0.
We will frequently use the notions of MV terms and term functions formalized as follows.
Definition A.2. [9] Let n ∈ N and Sn = {(, ), 0, ¬, ⊕, x1, . . . , xn}. An MV term in the variables x1, . . . , xn
is a finite string over Sn arising from a finite number of applications of the operations ¬ and ⊕ as follows. The
elements 0 and xi, for i = 1, . . . , n, considered as one-element strings, are MV terms.
1. If the string τ is an MV term, then ¬τ is also an MV term.
2. If the strings τ and γ are MV terms, then (τ ⊕ γ) is also an MV term.
We write τ(x1, . . . , xn) to emphasize that τ is an MV term in the variables x1, . . . , xn.
For instance, the following finite strings over S2 = {(, ), 0, ¬, ⊕, x1, x2} are MV terms in the vari-
ables x1 and x2:
0, x1, x2, ¬0, ¬x2, (x1 ⊕ ¬x2).
We shall always omit the outermost pair of brackets for conciseness, i.e., we write x1 ⊕ ¬x2 instead
of (x1 ⊕ ¬x2). Besides, for brevity we use the symbols ⊙ and 1 as abbreviations according to (32)
and (33) when writing MV terms.
MV terms are actually logical formulae without operational meaning. To endow them with mean-
ing, an MV algebra must be specified. The associated truth functions, a.k.a. term functions which we
define presently, are then obtained by interpreting the operations ⊕ and ¬ according to how they are
specified in the MV algebra.
Definition A.3. [9] Let τ(x1, . . . , xn) be an MV term in the variables x1, . . . , xn. Let A = ⟨A, ⊕, ¬, 0⟩ be
an MV algebra. The term function τ A : An → A associated with τ in A is defined as follows. For every input
(a1, . . . , an) ∈ An, first substitute ai for all occurrences of the variable xi in τ:
1. xA
i : (a1, . . . , an) 7→ ai,
for i = 1, . . . , n.
Then, proceed by induction on the number of operation symbols, i.e., ⊕ and ¬, occurring in τ by applying the
following rules:
2. (¬τ)A = ¬τ A,
for MV term τ,
3. (τ ⊕ γ)A = τ A ⊕ γA,
for MV terms τ and γ.
14
B
ReLU networks
Definition B.1. [29] Let L ∈ N and N0, N1, . . ., NL ∈ N. A ReLU network is a map Φ : RN0 → RNL given
by
Φ =





W1,
L = 1
W2 ◦ ρ ◦ W1,
L = 2
WL ◦ ρ ◦ WL−1 ◦ ρ ◦ · · · ◦ ρ ◦ W1,
L ≥ 3
,
where, for ℓ ∈ {1, 2, . . ., L}, Wℓ : RNℓ−1 → RNℓ, Wℓ(x) := Aℓx + bℓ are affine transformations with weight
matrices Aℓ = RNℓ×Nℓ−1 and bias vectors bℓ ∈ RNℓ, and the ReLU activation function ρ : R → R, ρ(x) :=
max{0, x} acts component-wise. Moreover, we denote by Nd,d′ the set of ReLU networks with input dimension
N0 = d and output dimension NL = d′. Moreover, we define the number of layers of the network Φ, denoted by
L(Φ), to be equal to L.
The next result formalizes properties of ReLU network compositions.
Lemma B.2.
[29] Let d1, d2, d3 ∈ N, Φ1 ∈ Nd1,d2, and Φ2 ∈ Nd2,d3. There exists a network Ψ ∈ Nd1,d3
with L(Ψ) = L(Φ1) + L(Φ2), and satisfying
Ψ(x) = (Φ2 ◦ Φ1)(x),
for all x ∈ Rd1.
Proof. The proof is based on the identity x = ρ(x) − ρ(−x). First, note that by Definition B.1, we can
write
Φ1 = W 1
L1 ◦ ρ ◦ W 1
L1−1 ◦ · · · ◦ ρ ◦ W 1
1
and
Φ2 = W 2
L2 ◦ ρ ◦ W 2
L2−1 ◦ · · · ◦ ρ ◦ W 2
1 .
Next, let N 1
L1−1 denote the width of layer L1 − 1 in Φ1 and N 2
1 the width of layer 1 in Φ2. We define
the affine transformations f
W 1
L1 : RN 1
L1−1 → R2d2 and f
W 2
1 : R2d2 → RN 2
1 according to
f
W 1
L1(x) : =

Id2
−Id2

W 1
L1(x)
f
W 2
1 (x) : = W 2
1
Now, applying Definition B.2 to concatenate the networks Φ1(x, y) =
D
Proof of Lemma 4.3
We can follow the line of arguments in the proof of Lemma 2.3 and consider four different cases.
Case 1: f◦(x) ≥ 1, for all x ∈ [0, 1]n. In this case, the LHS of (29) is
σ(f) = 1.
The RHS becomes
(σ(f◦) ⊕ (mxi)) ⊙ σ(f◦ + 1) = (1 ⊕ (mxi)) ⊙ 1 = 1.
Case 2: f◦(x) ≤ −1, for all x ∈ [0, 1]n. In this case, the LHS of (29) is
σ(f) = 0
and the RHS is given by
(σ(f◦) ⊕ (mxi)) ⊙ σ(f◦ + 1) = (0 ⊕ (mxi)) ⊙ 0 = 0.
Case 3: −1 < f◦(x) ≤ 0, for all x ∈ [0, 1]n. In this case, f ∈ (−1, 1] as mxi ∈ [0, 1]. The RHS of (4)
becomes
(σ(f◦) ⊕ (mxi)) ⊙ σ(f◦ + 1)
= (0 ⊕ (mxi)) ⊙ (f◦ + 1)
= (mxi) ⊙ (f◦ + 1)
= max{0, mxi + f◦ + 1 − 1}
= max{0, f}
= σ(f).
Case 4: 0 < f◦(x) < 1, for all x ∈ [0, 1]n. In this case, f ∈ (0, 2). The RHS of (29) becomes
(σ(f◦) ⊕ (mxi)) ⊙ σ(f◦ + 1)
= (f◦ ⊕ (mxi)) ⊙ 1
= f◦ ⊕ (mxi)
= min{1, f◦ + mxi}
= min{1, f}
= σ(f).
E
Construction of the network Φτ in Section 2
By Lemma B.3, we have the ReLU network associated with x ⊕ x according to
W ⊕
2 ◦ ρ ◦
"
"This paper extends multi-tildes (regular operators) to use any Boolean combination of tildes. It shows how to succinctly describe and efficiently compute these Boolean multi-tildes. It then explores their properties, providing insights into их computation and properties of the languages they describe.","Regular expressions are widely used to efficiently represent sets of words. Multi-tildes (a type of regular operator) can improve the factorization power of regular expressions by conditionally inserting the empty word in catenations. But previously, multi-tildes were restricted to continuous positions and applied across disjunctive combinations. This paper generalizes multi-tildes to any Boolean combination, allowing for non-contiguous intervals and conditional empty-word insertion.","The paper bases its work on foundational research in the following areas:

* Regular expressions and their properties
* Boolean formulae and their evaluation
* Glushkov automata, which are a type of finite automaton
* Multi-tildes and their use in regular languages

The paper cites several relevant prior works, including foundational research on multi-tildes and Glushkov automata, as well as algorithmic improvements to the construction of Glushkov automata.nan","The paper takes the following steps to achieve its objectives:

1. It defines constrained multi-tildes as a generalization of classical multi-tildes, allowing Boolean combinations of tildes.

2. It shows that the action of constrained multi-tildes preserves regularity.

3. It defines the partial derivative of a Boolean formula over a language sequence and proves properties of this derivative.

4. It defines extended regular expressions that incorporate constrained multi-tildes and shows how they can be exponentially smaller than equivalent automata.

5. It defines the partial derivative and derived term automaton for extended regular expressions and proves that the latter recognizes the language denoted by the expression.

6. It presents a method for computing the Glushkov automaton of an extended regular expression, generalizing the classical Glushkov construction.","The paper's main results include:

* A definition of constrained multi-tildes as a generalization of classical multi-tildes, allowing for Boolean combinations of tildes.

* A proof that the action of constrained multi-tildes preserves regularity.

* Definitions of the partial derivative and derived term automaton for extended regular expressions and proofs that they recognize the language denoted by the expression.

* A method for computing the Glushkov automaton of an extended regular expression, generalizing the classical Glushkov construction.

* A characterization of the structure of the Glushkov automaton for an extended regular expression, providing insights into its properties.","The paper successfully generalizes multi-tildes to any Boolean combination and explores the properties and computation of these constrained multi-tildes. It also provides insights into the structure of Glushkov automata for extended regular expressions, setting the stage for future work on converting automata to expressions.",Constrained Multi-Tildes: Derived Term and Position Automata,"Samira Attou, Ludovic Mignot, Clément Miklarz, Florent Nicart","January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
Constrained Multi-Tildes: Derived Term and Position Automata
Samira Attou, Ludovic Mignot, Clément Miklarz and Florent Nicart
GR2IF, Université de Rouen Normandie, Avenue de l’Université, 76801
Saint-Étienne-du-Rouvray, France
{samira.attou2, ludovic.mignot, clement.miklarz1, florent.nicart}@univ-rouen.fr
Multi-tildes are regular operators that were introduced to enhance the factorization
power of regular expressions, allowing us to add the empty word in several factors of
a catenation product of languages. In addition to multi-bars, which dually remove the
empty word, they allow representing any acyclic automaton by a linear-sized expression,
whereas the lower bound is exponential in the classic case.
In this paper, we extend multi-tildes from disjunctive combinations to any Boolean
combination, allowing us to exponentially enhance the factorization power of tildes ex-
pressions. Moreover, we show how to convert these expressions into finite automata and
give a Haskell implementation of them using advanced techniques of functional program-
ming.
Keywords: Regular expressions; Partial derivatives; Boolean formulae; Multi-tildes op-
erators.
1. Introduction
Regular expressions are widely used inductively defined objects that allow us to
easily represent (potentially infinite) set of words. In order to solve efficiently the
membership test, they can be turned into finite automata [1], where the number of
states is linear w.r.t. the number of symbols of the expressions. Numerous operators
where added in order to enhance their representation powers, such as Boolean op-
erators. However, the number of states after the conversion is not necessarily linear
anymore [6].
Another class of operators, the multi-tildes [3], was introduced in order to allow
a constrained adjunction of the empty word in some factors of the catenation prod-
uct of regular languages. In combination with multi-bars [4], multi-tildes allow to
improve the factorization power of regular languages: as an example, it is shown that
any acyclic automaton can be turned into a linear-sized equivalent multi-tildes-bars
expression, whereas the lower bound is exponential in the classical case [8]. However,
they can be applied only across continuous positions.
In this paper, we extend the idea behind the conception of (disjunctive) multi-
tildes to any Boolean combination of them. These Boolean combinations allow us
to extend the specification power of expressions, by e.g., applying tildes across non-
continuous intervals of positions. We show that their actions over languages preserve
regularity, that they may lead to exponentially smaller expressions and how to solve
1
arXiv:2401.12111v1  [cs.FL]  22 Jan 2024
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
2
S.Attou et al.
the membership test by defining a finite automaton.
This is the first step of a more general plan: we aim to develop a characterization
of the produced automaton in order to inverse the computation, i.e., the conversion
of an automaton into a short constrained tildes expression.
The paper is organized as follows. Section 2 contains general preliminaries. Then,
we recall in Section 3 classical definitions and constructions for Boolean formulae.
These latter allow us to define constrained tildes in Section 4. We study their fac-
torization power in Section 5, and show how to convert these expressions into finite
automata in Section 6 and Section 7. Finally, in Section 8, we present a Haskell
implementation of these objects.
2. Preliminaries
Throughout this paper, we use the following notations:
• B is the Boolean set {0, 1},
• 𝑆 → 𝑆′ is the set of functions from a set 𝑆 to a set 𝑆′,
• for a Boolean 𝑏 and a set 𝑆, 𝑆 | 𝑏 is the set 𝑆 if 𝑏, ∅ otherwise,
• ⊂ is to be understood as not necessarily strict subset.
A regular expression 𝐸 over an alphabet Σ is inductively defined by
𝐸 = 𝑎,
𝐸 = ∅,
𝐸 = 𝜀,
𝐸 = 𝐹 · 𝐺,
𝐸 = 𝐹 + 𝐺,
𝐸 = 𝐹∗,
where 𝑎 is a symbol in Σ and 𝐹 and 𝐺 two regular expressions over Σ. Classical
priority rules hold: ∗ > · > +. The language denoted by 𝐸 is the set 𝐿(𝐸) inductively
defined by
𝐿(𝑎) = {𝑎},
𝐿(∅) = ∅,
𝐿(𝜀) = {𝜀},
𝐿(𝐹 · 𝐺) = 𝐿(𝐹) · 𝐿(𝐺),
𝐿(𝐹 + 𝐺) = 𝐿(𝐹) ∪ 𝐿(𝐺),
𝐿(𝐹∗) = 𝐿(𝐹)∗,
where 𝑎 is a symbol in Σ and 𝐹 and 𝐺 two regular expressions over Σ. A (non-
deterministic) automaton 𝐴 over an alphabet Σ is a 5-tuple (Σ, 𝑄, 𝐼, 𝐹, 𝛿) where
• 𝑄 is a finite set of states,
• 𝐼 ⊂ 𝑄 is the set of initial states,
• 𝐹 ⊂ 𝑄 is the set of final states,
• 𝛿 is a function in Σ × 𝑄 → 2𝑄.
The function 𝛿 is extended to Σ × 2𝑄 → 2𝑄 by 𝛿(𝑎, 𝑃) = Ð
𝑝∈𝑃 𝛿(𝑎, 𝑝) and to
Σ∗ × 2𝑄 → 2𝑄 by 𝛿(𝜀, 𝑃) = 𝑃 and 𝛿(𝑎𝑤, 𝑃) = 𝛿(𝑤, 𝛿(𝑎, 𝑃)). The language denoted
by 𝐴 is the set 𝐿(𝐴) = {𝑤 ∈ Σ∗ | 𝛿(𝑤, 𝐼) ∩ 𝐹 ≠ ∅}.
Any regular expression with 𝑛 symbols can be turned into an equivalent au-
tomaton with at most (𝑛 + 1) states, for example by computing the derived term
automaton [1]. The partial derivative of 𝐸 w.r.t. a symbol 𝑎 in Σ is the set of
expressions 𝛿𝑎(𝐸) inductively defined as follows:
𝛿𝑎(𝑏) = ({𝜀} | 𝑎 = 𝑏),
𝛿𝑎(∅) = ∅,
𝛿𝑎(𝜀) = ∅,
𝛿𝑎(𝐹 + 𝐺) = 𝛿𝑎(𝐹) ∪ 𝛿𝑎(𝐺),
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
Constrained Multi-Tildes: Derived Term and Position Automata
3
𝛿𝑎(𝐹 · 𝐺) = 𝛿𝑎(𝐹) ⊙ 𝐺 ∪ (𝛿𝑎(𝐺) | Null(𝐹)),
𝛿𝑎(𝐹∗) = 𝛿𝑎(𝐹) ⊙ 𝐹∗,
where 𝑏 is a symbol in Σ, 𝐹 and 𝐺 two regular expressions over Σ, Null(𝐹) = 𝜀 ∈
𝐿(𝐹) and E ⊙ 𝐺 = Ð
𝐸∈E{𝐸 · 𝐺}, where ⊙ has priority over ∪. The partial derivative
of 𝐸 w.r.t. a word 𝑤 is defined by 𝛿𝜀(𝐸) = {𝐸} and 𝛿𝑎𝑤(𝐸) = Ð
𝐸′∈ 𝛿𝑎 (𝐸) 𝛿𝑤(𝐸′).
The derived term automaton of 𝐸 is the automaton (Σ, 𝑄, {𝐸}, 𝐹, 𝛿) where
𝑄 =
Ø
𝑤∈Σ∗
𝛿𝑤(𝐸),
𝐹 = {𝐸′ ∈ 𝑄 | Null(𝐸′)},
𝛿(𝑎, 𝐸′) = 𝛿𝑎(𝐸′).
The derived term automaton of 𝐸, with 𝑛 symbols, is a finite automaton with at
most (𝑛 + 1) states that recognizes 𝐿(𝐸).
A multi-tilde is an 𝑛-ary operator which is parameterized by a set 𝑆 of couples
(𝑖, 𝑗) (called tildes) in {1, . . . , 𝑛}2 with 𝑖 ≤ 𝑗. Such an expression is denoted by
MT𝑆(𝐸1, . . . , 𝐸𝑛) while it is applied over 𝑛 expressions (𝐸1, . . . , 𝐸𝑛). Two tildes
(𝑖, 𝑗) and (𝑖′, 𝑗′) are overlapping if {𝑖, . . . , 𝑗} ∩ {𝑖′, . . . , 𝑗′} ≠ ∅. A free subset of 𝑆 is a
subset where no tildes overlap each other. As far as 𝑆 = (𝑖𝑘, 𝑗𝑘)𝑘≤𝑚 is free, the action
of a tilde is to add the empty word in the catenation of the languages denoted by
the expression it overlaps in the catenation of all the denoted languages, i.e.
𝐿(MT𝑆(𝐸1, . . . , 𝐸𝑛)) = 𝐿(𝐸1)·· · ··𝐿(𝐸𝑖1−1)·(𝐿(𝐸𝑖1)·· · ··𝐿(𝐸 𝑗1)∪{𝜀})·𝐿(𝐸 𝑗1+1)·· · ·
· · · 𝐿(𝐸𝑖𝑚−1) · (𝐿(𝐸𝑖𝑚) · · · · · 𝐿(𝐸 𝑗𝑚) ∪ {𝜀}) · 𝐿(𝐸 𝑗𝑚+1) · · · 𝐿(𝐸𝑛).
Inductively extended with multi-tildes operators, regular expressions with 𝑛 symbols
can be turned into equivalent automata with at most 𝑛 states, using the position
automaton [5] or the partial derivation one [7].
In the following, we show how to extend the notion of tildes from unions of free
subsets to any Boolean combinations of tildes.
3. Boolean Formulae and Satisfiability
A Boolean formula 𝜙 over an alphabet Γ is inductively defined by
𝜙 = 𝑎,
𝜙 = 𝑜(𝜙1, . . . , 𝜙𝑛),
where 𝑎 is an atom in Γ, 𝑜 is an operator associated with an 𝑛-ary function 𝑜 𝑓 from
B𝑛 to B, and 𝜙1, . . . , 𝜙𝑛 are 𝑛 Boolean formulae over Γ.
As an example, ¬ is the operator associated with the Boolean negation, ∧ with
the Boolean conjunction and ∨ with the Boolean disjunction. We denote by ⊥ the
constant (0-ary function) 0 and by ⊤ the constant 1.
Let 𝜙 be a Boolean formula over an alphabet Γ. A function 𝑖 from Γ to B is said
to be an interpretation (of Γ). The evaluation of 𝜙 with respect to 𝑖 is the Boolean
eval𝑖(𝜙) inductively defined by
eval𝑖(𝑎) = 𝑖(𝑎),
eval𝑖(𝑜(𝜙1, . . . , 𝜙𝑛)) = 𝑜 𝑓 (eval𝑖(𝜙1), . . . , eval𝑖(𝜙𝑛)),
where 𝑎 an atom in Γ, 𝑜 is an operator associated with an 𝑛-ary function 𝑜 𝑓 from B𝑛
to B, and 𝜙1, . . . , 𝜙𝑛 are 𝑛 Boolean formulae over Γ. Non-classical Boolean functions
can also be considered, like in the following example.
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
4
S.Attou et al.
Example 1. The operator Mirrorn is associated with the (2 × 𝑛)-ary Boolean func-
tion 𝑓 defined for any (2 × 𝑛) Boolean (𝑏1, . . . , 𝑏2𝑛) by
𝑓 (𝑏1, . . . , 𝑏2𝑛) ⇔ (𝑏1, . . . , 𝑏𝑛) = (𝑏2𝑛, . . . , 𝑏𝑛+1) ⇔ (𝑏1 = 𝑏2𝑛) ∧ · · · ∧ (𝑏𝑛 = 𝑏𝑛+1)
⇔ (𝑏1 ∧ 𝑏2𝑛 ∨ ¬𝑏1 ∧ ¬𝑏2𝑛) ∧ · · · ∧ (𝑏𝑛 ∧ 𝑏𝑛+1 ∨ ¬𝑏𝑛 ∧ ¬𝑏𝑛+1).
A Boolean formula is said to be: satisfiable if there exists an interpretation
leading to a positive evaluation; a tautology if every interpretation leads to a positive
evaluation; a contradiction if it is not satisfiable.
Even if it is an NP-Hard problem [11], checking the satisfiability of a Boolean
formula can be performed by using incremental algorithms [12,13,19]. The following
method can be performed: If there is no atom in the formula, then it can be reduced
to either ⊥ or ⊤, and it is respectively a tautology or a contradiction; Otherwise,
choose an atom 𝑎, replace it with ⊥ (denoted by 𝑎 B ⊥), reduce and recursively
reapply the method; If it is not satisfiable, replace 𝑎 with ⊤ (denoted by 𝑎 B ⊤),
reduce and recursively reapply the method. The reduction step can be performed
by recursively simplifying the subformulae of the form 𝑜(𝜙1, . . . , 𝜙𝑛) such that there
exists 𝑘 ≤ 𝑛 satisfying 𝐹𝑘 ∈ {⊥, ⊤}. As an example, the satisfiability of ¬(𝑎 ∧ 𝑏) ∧
(𝑎 ∧ 𝑐) can be checked as shown in Figure 1.
¬(𝑎 ∧ 𝑏) ∧ (𝑎 ∧ 𝑐)
¬(⊥ ∧ 𝒃)∧(⊥ ∧ 𝒄)
¬⊥
∧
⊥
⊤
∧
⊥
⊥
¬(⊤ ∧ 𝒃)∧(⊤ ∧ 𝒄)
¬𝑏
∧
𝑐
¬⊥ ∧ 𝑐
⊤ ∧ 𝒄
𝑐
⊥
⊤
𝑎 B ⊥
𝑎 B ⊤
𝑏 B ⊥
𝑐 B ⊥
𝑐 B ⊤
Fig. 1. The formula ¬(𝑎 ∧ 𝑏) ∧ (𝑎 ∧ 𝑐) is satisfiable.
Two Boolean formulae 𝜙 and 𝜙′ are equivalent, denoted by 𝜙 ∼ 𝜙′, if for any
interpretations 𝑖, eval𝑖(𝜙) = eval𝑖(𝜙′).
Example 2. Let us consider the operator Mirror𝑛 defined in Example 1. It can be
shown, following the equation in Example 1, that for any (2𝑛 − 1) Boolean formulae
(𝜙1, . . . , 𝜙2𝑛−1),
Mirror𝑛(⊥, 𝜙1, . . . , 𝜙2𝑛−1) ∼ Mirror𝑛−1(𝜙1, . . . , 𝜙2𝑛−2) ∧ ¬𝜙2𝑛−1,
Mirror𝑛(⊤, 𝜙1, . . . , 𝜙2𝑛−1) ∼ Mirror𝑛−1(𝜙1, . . . , 𝜙𝑛−2) ∧ 𝜙2𝑛−1,
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
Constrained Multi-Tildes: Derived Term and Position Automata
5
Mirror𝑛() ∼ ⊤.
For any two Boolean formulae 𝜙 and 𝜙′ and for any atom 𝑎, we denote by 𝜙𝑎B𝜙′
the formula obtained by replacing any occurrence of 𝑎 in 𝜙 with 𝜙′. For any two
sequences (𝜙′
1, . . . , 𝜙′
𝑛) of Boolean formulae and (𝑎1, . . . , 𝑎𝑛) of distinct atoms, we
denote by 𝜙𝑎1B𝜙′
1,...,𝑎𝑛B𝜙′𝑛 the formula obtained by replacing any occurrence of 𝑎𝑘
in 𝜙 with 𝜙′
𝑘 for any 1 ≤ 𝑘 ≤ 𝑛.
It is well known that for any Boolean formula 𝜙 and for any atom 𝑎 in 𝜙,
𝜙 ∼ ¬𝑎 ∧ 𝜙𝑎B⊥ ∨ 𝑎 ∧ 𝜙𝑎B⊤.
(1)
4. Constrained Multi-Tildes
Multi-Tildes operators define languages by computing free sublists of tildes from
a set of couples. This can be viewed as a particular disjunctive combination of
these tildes, since sublists of a free list ℓ define languages that are included in
the one ℓ defines. This disjunctive interpretation can be extended to any Boolean
combination. One may choose to apply conjunctive sequences of not contiguous
tildes, or may choose to exclude some combinations of free tildes. In this section,
we show how to model this interpretation using Boolean formulae.
The action of a tilde is to add the empty word in the catenation of the lan-
guages it overhangs. If the tilde is considered as an interval of contiguous positions
(𝑝1, 𝑝2, . . . , 𝑝𝑘), its action can be seen as the conjunction of the substitution of each
language at position 𝑝1, position 𝑝2, etc. with {𝜀}.
In fact, each position can be considered as an atom of a Boolean formula 𝜙.
For any interpretation 𝑖 leading to a positive evaluation of 𝜙, we can use 𝑖(𝑘) to
determine whether the language 𝐿𝑘 can be replaced by {𝜀} in 𝐿1 · · · · · 𝐿𝑛. Let us
formalize these thoughts as follows.
Let 𝑖 be an interpretation over {1, . . . , 𝑛}. Let 𝐿1, . . . , 𝐿𝑛 be 𝑛 languages. We
denote by 𝑖(𝐿1, . . . , 𝐿𝑛) the language 𝐿′
1 · · · · · 𝐿′
𝑛 where 𝐿′
𝑘 =
(
{𝜀}
if 𝑖(𝑘),
𝐿𝑘
otherwise.
Let 𝜙 be a Boolean formula over the alphabet {1, . . . , 𝑛} and 𝐿1, . . . , 𝐿𝑛 be 𝑛
languages. We denote by 𝜙(𝐿1, . . . , 𝐿𝑛) the language
Ø
𝑖|eval𝑖 (𝜙)
𝑖(𝐿1, . . . , 𝐿𝑛).
(2)
Example 3. Let us consider the operator Mirror𝑛 defined in Example 1 and the
two alphabets Γ𝑛 = {1, . . . , 2𝑛} and Σ𝑛 = {𝑎1, . . . , 𝑎2𝑛}. Then:
Mirror𝑛(1, . . . , 2𝑛)({𝑎1}, . . . , {𝑎2𝑛})
= {𝑤1 · · · 𝑤2𝑛 | ∀𝑘 ≤ 2𝑛, 𝑤𝑘 ∈ {𝜀, 𝑎𝑘} ∧ (𝑤𝑘 = 𝜀 ⇔ 𝑤2𝑛−𝑘+1 = 𝜀)}
= {𝑎1 · · · 𝑎2𝑛, 𝑎1𝑎3𝑎4 · · · 𝑎2𝑛−3𝑎2𝑛−2𝑎2𝑛, . . . , 𝑎1𝑎2𝑛, . . . , 𝑎𝑛𝑎𝑛+1, 𝜀}.
First, we remark that the action of constrained tildes preserves regularity, since
it is a finite union of catenations of regular languages, following Equation (2).
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
6
S.Attou et al.
Theorem 4. Let 𝜙 be a Boolean formula over the alphabet {1, . . . , 𝑛} and 𝐿1, . . . , 𝐿𝑛
be 𝑛 regular languages. Then 𝜙(𝐿1, . . . , 𝐿𝑛) is regular.
Moreover, this definition also allows us to explicit some remarkable identities. As
an example, considering 𝑛 languages (𝐿1, . . . , 𝐿𝑛) and a Boolean formula 𝜙 over the
alphabet {1, . . . , 𝑛}, it can be shown that the two following identities hold:
(1) if 𝜙 is a contradiction, then 𝜙(𝐿1, . . . , 𝐿𝑛) = ∅;
(2) if 𝜙 is a tautology, then 𝜙(𝐿1, . . . , 𝐿𝑛) = (𝐿1 ∪ {𝜀}) · · · · · (𝐿𝑛 ∪ {𝜀}).
Some properties of Boolean formulae can also be transferred while acting over lan-
guage sequences, as direct consequences of Equation (2).
Lemma 5. Let 𝜙1 and 𝜙2 be two equivalent Boolean formulae over the alphabet
{1, . . . , 𝑛} and 𝐿1, . . . , 𝐿𝑛 be 𝑛 languages. Then 𝜙1(𝐿1, . . . , 𝐿𝑛) = 𝜙2(𝐿1, . . . , 𝐿𝑛).
Lemma 6. Let 𝜙1 and 𝜙2 be two Boolean formulae over {1, . . . , 𝑛} and 𝐿1, . . . , 𝐿𝑛
be 𝑛 languages. Then (𝜙1 ∨ 𝜙2)(𝐿1, . . . , 𝐿𝑛) = 𝜙1(𝐿1, . . . , 𝐿𝑛) ∪ 𝜙2(𝐿1, . . . , 𝐿𝑛).
Lemma 7. Let 𝜙 be a Boolean formula over the alphabet {2, . . . , 𝑛} and 𝐿1, . . . , 𝐿𝑛
be 𝑛 languages. Then
(1 ∧ 𝜙)(𝐿1, . . . , 𝐿𝑛) = 𝜙2B1,...,𝑛B𝑛−1(𝐿2, . . . , 𝐿𝑛),
(¬1 ∧ 𝜙)(𝐿1, . . . , 𝐿𝑛) = 𝐿1 · 𝜙2B1,...,𝑛B𝑛−1(𝐿2, . . . , 𝐿𝑛).
As a consequence of Equation (1), Lemma 5, Lemma 6 and Lemma 7, it holds:
Proposition 8. Let 𝜙 be a Boolean formula over the alphabet {1, . . . , 𝑛} and
𝐿1, . . . , 𝐿𝑛 be 𝑛 languages. Then
𝜙(𝐿1, . . . , 𝐿𝑛) = 𝐿1 · 𝜙′(𝐿2, . . . , 𝐿𝑛) ∪ 𝜙′′(𝐿2, . . . , 𝐿𝑛),
where
𝜙′ = 𝜙1B⊥,2B1,...,𝑛B𝑛−1
and
𝜙′′ = 𝜙1B⊤,2B1,...,𝑛B𝑛−1.
Proposition 9. Let 𝜙 be a Boolean formula over the alphabet {1, . . . , 𝑛} and
𝐿1, . . . , 𝐿𝑛 be 𝑛 languages. Then
𝜙(𝐿1, . . . , 𝐿𝑛) = 𝜙′(𝐿1, . . . , 𝐿𝑛−1) · 𝐿𝑛 ∪ 𝜙′′(𝐿1, . . . , 𝐿𝑛−1),
where
𝜙′ = 𝜙𝑛B⊥
and
𝜙′′ = 𝜙𝑛B⊤.
Example 10. Let us consider the language of Example 3:
𝐿𝑛 = Mirror𝑛(1, . . . , 2𝑛)({𝑎1}, . . . , {𝑎2𝑛}).
Following Proposition 8, Proposition 9 and rules in Example 2, it holds:
𝐿𝑛 = {𝑎1} · Mirror𝑛−1(1, . . . , 2𝑛 − 2)({𝑎2}, . . . , {𝑎2𝑛−1}) · {𝑎2𝑛}
∪ Mirror𝑛−1(1, . . . , 2𝑛 − 2)({𝑎2}, . . . , {𝑎2𝑛−1}).
This first proposition allows us to show how to easily determine whether the
empty word belongs to the action of a Boolean formula over a language sequence
and how to compute the quotient of such a computation w.r.t. a symbol.
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
Constrained Multi-Tildes: Derived Term and Position Automata
7
Corollary 11. Let 𝜙 be a Boolean formula over {1, . . . , 𝑛} and 𝐿1, . . . , 𝐿𝑛 be 𝑛
languages. Then:
𝜀 ∈ 𝜙(𝐿1, . . . , 𝐿𝑛) ⇔ 𝜀 ∈ 𝐿1 ∧ 𝜀 ∈ 𝜙′(𝐿2, . . . , 𝐿𝑛) ∨ 𝜀 ∈ 𝜙′′(𝐿2, . . . , 𝐿𝑛)
where
𝜙′ = 𝜙1B⊥,2B1,...,𝑛B𝑛−1
and
𝜙′′ = 𝜙1B⊤,2B1,...,𝑛B𝑛−1.
Corollary 12. Let 𝜙 be a Boolean formula over the alphabet {1, . . . , 𝑛}, 𝐿1, . . . , 𝐿𝑛
be 𝑛 languages and 𝑎 be a symbol. Then:
𝑎−1(𝜙(𝐿1, . . . , 𝐿𝑛)) = 𝑎−1(𝐿1) · 𝜙′(𝐿2, . . . , 𝐿𝑛)
∪ (𝑎−1(𝜙′(𝐿2, . . . , 𝐿𝑛)) | 𝜀 ∈ 𝐿1)
∪ 𝑎−1(𝜙′′(𝐿2, . . . , 𝐿𝑛)),
where
𝜙′ = 𝜙1B⊥,2B1,...,𝑛B𝑛−1
and
𝜙′′ = 𝜙1B⊤,2B1,...,𝑛B𝑛−1.
Let us now extend classical regular expressions with the action of a Boolean
formula considered as a constrained Multi-Tildes.
An extended to constrained multi-tildes expression 𝐸 over an alphabet Σ (ex-
tended expression in the following) is inductively defined by
𝐸 = 𝑎,
𝐸 = ∅,
𝐸 = 𝜀,
𝐸 = 𝐸1 + 𝐸2,
𝐸 = 𝐸1 · 𝐸2,
𝐸 = 𝐸∗
1,
𝐸 = 𝜙(𝐸1, . . . , 𝐸𝑛),
where 𝑎 is a symbol in Σ, 𝜙 is a Boolean formula over the alphabet {1, . . . , 𝑛} and
𝐸1, . . . , 𝐸𝑛 are 𝑛 extended expressions over Σ. The language denoted by an extended
expression 𝐸 is the language 𝐿(𝐸) inductively defined by
𝐿(𝑎) = {𝑎},
𝐿(∅) = ∅,
𝐿(𝜀) = {𝜀},
𝐿(𝐸1 + 𝐸2) = 𝐿(𝐸1) ∪ 𝐿(𝐸2),
𝐿(𝐸1 · 𝐸2) = 𝐿(𝐸1) · 𝐿(𝐸2),
𝐿(𝐸∗
1) = 𝐿(𝐸1)∗,
𝐿(𝜙(𝐸1, . . . , 𝐸𝑛)) = 𝜙(𝐿(𝐸1), . . . , 𝐿(𝐸𝑛)),
where 𝑎 is a symbol in Σ, 𝜙 is a Boolean formula over the alphabet {1, . . . , 𝑛} and
𝐸1, . . . , 𝐸𝑛 are 𝑛 extended expressions over Σ.
Since the Boolean satisfiability is an NP-hard problem [11], so is the emptiness
problem for extended expressions, as a direct consequence of Equation (2) and of
denoted language definition.
Proposition 13. Let Σ𝑘 = {𝑎1, . . . , 𝑎𝑘} be an alphabet and 𝜙 be a Boolean formula
over the alphabet {1, . . . , 𝑘}. Then 𝐿(𝜙(𝑎1, . . . , 𝑎𝑘)) ≠ ∅ ⇐⇒ 𝜙 is satisfiable.
Corollary 14. Determining whether the language denoted by an extended expres-
sion is empty is NP-hard.
5. Factorization Power
In this section, we exhibit a parameterized family of expressions 𝐸𝑛 such that the
smallest NFA recognizing 𝐿(𝐸𝑛) admits a number of states exponentially larger than
the sum of the number of symbols, the number of atoms and the number of operators
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
8
S.Attou et al.
of 𝐸𝑛. Let us consider the alphabet Σ2𝑛 = {𝑎1, . . . , 𝑎2𝑛} and the expression 𝐸𝑛 =
Mirror𝑛(1, . . . , 2𝑛)(𝑎1, . . . , 𝑎2𝑛). The expression 𝐸𝑛 contains 2𝑛 atoms, 2𝑛 symbols
and 1 operator. Using classical Boolean operators, like ∧, ∨ and ¬, the Boolean
formula Mirror𝑛(1, . . . , 2𝑛) can be turned into the equivalent one (1∧2𝑛∨¬1∧¬2𝑛)∧
· · · ∧ (𝑛 ∧ (𝑛 + 1) ∨ ¬𝑛 ∧ ¬(𝑛 + 1)) following equation in Example 1, that contains 4𝑛
atoms and (6𝑛 − 1) operators, which is a linearly larger Boolean formula. In order
to exhibit a lower bound of the number of states of any NFA recognizing 𝐿(𝐸𝑛), let
us consider the following property [14]:
Theorem 15 ( [14]) Let 𝐿 ⊂ Σ∗ be a regular language, and suppose there exists a
set of pairs 𝑃 = {(𝑥𝑖, 𝑤𝑖) : 1 ≤ 𝑖 ≤ 𝑛} such that 𝑥𝑖𝑤𝑖 ∈ 𝐿 for 1 ≤ 𝑖 ≤ 𝑛 and 𝑥𝑖𝑤 𝑗 ∉ 𝐿
for 1 ≤ 𝑖, 𝑗 ≤ 𝑛 and 𝑖 ≠ 𝑗. Then any NFA accepting 𝐿 has at least 𝑛 states.
For any sequences of 𝑛 Booleans 𝑏𝑠 = (𝑏1, . . . , 𝑏𝑛), let us consider the words
𝑣𝑏𝑠 = 𝑤1 · · · 𝑤𝑛 and 𝑣′
𝑏𝑠 = 𝑤′
1 · · · 𝑤′
𝑛 where
𝑤𝑘 =
(
𝑎𝑘
if ¬𝑏𝑘,
𝜀
otherwise,
𝑤′
𝑘 =
(
𝑎𝑛+𝑘
if ¬𝑏𝑘,
𝜀
otherwise.
(3)
Denoting by rev(𝑏1, . . . , 𝑏𝑛) the sequence (𝑏𝑛, . . . , 𝑏1), since the only words in
𝐿(𝐸𝑛) are the words 𝑣𝑏𝑠 ·𝑣′
rev(𝑏𝑠), and since the words 𝑣𝑏𝑠 ·𝑣′
𝑏𝑠′ for any 𝑏𝑠′ ≠ rev(𝑏𝑠)
are not in 𝐿(𝐸𝑛), it holds according Theorem 15 that
Proposition 16. There is at least 2𝑛 states in any automaton recognizing 𝐿(𝐸𝑛).
Theorem 17. There exist extended regular expressions exponentially smaller than
any automaton recognizing their denoted languages.
6. Partial Derivatives and Automaton Computation
Let us now show how to extend the Antimirov method in order to syntactically solve
the membership test and to compute a finite automaton recognizing the language
denoted by an extended expression. First, we define the partial derivative of an
expression w.r.t. a symbol, where the derivation formula for the action of a Boolean
combination is obtained by considering the fact that the empty word may appear
at the first position for two reasons: if the first operand is nullable, or because the
empty word is inserted by the multi-tilde.
Definition 18. Let 𝐸 be an extended expression and 𝑎 be a symbol. The partial
derivative of 𝐸 w.r.t. 𝑎 is the set 𝛿𝑎(𝐸) of extended expressions inductively defined
as follows:
𝛿𝑎(𝑏) = {𝜀} | 𝑏 = 𝑎,
𝛿𝑎(𝜀) = ∅,
𝛿𝑎(∅) = ∅,
𝛿𝑎(𝐸1 + 𝐸2) = 𝛿𝑎(𝐸1) ∪ 𝛿𝑎(𝐸2),
𝛿𝑎(𝐸1 · 𝐸2) = 𝛿𝑎(𝐸1) ⊙ 𝐸2 ∪ 𝛿𝑎(𝐸2) | 𝜀 ∈ 𝐿(𝐸1),
𝛿𝑎(𝐸∗
1) = 𝛿𝑎(𝐸1) ⊙ 𝐸∗
1,
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
Constrained Multi-Tildes: Derived Term and Position Automata
9
𝛿𝑎(𝜙(𝐸1, . . . , 𝐸𝑛)) = 𝛿𝑎(𝐸1) ⊙ 𝜙′(𝐸2, . . . , 𝐸𝑛)
∪ 𝛿𝑎(𝜙′(𝐸2, . . . , 𝐸𝑛)) | 𝜀 ∈ 𝐿(𝐸1)
∪ 𝛿𝑎(𝜙′′(𝐸2, . . . , 𝐸𝑛)),
where 𝑏 is a symbol in Σ, 𝜙 is a Boolean formula over the alphabet {1, . . . , 𝑛},
𝐸1, . . . , 𝐸𝑛 are 𝑛 extended expressions over Σ and
𝜙′ = 𝜙1B⊥,2B1,...,𝑛B𝑛−1,
𝜙′′ = 𝜙1B⊤,2B1,...,𝑛B𝑛−1.
In the following, to shorten the expressions in the next examples, we consider
the trivial quotients 𝐸 · 𝜀 = 𝜀 · 𝐸 = 𝐸 and 𝐸 · ∅ = ∅ · 𝐸 = ∅. Furthermore, when 𝜙 is
a contradiction, we consider that 𝜙(𝐸1, . . . , 𝐸𝑛) = ∅.
Example 19. Let us consider the alphabet Σ = {𝑎, 𝑏} and the expression 𝐸 =
Mirror2(1, 2, 3, 4)(𝑎+, 𝑏+, 𝑎+, 𝑏+), where 𝑥+ = 𝑥 · 𝑥∗. The derived terms of 𝐸 w.r.t.
the symbols in Σ are the following, where underlined computations equal ∅:
𝛿𝑎(𝐸) = 𝛿𝑎(𝑎+) ⊙ Mirror2(⊥, 1, 2, 3)(𝑏+, 𝑎+, 𝑏+) ∪ 𝛿𝑎(Mirror2(⊤, 1, 2, 3)(𝑏+, 𝑎+, 𝑏+))
= {𝑎∗} ⊙ (Mirror1(1, 2) ∧ ¬3)(𝑏+, 𝑎+, 𝑏+)
= {𝑎∗ · (Mirror1(1, 2) ∧ ¬3)(𝑏+, 𝑎+, 𝑏+)},
𝛿𝑏(𝐸) = 𝛿𝑏(𝑎+) ⊙ Mirror2(⊥, 1, 2, 3)(𝑏+, 𝑎+, 𝑏+) ∪ 𝛿𝑏(Mirror2(⊤, 1, 2, 3)(𝑏+, 𝑎+, 𝑏+))
= 𝛿𝑏((Mirror1(1, 2) ∧ 3)(𝑏+, 𝑎+, 𝑏+))
= 𝛿𝑏(𝑏+) ⊙ (Mirror1(⊥, 1) ∧ 2)(𝑎+, 𝑏+) ∪ 𝛿𝑏((Mirror1(⊤, 1) ∧ 2)(𝑎+, 𝑏+))
= {𝑏∗} ⊙ (¬1 ∧ 2)(𝑎+, 𝑏+) = {𝑏∗ · (¬1 ∧ 2)(𝑎+, 𝑏+)}.
As usual, the partial derivative is closely related to the computation of the quotient
of the denoted language, as a direct consequence of Corollary 12, and by induction
over the structure of 𝐸.
Proposition 20. Let 𝐸 be an extended expression and 𝑎 be a symbol. Then
Ø
𝐸′∈ 𝛿𝑎 (𝐸)
𝐿(𝐸′) = 𝑎−1(𝐿(𝐸)).
The partial derivative can be classically extended from symbols to words by repeated
applications. Let 𝐸 be an extended expression, 𝑎 be a symbol and 𝑤 be a word.
Then
𝛿𝜀(𝐸) = {𝐸},
𝛿𝑎𝑤(𝐸) =
Ø
𝐸′∈ 𝛿𝑎 (𝐸)
𝛿𝑤(𝐸′).
Example 21. Let us consider the expression 𝐸 and its derived terms computed in
Example 19. Then:
𝛿𝑎𝑎(𝐸) = 𝛿𝑎(𝑎∗ · (Mirror1(1, 2) ∧ ¬3)(𝑏+, 𝑎+, 𝑏+)) = {𝑎∗ · (Mirror1(1, 2) ∧ ¬3)(𝑏+, 𝑎+, 𝑏+)},
𝛿𝑎𝑏(𝐸) = 𝛿𝑏(𝑎∗ · (Mirror1(1, 2) ∧ ¬3)(𝑏+, 𝑎+, 𝑏+))
= 𝛿𝑏((Mirror1(1, 2) ∧ ¬3)(𝑏+, 𝑎+, 𝑏+))
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
10
S.Attou et al.
= 𝛿𝑏(𝑏+) ⊙ (Mirror1(⊥, 1) ∧ ¬2)(𝑎+, 𝑏+) ∪ 𝛿𝑏((Mirror1(⊤, 1) ∧ ¬2)(𝑎+, 𝑏+))
= {𝑏∗} ⊙ (¬1 ∧ ¬2)(𝑎+, 𝑏+) ∪ 𝛿𝑏((1 ∧ ¬2)(𝑎+, 𝑏+))
= {𝑏∗ · (¬1 ∧ ¬2)(𝑎+, 𝑏+)} ∪ 𝛿𝑏((¬1)(𝑏+))
= {𝑏∗ · (¬1 ∧ ¬2)(𝑎+, 𝑏+), 𝑏∗}.
Once again, this operation is a syntactical representation of the quotient computa-
tion, as a direct consequence of Proposition 20, and by induction over the structure
of words.
Proposition 22. Let 𝐸 be an extended expression and 𝑤 be a word. Then
Ø
𝐸′∈ 𝛿𝑤 (𝐸)
𝐿(𝐸′) = 𝑤−1(𝐿(𝐸)).
As a direct consequence, the membership test is solved for extended expressions.
Indeed, determining whether a word 𝑤 belongs to the language denoted by an
extended expression 𝐸 can be performed by computing the partial derivative of
𝐸 w.r.t. 𝑤 and then by testing whether it contains a nullable expression, i.e. an
expression whose denoted language contains the empty word.
Let us now show that the partial derivative automaton of an extended expression
𝐸 is a finite one that recognizes 𝐿(𝐸).
In the following, we denote by D𝐸 the set of derived terms of an expression 𝐸,
i.e., the set of expressions
Ø
𝑤∈Σ∗
𝛿𝑤(𝐸).
Moreover, given an expression 𝜙(𝐸1, . . . , 𝐸𝑛), an integer 1 ≤ 𝑘 ≤ 𝑛 −1 and an in-
terpretation 𝑖 in {1, . . . , 𝑘} → B, we denote by D𝐸,𝑘,𝑖 the set D𝐸𝑘 ⊙𝜙′(𝐸𝑘+1, . . . , 𝐸𝑛),
where
𝜙′ = 𝜙
1B


⊤
if 𝑖(1),
⊥
otherwise,
...,𝑘B


⊤
if 𝑖(𝑘),
⊥
otherwise,
𝑘+1B1,...,𝑛B𝑛−𝑘
.
First, the union of these sets includes the partial derivatives and is stable w.r.t.
derivation by a symbol, by induction over the structures of extended expressions,
of words and over the integers.
Proposition 23. Let 𝐸 be an extended expression and 𝑎 be a symbol. Then the two
following conditions hold:
(1) 𝛿𝑎(𝐸) ⊂
Ø
1≤𝑘≤𝑛,
𝑖∈{1,...,𝑘}→B
D𝐸,𝑘,𝑖,
(2)
Ø
𝐸′∈D𝐸,𝑘,𝑖
𝛿𝑎(𝐸′) ⊂
Ø
𝑘≤𝑘′≤𝑛,
𝑖′∈{1,...,𝑘′}→B
D𝐸,𝑘′,𝑖′.
As a direct consequence, the set of the derived terms of an extended expression is
included in the union of the D𝐸,𝑘,𝑖 sets.
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
Constrained Multi-Tildes: Derived Term and Position Automata
11
Corollary 24. Let 𝐸 be an extended expression. Then
D𝐸 ⊂
Ø
1≤𝑘≤𝑛,
𝑖∈{1,...,𝑘}→B
D𝐸,𝑘,𝑖.
According to a trivial inductive reasoning, one can show that such a set is finite.
Corollary 25. Let 𝐸 be an extended expression and 𝑤 be a word. Then
Ø
𝑤∈Σ∗
𝛿𝑤(𝐸) is a finite set.
As a direct consequence, the derived term automaton of an extended expression,
defined as usual with derived terms as states and transitions computed from partial
derivation, fulfils finiteness and correction.
Theorem 26. Let 𝐸 be an extended expression and 𝑎 be a symbol. The partial
derivative automaton of 𝐸 is a finite automaton recognizing 𝐿(𝐸).
Example 27. Let us consider the expression 𝐸 defined in Example 19. The derived
term automaton of 𝐸 is given in Figure 2.
𝐸
𝑏∗ · (¬1 ∧ 2) (𝑎+, 𝑏+)
𝑎∗ · 1(𝑏+)
𝑎∗ · (Mirror1(1, 2) ∧ ¬3) (𝑏+, 𝑎+, 𝑏+)
𝑏∗ · (¬1 ∧ ¬2) (𝑎+, 𝑏+)
𝑎∗ · (¬1) (𝑏+)
𝑏∗
𝑎
𝑎
𝑏
𝑏
𝑎
𝑎
𝑏
𝑏
𝑏
𝑏
𝑏
𝑎
𝑎
Fig. 2. The derived term automaton of 𝐸.
7. The Glushkov Automaton of an Expression
The Glushkov automaton [15] is a convenient way to produce an (𝑛+1)-state automa-
ton from a 𝑛-width regular expression. Naively, this automaton can be computed in
𝑂(𝑛3) time, but this complexity can be reduced to 𝑂(𝑛2) [2,18].
This complexity is also the best known one for the computation of the derived
term automaton using optimized techniques [10], reduced from the naive case in
𝑂(𝑛5) [10].
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
12
S.Attou et al.
If the derivation technique allows us to solve the membership test without com-
puting the whole derived term automaton, it is not the case for the Glushkov au-
tomaton. Its whole structure is computed inductively by five functions, Pos, First,
Last, Follow and Null. However, its structure has been deeply studied and charac-
terized [9].
Consequently, the extension of the Glushkov method to constrained tildes and
the characterization of its structure is a preliminary step to study the conversion
from automata to extended expressions.
7.1. The Computation for (Classical) Regular Expressions
As recalled before, the computation is based on five functions called position func-
tions, defined for any regular expression 𝐸 as follows:
Pos(𝐸) = Σ𝐸,
First(𝐸) = {𝑎 ∈ Pos(𝐸) | ∃𝑤, 𝑎𝑤 ∈ 𝐿(𝐸)},
Last(𝐸) = {𝑎 ∈ Pos(𝐸) | ∃𝑤, 𝑤𝑎 ∈ 𝐿(𝐸)},
Follow(𝐸) = {(𝑎, 𝑏) ∈ Pos(𝐸)2 | ∃(𝑤, 𝑤′), 𝑤𝑎𝑏𝑤′ ∈ 𝐿(𝐸)},
Null(𝐸) = 𝜀 ∈ 𝐿(𝐸),
where Σ𝐸 is the set of symbols that appears in 𝐸.
As far as there is no occurrence of ∅ in 𝐸, these functions can be inductively
computed over the structure of 𝐸. As an example, the Follow function is inductively
computed as follows:
Follow(𝑎) = Follow(𝜀) = ∅,
Follow(𝐸 + 𝐹) = Follow(𝐸) ∪ Follow(𝐹),
Follow(𝐸 · 𝐹) = Follow(𝐸) ∪ Follow(𝐹) ∪ Last(𝐸) × First(𝐹),
Follow(𝐸∗) = Follow(𝐸) ∪ Last(𝐸) × First(𝐸).
Once computed, these functions lead to the definition of the Glushkov automaton.
Definition 28. The Glushkov automaton of a regular expression 𝐸 is the automa-
ton 𝐺𝐸 = (Pos(𝐸), Pos(𝐸) ⊎ {0}, {0}, 𝐹, 𝛿) defined by
𝐹 = Last(𝐸) ∪ {0} | Null(𝐸),
𝛿(𝑎, 0) = {𝑎} | 𝑎 ∈ First(𝐸),
𝛿(𝑎, 𝑝) = {𝑏} | (𝑎, 𝑏) ∈ Follow(𝐸),
where 0 is not in Pos(𝐸) and 𝑝 is any state distinct from 0.
However, this automaton does not necessarily recognize 𝐿(𝐸). Indeed, if a symbol
appears twice or more in 𝐸, the occurrences may accept distinct following symbols
in the denoted language, or can be in a last position or not.
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
Constrained Multi-Tildes: Derived Term and Position Automata
13
As an example, let us consider the expression 𝐸 = (𝑎 + 𝑏)∗𝑎(𝑎 + 𝑏). The first
occurrence of the symbol 𝑏 makes it belonging to First(𝐸), the second one makes it
belonging to Last(𝐸). Therefore, by construction, 𝑏 is in 𝐿(𝐺𝐸), but not in 𝐿(𝐸).
A sufficient condition is when 𝐸 is linear, i.e. when any symbol appears only
once in 𝐸. In this case, 𝐿(𝐸) = 𝐿(𝐺𝐸). The position functions do not mix the data
obtained from several positions, since any symbol appear only at one position.
If 𝐸 is not linear, the position automaton is produced as follows:
(1) the expression 𝐸 is linearized by indexing distinctively the occurrences of
the symbols producing an expression denoted by 𝐸♯; as an example, if
𝐸 = (𝑎 + 𝑏)∗𝑎(𝑎 +𝑏), then 𝐸♯ = (𝑎1 + 𝑏2)∗𝑎3(𝑎4 +𝑏5). N.B.: Notice that the
starting index of the linearization and the order involved do not matter.
All we care about is having distinct indices.
(2) The automaton 𝐺𝐸♯ is then computed as before.
(3) The position automaton 𝐺𝐸 of 𝐸 is then obtained by relabelling the tran-
sitions of 𝐺𝐸♯ with unindexed symbols. More formally, the delinearization
function h, sending any indexed symbol 𝑎 𝑗 to the symbol 𝑎, is applied over
the transitions labels.
The automaton 𝐺𝐸 recognizes 𝐿(𝐸). It is a consequence of the fact that
• 𝐿(𝐸♯) is equal to 𝐿(𝐺𝐸♯) by construction,
• h(𝐿(𝐸♯)) is equal to 𝐿(𝐸),
• h(𝐿(𝐺𝐸♯)) is equal to 𝐿(𝐺𝐸)
where h is linearly extended to sets and as a (free) monoid morphism over words.
Finally, extending the inductive computation to expressions with occurrences of
∅, by setting
Pos(∅) = First(∅) = Last(∅) = Follow(∅) = ∅
preserves the correction of the computation (but may lead to not accessible states
or to not coaccessible states).
Example 29. Let us consider the expression 𝐸 = (𝑎 + 𝑏)∗𝑎(𝑎 + 𝑏) and its linearized
version 𝐸♯ = (𝑎1 + 𝑏2)∗𝑎3(𝑎4 + 𝑏5). The associated position functions produced the
following sets:
Null(𝐸♯) = false,
Pos(𝐸♯) = {𝑎1, 𝑏2, 𝑎3, 𝑎4, 𝑏5},
First(𝐸♯) = {𝑎1, 𝑏2, 𝑎3},
Last(𝐸♯) = {𝑎4, 𝑏5},
Follow(𝐸♯) = {(𝑎1, 𝑎1), (𝑎1, 𝑏2), (𝑎1, 𝑎3), (𝑏2, 𝑎1),
(𝑏2, 𝑏2), (𝑏2, 𝑎3), (𝑎3, 𝑏4), (𝑎3, 𝑏5)},
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
14
S.Attou et al.
leading to the automaton in Figure 3.
0
𝑎1
𝑏2
𝑎3
𝑎4
𝑏5
𝑎
𝑏
𝑎
𝑎
𝑏
𝑎
𝑏
𝑎
𝑎
𝑎
𝑏
Fig. 3. The Glushkov automaton of 𝐸.
7.2. Construction for Constrained Tildes
First, let us show that the linearization process is compatible with the constrained
tildes. For that purpose, let us consider the operation 𝐸♯, 𝑗 starting the linearization
at the index 𝑗. By convenience, we usually state that 𝐸♯ = 𝐸♯,1.
Lemma 30. Let 𝐸 be an extended expression. Then
𝐿(𝐸) = h(𝐿(𝐸♯)).
Proof. Let us show by induction the more general statement that
𝐿(𝐸) = h(𝐿(𝐸♯, 𝑗)).
We only exhibit the case of the sum and of a constrained tilde, the other case being
equivalently provable.
• If 𝐸 = 𝐹 + 𝐺, then by definition there exist two integers ( 𝑗1, 𝑗2) such that
𝐸♯ = 𝐹♯, 𝑗1 + 𝐺♯, 𝑗2.
𝐿(𝐹 + 𝐺) = 𝐿(𝐹) ∪ 𝐿(𝐺)
(Def.: language of a sum)
= h(𝐿(𝐹♯, 𝑗1)) ∪ h(𝐿(𝐺♯, 𝑗2))
(Induction Hypothesis)
= h(𝐿(𝐹♯, 𝑗1) ∪ 𝐿(𝐺♯, 𝑗2))
(linearity of h)
= 𝐿(𝐹♯, 𝑗1 + 𝐺♯, 𝑗2)
(Def.: language of a sum)
• If 𝐸 = 𝜙(𝐸1, . . . , 𝐸𝑛), then by definition there exist 𝑛 integers ( 𝑗1, . . . , 𝑗𝑛)
such that 𝐸♯ = 𝜙(𝐸1♯, 𝑗1, . . . , 𝐸𝑛♯, 𝑗𝑛).
𝐿(𝐸) =
Ø
𝑖|eval𝑖 (𝜙)
𝑖(𝐿(𝐸1), . . . , 𝐿(𝐸𝑛))
(Eq. (2))
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
Constrained Multi-Tildes: Derived Term and Position Automata
15
=
Ø
𝑖|eval𝑖 (𝜙)
𝐿′
𝑖,1 · · · 𝐿′
𝑖,𝑛
where 𝐿′
𝑖,𝑘 =
(
{𝜀}
if 𝑖(𝑘),
𝐿(𝐸𝑘)
otherwise.
(Def. of action of 𝑖)
=
Ø
𝑖|eval𝑖 (𝜙)
𝐿′
𝑖,1 · · · 𝐿′
𝑖,𝑛
where 𝐿′
𝑖,𝑘 =
(
{𝜀}
if 𝑖(𝑘),
h(𝐿(𝐸𝑘♯, 𝑗𝑘))
otherwise.
(Induction hypothesis)
=
Ø
𝑖|eval𝑖 (𝜙)
h(𝐿′
𝑖,1 · · · 𝐿′
𝑖,𝑛)
where 𝐿′
𝑖,𝑘 =
(
{𝜀}
if 𝑖(𝑘),
𝐿(𝐸𝑘♯, 𝑗𝑘)
otherwise.
(h: monoid morphism)
= h(
Ø
𝑖|eval𝑖 (𝜙)
𝐿′
𝑖,1 · · · 𝐿′
𝑖,𝑛)
where 𝐿′
𝑖,𝑘 =
(
{𝜀}
if 𝑖(𝑘),
𝐿(𝐸𝑘♯, 𝑗𝑘)
otherwise.
(linearity of h)
= h(
Ø
𝑖|eval𝑖 (𝜙)
𝑖(𝐿(𝐸1♯, 𝑗1), . . . , 𝐿(𝐸𝑛♯, 𝑗𝑛)))
(Eq. (2))
= 𝐿(𝐸♯)
However, the linearity of an extended expression is not sufficient anymore in the
case of constrained tildes to ensure the correction of the Glushkov construction.
Example 31.
Let us consider the extended expression 𝐸2 = Mirror2(1, 2, 3, 4)(𝑎, 𝑏, 𝑎, 𝑏), its lin-
earized version 𝐸2♯ = Mirror2(1, 2, 3, 4)(𝑎1, 𝑏2, 𝑎3, 𝑏4) and their denoted languages
𝐿(𝐸2) = {𝜀, 𝑎𝑏, 𝑏𝑎, 𝑎𝑏𝑎𝑏},
𝐿(𝐸2♯) = {𝜀, 𝑎1𝑏4, 𝑏2𝑎3, 𝑎1𝑏2𝑎3𝑏4}.
The symbol 𝑎3 is in the set Last(𝐸2♯) since it ends the word 𝑏2𝑎3. However, following
the Glushkov computation of the associated automaton, the state 𝑎3 should be final.
And since the word 𝑎1𝑏2𝑎3𝑏4 belongs to 𝐿(𝐸2♯), its prefix should be a path in the
position automaton, leading to the word 𝑎1𝑏2𝑎3 be recognized by the automaton,
without belonging to the denoted language of 𝐸2♯.
The action of the tildes restricts the following symbols of a given position w.r.t. to a
context, provided by the interpretations satisfying the involved Boolean formula. It
can also contextualize the finality of a position. Consequently, the context defined by
the different interpretations satisfying the Boolean formula of a constrained tilde
needs to be explicitly considered in the states of the position automaton, i.e. in
the definition of the function Pos, and therefore in all the position functions. In
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
16
S.Attou et al.
order to complete this contextualization, we need to surlinearize the positions by
adding other indices to represent the considered interpretations. N.B.: As a direct
consequence, the number of positions is not linear (w.r.t. the size if the expression)
anymore, and can be exponentially related.
Example 32. Let us consider the Boolean formula Mirror2(1, 2, 3, 4). There are
four interpretations satisfying it:
• 𝑖1, when the four atoms are false,
• 𝑖2, when 1 and 4 are true and the other atoms are false,
• 𝑖3, when 2 and 3 are true and the other atoms are false,
• 𝑖4, when the four atoms are true.
Consequently, following Equation (2), the expression 𝐸2♯ of Example 31 is equivalent
to the expression
𝐸′
2
♯ = 𝜀 + 𝑎1𝑏4 + 𝑎2𝑏3 + 𝑎1𝑏2𝑎3𝑏4,
since
𝐿(𝐸2♯) =
Ø
𝑖∈{𝑖1,𝑖2,𝑖3,𝑖4}
𝑖({𝑎1}, {𝑏2}, {𝑎3}, {𝑏4}).
A surlinearization can be performed by applying the index of the interpretation over
each part of 𝐸′
2
♯, leading to the expression
𝐸2] = 𝜀 + 𝑎1,2𝑏4,2 + 𝑎2,3𝑏3,3 + 𝑎1,4𝑏2,4𝑎3,4𝑏4,4.
Notice that this surlinearization may happen for several nested constrained tildes.
Consequently, the surlinearization will not produce an index based on a couple of
integers, but an index based on a couple of an integer and an integer list, where each
step adds an integer in the list. As an example, the surlinearization would produce
the extended expression
𝐸2] = 𝜀 + 𝑎1,[2]𝑏4,[2] + 𝑎2,[3]𝑏3,[3] + 𝑎1,[4]𝑏2,[4]𝑎3,[4]𝑏4,[4]
in Example 32. Therefore, from now, we consider that the linearization of an ex-
pression indexes each occurrence of an expression by a couple (𝑘, []), where 𝑘 is an
integer and [] the empty list.
However, we will not surlinearize a linear expression before the computation of
the position functions. Instead, we surlinearize it during the computation, whenever
a constrained tilde is reached, by considering the following computation. This can be
efficient for the implementation, as far as lazy evaluated languages are considered.
Definition 33. Let 𝜙(𝐸1, . . . , 𝐸𝑛) be a linearized constrained tildes expression and
{𝑖1, . . . , 𝑖𝑘} the set of the interpretations satisfying 𝜙. We denote by dev𝜙(𝐸1, . . . , 𝐸𝑛)
the extended expression Í
𝑗∈{1,...,𝑘} dev𝑖𝑗 (𝐸1, . . . , 𝐸𝑛), where dev𝑖𝑗 (𝐸1, . . . , 𝐸𝑛) =
Î
1≤𝑚≤𝑛|𝑖𝑗 (𝑚) ind𝑗 (𝐸𝑚) where ind𝑗 (𝐸𝑚) is the extended expression obtained by sub-
stituting any occurrence of 𝑎𝑘,[𝑐1,...,𝑐𝑙 ] with 𝑎𝑘,[ 𝑗,𝑐1,...,𝑐𝑙 ].
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
Constrained Multi-Tildes: Derived Term and Position Automata
17
Example 34. Let us consider the expression 𝐸 = |1 ↔ 3|(𝑎, (|1 → 2|(𝑏, 𝑐)), 𝑑) and
its linearized version 𝐸♯ = |1 ↔ 3|(𝑎1,[], (|1 → 2|(𝑏2,[], 𝑐3,[])), 𝑑4,[]).
dev|1↔3| (𝑎1, (|1 → 2|(𝑏2, 𝑐3)), 𝑑4) = 𝜀
+ (|1 → 2|(𝑏2,[2], 𝑐3,[2]))
+ 𝑎1,[3] · 𝑑4,[3]
+ 𝑎1,[4] · (|1 → 2|(𝑏2,[4], 𝑐3,[4])) · 𝑑4,[4]
dev|1→2| (𝑏2,[2], 𝑐3,[2]) = 𝜀
+ 𝑏2,[2,2]
+ 𝑏2,[3,2] · 𝑐3,[3,2]
dev|1→2| (𝑏2,[4], 𝑐3,[4]) = 𝜀
+ 𝑏2,[2,4]
+ 𝑏2,[3,4] · 𝑐3,[3,4]
This transformation preserves the delinearized language, since it is based on the
development associated with the language of a constrained tilde where the only
modification made is adding an index to the list, that does not modify the action
of h.
Lemma 35. Let 𝜙(𝐸1, . . . , 𝐸𝑛) be a linearized constrained tilde expression. Then
h(𝐿(𝜙(𝐸1♯, 𝑗, . . . , 𝐸𝑛♯, 𝑗𝑛))) = h(𝐿(dev𝜙(𝐸1♯, 𝑗, . . . , 𝐸𝑛♯, 𝑗𝑛))).
Using this operation, the position functions are extended to constrained tildes as
follows.
Definition 36. Let 𝜙(𝐸1, . . . , 𝐸𝑛) be a linearized constrained tilde expression. Then
Pos(E) = Pos(dev𝜙(𝐸1, . . . , 𝐸𝑛)),
First(E) = First(dev𝜙(𝐸1, . . . , 𝐸𝑛)),
Last(E) = Last(dev𝜙(𝐸1, . . . , 𝐸𝑛)),
Follow(E) = Follow(dev𝜙(𝐸1, . . . , 𝐸𝑛)).
The Null function can be computed inductively following Corollary 11.
Example 37. Let us consider the expression 𝐸♯ of Example 34. Then:
Pos(𝐸♯) = {(𝑎, 1, [3]), (𝑎, 1, [4]), (𝑏, 2, [2, 2]), (𝑏, 2, [2, 4]), (𝑏, 2, [3, 2]),
(𝑏, 2, [3, 4]), (𝑐, 3, [3, 2]), (𝑐, 3, [3, 4]), (𝑑, 4, [3]), (𝑑, 4, [4])},
First(𝐸♯) = {(𝑎, 1, [3]), (𝑎, 1, [4]), (𝑏, 2, [2, 2]), (𝑏, 2, [3, 2])},
Last(𝐸♯) = {(𝑏, 2, [2, 2]), (𝑐, 3, [3, 2]), (𝑑, 4, [3]), (𝑑, 4, [4])}
Follow(𝐸♯) = {((𝑎, 1, [3]), (𝑑, 4, [3]))} ∪ {(𝑎, 1, 4)} × {(𝑏, 2, 2, 4), (𝑏, 2, 3, 4), (𝑑, 4, 4)}
∪ {((𝑏, 2, [2, 4]), (𝑑, 4, [4])), ((𝑏, 2, [3, 2]), (𝑐, 3, [3, 2])),
((𝑏, 2, [3, 4]), (𝑐, 3, [3, 4])), ((𝑐, 3, [3, 4]), (𝑑, 4, [4]))}.
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
18
S.Attou et al.
The classical construction, detailed in Subsection 7.1, can then be applied to any
extended expression 𝐸:
(1) the expression 𝐸 is linearized by indexing distinctively the occurrences of
the symbols with a couple made of an integer and the empty list, producing
an expression denoted by 𝐸♯.
(2) The position functions are computed using Definition 36.
(3) The automaton 𝐺𝐸♯ is then computed as before following Definition 28;
(4) The position automaton 𝐺𝐸 of 𝐸 is then obtained by relabelling the tran-
sitions of 𝐺𝐸♯ with unindexed symbols.
Example 38. Let us consider the expression 𝐸 of Example 34. The Glushkov au-
tomaton of 𝐸 is presented in Figure 4.
0
𝑏(2,[2,2])
𝑎(1,[3])
𝑑(4,[3])
𝑏(2,[3,2])
𝑐(3,[3,2])
𝑎(1,[4])
𝑏(2,[2,4])
𝑑(4,[4])
𝑏(2,[3,4])
𝑐(3,[3,4])
𝑏
𝑎
𝑑
𝑏
𝑐
𝑎
𝑏
𝑑
𝑏
𝑐
𝑑
𝑑
Fig. 4. The Glushkov automaton of 𝐸.
7.3. Correction of the Construction
In order to prove the correction of the construction, let us consider the surlinearized
expression 𝐸] = dev(𝐸♯) of an extended expression 𝐸 computed inductively as
follows:
dev(𝑎 𝑗, 𝑗𝑠) = 𝑎 𝑗, 𝑗𝑠,
dev(∅) = ∅,
dev(𝜀) = 𝜀,
dev(𝐹 + 𝐺) = dev(𝐹) + dev(𝐺),
dev(𝐹 · 𝐺) = dev(𝐹) · dev(𝐺),
dev(𝐹∗) = dev(𝐹)∗,
dev(𝜙(𝐸1, . . . , 𝐸𝑛)) = dev(dev𝜙(𝐸1, . . . , 𝐸𝑛)).
Notice that by construction, 𝐸] is a regular expression.
Example 39. Let us consider the expression 𝐸 = |1 ↔ 3|(𝑎, (|1 → 2|(𝑏, 𝑐)), 𝑑) and
its linearized version 𝐸♯ = |1 ↔ 3|(𝑎1,[], (|1 → 2|(𝑏2,[], 𝑐3,[])), 𝑑4,[]) of Example 34.
Then:
𝐸] = dev(𝜀
+ (|1 → 2|(𝑏2,[2], 𝑐3,[2]))
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
Constrained Multi-Tildes: Derived Term and Position Automata
19
+ 𝑎1,[3] · 𝑑4,[3]
+ 𝑎1,[4] · (|1 → 2|(𝑏2,[4], 𝑐3,[4])) · 𝑑4,[4])
= 𝜀
+ (𝜀
+ 𝑏2,[2,2]
+ 𝑏2,[3,2] · 𝑐3,[3,2])
+ 𝑎1,[3] · 𝑑4,[3]
+ 𝑎1,[4] · (
𝜀
+ 𝑏2,[2,4]
+ 𝑏2,[3,4] · 𝑐3,[3,4]) · 𝑑4,[4]
By a trivial induction over the structure of extended expressions, it can be shown
that the surlinearization preserves the position functions.
Proposition 40. Let 𝐸 be an extended expression. Then:
Pos(𝐸]) = Pos(𝐸♯),
First(𝐸]) = First(𝐸♯),
Last(𝐸]) = Last(𝐸♯),
Follow(𝐸]) = Follow(𝐸♯),
Null(𝐸]) = Null(𝐸♯).
Proof. Let us prove the case of the First function by induction over the structure of
the expressions and by recurrence over the number of constrained tildes, the other
cases being equivalently provable. As in the proof of Lemma 30, let us show the
more general result that First(dev(𝐸♯, 𝑗)) = First(𝐸♯, 𝑗). Once again, we restrict the
proof to two cases, the other ones being equivalently provable.
• If 𝐸♯, 𝑗 = 𝐹♯, 𝑗 + 𝐺♯, 𝑗2, then
First(𝐸♯, 𝑗) = First(𝐹♯, 𝑗) ∪ First(𝐺♯, 𝑗2)
(Def.: First)
= First(dev(𝐹♯, 𝑗)) ∪ First(dev(𝐺♯, 𝑗2))
(Induction Hypothesis)
= First(dev(𝐹♯, 𝑗) + dev(𝐺♯, 𝑗2))
(Def.: First)
= First(dev(𝐸♯, 𝑗))
(Def.: dev)
• If 𝐸♯, 𝑗 = 𝜙(𝐸1♯, 𝑗, . . . , 𝐸𝑛♯, 𝑗𝑛), then
First(𝐸♯, 𝑗) = First(dev𝜙(𝐸1♯, 𝑗, . . . , 𝐸𝑛♯, 𝑗𝑛))
(Def.: First)
= First(dev(dev𝜙(𝐸1♯, 𝑗, . . . , 𝐸𝑛♯, 𝑗𝑛)))
(Induction Hypothesis)
= First(dev(𝐸♯, 𝑗))
(Def.: dev)
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
20
S.Attou et al.
As a direct consequence, since 𝐸] is a regular expression, the classical Glushkov
construction for regular expressions for 𝐸] coincides with the Glushkov automaton
of the extended expression 𝐸♯.
Corollary 41. Let 𝐸 be an extended expression. Then 𝐺𝐸♯ and 𝐺𝐸] are equal.
This allows us to characterize the language recognized by 𝐺𝐸♯.
Corollary 42. Let 𝐸 be an extended expression. Then 𝐿(𝐺𝐸♯) is equal to 𝐿(𝐸]).
Another important property of surlinearization is that after delinearization of
the languages denoted by 𝐸♯ or 𝐸], using the function h, we obtain the language
of the starting expression.
Proposition 43. Let 𝐸 be an extended expression. Then h(𝐿(𝐸♯)) = h(𝐿(𝐸])).
Proof. We proceed by induction over the structure of the expressions and by re-
currence over the number of constrained tildes. As in the proof of Lemma 30, let
us show the more general result that h(𝐿(dev(𝐸♯, 𝑗))) = h(𝐿(𝐸♯, 𝑗)). Once again, we
restrict the proof to two cases, the other ones being equivalently provable.
• If 𝐸♯, 𝑗 = 𝐹♯, 𝑗 + 𝐺♯, 𝑗2, then
h(𝐿(𝐸♯, 𝑗)) = h(𝐿(𝐹♯, 𝑗) ∪ 𝐿(𝐺♯, 𝑗2))
(Def.: language of sum)
= h(𝐿(𝐹♯, 𝑗)) ∪ h(𝐿(𝐺♯, 𝑗2))
(linearity of h)
= h(𝐿(dev(𝐹♯, 𝑗))) ∪ h(𝐿(dev(𝐺♯, 𝑗2)))
(Induction Hypothesis)
= h(𝐿(dev(𝐹♯, 𝑗)) ∪ 𝐿(dev(𝐺♯, 𝑗2)))
(linearity of h)
= h(𝐿(dev(𝐹♯, 𝑗) + dev(𝐺♯, 𝑗2)))
(Def.: language of sum)
= h(𝐿(dev(𝐸♯, 𝑗)))
(Def.: dev)
• If 𝐸♯, 𝑗 = 𝜙(𝐸1♯, 𝑗, . . . , 𝐸𝑛♯, 𝑗𝑛), then
h(𝐿(𝐸♯, 𝑗)) = h(𝐿(𝜙(𝐸1♯, 𝑗, . . . , 𝐸𝑛♯, 𝑗𝑛)))
(Def.: language of a tilde)
= h(𝐿(dev𝜙(𝐸1♯, 𝑗, . . . , 𝐸𝑛♯, 𝑗𝑛)))
(Lemma 35)
= h(𝐿(dev(dev𝜙(𝐸1♯, 𝑗, . . . , 𝐸𝑛♯, 𝑗𝑛))))
(Induction Hypothesis)
= h(𝐿(dev(𝐸♯, 𝑗)))
(Def.: dev)
In combination with Lemma 30, we obtain the following result.
Corollary 44. Let 𝐸 be an extended expression. Then
𝐿(𝐸) = h(𝐿(𝐸♯)) = h(𝐿(𝐸])).
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
Constrained Multi-Tildes: Derived Term and Position Automata
21
Finally, since after relabelling the transitions of 𝐺𝐸] we obtain an automaton
recognizing h(𝐿(𝐸])), we can determine the language recognized by 𝐺𝐸.
Theorem 45. Let 𝐸 be an extended expression. Then 𝐿(𝐺𝐸) = 𝐿(𝐸).
8. Haskell Implementation
The computations of partial derivatives, of derived term automata and of
Glushkov automata have been implemented in Haskell and is publicly available on
GitHub [17]. Constrained tildes are implemented using dependently typed program-
ming: a Boolean formula encoding a constrained tildes operator uses an alphabet
the size of which cannot be greater than the length of the list of expressions the
formula is applied on. Derived term and Glushkov automata can be graphically
represented using Dot and Graphviz, and converted in PNG. A parser from string
declaration is also included. A web interface is also available, constructed through
reactive functional programming using Reflex and Reflex-Dom [20,21]. Notice that
for implementation consideration, the Boolean formulae atoms start at position 0.
Fig. 5. The Web Interface.
9. Conclusion and Perspectives
In this paper, we have extended (disjunctive) multi-tildes operators to any Boolean
combinations of these tildes, the constrained multi-tildes, and defined their denoted
languages. We have shown that the action of these operators preserves regularity,
that they may lead to exponentially smaller expressions and how to solve the mem-
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
22
S.Attou et al.
bership test by defining the partial derivatives, the (finite) derived term automaton
and the position automaton.
The next step of our plan is to study the conversion of an automaton into an
equivalent expression, by first characterizing the structures of derived term automa-
ton and Glushkov automaton, like it was previously done in the classical regular
case [9,16].
References
[1] V. M. Antimirov, Partial derivatives of regular expressions and finite automaton
constructions, Theor. Comput. Sci. 155(2) (1996) 291–319.
[2] A. Brüggemann-Klein, Regular expressions into finite automata, Theor. Comput. Sci.
120(2) (1993) 197–213.
[3] P. Caron, J. Champarnaud and L. Mignot, Multi-tilde operators and their glushkov
automata, LATA, Lecture Notes in Computer Science 5457, (Springer, 2009), pp.
290–301.
[4] P. Caron, J. Champarnaud and L. Mignot, A new family of regular operators fitting
with the position automaton computation, SOFSEM , Lecture Notes in Computer
Science 5404, (Springer, 2009), pp. 645–655.
[5] P. Caron, J. Champarnaud and L. Mignot, Multi-bar and multi-tilde regular opera-
tors, J. Autom. Lang. Comb. 16(1) (2011) 11–36.
[6] P. Caron, J. Champarnaud and L. Mignot, Partial derivatives of an extended regular
expression, LATA, Lecture Notes in Computer Science 6638, (Springer, 2011), pp.
179–191.
[7] P. Caron, J. Champarnaud and L. Mignot, Multi-tilde-bar derivatives, CIAA, Lecture
Notes in Computer Science 7381, (Springer, 2012), pp. 321–328.
[8] P. Caron, J. Champarnaud and L. Mignot, Multi-tilde-bar expressions and their au-
tomata, Acta Informatica 49(6) (2012) 413–436.
[9] P. Caron and D. Ziadi, Characterization of glushkov automata, Theor. Comput. Sci.
233(1-2) (2000) 75–90.
[10] J. Champarnaud and D. Ziadi, From c-continuations to new quadratic algorithms for
automaton synthesis, Int. J. Algebra Comput. 11(6) (2001) 707–736.
[11] S. A. Cook, The complexity of theorem-proving procedures, STOC, (ACM, 1971),
pp. 151–158.
[12] M. Davis, G. Logemann and D. W. Loveland, A machine program for theorem-
proving, Commun. ACM 5(7) (1962) 394–397.
[13] M. Davis and H. Putnam, A computing procedure for quantification theory, J. ACM
7(3) (1960) 201–215.
[14] I. Glaister and J. O. Shallit, A lower bound technique for the size of nondeterministic
finite automata, Inf. Process. Lett. 59(2) (1996) 75–77.
[15] V. M. Glushkov, The abstract theory of automata, Russian Mathematical Surveys 16
(1961) 1–53.
[16] S. Lombardy and J. Sakarovitch, How expressions can code for automata, RAIRO
Theor. Informatics Appl. 39(1) (2005) 217–237.
[17] L. Mignot, An implementation of constrained tildes expressions: ConstrainedTildes-
Haskell https://github.com/LudovicMignot/ConstrainedTildesHaskell, (2023).
[18] J. Ponty, D. Ziadi and J. Champarnaud, A new quadratic algorithm to convert a
regular expression into an automaton, Workshop on Implementing Automata, Lecture
Notes in Computer Science 1260, (Springer, 1996), pp. 109–119.
January 23, 2024
3:7
WSPC/INSTRUCTION FILE
tildesContraintes
Constrained Multi-Tildes: Derived Term and Position Automata
23
[19] W. V. O. Quine, Methods of logic (Harvard University Press, 1982).
[20] R. Trinkle, reflex https://github.com/reflex-frp/reflex, (2023).
[21] R. Trinkle, reflex-dom https://github.com/reflex-frp/reflex-dom, (2023).
"
"This paper introduces an energy-aware trajectory design for UAV-mounted RISs and UAV-mounted FDRs using the decode-and-forward (DF) protocol, aiming to maximize the network’s minimum rate and enhance user fairness, while taking into consideration the available on-board energy. Moreover, a joint time-division multiple access (TDMA) user scheduling-UAV trajectory optimization problem is formulated, considering the power dynamics of both systems, while assuring that the UAV energy is not depleted mid-air. Finally, simulation results underscore the importance of energy considerations in determining the optimal trajectory and scheduling and provide insights into the performance comparison of UAV-mounted RISs and FDRs in UAV-assisted wireless networks.","Unmanned Aerial Vehicles (UAVs) are emerging as transformative tools in the landscape of future sixth-generation (6G) wireless networks [1], [2]. Specifically, their inherent flexibility allows them to follow optimized trajectories, dynamically adjusting their paths to offer line of sight (LoS) channels ubiquitously. This capability makes them invaluable in scenarios demanding adaptive connectivity solutions, such as bridging connectivity gaps in challenging terrains or enhancing network resilience in disaster-stricken areas [1], [3]. Furthermore, their ability to provide on-demand high-capacity coverage in crowded events or remote locations underscores their pivotal role in reshaping wireless communication. However, despite their numerous advantages, UAVs face the challenge of limited onboard energy that dictates their operational time [4], [5], underscoring the need to optimize their operational efficiency.","In recent years, the integration of UAVs with RIS and FDRs has gathered significant attention in the research community. Specifically, numerous studies have delved into the intricacies of UAV-mounted RIS and UAV-mounted FDRs, exploring various aspects ranging from performance optimization to energy efficiency, and underscoring their potential to reshape the dynamics of wireless communication.
1) UAV-mounted RIS: As researchers explore ways to optimize wireless communications, the integration of UAVs with RISs emerges as a promising solution to maintain LoS links, especially in propagation environments where the wireless links can often be obstructed [14]. Specifically, the authors in [15] showcased that UAV-mounted RIS can improve outage performance in dense urban scenarios, even with the dynamic mobility of UAVs. Additionally, [16] explored strategies for the optimal deployment of UAV-mounted RIS in URLLC systems, focusing on scenarios where user fairness is of paramount importance, while [17] proposed a novel system design that leverages ambient backscatter communication in UAV-mounted RIS networks. Interestingly, considering the different electromagnetic functionalities of RIS [10], the authors in [18] provided a rigorous path loss model for the case where a UAV-mounted absorbing metasurface is utilized and validated the findings experimentally in an anechoic chamber. Transitioning to trajectory design, [19] emphasized on three-dimensional (3D) trajectory design for UAVs in urban environments, aiming to optimize the signal-to-noise ratio (SNR) for ground users, while [20] presented a trajectory optimization framework for UAV-mounted RIS focusing on maximizing the network’s secure energy efficiency. Finally, [21] merged the concepts by presenting a UAV trajectory design combined with RIS design in IoT networks, in order to facilitate the offloading of computational tasks.nan","In this paper, a comprehensive analysis for both UAV-mounted RIS and UAV-mounted FDR employing the DF protocol is presented. In more detail, our contribution is the following:

• We devise appropriate energy consumption models for both UAV-mounted RIS and UAV-mounted FDR that accurately capture the intricate relationship between key factors such as weight, flight duration, and the operational needs of RISs and FDRs in terms of energy.

• Recognizing the intricacies of UAV-mounted RIS and FDR setups, we formulate a joint time division multiple access (TDMA) user scheduling and UAV trajectory optimization problem that accounts for the power dynamics associated with both technologies. Given the non-convex nature of this optimization problem, we employ a combination of alternate optimization and successive convex optimization techniques, ensuring an efficient approach to obtaining an approximate optimal solution.

• Through simulation results, we demonstrate how our proposed methods significantly enhance the network minimum rate and user fairness. More specifically, our results show that for UAV-mounted RIS, increasing the number of reflective elements does not necessarily translate into improved performance, largely due to the added weight of a larger RIS that limits operational flight time. In a notable shift from existing assumptions, the UAV-mounted FDR consistently outperforms the nearly passive RIS, which underscores the key role of UAV motors and the associated weight in overall UAV energy consumption. Additionally, the results highlight the crucial role of the UAV’s battery capacity in trajectory optimization, directly influencing the optimal trajectory and thereby necessitating UAV movement only when it is essential for minimizing energy consumption during traversal. To this end, our work emphasizes the importance of energy-aware design in UAV-assisted communication networks, focusing on balancing energy consumption with communication efficiency.","In Fig. 3, we illustrate the relationship between the network’s minimum rate rmin and the number of reflecting elements M for σ2 = −144 dB, for L = 750 m, 500 m, and 250 m, respectively. As it can be observed, rmin increases as L decreases, which is a consequence of the reduced path loss for the GNs. As it can be observed, across all examined L cases, there exists an optimal number of reflecting elements which results from the improved channel gains and the increased energy consumption due to the weight of additional elements. Specifically, adding more reflecting elements increases the rate for each GN logarithmically, while the number of time slots decreases quadratically, in line with (14) and (17) for the case where the UAv speed is equal to zero (i.e., hovering state). Moreover, Fig. 3 shows that for larger L values, a higher number of reflecting elements is optimal, suggesting a strategy to mitigate excess path loss with shorter flight durations. However, in smaller areas, a precise selection of reflecting elements is essential, as fewer elements can achieve enhanced coverage, thus negating the need for further increasing the reflecting elements and avoiding a compromise on the flight duration. Finally, in line with Remark 2, the optimal trajectory obtained from Algorithm 1 for the UAV-mounted RIS scenario, considering that q[1] coincides with the BS location, is the hovering trajectory, maximizing the minimum rate through minimized path loss and optimized energy use.

Fig. 4 shows the effect of the number of antennas on the UAV-mounted FDR on rmin, for σ2 = −144 dB and L = 750 m, 500 m, and 250 m, respectively.","In this work, a detailed comparison was performed between UAV-mounted RIS and FDR, underscoring their distinct energy consumption patterns and the critical importance of energy-aware design in UAV-assisted communication networks. Specifically, our findings confirmed the existence of an optimal number of reflecting elements for the UAV-mounted RIS and an optimal number of antennas for the UAV-mounted FDR, tailored for the scenario under study, while our optimization algorithm indicated a potential decrease in the optimal number of reflecting elements. Furthermore, it was shown that the primary energy consumption factor for the UAV is its motors, emphasizing the need for lightweight design, especially for UAV-mounted RISs. Despite the nearly passive nature of the RIS, its significant weight, coupled with the inherent double path loss, challenges its suitability in the considered scenario. In contrast, the FDR consistently emerged as a more efficient solution for optimizing network fairness due to its favorable path loss and the fact that its operational needs in terms of energy do not significantly affect the UAV’s consumption.",Energy-aware Trajectory Optimization for UAV-mounted RIS and Full-duplex Relay,"Dimitrios Tyrovolas, Nikos A. Mitsiou, Thomas G. Boufikos, Prodromos-Vasileios Mekikis, Sotiris A. Tegos, Panagiotis D. Diamantoulakis, Sotiris Ioannidis, Christos K. Liaskos, George K. Karagiannidis","1
Energy-aware Trajectory Optimization for
UAV-mounted RIS and Full-duplex Relay
Dimitrios Tyrovolas, Graduate Student Member, IEEE, Nikos A. Mitsiou, Student Member, IEEE,
Thomas G. Boufikos, Prodromos-Vasileios Mekikis, Member, IEEE, Sotiris A. Tegos, Member, IEEE,
Panagiotis D. Diamantoulakis, Senior Member, IEEE, Sotiris Ioannidis, Christos K. Liaskos, Member, IEEE,
and George K. Karagiannidis, Fellow, IEEE
Abstract—In the evolving landscape of sixth-generation (6G)
wireless
networks,
unmanned
aerial
vehicles
(UAVs)
have
emerged as transformative tools for dynamic and adaptive
connectivity. However, dynamically adjusting their position to
offer favorable communication channels introduces operational
challenges in terms of energy consumption, especially when
integrating advanced communication technologies like reconfig-
urable intelligent surfaces (RISs) and full-duplex relays (FDRs).
To this end, by recognizing the pivotal role of UAV mobility,
the paper introduces an energy-aware trajectory design for
UAV-mounted RISs and UAV-mounted FDRs using the decode-
and-forward (DF) protocol, aiming to maximize the network’s
minimum rate and enhance user fairness, while taking into
consideration the available on-board energy. Specifically, this
work highlights their distinct energy consumption characteris-
tics and their associated integration challenges by developing
appropriate energy consumption models for both UAV-mounted
RISs and FDRs that capture the intricate relationship between
key factors such as weight, and their operational characteristics.
Furthermore, a joint time-division multiple access (TDMA) user
scheduling-UAV trajectory optimization problem is formulated,
considering the power dynamics of both systems, while assuring
that the UAV energy is not depleted mid-air. Finally, simulation
results underscore the importance of energy considerations in
determining the optimal trajectory and scheduling and provide
insights into the performance comparison of UAV-mounted RISs
and FDRs in UAV-assisted wireless networks.
Index Terms—unmanned aerial vehicle (UAV), reconfigurable
intelligent surface (RIS), full-duplex relay, trajectory optimiza-
tion, energy efficiency
I. INTRODUCTION
Unmanned Aerial Vehicles (UAVs) are emerging as transfor-
mative tools in the landscape of future sixth-generation (6G)
D. Tyrovolas is with the Dept. of Electrical and Computer Engineering,
Aristotle University of Thessaloniki, 54124 Thessaloniki, Greece, and with
the Dept. of Electrical and Computer Engineering, Technical University of
Crete, Chania, Greece (tyrovolas@auth.gr).
N. A. Mitsiou, T. G. Boufikos, S. A. Tegos, and P. D. Diamantoulakis are
with the Dept. of Electrical and Computer Engineering, Aristotle University
of Thessaloniki, 54124 Thessaloniki, Greece ({nmitsiou, mpothogeo, tegosoti,
padiaman}@auth.gr).
P.-V. Mekikis is with Hilti Corporation, Feldkircher Strasse 100, 9494
Schaan, Liechtenstein (akis.mekikis@hilti.com)
S. Ioannidis is with the Dept. of Electrical and Computer Engineering,
Technical University of Crete, Chania, Greece (sotiris@ece.tuc.gr).
C. K. Liaskos is with the Computer Science Engineering Department,
University of Ioannina, Ioannina, and with the Foundation for Research and
Technology Hellas (FORTH), Greece (cliaskos@ics.forth.gr).
G. K. Karagiannidis is with the Dept. of Electrical and Computer Engi-
neering, Aristotle University of Thessaloniki, 54124 Thessaloniki, Greece and
with the Artificial Intelligence & Cyber Systems Research Center, Lebanese
American University (LAU), Lebanon (geokarag@auth.gr).
wireless networks [1], [2]. Specifically, their inherent flexibil-
ity allows them to follow optimized trajectories, dynamically
adjusting their paths to offer line of sight (LoS) channels ubiq-
uitously. This capability makes them invaluable in scenarios
demanding adaptive connectivity solutions, such as bridging
connectivity gaps in challenging terrains or enhancing net-
work resilience in disaster-stricken areas [1], [3]. Furthermore,
their ability to provide on-demand high-capacity coverage in
crowded events or remote locations underscores their pivotal
role in reshaping wireless communication. However, despite
their numerous advantages, UAVs face the challenge of limited
onboard energy that dictates their operational time [4], [5],
underscoring the need to optimize their operational efficiency.
To this end, innovative methods are essential to harness the full
potential of UAVs, taking into account their energy constraints.
Given the finite flight duration of UAVs, it becomes of
paramount importance to maximize data throughput and ef-
ficiency during their operational time. In this direction, full-
duplex (FD) communication, with its ability to simultaneously
transmit and receive data, emerges as a key solution in
this context [6]. Unlike traditional half-duplex systems that
alternate between transmission and reception, FD systems
effectively double the spectral efficiency, making every time
slot of the UAV’s flight time count [6], [7]. This capability
is particularly beneficial for UAVs, which can dynamically
adjust their positions to establish LoS communication links,
without the constraints of ground-based systems. To realize the
potential of FD communications in UAVs, the two promising
technologies that come to the forefront are Reconfigurable
Intelligent Surfaces (RIS) and Full-Duplex Relays (FDR) [8],
[9]. Specifically, an RIS is a programmable metasurface that
adjusts its electromagnetic response to manipulate the imping-
ing upon it electromagnetic (EM) waves in a desired manner,
supporting FD communications with minimal power for its
configuration mechanism [10]–[12]. On the other hand, an
FDR, equipped with distinct sets of antennas for transmission
and reception, actively sends and receives signals concur-
rently, enhancing spectral efficiency [7], [13]. Therefore, as
the integration of these technologies with UAVs continues to
evolve, understanding their distinct advantages and challenges
becomes crucial in shaping the future of UAV-assisted wireless
networks.
arXiv:2401.12107v1  [cs.IT]  22 Jan 2024
2
A. Related Works
In recent years, the integration of UAVs with RIS and FDRs
has gathered significant attention in the research community.
Specifically, numerous studies have delved into the intricacies
of UAV-mounted RIS and UAV-mounted FDRs, exploring
various aspects ranging from performance optimization to
energy efficiency, and underscoring their potential to reshape
the dynamics of wireless communication.
1) UAV-mounted RIS: As researchers explore ways to opti-
mize wireless communications, the integration of UAVs with
RISs emerges as a promising solution to maintain LoS links,
especially in propagation environments where the wireless
links can often be obstructed [14]. Specifically, the authors in
[15] showcased that UAV-mounted RIS can improve outage
performance in dense urban scenarios, even with the dynamic
mobility of UAVs. Additionally, [16] explored strategies for
the optimal deployment of UAV-mounted RIS in URLLC
systems, focusing on scenarios where user fairness is of
paramount importance, while [17] proposed a novel system
design that leverages ambient backscatter communication in
UAV-mounted RIS networks. Interestingly, considering the dif-
ferent electromagnetic functionalities of RIS [10], the authors
in [18] provided a rigorous path loss model for the case
where a UAV-mounted absorbing metasurface is utilized and
validated the findings experimentally in an anechoic chamber.
Transitioning to trajectory design, [19] emphasized on three-
dimensional (3D) trajectory design for UAVs in urban envi-
ronments, aiming to optimize the signal-to-noise ratio (SNR)
for ground users, while [20] presented a trajectory optimization
framework for UAV-mounted RIS focusing on maximizing the
network’s secure energy efficiency. Finally, [21] merged the
concepts by presenting a UAV trajectory design combined with
RIS design in IoT networks, in order to facilitate the offloading
of computational tasks. To this end, it is imperative to consider
both communication and trajectory design for realizing the full
potential of UAV-mounted RIS in diverse wireless network
scenarios.
2) UAV-mounted FDR: While UAV-mounted RISs offer
unique advantages in wireless communications, they are often
challenged by significant path loss due to the double path loss
phenomenon [11]. In contrast, UAV-mounted FDRs, equipped
with integrated electronic components such as amplifiers and
decoders, not only have the capability to transmit and re-
ceive signals simultaneously but can also process the received
signal, making them more robust to path loss than RISs.
Therefore, UAV-mounted FDRs present a compelling solution,
especially in dynamic environments demanding real-time data
exchange and reduced latency [7], [9]. By taking into account
the advantages of FDRs, [22] delved into optimizing the
source and the UAV-mounted FDR transmit power along
with its trajectory to enhance the system’s outage probability.
Furthermore, the potential of UAV-mounted FDRs in high-
frequency scenarios was highlighted by [23] and [24], with
a focus on millimeter-wave channels. Moreover, considering
the increasing importance of secure communications, [25]
introduced a secrecy communication scheme using a UAV-
mounted FDR, optimizing various parameters to ensure both
energy efficiency and security. Lastly, [26], and [27] proposed
an optimization algorithm for the UAV trajectory, user schedul-
ing, and FDR power, showcasing significant performance
improvements in scenarios with multiple ground users. These
advances underscore the versatility of UAV-mounted FDRs
in addressing diverse communication challenges in modern
wireless networks.
B. Motivation & Contribution
In light of the aforementioned works, both RIS and FDR
have been extensively compared to determine their respective
advantages. However, a critical oversight remains prevalent
in much of the existing literature, specifically the lack of
comprehensive energy consumption models in the analysis
of UAV-mounted systems. For instance, an RIS, while nearly
passive, can become considerably larger due to the numer-
ous reflecting elements, adding weight and impacting the
UAV’s energy efficiency [28]. Conversely, an FDR, though
lightweight, demands more energy for tasks like decoding
and signal amplification, and self-interference (SI) mitigation
[24], [29]. Additionally, the different path loss characteristics
of these systems introduce uncertainty in the UAV’s energy
consumption for traversing, as the UAV may need to navigate
to various locations to optimize path loss, further complicating
the UAV energy consumption. To this end, neglecting detailed
energy consumption models in trajectory optimization may
lead to incomplete or imprecise assessments of the opera-
tional capabilities and limitations of UAV-mounted RIS and
FDR systems. Building on this, few studies have focused
on the performance of UAV-mounted systems, while consid-
ering their energy consumption profiles. For instance, [28]
and [30] explored UAV-mounted RIS-based communications,
emphasizing the RIS weight in UAV energy consumption and
identifying an optimal number of reflecting elements, with the
latter adjusting this number with the addition of a solar panel.
Finally, even though [31] compared UAV-mounted RIS with
their relay counterparts, it overlooked the energy consumption
characteristics and trajectory planning, thus potentially leading
to incomplete conclusions. Therefore, to the best of the au-
thors’ knowledge, no existing work provides a comprehensive
comparison of UAV-mounted RIS and UAV-mounted FDRs,
especially in the context of optimal trajectory design, while
considering their distinct energy consumption profiles.
In this paper, a comprehensive analysis for both UAV-
mounted RIS and UAV-mounted FDR employing the DF
protocol is presented. In more detail, our contribution is the
following:
• We devise appropriate energy consumption models for
both UAV-mounted RIS and UAV-mounted FDR that
accurately capture the intricate relationship between key
factors such as weight, flight duration, and the operational
needs of RISs and FDRs in terms of energy.
• Recognizing the intricacies of UAV-mounted RIS and
FDR setups, we formulate a joint time division multiple
access (TDMA) user scheduling and UAV trajectory op-
timization problem that accounts for the power dynamics
associated with both technologies. Given the non-convex
3
Fig. 1: UAV-assisted network topology.
nature of this optimization problem, we employ a com-
bination of alternate optimization and successive convex
optimization techniques, ensuring an efficient approach
to obtaining an approximate optimal solution.
• Through simulation results, we demonstrate how our
proposed methods significantly enhance the network min-
imum rate and user fairness. More specifically, our results
show that for UAV-mounted RIS, increasing the number
of reflective elements does not necessarily translate into
improved performance, largely due to the added weight
of a larger RIS that limits operational flight time. In a no-
table shift from existing assumptions, the UAV-mounted
FDR consistently outperforms the nearly passive RIS,
which underscores the key role of UAV motors and the
associated weight in overall UAV energy consumption.
Additionally, the results highlight the crucial role of
the UAV’s battery capacity in trajectory optimization,
directly influencing the optimal trajectory and thereby
necessitating UAV movement only when it is essential
for minimizing energy consumption during traversal. To
this end, our work emphasizes the importance of energy-
aware design in UAV-assisted communication networks,
focusing on balancing energy consumption with commu-
nication efficiency.
C. Structure
The remaining of the paper is organized as follows. The
system model is described in Section II. Furthermore, the
examined optimization problem and its solution is presented
in Section III, while our simulation results are presented in
Section IV. Finally, Section V concludes the paper.
II. SYSTEM MODEL
A. System Overview
We examine a network of K ground nodes (GNs) that
are randomly distributed over a rectangular region with sides
equal to L, and a base station (BS), which also operates as a
UAV charging station (CS). However, given the challenging
propagation conditions due to excessive distances, physical
obstructions like buildings, and the GNs’ limited transmission
power, it is assumed that direct communication links between
each GN and the BS are not available. To address this, we
employ a rotary-wing UAV equipped with either an RIS or
an FDR, to act as an intermediate assisting node between the
GNs and the BS. Specifically, the UAV takes off from the
BS and establishes LoS communication between the GNs and
the BS, while flying along a designated trajectory. Afterwards,
the UAV returns to the BS for recharging purposes, leverag-
ing the BS’s dual functionality as a CS [32]. It should be
highlighted that the examined network is based on TDMA
to serve different GNs within the UAV’s flight duration.
Finally, as illustrated in Fig. 1, it is crucial to ensure that
the communication equipment is appropriately attached to the
UAV frame to avoid disrupting the airflow around the motors
and, thus, compromising its aerodynamics and stability [30].
Considering a 3D Cartesian coordinate system, we assume
that the rectangular region’s center coincides with the ori-
gin of the coordinate system, the BS location is equal to
lBS = [0, 0, HBS], where HBS represents the BS height,
and the K GNs are located at lk
= [xk, yk, 0], where
k ∈ {1, . . . , K}, respectively. Additionally, we assume that
the UAV flies at a fixed altitude Hu, which is selected
appropriately to ensure that the UAV navigates clear of any
environmental obstacles. Furthermore, considering that the
trajectory duration equals to T, the UAV location at time t
can be written as q(t) = [xq(t), yq(t), Hu], where 0 ≤ t ≤ T
and (xq(t), yq(t)) denote the x-y UAV coordinate at time
t. However, for tractability reasons, the flight duration T is
divided into N equal time slots, i.e., T = Nδt, where δt
is the duration of each time slot. Hence, the UAV trajectory
q(t) during T can be efficiently approximated by a N-length
sequence q[n] = [xq[n], yq[n], Hu], n ∈ N, N = {1, . . . , N},
where 4
imperative. Therefore, below, we express the achievable rate
of a network when the GN-BS communication is facilitated
by either a UAV-mounted RIS or a UAV-mounted FDR.
1) UAV-mounted RIS:
Over the last years, RISs have
emerged as efficient tools for manipulating EM waves with
minimal energy consumption. Therefore, the integration of
RISs on UAVs can offer a dynamic approach to wireless
networks, as a UAV can optimally position an RIS to steer
incoming EM waves directly to the BS, ensuring flexible 3D
network coverage. Hence, considering an RIS that performs
perfect beam-steering [11], the achievable rate of the k-th GN
in a UAV-mounted RIS-assisted TDMA network during the
n-th time slot is expressed as
RRIS,k[n] = Bak[n]log2
5
It should be mentioned that similarly with the UAV-mounted
RIS case, we assume that the GNs transmission power Pt[n]
is constant within the UAV flight, i.e., Pt[n] = Pt.
Remark 1: By setting γ1[n] = γ2[n] we can derive the op-
timal FDR transmission power that maximizes the achievable
rate of the k-th GN at the n-th time slot, which is given as
P ∗
u[n] =
−d1[n]σ2 + σ
»
d1[n]
2σ2 + 4ωPtd2[n]
2GtAr
2AtGrωd1[n]
. (13)
C. UAV Power Consumption
Given the inherent battery constraints of UAVs that result
in finite flight duration, it becomes crucial to understand
their power dynamics, particularly when integrating various
communication technologies. Specifically, the UAV power
consumption with varied communication equipment can be
described as
Pd[n] = Pth,d[n] + Pc,d + Ptr,
(14)
where Pth,d[n] refers to UAV thrusting and encompasses
the power demands for transitioning, countering wind drag,
and related activities. Furthermore, Pc,d denotes the power
required by the communication equipment (e.g., RIS or FDR),
while Ptr is a minimal constant power associated with the
UAV’s navigational communication (typically less than 1 W),
and can be considered negligible [28]. To elaborate further on
the required power for the consumption model, the required
power for the RIS operation Pc,RIS can be described as
Pc,RIS = MPe + Pct,
(15)
where Pe represents the power consumption of each reflecting
element, and Pct refers to the energy required by the RIS
controller to periodically adjust the RIS phase shift profile
within every time slot. Furthermore, the power consumption
for the FDR operation Pc,FD is given as
Pc,FD = Pu (1 + α) + AnP C
R ,
(16)
where Pu is the FDR transmission power, α is the inverse
of the power amplifier drain efficiency, and P C
R denotes the
power consumption of an An-antenna transceiver [34]. This
transceiver consumption encompasses the mixer power, the
power of phase shifters for each antenna during transmission
and reception, the power of each low-noise amplifier per
antenna, the frequency synthesizer power, and the encoder
power consumption.
Considering the inherent dynamics of UAVs, the thrusting
power Pth,d plays a pivotal role in the total power consump-
tion. Notably, Pth,d varies within each time slot, being heavily
influenced by the UAV speed, its weight, aerodynamic design,
and other onboard components such as the battery weight.
Thus, regarding the consumption model presented in [28],
Pth,d[n] can be reliably characterized as
Pth,d[n] = C1Wd[n]
2 + C2Wd[n] + C3,
(17)
where C1, C2, and C3 are motor-dependent parameters, while
Wd[n] encompasses all weight components impacting thrusting
power, which can be expressed as
Wd[n] = Uw + Dw + Rw,d + Sw,d[n].
(18)
In more detail, Uw expresses the weight of the UAV frame
and its battery, while Dw describes the wind drag given as
Dw = ρav2
aCdAUAV
2g
,
(19)
where ρa is the air density, g is the gravity acceleration, va
is the average wind velocity, Cd is the drag shape coefficient
given experimentally, and AUAV is UAV frame area. Moreover,
Rw,d is the communication equipment weight, where in the
RIS case is given by Rw,RIS = MEw, while for an FDR with
An antennas is equal to Rw,DF = AnAw, where Ew is the
weight of one reflecting element and Aw is the weight of each
FDR antenna, respectively. Finally, Sw,d[n] is the extra weight
added to the motors due to any change in the speed of the
UAV in each time slot given by
Sw,d[n] = (Tmax − Uw − Dw − Rw,d) υ[n]
υmax
,
(20)
with Tmax being the maximum achievable thrust, υ[n] =
∥q[n]−q[n−1]∥
δt
reflecting the average UAV speed within the n-th
time slot, and vmax expressing the maximum achievable UAV
speed. It should be mentioned that the choice of UAV motors
is directly influenced by the weight they are required to lift. As
such, a UAV-mounted RIS, which is generally heavier, would
demand different motors than a UAV-mounted FDR, to stay in
the air for the same amount of time slots as the UAV-mounted
FDR. Finally, considering (1) and the available Bc, by setting
υ[n] = 0 or υ[n] = υmax in (20), we can obtain the maximum
and the minimum flight duration in terms of time slots, i.e.
Nmax, Nmin.
III. ENERGY-AWARE TRAJECTORY DESIGN
In this section, we formulate and solve an optimization
problem to derive a UAV trajectory that maximizes the mini-
mum data rate across K GNs, ensuring GN fairness across the
network. Unlike existing works, our approach underscores the
importance of energy awareness, especially given the inherent
battery constraints of UAVs. Specifically, our optimization
problem intrinsically incorporates the UAV’s power consump-
tion, which is influenced by the mounted communication
device, as well as the UAV velocity. Considering these energy
dynamics, the formulated optimization problem not only en-
sures the practical relevance of our findings, but also provides
design insights into the characteristics of the employed RIS
and FDR that optimize network performance. Moreover, the
optimization guarantees that the power consumed by the UAV
during its trajectory does not surpass its available battery en-
ergy, ensuring the UAV’s safe return to the BS and highlighting
the significance of energy-aware trajectory design. To this end,
we will examine the network efficiency in terms of achievable
rate and fairness for both RIS and FDR.
A. Problem Formulation
To efficiently maximize the minimum data rate of the
network, we aim to jointly optimize the UAV trajectory and
TDMA user scheduling, taking into account mobility, user
scheduling, and UAV power consumption constraints. Given
these considerations, by leveraging the integer variable A that
represents the TDMA scheduling, and the continuous variable
Q that describes the UAV trajectory, the optimization problem
6
for both UAV-mounted RIS and UAV-mounted FDR cases can
be formulated as
max
A,Q min
k
( N
X
n=1
Rd,k[n]
)
s.t C1 : q[1] = q[N],
C2 : υ[n] ≤ υmax, ∀n ∈ N,
C3 :
K
X
k=1
ak[n] ≤ 1, ∀n ∈ N,
C4 : Bc − δt
N
X
n=1
Pd[n] ≥ 0,
C5 : ak[n] ∈ {0, 1}, ∀n ∈ N,
C6 : qmin ≤ q[n] ≤ qmax, ∀n ∈ N,
C7 : Pu[n] ≤ Pmax, ∀n ∈ N,
(P1)
where Rd,k[n] is the GN data rate which is given by
Rd,k[n] =





0,
γd
r,k[n] < γthr
Bak[n]log2
Ä
1 + γRIS
r,k[n]
ä
,
γd
r,k[n] > γthr, d = RIS
Bak[n]log2
Ä
1 + γFD
r,k[n]
ä
,
γd
r,k[n] > γthr, d = FDR,
(21)
where γthr is an SNR threshold. In more detail, C1 forces
the UAV trajectory to begin and end at the same point. In
addition, C2 indicates that the UAV speed υ[n] cannot exceed
the maximum UAV velocity υmax, while C3 defines that only
one GN-BS pair can be served by the UAV-mounted RIS or
the UAV-mounted FDR within a certain time slot. Moreover,
C4 indicates that the UAV’s power consumption during its
trajectory must not exceed its battery’s available energy, while
C5 and C6 set the lower and upper bounds for the optimization
variables a, q, with C6 ensuring the UAV remains within
the predefined rectangular field. Lastly, C7 describes that the
UAV’s transmission power should be less than Pmax, denoting
the peak power limit of the UAV, where for the UAV-mounted
RIS case, Pu[n] = 0 as the RIS does not consume any
transmission power.
B. Problem Solution
As it can be seen, problem (P1) is intractable since it
contains both continuous and integer variables while its ob-
jective function is non-convex. To this end, given that current
optimization methods struggle with problems containing non-
linear constraints paired with integer variables, a separation of
the integer variable A and the continuous variable Q becomes
essential. To address this, the alternate optimization technique
is employed, which relies on successively optimizing each
optimization variable block until convergence [26]. Therefore,
for a fixed trajectory Q we have
max
A min
k
( N
X
n=1
Rd,k[n]
)
s.t C1 :
K
X
k=1
ak[n] ≤ 1, ∀n ∈ N,
C2 : ak[n] ∈ {0, 1}, ∀n ∈ N.
(PA.1)
As it can be seen, problem (PA.1) is an integer programming
problem, however, it is not in canonical form, since the
objective function is a non-linear function. Thus, by utilizing
the auxiliary variable rmin, problem (PA.1) is equivalently
written as
max
A,rmin rmin
s.t C1 :
K
X
k=1
ak[n] ≤ 1, ∀n ∈ N,
C2 : ak[n] ∈ {0, 1}, ∀n ∈ N,
C3 :
N
X
n=1
ak[n]Rd,k[n] ≥ rmin, ∀k ∈ K,
(PA.2)
which is a mixed-integer linear programming problem (MILP),
since for given trajectory Q, Rd,k[n] is constant for either
the FDR or the RIS case. As a consequence, (PA.2) can be
optimally solved using off-the-self optimization tools such as
the Branch and Bound method.
1) Optimization of Q: Given the TDMA schedule A,
problem (P1) can be written as
max
Q min
k
( N
X
n=1
ak[n]Rd,k[n]
)
s.t C1 : q[1] = q[N],
C2 : υ[n] ≤ υmax, ∀n ∈ N,
C3 : Bc − δt
N
X
n=1
Pd[n] ≥ 0,
C4 : qmin ≤ q[n] ≤ qmax, ∀n ∈ N,
C5 : Pu[n] ≤ Pmax, ∀n ∈ N,
(PQ.1)
which is a non-convex problem due to the non-concave and
dual branch objective function Rd,k[n]. Furthermore, in (20),
Pd[n] is influenced by the UAV’s speed υ[n], which is derived
from the differences in consecutive positions xq[n] and yq[n],
thus, given that these positions are constants for each time
slot, the relationship of Pd[n] with xq and yq is affine, as it
comprises linear differences and constant terms. Thus, Pd[n]
is by definition convex which makes constraint C3 convex as
well. In addition, by introducing the auxiliary variable rmin,
problem (PQ.1) is equivalently transformed as follows
max
xq,yq,rmin rmin
s.t (PQ.1) : C1, C2, C3, C4, C5
C6 :
N
X
n=1,
ak[n]=1
Rd,k[n] ≥ rmin, ∀k ∈ K.
(PQ.2)
Again, problem (PQ.2) is still non-convex due to C6. To tackle
this, we can convert it from a dual branch function into a single
function by introducing an appropriate constraint that assures
that the UAV always serves a GN for which the received
SNR at the BS-side is above γthr. Additionally, ∀n ∈ N and
7
∀k ∈ K for which ak[n] = 1, we can introduce the auxiliary
variables rk[n], thus the problem (PQ.2) can be rewritten as
max
xq,yq,rmin,rk[n] rmin
s.t (PQ.1) : C1, C2, C3, C4, C5
C6 :
N
X
n=1,
ak[n]=1
rk[n] ≥ rmin, ∀k ∈ K
C7 : γd
r,k[n] ≥ γthr, ∀n ∈ N, ∀k ∈ K that ak[n] = 1,
C8 : Rd,k[n] ≥ rk[n], ∀n ∈ N, ∀k ∈ K that ak[n] = 1.
(PQ.3)
Due to the distinct achievable rates of the UAV-mounted RIS
and the UAV-mounted FDR, C7 and C8 have to be dealt
differently for each case. To this end, the distinct approaches
for the two cases are presented below.
• UAV-mounted RIS: Utilizing (6) and defining AR =
γtGC0
2d0
4M 2, we adopt the approximation log2(1 +
z) ≈ log2(z). This choice is motivated by the dominant
high SNR conditions inherent in our system, particularly
under LoS scenarios. In fact, given the UAV’s strategic
positioning relative to the GNs it serves, z is typically
large, making the approximation increasingly pertinent.
Thus, C8 can be expressed as
C8 : log2(d1[n]
2) + log2
Ç
d2[n]
2
AR
å
≤ −rk[n].
(22)
Furthermore, by introducing the auxiliary variables
sk,n and tk[n], such that log2(d1[n]
2)
≤
sk,n, and
log2(d2[n]
2) ≤ tk[n], C6 and C8 are rewritten as
C6 :
N
X
n=1
8
given as
max
xq,yq,rmin,rk[n] rmin
s.t (PQ.3) : C1, C2, C3, C4, C5, C6, C7.A, C7.B
C8.A : (xq[n] − xk)2 + (yq[n] − yk)2 + Hu
2
AFD,1[n]
− 2rk[n],0 − (rk[n] − rk[n],0)2rk[n],0 ≤ 0
C8.B : (xq[n]−xb)2 + (yq[n]−yb)2 + (Hu−HBS)2
AFD,2[n]
− 2rk[n],0 − (rk[n] − rk[n],0)2rk[n],0 ≤ 0.
(PQ.4-FDR)
As it can be observed, both problem (PQ.4-RIS) and problem
(PQ.4-FDR) are now convex, allowing them to be addressed
using standard optimization techniques like the interior-point
method. The procedure for the joint TDMA-trajectory design,
for both the UAV-mounted RIS and UAV-mounted FDR cases,
is outlined in Algorithm 1. It is worth noting that the values for
iter1 and iter2 are selected to ensure that the solutions from
both the SCA and alternate optimization methods converge to
a consistent solution [26], which is then presented as the final
output of Algorithm 1.
Algorithm 1 Trajectory Design for UAV-mounted RIS or
UAV-mounted FDR
1: Initialize iter1, iter2 Bc, δt, vmax, Uw, Dw, Rw,d
2: for N = Nmin, Nmin + 1, ...., Nmax do
3:
Initialize Ainit, Qinit
4:
for i = 0, 1, 2, ..., iter1 do
5:
For Qinit, solve (PA.2) and obtain Ai
6:
for j = 0, 1, 2, ..., iter2 do
7:
For Ai, solve (PQ.4-FDR) or (PQ.4-RIS)
and obtain Qj
8:
rk[n],0 ← rj
k[n], sk[n],0 ← sj
k[n], tk[n],0 ← tj
k[n]
9:
end for
10:
Qinit ← Qiter2
11:
end for
12: end for
13: Q∗ ← Qiter2
14: For Q∗, solve (PA.2) and obtain A∗
IV. SIMULATION RESULTS
In this section, we present numerical results to assess the
performance of the proposed UAV-assisted communication
scenarios, whose power consumption and network parameters
are set as detailed in Table I and Table II, respectively.
Specifically, we consider an uplink communication system
assisted by i) a UAV-mounted RIS or ii) a UAV-mounted
FDR with 10 GNs that transmit with power equal to Pt = 0
dBm, that are also randomly distributed over a rectangular
area with sides equal to L = 750 m, unless otherwise stated.
In addition, a single-antenna BS is located in the origin of the
rectangular area which also serves as a UAV CS. It should
be mentioned that, in alignment with practical scenarios, we
assume that the UAV starts and concludes its trajectory at
the same location, reflecting the common practice, where the
TABLE I: POWER CONSUMPTION MODEL PARAMETERS
Parameter
Notation
Value
UAV weight
Uw
3.25 kg
Reflecting element weight
Ew
3.43 × 10−3 kg
Antenna weight
Aw
8 × 10−3 kg
Battery weight
Bw
1.35 kg
Battery capacity
Bc
45 Wh
Reflecting element consumption
Pn
2 µW
Controller consumption
Pc
50 mW
Inverse of pow. ampl. drain eff.
α
1.875
Transceiver consumption
P C
R
1.5 W
Maximum achievable thrust
Tmax
17 kg
Maximum UAV speed
υmax
62 km/h
Air density
ρa
1.225 kg/m3
Air velocity
va
2.5 m/s (Light Air)
Drag shape coefficient
Cd
0.005@0◦
UAV frame
AUAV
0.5 × 0.5 m2
Reflecting element area
Are
λ
10 × λ
10 m2
Gravity acceleration
g
9.8 m/s2
MN505-s KV320 T-MOTOR
C1, C2, C3
4, 86, -21.2
AT4130 KV230 T-MOTOR
C1, C2, C3
10.5, -46, 744
TABLE II: NETWORK PARAMETERS
Parameter
Notation
Value
UAV height
Hu
100 m
BS height
HBS
15 m
Number of GNs
K
10
Max transmit FDR power
Pmax
0 dBm
time slot duration
δt
1 s
Reference distance
d0
1 m
Bandwidth
B
1 MHz
Transmit power
Pt
0 dBm
Noise Power
σ2
−144 dBm
Antenna gains
Gt, Gr
0 dB
Wavelength
λ
0.125 m
SIC coefficient
ω
−90 dB
Iterations
iter1, iter2
20, 20
UAV takes off and lands at the same place for recharging
purposes. Moreover, it is assumed that the UAV-mounted RIS
is equipped with the MN505-s KV320 T-MOTOR motors due
to its weight, while the lighter UAV-mounted FDR uses the
AT4130 KV230 T-MOTOR motors. Moreover, the size of
the UAV frame, combined with the size of each reflecting
element as outlined in Table I, allows for a maximum of 1600
reflecting elements to be fitted on the UAV frame. In contrast,
for the UAV-mounted FDR, the frame can accommodate
a maximum of 12 antennas, arranged as a uniform linear
array (ULA) with an inter-distance of
λ
2 along the frame’s
diagonal, for which we assume that At = Ar. In addition,
we consider a benchmark TDMA scheduling scheme where
each GN is allocated NGN =  N
K
 time slots, to clearly show
the efficiency of our algorithm. To be more precise, in the
benchmark scheme, the UAV serves the nearest GN for NGN
time slots and then serves the next nearest unserved GN,
continuing until every GN has been served before returning
to its initial location. Additionally, the benchmark trajectories
are determined through a detailed search for the most effective
combination of time slots and trajectory sizes, all considered
within the operational limits of the UAV’s battery life. Finally,
all of the results were calculated through Monte Carlo simu-
lations with 1000 iterations.
In Fig. 2, three distinct benchmark UAV trajectories are
illustrated, each serving as a feasible initial trajectory, denoted
9
−375−250−125 0
125 250 375
−375
−250
−125
0
125
250
375
Length (m)
Width (m)
GN
BS
(a)
−375−250−125 0
125 250 375
−375
−250
−125
0
125
250
375
Length (m)
Width (m)
GN
BS
(b)
−375−250−125 0
125 250 375
−375
−250
−125
0
125
250
375
Length (m)
Width (m)
GN
BS
(c)
Fig. 2: Benchmark UAV trajectories: (a) Circle (b) Rombus (c) Spiral.
.
as Qinit, for the implementation of Algorithm 1. These tra-
jectories include i) a straight line connecting the origin to the
midpoint on the right side of the rectangular field, followed
by a circle with radius
L
2 centered at the axis origin; ii) a
straight line connecting the origin to the midpoint on the
right side of the rectangle, succeeded by a rhombus with sides
measuring L
√
2
2 ; and iii) an Archimedean spiral described in
polar coordinates by r =
L
12πθ, with θ ∈ [0, 2π), followed
by a straight line leading back to the origin. In the circular
and rhombus trajectories, the UAV initially follows the straight
line, completes the circle or rhombus, and then retraces its
path along the straight line back to the origin, while in the
spiral trajectory the UAV navigates the spiral path before
returning to the origin via the straight line. Notably, in all
these trajectories, the UAV navigates counter-clockwise, with
the starting and ending points, q[1] and q[N], aligning with
the BS location. It should be highlighted that as shown in
(1), the calculation of N based on Bc and Pd informs the
minimum average speed required for the UAV to deplete its
battery at the last point of the trajectory, thereby defining the
feasible size of each trajectory within the allocated time slots.
Specifically, the maximum number of time slots corresponds
to the case where the UAV minimizes its power consumption
(i.e., hovering), while reducing the number of time slots allows
for larger trajectories. This indicates that an optimal number
of time slots must be determined, as too many slots could
restrict the UAV from performing large enough trajectories
due to energy limitations, while fewer slots may limit service
duration. Thus, the evaluation of different initial trajectories,
each corresponding to a specific number of time slots is
demanded, which can be done by initializing different values
of N in step 1 of Algorithm 1 and selecting the value of N
maximizing rmin.
In Fig. 3, we illustrate the relationship between the net-
work’s minimum rate rmin and the number of reflecting
elements on the UAV-mounted RIS, with the noise power
set at σ2 = −144 dB, for L = 750 m, 500 m, and 250
m, respectively. As it can be observed, rmin increases as L
decreases, which is a consequence of the reduced path loss for
the GNs. As it can be observed, across all examined L cases,
there exists an optimal number of reflecting elements which
600
800
1,000
1,200
1,400
1,600
0
20
40
60
80
100
M
rmin (Mbps)
L = 750 m
L = 500 m
L = 250 m
Benchmark
Algorithm 1
Fig. 3: Minimum rate vs the number of reflecting elements M
for σ2 = −144 dB.
2
4
6
8
10
12
100
120
140
160
180
200
An
rmin (Mbps)
L = 750 m
L = 500 m
L = 250 m
Benchmark
Algorithm 1
Fig. 4: Minimum rate versus UAV-mounted FDR Antennas for
σ2 = −144 dB
10
2
4
6
8
10
12
0
2
4
6
8
10
12
An
rmin (Mbps)
Circle
Rhombus
Spiral
Benchmark
Algorithm 1
Fig. 5: Minimum rate versus UAV-mounted FDR Antennas for
σ2 = −114 dB and L = 750 m.
results from the improved channel gains and the increased
energy consumption due to the weight of additional elements.
Specifically, adding more reflecting elements increases the rate
for each GN logarithmically, while the number of time slots
decreases quadratically, in line with (14) and (17) for the case
where the UAv speed is equal to zero (i.e., hovering state).
Moreover, Fig. 3 shows that for larger L values, a higher
number of reflecting elements is optimal, suggesting a strategy
to mitigate excess path loss with shorter flight durations. How-
ever, in smaller areas, a precise selection of reflecting elements
is essential, as fewer elements can achieve enhanced coverage,
thus negating the need for further increasing the reflecting
elements and avoiding a compromise on the flight duration.
Additionally, a key observation is the impact of the optimized
user scheduling by Algorithm 1, which significantly reduces
the required number of reflecting elements. For instance, under
the benchmark scheduling for L = 750 m, 500 m, and 250
m, the optimal numbers of reflecting elements are 1550, 1500,
and 1100, respectively, while with optimized user scheduling,
these numbers reduce to 1350, 1150, and 950, respectively,
emphasizing the advantages of network optimization in de-
termining the optimal RIS size. Finally, in line with Remark
2, the optimal trajectory obtained from Algorithm 1 for the
UAV-mounted RIS scenario, considering that q[1] coincides
with the BS location, is the hovering trajectory, maximizing
the minimum rate through minimized path loss and optimized
energy use.
Fig. 4 shows the effect of the number of antennas on the
UAV-mounted FDR on rmin, for σ2 = −144 dB and L = 750
m, 500 m, and 250 m, respectively. As it can be seen, decreas-
ing the value of L and increasing the number of FDR antennas
enhances rmin, with 12 antennas emerging as the optimal
number for network performance. Furthermore, similarly to
the UAV-mounted RIS scenario, the application of Algorithm
1 further enhances the minimum rate, demonstrating its value
on the network performance improvement. Interestingly, under
the network parameters in Table II, Algorithm 1 identifies
2
4
6
8
10
12
0
4
8
12
16
20
An
rmin (Mbps)
Bc = 30 Wh
Bc = 45 Wh
Bc = 60 Wh
Fig. 6: Minimum rate versus UAV-mounted FDR Antennas for
σ2 = −114 dB, and L = 750 m for various battery capacities.
the hovering trajectory as the optimal approach for all the
examined cases, which is a notable deviation from the ex-
pected optimization of the UAV-mounted FDR’s path loss at
intermediate distances between a GN and the BS. Specifically,
for the given network parameters, the received SNR for
the GNs consistently stays above γthr across all L values,
influencing the trajectory design, as it leads to the conclusion
that maximizing the flight duration, thus serving the GNs from
the initial UAV position, is more advantageous than moving
the UAV to each GN’s optimal point, which results in the
loss of time slots. In more detail, during the UAV’s traversal
to these optimal points, there would be instances where the
UAV is not optimally positioned to serve any GN, leading to
inefficient use of energy and further loss of time slots. Finally,
the results from Figs. 3 and 4 indicate that the UAV-mounted
FDR outperforms the UAV-mounted RIS for all values of L,
even with 2 antennas on the FDR. This superior performance
of the FDR is attributed to the severe impact of double
path loss in the RIS scenario and the FDR’s lower energy
consumption compared to the RIS, which incurs significant
energy costs due to its weight. Additionally, the UAV-mounted
FDR’s performance is favored as no extra energy is consumed
for transitioning, despite potential path loss optimization for
each GN. To this end, the provided results underscore the
practical implications of UAV system choices on network
performance, particularly emphasizing the balance between
energy consumption and effective communication, while also
highlighting the significant effect of the UAV motors over the
energy required for the RIS and FDR operation in the UAV
energy consumption.
Fig. 5 illustrates the trajectory optimization for a UAV-
mounted FDR, comparing the solution from Algorithm 1 with
the benchmark trajectories for which the benchmark TDMA
scheme is applied, in a scenario with an increased noise power
of −114 dB for an area of L equal to 750 meters. In this
scenario, unlike in Fig. 4 where σ2 = −144 dB and the UAV-
11
−375−250−125 0
125 250 375
−375
−250
−125
0
125
250
375
Length (m)
Width (m)
GN
BS
(a)
−375−250−125 0
125 250 375
−375
−250
−125
0
125
250
375
Length (m)
Width (m)
GN
BS
(b)
−375−250−125 0
125 250 375
−375
−250
−125
0
125
250
375
Length (m)
Width (m)
GN
BS
(c)
Fig. 7: Optimal UAV trajectories: (a) Bc = 30 Wh (b) Bc = 45 Wh (c) Bc = 60 Wh.
mounted FDR remains stationary, the increased noise level
results in areas where, if GNs are located within them, the
received SNR falls below γthr, UAV movement is necessary
to maintain effective communication. As it can be seen, for
An = 12, the circular trajectory achieves a minimum rate
of 0.51 Mbps, the rhombus 0.25 Mbps, and the spiral 0.28
Mbps, while the trajectory optimized through Algorithm 1
leads to a significantly higher rate of 12.28 Mbps. Moreover,
for all examined trajectories, when the number of An is
less than 6, the network’s minimum rate tends towards zero,
due to the increased probability of GNs experiencing outages
and the UAV’s battery limitations, which are insufficient to
enable the UAV to serve all GNs effectively. Therefore, Fig.
5 emphasizes the critical interplay between the number of
antennas, UAV battery capacity, and trajectory optimization in
ensuring robust network performance, especially in scenarios
with challenging communication conditions such as increased
noise or γthr thresholds. Finally, it should be noted that for
the UAV-mounted RIS case, if σ2 = −114dB then rmin = 0
across all the feasible M values, proving again the superiority
of UAV-mounted FDR over UAV-mounted RIS in establishing
communication link between the BS and the GNs.
Finally, Fig. 6 depicts how rmin is affected by the number
of antennas An of a UAV-mounted FDR across various battery
capacities, Bc, showcasing how an increase in Bc leads to an
improvement in the network’s minimum rate. Specifically, this
enhancement occurs due to the UAV’s ability to allocate more
time slots for serving each GN, which not only allows for
more extensive service coverage but also enables the UAV
to fly to more optimal locations for serving each GN, as
the increased number of time slots extends the operational
time before the battery depletes. Furthermore, Fig. 6 also
indicates that larger battery capacities permit the UAV to
execute broader trajectories, optimizing its positioning across
the field to serve the GNs more effectively, while smaller
battery capacities limit the trajectory size. Complementing
this, Fig. 7 illustrates the optimal trajectory for a specific GN
setup, which depicts the expansion in the size of the UAV’s
trajectory as Bc increases. This expansion reflects the variation
in the optimal number of time slots needed for different
Bc, illustrating the direct relation between battery capacity
and the effectiveness of the UAV’s flight path in enhancing
network performance. Additionally, a notable aspect of these
trajectories in Fig. 7, is the formation of a distinct spike in the
bottom-left region of the trajectory for all analyzed battery
capacities, which is a consequence of the strategic positioning
needed to address the unfavorable location of the bottom-left
GNs within the rectangular area. This contrasts with the right-
hand side of the rectangular area, where the GNs are fewer
in number, leading to a different trajectory pattern that does
not necessitate such pronounced adjustments. To this end, Fig.
6 and Fig. 7 underscore the effect of the battery capacity on
trajectory optimization, while emphasizing their significance in
ensuring robust network performance, particularly in scenarios
with demanding communication conditions.
V. CONCLUSIONS
In this work, a detailed comparison was performed be-
tween UAV-mounted RIS and FDR, underscoring their dis-
tinct energy consumption patterns and the critical impor-
tance of energy-aware design in UAV-assisted communication
networks. Specifically, our findings confirmed the existence
of an optimal number of reflecting elements for the UAV-
mounted RIS and an optimal number of antennas for the UAV-
mounted FDR, tailored for the scenario under study, while
our optimization algorithm indicated a potential decrease in
the optimal number of reflecting elements. Furthermore, it
was shown that the primary energy consumption factor for
the UAV is its motors, emphasizing the need for lightweight
design, especially for UAV-mounted RISs. Despite the nearly
passive nature of the RIS, its significant weight, coupled
with the inherent double path loss, challenges its suitability
in the considered scenario. In contrast, the FDR consistently
emerged as a more efficient solution for optimizing network
fairness due to its favorable path loss and the fact that its
operational needs in terms of energy do not significantly affect
the UAV’s consumption. Finally, our study illustrated that the
available energy within the UAV’s battery significantly affects
the optimal trajectory and that the UAV selects to move only
when necessary due to the substantial energy consumption of
the traversing process, emphasizing the importance of energy-
awareness in UAV-assisted networks.
12
REFERENCES
[1] M. Giordani and M. Zorzi, “Non-terrestrial networks in the 6G era:
Challenges and opportunities,” IEEE Netw., vol. 35, no. 2, pp. 244–251,
2021.
[2] P.-V. Mekikis, P. S. Bouzinis, N. A. Mitsiou, S. A. Tegos, D. Tyrovolas,
V. K. Papanikolaou, and G. K. Karagiannidis, “Enabling wireless-
powered IoT through incentive-based UAV swarm orchestration,” IEEE
Open J. Commun. Soc, vol. 4, pp. 2548–2560, 2023.
[3] M. Matracia, M. A. Kishk, and M.-S. Alouini, “Comparing aerial-
RIS- and aerial-base-station-aided post-disaster cellular networks,” IEEE
Open J. Veh. Technol., pp. 1–15, 2023.
[4] Y. Zeng and R. Zhang, “Energy-efficient UAV communication with
trajectory optimization,” IEEE Trans. Veh. Technol., vol. 16, no. 6, pp.
3747–3760, 2017.
[5] Y. Zeng, J. Xu, and R. Zhang, “Energy minimization for wireless
communication with rotary-wing UAV,” IEEE Trans. Wirel. Commun.,
vol. 18, no. 4, pp. 2329–2345, 2019.
[6] Z. Zhang, K. Long, A. V. Vasilakos, and L. Hanzo, “Full-duplex wireless
communications: Challenges, solutions, and future research directions,”
Proc. IEEE, vol. 104, no. 7, pp. 1369–1409, 2016.
[7] N. V. Shende, O. G¨urb¨uz, and E. Erkip, “Half-duplex or full-duplex
communications: Degrees of freedom analysis under self-interference,”
IEEE Trans. Wirel. Commun., vol. 17, no. 2, pp. 1081–1093, 2018.
[8] Y. Liu, X. Liu, X. Mu, T. Hou, J. Xu, M. Di Renzo, and N. Al-Dhahir,
“Reconfigurable intelligent surfaces: Principles and opportunities,” IEEE
Commun. Surv. Tutor., vol. 23, no. 3, pp. 1546–1577, 2021.
[9] L. Zhang, J. Liu, M. Xiao, G. Wu, Y.-C. Liang, and S. Li, “Performance
analysis and optimization in downlink NOMA systems with cooperative
full-duplex relaying,” IEEE J. Sel. Areas Commun., vol. 35, no. 10, pp.
2398–2412, 2017.
[10] C. Liaskos, S. Nie, A. Tsioliaridou, A. Pitsillides, S. Ioannidis, and
I. Akyildiz, “A new wireless communication paradigm through software-
controlled metasurfaces,” IEEE Commun. Mag., vol. 56, no. 9, pp. 162–
169, 2018.
[11] E. Basar, M. Di Renzo, J. De Rosny, M. Debbah, M.-S. Alouini, and
R. Zhang, “Wireless communications through reconfigurable intelligent
surfaces,” IEEE Access, vol. 7, pp. 116 753–116 773, 2019.
[12] M. Di Renzo, A. Zappone, M. Debbah, M.-S. Alouini, C. Yuen,
J. de Rosny, and S. Tretyakov, “Smart radio environments empowered
by reconfigurable intelligent surfaces: How it works, state of research,
and the road ahead,” IEEE J. Sel. Areas Commun., vol. 38, no. 11, pp.
2450–2525, 2020.
[13] G. Liu, F. R. Yu, H. Ji, V. C. M. Leung, and X. Li, “In-band full-duplex
relaying: A survey, research issues and challenges,” IEEE Commun.
Surv. Tutor., vol. 17, no. 2, pp. 500–524, 2015.
[14] M. Tatar Mamaghani and Y. Hong, “Aerial intelligent reflecting surface-
enabled terahertz covert communications in beyond-5G internet of
things,” IEEE Internet Things J., vol. 9, no. 19, pp. 19 012–19 033, 2022.
[15] T. N. Do, G. Kaddoum, T. L. Nguyen, D. B. da Costa, and Z. J. Haas,
“Aerial reconfigurable intelligent surface-aided wireless communication
systems,” in Proc. IEEE 32nd Annual International Symposium on
Personal, Indoor and Mobile Radio Communications (PIMRC), 2021,
pp. 525–530.
[16] Y. Li, C. Yin, T. Do-Duy, A. Masaracchia, and T. Q. Duong, “Aerial
reconfigurable intelligent surface-enabled URLLC UAV systems,” IEEE
Access, vol. 9, pp. 140 248–140 257, 2021.
[17] S. Solanki, S. Gautam, S. K. Sharma, and S. Chatzinotas, “Ambient
backscatter assisted co-existence in aerial-IRS wireless networks,” IEEE
Open J. Commun. Soc., vol. 3, pp. 608–621, 2022.
[18] A. Pitilakis, D. Tyrovolas, P.-V. Mekikis, S. A. Tegos, A. Papadopoulos,
A. Tsioliaridou, O. Tsilipakos, D. Manessis, S. Ioannidis, N. V. Kan-
tartzis, I. F. Akyildiz, and C. K. Liaskos, “On the mobility effect in
UAV-mounted absorbing metasurfaces: A theoretical and experimental
study,” IEEE Access, vol. 11, pp. 79 777–79 792, 2023.
[19] H. Mei, K. Yang, Q. Liu, and K. Wang, “3D-trajectory and phase-
shift design for RIS-assisted UAV systems using deep reinforcement
learning,” IEEE Trans. Veh. Technol., vol. 71, no. 3, pp. 3020–3029,
2022.
[20] H. Long, M. Chen, Z. Yang, B. Wang, Z. Li, X. Yun, and M. Shikh-
Bahaei, “Reflections in the sky: Joint trajectory and passive beamforming
design for secure UAV networks with reconfigurable intelligent surface,”
2020.
[21] B. Duo, M. He, Q. Wu, and Z. Zhang, “Joint dual-UAV trajectory and
RIS design for ARIS-assisted aerial computing in IoT,” IEEE Internet
Things J., pp. 1–1, 2023.
[22] M. Hua, Y. Wang, Z. Zhang, C. Li, Y. Huang, and L. Yang, “Outage
probability minimization for low-altitude UAV-enabled full-duplex mo-
bile relaying systems,” China Communications, vol. 15, no. 5, pp. 9–24,
2018.
[23] D. De Paiva Mucin, D. P. M. Osorio, and E. E. B. Olivo, “Wireless-
powered full-duplex UAV relay networks over FTR channels,” IEEE
Open J. Commun. Soc., vol. 2, pp. 2205–2218, 2021.
[24] L. Zhu, J. Zhang, Z. Xiao, X. Cao, X.-G. Xia, and R. Schober,
“Millimeter-wave full-duplex uav relay: Joint positioning, beamforming,
and power control,” IEEE J. Sel. Areas Commun., vol. 38, no. 9, pp.
2057–2073, 2020.
[25] B. Duo, Q. Wu, X. Yuan, and R. Zhang, “Energy efficiency maximiza-
tion for full-duplex UAV secrecy communication,” IEEE Trans. Veh.
Technol., vol. 69, no. 4, pp. 4590–4595, 2020.
[26] B. Li, S. Zhao, R. Zhang, and L. Yang, “Full-duplex UAV relaying for
multiple user pairs,” IEEE Internet Things J., vol. 8, no. 6, pp. 4657–
4667, 2021.
[27] W. Wang, N. Qi, L. Jia, C. Li, T. A. Tsiftsis, and M. Wang, “Energy-
efficient UAV-relaying 5G/6G spectrum sharing networks: Interference
coordination with power management and trajectory design,” IEEE Open
J. Commun. Soc, vol. 3, pp. 1672–1687, 2022.
[28] D. Tyrovolas, P.-V. Mekikis, S. A. Tegos, P. D. Diamantoulakis, C. K.
Liaskos, and G. K. Karagiannidis, “Energy-aware design of UAV-
mounted RIS networks for IoT data collection,” IEEE Trans. Commun.,
vol. 71, no. 2, pp. 1168–1178, 2023.
[29] M. H. N. Shaikh, V. A. Bohara, A. Srivastava, and G. Ghatak, “In-
telligent reflecting surfaces versus full-duplex relaying: Performance
comparison for non-ideal transmitter case,” in Proc. IEEE 32nd An-
nual International Symposium on Personal, Indoor and Mobile Radio
Communications (PIMRC), 2021, pp. 513–518.
[30] Y. Xiao, D. Tyrovolas, S. A. Tegos, P. D. Diamantoulakis, Z. Ma, L. Hao,
and G. K. Karagiannidis, “Solar powered UAV-mounted RIS networks,”
IEEE Commun. Lett., vol. 27, no. 6, pp. 1565–1569, 2023.
[31] C. Y. Goh, C. Y. Leow, and R. Nordin, “Energy efficiency of
unmanned aerial vehicle with reconfigurable intelligent surfaces: A
comparative study,” Drones, vol. 7, no. 2, 2023. [Online]. Available:
https://www.mdpi.com/2504-446X/7/2/98
[32] P.-V. Mekikis and A. Antonopoulos, “Breaking the boundaries of aerial
networks with charging stations,” in Proc. IEEE International Confer-
ence on Communications (ICC), 2019, pp. 1–6.
[33] A. Talebi and W. A. Krzymien, “Multiple-antenna multiple-relay co-
operative communication system with beamforming,” in Proc. IEEE
Vehicular Technology Conference, 2008, pp. 2350–2354.
[34] J. Ma, C. Huang, and Q. Li, “Energy efficiency of full- and half-duplex
decode-and-forward relay channels,” IEEE Internet Things J., vol. 9,
no. 12, pp. 9730–9748, 2022.
"
"In-context learning (ICL) has consistently demonstrated superior performance over zero-shot performance in large language models (LLMs). However, the understanding of the dynamics of ICL and the aspects that influence down-stream performance remains limited, especially for natural language generation (NLG) tasks. This work aims to address this gap by investigating the ICL capabilities of LLMs and studying the impact of different aspects of the in-context demonstrations for the task of machine translation (MT). Our preliminary investigations aim to discern whether in-context learning (ICL) is predominantly influenced by demonstrations or instructions by applying diverse perturbations to in-context demonstrations while preserving the task instruction. We observe varying behavior to perturbed examples across different model families, notably with BLOOM-7B derivatives being severely influenced by noise, whereas Llama 2 derivatives not only exhibit robustness but also tend to show enhancements over the clean baseline when subject to perturbed demonstrations. This suggests that the robustness of ICL may be governed by several factors, including the type of noise, perturbation direction (source or target), the extent of pretraining of the specific model, and fine-tuning for down-stream tasks if applicable. Further investigation is warranted to develop a comprehensive understanding of these factors in future research.","In recent times, large language models (LLMs) have showcased impressive performance over various natural language processing tasks. In-context learning (ICL), a technique that involves conditioning LLMs on a few hand-crafted task-specific demonstrations, has emerged as an efficient and versatile way to leverage their capabilities for diverse downstream tasks. ICL has been extensively employed in natural language understanding (NLU) scenarios, achieving state-of-the-art results. Studies have demonstrated the dependency of NLU model predictions on the in-context labels. However, it remains unclear to what extent ICL is directly influenced by the demonstrations or whether the instructions provided in the prompt play a more prominent role. Furthermore, the dynamics of ICL in natural language generation (NLG) tasks, where controlling the generation is more challenging, are not yet fully understood. In this work, we investigate the ICL capabilities of LLMs in the context of machine translation (MT), aiming to elucidate factors that influence their performance.","Prior research has explored various aspects of ICL. While some studies suggest that the correctness of output labels in demonstrations has limited impact, others have observed that input-label mapping can introduce variance in downstream performance. Additionally, the influence of semantic priors from pre-trained language models on ICL was found to be significant, with larger models exhibiting stronger biases. Despite these contributions, a comprehensive understanding of ICL dynamics remains elusive, particularly for NLG tasks like MT.nan","To investigate the factors impacting ICL in LLMs, we designed a series of experiments. We selected a diverse set of pre-trained models, including BLOOM, BLOOMZ, Llama 2 and ALMA, and evaluated their performance on the task of MT across multiple language pairs. To assess the robustness of ICL to input perturbations, we employed a variety of noise injection techniques, including span noise, OCR-like noise, word order variation, word duplication, and punctuation manipulation. By applying these perturbations to the in-context demonstrations while maintaining a consistent instruction format, we aimed to isolate the effects of noisy demonstrations on ICL performance.","Our experiments revealed distinct behaviors among different model families in response to the applied perturbations. Models based on BLOOM exhibited higher sensitivity to noisy demonstrations compared to Llama 2-based models. Interestingly, Llama 2 models demonstrated not only robustness to perturbed demonstrations but also improvements over the clean baseline in certain scenarios. These observations suggest that the ICL robustness of LLMs is influenced by several factors, including model architecture, pre-training data, and fine-tuning for specific tasks. A more nuanced understanding of these factors is crucial for designing effective ICL strategies and mitigating the impact of noisy demonstrations.","Our study provides insights into the ICL capabilities of LLMs in the context of MT, highlighting factors that affect their robustness to noisy demonstrations. The findings emphasize the need for model-specific robustness tests and underscore the limitations of generalizing robustness findings across different model families. Our work serves as a foundation for future research, guiding investigations into the interplay between in-context demonstrations, semantic priors, and task instructions in ICL, with the aim of developing more robust and controllable NLG models.",An Empirical Analysis of In-context Learning Abilities of LLMs for MT,"Pranjal A. Chitale, Jay Gala, Varun Gumma, Mitesh M. Khapra, Raj Dabre","An Empirical Analysis of In-context Learning Abilities of LLMs for MT
Pranjal A. Chitale1,2∗ Jay Gala1∗ Varun Gumma3 Mitesh M. Khapra1,2 Raj Dabre4
1Nilekani Centre at AI4Bharat 2IIT Madras 3Microsoft Corporation
4National Institute of Information and Communications Technology, Kyoto, Japan
Abstract
In-context learning (ICL) has consistently
demonstrated superior performance over zero-
shot performance in large language models
(LLMs). However, the understanding of the
dynamics of ICL and the aspects that influence
downstream performance remains limited, es-
pecially for natural language generation (NLG)
tasks.
This work aims to address this gap
by investigating the ICL capabilities of LLMs
and studying the impact of different aspects of
the in-context demonstrations for the task of
machine translation (MT). Our preliminary in-
vestigations aim to discern whether in-context
learning (ICL) is predominantly influenced by
demonstrations or instructions by applying di-
verse perturbations to in-context demonstra-
tions while preserving the task instruction. We
observe varying behavior to perturbed exam-
ples across different model families, notably
with BLOOM-7B derivatives being severely
influenced by noise, whereas Llama 2 deriva-
tives not only exhibit robustness but also tend
to show enhancements over the clean baseline
when subject to perturbed demonstrations. This
suggests that the robustness of ICL may be gov-
erned by several factors, including the type of
noise, perturbation direction (source or target),
the extent of pretraining of the specific model,
and fine-tuning for downstream tasks if appli-
cable. Further investigation is warranted to de-
velop a comprehensive understanding of these
factors in future research.
1
Introduction
Large Language Models can be used as a single ef-
fective solution for a diverse range of downstream
tasks via in-context learning (Brown et al., 2020;
Dong et al., 2023; Liu et al., 2023; OpenAI et al.,
2023; Chowdhery et al., 2022). This is an inference-
only procedure that involves conditioning on a few
handcrafted task-specific demonstrations consist-
ing of input-output pairs as a part of the prompt
∗Equal contribution
before the actual test example. Although several
works use in-context learning as an effective way of
gaining good performance on tasks, there is limited
understanding of how it works and which aspects
of the demonstrations contribute to end task per-
formance, whether all examples contribute equally
or not, and whether the end generation can be con-
trolled by in-context examples, whether in-context
exemplars can be an effective way to control pre-
training biases observed in models. While numer-
ous studies employ in-context learning as an ef-
fective method for achieving proficient task perfor-
mance, there remains a limited understanding of its
underlying mechanisms.
Min et al. (2022) showed that the correctness of
the output labels in the demonstrations has limited
impact on the downstream performance for Lan-
guage Understanding (NLU) tasks. However, Yoo
et al. (2022); Kossen et al. (2023) demonstrate that
the model’s predictions are dependent on in-context
labels. Wei et al. (2023) also shows that In-Context
Learning (ICL) can override semantic priors for
larger models even in the case of semantically unre-
lated input-label mappings. However, these studies
are restricted to NLU tasks which are discrimina-
tive and the label space is limited. For NLG tasks
where controlling the generation is harder, there is
limited exploration of these dynamics rendering the
open question of whether ICL is a mere contextual-
ization tool that elicits the appropriate set of memo-
rized knowledge or whether ICL can be used as an
effective means to control the generations. There
exists a lack of clarity or consensus regarding the
distinct contributions of various aspects of demon-
strations to end task performance, the individual
impact of each demonstration on performance, and
the potential for regulating end generation through
contextual examples. Additionally, exploring the
effectiveness of in-context exemplars as a means
to regulate pre-training biases in models warrants
further investigation.
arXiv:2401.12097v1  [cs.CL]  22 Jan 2024
Our work aims to bridge the gap by consider-
ing the task of machine translation and providing
a comprehensive analysis of the robustness of in-
context learning, specifically delving into its impact
on NLG tasks, taking Machine Translation as a use
case. Although Raunak et al. (2023) conducted
limited exploration across the machine translation
task, it was on a limited set of attacks and language
pairs, and the aforementioned work did not focus
on understanding to what extent ICL be used to
control the generations (translations). In this work,
we not only expand the scope of the above work by
focusing on conducting a meticulous examination
of LLMs across different scales to elucidate the
sensitivity of LLMs towards the in-context demon-
strations (using a diverse set of attacks focusing on
lexical, syntactic, and semantic properties) but also
aim to understand and quantify the degree to which
the generations of the LLMs can be improved or
controlled via in-context learning.
2
Related Works
This work lies at the intersection of in-context learn-
ing, model robustness, and machine translation.
In-context Learning: Large Language Models
have demonstrated strong performance in various
downstream tasks through supervised fine-tuning
on task-specific data (Devlin et al., 2019; Raffel
et al., 2020; Lewis et al., 2020; Radford et al.,
2019). Recently, instruction-tuning (Ouyang et al.,
2022) has emerged as a special variant of super-
vised fine-tuning, where models are fine-tuned on
instruction data to enhance task-level performance
and enforce alignment with human preferences.
However, collecting and generating training data
at optimal scales for every task may be infeasi-
ble. Additionally, due to computational constraints,
fine-tuning these LLMs, even via PEFT-based ap-
proaches (Houlsby et al., 2019; Pfeiffer et al., 2021;
Hu et al., 2022), may not be feasible. In contrast,
in-context learning (Brown et al., 2020; Dong et al.,
2023; Liu et al., 2023; OpenAI et al., 2023; Chowd-
hery et al., 2022) is a training-free method that
has emerged as a more cost-effective approach,
especially given the increasing size of LLMs (≥
7B parameters). ICL aims to implicitly learn the
task-solving abilities through a few hand-crafted
demonstrations. These emergent capabilities (Wei
et al., 2022; Lu et al., 2023) are driven by the dis-
tribution of the pretraining data (Chan et al., 2022).
Intuitively, ICL can be considered analogous to per-
forming a gradient descent implicitly during the in-
ference (Akyürek et al., 2023; Li et al., 2023; Zhang
et al., 2023b). Vilar et al. (2023) explored few-
shot prompting strategies for MT task on PaLM
(Chowdhery et al., 2022) suggesting that the down-
stream MT performance is largely correlated with
the quality of in-context examples. Subsequently,
several recent works have explored ICL capabil-
ities of LLMs for the MT task (Robinson et al.,
2023; Zhang et al., 2023a; Zhu et al., 2023). Re-
cently, Lin et al. (2023) have recently shown that
the alignment of instruction-tuned (or SFT) models
is superficial, and it is possible to elicit similar be-
havior in base models just by a few static stylistic
examples and system prompts by leveraging ICL
capabilities.
Factors impacting in-context learning: Several
aspects of the demonstrations in an in-context setup
such as input distribution, output distribution, or
input-output alignment can play a significant role
in the ICL performance of LLMs. Min et al. (2022)
demonstrated that label correctness (output dis-
tribution) in the demonstrations has limited sig-
nificance for classification tasks. However, Yoo
et al. (2022) countered this viewpoint, asserting
that ICL sensitivity to input-label mapping can in-
troduce variance in downstream performance, and
the findings of Min et al. (2022) may lack gener-
alizability across tasks. Furthermore, Wei et al.
(2023) showed that ICL can override semantic pri-
ors, being influenced even by semantically unre-
lated input-label mappings, especially in the case
of larger models. While prior works (Min et al.,
2022; Wei et al., 2023; Kossen et al., 2023) have
explored various aspects of in-context learning for
NLU tasks, it remains crucial to investigate the
generalizability of these findings for NLG tasks.
Raunak et al. (2023), in one of the initial explo-
rations into NLG tasks such as MT, explored the
impact of syntactic perturbations (e.g. jumbling
alignment, shuffling order, etc.) on demonstrations
across three Indo-European language pairs. Their
findings suggest that the distribution of target text
significantly impacts GPT-3 results in comparison
to the source text distribution. Our work builds
upon the above by incorporating a diverse array
of perturbations, including lexical, syntactic, and
semantic variations. The objective of the perturba-
tions we introduce is to closely mirror real-world
scenarios wherein the in-context demonstrations
are obtained from a noisy web data store, partic-
ularly relevant for low-resource languages where
there are little to no hand-crafted demonstrations
for every task. Furthermore, our study considers
a broader range of language families, including
Dravidian, Indo-Aryan, and Indo-European, con-
tributing to a more comprehensive understanding of
ICL dynamics across different linguistic contexts.
Chen et al. (2023b) observed an inverse relation-
ship between the sensitivity and accuracy of Lan-
guage Models (LLMs) when perturbing either the
instructions within demonstrations or the order of
demonstrations. Our work differs from the above
by applying perturbations to the demonstrations
while keeping the instruction as it is.
Perturbation and Robustness: Text perturbation
attacks (Xu et al., 2021; Moradi and Samwald,
2021; Zhang et al., 2022; Niu et al., 2020; Sali-
nas and Morstatter, 2024) are frequently employed
to assess the resilience of models to alterations in
the input text. Given the inherent noise in real-
world text, there is a need to gauge the robustness
and sensitivity of models to perturbed texts. It
is essential to differentiate perturbations from ad-
versarial attacks (Michel et al., 2019; Garg and
Ramakrishnan, 2020; Zhang et al., 2021), as the lat-
ter are designed to deliberately deceive or compel
the model into manifesting unintended behaviors,
whereas perturbations entail minor modifications to
inputs intended to simulate the presence of noise in
a real-world setting. Typically, to assess the robust-
ness of NMT models, various perturbation attacks
are applied either at the word-level (Zhao et al.,
2018; Cheng et al., 2020) or the character-level
(Belinkov and Bisk, 2018; Ebrahimi et al., 2018;
Formento et al., 2023). These perturbations may ex-
hibit non-linguistic characteristics, such as random
injections, or be linguistically aware, incorporat-
ing strategies like the introduction of frequently
confused or conflated characters. Additionally, per-
turbations may involve random removal or addition
of punctuations (Formento et al., 2023). Closest
to our work is the work by Moradi and Samwald
(2021), however, they applied word and character-
based attacks to the inputs of LMs to study their
robustness. In contrast, our work differs by apply-
ing these perturbations to demonstrations, leaving
the test examples unperturbed.
3
Methodology
In this section, we describe the various perturba-
tion methods we used to introduce noise into the
in-context demonstrations to understand the robust-
ness of different LLMs for the task of MT. Our
perturbation methods influence the lexical (alter-
ations to individual words), syntactic (alterations
to structure or ordering of words), and semantic
(alterations to meaning) attributes of the in-context
demonstrations.
Span Noise:
This perturbation involves modifica-
tion of random contiguous segments of the original
text to introduce noise by string operations such
as deletion, insertion, or replacement of charac-
ters within the selected spans similar to Maurya
et al. (2023) and Joshi et al. (2020). Based on the
noise percentage chosen, we consider the number
of characters to be perturbed until the budget is ex-
hausted. We uniformly select 1 to 3-gram character
spans and the selected span is then either deleted
or replaced by a single uniformly sampled char-
acter from the character set of the corresponding
language with equal probability and the number
of characters removed or altered is then subtracted
from the noise budget.
OCR:
This perturbation involves randomly se-
lecting words from the original text and perform-
ing operations such as fusion or split to simulate
noise which might commonly be introduced in the
pipelines involving OCR systems. The noise per-
centage controls the extent of the perturbation, de-
termining the number of words subjected to manip-
ulation. This perturbation involves a random choice
of either a fusion operation, which concatenates a
selected word with the next one, or a split opera-
tion, which breaks a selected word into two parts at
a uniformly chosen split point in the selected word.
Word Ordering:
This perturbation introduces
controlled permutations to disrupt the original word
order of some portions of the original text similar
to Raunak et al. (2023). Based on the noise budget,
a set of words from the original text is uniformly
sampled for reordering by generating a new order
for these words. It is important to note that this
perturbation can have different implications across
different languages as few languages have free-
form word order while other languages are more
rigid in terms of word order.
Word Duplication:
This perturbation introduces
some redundancy into the original text without al-
tering its semantics by introducing repetition to
certain words. Based on the noise percentage, a
Perturbation Method
Lexical
Syntactic
Semantic
Example
Clean
Wow! That place is so wonderful, and I would
love to go there again.
Span Noise
✓
✓
×
Wow! That place is po wonde9l, a I uld
love to go thyre again.
OCR
✓
×
×
Wow!That place isso wonderful, and I would
lo ve to go there again .
Word Ordering
×
✓
✓
Wow! and place is so to would I That wonderful,
love go there again.
Word Duplication
✓
×
✓
Wow! That place is is so so wonderful, and I I would
love to go there again. again.
Punctuationadd
✓
✓
✓
Wow! That% place is so wonderful, and I would""
love. to go there again.
Punctuationdrop
✓
✓
✓
Wow That place is so wonderful, and I would
love to go there again
Table 1: Categorization of different perturbation methods for the different attributes.
specified number of words are uniformly chosen
for duplication, followed by identifying the occur-
rences of these chosen words and duplicating them
at random positions within the original text.
Punctuation:
This perturbation introduces noise
by altering the syntactic cues and rhythm of the
original text, by either inserting or deleting existing
punctuations present in the original text. In the case
of the addition of punctuation, we uniformly ap-
pend punctuation to the words that do not contain
any punctuation based on the noise budget. Sim-
ilarly, in the case of deletion of punctuation, we
uniformly drop the punctuation characters from the
original text based on the noise budget.
Table 1 provides a categorization of various per-
turbation methods considered as a part of this study
with regards to the attributes it impacts.
4
Experimental setup
This section describes the different pre-trained
models,
evaluation
benchmarks,
in-context
prompts, and decoding hyperparameters used for
our current study.
Models:
We conduct our experimentation on off-
the-shelf open-source pre-trained LLMs like Llama
2 (Touvron et al., 2023), BLOOM (Fan et al., 2022)
and their corresponding instruction-tuned variants
such as BLOOMZ (Muennighoff et al., 2023) and
ALMA (Xu et al., 2023). First, models like Llama 2
(Touvron et al., 2023) are prominently pre-trained
with English or European languages and its tok-
enizer has poor fertility rates on Indic languages.
Further, Ahuja et al. (2023) showed that Llama 2
70B (Touvron et al., 2023) exhibit poor in-context
MT performance on Indian languages. As a re-
sult, we evaluate Llama 2 (Touvron et al., 2023)
and ALMA (Xu et al., 2023) on 3 European lan-
guages like German, Czech, and Russian which
share the same script as the pre-training data. Fur-
thermore, we evaluate BLOOM (Fan et al., 2022)
and BLOOMZ (Muennighoff et al., 2023) on 3
Indian languages like Bengali, Hindi, and Tamil.
Due to computational constraints, we restrict our
experimentation to 7B models for a fair comparison
across the base and instruction-tuned variants. We
adopt a greedy decoding strategy without sampling
to generate a maximum of 256 tokens with early
stopping to ensure reproducibility in our results.
Benchmarks:
FLORES-200 (Goyal et al., 2022;
Costa-jussà et al., 2022) is a N-way parallel bench-
mark that covers 200 languages, including vari-
ous low-resource languages. However, it is impor-
tant to note that data from FLORES-200 (Goyal
et al., 2022; Costa-jussà et al., 2022) has been con-
sumed in the multitask fine-tuning xP3 mixture of
BLOOMZ (Muennighoff et al., 2023), suggesting
data contamination. Therefore, we do not con-
sider FLORES-200 for the evaluation of Indian
languages and instead, we use the newly released
IN22-Gen (Gala et al., 2023). IN22-Gen is a 22-
way parallel benchmark for Indian languages that
focuses on demography-specific content and serves
as an unbiased evaluation set for both BLOOM and
BLOOMZ models.
Prompt Details:
Vilar et al. (2023) have demon-
strated that the models are typically unaffected
by the few-shot prompt template for the task of
MT. However, to maintain consistency in terms
of evaluation, we follow the same prompt that was
used for fine-tuning ALMA (Xu et al., 2023) across
all different models considered in this work (see
Figure 1). However, we find Llama 2 Chat mod-
els do not perform well with the above standard
prompt in the monolithic format, which is simi-
lar to Sclar et al. (2023). Therefore, we resort
to passing each demonstration as a user-assistant
turn and task description in the system prompt in
the chat format. We use uniformly sampled high-
quality translation pairs from the FLORES-200
dev set to compose the in-context demonstrations
with k = {1, 4, 8} across various noise scales with
δ = {0, 0.1, 0.25, 0.5, 0.75} for evaluating LLMs.
Translate
this
from
{src_lang}
into
{tgt_lang}:
{src_lang}: {src_text
{tgt_lang}: {tgt_text}
· · ·
· · ·
{src_lang}: {src_text}
{tgt_lang}:
Figure 1: Prompt for translation
Evaluation:
We use the ChrF++ metric (Popovi´c,
2017) as the primary evaluation metric for our anal-
ysis. Sai B et al. (2023) showed that it demon-
strates a strong correlation with human judgments
over the other string-based MT evaluation metrics.
We use the SacreBLEU (Post, 2018) library to en-
sure standardization in terms of tokenization by
using the built-in mteval-v13a tokenizer to ensure
a fair comparison. In the future, we plan to include
model-based metrics for further analysis.
5
Results and Analysis
Figure 2 shows the impact of various perturbation
attacks described in Section 3 across two different
model families: multilingual BLOOM and promi-
nently English Llama 2.
Findings about BLOOM family:
In both the
En-XX and XX-En directions, and across both per-
turbation categories, our observations indicate that
the task-specific fine-tuned BLOOM-7B-FT model
exhibits superior robustness. It is followed by the
BLOOM-7B model, while the multitask fine-tuned
BLOOMZ-7B model displays high vulnerability to
various perturbation attacks. A comparative analy-
sis of source and target-side perturbations suggests
that target-side perturbations are generally more
detrimental than source-side perturbations for all
BLOOM model variants in both directions. Specif-
ically, the span-noise attack proves to be the most
detrimental, causing significant performance drops
in the BLOOM and BLOOM-Z models, with a
comparatively lower drop observed in the case of
the BLOOM-7B-FT model.
BLOOM-7B model exhibits robustness to vari-
ous perturbations when applied on the source side,
except span noise. Conversely, the BLOOMZ-7B
model is highly sensitive to source-side perturba-
tions, particularly impacted by span noise. The
BLOOM-7B and BLOOM-7B-FT models demon-
strate comparable robustness, with span and OCR
attacks proving most detrimental. We observe vary-
ing trends across translation directions in case of
target-side perturbation. In the En-XX translation
direction, both the BLOOM-7B and BLOOMZ-7B
models exhibit susceptibility to various perturba-
tions, except for punctuation drop. Conversely, in
the XX-En direction, the BLOOM-7B model is
vulnerable to almost all perturbation attacks, while
the BLOOMZ-7B model demonstrates notable ro-
bustness to all attacks apart from span noise. On
the other hand, the BLOOM-7B-FT model exhibits
robustness across all evaluated attacks. Further-
more, considering the free-form word order in In-
dian languages, the anticipation that word order
attacks on the Indic side would minimally impact
performance is contradicted by our observation of
sensitivity to this attack on the target side perturba-
tion in the En-XX direction. Notably, we observe
a high susceptibility to word duplication attacks in
BLOOM-7B models, despite the expected benign
nature of this attack as it introduces redundancy
without content deletions. Figures 3 to 5 in Ap-
pendix A provide fine-grained results across shots
and perturbation attack variants.
Findings about Llama 2 family:
Across vari-
ous perturbation categories, in both the En-XX and
XX-En directions, we find that all models demon-
strate strong robustness to various attacks, except
for susceptibility to the span noise attack on the
target side. This contrasts with the BLOOM family,
where the task-specific fine-tuned model BLOOM-
7B-FT proved to be the most robust. We posit that
the robustness observed in Llama 2 7B model and
20
10
0
10
20
En-XX (Source)
En-XX (Target)
span
ocr
order
dup
add
drop
10
0
10
20
XX-En (Source)
span
ocr
order
dup
add
drop
XX-En (Target)
Types of Perturbations
Mean % Change in ChrF++ score
BLOOM-7B
BLOOMZ-7B
BLOOM-7B-FT
Llama-2-7B
Llama-2-7B-Chat
ALMA-7B
Figure 2: Mean percent change in ChrF++ score for each attack relative to the 0 noise baseline, for each model
across both translation directions (En-XX and XX-En) and both perturbation directions. Scores are averaged across
shots and noise percentages (δ). “Span"" denotes span noise, “order"" represents word order attack, “dup"" signifies
word duplication attack, “add"" indicates punctuation addition attack, and ""drop"" signifies punctuation removal
attack. Note: In certain cases, scores are bounded within minimum and maximum values for clarity in depicting
overarching trends.
its derivatives may be attributed to the extensive
pre-training they underwent and subsequent instruc-
tion tuning if applicable. A comparative analysis of
source and target-side perturbations generally high-
lights that target-side perturbations have a higher
impact. Specifically, the span-noise attack signifi-
cantly impacts the performance of the ALMA and
Llama 2 7B-Chat models, while OCR attacks ex-
hibit minimal impact across both translation and
perturbation directions. A surprising observation
in this setup is that the Llama 2 7B model when
subjected to various attacks demonstrates strong im-
provements in performance over the clean baseline
across both translation and perturbation directions.
Punctuation add and drop attacks have variable im-
pact with substantial improvements observed in the
case of Llama and Llama-Chat with punctuation
add attacks and minimal improvement in ALMA.
Analyzing source side perturbations in the XX-En
translation direction, all models demonstrate ro-
bustness to word order, word duplication, punctua-
tion add, and punctuation drop attacks, with Llama
and Llama-Chat displaying marginal improvements
in these scenarios. Under target side perturbation
in the XX-En translation direction, ALMA exhibits
severe susceptibility to span noise, while Llama-
Chat is moderately affected, and Llama 2 shows
slight improvement. Figures 6 to 8 in Appendix A
provide fine-grained results across shots and per-
turbation attack variants.
Our findings from BLOOM models align with
the assertions of Raunak et al. (2023), emphasizing
that these models are more sensitive to target-side
perturbations. Notably, our study offers a more
comprehensive evaluation, encompassing diverse
translation directions and perturbation types. While
we observe that in general target-side perturbations
are more detrimental, our findings reveal instances
where source-side attacks, particularly span noise,
exert substantial impact. Upon comparing our ob-
servations from BLOOM and Llama 2 models with
the findings of Raunak et al. (2023), it becomes
apparent that the sensitivity of models to perturba-
tion direction varies among different model fam-
ilies. This discrepancy may be attributed to fac-
tors such as pre-training data, training steps, archi-
tecture depth and width, and the mixture of SFT
data (wherever applicable). Further investigation
is imperative to comprehensively understand the
role of these factors in determining model robust-
ness, underscoring the necessity for a nuanced and
model-specific approach to robustness evaluation
and limited generalizability of findings from one
model to another.
6
Conclusion
ICL dynamics in LLMs are influenced by the inter-
play between context demonstrations and semantic
priors from parametric memory, and the degree of
interplay varies depending on the task. Prior stud-
ies (Min et al., 2022; Yoo et al., 2022; Wei et al.,
2023; Kossen et al., 2023) primarily focus on un-
derstanding these dynamics for NLU tasks, with
limited applicability to NLG tasks. In this study, we
investigated the sensitivity of LLMs to the quality
of in-context examples in the context of MT across
two model families: BLOOM and Llama 2. We
examine whether ICL is primarily driven by demon-
strations or if the clean instructions and pre-trained
knowledge can be leveraged by the model to miti-
gate the impact of noisy in-context examples. Our
findings indicate that the shifts in the output dis-
tribution of demonstrations significantly affect the
downstream MT performance, however, the shifts
in the input distribution cannot also be disregarded
which is contrary to Raunak et al. (2023). Notably,
generalizing the robustness of ICL across various
models proves challenging as there are several as-
pects at play beyond in-context examples which
needs to be further investigated. We advocate for
conducting specific robustness tests for the specific
model of interest, recognizing that findings from
other model families may not always universally
hold true.
7
Limitations and Future Work
Heterogeneous perturbations:
Our current ex-
periments only focused on homogenous perturba-
tions, where all in-context examples are equally
considered and applied uniform perturbation levels.
However, the assumption of models treating all ex-
amples equivalently is questionable as reflected by
findings of Yang et al. (2023) which suggest that
in-context examples might have varying relevance.
Thus, it becomes imperative to assess the models’
sensitivity to perturbation attacks that are more
heterogeneous and specifically target the most per-
tinent support samples.
Directionality of in-context examples:
The em-
phasis on target language distribution over source
language for in-context MT was highlighted by
Raunak et al. (2023) and is further reinforced by
our findings. Therefore, this warrants further ex-
ploration of whether the directionality of the in-
context examples influences the downstream MT
performance and whether target-original in-context
examples can elicit more fluent generations.
Non-linguistic perturbations:
The current ex-
periments were focused on only 5 non-linguistic
perturbations which conducted random textual at-
tacks which were mostly uniform and language-
independent. However, it would be more inter-
esting to explore more linguistically aware attack
types like causality alternation, entity, and number
replacements similar to Chen et al. (2023a).
Extending this study across different model
scales:
Our current investigation is limited only
to the 7B model scale due to computational con-
straints. However, exploring how the ICL capa-
bilities of different models vary with scales would
provide a more comprehensive understanding.
Understanding whether ICL is example-driven
or instruction-driven:
Our current experiments
focus on maintaining a consistent high-quality in-
struction while subjecting the in-context examples
to various attacks. An additional intriguing aspect
to explore is the model’s capability to comprehend
tasks through example-based learning when pro-
vided with random or noisy instructions.
This
investigation aims to understand whether ICL is
instruction-driven or example-driven or both.
8
Ethical Considerations
Potential toxic and hateful outputs: While we
have only focused on MT, it is not unlikely that our
attacks may lead to toxic and hateful content being
generated as a result of the model’s distributions
being perturbed.
Safety Circumvention and Jailbreaking Adver-
sarial perturbations might be able to circumvent
a model’s safety parameters and enable the gener-
ation of biased and harmful outputs. We plan to
release our perturbation scripts and resultant per-
turbed data for research, and we do not intend it to
be used to perform adversarial attacks in practice
intended to undo the security features, also known
as jailbreaking, of a model.
References
Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma,
Ishaan Watts, Ashutosh Sathe, Millicent Ochieng,
Rishav Hada, Prachi Jain, Maxamed Axmed, Ka-
lika Bali, and Sunayana Sitaram. 2023. Megaverse:
Benchmarking large language models across lan-
guages, modalities, models and tasks.
Ekin Akyürek, Dale Schuurmans, Jacob Andreas,
Tengyu Ma, and Denny Zhou. 2023. What learn-
ing algorithm is in-context learning? investigations
with linear models. In The Eleventh International
Conference on Learning Representations.
Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic
and natural noise both break neural machine transla-
tion. In International Conference on Learning Rep-
resentations.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners.
In Ad-
vances in Neural Information Processing Systems,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Stephanie C. Y. Chan, Adam Santoro, Andrew K.
Lampinen, Jane X. Wang, Aaditya Singh, Pierre H.
Richemond, Jay McClelland, and Felix Hill. 2022.
Data distributional properties drive emergent in-
context learning in transformers.
Mingda Chen, Kevin Heffernan, Onur Çelebi, Alexan-
dre Mourachko,
and Holger Schwenk. 2023a.
xSIM++: An improved proxy to bitext mining perfor-
mance for low-resource languages. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers),
pages 101–109, Toronto, Canada. Association for
Computational Linguistics.
Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKe-
own, and He He. 2023b. On the relation between
sensitivity and accuracy in in-context learning. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2023, pages 155–167, Singapore.
Association for Computational Linguistics.
Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang,
and Cho-Jui Hsieh. 2020. Seq2sick: Evaluating the
robustness of sequence-to-sequence models with ad-
versarial examples. In The Thirty-Fourth AAAI Con-
ference on Artificial Intelligence, AAAI 2020, The
Thirty-Second Innovative Applications of Artificial
Intelligence Conference, IAAI 2020, The Tenth AAAI
Symposium on Educational Advances in Artificial In-
telligence, EAAI 2020, New York, NY, USA, February
7-12, 2020, pages 3601–3608. AAAI Press.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways.
Marta R. Costa-jussà, James Cross, Onur Çelebi,
Maha Elbayad, Kenneth Heafield, Kevin Heffer-
nan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram
Sadagopan, Dirk Rowe, Shannon Spruit, Chau
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia
Gao, Vedanuj Goswami, Francisco Guzmán, Philipp
Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.
2022.
No language left behind: Scaling human-
centered machine translation.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong
Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and
Zhifang Sui. 2023. A survey on in-context learning.
Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018.
On adversarial examples for character-level neural
machine translation. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 653–663, Santa Fe, New Mexico, USA. Asso-
ciation for Computational Linguistics.
Angela Fan, Suzana Ilic, Thomas Wolf, and Matthias
Gallé, editors. 2022.
Proceedings of BigScience
Episode #5 – Workshop on Challenges & Perspec-
tives in Creating Large Language Models. Associa-
tion for Computational Linguistics, virtual+Dublin.
Brian Formento, Chuan Sheng Foo, Luu Anh Tuan, and
See Kiong Ng. 2023. Using punctuation as an ad-
versarial attack on deep learning-based NLP systems:
An empirical study. In Findings of the Association
for Computational Linguistics: EACL 2023, pages
1–34, Dubrovnik, Croatia. Association for Computa-
tional Linguistics.
Jay Gala, Pranjal A Chitale, A K Raghavan, Varun
Gumma, Sumanth Doddapaneni, Aswanth Kumar M,
Janki Atul Nawale, Anupama Sujatha, Ratish Pudup-
pully, Vivek Raghavan, Pratyush Kumar, Mitesh M
Khapra, Raj Dabre, and Anoop Kunchukuttan. 2023.
Indictrans2: Towards high-quality and accessible ma-
chine translation models for all 22 scheduled indian
languages. Transactions on Machine Learning Re-
search.
Siddhant Garg and Goutham Ramakrishnan. 2020. Bae:
Bert-based adversarial examples for text classifica-
tion.
In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-
Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-
ishnan, Marc’Aurelio Ranzato, Francisco Guzmán,
and Angela Fan. 2022. The Flores-101 evaluation
benchmark for low-resource and multilingual ma-
chine translation. Transactions of the Association for
Computational Linguistics, 10:522–538.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea Ges-
mundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for nlp.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. In International Conference on
Learning Representations.
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,
Luke Zettlemoyer, and Omer Levy. 2020. Spanbert:
Improving pre-training by representing and predict-
ing spans.
Jannik Kossen, Yarin Gal, and Tom Rainforth. 2023.
In-context learning learns label relationships but is
not conventional learning.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 7871–7880, Online. Association for Computa-
tional Linguistics.
Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopou-
los, and Samet Oymak. 2023. Transformers as al-
gorithms: Generalization and stability in in-context
learning.
Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu,
Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chan-
dra Bhagavatula, and Yejin Choi. 2023. The unlock-
ing spell on base llms: Rethinking alignment via
in-context learning.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2023. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ACM Comput. Surv., 55(9).
Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Har-
ish Tayyar Madabushi, and Iryna Gurevych. 2023.
Are emergent abilities in large language models just
in-context learning?
Kaushal Kumar Maurya, Rahul Kejriwal, Maunen-
dra Sankar Desarkar, and Anoop Kunchukuttan. 2023.
Utilizing lexical similarity to enable zero-shot ma-
chine translation for extremely low-resource lan-
guages.
Paul Michel, Xian Li, Graham Neubig, and Juan Pino.
2019. On evaluation of adversarial perturbations for
sequence-to-sequence models. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and
Short Papers), pages 3103–3114, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022. Rethinking the role of demonstrations:
What makes in-context learning work? In Proceed-
ings of the 2022 Conference on Empirical Methods in
Natural Language Processing, pages 11048–11064,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Milad Moradi and Matthias Samwald. 2021. Evaluating
the robustness of neural language models to input
perturbations. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1558–1570, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
Adam Roberts, Stella Biderman, Teven Le Scao,
M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-
ley Schoelkopf, Xiangru Tang, Dragomir Radev,
Alham Fikri Aji, Khalid Almubarak, Samuel Al-
banie, Zaid Alyafeai, Albert Webson, Edward Raff,
and Colin Raffel. 2023.
Crosslingual generaliza-
tion through multitask finetuning. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 15991–16111, Toronto, Canada. Association
for Computational Linguistics.
Xing Niu, Prashant Mathur, Georgiana Dinu, and Yaser
Al-Onaizan. 2020. Evaluating robustness to input
perturbations for neural machine translation. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 8538–
8544, Online. Association for Computational Lin-
guistics.
OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agar-
wal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-
ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,
Jake Berdine, Gabriel Bernadett-Shapiro, Christo-
pher Berner, Lenny Bogdonoff, Oleg Boiko, Made-
laine Boyd, Anna-Luisa Brakman, Greg Brockman,
Tim Brooks, Miles Brundage, Kevin Button, Trevor
Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
Chelsea Carlson, Rory Carmichael, Brooke Chan,
Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,
Ruby Chen, Jason Chen, Mark Chen, Ben Chess,
Chester Cho, Casey Chu, Hyung Won Chung, Dave
Cummings, Jeremiah Currier, Yunxing Dai, Cory
Decareaux, Thomas Degry, Noah Deutsch, Damien
Deville, Arka Dhar, David Dohan, Steve Dowl-
ing, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
Tyna Eloundou, David Farhi, Liam Fedus, Niko
Felix, Simón Posada Fishman, Juston Forte, Is-
abella Fulford, Leo Gao, Elie Georges, Christian
Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh,
Rapha Gontijo-Lopes, Jonathan Gordon, Morgan
Grafstein, Scott Gray, Ryan Greene, Joshua Gross,
Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse
Han, Jeff Harris, Yuchen He, Mike Heaton, Jo-
hannes Heidecke, Chris Hesse, Alan Hickey, Wade
Hickey, Peter Hoeschele, Brandon Houghton, Kenny
Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu
Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger
Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie
Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,
Ali Kamali, Ingmar Kanitscheider, Nitish Shirish
Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook
Kim, Christina Kim, Yongjik Kim, Hendrik Kirch-
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan
Leike, Jade Leung, Daniel Levy, Chak Ming Li,
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
Anna Makanju, Kim Malfacini, Sam Manning, Todor
Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott Mayer
McKinney, Christine McLeavey, Paul McMillan,
Jake McNeil, David Medina, Aalok Mehta, Jacob
Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Mossing, Tong Mu, Mira Murati, Oleg Murk, David
Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,
Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambat-
tista Parascandolo, Joel Parish, Emy Parparita, Alex
Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-
man, Filipe de Avila Belbute Peres, Michael Petrov,
Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-
ell, Alethea Power, Boris Power, Elizabeth Proehl,
Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,
Cameron Raymond, Francis Real, Kendra Rimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,
Girish Sastry, Heather Schmidt, David Schnurr, John
Schulman, Daniel Selsam, Kyla Sheppard, Toki
Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,
Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin
Sokolowsky, Yang Song, Natalie Staudacher, Fe-
lipe Petroski Such, Natalie Summers, Ilya Sutskever,
Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil
Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre-
ston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-
lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea Voss, Carroll Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,
CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-
ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,
Clemens Winter, Samuel Wolrich, Hannah Wong,
Lauren Workman, Sherwin Wu, Jeff Wu, Michael
Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-
ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong
Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret
Zoph. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback.
Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,
Kyunghyun Cho,
and Iryna Gurevych. 2021.
AdapterFusion: Non-destructive task composition
for transfer learning. In Proceedings of the 16th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Main Volume, pages
487–503, Online. Association for Computational Lin-
guistics.
Maja Popovi´c. 2017. chrF++: words helping charac-
ter n-grams. In Proceedings of the Second Confer-
ence on Machine Translation, pages 612–618, Copen-
hagen, Denmark. Association for Computational Lin-
guistics.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186–
191, Brussels, Belgium. Association for Computa-
tional Linguistics.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research,
21(140):1–67.
Vikas Raunak, Arul Menezes, and Hany Awadalla. 2023.
Dissecting in-context learning of translations in GPT-
3. In Findings of the Association for Computational
Linguistics: EMNLP 2023, pages 866–872, Singa-
pore. Association for Computational Linguistics.
Nathaniel Robinson, Perez Ogayo, David R. Mortensen,
and Graham Neubig. 2023. ChatGPT MT: Competi-
tive for high- (but not low-) resource languages. In
Proceedings of the Eighth Conference on Machine
Translation, pages 392–418, Singapore. Association
for Computational Linguistics.
Ananya Sai B, Tanay Dixit, Vignesh Nagarajan, Anoop
Kunchukuttan, Pratyush Kumar, Mitesh M. Khapra,
and Raj Dabre. 2023. IndicMT eval: A dataset to
meta-evaluate machine translation metrics for Indian
languages. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 14210–14228,
Toronto, Canada. Association for Computational Lin-
guistics.
Abel Salinas and Fred Morstatter. 2024. The butterfly
effect of altering prompts: How small changes and
jailbreaks affect large language model performance.
arXiv preprint arXiv: 2401.03729.
Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane
Suhr. 2023. Quantifying language models’ sensitiv-
ity to spurious features in prompt design or: How i
learned to start worrying about prompt formatting.
arXiv preprint arXiv: 2310.11324.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,
Viresh Ratnakar, and George Foster. 2023. Prompt-
ing PaLM for translation: Assessing strategies and
performance.
In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 15406–
15427, Toronto, Canada. Association for Computa-
tional Linguistics.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler,
E. Chi, Tatsunori Hashimoto, O. Vinyals, P. Liang,
J. Dean, and W. Fedus. 2022. Emergent abilities of
large language models. Trans. Mach. Learn. Res.
Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert
Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,
Da Huang, Denny Zhou, and Tengyu Ma. 2023.
Larger language models do in-context learning dif-
ferently.
Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Has-
san Awadalla. 2023. A paradigm shift in machine
translation: Boosting translation performance of
large language models.
Weiwen Xu, Ai Ti Aw, Yang Ding, Kui Wu, and Shafiq
Joty. 2021. Addressing the vulnerability of NMT
in input perturbations. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies: Industry Papers, pages 80–
88, Online. Association for Computational Linguis-
tics.
Zhe Yang, Damai Dai, Peiyi Wang, and Zhifang Sui.
2023. Not all demonstration examples are equally
beneficial: Reweighting demonstration examples for
in-context learning. In Findings of the Association
for Computational Linguistics: EMNLP 2023, pages
13209–13221, Singapore. Association for Computa-
tional Linguistics.
Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyun-
soo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee,
and Taeuk Kim. 2022. Ground-truth labels matter: A
deeper look into input-label demonstrations. In Pro-
ceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2422–
2437, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Biao Zhang, Barry Haddow, and Alexandra Birch.
2023a. Prompting large language model for machine
translation: A case study.
In Proceedings of the
40th International Conference on Machine Learning,
ICML’23. JMLR.org.
Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. 2023b.
Trained transformers learn linear models in-context.
Xinze Zhang, Junzhe Zhang, Zhenhua Chen, and Kun
He. 2021. Crafting adversarial examples for neural
machine translation. In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 1967–1977, Online. Association
for Computational Linguistics.
Yunxiang Zhang, Liangming Pan, Samson Tan, and Min-
Yen Kan. 2022. Interpreting the robustness of neural
NLP models to textual perturbations. In Findings of
the Association for Computational Linguistics: ACL
2022, pages 3993–4007, Dublin, Ireland. Association
for Computational Linguistics.
Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018.
Generating natural adversarial examples. In Inter-
national Conference on Learning Representations
(ICLR).
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,
Shujian Huang, Lingpeng Kong, Jiajun Chen, and
Lei Li. 2023. Multilingual machine translation with
large language models: Empirical results and analy-
sis. arXiv preprint arXiv: 2304.04675.
A
Additional Results
Figures 3 to 5 depict the trends in terms of
ChRF scores observed in BLOOM derivatives
while Figures 6 to 8 depict the same for Llama2-
derivatives when subject to different perturbation
attacks across both source and target side attacks
across both En-XX and XX-En directions.
20.0
22.5
25.0
27.5
30.0
span_noise
ocr
26
27
28
29
30
word_order
word_duplication
0
0.1
0.25
0.5
0.75
26
27
28
29
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(a) En-XX (Source) - 1 shot
5
10
15
20
25
30
span_noise
ocr
20
22
24
26
28
word_order
word_duplication
0
0.1
0.25
0.5
0.75
22
24
26
28
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(b) En-XX (Target) - 1 shot
20
30
40
span_noise
ocr
20
25
30
35
40
word_order
word_duplication
0
0.1
0.25
0.5
0.75
25
30
35
40
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(c) XX-En (Source) - 1 shot
10
20
30
40
span_noise
ocr
25
30
35
40
word_order
word_duplication
0
0.1
0.25
0.5
0.75
30
35
40
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(d) XX-En (Target) - 1 shot
Figure 3: Variation in ChrF++ scores across different attack types observed in BLOOM-7B derivatives when
prompted in a 1-shot manner.
10
15
20
25
30
span_noise
ocr
26
27
28
29
30
word_order
word_duplication
0
0.1
0.25
0.5
0.75
26
27
28
29
30
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(a) En-XX (Source) - 4 shot
10
20
30
span_noise
ocr
15
20
25
30
word_order
word_duplication
0
0.1
0.25
0.5
0.75
22
24
26
28
30
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(b) En-XX (Target) - 4 shot
10
20
30
40
span_noise
ocr
25
30
35
40
45
word_order
word_duplication
0
0.1
0.25
0.5
0.75
25
30
35
40
45
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(c) XX-En (Source) - 4 shot
10
20
30
40
span_noise
ocr
30
35
40
45
word_order
word_duplication
0
0.1
0.25
0.5
0.75
25
30
35
40
45
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(d) XX-En (Target) - 4 shot
Figure 4: Variation in ChrF++ scores across different attack types observed in BLOOM-7B derivatives when
prompted in a 4-shot manner.
5
10
15
20
25
30
span_noise
ocr
20
22
24
26
28
30
word_order
word_duplication
0
0.1
0.25
0.5
0.75
20
22
24
26
28
30
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(a) En-XX (Source) - 8 shot
5
10
15
20
25
30
span_noise
ocr
15
20
25
30
word_order
word_duplication
0
0.1
0.25
0.5
0.75
17.5
20.0
22.5
25.0
27.5
30.0
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(b) En-XX (Target) - 8 shot
0
10
20
30
40
span_noise
ocr
25
30
35
40
word_order
word_duplication
0
0.1
0.25
0.5
0.75
25
30
35
40
45
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(c) XX-En (Source) - 8 shot
10
20
30
40
span_noise
ocr
25
30
35
40
word_order
word_duplication
0
0.1
0.25
0.5
0.75
25
30
35
40
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(d) XX-En (Target) - 8 shot
Figure 5: Variation in ChrF++ scores across different attack types observed in BLOOM-7B derivatives when
prompted in a 8-shot manner.
10
20
30
40
50
span_noise
ocr
10
20
30
40
50
word_order
word_duplication
0
0.1
0.25
0.5
0.75
10
20
30
40
50
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(a) En-X - 1 shot - source
10
20
30
40
50
span_noise
ocr
10
20
30
40
50
word_order
word_duplication
0
0.1
0.25
0.5
0.75
20
30
40
50
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(b) En-X - 1 shot - target)
20
30
40
50
60
span_noise
ocr
30
40
50
60
word_order
word_duplication
0
0.1
0.25
0.5
0.75
30
40
50
60
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(c) X-En - 1 shot - Source
10
20
30
40
50
60
span_noise
ocr
20
30
40
50
60
word_order
word_duplication
0
0.1
0.25
0.5
0.75
30
40
50
60
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(d) X-En 1-shot Target
Figure 6: Variation in ChrF++ scores across different attack types observed in Llama2-7B derivatives when prompted
in a 1-shot manner.
10
20
30
40
50
span_noise
ocr
20
30
40
50
word_order
word_duplication
0
0.1
0.25
0.5
0.75
20
30
40
50
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(a) En-X - 4 shot - source
10
20
30
40
50
span_noise
ocr
20
30
40
50
word_order
word_duplication
0
0.1
0.25
0.5
0.75
30
40
50
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(b) En-X - 4 shot - target)
30
40
50
60
span_noise
ocr
30
40
50
60
word_order
word_duplication
0
0.1
0.25
0.5
0.75
30
40
50
60
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(c) X-En - 4 shot - Source
20
40
60
span_noise
ocr
30
40
50
60
word_order
word_duplication
0
0.1
0.25
0.5
0.75
30
40
50
60
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(d) X-En 4-shot Target
Figure 7: Variation in ChrF++ scores across different attack types observed in Llama2-7B derivatives when prompted
in a 4-shot manner.
20
30
40
50
span_noise
ocr
20
30
40
50
word_order
word_duplication
0
0.1
0.25
0.5
0.75
25
30
35
40
45
50
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(a) En-X - 8 shot - source
10
20
30
40
50
span_noise
ocr
20
30
40
50
word_order
word_duplication
0
0.1
0.25
0.5
0.75
30
35
40
45
50
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(b) En-X - 8 shot - target)
40
50
60
span_noise
ocr
40
45
50
55
60
65
word_order
word_duplication
0
0.1
0.25
0.5
0.75
30
40
50
60
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(c) X-En - 8 shot - Source
20
40
60
span_noise
ocr
30
40
50
60
word_order
word_duplication
0
0.1
0.25
0.5
0.75
30
40
50
60
punctuation_add_attack
0
0.1
0.25
0.5
0.75
punctuation_drop_attack
Noise percentage
ChrF++ score
(d) X-En 8-shot Target
Figure 8: Variation in ChrF++ scores across different attack types observed in Llama2-7B derivatives when prompted
in a 8-shot manner.
"
"This research study explores the computational complexity of the k-clique problem, which involves identifying complete subgraphs (cliques) of size k within a given graph. The authors demonstrate that determining the presence or absence of a k-clique requires a minimum number of monotone, constant-depth, and polynomial-sized circuits, particularly for large k values.","The study of circuit complexity aims to investigate the amount of computational resources (such as circuit size and depth) required to solve various computational problems. The authors focus on monotone circuits, a restricted class of circuits with simpler structure, and investigate the lower bounds for the k-clique problem under these constraints.",nanPrevious research has established lower bounds on the circuit complexity of the k-clique problem for both general and monotone circuits. The current study improves upon these bounds for monotone circuits.,The authors present a new monotone switching lemma specifically designed for cliques. This lemma enables them to convert a CNF formula (where clauses are connected by AND operations) to a DNF formula (where clauses are connected by OR operations) while losing only a negligible number of cliques.,"Utilizing the new switching lemma, the authors demonstrate that calculating k-clique on n vertex graphs necessitates at least 2n/4k monotone, constant-depth, and polynomial-sized circuits for sufficiently large values of k. This bound highlights the computational complexity of the problem and provides insights into the limitations of monotone circuits.","The study's findings contribute to the understanding of circuit complexity and offer a stepping stone towards addressing the broader question of whether the k-clique problem can be solved using a smaller number of monotone, constant-depth, and polynomial-sized circuits.",CLIQUE as an AND of Polynomial-Sized Monotone Constant-Depth Circuits,Levente Bodnár,"arXiv:2401.12094v1  [cs.CC]  22 Jan 2024
CLIQUE as an AND of Polynomial-Sized
Monotone Constant-Depth Circuits
Levente Bodn´ar
February 10, 2022
Abstract
This paper shows that calculating k-CLIQUE on n vertex graphs, re-
quires the AND of at least 2n/4k monotone, constant-depth, and polynomial-
sized circuits, for suﬃciently large values of k. The proof relies on a new,
monotone, one-sided switching lemma, designed for cliques.
1
Introduction
Almost all Boolean functions require exponentially large circuits [Sha49] how-
ever, a super-polynomial circuit lower bound on NP complete problems appear
out of reach. As lower bounds for general circuits seem diﬃcult to prove, there
has been more focus on restricted circuit models.
A well understood restriction, where there are strong lower bounds, is the
class of constant-depth circuits. Early results [Ajt83, FSS81] proved that PAR-
ITY requires super-polynomial size constant-depth circuits. This was subse-
quently reﬁned in [Yao85, Has86], developing the powerful switching method.
For monotone functions, another fruitful restriction is the class of mono-
tone circuits. Razborov was the ﬁrst to prove super-polynomial bound on the
NP complete problem CLIQUE, for monotone circuits using the method of
approximations [Raz85].
This was improved by [And85, AB87, Ros10] and
recently [CKR20] with the current best truly exponential bound nΩ(k) when
k ≤ n1/3−o(1).
The simplest circuit expressing CLIQUE, is an OR of monomials, one for
each possible clique. The top gate has fan-in
k′-clique, by simply adding k′ − k points connected to everything. This gives
a 2
1−1/ log4/3(n−k)+o(1)
4
(n−k)2 lower bound on the number of maximal, k -clique
free graphs, when k > log4/3(n). Correspondingly, there is a monotone, depth-
2 circuit, with top AND gate, calculating k-CLIQUE, with top fan-in lower
bounded by the same value. This paper is a small progress towards answering
the natural question, whether CLIQUE can be expressed as the AND of a smaller
number, monotone, constant-depth, and polynomial-sized circuits.
Theorem 1. For any cd, cs constants, large enough n and log(n)cd+6 < k, one
cannot have less than 2n/4k many monotone circuits {fj} on
input A. Therefore they can be used as functions f : [2]U → [2]. For convenience
use f = g to mean that f and g compute the same function, not that the circuits
are the same. The depth d(f) of a circuit is the longest path from the output
node to any of the input nodes. The size |f| is the number of nodes in the
circuit. For f, g on the same input set U, write f ≤ g if ∀A ⊆ U f(A) ≤ g(A).
For given A ⊆ [2]U the DNF with monomials deﬁned by A is iA : [2]U → [2].
This gives the function
iA(B) 7→
(
1
if ∃A ∈ A (A ⊆ B)
0
otherwise.
The letters ρ, σ are monotone restrictions.
On an input set U they are
functions ρ : U → {1, ⋆}. Given a restriction ρ and a circuit f the restricted
circuit fρ : [2]ρ−1(⋆) → [2] is the circuit where every input source from ρ−1(1) is
set to constant true. Given an input set U, use Rp
U as a distribution on monotone
restrictions on U, that maps ρ(u) to ⋆ with probability p (and therefore maps
to 1 with probability 1 − p) independently for each u ∈ U.
3
Outline
The result follows from the following proposition on a single bounded depth and
size monotone circuit.
Proposition 2. For any cd, cs constants, large enough n and any monotone
circuit f on
The main Lemma of the paper shows that the CNF to DNF switching reduces
the possible cliques forcing the circuit to true (the set Z(f)) by a negligible
amount.
Lemma 4 (CNF to DNF switching with small clique error). For any f :
[2]K([n]) → [2] monotone s-CNF and a monotone restriction ρ : K([n]) → {1, ⋆},
one can ﬁnd g monotone t-DNF, that satisﬁes gρ ≤ fρ and
Zρ(f) \ Zρ(g)
 ≤
n
k
 
sk
n
√
t/2
.
The DNF to CNF switching requires a random restriction, but after the
restriction the functions are the same
Lemma 5 (DNF to CNF switching). For any g monotone t-DNF after taking
ρ ∼ R1/(2t)
U
restriction g can be written as an equivalent s-CNF with probability
≥ 1 − 2−s−1.
The end result is a O(log(n)2) fan-in DNF that is still true on a large number
of cliques, therefore it holds that a random 0-1 assignment on the remaining
variables satisﬁes the circuit with high probability. The probability that one of
the DNF to CNF switching fails or that the ﬁnal random assignment does not
force the circuit to true, will be exponentially small, allowing the large fan-in
bound on the output gate.
4
Proofs
The following is a list of simple observations, the proof is not included.
Observation 6.
1. If G ⊆ H then iG ≤ iH.
2. If f ≤ g then Z(f) ⊆ Z(g).
3. If ρ ∼ Rp
U and σ ∼ Rq
ρ−1(⋆) then σ ◦ ρ ∼ Rpq
U .
4. If G = ρ−1(1) where ρ ∼ Rp
K([n]) then G ∼ ER(n, 1 − p).
The idea for Lemma 4 is to build a monotone decision tree both by cliques
and by edges simultaneously based on the formula. This gives enough control
over the cliques they imply.
The extension of edge sets to cliques can only
decrease the function but the clique implications Z(f) will not change. The
tree is pruned to include only clauses below the cut. This can be achieved with
a minimal loss of cliques.
4
Proof of Lemma 4. First simplify the function f to only include edges from
ρ−1(⋆). If any of the clauses become trivial, then fρ ≡ 1 and can set g ≡ 1.
Let’s name the clauses fρ = Vm
j=1
W
e∈qj e

where |qj| ≤ s.
Create two trees T ⊆ T ′. T has its nodes labeled by subsets of [n], while T ′
has the labels from subsets of K([n]). Furthermore associate to every non-leaf
a clause from fρ both in T and T ′. For v ∈ T denote A(v) ⊆ [n] the label
and qT (v) the associated clause. Similarly denote G(w) ⊆ K([n]) the label and
qT ′(w) for w ∈ T ′ the clause. Construct the labels such that for v ∈ T ⊆ T ′
it holds that S G(v) = A(v) meaning in particular that G(v) ⊆ K(A(v)). Also
for v ∈ T not a leaf in T the labels will have qT (v) = qT ′(v).
The root r ∈ T and r ∈ T ′ has ∅ = A(r) = G(r). A node v ∈ T is a leaf if
fρ(K(A(v))) = 1, similarly w ∈ T ′ is a leaf if fρ(G(w)). If v ∈ T is not a leaf
(therefore not a leaf in T ′) ﬁnd the clauses in fρ not satisﬁed by K(A(v)). Let
the ﬁrst clause be qj, then assign qT (v) = qT ′(v) = qj. If w is a leaf in T but
not in T ′ or is not a member of T at all, then choose qj to be the ﬁrst clause
not satisﬁed by G(w) and set qT ′(w) = qj. Then for v ∈ T with a clause label
qT (v) = qT ′(v) and for each edge e ∈ qT (v) add v′ a child of v both in T and
T ′ with label A(v′) = A(v) ∪ e and G(v′) = G(v) ∪ {e}. This preserves that
S G(v′) = A(v′). For w ∈ T ′ \ T not a leaf, only add the w′ children and G(w′)
labels without any constrains from T .
q1∅
q3{1, 2}
q2{1, 3}
{1, 2, 4}
{1, 2, 5}
{1, 2, 4, 5}
q3{1, 2, 3}
{1, 3, 4}
{1, 2, 3, 4}
{1, 2, 3, 5}
{1, 2, 3, 4, 5}
q1∅
q3{12}
q2{13}
q3{13, 12}
q3{13, 34}
{12, 14}
{12, 25}
{12, 45}
{13, 12, 45}
{13, 12, 25}
{13, 12, 14}
{13, 34, 14}
{13, 34, 25}
{13, 34, 45}
The trees on formula f = q1 ∧ q2 ∧ q3 = (12 ∨ 13) ∧ (12 ∨ 34) ∧ (14 ∨ 25 ∨ 45)
with clauses numbered accordingly.
Call a node’s depth its distance from root. Let Ad be the collection of A(v)
where v is a leaf with depth ≤ d in T . Let Bd be the collection of A(v) where v is
any node in T with depth exactly d (not necessarily a leaf). Similarly deﬁne Gd
to be the collection of G(w) for w ∈ T ′ leaf node of depth ≤ d. Let d(T ), d(T ′)
5
be the maximal depth of the trees.
Claim 7 (Relations involving the Gd, Ad, Bd sets).
1. iGd(T ′) = fρ
2. iK(Ad(T )) ≤ fρ
3. Zρ

iK(Ad(T ))

= Zρ(f)
4. iK(Ad(T )) ≤ iK(Ad∪Bd+1)
5. |Bd| ≤ sd
Proof of Claim 7.
1. Working in U = ρ−1(⋆), for any u ⊆ U it holds that fρ(u) iﬀ for all qj
it is true that u ∩ qj ̸= ∅. Therefore every non leaf w ∈ T ′ has an edge
ew ∈ qT ′(w) such that ew ∈ u. Following the label set increments G(w)
to G(w) ∪ {ew}, provides a path from root to a leaf w such that each
label in that path is a subset of u resulting in G(w) ⊆ u and therefore
iGd(T ′)(u) = 1. This gives iGd(T ′) ≥ fρ. For the other direction take u such
that iGd(T ′)(u) = 1. Then there is a leaf w ∈ T where G(w) ⊆ u. By
deﬁnition of a leaf, G(w) already satisﬁes fρ giving that by monotonicity
fρ(u) = 1.
2. Consider any u ⊆ U such that iK(Ad(T ))(u) = 1, then there must be a leaf
v ∈ T and a corresponding A(v) ∈ Ad(T ) with K(A(v)) ⊆ u. Then as v is
a leaf K(A(v)) satisﬁes fρ and by monotonicity so does u.
3. Using observation 6 and the previous point it holds that Zρ

iK(Ad(T ))

⊆
Zρ(f). For the other direction, take any B ∈ Zρ(f). Then K(B) satisﬁes
fρ which by above agrees with iGd(T ′). Take a leaf w ∈ T ′ where G(w) ⊆
K(B).
Then as T ⊆ T ′ the path from the root to w must contain a
leaf from T , say it is v. Then G(v) ⊆ G(w) ⊆ K(B) therefore A(v) =
S G(v) ⊆ S G(w) ⊆ B so K(B) satisﬁes iK(Ad(T )) as required.
4. Suppose for u ⊆ U it also holds that iK(A⌈(T ))(u) = 1, then there must be
a leaf v ∈ T and a corresponding A(v) ∈ Ad(T ) with K(A(v)) ⊆ u. If the
depth of v is ≤ d then v ∈ Ad and iK(Ad)(u) = 1. Otherwise there must
be a path from root to v going through a node at depth d + 1, giving that
iK(Bd+1)(u) = 1. Using observation 6, the claim follows.
5. Every set in Bd can be identiﬁed with a path from the root to a node at
level d. As the tree branches at each step to at most s new nodes, the
mentioned bound follows.
6
Combining the above with observation 6 and by noting for all d it holds that
Ad ⊆ Ad(T ),
Zρ

iK(Ad)

⊆ Zρ(f) ⊆ Zρ

iK(Ad∪Bd+1)

this gives
Zρ(f) \ Zρ

iK(Ad)
 ≤
Zρ

iK(Ad∪Bd+1)

\ Zρ

iK(Ad)
 ≤
X
B∈Bd+1
n − |B|
k − |B|

note that every B ∈ Bd+1 is constructed in d + 1 steps, each increasing the
cardinality by at least 1 but at most 2, therefore d + 1 ≤ |B| ≤ 2d + 2 meaning
Zρ(f) \ Zρ

iK(Ad)
 ≤ sd+1
n − (d + 1)
k − (d + 1)

≤
n
k
 
sk
n
d+1
as in iK(Ad) each clause has at most
Provided the bottom layer is a t-DNF, use Lemma 5 for each DNF, intro-
ducing at depth i a random ρj ∼ R1/(2t)
ρ−1
(<i)(⋆) where write ρ(<i) =
◦
j<iρj for the
composition of previous restrictions. The change is unsuccessful with probabil-
ity ≤ 2−s−1 at each DNF, otherwise results in a s-CNF.
Provided the bottom layer is a s-CNF use Lemma 4. Each gate g in the
bottom CNF layer is replaced with g′ a t-DNF satisfying g′ ≤ g and by the
selection of parameters, it holds that
Zρ(≤i)(g) \ Zρ(≤i)(g′)
 ≤
n
k
 
sk
n
√
t/2
=
n
k

n−cs−2.
The probability that any of the DNF to CNF switching fails is ≤ ncs+12−s−1
by the union bound. Write ρ = ρ(<cd+2). The total number of cliques lost from
CNF to DNF switches is at most
At each restriction a σ, ρj ∼ R1/(2t)
U
is used. Every second layer uses one such
restriction, and an additional restriction used at the last step, giving a total
σ ◦ ρ ∼ Rp
K([n]) where
p ≥
 1
2t
cd/2+2
= (2cs + 2)−cd−4 log(n)−cd−4.
Proof of Theorem 1. Use proposition 2 with the same cd and cs constants. This
gives every AC0 circuit satisfying the constraints is true on a G ∼ Rp
K([n])
input with probability 1 − 2−n/3k, therefore all the fj is true on input G with
probability 1 − o(1) using the union bound, but note that for log(n)cd+6 < k
the probability that a clique appears in G is upper bounded by
n
k

(1 − p)(k
2) ≤ 2
log(n)k−
k2
c′(cd,cs) log(n)cd+3 = o(1)
using p from proposition 2. Therefore the circuit can not express clique as there
is a nonzero probability that G forces all the fj to true, but G still has no
clique.
5
Concluding remarks
As outlined in the introduction, one can write CLIQUE as an OR of
is used, following from d + 1 ≤ |B| ≤ 2d + 2.
A better approximation of
the set sizes on average could provide a stronger result, in particular if all
|B| ≈ 2d + 2, then the resulting bound is S(n, k) ≤ 2−O(n2/k). The following
stronger conjecture captures this:
Conjecture 10. It holds that S(n, k) ≤ 2−Θ(n2/k) for k in a suitable range.
Note that while this paper discusses the monotone question, as the method
heavily uses that condition, the general question is equally interesting:
Question 11. What is the maximum one-sided correlation between polynomial-
sized circuits and k-CLIQUE on n vertices, which is correct on all positive
instances?
References
[AB87]
Noga Alon and Ravi B. Boppana. The monotone circuit complexity of
boolean functions. Combinatorica, 7(1):1–22, Mar 1987.
[Ajt83]
M. Ajtai. P1
1-formulae on ﬁnite structures. Ann. Pure Appl. Log.,
24:1–48, 1983.
[And85] A. E. Andreev. On a method for obtaining lower bounds for the com-
plexity of individual monotone functions. Sov. Math., Dokl., 31:530–
534, 1985.
[BHST14] Eric Blais, Johan H˚astad, Rocco A. Servedio, and Li-Yang Tan. On
DNF approximators for monotone boolean functions. In Automata,
Languages, and Programming, pages 235–246, Berlin, Heidelberg, 2014.
[BP14]
J´ozsef Balogh and ˇS´arka Petˇr´ıˇckov´a.
The number of the maximal
triangle-free graphs.
Bulletin of the London Mathematical Society,
46(5):1003–1006, July 2014.
[CKR20] Bruno Pasqualotto Cavalar, Mrinal Kumar, and Benjamin Rossman.
Monotone circuit lower bounds from robust sunﬂowers, 2020.
[FSS81] Merrick Furst, James B. Saxe, and Michael Sipser. Parity, circuits,
and the polynomial-time hierarchy. In Proceedings of the 22nd An-
nual Symposium on Foundations of Computer Science, SFCS ’81, page
260–270, 1981.
[Has86] J Hastad. Almost optimal lower bounds for small depth circuits. In
Proceedings of the eighteenth annual ACM symposium on Theory of
computing - STOC ’86, pages 6–20, Berkeley, California, United States,
1986. ACM Press.
[OW07] Ryan O’Donnell and Karl Wimmer. Approximation by DNF: Exam-
ples and counterexamples. In Automata, Languages and Programming,
pages 195–206, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.
10
[Raz85] A. A. Razborov. Lower bounds on monotone complexity of the logical
permanent.
Mathematical Notes of the Academy of Sciences of the
USSR, 37(6):485–493, June 1985.
[Ros10] Benjamin Rossman. The monotone complexity of k-clique on random
graphs.
In 2010 IEEE 51st Annual Symposium on Foundations of
Computer Science, pages 193–201, 2010.
[Sha49] Claude. E. Shannon. The Synthesis of Two-Terminal Switching Cir-
cuits. Bell System Technical Journal, 28(1):59–98, January 1949.
[Yao85] Andrew C-C. Yao. Separating the polynomial-time hierarchy by ora-
cles. In Proc. 26th Annual Symposium on Foundations of Computer
Science, page 1–10. IEEE Press, 1985.
11
"
"The eigenvalue problem is a cornerstone in linear algebra, providing profound insights into studying matrix properties. This problem is typically formulated for Hermitian matrices, which in turn assume spectral decomposition and thus limit the generalization to general matrices with complex and even defective eigenvalues. Here, we tackle this challenge and present a novel family of quantum eigensolver algorithms tailored for general matrices, allowing for more encompassing scenarios and with applications in diverse domains. Overall, our innovative approach paves the way for extending quantum algorithms to a broader spectrum of eigenvalue problems.","The eigenvalue problem, finding scalar factors for matrix transformations to stretch or compress space, is a fundamental concept in linear algebra. While this problem has been successfully addressed using quantum algorithms for Hermitian matrices, characterized by real eigenvalues and orthonormal eigenvectors, the extension to more general non-Hermitian matrices remains a formidable challenge. This is due to complex and even defective eigenvalues, where a matrix is called defective if it has nontrivial Jordan blocks with dimensions larger than one. Our work introduces a novel family of quantum eigensolvers specifically tailored for general matrices, encompassing scenarios with complex eigenvalues or defective matrices. This advancement opens up new possibilities for applying quantum algorithms to a diverse range of problems.","In the realm of quantum computing, substantial progress has been made in solving the eigenvalue problem for Hermitian matrices. These successful approaches exploit the inherent structure of Hermitian matrices, with real eigenvalues and an orthonormal basis of eigenvectors. However, these algorithms face limitations when attempting to generalize to non-Hermitian matrices due to the presence of complex and defective eigenvalues. To address this challenge, our work presents a fundamentally new approach to tackle the eigenvalue problem for general matrices.nan","As a first step, our algorithm targets the identification of a single eigenvalue without imposing additional constraints. This is achieved by leveraging the relationship between eigenvalues of a matrix A and the smallest singular value of A minus a scalar multiple of the identity matrix, denoted as A − μI, along with a quantum singular value threshold subroutine. By capitalizing on this relationship, our algorithm can efficiently search for an eigenvalue without the need for any prior knowledge about the matrix's spectrum. Subsequently, we refine our algorithm to tackle more specific scenarios. For diagonalizable matrices, characterized by distinct eigenvalues and no defective blocks, we enhance the accuracy of eigenvalue estimation. When the eigenvalues are real or equidistant from a reference point, we achieve further refinement of the algorithm's performance. Finally, our approach also addresses the identification of eigenvalues closest to a specified point or line, extending the scope of applicability to ground energy and energy gap problems originally defined for Hermitian matrices.","Through rigorous mathematical analysis, we establish the theoretical foundations of our quantum eigensolver algorithms. For the general case, our algorithm demonstrates a remarkable complexity scaling with respect to key matrix properties, including the largest Jordan block dimension and the condition number of the matrix. Notably, for diagonalizable matrices, the algorithm exhibits a nearly Heisenberg scaling in terms of the matrix size, mirroring the behavior of Hermitian matrices. Furthermore, our approach provides accurate eigenvalue estimations, with scaling behavior finely tuned to different scenarios, including real eigenvalues, equidistant eigenvalues, and eigenvalues close to a reference point or line. These theoretical results underscore the effectiveness and versatility of our quantum eigensolvers.","The development of quantum eigensolver algorithms for general matrices opens up new avenues for exploration in various domains. We showcase the broad applicability of our approach by presenting several compelling examples. In the realm of quantum chemistry, our algorithms facilitate the estimation of relaxation times for Markov chains, enabling a deeper understanding of the behavior of these systems. Turning to open quantum systems, our algorithms prove instrumental in solving Liouvillian gaps, a crucial quantity characterizing the decay behavior and phase transitions. Moreover, our algorithms pave the way for the verification of PT-symmetry broken/unbroken phases in non-Hermitian systems, providing a powerful tool for studying topological phenomena. These applications underscore the significance of our quantum eigensolvers for tackling problems across diverse scientific disciplines. As quantum technologies continue to advance, we anticipate that our algorithms will play an increasingly important role in unlocking the potential of quantum computing for scientific discovery.",Quantum Eigensolver for General Matrices,"Xiao-Ming Zhang, Yunkun Zhang, Wenhao He, Xiao Yuan","Quantum Eigensolver for General Matrices
Xiao-Ming Zhang,1, 2 Yukun Zhang,2 Wenhao He,3, 4 and Xiao Yuan2, ∗
1School of Physics, South China Normal University, Guangzhou 510006, China
2Center on Frontiers of Computing Studies, School of Computer Science, Peking University, Beijing 100871, China
3School of Physics, Peking University, Beijing 100871, China
4Center for Computational Science and Engineering, Massachusetts Institute of Technology, Cambridge, MA 02139, USA
The eigenvalue problem, a cornerstone in linear algebra, provides profound insights into studying matrix
properties.
Quantum algorithms addressing this problem have hitherto been constrained to special normal
matrices assuming spectral decomposition, leaving the extension to general matrices an open challenge. In this
work, we present a novel family of quantum algorithms tailored for solving the eigenvalue problem for general
matrices, encompassing scenarios with complex eigenvalues or even defective matrices. Our approach begins
by tackling the task of searching for an eigenvalue without additional constraints. For diagonalizable matrices,
our algorithm has ˜𝑂(𝜀−1) complexity with an error 𝜀, achieving the nearly Heisenberg scaling. Subsequently,
we study the identification of eigenvalues closest to a specified point or line, extending the results for ground
energy and energy gap problems in Hermitian matrices. We achieve an accuracy scaling of ˜𝑂(𝜀−2) for general
diagonalizable matrices, further refining to ˜𝑂(𝜀−1) under the condition of real eigenvalues or constant distance
from the reference point. The algorithm’s foundation lies in the synergy of three techniques: the relationship
between eigenvalues of matrix 𝐴 and the minimum singular value of 𝐴 − 𝜇𝐼, quantum singular value threshold
subroutine extended from quantum singular-value estimation, and problem-specific searching algorithms. Our
algorithms find applications in diverse domains, including estimating the relaxation time of Markov chains,
solving Liouvillian gaps in open quantum systems, and verifying PT-symmetry broken/unbroken phases. These
applications underscore the significance of our quantum eigensolvers for problems across various disciplines.
Eigenvalue, a fundamental concept in linear algebra, repre-
sents scalar factors by which a matrix transformation stretches
or compresses space. Formally, complex value 𝜆 𝑗 and nor-
malized vector |𝑣 𝑗⟩ are considered as the eigenvalue and the
corresponding eigenvector of matrix 𝐴 if
𝐴|𝑣 𝑗⟩ = 𝜆 𝑗|𝑣 𝑗⟩.
(1)
The applications of quantum computing in solving Eq. (1) have
mostly been restricted to the Hermitian cases 𝐴 = 𝐴† [1–3],
where 𝜆 𝑗 are real, and |𝑣 𝑗⟩ forms an orthonormal basis. For
a general matrix, however, we encounter complex and even
defective eigenvalues, and eigenvectors are not necessarily or-
thogonal. While quantum computing naturally favours Her-
miticity, its applicability in solving Eq. (1) for non-Hermitian
matrices remains an outstanding question.
Relating to the eigenvalues, one can perform Jordan decom-
position of 𝐴 as
𝐴 = 𝑃Λ𝑃−1,
(2)
where Λ is a matrix in the Jordan canonical form (JCF), whose
diagonal elements correspond to the eigenvalues, and 𝑃 is an
invertible matrix. 𝐴 is called diagonalizable if Λ is diagonal.
Note that a diagonalizable matrix is not necessarily Hermitian
since 𝑃 is generally non-unitary. Most generally, Λ is a block-
diagonal matrix as close to a diagonal matrix as possible
Λ = Λ1 ⊕ Λ2 ⊕ · · · ⊕ Λ𝑀 with 𝑀 ⩽ 𝑁. Each Jordan block Λ 𝑗
is in the form of
Λ 𝑗 =
©­­­­­
«
𝜆 𝑗
1
𝜆 𝑗
...
...
1
𝜆 𝑗
ª®®®®®
¬
.
(3)
If there exists a nontrivial Jordan block with a dimension larger
than one, we call 𝐴 a defective matrix. In this case, we have
strictly 𝑀 < 𝑁 and there are less than 𝑁 linearly independent
eigenvectors.
It is worth noting that eigenvalues are fundamentally differ-
ent from singular values. There is no direct correspondence
between eigenvalues and singular values in general unless the
matrix is normal with spectral decomposition. Therefore, the
eigenvalue problem does not trivially fit into the framework of
quantum singular value transformation [4, 5].
Here, we propose quantum eigensolver algorithms for gen-
eral matrices. The complexity of the algorithms for solving
Eq. (1) depends on two properties of the matrix 𝐴. The first
one is how defective 𝐴 is, which can be formally defined as the
largest dimension of the Jordan block 𝑚′
max ≡ max𝑗 dim(Λ 𝑗).
The second one is the condition number of the matrix 𝑃,
denoted as 𝜅𝑃. Regarding the nonuniqueness of the Jordan
decomposition, we define 𝜅𝑃 as the minimum value for all
possible 𝑃 in the following text. In our discussion, we assume
that we have knowledge of the upper bound of both quantities,
i.e. 𝑚max and 𝐾 satisfying 𝑚max ⩾ 𝑚′
max and 𝐾 ⩾ 𝜅𝑃.
Eigenvalue Problems. We first summarize the detailed def-
initions of the eigenvalue problems.
The first problem we
consider is to output an estimation of an eigenvalue defined as
follows.
arXiv:2401.12091v1  [quant-ph]  22 Jan 2024
2
Re[λ]
Im[λ]
<latexit sha1_base64=""K
cfBQytsgMQ+Tm0cfGsMDXIMS/s="">AB6XicdVDLSgMx
FL3js9ZX1aWbYBFcDZlSq8uiG5dV7APaoWTSTBuayQxJR
ihD/8CNC0Xc+kfu/BszfYCKHg5nHMv94TJIJrg/Gns7
K6tr6xWdgqbu/s7u2XDg5bOk4VZU0ai1h1AqKZ4JI1DTe
CdRLFSBQI1g7G17nfmBK81jem0nC/IgMJQ85JcZKd72
sXypjt4ZzIOxWl6QyJ547+3EZFmj0Sx+9QUzTiElDBdG6
6+HE+BlRhlPBpsVeqlC6JgMWdSKm/Wy26RSdWmWAw
ljZJw2aqd87MhJpPYkCWxkRM9K/vVz8y+umJrz0My6T1D
BJ54PCVCATo/xsNOCKUSMmlhCquN0V0RFRhBobTtGsLw
U/U9aFderue31XL9ahFHAY7hBM7Agwuow0oAkUQniE
Z3hxs6T8+q8zUtXnEXPEfyA8/4FsGSNew=</latexit
>
{
<latexit sha1_base64=""GeLjeJ3GeC47QfLzsS
1vptzGkI="">AB6HicbVDLSgNBEOz1GeMr6tHLYBA8hV3xdQx68ZiAeUCyhNlJbzJmdnaZmRXCki/w4kE
Rr36SN/GSbIHTSxoKq6e4KEsG1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43A
dqKQRoHAVjC6m/qtJ1Sax/LBjBP0IzqQPOSMGivVB71S2a24M5Bl4uWkDlqvdJXtx+zNEJpmKBadzw3MX
5GleFM4KTYTUmlI3oADuWShqh9rPZoRNyapU+CWNlSxoyU39PZDTSehwFtjOiZqgXvan4n9dJTXjZ1w
mqUHJ5ovCVBATk+nXpM8VMiPGlCmuL2VsCFVlBmbTdG4C2+vEya5xXvqnJZvyhXb/M4CnAMJ3AGHlxDF
e6hBg1gPAMr/DmPDovzrvzMW9dcfKZI/gD5/MHz7WM9Q=</latexit>g
(a)
<latexit sha1_base64=""GeLjeJ3GeC47QfLzsS1vptzGkI="">AB6HicbVDLSgNBEOz1GeMr6tHLYB
A8hV3xdQx68ZiAeUCyhNlJbzJmdnaZmRXCki/w4kERr36SN/GSbIHTSxoKq6e4KEsG1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6m/qtJ1Sax/LBjBP0IzqQPOSMGivVB
71S2a24M5Bl4uWkDlqvdJXtx+zNEJpmKBadzw3MX5GleFM4KTYTUmlI3oADuWShqh9rPZoRNyapU+CWNlSxoyU39PZDTSehwFtjOiZqgXvan4n9dJTXjZ1wmqUHJ5ovCVBATk+nXpM8VMiPGlCmuL2VsCFVlBmb
TdG4C2+vEya5xXvqnJZvyhXb/M4CnAMJ3AGHlxDFe6hBg1gPAMr/DmPDovzrvzMW9dcfKZI/gD5/MHz7WM9Q=</latexit>g
<latexit
 sha1_base64=""KcfBQytsgMQ+Tm0cfGsMDXIMS/s="">AB6XicdVDLSgMxFL3js9ZX1aWbYBFcDZlS
q8uiG5dV7APaoWTSTBuayQxJRihD/8CNC0Xc+kfu/BszfYC
KHg5nHMv94TJIJrg/Gns7K6tr6xWdgqbu/s7u2XDg5bOk4VZU0ai1h1AqKZ4JI
1DTeCdRLFSBQI1g7
G17nfmBK81jem0nC/IgMJQ85JcZKd72sXypjt4ZzIOxWl6
QyJ547+3EZFmj0Sx+9QUzTiElDBdG6+HE+BlRhlPBpsVeqlC6JgMWdSKm/Wy26RSdWmWAwljZJw
2aqd87MhJpPYkCWxkRM9K/vVz8y+umJrz0My6T1DBJ54PCV
CATo/xsNOCKUSMmlhCquN0V0RFRhBobTtGsLwU/U9aFderue31XL9ahFHAY7hBM7Agwuow0oAkUQ
niEZ3hxs6T8+q8zUtXnEXPEfyA8/4FsGSNew=</latexi
t>
{
Re[λ]
Im[λ]
<latexit sha1_base64=""yn72uj8s/DTU/8kMNFSQDJp4LEM="">AB6HicbVDLSgMxFL1TX7W+qi7dBIvg
qsyIr2XRjcsW7APaQTLpnTY2kxmSjFCGfoEbF4q49ZPc+Tem7Sy09UDgcM65N4TJIJr47rfTmFldW19o7hZ2tre2d0r7x+0dJwqhk0Wi1h1AqpRcIlNw43ATqKQRoHAdjC6nfrtJ1Sax/LejBP0IzqQPOSMGis16g/lil
t1ZyDLxMtJBXLY/FevH7M0QmYoFp3PTcxfkaV4UzgpNRLNSaUjegAu5ZKGqH2s9miE3JilT4JY2WfNGSm/p7IaKT1OApsMqJmqBe9qfif101NeO1nXCapQcnmH4WpICYm06tJnytkRowtoUxuythQ6oM7abki3BWzx5m
bTOqt5l9aJxXqnd5HU4QiO4RQ8uIa3EdmsA4Rle4c15dF6cd+djHi04+cwh/IHz+QOs2Yze</latexit>P
Re[λ]
Im[λ]
<latexit sha1_base64=""K
cfBQytsgMQ+Tm0cfGsMDXIMS/s="">AB6XicdVDLSgMx
FL3js9ZX1aWbYBFcDZlSq8uiG5dV7APaoWTSTBuayQxJR
ihD/8CNC0Xc+kfu/BszfYCKHg5nHMv94TJIJrg/Gns7
K6tr6xWdgqbu/s7u2XDg5bOk4VZU0ai1h1AqKZ4JI1DTe
CdRLFSBQI1g7G17nfmBK81jem0nC/IgMJQ85JcZKd72
sXypjt4ZzIOxWl6QyJ547+3EZFmj0Sx+9QUzTiElDBdG6
6+HE+BlRhlPBpsVeqlC6JgMWdSKm/Wy26RSdWmWAw
ljZJw2aqd87MhJpPYkCWxkRM9K/vVz8y+umJrz0My6T1D
BJ54PCVCATo/xsNOCKUSMmlhCquN0V0RFRhBobTtGsLw
U/U9aFderue31XL9ahFHAY7hBM7Agwuow0oAkUQniE
Z3hxs6T8+q8zUtXnEXPEfyA8/4FsGSNew=</latexit
>
{
<latexit sha1_base64=""GeLjeJ3GeC47QfLzsS1vptzGkI="">AB6HicbVDLSgNBEOz1GeMr6tHLYB
A8hV3xdQx68ZiAeUCyhNlJbzJmdnaZmRXCki/w4kERr36SN/GSbIHTSxoKq6e4KEsG1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6m/qtJ1Sax/LBjBP0IzqQPOSMGivVB
71S2a24M5Bl4uWkDlqvdJXtx+zNEJpmKBadzw3MX5GleFM4KTYTUmlI3oADuWShqh9rPZoRNyapU+CWNlSxoyU39PZDTSehwFtjOiZqgXvan4n9dJTXjZ1wmqUHJ5ovCVBATk+nXpM8VMiPGlCmuL2VsCFVlBmb
TdG4C2+vEya5xXvqnJZvyhXb/M4CnAMJ3AGHlxDFe6hBg1gPAMr/DmPDovzrvzMW9dcfKZI/gD5/MHz7WM9Q=</latexit>g
<latexit sha1_base64=""ZulQnuAxajCQqeGX0rA170utLio="">AB6HicbVDLSgNBEOyNrxhfUY9eBoPg
KeyKr2PQiwcPCZgHJEuYnXSMbOzy8ysEJZ8gRcPinj1k7z5N06SPWhiQUNR1U13VxALro3rfju5ldW19Y38ZmFre2d3r7h/0NBRohjWSQi1QqoRsEl1g03AluxQhoGApvB6HbqN59QaR7JBzO0Q/pQPI+Z9RYqXbfLZ
bcsjsDWSZeRkqQodotfnV6EUtClIYJqnXbc2Pjp1QZzgROCp1EY0zZiA6wbamkIWo/nR06ISdW6ZF+pGxJQ2bq74mUhlqPw8B2htQM9aI3Ff/z2onpX/spl3FiUL5on4iInI9GvS4wqZEWNLKFPc3krYkCrKjM2mYEPwF
l9eJo2zsndZvqidlyo3WRx5OIJjOAUPrqACd1CFOjBAeIZXeHMenRfn3fmYt+acbOYQ/sD5/AGmyYza</latexit>L
(b)
(c)
FIG. 1: Sketch of (a) the energy gap for Hermitian matrices with reference point 𝑃 = 0, (b) the point gap for non-Hermitian matrices with
reference point 𝑃 = 0, and (c) the line gap for non-Hermitian matrices with reference line 𝐿 = {𝑖𝑏, 𝑏 ∈ R}.
Problem 1 (Eigenvalue searching). Given a square matrix 𝐴
with ∥𝐴∥ ⩽ 1, output an estimation of an eigenvalue ˆ𝜆, such
that min | ˆ𝜆 − 𝜆| ⩽ 𝜀 for some error 𝜀, where 𝜆 is one of the
eigenvalue solutions to Eq. (1).
Here, ∥ · ∥ refers to the spectral norm of a matrix. Problem 1
has no restrictions on the eigenvalue. We may require that
the eigenvalue to be estimated have certain properties. Take
the Hermitian matrix as an example, there are two important
questions related to eigenvalues. The first one is the lowest
eigenvalue problem. For a quantum many-body system de-
scribed by a Hermitian Hamiltonian, this corresponds to the
ground-state energy of the system [1–3].
The second one
is the eigenvalue gap problem, which plays a critical role in
many-body physics phenomena, such as conductivity and su-
perconductivity. Extending from Hermitian to non-Hermitian
matrices with complex eigenvalues, the generalization of both
questions is not unique [6, 7], which correspond to the eigen-
value searching problems under different restrictions. In par-
ticular, we consider the following two problems.
Problem 2. Given a square matrix ∥𝐴∥ ⩽ 1, a reference point
𝑃 ∈ D(0, 1) and accuracy 𝜀 ∈ (0, 1). Let 𝑔 ≡ min𝜆𝑗≠𝑃
𝜆 𝑗 −
𝑃
, S ≡

𝜆 𝑗
|𝑃 − 𝜆 𝑗| ∈ [𝑔, 𝑔 + 𝜀]
	
.
The goal is to output
the gap estimation 𝑔′ and eigenvalue estimation 𝜆′, such that
|𝑔′ − 𝑔| ⩽ 𝜀 and |𝜆′ − 𝜆 𝑗| ⩽ 𝜀 for some 𝜆 𝑗 ∈ S.
Problem 3. Given a square matrix ∥𝐴∥ ⩽ 1, a reference
line 𝐿 in the complex plain such that 𝐿 Ð D(0, 1) ≠ ∅, and
accuracy 𝜀 ∈ (0, 1).
Let 𝑔 = min𝜆𝑗∉𝐿,𝑝∈𝐿 |𝜆 𝑗 − 𝑝|, S ≡

𝜆 𝑗
 min𝑝∈𝐿 |𝜆 𝑗 − 𝑝| ∈ [𝑔, 𝑔 + 𝜀]
	
. The goal is to output the
gap estimation 𝑔′ and eigenvalue estimation 𝜆′, such that |𝑔′ −
𝑔| ⩽ 𝜀 and |𝜆′ − 𝜆 𝑗| ⩽ 𝜀 for some 𝜆 𝑗 ∈ S.
Here, we have defined the disk as D(𝜇, 𝑟) ≡

𝑥
|𝑥 − 𝜇| ⩽ 𝑟
	
.
In case 𝑃 or 𝐿 have no overlap with eigenvalues, Problem 2
or 3 corresponds to finding an eigenvalue that is closest to the
reference point 𝑃, or line 𝐿 (up to an accuracy 𝜀). Therefore,
they can be considered as two different ways of the generaliza-
tion of the ground energy problem for Hermitian matrices. On
the other hand, when 𝑃 or 𝐿 overlaps with at least one of the
eigenvalues, Problems 2 and 3 become Point gap, or Line gap
problems [6–8]. As illustrated in Fig. 1, they can be considered
as two different ways of the generalizations from the energy
gap problem of the Hermitian case. We note that the point gap
and line gap are different. In some cases, both point gaps and
line gaps are non-vanishing. But there exists matrices with
non-vanishing point gap, but zero line gap. Physics systems
with Hamiltonian corresponding these two cases may emerge
from different symmetries and topologies [6–8]. In particular,
the second case does not have a Hermitian counterpart, the
corresponding physics system is also named to be genuinely
non-Hermitian [7].
Furthermore, eigenvalue and eigenvector are related in gen-
eral. We discuss in [9] that an accurate eigenvalue estimation
implies a good approximation to the corresponding eigenvec-
tors. In the remaining of the main text, however, we focus on
the eigenvalue problems.
Results. Here, we introduce our results for the three eigenvalue
problems. We only summarize the results in the main text and
refer to [9] for details.
We first discuss the assumptions of the algorithms. Given a
general square matrix 𝐴 ∈ C𝑁 ×𝑁 with 𝑁 = 2𝑛, we consider its
block encoding that provides unitary access to the matrix. For
a unitary 𝒪𝐴, we say it is a block encoding of 𝐴 if it encodes
the desirable matrix 𝐴 such that 𝐴 = (⟨0𝑎| ⊗ 𝐼) 𝒪𝐴 (|0𝑎⟩ ⊗ 𝐼),
with 𝐼 the 𝑁-dimensional identity. Note that we have neglected
a scaling factor compared to the conventional definition as it
can be absorbed in matrix 𝐴. Block-encoding is a standard way
of encoding the classical description of a matrix to quantum
operations [4, 5, 10–12]. In practice, 𝒪𝐴 may be constructed by
sparse-access input model or linear combination of unitaries
(LCU) [10], depending on the form of 𝐴 being presented.
For Hermitian matrices, the eigenvalue problem is typically
solved by assuming the existence of an initial state that can
be prepared to have a reasonable lower-bounded overlap with
the targeted eigenstate [1, 2]. Otherwise, the problems are in
general QMA-compete [13]. Here, a similar assumption is
also made for general matrices. We introduce an oracle 𝒫𝐴,
which given an input 𝜇 satisfying |𝜇| ⩽ 1, outputs a quantum
3
state |𝜓ini
𝜇 ⟩ satisfying |⟨𝜓ini
𝜇 |𝑢0(𝜇)⟩| ⩾ 𝛾. Here, |𝑢0(𝜇)⟩ is the
right singular vector of the matrix 𝐴 − 𝜇𝐼 corresponding to its
smallest singular value. 𝒫𝐴 may be constructed quantumly,
with methods like variational quantum algorithms [14–16] or
adiabatic state preparation [1]. Alternatively, one may find an
approximated model of 𝐴 whose eigenvalue can be calculated
efficiently on a classical computer.
Here and after, we assume that 𝒪𝐴, 𝒫𝐴, and their inverses
can be queried efficiently. For simplicity, we also count the
query to controlled 𝒪𝐴 or 𝒫𝐴 as a single query to them.
For Problem 1, we have the following result.
Theorem 1. With success probability at least 1−𝛿, Problem 1
can be solved with
˜O

𝐾3𝜀−3𝑚max+2𝛾−1
(4)
uses of the query to 𝒪𝐴, 𝒫𝐴 and their inverses, and extra
single- and two-qubit gates.
Here ˜O(·) omits the polylogarithmic dependence on 1/𝛿, 1/𝜀,
𝐾, and 𝑁. We also clarify that for qubit systems, the extra
single- and two-qubit gate number contains a dependency of
qubit number 𝑂(𝑛), which is however neglected by ˜𝑂.
When 𝑚max = 1, i.e. 𝐴 is diagonalizable, Eq. (4) reduces
to ˜O 4
Problem 2 and 3 are more challenging. Take Problem 2
as an example, the complication is that to claim 𝑔′ is a good
estimation of 𝑔 with accuracy 𝜀, we should ensure that there
is no eigenvalue in the region D(𝑃, 𝑔′ − 𝜀)/D(𝑃, 𝜀). Our
iterative strategy is as follows. Suppose at the 𝑗th step, we are
confidence that 𝑔 ∈ [𝑅min
𝑗
, 𝑅max
𝑗
]. Let Δ 𝑗 = 𝑅max
𝑗
− 𝑅min
𝑗
, we
reduce Δ 𝑗 by querying a set of SVTSs. The process is termi-
nated until Δ 𝑗 ⩽ 𝜀. After that, we search for an eigenvalue
near the circle with radius 𝑔′.
Now we discuss the applications of our results in different
problems.
Applications 1: Relaxation time of Markov chain. Markov
chains model systems transitioning between states with prob-
abilities determined solely by their current state, with broad
applications in both natural and social science [18–22].
A
finite Markov chain can be described by the stochastic matrix.
In this case, we have Í
𝑖 𝐴𝑖, 𝑗 = 1 for each column 𝑗. For every
finite Markov chain, the largest eigenvalue is 1. So we can de-
fine the absolute spectral gap of 𝐴 as 𝑔mar ≡ 1 − max𝜆𝑗≠1 |𝜆 𝑗|.
The relaxation time, defined as 𝑡rel ≡ 1/𝑔mar, is a crucial pa-
rameter in understanding the properties of the Markov chain.
In particular, for irreducible, time-reversible Markov chain,
𝑡rel can be used to upper bound the mixing time, i.e. the time
converging to the stationary distribution [22].
With a similar strategy to solving Problem 2 and 3, we can
estimate the relaxation time with complexity consistent with
Theorem 2 as follows.
Theorem 3. Suppose 𝐴 is a stochastic matrix describing a
Markov chain. Promised that the relaxation time 𝑡rel is larger
than 𝜀 ∈ (0, 1). Then, 𝑡rel can be estimated to accuracy 𝜀 with
˜𝑂(𝐾3𝜀−3𝑚max+1𝛾−1) queries to 𝒪𝐴, 𝒫𝐴 and their inverses, and
extra single- and two-qubit gates.
Applications 2: Liouvillian gap for open quantum systems.
The Liouvillian gap (LG) is an important quantity character-
izing the decaying behaviour and phase transitions of open
quantum systems [23–28]. The dynamics of an open quantum
system can be described by the Lindblad master equation ¤𝜌 =
−𝑖[𝐻, 𝜌]+Í
𝜇

2𝐿𝜇𝜌𝐿†
𝜇 −
n
𝐿†
𝜇𝐿𝜇, 𝜌
o
≡ L(𝜌) for the Hamil-
tonian 𝐻 and dissipators 𝐿𝜇. Because the Liouvillian operator
L is a linear superoperator, we can perform vectorization of
¤𝜌 = L(𝜌) as ¤˜𝜌 = ˜L· ˜𝜌, where ˜𝜌 = Í
𝑚,𝑛 𝜌𝑚𝑛|𝑚⟩⊗|𝑛⟩ and ˜L =
−𝑖𝐻 ⊗ 𝐼 +𝑖𝐼 ⊗ 𝐻𝑇 +Í
𝜇

2𝐿𝜇 ⊗ 𝐿∗
𝜇 − 𝐿†
𝜇𝐿𝜇 ⊗ 𝐼 − 𝐼 ⊗ 𝐿𝑇
𝜇𝐿∗
𝜇

.
Let 𝜆 𝑗 ( ˜L) be the eigenvalues of ˜L ordered according to the
magnitude of the real part, i.e. Re𝜆0( ˜L) ⩾ Re𝜆1( ˜L) ⩾ · · · .
For a general Lindbladian, there exists at least one steady state
𝜌ness satisfying ¤˜𝜌ness = ˜L ˜𝜌ness = 0, resulting in 𝜆0 = 0. The
LG is formally defined as
𝑔L ≡ |Re𝜆1( ˜L)|.
(11)
𝑔L has a close relation to the relaxation behaviour of the open
quantum system. In most cases, the relaxation time 𝜏 of an
open quantum system satisfies 𝜏 ≲ 1/𝑔L [26].
For many-body systems, analytic solutions to LG only ex-
ist for some special cases, while numerical calculation with
classical computers suffers from the exponential increase of
the Hilbert space.
On the other hand, LG can potentially
be solved with a quantum computer efficiently based on our
quantum eigensolver. One may assume that the vectorized Li-
ouvillian has a Jordan decomposition ˜L = 𝑃Λ𝑃−1, where the
condition number of satisfies 𝜅𝑝 ⩽ 𝐾. We may also assume
that ˜L is diagonalizable. Compared to Problem 3, LG is a line
gap problem with 𝐿 = {𝑖𝑏, 𝑏 ∈ R}. According to Theorem 2,
we have the following result.
Theorem 4. Promised that 𝑔L ⩾ 𝜀 and ˜L is diagonalizable
and ∥ ˜L∥ ⩽ 1. With success probability at least 1 − 𝛿, 𝑔L can
be estimated to accuracy 𝜀 with
˜𝑂(𝐾3𝜀−2𝛾−1)
(12)
queries to 𝒪 ˜L, 𝒫 ˜L and their inverses, and extra single- and
two-qubit gates.
In most open quantum system models, ˜L can be decomposed
into the linear combination of Pauli strings. Accordingly, the
block encoding of
˜L, up to a rescaling factor, can be con-
structed by the linear combination of unitaries technique [29].
Applications 3: PT-broken/unbroken phase classification.
In quantum systems described by non-Hermitian operators,
the eigenvalue does not necessarily to be complex. A typi-
cal example is the parity-time (𝑃𝑇) symmetry operators [30–
34].
An operator is called 𝑃𝑇 symmetry if it is invariant
under simultaneous application of parity-reversal operator P
and time-reversal operator T respectively. The eigenvalues of
the 𝑃𝑇-symmetry operator can either be real only or appear as
complex conjugate pairs. The former possesses 𝑃𝑇-symmetry
and is therefore categorized as 𝑃𝑇-unbroken phase when the
matrix is diagonalizable [34, 35]. In the second case, 𝑃𝑇-
symmetry is simultaneously broken and therefore categorized
as the 𝑃𝑇-broken phase. The transition between these two
phases is of broad interest with applications in quantum sens-
ing [36, 37].
To verify whether the quantum system is in the 𝑃𝑇-broken
or 𝑃𝑇-unbroken phase, it suffices to determine if it contains
complex eigenvalues. In practice, we may allow a certain error
𝜀. When all eigenvalues are at most 𝜀 distance away from the
real axis, the matrix is categorized as 𝑃𝑇-unbroken. With a
mild modification of the algorithms for solving Problem 3, one
can solve this problem with a similar complexity claimed in
Theorem 2. More specifically, we have the following result.
Theorem 5. Given a square, diagonalizable matrix ∥𝐴∥ ⩽ 1.
With success probability 1 − 𝛿, one can verify whether 𝐴 has
eigenvalues satisfying
Im[𝜆 𝑗]
 ⩾ 𝜀, or all eigenvalues are in
the real axis, with ˜𝑂(𝐾3𝜀−2𝛾−1) queries to 𝒪𝐴, 𝒫𝐴 and there
inverses, and extra single- and two-qubit gates.
Based
on
Theorem
5,
we
can
characterize
the
𝑃𝑇-
broken/unbroken phase readily under the diagonalizable as-
sumption.
5
Discussions. We have developed quantum algorithms for solv-
ing eigenvalue problems. The idea can also be generalized to
the study of the properties related to eigenvectors.
Future
works include finding more applications in physics, data sci-
ence, and other related fields.
Acknowledgement. We thank Seth Lloyd, Xiaogang Li and
Dong Yuan for their helpful discussions. This work is sup-
ported by the National Natural Science Foundation of China
(Grant No. 12175003, No. 12361161602, and No. 12247124),
NSAF (Grant No. U2330201), and Project funded by China
Postdoctoral Science Foundation (Grant No. 2023T160004)
Note-added. Another related work has appeared during the
preparation of this work [38]. In Theorem 3 and Theorem
12 of Ref [38], eigenvalue estimation is discussed based on
stronger assumptions that initial state with 𝑂(𝜀) distance to
the corresponding eigenvector can be prepared.
∗ Electronic address: xiaoyuan@pku.edu.cn
[1] T. Albash and D. A. Lidar, Adiabatic quantum computation,
Reviews of Modern Physics 90, 015002 (2018).
[2] L. Lin and Y. Tong, Near-optimal ground state preparation,
Quantum 4, 372 (2020).
[3] A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q. Zhou,
P. J. Love, A. Aspuru-Guzik, and J. L. O?brien, A variational
eigenvalue solver on a photonic quantum processor, Nature com-
munications 5, 4213 (2014).
[4] A. Gily´en, Y. Su, G. H. Low, and N. Wiebe, Quantum singular
value transformation and beyond: exponential improvements for
quantum matrix arithmetics, in Proceedings of the 51st Annual
ACM SIGACT Symposium on Theory of Computing (2019) pp.
193–204.
[5] J. M. Martyn, Z. M. Rossi, A. K. Tan, and I. L. Chuang, Grand
unification of quantum algorithms, PRX Quantum 2, 040203
(2021).
[6] K. Kawabata, K. Shiozaki, M. Ueda, and M. Sato, Symmetry
and topology in non-hermitian physics, Physical Review X 9,
041015 (2019).
[7] E. J. Bergholtz, J. C. Budich,
and F. K. Kunst, Exceptional
topology of non-hermitian systems, Reviews of Modern Physics
93, 015005 (2021).
[8] D. S. Borgnia, A. J. Kruchkov, and R.-J. Slager, Non-hermitian
boundary modes and topology, Physical review letters 124,
056802 (2020).
[9] See Supplemental Material, partially provided in this arXiv ver-
sion.
[10] G. H. Low and I. L. Chuang, Hamiltonian simulation by qubiti-
zation, Quantum 3, 163 (2019).
[11] S. Chakraborty, A. Gily´en, and S. Jeffery, The power of block-
encoded matrix powers: Improved regression techniques via
faster hamiltonian simulation, Leibniz international proceedings
in informatics 132 (2019).
[12] X.-M. Zhang and X. Yuan, On circuit complexity of quantum
access models for encoding classical data, arXiv:2311.11365
(2023).
[13] A. Y. Kitaev, A. Shen, and M. N. Vyalyi, Classical and quantum
computation, 47 (American Mathematical Soc., 2002).
[14] S. Endo, J. Sun, Y. Li, S. C. Benjamin, and X. Yuan, Variational
quantum simulation of general processes, Physical Review Let-
ters 125, 010501 (2020).
[15] N. Yoshioka, Y. O. Nakagawa, K. Mitarai, and K. Fujii, Vari-
ational quantum algorithm for nonequilibrium steady states,
Physical Review Research 2, 043289 (2020).
[16] X.-D. Xie, Z.-Y. Xue, and D.-B. Zhang, Variational quantum
eigensolvers for the non-hermitian systems by variance mini-
mization, arXiv:2305.19807 (2023).
[17] A. Y. Kitaev, Quantum measurements and the abelian stabilizer
problem, quant-ph/9511026 (1995).
[18] J. R. Norris, Markov chains, 2 (Cambridge university press,
1998).
[19] J. Odencrantz, Markov chains: Gibbs fields, monte carlo simu-
lation, and queues, Technometrics 42, 438 (2000).
[20] O. Ibe, Markov processes for stochastic modeling (Newnes,
2013).
[21] S. P. Meyn and R. L. Tweedie, Markov chains and stochastic
stability (Springer Science & Business Media, 2012).
[22] D. A. Levin and Y. Peres, Markov chains and mixing times, Vol.
107 (American Mathematical Soc., 2017).
[23] M. V. Medvedyeva, F. H. Essler, and T. Prosen, Exact bethe
ansatz spectrum of a tight-binding chain with dephasing noise,
Physical review letters 117, 137202 (2016).
[24] L. Banchi, D. Burgarth, and M. J. Kastoryano, Driven quantum
dynamics: Will it blend? Physical Review X 7, 041015 (2017).
[25] D. A. Rowlands and A. Lamacraft, Noisy spins and the
richardson-gaudin model, Physical review letters 120, 090401
(2018).
[26] T. Mori and T. Shirai, Resolving a discrepancy between liou-
villian gap and relaxation time in boundary-dissipated quan-
tum many-body systems, Physical Review Letters 125, 230604
(2020).
[27] D. Yuan, H.-R. Wang, Z. Wang, and D.-L. Deng, Solving the
liouvillian gap with artificial neural networks, Physical Review
Letters 126, 160401 (2021).
[28] B. Zhou, X. Wang, and S. Chen, Exponential size scaling of the
liouvillian gap in boundary-dissipated systems with anderson
localization, Physical Review B 106, 064203 (2022).
[29] A. M. Childs and N. Wiebe, Hamiltonian simulation using linear
combinations of unitary operations, arXiv:1202.5822 (2012).
[30] C. M. Bender and S. Boettcher, Real spectra in non-hermitian
hamiltonians having p t symmetry, Physical review letters 80,
5243 (1998).
[31] C. M. Bender, S. Boettcher, and P. N. Meisinger, Pt-symmetric
quantum mechanics, Journal of Mathematical Physics 40, 2201
(1999).
[32] A. Khare and B. P. Mandal, A pt-invariant potential with com-
plex qes eigenvalues, Physics Letters A 272, 53 (2000).
[33] E. Delabaere and D. T. Trinh, Spectral analysis of the complex
cubic oscillator, Journal of Physics A: Mathematical and General
33, 8771 (2000).
[34] A. Mostafazadeh, Pseudo-hermiticity versus pt symmetry: the
necessary condition for the reality of the spectrum of a non-
hermitian hamiltonian, Journal of Mathematical Physics 43, 205
(2002).
[35] X. Li, C. Zheng, J. Gao, and G. Long, Dynamics simulation and
numerical analysis of arbitrary time-dependent PT-symmetric
system based on density operators, (2022).
[36] S. Yu, Y. Meng, J.-S. Tang, X.-Y. Xu, Y.-T. Wang, P. Yin,
Z.-J. Ke, W. Liu, Z.-P. Li, Y.-Z. Yang, et al., Experimental
investigation of quantum p t-enhanced sensor, Physical Review
Letters 125, 240506 (2020).
[37] J.-H. Park, A. Ndao, W. Cai, L. Hsu, A. Kodigala, T. Lepetit, Y.-
H. Lo, and B. Kant´e, Symmetry-breaking-induced plasmonic
6
exceptional points and nanoscale sensing, Nature Physics 16,
462 (2020).
[38] G. H. Low and Y. Su, Quantum eigenvalue processing,
arXiv:2401.06240 (2024).
[39] W. Kahan, B. Parlett, and E. Jiang, Residual bounds on approx-
imate eigensystems of nonnormal matrices, SIAM Journal on
Numerical Analysis 19, 470 (1982).
[40] J. Erxiong, Bounds for the smallest singular value of a jordan
block with an application to eigenvalue perturbation, Linear
Algebra and its Applications 197, 691 (1994).
[41] Y. Dong, L. Lin, and Y. Tong, Ground-state preparation and
energy estimation on early fault-tolerant quantum computers via
quantum eigenvalue transformation of unitary matrices, PRX
Quantum 3, 040305 (2022).
[42] G. H. Low and I. L. Chuang, Optimal hamiltonian simulation by
quantum signal processing, Phys. Rev. Lett. 118, 010501 (2017).
7
Supplemental material (Partial)
I.
BOUND THE ACCURACY OF EIGENVALUE WITH 𝐶(𝜇): PROOF OF LEMMA. 1
𝐶(𝜇) ⩽ min |𝜇 − 𝜆 𝑗| follows straightforwardly from Weyl’s Theorem Below, we focus on the upper bound of min |𝜇 − 𝜆 𝑗|. We
begin with the diagonalizable matrix. We should proof that min |𝜇 − 𝜆 𝑗| ⩽ 𝜅𝑃𝐶(𝜇). Let ∥ · ∥ = 𝜎max(·) be the operator norm.
According to definition, the cost function satisfies
𝐶(𝜇) = 0
𝜇 = 𝜆 𝑗,
(S-1)
𝐶(𝜇) =
(𝑀 − 𝜇𝐼)−1−1
𝜇 ≠ 𝜆 𝑗.
(S-2)
When 𝜇 = 𝜆 𝑗, Lemma. 1 holds obviously. We now consider the case when 𝜇 ≠ 𝜆 𝑗. Because 𝑃𝑃−1 = 𝑃−1𝑃 = 𝐼, we have
𝑀 − 𝜇𝐼 = 𝑃(Λ − 𝜇𝐼)𝑃−1,
(S-3)
and
(𝑀 − 𝜇𝐼)−1 = 𝑃(Λ − 𝜇𝐼)−1𝑃−1.
(S-4)
Therefore,
∥(𝑀 − 𝜇𝐼)−1∥ = ∥𝑃(Λ − 𝜇𝐼)−1𝑃−1∥
⩽ ∥𝑃∥∥(Λ − 𝜇𝐼)−1∥∥𝑃−1∥
⩽ ∥𝑃∥∥𝑃−1∥∥(Λ − 𝜇𝐼)−1∥
(S-5)
By definition, we have 𝜅𝑃 ≡ 𝜎max(𝑃)/𝜎min(𝑃), where 𝜎min(𝑃) is the minimum singular value of 𝑃. Because 1/𝜎min(𝑃) =
𝜎max(𝑃−1), we have
𝜅𝑃 = 𝜎max(𝑃)𝜎max(𝑃−1) = ∥𝑃∥∥𝑃−1∥.
(S-6)
Moreover, we have
∥(Λ − 𝜇𝐼)−1∥ =
1
min |𝜇 − 𝜆 𝑗| .
(S-7)
Combining Eq. (S-5), (S-6) and (S-7), we have
𝐶(𝜇)−1 =
𝜅𝑃
min |𝜇 − 𝜆 𝑗| ,
(S-8)
which is equivalent to Eq. (8).
We then consider the defective matrix case. We first consider the Jordan normal form of the matrix 𝑀 − 𝜇𝐼. It can be expressed
as 𝑀 − 𝜇𝐼 = 𝑃 ˜Λ𝑃−1, where ˜Λ ≡ Λ − 𝜇𝐼 ≡ ˜Λ1 ⊕ ˜Λ2 ⊕ · · · ⊕ ˜Λ𝑀 is a block-diagonal matrix, where each Jordan block is
˜Λ 𝑗 =
©­­­­­
«
𝜆 𝑗 − 𝜇
1
𝜆 𝑗 − 𝜇 ...
...
1
𝜆 𝑗 − 𝜇
ª®®®®®
¬
.
(S-9)
8
According to Eq. (S-2), we have
𝐶(𝜇) =
(𝑀 − 𝜇𝐼)−1−1
=
𝑃diag

˜Λ−1
1 , ˜Λ−1
2 , · · · , ˜Λ−1
𝑁

𝑃−1
−1
⩾𝜅−1
𝑃
diag

˜Λ−1
1 , ˜Λ−1
2 , · · · , ˜Λ−1
𝑁

−1
⩾𝜅−1
𝑃
 ˜Λ−1
𝑗

−1
=𝜎min( ˜Λ 𝑗)
𝜅𝑃
.
(S-10)
Note that Eq. (S-10) is applied for arbitrary 𝑗. According to Ref.[39] (see also Ref. [40]), let 𝛿 𝑗 = |𝜆 𝑗 − 𝜇|, we have
𝜎min( ˜Λ 𝑗) ⩾
𝛿𝑚𝑗
𝑗
(1 + 𝛿 𝑗)𝑚𝑗 −1 .
(S-11)
Because the operator norm of 𝐴 is bounded by ∥𝐴∥ ⩽ 1, we also have |𝜆 𝑗| ⩽ 1 for all eigenvalues. Our searching region is also
restricted by |𝜇| ⩽ 1, so we have 𝛿 𝑗 ⩽ 2. We can simplify Eq. (S-11) as
𝜎min( ˜Λ 𝑗) ⩾

𝛿 𝑗
1 + 𝛿 𝑗
𝑚𝑗
(1 + 𝛿 𝑗) ⩾ (𝛿 𝑗/3)𝑚𝑗 .
(S-12)
Combining Eq. (S-10) with Eq. (S-12), we have
𝜅𝑃𝐶(𝜇) ⩾ (𝛿 𝑗/3)𝑚𝑗,
(S-13)
which gives
𝛿 𝑗 ⩽ 3(𝜅𝑃𝐶(𝜇))1/𝑚𝑗 .
(S-14)
Because min 𝑗 |𝜇 − 𝜆 𝑗| ⩽ 𝛿 𝑗, we have
min
𝑗
|𝜇 − 𝜆 𝑗| ⩽ 3(𝜅𝑃𝐶(𝜇))1/𝑚𝑗 .
(S-15)
When 𝐶(𝜇) ⩽ 1/𝜅𝑃, we have 𝜅𝑃𝐶(𝜇) ⩽ 1, and the right hand side of Eq. (S-15) increases monotonically with 𝑚 𝑗.
So
min 𝑗 |𝜇 − 𝜆 𝑗| ⩽ 3(𝜅𝑃𝐶(𝜇))1/𝑚max. When 𝐶(𝜇) > 1/𝜅𝑃, we have 𝜅𝑃𝐶(𝜇) > 1, so the right hand side of Eq. (S-15) is larger than
3. Because we always have min 𝑗 |𝜇 − 𝜆 𝑗| ⩽ 2, so we also have min 𝑗 |𝜇 − 𝜆 𝑗| ⩽ 3(𝜅𝑃𝐶(𝜇))1/𝑚max, and the proof is of Lemma. 1
completed.
II.
SOLUTIONS TO PROBLEM 1
In this section, we discuss the solution to Problem 1, which is summarized as the pseudo-code in Algorithm. 1. Our protocol
is based on the following lemma that can be straightforwardly verified from Lemma. 1 and the definition of 𝑂𝐶 in Lemma. 2.
Lemma 3. For diagonalizable matrix, if min 𝑗 |𝜇 − 𝜆 𝑗| ⩽ 𝑟/(2𝐾), with probability at least 1 − 𝛿, the output of 𝑂𝐶 (𝜇, 𝑟/𝐾, 𝛿) is
True. If min 𝑗 |𝜇 − 𝜆 𝑗| ⩾ 𝑟, with probability at least 1 − 𝛿, the output of 𝑂𝐶 (𝜇, 𝑟/𝐾, 𝛿) is False.
For defective matrix, we define
𝜈(𝑟) = (𝑟/3)𝑚max (2𝐾)−1.
(S-16)
if min 𝑗 |𝜇−𝜆 𝑗| ⩽ 𝜈(𝑟), with probability at least 1−𝛿, the output of 𝑂𝐶 (𝜇, 2𝜈(𝑟), 𝛿) is True. If min𝑗 |𝜇−𝜆 𝑗| ⩾ 𝑟, with probability
at least 1 − 𝛿, the output of 𝑂𝐶 (𝜇, 2𝜈(𝑟), 𝛿) is False.
Lemma. 3 is also illustrated in Fig. S1a. For each SVTS (i.e. 𝑂𝐶 (𝜇, 𝑟/𝐾, 𝛿) for defective matrix or 𝑂𝐶 (𝜇, 2𝜈(𝜇), 𝛿)), we
mark the region of 𝜇 in which the output of SVTS is “True” (with probability at least 1 − 𝛿) using yellow color. We also mark
9
Gray circle: Initial guess region
Updated guess region
Gray ring: Initial guess region
Updated guess region
True
False
SVTS
Eigenvalues inside yellow region: output “True” is likely 
Eigenvalues outside green region: output “False” is unlikely
(a)
(b)
(c)
Supplementary Figure S1: (a) Sketch of SVTS. (b) Sketch of Algorithm. 2 for shrinking the range of eigenvalue searching (Problem 1). The
initial and updated guess region is enclosed by grey circles. (c) Sketch of Algorithm. 4 for shrinking the range of point gap. The initial guess
region is a ring enclosed by two grey circles (Problem 2). The updated guess region is a ring enclosed by a grey circle and red (one of the
SVTS has output “True”) or blue (all of the SVTS has output “False”) circles.
a larger region with a green color. If all eigenvalues are outside the green region, the output of SVTS is unlikely to be “True”
(with probability at least 1 − 𝛿). Note that the radius of yellow regions is 𝑟/2𝐾 or 𝜈(𝑟) for diagonalizable and defective matrices,
the radius of green regions is 𝑟.
Because ∥𝐴∥ ⩽ 1, all eigenvalues are in D(0, 1). Our strategy is to iteratively shrink the region in which there is at least one
eigenvalue in it (with high probability). Our method contains 𝐽 = ⌈log2(1/𝜀)⌉ steps, and the process of each step is illustrated in
Fig. S1b (see also Algorithm. 2). Suppose that before the 𝑗th step, we are confidence that there is at least one eigenvalue in the
region D(𝜆gss, 𝐷). At this step, we shrink the radius of such confidence region from 𝐷 to 𝐷/2. This is achieved by introducing
a set of SVTSs, whose yellow region covers D(𝜆gss, 𝐷). This ensures that at least one of the SVTS has output “True”. Another
restriction is that the green region of each SVTS has a radius 𝐷/2. In this way, once we obtain an output “True”, we are confident
that at least one eigenvalue is in the region D(𝜆′
gss, 𝐷/2), where 𝜆′
gss is the center of such SVTS with output “True”.
Note that in Algorithm. 2, we have introduced a set of points Nnet(𝜆gss, 𝐷, 𝑚max). It represents the centers of all SVTSs
satisfying the criteria above. Equivalently, we have
D(𝜆gss, 𝐷) ⊂
Ø
𝜇∈Nnet(𝜆gss,𝐷,1)
D(𝜇, 𝐷/4𝐾),
(S-17)
when 𝑚max = 1, or
D(𝜆gss, 𝐷) ⊂
Ø
𝜇∈Nnet(𝜆gss,𝐷,𝑚max)
D(𝜇, 𝜈(𝐷/2)).
(S-18)
when 𝑚max > 1.
We first estimate the complexity of diagonalizable matrices. According to Lemma. 3, the query to each SVTS has complexity
˜𝑂(𝐾𝐷−1𝛾−1). Moreover, the area of D(𝜆gss, 𝐷) and the yellow regions of SVTSs are 𝜋𝐷2 and 𝜋𝐷2/(4𝐾)2 respectively. So it
suffices to use 𝑂(𝐾2) number of SVTSs to cover D(𝜆gss, 𝐷). Therefore, the complexity at each step is ˜𝑂(𝐾𝐷−1𝛾−1) × 𝑂(𝐾2) =
˜𝑂(𝐾3𝐷−1𝛾−1). In Algorithm. 1, the total algorithm contains 𝐽 = ⌈log2(1/𝜀)⌉ steps, and we have 𝐷 ⩾ 𝜀. So the total complexity
is ˜𝑂(𝐾3𝜀−1𝛾−1).
For defective matrix, the threshold of each SVTS is 2𝜈(𝐷/2) = 𝑂(𝐷𝑚max/𝐾). The complexity of each query to SVTS is
therefore ˜𝑂(𝐾𝐷−𝑚max𝛾−1). The yellow region of each SVTS has area 𝑂(𝐷2𝑚max/𝐾2), so totally 𝑂(𝐾2𝐷−2𝑚max+2) number
of SVTSs is required to cover D(𝜆gss, 𝐷). Therefore, the complexity for each step is ˜𝑂(𝐾3𝐷−3𝑚max+2𝛾−1), while the total
complexity of Algorithm. 1 is ˜𝑂(𝐾3𝜀−3𝑚max+2𝛾−1).
10
Algorithm 1 Quantum eigenvalue searching for Problem 1.
𝐷 ← 1, 𝛿′ ← 𝛿/⌈log2(𝐷/𝜀)⌉
while 𝐷 > 𝜀:
𝜆gss ← ℛdiag
11
Lemma 5. 𝒮(𝑅min, 𝑅max, 𝑟, 𝛿) defined in Algorithm. 4 can be realized with ˜𝑂 12
Algorithm 4 𝒮(𝑅min, 𝑅max, 𝑟, 𝛿)
if 𝑚max = 1
( ˜𝑅min, ˜𝑅max) ← 𝒮diag(𝑅min, 𝑅max, 𝑟, 𝛿)
elseif 𝑚max > 1
( ˜𝑅min, ˜𝑅max) ← 𝒮def(𝑅min, 𝑅max, 𝑟, 𝛿)
end if
return ( ˜𝑅min, ˜𝑅max)
Algorithm 4.1 𝒮diag(𝑅min, 𝑅max, 𝑟, 𝛿) (i.e. eigenvalue range shrinking subrutine for diagonalizable matrix)
𝛿′ ← 𝛿/|Nring(𝑅min, 𝑟/𝐾)|
for all 𝑡 ∈ Nring(𝑅min, 𝑟/𝐾):
𝐵 ← 𝑂𝐶 (𝑡, 𝑟/𝐾, 𝛿′)
if 𝐵 = True:
break for
end if
end for
if 𝐵 = True:
˜𝑅min ← 𝑅min
˜𝑅max ← 𝑅min + 𝑟
else if 𝐵 = False:
˜𝑅min ← 𝑅min + 𝑟/(4𝐾)
˜𝑅max ← 𝑅max
end if
return 13
yellow region of all SVTSs covers the edge of the circle with radius 𝑅min. If either of the SVTS has output true, we have 𝐵 =
True. In this case, with confidence at least 1 − 𝛿, there exists at least one eigenvalue in the region covered by the green circle. So
𝑅max is updated. Otherwise, we have 𝐵 = False. In this case, with confidence at least 1 − 𝛿, all of the eigenvalues are outside in
the region covered by the yellow circle. So 𝑅min is updated.
V.
SINGULAR VALUE THRESHOLD SUBROUTINE: PROOF OF LEMMA. 2
Given a general matrix 𝐴 ∈ C𝑁 ×𝑁 satisfying ∥𝐴∥ ⩽ 1, we can always rewrite it in the form of singular value transformation
form
𝐴 =
𝑁 −1
∑︁
𝑗=0
𝜎𝑗|𝑤 𝑗⟩⟨𝑢 𝑗|
(S-26)
for some singular value 0 ⩽ 𝜎0 ⩽ 𝜎1 · · · , orthonormal left singular vectors {|𝑤 𝑗⟩} and right singular vectors {|𝑢 𝑗⟩}. The SVTS
aims to determine if there are singular values smaller than a threshold. Below, we show how this can be realized with the
block-encoding of matrix 𝐴.
Let 𝑁 = 2𝑛, we choose the simplest definition of block encoding, i.e. an (𝑛 + 𝑎) qubit unitary 𝒪𝐴 is called the block encoding
of 𝐴 if (⟨0𝑎| ⊗ 𝐼)𝒪𝐴(|0𝑎⟩ ⊗ 𝐼) = 𝐴. It is typically required that 𝑎 = 𝑂(poly(𝑛)). Let 𝑃(·) be a real polynomial function, we
define the singular value transformation of a matrix as
𝑃(svt) (𝐴) =
 𝑃(𝜎𝑗)|𝑤 𝑗⟩⟨𝑢 𝑗|
if the degree of 𝑃(·) is odd
𝑃(𝜎𝑗)|𝑢 𝑗⟩⟨𝑢 𝑗|
if the degree of 𝑃(·) is even
(S-27)
According to [4], QSVT can be effectively constructed with 𝒪𝐴 and few extra elementary quantum gates, if 𝑃(·) satisfies some
reasonable criteria. More specifically, we have the following.
Lemma 6 (QSVT for real polynomials with definite parity, adapted from Theorem 4 in [4]). Let 𝑃 ∈ R be a polynomial function
satisfying (1) The degree of 𝑃 is at most 𝑑; (2) 𝑃 is either of even or odd parity; (3) For ∀𝑥 ∈ [−1, 1], |𝑃(𝑥)| ⩽ 1.
Then there exists a block encoding of 𝑃(𝐴) using 𝑑 queries of 𝒪𝐴, 𝒪†
𝐴, one extra ancillary qubit, and 𝑂(𝑎 + 1)𝑑 extra single-
and two-qubit gates.
As mentioned earlier, the aim of Lemma. 2 is approximately determine where the targeted eigenvalue lies, a task similar to
the fuzzy bisection scheme proposed in Ref. [41]. To this end, we also utilize the Heavised function for the bisection. Yet, the
main difference between our construction and Dong et al. [41]’s construction is that to deal with complex eigenvalues, we take
advantage of the relationship between eigendecomposition and singular value decomposition as given by Lemma. 1. That is given
the construction of the shifted matrix 𝐴 − 𝜇𝐼, if the shifted value 𝜇 is close enough to the targeted eigenvalue 𝜆 𝑗, 𝐶(𝜇) is then
close to zero. Therefore, we can decide whether there is an eigenvalue 𝜆 𝑗 that is close to the attempted shift 𝜇 by determining
the existence of singular value signals close to zero by QSVT techniques [4]. As such, an approximated Heaviside function of
the matrix is constructed. The assumption on the initial state |𝜓𝐼⟩ then dictates the threshold (lower-bound value) of the signal
that we are seeking. The main difference between our and Dong et al. [41]’s scenario is that the polynomial function we applied
is for the singular values but not eigenvalues.
The next step is thus to approximate a shifted Heaviside function 𝐻(𝑥 − 𝜃) with a polynomial function 𝑃(svt)
𝐻
(·) using QSVT
methods. The shifted Heaviside function is given by
𝐻(𝑥 − 𝜃) =
(
1,
𝑥 ≤ 𝜃
0,
𝑥 > 𝜃
(S-28)
Regarding the approximated block encoding, we say that a unitary𝑈𝐴 is the (𝛼, 𝑎, 𝜂)-block encoding of 𝐴 if ∥𝛼(⟨0𝑎|⊗𝐼)𝑈𝐴(|0𝑎⟩⊗
𝐼) − 𝐴∥ ⩽ 𝜂. From [42], we have the following lemma.
Lemma 7. Let Δ, 𝜂 ∈ (0, 0.5). Given a matrix 𝐴 with its (1, 𝑎, 0) block-encoding 𝒪𝐴, we can construct a (1, 𝑎 + 1, 𝜂)-block-
encoding of 𝑃svt
𝐻 (𝐴) satisfying |𝑃𝐻 (𝑥) − 1| ≤ 𝜂, ∀𝑥 ∈ [−1, Δ/2] and |𝑃𝐻 (𝑥)| ≤ 𝜂, ∀𝑥 ∈ [Δ, 1] using 𝑂

1
Δ log

1
𝜂

applications
of 𝒪𝐴 and 𝒪†
𝐴, and 𝑂

𝑎
Δ log

1
𝜂

extra one- and two-qubit gates.
14
Here, we have approximated the Heaviside function with a shift 𝜃 = 3Δ/4. It is worth noting that the function between interval
𝑥 ∈ [Δ/2, Δ] often takes values that smoothly interpolate the function value of the two endpoints of the interval. See Sec. V of
Ref. [41] for an example. For simplicity, we will denote the (1, 𝑎 + 1, 𝜂)-block encoding unitary of 𝑃svt
𝐻 (𝐴) as 𝑈𝐻.
As discussed in the main text, we further assume that we have a unitary oracle 𝒫𝐴 which prepares an initial state with nontrivial
overlap to |𝑢0⟩. More specifically, we have 𝒫𝐴|0𝑛⟩ = |𝜓⟩, for some |⟨𝑢0|𝜓⟩| ⩾ 𝛾. Applying 𝑈𝐻 to the join state of ancillary
qubits at state |+⟩|0𝑎+1⟩ and data qubit at state |𝜓⟩, we obtain
𝑈𝐻|+⟩|0𝑎+1⟩|𝜓⟩ = |+⟩|0𝑎+1⟩
∑︁
𝑗
𝑐 𝑗𝑃𝐻 (𝜎𝑗)|𝑢 𝑗⟩ + |garb⟩
(S-29)
for some |𝑐0| ⩾ 𝛾, and
(⟨+|⟨0𝑎+1| ⊗ 𝐼)|garb⟩ = 0.
(S-30)
If we project the ancillary qubits to |+⟩|0𝑎+1⟩, the success probability of the projection is given by
𝑝suss ≡
(⟨+|⟨0𝑎+1| ⊗ 𝐼)𝑈𝐻|+⟩|0𝑎+1⟩|𝜓⟩

(S-31)
=
∑︁
𝑗
|𝑐 𝑗|2|𝑃𝐻 (𝜎𝑗)|2.
(S-32)
If the smallest singular value of 𝐴 satisfies 𝜎0 ⩽ Δ/2, we have
𝑝suss ⩾ |𝑐0|2|𝑃𝐻 (𝜎0)|2 ⩾ |𝑐0|2(1 − 𝜂)2 ⩾ 𝛾/4.
(S-33)
If 𝜎0(𝐴) ⩾ Δ, we have
𝑝suss ⩽ 𝜂2.
(S-34)
We note that 𝜂 decays rapidly with order 𝑑 for the polynomial function. For example, we may require that the probability in the
second case is at most half of the probability in the first case, i.e.
𝜂2 ⩽ (𝛾/4)/2 = 𝛾/8.
(S-35)
This can be achieved with 𝑑 = 𝑂

log(1/𝛾)
Δ

. To distinguish whether Eq. (S-33) or Eq. (S-34) are satisfied, we can use the Monte
Carlo method by performing the projection process many times. To achieve a constant correct probability, this method requires
sampling size 𝑂(𝛾−2), and each run of the quantum circuit requires a single query to 𝑈𝐻 (Lemma 9 of Ref [41], see also Ref [2]).
Alternatively, we can improve the dependency on 𝛾 to 𝑂(𝛾−1) with the amplitude amplification method.
Lemma 8 (Lemma.12 in Ref [41]). Given a unitary 𝑊 applied at 𝑛𝑤 + 1 qubits, and let
𝜔 = ∥(⟨0| ⊗ 𝐼2𝑛𝑤 )𝑊|0⟩|0𝑛𝑤⟩∥,
(S-36)
where 𝐼2𝑛𝑤 is 2𝑛𝑤-dimensional identity. It is further promised that either 𝜔 ⩽ 𝛾1 or 𝜔 ⩾ 𝛾2 for some 0 ⩽ 𝛾1 < 𝛾2. These two
cases can be distinguished with success probability at least 1-𝛿 with 𝑂 15
We define |𝜓′⟩ = |+⟩|0𝑎+1⟩ Í
𝑗 𝑐 𝑗𝑃𝐻 (𝜎𝑗)|𝑢 𝑗⟩, and a controlled rotation 𝑅|𝜓′⟩ ≡ 𝐼2 ⊗ |𝜓′⟩⟨𝜓′| + 𝑋 ⊗ (𝐼 − |𝜓′⟩⟨𝜓′|). According
to Eq. (S-30), we have
𝑝suss = ∥(⟨0| ⊗ 𝐼2𝑎+2)𝑅|𝜓′⟩|0⟩|0𝑎+2⟩∥.
(S-40)
Here, 𝑝suss is the success probability of projection defined in Eq. (S-31). According to Eq. (S-33) and Eq. (S-34), we can set two
thresholds of projection success probabilities to be 𝛾1 = 𝜂2 ⩽ 𝛾/8 and 𝛾2 = 𝛾/4 respectively. According to Lemma. 8, we can
distinguish whether 𝑝suss ⩽ 𝛾1 or 𝑝suss ⩾ 𝛾2 with 𝑂 16
We note that for algorithms related to QSVT, the runtime is affected by the rescale factor 1 + |𝜇|. However, because we always
have |𝜇| ⩽ 1 in our application, the rescale factor is bounded by a constant and will affect the runtime significantly.
"
"Recipes provide clear instructions that capture the temporal and logical flow of a procedure. Modeling them as graphs can enable diverse applications that include robotic automation. However, training such models is challenging due to the lack of large-scale datasets of properly annotated pairs of recipes and graphs. The current paper introduces a novel unsupervised approach to training graph models from raw recipes. The main contributions of the work include proposing a scalable framework to generate graphs from recipes while learning the relationships among entities like ingredients and locations. They then present a novel method to jointly learn the graph connectivity where explicit supervision at the graph level is not available.","Procedural texts like recipes present instructions that describe actions that need to be performed in a particular order. Therefore, they have a logical and temporal flow of information. These texts are useful in training intelligent systems to carry out these procedures automatically, but the system needs a proper representation of the text. Traditional methods rely on named entity recognition sub-tasks that can fall short in the absence, misspellings, or pronouns referring to entities. Therefore, the authors propose a new method that can identify the relevant entities and actions from recipes and model their relationships using a graph structure.","nanThe paper first provides a brief overview of graph theory and graph neural networks (GNN). It introduces the concept of heterogeneous graphs and their representation using adjacency matrices. Further, it describes the Graph Convolutional Networks (GCN) and their approach to message passing for nodes in a graph. Subsequently, the paper presents related work in the areas of graph generation, text-to-graph generation, food understanding, and conditional text generation.","The authors address the challenge of generating recipe graphs without explicit supervision. Their method includes an Entity Identifier that identifies actions, ingredients, and locations in a recipe and applies a pre-trained model to encode the instruction. The Graph Structure Encoder module generates a graph representation of the recipe sequentially. It first generates a relation matrix based on the node features and then uses a Sinkhorn-Knopp algorithm to obtain an adjacency matrix. This adjacency matrix and the node features are then used to calculate a node classification loss. The Graph Structure Encoder is trained using a bi-level optimization approach, where the weights for the adjacency matrix and the weights for the GNN are updated alternately. A decoder component generates a sequence of text instructions conditioned on the generated graph representation. The decoder is a transformer-based architecture with a final softmax layer to predict words.","The authors evaluate their model on two different datasets, the ""Now You’re Cooking"" (NYC) dataset and the ""English Flow Corpus"" (EFC) dataset. They compare their results to various baselines, including a LSTM and GRU model trained for entity recognition, Adapted Recurrent Entity Network, and Neural Process Networks. The results show that the proposed method outperforms the baselines in terms of F1 score and recall for entity selection. In the graph generation task, their model achieves a competitive performance compared to the baselines and demonstrates the ability to generate meaningful graphs. Additionally, they conduct an experiment to analyze the effect of including the sentence representations in the graph encoder and find that it leads to a lower graph edit distance from the ground-truth set.","The paper presents an unsupervised approach to learning graph representations of recipes. It includes a scalable framework to generate graphs while learning the relationships among entities. The results demonstrate the effectiveness of the method in identifying relevant entities and their relationships. However, it acknowledges the limitations due to the lack of large publicly available datasets and work studies focusing on this specific issue. Future work involves generating richer graphs with more information and analyzing the semantic relations in the generated graphs to investigate whether they can predict programs that could lead to a finished product.",Unsupervised Learning of Graph from Recipes,"Aissatou Diallo, Antonis Bikakis, Luke Dickens, Anthony Hunter, Rob Miller","Unsupervised Learning of Graph from Recipes
Aïssatou Diallo∗,
Antonis Bikakis,
Luke Dickens,
Rob Miller,
Anthony Hunter
University College London, United Kingdom
Abstract
Cooking recipes are one of the most readily
available kinds of procedural text. They con-
sist of natural language instructions that can
be challenging to interpret. In this paper, we
propose a model to identify relevant informa-
tion from recipes and generate a graph to rep-
resent the sequence of actions in the recipe. In
contrast with other approaches, we use an un-
supervised approach. We iteratively learn the
graph structure and the parameters of a GNN
encoding the texts (text-to-graph) one sequence
at a time while providing the supervision by
decoding the graph into text (graph-to-text) and
comparing the generated text to the input. We
evaluate the approach by comparing the iden-
tified entities with annotated datasets, compar-
ing the difference between the input and output
texts, and comparing our generated graphs with
those generated by state of the art methods.
1
Introduction
Procedural texts are a common type of natural lan-
guage writing that provides instructions on follow-
ing a procedure. They are essential in everyday life
as they enable us to learn skills, complete tasks,
and solve problems. Generally, they are loosely
structured in step-by-step format with temporal or-
der (as they would occur in real life) and logical
flow (the results of previous steps feed into later
steps). A procedural text can be used to guide an
intelligent system for a wide range of applications,
such as automated robotics, but the representation
must be in a form that a computer-based system
can interpret (Papadopoulos et al., 2022). Given
this, these representations must ideally capture all
relevant entities and actions from the original text,
while encoding all relationships that exist between
them. A graph is a natural choice for this purpose
as it represents not only the attributes of the entities
∗ Corresponding author: a.diallo@ucl.ac.uk
Figure 1: Example of output graph with the associated
recipe. The identified entities are in bold.
but also their relationships. Moreover, being able
to represent procedural text within a graph struc-
ture can facilitate reasoning and inference, such as
adapting procedures when resources are unavail-
able or actions are impossible, see (Bikakis et al.,
2023).
More precisely, procedural text understanding
implies correctly identifying the resources and ac-
tions effects on resources, as well as understanding
the flow between the procedures. For example,
given the recipe ""combine flour, sugar and milk
in a bowl. Then, add the chocolate chips to the
mixture. Finally, bake the cookies at 250 degrees."",
an intelligent system must be able to: (i) correctly
identify entities and their types including implicit
entities, e.g. ""mixture"" as a combination of previ-
ously mentioned ingredients; (ii) understand the
temporal flow of the instructions, e.g. one must
combine the first three ingredients into a batter be-
fore adding the chocolate chips; (iii) recognize the
logical flow of information, e.g. the ""bake"" ac-
tion acts on the output of the""combine"" and ""add""
actions. This presents a challenge not present in
factoid based reading comprehension (Clark and
Gardner, 2018), in that our putative procedural text
interpreter must model the dynamics and causal
effects not explicitly mentioned in the text at hand
(Dalvi et al., 2018; Bosselut et al., 2018).
This paper presents a model, trained in a self-
supervised fashion to encode cooking recipes as
arXiv:2401.12088v1  [cs.CL]  22 Jan 2024
graphs, that identifies key actions, entities, includ-
ing those that are implicit, and their relationships.
We conduct a series of experiments showing that
our proposed method yields meaningful graphs
which can be used to reason about the original
recipe. Our key contributions can be summarized
as follows: (i) proposing a scalable approach to
transform procedural texts into graphs while learn-
ing discrete and sparse dependencies among the
entities using as a case study the cooking domain,
and (ii) presenting a new method for jointly learn-
ing the edge connectivity of a graph in absence of
explicit supervision at the graph-level.
2
Background
In this section, we introduce some preliminary no-
tions on graph theory and graph neural networks
(GNNs).
Graph Theory Basics
A (heterogeneous) graph
G can be described by the tuple G = {V, E},
where V = {v1, ..., vN} is a set of N nodes and
E = {e1, ..., eM} is a set of M edges. In the
case of heterogeneous graphs, there is a node
type mapping function ϕ : V
→ C.
For a
given graph G, the corresponding adjacency ma-
trix A ∈ {0, 1}N×N describes the connectivity
between the nodes. The connected nodes are called
neighbour nodes. Ai,j = 1 indicates that vi is ad-
jacent to vj, otherwise Ai,j = 0. Following the
notation of Franceschi et al. (2019), we denote the
set of all N × N adjacency matrices as HN.
Graph Convolutional Networks
GNNs are ma-
chine learning models designed for reasoning with
graph-structure data while modelling relational and
structural information by learning neighbour-aware
node representations. In this study, we use Graph
Convolutional Network (GCN) (Kipf and Welling,
2017) to aggregate the neighbour information for
each entity node.
hl+1
i
= σ

W
X
j∈N(i)∪{i}
1
p
d(i)d(j)
hl
i


(1)
Eq. 1 describes the propagation mechanism for the
GCN. At layer l, each node i aggregates the rep-
resentation of itself and all its neighbouring nodes
N(i) based on A. Then, the updated representation
hl+1
i
is computed based on the weight matrix of
the layer W, the activation function σ, normalized
by the degree of the source d(i) and its connected
nodes d(j).
3
Model
In this paper, we address the challenge of produc-
ing graphs of cooking recipes when no supervision
in terms of graphs is available. We aim to discover
implicit entities and relationships between actions,
ingredients and locations, and for the relationships
to carry a similar meaning, e.g. an ingredient re-
lates to an action that acts upon it without explicitly
enforcing them so that the output is loosely aligned
with the semantics of flow-graphs such as (Mori
et al., 2014b).
Task Definition
A recipe R consists of T sen-
tences R = {s0, s1, ..., sT }. Each sentence si con-
tains the action ai that operates on ii, a set of ingre-
dients in (a set of) li locations. All cooking actions,
ingredients and locations are part of a pre-specified
set of entities. Often, a cooking action can lead
to an intermediate output such as batter, dough or
mixture. In the scope of this work, these intermedi-
ate outputs are considered ingredients. The model
aims to learn a graph describing the interactions be-
tween actions, ingredients and locations for a given
recipe. These entities are represented as nodes
in the output graph and the edges are the connec-
tions between the |C| = 3 different types of nodes,
namely actions, ingredients and locations. In the
absence of a dataset containing pairs of recipes and
their corresponding cooking graphs, we tackle this
problem in an unsupervised way.
Overview
For doing this, given a recipe we first
parse it such that each instruction contains only one
cooking action (e.g., ""sprinkle with salt // mix well
// set aside in a bowl""). Then, a previously trained
model is applied to each instruction to encode the
sentence and to identify the relevant entities that
should be present in the cooking graph. Next, these
elements are sequentially passed to a graph en-
coder that jointly learns the graph structure as an
adjacency matrix, the node and graph representa-
tions. The resulting graph encoding is used as a
conditioning for a transformer-based module that
translates it back to textual cooking instructions.
Finally, the output text is compared to the original
set of cooking instructions in a cyclic way to guide
the learning process.
3.1
Entity Identifier
Given a sentence st with Ls tokens such that
st = {wi,1, ..., wi,Ls}, we use BERT (Devlin et al.,
2019) as a context encoder, which is based on a
Figure 2: Overview of the proposed model. The first part, in blue, encodes the procedures and identifies the relevant
entities (refer to Figure 3 for more details). These are passed to the second part (in red) containing the Graph
Structure Encoder module, which learns the adjacency matrix and the graph encoding (with cost Lgse) (refer to
Figure 4) and the decoder that back-translate the graph into a recipe (with cost Lgen).
stack of multilayer transformer blocks (Vaswani
et al., 2017). The instruction representation hst
is obtained by extracting the hidden state hst
0
corresponding to the special token [CLS] so that
hst = hst
[CLS]:
{hst
[CLS], ..., hst
Ls} = BERT{[CLS], ..., wst
Ls}
(2)
The first block of our model takes as input one
sentence at a time and extracts which actions are
executed on the ingredients while tracking the
changes in location and the transformation of the
ingredients. Existing procedural text understanding
models rely on some type of name entity recogni-
tion sub-task to obtain the entities the model should
focus on such as in (Tang et al., 2022). This strategy
would be sufficient if entities were always explic-
itly mentioned, but natural language often elides
these arguments or uses pronouns. As such, the
module must be able to consider entities mentioned
previously or under different names.
Figure 3: Details of the Entity Identifier. The recipe
is parsed such that each sentence contains only one
action whereas multiple ingredients and locations are
permitted.
This problem can be seen as a multitask learn-
ing problem where we need to learn to identify
actions, ingredients and locations simultaneously.
Specifically, a feed-forward network (FFN) with
a cross entropy loss is used to identify the correct
cooking action. Concurrently, two FFNs with a
binary cross entropy cost function are used to learn
the subsets of ingredients and locations for a given
instruction st. Figure 3 illustrates this component
of our approach.
3.2
Graph Structure Encoder
We aim to learn the adjacency matrix and the graph
representation of a graph G associated with a recipe
R and a set of (heterogeneous) entities. Our en-
coder takes as input one instruction at a time until
the end of the recipe. It recursively generates the
graph for the block of the cooking recipe seen up to
the t-th generation step. The output is conditioned
on the already generated graph representation at
time step t−1. The process is divided into two main
parts: (1) generating the structure of the graph, de-
scribed by the adjacency matrix; (2) learning the
parameters for the GCN as well as obtaining the
final graph representation. There are two main ap-
proaches for generating the adjacency matrix. The
first is to think of the problem as a probabilistic one
and to learn the parameters of a probability distribu-
tion over the graphs as in (Franceschi et al., 2019).
The second method is to work with a continuous
relaxation of the adjacency matrix. We chose the
latter approach in this work.
We recall that the node features of the entities are
not given and need to be learned along the graph
structure. For doing this, we initialize the feature
matrix X containing the node representations with
a pre-trained model (Pellegrini et al., 2021) spe-
cific to the cooking domain, acting as node encoder.
More specifically, given a recipe with T instruc-
tions each with an action ai, ii ingredients and li
locations with i ∈ {0, ..., T}, we first concatenate
the indices denoting the entities into a single list
of indices. We impose a specific ordering for the
concatenation: [action; ingredients; locations] and
within each subset, the items are sorted in alpha-
betical order. This yield X(t) for the t-th sentence
in the recipe with the initialized embedding layer
described earlier. We now explain the details of the
graph generation step below.
Relation matrix
We extract a relation matrix
R(t) characterizing the probability of a relation
(an edge) between two nodes based on X(t). This
is meant to capture the relationship between the
nodes and propagate this relation to the topological
structure of the graph. We first apply two different
mappings to X(t) to obtain a Z(t) = FFN(X(t))
and M(t) = FFN(X(t)). Finally, the relation ma-
trix is obtained as R(t) = Z(t)M(t)⊤.
Adjacency Processor
The relation matrix R(t)
is not suited to be used as an adjacency matrix, as
it may contain both positive and negative values
and it is non-normalized. Let us define a function
S that takes as input the relation matrix R(t) and
outputs a continuous relaxation of the adjacency
matrix A(t). S will take a matrix and transform it
into a low entropy one, meaning that most of its el-
ements will be equal to zero, hence augmenting its
sparsity. A simple algorithm for this purpose is the
Sinkhorn-Knopp algorithm (Knopp and Sinkhorn,
1967), which consists in iteratively rescaling the
rows and columns of a square matrix with positive
entries. By applying the algorithm in the log do-
main, we can apply the algorithm to the relation
matrix and obtain an adjacency matrix by finally
applying the exp operator to the output for getting
positive values. To sum up, A(t) = S(R(t))1.
Node Classification
We choose a two-layer GCN
(Kipf and Welling, 2017) that takes as input the fea-
ture matrix X(t) and the adjacency matrix A(t).
The first layer of the GCN architecture projects the
inputs into an intermediate latent space and the sec-
ond layer maps it to the output embedding space.
Due to the absence of labelled graphs to guide the
learning process, we include a pretext task, which
is a pre-designed task for the network to solve such
that it can be trained by the learning objective func-
tions of the pretext task and the features and param-
1For the sake of brevity, we slightly abuse the notation
because the S(R(t)) is not a (0, 1)-matrix but a continuous
relaxation.
Algorithm 1: Graph Structure Encoder
Input
: Trained model,
R = {s0, ..., sT }: the input recipe;
Output : A: adjacency matrix;
Y : nodes;
YC: nodes classes;
1 t ← 0
2 graphSequence = [ ]
3 for st in R do
4
X(t), Y (t) ← Entity Identifier(st)
5
Z(t), M(t) ← W1X(t) + b1,
W2X(t) + b2
6
R(t) ← Z(t)M(t)⊤
7
A(t) ← Adjacency Processor(R(t))
8
˜A(t) ← Normalization of A(t)
9
˜G(t) ← GCN(X(t), ˜A(t))
10
˜g(t) ← Graph pooling ( ˜G(t))
11
graphSequence.append(˜g(t))
12 end for
13 G ← GRU(graphSequence)
14 return G, A(T), Y (T), Y (T)
C
eters are learned through this process. A natural
choice for the pretext task is node classification and
the goal is to predict the entity type of each node by
providing the logits for each class through softmax
in this way ˆy = softmax(GCN(X, A)).
Lgse = ℓ(ˆy, y)
(3)
where ℓ corresponds to a cross-entropy loss func-
tion, y are the ground-truth labels known before-
hand and ˆy are the predicted labels. Eq. 3 will
constitute the first term of the global loss.
Recurrent Graph Embedding
We apply the
steps detailed above iteratively until reaching the
end of the recipe. At each time step, the node
vectors are the concatenation of the unique node in-
dices seen up to the t-th sentence of the recipe such
that the last iteration, the adjacency matrix A(t=T)
has m unique nodes with m = aT + iT + lT . Each
graph encoding H(t) goes through a mean pooling
function in order to obtain a global representation
as a single graph embedding. These vectors are
then stacked into a matrix ˜GR ∈ RT×d. We con-
catenate ˜GR to the sentence representations from
eq. 2 to obtain GR ∈ RT×2d.
The final sequence of graph representations is
obtained using a two-layer bidirectional GRU to
update the input sequence as seen in Figure 4.
Algorithm 1 describes the graph generation pro-
cess. Finally, this sequence is used to condition the
transformer-based decoder.
Figure 4: We exploit the ability of GRU at handling
temporal dependencies to increase the expressive power
of the iteratively constructed partial graphs. The GRU
has two inputs: the current input (the (partial) graph at t
concatenated with hst) and the previous state (output of
a GRU at t − 1).
3.3
Cooking Instruction Decoder
Given a graph sequence representing a recipe with
T steps, the goal of the cooking instruction decoder
is to produce a sequence of textual instructions
ˆR = {ˆs0, ˆs1, ..., ˆsT } by means of a transformer-
based architecture. This decoder is conditioned on
the sequence of graph embeddings provided by the
graph encoder described earlier.
The instruction decoder is composed of trans-
former blocks each of them containing a self-
attention layer applying self-attention over previ-
ously generated outputs, a cross-attention layer that
attends to the model conditioning in order to refine
the self-attention output and a linear layer. This
process is the same as the original Transformer
(Vaswani et al., 2017) and we omit it due to the
limited space. A final softmax provides a distri-
bution over the words for each time step. We use
the cross-entropy loss Lgen between the generated
instructions ˆst and the original instructions st.
3.4
Optimization
We train our model, outlined in Figure 2 in two
stages. In the first phase, we train the entity identi-
fier described in section 3.1. Then, in the second
stage, we train the graph structure encoder from
section 3.2 and the cooking instruction decoder
from section 3.3. The total loss used to perform
the learning is Ltot = Lgse + Lgen + λ∥A∥1. The
last term is a regularization term that encourages
the sparsity of the adjacency matrix. We apply a
bi-level optimization approach by alternatively up-
dating the weights involved in the learning of the
adjacency matrix and the weights for the GNN. The
cooking instruction decoder is trained with teacher
forcing (Williams and Zipser, 1989).
4
Experiments
For learning and evaluation, we use the ""Now
You’re Cooking"" (NYC) dataset used in (Bosselut
et al., 2018) that contains 65816 recipes for train-
ing, 175 recipes for validation and 700 recipes for
testing. This dataset contains annotations for the
ingredients, actions and locations for each sentence
in a given recipe. For evaluating the quality of the
graphs produced, we rely on the dataset ""English
Flow Corpus"" (EFC) by Yamakata et al. (2020).
Implementation details
We use the following
settings. We train the entity identifier for 5 epochs
with a learning rate of 2 · 10−5. The nodes’ hidden
dimension is 100. The 2-layer GCN outputs a graph
representation of hidden dimension 768. The graph
structure encoder is trained with a learning rate
of 10−3 and the instruction decoder has a base
learning rate of 10−4 and a step decay of 0.1 every
5 epochs. We train for 10 epochs. All experiments
are conducted on a Nvidia Titan X GPU.
4.1
Entity Selection
In this section, we evaluate the model performance
in identifying the correct entities and modelling
their interactions. This is framed as a classification
task. Following Bosselut et al. (2018), we rely on
the macro F1 score and the recall to measure the
performance of the model. The Table 1 shows the
results. A selected entity is denoted as one whose
sigmoid activation is greater than 0.5. The reported
scores are averaged for all three types of entities.
Baselines
The main baseline for this stage of the
framework consists of the Neural Process Networks
by Bosselut et al. (2018). For the sake of complete-
ness, we report the scores of their baseline to better
position our model. The first two baselines, repre-
sented by the first two lines of Table 1 consist of an
LSTM and GRU model trained to predict the enti-
ties without any other external information. This
is closer to our model. Adapted Recurrent Entity
Network (Henaff et al., 2017) and (Bosselut et al.,
2018) are both RNN-based models. The first one
is a network doted with external “memory chains”
with a delayed memory update mechanism to track
entities. The second one aims to simulate the ef-
fects of actions on entities in procedural text. It
embeds input sentences in a GRU, then uses a FFN
Model
F1
Recall
2-layer LSTM Entity Recognizer
50.98
13.33
Adapted Gated Recurrent Entity
45.94
7.74
EntNet
48.57
9.87
Neural Process Networks
55.39
20.45
Ours
65.14
63.44
Table 1: Results for entity selection. The reported scores
are averaged for all three types of entities.
NYC
EFC
BLEU
ROUGEL
BLEU
ROUGEL
NPN
0.37
0.35
-
-
Ours w/o s.
0.42
0.41
0.44
0.39
Ours
0.48
0.45
0.49
0.38
Table 2: Text generation results for text → graph and
graph → text.
to predict actions which occur in each sentence and
sentence-level and recurrent attention mechanisms
to identify entities affected by actions.
Graph-edit distance
RandomGraph
112.1
Ours w/o sent
71.4
Ours
67.1
Table 3: Graph edit distance. Due to the high computa-
tional cost for graph matching, we report the result for
10 sample pairs of generated graphs and ground truths.
Results
From the results, we can observe that
a transformer-based classifier outperforms all the
baselines. However, it should be noted that in the
ingredient type entity selection, the model some-
times fails. Given the sentence add sugar and re-
maining broth, the model selects chicken broth and
sugar as ingredients while ignoring most of the
other ingredients from the ground truth-set. This is
probably due to the fact that there is a need to inject
external information or explicitly track the ingredi-
ents that have been used and consumed throughout
the recipe because the sole information contained
in the sentence is not sufficient to infer a mapping
to previous ingredients. The situation slightly im-
proves, when including intermediates ""ingredients""
as an entity that should be tracked. The model is
not required to link the construct dough to its sin-
gle components but can continue to reason about
actions and locations related to this food item.
4.2
Graph Generation
In this section, we test the graph generation perfor-
mance of our model. The goal is to validate the
hypothesis that our proposed model is capable of
generating meaningful graphs. For doing this, we
compare against two different baselines and ana-
lyze the graphs obtained against the ground-truth
graph-flow from (Yamakata et al., 2020).
Baselines
The lack of large datasets with recipes
and graph pairs is the main reason that motivates
this study. In order to assess the validity of our
method, we evaluate two aspects of our method.
The first one is the quality of the generated text.
This allows us to indirectly estimate whether the
graph encoder is capable of extracting meaning-
ful relations. We report the scores of this indirect
evaluation using benchmark metrics for text gener-
ation. Additionally, we want to evaluate the quality
of the graphs. For this, we use the dataset by Ya-
makata et al. (2020). We apply our model to the
textual recipes and evaluate the text generation per-
formance and the output graph in terms of graph
edit distance (GED) (Sanfeliu and Fu, 1983). For a
novel line of work, the RandomGraph, a random
graph structure, is an intuitive baseline that gives
us insights into our approach. We include as a com-
parison the performance of our model without the
concatenation of the sentence representations but
only the node representation learned by the graph
encoder.
Metrics
For the graph → text task, we use BLEU
(Papineni et al., 2002) and ROUGEL (Lin, 2004) to
evaluate the closeness of our generated text to the
input recipe. These metrics measure the accuracy
of the n-grams between the predicted text and the
ground truth. For evaluating the performance of
the text → graph task, we measure the GED be-
tween our predicted graphs and the ground truth.
This score is to find the best set of transformations
that can transform graph g1 into graph g2 by means
of edit operations on graph g1. The allowed op-
erations are inserting, deleting and/or substituting
vertices and their corresponding edges:
GED(g1, g2) =
min
e1,...,ek∈γ(g1,g2)
k
X
i=1
c(ei)
(4)
where c is the cost function for an editing opera-
tion ei and γ(g1, g2) identifies the set of edit paths
transforming g1 into g2.
Reference text
add ingredients in the order suggested by your manufacturer.
set for dough setting and start the machine. when the unit signals remove dough.
pat dough into a greased 30cm ( 12 in ) round pizza pan. let stand 10 minutes.
preheat oven to 200 c / gas mark 6. spread pizza sauce over dough.
sprinkle toppings over sauce ( such as basil and mozzarella for a simple margherita pizza ).
bake 15 to 20 minutes, or until crust is golden brown.
Generated text
add the ingredients in the order listed.
set machine on, the dough cycle is best.
form the dough and pat into 12 - inch round pizza pan.
let stand 15 mins. preheat oven to 350f 200c. spread pizza sauce over dough.
sprinkle cheeses, then tomato sauce.
bake 10 to 15 minutes or until crust is golden brown
Table 4: Example of generated text from the graph representation of a recipe for making pizza.
Predicted
Ground truth
st−1
if you use canned chickpeas, reduce or
omit the salt.
garlic, yogurt,
chili
chili powder, curry powder, garlic,
ginger, parsley, salt , yogurt
st
mix yogurt , garlic and spices.
st−1
heat the oil.
beef
cornstarch, garlic clove, oil, oyster
sauce, pepper, sherry, soy sauce,
sugar
st
stir-fry beef quickly, until the meat is
medium rare.
st−1
dip bottom of balloon in melted choco-
late and sit on cookie sheet.
chocolate
chocolate, raspberry
st
refrigerate until hard.
st−1
sift flour in a bowl.
cornstarch, sugar cornstarch, sugar
st
combine the sugar and cornstarch in a
saucepan.
Table 5: Examples of selected ingredients. We provide the sentence st and sentence st−1 for context. Ingredients
are displayed in green if they are present in the ground truth set of ingredients and red otherwise. Best viewed in
color.
Results
As shown in Table 2, our model achieves
a good performance for the task of unsupervised
graph generation. The recipes generated (an exam-
ple is given in Table 4) are coherent and follow the
flow of the input recipes. Moreover, the analysis
of some examples shows that in most cases the
original ingredients are recognized by the model
and placed in the correct context. Considering the
graph generation, Table 3 shows the results of the
GED score. We compare against the ground-truth
flow graphs from the ""EFC"" corpus. It is impor-
tant to clarify that these graphs are structurally
different from ours. They are generated from a
pre-processing task involving tagging each word of
the recipe and predicting the edges between these
nodes. Before the evaluation, we aggregate the du-
plicate nodes and discard input recipes with less
than two steps. We then compute the GED by sam-
pling 10 graphs and averaging the scores. This
compromise is due to the high computational cost
of the graph-matching problem. We compare the
graphs obtained by our model to the processed
ground truth and additionally compare the GED
score to RandomGraph. We can notice how the
complete model including the sentence representa-
tion has a lower GED from the ground-truth set.
5
Related Work
Graph Generation
Graph generation is the task
of modeling and generating real-world graphs.
Early works generated graphs with structural as-
sumptions defined a priori (Albert and Barabási,
2002; Leskovec et al., 2010) although in many do-
mains, the network properties and generation prin-
ciples are largely unknown. Motivated by the need
to improve fidelity, researchers studied methods
to learn generative models from observed graphs.
Recent works in graph generation exploit deep gen-
erative models such as variational autoencoders
(Kingma and Welling, 2013) and generative adver-
sarial models(Goodfellow et al., 2014). There are
different approaches for generating graphs and they
differ based on the elements of the graph they gen-
erate at each step (or in one shot): (i) node (Su
et al., 2019; Assouel et al., 2018; Liu et al., 2018;
Kearnes et al., 2019), edge (Goyal et al., 2020; Bac-
ciu et al., 2020, 2019) and motif sequence (Liao
et al., 2019; Podda et al., 2020), respectively gen-
erate node, edge or sequence of smaller graphs at
each time step; (ii) adjacency matrix based (Ma
et al., 2018; Pölsterl and Wachinger, 2019; De Cao
and Kipf, 2018), that directly learns a mapping to
the latent embedding to the output graph in terms
of an adjacency matrix, generally with the addi-
tion of node/edge attribute matrices/tensors; and
(iii) methods generate the edge probability based
on pairwise relationships between the given nodes
embedding (Grover et al., 2019; Kipf and Welling,
2016; Shi et al., 2020) which is close to our method.
The intuition behind these models is that nodes that
are close in the embedding space should have a
high probability of being connected. However, we
do not follow the standard method of enforcing
semantic similarity between nodes as a mean for
generating the adjacency matrix as the relationship
we want to emerge from the graph is of the type
action → ingredient and location → action.
Text to graph
The literature on text-to-graph
generation is predominantly based of knowledge
graphs generation methods (Gardent et al., 2017;
Flanigan et al., 2016; Konstas and Lapata, 2013; Li
et al., 2021) although other works generate flows
graphs similar to ours. One specificity of our ap-
proach is that due to the generation methods, we
do not allow for nodes to repeat themselves which
is not the case for flow graphs. Additionally, our
work relates to procedural knowledge extraction
task (Qian et al., 2020; Hanga et al., 2020; Honkisz
et al., 2018) which is fundamental for transforming
natural texts into structured data like graphs. (Guo
et al., 2020) proposed an unsupervised training
method that can iteratively back-translate between
the text and graph data in a cyclic manner, such that
the problem becomes similar to finding an align-
ment between unpaired graphs and related texts.
(Guo et al., 2020). The fundamental difference is
that we do not have access to graph-structure data.
Food understanding
Large datasets focusing on
the cooking domain have gathered a new interest
leading to advancements in food understanding.
Most notable examples of these datasets are Food-
101(Bossard et al., 2014) and Recipe1M (Salvador
et al., 2017). Unlike our method, these benchmark
datasets have been mostly exploited as reference
benchmarks for the computer vision literature. The
text-based studies in natural language processing
have focused on tasks such as recipe generation in
the context of procedural text understanding in the
form of flow-graphs (Hammond, 1986; Mori et al.,
2014a,b) and more generally recipe parsing (Chang
et al., 2018; Jermsurawong and Habash, 2015; Kid-
don et al., 2015). However, the datasets used in
these studies are small (less than 300 recipes) and
the supervision is limited. The edge connectivity is
obtained from several structural assumptions and
the nodes are uniquely identified within the graphs.
Our work proposed an unsupervised approach that
provides a method to cope with scalability.
Conditional text generation
Conditional text
generation with auto-regressive models has been
widely studied in the literature. There are different
types of conditioning. The neural machine transla-
tion task is an example of text-based conditioning
(Gehring et al., 2017; Fan et al., 2018; Sutskever
et al., 2014; Vaswani et al., 2017) is The image
captioning task (Vinyals et al., 2015; Xu et al.,
2015; Lu et al., 2017) uses conditioning based on
data in other modalities, namely the input to the
decoder is an image representation. Different ar-
chitectures have been studied for this task, such
as RNN (Fan et al., 2018), CNNs (Gehring et al.,
2017) or attention-based methods (Vaswani et al.,
2017). In this paper, we exploit the conditional text
generation task using the learned graph representa-
tion as a prompt to the text decoder.
6
Conclusion
We proposed a model that transforms a procedural
text into a graph condensing the relevant informa-
tion necessary to perform the procedures. This is
performed in an unsupervised way as no supervi-
sion at the graph-level is provided. More specifi-
cally, only the raw recipe text is used to construct
a graph. Experimental results show the validity of
our approach. The objective relates to how faith-
ful the recovered text is to the original recipe text.
These representations are meant to able to provide
abstraction from the expression in surface strings.
The graphs produced can be integrated into auto-
mated reasoning frameworks. In future work, we
plan to generate richer graphs containing more in-
formation in terms of qualifiers and quantifiers, that
can be used to teach automated agents how to exe-
cute and reason about procedures.
7
Limitations
The scope of this work is to study an approach
to produce a formal representation in the form of
graphs when no supervision data is available. The
lack of large publicly available datasets and work
studies focusing on this specific issue limits the
depth of analysis that could be performed. More-
over, in this work, we did some specific choices
that may make the graphs produced less directly
human interpretable. This is highlighted by the
choice of having persistent entities in the recipe
represented by a single node that can have mul-
tiple connections whereas the flow graphs in the
cooking domain stress less this aspect by allowing
duplicates in the graph. This reflects the alternative
intended use of the representations.
There are additional concerns that need to be
considered when generating graphs from cooking
recipes. (i) bias: the model could be biased to-
wards certain cuisines. Our model autonomously
extracts the cooking ingredients to be included in
the graphs. However, there is a possibility that
some ingredients might not be recognized as such
due to not being part of the dataset of origin used
to train the entity selection module; (ii) inaccura-
cies or misinformation: the graph generator may
generate graphs that contain inaccurate or mislead-
ing information. This is due to the self-supervised
nature of the framework.
Finally, future work involves an analysis of the
semantic relations that are represented within the
learned graphs to investigate whether programs can
be predicted from this representation that could
lead to a finished product.
8
Ethics Statement
Data sources
Now You’re Cooking dataset
(Bosselut et al., 2018) and corpora used in this
work is publicly available.
The English Flow
Corpus dataset (Yamakata et al., 2020) is made
available upon requesting it from the original au-
thors. To quantify the performance of our model,
we use BLEU and ROUGEL scoring implementa-
tions using the pycocoevalcap tool from https:
//github.com/salaniz/pycocoevalcap.
Models
The output graphs are closer to abstract
representations and are not intended to be used by
users to interact directly. The generative models are
based on pre-trained language models, which may
generate misleading information if not prompted
correctly.
References
Réka Albert and Albert-László Barabási. 2002. Statis-
tical mechanics of complex networks. Reviews of
modern physics, 74(1):47. Publisher: APS.
Rim Assouel, Mohamed Ahmed, Marwin H. Segler,
Amir Saffari, and Yoshua Bengio. 2018. Defactor:
Differentiable edge factorization-based probabilistic
graph generation. arXiv preprint arXiv:1811.09766.
Davide Bacciu, Alessio Micheli, and Marco Podda.
2019.
Graph generation by sequential edge pre-
diction.
In 27th European Symposium on Artifi-
cial Neural Networks, Computational Intelligence
and Machine Learning, ESANN 2019, pages 95–100.
ESANN (i6doc. com).
Davide Bacciu, Alessio Micheli, and Marco Podda.
2020.
Edge-based sequential graph generation
with recurrent neural networks. Neurocomputing,
416:177–189. ISBN: 0925-2312 Publisher: Elsevier.
Antonis Bikakis, Aissatou Diallo, Luke Dickens, An-
thony Hunter, and Rob Miller. 2023. A Graphical For-
malism for Commonsense Reasoning with Recipes.
ArXiv:2306.09042 [cs].
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
2014. Food-101–mining discriminative components
with random forests. In European conference on
computer vision, pages 446–461. Springer.
Antoine Bosselut, Omer Levy, Ari Holtzman, Corin
Ennis, Dieter Fox, and Yejin Choi. 2018. Simulating
Action Dynamics with Neural Process Networks.
Minsuk Chang, Léonore V. Guillain, Hyeungshik Jung,
Vivian M. Hare, Juho Kim, and Maneesh Agrawala.
2018. Recipescape: An interactive tool for analyz-
ing cooking instructions at scale. In Proceedings
of the 2018 CHI Conference on Human Factors in
Computing Systems, pages 1–12.
Christopher Clark and Matt Gardner. 2018. Simple and
Effective Multi-Paragraph Reading Comprehension.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 845–855.
Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau
Yih, and Peter Clark. 2018. Tracking State Changes
in Procedural Text: a Challenge Dataset and Models
for Process Paragraph Comprehension. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), pages 1595–1604.
Nicola De Cao and Thomas Kipf. 2018. MolGAN: An
implicit generative model for small molecular graphs.
arXiv preprint arXiv:1805.11973.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
Deep Bidirectional Transformers for Language Un-
derstanding. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. arXiv preprint
arXiv:1805.04833.
Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime
Carbonell. 2016. Generation from Abstract Meaning
Representation using Tree Transducers. In Proceed-
ings of the 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 731–
739, San Diego, California. Association for Compu-
tational Linguistics.
Luca Franceschi, Mathias Niepert, Massimiliano Pontil,
and Xiao He. 2019. Learning Discrete Structures
for Graph Neural Networks. In Proceedings of the
36th International Conference on Machine Learning,
pages 1972–1982. PMLR. ISSN: 2640-3498.
Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017. The WebNLG
Challenge: Generating Text from RDF Data. In Pro-
ceedings of the 10th International Conference on
Natural Language Generation, pages 124–133, San-
tiago de Compostela, Spain. Association for Compu-
tational Linguistics.
Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N. Dauphin. 2017. Convolutional
sequence to sequence learning.
In International
conference on machine learning, pages 1243–1252.
PMLR.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C.
Courville, and Yoshua Bengio. 2014. Generative
Adversarial Nets. In NIPS.
Nikhil Goyal, Harsh Vardhan Jain, and Sayan Ranu.
2020. GraphGen: a scalable approach to domain-
agnostic labeled graph generation. In Proceedings of
The Web Conference 2020, pages 1253–1263.
Aditya Grover, Aaron Zweig, and Stefano Ermon. 2019.
Graphite: Iterative Generative Modeling of Graphs.
In Proceedings of the 36th International Conference
on Machine Learning, pages 2434–2444. PMLR.
ISSN: 2640-3498.
Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang,
David Wipf, and Zheng Zhang. 2020. CycleGT: Un-
supervised Graph-to-Text and Text-to-Graph Gener-
ation via Cycle Training. In Proceedings of the 3rd
International Workshop on Natural Language Gener-
ation from the Semantic Web (WebNLG+), pages 77–
88, Dublin, Ireland (Virtual). Association for Com-
putational Linguistics.
Kristian J. Hammond. 1986. CHEF: A Model of Case-
based Planning. In AAAI, volume 86, pages 267–271.
Khadijah M. Hanga, Yevgeniya Kovalchuk, and Mo-
hamed Medhat Gaber. 2020. A Graph-Based Ap-
proach to Interpreting Recurrent Neural Networks in
Process Mining. IEEE Access, 8:172923–172938.
Mikael Henaff, Jason Weston, Arthur Szlam, Antoine
Bordes, and Yann LeCun. 2017. Tracking the World
State with Recurrent Entity Networks. In 5th Inter-
national Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Con-
ference Track Proceedings. OpenReview.net.
Krzysztof Honkisz, Krzysztof Kluza, and Piotr Wis-
niewski. 2018. A Concept for Generating Business
Process Models from Natural Language Description.
In Knowledge Science, Engineering and Manage-
ment - 11th International Conference, KSEM 2018,
Changchun, China, August 17-19, 2018, Proceedings,
Part I, volume 11061 of Lecture Notes in Computer
Science, pages 91–103. Springer.
Jermsak Jermsurawong and Nizar Habash. 2015. Pre-
dicting the structure of cooking recipes. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 781–786.
Steven Kearnes, Li Li, and Patrick Riley. 2019. Decod-
ing molecular graph embeddings with reinforcement
learning. arXiv preprint arXiv:1904.08915.
Chloé Kiddon, Ganesa Thandavam Ponnuraj, Luke
Zettlemoyer, and Yejin Choi. 2015. Mise en place:
Unsupervised interpretation of instructional recipes.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
982–992.
Diederik P. Kingma and Max Welling. 2013. Auto-
encoding
variational
bayes.
arXiv
preprint
arXiv:1312.6114.
Thomas N. Kipf and Max Welling. 2016.
Vari-
ational graph auto-encoders.
arXiv preprint
arXiv:1611.07308.
Thomas N. Kipf and Max Welling. 2017.
Semi-
Supervised Classification with Graph Convolutional
Networks. ArXiv:1609.02907 [cs, stat].
Paul Knopp and Richard Sinkhorn. 1967. Concerning
nonnegative matrices and doubly stochastic matrices.
Pacific Journal of Mathematics, 21(2):343–348. Pub-
lisher: Pacific Journal of Mathematics, A Non-profit
Corporation.
Ioannis Konstas and Mirella Lapata. 2013. Inducing
Document Plans for Concept-to-Text Generation. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2013, 18-21 October 2013, Grand Hyatt Seattle, Seat-
tle, Washington, USA, A meeting of SIGDAT, a Spe-
cial Interest Group of the ACL, pages 1503–1514.
ACL.
Jure Leskovec, Deepayan Chakrabarti, Jon Kleinberg,
Christos Faloutsos, and Zoubin Ghahramani. 2010.
Kronecker graphs: an approach to modeling net-
works. Journal of Machine Learning Research, 11(2).
ISBN: 1532-4435.
Junyi Li, Tianyi Tang, Wayne Xin Zhao, Zhicheng
Wei, Nicholas Jing Yuan, and Ji-Rong Wen. 2021.
Few-shot Knowledge Graph-to-Text Generation with
Pretrained Language Models.
In Findings of the
Association for Computational Linguistics: ACL-
IJCNLP 2021, pages 1558–1568, Online. Association
for Computational Linguistics.
Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will
Hamilton, David K. Duvenaud, Raquel Urtasun, and
Richard Zemel. 2019. Efficient graph generation
with graph recurrent attention networks. Advances in
neural information processing systems, 32.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries.
In Text summarization
branches out, pages 74–81.
Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and
Alexander Gaunt. 2018. Constrained graph varia-
tional autoencoders for molecule design. Advances
in neural information processing systems, 31.
Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard
Socher. 2017. Knowing When to Look: Adaptive
Attention via a Visual Sentinel for Image Captioning.
In 2017 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2017, Honolulu, HI, USA,
July 21-26, 2017, pages 3242–3250. IEEE Computer
Society.
Tengfei Ma, Jie Chen, and Cao Xiao. 2018. Constrained
generation of semantically valid graphs via regular-
izing variational autoencoders. Advances in Neural
Information Processing Systems, 31.
Shinsuke Mori, Hirokuni Maeta, Tetsuro Sasada,
Koichiro Yoshino, Atsushi Hashimoto, Takuya Fu-
natomi, and Yoko Yamakata. 2014a. Flowgraph2text:
Automatic sentence skeleton compilation for proce-
dural text generation. In Proceedings of the 8th Inter-
national Natural Language Generation Conference
(INLG), pages 118–122.
Shinsuke Mori, Hirokuni Maeta, Yoko Yamakata, and
Tetsuro Sasada. 2014b.
Flow graph corpus from
recipe texts. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC’14), pages 2370–2377.
Dim P. Papadopoulos, Enrique Mora, Nadiia Chepurko,
Kuan Wei Huang, Ferda Ofli, and Antonio Torralba.
2022. Learning Program Representations for Food
Images and Cooking Recipes. ArXiv:2203.16071
[cs].
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics, pages 311–318.
Chantal Pellegrini, Ege Özsoy, Monika Wintergerst, and
Georg Groh. 2021. Exploiting Food Embeddings
for Ingredient Substitution. In HEALTHINF, pages
67–77.
Marco Podda, Davide Bacciu, and Alessio Micheli.
2020. A deep generative model for fragment-based
molecule generation. In International Conference
on Artificial Intelligence and Statistics, pages 2240–
2250. PMLR.
Sebastian Pölsterl and Christian Wachinger. 2019.
Likelihood-free inference and generation of molecu-
lar graphs. arXiv preprint arXiv:1905.10310.
Chen Qian, Lijie Wen, Akhil Kumar, Leilei Lin, Li Lin,
Zan Zong, Shuang Li, and Jianmin Wang. 2020. An
Approach for Process Model Extraction by Multi-
grained Text Classification. In Advanced Informa-
tion Systems Engineering - 32nd International Con-
ference, CAiSE 2020, Grenoble, France, June 8-12,
2020, Proceedings, volume 12127 of Lecture Notes
in Computer Science, pages 268–282. Springer.
Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier
Marin, Ferda Ofli, Ingmar Weber, and Antonio Tor-
ralba. 2017. Learning cross-modal embeddings for
cooking recipes and food images. In Proceedings of
the IEEE conference on computer vision and pattern
recognition, pages 3020–3028.
Alberto Sanfeliu and King-Sun Fu. 1983. A distance
measure between attributed relational graphs for pat-
tern recognition. IEEE transactions on systems, man,
and cybernetics, (3):353–362.
ISBN: 0018-9472
Publisher: IEEE.
Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang,
Ming Zhang, and Jian Tang. 2020. Graphaf: a flow-
based autoregressive model for molecular graph gen-
eration. arXiv preprint arXiv:2001.09382.
Shih-Yang Su, Hossein Hajimirsadeghi, and Greg Mori.
2019. Graph generation with variational recurrent
neural network. arXiv preprint arXiv:1910.01743.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
Advances in neural information processing systems,
27.
Jialong Tang, Hongyu Lin, Meng Liao, Yaojie Lu, Xi-
anpei Han, Le Sun, Weijian Xie, and Jin Xu. 2022.
Procedural Text Understanding via Scene-Wise Evo-
lution. arXiv preprint arXiv:2203.07600.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Pro-
cessing Systems, volume 30. Curran Associates, Inc.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural image
caption generator. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition,
pages 3156–3164.
Ronald J Williams and David Zipser. 1989. A learn-
ing algorithm for continually running fully recurrent
neural networks. Neural computation, 1(2):270–280.
Publisher: MIT Press One Rogers Street, Cambridge,
MA 02142-1209, USA journals-info ....
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron C. Courville, Ruslan Salakhutdinov, Richard S.
Zemel, and Yoshua Bengio. 2015. Show, Attend and
Tell: Neural Image Caption Generation with Visual
Attention. In Proceedings of the 32nd International
Conference on Machine Learning, ICML 2015, Lille,
France, 6-11 July 2015, volume 37 of JMLR Work-
shop and Conference Proceedings, pages 2048–2057.
JMLR.org.
Yoko Yamakata, Shinsuke Mori, and John Carroll. 2020.
English Recipe Flow Graph Corpus. In Proceedings
of the Twelfth Language Resources and Evaluation
Conference, pages 5187–5194, Marseille, France. Eu-
ropean Language Resources Association.
"
"Large language models (LLMs) have shown impressive in-context learning (ICL) abilities, but the performance of ICL varies with the choice of demonstrations, whose factors remain unclear. We investigate the impact of both test data and models on ICL and propose TopK + ConE, a data- and model-dependent demonstration selection method, which outperforms previous approaches. Extensive analyses validate the effectiveness, robustness, and universality of our method, providing a unified explanation for previous method effectiveness.","Large language models (LLMs) have achieved widespread success across various NLP tasks due to their remarkable emergent capabilities. One of the most exciting emergent abilities is in-context learning (ICL), which utilizes only a few input-output examples to help LLMs make better predictions. Recent work found ICL is sensitive to the choice of in-context examples, leading to researchers exploring methods to find high-performing demonstrations. Influenced by the success of the retrieval module, the retrieval module has become a standard module of the ICL framework. Extensive research has been conducted to search for demonstrations similar to the test samples. However, prior work usually only focuses on test data, overlooking the impact of models.","nanTo understand the influence of the test data, we divide the test set into subsets and select the best demonstrations for each of them, and compare their performance on different subsets. To investigate the role of models, we explore its performance across different retrieval modules and inference models. Experimental results show that the ICL performance can largely vary with different models even with the same demonstrations, indicating that the choice of demonstration is model-dependent.","We propose a demonstration selection method, denoted as TopK+ConE. Our method is based on the conjecture that effective demonstrations are those that enhance the inference model’s understanding of the test input. Specifically, we initially employed the TopK method to narrow down the pool of demonstration candidates, followed by ranking these candidates based on the conditional entropy (estimated by the model itself) of the test sample input. Extensive experiments demonstrate the effectiveness of our method across different model scales. Further analyses show the universality and robustness, and provide a unified view of why previous demonstration selection methods work.","Our method brings consistent performance improvements on almost all types of tasks. Results show the superior performance of our approach compared to the existing state-of-the-art method, TopK+MDL, across the majority of tasks, resulting in an average accuracy improvement of 1.2%. Compared with our selection method TopK, our method considerably improved the performance on 6 tasks out of the total 7 tasks, yielding an average gain of 1.8%, proving the effectiveness of improving the model’s understanding to test input.","In this paper, we take the first step to investigate the factors that influence the choice of demonstrations in in-context learning from both data and model perspectives. Based on the findings, we propose a data- and model-dependent selection method, which outperforms previous approaches. Extensive analyses validate the effectiveness, robustness, and universality of our method.",Revisiting Demonstration Selection Strategies in In-Context Learning,"Keqin Peng, Liang Ding, Yancheng Yuan, Xuebo Liu, Min Zhang, Yuanxin Ouyang, Dacheng Tao","Revisiting Demonstration Selection Strategies in In-Context Learning
Keqin Peng1, Liang Ding2∗, Yancheng Yuan3
Xuebo Liu4, Min Zhang4, Yuanxin Ouyang1, Dacheng Tao2
1Beihang University 2The University of Sydney
3The Hong Kong Polytechnic University
4Harbin Institute of Technology, Shenzhen, China
keqin.peng@buaa.edu.cn, liangding@gmail.com
Abstract
Large language models (LLMs) have shown an
impressive ability to perform a wide range of
tasks using in-context learning (ICL), where
a few examples are used to describe a task to
the model. However, the performance of ICL
varies significantly with the choice of demon-
strations, and it is still unclear why this happens
or what factors will influence its choice. In this
work, we first revisit the factors contributing
to this variance from both data and model as-
pects, and find that the choice of demonstration
is both data- and model-dependent. We further
proposed a data- and model-dependent demon-
stration selection method, TopK + ConE,
based on the conjecture that the performance of
a demonstration positively correlates with its
contribution to the model’s understanding of
the test samples, resulting in a simple and effec-
tive recipe for ICL. Empirically, our method
yields consistent improvements in both lan-
guage understanding and generation tasks with
different model scales. Further analyses con-
firm that, besides the generality and stability
under different circumstances, our method pro-
vides a unified explanation for the effectiveness
of previous methods. Code will be released.
1
Introduction
Large language models (LLMs, Ouyang et al.,
2022; Touvron et al., 2023) have achieved
widespread success across many NLP tasks (Zhong
et al., 2023; Peng et al., 2023; Lu et al., 2023) due
to their remarkable emergent abilities (Wei et al.,
2022). One of the most exciting emergent abilities
is in-context learning (ICL, Brown et al., 2020b),
which utilizes only a few input-output examples to
help LLMs make better predictions (Dong et al.,
2022). ICL has shown its effectiveness in eliciting
LLMs’ advanced capabilities and has (almost) be-
come a common practice in tackling complex tasks.
However, prior work (Liu et al., 2022; Lu et al.,
∗Corresponding Author.
GPT-J-6B
LLAMA2-7B
LLAMA2-13B
92
93
94
95
96
97
Accuracy
92.8
92.8
94.5
94.5
95.6
95.6
93.9
93.9
95.2
95.2
95.3
95.3
95.4
95.4
96.3
BM25
TopK
TopK + ConE (Ours)
Figure 1:
The different performance of data-
dependent methods (BM25 and TopK) and Our
methods in SST-2. The color in the number represents
the relative performance between BM25 and TopK. We
see that: 1) The data-dependent methods can not obtain
optimal demonstrations under different models; 2) Our
data- and model-dependent methods can achieve consis-
tent improvement across different models.
2022) has found that ICL is very sensitive to the
choice of in-context examples and their order in
the prompt, and even small changes can result in
large variance (Iter et al., 2023).
The sensitivity of ICL motivates researchers to
explore methods to find high-performing demon-
strations. Influenced by the success of leverag-
ing a retrieval module to augment neural net-
works (Hashimoto et al., 2018), the retrieval
module has become a standard module of ICL
framework for retrieval demonstrations from a
dataset (Liu et al., 2022; Rubin et al., 2022). Ex-
tensive research has been conducted to search for
demonstrations similar to the test samples (Liu
et al., 2022; Su et al., 2023; Luo et al., 2023). For
example, Liu et al. (2022) proposed to select the
samples that are closer to the test sample in the
embedding space as in-context examples, and Luo
et al. (2023) found that choosing the high word-
overlap samples can also improve the performance.
Despite empirical success to some extent, the
above methods usually only focus on the test data,
arXiv:2401.12087v1  [cs.CL]  22 Jan 2024
overlooking the impact of models. To figure out
what factors influence the choice of demonstrations,
we revisit the performance of ICL from both the
test data and model aspects. To study the influ-
ence of the test data, we divide the test set into
several subsets and select the best demonstrations
for each of them, respectively, and compare the per-
formance of these demonstrations upon different
subsets in §2.1. To understand the role of the mod-
els, we investigate its performance across different
retrieval modules and inference models in §2.2. Ex-
perimental results show that the ICL performance
can largely vary with different models even with
the same demonstrations (see Figure 1 as an exam-
ple), indicating that the choice of demonstration
is not only dependent on test data but also on the
retrieval modules and inference models.
Based on above findings, we propose a demon-
stration selection method, denoted as TopK+ConE.
Our method is based on the conjecture that effective
demonstrations are those that enhance the inference
model’s understanding of the test input. Specifi-
cally, we initially employed the TopK (Liu et al.,
2022) method to narrow down the pool of demon-
stration candidates, followed by ranking these can-
didates based on the conditional entropy (estimated
by the model itself) of the test sample input. Exten-
sive experiments demonstrate the effectiveness of
our method across different model scales. Further
analyses show the universality and robustness, and
provide a unified view of why previous demonstra-
tion selection methods work. Our contributions
are summarized as follows:
• To the best of our knowledge, we are the first
to examine the effect of both data and models
on the demonstration selection studies. We
substantiate that the choice of demonstrations
is not only dependent on the test data but also
on the retrieval module and inference model.
• We propose a data- and model-dependent
method TopK+ConE to effectively enhance
the models’ understanding of test input via
reducing the conditional entropy of test input
under the inference model.
• We achieve state-of-the-art performance on
a series of tasks, and prove the effectiveness
and universality of our method. Hopefully,
our proposed best practice can be employed
by more LLM participants.
D1
D2
D3
D4
80
82
84
86
88
90
92
Accuracy
d1
d2
d3
d4
Figure 2: The performance of different combinations
of subsets Di and demonstrations dj for SST-2 dataset
in 1-shot setting. di is the demonstrations selected by
TopK for subsets Di.
2
What Factors Influence the Choice of
Demonstrations?
While in-context learning (Brown et al., 2020a;
Dong et al., 2022) has shown its impressive few-
shot performance, recent work has found that
LLMs are very sensitive to the selected examples
leading to large variances in performance (Zhao
et al., 2021). Although many advanced in-context
learning strategies (Luo et al., 2023; Liu et al.,
2022; Wu et al., 2023) have been proposed to se-
lect effective demonstrations, why these demonstra-
tions work and what factors influence their selec-
tion have not been fully studied. To determine the
influencing factors, we conducted research from
both test data and model aspects, leveraging the
framework of retrieval-based in-context learning.
It should be noted that, unless otherwise stated,
all experiments in this section are conducted using
GPT2-XL (1.5B) (Radford et al., 2019).
2.1
Impact of Test Data
To explore the effect of test data, we divided
the binary classification dataset SST-2 (Socher
et al., 2013), denoted as D, into four equal subsets
D1, D2, D3, D4, and applied the TopK (Liu et al.,
2022) method to choose four corresponding groups
of demonstrations, namely d1, d2, d3, d4. We tra-
verse the performance of these demonstrations on
all subsets, the results are shown in Figure 2. We
can notice that the same demonstrations can pro-
duce varied results with different test data, for ex-
ample, d1 is the best demonstration group for test
set D1, while it is the worst for the other three
test sets. These results show that the choice of
demonstrations is data-dependent.
SST-2
Subj
72
74
76
78
80
82
84
86
88
Accuracy
all-MiniLM-L6-v2
all-MiniLM-L12-v2
all-distilroberta-v1
all-mpnet-base-v2
Figure 3: The 1-shot performance with different re-
trieval models on two classification datasets.
2.2
Impact of Models
While prior research has examined factors influ-
encing the selection of demonstrations, it has pre-
dominantly concentrated on testing data (Min et al.,
2022; Liu et al., 2022; Su et al., 2023), ignoring the
impact of models. To determine the influence of
models, we proceed from both the retrieval model
and inference model perspectives.
Impact of Retrieval Models.
We first conduct
experiments on classification tasks with differ-
ent retrieval models.
Specifically, we conduct
experiments on two classification tasks, SST-2
and Subj (Wang et al., 2018), with four common-
used sentence-transformer (Reimers and Gurevych,
2019) models, including all-MiniLM-L6-v2, all-
MiniLM-L12-v2, all-distilroberta-v1 and all-mpnet-
base-v2. As shown in Figure 3, the performance
varies with different retrieval models and different
datasets have different best retrievers. We spec-
ulate that the variance in model performance pri-
marily arises from distinctions in similarity judg-
ment between the retrieval model and the inference
model. A smaller disparity in similarity judgment
is expected to result in better in-domain demonstra-
tions, which can improve the in-context learning
performance (Moore and Lewis, 2010; Sia and Duh,
2023).
Impact of Inference Models.
The inference
model is another factor that may influence the per-
formance of in-context learning. To explore this,
we conducted experiments on two classification
tasks (e.g., SST-2 and SST-5) employing different
inference models in both 1-shot and 3-shot settings.
Specifically, we randomly sample different demon-
strations for each test sample and assess their per-
formance across various inference models. Results
on figure 4 show that the best demonstration varies
across different inference models. For example, the
performance of Random-2 is better than Random-
3 in 1-shot SST-2 setting under llama2-7b model,
while the situation is totally reversed with llama2-
13b. We can also notice the same phenomenon
under 3-shot settings, which implies increasing the
in-context examples can not eliminate the influ-
ence of inference models. These results above
show that the choice of demonstrations is model-
dependent.
3
Method
Based on the aforementioned findings, we propose
a simple and effective data- and model-dependent
demonstration selection method, named TopK +
ConE. Our method is based on the conjecture that
good demonstrations are the demonstrations that
can help the inference model better understand the
test input. In other words, these demonstrations
excel in reducing the conditional entropy of the test
input under the inference model. Mathematically,
we find the best demonstrations c∗ by solving the
following optimization problem:
c∗ = arg min
c∈C
Hθ(x|c),
(1)
where each c represents one possible demonstration
group, and Hθ(x|c) signifies the inference model’s
uncertainty regarding the test input x given the
demonstrations c, which can indicate the degree
of the understanding of test input by the inference
model. The lower the Hθ(x|c) is, the better the
understanding is. The equation (1) can be reformu-
lated as
c∗ = arg min
c∈C
(Hθ(x, c) − Hθ(c)),
(2)
where Hθ(x, c) and Hθ(c) are the cross entropy of
the whole prompt (including the demonstrations
and test input) and the demonstrations, respectively.
In other words, we are searching for demonstra-
tions that minimize the difference of the cross-
entropy between the prompt and the demonstra-
tions.
In the practical implementations, considering the
huge search space generated by a large number of
combinations, enumerating all combinations is in-
feasible. We adopt the selection-rerank framework
proposed in (Wu et al., 2023). Specifically, we first
use the selection module to select the candidate
demonstrations and then use our method to rank
the candidates to get effective demonstrations.
Model1
Model2
Model3
70
72
74
76
78
80
82
84
86
88
Accuracy
(a) 1-shot SST-2
Model1
Model2
Model3
80
82
84
86
88
90
Accuracy
(b) 3-shot SST-2
Model1
Model2
Model3
28
30
32
34
36
Accuracy
(c) 1-shot SST-5
Model1
Model2
Model3
36
38
40
42
44
Accuracy
(d) 3-shot SST-5
Random-1
Random-2
Random-3
Figure 4: The performance of different inference models with three randomly sampled demonstrations for
SST-2 and SST-5 datasets. Model1, Model2, Model3 represent GPT-J-6B, LLAMA2-7B, and LLAMA2-13B,
respectively. The impact of various demonstrations varies depending on the specific inference models.
4
Experimental Setup
We provide a brief introduction of the experimen-
tal settings, which includes the used models, data,
baselines, evaluation metrics, and experimental de-
tails.
Models.
We perform experiments across dif-
ferent sizes of models,
including GPT2-XL
(1.5B) (Radford et al., 2019), GPT-j-6b (6B) (Wang
and Komatsuzaki, 2021), Llama2-7b (7B) and
Llama2-13b (13B) (Touvron et al., 2023), which
are decoder-only dense LMs. We also conduct
experiments on extensive alignment models, e.g.,
Llama2-7b-chat and Llama2-13b-chat (Touvron
et al., 2023), Vicuna-7b, Vicuna-13b and Deepseek-
7b-chat (DeepSeek-AI, 2024) to verify the general-
izability of our approach.
Datasets.
We conduct a systematic study across
7 natural language understanding (NLU) tasks,
including binary, multi-class classification tasks
(SST-2,
SST-5
(Socher
et
al.,
2013),
CR,
Subj (Wang et al., 2018)) and natural language
inference tasks: MNLI (Williams et al., 2018) and
QNLI (Wang et al., 2018). We also evaluate our
method in 4 machine translation tasks, extracted
from Flores-200 (Goyal et al., 2022) dataset, which
contains 1012 examples for each language.
Baselines.
We mainly compare our method with
five widely used methods.
• Prompting is a special case of ICL without
in-context examples.
• Random baseline randomly select in context
examples for each testing sample.
• BM25 (Luo et al., 2023) baseline uses BM25
to calculate the word-overlap similarity be-
tween samples and test input, and select the
high similarity samples as demonstrations.
• TopK (Liu et al., 2022) baseline uses the near-
est neighbors of a given test sample as the
corresponding in-context examples.
• TopK + MDL (Wu et al., 2023) adopt a select-
then-rank framework, and rank the demonstra-
tions selected by TopK based on the Minimum
Description Length (MDL) principle.
Evaluation Metrics.
We adopt different eval-
uation methods for different tasks.
As for the
classification tasks, we report the performance
with the Accuracy metric. While for the gener-
ation tasks, we use neural network-based metrics
COMET (Rei et al., 2020) since they have demon-
strated a high correlation with human evaluation
and are resilient to domain shift. Furthermore, we
use the LLM-based metric, i.e., LLM-as-a-Judge,
to quantify the translation quality, which has been
widely used recently (Zhou et al., 2023; Chiang
et al., 2023). Specifically, we utilize the OpenAI
ChatGPT (gpt-3.5-turbo-1106) to perform the judg-
ment automatically. Additionally, we also report
BLEU scores (Papineni et al., 2002) using Sacre-
BLEU (Post, 2018) for completeness, but notably,
we mainly analyze the performance in terms of
model-based metrics.
Experimental Details.
We use the TopK method
to retrieve 30 candidates for each sample and then
rank each candidate using our ConE method. Tem-
plates are adopted from (Lu et al., 2022; Wu et al.,
2023) and detailed in Table 5. We ran all experi-
ments three times with different random seeds and
reported the average accuracies. We use 4-shot ICL
for GPT2-XL and 8-shot for others, the ablation
experiments are in §7.
5
Main Results
5.1
Natural Language Understanding Tasks
We first verify the effectiveness of our method in
NLU Tasks. Specifically, we conduct experiments
on 7 classification tasks, including binary classifi-
cation tasks, multi-class classification tasks, and
natural language inference tasks. Based on the
results on Table 1 and Figure 5, we can find that:
Our method brings consistent performance im-
provements on almost all types of tasks.
Re-
sults in Table 1 show the superior performance
of our approach compared to the existing state-
of-the-art method, TopK+MDL, across the major-
ity of tasks, resulting in an average accuracy im-
provement of 1.2%. Compared with our selection
method TopK, our method considerably improved
the performance on 6 tasks out of the total 7 tasks,
yielding an average gain of 1.8%, proving the effec-
tiveness of improving the model’s understanding
to test input. Furthermore, it is noteworthy that our
approach can achieve significant improvements in
challenging tasks, such as the Subj and QNLI tasks,
respectively bringing 4.6% and 1.9% gains com-
pared to the previously optimal methods, demon-
strating the superior performance of our method
for hard-to-understanding tasks.
Our method brings gains across different model
sizes.
Figure 5 presents the average performance
across 7 Natural Language Understanding (NLU)
tasks using various inference models, ranging in
size from 1.5B (GPT2-XL) to 13B (Llama2-13B).
Results reveal that advanced in-context learning
methods usually can achieve better performance
when we scale up the model size while prompt-
ing and random in-context learning methods will
produce unstable results. Notably, our approach
consistently outperforms previous methods across
different model scales, particularly in the case of
GPT2-XL, which yields an average gain of 2.6%
and 3.6% compared to TopK+MDL and TopK.
5.2
Natural Language Generation Tasks
We further evaluate our method on NLG tasks, i.e.
machine translation. Recent study (Hendy et al.,
2023) reveals that LLMs have achieved compara-
ble or better performance on par with their best
1.5B
6B
7B
13B
Model Scale
55
60
65
70
75
Accuracy
prompting
random
bm25
TopK
TopK+MDL
Ours
Figure 5: The average performance of 7 NLU tasks
across different model scales. Our method consistently
outperforms previous methods across different model
scales.
supervised counterpart systems (Zan et al., 2022)
in competing WMT Chinese-English task. We con-
duct experiments in 4 language pairs, including
English-centric language pairs and non-English-
centric language pairs.
Results.
The results across different language
pairs under different settings are presented in Ta-
ble 2 and Figure 6. Obviously, our method can
consistently improve the performance of in-context
learning in terms of COMET score compared with
TopK in both English-centric and non-english-
centric language pairs. Especially in non-english-
centric language pairs, our method brings +2.0
COMET score improvement in Ro⇒De under 1-
shot setting. We attribute this to the improvement
of the model’s understanding of the test sample,
and the more difficult the sample, the greater the
benefit from our method. Furthermore, we can
notice that the benefit of our method slightly de-
creases as we increase the number of shots, we
speculate that increasing the number of demonstra-
tions can further help the model better understand
the test sample. The results evaluated by Chat-
GPT on Figure 6 also show that our method can
consistently outperform previous methods in the
advanced evaluation system, further proving the
effectiveness of our method.
6
Analysis
To further demonstrate the effectiveness and gener-
ality of our method, we conduct further analyses on
NLU tasks (with the GPT2-XL model) and NLG
tasks (with Llama2-7b).
Our method is complementary to previous ap-
proaches.
To further explore the generality of
Method
SST-2
CR
Subj
SST-5
AGNews
MNLI
QNLI
Average
Prompting
68.7
81.1
49.4
25.3
67.0
47.5
53.3
56.0 (+21.9)
Random
94.4
92.3
70.9
50.4
83.5
51.0
56.2
71.2 (+6.8)
BM25
94.5
92.8
76.8
52.6
92.5
57.0
59.0
75.0 (+2.9)
TopK
95.2
92.8
80.4
52.6
92.4
57.8
61.3
76.1 (+1.8)
TopK + MDL
95.1
93.4
81.2
52.7
92.3
57.9
64.5
76.7 (+1.2)
Ours
95.4
93.1
85.8
52.5
92.8
59.5
66.4
77.9
Table 1: Performance of different methods across 7 Natural Language Understanding(NLU) tasks on Llama2-
7b model. The best results are in bold. We can see that our method improves the performance of almost all task
types. Numbers in the parenthesis represent the relative improvements our method achieved over baselines.
Method
En⇒Zh
En⇒Ru
Ru⇒En
Ro⇒De
COMET20
BLEU
COMET20
BLEU
COMET20
BLEU
COMET20
BLEU
-w/ 1-shot
Random
35.7
27.0
32.3
13.6
60.4
31.3
44.3
19.5
BM25
35.1
26.9
31.8
13.6
60.4
31.2
40.6
18.9
TopK
35.9
27.0
33.0
13.5
60.5
31.3
41.2
18.8
Ours
37.1
27.3
34.5
13.7
60.8
31.6
43.2
19.2
-w/ 3-shot
Random
40.1
28.3
33.9
14.0
61.5
32.0
48.0
20.4
BM25
39.6
28.2
35.1
14.7
61.8
31.9
47.2
19.9
TopK
39.9
28.0
34.7
13.9
61.7
32.1
47.8
19.8
Ours
40.7
28.0
34.8
14.2
62.2
32.3
48.5
20.1
Table 2: Performance on different methods across 5 language pairs on Llama2-7b. The best results are in bold.
0
20
40
60
80
100
(a) English to Chinese
Random
BM25
TopK
49.5
49.7
47.2
7.3
8.5
11.9
43.2
41.8
40.9
Ours Win
Tie
Ours Loses
0
20
40
60
80
100
(b) English to Russian
Random
BM25
TopK
49.1
45.8
45.6
19.8
21.7
23.3
31.1
32.5
31.1
Ours Win
Tie
Ours Loses
0
20
40
60
80
100
(c) Russian to English
Random
BM25
TopK
38.1
36.8
37.8
34.0
37.0
37.4
27.9
26.3
24.8
0
20
40
60
80
100
(d) Romanian to German
Random
BM25
TopK
40.0
40.2
39.6
20.7
21.2
23.2
39.3
38.5
37.2
Figure 6: Comparative winning of baselines v.s. Ours (TopK + ConE) in different translation tasks in 1-shot
settings. We can see that our method consistently outperforms previous methods among all translation datasets.
our method, we combine our method with differ-
ent selection methods, e.g. random and BM25, in
binary and multi-choice classification tasks. The
results in Figure 8 (a, b) show that our method can
further significantly improve the baseline perfor-
mance in different types of tasks, which indicates
that our method is complementary to previous ap-
proaches. We can also notice that TopK + Ours
achieves better performance compared with other
methods, hence we choose TopK as our selection
method.
Method
zhen
enzh
ecommerce
news
social
ecommerce
news
social
-w/ 1-shot
random
3.7
29.8
31.2
33.0
17.8
6.8
TopK
6.4
30.6
32.4
32.6
18.1
6.1
Ours
6.0
33.2
32.4
36.1
21.0
4.8
-w/ 3-shot
random
8.1
33.3
33.4
34.3
22.4
11.3
TopK
7.5
35.3
33.3
36.7
24.1
11.9
Ours
9.5
36.3
34.4
37.0
25.2
12.5
Table 3: Performance of our method for domain
dataset with a mixed-domain demonstration pool
with inference model Llama2-7b.
Llama2-7b-chat
Vicuna-7b
Llama2-13b-chat
Vicuna-13b
Deepseek-7b-chat
77
78
79
80
81
Accuracy
TopK
TopK + Ours
Figure 7: The average performance of different
aligned models in 7 NLU tasks.
Our method works for mix-domain demonstra-
tion pools.
Previous results have shown the supe-
rior performance of our method in single-domain
demonstration pools. Now, we evaluate the effec-
tiveness of our method in mixed demonstration
pools. Specifically, we evaluate the performance
of our method in three domains, e.g., e-commerce,
news and social, with a mix-domain demonstra-
tions pool in WMT22 translation task1. Experi-
mental results on Table 3 show that our method can
achieve consistent improvements on three domains
with 3-shot in-context learning, showing that our
method also works for mix-domain demonstration
pools..
Our method works for aligned models.
To ver-
ify the effectiveness of our method for the aligned
LLMs, we conducted extensive experiments on dif-
ferent instruction-tuned and RLHF-tuned LLMs,
including Vicuna, LLaMA-chat, and DeepSeek-
chat. The results in Figure 7 show that our method
can achieve consistent improvement in different
models, demonstrating that our method also works
for instruction-tuned and safety-enhanced models.
1https://www.statmt.org/wmt22/
translation-task.html
7
Impact of hyperparameter
In this section, we conduct ablation studies on the
hyperparameters in our method.
Impact of In-context Examples.
We gradually
increase the number of in-context examples (de-
noted as N) from 0 (prompting) to 16. The results
are listed in Figure 9(a, b), we see that increas-
ing N usually can consistently improve the perfor-
mance on average, but when N=16 the ICL per-
formance in GPT2-XL degrades. Through further
analysis, we found that the decrease comes from
the constraint of the maximum sentence length of
the model (GPT2-XL), and the phenomenon even
occurs when we set N as 8 for GPT2-XL. Hence,
we choose N=4 for GPT2-XL, and N=8 for other
models. Note that our method can consistently out-
perform the TopK method, and increasing the in-
context examples can further improve our method.
Impact of Candidate Numbers.
As mentioned
above, our method comprises two modules: TopK
selection module and ConE reranking module.
The selection module will reduce the space of
in-context examples to speed up the whole pro-
cess. Hence we explore the impact of the can-
didate numbers selected by TopK. The results in
Figure 9(c) list the performance of 4 in-context ex-
amples with the GPT2-XL model. We can notice
that our method is always better than the baseline
TopK, and increasing the number of candidates can
further improve the performance. Based on the
results, we set the default candidate number as 30.
8
Discussion
Whether our method can partially explain why
previous ICL methods work?
Intuitively, en-
hancing the model’s understanding to test input is
one of the reasons why previous methods work.
To prove this, we calculate the conditional entropy
of the test input with respect to previous baselines
across three classification tasks. The results pre-
sented in Figure 8(c) show that the previous meth-
ods will reduce the conditional entropy of test sam-
ples in all three tasks, demonstrating that previous
in-context learning methods can also be explained
by our conjecture.
Whether our method is sensitivity to the demon-
stration order?
Previous studies have proven
that ICL is very sensitive to the order of in-context
examples (Lu et al., 2022). To explore the sensi-
Random
BM25
TopK
SST-2
65.0
67.5
70.0
72.5
75.0
77.5
80.0
82.5
85.0
Accuracy
(a)
Origin
Ours
Random
BM25
TopK
SST-5
25.0
27.5
30.0
32.5
35.0
37.5
40.0
42.5
45.0
Accuracy
(b)
Origin
Ours
SST2
SST5
Subj
Tasks
2.4
2.6
2.8
3.0
3.2
3.4
3.6
Conditional Entropy
(c)
Random
BM25
TopK
TopK+MDL
TopK+Ours
Figure 8: (a, b) The effect of our method with different selection methods in SST-2 and SST-5, origin represents
the baseline method without our method, while Ours signifies with our method. (c) The conditional entropy of
the test input with different in-context learning methods.
0 1 2
4
8
16
Number of In-context Examples
50
55
60
65
70
Accuracy
(a)
Random
TopK
Ours + TopK
0 1 2
4
8
16
Number of In-context Examples
55
60
65
70
75
Accuracy
(b)
Random
TopK
Ours + TopK
8
10
20
30
50
Number of Candidates
65.0
65.5
66.0
66.5
67.0
67.5
68.0
68.5
69.0
Accuracy
(c)
TopK
TopK+Ours
Figure 9: The average performance of ablation experiments. (a, b) Impact of the number of in-context examples
for GPT2-XL and Llama2-7b; (c) Impact of the number of candidates selected by the TopK method.
Method
SST-2
Subj
CR
Avg.
Var.
Avg.
Var.
Avg.
Var.
Random
68.8
0.90
56.7
0.49
67.8
4.00
TopK
78.6
0.56
86.2
0.20
73.9
0.61
Ours
82.0
0.26
91.0
0.05
81.0
0.26
Table 4: The average performance and variance of
10 random permutations of four in-context examples
for GPT2-XL. The best results are in bold. Our method
achieves consistently better average performance with
lower variance.
tivity of our methods for the order of in-context
examples, we randomize the order of our chosen
demonstrations on three classification tasks and
compare the stability with Random and TopK meth-
ods. Results on Table 4 show that our method can
achieve better average performance with smaller
variance among all tasks, demonstrating that our
method could alleviate the order sensitivity issue
in ICL.
9
Related Work
Despite large language models have shown their
surprising zero-shot performance in various tasks,
recent works show that ICL can effectively elicit
their capability and further improve their perfor-
mance (Dong et al., 2022). However, the perfor-
mance of ICL is unstable (Lu et al., 2022), and
the small change of in-context examples and their
order can result in a large variance in performance.
Motivated by the instability of the ICL perfor-
mance, in-context example selection methods have
been widely investigated.
Lu et al. (2022) first
propose a validation-free corpus-level method for
determining the optimal order of in-context exam-
ples. However, they only investigate the influence
of order without proposing how to better select
in-context examples. Inspired by the success of
retrieval modules in augmenting neural networks,
Liu et al. (2022) find examples that are close to
each test sample in embedding space can serve as a
good choice for in-context learning. Following the
finding of Liu et al. (2022), Su et al. (2023) sub-
sequently extended their method by incorporating
increased diversity in the selection of in-context
examples. However, why these methods work is
still unclear and the methods only consider the in-
fluence of data.
Unlike the data-dependent demonstration selec-
tion methods, model-dependent methods are rarely
explored. Wu et al. (2023) proposed a demonstra-
tion rank method grounded in the minimum descrip-
tion length principle, which utilizes the inference
model to select the optimal in-context example or-
ganization. However, their ranked in-context orga-
nizations are randomly sampled, which may limit
their performance. Furthermore, they neglected to
investigate how the inference model affects ICL
performance.
On the other hand, although some previous meth-
ods (Wu et al., 2023; Iter et al., 2023; Wang et al.,
2023) have emphasized the significance of under-
standing the test samples, their primary empha-
sis lies in the confidence of test labels, neglecting
that of test input. For instance, Wu et al. (2023)
searches the demonstrations capable of losslessly
compressing testing labels, and Iter et al. (2023)
identify the in-domain demonstrations through the
cross-entropy difference of test labels computed
by the small model fine-tuned in demonstrations.
While Wang et al. (2023) propose to reweight label
anchors to improve ICL performance. Gonen et al.
(2023) found that using perplexity could be a good
heuristic for prompt selection, while the effect for
in-context learning has not been investigated.
10
Conclusion
In this paper, we take the first step to investigate the
factors that influence the choice of demonstrations
in in-context learning from both data and model
perspectives, and find that the demonstration selec-
tion is both data- and model-dependent. Based on
the findings, we conjecture that effective demon-
strations can improve the inference model’s under-
standing to test input, and correspondingly propose
a data- and model-dependent selection method.
Empirical results suggest that our method can sig-
nificantly outperform the previous ICL method.
Further analysis confirms the generalization of our
method and our approach can provide a unified
explanation for previous studies.
References
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020a. Language models are few-shot
learners. NeurIPS.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020b. Language models are few-shot learners. In
NeurIPS.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
et al. 2023. Vicuna: An open-source chatbot im-
pressing gpt-4 with 90%* chatgpt quality.
See
https://vicuna.lmsys.org (accessed 14 April 2023).
DeepSeek-AI. 2024.
Deepseek llm: Scaling open-
source language models with longtermism. arXiv
preprint arXiv:2401.02954.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-
ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and
Zhifang Sui. 2022. A survey for in-context learning.
arXiv preprint.
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,
Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy
Liang, and Tatsunori B Hashimoto. 2023. Alpaca-
farm: A simulation framework for methods that learn
from human feedback. arXiv preprint.
Hila Gonen, Srini Iyer, Terra Blevins, Noah Smith, and
Luke Zettlemoyer. 2023. Demystifying prompts in
language models via perplexity estimation. In Find-
ings of EMNLP.
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-
Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-
ishnan, Marc’Aurelio Ranzato, Francisco Guzmán,
and Angela Fan. 2022. The Flores-101 evaluation
benchmark for low-resource and multilingual ma-
chine translation. TACL.
Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and
Percy S Liang. 2018. A retrieve-and-edit framework
for predicting structured outputs. NeurIPS.
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf,
Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita,
Young Jin Kim, Mohamed Afify, and Hany Hassan
Awadalla. 2023. How good are gpt models at ma-
chine translation? a comprehensive evaluation. arXiv
preprint.
Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang,
Yang Liu, Yichong Xu, and Chenguang Zhu. 2023.
In-context demonstration selection with cross entropy
difference. In EMNLP.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B
Dolan, Lawrence Carin, and Weizhu Chen. 2022.
What makes good in-context examples for gpt-3? In
DeeLIO.
Qingyu Lu, Baopu Qiu, Liang Ding, Kanjian Zhang,
Tom Kocmi, and Dacheng Tao. 2023. Error analysis
prompting enables human-like translation evaluation
in large language models: A case study on chatgpt.
arXiv preprint.
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,
and Pontus Stenetorp. 2022. Fantastically ordered
prompts and where to find them: Overcoming few-
shot prompt order sensitivity. In ACL.
Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat,
Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and
Vincent Y Zhao. 2023.
Dr. icl: Demonstration-
retrieved in-context learning. arXiv preprint.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022.
Rethinking the role of demonstra-
tions: What makes in-context learning work?
In
EMNLP22.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In ACL.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. NeurIPS.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In ACL.
Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen,
Xuebo Liu, Min Zhang, Yuanxin Ouyang, and
Dacheng Tao. 2023.
Towards making the most
of chatgpt for machine translation. In Findings of
EMNLP 2023.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In WMT.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. COMET: A neural framework for MT
evaluation. In EMNLP.
Nils Reimers and Iryna Gurevych. 2019.
Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In EMNLP.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2022. Learning to retrieve prompts for in-context
learning. In NAACL.
Suzanna Sia and Kevin Duh. 2023. In-context learn-
ing as maintaining coherency: A study of on-the-fly
machine translation using large language models. In
MTSummit.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In EMNLP.
Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,
Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,
Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2023.
Selective annotation makes language models better
few-shot learners. In ICLR.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023.
Llama 2:
Open founda-
tion and fine-tuned chat models.
arXiv preprint
arXiv:2307.09288.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for
natural language understanding. In EMNLP.
Ben Wang and Aran Komatsuzaki. 2021.
GPT-J-
6B: A 6 Billion Parameter Autoregressive Lan-
guage Model. https://github.com/kingoflolz/
mesh-transformer-jax.
Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,
Fandong Meng, Jie Zhou, and Xu Sun. 2023. Label
words are anchors: An information flow perspective
for understanding in-context learning. In EMNLP.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
Liang, Jeff Dean, and William Fedus. 2022. Emer-
gent abilities of large language models. TMLR.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In NAACL.
Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-
peng Kong. 2023. Self-adaptive in-context learn-
ing: An information compression perspective for in-
context example selection and ordering. In ACL.
Changtong Zan, Keqin Peng, Liang Ding, Baopu Qiu,
Boan Liu, Shwai He, Qingyu Lu, Zheng Zhang,
Chuang Liu, Weifeng Liu, Yibing Zhan, and Dacheng
Tao. 2022. Vega-MT: The JD explore academy ma-
chine translation system for WMT22. In WMT.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In
ICML.
Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and
Dacheng Tao. 2023. Can chatgpt understand too?
a comparative study on chatgpt and fine-tuned bert.
arXiv preprint.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, et al. 2023. Lima: Less is more for alignment.
In NeurIPS.
A
Appendix
A.1
Templates
The templates used in this paper are detailed in
Table 5.
A.2
Evluation Prompts.
As mentioned in §2.1, we use the ChatGPT to eval-
uate the translation performance. Based on the
prompt in Dubois et al. (2023), we design a new
prompt to measure the winning rates of our method
against the baseline. The detailed evaluation sys-
tem prompts are shown in Table 6.
Task
Prompt
Class
SST-2
Review: ""<X>"" Sentiment: positive
positive
Review: ""<X>"" Sentiment: negative
negative
SST-5
Review: ""<X>"" Sentiment: terrible
terrible
Review: ""<X>"" Sentiment: bad
bad
Review: ""<X>"" Sentiment: okay
okay
Review: ""<X>"" Sentiment: good
good
Review: ""<X>"" Sentiment: great
great
Subj
Input: ""<X>"" Type: objective
objective
Input: ""<X>"" Type: subjective
subjective
CR
Review: ""<X>"" Sentiment: positive
positive
Review: ""<X>"" Sentiment: negative
negative
AgNews
""<X>"" It is about world.
World
""<X>"" It is about sports.
Sports
""<X>"" It is about business.
Business
""<X>"" It is about science and technology.
Sci/Tech
MNLI
<X1>? Yes, <X2>
Entailment
<X1>? Maybe, <X2>
Neutral
<X1>? No, <X2>
Contradiction
QNLI
<C> Can we know <X>? Yes.
Entailment
<C> Can we know <X>? No.
Contradiction
Table 5: Templates of tasks. Placeholders (e.g., <X> and <C>) will be replaced by real inputs.
Machine Translation Evaluation Prompt
System: In this task, we will ask you to help us compare the translation quality of two sentences
when translating from [src-lang] to [tgt-lang]. We will give you a [src-lang] source sentence and
two [tgt-lang] sentences (Sent 1 and Sent2). There are three options for you to choose from:
1. Sentence 1 is better: If you think Sentence 1 is a better translation, then choose this option.
2. Sentence 2 is better: If you think Sentence 2 is a better translation, then choose this option.
3. Sentence 1 and Sentence 2 are the same: If you think Sentence 1 and Sentence 2 have the same
translation quality, then choose this option.
Your response must only contain the chosen option.
User: Source Sentence: [SRC] Sentence 1: [Sent1]Sentence 2: [Sent2]
Table 6: The Machine Translation Evaluation Prompt we used for ChatGPT. [src-lang] and [tgt-lang] represent
the source language and target language, respectively. [SRC] signifies the source sentence, while [Sent1] and [Sent2]
represent the two translation sentences.
"
"On-device ASR models trained on speech data of a large population might underperform for individuals unseen during training. ASR personalisation aims to exploit user data to improve model robustness. We address unsupervised personalisation by developing a novel consistency based training method via pseudo-labelling. Our method achieves a relative Word Error Rate Reduction (WERR) of 17.3% on unlabelled training data and 8.1% on held-out data compared to a pre-trained model, and outperforms the current state-of-the art methods.","ASR models face voice characteristics (e.g. accent, tone) and background acoustics (e.g. noise, reverberation) unseen during training. Fine-tuning a pre-trained ASR model on device with collected user data (personalisation) improves performance, but requires labelled data. We focus on unsupervised self-training, aiming to improve robustness using only unlabelled user data. Recent work combines auxiliary speaker features, self-training methods and entropy minimisation. Our work applies Consistency Constraint (CC) to improve the robustness of unsupervised ASR personalisation.",nanData filtering is used to select samples with good quality labels. Confidence based filtering selects samples with low WER. Model adaptation learns highly compact speaker-dependent parameter representations. Entropy minimisation reduces uncertainty for samples in a target domain.,"We introduce CC to the common unsupervised self-training pipeline. Perturbations are applied to both pseudo-labelling and the training process, forcing the model to output a consistent label in the vicinity of the training sample after filtering. A data augmentation strategy is used as the data perturbation for both pseudo-labelling and training, and a dropout strategy is used as the model perturbation.","The proposed method achieves a 17.3% relative WERR compared to a pre-trained model, and outperforms the recently developed techniques in literature. The method is agnostic to the choice of data-filtering methods and robust on a broad range of English accents with pseudo-label WER ranging from 10% (high-quality) to 45% (low-quality).",We propose a novel consistency based training method for unsupervised personalisation of ASR models which outperforms current SOTA methods. The method is agnostic to the choice of data-filtering methods and is robust on a broad range of English accents with pseudo-label WER ranging from 10% (high-quality) to 45% (low-quality).,Consistency Based Unsupervised Self-training For ASR Personalisation,"Jisi Zhang, Vandana Rajan, Haaris Mehmood, David Tuckey, Pablo Peso Parada, Md Asif Jalal, Karthikeyan Saravanan, Gil Ho Lee, Jungin Lee, Seokyeong Jung","CONSISTENCY BASED UNSUPERVISED SELF-TRAINING FOR ASR PERSONALISATION
Jisi Zhang1∗, Vandana Rajan1∗, Haaris Mehmood1, David Tuckey1, Pablo Peso Parada1, Md Asif Jalal1,
Karthikeyan Saravanan1, Gil Ho Lee2, Jungin Lee2, Seokyeong Jung2
1Samsung Research UK, United Kingdom,
2AI R&D Group, Samsung Electronics, Suwon, South Korea
ABSTRACT
On-device Automatic Speech Recognition (ASR) models
trained on speech data of a large population might underper-
form for individuals unseen during training. This is due to a
domain shift between user data and the original training data,
differed by user’s speaking characteristics and environmental
acoustic conditions. ASR personalisation is a solution that
aims to exploit user data to improve model robustness. The
majority of ASR personalisation methods assume labelled
user data for supervision.
Personalisation without any la-
belled data is challenging due to limited data size and poor
quality of recorded audio samples. This work addresses un-
supervised personalisation by developing a novel consistency
based training method via pseudo-labelling.
Our method
achieves a relative Word Error Rate Reduction (WERR) of
17.3% on unlabelled training data and 8.1% on held-out data
compared to a pre-trained model, and outperforms the current
state-of-the art methods.
Index Terms— speech recognition, unsupervised, speaker
adaptation, personalisation
1. INTRODUCTION
End-to-end ASR models are known to underperform when
deployed in the wild as they face voice characteristics (e.g.
accent, tone) and background acoustics (e.g. noise, reverber-
ation) unseen during training [1–4]. A promising approach to
remedy this issue consists of fine-tuning a pre-trained ASR
model on device with collected user data [5–7]. This type of
approach is also known as “personalisation” or “adaptation”
as it tailors the ASR model to the single user of the device.
While supervised personalisation methods [5–7] sub-
stantially improve performance for end-users, they require
labelled data which is impractical in many use cases [8].
This paper focuses on unsupervised self-training, which
aims to improve the robustness of an ASR model using
only unlabelled user data. Recently, unsupervised ASR per-
sonalisation has made progress by incorporating auxiliary
speaker features into an ASR model [9], exploring self-
*Equal contribution
training methods [10, 11], and training based on entropy
minimisation [8,12].
A common pipeline for the unsupervised self-training
method contains data filtering, pseudo-labelling, and train-
ing [10, 11]. In [11], a confidence estimation module uses
ASR output probabilities to select a less erroneous subset
of utterances from the entire set of unlabelled samples. The
filtered samples are processed by the pre-trained model to
generate pseudo-labels, which are subsequently used during
training. However, the training process can be unstable with-
out accessing labelled data for supervision and the model
drifts away due to erroneous pseudo-labels [13].
Consistency Constraint (CC) forces a model to predict the
same results on the same input with various versions of per-
turbation and has been shown effective for exploring unla-
belled data [14–17]. Applying various perturbations intro-
duces randomisation to regularise a model, leading to more
stable model generalisation [14]. CC has been successfully
combined with supervised loss function for semi-supervised
learning in computer vision [15] and speech applications [16,
17]. However, it has not yet been explored for speech recog-
nition in a fully unsupervised self-training setting.
In this work, we exploit CC to improve the robustness
of training process for unsupervised ASR personalisation.
We introduce CC to the common unsupervised self-training
pipeline, however, perturbations are applied to both pseudo-
labelling and the training process, forcing the model to output
a consistent label in the vicinity of the training sample after
filtering. To the best of our knowledge, our work is the first
to apply CC in the context of a fully unsupervised setting for
ASR. We compare against a state-of-the-art (SOTA) unsu-
pervised self-training method that combines a data filtering
strategy with an adapter based training mechanism [10] and
a separate unsupervised adaptation method based on entropy
minimisation [8]. Our proposed method achieves a 17.3%
relative WERR compared to a pre-trained model, and outper-
forms the recently developed techniques in literature, leading
to a new SOTA performance. The main contributions in this
paper are summarised as follows:
• We propose a novel consistency based training method
for unsupervised personalisation of ASR models which
979-8-3503-0689-7/23/$31.00 ©2023 IEEE
arXiv:2401.12085v1  [eess.AS]  22 Jan 2024
outperforms current SOTA methods.
• We empirically show that our proposed method is ag-
nostic to the choice of data-filtering methods and thus
can be used in conjunction with any of them.
• We evaluate the robustness of our method on a broad
range of English accents with pseudo-label WER rang-
ing from 10% (high-quality) to 45% (low-quality).
The rest of the paper is organised as follows. Section 2 re-
views related work in the literature. Section 3 provides back-
ground details of an ASR model and a confidence-score based
filtering method used in the proposed framework. Section 4
describes the proposed training method based on consistency.
Section 5 presents implementation details and the experiment
setup. Results and analysis are presented in Section 6. Fi-
nally, the paper is concluded in Section 7.
2. RELATED WORK
Data filtering is a commonly used pre-processing step when
exploring unlabelled data [10, 18].
Confidence based fil-
tering methods have been developed to select samples with
good quality labels that yield lower Word Error Rate (WER)
among the whole unlabelled dataset [18–21].
For exam-
ple, in DUST [18, 21], multiple ASR hypotheses are cre-
ated using dropout, and edit distances from reference (w/o
dropout) are used to estimate the confidence of ASR model
on the generated transcript. Another class of methods use
a confidence estimation system that trains a separate neural
network using intermediate features derived from the ASR
model [10,19,20]. For example, both [20] and [10] use light-
weight binary classifier models to predict whether a given
set of ASR features correspond to an error-free pseudo label
(WER=0) or not.
Model adaptation methods focus on learning highly com-
pact speaker-dependent parameter representations [10, 22–
24]. Given a pre-trained model, a diagonal linear transfor-
mation of the input features is learned to match the distribu-
tion of test data to the training data [22]. An ASR model’s
batch normalization layer parameters can be learned from the
target speaker data for adaptation [23]. Additional speaker-
dependent parameters can be introduced to an ASR model
for adaptation [10, 24].
Specifically, learning hidden unit
contributions (LHUC) trains embedding vectors with fixed
dimensions to modify the amplitudes of hidden unit activa-
tions [24]. Recently, the LHUC method has been success-
fully applied to a Conformer based end-to-end ASR model
for speaker adaptation [10]. However, the major drawbacks
of these techniques are that the selection of adaptation layers
in the ASR model is non-trivial and adding new parameters
changes the model architecture.
Recently, entropy minimisation based approaches have
been shown effective for unsupervised ASR domain adapta-
tion [8, 12]. It aims to reduce uncertainty for samples in a
Fig. 1: Streaming two-pass end-to-end ASR model architecture. The
first pass model is a conformer based transducer. The second pass
model is an attention-based encoder-decoder model (LAS). NCM
classifier is a confidence estimation module that uses intermediate
ASR features for WER based data filtering.
target domain by minimising an entropy loss based on output
probabilities from a pre-trained model given the target sam-
ples. However, one issue of this method is that the model
tends to drift away when the initial predictions are incorrect.
3. BACKGROUND
This section provides the background details of a streaming
ASR system and the Neural Confidence Measure (NCM)
based data filtering method, which are required for our pro-
posed unsupervised personalisation pipeline.
3.1. ASR model
A state-of-the-art, two-pass Conformer-T model [25] is used
as a pre-trained ASR model for both filtering unlabelled data
and adaptation to a target speaker. As shown in Fig. 1, it
consists of two sub-models, namely, a parent model and
a second-pass model.
The parent model is a conformer
transducer [26], which consists of a transcription network,
a prediction network and a joint network.
The transcrip-
tion network contains a convolution subsampling module
followed by stacked convolution-augmented Transformer
(Conformer) blocks. The prediction network contains two
LSTM layers. The second-pass model is an LAS rescorer,
consisting of an LSTM based encoder-decoder architecture.
The LAS encoder takes as input the parent’s transcription
output, and the first-pass prediction is refined by attending to
the second-pass encoder outputs. The model is trained using
both a transducer [27] loss and a cross-entropy (CE) loss
calculated based on the output from the parent model and the
second-pass model, respectively.
3.2. Data filtering methods
Inspired by recent works that employ WER prediction mod-
els for data filtering [10, 11, 20, 28, 29], we use one such ex-
isting model that was exclusively developed for our custom
ASR model described previously. The NCM binary classi-
fication model [20] is used for filtering the transcripts gen-
erated by our two-pass ASR model (see Fig. 1). The NCM
model is made up of dense layers and self-attention mecha-
nism and takes two types of intermediate features from the
ASR model as input, namely, second pass decoder output and
beam scores. The second pass decoder outputs are logits from
the LAS second pass decoder. We use top-K logits corre-
sponding to each decoded token for this decoder. Beam scores
are the log-probability scores for each beam assigned by the
2nd pass decoder. Input features are obtained by running the
ASR model with beam search and the model is trained using
binary cross-entropy loss. The output consists of two classes,
WER=0 and WER>0 and only the predicted WER=0 sam-
ples are selected as the filtered set.
Note that different to
the original NCM model that used 6 types of features, we
use only two based on the relevance of features as shown in
the Table 1 of [20]. Once the training is complete, the saved
NCM model is used to perform pseudo-label filtering on the
on-device recorded personal data.
Additionally, we also experiment with two other tech-
niques from literature for data filtering, namely, DUST [18]
and Confidence Thresholding (CT) [30]. DUST is based on
the intuition that for confident predictions, the ASR output
would remain unchanged even if some amount of uncer-
tainty is introduced into the model in the form of dropout.
In practice, dropout layers are enabled in the ASR model
during evaluation and each utterance is forward propagated
through the network for multiple times to generate different
hypotheses. Levenshtein edit distances between a reference
(hypothesis with no dropout) and each of the hypotheses
are calculated. If any of the distances corresponding to an
utterance is above a predefined threshold, that utterance is
perceived as having a lower confidence by the ASR model
and hence rejected.
Confidence threshold for each utterance is obtained by
taking sum of log softmax scores across all tokens. WER
values are binarized by taking WER=0 as positive class and
WER>0 as negative class. The threshold is then found by
taking the geometric mean of sensitivity and specificity using
an ROC curve.
4. PROPOSED METHOD
The proposed unsupervised personalisation pipeline starts
from filtering unlabelled data to using the filtered data to
adapt a pre-trained ASR model based on the Consistency
Fig. 2: Unsupervised personalisation pipeline based on data filter-
ing and consistency constraint
Constraint (CC). The novelty of our adaptation method is that
it is the first work to apply CC in the context of fully unsuper-
vised ASR personalisation. The personalisation pipeline (see
Fig. 2) based on the proposed CC training is summarised in
Algorithm 1. We first apply data filtering DATAFILTER to the
entire unlabelled set X to obtain filtered set ˆ
X. Subsequently,
the model is trained on the filtered set, involving N rounds of
pseudo-labelling and training, and in each round the model
f is trained for M epochs with the paired audio samples and
pseudo-labels ˆD.
Algorithm 1 Proposed consistency based unsupervised per-
sonalisation pipeline
1: Input: ASR model f, weights θ, unlabelled data X
2:
ˆ
X = DATAFILTER(X), θ0 = θ
3: for i = 0, ..., N − 1 do
// N rounds
4:
ˆD = {(ˆx, f(SPECAUG(ˆx), θi)), . . .} ∀ ˆx ∈ ˆ
X
5:
θi+1 = TRAIN(f, θi, ˆD, M)
// M epochs
6: end for
7: Output: θN
The consistency is realised via pseudo-labelling on utter-
ances augmented with random data perturbations, after which
the generated pseudo-labels are used to train the model that
are perturbed in a different way. The pseudo-label for each ut-
terance is updated frequently with the latest ASR model dur-
ing the personalisation process. CC has been commonly ap-
plied as a regularisation loss to semi-supervised learning ap-
proaches [15–17,31], in which a main loss is calculated based
on the available labelled source domain data. This work uses
CC as the only loss for unsupervised personalisation instead
of using it as an auxiliary loss.
Pseudo-labels are generated by decoding the output of the
second-pass model via beam search to produce hard labels, a
transcription sequence ˆy. During the training phase, only the
parameters of the first-pass model are adapted. Specifically,
the first-pass transducer takes augmented input features ˜x and
outputs the posterior probabilities of all possible alignments,
which are used to calculate the loss against the pseudo-label
ˆy. The loss is then used to train the first-pass transducer com-
prised of transcription network, prediction network, and joint
network. Since the first-pass model is a conformer transducer
model, the loss function is implemented as incorporating the
CC within the standard RNN-T loss:
L = − ln Pr(ˆy|˜x)
(1)
SpecAugment [32,33] is used as the data perturbation for
both the pseudo-labelling and the training. Due to randomisa-
tion, the SpecAugment applied to a sample during training is
different to the pseudo-labelling for the same sample. During
training, besides the data perturbation, the model is perturbed
by a dropout strategy [34]. The combination of data as well as
model perturbation forms a stronger augmentation for train-
ing than that of pseudo-labelling.
5. EXPERIMENT SETUP
5.1. Data
The ASR model is pre-trained on 20K hours of English
speech data. This includes data from public speech datasets
such as LibriSpeech [35] as well as in-house data from a
variety of domains such as search, telephony, far-field, etc.
The performance of this model on an in-house validation set
of 6 hours (5K utterances) is 15.69% WER.
For personalisation experiments, the proposed method
is evaluated on an in-house synthetic user data for a mobile
phone use case. There are 12 speakers, each containing three
styles of speech: (i) application launch/download commands
(Apps), contact call/text commands (Contacts) and common
real-world voice assistant commands (Dictations). Table 1
provides examples for the three types of data. The average
audio length of Apps and Contacts samples is two seconds.
The filtered Apps and Contacts data are used to personalise
a pre-trained acoustic model. Then, the personalised model
is evaluated on the entire (filtered and unfiltered) Apps and
Contacts data, and the held-out Dictations data that is unseen
during the training. On average, for each speaker, the du-
ration of entire Apps, Contacts and Dictation sets are 8.37,
16.93 and 6.47 minutes respectively.
5.2. ASR model configuration
The transcription network in the two-pass ASR model con-
sists of 16 conformer blocks.
Each conformer block con-
sists of one feed-forward layer block, one Convolution Block,
one multi-head self-attention block, another one feed-forward
layer block, followed by a layer normalisation layer. The pre-
diction network is constructed by stacking two LSTM layers
with a dimension of 640. The joint network is a dense layer.
In the second-pass model, the encoder contains one LSTM
with dimension of 680, and the decoder contains two LSTMs
with dimension of 680. When decoding, the beam sizes for
the first-pass model and the second-pass LAS re-scoring are
set as 4 and 1, respectively. No language model is used for
decoding.
Table 1: Adaptation data examples: Apps, Contacts, and Dictations
Split
Examples
Apps
Open Messenger
Install Snapchat
Contacts
Send a message to Anne Hathaway
Call Emma Stone
Dictations
When does summer start
Set an alarm for seven thirty
Batch size of 16 and Adam optimizer [36] are used for all
the adaptation experiments. Learning rates for ASR model
fine-tuning and LHUC based training are set as 5e-6 and
1e-3, respectively. The SpecAugment used for both pseudo-
labelling and training has one frequency mask with size of
13 and two time masks with size of 12. During training, the
model is perturbed with dropout of 0.1.
5.3. Data filtering method setup
In the NCM, the first fully connected block consists of two
dense layers each with 64 neurons followed by Tanh activa-
tion. This is followed by a self-attention layer whose output
is summed across all tokens for each utterance and concate-
nated with the beam scores. This is then passed through an-
other fully connected block made up of two dense layers of
64 neurons each and an output dense layer for the binary class
prediction. Training the NCM model uses a batch size of 32
and Adam optimizer [36] with an initial learning rate of 1e-3.
An exponential decay scheduler with decay rate 0.5 for every
500 training steps was also used. The K value is set as 4. The
NCM model is trained using an in-house 6 hours data, where
it is split in the ratio 80:20 for training and validation.
The CT method uses the NCM training data (in-house
dataset of 6 hours) for finding the threshold, which is used
to identify correct pseudo labels from the Apps and Contacts
sets of personal data. For DUST, the dropout of 0.2 (which is
the same value used during the original ASR training) is en-
abled for the transformers in ASR model and 5 hypotheses are
generated for each utterance. We tried different thresholds for
edit distance from {0.1, 0.3, 0.5, 0.7} as recommended in [18]
and found that the best results are obtained with 0.1 threshold.
6. RESULTS AND ANALYSIS
This section presents the main results of the proposed method
and shows the comparison against recently developed exist-
ing unsupervised adaptation methods. The second part of this
section conducts a detailed ablation study on the proposed
method with respect to data filtering strategies and individual
user performance.
6.1. ASR personalisation results
Methods used as baselines in this work include noisy student
training (NST) [37] without accessing labelled source data,
confidence score based LHUC [10], and a recently proposed
unsupervised test-time adaptation approach based on entropy
minimization (EM) [8]. The additional LHUC modules are
inserted into the streaming two-pass model used in this work.
Different to the strategy used in [10] that applies only one
LHUC layer to the hidden output of the convolution sub-
sampling module, we apply multiple LHUC layers to the
transcription network, which yields better results than apply-
ing a single layer. Specifically, one LHUC layer is applied
to the output of the convolution subsampling module and
another to the output of each of the Conformer block. In
total, there are 17 LHUC layers. During adaptation, only the
LHUC layers are updated and the parameters of the main
ASR model are unchanged. The NST method employs data
augmentation (SpecAugment) during training, but not for
pseudo-labelling.
For the EM approach, we calculate and
minimise the entropy of the token-wise posterior probabil-
ities output from the second-pass LAS decoder. The NCM
data filtering is applied before the EM training. Both NST
and EM also update only the first-pass model parameters to
make fair comparison with the proposed method.
Table 2 summarises the ASR performance of the base-
line and proposed methods. The proposed method achieves
17.3%, 7.2%, and 8.1% relative WERR on Apps, Contacts,
and Dictation, respectively, compared to the pre-trained
model.
It outperforms both entropy minimisation and the
confidence score based LHUC, achieving a new SOTA result.
The results show that the proposed method not only improves
the ASR performance on unlabelled data used for training
(Apps & Contacts), but also generalises well to unseen data
(Dictation) spoken by the target speaker. The second half
of Table 2 shows the performance of the proposed method
(NCM+CC) by adaptation with and without LHUC and the
results indicate that addition of LHUC does not provide any
improvement.
NCM+CC outperforms the combination of data filtering
Table 2: Word Error Rate (WER) of the proposed method and exist-
ing methods for unsupervised personalisation
Methods
Apps
Contacts
Dictation
Pre-trained
22.66
23.49
9.43
NST
21.94
23.07
9.36
EM [8]
20.26
23.23
9.53
NCM+EM
19.12
22.22
8.86
NCM+LHUC [10]
20.30
22.70
9.10
NCM+CC+LHUC
19.30
21.99
8.64
NCM+CC
18.73
21.79
8.67
and entropy minimization (NCM+EM). EM aims to reduce
the output uncertainty when a model processes a test sam-
ple. When the model prediction is incorrect, the EM approach
trains the model to be more confident about its incorrect deci-
sion. For the consistency based approach, a model may output
different predictions for a highly uncertain sample with vari-
ous perturbations. Since the model is trained to map this same
sample to different predictions, this sample will have lesser
impact on the model weight change compared to samples that
provide consistent pseudo-labels.
6.2. Ablation study
We first investigate the effect of quality of audio samples on
the ASR personalisation performance. The Consistency Con-
straint (CC) based method is tested either on the unfiltered
whole data set or the filtered data set based on three filter-
ing strategies, namely Confidence Thresholding (CT), DUST,
and the NCM. The method is also tested on training with
only audio samples that the ASR system recognises correctly
(WER=0), selected based on the ground-truth transcriptions.
In Table 3, we first observe that the CC trained with audio
samples that the model recognises correctly (WER=0) per-
forms better compared with the CC trained with the whole
data (CC). This demonstrates the importance of adapting a
given model on samples that yield low WER. The second half
of Table 3 shows the results of CC with different filtering
strategies. Remarkably, though trained on filtered data that
contains samples with erroneous labels, the CC based train-
ing outperforms that trained with WER=0 samples. It sug-
gests that the CC based training is able to explore samples
with erroneous labels to adapt the model.
All three data filtering strategies achieve similar perfor-
mance improvement. Out of the three, NCM is favored for
on-device personalisation. NCM is a lightweight model that
requires only one-time training and can be easily deployed on
device. The CT method requires manual tuning of the thresh-
old value, which is challenging for an end-to-end ASR model
because of the well known overconfidence issue that exists in
the end-to-end ASR models [29]. However, the NCM can ex-
ploit large amounts of training data on server to automatically
learn the boundary between reliable and unreliable samples
Table 3: The effect of three data filtering methods (CT, DUST, NCM)
on the unsupervised personalisation performance
Methods
Apps
Contacts
Dictation
CC
21.25
22.71
9.04
CC (WER=0)
20.38
22.40
8.87
CT+CC
18.91
22.10
8.75
DUST+CC
18.93
21.87
8.69
NCM+CC
18.73
21.79
8.67
5
10
15
20
0
5
10
15
20
Rounds
WERR (%)
Apps
Epochs = 1
Epochs = 3
Epochs = 5
NST
CC
5
10
15
20
0
2
4
6
8
Rounds
WERR (%)
Contacts
5
10
15
20
0
2
4
6
8
10
12
Rounds
WERR (%)
Dictation
Fig. 3: Word Error Rate Reduction (WERR) compared to the pre-trained model for Apps, Contacts & Dictation using consistency training
(CC) and unsupervised NST for 20 rounds with a choice of 1, 3 or 5 epochs per round. Higher values are better. Plot values are smoothed
using an exponential moving average with weight of 0.6. Best viewed in colour.
0
1
2
3
4
5
6
7
8
9
10
11
0
20
40
WER (%)
Pre-trained
Unfilter+NST
NCM+Consistency
0
1
2
3
4
5
6
7
8
9
10
11
0
20
40
WER (%)
0
1
2
3
4
5
6
7
8
9
10
11
Speaker ID
0
10
20
WER (%)
Fig. 4: ASR personalisation results for each of the 12 individual
users. The pre-trained model, NST trained on unfiltered data, and
the proposed method are compared in the plot. (Top: Apps data,
Middle: Contacts data, Bottom: Dictation data.)
by exploiting multiple intermediate ASR features. Compared
to DUST that requires multiple forward passes, which is un-
desirable due to time and resource constraints, the NCM ob-
tains the result with a single forward pass.
We perform ablation experiments to study the effect of
increasing the number of rounds and epochs per round on
the overall WERR. We use epochs ∈ {1, 3, 5} for up to 20
rounds and compare our method against unsupervised NST.
Fig. 3 shows that training with five epochs per round can
lead to divergence for Dictation which is a classic example
of overfitting due to increased model updates. Conversely,
training for a single epoch per round leads to sub-optimal
convergence due to the increased stochasticity in regenerat-
ing pseudo-labels with input augmentation every round. Our
method performs up to 40% better than unsupervised NST
which is more susceptible to overfitting due to being easily
stuck in a local minima.
We investigate the performance of our proposed method
on each individual user, and the analysis is shown in Fig. 4.
In comparison to the pre-trained model and the baseline
NST trained on unfiltered data, our method achieves better
or equivalent recognition accuracy for most users on both
held-in data (Apps & Contacts) and held-out data (Dictation).
There is a wide range of speech recognition accuracy among
the test users, whose WERs range from 10% to 45%. The
results demonstrate that the proposed method improves the
robustness of the training process to erroneous labels.
7. CONCLUSIONS
This work introduces a novel unsupervised personalisation
training method to address a domain shift issue when an ASR
model is deployed in the wild. The proposed method per-
forms data filtering of unlabelled user data and applies a con-
sistency constraint to the training process. A neural confi-
dence measure approach has been employed for data filtering
and has demonstrated to effectively discard unreliable audio
samples that yield erroneous pseudo-labels. To apply consis-
tency constraint, data perturbation is introduced to the itera-
tive pseudo-labelling process, forcing the ASR model to pre-
dict the same labels on the same sample with various versions
of perturbations. Experiments show that the proposed method
reduces the negative impact of low quality samples during
training and improves model generalisation to test domain
data. We further separately evaluate the consistency based
training in combination with three existing filtering methods
and all filtering methods achieve similar results. This suggests
that the consistency based training can be used in conjunction
with a wide range of data filtering strategies.
8. REFERENCES
[1] Yong Zhao, Jinyu Li, Shixiong Zhang, Liping Chen, and
Yifan Gong, “Domain and speaker adaptation for cor-
tana speech recognition,” in ICASSP, 2018.
[2] Zhouyuan Huo, Dongseong Hwang, Khe Chai Sim,
Shefali Garg, Ananya Misra, Nikhil Siddhartha, Trevor
Strohman, and Franc¸oise Beaufays, “Incremental layer-
wise self-supervised learning for efficient unsupervised
speech domain adaptation on device,” in Interspeech,
2022.
[3] Khe Chai Sim, Petr Zadrazil, and Franc¸oise Beaufays,
“An investigation into on-device personalization of end-
to-end automatic speech recognition models,” in Inter-
speech, 2019.
[4] Peter Bell, Joachim Fainberg, Ondrej Klejch, Jinyu Li,
Steve Renals, and Pawel Swietojanski, “Adaptation al-
gorithms for neural network-based speech recognition:
An overview,” IEEE Open Journal of Signal Process-
ing, vol. 2, pp. 33–66, 2020.
[5] Khe Chai Sim, Petr Zadraˇzil, and Franc¸oise Beaufays,
“An investigation into on-device personalization of end-
to-end automatic speech recognition models,” in Inter-
speech, 2019.
[6] Zhong Meng, Jinyu Li, Yashesh Gaur, and Yifan Gong,
“Domain adaptation via teacher-student learning for
end-to-end speech recognition,”
in 2019 IEEE Auto-
matic Speech Recognition and Understanding Workshop
(ASRU), 2019, pp. 268–275.
[7] Yan Huang, Guoli Ye, Jinyu Li, and Yifan Gong, “Rapid
speaker adaptation for conformer transducer: Attention
and bias are all you need,” in Interspeech, 2021.
[8] Guan-Ting Lin, Shang-Wen Li, and Hung yi Lee, “Lis-
ten, adapt, better WER: Source-free single-utterance
test-time adaptation for automatic speech recognition,”
in Interspeech, 2022.
[9] Marc Delcroix, Shinji Watanabe, Atsunori Ogawa,
Shigeki Karita, and Tomohiro Nakatani, “Auxiliary fea-
ture based adaptation of end-to-end ASR systems,” in
Interspeech, 2018.
[10] Jiajun Deng, Xurong Xie, Tianzi Wang, Mingyu Cui,
Boyang Xue, Zengrui Jin, Mengzhe Geng, Guinan Li,
Xunying Liu, and Helen M. Meng, “Confidence score
based conformer speaker adaptation for speech recogni-
tion,” in Interspeech, 2022.
[11] Jiajun Deng, Xurong Xie, Tianzi Wang, Mingyu Cui,
Boyang Xue, Zengrui Jin, Guinan Li, Shujie Hu,
and Xunying Liu,
“Confidence score based speaker
adaptation of conformer speech recognition systems,”
IEEE/ACM Transactions on Audio, Speech, and Lan-
guage Processing, vol. 31, pp. 1175–1190, 2023.
[12] Changhun Kim, Joonhyung Park, Hajin Shim, and
Eunho Yang,
“SGEM: Test-time adaptation for auto-
matic speech recognition via sequential-level general-
ized entropy minimization,” in Interspeech, 2023.
[13] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong,
and Si Wu, “Model adaptation: Unsupervised domain
adaptation without source data,” 2020 IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition
(CVPR), 2020.
[14] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen,
“Regularization with stochastic transformations and per-
turbations for deep semi-supervised learning,” in NIPS,
2016.
[15] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,
Alexey Kurakin, and Chun-Liang Li, “Fixmatch: Sim-
plifying semi-supervised learning with consistency and
confidence,” Advances in neural information processing
systems, vol. 33, pp. 596–608, 2020.
[16] Felix Weninger, Franco Mana, Roberto Gemello, Jes’us
Andr’es-Ferrer, and Puming Zhan,
“Semi-supervised
learning with data augmentation for end-to-end ASR,”
in Interspeech, 2020.
[17] Ashtosh Sapru, “Using data augmentation and consis-
tency regularization to improve semi-supervised speech
recognition,” in Interspeech, 2022.
[18] Sameer Khurana, Niko Moritz, Takaaki Hori, and
Jonathan Le Roux,
“Unsupervised domain adapta-
tion for speech recognition via uncertainty driven self-
training,” ICASSP, 2020.
[19] Kaustubh Kalgaonkar, Chaojun Liu, Yifan Gong, and
Kaisheng Yao, “Estimating confidence scores on ASR
results using recurrent neural networks,”
in ICASSP,
2015.
[20] Ashutosh Gupta, Ankur Kumar, Dhananjaya N. Gowda,
Kwangyoun Kim, Sachin Singh, Shatrughan Singh, and
Chanwoo Kim, “Neural utterance confidence measure
for RNN-Transducers and two pass models,” ICASSP,
2021.
[21] Nauman Dawalatabad, Sameer Khurana, Antoine Lau-
rent, and James Glass, “On unsupervised uncertainty-
driven speech pseudo-label filtering and model calibra-
tion,” in ICASSP, 2023.
[22] Zhong-Qiu Wang and Deliang Wang,
“A joint train-
ing framework for robust automatic speech recognition,”
IEEE/ACM Transactions on Audio, Speech, and Lan-
guage Processing, vol. 24, pp. 796–806, 2016.
[23] Zhongqiu Wang and Deliang Wang,
“Unsupervised
speaker adaptation of batch normalized acoustic models
for robust ASR,” ICASSP, 2017.
[24] Pawel Swietojanski, Jinyu Li, and Steve Renals, “Learn-
ing hidden unit contributions for unsupervised acoustic
model adaptation,” IEEE/ACM Transactions on Audio,
Speech, and Language Processing, vol. 24, no. 8, pp.
1450–1463, 2016.
[25] Jinhwan Park, Sichen Jin, Junmo Park, Sungsoo Kim,
Dhairya Sandhyana, Chang heon Lee, Myoungji Han,
Jungin Lee, Seokyeong Jung, Chang Woo Han, and
Chanwoo Kim, “Conformer-based on-device streaming
speech recognition with KD compression and two-pass
architecture,” 2022 IEEE Spoken Language Technology
Workshop (SLT), pp. 92–99, 2023.
[26] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki
Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,
Zhengdong Zhang, Yonghui Wu, et al.,
“Conformer:
Convolution-augmented transformer for speech recog-
nition,” in Interspeech, 2020.
[27] Alex Graves,
“Sequence transduction with recurrent
neural networks,” ArXiv, vol. abs/1211.3711, 2012.
[28] Ankur Kumar, Sachin Singh, Dhananjaya Gowda, Ab-
hinav Garg, Shatrughan Singh, and Chanwoo Kim,
“Utterance confidence measure for end-to-end speech
recognition with applications to distributed speech
recognition scenarios.,” in Interspeech, 2020.
[29] Qiujia Li, David Qiu, Yu Zhang, Bo Li, Yanzhang
He, Philip C. Woodland, Liangliang Cao, and Trevor
Strohman, “Confidence estimation for attention-based
sequence-to-sequence models for speech recognition,”
ICASSP, 2020.
[30] Jacob Kahn, Ann Lee, and Awni Y. Hannun,
“Self-
training for end-to-end speech recognition,”
ICASSP,
2019.
[31] Marvin Zhang, Sergey Levine, and Chelsea Finn,
“MEMO: test time robustness via adaptation and aug-
mentation,” in NeurIPS, 2022.
[32] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng
Chiu, Barret Zoph, Ekin Dogus Cubuk, and Quoc V. Le,
“Specaugment: A simple data augmentation method for
automatic speech recognition,” in Interspeech, 2019.
[33] Daniel S. Park,
Yu Zhang,
Chung-Cheng Chiu,
Youzheng Chen, Bo Li, William Chan, Quoc V. Le, and
Yonghui Wu, “Specaugment on large scale datasets,”
ICASSP, 2019.
[34] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov, “Improving
neural networks by preventing co-adaptation of feature
detectors,” ArXiv, vol. abs/1207.0580, 2012.
[35] Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-
jeev Khudanpur, “Librispeech: an ASR corpus based on
public domain audio books,” in ICASSP, 2015.
[36] Diederik P. Kingma and Jimmy Ba, “Adam: A method
for stochastic optimization,” International Conference
for Learning Repreentations (ICLR), 2015.
[37] Daniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-
Cheng Chiu, Bo Li, Yonghui Wu, and Quoc V. Le,
“Improved noisy student training for automatic speech
recognition,” in Interspeech, 2020.
"
"Mirroring social cues is crucial in human and robot interactions. Yet, limited studies have compared robot imitation across platforms and control methods. We conducted two experiments analyzing people's views of affective mirroring between иCub and Pepper robots, plus movement mirroring between vision- and IMU-based иCub control. The иCub robot was viewed as more humanlike during affect mirroring, and vision-based control outperformed IMU-based control in the movement mirroring task. These insights can inform the design and use of humanoid robots in real-world interactions.","The mirror neuron system (MNS), recognizing others' movements by simulating their behavior, facilitates social interaction. Mirroring is the subconscious imitation of another's nonverbal cues, reflecting social integration and cue utilization. Mirroring dysfunction links to social communication issues in Autism Spectrum Disorders (ASD). Mirroring is significant in human-robot interaction, enhancing trust, awareness, and perceived human-like characteristics. Affective mirroring, where robots convey emotions, makes them seem capable of subjective states and empathetic. Previous studies show consistent findings, but comparisons across different robots in affective mirroring are limited.
Different robots convey emotions in various ways. For instance, the иCub robot displays LED-based facial expressions, while the Pepper robot changes shoulder and eyelid colors to express emotions, eliciting distinct interpretations. Movement mirroring improves robot sociability. Two techniques for enabling robots to mirror human movements are IMU-based and vision-based controlled mirroring. IMU-based mirroring uses IMU readings from head-mounted eye trackers, while vision-based mirroring employs external cameras and pose estimation algorithms. Although researchers have studied these two methods, their comparison on head and gaze mirroring is lacking.","Recent studies on robotic mirroring focus on improving accuracy, timeliness, and the subjective evaluation of robotic platforms and control methods, but have limited investigations.
Gonsior et al. found that adaptive robot behavior, where the robot mirrored human expressions, increased empathy and improved perceived task performance compared to non-adaptive modes. Previous research shows consistent findings, but few studies have compared people's perceptions of affective mirroring on different humanoid robots. Robots convey emotional signals in various ways, for example, the иCub robot uses LED lights and the Pepper robot uses color changes.
Movement mirroring enhances robots' sociability in human-robot interactions, making them more humanlike, empathetic, and socially intelligent. Two primary methods for enabling robots to mirror human movements are IMU-based controlled and vision-based controlled imitations. IMU-based controlled mirroring uses readings from an IMU attached to a head-mounted eye tracker to directly translate human head movements into robotic actions. In contrast, vision-based controlled mirroring uses external cameras and pose estimation algorithms to interpret a human's head movements and mirror them through a robot.
Liu et al. showed that a lightweight model surpasses state-of-the-art models on the same robot doing head movement mirroring. Geminiani et al. found that the Microsoft Kinect-based controlled NAO robot outperforms the IMU-based controlled NAO robot for limb movement mirroring in autism treatment. However, comparing different control methods of robots on head and gaze mirroring is yet to be explored.nan","We conducted two experiments with two humanoids, the иCub and Pepper robots, using a 3-item questionnaire to rate interaction with the robots (precision, delay, and a 5-point Likert scale for perceived socially intelligent, mechanical, responsive, and humanlike). Technical details are available as part of the Wrapyfi tutorial series.

In the affective mirroring experiment, participants made eight facial expressions in front of the Pepper or иCub robots, mirrored through affective signaling or robotic facial expressions. Afterwards, participants matched the displayed colors on the Pepper robot and facial expressions on the иCub robot to emotion categories.

In the gaze and head movement mirroring experiment, participants interacted with the иCub robot under vision-based and IMU-based controlled conditions. They assessed the robot's mirroring precision and delay, and rated their impression on four dimensions - socially intelligent, mechanical, responsive, and humanlike.

Participants were 30 individuals (7 female, 22 male) between 24 and 41 years of age (mean 28.7), with no history of neurological conditions. One participant's data was excluded due to self-reported color blindness, and another due to technical issues. The study followed ethical guidelines and obtained informed consent.","For affective mirroring, accuracy in recognizing emotions varied between robots. Participants were most accurate in recognizing anger (86.2%) and least accurate in recognizing fear (3.4%) on the Pepper robot. For the иCub robot, they were most accurate in recognizing happiness (100%) and least accurate in recognizing disgust (16.7%).

Regarding the ratings of interaction, there was no significant difference in precision or delay between the Pepper and иCub robots. However, the иCub robot was rated as significantly more humanlike than the Pepper robot. No significant differences were found for the other dimensions (socially intelligent, mechanical, and responsive).

For movement mirroring, the ratings showed that participants perceived the vision-based controlled robot as significantly more precise and less delayed than the IMU-based controlled robot. Vision-based mirroring was also perceived as more responsive than IMU-based mirroring. However, both methods were perceived as equally humanlike.","We investigated how different robot platforms and control methods impact people's perception of human-robot interaction in affective and movement mirroring tasks. Our findings suggest that the iCub robot with facial expressions is perceived as more humanlike than the Pepper robot with affective signaling. Additionally, the vision-based controlled method performs better than IMU-based controlled methods, attributed to latency in processing and transmitting filtered IMU readings. These findings highlight the importance of considering robotic platforms and control methods for mirroring tasks in HRI, thereby guiding future humanoid robot design decisions and aligning them with human needs.",Human Impression of Humanoid Robots Mirroring Social Cues,"Di Fu, Fares Abawi, Philipp Allgeuer, Stefan Wermter","Di Fu, Fares Abawi, Philipp Allgeuer, and Stefan Wermter. 2024. Human Impression of Humanoid Robots Mirroring Social Cues. In Companion of the 2024 ACM/IEEE International
Conference on Human-Robot Interaction (HRI ’24 Companion), March 11-14, 2024, Boulder, CO, USA. https://doi.org/10.1145/3610978.3640580.
Human Impression of Humanoid Robots Mirroring Social Cues
Di Fu∗
di.fu@uni-hamburg.de
University of Hamburg
Hamburg, Germany
Fares Abawi∗
fares.abawi@uni-hamburg.de
University of Hamburg
Hamburg, Germany
Philipp Allgeuer
philipp.allgeuer@uni-hamburg.de
University of Hamburg
Hamburg, Germany
Stefan Wermter
stefan.wermter@uni-hamburg.de
University of Hamburg
Hamburg, Germany
A
B
D
C
Figure 1: A participant performing the four mirroring tasks in random order: A) The iCub robot mirroring facial expressions;
B) The Pepper robot affectively signaling through LED color changes; C) The iCub robot mirroring head movement based on
an inertial measurement unit (IMU) readings. The red circle shows the IMU; D) The iCub robot mirroring head movement
according to a vision-based model. The red circle shows the camera.
ABSTRACT
Mirroring non-verbal social cues such as affect or movement can
enhance human-human and human-robot interactions in the real
world. The robotic platforms and control methods also impact peo-
ple’s perception of human-robot interaction. However, limited stud-
ies have compared robot imitation across different platforms and
control methods. Our research addresses this gap by conducting
two experiments comparing people’s perception of affective mirror-
ing between the iCub and Pepper robots and movement mirroring
between vision-based iCub control and Inertial Measurement Unit
(IMU)-based iCub control. We discovered that the iCub robot was
perceived as more humanlike than the Pepper robot when mirroring
affect. A vision-based controlled iCub outperformed the IMU-based
∗Both authors contributed equally to this research.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
HRI ’24 Companion, March 11–14, 2024, Boulder, CO, USA
© 2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0323-2/24/03
https://doi.org/10.1145/3610978.3640580
controlled one in the movement mirroring task. Our findings sug-
gest that different robotic platforms impact people’s perception of
robots’ mirroring during HRI. The control method also contributes
to the robot’s mirroring performance. Our work sheds light on the
design and application of different humanoid robots in the real
world.
CCS CONCEPTS
• Human-centered computing → User studies; Interaction de-
sign theory, concepts and paradigms.
KEYWORDS
affective mirroring, movement mirroring, gaze and head movement,
human-robot interaction
ACM Reference Format:
Di Fu, Fares Abawi, Philipp Allgeuer, and Stefan Wermter. 2024. Human
Impression of Humanoid Robots Mirroring Social Cues. In Companion of
the 2024 ACM/IEEE International Conference on Human-Robot Interaction
(HRI ’24 Companion), March 11–14, 2024, Boulder, CO, USA. ACM, New York,
NY, USA, 5 pages. https://doi.org/10.1145/3610978.3640580
arXiv:2401.12076v1  [cs.RO]  22 Jan 2024
HRI ’24 Companion, March 11–14, 2024, Boulder, CO, USA
Di Fu, Fares Abawi, Philipp Allgeuer, and Stefan Wermter
1
INTRODUCTION
The mirror neuron system (MNS) in humans facilitates the un-
derstanding of others by simulating their behaviors via sensori-
motor processes [5]. Mirroring, a fundamental element of social
interaction, involves subconsciously imitating another individual’s
nonverbal cues, such as gestures, expressions, and postures [10].
It can reflect an adaptive integration and utilization of social cues
within the social context [22]. This mechanism often leads indi-
viduals to collaborate with those who exhibit similar and familiar
behaviors [7]. Mirror system dysfunction contributes to difficulties
in social communication for individuals with Autism Spectrum
Disorders (ASD) [18]. Mirroring also plays a significant role in
human-robot social interaction. By mimicking non-verbal social
cues, humans feel socially closer to the robot and perceive it as
more aware of the intentions behind their social behaviors [14].
For robots, affective mirroring causes people to perceive the
robot as an agent capable of conveying internal states, displaying
social intelligence, and expressing humanlike characteristics [3, 6].
Gonsior et al. [11] investigated the impact of mirroring facial ex-
pressions on empathy and perceived subjective performance in
interactions with the robot head EDDIE [21], revealing that adap-
tive modes of robot behavior, where the robot mirrored human
expressions, led to increased levels of human empathy and im-
proved perceived task performance compared to a non-adaptive
mode—without facial expression mimicry. Although most previous
research shows consistent findings, few studies compare people’s
perceptions of affective mirroring on different humanoid robots.
Robots convey emotional signals in various ways. For instance, the
iCub robot can display simplified facial expressions with LED light
pattern changes, and the Pepper robot can change the color of the
shoulder and eyelids to represent emotions. It may cause people to
interpret them differently for the same expression.
Movement mirroring enhances robots’ sociability during human-
robot interactions, making them more humanlike, empathetic, and
socially intelligent [4]. Two primary methods of enabling robots
to mirror human movements include IMU-based controlled and
vision-based controlled imitations. IMU-based controlled mirroring
uses readings from an IMU attached to a head-mounted eye tracker
worn by an actor to directly translate their head movements into
robotic actions [9]. In contrast, vision-based controlled mirroring
uses external cameras and pose estimation algorithms to interpret
an actor’s head movements and mirror them through a robot [8].
Liu et al. [17] show that the lightweight model surpasses the other
state-of-the-art models on the same robot doing the head movement
mirroring. Geminiani et al. [9] find that the Microsoft Kinect-based
controlled NAO robot outperforms the IMU-based controlled NAO
robot regarding limb movement mirroring in the autism treatment.
However, comparing different control methods of robots on doing
head and gaze mirroring remains to be studied.
Social robots are designed to aid people, but individuals have
been adapting to the robots instead. This is due to the fact that
robots are not always designed with human preferences and interac-
tive needs [15, 19]. Researchers in robotic mirroring are constantly
improving humanoid robots’ accuracy and timeliness in simulat-
ing social cues. However, research about subjective evaluation and
preference of the robotic platform and control method is limited.
In this study, we conducted two experiments with two humanoids,
the iCub and Pepper robots, as shown in Figure 1. The first exper-
iment compared people’s perceptions of affective mirroring on
different humanoid robots. The second experiment assessed the
impact of various control methods on the same robot platform do-
ing movement mirroring. We evaluated the robots’ performance
by their mirroring speed and accuracy. People’s perception of the
robots was measured from four dimensions—Socially Intelligent,
Mechanical, Responsive, and Humanlike. Through these investiga-
tions, our goal is to enhance the alignment of robotic design with
human interaction preferences. We aim to solve these issues by
investigating the following research questions (RQ):
RQ1 How do different robotics platforms, specifically the iCub
and Pepper robots, compare in affective mirroring?
RQ2 How do various robotic control methods, especially vision-
based controlled and IMU-based controlled methods, impact
the iCub robot’s performance in movement mirroring tasks?
2
STUDY DESIGN
2.1
Affective Mirroring Task
In this experiment, participants were asked to make eight facial
expressions—Anger, Fear, Happiness, Disgust, Sadness, Neutral, Sur-
prise, and Contempt—in front of the Pepper or iCub robots. The
expressions were to be performed within one minute in any order.
The robot mirrored participants’ expressions either through affec-
tive signaling—by changing the Pepper robot’s eye and shoulder
LED colors [13, 16]—or robotic facial expressions—by changing the
iCub robot’s eyebrow and mouth LED patterns [2]. Next, partici-
pants were asked to match the colors displayed on the Pepper robot
(depicted in the top row of Figure 2) and facial expressions on the
iCub robot (depicted in the bottom row of Figure 2) to emotion cat-
egories. Technical details for running the experiment are provided
as part of the Wrapyfi [1] tutorial series1.
Upon completion of the task, participants were asked to scan a
QR code appearing on the Pepper’s tablet using their cell phones to
complete a three-item questionnaire, evaluating their experiences
with either robot. In both questionnaires, participants were asked
to rate their interaction with the robots using a 5-point Likert scale:
Q1 How precise was the robot in mirroring your facial expres-
sions? (1 = very imprecise, 5 = very precise)
Q2 Did the robot mirror your expressions with major delay? (1 =
no significant delay, 5 = significant delay)
Participants rated their impression of the robots on four dimensions
—Socially Intelligent, Mechanical, Responsive, and Humanlike—using
a 5-point Likert scale (1 = not at all, 5 = yes, a lot).
2.2
Gaze and Head Movement Mirroring Task
In this experiment, participants interacted with the iCub robot
given two conditions. Under the vision-based controlled condition,
the iCub robot’s movements were actuated by a vision-based head
pose estimation model. Under the inertial measurement unit (IMU)
controlled condition, the orientation readings arrived instead from
an IMU attached to a wearable eye tracker. Participants wore the
eye tracker and were asked to look at the iCub robot, freely moving
1https://wrapyfi.readthedocs.io/en/latest/tutorials/Multiple%20Robots.html
Human Impression of Humanoid Robots Mirroring Social Cues
HRI ’24 Companion, March 11–14, 2024, Boulder, CO, USA
their eyes and head. Participants observed the movements of the
iCub robot to evaluate the interaction. Technical details for running
the experiment are provided as part of the Wrapyfi [1] tutorial
series2.
Participants were asked to rate their interaction with the iCub
robot using a 5-point Likert scale:
Q1 How precise was the robot in mirroring your head movements?
(1 = very imprecise, 5 = very precise)
Q2 Did the robot mirror your head movements with major delay?
(1 = no significant delay, 5 = significant delay)
Q3 Did the robot move its eyes? (Yes/No)
Q4 How precise was the robot in mirroring your eye movements?
(1 = very imprecise, 5 = very precise)
Q5 Did the robot mirror your eye movements with major delay?
(1 = no significant delay, 5 = significant delay)
Participants rated their impression of the iCub robot on four dimen-
sions—Socially Intelligent, Mechanical, Responsive, and Humanlike—
using a 5-point Likert scale (1 = not at all, 5 = yes, a lot).
2.3
Experimental Setup
The participants were seated 80 cm away from the iCub robot’s
head, adjusting its height to match their eye level. A circular marker
was placed beside the iCub robot to calibrate the Pupil Core eye
tracker. Situated in front of the iCub robot was a Logitech C920
webcam facing the participants to perform tasks requiring a fixed
view of their faces while the iCub robot moved its head and eyes.
The Pepper robot stood facing the participants at an angle of 45
degrees with a distance of 1.2 m. The Pepper robot displayed an
illustration of the ongoing task on its tablet and communicated the
instructions verbally. The interaction was one minute long per task
condition and the condition order was randomized. We used the
Wrapyfi [1] framework for managing the task order, transmitting
data between models and robots using various middleware, and
orchestrating the experimental pipeline.
2.4
Participants
30 participants (female = 7, male = 22, preferred not to say = 1)
took part in both studies. Participants were between 24 and 41
years of age, with a mean age of 28.7. All participants reported
no history of neurological conditions—seizures, epilepsy, stroke,
etc.—and had normal or corrected-to-normal vision and hearing.
One participant’s data was excluded from the Pepper robot’s affec-
tive mirroring experiment because of self-reported color blindness.
Another participant’s data was excluded from the iCub robot’s
movement mirroring experiment due to technical issues. This study
adhered to the principles expressed in the Declaration of Helsinki.
Participants signed consent forms approved by the Ethics Commit-
tee at the Department of Informatics, University of Hamburg.
3
RESULTS
We evaluated the results of both mirroring tasks, studying the
perceived impression of the robot in each separate condition, as
well as comparing the paired conditions within each respective
task. Normality tests were conducted on the participants’ answers
2https://wrapyfi.readthedocs.io/en/latest/tutorials/Multiple%20Sensors.html
to each dimension of the questionnaires. Results showed that their
responses were normally distributed. In addition, all Post hoc tests
in this study used Bonferroni correction.
3.1
Affective Mirroring
For the affective mirroring task on either robot, the recognition
accuracy is listed in Figure 2. For the Pepper robot, participants
were most accurate in recognizing anger (86.2%) and least accurate
in recognizing fear (3.4%). For the iCub robot, participants were
most accurate in recognizing happiness (100%) and least accurate
in recognizing disgust (16.7%).
For participants’ rating of interaction with the robots, results
of paired-samples 𝑡-tests displayed no significant difference in pre-
cision (Q1) between the Pepper (mean ± SE = 2.79 ± .18) and
iCub (mean ± SE = 2.90 ± .15) robots, (𝑡 (28) = .46, 𝑝 = .65). No
significant difference in delay (Q2) was found between the Pepper
(mean ± SE = 2.38 ± .18) and iCub (mean ± SE = 2.48 ± .20)
robots, (𝑡 (28) = .52, 𝑝 = .61). For participants’ rating of the impres-
sion of the robots, results of paired-samples 𝑡-tests displayed that
the iCub (mean ± SE = 2.86 ± .20) robot was rated significantly
more humanlike than the Pepper (mean ± SE = 2.10 ± .16) robot,
(𝑡 (28) = 3.45, 𝑝 < .01). No significant differences were found for
the other three dimensions—Socially Intelligent, Mechanical, and
Responsive—between the two robots (𝑝𝑠 > .05) (See Table 1).
3.2
Movement Mirroring
A paired-samples 𝑡-tests showed that participants rated the vision-
based controlled robot (mean ± SE = 3.55 ± .24) significantly more
precise (Q1) than the IMU-based controlled robot (mean ± SE =
2.90 ± .19), (𝑡 (26) = 2.19, 𝑝 < .05). The vision-based controlled
robot (mean ± SE = 2.00 ± .17) was rated significantly less delayed
(Q2) than the IMU-based controlled robot (mean ± SE = 2.66 ± .21),
(𝑡 (26) = −3.09, 𝑝 < .01). Under the vision-based controlled con-
dition, all participants observed that the robot mirrored their eye
movements, whereas two did not under the IMU-based controlled
condition (Q3). Therefore, we only analyzed data from 27 partici-
pants who reported observing eye movement under both conditions.
The paired-samples 𝑡-test showed no significant difference in the
precision rating of the eye movement between the vision-based con-
trolled robot (mean ± SE = 2.48±.19) and the IMU-based controlled
robot (mean ± SE = 2.37±.19) (𝑝 > .05) (Q4). Also, no significant dif-
ference was found in the delay rating of the eye movement between
the vision-based controlled robot (mean ± SE = 3.07 ± .23) and
the IMU-based controlled robot (mean ± SE = 3.48 ± .24) (𝑝 > .05)
(Q5). For the impression of the robot, participants reported that
the vision-based controlled iCub (mean ± SE = 3.66 ± .22) robot
was significantly more responsive than the IMU-controlled robot
(mean ± SE = 3.17 ± .21), (𝑡 (26) = 2.39, 𝑝 < .05). However, no
significant differences were found in the remaining dimensions
—Socially Intelligent, Mechanical, and Humanlike—between the two
conditions (𝑝𝑠 > .05) (results are shown in Table 1).
4
DISCUSSION
Participants associated the iCub robot’s facial expressions with emo-
tions more than the Pepper robot’s affective signaling and found
the iCub robot more humanlike. Another observation relates to
HRI ’24 Companion, March 11–14, 2024, Boulder, CO, USA
Di Fu, Fares Abawi, Philipp Allgeuer, and Stefan Wermter
Anger
Fear
Happiness
Disgust
Sadness
Neutral
Surprise
Contempt
Pepper
iCub
86.2%
3.4%
65.5%
24.1%
34.5%
37.9%
10.3%
6.9%
73.3%
46.7%
100%
16.7%
26.7%
80.0%
60.0%
20.0%
Figure 2: Eight emotion categories mimicked on the Pepper (Top) and iCub (Bottom) robots in the form of affective signaling
and robotic facial expressions, respectively. Results of the human study are reported below each image in terms of the average
accuracy in matching each affective signal or facial expression to an emotion category.
Soc. Intelligent
Mechanical
Responsive
Humanlike
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
**
Affect iCub
Affect Pepper
(a) Affective Mirroring
Soc. Intelligent
Mechanical
Responsive
Humanlike
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
*
Mov(Model) iCub
Mov(IMU) iCub
(b) Movement Mirroring
Figure 3: Participants’ impressions (5-point Likert scale) of robots under different affective and movement mirroring conditions.
∗ denotes .01 < 𝑝 < .05, and ∗∗ .001 < 𝑝 < .01
Table 1: Impression of the robots under different task condi-
tions (Mean ± SE)
Affect
Affect
Mov.(Model)
Mov. (IMU)
iCub
Pepper
iCub
iCub
Soc. Intelligent
2.81 ± .22
2.89 ± .21
2.65 ± .20
2.46 ± .22
Mechanical
3.08 ± .24
2.93 ± .21
3.65 ± .24
3.85 ± .18
Responsive
3.31 ± .21
3.19 ± .16
3.65 ± .15
3.23 ± .22
Humanlike
2.81 ± .22
2.15 ± .16
2.46 ± .19
2.39 ± .21
the accuracy of recognizing different affective signals conveyed by
either robot. Participants could accurately associate Anger with the
color red and Happiness with green on the Pepper robot. This is
complemented by findings associating exposure to different colors
with physiological and psychological responses [20, 23]. Partici-
pants more accurately identified expressions of Happiness, Neutral,
and Surprise on the iCub robot compared to the Pepper robot. This
can be attributed to humans primarily relying on observing the
mouth and eyebrows to recognize these facial expressions [12],
features that the Pepper robot lacks.
We compared two movement mirroring methods. The vision-
based controlled method produced smoother, more precise, and
more responsive movements than the IMU-based controlled method.
The IMU-based controlled method transfers the IMU readings at
a faster rate, but this causes jittery movements due to hardware
limitations. These findings are also consistent with Geminiai et
al. [9] that the IMU-based NAO robot is more intrusive and requires
longer setup time than the Kinect-based NAO robot during the limb
movement mirroring. However, in our study, both methods were
perceived as equally humanlike, implying that less responsiveness
does not contradict humanlikeness.
Several limitations could be addressed and investigated in future
research. We could not compare movement mirroring on the two
humanoid robots. This is because the Pepper robot is not able to
roll its head or move its eyes, unlike the iCub robot. Our iCub
robot doesn’t have a full body, hence, we cannot study the limb
mirroring between the two robots. Future studies could address
the interaction effect between affective and movement mirroring.
Moreover, researchers could investigate how different humanoid
robots and control methods impact children with ASD, and whether
it affects their social functions [24].
5
CONCLUSIONS
We investigated human perceptions of two humanoid robots in the
affective and movement mirroring tasks. Our findings revealed that
a robot displaying facial expressions like an iCub robot was per-
ceived as more humanlike than a robot conveying affective signals
like a Pepper robot. For gaze and head mirroring, a vision-based
controlled robot performed better than an IMU-based controlled
robot. This could be attributed to latency in processing and trans-
mitting the filtered IMU readings. In summary, we showed that
robotic platforms and robot control methods played an essential
role in mirroring tasks during HRI. It may guide future humanoid
robot design decisions to align with humans’ needs.
ACKNOWLEDGMENTS
The authors gratefully acknowledge partial support from the Ger-
man Research Foundation DFG under project CML (TRR 169).
Human Impression of Humanoid Robots Mirroring Social Cues
HRI ’24 Companion, March 11–14, 2024, Boulder, CO, USA
REFERENCES
[1] Fares Abawi, Philipp Allgeuer, Di Fu, and Stefan Wermter. 2024. Wrapyfi: A
Python Wrapper for Integrating Robots, Sensors, and Applications across Multiple
Middleware. In ACM/IEEE International Conference on Human-Robot Interaction
(HRI). ACM. https://doi.org/10.1145/3610977.3637471
[2] Motonobu Aoki, Karthikeyan Kalyanasundaram Balasubramanian, Diego Torazza,
Francesco Rea, Doreen Jirak, Giulio Sandini, Takura Yanagi, Atsushi Takamatsu,
Stephane Bouet, and Tomohiro Yamamura. 2022. A Novel Wire-driven 3D Eye-
brow Design for Communication with Humanoid Robot iCub. In 2022 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). IEEE, 8248–
8254.
[3] Cynthia Breazeal, Kerstin Dautenhahn, and Takayuki Kanda. 2016. Social robotics.
Springer Handbook of Robotics (2016), 1935–1972. https://doi.org/10.1007/978-3-
319-32552-1_72
[4] Nicoleta Bugnariu, Carolyn Young, Katelyn Rockenbach, Rita M Patterson, Car-
olyn Garver, Isura Ranatunga, Monica Beltran, Nahum Torres-Arenas, and
Dan Popa. 2013.
Human-robot interaction as a tool to evaluate and quan-
tify motor imitation behavior in children with Autism Spectrum Disorders.
In 2013 International Conference on Virtual Rehabilitation (ICVR). IEEE, 57–62.
https://doi.org/10.1109/ICVR.2013.6662088
[5] Evan W Carr and Piotr Winkielman. 2014. When mirroring is both simple
and “smart”: how mimicry can be embodied, adaptive, and non-representational.
Frontiers in Human Neuroscience 8 (2014), 505. https://doi.org/10.3389/fnhum.
2014.00505
[6] Luisa Damiano, Paul Dumouchel, and Hagen Lehmann. 2015. Towards human–
robot affective co-evolution overcoming oppositions in constructing emotions
and empathy. International Journal of Social Robotics 7 (2015), 7–18.
https:
//doi.org/10.1007/s12369-014-0258-7
[7] Hinke M Endedijk, M Meyer, H Bekkering, AHN Cillessen, and Sabine Hunnius.
2017. Neural mirroring and social interaction: Motor system involvement during
action observation relates to early peer cooperation. Developmental Cognitive
Neuroscience 24 (2017), 33–41. https://doi.org/10.1016/j.dcn.2017.01.001
[8] Marco Ferro, Antonio Paolillo, Andrea Cherubini, and Marilena Vendittelli. 2019.
Vision-Based Navigation of Omnidirectional Mobile Robots. IEEE Robotics and
Automation Letters 4, 3 (2019), 2691–2698.
https://doi.org/10.1109/LRA.2019.
2913077
[9] Alice Geminiani, Laura Santos, Claudia Casellato, Andrea Farabbi, Nicola Farella,
José Santos-Victor, Ivana Olivieri, and Alessandra Pedrocchi. 2019. Design and
validation of two embodied mirroring setups for interactive games with autis-
tic children using the NAO humanoid robot. In 2019 41st Annual International
Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). IEEE,
1641–1644. https://doi.org/10.1109/EMBC.2019.8857576
[10] György Gergely. 2018. The social construction of the subjective self: The role of
affect-mirroring, markedness, and ostensive communication in self-development.
In Developmental science and psychoanalysis. Routledge, 45–88. https://doi.org/
10.4324/9780429473654-4
[11] Barbara Gonsior, Stefan Sosnowski, Christoph Mayer, Jürgen Blume, Bernd Radig,
Dirk Wollherr, and Kolja Kühnlenz. 2011. Improving aspects of empathy and
subjective performance for HRI through mirroring facial expressions. In IEEE
International Workshop on Robot and Human Interactive Communication (RO-
MAN). IEEE, 350–356. https://doi.org/10.1109/ROMAN.2011.6005294
[12] Maria Guarnera, Zira Hichy, Maura Cascio, Stefano Carrubba, and Stefania L
Buccheri. 2017. Facial expressions and the ability to recognize emotions from the
eyes or mouth: a comparison between children and adults. The Journal of Genetic
Psychology 178, 6 (2017), 309–318. https://doi.org/10.1080/00221325.2017.1361377
[13] David O Johnson, Raymond H Cuijpers, and David van der Pol. 2013. Imitating
human emotions with artificial facial expressions. International Journal of Social
Robotics 5 (2013), 503–513.
[14] Jamy Li, Wendy Ju, and Cliff Nass. 2015. Observer Perception of Dominance and
Mirroring Behavior in Human-Robot Relationships. In ACM/IEEE International
Conference on Human-Robot Interaction (HRI). ACM, 133–140. https://doi.org/10.
1145/2696454.2696459
[15] Velvetina Lim, Maki Rooksby, and Emily S Cross. 2021. Social Robots on a
Global Stage: Establishing a Role for Culture During Human-Robot Interaction.
International Journal of Social Robotics 13, 6 (2021), 1307–1333. https://doi.org/
10.1007/s12369-020-00710-4
[16] Pei-Chun Lin, Patrick CK Hung, Ying Jiang, Carolina Padilla Velasco, and
Marco Antonio Martínez Cano. 2023. An experimental design for facial and
color emotion expression of a social robot. The Journal of Supercomputing 79, 2
(2023), 1980–2009.
[17] Xiaofeng Liu, Yizhou Chen, Jie Li, and Angelo Cangelosi. 2022. Real-Time Robotic
Mirrored Behavior of Facial Expressions and Head Motions Based on Lightweight
Networks. IEEE Internet of Things Journal 10, 2 (2022), 1401–1413. https://doi.
org/10.1109/JIOT.2022.3205123
[18] Jellina Prinsen and Kaat Alaerts. 2022. Broken or socially mistuned mirroring in
ASD? An investigation via transcranial magnetic stimulation. Autism Research
15, 6 (2022), 1056–1067. https://doi.org/10.1002/aur.2720
[19] Selma Šabanović. 2010. Robots in Society, Society in Robots: Mutual Shaping of
Society and Technology as a Framework for Social Robot Design. International
Journal of Social Robotics 2, 4 (2010), 439–450. https://doi.org/10.1007/s12369-
010-0066-7
[20] Sichao Song and Seiji Yamada. 2017. Expressing Emotions through Color, Sound,
and Vibration with an Appearance-Constrained Social Robot. In ACM/IEEE In-
ternational Conference on Human-Robot Interaction (HRI). ACM, 2–11.
https:
//doi.org/10.1145/2909824.3020239
[21] Stefan Sosnowski, Ansgar Bittermann, Kolja Kühnlenz, and Martin Buss. 2006.
Design and Evaluation of Emotion-Display EDDIE. In IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS). IEEE, 3113–3118.
https:
//doi.org/10.1109/IROS.2006.282330
[22] Lyn M Van Swol. 2003. The Effects of Nonverbal Mirroring on Perceived Per-
suasiveness, Agreement with an Imitator, and Reciprocity in a Group Discus-
sion. Communication Research 30, 4 (2003), 461–480. https://doi.org/10.1177/
0093650203253318
[23] Lisa Wilms and Daniel Oberfeld. 2018. Color and emotion: effects of hue, sat-
uration, and brightness. Psychological Research 82, 5 (2018), 896–914.
https:
//doi.org/10.1007/s00426-017-0880-8
[24] Zhi Zheng, Eric M Young, Amy R Swanson, Amy S Weitlauf, Zachary E Warren,
and Nilanjan Sarkar. 2015. Robot-Mediated Imitation Skill Training for Children
With Autism. IEEE Transactions on Neural Systems and Rehabilitation Engineering
24, 6 (2015), 682–691. https://doi.org/10.1109/TNSRE.2015.2475724
"
"Due to the scarcity of annotated data in underrepresented languages, learning a structured representation of a sentence through structure learning becomes challenging. This paper aims to evaluate the efficacy of transfer learning in improving dependency parsing for Javanese, an Indonesian language with approximately 80 million speakers. The study utilizes the Universal Dependencies dataset, which consists of dependency treebanks from various languages, including Javanese. Two learning strategies are proposed: transfer learning (TL) and hierarchical transfer learning (HTL). While TL uses a single source language for pre-training the model, HTL employs both a source and an intermediate language in the learning process. The results demonstrate that HTL with Italian and English as the source and intermediate languages, respectively, achieves a 10% increase in both Unlabeled Attachment Score and Labeled Attachment Score compared to the baseline model, outperforming the TL method.","While structure learning achieves remarkable performance in high-resource languages, the situation differs for under-represented languages due to the scarcity of annotated data. This study focuses on assessing the efficacy of transfer learning in enhancing dependency parsing for Javanese—a language spoken by 80 million individuals but characterized by limited representation in natural language processing. We utilized the Universal Dependencies dataset consisting of dependency treebanks from more than 100 languages, including Javanese.","nanRecent work, Alfina et al. (2023) created a public gold standard dataset for Javanese with 1000 sentences, published as part of the Universal Dependencies dataset (Zeman et al., 2023). This dataset covers annotation for tokenization, POS tagging, morphological features tagging, and dependency parsing tasks. The most recent parser performance (Alfina et al., 2023) using this dataset is not satisfactory, with only 77.08% on Unlabeled Attachment Score (UAS) and 71.21% on Labeled Attachment Score (LAS). The lack of training data is a typical low-resource problem considered one of the biggest NLP research problems (Ruder, 2023).","This work uses an encoder-decoder architecture of Ahmad et al. (2019). No parameter modifications were made to maintain the success of the previous work. Because training and fine-tuning the model involves resources from several different languages, only language-independent labels are used where the subtype of the label is not involved.","The results show that the our best model uses the HTL method, which improves performance with an increase of 10 % for both UAS and LAS evaluations compared to the baseline model.","This work investigates whether cross-lingual transfer learning works for dependency parsing tasks of a low-resource language, Javanese. The result shows that the cross-lingual transfer learning model is significantly better than the baseline model. Models with transfer learning can improve performance on UAS and LAS metrics by up to 10%. The best model was obtained from the hierarchical transfer learning method using Italian and English as the source and Indonesian as the intermediary languages. Meanwhile, the standard transfer learning method achieved the best accuracy using Indonesian as the source language. However, the differences between standard transfer learning and hierarchical learning are insignificant, considering the margin of error from each scenario.",Cross-lingual Transfer Learning for Javanese Dependency Parsing,"Fadli Aulawi Al Ghiffari, Ika Alfina, Kurniawati Azizah","Cross-lingual Transfer Learning for Javanese Dependency Parsing
Fadli Aulawi Al Ghiffari, Ika Alfina, and Kurniawati Azizah
Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia
fadli.aulawi@ui.ac.id
{ika.alfina, kurniawati.azizah}@cs.ui.ac.id
Abstract
While structure learning achieves remarkable
performance in high-resource languages, the
situation differs for under-represented lan-
guages due to the scarcity of annotated data.
This study focuses on assessing the efficacy
of transfer learning in enhancing dependency
parsing for Javanese—a language spoken by 80
million individuals but characterized by limited
representation in natural language processing.
We utilized the Universal Dependencies dataset
consisting of dependency treebanks from more
than 100 languages, including Javanese. We
propose two learning strategies to train the
model: transfer learning (TL) and hierarchi-
cal transfer learning (HTL). While TL only
uses a source language to pre-train the model,
the HTL method uses a source language and
an intermediate language in the learning pro-
cess. The results show that our best model uses
the HTL method, which improves performance
with an increase of 10 % for both UAS and LAS
evaluations compared to the baseline model.
1
Introduction
Despite over 80 million native speakers of Javanese
(Simons et al., 2023), this language is underrepre-
sented in NLP due to a scarcity of annotated re-
sources. Limited works in Javanese have focused
on stemmer (Soyusiawaty et al., 2020), POS tag-
ger (Askhabi et al., 2020), sentiment analysis (Tho
et al., 2021), and machine translation (Lesatari
et al., 2021). However, few have explored language
structure prediction, such as dependency parsing.
Dependency parsing is a process that makes a struc-
tural representation of a sentence (Kübler et al.,
2009) that produces a structure in the form of a
dependency tree represented in a graph consisting
of several connected links between words in a sen-
tence.
Recent work, Alfina et al. (2023) created a pub-
lic gold standard dataset for Javanese with 1000
sentences, published as part of the Universal Depen-
dencies dataset (Zeman et al., 2023). This dataset
covers annotation for tokenization, POS tagging,
morphological features tagging, and dependency
parsing tasks. The most recent parser performance
(Alfina et al., 2023) using this dataset is not satisfac-
tory, with only 77.08% on Unlabeled Attachment
Score (UAS) and 71.21% on Labeled Attachment
Score (LAS). The lack of training data is a typ-
ical low-resource problem considered one of the
biggest NLP research problems (Ruder, 2023).
Transfer learning (TL) involves leveraging a
model’s knowledge from a high-resource source
domain to improve performance on various NLP
tasks, particularly in low-resource domains (Weiss
et al., 2016), by transferring learned information to
target tasks. Inspired by Maulana et al. (2022) that
utilizes cross-lingual transfer learning to develop
an Indonesian dependency parser, we want to try
to replicate its outcome in Javanese with a limited
available dataset. Moreover, we also implement hi-
erarchical transfer learning (HTL) with two stages
of transfer learning that offer increased flexibility
over TL by enabling knowledge transfer between
languages with a significant gap (Luo et al., 2019),
as demonstrated in diverse applications, including
Javanese text-to-speech (Azizah et al., 2020) and
biomedical named entity recognition models (Chai
et al., 2022).
We build the dependency parser model for Ja-
vanese by adopting model (Ahmad et al., 2019) that
uses a self-attention encoder and a graph-based de-
coder. We utilize the Universal Dependency dataset
v1.12 (Zeman et al., 2023) that provides depen-
dency treebanks for more than 100 languages, in-
cluding Javanese. Both TL and HTL use a selection
of source languages determined by LangRank (Lin
et al., 2020). Specifically, HTL employs Indone-
sian as an intermediary language, developing from
our referenced research (Maulana et al., 2022). The
empirical results show that transfer learning im-
proves accuracy with a margin of 10% compared to
arXiv:2401.12072v1  [cs.CL]  22 Jan 2024
the baseline. We also report the word embedding
comparison that fastText performs better than the
Javanese BERT, Javanese RoBERTa, and multilin-
gual BERT. In summary, the main contributions of
this paper are as follows:
1. Provide the first study of Javanese dependency
parsing using TL and HTL strategy. We re-
port that the HTL method can significantly
improve performance compared to the train-
ing from scratch method.
2. Report the investigation of which source lan-
guage and word embedding performs best for
TL and HTL strategy.
2
Related Works
2.1
Dependency Parser
The dependency parser model can be developed
using two methods, the transition-based and graph-
based methods (Das and Sarkar, 2020).
The
transition-based method works by processing the
word order one by one in a given sentence (Martin.,
2020). Meanwhile, the graph-based method gives
a score to each edge of the word relation (Martin.,
2020), then looks for the best tree formed from the
edges with the best scores.
Apart from these two methods, there is an ap-
proach in which the parser is built using an encoder-
decoder architecture. It was first developed using
a BiLSTM encoder and a deep biaffine decoder
(Dozat and Manning, 2017). Encoder variations be-
gan to develop using Transformers or self-attention
encoders (Vaswani et al., 2017), then subsequent
studies modified it using relative positional em-
bedding (Shaw et al., 2018). The first Javanese
dependency parser (Alfina et al., 2023) uses UD-
Pipe (Straka, 2018), which also utilizes the biaffine
attention mentioned before.
In the context of transfer learning, it was found
that the best combination is a self-attention encoder
and a graph-based decoder (Ahmad et al., 2019),
which will be used in this research. This combi-
nation has been better than other encoder-decoder
combinations in cross-lingual transfer learning.
2.2
Transfer Learning
Transfer learning involves leveraging a pre-trained
model’s knowledge to enhance the performance
of other models (Sarkar and Bali, 2022), address-
ing resource limitations in low-resource domains.
Besides that, hierarchical transfer learning offers
a transfer learning method in which a new layer
is added before the model is transferred to the
low-resource language (Luo et al., 2019). Recent
work has shown that transferring multiple times
could minimize the dissimilarity between the high-
resource and the low-resource domain languages
(Azizah et al., 2020).
Transfer learning strategy offers direct capability,
which means a model is trained on a source task
and then applied without any labeled data from the
target task. Specifically on the parsing task, pre-
vious research already done by Kurniawan et al.
(2021) and Ahmad et al. (2019) for developing an
unsupervised parsing model in several languages
using only English as its source language. That ap-
proach can be improved by adding fine-tuning with
the available small dataset from low-resource lan-
guage. Recent work (Maulana et al., 2022) shows
the fine-tuning approach is better than the zero-
shot one for building a parsing model in another
low-resource language, Indonesian.
3
Method
This section concerns the model’s architecture with
the addition of the transfer learning method, the
dataset and word embedding used to train the
model, and the evaluation method of how the model
is evaluated.
3.1
Model Architecture
This work uses an encoder-decoder architecture of
Ahmad et al. (2019). No parameter modifications
were made to maintain the success of the previous
work. Because training and fine-tuning the model
involves resources from several different languages,
only language-independent labels are used where
the subtype of the label is not involved.
3.1.1
Encoder
We convert the words and POS tags from the sen-
tence into their embedding form. The self-attention
encoder (Vaswani et al., 2017) in this study re-
ceived an embedding matrix, which concatenates
the word and POS embedding matrices. The en-
coder produces two matrices, M and N. M matrix
represents the probability of a word in column j
having the head of a word in row i. In comparison,
the N matrix represents the probability of a word
in column j having a label in row i.
3.1.2
Decoder
The decoder receives the two matrices and pro-
cesses them in two following processes. First, M
is processed with the maximum spanning tree algo-
rithm in the following way:
Let G = (V, E) be a graph constructed using
directed weighted graph M. In this case, a vertex
is a word representation, and an edge represents the
dependency score of the two words. Let w : E →
R be a function that assigns a weight to each edge
in E. Then, the maximum spanning tree problem
seeks to find a spanning tree T = (V, ET ) of G
such that:
T = arg max
T ′
X
e∈ET ′
w(e)
(1)
subject to the constraint that T is a tree. Then, a
list of head H is generated from all the destination
nodes in ET . It can be denoted as:
H = {di | ∃(si, di) ∈ ET }, i = 1, . . . , n
(2)
Meanwhile, N is processed to generate L, con-
taining the list of labels with the highest probability
for each word. Finally, the H and L arrays are used
to build the final resulting tree from this model.
3.1.3
Word Embedding
This research used two types of word embedding
approaches: the static type in the form of fastText
and the contextual type in the form of BERT. The
two types were selected to compare which type was
most suitable for the Javanese parser model.
We chose fastText because of the similarity with
that used in the previous research (Maulana et al.,
2022). We also used BERT with two scenarios:
using a different word embedding for each lan-
guage (BERT and RoBERTa) and only one word
embedding for all languages (multilingual BERT).
The BERT and RoBERTa scenario uses all the lan-
guages involved except Croatian due to the unavail-
able resources.
3.2
Training Method
We perform two training methods: transfer learning
and hierarchical transfer learning. Each method
generates several models based on the number of
source languages used. All models are fine-tuned
with the Javanese treebank.
Standard transfer learning only uses one transfer
stage from high-resource to low-resource language,
Figure 1: Illustration of standard transfer learning
method
Figure 2: Illustration of hierarchical transfer learning
method
as shown in Figure 1. Meanwhile, Figure 2 il-
lustrates a hierarchical transfer learning scenario,
where transferring stages are performed twice in hi-
erarchical transfer learning. The first stage is done
from a high-source language to an intermediate-
resource language, and the second stage is done
from an intermediate-resource language to a low-
resource language.
3.3
Choosing Source Languages
Some languages are selected as source languages
using the help of LangRank (Lin et al., 2020) and
references from previous studies. This tool con-
siders combining two main feature groups in each
language pair: corpus statistics and typological in-
formation.
3.4
Dataset
3.4.1
The Javanese dataset
For the Javanese dataset, we use the only Javanese
treebank available in the UD dataset v2.12, the
UD_Javanese-CSUI (Alfina et al., 2023). Table 1
shows the statistics of this dataset. The set available
for UD_Javanese-CSUI is only a test set because
the data size is still relatively small. We do our
split process by following the distribution rule of
the data into train, dev, and test sets by 80%, 10%,
and 10% percentages.
Table 1: The statistics of the Javanese treebank
Description
Statistic
Sentence count
1000
Word count
14344
Unique word count
3793
Average sentence length (in words)
14.32
Universal Part-of-Speech (UPOS) tag count
17
Universal dependency relation count
32
Language-specific dependency relation count
14
Total dependency relation count
46
Table 2: List of treebanks chosen for source languages,
with their corresponding size in the number of sentences
and words
Treebank
Sentences
Words
UD_Croatian-SET (Agic and Ljubesic, 2015)
9010
199409
UD_English-GUM (Zeldes, 2017)
9124
164396
UD_French-GSD (McDonald et al., 2013)
16341
400232
UD_Indonesian-GSD (McDonald et al., 2013)
5598
122021
UD_Italian-ISDT (Bosco et al., 2022)
14167
298343
UD_Korean-GSD (Chun et al., 2019)
6339
80322
3.4.2
The source language dataset
Langrank recommends the top 3 languages in the
following order: Indonesian, Croatian, and Ko-
rean. We also use English, one of the important
languages in NLP research. These four languages
are used in the standard transfer learning scenario.
For the hierarchical transfer learning scenario
using Indonesian as the intermediary language, we
choose English, French, and Italy as the source
languages suggested by Maulana et al. (2022). In
total, we use six languages as the source languages.
For each source language, we only use one tree-
bank. If a language has more than one treebank in
the UD dataset v2.12, we choose the treebank with
the biggest size, as shown in Table 2.
3.5
Experiments Setting
3.5.1
Scenarios
As explained in Section 3.2, we conducted three
main scenarios:
1. Training from scratch (FS) or baseline sce-
nario, in which the models are trained only
using the target language, Javanese.
2. Standard transfer learning (TL). We construct
four distinct models utilizing treebanks from
each source language. Then, each model is
fine-tuned using the Javanese treebank.
3. Hierarchical transfer learning (HTL). First,
we train three different models using treebank
from each source language. After that, the
models were fine-tuned with the Indonesian
treebank before being fine-tuned again with
the Javanese treebank.
We also compared the performance of the four
types of word embeddings for Javanese: fastText
(Grave et al., 2019), Javanese BERT (Wongso et al.,
2021), Javanese RoBERTa (Wongso et al., 2021),
and multilingual BERT (Devlin et al., 2019).
3.5.2
Environment
Implementation is done in Python environments.
The training process is supported by the NVIDIA-
DGX server with GPU NVIDIA A100 10GB,
RAM of 64GB, and storage of 1 TB.
3.6
Evaluation
All models are evaluated using the unlabeled attach-
ment score (UAS) and labeled attachment score
(LAS) metrics, which are the most frequently
used for evaluating the dependency parsing model
(Nivre and Fang, 2017).
The margin of error
(MOE) with a 95% confidence level is also used to
estimate the range of values within which the true
population value is likely to fall.
4
Result and Analysis
The evaluation results for all scenarios are shown
in Table 3. Scores in bold are marked as the best
model in a particular word embedding type metric.
4.1
Models Comparison: From Scratch (FS)
Model, Transfer Learning (TL) Model,
and Hierarchical Transfer Learning
(HTL) Model
Table 3 shows that the transfer learning model per-
forms better than the baseline model in all word
embeddings. The performance increase is quite
significant, up to 13% on UAS and 14% on LAS.
This verifies previous studies which explain the
advantages of using transfer learning (Sarkar and
Bali, 2022). The lack of resources in Javanese also
indicates that transfer learning is suitable for use.
Figure 3 also shows that the hierarchical trans-
fer learning method consistently outperforms the
transfer learning method even though it is not too
significant. Specifically, the comparison focused on
the TL-ID and HTL models, as all models from the
HTL scenario use the TL-ID model as its second
base for the transferring method. The difference
Table 3: Evaluation results of all scenarios
Word Embedding
Model
UAS
LAS
fastText
FS
75.87 ± 2.21
68.97 ± 2.39
TL-ID
84.80 ± 1.85
78.10 ± 2.14
TL-HR
83.40 ± 1.92
76.57 ± 2.19
TL-KO
80.68 ± 2.04
74.13 ± 2.26
TL-EN
83.47 ± 1.92
77.27 ± 2.16
HTL-EN-ID
84.94 ± 1.85
79.22 ± 2.10
HTL-FR-ID
84.87 ± 1.85
77.55 ± 2.15
HTL-IT-ID
85.84 ± 1.80
78.87 ± 2.11
jv-BERT
FS
74.69 ± 2.25
67.29 ± 2.42
TL-ID
79.08 ± 2.10
72.32 ± 2.31
TL-HR
-
-
TL-KO
77.06 ± 2.17
70.29 ± 2.36
TL-EN
81.73 ± 2.00
75.52 ± 2.22
HTL-EN-ID
83.47 ± 1.92
76.64 ± 2.19
HTL-FR-ID
81.80 ± 1.99
75.38 ± 2.22
HTL-IT-ID
81.03 ± 2.02
73.99 ± 2.27
jv-RoBERTa
FS
69.80 ± 2.37
62.97 ± 2.49
TL-ID
78.45 ± 2.12
72.11 ± 2.32
TL-HR
-
-
TL-KO
82.22 ± 1.97
76.22 ± 2.20
TL-EN
77.13 ± 2.17
70.92 ± 2.35
HTL-EN-ID
77.41 ± 2.16
70.85 ± 2.35
HTL-FR-ID
83.05 ± 1.94
77.20 ± 2.17
HTL-IT-ID
83.33 ± 1.92
77.20 ± 2.17
multi-BERT
FS
75.80 ± 2.21
69.04 ± 2.39
TL-ID
82.01 ± 1.98
76.01 ± 2.21
TL-HR
83.75 ± 1.90
77.68 ± 2.15
TL-KO
79.78 ± 2.07
73.29 ± 2.28
TL-EN
80.89 ± 2.03
74.13 ± 2.26
HTL-EN-ID
82.98 ± 1.94
76.71 ± 2.18
HTL-FR-ID
83.19 ± 1.93
77.75 ± 2.15
HTL-IT-ID
84.45 ± 1.87
78.52 ± 2.12
Figure 3: Comparison of the best model evaluation for
each scenario
Figure 4: Comparison of the best model evaluation for
each word embedding
between these two scenarios shows that adding suit-
able high-resource language for the initial source
model can give a better performance.
4.2
Source Languages Comparison
Table 3 shows that two of the top three recommen-
dations from LangRank have good results. The
conclusion is that LangRank can help predict the
source language in the Javanese dependency parser.
However, it does not rule out the possibility that
other languages also have good results. For TL,
it cannot be concluded which source language
achieves the best performance since different word
embedding used by the model gives different re-
sults. For HTL using Indonesian as the interme-
diate language, Italy performs best, followed by
English as the source language.
4.3
Word Embeddings Comparison
Figure 4 shows that the model with a higher UAS
score was obtained from word embedding fastText,
followed by multilingual BERT, Javanese BERT,
and Javanese RoBERTa. For LAS evaluation, the
sequence is fastText, multilingual BERT, Javanese
RoBERTa, and Javanese BERT. Although fastText
is slightly superior, the differences are insignificant
when considering the models’ margin of error.
Table 4: Top 10 errors of the from-scratch model and its
comparison with the transfer-learning model
Ground Truth
Prediction
FS
TL
HTL
obl
obj
17
16
15
obl
nsubj
7
3
7
obj
obl
7
13
12
advcl
xcomp
5
5
6
nmod
flat
4
2
1
xcomp
advcl
4
5
5
xcomp
obl
3
3
2
nmod
obl
3
1
1
nsubj
obj
3
1
1
obj
nsubj
2
0
3
4.4
Error Analysis
Table 4 displays more detail about the performance
difference. The ten labels taken are obtained from
pairs with the highest errors in the from-scratch
model. Some pairs significantly reduce error, but
there are also pairs with no significant changes
and even more errors in scenarios with transfer
learning.
One noteworthy insight is the significantly in-
creasing error of words with ""obj"" label that pre-
dicted with ""obl"". It seems contradictory that model
accuracy is increasing simultaneously with the ad-
dition of transfer learning. It turns out that there are
a few differences in the word labeling of both labels
between the source and the target language, so the
model could not predict the word label correctly.
5
Conclusions and Future Work
This section explains the conclusion and improve-
ments that can be developed from this work.
5.1
Conclusions
This work investigates whether cross-lingual trans-
fer learning works for dependency parsing tasks
of a low-resource language, Javanese. The result
shows that the cross-lingual transfer learning model
is significantly better than the baseline model. Mod-
els with transfer learning can improve performance
on UAS and LAS metrics by up to 10%.
The best model was obtained from the hierar-
chical transfer learning method using Italian and
English as the source and Indonesian as the interme-
diary languages. Meanwhile, the standard transfer
learning method achieved the best accuracy using
Indonesian as the source language. However, the
differences between standard transfer learning and
hierarchical learning are insignificant, considering
the margin of error from each scenario.
5.2
Future Work
We focused more on the model’s learning scheme
than the model’s development with the highest
score. We use architecture from Dozat and Man-
ning (2017) rather than the one built by Mrini et al.
(2020), the state-of-the-art dependency parsing task.
So, better architecture can be used to produce a
model with a higher evaluation score in the future.
Our future works also include further error anal-
ysis, especially related to the languages involved
that LangRank chose.
It could investigate lan-
guages with different demography and characteris-
tics (Croatian and Korean) compared to Javanese.
Limitations
The following are the limitations of this research:
1. There is no hyper-parameter tuning treatment
in the model creation process.
2. Cross-validation is not performed in the data
distribution process.
3. Only one language is used as an intermediary
language in hierarchical transfer learning.
Acknowledgements
We thank the Directorate of Research and
Development,
Universitas
Indonesia,
un-
der Hibah PUTI 2022 (Grant No.
NKB-
1384/UN2.RST/HKP.05.00/2022) for funding this
research. We also thank Tokopedia-UI AI Center,
Faculty of Computer Science Universitas Indone-
sia, and Program Kompetisi Kampus Merdeka
from the Ministry of Education, Culture, Research,
and Technology Republic of Indonesia for the
NVIDIA-DGX server we used for experiments.
We also gratefully acknowledge Dr. Fajri Koto for
the insightful feedback during the pre-submission
mentorship.
References
Zeljko Agic and Nikola Ljubesic. 2015. Universal De-
pendencies for Croatian (that work for Serbian, too).
In Proceedings of the Fifth Workshop on Balto-Slavic
Natural Language Processing (BSNLP 2015).
Wasi Uddin Ahmad, Zhisong Zhang, Xuezhe Ma, Ed-
uard Hovy, Kai Wei Chang, and Nanyun Peng. 2019.
On difficulties of cross-lingual transfer with order
differences: A case study on dependency parsing. In
NAACL HLT 2019 - 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies -
Proceedings of the Conference, volume 1.
Ika Alfina, Arlisa Yuliawati, Dipta Tanaya, Arawinda
Dinakaramani, and Daniel Zeman. 2023. A gold
standard dataset for Javanese tokenization, POS tag-
ging, morphological features analysis, and depen-
dency parsing.
Fa’iq Askhabi, Arie Ardiyanti Suryani, and Moch. Arif
Bijaksana. 2020. Part of speech tagging in Javanese
using support vector machine method. e-Proceeding
of Engineering, 7.
Kurniawati Azizah, Mirna Adriani, and Wisnu Jatmiko.
2020. Hierarchical transfer learning for multilingual,
multi-speaker, and style transfer DNN-based TTS on
low-resource languages. IEEE Access, 8.
Cristina Bosco, Felice Dell’Orletta, and Simonetta Mon-
temagni. 2022. The Evalita 2014 Dependency Pars-
ing Task. Proceedings of the First Italian Conference
on Computational Linguistics CLiC-it 2014 and of
the Fourth International Workshop EVALITA 2014
9-11 December 2014, Pisa.
Zhaoying Chai, Han Jin, Shenghui Shi, Siyan Zhan, Lin
Zhuo, and Yu Yang. 2022. Hierarchical shared trans-
fer learning for biomedical named entity recognition.
BMC Bioinformatics, 23.
Jayeol Chun, Na Rae Han, Jena D. Hwang, and Jinho D.
Choi. 2019.
Building universal dependency tree-
banks in Korean. In LREC 2018 - 11th International
Conference on Language Resources and Evaluation.
Ayan Das and Sudeshna Sarkar. 2020. A survey of
the model transfer approaches to cross-lingual de-
pendency parsing. ACM Transactions on Asian and
Low-Resource Language Information Processing, 19.
Jacob Devlin, Ming Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In NAACL HLT 2019 - 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies - Proceedings of the Conference, volume 1.
Timothy Dozat and Christopher D. Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. In 5th International Conference on Learning
Representations, ICLR 2017 - Conference Track Pro-
ceedings.
Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-
mand Joulin, and Tomas Mikolov. 2019. Learning
word vectors for 157 languages. In LREC 2018 - 11th
International Conference on Language Resources
and Evaluation.
Kemal Kurniawan, Lea Frermann, Philip Schulz, and
Trevor Cohn. 2021. PPT: Parsimonious parser trans-
fer for unsupervised cross-lingual adaptation.
In
EACL 2021 - 16th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Proceedings of the Conference.
Sandra Kübler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing, volume 2. Synthesis
Lectures on Human Language Technologies.
Aufa Eka Putri Lesatari, Arie Ardiyanti, Arie Ardiyanti,
Ibnu Asror, and Ibnu Asror. 2021.
Phrase-based
statistical machine translation Javanese-Indonesian.
Jurnal Media Informatika Budidarma, 5.
Yu Hsiang Lin, Chian Yu Chen, Jean Lee, Zirui Li,
Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junx-
ian He, Zhisong Zhang, Xuezhe Ma, Antonios Anas-
tasopoulos, Patrick Littell, and Graham Neubig. 2020.
Choosing transfer languages for cross-lingual learn-
ing. In ACL 2019 - 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, Proceedings
of the Conference.
Gongxu Luo, Yating Yang, Yang Yuan, Zhanheng Chen,
and Aizimaiti Ainiwaer. 2019. Hierarchical transfer
learning architecture for low-resource neural machine
translation. IEEE Access, 7.
Daniel Jurafsky & James H. Martin. 2020.
Speech
and Language Processing: An Introduction to Natu-
ral Language Processing, Computational Linguistics,
and Speech Recognition. Prentice Hall.
Andhika Yusup Maulana, Ika Alfina, and Kurniawati Az-
izah. 2022. Building Indonesian dependency parser
using cross-lingual transfer learning. In 2022 Inter-
national Conference on Asian Language Processing
(IALP), pages 488–493.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Täckström, Claudia Bedini, Núria Bertomeu Castelló,
and Jungmee Lee. 2013. Universal dependency an-
notation for multilingual parsing. In ACL 2013 -
51st Annual Meeting of the Association for Compu-
tational Linguistics, Proceedings of the Conference,
volume 2.
Khalil Mrini, Franck Dernoncourt, Quan Tran, Trung
Bui, Walter Chang, and Ndapa Nakashole. 2020. Re-
thinking self-attention: Towards interpretability in
neural parsing. In Findings of the Association for
Computational Linguistics Findings of ACL: EMNLP
2020.
Joakim Nivre and Chiao Ting Fang. 2017.
Univer-
sal Dependency evaluation. In Proceedings of the
NoDaLiDa 2017 Workshop on Universal Dependen-
cies, UDW 2017.
Sebastian Ruder. 2023. The 4 biggest open problems in
NLP.
Dipanjan Sarkar and Raghav Bali. 2022.
Transfer
Learning in Action, 1 edition. Manning Early Access
Program.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.
Self-attention with relative position representations.
In NAACL HLT 2018 - 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies -
Proceedings of the Conference, volume 2.
Gary Simons, David Eberhard, and Charles Fennig.
2023. Ethnologue: Languages of the World, 26nd
Edition. SIL International.
Dewi Soyusiawaty, Anna Hendri Soleliza Jones, and
Nora Lestari Lestariw. 2020. The stemming appli-
cation on affixed Javanese words by using Nazief
and Adriani algorithm. In IOP Conference Series:
Materials Science and Engineering, volume 771.
Milan Straka. 2018. UDPIPE 2.0 prototype at Conll
2018 UD Shared Task. In CoNLL 2018 - SIGNLL
Conference on Computational Natural Language
Learning, Proceedings of the CoNLL 2018 Shared
Task: Multilingual Parsing from Raw Text to Univer-
sal Dependencies, pages 197–207.
C. Tho, Y. Heryadi, L. Lukas, and A. Wibowo. 2021.
Code-mixed sentiment analysis of Indonesian lan-
guage and Javanese language using lexicon-based
approach. In Journal of Physics: Conference Series,
volume 1869.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, volume 2017-December.
Karl Weiss, Taghi M. Khoshgoftaar, and Ding Ding
Wang. 2016. A survey of transfer learning. Journal
of Big Data, 3.
Wilson Wongso, David Samuel Setiawan, and Derwin
Suhartono. 2021. Causal and masked language mod-
eling of Javanese language using transformer-based
architectures. In 2021 International Conference on
Advanced Computer Science and Information Sys-
tems, ICACSIS 2021.
Amir Zeldes. 2017. The GUM corpus: creating mul-
tilayer resources in the classroom. Language Re-
sources and Evaluation, 51.
Daniel Zeman, Joakim Nivre, Mitchell Abrams, Elia
Ackermann, Noëmi Aepli, Hamid Aghaei, Željko
Agi´c, Amir Ahmadi, Lars Ahrenberg, Chika Kennedy
Ajede, Salih Furkan Akkurt, Gabriel˙e Aleksandrav-
iˇci¯ut˙e, Ika Alfina, Avner Algom, Khalid Alnajjar,
Chiara Alzetta, Erik Andersen, Lene Antonsen, Tat-
suya Aoyama, Katya Aplonova, Angelina Aquino,
Carolina Aragon, Glyd Aranes, Maria Jesus Aranz-
abe, Bilge Nas Arıcan, Hórunn Arnardóttir, Gashaw
Arutie, Jessica Naraiswari Arwidarasti, Masayuki
Asahara, Katla Ásgeirsdóttir, Deniz Baran Aslan,
Cengiz Asmazo˘glu, Luma Ateyah, Furkan Atmaca,
Mohammed Attia, Aitziber Atutxa, Liesbeth Augusti-
nus, Mariana Avelãs, Elena Badmaeva, Keerthana
Balasubramani, Miguel Ballesteros, Esha Banerjee,
Sebastian Bank, Verginica Barbu Mititelu, Starkaður
Barkarson, Rodolfo Basile, Victoria Basmov, Colin
Batchelor, John Bauer, Seyyit Talha Bedir, Shab-
nam Behzad, Kepa Bengoetxea, Ibrahim Benli, Yifat
Ben Moshe, Gözde Berk, Riyaz Ahmad Bhat, Erica
Biagetti, Eckhard Bick, Agn˙e Bielinskien˙e, Kristín
Bjarnadóttir, Rogier Blokland, Victoria Bobicev,
Loïc Boizou, Emanuel Borges Völker, Carl Börstell,
Cristina Bosco, Gosse Bouma, Sam Bowman, Adri-
ane Boyd, Anouck Braggaar, António Branco,
Kristina Brokait˙e, Aljoscha Burchardt, Marisa Cam-
pos, Marie Candito, Bernard Caron, Gauthier Caron,
Catarina Carvalheiro, Rita Carvalho, Lauren Cassidy,
Maria Clara Castro, Sérgio Castro, Tatiana Caval-
canti, Gül¸sen Cebiro˘glu Eryi˘git, Flavio Massimiliano
Cecchini, Giuseppe G. A. Celano, Slavomír ˇCéplö,
Neslihan Cesur, Savas Cetin, Özlem Çetino˘glu, Fabri-
cio Chalub, Liyanage Chamila, Shweta Chauhan,
Ethan Chi, Taishi Chika, Yongseok Cho, Jinho
Choi, Jayeol Chun, Juyeon Chung, Alessandra T.
Cignarella, Silvie Cinková, Aurélie Collomb, Ça˘grı
Çöltekin, Miriam Connor, Daniela Corbetta, Fran-
cisco Costa, Marine Courtin, Mihaela Cristescu, In-
gerid Løyning Dale, Philemon Daniel, Elizabeth
Davidson, Leonel Figueiredo de Alencar, Mathieu
Dehouck, Martina de Laurentiis, Marie-Catherine
de Marneffe, Valeria de Paiva, Mehmet Oguz De-
rin, Elvis de Souza, Arantza Diaz de Ilarraza, Carly
Dickerson, Arawinda Dinakaramani, Elisa Di Nuovo,
Bamba Dione, Peter Dirix, Kaja Dobrovoljc, Adrian
Doyle, Timothy Dozat, Kira Droganova, Puneet
Dwivedi, Christian Ebert, Hanne Eckhoff, Masaki
Eguchi, Sandra Eiche, Marhaba Eli, Ali Elkahky,
Binyam Ephrem, Olga Erina, Tomaž Erjavec, Farah
Essaidi, Aline Etienne, Wograine Evelyn, Sidney Fa-
cundes, Richárd Farkas, Federica Favero, Jannatul
Ferdaousi, Marília Fernanda, Hector Fernandez Al-
calde, Amal Fethi, Jennifer Foster, Cláudia Freitas,
Kazunori Fujita, Katarína Gajdošová, Daniel Gal-
braith, Federica Gamba, Marcos Garcia, Moa Gär-
denfors, Fabrício Ferraz Gerardi, Kim Gerdes, Luke
Gessler, Filip Ginter, Gustavo Godoy, Iakes Goenaga,
Koldo Gojenola, Memduh Gökırmak, Yoav Goldberg,
Xavier Gómez Guinovart, Berta González Saavedra,
Bernadeta Grici¯ut˙e, Matias Grioni, Loïc Grobol, Nor-
munds Gr¯uz¯ıtis, Bruno Guillaume, Céline Guillot-
Barbance, Tunga Güngör, Nizar Habash, Hinrik Haf-
steinsson, Jan Hajiˇc, Jan Hajiˇc jr., Mika Hämäläi-
nen, Linh Hà M˜y, Na-Rae Han, Muhammad Yud-
istira Hanifmuti, Takahiro Harada, Sam Hardwick,
Kim Harris, Dag Haug, Johannes Heinecke, Oliver
Hellwig, Felix Hennig, Barbora Hladká, Jaroslava
Hlaváˇcová, Florinel Hociung, Petter Hohle, Marivel
Huerta Mendez, Jena Hwang, Takumi Ikeda, An-
ton Karl Ingason, Radu Ion, Elena Irimia, O. lájídé
Ishola, Artan Islamaj, Kaoru Ito, Siratun Jannat,
Tomáš Jelínek, Apoorva Jha, Katharine Jiang, An-
ders Johannsen, Hildur Jónsdóttir, Fredrik Jør-
gensen, Markus Juutinen, Hüner Ka¸sıkara, Nadezhda
Kabaeva, Sylvain Kahane, Hiroshi Kanayama, Jenna
Kanerva, Neslihan Kara, Ritván Karahóˇga, An-
dre Kåsen, Tolga Kayadelen, Sarveswaran Kengath-
araiyer, Václava Kettnerová, Jesse Kirchner, Elena
Klementieva, Elena Klyachko, Arne Köhn, Abdul-
latif Köksal, Kamil Kopacewicz, Timo Korkiakangas,
Mehmet Köse, Alexey Koshevoy, Natalia Kotsyba,
Jolanta Kovalevskait˙e, Simon Krek, Parameswari Kr-
ishnamurthy, Sandra Kübler, Adrian Kuqi, O˘guzhan
Kuyrukçu, Aslı Kuzgun, Sookyoung Kwak, Kris
Kyle, Veronika Laippala, Lorenzo Lambertino, Ta-
tiana Lando, Septina Dian Larasati, Alexei Lavren-
tiev, John Lee, Phuong Le Hong, Alessandro Lenci,
Saran Lertpradit, Herman Leung, Maria Levina,
Lauren Levine, Cheuk Ying Li, Josie Li, Keying
Li, Yixuan Li, Yuan Li, KyungTae Lim, Bruna
Lima Padovani, Yi-Ju Jessica Lin, Krister Lindén,
Yang Janet Liu, Nikola Ljubeši´c, Olga Loginova, Ste-
fano Lusito, Andry Luthfi, Mikko Luukko, Olga Lya-
shevskaya, Teresa Lynn, Vivien Macketanz, Menel
Mahamdi, Jean Maillard, Ilya Makarchuk, Aibek
Makazhanov, Michael Mandl, Christopher Manning,
Ruli Manurung, Bü¸sra Mar¸san, C˘at˘alina M˘ar˘an-
duc, David Mareˇcek, Katrin Marheinecke, Stella
Markantonatou, Héctor Martínez Alonso, Lorena
Martín Rodríguez, André Martins, Cláudia Mar-
tins, Jan Mašek, Hiroshi Matsuda, Yuji Matsumoto,
Alessandro Mazzei, Ryan McDonald, Sarah McGuin-
ness, Gustavo Mendonça, Tatiana Merzhevich, Niko
Miekka, Aaron Miller, Karina Mischenkova, Anna
Missilä, C˘at˘alin Mititelu, Maria Mitrofan, Yusuke
Miyao, AmirHossein Mojiri Foroushani, Judit Mol-
nár, Amirsaeid Moloodi, Simonetta Montemagni,
Amir More, Laura Moreno Romero, Giovanni
Moretti, Shinsuke Mori, Tomohiko Morioka, Shigeki
Moro, Bjartur Mortensen, Bohdan Moskalevskyi,
Kadri Muischnek, Robert Munro, Yugo Murawaki,
Kaili Müürisep, Pinkey Nainwani, Mariam Nakhlé,
Juan Ignacio Navarro Horñiacek, Anna Nedoluzhko,
Gunta Nešpore-B¯erzkalne, Manuela Nevaci, Lu-
ong Nguy˜ên Thi., Huy`ên Nguy˜ên Thi. Minh, Yoshi-
hiro Nikaido,
Vitaly Nikolaev,
Rattima Nitis-
aroj, Alireza Nourian, Hanna Nurmi, Stina Ojala,
Atul Kr. Ojha, Hulda Óladóttir, Adédayo. Olúòkun,
Mai Omura, Emeka Onwuegbuzia, Noam Ordan,
Petya Osenova, Robert Östling, Lilja Øvrelid,
¸Saziye Betül Özate¸s, Merve Özçelik, Arzucan Özgür,
Balkız Öztürk Ba¸saran, Teresa Paccosi, Alessio
Palmero Aprosio, Anastasia Panova, Hyunji Hayley
Park, Niko Partanen, Elena Pascual, Marco Passarotti,
Agnieszka Patejuk, Guilherme Paulino-Passos, Giu-
lia Pedonese, Angelika Peljak-Łapi´nska, Siyao Peng,
Siyao Logan Peng, Rita Pereira, Sílvia Pereira, Cenel-
Augusto Perez, Natalia Perkova, Guy Perrier, Slav
Petrov, Daria Petrova, Andrea Peverelli, Jason Phe-
lan, Jussi Piitulainen, Yuval Pinter, Clara Pinto,
Tommi A Pirinen, Emily Pitler, Magdalena Plamada,
Barbara Plank, Thierry Poibeau, Larisa Ponomareva,
Martin Popel, Lauma Pretkalnin, a, Sophie Prévost,
Prokopis Prokopidis, Adam Przepiórkowski, Robert
Pugh, Tiina Puolakainen, Sampo Pyysalo, Peng Qi,
Andreia Querido, Andriela Rääbis, Alexandre Rade-
maker, Mizanur Rahoman, Taraka Rama, Loganathan
Ramasamy, Joana Ramos, Fam Rashel, Moham-
mad Sadegh Rasooli, Vinit Ravishankar, Livy Real,
Petru Rebeja, Siva Reddy, Mathilde Regnault, Georg
Rehm, Arij Riabi, Ivan Riabov, Michael Rießler,
Erika Rimkut˙e, Larissa Rinaldi, Laura Rituma, Pu-
tri Rizqiyah, Luisa Rocha, Eiríkur Rögnvaldsson,
Ivan Roksandic, Mykhailo Romanenko, Rudolf Rosa,
Valentin Ros,ca, Davide Rovati, Ben Rozonoyer, Olga
Rudina, Jack Rueter, Kristján Rúnarsson, Shoval
Sadde, Pegah Safari, Aleksi Sahala, Shadi Saleh,
Alessio Salomoni, Tanja Samardži´c, Stephanie Sam-
son, Manuela Sanguinetti, Ezgi Sanıyar, Dage Särg,
Marta Sartor, Mitsuya Sasaki, Baiba Saul¯ıte, Yanin
Sawanakunanon, Shefali Saxena, Kevin Scannell,
Salvatore Scarlata, Nathan Schneider, Sebastian
Schuster, Lane Schwartz, Djamé Seddah, Wolfgang
Seeker, Mojgan Seraji, Syeda Shahzadi, Mo Shen,
Atsuko Shimada, Hiroyuki Shirasu, Yana Shishkina,
Muh Shohibussirri, Maria Shvedova, Janine Siew-
ert, Einar Freyr Sigurdsson, João Silva, Aline Sil-
veira, Natalia Silveira, Sara Silveira, Maria Simi,
Radu Simionescu, Katalin Simkó, Mária Šimková,
Haukur Barri Símonarson, Kiril Simov, Dmitri Sitchi-
nava, Ted Sither, Maria Skachedubova, Aaron Smith,
Isabela Soares-Bastos, Per Erik Solberg, Barbara
Sonnenhauser, Shafi Sourov, Rachele Sprugnoli, Vi-
vian Stamou, Steinhór Steingrímsson, Antonio Stella,
Abishek Stephen, Milan Straka, Emmett Strickland,
Jana Strnadová, Alane Suhr, Yogi Lesmana Sulestio,
Umut Sulubacak, Shingo Suzuki, Daniel Swanson,
Zsolt Szántó, Chihiro Taguchi, Dima Taji, Fabio Tam-
burini, Mary Ann C. Tan, Takaaki Tanaka, Dipta
Tanaya, Mirko Tavoni, Samson Tella, Isabelle Tellier,
Marinella Testori, Guillaume Thomas, Sara Tonelli,
Liisi Torga, Marsida Toska, Trond Trosterud, Anna
Trukhina, Reut Tsarfaty, Utku Türk, Francis Ty-
ers, Sveinbjörn Hórarson, Vilhjálmur Horsteinsson,
Sumire Uematsu, Roman Untilov, Zdeˇnka Urešová,
Larraitz Uria, Hans Uszkoreit, Andrius Utka, Elena
Vagnoni, Sowmya Vajjala, Socrates Vak, Rob van der
Goot, Martine Vanhove, Daniel van Niekerk, Gert-
jan van Noord, Viktor Varga, Uliana Vedenina,
Giulia Venturi, Veronika Vincze, Natalia Vlasova,
Aya Wakasa, Joel C. Wallenberg, Lars Wallin, Abi-
gail Walsh, Jonathan North Washington, Maximilan
Wendt, Paul Widmer, Shira Wigderson, Sri Hartati
Wijono, Seyi Williams, Mats Wirén, Christian Wit-
tern, Tsegay Woldemariam, Tak-sum Wong, Alina
Wróblewska, Mary Yako, Kayo Yamashita, Naoki
Yamazaki, Chunxiao Yan, Koichi Yasuoka, Marat M.
Yavrumyan, Arife Betül Yenice, Olcay Taner Yıldız,
Zhuoran Yu, Arlisa Yuliawati, Zdenˇek Žabokrtský,
Shorouq Zahra, Amir Zeldes, He Zhou, Hanzhi Zhu,
Yilun Zhu, Anna Zhuravleva, and Rayan Ziane. 2023.
Universal Dependencies 2.12. LINDAT/CLARIAH-
CZ digital library at the Institute of Formal and Ap-
plied Linguistics (ÚFAL), Faculty of Mathematics
and Physics, Charles University.
"
"Memory bandwidth is known to be a performance bottleneck for FPGA accelerators, especially when they deal with large multi-
dimensional data-sets. A large body of work focuses on reducing of off-chip transfers, but few authors try to improve the efficiency of transfers. This paper addresses the later issue by proposing (i) a compiler-based approach to accelerator’s data layout to maximize contiguous access to off-chip memory, and (ii) data packing and runtime compression techniques that take advantage of this layout to further improve memory performance. We show that our approach can decrease the I/O cycles up to 7× compared to un-optimized memory accesses.","FPGA accelerators have gained significant popularity in recent years, despite their inherent programming complexity. High-Level Synthesis tools play a pivotal role in reducing the design challenges associated with FPGA acceleration, and facilitate their adoption in new application domains (e.g. machine learning).
FPGA accelerator boards offer massive computational capabilities, but their performance is often hindered by an under-performing memory system, which becomes a performance bottleneck [16].
This is especially true for accelerators that target compute intensive kernels operating on large data-sets. This issue is generally addressed through program transformations that increase temporal reuse, trading off-chip memory transfers for on-chip storage resource.","Another approach consists in improving the effectiveness of the memory subsystem by reorganizing access patterns and data layout in order to exploit FPGA-specific constraints[17]. One classical way of doing so is by exploiting large burst-based transfers, which requires contiguous data in memory. This is however not easy when dealing with the multi-dimensional data-sets found in many applications, since the usual row-major and/or column major layouts only guarantee contiguity of data in only one dimension.
The problem becomes even more difficult when considering custom data formats (fixed/floating point) whose bitwidth do not correspond to the native memory bus interface. In such cases, the designer is left with two choices : padding the format to fit the bus width or deal with misaligned access, both choices incurring a loss of effective bandwidth.
Nevertheless, the ability to design application-specific hardware also brings opportunities to improve bandwidth efficiency. For example, the fact that successive and/or ajacent values are numerically close (typical case in physical simulation) makes runtime compression a viable strategy to increase the effectiveness of memory transfers.nan","In this paper, we present an automatic HLS optimisation flow that combines contiguity, data packing and compression to maximize the utilization of bandwidth with custom data types. More precisely, our contributions are the following:

• an algorithm to automatically derive (i) burst-friendly data layouts, and (ii) accelerator-specific access patterns that maximize contiguity while enabling data packing and compression,
• an automated code generation framework implementing the algorithm that generates synthesizable hardware,
• an evaluation of our approach on FPGA accelerators generated using the code generator that shows a up to 7× decrease in I/O cycles.","This paper is organized as follows: Section 2 presents the concepts and core optimization techniques our work is relying on; Section 3 describes the memory layout transformation, and Section 4 explains how we automatically apply it to FPGA accelerators. Finally, Section 5 validates our approach and discusses it on a series of benchmarks.","This work gives a twofold contribution: a compression-friendly, contiguous data layout, and an automated adaptation of FPGA accelerators to use this layout thanks to polyhedral compilation tools.
Thanks to the compression and contiguity, we can automatically reduce the number of I/O cycles spent by the accelerator.",An Irredundant and Compressed Data Layout to Optimize Bandwidth Utilization of FPGA Accelerators,"Corentin Ferry, Nicolas Derumigny, Steven Derrien, Sanjay Rajopadhye","An Irredundant and Compressed Data Layout to Optimize
Bandwidth Utilization of FPGA Accelerators
Corentin Ferry
Univ Rennes, CNRS, Inria, IRISA
Rennes, France
cferry@mail.colostate.edu
Nicolas Derumigny
Colorado State University,
Univ. Grenoble Alpes, Inria,
CNRS, Grenoble INP, LIG
38000 Grenoble, France
nicolas.derumigny@inria.fr
Steven Derrien
Univ Rennes, CNRS, Inria, IRISA
Rennes, France
Steven.Derrien@irisa.fr
Sanjay Rajopadhye
Colorado State University
Fort Collins, CO, USA
First.Last@colostate.edu
ABSTRACT
Memory bandwidth is known to be a performance bottleneck for
FPGA accelerators, especially when they deal with large multi-
dimensional data-sets. A large body of work focuses on reducing
of off-chip transfers, but few authors try to improve the efficiency
of transfers. This paper addresses the later issue by proposing (i) a
compiler-based approach to accelerator’s data layout to maximize
contiguous access to off-chip memory, and (ii) data packing and run-
time compression techniques that take advantage of this layout to
further improve memory performance. We show that our approach
can decrease the I/O cycles up to 7× compared to un-optimized
memory accesses.
KEYWORDS
memory access, redundancy, data packing, padding, arbitrary pre-
cision, memory allocation
1
INTRODUCTION
FPGA accelerators have gained significant popularity in recent
years, despite their inherent programming complexity. High-Level
Synthesis tools play a pivotal role in reducing the design challenges
associated with FPGA acceleration, and facilitate their adoption in
new application domains (e.g. machine learning).
FPGA accelerator boards offer massive computational capabili-
ties, but their performance is often hindered by an under-performing
memory system, which becomes a performance bottleneck [16].
This is especially true for accelerators that target compute inten-
sive kernels operating on large data-sets. This issue is generally
addressed through program transformations that increase temporal
reuse, trading off-chip memory transfers for on-chip storage re-
source. However, this approach does not primarily seek to optimize
bandwidth usage (i.e. total amount of data transferred), leaving
room for further improvement.
Another approach consists in improving the effectiveness of
the memory subsystem by reorganizing access patterns and data
layout in order to exploit FPGA-specific constraints[17]. One clas-
sical way of doing so is by exploiting large burst-based transfers,
which requires contiguous data in memory. This is however not
easy when dealing with the multi-dimensional data-sets found in
many applications, since the usual row-major and/or column major
layouts only guarantee contiguity of data in only one dimension.
The problem becomes even more difficult when considering
custom data formats (fixed/floating point) whose bitwidth do not
correspond to the native memory bus interface. In such cases, the
designer is left with two choices : padding the format to fit the bus
width or deal with misaligned access, both choices incurring a loss
of effective bandwidth.
Nevertheless, the ability to design application-specific hardware
also brings opportunities to improve bandwidth efficiency. For ex-
ample, the fact that successive and/or ajacent values are numer-
ically close (typical case in physical simulation) makes runtime
compression a viable strategy to increase the effectiveness of mem-
ory transfers.
In this paper, we present an automatic HLS optimisation flow that
combines contiguity, data packing and compression to maximize
the utilization of bandwidth with custom data types. More precisely,
our contributions are the following:
• an algorithm to automatically derive (i) burst-friendly data
layouts, and (ii) accelerator-specific access patterns that
maximize contiguity while enabling data packing and com-
pression,
• an automated code generation framework implementing
the algorithm that generates synthesizable hardware,
• an evaluation of our approach on FPGA accelerators gen-
erated using the code generator that shows a up to 7×
decrease in I/O cycles.
This paper is organized as follows: Section 2 presents the con-
cepts and core optimization techniques our work is relying on;
Section 3 describes the memory layout transformation, and Sec-
tion 4 explains how we automatically apply it to FPGA accelerators.
Finally, Section 5 validates our approach and discusses it on a series
of benchmarks.
2
BACKGROUND
2.1
Locality optimizations
As manually optimizing an HLS design at the source level is a
tedious process, automated approaches are now routinely used for
HLS/FPGA targets in order to exploit parallelism and locality at
arXiv:2401.12071v1  [cs.AR]  22 Jan 2024
Ferry et al.
0
5
10
15
i
0
2
4
6
8
10
12
14
t
Figure 1: Domain of the Jacobi stencil divided into tiles of
size 6 × 6. Each tile contains 18 (𝑡,𝑖) points corresponding to
18 computations of 𝑐𝑡,𝑖s.
Figure 2: Compiler flow (our contributions in green)
multiple levels, which are often implemented as source-to-source
compilers [5, 14, 15, 28]. Due to the inherent regularity of HLS-valid
code, loop transformations engines such as PolyOpt/HLS [21] and
POLSCA [29] excel at this task. However, such transformations
are calibrated to improve computation time of benchmarks and do
not seek to change the memory layout to enforce memory access
contiguity.
Indeed, to get the best runtime performance from an FPGA accel-
erator, it is necessary to limit its off-chip memory accesses as much
as possible. Only a fraction of large data sets can fit the limited
size of on-chip memory; programs operating on large data sets
must therefore be transformed to work on smaller workloads at a
time. Loop tiling does this: it breaks large spaces into smaller sub-
problems called tiles, where the on-chip memory and parallelism
requirements of each tile match those available on the chip.
An accelerator for a tiled program processes the domain tile by
tile. To execute a tile, the accelerator needs to retrieve intermediate
results from previously executed tiles. These intermediate results
are located outside of the accelerator, in off-chip memory, and need
to be copied into on-chip memory.
The amount of on-chip memory needed to run a tile is directly
influenced by the tile’s shape and size. In addition, when the tile
size increases, the overall off-chip memory access is reduced, thus
improving the overall arithmetic Intensity. Selection of the best tile
3
4
5
6
7
8
9
i
5
6
7
8
9
10
11
t
Figure 3: Inter-tile communication pattern for the Jacobi
stencil: red arrows indicate data input into the tile shown in
the center, and blue arrows indicate data output from this
tile.
shape and size is outside the scope of this work, and mainly depends
on the performance / area trade-off desired by the designer.
2.2
Illustrative example: 1D Jacobi stencil
To illustrate the flow proposed in this paper, we propose a Jacobi-1D
stencil as running example. This kernel updates a one-dimensional
sequence of values, and computes each point as a weighted average
of it and its neighbors:
𝑐𝑡+1,𝑖 = 1
3
An Irredundant and Compressed Data Layout to Optimize Bandwidth Utilization of FPGA Accelerators
Figure 4: Macro-pipeline structure: read-execute-write. Our
contribution focuses on the read and write stages.
transfers are the ones this work seeks to optimize; improvements
of the compute engine fall out of the scope of this paper.
2.3
Deriving parallel accelerators using HLS
The optimizations mentioned section 2.1 are only a part of all those
optimizations that need to be applied to get the best performance.
One also needs to extract parallelism to maximize utilization of
operators on the FPGA, and create a macro-pipeline to maximize
the compute throughput.
Loop tiling naturally yields a “read-execute-writeback” macro-
pipeline structure as illustrated in Figure 4: because tiles can be
executed atomically, all I/O operations can happen before and af-
ter execution. HLS tools such as Vitis HLS support such macro-
pipelines through manual code annotation, but through a restricted
set of conditions of the pipeline (i.e. absence of cyclic dependency
between stages). Moreover, automated macro-pipelining further
increase pressure on the on-chip memory usage, as buffers used for
inter-stage communication are either implemented using FIFOs or
duplicated. While standard coding techniques would pass the com-
plete tile data buffer across pipeline stages, this widely inefficient
in hardware as communication only requires a subset of the actual
tile data. On one tile of our Jacobi-1D example from subsection 2.2,
the required data to be communicated is represented in blue in
Figure 6.
Usually, the goal of the execute stage is to take advantage of
massive operation-level parallelism (thanks to loop pipelining and
unrolling), that have already been extensively addressed [4, 21,
22, 25]. In this paper, we only seek to optimize transfer times and
memory bandwidth usage; optimisation of the complete design
including crafting and balancing of a coarse-grain pipeline are not
evaluated.
In applications with low operational intensity, the limited off-
chip memory bandwidth turns the read and write stages into perfor-
mance bottlenecks, even with aggressive tiling transformations. In
most cases, this is due to a poor utilization of the off-chip memory
interface, where only a fraction of the peak bandwidth is effectively
used due to inefficient access patterns. As a matter of fact, approach-
ing the peak memory bandwidth requires that almost all access to
external memory consist of large transfers over contiguous memory
locations (called memory burst).
HLS tools can infer burst memory accesses depending on the
target interface. In the case of a shared bus (e.g. AXI, PCIe), which
is commonly found for off-chip accesses, a burst access may occur
32 bits
17 bits
Padding
17 bits
17 bits
? bits
? bits
Marker @ bit 14
17 bits
Marker @ bit 15
No packing
Packing
Compression
Figure 5: Data packing and compression reduce storage and
transfer redundancy at the expense of address alignment
and, for compression, predictability of addresses.
if the bus supports it and the compiler recognizes access to a series
of consecutive addresses. Tools such as Vitis HLS 2022.2 exploit
this using with either a call to a HLS-specific memcpy routine, or
through some form or pattern matching in the source code. In burst
mode, no cycle is spent stalling for a new value after a one-shot
initialization latency, which yields full utilization of the available
bandwidth.
The goal of this work is to propose a source level compiler
optimisation to (i) reorganize data in memory to enable con-
tiguous burst access and (ii) further improve bandwidth uti-
lization through packing and compression. Our optimization
pass is meant to be integrated within an HLS polyhedral compila-
tion flow, as illustrated in Figure 2; aiming at sitting between the
locality optimization phase (tiling) and the HLS synthesis stage. In
fact, our approach does not replace locality optimizations, it
complements them.
2.4
Padding vs packing
In order to maximize the utilization of bandwidth, every bit of data
transmitted must be useful. However, with domain-specific data
types (e.g., custom fixed point), unused bits must usually be trans-
mitted due to memory alignment requirements. In the following, we
explain how data contiguity can be leveraged to two ways: packing
data to reduce the unused bits transmitted; and compressing data
to further save bandwidth.
Most memories are byte-addressable and most processor archi-
tectures also require aligned accesses at word boundaries, usually at
32 or 64 bits. Although FPGA accelerators can operate on arbitrary-
precision data types, off-chip data transfers must abide by the ad-
dressing requirements of the external memory. They therefore need
to pad the incoming and outgoing data: in practice, for a 17-bit ac-
cess, 32 bits of data will be transferred, 15 bits of them being wasted
in padding.
Note that padding is necessary to enable random accesses to
data: it provides the guarantee that a given memory cell contains
only the requested data and no manipulation needs to be done to
extract it. Data accessed in a contiguous manner does not need this
guarantee and may overlap multiple adjacent cells, as simple wire
manipulations on the FPGA will give back the original data.
Ferry et al.
Data packing, as illustrated in Figure 5, consists in avoiding
padding the data so that words are adjacent at the bit level in mem-
ory. Figure 5 shows buffer structure for unpacked and packed data
of 17 bits in 32-bit words. Unpacked data has aligned addresses,
but requires extra storage and transfers unused data; packed data
has unaligned addresses but saves storage and avoids some redun-
dant bits from being transmitted. It becomes however impossible
to randomly seek in a packed stream due to misalignment without
additional data processing, but by definition such random seeks do
not happen with contiguous accesses.
In our approach, we leverage contiguous accesses to (i) avoid
the adverse effects of packing induced misalignment and (ii) to
maximize bandwidth utilization by not padding data.
2.5
Runtime data compression
Packing data saves bandwidth by eliminating the padding bits,
and is applied independently of the data itself. However, further
optimisation is possible by exploiting properties of the data (e.g.,
correlation between integers in an array) for compression. When
this technique is applied right before / after off-chip communica-
tions, the design benefits from a reduction of I/O cycles (as the
amount of data transferred is reduced) without increase of the com-
putation subsystem as the latency of the compression module can
be hidden by the pipelining structure
Compression is easy to apply to contiguous streams of data, but
is not to data where indexed or random accesses are necessary.
We must exhibit access contiguity, as it is in general impossible
to seek within a compressed block without decompressing more
data than needed. Figure 5 shows that the position of data within a
compressed block is unpredictable.
Our approach performs runtime compression and, to maximize
its efficiency, creates data blocks with a contiguous access guarantee
to ensure every decompressed piece of data is used.
In general, the compression algorithm is domain-specific, e.g.,
ADPCM for voice [6] or JPEG for images [24]. For FPGA implemen-
tations, the choice of the algorithm is also driven by its throughput:
compression and decompression must be able to sustain the input
and output throughput not to become the bottleneck. We choose to
illustrate the idea with a simple differential compression algorithm
which encodes a sequence 𝑤0𝑤1 . . .𝑤𝑛 of 𝑁-bit words as follows:
• Encode 𝑤0 as is.
• For 1 ⩽ 𝑖 ⩽ 𝑛:
(1) Compute Δ = 𝑤𝑖 − 𝑤𝑖−1,
(2) Let 𝐿 be the number of leading zeroes of Δ if Δ ⩾ 0, or
leading ones if Δ < 0,
(3) Encode 𝑁 − 𝐿 using ⌊1 + log2(𝑁)⌋ bits, followed by
the sign bit of Δ,
(4) Encode the 𝑁 − (𝐿 + 1) lowest bits of Δ.
This technique is especially effective when the distribution of
the transferred data is not spread, typically on benchmarks based
on the computation of the average such as our Jacobi-1D example
from subsection 2.2.
3
MEMORY LAYOUT OPTIMIZATION
This work seeks to minimize the I/O cycles for an accelerator to
transmit and retrieve its data into and from global memory. Locality
I1
I2
I3
I4
I5
I6
I7
O1
O2
O4
O3
Figure 6: MARS: Groups of points within a tile which data is
contiguous in global memory. In blue, the MARS produced
by the center tile (O1 to O4); in red, the MARS consumed by
that same tile (I1 to I7).
optimizations having already been applied, we do not want to
store or retrieve fewer values from memory, but rather make better
accesses to memory, from the bandwidth utilization, and therefore
I/O cycles, standpoints.
To this aim, we build a contiguous, irredundant and com-
pressed data layout. This section details the steps taken: first, we
analyze the program and extract sets of on-chip data as contiguous
data blocks; second, we lay out these data blocks to obtain further
contiguous accesses; third, we compress and pack the data blocks
together to save even more bandwidth.
3.1
Extracting Contiguous Data Blocks
The first step in our method consists in analyzing a program’s be-
havior with respect to memory, to determine which data can/should
be grouped together as contiguous blocks. The sought groups of
data honor two properties:
• Atomicity: If any data in the group is needed for an instance
of the accelerator’s execution flow (a tile), then the entire
group is also needed for the same tile.
• Irredundancy: No data is retrieved or stored more than once
into memory throughout the execution of a single tile.
These groups of data are determined by using the analysis tech-
nique from Ferry et al. [9] within a polyhedral compiler. This anal-
ysis yields sets of on-chip memory addresses, such that all the data
from these on-chip cells will be allocated a contiguous block of data
in off-chip memory.
Example. Applying the MARS analysis from [9] to the Jacobi stencil
of Section 2.2 gives the sets of addresses corresponding to the points
illustrated in Figure 6:
• For the input of each tile, seven contiguous blocks of data la-
beled I1 to I7 are to be taken, across three different producer
tiles.
An Irredundant and Compressed Data Layout to Optimize Bandwidth Utilization of FPGA Accelerators
• For the output, each tile will produce four contiguous blocks
of data labeled O1 to O4.
There is a correspondence between output blocks (MARS) O𝑥 from
a tile and input blocks I𝑦 from other tiles: each O𝑥 corresponds to
one I𝑦 in several other tiles.
Without any further information, the result of MARS analysis
would make the accelerator require seven input and four output
burst accesses. This number could potentially be reduced. If I1, I2
and I3 were adjacent in memory, it would be possible to make a
single access instead of three, and likewise for I5, I6 and I7. The
total number of input accesses would go down to just three.
In order to reduce the number of accesses to the above, we have
to show that it is actually achievable: the blocks I1 through I7 are
read by multiple tiles, and coalescing opportunities for one tile may
be incompatible with another tile’s coalescing opportunities.
The next subsection formalizes this example into an optimization
problem seeking to minimize the number of accesses.
3.2
Enabling Coalesced Accesses across
Contiguous Data Blocks
From the polyhedral analysis of the previous subsection, we have
determined sets of on-chip data to be grouped as contiguous blocks
of data, called MARS. How these blocks are laid out in memory is
important for access performance: if multiple MARS happen to be
accessed in a row and they are adjacent in memory, the accesses to
these MARS can be coalesced into a single access and better utilize
bandwidth.
This section explains how the “outer layout” of the MARS is
determined so as to maximize the coalescing opportunities.
3.2.1
Properties of the layout. The goal of this work is to minimize
the number of I/O cycles, and therefore the data layout must exhibit
contiguity (for both reading and writing). However, that contiguity
must not come at the price of an increase in I/O volume. To model
this constraint, we apply two hypotheses.
Contiguous tile-level allocation. We are looking for a layout of
MARS in memory, and know that compression will be applied to
them. Due to the size and position of compressed blocks being
unpredictable, it is not feasible to interleave MARS from multiple
tiles in memory. Therefore, we allocate each tile a contiguous
block of memory for its MARS output.
This allocation has two consequences: the write side can be done
entirely contiguously, and we only have to optimize contiguity at
the read side.
Irredundancy of storage. Under the previous hypothesis, we want
to maximize the coalescing opportunities between MARS accesses
for the read side only. While it is possible to obtain this contiguity
by replicating the MARS in multiple layouts, one per consumer,
doing so would defeat the goal to save I/O cycles. We therefore
choose to store each MARS only once in memory (irredundant
storage).
The goal is now to find a single layout for the MARS produced by
each tile, that exhibits as much read-side coalescing opportunities
as possible. We obtain it throgh an optimization problem that is
defined in the next subsections.
3.2.2
Example. In the example of Section 3.1, it appeared that the
number of burst accesses could go from 7 to 3. Let us show there
actually exists a layout achieving these 3 bursts.
Figure 6 shows the correspondence between input and output
MARS:
• I1, I2 and I3 come from the southwest tile, corresponding
to its O2, O3 and O4 blocks. We would like these three
MARS to be contiguous, regardless of which relative order,
to make a single burst.
• I4 comes from the south tile, corresponding to its O2 block.
• I5, I6 and I7 come from the southeast tile, corresponding to
its O1, O2 and O3 blocks. We would also like them to be
contiguous.
We do not make any hypothesis on the relative location of data
from the southwest tile, the south tile and the southeast tile. This
makes it impossible to obtain fewer than 3 burst accesses.
The information we have at this point can be used as the con-
straints and objective of an optimization problem: we want to max-
imize the number of contiguities in the layout among those desired,
under the irredundancy constraint. We provide a solver with the
following problem:
• Maximize the contiguities among the desired ones: make
MARS O2, O3 and O4 contiguous in any order, and make
MARS O1, O2 and O3 also contiguous in any order.
• Per the hypothesis of Section 3.2.1, we want a layout of
MARS O1, O2, O3 and O4.
• There can be no fewer read bursts than 3.
The solver returns the following layout of the output MARS for
each tile: O1, O3, O2, O4.
Looking from the consumers, I1, I2 and I3 (resp. southwest O2,
O3 and O4) are contiguous; I5, I6 and I7 (southeast O1, O2 and O3)
are also contiguous. We can therefore coalesce, for each tile, the
reads I1, I2 and I3 into a single burst, and I5, I6 and I7 into another
burst, achieving the three sought input bursts.
3.2.3
General case. In this section, we lay out the blocks of data
from the MARS analysis to maximize the coalescing opportunities
between them.
With the allocation choice of Sec. 3.2.1, writes are guaranteed to
be done without discontiguity. We therefore lay out the MARS to
make the read side as contiguous as possible. In other words, we
need to lay out the MARS in memory so that as many MARS as
possible can be read as a coalesced burst.
We propose to model this problem as an Integer Linear Program-
ming optimization problem as described in Algorithm 1. Intuitively,
if a pair of MARS is needed by a consumer tile and the two MARS
are next to each other in memory, then a coalesced access for the
two (a “contiguity”) is issued. We therefore seek to maximize the
number of such contiguities.
The solution to this optimization problem, given by the solver is
an ordered list of the MARS produced by each tile, that allows the
minimal number of transactions to read all MARS input of a tile.
The layout created in this section honors the irredundancy prop-
erty of the MARS (see Section 3.1), but does not yet take full ad-
vantage of their atomicity: the fact that the MARS are contiguous
Ferry et al.
Algorithm 1: Optimizing the MARS layout
Input: 𝑀𝐼 = (𝑚𝑖 ) : 𝑖 = 1, ..., 𝑁 = list of MARS,
P = list of producer tiles from which MARS are needed,
Result: 𝑀𝑂 = ordered list of MARS
Optimization Variables:
let 𝛿𝑖,𝑗 ∈ {0, 1} = successor variables: 𝛿𝑖,𝑗 = 1 encodes that MARS 𝑖
is immediately before MARS 𝑗 in memory.
let 𝛾𝑖 ∈ {1, . . . , 𝑁 } be a permutation; 𝛾𝑖 is the position where
MARS 𝑖 will be in the final layout.
Problem Constants:
let 𝑎𝑝,𝑖,𝑗 be equal to 1 if 𝑚𝑖 and 𝑚𝑗 (𝑖, 𝑗 ∈ [[1, 𝑁 ]]) from tile
𝑝 ∈ P are consumed together, 0 otherwise.
Maximize #{contiguities}: Í
𝑃 ∈P
Í𝑁
𝑖=1
Í𝑁
𝑗=1
𝑗≠𝑖
𝑎𝑃,𝑖,𝑗𝛿𝑖,𝑗 subject to:
• ∀𝑖 : 𝛿𝑖,𝑖 = 0 (a MARS is not its own predecessor)
• ∀𝑖 : Í
𝑗 𝛿𝑖,𝑗 ⩽ 1 (a MARS has at most 1 precedecessor)
• ∀𝑗 : Í
𝑖 𝛿𝑖,𝑗 ⩽ 1 (a MARS has at most 1 successor)
• Í
𝑖
Í
𝑗 𝛿𝑖,𝑗 = 𝑁 − 1 (number of successor relations)
• ∀𝑖 : 0 ⩽ 𝛾𝑖 ⩽ 𝑁 − 1 (permutation of length 𝑁)
• ∀𝑖, 𝑗 : (𝛾𝑗 − 𝛾𝑖 = 1) ⇔ (𝛿𝑖,𝑗 = 1) (definition of successor)
• ∀𝑖, 𝑗 : (𝑖 ≠ 𝑗) ⇒ |𝛾𝑖 − 𝛾𝑗 | ⩾ 1 (MARS have ≠ positions)
return 𝑀𝑂 = (𝑚𝛾 (1), ...,𝑚𝛾 (𝑁 ) ) (𝛾-ordered list of MARS)
Figure 7: MARS data shown without compression, with com-
pression (inside the MARS) and with MARS compression and
packing. Packing the compressed MARS preserves the conti-
guity of coalesced accesses.
blocks of data makes them ideal candidates for data packing and
compression. This is what we perform in the next subsection.
3.3
Contiguity-Preserving Block Compression
So far, our approach has given a layout of data in memory enabling
coalesced accesses to contiguous blocks of data produced and con-
sumed by an accelerator. These blocks have an atomicity property
that we can further exploit to save bandwidth, by applying data
packing and compression, as illustrated in Figure 7.
3.3.1
Combining compression and packing. Compressed blocks of
data must be considered atomic in the sense that no random accesses
into them are possible. This atomicity property is borne by the
MARS, as each MARS data block is entirely used when it is accessed,
i.e. there are no partial accesses to a MARS.
Compressing the MARS reduces the size of the data and therefore
saves bandwidth and storage space; however, it can also break the
contiguity brought by the layout of Section 3.2 as illustrated by
Figure 7. To preserve it, we also apply packing to the compressed
MARS, making them immediately adjacent to each other in memory.
Packing compressed MARS also spares the accelerators from unused
reads due to padding.
3.3.2
Need to preserve metadata. As the size of compressed blocks
depends on their data, it is impossible to know the exact size of
each access. However, the size of a burst access must be known
prior to the request being issued; additionally, using an estimation
of the size or an over-approximation would result in unused input
data or additional requests to fill in missing data.
In order to be able to exactly fetch the right size, it is necessary
to keep track of the size of each compressed MARS. Moreover, the
packing of compressed MARS means that the start of a compressed
block may be improperly aligned. It is also therefore necessary to
keep track of the alignment of each MARS for proper decompres-
sion. In our implementation, bookkeeping is done using on-chip
markers that are filled in after each MARS is compressed. Details
are in Section 4.2.2.
Packing will cause unused input data to enter; however, its size is
bounded to one aligned word at the beginning and one aligned word
at the end of each transaction. This input redundancy is notably
independent of the size of the MARS.
4
INTEGRATION INTO HLS DESIGN FLOW
In this section, we show how we transform an HLS accelerator
description in order to optimize its off-chip memory accesses for
bandwidth utilization.
The off-chip data layout and compression proposed in Section 3
can be automatically implemented around the existing description
of a tile in HLS. The result is a sequence of steps:
• Read MARS layout data and non-MARS input data from
off-chip memory into on-chip FIFOs,
• Decompress the input data into FIFOs,
• Dispatch MARS data into on-chip buffers with an allocation
suitable for computation,
• Perform the computations onto on-chip buffers,
• Collect MARS output data from the on-chip buffers into
FIFOs,
• Compress the collected data,
• Write back the results into MARS layout in off-chip mem-
ory.
The next subsections explain how the complex data structures
describing the MARS are turned into two simple decompression/dis-
patch and collect/compression steps.
4.1
From MARS to Collect/Dispatch Functions
The input and output data of each tile is respectively copied into
and out of on-chip buffers before the tile execution takes place and
after it has fully completed. This is the step where the data goes
from a contiguous layout to a non-contiguous layout (suitable for
execution) and vice-versa.
Implementing these dispatch and collect steps requires to de-
scribe each MARS so that the data contained in it is placed into, or
taken from, the right location in on-chip memory. Before dispatch
and after collect, the data is located into FIFOs in the contiguous
layout.
An Irredundant and Compressed Data Layout to Optimize Bandwidth Utilization of FPGA Accelerators
1 // MARS Dispatch
2 for(int i=0; i<33; ++i) {
3
// take on -chip address from ROM
4
struct mars_transfert mt = FPGA_MARS_IN_TBL[i];
5
switch (mt.array) {
6
// on -chip random write
7
case MARS_DATA_ENUM ::A: {
8
marsToMem_A(mt.dim0 , mt.dim1);
9
break;
10
}
11
case MARS_DATA_ENUM ::B: {
12
marsToMem_B(mt.dim0 , mt.dim1);
13
break;
14
}
15
}
16 }
Figure 8: Structure of the MARS dispatch implementation
(off-chip to on-chip layout)
MARS can have arbitrary complex shapes, and cannot in general
be described using simple loops. However, it is possible to fully
unroll these loops and obtain a list of on-chip addresses for each
MARS. Such unrolled lists are placed into read-only memories
on chip. Iterating through these ROMs as in Figure 8 gives the
corresponding addresses. The size of these ROMs is notably only
dependent on the tile size, and not on the problem size or data type.
4.2
Automatic compression
When the data is in the contiguous layout in the form of MARS,
it can be seamlessly compressed and decompressed, and the com-
pressed MARS can be packed to preserve contiguity. We explain
here the compression, packing and decompression steps, along with
how the compression metadata is taken care of.
4.2.1
Compressing Data and Packing MARS. The compression step
is relatively straightforward: the compression module takes its input
from the collect step FIFO, and generates a compressed stream of
data from it. The layout of the data in this FIFO is not altered by the
compression step. Likewise, the decompression step takes a stream
of compressed words and decompresses it into a FIFO, which is then
used by the MARS dispatch step. MARS packing is transparently
implemented by the compression step: because MARS are provided
in a contiguous manner from the collect step, the first word of each
MARS will be immediately adjacent to the last word of the previous
MARS in the compressed data stream.
Our compressor, which algorithm is given in Section 2.5, is
pipelined with an initiation interval of 1 cycle, despite a loop-carried
dependence.
The difficult part to implement is decompression: because not
all MARS from a given tile are decompressed, we need to be able
to seek at the start of a particular MARS. This ability is given by
metadata described in the next paragraph.
4.2.2
Metadata management. The consequence of MARS compres-
sion is that their size is unknown a priori. To preserve the contiguity
of the layout from Section 3.2, we must avoid padding the com-
pressed MARS to preserve alignment, Therefore, the compressed
MARS are packed and immediately adjacent to each other in mem-
ory.
To keep track of the position of each MARS, we use a data struc-
ture with two pieces of information: a coarse-grain position indi-
cating how far (in aligned words) to seek, and a fine-grain position
marker that specifies which bit is the first of the said MARS.
Because the length of MARS is known at compile time and con-
stant across tiles, the position of the markers within the uncom-
pressed stream is also constant. Therefore, like the MARS descrip-
tions, the positions of markers (i.e. start of each MARS) within the
uncompressed stream are put into a ROM:
1 #define NB_MARKERS 3
2 #define MARKERS {62, 63, 64}
The markers for the compressed stream are maintained within
an on-chip cache, which size is specified at synthesis time via a
macro:
1
struct compressed_marker <NB_MARS_POS_BITS ,
LOG_BUS_WIDTH > markers[COMPRESSION_METADATA_SIZE ][
NB_MARKERS ];
The allocation within this cache is done from the host: registers
are used to specify whether a tile’s MARS are compressed, whether
its dependences are, and where the markers for its dependences
are located. This location depends on the size of the space; for the
Jacobi stencil, the formula is:
1
unsigned compressionMetadataAllocation(
2
int tsteps , int n, int M1 , int M2 , int k1 , int k2) {
3
return (k2) + M2DEC_FORMULA + M2 * ((k1 - 1) & 0x01);
4 }
It should be noted that the markers structure is persistent be-
tween runs. It is updated by the MARS write step and used by
the MARS read step. This update prevents the current HLS tools
from constructing a macro-pipeline (e.g. using the HLS DATAFLOW
pragma) unless the structure is in a separate module.
4.3
Host/FPGA dispatching of tiles
The FPGA accelerator must have a simple control structure to
exhibit as much parallelism as possible. Therefore, only full tiles
are executed on FPGA. Full tiles also all have the same volume of
I/O, regardless of their position in the iteration space.
Partial tiles, i.e. those that contain space boundaries, are run on
the host CPU, using the original program’s allocation. To permit
this, data computed on FPGA is taken back from MARS into the
original program’s memory, and MARS are created back from partial
tiles results. It can be demonstrated that no FPGA tiles need any
missing MARS data from partial tiles, and therefore there is no
issue in writing part of the MARS for these tiles.
The operations performed to execute a partial tile (on the host)
are:
• Read MARS from neighboring full tiles that were executed
on FPGA, remap their data to its original location,
• Execute the tile’s iterations using the original allocation,
• Write back MARS by copying data from the original allo-
cation, skipping cells that would be in MARS yet have no
producer iteration.
Ferry et al.
The control flow necessary for compression would significantly
lengthen the execution of host tiles. Therefore, only tiles which
producers and consumers are all executed on FPGA will use com-
pression.
5
EVALUATION
We evaluate our approach with respect to the following questions:
• Compile-time performance: How much time does it take
to compute the MARS layout?
• Design quality: How does using MARS affect the FPGA
accelerator’s area consumption?
• Runtime performance: How much I/O cycles do com-
pressed MARS save with respect to a non-MARS memory
layout?
• Applicability: How does the data type, tile size and prob-
lem size affect the compression ratio?
5.1
Protocol and benchmarks
5.1.1
Benchmarks. We have selected the following applications
from the PolyBench/C suite[20]:
• jacobi-1d: Jacobi 1D stencil, as used in the running exam-
ple;
• jacobi-2d: Two-dimensional version of the Jacobi stencil,
exhibiting few and simple MARS;
• seidel-2d: More complex benchmark exhibiting a higher
number of MARS with more complex shapes.
Layout determination was done using the Gurobi solver (version
10.0.3 build v10.0.3rc0 (linux64)).
The data types used are fixed-point numbers (18 bits, 24 bits,
28 bits) and floating-point numbers (float, double). We also ran
simulations with a 12-bit fixed-point data type without synthesizing
it, Vitis HLS being unable to infer bursts from that data type.
The chosen applications provide a non-MARS data layout in
their original code. Because FPGA developers usually try to seek
burst accesses where possible, we have created two access patterns
on the non-MARS layout to compare against that try to exhibit
bursts:
• A minimal access pattern, fetching and storing the exact
I/O footprint of the tile, letting the HLS tool infer bursts
where possible.
• A rectangular bounding box of the accessed data like done
in PolyOpt/HLS [21], which description is simple enough
to infer only burst accesses.
Both access patterns are generated using a polyhedral code genera-
tor available in ISL [27].
5.1.2
Hardware platform. We used a Xilinx ZCU104 evaluation
board, equipped with a xczu7ev MPSoC. We ran Pynq 3.0.1 with
Linux 5.15 and synthesis was done using the Vitis/Vivado suite
version 2022.2.2 All benchmarks, are running at a clock frequency
of 187 MHz and communicate with the off-chip DDR using one non
cache-coherent AXI HP port.
5.1.3
Protocol. Each benchmark is run for each data type, each
space size and tile size. Part of the computation is done on the host:
incomplete tiles are executed on a single thread on the Cortex-A53
CPU of the MPSoC. Transfer cycles are measured only for the FPGA
tiles and do not account for the host.
Cycle measurements are gathered using an on-FPGA counter
and the area measurements are extracted from Vivado place and
route reports.
Table 1 shows the characteristics of each benchmark, in terms of
number of MARS, and number of bursts after coalescing optimiza-
tion of Sec. 3.2.
5.2
Results and discussion
5.2.1
Compile-time performance. Table 2 shows the time it took
for each benchmark to be run through the layout determination and
code generation framework. The compilation process does not take
more than a few seconds to execute for the benchmarks we selected,
starting from the polyhedral representation of the program to the
end of HLS code generation. Notably, the layout determination ILP
problem only depends on the number of MARS and is independent
of the tile size.
5.2.2
Design quality. Figure 9 shows the total area occupied by
our benchmarks, with respect to the different memory allocation
baselines. One tile size per benchmark is considered.
MARS introduces extra control logic and extra I/O functions that
the other baselines do not have. It is therefore normal to observe
area increases with this baseline. The most significant increases
in Figure 9a are for jacobi1d; in this benchmark, on-chip arrays
are implemented in logic instead of Block RAM. Figure 9b shows
little DSP and BRAM consumption by this benchmark compared to
others. FIFOs holding all the MARS are implemented only on the
MARS baseline, and require extra BRAMs. The extra DSP blocks
for MARS baselines come from the address computations that are
performed inside the I/O units; the size of the space is passed as a
parameter instead of being a constant, requiring true multipliers.
Figure 9a shows that the data width causes the logic area to
increase with it. This increase is more sensible in jacobi1d where
the on-chip arrays are implemented in logic instead of Block RAM,
which effect is also visible in Figure 9b.
5.2.3
Runtime performance. Figure 10 shows the transfer time rel-
ative to compressed MARS for each data type and each benchmark.
Impact of dimensionality. For the 2d examples that have three-
dimensional iteration spaces, using MARS layout is already prof-
itable versus the non-MARS layouts; most of the gains are due to
contiguity more than compression. On the one-dimensional Jacobi
example, the gains are on the contrary more due to compression:
the data being one-dimensional, non-MARS layouts are already
contiguous. For small tile sizes like 6 × 6, the gains are marginal
if any: the number of compressed elements is too small to exhibit
large gains from compression.
Effect of data type. On the jacobi-1d benchmark, the choice
of a 200 × 200 tile size shows a more significant benefit in using
compressed MARS for fixed-point data types than floating-point.
This is explained with the better compression ratio: when modeling
continuous spaces like those used on the Jacobi stencils, neighbor-
ing fixed-point values will have more higher bits in common than
An Irredundant and Compressed Data Layout to Optimize Bandwidth Utilization of FPGA Accelerators
Benchmark
Tile Sizes
#MARS In
#MARS Out
Read bursts
Write bursts
jacobi-1d
6 × 6 64 × 64, 200 × 200
7
4
3
1
jacobi-2d
4 × 5 × 7, 10 × 10 × 10
28
13
10
1
seidel-2d
4 × 10 × 10
33
13
10
1
Table 1: Characteristics of the selected benchmarks. The number of bursts per tile accounts for layout-induced access coalescing
and is independent of tile and problem size.
(a) Logic resource occupancy: LUT, Registers
(b) DSP and BRAM occupancy
Figure 9: Area statistics for the benchmarks
(a) jacobi-1d
jacobi-2d
jacobi-2d
seidel-2d
(b) seidel-2d, jacobi-2d
Figure 10: Transfer time relative to compressed MARS (lower is better).
Benchmark
Tile Size
Compile Time (s)
jacobi-1d
6 × 6
0.76
jacobi-1d
64 × 64
0.68
jacobi-1d
200 × 200
1.02
jacobi-2d
4 × 5 × 7
5.57
jacobi-2d
10 × 10 × 10
5.09
seidel-2d
4 × 10 × 10
3.21
Table 2: Layout Computation and Code Generation Time
floating-point data where neighboring values mostly only share
the exponent.
5.2.4
Applicablity. Figure 11 shows the compression rate for each
data type and tile size for the jacobi1d benchmark. Two ratios are
shown: the true ratio which accounts only for the bit savings due to
compression, and a ratio with padding that accounts for the savings
due to not padding the data. The ratio with padding is the one our
accelerators really benefit from, because the data is not packed in
memory except in compressed MARS form.
Ferry et al.
Figure 11: Compression ratio vs. data type and tile size for
jacobi1d
Overall, compressing the data for the selected benchmarks is
almost always profitable, possibly largely as the compression ratio
goes up to 5.09:1 for 200 × 200 tiles and 18-bit type.
We can observe that large tiles (64 × 64, 200 × 200) exhibit closer
compression ratios than smaller tiles (6 × 6). This discrepancy can
be explained by the compressed chunks being too small to benefit
from the data’s low entropy; for the smallest data type and tile size,
compressing data is even worse than not compressing.
6
RELATED WORK
This work comes as part of a global effort to relieve memory-
boundness of high-performance accelerators. In this section, we
study other techniques used to relieve the memory wall, some of
which may not apply to compilers due to not being automatable or
breaking program semantics.
6.1
Data Compression
Data compression saves bandwidth without requiring to modify the
program’s algorithm. It is therefore suitable for many bandwidth-
bound problems.
6.1.1
Compression techniques. Data compression in FPGA acceler-
ators is already a necessity for some intrinsically memory-bound
applications such as deep convolutional networks, as no locality
optimization can bring further bandwidth savings. We here focus
on two kinds of compression: lossless and lossy.
Lossless compression. Lossless compression guarantees that the
decompressed data is exactly the same as the data before it was
compressed. This property makes it possible to do seamless, in-
line compression and decompression as is done for MARS. This is
commonly performed in deep neural network accelerators [1, 11]
Sparse encoding can be considered a form of lossless compres-
sion, and is also commonly found in machine learning applications
[8, 13]. Sparse data structures often require indirections, which
make them unsuitable for use in polyhedral compiler flows unless
the sparse structure is immutable [12].
Lossy compression. It is possible to save more storage and band-
width by using lossy compression. Some applications in machine
learning can afford a loss of precision without degrading the qual-
ity of the result, e.g. using JPEG-compressed images [18] as inputs.
However, automatic compression alters the data and cannot be auto-
matically inserted by a compiler unless the user explicitly requests
it.
6.1.2
Dynamic data compression. In this work, we automate the
compression and decompression of data and it is transparent to
the computation engine on FPGA. Other works [19, 23] perform
dynamic, demand-driven compression without prior knowledge
of the data to be handled. Thanks to the static control flow of
polyhedral codes, all the data flow is statically known and it is not
necessary to maintain a cache policy.
6.2
Memory access optimization
The layout we propose in this work optimizes memory accesses
by exhibiting contiguity using polyhedral analysis. In this section,
we go through other polyhedral memory access optimizations, and
explain other non-polyhedral ways it is possible to improve memory
accesses.
6.2.1
Polyhedral-based optimizations. Using the polyhedral model
and loop tiling to capture the data flow is the subject of a number of
works, proposing different breakups of the dataflow. Datharthri et
al. [7] and Bondhugula [2] propose decompositions of the inter-tile
communications to minimze MPI communications. This work also
seeks to optimize the passing of intermediate results, but the data
allocation is not statically determined like in this work.
A MARS-like decomposition of the inter-tile data flow into
coarse-grain blocks for MPI has been proposed by Zhao et al. [30];
our work achieves irredundancy which requires a finer-grain mod-
eling than the one proposed by [30].
6.2.2
Domain-specific optimizations. Memory access optimizations
such as a change of data layout or access pattern can also be spe-
cific to each problem. We show here two cases of domain-specific
optimizations.
Data blocking. Data blocking (or tiling) is memory layout trans-
formation that chunks multi-dimensional arrays into contiguous
blocks. Similar to loop tiling, data blocking allows to coalesce ac-
cesses to entire regions of the input or output data.
Data blocking can be efficient when the memory footprint of
one iteration of an accelerator corresponds to a data tile. Although
it has been used to optimize machine learning accelerators [26],
it may break spatial locality and degrade performance of accesses
that cross tile boundaries.
Data blocking can be combined with loop tiling and polyhedral
analysis to coalesce inter-tile accesses. Ferry et al. [10] seeks to
exhibit the largest possible contiguous units spanning multiple
tiles.
Stencil optimization. Stencil computations have regular and stat-
ically known memory access patterns. Domain-specific optimizers
like SODA [3] derive an optimized FPGA architecture and memory
layout specific to each stencil.
An Irredundant and Compressed Data Layout to Optimize Bandwidth Utilization of FPGA Accelerators
7
CONCLUSION
This work gives a twofold contribution: a compression-friendly,
contiguous data layout, and an automated adaptation of FPGA ac-
celerators to use this layout thanks to polyhedral compilation tools.
Thanks to the compression and contiguity, we can automatically
reduce the number of I/O cycles spent by the accelerator.
REFERENCES
[1] Thea Aarrestad, Vladimir Loncar, Nicolò Ghielmetti, Maurizio Pierini, Sioni
Summers, Jennifer Ngadiuba, Christoffer Petersson, Hampus Linander, Yutaro
Iiyama, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris, Dylan Rankin,
Sergo Jindariani, Kevin Pedro, Nhan Tran, Mia Liu, Edward Kreinar, Zhenbin
Wu, and Duc Hoang. 2021.
Fast convolutional neural networks on FPGAs
with hls4ml. Machine Learning: Science and Technology 2, 4 (jul 2021), 045015.
https://doi.org/10.1088/2632-2153/ac0ea1
[2] Uday Bondhugula. 2013. Compiling Affine Loop Nests for Distributed-Memory
Parallel Architectures. In Proceedings of the International Conference on High
Performance Computing, Networking, Storage and Analysis. ACM. https://doi.
org/10.1145/2503210.2503289
[3] Yuze Chi, Jason Cong, Peng Wei, and Peipei Zhou. 2018. SODA: Stencil with
optimized dataflow architecture. In 2018 IEEE/ACM International Conference on
Computer-Aided Design (ICCAD). IEEE, 1–8. https://doi.org/10.1145/3240765.
3240850
[4] Young-kyu Choi and Jason Cong. 2018. HLS-based optimization and design
space exploration for applications with variable loop bounds. In 2018 IEEE/ACM
International Conference on Computer-Aided Design (ICCAD). IEEE, 1–8.
[5] Jason Cong, Muhuan Huang, Peichen Pan, Yuxin Wang, and Peng Zhang. 2016.
Source-to-source optimization for HLS. FPGAs for Software Programmers (2016),
137–163.
[6] P Cummiskey, Nikil S. Jayant, and James L. Flanagan. 1973. Adaptive Quantization
in Differential PCM Coding of Speech. Bell System Technical Journal 52 (09 1973).
https://doi.org/10.1002/j.1538-7305.1973.tb02007.x
[7] Roshan Dathathri, Chandan Reddy, Thejas Ramashekar, and Uday Bondhugula.
2013. Generating Efficient Data Movement Code for Heterogeneous Archi-
tectures with Distributed-Memory. In Proceedings of the 22nd International
Conference on Parallel Architectures and Compilation Techniques. IEEE. https:
//doi.org/10.1109/PACT.2013.6618833
[8] Yixiao Du, Yuwei Hu, Zhongchun Zhou, and Zhiru Zhang. 2022.
High-
Performance Sparse Linear Algebra on HBM-Equipped FPGAs Using HLS. In Pro-
ceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable
Gate Arrays. ACM. https://doi.org/10.1145/3490422.3502368
[9] Corentin Ferry, Steven Derrien, and Sanjay Rajopadhye. 2023. Maximal Atomic
irRedundant Sets: a Usage-based Dataflow Partitioning Algorithm. In 13th Inter-
national Workshop on Polyhedral Compilation Techniques (IMPACT’23).
[10] Corentin Ferry, Tomofumi Yuki, Steven Derrien, and Sanjay Rajopadhye. 2022.
Increasing FPGA Accelerators Memory Bandwidth with a Burst-Friendly Mem-
ory Layout. IEEE Transactions on Computer-Aided Design of Integrated Circuits
and Systems (2022), 1–1. https://doi.org/10.1109/tcad.2022.3201494
[11] Yijin Guan, Ningyi Xu, Chen Zhang, Zhihang Yuan, and Jason Cong. 2017. Using
Data Compression for Optimizing FPGA-Based Convolutional Neural Network
Accelerators. In Lecture Notes in Computer Science. Springer International Pub-
lishing, 14–26. https://doi.org/10.1007/978-3-319-67952-5_2
[12] Marcos Horro, Louis-Noël Pouchet, Gabriel Rodríguez, and Juan Touriño. 2023.
Custom High-Performance Vector Code Generation for Data-Specific Sparse
Computations. In Proceedings of the International Conference on Parallel Architec-
tures and Compilation Techniques (Chicago, Illinois) (PACT ’22). Association for
Computing Machinery, New York, NY, USA, 160–171. https://doi.org/10.1145/
3559009.3569668
[13] Shiqing Li, Di Liu, and Weichen Liu. 2023. Efficient FPGA-based Sparse Matrix-
Vector Multiplication with Data Reuse-aware Compression. IEEE Transactions
on Computer-Aided Design of Integrated Circuits and Systems (2023), 1–1. https:
//doi.org/10.1109/tcad.2023.3281715
[14] Junyi Liu, Samuel Bayliss, and George A Constantinides. 2015. Offline synthesis
of online dependence testing: Parametric loop pipelining for HLS. In 2015 IEEE
23rd Annual International Symposium on Field-Programmable Custom Computing
Machines. IEEE, 159–162.
[15] Junyi Liu, John Wickerson, Samuel Bayliss, and George A Constantinides. 2017.
Polyhedral-based dynamic loop pipelining for high-level synthesis. IEEE Trans-
actions on Computer-Aided Design of Integrated Circuits and Systems 37, 9 (2017),
1802–1815.
[16] Michael Lo, Young-kyu Choi, Weikang Qiao, Mau-Chung Frank Chang, and
Jason Cong. 2023. HMLib: Efficient Data Transfer for HLS Using Host Mem-
ory. In Proceedings of the 2023 ACM/SIGDA International Symposium on Field
Programmable Gate Arrays. 50–50.
[17] Florian Mayer, Julian Brandner, and Michael Philippsen. 2023. Employing Polyhe-
dral Methods to Reduce Data Movement in FPGA Stencil Codes. In International
Workshop on Languages and Compilers for Parallel Computing. Springer, 47–63.
[18] Hiroki Nakahara, Zhiqiang Que, and Wayne Luk. 2020. High-Throughput Convo-
lutional Neural Network on an FPGA by Customized JPEG Compression. In 2020
IEEE 28th Annual International Symposium on Field-Programmable Custom Com-
puting Machines (FCCM). 1–9. https://doi.org/10.1109/FCCM48280.2020.00010
[19] O. Ozturk, M. Kandemir, and M.J. Irwin. 2009. Using Data Compression for
Increasing Memory System Utilization. IEEE Transactions on Computer-Aided
Design of Integrated Circuits and Systems 28, 6 (jun 2009), 901–914.
https:
//doi.org/10.1109/tcad.2009.2017430
[20] Louis-Noël Pouchet and Tomofumi Yuki. 2016.
PolyBench/C 4.2.1.
http:
//polybench.sf.net
[21] Louis-Noel Pouchet, Peng Zhang, P. Sadayappan, and Jason Cong. 2013.
Polyhedral-based data reuse optimization for configurable computing. In Pro-
ceedings of the ACM/SIGDA international symposium on Field programmable gate
arrays - FPGA '13. ACM Press. https://doi.org/10.1145/2435264.2435273
[22] Tiago Santos and João MP Cardoso. 2020. Automatic selection and insertion of
hls directives via a source-to-source compiler. In 2020 International Conference
on Field-Programmable Technology (ICFPT). IEEE, 227–232.
[23] Somayeh Sardashti, Andre Seznec, and David A. Wood. 2016. Yet Another
Compressed Cache: A Low-Cost Yet Effective Compressed Cache. ACM Trans.
Archit. Code Optim. 13, 3, Article 27 (Sept. 2016), 25 pages. https://doi.org/10.
1145/2976740
[24] A. Skodras, C. Christopoulos, and T. Ebrahimi. 2001. The JPEG 2000 still image
compression standard. IEEE Signal Processing Magazine 18, 5 (2001), 36–58.
https://doi.org/10.1109/79.952804
[25] Atefeh Sohrabizadeh, Cody Hao Yu, Min Gao, and Jason Cong. 2022. AutoDSE:
Enabling software programmers to design efficient FPGA accelerators. ACM
Transactions on Design Automation of Electronic Systems (TODAES) 27, 4 (2022),
1–27.
[26] Teng Tian, Xi Jin, Letian Zhao, Xiaotian Wang, Jie Wang, and Wei Wu. 2020.
Exploration of Memory Access Optimization for FPGA-based 3D CNN Accelera-
tor. In 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE).
1650–1655. https://doi.org/10.23919/DATE48585.2020.9116376
[27] Sven Verdoolaege. 2010. isl: An Integer Set Library for the Polyhedral Model.
In Mathematical Software – ICMS 2010, Komei Fukuda, Joris van der Hoeven,
Michael Joswig, and Nobuki Takayama (Eds.). Springer Berlin Heidelberg, Berlin,
Heidelberg, 299–302.
[28] Hanchen Ye, Cong Hao, Jianyi Cheng, Hyunmin Jeong, Jack Huang, Stephen
Neuendorffer, and Deming Chen. 2022. Scalehls: A new scalable high-level
synthesis framework on multi-level intermediate representation. In 2022 IEEE
International Symposium on High-Performance Computer Architecture (HPCA).
IEEE, 741–755.
[29] Ruizhe Zhao, Jianyi Cheng, Wayne Luk, and George A Constantinides. 2022.
POLSCA: Polyhedral High-Level Synthesis with Compiler Transformations. In
2022 32nd International Conference on Field-Programmable Logic and Applications
(FPL). IEEE, 235–242.
[30] Tuowen Zhao, Mary Hall, Hans Johansen, and Samuel Williams. 2021. Improving
communication by optimizing on-node data movement with data layout. In
Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of
Parallel Programming. ACM. https://doi.org/10.1145/3437801.3441598
"
"We study singing voice cancellation, a subtask of music source separation, to estimate instrumental background from a stereo mix. We optimize a real-time speech separation network for stereo input and output. Improvements in quality are achieved by tuning parameters and expanding the training set. We introduce a stereo metric to detect inconsistencies. We evaluate our approach using objective and a large-scale MUSHRA trial.","We assess an optimized and evaluated Singing Voice Cancellation (SVC) system. We analyze system performance under constrained computational resources and identify key factors affecting output quality. SVC involves removing the singing voice from the background in a mixed song and is a special case of Music Source Separation (MSS). Deep learning advancements have improved SVC and MSS performance, leading to better objective metrics. MSS algorithms typically use spectral or waveform representations. Spectral methods express input/output in time-frequency domain, using U-Net architecture, band-split RNN, or multi-dilated convolutions. Waveform-based methods tackle MSS end-to-end, like Demucs with 1-D convolutional encoder and decoders and a bidirectional Long Short-Term Memory (LSTM). Later versions include a spectral branch and transformer layers. Similar approaches have been used for Singing Voice Separation. These methods produce high-quality results but are designed for offline and non-causal setups, with no memory or runtime constraints. They are unsuitable for challenging scenarios where separation occurs on low-memory edge devices under streaming conditions with strict processing time constraints. Efficient solutions have been proposed for speech source separation under these circumstances. Conv-TasNet is an end-to-end masking-based architecture with 1-D convolutional encoder and decoder, a Temporal convolutional network (TCN)-based separator, and depth-wise convolutions to reduce network size. This architecture has also been adapted and expanded to address MSS. In this work, Conv-TasNet is adapted and optimized for SVC to achieve comparable output quality to more resource-hungry networks.","nanRecent advances in deep learning have led to significant performance improvements in both SVC and MSS, leading to much improved objective metrics. MSS algorithms typically use a spectral [1] or waveform [2] representation. In the first case, both input and output are expressed in the time-frequency domain and popular solutions adopt a U-Net architecture [1, 3], combined with band-split RNN [4], conditioning mechanism [5] or multi-dilated convolutions [6]. Inspired by Wave-Net [7], waveform-based MSS methods tackle the problem end-to-end. An example is Demucs [2], which uses 1-D convolutional encoder and decoders with a bidirectional LST.","We propose Vox-TasNet, an adaptation of Conv-TasNet [12] to the task of SVC in a stereo setup, hence able to jointly estimate both left and right channels. Similar to Conv-TasNet, the network follows a masking-based approach and is composed of three modules, as shown in Fig. 1. The Encoder block reduces time resolution of the raw audio waveform using a two-dimensional convolutional (2D Conv) layer. This creates separate embedding for the left and right channels of the mix whilst benefiting from the information in both. These embed-dings are stacked and fed into the Separator block. The Separator block consists of multiple stacks of separable-depthwise convolutional (S-Conv) layers with increasing dilation and estimates masks for the left and right channel representations. Only the S-Conv layers in the first group are non-causal, making the Separator block almost entirely causal. This allows the network to look ahead for a small time interval. Mask-ing is applied via element-wise multiplication and the result-ing masked representations are fed into the Decoder (Transp 2D Conv), which outputs stereo audio accompaniment. Aside from the stereo setup, there are two main differences between Vox-TasNet and Conv-TasNet. First, the Separator's skip connections are removed to reduce memory footprint, as originally proposed in [15]. Second, we increase the number of S-Conv layers in each group but dimensionality is fixed inside each S-Conv layer, rather than expanded and squeezed. This choice allows the separator to learn longer time dependencies, without increasing the number of parameters.","We first focus on the analysis of objective metrics on the two evaluation datasets, MUSDB and B. In Table 2 we show the results for two versions of Vox-TasNet compared to the HybridDemucs baseline, together with correspondent latency and memory footprint. Both Vox-TasNet versions and the baseline perform differently on the two test datasets. Cross-dataset testing more strongly affects the baseline evaluated on B when compared to Vox-TasNet trained on A. Considering SI- SDR values together with the required resources, it is evident how Vox-TasNet trained on A is able to reach quality comparable to the baseline with less than 10% network parameters and small look-ahead. This analysis supports the importance of training dataset in achieving high-quality results while meeting the limited resource constraints. The second experiment aims at understanding how training dataset size and quality impact model performance. In this case, both A and C are partitiond into subsets of increasing size. In Fig 2 we report SI- SDR statistics obtained over the number of tracks of training partitions. As expected, in both cases larger training dataset leads to more effective voice cancelling. Comparing the two training dataset at similar dataset size, training on A always leads to higher SI- SDR and the best result is obtained with the largest A partition, despite C being much larger. This result highlights the importance of training data quality, Since C is not musically curated, unlike A. We also trained the model combining A and C dataset with different proportion but no significant improvements were achieved. The final experiment aims at analysing the differences between the proposed Vox-TasNet stereo model and its mono version, MonoVox-TasNet. Both models have been trained on A. In the first column of Table 3 we report SI- SDR computed separately on left and right channel, indicated as SI- SDRmono. We very that the difference between SI- SDR distributions for mono and stereo model on each evaluation dataset are not statistically significant, hence overall quality is not affected by stereo architecture. In the second column we report values for the stereo metric SSASI- SDR proposed in Sec 2.2. A lower value indicates lower stereo artifacts and higher symmetry between left and right channel vocal attenuation. Results highlight that stereo architecture greatly improves output quality in terms of multichannel artifacts on both test datasets. In Fig 3 we report MUSHRA scores for all conditions tested I and MUSDB. As expected, all models had lower scores than the reference audio on each evaluation dataset. Moreover, Vox-TasNet and MonoVox-TasNet had quality scores lower than HysridDemucs, but higher than the original Conv-TasNet architecture.","In this work we presented an efficient stereo SVC architecture, able to operate in real-time and with low memory requirements. By training the model on a large dataset, we reached performances comparable to larger and non-real-time models. Moreover, we show the benefits of using a stereo architecture through a new stereo separation asymmetry metric which can be formulated for any source separation metric. The results from objective evaluation are validated through a large-scale MUSHRA test. We believe this study may help in highlighting the key factors that enable the use of deep learning in real-time music processing.",Resource-constrained stereo singing voice cancellation,"Clara Borrelli, James Rae, Dogac Basaran, Matt McVicar, Mehrez Souden, Matthias Mauch","RESOURCE-CONSTRAINED STEREO SINGING VOICE CANCELLATION
Clara Borrelli
James Rae
Dogac Basaran
Matt McVicar
Mehrez Souden
Matthias Mauch
Apple
ABSTRACT
We study the problem of stereo singing voice cancellation, a
subtask of music source separation, whose goal is to estimate
an instrumental background from a stereo mix. We explore
how to achieve performance similar to large state-of-the-art
source separation networks starting from a small, efficient
model for real-time speech separation. Such a model is use-
ful when memory and compute are limited and singing voice
processing has to run with limited look-ahead. In practice,
this is realised by adapting an existing mono model to handle
stereo input. Improvements in quality are obtained by tuning
model parameters and expanding the training set. Moreover,
we highlight the benefits a stereo model brings by introducing
a new metric which detects attenuation inconsistencies be-
tween channels. Our approach is evaluated using objective
offline metrics and a large-scale MUSHRA trial, confirming
the effectiveness of our techniques in stringent listening tests.
Index Terms— singing voice cancellation, music source
separation
1. INTRODUCTION
In this work we present a study of optimisation and evaluation
of a Singing Voice Cancellation (SVC) system. We analyse
the performance of such as system in limited computational
resource scenarios and understand which factors most affect
output quality. SVC consists of removing the singing voice
from the instrumental background in a fully mixed song, and
can be interpreted as a special case of Music Source Sepa-
ration (MSS). The main difference lies in the fact that MSS
aims to retrieve one separate stem for each source present in
the mix, while SVC considers only instrumental accompani-
ment as desired source.
Recent advances in deep learning have led to significant
performance improvements in both SVC and MSS, leading
to much improved objective metrics. MSS algorithms typ-
ically use a spectral [1] or waveform [2] representation. In
the first case, both input and output are expressed in the
time-frequency domain and popular solutions adopt a U-Net
architecture [1, 3], combined with band-split RNN [4], con-
ditioning mechanism [5] or multi-dilated convolutions [6].
Inspired by Wave-Net [7], waveform-based MSS methods
tackle the problem end-to-end. An example is Demucs [2],
which uses 1-D convolutional encoder and decoders with
a bidirectional LSTM. Later versions of Demucs include a
spectral branch [8] and transformer layers [9]. Similar ap-
proaches have been adopted for Singing Voice Separation
[10, 11]. All of these methods produce high quality results
but are often designed to be run offline and in a non-causal
setup, i.e., having access to the complete audio input, and no
constraints on memory or run-time. These methods are not
suitable for challenging yet realistic scenarios, where separa-
tion happens on low-memory edge devices, under streaming
conditions and with hard constraints on processing time. Ef-
ficient solutions have been proposed to tackle speech source
separation under these circumstances. Conv-TasNet [12] is
an end-to-end masking-based architecture which employs
1-D convolutional encoder and decoder, a Temporal Con-
volutional Network (TCN)-based separator and depth-wise
convolutions to reduce network size. This architecture has
also been adapted and expanded to address MSS [13, 14].
In this work Conv-TasNet is adapted and optimised for
SVC such that the output is of comparable quality to more
resource-hungry networks.
Such architecture guarantees a
low-memory footprint thanks to a limited number of param-
eters, and is able to operate in real-time. We train the model
on a large dataset, to achieve high quality output and we show
how training dataset size and quality affect the model’s output
quality. The main contributions in this work are:
• To match a real-world setup, we modify Conv-TasNet to
take advantage of stereo input and to produce stereo output.
This design improves vocal attenuation consistency across
stereo channels, and we propose a new stereo metric to ver-
ify this improvement.
• We propose a stereo separation asymmetry metric able to
measure stereo artifacts and show that the proposed stereo
architecture helps to prevent them.
• To validate the proposed approach, we conduct experiments
to evaluate objectively and subjectively using a MUSHRA-
style test.
We show that a relatively small and specialised model with
appropriate training can reach outstanding high quality com-
parable to much larger models.
©2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or
future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works,
for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
arXiv:2401.12068v1  [cs.SD]  22 Jan 2024
Encoder
Separator
Decoder
2D Conv 
Nkernel=384 (N) 
Ksize =(1,64) (L) 
Kstride=(1,32) (L/2)
S-Conv 
D = 1
S-Conv 
D = 2
Concat
S-Conv 
D = 2X-1 
S-Conv 
D = 1
S-Conv 
D = 2
S-Conv 
D = 2X-1 
2D Conv 
Nkernel =2                 
Ksize =(1, 384)   
Sigmoid
Mask
Transp  
2D Conv 
Nkernel= 1 
Ksize=(1,64) (L) 
Kstride=(1,32) (L/2) 
1x1 Conv 
Nkernel =448 (B)
x(t)
˜y(t)
cLN Norm
[B, T/2L]
[2N, T/2L]
[1, 2, T]
X=9
R=4
Mix Embedding
Instrument Embedding
[N, 2, T/2L]
[N, 2, T/2L]
[1, 2, T]
Transpose
[2, N, T/2L]
×
[B, T/2L]
(a) Network Architecture
1x1 Conv 
Nkernel=448 (H=B)
D Conv 
Nkernel=448 (H=B) 
Ksize =3 (P) 
Kdilation= D 
Kgroups = 448 (B) 
PreLu + Norm
+
PreLu + Norm
[B, T/2L]
[B, T/2L]
[B, T/2L]
(b) S-Conv Block
Fig. 1: Vox-TasNet architecture and S-Conv block in detail. Letters in parenthesis follow the original notation used in [12]. In
square brackets we report input dimensionality as [Ch, W, H] throughout the network.
2. METHOD
A sampled stereo audio signal x(t) ∈ R2 with t ∈ [0, T),
which corresponds to a fully mixed track, can be modelled as
a linear sum of an instrumental or accompaniment stem y(t)
and a vocal stem v(t), i.e., The goal of our system is to es-
timate y(t) in real-time and with low memory requirements.
A real-time scenario implies that the network output should
be produced with low-latency. This forces the model to be al-
most causal, i.e., the output in a specific time instant depends
largely on past input samples and possibly a small portion of
future samples, i.e., look-ahead, which can be buffered. To
be able to satisfy low memory footprint requirements, it is
necessary to limit the number of network parameters.
2.1. Network Architecture
We propose Vox-TasNet, an adaptation of Conv-TasNet [12]
to the task of SVC in a stereo setup, hence able to jointly esti-
mate both left and right channels. Similar to Conv-TasNet, the
network follows a masking-based approach and is composed
of three modules, as shown in Fig. 1. The Encoder block
reduces time resolution of the raw audio waveform using a
two-dimensional convolutional (2D Conv) layer. This creates
separate embeddings for the left and right channels of the mix
whilst benefiting from the information in both. These embed-
dings are stacked and fed into the Separator block. The Sepa-
rator block consists of multiple stacks of separable-depthwise
convolutional (S-Conv) layers with increasing dilation and es-
timates masks for the left and right channel representations.
Only the S-Conv layers in the first group are non-causal, mak-
ing the Separator block almost entirely causal. This allows
the network to look ahead for a small time interval. Mask-
ing is applied via element-wise multiplication and the result-
ing masked representations are fed into the Decoder (Transp
2D Conv), which outputs stereo audio accompaniment. Aside
from the stereo setup, there are two main differences between
Vox-TasNet and Conv-TasNet. First, the Separator’s skip con-
nections are removed to reduce memory footprint, as orig-
inally proposed in [15]. Second, we increase the number of
S-Conv layers in each group but dimensionality is fixed inside
each S-Conv layer, rather than expanded and squeezed. This
choice allows the separator to learn longer time dependencies,
without increasing the number of parameters.
2.2. Stereo separation asymmetry metric
During informal listening experiments, we noticed that
single-channel model produced SVC results which were
audibly inconsistent across channels in terms of loudness and
attenuation. On the other side, a stereo-native architecture
is able to prevent these artifacts and exploit cross-channel
information as well. This lead us to devise a stereo separation
asymmetry metric which attempted to measure this effect.
Let’s consider a source separation metric, i.e., SI-SDR.
It can be formulated in a frame-wise setup, i.e., comparing
short frames of prediction and ground-truth signal. Let us
define (SI-SDRL(n), SI-SDRR(n)) as the frame-wise metric
computed for N windows of length W and hop size H on left
channel and right channel respectively, with n corresponding
to window index. We define ∆SI-SDR(n) = |SI-SDRL(n) −
SI-SDRR(n)| as the absolute value of difference between left
and right frame-wise metric values. We use as stereo met-
ric SSASI-SDR =
1
N
P
n ∆SI-SDR(n), as the average over time
of ∆SI-SDR(n) . Larger values of SSAM corresponds to large
difference between left and right channel’s separation met-
ric distance over time, hence quality inconsistency. Note that
∆SI-SDR(n) and SSASI-SDR can be defined for any other dB-
based source-separation metric (i.e., SIR or BSS-Eval met-
rics).
2
Dataset
Num Tracks (Train/Test)
Curated
Use
A
3,290
Yes
Train
B
3,797 (3,375/422)
Yes
Train/Test
C
13,625
No
Train
MUSDB
150 (100/50)
Yes
Train/Test
Table 1: Datasets specifications
3. EXPERIMENTAL SETUP
To train and validate our system, we consider four different
datasets. Three datasets are internal and we will refer to them
with A, B and C. For benchmarking against state-of-the-art
models, we use the publicly available MUSDB [16] dataset.
In order to be compatible with the task of SVC, the instrumen-
tal accompaniment track is obtained by linearly summing the
bass, drums and other stems. Dataset specifications are re-
ported in Table 1. For all internal datasets, we apply a pre-
processing step to filter out tracks for which the vocal stem is
silent for more than 50% of the song. Note from Table 1 that
C is larger than A and B, but not musically curated by experts
hence potentially contains noisier samples.
To have comparable experiments, we use a fixed set of
training parameters in all of our experiments. Sampling rate
is fixed at 44, 100 Hz, and each training sample is composed
of 4 s of stereo audio, randomly sampled from each song at
training time. The batch size is equal to 6 and each model is
trained for 500 epochs. We use the Adam optimizer [17] with
an initial learning rate 0.0001, scheduled with decay param-
eter equal to 0.99. The loss function is a weighted average
of L1 distance in time domain and multi-resolution spectral
L1 distance using window lengths equal to 0.01 s, 0.02 s and
0.09 s and hop-size equal to half of window lengths. Time and
spectral losses are weighted with weights 0.875 and 0.125 re-
spectively.
As a strong baseline, we use HybridDemucs [8], a large
music source separation model without real-time or mem-
ory constraints.
We aim to match the quality obtained
with HybridDemucs, while keeping as reference a resource-
constrained scenario. HybridDemucs is trained on the aug-
mented MUSDB dataset and extracts four separate stems
from the original mix: drums, bass, vocal and others. We
obtain the accompaniment mix as the sum of drums, bass
and others stems. Additionally, we consider a mono version
of Vox-TasNet, namely MonoVox-TasNet. It has a similar
architecture to Vox-TasNet, as shown in Fig 1, but 2-D con-
volutions are replaced with 1-D convolutions. A stereo output
is obtained by separately processing left and right channel and
concatenating the results.
To evaluate the proposed system, we take advantage of
both objective and subjective evaluation. For objective eval-
uation, we use Scale Invariant Source-to-Distortion Ratio
(SI-SDR) [18] computed between the ground-truth accom-
paniment and the estimated accompaniment. Moreover, we
use the separation symmetry metric presented in Section 2.2,
computed using SI-SDR with W = 1.5 s and H = 0.75 s,
to evaluate out stereo vs mono model. As objective metrics
do not always correlate strongly with human perception of
audio quality [19], we report subjective evaluation results
obtained via a Multiple Stimuli with Hidden Reference and
Anchor (MUSHRA)-style test [20]. For each MUSHRA trial,
participants hear a 10-second clip of a mix and reference
(i.e., all stems except the vocals).
Using the reference as
a benchmark, participants use a 0-100 quality scale to rate
the output obtained with HybridDemucs, Vox-TasNet, and
MonoVox-TasNet trained on A and B for 1500 epochs. Also
embedded in the test set was the hidden reference (expected
to be rated as excellent), as well as a lower anchor (expected
to be rated as poor). As a lower anchor we use the original
Conv-TasNet architecture without skip connections in the
Separator, and trained on A only. Each participate completed
MUSHRA trials for four songs. We exclude judgments from
participants if they (a) rated the hidden reference lower than
90 on more than one trial or (b) rated the lower anchor higher
than 90 for more than one trial. We test the robustness of
our results by comparing subjective model performance on
50 tracks from the MUSDB test set and 53 tracks from the
internal datasets obtaining I. Participation was completed (a)
online, (b) while using headphones, and (c) after receiving
training and the completion of practice trials. As MUSHRA
scores are bounded (between 0 and 100), we model 2,960
judgements obtained from 99 participants as proportions (di-
viding each score by 100) using beta regression with a logit
link function [21] and applying a transformation to prevent
0’s or 1’s [22]. Results are transformed back to the original
0-100 scale for ease of interpretation. Dependencies in the
data (e.g., each participant made multiple judgements) led us
to use a Bayesian multilevel model with weakly informative
priors. All parameters met the ˆR < 1.1 acceptance criterion
[23], indicating model convergence.
4. RESULTS
4.1. Objective Evaluation
We first focus on the analysis of objective metrics on the two
evaluation datasets, MUSDB and B.
In Table 2 we show
the results for two versions of Vox-TasNet compared to the
HybridDemucs baseline, together with correspondent latency
and memory footprint. Both Vox-TasNet versions and the
baseline perform differently on the two test datasets. Cross-
dataset testing more strongly affects the baseline evaluated
on B when compared to Vox-TasNet trained on A.
Con-
sidering SI-SDR values together with the required resources,
it is evident how Vox-TasNet trained on A is able to reach
quality comparable to the baseline with less than 10% net-
work parameters and small look-ahead. This analysis sup-
ports the importance of training dataset in achieving high-
quality results while meeting the limited resource constraints.
3
Model
Training
dataset
Test
dataset
SI-SDR (dB)↑
Receptive
field (s)
Look-
ahead(s)
Num
param (M)
Hybrid
Demucs
MUSDB
MUSDB
16.34 ± 3.03
5.00
2.5
80
B
14.02 ± 3.44
Vox-TasNet
MUSDB
MUSDB
9.90 ± 3.22
1.86
0.37
7.5
B
8.59 ± 3.09
A
MUSDB
13.27 ± 3.13
B
12.23 ± 3.28
Table 2: Mean and standard deviations of SI-SDR training
Vox-TasNet on A and MUSDB compared to the baseline, to-
gether with corresponding latency and memory impact.
The second experiment aims at understanding how training
dataset size and quality impact model performance. In this
case, both A and C are partitioned into subsets of increas-
ing size. In Fig 2 we report SI-SDR statistics obtained over
the number of tracks of training partitions. As expected, in
both cases larger training dataset leads to more effective voice
cancelling. Comparing the two training datasets at similar
dataset size, training on A always leads to higher SI-SDR
and the best result is obtained with the largest A partition,
despite C being much larger. This result highlights the im-
portance of training data quality, since C is not musically
curated, unlike A. We also trained the model combining A
and C datasets with different proportion but no significant
improvements were achieved. The final experiment aims at
103
104
Training dataset size
12
13
SI-SDR [dB]
A
C
(a)
103
104
Training dataset size
11.5
12.0
SI-SDR [dB]
A
C
(b)
Fig. 2: SI-SDR mean and standard error for Vox-TasNet
obtained training on different partitions of the two training
datasets tested on MUSDB (a) and on B (b).
analysing the differences between the proposed Vox-TasNet
stereo model and its mono version, MonoVox-TasNet. Both
models have been trained on A. In the first column of Ta-
ble 3 we report SI-SDR computed separately on left and right
channel, indicated as SI-SDRmono. We verify that the dif-
ference between SI-SDR distributions for mono and stereo
model on each evaluation dataset are not statistically signifi-
cant, hence overall quality is not affected by stereo architec-
ture. In the second column we report values for the stereo
Model
Test Dataset
SI-SDRmono (dB)↑
SSASI-SDR (dB)↓
Vox-TasNet
MUSDB
12.79 ± 3.19
1.10 ± 0.45
B
11.64 ± 3.19
1.08 ± 0.58
MonoVox-TasNet
MUSDB
13.13 ± 2.94
1.81 ± 0.73
B
11.85 ± 3.21
2.30 ± 1.34
Table 3: Mean and standard deviation of SI-SDRmono and of
SSASI-SDR for Vox-TasNet compared with the mono version
trained on A, tested on B and MUSDB. For SI-SDRmono, the
higher the better. For SSASI-SDR the lower the better.
metric SSASI-SDR proposed in Sec 2.2. A lower value indi-
cates lower stereo artifacts and higher symmetry between left
and right channel vocal attenuation. Results highlight that
stereo architecture greatly improves output quality in terms
of multichannel artifacts on both test datasets.
4.2. Subjective Evaluation
In Fig 3 we report MUSHRA scores for all conditions tested
I and MUSDB. As expected, all models had lower scores
than the reference audio on each evaluation dataset. More-
over, Vox-TasNet and MonoVox-TasNet had quality scores
lower than HybridDemucs, but higher than the original Conv-
TasNet architecture.
Generally, models were perceived to
have higher quality when tested on the MUSDB (vs.
I)
dataset. Finally, while there was no meaningful difference
between the mono and stereo models for the MUSDB dataset,
Vox-TasNet had significantly better quality than MonoVox-
TasNet on the I dataset.
Reference
HybridDemucs
Vox-TasNet
MonoVox-TasNet
Conv-TasNet
Model
40
60
80
Rating
MUSDB
I
Fig. 3: Estimated MUSHRA scores for all conditions tested
on I and MUSDB. Intervals are 95% credible intervals.
5. CONCLUSIONS
In this work we presented an efficient stereo SVC architec-
ture, able to operate in real-time and with low memory re-
quirements.
By training the model on a large dataset, we
reached performances comparable to larger and non-real-time
models. Moreover, we show the benefits of using a stereo ar-
chitecture through a new stereo separation asymmetry metric
which can be formulated for any source separation metric.
The results from objective evaluation are validated through a
large-scale MUSHRA test. We believe this study may help in
highlighting the key factors that enable the use of deep learn-
ing in real-time music processing.
4
6. REFERENCES
[1] M. Vardhana, N. Arunkumar, S. Lasrado, E. Abdulhay,
and G. Ramirez-Gonzalez, “Convolutional neural net-
work for bio-medical image segmentation with hard-
ware acceleration,”
Cognitive Systems Research, vol.
50, pp. 10–14, 2018.
[2] A. D´efossez, N. Usunier, L. Bottou, and F. Bach, “Mu-
sic source separation in the waveform domain,” arXiv
preprint arXiv:1911.13254, 2019.
[3] R. Hennequin, A. Khlif, F. Voituret, and M. Moussal-
lam, “Spleeter: a fast and efficient music source sepa-
ration tool with pre-trained models,” Journal of Open
Source Software, vol. 5, no. 50, pp. 2154, 2020.
[4] Y. Luo and J. Yu, “Music source separation with band-
split RNN,” IEEE/ACM Transactions on Audio, Speech,
and Language Processing (TASLP), vol. 31, pp. 1893–
1901, 2023.
[5] W. Choi, M. Kim, J. Chung, and S. Jung, “LaSAFT: La-
tent source attentive frequency transformation for condi-
tioned source separation,” in IEEE International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP), 2021.
[6] N. Takahashi and Y. Mitsufuji, “D3net: Densely con-
nected multidilated densenet for music source separa-
tion,” arXiv preprint arXiv:2010.01733, 2020.
[7] D. Stoller, S. Ewert, and S. Dixon, “Wave-U-Net: A
multi-scale neural network for end-to-end audio source
separation,” in International Society for Music Informa-
tion Retrieval Conference (ISMIR), 2018.
[8] A. D´efossez,
“Hybrid spectrogram and waveform
source separation,” in Proceedings of the MDX Work-
shop, 2021.
[9] S. Rouard, F. Massa, and A. D´efossez, “Hybrid trans-
formers for music source separation,” in IEEE Interna-
tional Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP), 2023.
[10] W. Choi, M. Kim, J. Chung, D. Lee, and S. Jung, “In-
vestigating U-nets with various intermediate blocks for
spectrogram-based singing voice separation,” in Inter-
national Society for Music Information Retrieval Con-
ference (ISMIR), 2020.
[11] X. Ni and J. Ren,
“FC-U2-Net: A novel deep neu-
ral network for singing voice separation,” IEEE/ACM
Transactions on Audio, Speech, and Language Process-
ing (TASLP), vol. 30, pp. 489–494, 2022.
[12] Y. Luo and N. Mesgarani,
“Conv-TasNet: Surpass-
ing ideal time–frequency magnitude masking for speech
separation,” IEEE/ACM Transactions on Audio, Speech,
and Language Processing (TASLP), vol. 27, no. 8, pp.
1256–1266, 2019.
[13] Y. Hu, Y. Chen, W. Yang, L. He, and H. Huang, “Hierar-
chic temporal convolutional network with cross-domain
encoder for music source separation,” IEEE Signal Pro-
cessing Letters, vol. 29, pp. 1517–1521, 2022.
[14] D. Samuel, A. Ganeshan, and J. Naradowsky, “Meta-
learning extractors for music source separation,”
in
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2020.
[15] A. Pandey and D. Wang, “TCNN: Temporal convolu-
tional neural network for real-time speech enhancement
in the time domain,” in IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP),
2019.
[16] Z. Rafii, A. Liutkus, F.-R. St¨oter, S. I. Mimilakis,
and R. Bittner,
“MUSDB18-HQ-an uncompressed
version of MUSDB18,” https://doi.org/10.
5281/zenodo.3338373.
[17] D. P. Kingma and J. Ba, “Adam: A method for stochastic
optimization,” arXiv preprint arXiv:1412.6980, 2014.
[18] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey,
“SDR–half-baked or well done?,” in IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019.
[19] E. Cano, D. FitzGerald, and K. Brandenburg, “Evalu-
ation of quality of sound source separation algorithms:
human perception vs quantitative metrics,” in European
Signal Processing Conference (EUSIPCO), 2016.
[20] B. Series,
“Method for the subjective assessment of
intermediate quality level of audio systems,” Interna-
tional Telecommunication Union Radiocommunication
Assembly, 2014.
[21] S. Ferrari and F. Cribari-Neto, “Beta regression for mod-
elling rates and proportions,” Journal of applied statis-
tics, vol. 31, no. 7, pp. 799–815, 2004.
[22] M. Smithson and J. Verkuilen,
“A better lemon
squeezer?
maximum-likelihood regression with beta-
distributed dependent variables.,” Psychological meth-
ods, vol. 11, no. 1, pp. 54, 2006.
[23] A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson,
A. Vehtari, and D. B. Rubin, Bayesian data analysis,
Chapman and Hall/CRC, 2015.
5
"
"We report improvements to the NeurIPS 2023 HomeRobot: Open Vocabulary Mobile Manipulation (OVMM) Challenge reinforcement learning baseline. In particular, we propose a more accurate semantic segmentation module, along with an enhanced place skill policy, and a high-level heuristic that outperforms the baseline by 2.4% in overall success rate (sevenfold improvement) and 8.2% in partial success rate (1.75 times improvement) on the Test Standard split of the challenge dataset. With these enhancements incorporated, our agent secured 3rd place in the challenge on both simulation and real-world stages.","HomeRobot Open Vocabulary Mobile Manipulation (OVMM) Challenge 2023 Participant Report TEAM KUZHUM Volodymyr Kuzma, Vladyslav Humennyy, Ruslan Partsey Ukrainian Catholic University {volodymyr.kuzma, vladyslav.humennyi, partsey}@ucu.edu.ua ABSTRACT We report an improvements to NeurIPS 2023 HomeRobot: Open Vocabulary Mobile Manipulation (OVMM) Challenge reinforcement learning baseline. More specifically, we propose more accurate semantic segmentation module, along with better place skill policy, and high-level heuristic that outperforms the baseline by 2.4% of overall success rate (sevenfold improvement) and 8.2% of partial success rate (1.75 times improvement) on Test Standard split of the challenge dataset. With aforementioned enhancements incorporated our agent scored 3rd place in the challenge on both simulation and real-world stages. Keywords OVMM Challenge, YOLOv8, MobileSAM, Detic, Semantic Segmentation.","Yenamandra et al. revealed that open-vocabulary mobile manipulation task (fundamental for embodied agents) was not standardized and described, making reproduction, comparison, and, therefore, improvement of existing methods nearly impossible. According to their study, current heuristic (composed of heuristic planners) and reinforcement learning (composed of low-level RL-trained skills) approaches could achieve at most 11.6% success rate with ground truth semantic segmentation and only at most 0.8% success rate with open-vocabulary segmentation mask computed online by Detic. These findings show that OVMM task is still an open frontier for the Embodied AI (EAI) research.nan","In this work, we report changes of the RL baseline proposed in the HomeRobot paper, improving semantic understanding of the agent by utilizing retrained YOLOv8 object detection model and MobileSAM segmentation model. We also analyze the performance of baseline’s place skill to improve training reward function and retrain the corresponding policy. Our agent reached the final real-world evaluation stage and was ranked third on NeurIPS 2023 HomeRobot: OVMM Challenge.","Overall, our results showcase the effectiveness of our proposed improvements to the OVMM agent. By integrating these enhancements, we significantly surpassed the baseline’s performance, demonstrating the potential for further advancements in this challenging task. Our agent excelled on the Test Standard split, securing 1st place, and ranked among the top three teams in the Test Challenge split, further validating the robustness and generalizability of our approach.","We have presented improvements to the baseline agent for the OVMM task, achieving remarkable success rate gains. Our agent excelled in the challenge, securing 3rd place in both the simulation and real-world stages. While these advancements are significant, there is still room for further enhancements. Future work may explore object tracking, skill policy improvements, and world representation to tackle the challenges presented by the OVMM task.",HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report (Team KuzHum),"Volodymyr Kuzma, Vladyslav Humennyy, Ruslan Partsey","HOMEROBOT OPEN VOCABULARY MOBILE MANIPULATION
CHALLENGE 2023 PARTICIPANT REPORT
TEAM KUZHUM
Volodymyr Kuzma
Vladyslav Humennyy
Ruslan Partsey
Ukrainian Catholic University
{volodymyr.kuzma, vladyslav.humennyi, partsey}@ucu.edu.ua
ABSTRACT
We report an improvements to NeurIPS 2023 HomeRobot: Open Vocabulary Mobile Manipulation
(OVMM) Challenge reinforcement learning baseline. More specifically, we propose more accurate
semantic segmentation module, along with better place skill policy, and high-level heuristic that
outperforms the baseline by 2.4% of overall success rate (sevenfold improvement) and 8.2% of
partial success rate (1.75 times improvement) on Test Standard split of the challenge dataset. With
aforementioned enhancements incorporated our agent scored 3rd place in the challenge on both
simulation and real-world stages.
Keywords OVMM Challenge, YOLOv8, MobileSAM, Detic, Semantic Segmentation.
1
Introduction
Nowadays, robotics is rapidly developing, making it more and more used across various domains. However, it is still
non-trivial task for robots to navigate and interact in human spaces, such as homes. Therefore, significant resources are
used to improve performance of embodied agents in domestic conditions across wide range of tasks.
1.1
Motivation
Yenamandra et al. [11] revealed that open-vocabulary mobile manipulation task (fundamental for embodied agents) was
not standardized and described, making reproduction, comparison, and, therefore, improvement of existing methods
nearly impossible. According to their study, current heuristic (composed of heuristic planners) and reinforcement
learning (composed of low-level RL-trained skills) approaches could achieve at most 11.6% success rate with ground
truth semantic segmentation and only at most 0.8% success rate with open-vocabulary segmentation mask computed
online by Detic [13]. These findings show that OVMM task is still an open frontier for the Embodied AI (EAI) research.
In this work, we report changes of the RL baseline proposed in the HomeRobot paper [11], improving semantic
understanding of the agent by utilizing retrained YOLOv8 [4] object detection model and MobileSAM [12] segmentation
model. We also analyze the performance of baseline’s place skill to improve training reward function and retrain the
corresponding policy.
Our agent reached the final real-world evaluation stage and was ranked third on NeurIPS 2023 HomeRobot: OVMM
Challenge [10]. Nevertheless, even with all the enhancements our approach is still far from solving the task, achieving
only 2.8% overall success rate on the Test Standard split of the challenge.
1.2
Report Structure
The report is structured as follows: Section 2 gives a detailed description of the OVMM task, Section 3 introduces the
OVMM Challenge, in Section 4 we identify the baseline’s limitations, Section 5 describes proposed agent improvements,
Section 6 includes analysis and training parameters for place skill, Section 7 shows our agent’s results in local
experiments (local simulation in Habitat [7, 8] environment) and on challenge leaderboard, Section 8 lists areas for
possible improvements, and in Section 9, we make concluding remarks.
arXiv:2401.12048v1  [cs.RO]  22 Jan 2024
HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report
TEAM KUZHUM
2
OVMM Task
OVMM is an EAI task in which an agent has to navigate in an unknown environment, find specified object, and move
(pick-navigate-place) this object to a specified target receptacle. The agent has to have the ability to recognize and
handle any objects (even never-before-seen categories during training) described in a text prompt (open-vocabulary
agent). At the same time, such condition is not placed on receptacles: all categories are seen in the training set. That is
an essential task for home robots, as they have to be able to understand and execute any command in the context of a
house.
At the start of an episode, the agent is provided with a prompt in the following form: ""Move (object) from the
(start_receptacle) to the (goal_receptacle)."" Object is a movable entity that can be grasped (see Figure 1). Recepta-
cles ({start, goal}_receptacle) are different types of furniture that have surfaces suitable for placement. Receptacles
can vary in height, size, and the presence of additional non-surface parts (e.g., the back of a chair). Besides, there may
be several instances of the objects and receptacles in scene. However, it is important that the object is picked from the
specified start_receptacle and placed on any goal_receptacle.
The task is considered successful if any object instance is picked up from any start_receptacle instance and then
placed on any goal_receptacle instance. Besides contact with the receptacle, object must have a stable position after
placement.
For evaluation and execution purposes, the task is divided into four distinct subtasks, which are considered for the
agent’s partial success:
1. Navigation to object
2. Pick (gaze)
3. Navigation to receptacle
4. Place
These subtasks are executed sequentially, and successful completion of one requires the completion of all preceding
subtasks. The success of the final subtask, Place, counts as the overall success of the task.
2.1
Navigation to object
Navigation to object is the initial subtask, which starts navigation within the environment and aims to find and
reach the object placed on start_receptacle. The agent searches for the object by executing navigation actions (e.g.
go forward, turn left/right in case of discrete action space). To successfully complete this subtask, the agent must come
close to object and keep it in sight.
2.2
Pick (Gaze)
The pick subtask is different in simulation and real-world. In the simulation, the task is limited to gazing at the
goal object without the requirement to manually pick it up. This implementation is due to limitations in recreating
manipulator-object interactions within the simulation environment. Therefore, in the simulation, agent only has to gaze
directly at the object and stand in close proximity to initiate the Snap action, which then teleports the object into the
robot manipulator. However, in real-world scenarios, this part of the task is executed entirely through the physical
picking of the object using the robot manipulator.
2.3
Navigation to receptacle
After picking up the object, the agent must find any goal_receptacle in the environment and approach it. Success is
counted only if the agent is close to the goal_receptacle instances.
2.4
Place
In the final stage of the task, the agent must place the object on the goal_receptacle. To achieve this, the agent can use
both manipulation and navigation skills, which add degrees of freedom (DoF) to the final part of the task. Additionally,
successful placement requires that the object is in a stable position on the goal_receptacle. In the simulation, this is
determined by examining the object’s speed; if it does not exceed a certain threshold, the object is considered to be in
a stable position.
2
HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report
TEAM KUZHUM
3
OVMM Challenge
The challenge [10] was divided into two phase: the virtual and the real-world. Based on the results of the first part,
top-3 teams were selected to proceeded to the final stage of the competition, at which the organizers tested the agents
using real-world facilities.
The final results of the challenge were determined based on the real-world phase that the organizers conducted.
3.1
Virtual Phase
Figure 1: Example of goal object (hat) on
start receptacle (cabinet).
The virtual phase of challenge was run from 1 July 2023 to 20 October
2023. During that time, participants were developing their agents in the
virtual environment Habitat [7, 8], in which OVMM task and basic robot
logic were implemented. The scenes were used from Habitat Synthetic
Scene Dataset [6].
Using those instruments, participants had to solve OVMM tasks described
in detail in Section 2. Environment action space could be divided into
two parts: navigation and manipulation. Navigation skills (navigation to
object, navigation to receptacle)1 were those that let you move in the
environment effectively. They were discrete, with three primary actions
- go forward, turn left, and turn right. On the other hand, manipulation
skills (pick, place) gave access to both the robot arm and stand, which was
helpful for accurate interactions with objects for picking or placing. This
part of the task used continuous action space, which let an agent make even
the slightest move to correct its base/arm positions.
Agents were evaluated on EvalAI [9], a platform designed for evaluating
and comparing ML models. Participants submitted their Docker containers
with executable agents to the platform, where they got scores on certain
dataset splits. Results at Test Challenge Split were used to determine top-3
teams, which then proceeded to the real-world phase.
3.2
Real-World Phase
During real-world phase, a dedicated scene was constructed. OVMM task
was carried out there with use of Hello Robot Stretch robot [5].
4
Exploratory Analysis
Our initial step was the analysis of NeurIPS 2023 HomeRobot: Open Vocabulary Mobile Manipulation Challenge [10],
as we were looking for more information about current agent, environment and problems that we could encounter
during the challenge. We identified two main components that had significant impact on the agent’s success: perception
and place skill. Thus, we decided to focus on improving these two components.
4.1
Perception Impact
For both heuristic and RL agent types, perception played a crucial role in algorithm performance, substantially
influencing all subtasks and overall success. All algorithms employed semantic masks to recognize the required objects
and receptacles within the environment. These masks were obtained using perception modules, and the authors of the
paper [11] compared two methods for computing them: ground truth and Detic (see Table 1).
Direct comparison revealed a stark contrast: transitioning to the Detic perception module resulted in a four-fold decrease
in partial success rate and a ten-fold decrease in overall success rate. This led to the conclusion that the initial agent
had significant potential for improvement through a perception upgrade alone. Moreover, this improvement positively
influenced all of the agent’s skills and ultimately had a greater impact on the overall task performance than upgrading a
single skill.
1Skills are separate policies that were meant to be used by agent for certain subtask. More about them in Subsection 5.1
3
HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report
TEAM KUZHUM
Table 1: Comparison Table for Ground Truth and Detic RL Agent (Absolute Percent)[11]
Perception
Partial Success Rates
Overall
Partial
FindObj2
Pick
FindRec
Success Rate
Success Metric
Ground Truth
55.7
50.2
35.2
11.6
38.2
Detic
19.8
11.8
6.3
0.8
9.7
4.2
Place Skill Bottleneck
Figure 2: Skill relative success rates.
Beyond the overall impact of perception on the entire task,
we also investigated the changes in performance for indi-
vidual skills with different perception modules (Figure 2).
The place skill stood out as the weakest performer, with
the lowest relative success rate under both perception con-
ditions (33.0% and 12.7%). Additionally, it, along with
the navigation to object skill, experienced a significant
drop in success (approximately 3 times) when shifting
from ground truth to Detic (for navigation to object suc-
cess dropped from 55.7% to 19.8%). This decrease was
probably caused by the problems Detic faces in recogniz-
ing small objects. Notably, the navigation to receptacle
skill exhibited the smallest relative drop (from 70.1% to
53.4%), suggesting that recognizing large furniture was
not a serious hurdle for Detic.
In contrast, the root cause of the drop in place task
success is not obvious. Thus, we decided to examine it
more closely during our experiments.
5
Our agent
5.1
High-level Heuristic
Start
Navigate to
object
Gaze At Object
and Pick
Navigate to
receptacle
Yes
Place
Was pick
successful?
End
No
Figure 3: Original high-level policy.
RL baseline, on which our approach was based, is divided into four skills that correspond to the separate parts of the
task: a skill for navigation to object; a skill for pick that is called gaze and actually performs only the preparation for
pick, as in simulation the process of grasping is automatic; a skill for navigation to receptacle, and one for placing
of the object. To control and switch skills a high-level policy is used. In case of the baseline the skills are simply
called one by one in a row: when previous skill calls Stop action, it is terminated and switched to next one. As the next
phases can not be successful unless all previous ones are, it is useful to check previous one’s success. For navigation
skills (navigation to object, navigation to receptacle) and place skill, it is a non-trivial task to distinguish successful
performance from the unsuccessful one. On the other hand, for pick skill there is an accessible sensor measurement
that directly shows whether the item was actually picked. Considering this, we added a conditional loop over first two
skills that was performed until the pick was successful.3
2Find object is named navigation to object in our work. Similarly, find receptacle is named navigation to
receptacle.
3The navigation to object skill was also inside the loop because of the constant wandering far from the object during unsuccessful
pick attempts, making restart of only Gaze skill mostly fruitless.
4
HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report
TEAM KUZHUM
ht
h t-1
RNN
Other sensor
measurements
Concat
FC
at
ot
CNN
Figure 4: Architecture of a low-level policy.
The change of high-level heuristic is better described in Figure 3.
5.2
Low-Level Policies
In the baseline, low-level policies, responsible for all skills, are neural networks where RGB, depth, and semantic
segmentation mask are passed through a CNN (ResNet [2] in our case). Then they are concatenated with features
obtained from other measurements (like GPS and Compass) passed through fully connected layers, and put into an
LSTM [3] block, along with previous step state ht−1 to get new state ht and distribution, from which action will be
sampled (discrete if skill is navigational and continuous if skill requires precise movements like gaze or place). The
architecture is shown in Figure 4.
We did not change the architecture of the policies, working only on retraining of the existing ones (more in section 6).
5.3
Perception Module
As shown in Subsection 4.1, the baseline, trained with ground truth semantic segmentation, shows lower performance
when computing segmentation mask online (no ground truth). Our approach enhanced existing Detic [13] perception,
utilizing MobileSAM [12] model for segmentation, prompted with bounding boxes from YOLOv8 [4] detection model,
retrained on a dataset of 50000 images from scenes with objects and receptacles on them. Obtained segmentation
masks for each class were overlaid, putting the task-important objects (goal object, start receptacle, and end receptacle
classes) on top of the resulting mask. Detic’s segmentation mask was merged with the previous one in places where
YOLO-SAM module failed to detect any objects, and for regions where Detic detected goal object. This way, the
final semantic segmentation module preserved strengths of the open-vocabulary Detic while adding more task-specific
YOLO-SAM, good at distinguishing furniture types and small objects. Perception module scheme is visualized in
Figure 5.
6
Place Improvement
6.1
Baseline Performance Analysis
As concluded in subsection 4.2, place skill significantly impacts overall success rate of the baseline (also fail in place
results in 0 overall success even though all previous skills succeded). To better understand the reasons behind some
Overlay
masks
YOLO
DETIC
MbSAM
RGB image
Final
Segmentation
bboxes
detect
segment
Figure 5: Semantic segmentation pipeline.
5
HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report
TEAM KUZHUM
failed episodes, we analyzed the agent’s performance in 32 cases where the final phase was reached. Here are all the
observed causes of failure:
• unstable place - 25.0%;
• missed receptacle - 21.9%;
• not annotated receptacle - 15.6%;
• camera overlap with manipulator - 15.6%;
• receptacle misdetection - 9.4%;
• did not start place skill - 6.3%;
• uncertain - 6.3%.
Though we were not able to address all of these cases in our work, we hope that this analysis can help future efforts
to improve the baseline. At the same time, other failures could be resolved via adjustment of training reward of the
skill (by penalizing the actions that lead to the errors or remaking the sparse reward into continuous for faster learning).
Therefore, we started experimenting with reward changes.
6.2
Reward Engineering
The original baseline’s place skill was trained on a sparse reward: the agent got +5 for drop on the receptacle surface
when object came into contact with it and +1 for each timestep when the object remains on the surface. A penalty of -1
was given when the object was dropped and did not come into contact with the surface [11]. Analyzing data from the
previous part, we highlighted important features to make reward better:
• strictly penalizes wandering very far from the starting position to avoid movement away from the goal (common
due to noisy semantic segmentation), as it is known that the goal should be close to the agent at the beginning;
• rewards approaching the goal as close as possible;
• rewards keeping the goal object in sight;
• penalizes blocking camera view with manipulator;
• penalizes waiting when in the final position to place (very close to the object and looking at it).
To implement the features we added continuous fixed4 rewards for approaching the goal and keeping it in the camera
view. For penalties, we used constant reward reductions when reaching certain thresholds each timestep (for example,
-3 each step when farther than 1.5m from start).
Final training reward parameters were:
• reward for place when object came into contact with surface is 70;
• reward for object being in contact with surface each step (per step) is 25;
• total distance reward is 40;
• minimal distance to goal threshold (when reward for distance reduction is no more given) is 0.2m;
• total view reward is 30;
• goal visibility threshold (after this threshold reward for bigger percentage of goal receptacle in screen space is
no more given) is 30% ;
• camera blocking penalty (per step) is -5;
• wandering far from start penalty (per step) is -5.
After selecting reward and threshold values, we performed final training of place skill, starting from the weights of
original baseline.
4This means that for each episode there is the same total possible reward for these actions achieved when agent reaches some
threshold of distance or presence of goal in camera (measured as percentage of pixels belonging to goal)
6
HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report
TEAM KUZHUM
6.3
Training Details
We performed experiments on two machines: first setup with 2 RTX3090 GPUs with 24Gb memory was used for
parameter adjustment, second machine with 4 A6000 GPUs with 48Gb memory was used for final training. We ran 32
environments per machine. The learning rate was set to 10−7 for better adaptation of original weights. All the other
parameters remained the same as in original implementation.
7
Results
7.1
Our Results
After implementation of each feature in our agent, we evaluated it on given validation split to gain information about
improvements of agent at task. They are showed in Table 2.
Table 2: Skill and Task Success Rate.
Perception
Skill relative success rate
Overall
Success Rate
Partial
Success Rate
NavToObj
Pick
NavToRec
Place
Detic Baseline
19.8
59.6
53.3
12.5
0.8
9.7
YOLO-SAM Baseline
22.75
57.1
48.1
0
0 (-0.8)
10.5 (+0.8)
YOLO-SAM Baseline + Detic
38.4
85.4
77.1
15.8
4 (+3.2)
25.1 (+15.4)
YOLO-SAM Fine-tuned + Detic
40.2
84.2
71
21.6
5.2 (+4.4)
25.8 (+16.1)
While introducing pure YOLO-SAM perception did not lead to an overall success rate increase, it did provide a boost in
partial success rate. However, the most significant improvements came with the combined use of YOLO-SAM and
Detic perception. In comparison to pure Detic, overall success rate improved fivefold, while the partial success rate
improved 2.5 times. We can see that even when the relative success rate of the place skill was almost the same, the
overall success rate rose significantly, as we gained a lot in all of the previous tasks (see Figure 2), resulting in such a
final result.
With the introduction of a new place checkpoint, we improved overall success rate, which resulted in a 1.2% increase
from our previous result.
7.2
EvalAI Leaderboard
In addition to evaluation result on our local machines we evaluated our agent on EvalAI [9], where competition was
hosted. There were two important splits: Test Standard and Test Challenge. First one was open and visible for all
participants, while the second one was closed and was used to determine top-3 teams of the challenge.
As of January 23, 2024, we are ranked first at Test Standard split, with 0.8 and 0.5 percents lead at overall success and
partial success respectively (see Table 3).
Table 3: EvalAI Challenge Test Standard Results
Rank
Participant Team
Overall Success
Partial Success
1
KuzHum
0.028
0.191
2
UniTeam
0.020
0.186
3
PieSquare
0.020
0.111
4
VCEI
0.012
0.123
5
PoorStandard (DJI)
0.008
0.110
6
scale_robotics
0.004
0.107
In the Test Challenge split, we got into top-3 teams, but without any specific information about our result at that split.
7.3
Sim-to-Real Transfer
A very important part of the challenge [10] was evaluation of robot in real world. We were curious about performance
of our agent outside the simulation (similarly to the work done by Anderson, et al. [1]). As we lacked any physical robot
with manipulators, we decided to perform test of our perception module, that we were working on. Thus, we tested
how perception, trained in simulations at virtual assets, performed in real world. After creation of several segmentation
7
HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report
TEAM KUZHUM
videos of both indoor and outdoors receptacles we saw, that perception module easily recognizes receptacles as chairs,
tables (see Figure 6). At similar courses with different number of objects and different illumination, perception gave
similar results.
8
Future Work
As task of OVMM is far from being ‘solved’, we share our insights and ideas for future work below.
8.1
Object Tracker
During navigation and manipulations, the perception module calls inference at each frame without including infor-
mation from previous steps. Thus, during inspection of the environment, we have seen that on consecutive frames
semantic masks of the same object may disappear (which does not happen during training with ground truth semantic
segmentation). This leads to unpredictable behavior of the agent, which can get lost in the environment.
To prevent this problem, we can implement an object tracker into the perception module. It would use information from
both the current and previous frames, which would prevent disappearing of an object from frame to frame. As the agent
is trained with use of ground truth, which has correct segmentation at all frames, we suggest that this may improve its
behavior, as with better perception evaluation would be more similar to training conditions.
8.2
Policy and Skills
Besides technical improvements to the agent’s perception modules, its performance also relies on its logic and actions.
Therefore, improving the agent’s policies to get more efficient behavior throughout the episode is crucial. We have
outlined two main components in this part of the agent: a high-level policy which controls calls to skills, and each
separate skill that performs a specific part of the task.
8.2.1
High-Level Policy
Throughout the entire episode, the agent is controlled by a high-level policy. This policy receives information about the
agent’s state in the environment, including its position and interactions with goal objects. Based on this information, the
policy chooses which skill to execute at the moment.
The current version of the policy is quite simple: it sequentially calls skills in their logical order. Additionally, during
our development of the agent, we also implemented an improvement to the gaze skill (see Figure 3). If the agent does
not successfully complete this subtask (does not have an object in its arm), the skill is executed repeatedly. Even this
basic policy change improved our success rates. However, this policy remains fairly simple and lacks flexibility in
scenarios with unsuccessful subtasks.
Therefore, we can improve our agent by implementing better skill calls, making it more robust to failures and incomplete
subtasks. A promising solution would be an NN-based policy that processes all input data from all sensors to determine
Figure 6: Real-life perception results.
8
HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report
TEAM KUZHUM
which skill to execute at the moment or whether a skill has been successfully completed, remains unfinished, or has
failed.
Furthermore, our participation in the final challenge phase, where the top three teams presented their solutions, revealed
that the top two teams both utilized heuristic agents with significantly modified main policies, resulting in substantial
performance improvements. This provides important additional insight and suggests that there is considerable room for
improvement in this agent component.
8.2.2
Skill Training/Fine-Tuning
In addition to enhancing skill calls through a more robust high-level policy, there is a need to improve each individual
skill. For the majority of the challenge, we relied on baseline checkpoints (v1) provided within the initial HomeR-
obot repository. By leveraging additional resources, we successfully fine-tuned the place skill, resulting in overall
performance improvements as previously demonstrated. Furthermore, after the challenge was over we tested new skill
checkpoints (v2) that had been added to the repository. Incorporating these checkpoints into our agent and utilizing
Ground-Truth segmentation led to an over 3% improvement in overall success compared to previous results. However,
we believe this is not the limit, and further advancements can be made in each skill.
8.3
World Representation
One of the key features of the OVMM task is that the environment remains the same across all subtasks. While this
characteristic may not be crucial for tasks like pick/gaze and place, focusing solely on the object and receptacle
within immediate sight, it becomes significantly relevant for navigation subtasks. During navigation to object,
the lack of pre-existing data necessitates exploration of the environment. However, when navigating to the target
receptacle, this exploration can be potentially enhanced by transferring or storing environment-related information
obtained during first navigation skill.
One potential solution to address this challenge is employing a world representation of the environment. By remembering
the locations of various objects, receptacles and the optimal paths to reach them, we can optimize exploration and
enhance the agent’s action selection. Additionally, dedicated data storage removes concerns about forgetting previously
gathered information stored within RNN layers. Without such a world representation, the agent risks simply erasing
previously encountered objects from its LSTM memory, leading to excessive and unnecessary environment exploration.
9
Conclusions
We reported improvements of RL-based baseline for OVMM task. Our agent utilized improved perception, place skill,
and high-level policy to obtain 5 times higher overall success rate (0.8% to 5.2%) and 2.5 times higher partial success
rate (9.7% to 25.7%) on validation split, and 7 times higher overall success rate (0.4% to 2.8%) and 1.75 times higher
partial success rate (10.9% to 19.1%) on Test Standard split of OVMM challenge dataset, scoring 3rd place in the
simulation stage, and later in the real-world stage.
While achieving quite high ranking place, our approach does not solve the stated task, leaving much space for further
improvements. Further work in the domain of semantic segmentation is necessary for achieving results in numerous
tasks in EAI. Hopefully, the ideas presented in this report can build a foundation for future solution of the OVMM
problem and similar problems in the field.
References
[1] P. Anderson, A. Shrivastava, J. Truong, A. Majumdar, D. Parikh, D. Batra, and S. Lee. Sim-to-real transfer
for vision-and-language navigation. In J. Kober, F. Ramos, and C. Tomlin, editors, Proceedings of the 2020
Conference on Robot Learning, volume 155 of Proceedings of Machine Learning Research, pages 671–681.
PMLR, 16–18 Nov 2021.
[2] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition, 2015.
[3] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
[4] G. Jocher, A. Chaurasia, and J. Qiu. Ultralytics yolov8, 2023.
[5] C. C. Kemp, A. Edsinger, H. M. Clever, and B. Matulevich. The design of stretch: A compact, lightweight mobile
manipulator for indoor human environments, 2022.
9
HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report
TEAM KUZHUM
[6] M. Khanna, Y. Mao, H. Jiang, S. Haresh, B. Shacklett, D. Batra, A. Clegg, E. Undersander, A. X. Chang, and
M. Savva. Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for
objectgoal navigation, 2023.
[7] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh,
and D. Batra. Habitat: A platform for embodied ai research, 2019.
[8] A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. Turner, N. Maestre, M. Mukadam, D. Chaplot,
O. Maksymets, A. Gokaslan, V. Vondrus, S. Dharur, F. Meier, W. Galuba, A. Chang, Z. Kira, V. Koltun, J. Malik,
M. Savva, and D. Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In Advances in Neural
Information Processing Systems (NeurIPS), 2021.
[9] D. Yadav, R. Jain, H. Agrawal, P. Chattopadhyay, T. Singh, A. Jain, S. B. Singh, S. Lee, and D. Batra. Evalai:
Towards better evaluation systems for ai agents, 2019.
[10] S. Yenamandra, A. Ramachandran, M. Khanna, K. Yadav, D. S. Chaplot, G. Chhablani, A. Clegg, T. Gervet,
V. Jain, R. Partsey, R. Ramrakhya, A. Szot, T.-Y. Yang, A. Edsinger, C. Kemp, B. Shah, Z. Kira, D. Batra,
R. Mottaghi, Y. Bisk, and C. Paxton. The homerobot open vocab mobile manipulation challenge. In Thirty-seventh
Conference on Neural Information Processing Systems: Competition Track, 2023.
[11] S. Yenamandra, A. Ramachandran, K. Yadav, A. Wang, M. Khanna, T. Gervet, T.-Y. Yang, V. Jain, A. W. Clegg,
J. Turner, Z. Kira, M. Savva, A. Chang, D. S. Chaplot, D. Batra, R. Mottaghi, Y. Bisk, and C. Paxton. Homerobot:
Open vocabulary mobile manipulation, 2023.
[12] C. Zhang, D. Han, Y. Qiao, J. U. Kim, S.-H. Bae, S. Lee, and C. S. Hong. Faster segment anything: Towards
lightweight sam for mobile applications. arXiv preprint arXiv:2306.14289, 2023.
[13] X. Zhou, R. Girdhar, A. Joulin, P. Krähenbühl, and I. Misra. Detecting twenty-thousand classes using image-level
supervision, 2022.
10
"
"This study introduces a novel task called character-aware audio-visual subtitling, where the goal is the generation of transcripts, timestamps, and identification of the speaker for each speech segment using audio-visual cues. An automatic pipeline for the task is developed to address challenges such as the detection of faces or tracking. A dataset comprising three TV series with character-labeled subtitles is prepared for evaluation. Results on the dataset show promising overall performance, demonstrating the potential of the proposed approach for improving accessibility and aiding further research in video understanding.","The increased accessibility of video content online and the reliance on subtitles by viewers highlight the importance of automatic subtitle generation and captioning. However, most subtitles lack speaker identification and sound effects, falling short of meeting the standards for Subtitles for Deaf and Hard-of-hearing. This work seeks to advance the generation of subtitles by making them character-aware, offering benefits to both users and researchers.","nanNumerous studies have focused on audio-visual networks for speech recognition, speaker diarisation, and character recognition, but they require additional processing for face detection and tracking. A simpler method is presented here that does not require these processes and relies solely on off-the-shelf neural network models and cast lists. Previous works in labeling people in videos often demand ancillary information, such as scripts, clean actor images, or accurate subtitles, which are not always readily available. Audio-only speaker diarisation methods suffer from performance degradation when dealing with a large number of speakers, as is often the case in TV shows or dramas. Audio-visual speaker diarisation approaches have been explored but typically utilize face recognition or lipsync models. This work introduces character-aware audio-visual subtitling, leveraging audio-visual cues to build a character bank and assign identities to speech segments, without the need for face detection or tracking.","The proposed approach consists of two stages. In stage 1, speech segments are detected, transcribed, and processed to construct a database of speech exemplars - video clips where a speaker is clearly visible, audible, and identifiable. Stage 2 utilizes the speech exemplars from stage 1 to assign identities to all speech segments. To label the characters, the following metadata for each episode is required: (i) names of the characters in the show; and (ii) 1-10 sample images of the actors and their names for use as visual examples. The specific steps involved in stage 1 and 2 are detailed in the paper.","The evaluation of the method is conducted on a semi-automatically annotated dataset consisting of three TV series episodes. Stage 1 yield and classification accuracy of the speech exemplars are assessed, demonstrating high accuracy in speaker recognition. Stage 2 performance is analyzed by varying the threshold for classifying speech segments as unknown, revealing a trade-off between the Proportion of Classified Segments (POCS) and overall precision. The overall performance on the test set shows promising results, with the model achieving the best performance on Frasier and the lowest on Scrubs due to the difference in cast size. Diarisation Error Rate (DER) and character recognition accuracy are reported, along with per-character precision and recall metrics. Word Error Rate (WER) is also computed to assess the speech transcription performance, demonstrating the outperformance of the WhisperX model over other state-of-the-art models due to reduced hallucination.","This work takes the initial steps towards a model for character-aware subtitling, demonstrating promising results. The paper acknowledges limitations in the model's ability to handle short segments, overlapping speech, and the classification of sounds other than speech. Future work will address these limitations to generate comprehensive subtitles that meet the standards for Subtitles for Deaf and Hard-of-hearing.","Look, Listen and Recognise: Character-Aware Audio-Visual Subtitling","Bruno Korbar, Jaesung Huh, Andrew Zisserman","LOOK, LISTEN AND RECOGNISE: CHARACTER-AWARE AUDIO-VISUAL SUBTITLING
Bruno Korbar∗
Jaesung Huh∗
Andrew Zisserman
Visual Geometry Group, Department of Engineering Science, University of Oxford, UK
Fig. 1: Character-aware audio-visual subtitling. The generated data covers what is said, when it said, and by whom it is said.
ABSTRACT
The goal of this paper is automatic character-aware subtitle genera-
tion. Given a video and a minimal amount of metadata, we propose
an audio-visual method that generates a full transcript of the dia-
logue, with precise speech timestamps, and the character speaking
identified. The key idea is to first use audio-visual cues to select a
set of high-precision audio exemplars for each character, and then
use these exemplars to classify all speech segments by speaker iden-
tity. Notably, the method does not require face detection or track-
ing. We evaluate the method over a variety of TV sitcoms, including
Seinfeld, Fraiser and Scrubs. We envision this system being use-
ful for the automatic generation of subtitles to improve the accessi-
bility of the vast amount of videos available on modern streaming
services. Project page : https://www.robots.ox.ac.uk/
˜vgg/research/look-listen-recognise/
Index Terms— character-aware subtitling, audio-visual speaker
diarisation, speech recognition, video understanding
1. INTRODUCTION
With the rise of streaming platforms that allow watching videos “on-
demand”, more video content is made available to the general public
and researchers than ever in history. With more than 80% of users
of one such platform relying on subtitles [1], automatic subtitle gen-
eration and captioning has become an important research topic in
the community [2, 3]. Unfortunately, many subtitles, whether au-
tomatically generated or not, do not comply with the standards for
Subtitles for Deaf and Hard-of-hearing (SDH): namely, they do not
∗ These authors contributed equally to this work. This research was funded
by EPSRC Programme Grant VisualAI EP/T028572/1, and a Royal Society
Research Professorship.
include information about speaker identification, nor do they contain
sound effects and music.
In this paper, we take the next step towards automatic generation
of SDH – we aim to make the subtitles character-aware. Character-
aware subtitles would also be of great benefit to researchers. They
would allow for the automatic generation of large-scale video
datasets, which could fuel the next generation of visual-language
models capable of learning higher-level semantics from the paired
data.
There has been a plethora of works using audio-visual networks for
speech recognition [4, 5], speaker diarisation [6, 7, 8] or character
recognition [9, 10, 11, 12] which are subtasks of our main goal.
However, these works require additional processing for detecting
and tracking faces. We present a simpler method that does not re-
quire face detection or tracking and uses only off-the-shelf deep neu-
ral network models and the cast list for each episode.
We make the following four contributions: (i) we propose a new
task, character-aware audio-visual subtitling, which aims to generate
the what, when and by whom for subtitles, with minimal required
metadata. (ii) we develop an automatic pipeline for this task that
does not require face detection or tracking (Section 2); (iii) we curate
an evaluation dataset that includes subtitles labelled with characters
individually for three different sitcom series: Fraiser, Scrubs and
Seinfeld (Section 3); and (iv) we assess the method on the evaluation
dataset and report the performance (Section 4).
1.1. Related work
Labelling people in videos. is a well studied topic in computer vi-
sion [10, 11, 12]. Often, the availability of various levels of prior
information is required such as scripts [10], clean images for actor-
level supervision [12], or ground truth subtitles with correct times-
tamps [13, 14]. [15] relaxes the need for cleaned data and makes
arXiv:2401.12039v1  [cs.CV]  22 Jan 2024
VAD detection 
and ASR
Audio-visual 
speaker detection
Elaine
Visual character
classification
×
Stage 1: Building audio exemplars
Stage 2: Assigning identities to speech segments
Nearest centroid classification
Jerry
Elaine
George
exemplars
centroids
query
Jerry
Elaine
unknown
George
Fig. 2: Overview of our method. We first build a database of au-
dio exemplars for each character by filtering speech segments until
only a high precision set remains (left). Each speech segment is then
assigned to a character by comparing its voice embedding to the ex-
emplar embeddings (right).
their method scalable by gathering a large amount of data via au-
tomated image search to obtain the corroborative evidence they use
for supervision. Like [15], our model retrieves the necessary infor-
mation via search engines, however, it does not pre-process video
frames, save for the transformations required by a neural network.
Audio-only speaker diarisation. Speaker diarisation is the task of
identifying “who spoke when” from a given audio file with human
speech. There are two branches of works in this area: (i) using ex-
isting Voice Activity Detection (VAD) and a speaker model together
with clustering [16, 17, 18] and (ii) using an end-to-end model which
goes from the VAD to assigning speakers [19, 20]. Both of them
suffer when the number of speakers is large such as in TV shows
or dramas. Furthermore, the current state-of-the-art speaker recog-
nition models assume that the input is long (> 2 sec), while most
of the speeches in TV shows are relatively short including exclama-
tions, which leads to the degradation of speaker clustering perfor-
mance. In this paper, we include the active speaker detection model
and person-identification model, which are strong in short videos, to
identify the character.
Audio-visual speaker diarisation.
In the last few years, efforts
were made to improve the performance of diarisation by borrowing
the power of face recognition models or lipsync models, which are
closely related to human speech [6, 8, 21]. [6] utilises audio-visual
active speaker detection model and speech enhancement models, but
mostly in celebrity interviews or news segment where the length of
speeches are generally short. [8] introduces an Audio-Visual Rela-
tion Network (AVR-Net) that leverages the cross-modal correlation
to recognise the speaker’s identity. Our approach is different from
these works in two ways: (i) we do not use any face detection or
tracking; and (ii) we introduce character-aware audio-visual subti-
tling that builds the character bank within each video and figures
out not only the speaker clusters but the speakers’ identity for each
utterances and the speech content.
Datasets. The Bazinga! dataset [22] also provides subtitles labelled
with characters for a large number of TV series. However, it is an
audio only dataset, and consequently is not directly suitable for ap-
plying the audio-visual approach we develop.
2. METHOD
This section explains our approach to creating subtitles for the video
and attributing speakers to each speech segment. Our method con-
sists of two distinct stages. First, we detect speech segments from
the video, recognise the spoken words, and process the data to create
a database of what we refer to as speech exemplars – sample video
clips where a speaker is clearly audible, visible and identifiable. In
the second stage, the speech exemplars for each character are used
to assign the identities to all speech segments.
In order to label the characters we require the following meta-
data for each episode: (i) the names of the characters in the show;
and (ii) for each character 1–10 sample images of the actor and their
names that we can use as visual examples. This metadata can be
obtained automatically from online database of movies or TV se-
ries [23].
2.1. Stage 1: building audio exemplars
The goal of stage 1 is to create a database of character voices. We
take multiple episodes of a TV series, and obtain a set of speech
segments for each character.
In order to do this, we first split videos into speech segments, and
transcribe them. For each segment we determine if only one speaker
is visible and is speaking – a crucial step because it allows us to
be confident that the speech segment corresponds to the face in the
frame. We collect a set of speech segments for each character that we
can confidently recognise from their face, and then filter the samples
in each set to remove potential label noise using voice embeddings.
We end up with a set of speech segments for each character that are
recognised with high precision, and refer to these as speech exem-
plars. The building of these exemplars is illustrated in Figure 2, and
we give details of each sub-step below.
1. VAD detection and Automatic Speech Recognition (ASR). In
this stage, we take an entire video and split it into segments where
speech is detected and recognised. We first detect the voice regions
across the entire dataset and determine the spoken content of each
segment. We do this with a language-guided VAD model. We apply
the WhisperX [3] model on the audio stream of our dataset which
detects the speech regions with word-level timestamps. We concate-
nate the generated words to obtain the entire transcription per video,
then use a sentence tokenizer to separate them by sentences. Assum-
ing each sentence is spoken by a single speaker, we use the start and
end times of the sentences as our unit of speech segments.
We also find that most TV shows contain laughter tracks (audience
laughter) which are voice regions but are not of interest to this work.
Thus, we run a pretrained laughter detector [24] for each of the re-
maining voice segments and remove the ones from the candidates of
exemplars if laughter is detected. After this step, we know precisely
when characters in the show are speaking and what they are saying.
We don’t yet know who is saying what.
2. Audio-visual speaker detection. The goal of this stage is to take
speech segments from the previous stage and select only those with a
single visible speaker. This will produce a subset of speech segments
where we can recognise the speaker. To achieve this, we localise
the speaker with an audio-visual synchronisation model [25] which
produces a spatial location of the audible objects and has been shown
to detect speakers well. In practice, it generates an audio-guided
heatmap over each video frame. We average the heatmaps over the
length of each speech segment to avoid unnecessary noise and detect
peaks in the heatmap through a combination of maximum filtering
and non-maximum suppression. Example heatmap outputs can be
seen in Figure 2. When a single peak is visible throughout the video
clip, we can assume that only one speaker is present. If there are
no detected peaks, or there are multiple ones, we discard that speech
segment from the candidates of exemplars.
3. Visual character classification. In this step a character name is
assigned to each of the single-speaker speech segments from the pre-
vious step where possible. This leaves us with a further reduced set
of speech segments, each having a character name associated with
it. Character classification is the only step in our annotation process
that external data is used. Specifically, the 1–10 sample images of
each actor are used to form a visual embedding of that character. Our
classification model [26] compares a visual embedding of the frames
of a speech segment to a combination of actor visual embedding and
actor name (details are given below). We select the best match or
discard the clips which cannot be classified with a high degree of
confidence. Note, (i) the comparison is at the frame level, no face
detector or cropping is required for this visual recognition; (ii) we
compute visual embeddings for all characters in a given season, but
only consider ones present in that episode at inference time.
4. Audio filtering. Finally, we group the labelled speech segments
from the previous stage by character, and for each character we filter
their voice samples to remove potential noise from the groupings as
follows: we compute voice embeddings for each sample, and con-
sider that a sample is positive for a given character if its 5 nearest
neighbours are labelled as the same character. Note that for char-
acters where the number of samples n is smaller than 5, we keep
all the samples in our database. This gives us the final exemplar set
for a given TV series and hopefully leaves us knowing what each
character sounds like.
2.2. Stage 2: Assigning characters to speech segments
The aim of this stage is to assign a character name to each of the de-
tected audio segments that we are confident of, regardless of whether
a speaker is visible or not. On a high-level, we achieve this by com-
paring the distance between each speech segment and the audio ex-
emplars for each character. We do not assign an identity if the mini-
mum distance is above a certain threshold.
Specifically, for each character we compute the mean of exemplar
embeddings and use it as a centroid representation for that character.
To classify speech segments, we embed them with the same model
used to generate the exemplar embeddings, and measure distances
to class centroids. The segment is assigned to the speaker corre-
sponding to the nearest centroid. However, if the minimum distance
between the segment embedding and each centroid is bigger than a
threshold d, then that segment is classified as “unknown”. This cov-
ers uncertainty and also the cases where we don’t have exemplars.
2.3. Implementation details
We detect speech and perform ASR with an off-the-shelf Whis-
perX [3] model, and the sentences are tokenized with NLTK [27]
tokenizer. We use the laughter detector by [24] with a detection
threshold of 0.8. All voice embeddings are encoded with ECAPA-
TDNN [28], which is pretrained with VoxCeleb [29]. For discovery
of speaking faces, we use a pretrained LWTNet [25]. For each gen-
erated heatmap we detect 4 peaks, and consider each a positive if it’s
larger than τdet = 0.7. For actor face classification, we use the CLIP-
PAD model [26] pretrained on VGGFace and VGGFace2 [30]. Ac-
tor text-image embeddings are formed as ""An image of <TKN>
Name Surname"" where <TKN> is an average representation of
query images computed using a face-embedding network, as in [26].
To classify the actors in the scene, we measure the cosine similarity
between the visual embedding of the frames and the text-image
embedding and choose the ones with highest similarity score where
the score is over threshold τrec = 0.85 as positives.
All hyper-
parameters are determined via grid search on the three validation
episodes, and kept fixed otherwise.
Table 1:
Evaluation dataset statistics.
# episode:
number of
episodes, duration: total duration of the dataset, #IDs: total number
of characters, speech %: percentage of video time that is speech and
# spks: min / mean / max of number of speakers per video.
Dataset
# episode
duration
# IDs
speech %
# spks
Seinfeld
6
2h 09m
36
60.6
6 / 9.2 / 12
Frasier
6
2h 11m
29
59.5
6 / 9.2 / 12
Scrubs
6
2h 02m
48
67.9
13 / 15.7 / 18
3. EVALUATION DATASET
In this section, we describe a semi-automatic annotation pipeline
used to generate the ground truth character names, timestamps and
subtitles for speech segments. The goal is to annotate the identities
for all subtitles with accurate time intervals in the video.
3.1. Annotation procedure
The dataset collection process consists of two stages: (i) automatic
initial annotations by aligning a transcript with timed subtitles; and
(ii) human annotators reviewing and further refining these annota-
tions. Note that our dataset differs from other speaker diarisation
datasets since we are also interested in the identity of each speaker
and speech transcriptions.
Aligning transcripts and timestamps.
To associate character
names with corresponding temporal timestamps, we leverage two
readily accessible source of textual video annotation: original tran-
scripts and subtitles with word-level timestamps. Transcripts are
obtained from multiple online sources [31, 32, 33]. They include
spoken lines and information about who is speaking. However, they
do not provide any timing information beyond the order in which
the lines are spoken.
We use WhisperX [3] to obtain the timed
subtitles. We find this suitable since its transcription and timestamps
are highly accurate, whereas the timestamps in subtitles from other
online sources often do not align with the actual speech in the video.
To align the original transcripts and timed subtitles, we employ the
approach from [34].
We use Dynamic Time Warping (DTW) to
obtain the word-level alignment between the transcript and timed
subtitles to associate the speaker with each of these words. Please
refer to the original paper for the detailed process.
Manual correction. The output of the automatic pipeline is prone
to several errors such as (i) a mismatch between the text of the tran-
script and WhisperX’s transcription results; and (ii) mispredicted
timestamps.
We correct any errors in timestamps and character
names manually using the VIA Video Annotator [35].
3.2. Dataset statistics
Three TV series datasets are used to evaluate our method. We anno-
tate the first six episodes of Season 2 of Frasier, Season 2 of Scrubs
and Season 3 of Seinfeld. We utilise the sixth episode in each season
as our validation set, while the remaining episodes serve as our test
set. The detailed statistics are shown in Table 1.
4. RESULTS
This section provides a detailed analysis of Stage 1 and 2, followed
by the overall result on our test set.
4.1. Detailed analysis of Stage 1 and 2
Performance evaluation of Stage 1. We evaluate the yield and clas-
sification accuracy of the speech exemplars on the five episodes of
Seinfeld in our test set. In Table 2, it can be seen that 19.3% of
voice activity segments can be considered as exemplars. We also
Table 2: Exemplar yield after steps in Stage 1 (on Seinfeld).
Step
# of exemplars
% of total
VAD detection
2107
100.0
Audio-visual speaker detection
1271
60.3
Visual character classification
806
38.3
Audio filtering
407
19.3
Table 3: Exemplar recognition performance for named characters in
Stage 1 in Seinfeld. ‘others’ is a group of 21 characters, all named
correctly.
Char. name
# exemplars
# correct
Acc (%)
Total
407
406
99.8
Jerry
273
272
99.6
Elaine
30
30
100
Kramer
12
12
100
George
14
14
100
others
78
78
100
Fig. 3: Stage 2 Precision-POCS Curves for the test set of the three
TV series, obtained by varying the threshold d (for classification as
“unknown”). The left figure shows the performance using all de-
tected speech segments. The right figure shows the performance
only for the long segments (> 2 sec). We also show the oracle points
(‘x’ in each graph) for each TV series. The oracle point is where all
segments for which there are character exemplars are correctly clas-
sified, and other segments are classified as “unknown”.
evaluate the performance quantitatively by manually inspecting the
exemplars. The results, shown in Table 3, demonstrate that the ac-
curacy of Stage 1 is almost perfect, being 100% correct for most
characters. There are 11 characters for which we have no exemplars
in the 5 episodes of Seinfeld. They cover only 1.8% of speech seg-
ments – most of them speak less than five sentences in the episodes.
Performance evaluation of Stage 2. We demonstrate the trade-off
between the Proportion of Classified Segments (POCS) and over-
all precision by varying the threshold d used in the nearest centroid
voice classification to assign speech segments as “unknown”. True
positives are the segments that overlap with the ground truth seg-
ments and the character is correctly identified. Figure 3 shows the
result. It can be seen that precision decreases as we classify more
segments. Also, long segments show higher precision in all three
TV series at any given POCS, which shows that the speaker model
produces better representations for longer segments.
4.2. Overall performance on the test set
Performance measures. In addition to the traditional diarisation
metric of Diarisation Error Rate (DER), we report the overall char-
acter recognition accuracy as well as the average of the per-character
Table 4: Performance on the test set. We report the Diarisation Error
Rate both with and without consideration of the overlapping regions,
DER(O) and DER respectively. Acc denotes a character recognition
accuracy for the segments that overlap with the groundtruth. Ppc and
Rpc are the average per-character precision and recall, respectively.
Showname
DER↓
DER(O)↓
Acc↑
Ppc↑
Rpc↑
Seinfeld
29.6
29.7
81.2
0.922
0.841
Frasier
23.8
24.3
83.1
0.933
0.888
Scrubs
32.6
36.4
76.1
0.883
0.853
Table 5: Word Error rate (WER) (%) on each dataset.
Model
Version
Seinfeld
Frasier
Scrubs
Wav2Vec2.0 [36]
ASR BASE 960H
45.0
36.9
36.3
Whisper [2]
medium.en
13.2
13.5
10.6
WhisperX [3]
medium.en
11.8
11.2
9.2
00:17:15,931 ~ 00:17:17,034
Jerry : Oh, please. I love her.
00:17:18,235 ~ 00:17:19,616
George : I’ve just met her, 
but I’m very impressed.
00:17:20,640 ~ 00:17:23,531
Ralph : I don’t understand. 
I’ve never had a problem with 
these notes before.
00:17:23,832 ~ 00:17:24,332
Jerry : What’s the next 
move?
…
…
Fig. 4: Qualitative example. Our method produces the speech seg-
ments with timestamps, and assigns the character who spoke it.
precision and recall metrics for the characters of each show. We use
a 0.25-second collar to calculate DER. Accuracy is calculated for the
segments that overlap with one of the ground truth segments.
The results are given in Table 4. We can see that the model
performs best on Frasier and worst on Scrubs in all metrics. This is
due to the difference in size of the casts in each dataset. Scrubs has
more characters than Frasier (48 > 29) for a similar total duration
(see Table 1). Thus, Scrubs provides more potential assignments for
each segment, making identification more challenging.
We also report the diarisation performance with and without the
overlapping speech in Table 4. The difference in DER for these two
categories is small in Seinfeld and Frasier, meaning that there is not
much overlapping speech within these two shows.
Speech transcription performance. Our method uses the Whis-
perX ASR model which also produces the speech transcription re-
sults. We compare the performance with the state-of-the-art models
in Table 5. Word Error Rate (WER) is computed after applying the
Whisper text normaliser to both ground truth and predictions which
can be found in the original paper [2]. We see that WhisperX outper-
forms both Wav2vec2.0 and Whisper. This is because the VAD Cut
& Merge preprocessing reduces the hallucination of Whisper, which
is also mentioned in the original paper [3].
Qualitative example. We show a qualitative example of our results
in Figure 4. As can be seen, our method assigns the character for
each speech segment, as well as timestamps and the transcription.
5. CONCLUSIONS
In this work, we show promising first steps towards a model for
character-aware subtitling, which we hope will be beneficial for im-
proving accessibility, and facilitating further research in video un-
derstanding. Our method is not perfect, however. Our recognition
efforts fail on short segments such as exclamations and also do not
deal with overlapping speech – though the latter does not appear
to be a serious limitation in practice. Furthermore, to generate the
true SDH subtitles, we would need to classify and categorise every
sound, not just speech – something our model is not yet capable of.
6. REFERENCES
[1] “Netflix player control tests,” https://about.netflix.
com/en/news/player-control-tests, 2023.
[2] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,
Christine McLeavey, and Ilya Sutskever,
“Robust speech
recognition via large-scale weak supervision,”
Proc. ICML,
2022.
[3] Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman,
“Whisperx: Time-accurate speech transcription of long-form
audio,” Proc. Interspeech, 2023.
[4] Triantafyllos Afouras, Joon Son Chung, Andrew Senior, Oriol
Vinyals, and Andrew Zisserman, “Deep audio-visual speech
recognition,” IEEE PAMI, 2019.
[5] Bowen Shi, Wei-Ning Hsu, and Abdelrahman Mohamed, “Ro-
bust self-supervised audio-visual speech recognition,” Proc.
Interspeech, 2022.
[6] Joon Son Chung, Jaesung Huh, Arsha Nagrani, Triantafyl-
los Afouras, and Andrew Zisserman, “Spot the conversation:
speaker diarisation in the wild,” in Proc. Interspeech, 2020.
[7] Yifan Ding, Yong Xu, Shi-Xiong Zhang, Yahuan Cong, and
Liqiang Wang,
“Self-supervised learning for audio-visual
speaker diarization,” in Proc. ICASSP, 2020.
[8] Eric Zhongcong Xu, Zeyang Song, Satoshi Tsutsui, Chao
Feng, Mang Ye, and Mike Zheng Shou,
“Ava-avd: Audio-
visual speaker diarization in the wild,” 2022, MM ’22.
[9] Rahul Sharma and Shrikanth Narayanan, “Audio visual char-
acter profiles for detecting background characters in entertain-
ment media,” arXiv preprint arXiv:2203.11368, 2022.
[10] Mark Everingham, Josef Sivic, and Andrew Zisserman, “Tak-
ing the bite out of automatic naming of characters in TV
video,” Image and Vision Computing, vol. 27, no. 5, 2009.
[11] Monica-Laura Haurilet, Makarand Tapaswi, Ziad Al-Halah,
and Rainer Stiefelhagen, “Naming tv characters by watching
and analyzing dialogs,” in Proc. WACV. IEEE, 2016, pp. 1–9.
[12] Arsha Nagrani and Andrew Zisserman, “From benedict cum-
berbatch to sherlock holmes: Character identification in tv se-
ries without a script,” in Proc. BMVC, 2017.
[13] Bogdan Mocanu, Ruxandra Tapu, and Titus Zaharia,
“En-
hancing the accessibility of hearing impaired to video con-
tent through fully automatic dynamic captioning,” in 2019 E-
Health and Bioengineering Conference (EHB), 2019.
[14] Wataru Akahori, Tatsunori Hirai, and Shigeo Morishima, “Dy-
namic subtitle placement considering the region of interest and
speaker location,” in International Conference on Computer
Vision Theory and Applications. SciTePress, 2017.
[15] Andrew Brown, Ernesto Coto, and Andrew Zisserman, “Auto-
mated video labelling: Identifying faces by corroborative evi-
dence,” in International Conference on Multimedia Informa-
tion Processing and Retrieval, 2021.
[16] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mans-
field, and Ignacio Lopz Moreno,
“Speaker diarization with
lstm,” in Proc. ICASSP, 2018.
[17] Aonan Zhang, Quan Wang, Zhenyao Zhu, John Paisley, and
Chong Wang, “Fully supervised speaker diarization,” in Proc.
ICASSP, 2019.
[18] Youngki Kwon, Hee Soo Heo, Jaesung Huh, Bong-Jin Lee,
and Joon Son Chung, “Look who’s not talking,” in 2021 IEEE
Spoken Language Technology Workshop (SLT), 2021.
[19] Yusuke Fujita, Naoyuki Kanda, Shota Horiguchi, Kenji Naga-
matsu, and Shinji Watanabe, “End-to-end neural speaker di-
arization with permutation-free objectives,” Proc. Interspeech,
2019.
[20] Shota Horiguchi, Yusuke Fujita, Shinji Watanabe, Yawen Xue,
and Kenji Nagamatsu, “End-to-end speaker diarization for an
unknown number of speakers with encoder-decoder based at-
tractors,” Proc. Interspeech, 2020.
[21] Joon Son Chung, Bong-Jin Lee, and Icksang Han, “Who said
that?: Audio-visual speaker diarisation of real-world meet-
ings,” Proc. Interspeech, 2019.
[22] Paul Lerner, Juliette Bergo¨end, Camille Guinaudeau, Herv´e
Bredin,
Benjamin
Maurice,
Sharleyne
Lefevre,
Martin
Bouteiller, Aman Berhe, L´eo Galmant, Ruiqing Yin, et al.,
“Bazinga! a dataset for multi-party dialogues structuring,” in
LREC, 2022.
[23] “International movie database,” https://www.imdb.com.
[24] Jon Gillick, Wesley Deng, Kimiko Ryokai, and David Bam-
man, “Robust laughter detection in noisy environments.,” in
Proc. Interspeech, 2021.
[25] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and
Andrew Zisserman, “Self-supervised learning of audio-visual
objects from video,” in Proc. ECCV, 2020.
[26] Bruno Korbar and Andrew Zisserman, “Personalised clip or:
how to find your vacation videos,” in Proc. BMVC, 2022.
[27] Steven Bird,
“Nltk: the natural language toolkit,”
in Pro-
ceedings of the COLING/ACL 2006 Interactive Presentation
Sessions, 2006, pp. 69–72.
[28] Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck,
“Ecapa-tdnn: Emphasized channel attention, propagation and
aggregation in tdnn based speaker verification,” Proc. Inter-
speech, 2020.
[29] Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zis-
serman,
“Voxceleb: Large-scale speaker verification in the
wild,” Computer Speech and Language, 2019.
[30] Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman,
“Deep face recognition,” in Proc. BMVC, 2015.
[31] “The Frasier Archives,” https://www.kacl780.net/.
[32] “Seinfeld
scripts
dot
com,”
https://www.
seinfeldscripts.com/seinfeld-scripts.html.
[33] “Scrubs
fandom,”
https://scrubs.fandom.com/
wiki/Category:Transcripts.
[34] Mark Everingham, Josef Sivic, and Andrew Zisserman,
“Hello! my name is... buffy”–automatic naming of characters
in tv video.,” in BMVC, 2006, vol. 2, p. 6.
[35] Abhishek Dutta and Andrew Zisserman,
“The VIA annota-
tion software for images, audio and video,” in Proceedings of
the 27th ACM International Conference on Multimedia, New
York, NY, USA, 2019, MM ’19, ACM.
[36] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and
Michael Auli, “wav2vec 2.0: A framework for self-supervised
learning of speech representations,”
NeurIPS, vol. 33, pp.
12449–12460, 2020.
"
"This paper proposes a simultaneous target tracking and multi-user communications system realized by a full duplex holographic Multiple-Input Multiple-Output (MIMO) node equipped with Dynamic Metasurface Antennas (DMAs) at both its communication ends. Focusing on the near-field regime, extending Fresnel's approximation to metasurfaces is done and a subspace tracking scheme with DMA-based hybrid Analog and Digital (A/D) reception as well as hybrid A/D transmission with a DMA for sum-rate maximization is devised. Simulation results corroborate the efficiency of the proposed framework for various system parameters.","The research delves into a novel Joint Communications and Sensing (JCAS) system that consists of a full duplex (FD) holographic MIMO framework equipped with Dynamic Metasurface Antennas (DMAs) at both transmission and reception ends. Operating in the challenging near-field environment, the system aims to seamlessly integrate multi-user communications and simultaneous target tracking.","Prior research on FD-enabled JCAS systems is comprehensive, ranging from single-antenna schemes to massive MIMO under millimeter-wave bands. A recent study explored the holographic MIMO concept using TX/RX DMA arrays achieving joint data transmission and target localization, however, intricate interactions between system parameters and effective target tracking within such intricate FD MIMO structures warranted further investigation.nan","The proposed system comprises a FD holographic MIMO transceiver employing DMA arrays and L-antenna Users' Equipment (UEs). A joint optimization framework that encompasses TX/RX A/D beamforming and digital Self Interference (SI) cancellation is formulated, aiming to maximize the downlink (DL) sum rate while maintaining precise target tracking. Employing dynamic metasurfaces, Fresnel's approximation is extended to Uniform Planar Arrays (UPAs) and consequently to DMAs, enabling the utilization of reflected DL signals from mobile targets for accurate tracking and DL communications maximization.","Simulation results showcase the effectiveness of the JCAS system. The interplay between system parameters, the number of nodes, and RX RF chains is evident, highlighting the impact on both tracking accuracy and DL sum rate. Intriguing observations reveal that optimizing RX DMA weights based on UE positions can compromise performance due to mobility.","The presented JCAS system demonstrates the potential for simultaneous multi-user communications and target tracking in the near-field regime. Numerical results emphasize the influence of system parameters, suggesting the need for careful optimization strategies to balance performance metrics.",Joint Near-Field Target Tracking and Communications with Full Duplex Holographic MIMO,"Ioannis Gavras, George C. Alexandropoulos","JOINT NEAR-FIELD TARGET TRACKING AND COMMUNICATIONS
WITH FULL DUPLEX HOLOGRAPHIC MIMO
Ioannis Gavras and George C. Alexandropoulos
Department of Informatics and Telecommunications, National and Kapodistrian University of Athens,
Panepistimiopolis Ilissia, 15784 Athens, Greece
ABSTRACT
In this paper, we present a simultaneous target tracking and
multi-user communications system realized by a full duplex
holographic Multiple-Input Multiple-Output (MIMO) node
equipped with Dynamic Metasurface Antennas (DMAs) at
both its communication ends.
Focusing on the near-field
regime, we extend Fresnel’s approximation to metasurfaces
and devise a subspace tracking scheme with DMA-based hy-
brid Analog and Digital (A/D) reception as well as hybrid A/D
transmission with a DMA for sum-rate maximization. The
presented simulation results corroborate the efficiency of the
proposed framework for various system parameters.
Index Terms— Joint communications and sensing, full
duplex, holographic MIMO, near field, tracking, metasurfaces.
1. INTRODUCTION
In-Band Full Duplex (FD) Multiple-Input Multiple-Output
(MIMO) systems [1–7] are lately being investigated as an
efficient technology for Joint Communications And Sensing
(JCAS) [8,9], which constitutes a key connectivity paradigm
for sixth Generation (6G) wireless networks. The main chal-
lenge for such simultaneous Transmit (TX) and Receive (RX)
operations is Self Interference (SI) appearing at the FD node,
which can become more severe as the number of TX and RX
antenna increases. To deal with this, solutions based on TX/RX
isolation, Analog and Digital (A/D) SI cancellation, and hybrid
A/D BeamForming (BF) have been presented [10–14].
Prior research on FD-enabled JCAS systems ranges from
single-antenna schemes [15, 16] to massive MIMO under
millimeter-wave frequencies [17–19]. Very recently, [20] capi-
talized on the holographic MIMO concept realized by TX/RX
Dynamic Metasurface Antenna (DMA) arrays [21] and studied
simultaneous data communications and target localization in
the challenging THz frequency band and the near-field regime.
The super focusing capability of massive DMAs was lever-
aged to efficient treat SI, while enabling communications in the
This work was supported by the Smart Networks and Services Joint
Undertaking (SNS JU) project TERRAMETA under the European Union’s
Horizon Europe research and innovation programme under Grant Agreement
No 101097101, including top-up funding by UK Research and Innovation
(UKRI) under the UK government’s Horizon Europe funding guarantee.
DownLink (DL) and a variant of MUltiple SIgnal Classifica-
tion (MUSIC) for locazation in the uplink. However, efficient
target tracking with such extremely massive FD MIMO sys-
tems has not been yet investigated.
In this paper, we capitalize on [20]’s FD-enabled JCAS
framework and present a simultaneous target tracking and
multi-user communications system. By extending Fresnel’s
approximation [22] to Uniform Planar Arrays (UPAs), and
consequently DMAs, we leverage the reflected DL signals
from mobile targets to achieve precise tracking of their param-
eters, while maximizing DL communications. For this goal, an
optimization framework for the joint design of TX/RX A/D BF
and the digital SI cancellation is presented. Our simulation re-
sults demonstrate the effectiveness of our approach, revealing
intricate interactions among various system parameters.
Notations: Boldface lowercase and boldface capital letters
represent vectors and matrices, respectively. AT, AH, [A]i,j,
and ∥A∥ denote A’s transpose, Hermitian transpose, (i, j)th
element, and Euclidean norm, respectively. C is the complex
number set and ȷ is the imaginary unit. E{·} is the expectation
operator and x ∼ CN(a, A) indicates a complex Gaussian
random vector with mean a and covariance matrix A.
2. SYSTEM AND SIGNAL MODELS
2.1. FD Holographic MIMO JCAS System Model
A DMA-based FD holographic MIMO transceiver, similar
to [20, Fig. 1], wishing to communicate in the DL direction
with U L-antenna Users’ Equipment (UEs), while simultane-
ously receiving in the uplink the reflections of its transmitted
signals from K ≥ U targets in its vicinity is considered. The
U served UEs belong to this set of K sensed targets, while
the remaining K − U targets are either passive objects or
non-cooperating UEs. An approach similar to [23] to sepa-
rate the reflections from the U served UEs and K − U targets
is assumed. The TX/RX DMAs are located in the xz-plane,
with each comprising NRF microstrips of NE metamaterials
each, where each microstrip is connected to a TX/RX Radio
Frequency (RF) chain (NRF > L in total). The inter-element
distance within each microstrip is dE and the horizontal separa-
tion between TX and RX DMAs is dpl = 2dP. Consequently,
arXiv:2401.12036v1  [cs.IT]  22 Jan 2024
both arrays have a total of N ≜ NRFNE metamaterials. All
U UEs requesting DL communications are equipped with an
L-element fully digital Uniform Linear Array (ULA) situated
along the z-axis.
Let the N × N diagonal matrices PTX and PRX, whose
elements model the signal propagation inside the microstrips
at the TX and RX DMAs, respectively. The former is defined
∀i = 1, 2, . . . , NRF and ∀n = 1, 2, . . . , NE as [24]:
[PTX](i−1)NE+n,(i−1)NE+n ≜exp (−ρi,n(αi + ȷβi)),
(1)
where αi is the waveguide attenuation coefficient, βi is the
wavenumber, and ρi,n denotes the location of the nth ele-
ment in the ith microstrip. Similar is the definition for PRX.
Let wTX
i,n and wRX
i,n denote the tunable responses of the TX/RX
DMAs, respectively, for each nth metamaterial and each ith mi-
crostrip, which are assumed to follow a Lorentzian-constrained
phase model and belong to the phase profile codebook:
W ≜
n
0.5
Fig. 1: Performance of the ru,ℓ,i,n and θu,ℓ,i,n approximations
considering a UE with L = 2 antennas and different values for NE.
The end-to-end channel between the TX and RX meta-
materials, including the single-bounch reflections from all K
targets when considered as point sources with coordinates
(rk, θk, φk) ∀k = 1, 2, . . . , K, is expressed as follows:
HR ≜
K
X
k=1
βkaRX(rk, θk, φk)aH
TX(rk, θk, φk)
(11)
with βk representing the complex-valued reflection coefficient
for each kth radar target, whereas, using (5) and the string
definition str ≜ {TX, RX}, astr(·) is obtained as:
[astr(rk, θk, ϕk)](i−1)NE+n ≜ak,i,n exp

ȷ2π
λ rk,i,n

. (12)
In this expression, the elevation angle θk,i,n and the distance
rk,i,n from the origin for each kth target are needed to compute
ak,i,n. The latter is given by the following expression:
rk,i,n ≜
(a) UEs position estimation.
(b) DL rate performance.
Fig. 2: Tracking and DL sum-rate performance for K = 3 targets, with U = {1, 2, 3} out of which being the UEs each with L = 2 antennas,
versus the transmit power Pmax for an FD holographic MIMO node with NRF = 4 TX/RX microstrips each with NE = 512 metamaterials.
3.2. Target Tracking and RX Combining
The targets existing in the system are tracked at the DMA-
based RX part of the considered FD holographic MIMO
transceiver, through the reception of their reflections of the DL
data signals and the PASTd algorithm [25]. This involves track-
ing the signal subspace Us and its corresponding eigenvalues
Λs in baseband. By exploiting the orthogonality between Us
and the noise subspace Un, with UnUT
n = I − UsUT
s , [20]’s
MUSIC variation is used for the PASTd initialization and the
targets’ parameters estimation.
As mentioned in the previous Section 3.1, initial estima-
tions of the UEs’ parameters are needed in OP1 to compute
WTX, while keeping a fixed wide analog combining configu-
ration WRX throughout the tracking process. The latter aims
to maximize the DL sum rate using a wide RX combiner to
detect reflecting signals, even when UE moves. It is noted
that, in the context of near-field operations, a wide RX beam
is deemed mandatory for increased precision positioning. We
acknowledge that optimizing the RX DMA weights based on
UEs’ positions may compromise performance as they move.
Assuming UE mobility with position changes every T Trans-
mission Time Intervals (TTIs), one can regularly update the
signal subspace and its corresponding eigenvalues. To this end,
at the beginning of each communication block, we recalculate
WTX to account for the UEs’ mobility.
4. NUMERICAL RESULTS
A DMA-based FD holographic MIMO transceiver with
NRF = 4 microstrips and NE = 512 metamaterials per mi-
crostrip, spaced at λ/2 and λ/5, respectively, is considered in
this section in a simulation scenario including K = 3 targets,
with varying subsets of them being the DL UEs equipped
with L = 2 antennas. The separation between TX and RX
DMAs is dpl = 2dP = 0.04 meters, and the JCAS system
operates at a central frequency of 120 GHz with a B = 150
KHz bandwidth. The UEs were randon placed within the
Fresnel region with coordinates ϕu = 90◦, θu ∈ [0◦, 90◦],
and ru ∈ [1, 20] meters. The results that follow were obtained
from independent Monte Carlo runs of T = 200 TTIs for
initial estimation and another T = 100 for tracking. We have
considered that, at every T TTIs, all UEs update their position
for 100 communication blocks using the uniform distribution
as knew = k + U(µk, dk), where k is a string referring to
either the range or the elevation angle (i.e., k ∈ {r, θ}),
and µ and dk denote their mean and maximum allowable
deviation, respectively. At periodic intervals of transmission
blocks, we set µr and µθ equal to the UEs’ coordinates. This
ensures smooth UE movement without causing jitter around
a fixed point. The noise variances σ2 and σ2
1 were set to
−174 + 10 log10(B), and a 10-bit beam codebook F was
utilized for the TX analog BF matrix WTX in OP1.
The tracking and sum-rate performance of the proposed
JCAS system is demonstrated in Figs. 2a and 2b, respectively,
versus Pmax in dBm. All UEs were assumed to move with
parameters µθ = µr = 2, dθ = 5, and dr = 2. In Fig. 2a, the
Root Mean square error (RMSE) for all targets averaged over
all TTIs is illustrated, whereas, Fig. 2b depicts the correspond-
ing sum-rate performance. As expected, both metrics improve
with increasing SNR values. It is also shown that the proposed
estimation scheme is robust in tracking multiple UEs, even
with the considered small number of RX RF chains. However,
as the number of UEs increases, localization gets degraded,
which also deteriorates the achievable sum rate. The latter
happens because the TX design relies on the composition of
the DL channel via the estimated UEs’ coordinates.
5. CONCLUSION
We proposed a JCAS system constituting of a DMA-based FD
holographic MIMO transceiver and presented a scheme for
simultaneous multi-user communications and target tracking
in the near-field regime. Our numerical results showcased that
this dual functionality performance depends on the number of
nodes in the system as well as the number of RX RF chains.
6. REFERENCES
[1] A. Sabharwal et al., “In-band full-duplex wireless: Chal-
lenges and opportunities,” IEEE J. Sel. Areas Commun.,
vol. 32, no. 9, pp. 1637–1652, 2014.
[2] G. C. Alexandropoulos and M. Duarte, “Joint design of
multi-tap analog cancellation and digital beamforming
for reduced complexity full duplex MIMO systems,” in
Proc. IEEE ICC, Paris, France, 2017.
[3] H. Iimori et al., “MIMO beamforming schemes for hy-
brid SIC FD radios with imperfect hardware and CSI,”
IEEE Trans. Wireless Commun., vol. 18, no. 10, pp. 4816–
4830, 2019.
[4] M. Duarte and G. C. Alexandropoulos, “Full duplex
MIMO digital beamforming with reduced complex-
ity AUXTX analog cancellation,” in Proc. IEEE ICC,
Dublin, Ireland, 2020.
[5] G. C. Alexandropoulos, “Low complexity full duplex
MIMO systems: Analog canceler architectures, beam-
forming design, and future directions,” ITU J. Future
Evolving Technol., vol. 2, no. 2, pp. 1–19, 2021.
[6] B. Smida et al., “Full-duplex wireless for 6G: Progress
brings new opportunities and challenges,” IEEE J. Sel.
Areas Commun., vol. 41, no. 9, pp. 2729–2750, 2023.
[7] M. Talha et al., “Multi-target two-way integrated sensing
and communications with full duplex MIMO radios,” in
Proc. IEEE Asilomar, Pacific Grove, USA, 2023.
[8] F. Liu et al., “Integrated sensing and communications:
Towards dual-functional wireless networks for 6G and
beyond,” IEEE J. Sel. Areas Commun., vol. 40, pp. 1728–
1767, 2022.
[9] K. V. Mishra et al., “Toward millimeter-wave joint radar
communications: A signal processing perspective,” IEEE
Signal Process. Mag., vol. 36, no. 5, pp. 100–114, 2019.
[10] I. P. Roberts et al., “Equipping millimeter-wave full-
duplex with analog self-interference cancellation,” in
Proc. IEEE ICC, Dublin, Ireland, 2020.
[11] G. C. Alexandropoulos et al., “Full duplex hybrid A/D
beamforming with reduced complexity multi-tap ana-
log cancellation,” in Proc. IEEE SPAWC, Atlanta, USA,
2020.
[12] M. A. Islam et al., “Direction-assisted beam management
in full duplex millimeter wave massive MIMO systems,”
in Proc. IEEE GLOBECOM, Madrid, Spain, 2021.
[13] G. C. Alexandropoulos et al., “Full duplex massive
MIMO architectures: Recent advances, applications, and
future directions,” IEEE Veh. Technol. Mag., vol. 17,
no. 4, pp. 83–91, 2022.
[14] M. A. Islam et al., “Joint analog and digital transceiver
design for wideband full duplex MIMO systems,” IEEE
Trans. Wireless Commun., vol. 21, no. 11, pp. 9729–9743,
2022.
[15] C. B. Barneto et al., “Full-duplex OFDM radar with LTE
and 5G NR waveforms: Challenges, solutions, and mea-
surements,” IEEE Trans. Microw. Theory Techn., vol. 67,
no. 10, pp. 4042–4054, 2019.
[16] S. D. Liyanaarachchi et al., “Optimized waveforms for
5G–6G communication with sensing: Theory, simula-
tions and experiments,” IEEE Trans. Wireless Commun.,
vol. 20, no. 12, pp. 8301–8315, 2021.
[17] C. B. Barneto et al., “Beamforming and waveform op-
timization for OFDM-based joint communications and
sensing at mm-waves,” in Proc. IEEE ASILOMAR, Pa-
cific Grove, USA, 2020.
[18] M. A. Islam et al., “Integrated sensing and communica-
tion with millimeter wave full duplex hybrid beamform-
ing,” in Proc. IEEE ICC, Seoul, South Korea, 2022.
[19] ——, “Simultaneous multi-user MIMO communications
and multi-target tracking with full duplex radios,” in Proc.
IEEE GLOBECOM, Rio de Janeiro, Brazil, 2022.
[20] I. Gavras et al., “Full duplex holographic MIMO for near-
field integrated sensing and communications,” in Proc.
EUSIPCO, Helsinki, Finland, 2023.
[21] T. Gong et al., “Holographic MIMO communications:
Theoretical foundations, enabling technologies, and fu-
ture directions,” IEEE Commun. Surveys & Tuts., to ap-
pear, 2024.
[22] Y. Pan et al., “RIS-aided near-field localization and chan-
nel estimation for the sub-terahertz system,” IEEE J. Sel.
Topics Signal Process., vol. 17, no. 4, pp. 878–892, 2022.
[23] H. Kim et al., “RIS-enabled and access-point-free simul-
taneous radio localization and mapping,” IEEE Trans.
Wireless Commun., to appear, 2024.
[24] J. Xu et al., “Near-field wideband extremely large-scale
MIMO transmission with holographic metasurface an-
tennas,” arXiv preprint arXiv:2205.02533, 2022.
[25] J. Sanchez-Araujo and S. Marcos, “An efficient pastd-
algorithm implementation for multiple direction of ar-
rival tracking,” IEEE Trans. Signal Process., vol. 47,
no. 8, pp. 2321–2324, 1999.
"
"This paper explores a localization approach for User Equipment (UE) in the near-field regime with a hybrid Analog and Digital (A/D) receiver architecture. The proposal comprises extremely large Dynamic Metasurface Antenna (DMA) and 1-bit resolution Analog-to-Digital Converter (ADC) at each reception radio-frequency chain. The approach scans the UE area of interest through identification of the DMA-based analog combining configuration resulting in a peak in a received pseudo-spectrum, obtaining the UE position estimation in three dimensions. Simulation results demonstrate the validity of the proposed scheme, highlighting interplay among various system parameters.","The motivation behind this work is the growth of extremely large antenna arrays and ultra-large bandwidths at (sub-)THz spectra, which hold potential for boosting communication data rates, energy efficiency, and sensing resolution. Holographic Multiple-Input and Multiple-Output (MIMO) transceivers with sub-wavelength-spaced elements in almost continuous antenna apertures are thus garnering substantial research interest. One promising technology is Dynamic Metasurface Antennas (DMAs), consisting of microstrips of metamaterial collections with tunable responses. This paper investigates an innovative DMA Receiver (RX) architecture with 1-bit resolution Analog-to-Digital Converters (ADCs) at each reception RF chain. Capitalizing on this hardware-efficient architecture, the study presents a localization approach for a single-antenna UE positioned in the near-field region.","nanPrior research on DMA technology has focused on hardware designs, fabrication schemes, and applications in (sub-)THz frequencies. Several studies have investigated the capabilities of DMAs in shaping Radio Frequency (RF) waves for diverse applications, demonstrating their potential for accurate communications and sensing. However, the study of localization schemes with hardware-efficient DMA structures is a relatively unexplored area. To fill this gap, the current work leverages the proposed DMA-based hybrid A/D RX architecture, utilizing 1-bit ADCs with a simple thresholding mechanism for accurate UE position parameter estimation.","The methodology includes defining a DMA-based RX architecture with an extremely large number of metamaterials grouped in microstrips. Each microstrip is attached to a reception RF chain comprising a 1-bit resolution ADC. The architecture aims to localize a single-antenna UE lying in the near-field region through optimization of the metamaterials' phase profiles and processing of the baseband received signal. A near-field signal propagation environment is considered, and a channel model is developed to account for molecular absorption and elevation angles. Furthermore, a received signal model incorporates the quantization effects of the 1-bit ADCs. The proposed near-field localization framework optimizes the free parameters of the DMA-based hybrid A/D RX architecture for precise UE localization.","Numerical results evaluate the performance of the proposed near-field localization framework for DMA-based hybrid A/D RXs with 1-bit resolution ADCs, in a sub-THz central frequency of 140 GHz. The study demonstrates that the proposed scheme outperforms a baseline approach employing full resolution ADCs, particularly for DMAs with large sizes of metamaterials per microstrip. The framework's scalability is explored with varying estimation overhead and transmit power levels. Insights are gained into the trade-off between estimation overhead and the number of DMA metamaterials for achieving accurate estimations.","The paper presents a near-field localization framework with a DMA-based hybrid A/D RX architecture. The proposed algorithm employs a grid search over predefined analog combiners at the DMA, representing candidate UE positions. The results demonstrate the superiority of the scheme over a baseline approach with full resolution ADCs, especially for DMAs with large metamaterial sizes per microstrip. The study highlights the interplay among various system parameters and emphasizes the importance of selecting an appropriate number of metamaterials and estimation overhead to optimize estimation performance.",Near-Field Localization with $1$-bit Quantized Hybrid A/D Reception,"Ioannis Gavras, Italo Atzeni, George C. Alexandropoulos","NEAR-FIELD LOCALIZATION WITH 1-BIT QUANTIZED HYBRID A/D RECEPTION
Ioannis Gavras,1 Italo Atzeni,2 and George C. Alexandropoulos1
1Dept. of Informatics and Telecommunications, National and Kapodistrian University of Athens, Greece
2Centre for Wireless Communications, University of Oulu, Finland
ABSTRACT
In this paper, we consider a hybrid Analog and Digital (A/D) re-
ceiver architecture with an extremely large Dynamic Metasur-
face Antenna (DMA) and an 1-bit resolution Analog-to-Digital
Converter (ADC) at each of its reception radio-frequency
chains, and present a localization approach for User Equipment
(UE) lying in its near-field regime. The proposed algorithm
scans the UE area of interest to identify the DMA-based ana-
log combining configuration resulting to the peak in a received
pseudo-spectrum, yielding the UE position estimation in three
dimensions. Our simulation results demonstrate the validity
of the proposed scheme, especially for increasing DMA sizes,
and showcase the interplay among various system parameters.
Index Terms— Dynamic metasurface antennas, hybrid
beamforming, localization, 1-bit ADC, near-field regime, THz.
1. INTRODUCTION
Extremely large antenna arrays [1] and ultra-large bandwidths
at millimeter wave, and beyond, spectra [2] constitute respec-
tively a promising technology and core feauture of upcom-
ing sixth Generation (6G) wireless networks, contributing in
boosting communication data rates, energy efficiency, and
sensing resolution [3]. To this end, holographic Multiple-
Input and Multiple-Output (MIMO) transceivers, comprising
sub-wavelength-spaced elements in almost continuous antenna
apertures, are lately receiving substantial research and devel-
opment interest [4]. One of the promising hybrid Analog and
Digital (A/D) HMIMO technologies are Dynamic Metasurface
Antennas (DMAs), consisting of microstrips of metamaterial
collections with tunable responses [5–10].
The flexibility of DMAs in shaping Radio Frequency (RF)
waves in the analog domain is recently exploited for different
applications in (sub-)THz frequencies, leveraging the short
The work of I. Gavras and G. C. Alexandropoulos was supported by the
Smart Networks and Services Joint Undertaking (SNS JU) project TERRAM-
ETA under the European Union’s Horizon Europe research and innovation
programme under Grant Agreement no. 101097101, including top-up funding
by UK Research and Innovation (UKRI) under the UK government’s Hori-
zon Europe funding guarantee. The work of I. Atzeni was supported by the
Research Council of Finland (336449 Profi6, 346208 6G Flagship, 348396
HIGH-6G, and 357504 EETCAMD).
wavelengths and large bandwidths to achieve remarkable spa-
tiotemporal resolution [2,7,11,12]. To this end, there has been
growing interest for efficient DMA hardware designs [11,13]
and fabrication schemes [14].
Extremely large DMAs at high frequencies usually result in
wireless system deployments in the near-field regime [15,16],
where precise control over the spatial characteristics of the
transmitted and received signals becomes crucial for accu-
rate communications, localization, and sensing. However,
to the best of the authors’ knowledge, localization schemes
with hardware-efficient DMA structures have not been stud-
ied. To fill this gap, in this paper, we capitalize on a recent
DMA Receiver (RX) architecture, comprising 1-bit resolution
Analog-to-Digital Converters (ADCs) at each of its reception
RF chains [17] attached to a disctinct group of metamateri-
als, and present a localization approach for single-antenna
User Equipment (UE) lying in its near-field regime. The pro-
posed approach is tailored to the hardware constraints of the
DMA-based hybrid A/D RX and deploys a simple threshold-
ing mechanism for accurate UE position parameter estimation.
The presented simulation results for a sub-THz frequency
range substantiate the effectiveness of the proposed localiza-
tion scheme, which is showcased to compensate the quantiza-
tion loss induced by the 1-bit ADCs with increasing numbers
of metamaterials at the DMA, outperforming a state-of-the-art
benchmark relying on full resolution ADCs.
Notations: Vectors and matrices are denoted by boldface
lowercase and boldface capital letters, respectively. The Her-
mitian transpose of A is denoted by AH, [A]i,j is the (i, j)th
element of A, ∥A∥ returns A’s Euclidean norm, and |a| is the
amplitude of a complex scalar a. C is the complex number
set, S returns the cardinality of set S, and ȷ is the imaginary
unit. E{·} is the expectation operator and x ∼ CN(a, A)
indicates a complex Gaussian random vector with mean a and
covariance matrix A.
2. SYSTEM AND SIGNAL MODELS
We consider a DMA-based RX with an extremely large num-
ber of metamaterials grouped in microstrips [5], with each
microstrip attached to a reception RF chain including an 1-
bit resolution ADC, as presented in [17] and illustrated in
Fig. 1. This hybrid A/D reception system, possibly integrating
arXiv:2401.12029v1  [cs.IT]  22 Jan 2024
RX DMA
propagation inside the microstrip
1-bit ADC
1-bit ADC
1-bit ADC
signal processing
 for localization
incident
signal(s)
Fig. 1: The proposed DMA-based hybrid A/D RX architecture
with an 1-bit resolution ADC at each reception RF chain.
sub-wavelength-spaced metamaterials within its aperture [4],
wishes to localize a single-antenna UE lying in its near-field re-
gion via appropriate optimization of the metamaterials’ phase
profiles and processing of the baseband received signal.
We assume that the RX DMA panel is placed on the pos-
itive xz-plane with its first microstrip lying at the origin. It
comprises NRF microstrips each consisting of NE metama-
terials. This arrangement results in a total of N ≜ NRFNE
metamaterials, acting as an extremely large planar antenna
array. To this end, the distance between any pair of microstrips
is represented by dRF, while the inter-element distance within
each microstrip is dE. We finally assume for simplicity that
the UE is positioned along the z-axis.
We define the N × N diagonal matrix PRX with each
non-zero elements modeling the signal propagation inside
the DMA microstrips. In particular, ∀i = 1, 2, . . . , NRF and
∀n = 1, 2, . . . , NE holds for this matrix that [18]:
[PRX](i−1)NE+n,(i−1)NE+n ≜exp (−ρi,n(αi + ȷβi)),
(1)
where, αi represents the waveguide attenuation coefficient,
βi corresponds to the wavenumber, and ρi,n indicates the
position of the nth element in the ith microstrip. Let wRX
i,n
represent the adaptable response (i.e., analog weight) of the
nth metamaterial in the ith microstrip. These weights adhere
to a Lorentzian-constrained phase model and belong to the
phase profile codebook W, as follows:
wRX
i,n ∈ W ≜
ȷ + eȷϕ
2
ϕ ∈
h
−π
2 , π
2
i
.
(2)
Hence, the analog RX combining matrix WRX ∈ CN×NRF is
given by:
[WRX](i−1)NE+n,j =
(
wRX
i,n ,
i = j
0,
i ̸= j .
(3)
2.1. Channel Model
We investigate wireless operations in the (sub-)THz frequency
band and focus specifically in an near-field signal propagation
environment. To this end, the complex-valued 1 × N channel
matrix between the DMA-based RX and the single-antenna
UE is modeled as follows:
[h](i−1)NE+n ≜ αi,n exp
ȷ2π
λ ri,n

,
(4)
where ri,n denotes the distance between the UE’s antenna and
the nth reception meta-element of the ith microstrip. Addi-
tionally, αi,n represents the attenuation factor including the
molecular absorption coefficient κabs at (sub-)THz frequencies
and defined as:
αi,n ≜
q
F(θi,n)
λ
4πri,n
exp

−κabsri,n
2

(5)
with λ being the signal wavelength, while F(·) represents each
metamaterial’s radiation profile. This profile is modeled for an
elevation angle θ as follows:
F(θ) =
(
2(b + 1) cosb(θ),
if θ ∈ [− π
2 , π
2 ]
0,
otherwise
.
(6)
In the latter expression, b determines the boresight antenna
gain which depends on the specific DMA technology.
The spherical coordinates of the UE’s antenna, situated in
the near-field of the RX DMA, are denoted as (r, θ, φ) with
elements representing respectively the distance from the origin,
elevation and azimuth angles. Each distance ri,n in (4) and (5)
can be computed as:
ri,n =

(r sin θ cos φ − (i−1)dRF)2+
(r sin θ sin φ)2 + (r cos θ−(n−1)dE)2 1
2 ,
(7)
Note that the elevation angle of the UE’s antenna with respect
to the nth reception meta-element of each ith microstrip is
given by the following expression:
θi,n ≜ sin−1
|(n − 1)dE − r cos θ|
ri,n

.
(8)
2.2. Received Signal Model
The baseband received signal y ∈ CNRF×1 at the output of the
RX RF chains can be mathematically expressed as follows:
y ≜ WH
RXPH
RXhHs + WH
RXPH
RXn,
(9)
where s ∈ C (in practice, it belong in a finite discrete mod-
ulation set) indicates the transmitted pilot symbol from the
UE and n ∼ CN(0, σ2IN) denotes the Additive White Gaus-
sian Noise (AWGN) vector due to thermal noise. It is further
assumed that, in each Transmission Time Interval (TTI), the
UE’s pilot symbol is power limited such that E{|s|2} ≤ Pmax
with Pmax representing the maximum transmission power. As
illustrated in Fig. 1, each RX RF chain comprises an 1-bit
resolution ADC. Following [19], the quantization procedure
for each ith microstrip output (i.e., each ith element of y) can
be represented via the following quantization function:
Q([y]i)=
(
0.5(sign (R{[y]i})+ȷsign (I{[y]i})),[g]i ≤γq
0,
otherwise ,
(10)
where [g]i ≜ |[y]i − ki| with ki being a Dynamic Direct
Current (DC) offset. Note that, according to this formula,
only when the amplitude of the received signal at each ith mi-
crostrip, minus the DC offset, falls below a predefined thresh-
old γq, it undergoes 1-bit quantization. Otherwise, the signal is
treated as environmental noise, providing a zero at the output
of the quantizer. We denote the NRF-element received signal
vector after quantization as yq ≜ Q(y). Capitalizing on our
near-field channel model, we can derive a closed-form formula
for the DC offset and the quantization threshold as follows:
k ≜ WH
RXPH
RXbhH,
γq ≜
√
Nσ2.
(11)
In the former expression, the DC offset k aims to replicate the
gain of the received pilot signal, by utilizing the reconstructed
uplink channel bh via (4) using a priori knowledge of the UE
3D position coordinates br, bθ, and bφ. Through this operation,
our objective is to neutralize the impact of the true signal,
leaving only the processed AWGN. The power of the processed
AWGN must be less than or equal to the value γq, where γq
is equivalent to the Root Mean Square Error (RMSE) value
of the expected power of the AWGN vector (i.e., E{∥n∥}),
ensuring conditions suitable for the quantization process.
3. PROPOSED NEAR-FIELD LOCALIZATION
In this section, we optimize the free parameters of the con-
sidered DMA-based hybrid A/D RX architecture with 1-bit
resolution ADCs for near-field localization.
3.1. Analog Combining Optimization
To achieve precise UE localization, we focus on maximizing
the received Signal-to-Noise Ratio (SNR) in the uplink direc-
tion through the optimization of the RX analog combining
matrix. This objective can be expressed mathematically as:
OP : max
f
WRX
∥f
WH
RXPH
RXbh∥2 s.t. ewRX
i,n ∈ W,
For a given UE coordinate tuple (br, bθ, bφ), the near-field chan-
nel gain vector bh can be constructed via (4). To solve OP,
we first constrain its f
WRX elements in the set F ∈ {ejϕ|ϕ ∈
Algorithm 1 Estimation of UE Position Coordinates
Input: S, γq, and Pmax.
Output: (br, bθ, bφ).
1: Initialize P(brp, bθp, bφp) = 0 ∀(brp, bθp, bφp) ∈ S.
2: for every (brp, bθp, bφp) ∈ S do
3:
Construct the virtual channel bh using (brp, bθp, bφp) in (4),
and update k via (11).
4:
Substitute bh into OP and solve for WRX.
5:
Apply WRX and obtain yq resulting from the quantiza-
tion in (10).
6:
Set P(brp, bθp, bφp) = ∥yq∥2.
7: end for
8: Conduct grid search in the pseudo-spectrum P to find the
peak corresponding to (br, bθ, bφ).
[−π/2, π/2]} (e.g., a Discrete Fourier Transform codebook)
having constant amplitude and variable phase values. Subse-
quently, we perform an 1D search. Finally, given f
WRX and
accounting for the signal propagation inside the microstrips,
the DMA analog combining weights are obtained as:
wRX
i,n ≜ 0.5
Fig. 2: RMSE performance of the UE position estimation versus
the transmit power Pmax in dBm for a DMA-based hybrid A/D RX
with NRF = 2 RF chains, NE = {128, 256, 512} metamaterials,
and |S| = T = 200 UE grid positions, and thus, analog combiners.
S, and consequently, conducting a grid search on the pseudo-
spectrum P, we can accurately determine the UE’s 3D position.
This procedure is summarized in Algorithm 1.
4. NUMERICAL RESULTS AND DISCUSSION
In this section, we evaluate the performance of the proposed
near-field localization framework for DMA-based hybrid A/D
RXs with 1-bit resolution ADCs. We focus on the sub-THz
central frequency of 140 GHz with a 150 KHz bandwidth, and
consider extremely large DMAs with inter-microstrip and inter-
element distances dRF = λ/2 and dE = λ/5, respectively.
The single-antenna UE was assumed randomly positioned
with spherical coordinates ϕu = 90◦, θu ∈ [0◦, 90◦], and
radial distance ru ∈ [1, 20] meters, ensuring placement within
the Fresnel region. Given the inevitable dependence of the
proposed localization approach on |S|, we have made the
assumption that prior knowledge of the UE’s coordinates is
available. To this end, the DMA analog combining focused on
the confidence interval [dℓ − ℓ, dℓ + ℓ], where string ℓ ∈ {r, θ}
represents the true value of the coordinate and dℓ signifies the
width of the interval. The confidence interval was considered
inside the set of the latter valid UE coordinates. The noise
variance σ2 was set to −174 + 10 log10(B) in dB and a 10-bit
beam codebook F was used for the DMA analog combiners
WRX. All performance evaluations were averaged over 300
independent Monte Carlo realizations.
In Figs. 2 and 3, we illustrate the RMSE of the proposed
localization approach in Algorithm 1 for different number of
metamaterials NE, grid searching overhead T, and transmit
power levels Pmax in dBm. As a baseline, we have include
in both figures the performance of the MUSIC-variant pre-
sented in [15] for full resolution ADCs at the RX RF chains.
In both figures, we have set dr = 5 meters and dθ = 10◦.
Fig. 3: RMSE performance of the UE position estimation versus
the transmit power Pmax in dBm for a DMA-based hybrid A/D RX
with NRF = 2 RF chains, NE = 128 metamaterials, and |S| =
T = {100, 200, 300} UE grid positions, and thus, DMA analog
combiners.
As shown, and as expected, the localization performance im-
proves with increasing SNR. In Fig. 2, considering an ex-
tremely large DMA with NE = {128, 256, 512} metamateri-
als and NRF = 2 RF chains, and |S| = T = 200 TTIs, it can
observed that the proposed scheme outperforms [15]’s localiza-
tion framework for increasing NE values. In Fig. 3, NRF = 2
RX RF chains and NE = 128 metamaterials per microstrip
were considered as well as |S| = T = {100, 200, 300} TTI
values. As shown, the proposed algorithm outperforms the
baseline even with a limited number of TTIs, indicating that its
scalability is compromised with fewer TTIs. This limitation is
reasonable because reducing |S| leads to a sparser sampling of
points in space, resulting in a less accurate estimation. On the
contrary, with a reduced number NE of metamaterials, the al-
gorithm can still achieve satisfactory estimation performance if
there is large |S|. This observation underscores the importance
of having an ample overhead to compensate for limitations
sourced on NE, demonstrating the trade-off between the esti-
mation overhead T and number NE of DMA metamaterials
for achieving accurate estimations.
5. CONCLUSION
In this paper, we presented a near-field localization framework
with a DMA-based hybrid A/D RX architecture, including
1-bit resolution ADCs, one at each of its reception RF chains
that is fed by a distinct microstrip. The proposed algorithm
includes a grid search over predefined analog combiners at the
DMA, which represent candidate UE positions in the near field.
It was demonstrated that the proposed scheme outperforms a
baseline approach with full resolution ADCs, especially for
DMAs with large sizes of metamaterials per microstrip.
6. REFERENCES
[1] Z. Wang et al., “Extremely large-scale MIMO: Funda-
mentals, challenges, solutions, and future directions,”
IEEE Wireless Commun., early access, 2023.
[2] G. C. Alexandropoulos et al., “Time reversal for 6G wire-
less communications: Novel experiments, opportunities,
and challenges,” IEEE Veh. Technol. Mag., vol. 17, no. 4,
pp. 74–82, 2022.
[3] S. P. Chepuri et al., “Integrated sensing and communi-
cations with reconfigurable intelligent surfaces,” IEEE
Signal Process. Mag., vol. 40, no. 6, pp. 41–62, Sep.
2023.
[4] T. Gong et al., “Holographic MIMO communications:
Theoretical foundations, enabling technologies, and fu-
ture directions,” IEEE Commun. Surveys & Tuts., to ap-
pear, 2024.
[5] N. Shlezinger et al., “Dynamic metasurface antennas
for 6G extreme massive MIMO communications,” IEEE
Wireless Commun., vol. 28, no. 2, pp. 106–113, 2021.
[6] K. Stylianopoulos et al., “Autoregressive attention neu-
ral networks for non-line-of-sight user tracking with dy-
namic metasurface antennas,” in Proc. IEEE CAMSAP,
Los Sue˜nos, Costa Rica, 2023.
[7] H. Zhang et al., “Beam focusing for near-field multiuser
mimo communications,” IEEE Trans. Wireless Commun.,
vol. 21, no. 9, pp. 7476–7490, 2022.
[8] Q. Yang et al., “Near-field localization with dynamic
metasurface antennas,” in Proc. IEEE ICASSP, Rhodes,
Greece, 2023.
[9] J. Xu et al., “Dynamic metasurface antennas for energy
efficient uplink massive MIMO communications,” in
IEEE GLOBECOM, Madrid, Spain, Dec 2021.
[10] L. Wei et al., “Tri-polarized holographic mimo surface
in near-field: Channel modeling and precoding design,”
in IEEE Trans. Wireless Commun., to appear, 2024.
[11] Q. Luo et al., “Ultra-wideband metasurface at SubTHz:
Hardware design and reflection optimization,” in Proc.
EUCAP, Florence, Italy, 2023.
[12] G. C. Alexandropoulos et al., “Reconfigurable intelligent
surfaces for THz: Signal processing and hardware design
challenges,” in Proc. EUCAP, Glasgow, Scotland, Mar.
2024.
[13] X. Ma et al., “1-bit sub-thz RIS via planar tightly coupled
dipoles: Beam shaping and proof of concept,” in Proc.
EUCAP, Glasgow, Scotland, 2024.
[14] A. D. Papadopoulos et al., “Adaptive polynomial chaos
expansion for uncertainty quantification of subTHz horn
antennas with flat-top radiation patterns,” in Proc. EU-
CAP, Glasgow, Scotland, 2024.
[15] I. Gavras et al., “Full duplex holographic MIMO for near-
field integrated sensing and communications,” in Proc.
EUSIPCO, Helsinki, Finland, 2023.
[16] ——, “Joint near-field target tracking and communica-
tions with full duplex holographic MIMO,” in Proc. IEEE
ICASSP, Seoul, South Korea, 2024.
[17] P. Gavriilidis et al., “Metasurface-based receivers with
1-bit ADCs for multi-user uplink communications,” in
IEEE ICASSP, Seoul, South Korea, 2024.
[18] J. Xu et al., “Near-field wideband extremely large-scale
MIMO transmission with holographic metasurface an-
tennas,” arXiv preprint arXiv:2205.02533, 2022.
[19] D. Abdelhameed et al., “Enhanced signal detection and
constellation design for massive SIMO communications
with 1-bit ADCs,” IEEE Access, vol. 11, pp. 11 749–
11 765, 2023.
"
"This paper introduces MViTac, a new method combining visual and tactile sensory data in a self-supervised manner for various robotic tasks. Leveraging contrastive learning, MViTac incorporates intra and inter-modality data losses for encoding, leading to enriched object property classification and successful grasping predictions. Experiments showcase its efficacy compared to state-of-the-art self-supervised and supervised techniques.","Blending visual and tactile sensory inputs is vital in robotics, as it provides nuanced representations of the environment. However, traditional approaches heavily rely on labeled datasets, which are expensive and time-consuming to generate. This paper presents MViTac, a novel self-supervised contrastive learning methodology using unlabeled data to fuse vision and touch sensations, offering superior representations.","nanThe research builds upon prior studies integrating visual and tactile modalities for enhanced robotic manipulation and interaction. It highlights methods like encoding tactile signals for object grasp stability estimation, tactile sensors for detecting local slips, and the use of vision-based optical tactile sensors for comprehensive object information. Previous works have also explored cross-modal prediction systems, joint group kernel sparse coding, and self-supervised tactile learning for robotic manipulation and exploration tasks.","The proposed MViTac approach consists of two training mechanisms, inter-modal and intra-modal contrastive learning. It utilizes dual encoders: one for visual data and the other for tactile data. The methodology aims to minimize the distance between augmented images and tactile representations in a lower-dimensional latent space. Intra-modal learning enhances representation discriminability, while inter-modal learning establishes relationships between the two sensory modalities.","Experiments conducted on the Touch-and-Go (TAG) dataset for material property identification and the Calandra dataset for robot grasp prediction substantiate MViTac's effectiveness. It surpasses existing self-supervised approaches and supervised learning methods in material category and hard/soft surface classification tasks. Moreover, it outperforms self-supervised state-of-the-art methods in predicting grasping success, showcasing its generalizability across different benchmarks.","MViTac demonstrates the successful integration of visual and tactile sensory data through self-supervised contrastive learning. It achieves superior performance in material property classification and robot grasp prediction tasks compared to contemporary approaches, highlighting its potential in advancing multimodal robotic capabilities. Future work aims to refine the learning architecture and validate MViTac on real robotic platforms for impactful real-world applications.",Multimodal Visual-Tactile Representation Learning through Self-Supervised Contrastive Pre-Training,"Vedant Dave, Fotios Lygerakis, Elmar Rueckert","Multimodal Visual-Tactile Representation Learning through
Self-Supervised Contrastive Pre-Training
Vedant Dave*, Fotios Lygerakis*, Elmar Rueckert
Cyber-Physical-Systems Lab at Montanuniversit¨at Leoben, Austria
Abstract— The rapidly evolving field of robotics necessitates
methods that can facilitate the fusion of multiple modalities.
Specifically, when it comes to interacting with tangible objects,
effectively combining visual and tactile sensory data is key
to understanding and navigating the complex dynamics of
the physical world, enabling a more nuanced and adaptable
response to changing environments. Nevertheless, much of the
earlier work in merging these two sensory modalities has relied
on supervised methods utilizing datasets labeled by humans.
This paper introduces MViTac, a novel methodology that
leverages contrastive learning to integrate vision and touch
sensations in a self-supervised fashion. By availing both sen-
sory inputs, MViTac leverages intra and inter-modality losses
for learning representations, resulting in enhanced material
property classification and more adept grasping prediction.
Through a series of experiments, we showcase the effectiveness
of our method and its superiority over existing state-of-the-art
self-supervised and supervised techniques. In evaluating our
methodology, we focus on two distinct tasks: material classi-
fication and grasping success prediction. Our results indicate
that MViTac facilitates the development of improved modality
encoders, yielding more robust representations as evidenced
by linear probing assessments. https://sites.google.com/
view/mvitac/home
I. INTRODUCTION
In the realm of robotics, visual perception has tradi-
tionally served as a central modality extensively leveraged
for acquiring nuanced environmental representations, a role
emphasized in a range of studies [1, 2]. However, this
approach harbors intrinsic limitations in fully encapsulating
the dynamic and intricate state of the surrounding environ-
ment [3]. Conversely, tactile sensing excels in delineating
fine-grained attributes that are beyond the grasp of visual
modalities, effectively capturing the subtleties that evade
visual systems.
Thus, a synergic integration of both visual and tactile
modalities offers a more robust and comprehensive world
representation, wherein the visual systems predominantly
decipher global features while tactile sensing augments this
with enriched local feature representations [4, 5]. However,
the fusion of these modalities is far from trivial, given
the disparate information density each conveys over time.
Especially in manipulation task settings, the reliance on
vision remains high until the robot engages physically with
an object or surface; following this interaction, the tactile
modality often becomes the principal source of nuanced data,
particularly in scenarios involving occlusion by the robotic
arm.
*These authors contributed equally to this work.
Corresponding Author: vedant.dave@unileoben.ac.at
Recent endeavors are increasingly focusing on the efficient
fusion of vision and tactile representations to navigate the
challenges delineated above [6–10]. This is further propelled
by the significant advancements in self-supervised learning
(SSL) approaches, catalyzed by the lack of labeled tactile and
visuotactile datasets [9–12]. In contrast to visual data, where
simulations and virtual environments can often provide sub-
stantial and rich datasets, tactile data collection requires
physical interaction with a wide array of materials and
objects to encapsulate a rich diversity of tactile experiences.
This not only escalates the complexity but also significantly
extends the time needed for data collection. Recognizing
these challenges, researchers are turning towards the SSL
strategy to leverage unlabeled data, which is easier and
quicker to collect [10–12]. SSL facilitates the learning of
useful representations from this unlabeled data, potentially
accelerating the development of sophisticated visuotactile
systems by reducing the dependency on labor-intensive la-
beled datasets, thus presenting a promising avenue to mit-
igate the hurdles in tactile data collection. In recent years,
contrastive learning (CL) has risen as a prominent subfield
of self-supervised learning (SSL), establishing itself as the
predominant methodology for pretraining visual encoders
[13–18].
In this study, we propose a Multi-modal Visual-Tactile
(MViTac) representation learning algorithm for effectively
fusing the two modalities. Our methodology facilitates the
learning of both intra and inter-modal representations, lever-
aging the richness of visual and tactile observations. We use
two sets of encoders to compute the InfoNCE loss for the two
ranges of losses. We employ the within-modality (intra) loss
to maximize the agreement of the representations of similar
modality instances. Similarly, we employ the across-modality
(inter) loss to maximize the similarity of representations of
different modalities across the same sample. Utilizing the
Touch-and-Go [11] and Calandra [4] datasets for training
the encoders, we undertake an exhaustive evaluation across
diverse downstream tasks, including material property identi-
fications and robot grasp prediction. We thereby substantiate
the efficacy of our approach in transcending the performance
benchmarks set by previous state-of-the-art frameworks.
II. RELATED WORK
The integration of visual and tactile modalities in robotic
systems is a growing research area, with numerous studies
delving into its complexities. This section highlights key
contributions in this field.
arXiv:2401.12024v1  [cs.RO]  22 Jan 2024
A. Tactile Sensing in Robotics
Humans can identify physical properties (hardness, rough-
ness, texture) of objects exclusively through tactile interac-
tions [19] and exhibit significant reliance on tactile feedback
for grasping and manipulation tasks. In recent years, various
methods have been developed for the extraction of tactile in-
formation, proving instrumental for robotic applications [20,
21]. Works such as [22–24] have utilised tactile signals to
estimate the object grasp stability, while Veiga et al. [25]
extended this idea to independent fingers for detecting local
slips. Significant robotics research focuses on manipulation
and grasping that integrates object properties and geometry,
gripper configurations, and environmental conditions
[26,
27]. Various methodologies like Gaussian Processes [28, 29],
Movement Primitives [30, 31], Reinforcement Learning [32,
33] are incorporated to obtain the correlation between the
manipulation skills and tactile sensations. In recent years,
vision-based optical tactile sensors such as Gelsight [34],
Digit [35], and XELA uSkin [36] have gained significant
traction due to their ability to provide rich information
concerning object geometry, forces, and shear.
B. Vision and Touch in Robotic Manipulation
Several advancements have been made in the integration
of tactile sensing for enhanced grasping and manipulation.
Calandra et al. [37] demonstrated that integrating tactile
sensing significantly enhances grasping results. Their ap-
proach involved training vision-only, tactile-only, and visual-
tactile grasp success predictors and selecting the one with the
highest success probability. In another study [38], a tactile
encoder was trained in simulation across three distinct tasks
and managed to achieve zero-shot sim-to-real transfer using
a generative adversarial network. Delving deeper into tactile
representations, Guzey et al. [12] found that utilizing a self-
supervised method to learn these representations led to supe-
rior results in manipulation tasks. Their approach gathered a
dataset of teleoperated, contact-rich, albeit arbitrary, interac-
tions with the environment, which they then used to derive
tactile representations. For subsequent tasks, they employed
a nearest neighbors approach to recall actions. Meanwhile,
the research outlined in [39] introduced Visuotactile-RL,
a methodology combining visual and tactile feedback for
manipulation. They harnessed tactile data sourced from op-
tical sensors and explored two image encoder architectures,
namely MultiPath (MP) and SinglePath (SP), complemented
by tactics such as tactile gating.
C. Visual-Tactile Joint Representation learning
In recent studies, the fusion of visual and tactile data has
emerged as a focal point in advancing object recognition
and manipulation tasks. Li et al. in [40] presented a cross-
modal prediction system, which addresses the significant
scale gap between visual and tactile signals using conditional
adversarial networks. The system synthesizes temporal tactile
signals from visual inputs and identifies touched object parts
from tactile inputs, enhancing the interaction between vision
and touch. The authors in [40] employ a data rebalancing
strategy to prevent mode collapse during GAN training and
the inclusion of touch scale and location data in the model.
H. Liu et al. [6] developed a visual-tactile fusion frame-
work using a joint group kernel sparse coding method to
address the weak pairing issue in visual-tactile data samples.
Further contribution by H. Li et al. leveraged three distinct
attention mechanisms including multi-head self-attentions
across different modalities and timelines, proving beneficial
in mastering dense packing and pouring tasks [7]. S. Luo et
al. [8] aimed at amplifying cloth texture recognition precision
by focusing on shared features across different modalities.
They tested their system in robotic tactile exploration tasks
for cloth material identification. The studies extended into
robot-assisted dressing with a notable focus on garment un-
folding, integrating visual-tactile prediction models with re-
inforcement learning to guide robotic movements effectively
during the unfolding process [41]. Chen et al. [9] introduced
the Visuo-Tactile Transformer (VTT) which utilized spatial
attention for merging visual and tactile data, demonstrating
enhanced efficiency in correlating tactile events with visual
cues. Kerr et al. [10] build a self-supervised learning strat-
egy that leverages intra-modal contrastive loss for learning
representations in tasks such as garment feature tracking
and manipulation. These endeavors collectively underscore
the promising trajectory of integrating visual and tactile
feedback for refined robotic functionalities in recognition and
manipulation tasks.
III. MULTIMODAL SELF-SUPERVISED LEARNING
We introduce a cross-modal self-supervised learning ap-
proach for learning representations between visual and tactile
data, which can subsequently be employed for downstream
applications such as material classification and robotic ma-
nipulation tasks.
A. Problem Statement
Given a visual observation OV and a tactile observation
OT both corresponding to the same object, our objective
is to find a function f that encapsulates the projections
zV = f(OV ) and zT = f(OT ), which lie close to each
other in a lower-dimensional latent space Z, zV , zT ∈ Z.
The observation spaces OV and OT are RGB images in
RH×W ×3, where H and W are the height and the width
of the images.
B. Multimodal Learning
Our approach is built upon Multimodal Contrastive Train-
ing (MCT) [42], a method originally developed for learning
relationships between visual and textual data. The overall
adapted architecture of the proposed approach is illustrated
in Figure 1. Within this framework, we utilize dual encoders:
one for processing visual data and another for handling
tactile information. The training is comprised of two dis-
tinct contrastive training paradigms: intra-modal and inter-
modal learning. Intra-modal refers to the learning that occurs
exclusively within the same data modality, either between
tactile-to-tactile or visual-to-visual representations. This loss
Fig. 1: Left: Architecture of MViTaC that consists of two training mechanisms, the inter-modal (green and pink) and intra-
modal contrastive learning (blue and cyan), which collectively contribute to the self-supervised learning loss. Right: The
SSL pre-trained model is subsequently evaluated on downstream tasks via linear probing. The task we consider are material
property classification (top) and grasp success prediction (bottom).
is designed to maintain the similarity among augmented
images while capturing their inherent structures, thereby
enhancing the model’s robustness to varying perspectives
(Details regarding the augmentations are in the Appendix).
Inter-modal refers to the learning that incorporates multiple
data modalities, facilitating the understanding of relation-
ships between different types of data, such as tactile and
visual representations.
C. Architecture
Our dataset, comprised of N samples, is formally defined
as D = {(vi, ti)}N
i=1, where the tuple (vi, ti) corresponds to
an image-tactile pair. Referring to the architecture illustrated
in Figure 1, the image encoder and its momentum-based
counterpart are convolutional neural networks, parameterized
by θ and θk respectively. These encoders are built upon
a ResNet-18 backbone [43], pre-trained on ImageNet [44].
We additionally use a 2-layer MLP Projection head, which
serves to map the representations into an embedding space
for contrastive estimation. For the image encoder and its
momentum counterpart, two distinct MLPs are utilized to
generate latent representations tailored for intra-modal and
inter-modal learning tasks. The momentum encoder as well
as the MLP projection heads (ϕk
v, ϕk
t ) are updated with the
momentum update [15]:
θk ← mθk + (1 − m)θ,
(1)
ϕk
v ← mϕk
v + (1 − m)ϕq
v,
(2)
ϕk
t ← mϕk
t + (1 − m)ϕq
t.
(3)
Regarding the tactile models, the encoder and its momentum
counterpart are instantiated with identical configurations and
are utilized to ensure consistency in the encoding process.
D. Intra-modal learning
Intra-modal learning aims at optimizing the feature repre-
sentations within the same sensory modality, such as visual-
to-visual or tactile-to-tactile, to enhance the discriminative
power of the extracted features and the generalization ca-
pabilities of the model for downstream tasks. One common
practice in contrastive learning is to randomly augment a
single image to obtain different representations [14], which
are then passed to the encoder to obtain a representation
called “query” and its momentum counterpart to obtain
a“key” [15]. We derive two encodings for each modality from
their respective inputs: for visual images {oq
v, ok
v} from ov
and for tactile image {oq
t, ok
t } from ot. The encodings for
visual data can be written as follows
zq
vv = g(f(oq
v; θ); ϕq
v),
(4)
zk
vv = g(f(ok
v; θk); ϕk
v),
(5)
where ϕq
v and ϕk
v are the query and key projection heads
for visual data respectively and zq
vv and zk
vv are their cor-
responding visual embeddings generated by their respective
query and key encoders (represented as f(·)) in conjunction
with their associated projection heads (represented as g(·)).
Similarly, we obtain encodings for the tactile images as
zq
tt = g(f(oq
t; ψ); Φq
t),
(6)
zk
tt = g(f(ok
t ; ψk); Φk
t ).
(7)
We adopt InfoNCE [13] loss to optimize the contrastive
learning objective. Given a query zq
vv with a positive sample
zk+
vv
and K negative samples denoted by zki
vv with ki =
{1...K}, the InfoNCE loss can be defined as
Lvv = − log
exp(zq
vv · zk+
vv /τ)
PK
i=0 exp(zq
vv · zki
vv/τ)
.
(8)
Here τ is the temperature hyper-parameter [45] for ℓ2-
normalized q and k. In contrast to MoCo [15] and MCT [42],
which employs a memory queue to maintain negative sam-
ples, our approach aligns with previous works [14, 46, 47],
utilizing the other keys present in the same batch as negative
samples. In our model, the intra-modal learning consists of
two parts, the visual-to-visual Lvv as Eq. (8) and the tactile-
to-tactile contrastive learning, formalised as Ltt loss and in
a similar manner as Eq. (8).
E. Inter-modal learning
Inter-modal learning aims at aligning the feature spaces
across the visual and touch sensory modalities, in order
to develop a unified multimodal latent representation that
captures the underlying relationships and shared attributes
between these distinct data sources.
1) Image-to-Tactile representation learning: For a visual
image ov and its corresponding tactile image ot, we utilize
the image encoder to produce the query feature and the
momentum tactile encoder for the generation of the key
feature. They are then projected to latent space by their
respective projection heads
zq
vt = g(f(oq
v; θ); ϕq
t),
(9)
zk
vt = g(f(ok
t ; ψk); Φk
v).
(10)
Contrary to the approach employed by MCT [42], which
utilizes dot products between learned representations and
introduces a ‘margin” hyper-parameter, our method adopts
the InfoNCE loss as specified in Eq. (8). This deviation
arises as we operate in the image space for both modalities,
a distinction from the scenario in [42] that handles textual
data instead. The corresponding loss Lvt is also defined as
InfoNCE loss and is formulated in the same manner as in
Eq. (8)
Lvt = − log
exp(zq
vt · zk+
vt /τ)
PK
i=0 exp(zq
vt · zki
vt/τ)
(11)
2) Tactile-to-Image representation learning: Similar to
Image-to-Tactile learning, given a visual-tactile image pair
ov, ot, we generate the query feature from the tactile encoder
and the key feature from the momentum image encoder,
which are subsequently projected to the latent space by their
respective projection heads as
zq
tv = g(f(oq
t; ψ); Φq
v)
(12)
zk
tv = g(f(ok
v; θk); ϕk
t )
(13)
The loss Ltv is also formalised as InfoNCE loss and is
formulated in the same manner as in Eq. (11).
F. Combined Loss
The overall multimodal contrastive loss used in MViTac is
formulated as a combination of both, inter-modal and intra-
modal contrastive loss, weighted by a term λinter that is re-
sponsible for the trade-off between between the optimization
objectives for inter-modal and intra-modal representations.
The overall loss Lmm is defined as
Lmm = Lvv + Ltt + λinter(Lvt + Ltv)
(14)
IV. EXPERIMENTS
In this section, we discuss the experiments aimed at
validating the performance of our proposed MViTac learning
framework. We elaborate on the two datasets used and
evaluate the learned representations on four downstream
tasks. Our model is benchmarked against state-of-the-art self-
supervised methods in visuotactile learning and supervised
learning method.
A. Experimental Setup
1) Material property identification: We evaluate on the
Touch-and-Go (TAG) [11] dataset to solve the task of
material property identification. This dataset encompasses
a diverse range of tactile features that are instrumental in
bifurcating various material properties. We consider three
downstream tasks: 1) categorization of materials, 2) dis-
tinction between hard and soft surfaces, and 3) distinction
between smooth and textured surfaces. We adhere to the
dataset splits prescribed by the authors of [11] to maintain
experimental consistency. This partitioning ensures that our
evaluations are directly comparable to prior work and their
baselines. For the classification task, the dataset comprises
a collection of 20 distinct objects, whereas Hard/Soft and
Rough/Smooth are binary classification task.
2) Robot Grasping Prediction: The Calandra dataset [4]
provides the data from a pair of tactile sensors attached to
a jaw gripper (left and right) alongside the RGB images. A
triplet of samples was captured ’before’, ’during’, and ’after’
grasping a plethora of objects. The objective is to determine
the success or the failure of the grasp attempt. Unlike in
TAG [11], the Calandra dataset does not provide a predefined
train/test split, leading us to create our own randomized
split. We train our model on a subset of 40 unique objects
from the total 106 of the original dataset, keeping only the
demonstrations with the most grasping attempts. In contrast
TABLE I: Comparison on different material property identification downstream tasks
Dataset
Method
Modality
Category
Accuracy%
Hard/Soft
Accuracy (%)
Rough/Smooth
Accuracy (%)
Chance
-
Tactile
18.6
66.1
56.3
ResNet18 [43]
Supervised Learning
Tactile
57.4
89.1
79.3
Tactile + Visual
48.0
85.9
80.0
TAG [11]
Contrastive Multiview Coding
Tactile
54.7
77.3
79.4
Tactile + Visual
68.6
87.1
82.4
SSVTP [10]
InfoNCE
Tactile
46.1
79.7
75.8
Tactile + Visual
70.7
88.6
83.6
MViTac (Ours)
Multimodal Contrastive Training
Tactile
57.6
86.2
82.1
Tactile + Visual
74.9
91.8
84.1
We report the evaluation of Top-1% accuracy across various downstream tasks. We evaluate on both, tactile-only and visual-
tactile data. The blue are the best results in tactile-only modality and red shows the best result in tactile+visual modality.
to TAG, which only utilizes tactile-tactile encoding in their
experiments, we use visual-tactile representations to predict
the grasp success. The only difference with the setup in
the previous section is that we stack the tactile images
from the two sensors across the channel dimension before
passing them through the respective encoder to obtain their
common representation. Finally, we evaluate the learned
representations by using the tactile pair and the RGB image
from the ’during’ samples to train a linear classifier.
B. Evaluation and Results
We evaluate our MViTac model against established models
on tactile-only and visual-tactile data. For reproducibility
and further research, we provide our test/train split of the
Calandra dataset, along with the models and code, on our
project website1.
1) Material property identification:
We first evaluate
MViTac on only the tactile data for all the three material
property identification tasks. For a comparative assessment,
we employed multiple methodologies within the same prob-
lem domain. These encompass a supervised learning frame-
work that relies on ResNet-18 architecture [43], where the
model is trained on labeled data. Furthermore, we incorporate
comparative metrics from TAG [11], a model that utilizes
the Contrastive Multiview Coding (CMC) approach [48] to
learn cross-modal representations. The baseline results for
TAG are sourced from the original paper as the experimental
conditions, including data splits, and were maintained in
strict accordance with the original study. In addition, we also
consider SSVTP [10], a recent approach that employs In-
foNCE [13] loss in its pre-training phase for self-supervised
learning.
In accordance with common practice, we evaluate the quality
of the learned representations through linear probing. Fol-
lowing the self-supervised pre-training phase, we detach the
projection heads and freeze the encoder. On these static rep-
resentations, we then train a linear classifier in a supervised
fashion.
As observed in Table I, the tactile-only model surpasses the
performance of both the cross-modal representation learning
1 https://sites.google.com/view/mvitac/home
methods, TAG and SSTVP. Notably, our method exceeds the
performance of not just self-supervised approaches like TAG
and SSTVP, but also the supervised learning model, which
commonly surpasses self-supervised methods in general effi-
cacy. Nonetheless, the supervised learning approach exhibits
a marginal advantage in tasks related to the classification of
hard and soft surfaces. Remarkably, the integration of visual
and tactile data results in substantial performance gains.
Across all methodologies, incorporating visual information
appears to uniformly elevate prediction accuracy—an out-
come that, while expected, underscores its significance.
Within this framework, our method consistently outperforms
all alternative approaches, signifying the effectiveness of
our multimodal strategy. Although both TAG and SSVTP
leverage contrastive learning frameworks and operate on
closely related loss functions, their performance lags behind
our model in terms of prediction accuracy. Even though TAG
and SSVTP are contrastive learning models, and work on
almost similar loss functions, they achieve less prediction
accuracy than our model. Our model not only distinguishes
similarities and differences between visual and tactile data
but also places substantial emphasis on learning within the
same sensory modality.
TABLE II: Comparison of predicting the success of grasping
Dataset
Method
Grasping Pred.
Accuracy%
TAG [11]
Contrastive Multiview Coding
56.3
MViTac (Ours)
Multimodal Contrastive Training
60.3
Calandra et. al. [4]
Supervised Learning (Baseline)
73.1
Top 1% Accuracy. The best result is in bold.
2) Robot Grasping Prediction: We compare the robust-
ness of the learned representations using again the common
linear probing method and train a linear classifier to predict
robot grasping outcomes. We compare our approach against
the CMC method [11] and the supervised method proposed
in [4] for solving this classification task. We report the
comparison results in Table I. The superiority of the learned
embeddings using MViTac is apparent, where we outperform
CMC by almost 4% on predicting the grasping of unseen
objects. We must note here, that the supervised method of
[4] greatly outperforms both self-supervised modalities. This
discrepancy in performance is expected due to the small size
of the training dataset which is around 18000 samples and
its imbalanced distribution of samples across objects. CL
techniques necessitate bigger and more diverse datasets to
perform comparably with supervised methods.
V. DISCUSSION AND LIMITATIONS
We have demonstrated that our model exhibits better
generalizability across real world datasets. For the material
property classification, we report considerable performance
improvements when visual and tactile data are combined.
The results suggest that while tactile information is adept
at capturing fine-grained material properties, it provides
insufficient information for the accurate classification of
complex surfaces, such as those encountered in uncontrolled
environments. While we acknowledge that self-supervised
approaches may not surpass supervised methods in scenarios
with limited data, as evidenced in our grasp prediction
experiment, efforts are underway to narrow this performance
gap. In the grasp prediction task, we acknowledge that
self-supervised methods do not yet outperform supervised
methods as data is limited; however, efforts are underway
to minimize this performance disparity. Our evaluations
leverage datasets that, albeit comprehensive, might not fully
encapsulate the complexity and variability seen in real-world
scenarios. Extending the evaluation to real robotic systems
and understanding how well the model performs in real-
time tasks is an essential next step. Expanding the scope of
evaluation to include a broader spectrum of tasks, including
more sophisticated manipulation tasks, could provide a more
comprehensive understanding of the model’s capabilities and
limitations. Nonetheless, it is important to note that our
experiments are based on data collected from real-world
tasks, thus affirming the relevance and value of our findings.
VI. CONCLUSIONS
In this paper, we presented MViTac, a novel approach
for incorporating both visual and tactile sensory for vari-
ous tasks. More specifically, our methodology learns inter-
modal and intra-modal representations via self-supervised
learning which leads to more efficient representations. As
a consequence, MViTac consistently surpasses existing self-
supervised methodologies across all the benchmarks and out-
performs supervised learning approaches on material prop-
erty recognition. Furthermore, it outperforms self-supervised
state-of-the-art method on grasping success prediction with
linear probing. Nonetheless, it necessitates further validations
on real robotic platforms to ensure its efficacy in real-world
applications. Our future works will aim at refining the learn-
ing architecture and venturing into real robot experiments to
foster advancements in multimodal robotic tasks.
APPENDIX
A. Implementation Details
For image and tactile modality, we use ResNet-18 [43] as
the backbone. In order to obtain representations, we apply
average pooling on the last layer of the backbone. For all
the encoders i.e. for inter-modal and intra-modal contrastive
learning, we use a 2-layer MLP as projection heads that
converts the 512-dimensional output from the backbone into
a 128-dimensional final representation. We use ReLU [49]
as the activation function for the first layer and no activation
function for the final layer. All the representation vectors
are normalised before calculating the contrastive loss. We
set the temperature τ as {0.07, 0.2, 0.5, 1}. The best results
were obtained when τ was 0.07.
All the networks are trained jointly using ADAM [50]
optimizer with parameters β1 = 0.9, β2 = 0.999, ϵADAM =
10−7. During the pre-training phase, a learning rate of 0.03
is employed, while for the subsequent downstream tasks, a
reduced learning rate of 10−4 is used. To mitigate overfitting,
a dropout layer is integrated into the classifier, featuring a
dropout probability of 0.2, serving as regularization. We use
the batch size of 256 training on a single 4090 GPU for 240
epochs for pre-training and 60 epochs for the downstream
tasks. For the downstream tasks, the projection heads [14],
are bypassed; instead, the encoder’s output is directly fed
into the input layer of the classifier.
B. Augmentations
We follow the standard practice of resizing the images into
256x256 pixels and subsequently normalized using prede-
fined mean and standard deviation metrics. Then the images
are subjected to a randomly resized crop with dimensions of
224x224 pixels, falling within a scale range of 0.2 to 1.0.
Additional stochastic operations include the application of
horizontal flipping with a 50% probability and the conversion
to grayscale with a 20% probability. It should be noted that
the grayscale transformation is excluded from the grasping
task, as it offers little utility in such a controlled experimental
setting.
C. Baseline Methods
In the case of supervised learning, we employ a pair of
ResNet-18 encoders [43] that independently generate tactile
and visual representations, which are subsequently concate-
nated and directly subjected to the classification process. This
strategy diverges from our other baselines, where we employ
a linear classifier post-encoding for task-specific adaptations.
In our study, we adopt a feature dimension of 128 and a
batch size of 128, diverging from SSVTP’s settings of 8
and 512 respectively, due to dataset size and computational
constraints [10]. In our study, the TAG model [11] was
executed as per the original specifications, leveraging the
code made publicly available by the authors. Encoders in
all the baselines are built upon a ResNet-18 backbone [43],
pre-trained on ImageNet [44], with a modification in the
final layer. For the grasping experiment [4], we employed
the implementation available in Tacto [51].
ACKNOWLEDGMENT
This project has received funding from the Deutsche
Forschungsgemeinschaft (DFG, German Research Founda-
tion) - No #430054590 (TRAIN).
REFERENCES
[1]
Chen Hua. “A Review of Visual Perception for Mobile
Robot Navigation: Methods, Limitation, and Appli-
cations”. In: 2022 2nd International Conference on
Algorithms, High Performance Computing and Artifi-
cial Intelligence (AHPCAI). 2022, pp. 729–737. DOI:
10.1109/AHPCAI57455.2022.10087821.
[2]
Md Tanzil Shahria et al. “A Comprehensive Re-
view of Vision-Based Robotic Applications: Current
State, Components, Approaches, Barriers, and Poten-
tial Solutions”. In: Robotics 11.6 (2022). ISSN: 2218-
6581. DOI: 10.3390/robotics11060139. URL:
https://www.mdpi.com/2218-6581/11/6/
139.
[3]
Chiara Bozzacchi, Robert Volcic, and Fulvio Domini.
“Effect of visual and haptic feedback on grasping
movements”. In: Journal of Neurophysiology 112.12
(2014). PMID: 25231616, pp. 3189–3196. DOI: 10.
1152/jn.00439.2014. eprint: https://doi.
org/10.1152/jn.00439.2014. URL: https:
//doi.org/10.1152/jn.00439.2014.
[4]
Roberto Calandra et al. “More Than a Feeling:
Learning to Grasp and Regrasp Using Vision and
Touch”. In: IEEE Robotics and Automation Let-
ters 3 (2018), pp. 3300–3307. URL: https : / /
api . semanticscholar . org / CorpusID :
38553603.
[5]
Benoit P. Delhaye, Katie H. Long, and Sliman J.
Bensmaia. “Neural Basis of Touch and Proprioception
in Primate Cortex”. In: Comprehensive Physiology.
John Wiley & Sons, Ltd, 2018, pp. 1575–1602. ISBN:
9780470650714.
DOI: https : / / doi . org /
10.1002/cphy.c170033. eprint: https://
onlinelibrary . wiley . com / doi / pdf /
10 . 1002 / cphy . c170033. URL: https : / /
onlinelibrary.wiley.com/doi/abs/10.
1002/cphy.c170033.
[6]
Huaping Liu et al. “Visual–Tactile Fusion for Object
Recognition”. In: IEEE Transactions on Automation
Science and Engineering 14.2 (2017), pp. 996–1008.
DOI: 10.1109/TASE.2016.2549552.
[7]
Hao Li et al. “See, Hear, and Feel: Smart Sensory
Fusion for Robotic Manipulation”. In: CoRL. 2022.
[8]
Shan Luo et al. “ViTac: Feature Sharing Between
Vision and Tactile Sensing for Cloth Texture Recog-
nition”. In: 2018 IEEE International Conference on
Robotics and Automation (ICRA). 2018, pp. 2722–
2727. DOI: 10.1109/ICRA.2018.8460494.
[9]
Yizhou Chen et al. “Visuo-Tactile Transformers for
Manipulation”. In: 6th Annual Conference on Robot
Learning. 2022. URL: https : / / openreview .
net/forum?id=JqqSTgdQ85F.
[10]
Justin Kerr et al. Self-Supervised Visuo-Tactile Pre-
training to Locate and Follow Garment Features.
2023. arXiv: 2209.13042 [cs.RO].
[11]
Fengyu Yang et al. “Touch and Go: Learning from
Human-Collected Vision and Touch”. In: Thirty-sixth
Conference on Neural Information Processing Systems
Datasets and Benchmarks Track. 2022.
[12]
Irmak Guzey et al. Dexterity from Touch: Self-
Supervised Pre-Training of Tactile Representations
with Robotic Play. 2023. arXiv: 2303 . 12076
[cs.RO].
[13]
A¨aron van den Oord, Yazhe Li, and Oriol Vinyals.
“Representation Learning with Contrastive Predictive
Coding”. In: CoRR abs/1807.03748 (2018). arXiv:
1807.03748. URL: http://arxiv.org/abs/
1807.03748.
[14]
Ting Chen et al. “A Simple Framework for Contrastive
Learning of Visual Representations”. In: Proceedings
of the 37th International Conference on Machine
Learning. Ed. by Hal Daum´e III and Aarti Singh.
Vol. 119. Proceedings of Machine Learning Research.
PMLR, July 2020, pp. 1597–1607. URL: https://
proceedings.mlr.press/v119/chen20j.
html.
[15]
Kaiming He et al. “Momentum Contrast for Unsu-
pervised Visual Representation Learning”. In: 2020
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR). 2020, pp. 9726–9735. DOI:
10.1109/CVPR42600.2020.00975.
[16]
Jean-Bastien Grill et al. “Bootstrap Your Own La-
tent a New Approach to Self-Supervised Learning”.
In: Proceedings of the 34th International Conference
on Neural Information Processing Systems. NIPS’20.
Vancouver, BC, Canada: Curran Associates Inc., 2020.
ISBN: 9781713829546.
[17]
Mathilde Caron et al. “Unsupervised Learning of
Visual Features by Contrasting Cluster Assignments”.
In: Proceedings of the 34th International Conference
on Neural Information Processing Systems. NIPS’20.
Vancouver, BC, Canada: Curran Associates Inc., 2020.
ISBN: 9781713829546.
[18]
Adrien Bardes, Jean Ponce, and Yann LeCun. “VI-
CReg: Variance-Invariance-Covariance Regularization
For Self-Supervised Learning”. In: ICLR. 2022.
[19]
Roberta L Klatzky, Susan J Lederman, and Victoria
A Metzger. “Identifying objects by touch: An expert
system”. In: Perception & psychophysics 37.4 (1985),
pp. 299–302.
[20]
Ravinder Dahiya et al. “Tactile Sensing—From Hu-
mans to Humanoids”. In: IEEE Transactions on
Robotics 26 (Feb. 2010), pp. 1–20.
[21]
Akihiko Yamaguchi and Christopher G. Atkeson. “Re-
cent progress in tactile sensing and sensors for robotic
manipulation: can we turn tactile sensing into vision?”
In: Advanced Robotics 33.14 (2019), pp. 661–673.
[22]
Zhen Deng et al. “Grasping force control of multi-
fingered robotic hands through tactile sensing for
object stabilization”. In: Sensors 20.4 (2020), p. 1050.
[23]
J. Schill et al. “Learning continuous grasp stability
for a humanoid robot hand based on tactile sensing”.
In: 2012 4th IEEE RAS & EMBS International Con-
ference on Biomedical Robotics and Biomechatronics
(BioRob). 2012, pp. 1901–1906. DOI: 10 . 1109 /
BioRob.2012.6290749.
[24]
Zilin Si et al. “Grasp stability prediction with sim-to-
real transfer from tactile sensing”. In: 2022 IEEE/RSJ
International Conference on Intelligent Robots and
Systems (IROS). IEEE. 2022, pp. 7809–7816.
[25]
Filipe Veiga, Benoni Edin, and Jan Peters. “Grip sta-
bilization through independent finger tactile feedback
control”. In: Sensors 20.6 (2020), p. 1748.
[26]
Akihiko Yamaguchi and Christopher G. Atkeson. “Re-
cent progress in tactile sensing and sensors for robotic
manipulation: can we turn tactile sensing into vision?”
In: Advanced Robotics 33.14 (2019), pp. 661–673.
DOI: 10 . 1080 / 01691864 . 2019 . 1632222.
eprint: https : / / doi . org / 10 . 1080 /
01691864 . 2019 . 1632222. URL: https : / /
doi . org / 10 . 1080 / 01691864 . 2019 .
1632222.
[27]
Qiang Li et al. “A review of tactile information:
Perception and action through touch”. In: IEEE Trans-
actions on Robotics 36.6 (2020), pp. 1619–1634.
[28]
Roberto Calandra et al. “Learning inverse dynamics
models with contacts”. In: 2015 IEEE International
Conference on Robotics and Automation (ICRA).
IEEE. 2015, pp. 3186–3191.
[29]
Filipe Veiga et al. “Tactile based forward modeling for
contact location control”. In: RSS Workshop on Tactile
Sensing for Manipulation. 2017.
[30]
Francois R Hogan et al. “Tactile dexterity: Manipula-
tion primitives with tactile feedback”. In: 2020 IEEE
international conference on robotics and automation
(ICRA). IEEE. 2020, pp. 8863–8869.
[31]
Vedant Dave and Elmar Rueckert. “Predicting full-arm
grasping motions from anticipated tactile responses”.
In: 2022 IEEE-RAS 21st International Conference
on Humanoid Robots (Humanoids). 2022, pp. 464–
471. DOI: 10.1109/Humanoids53995.2022.
9999743.
[32]
Yevgen Chebotar et al. “Self-supervised regrasping us-
ing spatio-temporal tactile features and reinforcement
learning”. In: 2016 IEEE/RSJ International Confer-
ence on Intelligent Robots and Systems (IROS). 2016,
pp. 1960–1966. DOI: 10 . 1109 / IROS . 2016 .
7759309.
[33]
Yi Zheng et al. “Autonomous Learning of Page Flip-
ping Movements via Tactile Feedback”. In: IEEE
Transactions on Robotics 38.5 (2022), pp. 2734–2749.
DOI: 10.1109/TRO.2022.3168731.
[34]
Wenzhen Yuan, Siyuan Dong, and Edward H. Adel-
son. “GelSight: High-Resolution Robot Tactile Sen-
sors for Estimating Geometry and Force”. In: Sensors
17.12 (2017). ISSN: 1424-8220. DOI: 10 . 3390 /
s17122762. URL: https://www.mdpi.com/
1424-8220/17/12/2762.
[35]
Mike Lambeta et al. “Digit: A novel design for a low-
cost compact high-resolution tactile sensor with ap-
plication to in-hand manipulation”. In: IEEE Robotics
and Automation Letters 5.3 (2020), pp. 3838–3845.
[36]
Tito Pradhono Tomo et al. “A New Silicone Structure
for uSkin—A Soft, Distributed, Digital 3-Axis Skin
Sensor and Its Integration on the Humanoid Robot
iCub”. In: IEEE Robotics and Automation Letters 3.3
(2018), pp. 2584–2591. DOI: 10.1109/LRA.2018.
2812915.
[37]
Roberto Calandra et al. “The Feeling of Success: Does
Touch Sensing Help Predict Grasp Outcomes?” In:
CoRR abs/1710.05512 (2017). arXiv: 1710.05512.
URL: http://arxiv.org/abs/1710.05512.
[38]
Yijiong Lin et al. “Tactile Gym 2.0: Sim-to-real
Deep Reinforcement Learning for Comparing Low-
cost High-Resolution Robot Touch”. In: ed. by R. Liu
A.Banerjee. Vol. 7. Proceedings of Machine Learning
Research 4. IEEE, Aug. 2022, pp. 10754–10761. DOI:
10.1109/LRA.2022.3195195. URL: https:
/ / ieeexplore . ieee . org / abstract /
document/9847020.
[39]
Johanna Hansen et al. “Visuotactile-RL: Learning
Multimodal Manipulation Policies with Deep Rein-
forcement Learning”. In: 2022 International Con-
ference on Robotics and Automation (ICRA). 2022,
pp. 8298–8304. DOI: 10 . 1109 / ICRA46639 .
2022.9812019.
[40]
Yunzhu Li et al. “Connecting touch and vision
via cross-modal prediction”. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition. 2019, pp. 10609–10618.
[41]
Fan Zhang and Yiannis Demiris. “Visual-Tactile
Learning of Garment Unfolding for Robot-Assisted
Dressing”. In: IEEE Robotics and Automation Letters
8.9 (2023), pp. 5512–5519. DOI: 10.1109/LRA.
2023.3296371.
[42]
Xin Yuan et al. “Multimodal contrastive training for
visual representation learning”. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 2021, pp. 6995–7004.
[43]
Kaiming He et al. “Deep residual learning for image
recognition”. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. 2016,
pp. 770–778.
[44]
Jia Deng et al. “ImageNet: A large-scale hierarchical
image database”. In: 2009 IEEE Conference on Com-
puter Vision and Pattern Recognition. 2009, pp. 248–
255. DOI: 10.1109/CVPR.2009.5206848.
[45]
Zhirong Wu et al. “Unsupervised feature learning via
non-parametric instance discrimination”. In: Proceed-
ings of the IEEE conference on computer vision and
pattern recognition. 2018, pp. 3733–3742.
[46]
Mang Ye et al. “Unsupervised embedding learning via
invariant and spreading instance feature”. In: Proceed-
ings of the IEEE/CVF conference on computer vision
and pattern recognition. 2019, pp. 6210–6219.
[47]
Xinlei Chen, Saining Xie, and Kaiming He. An Em-
pirical Study of Training Self-Supervised Vision Trans-
formers. 2021. arXiv: 2104.02057 [cs.CV].
[48]
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
“Contrastive multiview coding”. In: Computer Vision–
ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XI 16.
Springer. 2020, pp. 776–794.
[49]
Vinod Nair and Geoffrey E Hinton. “Rectified linear
units improve restricted boltzmann machines”. In:
Proceedings of the 27th international conference on
machine learning (ICML-10). 2010, pp. 807–814.
[50]
Diederik P Kingma and Jimmy Ba. “Adam: A
method for stochastic optimization”. In: arXiv preprint
arXiv:1412.6980 (2014).
[51]
Shaoxiong Wang et al. “TACTO: A Fast, Flexible, and
Open-source Simulator for High-resolution Vision-
based Tactile Sensors”. In: IEEE Robotics and Au-
tomation Letters (RA-L) 7.2 (2022), pp. 3930–3937.
ISSN: 2377-3766. DOI: 10 . 1109 / LRA . 2022 .
3146945. URL: https://arxiv.org/abs/
2012.08456.
"
We investigate the question of whether to walk or run in the rain to remain the least wet using a MATLAB-based simulation. The simulation revealed that moving at higher speeds resulted in fewer raindrop and snowflake collisions. Running parallel to the wind direction with the same speed as raindrops minimizes collisions.,"The popular television program Mythbusters addressed the question of whether to walk or run in the rain to remain the least wet. Our approach simulates movement through rainfall using MATLAB, expanding the scope to include snowfall.",nanPrevious studies have generally indicated that moving at higher speeds is the best strategy for staying dry. Mathematical approximations of human movement through rain have concluded that the best strategy is to run as quickly as possible toward the destination.,"The simulation of precipitation was created using MATLAB R2019a. Variables included number of raindrops/snowflakes, human approximation speed, horizontal wind speed, vertical rain speed, vertical snowfall speed, and angle with respect to the vertical. The human approximation was composed of 200 randomly generated points inside a rectangular grid.","The average number of collisions decreased with increased walking speed, regardless of horizontal wind or vertical rain speed. Walking along the same direction as the horizontal rain vector resulted in fewer raindrop collisions at lower walking speeds than walking in the opposite direction. A local minimum in collisions appeared at 1X wind speed when walking with the net rain vector.","One should run rather than walk in the rain or snow to stay as dry as possible. If moving against the wind, running as fast as possible would result in colliding with the least amount of precipitation. If one moves with the wind, matching the horizontal speed of the raindrops or snowflakes would be the optimal speed to achieve the local minimum of collisions.",A Simulation of Optimal Dryness When Moving in the Rain or Snow Using MATLAB,"Neil Zhao, Emilee Brockner, Asia Winslow, Megan Seraydarian"," 
1 
 
A Simulation of Optimal Dryness When Moving in the Rain or Snow 
Using MATLAB 
 
Neil Zhaoa,*, Emilee Brocknera, Asia Winslowa, Megan Seraydariana 
 
aThomas Jefferson University, Philadelphia, Pennsylvania, USA 
 
 
__________________________________________________________________________________________ 
 
 
The classic question of whether one should walk or run in the rain to remain the least wet has inspired a myriad 
of solutions ranging from physically performing test runs in raining conditions to mathematically modeling 
human movement through rain. This manuscript approaches the classical problem by simulating movement 
through rainfall using MATLAB. Our simulation was generalizable to include snowfall as well. An increase in 
walking speed resulted in a corresponding decrease in raindrop and snowflake collisions. When raindrops or 
snowflakes were given a horizontal movement vector due to wind, a local minimum in collisions was achieved 
when moving in parallel with the same horizontal speed as the raindrop; no local minimum was detected with 
antiparallel movement. In general, our simulation revealed that the faster one moves, the drier one remains.  
 
 
 
 
 
 
 
 
___________________________________ 
*E-mail of corresponding author: neil.zhao@students.jefferson.edu 
 
 
2 
 
Introduction 
 
 
Should one walk or run in the rain to stay the least wet? This question has been a curiosity since at least 
the 1970s [1]. The popular television program Mythbusters featured this problem in one of its earliest episodes 
and came to the conclusion that one should walk rather than run [2]. However, other approaches to this question 
using mathematical approximations of human movement through rain [3–5] have concluded that the best strategy 
is to run as quickly as possible toward your destination. Further studies into this topic have generally indicated 
that moving at higher speeds is the proper choice for staying as dry as possible [6]. 
 
We have approached this problem by constructing a MATLAB based simulation of movement through 
rain. We have also expanded the scope of our simulation to include snowfall. MATLAB is a mathematical 
computational program that is commonly used in academic and industrial environments. As such, it plays a crucial 
role in enlarging the computational arsenal beyond analytical solutions to include numerical solutions. The use of 
MATLAB in approaching this curious problem can therefore be an educational tool to demonstrate the power of 
computational software in answering questions that would otherwise require extensive mathematical rigor.  
 
Through our simulation, we demonstrated that overall moving at higher speeds resulted in fewer collisions 
with raindrops. This also applied to collisions with snowflakes when the simulations conditions were adjusted to 
model snowfall. 
 
 
 
3 
 
Methods 
 
The simulation of precipitation was created using MATLAB R2019a (MathWorks, Natick, MA). 
Precipitation was modeled over a 1 X 1 cartesian grid, with a moving rectangular shape used to approximate a 
human form. Variables that were allowed to be adjusted included the number of raindrops/snowflakes in the field  
 
per frame, speed of the human approximation, horizontal wind speed, vertical rain speed, vertical snowfall speed, 
angle with respect to the vertical for rain and snowfall. The human approximation was composed of 200 randomly 
generated points inside [0,1] x [0,1], which was then compressed along the x-axis to 0.03 the original length and 
along the y-axis to 0.25 the original length. Every simulation began with the human approximation at x=1 and 
moving toward x=0. At every frame, the distance between each of the 200 points that composed the human 
approximation and each raindrop/snowflake was calculated. Any distance less than or equal to 0.01 was 
considered a collision. Any raindrop/snowflake subject to a collision was removed from the simulation and reset 
at a random location along y=1. Any raindrop/snowflake that reached y=0 without a collision was reset at a 
random location along y=1. Any raindrop/snowflake that reached x=0 without a collision was reset to a random 
location along x=1. Any raindrop/snowflake that reached x=1 without a collision was reset to a random location 
along x=0. An option was given to allow oscillatory behavior of the precipitation, but this was not utilized during 
any segment of the simulation. The total number of collisions for each complete run of the human approximation 
A frame of the simulation 
Schematic of rain/snow and wind vectors 
 
4 
 
between x=1 and x=0 was recorded. Each speed of the human approximation was repeated 10 times and averaged. 
Complete MATLAB code for a run with wind speed set to 1/10X of rain speed is provided below with annotations. 
function rainrun(drops,angle,trials,v) 
  
%drops is the number of raindrops 
%angle is the angle of rain/snow 
%trials is the total factor increase in speed of person by increments of 1 
%0.1*v is the speed of person 
  
repeats=10; 
pop=200; 
runs=ceil(1/v); 
wind=-0.001; 
rainspeed=0.01; 
  
rainx=rand(1,drops); 
rainy=rand(1,drops); 
rain=zeros(2*runs,drops); 
angles=(rand(1,drops)*angle*2-angle)*pi/180; 
 
xdis=zeros(pop,drops); 
ydis=zeros(pop,drops); 
  
hits=zeros(trials,runs,repeats); 
  
for k=1:repeats 
    for j=1:trials 
        me=zeros(pop,2,runs); 
        me(:,:,1)=rand(pop,2); 
        me(:,1,1)=0.03*me(:,1,1)+1;me(:,2,1)=0.25*me(:,2,1); 
        i=1; 
        while mean(me(:,1,i))>=0 
            me(:,:,i+1)=me(:,:,i)-j*0.1*v*repmat([1 0],pop,1); 
             
            rain(1,:)=rainx;rain(2,:)=rainy; 
             
            %step is for oscillating the rain drops 
            %step is multiplied after rainspeed below 
            %step=rand(1,drops);step(step<0.5)=1;step(step>=0.5)=1; 
            rain(i*2+1,:)=rain(i*2-1,:)+rainspeed*tan(angles)+wind; 
            rain(i*2+2,:)=rain(i*2,:)-rainspeed; 
            a=find(rain(i*2+2,:)<=0); 
            e1=find(rain(i*2+1,:)<=0);e2=find(rain(i*2+1,:)>=1); 
            rain(i*2+1,a)=rand(1,length(a));rain(i*2+2,a)=ones(1,length(a)); 
            rain(i*2+1,e1)=ones(1,length(e1));rain(i*2+2,e1)=rand(1,length(e1)); 
            rain(i*2+1,e2)=zeros(1,length(e2));rain(i*2+2,e2)=rand(1,length(e2)); 
            angles(1,[a e1 e2])=(rand(1,length([a e1 e2]))*angle*2-angle)*pi/180; 
  
            xdis(:,:)=rain(i*2-1,:)-me(:,1,i);ydis(:,:)=rain(i*2,:)-me(:,2,i); 
  
            dis=sqrt(xdis.^2+ydis.^2); 
            b=dis<=0.01; 
            c=sum(b);d=find(c~=0); 
  
            hits(j,i,k)=sum(length(d)); 
            rain(i*2+1,d)=rand(1,length(d)); 
            rain(i*2+2,d)=ones(1,length(d)); 
  
            scatter(me(:,1,i),me(:,2,i),30,'r','filled') 
            xlim([0 1]) 
            ylim([0 1]) 
            hold on 
            scatter(rain(i*2+1,:),rain(i*2+2,:),'.') 
            hold off 
  
            pause(0.05) 
%             L(i)=getframe(gca); 
            i=i+1; 
        end 
end     
 
5 
 
end 
  
  
ax1=axes(); 
scatter(1:trials,mean(sum(hits,2),3)',20,'k','filled') 
hold on 
errorbar(1:trials,mean(sum(hits,2),3),std(sum(hits,2),1,3),'vertical','LineStyle','none','Color','k') 
hold off 
ylim(ax1,[0 max(mean(sum(hits,2),3))+2*max(std(sum(hits,2),1,3))]) 
xlabel(ax1,""Multiples of "" + 0.1*v/rainspeed + ""X speed of rain""); 
ylabel(ax1,'# of snowflake hits') 
xlim(ax1,[0 trials+1]) 
xticks(ax1,1:trials) 
ax2=axes('Position',get(ax1,'Position'),'XAxisLocation','top','Color','none'); 
ax2.YAxis.Visible='off'; 
xlabel(ax2,""Multiples of "" +abs(0.1*v/wind)+ ""X speed of wind"") 
xlim(ax2,[0 trials+1]) 
xticks(ax2,1:trials) 
set(ax1,'position',get(ax1,'position').*[1 1 1 0.95]) 
set(ax2,'position',get(ax2,'position').*[1 1 1 0.95]) 
  
% video=VideoWriter('Rain running','MPEG-4'); 
% video.FrameRate=50; 
% open(video); 
% writeVideo(video,L); 
% close(video)     
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
6 
 
Results 
 
Based on the average magnitude of rain terminal velocity of 10 m/s [7], wind speed of 1.3 m/s in New 
York City from 2005 to 2023 [8], and human walking speed of 1 m/s [9], the average number of collisions with 
raindrops was simulated as a function of walking and horizontal wind speed. Regardless of horizontal wind or 
vertical rain speed, the average number of collisions decreased with increased walking speed (Fig. 1A-D). 
Additionally, walking along the same x-direction as the horizontal component of the net rain vector, namely, the 
horizontal wind speed, resulted in fewer raindrop collisions (Fig. 1A,C) at lower walking speeds than walking 
along the opposite direction (Fig.1 B,D). With wind speed at 1/10X rain speed, the average number of raindrop 
hits rapidly decreased with increased walking speed, approaching an asymptote at 1.5-2X wind speed when 
walking in the same direction as the horizontal component of the net rain vector (Fig. 1A,C) and at 4-5X 
windspeed when walking in the opposite direction (Fig. 1B,D). Increasing the density of raindrops from 250/field 
(low) to 1,000/field (high) appeared to have only resulted in a proportional increase in raindrop hits. 
 
 
 
 
Figure 1: Simulation of wetness when running in the rain. Wind speed 1/10X of rain speed. 250 raindrops/field in (A) and (B), with x-
direction of running and rain in same direction in (A) and opposite direction in (B). 1000 raindrops/field in (C) and (D), with x-direction 
of running and rain in same direction in (C) and opposite direction in (D). n=10 for each datapoint. Error bars=standard deviation. 
A 
B 
C 
D 
 
7 
 
 
 
With wind speed set to 1/2X rain speed, we noticed the appearance of a local minimum at 1X wind speed 
when walking in the same direction as the horizontal component of the net rain vector (Fig. 2A,C). Following the 
local minimum, the average number of raindrop hits increased slightly, reaching an asymptote at 1.5X wind speed 
for both low and high rain densities. No local minimum appeared when walking in the opposite direction of the 
horizontal component of the net rain vector (Fig. 2C,D). Similarly, an asymptote was approached at 1.5X wind 
speed for both rain densities. With wind speeds set to 1X and 2X rain speeds, similar local minimums appeared 
at 1X wind speed only when walking with the horizontal component of the rain (Fig. 3A,C, Fig. 4A,C). In all 
cases, an increase in the initial walking speed resulted in a rapid decrease in raindrop collisions.  
 
 
 
 
Figure 2: Simulation of wetness when running in the rain. Wind speed 1/2X of rain speed. 250 raindrops/field in (A) and (B), with x-
direction of running and rain in same direction in (A) and opposite direction in (B). 1000 raindrops/field in (C) and (D), with x-direction 
of running and rain in same direction in (C) and opposite direction in (D). n=10 for each datapoint. Error bars=standard deviation. 
 
A 
B 
C 
D 
 
8 
 
 
 
 
 
Figure 3: Simulation of wetness when running in the rain. Wind speed 1X of rain speed. 250 raindrops/field in (A) and (B), with x-
direction of running and rain in same direction in (A) and opposite direction in (B). 1000 raindrops/field in (C) and (D), with x-direction 
of running and rain in same direction in (C) and opposite direction in (D). n=10 for each datapoint. Error bars=standard deviation. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
A 
B 
C 
D 
 
9 
 
 
 
 
 
Figure 4: Simulation of wetness when running in the rain. Wind speed 2X of rain speed. 250 raindrops/field in (A) and (B), with x-
direction of running and rain in same direction in (A) and opposite direction in (B). 1000 raindrops/field in (C) and (D), with x-direction 
of running and rain in same direction in (C) and opposite direction in (D). n=10 for each datapoint. Error bars=standard deviation. 
 
We next adjusted the ratios between wind speed, walking speed, and rain speed to repurpose the simulation 
for snowfall. Based on the average magnitude of snowfall terminal velocity of 1 m/s [10], we simulated the 
average number of snowflake collisions as functions of wind speed and walking speed. With a wind speed of 
1/10X snowfall speed, the number of snowflake hits decreased with increased walking speed (Fig. 5A,B). Walking 
in the same direction as the horizontal component of the net snowfall vector led to an asymptote at 3X windspeed, 
while walking against the horizontal component vector led to an asymptote at 12X windspeed. Additionally, 
walking against the wind resulted in greater snowflake collisions at lower walking speeds. 
A 
B 
C 
D 
 
10 
 
 
 
Figure 5: Simulation of wetness when running in the snow. Wind speed 1/10X of snow speed. 250 snowflakes/field in (A) and (B), with 
x-direction of running and rain in same direction in (A) and opposite direction in (B). n=10 for each datapoint. Error bars=standard 
deviation. 
 
 
With wind speed adjusted to 1X snowfall speed, a local minimum appeared at 1X wind speed when 
walking along the horizontal component of the net snowfall vector (Fig. 6A). Afterwards, the number of 
snowflake hits increased ~10X at a walking speed of 4X wind speed. No local minimum was seen when walking 
against the wind, with an asymptote appearing at 2-3X windspeed. When wind speed was adjusted to 5X and 10X 
snowfall speed, similar local minimums appeared at 1X windspeed only when walking with the horizontal vector 
(Fig. 7A, Fig. 8A). Similar to raindrops, overall, an increase in initial walking speed resulted in a decrease in 
snowflake hits. 
 
 
Figure 6: Simulation of wetness when running in the snow. Wind speed 1X of rain speed. 250 snowflakes/field in (A) and (B), with x-
direction of running and rain in same direction in (A) and opposite direction in (B). n=10 for each datapoint. Error bars=standard 
deviation. 
 
 
 
A 
B 
A 
B 
 
11 
 
 
 
 
Figure 7: Simulation of wetness when running in the snow. Wind speed 5X of snow speed. 250 snowflakes/field in (A) and (B), with x-
direction of running and rain in same direction in (A) and opposite direction in (B). n=10 for each datapoint. Error bars=standard 
deviation. 
 
 
 
Figure 8: Simulation of wetness when running in the snow. Wind speed 10X of snow speed. 250 snowflakes/field in (A) and (B), with 
x-direction of running and rain in same direction in (A) and opposite direction in (B). n=10 for each datapoint. Error bars=standard 
deviation. 
 
In cases of snow flurry, when there is no net wind vector and therefore no net horizontal component of the 
snowfall vector, an increase in the initial walking speed also resulted in a decrease in snowflake collisions, 
reaching an asymptote at 0.6X snowfall speed (Fig. 9A).  
A 
B 
A 
B 
 
12 
 
 
Figure 9: Simulation of wetness when running in the snow. Snow flurry, falling direction randomly between ±45° from vertical. 250 
snowflakes/field. n=10 for each datapoint. Error bars=standard deviation. 
 
 
 
 
13 
 
Discussion 
 
 
The question of whether one should walk or run in the rain to stay as dry as possible has been considered 
many times over the past few decades. Our conclusions using a MATLAB generated simulation of precipitation, 
that 1) moving faster initially always results in decreased wetness, 2) moving in the same direction as the 
horizontal component of the net rain vector, namely, the horizontal wind speed, allows one to approach the 
minimum number of collisions faster than in the opposite direction, and 3) a local minimum in collisions appears 
only when one moves in the same direction as the horizontal component of the net rain vector are in line with 
others who have approach this problem through different methods. 
 
In theory, assuming no horizontal component of rainfall, the minimum number of raindrop collisions is 
approached as one achieves increasingly faster speeds. Moving at many orders of magnitude faster than the speed 
of raindrops would result in one tracing out a cylinder of rain parallel to the direction of movement, while no rain 
collisions are experienced on any orthogonal surfaces. Assuming a constant density of rain per unit time, this 
would be the global minimum amount of raindrop hits.  
Our simulation was based on realistic assumptions of walking speed with respect to rain and wind speed. 
Under these conditions, the horizontal component of rainfall played a role in whether one can achieve a local 
minimum in rain drop hits. Namely, if one moved with the same horizontal direction and magnitude as the net 
rain vector, namely, the horizontal wind speed, an “optimum” number of raindrop collisions can be reached. By 
traversing horizontally at the same speed as the horizontal speed of the raindrops, one would minimize the number 
of collisions on surfaces whose normal vectors are parallel to the direction of motion. The only collisions would 
be experienced on orthogonal surfaces, the cumulative amount of which would be less than the sum of collisions 
along the parallel and orthogonal surfaces at slower or faster speeds. 
 
Using our simulation, we were able to perform the same analysis with snowfall. As snowflakes have a 
much greater surface area than raindrops, they are subject to higher magnitudes of air resistance and thus reach a 
slower terminal velocity earlier. Similar to rain, moving faster initially results in fewer collisions with snowflakes. 
However, a key difference with rainfall is that a global minimum appears to be achievable without approaching 
 
14 
 
very high speeds. The terminal speed of snowfall is comparable to normal human walking speed. Therefore, one 
would experience more collisions by moving at high speeds along the same direction as the horizontal component 
of the net snowfall vector. By moving with the same speed as the horizontal component, the number of collisions 
along the parallel direction of movement is minimized. While the number of collisions along the orthogonal 
direction would increase, the total number of collisions along the parallel and orthogonal surfaces would decrease 
due to the slower terminal speed of snowfall. 
 
In summary, one should run rather than walk in the rain or snow to stay as dry as possible. If one moves 
against the wind, running as fast as possible would result in colliding with the least amount of precipitation. If 
one moves with the wind, matching the horizontal speed of the raindrops or snowflakes would be the optimal 
speed to achieve the local minimum of collisions.     
 
 
15 
 
References 
 
[1] 
 Deakin M A B 1972 Walking in the rain Mathematics Magazine 45 246–53 
 
[2] 
 Anon “MythBusters” Ice Bullet/Exploding Toilet/Who Gets Wetter? (TV Episode 2003) - IMDb 
 
[3] 
 Hailman D and Torrents B 2009 Keeping dry: the mathematics of running in the rain Mathematics 
Magazine 82 266–77 
 
[4] 
 Bocci F 2012 Whether or not to run in the rain Eur. J. Phys. 33 1321–32 
 
[5] 
 Ehrmann A and Blachowicz T 2011 Walking or running in the rain—a simple derivation of a general 
solution Eur. J. Phys. 32 355–61 
 
[6] 
 Zaegel M, Vehils-Vinals M, Guastalla H, Benabou B and Gires A 2024 Should you walk, run or sprint in 
the rain to get less wet? Eur. J. Phys. 45 025802 
 
[7] 
 Anon How fast do raindrops fall? | NASA Global Precipitation Measurement Mission 
 
[8] 
 Anon CNYWeather.com - Wind Summary 
 
[9] 
 Edelstein J E 2012 Lower-limb orthoses for older adults Geriatric Physical Therapy (Elsevier) pp 412–25 
 
[10]  Anon Aggregate Terminal Velocity/Temperature Relations on JSTOR 
 
"
"In the self-supervised monocular depth estimation, the stereo-matching knowledge is distilled into a monocular depth network through pseudo-depth maps. Existing methods often utilize the learning-based stereo-confidence network to identify errors in the pseudo-depth maps for preventing their transfer. However, the networks need to be trained with ground truth (GT), which is not feasible in a self-supervised setting. This paper proposes a method to identify and filter out errors in the pseudo-depth map without the need for GT and extra training by checking the consistency among multiple disparity maps. The results demonstrate that the proposed method outperforms the previous ones and works well on various configurations by filtering out erroneous areas in the stereo-matching, especially texture-less regions, occlusion boundaries, and reflective surfaces.","Monocular depth estimation plays a crucial role in robotics and autonomous driving for the perception of surroundings. Self-supervised learning framework has been paid attention to as a solution to label-induced limitations. Among self-supervised learning methods, stereo-matching knowledge distillation methods have taken advantage of stereo-matching, which is more generalized even when trained only with synthetic datasets. However, the pseudo-depth map may contain errors in the ill-posed regions, which can be transferred to the monocular depth network. To mitigate this, existing training frameworks include a learning-based stereo-confidence network. However, those networks need to be trained with GT and an extra training step. This study proposes a method to filter out errors in the pseudo-depth map for the stereo-matching knowledge distillation-based monocular depth estimation without GT and extra training for stereo-confidence.","Self-supervised monocular depth estimation methods are typically divided into photometric loss-based and stereo-matching knowledge distillation methods. The existing stereo-matching knowledge distillation methods distill the knowledge of a stereo-matching network into a monocular depth network through pseudo-depth maps generated by a pre-trained self-supervised stereo-matching network. Previous methods have focused on identifying unreliable pixels and preventing their use in training, but these methods generally rely on a stereo-confidence network, which necessitates GT and a separate training process.nan",This work proposes a method to filter out errors in the pseudo-depth map provided by a stereo-matching network for stereo-matching knowledge distillation-based monocular depth estimation. The proposed method generates a weight map by checking the consistency between multiple disparity maps instead of using a learning-based stereo-confidence network. The multiple disparity maps are obtained from the stereo-matching network using the disparity plane sweep.,"The proposed method was evaluated on the KITTI Eigen split dataset and the Cityscapes dataset. It was shown to outperform the baseline and previous methods in terms of various metrics, such as Abs Rel, Sqr Rel, RMSE, RMSE log, δ1, δ2, and δ3. Furthermore, the method achieved better qualitative results than the baseline and a previous method.","This study proposed a method to filter out unreliable pseudo-depth in stereo-matching knowledge distillation methods. By harnessing the definition of disparity and multiple disparity maps, the proposed method effectively minimizes the weights of incorrect regions in the pseudo-depth map. Experimental results validate the potential of the method, showing performance improvement without additional networks, GT, and training steps.",Stereo-Matching Knowledge Distilled Monocular Depth Estimation Filtered by Multiple Disparity Consistency,"Woonghyun Ka, Jae Young Lee, Jaehyun Choi, Junmo Kim","STEREO-MATCHING KNOWLEDGE DISTILLED MONOCULAR DEPTH ESTIMATION
FILTERED BY MULTIPLE DISPARITY CONSISTENCY
Woonghyun Ka∗
Hyndai Motor Company
Seoul, South Korea
Jae Young Lee∗
Jaehyun Choi
Junmo Kim
KAIST, Electrical Engineering
Daejeon, South Korea
ABSTRACT
In stereo-matching knowledge distillation methods of the self-
supervised monocular depth estimation, the stereo-matching
network’s knowledge is distilled into a monocular depth net-
work through pseudo-depth maps.
In these methods, the
learning-based stereo-confidence network is generally uti-
lized to identify errors in the pseudo-depth maps to prevent
transferring the errors. However, the learning-based stereo-
confidence networks should be trained with ground truth
(GT), which is not feasible in a self-supervised setting. In
this paper, we propose a method to identify and filter errors
in the pseudo-depth map using multiple disparity maps by
checking their consistency without the need for GT and a
training process. Experimental results show that the proposed
method outperforms the previous methods and works well on
various configurations by filtering out erroneous areas where
the stereo-matching is vulnerable, especially such as texture-
less regions, occlusion boundaries, and reflective surfaces.
Index Terms— monocular depth estimation, deep learn-
ing, self-supervision, stereo-matching
1. INTRODUCTION
Recently, monocular depth estimation has been studied since
it plays a crucial role in robotics and autonomous driving for
the perception of surroundings. However, due to the high cost
of acquiring ground truth (GT) for training, self-supervised
learning framework [1, 2, 3] has been paid attention to as a
solution to label-induced limitations.
Self-supervised learning framework has solved the limi-
tations to some extent and can be divided into two groups: the
photometric loss-based and stereo-matching knowledge dis-
tillation methods. While the photometric loss-based methods
[2, 3] have used photometric loss as an alternative super-
visory signal, stereo-matching knowledge distillation meth-
ods [4, 5, 6, 7] have used the self-supervised stereo-matching
network to distill the knowledge to the monocular depth net-
work through pseudo-depth maps obtained from the stereo-
matching network. Especially in the case of stereo-matching
∗The authors are equally contributed.
knowledge distillation methods, the methods have taken ad-
vantage of stereo-matching, which is more generalized even
when trained only with synthetic datasets.
It is because
stereo-matching learns the correspondence between the left
and right images (disparity), while monocular depth estima-
tion directly learns the depth from a single input image, a
scale-ambiguous problem.
However, since pseudo-depth maps also can contain er-
rors in the ill-posed regions, it is necessary to prevent the er-
ror from being transferred to the monocular depth network.
To deal with the error, the existing training frameworks in-
clude a learning-based stereo-confidence network [8, 9]. Dur-
ing training time, the confidence map is used as a weight map
in loss computation to filter out the errors. The previous meth-
ods [5, 6, 7] applied the fixed or learnable threshold to the
confidence map.
Although the stereo-confidence networks
have helped to improve depth performance, the networks need
to be trained with GT and an extra training step.
In this paper, without GT and extra training for stereo-
confidence, we propose a method to filter out errors in the
pseudo-depth map provided by the stereo-matching network
for the stereo-matching knowledge distillation-based monoc-
ular depth estimation.
To filter out errors, the proposed
method generates a weight map by checking the consis-
tency between multiple disparity maps instead of using the
learning-based stereo-confidence network. The multiple dis-
parity maps are obtained from a stereo-matching network
using the disparity plane sweep, a fundamental algorithm in
stereo-matching. Experimental results show that the proposed
method improves performance on different configurations of
monocular depth networks, stereo-matching networks, and
datasets.
2. RELATED WORK
The self-supervised monocular depth estimation methods
can be divided into two groups: photometric loss-based and
stereo-matching knowledge distillation methods. Since the
proposed method is built upon the stereo-matching knowl-
edge distillation method, we briefly review the existing
stereo-matching knowledge distillation methods.
In the existing stereo-matching knowledge distillation
arXiv:2401.12019v1  [cs.CV]  22 Jan 2024
methods, the knowledge of the stereo-matching network is
distilled into a monocular depth network through pseudo-
depth maps generated by the pre-trained self-supervised
stereo-matching network [4, 5, 6, 7]. However, the pseudo-
depth maps contain errors due to the ill-posed regions. Thus,
previous methods focused on identifying unreliable pixels
and preventing those pixels in training time. Guo et al. [4]
utilized occlusion masks trained by left-right disparity con-
sistency to fine-tune a stereo-matching network trained with
a synthetic dataset on a real dataset. Cho et al. [5] used stereo
confidence network [8] to estimate confidence map with fixed
threshold. Tonioni et al. [6] also used a stereo confidence
network but with a learnable threshold. Choi et al. [7] pro-
posed ThresNet, which adjusts thresholds depending on the
input confidence map. Although the previous methods have
shown remarkable performances, they commonly relied on a
stereo-confidence network, which requires GT and a separate
training process. Unlike them, the proposed filtering method
is not a learning-based method, thereby independent of the
training process such for a stereo-confidence network.
3. PROPOSED METHOD
Figure 1 shows the principle of obtaining depth information
from a stereo camera. To obtain the depth information, using
a stereo camera, a pair of left and right images is captured by
two calibrated cameras assuming a pinhole camera model. In
the stereo camera, the distance between the two cameras is
defined as baseline B, and the distance between the camera
plane and image plane is defined as focal length f. An object
in the real world can be observed in each pixel coordinate
of the left and right images. The object is located in different
positions of each pixel coordinate of the left and right images.
When overlapping two images, the distance in the pixel unit
between each object in the left and right image coordinates is
defined as the disparity d. With the known values of baseline
B and focal length f, if we know the disparity d, the depth
D between an object and a camera plane can be calculated by
means of triangulation.
3.1. Preliminaries and Concept
The stereo-matching aims to find a disparity value in the form
of the 2-D image, i.e., the disparity map. Generally, the dis-
parity map is obtained with respect to the left image as a ref-
erence image. By consecutively shifting the right image with
respect to the left image, the stereo-matching methods calcu-
late the intensity differences or similarity in pixel level be-
tween the left and right images, resulting in the cost volume.
The disparity plane sweep is an algorithm consecutively shift-
ing the right image with respect to the left image.
Generally, in stereo-matching, the disparity plane sweep
is utilized to construct cost volume, which is leveraged as one
of the features. However, unlike the conventional use of the
disparity plane sweep, we utilize the disparity plane sweep to
Camera 
plane
Image
plane
𝐷
𝑓
𝐵
Pinhole camera model
𝐷: 𝐵 = 𝑓: 𝑑
𝐷 = 𝑓𝐵
𝑑
or 
𝑑 = 𝑓𝐵
𝐷
𝐷: Depth
𝐵 : Baseline
𝑓 : Focal length
𝑑 : Disparity
𝑑
Overlap
Disparity plane sweep
Left image
Right image
−2𝑘
𝑑 = 0
Shift:
−𝑘
𝑑 = 𝑘
Shift:
𝑑 = 2𝑘
0
Shift:
+𝑘
𝑑 = 3𝑘
Shift:
Fig. 1. Principle to obtain depth from a stereo camera system.
obtain multiple disparity maps to generate a weight map for
filtering out unreliable pseudo-depth. We focus on the def-
inition of disparity and utilize a rule by the disparity plane
sweep. Let us explain the rule by the disparity plane sweep
using an example (see right side of Fig. 1). Assume that
an object projected in the left and right images is overlapped
by shifting −2k. When the right image is shifted −2k, the
disparity of an object should be zero since the projected ob-
jects are completely overlapped.
On the other side, when
the right image is shifted 0, the disparity should be 2k be-
cause the displacement between objects in the left and right
images is 2k. Using the multiple disparity maps generated
through the disparity plane sweep, we check their consis-
tency to see whether the rule is kept. If the correspondence
of a point between the left and right images is clear (easy to
find), the rule is more likely to be kept across the multiple
disparity maps. If not (hard to find), the rule is less likely
to be kept. If the rule is kept, the prediction can be con-
sidered reliable. The rule was originally introduced in the
light field (LF) [10, 11] with the relationship among the cost-
based, foreground-background separation-based, and model-
based methods according to the shift from a signal process-
ing perspective. Considering the connection between LF and
stereo-matching, the proposed method brings the concept into
stereo-matching and utilizes the rule to generate a weight map
to filter out the unreliable pseudo-depth map obtained by the
stereo-matching network.
3.2. Overall Framework
Fig. 2 shows the overall framework of the proposed method.
Built upon the stereo-matching knowledge distillation meth-
ods, we add the disparity plane sweep to obtain multiple dis-
parity maps and replace the part of generating a weight map
with the proposed method. A set of disparity plane swept
stereo image pairs SDPS = {I−K, ..., Ik, ..., IK} are gener-
Disparity Plane
Sweep
Stereo-Matching
Network
Generating
Weight Map
Monocular Depth
Network
Loss Calculation
𝑊
𝐝𝑠DPS
𝐷𝑠0
𝐷𝑚
ℒ𝐷
𝐒DPS
𝐼𝐿
𝐈0
: backpropagation
: pre-trained, freeze during the training
Fig. 2. Overall framework of the proposed method.
Left image
Predicted disparity map 𝒅𝒔𝟎
Weight map 𝑾
: Ideal case
Disparity
Disparity shift
: Occlusion boundary
Disparity
Disparity shift
: Textureless region
Disparity
Disparity shift
: Reflective surface
Disparity
Disparity shift
: (predicted) Profile obtained by 𝑑𝑠𝑘 − 𝑘
: (desirable) Profile obtained by 𝑑𝑠0
Fig. 3. Disparity profile observation.
ated using input stereo image pair I0 = {IL, I0
R}, where IL
and I0
R denote left and right images, respectively. By shift-
ing the right image by k pixels between the disparity sweep
range k ∈ [−K, K], a disparity plane swept stereo image pair
Ik = {IL, Ik
R} is obtained. Then, a set of disparity maps
dDPS
s
= {d−K
s
, ..., dk
s, ..., dK
s } is acquired by feeding SDPS to
the pre-trained stereo-matching network [12], which is frozen
during the training time. Finally, a pixelwise weight map W
is obtained using dDPS
s
. W is used as a weight map in depth
regression loss LD between the predicted depth map of the
monocular network Dm and the pseudo-depth map D0
s.
3.3. Generating Weight Map
Since dk
s is obtained by the k pixel shifted disparity plane
swept stereo image pair Ik, dk
s can be equal to d0
s in an ideal
case by compensating −k, i.e., d0
s = dk
s − k. Thus, at each
pixel p, the unreliability score map U(p) can be computed by
U(p) =
1
N − 1
X
kn∈KN
∥d0
s(p) − (dkn
s (p) − kn)∥1,
(1)
where KN denotes a set of the N number of disparity planes
with constant step size and kn signifies its nth element. Using
U(p) and the maximum disparity dmax of stereo-matching
network, the weight map W(p) can be obtained by
W(p) = e−σ U(p)
dmax ,
(2)
where a scale factor σ is set to have a W(p) of 0.5 when
U(p) = 1. Figure 3 shows the proposed weight map and pro-
files of the ideal case and the ill-posed regions such as texture-
less region, occlusion boundary, and reflective surfaces. For
easy comparison, the dotted black lines represent d0
s. While
the profile obtained by dk
s − k in the ideal case is shaped into
Table 1. Quantitative evaluation on the KITTI Eigen split
dataset. The best and second-best scores are bold-faced and
underlined, respectively.
Methods
#param.
Metrics - ↓: lower is better, ↑: higher is better.
Abs Rel ↓
Sqr Rel ↓
RMSE ↓
RMSE log ↓
δ1 ↑
δ2 ↑
δ3 ↑
Monodepth [2]
56M
0.138
1.186
5.65
0.234
0.813
0.930
0.969
Monodepth2 [3]
14M
0.109
0.873
4.960
0.209
0.864
0.948
0.975
Uncertainty [13]
14M
0.107
0.811
4.796
0.200
0.866
0.952
0.978
MonoResMatch [14]
41M
0.111
0.867
4.714
0.199
0.864
0.954
0.979
DepthHint [15]
33M
0.102
0.762
4.602
0.189
0.880
0.960
0.981
Choi et al. [7]
29M
0.1
0.644
4.251
0.187
0.882
0.960
0.981
Baseline
18M
0.106
0.803
4.478
0.185
0.881
0.963
0.983
Baseline + Ours
18M
0.098
0.650
4.316
0.181
0.890
0.964
0.983
Table 2.
Four sets of experiments with different configu-
rations. For the backbone of the monocular depth network
(Mono.), VGGNet16 [16] and ResNet18 [17] are used. For
the stereo-matching networks (Stereo), Watson et al.’s method
[12] and Tonioni [18] are used to obtain pseudo-depth maps.
For datasets, KITTI (K) and Cityscape (C) are used. For each
set of experiments, the best scores are bold-faced.
Configurations
Metrics
Mono.
Stereo
Dataset
Ours
Abs Rel ↓
RMSE ↓
δ1 ↑
ResNet18
[12]
K
-
0.113
4.739
0.867
ResNet18
[12]
K
✓
0.105
4.555
0.874
VGGNet16
[18]
K
-
0.103
4.475
0.877
VGGNet16
[18]
K
✓
0.099
4.415
0.882
VGGNet16
[12]
C
-
0.104
5.708
0.897
VGGNet16
[12]
C
✓
0.101
5.633
0.901
a horizontal constant line as d0
s, those in ill-posed regions are
not due to matching ambiguity. By filtering the ill-posed re-
gions, a reliable pseudo-depth can be provided to monocular
depth networks properly.
3.4. Loss Function
The disparity map without shifting (d0
s) is converted to the
pseudo depth map D0
s using focal length f and baseline B,
and the depth regression loss LD is computed by
LD = 1
Z
X
p∈Ω
W(p) · ∥Dm(p) − D0
s(p)∥1,
(3)
where Ω denotes a set of pixel locations and Z = P
p∈Ω W(p).
4. EXPERIMENTS
4.1. Experimental Setup
Eigen split [19] of the KITTI [20] dataset consisting of 24K
images and the Cityscapes [21] dataset consisting of 5K im-
ages are used to train and evaluate the proposed method. We
train the overall framework of the proposed method over 20
epochs with Adam optimizer and batch size of 12 at 192×640
resolution. The learning rate is set to 1e−4 for the first 15
epochs and reduced to 1e−5 for the remaining epochs. In all
experiments, K and N are set to 16 and 5, respectively. For
KITTI evaluation, we set the maximum depth value to 80 me-
ters with Garg crop [1], and quantitative performance is eval-
Choi et al.
Input RGB image
Baseline + Weight map 𝑾
Disparity map 𝒅𝒔𝟎 obtained from a self-supervised stereo-matching network. The disparity maps are converted to depth maps and utilized as pseudo-depth map 𝐷𝑠0. 
Resulting depth map 𝑫𝒎(𝒑) obtained by monocular depth networks 
Baseline
Weight map 𝑾
Fig. 4. Qualitative comparison with the previous methods on the KITTI Eigen split dataset. d0
s and W denote the predicted
disparity map of the stereo-matching network [12] and the weight map of the proposed method (Ours), respectively.
Table 3. Comparison with other weight map generation meth-
ods on the KITTI Eigen split dataset.
Weight map
Metrics
Abs Rel ↓
Sqr Rel ↓
RMSE ↓
RMSE log ↓
δ1 ↑
δ2 ↑
δ3 ↑
× (Baseline)
0.106
0.803
4.478
0.185
0.881
0.963
0.983
CCNN [22]
0.101
0.709
4.295
0.180
0.885
0.964
0.983
LGCNet [23]
0.104
0.707
4.250
0.182
0.882
0.964
0.983
Ours
0.098
0.650
4.316
0.181
0.890
0.964
0.983
uated by various metrics such as Abs Rel, Sqr Rel, RMSE,
RMSE log, and accuracy within thresholds δ1, δ2, and δ3,
which are introduced in Eigen et al.’s method [19]. All ex-
periments are conducted using PyTorch and a machine with
4 NVIDIA TITAN X GPUs. For the baseline, we use U-Net
architecture with VGGNet16 as a backbone network. In all
Tables, ↓ and ↑ denote that the lower and the higher are bet-
ter, respectively.
4.2. Results
In Table 1, we compare the baseline to which the pro-
posed method is applied (Baseline + Ours) to the previous
self-supervised methods using stereo pairs in training time
[2, 3, 13, 14, 15] for the quantitative evaluation on the KITTI
Eigen split. Without the additional parameters, the Baseline
+ Ours achieves better performance except for Sqr Rel and
RMSE, demonstrating its effectiveness and efficiency. The
Baseline + Ours also shows better qualitative results than the
baseline (Baseline) and [7] (see green dotted boxes in Fig.
4). The resulting depth maps of the Baseline + Ours show
sharper results around object boundaries and maintain details,
while the other methods often lose details. The performance
improvement over the baseline supports that the generated
weight map (Ours) prevented the transfer of errors by reduc-
ing the weights of inaccurate regions in pseudo-depth maps
during the training time. In the last row of Fig. 4, weight
maps W (Ours) for d0
s show that the unreliable regions are
remarkably low.
Table 2 shows experiments to validate the effectiveness of
the proposed method with four configurations: 1) Backbone
of the monocular depth network (Mono.), 2) Stereo-matching
network (Stereo), 3) Dataset, and 4) Whether Ours is used or
not. When the proposed method is used, the performances are
consistently improved in different configurations.
In Table 3, we also compared the proposed method with
CCNN [22] and LGCNet [23], which are a learning-based
stereo-confidence method, to check the effectiveness of the
proposed method for generating a weight map. The proposed
method shows comparable performance to CCNN and LGC-
Net, even without extra training. It indicates that the proposed
method effectively filters out the unreliable pseudo-depth.
5. CONCLUSIONS
In this paper, we propose a method for filtering out unre-
liable pseudo-depth in the stereo-matching knowledge dis-
tillation methods. Based on the definition of disparity, us-
ing the multiple disparity maps, the proposed method effec-
tively gives low weights to the incorrect regions in the pseudo-
depth map. Experimental result shows the potential of the
proposed method through performance improvement without
additional networks, GT and training steps.
Acknowledgements. This work was partly supported by In-
stitute of Information & communications Technology Plan-
ning & Evaluation (IITP) grant funded by the Korea govern-
ment(MSIT) (No.2021-0-02068, Artificial Intelligence Inno-
vation Hub) and Artificial intelligence industrial convergence
cluster development project funded by the Ministry of Sci-
ence and ICT(MSIT, Korea)&Gwangju Metropolitan City.
6. REFERENCES
[1] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian
Reid, “Unsupervised cnn for single view depth estima-
tion: Geometry to the rescue,” in Proc. ECCV. Springer,
2016, pp. 740–756.
[2] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J Bros-
tow, “Unsupervised monocular depth estimation with
left-right consistency,” in Proc. CVPR, 2017, pp. 270–
279.
[3] Cl´ement Godard, Oisin Mac Aodha, Michael Firman,
and Gabriel J Brostow, “Digging into self-supervised
monocular depth estimation,” in Proc. CVPR, 2019, pp.
3828–3838.
[4] Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren,
and Xiaogang Wang,
“Learning monocular depth by
distilling cross-domain stereo networks,”
in Proc.
ECCV, 2018, pp. 484–500.
[5] Jaehoon Cho, Dongbo Min, Youngjung Kim, and
Kwanghoon Sohn,
“A large rgb-d dataset for semi-
supervised monocular depth estimation,” arXiv preprint
arXiv:1904.10230, 2019.
[6] Alessio Tonioni, Matteo Poggi, Stefano Mattoccia, and
Luigi Di Stefano,
“Unsupervised domain adaptation
for depth prediction from images,”
IEEE Trans. Pat-
tern Anal. Mach. Intell., vol. 42, no. 10, pp. 2396–2409,
2019.
[7] Hyesong Choi, Hunsang Lee, Sunkyung Kim, Sunok
Kim, Seungryong Kim, Kwanghoon Sohn, and Dongbo
Min, “Adaptive confidence thresholding for monocular
depth estimation,”
in Proc. ICCV, October 2021, pp.
12808–12818.
[8] Matteo Poggi and S. Mattoccia, “Learning from scratch
a confidence measure,” in Proc. BMVC, 2016.
[9] Sunok Kim, Seungryong Kim, Dongbo Min, and
Kwanghoon Sohn,
“Laf-net: Locally adaptive fusion
networks for stereo confidence estimation,”
in Proc.
CVPR, 2019.
[10] Jae Young Lee and Rae-Hong Park, “Reduction of alias-
ing artifacts by sign function approximation in light field
depth estimation based on foreground–background sep-
aration,” IEEE Signal Process. Lett., vol. 25, no. 11, pp.
1750–1754, 2018.
[11] Jae Young Lee and Rae-Hong Park, “Complex-valued
disparity: Unified depth model of depth from stereo,
depth from focus, and depth from defocus based on the
light field gradient,” IEEE Trans. Pattern Anal. Mach.
Intell., vol. 43, no. 3, pp. 830–841, 2021.
[12] Jamie Watson, Oisin Mac Aodha, Daniyar Turmukham-
betov, Gabriel J Brostow, and Michael Firman, “Learn-
ing stereo from single images,”
in Proc. ECCV.
Springer, 2020, pp. 722–740.
[13] Matteo Poggi, Filippo Aleotti, Fabio Tosi, and Ste-
fano Mattoccia, “On the uncertainty of self-supervised
monocular depth estimation,” in Proc. CVPR, 2020.
[14] Fabio Tosi, Filippo Aleotti, Matteo Poggi, and Stefano
Mattoccia, “Learning monocular depth estimation in-
fusing traditional stereo knowledge,” in Proc. CVPR,
June 2019.
[15] Jamie Watson, Michael Firman, Gabriel J Brostow, and
Daniyar Turmukhambetov, “Self-supervised monocular
depth hints,” in Proc. ICCV, 2019, pp. 2162–2171.
[16] Karen Simonyan and Andrew Zisserman, “Very deep
convolutional networks for large-scale image recogni-
tion,” arXiv preprint arXiv:1409.1556, 2014.
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
Proc. CVPR, 2016, pp. 770–778.
[18] Alessio Tonioni, Oscar Rahnama, Thomas Joy, Luigi Di
Stefano, Thalaiyasingam Ajanthan, and Philip HS Torr,
“Learning to adapt for stereo,” in Proc. CVPR, 2019,
pp. 9661–9670.
[19] David Eigen, Christian Puhrsch, and Rob Fergus,
“Depth map prediction from a single image using a
multi-scale deep network,” in Advances in Neural In-
formation Processing Systems, 2014, vol. 27.
[20] Andreas Geiger, Philip Lenz, Christoph Stiller, and
Raquel Urtasun,
“Vision meets robotics:
The kitti
dataset,” Int. J. Robot. Res., 2013.
[21] Marius Cordts, Mohamed Omran, Sebastian Ramos,
Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele,
“The
cityscapes dataset for semantic urban scene understand-
ing,” in Proc. CVPR, 2016.
[22] Matteo Poggi and Stefano Mattoccia, “Learning from
scratch a confidence measure,” in Proceedings of the
British Machine Vision Conference (BMVC), Edwin
R. Hancock Richard C. Wilson and William A. P. Smith,
Eds. September 2016, pp. 46.1–46.13, BMVA Press.
[23] Fabio Tosi, Matteo Poggi, Antonio Benincasa, and Ste-
fano Mattoccia, “Beyond local reasoning for stereo con-
fidence estimation with deep learning,”
in Proceed-
ings of the European Conference on Computer Vision
(ECCV), September 2018.
"
"It is necessary to develop efficient DNNs deployed on edge devices with limited computation resources. However, the compressed networks often execute new tasks in the target domain, which is different from the source domain where the original network is trained. It is important to investigate the robustness of compressed networks in two types of data distribution shifts: domain shifts and adversarial perturbations. In this study, we discover that compressed models are less robust to distribution shifts than their original networks. Interestingly, larger networks are more vulnerable to losing robustness than smaller ones, even when they are compressed to a similar size as the smaller networks. Furthermore, compact networks obtained by knowledge distillation are much more robust to distribution shifts than pruned networks. Finally, post-training quantization is a reliable method for achieving significant robustness to distribution shifts, and it outperforms both pruned and distilled models in terms of robustness.","Over the past few years, the field of deep learning has witnessed a remarkable surge in the development of large-scale models [1, 2]. The deeper and wider architectures of neural networks improve their performance but demand considerable storage and computational resources, preventing them from being deployed on edge devices [3, 4], e.g., mobile phones, smartwatches, IoT devices, etc. This is because these devices often have limited resources in terms of memory, processing power, and energy consumption, which makes it challenging to run large models efficiently.","Network Pruning: The comprehensive literature reviews on compressing DNNs are presented in [21, 22]. Weight pruning is a simple compression method that removes redundant weights of neural networks [23], which converts a dense network into a sparse one [5]. Filter pruning is a structured way to reduce computation costs in CNNs without sparse networks or special hardware [24] which is usually required for compressing and accelerating convolutional layers [25]. In addition, filter pruning is faster and more efficient than weight pruning by removing entire channels instead of pruning a single neuron connection. Knowledge Distillation: Knowledge distillation [8, 26, 27] is a popular compression method where knowledge is transferred from a large “teacher” model to a smaller “student” model. Mimicking the teacher’s performance, the lightweight student model achieves competitive results in tasks like visual recognition and natural language processing. The knowledge type, distillation algorithm, and teacher-student architectures are crucial in this process. The vanilla KD [8] uses the teacher model’s logits to train the student model with a distillation loss, which aligns the predictions between the two models. Quantization: Neural network quantization is a technique that can reduce the model size and lower computation overheads by decreasing the precision of parameter representation without sacrificing accuracy [28], i.e., converting a 32-bit float precision to an 8-bit integer representation.nan","We use three primary compression techniques to reduce the size of various ResNets [47], discussed below.","The ResNet baselines are achieved after fine-tuning ResNets pre-trained models on the corresponding source domain. Then, we compress the baseline models using the L1-FP with different pruning rates and distill the baselines to different sizes of student models using vanilla KD. Tab. 1 records the validation accuracies of various baselines and compressed ResNets to the training distribution, which is one of the three domains in the Office-31 dataset. The results show that the baseline models perform well in the source domain they were trained on. However, as we compress the model with the filter pruning technique, the validation accuracy of the compressed model in the source domain gets worse as the pruning ratio increases.","This paper explores the robustness of compressed networks to various distribution shifts using the Office-31 dataset. We observe that the compressed models perform worse in the unseen domain as the compression ratio increases due to distribution shifts. The results indicate that as the compression ratio increases, the compressed models perform more poorly in the unseen domain due to distribution shifts. Furthermore, we discover that compressed networks originating from smaller models demonstrate better generalization abilities in the target domain, indicating that they are more robust to distribution shifts compared to networks that were originally as large.",Robustness to distribution shifts of compressed networks for edge devices,"Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark","Robustness to distribution shifts of compressed networks for edge devices
Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark
McGill University
Montreal, Quebec, Canada
lulan.shen@mail.mcgill.ca, james.j.clark@mcgill.ca
Abstract
It is necessary to develop efficient DNNs deployed on
edge devices with limited computation resources. However,
the compressed networks often execute new tasks in the tar-
get domain, which is different from the source domain where
the original network is trained. It is important to inves-
tigate the robustness of compressed networks in two types
of data distribution shifts: domain shifts and adversarial
perturbations. In this study, we discover that compressed
models are less robust to distribution shifts than their orig-
inal networks. Interestingly, larger networks are more vul-
nerable to losing robustness than smaller ones, even when
they are compressed to a similar size as the smaller net-
works. Furthermore, compact networks obtained by knowl-
edge distillation are much more robust to distribution shifts
than pruned networks. Finally, post-training quantization
is a reliable method for achieving significant robustness to
distribution shifts, and it outperforms both pruned and dis-
tilled models in terms of robustness.
1. Introduction
Over the past few years, the field of deep learning has
witnessed a remarkable surge in the development of large-
scale models [1, 2].
The deeper and wider architectures
of neural networks improve their performance but demand
considerable storage and computational resources, prevent-
ing them from being deployed on edge devices [3, 4], e.g.,
mobile phones, smartwatches, IoT devices, etc. This is be-
cause these devices often have limited resources in terms of
memory, processing power, and energy consumption, which
makes it challenging to run large models efficiently.
To overcome this challenge, various network compres-
sion methods have been developed to reduce the memory
footprint and computational complexity while preserving
the model’s accuracy. These techniques can be categorized
into different main approaches, including network pruning
(i.e., weight pruning [5], filter pruning [6, 7]), knowledge
distillation (KD) [8], quantization [9–12], low-rank approx-
imation [13–15] and compact network design [16,17].
However, it is challenging to make compressed neural
networks perform well in the target domain. Typically, the
original networks are trained on a source domain, e.g., Im-
ageNet dataset [18], but compressed networks execute new
tasks in a target domain, which has the same classes as the
source domain but lacks labeled training data. Therefore,
there is always a distribution shift between the source and
target domains due to various factors, e.g., pose positions,
image quality, sensor noise, domain-specific characteristics,
etc. [19]. In this paper, we specifically define data distribu-
tion shifts as situations where data samples used for training
and testing are sourced from different datasets or domains.
Iofinova et al. [20] investigated the transfer performance
of pruned convolutional neural networks (CNNs) by eval-
uating the image classification accuracy of these networks
in transfer learning tasks. Specifically, they fine-tuned pre-
trained and compressed models on ImageNet on different
downstream tasks, which contained classes different from
those in ImageNet. However, the study did not evaluate the
performance of pruned CNNs or other types of compressed
models under distribution shifts between training and appli-
cation distributions with the same classes.
Further research is required to investigate the perfor-
mance of compressed models under distribution shifts, par-
ticularly when target domain data is scarce and unlabeled.
In such cases, it is challenging to train a standalone net-
work, which emphasizes the need to address distribution
shifts to ensure the model’s performance in the target do-
main. Furthermore, deploying neural networks on edge de-
vices is prevalent in real-life scenarios, and the effect of dis-
tribution shifts on compressed neural networks cannot be
ignored. Therefore, it is crucial to study the effect of differ-
ent compression techniques on neural network robustness to
distribution shifts, enabling people to prioritize a compres-
sion technique that prioritizes model robustness.
We empirically investigate the robustness of compressed
object classification networks to input data distribution
shift, using three commonly-used network compression
techniques: filter pruning, KD, and post-training quantiza-
1
arXiv:2401.12014v1  [cs.LG]  22 Jan 2024
tion. Our objective is to identify if any of them have an
advantage over the others in terms of handling distribution
shifts. This study examines two distribution shifts: domain
shifts due to input images acquired under varying environ-
mental conditions, and adversarial attacks, where input data
is perturbed to alter the network’s classification results.
Our findings reveal that the performance of compressed
models in target domains degrades as their compression ra-
tio increases. Additionally, compressing from a larger base
model instead of a smaller one to achieve a particular size
of the compact model results in a less robust performance
in distribution shifts. Importantly, we conclude that post-
training quantization is the most effective compression tech-
nique for handling distribution shifts, especially in the case
of domain shifts. Particularly, when compressing the model
to one-quarter of its original size is sufficient, post-training
quantization proves to be a favorable approach.
To summarize, our contributions are:
• Evaluating adversarial and domain shift robustness of
compressed ResNets using pruning, KD, and quantiza-
tion on the Office-31 dataset.
• Investigating the effect of compression rate on the ro-
bustness of pruned and distilled models.
• Evaluating the robustness of compressed models under
a wide range of attacks in different strengths.
2. Related Works
2.1. Model Compression
Network Pruning:
The comprehensive literature re-
views on compressing DNNs are presented in [21, 22].
Weight pruning is a simple compression method that re-
moves redundant weights of neural networks [23], which
converts a dense network into a sparse one [5]. Filter prun-
ing is a structured way to reduce computation costs in CNNs
without sparse networks or special hardware [24] which is
usually required for compressing and accelerating convo-
lutional layers [25]. In addition, filter pruning is faster and
more efficient than weight pruning by removing entire chan-
nels instead of pruning a single neuron connection.
Knowledge Distillation: Knowledge distillation [8, 26,
27] is a popular compression method where knowledge
is transferred from a large “teacher” model to a smaller
“student” model.
Mimicking the teacher’s performance,
the lightweight student model achieves competitive results
in tasks like visual recognition and natural language pro-
cessing. The knowledge type, distillation algorithm, and
teacher-student architectures are crucial in this process. The
vanilla KD [8] uses the teacher model’s logits to train the
student model with a distillation loss, which aligns the pre-
dictions between the two models.
Quantization: Neural network quantization is a tech-
nique that can reduce the model size and lower compu-
tation overheads by decreasing the precision of parameter
representation without sacrificing accuracy [28], i.e., con-
verting a 32-bit float precision to an 8-bit integer represen-
tation. Han et al. [14] introduced an influential technique
that combines pruning, quantization, and Huffman coding
to compress DNNs. Network binarization is another popu-
lar quantization approach, which constrains the weights and
activations of neural networks to be binary values [9–11].
2.2. Adversarial Attacks
In recent years, there has been an increasing amount of
research aimed at developing techniques to deceive neural
networks. These techniques, known as adversarial attacks,
involve making malicious yet subtle changes in the input to
fool the network (See Fig. 1) [29–33]. Alternately, various
techniques have been developed to enhance the robustness
of neural networks against such attacks [30, 34–36]. Also,
several works have introduced techniques to improve the
adversarial robustness of compressed models by combining
adversarial training [30] and compression [37–40].
Moreover, empirical and theoretical analyses presented
in [41] suggested that the sparsity and non-linearity of
DNNs improve their adversarial robustness. Later, Jordao
et al. [42] empirically showed that pruning improves ad-
versarial robustness without investigating the effect of the
pruning rate. Shumailov et al. [43] studied the transferabil-
ity of adversarially generated data samples between com-
pressed and uncompressed models in addition to the adver-
sarial robustness of the compressed models. However, they
only investigated pruning, quantization, and a few gradient-
based adversarial attacks.
By measuring the adversarial robustness of binarized
neural networks, Galloway et al. [44] revealed that quan-
tization causes gradient masking [45], which improves the
robustness of quantized models against gradient-based at-
tacks. In addition, Lin et al. [46] concluded that the robust-
ness of quantized models is improved against small pertur-
bations but reduced against large perturbations. They also
introduced defensive quantization to improve the adversar-
ial robustness of quantized models.
3. Methodology
3.1. Model Compression
We use three primary compression techniques to reduce
the size of various ResNets [47], discussed below.
3.1.1
Pruning
We apply L1-norm Filter Pruning (L1-FP) [24] for prun-
ing. This technique assumes filters or neurons with smaller
weights are less important in the model. Each filter, denoted
as Fi∈[1,I], has J weight elements in a model with I filters.
2
(a) Original: Speaker
(b) C&W: computer
(c) DeepFool: chair
(d) PGD: lamp
(e) No perturbation
(f) C&W perturbation
(g) DeepFool perturbation
(h) PGD perturbation
Figure 1. Subplots (a)-(d) show misclassified images of a speaker from the Amazon domain by ResNet-50 under C&W, DeepFool, and
PGD adversarial attacks. Subplots (e)-(h) show the corresponding perturbations generated under attacks, magnified by a factor of 500.
The importance score for each filter is calculated using:
Score(Fi) = ||Fi|| =
J
X
j=1
|Fi,j|.
(1)
L1-FP measures the sum of absolute weights for all filters
in a well-trained model and sorts the filters based on the
relative importance score of filters in each layer. The least
important filters in each layer are then pruned to achieve
a desired compression ratio. Finally, the pruned model is
trained again on the source domain.
3.1.2
Knowledge Distillation
We use vanilla KD [8], where we train the large teacher
model on the source domain and compute the logits for each
class using the softmax output layer, as shown below:
pi(τ) =
eti/τ
PN
j=1 etj/τ ,
qi(τ) =
esi/τ
PN
j=1 esj/τ ,
(2)
where ti and si are the logits generated by the classifica-
tion layers of the teacher and student network, respectively,
given a data sample for the class i; N is the number of
classes; and τ is the temperature parameter, which is set
to 1 by default. The higher τ value softens the probability
distribution output over classes. The “soft targets” of the
teacher and student model are outputs of the softmax lay-
ers: p = (p1, .., pN) and q = (q1, .., qN) respectively. We
use the class probabilities generated by the teacher model
as “soft targets” for student model training, transferring
the generalization ability of the larger teacher model to the
compact student model.
The objective function is calculated as below:
LKD = CrossEntropy(p, q) = −
N
X
i=1
pτ
i log qτ
i ,
(3)
LCE = CrossEntropy(y, q) = −
N
X
i=1
yi log qi,
(4)
Ltotal = ατ 2LKD + (1 − α)LCE,
(5)
where y is the one-hot vector of the ground-truth label, and
α is the weighting hyper-parameter.
LCE measures the
classification loss of the student model, while LKD encour-
ages the student model’s predicted probabilities to match
the teacher model’s probabilities.
3.1.3
Quantization
We also apply network quantization, which reduces the pre-
cision of computations and weight storage by using lower
bit-widths instead of floating-point precision. We choose
post-training static quantization (PTSQ) [48], which is one
of the most common and fastest quantization techniques
in practice. This technique determines the scales and zero
points prior to inference. Specifically, we quantize the 32-
bit weights (e.g., w ∈ [α, β]) and activations of the trained
baseline models to 8-bit integer values (e.g., wq ∈ [αq, βq]).
The quantization process is defined as
wq = round
1
sw + z

,
(6)
where s is the scale, and d is the zero-point, defined as
s =
β − α
βq − αq
,
z = round
βαq − αβq
β − α

.
(7)
3.2. Domain Shifts
In this study, we evaluate our approach on the Office-
31 dataset [49] (shown in Fig. 2). This dataset is a widely
used benchmark for domain adaptation research. It consists
3
of 4,110 images of 31 object classes from an office envi-
ronment, divided into three image domains: Amazon (DA),
Webcam (DW ), and DSLR (DD). The DA domain contains
2,871 medium-resolution images with studio lighting con-
ditions, downloaded from the Amazon website; the DW do-
main has 795 low-resolution images with significant noise
and white balance artifacts, captured with a simple webcam;
the DD domain contains 498 low-noise high-resolution im-
ages captured with a digital SLR camera in realistic envi-
ronments with natural lighting conditions [49]. Each do-
main is split into 90% train data and 10% validation data
using a random seed of 1, and the model is trained on the
train data. We report the validation accuracy on the source
domain and the testing accuracy on the entire target domain.
We perform six domain shifts in total, where we train on one
Office-31 domain and test on one of the other two domains.
Figure 2. Sample images of backpack, bike, and bookcase from
Amazon, Webcam, and DSLR domain of the Office-31 [49].
3.3. Adversarial Attacks
We evaluate the robustness of models against seven com-
monly used adversarial attacks, briefly discussed below.
• Fast Gradient Sign Method (FGSM): This method
generates an adversarial sample, xadv, by adding the
scaled gradient of the loss function (∇xJ(x, y, θ)) to
the input image (x) [29]. Equation (8) shows how this
method generates the adversarial data, where y repre-
sents the image label, θ represents the model parame-
ters, and ϵ is the perturbation strength.
xadv = x + ϵsign(∇xJ(x, y, θ))
(8)
• Projected Gradient Decent (PGD): This method [30]
is an iterative version of FGSM. The input image xt is
updated to xt+1 using
xt+1 =
Y
x+S
(xt + δsign(∇xJ(x, y, θ))),
(9)
where δ is the step size, and S is the set of possible
perturbations.
• DeepFool: This method approximates the model with
a linear classifier and measures the minimum distance
required to reach the decision boundary of each class.
Finally, the adversarial sample is obtained by adding
the minimum distance to the input image [33].
• Decoupled Direction and Norm (DDN): This attack
is an iterative approach that refines the noise added to
the input image in each iteration to make it adversarial.
At iteration i, the adversarial input image, xi, is gener-
ated as xi = x + ηi, where ηi is the noise with a norm
of σi. If xi is adversarial, the norm of the next iteration
noise is decreased (σi+1 = σi(1 − ϵ)). Otherwise, the
norm of the next noise is increased (σi+1 = σi(1+ϵ)).
This process repeats until the minimum required per-
turbation is found [31].
• Carlini and Wagner (C&W): This is considered one
of the strongest adversarial attacks as it aims to find
the minimum perturbation needed to change the pre-
dicted class of an input image. C&W reformulates the
mentioned optimization problem to find the adversarial
sample by using an optimizer [32] like Adam [50].
• Elastic-Net Attacks to DNNs (EAD): This technique
uses Iterative Shrinkage-Thresholding Algorithm [51]
to solve the same optimization problem that C&W is
trying to solve.
• Salt&Pepper: A non-gradient-based attack that re-
peatedly adds Salt & Pepper noise to the input to fool
the model.
We evaluate the strength of adversarial attacks by vary-
ing the perturbation strength (ϵ) and assessing both light and
heavy attacks based on the accuracy drop of ResNet-18. All
compressed models obtained through pruning and KD are
evaluated against all discussed attacks under both light and
heavy attacks, including gradient-based and non-gradient-
based ones. However, the PTSQ framework does not sup-
port backward path calculations, so we only evaluate the
quantized models against EAD, C&W, and Salt&Pepper.
4. Experimental Setup
We initialize ResNets [52] with various depths (i.e.,
ResNet-18, -34, -50, -101, and -152) using pre-trained pa-
rameters on ImageNet [18] from PyTorch [53].
To ob-
tain our baseline models (i.e., baseline-A, baseline-W, and
baseline-D), we fine-tuned these models on each source do-
main (i.e., DA, DW , DD). The input images are resized
to 224 × 224 and the same data transforms as the pre-
trained models are used. We use a stochastic gradient de-
scent (SGD) optimizer [54] with a momentum of 0.9, a fixed
learning rate (LR) of 1e-4, a batch size of 8 for {DA, DW }
and of 16 for DD, and trained for 100 epochs. We select the
4
fine-tuned models with the best validation accuracy as our
baseline models. These baseline models are then evaluated
directly on the target domain.
To obtain compact models, we compress the baseline
models using L1-FP pruning [24], KD [8], and PTSQ [48].
First, we apply L1-FP pruning to the baseline models, using
a single sensitivity hyper-parameter to control the pruning
ratios in each ResNet block. The compressed models are
then re-trained from scratch on the source domain for 100
epochs. We use an SGD optimizer with a momentum of 0.9,
a weight decay of 1e-4, and a batch size of 8. For ResNet-
50 and ResNet-152, we used a fixed LR of 1e-4, while for
ResNet-18, ResNet-34, and ResNet-101, we used an adap-
tive LR with a base LR of 1e-3, divided by 10 at epoch
70. For KD, we use the larger baseline models (i.e., al-
ready fine-tuned on the source domain) as our teacher mod-
els and the pre-trained smaller ResNet models (provided by
PyTorch) as our student models. We set α as 0.2 and τ as 10.
We train the student models for 100 epochs using an SGD
optimizer with a momentum of 0.9, a weight decay of 1e-3,
and a batch size of 8. We use a step LR scheduler, which
divided the base LR by 5 at epoch 70. For PTSQ, we use the
built-in quantization modules provided by PyTorch, which
allows us to fuse modules, calibrate the model using the
training data to determine the appropriate scale factor, and
finally quantize the weights and activation in the model. We
choose hyper-parameters for the pruned and distilled mod-
els that maximize their validation accuracy.
To evaluate the performance of compressed models and
baselines against adversarial attacks, we use the fine-tuned
checkpoints on Amazon (i.e., ResNets baseline-A and its
compressed models). Then we use Foolox1 [55,56] to gen-
erate an adversarial test dataset for each model, using the
original test dataset obtained by splitting 0.1 of randomly
selected data samples from DA.
The batch size for the
adversarial experiments is set to 8. We use default hyper-
parameters for most of the attacks, except for EAD, C&W,
and Salt&Pepper, which require 100, 100, and 400 steps,
respectively. Also, we set the step size of C&W to 0.001.
We implement our models and experiments using Py-
Torch and run them on V100LGPU. Hyper-parameters are
chosen using a basic grid search that tunes batch size, base
LR, step size, and gamma for the step LR scheduler, weight
decay of the SGD optimizer, α, and τ for KD.
5. Results and discussion
5.1. Domain Shifts
The ResNet baselines are achieved after fine-tuning
ResNets pre-trained models on the corresponding source
domain. Then, we compress the baseline models using the
L1-FP with different pruning rates and distill the baselines
1https://github.com/bethgelab/foolbox
to different sizes of student models using vanilla KD. Ta-
ble 1 records the validation accuracies of various baselines
and compressed ResNets to the training distribution, which
is one of the three domains in the Office-31 dataset. The
results show that the baseline models perform well in the
source domain they were trained on. However, as we com-
press the model with the filter pruning technique, the valida-
tion accuracy of the compressed model in the source domain
gets worse as the pruning ratio increases. In contrast, the
distilled models (i.e., the ResNet-18 student model) gener-
ally have better training performance than the model with
the same structure and size (i.e., the ResNet-18 baseline
model). This is expected since the larger teacher model
transfers its generalization ability to the student model.
Next, we evaluate the baseline and compressed/distilled
models on the target domains and report their accuracies
in Tab. 2. The results show that both baseline and com-
pressed models of ResNets perform poorly on the target
domains and their performance decreases as the compres-
sion rate increases, indicating a lack of robustness to do-
main shifts. However, after pruning their ResNet baseline
models to a particular size, the smaller baseline model has
a higher test accuracy on the target domains than the larger
baseline networks, which are less affected by domain shifts.
For example, the compressed ResNet with 4.51M param-
eters pruned from ResNet-18 has an average accuracy of
66.24% on target domains, while the pruned models with
the same size obtained from the base models ResNet-34,
ResNet-50, ResNet-101, and ResNet-152 have average ac-
curacies of 29.86%, 25.22%, 24.10%, and 20.06% respec-
tively. Similar patterns are observed for other compressed
network sizes. This suggests that it is more beneficial to
prune a smaller base model rather than a larger one to a par-
ticular size since its corresponding compressed model has
better generalization performance on unseen domains.
However, the situation differs for networks compressed
through KD. In this case, the student models obtained us-
ing KD can possess similar or even better performance in
target domains compared to the small baseline network of
the same size. For instance, consider the networks with a
parameter count of 11.19M. The average accuracy for the
unpruned baseline is 68.76%. However, as shown in Tab. 2,
the accuracy of student models with 11.19M is generally su-
perior to that of the small baseline network, and the decline
in performance when using larger teachers is minimal.
Furthermore, Tab. 3 demonstrates the performance of
quantized 8-bit ResNets trained on the Amazon domain.
The quantization process reduces the size of the model to
one-quarter of its original size without any significant loss
or degradation of accuracy in target domains. On average,
quantized ResNet models, compressed to 1/4 of their orig-
inal size, experience only a slight decrease in their target-
domain accuracy of about 1.2%. In contrast, models pruned
5
Table 1. The validation accuracies (%) of ResNets models on each source domain of the Office-31 dataset. The baseline (uncompressed)
ResNets are obtained after fine-tuning the pre-trained model on the ImageNet dataset. The pruned models are obtained using the L1-FP
method with different pruning ratios, and the distilled/student models are obtained using teacher networks of different sizes.
Base/Teacher Model
Compression Method
# Params (M)
Compression Rate (%)
Amazon
Webcam
DSLR
Avg Acc
ResNet-18
-
11.19
-
89.32
89.87
93.88
91.02
FP
4.51
59.70
89.68
87.34
87.76
88.26
ResNet-34
-
21.30
-
90.75
89.87
93.88
91.50
FP
11.19
47.46
85.41
89.87
93.88
89.72
FP
4.51
78.83
75.44
78.48
79.59
77.84
KD
ResNet18 (11.19)
47.46
90.04
91.14
97.96
93.05
ResNet-50
-
23.57
-
90.04
89.87
97.96
92.62
FP
21.30
9.63
92.17
89.87
97.96
93.33
FP
11.19
52.52
87.90
86.08
93.88
89.29
FP
4.51
80.87
75.44
83.54
81.63
80.20
KD
ResNet34 (21.30)
9.63
90.04
89.87
97.96
92.62
KD
ResNet18 (11.19)
52.52
90.39
91.14
97.96
93.16
ResNet-101
-
42.56
-
91.81
91.14
97.96
93.64
FP
23.57
44.62
90.04
89.87
97.96
92.62
FP
11.19
73.71
86.12
88.61
97.96
90.90
FP
4.51
89.40
72.95
84.81
77.55
78.44
ResNet-152
-
58.21
-
90.75
87.34
95.92
91.34
FP
42.56
26.89
91.46
91.14
95.92
92.84
FP
23.57
59.51
89.68
88.61
97.96
92.08
FP
11.19
80.78
82.56
88.61
95.92
89.03
FP
4.51
92.25
68.68
81.01
85.71
78.47
KD
ResNet18 (11.19)
80.78
90.04
91.14
95.92
92.37
Table 2. The test accuracies (%) of ResNets on target domains of the Office-31 dataset other than what they were trained on.
Base/Teacher Model
Compression Method
# Params (M)
A → W
A → D
W → A
W → D
D → A
D → W
Avg Acc
ResNet-18
-
11.19
64.15
60.84
50.05
97.39
47.43
92.70
68.76
FP
4.51
57.36
59.64
47.39
96.99
44.73
91.32
66.24
ResNet-34
-
21.30
67.80
68.27
54.67
98.59
52.01
92.70
72.34
FP
11.19
48.93
51.61
46.22
95.78
43.66
89.69
62.65
FP
4.51
16.73
14.06
16.68
63.86
10.47
57.36
29.86
KD
11.19
62.52
63.25
48.49
99.00
52.79
95.47
70.25
ResNet-50
-
23.57
68.93
71.89
64.25
99.00
60.95
94.84
76.64
FP
21.30
69.43
72.69
61.52
98.39
59.53
94.34
75.98
FP
11.19
53.46
61.45
41.36
93.57
35.64
85.28
61.79
FP
4.51
19.37
26.10
11.89
50.00
9.23
34.72
25.22
KD
21.30
57.74
59.04
56.59
98.80
58.18
97.11
71.24
KD
11.19
57.99
57.63
52.18
99.40
48.78
95.47
68.58
ResNet-101
-
42.56
74.59
75.70
64.32
99.00
24.35
90.19
71.36
FP
23.57
62.89
63.45
54.10
98.59
55.80
91.19
71.00
FP
11.19
43.02
40.16
28.19
90.36
19.63
84.65
51.00
FP
4.51
19.37
17.27
12.11
57.83
5.18
32.58
24.10
ResNet-152
-
58.21
74.97
74.70
63.97
98.39
63.76
95.72
78.59
FP
42.56
71.19
72.69
61.95
99.00
61.59
93.46
76.65
FP
23.57
65.41
66.47
55.13
99.00
43.02
86.79
69.30
FP
11.19
36.23
39.36
25.49
81.93
10.69
57.11
41.80
FP
4.51
17.86
15.26
9.83
50.00
4.40
23.02
20.06
KD
11.19
58.49
62.25
48.49
98.39
50.80
95.85
69.05
and distilled from baseline-A, even at a relatively modest
pruning ratio of 50% (i.e., compressing from ResNet-50 to
a model with 11.19M parameters), experience a much larger
decrease in target-domain accuracy of approximately 13%.
This suggests that quantized models exhibit significantly
greater robustness to domain shifts compared to pruned and
6
Table 3. Accuracies of ResNet baselines and quantized 8-bit ResNets trained on the Amazon domain and accuracies of quantized ResNets
for light and heavy adversarial perturbations. Note that a “-” in the compression method column indicates the baseline model is used.
Base
Model
Compression
Method
Memory Size
(MB)
Source
(A)
Target
(W)
Target
(D)
EAD
C&WL2
Salt&Pepper
ϵ = 1
ϵ = 10
ϵ = 0.4
ϵ = 4
ϵ = 1
ϵ = 15
ResNet-18
-
42.79
88.61
63.77
66.87
83.99
50.89
14.23
0.00
85.77
31.67
PTSQ
10.81
88.26
63.02
65.26
87.90
87.90
87.90
87.90
85.77
44.84
ResNet-34
-
81.42
90.39
64.28
68.27
81.49
49.47
14.95
0.71
84.70
50.89
PTSQ
20.55
89.32
64.28
67.47
89.32
88.97
88.97
88.97
87.54
46.98
ResNet-50
-
90.26
90.04
72.45
75.30
84.34
48.04
13.17
0.00
87.19
42.35
PTSQ
23.26
87.19
71.45
72.69
87.90
88.26
87.90
87.54
86.83
58.01
ResNet-101
-
163.03
90.39
75.09
76.71
83.99
57.30
12.10
00.36
87.90
64.41
PTSQ
42.17
89.68
74.21
74.50
87.19
87.54
88.26
86.48
82.92
61.92
ResNet-152
-
223.01
90.75
74.84
76.91
87.54
61.92
15.30
00.36
89.68
65.12
PTSQ
57.81
88.26
73.46
75.90
88.97
88.97
89.32
88.61
86.12
58.72
distilled models. However, further research should inves-
tigate the robustness of quantized 4-bit/2-bit integer neural
networks to better understand their potential benefits.
Taking into account the robustness to domain shift, it can
be concluded that distillation is a better approach to train a
network of a specific size than using pruning to obtain a
smaller network from a larger one. Furthermore, the post-
training quantization method is highly recommended over
network pruning and KD techniques if compressing three-
quarters of the model size is adequate for deployment, and
no further training is required. This is due to the superior ro-
bustness of quantized models in dealing with domain shifts.
5.2. Adversarial Attacks
The accuracy of the quantized ResNets and their corre-
sponding baselines against EAD, C&W, and Salt&Pepper
adversarial attacks are demonstrated in Tab. 3.
Our re-
sults indicate that while the gradient-based attacks, EAD
and C&W, significantly degrade the robustness of baselines,
these attacks barely reduce the accuracy of the quantized
models. For example, the accuracy of the uncompressed
ResNet-18 drops significantly from 88.61% on the clean
data to 50.89% and 0.00% under heavy EAD and C&W
attacks, respectively. However, the accuracy of the quan-
tized ResNet-18 is reduced from 88.26% on the clean data
to 87.90% under the gradient-based attacks. This observa-
tion shows the comparative robustness of quantized mod-
els against gradient-based attacks due to the phenomenon
of gradient masking [24, 44, 45], which hinders the attack-
ers’ ability to find gradients that can mislead the quantized
model. However, adversarial attacks can be specifically de-
signed to overcome gradient masking [57] and challenge the
quantized models.
While our experiments in Tab. 3 revealed that quan-
tized models are vulnerable to non-gradient-based attacks
such as Salt&Pepper, we also observed that quantization
improves the accuracy of smaller models (i.e., ResNet-18
and ResNet-50) compared to their baselines under heavy
Salt&Pepper attacks.
However, deeper models such as
ResNet-152 experienced a reduction in accuracy after quan-
tization.
We also investigate the robustness of models that are
compressed using pruning and KD against light and heavy
adversarial attacks, and the results are presented in Tabs. 4
and 5. In contrast to the quantized models, the pruned and
distilled models are vulnerable to all types of attacks. Com-
paring the accuracy of the pruned models with their corre-
sponding baselines, we observe that, with the exception of
ResNet-152, the pruned models with the lowest pruning rate
outperform the baselines under both light and heavy attacks.
However, the larger size of ResNet-152 and, consequently,
the larger amount of pruned parameters, may explain why
pruning does not improve the adversarial robustness of this
model. We also notice that the average adversarial robust-
ness decreases by increasing the pruning rate.
Furthermore, as shown in both Tabs. 4 and 5, the com-
pressed models that use KD consistently outperform both
their baselines/teacher models and pruned models of the
same size with a significant performance gap between 20%
and 60% under heavy attacks. Moreover, the adversarial ro-
bustness of a ResNet-18, trained by distillation from teach-
ers with different sizes (i.e., ResNet-34, ResNet-50, and
ResNet-152) remains roughly the same against both light
and heavy attacks. This suggests that, unlike pruning, KD
can maintain the model’s adversarial robustness across dif-
ferent compression rates.
A comparison between the quantized models and
pruned/distilled ones is not quite fair since they have dif-
ferent architectures.
However, according to our results
in Sec. 5.1, quantized models performed the best on do-
main shift experiments. Also, according to the accuracy of
compressed models under Salt&Pepper, quantized models
achieved the best results.
7
Table 4. Accuracies of both baseline-A and compressed ResNets under light adversarial perturbations.
Base/Teacher
Model
Compression
Method
# Params
(M)
In-Domain
DA Acc (%)
DeepFoolL∞
(ϵ = 0.0003)
PGDL∞
(ϵ = 0.0003)
FGSM
(ϵ = 0.0003)
C&WL2
(ϵ = 0.03)
DDN
(ϵ = 0.003)
EAD
(ϵ = 1)
Salt&Pepper
(ϵ = 1)
Avg Adversarial
Acc (%)
ResNet-18
-
11.19
89.32
77.93
78.29
79.00
84.34
88.61
83.99
85.77
82.56
FP
4.51
89.68
82.21
82.56
83.63
86.48
87.54
85.77
85.77
84.85
ResNet-34
-
21.30
90.75
76.87
76.51
78.29
83.63
89.32
81.49
84.70
81.54
FP
11.19
85.41
82.21
82.21
82.21
83.95
85.41
82.92
81.85
82.96
FP
4.51
75.44
71.17
71.17
71.17
73.31
75.44
71.17
70.82
72.03
KD
11.19
90.04
87.90
87.90
88.26
89.32
90.39
87.90
86.83
88.36
ResNet-50
-
23.57
90.04
81.85
80.78
83.27
86.83
90.03
84.34
87.19
84.90
FP
21.30
92.17
85.05
84.34
85.41
87.19
91.46
86.12
89.32
86.98
FP
11.19
87.90
66.90
64.06
72.60
75.80
86.83
75.80
82.92
74.99
FP
4.51
75.44
49.11
45.20
54.80
63.34
74.02
59.07
68.68
59.17
KD
21.30
90.04
87.19
87.19
87.19
88.97
90.03
87.54
88.61
88.10
KD
11.19
90.39
87.54
87.54
87.54
89.32
90.75
88.26
87.90
88.41
ResNet-101
-
42.56
91.81
80.07
79.00
80.78
85.05
91.10
83.99
87.90
83.98
FP
23.57
90.04
86.83
86.48
87.19
87.90
90.03
86.48
87.90
87.54
FP
11.19
86.12
81.49
81.14
83.27
84.70
86.12
82.92
83.27
83.27
FP
4.51
72.95
60.14
60.14
60.85
65.12
71.53
60.14
63.70
63.09
ResNet-152
-
58.21
90.75
82.92
81.85
84.70
88.97
91.10
87.54
89.68
86.68
FP
42.56
91.46
81.85
80.43
83.98
85.76
91.46
85.41
88.61
85.36
FP
23.57
89.68
77.93
75.09
79.71
82.56
89.32
82.92
86.12
81.95
FP
11.19
82.56
60.14
57.29
63.70
68.32
81.49
67.97
72.95
67.41
KD
11.19
90.04
85.76
85.76
85.76
87.90
89.68
87.19
85.77
86.83
Table 5. Accuracies of both baseline-A and compressed ResNets under heavy adversarial perturbations.
Base/Teacher
Model
Compression
Method
# Params
(M)
In-Domain
DA Acc (%)
DeepFoolL∞
(ϵ = 0.004)
PGDL∞
(ϵ = 0.004)
FGSM
(ϵ = 0.004)
C&WL2
(ϵ = 0.4)
DDN
(ϵ = 0.24)
EAD
(ϵ = 10)
Salt&Pepper
(ϵ = 15)
Avg Adversarial
Acc (%)
ResNet-18
-
11.19
89.32
2.13
1.78
34.52
14.23
20.64
50.89
31.67
22.27
FP
4.51
89.68
20.28
11.73
49.47
38.43
48.75
62.28
16.73
35.38
ResNet-34
-
21.30
90.75
3.91
0.36
47.69
14.95
18.51
49.47
50.89
26.54
FP
11.19
85.41
40.57
32.38
58.01
48.75
58.36
64.41
40.93
49.06
FP
4.51
75.44
30.60
21.71
46.95
40.21
49.82
48.04
21.71
37.01
KD
11.19
90.04
66.90
61.92
75.44
70.82
76.16
75.80
61.57
69.80
ResNet-50
-
23.57
90.04
0.36
0.00
45.51
13.17
6.05
48.04
42.35
22.21
FP
21.30
92.17
6.76
1.78
53.74
22.06
26.33
57.30
55.16
31.88
FP
11.19
87.90
0.71
0.35
32.38
4.63
1.78
37.37
40.57
16.83
FP
4.51
75.44
0.00
0.00
9.25
0.71
0.00
17.08
12.46
5.64
KD
21.30
90.04
69.03
64.06
73.31
70.82
77.58
76.87
66.55
71.17
KD
11.19
90.39
62.63
58.72
71.89
67.97
75.80
75.44
49.11
65.94
ResNet-101
-
42.56
91.81
6.05
1.78
59.07
12.10
11.39
57.30
64.41
30.30
FP
23.57
90.04
41.28
27.76
66.19
53.38
62.28
70.46
56.23
53.94
FP
11.19
86.12
28.47
17.44
49.82
43.42
53.38
56.94
27.40
39.55
FP
4.51
72.95
2.13
0.71
13.52
10.68
12.81
19.93
6.05
9.40
ResNet-152
-
58.21
90.75
7.12
4.98
63.34
15.30
12.46
61.92
65.12
32.89
FP
42.56
91.46
5.69
1.07
61.56
9.96
7.12
56.23
52.67
27.76
FP
23.57
89.68
1.42
0.71
55.87
7.83
5.34
50.53
35.59
22.47
FP
11.19
82.56
0.35
0.00
29.54
4.27
1.07
22.42
7.12
9.25
KD
11.19
90.04
69.75
65.48
75.80
72.95
78.65
77.94
53.74
70.62
6. Conclusion
This paper explores the robustness of compressed net-
works to various distribution shifts using the Office-31
dataset. We observe that the compressed models perform
worse in the unseen domain as the compression ratio in-
creases due to distribution shifts. The results indicate that
as the compression ratio increases, the compressed models
perform more poorly in the unseen domain due to distri-
bution shifts. Furthermore, we discover that compressed
networks originating from smaller models demonstrate bet-
ter generalization abilities in the target domain, indicating
that they are more robust to distribution shifts compared to
networks that were originally as large.
In terms of compression techniques for neural networks,
the pruning technique is known for generating highly sen-
sitive compressed networks that are vulnerable to domain
shifts and adversarial perturbations.
On the other hand,
compact networks produced through knowledge distillation
are less affected by these issues. It is worth emphasizing
that quantized networks, which are compressed to approx-
imately 25% of their original size, offer significantly more
robustness to distribution shifts, particularly in the case of
domain shifts, than other compressed networks.
8
References
[1] K. Simonyan and A. Zisserman, “Very deep convolutional
networks for large-scale image recognition,” in International
Conference on Learning Representations, 2015. 1
[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille, “DeepLab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected CRFs,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 40, no. 04, pp. 834–848, 2018. 1
[3] H. Chen, Y. Wang, C. Xu, B. Shi, C. Xu, Q. Tian et al.,
“AdderNet: Do we really need multiplications in deep learn-
ing?” in IEEE Conference on Computer Vision and Pattern
Recognition.
IEEE Computer Society, 2020, pp. 1465–
1474. 1
[4] Y. He, P. Liu, Z. Wang, Z. Hu, and Y. Yang, “Filter prun-
ing via geometric median for deep convolutional neural net-
works acceleration,” in IEEE Conference on Computer Vi-
sion and Pattern Recognition.
IEEE Computer Society,
2019, pp. 4335–4344. 1
[5] S. Han, J. Pool, J. Tran, and W. J. Dally, “Learning both
weights and connections for efficient neural networks,” in
International Conference on Neural Information Processing
Systems.
MIT Press, 2015, p. 1135–1143. 1, 2
[6] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz,
“Pruning convolutional neural networks for resource effi-
cient inference,” in International Conference on Learning
Representations, 2017. 1
[7] N. Yu, S. Qiu, X. Hu, and J. Li, “Accelerating convolutional
neural networks by group-wise 2D-filter pruning,” in Inter-
national Joint Conference on Neural Networks, 2017, pp.
2502–2509. 1
[8] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowl-
edge in a neural network,” in Advances in Neural Informa-
tion Processing Systems Workshop, 2014. 1, 2, 3, 5
[9] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi,
“XNOR-Net: ImageNet classification using binary convolu-
tional neural networks,” in Computer Vision – ECCV 2016.
Springer International Publishing, 2016, p. 525–542. 1, 2
[10] H. Qin, R. Gong, X. Liu, X. Bai, J. Song, and N. Sebe, “Bi-
nary neural networks: A survey,” Pattern Recognition, vol.
105, p. 107281, 2020. 1, 2
[11] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio, “Binarized neural networks,” in Advances in Neu-
ral Information Processing Systems, vol. 29.
Curran Asso-
ciates, Inc., 2016. 1, 2
[12] M. Courbariaux, Y. Bengio, and J.-P. David, “BinaryCon-
nect: Training deep neural networks with binary weights
during propagations,” in International Conference on Neu-
ral Information Processing Systems.
MIT Press, 2015, p.
3123–3131. 1
[13] F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J.
Dally, and K. Keutzer, “SqueezeNet: AlexNet-level accuracy
with 50x fewer parameters and <1MB model size,” CoRR,
vol. abs/1602.07360, 2016. 1
[14] S. Han, H. Mao, and W. J. Dally, “Deep compression: Com-
pressing deep neural network with pruning, trained quanti-
zation and huffman coding,” in International Conference on
Learning Representations, 2016. 1, 2
[15] E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus,
“Exploiting linear structure within convolutional networks
for efficient evaluation,” in International Conference on Neu-
ral Information Processing Systems.
MIT Press, 2014, p.
1269–1277. 1
[16] F. Chollet, “Xception: Deep learning with depthwise sepa-
rable convolutions,” in IEEE/CVF Conference on Computer
Vision and Pattern Recognition.
IEEE Computer Society,
2017, pp. 1800–1807. 1
[17] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand et al., “MobileNets: Efficient convolutional neu-
ral networks for mobile vision applications,” CoRR, vol.
abs/1704.04861, 2017. 1
[18] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
“ImageNet: A large-scale hierarchical image database,” in
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2009, pp. 248–255. 1, 4
[19] M. Wang and W. Deng, “Deep visual domain adaptation: A
survey,” Neurocomputing, vol. 312, pp. 135–153, 2018. 1
[20] E. Iofinova, A. Peste, M. Kurtz, and D. Alistarh, “How well
do sparse ImageNet models transfer?”
in Conference on
Computer Vision and Pattern Recognition.
IEEE Computer
Society, Jun. 2022, pp. 12 256–12 266. 1
[21] R. Mishra, H. Prabhat Gupta, and T. Dutta, “A survey on
deep neural network compression: Challenges, overview,
and solutions,” arXiv e-prints, p. arXiv:2010.03954, 2020.
2
[22] Y. Cheng, D. Wang, P. Zhou, and T. Zhang, “A survey of
model compression and acceleration for deep neural net-
works,” arXiv e-prints, p. arXiv:1710.09282, 2017. 2
[23] F. Meng, H. Cheng, K. Li, H. Luo, X. Guo, G. Lu et al.,
“Pruning filter in filter,” in Advances in Neural Information
Processing Systems, vol. 33.
Curran Associates, Inc., 2020,
pp. 17 629–17 640. 2
[24] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf,
“Pruning filters for efficient convnets,” in International Con-
ference on Learning Representations, 2017. 2, 5, 7
[25] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz
et al., “EIE: Efficient inference engine on compressed deep
neural network,” in International Symposium on Computer
Architecture.
IEEE Press, 2016, p. 243–254. 2
[26] X. Suau, u. Zappella, and N. Apostoloff, “Filter distillation
for network compression,” in IEEE Winter Conference on
Applications of Computer Vision, 2020, pp. 3129–3138. 2
[27] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distil-
lation: A survey,” International Journal of Computer Vision,
vol. 129, no. 6, p. 1789–1819, 2021. 2
[28] A. Gholami, S. Kim, D. Zhen, Z. Yao, M. Mahoney, and
K. Keutzer, Low-Power Computer Vision.
Chapman and
Hall/CRC, Jan. 2022, ch. A Survey of Quantization Methods
for Efficient Neural Network Inference, pp. 291–326. 2
9
[29] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and
harnessing adversarial examples,” in International Confer-
ence on Learning Representations, 2015. 2, 4
[30] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu, “Towards deep learning models resistant to ad-
versarial attacks,” in International Conference on Learning
Representations, 2018. 2, 4
[31] J. Rony, L. G. Hafemann, L. S. Oliveira, I. B. Ayed,
R. Sabourin, and E. Granger, “Decoupling direction and
norm for efficient gradient-based l2 adversarial attacks and
defenses,” in IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2019, pp. 4322–4330. 2, 4
[32] N. Carlini and D. Wagner, “Adversarial examples are not
easily detected: Bypassing ten detection methods,” in ACM
workshop on artificial intelligence and security, 2017, pp.
3–14. 2, 4
[33] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deep-
fool: A simple and accurate method to fool deep neural net-
works,” in IEEE/CVF conference on computer vision and
pattern recognition, 2016, pp. 2574–2582. 2, 4
[34] C. Qin, J. Martens, S. Gowal, D. Krishnan, K. Dvijotham,
A. Fawzi et al., “Adversarial robustness through local lin-
earization,” Advances in Neural Information Processing Sys-
tems, vol. 32, 2019. 2
[35] P. Bashivan, R. Bayat, A. Ibrahim, K. Ahuja, M. Faramarzi,
T. Laleh et al., “Adversarial feature desensitization,” Ad-
vances in Neural Information Processing Systems, vol. 34,
pp. 10 665–10 677, 2021. 2
[36] H. Zhang, H. Chen, C. Xiao, S. Gowal, R. Stanforth, B. Li
et al., “Towards stable and efficient training of verifiably ro-
bust neural networks,” in International Conference on Learn-
ing Representations, 2020. 2
[37] M. Goldblum, L. Fowl, S. Feizi, and T. Goldstein, “Adversar-
ially robust distillation,” AAAI Conference on Artificial Intel-
ligence, vol. 34, no. 04, pp. 3996–4003, Apr. 2020. 2
[38] S. Ye, K. Xu, S. Liu, H. Cheng, J.-H. Lambrechts, H. Zhang
et al., “Adversarial robustness vs. model compression, or
both?” in IEEE/CVF International Conference on Computer
Vision, Oct. 2019, pp. 111–120. 2
[39] S. Gui, H. Wang, H. Yang, C. Yu, Z. Wang, and J. Liu,
“Model compression with adversarial robustness: A unified
optimization framework,” in Advances in Neural Information
Processing Systems, vol. 32.
Curran Associates, Inc., 2019.
2
[40] A. W. Wijayanto, J. J. Choong, K. Madhawa, and T. Mu-
rata, “Towards robust compressed convolutional neural net-
works,” in IEEE International Conference on Big Data and
Smart Computing, 2019, pp. 1–8. 2
[41] Y. Guo, C. Zhang, C. Zhang, and Y. Chen, “Sparse dnns with
improved adversarial robustness,” in Advances in Neural In-
formation Processing Systems, vol. 31.
Curran Associates,
Inc., 2018. 2
[42] A. Jord˜ao and H. Pedrini, “On the effect of pruning on adver-
sarial robustness,” in IEEE/CVF International Conference on
Computer Vision Workshop, Oct. 2021, pp. 1–11. 2
[43] I. Shumailov, Y. Zhao, R. Mullins, and R. Anderson, “To
compress or not to compress: Understanding the interac-
tions between adversarial attacks and neural network com-
pression,” Machine Learning and Systems, vol. 1, pp. 230–
240, 2019. 2
[44] A. Galloway, G. W. Taylor, and M. Moussa, “Attacking bi-
narized neural networks,” in International Conference on
Learning Representations, 2018. 2, 7
[45] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik,
and A. Swami, “Practical black-box attacks against machine
learning,” in ACM on Asia conference on computer and com-
munications security, 2017, pp. 506–519. 2, 7
[46] J. Lin, C. Gan, and S. Han, “Defensive quantization: When
efficiency meets robustness,” in International Conference on
Learning Representations, 2019. 2
[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learn-
ing for image recognition,” in IEEE conference on computer
vision and pattern recognition, 2016, pp. 770–778. 2
[48] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard,
H. Adam, and D. Kalenichenko, “Quantization and training
of neural networks for efficient integer-arithmetic-only in-
ference,” in IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2018, pp. 2704–2713. 3, 5
[49] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, “Adapting vi-
sual category models to new domains,” in European Confer-
ence on Computer Vision, 2010. 3, 4
[50] D. P. Kingma and J. Ba, “Adam: A method for stochastic op-
timization,” in International Conference on Learning Repre-
sentations, 2015. 4
[51] A. Beck and M. Teboulle, “A fast iterative shrinkage-
thresholding algorithm for linear inverse problems,” SIAM
Journal on Imaging Sciences, vol. 2, pp. 183–202, 2009. 4
[52] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
for image recognition,” in IEEE Conference on Computer
Vision and Pattern Recognition, 2016, pp. 770–778. 4
[53] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,
G. Chanan et al., “PyTorch: An imperative style, high-
performance deep learning library,” in Advances in Neural
Information Processing Systems, vol. 32.
Curran Asso-
ciates, Inc., 2019. 4
[54] L. Bottou, “Large-scale machine learning with stochastic
gradient descent,” in Computational Statistics.
Physica-
Verlag HD, 2010, pp. 177–186. 4
[55] J. Rauber, W. Brendel, and M. Bethge, “Foolbox: A python
toolbox to benchmark the robustness of machine learning
models,” in International Conference on Machine Learning
Workshop, 2017. 5
[56] J. Rauber, R. Zimmermann, M. Bethge, and W. Brendel,
“Foolbox native: Fast adversarial attacks to benchmark the
robustness of machine learning models in PyTorch, Tensor-
Flow, and JAX,” Journal of Open Source Software, vol. 5,
no. 53, p. 2607, 2020. 5
10
[57] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated gradi-
ents give a false sense of security: Circumventing defenses
to adversarial examples,” in International conference on ma-
chine learning.
PMLR, 2018, pp. 274–283. 7
11
"
"Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients are typically strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and can significantly accelerate convergence for federated classification task, especially when clients are “lazy” and train their models solely for few epochs for next global aggregation. TurboSVM-FL extensively utilizes support vector machine to conduct selective aggregation and max-margin spread-out regularization on class embeddings. We evaluate TurboSVM-FL on multiple datasets including FEMNIST, CelebA, and Shakespeare using user-independent validation with non-iid data distribution. Our results show that TurboSVM-FL can significantly outperform existing popular algorithms on convergence rate and reduce communication rounds while delivering better test metrics including accuracy, F1 score, and MCC.","Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. As one promising distributed machine learning paradigm, federated learning (FL) has been growing at an astounding rate after its introduction (McMahan et al. 2017). In the common FL settings, data is distributed over numerous end clients, while the central server possesses no data by itself. After the server initiates a model and sends the model to clients, each client trains the model locally using its own data. The server periodically aggregates the locally trained models and synchronizes local models of clients with the latest aggregated one. With such a process, FL provides a primary privacy guarantee to a large extent since the server does not require data sharing and is hence preferred in many privacy-preserving scenarios where sensitive data is utilized. Based on the characteristics of participating entities, FL can be further categorized into cross-silo FL and cross-device FL (Kairouz et al. 2021). In cross-silo FL, the target clients are often large-scale institutions such as hospitals, data centers, educational organizations, and high-tech companies. Such stakeholders commonly possess decent resource for computing, storage, and internet connection, while the number of attending institutions is relatively low. Therefore, the probability that each client takes part in all aggregation rounds is high. In contrast to cross-silo FL, cross-device FL focuses more on training on end-user devices like smartphones and personal computers using user data. The scale of participating clients in cross-device FL can be fairly large, while each client may be strongly limited by its computing power and connectivity. As a consequence, only a (small) portion of clients could share their models during a global aggregation.","Despite recent advancements in FL, its implementation in practice is still facing some challenges. Among these, the slow convergence is a primary concern: a significantly greater number of aggregation rounds are often needed to reach convergence compared to non-FL setups. Several factors contribute to this inefficiency according to (Wu et al. 2023), such as client drift caused by data heterogeneity (Karimireddy et al. 2020), lack of adaptive optimizers (Reddi et al. 2020), and the increase in model complexity and data size. One trivial solution proposed in (McMahan et al. 2017) is to increase client computation load by a larger number of local training iterations. Although this solution vastly speeds up the convergence, it multiplies the computation load on the client side, which can be problematic when clients are constrained by computing power, like in cross-device FL case. Other existing solutions mainly target either client drift or adaptive optimization. The former often requires to optimize additional objective functions on the client side, which in turn also increases the client computation load and even the need for storage, while the latter can be particularly hard to tune because it is often necessary to decide the choice of optimizers and learning rates jointly between server and clients. There also exist solutions that require additional data on the server side, which may increase the risk of privacy leaks if not handled properly.nan","In this paper, we propose a novel federated aggregation strategy for classification tasks called TurboSVM-FL. TurboSVM-FL induces no additional computation cost or storage consumption on the client side compared to the vanilla FL algorithm, and shows great potential in reducing communication rounds, especially when clients are “lazy” and train their local models for very few iterations. TurboSVM-FL extensively exploits support vector machine (SVM) to conduct selective aggregation and max-margin spread-out regularization. By adopting adaptive methods in max-margin spread-out regularization, TurboSVM-FL can also benefit from adaptive optimization. The main contributions of this work can be summarized as follows:
• We introduce a novel perspective to interpret classification in FL setting and a model-as-sample strategy, which lay the foundation for further FL improvements such as selective aggregation and outlier detection.
• We propose a novel federated aggregation algorithm named TurboSVM-FL that vastly boosts convergence for federated classification tasks using deep neural networks. The proposed method extensively exploits support vector machine (SVM) to conduct selective model aggregations and max-margin spread-out regularization.
• We conduct experiments on various benchmarks in user-independent validation and show the potential of TurboSVM-FL in reducing communication cost. The benchmarks contain three different tasks covering image classification and natural language processing with non-iid data distribution over clients.","We benchmarked TurboSVM-FL on three datasets against six FL algorithms, including FedAvg (McMahan et al. 2017), FedAdam (Reddi et al. 2020), FedAMS (Wang, Lin, and Chen 2022), FedProx (Li et al. 2020), MOON (Li, He, and Song 2021), and FedAwS (Yu et al. 2020). In this section, we provide task descriptions and results. For more details such as reproducibility and model structures, we redirect readers to the Appendix and our GitHub repository1.
1 https://github.com/Kasneci-Lab/TurboSVM-FL.","In this work, we proposed a novel federated aggregation strategy called TurboSVM-FL, which extensively exploits SVM to conduct selective aggregation and max-margin spread-out regularization for class embeddings and can vastly reduce communication rounds. We tested our approach on three publicly available datasets, and our results show that TurboSVM-FL outperforms existing FL methods largely on convergence rate regarding various metrics.",TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients,"Mengdi Wang, Anna Bodonhelyi, Efe Bozkir, Enkelejda Kasneci","TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy
Clients
Mengdi Wang*, Anna Bodonhelyi*, Efe Bozkir, Enkelejda Kasneci
Chair for Human-Centered Technologies for Learning, Technical University of Munich, Munich, Bavaria, Germany
{mengdi.wang, anna.bodonhelyi, efe.bozkir, enkelejda.kasneci}@tum.de
Abstract
Federated learning is a distributed collaborative machine
learning paradigm that has gained strong momentum in re-
cent years. In federated learning, a central server periodically
coordinates models with clients and aggregates the models
trained locally by clients without necessitating access to local
data. Despite its potential, the implementation of federated
learning continues to encounter several challenges, predom-
inantly the slow convergence that is largely due to data het-
erogeneity. The slow convergence becomes particularly prob-
lematic in cross-device federated learning scenarios where
clients may be strongly limited by computing power and stor-
age space, and hence counteracting methods that induce addi-
tional computation or memory cost on the client side such as
auxiliary objective terms and larger training iterations can be
impractical. In this paper, we propose a novel federated ag-
gregation strategy, TurboSVM-FL, that poses no additional
computation burden on the client side and can significantly
accelerate convergence for federated classification task, espe-
cially when clients are “lazy” and train their models solely
for few epochs for next global aggregation. TurboSVM-FL
extensively utilizes support vector machine to conduct se-
lective aggregation and max-margin spread-out regularization
on class embeddings. We evaluate TurboSVM-FL on multi-
ple datasets including FEMNIST, CelebA, and Shakespeare
using user-independent validation with non-iid data distribu-
tion. Our results show that TurboSVM-FL can significantly
outperform existing popular algorithms on convergence rate
and reduce communication rounds while delivering better test
metrics including accuracy, F1 score, and MCC.
Introduction
With the increasing importance of data privacy, a giant stride
in distributed machine learning has been observed in re-
cent years. As one promising distributed machine learning
paradigm, federated learning (FL) has been growing at an
astounding rate after its introduction (McMahan et al. 2017).
In the common FL settings, data is distributed over numer-
ous end clients, while the central server possesses no data by
itself. After the server initiates a model and sends the model
to clients, each client trains the model locally using its own
data. The server periodically aggregates the locally trained
*These authors contributed equally.
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
models and synchronizes local models of clients with the
latest aggregated one. With such a process, FL provides a
primary privacy guarantee to a large extent since the server
does not require data sharing and is hence preferred in many
privacy-preserving scenarios where sensitive data is utilized.
Based on the characteristics of participating entities, FL
can be further categorized into cross-silo FL and cross-
device FL (Kairouz et al. 2021). In cross-silo FL, the tar-
get clients are often large-scale institutions such as hos-
pitals, data centers, educational organizations, and high-
tech companies. Such stakeholders commonly possess de-
cent resource for computing, storage, and internet connec-
tion, while the number of attending institutions is relatively
low. Therefore, the probability that each client takes part in
all aggregation rounds is high. In contrast to cross-silo FL,
cross-device FL focuses more on training on end-user de-
vices like smartphones and personal computers using user
data. The scale of participating clients in cross-device FL
can be fairly large, while each client may be strongly limited
by its computing power and connectivity. As a consequence,
only a (small) portion of clients could share their models
during a global aggregation. An additional critical aspect of
cross-device FL is that each client might only contain data
collected from a single user, which exacerbates data hetero-
geneity across clients.
Despite recent advancements in FL, its implementation
in practice is still facing some challenges. Among these,
the slow convergence is a primary concern: a significantly
greater number of aggregation rounds are often needed
to reach convergence compared to non-FL setups. Sev-
eral factors contribute to this inefficiency according to (Wu
et al. 2023), such as client drift caused by data heterogene-
ity (Karimireddy et al. 2020), lack of adaptive optimiza-
tion (Reddi et al. 2020), and the increase in model complex-
ity and data size. One trivial solution proposed in (McMa-
han et al. 2017) is to increase client computation load by
a larger number of local training iterations. Although this
solution vastly speeds up the convergence, it multiplies the
computation load on the client side, which can be problem-
atic when clients are constrained by computing power, like
in cross-device FL case. Other existing solutions mainly tar-
get at either client drift or adaptive optimization. The former
often requires to optimize additional objective functions on
the client side, which in turn also increases the client com-
arXiv:2401.12012v1  [cs.LG]  22 Jan 2024
Figure 1: Left: pipeline of TurboSVM-FL. Right: test performance of TurboSVM-FL against FedAvg. E indicates the number
of client local training epochs. The results were obtained on FEMNIST dataset using a suboptimal client learning rate.
putation load and even the need for storage, while the latter
can be particularly hard to tune because it is often necessary
to decide the choice of optimizers and learning rates jointly
between server and clients. There also exist solutions that re-
quire additional data on the server side, which may increase
the risk of privacy leaks if not handled properly.
In this paper, we focus on embedding-based neural net-
work classifiers, which means the input samples are en-
coded into the same space as class representations and
their similarity determines the probability of the sample
belonging to the class (Yu et al. 2020). We propose a
novel federated aggregation strategy for classification tasks
called TurboSVM-FL. TurboSVM-FL induces no additional
computation cost or storage consumption on the client side
compared to the vanilla FL algorithm, and shows great po-
tential in reducing communication rounds, especially when
clients are “lazy” and train their local models for very
few iterations. TurboSVM-FL extensively exploits the prop-
erty of support vector machine (SVM) and consists of
three steps. Firstly, TurboSVM-FL reformalizes classifica-
tion while treating client models in a model-as-sample way,
and fits SVMs using latent class embeddings. Then, it con-
ducts selective aggregation on latent class features that form
support vectors. Lastly, TurboSVM-FL applies max-margin
spread-out regularization on aggregated class representa-
tions upon SVM hyperplanes. By adopting adaptive meth-
ods in max-margin spread-out regularization, TurboSVM-
FL can also benefit from adaptive optimization. The main
contributions of this work can be summarized as follows:
• We introduce a novel perspective to interpret classifica-
tion in FL setting and a model-as-sample strategy, which
lay the foundation for further FL improvements such as
selective aggregation and outlier detection.
• We propose a novel federated aggregation algorithm
named TurboSVM-FL that vastly boosts convergence for
federated classification tasks using deep neural networks.
The proposed method extensively exploits support vector
machine (SVM) to conduct selective model aggregations
and max-margin spread-out regularization.
• We conduct experiments on various benchmarks in
user-independent validation and show the potential
of TurboSVM-FL in reducing communication cost. The
benchmarks contain three different tasks covering image
classification and natural language processing with non-
iid data distribution over clients.
Related Work
Federated Learning
The concept of FL was originally introduced in (McMahan
et al. 2017). Unlike centralized learning where the goal is
to fit an optimal model on a collection of data, FL aims
to train a model that delivers superior performance across
data subsets. In the remaining of this work, we narrow
down our focus to federated classification tasks. In a fed-
erated classification task with K classes and N clients, de-
note the local dataset of each client as D1, ..., DN with
Dn = {(x, y)}, n ∈ [N], where (x, y) ∈ RP × [K] is
a sample point of class y ∈ [K]. Let DG = UN
n=1 Dn
with |DG| = PN
n=1 |Dn| describe the collection of local
datasets and ℓ(x, y, θ) be the objective function measured on
the sample (x, y) with model θ. Then, the goal of centralized
learning is to find an optimal model θ∗ that satisfies:
θ∗ = argmin
θ
E(x,y)∼DG[ℓ(x, y, θ)]
(1)
In contrast, FL aims to fit a model that performs optimally
across clients:
θ∗ = argmin
θ
N
X
n=1
|Dn|
|DG|E(x,y)∼Dn[ℓ(x, y, θ)]
(2)
The typical workflow of FL can be broken down into three
steps. First, a server initializes a model and broadcasts this
model to all clients. Then, each client trains the received
model on its own dataset for E epochs and sends the trained
model back to the server. In the next step, the server aggre-
gates locally trained models into a new global model and
synchronizes clients with the latest global model. The last
two steps are repeated for multiple rounds until convergence.
The first federated aggregation algorithm, FedAvg, was in-
troduced in (McMahan et al. 2017) and applies weighted av-
erage over client models:
θG =
N
X
n=1
|Dn|
|DG|θn, θn = argmin
θ
E(x,y)∼Dn[ℓ(x, y, θ)]
(3)
where θG and θn denote the aggregated global model and
the local model of n-th client, respectively.
One of those challenges that FL is facing is the large
amount of aggregation rounds needed to approach conver-
gence. While increasing local training iterations E can sig-
nificantly advance convergence, it also vastly increases the
computation load on the client side, which can be extremely
problematic in cross-device FL. Many follow-up works aim
to speed up FL convergence, and they can be mainly cate-
gorized into two groups. The first group endeavor to address
client drift (Karimireddy et al. 2020) caused by data hetero-
geneity, while the other group attempt to benefit FL with
adaptive learning methods, which we describe as follows.
Client Drift in Federated Learning
Client drift (Karimireddy et al. 2020) describes the phe-
nomenon that client local models approach individual lo-
cal optima rather than global optima and their average is
drifted away from global optima as well, which is caused
by data heterogeneity across client local datasets and can
dramatically impact convergence behavior. Various recent
works attempt to solve client drift on the client side. SCAF-
FOLD (Karimireddy et al. 2020) introduces a control variate
term to stabilize gradient update. FedProx (Li et al. 2020)
proposes an additional loss term based on L2 distance be-
tween global model θG and client model θn during local
training. MOON (Li, He, and Song 2021) and FedProc (Mu
et al. 2023) suggest the use of contrastive learning to combat
data heterogeneity. The former introduces an objective based
on latent features extracted respectively by the global model,
current client model, and previous client model, while the
latter penalizes the dissimilarity between latent features and
class representations. A common drawback of the aforemen-
tioned methods lies in that they increase either computation
burden or memory consumption or even both on the client
side, which can be quite a challenge for end-user devices
like smartphones and tablets.
There also exists works that aim to solve client drift on
the server side. For instance, FedAwS (Yu et al. 2020) ad-
dresses an extreme data distribution case where clients may
have only data from a single class. FedAwS utilizes spread-
out regularizer (Zhang et al. 2017) and raises a penalty term
based on cosine similarities among class embeddings on the
server side.
Adaptive Federated Learning
In centralized learning, advanced adaptive and momentum-
based optimization techniques such as AdaGrad (Duchi,
Hazan, and Singer 2011), Adam (Kingma and Ba 2014), and
Yogi (Zaheer et al. 2018) have shown great success in con-
vergence acceleration. In contrast, in vanilla FedAvg, client
models are trained with stochastic gradient descent (Robbins
and Monro 1951; Kiefer and Wolfowitz 1952) and server
aggregation is (weighted) averaging. Numerous works have
been devoted to benefiting FL with advanced server-side op-
timizers. As a forerunner in this field, (Reddi et al. 2020)
proposed a family of adaptive aggregation methods called
FedOpt. Different from weighted average in Equation 3,
FedOpt computes pseudo-gradient (Chen and Huo 2016;
Nichol, Achiam, and Schulman 2018) from client models
and updates the global model with a chosen optimizer:
∆ ←
N
X
n=1
|Dn|
|DG|θn − θG
(4)
θG ← server optimizer(θG, −∆, η)
(5)
where η indicates the learning rate. Depending on the
choice of optimizer, FedOpt can be derivated into multi-
ple variants. For instance, in (Reddi et al. 2020), the re-
searchers introduced FedAdaGrad, FedAdam, and FedYogi
with their names indicating the choice of optimization tech-
nique. FedAMS (Wang, Lin, and Chen 2022) suggests the
use of AMSGrad (Reddi, Kale, and Kumar 2019) optimizer
on the server side, which is an improved version of Adam.
According to (Wang et al. 2021a), it can be hard to tune
FedOpt-family methods due to the additional implementa-
tion of optimizer on the server side, and it is often necessary
to search for optimal learning rates jointly for client opti-
mizer and server optimizer.
Compared to server-level adaptive learning, adaptive op-
timization on the client side is less studied. Client adaptiv-
ity poses its own challenges, particularly due to the poten-
tial for the states of client optimizers to significantly di-
verge from each other as a result of data heterogeneity. To
address this challenge, (Wang et al. 2021b) proposes to re-
set client optimizer status in each global round, while Lo-
calAMSGrad (Chen, Li, and Li 2020) and FAFED (Wu
et al. 2023) suggest the sharing and aggregation of client
optimizer states similarly to client models. These methods
can be meaningless if clients are limited by computation re-
source and can only train their local models for few epochs.
Support Vector Machine
Support vector machine (SVM) (Cortes and Vapnik 1995)
is a widely-used and robust supervised learning model that
can be used for both regression and classification tasks. Un-
like traditional linear classifiers, where the decision bound-
ary is a linear combination of all data points, the separating
hyperplane of SVM is a combination of selected samples,
which are also called support vectors and lie the closest to
the decision boundary. While common classifiers minimize
solely the classification objective, SVM struggles to con-
trol the trade-off between discriminative error minimization
and margin maximization while allowing some misclassifi-
cations. The margin refers to the distance between the sup-
port vectors of different classes and the decision boundary.
The primal problem of SVM can be formalized as:
argmin
w,ζ1,...,ζm
1
2||w||2+λ
m
X
i=1
ζi, s.t. yiwτxi ≥ 1−ζi and ζi ≥ 0
(6)
where ζi defines the distance of a misclassified sample to its
correct margin plane. The coefficient λ controls the magni-
tude of regularization. A smaller λ prioritizes larger margins
and may result in a greater number of support vectors.
It is important to note that there are several prior works
that integrate FL and SVM, such as (Bakopoulou, Tillman,
and Markopoulou 2021; Navia-V´azquez, D´ıaz-Morales, and
Fern´andez-D´ıaz 2022; Bemani and Bj¨orsell 2022). Our ap-
proach is distinctly different from them in the sense that our
algorithm leverages SVM to improve global aggregation and
offers a solution to the problem “how to FL”. In contrast, in
previous works, SVM serves as the core model to be trained
in FL and thus addresses the question of “what to FL”.
Methodology
In this paper, we propose a novel federated aggregation al-
gorithm for classification task called TurboSVM-FL that is
able to boost convergence significantly. TurboSVM-FL so-
phisticatedly leverages SVM to conduct selective aggrega-
tion and max-margin spread-out regularization. By adopting
an adaptive optimizer, TurboSVM-FL can also benefit from
adaptivity. Compared to vanilla FedAvg (McMahan et al.
2017), TurboSVM-FL requires no additional computation,
storage, or communication cost on the client side. A pseu-
docode for TurboSVM-FL is given in Algorithm 3, and a
graphical illustration is depicted Figure 1.
In the following, we present our algorithm in detail, start-
ing by reformalizing the classification task as “finding near-
est neighbor”. We reduce our discussion to embedding-
based deep learning networks and ignore the logit activation,
which means the model θ can be divided into two parts: g
and W, with fθ(x) = Wg(x), where g : RP → Rd is the
feature encoder that maps input x ∈ RP to a latent repre-
sentation in Rd and W is the last projection layer containing
class embeddings. In a classification task with K classes,
W will be of shape RK×d and is also called logit layer.
Then, the class inference ˆy of a sample (x, y) is indiffer-
ent from finding the nearest neighbor to g(x) in w1, ..., wK
with wk ∈ Rd being the k-th row in W indicating the class
embedding of class k. Implicitly, the metric used to measure
distance is vector inner product, and the choice of nearest
neighbor is regardless of activation function and loss func-
tion. For simplicity, we ignore bias terms, and the class in-
ference can then be represented as:
ˆy = argmax
k∈[K]
sim(wk, g(x)), sim(wk, g(x)) = wτ
k · g(x)
(7)
Hence, training the last layer in classification can be re-
garded as encouraging the correct class embedding to ap-
proach instance embedding while discouraging all other
class embeddings to be close.
Next, TurboSVM-FL treats client models as sample
points for a secondary classification task at higher level, and
fits SVM using these samples. The SVM is constructed as
a multi-class classification among K classes, and the SVM
training samples are exactly the collected class embeddings,
i.e., {(wn
k, k)|k ∈ [K], n ∈ [N]}. In other words, for each
class k, there are N sample points {(w1
k, k), ..., (wN
k , k)},
and each sample point is the k-th row of the weight matrix
of the logit layer from a client model.
In vanilla FL, the class embeddings in the logit layer of
the global model are obtained by averaging client models,
in other words, wG
k = PN
n=1
|Dn|
|DG|wn
k. Due to data hetero-
geneity among clients, the class embeddings of some clients
can be drifted away from global optima and hence seriously
disturb the aggregation. TurboSVM-FL addresses this prob-
lem with the help of support vectors during global update.
SVM aims at a margin-maximization decision boundary that
is a linear combination of selected samples, which are also
called support vectors. The support vectors can be regarded
as the most informative samples of each class and function
similarly to contrastive anchors. In other words, fitting SVM
is to some extent equivalent to selective aggregation over
samples. TurboSVM-FL brings this property to federated
aggregation by averaging only class embeddings that form
support vectors, as depicted in Algorithm 1.
Moreover, TurboSVM-FL employs spread-out regulariza-
tion across projected global class embeddings to maintain
the margin-maximization property. This is crucial for two
reasons: first, although support vectors are the most infor-
mative data points, they are close to decision boundary and
can be misclassified; second, the weights used during FL ag-
Algorithm 1: TurboSVM-FL part 1: selective aggregation
Input: fitted SVM, sizes of local datasets |D1|, ..., |DN|.
for k ∈ [K] do
retrieve support vectors {vm
k } for class k from SVM
wG
k ←
P
m |Dm|vm
k
P
m |Dm|
▷ m: index to client model
end for
return global class embeddings wG
1 , ..., wG
K
Algorithm 2: TurboSVM-FL part 2: max-margin spread-out
regularization
Input: fitted SVM, global class embeddings wG
k , k ∈
[K], server learning rate ηG.
ℓsp ← 0
for k ∈ [K] do
for k′ ∈ [K] with k′ > k do
retrieve hyperplane hk,k′ from SVM
ℓsp ← ℓsp + exp(−
(wG
k
τ ·hk,k′−wG
k′
τ ·hk,k′)2
2||hk,k′||2
)
end for
end for
for k ∈ [K] do
wG
k ← server optimizer(wG
k , −∇wG
k ℓsp, ηG)
end for
return global class embeddings wG
1 , ..., wG
K
gregation may differ from the coefficients assigned to sup-
port vectors during SVM fitting, which could undermine the
SVM property. Spread-out regularization like in (Yu et al.
2020) offers the potential to distinguish class embeddings.
Nevertheless, we propose that omnidirectional regulariza-
tion is not the most efficient method. Instead, we leverage
once again the SVM property, namely we project the aggre-
gated embeddings back onto the SVM decision boundaries,
and penalize the similarities among projected embeddings:
ℓsp(w1, ..., wK) =
X
k∈[K]
X
k′̸=k
sim(wτ
k · hk,k′
||hk,k′|| , wτ
k′ · hk,k′
||hk,k′|| )
(8)
where hk,k′ is the normal of the separating hyperplane for
classes k and k′ retrieved from fitted SVM.
In (Yu et al. 2020), the authors proved that the classifica-
tion error can be upper-bounded by the separation of class
embeddings. We extend their analysis for TurboSVM-FL by
showing that by applying selective aggregation and max-
margin spread-out regularization, TurboSVM-FL effectively
enlarges the difference between projected logits. For sim-
plicity, we narrow down to binary classification and denote
the distance relaxation terms for embeddings of each class
as ζ+
n and ζ−
n , n ∈ [N], respectively. Let h be the decision
boundary of the fitted SVM. Then, under further simplifica-
tion that all class embeddings serve as support vectors and
all clients have same amount of samples, given a new posi-
tive sample x∗, the difference between the positive and neg-
ative logits when projected on h can be bounded as follows:
proj(logit+(x∗), h) − proj(logit−(x∗), h)
≥[2N − PN
n=1(ζ+
n + ζ−
n )](1 − ζ∗)
N||h||2
(9)
where ζ∗ is the SVM relaxation term for x∗. By aver-
aging support vectors and applying max-margin spread-
out regularization,TurboSVM-FL reduces ||h|| and ζ±
n in
essence according to SVM theory, and the term above that
bounds logit distance from below is hence increased. A more
detailed analysis of this is given in the Appendix.
Algorithm 3: The TurboSVM-FL Framework
Input: clients n ∈ [N], client local datasets D1, ..., DN,
|DG| = |D1| + ... + |DN|, number of global epochs T,
number of client epochs E, number of classes K, server
learning rate ηG, client learning rate η, mini-batch size B
ServerUpdate:
initialize global model θG
0 = (gG
0 , W G
0 )
for t = 0, 1, ..., T − 1 do
for client n ∈ [N] do
θn
t+1 ← ClientUpdate(n, θG
t )
end for
gG
t+1 ← PN
n=1
|Dn|
|DG|gn
t+1
fit SVM using samples {(wn
k, k)|k ∈ [K], n ∈ [N]}
W G
t+1 ← Algorithm 1 TurboSVM-FL part 1
W G
t+1 ← Algorithm 2 TurboSVM-FL part 2
end for
return θG
T = (gG
T , W G
T )
ClientUpdate(n, θG
t ): ▷ Run on client n with model θG
t
θn
t+1 ← θG
t
for e = 0, 1, ..., E − 1 do
for mini-batch B of size B in Dn do
θn
t+1 ← client optimizer(θn
t+1, −∇θn
t+1ℓCE(B), η)
end for
end for
return θn
t+1
Since projections onto the same axis are always co-linear,
the common cosine similarity between them is always ei-
ther 1 or -1 and thus not meaningful. We hence use Gaus-
sian function as a similarity measurement because of its out-
standing capability (Yang et al. 2021) as similarity kernel:
sim(wτ
k · hk,k′
||hk,k′|| , wτ
k′ · hk,k′
||hk,k′|| ) =
exp(−(wτ
k · hk,k′ − wτ
k′ · hk,k′)2
2||hk,k′||2
)
(10)
TurboSVM-FL then optimizes class embeddings regarding
the objective ℓsp and can benefit from adaptivity and mo-
mentum with a proper choice of optimizer. The max-margin
spread-out regularization part of TurboSVM-FL is illus-
trated in Algorithm 2. The pseudocode for the whole algo-
rithm is given in Algorithm 3.
Experiments and Results
We benchmarked TurboSVM-FL on three datasets against
six FL algorithms, including FedAvg (McMahan et al.
2017), FedAdam (Reddi et al. 2020), FedAMS (Wang, Lin,
and Chen 2022), FedProx (Li et al. 2020), MOON (Li, He,
and Song 2021), and FedAwS (Yu et al. 2020). In this sec-
tion, we provide task descriptions and results. For more de-
tails such as reproducibility and model structures, we redi-
rect readers to the Appendix and our GitHub repository1.
1 https://github.com/Kasneci-Lab/TurboSVM-FL.
Dataset
Source
Task
# Classes
# Users
Type & Dim
Mean/Std per User
FEMNIST
(LeCun 1998)
image
62
3550
BW image
226.8 / 88.9
(Cohen et al. 2017)
classification
28 × 28
CelebA
(Liu et al. 2015)
smile
2
9343
RGB image
21.4 / 7.6
detection
84 × 84
Shakespeare
(Shakespeare 2014)
next char
80
1129
string
3743.2 / 6212.3
(McMahan et al. 2017)
prediction
80
Table 1: Overview of used datasets.
(a)
(b)
(c)
Figure 2: Test metrics on FEMNIST dataset.
Tasks
We benchmarked TurboSVM-FL on three different datasets
covering data types of both image and nature language,
namely FEMNIST (LeCun 1998; Cohen et al. 2017),
CelebA (Liu et al. 2015), and Shakespeare (Shakespeare
2014; McMahan et al. 2017) (Table 1). The task in FEM-
NIST dataset is handwritten digit and letter classification us-
ing grayscale images. The number of classes in FEMNIST
is 62 (10 digits, 26 lowercase letters, and 26 uppercase let-
ters) and the resolution of images is 28 × 28. The CelebA
dataset contains 84 × 84 RGB images of faces of celebri-
ties, and the task is binary classification between smiling
and non-smiling faces. The Shakespeare dataset consists of
scripts of different roles from Shakespeare’s works, and the
task is next-character prediction given an input string of
length 80. The number of unique characters and symbols
in this dataset is 80. All three datasets can be acquired on
LEAF (Caldas et al. 2018). For the two image classification
tasks, CNN models were implemented, while for the lan-
guage task LSTM model was utilized. We adopted the model
structures as given in LEAF. Details about data distributions
and models can be found in the Appendix.
We also adopted the data split given in LEAF. More
specifically, we conducted 90% − 10% train-test-split in a
user-independent way, which means we had a held-out set
of clients for validation rather than a fraction of validation
data on each client (Wang et al. 2021a). The main reason for
conducting user-independent validation is that such a test is
a more valid approach for unseen data and, thus, more rep-
resentative for real-world applications. Moreover, it is more
challenging to fit a model in a user-independent setting com-
pared to a user-dependent data split.
Results
To compare the convergence rate of different FL algorithms,
we reported two groups of metrics: number of global aggre-
gation rounds needed to reach certain validation accuracy
(70% for FEMNIST and CelebA, 50% for Shakespeare), and
the achieved F1 score, accuracy, and MCC (Matthews Corre-
lation Coefficient) after 100 aggregation rounds. The results
are given in Tables 2 and 3, and also visualized in Figures 2,
4 and 5 (Appendix) for each task respectively.
Algorithm
FEMNIST
CelebA
Shakespr.
FedAvg
144.4±4.6
91.6±18.2
51.0±5.0
FedAdam
110.8±16.2
>200
54.8±3.2
FedAwS
81.4±2.2
84.2±24.4
45.0±2.3
FedProx
145.4±3.4
94.4±18.4
157.2±5.3
FedAMS
116.4±19.6
>200
51.6±2.1
MOON
145.6±3.7
94.2±18.8
52.4±3.1
TurboSVM-FL
54.6±1.6
46.4±9.4
43.4±2.9
Table 2: Number of communication rounds needed to reach
certain test accuracy on FEMNIST (70%), CelebA (70%),
and Shakespeare (50%) datasets. Smaller is better.
Our results clearly indicate that on FEMNNIST and
CelebA datasets TurboSVM-FL yields a significantly faster
convergence in contrast to other FL methods, while on
the Shakespeare dataset TurboSVM-FL slightly outperforms
others. Compared to the baseline FedAvg, TurboSVM-FL
successfully reduces the number of global rounds by 62.2%,
49.3%, and 14.9% to reach the same test accuracy as given
in Table 2. When all FL methods are run for the same
rounds, TurboSVM-FL yields in much better test metrics on
both image classification tasks in comparison to other meth-
Algorithm
[%]
FEMNIST CelebA
Shakespr.
FedAvg
F1
33.1±1.3
70.5±2.2
17.4±0.5
Acc
59.3±1.8
70.6±2.1
52.9±0.8
MCC 57.9±1.9
41.6±4.2
48.9±0.8
FedAdam
F1
48.1±4.9
34.2±0.3
18.1±0.2
Acc
66.6±3.9
51.6±0.1
53.8±0.9
MCC 65.4±4.1
1.0±2.3
49.8±0.9
FedAwS
F1
54.8±0.8
72.2±3.0
18.3±0.6
Acc
73.1±0.6
72.3±2.8
53.3±0.7
MCC 72.2±0.6
44.9±5.6
49.3±0.7
FedProx
F1
32.8±1.3
70.4±2.1
12.5±0.6
Acc
59.2±1.8
70.6±2.1
46.0±1.2
MCC 57.7±1.9
41.4±4.1
41.2±1.2
FedAMS
F1
46.8±6.5
36.3±5.0
18.2±0.4
Acc
65.8±5.0
52.6±2.3
54.0±0.5
MCC 64.6±5.2
4.7±10.5
50.0±0.6
MOON
F1
33.4±3.7
70.7±2.3
17.6±0.6
Acc
58.2±3.0
70.9±2.2
52.8±1.0
MCC 56.8±3.1
41.8±4.4
48.8±1.0
TurboSVM-
FL
F1
61.9±0.7
77.2±1.3
19.2±0.3
Acc
76.6±1.0
77.3±1.2
53.7±0.2
MCC 75.8±1.0
55.0±2.4
49.7±0.2
Table 3: Achieved test F1 score, accuracy, and MCC score
after 100 global aggregation rounds. Greater is better.
ods, while its performance is a fair match to the adaptive
algorithms on the next-character prediction task. Moreover,
we show in Figure 4 that while adaptive FL methods like
FedAdam and FedAMS are not stable, TurboSVM-FL can
still robustly benefit from adaptivity on the server side.
Impact of Embedding Size on Selectivity
We further explored the impact of embedding size on the
number of client models that form support vectors. To ap-
proach this, we ran experiments on the FEMNIST dataset
with varying embedding size and number of participating
clients C. We recorded the number of support vectors for
class 1 at 200th round in Table 4. It is clear that a higher
embedding size is associated with better performance and
fewer support vectors when there exist enough participating
clients, while a scarcity of clients can lead to full use of class
embeddings as support vectors. Furthermore, larger embed-
ding size also leads to higher complexity and burden, which
needs to be balanced off depending on the specific tasks.
Embedding Dimension
C
4
16
64
256
1024
8
#SV
8
8
8
8
8
F1[%]
6.1
14.2
58.9
63.9
67.4
64
#SV
62
63
58
52
37
F1[%]
16.3
43.2
60.5
65.9
68.3
512
#SV
197
150
147
100
77
F1[%]
16.7
43.2
59.7
64.5
67.0
Table 4: Influence of embedding size on support vectors.
Discussion
As TurboSVM-FL focuses on class representations on the
server side while other parts of the global model are still ag-
gregated with average and client training is done with vanilla
SGD, the use of TurboSVM-FL in combination with other
FL algorithms is promising. For instance, on the client side
FedProx can be applied to counteract data heterogeneity,
while on the server side, class embeddings are aggregated
with TurboSVM-FL. Another example is the use of adaptive
FL methods like FedAdam for encoder aggregation while
logit layers are aggregated with TurboSVM-FL. Moreover
the idea of model-as-sample can be further explored, for ex-
ample, for anomaly client detection and client clustering.
TurboSVM-FL is particularly suitable for cross-device
FL where edge devices are often constrained by computa-
tion and storage resources, but its improvement is also not
excluded from cross-silo case. A typical application sce-
nario of TurboSVM-FL is federated transfer learning, where
a pre-trained model like VGG16 and Resnet50 is adopted,
and all of the layers except the last few ones are frozen. In
this case, each client only needs to train and share the last
few layers, which makes TurboSVM-FL extremely efficient.
The capability of TurboSVM-FL is also not constrained to
single-output tasks. For multi-output tasks such as multi-
label classification and multi-task learning, TurboSVM-FL
can also be applied. To approach this, separate classification
heads for different tasks should be implemented where the
backbone encoder shares its weights among tasks, and then
TurboSVM-FL should be applied to each head in parallel.
One improvement direction of TurboSVM-FL is to relax
the implicit assumption about linear separability of class em-
beddings with kernelization during SVM fitting and class
inference. A piloting ablation study is included in the Ap-
pendix in this regard. Furthermore, while posing no addi-
tional computation cost on the client side, TurboSVM-FL
requires the server to be powerful such that it can fit SVMs
efficiently, especially when the SVMs are in one-vs-one
(OVO) pattern. We chose OVO instead of OVR (one-vs-rest)
mainly for two reasons: 1. in general, OVO performs better
than OVR; 2. for TurboSVM-FL, OVO never suffers from
class imbalance while OVR always does, since the numbers
of samples for each class are always the same. Although
OVO imposes more computation on the server side, we think
that to approach FL, a powerful server is a must-have, and
OVO is no burden for such a server. In case the number of
classes is large, the computation burden can be further re-
solved by sampling a proportion of classes on which SVMs
are fitted.
Conclusion
In this work, we proposed a novel federated aggregation
strategy called TurboSVM-FL, which extensively exploits
SVM to conduct selective aggregation and max-margin
spread-out regularization for class embeddings and can
vastly reduce communication rounds. We tested our ap-
proach on three publicly available datasets, and our results
show that TurboSVM-FL outperforms existing FL methods
largely on convergence rate regarding various metrics.
Acknowledgements
We acknowledge the funding by the Deutsche Forschungs-
gemeinschaft (DFG, German Research Foundation) –
Project number KA 4539/5-1.
References
Bakopoulou, E.; Tillman, B.; and Markopoulou, A. 2021.
Fedpacket: A federated learning approach to mobile packet
classification.
IEEE Transactions on Mobile Computing,
21(10): 3609–3628.
Bemani, A.; and Bj¨orsell, N. 2022. Aggregation Strategy
on Federated Machine Learning Algorithm for Collaborative
Predictive Maintenance. Sensors, 22(16): 6252.
Caldas, S.; Duddu, S. M. K.; Wu, P.; Li, T.; Koneˇcn`y,
J.; McMahan, H. B.; Smith, V.; and Talwalkar, A. 2018.
Leaf: A benchmark for federated settings. arXiv preprint
arXiv:1812.01097.
Chen, K.; and Huo, Q. 2016. Scalable training of deep learn-
ing machines by incremental block training with intra-block
parallel optimization and blockwise model-update filtering.
In 2016 ieee international conference on acoustics, speech
and signal processing (icassp), 5880–5884. IEEE.
Chen, X.; Li, X.; and Li, P. 2020. Toward communication
efficient adaptive gradient method. In Proceedings of the
2020 ACM-IMS on Foundations of Data Science Confer-
ence, 119–128.
Cohen, G.; Afshar, S.; Tapson, J.; and Van Schaik, A. 2017.
EMNIST: Extending MNIST to handwritten letters. In 2017
international joint conference on neural networks (IJCNN),
2921–2926. IEEE.
Cortes, C.; and Vapnik, V. 1995. Support-vector networks.
Machine learning, 20: 273–297.
Duchi, J.; Hazan, E.; and Singer, Y. 2011. Adaptive subgra-
dient methods for online learning and stochastic optimiza-
tion. Journal of machine learning research, 12(7).
Kairouz, P.; McMahan, H. B.; Avent, B.; Bellet, A.; Bennis,
M.; Bhagoji, A. N.; Bonawitz, K.; Charles, Z.; Cormode, G.;
Cummings, R.; et al. 2021. Advances and open problems in
federated learning. Foundations and Trends® in Machine
Learning, 14(1–2): 1–210.
Karimireddy, S. P.; Kale, S.; Mohri, M.; Reddi, S.; Stich, S.;
and Suresh, A. T. 2020. Scaffold: Stochastic controlled av-
eraging for federated learning. In International Conference
on Machine Learning, 5132–5143. PMLR.
Kiefer, J.; and Wolfowitz, J. 1952.
Stochastic estimation
of the maximum of a regression function. The Annals of
Mathematical Statistics, 462–466.
Kingma, D. P.; and Ba, J. 2014.
Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980.
LeCun, Y. 1998. The MNIST database of handwritten digits.
http://yann. lecun. com/exdb/mnist/.
Li, Q.; He, B.; and Song, D. 2021. Model-contrastive feder-
ated learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 10713–10722.
Li, T.; Sahu, A. K.; Zaheer, M.; Sanjabi, M.; Talwalkar, A.;
and Smith, V. 2020.
Federated optimization in heteroge-
neous networks. Proceedings of Machine learning and sys-
tems, 2: 429–450.
Liu, Z.; Luo, P.; Wang, X.; and Tang, X. 2015. Deep learn-
ing face attributes in the wild. In Proceedings of the IEEE
international conference on computer vision, 3730–3738.
McMahan, B.; Moore, E.; Ramage, D.; Hampson, S.; and
y Arcas, B. A. 2017. Communication-efficient learning of
deep networks from decentralized data. In Artificial intelli-
gence and statistics, 1273–1282. PMLR.
Mu, X.; Shen, Y.; Cheng, K.; Geng, X.; Fu, J.; Zhang, T.; and
Zhang, Z. 2023. Fedproc: Prototypical contrastive federated
learning on non-iid data. Future Generation Computer Sys-
tems, 143: 93–104.
Navia-V´azquez, A.; D´ıaz-Morales, R.; and Fern´andez-D´ıaz,
M. 2022. Budget Distributed Support Vector Machine for
Non-ID Federated Learning Scenarios. ACM Transactions
on Intelligent Systems and Technology (TIST), 13(6): 1–25.
Nichol, A.; Achiam, J.; and Schulman, J. 2018.
On
first-order meta-learning algorithms.
arXiv preprint
arXiv:1803.02999.
Reddi, S.; Charles, Z.; Zaheer, M.; Garrett, Z.; Rush,
K.; Koneˇcn`y, J.; Kumar, S.; and McMahan, H. B.
2020.
Adaptive federated optimization.
arXiv preprint
arXiv:2003.00295.
Reddi, S. J.; Kale, S.; and Kumar, S. 2019.
On
the convergence of adam and beyond.
arXiv preprint
arXiv:1904.09237.
Robbins, H.; and Monro, S. 1951. A stochastic approxima-
tion method. The annals of mathematical statistics, 400–
407.
Shakespeare, W. 2014.
The complete works of William
Shakespeare. Race Point Publishing.
Wang, J.; Charles, Z.; Xu, Z.; Joshi, G.; McMahan, H. B.;
Al-Shedivat, M.; Andrew, G.; Avestimehr, S.; Daly, K.;
Data, D.; et al. 2021a. A field guide to federated optimiza-
tion. arXiv preprint arXiv:2107.06917.
Wang, J.; Xu, Z.; Garrett, Z.; Charles, Z.; Liu, L.; and Joshi,
G. 2021b. Local adaptivity in federated learning: Conver-
gence and consistency. arXiv preprint arXiv:2106.02305.
Wang, Y.; Lin, L.; and Chen, J. 2022.
Communication-
efficient adaptive federated learning. In International Con-
ference on Machine Learning, 22802–22838. PMLR.
Wu, X.; Huang, F.; Hu, Z.; and Huang, H. 2023. Faster adap-
tive federated learning. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence, volume 37, 10379–10387.
Yang, J.; Wu, Z.; Peng, K.; Okolo, P. N.; Zhang, W.; Zhao,
H.; and Sun, J. 2021. Parameter selection of Gaussian kernel
SVM based on local density of training set. Inverse Prob-
lems in Science and Engineering, 29(4): 536–548.
Yu, F.; Rawat, A. S.; Menon, A.; and Kumar, S. 2020. Fed-
erated learning with only positive labels. In International
Conference on Machine Learning, 10946–10956. PMLR.
Zaheer, M.; Reddi, S.; Sachan, D.; Kale, S.; and Kumar, S.
2018. Adaptive methods for nonconvex optimization. Ad-
vances in neural information processing systems, 31.
Zhang, X.; Yu, F. X.; Kumar, S.; and Chang, S.-F. 2017.
Learning spread-out local feature descriptors. In Proceed-
ings of the IEEE international conference on computer vi-
sion, 4595–4603.
Appendix
Implementation
Our implementation of TurboSVM-FL and instructions for reproducing experiment results can be found in our GitHub reposi-
tory (https://github.com/Kasneci-Lab/TurboSVM-FL).
Analysis
It is hard to give an analysis to the convergence rate of TurboSVM-FL directly. Instead, we provide informal descriptive analysis
to how TurboSVM-FL is potential to reduce misclassification error. In (Yu et al. 2020) it is proven that misclassification error
is related to the separation of class embeddings and that the cosine contrastive loss with spread-out regularization surrogates
misclassification error, which inspire our work. For detailed proof in this direction, we redirect readers to (Yu et al. 2020).
To simplify the analysis, we narrow down to binary classification without loss of generality and ignore all bias terms. In a
federated classification task with N clients, denote the class embeddings of positive and negative classes as x+
1 , ..., x+
N and
x−
1 , ..., x−
N respectively. Let I+
n ∈ {0, 1}, I−
n ∈ {0, 1} be the indicator of whether a class embedding is used as support
vector and |Dn| denote the sample size of client n. Then, the aggregated class embeddings can be given as
PN
n=1 I+
n |Dn|x+
n
PN
n=1 I+
n |Dn|
and
PN
n=1 I−
n |Dn|x−
n
PN
n=1 I−
n |Dn| . Let h be the decision boundary of SVM fitted on {(x+
n , +)|n ∈ [N]} ∪ {(x−
n , −)|n ∈ [N]} and
ζ+
1 , ..., ζ+
N, ζ−
1 , ..., ζ−
N be the corresponding SVM relaxation terms, i.e.:
h, ζ+
1 , ..., ζ+
N, ζ−
1 , ..., ζ−
N = argmin
h,ζ±
1 ,...,ζ±
N
1
2||h||2 + λ
N
X
n=1
(ζ+
n + ζ−
n ),
s.t. hτ · x+
n ≥ 1 − ζ+
n , hτ · x−
n ≤ −(1 − ζ−
n ) and ζ+
n , ζ−
n ≥ 0
where the first objective term 1
2||h||2 corresponds to margin maximization and the second term allows each sample to be away
from its correct margin up to distance ζn. Given a new sample with extracted feature x∗, without loss of generality, assume its
true label is positive. Further, we assume x∗ is a “good” sample and can be not only correctly but also “well” classified, which
means hτ · x∗ ≥ 1 − ζ∗ with ζ∗ ≤ 1. Let ζ+
n , ζ−
n ≤ 1 for all n ∈ [N]. We then take a look into the difference between the
positive and negative logits when class embeddings are projected onto SVM separating hyperplane, i.e.:
proj(logit+(x∗), h) − proj(logit−(x∗), h)
=(PN
n=1 I+
n |Dn|x+
n )τ · h
PN
n=1 I+
n |Dn|||h||
hτ
||h|| · x∗ − (PN
n=1 I−
n |Dn|x−
n )τ · h
PN
n=1 I−
n |Dn|||h||
hτ
||h|| · x∗
=λ− PN
n=1 I+
n |Dn|(x+τ
n
· h) − λ+ PN
n=1 I−
n |Dn|(x−τ
n
· h)
λ+λ−||h||2
hτ · x∗
(λ+ =
N
X
n=1
I+
n |Dn|, λ− =
N
X
n=1
I−
n |Dn|)
≥2λ+λ− − PN
n=1(λ−I+
n |Dn|ζ+
n − λ+I−
n |Dn|ζ−
n )
λ+λ−||h||2
(hτ · x∗)
(SVM property)
≥[2λ+λ− − PN
n=1(λ−I+
n |Dn|ζ+
n − λ+I−
n |Dn|ζ−
n )](1 − ζ∗)
λ+λ−||h||2
(x∗ a “good” sample)
≥[2N − PN
n=1(ζ+
n + ζ−
n )](1 − ζ∗)
N||h||2
(assume I±
n = 1 for all n ∈ [N] and |D1| = ... = |DN|)
When we optimize the aggregated class embeddings
PN
n=1 I+
n |Dn|x+
n
PN
n=1 I+
n |Dn| ,
PN
n=1 I−
n |Dn|x−
n
PN
n=1 I−
n |Dn|
with max-margin spread-out regulariza-
tion in TurboSVM-FL, we reduce ||h|| and ζ±
n in essence according to SVM theory, and the term above that bounds logit
distance from below is hence increased. As discovered in (Yu et al. 2020), a larger distance between class embeddings or logits
has the capability to reduce the probability of misclassification. Therefore, the max-margin spread-out regularization on class
embeddings is potential to reduce misclassification error when class embeddings are projected onto SVM decision boundary
w. The effect also propagates to the case when class embeddings are not projected onto h given hτ · x∗ ≥ 0.
Experiment Environment
We implemented TurboSVM-FL and other FL methods with PyTorch and ran experiments on multiple computers. All comput-
ers are equipped with exactly the same hardware (32 GB RAM, Intel i7-13700K, NVIDIA RTX 4080 16 GB), operating system
(WSL2 Ubuntu 22.04.2 LTS), and software (Python 3.10.12, PyTorch 2.0.1 for CUDA 11.7).
Randomness
All our experiments were replicated five times with different random seeds from {0, 1, 2, 3, 4} each time. The random seed
applies to numpy.random.seed, torch.manual seed, and random.seed to guarantee reproducibility. We reported the mean
and standard deviation (std) over five seeds for each metric.
Data Distribution
The histograms of number of samples per client of each dataset are given in Figure 3.
(a) FEMNIST dataset
(b) Celeba dataset
(c) Shakespeare dataset
Figure 3: Histogram of number of samples per user in the datasets from LEAF (Caldas et al. 2018).
Model Architectures
The FL benchmark framework LEAF (Caldas et al. 2018) provides standard model in Tensorflow for each task. We translated
all these models into PyTorch and kept their architectures as given in LEAF. Detailed model architectures are described in
Tables 5, 6, 7 in the Appendix. For all tasks, the activation function for logit layer is softmax, while the classification objective
is cross entropy, regardless of whether the classification task is binary or multi-class.
Layer
Architecture
Input
shape 1 × 28 × 28
Conv2d
kernel size 5, in/out channel 1/32, same padding
ReLU
-
MaxPooling
kernel size 2, stride 2
Conv2d
kernel size 5, in/out channel 32/64, same padding
ReLU
-
MaxPooling
kernel size 2, stride 2
Flatten
-
Linear
in/out dimension 3136/2048
ReLU
-
Linear
in/out dimension 2048/62
Table 5: Model structure (CNN) for FEMNIST dataset, following https://github.com/TalwalkarLab/leaf/blob/master/models/
femnist/cnn.py.
Layer
Architecture
Input
shape 3 × 84 × 84
Conv2d
kernel size 3, in/out channel 3/32, same padding
BatchNorm2d
-
MaxPolling
kernel size 2, stride 2
ReLU
-
Conv2d
kernel size 3, in/out channel 32/32, same padding
BatchNorm2d
-
MaxPolling
kernel size 2, stride 2
ReLU
-
Conv2d
kernel size 3, in/out channel 32/32, same padding
BatchNorm2d
-
MaxPolling
kernel size 2, stride 2
ReLU
-
Conv2d
kernel size 3, in/out channel 32/32, same padding
BatchNorm2d
-
MaxPolling
kernel size 2, stride 2
ReLU
-
Flatten
-
Linear
in/out dimension 800/2
Table 6: Model structure (CNN) for CelebA dataset, following https://github.com/TalwalkarLab/leaf/blob/master/models/
celeba/cnn.py.
Layer
Architecture
Embedding
number of embeddings 80, dimension 8
LSTM
3n/hidden dimension 8/256, hidden layers 2
Linear
in/out dimension 256/80
Table 7: Model structure (LSTM) for Shakespeare dataset, following https://github.com/TalwalkarLab/leaf/blob/master/models/
shakespeare/stacked lstm.py.
Hyperparameters
In the process of hyperparameter tuning and experimentation, we set the client epoch E to 1 and the number of participating
clients C to 8 by default to simulate “lazy” clients. We also ran experiments with E = 2, 4 and C = 16, 32. The results of these
experiments can be found in the Appendix.
One key hyperparameter for TurboSVM-FL is the regularization coefficient λ for SVM fitting. We tested three different strate-
gies for this coefficient, namely linearly increasing, linearly decreasing, and constant. Among these three strategies, linearly
increasing yields the best model performance. Our explanation to this phenomenon is that as training procedure progresses,
more client models approach their optima and hence become more informative. A decreasing regularization factor tends to re-
sult in an increasing number of support vectors, which matches the increase of client model informativity. We then implemented
this strategy as default. Specifically, the regularization coefficient is set to 1.0 in the beginning and is successively reduced in
each global aggregation round.
Another SVM-related factor is whether the SVMs are fitted in one-vs-one (OVO) pattern or one-vs-rest (OVR) pattern. By
default, our method trains SVM in OVO pattern, which means that one binary SVM is trained for each class pair, and in
total, K(K−1)
2
SVMs are trained for a multi-classification task with K classes. We choose OVO instead of OVR mainly for
two reasons. Firstly, OVO performs better than OVR in general. Secondly, for TurboSVM-FL , OVO never suffers from class
imbalance while OVR always does, since the numbers of samples for each class in the higher level SVM problem are always
the same. Although OVO imposes more computation on the server side, we think that to approach FL, a powerful server is
a must-have, and OVO is no burden for such a server. In case the number of classes is large, the computation burden can be
further resolved by sampling a proportion of classes on which SVMs are fitted.
When implementing all seven FL algorithms, we followed the recommendations provided in (Reddi et al. 2020; Wang
et al. 2021a) on the choice of optimizers, namely: the optimizer on the client side is SGD, while Adam is applied
on the server side for all methods that require a server-level optimizer during central aggregation, including FedAdam,
FedAMS, FedAwS, and TurboSVM-FL. For each task, we first ran a grid search for optimal client learning rates in
{1e−5, 1e−4, 1e−3, 1e−2, 1e−1, 1e0} for FedAvg. Then, we fixed the client learning rate to its optima and conducted a grid
search for optimal global learning rates in the same range. As FedAMS is an improved version of FedAdam, we applied the
same global learning rate for them. The details of learning rates are listed in Table 8 in the Appendix. Moreover, we decided the
sizes of mini-batch based on the sample histograms (Figure 3). The final batch sizes are 64 for FEMNIST, 8 for CelebA, and
64 for Shakespeare. The coefficients for additional penalty terms were set to 0.01 and 1 respectively for FedProx and MOON.
Algorithm
FEMNIST
CelebA
Shakespeare
FedAvg
1e − 1
1e − 3
1e0
FedAdam
1e − 3
1e − 3
1e − 2
FedAMS
1e − 3
1e − 3
1e − 2
FedProx
1e − 1
1e − 3
1e0
MOON
1e − 1
1e − 3
1e0
FedAwS
1e − 2
1e − 2
1e0
TurboSVM-FL
1e − 2
1e − 2
1e0
Table 8: Learning rates used in the experiments. All methods share the same client learning rates as FedAvg. For FedAvg,
FedProx, and MOON, client learning rate is listed in the table, while for FedAdam, FedAwS, FedAMS, and TurboSVM-FL,
server learning rate is given.
Additional Experiment Results
The results for the experiments using the CelebA dataset and the Shakespeare dataset with default settings (C = 8, E = 1)
are plotted in Figures 4 and 5. The results corresponding to varying number of participating clients (C) are given in Tables 9
and 10. The results regarding varying number of client local training epochs (E) are given in Table 11.
(a)
(b)
(c)
Figure 4: Test metrics on CelebA dataset.
(a)
(b)
(c)
Figure 5: Test metrics on Shakespeare dataset.
Algorithm
FEMNIST
CelebA
Shakespeare
C
8
16
32
8
16
32
8
16
32
FedAvg
144
140
136
92
97
90
51
38
32
FedAdam
111
95
83
>200
>200
>200
55
41
38
FedAwS
81
75
72
84
77
74
45
34
28
FedProx
145
140
136
94
97
90
157
137
129
FedAMS
116
97
83
>200
>200
>200
52
43
36
MOON
146
140
135
94
97
91
52
42
33
TurboSVM-FL
55
51
54
46
44
40
43
34
28
Table 9: Number of communication rounds needed to reach certain test accuracy on FEMNIST (70%), CelebA (70%), and
Shakespeare (50%) datasets respectively.
Algorithm
FEMNIST
CelebA
Shakespeare
[%] / C
8
16
32
8
16
32
8
16
32
FedAvg
F1
33.1
34.8
36.4
70.5
68.9
70.5
17.4
18.9
20.0
Acc
59.3
60.7
61.7
70.6
69.4
70.6
52.9
54.8
55.9
MCC
57.9
59.2
60.3
41.6
40.2
41.5
48.9
51.0
52.1
FedAdam
F1
48.1
54.8
58.2
34.2
37.1
40.0
18.1
19.6
20.2
Acc
66.6
71.2
74.4
51.6
52.7
54.5
53.8
55.4
56.2
MCC
65.4
69.9
73.1
1.0
5.8
7.0
49.8
51.6
52.5
FedAwS
F1
54.8
57.3
58.3
72.2
70.6
72.4
18.3
19.5
20.8
Acc
73.1
74.6
74.9
72.3
71.2
72.5
53.3
55.1
56.1
MCC
72.2
73.7
74.0
44.9
43.6
45.3
49.3
51.3
52.3
FedProx
F1
32.8
34.5
36.2
70.4
68.9
70.5
12.5
13.2
13.8
Acc
59.2
60.5
61.6
70.6
69.4
70.6
46.0
47.7
48.2
MCC
57.7
59.0
60.2
41.4
40.1
41.6
41.2
43.1
43.7
FedAMS
F1
46.8
54.4
59.6
36.3
37.0
34.9
18.2
19.5
20.5
Acc
65.8
70.9
74.3
52.6
52.6
51.9
54.0
55.4
56.2
MCC
64.6
69.9
73.4
4.7
6.2
2.3
50.0
51.6
52.7
MOON
F1
33.4
36.7
37.6
70.7
69.8
70.8
17.6
18.8
20.0
Acc
58.2
61.1
61.96
70.9
70.2
70.8
52.8
54.7
55.7
MCC
56.8
59.7
60.6
41.8
41.1
41.9
48.8
50.8
52.0
TurboSVM-FL
F1
61.9
62.4
62.2
77.2
78.2
77.6
19.2
20.2
21.1
Acc
76.6
76.7
77.0
77.3
78.2
77.8
53.7
55.0
56.0
MCC
75.8
75.9
76.2
55.0
56.5
56.2
49.7
51.1
52.2
Table 10: Achieved validation accuracy and F1 score after 100 global aggregation rounds. For all runs E = 1. σ is not reported
since for all runs σ < 13.0 and in most cases σ < 3.0.
Kernelization
We extended the vanilla TurboSVM-FL with kernelization during SVM fitting and investigated the influence of kernel on model
performance. Specifically, we benchmarked polynomial kernel Kpoly(x1, x2) = (axτ
1·x2+b)d of different degrees (d = 2, 3, 4),
rbf kernel Krbf(x1, x2) = exp(−a||x1 − x2||2), and sigmoid kernel Ksig(x1, x2) = tanh(axτ
1 · x2 + b) on the CelebA dataset
with all coefficients a and biases b set to 1.0. The experiments were run with a single random seed for 200 global aggregation
rounds. The number of participating clients C in each aggregation round was 8, and each of them trained its local model for
E = 1 epoch. The obtained results are given in Table 12. For polynomial kernel, the degree and server learning rate have a
large impact: a higher degree and higher server learning rate generally lead to better F1 but also overfitting, mostly due to the
simplicity of the task (binary) and the scarcity of participating clients (only 8). In comparison, the rbf kernel and sigmoid kernel
are not sensitive to server learning rate when coefficient and bias are 1, and both kernels do not yield improvement. Our results
show that kernelized SVM can be used for TurboSVM-FL, and we believe with kernelization, more complex FL tasks can be
addressed.
Algorithm
FEMNIST
CelebA
Shakespeare
[%] / E
1
2
4
1
2
4
1
2
4
FedAvg
F1
33.1
57.7
64.4
70.5
77.0
83.8
17.4
19.4
18.3
Acc
59.3
73.4
76.6
70.6
77.2
84.0
52.9
53.6
51.0
MCC
57.9
72.5
75.8
41.6
54.9
68.4
48.9
49.7
47.0
FedAdam
F1
48.1
53.1
58.1
34.2
34.0
34.0
18.1
19.0
17.9
Acc
66.6
69.2
71.6
51.6
51.6
51.6
53.8
54.2
52.0
MCC
65.4
68.2
70.6
1.0
0.0
0.0
49.8
50.3
48.0
FedAwS
F1
54.8
63.8
54.8
72.2
77.9
83.2
18.3
19.6
18.4
Acc
73.1
77.1
70.0
72.3
78.1
83.3
53.3
53.8
51.1
MCC
72.2
76.3
69.0
44.9
56.6
66.8
49.3
49.9
47.1
FedProx
F1
32.8
57.7
60.3
70.4
77.0
83.9
12.5
13.5
13.9
Acc
59.2
73.4
74.1
70.6
77.2
84.0
46.0
47.6
47.6
MCC
57.7
72.5
73.2
41.4
55.0
68.5
41.2
43.0
43.2
FedAMS
F1
46.8
53.2
57.8
36.3
34.1
34.1
18.2
19.0
17.8
Acc
65.8
69.2
71.4
52.6
51.6
51.6
54.0
54.1
52.0
MCC
64.6
68.2
70.5
4.7
0.0
0.0
50.0
50.2
48.0
MOON
F1
33.4
57.9
64.6
70.7
76.6
83.2
17.6
18.0
17.2
Acc
58.2
73.3
76.7
70.9
76.9
83.3
52.8
53.2
50.0
MCC
56.8
72.4
75.9
41.8
54.6
67.2
48.8
49.3
45.9
TurboSVM-FL
F1
61.9
63.3
60.8
77.2
79.9
81.4
19.2
19.5
18.1
Acc
76.6
76.2
74.2
77.3
80.0
81.5
53.7
53.8
51.0
MCC
75.8
75.4
73.4
55.0
60.2
63.2
49.7
50.0
47.0
Table 11: Achieved validation accuracy and F1 score after 100 global aggregation rounds (or before overfitting and model
crash). For all runs C = 8. σ is not reported since for all runs σ < 13.0 and in most cases σ < 3.0.
kernel
linear
poly (d=2)
poly (d=3)
poly (d=4)
rbf
sig
server lr
1e−2
1e−4
1e−2
1e−4
1e−2
1e−4
1e−1
-
-
F1[%]
82.1
78.4
82.0
78.3
82.8
78.4
85.2
76.8
77.3
BCE loss
0.44
0.46
0.92
0.46
1.12
0.46
2.43
0.51
0.50
Table 12: Influence of kernel functions.
"
"This paper presents a computational complexity analysis of a matroid interdiction problem (MIP), where the feasible sets of the leader and the follower are given by partition matroids. We establish the complexity of the MIP in the general case and discuss two polynomial-time solvability cases. When the leader's feasible set is a uniform matroid, we develop a 2-flip local search algorithm for the leader. We also demonstrate the validity of our results under the condition that the leader's and follower's objective function coefficients maintain the same relative order.","This study explores a novel matroid interdiction problem (MIP), where the feasible sets of the leader and the follower are specified by partition matroids. We examine the computational complexity of the MIP and establish conditions under which the problem is either polynomially solvable or strongly NP-hard. Our findings contribute to the body of knowledge on matroid interdiction by identifying polynomial-time solution strategies for specific instances of the MIP, and providing a rigorous analysis of the complexity of the general problem.","Existing literature on interdiction problems and matroid optimization is reviewed, highlighting relevant studies on knapsack interdiction, bilevel MILP problems, and network interdiction models. We discuss related approaches that incorporate matroid structures, such as Stackelberg pricing problems with matroid constraints and parametric matroid interdiction problems. Additionally, we explore prior work on local search algorithms for bilevel optimization problems and their relevance to our study.nan","To investigate the computational complexity of the MIP, we employ a series of methodologies. First, we establish the strong NP-hardness of the MIP in the general case via a reduction from the independent set problem. This result demonstrates that the problem is inherently difficult to solve, even for non-fixed numbers of cardinality constraints for the leader and follower. Next, we consider two specific cases where either the number of leader's cardinality constraints or the follower's cardinality constraints is fixed. For these cases, we propose two polynomial-time algorithms: a dynamic programming (DP)-based approach and a single-level dual reformulation of the MIP, respectively. The DP-based approach constructs an optimal leader's decision sequentially for each subset of the ground set controlled by the follower. On the other hand, the single-level dual reformulation is reduced to a series of linear optimization problems over a partition matroid, which can be efficiently solved using the well-known greedy algorithm. Furthermore, we analyze a standard 2-flip local search algorithm for the leader, in the case where the follower's feasible set is a uniform matroid. We establish that this algorithm terminates in polynomial time and converges to a globally optimal solution under mild assumptions.","Our key findings include the following: - The MIP is strongly NP-hard in the general case, even when the objective function coefficients are binary. - For fixed numbers of leader's or follower's cardinality constraints, we propose polynomial-time solution approaches: a DP-based algorithm and a duality-based algorithm, respectively. - For the special case of a uniform matroid at the follower's side, we design a 2-flip local search algorithm for the leader that is less efficient than the duality-based approach but applicable to a more general version of the MIP. - Our results extend to cases where the leader's and follower's objective function coefficients differ but maintain the same relative order.","This study contributes to the understanding of the computational complexity of matroid interdiction problems. We provide a comprehensive analysis of the MIP when the feasible sets are partition matroids, establishing both polynomial-time solvability and NP-hardness under various conditions. We also propose several polynomial-time algorithms and analyze a local search heuristic for specific instances of the MIP. Future research directions include exploring the complexity of the MIP with laminar matroids, investigating constant factor approximation algorithms, and examining the performance of the proposed algorithms on real-world problem instances.",On a class of interdiction problems with partition matroids: complexity and polynomial-time algorithms,"Sergey S. Ketkov, Oleg A. Prokopyev","On a class of interdiction problems with partition matroids: complexity and
polynomial-time algorithms
Sergey S. Ketkov†a, Oleg A. Prokopyeva
aDepartment of Business Administration, University of Zurich, Zurich, 8032, Switzerland
Abstract
In this study, we consider a class of linear matroid interdiction problems, where the feasible sets
for the upper-level decision-maker (referred to as the leader) and the lower-level decision-maker (re-
ferred to as the follower) are given by partition matroids with a common ground set. In contrast
to classical network interdiction models where the leader is subject to a single budget constraint,
in our setting, both the leader and the follower are subject to several independent cardinality con-
straints and engage in a zero-sum game. While a single-level linear integer programming problem
over a partition matroid is known to be polynomially solvable, we prove that the considered bilevel
problem is NP-hard, even when the objective function coefficients are all binary. On a positive
note, it turns out that, if the number of cardinality constraints is fixed for either the leader or
the follower, then the considered class of bilevel problems admits several polynomial-time solution
schemes.
Specifically, these schemes are based on a single-level dual reformulation, a dynamic
programming-based approach, and a 2-flip local search algorithm for the leader.
Keywords:
Bilevel optimization; Interdiction; Partition matroid; Dynamic programming;
Local search
1. Introduction
To simplify our further discussion, we begin with some basic definitions. A finite matroid M is
given by a pair (E, I), where E is a finite ground set and I is a collection of independent sets of E
that satisfy the following three properties:
M1. The empty set is independent, i.e., ∅ ∈ I.
M2. Every subset of an independent set is independent, i.e., for each A′ ⊆ A ⊆ E, if A ∈ I, then
A′ ∈ I.
M3. If A and B are independent sets and |A| < |B|, then there exists an element e ∈ B \ A such
that A ∪ e ∈ I.
The standard types of finite matroids include vector matroids (sets of linearly independent vectors),
graphic matroids (sets of edges in a graph that do not contain cycles), transversal matroids (sets
†Corresponding author. Email: sergei.ketkov@business.uzh.ch; phone: +41 078 301 85 21.
Preprint submitted to Elsevier
January 23, 2024
arXiv:2401.12010v1  [cs.CC]  22 Jan 2024
Parameter(s)
Definition
X = Xp and Yp(x)
feasible sets of the leader and the follower, respectively;
also, X may refer to an arbitrary matroid/binary set
N := {1, . . . , n}
a ground set
β ∈ Rn
+
a weight vector
K1 := {1, . . . , K1} and K2 := {1, . . . , K2}
indices of cardinality constraints
for the leader and the follower
distinct partitions of N for the leader and the follower, i.e.,
Ak′ ⊂ N, k′ ∈ K1, and Ck ⊂ N, k ∈ K2
S
k′∈K1 Ak′ = S
k∈K2 Ck = N, Ak′ ∩ A˜k′ = ∅ and Ck ∩ C˜k = ∅
for any ˜k′ ̸= k′ and ˜k ̸= k
Bk′ ∈ Z+, k′ ∈ K1, and Dk ∈ Z+, k ∈ K2
budgets of the leader and the follower;
we assume that P
k′∈K1 Bk′ ≤ n and P
k∈K2 Dk ≤ n
Table 1: Summary of the notations used in formulation [MIP].
of edges in a graph that form a matching), uniform matroids (subsets with a bounded cardinality)
and partition matroids (a direct sum of uniform matroids). We refer the reader to [25] for further
details on matroids and their properties.
In this study, we consider a matroid interdiction problem (MIP) with partition matroids given
by:
[MIP]:
min
x∈X β⊤y∗(x)
s.t. y∗(x) ∈ argmaxy∈Yp(x) β⊤x,
where
X = Xp =
n
x ∈ {0, 1}n :
X
i∈Ak′
xi ≤ Bk′
∀k′ ∈ K1
o
and
(1a)
Yp(x) =
n
y ∈ {0, 1}n : y ≤ 1 − x,
X
i∈Ck
yi ≤ Dk
∀k ∈ K2
o
.
(1b)
In the above formulation, all vectors are labeled by bold letters, and a vector of all ones is de-
noted as 1. For a detailed summary of the notations used, we refer the reader to Table 1. Fur-
thermore, it is rather straightforward to verify that the feasible sets of the leader and the follower,
respectively, Xp and Yp(x) for a given x ∈ Xp, are partition matroids with respect to the ground
set N = {1, . . . , n}.
The aim of this study is to explore the computational complexity of [MIP] and, if possible, to
demarcate between problem classes that can be solved in polynomial time and those that exhibit
NP-hardness. Also, following the majority of bilevel interdiction models in the literature, see,
e.g., [7, 14, 20, 33], our primary focus is on a min-max model, where both decision-makers have
2
identical linear objective functions and engage in a zero-sum game. However, we also provide a
brief analysis of the case, where the objective function coefficients of the leader and the follower
are different but maintain the same relative order.
The remainder of this section is organized as follows. In Section 1.1 we overview the related
literature. Section 1.2 provides a summary of our main theoretical results and contributions.
1.1. Related literature
Interdiction forms a rather broad class of deterministic and stochastic optimization problems
arising, for example, in the military, law-enforcement and infectious disease control contexts; see,
e.g., the surveys in [29, 30] and the references therein. While the majority of interdiction models in
the literature focus on network interdiction, e.g., interdicting the shortest path [16, 20], the mini-
mum spanning tree [14], or the maximum matching [34], more recent studies explore interdiction
problems not directly connected to graphs. These problems include knapsack interdiction [7, 12],
matroid interdiction [19], and interdiction for some specific classes of linear programs [8, 9]. Taking
into account our formulation [MIP], in this study we focus on a relatively new problem setting,
where the feasible sets of both the leader and the follower are given by partition matroids.
Interdiction models with a single budget constraint. It can be argued that among all
MIPs, the most well-explored problems are those where the leader is subject to a single budget
constraint and the follower’s feasible set is a matroid; see, e.g., [8, 14, 33]. The leader’s budget
constraint can be defined by assigning a cost ce ∈ R+ for each ground element e ∈ E of the matroid.
In other words, it can be expressed as:
X
e∈E
cexe ≤ B,
(2)
where x ∈ {0, 1}|E| is a vector of leader’s decision variables and B ∈ R+ is a predefined budget. In
particular, if B is integer and ce = 1 for all e ∈ E, then the leader’s feasible set induced by (2) is a
uniform matroid and constraint (2) can also be referred to as a cardinality constraint.
MIPs with a budget constraint of the form (2) and a matroid at the lower level are considered,
for example, in the context of the minimum spanning tree (MST) interdiction problem [14, 34] and
the matching interdiction problem [33, 35]. First, it is proved in [14] that the MST interdiction
problem is NP-hard, even if (2) is a cardinality constraint and the edge weights are restricted to be
binary. On a positive note, it is shown that the MST interdiction problem admits a polynomial-time
constant factor approximation algorithm [34]. To a certain extent, similar results are obtained for
the matching interdiction problem in the related studies in [33, 35].
Furthermore, Chestnut and Zenklusen [8] consider a more general class of interdiction problems
with binary objective function coefficients and particular assumptions on the follower’s feasible
set. For this class of problems, the authors propose an efficient 2-pseudoapproximation algorithm,
which can either provide a 2-approximation of the optimal solution or a solution that is at least as
good as the optimal solution but with a maximum twice budget overrun. In addition, the results
3
in Chestnut and Zenklusen [8] can be applied to a matroid interdiction problem with monotone
nonnegative submodular interdiction costs.
Finally, we refer to an unpublished preprint by Hausbrandt et al. [19], who analyze a para-
metric matroid interdiction problem with a unit budget. In their problem setting, it is given a
finite matroid, where the weights of ground elements depend linearly on a real parameter from a
predefined parameter interval. The goal is to find, for each parameter value, a single element that,
being removed from the ground set, maximizes the weight of a minimum weight independent set.
For this problem, Hausbrandt et al. [19] develop a solution algorithm, whose running time depends
on different matroid operations and, in particular, is polynomial for graphic matroids.
Another way to model a budget constraint for the leader is to charge δce resources to increase
the weight of element e ∈ E by δ. This model can be viewed as a continuous model and described
by a budget constraint of the same form as (2), i.e.,
X
e∈E
cexe ≤ B,
but with the leader’s decision variables, x such that x ≥ 0; see, e.g., [13, 14].
In relation to the described model, Frederickson and Solis-Oba [13] focus on a particular class
of MIPs, where the feasible set of the follower is an arbitrary matroid. As we outline in Section 3,
in this setting, provided that the follower’s objective function is linear, the integrality constraint
with respect to the follower’s decision variables can be relaxed. Hence, the problem analyzed in
[13] can be viewed as a continuous MIP. For this problem, Frederickson and Solis-Oba [13] pro-
pose a solution algorithm that achieves strong polynomial-time complexity for matroids with a
polynomial-time independence test. Their approach involves, first, reducing the MIP to a problem
over an unweighted matroid and then, transforming it into the membership problem on matroid
polyhedra. Furthermore, in [15] the authors design more efficient algorithms for continuous MIPs
with scheduling and partition matroids by taking advantage of their special structure.
Other related models. Perhaps, the most related to our problem setting is the study by
B¨ohnlein and Schaudt [3], who consider Stackelberg pricing problems that are based on matroids.
Specifically, they explore a problem setting, where one or multiple followers possess their own ma-
troids with a common ground set and seek a basis with a minimal total weight. Furthermore,
the ground elements are divided into two groups: the first group consists of elements with fixed
weights (prices), while the second group comprises elements for which the prices are determined
by the leader. The authors in [3] do not enforce any constraints on the leader’s price function (but
make some assumptions to guarantee that the leader’s revenue is bounded), and consider uniform,
partition and laminar matroids as follower’s feasible sets. In particular, laminar matroids can be
viewed as a generalization of partition matroids, where the cardinality constraints may refer to
either disjoint or nested subsets of the ground set.
The main results of [3] can be summarized as follows. First, by leveraging a dynamic programming-
based approach, it is proved that for a single follower and all types of follower’s feasible sets outlined
4
above, the problem of computing leader-optimal prices can be solved in polynomial time. Further-
more, it is established that the leader’s problem remains polynomially solvable when there are
multiple followers and the follower’s feasible set is a uniform matroid. In conclusion, by a reduction
from the hitting set problem, it is shown that the problem with multiple followers and partition or
laminar matroids is NP-hard.
Another related problem is a knapsack interdiction problem considered by Caprara et al.
[5, 6, 7]. Specifically, the authors analyze a min-max formulation similar to [MIP], where both the
leader and the follower hold their own private knapsacks and choose items from a common item set.
We note that the knapsack interdiction problem in [5, 7] has a non-matroid structure and is proved
to be Σp
2-hard. In other words, this problem is located at the second level of the polynomial hierar-
chy and there is no way of formulating it as a single-level linear mixed-integer programming (MILP)
problem of polynomial size, unless the polynomial hierarchy collapses; see, e.g., [22].
Next, we refer to Shi et al. [27], who consider a class of bilevel MILP problems, where the
decision variables of the follower are all binary. In their setting, it is assumed that the follower’s
computational capabilities are limited and, therefore, the follower does not solve its optimization
problem to optimality. Instead, it leans towards a locally optimal solution, which is defined in
terms of a k-flip neighborhood; see, e.g., [24]. More precisely, the authors in [27] demonstrate
that for fixed k the resulting problem admits a single-level MILP reformulation of polynomial size.
Furthermore, if the follower’s feasible set is a matroid, then any 2-flip locally optimal solution for
the follower is also globally optimal. In other words, for this class of bilevel problems the proposed
MILP reformulation with k ≥ 2 yields an exact solution. As outlined further in Section 1.2, in our
model we adopt a similar approach to [27] by analyzing a 2-flip local search algorithm from the
leader’s perspective.
Finally, there are several bilevel optimization models in the related literature, in which both the
leader and the follower construct an independent set of a matroid together; see, e.g., [2, 4, 18, 28].
We do not provide a thorough discussion of these models because they are related to network
optimization problems, such as the MST and the maximum matching problems. Moreover, the
feasible sets in the aforementioned models, individually for the leader and for the follower, do not
necessarily form a matroid.
1.2. Our approach and contributions
As outlined earlier, the main goal of this study is to explore the computational complexity
of [MIP]. First, by a reduction from the independent set problem, it is shown that [MIP] is
strongly NP-hard without any restrictions on the numbers of cardinality constraints, even if the
objective function coefficients, β, are all binary. Secondly, we explore two specific cases where either
the number of leader’s cardinality constraints, K1, or the follower’s cardinality constraints, K2, is
fixed. For these cases, we propose two polynomial-time algorithms: a dynamic programming (DP)-
based approach and a single-level dual reformulation of [MIP], respectively. In particular, in the
DP-based approach we construct an optimal leader’s decision sequentially, for each subset of the
ground set controlled by the follower. On the other hand, the single-level dual reformulation is
5
reduced to a series of linear optimization problems over a partition matroid. These problems can
be efficiently solved using the well-known greedy algorithm [10]. The summary of our main results
is provided in Table 2.
Also, motivated by the study in [27], we analyze a standard 2-flip local search algorithm for the
leader, assuming that the follower’s feasible set is a uniform matroid, i.e., K2 = 1. We establish
that the outlined algorithm terminates in O(n2) steps, without any additional restrictions on the
leader’s feasible set; for a detailed estimate of the number of arithmetic operations, please refer to
Section 3.3. Furthermore, if the leader’s feasible set is a matroid, then, under a mild assumption,
the 2-flip local search algorithm identifies a globally optimal solution of [MIP]. As an additional
finding, we demonstrate that both aforementioned results remain valid as long as the leader’s and
the follower’s objective function coefficients are different but maintain the same relative order.
K2 is fixed
K2 is not fixed
Complexity: O(nmin{2K1,K2}+1)
Complexity: O(n2K1+1)
K1 is fixed
SA: Duality-based, DP-based
SA: DP-based (Section 3.2)
Complexity: O(nK2+1)
K1 is not fixed
SA: Duality-based (Section 3.1)
Complexity: NP-hard (Section 2)
Table 2: The computational complexity of [MIP] and the respective solution approaches (SA) under different as-
sumptions on K1 and K2. Also, for a particular case of K2 = 1, we design a local search algorithm for the leader,
which is less efficient than the duality-based approach but can be applied to a more general version of [MIP]; see
Section 3.3.
One practical motivation behind our formulation [MIP] of the matroid interdiction problem
can be described as follows. We recall that the main goal of the leader in [MIP] is to minimize the
weight of the follower’s decision (which can also be interpreted as the follower’s profit) by selecting
a subset x ∈ Xp to be removed from the ground set. Alternatively, we may consider K1 distinct
leaders, who act independently and are subject to individual cardinality constraints. This concept
simplifies several network interdiction models described in [23, 32], where the leader has different
types of resources and budget constraints, and each arc in the network can be affected by multiple
resources simultaneously. Similarly, we may assume that there are K2 independent followers, each
having individual cardinality constraints, who aim to maximize their total profit. The latter as-
sumption is somewhat similar, by construction, to the model of B¨ohnlein and Schaudt [3], where,
in a particular case, we also have multiple followers with uniform matroids but with a common
ground set; recall our discussion in Section 1.1.
In view of the discussion above, our contributions and differences compared to the existing
literature can be summarized as follows:
• We propose a novel version of the matroid interdiction problem, referred to as [MIP], where
the feasible sets of the leader and the follower are given by partition matroids.
• We explore the computational complexity of [MIP] and identify conditions under which there
6
exists a transition between polynomially solvable problem classes and those that are strongly
NP-hard.
• In contrast to the majority of MIPs with a single budget constraint [8, 14, 33], our model
allows using several independent cardinality constraints for the leader.
• In contrast to the bilevel pricing model of B¨ohnlein and Schaudt [3], we assume that the
leader’s feasible set is also a partition matroid and the leader selects a subset of ground
elements but not the vector of associated weights.
• Unlike the knapsack interdiction models in [5, 7], [MIP] is not Σp
2-hard and, as we demon-
strate later in Section 3.1, admits a single-level linear integer programming reformulation of
polynomial size.
The remainder of this study is organized as follows. In Section 2, we prove that [MIP] is NP-
hard in the general case. In Sections 3.1–3.3, we develop a duality-based algorithm, a DP-based
algorithm, and a 2-flip local search algorithm, respectively, for several particular versions of [MIP].
Finally, Section 4 presents our conclusions and suggests potential directions for future research.
2. General case: NP -hardness
In this section, we demonstrate that the matroid interdiction problem [MIP] is NP-hard for
non-fixed K1 and K2, even when the weight vector β is restricted to binary values, i.e., β ∈ {0, 1}n.
To establish this result, we construct a polynomial-time reduction from the maximum independence
set problem (MISP), which is known to be NP-hard problem in the strong sense [1]. A decision
version of MISP is formulated as follows:
[MISP-D]: Given an integer Q ∈ Z>0 and a graph G = (V, E), where V is a set of vertices
and E is a set of edges in G, is there a subset S ⊆ V such that no two vertices in S are
adjacent to each other in E, and |S| ≥ Q?
Given an instance of [MISP-D] we construct an instance of the matroid interdiction prob-
lem [MIP], say [MIP-D], as follows. First, we assume that n = |V | · |E|, K1 = |E| and K2 = |V |.
Furthermore, let Bk′ = 1 for k′ ∈ {1, . . . , |E|} and Dk = 1 for k ∈ {1, . . . |V |}. Then, we define the
leader’s and the follower’s feasible sets, respectively, as follows:
e
Xp =
n
x ∈ {0, 1}|V |×|E| :
X
v∈V
xv,e ≤ 1
∀e ∈ E
o
,
(3a)
eYp(x) =
n
y ∈ {0, 1}|V |×|E| : y ≤ 1 − x,
X
e∈E
yv,e ≤ 1
∀v ∈ V
o
.
(3b)
Finally, let eβv,e = 1, if the edge e ∈ E is incident to the vertex v ∈ V and eβv,e = 0, otherwise.
Then, [MIP-D] is formulated as follows:
7
[MIP-D]: Given an integer Q′ = |V | − Q ∈ Z>0, is there a blocking decision x ∈ e
Xp that
satisfies
max
y∈ eYp(x)
eβ
⊤y = eβ
⊤y∗(x) ≤ Q′?
The following result holds.
Theorem 1. Any “yes”-instance of [MISP-D] corresponds to a “yes”-instance of [MIP-D], and
vice versa.
Proof. ⇐ Assume that we have a “yes” instance of [MIP-D]. Note that
|V | − Q ≥
max
y∈ eYp(x)
eβ
⊤y =
X
v∈V
max
yv∈{0,1}|E|
n X
e∈E
eβv,eyv,e : yv ≤ 1 − xv,
X
e∈E
yv,e ≤ 1
o
.
Since eβ ∈ {0, 1}|V |×|E|, we conclude that for each v ∈ V
Rv :=
max
yv∈{0,1}|E|
n X
e∈E
eβv,eyv,e : yv ≤ 1 − xv,
X
e∈E
yv,e ≤ 1
o
=



0, if eβv,e(1 − xv,e) = 0
∀e ∈ E,
1, otherwise.
Consequently, at least Q values among Rv, v ∈ V , are equal to zero. Let S ⊆ V , |S| ≥ Q, be a set
of vertices such that Rv = 0 for each v ∈ S. We demonstrate that S is an independent set of G.
Assume to the contrary that there exist two vertices, v1 ∈ S and v2 ∈ S, that are incident to
each other. Then, there exists an edge e ∈ E, which is incident to both v1 and v2. By construction
of eβ, we have eβv1,e = eβv2,e = 1. However, since x ∈ e
Xp, it can be observed that
xv1,e + xv2,e ≤ 1
and, hence, either eβv1,e(1 − xv1,e) = 1 or eβv2,e(1 − xv2,e) = 1. As a result, either Rv1 = 1 or Rv2 = 1
that contradicts to the definition of S.
⇒ Suppose that there exists an independent set S in G such that |S| ≥ Q. Then, any edge
e ∈ E is incident to at most one vertex from S, i.e.,
X
v∈S
eβv,e ≤ 1
∀e ∈ E.
We define a blocking decision in the following way. For any v ∈ V and e ∈ E let
xv,e =



1, if eβv,e = 1 and v ∈ S,
0, otherwise.
Clearly, x ∈ e
Xp and, furthermore,
Rv =
max
yv∈{0,1}|E|
n X
e∈E
eβv,eyv,e : yv ≤ 1 − xv,
X
e∈E
yv,e ≤ 1
o
= 0
∀v ∈ S.
8
Since Rv ∈ {0, 1}, we conclude that
max
y∈ eYp(x)
eβ
⊤y =
X
v∈V
Rv ≤ |V | − Q,
which implies a “yes” instance of [MIP-D]. This observation concludes the proof.
It is rather straightforward to verify that Theorem 1 provides a polynomial-time reduction
from [MISP-D] to [MIP-D]. The strong NP-completeness of [MIP-D] also implies that there is
no fully polynomial-time approximation scheme (or FPTAS) for the matroid interdiction problem
[MIP] unless P = NP [17].
3. Polynomially solvable cases
3.1. Duality-based algorithm
In this section, we consider a particular case where the leader’s feasible set X is either an
arbitrary matroid or an arbitrary binary set, and the number of follower’s cardinality constraints,
K2, is fixed.
In the first step, we observe that the follower’s problem in [MIP] for any fixed
x ∈ {0, 1}n can be viewed as a linear programming problem of the form (recall Table 1 for the
notations used):
max
y
β⊤y
(4a)
s.t. y ≥ 0
(4b)
y ≤ 1 − x
(4c)
X
i∈Ck
yi ≤ Dk
∀k ∈ K2.
(4d)
In particular, the integrality constraints y ∈ {0, 1}n are relaxed, since any matroid polyhedron, i.e.,
a convex hull of incidence vectors of its independent sets, is integral and a polyhedron related to
the partition matroid coincides with the feasible set in (4); see, e.g., [11, 26].
In the following, we assume that the superscript “(k)” for k ∈ K2 refers to the subset of
indices Ck. Furthermore, without loss of generality, the weight vector β(k) ∈ R|Ck|
+
is sorted in a
non-decreasing order, i.e.,
β
(k)
0
:= 0 ≤ β
(k)
1
≤ β
(k)
2
≤ . . . ≤ β
(k)
|Ck| < β
(k)
|Ck|+1 := +∞.
(5)
The following result provides an equivalent dual reformulation of [MIP].
Theorem 2. Assume that the leader’s feasible set X is an arbitrary binary set, i.e., X ⊆ {0, 1}n.
Then, the matroid interdiction problem [MIP] can be equivalently reformulated as:
min
x∈X
X
k∈K2
min
jk∈{0,...,|Ck|}
n
β
(k)
jk Dk +
|Ck|
X
i=jk+1
(β
(k)
i
− β
(k)
jk )(1 − x
(k)
i )
o
.
(6)
9
Proof. First, we obtain a dual reformulation of the follower’s problem (4) for a fixed x ∈ X. In this
regard, we observe that (4) can be expressed as:
X
k∈K2
max
y(k)
n
β(k)⊤y(k) : 0 ≤ y(k) ≤ 1 − x(k), 1⊤y(k) ≤ Dk
o
,
(7)
where the superscript “(k)” for k ∈ K2 refers to the indices in Ck. Then, the dual reformulation of
each term in (7) yields an equivalent dual reformulation of (4) given by:
X
k∈K2
min
α(k),γ(k)
n
α(k)Dk + γ(k)⊤(1 − x(k))
o
(8a)
s.t. γ(k) ≥ β(k) − α(k)1
(8b)
γ(k) ≥ 0
(8c)
α(k) ≥ 0,
(8d)
where γ(k) ∈ R|Ck|
+
and α(k) ∈ R+, k ∈ K2, are dual variables corresponding to the primal constraints
(4c) and (4d), respectively.
We observe that the objective function (8a) is bilinear.
In order to simplify the resulting
problem, we make use of our assumption in (5) and express the dual reformulation (8) as:
X
k∈K2
min
jk∈{0,...,|Ck|}
min
α(k,jk),γ(k,jk)
n
α(k,jk)Dk + γ(k,jk)⊤(1 − x(k))
o
s.t. γ(k,jk) ≥ β(k) − α(k,jk)1
γ(k,jk) ≥ 0
α(k,jk) ∈ [β
(k)
jk , β
(k)
jk+1],
where the feasible region α(k) ≥ 0 is divided into |Ck|+1 disjoint intervals [β
(k)
l , β
(k)
l+1], l ∈ {0, . . . , |Ck|},
and, with some abuse of notation, the last interval is assumed to be half-open. As a result, for each
k ∈ K2 and jk ∈ {0, . . . , |Ck|} the optimal value of γ(k,jk) is given by:
γ∗(k,jk)
i
=



β
(k)
i
− α(k,jk), if i ≥ jk + 1,
0, if i ≤ jk.
(9)
Using (9), the follower’s problem (4) can be reformulated as:
X
k∈K2
min
jk∈{0,...,|Ck|} min
α(k,jk)
n
α(k,jk)Dk +
|Ck|
X
i=jk+1
(β
(k)
i
− α(k,jk))(1 − x
(k)
i )
o
s.t. α(k,jk) ∈ [β
(k)
jk , β
(k)
jk+1].
Hence, optimal α∗(k,jk) ∈ {β
(k)
jk , β
(k)
jk+1} for jk ≤ |Ck| − 1 and α∗(k,jk) = β
(k)
jk for jk = |Ck|. Taking into
10
account these observations, we derive a final reformulation of (4) given by:
X
k∈K2
min
jk∈{0,...,|Ck|−1}
n
min

β
(k)
jk Dk +
|Ck|
X
i=jk+1
(β
(k)
i
− β
(k)
jk )(1 − x
(k)
i ); β
(k)
jk+1Dk +
|Ck|
X
i=jk+1
(β
(k)
i
− β
(k)
jk+1)(1 − x
(k)
i )
	o
=
X
k∈K2
min
jk∈{0,...,|Ck|}
n
β
(k)
jk Dk +
|Ck|
X
i=jk+1
(β
(k)
i
− β
(k)
jk )(1 − x
(k)
i )
o
.
(10)
Here, we exploit the fact that the overall minimum in the left-hand side of (10) is taken with respect
to 2|Ck| terms, |Ck| − 1 of which are duplicated. Eventually, the dual reformulation of [MIP] is
obtained by combining the minimum over x ∈ X and the right-hand side of (10).
The first observation from Theorem 2 is that the matroid interdiction problem [MIP] with
a partition matroid, i.e., X = Xp, and rational parameters admits a single-level linear integer
programming (ILP) reformulation of polynomial size. That is, taking into account the dual refor-
mulation (6), for each k ∈ K2 we may introduce new variables
tk =
min
jk∈{0,...,|Ck|}
n
β
(k)
jk Dk +
|Ck|
X
i=jk+1
(β
(k)
i
− β
(k)
jk )(1 − x
(k)
i )
o
.
(11)
If all parameters in (11) are rational, then, after multiplying (11) by a sufficiently large constant,
we may redefine tk, k ∈ K2, as integer variables. Then, an equivalent ILP reformulation of (6) is
obtained by applying standard linearization techniques to the right-hand side of (11); see, e.g., [31].
Next, by leveraging Theorem 2, we demonstrate that [MIP] is fixed-parameter tractable, if X
is a partition matroid, i.e., X = Xp, and the number of follower’s cardinality constraints, K2, is
fixed. The following result holds.
Corollary 1. Assume that X = Xp and K2 ≥ 1 is fixed. Then, the matroid interdiction prob-
lem [MIP] can be solved in O(nK2+1) arithmetic operations. Moreover, if X ⊆ {0, 1}n is a general
matroid, then the number of operations required is given by O
tion (6) can be written as:
min
x∈X
X
k∈K2
min
jk∈{0,...,|Ck|}
n
β
(k)
jk Dk +
|Ck|
X
i=jk+1
(β
(k)
i
− β
(k)
jk )(1 − x
(k)
i )
o
=
min
x∈Xp min
j ∈S
X
k∈K2
n
β
(k)
jk Dk +
|Ck|
X
i=jk+1
(β
(k)
i
− β
(k)
jk )(1 − x
(k)
i )
o
=
min
j ∈S min
x∈Xp
X
k∈K2
n
β
(k)
jk Dk +
|Ck|
X
i=jk+1
(β
(k)
i
− β
(k)
jk )(1 − x
(k)
i )
o
,
(12)
where we use the fact that the sum of minima equals to the minimum value obtained from all possible
sums of corresponding elements and the minimization can be realized in any predefined order.
Next, we observe that for any fixed realization of j ∈ S the problem in the right-hand side
of (12) is a linear optimization problem over a partition matroid. We establish that this problem
can be solved in O(n log n) operations by the standard greedy algorithm [10]. First, it requires O(n)
operations to compute the coefficients of the objective function in (12) and O(n log n) operations
to sort them in a non-decreasing order. Then, at each step of the greedy algorithm we need to solve
a feasibility problem of the form ex ∈ Xp for some ex ∈ {0, 1}n. There are n feasibility problems
and each problem can be solved in O(1) operations, if we maintain residual budgets with respect
to each cardinality constraint of the leader.
Eventually, by leveraging the inequality of arithmetic and geometric means, we observe that
|S| = | eC1 × . . . × eCK2| =
K2
Y
k=1
problem setting we design a DP-based algorithm that sequentially computes an optimal blocking
decision of the leader for each subset Ck, k ∈ K2, controlled by the follower.
A key idea of the DP-based algorithmic scheme can be summarized as follows. First, according
to the proof of Theorem 2, the matroid interdiction problem [MIP] with X = Xp can be expressed
as:
min
x∈Xp
X
k∈K2
β(k)⊤y∗(k)(x(k)),
where
y∗(k)(x(k)) ∈ argmaxy(k)
n
β(k)⊤y(k) : 0 ≤ y(k) ≤ 1 − x(k), 1⊤y(k) ≤ Dk
o
.
In other words, the follower’s decision y∗(k)(x(k)), in each subset Ck, k ∈ K2, is only defined by the
leader’s decision, x(k), within this subset. Then, a feasible set for x(k), k ∈ K2 can be defined in a a
parametric form as:
X (k)(bk) =
n
ex(k) ∈ {0, 1}|Ck| :
X
i∈ Ck∩Ak′
exi ≤ bk,k′
∀k′ ∈ K1
o
,
(13)
where ex(k) = {exi, i ∈ Ck}, bk,k′ ∈ Z+ and bk,k′ ≤ Bk′ for k′ ∈ K1; for simplicity of exposition and
with some abuse of notation, we do not maintain additional indices 1, . . . , |Ck| for the elements
in Ck. In addition, for a fixed budget vector bk a bilevel subproblem related to Ck can be expressed
as:
min
x(k)∈X (k)(bk) max
y(k)
n
β(k)⊤y(k) : 0 ≤ y(k) ≤ 1 − x(k), 1⊤y(k) ≤ Dk
o
.
(14)
Next, following the DP paradigm, for each stage k ∈ K2 we define a state, bk, as a vector of
leader’s residual budgets at the beginning of stage k, i.e.,
bk,k′ ∈ {0, 1, . . . , Bk′}
∀k′ ∈ K1;
in particular, b1,k′ = Bk′ for each k′ ∈ K1. Also, for each k ∈ K2 and each state bk we introduce a
set of actions
x(k)(bk) ∈ X (k)(bk) ⊆ {0, 1}|Ck|
that corresponds to a blocking decision in Ck.
A pseudocode of the proposed DP-based algorithm is illustrated by Algorithm 1. Initially, at
each stage l ∈ {K2, . . . , 1} we compute a value function S∗
l (bl) and the associated optimal action
x∗(l)(bl) for any feasible state vector bl; see lines 4–5 and 9–10 of Algorithm 1.
In particular,
in line 8 we define a new state, bl+1, as a function of the previous state, bl, and the previously
selected blocking decision, x(l). Finally, an optimal blocking decision, x∗, is restored in lines 12–13
and 16–17. The following result provides the computational complexity of Algorithm 1.
Theorem 3. Assume that X = Xp and K1 ≥ 1 is fixed. Then, Algorithm 1 solves the matroid
interdiction problem [MIP] in O(n2K1+1) operations.
13
Algorithm 1: A DP-based algorithm for solving [MIP].
1 Input: feasible sets of the leader and the follower, respectively, Xp and Yp(x) for x ∈ Xp.
2 Output: an optimal decision, x∗ ∈ Xp, and an optimal objective function value, β⊤y∗(x∗).
3 l ←− K2
4 S∗
l (bl) ←− minx(l)∈X (l)(bl)
n
β(l)⊤y∗(l)(x(l))
o
for all feasible bl
5 x∗(l)(bl) ←− argminx(l)∈X (l)(bl)
n
β(l)⊤y∗(l)(x(l))
o
for all feasible bl
6 for l ∈ {K2 − 1, . . . , 1} and all feasible bl
7 begin
8
bl+1,k′ ←− bl,k′ − P
i∈Cl∩Ak′ xi
∀k′ ∈ K1
9
S∗
l (bl) ←− minx(l)∈X (l)(bl)
n
β(l)⊤y∗(l)(x(l)) + S∗
l+1(bl+1)
o
10
x∗(l)(bl) ←− argminx(l)∈X (l)(bl)
n
β(l)⊤y∗(l)(x(l)) + S∗
l+1(bl+1)
o
11 end
12 b∗
1,k′ ←− Bk′
∀k′ ∈ K1
13 x∗(1) ←− x∗(1)(B)
14 for l ∈ {2, . . . , K2}
15 begin
16
b∗
l,k′ ←− b∗
l−1,k′ − P
i∈Cl−1∩Ak′ x∗
i
∀k′ ∈ K1
17
x∗(l) ←− x∗(l)(b∗
l )
18 end
19 return x∗, S∗
1(B).
Proof. Algorithm 1 starts with l = K2 and computes a value function
S∗
K2(bK2) =
min
x(K2)∈X (K2)(bK2)
n
β(K2)⊤y∗(K2)(x(K2))
o
.
(15)
for all feasible bK2,k′ ∈ {0, 1, . . . , Bk′}, k′ ∈ K1. Without loss of generality, we assume that the
weight vector β ∈ Rn
+ is sorted in a non-decreasing order and, hence, β(K2) = {βi, i ∈ CK2} satisfies
0 ≤ βi1 ≤ βi2 ≤ . . . ≤ βi|CK2 |,
where ij ∈ CK2, j ∈ {1, . . . , |CK2|}. Thus, for each k′ ∈ K1, i′ ∈ CK2 ∩ Ak′ and i′′ ∈ CK2 ∩ Ak′ such
that i′ < i′′, an optimal optimal blocking decision x∗(K2) satisfies
x∗
i′(bK2) ≤ x∗
i′′(bK2).
Indeed, if x∗
i′(bK2) = 1 and x∗
i′′(bK2) = 0, then it is always possible to switch these two elements.
More specifically, the obtained solution remains feasible in (15) and cannot increase the optimal
follower’s objective function value; recall the definitions of feasible set (13) and subproblem (14),
respectively.
Summarizing the discussion above, we observe that for each fixed bK2 a number of candidate
14
optimal values for x∗(K2)(bK2) can be reduced to
K1
Y
k′=1
3.3. Local search for the case of a uniform matroid
In this section, we consider a particular version of the matroid interdiction problem [MIP],
where the leader’s feasible set, X, is either an arbitrary matroid or an arbitrary binary set and the
follower’s feasible set, Yp(x), is a uniform matroid. Formally, we set K2 = 1 and assume that for
any x ∈ X
Yp(x) = Yu(x) =
n
y ∈ {0, 1}n : y ≤ 1 − x,
n
X
i=1
yi ≤ D
o
.
A duality-based approach to solving the considered version of [MIP] is provided by Theorem 2
and Corollary 1; see Section 3.1. In addition to this approach, we demonstrate that [MIP] with
an arbitrary matroid X and K2 = 1 can be alternatively solved using a standard 2-flip local search
algorithm for the leader. Specifically, we prove that the aforementioned algorithm takes at most
O(n2) steps for any binary set X and identifies a global minimum of [MIP] if X is a matroid.
This result can be seen as an extension of Theorem 5 in [27], where it is established that the 2-flip
local search algorithm identifies a globally optimal solution for linear optimization problems over
an arbitrary matroid. Initially, we introduce some technical definitions.
Definition 1. A solution ex∗ is called m-flip locally optimal for [MIP], if ex∗ ∈ X and for any
x′ ∈ X such that ∥x′ − ex∗∥1 ≤ m, m ∈ Z>0, we have
β⊤y∗(ex∗) ≤ β⊤y∗(x′).
(17)
Also, ex∗ is globally optimal for [MIP], if (17) is satisfied for any x′ ∈ X.
□
Algorithm 2: A 2-flip local search algorithm for [MIP].
1 Input: feasible sets of the leader and the follower, respectively, X and Yu(x) for x ∈ X, an
initial point x0 ∈ X.
2 Output: a 2-flip locally optimal solution, ex∗, and its objective function value, β⊤y∗(ex∗).
3 x ←− x0
4 L(x) ←− a list of points x′ ∈ X such that ∥x′ − x∥1 ≤ 2 and β⊤y∗(x′) < β⊤y∗(x)
5 while L ̸= ∅
6 begin
7
x ←− x′ for some x′ ∈ L
8
L(x) ←− L(x′)
9 end
10 ex∗ ←− x
11 return ex∗, β⊤y∗(ex∗).
Then, an m-flip local search algorithm at a current point x ∈ X attempts to make a feasible flip
by moving to a “neighboring” point x′ ∈ X such that ∥x′ − x∥1 ≤ m and
β⊤y∗(x′) < β⊤y∗(x).
16
The algorithm terminates whenever a locally optimal solution of [MIP] is identified. For com-
pleteness, we illustrate a pseudocode of the outlined local search approach in Algorithm 2. The
following result holds.
Theorem 4. Let X ⊆ {0, 1}n be an arbitrary binary set and K2 = 1. Then, Algorithm 2 from
any prespecified initial point x0 ∈ X takes at most O(n2) steps and requires O(n4f(n)) arithmetic
operations, where f(n) is the worst-case complexity of solving the feasibility problem x ∈ X for a
given x ∈ {0, 1}n.
Proof. Without loss of generality, we assume that the weight vector β ∈ Rn
+ is sorted in a non-
decreasing order, i.e.,
β0 := 0 ≤ β1 ≤ . . . ≤ βn.
Let R(βi) ∈ {1, . . . , n} be a rank of βi, i ∈ N, in the outlined sequence; we assume that R(βi) =
R(βj) whenever βi = βj for some i, j ∈ N and R(β0) = 0. Also, for any x ∈ X we define j(x, D)
as an index of the D-th zero element from the end in vector x, i.e., j(x, D) satisfies equation
n
X
i=j(x,D)
(1 − xi) = D
with xj(x,D) = 0. Otherwise, if the number of zero elements is smaller than D, i.e., n − ∥x∥1 < D,
then let j(x, D) = 0.
Based on the above definitions, we introduce a potential function
Φ(x) =
n
X
t=1
R(βt)y∗
t (x) =
n
X
t=j(x,D)
R(βt)(1 − xt) ≥ 0,
where the last equality follows from the definition of j(x, D). It can be observed that Φ(x) is
essentially the objective function of [MIP] with a modified weight vector.
In the following, we demonstrate that any feasible 1- and 2-flip decreases the value of Φ(x) at
least by one. Therefore, the total number of steps taken by Algorithm 2 is bounded from above by
the maximal possible value of Φ(x).
Let x ∈ X be a current point of Algorithm 2. Then, for 1-flips of the form x(1) = x + ei ∈ X,
i ∈ N, we observe that
j(x(1), D) =



j(x, D), if i < j(x, D),
j(x, D + 1), if i ≥ j(x, D),
(18)
17
and
Φ(x(1)) − Φ(x) =
n
X
t=j(x(1),D)
R(βt)(1 − x
(1)
t ) −
n
X
t=j(x,D)
R(βt)(1 − xt)
=



0, if i < j(x, D),
R(βj(x,D+1)) − R(βi), if i ≥ j(x, D).
(19)
In particular, we use the fact that xt = x
(1)
t
= 1, if j(x, D + 1) + 1 ≤ t ≤ j(x, D) − 1 and
i ≥ j(x, D). Furthermore, it is clear that the variation in the objective function of the leader,
β⊤y∗(x), can be defined in a similar way to equation (19). Consequently, any feasible 1-flip at x
satisfies βj(x,D+1) < βi and decreases the potential function Φ(x) at least by one.
Analogously, by setting x(1) = x − ei ∈ X, i ∈ N, we observe that
j(x(1), D) =









j(x, D), if i < j(x, D),
j(x, D − 1), if i > j(x, D − 1),
i, if j(x, D) < i < j(x, D − 1),
and
Φ(x(1)) − Φ(x) =
n
X
t=j(x(1),D)
R(βt)(1 − x
(1)
t ) −
n
X
t=j(x,D)
R(βt)(1 − xt)
=



0, if i < j(x, D),
R(βi) − R(βj(x,D)), if i > j(x, D).
(20)
Since βi ≥ βj(x,D) for i > j(x, D), 1-flips of the form x(1) = x − ei are always infeasible.
Next, we consider 2-flips of the form x(2) = x + ei − el ∈ X for some i, l ∈ N, i ̸= l. Using
equations (18), (19) and (20) with x(1) = x + ei, it can be observed that
Φ(x + ei − el) − Φ(x) =

Φ(x(1) − el) − Φ(x(1))

+

Φ(x + ei) − Φ(x)

=















0, if i < j(x, D) and l < j(x, D),
R(βl) − R(βj(x,D)), if i < j(x, D) and l > j(x, D),
R(βj(x,D+1)) − R(βi), if i ≥ j(x, D) and l < j(x, D + 1),
R(βl) − R(βi), if i ≥ j(x, D) and l > j(x, D + 1).
(21)
Similarly to 1-flips, we conclude that any feasible 2-flip of the form x(2) = x + ei − el decreases
Φ(x) at least by one.
For 2-flips of the form x(2) = x + ei + el ∈ X, i, l ∈ N, i ̸= l, equation (19) yields that
Φ(x + ei + el) − Φ(x) =

Φ(x + ei + el) − Φ(x + ei)

+

Φ(x + ei) − Φ(x)

≤ 0.
Moreover, if Φ(x + ei + el) − Φ(x) = 0, then Φ(x + ei + el) = Φ(x + ei) = Φ(x) and the resulting
18
2-flip is infeasible. Therefore, any feasible 2-flip of the form x(2) = x + ei + el decreases Φ(x) at
least by one. Finally, two flips the form x(2) = x − ei − el ∈ X, i, l ∈ N, i ̸= l, are always infeasible
due to equation (20).
As a result, since
0 ≤ Φ(x) ≤ 1 + . . . + n = n(n + 1)
2
,
the number of steps taken by Algorithm 2 from any given initial point x0 ∈ X is at most O(n2).
With respect to the number of arithmetic operations, we need O(n log n) operations to compute
the index j(x0, D) and the initial objective function value β⊤y∗(x0); these values need to be main-
tained at any point of Algorithm 2. Then, at each current point x ∈ X we analyze at most O(n2)
neighboring points, for which the respective change in the leader’s objective function value can be
computed in O(1) operations and feasibility can be checked in O(f(n)) operations. Therefore, the
total number of operations taken by Algorithm 2 is given by O(n4f(n)).
In the following, we demonstrate that if X ⊆ {0, 1}n is an arbitrary matroid, then Algorithm 2
identifies a globally optimal solution of [MIP]. In order to establish this, the following mild as-
sumption about the weight vector β is made:
A1. The weight coefficients βi, i ∈ N, are distinct, i.e., 0 ≤ β1 < β2 < . . . < βn.
The following results hold.
Lemma 1. Let Assumption A1 hold, X ⊆ {0, 1}n be an arbitrary matroid and K2 = 1. Then, for
any 2-flip locally optimal solution ex∗ ∈ X of [MIP], a solution ex(1) = ex∗ ± ei such that ex(1) ∈ X
and i < j(ex∗, D) is also 2-flip locally optimal for [MIP].
Proof. See Appendix.
Theorem 5. If conditions of Lemma 1 are satisfied, then any 2-flip locally optimal solution of
[MIP] is also globally optimal.
Proof. Let x∗ ∈ X and ex∗ ∈ X be a globally and a 2-flip locally optimal solutions of [MIP].
Suppose that oppositely to the statement we have
β⊤y∗(x∗) < β⊤y∗(ex∗).
(22)
In the first step of the proof, we show that, if ∥x∗∥1 ̸= ∥ex∗∥1, then we can always modify the
globally optimal solution, x∗, and the locally optimal solution, ex∗, so that ∥x∗∥1 = ∥ex∗∥1 and the
inequality (22) remains satisfied.
If ∥x∗∥1 < ∥ex∗∥1, then by property M3 of matroids there exists i ∈ N such that x∗
i = 0, ex∗
i = 1,
and x∗ + ei ∈ X. Furthermore, for 1-flips of the form x(1) = x∗ + ei ∈ X we observe that
β⊤y∗(x∗) = β⊤y∗(x∗ + ei),
19
recall that x∗ is a global minimum of [MIP] and extending a blocking decision cannot increase the
leader’s objective function value in [MIP]. Hence, x∗ := x(1) is also a global minimum of [MIP].
This procedure can be repeated until the global minimum x∗ of [MIP] satisfies ∥x∗∥1 = ∥ex∗∥1.
If ∥x∗∥1 > ∥ex∗∥1, then by property M3 of matroids there exists i ∈ N such that ex∗
i = 0, x∗
i = 1
and ex∗ + ei ∈ X. Analogously to the previous case, we have
β⊤y∗(ex∗) = β⊤y∗(ex∗ + ei).
From Assumption A1 and equation (19) we observe that i < j(ex∗, D) and, hence, by Lemma 1,
ex(1) = ex∗ + ei is a 2-flip local minimum of [MIP]. As a result, we can set ex∗ := ex(1) and after
repeating this procedure a sufficient number of times we obtain a 2-flip local minimum ex∗ of [MIP],
which satisfies both ∥ex∗∥1 = ∥x∗∥1 and inequality (22).
In the second step, we show that inequality (22) cannot be satisfied provided that ∥x∗∥1 = ∥ex∗∥1.
Let t = |{i ∈ N : x∗
i ̸= ex∗
i }| and l = min{i : x∗
i ̸= ex∗
i , i ∈ N}. We consider the following two cases:
• Let x∗
l = 1 and ex∗
l = 0. By properties M2 and M3 of matroids we observe that x∗ − el ∈ X
and there exists i > l such that x∗
i = 0, ex∗
i = 1 and x(2) = x∗−el+ei ∈ X. Using equation (21)
and the fact that βi > βl, we conclude that
β⊤y∗(x(2)) ≤ β⊤y∗(x∗)
and, furthermore, β⊤y∗(x(2)) = β⊤y∗(x∗), as x∗ is a global minimum of [MIP]. Hence, we
may set x∗ := x(2) and t := t − 2.
• Let x∗
l = 0 and ex∗
l = 1. By properties M2 and M3 of matroids we observe that ex∗ − el ∈ X
and there exists i > l such that ex∗
i = 0, x∗
i = 1, and ex(2) = ex∗ − el + ei ∈ X. Hence, following
the previous case, we have
β⊤y∗(ex∗(2)) = β⊤y∗(ex∗).
The latter equality, (21) and Assumption A1 imply that i < j(ex∗, D) and l < j(ex∗, D).
Thus, by Lemma 1, ex(2) is a 2-flip locally optimal solution of [MIP] and we set ex∗ := ex∗(2)
and t := t − 2.
The outlined process, after a finite number of steps, results in t = 0 and x∗ = ex∗ that contradicts
inequality (22). This observation concludes the proof.
We conclude that Algorithm 2 requires O(n4f(n)) operations for an arbitrary binary set X and
any prespecified initial point. However, if X is an arbitrary matroid, then Algorithm 2 becomes
considerably less efficient in comparison with the duality-based approach; recall Corollary 1. At the
same time, there may exist a particular class of matroids and initial points, for which the desired
complexity, O(n2f(n)), is achieved by Algorithm 2. We leave this question as a possible direction
of future research.
20
On a positive note, it is rather straightforward to verify that Theorems 4 and 5 remain valid,
if the leader’s and the follower’s objective function coefficients maintain the same relative order.
This version of [MIP] with a uniform matroid can be expressed as follows:
[MIP′]:
min
x∈X β⊤y∗(x)
s.t. y∗(x) ∈ argmaxy∈Yu(x) β′⊤y,
where β′ ∈ Rn
+, β′ ̸= β, and both vectors are sorted in a non-decreasing order. In particular,
correctness of Theorems 4 and 5 for [MIP′] is implied by the structure of Algorithm 2 and cannot
be derived explicitly from the duality-based approach discussed in Section 3.1.
As an additional observation, in the following result we demonstrate that Theorem 4 remains
valid, if no additional restrictions on β′ are made but we use a “reduced” 2-flip local search algo-
rithm. More specifically, the “reduced” algorithm coincides with Algorithm 2 but prohibits 2-flips of
the form x(2) = x+el+ei ∈ X and x(2) = x−el−ei ∈ X, i, l ∈ N, i ̸= l, at any current point x ∈ X.
Corollary 2. Let X ⊆ {0, 1}n be an arbitrary binary set, β′ ∈ Rn
+ and β′ ̸= β. Then, the “reduced”
2-flip local search algorithm for [MIP′] from any prespecified initial point takes at most O(n2) steps
and requires O(n4f(n)) arithmetic operations, where f(n) is the worst-case complexity of solving
the feasibility problem x ∈ X for a given x ∈ {0, 1}n.
Proof. See Appendix.
The motivation behind not using 2-flips of the form x(2) = x + ei + el and x(2) = x − ei − el,
i, l ∈ N, i ̸= l, in the abovementioned “reduced” 2-flip local search algorithm is that these flips
induce 4-flips for the associated follower’s decision, y∗(x).
In other words, the change in the
objective function value of the leader is no more proportional to the change in the associated
potential function.
Finally, as it might be expected, the “reduced” 2-flip local search algorithm (and, furthermore,
any standard m-flip local search algorithm with fixed m) cannot identify a globally optimal solution
of [MIP′], if X is an arbitrary matroid and no restrictions on β′ are made. In this regard, we
consider the following counterexample.
Example 1. Consider an MIP of the form:
min
x∈X
n
y∗
1(x) +
n−1
X
i=2
(n − i + 1)y∗
i (x) + ny∗
n(x)
o
(23a)
s.t. y∗(x) ∈ argmaxy∈{0,1}n
n
n
X
i=1
iyi : y ≤ 1 − x,
n
X
i=1
yi ≤ 1
o
,
(23b)
where X =
n
x ∈ {0, 1}n : Pn
i=1 xi ≤ n − 1
o
is a uniform matroid. It can be verified that
x∗ = (0, 1, . . . , 1)⊤ ∈ X
21
is a globally optimal solution of (23) with the leader’s objective function value of 1.
On the
other hand,
ex∗ = (0, . . . , 0, 1)⊤ ∈ X
is a locally optimal solution of (23) with respect to the “reduced” 2-flip local search algorithm.
That is, β⊤y∗(ex∗) = 2 and there exist no 1-flips or 2-flips of the form ex(2) = ex + ei − en for some
i ∈ {1, . . . , n − 1} that may reduce the leader’s objective function value in (23). As a byproduct,
we observe that ex∗ is locally optimal with respect any m-flip local search algorithm with a constant
parameter m ∈ Z+. That is, even if these algorithms terminate in a polynomial number of iterations,
they do not identify the globally optimal solution of (23).
□
4. Conclusion
In this study, we explore the theoretical computational complexity of a matroid interdiction
problem (MIP), where both the leader’s and the follower’s feasible sets are given by partition ma-
troids.
From a practical perspective, the problem can be viewed as a zero-sum game between
multiple independent leaders and multiple independent followers, where each leader and follower is
subject to its own cardinality constraint and aims to minimize (and, respectively, maximize) the
follower’s total profit.
We prove that the considered MIP is strongly NP-hard in the general case and admits two poly-
nomially solvable cases, whenever either the number of leader’s or follower’s cardinality constraints
is fixed.
For the former case, we design a dynamic programming algorithm that subsequently
identifies a blocking decision of the leader for each subset of elements controlled by the follower.
For the latter case, we provide a single-level dual reformulation of the MIP that is solved using a
polynomial number of calls to the well-known greedy algorithm for matroids [10]. Furthermore,
in the case where the follower’s feasible set is a uniform matroid, we develop a 2-flip local search
algorithm for the leader. While this algorithm is not as efficient as the duality-based approach, it
can still be employed whenever the leader’s and follower’s objective function coefficients differ but
maintain the same relative order.
With respect to future research directions, in view of the study by B¨ohnlein and Schaudt [3],
it could be of interest to consider an MIP, where one of the feasible sets, either for the leader or
the follower, is a laminar matroid with a fixed number of cardinality constraints. This problem is
at least as hard as the problem with a partition matroid, but its complexity status remains open.
Another possible direction is to explore existence of constant factor approximation algorithms
for the considered MIP formulation over partition matroids. In particular, taking into account
our reduction from Section 2, the absence of constant factor approximation algorithms for the
independent set problem; see, e.g., [1], does not imply the same result for the MIP over partition
matroids.
Therefore, an analysis of approximation algorithms for the considered MIP calls for
application of more advanced techniques.
22
References
[1] Arora, S. and Barak, B. (2009). Computational Complexity: a Modern Approach. Cambridge
University Press.
[2] Beheshti, B., Prokopyev, O. A., and Pasiliao, E. L. (2016). Exact solution approaches for bilevel
assignment problems. Computational Optimization and Applications, 64:215–242.
[3] B¨ohnlein, T. and Schaudt, O. (2020). On the complexity of stackelberg matroid pricing prob-
lems.
In Combinatorial Algorithms: 31st International Workshop, IWOCA 2020, Bordeaux,
France, June 8–10, 2020, Proceedings 31, pages 83–96. Springer.
[4] Buchheim, C., Henke, D., and Hommelsheim, F. (2022).
On the complexity of the bilevel
minimum spanning tree problem. Networks, 80(3):338–355.
[5] Caprara, A., Carvalho, M., Lodi, A., and Woeginger, G. J. (2013).
A complexity and ap-
proximability study of the bilevel knapsack problem. In Integer Programming and Combinatorial
Optimization: 16th International Conference, IPCO 2013, Valpara´ıso, Chile, March 18-20, 2013.
Proceedings 16, pages 98–109. Springer.
[6] Caprara, A., Carvalho, M., Lodi, A., and Woeginger, G. J. (2014). A study on the computational
complexity of the bilevel knapsack problem. SIAM Journal on Optimization, 24(2):823–838.
[7] Caprara, A., Carvalho, M., Lodi, A., and Woeginger, G. J. (2016).
Bilevel knapsack with
interdiction constraints. INFORMS Journal on Computing, 28(2):319–333.
[8] Chestnut, S. R. and Zenklusen, R. (2017). Interdicting structured combinatorial optimization
problems with {0, 1}-objectives. Mathematics of Operations Research, 42(1):144–166.
[9] Dinitz, M. and Gupta, A. (2013).
Packing interdiction and partial covering problems.
In
International Conference on Integer Programming and Combinatorial Optimization, pages 157–
168. Springer.
[10] Edmonds, J. (1971). Matroids and the greedy algorithm. Mathematical Programming, 1:127–
136.
[11] Edmonds, J. (2003). Submodular functions, matroids, and certain polyhedra. In Combina-
torial Optimization—Eureka, You Shrink! Papers Dedicated to Jack Edmonds 5th International
Workshop Aussois, France, March 5–9, 2001 Revised Papers, pages 11–26. Springer.
[12] Fischetti, M., Ljubi´c, I., Monaci, M., and Sinnl, M. (2019). Interdiction games and monotonic-
ity, with application to knapsack problems. INFORMS Journal on Computing, 31(2):390–410.
[13] Frederickson, G. N. and Solis-Oba, R. (1997). Efficient algorithms for robustness in matroid op-
timization. In Proceedings of the Eight Annual ACM-SIAM Symposium on Discrete Algorithms,
pages 659–668.
23
[14] Frederickson, G. N. and Solis-Oba, R. (1999). Increasing the weight of minimum spanning
trees. Journal of Algorithms, 33(2):244–266.
[15] Frederickson, G. N. and Solis-Oba, R. (2006). Efficient algorithms for robustness in resource
allocation and scheduling problems. Theoretical Computer Science, 352(1-3):250–265.
[16] Fulkerson, D. R. and Harding, G. C. (1977). Maximizing the minimum source-sink path subject
to a budget constraint. Mathematical Programming, 13(1):116–118.
[17] Garey, M. R. and Johnson, D. S. (2002). Computers and Intractability, volume 29. New York,
WH Freeman.
[18] Gassner, E. and Klinz, B. (2009). The computational complexity of bilevel assignment prob-
lems. 4OR, 7:379–394.
[19] Hausbrandt, N., Bachtler, O., Ruzika, S., and Sch¨afer, L. E. (2023).
Parametric matroid
interdiction. arXiv preprint arXiv:2310.05147.
[20] Israeli, E. and Wood, R. K. (2002). Shortest-path network interdiction. Networks, 40(2):97–
111.
[21] Jensen, P. M. and Korte, B. (1982). Complexity of matroid property algorithms. SIAM Journal
on Computing, 11(1):184–190.
[22] Jeroslow, R. G. (1985). The polynomial hierarchy and a simple model for competitive analysis.
Mathematical Programming, 32(2):146–164.
[23] Lunday, B. J. and Sherali, H. D. (2012).
Network interdiction to minimize the maximum
probability of evasion with synergy between applied resources. Annals of Operations Research,
196:411–442.
[24] Orlin, J. B., Punnen, A. P., and Schulz, A. S. (2004). Approximate local search in combinatorial
optimization. SIAM Journal on Computing, 33(5):1201–1214.
[25] Oxley, J. (2011). Matroid theory. In Oxford Graduate Texts in Mathematics, volume 21. 2nd
edn. Oxford University Press.
[26] Schrijver, A. (2003).
Combinatorial Optimization:
Polyhedra and Efficiency, volume 24.
Springer.
[27] Shi, X., Prokopyev, O. A., and Ralphs, T. K. (2023). Mixed integer bilevel optimization with a
k-optimal follower: a hierarchy of bounds. Mathematical Programming Computation, 15(1):1–51.
[28] Shi, X., Zeng, B., and Prokopyev, O. A. (2019). On bilevel minimum and bottleneck spanning
tree problems. Networks, 74(3):251–273.
24
[29] Smith, J. C., Prince, M., and Geunes, J. (2013). Modern network interdiction problems and
algorithms. In Pardalos, P. M., Du, D.-Z., and Graham, R. L., editors, Handbook of Combinatorial
Optimization, pages 1949–1987. Springer.
[30] Smith, J. C. and Song, Y. (2020). A survey of network interdiction models and algorithms.
European Journal of Operational Research, 283(3):797–811.
[31] Wolsey, L. A. (2020). Integer Programming. John Wiley & Sons.
[32] Xiang, Y. (2023).
Minimizing the maximal reliable path with a nodal interdiction model
considering resource sharing. Reliability Engineering & System Safety, 239:109495.
[33] Zenklusen, R. (2010). Matching interdiction. Discrete Applied Mathematics, 158(15):1676–
1690.
[34] Zenklusen, R. (2015). An O(1)-approximation for minimum spanning tree interdiction. In 2015
IEEE 56th Annual Symposium on Foundations of Computer Science, pages 709–728. IEEE.
[35] Zenklusen, R., Ries, B., Picouleau, C., De Werra, D., Costa, M.-C., and Bentz, C. (2009).
Blockers and transversals. Discrete Mathematics, 309(13):4306–4314.
Appendix
Proof of Lemma 1. First, in view of (19), (20) and the fact that i < j(ex∗, D), we conclude
that β⊤y∗(ex∗) = β⊤y∗(ex(1)). Then, we consider the following four particular cases:
• For 1-flips of the form ex(2) = ex(1) ± el = ex∗ ± ei ± el ∈ X, l ̸= i, we observe that
β⊤y∗(ex∗) = β⊤y∗(ex(1)) ≤ β⊤y∗(ex(2)),
since ex∗ is a 2-flip local minimum of [MIP]. Hence, ex(1) is 1-flip locally optimal for [MIP].
• For 2-flips of the form ex(3) = ex∗ ± ei ± el − er = ex(2) − er ∈ X, r /∈ {i, l}, we have
β⊤y∗(ex∗) = β⊤y∗(ex(1)) ≤ β⊤y∗(ex(2)) ≤ β⊤y∗(ex(2) − er) = β⊤y∗(ex(3)),
where we recall that reducing a blocking decision cannot decrease the objective function value
of the leader.
• For 2-flips of the form ex(3) = ex∗ + ei + el + er ∈ X we note that ex∗ + el ∈ X and ex∗ + er ∈ X
by property M2 of matroids. Hence, by the definition of a local minimum we have (recall
that extending a blocking decision cannot increase the objective function value of the leader):
β⊤y∗(ex∗) = β⊤y∗(ex∗ + ei) = β⊤y∗(ex∗ + el) = β⊤y∗(ex∗ + er)
25
In view of Assumption A1 and equation (19), it can be observed that u < j(ex∗, D) for all
u ∈ {i, l, r}. Therefore, β⊤y∗(ex∗) = β⊤y∗(ex∗ + ei) = β⊤y∗(ex(3)) and ex∗ + ei satisfies the
local optimality condition.
• For 2-flips of the form ex(3) = ex∗ − ei + el + er ∈ X we note that ex∗ − ei + el ∈ X and
ex∗ − ei + er ∈ X by property M2 of matroids. Hence, by the definition of a local minimum
we have
β⊤y∗(ex∗) = β⊤y∗(ex∗ − ei) = β⊤y∗(ex∗ − ei + el) = β⊤y∗(ex∗ − ei + er).
In view of Assumption A1, equation (19) and the fact that i < j(ex∗, D), it can be observed
that
u < j(ex∗ − ei, D) = j(ex∗, D)
for all u ∈ {l, r}. Therefore, β⊤y∗(ex∗) = β⊤y∗(ex∗ − ei) = β⊤y∗(ex(3)) and ex∗ − ei satisfies
the local optimality condition.
We conclude that ex(1) = ex∗ ± ei is a 2-flip locally optimal solution of [MIP].
□
Proof of Corollary 2. Without loss of generality, we may assume that
0 ≤ β′
1 ≤ . . . ≤ β′
n,
whereas the weight vector β ∈ Rn
+ is not sorted. Then, the potential function can be written in the
same form as in Theorem 4, i.e.,
Φ(x) =
n
X
t=1
R(βt)y∗
t (x) =
n
X
t=j(x,D)
R(βt)(1 − xt),
where the last equality follows from the fact that β′ is sorted in a non-decreasing order. Conse-
quently, equations (19) and (20) remain valid for (23a).
Using (19) and (20), we observe that a 1-flip of the form x(1) = x + ei ∈ X, i ∈ N, is feasible
only if i ≥ j(x, D) and βi < βj(x,D+1). Furthermore, a 1-flip of the form x(1) = x − ei ∈ X, i ∈ N,
is feasible only if i > j(x, D) and βi < βj(x,D). Hence, any feasible 1-flip decreases Φ(x) at least
by one. The same argument applies to 2-flips of the form x(2) = x + ei − el ∈ X for some i, l ∈ N,
i ̸= l; see equation (21). The remainder of the proof follows the proof of Theorem 4.
□
26
"
"We introduce ALMs, a new authorship attribution method based on the perplexity of a questioned document for multiple adapted authorial language models. We score higher than other methods on several datasets, and our text ablation testing suggests that ALMs needs just 40 and 70 tokens to reach 70% and 60% macro-average accuracy, respectively.","For more than a century, researchers have used stylometry, or the statistical analysis of style, to determine the author of a questioned document by comparing its style to writing samples from a set of candidate authors. Despite the usefulness of stylometry in resolving certain types of authorship attribution tasks, there are limitations. For example, performance declines as the number of candidate authors and the amount of training data from the candidate authors decreases. Recently, researchers have begun to experiment with modern Large Language Models (LLMs) to address these issues, using LLM predictability metrics for authorship attribution.","Barlas and Stamatatos (2020) extend this approach by training a multi-head classifier using the cross entropy of a single pretrained LLM. Tyo et al. (2022) include Barlas and Stamatatos (2020)’s BERT-based approach, which they referred to as pALM (per Author Language Model), in their state-of-the-art authorship attribution benchmarking study, but find that pALM has the worst performance of all methods considered. Meanwhile, in recent years there has been growing concern about the misuse of LLMs, spawning a new type of attribution task called LLM detection, which involves identifying whether a questioned text was written by a human or an LLM. In LLM detection, casual language model perplexity has been found to be an effective indicator of authorship, where LLM-authored texts tend to be associated with relatively low perplexity scores in comparison to human-authored texts.nan","Our approach for authorship attribution involves fine-tuning a set of causal language models based on the known writing of a set of candidate authors, creating one model for each author. We then measure the perplexity of a questioned text over each of the fine- tuned authorial language models to predict authorship. We compare ALMs to state-of-the-art approaches in NLP for authorship attribution on the standard Blogs50 and CCAT50 datasets. To evaluate the robustness of our method across individual authors, we calculated single author accuracy scores for both the Blogs50 and CCAT50 datasets.","Our method outperforms all other methods on Blogs50, achieving a macro-average accuracy of 83.6%, and outperforms all but one method on the CCAT 50 dataset, achieving a macro-average accuracy of 74.9%, nearly matching the Ngram method, which achieves a macro-average accuracy of 76.7%. Furthermore, our method's robustness across individual authors is demonstrated by the fact that for 38 out of the 50 authors in Blogs50 and for 31 out of the 50 authors in CCAT50 we obtained an accuracy of over 80%. However, there are a few authors whose texts prove especially difficult to attribute. An analysis of text ablation on the test texts from both datasets shows that to reach a macro-average accuracy of 70%, ALMs requires test texts of at least 40 tokens on Blogs50 and of at least 400 tokens on CCAT50, while ALMs only needs a test text length of at least 20 tokens on Blogs50 and at least 70 tokens on CCAT50 to reach a macro-average accuracy of 60%.","Our paper proposes a new method for authorship attribution based on the perplexity of a set fine- tuned authorial causal language models that we refer to ALMs (Authorial Language Models). On the Blogs50 dataset our method achieves a macro-accuracy of 83.6% which outperformed SOTA, while on the CCAT50 dataset our method achieves a macro-accuracy of 74.9% nearly matching SOTA. The performance of ALMs demonstrates that model predictability metrics from multiple authorial LLMs can be highly informative indicators in authorship attribution. Compared to standard type-based methods in stylometry, which are based on the relative frequencies of common words, n-grams, and other types, perplexity-based methods are capable of capturing authorial information for each word token rather, thereby offering greater flexibility and finer granularity. However, we note that topical patterns in both datasets may positively influence the benchmarking scores for ALMs and other methods.",ALMs: Authorial Language Models for Authorship Attribution,"Weihang Huang, Akira Murakami, Jack Grieve","ALMs: Authorial Language Models for Authorship Attribution
Weihang Huang and Akira Murakami and Jack Grieve
Department of English Language and Linguistics, University of Birmingham
wxh207@student.bham.ac.uk a.murakami@bham.ac.uk j.grieve@bham.ac.uk
Abstract
In this paper, we introduce an authorship attri-
bution method called Authorial Language Mod-
els (ALMs) that involves identifying the most
likely author of a questioned document based
on the perplexity of the questioned document
calculated for a set of causal language models
fine-tuned on the writings of a set of candi-
date author. We benchmarked ALMs against
state-of-art-systems using the CCAT50 dataset
and the Blogs50 datasets. We find that ALMs
achieves a macro-average accuracy score of
83.6% on Blogs50, outperforming all other
methods, and 74.9% on CCAT50, matching
the performance of the best method. To assess
the performance of ALMs on shorter texts, we
also conducted text ablation testing. We found
that to reach a macro-average accuracy of 70%,
ALMs needs 40 tokens on Blogs50 and 400
tokens on CCAT50, while to reach 60% ALMs
requires 20 tokens on Blogs50 and 70 tokens
on CCAT50.
1
Introduction
For over a century, researchers have developed
methods for authorship attribution to resolve cases
of disputed authorship by comparing the style of
a questioned document to writing samples from a
set of candidate authors (Juola, 2006; Stamatatos,
2009). The goal of authorship attribution is to
identify the candidate whose style of writing is
most similar to a questioned document. Stylom-
etry is the quantitative analysis of style and is a
common approach to authorship attribution (Juola,
2006; Stamatatos, 2009). A wide range of different
measurements and methods for authorship attribu-
tion have been developed in stylometry (Grieve,
2007; Stamatatos, 2009). Popular techniques in-
clude Principal Component Analysis of function
word frequencies (Binongo, 2003; Grieve, 2023)
and distance-based comparisons of the frequen-
cies of common words (Argamon, 2007; Burrows,
2002).
Although stylometric approaches are useful for
resolving certain types of authorship attribution
tasks, there are clear limitations with these tech-
niques.
Overall performance declines dramati-
cally when the number of candidate authors in-
creases (Grieve, 2007; Luyckx and Daelemans,
2011), when the length of the question document
decreases (Eder, 2015; Grieve et al., 2018), and
when the amount of training data from the candi-
date authors decreases (Luyckx and Daelemans,
2011; Grieve et al., 2018).
Recent research in authorship analysis has be-
gun to explore the use of modern Large Language
Models (LLMs) to address these issues. Exam-
ples include universal authorial embeddings using
Siamese BERT (Rivera-Soto et al., 2021) and Char-
acter BERT (El Boukkouri et al., 2020), and using
BERT for classification (Fabien et al., 2020; Tyo
et al., 2022). LLM predictability metrics, such
as perplexity and cross-entropy, have also been
tested in a small number of studies.
Fourkioti
et al. (2019) found that the perplexity of a sin-
gle LLM pretrained on PoS-tags can be effective
for authorship attribution. Barlas and Stamatatos
(2020) extended this approach by training a multi-
head classifier using the cross entropy of a single
pretrained LLM, achieving their best performance
using BERT, although they also considered other
LLMs, including causal language models. Sub-
sequently, Tyo et al. (2022) included Barlas and
Stamatatos (2020)’s BERT-based approach, which
they referred to as pALM (per Author Language
Model), in their state-of-the-art authorship attribu-
tion benchmarking study, but found that pALM
has the worst performance of all methods consid-
ered, which included an n-gram based classifier
(Ngram) (Tyo et al., 2022), a prediction by partial
matching compression model (PPM)(Teahan and
Harper, 2003; Neal et al., 2018), and a pre-trained
BERT model with a dense layer for classification
(BERT)(Fabien et al., 2020).
arXiv:2401.12005v1  [cs.CL]  22 Jan 2024
Although previous research has had relatively lit-
tle success using LLM predictability metrics for hu-
man authorship attribution, this approach currently
underlies state-of-the-art methods for LLM detec-
tion. LLM detection is a new type of attribution
task that involves identifying whether a questioned
text was written by a human or an LLM. The task
has gained prominence in recent years due to in-
creasing concerns about the misuse of LLMs (Bom-
masani et al., 2022; Gehrmann et al., 2019; Tian
et al., 2023; Wu et al., 2023; Gehrmann et al., 2019;
Wu et al., 2023). In these studies, casual language
model perplexity has been found to be an effective
indicator of authorship, where LLM-authored texts
tend to be associated with relatively low perplex-
ity scores in comparison to human-authored texts:
LLM-authored texts are generally more predictable
to a LLM. Approaches include both fully auto-
mated detection and computer-assisted detection,
e.g., GLTR (Gehrmann et al., 2019) and GPTZero
(Chakraborty et al., 2023).
Building on this research, in this paper, we re-
visit the idea of using LLM perplexity for human
authorship attribution. However, rather than work
with a single LLM, as has been the case in previous
research on both human and machine attribution,
we build a set of adapted Authorial Language Mod-
els (ALMs), each of which is fine-tuned on the
writings of a single candidate author. Instead of
computing the perplexity or cross entropy based on
a single LLM, our approach involves predicting the
authorship of a questioned document by compar-
ing the perplexities of this text for multiple ALMs,
selecting the author whose associated LLM yields
the lowest perplexity. We benchmark ALMs on
the standard Blogs50 and CCAT50 datasets, fol-
lowing Tyo et al. (2022), finding that our approach
achieves state-of-the-art overall performance.
2
Methodology
2.1
Authorship Attribution
Authorial LLM Fine-tuning The first step of our
approach for authorship attribution involves fine-
tuning a set of causal language models based on
the known writing of a set of candidate authors,
creating one model for each author.
For this study, we fine-tuned all authorial GPT-2
models with 100 epochs on a single Graphcore IPU
Pod 4 Machine at PaperSpace. We make all scripts
accessible online1.
1https://github.com/Weihang-Huang/ALMs
Perplexity Calculation The second step of our
approach involves measuring the perplexity of a
questioned text over each of the fine-tuned autho-
rial language models to predict authorship.
The perplexity of a fixed-length causal lan-
guage model M over a token sequence T
=
{x1, x2, ..., xt} is defined as
ppl (M, T) = exp
(
−1
t
t
X
i
log (pM (xi|x<i))
)
In other words, perplexity is the exponentiated
mean negative log likelihood of tokens in the se-
quence, which represents the average predictability
of tokens in the sequence.
In practice, we calculate perplexity as the cross
entropy between the true token and the predicted
logits, namely exp {CrossEntropy (Logits, T)}.
Given a questioned text Q, and a fine-tuned autho-
rial GPT-2 model M, we first pass Q to the GPT-2
BPE Tokenizer to extract a token sequence T. T
is then passed to M for language modeling, whose
output is Logits. Here Logits reflects the pre-
dicted probabilities of all tokens in T, where T rep-
resents the ground truth. Therefore, in the next step,
we measure the predictability of all tokens in T by
comparing the predicted Logits and the ground
truth T via cross entropy CE, which we calcu-
lated using torch.nn.CrossEntropyLoss from
PyTorch. Finally, we obtain the perplexity of Q
under M as e raised to the power of CE.
Authorship Prediction The third and final step
of our approach involves attributing the questioned
document to the author whose authorial LLM
was associated with the lowest perplexity. Given
a text Q from author i, and a set of authorial
language models {M1, M2, ...Mn} fine-tuned on
texts from a set of candidate authors {1, 2, ..., n},
we expect ppl (Mi, Q) to be lowest among
{ppl (M1, Q) , ppl (M2, Q) , ..., ppl (Mn, Q)}, be-
cause we expect Q to be most predictable for the
model that was fine-tuned on the training corpus
for author i.
2.2
Data
To evaluate ALMs, we used the Blogs50 (Schler
et al., 2006) and the CCAT50 (Lewis et al., 2004)
datasets. We chose these two datasets because
they are accessible and allow us to benchmark our
methods against the methods evaluated in Tyo et al.
(2022). Both CCAT50 and Blogs50 also contain
sufficient numbers of authors, texts and tokens to
adequately test and train our method and are fo-
cused on a relatively consistent registers: CCAT50
contains news articles, while Blogs50 contains
texts from top bloggers. We retrieved both datasets
from the repository of Tyo et al. (2022)2. Table 1
lists basic information for both datasets.
Dataset
A
T
TK
T/A
TTL
CCAT50
50
5k
2.5M
100
506
Blogs50
50
66k
8.1M
1.3k
122
A: author count; T: text count; TK: token
count; TTL: test text length, in token count
Table 1: Information on Datasets
3
Results
To compare ALMs to state-of-the-art approaches
for authorship attribution in NLP, we evaluated the
performance of our method on Blogs50 dataset
and CCAT50 dataset, using macro-average accu-
racy following Tyo et al. (2022). Table 2 com-
pares the macro-average accuracy of our method
against other methods. We find that our method
outperforms all other methods on Blogs50, achiev-
ing a macro-average accuracy of 83.6%, and out-
performs all but one method on the CCAT 50
dataset, achieving a macro-average accuracy of
74.9%, nearly matching the Ngram method, which
achieves a macro-average accuracy of 76.7%.
Method
Blogs50
CCAT50
ALMs
83.6%
74.9%
Ngram
72.3%
76.7%
BERT
75.0%
65.7%
PPM
72.2%
69.4%
pALM
-
63.4%
Table 2: Comparison Between Our Method (ALMs)
And Recent SOTA Methods (Tyo et al., 2022)
Furthermore, to evaluate the robustness of our
method across individual authors, we calculated
single author accuracy scores for the both the
Blogs50 (see Table 3) and CCAT50 (see Table 4)
datasets. For 38 out of the 50 authors in Blogs50
and for 31 out of the 50 authors in CCAT50 we
obtained an accuracy of over 80%. However, we
note that there are a few authors whose texts prove
especially difficult to attribute, including Author
46 in Blogs50 and Author 30 in CCAT50.
2https://github.com/JacobTyo/Valla
In addition, we conducted an analysis of text ab-
lation on the test texts from both datasets to assess
the robustness of our method when working with
questioned documents of a limited length. We find
that to reach a macro-average accuracy of 70%,
ALMs requires test texts of at least 40 tokens on
Blogs50 and of at least 400 tokens on CCAT50,
while ALMs only needs a test text length of at
least 20 tokens on Blogs50 and at least 70 tokens
on CCAT50 to reach a macro-average accuracy of
60%. We also plot the macro-average accuracy of
ALMs under different test text lengths in Figure 1,
which shows not only that an increase in test text
length results in higher macro-average accuracy
scores, but that the performance of our approach
falls off quickly on texts that contain fewer than 50
tokens.
4
Discussion
In this paper we proposed a new method for au-
thorship attribution based on the perplexity of a
set fine-tuned authorial causal language models
that we refer to ALMs (Authorial Language Mod-
els). On the Blogs50 dataset our method achieves
a macro-accuracy of 83.6% which outperformed
SOTA, while on the CCAT50 dataset our method
achieves a macro-accuracy of 74.9% nearly match-
ing SOTA.
The performance of ALMs demonstrates that
model predictability metrics from multiple autho-
rial LLMs can be highly informative indicators in
authorship attribution. Beyond fine-tuning for each
author, using a large number of epochs and choos-
ing causal language model of GPT-2 where per-
plexity is well defined may contribute to the excel-
lent performance of ALMs compared to other tech-
niques, including previous attempts to use LLM
predictability metrics.
In addition, we attribute the excellent perfor-
mance of ALMs compared to traditional stylomet-
ric methods based on its ability access to token-
level authorial features. Compared to standard
type-based methods in stylometry, which are based
on the relative frequencies of common words, n-
grams, and other types, perplexity-based methods
are capable of capturing authorial information for
each word token rather, thereby offering greater
flexibility and finer granularity.
For instance, the use of the word baseball is not
generally a good feature for stylometric authorship
attribution for two reasons. First, it is relatively
Figure 1: Performance of ALMs Under Different Test Text Lengths
infrequent, making it difficult to obtain a mean-
ingful measurement of its relative frequency. This
is why stylometric methods tend to focus on high
frequency forms. Second, even if sufficient data
were available, the relative frequency of this word
type in any text would primarily reflect the topic of
that text, as opposed to the style of its author. For
example, a text with the frequent use of the word
baseball will tend to be about this sport. This is
problematic in the context of authorship attribution
because the goal is to attribute texts to the correct
authors regardless of the topic. This is why stylo-
metric methods tend to focus not only on common
features, but on grammatical features, like function
words.
A token-based approach like ALMs, however,
avoids these issues as it effectively assigns a proba-
bility to every token in a text. In general, we can
assume that if a questioned document is about base-
ball, occurrences of the word baseball will gener-
ally carry very little authorial information, and that
the probability of the tokens of the word baseball
in that text will consistently be low for all authors.
However, given a questioned document on some
other topic, a token of the term baseball (e.g., as
an example or as a metaphor) would potentially be
highly discriminatory – extremely unexpected for
most authors, unless, for example, an author often
uses baseball metaphors out of context.
In this sense, a token-based approach is similar
to the type of qualitative stylistic authorship analy-
sis often conducted manually in a forensic context,
where forensic linguists examine a questioned doc-
ument word by word (Coulthard et al., 2016; Grant,
2008). Like a forensic stylistic analysis, a great
advantage of our approach compared to a standard
stylometric analysis is that we can extract consider-
ably more information from each text: every token
is now a valid feature, whereas for traditional meth-
ods only frequent types can potentially be features.
5
Limitations
Currently, our evaluation of ALMs was made un-
der a fixed hyper-parameter settings where other
combination of hyper-parameters remain untested.
We also note that topical patterns in both datasets
may positively influence the benchmarking scores
for ALMs and other methods.
6
Ethics and Impact
Our research is based on publicly available base
model; we are not aware of specific risks except
biases inherited from data or base model, which
needs to be examined before any implementations
in a large scale. When put in practice, the pre-
dicted author from this method should be treated as
reference to form the final decision of authorship to-
gether with other clues and evidences. Finally, it is
important to stress that all methods for authorship
analysis are limited by the quality of the dataset,
and all methods for authorship attribution are lim-
ited by the completeness of the set of candidate
authors.
References
Shlomo Argamon. 2007. Interpreting burrows’s delta:
Geometric and probabilistic foundations. Literary
and Linguistic Computing, 23(2):131–147.
Georgios Barlas and Efstathios Stamatatos. 2020. Cross-
Domain Authorship Attribution Using Pre-trained
Language Models, volume 583 of IFIP Advances in
Information and Communication Technology, page
255–266. Springer International Publishing, Cham.
José Nilo G. Binongo. 2003. Who wrote the 15th book
of oz? an application of multivariate analysis to au-
thorship attribution. Chance, 16(2):9–17.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ
Altman, Simran Arora, Sydney von Arx, et al. 2022.
On the opportunities and risks of foundation models.
(arXiv:2108.07258). ArXiv:2108.07258 [cs].
John Burrows. 2002. “delta”: a measure of stylistic
difference and a guide to likely authorship. Literary
and Linguistic Computing, 17(3):267–287.
Souradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu,
Bang An, Dinesh Manocha, and Furong Huang. 2023.
On the Possibilities of AI-Generated Text Detection.
ArXiv:2304.04736 [cs].
Malcolm Coulthard, Alison Johnson, and David Wright.
2016. An introduction to forensic linguistics: Lan-
guage in evidence. Routledge.
Maciej Eder. 2015. Does size matter? authorship attri-
bution, small samples, big problem. Digital Scholar-
ship in the Humanities, 30(2):167–182.
Hicham El Boukkouri, Olivier Ferret, Thomas Lavergne,
Hiroshi Noji, Pierre Zweigenbaum, and Jun’ichi
Tsujii. 2020. Characterbert: Reconciling elmo and
bert for word-level open-vocabulary representations
from characters. In Proceedings of the 28th Inter-
national Conference on Computational Linguistics,
page 6903–6915, Barcelona, Spain (Online). Interna-
tional Committee on Computational Linguistics.
Maël Fabien, Esaú Villatoro-Tello, Petr Motlicek, and
Shantipriya Parida. 2020. Bertaa: Bert fine-tuning
for authorship attribution. In Proceedings of the 17th
International Conference on Natural Language Pro-
cessing (ICON), page 127–137.
Olga Fourkioti, Symeon Symeonidis, and Avi Aram-
patzis. 2019. Language models and fusion for author-
ship attribution. Information Processing & Manage-
ment, 56(6):102061.
Sebastian Gehrmann, Hendrik Strobelt, and Alexander
Rush. 2019. Gltr: Statistical detection and visual-
ization of generated text. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics: System Demonstrations, page 111–116,
Florence, Italy. Association for Computational Lin-
guistics.
Tim Grant. 2008. Approaching questions in forensic au-
thorship analysis. Dimensions of forensic linguistics,
5:215–229.
Jack Grieve. 2007. Quantitative authorship attribution:
An evaluation of techniques. Literary and Linguistic
Computing, 22(3):251–270.
Jack Grieve. 2023. Register variation explains stylo-
metric authorship analysis. Corpus Linguistics and
Linguistic Theory, 19(1):47–77.
Jack Grieve, Isobelle Clarke, Emily Chiang, Hannah
Gideon, Annina Heini, Andrea Nini, and Emily
Waibel. 2018. Attributing the Bixby Letter using
n-gram tracing. Digital Scholarship in the Humani-
ties, 34(3):493–512.
Patrick Juola. 2006. Authorship attribution. Founda-
tions and trends in information retrieval. Now Publ,
Boston, Mass.
David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for text
categorization research. Journal of machine learning
research, 5:361–397.
Kim Luyckx and Walter Daelemans. 2011. The effect of
author set size and data size in authorship attribution.
Literary and Linguistic Computing, 26(1):35–55.
Tempestt Neal, Kalaivani Sundararajan, Aneez Fatima,
Yiming Yan, Yingfei Xiang, and Damon Woodard.
2018. Surveying Stylometry Techniques and Appli-
cations. ACM Computing Surveys, 50(6):1–36.
Rafael A. Rivera-Soto, Olivia Elizabeth Miano, Juanita
Ordonez, Barry Y. Chen, Aleem Khan, Marcus
Bishop, and Nicholas Andrews. 2021. Learning uni-
versal authorship representations. In Proceedings of
the 2021 Conference on Empirical Methods in Natu-
ral Language Processing, page 913–919, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James W Pennebaker. 2006. Effects of age and gen-
der on blogging. In AAAI spring symposium: Compu-
tational approaches to analyzing weblogs, volume 6,
page 199–205.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538–556.
William J. Teahan and David J. Harper. 2003. Using
Compression-Based Language Models for Text Cate-
gorization. In W. Bruce Croft and John Lafferty, edi-
tors, Language Modeling for Information Retrieval,
pages 141–165. Springer Netherlands, Dordrecht.
Yuchuan Tian, Hanting Chen, Xutao Wang, Zheyuan
Bai, Qinghua Zhang, Ruifeng Li, Chao Xu, and
Yunhe Wang. 2023. Multiscale positive-unlabeled
detection of ai-generated texts. (arXiv:2305.18149).
ArXiv:2305.18149 [cs].
Jacob Tyo, Bhuwan Dhingra, and Zachary C. Lipton.
2022. On the state of the art in authorship attribu-
tion and authorship verification. (arXiv:2209.06869).
ArXiv:2209.06869 [cs].
Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng,
and Tat-Seng Chua. 2023.
Llmdet: A large lan-
guage models detection tool. (arXiv:2305.15004).
ArXiv:2305.15004 [cs].
Author Name
Text #
Token #
Mean Token # per Text
Accuracy(%)
0
2623
306575
116.9
90.85
1
1405
341521
243.1
94.89
2
1311
298754
227.9
86.89
3
1301
207581
159.6
83.69
4
1215
254051
209.1
96.05
5
1207
194987
161.5
92.38
6
1125
144410
128.4
86.48
7
1100
119820
108.9
95.64
8
1083
257212
237.5
83.39
9
1078
182824
169.6
89.63
10
1046
456278
436.2
88.17
11
1019
166745
163.6
43.53
12
1009
164719
163.2
73.41
13
947
132354
139.8
88.19
14
910
191351
210.3
91.67
15
894
211436
236.5
95.09
16
835
237192
284.1
80.86
17
830
162396
195.7
84.13
18
811
288834
356.1
95.07
19
808
63683
78.8
99.5
20
807
160624
199
91.58
21
795
132298
166.4
84.92
22
782
290040
370.9
90.77
23
755
331710
439.4
83.07
24
753
91592
121.6
83.51
25
743
213729
287.7
76.34
26
740
102774
138.9
67.57
27
740
148279
200.4
98.92
28
726
111685
153.8
83.43
29
726
96883
133.4
75.14
30
718
87430
121.8
88.27
31
716
499373
697.4
97.77
32
707
128940
182.4
76.27
33
706
181419
257
88.64
34
705
118039
167.4
80.68
35
689
79639
115.6
83.72
36
661
87402
132.2
64.24
37
636
203631
320.2
98.74
38
631
359944
570.4
93.04
39
621
248852
400.7
67.1
40
609
50015
82.1
82.24
41
609
128077
210.3
88.16
42
605
81499
134.7
75.5
43
605
90913
150.3
84.77
44
605
456833
755.1
96.03
45
600
78737
131.2
50
46
593
86792
146.4
39.19
47
592
132766
224.3
60.81
48
576
202813
352.1
94.44
49
565
118027
208.9
95.04
Table 3: Performance of Our Method for Each of the 50 Authors in Blogs50
Author Name
Text #
Token #
Mean Token # per Text
Accuracy(%)
0
90
55807
620.1
50.00
1
90
63795
708.8
68.00
2
90
65540
728.2
66.00
3
90
62633
695.9
88.00
4
90
63459
705.1
96.00
5
90
62785
697.6
88.00
6
90
58634
651.5
90.00
7
90
51316
570.2
82.00
8
90
59730
663.7
30.00
9
90
57452
638.4
88.00
10
90
57416
638
92.00
11
90
54029
600.3
40.00
12
90
61247
680.5
60.00
13
90
58148
646.1
34.00
14
90
53918
599.1
80.00
15
90
53871
598.6
82.00
16
90
65597
728.9
60.00
17
90
65092
723.2
30.00
18
90
53021
589.1
84.00
19
90
70496
783.3
80.00
20
90
57850
642.8
78.00
21
90
64731
719.2
94.00
22
90
68547
761.6
80.00
23
90
61034
678.2
88.00
24
90
69984
777.6
100.00
25
90
56861
631.8
92.00
26
90
55312
614.6
90.00
27
90
66168
735.2
68.00
28
90
52956
588.4
86.00
29
90
57617
640.2
86.00
30
90
65783
730.9
14.00
31
90
43818
486.9
82.00
32
90
53929
599.2
90.00
33
90
59234
658.2
60.00
34
90
66891
743.2
28.00
35
90
63705
707.8
100.00
36
90
59393
659.9
80.00
37
90
64213
713.5
78.00
38
90
53000
588.9
62.00
39
90
58239
647.1
48.00
40
90
54635
607.1
86.00
41
90
68160
757.3
82.00
42
90
56512
627.9
98.00
43
90
63766
708.5
92.00
44
90
62480
694.2
42.00
45
90
56889
632.1
92.00
46
90
57731
641.5
72.00
47
90
55562
617.4
100.00
48
90
67006
744.5
88.00
49
90
41518
461.3
100.00
Table 4: Performance of Our Method for Each of the 50 Authors in CCAT50
"
"We introduce a novel stereo-confidence that can be measured externally to various stereo-matching algorithms, enabling an alternative input modality choice of the cost volume for learning-based approaches. We ground our stereo-confidence method in the foundational concepts of disparity definition and the disparity plane sweep. Based on these concepts, the proposed stereo-confidence method can be condensed into three integral components: 1) Using the disparity plane sweep, multiple disparity maps can be obtained and treated as a 3-D volume (predicted disparity volume). 2) One of these disparity maps serves as an anchor, allowing us to define a desirable (or ideal) disparity profile. 3) By comparing the desirable and predicted disparity profiles, the level of matching ambiguity between left and right images can be quantified. This comprehensive experimental study evaluates the proposed stereo-confidence method under various metrics, datasets, and stereo-matching algorithms, demonstrating its effectiveness both as confidence and as an input modality for learning-based stereo-confidence methods.","Stereo-matching is a fundamental task in computer vision that aims to estimate the depth information of a scene from a pair of stereo images. Accurate depth estimation is essential for a wide range of applications, such as autonomous driving, robotics, and augmented reality.","Over decades, various stereo-matching methods have been proposed, ranging from traditional methods based on hand-crafted features to more recent learning-based methods.nan","In this paper, we propose a novel stereo-confidence method that can be measured externally to various stereo-matching algorithms. Grounded in the foundational concepts of disparity definition and the disparity plane sweep, the proposed stereo-confidence method can be condensed into three integral components: 1) Using the disparity plane sweep, multiple disparity maps can be obtained and treated as a 3-D volume (predicted disparity volume). 2) One of these disparity maps serves as an anchor, allowing us to define a desirable (or ideal) disparity profile. 3) By comparing the desirable and predicted disparity profiles, the level of matching ambiguity between left and right images can be quantified.","To demonstrate the effectiveness of the proposed method, we compare the performance of the proposed confidence to the existing learning-based stereo-confidence methods. Furthermore, we present the experimental results when the proposed method is leveraged as an additional input modality for the existing learning-based stereo-confidence methods.","We present a stereo-confidence measurement that operates outside end-to-end stereo-matching networks, which is a recent paradigm of the learning-based stereo-matching methods. The key idea of the proposed method is to reinterpret the conventional stereo-confidence method analyzing the cost profile to be suitable for end-to-end stereo-matching networks by analyzing disparity profiles.",Modeling Stereo-Confidence Out of the End-to-End Stereo-Matching Network via Disparity Plane Sweep,"Jae Young Lee, Woonghyun Ka, Jaehyun Choi, Junmo Kim","Modeling Stereo-Confidence Out of the End-to-End Stereo-Matching Network
via Disparity Plane Sweep
Jae Young Lee1*, Woonghyun Ka2*, Jaehyun Choi1, Junmo Kim1
1School of Electrical Engineering, KAIST, Daejeon, South Korea
2Hyundai Motor Company, Seoul, South Korea
{mcneato, chlwogus, junmo.kim}@kaist.ac.kr, kwh950724@hyundai.com
Abstract
We propose a novel stereo-confidence that can be measured
externally to various stereo-matching networks, offering an
alternative input modality choice of the cost volume for
learning-based approaches, especially in safety-critical sys-
tems. Grounded in the foundational concepts of disparity def-
inition and the disparity plane sweep, the proposed stereo-
confidence method is built upon the idea that any shift in
a stereo-image pair should be updated in a corresponding
amount shift in the disparity map. Based on this idea, the pro-
posed stereo-confidence method can be summarized in three
folds. 1) Using the disparity plane sweep, multiple disparity
maps can be obtained and treated as a 3-D volume (predicted
disparity volume), like the cost volume is constructed. 2) One
of these disparity maps serves as an anchor, allowing us to
define a desirable (or ideal) disparity profile at every spatial
point. 3) By comparing the desirable and predicted dispar-
ity profiles, we can quantify the level of matching ambiguity
between left and right images for confidence measurement.
Extensive experimental results using various stereo-matching
networks and datasets demonstrate that the proposed stereo-
confidence method not only shows competitive performance
on its own but also consistent performance improvements
when it is used as an input modality for learning-based stereo-
confidence methods.
Introduction
Stereo-matching is a fundamental task in computer vision
that aims to estimate the depth information of a scene from
a pair of stereo images. Accurate depth estimation is essen-
tial for a wide range of applications, such as autonomous
driving, robotics, and augmented reality. Over decades, var-
ious stereo-matching methods have been proposed, ranging
from traditional methods based on hand-crafted features to
more recent learning-based methods. These methods typi-
cally generate the cost volume or feature maps to estimate
the disparity value for each pixel. Despite impressive results
achieved by recent learning-based stereo-matching methods,
they occasionally fail in ill-posed regions such as occlusion
boundaries, repeated patterns, textureless regions, and non-
Lambertian surfaces due to inherent ambiguities tied to the
correspondence problem (Zhou et al. 2020).
*Equal contribution. Alphabetical order.
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
To deal with such ambiguous correspondences, stereo-
confidence measurement has been utilized to identify
whether the results of stereo-matching methods are reliable
at each pixel. In the past, conventional approaches com-
monly measure confidence by analyzing the cost volume
obtained from stereo-matching methods. With the rise of
the recent deep learning paradigm, learning-based stereo-
confidence approaches have also been widely studied lever-
aging various combinations of input modalities (Poggi et al.
2021a). Especially, in the case of LAF-Net (Kim et al. 2019),
it was the first one among learning-based approaches to
leverage the cost volume as an input modality, achieving
state-of-the-art performance through tri-modal inputs. Both
conventional and learning-based approaches have demon-
strated that the cost volume is an effective input cue among
input modalities. The cost profile in the cost volume pro-
vides information about how confident the stereo-matching
method is in its predictions by matching cost (or probability)
for each disparity plane shift. The cost volume can be further
utilized for tasks such as uncertainty estimations (Wang et al.
2022) and refinement of stereo-matching networks (Zeng
et al. 2023).
However, the cost volumes (or internal features) are be-
ing encapsulated within the stereo-matching network itself
due to the recent trend of stereo-matching networks oper-
ating in an end-to-end manner. Furthermore, especially in
safety-critical systems closely tied to the stereo-matching
networks, the architecture is kept confidential, and access to
its internal information is restricted to protect the network
from potential external threats (Wang et al. 2021; Wong,
Mundhra, and Soatto 2021; Cheng, Healey, and Wu 2021).
As a result, this prompts the necessity of the exploration of
estimating confidence without accessing the cost volumes
(or internal features) and from outside the stereo-matching
network (Poggi et al. 2020).
Unfortunately, the cost volume is an effective cue in
stereo-confidence estimation. Thus, in this paper, we aim
to quantify the level of matching ambiguity from the ex-
ternal stereo-matching method and achieve effects compa-
rable to using the cost volume as an input without perfor-
mance degradation. Furthermore, we intend to introduce the
proposed confidence as an alternative input of the cost vol-
ume to learning-based stereo-confidence methods, making
it suitable for use even in safety-critical systems. It can
arXiv:2401.12001v1  [cs.CV]  22 Jan 2024
be achieved by reinterpreting confidence based on the re-
lationship between the cost volume and the multiple dis-
parity maps. The proposed method is built upon two main
concepts: the definition of disparity and the disparity plane
sweep. In stereo-matching, the disparity is defined as a dis-
placement between a point in the left image and its corre-
sponding point in the right image of a stereo-image pair. The
disparity plane sweep consecutively shifts an image with re-
spect to the reference image in stereo images, which is gen-
erally used to construct the cost volume to find the corre-
spondence between a stereo-image pair in stereo-matching.
By the definition of disparity itself, any shift by the disparity
plane sweep in a stereo-image pair indicates that the dispar-
ity map should be updated in line with the corresponding
shift amount. Based on the definition of the disparity and
the disparity plane sweep, the proposed stereo-confidence
method can be condensed into three main aspects: 1) just as
the cost volume is constructed, multiple disparity maps can
be obtained from any stereo-matching network and treated
as a 3-D volume (disparity volume) using the disparity plane
sweep. 2) Using an obtained disparity map without shift as
an anchor, the desirable disparity profile can be defined and
treated as an ideal one, as the ideal cost profile is defined. 3)
By comparing the desirable and predicted disparity profiles
at every spatial point in the disparity volume, we can quan-
tify the level of matching ambiguity between left and right
images for confidence measurement.
To demonstrate the effectiveness of the proposed method
not only as confidence but also as an input modality, we
compare the performance of the proposed confidence to the
existing learning-based stereo-confidence methods. Further-
more, we present the experimental results when the pro-
posed method is leveraged as an additional input modality
for the existing learning-based stereo-confidence methods.
While the reinterpreted confidence might seem simple, and
its measurement is a conventional method, the experimental
results consistently demonstrate the effectiveness of the pro-
posed method across diverse datasets, regardless of the type
of the stereo-matching networks. Furthermore, the proposed
method shows potential in safety-critical systems as it suc-
cessfully works as an alternative choice of the cost volume.
Related Works
In this section, the conventional and learning-based stereo-
confidence methods are briefly introduced.
Conventional Methods
The conventional methods have been extensively studied
over the past few decades and mainly have relied on ana-
lyzing the matching cost volume. Hu and Mordohai (2012)
thoroughly investigated 17 confidence measures, focusing
on conventional methods. These methods developed algo-
rithms based on the minimum cost and local properties of
the cost profile (Egnal, Mintz, and Wildes 2004; Haeusler,
Nair, and Kondermann 2013; Haeusler and Klette 2012;
Wedel et al. 2009; Kim, Yoo, and Kim 2014; Kim, Jang,
and Kim 2016). Some conventional methods examine the
entire curve of the cost profile to extract useful informa-
tion for measuring confidence (Haeusler, Nair, and Konder-
mann 2013; Matthies 1992; Scharstein and Szeliski 1996;
Het Veld et al. 2018). Unlike these methods that concentrate
on the cost profile inside the stereo-matching model, the pro-
posed approach examines the disparity profile outside the
stereo-matching network, making it suitable for learning-
based end-to-end stereo-matching methods.
Learning-Based Methods
Recently, learning-based methods have used various deep-
learning techniques to measure confidence. These meth-
ods extract features directly from input modalities, such
as the disparity maps, reference image, and cost volume,
and then estimate the confidence from features. Various
learning-based methods have been proposed using differ-
ent combinations of input modalities. Single-modality (dis-
parity), CCNN (Poggi and Mattoccia 2016) measured con-
fidence using a convolutional neural network for the first
time. Kim et al. (2022) proposed meta-confidence to im-
prove stereo-confidence quality by encoding the reliabil-
ity of confidence once more. Bi-modality (disparity, im-
age), LFN (Fu, Ardabilian, and Stern 2019) pioneered the
stereo-confidence with the bi-modal inputs and fusion strate-
gies. Tosi et al. (2018) proposed ConfNet to obtain global
confidence with large receptive fields. They also proposed
LGC-Net, a local-global confidence framework, combining
ConfNet with local confidence network, such as CCNN and
LFN. Tri-modality (disparity, image, cost volume), LAF-
Net (Kim et al. 2019) achieved remarkable performance
with tri-modal inputs fused by an attention mechanism.
Nevertheless, a notable limitation of these methods is
their reliance on ground truth disparity maps during the
training phase. As a result, their performance may be sub-
optimal when faced with out-of-distribution data domains.
In contrast, our proposed approach itself, like a conventional
method, is independent of training processes.
Method
In this section, first, we revisit the definition of disparity and
briefly explain the concept of the proposed method. Second,
to reinterpret confidence, a property of the disparity profile
derived from the relationship between the cost volume and
the disparity volume is presented. Third, the process to ob-
tain the disparity volume is explained. Then, the proposed
stereo-confidence method is described.
Preliminaries and Concept
In stereo imaging, points from 3D real-world coordinates are
projected onto different pixel coordinates within the left and
right images of a stereo-image pair. In terms of pixel units,
the disparity refers to the horizontal displacement between
a pixel in the left image and a corresponding pixel in the
right image when overlaying the stereo images. Using the
left image as the reference, the stereo-matching aims to esti-
mate the disparity values within a given stereo-image pair.
Both conventional and learning-based approaches rely on
visual similarity (or difference) to find image regions iden-
tical to the reference image regions, which are determined
Non-ideal case
Ideal case
Step
function
Ramp
function
Integration
Differentiation
Dirac delta
function
Integration
Differentiation
Cost or
Similarity
Disparity shift
Cost or
Similarity
Disparity shift
Foreground
Probability
Disparity shift
Foreground
Probability
Disparity shift
Disparity
Disparity shift
Disparity
Disparity shift
Cost
Similarity
Cost
Similarity
Figure 1: Conceptual description of ideal and non-ideal pro-
files according to the disparity plane sweep in signal pro-
cessing perspective.
by the cost obtained by comparing the pixel intensities or
features of image patches. Through disparity plane sweep,
i.e., shifting the right image concerning the reference image
within a predefined maximum disparity range, the costs be-
tween the reference image and the shifted right image are
calculated. The accumulated costs result in the form of a 3D
cost volume. Then, the disparity plane shift with the low-
est cost is determined as the disparity value. However, as
the cost is computed within an image patch (or limited re-
ceptive field), stereo-matching frequently encounters chal-
lenges in regions where the correspondence is ambiguous,
such as occlusion boundaries, repeated patterns, textureless
regions, and non-Lambertian surfaces (Zhou et al. 2020). In
essence, ambiguous correspondences in the stereo-matching
result in the misidentification of image patches, causing un-
reliable disparity estimations. Conversely, clear correspon-
dences yield trustworthy results.
To quantify matching ambiguity, the proposed method uti-
lizes two fundamental components: a definition of the dis-
parity and the disparity plane sweep. The outline of the pro-
posed method can be summarized in three folds. 1) Like
the cost volume is constructed, multiple disparity maps in
the form of 3-D volume can be obtained using the disparity
plane sweep. 2) Using a disparity map obtained by zero shift
as an anchor, the desirable disparity profile in 3-D disparity
volume can be defined and treated as an ideal one, as an
ideal cost profile is defined. 3) Matching ambiguity can be
quantified by comparing the desirable and obtained disparity
profiles at every spatial location in the disparity volume.
Reinterpretation of Confidence
The relationship between the cost volume and dispar-
ity maps is introduced in Lee and Park (2021). In the
light field (LF), they (Lee and Park 2018; Lee and Park
2021) showed the relationship among the cost volume-based
(Zhang et al. 2016; Sheng et al. 2018; Williem, Park, and Lee
2018; Wang, Efros, and Ramamoorthi 2016), foreground-
background separation (FBS)-based (Lee and Park 2017b,a;
Lee, Park, and Kim 2021), and depth model-based methods
(Zhou et al. 2019b,a; Shin et al. 2018) from a signal pro-
cessing perspective (See Fig. 1) according to the LF param-
eterization, which corresponds to the disparity plane sweep
in the stereo-matching. Similar to the LF, stereo can have
such a relationship among the cost volume-based (Zbontar
and LeCun 2015), FBS-based (Badki et al. 2020), and depth
model-based methods (Chang and Chen 2018; Zhang et al.
2019; Xu and Zhang 2020) according to the disparity plane
sweep. Notably, the majority of recent stereo-matching net-
works operate in an end-to-end manner, with such end-to-
end networks predominantly falling into the group of depth
model-based methods.
In the stereo-matching methods, the disparity plane sweep
is generally utilized to obtain the cost volume, which is an
internal feature in their networks. Differently, in this pa-
per, using the disparity plane sweep, multiple disparity maps
from the end-to-end stereo-matching networks are obtained
to measure the confidence of disparity maps. Recall that the
disparity is defined as the pixel-unit horizontal displacement
between corresponding pixels when overlaying the stereo-
image pair. According to the definition of disparity, when the
right image is horizontally shifted by k pixels with respect
to the left (reference) image, the disparity map obtained by
an end-to-end stereo-matching model Φs should change by
k pixels under the identical correspondence as follows:
Φs(IL, IR) + k = Φs(IL, Ik
R),
(1)
where IL, IR, and Ik
R denote the left image, the right image,
and the right image horizontally shifted by k, respectively.
Ideally, according to the disparity plane sweep, the dis-
parity profile should be shaped into a linear line, resembling
a ramp function as shown in Fig. 1. However, in real-world
scenarios, if the right image is shifted using the disparity
plane sweep, stereo-matching methods often fail to main-
tain identical correspondence due to ambiguous correspon-
dence. The ambiguous correspondence results in the distor-
tion of the disparity profile. At the bottom of Fig. 2, exam-
ples of disparity profiles are presented (See supplementary
material for more examples). While the disparity profile of
the ideal case is shaped into a linear line, that of the oc-
clusion boundary, repeated pattern, textureless region, and
non-Lambertian surface is distorted. Based on the observa-
tions of the disparity profile, we reinterpret the conventional
stereo-confidence methods. While the conventional stereo-
confidence methods analyze the cost profile with respect to
the ideal cost profile (Dirac-delta), the proposed method an-
alyzes the disparity profile with respect to the ideal disparity
profile, i.e., linear line (ramp). Using the disparity map ob-
tained without shift as an anchor, we measure whether the
disparity profile is shaped into a linear line or not. With re-
spect to the linear line anchored by the zero-shifted disparity
map, a low distortion indicates reduced ambiguity and high
confidence. Conversely, a high distortion signifies increased
: Repeated pattern
Disparity
Disparity plane shift
: Occlusion boundary
Disparity
Disparity plane shift
: Disparity profile w/ sample points
: Ideal disparity profile
Left image 𝐼𝐿
Right image 𝐼𝑅
× 𝑁
𝐈𝐿
DPS
𝐈𝑅
DPS
Stereo-Matching 
Network Φs
𝐃𝑝𝑟𝑒𝑑
DPS
𝐷𝑝𝑟𝑒𝑑
0
𝑘𝑖 ∈ [−𝐾, 𝐾]
× 𝑁
𝐃𝑡𝑔𝑡
DPS
Unreliability 𝑈
Confidence 𝐶
𝑦
𝑁
𝑥 (𝑑𝑖𝑠𝑝𝑎𝑟𝑖𝑡𝑦 𝑝𝑙𝑎𝑛𝑒)
𝐻
𝑁
𝑊
Disparity Profiles Observations
Disparity
Disparity plane shift
: Ideal case
: Textureless region
Disparity
Disparity plane shift
: Non-Lambertian surface
Disparity
Disparity plane shift
Disparity Plane Sweep (DPS)
Figure 2: The entire process of the proposed method for quantifying unreliability (i.e., matching ambiguity) and measuring
confidence out of the stereo-matching network using disparity plane sweep and the observations of disparity profiles sampled
from corresponding pixels in the left image IL.
Method
Input Modality Type
Disparity
Image
Cost Volume
Ours
Muliple
CCNN
Single
CCNN†
Muliple
LFN
Single
✓
ConfNet
Single
✓
LGC-Net
Single
✓
LAF-Net∗
Single
✓
LAF-Net∗†
Multiple
✓
LAF-Net
Single
✓
✓
LAF-Net†
Multiple
✓
✓
Table 1: Various combinations of input modalities according
to the stereo-confidence methods in our experiments. The
LAF-Net∗ denotes LAF-Net that uses bi-modal inputs (dis-
parity, image) without cost volume. The superscript † de-
notes the methods using the proposed method as an addi-
tional input modality.
ambiguity and low confidence.
Obtaining Disparity Profiles
Fig. 2 describes an entire pipeline of the proposed method to
obtain the stereo-confidence. First, with input stereo-image
pair IL, IR ∈ R3×H×W and the number of disparity plane
shifts N, we generate a set of disparity plane swept right im-
ages IDPS
R
= {Iki
R ∈ R3×H×W | i = 1, 2, ..., N} by concate-
nating N right images shifted by ki pixels obtained from the
disparity plane sweep in range of ki ∈ [−K, K]. A set of ref-
erence images, IDPS
L
is simply generated by repeating the left
image IL by N times. Then, IDPS
L
and IDPS
R
∈ RN×3×H×W
have the same dimensions, and they go through a pre-trained
end-to-end stereo-matching network Φs. By doing so, a set
of predicted disparity maps DDPS
pred can be obtained in the
form of the disparity volume as follows:
DDPS
pred = {Dki
pred ∈ RH×W | i = 1, ..., N} = Φs(IDPS
L , IDPS
R ).
(2)
Based on a zero-shifted disparity map D0
pred, a set of target
disparity maps DDPS
tgt is generated by adding ki in the form
of the disparity volume as follows:
DDPS
tgt = {D0
pred + ki ∈ RH×W | i = 1, 2, ..., N}.
(3)
For the main experiments, N and K are set to 5 and 2, re-
spectively.
Proposed Stereo-Confidence Method
Using the sets of predicted and target disparity maps
DDPS
pred, DDPS
tgt
∈ RN×H×W , we can measure the degree of
distortion, which represents the level of ambiguity, the unre-
liability U(p) ∈ R1×H×W at each pixel p as follows:
U(p) =
1
N − 1
X
∥DDPS
tgt (p) − DDPS
pred(p)∥1.
(4)
Using the unreliability U(p), the confidence C(p) at each
pixel p can be obtained by
C(p) = e−σ U(p)
dmax ,
(5)
where a scale factor σ is set to have a confidence of 0.5 when
the unreliability U(p) is 1 and dmax is the maximum dispar-
ity value of Φs, respectively.
Dataset
Stereo
Single-Modality
Bi-Modality
Tri-Modality
Optimal
Ours
CCNN
CCNN†
LFN
ConfNet
LGC-Net
LAF-Net∗
LAF-Net∗†
LAF-Net
LAF-Net†
K2012
PSMNet
0.1659
0.2748
0.1306
0.4182
0.8125
0.2751
0.2138
0.1084
0.0982
0.0903
0.0114
GANet
0.1094
0.1411
0.0737
0.1514
0.5348
0.1615
0.1225
0.0632
0.0540
0.0522
0.0052
STTR
0.2036
0.1810
0.1467
0.1766
0.6639
0.1943
0.1653
0.1285
0.1182
0.1166
0.0558
RAFT
0.4729
0.6997
0.4129
1.0849
1.3327
0.8685
0.7820
0.3563
0.3862
0.3414
0.0776
LEAStereo
0.1886
0.2636
0.1569
0.3217
0.8333
0.2506
0.2469
0.1284
0.1059
0.1030
0.0146
ACVNet
0.1189
0.1848
0.0915
0.3426
0.7476
0.2959
0.1862
0.0747
0.0798
0.0726
0.0107
IGEV
0.0856
0.1327
0.0592
0.1770
0.4834
0.1544
0.1462
0.0552
0.0819
0.0555
0.0057
K2015
PSMNet
1.2924
1.5767
1.1146
1.7879
2.6216
1.5192
1.3849
1.1574
1.1475
1.0751
0.4152
GANet
0.2639
0.3641
0.1798
0.3439
0.8074
0.4174
0.3126
0.1789
0.1446
0.1429
0.0231
STTR
0.2027
0.2006
0.1531
0.2040
0.5377
0.2314
0.1719
0.1308
0.1215
0.1196
0.0454
RAFT
0.3692
0.4925
0.3110
0.8226
0.7721
0.6607
0.4071
0.2632
0.3087
0.2507
0.0409
LEAStereo
0.4809
0.5735
0.3955
0.7827
1.4229
0.6322
0.5308
0.3954
0.3875
0.3448
0.1090
ACVNet
0.7263
0.9087
0.6202
1.1663
1.6532
1.1239
0.7437
0.6119
0.5824
0.5572
0.2351
IGEV
0.0752
0.1404
0.0566
0.1239
0.2857
0.1214
0.1080
0.0504
0.0700
0.0555
0.0039
VK2-S6
PSMNet
0.9487
1.7111
0.8283
2.0130
3.8305
1.4161
0.9103
0.5897
0.6592
0.6378
0.1688
GANet
0.7156
0.5764
0.4316
0.5689
2.1518
0.4978
0.4343
0.2716
0.3757
0.3393
0.0504
STTR
0.6934
0.7067
0.5851
1.2171
2.9325
0.7810
0.9654
0.4719
0.4864
0.4646
0.1683
RAFT
0.4470
0.6569
0.3617
1.0762
1.1631
0.6991
0.4815
0.2636
0.4420
0.3108
0.0636
LEAStereo
0.4804
0.4590
0.3765
1.2508
2.5696
0.6148
0.3155
0.2615
0.3755
0.2915
0.0550
ACVNet
0.6345
1.1484
0.4208
3.1008
2.6430
2.9222
0.5093
0.2940
0.4768
0.3339
0.0712
IGEV
0.7486
0.6770
0.3472
1.3300
2.2009
0.6735
0.4810
0.3259
0.5955
0.3432
0.0707
M2014
PSMNet
7.1652
12.4301
6.4687
14.8883
16.5317
11.9619
11.0884
6.2982
6.4352
5.9379
2.4806
GANet
5.4232
7.2175
4.3756
8.8204
14.5286
10.6207
6.8133
4.3614
4.3084
4.0169
1.4643
STTR
5.9919
6.2478
4.8673
5.9441
13.0369
6.2403
5.8629
4.1321
4.4385
4.1064
2.1278
RAFT
3.0177
5.5518
2.7924
7.0148
7.2556
6.6081
6.0780
3.0382
3.4659
2.5483
0.7726
LEAStereo
4.2187
5.9621
3.5312
7.1150
11.1104
6.7709
6.9652
3.2800
3.4835
3.1245
1.0799
ACVNet
4.5361
8.7304
3.9213
11.114
13.5518
11.2322
8.6013
3.8292
4.4198
3.6391
1.3322
IGEV
3.8321
5.8557
3.2527
5.9323
9.8204
5.8438
5.7671
3.1207
4.2789
2.8119
0.8946
Table 2: The average AUC values for K2012, K2015, M2014, and VK-S6 datasets. The ‘Stereo’ denotes the stereo-matching
network. The ‘Optimal’ denotes the AUC value of ground truth confidence map. The best and second-best results in each
combination of input modalities are highlighted and underlined, respectively.
Experiments
Datasets
KITTI 2012 (K2012) (Menze and Geiger 2015a) and
KITTI 2015 (K2015) (Menze and Geiger 2015b), which
are outdoor driving scene datasets, consist of 194 and 200
stereo-image pairs and corresponding sparse ground truth
disparity maps obtained from LiDAR sensor measurements,
respectively. We split the K2012 into 20 images for train-
ing and 174 images for testing following (Poggi, Tosi, and
Mattoccia 2017). Virtual KITTI 2 (VK2) (Cabon, Mur-
ray, and Humenberger 2020), which is a photo-realistic vir-
tual driving scene dataset, contains 21,260 stereo-image
pairs of different 6 scenes with various weather and illu-
mination conditions (fog, overcast, rain, morning, and sun-
set). We only use Scene06 (VK2-S6) for testing, which is
referred to test dataset by the authors. Middlebury 2014
(M2014) (Scharstein et al. 2014) is an indoor scene dataset,
which is composed of high-resolution 15 stereo pair images
and corresponding dense ground truth disparity maps. For
the M2014, we use quarter-resolution images in all experi-
ments following Poggi, Tosi, and Mattoccia (2017). We train
all stereo-confidence networks using the K2012 training set
(20 images) and evaluate them on the K2012 test set (174
images), K2015 (200 images), VK2-S6 (270 images), and
M2014 (15 images). Also, we experiment with challenging
subsets of VK2-S6 with four different weather and illumina-
tion conditions (fog, rain, morning, and sunset). We exclude
pixels with disparities d > 192 in training and test in all
datasets.
Stereo-Matching Networks
To demonstrate the adaptability, end-to-end stereo-matching
networks with various architectures and state-of-the-art
performance, such as PSMNet (Chang and Chen 2018),
GANet (Zhang et al. 2019), STTR (Li et al. 2021), LEASt-
ereo (Cheng et al. 2020), RAFT-Stereo (Lipson, Teed,
and Deng 2021), ACVNet (Xu et al. 2022), and IGEV-
Stereo (Xu et al. 2023) are used to obtain predicted disparity
maps and cost volume, which are used as input modalities
of stereo-confidence estimation networks. Regardless of the
dataset and network type, we use weights fine-tuned on the
KITTI datasets provided by authors for all experiments.
Confidence Networks
As classified in Table 1, we set CCNN (Poggi and Mattoccia
2016), LFN (Fu, Ardabilian, and Stern 2019), ConfNet (Tosi
et al. 2018), LGC-Net (Tosi et al. 2018), LAF-Net∗ (Kim
et al. 2019), and LAF-Net (Kim et al. 2019) as compar-
ison groups of the proposed method considering various
combinations of input modalities. These methods still show
state-of-the-art performance as mentioned in (Poggi et al.
2021a; Kim et al. 2022). The LAF-Net∗ denotes LAF-Net
that uses bi-modal inputs (disparity, image) without cost vol-
ume. For CCNN†, LAF-Net∗†, and LAF-Net†, which use
the proposed method (Ours) as an additional input modality,
the modifications are limited to fundamental aspects such
Figure 3: The confidence maps on K2012 (1st and 2nd rows), K2015 (3rd and 4th rows), VK2-S6 (5th and 6th rows), and
M2014 (last two rows) datasets using PSMNet. (From top to bottom, left to right) left image, predicted disparity map, estimated
confidence maps by Ours, CCNN, CCNN†, LFN, ConfNet, LGC, LAF-Net∗, LAF-Net∗†, LAF-Net, and LAF-Net†.
as concatenating input modalities or adding a few layers.
Among the existing stereo-confidence methods using bi-
modal inputs, since LAF-Net∗ generally shows better per-
formance than LFN, ConfNet, and LGC-Net, we experi-
ment with LAF-Net∗ to check the usefulness of Ours as an
additional input modality (LAF-Net∗†). We obtain all ex-
perimental results using codes provided by authors without
modifying any hyperparameters. All experiments are con-
ducted on a machine with 8 GeForce RTX 2080 Ti GPUs.
Evaluation Metrics
As introduced in Hu and Mordohai (2012), we evaluate the
performance of each stereo-confidence method by an area
under the curve (AUC) value of the ROC curve, which repre-
sents how well the measurement identifies correct matches.
Ideally, if the measurement identifies all correct matches, the
optimal AUC value can be obtained as
Z 1
1−ε
x − (1 − ε)
x
dx = ε + (1 − ε) ln (1 − ε),
(6)
where ε denotes the error rate computed over the entire dis-
parity map. As in Poggi et al. (2021b), we set the thresh-
old value τ to 3 in obtaining ground truth confidence maps.
The optimal AUC values are measured by using ground truth
confidence maps. All reported AUC values in tables are mul-
tiplied by a factor of 102 to ease the readability.
Confidence Estimation Analysis
Quantitative results. In Table 2, we compare the average
AUC values of Ours and existing learning-based methods
on the K2012, K2015, VK2-S6, and M2014 datasets to val-
idate the effectiveness of Ours. Although Ours is conven-
tional and uses a single-modality input, it shows competitive
results compared to existing methods in most cases regard-
less of the datasets and stereo-matching networks.
Although internal features such as cost volume are not
feasible to be used in the stereo-confidence networks in
safety-critical systems, there are some exceptions where the
cost volume can be utilized. For such cases, we experiment
with LAF-Net using Ours as an additional input modality.
Without exceptions, when Ours is used as an additional in-
put, LAF-Net† shows better performance than LAF-Net.
To further check compatibility with the existing learning-
based stereo-confidence methods, we also experiment us-
ing Ours as an additional input modality to CCNN and
LAF-Net∗. CCNN† and LAF-Net∗† also show better per-
formance than CCNN and LAF-Net∗. It demonstrates that
Ours can be utilized as a useful input even for learning-based
stereo-confidence networks. Surprisingly, there are various
cases in which LAF-Net∗† shows better performance than
LAF-Net even if the cost volume is not utilized. It indi-
cates that Ours properly reinterprets the conventional stereo-
confidence methods to be suitable for end-to-end stereo-
matching networks by analyzing the disparity profile out of
the network. We believe Ours can serve as an alternative to
Dataset
Stereo
Single-Modality
Bi-Modality
Tri-Modality
Optimal
Ours
CCNN
CCNN†
LFN
ConfNet
LGC-Net
LAF-Net∗
LAF-Net∗†
LAF-Net
LAF-Net†
VK2-S6
-fog
PSMNet
1.5714
3.9270
1.5670
4.2668
4.3988
1.8486
2.0230
1.2546
1.3938
1.1294
0.3102
GANet
0.7645
1.9908
0.5007
3.0603
4.1442
3.1586
0.8559
0.3242
0.6379
0.4782
0.0839
STTR
1.1953
1.2905
1.0104
3.9370
3.9897
1.3478
1.3715
1.1486
0.8871
0.8606
0.2735
RAFT
0.7572
1.7562
0.5356
4.6943
1.1980
1.2518
1.4674
0.4874
0.7345
0.5278
0.0918
LEAStereo
0.5285
1.5298
0.4032
2.1754
2.9666
3.2066
0.8729
0.2934
0.4875
0.3315
0.0696
ACVNet
1.0103
2.5226
0.7175
7.4959
4.3985
5.5526
1.9168
0.4846
0.8232
0.6108
0.1134
IGEV
0.6386
0.8663
0.2981
1.2349
2.9868
0.6100
1.0303
0.2814
1.1326
0.3224
0.0657
VK2-S6
-morning
PSMNet
1.1007
1.9984
0.9888
3.2150
5.3708
1.7921
1.1212
0.6655
0.7872
0.7284
0.1990
GANet
0.7917
0.8453
0.5879
0.9904
3.0191
0.5635
0.5562
0.4603
0.4286
0.4108
0.0677
STTR
0.8737
0.8032
0.7455
1.3725
3.2641
0.8398
1.3731
0.5812
0.5825
0.5648
0.1887
RAFT
0.5524
0.7493
0.4553
1.3715
1.6059
0.8357
1.0748
0.3453
0.9491
0.4893
0.0776
LEAStereo
0.6015
0.5518
0.4961
1.6568
2.5703
0.8983
0.4954
0.3562
0.4910
0.3657
0.0685
ACVNet
0.7416
1.4871
0.5607
3.8478
4.6123
3.5231
1.3998
0.3796
0.6356
0.4019
0.0924
IGEV
0.7853
0.7131
0.3666
1.8922
3.6770
0.6899
1.1511
0.4087
0.8151
0.3590
0.0800
VK2-S6
-rain
PSMNet
2.4996
6.2982
2.3545
9.4548
8.4538
4.5375
5.2998
1.7692
1.8509
1.6727
0.7111
GANet
1.5227
2.6191
1.0581
3.6266
6.1154
2.8830
2.4524
0.8130
0.9638
0.9056
0.2286
STTR
2.6439
2.8803
2.2719
4.4403
10.3646
3.1961
3.7272
2.0025
2.1443
1.9899
0.8483
RAFT
2.2389
4.8650
2.0608
8.6372
10.7812
6.8327
4.8524
1.8744
2.7674
1.9274
0.6331
LEAStereo
1.3635
2.1458
1.1759
5.3306
8.2161
3.8112
2.5093
1.0158
1.3694
1.0810
0.3362
ACVNet
1.9070
4.9129
1.5718
7.8770
9.4251
6.0507
3.7207
1.2443
1.6612
1.5304
0.4879
IGEV
1.8363
2.4547
1.1263
3.9116
6.8699
2.4990
3.6585
1.1037
2.5958
1.1617
0.3769
VK2-S6
-sunset
PSMNet
1.2313
1.9764
1.0755
3.4468
5.2673
1.8211
1.4248
0.7716
0.8089
0.7719
0.1999
GANet
0.8773
0.8102
0.5619
0.9904
3.0339
0.6094
0.5207
0.3475
0.4516
0.4204
0.0659
STTR
0.9846
0.8723
0.7970
1.2677
4.3465
0.9322
0.9880
0.6915
0.6521
0.6083
0.1953
RAFT
0.5739
0.7588
0.4599
0.9453
1.3361
0.7941
0.8583
0.3745
1.1111
0.5814
0.0787
LEAStereo
0.6104
0.5405
0.4788
1.0449
3.4167
0.7635
0.3694
0.3185
0.4908
0.3952
0.0663
ACVNet
0.8389
1.3606
0.6004
3.0129
4.1981
3.1625
0.8184
0.4192
0.6201
0.4870
0.0921
IGEV
0.7873
0.7688
0.3583
1.2189
3.2871
0.7139
0.8050
0.3564
0.6740
0.3628
0.0816
Table 3: The average AUC values for VK2-S6 dataset with 4 different weather conditions (fog, morning, rain, and sunset). The
‘Stereo’ denotes the stereo-matching network. The ‘Optimal’ denotes the AUC value of ground truth confidence map. The best
and second-best results in each combination of input modalities are highlighted and underlined, respectively.
Method
# Params.
AUC ↓
Latency (s)
1 GPU
N GPUs
Ours (N=2, k0=0, k1=1)
-
0.7917
0.6490
0.6429
Ours (N=3, K=1)
-
0.5455
0.9301
0.7435
Ours (N=5, K=2)
-
0.4729
1.5114
0.8551
Ours (N=7, K=3)
-
0.4383
2.0735
0.9959
LAF-Net∗
0.57M
0.7820
0.6552
-
LAF-Net∗† (N=5)
0.69M
0.3563
1.6446
0.9590
LAF-Net
0.69M
0.3862
0.7325
-
LAF-Net† (N=5)
0.80M
0.3414
1.7014
1.1915
Table 4: Further study for the average AUC value and the la-
tency according to the number of disparity plane shifts N on
K2012 using RAFT. The best and second-best results in each
experiment are highlighted and underlined, respectively.
Method
K=1
K=2
K=4
K=8
Ours (N=3)
0.5455
0.6022
0.6158
0.6984
Table 5: Ablation study for the shifting step size on K2012
using RAFT. The best and second-best results are high-
lighted and underlined, respectively.
the cost volume as an input modality in the existing and fu-
ture learning-based stereo-confidence networks.
Qualitative results. In Fig. 3, we visualize the qualitative
results of Ours and the existing methods on the K2012,
K2015, VK2-S6, and M2014. Although the existing meth-
ods generally show strong capabilities in detecting occlu-
sion boundaries, they struggle to identify other remaining
ill-posed regions like repeated patterns, textureless regions,
and non-Lambertian surfaces. Ours effectively identifies not
only inaccurate but also ill-posed regions such as occlusion
boundaries, repeated patterns, textureless regions, and non-
Lambertian surfaces. See supplementary material for more
results.
Further Studies
Different weather conditions. In Table 3, we also evalu-
ate Ours and existing methods on VK2-S6 of four different
weather conditions (fog, morning, rain, sunset) for further
studies on various driving environments. The performances
of VK-S6-fog and VK-S6-rain are generally worse than
those of VK-S6-morning and VK-S6-sunset because VK-
S6-fog and VK-S6-rain cannot be observed in the K2012
dataset, which is a training dataset. Similar to Table 2, the
methods with the proposed method, which are denoted to su-
perscript †, generally show better performance than the other
methods, even in different weather conditions, especially in
VK-S6-fog and VK-S6-rain. See supplementary material for
more qualitative results.
The trade-off between AUC value and latency. In Table 4,
we examine the influence of varying the number of disparity
plane shifts N on the average AUC value and latency using
the input samples from the K2012 dataset with full resolu-
tion (H ×W = 384×1248) and the RAFT stereo-matching
network. Since the latency of Ours depends on N, the la-
ConfNet
OTB
OTB-online
ConfNet†
Ours
Optimal
1.7420
1.7419
1.4632
0.9581
1.3013
0.0654
Table 6: Comparison to self-adaptation method on Driving-
Stereo using GANet. The ‘Optimal’ denotes the AUC value
of ground truth confidence map. The best and second-best
results are highlighted and underlined, respectively.
tency shown in Table 4 is composed of the processing time
of RAFT and that of the stereo-confidence methods.
Ours requires a minimum of two disparity maps to work.
With the minimum requirement setting N = 2, it exhibits
marginally reduced performance but boasts faster latency on
both single and multiple GPUs compared to LAF-Net∗. As
N increases, the average AUC value of Ours consistently
improves, but at the cost of increased latency. By adjusting
the batch size to N during inference, N disparity maps can
be obtained by a single forward from RAFT. A clear limita-
tion of Ours is the slower latency under resource-constrained
environments, like using a single GPU. Yet, this can be mit-
igated by scaling the number of GPUs in tandem with N. To
break it down: LAF-Net∗ has a latency comprised of 0.5817
seconds for RAFT processing and 0.0735 seconds for LAF-
Net∗ itself. In contrast, Ours (N = 2) with a single GPU
consists of 0.6490 seconds for RAFT and a negligible time
from 10−5 to 10−6 seconds for our own processing time.
The latency is averaged over K2012 test set (174 images).
Varing shifting step size K. As shown in Table 5, we fur-
ther examine the performance according to the shifting step
size K. With N = 3, the performance is typically decreased
as the shifting step size K is increased. Thus, we fix the
shifting step size K at the minimum pixel unit of 1.
Comparison to self-adaptation method Self-adapting con-
fidence (Poggi et al. 2020) is close to Ours. While the self-
adapting confidence is rooted in a conventional approach, it
is built upon the learning-based framework without using in-
ternal information from stereo-matching methods. The self-
adapting confidence is achieved by integrating three stereo-
confidence cues: image reprojection error, agreement among
neighboring matches, and a uniqueness constraint. Table 6
presents a comparison between Ours and self-adapt confi-
dence, which denoted OTB and OTB-online. The experi-
ments utilize GANet on 6905 samples of the DrivingStereo
dataset (Yang et al. 2019) provided by authors, as following
the procedures laid out in the self-adapt confidence method
(Poggi et al. 2020). Both OTB and OTB-online are trained
in a self-supervised manner and OTB-online is OTB with
online adaptation. Since OTB and OTB-online are imple-
mented on ConfNet, the performance of ConfNet is also pre-
sented as a baseline for the supervised method. While both
OTB and OTB-online outperform ConfNet, our method sur-
passes the results of both. ConfNet† stands out with the best
performance, but it is trained under supervision, making di-
rect comparisons inequitable.
Conclusion
We present a stereo-confidence measurement that operates
outside end-to-end stereo-matching networks, which is a re-
cent paradigm of the learning-based stereo-matching meth-
ods. The key idea of the proposed method is to reinterpret the
conventional stereo-confidence method analyzing the cost
profile to be suitable for end-to-end stereo-matching net-
works by analyzing disparity profiles. To measure the con-
fidence, using a predicted disparity map without the dispar-
ity plane sweep as an anchor, the desirable disparity pro-
file shaped into a linear line is generated and compared to
the disparity maps obtained using the disparity plane sweep.
We also investigate compatibility with the learning-based
stereo-confidence networks using the proposed method as
an additional input modality. Our extensive experimental re-
sults demonstrate that the proposed method not only shows
competitive confidence performance but also significantly
enhances the performance of learning-based methods as an
additional input modality. In future research, we plan to uti-
lize the proposed method in the disparity refinement frame-
work working in a self-supervised manner.
Acknowledgements
This work was supported by Institute of Information &
communications Technology Planning & Evaluation (IITP)
grant funded by the Korea government(MSIT) [NO.2022-0-
00184, Development and Study of AI Technologies to Inex-
pensively Conform to Evolving Policy on Ethics]
References
Badki, A.; Troccoli, A.; Kim, K.; Kautz, J.; Sen, P.; and
Gallo, O. 2020. Bi3D: Stereo Depth Estimation via Binary
Classifications. In Proc. CVPR.
Cabon, Y.; Murray, N.; and Humenberger, M. 2020. Virtual
KITTI 2. arXiv:2001.10773.
Chang, J.; and Chen, Y. 2018. Pyramid Stereo Matching
Network. CoRR, abs/1803.08669.
Cheng, K.; Healey, C.; and Wu, T. 2021.
Towards ad-
versarially robust and domain generalizable stereo match-
ing by rethinking dnn feature backbones.
arXiv preprint
arXiv:2108.00335.
Cheng, X.; Zhong, Y.; Harandi, M.; Dai, Y.; Chang, X.; Li,
H.; Drummond, T.; and Ge, Z. 2020. Hierarchical Neural
Architecture Search for Deep Stereo Matching. Advances in
Neural Information Processing Systems, 33.
Egnal, G.; Mintz, M.; and Wildes, R. P. 2004. A stereo con-
fidence metric using single view imagery with comparison
to five alternative approaches. Image and Vision Computing,
22(12): 943–957. Proceedings from the 15th International
Conference on Vision Interface.
Fu, Z.; Ardabilian, M.; and Stern, G. 2019. Stereo Match-
ing Confidence Learning Based on Multi-modal Convolu-
tion Neural Networks.
In Chen, L.; Ben Amor, B.; and
Ghorbel, F., eds., Representations, Analysis and Recogni-
tion of Shape and Motion from Imaging Data, 69–81. Cham:
Springer International Publishing. ISBN 978-3-030-19816-
9.
Haeusler, R.; and Klette, R. 2012. Evaluation of stereo con-
fidence measures on synthetic and recorded image data. In
2012 International Conference on Informatics, Electronics
& Vision (ICIEV), 963–968.
Haeusler, R.; Nair, R.; and Kondermann, D. 2013. Ensemble
Learning for Confidence Measures in Stereo Vision. In 2013
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 305–312.
Het Veld, R. O.; Jaschke, T.; B¨atz, M.; Palmieri, L.; and
Keinert, J. 2018. A Novel Confidence Measure for Disparity
Maps by Pixel-Wise Cost Function Analysis. In 2018 25th
IEEE International Conference on Image Processing (ICIP),
644–648.
Hu, X.; and Mordohai, P. 2012. A Quantitative Evaluation of
Confidence Measures for Stereo Vision. IEEE Trans. Pattern
Anal. Mach. Intell., 34(11): 2121–2133.
Kim, S.; Jang, C. Y.; and Kim, Y. H. 2016. Weighted peak
ratio for estimating stereo confidence level using color sim-
ilarity. In 2016 IEEE Asia Pacific Conference on Circuits
and Systems (APCCAS), 196–197.
Kim, S.; Kim, S.; Min, D.; and Sohn, K. 2019. LAF-Net:
Locally Adaptive Fusion Networks For Stereo Confidence
Estimation. In Proc. CVPR.
Kim, S.; Poggi, M.; Kim, S.; Sohn, K.; and Mattoccia, S.
2022. Meta-confidence estimation for stereo matching. In
2022 International Conference on Robotics and Automation
(ICRA), 10624–10631.
Kim, S.; Yoo, D.-g.; and Kim, Y. H. 2014. Stereo confidence
metrics using the costs of surrounding pixels. In 2014 19th
International Conference on Digital Signal Processing, 98–
103.
Lee, J. Y.; and Park, R.-H. 2017a.
Depth Estimation
From Light Field by Accumulating Binary Maps Based on
Foreground-Background Separation. IEEE Journal of Se-
lected Topics in Signal Processing, 11(7): 955–964.
Lee, J. Y.; and Park, R.-H. 2017b. Separation of foreground
and background from light field using gradient information.
OSA Applied Optics, 56(4): 1069–1078.
Lee, J. Y.; and Park, R.-H. 2018.
Reduction of Aliasing
Artifacts by Sign Function Approximation in Light Field
Depth Estimation Based on Foreground-Background Sepa-
ration. IEEE Signal Processing Letters, 25(11): 1750–1754.
Lee, J. Y.; and Park, R.-H. 2021. Complex-Valued Dispar-
ity: Unified Depth Model of Depth from Stereo, Depth from
Focus, and Depth from Defocus Based on the Light Field
Gradient. IEEE Trans. Pattern Anal. Mach. Intell., 43(3):
830–841.
Lee, J. Y.; Park, R.-H.; and Kim, J. 2021.
Occlusion
Handling by Successively Excluding Foregrounds for Light
Field Depth Estimation Based on Foreground-Background
Separation. IEEE Access, 9: 103927–103936.
Li, Z.; Liu, X.; Drenkow, N.; Ding, A.; Creighton, F. X.; Tay-
lor, R. H.; and Unberath, M. 2021. Revisiting Stereo Depth
Estimation From a Sequence-to-Sequence Perspective With
Transformers.
In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV), 6197–6206.
Lipson, L.; Teed, Z.; and Deng, J. 2021. RAFT-Stereo: Mul-
tilevel Recurrent Field Transforms for Stereo Matching. In
International Conference on 3D Vision (3DV).
Matthies, L. 1992.
Stereo vision for planetary rovers:
Stochastic modeling to near real-time implementation. In-
ternational Journal of Computer Vision, 8(1): 71–91.
Menze, M.; and Geiger, A. 2015a. Object scene flow for au-
tonomous vehicles. In 2015 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 3061–3070.
Menze, M.; and Geiger, A. 2015b. Object scene flow for au-
tonomous vehicles. In Proceedings of the IEEE conference
on computer vision and pattern recognition, 3061–3070.
Poggi, M.; Aleotti, F.; Tosi, F.; Zaccaroni, G.; and Mattoccia,
S. 2020. Self-adapting confidence estimation for stereo. In
Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIV
16, 715–733. Springer.
Poggi, M.; Kim, S.; Tosi, F.; Kim, S.; Aleotti, F.; Min, D.;
Sohn, K.; and Mattoccia, S. 2021a. On the confidence of
stereo matching in a deep-learning era: a quantitative eval-
uation. IEEE transactions on pattern analysis and machine
intelligence, 44(9): 5293–5313.
Poggi, M.; Kim, S.; Tosi, F.; Kim, S.; Aleotti, F.; Min, D.;
Sohn, K.; and Mattoccia, S. 2021b. On the confidence of
stereo matching in a deep-learning era: a quantitative eval-
uation. IEEE transactions on pattern analysis and machine
intelligence, 44(9): 5293–5313.
Poggi, M.; and Mattoccia, S. 2016. Learning from scratch
a confidence measure. In Richard C. Wilson, E. R. H.; and
Smith, W. A. P., eds., Proceedings of the British Machine Vi-
sion Conference (BMVC), 46.1–46.13. BMVA Press. ISBN
1-901725-59-6.
Poggi, M.; Tosi, F.; and Mattoccia, S. 2017. Quantitative
Evaluation of Confidence Measures in a Machine Learning
World.
In 2017 IEEE International Conference on Com-
puter Vision (ICCV), 5238–5247.
Scharstein, D.; Hirschm¨uller, H.; Kitajima, Y.; Krathwohl,
G.; Neˇsi´c, N.; Wang, X.; and Westling, P. 2014.
High-
Resolution Stereo Datasets with Subpixel-Accurate Ground
Truth. In Jiang, X.; Hornegger, J.; and Koch, R., eds., Pat-
tern Recognition, 31–42. Cham: Springer International Pub-
lishing. ISBN 978-3-319-11752-2.
Scharstein, D.; and Szeliski, R. 1996. Stereo matching with
non-linear diffusion. In Proceedings CVPR IEEE Computer
Society Conference on Computer Vision and Pattern Recog-
nition, 343–350.
Sheng, H.; Zhao, P.; Zhang, S.; Zhang, J.; and Yang, D.
2018. Occlusion-aware depth estimation for light field using
multi-orientation EPIs. Pattern Recognition, 74: 587 – 599.
Shin, C.; Jeon, H.-G.; Yoon, Y.; Kweon, I. S.; and Kim, S. J.
2018. EPINET: A Fully-Convolutional Neural Network Us-
ing Epipolar Geometry for Depth from Light Field Images.
In 2018 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, 4748–4757.
Tosi, F.; Poggi, M.; Benincasa, A.; and Mattoccia, S. 2018.
Beyond local reasoning for stereo confidence estimation
with deep learning. In Proceedings of the European Con-
ference on Computer Vision (ECCV).
Wang, C.; Wang, X.; Zhang, J.; Zhang, L.; Bai, X.; Ning, X.;
Zhou, J.; and Hancock, E. 2022. Uncertainty estimation for
stereo matching based on evidential deep learning. Pattern
Recognition, 124: 108498.
Wang, J.; Pun, A.; Tu, J.; Manivasagam, S.; Sadat, A.; Casas,
S.; Ren, M.; and Urtasun, R. 2021.
Advsim: Generating
safety-critical scenarios for self-driving vehicles.
In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 9909–9918.
Wang, T.; Efros, A. A.; and Ramamoorthi, R. 2016. Depth
Estimation with Occlusion Modeling Using Light-Field
Cameras. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 38(11): 2170–2181.
Wedel, A.; Meißner, A.; Rabe, C.; Franke, U.; and Cremers,
D. 2009.
Detection and Segmentation of Independently
Moving Objects from Dense Scene Flow. In Cremers, D.;
Boykov, Y.; Blake, A.; and Schmidt, F. R., eds., Energy Min-
imization Methods in Computer Vision and Pattern Recogni-
tion, 14–27. Berlin, Heidelberg: Springer Berlin Heidelberg.
ISBN 978-3-642-03641-5.
Williem; Park, I. K.; and Lee, K. M. 2018. Robust Light
Field Depth Estimation Using Occlusion-Noise Aware Data
Costs. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 40(10): 2484–2497.
Wong, A.; Mundhra, M.; and Soatto, S. 2021. Stereopag-
nosia: Fooling stereo networks with adversarial perturba-
tions. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 35, 2879–2888.
Xu, G.; Cheng, J.; Guo, P.; and Yang, X. 2022. Attention
Concatenation Volume for Accurate and Efficient Stereo
Matching. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 12981–12990.
Xu, G.; Wang, X.; Ding, X.; and Yang, X. 2023. Iterative
Geometry Encoding Volume for Stereo Matching. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 21919–21928.
Xu, H.; and Zhang, J. 2020.
AANet: Adaptive Aggre-
gation Network for Efficient Stereo Matching.
CoRR,
abs/2004.09548.
Yang, G.; Song, X.; Huang, C.; Deng, Z.; Shi, J.; and
Zhou, B. 2019. DrivingStereo: A Large-Scale Dataset for
Stereo Matching in Autonomous Driving Scenarios. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR).
Zbontar, J.; and LeCun, Y. 2015. Stereo Matching by Train-
ing a Convolutional Neural Network to Compare Image
Patches. CoRR, abs/1510.05970.
Zeng, K.; Wang, Y.; Wang, W.; Zhang, H.; Mao, J.; and Zhu,
Q. 2023.
Deep Confidence Propagation Stereo Network.
IEEE Transactions on Intelligent Transportation Systems,
24(8): 8097–8108.
Zhang, F.; Prisacariu, V. A.; Yang, R.; and Torr, P. H. S.
2019.
GA-Net: Guided Aggregation Net for End-to-end
Stereo Matching. CoRR, abs/1904.06587.
Zhang, S.; Sheng, H.; Li, C.; Zhang, J.; and Xiong, Z. 2016.
Robust depth estimation for light field via spinning parallel-
ogram operator. Computer Vision and Image Understand-
ing, 145: 148–159.
Zhou, K.; Meng, X.; Cheng, B.; et al. 2020. Review of stereo
matching algorithms based on deep learning. Computational
intelligence and neuroscience, 2020.
Zhou, W.; Wei, X.; Yan, Y.; Wang, W.; and Lin, L. 2019a.
A hybrid learning of multimodal cues for light field depth
estimation. Digital Signal Processing, 95: 102585.
Zhou, W.; Zhou, E.; Yan, Y.; Lin, L.; and Lumsdaine, A.
2019b. Learning Depth Cues from Focal Stack for Light
Field Depth Estimation. In 2019 IEEE International Con-
ference on Image Processing (ICIP), 1074–1078.
"
"We present Expert Monitoring, an approach that leverages domainexpertise to enhance detecting and mitigating concept drift in machine learning (ML) models.","The integration of ML in modern software systems marks a shift from deterministic to stochastic behavior. This transition brings a challenge: ensuring the consistent performance of ML models susceptible to data distribution changes due to external events and data integrity issues. This phenomenon known as data drift comes with variations in the input distribution (Φ (X)) and conditional probability distribution of the target variable given an input (Φ (Y|X)) known as feature drift and concept drift respectively. Since concept drift significantly impacts model performance, it requires mitigation actions. The growing use of ML demands addressing concept drift, especially in sensitive areas like credit card fraud detection, to prevent discriminatory actions. This work aims to scrutinize concept drift mitigation solutions, integrating them into the emerging ML operations (MLOps) framework.","The research delves into MLOps practices and challenges, highlighting the inherent limitations of automated concept drift detection and mitigation. The inherent limitations of automated concept drift detection and mitigation are then presented, motivating the necessity for domain expertise. The approach to integrating expert knowledge into a monitoring system is outlined based on this understanding.nan","Expert Monitoring, a method that addresses operational challenges, is proposed. The method integrates domain expertise through scenario specification. It then makes this expertise accessible to on-call ML engineers via scenario identification, providing insights into the potential causes of feature drift upon detection. Scenario specification begins by consolidating domain expertise through knowledge elicitation and retrospective analyses. Knowledge elicitation involves collaborations between ML engineers and domain experts to elicit scenarios of events that can induce concept drift in an ML model's application context. Retrospective analyses are conducted by ML engineers to isolate recurring problematic events for future identification by examining the model's historical performance and correlating performance drops with feature drift events. The acquired scenarios are assembled by the ML engineer and stored in a standardized format. These scenarios include information like the ML model, scenario description, a Bayesian model, scenario understanding, and scenario response.","At runtime, feature drift triggers scenario inference using the pre-defined Bayesian models. Bayesian model comparison assesses the likelihood of each scenario model based on recent observations. Bayes factors are displayed indicating the relative likelihood of each scenario model compared to the reference model. The Bayes factors assist ML engineers in response selection for feature drift, with the option of automating concept drift mitigation by triggering the top-ranked scenario's response.","Expert Monitoring leverages domain expertise to enhance detecting and mitigating concept drift in ML models. The research identifies key challenges in detecting and deciphering concept drift's latent aspects, emphasizing the need for domain expertise. It also highlights the difficulty in relying solely on domain expertise due to factors such as staff turnover and limited documentation. The approach helps consolidate domain expertise and make it accessible to on-call ML engineers by utilizing Bayesian models to compare scenarios. Future plans for refining the approach and validating it through collaboration with practitioners are discussed.",Expert-Driven Monitoring of Operational ML Models,"Joran Leest, Claudia Raibulet, Ilias Gerostathopoulos, Patricia Lago","Expert-Driven Monitoring of Operational ML Models
Joran Leest
Vrije Universiteit
Amsterdam
The Netherlands
j.g.leest@vu.nl
Claudia Raibulet
Vrije Universiteit
Amsterdam
The Netherlands
c.raibulet@vu.nl
Ilias Gerostathopoulos
Vrije Universiteit
Amsterdam
The Netherlands
i.g.gerostathopoulos@vu.nl
Patricia Lago
Vrije Universiteit
Amsterdam
The Netherlands
p.lago@vu.nl
ABSTRACT
We propose Expert Monitoring, an approach that leverages domain
expertise to enhance the detection and mitigation of concept drift in
machine learning (ML) models. Our approach supports practitioners
by consolidating domain expertise related to concept drift-inducing
events, making this expertise accessible to on-call personnel, and
enabling automatic adaptability with expert oversight.
ACM Reference Format:
Joran Leest, Claudia Raibulet, Ilias Gerostathopoulos, and Patricia Lago.
2024. Expert-Driven Monitoring of Operational ML Models. In Proceedings
of ACM Conference (Conference’17). ACM, New York, NY, USA, 5 pages.
https://doi.org/10.1145
1
INTRODUCTION
The ubiquitous integration of machine learning (ML) in modern
software systems marks a shift from deterministic behavior of soft-
ware to behavior derived from stochastic processes. This transition
brings a significant challenge: ensuring the consistent performance
of ML models that are subject to changes in the data distribution,
due to external events and data integrity issues [12]. This phenome-
non, known as data drift, encompasses feature drift (changes in the
input distribution, 𝑃(𝑋)) and concept drift (changes in the condi-
tional probability distribution of the target variable given an input,
𝑃(𝑌 |𝑋)) [29]. Notably, concept drift has a substantial influence on
model performance and necessitates mitigative action [6, 14].
The growing use of ML underscores the importance of address-
ing concept drift, particularly in sensitive areas like credit card
fraud detection, to prevent discriminatory actions [21]. We must
scrutinize concept drift mitigation solutions, including insights
from data mining [6, 17], and integrate them into the emerging ML
operations (MLOps) framework used by practitioners [13].
In this paper, we explore MLOps practices and challenges, high-
lighting the inherent limitations of automated concept drift de-
tection and mitigation, and motivate the the necessity of domain
expertise (Section 2). Based on this understanding, we outline an
approach to integrate expert knowledge into a monitoring system
(Section 3). Finally, we discuss the key aspects of our approach
(Section 4), and conclude by outlining our future plans (Section 5).
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2024 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00
https://doi.org/10.1145
2
NAVIGATING THE DARKNESS: CONCEPT
DRIFT IN PRACTICE
This section discusses the challenge of detecting and mitigating
concept drift, and how domain expertise is utilized to address it.
2.1
The Practical Challenges of Concept Drift
Detection – Where the Shadows Lie
We see the challenges in detecting and deciphering concept drift’s
latent aspects, often leaving practitioners ""wandering in the dark"".
2.1.1
Concept Drift Detection Without Labeled Data. Unlike feature
drift, which can be readily observed in input data and its effects on
model performance inferred through various estimation methods
[3, 7, 10, 22, 23, 26], concept drift detection depends on monitoring
metrics such as accuracy [6, 12]. This is challenging in real-world
scenarios, as labeled data is often delayed or completely absent [12].
For example, in e-commerce churn prediction, churn determination
only occurs after a specified time frame (e.g., a month or year) [1].
In the absence of labeled data, practitioners typically rely on
detecting drift in the model’s predictions and features, such as
through the use of a Kolmogorov-Smirnov test, which can act as
a proxy to infer the presence of concept drift and its effect on
model performance [2, 12, 14]. This strategy makes sense because
it leverages the common co-occurrence of feature drift and concept
drift [6, 14, 16]. In real-world scenarios, events impact how data
is generated, collected, and managed, leading to changes in our
models that capture these processes [9]. Feature drift can act as a
visible signal of these changes. Nonetheless, inferring concept drift
from feature drift remains challenging, as their presence or severity
does not always correlate [14]. As a result, triggering alerts for
every instance of feature drift generates many false alarms [25–27].
To illustrate the challenge of concept drift detection without la-
beled data, let us consider a model that predicts whether a customer
will churn based on the customer age and recent page visits (Fig. 1).
Figure 1: Data drift in a customer churn prediction model.
arXiv:2401.11993v1  [cs.LG]  22 Jan 2024
Conference’17, July 2017, Washington, DC, USA
Joran Leest, Claudia Raibulet, Ilias Gerostathopoulos, and Patricia Lago
In Fig. 1, (a) depicts the post-deployment observations and the
learned decision boundary (represented by the black line). After
deployment, two events occur: (b) the web shop launches a market-
ing campaign targeting young people, causing drift in the customer
age feature, and (c) a competitor’s marketing campaign for a new
product line aimed at young people makes the web shop’s young
customers switch to the competitor’s service, causing drift in the
recent page visits feature. Event (b) does not affect customer satis-
faction; the learned decision boundary remains valid. Conversely,
event (c) negatively affects the (latent) customer satisfaction among
the web shop’s younger customers, whom have become aware of
the new offering. Consequently, concept drift occurs, rendering the
learned mapping function invalid. The core problem emerges: in
the absence of labeled data, monitoring systems that rely on
feature drift detection do not discern event (b) from (c).
2.1.2
Deciphering the Nature of Concept drift Post-Detection. Even
when the presence of concept drift can be confidently inferred,
effective response selection requires an understanding of the de-
tected drift’s characteristics [6, 11]. This includes the drift severity,
recurrence, duration, and transition speed [29]. For example, in the
case of recurrent drift, reactivating a previous model version might
effectively resolve it [18], whereas abrupt drift might necessitate a
complete model retrain. Conversely, for a short-lived drift duration,
retraining the model is not desirable; instead, it might suffice to
temporarily fall back on a more simple model.
Comprehending the nature of concept drift after its detection
(e.g. whether it is a recurring event) remains a significant challenge
[27], with current concept drift detection methods falling short in
facilitating comprehension of the drift’s characteristics [17].
2.2
Domain Expertise – A Light in Dark Places,
When All Other Lights Go Out
In recent works, Shankar et al. [25] and Shergadwala et al. [27]
conducted insightful interview studies involving ML engineers.
These studies highlighted the practical aspects of monitoring ML
models and the strategies employed by ML engineers in practice.
Their findings showed a common theme: automated concept drift
detection and mitigation is not a predominant tool in the
arsenal of practitioners. Instead, human intervention and
on-call rotations play a crucial role in monitoring ML models.
In doing so, practitioners are actively involved in maintaining the
model’s overall and subgroup performances, aiming to optimize
business value and ensure the model’s fairness, respectively [12, 25].
Now, let us first revisit the challenges we pinpointed in the previ-
ous section and consider how the domain expertise of a human-in-
the-loop can address them. Afterwards, we examine the difficulties
that arise when relying on domain expertise for model monitoring.
2.2.1
Addressing Concept Drift with Domain Expertise. To demon-
strate how domain expertise can aid in detecting and mitigating
concept drift, we again consider customer churn prediction and
provide an illustrative example (see Fig. 2). Here, we present three
illustrative instances of multivariate feature drift, along with an
expert’s assessment of the potential underlying event and the pres-
ence of concept drift. Subsequently, we describe an appropriate
response tailored to the nature of the detected drift.
Figure 2: An illustrative case for customer churn prediction,
showing expert assessments for three cases of feature drift.
The events depicted in Fig. 2 emphasize the vital importance
of domain expertise in the process of concept drift detection and
mitigation. While all events were effectively identified through
feature drift detection, they varied significantly in their impact on
model performance and the proper course of action.
2.2.2
Why Is It Hard to Rely on Domain Expertise? In the example
(Fig. 2), we assumed the assessments were conducted by an expert
with a deep understanding of the model and its application context.
In practice, this responsibility often falls to on-call ML engineers,
who oversee the monitoring of ML models [25, 27]. These models
are often developed by different teams and operate in various con-
texts – processes that ML engineers have little to no involvement
in! This proves indeed challenging, as quoted by an ML engineer:
""The pain point is dealing with that alert fatigue and the domain
expertise necessary to know when to take action when on-call"" [25].
In addition to the need for domain expertise, ML engineers have
also expressed a need for centralized model governance [27], as
knowledge about ML models and their application context is often
dispersed and inaccessible [20]. This issue becomes particularly
difficult to manage for organizations that run numerous models,
each with its unique feature set and context. For example, in addi-
tion to churn prediction, organizations might use models for tasks
like demand prediction, product recommendation, and personalized
search. The issue of acquiring and retaining domain expertise are
exacerbated by factors such as staff turnover, limited documenta-
tion, and the need for extensive training [25, 27]. These challenges
highlight the need to consolidate domain expertise and make it
accessible to on-call ML engineers.
Expert-Driven Monitoring of Operational ML Models
Conference’17, July 2017, Washington, DC, USA
3
APPROACH
We propose a method called Expert Monitoring to address opera-
tional challenges. This method integrates domain expertise through
scenario specification. It then makes this expertise accessible to on-
call ML engineers via scenario identification, providing insights into
the potential causes of feature drift upon detection (see Fig. 3).
3.1
Scenario Specification
We consolidate domain expertise through expert knowledge elicita-
tion and retrospective analyses, creating a standardized resource
for integration into our monitoring and response system.
3.1.1
Expert Knowledge Elicitation. Domain expertise is distributed
among multiple experts in an organization, including marketing,
product development, business strategy, and data engineering prac-
titioners. We are interested in prior knowledge of events that can
induce concept drift in the application context of an ML model. ML
engineers collaborate with the domain experts to elicit scenarios of
these events, using traditional requirements engineering methods
such as interviews, focus groups, and observation studies [28].
3.1.2
Retrospective Analysis. In addition to the elicitation process,
ML engineers, either in collaboration with domain experts or in-
dependently, conduct retrospective analyses. By examining the
model’s historical performance and correlating performance drops
with feature drift events (assuming access to labeled data), they can
isolate recurring problematic events for future identification.
3.1.3
Scenario Specification Format. The acquired scenarios are
compiled by the ML engineer and stored in a standardized format.
Below, we provide a description of its components.
ML Model. The name and version of the ML model that is subject
to concept drift in the specified scenario.
Scenario Description. This description provides the context
for the event that can potentially induce concept drift.
Bayesian Model. We utilize Bayesian models to provide experts
an intuitive method for incorporating their prior knowledge of how
the feature distribution(s) would be affected under the specified
scenarios. These models are central to the next phase of our ap-
proach, namely scenario detection, detailed in Section 3.2. They
enable the estimation of the distributions of the relevant feature(s)
as either univariate or multivariate, simultaneously quantifying
the uncertainty in the experts’ subjective beliefs. Specifically, ex-
perts estimate the parameters (e.g. mean or standard deviation of
an affected feature) as 𝜃 = 𝐷𝑖𝑠𝑡(𝑙𝑜𝑐𝑎𝑡𝑖𝑜𝑛,𝑠𝑝𝑟𝑒𝑎𝑑), where the loca-
tion is the estimated parameter value, and the spread indicates the
expert’s uncertainty. A sharp distribution implies high certainty,
while a wide one suggests low certainty. For example, for a highly
certain estimate that the mean customer age in the marketing cam-
paign scenario (Fig. 2) will be eighteen, we can define a normal
distribution with location 18 and spread 1.
In addition to quantifying uncertainty, the representation of do-
main expertise can be flexibly determined in a fine-grained manner.
Experts can: (1) select alternative distributions, such as a uniform
distribution, to assign equal probabilities within specific ranges
(e.g., ages 16 to 20); (2) provide relative estimates, in addition to
absolute ones, in relation to the ML model’s training data (see Fig.
3); (3) define the distribution parameters of affected features for
specific subgroups (e.g., in Fig. 1c, where the distribution of recent
page visits can be estimated specifically for young people).
Scenario Understanding. This includes estimating concept
drift characteristics like severity, transition speed, duration, and
recurrence to inform response selection (see Section 3.2). Addition-
ally, the likelihood of the scenario, whether it is common or rare,
can also be specified based on prior knowledge, using a simple
three-point scale (e.g., low, moderate, high) for consistency.
Scenario Response. An expert can optionally provide this re-
sponse to guide the on-call ML engineer or automate scenario
mitigation upon detection (see Section 3.2).
Figure 3: A visual depiction of our approach, Expert Monitoring. In step (A), ML engineers consolidate domain expertise within
the organization using a standardized format. In step (B), upon detecting feature drift, scenarios are evaluated using Bayesian
model comparison. Afterwards, the ML engineer is informed about potential causes, or an automated response is triggered.
Conference’17, July 2017, Washington, DC, USA
Joran Leest, Claudia Raibulet, Ilias Gerostathopoulos, and Patricia Lago
3.2
Scenario Identification
At runtime, upon detecting feature drift, we infer the occurrence
of a scenario using the Bayesian models defined in the prior step.
3.2.1
Bayesian Model Comparison. Each (Bayesian) scenario model
is treated as a hypothesis and has its posterior probability 𝑃(𝑀|𝐷)
computed to assess its likelihood based on recent observations,
obtained from a sliding window over the data stream. The posterior
probability of a model is computed as follows:
𝑃(𝑀|𝐷) = 𝑃(𝑀)𝑃(𝐷|𝑀)
(1)
Here, 𝑃(𝑀) represents the scenario likelihood of the model, reflect-
ing the expert’s belief about the likelihood of a scenario occurring,
as discussed in Section 3.1.3. Scenario likelihoods, which can be set
to equal by default, are specified on a three-point scale and normal-
ized to sum to one. 𝑃(𝐷|𝑀) denotes the marginal likelihood and is
computed using the following integral:
𝑃(𝐷|𝑀) =
∫
1
· · ·
∫
𝑛
𝑃(𝐷|𝜃, 𝑀)𝑃(𝜃|𝑀) 𝑑𝜃1 · · · 𝑑𝜃𝑛
(2)
This represents an n-dimensional integral over all parameters 𝜃
[8]. However, due to the impracticality of a closed-form solution,
we instead use one of the following approximation methods: (1)
Markov Chain Monte Carlo sampling [4], a computationally inten-
sive method, or (2) calculating each feature’s marginal likelihood
in closed-form, by leveraging the conjugate prior assumption (the
observed data and expert estimates follow the same distribution)
[19], and then multiplying these likelihoods, assuming minimal
inter-feature correlation.
After calculating the scenario models’ posterior probabilities,
we compare them with a reference model built from the observed
parameter values in the ML model’s training data. This comparison
yields the Bayes factor [8], indicating the relative likelihood of each
scenario model compared to the reference model.
In assessing the role of expert estimates in Bayesian model com-
parison for scenario identification, we find that the precision of
these estimates directly influences accuracy (Fig. 4)1. Specifically,
scenarios identified using low-error (small deviation from the true
parameter value) and high-certainty (low standard deviation speci-
fied in the estimate) estimates are typically more accurate. More-
over, even scenarios with higher estimate errors can be correctly
identified if the associated uncertainty is correctly deemed high.
Figure 4: Detection accuracy vs. estimate uncertainty and
error (in proportion relative to actual parameter values) on
simulated scenarios, with a Bayes factor threshold of 5.
1For more information, see https://github.com/JoranLeest/expert_monitoring
3.2.2
In Response to a Scenario. The Bayes factors from Bayesian
model comparison are displayed with scenario identifiers, assisting
ML engineers in response selection for feature drift. Our method
also automates concept drift mitigation by triggering the top-ranked
scenario’s response based on a Bayes factor threshold.
4
DISCUSSION
In further developing our approach, there is an open question
about extracting domain expertise on concept drift-inducing events
through the reuse of requirement [28] and knowledge [24] elicita-
tion methods. A second key question is: What is all the knowledge
that we can incorporate regarding concept drift-inducing events?
We have shown how a Bayesian model can be constructed to repre-
sent a scenario based on feature distributions, providing the base
knowledge required from practitioners to explain feature drift oc-
currences. However, adopting the Bayesian framework offers the
flexibility allows integrating extra knowledge and extending in
various directions. For instance, experts might include scenario
temporal distributions and likelihood at specific times, like higher
sales in then summer. Furthermore, our method allows for updating
expert estimates and facilitates the use of scenario-specific machine
learning models, both of which help mitigate recurring scenarios.
The literature reveals a gap in understanding human-in-the-loop
systems, especially domain expertise, for concept drift challenges.
This area warrants more study to aid practitioners with appropriate
processes, tools, and methodologies. While previous works offer
methods for detecting feature drift-related model failures in prac-
tical settings [7, 10, 22, 23, 26], they overlook the latent issue of
concept drift. More closely related to our research, Chen et al. [3]
and Cobb et al. [5] incorporate domain expertise in performance es-
timation and feature drift detection, but also do not address concept
drift. Our work uniquely integrates domain expertise in identify-
ing and addressing concept drift, contributing to human-centered
model monitoring [27]. We believe that scenario-based methods
like ours, similar to those used in software architecture [15], are
promising for advancing model monitoring.
5
FUTURE PLANS
We contend that the inherent intricacy of the human factors in-
volved in our approach needs to be addressed through rigorous
evaluation and collaboration with practitioners. Specifically, we
identified the following research questions: (1) What domain ex-
pertise of concept drift-inducing events can be elicited, and repre-
sented with sufficient detail? (2) Can scenarios be identified through
Bayesian model comparison with sufficient accuracy? (3) Is the Ex-
pert Monitoring approach perceived as helpful by ML engineers
and does it enable them to improve on business-related metrics?
To answer (1) and (2), we’ll conduct action research via workshops
and focus groups with diverse domain practitioners, refining our
approach based on real-world industrial needs. For (3), we’ll use
surveys and interviews to gauge our approach’s usefulness.
ACKNOWLEDGMENT
This research is supported by ExtremeXP, a project co-funded by
the European Union Horizon Programme under Grant Agreement
No. 101093164.
Expert-Driven Monitoring of Operational ML Models
Conference’17, July 2017, Washington, DC, USA
REFERENCES
[1] Jaehyun Ahn, Junsik Hwang, Doyoung Kim, Hyukgeun Choi, and Shinjin Kang.
2020. A survey on churn analysis in various business domains. IEEE Access 8
(2020), 220816–220839.
[2] Eric Breck, Shanqing Cai, Eric Nielsen, Michael Salib, and D Sculley. 2017. The
ML test score: A rubric for ML production readiness and technical debt reduction.
In 2017 IEEE International Conference on Big Data (Big Data). IEEE, 1123–1132.
[3] Mayee Chen, Karan Goel, Nimit S Sohoni, Fait Poms, Kayvon Fatahalian, and
Christopher Ré. 2021. Mandoline: Model evaluation under distribution shift. In
International conference on machine learning. PMLR, 1617–1629.
[4] Siddhartha Chib. 2001. Markov chain Monte Carlo methods: computation and
inference. Handbook of econometrics 5 (2001), 3569–3649.
[5] Oliver Cobb and Arnaud Van Looveren. 2022. Context-aware drift detection. In
International Conference on Machine Learning. PMLR, 4087–4111.
[6] João Gama, Indr˙e Žliobait˙e, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid
Bouchachia. 2014. A survey on concept drift adaptation. ACM computing surveys
(CSUR) 46, 4 (2014), 1–37.
[7] Saurabh Garg, Sivaraman Balakrishnan, Zachary C Lipton, Behnam Neyshabur,
and Hanie Sedghi. 2022. Leveraging unlabeled data to predict out-of-distribution
performance. arXiv preprint arXiv:2201.04234 (2022).
[8] Jeff Gill. 2014. Bayesian methods: A social and behavioral sciences approach. Vol. 20.
CRC press.
[9] Matthew Groh. 2022. Identifying the Context Shift between Test Benchmarks
and Production Data. arXiv preprint arXiv:2207.01059 (2022).
[10] Devin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell, and Ludwig
Schmidt. 2021. Predicting with confidence on unseen distributions. In Proceedings
of the IEEE/CVF international conference on computer vision. 1134–1144.
[11] Meng Han, Zhiqiang Chen, Muhang Li, Hongxin Wu, and Xilong Zhang. 2022.
A survey of active and passive concept drift handling methods. Computational
Intelligence 38, 4 (2022), 1492–1535.
[12] Chip Huyen. 2022. Designing machine learning systems. "" O’Reilly Media, Inc."".
[13] Dominik Kreuzberger, Niklas Kühl, and Sebastian Hirschl. 2023. Machine learning
operations (mlops): Overview, definition, and architecture. IEEE Access (2023).
[14] Cloudera Fast Forward Labs. 2021. Inferring Concept Drift Without Labeled Data.
(2021).
[15] Joran Leest, Ilias Gerostathopoulos, and Claudia Raibulet. 2023. Evolvability of
Machine Learning-based Systems: An Architectural Design Decision Framework.
In 2023 IEEE 20th International Conference on Software Architecture Companion
(ICSA-C). IEEE, 106–110.
[16] Ziyi Liu, Rakshitha Godahewa, Kasun Bandara, and Christoph Bergmeir. 2023.
Handling Concept Drift in Global Time Series Forecasting.
arXiv preprint
arXiv:2304.01512 (2023).
[17] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. 2019.
Learning under concept drift: A review. IEEE Transactions on Knowledge and Data
Engineering 31, 12 (2019), 2346–2363. https://doi.org/10.1109/TKDE.2018.2876857
[18] Ankur Mallick, Kevin Hsieh, Behnaz Arzani, and Gauri Joshi. 2022. Matchmaker:
Data drift mitigation in machine learning for large-scale systems. Proceedings of
Machine Learning and Systems 4 (2022), 77–94.
[19] Kevin P Murphy. 2007. Conjugate Bayesian analysis of the Gaussian distribution.
def 1, 2𝜎2 (2007), 16.
[20] Felix Neutatz, Binger Chen, Ziawasch Abedjan, and Eugene Wu. 2021. From
Cleaning before ML to Cleaning for ML. IEEE Data Eng. Bull. 44, 1 (2021), 24–41.
[21] José Pombal, André F Cruz, João Bravo, Pedro Saleiro, Mário AT Figueiredo,
and Pedro Bizarro. 2022. Understanding Unfairness in Fraud Detection through
Model and Data Bias Interactions. arXiv preprint arXiv:2207.06273 (2022).
[22] Stephan Rabanser, Stephan Günnemann, and Zachary Lipton. 2019. Failing
loudly: An empirical study of methods for detecting dataset shift. Advances in
Neural Information Processing Systems 32 (2019).
[23] Tegjyot Singh Sethi and Mehmed Kantardzic. 2017. On the reliable detection of
concept drift from streaming unlabeled data. Expert Systems with Applications 82
(2017), 77–99.
[24] Nigel R Shadbolt, Paul R Smart, J Wilson, and S Sharples. 2015. Knowledge
elicitation. Evaluation of human work (2015), 163–200.
[25] Shreya Shankar, Rolando Garcia, Joseph M Hellerstein, and Aditya G
Parameswaran. 2022. Operationalizing machine learning: An interview study.
arXiv preprint arXiv:2209.09125 (2022).
[26] Shreya Shankar and Aditya Parameswaran. 2021. Towards observability for
production machine learning pipelines. arXiv preprint arXiv:2108.13557 (2021).
[27] Murtuza N Shergadwala, Himabindu Lakkaraju, and Krishnaram Kentha-
padi. 2022.
A Human-Centric Take on Model Monitoring.
arXiv preprint
arXiv:2206.02868 (2022).
[28] Ian Sommerville. 2011. Software engineering (ed.). America: Pearson Education
Inc (2011).
[29] Geoffrey I Webb, Roy Hyde, Hong Cao, Hai Long Nguyen, and Francois Petitjean.
2016. Characterizing concept drift. Data Mining and Knowledge Discovery 30, 4
(2016), 964–994.
"
"Memory is essential for various applications like robotics, engineering, graphics, and design. Learned simulators based on graph networks have shown promise in capturing the complex real dynamics. However, scaling these simulators to handle the complexity of real world scenes and handling inputs from perception rather than 3D state information are major challenges. Our method substantially reduces the memory required to run graph-based learned simulators and presents a perceptual interface in the form of editable NeRFs that can convert real-world scenes into structured representation processable by graph network. We show that our memory consumption is less than previous graph-based simulators while retaining their accuracy and simulators learned in synthetic environments can be applied to real-world scenes captured from multiple camera angles. This paves the way for expanding the application of learned simulators to settings where only perceptual information is available at inference time.","Simulating rigid body dynamics is an important but challenging task with broad applications ranging from robotics to graphics to engineering. Analytic rigid body simulators like Bullet, MuJoCo, and Drake are widely used in robotics. However, these simulators often have difficulties in bridging the gap between real-world scenes and their simulations due to challenges in estimating the fine-grained surface structure of objects. Learned simulators have shown the potential to fill this gap by representing rigid body dynamics with graph neural networks, as they can be applied directly to real world object trajectories. However, real-world scenes present major challenges to learned simulators. First, learned simulators usually require access to full state information, which must be inferred from sensor measurements. Second, learned simulators can be memory intensive, especially for intricate, irregular objects. The best-performing graph-based methods often induce graphs that consume vast amounts of memory for complex object geometries or when there are many objects in the scene. Here, we propose a simple, yet surprisingly effective modification (FIGNet*) to the learned, mesh-based FIGNet rigid body simulator that can address these challenges with representing and simulating real world scenes.","Learned simulators attempt to replicate analytical simulators by employing a learned function approximator. They are typically trained using ground truth state information. Some approaches train a perceptual input system jointly with a dynamics model, often assuming access to ground truth state information like object masks. Others learn a perceptual encoder and decoder, and then fix these to train a dynamics model in latent space. Most related to our approach are methods that use neural radiance fields to reconstruct 3D scenes from 2D multi-view scenes to enable simulation.nan","FIGNet* closely follows the method of Face Interaction Graph Networks (FIGNet), which is a graph neural network approach designed for modeling rigid body dynamics. It consists of two types of nodes (mesh nodes and object nodes), and three types of bi-directional edges. Node-node edges enable the propagation of messages locally along an object’s surface. Object-node edges enable instantaneous propagation of collision information from one side of the object to the other, irrespective of the mesh complexity. Face-face edges are used to model the collision dynamics between rigid objects. For rigid body dynamics, the collision edges reason about the local geometry of two objects involved in contact. This information can be directly broadcast to the whole shape using object-node edges. Removing the node-node (surface mesh) edges makes FIGNet* utilize much less memory without hurting the accuracy of the simulation. This is because the collision edges reason about the local geometry of two objects involved in contact, and this information can then be directly broadcast to the whole shape using object-node edges. Afterwards, we describe the procedure used to connect FIGNet* to the real world. We leverage Neural Radiance Fields (NeRFs) as a perceptual front end to (1) extract the meshes required by FIGNet* for simulation and (2) re-render the scene with the transformations predicted by FIGNet*. In this approach, a static NeRF scene is first trained using a collection of images capturing a real-world scene, enabling the extraction of the necessary meshes for FIGNet*. Then, to obtain the rollout trajectory, we derive a set of rigid body transformations, which are used to edit the original NeRF.","We first tested FIGNet* on simulated data on the MOVi-B and MOVi-C Kubric datasets, which consist of scenes involving multiple rigid objects tossed together onto the floor. FIGNet* outperforms FIGNet in memory consumption, runtime, while maintaining accuracy for both datasets. For real data, we show that FIGNet* can be run on views of real scenes collected from multiple cameras, making plausible trajectories despite training in simulation on perfect state information. We used two scenes: our custom-made KITCHEN scene filled with common elements such as fruits and baskets, the GARDEN-outdoor and KITCHEN COUNTER-indoor scenes introduced in (Barron et al., 2022) and the FIGURINES scene introduced in (Kerr et al., 2023).","We introduced a method that substantially reduces the memory required to run graph-based learned simulators. Based on this memory-efficient simulation model, we present a perceptual interface in the form of editable NeRFs that can convert real-world scenes into a structured representation that can be processed by graph network simulator. We show that our method uses substantially less memory than previous graph-based simulators while retaining their accuracy, and that the simulators learned in synthetic environments can be applied to real world scenes captured from multiple camera angles. This paves the way for expanding the application of learned simulators to settings where only perceptual information is available at inference time.",Scaling Face Interaction Graph Networks to Real World Scenes,"Tatiana Lopez-Guevara, Yulia Rubanova, William F. Whitney, Tobias Pfaff, Kimberly Stachenfeld, Kelsey R. Allen","SCALING FACE INTERACTION GRAPH NETWORKS TO
REAL WORLD SCENES
Tatiana Lopez-Guevara, Yulia Rubanova, William F. Whitney, Tobias Pfaff,
Kimberly Stachenfeld, Kelsey R. Allen
Google DeepMind
ABSTRACT
Accurately simulating real world object dynamics is essential for various appli-
cations such as robotics, engineering, graphics, and design. To better capture
complex real dynamics such as contact and friction, learned simulators based
on graph networks have recently shown great promise (Allen et al., 2023; 2022).
However, applying these learned simulators to real scenes comes with two major
challenges: first, scaling learned simulators to handle the complexity of real world
scenes which can involve hundreds of objects each with complicated 3D shapes,
and second, handling inputs from perception rather than 3D state information. Here
we introduce a method which substantially reduces the memory required to run
graph-based learned simulators. Based on this memory-efficient simulation model,
we then present a perceptual interface in the form of editable NeRFs which can
convert real-world scenes into a structured representation that can be processed
by graph network simulator. We show that our method uses substantially less
memory than previous graph-based simulators while retaining their accuracy, and
that the simulators learned in synthetic environments can be applied to real world
scenes captured from multiple camera angles. This paves the way for expanding
the application of learned simulators to settings where only perceptual information
is available at inference time.
1
INTRODUCTION
Simulating rigid body dynamics is an important but challenging task with broad applications ranging
from robotics to graphics to engineering. Widely used analytic rigid body simulators in robotics such
as Bullet (Coumans, 2015), MuJoCo (Todorov et al., 2012), and Drake (Tedrake, 2019) can produce
plausible predicted trajectories in simulation, but system identification is not always sufficient to
bridge the gap between real world scenes and these simulators (Wieber et al., 2016; Stewart & Trinkle,
1996; Fazeli et al., 2017; Lan et al., 2022; Parmar et al., 2021; Guevara et al., 2017). This is due,
in part, to the challenges of estimating fine-grained surface structures of objects which often have
large impacts on their associated dynamics (Bauza & Rodriguez, 2017). This fundamental issue
contributes to the well-documented sim-to-real gap between outcomes from analytical solvers and
real-world experiments.
Learned simulators have shown the potential to fill the sim-to-real gap (Allen et al., 2023; 2022) by
representing rigid body dynamics with graph neural networks. These fully learned simulators can be
applied directly to real world object trajectories, and do not assume any analytical form for rigid body
contacts. As a result, they can learn to be more accurate than system identification with an analytic
simulator even with reasonably few real world trajectories.
However, real world scenes present major challenges for learned simulators. First, learned simulators
generally assume access to full state information (the positions, rotations, and exact shapes of all
objects) in order to simulate a trajectory. This information must be inferred from a collection of
sensor measurements. Second, learned simulators can be memory intensive, especially for the
kinds of intricate, irregular objects that often comprise real-world scenes. The currently best-
performing graph-based methods operate on explicit surface representations, i.e. point clouds or
triangulated meshes (Pfaff et al., 2021). The induced graphs of these methods tend to consume vast
amounts of GPU memory for complex object geometries, or when there are many objects in the
1
arXiv:2401.11985v1  [cs.LG]  22 Jan 2024
scene. Consequently, results are generally shown for scenes containing fewer than 10 objects with
reasonably simple object geometries.
Here we propose a simple, yet surprisingly effective modification (FIGNet*) to the learned, mesh-
based FIGNet rigid body simulator (Allen et al., 2023) that can address these challenges with
representing and simulating real world scenes:
• FIGNet* consumes much less memory, while maintaining translation and rotation rollout
accuracy. This allows us to train FIGNet* on datasets with more objects with complex
geometries such as Kubric MOVi-C, which FIGNet cannot train on due to memory cost.
• We connect a NeRF perceptual front-end (Barron et al., 2022) to FIGNet*, and show that
we can simulate plausible trajectories for complex, never-before-seen objects in real world
scenes.
• We show that despite training FIGNet* on simulated rigid body dynamics with ground-truth
meshes, the model is robust to noisy mesh estimates obtained from real-world NeRF data.
2
RELATED WORK
Learned simulators
attempt to replicate analytical simulators by employing a learned function
approximator. Typically, they are trained using ground truth state information, and consequently
cannot be directly applied to visual input data. The representation of state varies depending on the
method, but can range from point clouds (Li et al., 2019; Sanchez-Gonzalez et al., 2020; Mrowca
et al., 2018; Linkerh¨agner et al., 2023), to meshes (Pfaff et al., 2021; Allen et al., 2023), to signed
distance functions (SDFs) (Le Cleac’h et al., 2023). Subsequently, learned function approximators
such as multi-layer perceptrons (MLPs) (Li et al., 2021), graph neural networks (GNNs) (Battaglia
et al., 2018; Sanchez-Gonzalez et al., 2018), or continuous convolutional kernels (Ummenhofer et al.,
2019) can be employed to model the temporal evolution of the state. Our approach follows the
mesh-based state representation options, but aims to provide a more efficient graph neural network
dynamics model.
Bridging simulators to perception.
Multiple approaches aim to bridge these learned simulators to
perceptual data. Some approaches are “end-to-end” – they train a perceptual input system jointly with
a dynamics model, often assuming access to ground truth state information like object masks (Janner
et al., 2019; Driess et al., 2022; Shi et al., 2022; Xue et al., 2023; Whitney et al., 2023). Others first
learn a perceptual encoder and decoder, and then fix these to train a dynamics model in latent space
(Li et al., 2021).
Most related to our approach are methods that use neural radiance fields to reconstruct 3D scenes from
2D multi-view scenes to enable simulation. Some of these assume hand-crafted but differentiable
dynamics models (Qiao et al., 2023; 2022; Mengyu et al., 2022), while others learn the dynamics
model separately from state information Guan et al. (2022). We similarly aim to simply apply our
pre-trained learned simulators to real scenes by using a NeRF perceptual front-end. We show that this
approach can work without fine-tuning even when simulators are trained only from synthetic data.
3
METHOD
3.1
FIGNET*
FIGNet* closely follows the method of Face Interaction Graph Networks (FIGNet) (Allen et al.,
2023) which is a graph neural network approach designed for modeling rigid body dynamics.
In FIGNet, each object is represented as a triangulated mesh M made of triangular mesh faces
{FM} with mesh vertices {VM}. A scene graph G then consists of O objects, each with their
own triangulated meshes Mo. At any given time t, M t
o can be represented using the object’s
transformation matrix, M t
o = Rt
o × Mo. A simulation trajectory is represented as a sequence of
scene graphs G = (Gt0, Gt1, Gt2, . . . ) constructed from these meshes. FIGNet is then a simulator S
parameterized by neural network weights Θ, trained to predict the next state of the physical system
˜Gt+1 based on the previous two scene graphs {Gt, Gt−1}, ie Gt+1 = SΘ(Gt, Gt−1). We train with
2
a mean-squared-error loss on the predicted positions of the vertices for each object {VM}. During
inference, SΘ can be recursively applied to yield a rollout of any length T.
object-node
node-node
face-face
FIGNet* Connectivity
✓
✓
✗
vm
vo
vm
vm
vo
vm
fs
fr
fs
fr
Figure 1: Architectural changes:
FIGNet* with respect to FIGNet.
FIGNet consists of two types of nodes (mesh nodes {VM} and
object nodes {VO}), and three types of bi-directional edges.
The mesh nodes {VM} have input features vM,features
i
= [xt
i −
xt−1
i
, pi, ai, f t
i ], where xt
i is the position of the node at time
t, pi are static object properties like density and friction, ai
is a binary “static” feature that indicates whether the node is
subject to dynamics (e.g. the moving objects), or its position
is set externally (e.g. the floor), and f t
i = ki(xt+1
i
− xt
i) is a
feature that indicates how much kinematic nodes are going to
move at the next time step. Object nodes {VO} use the same
feature description, with their positions xt
i being the object’s
center of mass.
The three types of bi-directional edges include node-node,
object-node, and face-face edges. Node-node edges vm → vm
connect surface mesh nodes on a single object to one another.
Object-node edges vo → vm connect object nodes vo to each
mesh vertex vm of that object. Face-face edges connect faces
on one sender object fs to another receiver object fr. See Figure 1.
Conceptually, the node-node edges enable the propagation of messages locally along an object’s
surface. However, in the case of rigid body collisions, collision information needs to be propagated
instantaneously from one side of the object to the other, irrespective of the mesh complexity. Object-
node edges enable this by having a single virtual object node vo at the center of each object which
has bidirectional edges to each mesh node vm on the object’s surface. Finally, to model the collision
dynamics between rigid objects, face-face edges convey information about face interactions between
objects. FIGNet proposes a special hypergraph architecture for how to incorporate face-face edges
into an Encode-Process-Decode graph network architecture. We defer further details of the FIGNet
approach to (Allen et al., 2023).
This approach works remarkably well for rigid body shapes but becomes intractably expensive as
the complexity of each object mesh grows, since this will add a significant number of node-node
(surface mesh) edges. Empirically, node-node edges often account for more than 50% of the total
edges in FIGNet. FIGNet* makes a simple modification to FIGNet which removes the node-node
(surface mesh) edges, keeping everything else identical. Surprisingly, this does not hurt the accuracy
of FIGNet*, but dramatically improves memory and runtime performance for the rigid body settings
examined in this paper. This works for rigid body dynamics because the collision edges reason
about the local geometry of two objects involved in contact, and this information can then be directly
broadcasted to the whole shape using object-node edges.
This simple change to FIGNet unlocks the ability to train on much more complex scenes than was
previously possible, as larger scenes fit into accelerator memory during training. We can therefore
run FIGNet* on meshes extracted from real-world scenes, as well as simulations with more complex
object geometries than previously possible.
3.2
CONNECTING FIGNET* TO PERCEPTION
In this section we describe the procedure used to connect FIGNet* to the real world. We leverage
Neural Radiance Fields (NeRFs) (Mildenhall et al., 2021; Barron et al., 2022) as a perceptual front
end to (1) extract the meshes required by FIGNet* for simulation and (2) re-render the scene with the
transformations predicted by FIGNet* (Figure 2). This approach shares similarities with the method
presented in (Qiao et al., 2023), however, here we demonstrate its implementation using a learned
simulator.
3
Gt : t-h
Gt
F
NeRF
Θ
S
PERCEPTION
DYNAMICS
G t+1
(x, d)
(c, σ)
Φ
FIGNet*
Figure 2: Perception Pipeline. We demonstrate a two-way coupling approach, integrating FIGNet*
with real-world scenes through NeRF. Initially, a static NeRF scene is trained using a collection of
images capturing a real-world scene, enabling the extraction of the necessary meshes for FIGNet*.
Upon obtaining the rollout trajectory, we derive a set of rigid body transformations, which are then
utilized to edit the original NeRF. See subsection 3.2 for details.
3.2.1
FROM NERF TO FIGNET*
Learning a Neural Radiance Field:
We first learn a NeRF from W sparse input views {I}W
1 and
their associated camera intrinsics K and extrinsics. This representation models a view-dependent
appearance function FΦ that maps a 3D location x = (x, y, z) and a viewing direction d to a radiance
color c and a density σ.
FΦ : (x, d) → (c, σ)
(1)
The geometries of all the objects in a scene represented by a NeRF are implicitly captured by FΦ. We
only care about the density σ for the geometry and can ignore the color c and the viewing direction
d. We slightly abuse the notation and define F σ
Φ(x) → σ to denote the subpart of the NeRF that
evaluates the density only.
Mesh Extraction:
To extract the mesh of an individual object from the implicit function F σ
Φ, we
first need to define a volumetric boundary of the object.
We begin by generating N binary segmentation masks, each capturing the object’s shape from one
of N distinct viewpoints. Each mask is created by calling XMEM (Cheng & Schwing, 2022) with
the corresponding RGB image and a point prompt located at the center of the object. XMEM then
identifies and labels all active pixels belonging to the object in each mask at the prompted location,
resulting in a set of N segmentation masks {mn}N
1 that capture the object’s shape from various
perspectives. Empirically, we found that for simple objects like spheres, as few as two views from
different angles are sufficient to accurately segment the object. However, one can use additional
views for increased robustness or to capture finer details, particularly for more complex shapes.
We use the same procedure as described in (Cen et al., 2023) to unproject the pixels of the 2D masks
into 3D points by leveraging the estimated depth z(mn) from the NeRF and the known camera
intrinsics from which each mask was generated:
xmn = z(mn) ∗ K−1 · (x(mn), y(mn), 1)T
(2)
The volumetric boundary Vo ∈ R2×3 can be then obtained as
Vo = {min(xmn), max(xmn)}N
1
(3)
To extract the mesh of the object Mo within the volume Vo, we employ the Marching Cubes algorithm
(m cubes) (Lorensen & Cline, 1998). This algorithm uses samples of the density field from a regular
grid of J points inside the boundary xj ∈ Vo as σo = {F σ
Φ(xj)}J
1 and a threshold value σthrs. To
manage the potentially high number of vertices and faces in the generated mesh, we perform an
4
additional decimation step (decimate). We employ the Quadric Error Metric Decimation method
by Garland and Heckbert (Garland & Heckbert, 1997). This technique preserves the primary features
of the mesh while allowing us to control the final mesh complexity through a user-specified target
number of faces nf.
Mo = decimate( m cubes(σo, σthrs), nf)
(4)
Building the Graph
To specify the object whose motion we want to simulate, we define the mesh
Mo as the active object in the graph, with all other objects considered static. We then repeat the same
mesh extraction procedure described above on an offset version of the scene volume (Vo − ∆xVo)
to obtain the passive mesh Mpassive representing the static environment with ai set to True. Both
meshes are used to construct the initial graph Gt for FIGNet and FIGNet*. We do not infer static
properties like mass, friction, elasticity, etc for meshes extracted from the scene. Instead we use the
default parameters provided in Table 3. Future work will be needed to infer these properties from
object dynamics.
We generate the history Gt−1 using the same mesh but shifted downwards by a ∆z amount twice to
simulate an object being dropped vertically.
3.2.2
FROM FIGNET* TO NERF
We obtain a rollout trajectory by iteratively applying FIGNet* over T time steps. Starting from the
initial graph and its history to obtain (Gt+1, Gt+2, · · · , Gt+T ). This can be equivalently seen as a
sequence of rigid transformations (Rt+1
o
, Rt+2
o
, · · · , Rt+T
o
) that are applied to Mo.
Given the bounding volume of each object Vo and a rigid transformation Rt at time t, we can reuse
the static NeRF function FΦ to render the rollout by editing the original static NeRF described
by FΦ via ray bending (Jambon et al., 2023). We restrict the bending of the ray b to be the rigid
transformation returned by FIGNet* as
ˆ
FΦ : ( b(x, Rt
o ), d) → (c, σ),
(5)
where b(x, Rt
o ) can be either
bmove(x, Rt
o) =



Rt
o × x
if x ∈ Vo,
(Rt
o)−1 × x
if x ∈ Rt
o × Vo,
x
otherwise.
(6)
or
bduplicate(x, Rt
o) =
(Rt
o)−1 × x
if x ∈ Rt
o × Vo,
x
otherwise.
(7)
meaning that the active object has the option to be either moved or copy-pasted during the rollout.
We then generate the final sequence of rollout images from a chosen viewpoint ˆd across all time steps.
This involves applying NeRF’s classic volume rendering pipeline with the transformed radiance
field ˆ
FΦ incorporating object movement. At each step, we adjust the radiance field based on the
applied rigid transformation, effectively capturing the dynamic appearance of the object throughout
the rollout sequence { ˆ
FΦ(b(x, Rt ), ˆd)}k
t=1.
4
RESULTS
We test FIGNet* on both simulated and real data. In simulation, we show that FIGNet* outperforms
FIGNet in memory consumption and runtime while maintaining accuracy for a standard rigid body
dynamics benchmark (Greff et al., 2022). For real data, we show that FIGNet* can be run on views
of real scenes collected from multiple cameras, making plausible trajectories despite training in
simulation on perfect state information.
5
Ground
truth
FIGNet*
Figure 3: Qualitative results for simulation. FIGNet* rollout for complex MOVi-C simulation
which could not be represented in memory for FIGNet.
4.1
SIMULATION
For our simulation results, we use the MOVi-B and MOVi-C Kubric datasets (Greff et al., 2022). In
both setups, multiple rigid objects are tossed together onto the floor using the PyBullet (?) simulator
to predict trajectories. MOVi-B consists of scenes involving 3-10 objects selected from 11 different
shapes being tossed. The shapes include teapots, gears, and torus knots, with a few hundred up to just
over one thousand vertices per object. MOVi-C consists of scenes involving 3-10 objects selected
from 1030 different shapes taken from the Google Scanned Objects dataset (Downs et al., 2022).
MOVi-C shapes tend to be more complex than MOVi-B shapes, and have up to several thousand or
tens of thousands of vertices.
We report four metrics in Table 2: peak memory consumption, runtime per simulation step, translation
error, and rotation error. Translation and rotation root-mean-squared error (RMSE) are calculated
with respect to the ground truth state after 50 rollout steps.
Table 1: Comparison metrics for FIGNet and FIGNet* on Kubric MOVi-B and MOVi-C
Dataset
Model
Memory (MiB)
Runtime (ms)
Trans. Err. (m)
Rot. Err. (deg)
Edge Count (#)
MOVi-B
FIGNet
63.38 ± 3.32
26.38 ± 0.73
0.14 ± 0.01
14.99 ± 0.67
24514 ± 906
FIGNet*
50.08 ± 3.37
19.41 ± 0.24
0.13 ± 0.01
15.96 ± 0.87
8630 ± 714
MOVi-C
FIGNet
OOM
–
–
–
–
FIGNet*
71.79 ± 6.39
20.42 ± 0.64
0.18 ± 0.01
19.82 ± 0.64
11401 ± 975
For MOVi-B, FIGNet* matches FIGNet’s performance in translation and rotation error, performing
slightly better in translation, and slightly worse on rotation. However, FIGNet* uses significantly
less memory than FIGNet while also having a 20% faster runtime. These differences in memory
consumption and runtime allow us to train FIGNet* on the much more complex MOVi-C dataset
(example trajectory in Figure 3), which causes OOM errors when attempting to train FIGNet even
with 16 A100 GPUs. On MOVi-C, the memory consumption is higher, but runtime remains almost as
fast. Similarly, since MOVi-C is more complex than MOVi-B, the translation and rotation errors for
FIGNet* are higher, but not significantly so.
Overall, this suggests that FIGNet* is a viable alternative to FIGNet. It maintains accuracy while
significantly reducing memory consumption and runtime, allowing us to train FIGNet* on more
complex datasets than can be fit into FIGNet memory.
4.2
REAL WORLD
We present our results on linking FIGNet* with real-world scene inputs. Note that this is a proof-of-
concept only, that is we do not compare to real ground truth dynamics, instead leaving that for future
6
work. For comparisons between FIGNet and FIGNet* on real data, FIGNet models were trained in
simulation on Kubric MOVi-B, while FIGNet* models were trained in simulation on Kubric MOVi-C.
For our real-world results, we used two scenes: our custom-made KITCHEN scene filled with common
elements such as fruits and baskets (See Appendix C for details), the GARDEN-outdoor and KITCHEN
COUNTER-indoor scenes introduced in (Barron et al., 2022) and the FIGURINES scene introduced in
(Kerr et al., 2023). These scenes consist of 360-degree image sets captured with different cameras.
We used a MipNerf360 (Barron et al., 2022) implementation for the NeRF front end.
t
KITCHEN
GARDEN
FIGURINES
K. COUNTER
Figure 4: Qualitative results for real world scenes. Left: Initial NeRF rendering of the static
real-world scene. The desired active object is outlined in red, with a red arrow indicating its intended
starting position. Right: FIGNet* rollouts simulating the object’s motion for k = 30 time steps
(rendered from a different viewpoint) after being dropped from the initial position. The complete
trajectory is traced in yellow. Here we used bduplicate as the ray bending function meaning the active
object is copy pasted into the starting position at the beginning of the rollout (See the website for
videos and Appendix B for details on the mesh extraction procedure described in subsection 3.2).
Qualitative Results.
We show qualitative FIGNet* rollouts on both real world scenes using the
full pipeline described in subsection 3.2. For all the scenes, we manually selected 2 views of the
active object (highlighted in the red boxes) to compute the bounding volume Vo and the subsequent
mesh Mo (See Appendix B). By creating a history based on downward vertical displacement of
the chosen mesh, we are effectively simulating a motion similar to dropping. Figure 4 illustrates
the bouncing behaviors of various objects falling onto other objects. Note the sharp rotation of the
orange at the end of the bounce (last frame) in the KITCHEN scene, and how rendering with the
7
transformed ˆ
FΦ works when the orange is flipped upside down. We can observe similar results for the
FIGURINES scene, where we selected two views of the dog figurine with long thing legs and simulate
a dropping motion onto a duck. Our perception pipeline can realistically simulate and re-render the
dropping motion of objects captured within these real scenes by reusing the static NeRF scene with
the FIGNet* transformations 1.
FIGNet*
20K FACES
40K FACES
FIGNet
OOM
nf
nf
Decimating faces from [494234] to [20000]
Decimating faces from [940579] to [20000]
Morange
Mpassive
10K FACES
nf
1K FACES
nf
Figure 5: FIGNet and FIGNet* comparison for different levels of decimation: High-quality meshes
lead to out-of-memory issues on FIGNet, while lower resolutions result in implausible trajectories
(e.g., orange penetrating the basket). Notably, FIGNet*’s performance gracefully degrades with mesh
quality, indicating enhanced robustness and memory efficiency. The gray mesh depicts the passive
object, and the colored mesh corresponds to the active object.
Effect of decimation.
The marching cube algorithm often results in oversampled meshes charac-
terized by an elevated node count. While the implementation of a controllable parameter for mesh
decimation (nf) is an effective strategy to address this challenge, it is important to note that the extent
of decimation can adversely affect the quality of simulations, especially in cases involving complex
geometries. The advantage of using FIGNet* lies in its reduced memory requirements, which permits
a less rigorous decimation process in comparison to FIGNet. To demonstrate this, we simulated a
1See https://sites.google.com/view/fignetstar/ for videos.
8
scene with two distinct levels of decimation (Figure 5). This experiment highlights instances where
FIGNet’s memory capacity is exceeded, showcasing the benefits of FIGNet* in such scenarios.
Effect of perception noise.
Real-world meshes extracted from pipelines like NeRF, primarily
optimized for rendering quality, often exhibit noise and imperfections (Figure 6). Unlike the clean
training data used for FIGNet* and FIGNet, these meshes are far from ideal. Nevertheless, both
models can successfully handle rollouts even with such challenging real-world data.
5
DISCUSSION
We showed that a surprisingly simple modification to FIGNet, the removal of the surface mesh
edges, allowed us to create a model with low enough memory consumption to support training on
unprecedentedly complex scenes. This unlocked the ability to interface FIGNet* with real world
scenes by using a combination of Neural Radiance Fields (NeRFs) and object selection (XMem) to
convert real scenes into object-based mesh representations. In combination with volumetric NeRF
editing, this allowed us to simulate videos of alternative physical futures for real scenes.
We believe that this explicitly 3D approach to video editing and generation has significant promise
for robotics and graphics applications. It allows a model to be pre-trained from simulation data, while
still generalizing to real scenes. FIGNet* generalizes surprisingly well to noisy meshes extracted from
NeRFs, especially considering that it was trained in simulation with nearly perfect state information
(positions, rotations, and shapes of objects). We imagine that this approach could further support
future applications including “virtualization” of real scenes, where users may be interested in editing
those scenes and simulating possible future outcomes.
There are many exciting directions for future work with FIGNet*. In particular, while fine-tuning
a pre-trained FIGNet* model to a real video was outside the scope of this paper, we believe this
is a natural next step. Since FIGNet* is entirely composed of neural networks, fine-tuning from
real world dynamics directly into the weights of FIGNet* could be a viable alternative to system
identification for robotics. Future work will be needed to determine the details of how to perform
fine-tuning in a data efficient manner.
REFERENCES
Kelsey R Allen, Tatiana Lopez Guevara, Yulia Rubanova, Kim Stachenfeld, Alvaro Sanchez-Gonzalez,
Peter Battaglia, and Tobias Pfaff. Graph network simulators can learn discontinuous, rigid contact
dynamics. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.
net/forum?id=rbIzq-I84i_.
Kelsey R. Allen, Yulia Rubanova, Tatiana Lopez-Guevara, William Whitney, Alvaro Sanchez-
Gonzalez, Peter Battaglia, and Tobias Pfaff. Learning rigid dynamics with face interaction graph
networks. In International Conference on Learning Representations, 2023.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf
360: Unbounded anti-aliased neural radiance fields. CVPR, 2022.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018.
Maria Bauza and Alberto Rodriguez. A probabilistic data-driven model for planar pushing. In IEEE
International Conference on Robotics and Automation (ICRA), pp. 3008–3015, 2017.
Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie, Dongsheng Jiang,
Xiaopeng Zhang, and Qi Tian. Segment anything in 3d with nerfs. In NeurIPS, 2023.
9
Ho Kei Cheng and Alexander G. Schwing. XMem: Long-term video object segmentation with an
atkinson-shiffrin memory model. In ECCV, 2022.
Erwin Coumans. Bullet physics simulation. In ACM SIGGRAPH 2015 Courses, pp. 7, 2015.
Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann,
Thomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of
3d scanned household items. arXiv preprint arXiv:2204.11918, 2022.
Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake, and Marc Toussaint. Learning multi-object
dynamics with compositional neural radiance fields. arXiv preprint arXiv:2202.11855, 2022.
Nima Fazeli, Elliott Donlon, Evan Drumwright, and Alberto Rodriguez. Empirical evaluation of
common contact models for planar impact. In 2017 IEEE international conference on robotics and
automation (ICRA), pp. 3418–3425. IEEE, 2017.
Michael Garland and Paul S Heckbert. Surface simplification using quadric error metrics. In
Proceedings of the 24th annual conference on Computer graphics and interactive techniques, pp.
209–216, 1997.
Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J
Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al.
Kubric: A scalable
dataset generator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 3749–3761, 2022.
Shanyan Guan, Huayu Deng, Yunbo Wang, and Xiaokang Yang. Neurofluid: Fluid dynamics
grounding with particle-driven neural radiance fields, 2022.
Tatiana L´opez Guevara, Nicholas Kenelm Taylor, Michael Gutmann, Subramanian Ramamoorthy,
and Kartic Subr. Adaptable pouring: Teaching robots not to spill using fast but approximate fluid
simulation. In 1st Conference on Robot Learning 2017, pp. 77–86, 2017.
Cl´ement Jambon, Bernhard Kerbl, Georgios Kopanas, Stavros Diolatzis, George Drettakis, and
Thomas Leimk¨uhler. Nerfshop: Interactive editing of neural radiance fields. Proceedings of the
ACM on Computer Graphics and Interactive Techniques, 6(1), 2023.
Michael Janner, Sergey Levine, William T. Freeman, Joshua B. Tenenbaum, Chelsea Finn, and Jiajun
Wu. Reasoning about physical interactions with object-oriented prediction and planning, 2019.
Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language
embedded radiance fields. In International Conference on Computer Vision (ICCV), 2023.
Lei Lan, Danny M Kaufman, Minchen Li, Chenfanfu Jiang, and Yin Yang. Affine body dynamics:
Fast, stable & intersection-free simulation of stiff materials. ACM Trans. Graph, 2022.
Simon Le Cleac’h, Hong-Xing Yu, Michelle Guo, Taylor Howell, Ruohan Gao, Jiajun Wu, Zachary
Manchester, and Mac Schwager. Differentiable physics simulation of dynamics-augmented neural
objects. IEEE Robotics and Automation Letters, 8(5):2780–2787, 2023.
Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum, and Antonio Torralba. Learning
particle dynamics for manipulating rigid bodies, deformable objects, and fluids. In International
Conference on Learning Representations, 2019.
Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, and Antonio Torralba. 3d neural scene
representations for visuomotor control. arXiv preprint arXiv:2107.04004, 2021.
Jonas Linkerh¨agner, Niklas Freymuth, Paul Maria Scheikl, Franziska Mathis-Ullrich, and Gerhard
Neumann. Grounding graph network simulators using physical sensor observations. arXiv preprint
arXiv:2302.11864, 2023.
William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction
algorithm. In Seminal graphics: pioneering efforts that shaped the field, pp. 347–353. 1998.
10
Chu Mengyu, Liu Lingjie, Zheng Quan, Franz Erik, Seidel Hans-Peter, Theobalt Christian, and
Zayer Rhaleb. Physics informed neural fields for smoke reconstruction with sparse data. ACM
Transactions on Graphics, 41(4):119:1–119:14, aug 2022.
Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications
of the ACM, 65(1):99–106, 2021.
Damian Mrowca, Chengxu Zhuang, Elias Wang, Nick Haber, Li F Fei-Fei, Josh Tenenbaum, and
Daniel L Yamins. Flexible neural representation for physics prediction. Advances in neural
information processing systems, 31, 2018.
Mihir Parmar, Mathew Halm, and Michael Posa. Fundamental challenges in deep learning for stiff
contact dynamics. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pp. 5181–5188. IEEE, 2021.
Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learning mesh-based
simulation with graph networks. In International Conference on Learning Representations, 2021.
Yi-Ling Qiao, Alexander Gao, and Ming C. Lin. Neuphysics: Editable neural geometry and physics
from monocular videos. In Conference on Neural Information Processing Systems (NeurIPS),
2022.
Yi-Ling Qiao, Alexander Gao, Yiran Xu, Yue Feng, Jia-Bin Huang, and Ming C. Lin. Dynamic
mesh-aware radiance fields. ICCV, 2023.
Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller,
Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for inference and
control. In International Conference on Machine Learning, pp. 4470–4479. PMLR, 2018.
Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter
Battaglia. Learning to simulate complex physics with graph networks. In International Conference
on Machine Learning, pp. 8459–8468. PMLR, 2020.
Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pp. 4104–4113, 2016.
Haochen Shi, Huazhe Xu, Zhiao Huang, Yunzhu Li, and Jiajun Wu. Robocraft: Learning to see,
simulate, and shape elasto-plastic objects with graph networks, 2022.
D Stewart and JC J.C. Trinkle. An implicit time-stepping scheme for rigid body dynamics with
Coulomb friction. International Journal for Numerical Methods in Engineering, 39(15):2673–2691,
1996.
Russ Tedrake. Drake: Model-based design and verification for robotics, 2019. URL https:
//drake.mit.edu.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026–5033.
IEEE, 2012.
Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and Vladlen Koltun. Lagrangian fluid simulation
with continuous convolutions. In International Conference on Learning Representations, 2019.
William F. Whitney, Tatiana Lopez-Guevara, Tobias Pfaff, Yulia Rubanova, Thomas Kipf, Kimberly
Stachenfeld, and Kelsey R. Allen. Learning 3d particle-based simulators from rgb-d videos, 2023.
Pierre-Brice Wieber, Russ Tedrake, and Scott Kuindersma. Modeling and control of legged robots.
In Springer handbook of robotics, pp. 1203–1234. Springer, 2016.
Haotian Xue, Antonio Torralba, Joshua B. Tenenbaum, Daniel LK Yamins, Yunzhu Li, and Hsiao-
Yu Tung. 3d-intphys: Towards more generalized 3d-grounded visual intuitive physics under
challenging scenes, 2023.
11
Appendix
A
DECIMATION EXPERIMENTS
We qualitatively evaluated the impact of mesh decimation on rollouts for FIGNet and FIGNet* in the
KITCHEN scene (Figure 5). With higher quality meshes (lower decimation), FIGNet tends to run out
of memory, whereas lower quality meshes (higher decimation) often result in unrealistic rollouts. In
such cases, objects (orange) may pass through solid objects (basket), as observed with meshes of 1k
faces. In contrast, FIGNet*’s rollout trajectories exhibit a graceful degradation with increased levels
of decimation, maintaining relative stability even at very high decimation levels (nf = 1000, which
means approximately 1% of the original faces are preserved)
Side-view
Bottom-view
Side-view
EXTRACTED MESHES (MARCHING CUBES)
FROM PLATES
FROM ORANGE
Figure 6: Noisy meshes extracted from NeRF, including the orange object on the left missing its
bottom face (from Figure 4) and the plates (from Figure 11). Notably, both FIGNet and FIGNet* can
handle rollouts even with such mesh imperfections, demonstrating their robustness to real-world data
challenges.
40K FACES
NONE
nf
20K FACES
1K FACES
Mpassive
nf
nf
nf
Figure 7: Effect of the decimation parameter on the mesh quality. Left: no decimation. Right: high
decimation.
B
IMAGE SEGMENTATIONS
We provide some examples of how the mesh extraction procedure described in subsection 3.2 works
in Figure 8 and Figure 9.
C
KITCHEN SCENE DETAILS
We collected 1027 images of a KITCHEN scene that included different elements such as apples,
oranges, baskets and plates. We extracted the images from a video recorded with an iPhone 14 Pro at
60fps and HEVC format (Figure 10). We used COLMAP (Schonberger & Frahm, 2016) to estimate
the camera poses from the images.
12
2D MASKS OF OBJECT FROM DIFFERENT VIEWS
m1
m2
Morange
KITCHEN - PLATES
m1
m2
m3
MESHES
Mplates
KITCHEN - ORANGE
FIGURINES - DOG
m1
m2
Mdog
Figure 8: Left: Selected views to generate the objects masks for the FIGURINES and KITCHEN scenes.
The top row corresponds to the rendered image in RGB with each orange mask {mn}N
1 (overlaid in
light orange) obtained by XMEM’s (Cheng & Schwing, 2022). The bottom row illustrates the same
procedure for the plates on the same scene. Note that partial segmentations from different views can
also be used to build the volumetric boundary of the object. Right: the obtained mesh Mo from each
of the masks after decimation.
DEPTH MASKS FROM DIFFERENT VIEWS
m1
m2
VOLUMETRIC BOX
Vo
z(m1)
z(m2)
xm1
xm2
Figure 9: Visualizing the generation of the orange’s volumetric box from depth masks in the KITCHEN
scene.
D
IMPLEMENTATION DETAILS
D.1
HYPER-PARAMETERS
FIGNet* is trained identically to FIGNet (Allen et al., 2023).
MLPs for Encoder, Processor, Decoder
We use MLPs with 2 hidden layers, and 128 hidden and
output sizes (except the decoder MLP, with an output size of 3). All MLPs, except for those in the
decoder, are followed by a LayerNorm(Ba et al., 2016) layer.
13
Figure 10: Example frames from the KITCHEN scene video.
Optimization
All models are trained to 1M steps with a batch size of 128 across 8 TPU devices.
We use Adam optimizer, and an an exponential learning rate decay from 1e-3 to 1e-4.
Table 2: NeRF Training Parameters
Type
Parameter
Value
General
near
0.
General
far
1e6
General
lr delay steps
100
General
batch size
65536
General
lr init
1e-2
General
lr final
1e-3
General
adam beta1
0.9
General
adam beta2
0.99
General
adam eps
1e-15
General
cast rays in eval step
True
General
cast rays in train step
True
General
num glo features
4
Model
sampling strategy
((0, 0, 64), (0, 0, 64), (1, 1, 32))
Model
grid params per level
(1, 4)
Hash
hash map size
2097152
Hash
scale supersample
1.
Hash
max grid size
8192
MLP
net depth
1
MLP
net width
64
MLP
disable density normals
True
MLP
density activation
@math.safe exp
MLP
bottleneck width
15
MLP
net depth viewdirs
2
MLP
net width viewdirs
64
Table 3: Default Physical Parameters
Model
Type
Mass
Friction
Restitution
FIGNet*
Active
1e-3
0.5
0.5
FIGNet*
Passive
0
0.5
0.3
FIGNet
Active
1.0
0.8
0.7
FIGNet
Passive
0
0.5
0.3
E
ADDITIONAL ROLLOUTS FOR THE REAL WORLD SCENES
We provide additional rollout examples for the KITCHEN scene in Figure 11 and in the website.
PLATES-FLOOR: Duplicating the stack of plates on the right and shifting their initial position to
14
the left. ORANGE-BASKET: The top orange from the stack of fruits is duplicated and dropped on
top of a basket of oranges. Note the correct depth ordering of the orange with respect to the basket.
ORANGE-TABLE: the orange is dropped on the table.
t
PLATES-FLOOR
ORANGE-BASKET
KITCHEN
ORANGE-TABLE
Figure 11: Additional examples of FIGNet* rollouts on the KITCHEN scene. The final row was
generated using the bmove ray bending function (moving the orange from the fruit tower to the starting
position), while the other rows used bduplicate (copy-pasting the object).
F
EXAMPLE ROLLOUTS FOR MOVI-C
Additional simulation rollouts of FIGNet* on Kubric MOVI-C.
15
Ground
truth
FIGNet*
Ground
truth
FIGNet*
Ground
truth
FIGNet*
Figure 12: Rollout of FIGNet* Kubric MOVi-C.
16
"
