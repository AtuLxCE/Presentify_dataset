abstract,introduction,literature review,methodology,results,conclusion,title,author,textdata
"Cloud-based speech recognition systems pose privacy risks, as speech is commonly input via mobile devices. XXX, a novel system optimized for such devices, selectively masks ephemeral aspects of speech to preserve the quality of long-term speech understanding while safeguarding user privacy. A differential mask generator, grounded in interpretable learning, fine-tunes the masking process. XXX was assessed on the STM32H7 microcontroller in various attack scenarios, demonstrating comparable accuracy and privacy protection to existing encoders, with significantly improved efficiency. Compared to prior approaches, XXX processes faster (up to 53.3x) and occupies 134.1x less memory.","Cloud speech services are vulnerable to privacy concerns due to speech data exposure. Prior approaches address this issue using disentanglement-based encoders, but their substantial resource demands limit their feasibility on less powerful devices. This issue is exacerbated by the increasing adoption of wimpy devices as speech input methods. XXX is introduced to address these concerns by selectively masking short-term speech segments while preserving overall intelligibility, resulting in a lightweight and effective speech protection system.","Current privacy-preserving speech processing methods primarily employ disentanglement-based encoders to safeguard user privacy. These encoders typically utilize pre-trained acoustic models and adversarial training to extract speech representations that separate sensitive information from the speech signal. While effective, their computational requirements and memory usage make them unsuitable for wimpy devices. Alternative approaches like crypto-based, voice conversion, and ultrasonic microphone jammers have various drawbacks, such as slow computation, technical feasibility, and collateral damage to speech semantics. Disentanglement-based encoders, despite their prevalence and effectiveness, are too resource-intensive for practical deployment on wimpy devices.nannannan","The core of XXX is an innovative differential mask generator, which builds upon interpretable learning methods to dynamically identify which segments of the speech signal should be masked. This allows for a more balanced trade-off between privacy and utility in speech understanding tasks. The mask generator is trained to minimize the discrepancy between the masked and original output of a pre-trained speech understanding model, ensuring that masked inputs yield similar predictions to those from the original model. In addition, a cloud model adaptation phase is introduced to fine-tune the cloud-based speech understanding model to the masked inputs, further enhancing the accuracy of the system.nan","XXX was extensively evaluated using the SLURP dataset under various attack scenarios, including black-box and white-box attacks. It achieved 81.2% intent classification accuracy, outperforming previous privacy-preserving speech understanding systems by up to 8.3%. In terms of privacy protection, XXX demonstrated comparable security levels, achieving word error rates of up to 81.6% and entity error rates of 90.7% against malicious automatic speech recognition attacks. Even under white-box attacks, where adversaries possess the same encoder structure and weights as XXX, it maintained a word error rate of 67.3% and an entity error rate of 64.3%. Additionally, XXX proved to be resource-efficient and feasible for wimpy devices, requiring only 394.9KB of memory and 912.0ms to encode a 4-second speech signal on an STM32H7 microcontroller. Compared to prior state-of-the-art systems, XXX demonstrated significant improvements in both memory usage (up to 134.1x less) and processing speed (up to 53.3x faster).",XXX is a privacy-preserving speech understanding system optimized for wimpy devices that effectively balances utility and privacy protection. It selectively masks short-term speech elements to enhance privacy while retaining long-term dependencies crucial for speech understanding. The differential mask generator enables XXX to strike an optimal balance between privacy and usability. Evaluation results demonstrate that XXX achieves comparable accuracy and privacy levels to existing systems while significantly improving efficiency and feasibility on wimpy devices.,Lightweight Protection for Privacy in Offloaded Speech Understanding,"Dongqi Cai, Shangguang Wang, Zeling Zhang, Felix Xiaozhu Lin, Mengwei Xu","Lightweight Protection for Privacy in Offloaded
Speech Understanding
Dongqi Cai
Beiyou Shenzhen Institude
Shangguang Wang
Beiyou Shenzhen Institude
Zeling Zhang
Beiyou Shenzhen Institude
Felix Xiaozhu Lin
University of Virginia
Mengwei Xu
Beiyou Shenzhen Institude
ABSTRACT
Speech is a common input method for mobile embedded
devices, but cloud-based speech recognition systems pose
privacy risks. Disentanglement-based encoders, designed
to safeguard user privacy by filtering sensitive information
from speech signals, unfortunately require substantial mem-
ory and computational resources, which limits their use in
less powerful devices. To overcome this, we introduce a novel
system, XXX, optimized for such devices. XXX is built on the
insight that speech understanding primarily relies on un-
derstanding the entire utterance’s long-term dependencies,
while privacy concerns are often linked to short-term de-
tails. Therefore, XXX focuses on selectively masking these
short-term elements, preserving the quality of long-term
speech understanding. The core of XXX is an innovative dif-
ferential mask generator, grounded in interpretable learning,
which fine-tunes the masking process. We tested XXX on the
STM32H7 microcontroller, assessing its performance in var-
ious potential attack scenarios. The results show that XXX
maintains speech understanding accuracy and privacy at
levels comparable to existing encoders, but with a signifi-
cant improvement in efficiency, achieving up to 53.3× faster
processing and a 134.1× smaller memory footprint.
1
INTRODUCTION
Privacy concern for cloud speech service The volume
of speech data uploaded to the cloud for spoken language
understanding (SLU) is steadily increasing [1, 2, 12], partic-
ularly in ubiquitous wimpy devices where textual input is
inconvenient [3, 17, 41], e.g., home automation devices [32],
smartwatches [37], telehealth sensors [22] and smart fac-
tory sensors [29] . However, exposing raw speech signal
to the cloud raises privacy concerns [42]. It was revealed
that contractors regularly listened to confidential details in
Siri recordings to improve its accuracy [4]. This included
private discussions, medical information, and even intimate
moments.
There are many aspects of potential privacy leakage in
cloud-based SLU. Among them: biometric or contextual pri-
vacy leakage have been well studied and somewhat solved by
Semi-honest cloud
Audio
Wimpy
Device
Intent
Biometric, Sensitive entity,
Semantics, Transcript…
Raw 
Signal
Trust-worthy embedded devices
Raw 
Signal
Disentangled
Encoder
Hidden
States
Intent
(a). Offloaded speech understanding on wimpy devices
(b). Previous approaches to protect speech privacy
(c). Our solution: a novel asymmetric dependency-based encoder
…
Resource-intensive
Raw 
Signal
XXX
Obscured
Signal
Intent
???????????????????
??????
Obscured sensitive 
information
…
Disentangled
Encoder
Disentangled
Encoder
Light-weight
???????????????????
??????
Obscured sensitive 
information
Figure 1: Illustration of offloaded speech understand-
ing on wimpy devices and its privacy protection.
removing information relevant to such tasks without com-
promising the SLU accuracy [18, 35]; transcript protection
(especially sensitive entities) is more challenging since it
is deeply entangled with the SLU task itself. As shown in
Figure 1, this paper focus on ensuring that cloud-based sys-
tems could efficiently classify the intent of SLU task (e.g.,
scheduling appointments or controlling home devices) while
refraining from identifying the concrete entities (e.g., unin-
tended names or passwords) in the spoken utterance, i.e.,
high word error rate (WER) of Automatic Speech Recog-
nition (ASR) task. This is also a setting commonly used in
speech privacy protection [9, 15, 16, 42, 44].
Prior approaches A prevalent method for private speech
processing is employing encoders1 based on disentanglement
representation learning [9, 28, 34, 44], as illustrated in Fig-
ure 1(b). Those encoders extract the speech representations
using pre-trained acoustic models, e.g., wav2vec [9, 40], con-
former [26, 34] and Preformer [20, 44]. Furthermore, they
promote representation disentanglement through adversar-
ial training [25]. For example, PPSLU [44] uses a 12-layer
transformer-based Preformer as its encoder.
As a result, disentanglement-based encoders still demand
considerable computational resources, often exceeding tens
of GFLOPs, to achieve effective disentanglement [11]. They
are also memory-intensive, often comprising tens of millions
1Note that these encoders are not specifically transformer encoders; rather,
they can be implemented using any NNs to encode speech signals.
1
arXiv:2401.11983v1  [cs.SD]  22 Jan 2024
Conference’17, July 2017, Washington, DC, USA
Dongqi Cai, Shangguang Wang, Zeling Zhang, Felix Xiaozhu Lin, and Mengwei Xu
of parameters. Consequently, they are unsuitable for embed-
ded devices with limited memory and are prone to be the
victim of mobile OSes’ low memory killer mechanisms [10].
We refer to the latest resource-efficient LLM survey [47] for
more evidence. Moreover, it takes time-consuming adversar-
ial training to disentangle the encoded representation for
each specific SLU task. This aspect limits the flexibility and
scalability for emerging SLU tasks. More motivating details
will be presented in §2.2.
In this paper, we aim to achieve real-time, privacy-preserving
speech understanding task offloading on wimpy devices like
STM32H7 microcontroller [5] with only 1MB RAM. This goal
necessitates a novel encoder design that must be both light-
weight and effective in filtering out sensitive information on
such devices, as illustrated in Figure 1(c).
Our solution: XXX We therefore present XXX, a SImpLe
ENCodEr designed for efficient privacy-preserving SLU of-
floading, as depicted in Figure 3. It is based on the asym-
metric dependency observation: SLU intent extraction (e.g.,
scenario identification) typically requires only long-term de-
pendency knowledge across the entire utterance, while ASR
task (e.g., recognizing individual words or phrases) needs
short-term dependency, as confirmed by our experiments in
§3.1. Based on it, XXX strategically partitions the utterance
into several segments, selectively masking out the majority
to enhance privacy by obscuring short-term details, without
significantly damaging the long-term dependencies. The pro-
cessed audio waveform is then transmitted to the cloud for
SLU intent analysis. Additionally, we integrate a differential
mask generator, inspired by interpretable learning methods
[19], to optimize performance by automatically identifying
how many and which segments to mask.
Results We deploy XXX on the STM32H7 microcontroller [5]
and assess its performance using the SLURP dataset [13]
in both black-box and white-box attack environments. XXX
achieves 81.2% intent classification accuracy on SLURP, sur-
passing previous privacy-preserving SLU systems by up to
8.3%. Regarding privacy protection, XXX offers comparable
security to earlier systems, with a word error rate of up
to 81.6% and an entity error rate of 90.7% under malicious
ASR attacks. Even against white-box attacks, where attack-
ers is strongly assumed to have the same encoder structure
and weights as XXX, plus partial malicious client data, XXX
maintains 67.3% word error rate and 64.3% entity error rate.
Additionally, XXX proves to be resource-efficient and feasible
for wimpy devices, using only 394.9KB of memory and taking
just 912.0ms to encode a 4-second speech signal. Integrated
with RPI-4B for a fair comparison, XXX uses up to 134.1× less
memory and operates up to 53.3× faster than prior systems.
The accuracy of XXX is only 7% lower than unprotected SLU
systems.
Contribution We have made the following contributions.
• Based on the observation of asymmetric dependency
between SLU and ASR tasks, we propose XXX, a simple
yet effective encoder system for conducting privacy-
preserving SLU.
• We are the first to retrofit interpretable learning meth-
ods to automatically configure the masking process for
a better balance between privacy and utility in speech
understanding tasks.
• We evaluate XXX on a wimpy MCU and demonstrate
its effectiveness under various attack scenarios.
2
RELATED WORK AND BACKGROUND
2.1
Privacy-preserving SLU
Spoken Language Understanding (SLU) is a critical com-
ponent of modern voice-activated systems, responsible for
interpreting human speech and translating it into structured,
actionable commands. For instance, when a user says, ""Set a
meeting for tomorrow at 10 AM,"" the SLU system might map
this to a structured intent such as {scenario: Calendar,
action: Create_entry}.
Evolution of SLU Systems The evolution of SLU systems
has seen a shift from traditional two-component systems,
comprising ASR and Natural Language Understanding (NLU),
to modern end-to-end neural networks [27, 39]. These ad-
vanced systems bypass the intermediate textual representa-
tion and directly map speech signals to their semantic mean-
ing, enhancing efficiency and reducing error propagation. A
typical end-to-end SLU model features an encoder, often with
convolution and attention-based elements, and a decoder,
including a transformer decoder and a connectionist tem-
poral classification decoder. Many SLU systems incorporate
encoders from pre-trained ASR models like HuBERT [45],
replacing the original ASR decoder with one tailored for SLU
tasks.
Threat Model Our threat model aligns with prior work [9,
44] where users (the victims) actively offloads their audio
data to the cloud server (the adversary) for intended SLU
tasks. Upon receiving the data, the adversary may employ
automatic speech recognition to transcribe the audio and
identify private entities [15, 16, 42]. Note that the transcrip-
tions are often exceedingly detailed, containing much more
information than the users intend to disclose. The goal of
this paper is to ensure that the victims can reliably obtain the
predefined SLU intent from the adversary, while preserving
the adversary from discerning sensitive details or private
entities in the transcript.
2
Lightweight Protection for Privacy in Offloaded Speech Understanding
Conference’17, July 2017, Washington, DC, USA
RPI-4B
101
102
103
Memory (MB)
753
649
1.89
500M
OnDevice
Encoder+Cloud
AllOffload
RPI-4B
103
104
Elapsed time 
(ms)
30194
12766
907
TX2
0
500
1000
1500
Elapsed time 
(ms)
1510 1500
907
Figure 2: Cost of disentangling-based encoders [44] for
a 4-second audio inference.
2.2
Inefficiency of Existing Approaches
Privacy-preserving methods Crypto-based approaches,
such as HE [49] and MPC [24], have been proposed to pro-
vide encrypted computation. Unfortunately, they are techni-
cally slow and thus impractical for deployment on wimpy
audio devices due to the significant increase in computation
and communication complexity. For example, MPC-based
PUMA [21] takes 5 minutes to complete one token infer-
ence, which is far too slow for real-time. Voice conversion
is another method to protect speech content. Pr𝜀𝜀ch [8] in-
tegrates voice conversion with GPT-based generated noise
protect privacy, but it is far from feasible for deployment
on wimpy devices. Traditional peripheral devices, such as
ultrasonic microphone jammers (UMJ), are designed to ob-
scure raw speech by inserting non-linearity noise, thereby
preventing illegal eavesdropping[15, 23]; however, they also
corrupt speech semantics as well. A emerging and prevailing
strategy is disentangling-based encoders [9, 28, 44]; they aim
to create a disentangled and hierarchical representation of
the speech signal devoid of sensitive data. But we reveal their
performance issue next.
We conduct preliminary experiments to measure the re-
source consumption of the disentangling-based encoder of a
pre-trained SLU model on a Raspberry Pi 4B (RPI-4B) [6] and
Jetson TX2 (TX2) [7]. Our key observation is that disentangling-
based privacy-preserving SLU system is too resource-intensive
for practical deployment. As illustrated in Figure 2, a disen-
tanglement encoder consumes 648.7MB memory and 12.8s
for complete one inference on RPI-4B. Even in the strong TX2
with GPU, the encoder still takes 593.0ms to complete one
inference. Considering the network latency, the end-to-end
latency of the disentangling-based SLU offloading system
only saves 0.7% wall-clock time compared to the no offload-
ing OnDevice inference, with a similar memory footprint
over 500M.
Implications Disentangling-based encoders is slow and
memory-intensive due to the complex encoder structure de-
signed to separate sensitive information from the speech
signal. Given the limited resource of wimpy devices, it is
B
Transcript: set a meeting for tomorrow at 10 AM
Intent: {scenario: Calendar, action: Create_entry}
User
XXX
Mask 
generator
Transcript: set a meeting for tomorrow at 10 AM
Intent: {scenario: Calendar, action: Create_entry}
Cloud
Figure 3: XXX overview. Red hard line represents the
long-term dependency, while the green dotted line rep-
resents the short-term dependency.
not practical for common privacy-preserving SLU scenar-
ios. To enable practical privacy-preserving SLU, the encoder
structure and the inference process need to be simplified.
3
XXX DESIGN
3.1
System Design and Rationales
We introduce XXX to efficiently scrub raw audio for privacy-
preserving SLU, as depicted in Figure 3. The key idea of XXX
is simple and novel: it masks out a portion of audio segments
before sending them to the cloud for SLU tasks. This design is
based on an unique observation shown in Figure 4(c): when
a portion of audio segments is masked out, the ASR model
becomes incapable to recognize the phonemes in the masked
frames, while the SLU model can still recognize the intent.
The rationales behind XXX Why is XXX able to protect
the sensitive entity privacy while maintaining SLU accuracy?
This capability is rooted in the asymmetrical dependency
between the ASR and SLU task.
Speech is composed of many meta phonemes, and the
generation of a single meta phoneme depends on its ad-
jacent frame [42]. Dependency is defined as the length of
frame that a model’s output depends on. Figure 4 shows each
phoneme is mainly dependent on a few frames, indicating
short-term dependency. This phenomenon is referred to as
""peaky behavior"" in the ASR literature [48]. In contrast, an
SLU model utilizes an attention-based decoder [45] to cap-
ture the relationship between the entire utterance and the
intent, implying that the intent is long-term dependent on
the whole utterance.
Formally, XXX is a simple encoder based on asymmetrical
dependency-based masking. This simple masking encoder is
defined as: ˆ𝑥 = 𝑥 ⊙ Z, where 𝑥 is the input audio signal, ⊙
represents the element-wise multiplication, ˆ𝑥 is the masked
audio signal and Z is the binary masking vector with the
same dimension as𝑥. Z consists of𝑘 uniform portion, with all
0s or 1s in one portion to mask-out or preserve the complete
3
Conference’17, July 2017, Washington, DC, USA
Dongqi Cai, Shangguang Wang, Zeling Zhang, Felix Xiaozhu Lin, and Mengwei Xu
Intent {Scenario: Calender, action: Set}
s e t a mee t
f o r t
o mo r r o w
i ng
Intent {Scenario: Calender, action: Set}
s e t a mee t
mo r r o w
i ng <blank>
masked
(a) Peaky phoneme is short-dependent
(b) Attention seeks intent globally
{Scenario: Calendar, Action: Set}
masked
(c) Empirical performance 
under different ratios of 
masked portion.
{Scenario: Calendar, Action: Set}



	





	





Figure 4: Foundation of XXX: asymmetrical dependency. (a). ASR task is short-term dependent on the peaky
phoneme probability. (b). SLU task is long-term dependent on knowledge from the whole utterance. (c). Empirical
results.
adjacent frames, respectively. This simple encoder forms the
basis of XXX’s efficiency and privacy-preservation capacity,
enabling secure offloading of speech understanding tasks on
wimpy devices.
The configuration challenges: Figure 4(c) demonstrates
that the ratio of masked portion plays a crucial role in bal-
ancing the privacy (ASR-WER) and utility (SLU-ACC). Cur-
rently, XXX employs a trivial masking mechanism, necessitat-
ing clients to undertake a time-intensive hyper-parameter
adjustment about the extent and location of masking. Incor-
rect masking configurations can result in significant loss of
global long-term dependency, negatively affecting SLU accu-
racy, or insufficient masking of sensitive information, thus
compromising privacy. Therefore, we face critical questions:
how many and which portions should be masked?
3.2
Online Configurator for XXX
To address these challenges, we derive a differential mask
generator from the interpretable learning [19] as a online
configurator for XXX. This automatically generate the mask-
ing vector Z. The mask generator is trained to identify how
many and which portions to mask, optimizing the privacy-
utility balance.
Differentiable mask generator The configurator model
aims to minimize the discrepancy between masked and orig-
inal output by generating a mask Z Formally, we define the
number of unmasked portions as L0 loss:
L0(𝜙,𝑥) =
𝑛
∑︁
𝑖=1
1[R≠0] (Z𝑖)
(1)
<x,y>
Mask
Generator
Gate z
SLU
Masked x
x
y
y'
D[y][y']
SLU
Mask
Generator
SLU
Gate z
x
Masked x
y
Mask
Generator
Gate z
x
Masked x
predicted y
Edge
Cloud
 
 
 
(1a)
(1b)
(2)
Figure 5: XXX workflow. (1) Offline phase: (1a) Training
mask generator and (1b) adapting cloud SLU model to
it; (2) Online phase: Conducting could inference with
the masked x. Only masked input audio x and insensi-
tive intent label y are exposed to the cloud.
where 𝜙 is the mask generator, 1(·) is the indicator function.
We minimize L0 for dataset D, ensuring that predictions
from masked inputs resemble those from the origin model:
min
𝜙
∑︁
𝑥 ∈D
L0(𝜙,𝑥)
(2)
s.t. D★[𝑦∥ ˆ𝑦] ≤ 𝛾
∀𝑥 ∈ D
(3)
where ˆ𝑦 = 𝑓 ( ˆ𝑥),𝑦 is the tokenized label, D★[𝑦∥ ˆ𝑦] is the
KL divergence and the margin 𝛾 ∈ R>0 is a hyperparameter.
Given that L0 is discontinuous and has zero derivative
almost everywhere, and the mask generator 𝜙 requires a
discontinuous output activation (like a step function) for
binary masks, we utilize a sparse relaxation to binary vari-
ables [14, 30] instead of the binary mask during training.
Holistic workflow As shown in Figure 5, XXX encom-
passes two phases:
4
Lightweight Protection for Privacy in Offloaded Speech Understanding
Conference’17, July 2017, Washington, DC, USA
(1) Offline phase: (1a) First, XXX trains a differentiable mask
generator. The client selects a mask generator model, po-
tentially a submodule of a pre-trained ASR model, such as
HuBERT’s CNN feature extractor. A small gate model is
then integrated with this submodule. The combined model
processes the input audio and generates a mask. This mask
selectively conceals parts of the input, ensuring retention of
only vital SLU information while hiding sensitive data. The
masked input is then forwarded to either a trusted cloud ser-
vice or a local SLU model for obtaining masked output. The
mask generator is fine-tuned to minimize the discrepancy
between the masked output logits and the original intent, as
defined in Equation (1-3).
(1b) Second, XXX adapts the cloud model . Here, the client
forwards the masked input and a specific SLU intent (e.g.,
""set alarm"") to the cloud-based SLU model. The model under-
goes fine-tuning to adapt to the masked inputs. This process
includes adjusting the model parameters for accurate recog-
nition and response to SLU commands based on the masked
input.
(2) Online phase: In online speech understanding, the client
sends the masked input to the cloud SLU model. Using the
adapted model, the cloud-based SLU accurately identifies
and executes the intended SLU action or response.
Configurator cost analysis Training the differentiable
mask generator is affordable for the client. Our experiments
indicate that convergence is achieved with approximately
200 audio samples, equivalent to 800 seconds of audio. This
process takes up to 40 seconds on an A40 GPU. Adapting the
SLU model to each mask generator is a one-pass effort. This
adaptation is relatively trivial, especially when starting from
a fine-tuned SLU model rather than building from scratch.
This aspect of the process incurs minimal cost compared to
the training of the cloud SLU model. Moreover, these costs
can be amortized over a large number of edge users in the
long run, making it an economically viable solution.
Remark Note that the mask generator is not developed
for tagging sequences at a semantic level. Rather, its design
focuses on identifying segments that are more relevant to the
SLU task. This task is essentially a relatively straightforward
binary classification problem, which is proven to be effective
in prior interpretable learning literature [14, 19] and light-
weight enough for real-time inference.
4
IMPLEMENTATION AND
METHODOLOGY
We have fully implemented the XXX prototype atop Speech-
Brain [38], a PyTorch-based and unified speech toolkit. As
prior work [45], we use SpeechBrain to train the differen-
tial mask generator and simulate the cloud training process.
After that, we deploy the trained mask generator into the
embedded devices and evaluate the end-to-end performance.
Hardware and environment Offline training is simulated
on a server with 8 NVIDIA A40 GPUs. The trained mask
generator is deployed into the STM32H7 [5] or Raspberry PI
4 (RPI-4B) [6]. STM32H7 is a wimpy microcontroller with
1MB RAM. RPI-4B is a popular development board with 4GB
RAM. We embed the approaches not feasible to fit in the
STM32H7 into the Raspberry PI 4.
Models We design four types of mask generator structures:
(1) Random: a random binary vector generator with 50% por-
tion masked; (2) XXX-S: a learnable mask generator with only
one MLP gate; (3) XXX-M: a learnable mask generator with
one HuBERT encoder layer and one MLP gate; (4) XXX-L: a
learnable mask generator with three HuBERT encoder layer
and one MLP gate. As for the cloud SLU model, we simulate
it using the SoTA end-to-end SLU model [45]. It replaces the
ASR decoder of pre-trained HuBERT with SLU attentional
decoder.
Dataset and Metrics We run our experiments on SLURP [13]
with 102 hours of speech. SLURP’s utterances are complex
and closer to daily human speech. We select scenario classi-
fication accuracy to measure the SLU understanding perfor-
mance (ACC-SLU). We choose large-scale English reading
corpus LibriSpeech [33] for the multi-task protection sce-
nario, following prior work [44]. In the multi-task protec-
tion scenario, not only the SLU command utterance (SLURP)
but also the background or the subsequent utterance (Lib-
riSpeech) are uploaded to the cloud. WER is used to mea-
sure the attack performance. More specifically, we utilize
WER-SLU to measure the attacker’s capacity to recognize
the word information in the uploaded SLU audio itself, and
WER-ASR as the WER of recognized accompanying audio,
i.e., LibriSpeech dataset. We also report the private entity
recognition error rate (EER) to ensure that the cloud model
is not able to recognize the private information in the speech
signal.
Baselines We compare XXX to the following alternatives:
(1) OnDevice means the cloud SLU model is downloaded and
run locally on the client device. (2) AllOffload means the
raw audio is uploaded to the cloud for SLU inference. (3)
VAE [9] is the vanilla variational auto-encoder method that
uses adversarial training to disentangle the private informa-
tion from speech signal. (4) PPSLU [44] is the state-of-the-art
disentangling-based SLU privacy-preserving system, which
uses 12 transformer layers to separate the SLU information
into a part of the hidden layer and only sends those hidden
layers to the cloud for SLU inference.
Attack scenarios. We use three attacks encompassing
both black-box and white-box attacks: (1) Azure represents
a black-box attacker scenario, in which the masked audio is
5
Conference’17, July 2017, Washington, DC, USA
Dongqi Cai, Shangguang Wang, Zeling Zhang, Felix Xiaozhu Lin, and Mengwei Xu
OnDevice
AllOffload
Ours-Learnable
Ours-Random
PPSLU
VAE
xxx-S
xxx-M
xxx-L
Figure 6: Performance of different privacy-preserving
SLU approaches. OnDevice offloads no signals to
the cloud and thus has the best privacy protection
(WER=100).
transmitted to Microsoft’s ASR model, Azure, for automatic
speech recognition [31]. We propose three attack scenar-
ios encompassing both black-box and white-box attacks: (2)
Whisper simulates a cloud-based ASR model. This black-box
attacker uses the pre-trained𝑊ℎ𝑖𝑠𝑝𝑒𝑟.𝑚𝑒𝑑𝑖𝑢𝑚.𝑒𝑛 model [36],
directly downloaded from HuggingFace [46]. (3) Whisper(White-box)
constitutes a white-box attack. Here, we hypothesize that
certain users are malicious and disclose the mask generator’s
structure and weights, along with their own audio data, to
the Whisper attack model. Whisper(White-box) then uti-
lizes this collected data from malicious users to adapt the
pre-trained 𝑊ℎ𝑖𝑠𝑝𝑒𝑟.𝑚𝑒𝑑𝑖𝑢𝑚.𝑒𝑛 model to the specific mask-
ing pattern.
Hyper-parameters During the offline phase in Figure 5,
we use the Adam optimizer with a learning rate of 1e-5 and
a batch size of 4. For the inference step, we use the batch
size of 1 to simulate the real streaming audio input scenario.
The end-to-end cloud SLU latency is measured by invoking
Azure APIs following previous work [43]. KL threshold 𝜆 is
set as 0.15 for all mask generators. Attack model is set as
Whisper without special declaration.
5
EVALUATION
5.1
End-to-end performance
XXX achieves comparable accuracy performance and
privacy protection capacity to previous encoders. As
shown in Figure 6, we compare the accuracy of XXX with
all baselines. Is observed that XXX could achieve up to 81.1%
accuracy, with less than 7% accuracy loss compared to unpro-
tected AllOffload and local OnDevice SLU model. Its ratio-
nale is that we mainly mask the short-dependent frames that
does not significantly affect the SLU performance. We also
compare the performance of XXX with the state-of-the-art
privacy-preserving SLU system, i.e., PPSLU [44]. XXX achieves
7.2% higher accuracy than PPSLU which tries to apply com-
plex non-linear transformation to the hidden layer to prevent
malicious re-construction, but this might also damage part
of the SLU information. In terms of privacy preservation,
Figure 7: XXX privacy-preserving capacity under differ-
ent attack models.
Data
5kb
Disentanglement
Encoder
Decoder
200M
10M
Ours-Le
Ours-Random
Disentanglement
Encoder
Ours-Random
Disentan
Enc
Selected
xxx
xxx
xxx
Figure 8: Effect of threshold with different mask gen-
erators.
our learnable mask generator achieves up to 78.6% WER us-
ing XXX-L, indicating a privacy-preserving capacity on par
with PPSLU. Furthermore, we complete the inference with
much lower delays and memory footprint as will be shown
in Figure 9.
XXX is resistant to different attack models. As illus-
trated in Figure 7, XXX increases the SLU-WER from 14.7% to
78.6% under the attack model Whisper. As for the online at-
tack model Azure, XXX increases the SLU-WER from 14.7% to
81.6%. According to our returned service details, we find that
over 50% of the sent audios are tagged as ”𝑅𝑒𝑠𝑢𝑙𝑡𝑅𝑒𝑎𝑠𝑜𝑛.𝑁𝑜𝑀𝑎𝑡𝑐ℎ”,
which means audios are recognized as null utterances by the
Azure ASR model. Whisper(White-box) is a white-box at-
tack model, which means the attacker has the same mask
generator structure and weights as the XXX. We still achieve
more than 50% SLU-WER under this attack model. This is be-
cause even Whisper(White-box) is fine-tuned to fill some of
the missing frames, it still could not recover the private miss-
ing frames because ,masking the short-dependent frames
have fundamentally destroy the raw audio signal. It is not
possible to re-construct the phoneme without knowing any
speech information. In the last row, we show the high entity
error rate to demonstrate that the private entity is not leaked.
XXX scales to better privacy-accuracy trade-off with
a larger mask generator. We explore the impact of the
threshold 𝛾 of XXX under different mask generator struc-
tures. As shown in Figure 8, the threshold 𝛾 controls the
trade-off between the privacy and utility. When 𝛾 is small,
the mask generator is more conservative, leading to higher
the utility a lower the masking portion. As we have dis-
cussed in Section 3, a lower rate of masking portions leads to
higher possibility of privacy entity leakage. When 𝛾 is large,
the mask generator is more aggressive, enhancing privacy.
6
Lightweight Protection for Privacy in Offloaded Speech Understanding
Conference’17, July 2017, Washington, DC, USA
RPI-4B
STM32H7
xxx
xxx
xxx
(a) Memory footprint
RPI-4B
STM32H7
xxx
xxx
xxx
(b) End-to-end latency
Figure 9: Comparison of resource cost in different SLU
approaches. Ours are highlighted in red.
Another way to achieve more practical privacy-utility bal-
ance is using a more complex mask generator structure, e.g.,
XXX-L. It achieves higher utility with the same privacy level
compared to XXX-S, albeit with less efficiency, as shown in
§ 5.2.
5.2
System cost
XXX protects the private entities efficiently. Different from
prior encoders using complex disentanglement model, XXX
only requires a light-weight mask generator to scrub the
private information. The size of this generator varies accord-
ing to different mask generator structures. For the small-
est mask generator, XXX-S, it only requires a 394.9KB mem-
ory footprint, and could successfully embed into the wimpy
STM32H7 with 2MB RAM. XXX is efficient not only in terms
of memory footprint but also in latency. XXX-S completes the
local encoding with only 912.2ms on the wimpy STM32H7.
For a fair comparison, we embed XXX-S into RPI-4B and
find that it is 18.1× faster and 134.1× less memory footprint
than PPSLU. Even with the strong mask generator XXX-L, XXX
achieves up to 7.5× lower encoding latency and consumes
1.9× less memory compared to OnDevice.
6
CONCLUSIONS
XXX is an efficient and privacy-preserving end-to-end SLU
system based on the asymmetrical dependency between ASR
and SLU. XXX selectively mask the short-dependent sensi-
tive words while retaining the long-dependent SLU intents.
Together with the differentiable mask generator, XXX shows
superior end-to-end inference speedup and privacy protec-
tion under different attack scenarios.
REFERENCES
[1] https://openai.com/blog/chatgpt-can-now-see-hear-and-speak.
[2] https://huggingface.co/models?sort=downloads.
[3] https://safeatlast.co/blog/siri-statistics/.
[4] https://www.cnbc.com/2019/08/28/apple-apologizes-for-listening-
to-siri-conversations.html.
[5] https://www.st.com/en/microcontrollers-microprocessors/stm32h7-
series.html.
[6] https://www.raspberrypi.com/products/raspberry-pi-4-model-b/.
[7] https://developer.nvidia.com/embedded/jetson-tx2.
[8] Shimaa Ahmed, Amrita Roy Chowdhury, Kassem Fawaz, and Parmesh
Ramanathan. Pr𝜀𝜀ch: A system for privacy-preserving speech tran-
scription. arXiv preprint arXiv:1909.04198 v2, 2019.
[9] Ranya Aloufi, Hamed Haddadi, and David Boyle. Privacy-preserving
voice analysis via disentangled representations. In Proceedings of
the 2020 ACM SIGSAC Conference on Cloud Computing Security
Workshop, pages 1–14, 2020.
[10] Android.
Android: Low memory killer daemon.
https://
source.android.com/docs/core/perf/lmkd, 2022.
[11] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan
Black, and Shinji Watanabe. Two-pass low latency end-to-end spoken
language understanding. arXiv preprint arXiv:2207.06670, 2022.
[12] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael
Auli. wav2vec 2.0: A framework for self-supervised learning of speech
representations. Advances in neural information processing systems,
33:12449–12460, 2020.
[13] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena
Rieser. Slurp: A spoken language understanding resource package.
arXiv preprint arXiv:2011.13205, 2020.
[14] Jasmijn Bastings, Wilker Aziz, and Ivan Titov. Interpretable neu-
ral predictions with differentiable binary variables. arXiv preprint
arXiv:1905.08160, 2019.
[15] Yike Chen, Ming Gao, Yimin Li, Lingfeng Zhang, Li Lu, Feng Lin,
Jinsong Han, and Kui Ren. Big brother is listening: An evaluation
framework on ultrasonic microphone jammers. In IEEE INFOCOM
2022-IEEE Conference on Computer Communications, pages 1119–
1128. IEEE, 2022.
[16] Peng Cheng and Utz Roedig. Personal voice assistant security and
privacy—a survey. Proceedings of the IEEE, 110(4):476–507, 2022.
[17] Leigh Clark, Philip Doyle, Diego Garaialde, Emer Gilmartin, Stephan
Schlögl, Jens Edlund, Matthew Aylett, João Cabral, Cosmin Munteanu,
Justin Edwards, et al. The state of speech in hci: Trends, themes and
challenges. Interacting with computers, 31(4):349–371, 2019.
[18] Trung Dang, Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews,
Peter Chin, and Françoise Beaufays. A method to reveal speaker
identity in distributed asr training, and how to counter it. In ICASSP
2022-2022 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 4338–4342. IEEE, 2022.
[19] Nicola De Cao, Michael Schlichtkrull, Wilker Aziz, and Ivan Titov. How
do decisions emerge across layers in neural models? interpretation
with differentiable masking. arXiv preprint arXiv:2004.14992, 2020.
[20] Keqi Deng, Songjun Cao, Yike Zhang, and Long Ma. Improving hybrid
ctc/attention end-to-end speech recognition with pretrained acoustic
and language models. In 2021 IEEE Automatic Speech Recognition
and Understanding Workshop (ASRU), pages 76–82. IEEE, 2021.
[21] Ye Dong, Wen-jie Lu, Yancheng Zheng, Haoqi Wu, Derun Zhao, Jin
Tan, Zhicong Huang, Cheng Hong, Tao Wei, and Wenguang Cheng.
Puma: Secure inference of llama-7b in five minutes. arXiv preprint
arXiv:2307.12533, 2023.
[22] Lloyd E Emokpae, Roland N Emokpae, Wassila Lalouani, and Mohamed
Younis. Smart multimodal telehealth-iot system for covid-19 patients.
IEEE Pervasive Computing, 20(2):73–80, 2021.
[23] Ming Gao, Yike Chen, Yajie Liu, Jie Xiong, Jinsong Han, and Kui
Ren. Cancelling Speech Signals for Speech Privacy Protection against
Microphone Eavesdropping. Association for Computing Machinery,
New York, NY, USA, 2023.
[24] Oded Goldreich.
Secure multi-party computation.
Manuscript.
Preliminary version, 78(110):1–108, 1998.
[25] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Gen-
erative adversarial nets. Advances in neural information processing
7
Conference’17, July 2017, Washington, DC, USA
Dongqi Cai, Shangguang Wang, Zeling Zhang, Felix Xiaozhu Lin, and Mengwei Xu
systems, 27, 2014.
[26] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang,
Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu,
et al. Conformer: Convolution-augmented transformer for speech
recognition. arXiv preprint arXiv:2005.08100, 2020.
[27] Parisa Haghani, Arun Narayanan, Michiel Bacchiani, Galen Chuang,
Neeraj Gaur, Pedro Moreno, Rohit Prabhavalkar, Zhongdi Qu, and
Austin Waters.
From audio to semantics: Approaches to end-to-
end spoken language understanding. In 2018 IEEE Spoken Language
Technology Workshop (SLT), pages 720–726. IEEE, 2018.
[28] Diederik P Kingma and Max Welling. Auto-encoding variational bayes.
arXiv preprint arXiv:1312.6114, 2013.
[29] Naveen Kumar and Seul Chan Lee. Human-machine interface in smart
factory: A systematic literature review. Technological Forecasting and
Social Change, 174:121284, 2022.
[30] Christos Louizos, Max Welling, and Diederik P Kingma. Learning
sparse neural networks through 𝑙_0 regularization. arXiv preprint
arXiv:1712.01312, 2017.
[31] Microsoft. Azure asr. https://azure.microsoft.com/en-us/products/ai-
services/speech-to-text/.
[32] Nombulelo CC Noruwana, Pius Adewale Owolawi, and Temitope Ma-
payi. Interactive iot-based speech-controlled home automation system.
In 2020 2nd International Multidisciplinary Information Technology
and Engineering Conference (IMITEC), pages 1–8. IEEE, 2020.
[33] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.
Librispeech: an asr corpus based on public domain audio books. In
2015 IEEE international conference on acoustics, speech and signal
processing (ICASSP), pages 5206–5210. IEEE, 2015.
[34] Cal Peyser, Ronny Huang Andrew Rosenberg Tara N Sainath, Michael
Picheny, and Kyunghyun Cho. Towards disentangled speech repre-
sentations. arXiv preprint arXiv:2208.13191, 2022.
[35] Jianwei Qian, Haohua Du, Jiahui Hou, Linlin Chen, Taeho Jung, and
Xiang-Yang Li. Hidebehind: Enjoy voice input with voiceprint unclon-
ability and anonymity. In Proceedings of the 16th ACM Conference
on Embedded Networked Sensor Systems, pages 82–94, 2018.
[36] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine
McLeavey, and Ilya Sutskever. Robust speech recognition via large-
scale weak supervision.
In International Conference on Machine
Learning, pages 28492–28518. PMLR, 2023.
[37] Joel M Raja, Carol Elsakr, Sherif Roman, Brandon Cave, Issa Pour-
Ghaz, Amit Nanda, Miguel Maturana, and Rami N Khouzam. Apple
watch, wearables, and heart rhythm: where do we stand? Annals of
translational medicine, 7(17), 2019.
[38] Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe,
Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad,
Abdelwahab Heba, Jianyuan Zhong, et al. Speechbrain: A general-
purpose speech toolkit. arXiv preprint arXiv:2106.04624, 2021.
[39] Subendhu Rongali, Beiye Liu, Liwei Cai, Konstantine Arkoudas, Cheng-
wei Su, and Wael Hamza. Exploring transfer learning for end-to-
end spoken language understanding. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 35, pages 13754–13761,
2021.
[40] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli.
wav2vec: Unsupervised pre-training for speech recognition. arXiv
preprint arXiv:1904.05862, 2019.
[41] Suranga Seneviratne, Yining Hu, Tham Nguyen, Guohao Lan, Sara
Khalifa, Kanchana Thilakarathna, Mahbub Hassan, and Aruna Senevi-
ratne.
A survey of wearable devices and challenges.
IEEE
Communications Surveys & Tutorials, 19(4):2573–2620, 2017.
[42] Ke Sun, Chen Chen, and Xinyu Zhang. "" alexa, stop spying on me!""
speech privacy protection against voice assistants. In Proceedings of
the 18th conference on embedded networked sensor systems, pages
298–311, 2020.
[43] Rongxiang Wang and Felix Lin. Efficient deep speech understanding
at the edge. arXiv preprint arXiv:2311.17065, 2023.
[44] Yinggui Wang, Wei Huang, and Le Yang.
Privacy-preserving
end-to-end spoken language understanding.
In Proceedings
of the Thirty-Second International Joint Conference on Artificial
Intelligence, pages 5224–5232, 2023.
[45] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba. A
fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recog-
nition, speaker verification and spoken language understanding. arXiv
preprint arXiv:2111.02735, 2021.
[46] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi
Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-
the-art natural language processing. arXiv preprint arXiv:1910.03771,
2019.
[47] Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu,
Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang,
et al. A survey of resource-efficient llm and multimodal foundation
models. arXiv preprint arXiv:2401.08092, 2024.
[48] Albert Zeyer, Ralf Schlüter, and Hermann Ney. Why does ctc result in
peaky behavior? arXiv preprint arXiv:2105.14849, 2021.
[49] Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and
Yang Liu. Batchcrypt: Efficient homomorphic encryption for cross-
silo federated learning. In 2020 USENIX annual technical conference
(USENIX ATC 20), pages 493–506, 2020.
8
"
"This research paper proposes a novel adaptive motion planning method for multi-fingered functional grasping utilizing force feedback. The method employs joint torque feedback as part of the state in a Markov Decision Process, enabling a robot to learn the skill of grasp through trial and error under pose uncertainty. Experiments in simulation demonstrate the significant effect of force feedback in generating smooth trajectories for functional grasps.","Multi-fingered robots face difficulties in grasping and manipulating objects due to the exponential growth of possible interaction modes with the number of hand links and object contact points. Closed-loop feedback control based on sensory observations is essential to dynamically update and adapt the grasping process to disturbances and errors. Deep reinforcement learning has emerged as an approach to control complex dynamical systems, especially in high-dimensional dexterous manipulation. However, this approach often relies on human demonstrations or assumes precise knowledge of the object's pose. Force feedback with dynamic responses to disturbances is crucial to compensate for the object's shape and pose mismatch. This work aims to go beyond simple pick-and-place tasks and achieve functional grasp for multi-fingered hands, which involves task-oriented grasping on specific functional regions of objects.","nanAnalytical Motion Planning: There are efficient search and optimization algorithms for motion planning problems. However, dexterous hand motion planning is challenging due to the high-dimensional freedom of motion and the complexity of making and breaking contacts between hands and objects. Researchers have improved planning performance on high-dimensional planning problems by using multilevel abstractions and generalized sparse roadmaps. Other approaches include CMGMP, TrajectoTree, and efficient global motion planning for highly contact-rich and high-dimensional systems.

Learning Based Motion Planning: Techniques such as CVAE, trajectory imitation learning, and auto-regressive network architecture have been utilized to generate grasping paths. However, the majority of research in this area relies on deep reinforcement learning, which has demonstrated outstanding performance in sequential decision-making problems. A recent study used a teacher-student distillation approach to teach the reinforcement learning of operational strategies under the assumption of known object states, followed by imitation when the object state is unknown. Vision is advantageous for guiding dexterous hands to approach and grasp objects, but once the hand and object make contact, obtaining precise object position and interaction forces becomes difficult due to occlusion.

Research on force feedback has focused on exploring control methods and effects on two-finger and three-finger grippers, but extending these techniques to five-finger dexterous hands remains challenging. Most studies in this area have focused on fingertip tactile feedback. However, tactile sensors often provide limited low-precision tactile perception, and frequent hand-object interactions can lead to costly damage to these devices. In contrast, the robot's body typically offers joint torque sensing information, which dynamically changes in response to dexterous hand motion, contact, and collision, providing valuable insights for manipulation. Though several studies have utilized joint torque information, it remains uncertain whether dexterous hands have obtained the control skills to utilize joint torque feedback for performing operations and the role of joint torque sensing during such manipulations.nannan","Force-feedbacked Motion Planning via Reinforcement Learning:

The task of motion planning from pre-grasp to a goal functional grasp is challenging when relying solely on visual observations. This work utilizes force sensing in the joints of a multi-fingered hand to perceive the hand-object interaction state. The movement of dexterous hands should be gentle to avoid moving objects.
A deep reinforcement learning model is trained using joint torques, hand pose, and joint angles of a multi-fingered hand as observations and hand configuration changes as actions.

Action and State Space:

The action vector contains 24 joints controlling the fingers and wrist and 6 joints controlling the hand base translation and rotation, all controlled in position mode. The objective of the action control is to generate an optimized hand trajectory that allows the hand to approach an object from a pre-grasp position to a goal functional grasp.

The state vector consists of joint angles J, joint torques F on each of the 24 driving joints, and the hand's 6D pose (T, R), as well as the parameter h that controls joint motion.

Reward:

The reward function ra guides the robot's joint angles towards the goal pose, with the observation variable h indicating the closeness of the joint angles to the goal angles.

r_a = - | 1 - h |. 

The reward function rb guides the hand palm towards the goal grasping position.

r_b = -||p - p_g||^2.

The reward function rc incentivizes the agent to minimize object displacement during operation.

r_c = -||T_obj^c - eT_obj||^2 

eT_obj = T_obj + ϵ.

The total reward is defined as:

r = ω + ω_ara + ω_brb + ω_crc

where ω = 1.5, ω_a = 1, ω_b = 20, and ω_c = 100.

Terminal Conditions:

Task completion is based on three conditions: when the object's movement is less than 0.01m, the final joint angles of the hand are 95% close to the goal angles, and the deviation of the palm position is less than 0.01m. The agent receives an additional reward of 1000 if these conditions are met. A reward of 1.5 * (200 - step) is designed to incentivize the agent to complete the task in as fewer steps as possible.nan","Contribution of Force Feedback:

Experiments were conducted using the MuJoCo physics simulator and the ADROIT dexterous hand. It was observed that the hand gradually understood the task of approaching and grasping the object after several trial and error cycles. However, it took considerable time for the hand to learn to perceive collisions and adjust its grasping pose to avoid moving the object. A comparison of success rates with and without force feedback was conducted under two different conditions of initial position changes.

When the initial position error was smaller than the maximum allowable displacement of the objects during operation, several fixed grasping paths resulted in task completion, regardless of force feedback. However, when the initial position error was larger, no fixed paths achieved a high success rate, and very different grasp paths were generated for varying initial positions.

Additional experiments demonstrated that force feedback plays a helpful role in the learning process, particularly when the initial position deviation of the object is relatively large. Moreover, testing on heavier objects further validated the importance of joint torques in dexterous manipulation.

Adaptive Functional Grasp:

Experiments on functional grasping of four distinct objects with randomized initial positions showcased the adaptability of the method in generating diverse grasping trajectories and grasp types tailored to different initial object positions.",This work introduces a novel adaptive motion planning method for multi-fingered functional grasping utilizing force feedback. The method demonstrates promising capabilities of human dexterity for adaptive robotic manipulation. Limitations include sensitivity to initial random exploration and the manipulation of relatively heavy objects compared to real-world environments. Future work will focus on improving manipulation dexterity by incorporating more perceptual information.,Adaptive Motion Planning for Multi-fingered Functional Grasp via Force Feedback,"Dongying Tian, Xiangbo Lin, Yi Sun","Adaptive Motion Planning for Multi-fingered Functional Grasp via
Force Feedback
Dongying Tian1,2, Xiangbo Lin1 and Yi Sun1
Abstract—Enabling multi-fingered robots to grasp and manip-
ulate objects with human-like dexterity is especially challenging
during the dynamic, continuous hand-object interactions. Closed-
loop feedback control is essential for dexterous hands to dynam-
ically finetune hand poses when performing precise functional
grasps. This work proposes an adaptive motion planning method
based on deep reinforcement learning to adjust grasping poses
according to real-time feedback from joint torques from pre-
grasp to goal grasp. We find the multi-joint torques of the
dexterous hand can sense object positions through contacts and
collisions, enabling real-time adjustment of grasps to generate
varying grasping trajectories for objects in different positions.
In our experiments, the performance gap with and without
force feedback reveals the important role of force feedback in
adaptive manipulation. Our approach utilizing force feedback
preliminarily exhibits human-like flexibility, adaptability, and
precision.
Index Terms—Multifingered Hands, Force feedback, Motion
planning, Grasping.
I. INTRODUCTION
E
NABLING multi-fingered robots to grasp and manipulate
objects with human-like dexterity is a challenging task
that typically involves two steps: grasp synthesis [1] and
motion planning. Previous studies have primarily focused
on synthesizing static grasps for specific objects [2]–[4].
However, specifying only the static grasp configuration is
insufficient, as grasp and manipulation involve continuous
hand-object interaction. Errors in the hand or object pose
can result in collisions, complicating motion planning and
trajectory optimization. Multi-fingered robot hands face even
greater difficulty as the possible interaction modes grow ex-
ponentially with the number of hand links and object contact
points. This is particularly true for functional grasps, such as
those required for dexterous tool use, which require accurate
touching of specific functional parts, such as the nozzle of a
watering can or the button of an electric drill. Closed-loop
feedback control based on sensory observations is therefore
essential to dynamically update and adapt the grasping process
to disturbances and errors.
Recently, deep reinforcement learning has emerged as an
approach to control complex dynamical systems, especially
in high-dimensional dexterous manipulation [5], [6]. These
approaches often rely on human demonstrations [7] or assume
precise knowledge of the object’s pose throughout the grasp
This work was supported by the National Natural Science Foundation of
China [grant numbers 61873046, U1708263].(Corresponding author:Yi Sun)
1Dongying Tian, Xiangbo Lin and Yi Sun are with the School of Informa-
tion and Communication and Engineering, Dalian University of Technology,
Dalian, 116024, China. (email: tiandongying@sia.cn, linxbo@dlut.edu.cn,
lslwf@dlut.edu.cn.)
2Dongying Tian is also with Shenyang Institute of Automation, Chinese
Academy of Sciences, Shenyang 110016, China.
and manipulation process [6]. However, partial occlusion of
the object due to its own shape or the manipulator can
cause errors in object shape and pose estimation, leading to
failed grasps. In such situations, force feedback with dynamic
responses to disturbances is essential to compensate for the
object’s shape and pose mismatch. Previous studies [8]–
[10] have shown that learning grasping policies under tactile
and force feedback can adapt to environmental uncertainty,
improving the grasp success rate, particularly for low-DOF
grippers. However, this ability is still challenging to transfer to
high-DOF multi-fingered robot hands. Until recently, the work
of [11] trained a five-fingered hand to learn robust motions and
stable grasps using force feedback without visual sensing in a
pick-up task.
Our work aims to go beyond the simple pick-and-place
task and achieve functional grasp for multi-fingered hands.
Functional grasp involves task-oriented grasping on specific
functional regions of objects, enabling the completion of
manipulations that are both stable and functional. For instance,
the index finger presses the shutter of a camera to take photos.
To differentiate our task from stable grasping of objects
such as pick-and-place, we refer to this type of grasp as
functional grasp. In this work, we formulate functional grasp
as a dynamic motion planning process, from pre-grasp to goal
grasp, highlighting the challenges in contact-rich functional
grasp, which requires high precision and robustness against
uncertainty. During this process, the object can hardly remain
steady due to errors in hand or object pose, leading to
unsynchronized finger touching and translation plus rotation
of objects in 3D space. Given the vision limitations due
to occlusion by the object or manipulator, we study the
effectiveness of force feedback control to handle uncertainty
by disabling visual sensing and designing a force-aware path
planning method to adapt the manipulation of the object.
This is a more challenging task as there are neither full
demonstration trajectories nor visual sensing during grasping.
The contributions of this paper can be summarized as
follows:
• We propose an adaptive motion planning method for
multi-fingered functional grasp via joint torque feedback,
which is constantly updated and dynamically adapted to
object uncertainty in the absence of visual feedback.
• We build a reinforcement learning model in which joint
torque feedback is considered as part of the state in the
Markov Decision Process, which enables the robot to
learn the skill of grasp through trial and error under pose
uncertainty.
• We conduct functional grasp experiments in simulation
and observe a significant effect of force feedback in
generating smooth trajectories for functional grasps.
arXiv:2401.11977v1  [cs.RO]  22 Jan 2024
II. RELATED WORK
A. Analytical Motion Planning
Nowadays, there are efficient search and optimization al-
gorithms developed to solve motion planning problems [12].
However, dexterous hand motion planning is still challenging
due to the high-dimensional freedom of motion and the com-
plexity of making and breaking contacts between hands and
objects [13]. Orthey et al. improved the planning performance
on high-dimensional planning problems by using multilevel
abstractions to simplify state spaces [14] and generalized
sparse roadmaps to multilevel abstractions afterwards [13].
The CMGMP algorithm [15] utilizes automatically enumer-
ated contact modes of environment-object contacts to guide the
tree expansions during the search, generating hybrid motion
plans including both continuous state transitions and discrete
contact mode switches. The TrajectoTree method [16] plans
trajectories for dexterous manipulation tasks involving con-
tact switching using contact-implicit trajectory optimization
(CITO) [17] augmented with a high-level discrete contact
sequence planner. Another proposed method [18] enables effi-
cient global motion planning for highly contact-rich and high-
dimensional systems. However, analysis methods are usually
based on conditional assumptions, and the models are complex
and difficult to reproduce.
B. Learning Based Motion Planning
Currently, there have been a few studies that utilize tech-
niques such as CVAE [19], trajectory imitation learning [20],
or auto-regressive network architecture [21] to generate grasp-
ing paths. However, the majority of research in this area relies
on deep reinforcement learning, which has demonstrated out-
standing performance in sequential decision-making problems
[6], [22]–[26]. To facilitate exploration and reduce sampling
complexity, it is common to incorporate prior knowledge such
as expert demonstrations [22], [24], contact maps [25], or
grasping poses [23], [26]. For example, learning efficiency can
be enhanced by using a small amount of teaching data [24], or
by directly acquiring operational experience from human hand
manipulation videos [22]. Moreover, a binary affordance map
has been employed [25] to guide the agent towards functional
grasp regions on the object. Sudeep et al. [23] found that the
pregrasp finger pose is crucial for successful behavior learning.
Our research, similar to [6], [26], utilizes goal grasp poses.
However, unlike previous approaches, we specifically focus
on functional grasping. In order to accomplish subsequent
operational tasks, the hand configuration is typically more
complex and requires precise execution, rather than solely
pick-up the object.
For the hand-object interaction phase we are studying, not
only is there initial position uncertainty in the object, but
the object can also be perturbed during the operation. Some
studies assume that the object’s pose is completely known
throughout the entire grasping process, which does not align
with real-world scenarios [6], [8], [22], [23], [27], [28]. [26]
addressed this issue by employing a teacher-student distillation
approach, where the reinforcement learning of operational
strategies is initially performed under the assumption of
known object states and later imitated when the object state
is unknown. In practice, vision is advantageous in guiding
dexterous hands to approach and grasp objects. However, once
the hand and object come into contact, precise object position
and the interaction forces between the hand and object are
difficult to obtain through vision due to occlusion [29]. As a
result, purely vision-guided grasping often leads to imprecise
and jarring operations [27], [30].
Based on human operational experience, real-time force
feedback is indispensable for guiding smooth and dexterous
manipulation actions. Current research has focused on explor-
ing the control methods and effects of force feedback on two-
finger [28] and three-finger grippers [8]–[10]. These studies
emphasize the potential of incorporating tactile feedback in
robotic grasping, but it remains highly challenging to extend
these techniques to five-finger dexterous hands. Currently,
most research in this area has focused on fingertip tactile
feedback [11], [31]–[33]. In [31], fingertip tactile information
was mapped to force signals for force feedback control.
[32] utilized fingertip tactile feedback to compensate for the
reduction in geometric information caused by coarse bounding
boxes and uncertainties in pose estimation. [33] used fingertip
tactile feedback to adapt to online estimates of the object’s
surface, correcting errors in the initial plan. Liang et al.
[11] proposed a fusion of binary contact information from
the fingertip, torque sensors, and robot proprioception (joint
positions), which shows promise for achieving more robust
and stable grasping. Additionally, [24] demonstrated that touch
sensors capable of sensing on-off contact events enable faster
learning and better asymptotic performance in tasks with high
degrees of occlusion.
However, there exists a significant disparity between the
perception capabilities of tactile sensors and human touch [34].
When tactile sensors are placed on the finger surface, due
to the limitations of the mechanical structure, they usually
can only provide limited low-precision tactile perception.
Additionally, frequent hand-object interactions can lead to
costly damage to tactile sensing devices. In contrast, the
robot’s body typically offers joint torque sensing information,
which dynamically changes in response to dexterous hand
motion, contact, and collision, providing valuable insights for
manipulation. Though several studies have utilized joint torque
information [11], [26], [35], it remains uncertain whether
dexterous hands have obtained the control skills to utilize
joint torque feedback for performing operations and the role
of joint torque sensing during such manipulations. In our
research, we solely rely on joint torque feedback to design
training tasks and rewards, guiding the dexterous hand to learn
through employing joint torque feedback, thereby revealing the
potential value of joint torque feedback.
III. PROBLEM FORMULATION OF FUNCTIONAL
GRASP PLANNING
This work aims to plan a motion trajectory for a functional
grasp that is not only stable but also functional. It focuses on
the last path planning involving rich contact between a multi-
fingered hand and an object, which requires high precision and
Fig. 1. In the scenario where there is uncertainty in the initial position of an object and the pose of the object is unknown during the grasping process, an
agent uses hand perception to obtain information about the hand-object interaction state, and makes action decisions accordingly. The rewards obtained from
the dexterous hand executing actions will guide the update of the critic-net and further update the actor-net.
robustness against uncertainty. Given a pre-grasp and a goal
functional grasp, the motion trajectory of this path starts from
the pre-grasp and ends at the goal functional grasp. During
this process, it is difficult to obtain the contact state of the
hand-object from visual observation due to occlusion, so the
joint torque located on each driving joint of the hand is taken
as the force feedback information in this work.
The force feedback information, which dynamically updates
due to contact collision, can be used to sense the hand-
object interaction state. If the multi-fingered hand automati-
cally identifies and touches the functional part of the object
while the object can still rest steadily with placement tol-
erances (< 0.01m), the motion trajectory of the functional
grasp is considered successful. We further add disturbance ϵ
(< 0.02m) to the initial horizontal position of the object to
verify the adaptability to uncertainty by utilizing the force
feedback information.
We denote a pre-grasp hand pose as G0 = (T0, R0, J0)
and a goal grasp Gg = (Tg, Rg, Jg), relative to the object at
(T obj, Robj), where J stands for the joint angle of the hand,
and T and R stand for the position and orientation of the 6D
pose. Our goal is to plan the motion from G0 to Gg, guided
by the force feedback of the hand joints. The movements of
dexterous hands should be gentle to avoid moving objects.
IV. METHOD
A. Force-feedbacked
Motion
Planning
via
Reinforcement
Learning
The task of motion planning from pre-grasp to a goal
functional grasp is challenging when relying solely on visual
observations due to occlusion. Any errors in the hand or object
positioning can result in knocking over or even destroying
the object. To minimize such risks, we utilize force sensing
in this work, which provides essential information about the
local object geometry, contact forces, and grasp stability. Our
motion planning strategy is based on joint torque feedback
from each driving joint of the hand, which directly translates
force cues into adaptive finger motion predictions. As the
hand makes/breaks contacts with the object, the set of joint
torques applied to the object dynamically and quickly changes.
We optimize the motion trajectory based on the observed
joint torques, current hand pose, contact points on the object,
and contact forces in order to predict the next desired hand
pose and joint angles that will keep the object stable dur-
ing grasping. As optimization progresses, the grasp becomes
increasingly stable and plausible towards the goal functional
grasp. Force feedback plays a crucial role in smoothly fitting
the multi-fingered hand to the object surface to achieve a
compliant grasp.
We train a deep reinforcement learning model using joint
torques, hand pose, and joint angles of a multi-fingered hand
as the observations and hand configuration changes as actions.
An overview of the optimization trajectory for the Shadow
hand is presented in Figure 1. To achieve a functional grasp
of a given object at the end of the planning, we define a grasp
reward function that reflects the grasp quality. Additionally,
to minimize object movement when the hand touches it, we
design another reward function that encourages the agent to
move the object as little as possible. We employ the Soft Actor-
Critic (SAC) algorithm [36], [37], an off-policy reinforcement
learning technique to learn adaptive action policies through
trial-and-error.
B. Action and State Space
The action vector contains 24 joints controlling the fingers
and wrist, as well as 6 joints controlling the hand base
translation and rotation. All of these joints are controlled
in position mode. The objective of the action control is to
generate an optimized hand trajectory, which allows the hand
to approach an object from a pre-grasp position to a goal
functional grasp. We assume that the pre-grasp position is
slightly more than 2 centimeters away from the surface of the
object to avoid contact with the object due to initial positioning
deviations, which can be obtained using a computer vision
TABLE I
OBSERVATIONS TABLE
Observation
Dimension
Current joint torques of hand F
24
Current joint angles of hand J
24
Current 6D pose of hand
6
Joint motion controller h
1
Current driving joints of hand base
6
Previous 6D pose of hand
6
Previous joint motion controller
1
Goal 6D pose of hand Tg, Rg
6
Goal palm position
3
system and a grasp planner. The goal functional grasp is either
obtained from recent static grasp synthesis methods [4] or a
grasp dataset [38]. An example of the pre-grasp and goal grasp
of a camera is shown in Figure 1.
The task of motion planning from pre-grasp to a goal
functional grasp is not a trivial one. Unlike the pick-and-
place task that simply involves closing fingers, achieving a
goal functional grasp, such as for dexterous tool use, requires
precise positioning of the hand on the functional parts of an
object. This task is further complicated by the occlusion of
the object during hand-object interaction, making it difficult
to determine the exact object position. In this work, we rely
on the goal functional grasp and force feedback information
to guide the hand motion. The reinforcement learning policy
is designed such that the desired grasp pose drives the hand to
achieve the desired pose on the object, while the joint torque
feedback helps to locate the object, perceive its geometry, and
provide compliant contact forces.
The entire multi-fingered hand is controlled by a 6 DoF hand
base and one actuator per joint. The hand base globally adjusts
the orientation and translation of the whole hand to make it
reach the feasible approaching direction and distance to the
object, while the actuator per joint locally controls the joint
angle to force multiple fingers to contact the corresponding
functional parts of the object. Since the hand leaves a small
gap between the fingers and the object from a pre-grasp to a
goal grasp, we employ a locally linear trajectory for each joint.
This trajectory takes the pre-grasp joint angle J0, goal joint
angle Jg, and parameter h as inputs and outputs the current
joint angle as follows:
J = h(Jg − J0) + J0, h ∈ [0, 1]
(1)
Here, Jg−J0 denotes the range of joint angle, and h ∈ [0, 1]
is an updated parameter that progressively controls the joint
from the initial pre-grasp joint angle to the final goal joint
angle. When h = 0, the joint is at the pre-grasp angle, and
when h = 1, it is at the final goal angle. We use the same
value of h at each joint to synchronously drive multiple fingers
towards the goal position.
The state vector consists of joint angles J, joint torques
F on each of the 24 driving joints, and the hand’s 6D pose
(T, R), as detailed in Table 1. To monitor the execution
progress, the parameter h that controls joint motion is also
designed as an observation variable, which is defined as:
Fig. 2.
Terminal conditions: The hand moves in the incorrect orientation
(Left). The object’s displacement surpasses the predetermined limit (Right).
h = 1 − 0.5 max
i
|ji − jg
i | , ji ∈ J, jg
i ∈ Jg
(2)
Here, ji and jg
i denote the current and goal joint angles
for the i-th joint, respectively. In all joint angle changes,
if the maximum absolute difference (normalized to [0,1])
between the current joint angles J and the goal joint angles Jg
approaches 0, h will be close to 1, indicating that all fingers
have reached the goal hand pose. It is worth noting that we do
not include any information about the object pose, geometry
or mass in the state vector.
C. Reward
A carefully designed reward function is essential to guide
the agent in learning the desired behavior. In our manipulation
tasks, due to the initial uncertainty of the object’s position,
dexterous hands need to sense the real position of the object
through touch. During this process, contact and collision
between the hand and the object can cause further displace-
ment of the object. Dexterous grasping requires reducing the
movement of the object, allowing the hand to adjust to the
object’s position. Therefore, the ideal grasping result should be
based on the relative position between the hand and the object
in the goal grasp, achieving accurate functional grasping at the
final position of the object. In order to achieve a functional
grasp, we have designed a reward function ra that guides the
robot’s joint angles towards the goal pose. Specifically, the
observation variable h indicates how close the joint angles are
to the goal angles, and the closer they are, the greater the
reward obtained.
ra = − | 1 − h |
(3)
We further design the reward function rb to guide the palm
to the goal grasping position. Due to the multi-point contact
constraints between the hand and the object, we only use the
position of a single point on the palm to guide its movement.
We obtain the current position of the palm relative to the object
from the simulation system, denoted as p. The reward function
is used to guide the palm to minimize the deviation between
its current position p and the goal position pg. It should be
noted that the variable pg here denotes the desired position of
the palm relative to the current object position, as opposed to
the goal palm position provided in the state, which is defined
relative to T obj. This approach has shown promising results
in simulation experiments.
rb = −∥p − pg∥2
(4)
Moreover, we employ rc to incentivize the agent to mini-
mize object displacement during operation, which is calculated
as the negative distance between the object positions before
and after the operation:
rc = −∥T obj
c
− eT obj∥2
(5)
eT obj = T obj + ϵ
(6)
Here, T obj
c
denotes the current object position, and eT obj
represents the initial position of the object with added uncer-
tainty.
Finally, we use the weight coefficients to scale the reward
functions mentioned above, approximating a range of [-1,0].
The total reward is defined as:
r = ω + ωara + ωbrb + ωcrc
(7)
where ω = 1.5, ωa = 1, ωb = 20, and ωc = 100.
D. Terminal Conditions
It is crucial to encourage the agent to focus on important re-
gions of the state space and avoid unnecessary exploration. We
define successful task completion based on three conditions:
when the object’s movement is less than 0.01m (|rc| < 0.01),
the final joint angles of the hand are 95% close to the goal
angles (|ra| < 0.05), and the deviation of the palm position is
less than 0.01m (|rb| < 0.01). Once these conditions are met,
the agent receives an additional reward of 1000. Moreover,
a reward of 1.5 ∗ (200 − step) is designed to incentivize the
agent to complete the task in as fewer steps as possible, where
the maximum number of steps allowed to execute a task is
200. The values of ra and rc are continuously monitored in
real-time. If the object’s movement exceeds 0.01m or the palm
position deviation surpasses 0.1m, the grasp fails immediately,
as shown in Figure 2. In addition, if the total reward falls below
−50, indicating inefficient exploration, the task will be aborted
promptly.
V. EXPERIMENTS
A. Simulation Experiment Settings
We performed experiments using the MuJoCo physics sim-
ulator [39] and the ADROIT dexterous hand [40], which is a
24-DOF anthropomorphic platform equipped with six driving
joints on the hand base, allowing the hand to move freely in
space. As our study focused on the adaptive actions guided
by force feedback, factors such as tendon elasticity and weak
driving forces could hinder the perception of force feedback.
Therefore, we made modifications to the platform, including
removing hand tendons, increasing the range of joint forces,
and building a position controller to enhance controllability.
Four tasks were designed, including grasping the nozzle of
a watering can, preparing to take pictures with a camera, using
an electric drill, and grasping the handle of a cup. These tasks
require accurate matching the pose of the hand with that of
Fig. 3.
Comparison of the success rates with and without force feedback
under two different conditions: (Left) ϵ = 0.005 and (Right) ϵ = 0.02.
Fig. 4. Comparison of the success rates with and without force feedback for
heavy objects.
the object, which is challenging due to errors in the pose of
the object and hand. The pre-grasp hand pose and the goal
grasp hand pose of the four objects were labeled according to
[4], with the weight of the camera set to 1.0kg and the others
set to 0.5kg.
During training, the simulation system was reset when the
termination condition was reached, and the total number of
learning steps was 1000000. The first 1000 steps adopted a
random policy, and then a batch of data was randomly selected
from the buffer to update the policy in each step. We tested the
policy learned by the agents every 2000 steps and retained the
five best policies during the entire learning process. Finally, the
best action policy was selected through 100 random tests. This
learning process took approximately 4 hours on an RTX2080
Ti.
B. Contribution of Force Feedback
During the dexterous hand learning process, we observed
that the hand gradually understood the task of approaching
and grasping the object after several trial and error cycles.
However, it took a considerable amount of time for the hand
to learn to perceive collisions and adjust its grasping pose
to avoid moving the object. The larger the range of initial
position changes, the more challenging the learning process
became. Nevertheless, it was fascinating to witness the hand’s
efforts to find the optimal grasping policy that could adapt to
initial displacement of the object.
Fig. 5. We conducted one hundred randomized tests, with the initial positions of the object drawn in the horizontal (x, y) direction, as shown in the figure.
Successful grasps are indicated by green circles, while failed grasps are indicated by red triangles.
We conducted a comparison under two different initial posi-
tions of an object with errors at ϵ = 0.005m and ϵ = 0.02m,
which are lower and higher than the threshold value of rc
(0.01m), respectively. When ϵ = 0.005m, the initial position
error is smaller than the maximum allowable displacement
of the objects during operation. In this case, several fixed
grasping paths can enable the successful completion of the
task, regardless of force feedback. However, when ϵ = 0.02m,
there are no fixed paths that can achieve a high success rate,
and very different grasp paths may be generated for varying
initial positions. We conducted a comparative experiment by
masking the joint torque in the state information to verify the
contribution of force feedback. We learned the policy in 5 to
30 times under different conditions and collected the success
rate of the best policy into a chart. From Figure 3, it is evident
that a higher success rate can be obtained when the objects
are grasped with force feedback, which is consistent for most
objects in most cases.
Due to inertia, the dexterous hand tends to maintain its
original motion state. Only when the contact force is large
enough to overcome inertia will the joint torques change,
allowing perception of the contact collision. Lighter objects
provide smaller contact resistance that is difficult to perceive.
To validate the role of joint torques in dexterous manipulation,
we subsequently increased the weight of the object by a factor
of five in follow-up experiments.
As shown in Figure 4, when ϵ = 0.005m, we were able to
learn a high-scoring grasp policy for the power drill and mug
even without force feedback, but failed to do so for the camera
and spray bottle due to the low probability of learning a control
strategy. With force feedback, the success rate for every object
was higher than 70%. Therefore, this experiment demonstrates
that force feedback plays a helpful role in the learning process.
When the object’s initial position has a random deviation of
0.02m, the grasp success rate for most objects reaches 90%
with force feedback, which is much higher than the success
rate without force feedback (<= 35%). This result highlights
the particular importance of force feedback when the initial
position deviation of the object is relatively large.
Additionally, we further verified the ability of force feed-
back to adapt to changes in the object’s position. One hundred
random tests were conducted for each object on the learned
grasping policy, and the results are shown in Figure 5. For
each test, the initial X and Y coordinates of the object were
randomly selected within a range of ±2 cm and plotted on
the chart. A successful grasp was marked with a green circle,
while a failed grasp was marked with a red triangle. Through
multiple experiments, the distribution of successful grasps
can be observed. When force feedback is present, there are
more successful grasps across the entire distribution space
compared to the case without force feedback. In the absence
of force feedback, the successful grasps are concentrated in
a small area, indicating that a robust grasp is only obtained
for a relatively small range of disturbance. This experiment
further confirms that force feedback improves the adaptability
of grasping.
C. Adaptive Functional Grasp
Take the spray bottle as an example shown in Figure 6,
we selected three different initial positions (-0.017, 0.01), (-
0.007, 0.01), and (0.015, 0.01) to create scenes A, B, and
C, respectively, for further observation of the grasping action.
The position of the whole palm was represented using a single
point, and we drew the change curves of the palm position
in the X-axis during the grasping process to observe the
difference between the movement of the hand with and with-
out force feedback(solid line). Additionally, we recorded the
change of the real-time X-axis coordinate of the object’s center
to express object movement(dashed line). The experimental
results demonstrate that in the absence of force feedback, the
dexterous hand performs similar grasping actions for scenes A
Fig. 6.
We observed the grasping process at three fixed positions and compared the difference caused by with and without force feedback. For the three
different scenes represented as A, B, and C, the initial positions of the objects are different, but they all move right along the X-axis. The curve in the figure
shows the real-time X-coordinate values of the palm and object during the movement, which is used to observe the movement process. With force feedback,
an adaptive trajectory is generated, leading to smaller object movement, as marked in the result picture.
Fig. 7.
For objects located in various initial positions, our approach achieved accurate functional grasping through different grasping paths. The figure
illustrates three functional grasping processes for each object.
and B, resulting in a large degree of object movement, while
the grasp fails for scene C. The difference in palm trajectories
between scenes A and B demonstrates a certain degree of
adaptability, which can be attributed to the real-time hand
pose feedback during the hand-object interaction process by
the agent. In contrast, when force feedback is present, the hand
can adjust its action under the guidance of force feedback.
Therefore, for the three grasps at different initial positions of
the object, different adaptive grasping trajectories are formed,
resulting in relatively small object movement.
Figure 7 illustrates the movement of the dexterous hand as it
performs functional grasping of four distinct objects, all guided
by force feedback. The figure highlights the adaptability of our
method in generating diverse grasping trajectories and grasp
types tailored to different initial object positions for each item.
These diverse paths are continually adjusted in real-time as
the hand interacts and collides with the objects, aiming to
minimize object displacement and achieve precise grasps. We
also submitted 4 videos to demonstrate the grasping effect with
or without feedback. It can be observed that the generated
grasps are very similar to those of humans with dexterity,
flexibility, and accuracy.
VI. CONCLUSION AND FUTURE WORK
We have introduced a novel adaptive motion planning
method for multi-fingered functional grasping utilizing force
feedback and analyzed its performance on four distinct objects.
The reinforcement learning model established is capable of
learning policies to adjust actions in real-time based on joint
torque feedback, enabling adaptation to disturbances in ini-
tial object poses. The achieved dexterous grasps demonstrate
promising capabilities of human dexterity for adaptive robotic
manipulation. However, our method still has limitations, as
dexterous hands seek hand-object interaction policies through
trial and error, which are sensitive to initial random explo-
ration. In addition, the objects we manipulate are relatively
heavy compared to objects in real-world environments. We
will continue to improve the manipulation dexterity by incor-
porating more perceptual information in the future.
REFERENCES
[1] R. Newbury, M. Gu, L. Chumbley, A. Mousavian, C. Eppner, J. Leitner,
J. Bohg, A. Morales, T. Asfour, D. Kragic, et al., “Deep learning ap-
proaches to grasp synthesis: A review,” IEEE Transactions on Robotics,
2023.
[2] P. Grady, C. Tang, C. D. Twigg, M. Vo, S. Brahmbhatt, and C. C. Kemp,
“Contactopt: Optimizing contact to improve grasps,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2021, pp. 1471–1481.
[3] H. Jiang, S. Liu, J. Wang, and X. Wang, “Hand-object contact con-
sistency reasoning for human grasps generation,” in Proceedings of
the IEEE/CVF International Conference on Computer Vision, 2021, pp.
11 107–11 116.
[4] T. Zhu, R. Wu, X. Lin, and Y. Sun, “Toward human-like grasp: Dexterous
grasping via semantic representation of object-hand,” in Proceedings of
the IEEE/CVF International Conference on Computer Vision, 2021, pp.
15 741–15 751.
[5] Q. She, R. Hu, J. Xu, M. Liu, K. Xu, and H. Huang, “Learning high-
dof reaching-and-grasping via dynamic representation of gripper-object
interaction,” arXiv preprint arXiv:2204.13998, 2022.
[6] S. Christen, M. Kocabas, E. Aksan, J. Hwangbo, J. Song, and
O. Hilliges, “D-grasp: Physically plausible dynamic grasp synthesis for
hand-object interactions,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2022, pp. 20 577–20 586.
[7] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman,
E. Todorov, and S. Levine, “Learning complex dexterous manipulation
with deep reinforcement learning and demonstrations,” arXiv preprint
arXiv:1709.10087, 2017.
[8] H. Merzi´c, M. Bogdanovi´c, D. Kappler, L. Righetti, and J. Bohg,
“Leveraging contact forces for learning to grasp,” in 2019 international
conference on robotics and automation (ICRA). IEEE, 2019, pp. 3615–
3621.
[9] B. Wu, I. Akinola, J. Varley, and P. Allen, “Mat: Multi-fingered
adaptive tactile grasping via deep reinforcement learning,” arXiv preprint
arXiv:1909.04787, 2019.
[10] A. Koenig, Z. Liu, L. Janson, and R. Howe, “The role of tactile
sensing in learning and deploying grasp refinement algorithms,” in 2022
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS).
IEEE, 2022, pp. 7766–7772.
[11] H. Liang, L. Cong, N. Hendrich, S. Li, F. Sun, and J. Zhang, “Multi-
fingered grasping based on multimodal reinforcement learning,” IEEE
Robotics and Automation Letters, vol. 7, no. 2, pp. 1174–1181, 2021.
[12] T. Lozano-P´erez and L. P. Kaelbling, “A constraint-based method for
solving sequential manipulation planning problems,” in 2014 IEEE/RSJ
International Conference on Intelligent Robots and Systems.
IEEE,
2014, pp. 3684–3691.
[13] A. Orthey and M. Toussaint, “Sparse multilevel roadmaps for high-
dimensional robotic motion planning,” in 2021 IEEE International
Conference on Robotics and Automation (ICRA).
IEEE, 2021, pp.
7851–7857.
[14] A. Orthey, S. Akbar, and M. Toussaint, “Multilevel motion planning: A
fiber bundle formulation,” arXiv preprint arXiv:2007.09435, 2020.
[15] X. Cheng, E. Huang, Y. Hou, and M. T. Mason, “Contact mode guided
motion planning for quasidynamic dexterous manipulation in 3d,” in
2022 International Conference on Robotics and Automation (ICRA).
IEEE, 2022, pp. 2730–2736.
[16] C. Chen, P. Culbertson, M. Lepert, M. Schwager, and J. Bohg, “Tra-
jectotree: Trajectory optimization meets tree search for planning multi-
contact dexterous manipulation,” in 2021 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems (IROS).
IEEE, 2021, pp.
8262–8268.
[17] M. Posa, C. Cantu, and R. Tedrake, “A direct method for trajectory
optimization of rigid bodies through contact,” The International Journal
of Robotics Research, vol. 33, no. 1, pp. 69–81, 2014.
[18] T. Pang, H. T. Suh, L. Yang, and R. Tedrake, “Global planning for
contact-rich manipulation via local smoothing of quasi-dynamic contact
models,” IEEE Transactions on Robotics, 2023.
[19] J. Ye, J. Wang, B. Huang, Y. Qin, and X. Wang, “Learning continuous
grasping function with a dexterous hand from human demonstrations,”
IEEE Robotics and Automation Letters, vol. 8, no. 5, pp. 2882–2889,
2023.
[20] Z. Q. Chen, K. Van Wyk, Y.-W. Chao, W. Yang, A. Mousavian, A. Gupta,
and D. Fox, “Dextransfer: Real world multi-fingered dexterous grasping
with minimal human demonstrations,” arXiv preprint arXiv:2209.14284,
2022.
[21] O. Taheri, V. Choutas, M. J. Black, and D. Tzionas, “Goal: Generating
4d whole-body motion for hand-object grasping,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2022, pp. 13 263–13 273.
[22] A. Patel, A. Wang, I. Radosavovic, and J. Malik, “Learning to imitate ob-
ject interactions from internet videos,” arXiv preprint arXiv:2211.13225,
2022.
[23] S. Dasari, A. Gupta, and V. Kumar, “Learning dexterous manipulation
from exemplar object trajectories and pre-grasps,” in 2023 IEEE Inter-
national Conference on Robotics and Automation (ICRA).
IEEE, 2023,
pp. 3889–3896.
[24] D. Jain, A. Li, S. Singhal, A. Rajeswaran, V. Kumar, and E. Todorov,
“Learning deep visuomotor policies for dexterous hand manipulation,”
in 2019 international conference on robotics and automation (ICRA).
IEEE, 2019, pp. 3636–3643.
[25] P. Mandikal and K. Grauman, “Dexvip: Learning dexterous grasping
with human hand pose priors from video,” in Conference on Robot
Learning.
PMLR, 2022, pp. 651–661.
[26] Y. Xu, W. Wan, J. Zhang, H. Liu, Z. Shan, H. Shen, R. Wang, H. Geng,
Y. Weng, J. Chen, et al., “Unidexgrasp: Universal robotic dexterous
grasping via learning diverse proposal generation and goal-conditioned
policy,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2023, pp. 4737–4746.
[27] Q. Liu, Y. Cui, Z. Sun, H. Li, G. Li, L. Shao, J. Chen, and
Q. Ye, “Dexrepnet: Learning dexterous robotic grasping network with
geometric and spatial hand-object representations,” arXiv preprint
arXiv:2303.09806, 2023.
[28] N. Vulin, S. Christen, S. Stevˇsi´c, and O. Hilliges, “Improved learning of
robot manipulation tasks via tactile intrinsic motivation,” IEEE Robotics
and Automation Letters, vol. 6, no. 2, pp. 2194–2201, 2021.
[29] H. Hu, X. Yi, H. Zhang, J.-H. Yong, and F. Xu, “Physical interaction:
Reconstructing hand-object interactions with physics,” in SIGGRAPH
Asia 2022 Conference Papers, 2022, pp. 1–9.
[30] Y. Qin, B. Huang, Z.-H. Yin, H. Su, and X. Wang, “Dexpoint: Gen-
eralizable point cloud reinforcement learning for sim-to-real dexterous
manipulation,” in Conference on Robot Learning.
PMLR, 2023, pp.
594–605.
[31] B. Sundaralingam, A. S. Lambert, A. Handa, B. Boots, T. Hermans,
S. Birchfield, N. Ratliff, and D. Fox, “Robust learning of tactile force
estimation through robot interaction,” in 2019 International Conference
on Robotics and Automation (ICRA).
IEEE, 2019, pp. 9035–9042.
[32] V. Kumar, T. Hermans, D. Fox, S. Birchfield, and J. Tremblay, “Con-
textual reinforcement learning of visuo-tactile multi-fingered grasping
policies,” arXiv preprint arXiv:1911.09233, 2019.
[33] M. Matak and T. Hermans, “Planning visual-tactile precision grasps via
complementary use of vision and touch,” IEEE Robotics and Automation
Letters, vol. 8, no. 2, pp. 768–775, 2022.
[34] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter,
“A review of tactile information: Perception and action through touch,”
IEEE Transactions on Robotics, vol. 36, no. 6, pp. 1619–1634, 2020.
[35] Y. Chen, T. Wu, S. Wang, X. Feng, J. Jiang, Z. Lu, S. McAleer, H. Dong,
S.-C. Zhu, and Y. Yang, “Towards human-level bimanual dexterous ma-
nipulation with reinforcement learning,” Advances in Neural Information
Processing Systems, vol. 35, pp. 5150–5163, 2022.
[36] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic
actor,” in International conference on machine learning.
PMLR, 2018,
pp. 1861–1870.
[37] M. Laskin, A. Srinivas, and P. Abbeel, “Curl: Contrastive unsupervised
representations for reinforcement learning,” in International Conference
on Machine Learning.
PMLR, 2020, pp. 5639–5650.
[38] R. Wang, J. Zhang, J. Chen, Y. Xu, P. Li, T. Liu, and H. Wang,
“Dexgraspnet: A large-scale robotic dexterous grasp dataset for general
objects based on simulation,” in 2023 IEEE International Conference
on Robotics and Automation (ICRA).
IEEE, 2023, pp. 11 359–11 366.
[39] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for model-
based control,” in 2012 IEEE/RSJ international conference on intelligent
robots and systems.
IEEE, 2012, pp. 5026–5033.
[40] V. Kumar, Z. Xu, and E. Todorov, “Fast, strong and compliant pneumatic
actuation for dexterous tendon-driven hands,” in 2013 IEEE interna-
tional conference on robotics and automation.
IEEE, 2013, pp. 1512–
1519.
"
"Conformal risk control (CRC) is a technique applied post-hoc to a conventional point predictor to provide calibration guarantees. This paper introduces a novel CRC method based on cross-validation, generalizing the jackknife-minmax from conformal prediction to CRC, allowing for the control of a broader range of risk functions.","{'Context and Motivation': 'AI tools used in risk-sensitive fields such as healthcare and engineering require the capacity to quantify their uncertainty. This necessitates guarantees on the adherence of the “error bars” produced by the AI model to the true predictive uncertainty.', 'Contributions': 'This paper introduces a novel cross-validation conformal risk control (CV-CRC) method that generalizes a version of the jackknife-minmax from conformal prediction (CP) to CRC, allowing for the control of a broader range of risk functions. CV-CRC is theoretically shown to offer guarantees on the average risk of the set predictor. Numerical experiments demonstrate that CV-CRC can reduce the average set size with respect to CRC when the available data are limited.'}","nanConformal prediction (CP) is a technique that applies post-hoc to a conventional point predictor to provide calibration guarantees. CP generalizes confidence intervals by constructing predictive sets that contain the true label with a user-specified probability, known as the confidence level. Recent work has extended CP to conformal risk control (CRC), which generalizes CP to losses beyond miscoverage. However, existing CRC methods require the available data set to be split between training and validation data sets, which can be problematic when data availability is limited, resulting in inefficient set predictors.nannan","{'CV-CRC': 'The proposed cross-validation CRC (CV-CRC) extends a version of the jackknife-minmax from CP to CRC, allowing for the control of a broader range of risk functions. CV-CRC is proved to offer theoretical guarantees on the average risk of the set predictor.', 'Implementation': 'We numerically validate the proposed CV-CRC using two synthetic examples: a vector regression problem and a temporal point process prediction problem. Our code is publicly available.'}nan","{'Vector Regression': 'For the vector regression problem, CV-CRC with K = 20 folds is observed to strike a good balance between inefficiency and computational complexity in the small data set size regime.', 'Temporal Point Process Prediction': 'For the temporal point process prediction problem, CV-CRC with K = N reduces the average size of the predicted intervals.'}","CV-CRC was shown to provably control the average risk, with experiments demonstrating it to be more efficient than VB-CRC when the available data for training and calibration are scarce.",Cross-Validation Conformal Risk Control,"Kfir M. Cohen, Sangwoo Park, Osvaldo Simeone, Shlomo Shamai","Cross-Validation Conformal Risk Control
Kfir M. Cohen, Sangwoo Park, Osvaldo Simeone
King’s Communications, Learning, and Information Processing (KCLIP) lab
Centre for Intelligent Information Processing Systems (CIIPS)
Department of Engineering, King’s College London
Shlomo Shamai (Shitz)
Viterbi Faculty of Electrical and Computing Engineering
Technion—Israel Institute of Technology
Abstract—THIS PAPER IS ELIGIBLE FOR THE STUDENT
PAPER AWARD. Conformal risk control (CRC) is a recently
proposed technique that applies post-hoc to a conventional
point predictor to provide calibration guarantees. Generalizing
conformal prediction (CP), with CRC, calibration is ensured for a
set predictor that is extracted from the point predictor to control
a risk function such as the probability of miscoverage or the
false negative rate. The original CRC requires the available data
set to be split between training and validation data sets. This
can be problematic when data availability is limited, resulting
in inefficient set predictors. In this paper, a novel CRC method
is introduced that is based on cross-validation, rather than on
validation as the original CRC. The proposed cross-validation
CRC (CV-CRC) extends a version of the jackknife-minmax from
CP to CRC, allowing for the control of a broader range of risk
functions. CV-CRC is proved to offer theoretical guarantees on
the average risk of the set predictor. Furthermore, numerical
experiments show that CV-CRC can reduce the average set size
with respect to CRC when the available data are limited.
I. INTRODUCTION
A. Context and Motivation
One of the key requirements for the application of artificial
intelligence (AI) tools to risk-sensitive fields such as healthcare
and engineering is the capacity of AI algorithms to quantify
their uncertainty [1], [2]. This requires guarantees on the
adherence of the “error bars” produced by the AI model
to the true predictive uncertainty. The predictive uncertainty
encompasses both the epistemic uncertainty caused by limited
availability of data and the aleatoric uncertainty inherent
in the randomness of data generation [3]. Without making
strong assumptions on the data generation mechanism it is
The work of Kfir M. Cohen, Sangwoo Park and Osvaldo Simeone has
been supported by the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme, grant agreement No.
725731. The work of Osvaldo Simeone has also been supported by an Open
Fellowship of the EPSRC with reference EP/W024101/1, by the European
Union’s Horizon Europe Project CENTRIC under Grant 101096379, and by
Project REASON, a UK Government funded project under the Future Open
Networks Research Challenge (FONRC) sponsored by the Department of
Science Innovation and Technology (DSIT). The work of Shlomo Shamai has
been supported by the German Research Foundation (DFG) via the German-
Israeli Project Cooperation (DIP), under Project SH 1937/1-1. The authors
acknowledge King’s Computational Research, Engineering and Technology
Environment (CREATE). Retrieved January 18, 2024, from https://doi.org/10.
18742/rnvf-m076.
KMC, SP and OS conceived the project; KMC and SP developed the theory
with the supervision and guidance of OS; KMC performed the simulation;
KMC, SP and OS prepared the manuscript; and SS reviewed the text and
contributed to the vision of the paper. All authors discussed the results and
contributed to the final manuscript.
Dtr
Dval
D
predictive set
Γλ (x|Dtr)
ΓVB(x|D)
target risk α
test
input
x
VB
risk control
split
threshold
λ
pair (x, y), we wish to guarantee the calibration condition
ED
D
D,x,y∼p0(D,x,y)

ℓ
Examples of NC scores include the residual between the label
and a trained predictor for regression problems and the log-loss
for classification problems [6], [27], [28]. With the given NC
score, the set prediction is obtained as
Γλ(x|Dtr) =
n
y′ ∈ Y
NC((x, y′)|Dtr) ≤ λ
o
,
(7)
thus including all labels y′ ∈ Y with NC score smaller or equal
to the threshold λ. By design, the set (7) satisfies the nesting
property
λ1 < λ2
⇒
Γλ1(x|Dtr) ⊆ Γλ2(x|Dtr)
(8)
for any input x and data sets Dtr.
We define the risk as the population, or test, loss of the
predicted set (7) as
R(λ|Dtr) = Ex,y∼p0(x,y)
h
ℓ
Theorem 1 requires the NC score to be permutation-invariant
with respect to the data points in the training set, which is not
the case for VB-CRC. Permutation-invariance is also needed
for CV-CP [9], as well as for full CP [5]. In practice, a
permutation-invariant NC score can be obtained by imple-
menting permutation-invariant training schemes such as full
gradient descent, in which the final trained model does not
depend on the ordering of the training data points.
IV. EXAMPLES
In this section, we numerically validate the proposed CV-
CRC using two synthetic examples. The first is a vector
regression problem, whereas the second concerns the problem
of temporal point process prediction [29], [30]. Our code is
publicly available1.
A. Vector Regression
Inspired by the example in [9], we first investigate a
vector regression problem in which the output variable y =
[y1, . . . , ym]⊤ is m-dimensional. The joint distribution of data
set D and test pair (x, y) is obtained as
p0(D, x, y)=
Z
p0(ϕ)
 N+1
Y
i=1
p0(x[i])p0(y[i]|x[i], ϕ)

dϕ,
(19)
where (x[N + 1] = x, y[N + 1] = y) is the test example, and
we have the Gaussian distributions
p0(x)
=
N(x|0, d−1Id),
(20a)
p0(y|x, ϕ)
=
N(y|ϕ⊤ · x, β−1
0 Im),
(20b)
while p0(ϕ) is a mixture of Gaussians with means determined
by an i.i.d. Bernoulli vector b as
p0(ϕ) = Eb
i.i.d.
∼ Bern(0.5)

N(ϕ|µ0b, γ−1
0 Id)

.
(21)
We set µ0 = 10, γ0 = 1, β0 = 4, d = |X| = 50, and
m = |Y| = 30. Note that the distribution (19) is exchangeable.
Using maximum-likelihood learning, given a training data
set Dtr, we obtain the model parameter ϕML
Dtr used for the linear
prediction model ˆy(x|Dtr) = (ϕML
D )⊤x as ϕML
Dtr = X†
DtrYDtr,
where (·)† denotes the pseudo-inverse, (·)⊤ denotes transpose,
and the input and label data matrices XD ∈ RN×d and YD ∈
RN×m have input (xtr[i])⊤ and label (ytr[i])⊤ as their ith rows,
respectively.
The NC score is set to the maximum prediction residual
across the m dimensions of the output variable y as
NC((x, y)|Dtr) = 2
y − ˆy(x|Dtr)

∞,
(22)
where the infinity norm
 ·

∞ returns the largest magnitude
of its input vector. This results in predictive sets (12) and (14)
with (17) in the form of Γ = Γ1 × · · · × Γm, with × being the
Cartesian product and
ΓVB
j
=
n
yj
 |yj − [ˆy(x|Dtr)]j| ≤ λVB(Dval|Dtr)/2
o
(23)
1https://github.com/kclip/cvcrc
100
200
300
400
500
0
0.02
0.04
0.06
0.08
0.1
0.12
Fig. 2. Empirical risk of VB-CRC and CV-CRC for the vector regression
problem.
100
200
300
400
500
2
4
8
10
20
40
50
Fig. 3.
Empirical inefficiency of VB-CRC and CV-CRC for the vector
regression problem.
with [·]j standing for the jth element of its argument for VB-
CRC, and
ΓCV
j
=
K
[
k=1
n
yj
 |yj − [ˆy(x|D−k)]j| ≤ λCV(D)/2
o
(24)
for CV-CRC. The loss function used in the risk (1) is defined
as
ℓ(y, Γ) = 1
m
m
X
j=1
1
t1
t2 . . .
td
td+1
td+2 . . .
td+m
point process
set predictor
desired risk α
past events x ∈ Rd
predicted set
Γ(x|D) ⊆ Rm
time
Γ1
D
Γm
. . .
future events y ∈ Rm
Fig. 4. Temporal point process prediction: After observing the past d times
t1, . . . , tn, a point process set predictor outputs predictive intervals Γj(x|D)
for each of the next m points with j = 1, . . . , m.
For target risk α = 0.1, the empirical risk and empirical
inefficiency of N te = 200 test covariate-output pairs, averaged
over 50 independent simulations, are shown in Fig. 2 and Fig. 3.
Fig. 2, validates the theoretical result that CRC schemes satisfy
condition (1). However, from Fig. 3, VB-CRC is observed to
have a larger inefficiency than CV-CRC, particularly in the
small data set size regime. Thus, CV-CRC uses data more
efficiently, with K = 20 folds striking a good balance between
inefficiency and computational complexity in this regime.
B. Temporal Point Process Prediction
A temporal process consists of a sequence of events at
random times t1, t2, . . . with t1 < t2 < . . . As illustrated in
Fig. 4, given the past d events’ timings x = {t1, . . . , td}, the
goal is to output intervals Γj(x|D) for each of the following
m events with j = 1, . . . , m. The loss function is defined as
in (25).
Data and test sequences of timings are generated following
a self-exciting Hawkes process [31] with intensity function
λ(t|Ht) = µ +
X
i:ti<t

α1β1e−β1(t−ti) + α2β2e−β2(t−ti)
,
with µ = 0.2, α1 = α2 = 0.4, β1 = 1 and β2 = 20 [29]. The
predictor is a recurrent neural network that outputs a predictive
density function p(ti+1|t1, . . . , ti, ϕDtr) with trained parameter
ϕDtr [29]. The median ˆti+1(t1, . . . , ti, ϕDtr) of the predictive
distribution is used as the point estimate for the (i + 1)-th
event. For i > d, estimates {ˆtj}i−1
j=d+1 are used in lieu of the
correct timings in the point prediction.
VB-CRC (12) produces intervals
ΓVB
j
=
n
yj
 |yj − ˆtd+j(Dtr)| ≤ γjλVB(Dval|Dtr)/2
o
,
(26)
where multiplication by the interval common ratio γ = 1.2
increases the interval sizes for later predictions, and for the
CV-CRC (14), we have
ΓCV
j
=
K
[
k=1
n
yj
 |yj − ˆtd+j(D−k)| ≤ γjλCV(D)/2
o
.
(27)
We set the length of the observed sequence as d = 60, and
predict the next m = 6 events. We allow one event on average
20
30
40
50
60
70
0.1
0.15
0.2
20
30
40
50
60
70
10
15
20
25
30
Fig. 5. Empirical risk (top) and inefficiency (bottom) of VB-CRC and N-CV-
CRC for the temporal point process prediction problem.
to lie outside the predicted intervals, i.e., α = 1/6. We average
over 200 independent simulations with N te = 1000 test points
in each run.
The top panel of Fig. 5 illustrate the test risk (25) as function
of data set size N, validating that both scheme attain risks
lower than the desired level α. The bottom panel of the figure
shows that CV-CRC with K = N reduces the average size of
the predicted intervals.
V. CONCLUSION
In this paper, we have introduced a novel conformal risk
control (CRC) scheme based on cross-validation, generalizing
cross-validation CP to losses beyond miscoverage. The pro-
posed CV-CRC was shown to provably control the average risk,
with experiments demonstrating it to be more efficient than VB-
CRC when the available data for training and calibration are
scarce. Further work may consider using the jackknife+ of [9]
instead of the jackknife-minmax for more efficient predictive
sets; and extending the scheme to meta-learning [32].
5
REFERENCES
[1] D. Tran, J. Liu, M. W. Dusenberry, D. Phan, M. Collier, J. Ren, K. Han,
Z. Wang, Z. Mariet, H. Hu et al., “Plex: Towards reliability using
pretrained large model extensions,” arXiv preprint arXiv:2207.07411,
2022.
[2] B. Rajendran, O. Simeone, and B. M. Al-Hashimi, “Towards efficient and
trustworthy ai through hardware-algorithm-communication co-design,”
arXiv preprint arXiv:2309.15942, 2023.
[3] O. Simeone, Machine Learning for Engineers.
Cambridge University
Press, 2022.
[4] J. Lei and L. Wasserman, “Distribution-Free Prediction bands for non-
parametric Regression,” Journal of the Royal Statistical Society Series
B: Statistical Methodology, vol. 76, no. 1, pp. 71–96, 2014.
[5] V. Vovk, A. Gammerman, and G. Shafer, Algorithmic Learning in a
Random World.
Springer, 2005, springer, New York.
[6] A. N. Angelopoulos and S. Bates, “Conformal Prediction: A Gentle
Introduction,” Foundations and Trends® in Machine Learning, vol. 16,
no. 4, pp. 494–591, 2023.
[7] A. N. Angelopoulos, S. Bates, A. Fisch, L. Lei, and T. Schuster,
“Conformal Risk Control,” arXiv preprint arXiv:2208.02814, 2022.
[8] M. Fontana, G. Zeni, and S. Vantini, “Conformal Prediction: A Unified
Review of Theory and New Challenges,” Bernoulli, vol. 29, no. 1, pp. 1
– 23, 2023. [Online]. Available: https://doi.org/10.3150/21-BEJ1447
[9] R. F. Barber, E. J. Candes, A. Ramdas, and R. J. Tibshirani, “Predictive
Inference with the Jackknife+,” The Annals of Statistics, vol. 49, no. 1,
pp. 486–507, 2021.
[10] K. M. Cohen, S. Park, O. Simeone, and S. Shamai Shitz, “Calibrating AI
Models for Wireless Communications via Conformal Prediction,” IEEE
Transactions on Machine Learning in Communications and Networking,
vol. 1, pp. 296–312, 2023.
[11] N. Deutschmann, M. Rigotti, and M. R. Martinez, “Adaptive Con-
formal Regression with Jackknife+ Rescaled Scores,” arXiv preprint
arXiv:2305.19901, 2023.
[12] C. Gupta, A. K. Kuchibhotla, and A. Ramdas, “Nested Conformal Predic-
tion and Quantile Out-Of-Bag Ensemble Methods,” Pattern Recognition,
vol. 127, p. 108496, 2022.
[13] D. Stutz, K. D. Dvijotham, A. T. Cemgil, and A. Doucet, “Learning
Optimal Conformal Classifiers,” in International Conference on Learning
Representations, 2021.
[14] B.-S. Einbinder, Y. Romano, M. Sesia, and Y. Zhou, “Training
Uncertainty-Aware Classifiers with Conformalized Deep Learning,”
Advances in Neural Information Processing Systems, vol. 35, pp. 22 380–
22 395, 2022.
[15] R. J. Tibshirani, R. Foygel Barber, E. Candes, and A. Ramdas, “Conformal
Prediction under Covariate Shift,” Advances in Neural Information
Processing Systems, vol. 32, 2019.
[16] Y. Yang and A. K. Kuchibhotla, “Finite-Sample Efficient Conformal
Prediction,” arXiv preprint arXiv:2104.13871, 2021.
[17] A. Kumar, S. Sarawagi, and U. Jain, “Trainable Calibration Measures
for Neural Networks from Kernel Mean Embeddings,” in International
Conference on Machine Learning.
PMLR, 2018, pp. 2805–2814.
[18] M. J. Holland, “Making Learning More Transparent using Conformalized
Performance Prediction,” arXiv preprint arXiv:2007.04486, 2020.
[19] A. Perez-Lebel, M. Le Morvan, and G. Varoquaux, “Beyond Calibration:
Estimating the Grouping Loss of Modern Neural Networks,” in ICLR
2023–The Eleventh International Conference on Learning Representa-
tions, 2023.
[20] S. Park and O. Simeone, “Quantum Conformal Prediction for Reliable
Uncertainty Quantification in Quantum Machine Learning,” IEEE Trans-
actions on Quantum Engineering, no. 01, pp. 1–24, nov 2023.
[21] I. Gibbs and E. Candes, “Adaptive Conformal Inference Under Distribu-
tion Shift,” Advances in Neural Information Processing Systems, vol. 34,
pp. 1660–1672, 2021.
[22] S. Feldman, L. Ringel, S. Bates, and Y. Romano, “Achieving Risk
Control in Online Learning Settings,” Transactions on Machine Learning
Research, 2023. [Online]. Available: https://openreview.net/forum?id=
5Y04GWvoJu
[23] Y. Zhang, S. Park, and O. Simeone, “Bayesian Optimization with Formal
Safety Guarantees via Online Conformal Prediction,” arXiv preprint
arXiv:2306.17815, 2023.
[24] K. M. Cohen, S. Park, O. Simeone, P. Popovski, and S. Shamai,
“Guaranteed dynamic scheduling of ultra-reliable low-latency traffic via
conformal prediction,” IEEE Signal Processing Letters, vol. 30, pp. 473–
477, 2023.
[25] S. Bates, A. Angelopoulos, L. Lei, J. Malik, and M. Jordan, “Distribution-
Free, Risk-Controlling Prediction Sets,” Journal of the ACM (JACM),
vol. 68, no. 6, pp. 1–34, 2021.
[26] A. N. Angelopoulos, S. Bates, E. J. Candès, M. I. Jordan, and L. Lei,
“Learn Then Test: Calibrating Predictive Algorithms to Achieve Risk
Control,” arXiv preprint arXiv:2110.01052, 2021.
[27] A. N. Angelopoulos, S. Bates, M. I. Jordan, and J. Malik, “Uncertainty
Sets for Image Classifiers using Conformal Prediction,” in 9th
International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021.
OpenReview.net, 2021. [Online].
Available: https://openreview.net/forum?id=eNdiU_DbM9
[28] Y. Romano, E. Patterson, and E. Candes, “Conformalized Quantile
Regression,” Advances in Neural Information Processing Systems, vol. 32,
2019.
[29] T. Omi, N. Ueda, and K. Aihara, “Fully Neural Network Based Model
for General Temporal Point Processes,” in Proceedings of the 33rd
International Conference on Neural Information Processing Systems,
2019, pp. 2122–2132.
[30] M. Dubey, R. Palakkadavath, and P. Srijith, “Bayesian Neural Hawkes
Process for Event Uncertainty Prediction,” International Journal of Data
Science and Analytics, pp. 1–15, 2023.
[31] A. G. Hawkes, “Spectra of Some Self-Exciting and Mutually Exciting
Point Processes,” Biometrika, vol. 58, no. 1, pp. 83–90, 1971.
[32] S. Park, K. M. Cohen, and O. Simeone, “Few-Shot Calibration of
Set Predictors via Meta-Learned Cross-Validation-Based Conformal
Prediction,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, no. 01, pp. 1–13, oct 2023.
[33] A. K. Kuchibhotla, “Exchangeability, Conformal Prediction, and Rank
Tests,” arXiv preprint arXiv:2005.06095, 2020.
[34] A. Dean and J. Verducci, “Linear Transformations that Preserve Ma-
jorization, Schur Concavity, and Exchangeability,” Linear Algebra and
Its Applications, vol. 127, pp. 121–138, 1990.
6
Cross-Validation Conformal Risk Control:
Supplementary Material
APPENDIX A
PROOF THAT VB-CRC ACHIEVES TARGET RISK
In this appendix, we prove condition (13) for VB-CRC. While this result was originally shown in [7], here we provide an
equivelant proof that is more convenient to support the proof of Theorem 1 in Appendix C. We start by bounding the VB-CRC
threshold (11) using the following steps
λVB(Dval|Dtr)
=
inf
λ
(
λ

1
N val+1
 N val
X
i=1
ℓ
where the sum is over the set ΠM of all M! permutations. For example, three Bernoulli variables v1, v2, v3 ∼
i.i.d. Bern(q) with
parameter q ∈ [0, 1] can constitute four different bags. In fact, bag u
bag
= Hv1, v2, v3I equals H0, 0, 0I with probability (w.p.)
(1 − q)3, H0, 0, 1I w.p. 3(1 − q)2q, H0, 1, 1I w.p. 3(1 − q)q2, and H1, 1, 1I w.p. q3.
With these definitions, we obtain the following chain of inequalities
Ev1:M∼p0(v1:M)[vm]
=
Eu∼p0(u)
h
Ev1:M∼p0(v1:M|u)
h
vm
Hv1, ..., vMI
bag
= u
ii
(34a)
=
Eu∼p0(u)
h
1
M
M
X
l=1
ul
i
(34b)
=
Ev1:M∼p0(v1:M)
h
Eu∼p0(u|v1:M)
h
1
M
M
X
l=1
ul
u
bag
= Hv1, . . . , vMI
ii
(34c)
=
Ev1:M∼p0(v1:M)
h
Eu∼p0(u|v1:M)
h
1
M
M
X
l=1
vl
u
bag
= Hv1, . . . , vMI
ii
(34d)
=
Ev1:M∼p0(v1:M)
h
1
M
M
X
l=1
vl
i
(34e)
=
α.
(34f)
The inequalities of (34) are justified as follows: (34a) and (34c) stem from the law of iterated expectations over all possible
bags of M items; (34b) arises from the fact that each item in the bag has an equal likelihood to be the realization of the m-th
variable vm; is again the law of iterated expectation with the reintroduction of the random vector; (34d) stems from the fact
that if two bags have the same items, their sum is identical; (34e) leverages the fact that the bag given its random variables is a
deterministically specified; and lastly, (34f) is by the assumption in Lemma 2. This concludes the proof of Lemma 2.
APPENDIX C
PROOF OF THEOREM 1
To prove Theorem 1, let us introduced an augmented data set, such that the last, (K + 1)-th, fold Dte = DK+1 is composed
of N/K arbitrary test points
˜D =

D1, D2, . . . , DK
|
{z
}
=D
, DK+1
| {z }
=Dte
	
(35)
with the test point (x, y) included as the first point in the test set, i.e., (x, y) = (xte[1], yte[1]) = (xK+1[1], yK+1[1]). By
construction, all N + N/K points in the augmented data set ˜D are exchangeable and distributed according to joint distribution
p0( ˜D) = p0(D, Dte). We denote the elements of the augmented set ˜D in (35) as
(˜xk[j], ˜yk[j])
=
(xk[j], yk[j]) for k ∈ {1, . . . K}
(36a)
(˜xK+1[j], ˜yK+1[j])
=
(xte[j], yte[j]).
(36b)
Note that the augmented set ˜D in (35) is different than the augmented set using dummy points Daug (16). For a pair of folds
indices k′, k ∈ {1, . . . , K + 1} with k ̸= k′, we also define the augmented leave-two-folds-out (L2O) set as the augmented set
without the two indexed folds, i.e.,
˜D−(k′,k) = ˜D \ {Dk′, Dk}.
(37)
As a special case, when one of the indices points to the (K + 1)-th fold, which is the test fold, the L2O reduces to the
leave-one-out of the available data set ˜D−(K+1,k) = D−k. For every fold within the augmented data set ˜D (35), we evaluate
the average L2O loss (37), minimized over the second fold index as
ˆRCV
L2O(λ| ˜D) =
1
K+1
K+1
X
k=1
K
N
N/K
X
j=1
min
k′∈{1,...,K+1}\{k}
n
ℓ
Corollary 4. The L2O threshold λCV
L2O( ˜D) in (39) is fold-permutation-invariant, i.e., for any of the (K + 1)! possible fold-
permutation mappings π, we have
λCV
L2O
APPENDIX D
PROOF OF LEMMA 5
The proof of Lemma 5 stated in Appendix C follows the steps
λCV
L2O( ˜D) = inf
λ
(
λ

1
K+1
K+1
X
k=1
K
N
N/K
X
j=1
min
k′∈{1,...,K+1}\{k}
n
ℓ
"
"With the rise of cost-effective consumer electronics and cloud computing, Internet-of-Things (IoT) devices are widely deployed for supporting next-generation distributed systems such as smart cities and industrial control systems. IoT devices often face cyber attacks. Intrusion Detection Systems (IDSs) have emerged as an effective way of securing IoT networks by monitoring and detecting anomalous activities. Existing methods, however, rely on centralized servers and encounter high response time and operating costs. Moreover, sharing behavior data can violate on-device privacy. Federated Learning (FL) is proposed to collaboratively train a decentralized shared model of IDS without exposing training data to others. In addition, an effective method called Federated Learning Ensemble Knowledge Distillation (FLEKD) is proposed to mitigate the heterogeneity problems across various clients. Experiment results on the public dataset CICIDS2019 demonstrate the outperformance of the proposed approach against local training and traditional FL in terms of speed and performance and significant improvement on detecting unknown attacks. The performance of the proposed framework is also evaluated in three scenarios and FLEKD is found to have a clear advantage.","With digitization, our daily lives are gradually changing by ubiquitous Internet of Things (IoT) devices. These devices record and upload our personal information from day to night, thus generating exponential data every day for every person. Big data and edge computing devices brought numerous applications such as intelligent industrial control systems and smart cities. Due to the openness of the IoT network deployment environment and weak security control protocol operation caused by limited computing capabilities, IoT devices are prone to network attacks. Attackers may comprise edge devices or databases through the network to steal or tamper with data, causing significant social and economic impact. Thus, IDS is considered for enhancing IoT security. IDS can notify an administrator of potential cyber attacks by analyzing network traffic packets or system logs. However, traditional rule-based IDSs are ineffective against novel attacks. Big data is another challenge for the rule-based IDSs. Machine Learning (ML) and Deep Learning (DL) are effective techniques for constructing automatic IDSs. However, these techniques also bring challenges such as poor performance due to constrained computation ability and insufficient data caused by local training, and privacy concern due to transferring raw data to a central server for collaborative training. To address these challenges, FL is used to develop an on-device collaborative deep intrusion detection model for edge devices in IoT networks along with FLEKD to mitigate the negative influence of clients' heterogeneity.","nanIDS is a critical cybersecurity tool in safeguarding the security and integrity of networked systems by monitoring network traffic packets or system logs to identify anomalous behaviour or patterns that may indicate unauthorized use, misuse, or abuse. IDS also plays a crucial role in the context of IoT security. Traditional rule-based IDSs are effective in detecting known threats, but may not be as capable as ML/DL-based IDSs in analyzing vast volumes of data efficiently and detecting emerging attacks. FL is designed for collaborative training of aggregated models with multiple devices or nodes without sharing raw data. FL has several advantages over centralized ML and DL models: 1) Keep raw data in the owner, 2) Mitigate data scarcity, and 3) Train with heterogeneous data. FL has been utilized in the development of models to combat unknown attacks, achieve efficient anomaly detection, and realize privacy-enhancing FL-based intrusion detection. A practical challenge of FL models is to maintain high performance when dealing with heterogeneous data collected from different devices. However, limited FL-IDS models have comprehensively considered this issue. Local model heterogeneity brought about by heterogeneous data will cause these problems: 1) slow convergence, 2) bias in the model, and 3) poor model accuracy.nannan","FLEKD includes two primary components: Knowledge Distillation and FLEKD: Using Ensemble Knowledge Distillation for IDS in Heterogeneous IoT Networks. Knowledge distillation transfers the knowledge of a large, complex model (teacher) to a smaller, simpler model (student). The process of knowledge transfer usually needs a proxy dataset as the medium. The class probabilities of the model's last layer (logits) or feature representations of middle hidden layers (feature maps) are usually used as the soft targets for knowledge transfer. For the original knowledge distillation, loss typically consists of two terms: a standard cross-entropy loss term and a distillation loss term. The distillation loss term is often formulated as the Kullback-Leibler (KL) divergence between the teacher and student's softmax output probabilities. In FLEKD, federated learning is used as a privacy-preserving collaborative training paradigm for the IDS model. The conventional FL algorithm involves three steps: 1) Synchronizing the current global model parameters to maintain consistency among each client, 2) Updating the local model parameters on private data using Adam or SGD as optimizer, and 3) Transmitting the models of each client to the server side and integrating them by using a specific aggregation algorithm. A dynamic weight ensemble method is proposed to effectively utilize heterogeneous IDS data in FLEKD. The dynamic weight for each client at this round is generated by testing each client model to obtain its score on the test set. The student model acquires knowledge from logits ensembled overall the received teacher models, and thus mutually beneficial information can be shared. In the next round, each activated client receives the corresponding fused prototype model.nan","The main results of the study show that for each setting, the number of communication rounds required to achieve a 95% F1-score was reduced by one when using the FLEKD method, indicating improved efficiency. In addition, the convergence speed of FLEKD is notably faster than that of the original FL under the same conditions, allowing for faster achievement of optimal final results, demonstrating the effectiveness of FLEKD. Furthermore, the proposed ensemble knowledge distillation fine-tuning further enhances the performance of the central model and accelerates the convergence rate.","FLEKD outperforms local training and traditional FL methods in terms of both speed and performance on the CICIDS2019 dataset. FLEKD demonstrates clear superiority in handling heterogeneous data and achieves better results over three real-world scenarios. Overall, FLEKD offers a promising solution for IoT networks' security.",Effective Intrusion Detection in Heterogeneous Internet-of-Things Networks via Ensemble Knowledge Distillation-based Federated Learning,"Jiyuan Shen, Wenzhuo Yang, Zhaowei Chu, Jiani Fan, Dusit Niyato, Kwok-Yan Lam","Effective Intrusion Detection in Heterogeneous
Internet-of-Things Networks via Ensemble
Knowledge Distillation-based Federated Learning
Jiyuan Shen1, Wenzhuo Yang1*, Zhaowei Chu2, Jiani Fan2, Dusit Niyato2, Kwok-Yan Lam1,2*
1Strategic Centre for Research in Privacy-Preserving Technologies & Systems, Nanyang Technological University, Singapore
2School of Computer Science and Engineering, Nanyang Technological University, Singapore
{jiyuan001, zhaowei001, jiani001}@e.ntu.edu.sg, {wenzhuo.yang, dniyato, kwokyan.lam}@ntu.edu.sg
Abstract—With the rapid development of low-cost consumer
electronics and cloud computing, Internet-of-Things (IoT) devices
are widely adopted for supporting next-generation distributed
systems such as smart cities and industrial control systems. IoT
devices are often susceptible to cyber attacks due to their open
deployment environment and limited computing capabilities for
stringent security controls. Hence, Intrusion Detection Systems
(IDS) have emerged as one of the effective ways of securing
IoT networks by monitoring and detecting abnormal activities.
However, existing IDS approaches rely on centralized servers to
generate behaviour profiles and detect anomalies, causing high
response time and large operational costs due to communication
overhead. Besides, sharing of behaviour data in an open and dis-
tributed IoT network environment may violate on-device privacy
requirements. Additionally, various IoT devices tend to capture
heterogeneous data, which complicates the training of behaviour
models. In this paper, we introduce Federated Learning (FL) to
collaboratively train a decentralized shared model of IDS, without
exposing training data to others. Furthermore, we propose an
effective method called Federated Learning Ensemble Knowledge
Distillation (FLEKD) to mitigate the heterogeneity problems
across various clients. FLEKD enables a more flexible aggrega-
tion method than conventional model fusion techniques. Experi-
ment results on the public dataset CICIDS2019 demonstrate that
the proposed approach outperforms local training and traditional
FL in terms of both speed and performance and significantly
improves the system’s ability to detect unknown attacks. Finally,
we evaluate our proposed framework’s performance in three
potential real-world scenarios and show FLEKD has a clear
advantage in experimental results.
Index Terms—Intrusion Detection System, Federated Learn-
ing, Internet of Things, Knowledge Distillation, Data Hetero-
geneity
I. INTRODUCTION
In the era of digitalization, our daily life is gradually
changed by ubiquitous Internet of Things (IoT) devices. These
devices record and upload our personal information from day
to night, thus generating exponential data every day for every
person. By integrating with big data and edge computing
devices, a lot of promising applications have emerged, such as
intelligent industrial control systems and smart cities. Due to
the openness of the IoT network deployment environment and
the weak security control protocol operation ability caused by
limited computing capabilities, IoT devices are very vulnerable
to network attacks. Attackers may comprise edge devices or
databases through the network to steal or tamper with data,
Fig. 1. The overview of the FLEKD-IDS framework. ①: Train local model.
②: Transmit local models to server. ③: Fuse clients’ models. ④: Ensemble
knowledge distillation and fine-tune the global model. ⑤: Distribute the latest
global model.
causing great social and economic impact [1], [2]. Hence,
security monitor detection measures should be considered to
enhance IoT security.
As one of the most important security posture monitoring
tools, the Intrusion Detection System (IDS) has attracted
much attention in the context of IoT [3]. IDS can notify an
administrator of potential cyber attacks by analyzing network
traffic packets or system logs. Traditional IDS systems rely
on rule-based approaches [4] that are not effective to detect
novel attacks. Furthermore, big data is another challenge for
the rule-based IDSs. With the growth of IoT networks and
increasing complexity of attacks, Machine Learning (ML)
and Deep Learning (DL) methods have emerged as effective
techniques for constructing automatic IDSs
[5], [6]. These
techniques enable the system to learn from vast amounts
of behaviour data, establish specific models efficiently, and
identify attacking patterns that are difficult to detect manually.
However, the use of ML and DL in IDS for IoT networks
comes with its own set of challenges. First, training models
by local devices themselves will lead to poor performance
because of the constrained computation ability and insufficient
data. Second, the communication overhead is large and will
cause privacy concerns to transferring raw data of all devices
to a central server for collaborative training. Another challenge
is handling the heterogeneous data collected by different
devices [7]. The data distribution, number of samples, and
collection time on each edge device may be different. Besides,
arXiv:2401.11968v1  [cs.CR]  22 Jan 2024
due to system updates or different device functions, the feature
dimensions and attack types of data collected from different
devices/clients may also be inconsistent. This heterogeneity
further complicates the centralized ML and DL models for
IoT security detection.
To address the first two concerns, we utilize an on-device
privacy-preserving distributed machine learning paradigm,
Federated Learning (FL). Instead of sharing local privacy
datasets directly, FL can collaboratively train a global IDS
model by transmitting local model parameters. Then, the
central server aggregates local model parameters into a global
one. Finally, the central server will dispatch the updated
parameters to all local clients. The model will stop after
the specified number of such communication synchronization
rounds or after the model converges.
To mitigate the negative impacts, including slow model
convergence speed, poor model prediction ability, etc, brought
by data heterogeneity on diverse devices, we propose an
effective heterogeneous federated learning framework using
ensemble knowledge distillation (FLEKD), as shown in Fig-
ure 1. Our scheme leverages mutually exclusive and unlabeled
data to aggregate and transfer knowledge from all received
heterogeneous client models toward a global model. This
ensemble knowledge distillation technique allows a flexible
aggregation method that can reduce the impact of clients’
heterogeneity while not disclosing users’ privacy and also can
accelerate the model convergence speed. Empirical experiment
results demonstrated that our proposed framework can not only
achieve accurate and timely intrusion detection but also narrow
the knowledge gaps among heterogeneous clients. Our main
contributions are summarized as follows:
• We introduce an FL framework to develop an on-device
collaborative deep intrusion detection model for edge
devices in IoT networks.
• We propose a dynamic weight ensemble knowledge dis-
tillation scheme (FLEKD) to assist in mitigating the neg-
ative influence of clients’ heterogeneity without violating
users’ personal privacy.
• We conduct extensive experiments on the public dataset
CICIDS2019 to demonstrate better detection performance
and improved ability to identify unknown attacks over
local training models and naive FL global models.
• We assess the performance of our proposed framework in
three possible real-world scenarios, namely, diverse data
features, sample quantity, and missing certain classes. Our
experimental results demonstrate that FLEKD exhibits
clear and strong advantages.
II. BACKGROUND AND RELATED WORK
A. Intrusion Detection System
The Intrusion Detection System (IDS) is a critical cyberse-
curity tool that plays a vital role in safeguarding the security
and integrity of networked systems. By monitoring network
traffic packets or system logs, IDS can identify anomalous be-
haviour or patterns that may indicate unauthorized use, misuse,
or abuse [1] and alert system administrators of potential secu-
rity breaches in real-time. IDS also performs critical impact in
the context of IoT security [8]. Traditional IDSs are rule-based
or signature-based, which are effective for detecting known
threats [4]. However, they may not be as advanced as ML/DL-
based IDSs in analyzing large volumes of data in an efficient
manner and detecting emerging attacks [5], [6]. In recent years,
there are also research efforts investigating emerging ML or
DL paradigms to improve specific performance of IDS models,
such as improving model adaptability to handle heterogeneous
data by collaborative learning [9] and reducing data annotation
pressure by weakly supervised learning [10].
B. Federated Learning for Intrusion Detection
Federated learning is designed for collaborative training of
aggregated models with multiple devices or nodes without
sharing raw data [11]. FL is a distributed paradigm that has
the following advantages over centralized ML and DL models:
• Keep raw data in the owner: In FL, devices/clients can
maintain data privacy by training local models without
sharing their own data with the central server.
• Mitigate data scarcity: FL can access a much larger and
more diverse dataset, which can help overcome issues re-
lated to limited data on some devices. The generalization
ability of the aggregated model and the prediction ability
of unknown items for the clients will be improved.
• Train with heterogeneous data: FL makes it possible
to train models with large quantities of diverse data
(features, sample quantity, and distribution, as well as
the types of data that may be different) from multiple
sources.
FL has demonstrated its usefulness in the development of
models to combat unknown attacks [12], achieve efficient
anomaly detection [13], [14], and realize privacy-enhancing
FL-based intrusion detection [15], [16]. Moreover, FL has con-
tributed to the development of various other IDSs, highlighting
its potential as a valuable approach in the field of cybersecurity
and IoT security [7], [17].
A practical challenge of FL models is to maintain high
performance when dealing with heterogeneous data collected
from different devices. However, a limited number of FL-
IDS models have comprehensively considered this issue. Local
model heterogeneity brought about by heterogeneous data will
cause these problems: 1) slow convergence, 2) bias in the
model, and 3) poor model accuracy. Therefore, in this paper,
we propose an effective and efficient FL framework to address
the data heterogeneity (diverse feature dimensions, sample
quantity and distribution, and different types of data) for IDS
in IoT security.
III. AN EFFECTIVE HETEROGENEOUS INTRUSION
DETECTION SYSTEM FRAMEWORK
A. Knowledge Distillation
Knowledge distillation, first proposed by Hinton [18], al-
lows transferring the knowledge of a large, complex model
(known as the teacher) to a smaller, simpler model (known
as the student). The motivation behind knowledge distillation
is to train the student model to mimic the behaviour of the
teacher model. The process of knowledge transfer usually
needs a proxy dataset as the medium. As a result, most of
the works will choose a mutually exclusive dataset while
some others may use an autoencoder or GAN to generate
some synthetic dataset. Once the proxy dataset is chosen, the
class probabilities of the model’s last layer (logits) or feature
representations of middle hidden layers (feature maps) are
usually used as the soft targets for knowledge transfer. The
reason is that they contain more valuable information than the
hard labels used in normal training.
For the original knowledge distillation, loss typically con-
sists of two terms: a standard cross-entropy loss term and a
distillation loss term. The former uses the hard label as the
target while the latter uses a soft target. For the federated
learning scenario, we usually do not have the publicly labelled
dataset. Therefore, the combined loss is reduced to only
the distillation loss term. The distillation loss term is often
formulated as the Kullback-Leibler (KL) divergence between
the teacher and student’s softmax output probabilities, shown
in Equation (1).
σ(zi) =
exp (zi/T)
P
j exp (zj/T)
LKL(S∥T) =
X
Si(σ(z)) log
Si(σ(z))
Ti(σ(z))

(1)
where S and T represent the student and teacher logits
respectively. σ is the softmax function and a temperate T is
added.
B. FLEKD: Using Ensemble Knowledge Distillation for IDS
in Heterogeneous IoT Networks
We introduce federated learning as a privacy-preserving
collaborative training paradigm for the IDS model. The con-
ventional FL algorithm commonly involves three steps: First,
synchronizing the current global model parameters to maintain
consistency among each client. Second, updating the local
model parameters on private data using Adam or SGD as
optimizer. Third, transmitting the models of each client to
the server side and integrating them by using a specific
aggregation algorithm in Equation (2).
Wt+1 = Wt + 1
Ct
Ct
X
i=1
F i
t+1
(2)
where
1
Ct
PCt
i=1 F i
t+1 denotes the average aggregation of client
models F i
t+1. These three steps constitute a loop until the
global model converges, as shown in steps 3-8 of Algorithm 1.
Usually, average or weighted average algorithms are
used for server-side aggregation. However, many research
works [19], [20] suggest that this may not be the best aggre-
gation mode, especially on heterogeneous data. In fact, simple
averaging algorithms result in the loss of a lot of useful infor-
mation from client models. Specifically, due to the imbalance
in data distribution, some clients may be better at detecting
certain attack patterns but not others. After aggregation, it
is highly likely that the classification boundaries that were
originally clear for certain categories become fuzzy, which
affects the overall detection performance [21]. Therefore, this
work aims to better utilize the effective information from each
client to further improve the generalization and performance
of the server model after aggregation.
Our proposed FLEKD method, shown in Figure 1, can
effectively utilize heterogeneous IDS data. To be specific, we
combine the idea of one-to-many knowledge distillation with
federated learning, using the server-side model obtained by
simple aggregation as the student model, and client models
as the teacher models. The student model acquires knowledge
from logits ensembled overall the received teacher models, and
thus mutually beneficial information can be shared. In the next
round, each activated client receives the corresponding fused
prototype model.
In one-to-many knowledge distillation, we propose to use
an ensemble method to compress the knowledge of the teacher
model. Specifically, we propose a dynamic weight ensemble
method. We first test each client model to obtain its score on
the test set. Then, we perform a Softmax operation on the
test scores, with the addition of a deterministic temperature to
enlarge the difference between clients and increase the teacher
knowledge’s reliance on higher-scoring clients. Therefore, the
dynamic weight for each client at this round is generated. Fi-
nally, we perform a matrix multiplication between the dynamic
weights and the logits from each client, shown in Equation (3),
to obtain the final ensemble knowledge.
Clienti =
exp (acci/DT)
P
j exp (accj/DT)
EKD = Client × logits
(3)
After acquiring the ensemble knowledge distillation, similar
to the original KD, we use KL divergence to fine-tune the
server-side model, as detailed in steps 9-13 of Algorithm 1.
Notably, as federated learning requires each client to upload
its own model to the server, our proposed method does not
impose additional burdens or communication overhead.
IV. EXPERIMENTS
A. Experiment Setup
Dataset and model. We perform our experiments on the
CICIDS2019 dataset. It contains normal and the latest common
DDoS attack events, similar to real data (PCAPs). It also
includes network traffic analysis results using CICFlowMeter-
V3, which is based on timestamp, source and destination IP,
source and destination port, protocol and attacking tag stream.
According to the feature dimensions of CICIDS2018 and 2017,
we split the dataset into three groups, clients 0-2 with 82
full feature dimensions, clients 3-5 with 79 dimensions, and
clients 6-8 with only 24 dimensions. Note that the number of
UDPLag attack samples in the original CICIDS2019 dataset
is extremely small, about twenty-third of the number of other
categories.
Algorithm 1: Federated Learning with Ensemble Knowledge Distillation
Input: Proxy IDS dataset (unlabeled) D0, private IDS datasets (labeled) DK, initialize clients models Wi, the number of data
points per client ni.
Output: Trained server model WG
1 for each communication round t = 1 to T do
2
Select a subset of clients Ct to participate in the round;
3
for each client i in Ct do
4
Synchronize the current global model WG to client i;
5
Update a local model Wi using client i’s private dataset DK;
6
Transmit the client models Wi to the central server;
7
end
8
Model Fusion: The server computes an updated consensus, which is an average of client models parameters
WG = P
i∈Ct
ni
P
i∈Ct ni Wi;
9
Ensemble Knowledge Distillation:
10
Student: Calculate the logit vectors of server model xs
t based on the proxy dataset D0;
11
Teacher: Calculate the logit vectors of client models based on the proxy dataset D0. Use ensemble algorithm based on
Equation (3) to get the final teacher knowledge ˆxk
t ;
12
Fine-tune the global model WG using Kullback-Leibler divergence L = KL(xs
t, ˆxk
t );
13
end
14 end
15 Return WG
For the model selection, we use the naive MLP neural
network directly. Our network contains three layers and the
hidden neuron is set to 128. We use ReLU as the activation
function. All the clients’ model is initialized with the same
random parameters.
Heterogeneous distribution of client data. To simulate a
realistic and practical environment, we introduce heterogeneity
in the distribution of client data. We adopt the use of the
Dirichlet distribution, as proposed in previous studies such
as [22]. In this approach, we create disjoint non-i.i.d. client
training data. The parameter α is used to control the degree
of non-i.i.d. For example, setting α to 100 can mimic identical
local data distributions, while smaller α values result in more
heterogeneous data distributions.
Parameters setting. In the federated learning process, the
client number is set to 9. According to the client number, every
three clients have private data sets of the same dimension. We
conduct two rounds of local training before the communication
with the server side. We use Adam as the optimization and set
the learning rate to 0.001 with a uniform decay strategy of five
per cent. In the ensemble knowledge distillation process, we
first need to generate a proxy dataset. The proxy dataset con-
tains 1260 samples. We adopted a uniform sampling method,
so the samples contained in each category except UDPLag
are basically consistent, while the UDPLag is about a third of
the number of other attacks. We set the temperature at 1.5 for
knowledge distillation. For the ensembling weights, we set the
deterministic temperature to 0.5 to enlarge the discrepancy of
each client. Similarly, we also use Adam as the optimization
and set the learning rate to 0.001 with a decay strategy of five
per cent for each epoch.
B. Performance of Proposed Framework
In Table I, we present our main result. The table shows
the performance of our model using three different sample
parameters for the Dirichlet distribution: α = 10, 1, and
0.5. The first part of the table displays the performance of
the local training process conducted by each client on their
private dataset. We observe that as the data becomes more
heterogeneous, there is a significant decrease in the F1-score
of each client. In fact, when α is set to 0.5, some clients exhibit
completely incorrect predictions for certain classes. This is
due to the non-i.i.d. nature of the private datasets, which may
result in some clients having an imbalanced amount of data
and experiencing underfitting. This can lead to a decrease in
the prediction accuracy for other classes as well. Therefore,
when the problem of imbalanced data occurs in on-device local
training, it can easily lead to a significant decrease in the IoT
device’s prediction performance.
Fig. 2. Error rate of the server-side model on the test set.
However, by introducing distributed training, as shown in
the second part of Table I, the predictive performance can be
improved significantly. For example, in the case of UDPLag,
a class with very few examples, an F1-score of 95.40% can be
achieved with α set to 0.5. This is a large improvement com-
pared to the results obtained from local training, where clients
1, 6, and 8 achieved less than 50% accuracy. Furthermore, F1-
score improves in various other intrusion attack scenarios. Our
proposed EKD fine-tuning further enhances the performance
of our central model and accelerates the convergence rate. For
each setting, the number of communication rounds required to
TABLE I
MAIN RESULTS. CLIENTS 0-8 CONDUCT LOCAL TRAINING AND ALL THE SCORES ARE CALCULATED BY F1-SCORE.
Local
Training
Portmap
LDAP
MSSQL
NetBIOS
Syn
UDP
UDPLag
α =10
1
0.5
10
1
0.5
10
1
0.5
10
1
0.5
10
1
0.5
10
1
0.5
10
1
0.5
Client 0
99.89% 99.74% 70.08% 99.83% 99.78%
0.00%
99.96% 99.85% 99.97% 99.84% 99.69% 99.54% 99.91% 99.90% 99.92% 99.98%
99.96%
92.73% 97.27% 96.77% 97.99%
Client 1
99.88% 99.89% 99.51% 99.86% 99.80% 96.59% 99.96% 99.97% 95.78% 99.88% 99.83% 99.72% 99.93% 99.94% 98.78% 99.98%
99.92%
99.96% 97.52% 98.42%
0.00%
Client 2
99.92% 99.82% 99.66% 99.94% 99.92% 99.83% 99.93% 99.93% 99.71% 99.94% 99.87% 99.81% 99.93% 99.90% 99.87% 99.99% 100.00% 99.46% 97.30% 97.03% 76.48%
Client 3
99.81% 99.60% 98.96% 99.17% 99.76% 96.76% 99.89% 99.95% 99.92% 99.80% 99.51% 98.70% 99.92% 99.89% 99.93% 99.29%
99.90%
99.94% 97.01% 97.11% 56.63%
Client 4
99.84%
2.30%
64.93% 99.92% 99.86% 99.77% 99.90% 66.47% 94.97% 99.89% 77.55%
0.00%
99.92% 99.90% 99.11% 99.99%
99.86%
99.73% 97.40% 94.12% 67.13%
Client 5
99.83% 99.97% 98.91% 99.90% 99.90% 83.64% 99.93% 99.98% 98.77% 99.89% 99.90% 99.52% 99.87% 99.92% 99.86% 99.96%
99.99%
75.67% 97.03% 97.63% 89.07%
Client 6
90.65% 95.13%
8.94%
99.78% 99.84% 99.29% 80.23% 67.99%
2.42%
93.57% 39.35% 52.28% 99.83% 99.84% 98.95% 99.99%
99.99%
99.91% 93.55% 93.28% 46.65%
Client 7
94.86% 14.50% 32.67% 99.84% 99.89% 99.71% 89.36% 21.53% 13.57% 95.19% 70.78% 53.99% 99.81% 99.90% 99.05% 99.91%
99.99%
99.99% 94.22% 93.05% 54.84%
Client 8
97.19% 91.88% 96.53% 99.82% 99.58% 94.26% 93.11% 84.77%
0.00%
95.94% 95.06% 68.50% 99.89% 99.92% 99.82% 99.94%
99.90%
97.11% 93.31% 95.59% 49.36%
Distributed
Training
Portmap
LDAP
MSSQL
NetBIOS
Syn
UDP
UDPLag
α =10
1
0.5
10
1
0.5
10
1
0.5
10
1
0.5
10
1
0.5
10
1
0.5
10
1
0.5
FL
99.00% 99.45% 99.54% 99.67% 99.64% 99.83% 99.98% 99.86% 99.62% 98.97% 99.57% 99.73% 99.93% 99.93% 99.88% 99.87%
99.78%
99.99% 97.55% 97.21% 95.40%
FLEKD
99.80% 99.73% 99.77% 99.84% 99.70% 99.91% 99.99% 99.98% 99.90% 99.82% 99.73% 99.86% 99.94% 99.93% 99.92% 99.91% 99.88% 99.99% 98.34% 97.64% 96.62%
achieve a 95% F1-score was reduced by one when using the
FLEKD method. This improved efficiency is further illustrated
in Figure 2, where the convergence speed of FLEKD is
noticeably faster than that of the original FL under the same
conditions, allowing for faster achievement of optimal final
results, which demonstrates the effectiveness of using FLEKD.
C. Analysis
In this paper, we focus on evaluating the effectiveness of
our proposed method in the context of an intrusion detection
system in IoT security. To this end, we consider three key
scenarios that are likely to arise in practice and investigate the
performance of our method under each of them. Specifically,
we examine the impact of (1) varying dimensions of the data,
(2) different sample sizes, and (3) different attack category
distributions in various clients. Through our experiments, we
demonstrate the superiority of our approach in these scenarios.
The impact of different dimensions. We consider the
possibility that data collected from different time periods
may have varying dimensions, for instance, the CICIDS2017
dataset contains only 24 feature dimensions compared to
CICIDS2019 with 82 dimensions. In real-world application
scenarios, it is necessary to combine data collected from
different IoT devices for training. We present three scenarios in
Table II, where we compare the results of training with datasets
of different dimensions locally and with federated learning.
Our results show that when the available feature vectors are
reduced to 24 dimensions, there is a significant decrease in
performance, with F1-score dropping from the original 96.02%
to 84.71% using only local training. However, when we apply
federated learning and our proposed method of ensemble
knowledge distillation, we achieve a significant improvement
in accuracy, with a score of 99.80%. This indicates that
FLEKD can effectively solve the problem of dimensionality
differences in IoT device data.
TABLE II
THE IMPACT OF DIFFERENT DIMENSIONS.
Precision
Recall
F1-score
82 dim
95.93%
96.39%
96.02%
79 dim
86.31%
86.75%
86.17%
24 dim
84.17%
85.48%
84.71%
FL
99.37%
99.36%
99.37%
FLEKD
99.80%
99.80%
99.80%
The impact of different sample sizes. Due to the charac-
teristics of network attacks, which may occur in short periods
with high frequency and target certain vulnerable devices,
there may be significant differences in the number of samples
between IoT devices. Therefore, we divided the dataset into
three groups of clients with different sample sizes. Specifically,
the number of samples in each group differs by a factor of
ten, with the group containing the least number of samples
represented as ‘Base’. In Table III, we observe a negative
correlation between the number of samples and the detection
performance when using local training only. We achieved the
best performance of 99.50% on 100∗Base. However, due to
the lack of communication between different clients, the base
model was not able to benefit from the larger number of sam-
ples in 100∗Base, and only achieved an F1-score of 84.71%.
When we used FLEKD to aggregate and optimize the models
of different clients, the final F1-score reached 99.80%, which
is higher than all results obtained by local training and ordinary
FL. This demonstrates that FLEKD can effectively address
the sample size differences between different IoT devices and
significantly improve intrusion detection performance.
The impact of different attack category distributions.
In real-world scenarios, different IoT devices usually own
heterogeneous data distributions. It is common to encounter
unknown attacks but traditional centralized IDSs unable to
identify novel attacks effectively. Hence, in our experiments,
we simulated the situation where each IoT device holds a
TABLE III
THE IMPACT OF DIFFERENT SAMPLE SIZES.
Precision
Recall
F1-score
Base
84.17%
85.48%
84.71%
10*Base
92.97%
88.55%
87.25%
100*Base
99.51%
99.50%
99.50%
FL
99.37%
99.36%
99.37%
FLEKD
99.80%
99.80%
99.80%
different data distribution. There are seven classes of attacks
in the CICIDS2019 dataset, so we set up seven local clients,
each device missing a specific category of attack. As shown in
Table IV, it is evident that all clients failed to detect the lacking
attack. However, the detection abilities for unknown attacks
of the FL-based and FLEKD-based IDS models are high. The
overall detection performance of the proposed method for all
attacks is also significantly improved. By adopting FLEKD,
we find that even the most difficult-to-detect ‘UDPLag’ can
reach 80.86%, while other corresponding attack categories can
be improved to about 99%. This demonstrates that FLEKD-
based IDS can effectively deal with the category heterogeneity
problem, enabling different devices to identify attacks they
have not seen before.
TABLE IV
THE IMPACT OF DIFFERENT ATTACK CATEGORY DISTRIBUTIONS.
0: Portmap
1: LDAP
2:MSSQL
3:NetBIOS
4: Syn
5: UDP
6: UDPLag
Drop Label 0
0.00%
98.72%
59.09%
95.62%
99.67%
95.10%
0.00%
Drop Label 1
99.30%
0.00%
66.45%
99.62%
99.91%
99.98%
94.95%
Drop Label 2
66.77%
99.96%
0.00%
99.75%
99.95%
99.99%
96.81%
Drop Label 3
91.06%
97.03%
54.03%
0.00%
99.68%
95.84%
0.00%
Drop Label 4
75.76%
99.69%
94.54%
95.72%
0.00%
99.90%
11.05%
Drop Label 5
99.74%
66.62%
99.78%
99.83%
99.92%
0.00%
97.47%
Drop Label 6
91.21%
97.93%
0.00%
72.35%
99.65%
94.46%
0.00%
FL
99.05%
99.80%
94.37%
97.31%
99.92%
98.85%
75.28%
FLEKD
99.66%
99.88%
98.97%
99.80%
99.94%
99.97%
80.86%
V. CONCLUSION
In this work, we introduce federated learning towards IDS in
IoT networks to address the privacy issue and central training
problem. For non-i.i.d. situations, We propose FLEKD to mit-
igate the negative influence of clients’ heterogeneity without
increasing additional communication overhead. Our proposed
framework FLEKD outperforms the local training and original
FL results in both speed and performance on the CICIDS2019
dataset. Besides, FLEKD exhibits clear superiority in handling
heterogeneous data and achieves better results over three real-
world scenarios. Overall, our work offers a promising solution
for IoT networks’ security.
VI. ACKNOWLEDGE
This research is supported by the National Research Foun-
dation, Singapore under its Strategic Capability Research Cen-
tres Funding Initiative. Any opinions, findings and conclusions
or recommendations expressed in this material are those of the
author(s) and do not reflect the views of National Research
Foundation, Singapore.
REFERENCES
[1] B. Mukherjee, L. T. Heberlein, and K. N. Levitt, “Network intrusion
detection,” IEEE network, vol. 8, no. 3, pp. 26–41, 1994.
[2] H.-J. Liao, C.-H. R. Lin, Y.-C. Lin, and K.-Y. Tung, “Intrusion detection
system: A comprehensive review,” Journal of Network and Computer
Applications, vol. 36, no. 1, pp. 16–24, 2013.
[3] B. B. Zarpel˜ao, R. S. Miani, C. T. Kawakani, and S. C. de Alvarenga, “A
survey of intrusion detection in internet of things,” Journal of Network
and Computer Applications, vol. 84, pp. 25–37, 2017.
[4] S.-J. Han and S.-B. Cho, “Detecting intrusion with rule-based integration
of multiple models,” Computers & Security, vol. 22, no. 7, pp. 613–623,
2003.
[5] P. Mishra, V. Varadharajan, U. Tupakula, and E. S. Pilli, “A detailed
investigation and analysis of using machine learning techniques for
intrusion detection,” IEEE communications surveys & tutorials, vol. 21,
no. 1, pp. 686–728, 2018.
[6] Z. Ahmad, A. Shahid Khan, C. Wai Shiang, J. Abdullah, and F. Ahmad,
“Network intrusion detection system: A systematic study of machine
learning and deep learning approaches,” Transactions on Emerging
Telecommunications Technologies, vol. 32, no. 1, p. e4150, 2021.
[7] S. Agrawal, S. Sarkar, O. Aouedi, G. Yenduri, K. Piamrat, M. Alazab,
S. Bhattacharya, P. K. R. Maddikunta, and T. R. Gadekallu, “Federated
learning for intrusion detection system: Concepts, challenges and future
directions,” Computer Communications, 2022.
[8] J. Fan, W. Yang, Z. Liu, J. Kang, D. Niyato, K.-Y. Lam, and H. Du,
“Understanding security in smart city domains from the ant-centric
perspective,” IEEE Internet of Things Journal, 2023.
[9] Z. Ma, L. Liu, W. Meng, X. Luo, L. Wang, and W. Li, “Adcl: Towards an
adaptive network intrusion detection system using collaborative learning
in iot networks,” IEEE Internet of Things Journal, 2023.
[10] W. Yang and K.-Y. Lam, “Effective anomaly detection model training
with only unlabeled data by weakly supervised learning techniques,” in
ICICS 2021, 2021, pp. 402–425.
[11] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in Artificial intelligence and statistics.
PMLR, 2017, pp. 1273–
1282.
[12] S. I. Popoola, R. Ande, B. Adebisi, G. Gui, M. Hammoudeh, and O. Jo-
gunola, “Federated deep learning for zero-day botnet attack detection in
iot-edge devices,” IEEE Internet of Things Journal, vol. 9, no. 5, pp.
3930–3944, 2021.
[13] Y. Liu, N. Kumar, Z. Xiong, W. Y. B. Lim, J. Kang, and D. Niyato,
“Communication-efficient federated learning for anomaly detection in
industrial internet of things,” in IEEE Global Communications Confer-
ence.
IEEE, 2020, pp. 1–6.
[14] V. Mothukuri, P. Khare, R. M. Parizi, S. Pouriyeh, A. Dehghantanha,
and G. Srivastava, “Federated-learning-based anomaly detection for iot
security attacks,” IEEE Internet of Things Journal, vol. 9, no. 4, pp.
2545–2554, 2021.
[15] B. Li, Y. Wu, J. Song, R. Lu, T. Li, and L. Zhao, “Deepfed: Federated
deep learning for intrusion detection in industrial cyber–physical sys-
tems,” IEEE Transactions on Industrial Informatics, vol. 17, no. 8, pp.
5615–5624, 2020.
[16] P.
Ruzafa-Alc´azar,
P.
Fern´andez-Saura,
E.
M´armol-Campos,
A.
Gonz´alez-Vidal,
J.
L.
Hern´andez-Ramos,
J.
Bernal-Bernabe,
and A. F. Skarmeta, “Intrusion detection based on privacy-preserving
federated learning for the industrial iot,” IEEE Transactions on
Industrial Informatics, vol. 19, no. 2, pp. 1145–1154, 2021.
[17] E. M. Campos, P. F. Saura, A. Gonz´alez-Vidal, J. L. Hern´andez-Ramos,
J. B. Bernab´e, G. Baldini, and A. Skarmeta, “Evaluating federated
learning for intrusion detection in internet of things: Review and
challenges,” Computer Networks, vol. 203, p. 108661, 2022.
[18] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
network,” arXiv preprint arXiv:1503.02531, 2015.
[19] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated
learning with non-iid data,” arXiv preprint arXiv:1806.00582, 2018.
[20] X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang, “On the convergence
of fedavg on non-iid data,” arXiv preprint arXiv:1907.02189, 2019.
[21] T. Lin, L. Kong, S. U. Stich, and M. Jaggi, “Ensemble distillation
for robust model fusion in federated learning,” Advances in Neural
Information Processing Systems, vol. 33, pp. 2351–2363, 2020.
[22] M. Yurochkin, M. Agarwal, S. Ghosh, K. Greenewald, N. Hoang,
and Y. Khazaeni, “Bayesian nonparametric federated learning of neural
networks,” in International conference on machine learning.
PMLR,
2019, pp. 7252–7261.
"
"Blind Image Quality Assessment (BIQA) aims to evaluate image quality in accordance with human perception, without reference benchmarks. Data-driven BIQA models based on deep neural networks have made significant progress. However, the inherent differences between BIQA and these high-level tasks inevitably introduce noise into the quality-aware features. We take an initial step towards exploring the diffusion model for feature denoising in BIQA. Extensive experiments on eight standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods.","Image Quality Assessment (IQA) methods aim to match the human perception of image distortions. As reference images are often unavailable, BIQA gains attention for tasks like image restoration and super-resolution without references. This paper proposes a novel BIQA framework based on the diffusion model, namely, the Perceptual Feature Diffusion model for IQA (PFD-IQA). We formulate the feature noise filtering problem as a progressive feature denoising process, enabling effective enhancement of quality-aware features.","State-of-the-art BIQA methods typically depend on using features from high-level tasks for transfer learning. However, the inherent differences between BIQA and these high-level tasks inevitably introduce noise into the quality-aware features. The diffusion model defines a Markov chain where noise is gradually added to the input samples (forward process) and then used to remove the corresponding noise from the noisy samples (reverse process), showcasing its effective noise removal ability. Inspired by this, PFD-IQA consists of two main modules to overcome the challenges in the diffusion model for BIQA.nannannan","PFD-IQA introduces a Perceptual Prior Discovery and Aggregation (PDA) module to establish two auxiliary tasks to discover potential low-level features in images that are used to aggregate perceptual text conditions for the diffusion model. We also propose a Perceptual Prior-based Feature Refinement strategy, which matches noisy features to predefined denoising trajectories and then performs exact feature denoising based on text conditions.nan",PFD-IQA achieves the PLCC values of 0.935 (+↑ 3.0% vs. 0.905 in KADID) and 0.922 (+↑ 2.8% vs. 0.894 in LIVEC) on eight standard BIQA datasets. Ablation experiments demonstrate the effectiveness of each module and the lightweight diffusion model with very few sampling steps.,"PFD-IQA is a pioneering BIQA framework leveraging the diffusion model's noise removal capabilities. Addressing key challenges in BIQA, PFD-IQA features two novel modules: the Perceptual Prior Discovery and Aggregation module for improved feature preservation and noise elimination, and the Perceptual Prior-based Feature Refinement Strategy for defining denoising trajectories in the absence of explicit benchmarks.",Feature Denoising Diffusion Model for Blind Image Quality Assessment,"Xudong Li, Jingyuan Zheng, Runze Hu, Yan Zhang, Ke Li, Yunhang Shen, Xiawu Zheng, Yutao Liu, ShengChuan Zhang, Pingyang Dai, Rongrong Ji","Feature Denoising Diffusion Model for Blind Image Quality Assessment
Xudong Li1 , Jingyuan Zheng2 , Runze Hu3 , Yan Zhang1,∗ , Ke Li4 , Yunhang Shen4 ,
Xiawu Zheng1 , Yutao Liu5 , ShengChuan Zhang1 , Pingyang Dai1 , Rongrong Ji1
1Key Laboratory of Multimedia Trusted Perception and Efficient Computing,
Ministry of Education of China, Xiamen University 2School of Medicine, Xiamen University
3School of Information and Electronics, Beijing Institute of Technology
4Tencent Youtu Lab 5 School of Computer Science and Technology, Ocean University of China
{lxd761050753, jyzheng0606, bzhy986, hrzlpk2015, shenyunhang01, tristanli.sh}@gmail.com,
liuyutao@ouc.edu.cn,{zhengxiawu, zsc 2016, pydai, rrji}@xmu.edu.cn
Abstract
Blind Image Quality Assessment (BIQA) aims to
evaluate image quality in line with human per-
ception, without reference benchmarks. Currently,
deep learning BIQA methods typically depend on
using features from high-level tasks for transfer
learning.
However, the inherent differences be-
tween BIQA and these high-level tasks inevitably
introduce noise into the quality-aware features. In
this paper, we take an initial step towards explor-
ing the diffusion model for feature denoising in
BIQA, namely Perceptual Feature Diffusion for
IQA (PFD-IQA), which aims to remove noise from
quality-aware features.
Specifically, (i) We pro-
pose a Perceptual Prior Discovery and Aggregation
module to establish two auxiliary tasks to discover
potential low-level features in images that are used
to aggregate perceptual text conditions for the dif-
fusion model. (ii) We propose a Perceptual Prior-
based Feature Refinement strategy, which matches
noisy features to predefined denoising trajectories
and then performs exact feature denoising based
on text conditions. Extensive experiments on eight
standard BIQA datasets demonstrate the superior
performance to the state-of-the-art BIQA methods,
i.e., achieving the PLCC values of 0.935 (↑ 3.0%
vs. 0.905 in KADID) and 0.922 (↑ 2.8% vs. 0.894
in LIVEC).
1
Introduction
Image Quality Assessment (IQA) methods aim to match
the human perception of image distortions [Wang et al.,
2004]. Reliable IQA models are important for image-driven
applications, while also serving as benchmarks for image pro-
cessing. Objective IQA includes Full-Reference IQA (FR-
IQA) [Shi and Lin, 2020], Reduced-Reference IQA (RR-
IQA) [Tao et al., 2009], and no-reference or Blind IQA
(BIQA) [Zhou et al., 2019].
As reference images are of-
ten unavailable, BIQA gains attention for tasks like im-
∗Corresponding author
Denoised Feature
Mos: 60.95 
48.06 (  11.63 ) 
...
Semantic Feature
...
T Diffusion steps 
Mos: 48.69
Figure 1: Image on top: the sample image. Images at bottom: Be-
fore and after diffusion denoising, the feature map significantly re-
fines, effectively pinpointing areas with visible image quality degra-
dation. The initial semantic focus is on “human,“ but after denois-
ing, attention notably shifts to the fuzzy region (the orange region
with the blurred crowd and arms), resulting in a closer alignment
with the actual Mean Opinion Scores (MOS).
age restoration [Banham and Katsaggelos, 1997] and super-
resolution [Dong et al., 2015] without references.
Data-driven BIQA models based on deep neural networks
[Bosse et al., 2017; Wu et al., 2020] have made significant
progress. The quality score of distorted images is typically
measured using the Mean Opinion Score (MOS), making
BIQA a small-sample task. To address this, a promising strat-
egy utilizes a pre-training and fine-tuning paradigm, transfer-
ring shared features from the large-scale ImageNet source do-
main to the IQA target domain to accomplish the IQA task.
However, during the pre-training of large-scale classification
tasks, synthetic distortions are typically used as a data aug-
mentation method, inevitably reducing the sensitivity of the
model to image distortion [Zhang et al., 2023; Hendrycks and
Dietterich, 2019]. Consequently, the pre-trained features’ in-
sensitivity to distortion degradation can lead the model to ex-
cessively concentrate on high-level information during qual-
ity assessment, overlooking distortions information critical to
quality perception [Zhao et al., 2023; Zhang et al., 2023]. We
provide an example to explain such a problem in Fig. 1. As
we observe, the baseline focuses excessively on the high-level
arXiv:2401.11949v1  [cs.CV]  22 Jan 2024
information (such as semantic information, i.e., “human“ in
the red box) in the foreground of the distorted image, while
neglecting the low-level quality-relevant information (such as
the blur and geometric distortion in the yellow box), leading
to inaccurate quality predictions. Therefore, these pre-trained
features are not always beneficial, and some may even be con-
sidered as noise in the quality-aware features. It is necessary
to meticulously filter out the noisy features.
The diffusion model [Ho et al., 2020; Rombach et al.,
2022] defines a Markov chain where noise is gradually added
to the input samples (forward process) and then used to re-
move the corresponding noise from the noisy samples (re-
verse process), showcasing its effective noise removal ability.
Inspired by this, a novel BIQA framework based on the dif-
fusion model is first proposed, namely the Perceptual Feature
Diffusion model for IQA (PFD-IQA). We formulate the fea-
ture noise filtering problem as a progressive feature denois-
ing process, enabling effective enhancement of quality-aware
features. However, there are two challenges in directly utiliz-
ing diffusion models for denoising in BIQA: (i) Traditional
diffusion models may offer limited control over quality-aware
feature preservation and noise feature elimination, possibly
leading to suboptimal denoising. (ii) In BIQA, explicit bench-
marks or ground truths are often absent for denoising targets.
This makes it challenging to define a clear denoising trajec-
tory for the diffusion model.
To this end, our PFD-IQA consists of two main modules
to overcome the above two challenges in the diffusion model
for BIQA. To address the problem (i), we introduce a Per-
ceptual Prior Discovery and Aggregation module that merges
text prompts representing various quality perceptions to guide
the diffusion model. Specifically, we initially acquire poten-
tial distortion-aware and quality-level priors through auxil-
iary tasks. These are then combined based on their similarity
to text prompts, creating perceptual text prompts, which serve
as conditions to guide the model for more accurate feature
denoising. We select these priors for text descriptions pri-
marily for two reasons: Firstly, understanding the diversity in
distortion enhances prediction accuracy and generalization in
IQA[Song et al., 2023; Zhang et al., 2023]. Secondly, qual-
ity level recognition categorizes distorted images into lev-
els (e.g. high and bad ), based on human-perceptible seman-
tic features. This natural language-based, range-oriented ap-
proach helps minimize errors in absolute scoring across dif-
ferent subjects [Yang et al., 2020].
To address the problem (ii), we introduce a novel Percep-
tual Prior-based Feature Refinement Strategy for BIQA. Ini-
tially, we use pre-trained teacher pseudo features to establish
a quality-aware denoising process based on text-conditioned
DDIM. We then consider student features as noisy versions
of teacher pseudo features. Through an adaptive noise align-
ment mechanism, we adaptively assess the noise level in
each student feature and apply corresponding Gaussian noise,
aligning these features with the teacher’s predefined denois-
ing path. During the reverse denoising, cross-attention with
text conditions is conducted to precisely refine the quality-
aware features. Notably, our goal is to utilize the powerful
denoising modeling capability of the diffusion model for fea-
ture refinement rather than learning the distribution of teacher
pseudo features. Experiments show that we achieve superior
performance with very few sampling steps (e.g. 5 iterations)
and a lightweight diffusion model for denoising. We summa-
rize the contributions of this work as follows:
• We make the first attempt to convert the challenges of
BIQA to the diffusion problem for feature denoising. We
introduce a novel PFD-IQA, which effectively filters out
quality-irrelevant information from features.
• We propose a Perceptual Prior Discovery and Aggrega-
tion Module to identify perceptual features of distortion
types and quality levels. By leveraging the correlation
between perceptual prior and text embedding, we adap-
tively aggregate perceptual text prompts to guide the
diffusion denoising process, which ensures attention to
quality-aware features during the denoising process.
• We introduce a novel Perceptual Prior-based Feature Re-
finement Strategy for BIQA. Particularly, we pre-define
denoising trajectories of teacher pseudo-labels. Then,
by employing an adaptive noise alignment module, we
match the student noise features to predefined denoising
trajectories and subsequently perform precise feature de-
noising based on the given prompt conditions.
2
Related Work
2.1
BIQA with Deep Learning.
The early BIQA method [Liu et al., 2017; Zhou et al.,
2019; Li et al., 2009] was based on the convolutional neural
network (CNN) thanks to its powerful feature expression abil-
ity. The CNN-based BIQA methods [Zhang et al., 2018; Su
et al., 2020] generally treated the IQA task as the downstream
task of object recognition, following the standard pipeline of
pre-training and fine-tuning. Such a strategy is useful as these
pre-trained features share a certain degree of similarity with
the quality-aware features of images [Su et al., 2020]. Re-
cently, Vision Transformer (ViT) based methods for Blind
Image Quality Assessment (BIQA) have become popular due
to their strong capability in modeling non-local perceptual
features in images. There are two main types of architectures
used: the hybrid transformer and the pure transformer. Exist-
ing ViT-based methods typically rely on the CLS token for as-
sessing image quality. Originally designed for describing im-
age content, like object recognition, the CLS token focuses on
higher-level visual abstractions, such as semantics and spatial
relationships of objects. Therefore, it is still a challenge to
fully adapt these methods from classification tasks to image
quality assessment (IQA) tasks, due to the abstract nature of
classification features.
2.2
Diffusion Models.
Diffusion models [Rombach et al., 2022; Huang et al.,
2023], generally comprising a forward process for adding
noise and a reverse process for denoising, gained popularity
with Ho et al.’s introduction of the denoising diffusion prob-
abilistic model. Building on this, methods like [Rombach
et al., 2022] have integrated attention mechanisms into dif-
fusion models, stabilizing them and producing high-quality
a photo with 
{d} artifacts
  Image
Encoder
Perception  
Generation
Perception  
Generation
Perceptual Prior Discovery
Perception
Prompt Aggregation 
Distortion 
Prior
Random mask
Random mask
Enhanced
    Feature
Quality 
Prior 
dF
q
F
qF
dF
ˆ
h
F
Similarity
& Softmax
Channel-Wise 
Masked Map   
Sum Operation
Gaussian 
Noise
Multiplication  
Operation
sF
(a) Perceptual Prior Discovery and Aggregation 
(b) Perceptual Prior-based Diffusion Refinement 
Conditional Diffusion Refinement


Denoising Network 
ˆ
h
F
Feature ˆ
tF
Feature
0ˆF
 Similarity
& Softmax
q
G
d
G
...
ˆ ( )
dp x
ˆ ( )
qp x


ˆ
i
q
i
p j x G
∣


ˆ
i
d
i
p j x G
∣


1
)
1(
 
ˆ
tF
Feature
Prior 
Prior 
Image
Decoder
Noise Level 
Predictor
a photo with 
{q} quality
E
E
ˆada
e
Perceptual Text 
Noise Alignment
Enhancement
 Feature
CLS 
Figure 2: The overview of PFD-IQA, which consists of a teacher model used for creating pseudo-labels and a student model equipped with
PDA and PDR modules. Specifically, we begin by developing a learning perceptual prior (Sec. 3.2) through the random mask reconstruction
process. Subsequently, we use the prior knowledge to aggregate text information as the condition to guide the feature-denoising process of
the diffusion model and refine the features (Sec. 3.3).
images. To further extend diffusion models (dm) into main-
stream computer vision tasks, latent representation learning
methods based on dm have been proposed, including Diffu-
sionDet for object detection [Chen et al., 2023] and SegDiff
for segmentation [Amit et al., 2021].
However, diffusion
models are seldom used for specific feature denoising. In this
study, we treat the feature optimization process in IQA as an
inverse denoising approximation and iteratively use diffusion
models to enhance representations for accurate quality aware-
ness. To the best of our knowledge, ours is the first work to
introduce diffusion models into IQA for feature denoising.
3
Methodology
In the context of BIQA, we introduce common notations.
Bold formatting is used to denote vectors (e.g., x, y), ma-
trices (e.g., X, Y ), and tensors. The training data consists
of D = {x, yg, yd, yq}, where x is the labeled image with
ground-truth scores yg. yd, and yq represent the distortion
type and quality level pseudo-labels associated with the input
image, respectively. Additionally, image embeddings F and
textual embeddings G are denoted. The probability distribu-
tion of logits for the network is represented as p.
3.1
Overview
The paper introduces a model called the Perceptual Feature
Diffusion model for Image Quality Assessment (PFD-IQA),
which progressively refines quality-aware features. As de-
picted in Fig. 2, PFD-IQA seamlessly integrates two main
components: A Perceptual Prior Discovery and Aggregation
(PDA) and A Perceptual Prior-based Diffusion Refinement
Module (PDR). Initially, PFD-IQA inputs the given image x
into a Vision Transformer (ViT) encoder [Dosovitskiy et al.,
2021] to obtain a feature representation F s. Under the super-
vision of pseudo-labels for distortion types and quality levels,
we use the PDA module to discover potential distortion priors
ˆ
F d and Perceptual priors ˆ
F q, which then adaptively aggre-
gate perceptual text embeddings as conditions for the diffu-
sion process (Sec. 3.2). Next, in the PDR module, these prior
features are used to modulate F s for feature enhancement to
obtain ˆ
Fh. This is followed by matching it to a predefined
noise level ˆ
F t through an adaptive noise matching module ϵ,
and finally employing a lightweight feature denoising mod-
ule to progressively denoise under the guidance of the per-
ceptual text embeddings (Sec. 3.3). After the PDR module,
a layer of transformer decoder is used to further interpret the
denoised features for predicting the final quality score [Qin et
al., 2023]. It is important to emphasize that pseudo-labels are
only used for training.
3.2
Perceptual Prior Discovery and Aggregation
Considering the intricate nature of image distortions in
the real world, the evaluation of image quality necessitates
discriminative representations that can distinguish different
types of distortions [Zhang et al., 2022], as well as the degrees
of degradation. To achieve this, an auxiliary task involving
the classification of distortion types is introduced Which is
designed to refine the differentiation among diverse distor-
tion types, thereby providing nuanced information. Addition-
ally, the quality levels classification task is further employed
to offer a generalized classification that compensates for the
uncertainty and error inherent in predicting absolute image
quality scores.
Perceptual Prior Discovery. In this context, two feature re-
constructors denoted as R(·) are trained to reconstruct the
mentioned two prior features, respectively. These reconstruc-
tors consist of two components: (1) a stochastic channel
mask module and (2) a module for the feature reconstruc-
tions. Specifically, given an image x and its feature F s that
has been generated by a VIT encoder. The first step involves
applying a channel-wise random mask Mc to the channel di-
mension of this feature to obtain F m.
Mc =
0,
if Rc < β
1,
Otherwise,
F m = falign(F s) · Mc,
(1)
where Rc is a random number in (0, 1) and c are channel
number of the feature. β is a hyper-parameter that denotes the
masked ratio and falign is a adaptation layer with 1×1 convo-
lution. The random mask helps to train a more robust feature
reconstructor [Yang et al., 2022]. Subsequently, we utilize
the two feature reconstruction modules R(·) to generate prior
features. Each R(·) consists of a sequence of operations in-
cluding a 1×1 convolution Wl1, a Batch Normalization (BN)
layer, and another 1×1 convolutional layer. Wl2.
ˆF j = R(F m) = Wl2,j · (ReLU (Wl1,j(F m))) ,
(2)
where, j ∈ {d, q}, ˆF d and ˆF q stands for distortion and qual-
ity level classification auxiliary tasks. These tasks are linked
to the original image feature F s and involve capturing differ-
ent aspects of information.
To effectively supervise the auxiliary tasks related to qual-
ity level classification Q and distortion classification D for
the discovery of potential prior features, we divide the tasks
into five quality levels and eleven types of distortions fol-
lowing previous study [Zhang et al., 2023]. As illustrated
in Fig. 2, let D denote the set of image distortions, i.e.,
D = {d1, d2, . . . , dK}, where di is an image distortion type,
e.g., “noise”.
Let Q denote the set of image quality lev-
els, i.e., Q = {q1, q2, . . . , qK}, where qi is quality level,
e.g., “bad”, and K is the number of distortions or quality
levels we consider. The textural description prompt set is
Td = {Td | Td = “a photo of with {d} artifact.”, d ∈ D}
and Tq = {Tq | Tq = “a photo of with {q} quality.”, q ∈ Q}.
Given an image x, we compute the cosine similarity be-
tween image prior embedding ˆFj and each prompt embed-
ding Gj = E(Tj) ∈ RK×C from text encoder E resulting in
the logits output for the auxiliary tasks, namely ˆpd and ˆpq,
which the parameters of the text encoder is freeze:
ˆpj(x) = logit(j|x) =
ˆF · GT
j
∥ ˆF j∥2∥Gj∥2
.
(3)
To supervise the feature reconstruction module, we utilize
the soft pseudo-labels pd for distortion and pq for quality,
which are generated by the pre-trained teacher model. This
guidance is accomplished by applying the KL divergence as
follows:
LKL =
X
j∈{d,q}
Lj
KL
the normalized feature norm F s and then employ two convo-
lutional transformations modulate the normalized feature F s
with scaling and shifting parameters from additive features
ˆ
F dq, resulting in the feature representation ˆ
F h:
ˆF h = (conv( ˆ
F dq) × norm(F s) + conv( ˆ
F dq)) + F s. (7)
Predefined Conditional Denoising Trajectories. The pro-
posed PFD-IQA iteratively optimizes the feature ˆ
Fh to at-
tain accurate and quality-aware representations. This process
can be conceptualized as an approximation of the inverse fea-
ture denoising procedure. However, the features represent-
ing the ground truth are often unknown. Therefore, we in-
troduce features F tea generated by a pre-trained teacher as
pseudo-ground truth to pre-construct a denoising trajectory
of quality-aware features. As depicted in Fig. 3, for the for-
ward diffusion process, F tea
t
is a linear combination of the
initial data F tea and the noise variable ϵt.
F tea
t
= √¯αtF tea +
√
1 − ¯αtϵt.
(8)
The parameter ¯αt is defined as ¯αt
:=
Qt
s=0 αs
=
Qt
s=0(1 − βs), offering a method to directly sample F tea
t
at any time step using a noise variance schedule denoted
by β [Ho et al., 2020].
During training, a neural net-
work ϵθ(F tea
t
, ˆetea
ada, t) conditioned on perceptual text ˆetea
ada
is trained to predict the noise ϵt ∈ N(0, I) by minimizing
the ℓ2 loss, i.e.,
Lldm = ∥ϵt − ϵθ(F tea
t
, ˆetea
ada, t)∥2
2,
(9)
Adaptive Noise-Level Alignment (ANA). We treat the fea-
ture representations extracted by students according to the
fine-tuning paradigm as noisy versions of the teacher’s
quality-aware features. However, the extent of noise that sig-
nifies the dissimilarity between the teacher and student fea-
tures remains elusive and may exhibit variability across dis-
tinct training instances. As a result, identifying the optimal
initial time step to initiate the diffusion process presents a
challenging task. To overcome this, we introduce an Adap-
tive Noise Matching Module to match the noise level of stu-
dent features with a predefined noise level.
As depicted in Fig. 2, we develop a Noise-level predictor
using a straightforward convolutional module aimed at learn-
ing a weight γ to combine the fusion feature ˆ
F h of the student
with Gaussian noise, resulting in ˆ
F t that aligns with F t. This
weight ensures that the student’s outputs are harmonized with
the noise level corresponding to the initial time step t. Con-
sequently, the initial noisy feature involved in the denoising
process is altered subsequently:
ˆ
F t = γ ⊙ ˆ
F h + (1 − γ) ⊙ N(0, 1)
(10)
Lightweight Architecture. Considering the huge dimension
of transformers, performing the denoising process on features
during training requires considerable iterations, which may
result in a huge computational load. To address this issue, this
paper proposes a lightweight diffusion model ϵθ(·) as an alter-
native to the U-net architecture, as shown in Fig. 3. It consists
of two bottleneck blocks from ResNet [He et al., 2016] and a
1×1 convolution. In addition, A cross-attention layer [Rom-
bach et al., 2022] is added after each bottleneck block to ag-
gregate the text and image features. We empirically find that
this lightweight network is capable of effective noise removal
with less than 5 sampling iterations which is more than 200×
faster sampling speed compared to the DDPM.
During the sampling process, with the initial noise F t ob-
tained in Equ. 10, the trained network is employed for itera-
tive denoising to reconstruct the feature ˆ
F 0:
pθ

ˆ
F t−1 | ˆ
F t

:= N

ˆ
F t−1; ϵθ

ˆ
F t, ˆeada, t

, σ2
t I

(11)
Subsequently, we employ the features F tea derived from the
pseudo-labels generated by the pre-trained teacher to super-
vise the denoising procedure using MSE loss which ensures
the stability of the feature denoising process.
Lfea = ∥ ˆ
F 0 − F tea)∥2
2
(12)
To sum up, the overall loss at the training stage is described
as follows:
L = λ1LKL + λ2Lldm + λ3Lfea + ∥ˆy − yg)∥1
(13)
Here, ˆy represents the predicted score of image x based on
the denoised feature obtained from the transformer decoder.
yg stands for the ground truth corresponding to the image
x. The notation ∥ · ∥1 denotes the ℓ1 regression loss. In this
paper, We simply set λ1 = 0.5, λ2 = 1, and λ3 = 0.01 in all
experiments.
4
Experiments
4.1
Benchmark Datasets and Evaluation Protocols
We evaluate the performance of the proposed PFD-IQA on
eight typical BIQA datasets, including four synthetic datasets
of LIVE [Sheikh et al., 2006], CSIQ [Larson and Chandler,
2010], TID2013 [Ponomarenko et al., 2015], KADID [Lin et
al., 2019], and four authentic datasets of LIVEC [Ghadiyaram
and Bovik, 2015] KONIQ [Hosu et al., 2020], LIVEFB [Ying
et al., 2020], SPAQ [Fang et al., 2020]. Specifically, for
the authentic dataset, LIVEC contains 1162 images from
different mobile devices and photographers.
SPAQ com-
prises 11,125 photos from 66 smartphones. KonIQ-10k in-
cludes 10,073 images from public sources, while LIVEFB
is the largest real-world dataset to date, with 39,810 images.
The synthetic datasets involve original images distorted arti-
ficially using methods like JPEG compression and Gaussian
blur. LIVE and CSIQ have 779 and 866 synthetically dis-
torted images, respectively, with five and six distortion types
each. TID2013 and KADID include 3000 and 10,125 syn-
thetically distorted images, respectively, spanning 24 and 25
distortion types.
In our experiments, we employ two widely used metrics:
Spearman’s Rank Correlation Coefficient (SRCC) and Pear-
son’s Linear Correlation Coefficient (PLCC). These metrics
evaluate prediction monotonicity and accuracy, respectively.
LIVE
CSIQ
TID2013
KADID
LIVEC
KonIQ
LIVEFB
SPAQ
Method
PLCCSRCCPLCCSRCCPLCCSRCCPLCCSRCC PLCCSRCCPLCCSRCCPLCCSRCCPLCCSRCC
DIIVINE [Saad et al., 2012]
0.908 0.892 0.776 0.804 0.567 0.643 0.435 0.413 0.591 0.588 0.558 0.546 0.187 0.092 0.600 0.599
BRISQUE [Mittal et al., 2012]
0.944 0.929 0.748 0.812 0.571 0.626 0.567 0.528 0.629 0.629 0.685 0.681 0.341 0.303 0.817 0.809
ILNIQE [Zhang et al., 2015]
0.906 0.902 0.865 0.822 0.648 0.521 0.558 0.534 0.508 0.508 0.537 0.523 0.332 0.294 0.712 0.713
BIECON [Kim and Lee, 2016]
0.961 0.958 0.823 0.815 0.762 0.717 0.648 0.623 0.613 0.613 0.654 0.651 0.428 0.407
-
-
MEON [Ma et al., 2017]
0.955 0.951 0.864 0.852 0.824 0.808 0.691 0.604 0.710 0.697 0.628 0.611 0.394 0.365
-
-
WaDIQaM [Bosse et al., 2017]
0.955 0.960 0.844 0.852 0.855 0.835 0.752 0.739 0.671 0.682 0.807 0.804 0.467 0.455
-
-
DBCNN [Zhang et al., 2018]
0.971 0.968 0.959 0.946 0.865 0.816 0.856 0.851 0.869 0.851 0.884 0.875 0.551 0.545 0.915 0.911
TIQA [You and Korhonen, 2021] 0.965 0.949 0.838 0.825 0.858 0.846 0.855 0.85
0.861 0.845 0.903 0.892 0.581 0.541
-
-
MetaIQA [Zhu et al., 2020]
0.959 0.960 0.908 0.899 0.868 0.856 0.775 0.762 0.802 0.835 0.856 0.887 0.507 0.54
-
-
P2P-BM [Ying et al., 2020]
0.958 0.959 0.902 0.899 0.856 0.862 0.849 0.84
0.842 0.844 0.885 0.872 0.598 0.526
-
-
HyperIQA [Su et al., 2020]
0.966 0.962 0.942 0.923 0.858 0.840 0.845 0.852 0.882 0.859 0.917 0.906 0.602 0.544 0.915 0.911
TReS [Golestaneh et al., 2022]
0.968 0.969 0.942 0.922 0.883 0.863 0.858 0.859 0.877 0.846 0.928 0.915 0.625 0.554
-
-
MUSIQ [Ke et al., 2021]
0.911 0.940 0.893 0.871 0.815 0.773 0.872 0.875 0.746 0.702 0.928 0.916 0.661 0.566 0.921 0.918
DACNN [Pan et al., 2022]
0.980 0.978 0.957 0.943 0.889 0.871 0.905 0.905 0.884 0.866 0.912 0.901
-
-
0.921 0.915
DEIQT [Qin et al., 2023]
0.982 0.980 0.963 0.946 0.908 0.892 0.887 0.889 0.894 0.875 0.934 0.921 0.663 0.571 0.923 0.919
PFD-IQA (ours)
0.985 0.985 0.972 0.962 0.937 0.924 0.935 0.931 0.922 0.906 0.945 0.930 0.667 0.572 0.925 0.922
Table 1: Performance comparison measured by averages of SRCC and PLCC, where bold entries indicate the best results, underlines indicate
the second-best.
Training
LIVEFB
LIVEC
KonIQ
LIVE
CSIQ
Testing
KonIQ
LIVEC
KonIQ
LIVEC
CSIQ
LIVE
DBCNN
0.716
0.724
0.754
0.755
0.758
0.877
P2P-BM
0.755
0.738
0.740
0.770
0.712
-
HyperIQA
0.758
0.735
0.772
0.785
0.744
0.926
TReS
0.713
0.740
0.733
0.786
0.761
-
DEIQT
0.733
0.781
0.744
0.794
0.781
0.932
PFD-IQA
0.775
0.783
0.796
0.818
0.817
0.942
Table 2: SRCC on the cross datasets validation. The best results are
highlighted in bold, second-best is underlined.
4.2
Implementation Details
For the student network, we follow the typical training
strategy of randomly cropping the input image into 10 im-
age patches with a resolution of 224×224. Each image patch
is then reshaped as a sequence of patches with patch size p
= 16 and the dimension of input tokens as in D = 384. We
create the Transformer encoder based on the ViT-B proposed
in DeiT III [Touvron et al., 2022]. The encoder depth is set
to 12 and the number of heads h = 12. For Decoder, the depth
is set to one. Our model is trained for 9 epochs. The learn-
ing rate is set to 2 × 10−4 with a decay factor of 10 every
3 epochs. The batch size depends on the size of the dataset,
which is 16 and 128 for LIVEC and KonIQ, respectively. For
each dataset, 80% of the images are used for training and the
remaining 20% of the images are used for testing. We re-
peat this process 10 times to mitigate performance bias and
report the average of SRCC and PLCC. For the pre-trained
teacher network, We adopt ViT-B/16 [Radford et al., 2021]
as the visual encoder and text encoder. The re-training hyper-
parameter Settings are consistent with [Zhang et al., 2023].
4.3
Overall Prediction Performance Comparison
For competing models, we either directly adopt the pub-
licly available implementations, or re-train them on our
datasets with the training codes provided by the respective
authors. Tab. 1 reports the comparison results between the
proposed PFD-IQA and 14 state-of-the-art BIQA methods,
including hand-crafted feature-based BIQA methods, such
as ILNIQE (Zhang, Zhang, and Bovik 2015) and BRISQUE
(Mittal, Moorthy, and Bovik 2012), and deep learning-based
methods, i.e., MUSIQ [Ke et al., 2021] and MetaIQA [Zhu et
al., 2020]. It is observed from these eight datasets that PFD-
IQA achieves superior performance over all other methods
across the 8 datasets. Since the images on these 8 datasets
cover various image content and distortion types, it is very
challenging to consistently achieve leading performance on
all these datasets. Accordingly, these observations confirm
the effectiveness and superiority of PFD-IQA in characteriz-
ing image quality.
4.4
Generalization Capability Validation
We further evaluate the generalization ability of PFD-
IQA by a cross-dataset validation approach, where the BIQA
model is trained on one dataset and then tested on the oth-
ers without any fine-tuning or parameter adaptation.
Tab.
2 reports the experimental results of SRCC averages on the
five datasets. As observed, PFD-IQA achieves the best per-
formance on six cross-datasets, achieving clear performance
gains on the LIVEC dataset and competitive performance on
the KonIQ dataset. These results strongly verify the general-
ization ability of PFD-IQA.
4.5
Qualitative Analysis
Visualization of activation map We employ GradCAM [Sel-
varaju et al., 2017] to visualize the feature attention map,
as shown in Fig. 4. Our findings indicate that PFD-IQA ef-
fectively focuses on the quality degradation areas, while the
DEIQT[Qin et al., 2023] (the second best in Tab. 1) overly
Index P DA P DR
LIVE
CSIQ
TID2013
KADID
LIVEC
Avg.
PLCC
SRCC
PLCC
SRCC
PLCC
SRCC
PLCC
SRCC
PLCC
SRCC
a)
0.966
0.964
0.952
0.935
0.899
0.888
0.878
0.884
0.881
0.863
0.911
b)
""
0.984
0.981
0.963
0.954
0.927
0.915
0.925
0.925
0.911
0.895
0.938
c)
""
0.983
0.982
0.968
0.959
0.910
0.890
0.918
0.919
0.916
0.897
0.934
d)
""
""
0.985+1.9% 0.985+2.1% 0.972+2.0% 0.962+2.7% 0.937+3.8% 0.924+3.6% 0.935+6.0% 0.931+4.7% 0.922+4.1% 0.906+4.3% 0.946+3.5%
Table 3: Ablation experiments on LIVE, CSIQ, TID2013, KADAD and LIVEC datasets. Here, P DA and P DR refer to the Perceptual
Prior Discovery and Aggregation module and Diffusion Refinement module, where bold entries indicate the best result.
(a) Input Image
(b) DEIQT
(c) PFD-IQA
16.41 
9.91 
6.39 
8.41     5.37 
13.78 
10.21    6.20 
Figure 4: Visualization of the feature of DEIQT and PFD-IQA.
Index
ANA
P P A
LIVEC
KonIQ
Avg. Std.
PLCC
SRCC
PLCC
SRCC
a)
0.881
0.863
0.929
0.914
±0.011
b)
""
0.914
0.896
0.938
0.928
±0.005
c)
""
0.918
0.900
0.940
0.928
±0.008
d)
""
""
0.922
0.906
0.945
0.930
±0.004
Table 4: Ablation experiments about PDR component on LIVEC
and KonIQ datasets. Bold entries indicate the best performance.
relies on semantics and even focuses on less important for
the image quality(e.g., in the second line, DEIQT incorrectly
focuses on the little girl in the bottom left, while ignoring
the overexposure and blurriness in the center of the picture.).
Furthermore, Fig. 4 presents a comparison of quality predic-
tions between our proposed PFD-IQA and the DEIQT. PFD-
IQA consistently outperforms the DEIQT across all levels of
image quality assessment, particularly demonstrating signif-
icant improvement for moderately distorted images, refer to
the supplementary material for more visualizations.
4.6
Ablation Study
This section presents an ablation experiment, and the re-
sults are shown in Tab. 3. The experiment is focused on the
examination of two main modules: the Perceptual Prior Dis-
covery and Aggregation (PDA) module and the Perceptual
Prior-based Diffusion Refinement (PDR) module.
Perceptual Prior Discovery and Aggregation. When ex-
clusively incorporating the PDA module (referred to as sce-
nario b)), discernible enhancements are observed in the PLCC
(Pearson Linear Correlation Coefficient) ranging from 0.8%
to 4.7% across distinct datasets. This outcome underscores
Sampling Number T
LIVE
LIVEC
Avg.
PLCC
SRCC
PLCC
SRCC
1
0.982
0.980
0.913
0.890
0.941
3
0.984
0.982
0.918
0.899
0.945
5
0.985
0.985
0.922
0.906
0.950
10
0.983
0.983
0.921
0.901
0.947
Table 5: Ablation experiments about the number of sampling itera-
tions. Bold entries indicate the best performance.
the module’s efficacy in augmenting the model’s awareness
of quality through the assimilation of supplementary quality-
related knowledge.
A similar positive impact is observed
in the SRCC, manifesting as an improvement ranging from
0.7% to 4.1% across different datasets.
Feature Diffusion Refinement Module.
Upon sole inte-
gration of the PDR module (referred to as scenario c)),
discernible improvements are discerned in the PLCC, with
advancements ranging from 0.9% to 4.0% across diverse
datasets. Correspondingly, there is a notable enhancement
in SRCC, with gains varying from 0.8% to 3.5% across these
datasets. This observation suggests that the PDR module pos-
sesses the capacity to effectively optimize features, even in
the absence of the perceptual text prompts condition. Ad-
ditionally, combining the PDR and PDA modules (scenario
d)) notably enhances PLCC (3.8 to 6.0%) and SRCC (3.6 to
4.7%). This highlights their synergistic effect, substantially
improving PFD-IQA’s robustness and accuracy.
ANA and PPA module. As shown in Tab. 4, the PPA mod-
ule alone enhances performance by up to 3.4% over baseline,
leveraging perceptual text embedding cues, though lacking
in stability. The ANA module, on the other hand, signifi-
cantly lowers the model’s standard deviation and offers com-
petitive performance improvements. When combined, ANA
aligns features with predefined denoising modules, reducing
randomness, while PPA precisely refines perceptual features
via image-text interaction. Consequently, the synergistic co-
operation of both modules helps achieve SOTA performance.
Number of Sampling Iterations. In our study, we employ
DDIM [Song et al., 2020] for acceleration. The experiments
emphasize how different sampling numbers impact perfor-
mance. Tab. 5 demonstrates that single-step denoising sig-
nificantly outperforms the baseline. We find that an iteration
of 5 is adequate for effective performance in our approach.
Therefore, this method is adopted in all experiments to bal-
ance efficiency and accuracy.
5
Conclusion
In conclusion, our study introduces the PFD-IQA, a pio-
neering Blind Image Quality Assessment framework lever-
aging the diffusion model’s noise removal capabilities. Ad-
dressing key challenges in BIQA, PFD-IQA features two
novel modules: the Perceptual Prior Discovery and Aggre-
gation module for improved feature preservation and noise
elimination, and the Perceptual Prior-based Feature Refine-
ment Strategy for defining denoising trajectories in the ab-
sence of explicit benchmarks. These innovations, combin-
ing text prompts with perceptual priors and employing an
adaptive noise alignment mechanism, enable PFD-IQA to re-
fine quality-aware features with precision. Our experiments
demonstrate that PFD-IQA achieves exceptional performance
with minimal sampling steps and a lightweight model, mark-
ing a significant advancement in the application of diffusion
models to image quality assessment.
References
Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior
Wolf. Segdiff: Image segmentation with diffusion prob-
abilistic models. arXiv preprint arXiv:2112.00390, 2021.
Mark R Banham and Aggelos K Katsaggelos. Digital image
restoration. IEEE signal processing magazine, 14(2):24–
41, 1997.
Sebastian Bosse, Dominique Maniry, Klaus-Robert M¨uller,
Thomas Wiegand, and Wojciech Samek.
Deep neural
networks for no-reference and full-reference image qual-
ity assessment. IEEE Transactions on image processing,
27(1):206–219, 2017.
Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Diffu-
siondet: Diffusion model for object detection. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision, pages 19830–19843, 2023.
Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang.
Image super-resolution using deep convolutional
networks. IEEE transactions on pattern analysis and ma-
chine intelligence, 38(2):295–307, 2015.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-
age is worth 16x16 words: Transformers for image recog-
nition at scale. In International Conference on Learning
Representations, 2021.
Stefan Elfwing, Eiji Uchibe, and Kenji Doya.
Sigmoid-
weighted linear units for neural network function approxi-
mation in reinforcement learning. Neural networks, 107:3–
11, 2018.
Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou
Wang. Perceptual quality assessment of smartphone pho-
tography.
In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 3677–
3686, 2020.
Deepti Ghadiyaram and Alan C Bovik.
Massive on-
line crowdsourced study of subjective and objective pic-
ture quality.
IEEE Transactions on Image Processing,
25(1):372–387, 2015.
S Alireza Golestaneh, Saba Dadsetan, and Kris M Kitani.
No-reference image quality assessment via transformers,
relative ranking, and self-consistency. In Proceedings of
the IEEE/CVF Winter Conference on Applications of Com-
puter Vision, pages 1220–1230, 2022.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pat-
tern recognition, pages 770–778, 2016.
Dan Hendrycks and Thomas Dietterich. Benchmarking neu-
ral network robustness to common corruptions and pertur-
bations. arXiv preprint arXiv:1903.12261, 2019.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in neural information
processing systems, 33:6840–6851, 2020.
Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe.
Koniq-10k: An ecologically valid database for deep learn-
ing of blind image quality assessment. IEEE Transactions
on Image Processing, 29:4041–4056, 2020.
Weilong Hou, Xinbo Gao, Dacheng Tao, and Xuelong Li.
Blind image quality assessment via deep learning. IEEE
transactions on neural networks and learning systems,
26(6):1275–1286, 2014.
Tao Huang, Yuan Zhang, Mingkai Zheng, Shan You, Fei
Wang, Chen Qian, and Chang Xu. Knowledge diffusion
for distillation. arXiv preprint arXiv:2305.15712, 2023.
Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and
Feng Yang. Musiq: Multi-scale image quality transformer.
In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 5148–5157, 2021.
Jongyoo Kim and Sanghoon Lee.
Fully deep blind image
quality predictor. IEEE Journal of selected topics in signal
processing, 11(1):206–220, 2016.
Eric Cooper Larson and Damon Michael Chandler. Most ap-
parent distortion: full-reference image quality assessment
and the role of strategy.
Journal of electronic imaging,
19(1):011006, 2010.
Xuelong Li, Dacheng Tao, Xinbo Gao, and Wen Lu. A nat-
ural image quality evaluation metric. Signal Processing,
89(4):548–555, 2009.
Hanhe Lin, Vlad Hosu, and Dietmar Saupe.
Kadid-10k:
A large-scale artificially distorted iqa database. In 2019
Eleventh International Conference on Quality of Multime-
dia Experience (QoMEX), pages 1–3. IEEE, 2019.
Xialei Liu, Joost van de Weijer, and Andrew D. Bagdanov.
Rankiqa: Learning from rankings for no-reference image
quality assessment. In The IEEE International Conference
on Computer Vision (ICCV), Oct 2017.
Kede Ma, Wentao Liu, Kai Zhang, Zhengfang Duanmu, Zhou
Wang, and Wangmeng Zuo. End-to-end blind image qual-
ity assessment using deep neural networks. IEEE Transac-
tions on Image Processing, 27(3):1202–1213, 2017.
Anish Mittal, Anush Krishna Moorthy, and Alan Conrad
Bovik. No-reference image quality assessment in the spa-
tial domain.
IEEE Transactions on image processing,
21(12):4695–4708, 2012.
Zhaoqing Pan, Hao Zhang, Jianjun Lei, Yuming Fang, Xiao
Shao, Nam Ling, and Sam Kwong.
Dacnn: Blind im-
age quality assessment via a distortion-aware convolutional
neural network. IEEE Transactions on Circuits and Sys-
tems for Video Technology, 32(11):7518–7531, 2022.
Nikolay Ponomarenko, Lina Jin, Oleg Ieremeiev, Vladimir
Lukin, Karen Egiazarian, Jaakko Astola, Benoit Vozel,
Kacem Chehdi, Marco Carli, Federica Battisti, et al. Im-
age database tid2013: Peculiarities, results and perspec-
tives. Signal processing: Image communication, 30:57–77,
2015.
Guanyi Qin, Runze Hu, Yutao Liu, Xiawu Zheng, Haotian
Liu, Xiu Li, and Yan Zhang. Data-efficient image quality
assessment with attention-panel decoder. In Proceedings
of the Thirty-Seventh AAAI Conference on Artificial Intel-
ligence, 2023.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language su-
pervision. In International conference on machine learn-
ing, pages 8748–8763. PMLR, 2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 10684–10695, 2022.
Michele A Saad, Alan C Bovik, and Christophe Charrier.
Blind image quality assessment: A natural scene statistics
approach in the dct domain. IEEE transactions on Image
Processing, 21(8):3339–3352, 2012.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,
Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
Grad-cam: Visual explanations from deep networks via
gradient-based localization. In Proceedings of the IEEE
international conference on computer vision, pages 618–
626, 2017.
Hamid R Sheikh, Muhammad F Sabir, and Alan C Bovik. A
statistical evaluation of recent full reference image quality
assessment algorithms. IEEE Transactions on image pro-
cessing, 15(11):3440–3451, 2006.
Chenyang Shi and Yandan Lin. Full reference image quality
assessment based on visual salience with color appearance
and gradient similarity.
IEEE Access, 8:97310–97320,
2020.
Jiaming Song, Chenlin Meng, and Stefano Ermon.
De-
noising diffusion implicit models.
arXiv preprint
arXiv:2010.02502, 2020.
Tianshu Song, Leida Li, Deqiang Cheng, Pengfei Chen,
and Jinjian Wu.
Active learning-based sample selection
for label-efficient blind image quality assessment. IEEE
Transactions on Circuits and Systems for Video Technol-
ogy, 2023.
Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jin-
qiu Sun, and Yanning Zhang. Blindly assess image qual-
ity in the wild guided by a self-adaptive hyper network.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 3667–3676, 2020.
Dacheng Tao, Xuelong Li, Wen Lu, and Xinbo Gao.
Reduced-reference iqa in contourlet domain. IEEE Trans-
actions on Systems, Man, and Cybernetics, Part B (Cyber-
netics), 39(6):1623–1627, 2009.
Hugo Touvron, Matthieu Cord, and Herv´e J´egou. Deit iii: Re-
venge of the vit. arXiv preprint arXiv:2204.07118, 2022.
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility
to structural similarity. IEEE transactions on image pro-
cessing, 13(4):600–612, 2004.
Jinjian Wu, Jupo Ma, Fuhu Liang, Weisheng Dong, Guang-
ming Shi, and Weisi Lin. End-to-end blind image qual-
ity prediction with cascaded deep neural network. IEEE
Transactions on image processing, 29:7414–7426, 2020.
Xiaohan Yang, Fan Li, and Hantao Liu. Ttl-iqa: Transitive
transfer learning based no-reference image quality assess-
ment. IEEE Transactions on Multimedia, 23:4326–4340,
2020.
Zhendong Yang, Zhe Li, Mingqi Shao, Dachuan Shi, Zehuan
Yuan, and Chun Yuan. Masked generative distillation. In
European Conference on Computer Vision, pages 53–69.
Springer, 2022.
Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Maha-
jan, Deepti Ghadiyaram, and Alan Bovik. From patches
to pictures (paq-2-piq): Mapping the perceptual space of
picture quality. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pages
3575–3585, 2020.
Junyong You and Jari Korhonen. Transformer for image qual-
ity assessment. In 2021 IEEE International Conference on
Image Processing (ICIP), pages 1389–1393. IEEE, 2021.
Lin Zhang, Lei Zhang, and Alan C Bovik. A feature-enriched
completely blind image quality evaluator. IEEE Transac-
tions on Image Processing, 24(8):2579–2591, 2015.
Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, and Zhou
Wang. Blind image quality assessment using a deep bilin-
ear convolutional neural network. IEEE Transactions on
Circuits and Systems for Video Technology, 30(1):36–47,
2018.
Linfeng Zhang, Xin Chen, Junbo Zhang, Runpei Dong, and
Kaisheng Ma. Contrastive deep supervision. In Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv,
Israel, October 23–27, 2022, Proceedings, Part XXVI,
pages 1–19. Springer, 2022.
Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang Yang,
and Kede Ma. Blind image quality assessment via vision-
language correspondence: A multitask learning perspec-
tive. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 14071–14081,
2023.
Kai Zhao, Kun Yuan, Ming Sun, Mading Li, and Xing Wen.
Quality-aware pre-trained models for blind image qual-
ity assessment. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pages
22302–22313, 2023.
Yu Zhou, Leida Li, Shiqi Wang, Jinjian Wu, Yuming Fang,
and Xinbo Gao.
No-reference quality assessment for
view synthesis using dog-based edge statistics and tex-
ture naturalness. IEEE Transactions on Image Processing,
28(9):4566–4579, 2019.
Hancheng Zhu, Leida Li, Jinjian Wu, Weisheng Dong, and
Guangming Shi.
Metaiqa: Deep meta-learning for no-
reference image quality assessment.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 14143–14152, 2020.
"
"In many existing coverless steganography methods, a mapping relationship is established between cover images and hidden data, resulting in an exponential growth in the number of images stored in the database as steganographic capacity increases. Building an image database is challenging due to the high steganographic capacity requirement. To address this issue, we propose an efficient coverless scheme based on dynamically matched substrings. YOLO is employed for selecting optimal objects, and a mapping dictionary is established between these objects and scrambling factors. With the aid of this dictionary, each image is effectively assigned to a specific scrambling factor, which is used to scramble the receiver’s sequence key. To achieve sufficient steganography capability based on a limited image library, all substrings of the scrambled sequences hold the potential to hide data. After completing the secret information matching, the ideal number of stego images will be obtained from the database. According to experimental results, this technology outperforms most previous works in data load, transmission security, and hiding capacity. Under typical geometric attacks, it can recover 79.85% of secret information on average. Furthermore, only approximately 200 random images are needed to meet a capacity of 19 bits per image.","Steganography involves transmitting secret information through various media such as texts, videos, audio, or images. Image steganography has advanced significantly, but modifications to image pixels or transform domain coefficients often result in changes to the statistical characteristics of the images, which may still be recognized by some steganalyzers. Coverless steganography directly selects images as stego ones based on mapping rules. Recently, deep learning models have been increasingly integrated into coverless steganography based on mapping rules. These techniques usually require building a sizable image database, posing challenges in maintaining uniqueness and preventing feature collisions among those extracted from different images. Cover image generation is an alternate way of coverless steganography that uses generative models, such as generative adversarial networks (GAN) to produce stego images for hiding secret information. For the idea of coverless steganography, we consider making the length of the information hidden in each cover image variable rather than fixed, in order to improve the hiding capability while reducing the burden on the database.","nanIn 2017, an image texture synthesis technique based on ACGAN was proposed, wherein the hidden data are transformed into noise and fed into the ACGAN network. However, there was a lack of realism in the generated images. Zhang et al. used GAN to generate multiple texture images, which then formed a mapping to binary sequences. However, diversity breaks down when the number of produced images reaches a threshold. An image steganography method based on multi-domain image transformation was proposed to address the issue of insufficient cover images. The secret message is hidden using a generator and recovered using a classifier. Inspired by irreversibility issues with traditional neural networks, a method iteratively updates the noise vector using gradient descent for data extraction. While these methods lessen the load on the image database, the resulting images lack quality and diversity, making it easier for attackers to detect variations.nannan","We explored an Optimal Object Filtering Algorithm (OOFA) using YOLO-v5 to establish a robust mapping rule between the secret information and cover images. To increase the hidden bits per image, we extend the sequence key length sufficient to dynamically match the secret information with multiple substrings. The previously created mapping dictionary has established a mapping relationship between the image objects and scrambling factors. After the previous three steps, we can build a four-level data architecture. Then match the image in the data architecture mentioned above, which can be divided into the following steps: For calculating the First n bits, the value of n in the first matching is computed by formula 5 to steer clear of such futile matching endeavors. Match all substrings with length n in the third level of the data architecture, Match(MG, SS) == 0 indicates the matching is failed, then set n to n − 1, repeat this step. Otherwise, a stego image sim could be obtained. For Record position and length, the location information keym = {kf, kl} (including the left position index and length of the substring) should be record.nan","Within the same size database, steganography capacity increases with key length, but the increment diminishes. Considering retrieval efficiency, we recommend a 10000-bit key, yielding around 19 bits of capacity. Our method has a slight advantage over these methods. In particular, in [11], [14], [24], the value of n is 1 ∼ 3 in general. A database size within the 1500 range significantly influences steganography capacity. We randomly selected images (50 to 1200) from the COCO dataset, conducted experiments, and determined the average scrambling factors per dataset. Table III shows a substantial increase in scrambling factors as the database size rises from 50 to 200. Beyond 200 images, increasing the database size to 1200 does not notably affect scrambling factors, indicating that 200 cover images suffice for steganography requirements. For geometric attacks, Table V shows that our method have a good promotion in robustness, this is because the precision of the object detection model and the details of the mapping rules. Facing to cropping attacks, in particular, it has an ascending advantage over other deep learning-based methods. Table VI shows that we achieve an average of 87.4%, which is marginally better than other deep learning models.","Our innovative coverless steganography relies on dynamic mapping, avoiding direct connections between images and binary sequences. A scrambling factor is extracted from the cover image, generating a new sequence for information hiding. Dynamic stego image selection maximizes capacity within sequence length constraints, effectively reducing the database load. Each image corresponds to numerous sequences with varying lengths, easing database construction and maintenance. Our method withstands all steganalysis, ensuring impeccable safety. Experimental results demonstrate robustness to common attacks. While our capacity currently lags behind traditional steganography, future work will aim to significantly enhance concealed information capabilities in coverless steganography.",A Dynamic YOLO-Based Sequence-Matching Model for Efficient Coverless Image Steganography,"Jiajun Liu, Lina Tan, Zhili Zhou, Yi Li, Peng Chen","1
A Dynamic YOLO-Based Sequence-Matching Model for Efficient
Coverless Image Steganography
Jiajun Liu1, Lina Tan1,*, Zhili Zhou2, Yi Li1, Peng Chen1
1School of Computer Science at Hunan University of Technology and Business, Changsha 410205, China
2Institute of Artificial Intelligence, Guangzhou University, China
Many existing coverless steganography methods establish a
mapping relationship between cover images and hidden data.
There exists an issue that the number of images stored in the
database grows exponentially as the steganographic capacity
rises. The need for a high steganographic capacity makes it
challenging to build an image database. To improve the image
library utilization and anti-attack capability of the steganog-
raphy system, we present an efficient coverless scheme based
on dynamically matched substrings. YOLO is employed for
selecting optimal objects, and a mapping dictionary is established
between these objects and scrambling factors. With the aid of
this dictionary, each image is effectively assigned to a specific
scrambling factor, which is used to scramble the receiver’s
sequence key. To achieve sufficient steganography capability
based on a limited image library, all substrings of the scrambled
sequences hold the potential to hide data. After completing the
secret information matching, the ideal number of stego images
will be obtained from the database. According to experimental
results, this technology outperforms most previous works on data
load, transmission security, and hiding capacity. Under typical
geometric attacks, it can recover 79.85% of secret information on
average. Furthermore, only approximately 200 random images
are needed to meet a capacity of 19 bits per image.
Index
Terms—coverless;
steganography;
object
detection;
YOLO
I. INTRODUCTION
Steganography can transmit secret information through vari-
ous media such as videos, images, audio, or texts. Even though
image steganography has advanced significantly [1]–[5], mod-
ifications to image pixels or transform domain coefficients
always result in changes to the statistical characteristics of the
images, which may still be recognized by some steganalyzers.
Coverless steganography [6] directly selects images as stego
ones based on mapping rules. Zheng et al. [7] enhanced the
method with SIFT feature points, improving resistance to
rotation and scaling attacks. Zou et al. [8] introduced a model
based on average pixel values for increased steganography
capacity. In [9], [10], sub-block coefficients in the transform
domain are used for robust feature sequences, which have
improved resistance to noise attacks compared to previous
models, but are sensitive to geometric attacks.
Recently, deep learning models have been increasingly
integrated into coverless steganography based on mapping
rules. In 2020, Luo et al. [11] proposed a Faster RCNN-based
steganography, establishing a mapping dictionary between
objects and labels for secret information concealment. This
method is robust against geometric attacks but less tolerant to
noise compared to previous works. In the same year, Liu et
al. [12] employed DenseNet [13] to extract high-dimensional
CNN features for hash sequence mapping. Another approach
[14] hides secret information by semantic feature extraction
and image object region segmentation. Additionally, a cov-
erless scheme using camouflage images and CNN features
was introduced [15], which has more flexible capacity settings
and robustness against image attacks. However, the above
techniques usually require building a sizable image database,
posing challenges in maintaining uniqueness and preventing
feature collisions amongst those extracted from different im-
ages.
Cover image generation is an alternate way of coverless
steganography that uses generative models, such as generative
adversarial networks (GAN) [16], to produce stego images for
the purpose of hiding secret information. An image texture
synthesis technique based on ACGAN was proposed in 2017
[17], wherein the hidden data are transformed into noise and
fed into the ACGAN network. However, there was a lack
of realism in the generated images. Zhang et al. [18] used
GAN to generate multiple texture images, which then formed a
mapping to binary sequences. However, diversity breaks down
when the number of produced images reaches a threshold. To
increase the hiding capacity, Chen et al. [19] applied SIFT
[20] to select images and StarGAN [21] to generate new
images based on mapping face attribute features to hidden
data. Although the security and robustness are achieved, the
capacity remains limited. A steganography method based on
multi-domain image transformation was proposed [22] to
address the issue of insufficient cover images. The secret
message is hidden using a generator and recovered using
a classifier. Inspired by irreversibility issues with traditional
neural networks, a method [23] iteratively updates the noise
vector using gradient descent for data extraction. While these
methods lessen the load on the image database, the resulting
images lack quality and diversity, making it easier for attackers
to detect variations. Moreover, the image content may be lost
or altered during network operations, hindering accurate data
extraction.
For the idea of coverless steganography, we consider making
the length of the information hidden in each cover image
variable rather than fixed, in order to improve the hiding
capability while reducing the burden on the database. A
sequence of length u generates (u2 + u)/2 binary substrings,
representing an image as such. This enables dynamic matching
of secret information fragments with substrings of varying
arXiv:2401.11946v1  [cs.CR]  22 Jan 2024
2
lengths. The larger the u is, the more substrings are yielded
for information matching, and relatively fewer images are
needed in the database. To address coverless steganography’s
limited robustness, we leverage high-level semantic features
for flexible mapping rules. Recent advancements in object
detection models, especially YOLO, have shown superior real-
time performance and high average accuracy compared to
techniques like DPM and R-CNN. This solution uses YOLO
mainly due to its fast inference speed and stable feature
extraction. The main contributions of this paper are as follows:
1) In view of the superior detection accuracy and inference
time of YOLOs, we explored an Optimal Object Fil-
tering Algorithm (OOFA) using YOLO-v5 to establish
a robust mapping rule between the secret information
and cover images. Tests conducted under geometric and
noise attacks demonstrate the remarkable robustness of
our method, which reaches nearly 87.4%.
2) To increase the hidden bits per image, we extend the
sequence key length sufficient to dynamically match the
secret information with multiple substrings. Experiments
show that, under ideal conditions, our approach outper-
forms existing methods with an average hiding capacity
of 19 bits per image.
3) To mitigate the impact of hiding capacity on image
database load, we hide the information indirectly in the
keys rather than in the cover images. In contrast to other
approaches, ours requires fewer cover images to meet the
same hiding capacity.
II. THE PROPOSED METHOD
A. Overall Framework
Fig. 1 display the suggested framework. Its three main
components are preprocessing, information hiding, and in-
formation extraction. During the preprocessing stage, each
receiver has a unique sequence key. Once YOLO identifies all
cover images in the database, a mapping dictionary must be es-
tablished to generate the scrambling factors. These factors will
then be used to scrambled the sequence keys, leading to the
development of a data architecture based on the relationships
between the sequence keys, scrambling factors, scrambled se-
quences, and cover images. In the stage of information hiding,
after secret data matches substrings of scrambled sequences,
the sender transmits the position key and corresponding stego
images to the receiver. For each stego image, the receiver
employs YOLO with the same settings to detect the optimal
objects in it, and extract secret message segments through the
sequence key, factors, and position key.
B. Preprocessing
In our system, the sender and each receiver agree on a
specific sequence key while ensuring perfect confidentiality.
Preprocessing tasks like establishing the mapping dictionary,
producing the scrambling factors, and creating the inverted
index, are necessary before transferring the secret information.
1) Mapping Dictionary Creation
A mapping dictionary needs to be built to convert the object
labels of images recognized by YOLO into scrambling factors.
After all images in the database are detected by YOLO, a
label list is obtained for all the objects. These labels are
sorted alphabetically and each object will be assigned a unique
scrambling factor according to the mapping dictionary. The
mapping rules can be updated periodically to ensure security,
such as in ascending or descending order.
2) Sequence Key Distribution
Each receiver must be issued a unique sequence key of
length t whose value is empirically set to 10000 following
capacity and time cost testing. There are two approaches to
assigning each receiver a specific sequence key. One way is to
create a unique sequence by using the recipient ID as a random
seed and reordering the initial sequence by the pseudo-random
number generator. Another way is to manually distribute a
bit-string of length t to each recipient, which exhibits a
high degree of randomness, and when combined with proper
security measures, it becomes exceedingly challenging for any
third party to deduce the assigned bit strings. Considering that
the system is more vulnerable to attacks if the attacker is
skilled with the pseudo-random function and receiver IDs, we
choose the first method for key distribution.
3) Scrambling Factor Generation
The previously created mapping dictionary has established
a mapping relationship between the image objects and scram-
bling factors. YOLO may find multiple objects from an image
since it handles multi-object classification and localization. It’s
worth noting that those objects with lower category probabil-
ities may not be detected again after the image is attacked,
which would result in poor robustness of the steganography
model. Therefore, our algorithm will eliminate those undesir-
able bounding boxes and keep only the ones with the highest
category probability. The generation process of scrambling
factors is given as follows.
1) Detect an input image c with YOLO to get all the
objects OB. The area of the bounding box and class
probability of each object are recorded as A(ob) and
P(ob) respectively.
OB = ob1||ob2||...||obn
(1)
2) In general, objects with larger area or higher cate-
gory probability tend to represent better robustness.
Therefore, we set a threshold P and A to filter out
the objects with low robustness performance. After the
objects whose bounding box area is less than A are
filtered out, the one with class probability exceeding P
and maximum is selected among the remaining objects.
OBfiltered = {obi ∈ OB|Ai > A}
(2)
obopt =
arg max
obi∈OBfiltered
{Pi|Pi > P}
(3)
3) If the obopt can not be found, it means that the image
is not suitable as a stego image, then the image is
3
Fig. 1: Flowchart of The Coverless Image Steganography Algorithm
supposed to be discarded, otherwise, a scrambling factor
is generated according to the dictionary D.
f =
(
Null,
if obopt == Null;
D[obopt],
else;
(4)
After the above steps, images with a suitable object can
generate a scrambling factor. Through a lot of experiments,
we believe that the value of A is 15% of the total pixels
of the image is suitable. Moreover, we found that the class
probability of some objects may not decrease after being
attacked, or even increase, so we set the P to 50%.
4) Building Data Architecture
After the previous three steps, we can build a four-level
data architecture. Since it is generated and saved in advance,
it is unnecessary for the sender to reprocess all images before
hiding the secret information, which can save vast amount of
time for the information hiding process. The first level are
the sequence keys owned by all receivers, the second level
are the scrambling factors generated by the images, and the
third level are the scrambled sequence obtained by scrambling
sequence keys. Each fourth level contains a list of images, and
the same scrambling factor can be extracted from the images
in the same list.
It is important to note that some images have the same
suitable object, they will be mapped to the same scrambling
factor, and these images will be putted into the same image
list. Fig. 2 shows the data architecture.
C. Information Hiding
Before selecting stego images, the secret information SM
needs to be converted into a binary stream M whose length
is denoted as L. The length of the sequence key also needs
to be considered during the matching process, which is noted
t. Then match the image in the data architecture mentioned
above, which can be divided into the following steps:
Fig. 2: The Data Architecture of Image Database
1) For
MG,
select
its
first
n
bits
MG
=
mg1||mg2||...||mgn.
Due
to
the
impossibility
of
matching extensive information within brief sequences,
the value of n in the first matching is computed
by formula 5 to steer clear of such futile matching
endeavors.
nfirst = min(t, L)
(5)
2) Match all substrings with length n in the third level of
the data architecture , Match(MG, SS) == 0 indicates
the matching is failed, then set n to n − 1, repeat this
step. Otherwise, a stego image sim could be obtained.
(
n = n − 1,
if Match(MG, SS) == 0;
SI = SI||sim,
else;
(6)
Match(MG, SS) =
L−n+1
max
i=1
δ(MG, SS[i : i + n − 1])
(7)
In addition, the location information keym = {kf, kl}
(including the left position index and length of the
substring) should be record.
kf = argmax(Match(MG, SS))
(8)
kl = n
(9)
3) Regard the rest of information as a new piece of infor-
mation, and set L to L − n, repeat the previous steps
4
until all the secret information is matched successfully,
and get the stego images SI.
SI = si1||si2||...||sim
(10)
4) Since the position and length of substrings are also the
key factors to recover secret information accurately, the
Key should be transmitted to the receiver. We encrypt
Key prior to transmission using the AES algorithm out
of consideration for security.
Key = key1||key2||...||keym
(11)
This algorithm design ensures the security of secret informa-
tion while maximizing the hiding capacity of the stego image.
Furthrmore, we can pass Key and stego images to the receiver
respectively in different time periods and channels.
D. Information Extraction
Some stego images may be attacked during transmission
and the suitable objects fails to be detected, so that the re-
ceiver cannot restore the information accurately. For avoiding
a missing object affecting the extraction of other location
information, the the same number of ’0’ need to be added
to the missing position. The Key can provide us with the
lengths of information hidden in stego images. This operation
is shown in Fig. 3.
Fig. 3: Process Chart for Information Extraction
The steps of information extraction are as follows
1) Firstly, get all the objects OB after using YOLO to
detect the jth stego image sij, with 1 ≤ j ≤ m.
OBj = ob1||ob2||...||obn
(12)
2) Secondly, the objects with small bounding box area less
than A are filtered out from OBj, then The object with
the highest classification probability is chosen as obopt.
OBfiltered = {obi ∈ OBj|Ai > A}
(13)
obopt =
arg max
obi∈OBfiltered
{Pi|Pi > P}
(14)
3) Match obopt in the mapping dictionary D to get a
scrambling factor fj. Then fj is used to scramble the
sequence key SKx own by receiver to generate the
scrambled sequence SSj
x.
fj = D[obopt]
(15)
SSj
x = Scrambling(SKx, fj)
(16)
4) Extract the hidden secret information mgj of the image
from SSx through keyj.
mgj = SSj
x[keyj[0], keyj[0] + keyj[1] − 1]
(17)
5) Finally, the all information fragments extracted from
stego images are spliced together to recover the binary
stream representation of secret information M, then M
is converted to secret information SM.
M = mg1||mg2||...||mgm
(18)
III. EXPERIMENTS AND ANALYSIS
Experimental environment: Intel(R) Xeon(R) Silver 4310
CPU @ 2.10GHz, 30.00GB RAM and one Nvidia GeForce
RTX A4000 GPU. All experiments are completed in Pycharm
and MATLAB R2021a.
Data set: MS COCO 2017 includes 118287 training images,
5000 validation images, and 40670 test images. This data set
contains natural pictures and common object pictures in life
and and is known for its complex background and relatively
large number of objects, making it ideal for training YOLO-
v5.
A. Hiding Capacity
1) Capacity Test of Our Method
The steganography capacity is influenced by the sequence
key length and scrambling factors. We randomly selected
images (100 to 5000) from the COCO dataset, forming six
databases. Tests were conducted with sequence key lengths
ranging from 100 to 50000 bits. Results in Table I indicate
that beyond a certain image number, capacity impact is negli-
gible. Within the same size database, steganography capacity
increases with key length, but the increment diminishes. Con-
sidering retrieval efficiency, we recommend a 10000-bit key,
yielding around 19 bits of capacity.
2) Comparison of Capacity
In this comparison, our method is evaluated against others
relying on mapping rules. Due to some unreproducible models,
comparison data was extracted from the original papers or
their citations. The comparison results are given in the Table
II. In particular, in [11], [14], [24], the value of n is 1 ∼ 3
in general. It is concluded that our method still has a slight
advantage over these methods.
B. Database Size
In order to carry out image matching on every conceivable
binary stream of confidential information, it is essential to
have a comprehensive database. However, the current coverless
steganography is limited by the length of the hash, and the
reason why the length of the hash cannot not be overly long
is because of the difficulty in constructing a comprehensive
database. Therefore, it is worth exploring how to effectively
reduce the size of the image database.
5
TABLE I: Hiding capacity of our scheme
images
length(bits)
100
400
800
1000
2000
5000
8000
10000
15000
20000
100
11.688
14.754
14.876
15.517
16.364
17.822
18.182
18.947
19.149
19.565
500
12.676
14.876
15.652
16.071
17.143
18.557
19.149
19.565
20.0
20.455
1000
12.676
14.634
15.929
16.216
17.143
18.557
19.355
19.355
20.225
20.690
1500
12.857
15.0
16.216
16.364
17.308
18.75
19.565
19.565
20.225
20.455
2000
12.950
15.0
16.216
16.822
17.647
18.75
19.565
19.565
20.455
20.690
5000
13.043
15.0
16.364
16.822
17.876
18.75
19.565
19.565
20.455
20.930
TABLE II: The capacity of schemes
methods
capacity (bits/image)
ours
19
CI-CIS [15]
1 ∼ 15
Mask RCNN [14]
(8 ∼ 15) × n
Multiple Objects [11]
6 × n
Faster RCNN [24]
5 × n
Pixel [6]
8
SIFT [7]
8
DCT [9]
1 ∼ 15
DWT [10]
1 ∼ 15
1) Database Size Test of Our Method
In our setup, each object is linked to a unique scram-
bling factor, determined by the quantity of identified enti-
ties fulfilling the hidden demand. We explore the impact
of different database sizes on scrambling factors, conducting
experiments and evaluating their numbers. Notably, in Table I,
a database size within the 1500 range significantly influences
steganography capacity. We randomly selected images (50 to
1200) from the COCO dataset, conducted experiments, and
determined the average scrambling factors per dataset. Table
III shows a substantial increase in scrambling factors as the
database size rises from 50 to 200. Beyond 200 images,
increasing the database size to 1200 does not notably affect
scrambling factors, indicating that 200 cover images suffice
for steganography requirements.
TABLE III: Database size test(the length of sequence is 10000)
number of images
number of factors
capacity(bits/image)
50
22
17.947
100
34
18.680
200
50
19.151
400
55
19.273
600
62
19.440
800
66
19.566
1000
67
19.608
1200
75
19.620
2) Comparison of Database Size
As should be expected, we compare our method with other
models based on mapping rules. Due to the different capacities
utilized in these methods, it is impossible to make a direct
comparison between their image database sizes. All methods
are supposed to be expanded to 19 bits based on their original
methods, and then the database size value can be attained
via calculation. Considering that we could not get a sufficient
amount of images to finish up the experiment, we had to make
a manual estimation of the theoretical minimum. The results
are shown in the Table IV, which show us that other methods
require an enormous amount of effort and time to construct an
image database, whereas our method only necessitates around
200 random images to build the same. Consequently, our
method has a great predominance in database management
by comparison.
C. Robustness
We use 4 common geometric attacks and 7 common noise
attacks test the robustness of some typical methods under
different parameters. And the rate of secret information ex-
tracted correctly is used to measure robustness. The calculation
method for:
R = SMc
SMo
(19)
Where SMc is the length of the correctly extracted secret
information, and SMo is the length of the original information.
For geometric attacks, Table V shows that our method have
a good promotion in robustness, this is because the precision
of the object detection model and the details of the mapping
rules. Facing to cropping attacks, in particular, it has an
ascending advantage over other deep learning-based methods.
From Table VI , compared to noise attacks of traditional
methods, our robustness is slightly weaker by reason of the
limit to the accuracy of existing object detection models. But
the performance is still impressive, with our method achieving
an average of 87.4%, which is marginally better than other
deep learning models.
D. Security
Our method effectively conceals secret information by
forming a bond between objects, scrambling elements and a
sequence key. The statistical properties of the image remain
unchanged, meaning that existing steganalysis techniques can-
not detect stego images that contain the secret information.
Thus it can be seen that our method has an impressive level
of resistance to steganalysis.
Even if the stego images are suspected by the attacker and
the Key is also cracked, the attacker would still be unable to
extract the secret information, because the mapping dictionary
and sequence key are agreed upon in advance and kept strictly
confidential by both the sender and receiver. Despite the fact
that one receiver inadvertently leaked the mapping dictionary
and its sequence key, the security of the other receivers was not
compromised, since each receiver has its own unique sequence
key. This demonstrates the robustness of our method in terms
of safety.
6
TABLE IV: Database size required for different methods (for 20-bit capacity)
Methods
Descriptions
Number of database images
ours
It generates a scrambling factor to hide information
200
Mask RCNN [14]
It generates k binary sequence based on object areas
≥ 174763
Multiple Objects [11]
It generates a sequence with fix length based on the number of objects
≥ 524288
Faster RCNN [24]
ditto
≥ 524288
Pixel [6]
It generates a sequence with fix length based on average pixel
≥ 524288
SIFT [7]
It generates a sequence with fix length based on SIFT feature
≥ 524288
DCT [9]
It generates l binary sequence based on the DCT coefficients of the sub-blocks
≥ 2048
DWT [10]
It generates l binary sequence based on the DWT coefficients of the sub-blocks
≥ 2048
TABLE V: The robustness of geometric attacks
Attack
parameters
Ours
Multiple objects [11]
Faster RCNN [24]
Pixel [6]
SIFT [7]
DCT [9]
DWT [10]
Center cropping
5%
84.4%
51.2%
27.6%
47.4%
42.8%
48.4%
47.6%
10%
76.0%
46.2%
23.2%
29.4%
22.6%
30.2%
27.0%
20%
57.3%
34.8%
16.4%
22.4%
6.0%
22.6%
15.2%
Edge cropping
5%
94.2%
87.6%
59.6%
58.2%
31.2%
59.4%
64.2%
10%
95.6%
86.2%
57.8%
38.8%
13.0%
39.8%
45.8%
20%
90.2%
82.0%
54.0%
23.2%
6.8%
21.6%
23.8%
Rotation
10
86.7%
78.4%
50.8%
8.0%
2.6%
8.0%
8.6%
30
61.3%
63.2%
40.2%
1.4%
1.8%
1.4%
0.8%
50
50.7%
46.4%
29.8%
1.6%
1.2%
1.8%
0.8%
Translation
(36,20)
89.8%
83.2%
54.8%
20.4%
4.6%
20.2%
17.2%
(40,25)
87.6%
83.6%
54.8%
16.4%
3.8%
16.6%
13.0%
(80,50)
84.4%
77.0%
50.2%
6.0%
2.0%
5.2%
4.8%
TABLE VI: The robustness of noise attacks
Attack
parameters
Ours
Multiple objects [11]
Faster RCNN [24]
Pixel [6]
SIFT [7]
DCT [9]
DWT [10]
Gaussian noise
0.001
67.1%
71.8%
43.2%
95.8%
65.6%
95.4%
98.0%
Salt and pepper noise
0.001
89.8%
86.0%
59.2%
99.0%
90.8%
99.2%
99.6%
Speckle noise
0.01
89.3%
83.6%
56.4%
96.6%
74.4%
96.2%
98.0%
Median filtering
3 × 3
89.7%
87.2%
56.6%
99.6%
88.4%
99.4%
100%
Mean filtering
3 × 3
90.2%
86.0%
57.0%
98.8%
73.6%
95.8%
97.8%
Gaussian filtering
3 × 3
91.6%
89.6%
61.2%
99.8%
92.6%
100%
100%
Scaling
3
94.2%
91.2%
67.8%
99.6%
95.2%
100%
100%
IV. CONCLUSIONS
Our innovative coverless steganography relies on dynamic
mapping, avoiding direct connections between images and
binary sequences. A scrambling factor is extracted from the
cover image, generating a new sequence for information
hiding. Dynamic stego image selection maximizes capacity
within sequence length constraints, effectively reducing the
database load. Each image corresponds to numerous se-
quences with varying lengths, easing database construction
and maintenance. Our method withstands all steganalysis,
ensuring impeccable safety. Experimental results demonstrate
robustness to common attacks. While our capacity currently
lags behind traditional steganography, future work will aim
to significantly enhance concealed information capabilities in
coverless steganography.
REFERENCES
[1] C. H. Yang, C. Y. Weng, S. J. Wang, and H. M. Sun, “Adaptive
data hiding in edge areas of images with spatial lsb domain systems,”
IEEE Transactions on Information Forensics & Security, vol. 3, no. 3,
pp. p.488–497, 2008.
[2] R. T. Mckeon, “Strange fourier steganography in movies,” IEEE Inter-
national Conference on Electro/information Technology, 2007.
[3] M. Y. Valandar, P. Ayubi, and M. J. Barani, “A new transform domain
steganography based on modified logistic chaotic map for color images,”
Journal of Information Security & Applications, vol. 34, 2017.
[4] I. J. Cox, “Secure spread spectrum watermarking for images, audio and
video,” IEEE Int.conf.image Processing, vol. 3, 2002.
[5] W. H. Lin, S. J. Horng, T. W. Kao, P. Fan, C. L. Lee, and Y. Pan, “An
efficient watermarking method based on significant difference of wavelet
coefficient quantization,” IEEE transactions on multimedia, vol. 10,
no. 5, pp. p.746–757, 2008.
[6] Z. Zhou, H. Sun, R. Harit, X. Chen, and X. Sun, “Coverless image
steganography without embedding,” International Conference on Cloud
Computing & Security, 2015.
[7] S. Zheng, W. Liang, B. Ling, and D. Hu, “Coverless information hiding
based on robust image hashing,” International Conference on Intelligent
Computing, 2017.
[8] L. Zou, J. Sun, G. Min, W. Wan, and B. B. Gupta, “A novel coverless
information hiding method based on the average pixel value of the sub-
images,” Multimedia Tools and Applications, 2019.
[9] X. Zhang, F. Peng, and M. Long, “Robust coverless image steganog-
raphy based on dct and lda topic classification,” IEEE Transactions on
Multimedia, pp. 3223–3238, 2018.
[10] Q. Liu, X. Xiang, J. Qin, Y. Tan, and Y. Luo, “Coverless steganogra-
phy based on image retrieval of densenet features and dwt sequence
mapping,” Knowledge-Based Systems, vol. 192, p. 105375, 2019.
[11] Y. Luo, J. Qin, X. Xiang, and Y. Tan, “Coverless image steganography
based on multi-object recognition,” IEEE Transactions on Circuits and
Systems for Video Technology, vol. PP, no. 99, pp. 1–1, 2020.
[12] Q. Liu, X. Xiang, J. Qin, Y. Tan, and Y. Qiu, “Coverless image
steganography based on densenet feature mapping,” EURASIP Journal
on Image and Video Processing, vol. 2020, no. 1, p. 39, 2020.
[13] G. Huang, Z. Liu, V. Laurens, and K. Q. Weinberger, “Densely con-
nected convolutional networks,” IEEE Computer Society, 2016.
[14] Y. J. Luo, J. Qin, X. Xiang, Y. Tan, and N. N. Xiong, “Coverless image
steganography based on image segmentation,” Computers, Materials and
Continua, vol. 64, no. 2, pp. 1281–1295, 2020.
[15] Q. Liu, X. Xiang, J. Qin, Y. Tan, and Q. Zhang, “A robust coverless
steganography scheme using camouflage image.,” IEEE Transactions on
Circuits and Systems for Video Technology., 2021.
[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”
Neural Information Processing Systems, 2014.
7
[17] M. M. Liu, M. Q. Zhang, J. Liu, Y. N. Zhang, and Y. Ke, “Coverless
information hiding based on generative adversarial networks,” Journal
of Applied Sciences, 2017.
[18] S. Zhang, S. Su, L. Li, Q. Zhou, and C. C. Chang, “An image style
transfer network using multilevel noise encoding and its application in
coverless steganography,” Symmetry, vol. 11, no. 9, p. 1152, 2019.
[19] X. Chen, Z. Zhang, A. Qiu, Z. Xia, and N. Xiong, “A novel coverless
steganography method based on image selection and stargan,” IEEE
Transactions on Network Science and Engineering, vol. PP, no. 99,
pp. 1–1, 2020.
[20] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
International Journal of Computer Vision, 2004.
[21] Y. Choi, M. Choi, M. Kim, J. W. Ha, and J. Choo, “Stargan: Unified
generative adversarial networks for multi-domain image-to-image trans-
lation,” 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2018.
[22] W. Y. Xue R, “Message drives image: A coverless image steganography
framework using multi-domain image translation,” International Joint
Conference on Neural Networks (IJCNN). IEEE., 2021.
[23] L. M. Peng F, Chen G, “A robust coverless steganography based on
generative adversarial networks and gradient descent approximation,”
IEEE Transactions on Circuits and Systems for Video Technology., 2022.
[24] Z. Zhou, Y. Cao, and M. Wang, “Faster-rcnn based robust coverless
information hiding system in cloud environment,” IEEE Access, vol. PP,
no. 99, pp. 1–1, 2019.
"
"Formal Modelling is often part of the design and testing process for software, ensuring components operate within suitable bounds, even under unexpected circumstances. We integrate predictive formal modelling (PFM) at runtime in a human-swarm mission and reveal how this improves human-swarm team performance. We recruited 60 participants to operate a simulated aerial swarm delivering parcels to target locations. Operators were informed of estimated completion times (PFM condition) or not (No-PFM condition), controlling the mission by adding or removing drones, thereby increasing or decreasing the overall mission cost. Human-swarm performance was evaluated based on mission time, agent involvement, task accomplishment, and overall cost. PFM modelling during runtime enhances mission performance without significantly affecting the operator’s workload or system usability.","Aerial swarms boost our ability to observe and engage with challenging or remote areas. Search and rescue missions are one promising application, aiming to locate casualties on time or deliver essential supplies to remote disaster-struck areas. However, several challenges accompany these missions, ranging from design and deployment to safety, regulations, and operator mental workload. Additionally, optimizing performance, shared control, the degree of automation, and determining the appropriate time to present necessary information to the human operator are ongoing concerns. Prior research on human-swarm interaction (HSI) has identified prerequisites for the successful operation of aerial swarms. For systems relying on human intervention, a crucial requirement is the efficient timing and selection of relevant data. Gu et al. proposed a predictive formal modelling technique to estimate mission success at runtime. This capability allows the operator to make informed decisions promptly, increasing the overall efficiency and adaptability of human-swarm collaborations. With PFM providing critical insights, the interaction becomes more streamlined and responsive, ensuring decisions align with the mission’s success criteria. This enhanced decision support contributes to a more seamless and effective collaboration between humans and robots in achieving mission objectives. Our hypothesis is that incorporating predictive formal modelling (PFM) into the real-time execution of human-swarm tasks can empower swarm operators to make more informed decisions, optimizing swarm capabilities and resource utilization.","Abioye et al. examined the impact of adding an extra feature to the human-swarm interface, i.e., operator option to request high-quality images of a search area. They found that this led to higher trust perception but did not enhance the overall human-swarm performance. Schneiders et al. indicated the demand for studying non-dyadic human-in-the-loop system configurations, such as that presented in this work. Hunt et al. proposed a method of dynamic re-tasking and triage based on operator feedback. Wilson et al. identified key challenges for the deployment and use of robot swarms, which included how humans understand, monitor, control, and interact with swarms. Kouvaros and Lomuscio show a formal model for swarms and determine whether the emergent behaviour is satisfied; Boureanu et al. analyze unbounded swarm systems with respect to security requirements by verifying a parameterized model; Lomuscio and Pirovano outline a verification procedure to reason about the fault-tolerance in probabilistic swarm systems. However, none of these approaches can give guarantees after deployment. A close approach to ours is runtime monitoring, where pre-constructed monitors are used to analyze the system execution traces that are generated at runtime against formal specifications. These monitors can evolve with system dynamics, such as the size and topology, but cannot reason about mission-level specifications, like the human-swarm interactions, where finite observations are not sufficient. Instead, Gu et al. propose a framework to integrate runtime modelling, that has been deployed in system reasoning for unforeseen situations during execution, with formal methods and focus on formal runtime modelling.nannannan","We conducted a within-subject user study with 60 participants, counterbalanced into two groups. Participants operated a drone delivery mission in two scenarios: PFM and No-PFM. To constrain their strategies, we added a £ 2,000 cost and 6 minutes time limit. They controlled the swarm by adding or removing UAVs, affecting mission completion time and cost. PFM was used to predict the probability of completing all deliveries with the given number of UAVs, presented to the operator as an estimated completion time. HARIS simulator was used to simulate the mission and integrate PFM. Participants performed two operations: adding/removing UAVs and interacting with the PFM feature. We measured performance in terms of mission completion time, number of agents deployed, completed tasks, and mission cost.nan","Our results show that participants using PFM were able to complete more tasks in less time compared to the No-PFM scenario. This increase did not result in a higher demand on mental workload, showing the potential benefit of predictive formal modelling in a human-swarm interaction scenario.","We conducted a within-subject user study to determine the impact of deploying predictive formal modelling (PFM) at runtime on performance and mental workload. We recruited 60 participants to perform the role of a UAV swarm operator facilitating the delivery of parcels to target locations in a simulation environment. The role required the participant to add or remove agents as needed to complete the mission within the given time. We find that participants using PFM were able to complete more tasks in less time compared to the No-PFM scenario. This increase did not result in a higher demand on mental workload, showing the potential benefit of predictive formal modelling in a human-swarm interaction scenario.",The Effect of Predictive Formal Modelling at Runtime on Performance in Human-Swarm Interaction,"Ayodeji O. Abioye, William Hunt, Yue Gu, Eike Schneiders, Mohammad Naiseh, Joel E. Fischer, Sarvapali D. Ramchurn, Mohammad D. Soorati, Blair Archibald, Michele Sevegnani","The Effect of Predictive Formal Modelling at Runtime on
Performance in Human-Swarm Interaction
Ayodeji O. Abioye1, William Hunt1, Yue Gu2, Eike Schneiders3, Mohammad Naiseh4, Joel E. Fischer3,
Sarvapali D. Ramchurn1, Mohammad D. Soorati1, Blair Archibald2, and Michele Sevegnani2
ABSTRACT
Formal Modelling is often used as part of the design and testing
process of software development to ensure that components op-
erate within suitable bounds even in unexpected circumstances.
In this paper, we use predictive formal modelling (PFM) at run-
time in a human-swarm mission and show that this integration can
be used to improve the performance of human-swarm teams. We
recruited 60 participants to operate a simulated aerial swarm to
deliver parcels to target locations. In the PFM condition, operators
were informed of the estimated completion times given the number
of drones deployed, whereas in the No-PFM condition, operators
did not have this information. The operators could control the mis-
sion by adding or removing drones from the mission and thereby,
increasing or decreasing the overall mission cost. The evaluation
of human-swarm performance relied on four key metrics: the time
taken to complete tasks, the number of agents involved, the to-
tal number of tasks accomplished, and the overall cost associated
with the human-swarm task. Our results show that PFM modelling
at runtime improves mission performance without significantly
affecting the operator’s workload or the system’s usability.
CCS CONCEPTS
• Human-centered computing → Empirical studies in interac-
tion design; User interface design; User studies; Collaborative
interaction; Graphical user interfaces.
KEYWORDS
Human-Robot Interaction (HRI), Human-Swarm Interaction (HSI),
Predictive Formal Modelling (PFM), Task Performance
1
INTRODUCTION
Aerial swarms amplify our ability to observe and engage with areas
that are challenging for us to reach or oversee. One of the promising
applications of aerial swarms is in search and rescue missions to lo-
cate and identify casualties on time or to deliver essential life-saving
supplies to remote and difficult-to-reach areas in the aftermath of
a natural disaster [1]. Such applications are often accompanied by
several challenges, ranging from design and deployment [14] to
issues related to safety [17], regulations [19], and the operator’s
mental workload [2]. Other challenges include performance, shared
control and degree of automation, as well as determining the appro-
priate timing for presenting necessary information to the human
operator. Prior research on human-swarm interaction (HSI) has
identified essential prerequisites for the successful operation of
aerial swarms [9]. For systems relying on human supervision and
1Electronics and Computer Science, University of Southampton, UK.
2School of Computing Science, University of Glasgow, UK.
3School of Computer Science, University of Nottingham, UK.
4Computing and Informatics, Bournemouth University, UK.
intervention, a critical requirement for the smooth operation of the
swarm is the efficient timing and selection of relevant data provided
to the operator. Gu et al. [12] proposed a predictive formal mod-
elling (PFM) technique to estimate the mission success at runtime.
PFM can be used to inform the swarm operator about the crucial
information required to make appropriate decisions in order for the
mission to be successful. This capability allows the swarm operator
to make informed decisions promptly, increasing the overall effi-
ciency and adaptability of the human-swarm collaborative efforts.
With PFM providing crucial insights, the interaction becomes more
streamlined and responsive, ensuring that decisions are aligned
with the mission’s success criteria. This enhanced decision support
contributes to a more seamless and effective collaboration between
humans and robots in achieving mission objectives. Our hypothesis
posits that incorporating predictive formal modelling (PFM) into
the real-time execution of human-swarm tasks can empower swarm
operators to make more informed decisions, thereby optimising the
utilisation of swarm capabilities and resources.
In this paper, we integrate PFM into the ‘Human And Robot In-
teractive Swarm’ (HARIS) simulator [15] to provide human swarm
operators with real-time mission and swarm status updates, along
with predictions of mission success. Following a within-subject de-
sign, we recruited 60 participants to complete a human-swarm task
of delivering packages to different areas with two conditions (with
and without PFM). We assess the impact of PFM on the performance
and required mental workload during the completion of a task as
a human-swarm team. Results of the study showed that the PFM
condition was able to significantly improve mission performance
without having a significant effect on the operator’s workload.
2
RELATED WORK
In Abioye et al. [1], the authors evaluated the effect of adding an
extra feature to the human-swarm interface, i.e. operator option
to request high-quality images of a search area. They found that
this led to higher trust perception but did not enhance the overall
human-swarm performance. Schneiders et al. [24] indicated the
demand for studying non-dyadic human-in-the-loop system con-
figurations, such as that presented in this work. Hunt et al. [15]
proposed a method of dynamic re-tasking and triage based on op-
erator feedback. Wilson et al. [27] identified key challenges for the
deployment and use of robot swarms, which included how humans
understand, monitor, control, and interact with swarms.
Kouvaros and Lomuscio [16] show a formal model for swarms
and determine whether the emergent behaviour is satisfied; Boure-
anu et al. [6] analyse unbounded swarm systems with respect to se-
curity requirements by verifying a parameterised model; Lomuscio
and Pirovano [18] outline a verification procedure to reason about
the fault-tolerance in probabilistic swarm systems. However, none
of these approaches can give guarantees after deployment. A close
1
arXiv:2401.11945v1  [cs.RO]  22 Jan 2024
HRI ’24 (LBR Accepted Manuscript), March 11–14, 2024, Boulder, CO, USA
Abioye et al.
approach to ours is runtime monitoring [3], where pre-constructed
monitors are used to analyse the system execution traces that are
generated at runtime against formal specifications [11]. These mon-
itors can evolve with system dynamics, such as the size and topol-
ogy, but cannot reason about mission-level specifications, like the
human-swarm interactions, where finite observations are not suf-
ficient. Instead, Gu et al. [12] propose a framework to integrate
runtime modelling [5], that has been deployed in system reasoning
for unforeseen situations during execution [4], with formal methods
and focus on formal runtime modelling. Quantitative formal models
can provide predictions, and this has been used at design time, e.g.,
for predicting failures and service availability of components [8].
In this work, we adopt an existing model [12] implementing PFM
at runtime, to predict the feasibility of a human-swarm mission
succeeding and check whether presenting PFM output in the user
interface can support human operator in their decision-making
during the human-swarm task.
3
STUDY
We conducted a within-subject user study with 60 participants
divided into two counterbalanced groups as shown in Table 1, in
order to directly compare usability, workload, and performance
between the PFM and No-PFM conditions.
Table 1: Showing counterbalanced distribution of the 60 re-
cruited participants.
Group
Scenario 1
Scenario 2
#
1
No Prediction (No-PFM)
With Prediction (PFM)
30
2
With Prediction (PFM)
No Prediction (No-PFM)
30
3.1
Participants
We recruited 60 participants (38 female, 22 male, average age: 34.6,
age range 18 - 64) through Prolific [22]1. 70% of participants have at
least a bachelor’s degree, 50% were above-average computer users,
and 47% were familiar with UAV or swarm robotics. Participants
were recruited from the US and the UK. Participants were randomly
allocated into groups and received £ 9. The average study duration
was 33.6 minutes.
3.2
Study task
To investigate the impact of PFM, in a human-swarm interaction
context, we developed a drone delivery mission scenario which
was presented to participants through two scenarios: the PFM and
No-PFM scenarios. To constrain the operators’ strategies, we added
a cost (£ 2,000) and time (6 minutes) limit. These two constraints
meant that the participants were occupied trying to meet the time
limit while staying within the mission budget. Users could control
the swarm and the mission by performing two operations: adding
or removing UAVs from the mission. The more UAVs they add, the
faster they complete the mission, but they incur a higher mission
cost. The reverse is also applicable in that the more UAVs they
1We received ethics approval from the University of Southampton’s ethics committee
(confirmation number: ERGO/FEPS/85523).
Figure 1: HARIS simulator interface showing the predictive
formal modelling based estimated completion time feature
in the PFM scenario.
remove from the mission, the longer the mission completion time,
and the less the overall mission cost. We set the minimum and
maximum number of UAVs allowed in the mission to be 4 and 10
respectively. The final mission cost was a cumulative sum of the
upkeep cost per second. We implemented a non-linear per-second
upkeep cost function that makes the upkeep per-second cost higher
each time a new UAV is added. This reduces the participant’s ability
to predict the cost of adding or removing a UAV, especially for
the No-PFM condition without the predictive model. We added
40 delivery tasks to each scenario. PFM was used to predict the
probability of completing all deliveries with the given number of
UAVs. This prediction was presented to the operator as an estimated
completion time, as shown inside the circle in the upper right corner
(see Figure 1). The colour of the circle changes from green to yellow
and red depending on the estimated time of completion (green for
finishing well below the given time, yellow for finishing near the
given time, and red for exceeding it).
3.3
HARIS Model
The HARIS simulator is a browser-based platform that was specifi-
cally designed for human-in-the-loop multi-agent and swarm ro-
botics experimentation. HARIS is a successor of HutSim [23] which
was designed with a specific focus on usability by consulting with
industry experts to model not only their typical command structure
but also make it operable with real-life or simulated agents. Building
on its predecessor, HARIS was further tailored to its use case de-
rived from interviews with drone pilots [21] and swarm experts [20]
to make the platform as usable and realistic as possible while max-
imising the ease of use for multiple human operators [25], making
this simulator a useful tool for the investigations on human-swarm
simulations.
We use the model from [12] with slight modifications to bet-
ter reflect the scenario. For example, the background failure in
each region is removed to relieve participants’ stress from the unex-
pected loss of UAVs; Erlang-k law [10] is implemented to represent a
smooth transition delay in CTMCs. The integration with the HARIS
simulator follows a similar process to [12], but with the Sim2PRISM
2
The Effect of Predictive Formal Modelling at Runtime on Performance in Human-Swarm Interaction HRI ’24 (LBR Accepted Manuscript), March 11–14, 2024, Boulder, CO, USA
middleware directly embedded in HARIS. Instead of showing the
probability of success directly, which might be difficult for partici-
pants to interpret, we consider the feasibility over different time
intervals and give an estimated completion time as the time when
the probability of success reaches 0.99. Additionally, we implement
What-if scenarios to give the participants extra information on the
effect of adding/removing a UAV before making a decision.
3.4
Procedure
Following recruitment, participants were presented with the par-
ticipant information sheet and consent form, after which they com-
pleted a brief demographics survey which collected data on gender,
age group, education level, self-rated computer expertise, as well as
self-rated UAV or Swarm robotics knowledge. Subsequently, partici-
pants were asked to watch a short study briefing video and asked to
answer three questions to test their preliminary understanding of
the task. To ensure that participants understood the study task, two
of these three had to be answered correctly in order to proceed with
the study. Participants were required to perform a short tutorial
scenario which allowed them to experiment with all the provided
functionality and experience the interface prior to the actual data
collection. Participants then proceeded to their first scenario. Fol-
lowing its completion, they completed the post-task survey which
included the 6-item NASA-TLX [13] questionnaire and the 10-item
System Usability Scale (SUS) [7]. They continued with the second
scenario, followed by the same set of questionnaires. Finally, par-
ticipants were asked to complete a short survey in relation to their
preferred scenario condition, before returning to Prolific. These
questions were related to (a) the perceived accuracy of the time esti-
mation feature provided (for the PFM condition), (b) their preferred
scenario, (c) a selection of reasons for perceived success during
task completion, (d) the primary reason for their success, and (e) a
binary selection if they used the estimated completion time. Each
participant’s performance was measured and recorded in real-time
during the scenario tasks as HARIS generated log files.
The survey questionnaires and HARIS simulator were dockerised
and deployed online2 on an AWS EC2 c5.4xlarge (32GB RAM, 16
vCPUs) instance running the Ubuntu 22.04 operating system. The
dockerisation was necessary for a scalable deployment due to the
high computing resource requirement of the prediction model in
the simulator.
4
RESULTS AND ANALYSIS
The result of the participants’ performance over time is presented
in Figure 2. Figure 2a shows the mean mission cost of each scenario
over time. The No-PFM scenario incurred a lower cost over time
than the PFM scenario. Figure 2b shows the mean number of agents
used over time. The No-PFM group started with the least number of
agents but finished with the most. Since this group did not have the
estimated completion time displayed, it is possible that they realised
very late that they may not finish, and therefore started adding more
agents towards the end. This might indicate that participants in the
No-PFM condition found it more difficult to balance the number
of agents with the two constraints defined. Figure 2c compares
the mean number of completed tasks over time and shows that
2Online HARIS simulator: https://uos-haris.online/
participants completed more tasks on average over time in their
PFM scenario compared to their No-PFM scenario.
To determine the impact of our formal model on the performance
of the human-swarm teaming, we analysed the results to under-
stand whether the prediction feature contributes to participants
completing tasks more efficiently, with minimal influence on the
overall mission cost. We evaluated four dependent variables: a)
Time Completion: refers to the mission completion time i.e. the
time taken to complete 40 delivery tasks. b) No. of Agents: refers
to the mean number of agents deployed by each participant to
complete the delivery task. c) Completed Tasks: We considered a
delivery task to be successfully completed when the UAV reaches
the target coordinate. After this, the UAV returns to the hub to
collect parcels for the next delivery. d) Cost per Task: This was
computed as a ratio of the mean total cost incurred over the mean
number of tasks completed per study scenario.
Table 2: Descriptive statistics and one-way ANOVA results.
Significance levels are indicated as: * 𝑝 < 0.05 , ** 𝑝 < 0.01
Variable
Scenario
Mean
Std.
F value
p value
Time Completion
No-PFM
329s
36.93
5.363
0.022*
PFM
314s
34.77
No. of Agents
No-PFM
5.79
1.06
3.074
0.082
PFM
6.10
0.86
Completed Tasks
No-PFM
38.80
1.71
7.255
0.008**
PFM
39.55
1.30
Cost per Task
No-PFM
£52.85
7.31
0.001
0.988
PFM
£52.83
3.80
In terms of data analysis, our dataset meets the prerequisites
necessary for conducting one-way ANOVA testing. Additionally, we
performed a G*Power analysis to verify that our sample size aligns
with the required criteria. Specifically, we have 60 participants
across experimental groups, with an assumed effect size of 0.2 and
a significance level of 0.05. The G*Power analysis helps to confirm
that our study is adequately powered to detect the expected effects.
For workload, participants had a mean of 4.77 (SD =1.50) in the
PFM and a mean of 4.74 (SD = 1.56) in the No-PFM scenarios. One-
way ANOVA for workload revealed no significant main effect (F(1,
118) = 0.009, p = 0.924. This suggests that the PFM feature did not
add extra workload to the participants. Regarding usability, we used
the system usability scale (SUS) to compare the mean values for the
two conditions. In line with the guidelines [7, 26], interfaces with
a value of 68 or above are considered good. Mean SUS scores for
PFM and No-PFM scenarios were 70.75 (SD = 17.52) and 74.38 (SD
= 15.15). This shows that the usability of both systems was good.
One-way ANOVA yielded no significant effect on usability (F(1,
118) = 1.470, p = 0.228). This suggests that the PFM feature did not
make the system more or less usable than without it.
As depicted in Table 2, the PFM condition led to enhanced task
completion rates and reduced time requirements when compared to
the No-PFM scenario where no prediction was presented to partici-
pants. Specifically, participants, on average, completed 39.55 tasks
(SD No. of Tasks = 1.30) within 314 seconds (SD Time Completion
= 34.77) in the PFM condition. This performance contrasted with
the No-PFM condition, where participants completed an average of
3
HRI ’24 (LBR Accepted Manuscript), March 11–14, 2024, Boulder, CO, USA
Abioye et al.
(a) Mission Cost
(b) Number of Agents
(c) Completed Tasks
Figure 2: Comparing the mean performance of all study conditions over time.
38.80 tasks (SD No. of Tasks = 1.71) in 329 seconds (SD Time Com-
pletion = 36.93). An ANOVA test was conducted and showed that
this difference was significant both in terms of Time Completion
[F(1,59)= 5.363, p= 0.022*] and No. of Completed Tasks [F(1,59)=
7.255, p= 0.008*]. Moreover, our findings indicate that employing
PFM prediction did not influence the utilisation of additional agents
or the associated task cost in the context of human-swarm collabo-
ration when compared to scenarios without prediction (No-PFM). In
the PFM condition, participants, on average, employed 6.10 agents
(SD No. of Agents = 0.86) at an average cost of £52.83 (SD Cost =
3.80) per task. Conversely, in the No-PFM condition, participants
used an average of 5.79 agents (SD No. of Agents = 1.06) at a cost of
£52.85 (SD Cost = 7.31) per task. An ANOVA test was conducted and
showed that this difference between PFM and No-PFM was not sig-
nificantly different for No. of Agents [F(1,59) = 3.074, p=0.082] and
Cost per task [F(1,59)= 0.988]. Figure 2a shows the mean mission
cost of each scenario over time.
The significant results in relation to mission completion times
were also reflected in the anecdotal open-ended statements made by
participants following both conditions. Participants indicated the
perceived usefulness of the features provided in the PFM condition
as, e.g., expressed by P46:
“I found the presence of the estimated completion time feature
[PFM] helped me decide whether to add or remove agents,
whereas in the first scenario [No-PFM] I was trying to estimate
it myself based on the remaining time and the percentage
completion of the task.” - P46
indicating the usefulness of the additional information provided to
complete the task successfully. A similar sentiment was presented
by P20 who describes the use of the PFM feature as a guiding mark
for optimising the addition and removal of drones.
“I used the estimated time to allow me to hover around the
6-minute mark, adding and taking away planes where neces-
sary” - P20
5
DISCUSSION AND FUTURE WORKS
We found that there was no significant change in workload between
the two conditions and both the PFM and No-PFM interfaces were
found to be usable based on the systems usability survey. Although
our result show that there is a performance gain when using the
predictive formal modelling feature at runtime, our analysis does
not take into account how the accuracy of the prediction could af-
fect the users’ performance or trust in the system. Furthermore, this
study did not investigate how the PFM feature increases explainabil-
ity and hence trust in the human swarm interaction. A follow-up
study could collect data on trust, acceptability, and user preferences
in order to evaluate these measures. In future work, we may also
consider embedding recommendations for the operators to help in
controlling the swarm, i.e., when to add or remove drones. In order
to expand on the assessment of mental workload, different data
streams about the users’ interaction, such as neurophysiological
responses (e.g., error potentials), might be useful. This could indi-
cate the operator’s cognitive workload and situational awareness
as they operate the swarm in a disaster response scenario.
6
CONCLUSION
Building on previous work in human-swarm interaction on deploy-
ing predictive formal modelling (PFM) at runtime, we conducted a
within-subject user study to determine its impact on performance
and mental workload. We recruited 60 participants to perform the
role of a UAV swarm operator facilitating the delivery of parcels
to target locations in a simulation environment. The role required
the participant to add or remove agents as needed to complete the
mission within the given time. We find that participants using PFM
were able to complete more tasks in less time compared to the
No-PFM scenario. This increase did not result in a higher demand
on mental workload, showing the potential benefit of predictive
formal modelling in a human-swarm interaction scenario.
ACKNOWLEDGMENTS
The authors wish to acknowledge the support received from the
UKRI Trustworthy Autonomous Systems Hub (EP/V00784X/1), the
EPSRC project on Smart Solutions Towards Cellular-Connected
Unmanned Aerial Vehicles System (EP/W004364/1), and an Amazon
Research Award on Automated Reasoning.
REFERENCES
[1] Ayodeji O. Abioye, Mohammad Naiseh, William Hunt, Jediah Clark, Sarvapali D.
Ramchurn, and Mohammad D. Soorati. 2023. The Effect of Data Visualisation
Quality and Task Density on Human-Swarm Interaction. In 2023 32nd IEEE
International Conference on Robot and Human Interactive Communication (RO-
MAN). 1494–1501. https://doi.org/10.1109/RO-MAN57019.2023.10309454
4
The Effect of Predictive Formal Modelling at Runtime on Performance in Human-Swarm Interaction HRI ’24 (LBR Accepted Manuscript), March 11–14, 2024, Boulder, CO, USA
[2] Ayodeji Opeyemi Abioye, Stephen D. Prior, Peter Saddington, and Sarvapali D.
Ramchurn. 2022. The performance and cognitive workload analysis of a multi-
modal speech and visual gesture (mSVG) UAV control interface. Robotics and Au-
tonomous Systems 147 (2022), 103915. https://doi.org/10.1016/j.robot.2021.103915
[3] Ezio Bartocci, Yliès Falcone, Adrian Francalanza, and Giles Reger. 2018. Introduc-
tion to runtime verification. Lectures on Runtime Verification: Introductory and
Advanced Topics (2018), 1–33.
[4] Nelly Bencomo, Svein Hallsteinsen, and Eduardo Santana De Almeida. 2012. A
view of the dynamic software product line landscape. Computer 45, 10 (2012),
36–41.
[5] Gordon Blair, Nelly Bencomo, and Robert B France. 2009. Models@ run. time.
Computer 42, 10 (2009), 22–27.
[6] Ioana Boureanu, Panagiotis Kouvaros, and Alessio Lomuscio. 2016. Verifying
security properties in unbounded multiagent systems. In Proceedings of the 2016
international conference on autonomous agents & multiagent systems. 1209–1217.
[7] John Brooke. 1995. SUS: A quick and dirty usability scale. Usability Eval. Ind. 189
(11 1995).
[8] Muffy Calder and Michele Sevegnani. 2019. Stochastic Model Checking for
Predicting Component Failures and Service Availability. IEEE Transactions on
Dependable and Secure Computing 16, 1 (2019), 174–187. https://doi.org/10.1109/
TDSC.2017.2650901
[9] Jediah R Clark, Mohammad Naiseh, Joel Fischer, Marisé Galvez Trigo, Katie Par-
nell, Mario Brito, Adrian Bodenmann, Sarvapali D Ramchurn, and Mohammad D
Soorati. 2022. Industry Led Use-Case Development for Human-Swarm Oper-
ations. In AAAI 2022 Spring Symposium Series (Putting AI in the Critical Loop:
Assured Trust and Autonomy in Human-Machine Teams). AAAI, 1–6.
[10] Agner Krarup Erlang. 1917. Solution of some problems in the theory of proba-
bilities of significance in automatic telephone exchanges. Post Office Electrical
Engineer’s Journal 10 (1917), 189–197.
[11] Yliès Falcone, Srđan Krstić, Giles Reger, and Dmitriy Traytel. 2021. A taxonomy
for classifying runtime verification tools. International Journal on Software Tools
for Technology Transfer 23, 2 (2021), 255–284.
[12] Yue Gu, William Hunt, Blair Archibald, Mengwei Xu, Michele Sevegnani, and
Mohammad D. Soorati. 2023. Successful Swarms: Operator Situational Awareness
with Modelling and Verification at Runtime. In 2023 32nd IEEE International
Conference on Robot and Human Interactive Communication (RO-MAN). 541–548.
https://doi.org/10.1109/RO-MAN57019.2023.10309626
[13] Sandra G Hart. 1986. NASA task load index (TLX). (1986).
[14] Maria-Theresa Oanh Hoang, Niels van Berkel, Mikael B. Skov, and Timothy R.
Merritt. 2023. Challenges and Requirements in Multi-Drone Interfaces. In Ex-
tended Abstracts of the 2023 CHI Conference on Human Factors in Computing Sys-
tems (Hamburg, Germany) (CHI EA ’23). Association for Computing Machinery,
New York, NY, USA, Article 65, 9 pages. https://doi.org/10.1145/3544549.3585673
[15] William Hunt, Jack Ryan, Ayodeji O. Abioye, Sarvapali D. Ramchurn, and Moham-
mad D Soorati. 2023. Demonstrating Performance Benefits of Human-Swarm
Teaming. In Proceedings of the 2023 International Conference on Autonomous
Agents and Multiagent Systems (London, United Kingdom). IFAAMAS, Richland,
SC, 3062–3064.
[16] Panagiotis Kouvaros and Alessio Lomuscio. 2015. Verifying emergent prop-
erties of swarms. In Twenty-Fourth International Joint Conference on Artificial
Intelligence.
[17] Dasom Lee, David J. Hess, and Michiel A. Heldeweg. 2022. Safety and privacy
regulations for unmanned aerial vehicles: A multiple comparative analysis. Tech-
nology in Society 71 (2022), 102079. https://doi.org/10.1016/j.techsoc.2022.102079
[18] Alessio Lomuscio and Edoardo Pirovano. 2021. Verifying fault-tolerance in
probabilistic swarm systems. In Proceedings of the Twenty-Ninth International
Conference on International Joint Conferences on Artificial Intelligence. 325–331.
[19] Mingyang Lyu, Yibo Zhao, Chao Huang, and Hailong Huang. 2023. Unmanned
Aerial Vehicles for Search and Rescue: A Survey. Remote Sensing 15, 13 (2023).
https://doi.org/10.3390/rs15133266
[20] Mohammad Naiseh, Mohammad D Soorati, and Sarvapali Ramchurn. 2023. Out-
lining the design space of eXplainable swarm (xSwarm): experts perspective.
arXiv preprint arXiv:2309.01269 (2023).
[21] Katie J Parnell, Joel E Fischer, Jediah R Clark, Adrian Bodenmann, Maria Jose
Galvez Trigo, Mario P Brito, Mohammad D Soorati, Katherine L Plant, and Sar-
vapali D Ramchurn. 2022. Trustworthy UAV relationships: Applying the Schema
Action World taxonomy to UAVs and UAV swarm operations. International
Journal of Human–Computer Interaction (2022), 1–17.
[22] Prolific. 2023. Prolific - quickly find research participants you can trust. https:
//www.prolific.co/
[23] Sarvapali D Ramchurn, Joel E Fischer, Yuki Ikuno, Feng Wu, Jack Flann, and
Antony Waldock. 2015. A study of human-agent collaboration for multi-UAV
task allocation in dynamic environments. In Proceedings of the 24th International
Joint Conference on Artificial Intelligence (IJCAI). 1184–1192.
[24] Eike Schneiders, EunJeong Cheon, Jesper Kjeldskov, Matthias Rehm, and Mikael B.
Skov. 2022. Non-Dyadic Interaction: A Literature Review of 15 Years of Human-
Robot Interaction Conference Publications. J. Hum.-Robot Interact. 11, 2, Article
13 (feb 2022), 32 pages. https://doi.org/10.1145/3488242
[25] Mohammad D Soorati, Mohammad Naiseh, William Hunt, Katie Parnell, Jediah
Clark, and Sarvapali D Ramchurn. 2023. Enabling Trustworthiness in Human-
swarm Systems Through a Digital Twin. Putting AI in the Critical Loop (2023).
[26] Usability.gov Improving the User Experience. 2023.
System Usability Scale
(SUS). https://www.usability.gov/how-to-and-tools/methods/system-usability-
scale.html
[27] James Wilson, Greg Chance, Peter Winter, Suet Lee, Emma Milner, Dhaminda
Abeywickrama, Shane Windsor, John Downer, Kerstin Eder, Jonathan Ives, and
Sabine Hauert. 2023. Trustworthy Swarms. In Proceedings of the First International
Symposium on Trustworthy Autonomous Systems (Edinburgh, United Kingdom)
(TAS ’23). Association for Computing Machinery, New York, NY, USA, Article 10,
11 pages. https://doi.org/10.1145/3597512.3599705
5
"
"This study aims to establish a systematic approach for evaluating LEO mega-constellation networks' performance using KPIs. An efficient evaluation technique and related KPIs, such as N-asset coverage and user plane latency, are introduced. Numerical simulations demonstrate the framework's utility.","The paper begins by highlighting the significance of non-terrestrial networks (NTNs) and the integration of satellite constellations into 5G and beyond communication systems. It discusses the requirements for key performance indicators (KPIs) to characterize specific features of satellite networks, considering aspects such as access, mobility, and constellation-wide performance. It also acknowledges existing efforts in KPI definition for LEO constellations and the need for a comprehensive framework.","nanSeveral relevant research efforts are reviewed. The authors mention studies on handover-related KPIs, constellation design optimization based on service availability, a new performance metric called service coverage, and stochastic geometry analysis of LEO constellations. However, these works focus on partial aspects of the KPI system, and a systematic framework is still lacking.nannan","To address the research gap, the authors propose a methodology for evaluating a KPI system designed for LEO mega-constellations. They emphasize the challenges in defining and assessing KPIs in the context of satellite networks due to distinct characteristics such as irregular beam layout and complex interference patterns. The proposed methodology aims to overcome these challenges and enable a comprehensive assessment of LEO constellation performance. It involves defining a set of KPIs, including both constellation-specific and RIT KPIs, utilizing the concept of interfering area and hexagonal spherical cells (SCs) for multi-satellite interference modeling, and developing an efficient system-level simulator. The goal is to achieve a proper assessment of LMCSN with a balance between simulation complexity and modeling fidelity.nan","The paper presents extensive simulations and numerical results to evaluate the proposed KPI framework under various scenarios. Using practical LMCSN simulation settings following 3GPP NTN assumptions, the authors analyze user/control plane related KPIs such as area traffic capacity, access success probability, and handover failure rate. The results provide insights into the performance limitations and system design considerations for LEO mega-constellations. For instance, the study reveals that the achieved area traffic capacity of the reference LMCSN is limited to around 4 Kbps/km2, highlighting the need for further optimization and enhancements.","The study concludes by emphasizing the significance of the proposed systematic KPI framework as a foundation for further investigation of LEO mega-constellation networks. It acknowledges the importance of evaluating KPI results for various LEO configurations, traffic data types, and time snapshot durations. The authors also suggest exploring potential LMCSN-specific technologies to improve network performance metrics like area traffic capacity and handover failure rate, leaving room for future research and advancements in the field.",Systematic Performance Evaluation Framework for LEO Mega-Constellation Satellite Networks,"Yu Wang, Chuili Kong, Xian Meng, Hejia Luo, Ke-Xin Li, Jun Wang","arXiv:2401.11934v1  [cs.PF]  22 Jan 2024
Systematic Performance Evaluation Framework for
LEO Mega-Constellation Satellite Networks
Yu Wang, Chuili Kong, Xian Meng, Hejia Luo, Ke-Xin Li, and Jun Wang
Huawei Technologies Co., Ltd., Hangzhou, China.
Email: wangyu207@huawei.com
Abstract—Low Earth orbit (LEO) mega-constellation satellite
networks have shown great potential to extend the coverage capa-
bility of conventional terrestrial networks. How to systematically
deﬁne, quantify, and assess the technical performance of LEO
mega-constellation satellite networks remains an open issue. In
this paper, we propose a comprehensive key performance indica-
tor (KPI) framework for mega-constellation based LEO satellite
networks. An efﬁcient LEO constellation oriented performance
evaluation methodology is then carefully designed by resorting to
the concept of interfering area and spherical geographic cell. We
have carried out rigorous system-level simulations and provided
numerical results to assess the KPI framework. It can be observed
that the achieved area trafﬁc capacity of the reference LEO
constellation is around 4 Kbps/km2, with service availability
ranging from 0.36 to 0.39. Besides, the average access success
probability and handover failure rate is approximate to 96% and
10%, respectively, in the nearest satellite association scheme.
Index Terms—LEO satellite constellation, KPI, performance
evaluation, quasi-earth-ﬁxed, beam hopping, 3GPP NTN.
I. INTRODUCTION
Next generation communication era is expected to assure
three-dimensional global wireless connectivity and bridge the
digital divide through seamlessly integrating non-terrestrial
networks (NTNs) [1]–[3]. To accomplish the ambitious vision,
innovative standardization endeavors have been sponsored by
the Third Generation Partnership Project (3GPP) to study a set
of necessary adaptations enabling the operation of 5G New
Radio (NR) protocol in the NTN context [4], [5]. Speciﬁ-
cally, LEO mega-constellation satellite networks (LMCSNs)
represent a burgeoning frontier scenario for NTN, and thus
have recently attracted substantial academical and industrial
interests. Composed by a large number of inter-connected LEO
satellites (typically several thousands or even more), LMCSNs
can provide increased network coverage, improved broadband
capacity, and reduced end-to-end delay. Numerous commercial
solutions have been envisaged to provide broadband Internet
access by deploying LEO satellite mega-constellations, e.g.,
Oneweb, Kuiper, Starlink and AST SpaceMobile.
Before delving into LMCSN, we should ﬁrst address the
fundamental issue of what critical capability the network can
provide. As such, it is requisite to deﬁne and characterize the
corresponding key performance indicators (KPIs) therein. On
one hand, a well known KPI framework has been put forward
in authoritative standard organizations such as International
Telecommunication Union (ITU) [6] and 3GPP [7]. However,
the baseline KPI framework can not fully reﬂect the distinct
characteristic for LMCSNs in terms of access, mobility, and
Figure 1. Beam layout in existing single-satellite simulation.
constellation-speciﬁc networking performance. On the other
hand, there are some pioneering efforts devoted to inves-
tigating LMSCN-speciﬁc KPIs. More speciﬁcally, satellite
handover (HO) related KPIs (e.g., time-of-stay, and radio link
failures) [8] are evaluated based on system-level simulation for
earth-moving LEO satellite networks. In [9], the LEO constel-
lation design problem is studied to optimize constellation KPIs
such as service availability and scalability. A new performance
metric, i.e., service coverage, deﬁned as the ratio of trafﬁc
density to service requirement per unit area is introduced in
[10] to describe the coverage capability for LMSCNs. Besides,
by resorting to stochastic geometry, the downlink coverage
probability and average data rate are theoretically analyzed
for LEO constellations. To sum up, state-of-the-art studies
only concentrate on partial aspects of the KPI system, and
a systematic KPI framework for LEO mega-constellations is
still missing in the literature.
Accordingly, to evaluate the aforementioned KPI system
for the emerging LMCSN scenario, an efﬁcient performance
assessment methodology is necessary. Considering the distinct
satellite network feature such as frequency/polarization reuse
and beam layout conﬁgurations, an extension of conventional
wrap-around mechanism with additional surrounding beams is
applied in case of single-satellite interference modeling [4],
[6]. However, the above approach is only effective in the
speciﬁc case of central beams at satellite nadir (i.e., around
90◦ elevation angle), and cannot be directly extended to the
multi-satellite simulation case. This is because the beam layout
is deﬁned by hexagonal mapping of the beam bore sight
directions on UV plane, and the generated beams in the
geodetic plane exhibit severe distortion especially at the edge
(corner beams as depicted in Fig. 1) within satellite cover-
age area. To solve the problem, [4] presents two alternative
evaluation solutions for multi-satellite scenario. Unfortunately,
both the realistic beam layout conﬁguration details and multi-
satellite interference modeling technique under highly dynamic
network environment are not explicitly investigated. Therefore,
more endeavors should be made for thorough constellation-
wise performance assessment in LMCSNs.
To circumvent the above issues, a systematic KPI frame-
work including the target KPI set deﬁnition and relevant
evaluation methodology should be devised for LEO satellite
networks. However, it is technically challenging to develop a
comprehensive KPI framework for LMCSNs, due to several
reasons: 1) In addition to radio interface technology (RIT)
related KPIs, the impact on access and mobility related perfor-
mance should be characterized in the speciﬁc satellite scenario.
More importantly, constellation KPIs need to be exploited to
evaluate the performance of a whole satellite constellation;
2) A LEO mega-constellation exhibits the following distinct
characteristics such as no near-far effect and irregular beam
layout, and thus the interference problem in satellite environ-
ment becomes critical. To make the matter worse, the fast
mobility of LEO satellites induces the complicated and time-
varying multi-satellite interference problem; 3) It is prohibitive
to simulate a complete mega-constellation taking into account
the large time-space span of the network. To strike a good
balance between simulation complexity and modeling ﬁdelity,
an efﬁcient performance evaluation scheme is necessary [11].
To ﬁll in this gap, in this paper, we try to answer these
challenging questions for LMCSNs. To the authors’ best
knowledge, this is the ﬁrst work in the literature to deﬁne and
assess the KPI system especially for LEO mega-constellation
networks. The main contributions are summarized as follows.
• By referring to the ITU/3GPP KPI framework, we further
propose a comprehensive KPI system to characterize
the speciﬁc satellite network performance. The proposed
KPI system can fully capture the constellation capability,
access capacity, and mobility performance for LMCSNs.
• A highly-efﬁcient constellation-wise performance eval-
uation methodology is devised through exploitation the
concept of interfering area and hexagonal spherical cells
(SCs), such that multi-satellite interference can be ap-
proximately modeled in the dynamic and complex LM-
CSN environment. The evaluation scheme can achieve a
proper assessment of LMCSN with reasonable simulation
complexity while not sacriﬁcing too much modeling
ﬁdelity. This evaluation methodology will establish the
base for further deep investigation of LMCSNs.
• We have developed a versatile system-level simulator that
is rigorously calibrated upon many ITU/3GPP baseline
conﬁgurations. Extensive experiments are implemented
and numerous simulation results of target KPIs are
present. Under practical LMCSN simulation conﬁgu-
rations, some system design insights and quantitative
performance limitations can be found. For instance, the
achieved area trafﬁc capacity of the reference LMCSN
SAT-1
SAT-2
SC#1
SC#2
SC#3
SC#4
SC#5
SC#6
SC#7
SC#8
SC#9
SC#10
SC#11
SC#12
Orbit-1
SC#13
SC#14
SC#15
SC#16
SC#17
SC#18
SC#19
SC#20
SC#21
SC#22
SC#23
SC#24
V
SAT-4
Orbit-2
Orbit-3
SAT-3
SAT-5
Figure 2. An example quasi-earth-ﬁxed LEO NTN system.
with 1800 satellites is only around 4 Kbps/km2, with
service availability ranging from 0.36 to 0.39.
The remainder of this paper is structured as follows. Section
II introduces the system model and presents the KPI frame-
work in detail. The LMCSN-speciﬁc evaluation methodology
is elaborated in Section III. Section IV provides the simulation
results, followed by conclusions in Section V.
II. KPI FRAMEWORK DEFINITION
In this section, we describe the system model under con-
sideration and the proposed KPI framework for LMCSNs.
A. System Model
We consider a LMCSN operating at a center frequency f0
with total system bandwidth W. The LMCSN comprises a set
of L = {1, . . . , L} LEO satellites. Each satellite l is equipped
with M = MxMy phased array antennas to ﬂexibly generate a
maximum of Bl beams, where Mx and My denote the number
of antennas along the x- and y-axes, respectively. Without loss
of generality, the antenna elements are equally spaced by 0.5
wavelength along both axes.
As adopted in commercial LEO constellations, e.g., Starlink
and AST, the satellite beams work in the quasi-earth-ﬁxed
mode. That is, beams continuously cover one geographic area
for a limited period, and turn to cover a different geographic
area during another period. The target coverage area is divided
into C = {1, . . . , C} SCs, wherein a SC corresponds to
a ﬁxed hexagonal geographical area on the Earth’s surface.
In addition, SC-centric beam hopping method [12], [13] is
applied to serve the wide coverage area. To be speciﬁc, once
being activated, a beam points to the center position of a target
SC. An example LMCSN system with 5 satellites and 24 SCs
is shown in Fig. 2, wherein only 2 SCs (i.e., SC#6 and SC#14)
are illuminated by satellite beams at the time period.
A set of U = {1, . . . , U} UEs are distributed in the target
area. Time horizon is discretized into T = {1, . . . , T } equal
time slots. We deﬁne a binary scheduling variable αc
b,l(t) = 1
if beam b of satellite l is illuminated to SC c at time slot t,
and αc
b,l(t) = 0 otherwise. The received power P u
b,l(t) for UE
u served by beam b of satellite l at time slot t is derived as
P u
b,l(t) = PT + GT (θu
b,l(t), φu
b,l(t)) + GR − PL(du
b,l(t)), (1)
where PT is the beam transmit power, GR corresponds to
the UE receive gain, and GT (θu
b,l(t), φu
b,l(t)) represents the
transmit antenna gain [13], with θu
b,l(t) and φu
b,l(t) denoting
the elevation angle and azimuth angle, respectively. Besides,
PL(du
b,l(t)) denotes the total path loss (in dB), which is further
calculated as speciﬁed by 3GPP NTN speciﬁcations [5]
PL(du
b,l(t)) = 32.45+20 log10(f0·du
b,l(t))+Fs+Lg+Ls, (2)
wherein du
b,l(t) is the slant path distance, Fs represents the log-
normal distributed shadow fading, and Lg and Ls represent the
atmospheric absorption and scintillation loss, respectively.
Consequently, the overall signal-to-interference plus noise
ratio (SINR) γu,c
b,l (t) of UE u located in SC c from serving
beam b of satellite l at time slot t is denoted by
γu,c
b,l (t) =
P u
b,l(t)αc
b,l(t)
Iintra + Iinter + N0
,
(3)
where Iintra = P
b′ ̸=b,l′=l,c′ P u
b′ ,l′ (t)αc′
b′ ,l′ (t) is the intra-
satellite interference, Iinter = P
b′ ,l′̸=l P u
b′ ,l′ ,c′(t)αc′
b′ ,l′ (t) is
the inter-satellite interference, and N0 is the noise power
determined by UE noise ﬁgure and antenna temperature.
B. Proposed KPI System for LMCSN
In order to identify the new services, capabilities, and min-
imum technical performance requirements, a set of represen-
tative and multifaceted KPI parameters should be accordingly
exploited. In particular, the KPI system deﬁned in ITU/3GPP
[6], [7] comprises the following principal aspects such as
peak data rate, user experienced data rate, latency, mobility,
connection density, energy efﬁciency, spectrum efﬁciency, and
area trafﬁc capacity. As can be seen, the KPI parameters are
envisaged only from RIT’s perspective.
Motivated by this, the following LMCSN-speciﬁc KPIs
are carefully selected, which can be further categorized into
constellation KPIs and RIT KPIs as summarized in Table I.
To be speciﬁc, constellation KPIs are employed to reﬂect the
performance of a satellite constellation.
• N-asset coverage: the number of simultaneously service-
able satellites per geographic area wherein the received
satellite signal quality is above a predeﬁned threshold,
e.g., the perceived SNR at a target SC from each of
the N satellites is better than a predetermined threshold.
The KPI is introduced to characterize the multi-fold
constellation coverage performance for a LMCSN.
• Area trafﬁc capacity: the total trafﬁc throughput served
per geographic area. This is a measure of how much
trafﬁc a network can carry per unit area.
• Service availability: the probability that a (service) beam
can be immediately scheduled to serve a target geographic
area. It is worth noticing that the KPI is carefully selected
to capture the LMCSN-speciﬁc beam hopping service
ability, i.e., how soon a speciﬁc SC can be served again
by at least a satellite beam.
Meanwhile, similar to the deﬁnition in conventional cellular
networks, RIT KPIs are also adopted to reﬂect the perfor-
Table I
SELECTED KPI SET FOR LEO SATELLITE NETWORKS.
Category
Characteristic
Evaluation
N-asset coverage
Simulation
Constellation KPI
Area trafﬁc capacity
Simulation
Service availability
Simulation
Peak data rate
Analytical
User experienced data rate
Simulation
Unmet capacity
Simulation
Energy efﬁciency
Inspection
RIT KPI
User plane latency
Analytical
Control plane latency
Analytical
Access success probability
Simulation
Access capacity
Simulation
Mobility interruption time
Simulation
Handover failure rate
Simulation
mance, e.g., access capacity and mobility performance, of
possibly multiple cells within each satellite.
• Peak data rate: the highest theoretical data rate assuming
error-free conditions and all assignable radio resources.
We use this conventional KPI to quantify the satellite
transmission capacity.
• User experienced data rate: achievable data rate that
is available ubiquitously across the coverage area to a
mobile UE. We utilize this conventional KPI to reﬂect
the UE perceived data rate experience in practice.
• Unmet capacity: the gap between required UE data rate
and offered data rate. The KPI is chosen herein to
demonstrate the effect of unbalanced spatial-temporal
trafﬁc distribution on network service satisfaction degree.
• Energy efﬁciency: the quantity of information bits per unit
of energy consumption of the radio access network/device
in bit/Joule. This is a sustainability KPI.
• User plane latency: the time it takes to successfully
deliver an application layer packet/message from the
ingress point to the egress point. The KPI quantiﬁes
how fast a data transmission can be achieved. Notably,
propagation delay should be accounted in computing the
user plane latency for LMCSNs.
• Control plane latency: This refers to the time to move
from a battery efﬁcient state to start of continuous data
transfer. Similarly, propagation delay is a non-negligible
factor when analyzing the control plane latency.
• Access success probability: the probability to successfully
complete the random access procedure within the max-
imum number of preamble transmissions. We capitalize
this KPI to measure the RACH relevant capability.
• Access capacity: the number of successfully accessed
UE number per geographical area. In association with
access success probability, access capacity is proposed to
quantify the critical LMCSN access capability.
• Mobility interruption time: the shortest time duration
supported by the system during which a UE cannot
exchange user plane packets with any satellite during HO
transitions. This is a mobility related measure.
• Handover failure rate: If a UE does not HO to another
cell despite the weak signal quality of the serving cell,
the UE experiences a HO failure. The HO failure rate is
deﬁned as the ratio of HO failure number to total HO
Interfering Satellites
Interfering Area
Target 
Area
SAT-1
SAT-2
SAT-3
SAT-4
SAT-5
SAT-6
SAT-7
SAT-8
SAT-9
SAT-10
SAT-11
SAT-12
SAT-13
SAT-14
SAT-15
SAT-16
SAT-17
SAT-18
SAT-19
Target Satellites
Figure 3.
LEO constellation evaluation methodology overview.
number. We employ the KPI to show the performance of
high network mobility triggered frequent HOs.
III. EVALUATION METHODOLOGY
In this section, an overview of the proposed evaluation
methodology is ﬁrstly given. After that, the key modules
underlying the evaluation scheme are discussed.
A. Overview of the Proposed Evaluation Scheme
Generally, it is complex and time-consuming to simulate
an entire mega-constellation. To achieve a tradeoff between
simulation complexity and modeling ﬁdelity, a highly efﬁcient
evaluation scheme for the KPI system is indispensable. In light
of this, we exploit the concepts of interfering area and SCs for
assessing the multi-satellite case. The outline of the proposed
evaluation methodology is described as follows.
Step 1): Deﬁne a reference LEO constellation (walker polar
constellation and/or walker delta constellation) with detailed
constellation parameters such as orbit height, orbit inclination,
number of orbits, and number of satellites per orbit. Note that
full constellation simulation is not required, at least for the
initial calibration process in the system-level simulation.
Step 2): Set one or more target areas (e.g., low-latitude area
and/or high-latitude area) for simulation. The set of SCs can
be obtained for the target area based on the H3 hexagonal
hierarchical geospatial indexing system [14]. The cell radius
of the SC is determined by the beam radius at satellite nadir.
Step 3): Divide the simulated time duration into S snapshots
with equal time length. The network topology, e.g., the inter-
satellite distance (ISD), is ﬁxed in each snapshot, and dy-
namically updated during snapshot transitions. The snapshot
duration (e.g., 10s) is typically much larger than that of a time
slot (e.g., 1ms) as deﬁned in Section II-A. The two time scale
conﬁguration can reduce the simulation complexity.
Step 4): Select a subset of N target satellites, and determine
the associated interfering area for approximate interference
analysis. The rule for choosing the interfering area will be
elaborated later in Section III-B. Both the target satellite set
and interfering area are renewed as snapshot iterates.
Step 5): Collect the KPI statistics only for the example N
target satellites. KPI statistics from other satellites in the
interfering area are not required. To this end, the computational
load of the system-level simulations is further decreased.
B. Key Module Analysis
Herein, we investigate the key modules for assessing the
proposed KPI system. Compared with traditional cellular
networks, the following key modules should be paid special
attention and explicitly modeled in the LMCSN simulation.
• Module I: Multiple-satellite interference modeling: To
model the multi-satellite interference, interfering area is
carefully deﬁned. All satellites (including the serving
satellite itself) located inside the interfering area should
be accounted for interference calculation. The interfer-
ence is calculated using dynamic ISD based network
topology and realistic beam layout generated by the
phased array antenna pattern. It should be emphasized
that besides the target area, the interfering area should at
least contain the area covered by a complete surrounding
layer of interfering satellites outside the target area. As
shown in Fig. 3, the interfering area comprises two
components. One is the target area that consists of 7
target satellites, i.e., {SAT-1,...,SAT-7}. The other is the
surrounding area with 12 interfering satellites, i.e., {SAT-
8,...,SAT-19}. Interference from all the 19 satellites is
computed for any target satellite inside the target area.
• Module II: Dynamic topology modeling: Snapshot based
topology generation is adopted to characterize the dy-
namic evolution of network connectivity. With the change
of snapshots, the satellite locations and ISDs are updated
using popular orbit propagation models, e.g., Keplerian
model, and Secular J2 model.
• Module III: Large propagation delay modeling: The
propagation delay is a non-negligible factor in NTN
scenario. It is computed based on real-time UE location
and satellite ephemeris in each time snapshot. Besides,
enhancing features such as timing relationship and HARQ
process should be designed for the simulation.
• Module IV: Unbalanced trafﬁc modeling: The unbalanced
and area-speciﬁc trafﬁc distribution is common in satellite
scenario. To model this effect, we generate non-uniform
service requests by deﬁning hotspot and non-hotspot
areas, where different levels of UE density are used.
IV. PERFORMANCE EVALUATION
In this section, practical simulation settings following 3GPP
NTN assumptions are conﬁgured. Afterwards, numerical re-
sults are presented to evaluate user/control plane related KPIs.
A. Simulation Settings
We use a self-developed system-level simulator with C++
programming for the technical performance evaluation. The
simulator has been accurately calibrated under massive base-
line conﬁgurations deﬁned by 3GPP/ITU. With the simulator,
we have provided the ﬁrst and complete throughput perfor-
mance results among all participant companies in the 3GPP
NTN Release 16 study phase. The throughput results are
captured in the formal technical report [4].
In the simulation, a LMCSN consisting of 1800 satellites
is considered. The target area with longitude and latitude
Table II
KEY SIMULATION PARAMETERS.
Parameters
Values
The number of orbit
60
The number of satellite per orbit
30
Orbit inclination
55
Orbit height
508 km
Available broadcasting beams
5
Available service beams
50
Broadcast beam EIRP density
32.29 dBW/MHz
Service beam EIRP density
41.41 dBW/MHz
Antenna conﬁguration for broadcasting beams
7×7
Antenna conﬁguration for service beams
20×20
Carrier frequency
3.65 GHz
System bandwidth
30 MHz
Channel model
Clear sky with LOS
Scintillation loss and other loss
5.5 dB
UE antenna G/T
-33.62 dB/K
Packet size
0.5 MByte
UE attachment
RSRP
Radio resource scheduler
RR+PF scheduler
PRACH occasion periodicity
80ms
The number of preambles for HO/initial access
10/54
setting to [90,110] and [25,45], respectively, is selected for
simulation. Satellites whose nadir points located inside the
target area are chosen as target satellites, while the interfering
area corresponds to the area covered by all satellites visible
to the target area. The H3 geospatial indexing system [14]
is employed to create a set of SCs, wherein the cell radius
corresponding to broadcasting beams and service beams is
59.8km and 22.6km, respectively. Within the target area, 10
SCs are randomly selected as hotspot SCs, while the other SCs
are non-hotspot SCs. For trafﬁc generation, different number
of UEs are deployed depending on the SC type. The number of
UEs per hotspot SC is set to 500, and the number of UEs per
non-hotspot SC is chosen from the set {100,300} depending on
the scenario conﬁguration. Each UE generates session requests
following poisson arrival rate of 1/300 and average session
duration of 30 seconds. The time span for simulation is 6000
seconds, which is further divided into 600 equal snapshots.
The set of key parameters are summarized in Table II.
For user plan capability related simulations, the round-robin
(RR) beam hopping scheme with interference avoidance is
adopted as the baseline [13]. As in [4], the classical propor-
tional fairness (PF) scheduler with 1ms time slot length is
utilized for radio resource management. While for simulating
control plane related capabilities such as access and mobility,
two benchmark schemes are adopted. The ﬁrst scheme is
termed as Nearest scheme, where each SC is only served
by its nearest satellite. UEs located in a SC are required to
perform HO at the next snapshot if their serving satellite turns
to change. The second scheme is termed as SSB-plan-nearest
scheme, where each SC is covered by two nearest satellites
to ensure seamless HO. The triggering event for HO is based
upon both time CondEvent T1 and signal quality Event A4
as deﬁned in [15]. In CondEvent T1, the HO time duration is
determined as the snapshot when the source satellite cannot
provide broadcasting beams (i.e., SSBs) for current SCs. In
signal quality Event A4, the HO can be executed when the
neighbor satellite’s SSB-SINR becomes better than -6dB. To
further reduce interference, the two satellites coordinate to
Figure 4. Unmet capacity performance of 10 sampling SCs.
Table III
CONSTELLATION SPECIFIC KPI SIMULATION RESULTS.
Characteristic
5%
50%
95%
N-asset coverage (1)
7
9
11
Area trafﬁc capacity (Kbps/km2)
3.96
4.34
4.48
Service availability (%)
0.36
0.37
0.39
illuminate broadcasting beams for a given SC at different time
slots as much as possible.
B. System-Level Simulation Results
The 5-th, 50-th, and 95-th percentile values for N-asset
coverage, area trafﬁc capacity, and service availability are
listed in Table III. As can be seen, the 50% N-asset coverage
for SCs in the target areas is 9, which means a SC can have
9 service satellites whose signal quality is above a predeﬁned
threshold (i.e., SNR > -6dB in our simulation). Besides, the
achieved area trafﬁc capacity is around 4 Kbps/km2 for the
simulated target area. This value is quite smaller compared
with the requirement (i.e., 10 Mbit/s/m2 for hotspots) deﬁned
for terrestrial cellular networks. The service availability ranges
from 0.36 to 0.39, and thus each SC has nearly 1/3 time of
satellite beam service. With respect to the unmet capacity per-
formance, we randomly sample 10 SCs and plot the required
data rate and offered data rate performance in Fig. 4. It can
be observed that the gap between offered and required data
rate in some hotspot SCs, e.g., SC#3, is quite large, and more
than 20 percent of the total capacity demand is not satisﬁed.
The access related performance is plotted in Fig. 5 and
Fig. 6. In particular, Fig. 5 shows the number of connected
users per SC. As the number of users increases, the number
of successfully connected users for both schemes grows. This
indicates that higher collision rate due to more users is not
dominant, and the larger available user pool ﬁnally leads to
higher number of successful connected users. Besides, the
SSB-plan-nearest scheme outperforms the Nearest method,
since in the former case, users have the capability to smoothly
switch to the next satellite when the current satellite is not
available anymore. In contrast, for the latter case, users directly
fallback to the idle state because no more satellites can be
accessed in current time snapshot. Furthermore, Fig. 6 plots
the heat map in terms of the access success probability for
the Nearest scheme. The success rates for all SCs exceed to
95%, which implies that almost all users can access the mega-
constellation networks after few preamble retransmissions.
The mobility related simulation results are illustrated in Fig.
7 and Fig. 8. From Fig. 7, we observe that for the Nearest
Figure 5. CDF of access capacity in target SCs.
Figure 6. Access success probability for the Nearest scheme in target SCs.
scheme, the mobility interruption time distributes uniformly
between 0 and 1 second. This can be accounted by the fact that
UEs can only take HOs to a target satellite within a time limit
of 1s from the starting point of the upcoming snapshot. While
in the SSB-plan-nearest scheme, most of UEs can initiate HO
at current snapshot after measuring the SSB signal quality of
a target satellite. In addition, a small portion of UEs make
HOs from the starting point of the next snapshot when both
the target satellites cannot provide service in the upcoming
snapshot. Fig. 8 depicts the heatmap of HO failure rate for all
target SCs in the SSB-plan-nearest scheme. Notably, only a
few SCs (especially the 10 hotspot cells) experience relatively
higher HO failure rates (e.g., as much as 40%). This is due
to that more UEs in the hotspots require HOs nearly at the
same time. Therefore, the RO/preamble resources become a
bottleneck during such kind of group HOs.
V. CONCLUSION
In this paper, we have proposed a comprehensive KPI
framework for LEO mega-constellation networks. An efﬁcient
multiple-satellite simulation scheme have been devised. Exten-
sive system-level simulation results are present. We hope that
the envisioned systematic KPI framework lays a solid foun-
dation for further LMCSN-oriented performance assessment.
As for future work, we tend to evaluate the KPI results for
various LEO mega-constellation conﬁgurations, different types
of trafﬁc data, and varying duration of time snapshot. Besides,
potential LMCSN speciﬁc technologies can be investigated to
Figure 7. CDF of Mobility interruption time performance.
Figure 8. HO failure rate for the SSB-plan-nearest scheme in target SCs.
improve the network performance, e.g., area trafﬁc capacity
and HO failure rate.
REFERENCES
[1] M. M. Azari and et al., “Evolution of non-terrestrial networks from 5G
to 6G: A survey,” IEEE Communications Surveys & Tutorials, vol. 24,
no. 4, pp. 2633–2672, Fourth Quarter, 2022.
[2] O. Kodheli and et al., “Satellite communications in the new space
era: A survey and future challenges,” IEEE Communications Surveys
& Tutorials, vol. 23, no. 1, pp. 70–109, First Quarter, 2021.
[3] H. Luo and et al., “6G VLEO satellite networks,” Communications of
Huawei Research, vol. 2, pp. 34–45, Sep. 2022.
[4] 3GPP TR 38.821 V16.0.0, “Solutions for NR to support non-terrestrial
networks (NTN),” Release 16, Dec. 2019.
[5] 3GPP TR 38.811 V15.0.0, “Study on new radio (NR) to support non
terrestrial networks,” Release 15, Jun. 2018.
[6] Report ITU-R M.2514-0, “Vision, requirements and evaluation guide-
lines for satellite radio interface(s) of imt-2020,” Sep. 2022.
[7] 3GPP TR 38.913 V17.0.0, “Study on scenarios and requirements for
next generation access technologies,” Release 17, Mar. 2022.
[8] E. Juan, M. Lauridsen, J. Wigard, and P. Mogensen, “Handover solutions
for 5G low-earth orbit satellite networks,” IEEE Access, vol. 10, pp.
2169–3536, Aug. 2022.
[9] I. Leyva-Mayorga and et al., “NGSO constellation design for global
connectivity,” arXiv:2203.16597, pp. 1–26, Apr. 2022.
[10] M. Sheng and et al., “6G service coverage with mega satellite constel-
lations,” China Commu., vol. 19, no. 1, pp. 64–76, Jan. 2022.
[11] N. Chen and et al., “A comprehensive simulation platform for space-
air-ground integrated network,” IEEE Wireless Commu., vol. 27, no. 1,
pp. 178–185, Feb. 2020.
[12] Y. Wang and et al., “Cooperative beam hopping for accurate positioning
in ultra-dense leo satellite networks,” in IEEE ICC Workshops, Jul. 2022.
[13] J. Zhang and et al., “System-level evaluation of beam hopping in NR-
based LEO satellite communication system,” in IEEE WCNC, Mar. 2023.
[14] https://h3geo.org.
[15] 3GPP TS 38.331 V17.2.0, “NR radio resource control (RRC) protocol
speciﬁcation,” Release 17, Sep. 2022.
"
"This paper proposes an approach for scaling causal algorithms, leveraging the Ray distributed computing framework, to meet the demands of industrial-scale data analysis. By parallelizing key steps within causal algorithms, our solution, Nexus, achieves faster estimation times and cost optimization. Ray's scalability and flexibility enable us to conduct substantial parallelism, efficient hyperparameter tuning, and cost optimization, demonstrating improved performance in a case study.","With growing interest in using causal inference for data-driven decisions in large companies like Dream11, traditional AB testing approaches often fall short. Causal studies are crucial, but the limitations of existing causal discovery and inference libraries hinder their scalability in industrial settings. This study presents a distributed computing platform, Nexus, that addresses these challenges.","Several efforts have aimed to scale causal machine learning (CML). Implementing Double ML using AWS Lambda is resource-intensive and has limitations. CausalAI library uses Ray Framework but for limited library implementation. Existing open-source distributed computing frameworks like Apache Spark and Dask often rely on coarse-grained parallelism and single-machine distribution, making them less suitable for complex ML tasks. Ray, with its fine-grained parallelism and distributed task scheduling capabilities, emerged as the preferred choice for scaling Orthogonal Machine Learning (OCML).nannannan","To scale OCML using Ray, we employed distributed cross-fitting to reduce overfitting, leveraging Ray's task parallelism for simultaneous execution of cross-validation folds. Additionally, Ray's Tune library facilitated distributed hyperparameter tuning for optimal model selection. The solution's performance was evaluated by comparing DML implementation from EconML with the DML-Ray modification, demonstrating significantly faster runtimes with varying data scales.nan","The DML-Ray approach outperformed the single-node implementation of DML in terms of runtime, showcasing the advantages of distributed computing in accelerating causal algorithms for large-scale industrial data analysis. The improvements in estimation times and cost efficiency validate the effectiveness of the proposed approach.","This paper highlights the significance of developing scalable causal algorithms. Our approach, Nexus, integrates causal algorithm libraries with Ray's distributed capabilities, facilitating substantial parallelism, cost-effective hyperparameter tuning, and seamless deployment. Future work will focus on expanding the platform's capabilities, including scaling other causal algorithms and enhancing accessibility for a wider user base.",Accelerating Causal Algorithms for Industrial-scale Data: A Distributed Computing Approach with Ray Framework,"Vishal Verma, Vinod Reddy, Jaiprakash Ravi","Accelerating Causal Algorithms for Industrial-scale Data: A
Distributed Computing Approach with Ray Framework
Vishal Verma
vishal.verma@dream11.com
Dream11
Mumbai, India
Vinod Reddy
vinod.reddy@dream11.com
Dream11
Mumbai, India
Jaiprakash Ravi
jaiprakash.r@dream11.com
Dream11
Mumbai, India
ABSTRACT
The increasing need for causal analysis in large-scale industrial
datasets necessitates the development of efficient and scalable
causal algorithms for real-world applications. This paper addresses
the challenge of scaling causal algorithms in the context of conduct-
ing causal analysis on extensive datasets commonly encountered
in industrial settings. Our proposed solution involves enhancing
the scalability of causal algorithm libraries, such as EconML, by
leveraging the parallelism capabilities offered by the distributed
computing framework Ray. We explore the potential of parallelizing
key iterative steps within causal algorithms to significantly reduce
overall runtime, supported by a case study that examines the impact
on estimation times and costs. Through this approach, we aim to
provide a more effective solution for implementing causal analysis
in large-scale industrial applications.
CCS CONCEPTS
• Computer systems organization → Cloud computing; Ma-
chine Learning; • General and reference → Performance.
KEYWORDS
causal machine learning, distributed computing, observational study
ACM Reference Format:
Vishal Verma, Vinod Reddy, and Jaiprakash Ravi. 2023. Accelerating Causal
Algorithms for Industrial-scale Data: A Distributed Computing Approach
with Ray Framework. In Proceedings of The Third International Conference
on Artificial Intelligence and Machine Learning Systems (AIMLSystems 2023).
ACM, New York, NY, USA, 6 pages. https://doi.org/XXXXXXX.XXXXXXX
1
INTRODUCTION
The industry has shown increasing interest in the field of causal
inference in recent years, as it offers the potential to make data-
driven decisions that can significantly impact a business. This is
particularly relevant for large companies like Dream11 [8], which
operate on a complex scale of data, and thus relying solely on AB
[12] testing for causal inference is not always feasible. As a result,
causal studies have become essential in the data-driven decision-
making process, as they help companies shape their future business
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
AIMLSystems 2023, October 25–28, 2023, Bangalore, India
© 2023 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00
https://doi.org/XXXXXXX.XXXXXXX
strategies. However, while traditional AB testing [21, 24] is helpful,
many user-related questions cannot be answered since it is not
possible to intervene with users in many of these, making them
even more crucial to the company’s success. Several libraries have
been introduced for causal discovery and inference (Kalainathan
and Goudet, 2019; Sharma and Kiciman, 2020; Beaumont et al.,
2021)[10, 11, 20] due to the importance of this issue. Nevertheless,
these libraries have certain restrictions and limitations when it
comes to large-scale of data having hundreds of covariates and
confounders.
In this study, we show how we employ a distributed computing
framework to create our own causal inference platform (NEXUS),
which helps us overcome these difficulties and accomplish the fol-
lowing objectives:
• A substantial parallelism
• Scalable and quick tweaking of the hyperparameters
• Cost optimizations
The purpose of this study is to illustrate and investigate the extent to
which such objectives are feasible. We also present a prototype Or-
thogonal Machine Learning implementation by scaling the EconML
Orthogonal ML algorithm with Ray.
2
PRELIMINARIES
2.1
Observational Causal Inference (OCI) :
Setup and Assumptions
For ease of exposition, we consider a situation where our treatment
(𝑡𝑖) is binary. Let {𝑥𝑖,𝑡𝑖,𝑌𝑖} represent the data of user 𝑖, where 𝑥𝑖
is a multidimensional vector of confounders, 𝑡𝑖 ∈ {0, 1} is a binary
treatment and 𝑦𝑖 is the corresponding outcome of interest. Let
𝑌 (𝑡𝑖 = 𝑡) represent the outcome of user 𝑖 under the treatment 𝑡. If
𝑡 is the actual treatment received by 𝑖 then 𝑌 (𝑡𝑖 = 𝑡) is the actual
outcome, otherwise it is the potential (counterfactual) outcome of
user 𝑖 under the treatment. Of course, the fundamental problem
in causal inference is that for each user, we only observe either
𝑌 (𝑡𝑖 = 0) or 𝑌 (𝑡𝑖 = 1)1. For the sake of exposition, we consider two
widely used estimands of interest, the Average Treatment Effect
(ATE), and Conditional Average Treatment Effect (CATE).
𝜏𝑡1,𝑡0 = 𝐴𝑇𝐸(𝑡1,𝑡0) = 𝐸[𝑌 (𝑡𝑖 = 𝑡1) − 𝑌 (𝑡𝑖 = 𝑡0)]
(1)
𝜏𝑡1,𝑡0 (𝑥) = 𝐶𝐴𝑇𝐸(𝑥,𝑡0,𝑡1) = 𝐸[𝑌 (𝑡𝑖 = 𝑡1) − 𝑌 (𝑡𝑖 = 𝑡0)|𝑋 = 𝑥] (2)
Note: 𝜏𝑡1,𝑡0 = 𝐸𝑋 [𝜏𝑡1,𝑡0 (𝑥)|𝑋 = 𝑥]
Of course, as mentioned before, for any user 𝑖 we will only be
able to observe one of the potential outcomes, and therefore the
two equations above are not estimable in their current format[Table
1We work under the Rubin Causal Model for now
arXiv:2401.11932v1  [cs.DC]  22 Jan 2024
AIMLSystems 2023, October 25–28, 2023, Bangalore, India
Vishal Verma, Vinod Reddy, and Jaiprakash Ravi
Table 1: Fundamental Problem of Causal Inference
𝑋
𝑇
𝑌

𝑌 (0)

𝑌 (1)
0
0
0
0
0
0
1
1
1
1
0
0
0
1
1
1
0
0
(1)]. We need to make certain ’identification’ assumptions in or-
der to convert the above expressions into terms that are directly
estimable through observable data. We now proceed to outline the
assumptions under ’selection on observables (unconfoundedness)’.
2.2
Selection on Observables
Identification assumptions are required to go from causal estimands
[eq (1), (2)] to statistical estimands[such as eq(3)] which can be
computed from observed data.
b𝜏𝑡1,𝑡0 (𝑥) = 𝐸[𝑌 |𝑇 = 𝑡1|𝑋 = 𝑥] − 𝐸[𝑌 |𝑇 = 𝑡0|𝑋 = 𝑥]
(3)
Selection of Observables refers to consistent estimation of causal
estimands by controlling for all confounders by making the follow-
ing assumptions.
Assumption 1: Consistency
𝑌𝑖 = 𝑌 (𝑡𝑖); ∀𝑖 = 1, 2, ...,𝑛
(4)
Assumption 2: SUTVA SUTVA essentially requires that a unit’s
outcome only depends on what treatment was assigned to him/her
and does not depend on either what treatment is assigned to other
users (no spill-over) or how the treatment was assigned (assignment
mechanism).
𝑌𝑖 ⊥ 𝑇𝑗; ∀𝑗 ≠ 𝑖
(5)
Assumption 3: Overlap This assumption essentially requires
that there is no unit that has an arbitrarily large (closer to 1) or
arbitrarily small (closer to 0) probability of being selected for treat-
ment.
0 < 𝑃(𝑇 = 𝑡|𝑋) < ∞; ∀𝑡 ∈ 𝒯
(6)
Assumption 4 : Unconfoundedness a.k.a Ignorability No
unobserved confounders.
𝑌 (𝑡) ⊥ 𝑇 |𝑋;∀𝑡 ∈ 𝒯
(7)
T
X
Y
U
?
Figure 1: U are unobserved entities. Assumption 4 means that
there is no causal link between U and the observed data.
We now show how each of these assumptions is necessary for
identification in a setting with no interference [SUTVA holds true]:
Proof.
𝜏𝑡1,𝑡0 = 𝐸𝑋 [𝐸[𝑌𝑖 (𝑇𝑖 = 𝑡1) − 𝑌𝑖 (𝑇𝑖 = 𝑡0)|𝑋]]
= 𝐸𝑋 [𝐸[𝑌𝑖 (𝑇𝑖 = 𝑡1)|𝑋] − 𝐸[𝑌𝑖 (𝑇𝑖 = 𝑡0)|𝑋]]
= 𝐸𝑋 [𝐸[𝑌𝑖 (𝑇𝑖 = 𝑡1)|𝑇,𝑋] − 𝐸[𝑌𝑖 (𝑇𝑖 = 𝑡0)|𝑇,𝑋]] [eq(7)]
= 𝐸𝑋 [𝐸[𝑌 |𝑇 = 𝑡1,𝑋] − 𝐸[𝑌 |𝑇 = 𝑡0,𝑋]] [eq(6)]
□
Note that
𝐸[𝑌 |𝑇 = 𝑡,𝑋] =
∫
𝑦
𝑦𝑓 (𝑦|𝑇 = 𝑡,𝑋)𝑑𝑦
=
∫
𝑦
𝑦
𝑓 (𝑦,𝑇,𝑋)
𝑃(𝑇 = 𝑡|𝑋)𝑓 (𝑋)𝑑𝑦
This necessitates the Overlap assumption so that the conditional ex-
pectation is finite. These assumptions allow methods like Covariate
Matching[22], Metalearners[14], Doubly Robust(DR) Learners[9],
Debiased Machine Learning(DML)[4] etc... to estimate the causal
effects consistently. One of the major challenges of these methods
is that unconfoundedness is fundamentally untestable from data.
Although, efforts have been made to assess the sensitivity of the
obtained estimates to its [6, 7].
2.3
Orthogonal/ Debiased Machine learning
Orthogonal Causal ML (OCML) is a machine learning method for
causal inference that uses orthogonalization to de-bias estimates
of causal effects. It works by first regressing all variables on a set
of nuisance variables, such as covariates and confounders. The
residuals from this regression are then used to estimate the causal
effects of interest. OCML has been shown to be effective in a variety
of settings, including both observational and experimental data.
The scaling of OCML is more covered in section 5. Orthogonal
statistical learning for treatment effect estimation is a general class
of algorithms that work on the following principles:
• The 𝐶𝐴𝑇𝐸 function 𝜃 (𝑋) can be thought of as the minimizer
of some population risk function 𝐸[𝑙(𝑉,𝜃 (𝑋),ℎ(𝑉 ))] where
ℎ is the nuisance function, and 𝑉 = (𝑋,𝑇,𝑌). The loss func-
tion is assumed to be Neyman orthogonal. This is achieved
by residualising 𝑇 and 𝑌 with respect to 𝑋 after estimating
ℎ.
• The nuisance function is fit in a cross-fitting manner for each
sample to obtain ˆℎ(𝑉𝑖);∀𝑖 ∈ {1, 2, ...,𝑛}.
• If the nuisance function ℎ has been estimated consistently,
then the 𝐶𝐴𝑇𝐸 function can be estimated consistently by
minimizing the empirical risk function 𝐸[𝑙(𝑉,𝜃 (𝑋), ˆℎ(𝑉 ))] =
1
𝑛
𝑛Í
𝑖=1
𝑙(𝑉𝑖,𝜃 (𝑋𝑖), ˆℎ(𝑉𝑖)).
2.4
Distributed computing using Ray
Ray [16] is an open-source, unified distributed computing frame-
work that makes it easy to scale machine learning (ML) libraries and
Accelerating Causal Algorithms for Industrial-scale Data: A Distributed Computing Approach with Ray Framework
AIMLSystems 2023, October 25–28, 2023, Bangalore, India
causal machine learning (CML) tasks. It does this by implementing
a unified interface for both task-parallel and actor-based computa-
tion To meet the performance requirements of ML and CML tasks,
Ray distributes two components that are typically centralized in
existing frameworks, such as Spark [26], Dask [17] etc:
• The task scheduler: This component is responsible for sched-
uling tasks across multiple machines. Ray distributes the
task scheduler to ensure that tasks are evenly distributed
and that no machine is overloaded.
• The metadata store: This component maintains the lineage of
computations and a directory for data objects. Ray distributes
the metadata store to ensure that data is accessible to all
machines.
This distribution of components allows Ray to schedule millions of
tasks per second with millisecond-level latency. Ray also provides
lineage-based fault tolerance for tasks and actors and replication-
based fault tolerance for the metadata store. This means that if a
machine fails, Ray can recover the tasks and data that were running
on that machine. Overall, Ray is a powerful and flexible distributed
computing framework that is well-suited for scaling ML and CML
tasks.
Ray was the superior choice for scaling Orthogonal Machine
Learning (OCML) compared to Spark or Joblib due to several key
advantages. Ray’s lower task overhead and support for distributed
state management make it exceptionally well-suited for complex
ML tasks that require fine-grained parallelism, as OCML often
does. In contrast, Spark and Joblib tend to rely on coarse-grained
parallelism, which may not efficiently exploit the full capabilities
of modern distributed systems.
Furthermore, Ray’s ability to efficiently distribute tasks across
multiple machines is a significant advantage over Joblib, which
is primarily designed for distributing tasks on a single machine.
This allows Ray to harness the full potential of distributed comput-
ing resources, making it a more powerful and scalable choice for
OCML workloads that require extensive parallelism and distributed
processing.
3
RELATED WORK
Given the cruciality of the problem, Several implementations have
been done tackling the scale and parallelization of CML. Serverless
Distributed computing on AWS lambda (Malte S. Kurz et. al)[13]
demonstrate implementation of Double ML using AWS Lambda for
parallelization which had shortcomings when it comes to Launch
overheads, Limits on memory and run-time, Limits on hyperparam-
eter tuning, Data Transfer, etc. all these limitations can be addressed
by our implementation using Ray. CausalAI library (Devansh,et
al.)[1] also uses Ray Framework for scaling components limited to
library implementation. Libraries like EconMl [2], CausalML [3],
and DoWhy [20] use libraries like Joblib [25] or multiprocessing
for multi-threading provide parallelism within a single machine,
they do not offer built-in support for distributed computing across
multiple machines. Our implementation on the other hand provides
efficient implementation and workflow for scaling and serving
Causal ML.
4
APPLICATIONS AT DREAM11
As a data-driven company, Dream11 has a culture of experimenta-
tion [23]that enables the exploration of numerous causal inquiries.
However, as outlined in Section 1, there are various user-related
questions that cannot be answered directly through experimen-
tal approaches. Therefore, the importance of observational studies
and quasi-experiments becomes evident in addressing these ques-
tions. Dream11’s diverse array of categories and the corresponding
metrics associated with them present a substantial number of cause-
and-effect investigations and connections to be established within
the graph. Furthermore, the utilization of pre-existing algorithms
from open-source packages such as CausalML[10] and EconML[6]
proves inadequate in handling the voluminous datasets of Dream11
out of the box, which frequently surpasses hundreds of gigabytes in
size. In order to address the aforementioned limitation and success-
fully scale our causal algorithm, we developed our own in-house
causal inference platform called Nexus. Leveraging the capabili-
ties of the scaling and distributed computing framework Ray, we
extensively evaluated various existing open-source distributed com-
puting frameworks like Apache Spark and Dask [17, 26]. Ultimately,
we determined that Ray was the ideal choice for our specific causal
scaling requirements. This comprehensive solution is supported by
our internally developed unified analytics and machine learning
platform, Darwin [Figure 1]. In terms of capabilities Nexus Offers
the following functionalities :
• A user-friendly UI for conducting causal analysis.
• Functionality to leverage distributed scaling with existing
open-source libraries like CausalML, EconML [2, 3]
• Efficient Deployment and Autoscaling capabilities using Ray
Serve.
• Includes integrated validation features such as diagnostic
tests, and refutations tests [18, 19].
Figure 2: End To End OCI workflow at Dream11
5
CASE STUDY: ACCELERATING OCML
5.1
Distributed Crossfitting
OCML utilizes two ML models (section 2.3). There is a possibility
of either one or both models overfitting, which is a common issue
with ML models. The solution to this problem involves a technique
called cross-prediction or out-of-fold residuals.
Cross-prediction can be performed sequentially or in parallel us-
ing multiple threads. However, if we are dealing with large datasets
and attempting to scale the approach, the aforementioned methods
may not be efficient and may take an exponential amount of time.
AIMLSystems 2023, October 25–28, 2023, Bangalore, India
Vishal Verma, Vinod Reddy, and Jaiprakash Ravi
To overcome this problem, we can use Ray remote functions,
which are called Ray tasks. This allows for remote and asynchro-
nous invocations of each of the K folds simultaneously on separate
Python workers.
For demonstration we modified the DML[5] method from EconML
Library to create class DML_Ray modified to run cross-fitting in dis-
tributed manner as ray tasks having value of parameter cv defined
as number of folds
import DML_Ray
from econml.sklearn_extensions.linear_model import
StatsModelsLinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble impor import
RandomForestClassifier
import ray
import os
runtime_env = {""working_dir"": os.getcwd (), ""pip"":
[""dowhy"", ""econml""]}
#Initialize Ray
ray.init(address='auto', ignore_reinit_error=True ,
log_to_driver=False ,runtime_env=runtime_env)
np.random.seed (123)
#Generating Synthetic Data
X = np.random.normal(size =(1000000 , 500))
T = np.random.binomial(1, scipy.special.expit(X[:,
0]))
y = (1 + .5*X[:, 0]) * T + X[:, 0] + np.random.
normal(size =(1000000 ,))
#invoking distributed DML method
est_ray = DML_Ray(
model_y=RandomForestRegressor (),
model_t=RandomForestClassifier (),
model_final=StatsModelsLinearRegression(
fit_intercept=False),
linear_first_stages=False ,
discrete_treatment=True ,
cv=5
)
est_ray.fit(y, T, X=X, W=None)
Figure 3: Sequential Cross Validation
Figure 4: Parallel Cross Validation using Ray Tasks
2 3
5.2
Distributed Tuning
In the above discussion, we learned how distributed cross-fitting
can be utilized to speed up the Double ML algorithm. However, it is
equally important to choose the appropriate model_y and model_t
with the correct hyper-parameters to achieve the best possible
results. One solution to this is to leverage the functionality of Ray
Tune [15] to search through the parameter space and identify the
best hyper-parameters. Ray Tune provides a drop-in replacement
for the scikit-learn wrapper, which allows for easy integration into
the Double ML algorithm. est_ray Above example can be modified
to
est_ray = DML_Ray(
model_y= tune_grid_search_reg (),
model_t= tune_grid_search_clf (),
model_final=StatsModelsLinearRegression(
fit_intercept=False),
linear_first_stages=False ,
discrete_treatment=True ,
cv=5
)
here tune_grid_search_reg and tune_grid_search_clf is Ray
Tune implementation to search the best estimator and parameter.
5.3
Running Time and Scalability
We conducted a performance analysis of the EconML implementa-
tion of DML and our version of DML_Ray at varying scales (10k, 100k,
and 1Million) of treated units and using approximately 500 covari-
ates generated by a synthetic data generator API sourced from https:
//github.com/py-why/dowhy/blob/main/dowhy/datasets.py [20].
The results in Figure 5 demonstrate that using Ray to scale the DML
algorithm leads to significantly better performance compared to
the single-node implementation.
6
CONCLUSION AND FUTURE SCOPE
In conclusion, our paper highlights the importance of developing ef-
ficient and scalable causal algorithms to meet the increasing demand
2For
complete
code
refer
github
Gist
:
https://gist.github.com/vishal-
d11/cd886eb6bdff96ad5a04711cb18339ed
3EconML PR : https://github.com/py-why/EconML/pull/800/files
Accelerating Causal Algorithms for Industrial-scale Data: A Distributed Computing Approach with Ray Framework
AIMLSystems 2023, October 25–28, 2023, Bangalore, India
Figure 5: Distributed HyperParam Optimization using Ray
Tune (Img source: https://speakerdeck.com/anyscale/fast-
and-efficient-hyperparameter-tuning-with-ray-
tune?slide=51)
Figure 6: DML vs DMLRay Runtime on EC2-Highmemory 5
Nodes cluster
for causal analysis in large-scale industrial datasets. We presented
a solution for addressing this challenge by leveraging the power of
distributed computing through the integration of the Ray frame-
work with causal algorithm libraries like EconML. Our approach
involved parallelizing iterative steps in causal algorithms to reduce
overall run-time, as supported by the results of our case study. By
providing a more effective and scalable solution for implementing
causal analysis in industrial applications, we hope our work con-
tributes to advancing the field of causal inference. In future work,
we plan to expand our approach to scale other causal algorithms
and look forward to contributing to the open-source community by
enhancing the scalability of libraries like EconML and making the
approach more accessible to a wider range of users. Moreover, we
look forward to scaling up causal discovery algorithms, including
those based on Bayesian networks and causal graphical models,
using the same principles of distributed computing in the future
7
ACKNOWLEDGEMENTS
We express our gratitude to Hitesh Kapoor and Vinay Jain for setting
up the foundation of exploring Ray Framework for data-science
causal usecases at Dream11. We would also like to acknowledge the
contributions of our Causal inference team members Bihari Pandey,
Namita Porwal, Souvik Mohanta, and Nitesh Kumar for their helpful
feedback and insights during the course of the development of this
paper. We would like to extend our gratitude to Nilesh Patil, Aditya
Narisetty Prasad, Rituj Kate, and Darwin team at Dream11 for
their assistance in building Nexus. Their support was essential in
ensuring the smooth deployment and operation of our platform,
and we greatly appreciate their contribution to the success of this
project. We acknowledge the use of ChatGPT, an AI language model
developed by OpenAI, for assisting in paraphrasing some sections
of this research paper.
REFERENCES
[1] Devansh Arpit, Matthew Fernandez, Chenghao Liu, Weiran Yao, Wenzhuo Yang,
Paul Josel, Shelby Heinecke, Eric Hu, Huan Wang, Stephen Hoi, et al. 2023.
Salesforce CausalAI Library: A Fast and Scalable Framework for Causal Analysis
of Time Series and Tabular Data. arXiv preprint arXiv:2301.10859 (2023).
[2] Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, Miruna
Oprescu, and Vasilis Syrgkanis. 2019.
EconML: A Python Package for ML-
Based Heterogeneous Treatment Effects Estimation.
https://github.com/py-
why/EconML. Version 0.x.
[3] Huigang Chen, Totte Harinen, Jeong-Yoon Lee, Mike Yung, and Zhenyu
Zhao. 2020.
CausalML: Python Package for Causal Machine Learning.
arXiv:2002.11631 [cs.CY]
[4] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Chris-
tian Hansen, Whitney Newey, and James Robins. 2018.
Double/debiased
machine learning for treatment and structural parameters.
The Economet-
rics Journal 21, 3 (feb 2018).
Issue 1.
https://doi.org/10.1111/ectj.12097
https://hdsr.mitpress.mit.edu/pub/wjhth9tr.
[5] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian
Hansen, Whitney Newey, James Robins, et al. 2017. Double/debiased machine
learning for treatment and causal parameters. Technical Report.
[6] Victor Chernozhukov, Carlos Cinelli, Whitney Newey, Amit Sharma, and Vasilis
Syrgkanis. 2022. Long Story Short: Omitted Variable Bias in Causal Machine
Learning. arXiv:2112.13398 [econ.EM]
[7] Carlos Cinelli and Chad Hazlett. 2019.
Making Sense of Sensitiv-
ity: Extending Omitted Variable Bias.
Journal of the Royal Statis-
tical Society Series B: Statistical Methodology 82, 1 (12 2019), 39–67.
https://doi.org/10.1111/rssb.12348 arXiv:https://academic.oup.com/jrsssb/article-
pdf/82/1/39/49320681/jrsssb_82_1_39.pdf
[8] Dream11. 2023. About Us. Retrieved May 31, 2023 from https://www.dreamsports.
group/dream11/
[9] Dylan J. Foster and Vasilis Syrgkanis. 2019. Orthogonal Statistical Learning.
Papers. arXiv.org.
[10] Grace Guo, Ehud Karavani, Alex Endert, and Bum Chul Kwon. 2023. Causalvis:
Visualizations for Causal Inference. In Proceedings of the 2023 CHI Conference on
Human Factors in Computing Systems. 1–20.
[11] Diviyan Kalainathan and Olivier Goudet. 2019. Causal discovery toolbox: Uncover
causal relationships in python. arXiv preprint arXiv:1903.02278 (2019).
[12] Ron Kohavi, Diane Tang, and Ya Xu. 2020.
Trustworthy Online Controlled
Experiments: A Practical Guide to A/B Testing.
Cambridge University Press.
https://doi.org/10.1017/9781108653985
[13] Malte S Kurz. 2021. Distributed double machine learning with a serverless archi-
tecture. In Companion of the ACM/SPEC International Conference on Performance
Engineering. 27–33.
[14] Sören R. Künzel, Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. 2019.
Met-
alearners for estimating heterogeneous treatment effects using machine learn-
ing. Proceedings of the National Academy of Sciences 116, 10 (2019), 4156–4165.
https://doi.org/10.1073/pnas.1804597116
[15] Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E Gonzalez,
and Ion Stoica. 2018. Tune: A research platform for distributed model selection
and training. arXiv preprint arXiv:1807.05118 (2018).
[16] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard
Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan,
et al. 2018. Ray: A distributed framework for emerging {AI} applications. In 13th
{USENIX} Symposium on Operating Systems Design and Implementation ({OSDI}
18). 561–577.
[17] Matthew Rocklin. 2015. Dask: Parallel computation with blocked algorithms and
task scheduling. In Proceedings of the 14th python in science conference, Vol. 130.
SciPy Austin, TX, 136.
AIMLSystems 2023, October 25–28, 2023, Bangalore, India
Vishal Verma, Vinod Reddy, and Jaiprakash Ravi
[18] Alejandro Schuler, Ken Jung, Robert Tibshirani, Trevor Hastie, and Nigam Shah.
2017. Synth-Validation: Selecting the Best Causal Inference Method for a Given
Dataset. arXiv:1711.00083 [stat.ML]
[19] Amit Sharma. [n. d.]. Causal Machine Learning. http://www.amitsharma.in/files/
causal-ml-dowhy-prediction-decisionmaking.pdf
[20] A Sharma and E Kiciman. 2021.
DoWhy: An end-to-end library for causal
inference. arXiv 2020. arXiv preprint arXiv:2011.04216 (2021).
[21] R. Srinivasan, D. Rajeswari, and G. Elangovan. 2023. Customer Churn Prediction
Using Machine Learning Approaches. In 2023 International Conference on Artificial
Intelligence and Knowledge Discovery in Concurrent Engineering (ICECONF). 1–6.
https://doi.org/10.1109/ICECONF57129.2023.10083813
[22] Elizabeth A. Stuart. 2010. Matching Methods for Causal Inference: A Review and
a Look Forward. Statist. Sci. 25, 1 (2010), 1–21. https://doi.org/10.1214/09-STS313
[23] Economic Times. 2023.
Technology will bridge gap between sports and
sports fans: Dream11 CTO Amit Sharma.
Retrieved April 13, 2023 from
https://telecom.economictimes.indiatimes.com/news/internet/technology-
will-bridge-gap-between-sports-and-sports-fans-dream-sports-cto-amit-
sharma/99430836
[24] Martin Tingley, Wenjing Zheng, Simon Ejdemyr, Stephanie Lane, and Colin
McFarland. 2021. Decision Making at Netflix. Retrieved Sep 7, 2021 from https:
//netflixtechblog.com/decision-making-at-netflix-33065fa06481
[25] G Varoquaux and O Grisel. 2009. Joblib: running Python function as pipeline
jobs. packages. python. org/joblib. (2009).
[26] Matei Zaharia, Reynold S Xin, Patrick Wendell, Tathagata Das, Michael Armbrust,
Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J
Franklin, et al. 2016. Apache spark: a unified engine for big data processing.
Commun. ACM 59, 11 (2016), 56–65.
"
"Reconfigurable intelligent surface (RIS) is a promising technology for spectral and energy efficiency enhancement in multi-user orthogonal frequency division multiplexing (OFDM) systems. However, the benefits of RIS may not be as promising as in single-carrier systems since an independent optimization of RIS elements at each subcarrier is impossible in multi-carrier systems. Thus, this paper investigates the performance of various RIS technologies like regular (reflective and passive), simultaneously transmit and reflect (STAR), and multi-sector beyond diagonal (BD) RIS in multi-user MIMO OFDM broadcast channels (BC). This requires to formulate and solve a joint MIMO precoding and RIS optimization problem. The obtained solution reveals that RIS can significantly improve the system performance even when the number of RIS elements is relatively low. Moreover, we develop resource allocation schemes for STAR-RIS and multi-sector BD-RIS in MIMO OFDM BCs, and show that these RIS technologies can outperform a regular RIS, especially when the regular RIS cannot assist the communications for all the users.","Smart radio environments can be realized by employing modern intelligent metasurface technologies [2]. Indeed, wireless channels in systems aided by reconfigurable intelligent surfaces (RIS) are not determined by nature only and can be optimized [3]. This can yield additional degrees of freedom in designing systems, which can be utilized to substantially improve the performance of wireless systems, especially when the system is single carrier [4], [5]. However, in multi-carrier systems, the sub-carriers undergo different channel states. Thus, the relative improvement per carrier might be lower since RIS elements cannot be optimized independently at each sub-carrier. Hence, it should be investigated how well RIS can perform in multi-carrier systems in which one is unable to optimize each carrier independently of other carriers. In this paper, we address this issue and propose a framework to enhance the spectral efficiency (SE) and energy efficiency (EE) of multi-user multiple-input, multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) systems by jointly optimizing the power/transmit covariance matrices and the RIS elements.","Note that there are various technologies for intelligent metasurfaces. The simplest architecture is the passive reflective RIS, which is referred to as a regular RIS in this paper. It has been shown that RIS can be a powerful technology to improve the SE and EE of several single-carrier systems [8]–[24]. For example, the papers in [9], [10] illustrated that RIS can improve the performance of a single-cell multiple-input single-output (MISO) broadcast channel (BC). In [11], [14], the superiority of RIS was examined in multi-cell MIMO BCs by considering different performance metrics such as the minimum and sum rate, global EE and minimum EE of users when transceivers suffer from hardware impairment (HWI). The authors in [17] showed that RIS can increase the weighted sum rate of the secondary users in a MIMO cognitive radio system. In [12], it was demonstrated that RIS can enlarge the achievable rate region of a single-cell MIMO BC. In [18], [19], RIS was employed as a tool to reduce the interference leakage of the K-user interference channels. The paper in [20] exhibited that RIS can enhance the resilience of cell-free MIMO systems. Most of the modern wireless communication systems such as 4G and 5G employ MIMO and OFDM, which suggests developing resource management techniques for RIS-aided MIMO OFDM systems. Additionally, there is no work on RIS-aided OFDM systems that considers the EE metrics, even in single-antenna, single-user OFDM systems. As indicated before, a main goal of 6G is to increase EE by an order of approximately 100 times compared to 5G systems, which makes energy-efficient schemes vital for 6G.nannannan","To solve the non-convex problem (22), we employ majorization minimization (MM) and alternating optimization (AO). To this end, we first fix the RIS coefficients to {\Phi(t-1)}, and solve (22) to obtain P(t)o. We then fix the transmit covariance matrices to P(t)o and update {\Phi} by solving (22). In the following, we describe our proposed algorithm to update {P} and \Phi in separate subsections.nan","Fig. 2 shows the average minimum rate versus P for NB = NU = 2, K = 3, NR = 100, L = 2, N = 2, and Ni = 16. In this figure, we can observe that the regular RIS can substantially increase the minimum rate even when the RIS elements are not optimized (RIS-Rand). Additionally, there is almost a constant gap between the proposed scheme for RIS-aided systems and the scheme with random RIS coefficients (RIS-Rand). Indeed, even though the RIS elements cannot be independently optimized at each subband, we can get a significant gain by optimizing RIS coefficients, which shows the effectiveness of RIS in multi-user MIMO OFDM systems.","This paper proposed resource allocation algorithms with both precoding and RIS element optimizations for a multi-user MIMO RIS-aided OFDM BC with HWI to maximize the SE and EE. We showed that RIS can substantially enhance the SE and EE of the OFDM BC even when the number of RIS elements is low. Moreover, we showed that the benefits of optimizing RIS elements in low SNR regimes are much higher than in high SNR regimes. Additionally, we showed that the benefits of optimizing RIS elements are still significant in OFDM systems even though the RIS coefficients cannot be optimized at each subband independently. Furthermore, we showed that IQI can significantly reduce the average minimum rate even if it is compensated by employing IGS. Of course, the performance degradation by neglecting IQI in the design can be very significant and highly increases with the level of the imbalance at the transceivers. We, moreover, showed that multi-sector BD-RIS can significantly outperform a STAR-RIS as well as a regular RIS.",Maximizing Spectral and Energy Efficiency in Multi-user MIMO OFDM Systems with RIS and Hardware Impairment,"Mohammad Soleymani, Ignacio Santamaria, Aydin Sezgin, Eduard Jorswieck","1
Maximizing Spectral and Energy Efficiency in
Multi-user MIMO OFDM Systems with
RIS and Hardware Impairment
Mohammad Soleymani, Member, IEEE, Ignacio Santamaria, Senior Member, IEEE,
Aydin Sezgin, Senior Member, IEEE, and Eduard Jorswieck, Fellow, IEEE
Abstract—An emerging technology to enhance the spectral
efficiency (SE) and energy efficiency (EE) of wireless communica-
tion systems is reconfigurable intelligent surface (RIS), which is
shown to be very powerful in single-carrier systems. However, in
multi-user orthogonal frequency division multiplexing (OFDM)
systems, RIS may not be as promising as in single-carrier systems
since an independent optimization of RIS elements at each sub-
carrier is impossible in multi-carrier systems. Thus, this paper
investigates the performance of various RIS technologies like reg-
ular (reflective and passive), simultaneously transmit and reflect
(STAR), and multi-sector beyond diagonal (BD) RIS in multi-
user multiple-input multiple-output (MIMO) OFDM broadcast
channels (BC). This requires to formulate and solve a joint
MIMO precoding and RIS optimization problem. The obtained
solution reveals that RIS can significantly improve the system
performance even when the number of RIS elements is relatively
low. Moreover, we develop resource allocation schemes for STAR-
RIS and multi-sector BD-RIS in MIMO OFDM BCs, and show
that these RIS technologies can outperform a regular RIS,
especially when the regular RIS cannot assist the communications
for all the users.
Index Terms—Improper Gaussian signaling, I/Q imbalance,
MIMO OFDM systems, reconfigurable intelligent surface, spec-
tral and energy efficiency.
I. INTRODUCTION
Smart radio environments can be realized by employing
modern intelligent metasurface technologies [2]. Indeed, wire-
less channels in systems aided by reconfigurable intelligent
surfaces (RIS) are not determined by nature only and can be
optimized [3]. This can yield additional degrees of freedom
A preliminary version of a portion of this work was presented in Proc. of
IEEE CAMSAP 2023 [1].
Mohammad Soleymani is with the Signal and System Theory Group, Uni-
versit¨at Paderborn, Germany, (email: mohammad.soleymani@uni-paderborn.
de).
Ignacio Santamaria is with the Department of Communications Engineer-
ing, Universidad de Cantabria (email: i.santamaria@unican.es).
Aydin Sezgin is with the Ruhr University Bochum, Germany (email: aydin.
sezgin@rub.de).
Eduard Jorswieck is with the Institute for Communications Technology,
Technische Universit¨at Braunschweig, 38106 Braunschweig, Germany (e-
mail: jorswieck@ifn.ing.tu-bs.de)
The
work
of
Ignacio
Santamaria
was
funded
by
MCIN/
AEI
/10.13039/501100011033, under Grant PID2022-137099NB-C43 (MADDIE).
This work of Aydin Sezgin is funded by the German Federal Ministry of
Education and Research (BMBF) in the course of the 6GEM Research Hub
under Grant 16KISK037. The work of Eduard Jorswieck was supported by
the Federal Ministry of Education and Research (BMBF, Germany) through
the Program of “Souver¨an. Digital. Vernetzt.” joint Project 6G-RIC, under
Grant 16KISK031.
(DoF) in designing systems, which can be utilized to substan-
tially improve the performance of wireless systems, especially
when the system is single carrier [4], [5]. However, in multi-
carrier systems, the sub-carriers undergo different channel
states. Thus, the relative improvement per carrier might be
lower since RIS elements cannot be optimized independently
at each sub-carrier. Hence, it should be investigated how well
RIS can perform in multi-carrier systems in which one is
unable to optimize each carrier independently of other carriers.
In this paper, we address this issue and propose a framework
to enhance the spectral efficiency (SE) and energy efficiency
(EE) of multi-user multiple-input, multiple-output (MIMO)
orthogonal frequency division multiplexing (OFDM) systems
by jointly optimizing the power/transmit covariance matrices
and the RIS elements. Note that SE and EE improvements
are always among the main concerns of modern wireless
communication systems, where a goal in 6G is to achieve SE
10 times and EE 100 times higher than 5G systems [6], [7].
This work shows that the RIS benefits in multi-user MIMO
OFDM systems decrease with the number of sub-carrier/sub-
bands, but the benefits are still significant even when the
number of RIS elements per user/sub-carrier is very low.
A. Literature review
Note that there are various technologies for intelligent
metasurfaces. The simplest architecture is the passive reflective
RIS, which is referred to as a regular RIS in this paper.
It has been shown that RIS can be a powerful technology
to improve the SE and EE of several single-carrier systems
[8]–[24]. For example, the papers in [9], [10] illustrated that
RIS can improve the performance of a single-cell multiple-
input single-output (MISO) broadcast channel (BC). In [11],
[14], the superiority of RIS was examined in multi-cell MIMO
BCs by considering different performance metrics such as the
minimum and sum rate, global EE and minimum EE of users
when transceivers suffer from hardware impairment (HWI).
The authors in [17] showed that RIS can increase the weighted
sum rate of the secondary users in a MIMO cognitive radio
system. In [12], it was demonstrated that RIS can enlarge the
achievable rate region of a single-cell MIMO BC. In [18],
[19], RIS was employed as a tool to reduce the interference
leakage of the K-user interference channels. The paper in
[20] exhibited that RIS can enhance the resilience of cell-free
MIMO systems.
arXiv:2401.11921v1  [cs.IT]  22 Jan 2024
2
A drawback of a regular RIS is that it cannot provide an
omni-directional coverage. In other words, the transmitter and
receiver have to be in the reflection space of a regular RIS so
that the regular RIS can bring any benefit to the system. Thus,
a regular RIS supports only a half-space coverage. To address
this issue, another technology for intelligent metasurfaces is
proposed that can allow transmission and reflection at the
same time, which in known as simultaneously transmit and
reflect (STAR) RIS [25]. Note that STAR-RIS is also known
as intelligent omni-surfaces since it can provide an omni-
directional coverage [25], [26]. The superiority of STAR-
RIS over regular RIS has been investigated in [16], [27]–
[31] by taking into account different scenarios. For instance,
in [27]–[29], it was shown that STAR-RIS can improve the
performance of a two-user BC with single-antenna receivers.
In [16], we proposed schemes to increase the EE of a single-
cell MIMO STAR-RIS-aided BC. In [30], it was demonstrated
that STAR-RIS can increase the minimum rate of users in a
multi-cell BC with I/Q imbalance (IQI). Moreover, in [31],
it was shown that STAR-RIS can enhance the performance
of MISO ultra-reliable low-latency communications (URLLC)
BCs.
The concept of STAR-RIS can further be generalized to
multi-sector beyond diagonal (BD) RIS in which the coverage
area is divided into multiple sectors (possibly more than two)
[32], [33]. In this case, the multi-sector BD-RIS can receive
signal in a sector and partially reflect it in all the sectors,
which enables a full-space coverage similar to STAR-RIS. In
this sense, multi-sector BD-RIS can be also categorized as
omni-directional surfaces. Note that in BD-RIS, the matrix
for the RIS coefficients can be non-diagonal since the BD-RIS
elements can be connected via a circuit design. In this paper,
we consider the single-connected multi-sector BD-RIS in
which the RIS elements in each sector are not connected to the
other RIS elements in the sector. It should be emphasized that
the main advantage of multi-sector BD-RIS over STAR-RIS
is that the multi-sector BD-RIS can provide more directional
beams in each sector, which can enhance the channel gain
[33]. Additionally, the channel of a user in a sector can be
optimized independently of the channels of the users in the
other sectors. This simplifies optimization of the channels of
the users and enhance the performance of RIS.
Yet, one of the main factors limiting the performance of
wireless communication systems is HWI. In practice, devices
never perform ideally, and if we do not take the device
imperfections into consideration when designing a system,
the system performance can drop significantly [34]–[37]. A
common source of HWI is I/Q imbalance (IQI), which happens
because of an imbalance in the in-phase and quadrature signals
[35], [36]. IQI is modeled as a widely linear transformation
(WLT), which makes the output signal improper when the
input signal is proper [35], [36]. To compensate for IQI, one
can resort to improper Gaussian signaling (IGS), which is
also an effective interference-management tool, especially in
single-carrier systems [34], [38], [39]. However, the benefits of
IGS as an interference-management technique may disappear
in multi-carrier systems as the number of resources per users
increases, which in turn may help to easier manage interfer-
TABLE I: Summary of the most related papers on OFDM RIS-aided
systems.
This paper
[41]–[50]
[51], [52]
[53]
MIMO
√
√
EE metrics
√
Fairness rate
√
Multi-user
√
√
HWI
√
√
IGS
√
STAR-RIS
√
Multi-sector BD-RIS
√
ence by a simpler transmission strategy [40].
Now, single-carrier systems are very efficient in frequency-
flat channels. However, this might not necessarily be the case
when the channels are frequency selective since a demand-
ing process is needed for equalizing the channels in single-
carrier systems. To easier cope with frequency selectivity of
wireless channels, multi-carrier techniques such as OFDM can
be employed, which are able to more efficiently utilize the
spectrum. Multi-carrier systems divide the frequency band into
several sub-bands, which permits a more intelligent power
allocation, based on the channel response on each frequency
sub-band. However, RIS cannot exploit this feature since it is
impossible to independently optimize RIS at each sub-band
[45]. In a more realistic scenario, the RIS elements remain
approximately constant across all frequency subbands, which
is highly suboptimal when the number of subbands grows.
Thus, one might expect that the benefits of RIS may disappear
when the bandwidth is large, and consequently, the number of
subbands is high.
Although limited, yet a few studies exist on RIS-aided
multi-user OFDM systems [41]–[66]. Many studies on RIS-
aided OFDM systems considered a single-user system [51]–
[66]. For instance, in [55], the authors proposed a joint channel
estimation and passive beamforming scheme for a RIS-aided
OFDM system and showed that RIS can substantially improve
the SE of the system. A low-cost passive beam forming was
proposed in [56] for point-to-point SISO RIS-aided OFDM
systems, and it was shown that RIS can increase the achievable
rate of the system. The authors in [58] considered a point-
to-point single-input single-output (SISO) RIS-aided OFDM
system and proposed schemes to estimate channels as well as
to optimize RIS elements. In [59], the authors developed a
practical transmission protocol for point-to-point SISO RIS-
aided OFDM systems. The authors in [51] proposed schemes
to optimize a MIMO OFDM simultaneous wireless infor-
mation and power transfer (SWIPT) RIS-aided system with
a non-linear energy harvesting model and showed that RIS
can enhance the system performance. In [52], the authors
considered a point-to-point MIMO RIS-aided OFDM system
and showed that RIS can improve the SE of the system.
The performance of RIS in a multi-user OFDM system has
been studied in [41]–[49]. In [43], [45], it was shown that
RIS can increase the sum-rate of multi-user multiple-input
single-output (MISO) RIS-aided OFDM systems. In [44], the
authors studied a multi-user SISO RIS-aided OFDM BC and
optimized the RIS elements to maximize the total power
received by users for a given transmit power. The authors in
3
[42] considered an uplink of a multi-user OFDM system, and
showed that RIS can decrease the bit error rate of the system.
B. Motivation
We provide a brief summary for the papers on RIS in
OFDM systems in Table I, based on the system model, the
considered performance metric, assumptions regarding the
devices, and the considered RIS technology. Even though the
studies in [41]–[49], [51], [52], [54]–[66] have provided a
valuable insight on the performance of RIS in OFDM systems,
there are still some open problems in RIS-aided OFDM
systems, which are the focus of this paper. For instance, the
performance of RIS has not been studied in multi-user MIMO
OFDM systems. Most of the modern wireless communication
systems such as 4G and 5G employ MIMO and OFDM,
which suggests developing resource management techniques
for RIS-aided MIMO OFDM systems. Additionally, there is
no work on RIS-aided OFDM systems that considers the EE
metrics, even in single-antenna, single-user OFDM systems.
As indicated before, a main goal of 6G is to increase EE
by an order of approximately 100 times compared to 5G
systems, which makes energy-efficient schemes vital for 6G.
Furthermore, there is only one paper (i.e., [53]) on RIS-aided
OFDM systems, which considered a single-user user system
with additive hardware distortion noise. Indeed, IQI has not
been studied in RIS-aided OFDM systems, while in practice,
devices are never ideal and may suffer from IQI and/or other
sources of imperfections. Moreover, there is no work on RIS-
aided OFDM systems with more recent RIS technologies such
as STAR-RIS or multi-sector BD-RIS.
As a summary, there are still some open research lines in
MU-MIMO RIS-aided OFDM systems, which require further
investigations. These include developing HWI-aware designs,
energy-efficient techniques and fairness-aware schemes. More-
over, there are some new technologies and/or concepts for RIS
that have not been studied in MU-MIMO OFDM systems.
C. Contribution
In this paper, we maximize the SE and the EE of multi-
user MIMO RIS-aided OFDM systems by considering various
technologies for RIS like regular, STAR and multi-sector BD-
RIS. To the best of our knowledge, this is the first work on
RIS-aided OFDM systems that considers EE metrics and pro-
poses IQI-aware schemes for RIS-aided OFDM systems. In the
conference version of this work [1], we proposed optimization
schemes for maximizing the minimum rate of users in a single-
cell MIMO RIS-aided BC with perfect devices. This paper
extends the results in [1] to multi-cell MIMO RIS-aided BC
with IQI, and considers other technologies for RIS, including
STAR-RIS and multi-sector BD-RIS.
The proposed algorithms in this work can be applied to a
wide range of optimization problems in which the objective
function and/or constraints are linear functions of the rates
and/or EE of users. Such optimization problems include many
practical objective functions such as minimum and/or sum rate,
global EE, and minimum EE. Furthermore, we can apply the
algorithms to any multi-user MIMO RIS-aided OFDM system
with IQI at transceivers. In this work, we consider a multi-
cell MIMO RIS-aided OFDM BC as an example of such a
system and show that RIS (either regular, STAR, or multi-
sector) can substantially increase the SE and EE even with a
relatively low number of RIS components. In more detail, our
numerical results show that optimizing the RIS elements can
provide a high gain in OFDM systems even when we cannot
independently optimize the RIS elements at each subband.
These results show that RIS can be a promising technology to
enhance the performance of OFDM systems.
We, additionally, propose IQI-aware schemes for MU
MIMO RIS-aided OFDM systems. To this end, we employ
IGS since IQI makes the signals improper if proper signaling
is employed, which reduces the achievable rate [35]. We can
also employ IGS as an interference-management technique,
especially in single-carrier systems. However, as shown in
[40], the IGS benefits as an interference-management tool
vanish when the number of subbands increases. Indeed, the
higher the number of subbands is, the more resources per user
are available, which reduces the interference level. Hence, the
IGS gains in this scenario are mainly because of the ability to
compensate for IQI. Note that IQI can significantly decrease
the system performance, especially when we neglect it in
designing signaling schemes.
Finally, we develop optimization schemes for STAR-RIS
as well as multi-sector BD-RIS in multi-user MIMO OFDM
BCs. We show that STAR-RIS even with random coefficients
can outperform a regular RIS with optimized elements. Addi-
tionally, it is shown that optimizing STAR-RIS elements can
result in substantial improvements, and STAR-RIS can highly
outperform a regular RIS when all the users are not in the
coverage area of the regular RIS. Moreover, our results show
that multi-sector BD-RIS can significantly outperform STAR-
RIS. We consider different schemes for operating STAR-
RIS and multi-sector BD-RIS. Firstly, we assume that all
the multi-sector BD-RIS elements can operate in all sectors
simultaneously, which we refer to as the energy splitting (ES)
scheme. Secondly, we assume that each multi-sector BD-RIS
element can operate in only one sector at a time, which
is called the mode switching (MS) scheme. We show that
the performance of the MS scheme is comparable with ES,
while it has much lower computational and implementation
complexities.
II. SYSTEM MODEL
We consider a multi-cell BC consisting of L multiple-
antenna BS with NB antennas each. In each cell, there are
K users with NU antennas each. Moreover, there are N RISs
(either multi-sector, STAR or regular) with NR elements to
assists the BS, and there is at least one RIS in each cell.
We assume that each BS transmits a linear superposition of
multicarrier OFDM signals with Ni sub-carriers.
A. RIS model
This paper studies different RIS technologies such as reg-
ular, STAR and multi-sector in a multi-user MIMO OFDM
system. Below, we first describe the concept of STAR-RIS
4
STAR RIS
Reflection Space TX Space
Transmit
Reflect
1
BS
12
U
11
U
13
U
14
U
(a) STAR-RIS (Ns = 2).
1
BS
11
U
12
U
15
U
Sector 1 Sector 2 
Sector 3 
Sector 4 
BD-RIS
13
U
14
U
U16
17
U
18
U
(b) Four-sector BD-RIS (Ns = 2).
Fig. 1: A broadcast channel aided by a multi-sector BD-RIS.
and multi-sector BD-RIS briefly. Then we state the effective
channels in MIMO OFDM systems, aided by these RIS tech-
nologies. Moreover, we explain different operational modes
for a multi-sector BD-RIS and/or STAR-RIS.
1) Concept of multi-sector BD-RIS: Multi-sector BD-RIS
is a generalization of STAR-RIS and can be categorized as
intelligent omni-surfaces, which can provide a full 360◦ cov-
erage. In a STAR-RIS, each element can transmit and reflect
simultaneously, which realizes an omni-directional coverage
[26]. Thus, there are two spaces for each STAR-RIS, which
are referred to as the reflection space and the transmission
space (see Fig. 1a). The concept/technology of STAR-RIS can
be extended to multi-sector BD-RIS in which each RIS can
have multiple sectors, each covering a set of users and/or a
sub-space, as shown in Fig. 1b. In the most general case, a
multi-sector BD-RIS can have Ns sectors, and when Ns = 2,
the multi-sector BD-RIS is equivalent to a STAR-RIS. Thus,
in a multi-sector-BD-RIS-aided system, each user belongs to a
region/space, covered by a sector of the multi-sector BD-RIS.
For instance, in Fig. 1b, users u11, u12 and u13 are covered
by the first sector of the multi-sector BD-RIS, while users u17
and u18 are covered by the fourth sector of the multi-sector
BD-RIS.
2) Effective channels in frequency domain: We consider
the channels in frequency domain. To this end, we employ the
channel model in [45, Eq. (12)]. Thus, the channel between
the j-th BS and the k-th user associated to the l-th BS, denoted
by ulk, at subband i is
Hlk,j,i({Φ})=
N
X
n=1
Glk,n,iΦns
n Gnj,i+Flk,j,i
(1)
where Glk,n,i is the channel between ulk and the n-th RIS in
subband i, Gnj,i is the channel between the n-th multi-sector
BD-RIS and the j-th BS in subband i, Flk,j,i is the direct
link between ulk and the j-th BS in subband i. Moreover, the
matrix Φns
n
contains the coefficients for the ns-th sector of
the multi-sector BD-RIS in which ulk is located. The matrices
Φns
n s for ns = 1, · · · , Ns are diagonal as
Φns
n = diag
5
|ϕ1
nm|2 + |ϕ2
nm|2 = 1.
(10)
Indeed, TSN is only valid for STAR-RIS (Ns = 2) and is more
stringent than the other feasibility sets since it has additional
constraints on the phases of the STAR-RIS components.
Moreover, it can be easily verified that TSN ⊂ TI ⊂ TU.
Hereafter, we represent the feasibility set of RIS coefficients
by T , unless we explicitly refer to one of these feasibility sets.
Note that according to the model in [45], the reflecting co-
efficients cannot be independently optimized at each subband.
Thus, we assume that the coefficients remain constant across
all the frequency subbands. Additionally, it should be noted
that as can be verified through (1), the channels are linear
functions of {Φ} = {Φns
n }∀ns,n. However, for notational
simplicity, we do not state this explicitly hereafter.
3) Operational modes for multi-sector BD-RIS: There are
different possibilities to operate a multi-sector BD-RIS. For
instance, all the multi-sector BD-RIS components can actively
operate in all sectors, which is referred to as the ES scheme.
This scheme is the most general case to operate a multi-sector
BD-RIS, and is expected to outperform the other operational
schemes. However, the ES scheme has also higher computa-
tional and implementation complexities. In this mode, there are
Ns complex-valued optimization parameters per multi-sector-
BD-RIS element, which can provide more design flexibility at
the cost of higher computational complexities. Additionally,
operating each component of the multi-sector BD-RIS in
all sectors may require more advanced circuit designs. As
indicated, the coefficients corresponding to each sector might
be also highly dependent not only through the amplitudes,
but also through the phases, which may make ES schemes
inefficient in practice.
To cope with the challenges of ES schemes, we can consider
different operational modes for multi-sector BD-RIS with
lower complexities. For instance, each component of the multi-
sector BD-RIS can operate only in a sector at a time. To realize
such schemes, one possibility can be to divide the components
of the multi-sector BD-RIS into Ns groups. In the ns-th group,
all the components of the multi-sector BD-RIS operate only
in the ns-th sector, which is called the MS scheme. To further
clarify MS, we provide an example for Ns = 2, which is
equivalent to a STAR-RIS. In this case, the components are
divided into two groups. In the first group, all the STAR-RIS
components operate only in the reflection mode, while the
STAR-RIS elements in the other group operate only in the
transmission mode. Another possibility could be to operate all
the elements of a multi-sector BD-RIS in one sector in a time
slot, and switch the operating sector in next time slots in a
round robin to cover all the sectors, which we call the time
switching (TS) scheme. In a STAR-RIS, it means that all the
STAR-RIS elements operate in a reflection mode in a time
slot, while they all operate in a transmission mode in the next
time slot. Of course, there can be also hybrid schemes, which
are a combination of the ES, MS and TS schemes, but we do
not consider such schemes in this work.
The ES and MS schemes can provide a full coverage at
a time, while the TS scheme covers only a subspace and is
unable to provide a full coverage in a single time slot. As
a result, we consider only the ES and MS schemes, but our
proposed schemes can be easily applied to TS and/or hybrid
schemes.
B. I/Q imbalance model
We consider the IQI model in [35]. Note, that the model
in [35] considers single-carrier systems, however we can be
extend it to multi-carrier systems. If the system at hand is
wide-band, it is expected that the IQI parameters differ at each
subband, based on [68, Table I]. However, in narrow-band
systems, the IQI parameters can be the same in all subbands.
In this paper, we investigate the most general case in which
each subband can have different IQI parameters. We briefly
restate the model in [35] for the sake of completeness below.
When IQI occurs at a device, the output signal is a WLT
of the input signal. Hence, if we denote the input signal
of a MIMO OFDM system at subband i as xi, the actual
transmitted signal is
xt,i = Γt1,ixi + Γt2,ix∗
i ,
(11)
where x∗
i is the conjugate of xi, and the coefficients Γt1,i and
Γt2,i are, respectively, given by
Γt1,i = I + AT,iejψT,i
2
,
Γt2,i = I − Γ∗
t1,i,
(12)
where AT,i = at,iINU and ψT,i = ψt,iINU are, respectively,
the coefficients corresponding to the amplitude and phase
mismatches at the transmitter side. Note that at,i and ψt,i
are real-valued scalar parameters, and the device is ideal if
at,i = 1 and ψt,i = 0.
At the receiver side, we have a similar model in which the
output signal is a WLT of the received signal as
yi = Γr1,iyr,i + Γr2,iy∗
r,i,
(13)
where yr,i = Hixt,i+ni is the received signal, where Hi and
ni are, respectively, the MIMO channel, and additive noise at
the i-th subband. Moreover, the coefficients Γr1,i and Γr2,i
are
Γr1,i = I + AR,iejϕR,i
2
,
Γr2,i = I − Γ∗
r1,i,
(14)
respectively, where AR,i = ar,iINB and ϕR,i = ϕr,iINB are,
respective, the coefficients corresponding to the amplitude and
phase mismatches at the receiver side. Similarly, ar,i and ϕr,i
are real-valued scalar parameters, and the receiver is perfect
if ar,i = 1 and ϕr,i = 0. Finally, the signal at the output of
the receiver is
yi =
6
where yi, xi, and ni are, respectively, the real decomposition
of yi, xi, and Γr1,ini+Γr2,in∗
i . Note that the effective noise at
the output of the receiver is ni, which is zero-mean Gaussian
with covariance matrix Ci = σ2ΓiΓT
i , where σ2 is the noise
variance at each received antenna, and Γi can be obtained
as in [14, Eq. (13)]. Moreover, the equivalent channel Hi is
given by [14, Eq. (11)].
C. Signal model
BS l intends to transmit
xl,i =
K
X
k=1
xlk,i,
(16)
at subband i, where xlk,i is the transmit signal of BS l intended
for ulk at subband i. As described in Section II-B, the signal
that BS l transmits is a WLT of xl,i, based on (11). Employing
Lemma 1, the received signal received by ulk in subband i is
ylk,i =
L
X
j=1
Hlk,j,ixj,i + nlk,i,
where nlk,i is the real-decomposition of the effective addi-
tive zero-mean Gaussian noise at user k in subband i with
covariance matrix Clk,i, and Hlk,j,i is the equivalent channel
between BS j and ulk at subband i, given by Lemma 1. Note
that each channel is a linear function of {Φ}, according to (1).
Hence, the effective channel is also a linear function of {Φ}.
Moreover, note that all the signals xlk,is, are independent zero-
mean and possibly improper Gaussian random vectors, where
Plk,i = E{xlk,ixT
lk,i}.
The achievable rate of ulk is equal to
rlk =
Ni
X
i=1
rlk,i,
(17)
where rlk,i is the rate of decoding xlk,i at ulk treating intra-cell
and inter-cell interference as noise
rlk,i = 1
2 log2
I + D−1
lk,iSlk,i

(18a)
= 1
2 log2 |Dlk,i + Slk,i|
|
{z
}
rlk,i1
− 1
2log2|Dlk,i|
|
{z
}
rlk,i2
,
(18b)
where Slk,i = Hlk,l,iPlk,iHT
lk,l,i is the covariance matrix
of the useful signal at ulk in subband i, and Dlk,i is the
covariance matrix of noise plus interference at ulk in subband
i, given by
Dlk,i =
L
X
n=1,n̸=l
Hlk,n,iPn,iHT
lk,n,i
+
K
X
m=1,m̸=k
Hlk,l,iPlm,iHT
lk,l,i + Clk,i.
(19)
The EE of ulk can be written as [69]
elk =
rlk
pc + η P
i Tr (Plk,i),
(20)
where η and pc are defined as in [14, Eqs. (26)-(28)]. Addi-
tionally, the global EE (GEE) can be obtained as [69]
GEE =
P
∀l,k rlk
LKpc + η P
l Tr (Pl),
(21)
where Pl = P
k,i Plk,i.
D. Problem statement
We aim at maximizing the SE and EE, which can be
formulated as
max
{P},{Φ}∈T
f0({P},{Φ})
(22a)
s.t. fg ({P},{Φ}) ≥ 0,
∀g,
(22b)
X
∀k,i
Tr (Plk,i) ≤ Pl,
∀l,
(22c)
Plk,i ≽ 0,
∀l, k, i
(22d)
where {P} = {Plk,i}∀l,k,i and {Φ} = {Φm}∀m are the
optimization variables, and Pl is the power budget of BS
l. The functions f0 and fi include EE and SE metrics, and
thus, can be considered as a linear function of rlk, and/or
elk, and/or GEE. The optimization problems that can be
cast as (22) have been discussed in [11, Sec. II.D]. Such
problems include, but not limited to, the maximization of
the minimum/sum rate, minimum EE and global EE. Note
that these objective functions and/or optimization problems
are among the most practical performance metrics in wireless
communication systems.
III. OPTIMIZATION ALGORITHMS TO SOLVE (22)
To solve the non-convex problem (22), we employ majoriza-
tion minimization (MM) and alternating optimization (AO).
To this end, we first fix the RIS coefficients to {Φ(t−1)},
and solve (22) to obtain

P(t)	
. We then fix the transmit
covariance matrices to

P(t)	
and update {Φ} by solving
(22). In the following, we describe our proposed algorithm to
update {P} and Φ in separate subsections.
A. Updating transmit covariance matrices
When RIS components are fixed to Φ(t−1), (22) is
max
{P} f0

{P},{Φ(t−1)}

(23a)
s.t. fg

{P},{Φ(t−1)}

≥ 0, ∀g,
(23b)
(22c), (22d),
(23c)
which is non-convex since rlk
7
−
K
X
j=1,̸=k
R
(
Tr
 
HH
lk,l,i(D(t−1)
lk,i )−1Hlk,l,i
ln 2

Plj,i − P(t−1)
lj,i
!)
−
L
X
n=1,̸=l
R
(
Tr
 
HH
lk,n,i(D(t−1)
lk,i )−1Hlk,n,i
ln 2

Pn,i − P(t−1)
n,i
!)
,
where
r(t−1)
lk,i2
=
rlk,i2
8
s.t.
ˆfg
n
P(t)o
,{Φ}

≥ 0,
∀g
(35b)
(9), (8), (28), (30),
(35c)
Then we update {Φ} according to the rule in (33).
Now we present our solution for the MS scheme. That is,
we randomly divide the multi-sector BD-RIS elements into
Ns groups such that there are at least ⌊ NRIS
Ns ⌋ elements per
group. Then the RIS elements in the ns-th group, indicated
by Gmns, operate only in the sector ns, and thus, ϕns
mn = 0 if
n /∈ Gmns. Inserting this rule in (27) and (31) gives the MS
solution for TU and TI, respectively. For TI, we have to update
{Φ} based on the rule in (33). Note that the sets TI and TSN
are equivalent for MS schemes since the phase dependency
constraints are automatically satisfied when each STAR-RIS
element is active in only one sector.
IV. NUMERICAL RESULTS
In this section, we employ Monte Carlo simulations to
provide numerical results. We assume that the small-scale
fading for the channels Gi and Gki for all i, k is Rician
similar to [11], [13] since there is a line of sight (LoS) link
between the BS and RIS as well as between the RIS and the
users. However, as the links between the BS and the users are
assumed to be non-LoS (NLoS), the small-scale fading for
Fki for all k, i is assumed to be Rayleigh distributed. Note
that it is also assumed that there is no correlation between the
channels at different subbands, which is an extreme case, and
it can be expected that the performance of RIS is improved
with correlated channels. The simulation scenario is based on
[11, Fig. 2], unless explicitly mentioned otherwise. For more
descriptions on the simulation parameters and setup, we refer
the reader to [11], [15].
A. Maximization of the minimum rate
Here, we investigate the effectiveness of the various RIS
technologies from a SE point of view by considering the
maximization of the minimum rate. To this end, we consider
the impact of different system parameters, including the power
budget at the BSs, the number of subbands, the number of RIS
elements, and IQI parameters.
1) Impact of power budget: Fig. 2 shows the average
minimum rate versus P for NB = NU = 2, K = 3, NR = 100,
L = 2, N = 2, and Ni = 16. In this figure, we can observe
that the regular RIS can substantially increase the minimum
rate even when the RIS elements are not optimized (RIS-
Rand). Additionally, there is almost a constant gap between
the proposed scheme for RIS-aided systems and the scheme
with random RIS coefficients (RIS-Rand). Indeed, even though
the RIS elements cannot be independently optimized at each
subband, we can get a significant gain by optimizing RIS
coefficients, which shows the effectiveness of RIS in multi-
user MIMO OFDM systems.
2) Impact of the number of subbands: Fig. 3 shows the av-
erage performance improvements versus P for NB = NU = 2,
K = 3, NR = 100, L = 1, N = 1, different Ni and
at,i = ar,i = 1 for all i. The relative performance curves
in Fig. 3a and Fig. 3b are obtained by comparing the average
0
4
8
12
17
10
20
30
35
P (dB)
Minimum Rate (b/s/Hz)
RIS
RIS-Rand
No-RIS
Fig. 2: Average minimum rate versus P for the case without RIS (No-
RIS), randomly configured RIS (RIS-Rand), and optimized regular
RIS (RIS) with NB = NU = 2, K = 3, NR = 100, L = 2, N = 2,
and Ni = 16.
0
4
8
12
17
100
200
300
400
500
600
700
P (dB)
Relative Performance Improvement (%)
Ni = 2
Ni = 16
Ni = 32
(a) RIS compared to No-RIS.
0
4
8
12
17
20
40
60
80
100
110
P (dB)
Relative Performance Improvement (%)
Ni = 2
Ni = 16
Ni = 32
(b) RIS compared to RIS-Rand.
Fig. 3: Improvements by RIS versus P for NB = 2, NU = 2, K = 3,
L = 1, N = 1, and different Ni.
minimum rate achieved by our scheme for RIS-aided OFDM
systems with the average minimum rate of OFDM systems
without RIS and with the average minimum rate of OFDM
systems with random RIS coefficients, respectively. According
to Fig. 3, RIS can provide a huge gain. However, the benefits of
optimizing RIS components highly decrease with Ni since RIS
elements cannot be independently optimized at each subband,
and as the number of subbands for a fixed NR increases,
the effectiveness of optimizing Φ decreases. Interestingly, the
benefits of optimizing RIS elements are still significant even
when there are slightly higher than 1 RIS elements per user
per subband (NR/(KNi) ≃ 1.04 when Ni=32).
Furthermore, the gains of optimizing RIS elements are
much higher in low SNR regimes. Since we also consider
a power/covariance matrix optimization, it may happen that
the signals for a user are transmitted over a few subbands
when the BS power budget is low. Thus, the effective number
of the utilized subbands is lower than Ni, especially at low
SNR regimes, which enhances the benefits of optimizing RIS
elements. Additionally, as shown in our previous studies [11],
[14], the benefits of employing RIS are higher in low SNR
regimes, which may enhance the gain of a proper optimization
of RIS elements.
3) Impact of NR: Fig. 4 shows the average performance
improvements versus NR for NB = NU = 1, K = 3, L = 1,
9
20
40
60
80
100
0
200
400
600
800
NRIS
Relative Performance Improvement (%)
SNR=0dB
SNR=10dB
SNR=17dB
(a) RIS compared to No-RIS.
20
40
60
80
100
0
20
40
60
80
100
120
NRIS
Relative Performance Improvement (%)
SNR=0dB
SNR=10dB
SNR=17dB
(b) RIS compared to RIS-Rand.
Fig. 4: Improvements by RIS versus P for NB = 2, NU = 2, K = 3,
L = 2, N = 2, and Ni = 16.
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10
15
20
25
at
Minimum Rate (b/s/Hz)
IQI-aware (RIS)
IQI-Unaware (RIS)
IQI-aware (No-RIS)
IQI-Unaware (No-RIS)
Fig. 5: Impact of IQI on the average minimum rate of users for
NB = 2, NU = 2, K = 2, NR = 100, L = 2, N = 2, and Ni = 16.
Ni = 16, N = 1, and different SNRs. In this figure, the
benefits of employing RIS almost linearly increases with NR,
especially at low SNR. Additionally, we can observe that the
slope of the RIS benefits is higher in lower SNR, which is in
line with the results in the Figs. 2 and 3. Moreover, optimizing
RIS elements can yield substantial gains even when NR = 10,
which is lower than the number of sub-bands (in this case
NR/(KNi) ≃ 0.21).
4) Impact of IQI: Fig. 5 shows the average minimum rate
of users versus at for NB = 2, NU = 2, K = 2, NR = 100,
L = 2, N = 2, and Ni = 16. In this figure, we assume that
both transceivers suffer from IQI with the same IQI parameters
at all subbands, which means that ar,i = at,i = at for all i.
This figure shows that the average minimum rate decreases
with IQI level at transceivers even if it is considered in the
system designed and compensated by IGS. Furthermore, we
can observe that IQI can significantly decrease the average
minimum rate especially when we do not take it into account
in optimizing the parameters. Indeed, IQI may even vanish the
benefits of RIS in systems with highly imbalanced devices if
the employed schemes are not robust against IQI. Additionally,
we observe that IQI has a similar impact on the systems with
and without RIS; however, the benefits of IQI-aware schemes
are a bit higher in RIS-aided systems.
0
4
8
12
17
0
20
40
60
80
100
120
P (dB)
Minimum Rate (b/s/Hz)
S-RIS-ES
S-RIS-ESN
S-RIS-MS
S-RIS-Rand
RIS
No-RIS
Fig. 6: Average minimum rate versus P for the case without RIS
(No-RIS), randomly configured STAR-RIS (S-RIS-Rand), optimized
STAR-RIS (S-RIS) with different operational mode and feasibility
sets, and optimized regular RIS (RIS) for NB = NU = 2, K = 2,
NR = 100, L = 1, N = 1, and Ni = 16.
5) Comparison of different technologies for RIS: Here, we
assume L = 1 and N = 1, which means that we consider only
one RIS (either multi-sector BD-RIS, STAR or regular). To
show the impact of an omin-directional coverage, we consider
a scenario in which the users are located such that the regular
RIS can cover only half of the users, while the STAR-RIS
and/or multi-sector BD-RIS can cover all the users.
In Fig. 6, we show the average minimum rate versus P for
NB = NU = 2, K = 2, NR = 100, L = 1, N = 1, and
Ni = 16. This figure compares the performance of a regular
RIS and a STAR-RIS by considering different feasibility sets
as well as different operational modes for the STAR-RIS.
In this example, RIS (either regular or STAR) can highly
increase the average minimum rate. Additionally, a STAR-RIS
with even random coefficients can outperform the proposed
scheme for the regular RIS. Moreover, optimizing the STAR-
RIS coefficients can significantly increase the average mini-
mum rate. Furthermore, the ES scheme outperforms the MS
scheme; however, considering the higher computational and
implementation complexities of ES schemes, the MS scheme
performance is comparable with the ES scheme, especially for
TSN in which the phases of transmit and reflection coefficients
cannot be independently optimized.
Fig. 7 compares the performance of various RIS technolo-
gies from an average minimum rate point of view for K = 4,
NR = 32, L = 1, N = 1, Ni = 16, and different NB, NU.
To this end, we consider a four-sector BD-RIS (Ns = 4), a
STAR-RIS and a regular RIS. We assume that there is one
user in each sector of the multi-sector BD-RIS. Moreover, to
make the comparison fair, we assume that the total number of
the RIS elements is the the same and is equal to 32 in these
different technologies. It means that there are only NR/Ns
elements per each sector of the multi-sector BD-RIS, which is
equal to 8 in this example. Note that we assume that STAR-
RIS has NR (32 in this example) elements since STAR-RIS
10
0
4
8
12
17
0
5
10
15
20
25
30
P (dB)
Minimum Rate (b/s/Hz)
MSBD-RIS
RIS
S-RIS-MS
No-RIS
S-RIS-Rand
(a) NB = NU = 1.
0
4
8
12
17
0
10
20
30
40
50
P (dB)
Minimum Rate (b/s/Hz)
MSBD-RIS
RIS
S-RIS-MS
No-RIS
S-RIS-Rand
(b) NB = NU = 2.
Fig. 7: The average minimum rate versus P for the case without RIS
(No-RIS), randomly configured STAR-RIS (S-RIS-Rand), optimized
STAR-RIS with mode switching scheme (S-RIS-MS), optimized
regular RIS (RIS), and optimized multi-sector BD-RIS with mode
switching scheme (MSBD-RIS) for K = 4, NR = 32, L = 1,
N = 1, and Ni = 16.
0
4
8
12
17
0
200
400
600
800
1,000
P (dB)
Relative Performance Improvement (%)
MSBD-RIS over STAR-RIS (NRIS = 32)
MSBD-RIS over No-RIS (NRIS = 32)
MSBD-RIS over STAR-RIS (NRIS = 64)
MSBD-RIS over No-RIS (NRIS = 64)
Fig. 8: The average improvements by multi-sector BD-RIS versus P
for NB = NU = 1, K = 4, L = 1, N = 1, Ni = 16, and NR.
can employ a different technology comparing to multi-sector
BD-RIS. Furthermore, we assume that the antenna gain of each
multi-sector BD-RIS is computed based on the idealized model
in [33, Eq. (17)]. Here, we consider only the MS scheme since
it has lower computational and implementation complexities
while providing a competitive performance, as shown in Fig.
6.
In Fig. 7, all the considered RIS technologies can substan-
tially increase the average minimum rate with even a very
low NR. In this example, the number of RIS elements per
user per subband is 0.5, which is even lower than 1. We can
also observe that STAR-RIS can highly outperform a regular
RIS. Additionally, the multi-sector BD-RIS can provide a
significant gain even though the number of RIS elements per
sector is much lower than the number of STAR-RIS elements.
Fig. 8 shows the average improvements by multi-sector BD-
RIS over STAR-RIS and/or No-RIS versus P for NB = 1,
NU = 1, K = 4, L = 1, N = 1, Ni = 16, and NR. As can be
observed, multi-sector BD-RIS can substantially improve the
SE, especially in low SNR regimes. Additionally, the benefits
of employing RIS increase with NR.
0
4
8
12
17
0
50
100
150
200
P (dB)
Sum Rate (b/s/Hz)
RIS
No-RIS
RIS-Rand
(a) Average sum rate.
0
4
8
12
17
0
50
100
150
200
250
300
350
P (dB)
Relative Performance Improvement (%)
RIS over No-RIS
RIS over RIS-Rand
(b)
Relative
performance
im-
provement.
Fig. 9: The average sum rate versus P for NB = 2, NU = 2, K = 3,
L = 2, N = 2, Ni = 16, and NR = 50.
3
4
5
6
7
0.1
0.2
0.3
0.4
0.5
0.6
Pc (W)
Minimum EE (b/J/Hz)
RIS
No-RIS
RIS-Rand
(a) Average minimum EE.
3
4
5
6
7
20
40
60
80
100
120
140
Pc (W)
Relative Performance Improvement (%)
RIS over No-RIS
RIS over RIS-Rand
(b)
Relative
performance
im-
provement.
Fig. 10: The average minimum EE versus Pc for NB = NU = 2,
K = 2, NR = 50, L = 2, N = 2, and Ni = 16.
B. Maximization of the sum rate
Fig. 9 shows the average sum rate versus P for NB =
NU = 2, K = 3, L = 2, N = 2, Ni = 16, and NR = 50.
In this example, RIS can substantially increase the average
sum rate. Moreover, the benefits of RIS decrease with the
power budget, which supports the results in Fig. 2. Indeed,
in low SNR regimes, it is likely that the BS transmits over
a few number of subbands, which can increase the benefits
of optimizing RIS elements since the RIS coefficients cannot
be independently optimized at each subband. Additionally, as
shown in, e.g., [14], RIS can provide higher gains in low SNR
regimes in single-carrier systems since the received power of
cell-edge users is very low in systems without RIS when the
BS transmission power is low. Hence, we can expect that this
result hold in multi-carrier case, which highly increases the
benefits of RIS in low SNR regimes.
C. Maximization of the minimum EE
Fig. 10 shows the average minimum EE versus Pc for
NB = NU = 2, K = 2, NR = 50, L = 2, N = 2, and
Ni = 16. In this example, RIS can significantly improve
the average minimum EE of OFDM systems with different
Pc. However, the EE benefits of RIS are lower than its SE
benefits. Additionally, the RIS benefits slightly increase with
Pc. The reason is that, when Pc asymptotically increases,
the solutions of the maximization of the minimum rate and
11
3
4
5
6
7
0.2
0.4
0.6
Pc (W)
Global EE (b/J/Hz)
RIS
No-RIS
RIS-Rand
Fig. 11: The average global EE versus Pc for NB = 2, NU = 2,
K = 2, NR = 40, L = 2, N = 2, and Ni = 16.
2
8
16
24
32
20
40
60
80
100
120
140
Ni
Relative Performance Improvement (%)
RIS over No-RIS
RIS over RIS-Rand
Fig. 12: The average benefits of RIS from a global EE point of view
versus Ni for NB = 2, NU = 2, K = 2, NR = 40, L = 2, N = 2,
and Pc = 5 W.
the maximization of the minimum EE are identical since the
EE maximization for a large Pc is equivalent to the rate
maximization. Hence, as Pc increases, the EE benefits of RIS
become closer to the SE benefits, which makes the EE benefits
of RIS an increasing function of Pc.
D. Maximization of the global EE
Fig. 11 shows the average global EE versus Pc for NB = 2,
NU = 2, K = 2, NR = 40, L = 2, N = 2, and Ni = 16.
In this figure, RIS can significantly enhance the global EE
of the OFDM system for different Pc. Moreover, optimizing
RIS elements provides more that 32% improvements in the
particular example with a relatively low number of RIS
elements per user per subband (NR/(KNi) ≃ 1.25).
Fig. 12 shows the average benefits of RIS from a global
EE point of view versus Ni for NB = 2, NU = 2, K =
2, NR = 40, L = 2, N = 2, and Pc = 5 W. As can be
observed, the benefits of employing RIS (compared to No-
RIS) increase with Ni, while the benefits of optimizing RIS
elements decrease with Ni. It should be noted that the benefit
of optimizing RIS components in more than 25% for Ni = 32
in this particular example, which is still significant.
V. CONCLUSION
This paper proposed resource allocation algorithms with
both precoding and RIS element optimizations for a multi-
user MIMO RIS-aided OFDM BC with HWI to maximize the
SE and EE. We showed that RIS can substantially enhance
the SE and EE of the OFDM BC even when the number of
RIS elements is low. Moreover, we showed that the benefits
of optimizing RIS elements in low SNR regimes are much
higher than in high SNR regimes. Additionally, we showed
that the benefits of optimizing RIS elements are still significant
in OFDM systems even though the RIS coefficients cannot
be optimized at each subband independently. Furthermore, we
showed that IQI can significantly reduce the average minimum
rate even if it is compensated by employing IGS. Of course,
the performance degradation by neglecting IQI in the design
can be very significant and highly increases with the level of
the imbalance at the transceivers. We, moreover, showed that
multi-sector BD-RIS can significantly outperform a STAR-RIS
as well as a regular RIS.
REFERENCES
[1] M. Soleymani, I. Santamaria, A. Sezgin, and E. Jorswieck, “Maxi-
mization of minimum rate in MIMO OFDM RIS-assisted broadcast
channels,” IEEE Int. Workshop Comput. Adv. Multi-Sensor Adaptive
Process. (CAMSAP), 2023.
[2] M. D. Renzo et al., “Smart radio environments empowered by recon-
figurable AI meta-surfaces: An idea whose time has come,” EURASIP
J. Wireless Commun. and Netw., vol. 2019, no. 1, pp. 1–20, 2019.
[3] Q. Wu and R. Zhang, “Towards smart and reconfigurable environment:
Intelligent reflecting surface aided wireless network,” IEEE Commun.
Mag., vol. 58, no. 1, pp. 106–112, 2019.
[4] Q. Wu et al., “Intelligent reflecting surface aided wireless communica-
tions: A tutorial,” IEEE Trans. Commun., vol. 69, no. 5, pp. 3313–3351,
2021.
[5] M. Di Renzo et al., “Smart radio environments empowered by reconfig-
urable intelligent surfaces: How it works, state of research, and the road
ahead,” IEEE J. Sel. Areas Commun., vol. 38, no. 11, pp. 2450–2525,
2020.
[6] C.-X. Wang et al., “On the road to 6G: Visions, requirements, key
technologies and testbeds,” IEEE Commun. Surv. Tutor., pp. 1–1, 2023.
[7] T. Gong et al., “Holographic MIMO communications: Theoretical foun-
dations, enabling technologies, and future directions,” IEEE Commun.
Surv. Tutor., 2023.
[8] A. Zappone, M. Di Renzo, X. Xi, and M. Debbah, “On the optimal
number of reflecting elements for reconfigurable intelligent surfaces,”
IEEE Wireless Commun. Lett., vol. 10, no. 3, pp. 464–468, March 2021.
[9] C. Huang, A. Zappone, G. C. Alexandropoulos, M. Debbah, and
C. Yuen, “Reconfigurable intelligent surfaces for energy efficiency in
wireless communication,” IEEE Trans. Wireless Commun., vol. 18, no. 8,
pp. 4157–4170, 2019.
[10] Q. Wu and R. Zhang, “Intelligent reflecting surface enhanced wireless
network via joint active and passive beamforming,” IEEE Trans. Wireless
Commun., vol. 18, no. 11, pp. 5394–5409, 2019.
[11] M. Soleymani, I. Santamaria, and E. Jorswieck, “Rate splitting in
MIMO RIS-assisted systems with hardware impairments and improper
signaling,” IEEE Trans. Veh. Technol., vol. 72, no. 4, pp. 4580–4597,
April 2023.
[12] ——, “Rate region of MIMO RIS-assisted broadcast channels with rate
splitting and improper signaling,” in Proc. IEEE 26th Int. ITG Workshop
Smart Antennas & 13th Conf. Syst., Commun., and Coding (WSA &
SCC), 2023, pp. 1–6.
[13] C. Pan, H. Ren, K. Wang, W. Xu, M. Elkashlan, A. Nallanathan,
and L. Hanzo, “Multicell MIMO communications relying on intelligent
reflecting surfaces,” IEEE Trans. Wireless Commun., vol. 19, no. 8, pp.
5218–5233, 2020.
[14] M. Soleymani, I. Santamaria, and P. J. Schreier, “Improper signaling for
multicell MIMO RIS-assisted broadcast channels with I/Q imbalance,”
IEEE Trans. Green Commun. Netw., vol. 6, no. 2, pp. 723–738, 2022.
[15] M. Soleymani, I. Santamaria, E. Jorswieck, and S. Rezvani, “NOMA-
based improper signaling for multicell MISO RIS-assisted broadcast
channels,” IEEE Trans. Signal Process., vol. 71, pp. 963–978, 2023.
[16] M. Soleymani, I. Santamaria, and E. Jorswieck, “Energy-efficient rate
splitting for MIMO STAR-RIS-assisted broadcast channels with I/Q
imbalance,” Proc. IEEE Eu. Signal Process. Conf. (EUSIPCO), 2023.
12
[17] L. Zhang, Y. Wang, W. Tao, Z. Jia, T. Song, and C. Pan, “Intelligent
reflecting surface aided MIMO cognitive radio systems,” IEEE Trans.
Veh. Technol., vol. 69, no. 10, pp. 11 445–11 457, 2020.
[18] T. Jiang and W. Yu, “Interference nulling using reconfigurable intelligent
surface,” IEEE J. Sel. Areas Commun., vol. 40, no. 5, pp. 1392–1406,
2022.
[19] I. Santamaria et al., “Interference leakage minimization in RIS-assisted
MIMO interference channels,” in Proc. IEEE Int. Conf. on Acoust.,
Speech and Signal Processing (ICASSP), 2023.
[20] K. Weinberger, R.-J. Reifert, A. Sezgin, and E. Basar, “RIS-enhanced
resilience in cell-free MIMO,” in WSA & SCC 2023; 26th International
ITG Workshop on Smart Antennas and 13th Conference on Systems,
Communications, and Coding.
VDE, 2023, pp. 1–6.
[21] H. Esmaeili, A. A. Ahmad, A. Chaaban, and A. Sezgin, “Fairness
analysis in IRS assisted C-RAN with imperfect csi,” in IEEE Globecom
Workshops (GC Wkshps).
IEEE, 2022, pp. 1010–1015.
[22] K. Weinberger, A. A. Ahmad, A. Sezgin, and A. Zappone, “Synergistic
benefits in IRS-and RS-enabled C-RAN with energy-efficient cluster-
ing,” IEEE Trans. Wireless Commun., vol. 21, no. 10, pp. 8459–8475,
2022.
[23] M. Soleymani, I. Santamaria, E. Jorswieck, and B. Clerckx, “Optimiza-
tion of rate-splitting multiple access in beyond diagonal RIS-assisted
URLLC systems,” IEEE Trans. Wireless Commun., 2023.
[24] I. Santamaria, M. Soleymani, E. Jorswieck, and J. Guti´errez, “SNR
maximization in beyond diagonal RIS-assisted single and multiple
antenna links,” IEEE Signal Process. Lett., vol. 30, pp. 923–926, 2023.
[25] Y. Liu, X. Mu, J. Xu, R. Schober, Y. Hao, H. V. Poor, and L. Hanzo,
“STAR: Simultaneous transmission and reflection for 360 coverage by
intelligent surfaces,” IEEE Wireless Commun., vol. 28, no. 6, pp. 102–
109, 2021.
[26] H. Zhang and B. Di, “Intelligent omni-surfaces: Simultaneous refraction
and reflection for full-dimensional wireless communications,” IEEE
Commun. Surv. Tutor., vol. 24, no. 4, pp. 1997–2028, 2022.
[27] J. Xu, Y. Liu, X. Mu, R. Schober, and H. V. Poor, “STAR-RISs: A
correlated T&R phase-shift model and practical phase-shift configuration
strategies,” IEEE J. Sel. Topics Signal Process., pp. 1–1, 2022.
[28] X. Mu, Y. Liu, L. Guo, J. Lin, and R. Schober, “Simultaneously
transmitting and reflecting (STAR) RIS aided wireless communications,”
IEEE Trans. Wireless Commun., vol. 21, no. 5, pp. 3083–3098, 2022.
[29] J. Xu, Y. Liu, X. Mu, and O. A. Dobre, “STAR-RISs: Simultaneous
transmitting and reflecting reconfigurable intelligent surfaces,” IEEE
Commun. Lett., vol. 25, no. 9, pp. 3134–3138, 2021.
[30] M. Soleymani, I. Santamaria, and E. Jorswieck, “NOMA-based improper
signaling for MIMO STAR-RIS-assisted broadcast channels with hard-
ware impairments,” IEEE Global Commun. Conf. (GLOBECOM), 2023.
[31] ——, “Spectral and energy efficiency maximization of MISO STAR-
RIS-assisted URLLC systems,” IEEE Access, vol. 11, pp. 70 833–70 852,
2023.
[32] H. Li, S. Shen, M. Nerini, and B. Clerckx, “Reconfigurable intelligent
surfaces 2.0: Beyond diagonal phase shift matrices,” arXiv preprint
arXiv:2301.03288, 2023.
[33] H. Li, S. Shen, and B. Clerckx, “Beyond diagonal reconfigurable
intelligent surfaces: A multi-sector mode enabling highly directional
full-space wireless coverage,” IEEE J. Sel. Areas Commun., vol. 41,
no. 8, pp. 2446–2460, 2023.
[34] M. Soleymani, C. Lameiro, I. Santamaria, and P. J. Schreier, “Improper
signaling for SISO two-user interference channels with additive asym-
metric hardware distortion,” IEEE Trans. Commun., vol. 67, no. 12, pp.
8624–8638, 2019.
[35] M. Soleymani, I. Santamaria, and P. J. Schreier, “Improper Gaussian
signaling for the K-user MIMO interference channels with hardware
impairments,” IEEE Trans. Veh. Technol., vol. 69, no. 10, pp. 11 632–
11 645, 2020.
[36] S. Javed, O. Amin, S. S. Ikki, and M.-S. Alouini, “Multiple antenna
systems with hardware impairments: New performance limits,” IEEE
Trans. Veh. Technol., vol. 68, no. 2, pp. 1593–1606, 2019.
[37] M. Soleymani, I. Santamaria, B. Maham, and P. J. Schreier, “Rate region
of the K-user MIMO interference channel with imperfect transmitters,”
in Proc. IEEE Eu. Signal Process. Conf. (EUSIPCO), 2020, pp. 1–5.
[38] V. R. Cadambe, S. A. Jafar, and C. Wang, “Interference alignment with
asymmetric complex signaling–Settling the Høst-Madsen-Nosratinia
conjecture,” IEEE Trans. Inf. Theory, vol. 56, no. 9, pp. 4552–4565,
2010.
[39] M. Soleymani, C. Lameiro, I. Santamaria, and P. J. Schreier, “Robust
improper signaling for two-user SISO interference channels,” IEEE
Trans. Commun., vol. 67, no. 7, pp. 4709–4723, 2019.
[40] M. Soleymani, C. Lameiro, P. J. Schreier, and I. Santamaria, “Improper
signaling for OFDM underlay cognitive radio systems,” in 2018 IEEE
Statistical Signal Process. Workshop (SSP).
IEEE, 2018, pp. 722–726.
[41] J. Li, W. Wang, R. Jiang, X. Wang, Z. Fei, S. Huang, and X. Li,
“Piecewise-DRL: Joint beamforming optimization for RIS-assisted MU-
MISO communication system,” IEEE Internet Things J., 2023.
[42] M. Yue, L. Liu, and X. Yuan, “RIS-aided multiuser MIMO-OFDM with
linear precoding and iterative detection: Analysis and optimization,”
IEEE Trans. Wireless Commun., pp. 1–1, 2023.
[43] W. Wu, F. Yang, F. Zhou, Q. Wu, and R. Q. Hu, “Intelligent resource
allocation for IRS-enhanced OFDM communication systems: A hybrid
deep reinforcement learning approach,” IEEE Trans. Wireless Commun.,
vol. 22, no. 6, pp. 4028–4042, June 2023.
[44] D. Kompostiotis, D. Vordonis, and V. Paliouras, “Received power
maximization with practical phase-dependent amplitude response in
RIS-aided OFDM wireless communications,” in Proc. IEEE Int. Conf.
on Acoust., Speech and Signal Process. (ICASSP).
IEEE, 2023, pp.
1–5.
[45] H. Li, W. Cai, Y. Liu, M. Li, Q. Liu, and Q. Wu, “Intelligent reflecting
surface enhanced wideband MIMO-OFDM communications: From prac-
tical model to reflection optimization,” IEEE Trans. Commun., vol. 69,
no. 7, pp. 4807–4820, 2021.
[46] H. Dong, C. Ji, L. Zhou, J. Dai, and Z. Ye, “Sparse channel estimation
with surface clustering for IRS-assisted OFDM systems,” IEEE Trans.
Commun., 2022.
[47] C. Pradhan, A. Li, L. Song, J. Li, B. Vucetic, and Y. Li, “Reconfigurable
intelligent surface (RIS)-enhanced two-way OFDM communications,”
IEEE Trans. Veh. Technol., vol. 69, no. 12, pp. 16 270–16 275, Dec.
2020.
[48] C. Ji, H. Dong, W. Xu, and J. Dai, “Joint channel estimation and data
detection for IRS-assisted OFDM systems,” IEEE Trans. Veh. Technol.,
2023.
[49] R. Jiang, Z. Fei, S. Huang, X. Wang, Q. Wu, and S. Ren, “Bivariate
pilot optimization for compressed channel estimation in RIS-assisted
multiuser MISO-OFDM systems,” IEEE Trans. Veh. Technol., 2023.
[50] O. Abbas, Q.-U.-A. Nadeem, L. Markley, and A. Chaaban, “Unit cell
phase-frequency profile optimization in RIS-assisted wide-band OFDM
systems,” arXiv preprint arXiv:2308.13660, 2023.
[51] X. Peng, P. Wu, H. Tan, and M. Xia, “Optimization for IRS-assisted
MIMO-OFDM SWIPT system with nonlinear EH model,” IEEE Internet
Things J., vol. 9, no. 24, pp. 25 253–25 268, 2022.
[52] H. Wang, J. Fang, and H. Li, “Joint beamforming and channel re-
configuration for RIS-assisted millimeter wave massive MIMO-OFDM
systems,” IEEE Trans. Veh. Technol., vol. 72, no. 6, pp. 7627–7638, June
2023.
[53] N. Ginige, K. S. Manosha, N. Rajatheva, and M. Latva-aho, “Untrained
DNN for channel estimation of RIS-assisted multi-user OFDM system
with hardware impairments,” in 2021 IEEE 32nd Annual International
Symposium on Personal, Indoor and Mobile Radio Communications
(PIMRC).
IEEE, 2021, pp. 561–566.
[54] T. Zhang, S. Wang, Y. Zhuang, C. You, M. Wen, and Y.-C. Wu,
“Reconfigurable intelligent surface assisted OFDM relaying: Subcarrier
matching with balanced SNR,” IEEE Trans. Veh. Technol., vol. 72, no. 2,
pp. 2216–2230, Feb. 2023.
[55] S. Lin, B. Zheng, G. C. Alexandropoulos, M. Wen, F. Chen et al.,
“Adaptive transmission for reconfigurable intelligent surface-assisted
OFDM wireless communications,” IEEE J. Sel. Areas Commun., vol. 38,
no. 11, pp. 2653–2665, 2020.
[56] Z. He, H. Shen, W. Xu, and C. Zhao, “Low-cost passive beamforming
for RIS-aided wideband OFDM systems,” IEEE Wireless Commun. Lett.,
vol. 11, no. 2, pp. 318–322, Feb. 2022.
[57] E. Bj¨ornson, “Optimizing a binary intelligent reflecting surface for
OFDM communications under mutual coupling,” in WSA 2021; 25th
International ITG Workshop on Smart Antennas.
VDE, 2021, pp. 1–6.
[58] B. Zheng and R. Zhang, “Intelligent reflecting surface-enhanced OFDM:
Channel estimation and reflection optimization,” IEEE Wireless Com-
mun. Lett., vol. 9, no. 4, pp. 518–522, April 2020.
[59] Y. Yang, B. Zheng, S. Zhang, and R. Zhang, “Intelligent reflecting
surface meets OFDM: Protocol design and rate maximization,” IEEE
Trans. on Commun., vol. 68, no. 7, pp. 4522–4535, 2020.
[60] Z. He, H. Shen, W. Xu, and C. Zhao, “Low-cost passive beamforming
for RIS-aided wideband OFDM systems,” IEEE Wireless Commun. Lett.,
vol. 11, no. 2, pp. 318–322, 2022.
[61] J. An, Q. Wu, and C. Yuen, “Scalable channel estimation and reflection
optimization for reconfigurable intelligent surface-enhanced OFDM sys-
tems,” IEEE Wireless Commun. Lett., vol. 11, no. 4, pp. 796–800, April
2022.
13
[62] X. Xu, S. Zhang, F. Gao, and J. Wang, “Sparse bayesian learning based
channel extrapolation for RIS assisted MIMO-OFDM,” IEEE Trans.
Commun., vol. 70, no. 8, pp. 5498–5513, 2022.
[63] Y. Huang, P. Yang, B. Zhang, Z. Liu, and M. Xiao, “A novel maximum
distance separable coded OFDM-RIS for 6G wireless communications,”
IEEE Wireless Commun. Lett., vol. 12, no. 5, pp. 927–931, May 2023.
[64] W. Yang, H. Li, M. Li, Y. Liu, and Q. Liu, “Channel estimation for
practical IRS-assisted OFDM systems,” in IEEE Wireless Commun. and
Netw. Conf. Wkshps (WCNCW), 2021, pp. 1–6.
[65] M. He, W. Xu, H. Shen, G. Xie, C. Zhao, and M. Di Renzo, “Cooper-
ative multi-RIS communications for wideband mmwave MISO-OFDM
systems,” IEEE Wireless Commun. Lett., vol. 10, no. 11, pp. 2360–2364,
2021.
[66] W. Yan, X. Yuan, and X. Cao, “Frequency reflection modulation for
reconfigurable intelligent surface aided OFDM systems,” IEEE Trans.
Wireless Commun., vol. 21, no. 11, pp. 9381–9393, 2022.
[67] H. Li, S. Shen, and B. Clerckx, “Synergizing beyond diagonal recon-
figurable intelligent surface and rate-splitting multiple access,” arXiv
preprint arXiv:2303.06912, 2023.
[68] Z. Sha and Z. Wang, “Channel estimation and equalization for terahertz
receiver with RF impairments,” IEEE J. Sel. Areas Commun., vol. 39,
no. 6, pp. 1621–1635, 2021.
[69] A. Zappone and E. Jorswieck, “Energy efficiency in wireless networks
via fractional programming theory,” Found Trends in Commun. Inf.
Theory, vol. 11, no. 3-4, pp. 185–396, 2015.
"
"This research focuses on secure, reliable, all-to-all multi-hop broadcast communication for UAV swarms during mission execution and collision avoidance. The approach integrates fast and efficient mesh communication with secure broadcast techniques, utilizing flooding and message relaying.","UAV swarms require secure and efficient communication for mission execution and collision avoidance. Mesh protocols like B.A.T.M.A.N and Babel provide multi-hop point-to-point communication, but they may be inefficient for multi-hop broadcasts. This paper proposes an improved approach for multi-hop telemetry broadcasts.",Relevant studies on security and cyber-attacks in UAV swarms are reviewed. The authors discuss the limitations of existing approaches and the need for dynamic key management schemes in securing mesh protocols.nannannan,"The proposed multi-hop telemetry broadcast approach incorporates all-to-all broadcasts using flooding and message relaying. It employs ECDH group key exchange protocol to establish a shared secret, ensuring secure communication channels within the mesh network. AES-128 encryption is utilized to protect sensitive information, while HMAC-256 with a timestamp ensures message integrity and authenticity.nan",N/A,"The proposed approach provides a fast, efficient, and secure means for multi-hop telemetry broadcasts within UAV swarms. The combination of flooding and message relaying, along with ECDH key exchange, AES-128 encryption, and HMAC-256, enables secure and reliable communication among UAV swarm members.",Secure Multi-hop Telemetry Broadcasts for UAV Swarm Communication,"Randolf Rotta, Pavlo Mykytyn","arXiv:2401.11915v1  [cs.CR]  22 Jan 2024
Secure Multi-hop Telemetry Broadcasts
for UAV Swarm Communication
Randolf Rotta1, Pavlo Mykytyn1,2
1Brandenburg University of Technology, Cottbus, Germany
2IHP - Leibniz Institute for High-Performance Microelectronics, Frankfurt (Oder), Germany
Abstract—Unmanned Aerial Vehicles (UAVs) are evolving as
adaptable platforms for a wide range of applications such as
precise inspections, emergency response, and remote sensing.
Autonomous UAV swarms require efﬁcient and stable commu-
nication during deployment for a successful mission execution.
For instance, the periodic exchange of telemetry data between
all swarm members provides the foundation for formation ﬂight
and collision avoidance. However, due to the mobility of the
vehicles and instability of wireless transmissions, maintaining a
secure and reliable all-to-all communication remains challenging.
This paper investigates encrypted and authenticated multi-hop
broadcast communication based on the transmission of custom
IEEE 802.11 Wi-Fi data frames.
Index Terms—Unmanned Aerial Vehicles, Multi-hop Networks,
Vehicular Networks, Swarm Flight
I. INTRODUCTION
To share data with a ground control station (GCS), other
UAVs in a swarm, or other centralized infrastructure, UAVs
rely on wireless communication. UAVs communicate with the
base station to receive commands and transmit sensor data.
Such point-to-point communication between a single vehicle
and a GCS is straightforward.
However, when considering a swarm with multiple vehicles
and ground stations, enabling efﬁcient and secure communica-
tion becomes increasingly challenging. The communication se-
curity is critical because the swarm relies on it for cooperation,
formation forming, or collision avoidance. However, wireless
communications is particularly vulnerable to intentional and
unintentional interference, jamming, interception, eavesdrop-
ping, and enables cyber-attacks targeting data privacy and
integrity [1]. Furthermore, the dynamic nature of UAV net-
works with nodes continuously moving and re-establishing the
connection between one another makes dependable communi-
cation links much more difﬁcult to maintain. The dynamic
and mobile nature of UAVs, combined with the limited range
of wireless communication, necessitates the use of multi-hop
communication techniques.
In this paper we focus on security and reliability aspects
of all-to-all multi-hop broadcast communication between UAV
swarm members and the GCS. Low-cost microprocessors with
a few megabytes of RAM and integrated Wi-Fi radio open up
the opportunity for new experiments with mesh protocols. One
This research has been funded partly by the Federal Ministry of Education
and Research of Germany under grant numbers 16ES1131 and 16ES1128K.
The authors are responsible for the content of this publication.
example is the Espressif ESP32 series, which allows broad-
casting and receiving custom Wi-Fi frames without the need
to associate to any Access Point. Securing these transmissions
is left to be the protocol implementation.
II. RELATED WORK
Mesh communication protocols such as B.A.T.M.A.N and
Babel are often considered as a basis for UAV swarms [2].
Their design assumes that security mechanisms are handled
on higher layers. Because of this assumption, mesh protocol
layers are open to a variety of cyber-attacks. The authors in
[3] mention that in multi-hop UAV swarm communication,
active RF jamming and eavesdropping are among the most
common cyber-attacks. Additionally, UAV swarms using the
Robot Operating System (ROS 1) are vulnerable to a variety of
cyber-attacks. ROS 2, on the other hand, has eliminated some
of the issues mentioned above by introducing authentication
and encryption based on a public key infrastructure.
Babel [4] proposes two optional mechanisms based on
shared keys or Datagram Transport Layer Security (DTLS).
SecBATMAN [5] proposes a security extension but lacks a
dynamic key exchange. Studies like [6] argue, that dynamic
key management schemes are necessary in order to secure
these mesh protocols. Beyond mesh protocols that try to
optimize forwarding routes through the network, Synchronous
Flooding (SF) provides a much simpler alternative, c.f. [7].
These protocols synchronize the forwarding of broadcasts such
that a n-hop broadcast needs just n consecutive time slots
independent of the actual number of participating nodes. Un-
fortunately, existing implementations focus on Bluetooth Low
Energy and IEEE 802.15.4 radios, which limits the available
throughput. To our best knowledge no implementation based
on IEEE 802.11 exists.
III. MULTI-HOP TELEMETRY BROADCASTS
Point-to-point communication refers to a direct communi-
cation link established between two devices, such as an indi-
vidual UAV and the GCS. Multi-point communication in UAV
swarms connects a single GCS to multiple UAVs and multi-
point meshes add multi-hop routing between GCS and UAVs.
While this link to the ground provides essential connectivity,
it is insufﬁcient to facilitate seamless data exchange within the
swarm. Mesh protocols like B.A.T.M.A.N and Babel focus on
providing multi-hop point-to-point communication between ar-
bitrary network nodes. This would be perfect, for example, for
two vehicles that cooperate on a task. However, cooperation
between multiple vehicles requires to broadcast at least their
position telemetry frequently enough to all the other UAVs.
Flooding the mesh network with each vehicle’s telemetry
would work, but is inefﬁcient because it re-broadcasts more
often than necessary. This reduces the available throughput
and increases the risk of colliding transmissions. Therefore,
efﬁcient multi-hop broadcast mechanisms are needed. We
propose to revisit the ﬂooding of route discovery messages in
proactive mesh protocols like B.A.T.M.A.N and Babel. They
use ﬂooding to learn the best path to each possible destination
node. Only the best next hop towards each destination node is
stored, which is repeated at the next node until the destination
node is reached. This approach provides an advantage of
adjusting the path, while messages are already traveling. The
next hops essentially form a collection tree toward each des-
tination node. Our approach aims at inverting these trees into
broadcast trees originating from that node. Thus, together with
each outgoing message the node broadcasts its next hop table.
The receiver of that message can then ﬁgure out its position
in each broadcast tree and avoid unnecessary transmissions.
This information allows each node to selectively forward
pending messages from the queue based on the neighbor’s
needs. Typically, telemetry messages in the MAVLink protocol
are much smaller than 256 bytes. Thus, during forwarding,
multiple messages from different sources can be aggregated
into a single IEEE 802.11 frame. A similar pattern can be
achieved with ROS2-based communication. The underlying
data distribution service allows to conﬁgure forwarding of
published messages, for example, to multicast IP addresses.
It can also be conﬁgured to receive such multicast messages.
However, the Real-time Publish-Subscribe Protocol (RTPS)
that is used between the ROS2 nodes is much more complex
than MAVLink. Instead of maintaining a separate broadcast
tree rooted at each node, a single spanning tree could be
sufﬁcient [8]. A broadcast message is re-broadcasted only
in case if it was received via a neighbor in the spanning
tree, it is not a leaf node, and the message was not re-
broadcasted before. This approach combined with the loop
avoidance techniques of the Babel protocol [4] should generate
even more effective results.
IV. SECURITY IN UAV COMMUNICATION
UAV swarm communication is susceptible to RF jamming,
Man-in-the-Middle (MITM), Eavesdropping, Trafﬁc Analysis
(TA), and Replay attacks. Eavesdropping and TA are passive
cyber-attacks and require additional hardware in order to detect
them. However, by integrating data encryption mechanisms,
the effects of these cyber-attacks can be mitigated. The effects
of a MITM attack, can be mitigated by integrating data
authentication mechanisms. The effects of a Replay attack
can be mitigated by integrating a timestamp to deem old and
repeated messages invalid. Our approach is based on providing
a secure and authenticated communication for all of the UAV
swarm members. As a key exchange protocol we plan to use
the Elliptic-curve Difﬁe–Hellman (ECDH) adapted to be used
for multiple parties. Once all of the parties have generated their
private and public keys and calculated a common shared secret
(session key), we will authenticate the message using Hash-
based Message Authentication Code (HMAC) based on Secure
Hashing Algorithm (SHA-2) by generating a hash of the
message together with the session key and appending the ﬁrst
16 bytes of the hash to the end of the message, thus providing
authentication of the contents of the broadcasted message. To
encrypt the broadcasted message we will utilize the Advanced
Encryption Standard (AES) with the 128-bit key. Each sent
message will also include a timestamp to protect against
Replay attacks and deem old messages invalid. By using this
approach, the UAV swarm members can establish a common
shared secret through ECDH key exchange, enabling secure
and authenticated communication within the UAV swarm.
V. CONCLUSIONS
This paper has presented an idea for multi-hop telemetry
broadcasts communication within a UAV swarm, speciﬁcally
designed to enable fast, efﬁcient, and secure mesh communi-
cation for mission execution and collision avoidance purposes.
The proposed approach incorporates all-to-all broadcasts using
ﬂooding and message relaying. By leveraging the ECDH group
key exchange protocol, drones establish a shared secret, ensur-
ing secure communication channels within the mesh network.
The use of AES-128 encryption guarantees the conﬁdentiality
of telemetry broadcasts, protecting sensitive information from
unauthorized access. To ensure message integrity and authen-
ticity, each broadcast message includes a HMAC-256 signature
with a timestamp. This signature provides a reliable means
to verify the origin and integrity of the message, preventing
tampering or spooﬁng attempts.
REFERENCES
[1] K.-Y. Tsao, T. Girdler, and V. G. Vassilakis, “A survey of cyber security
threats and solutions for uav communications and ﬂying ad-hoc net-
works,” Ad Hoc Networks, vol. 133, p. 102894, 2022. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S1570870522000853
[2] A.
Guillen-Perez,
A.-M.
Montoya,
J.-C.
Sanchez-Aarnoutse,
and
M.-D.
Cano,
“A
comparative
performance
evaluation
of
routing
protocols
for
ﬂying
ad-hoc
networks
in
real
conditions,”
Applied
Sciences,
vol.
11,
no.
10,
2021.
[Online].
Available:
https://www.mdpi.com/2076-3417/11/10/4363
[3] M. A. Lopez, M. Baddeley, W. T. Lunardi, A. Pandey, and J.-P. Giacalone,
“Towards secure wireless mesh networks for uav swarm connectivity:
Current threats, research, and opportunities,” in 2021 17th International
Conference on Distributed Computing in Sensor Systems (DCOSS), 2021,
pp. 319–326.
[4] J. Chroboczek and D. Schinazi, “The Babel Routing Protocol,” RFC 8966,
Jan. 2021. [Online]. Available: https://www.rfc-editor.org/info/rfc8966
[5] P. Racz, A. Lunn, and J. Paatero, “A security extension for ad-hoc routing
protocols,” in 7th Workshop on Wireless and Mobile Ad-Hoc Networks
(WMAN 2013), 2013.
[6] M. Sbeiti and C. Wietfeld, “One stone two birds: On the security and
routing in wireless mesh networks,” in 2014 IEEE Wireless Communica-
tions and Networking Conference (WCNC), 2014, pp. 2486–2491.
[7] M. Zimmerling, L. Mottola, and S. Santini, “Synchronous transmissions
in low-power wireless: A survey of communication protocols and
network services,” ACM Comput. Surv., vol. 53, no. 6, dec 2020.
[Online]. Available: https://doi.org/10.1145/3410159
[8] A. J¨uttner and ´A. Magi, “Tree based broadcast in ad hoc networks,”
Mobile Networks and Applications, vol. 10, pp. 753–762, 2005.
"
"Advanced techniques for 3D object detection have been developed to ensure accurate environment perception and enable autonomous driving. The crucial information comes from LiDAR point clouds that manifest as a sparse and disorganized representation of three-dimensional (3D) data. LiDAR's inherent feature affords it a distinct advantage in 3D object detection. This paper addresses the challenges involved in enhancing LiDAR-based 3D detection networks. A Dynamic Feature Fusion Module (DFFM) is devised to extend the receptive field of the 3D convolutional kernel. Simultaneously, experiments confirm that DFFM and FSM not only enhance current benchmarks, particularly in small target detection, but also accelerate network performance. The modules exhibit effective complementarity.","A comprehensive autonomous driving system confronts significant challenges, wherein the perception system plays a crucial role in extracting raw sensor data. This data is transmitted to regulation and control modules for further processing. The perception system aims to collect semantic and geometric data from the environment, such as vehicles and lane lines. Consequently, as the core component of the perception system, 3D object detection serves a critical role and forms the foundation for autonomous driving. Point clouds obtained from LiDAR exhibit distinctive characteristics. In contrast to images arranged in a regular grid on a two-dimensional plane, point clouds manifest as a sparse and disorganized representation of three-dimensional (3D) data. These point clouds provide precise depth information unaffected by external environmental conditions, such as variations in lighting, which can impact camera-based systems.","nanRecent research endeavors have identified that in 2D convolution-based networks, smaller convolution kernels are commonly employed for feature extraction. Consequently, attempts have been made to augment the receptive field by increasing the size of the convolution kernel, thereby enhancing the network's overall performance. Nevertheless, 3D point clouds are not a simple extension of 2D images. The challenge arises as the enlargement of the convolution kernel size results in the cubic growth of computational and parametric quantities for 3D convolution. Our network's superior performance over existing benchmarks showcases improvements in detection on the KITTI dataset, particularly for small objects.nannan","We introduce the Dynamic Feature Fusion Module (DFFM) to address the computational challenges associated with an expanding receptive field, enhancing overall model optimization. Furthermore, a plug-and-play Feature Selection Module (FSM) is proposed to eliminate non-essential features, enabling the detector to concentrate on fitting crucial features. To validate the effectiveness of the method, experiments are conducted on the KITTI dataset demonstrating that our approach significantly improves performance while maintaining a faster detection speed.nan","Tab. II summarizes the outcomes of the 3D object detection experiments on the KITTI validation set. The evaluation criterion is AP|R40. The improved results are highlighted in bold. The combination of DFFM and FSM enhances the network performance, as shown in Tab. V. This not only validates the effectiveness of DFFM but also underscores its complementary role with FSM.","This study introduces two pivotal modules: the Dynamic Feature Fusion Module (DFFM) and the Feature Selection Module (FSM). These modules are dedicated to achieving large receptive field and extracting important features, respectively, presenting a novel approach to large receptive field and important feature extraction strategies.",Large receptive field strategy and important feature extraction strategy in 3D object detection,"Leichao Cui, Xiuxian Li, Min Meng","Large receptive field strategy and important feature extraction strategy
in 3D object detection*
Leichao Cui1, Xiuxian Li1, and Min Meng1
Abstract— The enhancement of 3D object detection is pivotal
for precise environmental perception and improved task execu-
tion capabilities in autonomous driving. LiDAR point clouds,
offering accurate depth information, serve as a crucial infor-
mation for this purpose. Our study focuses on key challenges
in 3D target detection. To tackle the challenge of expanding the
receptive field of a 3D convolutional kernel, we introduce the
Dynamic Feature Fusion Module (DFFM). This module achieves
adaptive expansion of the 3D convolutional kernel’s receptive
field, balancing the expansion with acceptable computational
loads. This innovation reduces operations, expands the receptive
field, and allows the model to dynamically adjust to different
object requirements. Simultaneously, we identify redundant
information in 3D features. Employing the Feature Selection
Module (FSM) quantitatively evaluates and eliminates non-
important features, achieving the separation of output box fit-
ting and feature extraction. This innovation enables the detector
to focus on critical features, resulting in model compression,
reduced computational burden, and minimized candidate frame
interference. Extensive experiments confirm that both DFFM
and FSM not only enhance current benchmarks, particularly in
small target detection, but also accelerate network performance.
Importantly, these modules exhibit effective complementarity.
I. INTRODUCTION
Developing a comprehensive autonomous driving system
that can be applied to diverse scenarios faces significant
challenges. The perception system plays a crucial role in
extracting and processing raw sensor data, serving as the
initial module for information acquisition and processing.
This information is then transmitted to regulation and con-
trol modules for further processing. The main goal of the
perception system is to collect semantic and geometric data
from the environment, such as vehicles and lane lines.
Therefore, 3D object detection, at the heart of the perception
system, plays a critical role and serves as the foundation for
autonomous driving.
Point clouds obtained through LiDAR exhibit distinctive
characteristics. In contrast to images arranged in a regular
grid on a two-dimensional plane, point clouds manifest as a
sparse and disorganized representation of three-dimensional
(3D) data. Moreover, these point clouds provide precise
depth information unaffected by external environmental con-
ditions, such as variations in lighting, which can impact
*This work was supported by the National Natural Science Foundation
of China under Grant 62003243 and Grant 62103305, and the Shanghai
Municipal Science and Technology Major Project, No. 2021SHZDZX0100.
1Department of Control Science and Engineering, College of Electronics
and Information Engineering, and the Shanghai Research Institute for
Intelligent Autonomous Systems, Tongji University, Shanghai 201800,
China
2130715@tongji.edu.cn, xxli@ieee.org,
mengmin@tongji.edu.cn
2D 
Convolution Kernel 
Image
Image
Larger 2D
Convolution Kernel 
Fig. 1.
The sizeable receptive field helps fully understand the object’s
overall structure and the surrounding environment’s contextual information.
camera-based systems. Consequently, point clouds can faith-
fully portray the structure and contours of 3D objects. This
inherent feature affords LiDAR a distinct advantage in 3D
object detection.
Hence, it is essential to improve the performance of
LiDAR-based 3D detection networks in complex scenes,
ensuring their robust detection capabilities and processing
efficiency in diverse and challenging environments. Despite
major advances in current LiDAR-based 3D object detection
methods, specific challenges remain and require solutions.
One concern pertains to extending the receptive field of the
3D convolutional kernel. Illustrated in Fig. 1, a substantial re-
ceptive field proves advantageous by encompassing a broader
range of input data. This, in turn, facilitates the assimilation
of more comprehensive global information and contextual
features, allowing for a more profound modeling of features
at an elevated level. Recent research endeavors [1], [2] have
identified that in 2D convolution-based networks, smaller
convolution kernels are commonly employed for feature ex-
traction. Consequently, attempts have been made to augment
the receptive field by increasing the size of the convolution
kernel, thereby enhancing the network’s overall performance.
Nevertheless, 3D point clouds are not a simple extension
of 2D images. The challenge arises as the enlargement of the
convolution kernel size results in the cubic growth of com-
putational and parametric quantities for 3D convolution. For
instance, when transitioning from a 3×3×3 to a 9×9×9 3D
convolution kernel, the increase in parameter count and com-
putational load is much greater than that of a 2D convolution
kernel operating under the same conditions. Simultaneously,
the sparsity inherent in point cloud data poses a limitation, as
the data volume and processing capacity struggle to handle
the rapidly increasing number of parameters. Consequently,
the optimization of the network becomes imperative.
Therefore, we introduce the Dynamic Feature Fusion
Module (DFFM) to broaden the receptive field of the 3D
convolutional kernel. DFFM dynamically extends the recep-
arXiv:2401.11913v1  [cs.CV]  22 Jan 2024
Point Clouds
Plane Features
Voxels
Random 
Sampling 
FPS
Graph  
Convs.
MLP  
Pooling 
Pooling
Compress
3D Convs.
Data Processing
Feature Extraction
RPN
PRN
...
Proposals
RoIs' Feats.
Detector
Features
Refinements
2D Convs.
Voxelization
Dimensional
Compression
Sampled Point Clouds 
3D Feats.
2D Feats.
RPN
Features
2D Feats.
Fig. 2.
The visual architecture of the point cloud detection network consists of three key components: data processing, feature extraction, and detector.
(a) Point clouds undergo various data processing methods for transformation. (b) Feature extraction employs diverse operations. (c) Extracted features are
input into the detection network for object detection.
tive field based on actual demand, ensuring that parameters
remain within an acceptable range.
We additionally note that the features supplied to the 3D
detector often include extraneous information. This causes
the detector to execute numerous invalid computations, ineffi-
ciently utilizing valuable computational resources. Moreover,
this feature disparity may lead to the hiding or masking of
information with genuine value, posing challenges for the
detector in learning them. At the same time, this can also
trigger mutual interference between multiple candidate boxes
because the Non-Maximum Suppression (NMS) operation in
the post-processing stage is exclusive, which eliminates the
detection results with high overlap, thus possibly removing
some valid targets and reducing the overall performance of
the detector.
Thus we devise a plug-and-play Feature Selection Module
(FSM) to effectively filter features, enabling the detector to
concentrate on high-quality feature regression. The module
decouples the output box fitting and feature selection, seam-
lessly integrating into existing network architectures without
necessitating intricate modifications to the detector’s design.
To validate the effectiveness of the above method, we
improve on the existing 3D object detection networks [3],
[4]. Experiments on the KITTI dataset demonstrate that our
approach significantly improves performance while maintain-
ing a faster detection speed.
Our contributions can be summarized as follows:
• Introduction of the DFFM to address the computational
challenges associated with an expanding receptive field,
enhancing overall model optimization.
• Proposal of a plug-and-play FSM designed to eliminate
non-essential features. This enables the detector to con-
centrate on fitting crucial features.
• Demonstration of our network’s superior performance
over existing benchmarks, showcasing improvements in
detection on the KITTI dataset, particularly for small
objects.
II. RELATED WORKS
A. LiDAR-based 3D Object Detection
Visual architecture. The visual-like architecture of the
point cloud detection network, as depicted in Fig. 2, is struc-
tured into three primary components: data processing, feature
extraction, and the detector. Initially, the data processing
phase transforms point cloud data into various representa-
tions. Following this, the feature extraction network and 3D
object detector are employed to extract features and predict
3D bounding boxes on the processed data.
Specifically, the initial point clouds undergo processing
through farthest point sampling [5], random downsampling
[6], F-FPS [7], and coordinate refinement [8] to yield the
processed point clouds. Voxelization [4], [9]–[13] is applied
to obtain voxels, while compressive mapping [14], [15]
generates 2D feature maps. Subsequently, the processed
voxels, point clouds, and 2D feature maps undergo 3D sparse
convolution [4], graph convolution [16] or Set Abstraction
[5], and 2D convolution [17], [18], respectively. Finally, the
output is derived through a detector based on point clouds
[7], [10], [19] or feature maps [4], [12], [20].
Transformer architecture. In contrast to traditional visual
architecture, the Transformer architecture places a greater
emphasis on end-to-end unified forms of data processing
using the attention mechanism. PT [21] and PCT [22] have
devised attention modules tailored for point cloud structures,
employing the self-attention mechanism within the local
point set. Conversely, Voxel Transformers [23] leverage the
attention mechanism instead of the convolutional feature
extraction backbone to address the sparse nature of voxels.
Additionally, a dilated attention mechanism is proposed to
expedite computation. SWFormer [24] prioritizes voxel fea-
ture diffusion and multi-scale fusion through the utilization
of the attention mechanism.
B. Large Receptive Field Strategy
The LargeKernel3D [25] introduces a strategy known as
local convolution unit weight sharing. In dealing with larger
3D convolution kernels, the convolution units k ∈ Klocal
located at local positions share identical weights. Through
Decouple
C
Avg  
Pooling
Max
Pooling
C
Conv. Network 
+
C
Concat
Sigmoid
+
Add
Mul
Output 
Input 
Larger 
Conv Kernel 
Fig. 3.
DFFM Components: (a) Convolutional decoupling module decomposes large receptive field kernels into smaller ones. (b) Adaptive perception
module dynamically adjusts weights of intermediate features across various receptive fields.
weight sharing across spatial locations, this approach reduces
the original large kernel convolution size from 7×7×7 to
3×3×3.
Larger kernels cover more extensive processing areas,
and accelerating large kernel convolution operations involves
multiplexing overlapping regions. LinK [26] addresses this
by introducing a linear kernel. This method maximizes the
utilization of global coordinates and feature representations
of overlapping blocks, ensuring efficient feature reuse in
overlapping regions.
C. Important Feature Extraction Strategy
CasA [27] introduces a cascading attention detector. In
contrast to most detectors using a single network for candi-
date region generation, CasA employs a progressive approach
by utilizing a series of sub-networks to extract essential
features for prediction boxes. Simultaneously, candidate
boxes undergo total refinement by integrating crucial features
from different stages, utilizing cascading attention to achieve
higher-quality prediction results.
A series of studies [28], [29], exemplified by DETR3D
[30], employs a decoding detector based on the Trans-
former architecture. This approach initially defines a set
of 3D object queries, each corresponding to the essential
features of an output box. These queries engage with the
feature map through the attention mechanism to extract
feature information corresponding to candidate boxes. The
continuous optimization of queries facilitates obtaining their
corresponding crucial features.
III. METHOD
Firstly, Sec. III-A details the DFFM, specifically designed
to accommodate a large receptive field. Subsequently, Sec.
III-B introduces the FSM, proficient in quantitative feature
filtering. These two modules offer crucial components for
enhancing the performance of 3D object detection from
distinct perspectives. Finally, these modules are incorporated
into the enhanced network discussed in Sec. III-C.
A. Dynamic Feature Fusion Module
LargeKernel3D [25] effectively reduces the parameter
number but falls short in reducing the total atomic operations,
which remain tied to the cubic growth of the kernel size.
Additionally, its stringent assumption of weight sharing
limits the flexibility of the convolution operation. In contrast,
LinK [26] maps features and coordinates to the range [−1, 1],
constraining the variation range. The predefined positional
deviation further hampers flexible weight learning compared
to the conventional convolution.
Most crucially, the above method is statically fixed for
the extension range of the perceptual field, lacking dynamic
adjustment based on the actual required field of view. For
instance, a convolutional kernel of size 7×7×7 might cover
a vehicle, but it could introduce interfering information when
applied to pedestrian detection. This static, fixed sensing field
range may not adapt adequately to different scenarios.
Overall Architecture. In order to overcome the defects of
the above methods, we propose the DFFM, depicted in Fig.
3. It consists of the convolutional decoupling and adaptive
perception modules. This strategy showcases the ability
to leverage intermediate features as a bridge, dynamically
expanding the sensing field based on actual requirements
while upholding acceptable resource consumption.
Convolutional Decoupling Module. The receptive field
RFi of the convolutional kernel of the ith layer is calculated
as follows:
RF1 = k1,
RFi = RFi−1 + disi(ki − 1).
(1)
Here, di represents the dilation rate of the convolution, and
si is the step size of the convolution-both typically set to 1 in
DFFM. Additionally, ki denotes the size of the convolution
kernel. This formulation enables the decomposition of a large
convolution kernel into a collection of convolution kernels
C = {C1, C2, . . . , Ci}-both possessing equivalent receptive
fields. Tab. I shows that this decomposition significantly
reduces Floating Point Operations (FLOPs) during training.
TABLE I
FLOPS CORRESPONDING TO DIFFERENT STRUCTURES.
RF
sequence (ki, di)
FLOPs (M)
5
(5, 1)
91.20
(3, 1) → (3, 1)
79.90
The input features are denoted by X, and the intermediate
...
...
Top 50%
Importance Weights
Focal 
Loss
Training
Voxels' Weights
Real-object Boxes
Background Label
Foreground Label
Empty Voxel
Output '
Prediction Network
Corr. 
Location
Multiply
Input 
Training 
Methods
Voxels
Fig. 4.
The general structure of the FSM. (a) Importance weights are predicted for each voxel using the network. (b) The top 50% importance weight
voxels are retained, discarding the rest. (c) Retained voxel features are multiplied by their weights to create output features. (d) The detection network
categorizes samples as positive or negative based on truth box inclusion and adjusts the training accordingly.
features of the ith layer are expressed as:
F1 = C1(X), · · · , Fi = Ci(Fi−1).
(2)
Next, we will elaborate on how the adaptive perception mod-
ule utilizes these intermediate features to construct flexible
receptive fields that can be effectively adapted to different
scenarios.
Adaptive Perception Module. The adjustment of inter-
mediate features at various receptive field sizes is crucial for
the dynamic adaptation of the model’s receptive field. Firstly,
we concatenate features at different receptive field sizes:
F = [F1; F2; . . . ; Fi].
(3)
Then, channel average pooling Pavg and maximum pooling
Pmax are applied to these features, facilitating the effective
fusion of the same positional weights in different features:
Wavg = Pavg(F), Wmax = Pmax(F).
(4)
Following this, we employ Eq. 5 to derive the set of weights
W = [W1; W2; . . . ; Wi] corresponding to different receptive
fields, with i denoting the number of feature maps:
W = σ(C2→i([Wavg; Wmax])).
(5)
In this context, C2→i(·) signifies the convolution operation,
transitioning from 2 dimensions to i dimensions, while σ(·)
represents the weight normalization through the Sigmoid
activation function. Ultimately, we apply the weights Wn to
the intermediate features Fn and subject them to convolution
Cout, yielding the output Y .
Y = Cout(
i
X
n=1
(Wn · Fn)) + X.
(6)
B. Feature Selection Module
We observe that the previous research outlined in Sec. II-C
primarily emphasizes enhanced feature extraction precision
by the design of the detector. However, these methods fall
short of effectively minimizing the overall quantity of fea-
tures. Instead, they rely on the detector’s capacity to identify
and actively exclude certain invalid features. This strategy not
only imposes greater demands on the detector’s performance
but also introduces redundancy between the detector and the
preceding feature extraction network.
Consequently, we introduce the Feature Selection Module
to diminish irrelevant features, thereby alleviating the com-
putational burden and enabling the detector to concentrate
on essential features.
Feasibility of Feature Filtering. In initial experiments,
we assessed the viability of feature filtering through a simple
and intuitive method. Our approach involved applying a
masking operation to the feature maps. Specifically, we
randomly selected a proportions of features from these
maps and supplied them to the detector, discarding the
remaining features. Encouragingly, the experimental results
indicate that the detector’s performance is not substantially
compromised; instead, there is some degree of improvement.
This observation initially validates the redundancy of existing
features and underscores the effectiveness of our proposed
feature filtering approach.
Overall Architecture. Many tasks are linked to object
detection intricately. Thus, several methods incorporate aux-
iliary tasks to augment spatial features and offer implicit
guidance to achieve high-precision 3D object detection.
To assess the significance of features precisely, we intro-
duce prior knowledge that foreground points hold greater
importance than background points. To this end, as illustrated
in Fig. 4, we devise an additional importance prediction
branching task for predicting the importance weights for each
voxel grid.
Specifically, the voxel features fV ∈ Rnx×ny×nz×c are
processed by a prediction network Predictionc→1(·) to ob-
tain the importance weight matrix W ∈ [0, 1]nx×ny×nz of
the voxels, as shown in Eq. 7.
W = Predictionc→1(fV).
(7)
Following this, the system will retain the voxel grids Vi
that rank in the top 50% of the importance weights. The
Layer 2
Sparse 
Block
Regular Convs
Sparse 
Block
+
2× 
DS Block
Sparse 
Block
Regularization
Layer
Sparse 
Block
Layer 1
...
Submanifold Convs
Layer 4
...
3D Feat.
Voxels
Feature Extraction
Network
3D Detector
Layer 3
Res Block
Res Block
Res Block
Res Block
DFFM
FSM
Regularization
Layer
Relu
Relu
3D Feat.
3D Feat.
3D Feat.
Sparse 
Block 
Fig. 5.
Network components: feature extraction network, FSM, and 3D detector. (a) Derived from SECOND [4], the feature extraction network has four
layers. The first layer includes two residual blocks, and additional downsampling blocks exist the remaining three layers for feature size reduction. (b)
Enhancements include DFFM and FSM, added to the position indicated by the dotted line.
remaining non-empty grids will be set to null, i.e.:
V′ = {Vi | wi ≥ σ, Vi ∈ V},
(8)
where V′ represents the new voxels obtained after the filter-
ing, and σ is the threshold for ranking the top 50% impor-
tance weights. Eventually, these retained voxel features Vi
will be multiplied by the corresponding importance weights
wi to form the output features fV′.
fV′ = w ⊙ fV.
(9)
During training, the prediction network treats the voxels
within the truth box as positive samples while the remaining
are negative. Focal Loss [31] is utilized as the loss function,
and parameters are adjusted through backpropagation.
This strategy incorporates model compression by dimin-
ishing unimportant features and accentuating critical ones.
The subsequent detectors can then prioritize learning the
most essential features. Simultaneously, the reduction of
empty anchor frames mitigates mutual interference between
candidate frames. This approach significantly diminishes the
computational load and enhances the network’s convergence
speed and processing efficiency during training.
C. Overall Network Structure
The SECOND network [4] adopts a 3D sparse convolu-
tional network structure, which has become the standard for
voxel feature extraction networks. Specifically, the network
comprises four layers. In the first layer, raw voxels undergo
processing by a residual block consisting of two submanifold
sparse convolutions. Each subsequent layer initially employs
a 2× regular sparse convolution downsampling block to
halve the feature size before engaging in feature extraction
using the residual block. The same structure is used for many
subsequent works [3], [12], [32]–[34].
We enhance the baseline network by integrating the DFFM
into the feature extraction network, as illustrated in Fig.
5. Subsequently, the FSM is added before the detector.
To validate the effect of our strategy, we implement the
improved network on SECOND [4] and VoxelNext [3].
IV. EXPERIMENTS
A. Dataset
KITTI [35] serves as a widely recognized benchmark
for autonomous driving datasets. It encompasses three cate-
gories: cars, cyclists, and pedestrians, each further classified
into three difficulty levels—easy, medium, and hard—based
on object size, occlusion level, and truncation level. The
dataset consists of 7481 training samples and 7518 test
samples. For validation, the training samples are commonly
split into a training set of 3712 and a validation set of 3769.
The official evaluation criterion of the KITTI dataset
is AP|R40. The KITTI dataset uses AP|R40 as its official
evaluation criterion. It first splits the horizontal coordinate
into forty equal parts corresponding to the recall points
ri ∈ R = {1/40, 2/40, . . . , 1}. Next, the Precision-Recall
curve is smoothed by setting the recall value P′(ri) to the
largest recall value P(r′) on the right. The value of AP|R40
is calculated by the area under the smoothed curve, as:
AP |R40 =
1
|40|
X
ri∈R
P′(ri),
P′(ri) = max P(r′)
subject to r′ ≥ ri.
(10)
B. Setup Details
Data augmentation. Data augmentation is an effective
strategy for diversifying training data and thus improving
model’s generalization ability. Following [4], we extract
some ground truth boxes and place them randomly. Then,
point clouds undergo random transformations, including flip-
ping along the x-axis, rotation along the z-axis in the range
of [-π/4, π/4], and scaling within the range [0.95, 1.05].
Input Parameters. The KITTI dataset provides annotated
information only for objects in the field of the front camera’s
view. Therefore, we constrain the point clouds along the
(x, y, z) axes to [0, 70.4m], [-40m, 40m] and [-3m, 1m],
TABLE II
3D OBJECT DETECTION RESULTS ON THE KITTI VALIDATION SET. THE RESULTS ARE EVALUATED WITH AP|R40 .
Method
Car
Cyclist
Pedestrian
3D mAP
Easy
Mod.
Hard
Easy
Mod.
Hard
Easy
Mod.
Hard
SECOND† [4]
90.11
81.08
78.11
85.53
68.58
64.45
57.67
51.92
47.02
69.38
DFFM+SECOND
90.42‡
81.37
78.52
86.11
68.86
64.55
58.70
53.15
49.12
70.09
Improvement
+0.31
+0.29
+0.41
+0.58
+0.28
+0.10
+1.03
+1.23
+2.10
+0.71
FSM+SECOND
90.25
81.64
78.75
86.29
68.58
64.15
57.87
53.32
49.16
70.17
Improvement
+0.14
+0.56
+0.64
+0.76
+0.00
-0.30
+0.20
+1.40
+2.14
+0.79
VoxelNext† [3]
86.89
78.00
75.50
87.28
68.76
65.71
61.46
55.51
50.76
69.99
DFFM+VoxelNext
88.92
80.48
78.09
90.56
70.03
65.89
63.79
58.23
53.33
72.15
Improvement
+2.03
+2.48
+2.59
+3.28
+1.27
+0.18
+2.33
+2.72
+2.57
+2.12
FSM+VoxelNext
86.24
74.59
72.34
88.25
70.24
66.12
69.04
63.05
57.61
71.94
Improvement
-0.65
-3.41
-3.16
+0.97
+1.48
+0.41
+7.58
+7.54
+6.85
+1.95
† The detection results of the benchmark are reproduced by its official released code.
‡ The improved results are in bold.
TABLE III
EFFECTS OF DIFFERENT METHOD ON THE KITTI VALIDATION SET.
Method
Baseline [4]
LargeKernel3D [25]
DFFM
3D mAP(%)
69.38
69.84
70.09
respectively. We also adjust the voxel size to (0.05m, 0.05m,
0.1m). Consequently, the size of the entire 3D space after
voxelization is 1600×1408×40. The maximum number of
non-empty voxels is set to 16000 in the training set and
40000 in the test set.
Training. We use OpenPCDet [36] as the code library
for our development. All models are trained on two 3090Ti
graphics cards, using a Batch Size setting of 4, for 80 training
epochs. Following the optimization strategy of the baseline
detector [4], we train the models using the Adam optimizer
with an initial learning rate of 0.003, Momentum of 0.9, and
a weight decay parameter of 0.01.
C. Ablation Studies
We first evaluate our strategies using SECOND [4] and
VoxelNext [3] as benchmark models, representing voxel and
sparse detection networks, respectively.
DFFM. As indicated in Tab. II, the incorporation of
DFFM enhances the overall 3D mAP performance of the
SECOND by 0.71% and the VoxelNext network by an
astonishing 2.12%. Notably, DFFM improves the model’s
detection performance for pedestrians by (1.03%, 1.23%,
2.10%) and (2.33%, 2.72%, 2.57%), respectively, on AP|R40.
Tab. III also shows DFFM outperforms the similar method
LargeKernel3D [25] in overall performance.
We further conduct a comparative experiment on the effect
of integrating DFFM at different stages. As shown in Fig. 6,
no matter which stage the DFFM is placed in, it exhibits a
certain degree of performance improvement compared to the
original network. Notably, the most substantial enhancements
are observed when DFFM is set into the middle stage.
Moreover, the network’s performance exhibits a trend of
increase and decrease with the increase of the placement
stage.
FSM. The data in Tab. II indicate that after applying
FSM, the 3D mAP of SECOND and VoxelNext improves
Fig. 6.
Effect of DFFM in different stages.
by 0.79% and 1.95%, respectively. The SECOND detector
exhibits enhanced detection across various objects, and the
feature-filtered detector demonstrates improved detection of
small objects, particularly in VoxelNext, the detection perfor-
mance of pedestrians with different difficulties significantly
improves by (7.58%, 7.54%, and 6.85%).
The VoxelNext detector experiences a reduction in ve-
hicle detection performance after applying the FSM. We
believe this is not due to the FSM mistakenly discarding
essential features, but rather because of challenges posed
by the vehicle’s large size and the limited receptive field of
the feature extraction network. This limitation impedes the
learning of distinct features between important voxels inside
the vehicle and the background voxels. Consequently, the
FSM leads to the loss of front-back hierarchical information,
making it difficult for the detector to accurately delineate the
boundaries of the real box. The following overall experiment
also verified this.
TABLE IV
AVERAGE TIME FOR SINGLE-FRAME INFERENCE ACROSS NETWORKS ON
THE KITTI VALIDATION SET.
Method
Average time (ms)
FPS
Improvement
SECOND [4]
69.59
14.37
-
SECOND+FSM
63.10
15.85
9.33%
VoxelNext [3]
51.73
19.33
-
VoxelNext+FSM
41.60
24.04
19.56%
As shown in Tab. IV, the FSM reduces the inference
Fig. 7.
Effect of FSM with different settings.
time of SECOND and VoxelNext by 9.33% and 19.56%,
effectively improving the models’ real-time running speed.
We use VoxelNext as the benchmark and adjust the
importance filtering ratio along with the weight γ of the
importance prediction loss. In Fig. 7, when γ = 15, the
loss weight is excessively large, hindering the training of the
detection and yielding poor results. However, with γ = 5
or γ = 10, the ratio is reduced within a specific range,
leading to decreased interference between candidate boxes
and improved detector performance. Although the lower ratio
excludes many foreground voxels, resulting in a slight per-
formance degradation, it still surpasses the original network,
providing further evidence of feature redundancy.
Both. The combination of DFFM and FSM enhances the
network performance, as shown in Tab. V. This not only
validates the effectiveness of DFFM but also underscores
its complementary role with FSM. The dynamic receptive
field enables flexible extraction of required information from
various classes of voxels. Subsequent feature filtering di-
rectly eliminates unimportant voxels without compromising
front and back hierarchical information, leading to improved
model performance.
TABLE V
EFFECTS OF PROPOSED COMPONENTS ON THE KITTI VALIDATION SET.
Baseline
FSM
DFFM
Two-stage
3D mAP(%)
✓
69.99
✓
✓
71.94
✓
✓
✓
72.90(+2.91)
✓
✓
✓
✓
75.20(+5.21)
D. Visualizations
Feature Visualization. We employ geometric relations to
map voxels to the image’s coordinate system, allowing us
to observe changes in voxel features after passing through
the FSM. In Fig. 8, visualized images progress sequentially
from top to bottom, showcasing the original picture, original
point clouds mapped to the picture, and voxels mapped to
the image before and after going through the FSM. Different
colors of points on the picture indicate varying distances.
Observations indicate that critical voxels of real objects
remain unchanged in the locally enlarged right-column im-
ages. In contrast, the left-column images effectively remove
unimportant voxels in background elements such as high-
ways, grass, and signage.
Fig. 8.
Visual representations of various features: sequential images of
the original picture, original point clouds mapped to the picture, and voxels
before and after FSM processing.
Outputs. Considering the substantial enhancement in
small object detection, we focused on specific scenarios such
as people and bicycles, highlighting this progress through a
comparative analysis of detection outcomes. In Fig. 9, red
boxes indicate true labels, blue boxes represent results from
the original model, and outcomes from the improved model
are depicted with green boxes. Encouragingly, the improved
model not only adeptly identifies previously undetected dis-
tant pedestrians but also markedly reduces false detections
compared to the original model. This robustly substantiates
the pivotal role of our strategy in advancing small object
detection.
Fig. 9.
Comparing model outputs: true labels (red), original model results
(blue), and improved model outcomes (green).
V. CONCLUSIONS
In addressing the challenges associated with 3D target
detection, this article introduces two pivotal modules: the
Dynamic Feature Fusion Module (DFFM) and the Feature
Selection Module (FSM). These modules are dedicated to
achieving large receptive field and extracting important fea-
tures, respectively, presenting a novel approach to large
receptive field and important feature extraction strategies. To
overcome the challenge of expanding the receptive field of a
3D convolutional kernel, our proposed DFFM introduces an
adaptive approach. This module effectively balances the ex-
pansion of the 3D convolutional kernel’s receptive field with
acceptable computational loads, reducing operations while
enabling dynamic adjustments to different object require-
ments. Simultaneously, the FSM identifies and eliminates
redundant information in 3D features. The FSM segregates
output box fitting from feature extraction through quantitative
evaluation and discarding of unimportant features. This leads
to model compression, decreasing computational burden,
and mitigating interference among candidate boxes. Our
extensive experiments demonstrate the effectiveness of both
DFFM and FSM, particularly in improving small target
detection and accelerating network performance. Importantly,
these modules exhibit effective complementarity, showcasing
their combined impact on advancing the state-of-the-art in
3D object detection.
REFERENCES
[1] X. Ding, X. Zhang, J. Han, and G. Ding, “Scaling up your kernels to
31x31: Revisiting large kernel design in CNNs,” in CVPR, 2022, pp.
11 963–11 975.
[2] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie,
“A ConvNet for the 2020s,” in CVPR, 2022, pp. 11 976–11 986.
[3] Y. Chen, J. Liu, X. Zhang, X. Qi, and J. Jia, “VoxelNext: Fully sparse
VoxelNet for 3D object detection and tracking,” in CVPR, 2023, pp.
21 674–21 683.
[4] Y. Yan, Y. Mao, and B. Li, “SECOND: Sparsely embedded convolu-
tional detection,” Sensors, vol. 18, no. 10, p. 3337, 2018.
[5] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “PointNet++: Deep
hierarchical feature learning on point sets in a metric space,” ANIPS,
vol. 30, 2017.
[6] J. Ngiam, B. Caine, W. Han, B. Yang, Y. Chai, P. Sun, Y. Zhou,
X. Yi, O. Alsharif, P. Nguyen et al., “StarNet: Targeted computation
for object detection in point clouds,” arXiv preprint arXiv:1908.11069,
2019.
[7] Z. Yang, Y. Sun, S. Liu, and J. Jia, “3DSSD: Point-based 3D single
stage object detector,” CVPR, 2020.
[8] X. Pan, Z. Xia, S. Song, L. E. Li, and G. Huang, “3D object detection
with pointformer,” in CVPR, 2021, pp. 7463–7472.
[9] C. He, H. Zeng, J. Huang, X.-S. Hua, and L. Zhang, “Structure aware
single-stage 3D object detection from point cloud,” in CVPR, 2020,
pp. 11 873–11 882.
[10] T. Yin, X. Zhou, and P. Krahenbuhl, “Center-based 3D object detection
and tracking,” in CVPR, 2021, pp. 11 784–11 793.
[11] S. Shi, Z. Wang, J. Shi, X. Wang, and H. Li, “From points to parts: 3D
object detection from point cloud with part-aware and part-aggregation
network,” TPAMI, vol. 43, no. 8, pp. 2647–2664, 2020.
[12] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, “PV-
RCNN: Point-voxel feature set abstraction for 3D object detection,”
in CVPR, 2020, pp. 10 529–10 538.
[13] L. Cui, X. Li, M. Meng, and X. Mo, “MMFusion: A generalized multi-
modal fusion detection framework,” in IEEE International Conference
on Development and Learning, 2023, pp. 415–422.
[14] B. Li, T. Zhang, and T. Xia, “Vehicle detection from 3D LiDAR using
fully convolutional network,” arXiv preprint arXiv:1608.07916, 2016.
[15] K. Minemura, H. Liau, A. Monrroy, and S. Kato, “LMNet: Real-time
multiclass object detection on CPU using 3D LiDAR,” in Asia-Pacific
Conference on Intelligent Robot Systems.
IEEE, 2018, pp. 28–34.
[16] M. Feng, S. Z. Gilani, Y. Wang, L. Zhang, and A. Mian, “Relation
graph network for 3D object detection in point clouds,” IEEE Trans-
actions on Image Processing, vol. 30, pp. 92–107, 2020.
[17] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in CVPR, 2016, pp. 770–778.
[18] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and
B. Guo, “Swin Transformer: Hierarchical vision transformer using
shifted windows,” in ICCV, 2021, pp. 10 012–10 022.
[19] S. Shi, X. Wang, and H. Li, “PointRCNN: 3D object proposal
generation and detection from point cloud,” in CVPR, 2019, pp. 770–
779.
[20] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom,
“PointPillars: Fast encoders for object detection from point clouds,”
in CVPR, 2019, pp. 12 697–12 705.
[21] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, “Point Trans-
former,” in ICCV, 2021, pp. 16 259–16 268.
[22] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu,
“PCT: Point cloud transformer,” Computational Visual Media, vol. 7,
pp. 187–199, 2021.
[23] J. Mao, Y. Xue, M. Niu, H. Bai, J. Feng, X. Liang, H. Xu, and C. Xu,
“Voxel transformer for 3D object detection,” in ICCV, 2021, pp. 3164–
3173.
[24] P. Sun, M. Tan, W. Wang, C. Liu, F. Xia, Z. Leng, and D. Anguelov,
“SWFormer: Sparse window transformer for 3D object detection in
point clouds,” in ECCV.
Springer, 2022, pp. 426–442.
[25] Y. Chen, J. Liu, X. Zhang, X. Qi, and J. Jia, “LargeKernel3D: Scaling
up kernels in 3D sparse CNNs,” in CVPR, 2023, pp. 13 488–13 498.
[26] T. Lu, X. Ding, H. Liu, G. Wu, and L. Wang, “LinK: Linear kernel
for LiDAR-based 3D perception,” in CVPR, 2023, pp. 1105–1115.
[27] H. Wu, J. Deng, C. Wen, X. Li, C. Wang, and J. Li, “CasA: A cascade
attention network for 3D object detection from LiDAR point clouds,”
IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp.
1–11, 2022.
[28] Y. Liu, T. Wang, X. Zhang, and J. Sun, “PETR: Position embed-
ding transformation for multi-view 3D object detection,” in ECCV.
Springer, 2022, pp. 531–548.
[29] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Y. Qiao, and J. Dai,
“BEVFormer: Learning bird’s-eye-view representation from multi-
camera images via spatiotemporal transformers,” in ECCV.
Springer,
2022, pp. 1–18.
[30] Y. Wang, V. C. Guizilini, T. Zhang, Y. Wang, H. Zhao, and J. Solomon,
“DETR3D: 3D object detection from multi-view images via 3D-to-2D
queries,” in CRL.
PMLR, 2022, pp. 180–191.
[31] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar, “Focal loss
for dense object detection,” in Proceedings of the IEEE International
Conference on Computer Vision, 2017, pp. 2980–2988.
[32] J. S. Hu, T. Kuai, and S. L. Waslander, “Point density-aware voxels
for LiDAR 3D object detection,” in CVPR, 2022, pp. 8469–8478.
[33] S. Shi, L. Jiang, J. Deng, Z. Wang, C. Guo, J. Shi, X. Wang, and
H. Li, “PV-RCNN++: Point-voxel feature set abstraction with local
vector representation for 3D object detection,” International Journal
of Computer Vision, vol. 131, no. 2, pp. 531–551, 2023.
[34] J. Deng, S. Shi, P. Li, W. Zhou, Y. Zhang, and H. Li, “Voxel R-CNN:
Towards high performance voxel-based 3D object detection,” in AAAI,
vol. 35, no. 2, 2021, pp. 1201–1209.
[35] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous
driving? The KITTI vision benchmark suite,” in CVPR, 2012, pp.
3354–3361.
[36] O. D. Team, “OpenPCDet: An open-source toolbox for 3D ob-
ject detection from point clouds,” https://github.com/open-mmlab/
OpenPCDet, 2020.
"
"Several classical results in social choice theory demonstrate that in order for many
voting systems to be well-behaved the set domain of individual preferences must satisfy some
kind of restriction, such as being single-peaked on a political axis. As a consequence it becomes
interesting to measure how diverse the preferences in a well-behaved domain can be.
In this paper we introduce an egalitarian approach to measuring preference diversity, focusing
on the abundance of distinct suborders one subsets of the alternative.
We provide a common
generalisation of the frequently used concepts of ampleness and copiousness.
We give a detailed investigation of the abundance for Condorcet domains. Our theorems imply
a ceiling for the local diversity in domains on large sets of alternatives, which show that in this
measure Black’s single-peaked domain is in fact optimal.
We also demonstrate that for some
numbers of alternatives, there are Condorcet domains which have largest local diversity without
having maximum order.","In many situations studied in social choice a group of agents, which may be voters in a political system
or artificial agents in a computer environment, need to reach a common decision based on their indi-
vidual preferences. In situations where the individual preferences are completely unrestricted the now
classical theorem of Arrow (1950) shows that all decision procedures fail to satisfy several desirable
properties. Similarly the Gibbard-Satterwhaite theorem Gibbard (1973); Satterthwaite (1975) demon-
strates the only social choice procedure which is proof against strategic manipulation is dictatorial.
On the other hand Black (1948) showed that when preferences are based on preferences which are
single-peaked on a common political axis, majority voting will be both decisive and strategyproof.","These contrasting results lead to the question of how diverse a collection of individual preferences can
be while also having a well-behaved social choice procedure. One of the most studied classes of well-
behaved preferences is the Condorcet domain. A Condorcet domain is a set of linear orders such that
the majority voting decision is transitive when each voter chooses their preferences from this domain.
Condorcet domains secure strategyproofness in both deterministic (Campbell and Kelly, 2003) and
randomized environments (Brandt et al., 2023).
One of the most obvious measures for the diversity of opinions in a Condorcet domain is the size
of the domain. This is a well-studied problem in social choice theory Kim et al. (1992); Puppe and
Slinko (2023). Recent studies (Karpov and Slinko, 2023; Karpov et al., 2023) improved the asymptotic
lower bound for the size of the largest Condorcet domains. Up to seven alternatives, all maximal
Condorcet domains are investigated in Akello-Egwell et al. (2023), and in Leedham-Green et al. (2023)
the largest domains on eight alternatives were determined. For higher numbers of alternatives the
maximum possible size of a Condorcet domain remains an open problem.
However, the size is only the simplest measure of diversity and domains of the same size may have
very different structures. With this in mind, other types of diversity measures have been studied.nannannan","Puppe (2018) justified the single-peaked domain of preferences by requiring maximal diversity of the
top elements. A more general measure of diversity is the support-based preference diversity index
Hashemi and Endriss (2014) that evaluates diversity by the total number of distinct suborders of a
given size. It reflects the idea of utilitarianism. It has substitutability, i.e. low diversity of preferences
within one subset of alternatives can be compensated by high diversity within another subset of
alternatives. By measuring the lowest number of distinct suborders over all subsets of a given size, we
implement an egalitarian (Rawlsian) approach to preference diversity measurement. Note that this is
egalitarian with respect to alternatives (subsets of alternatives), not voters/agents.nan","Some Condorcet domains have very low local diversity, e.g. non-ample Condorcet domains Akello-
Egwell et al. (2023). On the other hand, there are Condorcet domains with at least 2k⁄1 distinct
suborders for each k-elements subset. Single-peaked, single-dipped, and group-separable domains are
examples of such domains. Utilizing Ramsey’s theorem, we show that for sufficiently big numbers of
alternatives n and a fixed k, 2k⁄1 distinct suborders is the highest possible local diversity. This fact
is an additional justification for the classical domains such as the single-peaked, and group-separable
domains. It is possible to find significantly larger Condorcet domains than the single-peaked domain
(see e.g. set-alternating scheme Karpov et al. (2023)), but their local diversity properties cannot be
strictly better.
For a small number of alternatives, the Condorcet domain with the highest number of orders also
has the best local diversity properties. Starting with n = 7, there are maximal Condorcet domains
that do not have the highest number of orders but have better local diversity parameters than the
Condorcet domains of the maximal size. The study of local diversity properties thus opens a new line
of research on large Condorcet domains.
In this paper we focus our discussion on Condorcet domains, however the concept of abundance
applies to general domains and we will in our experimental analysis also include less restricted domains.","This paper links the study of local diversity to Condorcet domains. For a given number of alternatives
n, we determine the Condorcet domains that exhibit the highest possible diversity for their least
diversified restriction. The results are valid for an unlimited number N of agents. For n ≥ 7, several
of these domains are novel and have not been previously explored, while for smaller values of n,
Fishburn’s alternating domains serve as examples of maximum diversity. Notably, in several instances,
the domains with the highest local diversity are not the ones with the greatest number of preference
orders, underlining that maximizing the number of preference orders and maximizing diversity are two
separate challenges for Condorcet domains.",Local Diversity of Condorcet Domains,"Alexander Karpov, Klas Markström, Søren Riis, Bei Zhou","Local Diversity of Condorcet Domains
Alexander Karpov∗1,2, Klas Markstr¨om3, Søren Riis4, and Bei Zhou4
1HSE University, Moscow, Russia
2Institute of Control Sciences, Russian Academy of Sciences, Moscow, Russia
3Ume˚a University
4Queen Mary University of London
Abstract
Several of the classical results in social choice theory demonstrate that in order for many
voting systems to be well-behaved the set domain of individual preferences must satisfy some
kind of restriction, such as being single-peaked on a political axis. As a consequence it becomes
interesting to measure how diverse the preferences in a well-behaved domain can be.
In this paper we introduce an egalitarian approach to measuring preference diversity, focusing
on the abundance of distinct suborders one subsets of the alternative.
We provide a common
generalisation of the frequently used concepts of ampleness and copiousness.
We give a detailed investigation of the abundance for Condorcet domains. Our theorems imply
a ceiling for the local diversity in domains on large sets of alternatives, which show that in this
measure Black’s single-peaked domain is in fact optimal.
We also demonstrate that for some
numbers of alternatives, there are Condorcet domains which have largest local diversity without
having maximum order.
1
Introduction
In many situations studied in social choice a group of agents, which may be voters in a political system
or artificial agents in a computer environment, need to reach a common decision based on their indi-
vidual preferences. In situations where the individual preferences are completely unrestricted the now
classical theorem of Arrow (1950) shows that all decision procedures fail to satisfy several desirable
properties. Similarly the Gibbard-Satterwhaite theorem Gibbard (1973); Satterthwaite (1975) demon-
strates the only social choice procedure which is proof against strategic manipulation is dictatorial.
On the other hand Black (1948) showed that when preferences are based on preferences which are
single-peaked on a common political axis, majority voting will be both decisive and strategyproof.
These contrasting results lead to the question of how diverse a collection of individual preferences can
be while also having a well-behaved social choice procedure. One of the most studied classes of well-
behaved preferences is the Condorcet domain. A Condorcet domain is a set of linear orders such that
the majority voting decision is transitive when each voter chooses their preferences from this domain.
Condorcet domains secure strategyproofness in both deterministic (Campbell and Kelly, 2003) and
randomized environments (Brandt et al., 2023).
One of the most obvious measures for the diversity of opinions in a Condorcet domain is the size
of the domain. This is a well-studied problem in social choice theory Kim et al. (1992); Puppe and
Slinko (2023). Recent studies (Karpov and Slinko, 2023; Karpov et al., 2023) improved the asymptotic
lower bound for the size of the largest Condorcet domains. Up to seven alternatives, all maximal
Condorcet domains are investigated in Akello-Egwell et al. (2023), and in Leedham-Green et al. (2023)
the largest domains on eight alternatives were determined. For higher numbers of alternatives the
maximum possible size of a Condorcet domain remains an open problem.
However, the size is only the simplest measure of diversity and domains of the same size may have
very different structures. With this in mind, other types of diversity measures have been studied.
Puppe (2018) justified the single-peaked domain of preferences by requiring maximal diversity of the
∗Authors are listed in alphabetical order
1
arXiv:2401.11912v1  [econ.TH]  22 Jan 2024
top elements. A more general measure of diversity is the support-based preference diversity index
Hashemi and Endriss (2014) that evaluates diversity by the total number of distinct suborders of a
given size. It reflects the idea of utilitarianism. It has substitutability, i.e. low diversity of preferences
within one subset of alternatives can be compensated by high diversity within another subset of
alternatives. By measuring the lowest number of distinct suborders over all subsets of a given size, we
implement an egalitarian (Rawlsian) approach to preference diversity measurement. Note that this is
egalitarian with respect to alternatives (subsets of alternatives), not voters/agents.
Recent literature has introduced two local diversity conditions for domains: ampleness and co-
piousness, see Slinko (2019); Puppe and Slinko (2023). The first means that the restriction of the
domain to any pair of alternatives contains both of the possible orders, and the latter means that the
restriction to any triple contains four orders. In this paper we generalize these notions by introducing
a new diversity measure called abundance. Here we say that a Condorcet domain that contains at
least s suborders for each k-elements subset of alternatives is more diverse than a Condorcet domain
that contains at least s′ suborders for each k-elements subset of alternatives if we have s > s′.
Some Condorcet domains have very low local diversity, e.g. non-ample Condorcet domains Akello-
Egwell et al. (2023). On the other hand, there are Condorcet domains with at least 2k−1 distinct
suborders for each k-elements subset. Single-peaked, single-dipped, and group-separable domains are
examples of such domains. Utilizing Ramsey’s theorem, we show that for sufficiently big numbers of
alternatives n and a fixed k, 2k−1 distinct suborders is the highest possible local diversity. This fact
is an additional justification for the classical domains such as the single-peaked, and group-separable
domains. It is possible to find significantly larger Condorcet domains than the single-peaked domain
(see e.g. set-alternating scheme Karpov et al. (2023)), but their local diversity properties cannot be
strictly better.
For a small number of alternatives, the Condorcet domain with the highest number of orders also
has the best local diversity properties. Starting with n = 7, there are maximal Condorcet domains
that do not have the highest number of orders but have better local diversity parameters than the
Condorcet domains of the maximal size. The study of local diversity properties thus opens a new line
of research on large Condorcet domains.
In this paper we focus our discussion on Condorcet domains, however the concept of abundance
applies to general domains and we will in our experimental analysis also include less restricted domains.
The structure of the paper is organised as follows: Section 2 introduces the notation and definitions
employed throughout the paper. Section 3 defines the abundance of a domain and presents a series of
theorems about abundance. Section 4 discusses the maximality of Condorcet domains, used in some
of the later computational work. Section 5 establishes asymptotic upper bounds for abundance via
a Ramsey-type property for Condorcet domains. Section 6 discusses abundance in terms of diversity
indices and diversity orders for profiles. It follows this by an experimental study that compares the
abundance of profiles randomly sampled from various domain types, including domains arising from
empirical data. Finally, section 7 gives some conclusions.
2
Background and Notation
Let a finite set X = [n] = {1, . . . , n} be the set of alternatives. Let L(X) be the set of all linear orders
over X. Each agent i ∈ N has a preference order Pi over X (each preference order is a linear order).
For brevity, we will write preference orders as strings, e.g. 12 . . . n means that 1 is the best alternative,
n is the worst.
A subset of preference orders D ⊆ L(X) is called a domain of preference orders. A domain D
is a Condorcet domain if whenever the preferences of all agents belong to the domain, the majority
relation of any preference profile with an odd number of agents is transitive. A Condorcet domain D
is maximal if every Condorcet domain D′ ⊃ D (on the same set of alternatives) coincides with D. A
Condorcet domain D is unitary if it contains order 12 . . . n. Each Condorcet domain can be rearranged
to a unitary form by renaming alternatives. We will consider mainly unitary Condorcet domains.
Two Condorcet domains are isomorphic if they differ only by a relabeling of the alternatives.
The restriction of a domain D to a subset A ⊂ X is the set of linear orders from L(A) obtained
by restricting each linear order from D to A.
Sen (1966) proved that a domain is a Condorcet domain if the restriction of the domain to any
triple of alternatives (a, b, c) satisfies a never condition. A never condition can be of three forms xNb,
2
xNm xNt, referred to as a never bottom, a never middle, and a never top condition respectively. Here
x is an alternative from the triple and xnb, xNm, and xNt means that x is not ranked last, second, or
first respectively in the restricted domain. Fishburn noted that for domains with a societal axis never
conditions can instead be described as iNj, i, j ∈ [3]. iNj means that ith alternative from the triple
according to societal axis does not fall in jth place within this triple in any order from the domain.
Unitary domains have at most six types of never conditions in Fishburn’s format.
A profile is a vector R = (R1, R2, . . . , RN) such that Ri is the preferences of voter/agent i. The
support Supp(R) of a profile is the set of distinct preference orders occurring in R.
A profile is
unanimous if the support has size 1. The census of R is the vector C(R) = ((f1, R1), (f2, R2), . . .)
with one pair (fi, Ri) for each Ri ∈ Supp(R) where fi is the number of agents with preference Ri.
From Hashemi and Endriss (2014) we have the following definitions. A preference diversity order
is a partial order ⪯ on the set of profiles such that R1 ⪯ R2 if R1 is unanimous. A preference diversity
index is a function ∆ from the set of profiles to the non-negative real numbers such that ∆(R) = 0 if
R is unanimous.
A diversity index ∆ also defines a natural diversity order ⪯∆ by saying that R1 ⪯∆ R2 if ∆(R1) ≤
∆(R2).
A Condorcet domain is said to be ample if restriction of the domain to any pair of alternatives
contains both of the possible orders, and copious if the restriction to any triple contains four orders.
A domain which satisfies a never condition of the form xN3 for every triple is called an Arrow’s
single-peaked domain Arrow (1963).
Black’s single-peaked domain is the maximal Arrow’s single-
peaked domain isomorphic to the domain defined by using the never condition 2N3 on all triples Black
(1948). A domain is a single-crossing domain if there is an ordering for the set of preference orders
in the domain such that for any pair of alternatives (i, j) there exists a unique k so that their relative
order is (i, j) for the top k preference orders and (j, i) for the rest Slinko et al. (2021). Fishburn’s
alternating domains are defined by assigning a triple (i, j, k) the never condition 2N1 if j is odd and
2N3 if j is even Fishburn (1997). Karpov et al. (2023) introduced the set-alternating domains. Given
a subset A of the alternatives the corresponding set-alternating domain has the never condition 1N3
on each triple (i, j, k) where j ∈ A, and 3N1 on the other triples.
Ramsey’s theorem Ramsey (1930) asserts that given positive integers k, r, n there is a positive
integer R(k, r, n) such that if N ≥ R(k, r, n), and each subset of size r from [N] is assigned a label
from [k], then all r-subsets from some subset of [N] of size n has the same label.
2.1
Code and data
For our computational results we have used the CDL library Zhou et al. (2023), which provides
functions for the study of Condorcet domains and other types of restricted preference orders. This
library includes a function which computes the restriction of a domain to a subset of the alternatives.
The programs and data used to derive the results in this paper are publicly available1.
Whenever possible we have also verified our results by using independently coded functions in
Mathematica.
3
Local diversity conditions for Condorcet domains
There are two kinds of local conditions on domains that have been much used in recent literature, am-
pleness and copiousness. The first means that the restriction of the domain to any pair of alternatives
will contain both of the possible orders, and the latter that the restriction to any triple will see the
maximum possible four orders.
One way of looking at these conditions is that they require a minimum diversity of orders on sets
of size 2 and 3 respectively. This point of view can be naturally generalised as follows.
Definition 1. A domain D is (k, s)-abundant if the restriction of D to any subset of k alternatives
has size at least s.
A domain is exactly (k, s)-abundant if it is (k, s)-abundant but not (k, s + 1)-abundant.
Being (3, 4)-abundant is equivalent to being copious and (2, 2)-abundant is equivalent to being
ample. Here we also note that being (k, s)-abundant is a monotone-increasing property under the
1GitHub. https://github.com/sagebei/Local_Diversity_of_Condorcet_Domains.git
3
addition of new orders to the domain; that is, for a given k the domain C ∪ {σ} always has at least
as large abundance as the domain C. This in turn implies that the largest possible abundance can
always be found among the maximal Condorcet domains, but it can also occur already in non-maximal
Condorcet domains.
Let us look at the abundance properties of some classes of Condorcet domains.
Theorem 1.
1. The maximal Black’s and Arrow’s single-peaked domains on n alternatives are exactly (k, 2k−1)-
abundant for every k ≤ n.
2. Set-alternating domains Karpov et al. (2023) are (k, 2k−1)-abundant for every k ≤ n. If k < n/2
then they are exactly (k, 2k−1)-abundant.
3. Fishburn’s alternating domains are exactly (k, 2k−1)-abundant if k ≤ ⌈n/2⌉ + 1.
For n = 4, 5 they are (4, s)-abundant for s = 9, 8.
For n = 5, 6, 7 they are (5, s)-abundant for s = 20, 19, 16.
For n = 6, 7, 8, 9 they are (6, s)-abundant for s = 45, 42, 39, 32.
For n = 7, 8, 9, 10, 11 they are (7, s)-abundant for s = 100, 96, 86, 79, 64.
4. The maximum Condorcet domain for n = 8 is (4, 8), (5, 18), (6, 40), and (7, 96)-abundant.
Proof. Point 1 follows since the restriction of an Arrow’s single-peaked domain to a subset of the
alternatives is also an Arrow’s single-peaked domain, and by Slinko (2019) any maximal Arrow’s
single-peaked domain on k alternatives has size 2k−1.
Point 2 follows using the construction in Karpov et al. (2023) of set-alternating domains. It is shown
that every such domain on t alternatives has size at least 2t−1, and the restriction of a set-alternating
domain to a subset of the alternatives is also a set-alternating domain.
The first part of point 3 follows by taking the restriction of the domain to the set of even integers
together with 1 n, if the latter is odd. This set has size ⌈n/2⌉ + 1 and the restricted domain uses only
the never condition 2N1. If k is smaller than ⌈n/2⌉ + 1 we further restrict to an arbitrary subset A of
size k. By Raynaud (1981) the restricted domain has size 2k−1, since it uses a single never condition.
The remainder of point 3 follows from a computational test of these domains, done independently
in Mathematica and CDL.
Point 4 follows by a computational test of the maximum domain from Leedham-Green et al. (2023).
Theorem 2. For n ≤ 6 and k = 2, 3, 4, 5, 6, Fishburn’s alternating domains have the maximum
possible abundance among all maximal Condorcet domains with a given number of alternatives. For
n = 7, they do not reach the maximum possible for k = 6.
Proof. The abundances for all maximal Condorcet domains in this range were computed using the
data from Akello-Egwell et al. (2023).
For n = 7, n = 8, with k = 6, there are maximal Condorcet domains with higher abundance than
the Condorcet domains of maximum size.
Given two domains with large abundances, we have a natural construction for larger domains which
inherit some abundances from the first domains.
Definition 2. Given two domains D1 and D2 on the sets A and B respectively we define a domain
S(D1, D2) on the set A ∪ B as follows: Given any order u from D1 and any order v from D2 the
domain contains both the concatenation uv and the concatenation vu.
The domain S(D1, D2) has size 2|D1||D2| and cannot have low abundance when D1 and D2 do
not.
Proposition 1. If D1 and D2 are (k, 2k−1)-abundant for each k ≤ t then so S(D1, D2).
4
Proof. This obviously holds for k-tuples which only contain alternatives from one of A and B. Let us
assume that the tuple has elements T1 from A and T2 from B. Then S(D1, D2) contains both every
concatenation of the, at least, 2|T1|−1 elements in the restriction of D1 to T1 with the, at least, 2|T2|−1
elements in the restriction of D2 to T2, and the same number of concatenations om the opposite order.
This gives a total of at least 2 × 2|T1|−1 × 2|T2|−1 = 2k−1 linear orders in the restriction to this
k-tuple.
Given that abundance is a monotone-increasing property it is natural to ask how many linear orders
we must have in order to achieve a specified abundance. This is equivalent to asking, given a fixed k
how small can a minimal (k, s)-abundant domain D be? Here minimal means that every subset of D
is a domain which is not (k, s)-abundant.
Theorem 3. We have the following existence results for small domains with given abundance:
1. For every k ≥ 2 there are (k, 2)-abundant domains of size 2 for every n ≥ 2.
2. There exists (3, 3)-abundant and (3, 4)-abundant domains of size 3 and 4 respectively if and only
if 3 ≤ n ≤ 8.
3. There exists single-crossing maximal Condorcet domains with size
Table 1: Implications for induced abundance
(3, 4) → (2, 2)
(4, 9) → (2, 2)
(5, 19) → (2, 2)
(6, 43) → (2, 2)
(4, 4) → (3, 1)
(4, 5 − 6) → (3, 2)
(4, 7 − 8) → (3, 3)
(4, 9) → (3, 4)
(5, 12) → (3, 1)
(5, 13 − 16) → (3, 2)
(5, 17 − 18) → (3, 3)
(5, 19 − 20) → (3, 4)
(6, 40) → (3, 2)
(6, 41 − 42) → (3, 3)
(6, 43) → (3, 4)
(5, 5) → (4, 1)
(5, 6 − 8) → (4, 2)
(5, 9 − 11) → (4, 3)
(5, 12 − 13) → (4, 4)
(5, 14 − 15) → (4, 5)
(5, 16 − 17) → (4, 6)
(5, 18) → (4, 7)
(5, 19 − 20) → (4, 8)
(6, 40 − 41) → (4, 6)
(6, 42 − 43) → (4, 7)
(6, 44 − 45) → (4, 8)
(6, 37) → (5, 13)
(6, 38) → (5, 13)
(6, 39) → (5, 14)
(6, 40) → (5, 15)
(6, 41) → (5, 16)
(6, 42) → (5, 16)
(6, 43) → (5, 17)
(6, 44) → (5, 18)
(6, 45) → (5, 19)
4
A note on maximality
In this section we will consider the restrictions of a maximal Condorcet domain to all subsets of
alternatives of size n − 1. This is done for two reasons. First, this will give us information about how
compatible the rankings of alternatives from different subsets are, and as we will see there are domains
where this is so context dependent that none of these restrictions are maximal domains.
Second,
domains which can be restricted to maximal subdomains are sometimes easier to treat computationally
and some of the results in the next section will depend on this.
When considering the collection of restrictions to n − 1 alternatives there are in principle three
cases. Either all restrictions to n − 1 alternatives are maximal Condorcet domains, or there are both
maximal and non-maximal restrictions, or all such restrictions are non-maximal Condorcet domains.
In the last case we have mutually incompatible preferences coming from all subsets of the alternatives.
Let us say that a Condorcet domain D on n alternatives is discordant if D is maximal but does not
have a subdomain D′ on n−1 alternatives which is also a maximal Condorcet domain. By eliminating
an alternative from a discordant domain we resolve conflicting never conditions, and we can find some
orders that do not belong to the restriction, but satisfy all never conditions.
Note that the copiousness property of Slinko (2019) is equivalent to saying that every subdomain
on 3 alternatives is maximal. Slinko gave examples of domains which are not copious, and many such
examples were given in Akello-Egwell et al. (2023). However, all the non-copious examples for small
n also have subdomains on 3 alternatives which are maximal, i.e. they have triples of both kinds. In
fact, using the data from Akello-Egwell et al. (2023) exhaustive testing shows that for 4 ≤ n ≤ 5 there
are no discordant domains. However, by extending this exhaustive search to the domains for n = 6, 7
are found.
We find that for n = 6 there are in total five discordant domains. One of these has size 4 and
the other four have size 25. We display the example of size 4 domain in Figure 2. The restriction
of this domain to each five-alternatives subset contains four orders, but the corresponding maximal
Condorcet domain on five alternatives has eight orders.
1
2
3
4
5
6
3
6
2
5
1
4
4
1
5
2
6
3
6
5
4
3
2
1
Figure 2: The smallest discordant Condorcet domain
For n = 7 we have a much wider range of domain sizes for which discordant domains exist. In
6
Table 2 we show the number of non-isomorphic discordant Condorcet domains of each size for n = 7.
These domains can be downloaded from ”” (2199).
Table 2: The number of discordant domains and their sizes for n = 7
Size
12
14
15
16
17
18
19
20
21
22
Count
2
4
2
20
14
8
66
48
48
24
Size
23
24
25
26
27
28
29
30
31
32
Count
8
28
32
38
26
38
32
69
36
89
Size
33
34
35
36
37
38
39
40
41
42
Count
64
30
90
102
78
70
70
304
34
49
Size
43
44
45
46
47
48
49
50
51
52
Count
44
95
92
74
64
94
56
110
42
100
Size
53
54
55
56
57
58
60
Count
38
78
10
74
4
48
15
As we can see, discordant domains exist for most small sizes for n = 7 but do not exist for any size
above 60. For domains close to the maximum possible size it seems likely that restrictions will also
remain large and we pose the following problem:
Problem 1. Let f(n) denote the maximum possible size for a Condorcet domain on n alternatives.
Is it true that every discordant Condorcet domain has size at most 0.8f(n)?
The constant 0.8 in the problem is not intended to be sharp, but rather is a description of the
intuitive picture that very large domains should be so diverse that this is true for their restrictions as
well. From our data, we see that for n ≤ 7 the problem would have a positive answer for a constant
much smaller than 0.8.
5
Ramsey properties of diversity
From e.g. Black’s single-peaked domain we know that (k, 2k−1) domains exist for all n ≥ k. However,
for a fixed k ≥ 4 there are values of s such that (k, s)-abundant domains exist only for a finite set
of values of n. This connects to the observation in Zhou and Riis (2023) that large enough domains
should have subdomains of size six using only a single never condition. We here provide a proof using
the hypergraph version of Ramsey’s theorem and also note that this implies the existence of a more
specifically order-theoretic Ramsey property.
Theorem 4. Given an integer k there exists two numbers Rs(k) ≤ Ru(k) such that
• For n ≥ Rs(k), no domain on n alternatives is (k, s)-abundant for s > 2k−1.
• For n ≥ Ru(k), every domain on n alternatives has a subset A′ of k alternatives such that every
triple in A′ satisfies the same never condition.
Proof. Given a unitary Condorcet domain C on n alternatives we define an edge colouring of the
complete 3-uniform hypergraph K(3)
n
by using the name of the never condition on a triple as the colour
of the corresponding edge of the hypergraph. This colouring uses at most 6 colours.
By Ramsey’s theorem for hypergraphs Ramsey (1930) there exists an integer N such that if n ≥ N
then there must exist a subset A′ of the vertices such that every triple has the same colour. This
proves the existence of Ru(k).
By Theorems 3 and 4 of Raynaud (1981) a domain on k alternatives in which every triple satisfies
the same never condition has size 2k−1. This proves that Rs(k) exists and is at most Ru(k).
The general theorem shows that Rs(k) ≤ Ru(k) but for small values of k we find that Rs(k) is
typically much smaller than Ru(k), which is equal to the corresponding Ramsey number. The latter
is known to grow super-exponentially in k Conlon et al. (2010) and the only known exact value is that
with two colours an k = 4 we get R = 13 McKay and Radziszowski (1991).
We have computed the exact value of Rs(k) for several small values of k.
7
Theorem 5. The following bounds on the number of alternatives in a (k, s)-abundant domain hold:
1. (4, 9)-abundant domains exist only for n = 4.
2. (5, 20) and (5, 19)-abundant domains exist if and only 5 ≤ n ≤ 6.
3. (5, 18) and (5, 17)-abundant domains exist if and only if 5 ≤ n ≤ 8.
4. (6, 43), (6, 44), and (6, 45)-abundant domains exist only for n = 6.
5. (6, 42)-abundant domains exist for n = 6, 7. If problem 1 has an affirmative answer, they do not
exist for n ≥ 8.
6. (6, 41)-abundant domains exist for n = 6, 7, 8. If problem 1 has an affirmative answer, they do
not exist for n ≥ 9.
Proof. Points 1 to 4 follow by exhaustively checking the abundance for the maximal CDs generated
in Akello-Egwell et al. (2023) and the maximum CD from Leedham-Green et al. (2023), followed by a
search for n = 9 using CDL.
For point 5, the existence of (6, 42)-abundant domains again follows by analysing the data set from
Akello-Egwell et al. (2023). From that data we found all maximal (6, 42)-abundant domains for n = 7,
all these domains had size 92 or more. Using CDL we found all extensions of these domains to n = 8
and checked that none were (6, 42)-abundant.
For point 6, the existence of (6, 41)-abundant domains for n = 6, 7 again follows by analysing the
data set from Akello-Egwell et al. (2023). From that data, we found all maximal (6, 41)-abundant
domains for n = 7, all these domains had size 91 or more. Using CDL we found all extensions of these
domains to n = 8, where examples of such domains were found. These were in turn extended to n = 9
checked that none were (6, 41)-abundant.
Abundance (5, 18) is achieved for n = 8 by the maximum domain from Akello-Egwell et al. (2023),
and for n = 7 an example of size 96 can be constructed as a subdomain of the n = 8 domain.
Abundance (6, 42) for n = 7 is achieved by Fishburn’s alternating domain.
Abundance (6, 41) is achieved for n = 8 by two maximal domains of size 219. The first of these can
be viewed as a variation of Fishburn’s alternating domain where the never conditions on four triples
have been changed as follows: ((1, 2, 4),1N3),((1, 2, 6),1N3), ((3, 7, 8),3N1),((5, 7, 8),3N1).
6
Abundance based diversity indices
Our next aim is to connect the local diversity in the form of abundance to the study of diversity indices,
as outlined by Hashemi and Endriss Hashemi and Endriss (2014).
We define the abundance vector A(D) of a domain D to the vector in which entry number k is the
s such that D is exactly (k, s)-abundant. Here position k = 1 will be 1 for every non-empty domain.
We say that D1 is ranked higher than D2 if A(D1) is lexicographically larger than A(D2)
This diversity order emphasises diversity according to set size, so that the smallest sets are the
most significant. This is often the prioritisation since two domains can easily have the same, large,
size without having the same abundance even for k = 2.
Hashemi and Endriss Hashemi and Endriss (2014) considered, among others, two basic diversity
indices. The simple support-based index ∆supp(D) = |D| − 1 and the support-based index ∆k
supp =
| ∪A⊂X,|A|=k Supp(D(A))| −
in A and minimized if as many agents as possible use a single order from D(A). This index can either
be seen as measuring how uniform the agent’s preferences are on A or as a proxy for how robustly
(k, s)-abundant the profile is. Here the robustness refers to how many preference orders would one
have to change in order to reduce the abundance for k-subsets.
The highest entropy is seen for maximal group-separable domains. For each pair of alternatives
each group-separable domain is partitioned on two equal-sized parts with different preferences over
the pair of alternatives.
For each subset of alternatives of size k, each group-separable domain is
partitioned on 2k−1 equal-sized parts with different preferences over the subset alternatives.
A less egalitarian alternative for comparing domains with equal abundance vector is to use the
sum of the abundances for all k-subsets: P
|A|=k |D(A)| or, for a profile, the sum of the entropies
P
|A|=k |H(D(A))|.
6.1
Experimental analysis
In order to demonstrate the concepts developed in this paper we applied them to both profiles based on
empirical data and profiles based on sampling from different types of known domains, both Condorcet
and not.
Our empirical data comes from the AGH data set2 from Mattei and Walsh (2013). This is a data
set based on the choices of courses for a group of students during two consecutive years. There were
9 available courses in 2003 and 7 in 2004. The first year had 123 students and the second 70. Neither
year leads to a CD. This data set was also used for simulations in Hashemi and Endriss (2014).
We first investigate the empirical data as it is. The support is not a Condorcet domain for either
of the two years. Using the full data set and all alternatives the two years give abundance vectors
(1, 1, 2, 6, 16, 39, 75, 101),
(1, 1, 2, 6, 16, 39).
The domains are non-ample due to a single course which every student takes. If we delete that course
both years lead to an ample domain. If we delete the mandatory course and restrict to the 5 most
popular courses we get the vectors
(1, 2, 5, 15, 31),
(1, 2, 6, 16, 32).
Here we see that despite the first profile having more students, 123 vs 70, it is the second one which
has higher abundance.
We have also made a study for different kinds of randomly sampled profiles, similar to that in
Hashemi and Endriss (2014). We generate random samples from six different models, and in each case
we generate domains with n = 7 alternatives and N = 50 agents. In the first model we pick preferences
uniformly at random from Black’s single-peaked domain for n = 5. In the second model we pick orders
from the 2004 AGH data restricted by removing alternatives 8 and 9. In the third model we pick orders
from the unrestricted domain for n = 7, a probabilistic model known at the Impartial culture. In the
next models we pick preferences from Fishburn’s alternating domain, a group-separable/symmetric
domain, and a (5,18)-abundant domain of size 96.
In Figure 3 we show the empirical distribution for the abundance when 2000 domains are sampled
uniformly from each model. For both k = 3 and k = 4 we see that the unrestricted domain leads to
the largest expected abundance, and the single-peaked domain with given axis to the lowest. Among
Condorcet domains the group-separable domains (caterpillar group-separable, see Faliszewski et al.
(2022)) give the highest expected abundance, except for k = 6. In this domain different suborders
emerge with equal probabilities, and we have high abundance with a higher probability, than in domains
in which diversity is based on several rare orders. For k = 6, (5,18)-abundant domain surpasses the
group-separable domain. It confirms the superiority of the investigated domain.
The data-based model has higher diversity than Condorcet domains, and since the support is not
a Condorcet domain one might expect more varied preferences there.
Most of the abundance distributions are unimodal or bell-type for the chosen parameters. However,
for each size k we will see the abundance concentrating on the maximum possible value for the under-
lying domain when the number of agents is large enough. In particular the classical Coupon collector
2PrefLib. https://www.preflib.org/dataset/00009
9
problem shows that if the support is (k, s)-abundant then a uniformly sampled profile will typically also
be that if the number of agents substantially exceeds s log s. However, for a well-structured support
one might see such concentration already for smaller numbers of agents.
The abundance (diversity) axis is an additional dimension which can be used to distinguish statis-
tical cultures (see the survey Szufa et al. (2023) about statistical cultures). Other such distinguishing
indices have been studied but unlike e.g. the distance-based indices utilized in Faliszewski et al. (2023)
abundance calculation is quite simple, as long as k is moderate in size.
1
2
3
5
6
500
1000
1500
2000
(a) k = 3
5
10
15
20
500
1000
1500
(b) k = 4
10
15
20
25
30
35
40
200
400
600
800
1000
(c) k = 5
10
20
40
200
400
600
(d) k = 6
Figure 3: Frequencies for given abundances. Dotted is Black’s single-peaked domain, solid is 2004 AGH,
dot-dashed is single-crossing, the thick solid line is Fishburn, the orange line is the symmetric/group-
separable domain, red is the (5,18)-abundant domain, and dashed is the unrestricted domain/Impartial
culture.
7
Conclusion
This paper links the study of local diversity to Condorcet domains. For a given number of alternatives
n, we determine the Condorcet domains that exhibit the highest possible diversity for their least
diverse restriction. The results are valid for an unlimited number N of agents. For n ≥ 7, several
of these domains are novel and have not been previously explored, while for smaller values of n,
Fishburn’s alternating domains serve as examples of maximum diversity. Notably, in several instances,
the domains with the highest local diversity are not the ones with the greatest number of preference
orders, underlining that maximizing the number of preference orders and maximizing diversity are two
separate challenges for Condorcet domains.
We have shown that for restrictions to subsets of a fixed size k, the abundance cannot be higher
than (k, 2k−1) if n is large enough. This demonstrates that even though Condorcet domains can be
much larger than Black’s single-peaked domain, their abundance for small subsets cannot be higher
than that of Black’s single-peaked domain, when the number of alternatives is large.
Acknowledgement
Bei Zhou was funded by the China Scholarship Council (CSC). The Basic Research Program of the
National Research University Higher School of Economics partially supported Alexander Karpov. This
10
research utilised Queen Mary’s Apocrita HPC facility, supported by QMUL Research-IT.
Compliance with Ethical Standards
Conflict of interest The authors have no relevant financial or non-financial interests that could
potentially compromise the integrity, impartiality, or credibility of the research presented in this paper.
References
””, “Maximal Condorcet Domains,” http://abel.math.umu.se/˜ klasm/Data/CONDORCET/.
Akello-Egwell, Dolica, Charles Leedham-Green, Alastair Litterick, Klas Markstr¨om, and Søren Riis
(2023) “Condorcet Domains of Degree at most Seven,” arXiv preprint arXiv:2306.15993.
Arrow, Kenneth J (1950) “A difficulty in the concept of social welfare,” Journal of political economy,
58 (4), 328–346.
(1963) Social choice and individual values, 12: Yale university press, 2nd edition.
Black, Duncan (1948) “On the Rationale of Group Decision-making,” Journal of Political Economy,
56 (1), 23–34, http://www.jstor.org/stable/1825026.
Brandt, F., P. Lederer, and S. Tausch (2023) “Strategyproof social decision schemes on super Condorcet
domains,” in Proc. of the 22nd International Conference on Autonomous Agents and Multiagent
Systems, 1734–1742.
Campbell, D.E. and J.S. Kelly (2003) “A strategy-proofness characterization of majority rule,” Eco-
nomic Theory, 22 (3), 557–568.
Conlon, David, Jacob Fox, and Benny Sudakov (2010) “Hypergraph Ramsey numbers,” J. Amer.
Math. Soc., 23 (1), 247–266, 10.1090/S0894-0347-09-00645-6.
Faliszewski, P., A. Kaczmarczyk, K. Sornat, S. Szufa, and T. Was (2023) “Diversity, Agreement, and
Polarization in Elections,” Arxiv preprint arXiv:2305.09780.
Faliszewski, P., A. Karpov, and S Obraztsova (2022) “The complexity of election problems with group-
separable preferences,” Autonomous Agents and Multi-Agent Systems volume, 36, 18.
Fishburn, Peter (1997) “Acyclic sets of linear orders,” Social choice and Welfare, 113–124.
Gibbard, A. (1973) “Manipulation of Voting Schemes: A General Result,” Econometrica, 41 (4),
587–601, http://www.jstor.org/stable/1914083.
Hashemi, V. and U. Endriss (2014) “Measuring Diversity of Preferences in a Group,” in Proc. of the
21st European Conference on Artificial Intelligence, 423–428.
Karpov, A., K. Markstr¨om, S. Riis, and B. Zhou (2023) “Bipartite peak-pit domains,” Arxiv preprint
arXiv:2308.02817.
Karpov, A. and A. Slinko (2023) “Constructing large peak-pit Condorcet domains,” Theory and De-
cision, 94 (1), 97–120.
Kim, K.H, F.W Roush, and M.D Intriligator (1992) “Overview of mathematical social sciences,” The
American Mathematical Monthly, 99 (9), 838–844.
Leedham-Green, Charles, , Klas Markstr¨om, and Søren Riis (2023) “The Largest Condorcet Domain
on 8 Alternatives,” arXiv preprint arXiv:2303.15002.
Mattei,
Nicholas
and
Toby
Walsh
(2013)
“PrefLib:
A
Library
of
Preference
Data
http://preflib.org,”
in Proceedings of the 3rd International Conference on Algorithmic
Decision Theory (ADT 2013), Lecture Notes in Artificial Intelligence: Springer.
11
McKay, Brendan D. and Stanis law P. Radziszowski (1991) “The first classical Ramsey number for
hypergraphs is computed,” in Proceedings of the Second Annual ACM-SIAM Symposium on Discrete
Algorithms (San Francisco, CA, 1991), 304–308: ACM, New York.
Puppe, C. (2018) “The single-peaked domain revisited: A simple global characterization,” Journal of
Economic Theory, 176, 55–80.
Puppe, Clemens and Arkadii Slinko (2023) “Maximal Condorcet domains. A further progress report,”
A Further Progress Report.
Ramsey, F. P. (1930) “On a Problem of Formal Logic,” Proceedings of the London Mathematical
Society, s2-30 (1), 264–286, https://doi.org/10.1112/plms/s2-30.1.264.
Raynaud, H. (1981) Paradoxical results from Inada’s conditions for majority rule: Institute for Math-
ematical Studies in the Social Sciences, Stanford University.
Satterthwaite, Mark Allen (1975) “Strategy-proofness and Arrow’s conditions: Existence and cor-
respondence theorems for voting procedures and social welfare functions,” Journal of Economic
Theory, 10 (2), 187–217, https://doi.org/10.1016/0022-0531(75)90050-2.
Sen, A. K. (1966) “A Possibility theorem on majority decisions,” Econometrica, 34 (2), 491–499.
Slinko, A. (2019) ”Condorcet domains satisfying Arrow’s single-peakedness,” Journal of Mathematical
Economics, 84, 166–175.
Slinko, A., Q. Wu, and X Wu (2021) “A characterization of preference domains that are single-crossing
and maximal Condorcet,” Economics Letters, 204, 109918.
Szufa, S., P. Faliszewski, P. Skowron, A. Slinko, and N. Talmon (2023) “Drawing a Map of Elections
in the Space of Statistical Cultures,” in Proc. of the 19th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS 2020), 1341-1349.
Zhou, Bei, Klas Markstr¯om, and Søren Riis (2023) “CDL: A fast and flexible library for the study of
permutation sets with structural restrictions,” arXiv preprint arXiv:2309.06306.
Zhou, Bei and Søren Riis (2023) “New Record-Breaking Condorcet Domains on 10 and 11 Alterna-
tives,” arXiv preprint arXiv:2303.06524.
12
"
"In geometric design, for rational parameterizations of curves, their angular speed is desired to be as uniform as possible to obtain more suitable parameterizations for certain applications. Therefore, given a rational parameterization, one often wants to find a reparameterization with improved uniformity. One approach is to utilize piecewise rational reparameterization. However, these methods do not work well when the angular speed of the rational parameterization is zero at some points on the curve. To overcome this challenge, the authors propose to use piecewise radical transformations instead of rational ones. This idea is inspired by the fact that radical transformations allow for increasing the angular speed toward the average value at points where it vanishes. The effectiveness of the proposed method is demonstrated through a numerical example.","Parameterizations of curves and surfaces are fundamental objects extensively used in computer aided geometric design. Different parameterizations of a given curve or surface may possess different properties, making some more desirable for certain applications than others. Therefore, re-parameterization, or converting one parameterization into another, is often necessary. Among different parameterizations, uniform (angular-speed) parameterizations, where the distribution of points is determined by the local curvature, have been studied extensively. However, existing methods for uniformizing angular speed either produce irrational reparameterizations or do not handle cases where the angular speed is zero at some points on the curve. This limitation stems from the fact that rational transformations cannot increase the angular speed at these points towards the average value.","The authors provide a comprehensive review of previous research on uniform parameterizations, including methods for computing uniform reparameterizations. They highlight the limitations of existing methods in handling cases with zeros in the angular speed function. Some of the key works referenced in the review include [4, 6, 8, 11, 13, 14, 12].nannannan","To overcome the challenge of zeros in the angular speed function, the authors propose using piecewise radical transformations instead of rational ones. The rationale behind this approach is that radical transformations allow for increasing the angular speed toward the average value at points where it vanishes. The authors first introduce the concept of elementary piecewise radical transformation and show that it can be used to ensure that the angular speed of the transformed curve is nonzero everywhere. They then develop a strategy for optimizing the choice of intermediate points in the transformation to maximize the uniformity of the angular speed. Finally, they propose an optimization method for determining the parameters of a piecewise Möbius transformation to further improve the uniformity of the angular speed.nan","The authors demonstrate the effectiveness of their proposed approach through a numerical example, where they apply it to a cubic curve with a zero in its angular speed function. The results show that the proposed method can significantly improve the angular speed uniformity, outperforming existing methods that use rational transformations. The authors also provide plots illustrating the equi-sampling and angular speed functions of the original and transformed curves.","The authors successfully address the challenge of improving angular speed uniformity for parameterizations with zeros in the angular speed function by employing piecewise radical transformations. Their method effectively increases the angular speed toward the average value at points where it vanishes, resulting in more uniform parameterizations. The numerical example demonstrates the significant improvement achieved by the proposed approach compared to existing methods. The authors acknowledge the potential for numerical instability when implementing the algorithm with floating-point arithmetic and suggest techniques to avoid such issues. Overall, this work contributes to the field of geometric design by providing a novel and effective method for improving the uniformity of angular speed parameterizations.",Improving Angular Speed Uniformity by Piecewise Radical Reparameterization,"Hoon Hong, Dongming Wang, Jing Yang","P. Quaresma and Z. Kovács (Ed.): Automated Deduction
in Geometry 2023 (ADG 2023).
EPTCS 398, 2024, pp. 165–178, doi:10.4204/EPTCS.398.19
© H. Hong, D. Wang & J. Yang
This work is licensed under the
Creative Commons Attribution License.
Improving Angular Speed Uniformity
by Piecewise Radical Reparameterization
Hoon Hong
Department of Mathematics
North Carolina State University
Box 8205, Raleigh, NC 27695, USA
hong@ncsu.edu
Dongming Wang
LMIB – IAI – School of Mathematical Sciences
Beihang University
Beijing 100191, China
Dongming.Wang@cnrs.fr
Jing Yang*
SMS – HCIC – School of Mathematics and Physics
Center for Applied Mathematics of Guangxi
Guangxi Minzu University
Nanning 530006, China
yangjing0930@gmail.com
For a rational parameterization of a curve, it is desirable that its angular speed is as uniform as
possible. Hence, given a rational parameterization, one wants to find re-parameterization with better
uniformity. One natural way is to use piecewise rational reparameterization. However, it turns out that
the piecewise rational reparameterization does not help when the angular speed of the given rational
parameterization is zero at some points on the curve. In this paper, we show how to overcome the
challenge by using piecewise radical reparameterization.
1
Introduction
Parametric curves and surfaces are fundamental objects that are most frequently used in computer aided
geometric design. A given curve or surface may have many different parameterizations, of which some
may possess better properties and thus are more suitable for certain applications than the others. Thus,
one often needs to convert one parameterization into another, i.e., to re-parameterize the given parame-
terization (see, e.g., [1, 2, 3, 5, 6, 7, 8, 9, 10]). In this paper, we focus our investigation on an important
class of parameterizations, called uniform (angular-speed) parameterizations, where the distribution of
points are determined by the local curvature and show how to construct such reparameterizations for a
specific class of curves.
Uniform parameterization has been studied in a series of papers (see [4, 6, 8, 11, 14, 13, 12] and
references therein). The authors have defined a function of angular speed uniformity to measure the
quality of any given parameterization of a plane curve and proposed a method to compute its uniform
reparameterization. However, the computed reparameterization is irrational in most cases (with straight
lines as exceptions). For the sake of efficiency, a framework has been proposed for the computation of
rational approximations of uniform parameterizations [4]. Four different methods of reparameterization
(i.e., optimal reparameterization with fixed degree, C0 and C1 optimal piecewise reparameterization,
and nearly optimal C1 piecewise reparameterization) have been integrated into this framework. They
have also been generalized to compute uniform quasi-speed reparameterizations of parametric curves in
n-dimensional space.
*Corresponding author.
166
Piecewise Radical Reparameterization
However, there is still a major challenge: all the above-mentioned methods do not work well when
the angular speed of the given rational parameterization is zero at some points on the curve. This is due
to an intrinsic property of the angular speed function [13]:
Let ωp be an angular speed function of a curve p and r be a proper transformation. Then
ωp◦r = (ωp ◦r)·r′.
(1)
Uniformizing the angular speed can be seen as modifying the angular speed value at each point under the
constraint (1) iteratively until all the values are equal to the average. However, the constraint indicates
that ωp◦r will never reach the average value for any rational r′ when ωp(t) = 0 for some t.
In this paper, we propose to overcome the challenge by using radical transformations instead of
rational ones. We show that radical transformations allow one to increase the angular speed toward the
average value at the points where the angular speed is zero. Then we adapt the idea of piecewise Möbius
transformation from [11] and the strategies in [12] to optimally improve the uniformity of angular speed.
Experiments show that the proposed approach can improve the angular speed uniformity significantly
when the angular speed of the given parameterization vanishes at some point on the curve.
The rest of the paper is structured as follows. In Section 2, we formulate the problem precisely.
For this, we also introduce all the needed notations and notions. In Section 3, we develop mathemat-
ical theory to tackle the problem. In particular, we show how to use piecewise radical transformation
to transform an angular speed function with zeros into one without zero in such a way that the param-
eters involved are also optimized. In Section 4, we summarize the theoretical results into an algorithm
and illustrate its performance on an example. In Section 5, we briefly discuss implementational is-
sues/suggestions when floating point arithmetic is used.
2
Problem
Consider a regular parametric curve
p = (x1(t),...,xn(t)) : R 7→ Rn.
Its angular speed ωp is given by the following expression (see [4]):1
ωp =
s
∑
1≤i<j≤n

x′′
i
x′′
j
x′
i
x′
j

2
n
∑
i=1
x′
i2
.
(2)
Recall that the mean µp and the variation σ2
p of ωp are given by
µp =
Z 1
0 ωp(t)dt,
and
σ2
p =
Z 1
0 (ωp(t)− µp)2 dt.
Definition 1 The angular speed uniformity up of a parameterization p is defined as
up =



1
1+σ2p/µ2p
if µp ̸= 0,
1
otherwise.
(3)
1The concept of angular speed is defined in the same manner as the one in physics but for p′(t). The reason is to make the
angular speed independent of the origin.
H. Hong, D. Wang & J. Yang
167
Example 2 (Running) Consider the parametric curve p = (t,t3). Then
ωp =
6t
9t4 +1,
µp .= 1.249 and up .= 0.846. The goal is to find a proper parameter transformation r over [0,1] in order
to increase the uniformity.
Recall the following results from [4]. For any proper parameter transformation r over [0,1], we have
ωp◦r(s) = (ωp ◦r)(s)·r′(s)
(4)
and
up◦r = µ2
p
ηp,r
,
where
ηp,r =
Z 1
0
ω2
p
(r−1)′ (t)dt.
(5)
By [13, Theorem 2], one can construct a uniform reparameterization from p, but such a reparameter-
ization is irrational in most cases. Therefore, we proposed several methods in [4] to improve the angular
speed uniformity by computing piecewise rational reparameterizations. However, those methods are not
applicable to curves whose angular speed may vanish over [0,1]. Intuitively speaking, uniformizing the
angular speed over [0,1] can be viewed as getting all the values of ωp(t) (for all t ∈ [0,1]) as close to µp
as possible.
If r is a continuous rational function over [0,1], then r′ is bounded. Suppose that ωp(t0) = 0 for some
t0 ∈ [0,1] and µp ̸= 0. Then by (4), there must exist some s0 ∈ [0,1] such that ωp◦r(s0) = 0, which is
not close to µp at all. This makes rational proper parameter transformations invalid. In what follows,
we resort to radical transformations and develop a new approach to uniformize the angular speed of
parametric curves which has zeros over [0,1].
Let p be a parametric curve. Without loss of generality, we assume that
p′(t) = (x′
1(t),...,x′
n(t)) =
X1(t)
W(t) ,··· , Xn(t)
W(t)

,
where Xi(t),W(t) ∈ R[t] and gcd(X1(t),...,Xn(t),W(t)) = 1. One can verify that
ωp =
√
F
∑
i
Xi2 ,
where
F = ∑
i̸=j

X′
i
X′
j
Xi
Xj

2
.
Let F be written as F =
168
Piecewise Radical Reparameterization
• ωp(t) ̸= 0 for all t ∈ (ti,ti+1).
Definition 3 (Elementary Piecewise Radical Transformation) Let p be a parametric curve with T,S
defined above. Then ϕ is called an elementary piecewise radical transformation associated to p if ϕ has
the following form:
ϕ(s) =







...
ϕi(s)
if
s ∈ [si,si+1],
...
where
ϕi(s) =







ti +∆ti
µi+1√
˜s
if ωp(ti) = 0;
ti +∆ti(1− µi+1+1√1− ˜s)
if ωp(ti+1) = 0;
ti +∆ti · ˜s
otherwise,
(6)
and ∆ti = ti+1 −ti, ∆si = si+1 −si, ˜s = (s−si)/∆si.
Remark 4
1. It can be verified that ϕ(si) = ti and ϕ(si+1) = ti+1, which implies that ϕ is with C0 continuity.
2. It is allowed that more than one intermediate point lie between two zeros of ωp because it can
reduce the number of radical pieces and thus enhance the efficiency of generating points with the
new parameterization.
It can be shown that ωp◦ϕ(s) ̸= 0 (see Theorem 7). Next let q = p ◦ ϕ and thus q has no inflation
point. We adapt the reparameterization methods from [12] to increase the uniformity of ωq to any value
close to 1. For this purpose, we recall the following piecewise Möbius transformation.
Definition 5 (Piecewise Möbius Transformation) Let p be a parametric curve with S,Z,α defined
above. Then m is called a piecewise Möbius transformation associated to p if m has the following
form:
m(z) =







...
mi(z)
if
z ∈ [zi,zi+1],
...
where
mi(z) = si +∆si ·
(1−αi)˜z
(1−αi)˜z+αi(1− ˜z)
(7)
and ∆zi = zi+1 −zi, ∆si = si+1 −si, ˜z = (z−zi)/∆zi.
The problem addressed in this paper may be formulated as follows.
Problem 6 Given a parametric curve p with ωp(t) = 0 for some t ∈ [0,1], find a radical piecewise
transformation ϕ and an optimal piecewise Möbius transformation m over [0,1] such that
• up◦ϕ◦m .= 1;
• ∀s ∈ [0,1], ωp◦ϕ◦m(s) ̸= 0.
H. Hong, D. Wang & J. Yang
169
3
Theory
3.1
Property of ϕ
Theorem 7 For any s ∈ [0,1], ωp◦ϕ(s) ̸= 0.
Proof: Taking derivative of ϕi, we have
ϕ′
i(s) =













∆ti
µi +1 ·
1
µi+1√
˜sµi · 1
∆si
if ωp(ti) = 0;
∆ti
µi+1 +1 ·
1
µi+1+1p
(1− ˜s)µi+1 · 1
∆si
if ωp(ti+1) = 0;
∆ti
∆si
otherwise.
Next we show that in the above three cases, ωp◦ϕ(s) ̸= 0.
Case 1: ωp(ti) = 0.
Assume that µi = mult(ωp,ti). Then ωp can be written as
ωp = |t −ti|µi · ˜ζ(t),
where ˜ζ(t) > 0 for t ∈ [ti,ti+1]. Therefore,
ωp◦ϕ(s) = |ϕi(s)−ti|µi ·( ˜ζ ◦ϕi)(s)·ϕ′
i(s)
= (∆ti ˜s
1
µi+1 )µi ·( ˜ζ ◦ϕi)(s)·
 ∆ti
µi +1 ·
1
µi+1√
˜sµi · 1
∆si

= ∆tµi+1
i
µi +1 ·( ˜ζ ◦ϕi)(s)· 1
∆si
= ∆tµi+1
i
µi +1 · 1
∆si
· ˜ζ(t) ̸= 0
for s ∈ [si,si+1].
Case 2: ωp(ti+1) = 0.
Assume that µi+1 = mult(ωp,ti+1). Then ωp can be written as
ωp = |ti+1 −t|µi+1 · ˜ζ(t),
where ˜ζ(t) > 0 for t ∈ [ti,ti+1]. Therefore,
ωp◦ϕ(s) = |ti+1 −ϕi(s)|µi+1 ·( ˜ζ ◦ϕi)(s)·ϕ′
i(s)
=
∆ti
µi+1 +1 ·
1
µi+1+1p
(1− ˜s)µi+1 · 1
∆si
= [∆ti(1− ˜s)
1
µi+1+1 ]µi+1
·( ˜ζ ◦ϕi)(s)·
∆ti
µi+1 +1 ·
1
µi+1+1p
(1− ˜s)µi+1 · 1
∆si
= ∆tµi+1+1
i
µi+1 +1 ·( ˜ζ ◦ϕi)(s)· 1
∆si
= ∆tµi+1+1
i
µi+1 +1 · ˜ζ(t)· 1
∆si
̸= 0.
170
Piecewise Radical Reparameterization
Case 3: ωp(ti)ω(ti+1) ̸= 0.
Combining ωp(t) ̸= 0 for t ∈ [ti,ti+1], ∆ti > 0 and ∆si > 0, we have
ωp◦ϕ(s) = (ωp ◦ϕ)(s)·ϕ′(s) = ωp(t)· ∆ti
∆si
̸= 0.
To sum up, we have ωp◦ϕ(s) ̸= 0 when s ∈ [si,si+1]. □
Example 8 (Continued from Example 2) For the cubic curve p = (t,t3) whose angular speed is ωp =
6t
9t4 +1, it is easy to see that t = 0 is a zero of ωp with multiplicity 1. Let T = (0,1) and S = (0,1). Then
the constructed ϕ is ϕ(s) = √s. It follows that
ωp◦ϕ(s) = (ωp ◦ϕ)(s)·ϕ′(s) =
6√s
9s2 +1 ·
1
2√s =
3
9s2 +1
which is nonzero over [0,1].
Remark 9 It may be further deduced that ωp◦ϕ(s) is discontinuous at s = si.
3.2
Choice of T
By Definition 3, T should contain all the zeros of ωp over [0,1] and some intermediate points in the
subintervals separated by the zeros of ωp. One question is how to choose intermediate points to make
the uniformity improvement as significant as possible. In this subsection, we present a strategy similar
to the one introduced in [12] for determining such points.
Recall [13, Theorem 2] which states that the uniformizing parameter transformation rp of p satisfies
(rp)−1 =
Z t
0 ωp(γ)dγ/µp.
Let ϕ be a piecewise radical transformation associated to p. If (rp)−1 and ϕ−1 share some common
properties, we say informally that rp and ϕ are similar to each other.
First of all, the following can be derived:
ϕ−1(t) =







si +∆si · ˜tµi+1
if ωp(ti) = 0;
si +∆si ·[1−(1− ˜t)µi+1+1]
if ωp(ti+1) = 0;
si +∆si · ˜t
otherwise,
(8)
where ˜t = (t −ti)/∆ti. Furthermore,

ϕ−1′ =















∆si
∆ti
·(µi +1)· ˜tµi
if ωp(ti) = 0;
∆si
∆ti
·(µi+1 +1)·(1− ˜t)µi+1
if ωp(ti+1) = 0;
∆si
∆ti
otherwise;
H. Hong, D. Wang & J. Yang
171

ϕ−1′′ =











∆si
∆t2
i
·(µi +1)µi · ˜tµi−1
if ωp(ti) = 0;
− ∆si
∆t2
i
·(µi+1 +1)µi+1 ·(1− ˜t)µi+1−1
if ωp(ti+1) = 0;
0
otherwise.
Note that ϕ has the properties listed below.
• ϕ−1(0) = 0, ϕ−1(1) = 1.
• ϕ−1
i
is monotonic over (ti,ti+1) because (ϕ−1
i
)′(t) ≥ 0 for all t ∈ (ti,ti+1); since ϕ−1 is continuous
over [0,1], ϕ−1 is monotonic over [0,1].
• [ϕ−1
i
]′ is monotonic over (ti,ti+1) because [ϕ−1
i
]′′ has a constant sign over (ti,ti+1).
The above properties indicate that ϕ is composed of some monotonically increasing convex or con-
cave pieces. Moreover, it can be verified that
• r−1
p (0) =
R 0
0 ωp(γ)dγ/µp = 0, r−1
p (1) =
R 1
0 ωp(γ)dγ/µp = 1;
• r−1
p
is monotonic over [0,1] because (r−1
p )′(t) = ωp(t)/µp ≥ 0.
One may observe that ϕ shares the first two properties with rp. If rp possesses the third property of
ϕ, then rp and ϕ are expected to be similar. This inspires us to divide [0,1] into some monotonic intervals
of (r−1
p )′(t) (i.e., ωp). Thus we may try to choose the intermediate ti in T by solving
ωp(ti)ω′
p(ti) = 0.
Note that ωp(t) is nonnegative. Thus 0 is the local minimum value of ωp. In this sense, T consists of all
the local extreme points of ωp and the two boundary points of the unit interval.
With the above operation, rp is divided into some monotonically increasing/decreasing convex or
concave pieces with each piece having a corresponding one in ϕ. Therefore, T can be obtained by
collecting and inserting the zeros of ωp and ω′
p into [0,1] in order.
Example 10 (Continued from Example 8) One may compute that
ω′
p(t) = −6(27t4 −1)
(9t4 +1)2 .
Then the solution of ωp(t)ω′
p(t) = 0 over [0,1] gives us a partition of [0,1], i.e.,
T .= (0,0.439,1).
Furthermore, one may check that the multiplicities of t0,t1,t2 as roots of ωp are 1,0 and 0, respectively.
3.3
Determination of S
Once a partition T of [0,1] is obtained, one can compute the sequence S in various ways. In this subsec-
tion, we present an optimization strategy for the computation of S.
When T is fixed, up◦ϕ becomes a function of si (i = 1,...,N −1). The following theorem provides a
formula for computing the optimal values for si’s.
172
Piecewise Radical Reparameterization
Theorem 11 The uniformity up◦ϕ reaches the maximum when
si = s∗
i = ∑i−1
k=0
√Lk
∑N−1
k=0
√Lk
,
(9)
where
Lk =















∆tk
R tk+1
tk
ω2
p(t)
(µk +1)˜tµk dt
if
ωp(tk) = 0;
∆tk
R tk+1
tk
ω2
p(t)
(µk+1 +1)(1− ˜t)µk+1 dt
if
ωp(tk+1) = 0;
∆tk
R tk+1
tk
ω2
p(t)dt
otherwise.
(10)
The maximum value of up◦ϕ is
u∗
p◦ϕ = µ2
p

η∗
p,ϕ,
where
η∗
p,ϕ =
 
N−1
∑
i=0
√Li
!2
.
Proof: Recall (5). Since µp is a constant for any given p, the problem of maximizing up◦ϕ can be reduced
to that of minimizing
ηp,ϕ =
Z 1
0
ω2
p
(ϕ−1)′ (t)dt =
N−1
∑
i=0
Z ti+1
ti
ω2
p
(ϕ−1
i
)′ (t)dt.
We first simplify each component in the above equation. Denote
R ti+1
ti
ω2
p
(ϕ−1
i
)′ (t)dt by Ii. Note that
1
(ϕ−1
i
)′(t) =













∆ti
∆si
·
1
µi +1 · 1
˜tµi
if
ωp(ti) = 0;
∆ti
∆si
·
1
µi+1 +1 ·
1
(1− ˜t)µi+1
if
ωp(ti+1) = 0;
∆ti
∆si
otherwise.
When ωp(ti) = 0,
Ii = ∆ti
∆si
·
1
µi +1
Z ti+1
ti
ω2
p
˜tµi dt = Li/∆si.
Similarly, when ωp(ti+1) = 0,
Ii = ∆ti
∆si
·
1
µi+1 +1
Z ti+1
ti
ω2
p
(1− ˜t)µi+1 dt = Li/∆si.
When ωp(ti)·ωp(ti+1) ̸= 0,
Ii = ∆ti
∆si
·
Z ti+1
i
ω2
p dt = Li/∆si.
It is obvious that ηp,ϕ = ∑N−1
i=0 Ii > 0; it increases to +∞ when si approaches the boundary of the feasible
set of parameters. Now we compute the extrema of ηp,ϕ. Let
∂ηp,ϕ
∂si
= 0,
H. Hong, D. Wang & J. Yang
173
i.e.,
Li
∆s2
i
− Li−1
∆s2
i−1
= 0,
where Li is as in (10). Solving the above equation, we obtain
∆si = ∆s∗
i = ∆s∗
0
p
Li/L0.
Note that ∑N−1
i=0 ∆s∗
i = 1. Thus
∆s∗
0 =
 
N−1
∑
k=0
p
Lk/L0
!−1
,
s∗
i =
i−1
∑
k=0
∆s∗
k = ∑i−1
k=0
p
Lk/L0
∑N−1
k=0
p
Lk/L0
= ∑i−1
k=0
√Lk
∑N−1
k=0
√Lk
.
Therefore,
∆s∗
i =
√Li
∑N−1
k=0
√Lk
.
Moreover, the optimal value of ηp,ϕ is
ηp,ϕ = η∗
p,ϕ =
N−1
∑
i=0
Li
∆s∗
i
=
N−1
∑
k=0
Li
√Li
∑N−1
i=0
√Lk
=
 
N−1
∑
k=0
p
Lk
!2
,
from which it follows that the optimal value of up◦ϕ is
up◦ϕ = u∗
p◦ϕ = µ2
p
η∗p,ϕ
=
µ2
p
174
Piecewise Radical Reparameterization
Theorem 13 Let q be a rational parameterization such that ωq(s) ̸= 0 over [0,1] and m be a piecewise
Möbius transformation determined by S, Z and α. For a given sequence S, the uniformity uq◦m reaches
the maximum when
αi = α∗
i =
1
1+
p
Ci/Ai
,
zi = z∗
i = ∑i−1
k=0
√Mk
∑N−1
k=0
√Mk
,
(11)
where
Ai =
Z si+1
si
ω2
q ·(1− ˜s)2ds,
Bi =
Z si+1
si
ω2
q ·2˜s(1− ˜s)ds,
Ci =
Z si+1
si
ω2
q · ˜s2ds,
Mk = ∆sk

2
p
AkCk +Bk

.
Let m∗ be the piecewise Möbius transformation determined by S, Z∗ and α∗. Then the maximum value
of uq◦m is uq◦m∗ = µ2
q/ηq,m∗ where
ηq,m∗ =
 
N−1
∑
i=0
p
Mk
!2
.
Remark 14 Let ϕ be an elementary radical transformation as in Definition 3 and q = p◦ϕ. Note that
µq =
Z 1
0 ωqds =
Z 1
0 ωp◦ϕds =
Z 1
0 (ωp ◦ϕ)(s)·ϕ′(s)ds =
Z 1
0 ωpdt = µp.
Thus
up◦ϕ◦m∗ = uq◦m∗ = µ2
q/ηq,m∗ = µ2
p
 
N−1
∑
i=0
p
Mk
!2
.
Let ϕ and q be defined as before. By Theorem 7, ωq ̸= 0 over [0,1]. One may compute the optimal
values of S, α and Z by Theorems 11 and 13. However, p ◦ ϕ is a composition of radical function and
rational function and the composition will cause an increase of complexity because ωq is radical. In what
follows, we simplify the formulae for Ai,Bi and Ci with the goal of computing the values of Ai,Bi and Ci
directly from p.
The formula of Ai (0 ≤ i ≤ N −1) is derived via the following steps:
Ai =
Z si+1
si
ω2
q ·(1− ˜s)2ds
=
Z si+1
si
[(ωp ◦ϕ)(s)]2 ·[ϕ′(s)]2 ·(1− ˜s)2ds
=
Z si+1
si
[(ωp ◦ϕ)(s)]2 ·[ϕ′(s)]·(1− ˜s)2 
ϕ′(s)ds

=
Z ti+1
ti
ω2
p
(ϕ−1)′ (t)·

1− ϕ−1 −si
∆si
2
dt
by (8)
=

















R ti+1
ti
ω2
p
(ϕ−1)′ (t)·(1− ˜tµi+1)2dt
if ω(ti) = 0;
R ti+1
ti
ω2
p
(ϕ−1)′ (t)·(1− ˜t)2(µi+1+1)dt
if ω(ti+1) = 0;
R ti+1
ti
ω2
p
(ϕ−1)′ (t)·(1− ˜t)2dt
otherwise;
H. Hong, D. Wang & J. Yang
175
=

















∆ti
∆si
R ti+1
ti
ω2
p(t)
(µi +1)˜tµi ·(1− ˜tµi+1)2dt
if ω(ti) = 0;
∆ti
∆si
R ti+1
ti
ω2
p(t)
µi+1 +1 ·(1− ˜t)µi+1+2dt
if ω(ti+1) = 0;
∆ti
∆si
R ti+1
ti
ω2
p(t)·(1− ˜t)2dt
otherwise.
Similarly, we have
Bi =

















∆ti
∆si
R ti+1
ti
ω2
p(t)
µi +1 ·2˜t(1− ˜tµi+1)dt
if ω(ti) = 0;
∆ti
∆si
R ti+1
ti
ω2
p(t)
µi+1 +1 ·2[1−(1− ˜t)µi+1+1](1− ˜t)dt
if ω(ti+1) = 0;
∆ti
∆si
R ti+1
ti
ω2
p(t)·(1− ˜t)2dt
otherwise;
Ci =

















∆ti
∆si
R ti+1
ti
ω2
p(t)
µi +1 · ˜tµi+2dt
if ω(ti) = 0;
∆ti
∆si
R ti+1
ti
ω2
p(t)
(µi+1 +1)(1− ˜t)µi+1 ·[1−(1− ˜t)µi+1+1]2dt
if ω(ti+1) = 0;
∆ti
∆si
R ti+1
ti
ω2
p(t)·(1− ˜t)2dt
otherwise.
Example 15 (Continued from Example 12) With the above formulae and T, S as in Examples 10 and
12, one may obtain the following:
A0 .= 0.258,
B0 .= 0.229,
C0 .= 0.193,
A1 .= 0.518,
B1 .= 0.317,
C1 .= 0.159.
Thus
M0 .= 0.406(2
√
0.258·0.193+0.229) .= 0.274,
M1 .= (1−0.406)(2
√
0.518·0.159+0.317) .= 0.529.
By Theorem 13, we obtain
α0 .= 1/(1+
p
0.193/0.258) .= 0.536,
α1 .= 1/(1+
p
0.159/0.518) .= 0.643,
and z∗
1
.= 0.419. Thus α .= (0.536,0.643) and Z .= (0,.419,1). One may further calculate
uq◦m∗ .=
1.2492
(
√
0.274+
√
0.529)2
.= 0.997.
4
Algorithm
In this section, we summarize the above ideas and results as Algorithm 1 and illustrate how the algorithm
works for the cubic curve in Example 2.
176
Piecewise Radical Reparameterization
Algorithm 1: Optimal_Radical_Transformation
Input:
p, a rational parameterization of a plane curve.
Output:
r, the optimal piecewise radical transformation of p such that up◦r > up.
1.
Compute ωp and µp using (2), up using (3) and ω′
p.
2.
Solve ωpω′
p = 0 and get T.
3.
Compute S, Z, α and u using (9) and (11).
4.
Construct ϕ with T, S and m with S, Z, α using (6) and (7).
5.
r ← ϕ ◦m.
6.
Return r.
Example 16 (Continued from Example 15) Given p = (t,t3), after the above calculation, one may
obtain
T .= (0,0.439,1), S .= (0,0.406,1), Z .= (0,.419,1), α .= (0.536,0.643).
Then one may construct ϕ with T and S, and m with S, Z and α, and obtain
ϕ .=
(
0.688√s
if
0.000 ≤ s ≤ 0.406;
0.055+0.945s
if
0.406 ≤ s ≤ 1.000;
m .=







−0.450z
0.172z−0.536
if
0.000 ≤ z ≤ 0.049;
−0.165z+0.192z
0.492z−0.849
if
0.419 ≤ z ≤ 1.000.
Then the optimal transformation r is constructed below.
r = ϕ ◦m .=







0.462
r
−
z
0.172z−0.536
if
0.000 ≤ z ≤ 0.419;
−0.129z−0.228
0.492z−0.849
if
0.419 ≤ z ≤ 1.000.
With the optimal radical transformation r, one may construct p◦r and obtain
p◦r .=











 
−0.079√z(z−3.116)
(−0.172z+0.5359)3/2 ,
0.098z3/2
(−0.172z+0.536)3/2
!
if
0.000 ≤ z ≤ 0.419;
 
−0.129z−0.228
0.492z−0.849 ,−0.002 (z+1.771)3
(0.492z−0.849)3
!
if
0.419 ≤ z ≤ 1.000.
The angular speed function of p◦r is
wp◦r .=







0.781
z2 −0.420z+0.655
if
0.000 ≤ z ≤ 0.4187;
−1.379 (z+1.771)(z−1.725)
(z2 −1.456z+0.899)(z2 −4.876z+9.888)
if
0.4187 ≤ z ≤ 1.000.
H. Hong, D. Wang & J. Yang
177
Furthermore, one may calculate its uniformity up◦r .= 0.997.
The plots of p and p◦r as well as the behavior of their angular speed functions are shown below:
where
• the left plot shows the equi-sampling of the original parameterization p (green);
• the middle plot shows the equi-sampling of the optimal piecewise radical reparameterization p◦r
(red);
• the right plot shows the angular speed functions of p and p◦r.
It is seen that the angular speed uniformity is greatly improved by the piecewise radical reparameteriza-
tion.
5
Implementational Issues/Suggestions
If one chooses to implement the proposed algorithm using floating-point arithmetic, then, as usual, one
should be careful to avoid numerical instability.
For instance, if Li is computed by (10) naively, then it leads to instability. For example, ti = 1/
√
2 is
a zero of ωp with multiplicity 1, so
ωp(t) = |t −1/
√
2|·m(t),
where m(1/
√
2) ̸= 0. The numeric solution over [0,1] is ti = 0.707. Thus
Li = ∆ti
Z ti+1
ti
ω2
p(t)
(µi +1)˜tµi dt = ∆t2
i
Z ti+1
ti
(t −1/
√
2)2 ·m2(t)
2(t −0.707)
dt.
During integration, it is necessary to evaluate the integral at t = 0.707. When t approaches 0.707, the
integral quickly increases to +∞, causing numerical instability.
To avoid such cases, one could adopt a technique from symbolic computation to represent algebraic
numbers. Suppose that t = γ is a zero of ωp(t) with multiplicity µi and ti is its numerical approximation.
By (2), ω2
p(t) is a rational function. Let G and H be its numerator and denominator. Then t = γ is a zero
of G with multiplicity 2µi. Carrying out the Euclidean division with G as the dividend and (t −γ)2µi as
the divisor, we obtain
G(t) = (t −γ)2µiQ(t,γ)+R(t,γ).
Since t = γ is a zero of G with multiplicity 2µi, it is also a zero of R(t,γ) with multiplicity at least 2µi.
Given that deg(R,t) < 2µi, R(t,γ) must be zero, which leads to the following conclusion:
Li = ∆ti
Z ti+1
ti
ω2
p(t)
(µi +1) ˜tµi dt .= ∆tµi+1
i
µi +1 ·
Z ti+1
ti
Q(t,ti)(t −ti)µi
H(t)
dt.
178
Piecewise Radical Reparameterization
Acknowledgements
Hoon Hong’s work was supported by National Science Foundations of USA un-
der Grant No. 1813340 and Jing Yang’s work was supported by National Natural Science Foundation of
China under Grant Nos. 11526060 and 12261010.
References
[1] P. Costantini, R. T. Farouki, C. Manni & Sestini A (2001):
Computation of optimal composite
re-parameterizations.
Computer Aided Geometric Design 18(9), pp. 875–897, doi:10.1016/S0167-
8396(01)00071-1.
[2] G. Farin (2006): Rational quadratic circles are parametrized by chord length. Computer Aided Geometric
Design 23(9), pp. 722–724, doi:10.1016/j.cagd.2006.08.002.
[3] R. T. Farouki (1997): Optimal parameterizations. Computer Aided Geometric Design 14(2), pp. 153–168,
doi:10.1016/S0167-8396(96)00026-X.
[4] H. Hong, D. Wang & J. Yang (2013): A framework for improving uniformity of parameterizations of curves.
Science China Information Sciences 56(10), pp. 1–22, doi:10.1007/s11432-013-4924-4.
[5] B. Jüttler (1997): A vegetarian approach to optimal parameterizations. Computer Aided Geometric Design
14(9), pp. 887–890, doi:10.1016/S0167-8396(97)00044-7.
[6] M. Kosters (1991): Curvature-dependent parameterization of curves and surfaces. Computer-Aided Design
23(8), pp. 569–578, doi:10.1016/0010-4485(91)90058-5.
[7] X. Liang, C. Zhang, L. Zhong & Y. Liu (2005): C1 continuous rational re-parameterization using monotonic
parametric speed partition. In: Ninth International Conference on Computer Aided Design and Computer
Graphics, IEEE Computer Society, Los Alamitos, CA, USA, pp. 16–21, doi:10.1109/CAD-CG.2005.23.
[8] R. Patterson & C. Bajaj (1989): Curvature adjusted parameterization of curves. Technical Report CSD-TR-
907, Department of Computer Science, Purdue University, US. Available at https://docs.lib.purdue.
edu/cstech/773/.
[9] J. R. Sendra & C. Villarino (2001): Optimal reparameterization of polynomial algebraic curves. International
Journal of Computational Geometry & Applications 11(04), pp. 439–453, doi:10.1142/S0218195901000572.
[10] J. R. Sendra, F. Winkler & S. Pérez-Díaz (2008): Rational algebraic curves. A computer algebra approach.
Algorithms and Computation in Mathematics 22, Springer Verlag, doi:10.1007/978-3-540-73725-4.
[11] J. Yang, D. Wang & H. Hong (2012): Improving angular speed uniformity by optimal C0 piecewise repa-
rameterization. In V. P. Gerdt, W. Koepf, E. W. Mayr & E. V. Vorozhtsov, editors: Computer Algebra in
Scientific Computing, Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 349–360, doi:10.1007/978-3-642-
32973-9_29.
[12] J. Yang, D. Wang & H. Hong (2013): Improving angular speed uniformity by C1 piecewise reparameteriza-
tion. In T. Ida & J. Fleuriot, editors: Automated Deduction in Geometry, Springer Berlin Heidelberg, Berlin,
Heidelberg, pp. 33–47, doi:10.1007/978-3-642-40672-0_3.
[13] J. Yang, D. Wang & H. Hong (2013): Improving angular speed uniformity by reparameterization. Computer
Aided Geometric Design 30(7), pp. 636–652, doi:10.1016/j.cagd.2013.04.001.
[14] J. Yang, D. Wang & H. Hong (2014): ImUp: a Maple package for uniformity-improved reparameterization
of plane curves. In R. Feng, W. Lee & Y. Sato, editors: Computer Mathematics, Springer Berlin Heidelberg,
Berlin, Heidelberg, pp. 437–451, doi:10.1007/978-3-662-43799-5_29.
"
"This paper showcases models of trajectories in space, exploring bicircular and tricircular moves in 2D and 3D, inspired by extraplanetary movements. Increasing complexity is introduced by non-planar settings, and these approaches are utilized for 2D and 3D printed mathematical art. Students' activities are organized around these exploration and creation processes.","The study connects mathematical models with space exploration topics, attracting students' interests in mathematical applications and modeling through reports of space missions, satellite launches, and planetary exploration. It emphasizes the importance of understanding the orbits and trajectories of celestial bodies. Simple models of planetary orbits are introduced.","Relevant literature is not directly referenced in the provided text, so there is no information to extract for the literature review section.nannannan","GeoGebra and Maple software are utilized to create dynamic visualizations and animations of the trajectories. The constructions are explored using slider bars, animated gifs, and commands for rotations and plane transformations. Parameters such as ratios of radii and angular velocities are varied to observe the effects on the shapes and topology of the curves.nan","A range of visualizations and animations are obtained, including epicycloids, epitrochoids, and novel curves not found in catalogs. The exploration of these curves reveals interesting rotational symmetries of odd orders. The parametric equations are analyzed to identify these symmetries, and the data precision and rounding issues are addressed. Math art creations are generated by varying parameters and visualizing the resulting multicolor curves.","The study highlights the benefits of integrating dynamic geometry software and 3D printing technology into mathematical exploration. It demonstrates the potential of STEAM education to engage students in multidisciplinary activities, foster technological skills, and encourage creativity. The 3D printing of selected curves and math art creations provides tangible outcomes for students' understanding and appreciation of mathematical concepts.",3D Space Trajectories and beyond: Abstract Art Creation with 3D Printing,"Thierry Dana-Picard, Matias Tejera, Eva Ulbrich","P. Quaresma and Z. Kovács (Ed.): Automated Deduction
in Geometry 2023 (ADG 2023).
EPTCS 398, 2024, pp. 142–152, doi:10.4204/EPTCS.398.17
© T. Dana-Picard, M. Tejera & E. Ulbrich
This work is licensed under the
Creative Commons Attribution License.
3D Space Trajectories and beyond:
Abstract Art Creation with 3D Printing
Thierry Dana-Picard
Jerusalem College of Technology
Jerusalem, Israel
ndp@jct.ac.il
Matias Tejera
Eva Ulbrich
Johannes Kepler University
Linz, Austria
Mathias.Tejera@jku.at
Eva.Ulbrich@jku.at
We present simple models of trajectories in space, both in 2D and in 3D. The first examples, which
model bicircular moves in the same direction, are classical curves (epicycloids, etc.). Then, we
explore bicircular moves in reverse direction and tricircular moves in 2D and 3D, to explore complex
visualisations of extraplanetary movements. These moves are studied in a plane setting. Then, adding
increasing complexity, we explore them in a non planar setting (which is a closer model of the real
situation). The exploration is followed by using these approaches for creating mathematical art in 2D
and 3D printed objects, providing new ways of mathematical representations. Students’ activities are
organized around this exploration.
1
Introduction
All over the world, newspapers and TV news are full of reports about launching satellites, the Interna-
tional Space Station, the Chinese space station, Mars exploration and the Artemis project to establish a
permanent human presence on the Moon. Nowadays, the NASA offers the public to send their names
on a probe to be launched in 2024 and arrive to Encelade, an icy moon of Jupiter, in 2030. With such
an ubiquitous topic, students asked a lot of questions, about spacecrafts, their trajectories, their trajec-
tories, why these are curved and sometimes complicated, etc. Numerous dedicated websites are freely
accessible, showing representations of trajectories of extraplanetary objects. These are connected to the
students’ cultural background, on which it is worth to rely in order to attract students to mathematics,
and to show applications in real world [4]. This paper explores mathematical situations with a STEAM
approach visualising curves in 2D and 3D with various technologies to use the motivational fascination
of outer space from students to connect to mathematical modelling.
When asking about spacecrafts, they wish to understand the trajectories. Not all the news items
include graphs and maps of the trajectories, but they frequently do so and can be the source of ques-
tions, whence of mathematical activities. These are good reasons for mathematics educators to be part
of this atmosphere, showing complex real world applications of mathematics. Examples could be curves
describing trajectories or calculating the speed of objects in an accessible way by interactive visualiza-
tions and explanatory animations. Students have the opportunity to create and explore these trajectories
themselves by visualising them using mathematical modelling and certain technologies and we present
possible approaches in 2D and 3D.
According to the 1st Kepler’s law (see [8], p. 127), the orbit of a planet around the Sun is an ellipse,
with the Sun at one of the foci. As the foci are very close, actually both inside the Sun,1 in order to make
1Actually, in a system of two objects, both orbit their common center of gravity. The system Sun-Earth’s center of gravity is
inside the Sun, therefore considering the Earth as orbiting the Sun is acceptable. Of course, every other pair Sun-Planet presents
the same situation.
T. Dana-Picard, M. Tejera & E. Ulbrich
143
the example as simple as possible, we consider an approximation of the orbits as coplanar concentric
circles. Kepler’s 2nd law is illustrated by figure 1, taken from [8] p. 129: the areas of the shaded sectors,
covered by the radius in equal times (i.e. it takes equal times to travel distances AB, CD and EF), are
equal.
Figure 1: Kepler’s 2nd law of planetary motion
In our simplified model, we consider motion with constant angular velocity on circular orbits. We
compute the velocities according to the year length of the planet, with Earth year equal to 1. Note that Ta-
ble 1 displays just the eight official planets acknowledged by the international astronomical organization.
According to the 3rd Kepler’s law, the orbital velocity is a function of the distance to the Sun.
Planet
Distance to the Sun (km)
Period (1=terrestrial year)
Mercury
57.91 106 km
0.2408
Venus
108.2 106 km
0.6152
Earth
149.6 106 km
1
Mars
227.9 106 km
1.8808
Jupiter
778.5 106 km
11.862
Saturn
1.434 109 km
29.457
Uranus
2.871 109 km
84.018
Neptune
4.495 109 km
164.78
Table 1: Some orbital data
Because of the huge differences between the distances and the hardware constraints2 (we mean
mostly the size of the screen and the number of available pixels), we will consider examples with Earth
and Mars only. The same activities can be done with the pair Venus-Earth, they will produce the same
family of curves. Note that in order to make the first examples easy, we use approximations less precise
than in Table 1.
The visualisations we explore in this paper are created by two softwares called GeoGebra and Maple
to utilise their respective strengths. We use GeoGebra,3 whose main characteristic is devoted to Dynamic
Geometry. For some applications, including automated determination of loci and envelopes, it can be
supplemented with the package GeoGebra-Discovery.4 A general analysis of the automated methods for
2A general study of constraints, either of the hardware or of the software can be found in [9], with some extension in [2].
3Freely downloadable from http://www.geogebra.org.
4Look for the last version, freely downloadable from https://github.com/kovzol/geogebra-discovery.
144
3D Space Trajectories, Art and 3D Printing
loci and envelopes is given in [6]. We will also use the Computer Algebra System Maple for its specific
animated affordances, which are different from those of GeoGebra.
Exploration of curves obtained as trajectories of points modeling moves in space, such as midpoint
or center of gravity of two planets, is described and analyzed in [3,5]. The present paper is a new contri-
bution, with more complex constructions. Its goal is to present mathematical situations with a STEAM5
approach, where plane curves, either algebraic or not, are presented and some of their properties explored
using technology. We see this kind of study as an opportunity to connect to the classical families of plane
curves in a motivating manner for students and can be used as a unifying frame for cases previously seen
as separate cases. Later, space curves given by similar parametric equations are explored, also modelling
spatial phenomenon. The ratio of the mean radii of two neighboring planets (such as Venus-Earth, or
Earth-Mars) is huge, and still more the ratio between the mean radius of the Earth orbit around the Sun
(about 149 Mkm) and the radius of the Moon’s orbit around the Earth (about 360,000 km), it is impos-
sible to represent both on a computer screen. Therefore, we chose to work with arbitrary6 coefficients,
whose variations provide different curves. We explore the composition of two circular movements in the
same direction (the general case in the Solar System). Figure 2 shows a simplified model of a spacecraft
flying to Mars, without explicit presentation of the orbiting direction around Mars.7
Figure 2: Trajectory from the Earth to the Mars orbit
We also consider the composition of 3 circular movements, inspired by lunar orbiters and observed
also from the Sun: they orbit the Moon, which orbits the Earth, which in its turn orbits the Sun. In
Subsection 2.2, we explore the composition of 3 circular movements, all in the same direction. In Sub-
section 2.3, we explore also models of a composition of movements, two in one direction and the 3rd
5STEAM = acronym for Science, Technology, Engineering, Arts and Mathematics.
6By arbitrary, we mean coefficients enabling a representation on the screen, not taken from the orbital data.
7Credit: NASA/JPL, https://marspedia.org/File:InSight_Trajectory.jpg
T. Dana-Picard, M. Tejera & E. Ulbrich
145
in reversed direction. The motivation for this is provided by the trajectories of spacecrafts to the Moon;
figure 3 shows a diagram of the trajectory of the Artemis 1 spacecraft, elliptic around the Earth, followed
by a transfer orbit made of arcs of ellipses, then elliptic around the Moon in reversed direction.8 The
geometric locus of the moving object around the Moon, when observed from the Sun, may be an epicy-
cloid, a hypocycloid or another already known curve. Here, more “exotic” curves are also explored and
plotted; in particular, rotational symmetries of order 7, 11, 13, etc. may be discovered. This provides an
opportunity for an interactive exploration of such symmetries.
Figure 3: Artemis orbit to the Moon and around (Credit: NASA)
Jablonski [7] says that “Mathematical modelling is characterized through its interplay of reality and
mathematics. It offers a way to integrate references to reality into the classroom and shows students
where in everyday life their mathematical knowledge can be applied.” Therefore, we started from real
world situations utilizing the amazement created by media reports. The first examples provide some
understanding of how the orbit of the Moon around the Sun looks like, but quickly we explored compo-
sitions of movements without a connection to reality. Changing the parameters (either ratio of radii or
ratio of angular velocities) induces important changes on the shape and topology of the curves. Tricir-
cular moves are inspired by, for example, lunar orbiters (which orbit the Moon, which orbits the Earth,
which in its turn orbits the Sun), or Mars orbiters. As already mentioned, figure 2 shows the trajectory
of Mars Orbiter, from start to arrival: at first ellipses around the Earth, then a transfer orbit (made of arcs
of ellipses), then elliptic orbits around Mars. This can be explained to students.
Instead of returning from models to the real world situation, which had to be understood, numerous
new directions are possible. As an example, curves of degree 8, obtained from a construction discon-
nected from the physical data, have been explored recently; see [6].
Finally, we explore artistic creation using these mathematical models. We obtain curves presenting
non usual symmetries and explore them using our software.
In the real world of the software may change. The exploration of the curves is an important incentive
to 3D print them. We quote once again Jablonski [7]: “The idea of involving real objects in mathematical
modelling leads to the question of how much the way in which a real object is introduced might influ-
ence the modelling processes of students. Despite its actual physical presence in reality, a real object
could be introduced through different representations and provided artefacts, e.g., newspaper articles,
8The trajectories of future Artemis missions will be different from this one, but based on the same principle.
146
3D Space Trajectories, Art and 3D Printing
photographs, videos, 3D print replications or combinations. Potentially, the different representations of
the real object might lead to differences in the modelling activities of students and motivate a comparison
of them.”
2
Classical Curves and beyond
In all the examples we consider a planet (let us call it the Earth, orbiting the Sun at distance 1 (a reference
to 1 astronomical unit, 1 AU) at constant velocity, and completing 1 orbit in 1 year. The other coefficients
describe the mean radius of another planet and the length of its own year. The is described by the
following parametric presentation:
(x,y) = (cosu, sinu), u ∈ R.
(1)
For the animations with software, u ∈ [−0,2π] is enough with repetitive animation. We denote the pa-
rameter by u, as in GeoGebra t has a special role. The second planet is described by
(x,y) = r

cos u
h, sin u
h

, u ∈ R,
(2)
where r > 0 denotes the radius of the planet’s orbit and h encodes the length of its year.
2.1
Epicycloids in 2D and Extension towards 3D
Figure 4 shows a screenshot of a dedicated GeoGebra applet.9 The parameters can be changed with
slider bars. The figure on the left shows the trajectories in the plane containing the Sun, the planet and
its satellite. Here the satellite orbits the planet 12 times a year, almost modelling the Moon around the
Earth. The figure on the right shows a simulation when the Sun travels on a straight line; note that the 3
objects remain all the time in a plane which moves according to the Sun. For the 2D representation, the
orbits can be either plotted in a non-animated way using GeoGebra’s Locus command or to be animated
using the corresponding option of the slider bar. Nite that other way to animate the constructions are
available.
As already mentioned, the Sun is also mobile, it has its own orbit. Figure 4(a) shows a model where
the Sun moves along a segment of line. The planet and its satellite move in a plane containing the 3
objects (this plane is visible in blue). The commands are similar in 3D as in 2D. Note that the projection
on the plane is on display in the adjacent window. This is due to the total synchronisation of the 3D and
2D windows in GeoGebra. This task was an incitement to go to 3D printing.
A similar animation can be programmed with Maple. The code is easy: each object is defined in a
separate plot[animate] command, them all together they are displayed using the display command. The
Sun has two commands: one for plotting a large dot, the other one to plot the trajectory. An animated
gif can be obtained with a right-click on the output of the display command. A screenshot is shown in
figure 4(b).
c1 := spacecurve([cos(t) + 1/5*cos(12*t), sin(t) + 1/5*sin(12*t), t],
t = 0 .. 4*Pi, thickness = 3, labels = [x, y, z]):
sun := plots[animate](spacecurve, [[0, 0, t], t = 0 .. A], A = 0 .. 4*Pi,
thickness = 3, color = yellow)
sunplo := plots[animate](pointplot3d, [[0, 0, A]], A = 0 .. 4*Pi,
9See https://www.geogebra.org/m/ksyd6hat.
T. Dana-Picard, M. Tejera & E. Ulbrich
147
(a) GeoGebra
(b) Maple
Figure 4: A satellite around a planet orbiting the Sun
color = orange, symbol = sphere)
planet := plots[animate](spacecurve, [[cos(t), sin(t), t], t = 0 .. A],
A = 0 .. 4*Pi, thickness = 3, color = navy)
sat := plots[animate](spacecurve, [[cos(t) + 1/5*cos(12*t), sin(t)
+ 1/5*sin(12*t), t], t = 0 .. A], A = 0 .. 4*Pi,
color = sienna, labels = [x, y, z]):
display(sun, planet, sat, sunplo)
2.2
Three Circular Movements with Constant Angular Velocity – Same Direction
Figure 5 shows snapshots of GeoGebra sessions based on the Locus command. Subfigure (c) is a snap-
shot of a GeoGebra applet10 with 2 parameters encoding the distances. A further step consists in adding
parameters to change the ratios of angular velocities.
(a)
(b)
(c)
Figure 5: Screenshots of a tricircular motion in the same direction
10See https://www.geogebra.org/m/sagpjzzb.
148
3D Space Trajectories, Art and 3D Printing
2.3
Three Circular Movements with Constant Angular Velocity – One in Reverse Direc-
tion
Figure 6 displays 3 curves obtained with the Locus command, in an applet11 where all the parameters
can vary. In what follows, we explore the symmetries of the obtained curves. These symmetries are often
of odd order, a situation which is not frequent in classroom.
(a) A 4-star
(b) A strange star
(c) A bat curve
Figure 6: Screenshots of a tricircular motion with the middle in reversed direction
We consider now the family of curves whose parametric equations are as follows:
(x,y,z) = (cosu,sinu)+ 1
3(cosau,sinau)+ 1
2(sinbu,ccosbu),
(3)
where a,b encode the ratios of circular velocities. In the applet https://www.geogebra.org/m/jug
rcbx5, their increment is defined to be 1.
For a = b = 1, the curve is an ellipse. But there are other cases, maybe more interesting. Figure
7(a) has been obtained for (a,b) = (6,14) with the Curve command. It presents a 5-fold rotational
symmetry, i.e. it is invariant under a rotation whose center is the origin and of angle 2π/5. This has been
checked with a plot of the parametric equations for u ∈ [0,2π/5], the applying the automated command
for rotations. The colors have to be manually adapted to create visualisations that are easier to interpret
where each curve has a unique style. Part of GeoGebra’s algebraic display can be seen in figure 7(a) to
illustrate what has been done. The curve can also be plotted defining a variable point depending on the
parameter u, then applying the Locus command. The definition of a variable point provides a dynamic
plot of the curve, but both in this case and with the Locus command, the output is not a geometric object
on which a plane transformation can be applied.
Other cases have to be cautiously explored for symmetries. For example, the case (a,b) = (10,14)
shows a 3-fold symmetry (see figure 7(b). Experomentation will show that this also true for (a,b) = (7,14)
and (a,b) = (7,17).
2.4
Math Art Creation
The applet mentioned in the previous subsection has been opened, running animations for the parameters
a and b separately. Exploration has been preformed according the following steps:
• The entire curve is plotted, using Trace On;
• Analyzing the graphical display, the existence of rotational symmetry is conjectured;
11See https://www.geogebra.org/m/xgrx7ntx.
T. Dana-Picard, M. Tejera & E. Ulbrich
149
(a) 5-fold
(b) 3-fold
Figure 7: Tricircular moves creating multicolor curves with rotational symmetries
• The rotational symmetry is checked by first reducing the plot to a subset of the interval chosen
for the parameter; we mean taking an interval of the form [0,2π/m], where m is te order of the
conjectured symmetry, and then using the automated command for a rotation about the origin with
angle 2π/m.
• Of course, this has to be checked afterwards by symbolic means, using a substitution.
Later, an experiment has been made, choosing an arbitrary number m, not the order of the rotational sym-
metry which has been discovered. The obtained multicolor plot does not describe a specific mathematical
situation. Some of the results are displayed in figure 8.
(a)
(b)
(c)
Figure 8: Some random math art creations
Discovering such creations has been greeted with enthusiasm by the audience of lectures delivered
by the authors, whose topics was linked to curves and math art.
3
Some More Remarks
The starting point of the study is STEAM oriented, namely using a scientific model from an item in
the news. Students may have prior interest in the domain, without having a strong knowledge. The
present topic offers an opportunity to collaborate between educators, between man and machine, of
course between students. The study output is multiple, and among the “rewards” we have:
150
3D Space Trajectories, Art and 3D Printing
• Acquisition of new mathematical knowledge: classical curves (epicycloids, epitrochoids, etc.),
which are not part of the regular curriculum, have been discovered and studied. Epitrochoids are
members of a larger family of curves, which involves roses, epicycloids, etc. Activities as in this
work may be a nice incitement to explore other situations and to broaden horizons. The literature
describes generally the epitrochoids for integer values of the parameters, and our experimentations
showed also more general settings.
• Discovery of new curves; we mean curves which do not appear in the catalogues such as [10].
• Emphasis on the importance of the data precision (in space, contrary to most classrooms, nothing
is measured by integers) and of rounding. We considered non integer ratios of radii of orbits, and
of orbital angular velocity, approximations and rounding became an important issue. We could
discover that different precisions in the approximation yield very different output. This is probably
a central outcome of this work: students do not always believe that mastering errors is important,
and they believe that the answers provided by a numerical calculator are always accurate. Asking
them which answer is true among the cases that we studied with different rounding should lead at
least to some questioning.
• Development on new technological skills, which are part of the new mathematical knowledge [1].
• Emphasis on multidisciplinary tasks, whence development of STEAM skills.
Note that generally, modeling is intended to construct mathematical descriptions of a concrete situa-
tion. Then, the model is applied to enhance more understanding of the concrete situation. The process is
summarized in figure 9.
Figure 9: A classical diagram for a modelling process
In the present paper, we go in a totally different way in this case: modeling a concrete astronomical
situation (orbits), the activities provide more abstract curves without a physical meaning. Finally 3D
printing could provide both outcomes: a concrete object modeling planets and trajectories, and also
T. Dana-Picard, M. Tejera & E. Ulbrich
151
some pieces of visual art, either in 2D or in 3D to apply constructivist as well as constructionist ideas.
This is summarized in figure 10.
Figure 10: A modelling process leading in other directions
We performed the same experiments and constructions using Maple. The characteristics of the work
CAS is slightly different.
• After a command line to define a parametric curve, an animate command has to be entered. Its
output is not immediately visible.
• A left-click on the graphical window is necessary, and it switches automatically to the row of
graphical buttons.
• Here too, the relevant values for the parameters (number of frames, speed, etc.), in order to obtain
a significant graphical output have to be experimentally looked for, using the buttons.
• Other modifications of the output may require changes in the written commands.
After having presented some of the applets to a certain audience, the authors decided to 3D print
part of them, together with some other cases. In parallel, tasks have been defined for groups of students,
either gifted High-School students having benefit of an extension of the curriculum, or undergraduates.
These students belong to two different countries. The tasks include the 3D printing of some examples.
The transfer of the CAS output to a 3D printer requested the translation of this output into a language
that the 3D printer understands. In our presentation, we will report on the math part and on the outcome
of the activities with students.
4
Acknowledgements
The first author was partially supported by the CEMJ Chair at JCT.
152
3D Space Trajectories, Art and 3D Printing
References
[1] M. Artigue (2002): Learning Mathematics in a CAS Environment: The Genesis of a Reflection about Instru-
mentation and the Dialectics between Technical and Conceptual Work. International Journal of Computers
for Mathematical Learning 7(3), pp. 245–274, doi:10.1023/A:1022103903080.
[2] T. Dana-Picard (2007): Motivating constraints of a pedagogy embedded Computer Algebra System. Interna-
tional Journal of Science and Mathematics Education 5(2), pp. 217–235, doi:10.1007/s10763-006-9052-9.
[3] T. Dana-Picard (2022): The loci of virtual points constructed with elementary models of planetary orbits. In:
Electronic Proceedings of the Asian Conference on Technology in Mathematics ACTM 2021, Mathematics
and Technology.
[4] T. Dana-Picard & S. Hershkovitz (2022): STEAM Education: technological skills, students’ cultural back-
ground and Covid-19 crisis. Open Education Studies 2(1), pp. 171–179, doi:10.1515/edu-2020-0121.
[5] T. Dana-Picard & S. Hershkovitz (2024): From Space to Maths and to Arts: Virtual Art in Space with
Planetary Orbits. to appear in Electronic Journal of Mathematics & Technology.
[6] T. Dana-Picard & T. Recio (2023): Dynamic construction of a family of octic curves as geometric loci. AIMS
Mathematics 8(8), pp. 19461–19476, doi:10.3934/math.2023993.
[7] S. Jablonski (2023): Is it all about the setting? – A comparison of mathematical modelling with real objects
and their representation. Educational Studies in Mathematics 113(2), doi:10.1007/s10649-023-10215-2.
[8] H. Karttunen, P. Kröger, H. Oja, M. Poutanen & K.J. Donner, editors (2008): Fundamental Astronomy.
Springer, doi:10.1007/978-3-662-53045-0.
[9] L. Trouche (2000): La parabole du gaucher et de la casserole à bec verseur: étude des processus
d’apprentissages dans un environnement de calculatrices symboliques. Educational Studies in Mathemat-
ics 41(3), pp. 239–264, doi:10.1023/A:1003939314034.
[10] R. Yates (1947): A Handbook on Curves and their Properties. J.W. Edwards, MI: Ann Arbor.
"
"We present a case study of automated geometry reasoning as a part of a STEM/STEAM project involving prospective mathematics teachers. The project stemmed from examining the movement of a rocking camel installed at a medical center. Initial conjectures and experimentation led to the development of a precise mathematical model in GeoGebra Discovery, which enabled automated reasoning to prove that the hump of the camel moves along a sextic curve. The project highlights the role of automated reasoning in extending mathematical knowledge and the potential for combining real-life applications with geometry reasoning in education.",Automated geometry reasoning is available in various software tools. GeoGebra Discovery is a software tool that aims to reach secondary schools with its intuitive user interface.,We focus on the symbolic support of the Dilate command and tool that was added in GeoGebra Discovery version 2023Feb01. This makes it easy to divide a line segment in a given ratio.nannannan,"The project consisted of several experiments that were discussed in a group of prospective mathematics teachers at the Private University College of Education of the Diocese of Linz in Upper Austria during the winter semester 2022/23 in the frame of a course that focuses on exploiting technology in mathematics education. The students made an exact measurement of the toy and its parts, modeled the toy in GeoGebra and traced the movement of the hump, made a conjecture, and showed the locus of the trace points. They also made a second conjecture, computed the mathematical equation of the locus, checked the conjectures, and generalized the problem with different inputs.nan",The conjecture that the movement of the hump of the camel is an ellipse was disproven. The students obtained a sextic polynomial equation for the locus of the trace points. They were able to generalize the problem setting using sliders and the Dilate command in GeoGebra.,"The paper argues for the possible classroom use of GeoGebra Discovery on the one hand, and also for activities that combine real-life applications and automated geometry reasoning. These tools can help extend mathematical knowledge and encourage students to explore the connection between geometry and real-world applications.",The Locus Story of a Rocking Camel in a Medical Center in the City of Freistadt,"Anna Käferböck, Zoltán Kovács","P. Quaresma and Z. Kovács (Ed.): Automated Deduction
in Geometry 2023 (ADG 2023).
EPTCS 398, 2024, pp. 132–141, doi:10.4204/EPTCS.398.16
© A. Käferböck & Z. Kovács
This work is licensed under the
Creative Commons Attribution License.
The Locus Story of a Rocking Camel
in a Medical Center in the City of Freistadt
Anna Käferböck
The Private University College of Education of the Diocese of Linz, Austria
anna.kaeferboeck@gmx.at
Zoltán Kovács
The Private University College of Education of the Diocese of Linz, Austria
zoltan.kovacs@ph-linz.at
We give an example of automated geometry reasoning for an imaginary classroom project by using
the free software package GeoGebra Discovery. The project is motivated by a publicly available toy,
a rocking camel, installed at a medical center in Upper Austria. We explain how the process of a false
conjecture, experimenting, modeling, a precise mathematical setup, and then a proof by automated
reasoning could help extend mathematical knowledge at secondary school level and above.
1
Introduction
Automated reasoning in geometry is available in various software tools for several years, mostly in prover
packages. In this paper we pay our attention to a non-trivial presence of a geometry prover in the software
tool GeoGebra Discovery [4,5] that aims at reaching secondary schools with its intuitive user interface.
Most importantly, we give a report on a STEM/STEAM project that was discussed in a group of
prospective mathematics teachers at the Private University College of Education of the Diocese of Linz
in Upper Austria during the winter semester 2022/23, in the frame of a course that focuses on exploiting
technology in mathematics education (36 students in 2 working groups). This project consisted of sev-
eral other experiments that were already communicated by the second author. The discussed activity, a
detailed study of the movement of a rocking camel, is however, completely new. Also, some major im-
provements in the underlying software tool (implemented by the second author with a substantial help of
the students’ feedback), makes it much easier to model similar project setups and conclude mathematical
knowledge in an automated way.
2
GeoGebra Discovery and its Automated Reasoning Tools
GeoGebra Discovery is a fork of GeoGebra,12 a de facto standard tool that supports mathematics edu-
cation at various levels of learners. GeoGebra 5.0 and above come with a built-in automated reasoning
subsystem. The supported commands: Prove, ProveDetails, LocusEquation and Envelope are further
developed in GeoGebra Discovery by an addition of various other commands like Discover, Compare
and RealQuantifierElimination. Also, several improvements of the existing commands are included.
1GeoGebra is an interactive geometry, algebra, statistics and calculus application, intended for learning and teaching math-
ematics and science from primary school to university level, available at https://geogebra.org.
2GeoGebra Discovery is available at https://kovzol.github.io/geogebra-discovery.
A. Käferböck & Z. Kovács
133
Figure 1: The exhibited toy in the medical center of the city of Freistadt, Upper Austria.
Most importantly, we focus on the symbolic support of the Dilate command and tool that was added
in GeoGebra Discovery version 2023Feb01.3 It can be used to dilate an object from a point (which is
the dilation center point), using a given factor, a rational number. This makes it easy to divide a line
segment in a given ratio. Formerly, for such constructions the intercept theorem, or a consecutive use of
midpoints, reflections or rotations had to be used. As a further result, the user can use the slider feature
of GeoGebra (which is a numerical tool) and at the same time precise discovery and proofs (which are
symbolic tools) can be automatically obtained now.
Our paper argues for the possible classroom use of GeoGebra Discovery on the one hand, and also
for activities that combine real-life applications and automated geometry reasoning.
3
A Rocking Camel
The toy shown in figure 1 is exhibited in a medical center in Freistadt, Upper Austria. It is installed for
amusement purposes for children who are waiting for medical treatments. An obvious question is, from
the mathematical point of view, to identify the movement of certain points of the camel. Clearly, some
points of the camel move on circular paths. For example, the suspension points, close to the legs, move
along a circle. When asking for a general point of the camel, however, some non-trivial movements
can show up. For example, the movement of the hump of the camel seems to move on an elliptical
path, after the first experiments are performed by using a former version of the GeoGebra applet at
https://geogebra.org/m/b8mbjxcz (Fig. 2).
Here we can also note that points E and H (they are the above-mentioned suspension points close to
the legs of the camel) indeed move on a circular path. In fact, here the lengths of the quadrilateral ABHE
are given with the segments f and g, and point M was constructed by consecutive use of midpoints
and a rotation by 90 degrees. At a later point of our paper we will use a better approach, based on the
improvements on the Dilate command.
3See https://github.com/kovzol/geogebra/releases/tag/v5.0.641.0-2023Feb01.
134
The Locus Story of a Rocking Camel in a Medical Center in the City of Freistadt
Figure 2: A former version of a GeoGebra applet that suggests that the motion of the hump of the camel
is a part of an ellipse.
3.1
The History of the Rocking Camel
As a part of our project we researched after on how the camel got to the medical center. In fact, the camel
was in the attic for many decades and no one knew what it was all about. It was probably passed down
from generation to generation. Unfortunately, this much is known about it.
How many more toys are there in the attic that have a nice mathematical background but we have
forgotten about them? How many forgotten mathematical books, writings and ideas are there that the
modern age has put aside and not even superficially exploited their excitement?
We believe that our contribution will help dust off these forgotten gems and put them at the service
of education today.
4
A STEM Activity
The abbreviation STEM stands for “science, technology, engineering and mathematics” [8]. It is a popu-
lar approach to teach mathematics via real-life applications. Sometimes STEM is extended with an “A”
(“arts”) and it becomes the abbreviation “STEAM”. Later we will learn how this engineering experiment
can be extended to an artistic activity.
Now, the main question of the activity, raised to the prospective mathematics students, was to describe
the movement of the hump of the camel by following the steps below:
1. Make an exact measurement of the toy and its parts. (As a first approach, this was prepared by
a student by providing photos. Later, with another student, more exact data was collected in the
medical center, by using measuring tapes, a camera and some graphical analysis with technology.)
A. Käferböck & Z. Kovács
135
Figure 3: A small lamp mounted on a battery with cables. It can be attached to a moving object by a glue
tape.
2. Model the toy in GeoGebra and trace the movement of the hump. (The students already had an
acceptable background of GeoGebra knowledge to make it possible to do experiments on their
own.)
3. Make a conjecture. (Here most students conjectured that the movement was an ellipse.)
4. Show the locus of the trace points. (We will see later that the conjecture was wrong, because the
trace shows a different curve, namely, something like a form “8”).
5. Make a second conjecture. (This was a very difficult question, since an 8-formed curve is not
present in the curriculum, neither at secondary nor university level.)
6. Compute the mathematical equation of the locus. (This is easy by using the command or tool
LocusEquation. Without this step, no satisfactory conjecture can be done.)
7. Check the conjectures. (This is possible by setting up an equation system by using pencil and
paper, and then compute the locus curve by using technological means. For this problem, however,
the students skipped this step. It was used to check a different problem, publicized in the LEGO
4094 set as the “moving monkey” [7].)
8. Generalize the problem with different inputs. (In general, we have a 4-bar linkage problem [2] that
leads to a sextic movement.)
In the next subsections we give some details on the steps described above.
4.1
Exact Measurements
After measuring the distances among the most significant parts of the camel we mounted a small lamp
with a battery on the camel (Fig. 3). Then we switched the light off and recorded the movement with
a camera of a mobile phone. When attaching the lamp to the top of the camel, we can get a motion
like shown in figure 4. These pictures were created after saving individual frames (25 images) with
136
The Locus Story of a Rocking Camel in a Medical Center in the City of Freistadt
Figure 4: Motion of the lamp, by using two steps of preprocessing.
Figure 5: An attempt to identify the motion as an ellipse.
the VLC media player4 and then opening them in GIMP.5 Then the individual layers were edited with
the “Exposure” function by changing the value of Black level to 0.1 (instead of 0.0). Furthermore the
background of all layers was removed with the help of the function “Color by Alpha”, so that only the
red light was left and the single layers did not cover each other anymore.
4.2
Modeling in GeoGebra
An option to continue with is to try to fit a curve on the output. This is well-supported in GeoGebra
by the possibility to insert a transparent figure, making it as a background picture, and then create some
free points by hand that approximately cover the curve. GeoGebra’s ImplicitCurve command can find
the best fitting implicit polynomial (see figure 5 or https://geogebra.org/m/c93pegab for an
online applet): for a curve of degree n one needs to enter n·(n+3)
2
input points. That is, if we expect that
the motion follows an ellipse (which is of degree 2), then 5 points are required. During the university
course, however, we followed a different path. In GeoGebra we constructed the drawing as in figure 2
by creating free points A = (0,0), B = (15,0), then creating a segment CD with length f = 5.5, drawing
a circle c with center A and radius f, and another circle d with center B and radius f. Then we attached
point E on c, and after this step we created another segment FG with length g = 12. Next, we drew a
third circle e with center E and radius g. One of the intersection points of d and e was designated to be
point H. Then, as mentioned above, point M was created with some further steps by halving and rotating
some additional points.
4VLC media player is a free and open-source, portable, cross-platform media player software and streaming media server
developed by the VideoLAN project, available at https://www.videolan.org/vlc.
5GIMP (GNU Image Manipulation Program) is a free and open-source raster graphics editor used for image manipulation
(retouching) and image editing, free-form drawing, transcoding between different image file formats, and more specialized
tasks, available at https://www.gimp.org.
A. Käferböck & Z. Kovács
137
Figure 6: Dragging point E in an unrealistic position to disprove that the searched motion is elliptical.
We remark that this construction is a special case of a planar 4-bar linkage, which is well-known in
the study of mechanisms, and has important applications like Watt’s steam engine or a pumpjack.
An exact GeoGebra model helped the students to make experiments with the linkage without visiting
the medical center and making their own measurements.
4.3
A First Conjecture
The students had one week of working time to make a conjecture. Several learners made a false con-
jecture, however, because they had no idea that there could be a solution other than the ellipse. This
also raises the general question of the pedagogical consequences of oversimplifying the mathematical
modeling of world problems.
4.4
A Numerical Locus
Some students, however, continued dragging point E to unrealistic positions and they obtained visual
evidence that the searched curve is clearly not an ellipse (Fig. 6). This can also be checked in the above
mentioned applet by enabling the “Locus” checkbox.
4.5
A Second Conjecture
At this point, a second conjecture could be made, but due to the lack of ideas, we more or less skipped
this step. In fact, if you do not know the concept of higher degree curves, there is no chance to have a
conjecture that the output is a polynomial curve.
4.6
A Symbolic Locus
This step can be reproduced by enabling the “LocusEquation” checkbox in the above mentioned ap-
plet. We obtain, by using some computer algebra (which is not further explained in this step) a sextic
polynomial equation,
256·1014x6 −1152·1017x5y2 −768·1014x4y2 −312·1015x4y+... = 0
138
The Locus Story of a Rocking Camel in a Medical Center in the City of Freistadt
Figure 7: Obtaining a symbolic locus equation.
(Fig. 7). Here the students can only rely on the underlying computer algebra system, it is just a black
box, but the coincidence of the numerical and symbolic loci can confirm, at least, partially, that the
computations are hopefully correct.
4.7
A Proof
Now we need to prove that the obtained curve is indeed a sextic. To achieve this, we can set up an
equation system with equations a2 +b2 = 5.52 (here E = (a,b)), (c−15)2 +d2 = 5.52 (here H = (c,d)),
(a − c)2 + (b − d)2 = 122, and for obtaining the coordinates M we might compute the coordinates of
the midpoint I of segment EH and then rotate E around H by −90 degrees to get E′. Having E′, the
midpoint J of IE′ can help to create the midpoint K of JE′, and midpoint L of KE′. Finally, M is the
midpoint of LM. This process is, of course, quite complicated, but it shows how we can be arbitrarily
close to any point of the camel, by using just simple geometric operations. Later, by using dilations, this
will be easier.
Now, by using elimination from algebraic geometry we can obtain the locus equation by using Geo-
Gebra’s Eliminate command. This is still a black box operation, but at least the students can have an idea
what the exact input is, and the teacher can argue that by using the first three basic operations (addition,
subtraction and multiplication), there is a finite algorithm [1] that indeed produces the result.
And this is actually a proof, in the deepest sense of the notion. Even if the atomic steps of the
computation remain hidden, a reliable computer algebra system on reliable hardware will indeed compute
the expected equation of the searched curve.
Let us highlight this fact even more. In classical geometry we are used to proofs that give arguments
why the studied outputs are certain curves like lines, circles or maybe ellipses. The argumentation is
sometimes purely synthetic, but sometimes analytic. Here we cannot really give a synthetic argumen-
tation why a sextic curve appears. Only an analytic proof is applicable. But, because of the technical
difficulty of the proof there is no way to check each step in a manual way. Therefore, a computer assisted
proof is required, and as such, the automated way of elimination is satisfactory.
A. Käferböck & Z. Kovács
139
Figure 8: Generalization with sliders via the Dilate command.
4.8
Generalization
With some feedback from the students it was possible to improve GeoGebra Discovery to support gener-
alizing the problem setting in the following way: How does the output curve change when the lamp has
a different position than the hump of the camel?
To achieve this, the Dilate command in GeoGebra required symbolic support. The applet at https:
//matek.hu/zoltan/camel.php (see figure 8) allows the user to conveniently change the length
of the bars AE and BH (they are still equally long) and the bar EH. By using dilation and sliders, the
background computation requires less variables, because instead of 4 free variables just one needs to
be used. This speeds up the computation substantially. To avoid the difficult way of defining M we
introduced two sliders Mx and My that help find the position of M in an intuitive way. In addition,
the user is notified immediately when the locus equation changes by using GeoGebra’s JavaScript API6
(Fig. 9).
This applet was created by the use of the Dilate command. Dilation allows the user to create
an arbitrary linear combination of two vectors. The coefficients of the linear combination can usually
be rational numbers. Using one direct step to define ratios of certain quantities, instead of using the
intercept theorem or utilizing midpoints, helps simplify the construction and avoid slow computation
because of the high amount of variables. As well-known, elimination may be double exponentially slow
in the number of variables in its worst case [6]. Therefore each optimization step may be crucial.
As a conclusion, the students can have a general conjecture after some further experiments, that 4-
bar linkages usually yield sextic curves [3]. Of course, such experiments are insufficient to get a general
6Available at https://wiki.geogebra.org/en/Reference:GeoGebra_Apps_API.
140
The Locus Story of a Rocking Camel in a Medical Center in the City of Freistadt
Figure 9: A particular sextic equation, the corresponding curve is plotted in figure 8.
proof for all possible parameters. And, in fact, in some degenerate cases these results are actually not
true, for example if the construction collapses into one point.
5
Final Thoughts and Conclusion
Automated geometric proofs may play an even more important role as before at secondary school level
and above. The concept of analytic proofs (instead of synthetic ones) can already be familiar with
algebraization of the geometric setup. For example, the well-known theorem by Thales that highlights a
connection between right triangles and their circumcircles, can be easily translated into an algebraic setup
and proven without difficulty. Indeed, let A = (−1,0), B = (1,0), C = (x,y), and assume that x2 +y2 = 1,
that is, C lies on a circle whose diameter is segment AB. Now, checking if AC is perpendicular to BC
means exactly that (x−(−1))·(x−1)+(y−0)·(y−0) = 0, and this is equivalent with our assumption
on the sum of squares. That is, after making sure that the algebraization is performed correctly and
generally enough, some algebraic manipulation will give the required argumentation.
Such an easy derivation is, unfortunately, not always possible. But we can learn that it is possible
to formulate also the converse of the statement, that is, to ask: What is the geometric locus of points
(x,y) such that AC is perpendicular to BC, when A and B are fixed? And here we conclude that the
searched equation is x2 + y2 = 1, a quadratic one, in particular, the equation of a circle. In general,
however, we may obtain non-linear and non-quadratic results as well. In our example in this paper we
obtained a sextic equation, with huge coefficients. And this can happen in many other situations. Real
life examples (of study of mechanisms, or optics) are full of higher degree polynomial curves. Here we
mention conchoids, cissoids, strophoids (of degree 3) or cardioids, deltoids or lemniscates (of degree 4),
many of them already well-known by the ancient Greek mathematicians.
In such higher degree cases, a proof that a certain curve is the expected result is nothing else than a
long elimination process. Even if the computations are hidden, we expect that each step of the derivation
is performed correctly, and therefore the result is correct.
That is, STEM/STEAM education cannot avoid such proofs in the long term. But, luckily, the exist-
ing tools are already safe and rich enough to support the learners in both the exploration and verification.
Acknowledgments
We are grateful to students Eva Erhart and Engelbert Zeintl for their help in many aspects of this paper.
Benedek Kovács kindly helped us in preparing the photo of the rocking camel for further work in Geo-
Gebra. He also helped in the preparation of the mounted small lamp. The second author was partially
A. Käferböck & Z. Kovács
141
supported by a grant PID2020-113192GB-I00 (Mathematical Visualization: Foundations, Algorithms
and Applications) from the Spanish MICINN.
References
[1] Bruno Buchberger (2006): Bruno Buchberger’s PhD thesis 1965: An algorithm for finding the basis elements
of the residue class ring of a zero dimensional polynomial ideal. Journal of Symbolic Computation 41, pp.
475–511, doi:10.1016/j.jsc.2005.09.007.
[2] Kenneth Henderson Hunt (1990): Kinematic Geometry of Mechanisms, 2 edition. 7, Oxford Engineering
Science Series.
[3] Zoltán Kovács, Tomás Recio & M. Pilar Vélez (2020): Reasoning about linkages with dynamic geometry.
Journal of Symbolic Computation 97, pp. 16–30, doi:10.1016/j.jsc.2018.12.003.
[4] Zoltán Kovács, Tomás Recio & M. Pilar Vélez (2021): Automated reasoning tools in GeoGebra Discovery.
ACM Communications in Computer Algebra 55(2), pp. 39–43, doi:10.1145/3493492.3493495.
[5] Zoltán Kovács, Tomás Recio & M. Pilar Vélez (2021): GeoGebra Discovery in Context. In Predrag Janiˇci´c
& Zoltán Kovács, editors: Proceedings of the 13th International Conference on Automated Deduction in Ge-
ometry, Hagenberg, Austria/virtual, September 15-17, 2021, Electronic Proceedings in Theoretical Computer
Science 352, Open Publishing Association, pp. 141–147, doi:10.4204/EPTCS.352.16.
[6] Ernst W. Mayr & Albert R. Meyer (1982): The Complexity of the Word Problem for Commutative Semigroups
and Polynomial Ideals. Advances in Mathematics 46, pp. 305–329, doi:10.1016/0001-8708(82)90048-2.
[7] Reinhard Oldenburg (2008): FeliX – mit Algebra Geometrie machen. Computeralgebra Rundbrief, Sonderheft
zum Jahr der Mathematik. Available at http://www.fachgruppe-computeralgebra.de/data/JdM-200
8/Sonderheft.pdf.
[8] Bryan Edward Penprase (2020): STEM Education for the 21st Century. Springer Cham, doi:10.1007/978-3-
030-41633-1.
"
"Using automated reasoning tools in GeoGebra Discovery, we address a problem from the Austrian Mathematics Olympiad. It allows us to demonstrate the truth of the problem, discover a generalization, discover degenerate cases, and gauge problem complexity. The combination of these features makes GeoGebra Discovery a useful tool for teaching and learning.",Automated reasoning tools (ART) offer valuable insights into solving mathematical problems. The GeoGebra and GeoGebra Discovery software enables us to examine how ART behaves regarding various aspects of a problem from the Austrian Mathematics Olympiad.,"ART provides a benchmark to test the performance of humans and machines, evaluates the performance of recently proposed measures of geometric statements' complexity, and offers an exemplar role in showcasing the advantages and limitations of GeoGebra Discovery.nannannan","We demonstrate how GeoGebra Discovery confirms the truth of a problem, enables discovery of a generalization, encounters difficulties analyzing degenerate cases, and estimates the problem's complexity.nan","GeoGebra Discovery confirms the truth of the problem, reveals a more general assertion, unveils challenging degenerate cases, and assigns a complexity grade to the problem, indicating its difficulty level.","GeoGebra Discovery's strength lies in providing immediate solutions, assessing complexity, aiding in conjecture development and confirmation, and promoting a shift in focus toward analyzing and constructing locus characteristics, rather than just finding equations and graphs.",Solving with GeoGebra Discovery an Austrian Mathematics Olympiad problem: Lessons Learned,"Belén Ariño-Morera, Zoltán Kovács, Tomás Recio, Piedad Tolmos","P. Quaresma and Z. Kovács (Ed.): Automated Deduction
in Geometry 2023 (ADG 2023).
EPTCS 398, 2024, pp. 101–109, doi:10.4204/EPTCS.398.13
© B. Ariño-Morera et al.
This work is licensed under the
Creative Commons Attribution License.
Solving with GeoGebra Discovery an Austrian Mathematics
Olympiad Problem: Lessons Learned
Belén Ariño-Morera
Departamento de Economía Financiera y Contabilidad, Universidad Rey Juan Carlos, Madrid, Spain
belen.arino@urjc.es
Zoltán Kovács
The Private University College of Education of the Diocese of Linz, Austria*
zoltan.kovacs@ph-linz.at
Tomás Recio
Escuela Politécnica Superior, Universidad Antonio de Nebrija, Madrid, Spain
trecio@nebrija.es
Piedad Tolmos
Departamento de Economía Financiera y Contabilidad, Universidad Rey Juan Carlos, Madrid, Spain
piedad.tolmos@urjc.es
We address, through the automated reasoning tools in GeoGebra Discovery, a problem from a re-
gional phase of the Austrian Mathematics Olympiad 2023. Trying to solve this problem gives rise to
four different kind of feedback: the almost instantaneous, automated solution of the proposed prob-
lem; the measure of its complexity, according to some recent proposals; the automated discovery of
a generalization of the given assertion, showing that the same statement is true over more general
polygons than those mentioned in the problem; and the difficulties associated to the analysis of the
surprising and involved high number of degenerate cases that appear when using the LocusEquation
command in this problem. In our communication we will describe and reflect on these diverse is-
sues, enhancing its exemplar role for showing some of the advantages, problems, and current fields
of development of GeoGebra Discovery.
1
Introduction
In the past years we have been developing and including, both in the standard version of GeoGebra1
as well as in the fork version GeoGebra Discovery2, different automated reasoning tools (ART) [1].
Mathematics Olympiads problems provide an interesting benchmark for testing the performance of such
instruments. Interesting, from multiple perspectives: by itself, as a source of challenging mathematical
questions that our ART should be able to deal with; as a test to measure and compare the performance of
humans and machines on the same problems [2],[3]; as a test on the suitability of the recently proposed
intrinsic measure of complexity of a geometric statement [4].
In this communication we will focus on how our ART behave concerning some of these issues, when
addressing an interesting problem recently proposed in a regional phase of the Austrian Mathematics
*The work was partially supported by the grant PID2020-113192GB-I00 from the Spanish MICINN.
1www.geogebra.org
2https://kovzol.github.io/geogebra-discovery/
102
GeoGebra Discovery and an Austrian Mathematics Olympiad Problem
Figure 1: Problem 2. Österreichische Mathematik-Olympiade Regionalwettbewerb für Fortgeschrittene
30. März 2023.
Olympiad 2023. Namely, Problem 2 at the 54. Austrian Mathematics Olympiad 2023,3 stated as follows
(see Figure 1):
Sei ABCD eine Raute mit ̸ BAD < 90◦. Der Kreis durch D mit Mittelpunkt A schneide
die Gerade CD ein zweites MaI im Punkt E. Der Schnittpunkt der Geraden BE und AC sei
S.
Man beweise, dass die Punkte A,S,D und E auf einem Kreis liegen.
that is
Let ABCD be a rhombus with ̸ BAD < 90◦. The circle through D with center A intersects
straight line CD a second time at point E. The intersection of the lines BE and AC is S.
Prove that the points A,S,D and E lie on a circle.
In the next section we will show how GeoGebra Discovery is able to confirm (by proving internally,
in a mathematically rigorous way) the truth of the proposed problem.
2
Solving the Problem
In what follows, to exemplify how GeoGebra Discovery handles this problem, we will use in different
moments, the web version GeoGebra Discovery 6 or the app GeoGebra Discovery 5. To start with,
Figure 2 shows how GeoGebra Discovery version 6.0.641.0-2023Apr22 confirms the truth of the given
statement.
3Österreichische Mathematik-Olympiade Regionalwettbewerb für Fortgeschrittene, 30. März 2023. https://oemo.at/
OeMO/Termine/2023/
B. Ariño-Morera et al.
103
Figure 2: Confirming the truth of the statement in Problem 2.
First, we have chosen some free points A,B, then the circle c centered at A through B, then another
point D on this circle, such that ̸ BAD < 90◦. Next, we have built the (hidden) segment f = BD, and
point C as the symmetrical of A with respect to f. Thus, ABCD is a rhombus. Finally, points E,S are
displayed, following the hypotheses, as the intersection of line CD and c (ditto, as the intersection of line
BE and AC).
Then we have introduced the commands
Prove(AreConcyclic(A,S,D,E))
and
ProveDetails(AreConcyclic(A,S,D,E)),
yielding in both cases just the declaration that the statement is true. See items a,b in the input column in
the Figure 2 or in Figure 3. Notice that Figure 3 displays the circle through A,S,D,E, even if the angle
̸ BAD > 90◦. Indeed, we have not made any formal implementation of such restriction, and this implies
the statement holds even without such conditions. Thus, we can say we have already proved an extended
version of the given problem.
Let us conclude by remarking two more things: one, that the internal proof is mathematically rigor-
ous, dealing with symbolic computation algorithms (not through a numerical or probabilistic approach);
two, that the complexity of the involved computation has made impossible to output the list of asso-
ciated degeneracy conditions (i.e. geometric situations that must be avoided for the statement to hold
true, like having ABCD aligned) that could have been displayed (in simpler cases) on the output of the
ProveDetails command.
104
GeoGebra Discovery and an Austrian Mathematics Olympiad Problem
Figure 3: Confirming the truth of the statement in Problem 2 and showing the circle through A,S,D,E.
3
Generalizing the Problem
Conceptually speaking, GeoGebra Discovery is prone to offer tools that help, not only to check the
truth/falsity of a given assertion, but to automatically test the verification of a large collection of prop-
erties among the elements that the user is adding in the construction of the geometric figure. That is,
to help the user discovering different properties holding under the given hypotheses. Figure 4 shows,
in colors, different properties that the program has discovered, after enabling the StepwiseDiscovery
command, along each of the steps towards the construction of the rhombus, of the points E,S, etc.
For instance, just after introducing free points A,B and point D in the circle c, GeoGebra Discovery
tells the user that segments f = DA, t = BA have equal length, a trivial result, since A is the center
of circle c and B,D are points on c. Perhaps more interesting is to learn that circle d contains points
A,B,C,E, i.e. that these points are concyclic; or that t = BS, a = AS, b = DS have equal length, a couple
of non-trivial results that GeoGebra Discovery outputs automatically towards the end of the construction,
without being asked to consider such specific relations.
We would like to focus on some other way, perhaps less automatic but more relevant for the user, of
discovering results with GeoGebra Discovery. Namely, let us assume the user is interested in a precise
question: finding necessary conditions for the converse of the given Problem 2. This can be addressed
through the LocusEquation(AreConcyclic(D,E,A,S),C) command. Figure 5 shows the output of
this command, a very complicated degree-10-equation that (seems) to be the product of eight lines and
the circle c. The locus described by this equation includes all positions of a free point C such that
A,S,D,E are concyclic (where A,S,D,E are defined repeating the construction of the previous figures,
except for point C, that here is just a free point, not the mirror of A with respect to the line BD).
Let us remark that the output is a numerical equation, with coefficients depending on the coordinates
B. Ariño-Morera et al.
105
Figure 4: Confirming the truth of the statement in Problem 2, and many others, through the Stepwise-
discover command.
Figure 5: Locus of C (in red color) for the concyclicity of A,S,D,E, assuming only that B,D are in a
circle centered at A.
106
GeoGebra Discovery and an Austrian Mathematics Olympiad Problem
Figure 6: Visually checking the validity of the locus of C for the bisector line of BD.
of the free points A,B,D in the construction. So, it is neither a symbolic geometric object, nor an object
we could enter in the ART to check if placing C over some of the components implies A,S,D,E are
concyclic. We would have to learn how to build the locus starting from A,B,D, we would have to find
some intrinsic geometric description of the locus, say, the analysis and discussion of the different lines.
In this particular case is not difficult to state that placing C in the circle yields a degenerate case (since
then C = E = S). And the same happens for several of the lines, except for the line that is perpendicular
to BD, see Figure 6 and, for a rigorous verification, Figure 7, displaying the original construction, but
now ABCD forms a kite, since C is placed anywhere on the bisector line of BD.
Leaving aside the curious fact that, using GeoGebra Discovery ART we have been able to solve, and
to generalize, an Olympiad Problem, we consider it is important to reflect on some other consequences—
on the educational context—of the sequence of facts we have described in this section.
Indeed, it is well known the use of locus computation as a relevant methodology in mathematics
education. For example, let us refer to the recent chapter [5], where the authors describe an experience
involving over 200 secondary education students from Sicily (Italy), analyzing in detail the impact of
GeoGebra on the performance of the students that had to find and to describe different geometric loci.
We have realized (see [6]) that the use of GeoGebra Discovery ART for accomplishing the same tasks,
would imply a substantial methodological change: looking for most of the considered loci would turn
out to be quite trivial, if it is just required finding the equation or displaying the graph of the locus. On
the other hand, we think that the considered rich learning environment remains if the proposed activities
focus, instead, on the description of the intrinsic geometric features of the obtained locus, as well as
on their construction, two tasks that GeoGebra Discovery is not able to address automatically, but can
contribute to its achievement.
In this context, we consider that the extended version of Problem 2 we have considered in this sec-
B. Ariño-Morera et al.
107
Figure 7: Proving the validity of Problem 2 extended by placing C just on the bisector line of BD.
tion, provides another excellent environment for exploring geometric locus with the help of automated
reasoning tools, showing both their limitations as well as their useful features.
4
Grading Problem 2
Very recently we have proposed an algorithmic way to associate, to each geometry statement, a grade
that intends to estimate its difficulty or interest. It is yet a proposal in a very initial state—although
already implemented in the last version of GeoGebra Discovery, through the ShowProof command—
that, roughly speaking, computes (a bound on) the degree of the polynomials gi expressing the thesis
polynomial T as a combination of the hypotheses polynomials hi, i.e. T = ∑i gi ·hi.
We refer to [4] for further details and examples. Let us just mention here that most classical, ele-
mentary theorems (e.g. Pythagoras, Intersection of Medians, Intersection of Heights, etc.) get grades 1
or 2. Or a partial formulation of the 9-point circle theorem gets complexity 4 (see Figure 8) while this
Problem 2 of the Austrian Mathematical Olympiad has got grade 10! See Figure 9.
To obtain this grade, GeoGebra Discovery internally computes a Gröbner basis of the hypotheses
ideal (with respect to a certain degree ordering: the impact of the choice of order is still under study),
with 13 elements. This computation outputs, as well, the expression of each element of the basis in
terms of the hypotheses, with coefficients of degree bounded by 8. Then the expression of the thesis
as a combination of the elements in the Gröbner basis is computed. Let us remark that in this formula,
the degree of the multiplier polynomials is bounded by 3, but considering more precisely the sum of
the involved degrees (of the polynomial multiplying a certain element of the Gröbner basis times the
maximum of the degrees of the polynomials expressing this element in terms of the hypotheses), the
bound is limited to 10, a number that we think is adequate to be associated to a Mathematical Olympiad
108
GeoGebra Discovery and an Austrian Mathematics Olympiad Problem
Figure 8: Estimating the complexity of proving the concyclicity of the midpoints of the sides and the feet
of a height in a triangle.
Figure 9: Estimating the complexity of Problem 2.
B. Ariño-Morera et al.
109
Problem of an intermediate level of difficulty.
5
Conclusions
In our communication we have illustrated, considering just a single problem from the Austrian Mathe-
matical Olympiad 2023, several facts concerning the use of GeoGebra Discovery in the classroom:
1. the ability of GeoGebra Discovery Automated Reasoning Tools (ART) to immediately solve a
problem presented at a regional Mathematics Olympiad, that the recent GeoGebra ART complexity
measure ranks quite highly,
2. the use of GeoGebra Discovery as a decisive auxiliary tool to develop and confirm new, non-trivial,
conjectures, such as the generalization of the proposed problem,
3. the need to change the methodological focus when working with locus computation in the class-
room with Dynamic Geometry programs, from finding equations and displaying its graph, to an-
alyzing and obtaining the geometric characteristics of the involved locus, and its construction, by
using GeoGebra Discovery ART.
The opportunity to consider simultaneously all these items around a single problem, is probably the
most relevant contribution of this communication.
References
[1] Kovács, Z.; Recio, T.; Vélez, M. P.: Automated reasoning tools with GeoGebra: What are they? What are
they good for? In: P. R. Richard, M. P. Vélez, S. van Vaerenbergh (eds): Mathematics Education in the
Age of Artificial Intelligence: How Artificial Intelligence can serve mathematical human learning. Series:
Mathematics Education in the Digital Era, Vol. 17, pp. 23–44. Springer Cham (2022). doi:10.1007/978-3-030-
86909-0_2
[2] Ariño-Morera, B.; Recio, T.; Tolmos, P.: Olympic geometry problems: human vs. machine. Communication
to the CADGME (Digital Tools in Mathematics Education) 2022 Conference. Abstracts available at https:
//drive.google.com/file/d/1qF4ceMg6gNklOPa1JVkgKND1dOqNmyka/viewpp
[3] Ariño-Morera, M. B.: GeoGebra Discovery at EGMO 2022. Revista Do Instituto GeoGebra Internacional De
São Paulo, 11(2), 005-016.2022. doi:10.23925/2237-9657.2022.v11i2p005-016
[4] Kovács, Z.; Recio, T.; Vélez, M. P.:
Showing proofs, assessing difficulty with GeoGebra Discovery.
Communication to the ADG (Automated Deduction in Geometry) Conference, Belgrade, 2023. https:
//adg2023.matf.bg.ac.rs/downloads/slides/ShowingProofsZoltanRecioVelez.pdf
doi:10.13140/RG.2.2.31885.31205
[5] Mammana, M.F.; Pennisi, M.; Taranto, E.: Teaching Intriguing Geometric Loci with DGS. In: Aldon, G.,
Hitt, F., Bazzini, L., Gellert, U. (Eds). Mathematics and Technology. Advances in Mathematics Education.
Springer, Cham (2017). doi:10.1007/978-3-319-51380-5_26
[6] Ariño-Morera, B.; Recio, T.; Tolmos, P.: Teaching Intriguing Geometric Loci with GeoGebra Discovery.
Communication to the CADGME (Digital Tools in Mathematics Education) 2023 Conference. https://si
tes.google.com/view/cadgme2023/program
"
"The pursuit of identifying properties that allow reasoning software to create and locate new and intriguing theorems is a fascinating research goal. The automated discovery of fresh theorems is a goal in and of itself; different approaches have addressed it in various areas with varying techniques. The divide between ""weeds,"" uninteresting, basic facts, and ""wheat,"" new and fascinating facts, is considerably more difficult yet is also being solved by various authors using various approaches. We'll concentrate on geometry in this essay. Different strategies for automatically finding geometric theorems (and characteristics) and various criteria for identifying interesting theorems among the ones produced are presented and talked about. We introduce the first finding of this article after this explanation: an undecidability result that demonstrates that developing an algorithmic procedure to determine whether every possible Turing Machine that produces theorems is also capable of generating interesting theorems is an unsolvable problem. Consequently, we contend that figuring out whether a theorem prover is capable of generating interesting theorems remains a nondeterministic operation, at best one that is handled by a program based on algorithms guided by heuristic criteria. As a result, two things are required as a human to complete this assignment: an expert survey that clarifies what a theorem finder/prover of interesting geometric theorems is and other surveys that clarify metrics and approaches related to the intriguingness of geometric theorems to enable this analysis.","Larry Wos discussed the problems that computer programs that reason encounter in ""Automated Reasoning: 33 Fundamental Research Problems."" Problem 31 is still unanswered and the subject of continuing research [56].
Wos' Problem 31: What qualities can be recognised to allow an automatic reasoning program to locate fresh and intriguing theorems as opposed to verifying postulated theorems?
In a single sentence, two issues: new and intriguing theorems. The automated discovery of fresh theorems is a goal in and of itself; different approaches have addressed it in various areas with varying techniques. Additionally being addressed by various authors utilising various techniques is the distinction between ""weeds,"" uninteresting, fundamental facts, and ""wheat,"" new and fascinating facts.","nanRegarding the originality aspect, there are many ways to approach fresh mathematical results. One of these strategies is to systematically study a broad area of mathematical information, generating new theorems by various means in the hopes of finding interesting ones among those created (which will be examined in section 4) [14, 15, 19, 25, 30, 35, 36, 46]. The focus of another strategy is the pursuit of mathematical discovery in particular areas, such as Computing Locus Equations [1, 9], Automated Discovery of Angle Theorems [54], Automated Discovery of Geometric Theorems Based on Vector Equations [42], Automated Generation of Geometric Theorems from Images of Diagrams [13], and Automatic Discovery of Theorems in Elementary Geometry [48]. These techniques do not aim to directly address the issue of automated theorem finding, but rather to find complementary hypotheses for a provided geometric statement to become true, for instance, automated discovery for particular areas.nannan","Some systems tackle the generation of new geometric results head-on. In the following paragraphs, several of these approaches are described.
3.1 Strong Relevant Logic-based Forward Deduction Approach
In [27], the writers argue that Automated Theorem Proving (ATP) and Automated Theorem Finding (ATF) are intrinsically different. ATP is the process of finding support for a precisely stated statement using previously established facts or previously accepted hypotheses. ATF is the process of revealing or bringing to light previously unknown information. Where ATP is all about known (old) truths, ATF is about previously unknown conclusions based on known facts.nan","The different approaches found in the literature [18, 27, 46] have the same algorithm in common: for a given logical fragment, choose an initial set of facts, then iterate through a cycle of generation/filtering until a stopping condition is found (see Fig. 4.1).","In light of our undecidability result, to grasp what experts mean by ""a program that is also capable of proving interesting theorems,"" empirical data must be gathered via an expert survey. To achieve this agreement, a thorough empirical examination of the concept of interestingness and what it specifically entails is essential. This investigation calls for placing the idea of interestingness in a historical and socio-cultural context and taking into account logical, epistemological, sociological, cognitive, semiotic, and educational aspects of the subject. Probably—and as Wos already implies—interestingness entails several tangible characteristics, which vary across centuries, geographical locations, and societies.",Considerations on Approaches and Metrics in Automated Theorem Generation/Finding in Geometry,"Pedro Quaresma, Pierluigi Graziani, Stefano M. Nicoletti","P. Quaresma and Z. Kov´acs (Ed.): Automated Deduction
in Geometry 2023 (ADG 2023).
EPTCS 398, 2024, pp. 85–100, doi:10.4204/EPTCS.398.12
© P. Quaresma, P. Graziani & S.M. Nicoletti
This work is licensed under the
Creative Commons Attribution License.
Considerations on Approaches and Metrics in Automated
Theorem Generation/Finding in Geometry
Pedro Quaresma*
CISUC / Department of Mathematics,
University of Coimbra, Portugal
pedro@mat.uc.pt
Pierluigi Graziani†
Department of Pure and Applied Sciences, University of Urbino, Italy
pierluigi.graziani@uniurb.it
Stefano M. Nicoletti‡
Formal Methods and Tools, University of Twente, Enschede, the Netherlands
s.m.nicoletti@utwente.nl
The pursue of what are properties that can be identified to permit an automated reasoning program
to generate and find new and interesting theorems is an interesting research goal (pun intended).
The automatic discovery of new theorems is a goal in itself, and it has been addressed in specific
areas, with different methods. The separation of the “weeds”, uninteresting, trivial facts, from the
“wheat”, new and interesting facts, is much harder, but is also being addressed by different authors
using different approaches. In this paper we will focus on geometry. We present and discuss different
approaches for the automatic discovery of geometric theorems (and properties), and different metrics
to find the interesting theorems among all those that were generated. After this description we will
introduce the first result of this article: an undecidability result proving that having an algorithmic
procedure that decides for every possible Turing Machine that produces theorems, whether it is able
to produce also interesting theorems, is an undecidable problem. Consequently, we will argue that
judging whether a theorem prover is able to produce interesting theorems remains a non deterministic
task, at best a task to be addressed by program based in an algorithm guided by heuristics criteria.
Therefore, as a human, to satisfy this task two things are necessary: an expert survey that sheds light
on what a theorem prover/finder of interesting geometric theorems is, and—to enable this analysis—
other surveys that clarify metrics and approaches related to the interestingness of geometric theorems.
In the conclusion of this article we will introduce the structure of two of these surveys —the second
result of this article— and we will discuss some future work.
1
Introduction
In Automated Reasoning: 33 Basic Research Problems, Larry Wos, wrote about the problems that com-
puter programs that reason face. Problem 31 is still open and object of active research [56, 57]:
Wos’ Problem 31—What properties can be identified to permit an automated reasoning pro-
gram to find new and interesting theorems, as opposed to proving conjectured theorems?
Two problems in a single sentence: new and interesting theorems. The automatic discovery of new
theorems is a goal in itself, it has been addressed in specific areas, with different methods. The separation
*Partially supported by FCT – Foundation for Science and Technology, I.P., within the scope of the project CISUC –
UID/CEC/00326/2020 and by European Social Fund, through the Regional Operational Program Centro 2020.
†Partially supported by Italian Ministry of Education, University and Research through the PRIN 2017 project “The Manifest
Image and the Scientific Image” prot. 2017ZNWW7F 004.
‡Funded by ERC Consolidator Grant 864075 (CAESAR).
86
Considerations on Approaches and Metrics in ATG/F in Geometry
of the “weeds”, uninteresting, trivial facts, from the “wheat”, new and interesting facts, is much harder,
but is being addressed also, by different authors using different approaches.
Paraphrasing, again, Wos, “since a reasoning program can be instructed to draw some (possible large)
set of conclusions” what should be the “criteria that permit the program to select from those the ones (if
any) that correspond to interesting results.”
Different fields have come across the finding of new and interesting theorems’ questions.
Regarding the novelty side: there are different views of approaching new mathematical results. One
of those approaches is the systematic exploration of a given broad area of mathematical knowledge, gen-
erating, by different means, new theorems and expecting to find interesting ones among those generated
(that will be analysed in section 4) [14, 15, 19, 25, 30, 35, 36, 46]. Another approach is given by the
pursue of mathematical discovery in specific areas, e.g. Computing Locus Equations [1, 9], Automated
Discovery of Angle Theorems [54], Automated Discovery of Geometric Theorems Based on Vector Equa-
tions [42], Automated Generation of Geometric Theorems from Images of Diagrams [13], Automatic
Discovery of Theorems in Elementary Geometry [48]. These approaches do not aim to address the prob-
lem of automated theorem finding in itself but, for example, to find complementary hypotheses for a
given geometric statements to become true [48] i.e. automatic discovery for specific areas.1
Regarding the interestingness side we are aware that relevant literature can be found in different areas.
For example in automated theorem proving [19, 25, 30, 46] and in sociological studies on the concept of
proving [20, 39, 40], in cognitive and educational science studies on the concept of proving [2, 11, 21,
33, 45, 52] and in semiotics and epistemology of mathematics [3, 4, 5, 8, 12].
Despite the cited studies, the Wos’ problem is still on the table. On the contrary, a new result of
undecidability can be added to the problem, i.e. having an algorithmic procedure that decides for every
possible Turing Machine that produces theorems, whether it is able to produce also interesting theorems,
is an undecidable problem. Consequently, we can argue that judging whether a theorem prover is able to
produce interesting theorems remains a non deterministic task, at best a task to be addressed by program
based in an algorithm guided by heuristics criteria. Therefore, as a human, to satisfy this task we need
expert survey that sheds light on what a theorem prover/finder of interesting geometric theorems is, and—
to enable this analysis—other surveys that clarify metrics and approaches related to the interestingness
of geometric theorems.
Structure of the paper. In section 2 the issue of Automated Theorem Generation (ATG) is dis-
cussed. In section 3 we discuss the deductive approach in ATG. In section 4 the issue of Automated
Theorem Finding (ATF) is analysed. In section 5 we present an undecidability result concerning the
problem of finding interesting theorems and its conceptual consequences. In section 6 we will introduce
the structure of two surveys to empirically explore the interestingness of theorems in geometry and its
potential application in theorem proving/finding (a third survey). Finally, we will discuss some future
work.
2
Automated Theorem Generation
Automated theorem generation, independently of being interesting, or not, can be addressed in several
ways [46].
1We left aside the notion of discovery in education, given that, in that area, the goal is the student’s discovery of “new” (for
them) theorems, giving the student the possibility of freely making conjectures and having an interactive/automatic deduction
support in the exploration of those “new” theorems [10, 32, 37, 38].
P. Quaresma, P. Graziani & S.M. Nicoletti
87
The Inductive approach,
is a natural approach. Conclusions are drawn by going from the specific to
the general. Exploring a given domain, seeking for properties that emerge from a set of particular cases
and making a conjecture about the general case.
Dynamic Geometry Software (DGS) can be seen as software environments to inductively explore
new knowledge. Making a geometric construction, constrained by a given set of geometric properties,
and then moving the free point around will show all the fix-points, conjecturing if those new fixed re-
lations between objects are true in all cases, or not. For example the Pappus’ Theorem, in this case, a
well-known theorem: are the intersection points (see Figure 1) G, H and I, collinear? By moving, in the
DGS, the free-points it seems that they are, it remains to prove it.
Figure 1: Pappus’ Theorem
The inductive approach has the advantage of being stimulated by observations in the domain. but has
the disadvantage that induction is unsound. A famous example of such unsound inductive approach can
be seen in the Euclid Parallel lines Postulate, that nevertheless was very fruitful, giving raise to different
geometries.
The Generative approach,
i.e. the generation of conjectures, testing them for theorem-hood. The
simplest form of generation is syntactic, in which conjectures are created by mechanical manipulation
of symbols, e.g. [44]. The MCS program generates conjectures syntactically and filters them against
models of the domain [59]. A stronger semantically based approach is taken by the HR program, which
generates conjectures based on examples of concepts in the domain [18]. A theory exploration system
called QuickSpec, works by interleaving term generation with random testing to form candidate con-
jectures [34]. In [34] the conjecture generation approaches are classified into three categories: heuristic
rule-based systems, term generation-and-testing and neural network-based systems. The RoughSpec sys-
tem adds to QuickSpec the notion of shapes of theorems, specifying the shapes of conjectures the user is
interested in, and thus limiting the search [22].
Like induction, generation is unsound. However, if the rules by which the generation is performed are
sufficiently conservative then this approach may generate a higher fraction of theorems than the inductive
approach.
The Manipulative Approach,
conjectures are generated from existing theorems. An existing theorem
is modified by operations such as generalisation, specialisation, combination, etc. This approach is used
in abstraction mapping, which converts a theorem to a simpler theorem, and uses a solution to the simpler
88
Considerations on Approaches and Metrics in ATG/F in Geometry
theorem to help find a solution to the original theorem [43]. Manipulation of ATP theorems has also been
used to produce new theorems for testing the robustness of ATP systems’ performances [55].
An advantage of the manipulative approach is that, if the manipulations are satisfiability preserving,
then theorems, rather than conjectures, are produced from existing theorems. However, the conjectures
produced by the manipulative approach are typically artificial in nature, and thus uninteresting.
The Deductive Approach,
consequences are generated by application of sound inference rules to the
axioms and previously generated logical consequences. This can be done by an appropriately configured
saturation-based ATP system.
The advantage of this approach is that only logical consequences are ever generated. The challenge
of this approach is to avoid the many uninteresting logical consequences that can be generated.
3
The Deductive Approach
Some systems addresses, explicitly, the generation of new geometric results using different approaches.
In the following some of these approaches are described.
3.1
Strong Relevant Logic-based Forward Deduction Approach
In [27] the authors argue for the fundamental difference between the Automated Theorem Proving (ATP)
and the Automated Theorem Finding (ATF). ATP is the process of finding a justification for an explicitly
specified statement from given premises which are already known facts or previously assumed hypo-
theses. ATF is the process to find out or bring to light that which was previously unknown. Where
ATP is all about known (old) facts, ATF is about previously unknown conclusions from given premises.
Jingde Cheng [15] claims that classical mathematical logic, its various classical conservative extensions,
and traditional (weak) relevant logics cannot satisfactorily underlie epistemic processes in scientific dis-
covery, presenting an approach based on strong relevant logic. Hongbiao Gao et al. have followed
this approach applying it for several domains such as NBG set theory, Tarski’s Geometry and Peano’s
Arithmetic [26, 27, 29, 30]
3.2
Rule Based Systems
The rule-based automated deduction system are often used when the proof itself is an object of interest
(and not only the end result), given that the proofs are developed from the hypothesis and sets of axioms,
to the conclusion by application of the inference rules, the proofs are “readable”.
Example of such approaches can be seen in systems like QED-Tutrix [23, 24] and JGEx [58], both
for geometry. In the tutorial system QED-Tutrix, the rule based automated theorem prover goal is to
find the many possible branches of the proof tree, in order to be able to help the student approaching the
proof of a geometric conjecture. In the JGEx system we can have the proof in a “readable” and “visual”
renderings and also the set of all properties that can be deduced from the construction.
One of the ATP built-in in JGEx is an implementation of the geometry deductive database method [16,
58]. Using a breadth-first forward chaining a fix-point for the conjecture at hand is reached. For that
geometric construction and the rules of the method, the fix-point gives us all the properties that can be
deduced, some already known facts, but also new facts (not necessary interesting ones).
P. Quaresma, P. Graziani & S.M. Nicoletti
89
The geometry deductive database method proceeds by using a simple algorithm where, starting from
the geometric construction D0, the rules, R, are applied over and over till a fix-point, Dk is reached:
D0
R⊂
D1
R⊂
···
R⊂
Dk
(fix-point)
(1)
In figure 2 an example, using JGEx, is shown. On the right, the geometric construction, on the left,
the fix-point, with all the facts that were found for that construction.
Figure 2: Fix-point in JGEx
A new open source implementation of this method, OGP-GDDM,2 is described in [7]. It will be
integrated in the Open Geometry Prover Community Project (OGPCP) [6]. One of the medium-term
goals of the OGP-GDDM project, is to develop a meta-prover, a program capable to receive different
sets of rules and synthesise a specific ATP for those rules.
3.3
Algebraic Approaches
A similar approach is taken in the well-known dynamic geometry system GeoGebra.3 The GeoGebra
Discovery version4 has the capability to find, from a user defined geometric construction, properties
about that construction. GeoGebra Discovery reports some facts that were systematically checked from
a list of possible features including identical points, parallel or perpendicular lines, equal long segments,
collinearity or concyclicity. This is not a deductive method so the generation process must be externally
verified, GeoGebra Discovery do that by recurring to a built-in algebraic automated theorem prover based
in the Gr¨obner bases method [35, 36].
2https://github.com/opengeometryprover/OpenGeometryProver
3https://www.geogebra.org/
4https://github.com/kovzol/geogebra-discovery
90
Considerations on Approaches and Metrics in ATG/F in Geometry
4
Automated Theorem Finding
Apart from our research goal of finding the interesting geometric theorems among all those that were
automatically generated, the pursue of measures of interestingness has applicability in the interactive
and automated theorem proving area. In that area a common use of interestingness is to improve the
efficiency of the programs, tailoring the search space, making the search depth limited and guaranteeing
that only comprehensible concepts are produced [19].
A goal, pursued with different approaches by many researchers, is the creation of strong AI meth-
ods capable of complex research-level proofs, mathematical discovery, and automated formalisation of
today’s vast body of mathematics [47]. The MATHsAiD (Mechanically Ascertaining Theorems from
Hypotheses, Axioms and Definitions) project aimed to build a tool for automated theorem-discovery,
from a set of user-supplied axioms and definitions. In the words of its authors, MATHsAiD 2.0 can
conjecture and prove interesting Theorems in high-level theories, including Theorems of current math-
ematical significance, without generating an unacceptable number of uninteresting theorems [41]. The
TacticToe system, combines reinforcement-learning with Monte-Carlo proof search on the level of HOL4
tactics [31]. The ENIGMA-NG system uses efficient neural and gradient-boosted inference guidance for
the ATP E, improving its efficiency [17]. This two systems, one for interactive provers and the other to
automatic provers, are examples of systems that uses discovery and filtering for improving the efficiency
of automated deduction systems.
4.1
The Deductive Approach Algorithm
The different approaches found in the literature [18, 27, 46] share, in their general lines, the same al-
gorithm: for a given logical fragment, select a initial set of facts and then a cycle of generation/filtering
is applied until some stopping condition is matched (see Fig. 4.1).
4.2
Filtering Interesting Theorems
A first level of filtering (run-time filter) should discard the obvious tautologies and also conjectures
proved false by empirical evidence.
The filtering for interesting theorems or for uninteresting conjectures, two sides of the same coin,
is done by application of a series of filters. These filters are still to be validated, being of speculative
nature [19, 29, 30, 46].
Obviousness: the number of inferences in its derivation. Obviousness estimates the difficulty of proving
a formula, it can be given by the number of inferences in its derivation.
Weight: the effort required to read a formula. The weight score of a formula is the number of symbols
it contains.
Complexity: the effort required to understand a formula, the number of distinct function and predicate
symbols it contains.
Surprisingness: measures new relationships between concepts and properties.
Intensity: measures how much a formula summarises information from the leaf ancestors in its deriva-
tion tree.
Adaptivity: measures how tightly the universally quantified variables of a formula are constrained (for
formulae in clause normal form).
P. Quaresma, P. Graziani & S.M. Nicoletti
91
List of axioms/deduction rules
for a given logic fragment
New Fact List:
hypothesis + interesting theorems
•
•
New Fact
Exist?
Apply
Deduction
Run-time
Filter
Interesting Theorems
Filter
List of
Interesting Theorems
yes
no
Interesting Theorem
(added)
(discarded)
no
(discarded)
tautologies/false conj.
Figure 3: New and Interesting Theorems Algorithm
Focus: measures the extent to which a formula is making a positive or negative statement about the
domain of application.
Usefulness: measures how much an interesting theorem has contributed to proofs of further interesting
theorems.
In spite of the relevance of these metrics, it would be appropriate to construct an expert survey with
which we could validate them by referring to a significant public of experts. We believe this kind of
survey would be relevant not only to face Wos’ problem, but also to better understand how to construct
and evaluate software that generates/finds interesting theorems. Despite having only relevant metrics and
approaches regarding Wos’ problem, while not yet having formal results, we can prove a relevant result
that concerns the second issue, i.e., the question regarding Interesting Turing Machines, i.e., programs
capable of generating interesting new geometric results.
5
Undecidability Result
In section 4.2 the application of filters was discussed, these filters are based on some measures of inter-
estingness that are still to be validated and that are applied in an heuristic way. Is it possible to have a
deterministic approach, i.e., is it possible to write a computer program that in a deterministic way, find
interesting theorems? We show, as an application of the Rice’s theorem [49, 50, 51] (see Lemma 1), that
it is undecidable to determine, for a given Turing Machine, whether the language recognised by it has
the (non-trivial) property of finding interesting theorems.
Definition 1 (Non-Trivial Property). A property p of a formal language is non-trivial if:
92
Considerations on Approaches and Metrics in ATG/F in Geometry
• there exist a recursively enumerable language having the property p;
• there exist a recursively enumerable language not having the property p.
Lemma 1 (Rice’s Theorem). Let p be any non-trivial property of the language of a Turing machine. The
problem of determining whether a given Turing machine’s language has property p is undecidable.
Theorem 1 (Undecidability Result). For any given Turing Machine, it is undecidable to determine,
whether the language recognised by it has the property of finding interesting theorems.
Proof. All programs (Turing machines) capable of automated theorem proving and by extension gen-
erating/finding geometric theorems rely on a formal language to describe the geometric constructions,
conjectures and proofs. For example we can consider the (full) First-Order Form (FOF)5 of TPTP [53]
and the formal axiomatic theories for geometry based on that language.6
Let p be the property of that language that says that theorem t is interesting, for any conceivable
definition of interestingness, then there exist a recursively enumerable language having the property p.
It will be enough to restrict the language in such a way that the theorem t, and only this, would be
recognised. But, it also exist a recursively enumerable language not having the property p. It would be
enough to restrict that language in such a way that only tautologies would be recognised. Tautologies
are, for any conceivable definition of interestingness, uninteresting. We have proved that p, the property
that can establish if a given theorem is interesting, is a non-trivial property.
Having establish that the property p is non-trivial, then, by application of Rice’s theorem, it is un-
decidable to determine for any given Turing machine M, whether the language recognised by M has the
property p.
In other words, it is undecidable to have a deterministic program that can find interesting problems.
At best this is a task to be addressed by programs based on algorithms guided by heuristics criteria.
6
Designing Interesting Surveys
In light of our undecidability result, to understand what experts mean by, “a program that is able to
also prove interesting theorems”, must be done referring to empirical data, via the formulation of an
expert survey. However, for it to be fulfilled, one has to first reach a minimal degree of agreement on
the definition of interestingness of theorems. How could one speak about programs that produce such
theorems? In order to achieve this agreement, an empirical exploration of the notion of interestingness
and of what it concretely entails is paramount. This exploration requires to situate the notion of inter-
estingness historically and socio-culturally, considering logical, epistemological, sociological, cognitive,
semiotic and pedagogical aspects of the issue. Probably—and as Wos already implies—interestingness
entails different tangible properties, which differ in given centuries, geographical locations and societies.
Moreover, in some cases we say that a theorem is interesting for what we can call global reasons e.g.,
Euclid’s theorem on the infinitude of the set of prime numbers, Zorn’s lemma and G¨odel’s Theorems are
interesting due to their role in mathematics, logic and computer science. Other times for local reasons
e.g., in relation to what we are teaching our students at that moment. In order to assess which tangible
properties—both global and local—interestingness entails today, we are proposing to conduct two expert
surveys with two statistically significant pools of participants.
5http://tptp.cs.miami.edu/TPTP/QuickGuide/
6TPTP Axioms Files for geometry, https://www.tptp.org/cgi-bin/SeeTPTP?Category=Axioms, e.g. Tarski geo-
metry axiom, GEO001 and GEO002, Deductive Databases Method in Geometry, GEO012.
P. Quaresma, P. Graziani & S.M. Nicoletti
93
Influencing factors.
Gao et al. performed an extensive analysis of areas like Set Theory, Peano’s arith-
metic and Tarski’s Geometry, looking for the relevance of structural factors, such as the degree of logical
connectives in the theorem, the propositional schema of the formula formalising the theorem, the abstract
level of predicates and functions in the theorem and the deduction distance of a theorem [19, 28, 30, 46].
Some of these structural aspects might be related to our cognitive dynamics. But also the epistemolo-
gical role of a theorem with respect to other theorems might be a relevant feature; or the educational role
that some theorems have with respect to some notions might influence their interestingness. Finally, the
history of a theorem—e.g. Fermat’s last theorem—could add points to its interestingness, which, in the
case of Fermat’s last theorem, might be already caused by the technicalities of the proof itself.
Designing the surveys.
Taking all these factors into consideration, we would propose to design three
surveys that question experts from different fields.
Before describing the surveys below some clarifications are necessary. We will use the term “expert”
to mean mathematics teachers at primary, middle, and high schools, and professors or researchers in pure
and applied mathematics at universities or at research centres. Furthermore, we will focus on the case
study of geometry, hence interesting theorems in geometry. The reasons for this restriction to geometry
are as follows: on the one hand, considering all fields of research in mathematics might require a too large
number of experts and could produce too many divergent ideas. On the other hand, having in mind an
application of the results in automatic theorem proving as a target, it seems appropriate to move into an
area were there are many different methods and many automated provers implementing those methods.
Finally, geometry is a kind of language common to many areas of mathematics and has been a domain
for reflection since the early years of mathematics teaching.
Finally, these surveys are intended to involve mathematics teachers, but their outcome does not target
mathematics education. Of course, this is a possible target, but it is not the primary goal of these starting
surveys.
6.1
Three Surveys
In the first survey, we will ask the experts both to indicate some situations in which they remember to have
used the adjective interesting concerning a theorem, and to explain the use of this expression. In addition,
we will ask experts to list several geometric theorems they find interesting, and to list several geometric
theorems they find not interesting, both from elementary and higher geometry, explaining the reasons for
their answers (see Appendix A). This first survey is already under way, the steering committee is already
approaching it and the authors of accepted papers in the conference, 14th International Conference on
Automated Deduction in Geometry (ADG 2023)7, were invited to participate. We are planning to enlarge
it to our network of contacts and we invite the interested reader to also participate, answering it.8 We are
planning to begin collect and analyse the answers in February, 2024.
We will use the information from this survey to define a list of characteristics (A,B,C, ...) of a
theorem that offer sufficient reasons to attribute interestingness to it. We will assign weights to the
various characteristics by considering the answers to this first survey.
After the first survey, we will implement a second one. This second survey will consider a list of
theorems that, in different percentages, have the characteristics inferred from the first survey. We will
7ADG 2023, 14th International Conference on Automated Deduction in Geometry, Belgrade, Serbia, September 20-22,
2023.
8https://docs.google.com/forms/d/e/1FAIpQLScIXZbLPBHTLvmQ28P30Cm_-lkOrM7e6rab7ho0WrAFwf_mbQ/
viewform?usp=sf_link
94
Considerations on Approaches and Metrics in ATG/F in Geometry
submit the second survey to a set of experts different from those used in the first survey. We will ask these
experts whether they find the theorems listed interesting or not. We will ask them to rate, using a Likert
scale,9 the degree of impact that having certain characteristics plays in their attribution of interestingness
(see Appendix B).
This second group will allow us to understand whether the characteristics isolated through the first
survey are sufficient conditions to affirm that a theorem is interesting.
With an agreement on what an interesting theorem is, based on empirical research, we could query
experts in theorem generators/finders design, with another survey (the third survey) asking how to design
software able to produce these interesting theorems.
After that, we will focus our empirical inquiry on programs, driven by heuristics based on our find-
ings, able to find interesting theorems.
We have established a steering committee to design the surveys and who will oversee the submission
of the surveys to experts around the world.
The steering committee consists of the following scholars:
• Thierry Dana-Picard, Jerusalem College of Technology, Jerusalem, Israel;
• James Davenport, University of Bath, United Kingdom;
• Pierluigi Graziani, University of Urbino, Urbino, Italy;
• Pedro Quaresma, University of Coimbra, Coimbra, Portugal;
• Tom´as Recio, University Antonio de Nebrija, Madrid, Spain.
7
Conclusions
The pursuit of new and interesting theorems in geometry, by automatic means is an interesting open
problem. From the point of view of generating new information the deductive approach seems the most
appropriated, given that: only logical consequences are ever generated and also the paths to those new
theorems can be analysed from the point of view of the geometric theory used, i.e. in the process of
generating new facts, geometric proofs of their validity are produced. Already existing implementations,
e.g. the deductive databases method (DDM) implemented in JGEx, and new implementations, e.g. the
GeoGebra Discovery and the new implementation of the DDM, the OGPCP-GDDM prover, can be used.
The separation of the uninteresting, trivial facts, from the new and interesting facts is much harder. The
current approaches are based in ad-hoc measures, proposed by experts from the field, but nevertheless,
not substantiated by any study approaching that problem. Our goal is to fulfil that gap, to produce a
comprehensive survey, supported in a large set of mathematicians, in order to be able to return to that
question and to develop filters supported by the findings of that survey.
Acknowledgements
The authors wish to thank Francisco Botana, Thierry Dana-Picard, James Daven-
port and Tom´as Recio for their support in the pursue of this long term project.
9A Likert scale is a question which is a five-point or seven-point scale. The choices range from Strongly Agree to Strongly
Disagree so the survey maker can get a holistic view of people’s opinions. It was developed in 1932 by the social psychologist
Rensis Likert.
P. Quaresma, P. Graziani & S.M. Nicoletti
95
References
[1] Miguel
´A. Ab´anades, Francisco Botana, Antonio Montes & Tom´as Recio (2014):
An algebraic
taxonomy for locus computation in dynamic geometry.
Computer-Aided Design 56, pp. 22–33,
doi:10.1016/j.cad.2014.06.008.
[2] Gilles Aldon, Pierre-Yves Cahuet, Viviane Durand-Guerrier, Mathias Front, Didier Krieger, Michel Mizony
& Claire Tardy C (2010): Exp´erimenter des probl`emes innovants en math´ematiques `a l’´ecole. INRP. Avail-
able at https://hal.archives-ouvertes.fr/hal-00989132.
[3] Ferdinando Arzarello & Cristina Sabena (2010): Semiotic and theoretic control in argumentation and proof
activities. Educational Studies in Mathematics 77(2-3), pp. 189–206, doi:10.1007/s10649-010-9280-3.
[4] Michael Aschbacher (2005): Highly complex proofs and implications of such proofs. Philosophical Transac-
tions of the Royal Society A: Mathematical, Physical and Engineering Sciences 363(1835), pp. 2401–2406,
doi:10.1098/rsta.2005.1655.
[5] Jeremy
Avigad
(2006):
Mathematical
Method
and
Proof.
Synthese
153(1),
pp.
105–159,
doi:10.1007/s11229-005-4064-5. Available at http://www.jstor.org/stable/27653412.
[6] Nuno Baeta & Pedro Quaresma (2021): Open Geometry Prover Community Project. Electronic Proceedings
in Theoretical Computer Science 352, pp. 129–138, doi:10.4204/EPTCS.352.14.
[7] Nuno Baeta & Pedro Quaresma (2023): Towards a Geometry Deductive Database Prover. Annals of Math-
ematics and Artificial Intelligence 91(6), pp. 851–863, doi:10.1007/s10472-023-09839-0.
[8] Henk Barendregt & Freek Wiedijk (2005): The challenge of computer mathematics. Philosophical Transac-
tions of the Royal Society A: Mathematical, Physical and Engineering Sciences 363(1835), pp. 2351–2375,
doi:10.1098/rsta.2005.1650.
[9] Francisco Botana, Miguel A. Ab´anades & Jes´us Escribano (2007): Computing Locus Equations for Standard
Dynamic Geometry Environments. In Yong Shi, G. Dick van Albada, Jack Dongarra & Peter M. A. Sloot, edit-
ors: International Conference on Computational Science, Lecture Notes in Computer Science 4488, Springer
Berlin Heidelberg, pp. 227–234, doi:10.1007/978-3-540-72586-2 32.
[10] Francisco Botana & Jos´e L. Valcarce (2002): A dynamic-symbolic interface for geometric theorem discovery.
Computers and Education 38(1-3), pp. 21–35, doi:10.1016/S0360-1315(01)00089-6.
[11] Alan Bundy, Mateja Jamnik & Andrew Fugard (2005): What is a proof?
Philosophical Transactions
of the Royal Society A: Mathematical, Physical and Engineering Sciences 363(1835), pp. 2377–2391,
doi:10.1098/rsta.2005.1651.
[12] Tyler Burge (1998): Computer proof, apriori knowledge, and other minds: The sixth philosophical perspect-
ives lecture. Philosophical perspectives 12, pp. 1–37, doi:10.1093/acprof:oso/9780199672028.003.0013.
[13] Xiaoyu Chen, Dan Song & Dongming Wang (2014): Automated generation of geometric theorems from im-
ages of diagrams. Annals of Mathematics and Artificial Intelligence 74(3-4), pp. 1–26, doi:10.1007/s10472-
014-9433-7.
[14] Jingde Cheng (1995): Entailment calculus as the logical basis of automated theorem finding in scientific
discovery. In: Systematic Methods of Scientific Discovery: Papers from the 1995 Spring Symposium, AAAI
Press, pp. 105–110.
[15] Jingde Cheng (2000): A Strong Relevant Logic Model of Epistemic Processes in Scientific Discovery. In
E. Kawaguchi, H. Kangassalo, H. Jaakkola & I.A. Hamid, editors: Information Modelling and Knowledge
Bases XI, Frontiers in Artificial Intelligence and Applications 61, IOS Press, pp. 136–159, doi:10.1007/3-
540-49292-5 42.
[16] Shang-Ching Chou, Xiao-Shan Gao & Jing-Zhong Zhang (2000): A Deductive Database Approach to Auto-
mated Geometry Theorem Proving and Discovering. Journal of Automated Reasoning 25(3), pp. 219–246,
doi:10.1023/A:1006171315513.
96
Considerations on Approaches and Metrics in ATG/F in Geometry
[17] Karel Chvalovsk´y, Jan Jakub˚uv, Martin Suda & Josef Urban (2019): ENIGMA-NG: Efficient Neural and
Gradient-Boosted Inference Guidance for E. In: Lecture Notes in Computer Science, Springer International
Publishing, pp. 197–215, doi:10.1007/978-3-030-29436-6 12.
[18] Simon Colton (2002): The HR Program for Theorem Generation. In Andrei Voronkov, editor: Automated
Deduction—CADE-18, Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 285–289, doi:10.1007/3-540-
45620-1 24.
[19] Simon Colton, Alan Bundy & Toby Walsh (2000):
On the notion of interestingness in automated
mathematical discovery.
International Journal of Human-Computer Studies 53(3), pp. 351–375,
doi:10.1006/ijhc.2000.0394.
[20] Richard A De Millo, Richard J Lipton & Alan J Perlis (1979): Social processes and proofs of theorems and
programs. Communications of the ACM 22(5), pp. 271–280, doi:10.1145/359104.359106.
[21] Bruno D’Amore & MI Fandi˜no Pinilla (2016): Una formula per la misurazione oggettiva della difficolt`a di
comprensione di un testo di matematica da parte degli studenti. Uso valutativo e uso didattico. La matematica
e la sua didattica, 1 2, pp. 59–78.
[22] S´olr´un Halla Einarsd´ottir, Nicholas Smallbone & Moa Johansson (2020): Template-based Theory Explora-
tion: Discovering Properties of Functional Programs by Testing. In: IFL 2020: Proceedings of the 32nd Sym-
posium on Implementation and Application of Functional Languages, ACM, doi:10.1145/3462172.3462192.
[23] Ludovic Font (2021): G´en´eration automatique de preuves pour un logiciel tuteur en g´eom´etrie. phdthesis,
Polytechnique Montr´eal. Available at https://publications.polymtl.ca/9090/.
[24] Ludovic Font, Philippe R. Richard & Michel Gagnon (2018): Improving QED-Tutrix by Automating the
Generation of Proofs.
In Pedro Quaresma & Walther Neuper, editors: Proceedings 6th International
Workshop on Theorem proving components for Educational software, Gothenburg, Sweden, 6 Aug 2017,
Electronic Proceedings in Theoretical Computer Science 267, Open Publishing Association, pp. 38–58,
doi:10.4204/EPTCS.267.3.
[25] Hongbiao Gao & Jingde Cheng (2015): An epistemic programming approach for automated theorem finding.
In: 2015 IEEE 14th International Conference on Cognitive Informatics & Cognitive Computing (ICCIxCC),
IEEE, doi:10.1109/ICCI-CC.2015.7259365.
[26] Hongbiao Gao & Jingde Cheng (2017): Measuring Interestingness of Theorems in Automated Theorem Find-
ing by Forward Reasoning: A Case Study in Peano’s Arithmetic. In Ngoc Thanh Nguyen, Satoshi Tojo,
Le Minh Nguyen & Bogdan Trawinski, editors: Intelligent Information and Database Systems, Lecture Notes
in Computer Science 10192, Springer International Publishing, pp. 115–124, doi:10.1007/978-3-319-54430-
4 12.
[27] Hongbiao Gao, Yuichi Goto & Jingde Cheng (2014): A systematic methodology for automated theorem
finding. Theoretical Computer Science 554, pp. 2–21, doi:10.1016/j.tcs.2014.06.028.
[28] Hongbiao Gao, Yuichi Goto & Jingde Cheng (2015): A Set of Metrics for Measuring Interestingness of
Theorems in Automated Theorem Finding by Forward Reasoning: A Case Study in NBG Set Theory. In:
Intelligence Science and Big Data Engineering. Big Data and Machine Learning Techniques, Springer Inter-
national Publishing, pp. 508–517, doi:10.1007/978-3-319-23862-3 50.
[29] Hongbiao Gao, Jianbin Li & Jingde Cheng (2018): Measuring Interestingness of Theorems in Automated
Theorem Finding by Forward Reasoning: A Case Study in Tarski's Geometry.
In: 2018 IEEE Smart-
World, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing &
Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (Smart-
World/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), IEEE, doi:10.1109/SmartWorld.2018.00064.
[30] Hongbiao Gao, Jianbin Li & Jingde Cheng (2019): Measuring Interestingness of Theorems in Automated
Theorem Finding by Forward Reasoning Based on Strong Relevant Logic. In: 2019 IEEE International
Conference on Energy Internet (ICEI), IEEE, pp. 356–361, doi:10.1109/ICEI.2019.00069.
P. Quaresma, P. Graziani & S.M. Nicoletti
97
[31] Thibault Gauthier, Cezary Kaliszyk, Josef Urban, Ramana Kumar & Michael Norrish (2021): TacticToe:
Learning to Prove with Tactics. Journal of Automated Reasoning 65(2), pp. 257–286, doi:10.1007/s10817-
020-09580-x.
[32] Gila Hanna, David Reid & Michael de Villiers, editors (2019): Proof Technology in Mathematics Research
and Teaching. Springer, doi:10.1007/978-3-030-28483-1.
[33] Kirsti Hemmi, Erika Julin & Ray P¨orn (2017): Misconceptions and developmental proof. In: CERME 10.
[34] Moa Johansson & Nicholas Smallbone (2021): Automated Conjecturing in QuickSpec. In: 1st Mathematical
Reasoning in General Artificial Intelligence Workshop, ICLR 2021.
[35] Zolt´an Kov´acs, Tom´as Recio & M. Pilar V´elez (2021): Automated reasoning tools in GeoGebra discovery.
ACM Communications in Computer Algebra 55(2), pp. 39–43, doi:10.1145/3493492.3493495.
[36] Zolt´an Kov´acs & Jonathan H. Yu (2022): Automated Discovery of Geometrical Theorems in GeoGebra.
In Jo˜ao Marcos, Walther Neuper & Pedro Quaresma, editors: Proceedings 10th International Workshop on
Theorem Proving Components for Educational Software, Electronic Proceedings in Theoretical Computer
Science 354, pp. 1–12, doi:10.4204/EPTCS.354.1.
[37] Fou-Lai Lin, Feng-Jui Hsieh, Gila Hanna & Michael de Villiers, editors (2009): Proceedings of the ICMI
Study 19 conference: Proof and Proving in Mathematics Education. 1, The Department of Mathematics,
National Taiwan Normal University.
[38] Fou-Lai Lin, Feng-Jui Hsieh, Gila Hanna & Michael de Villiers, editors (2009): Proceedings of the ICMI
Study 19 conference: Proof and Proving in Mathematics Education. 2, The Department of Mathematics,
National Taiwan Normal University.
[39] Donald MacKenzie (1995): The automation of proof: A historical and sociological exploration. IEEE Annals
of the History of Computing 17(3), pp. 7–29, doi:10.1109/85.397057.
[40] Donald MacKenzie (1999): Slaying the Kraken: The sociohistory of a mathematical proof. Social studies of
science 29(1), pp. 7–60, doi:10.1177/030631299029001002.
[41] R. L. McCasland, A. Bundy & P. F. Smith (2017): MATHsAiD: Automated mathematical theory exploration.
Applied Intelligence 47(3), pp. 585–606, doi:10.1007/s10489-017-0954-8.
[42] Xicheng Peng, Qihang Chen, Jingzhong Zhang & Mao Chen (2021):
Automated Discovery of Geo-
metric Theorems Based on Vector Equations.
Journal of Automated Reasoning 65(6), pp. 711–726,
doi:10.1007/S10817-021-09591-2.
[43] D. A. Plaisted (1980): Abstraction mappings in mechanical theorem proving.
In: 5th Conference on
Automated Deduction Les Arcs, France, July 8–11, 1980, Springer Berlin Heidelberg, pp. 264–280,
doi:10.1007/3-540-10009-1 21.
[44] David A. Plaisted (1994): The search efficiency of theorem proving strategies. In: Automated Deduction —
CADE-12, Springer Berlin Heidelberg, pp. 57–71, doi:10.1007/3-540-58156-1 5.
[45] George Polya (2004): How to solve it: A new aspect of mathematical method. 246, Princeton university
press.
[46] Yury Puzis, Yi Gao & G. Sutcliffe (2006): Automated Generation of Interesting Theorems. In: FLAIRS
Conference.
[47] Markus N. Rabe & Christian Szegedy (2021): Towards the Automatic Mathematician. In Andr´e Platzer &
Geoff Sutcliffe, editors: Automated Deduction – CADE 28, Springer International Publishing, pp. 25–37,
doi:10.1007/978-3-030-79876-5 2.
[48] T. Recio & M. P. V´elez (1999): Automatic Discovery of Theorems in Elementary Geometry. J. Autom.
Reason. 23, pp. 63–82, doi:10.1023/A:1006135322108.
Available at http://dl.acm.org/citation.
cfm?id=594128.594243.
[49] H. G. Rice (1953): Classes of recursively enumerable sets and their decision problems. Transactions of the
American Mathematical Society 74(2), pp. 358–366, doi:10.2307/1990888.
[50] Hartley Rogers Jr (1987): Theory of recursive functions and effective computability. MIT press.
98
Considerations on Approaches and Metrics in ATG/F in Geometry
[51] Michael Sipser (1997): Introduction to the Theory of Computation. PWS Publishing Company.
[52] Andreas J Stylianides & Guershon Harel (2018): Advances in mathematics education research on proof and
proving: An international perspective. Springer, doi:10.1007/978-3-319-70996-3.
[53] Geoff Sutcliffe (2017): The TPTP Problem Library and Associated Infrastructure. Journal of Automated
Reasoning 59(4), pp. 483–502, doi:10.1007/s10817-017-9407-7.
[54] Philip Todd (2021): A Method for the Automated Discovery of Angle Theorems. In: Proceedings of ADG
2021, 352, Open Publishing Association, pp. 148–155, doi:10.4204/EPTCS.352.17.
[55] Andrei Voronkov (2000): CASC-16-1/2. Preprint Series CSSPP-4, The University of Manchester. Available
at http://www.s.man.a.uk/preprints/index.html.
[56] Larry Wos (1988): Automated Reasoning: 33 Basic Research Problems. Prentice-Hall.
[57] Larry Wos (1993): The problem of automated theorem finding. Journal of Automated Reasoning 10(1), pp.
137–138, doi:10.1007/BF00881868.
[58] Zheng Ye, Shang-Ching Chou & Xiao-Shan Gao (2011): An Introduction to Java Geometry Expert. In
Thomas Sturm & Christoph Zengler, editors: Automated Deduction in Geometry, Lecture Notes in Computer
Science 6301, Springer Berlin Heidelberg, pp. 189–195, doi:10.1007/978-3-642-21046-4 10.
[59] Jian Zhang (1999): System Description: MCS: Model-Based Conjecture Searching. In: Automated Deduc-
tion — CADE-16, Springer Berlin Heidelberg, pp. 393–397, doi:10.1007/3-540-48660-7 37.
A
First Survey—Interesting Theorems
With this survey the goal will be to find the characteristics that make a theorem interesting, or not. A list
of questions about geometric theorems found to be interesting, or not interesting.
For an initial pool of expert on the area it is our intention to use the network created for the submission
of the COST proposal, iGEOMXXI.10 This survey will be available online, based on an online survey
tool.11
A.1
Interesting and Why?
A list of situations/explanations about interesting theorems.
10OC-2020-1-24509, Building a Networked Environment for Geometric Reasoning (iGEOMXXI), The submitted Action
(not funded) focused on the exploration of new paradigms and methodologies for supporting formal reasoning in the field of
Geometry. A network of 49 experts from 19 countries.
11e.g. LimeSurvey, https://www.limesurvey.org/
P. Quaresma, P. Graziani & S.M. Nicoletti
99
Can you describe in detail a situation (during classes or lectures) in which you have used the adjective
interesting applied to a theorem in geometry?
nth Situation
Can you explain in detail the reasons why you used the adjective interesting in the first situation?
nth Explanation
A.2
Five Interesting Theorems in Geometry
A list of 5 questions, each about an interesting theorem.
Can you list at least five theorems in geometry that you consider interesting?
Theorem n
Can you explain in detail the reason for your choice by listing at least five adjectives that describe
characteristics of the previous theorem making it interesting?
A.3
Five Not Interesting Theorems in Geometry
A list of 5 questions, each about a not interesting theorem.
100
Considerations on Approaches and Metrics in ATG/F in Geometry
Can you list at least five theorems in geometry that you consider NOT interesting?
Theorem n
Can you explain in detail the reason for your choice by listing at least five adjectives that describe
characteristics of the previous theorem making it NOT interesting?
B
Second Survey—Characteristics of Interesting Theorems
This survey will only be designed after studying the results of the first survey. The second survey will
propose theorems (taken from the first survey) and will provide characteristics (taken from the first
survey) for each of them. The survey will ask the participants to express their opinion on characteristics
that (presumably) make the theorems interesting or not interesting.
This survey will be available online, based on an online survey tool.9
Please express whether you consider the following theorems interesting or not, and why?
Is Theorem n interesting?
□ YES
□ NO
Why? Because it has the characteristic A.
□ Strongly disagree □ Disagree □ Neutral □ Agree □ Strongly Agree
Why? Because it has the characteristic B.
□ Strongly disagree □ Disagree □ Neutral □ Agree □ Strongly Agree
Why? Because it has the characteristic C.
□ Strongly disagree □ Disagree □ Neutral □ Agree □ Strongly Agree
Why? Because it has the characteristic D.
□ Strongly disagree □ Disagree □ Neutral □ Agree □ Strongly Agree
"
"In this paper, we present a constraint solving-based approach for automated triangle construction. A finite-domain constraint model is developed using the MiniZinc modeling language, which is based on automated planning. We evaluate our approach on 74 solvable problems from the Wer-nick’s list and compare it to the ArgoTriCS dedicated triangle construction solver. Empirical results show that our approach is comparable to dedicated tools while requiring less implementation effort. Additionally, our approach often finds shorter constructions due to the optimization capabilities offered by constraint solvers.","Geometric construction problems, which involve constructing geometric figures using tools like rulers and compasses, have been studied extensively. While mathematicians enjoy solving such problems manually, computer scientists are drawn to automating the process. In this paper, we focus on triangle construction problems, where the goal is to construct the remaining vertices of a triangle given some elements (such as points or lines). We advocate for using finite-domain constraint solvers for this purpose, as they are efficient search engines that allow us to focus on the geometric aspects of the problem. We also highlight the optimization capabilities of these solvers, which enable us to search for constructions that are optimal in some sense, such as the shortest construction.","There are two main approaches to automating construction problems: (1) developing a specific search algorithm that incorporates geometric knowledge and (2) utilizing existing artificial intelligence tools designed for solving search problems. We adopt the second approach, using finite-domain constraint solvers. These solvers have proven to be efficient search engines, and they offer optimization capabilities, enabling us to search for the best construction in a given sense. We compare our approach to ArgoTriCS, a state-of-the-art tool for automated triangle constructions developed in Prolog.nannannan","Our approach is based on constraint solving. We encode construction problems as automated planning problems, where states correspond to sets of constructed objects, operators represent construction steps, and the goal is to construct the vertices of the triangle. We use the MiniZinc modeling language to represent the problem and rely on a constraint solver, specifically the chuffed solver, to find a plan (construction) of minimal length.nan","We evaluate our approach on 74 solvable problems from the Wernick’s list, which are triangle construction problems with three given elements (points, lines, or angles). We compare our approach to the ArgoTriCS solver, which is a dedicated triangle construction tool developed in Prolog. Our approach exhibits comparable performance to ArgoTriCS in terms of solving time, but our model often finds shorter constructions due to the optimization capabilities of constraint solvers. Our approach also requires significantly less implementation effort compared to ArgoTriCS.","Our constraint solving-based approach for automated triangle construction shows promising results. It is comparable to dedicated tools in terms of performance while requiring less implementation effort. Additionally, our approach often finds shorter constructions due to the optimization capabilities of constraint solvers. For future work, we plan to extend our model to support construction problems from other sets, which requires incorporating additional geometric knowledge into the model.",Automation of Triangle Ruler-and-Compass Constructions Using Constraint Solvers,Milan Banković,"P. Quaresma and Z. Kovács (Ed.): Automated Deduction
in Geometry 2023 (ADG 2023).
EPTCS 398, 2024, pp. 62–72, doi:10.4204/EPTCS.398.10
© Milan Bankovi´c
This work is licensed under the
Creative Commons Attribution License.
Automation of Triangle Ruler-and-Compass Constructions
Using Constraint Solvers
Milan Bankovi´c
Faculty of Mathematics, University of Belgrade, Serbia
milan.bankovic@matf.bg.ac.rs
In this paper, we present an approach to automated solving of triangle ruler-and-compass construction
problems using finite-domain constraint solvers. The constraint model is described in the MiniZinc
modeling language, and is based on the automated planning. The main benefit of using general con-
straint solvers for such purpose, instead of developing dedicated tools, is that we can rely on the
efficient search that is already implemented within the solver, enabling us to focus on geometric as-
pects of the problem. We may also use the solver’s built-in optimization capabilities to search for the
shortest possible constructions. We evaluate our approach on 74 solvable problems from the Wer-
nick’s list, and compare it to the dedicated triangle construction solver ArgoTriCS. The results show
that our approach is comparable to dedicated tools, while it requires much less effort to implement.
Also, our model often finds shorter constructions, thanks to the optimization capabilities offered by
the constraint solvers.
1
Introduction
One of the oldest and the most studied classes of problems in geometry is the class of construction
problems: given some elements of a figure (such as a triangle), we want to find a sequence of steps to
construct the remaining elements of the figure using the available tools – typically a ruler1 and a compass.
The beauty of this class of problems is that each problem is different and requires a specific, often very
deep geometric knowledge to be solved. Moreover, many problems are even proven to be unsolvable.
Although geometricians love to deal with such problems by hand, for computer scientists (who also
love geometry) it is tempting to try to automate the solving of construction problems. From the algo-
rithmic point of view, the construction problems are search problems, and the search space is usually
very large. There are two main lines of approaches here: one is to develop a specific search algorithm
in some programming language with required geometric knowledge compiled into it, and the other is to
use existing artificial intelligence tools that are good in solving search problems in general. In the second
case, one should only specify the problem and its constraints using some input language and then leave
the search to the tool.
In this paper, we advocate the second approach. More specifically, we show how finite-domain
constraint solvers [6] may be used for such purpose. We develop a constraint model in the MiniZinc
modeling language [4], based on the automated planning [2]. There are two main benefits of using
constraint solvers for this purpose:
• the constraint solvers are very efficient search engines, and by using them we may focus on geo-
metric aspects of the problem and on modeling the geometric knowledge required for its solving,
and leave the search to the tool that is good at it.
1A more accurate term would be straightedge, since a ruler is usually equipped with measuring marks, so it can be used to
measure lengths, which is typically not allowed in geometric constructions. Nevertheless, in this paper we use the term ruler
and consider it as a synonym for a straightedge.
Milan Bankovi´c
63
• the constraint solvers are usually equipped with optimization capabilities, enabling us to search for
a construction that is the best in some sense (for instance, the shortest possible construction may
be required). This can be done with the minimal effort, compared to developing specific search
algorithms with optimization capabilities (such as branch-and-bound algorithms).
We compare our approach to the state-of-the-art tool for automated generation of triangle construc-
tions ArgoTriCS [3], developed in the Prolog programming language. A detailed evaluation is performed
on 74 solvable problems from the Wernick’s set of triangle construction problems [7].
The rest of this paper is organized as follows. In Section 2, we introduce needed concepts and
notation used in the rest of this paper. In Section 3 we describe our constraint model. Section 4 provides
a detailed evaluation of the approach. Finally, in Section 5, we give some conclusions and mention some
directions of the further work.
2
Background
2.1
Ruler-and-Compass Constructions
In this paper, we consider ruler-and-compass triangle constructions, where the goal is to construct all
vertices of a triangle, assuming that some elements of the triangle (points, lines or angles) are given in
advance. A construction consists of a sequence of steps, where in each step some new objects (points,
lines, angles or circles) are constructed based on the objects constructed in previous steps. Constructions
performed in each of the steps are usually elementary ones, such as constructing the line passing through
two given points, or the point that is the intersection of two given lines, or the circle centered at a
given point that contains another given point. However, in order to simplify the description of a triangle
construction, some higher-level construction steps are also considered, such as constructing the tangents
to a given circle from a given point, or the line perpendicular or parallel to a given line and passing
through a given point, etc. Such higher-level constructions are called compound constructions, since
they can be easily decomposed into sequences of elementary construction steps.
In this paper, we focus on the Wernick’s list of triangle construction problems [7], where the fol-
lowing set of 16 characteristic points of a triangle is considered: the triangle vertices (A, B, C), the
circumcenter O, the incenter I, the orthocenter H, the centroid G, the feet of the altitudes (Ha, Hb, Hc),
the feet of the internal angles bisectors (Ta, Tb, Tc) and the midpoints of the triangle sides (Ma, Mb, Mc).
Each problem from the list assumes that three different points from this set are given, and the goal is
to construct all the vertices of the triangle. There are 560 such point triplets, but only 139 among them
represent significantly different problems (that is, mutually non-symmetric). Among these, only 74 prob-
lems are proven to be solvable by a ruler and a compass (others either contain redundant points, or are
undetermined, i.e. may have infinitely many solutions, or are proven to be unsolvable). In our work, we
consider only these 74 solvable problems from the Wernick’s list.
In further text, we rely on the notation used by Marinkovi´c [3]. We also assume the geometric
knowledge presented in [3], as well as the set of elementary and compound construction steps used in
that work.
2.2
Constraint Solving
In this work, we reduce triangle construction problems to constraint solving [6]. A finite-domain con-
straint satisfaction problem (CSP) consists of a finite set of variables X = {x1,...,xn}, each taking
64
Automation of Triangle Ruler-and-Compass Constructions Using Constraint Solvers
values from its given finite domain Di = D(xi), and a finite set of constraints C = {C1,...,Cm}, which
are relations over subsets of these variables. A solution of a CSP is an assignment (x1 = d1,...,xn = dn)
of values to variables (di ∈ Di), such that all the constraints of that CSP are satisfied. A CSP is satisfiable
if it has at least one solution, otherwise is unsatisfiable. The optimization version of CSP, known as
a constrained optimization problem (COP) additionally assumes a function f over the variables of the
problem that should be minimized (or maximized), with respect to the constraints from C .
Tools that implement procedures for solving CSPs (and COPs) are called constraint solvers. They
are usually based on a combination of a backtrack-based search and constraint propagation [6]. Con-
straint solvers have been successfully used for solving many real-world problems in many fields, such as
scheduling, planning, timetabling, combinatorial design, and so on.
An important step in using constraint solvers is constraint modeling, that is, representing a real-
world problem in terms of variables and constraints. A constraint model is usually described using
an appropriate modeling language. One such language supported by many modern constraint solvers
is MiniZinc [4]. This language offers a very flexible high-level environment for modeling different
kinds of constraints, enabling a compact and elegant way to represent some very complex problems.
Examples of some high level language elements include tuples, multi-dimensional arrays, sets, aggregate
functions, finite quantification and so on. Since most of these high level constructs are not supported by
backend solvers, each MiniZinc model must be translated into an equivalent FlatZinc form, containing
only primitive language constructs and constraints supported by a chosen backend solver. MiniZinc
supports modeling of both CSPs and COPs.
In MiniZinc, we distinguish variables from parameters. MiniZinc variables correspond to the vari-
ables of a CSP, i.e. we declare their domains and expect from the solver to find their values satisfying
the constraints. On the other hand, parameters are just named constants, and their values must be known
when the model is translated to the FlatZinc form (i.e. before the solving starts). Parameters are the
language’s construct that allow us to specify a general model for a class of problems, and then to choose
a specific instance of the problem by fixing the values of the model’s parameters. Parameter values are
usually provided in separate files (called data files), so that we can easily combine the same model with
different data.
In our work, we use MiniZinc as a modeling language.
2.3
Automated Planning
In our approach, triangle construction problems are considered as problems of automated planning [2].
An automated planning problem consists of the following:
• a set S of possible states, which are usually encoded by a set of variables V and the values
assigned to them. One distinguished state S0 ∈ S is the initial state.
• a set of operators O, where each operator o ∈ O consists of a precondition Co describing the con-
ditions (in terms of the variables from V ) that must be satisfied in the current state for the operator
to be applied, and a set of effects Eo (represented as variable-value assignments) describing how
the current state is changed when o is applied to it. The state obtained by applying an operator o
to some state S is denoted by o(S).
• a goal G, describing the conditions (in terms of the variables from V ) that must be satisfied in the
final state.
The objective of automated planning is to find a plan, that is, a finite sequence of operators o1,...,on
from O that can be successively applied to the initial state S0 (i.e. for each i ∈ {1,...,n}, we have
Milan Bankovi´c
65
Si = oi(Si−1), and the state Si−1 satisfies the precondition Coi) producing the final state Sn satisfying the
goal G. The number n of operators used in a plan is called the length of the plan.
The problem of checking whether a plan (of any finite length) exists is PSPACE-complete in general
[1]. For a fixed plan length n, the problem is NP-complete in general, and can be encoded as a CSP [2, 5].
The optimization variant of the planning problem (i.e. finding a plan of the minimal possible length)
can be solved by successively checking for existence of plans of lengths n = 1,2,3,..., that is, by solving
the corresponding sequence of CSPs until a satisfiable one is encountered.
3
Model Description
The triangle construction problems that we consider in this paper can be naturally described as problems
of automated planning:
• states correspond to the sets of constructed objects, and the initial state is the set consisting of the
given elements of the triangle (three points in case of Wernick’s problems).
• operators correspond to the construction steps; the precondition for each operator is that objects
used in the corresponding construction step are already constructed (i.e. belong to the current
state), and that corresponding non-degeneracy and determination conditions are satisfied (e.g. two
lines must be distinct and non-parallel in order to construct their intersection); the effect of each
operator is the addition of the objects constructed by the corresponding construction step to the
current state.
• the goal condition is that vertices A, B and C belong to the final state.
The corresponding planning problem for a fixed plan (construction) length is encoded as a CSP using
the MiniZinc language.2 In the rest of this section, we discuss different aspects of the encoding in more
detail.
3.1
Encoding of Geometric Knowledge
Encoding objects.
We consider four types of objects: points, lines, circles and angles. Each of these
types is encoded as an enumeration type in MiniZinc (Point, Line, Circle and Angle, respectively),
and each object is represented by one enumerator of the corresponding type. The enumerated objects
are the only objects that can be constructed. This means that we have to anticipate in advance the set of
objects that might be needed during the construction.
Encoding relations.
Different relations between the enumerated objects are encoded by the parameters
of the model, using MiniZinc’s arrays, sets and tuples. These relations are used to statically encode the
geometric knowledge used in the constructions. We define the following types of relations:
• incidence relations: we define two arrays of sets, inc_lines and inc_circles, indexed by
points. The set inc_lines[p] contains the lines incident with the point p, and the set inc_-
circles[p] contains the circles incident with the point p.
• relations between lines: we define two arrays of sets, perp_lines and paralell_lines, in-
dexed by lines. The set perp_lines[l] contains the lines perpendicular to the line l, and the set
parallel_lines[l] contains the lines parallel to the line l.
2The model is available at: https://github.com/milanbankovic/constructions/.
66
Automation of Triangle Ruler-and-Compass Constructions Using Constraint Solvers
• circle tangents, diameters and centers: the array of points circle_center indexed by circles
contains information about circle centers; the array circle_diameter of point pairs indexed by
circles contains information about circle diameters; the array tangent_lines of sets of lines is
indexed by circles, and the set tangent_lines[c] contains the lines that are tangents of the circle
c.
• vector ratios: we use the array known_ratio_triplets of point triplets to store the information
about the triplets of collinear points (X,Y,Z) such that the ratio −→
XY/−→
YZ is known. The exact value
of the ratio is not encoded, since it is not important for the search (it is only important to know that
we can construct one of the points if the remaining two are already constructed). Similarly, we use
the array known_ratio_quadruplets to encode quadruplets of points (X,Y,Z,W) such that the
ratio −→
XY/−→
ZW is known.
• angles between the lines: we use the array angle_defs of Line×Line×Angle triplets, to encode
the information about the angles between the lines. A triplet (p,q,φ) means that the angle between
the lines p and q is determined by φ (e.g. is equal to φ/2 or φ + π/2). Such information can be
used in two directions: if we have constructed p and q, we can construct the angle φ; also if we
have constructed p and φ, and the intersection point X of p and q, we can then construct the line q.
• perpendicular bisectors of segments: we use the array perp_bisectors of Point×Point×Line
triplets to encode the information about the perpendicular bisectors of line segments.
• harmonic conjugates: we use the array harmonic_quadruplets of point quadruplets, where a
quadruplet (X,Y;Z,W) encodes that the points X and Y are harmonic conjugates of each other
with respect to the pair (Z,W).
• loci of points: we use the array locus_defs of Point×Point×Angle×Circle tuples, where
a tuple (X,Y,φ,c) encodes that the locus of points such that the segment XY is seen at an angle
determined by φ is an arc of the circle c.
• homothetic images of lines: we use the array homothety_triplets of Point × Line × Line
triplets, where a triplet (X, p,q) encodes that the line q is the image of the line p by homothety
centered in the point X (again, homothety coefficient is not stored in the database).
3.2
Encoding of the Planning Problem
Encoding of states.
Let n be the length of a plan that we are searching for, let S0 be the initial state, and
let Si be the state after the ith step. To encode these states, we introduce arrays of variable sets known_-
points, known_lines, known_circles and known_angles, where, for instance, known_points[i]
(i ∈ {0,...,n}) denotes the set of points belonging to the state Si (similarly for other arrays). The initial
state S0 is fixed in advance by appropriate constraints (for instance known_points[0] = {A,G,O}).
Encoding the plan.
We define the enumeration type ConsType, with one enumerator for each sup-
ported type of construction step. We also define the array construct of variables of type ConsType
(with indices in {1,...,n}) encoding operators used in each step (i.e. the construction step types). For
each step, we also need additional information to fully determine the actual construction (for instance, if
we choose to construct the intersection of two lines, we must also choose the lines that we want to in-
tersect). For this reason, we also introduce additional two-dimensional arrays of variables: for instance,
points[i][ j] denotes the jth point used in the ith construction step (similarly we have lines[i][ j],
circles[i][j] and angles[i][ j]).
Milan Bankovi´c
67
Encoding the state transitions.
Finally, to glue the whole plan together, we must add the constraints
that connect the state variables in the successive states, depending on the chosen operator in the corre-
sponding step. This must be done for each i ∈ {1,...,n}, and that is where MiniZinc’s finite universal
quantification comes in handy:
constraint forall(i in 1..n)
(
construct[i] = LineIntersect ->
% Precondition
(lines[i,1] in known_lines[i-1] /\
lines[i,2] in known_lines[i-1] /\
lines[i,1] != lines[i,2] /\
not (lines[i,1] in parallel_lines[lines[i,2]]) /\
lines[i,1] in inc_lines[points[i,1]] /\
lines[i,2] in inc_lines[points[i,1]] /\
not (points[i,1] in known_points[i-1]) /\
% Effects
known_points[i] = known_points[i-1] union { points[i,1] } /\
known_lines[i] = known_lines[i-1] /\
known_circles[i] = known_circles[i-1] /\
known_angles[i] = known_angles[i - 1]
)
);
That is, for all i ∈ {1,...,n}, if the chosen operator is LineIntersect (constructing the intersection
of two lines), then the chosen two lines lines[i,1] and lines[i,2] must belong to the current state
Si−1 (i.e. they must have been already constructed), they must be distinct and not parallel. Also, the
chosen point points[i,1] must belong to both chosen lines (i.e. it must be their intersection), and it
must not belong to the current state (we do not want to construct a point that is already constructed).
If all these preconditions are met, then the effect is that the set known_points[i] is obtained by adding
the intersection point points[i,1] to the set known_points[i − 1] (the sets of lines, circles and angles
remain the same). Similar constraints are defined for all other types of construction steps.
Encoding the goal.
The goal is encoded simply by adding the constraints that require that the vertices
A, B and C belong to the set known_points[n]:
{ A, B, C } subset known_points[n];
4
Evaluation
The model described in the previous section is evaluated on 74 solvable instances from Wernick’s set
[7]. The experiments were performed on a computer with 3.1GHz processor and 8Gb of RAM. We used
official MiniZinc distribution3 for experiments (version 2.7.2). We have experimented with different
3https://www.minizinc.org/software.html
68
Automation of Triangle Ruler-and-Compass Constructions Using Constraint Solvers
backend constraint solvers provided within MiniZinc distribution, and by far the best results were ob-
tained by the chuffed4 solver. Therefore, in the rest of this section, we present only the results obtained
by chuffed.
We looked for plans of minimal lengths (i.e. constructions with the minimal possible numbers of
steps). We used three different setups:
• linear setup: for each of the problems, we successively look for plans of length n = 1,2,3,..., and
stop when we encounter a satisfiable CSP, or when some upper limit maxSteps is exceeded. This is
the usual way for finding plans of minimal lengths in automated planning [5]. In our experiments,
the upper limit for the plan length was set to 11, since our preliminary experiments had shown that
all the problems that our model could solve had been solved in at most 11 steps. Note that in this
setup the value of maxSteps does not affect the solving time for problems that our model can solve
(that is, using a greater value of maxSteps would not slow down the search).
• minimization setup: we reformulate our model such that the plan length n is not fixed. Instead, n
is a variable with a domain {1,...,maxSteps} and we are trying to minimize the value of n (that
is, we are solving a constrained optimization problem). The problem with this approach is how
to determine the value of maxSteps parameter, since in this setup greater values of this parameter
make the model larger and the search becomes slower, even for problems that can be solved in
a small number of steps. In our experiments, we used the value maxSteps = 11, but this was
somewhat artificial choice, since we used the previous knowledge to choose the minimal possible
number of steps sufficient to solve all the problems that our model was able to solve.
• incremental setup: just like in the previous setup, we reformulate our model such that we are
trying to minimize n, but this time the domain for n is some interval {l,...,u}, where l and u are
parameters. Now we successively solve constrained optimization problems for intervals {1,...,k},
{k + 1,...,2k},{2k + 1,...,3k},. . ., until some of them turns out to be satisfiable, or until some
upper limit maxSteps is exceeded. Like in the first setup, the choice for the value of the parameter
maxSteps does not affect the solving time for the problems that are solvable by our model. On the
other hand, the number of COPs solved is smaller roughly by the factor k, compared to the first
setup. We have experimented with multiple choices for k, and the best results were obtained for
k = 3.
In Table 1, we provide the main results of our evaluation. We have evaluated all three setups described
above. We also compared our approach to the results obtained by the ArgoTriCS dedicated triangle
construction solver developed by Marinkovi´c [3]. ArgoTriCS is implemented in Prolog programming
language, but it uses a very similar knowledge base and an almost identical set of available construction
steps.
Setup
# solved
Avg. time
Median time
Avg. time on solved
Avg. length
linear
63
97.9
22.0
58.5
6.3
minimization
63
43.8
10.8
29.7
6.3
incremental (k = 3)
63
66.1
12.0
39.9
6.3
ArgoTriCS
65
54.5
21.6
54.4
7.5
Table 1: Overall results for different setups, compared to ArgoTriCS. Times are given in seconds
4https://github.com/chuffed/chuffed
Milan Bankovi´c
69
Note that the choice of the setup does not affect how many problems from Wernick’s list will be
solved, since this depends only on the geometric knowledge that is compiled into our model.5 In total,
we managed to solve 63 of 74 problems (for the remaining 11 problems, the constraint solver reported
unsatisfiability). On the other hand, ArgoTriCS solved 2 problems more. This is because we missed to
incorporate some of the objects and lemmas known to ArgoTriCS to our model.
The best average solving time is obtained by the minimization setup. However, as we mentioned
earlier, the average solving time in this setup greatly depends on the choice for the maximal possible
value of n. The results shown in Table 1 are obtained for maxSteps = 11. We also experimented with
some greater values. For instance, for maxSteps = 20 the average solving time was over 100 seconds,
that is, more than twice greater (of course, the number of solved problems remained the same).
The linear setup has shown the worst performance. This is because in this setup we were solving
many unsatisfiable CSPs until we possibly reached some satisfiable CSP. Unsatisfiable CSPs tend to
consume more time, especially those that are “almost satisfiable”, that is, that are close to some phase
transition point. This phenomenon is well-known in automated planning [5].
Figure 1: Per-instance comparison of different setups. Times are given in seconds
The performance of the incremental setup was much better on average than in case of the linear
setup, and a little worse than in case of the minimization setup, but still comparable. A more detailed,
per-instance comparison is shown in Figure 1. We can see that the incremental setup was uniformly
better than linear setup, and was also better than the minimization setup on easier instances, but it was
outperformed by the minimization setup on harder problems. Overall, the incremental setup seems as a
good choice in a realistic context, when we do not know in advance the value of maxSteps parameter.
The overall performance of the ArgoTriCS solver was comparable to our approach, when the average
solving time is concerned. However, we may notice that its median solving time was almost twice
5This means that we can improve our results by carefully examining the knowledge needed for solving the unsolved prob-
lems, and incorporating that knowledge into our model. However, such enrichment of the model enlarges the search space and
makes the solving slower even for the problems that are already solvable by our model.
70
Automation of Triangle Ruler-and-Compass Constructions Using Constraint Solvers
Figure 2: Survival plot for all three setups, compared to ArgoTriCS. Times are given in seconds
greater than in case of our minimization or incremental setup. Also, the average solving time on solved
instances was much better in our approach. This suggests that our approach performed better than (or
comparable to) ArgoTriCS on problems for which it managed to find a construction plan, especially on
easier instances. This is confirmed in Figure 2, which shows the survival plot for all three setups and
ArgoTriCS. The minimization setup was clearly the best, while the linear setup was the worst. When
compared to ArgoTriCS, the incremental setup was cumulatively better on more than 60 instances, which
were roughly all the instances that our model managed to solve. This means that if our model can find a
solution, it can do it fast, while its performance is much worse when it comes to the instances that are out
of its reach (that is, when the corresponding CSPs are unsatisfiable). On the other hand, the performance
of ArgoTriCS had much smaller variance – it performed almost equally solid on all instances (as it can
be seen from Table 1, the average solving time on solved instances for ArgoTriCS is almost the same as
the average solving time on all instances).
The final comparison between ArgoTriCS and our approach concerns the lengths of the obtained
constructions. Table 1 shows that the average plan length in our approach was 6.3 (again, this does not
depend on the chosen setup). On the other hand, the average number of steps in ArgoTriCS’s construc-
tions was 7.5. Notice that these numbers are comparable, since the sets of available construction steps
in both systems are almost identical. A more detailed, per-instance comparison is shown in Figure 3.
The plot clearly confirms that our approach is by far superior when finding the shortest constructions
is concerned. However, for the sake of fairness, we must stress that ArgoTriCS was not designed with
that optimization in mind, that is, it does not even search for the shortest solutions. We guess that such
a capability could be integrated in ArgoTriCS, but with much more effort, since it would have to be
manually implemented in Prolog (just like the search itself). On the other hand, in our approach, we rely
on the built-in capabilities of constraint solvers to solve optimization problems efficiently, imposing the
minimal possible effort on our side.
Milan Bankovi´c
71
Figure 3: A per-instance comparison of construction (plan) lengths between ArgoTriCS and our approach
5
Conclusions and Further Work
In this paper we presented and evaluated a method for automated triangle construction based on con-
straint solving. We compared our method to the state-of-the-art dedicated triangle construction solver
ArgoTriCS, developed in Prolog programming language. We advocate that our approach has two impor-
tant advantages. First, our approach is much simpler to implement, since we rely on powerful constraint
solvers which can efficiently do the search for us, and we may focus only on modeling. On the other side,
in the ArgoTriCS solver the search is implemented by hand, in more than 500 lines of code. Second, we
can easily employ the optimization capabilities of modern constraint solvers to search for the shortest
possible constructions, while implementing such functionality in ArgoTriCS would require much more
effort.
We evaluated our approach on 74 solvable problems from the Wernick’s list. The results showed that
our approach is comparable to ArgoTriCS when solving time is concerned. On the other hand, our model
often finds shorter constructions, due to built-in optimization capability which is missing in ArgoTriCS.
For further work, we plan to extend our model to support construction problems from other sets.
This should not be a hard task in the technical sense, since the model is developed such that it can be
easily extended (that is, we can easily add new objects, relations and construction step types). The real
challenge is to recognize and integrate the geometric knowledge needed for such constructions into the
model. Of course, this is a job for geometricians, and our goal was to provide them with (what we hope
is) a useful tool that can free them from the tedious task of programming, and let them focus on what
they do the best and love the most.
72
Automation of Triangle Ruler-and-Compass Constructions Using Constraint Solvers
Acknowledgements.
This work was partially supported by the Serbian Ministry of Science grant
174021. We are very grateful to the anonymous reviewers whose insightful comments and remarks
helped us to make this paper much better.
References
[1] Tom Bylander (1994): The computational complexity of propositional STRIPS planning. Artificial Intelligence
69(1-2), pp. 165–204, doi:10.1016/0004-3702(94)90081-7.
[2] Malik Ghallab, Dana Nau & Paolo Traverso (2004): Automated Planning: theory and practice. Elsevier.
[3] Vesna Marinkovi´c (2017): ArgoTriCS–automated triangle construction solver. Journal of Experimental &
Theoretical Artificial Intelligence 29(2), pp. 247–271, doi:10.1080/0952813X.2015.1132271.
[4] Nicholas Nethercote, Peter J Stuckey, Ralph Becket, Sebastian Brand, Gregory J Duck & Guido Tack
(2007): MiniZinc: Towards a standard CP modelling language. In: Principles and Practice of Constraint
Programming–CP 2007: 13th International Conference, CP 2007, Providence, RI, USA, September 23-27,
2007. Proceedings 13, Springer, pp. 529–543, doi:10.1007/978-3-540-74970-7_38.
[5] Jussi Rintanen (2009): Planning and SAT. Handbook of Satisfiability 185, pp. 483–504.
[6] Francesca Rossi, Peter Van Beek & Toby Walsh (2006): Handbook of constraint programming. Elsevier.
[7] William Wernick (1982): Triangle constructions with three located points. Mathematics Magazine 55(4), pp.
227–230, doi:10.1080/0025570X.1985.11976988.
"
"We explore the vulnerability of learned image compression models against adversarial attacks and propose a training-free defense method grounded on simple image transformation functions. Recent learned image compression models are susceptible to adversarial attacks that result in weakened compression rate, deteriorated reconstruction quality, or anomalous artifacts. To address these limitations, we propose a straightforward but effective two-way compression algorithm with haphazard input transforms, which is conveniently applicable to existing image compression models. Unlike the naive approaches, our method preserves the models' original rate-distortion performance on clean images. Furthermore, the proposed algorithm requires no additional training or modification of existing models, making it more practical. We demonstrate the effectiveness of the proposed technique through extensive experiments under multiple compression models, evaluation metrics, and attack scenarios.","It is well-known that deep neural networks trained for image recognition are susceptible to adversarial attacks [Szegedy et al., 2014]. By applying small and indiscernible perturbations to the input images, the networks are effortlessly deceived to behave in accordance with the attacker's intentions. These attacks often cause a significant drop in the models' performance, which directly undermines the overall system's security and robustness. Similarly, adversarial attacks against learned image compression models are also possible. Two feasible threats to lossy image compression exist, namely, failure of bitrate reduction and severe distortion of decoded images.","Previous attack algorithms proposed for other tasks have turned out to be generalizable to image compression models [Chen and Ma, 2023; Liu et al., 2023; Sui et al., 2023; Yu et al., 2023]. However, defense techniques for image compression are still in their infancy, and naively applying defense methods designed for other tasks to image compression may not yield satisfactory results. To enhance the robustness of image compression models, one can adopt approaches such as adversarial fine-tuning, a straightforward method suggested in [Chen and Ma, 2023]. However, this approach necessitates additional model training, which consequently degrades the models' original compression performance on normal, unattacked images. Another defense strategy preprocesses the input images through operations such as Gaussian blurring and bit depth reduction [Xu et al., 2018]. However, these methods inevitably increase the reconstruction errors of normal images due to the content loss caused by the preprocessing, as discussed in [Yu et al., 2023].nannannan","Our proposed framework achieves robustness against adversarial perturbations while retaining performance on clean images without the need for additional model training. The core idea is to select the better option out of two different types of compression methods, which is feasible due to the availability of self-supervision in image compression. The encoding process for an input image x is as follows: First, we calculate the rate-distortion loss of x by encoding followed by decoding, without input transform. If the rate-distortion loss with the input transform is lower than without the transform, we select the transformed image for compression; otherwise, we choose the original image. The decoding process is similar, with the transform index used to reconstruct the original image. Our method prevents compression quality degradation on the original images while improving the adversarial robustness of the compression model.nan","Our proposed two-way compression approach effectively mitigates adversarial effects on image compression models while preserving performance on clean images, without the need for additional training. The approach outperforms naive input randomization in image compression and adversarial training. Additionally, it exhibits generalizability to feature-based attacks like FDA. The encoding time overhead is minimal, especially for high-performance models utilizing masked convolutions.","We investigated the vulnerability of learned image compression models to adversarial attacks and introduced a training-free defense strategy. Our findings demonstrate that basic attack algorithms can easily harm recent image compression models in terms of rate and distortion. To address these risks, we propose a simple yet effective image compression framework for defense. Our framework improves the stability of compression performance against various adversarial attacks with negligible performance degradation on clean images. It leverages input randomization in a safe manner based on the self-supervised nature of the image compression problem. Our approach is directly applicable to pretrained compression models without additional training, making it practical. The effectiveness of our defense method against attacks is illustrated through extensive experiments.",A Training-Free Defense Framework for Robust Learned Image Compression,"Myungseo Song, Jinyoung Choi, Bohyung Han","A Training-Free Defense Framework for Robust Learned Image Compression
Myungseo Song
Jinyoung Choi
Bohyung Han
Computer Vision Laboratory, Seoul National University
{micmic123, jin0.choi, bhhan}@snu.ac.kr
Abstract
We study the robustness of learned image compres-
sion models against adversarial attacks and present
a training-free defense technique based on simple
image transform functions. Recent learned image
compression models are vulnerable to adversarial
attacks that result in poor compression rate, low re-
construction quality, or weird artifacts. To address
the limitations, we propose a simple but effective
two-way compression algorithm with random in-
put transforms, which is conveniently applicable
to existing image compression models. Unlike the
na¨ıve approaches, our approach preserves the orig-
inal rate-distortion performance of the models on
clean images. Moreover, the proposed algorithm
requires no additional training or modification of
existing models, making it more practical.
We
demonstrate the effectiveness of the proposed tech-
niques through extensive experiments under mul-
tiple compression models, evaluation metrics, and
attack scenarios.
1
Introuduction
It is well-known that deep neural networks trained for image
recognition are vulnerable to adversarial attacks [Szegedy et
al., 2014]. By small and imperceptible perturbations on input
images, the networks are easily deceived to behave for the
intent of the attackers. The performance of the models often
drops significantly, which directly hampers the security and
robustness of a whole system.
As with other fields, adversarial attacks against learned im-
age compression models are possible as well. There are two
feasible threats to lossy image compression, i.e., failure of
bitrate reduction and severe distortion of decoded images.
Figure 1 presents an example of perturbed image and cor-
responding decoded image by an image compression model
with weird artifacts. These limitations of image compression
have far-reaching power affecting subsequent downstream
tasks such as classification and detection. In this respect, it
is worth paying attention to the robustness of image compres-
sion models and their defense techniques against attacks.
Compared to the recognition domains, the robustness of
Clean
Perturbed
Input
Source Image
Reconstruction without defense
Reconstruction with defense
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 / 0.9770 
0.8110 / 32.0145 / 0.9602
0.3262 / 39.3297 / 0.9873
0.3258 / 39.3297 / 0.9873
0.9136 / 8.3073 / 0.2963
0.3262 / 39.3297 / 0.9873
Source Image
Reconstruction without defense
Rec
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 /
0.3258 / 39.3297 / 0.9873
0.9136 / 8.3073 / 0.2963
0.3262 / 39.329
w/o Defense
Source Image
Reconstruction without defense
Reconstruction with defense
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 / 0.9770 
0.8110 / 32.0145 / 0.9602
0.3262 / 39.3297 / 0.9873
0.3258 / 39.3297 / 0.9873
0.9136 / 8.3073 / 0.2963
0.3262 / 39.3297 / 0.9873
0.3258 / 39.3297 / 0.9873
Source Image
Reconstruction without defense
Rec
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 /
0.3258 / 39.3297 / 0.9873
0.9136 / 8.3073 / 0.2963
0.3262 / 39.329
0.4535 / 10.0249 / 0.5533 
w/ Defense
Source Image
Reconstruction without defense
Reconstruction with defense
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 / 0.9770 
0.8110 / 32.0145 / 0.9602
0.3262 / 39.3297 / 0.9873
0.3258 / 39.3297 / 0.9873
0.9136 / 8.3073 / 0.2963
0.3262 / 39.3297 / 0.9873
0.3262 / 39.3297 / 0.9873
Source Image
Reconstruction without defense
Rec
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 /
0.3258 / 39.3297 / 0.9873
0.9136 / 8.3073 / 0.2963
0.3262 / 39.329
0.4192 / 35.6980 / 0.9770 
Figure 1: Demonstration of the vulnerability of learned image com-
pression model to adversarial attacks and effectiveness of our de-
fense method. The yellow annotations in each reconstructed image
denote bits per pixel (bpp)/PSNR (dB)/MS-SSIM.
deep image compression models have not been studied com-
prehensively.
Some attack algorithms proposed for other
tasks have turned out to be generalizable to image com-
pression models [Chen and Ma, 2023; Liu et al., 2023;
Sui et al., 2023; Yu et al., 2023]. However, defense tech-
niques for image compression are not mature yet, and a na¨ıve
application of defense methods designed for other tasks may
not work properly in image compression.
To enhance the robustness of image compression models,
one can adopt approaches such as adversarial fine-tuning, a
straightforward method suggested in [Chen and Ma, 2023].
However, this approach requires additional model training
and consequently degrades the original compression perfor-
mance of the models on normal, unattacked images.
An-
other defense strategy performs preprocessing on input im-
ages such as Gaussian blurring and bit depth reduction [Xu
et al., 2018]. However, these methods inevitably increase re-
construction errors of normal images due to the content loss
caused by the preprocessing, as discussed in [Yu et al., 2023].
This work investigates the vulnerability of learned image
arXiv:2401.11902v1  [eess.IV]  22 Jan 2024
Figure 2: Examples of adversarially perturbed images (top) and cor-
responding reconstructed images (bottom).
compression models and introduces a training-free defense
strategy. We show that the performance of recent image com-
pression models are easily harmed by basic attack algorithms
in terms of rate and distortion. To avoid these risks, we pro-
pose a simple yet effective image compression framework for
defense. Our framework improves the stability of compres-
sion performance to diverse adversarial attacks with negligi-
ble performance degradation on clean images. It leverages in-
put randomization in a safe way based on the self-supervised
nature of the image compression problem. Our approach is
directly applicable to pretrained compression models without
additional training, hence practical. The effectiveness of our
defense method against the attack is illustrated in Figure 1.
The main contributions of this paper are summarized as
(i) the investigation of adversarial attacks on learned image
compression models, (ii) the proposal of simple and effective
defense techniques against the attacks, and (iii) the evaluation
on the robustness of the proposed compression framework.
2
Related Works
This section briefly describes adversarial attack and defense
methods in classificaiton and compression fields.
2.1
Adversarial Robustness of Image Classification
After Szegedy et al. [2014] first showed the adversarial vul-
nerabilites of classifiers, several attack methods have been
introduced, including FGSM [Goodfellow et al., 2015],
C&W [Carlini and Wagner, 2017], DeepFool [Moosavi-
Dezfooli et al., 2016], and PGD [Madry et al., 2018]. They
share the key idea of adding minimal perturbations on an im-
age iteratively towards the decision boundary of a classifier.
FDA [Ganeshan et al., 2019] perturbs an image by disrupt-
ing the statistics of the intermediate features of a model. For
defense, the adversarial training, adding adversarial examples
into training dataset, is a mainstream technique [Goodfellow
et al., 2015; Madry et al., 2018; Tram`er et al., 2018; Kannan
et al., 2018]. As another line of research, [Guo et al., 2018;
Xie et al., 2018] attempts to reduce the chance of success-
ful attacks by randomizing inputs while [Xu et al., 2018;
Samangouei et al., 2018] defend the models by denoising
through optimization.
2.2
Adversarial Robustness of Image Compression
Learend image compression methods typically adopt autoen-
coder networks with auxiliary entropy models for probabil-
ity distribution estimation of latent representations [Ball´e et
Model
Low bitrate
High bitrate
SH
5M
12M
M&S
7M
18M
M&S+C
14M
26M
Anchor
12M
27M
Table 1: The number of parameters of the compression models used
in our experiments with respect to their target bitrates.
al., 2018; Minnen et al., 2018; Cheng et al., 2020]. Adver-
sarial attacks on image compression models are achieved by
either increasing the bitstream lengths of latent representa-
tions or degrading the quality of decoded images. Recently,
researchers start to explore and investigate the adversarial ro-
bustness of image compression models. For example, Chen
and Ma [2023] corrupt the reconstruction quality of the mod-
els via distortion attack. Although they leverage adversar-
ial fine-tuning to address the vulnerabilites of the models, it
leads to compression quality degradation of unattacked im-
ages. Liu et al. [2023] conduct transferring attacks [Papernot
et al., 2016] using a JPEG-like substitution model in a black-
box attack scenario. Sui et al. [2023] propose a distortion at-
tack algorithm with less perceptible perturbations, and Yu et
al. [2023] introduce a trigger injection model for backdoor
attack.
3
Adversarial Attack on Learned Image
Compression
This section presents the basic techniques of learned image
compression and adversarial attacks on it. Next, we discuss
the vulnerability of image compression in diverse apsects.
3.1
Preliminaries
The goal of lossy image compression is to minimize the bit-
stream length of an image while preserving the content in the
image as much as possible. Typically, a compression system
consists of an encoder E, a decoder D, a quantizer Q, and an
entropy model P.
Given a source image x, E transforms x to a latent rep-
resentation y = E(x), which is then converted to a quan-
tized latent representation ˆy = Q(y). To save ˆy, an entropy
coding algorithm like the arithmetic coding [Rissanen and
Langdon, 1981] encodes ˆy into a bitstream with the prob-
ability distribution of ˆy estimated by P. The length of the
resulting bitstream is approximately − log P(ˆy) with minor
overhead hence is often used as a surrogate of the rate loss
term. For decoding, D generates the reconstructed image ˆx
from the quantized latent representation ˆy, i.e., ˆx = D(ˆy).
Given a distrotion metric d(·, ·) such as the mean squared er-
ror (MSE), the rate-distortion loss LRD is given by the sum
of the rate loss Lrate = − log P(ˆy) and the distortion loss
Ldist = d(x, ˆx) as follows:
LRD = Lrate + λLdist = − log P(ˆy) + λd(x, ˆx),
(1)
where a Lagrangian multiplier λ controls the rate-distortion
trade-off.
Then, the objective of the image compression
model is given by
min Ex∼px [LRD] .
(2)
(a) SH
(b) M&S
(c) M&S+C
(d) Anchor
Figure 3: Results of adversarial attacks on image compression models for poor compression rates with various ϵ values for PGD algorithm.
Top: results of low-bitrate models. Bottom: results of high-bitrate models. Clean denotes the performance on clean (i.e., unperturbed) images.
Our experiments use four pretrained lossy image com-
pression models available at an open-source compression li-
brary [B´egaint et al., 2020]: Scale Hyperprior (SH) [Ball´e
et al., 2018], Mean & Scale Hyperprior (M&S) [Min-
nen et al., 2018], Mean & Scale Hyperprior with context
model (M&S+C) [Minnen et al., 2018], and Anchor (An-
chor) [Cheng et al., 2020]. Table 1 shows the number of pa-
rameters of the models. Note that the models for high bitrates
have more parameters than the low-bitrate counterparts.
3.2
Attack Algorithm for Image Compression
Among the adversarial attack strategies, we mainly adopt a
famous optimization-based attack method, called the PGD al-
gorithm [Madry et al., 2018]. To generate an adversarial ex-
ample from a source image x, PGD iteratively updates x with
a step size α under the ℓ∞-norm constraint of the maximum
per-pixel perturbation ϵ, which is given by
xt+1 = xt + α · sgn(▽L),
(3)
where L denotes a task-specific loss and sgn(·) ∈ {−1, 1}
is the sign function.
Since compression models minimize
the rate-distortion trade-off LRD, one can attack the model in
terms of rate and distortion, for which the objective functions
L are defined as Lrate and Ldist, respectively. It is also possi-
ble to employ the joint rate-distortion objective for attack by
setting L = LRD, but it makes the analysis more complex due
to the conflicting properties of the two terms. For the lossless
image compression, only the rate loss is treated as a target
since the source image content should be perfectly recovered.
3.3
Results on Adversaries
Qualitative results
Figure 2 illustrates several adversaries
of distortion attacks on M&S and their corresponding recon-
structed images. The weird artifacts in the reconstructed im-
ages are easily induced by the attack, which shows the vul-
nerability of the model.
Quantitative results
Figure 3 presents the results of adver-
sarial attacks on four compression models with respect to the
rate by varying the value of ϵ for the PGD algorithm. The
larger ϵ is, the more performance degradation is observed
consistently for all models. Also, the high-bitrate models tend
to be more vulnerable to the attacks than the low-bitrate ones.
This is partly because (i) the high-bitrate models with more
parameters have more overfitting issues than the low-bitrate
ones and (ii) the low-bitrate models have high reconstruction
errors especially for high-frequency signals and hence tend to
be robust to the adversarial noise given to input images. The
relationship between the model complexity and the vulnera-
bility is discussed more in Appendix A. The result of distor-
tion attack is presented in Appendix B. To mitigate these ad-
versarial effects, appropriate defense techniques are requird.
4
Defending Adversarial Attacks
This section reviews the input randomization defense tech-
nique [Xie et al., 2018] proposed for image classification,
and discusses its limitations of direct application to image
compression. Then, we present our main idea of training-free
defense technique for image compression models.
4.1
Input Randomization for Image Classification
The input randomization [Xie et al., 2018] is a technique
without training for mitigating the adversarial effects of im-
age classification models. It first defines a set of image trans-
formations T = {τ1, ..., τn}, where τθ is an image transfor-
mation (e.g., cropping). For an input image x, a transform
Quantizer
Source Image
Random Transform
Encoder
Recon. Image
Bitstream
𝛳
Decoder
Inverse Transform
Source Image
Random Transform
Classifier
Class probabilities
Source Image
Random Transform
Encoder
Decoded Image
Bitstream
𝛳
Decoder
Inverse Transform
Bitstream
𝛳
Source Image
Random Transform
Classifier
Probabilities
Source Image
Random Transform
Classifier
Probabilities
Quantizer
Source Image
Random Transform
Encoder
Recon. Image
Bitstream
𝛳
Decoder
Inverse Transform
Source Image
Random Transform
Classifier
Class probabilities
Source Image
Random Transform
Encoder
Decoded Image
Bitstream
𝛳
Decoder
Inverse Transform
Bitstream
𝛳
Source Image
Random Transform
Classifier
Probabilities
Source Image
Random Transform
Encoder
Bitstream
𝛳
Quantizer
Source Image
Random Transform
Encoder
Recon. Image
Bitstream
𝛳
Decoder
Inverse Transform
Source Image
Random Transform
Classifier
Class probabilities
Source Image
Random Transform
Encoder
Decoded Imag
Bitstream
𝛳
Decoder
Inverse Transfo
Bitstream
𝛳
Source Image
Random Transform
Classifier
Probabilities
Decoded Image
Decoder
Inverse Transform
Bitstream
𝛳
(a)
(b)
(c)
Figure 4: (a) Input randomization for image classification. (b), (c)
Input randomization for encoder and decoder of image compression.
τθ is randomly sampled from T and the transformed image is
given by
xt = τθ(x), where τθ ∈ T .
(4)
Then, xt is fed to the classification model for prediction.
Specifically, [Xie et al., 2018] adopts resizing followed by
zero padding for the transforms, T .
The randomness provided by random transforms improves
the robustness of the model. The attackers cannot perform
precise inference due to the randomness; the attack is subop-
timal because the attackers should consider all possible trans-
forms if n is sufficiently large. Next, we describe how to
apply it to image compression and its challenges.
4.2
Input Randomization for Image Compression
To alleviate the adversarial effects on image compression
models without additional training, we leverage the afore-
mentioned input randomization technique [Xie et al., 2018].
Figure 4 compares the input randomization in between image
classification and image compression.
Suppose that we have a pretrained image compression
model consisting of an encoder E, a quantizer Q and a de-
coder D. To encode an input image x, we first sample a
transformation τθ from T and transform x to get xt as Equa-
tion (4). Then, we encode xt instead of x as follows:
ˆy = Q(E(xt)).
(5)
The decoding is given by
ˆxt = D(ˆy)
and
ˆx = τ −1
θ
(ˆxt),
(6)
where τ −1
θ
is an inverse transform of τθ. Note that T consists
of (pseudo) invertible transforms for reconstruction and the
additional cost to store the transform index θ, log n bits, is
negligible (about 4×10−4 bpp in our experiments), compared
to the bitstream of an image.
Although such a na¨ıve randomization approach improves
adversarial robustness, the compression performance on nor-
mal images is degraded by some input transforms, which is
further discussed below:
• The cropping operations used in [Xie et al., 2018] are
inappropriate due to incomplete reconstruction given by
missing content.
• The transforms such as rotation, resizing and shifting
have their corresponding inverse transforms, but the in-
versions are imperfect in general because of the informa-
tion loss caused by the transforms, i.e., x ̸= τ −1(xt).
1.0
1.5
2.0
Bits per pixel (BPP)
32
34
36
38
PSNR
Original
Random Shifting
Random Padding
Random Resizing
Random Rotating
Figure 5: Performance degradation of an image compression model
caused by a variety of input transforms.
• The zero padding operations utilized in [Xie et al., 2018]
allow us to recover the original image, but the per-
formance of the models would be degraded since the
paddings lead to out-of-distribution images.
Figure 5 demonstrates the performance degradation of the
image compression model [Minnen et al., 2018] on clean im-
ages when various input transforms are applied. Refer to Ap-
pendix C for details. It is not trivial to maintain the perfor-
mance for these input transforms without additional training.
4.3
Two-way Compression
To defend against adversarial perturbations while preserving
performance on clean images without additional model train-
ing, we propose a straightforward and training-free defense
technique via two-way compression. Our method is appli-
cable to existing compression models without performance
degradation on clean images by effectively leveraging the
random transform. In the framework, we select the better op-
tion out of two compression results of the original image and
the randomly transformed image. We summarize the encod-
ing and decoding process of the proposed approach on Al-
gorithm 1 and Algorithm 2, respectively, where the entropy
coding process is omitted for simplicity.
Our core idea is to choose the best compression strategy
with the lowest loss value out of two different types of com-
pression methods, which is feasible due to the availability of
self-supervision in image compression. The encoding pro-
cess for an input image x is as follows. First, we compute the
rate-distortion loss of x given by encoding followed by de-
coding, without input transform. The encoding and decoding
are expressed as
ˆy1 = Q(E(x))
and
ˆx1 = D(ˆy1),
(7)
respectively.
Then, the rate-distortion loss of input image
without transform is calculated by
L1 = − log2 P(ˆy1) + λd(x, ˆx1),
(8)
where d(·, ·) is a distortion metric and λ is a Lagrangian mul-
tiplier. Next, we compute the rate-distortion loss of x with the
input randomization as described in Section 4.2. The encod-
ing and decoding with the random input transformation are
Algorithm 1 Encoding phase of two-way compression
Require: Pretrained image compression model of encoder E,
decoder D, quantizer Q, and entropy model P.
Require: Distortion metric d(·, ·), Lagrangian multiplier λ,
and Image transform set T = {τ1, ..., τn}.
Input: Source image x.
Output: Compressed latent representation ˆy∗ and transform
index θ∗.
1. Compute the loss for encoding without transform:
Encode: ˆy1 ← Q(E(x)).
Decode: ˆx1 ← D(ˆy1).
Compute loss: L1 ← − log2 P(ˆy1) + λd(x, ˆx1).
2. Compute the loss for encoding with random transform:
Sample τθ ∈ T .
Apply transformation: xt ← τθ(x).
Encode: ˆy2 ← Q(E(xt)).
Decode: ˆxt ← D(ˆy2).
Apply inverse transformation: ˆx2 ← τ −1
θ
(ˆxt).
Compute loss: L2 ← − log2 P(ˆy2) + λd(x, ˆx2).
3. Select the latent representation with the lowest loss:
if L1 < L2 then
ˆy∗ ← ˆy1.
θ∗ ← 0.
else
ˆy∗ ← ˆy2.
θ∗ ← θ.
end if
given by Equation (4) to (6), but we redefine the latent repre-
sentation and reconstructed image as ˆy2 and ˆx2, respectively.
The rate-distortion loss of input image with the random trans-
form is given by
L2 = − log2 P(ˆy2) + λd(x, ˆx2).
(9)
Finally, we determine the optimal compression result ˆy∗ and
use it as the encoding result, which is given by
ˆy∗ =
ˆy1,
if L1 < L2.
ˆy2,
otherwise.
(10)
For reconstruction, we save the transform index θ∗ yielding
the better result. The decoding process is similar to Equa-
tion (6) with an input of ˆy∗.
The proposed two-way compression approach prevents
the compression quality degradation on the original images
while improving the adversarial robustness of the compres-
sion model. The original model performance (L1) is guaran-
teed at least because we select the better option for compres-
sion by the comparison between L1 and L2, This attribute
is especially valuable for normal images. Besides, the risk
of the adversarial attack is mitigated by our input random-
ization scheme. The proposed framework is simple, easy-to-
implement, and even free from additional training. Note that
this strategy is feasible due to the nature of image compres-
sion problem, availability of self-supervision, i.e., the ground-
truth that the model has to reconstruct is identical to the input
image of the encoder.
Algorithm 2 Decoding phase of two-way compression
Require: Pretrained decoder D.
Require: Image transform set T = {τ1, ..., τn}.
Input: Compressed latent representation ˆy∗ and transform
index θ∗.
Output: Reconstructed image ˆx.
Decode: ˆxt ← D(ˆy∗).
if θ∗ = 0 then
ˆx ← ˆxt.
else
Apply the inverse transform: ˆx ← τ −1
θ∗ (ˆxt).
end if
Computational efficiency
Our approach requires more
computation in the encoding phase because it has to per-
form an extra encoding for the transformed image and de-
code two encoded images, for both the clean and transformed
images. However, learned compression algorithms involves
several time-consuming modules other than encoders and de-
coders, such as entropy coders and entropy models. Also,
we can adopt a lightweight encoding algorithm in our en-
coding phase based on masked convolution instead of ex-
pensive serial prediction, which saves computational cost
significantly, especially in high-performance models adopt-
ing autoregressive entropy models [Minnen et al., 2018;
Cheng et al., 2020]. This trick is frequently used for train-
ing models with heavy entropy models [Minnen et al., 2018;
Minnen and Singh, 2020]. Moreover, the costly operation of
decoding the bitstream to ˆy is not needed because ˆy is already
available. The computational cost in the decoding phase is al-
most identical except the overhead of applying inverse trans-
form, which is negligible in practice. We present empirical
results related to computational cost in Section 5.
Scalability
We can generalize the proposed framework to
K-way compression for more gain in robustness. We sample
K − 1 transforms from T and choose the best among the K
compression results including the one with no transform. In
this way, we easily scale-up the robustness of the model with
trade-off between the robustness and encoding cost. How-
ever, we show that K = 2 (i.e., two-way compression) is
practically sufficient in Section 5.
5
Experiments
We now present the experimental results of the proposed de-
fense framework.
5.1
Experimental Setup
The main experiments are conducted on 1000 validation im-
ages of 256 × 256 size randomly sampled from the ImageNet
dataset [Russakovsky et al., 2015]. We use the pretrained
high-bitrate models, Mean & Scale Hyperprior (M&S) [Min-
nen et al., 2018], Mean & Scale Hyperprior with context
model (M&S+C) [Minnen et al., 2018], and Anchor (An-
chor) [Cheng et al., 2020], as in Section 3. For image trans-
form, we use the combinations of all elements in T , which in-
clude (1) horizontal & vertical flipping and rotating in multi-
ples of 90 degrees (8 cases), (2) horizontal & vertical stretch-
0
5
10
15
Bits per pixel (BPP)
34
36
38
PSNR
 
  
 
 
  
 
  
 
Original  +  Clean
Original  +  Vanilla
Two-way + Clean
Two-way + Vanilla
Two-way + EoT
0
5
10
Bits per pixel (BPP)
34
36
38
PSNR
  
 
  
 
  
 
 
 
  
  
  
  
  
Origin
Ori in
a
a
l + Clean
l + Vanilla
g
+ Clean
+ Vanilla
+ EoT
Two-way
Two-way
Two-way
0
1
2
3
Bits per pixel (BPP)
32
33
34
35
PSNR
  
 
  
 
  
 
 
 
  
  
  
  
  
Original + Clean
Original + Vanilla
Two-way + Clean
Two-way + Vanilla
Two-way + EoT
(a) M&S
(b) M&S+C
(c) Anchor
Figure 6: Rate-distortion performance of models without defense method (Original) and models with our defense method (Two-way) on clean
images (Clean) and adversarial examples (Vanilla / EoT). Best viewed in color.
Figure 7: Bitrate histogram of test samples under rate attacks.
ing from 0 to 64 pixels (65 × 65 = 4225 cases), and (3) hori-
zontal & vertical shifting from 0 to 64 pixels (65×65 = 4225
cases). These combinations result in n = |T | ≈ 1.43 × 108
transforms, where we only require less than 30 bits to store
all possible indices.
Attack scenarios
We assume that the model weights are
known to an attacker. Our defense technique is tested in the
following two scenarios depending on whether the attacker is
aware of the existence of the defense method:
• Vanilla attack: The attacker is not aware of the defense
methods in the encoding algorithm, hence assumes input
images are always fed to the model without modification
(i.e., gray-box attack).
• Expectation over Transformation (EoT) attack: The at-
tacker is aware of our two-way compression algorithm
and transforms in T , hence ideally aims to fool all the
input transforms including the identity transform (i.e.,
white-box attack).
For the vanilla attack, we use the PGD algorithm as in Sec-
tion 3 with α = 2/255, ϵ = 4/255, and 50 iterations. The
EoT attack [Athalye et al., 2018] is a strong white-box attack
method for the two-way compression, which is often effective
on the input randomization-based defense techniques [Xie et
al., 2018; Guo et al., 2018] in classification. Specifically, EoT
attack randomly selects 24 target transforms from T and av-
erage the losses of the target transforms at each optimization
step of the PGD algorithm.
Model
Original
Two-way
M&S
0.0219
0.0391
M&S+C
0.7617
0.7952
Anchor
0.7649
0.8437
Table 2: Average encoding time of models in seconds.
5.2
Results
Main results
Figure 6 presents the performance of the pro-
posed defense technique against rate attacks. Overall, the
proposed approach consistently improves the robustness of
the models against the attacks. In comparison to the severe
performance degradation of original models by the attacks
(‘Original + Vanilla’), our method mitigates the adversarial
effects (‘Two-way + Vanilla’ and ‘Two-way + EoT’). Further-
more, the performance of our method on clean images (‘Two-
way + Clean’) is almost identical to the original one (‘Orig-
inal + Clean’). The attacks with multiple targets in EoT are
more effective than the vanilla attack, which is highlighted in
the Anchor model.
Figure 7 visualizes the bitrate distribution of test samples
for the highest bitrate models tested in the experiments for
Figure 6(a). Note that the results of our method exhibit low
bpps by avoiding failure cases with high probability. The his-
togram of rate-distortion loss is provided in Appendix D.
Scalability and na¨ıve input randomization
Figure 8(a)
shows the defense results by varying K in the K-way com-
pression. We used the Kodak dataset [Kodak, 1993] and iter-
atively evaluated performance 40 times for each sample. Us-
ing a larger K further improves the robustness of the model
although the performance gains are saturated; two-way com-
pression is sufficient for defense in practice. Also, we test
the na¨ıve approach, applying the input randomization in im-
age compression as described in Section 4.2, and report the
results denoted by ‘Na¨ıve’ in Figure 8(a). The difference be-
tween the na¨ıve and two-way compression is that the former
always encodes an input image with a random input transform
while sharing T . Our defense framework clearly outperforms
the na¨ıve approach for both clean and perturbed images.
0
1
2
3
Bits per pixel (BPP)
32
34
36
38
PSNR
Original + Clean
10-way + Clean
3-way + Clean
2-way + Clean
Naïve + Clean
10-way + EoT
3-way + EoT
2-way + EoT
 + EoT
Naïve
1
2
3
Bits per pixel (BPP)
34
36
38
PSNR
Original + Clean
  
  
  
  
2-way +
Clean
Advt +
Clean
Advt + Vanilla
2-way + EoT
1
2
3
Bits per pixel (BPP)
32
34
36
38
PSNR
 
Original + Clean
Original + Vanilla
2-way + Clean
2-way + Vanilla
2-way + EoT
(a)
(b)
(c)
Figure 8: Rate-distortion results of M&S models for extensive studies. (a) Results of K-way compression for multiple K values and
direct applicaiton of input randomization on image compression (Na¨ıve). (b) Performance comparison between two-way compression and
adversarial training (Advt). (c) Results of FDA attacks on original models and ones with our defense method.
Clean
Decoded
Perturbed
Without defense
With defense
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 / 0.9770 
0.3262 / 39.3297 / 0.9873
0.4192 / 35.6980 / 0.9770 
6.8520 / 0.9952
2.01
0.8157 / 10.0322 / 0.5519
0.7717 / 4.4495 / 0.9876
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 / 0.9770 
0.3262 / 39.3297 / 0.9873
0.4192 / 35.6980 / 0.9770 
0.6902 / 36.8520 / 0.9952
2.01
0.8157 / 10.0322 / 0.5519
0.7717 / 4.4495 / 0.9876
0.6902 / 36.8520 / 0.9952
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 / 0.9770 
0.3262 / 39.3297 / 0.9873
0.4192 / 35.6980 / 0.9770 
0.6902 / 36.8520 / 0.9952
2
0.8157 / 10.0322 / 0.5519
0.7717 / 4.4495 / 0.9876
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 / 0
0.3262 / 39.329
0.4192 / 35.6980 / 0.9770 
0.6902 / 36.8520 / 0.9952
0.8157 / 10.0322 / 0.5519
0.7717 / 4.4495
0.8157 / 10.0322 / 0.5519
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 / 0.9770 
0.6902 / 36.8520 / 0.9952
0.8157 / 10.0322 / 0.5
0.7717 / 34.4495 / 0.9876
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 / 0.9770 
0.3262 / 39.3297 / 0.9873
6.8520 / 0.9952
0.8157 / 10.0322 / 0.5519
0.7717 / 34.4495 / 0.9876
8.9102 / 0.9938
13.1780 / 35.4537 / 0.9818
0.6820 / 35.6681 / 0.9814
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 / 0.9770 
0.3262 / 39.3297 / 0.9873
0.6902 / 36.8520 / 0.9952
0.8157 / 10.0322 / 0.5519
0.7717 / 34.4495 / 0.9876
0.5973 / 38.9102 / 0.9938
13.1780 / 35.4537 / 0.9818
0.6820 / 35.6681 / 0.9814
0.5973 / 38.9102 / 0.9938
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 / 0.9770 
0.3262 / 39.3297 / 0.9873
0.6902 / 36.8520 / 0.9952
0.8157 / 10.0322 / 0.5519
0.7717 / 34.4495 / 0.9876
0.5973 / 38.9102 / 0.9938
13.1780 / 35.4537 / 0.9818
0.6820 / 35.6681 / 0.9814
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.4192 / 35.6980 / 0
0.3262 / 39.329
0.6902 / 36.8520 / 0.9952
0.8157 / 10.0322 / 0.5519
0.7717 / 34.449
0.5973 / 38.9102 / 0.9938
13.1780 / 35.4537 / 0.9818
0.6820 / 35.6681
13.1780 / 35.4537 / 0.9818
0.3258 / 39.3297 / 0.9873
0.4535 / 10.0249 / 0.5533 
0.6902 / 36.8520 / 0.9952
0.8157 / 10.0322 / 0.5
0.5973 / 38.9102 / 0.9938
13.1780 / 35.4537 / 0.
0.6820 / 35.6681 / 0.9814
Figure 9: Qualitative results of distortion attack (top) and rate attack (bottom). The first and second columns: original images and decoded
results. The third and fourth columns: perturbed images and decoded results without our defense method. The last column: decoded results
for the adversarial examples with our defense method. The yellow annotations denote bits per pixel (bpp)/PSNR (dB)/MS-SSIM.
Comparison with adversarial training
Figure 8(b) com-
pares our defense method with adversarial training typically
used in classification task. We fine-tune the pretrained M&S
models using both the original images and the adversarial ex-
amples generated by FGSM with random initializations, fol-
lowing [Wong et al., 2020]. Our method outperforms the ad-
versarial training in terms of the robustness to the attacks and
the performance on clean images, even without training.
Generalizability
To demonstrate the generalizability of the
proposed defense method, we additionally test a feature-
based attack method, feature disruptive attack (FDA) [Gane-
shan et al., 2019]. For faster evaluation, we randomly sample
100 images from the test set and iteratively measure the per-
formance 10 times for each sample. As shown in Figure 8(c),
our method consistently improves the robustness to FDA.
Encoding time
Table 2 compares the encoding time of the
original models and the models with our two-way compres-
sion technique on a single Titan Xp GPU. The result shows
the efficiency of our defense method. Especially, the increase
of encoding time is marginal for the high performance models
(M&S+C and Anchor) by utilizing masked convolutions for
the loss computation as discussed in Section 4.3. Note that
the extra cost for decoding is truly negligible and not tested.
Qualitative results
Figure 9 qualitatively compares the im-
pact of attacks and our defense methods along with the recon-
structions of clean images. Our defense methods decode the
adversarial images as well as the clean ones, while maintain-
ing a low bitrate that is competitive with the clean images.
6
Conclusion
We investigated the vulnerability of the learned image com-
pression models and designed a simple yet effective defense
method for image compression. We observe that the perfor-
mance of the recent image compression models can be eas-
ily harmed by the basic adversarial attacks in terms of rate
and distortion. The na¨ıve defense approaches for image com-
pression inevitably lead to performance degradation on clean
images. To address this, we present a robust defense frame-
work for image compression that requires no additional train-
ing and preserves the original performance on clean images
by exploiting the input randomization and characteristics of
the self-supervised task. The proposed algorithm computes
the rate-distortion losses of the source image with random
input transformation and identity transform, and chooses the
best option in encoding. The combination of these two opera-
tions turns out to be effective while incurring a small amount
of additional cost in the encoding phase.
Our framework
is free from extensive training and modification of existing
models, and can be easily integrated with various existing
models. This property is particularly desirable for robust im-
age compression algorithms exposed to white-box adversarial
attacks, where any trained models are vulnerable and unreli-
able. We demonstrate the effectiveness of the proposed algo-
rithm in white-box and gray-box attack scenarios and analyze
the characteristics of our approach.
References
[Athalye et al., 2018] Anish Athalye, Logan Engstrom, An-
drew Ilyas, and Kevin Kwok. Synthesizing robust adver-
sarial examples. In International conference on machine
learning, pages 284–293. PMLR, 2018.
[Ball´e et al., 2018] Johannes Ball´e, David Minnen, Saurabh
Singh, Sung Jin Hwang, and Nick Johnston. Variational
image compression with a scale hyperprior. In ICLR, 2018.
[B´egaint et al., 2020] Jean B´egaint, Fabien Racap´e, Simon
Feltman, and Akshay Pushparaja. CompressAI: a PyTorch
library and evaluation platform for end-to-end compres-
sion research. arXiv preprint arXiv:2011.03029, 2020.
[Carlini and Wagner, 2017] Nicholas
Carlini
and
David
Wagner.
Towards evaluating the robustness of neural
networks. In 2017 ieee symposium on security and privacy
(sp), pages 39–57. IEEE, 2017.
[Chen and Ma, 2023] Tong Chen and Zhan Ma. Towards ro-
bust neural image compression: Adversarial attack and
model finetuning. TCSVT, 2023.
[Cheng et al., 2020] Zhengxue Cheng, Heming Sun, Masaru
Takeuchi, and Jiro Katto.
Learned image compression
with discretized gaussian mixture likelihoods and attention
modules. In CVPR, pages 7939–7948, 2020.
[Ganeshan et al., 2019] Aditya Ganeshan, Vivek BS, and
R Venkatesh Babu.
Fda: Feature disruptive attack.
In
ICCV, 2019.
[Goodfellow et al., 2015] Ian
J
Goodfellow,
Jonathon
Shlens, and Christian Szegedy. Explaining and harnessing
adversarial examples. ICLR, 2015.
[Guo et al., 2018] Chuan Guo, Mayank Rana, Moustapha
Cisse, and Laurens Van Der Maaten. Countering adver-
sarial images using input transformations. ICLR, 2018.
[Kannan et al., 2018] Harini Kannan, Alexey Kurakin, and
Ian Goodfellow. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373, 2018.
[Kodak, 1993] Eastman Kodak.
Kodak lossless true color
image suite (PhotoCD PCD0992), 1993.
[Liu et al., 2023] Kang Liu, Di Wu, Yangyu Wu, Yiru Wang,
Dan Feng, Benjamin Tan, and Siddharth Garg. Manipula-
tion attacks on learned image compression. TAI, 2023.
[Madry et al., 2018] Aleksander
Madry,
Aleksandar
Makelov,
Ludwig
Schmidt,
Dimitris
Tsipras,
and
Adrian Vladu. Towards deep learning models resistant to
adversarial attacks. ICLR, 2018.
[Minnen and Singh, 2020] David
Minnen
and
Saurabh
Singh. Channel-wise autoregressive entropy models for
learned image compression. In ICIP, pages 3339–3343.
IEEE, 2020.
[Minnen et al., 2018] David Minnen, Johannes Ball´e, and
George D Toderici.
Joint autoregressive and hierarchi-
cal priors for learned image compression.
NeurIPS,
31:10771–10780, 2018.
[Moosavi-Dezfooli et al., 2016] Seyed-Mohsen
Moosavi-
Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deep-
fool: a simple and accurate method to fool deep neural
networks. In CVPR, pages 2574–2582, 2016.
[Papernot et al., 2016] Nicolas Papernot, Patrick McDaniel,
and Ian Goodfellow. Transferability in machine learning:
from phenomena to black-box attacks using adversarial
samples. arXiv preprint arXiv:1605.07277, 2016.
[Rissanen and Langdon, 1981] Jorma Rissanen and Glen
Langdon. Universal modeling and coding. IEEE Trans.
Inf. Theory, 27(1):12–23, 1981.
[Russakovsky et al., 2015] Olga Russakovsky,
Jia Deng,
Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael
Bernstein, et al. Imagenet large scale visual recognition
challenge. IJCV, 115(3):211–252, 2015.
[Samangouei et al., 2018] Pouya
Samangouei,
Maya
Kabkab, and Rama Chellappa. Defense-gan: Protecting
classifiers against adversarial attacks using generative
models. ICLR, 2018.
[Sui et al., 2023] Yang Sui, Zhuohang Li, Ding Ding, Xiang
Pan, Xiaozhong Xu, Shan Liu, and Zhenzhong Chen. Re-
construction distortion of learned image compression with
imperceptible perturbations. ICMLW, 2023.
[Szegedy et al., 2014] Christian
Szegedy,
Wojciech
Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus.
Intriguing properties of
neural networks. ICLR, 2014.
[Tram`er et al., 2018] Florian Tram`er, Alexey Kurakin, Nico-
las Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and de-
fenses. ICLR, 2018.
[Wong et al., 2020] Eric Wong, Leslie Rice, and J Zico
Kolter. Fast is better than free: Revisiting adversarial train-
ing. arXiv preprint arXiv:2001.03994, 2020.
[Xie et al., 2018] Cihang Xie, Jianyu Wang, Zhishuai Zhang,
Zhou Ren, and Alan Yuille. Mitigating adversarial effects
through randomization. In ICLR, 2018.
[Xu et al., 2018] Weilin Xu, David Evans, and Yanjun Qi.
Feature squeezing: Detecting adversarial examples in deep
neural networks. NDSS, 2018.
[Yu et al., 2023] Yi Yu, Yufei Wang, Wenhan Yang, Shijian
Lu, Yap-Peng Tan, and Alex C Kot.
Backdoor attacks
against deep image compression via adaptive frequency
trigger. In CVPR, 2023.
Appendix
A
Impact of Model Complexity to Robustness
To investigate the robustness of image compression models
depending on the model complexity, we trained a lightweight
variant of high-bitrate M&S model, by halving its channel
size. Figure 10 compares the results of the original model
(18M parameters) and the lightweight model (7M parame-
ters) under rate attacks. While the model with higher capac-
ity achieves slightly better performance on clean images, it
suffers from significant failures on perturbed images. This
implies that the model with higher capacity is more suscepti-
ble to adversarial attacks and rather overfitted.
B
Results of Distortion Attacks
Figure 11 presents the result of distortion attack on M&S
model with ϵ = 4/255 for PGD algorithm. The attacks for
poor reconstruction quality successfully degraded the model
performance.
C
Details of Input Transforms
This section explains the details of the image transforms used
in the experiments for Figure 5 of the main paper. The exam-
ples of the transformed images are illustrated in Figure 12.
For the image transforms, we use the operaitons including
(1) horizontal and vertical shifting from 0 to 64 pixels, (2)
horizontal and vertical zero-padding from 0 to 32 pixels, (3)
horizontal & vertical stretching from 0 to 64 pixels, and (4)
rotating from -10 to 10 degrees.
D
Loss Histogram Under Attacks
Figure 13 visualizes the rate-distortion loss value distribution
of test samples for the highest bitrate models tested in the
experiments for Figure 6(a) of the main paper. Note that the
results of our method exhibit low losses by avoiding extreme
failure cases with high probability.
E
Comparison to Hand-crafted Codecs
Figure 14 compares the compression performance between
the attacked models and hand-crafted codecs. We observe
the severe performance degradation of the attacked models,
which is even worse than the hand-crafted codecs.
Figure 10:
Rate-distortion results of original model and its
lightweight verison with the half channel size.
Figure 11: Rate-distortion result of distortion attacks.
Original
Shifting
Padding
Rotating
Resizing
Figure 12: Examples of image transforms used in the experiments.
Figure 13: Rate-distortion loss histogram for test samples under rate attacks.
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
Bits per pixel (BPP)
30.0
32.5
35.0
37.5
PSNR
Clean-Anchor
Clean-M&S+C
Clean-M&S
Clean-BPG (4:4:4)
Clean-WebP
Clean-JPEG (4:2:0)
PGD-Anchor
PGD-M&S+C
PGD-M&S
Figure 14: Rate-distortion results of attacked learned image compression models and traditional codecs.
"
"Guessing random additive noise decoding (GRAND) has gained popularity in recent years due to its efficient utilization of soft information. Ordered reliability bits GRAND (ORBGRAND), a variant of GRAND, is particularly appealing because it allows for offline error pattern generation and is conducive to hardware implementation. Existing studies demonstrate ORBGRAND's near-capacity-achieving capability for antipodal inputs in additive white Gaussian noise channels. This work aims to extend the achievable rate analysis to memoryless binary-input bit channels with general output conditional probability distributions to gauge ORBGRAND's performance for high-order coded modulation schemes like bit-interleaved coded modulation (BICM).

An information theoretic analysis is conducted to study ORBGRAND's performance limit for general memoryless binary-input bit channels. The motivation is that achievable rates for general bit channels serve as basic building blocks for assessing ORBGRAND's performance for channels with high-order coded modulation schemes. Generalized mutual information (GMI) is employed to quantify ORBGRAND's achievable rate, as it is a mismatched decoding method. A comparison between the derived GMI and the channel mutual information is also presented, providing insights into the gap between them and explaining when and why the GMI of ORBGRAND may approach the channel mutual information, as often observed in numerical studies. As an application, the ORBGRAND achievable rate of BICM is investigated for QPSK, 8PSK, and 16QAM with Gray and set-partitioning labelings over AWGN and Rayleigh fading channels. Numerical results indicate a small gap between the ORBGRAND achievable rate and the channel mutual information, suggesting ORBGRAND's feasibility for channels with high-order coded modulation schemes.","Guessing random additive noise decoding (GRAND) has drawn considerable attention recently, particularly its variant called ordered reliability bits GRAND (ORBGRAND) which offers efficient utilization of soft information and amenable to hardware implementation. ORBGRAND has been demonstrated to achieve information rate close to channel mutual information in additive white Gaussian noise channels under antipodal input. This paper extends the analysis of ORBGRAND to general memoryless binary-input bit channels with general output conditional probability distributions.

The analysis will serve as a foundation to evaluate ORBGRAND's performance for high-order coded modulation schemes like bit-interleaved coded modulation (BICM) which utilizes achievable rates of general bit channels as building blocks. As ORBGRAND is a mismatched decoding method, generalized mutual information (GMI) is employed to quantify its achievable rate. The GMI of ORBGRAND is derived, and the gap between GMI and channel mutual information is examined to understand when and why the GMI approaches the channel mutual information. The analysis is applied to BICM schemes for QPSK, 8PSK, and 16QAM with Gray and set-partitioning labelings over AWGN and Rayleigh fading channels to demonstrate the feasibility of ORBGRAND for high-order coded modulation schemes.","nanGRAND has attracted attention due to its universal decoding paradigm, where random error patterns are generated and tested until a valid codeword is found. ORBGRAND, a variant of GRAND, does not require exact channel output values but instead utilizes the ranked relationship among channel outputs to generate error patterns. This enables offline error pattern generation and facilitates hardware implementation. Previous work has established ORBGRAND's near-capacity-achieving capability in AWGN channels with antipodal inputs.

In order to assess ORBGRAND's performance for channels with high-order coded modulation schemes, analysis of achievable rates for general bit channels is needed, which then serve as building blocks for the assessment. Generalized mutual information quantifies achievable rate for mismatched decoding methods like ORBGRAND. Moreover, comparison between the GMI of ORBGRAND and the channel mutual information provides insights into the gap between them and explains why the GMI often approaches the channel mutual information in numerical studies.nannan","The methodology employed in this paper includes:

* System Model and ORBGRAND for General Bit Channels:
    * Defines the system model for general memoryless binary-input bit channels with general output probability distributions.
    * Introduces the ORBGRAND decoding algorithm for general bit channels and describes its error pattern generation process.

* GMI Analysis of ORBGRAND:
    * Presents Theorem 1, which characterizes the GMI of ORBGRAND for general memoryless bit channels.
    * Provides proofs for three Lemmas used in the GMI derivation.
    * Discusses the gap between the GMI of ORBGRAND and the channel mutual information.

* Application in BICM:
    * Describes the experimental setup, including the BICM scheme, transmission model, and modulation labeling.
    * Presents numerical results for ORBGRAND achievable rate and channel mutual information under AWGN and Rayleigh fading channels for QPSK, 8PSK, and 16QAM.nan","The key results obtained in the paper are:

* Achievable Rate Analysis:
    * Theorem 1 establishes an expression for the GMI of ORBGRAND for general memoryless bit channels.
    * The GMI is formulated as a function of the channel parameters and the decoding algorithm.

* ORBGRAND Gap Analysis:
    * Proposition 1 shows the equivalence between the GMI of SGRAND (which is the maximum-likelihood decoding and achieves channel mutual information) and the channel mutual information.
    * The gap between the ORBGRAND achievable rate and the channel mutual information is attributed to the difference between the cumulative distribution function of the magnitude of the channel LLR and the uniform distribution.

* BICM Performance Analysis:
    * Numerical results demonstrate that for BICM with QPSK, 8PSK, and 16QAM over AWGN and Rayleigh fading channels, the gap between the ORBGRAND achievable rate and the channel mutual information is small, suggesting the feasibility of ORBGRAND for channels with high-order coded modulation schemes.","This paper conducts an achievable rate analysis of ORBGRAND for memoryless binary-input channels with general output conditional probability distributions. The analysis sheds light on ORBGRAND's performance limit and provides insights into why the GMI of ORBGRAND is close to the channel mutual information, a phenomenon observed in several channels of practical interest. Additionally, the study paves the way for analyzing ORBGRAND's performance for high-order coded modulation schemes, such as BICM. Numerical results for BICM indicate near-optimal performance of ORBGRAND, suggesting its suitability for high-rate transmission systems employing higher-order modulations.",ORBGRAND: Achievable Rate for General Bit Channels and Application in BICM,"Zhuang Li, Wenyi Zhang","ORBGRAND: Achievable Rate for General Bit
Channels and Application in BICM
Zhuang Li and Wenyi Zhang
Department of Electronic Engineering and Information Science
University of Science and Technology of China
Hefei, China
Email: wenyizha@ustc.edu.cn
Abstract—Guessing random additive noise decoding (GRAND)
has received widespread attention recently, and among its vari-
ants, ordered reliability bits GRAND (ORBGRAND) is particu-
larly attractive due to its efficient utilization of soft information
and its amenability to hardware implementation. It has been
recently shown that ORBGRAND is almost capacity-achieving in
additive white Gaussian noise channels under antipodal input. In
this work, we first extend the analysis of ORBGRAND achievable
rate to memoryless binary-input bit channels with general output
conditional probability distributions. The analytical result also
sheds insight into understanding the gap between the ORB-
GRAND achievable rate and the channel mutual information. As
an application of the analysis, we study the ORBGRAND achiev-
able rate of bit-interleaved coded modulation (BICM). Numerical
results indicate that for BICM, the gap between the ORBGRAND
achievable rate and the channel mutual information is typically
small, and hence suggest the feasibility of ORBGRAND for
channels with high-order coded modulation schemes.
Index Terms—Bit channel, bit-interleaved coded modulation,
generalized mutual information, guessing random additive noise
decoding.
I. INTRODUCTION
Guessing random additive noise decoding (GRAND) [1] [2]
has been recently proposed as a universal decoding paradigm.
Its basic idea is to sort and test a sequence of possible error
patterns until finding a valid codeword. If all error patterns are
sorted from the most likely to the least likely, then GRAND
is, in fact, equivalent to maximum-likelihood (ML) decoding.
Utilizing soft symbol reliability information can improve
decoding performance [3, Ch. 10]. Symbol reliability GRAND
(SRGRAND) [4] uses one bit of soft information to specify
whether a channel output symbol is reliable. Soft GRAND
(SGRAND) [5] uses magnitudes of channel output to generate
error patterns, so it can make full usage of channel soft
information and it in fact implements the maximum-likelihood
decoding. But SGRAND requires a sequential online algorithm
to generate the sequence of error patterns, and this limits
its effciency of hardware implementation. Another variant
of GRAND, called ordered reliability bits GRAND (ORB-
GRAND) [6], does not require exact values of channel output,
and instead uses only the rank relationship among channel
This work was supported in part by the National Natural Science Foundation
of China under Grant 62231022.
outputs to generate error patterns. This property enables OR-
BGRAND to generate error patterns offline and facilitates
hardware implementation [7] [8] [9]. Noteworthily, it has
been shown via an information theoretic analysis in [10] that,
ORBGRAND achieves an information rate almost approaching
the channel mutual information in additive white Gaussian
noise (AWGN) channels under antipodal input.
Several variants of GRAND have been proposed for fading
channel. In [11], ORBGRAND based on reduced-complexity
pseudo-soft information has been studied. In [12], fading-
GRAND has been proposed for Rayleigh fading channels,
shown to outperform traditional hard-decision decoders, and
in [13], a hardware architecture of fading-GRAND has been
studied. In [14], symbol-level GRAND has been proposed for
block fading channels, utilizing the knowledge of modulation
scheme and channel state information (CSI). In [15] [16],
GRAND has been applied to multiple-input-multiple-output
channels.
In this paper, along the line of [10], we conduct an in-
formation theoretic analysis to study the performance limit
of ORBGRAND, for general memoryless binary-input bit
channels. The motivation is that, achievable rates for general
bit channels serve as basic building blocks for assessing
the performance of ORBGRAND for channels with high-
order coded modulation schemes, such as bit-interleaved coded
modulation (BICM) [17] [18], a technique extensively used
in fading channels. Since ORBGRAND is a mismatched
decoding method, rather than the maximum-likelihood one, we
utilize generalized mutual information (GMI) to quantify its
achievable rate [19] [20]. A comparison between the derived
GMI of ORBGRAND and the channel mutual information
also sheds insight into understanding the gap between them,
helping explain when and why the GMI of ORBGRAND is
close to the channel mutual information, as usually observed
in numerical studies. As an application of the analysis, we
study the ORBGRAND achievable rate of BICM. Numerical
results for QPSK, 8PSK, and 16QAM with Gray and set-
partitioning labelings over AWGN and Rayleigh fading chan-
nels are presented. These results indicate that for BICM, the
gap between the ORBGRAND achievable rate and the channel
mutual information is typically small, and hence suggest the
feasibility of ORBGRAND for channels with high-order coded
arXiv:2401.11901v1  [cs.IT]  22 Jan 2024
modulation schemes.
The remaining part of this paper is organized as follows:
Section II introduces the system model and ORBGRAND for
general memoryless binary-input bit channels. Section III de-
rives the GMI of ORBGRAND and discusses the gap between
it and the channel mutual information. Section IV presents
the corresponding numerical results for BICM. Section V
concludes this paper.
II. SYSTEM MODEL AND ORBGRAND FOR GENERAL
BIT CHANNELS
A. System Model
In this paper, we study memoryless binary-input channels
with general output conditional probability distributions. With-
out loss of generality, let the input alphabet be {+1, −1}, and
let the output probability distribution be q+(y) under input
x = +1 and q−(y) under input x = −1, respectively. Note
that q+(y) and q−(y) are general and we do not require them
to possess any symmetric property.
We consider a codebook with code length N and code
rate R nats per channel use, so the number of messages is
M = ⌈eNR⌉. When sending message m, the transmitted
codeword is x(m) = [x1(m), x2(m), · · · , xN(m)]. We as-
sume that the elements of x(m) are indepedent and identically
distributed (i.i.d.) uniform {+1, −1} random variables. This
is a common assumption in random coding analysis, and is
satisfied for many linear codes. The channel output vector is
denoted as Y = [Y1, Y2, · · · , YN]. Define the log-likelihood
ratio (LLR) random variable Tn = ln q+(Yn)
q−(Yn) and the reliability
vector [|T1|, |T2|, · · · , |TN|]. For n = 1, 2, · · · , N, denote
Rn as the rank of |Tn| among the sorted array consisting
of {|T1|, |T2|, · · · , |TN|}, from 1 (the smallest) to N (the
largest). The cumulative distribution function (cdf) of |T| is
denoted as Ψ(t), for t ≥ 0. We use lowercase letters to denote
the realizations of random variables; for example, rn as the
realization of Rn, and so on.
B. ORBGRAND for General Bit Channels
For
a
general
bit
channel,
the
procedure
of
ORB-
GRAND can be described as follows: when a channel out-
put vector y is received, we calculate its reliability vec-
tor [|t1|, |t2|, · · · , |tN|] and its hard-decision vector xhard =
[sgn(t1), sgn(t2), · · · , sgn(tN)], where ti = ln q+(yi)
q−(yi) is the
LLR and sgn(t) = 1 if t ≥ 0 and −1 otherwise. We generate
an error pattern matrix P of size 2N × N. The elements of P
are +1 or −1, and the rows of P are all distinct: if the q-th
row n-th column element Pq,n is −1, then in the q-th query,
we flip the sign of the n-th element of xhard. So each row of P
represents a different query and we conduct the queries from
top to bottom, until finding a valid codeword and declaring it
as the decoded codeword, or exhausting all the rows without
finding a valid codeword and declaring a decoding failure. We
arrange the rows of P so that the sum reliability of the q-th
row defined as P
n:Pq,n=−1 rn is non-decreasing with q, where
rn is the rank of |tn| introduced in the previous subsection.
There exist efficient algorithms for generating the matrix P;
see, e.g., [6] [7]. Since 2N is typically an exceedingly large
quantity, in practice we can truncate the matrix to keep only
its first Q rows, where Q is the maximum number of queries
permitted.
As shown in [10], the following form of decoding criterion
provides a unified description of GRAND and its variants
including ORBGRAND, if Q is set to its maximum possible
value 2N: for a received channel output vector y, the decoder
decides the message to be
ˆm =
arg min
m=1,2,··· ,M
1
N
N
X
n=1
γn(y) · 1

sgn

lnq+(yn)
q−(yn)

· xn(m)<0

.
(1)
It can be shown (for details see, e.g., [10, Sec. II]) that
the decoding criterion (1) produces the same decoding result
as, and is hence equivalent to, GRAND and its variants,
if we are permitted to conduct an exhaustive query of all
possible error patterns, i.e., Q = 2N. 1 Different choices of
{γn}n=1,2,··· ,N correspond to different sorting criteria of error
patterns: if γn(y) = 1, (1) is the original GRAND [1]; if
γn(y) =
ln q+(yn)
q−(yn)
, (1) is SGRAND [5], which is equivalent
to the maximum-likelihood decoding; if γn(y) = rn
N , where
rn is the realization of the rank random variable Rn, (1) is
ORBGRAND [6].
III. GMI ANALYSIS OF ORBGRAND
A. GMI of ORBGRAND
Since ORBGRAND is not the maximum likelihood decod-
ing, we resort to mismatched decoding analysis (see, e.g., [19]
[20]) for characterizing its information theoretic performance
limit. For this purpose, GMI is a convenient tool and has been
widely used. GMI quantifies the maximum rate such that the
ensemble average probability of decoding error asymptotically
vanishes as the code length grows without bound. For general
memoryless bit channels, the GMI of ORBGRAND is char-
acterized by the following theorem.
Theorem 1: For the system setup in Section II, the GMI of
ORBGRAND is given by
IORBGRAND = ln2 − inf
θ<0
( Z 1
0
ln(1 + eθt)dt
− θ · 1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy
− θ · 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy
)
(2)
in nats/channel use.
Proof: Since in ORBGRAND, the terms inside the sum-
mation in (1) are correlated due to the ranking relationship,
1As mentioned in the previous paragraph, in practice Q is usually set as a
number smaller than 2N, but the form of (1) renders an information theoretic
analysis amenable, as will be seen in the next section.
we cannot directly invoke the standard formula of GMI (see,
e.g., [19, Eqn. (12)]) to evaluate the GMI of ORBGRAND.
Instead, we conduct analysis and calculation from the first
principle, similar to [10] which considers the special case
of AWGN channels only. We calculate the ensemble average
probability of decoding error. As a consequence of i.i.d.
random coding, the average probability of decoding error is
equal to the probability of decoding error under the condition
of transmitting message m = 1.
Based on the general decoding rule (1), we define the
decoding metric of ORBGRAND by
D(m) = 1
N
N
X
n=1
Rn
N · 1 (sgn (Tn) · Xn(m)<0) ,
m = 1, 2, · · · , M.
(3)
Under i.i.d random coding, {|Tn|}n=1,2,··· ,N are also i.i.d.
with cdf Ψ(t), for t ≥ 0.
When the transmitted message is m = 1, we can charac-
terize the asymptotic behavior of the decoding metric in (3)
using the following three lemmas, whose proofs are placed in
Appendix A.
Lemma 1: As N → ∞, for the transmitted codeword, the
expectation of the decoding metric in (3) is given by
lim
N→+∞ ED(1) = 1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy
+ 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy.
(4)
Lemma 2: As N → ∞, for the transmitted codeword, the
variance of the decoding metric in (3) is given by
lim
N→+∞ varD(1) = 0.
(5)
Lemma 3: As N → ∞, for any codeword not transmitted,
i.e., m′ ̸= 1 and any θ<0, the decoding metric in (3) satisfies,
almost surely,
∆(θ) :=
lim
N→+∞
1
N lnE
n
eNθD(m′)T
o
=
Z 1
0
ln(1 + eθt)dt − ln2.
(6)
For any ϵ>0, we define event
Uϵ =
(
D(1) ≥ 1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy
+ 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy + ϵ
)
,
so the ensemble average probability of decoding error is
Pr[ ˆm ̸= 1]
=
Pr[ ˆm ̸= 1|Uϵ]Pr[Uϵ] + Pr[ ˆm ̸= 1|Uc
ϵ ]Pr[Uc
ϵ ]
≤
Pr[Uϵ] + Pr[ ˆm ̸= 1|Uc
ϵ ].
(7)
Using Lemma 1, Lemma 2 and Chebyshev’s inequality, we
can deduce that for any ϵ>0,
lim
N→+∞Pr
""
D(1) ≥ 1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy
+ 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy + ϵ
#
= 0;
(8)
this shows that Pr[Uϵ] can be arbitrarily close to zero as the
code length N grows without bound.
Meanwhile, based on the decoding rule (1) and the union
bound, we have
Pr[ ˆm ̸= 1|Uc
ϵ ]
≤
Pr
""
∃m′ ̸= 1, D(m′)<1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy
+ 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy + ϵ
#
≤
eNRPr
""
D(m′)<1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy
+ 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy + ϵ
#
.
(9)
Considering the conditional version of the probability in (9),
and applying Chernoff’s bound, we have that for any N and
any θ<0,
− 1
N lnPr
""
D(m′)<1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy
+ 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy + ϵ
T
#
≥
θ
""
1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy + 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy + ϵ
#
− 1
N lnE
n
eNθD(m′)T
o
.
(10)
Letting ϵ → 0, the code length N → +∞, and applying the
almost surely limit in Lemma 3, we have
Pr
""
D(m′)<1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy
+ 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy + ϵ
T
#
≤
exp
(
− N
""
θ · 1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy
+ θ · 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy
−
Z 1
0
ln(1 + eθt)dt + ln2
#)
.
(11)
Substituting (11) into (9), and applying the law of total
expectation to remove the conditioning, we assert that the
ensemble average probability of decoding error asymptotically
vanishes as the code length N grows without bound if the
code rate R satisfies the following inequality, i.e., (2) in the
statement of Theorem 1.
R
<
sup
θ<0
(
θ · 1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy
+ θ · 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy
−
Z 1
0
ln(1 + eθt)dt + ln2
)
=
ln2 − inf
θ<0
( Z 1
0
ln(1 + eθt)dt
− θ · 1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy
− θ · 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy
)
.
(12)
B. Discussion on Gap between GMI of ORBGRAND and
Channel Mutual Information
For general memoryless bit channels under uniform binary
input, as described in Section II-A, the channel mutual infor-
mation is given by
I = I(X; Y)
= ln2 − 1
2
Z +∞
−∞
ln

1 + q−(y)
q+(y)

q+(y)dy
− 1
2
Z +∞
−∞
ln

1 + q+(y)
q−(y)

q−(y)dy
(13)
in nats/channel use.
In order to analyze the gap between (2) and (13), we rewrite
(13) into its equivalent form as the GMI of SGRAND, notic-
ing that SGRAND is equivalent to the maximum-likelihood
decoding and thus achieves the channel mutual information.
This leads to the following result.
Proposition 1: For the general memoryless bit channel
model in Section II, the GMI of SGRAND is given by
ISGRAND = ln2 − inf
θ<0
(
E

ln

1 + e
θ
ln
q+(Y)
q−(Y)


− θ · 1
2
Z
q+(y)<q−(y)
lnq+(y)
q−(y)
 q+(y)dy
− θ · 1
2
Z
q+(y)>q−(y)
lnq+(y)
q−(y)
 q−(y)dy
)
(14)
in nats/channel use, and this is equal to the channel mutual
information I in (13).
Proof: See Appendix B.
It is interesting to note that, if
ln q+(Y)
q−(Y)
 and
ln q+(y)
q−(y)

in (14) are replaced by Ψ
ln q+(Y)
q−(Y)


and Ψ
ln q+(y)
q−(y)


respectively, then (14) will become (2). This is obtained by
noting that Ψ
ln q+(Y)
q−(Y)


obeys the uniform distribution over
[0, 1] and hence
E

ln

1 + e
θΨ
ln
q+(Y)
q−(Y)


=
Z 1
0
ln(1 + eθt)dt.
(15)
Therefore, the gap between IORBGRAND and I is essentially
caused by the difference between t and Ψ(t). If Ψ(t) behaves
close to a linear function, then the GMI of ORBGRAND will
be close to the channel mutual information.
Here we give a simple example to illustrate the above
discussion. We consider BPSK modulation over the Rayleigh
fading channel with perfect CSI and the AWGN channel.
The curves of Ψ(t) and IORBGRAND under different values of
signal-to-noise ratio (SNR) are displayed in Fig. 1 and Fig. 2,
respectively. Taking SNR = 3dB as an example, we can see
from Fig. 1 that the linearity of Ψ(t) in the Rayleigh fading
channel is obviously worse than that in the AWGN channel.
Therefore, we can see from Fig. 2 that there is a noticeable
gap between IORBGRAND and I in the Rayleigh fading channel,
while there is virtually no gap in the AWGN channel.
0
2
4
6
8
10
12
14
16
18
20
t
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
(t)
SNR = 1dB(AWGN)
SNR = 3dB(AWGN)
SNR = 5dB(AWGN)
SNR = 1dB(Rayleigh)
SNR = 3dB(Rayleigh)
SNR = 5dB(Rayleigh)
Fig. 1. Plots of Ψ(t) under under AWGN and Rayleigh fading channels.
1
1.5
2
2.5
3
3.5
4
4.5
5
SNR(dB)
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
Rate(bits/channel use)
Rate of ORBGRAND(AWGN)
Channel mutual information(AWGN)
Rate of ORBGRAND(Rayleigh)
Channel mutual information(Rayleigh)
Fig. 2.
Plots of IORBGRAND and I under AWGN and Rayleigh fading
channels.
IV. APPLICATION IN BICM
BICM is an effective coded modulation scheme and has
been widely used in contemporary communication systems.
In this section, we use the analytical results in the previ-
ous section to calculate the ORBGRAND achievable rate of
BICM, and compare it with the channel mutual information.
This study serves as a theoretical basis for the feasibility of
ORBGRAND for channels with high-order coded modulation
schemes.
A. Experimental Setup
In our experiment, we consider QPSK, 8PSK and 16QAM
with ideal interleaving and perfect CSI. For each modulation
type, we consider both Gray and set-partitioning labelings.
For example, the constellation diagrams of the two labelings
for 16QAM are shown in Fig. 3. The channel input-output
relationship is
Y = HS + Z.
(16)
H is the channel gain: when H = 1, (16) is the AWGN chan-
nel, and when H obeys a unit-variance circularly symmetric
complex Gaussian distribution, (16) is the Rayleigh fading
channel; S is the channel input, corresponding to a point in the
constellation diagram; Z is the standard circularly symmetric
complex Gaussian noise. In BICM, the codeword X is first
passed to an interleaver π, and the interleaved sequence π(X)
is then divided into multiple subsequences, each of length
matched to the order of the constellation, and is thus mapped
to a point in the constellation diagram according to a certain
labeling rule; for details, see, e.g., [17] [18].
I
Q
1110
1010
0010
0110
1111
1011
0011
0111
1101
1001
0001
0101
1100
1000
0000
0100
I
Q
1001
1100
1101
1000
1110
1011
1010
1111
0101
0000
0001
0100
0010
0111
0110
0011
Fig. 3. Gray (left) and set-partitioning (right) labelings for 16QAM.
Due to the nature of ideal interleaving, we can adopt the
concept of parallel channel model in [17], as shown in Fig. 4.
The ORBGRAND achievable rate of the i-th parallel channel
is denoted as Ii
ORBGRAND, so IORBGRAND =
m
P
i=1
Ii
ORBGRAND,
where m is the number of parallel channels. For the i-th
parallel channel, the conditional probability distribution of y
is given by
qi
+(y) =
P
s∈X i
1
p(y|s)
|X i
1|
,
qi
−(y) =
P
s∈X i
0
p(y|s)
|X i
0|
,
(17)
where X i
1 is the set of s whose i-th bit is 1, and X i
0 is the set
of s whose i-th bit is 0. 2 Plugging (16) and (17) into (2), we
can calculate Ii
ORBGRAND, and thus get IORBGRAND.
Encoder
Channel
1 
Channel
i
Channel
m 
Fig. 4. Parallel channel model of BICM with ideal interleaving.
B. Numerical Results
In general, the ORBGRAND achievable rate and the channel
mutual information in BICM do not yield closed-form expres-
sions, so we use numerical methods such as Monte Carlo to
evaluate them.
-10
-5
0
5
10
15
20
SNR(dB)
0
0.5
1
1.5
2
2.5
3
3.5
4
Rate(bits/channel use)
Rate of ORBGRAND(QPSK)
Channel mutual information(QPSK)
Rate of ORBGRAND(8PSK)
Channel mutual information(8PSK)
Rate of ORBGRAND(16QAM)
Channel mutual information(16QAM)
Fig. 5. ORBGRAND achievable rate and channel mutual information under
QPSK, 8PSK and 16QAM for AWGN channel in the case of Gray labeling.
-10
-5
0
5
10
15
20
SNR(dB)
0
0.5
1
1.5
2
2.5
3
3.5
4
Rate(bits/channel use)
Rate of ORBGRAND(QPSK)
Channel mutual information(QPSK)
Rate of ORBGRAND(8PSK)
Channel mutual information(8PSK)
Rate of ORBGRAND(16QAM)
Channel mutual information(16QAM)
Fig. 6. ORBGRAND achievable rate and channel mutual information under
QPSK, 8PSK and 16QAM for AWGN channel in the case of set-partitioning
labeling.
2Here, 1 (resp. 0) corresponds to +1 (resp. −1) in our bit channel model
in Sections II and III.
The numerical results for the AWGN channel are shown
in Fig. 5 and Fig. 6, which show that although ORBGRAND
is a mismatched decoder, the ORBGRAND achievable rate
under QPSK, 8PSK and 16QAM over the AWGN channel is
very close to the channel mutual information, regardless of the
labeling. The numerical results for the Rayleigh fading channel
are shown in Fig. 7 and Fig. 8, which exhibit essentially the
same trend as that in the AWGN channel, with a slightly larger
gap between the ORBGRAND achievable rate and the channel
mutual information in the low SNR regime. As discussed
in Section III-B, the gap is due to the nonlinearity of the
cdf of the magnitude of the channel LLR. These numerical
results suggest that ORBGRAND can still maintain good
decoding performance for channels adopting high-order coded
modulation schemes.
-10
-5
0
5
10
15
20
25
30
SNR(dB)
0
0.5
1
1.5
2
2.5
3
3.5
4
Rate(bits/channel use)
Rate of ORBGRAND(QPSK)
Channel mutual information(QPSK)
Rate of ORBGRAND(8PSK)
Channel mutual information(8PSK)
Rate of ORBGRAND(16QAM)
Channel mutual information(16QAM)
Fig. 7. ORBGRAND achievable rate and channel mutual information under
QPSK, 8PSK and 16QAM for Rayleigh fading channel with perfect CSI in
the case of Gray labeling.
-10
-5
0
5
10
15
20
25
30
SNR(dB)
0
0.5
1
1.5
2
2.5
3
3.5
4
Rate(bits/channel use)
Rate of ORBGRAND(QPSK)
Channel mutual information(QPSK)
Rate of ORBGRAND(8PSK)
Channel mutual information(8PSK)
Rate of ORBGRAND(16QAM)
Channel mutual information(16QAM)
Fig. 8. ORBGRAND achievable rate and channel mutual information under
QPSK, 8PSK and 16QAM for Rayleigh fading channel with perfect CSI in
the case of set-partitioning labeling.
V. CONCLUSION
In this paper, we conduct an achievable rate analysis of OR-
BGRAND for memoryless binary-input channels with general
output conditional probability distributions. The achievable
rate is characterized by the GMI of ORBGRAND, and its
analysis sheds insight into why and when the GMI is close
to the channel mutual information, a phenomenon usually ob-
served in several representative channels of practical interest.
This analysis further paves the way towards analyzing the
performance of ORBGRAND for high-order coded modula-
tion schemes, such as BICM. Numerical results for BICM
indicate the near-optimal performance of ORBGRAND, and
thus suggest its feasibility for high-rate transmission systems,
where high-order modulations are necessary.
APPENDIX A
PROOF OF LEMMAS 1-3
A. Proof of Lemma 1
We have
ED(1) =
1
N 2
N
X
n=1
E[Rn1(sgn(Tn) · Xn(1)<0)].
(18)
For each term in (18), we have
E[Rn1(sgn(Tn) · Xn(1)<0)]
=
1
2E
h
Rn1(Tn<0)
Xn(1) = +1
i
+ 1
2E
h
Rn1(Tn>0)
Xn(1) = −1
i
.
(19)
For the first expectation in (19), based on the law of total
expectation, we have
E[Rn1(Tn<0)|Xn(1) = +1]
=
E
h
E
h
Rn1(q+(Yn)<q−(Yn))
Xn(1) = +1, Yn
ii
.
(20)
Next we have
E
h
Rn1(q+(Yn)<q−(Yn))
Xn(1) = +1, Yn = y
i
=
(
E
h
Rn
Xn(1) = +1, Yn = y
i
if
q+(y)<q−(y),
0
else.
(21)
Based on the definition of Rn and the i.i.d. nature of
{|Tn|}n=1,2,··· ,N, we notice that the expectation in the first
branch of (21) is exactly the expectaion of the rank when
inserting
ln q+(y)
q−(y)
 into a sorted array of N − 1 samples of
{|T|}. For simplicity, denoting the expectation in the first
branch of (20) as u(y), so
E[Rn1(Tn<0)|Xn(1) = +1] =
Z
q+(y)<q−(y)
u(y)q+(y)dy.
(22)
The second expectation in (19) can be treated in the same
approach. Therefore, we have
ED(1) = 1
2
Z
q+(y)<q−(y)
u(y)
N q+(y)dy
+ 1
2
Z
q+(y)>q−(y)
u(y)
N q−(y)dy.
(23)
Utilizing the asymptotic behavior of binomial distribution
(for details see [10, Appendix F]), we obtain
lim
N→+∞ ED(1) = 1
2
Z
q+(y)<q−(y)
Ψ
lnq+(y)
q−(y)


q+(y)dy
+ 1
2
Z
q+(y)>q−(y)
Ψ
lnq+(y)
q−(y)


q−(y)dy.
(24)
B. Proof of Lemma 2
Defining Wn = Rn
N 1(sgn(Tn)·Xn(1)<0) and ˜Wn = Wn −
EWn, we have
varD(1) =
1
N 2
N
X
i=1
N
X
j=1
E[ ˜Wi ˜Wj].
(25)
We disassemble (25) into two situations: j = i and j ̸= i,
and then separately treat them following similar techniques
as in [10, Appendix C]. The analysis reveals that varD(1)
asymptotically vanishes as N → ∞.
C. Proof of Lemma 3
Since T is induced by X(1), it is independent of X(m′), and
we have
E
n
eNθD(m′)T
o
=
N
Y
n=1
E
n
eθ Rn
N 1(sgn(Tn)·Xn(m′)<0)T
o
.
(26)
We can use similar approach as in [10, Appendix D],
exploiting the fact that Rn is determinisitc once T is given, to
obtain
E
n
eθ Rn
N 1(sgn(Tn)·Xn(m′)<0)T
o
= 1
2(1 + eθ Rn
N ).
(27)
Substituting
(27)
into
(26)
and
using
the
fact
that
{Rn}n=1,2,··· ,N is a permutation of {1, 2, . . . , N}, we obtain
∆(θ) =
Z 1
0
ln(1 + eθt)dt − ln2.
(28)
APPENDIX B
GMI OF SGRAND
The GMI of SGRAND can be calculated by the formula of
GMI [19, Eqn. (12)] as
ISGRAND = sup
θ<0


θEd(X, Y) − E

ln
X
x∈{+1,−1}
eθd(x,Y)
2




 ,
(29)
where d(x, y) =
ln q+(y)
q−(y)
 · 1

sgn

ln q+(y)
q−(y)

· x<0

.
With some calculations, we have
ISGRAND = ln2 − inf
θ<0
(
E

ln

1 + e
θ
ln
q+(Y)
q−(Y)


− θ · 1
2
Z
q+(y)<q−(y)
lnq+(y)
q−(y)
 q+(y)dy
− θ · 1
2
Z
q+(y)>q−(y)
lnq+(y)
q−(y)
 q−(y)dy
)
.
(30)
REFERENCES
[1] K. R. Duffy, J. Li, and M. M´edard, “Capacity-achieving guessing random
additive noise decoding,” IEEE Transactions on Information Theory,
vol. 65, no. 7, pp. 4023–4040, 2019.
[2] A. Riaz, M. Medard, K. R. Duffy, and R. T. Yazicigil, “A universal
maximum likelihood GRAND decoder in 40nm CMOS,” in 2022 14th
International Conference on COMmunication Systems & NETworkS
(COMSNETS), pp. 421–423, IEEE, 2022.
[3] S. Lin and D. J. Costello, “Error control coding, second edition,” 2004.
[4] K. R. Duffy, M. M´edard, and W. An, “Guessing random additive
noise decoding with symbol reliability information (SRGRAND),” IEEE
Transactions on Communications, vol. 70, no. 1, pp. 3–18, 2021.
[5] A. Solomon, K. R. Duffy, and M. M´edard, “Soft maximum likelihood
decoding using GRAND,” in ICC 2020-2020 IEEE International Con-
ference on Communications (ICC), pp. 1–6, IEEE, 2020.
[6] K. R. Duffy, W. An, and M. M´edard, “Ordered reliability bits guessing
random additive noise decoding,” IEEE Transactions on Signal Process-
ing, vol. 70, pp. 4528–4542, 2022.
[7] C. Condo, V. Bioglio, and I. Land, “High-performance low-complexity
error pattern generation for ORBGRAND decoding,” in 2021 IEEE
Globecom Workshops (GC Wkshps), pp. 1–6, IEEE, 2021.
[8] S. M. Abbas, T. Tonnellier, F. Ercan, M. Jalaleddine, and W. J. Gross,
“High-throughput and energy-efficient VLSI architecture for ordered
reliability bits GRAND,” IEEE Transactions on Very Large Scale
Integration (VLSI) Systems, vol. 30, no. 6, pp. 681–693, 2022.
[9] C. Condo, “A fixed latency ORBGRAND decoder architecture with
LUT-aided error-pattern scheduling,” IEEE Transactions on Circuits and
Systems I: Regular Papers, vol. 69, no. 5, pp. 2203–2211, 2022.
[10] M. Liu, Y. Wei, Z. Chen, and W. Zhang, “ORBGRAND is almost
capacity-achieving,” IEEE Transactions on Information Theory, vol. 69,
no. 5, pp. 2830–2840, 2022.
[11] H. Sarieddeen, M. M´edard, and K. R. Duffy, “GRAND for fading
channels using pseudo-soft information,” in GLOBECOM 2022-2022
IEEE Global Communications Conference, pp. 3502–3507, IEEE, 2022.
[12] S. M. Abbas, M. Jalaleddine, and W. J. Gross, “GRAND for Rayleigh
fading channels,” in 2022 IEEE Globecom Workshops (GC Wkshps),
pp. 504–509, IEEE, 2022.
[13] S. M. Abbas, M. Jalaleddine, and W. J. Gross, “Hardware architecture
for fading-GRAND,” in Guessing Random Additive Noise Decoding: A
Hardware Perspective, pp. 125–140, Springer, 2023.
[14] I. Chatzigeorgiou and F. A. Monteiro, “Symbol-level GRAND for high-
order modulation over block fading channels,” IEEE Communications
Letters, vol. 27, no. 2, pp. 447–451, 2022.
[15] S. Allahkaram, F. A. Monteiro, and I. Chatzigeorgiou, “URLLC with
coded massive MIMO via random linear codes and GRAND,” in 2022
IEEE 96th Vehicular Technology Conference (VTC2022-Fall), pp. 1–5,
IEEE, 2022.
[16] S. Allahkaram, F. A. Monteiro, and I. Chatzigeorgiou, “Symbol-level
noise-guessing decoding with antenna sorting for URLLC massive
MIMO,” arXiv preprint arXiv:2305.13113, 2023.
[17] G. Caire, G. Taricco, and E. Biglieri, “Bit-interleaved coded modula-
tion,” IEEE Transactions on Information Theory, vol. 44, no. 3, pp. 927–
946, 1998.
[18] A. G. i Fabregas, A. Martinez, G. Caire, et al., “Bit-interleaved coded
modulation,” Foundations and Trends® in Communications and Infor-
mation Theory, vol. 5, no. 1–2, pp. 1–153, 2008.
[19] A. Ganti, A. Lapidoth, and I. E. Telatar, “Mismatched decoding revisited:
General alphabets, channels with memory, and the wide-band limit,”
IEEE Transactions on Information Theory, vol. 46, no. 7, pp. 2315–
2328, 2000.
[20] A. Lapidoth and S. Shamai, “Fading channels: how perfect need ’perfect
side information’ be?,” IEEE Transactions on Information Theory,
vol. 48, no. 5, pp. 1118–1134, 2002.
"
"The paper presents ongoing improvements to the Automated Reasoning Tools in GeoGebra Discovery, showing examples of these new features. A new ShowProof command outputs both the steps for confirming a statement and a difficulty or interest grading. The difficulty assessment involves comparing the thesis (or conclusion) expression with the hypotheses, involving the notion of syzygy. The relevance of showing proof steps in geometric statements is discussed, along with evaluating the difficulty of statements, especially in relation to automated proving. A preliminary proposal for establishing difficulty criteria based on syzygy is introduced, considering statements where the thesis is a combination of products of the hypotheses polynomials (formally true), or can be derived as a combination of the hypotheses and negation of the thesis (geometrically true), with higher degree polynomials indicating more difficulty.","In recent years, different automated reasoning tools have been developed and included in both the standard version of GeoGebra and its fork version GeoGebra Discovery. The goal of this paper is to present ongoing work on two important improvements to GeoGebra Discovery: (1) visualizing the steps taken by the program to confirm a geometric statement, and (2) testing the suitability of a novel proposal to assess the interest, complexity, or difficulty of a given statement. The assessment method involves the concept of syzygy, which refers to combinations of polynomials that sum to zero.","The paper mentions various related works, including the formalization of Euclidean plane geometry arithmetization, a technique for evaluating the difficulty of geometric constructions, and research on the notion of interestingness in theorems. The proposal for difficulty assessment is based on the idea of comparing the expression of the thesis (or conclusion) with the hypotheses using syzygies.nannannan","The paper introduces the ShowProof command, which provides a detailed view of the steps taken by GeoGebra Discovery to confirm a geometric statement. This includes visualizing the algebraic translation of the input construction, numerical specialization of free points, automatic inclusion of non-degeneracy conditions, and the expression of 1 (or the thesis) as a combination of the hypotheses and the negation of the thesis (thus proving the statement by reductio ad absurdum), or as a combination of the hypotheses (direct proof of the statement). The complexity of a result is defined as the maximum degree of the syzygies expressing the thesis (or 1) as a combination of the hypotheses.nan","The paper presents examples to illustrate the ShowProof command and the proposed complexity measure. The results show that the complexity of a statement can vary depending on the specific formulation and choice of equations. For instance, the thesis describing the intersection of the three heights of a triangle can have different complexity values under different conditions.","The paper concludes that visualizing the proof steps and assessing the difficulty of geometric statements are valuable improvements to GeoGebra Discovery. The authors acknowledge that the proposal for complexity assessment requires further research, including a deeper analysis of the relationship between complexity and the ideal membership problem, as well as the impact of different factors such as statement formulation and variable specialization. The authors intend to address these issues in future work.","Showing Proofs, Assessing Difficulty with GeoGebra Discovery","Zoltán Kovács, Tomás Recio, M. Pilar Vélez","P. Quaresma and Z. Kovács (Ed.): Automated Deduction
in Geometry 2023 (ADG 2023).
EPTCS 398, 2024, pp. 43–52, doi:10.4204/EPTCS.398.8
© Z. Kovács et al.
This work is licensed under the
Creative Commons Attribution License.
Showing Proofs, Assessing Difficulty
with GeoGebra Discovery*
Zoltán Kovács
The Private University College of Education of the Diocese of Linz, Austria
zoltan.kovacs@ph-linz.at
Tomás Recio
Escuela Politécnica Superior, Universidad Antonio de Nebrija, Madrid, Spain
trecio@nebrija.es
M. Pilar Vélez
Escuela Politécnica Superior, Universidad Antonio de Nebrija, Madrid, Spain
pvelez@nebrija.es
In our contribution we describe some on-going improvements concerning the Automated Reasoning
Tools developed in GeoGebra Discovery, providing different examples of the performance of these
new features. We describe the new ShowProof command, that outputs both the sequence of the
different steps performed by GeoGebra Discovery to confirm a certain statement, as well as a number
intending to grade the difficulty or interest of the assertion. The proposal of this assessment measure,
involving the comparison of the expression of the thesis (or conclusion) as a combination of the
hypotheses, will be developed.
1
Introduction
In the past years we have been developing and including, both in the standard version of GeoGebra (GG)
as well as in the fork version GeoGebra Discovery,1 different automated reasoning tools. See [1] for a
general description and references.
The goal of the current contribution is to present some ongoing work regarding two different, but
related, important improvements of GeoGebra Discovery. One, to visualize the different steps that GG
Discovery performs with a given geometric statement until it declares its truth (or failure). Two, to test,
through different elementary examples, the suitability of an original proposal to evaluate the interest,
complexity or difficulty of a given statement. Let us advance that our proposal involves the notion of
syzygy of a set of polynomials.
The relevance of showing details about each of the steps performed by our automated reasoning
algorithms implemented in GG Discovery is quite evident. In fact, as a consequence of the result in [2],
describing the formalization of the arithmetization of Euclidean plane geometry, proofs of geometric
statements obtained using algebraic geometry algorithms are also valid on the synthetic geometry realm.
Although it might seem obvious that synthetic proofs facilitate human understanding of a geometric
statement, as compared with the difficult interpretation associated to algebraic proofs, this assertion
could be a matter of discussion if considered for statements (and for human minds) of a certain level,
*Authors supported by a grant PID2020-113192GB-I00 (Mathematical Visualization: Foundations, Algorithms and Appli-
cations) from the Spanish MICINN.
1See project page https://github.com/kovzol/geogebra-discovery.
44
Showing Proofs, Assessing Difficulty with GeoGebra Discovery
a discussion that it is out of the scope of the present paper, and that could be the subject of a future
research, in an educational context.
On the other hand, the evaluation of the difficulty of geometric statements is an old subject, regardless
of its relation to automated proving. We can mention the work of Lemoine on the number of steps
required by a geometric construction (a higher number of steps = a more complicated construction), a
proposal that has been thoroughly studied, adapted to the Dynamic Geometry context, and exemplified
in different repositories, e.g., in several recent works by Quaresma and collaborators, such as [3, 4].
But the complexity of a geometric construction is not, in general, a good measure to establish the
difficulty of a statement involving the same construction: one can make a very complicated figure and
then conjecture some obvious property, easy to derive from the construction steps.
Thus, in this paper we make a preliminary report of our current work aiming to establish some
difficulty criteria, that profits from our algebraic approach to proving geometric statements. Roughly: we
propose to label as more difficult those statements where:
1. first of all, the polynomial involved in the description of the thesis (or conclusion) is a sum of
products of the hypotheses polynomials multiplied by some other polynomials (thus, the statement
is just formally true). These other polynomials are usually named as syzygies,
2. else, there is a combination of 1 as a sum of products of the hypotheses polynomials and of the
negation of the thesis T (expressed as T ·t − 1) multiplied by some other polynomials (thus, the
statement is, by reductio ad absurdum, geometrically true, meaning that a power of the thesis is a
combination of the hypotheses). Again, these other polynomials that multiply the negation of the
thesis and the hypotheses, are usually named as syzygies,
3. these expressions of the thesis (or of 1) as a combination of the hypotheses (of the hypotheses and
the negation of the thesis) requires higher degree polynomials, i.e. more complicated syzygies.
Of course, although we are aware that difficulty and interest are not identical concepts, let us recall
that this notion, the interestingness of theorems, is also subject to current research (see [5, 6, 7, 8])
in different contexts (A.I., Big Data, etc.), sometimes explicitly referring to our particular automated
reasoning in geometry approach.
In what follows we will describe, mostly through some examples, our ongoing work on these two
subjects.
2
ShowProof Command
Unfortunately, till now, the algebraic geometry nature of the algorithms behind the automatic reasoning
tools implemented in GeoGebra or GeoGebra Discovery do not allow providing readable arguments jus-
tifying their outputs. Computations are performed in the background, using some embedded Computer
Algebra System, such as Giac [10]. The user only gets a kind of yes/no answer.
The ShowProof command is a first attempt to enhance the visibility of the proofs achieved by Ge-
oGebra Discovery, by showing the result of the different steps performed by GeoGebra Discovery to
confirm a certain statement: algebraic translation of the geometric input construction, numerical special-
ization of the coordinates of some free points, automatic inclusion of non-degeneracy conditions, and
writing – using the concept of syzygy and its computation – the expression of 1 as a combination of the
hypotheses and the negation of the thesis (thus proving the statement by reductio ad absurdum), or of the
thesis as a combination of the hypotheses (direct proof of the statement).
Z. Kovács et al.
45
Visualizing the output of most of these steps is just a question of improving the user interface, as it
does not involve any new computation. This is so except for the (most important) last two items: the
concrete expression of 1 (or of the thesis) as a combination of other polynomials. Indeed, to decide that a
statement is generally true [1] GeoGebra Discovery just has to perform some elimination of the ideal of
hypotheses plus the negation of the thesis, and to verify that it is not zero. Then, adding to the hypotheses
ideal the negation of some of the generators g of this elimination (using Rabinowitsch trick, as g·t −1),
it is obvious that the (new hypotheses + negation of thesis) ideal contains 1, since it will include both g
and g·t −1.
But the user, who has not any concrete evidence about the result of the previous elimination (for
example, expressing the added non-degeneracy condition as a combination of the (given hypotheses
+ negation of thesis) ideal, probably would appreciate handling an expression of 1 as a combination
of the (new hypotheses + negation of thesis) ideal. Or, even more impacting, viewing the thesis as
a combination of the hypotheses. Notice that the first possibility just means that over the complexes
the thesis vanishes over all points satisfying the new hypotheses (the given ones and the added non-
degeneracy condition) and, thus, that a power of the thesis is a combination of the hypotheses. We can
say that the statement is, in this case, geometrically true, while, if the thesis itself is a combination of the
hypotheses, we can declare we have a formally true statement.
Both issues are now addressed through the ShowProof command, using the concept of syzygy
(e.g. [9], page 104):
Given any G = (g1,...,gs) ∈ k[x1,...,xn]s, we can define a syzygy on G to be an s-tuple S =
(h1,...,hs) ∈ k[x1,...,xn]s, such that ∑i hi ·gi = 0.
In particular, if we include 1 (respectively, the thesis) as the first element of the collection of poly-
nomials G, and the remaining elements are the generators of the ideal of the new hypotheses plus the
negation of the thesis (respectively, of the new hypotheses), then we will get (if the statement is geo-
metrically true, respectively, symbolically true) syzygies of the kind (1,−h2,...,−hs), allowing us to
output a concrete expression of 1 (respectively, the thesis) as a combination of the new hypotheses plus
the negation of the thesis (respectively, of the new hypotheses).
The next figures illustrate the current output of the ShowProof command. Figure 1 displays, first,
the automatically and internally assigned coordinates of the free vertices of the triangle A,B,C; then,
the equations of the different heights; finally, the thesis (the fact that the last height goes through the
intersection of the other two).
Next, following our algorithm and without loss of generality, the program automatically specializes
the given free coordinates, to reduce the number of variables before starting the computations. This is
shown in figure 2, that ends declaring that the statement is geometrically true by explicitly showing 1
as a combination of the negation of the thesis and the hypotheses equations (thus, 1 = 0, since all these
equations are equal to zero). Finally, the last line declares that this statement is of difficulty 2, a measure
that we will roughly describe in the next section.
3
Interestingness
Although the precise formulation of the following reflections require a serious and future research anal-
ysis, that is not the goal of the present paper, we dare to consider quite evident that showing argu-
ments for the truth of a geometric statement is important in the scientific and educational context, even
more relevant nowadays, in a context of close collaboration (or leadership?) of machine-driven learning.
Analogously, under the dominance and ubiquitousness of machine learning environments, very prone to
46
Showing Proofs, Assessing Difficulty with GeoGebra Discovery
Figure 1: Viewing proof of the Intersection of heights theorem through ShowProof. Initial steps.
Figure 2: Viewing proof of the Intersection of heights theorem through ShowProof. Specialized equa-
tions, conclusion and difficulty.
Z. Kovács et al.
47
Figure 3: Automated Geometer: relevant (e.g. Theorem 7, medians intersect at a common point) and
trivial (e.g. Theorem 1, midpoint F of AB in the line AB, or Theorem 9, midpoint F is equidistant from
A and B) results.
automatically produce large amounts of geometric information, we think it is very relevant to develop
instruments that allow humans to assess the relevance of the obtained results.
For example, GeoGebra Discovery has already a Discover command that automatically finds all
statements of a certain kind that hold over some element of a construction (and a more general com-
mand in a web version, the Automated Geometer,2 automatically finding all the statements of a selected
kind (collinearity of three points, equality of distances between two points, etc., as declared in the dark
box below the geometric figure in figure 3) holding over all elements of a figure.) See also the recent
StepwiseDiscovery command, that discovers automatically all statements involving each of the new
elements that the user is adding in each construction step. Now, it happens that a great number of such
discovered statements are just obvious! See figure 3, for some examples.
Thus, we dare to introduce, as a measure of the complexity of a result, the following definition:
We say that a statement H ⇒ T is of complexity d if d is the maximum degree of the syzigies
expressing T (or 1) as a combination of the hypotheses (correspondingly, of the hypotheses and the
negation of the thesis).
In what follows we will describe different examples, towards analyzing the potential of this proposal
for the concept of an interesting statement.
2http://autgeo.online/ag/automated-geometer.html?offline=1
48
Showing Proofs, Assessing Difficulty with GeoGebra Discovery
3.1
Example 1
Let us build a point F as the midpoint of AB (see Figure 3), and ask about Discover(F). The program
discovers (among other, really interesting statements), that the length of segment FA is equal to the length
of FB. But, since the definition of F as midpoint of the segment with extremes A(a1,a2),B(b1,b2) is that
the coordinates of F are ((a1 +b1)/2,(a2 +b2)/2), it immediately follows that FA = ((a1 −b1)/2,(a2 −
b2)/2) and FB = (b1 −a1)/2,(b2 −a2)/2). Obviously, from these coordinates, it follows that the length
of both segments is identical, it only requires to simplify ((a1 − b1)/2)2 + ((a2 − b2)/2)2 − (((b1 −
a1)/2)2 + ((b2 − a2)/2)2), yielding 0. Thus, we could roughly declare that this thesis is just the trivial
equation 0 = 0 and thus it is always a combination of whatever set of hypotheses multiplied by zero.
Thus the degree of the zero polynomial 0 (that some algebraist consider as a negative number) could be
a measure of the complexity of this highly trivial statement.
3.2
Example 2
On a different example about elements in the same figure, if we consider the coordinates of point
G(g1,g2), the intersection of the line that goes from A to the midpoint E(e1,e2) of side BC, and of
the line that goes from B to the midpoint D(d1,d2) of side AC, where C(c1,c2), we have:
• Coordinates of E,D:
H1,H2 : e1 = (b1 +c1)/2,e2 = (b2 +c2)/2,
(1)
H3,H4 : d1 = (a1 +c1)/2,d2 = (a2 +c2)/2.
(2)
• Equations of lines A,E and B,D
line AE : (x−a1)·(e2 −a2)−(y−a2)·(e1 −a1) = 0,
(3)
line BD : (x−b1)·(d2 −b2)−(y−b2)·(d1 −b1) = 0.
(4)
• Thus, when we declare G as the intersection of these two lines, we introduce the equations
H5 : line AE : (g1 −a1)·(e2 −a2)−(g2 −a2)·(e1 −a1) = 0,
(5)
H6 : line BD : (g1 −b1)·(d2 −b2)−(g2 −b2)·(d1 −b1) = 0.
(6)
• Moreover, let us recall that midpoint F of side AB has coordinates:
H7,H8 : f1 = (a1 +b1)/2, f2 = (a2 +b2)/2.
(7)
• Finally, the Theorem 6 in figure 3, namely, G ∈ CF, means that
Thesis: G ∈ line CF : (g1 −c1)·(f2 −c2)−(g2 −c2)·(f1 −c1) = 0.
(8)
Now, after some involved computations on the syzygies of the set
(Thesis,H1,H2,...,H8),
here we use the same notation for the Thesis and the hypotheses Hi, but considering just the involved
polynomials, not the equality to zero. We find that we can express the Thesis as a combination of
(H1,H2,...,H8) multiplied by linear polynomials in the involved variables
a1,a2,b1,b2,c1,c2,d1,d2,e1,e2, f1, f2,g1,g2,
Z. Kovács et al.
49
namely, the thesis is equal to
Thesis = (−g1 + f1)·G[1]+(−f2 +g2)·G[2]+(1)·G[7]
(9)
where G[1],G[2],G[7] are elements of the Gröbner basis of the hypotheses ideal with respect to the
plex order, and can be expressed, respectively, as sums of products of [0,0,0,−1/2,0,−1/2,0,1/2],
[0,0,−1/2,0,−1/2,0,1/2,0], [0,2·g2 −2,−2·g1,−2·g2,0,2,2·g1,−2], times [H1,H2,...,H8].
In summary, the thesis is a combination of the hypotheses multiplied by polynomials of degree at
most 1. This degree 1, vs. the negative degree of the syzygies in the previous statement shows, it is our
proposal, that this second statement, about the coincidence of the intersection of the three medians of a
triangle, is more difficult than the statement about the equality of lengths of the two segments determined
by the mid-point.
Let us remark that this same statement, but specializing vertices A = (0,0), B = (0,1), leads to degree
2 syzygies
Thesis = (−c1/2+g1/2)·G[1]+(−g2 +c2)·G[2]+(−g1)·G[6]
+(g2 −1/2)·G[7]−1·G[8]
(10)
where, again, G[1],G[2],G[6],G[7],G[8] are elements of the Gröbner basis of the (specialized) hypothe-
ses ideal with respect to the tdeg order, and can be expressed, respectively, as sums of products of
[0,0,0,0,2,0,0,0], [1,0,0,0,0,0,0,0], [0,0,0,0,0,0,−2,0], [0,2·g2 −2,−2·g1,−2·g2,0,2,2·g1,−2],
[0,2·(g2 −1)·g2,−2·g1 ·g2,−2·(g2 −1)·g2,0,2·g2,2·g1 ·g2,−2·g2 +2], times [H1,H2,...,H8], notice
that only the summand involving G[7] rises the degree to degree two.
3.3
Example 3
As a third example, let us start (see Figure 1) with a triangle with vertices A(v13,v14), B(v15,v16),
C(v17,v18). Then, consider the perpendicular line through A to side BC. Considering P(v19,v20) as the
coordinates of a generic point P in this line, we obtain the equation
H1 : (v19−v13)·(v17−v15)+(v20−v14)·(v18−v16) = 0.
(11)
Similarly for the other two heights.
Thus, the statement the three heights of a triangle have a common intersection is a matter of con-
sidering two hypotheses (the equation of height from A and from, say, B) with a common generic point
P. And the thesis is also one equation, namely, showing that this generic point P satisfies that line PC is
perpendicular to line AB. More precisely:
• Hypotheses:
– Height from A:
(v19−v13)·(v17−v15)+(v20−v14)·(v18−v16) = 0.
(12)
– Height from B:
(v19−v15)·(v17−v13)+(v20−v16)·(v18−v14) = 0.
(13)
• Thesis:
(v19−v17)·(v15−v13)+(v20−v18)·(v16−v14) = 0.
(14)
50
Showing Proofs, Assessing Difficulty with GeoGebra Discovery
Notice that these equations seem different from those describing this statement in figure 1. There, the
height from A is described by considering the coordinates of point X(v19,v20) in this line, and regarding
such point as the translation of A by a vector AX perpendicular to BC so that −v20+v17−v15+v14 =
0,−v19−v18+v16+v13 = 0. Finally, Equation (17) in figure 1, describes a generic point (v25,v26) of
this line, verifying v25·v20−v26·v19−v25·v14+v19·v14+v26·v13−v20·v13 = 0.
But let us remark that this equation is practically the same as the one above introduced for the height
from A, replacing there generic point coordinates (v19,v20) by (v25,v26) and, in the new equation,
v20 = v17−v15+v14,v19 = v16+v13−v18, yielding
v25·v20−v26·v19−v25·v14+v19·v14+v26·v13−v20·v13 =
= v25·(v17−v15+v14)−v26·(v16+v13−v18)−v25·v14+
(v16+v13−v18)·v14+v26·v13−(v17−v15+v14)·v13 =
= (v15−v17)·(v13−v25)+(v16−v18)·(v14−v26).
Similarly, the thesis is now, with the notation of figure 1:
v25·v24−v26·v23−v25·v18+v23·v18+v26·v17−v24·v17 = 0
(15)
that belongs to the new ideal of hypotheses ⟨−v20+v17−v15+v14,−v19−v18+v16+v13,−v22−
v17+v16+v13,−v21+v18+v15−v14,−v24+v18+v15−v13,−v23+v17−v16+v14,v25·v20−
v26·v19−v25·v14+v19·v14+v26·v13−v20·v13,v25·v22−v26·v21−v25·v16+v21·v16+v26·
v15−v22·v15⟩.
The interesting point here is that the complexity of the statement is different if one considers the
previous equations or the ones in figure 1. In the first case, it is immediate to see that the thesis is the
difference of the height from B minus the height from A. The same result holds if we do specialize
numerically A,B, as it is usual in GeoGebra Discovery, considering A(0,0),B(0,1), so that the above
hypotheses turn out to be:
• Height from A:
(v19−0)·(v17−0)+(v20−0)·(v18−1) = 0.
(16)
• Height from B:
(v19−0)·(v17−0)+(v20−1)·(v18−0) = 0.
(17)
• Thesis:
(v19−v17)·(0−0)+(v20−v18)·(1−0) = 0.
(18)
So we could say that here the syzygies are of degree 0, and thus we can declare the complexity of the
statement is 0, as the thesis is just a linear combination of the hypotheses (multiplied by constants).
But, with the equations in figure 1, using the same tdeg order, one gets the thesis as a combination
of the hypothesis multiplied by polynomials of degree at most one, so the complexity rises one element
and, if we do specialize A(0,0),B(0,1) in the equations of figure 1, we get that the syzygies have to be
of degree 2, as stated in figure 2!
4
Conclusions
The explicit visualization of the algebraic expression that connects hypotheses and thesis (or expresses
number 1 as a combination of hypotheses and the negation of the thesis) seems a relevant improvement
Z. Kovács et al.
51
of GeoGebra Discovery automated reasoning tools, allowing the user not only to be able to personally
confirm the truth of the given statement, but also to measure its difficulty.
Of course, the proposal of this measure is just on its initial steps and requires much further research.
For example, extending the already considered examples, including others of higher difficulty, such
as those concerning Mathematics Olympiad problems. Indeed, we have already verified with some
of them that our measure of the computed difficulty agrees that they are much more difficult than the
traditional school problems, but analyzing more problems from different sources, comparing our rank
and the behavior of the olympic teams on the same problems, is still on-going work [11].
Moreover, the proposed research should not just be restricted to producing benchmarks, but to reflect
on several more conceptual issues, such as the role, on the measure of the complexity, of the different
ways to express the same statement (as remarked in the previous examples). Ditto, for the specializa-
tion of some variables, for choosing different ordering for computing syzygies, etc. We also need to
understand the difference, or the connection, between the complexity of expressing a statement and the
complexity of deciding its truth, the relation between the complexity of the statement and the complexity
of the associated ideal membership problem, etc.
Indeed, we think there is plenty of work in this context and it is our intention to address such issues
in a near future.
References
[1] Kovács, Z.; Recio, T.; Vélez, M. P.: Automated reasoning tools with GeoGebra: What are they? What are
they good for? In: P. R. Richard, M. P. Vélez, S. van Vaerenbergh (eds): Mathematics Education in the
Age of Artificial Intelligence: How Artificial Intelligence can serve mathematical human learning. Series:
Mathematics Education in the Digital Era, Vol. 17, 23–44. Springer Cham (2022). doi:10.1007/978-3-030-
86909-0_2
[2] Boutry, P.; Braun, G.; Narboux, J.: Formalization of the arithmetization of Euclidean plane geometry and
applications. Journal of Symbolic Computation 90, 149–168 (2019). doi:10.1016/j.jsc.2018.04.007
[3] Quaresma, P.; Santos, V.; Graziani, P.; Baeta, N.: Taxonomy of geometric problems. Journal of Symbolic
Computation 97, 31–55 (2020). doi:10.1016/j.jsc.2018.12.004
[4] Santos, V.; Baeta, N.; Quaresma, P.: Geometrography in Dynamic Geometry. The International Journal for
Technology in Mathematics Education, vol. 26, no. 2, June 2019, 89–96 (2019).
[5] Gao, H.; Goto, Y.; Cheng, J.: A set of metrics for measuring interestingness of theorems in automated
theorem finding by forward reasoning: A case study in NBG set theory. Proceedings of the International
Conference on Intelligence Science and Big Data Engineering (2015). Part II. Lecture Notes in Computer
Science 9243, 508–517 (2015). doi:10.1007/978-3-319-23862-3_50
[6] Gao, H.; Li, J.; Cheng J.: Measuring Interestingness of Theorems in Automated Theorem Finding by Forward
Reasoning Based on Strong Relevant Logic. In: 2019 IEEE International Conference on Energy Internet
(ICEI), 356–361 (2019). doi:10.1109/ICEI.2019.00069
[7] Puzis, Y.; Gao, Y.; Sutcliffe G.: Automated generation of interesting theorems. In G. Sutcliffe and R. Goebel,
editors: Proceedings of the 19th International FLAIRS Conference, AAAI Press, Menlo Park, 49–54 (2006).
[8] Colton, S.; Bundy, A.; Walsh, T.: On the notion of interestingness in automated mathematical discovery.
International Journal of Human-Computer Studies Volume 53, Issue 3, September 2000, 351–375 (2000).
doi:10.1006/ijhc.2000.0394
[9] Greuel, G-M.; Pfister, G.: A Singular introduction to commutative algebra. Springer Berlin, Heidelberg.
Second Edition (2008). doi:10.1007/978-3-540-73542-7
52
Showing Proofs, Assessing Difficulty with GeoGebra Discovery
[10] Kovács, Z.; Parisse, B.: Giac and GeoGebra – improved Gröbner basis computations. Presentation at RICAM
Special semester on Applications of Algebra and Number Theory, Workshop 3 on Computer Algebra and
Polynomials. https://www.ricam.oeaw.ac.at/specsem/specsem2013/workshop3/slides/pariss
e-kovacs.pdf. November (2013).
[11] Ariño-Morera, B.; Recio, T.; Tolmos, P.: Olympic geometry problems: human vs. machine. Communication
to the CADGME (Digital Tools in Mathematics Education) 2022 Conference. Abstracts available at https:
//drive.google.com/file/d/1qF4ceMg6gNklOPa1JVkgKND1dOqNmyka/view.
"
"In this paper, the authors propose a framework for completing incomplete conjectures and incomplete proofs. The framework can turn a conjecture with missing assumptions and with an underspecified goal into a proper theorem. Also, the proposed framework can help in completing a proof sketch into a human-readable and machine-checkable proof.","Automated theorem provers take as input the formal statement of a conjecture in a theory described by axioms and lemmas, and try to generate a proof or a counter-example for this conjecture. However, in mathematical practice, in the context of education and also in mathematical research, the conjecturing and proving activities are not separated but interleaved. The practitioner may try to prove a statement which is valid only assuming some implicit or unknown assumptions, while the list of lemmas and theorem which can be used may not be complete. In education, for some kind of exercises, a precise formulation of the statement to be proved is also left to the student, with questions such as: “What is the nature of the quadrilateral ABCD?”. Hence, the conjecture can contain unknown assumptions called abducts, and the goal may be not completely specified. One may also ask for a proof using a particular theorem or an intermediate fact, i.e., a proof partially specified using constraints specifying some proof steps.","nannannanThis section provides some necessary background information on a fragment of first-order logic called coherent logic that our approach uses. There are several automated provers for coherent logic, including Larus, which is based on “theorem proving as constraint solver” paradigm.","In this paper, the authors restrict consideration of abduction only to coherent logic and only to abducts that are conjunctions of ground atomic formulae. Larus was not implemented with abduction in mind, yet implementation of support for abduction turned out to be very simple, almost trivial, and took less than 100 lines of C++ code. In order to find abducts using Larus, the authors treat them as a special case of proof steps, in the main proof branch, just after assumptions. They have to add constraints on what such an abduct can be:

1. the abduct is treated as an assumption;
2. the nesting of the abduct equals 1;
3. the abduct is an atomic formula (no branching);
4. the predicate symbol is one of the predicate symbols in the signature;
5. the arguments are among existing symbols of constants;
6. the abduct is not the goal itself;
7. the abduct is not ⊥.

The given conditions may be written in the following way, assuming that the abduct is placed in i-th step of the proof:

1. StepKind (i) =ASSUMPTION
2. Nesting (i) = 1
3. Cases (i) = false
4. ContentsPredicate (i,0) < sizeof(Signature)
5. for each argument j (up to maximal arity): ContentsArgument (i,0, j) < sizeof(Constants)
6. Goal (i) = false
7. ContentsPredicate (i,0) ≠ ⊥

One can also choose a number of abducts, each leading to the constraints given above.nan","For the first inverse problem (Problem 2 from Section 1), Larus produces two consistent, symmetric abducts (the proof obtained with the first abduct is presented in Appendix 7.2):

• “ H is the midpoint of AD”
• “ H is the midpoint of DA”

Example 2

For the second inverse problem (Problem 3 from Section 1), Larus produces more than 150 consistent abducts, most of which give degenerate cases, hence are less interesting. Apart from such abducts, we obtained the following abducts and their symmetric variants (the proof with the first abduct is presented in Appendix 7.3):

• “ the diagonals HF and EG are congruent”
• “∉ FGH is a right angle”
• “∉ EHG is a right angle”
• “∉ HEF is a right angle”
• “∉ EFG is a right angle”","In this paper we have shown how a prover using the “theorem proving as constraint solving” paradigm can be extended such that it can complete partially specified conjectures and partially specified proofs. This extension is simple, and the implementation update is very small. The completion algorithm is uniform, since all three completion tasks (completing assumptions, completing goals, completing proofs) are handled in the same spirit – in terms of adding or deleting some constraints. To our knowledge, this approach is new, and we are not aware of other systems that can address all three sorts of completion tasks.",Automated Completion of Statements and Proofs in Synthetic Geometry: an Approach based on Constraint Solving,"Salwa Tabet Gonzalez, Predrag Janičić, Julien Narboux","P. Quaresma and Z. Kov´acs (Ed.): Automated Deduction
in Geometry 2023 (ADG 2023).
EPTCS 398, 2024, pp. 21–37, doi:10.4204/EPTCS.398.6
Automated Completion of Statements and Proofs in Synthetic
Geometry: an Approach based on Constraint Solving
Salwa Tabet Gonzalez
UMR 7357 CNRS
University of Strasbourg
Pˆole API, Bd S´ebastien Brant
BP 10413
67412 Illkirch, France
tabetgonzalez@unistra.fr
Predrag Janiˇci´c
Department for Computer Science
Faculty of Mathematics
University of Belgrade
Studentski trg 16
11000 Belgrade, Serbia
janicic@matf.bg.ac.rs
Julien Narboux
UMR 7357 CNRS
University of Strasbourg
Pˆole API, Bd S´ebastien Brant
BP 10413
67412 Illkirch, France
narboux@unistra.fr
Conjecturing and theorem proving are activities at the center of mathematical practice and are dif-
ficult to separate. In this paper, we propose a framework for completing incomplete conjectures
and incomplete proofs. The framework can turn a conjecture with missing assumptions and with an
under-specified goal into a proper theorem. Also, the proposed framework can help in completing a
proof sketch into a human-readable and machine-checkable proof. Our approach is focused on syn-
thetic geometry, and uses coherent logic and constraint solving. The proposed approach is uniform
for all three kinds of tasks, flexible and, to our knowledge, unique such approach.
1
Introduction
Automated theorem provers take as input the formal statement of a conjecture in a theory described by
axioms and lemmas, and try to generate a proof or a counter-example for this conjecture. In the field
of geometry, several efficient automated theorem proving approaches have been developed, including
algebraic ones such as Wu’s method, Gr¨obner bases method, and semi-synthetic methods such as the
area method. In these approaches, typically, the conjecture and the axioms being considered are fixed.
However, in mathematical practice, in the context of education and also in mathematical research, the
conjecturing and proving activities are not separated but interleaved. The practitioner may try to prove a
statement which is valid only assuming some implicit or unknown assumptions, while the list of lemmas
and theorem which can be used may not be complete. In education, for some kind of exercises, a precise
formulation of the statement to be proved is also left to the student, with questions such as: “What is
the nature of the quadrilateral ABCD?”. Hence, the conjecture can contain unknown assumptions called
abducts, and the goal may be not completely specified. One may also ask for a proof using a particular
theorem or an intermediate fact, i.e., a proof partially specified using constraints specifying some proof
steps.
In this paper, we consider the problems of (simultaneously) completing (a) the assumptions of the
conjecture; (b) the goal of the conjecture; (c) a proof sketch for the conjecture. The completion process
should lead to a proof that is both machine-checkable and human-readable. Because we aim at produc-
ing intelligible and readable proofs, with a similar level of granularity as paper-and-pencil proofs, our
approach is logic-based, uses a fragment of first-order logic called coherent logic, and is focused on
synthetic geometry (in contrast to algebraic methods). Our approach for dealing with partial conjectures
and partial proofs is implemented as an extension of the automated theorem prover Larus developed
previously [14]. The approach is uniform for all three kinds of completion tasks, flexible and, to our
knowledge, unique such approach.
22
Automated Completion of Statements and Proofs in Synthetic Geometry
A
B
C
D
E
F
G
H
A
B
C
D
E
F
G
H
A
B
C
D
E
F
G
H
Figure 1: Illustrations for five problems related to Varignon’s theorem, respectively: Problem 1; Problem
2; Problem 3.
We list five high-school level synthetic geometry problems related to Varignon’s theorem (Figure 1),
that we will try to solve using our approach.
Problem 1 (Fully specified statement) Consider a quadrilateral ABCD, let E, F, G and H be the mid-
points of AB, BC, CD, DA respectively. Prove that the quadrilateral EFGH is a parallelogram
(assuming that there are no two sides that are aligned).
Problem 2 (First inverse problem) Consider a quadrilateral ABCD, let E, F, and G be the midpoints
of AB, BC and CD respectively. Let H be a point. Under which assumption is the quadrilateral
EFGH a parallelogram?
Problem 3 (Second inverse problem) Consider a quadrilateral ABCD, let E, F, G and H be the mid-
points of AB, BC, CD, DA respectively. Under which assumption is the quadrilateral EFGH a
rectangle?
Problem 4 (Partially specified goal) Consider a quadrilateral ABCD, let E, F, G and H be the mid-
points of AB, BC, CD, DA respectively. What is the nature of the quadrilateral EFGH?
Problem 5 (Partially specified proof) Consider a quadrilateral ABCD, let E, F, G and H be the mid-
points of AB, BC, CD, DA respectively. We have that EG = FH. Prove that EFGH is a rectangle
using the axiom “If the diagonals of a parallelogram are congruent, then it’s a rectangle”.
The above examples are inspired by exercises given in a teacher training session. A more detailed
discussion about how these examples can be used in a didactic context, issues related to the formalization
can be found in [11, 19]
2
Background
This section provides some necessary background information on a fragment of first-order logic called
coherent logic that our approach uses. There are several automated provers for coherent logic, including
Larus, which is based on “theorem proving as constraint solver” paradigm.
2.1
Coherent Logic
A formula of first-order logic is said to be coherent if it has the following form:
A0(⃗x)∧...∧An−1(⃗x) ⇒ ∃⃗y(B0(⃗x,⃗y)∨...∨ Bm−1(⃗x,⃗y))
Gonzalez & Janiˇci´c & Narboux
23
where universal closure is assumed, and where⃗x denotes a sequence of variables x0,x1,...,xk−1; Ai (for
0 ≤ i ≤ n−1) denotes an atomic formula (involving zero or more variables from⃗x);⃗y denotes a sequence
of variables y0,y1,...,yl−1; B j (for 0 ≤ j ≤ m−1) denotes a conjunction of atomic formulae (involving
zero or more of the variables from⃗x and⃗y) [14]. If there are no formulae Ai, then the left-hand side of the
implication is assumed to be ⊤. If there are no formulae B j, then the right-hand side of the implication
is assumed to be ⊥. There are no function symbols with arity greater than zero. Coherent formulae do
not involve the negation connective. A coherent theory is a set of sentences, axiomatized by coherent
formulae, and closed under derivability. A number of theories and theorems can be formulated directly
and simply in coherent logic (CL). In addition, any first-order theory can be translated into CL, possibly
with additional predicate symbols [12, 21]. Synthetic geometry can be expressed easily using CL. For
example, the central part of axioms system of Euclid (as formalized by Beeson et al. [3]), or Hilbert
(as formalized by Braun et al. [6]), or Tarski [26] can be expressed in first-order logic without function
symbols, and the axioms are mostly in CL form.
Translation of FOL formulae into CL involves elimination of the negation connectives: negations
can be kept in place and new predicates symbols for corresponding sub-formula have to be introduced,
or negations can be pushed down to atomic formulae [21]. In the latter case, for every predicate symbol
R (that appears in negated form), a new symbol R is introduced that stands for ¬R, and the following
axioms are introduced: ∀⃗x(R(⃗x)∧R(⃗x) ⇒ ⊥), ∀⃗x(R(⃗x)∨R(⃗x)).
In contrast to resolution-based theorem proving, in forward reasoning for CL, the conjecture being
proved is kept unchanged and proved without using refutation, Skolemization and clausal form. Thanks
to this, CL is suitable for producing human-readable synthetic proofs and also machine verifiable proofs
[4, 12]. The problem of provability in CL is semi-decidable. CL admits a simple proof system, a sequent-
based variant is as follows [27]:
Γ,ax,A0(⃗a),...,An−1(⃗a),B0(⃗a,⃗b)∨...∨Bm−1(⃗a,⃗b) ⊢ P
Γ,ax,A0(⃗a),...,An−1(⃗a) ⊢ P
MP
Γ,B0(⃗c) ⊢ P
...
Γ,Bm−1(⃗c) ⊢ P
Γ,B0(⃗c)∨...∨Bm−1(⃗c) ⊢ P
QEDcs (case split)
Γ,Bi(⃗a,⃗b) ⊢ ∃⃗y(B0(⃗a,⃗y)∨...∨ Bm−1(⃗a,⃗y))
QEDas (assumption)
Γ,⊥ ⊢ P QEDefq (ex falso quodlibet)
In the rules given above, it is assumed: ax is a formula A0(⃗x) ∧ ... ∧ An−1(⃗x) ⇒ ∃⃗y(B0(⃗x,⃗y) ∨ ... ∨
Bm−1(⃗x,⃗y));1 ⃗a,⃗b, ⃗c denote sequences of constants (possibly of length zero); in the rule MP (extended
modus ponens),⃗b are fresh constants; ⃗x and ⃗y denote sequences of variables (possibly of length zero);
Ai(⃗x) (respectively Bi(⃗x,⃗y)) have no free variables other than from ⃗x (respectively ⃗x and ⃗y); Ai(⃗a) are
ground atomic formulae; Bi(⃗a,⃗b) and Bi(⃗c) are conjunctions of ground atomic formulae; Φ denotes the
list of conjuncts in Φ if Φ is conjunction, and otherwise Φ itself. In the proving process, the rules are
1Notice the hidden link between the formulae Bi(⃗a,⃗b) from the rule MP and the formula ax: the formulae Bi(⃗a,⃗b) from the
rule are instances of the formulae Bi(⃗x,⃗y) from ax.
24
Automated Completion of Statements and Proofs in Synthetic Geometry
read from bottom to top, i.e., by a rule application one gets the contents (new sub-goals) above the line.
For a set of coherent axioms AX and the statement A0(⃗x) ∧ ... ∧ An−1(⃗x) ⇒ ∃⃗y(B0(⃗x,⃗y) ∨ ... ∨
Bm−1(⃗x,⃗y)) to be proved, within the above proof system one has to derive the following sequent (where⃗a
denotes a sequence of new symbols of constants): AX,A0(⃗a),...,An−1(⃗a) ⊢ ∃⃗y(B0(⃗a,⃗y)∨...∨ Bm−1(⃗a,⃗y)).
Notice that, in the above proof system, case split may occur only at the end of a (sub)proof. However,
it is not a substantial restriction: any proof with unrestricted use of case split can be transformed to such
form.
2.2
Theorem Proving as Constraint Solving and the Larus System
“Theorem proving as constraint solving” is a paradigm for automated theorem proving recently proposed
[14]. In contrast to common automated theorem proving approaches, in which the search space is a set
of some formulae and what is sought is again a (goal) formula, this new approach is based on searching
for a proof (of a given length) as a whole. Namely, a proof of a formula in a fixed logical setting can be
encoded as a sequence of natural numbers obeying some constraints. A suitable solver can find such a
sequence and from that sequence a sought proof can be reconstructed. This approach is implemented in
C++, within an open-source prover Larus,2 specialized in proofs in coherent logic and using SAT, SMT,
and CSP solvers for solving sets of constraints. Larus can generate readable, human understandable
proofs in natural language and also machine-verifiable proofs for the interactive provers Coq, Isabelle,
and Mizar.
Each CL proof consists of several proof steps, while each of them has one of the following kinds
(with obvious meaning): ASSUMPTION, MP, FIRSTCASE, SECONDCASE, QEDBYCASES, QED-
BYASSUMPTION, QEDBYEFQ. The information relevant for MP steps include: AxiomApplied, From
(the ordinal numbers of proof steps justifying premises of the axiom applied), Instantiation (of the
variables in the axiom), Contents (the atoms in formula in the proof step), etc. Nesting denotes the
nesting of the proof step (the nesting of the first step is 1).
The proof can be represented by a sequence of numbers, meeting some constraints (that correspond
to definitions of inference steps given in Section 2.1). For instance, if the proof step s is of the kind
QEDBYEFQ, then the following conditions must hold (given almost in verbatim as in our C++ code):3
1. StepKind (s) = QEDBYEFQ;
2. s > 0;
3. Contents (s−1)(0) = ⊥;
4. Goal (s);
5. Nesting (s) = Nesting (s−1).
The above conditions can be understood in the following way: if there is a proof of the given conjecture,
the proof step s in that proof is of the kind QEDBYEFQ iff the natural number StepKind (s) equals the
code for QEDBYEFQ, s > 0 (since there must be a previous step), the contents of the previous proof
step is ⊥, the contents of the step is the goal itself, and the nesting of the steps s−1 and s is the same.
Each proof step has one of the listed kinds and meet corresponding conditions. There are also some
additional, global constraints, like that the last proof step has Nesting equal 1.
2https://github.com/janicicpredrag/Larus
3The corresponding C++ implementation is an improved version of the implementation presented earlier [14].
Gonzalez & Janiˇci´c & Narboux
25
Larus works in the following way. If there is a set of axioms, a conjecture, and a proof length, a
corresponding proof can be represented as a sequence of natural numbers, still unknown, so they will be
represented by variables V. The constraints that have to be met for each proof step and for the proof as
a whole can be expressed in terms of these variables V. If a solver can find a model for the constraint,
from it the proof in logical terms can be reconstructed. All constraints involved are linear constraints
over natural numbers. Since linear arithmetic is decidable, decision procedures for it can decide, for
each input constrains, whether or not it has a model. For this purpose, Larus can use SAT, SMT, and CSP
solvers. For input, Larus uses axioms and conjectures stored in a file in TPTP/fof format.
3
Abducts and Completing Assumptions
There are three major types of logical inference: induction, deduction, and abduction. The concept of
abduction has been introduced by Peirce [20]. In deduction, everything inferred is necessarily true, while
it is not the case with the remaining two types of inference. Induction tries to infer general rules based
on individual instances. The aim of abduction is to produce additional hypotheses to explain observed
facts. Abduction has a wide spectrum of implicit or explicit applications – in everyday life, in education,
and in scientific reasoning, including in building mathematical theories, or in software verification. One
definition of abduct is given below.
Definition 1 Given a theory T and a formula G (the goal to be proved), such that T ̸|= G, an explanations
or abduct is a formula A meeting conditions: T,A |= G and T,A ̸|= ⊥.
It is clear that some abducts are not interesting, so there are often some additional restrictions given.
There is no general agreement about such restrictions, but two types are most usual: syntactical restric-
tions (abducts should be of a specific syntactical form) and minimality restrictions (for any other abduct
A′, if T,A |= A′ then A ≡ A′). It is reasonable to ask that A is not G, as it is trivial. Some authors also add
stronger a restriction that A ̸|= G (i.e., at least one axiom of T has to be used to prove G).
Approaches for Computing Abducts.
Various algorithms to produce different kind of abducts have
been developed [1]. In abductive logic programming, techniques for abductive reasoning are developed
in the context of logic programming. Rules are considered to be Horn clauses [8]. According to Russo
et al. [25], some systems assume that predicate symbols appearing in abducts do not appear in the con-
clusion of any rule and that negation does not appear in the conclusion of any rule. This restriction is
not realistic in the context of geometry. In our example, we want to accept geometric predicate symbols
both in abducts, and in the assumptions and conclusion of theorems. Some approaches are based on
Robinson’s resolution algorithm, extended such that when no more clauses can be produced, the atomic
clauses are considered as a potential abduct and consistency if checked [17]. There are also approaches
developed for the context of SMT solving, dealing with decidable theories like linear arithmetic [10, 23]
In the context of geometry, some algebraic algorithms can generate additional assumptions for the
statement to be true. For example, Wu’s method [28] can produce non-degeneracy conditions. Algebraic
methods can also be used to generate more general abducts [22]. These methods are more efficient than
ours, but more specific so cannot be used for arbitrary geometric theories. Also, they cannot generate
readable proofs. Moreover, expressing algebraic non-degeneracy conditions in simple geometrical terms
is not easy and not always possible [7].
26
Automated Completion of Statements and Proofs in Synthetic Geometry
Abduction in Synthetic Euclidean Geometry.
In this paper, the theory T from Definition 1 is a syn-
thetic Euclidean geometry. In this context, automated finding of proofs allowing abducts may have
several applications. For instance, an automated system may help a student or a researcher who tries
to prove (or formalize) a theorem with a missing assumption. Barbosa et al. have proposed such goal
(although not for geometry) in the context of interactive proof assistants where conjectures are sent to an
SMT solver [2].
Non-degeneracy conditions are often overlooked and missing in informal geometry statements. Ab-
ductive reasoning is also a task which can be asked explicitly to students. The answer expected by the
teacher for Problem 2 is that H should be the midpoint of AD.
Finding Abducts using Larus.
In this paper, we restrict consideration of abduction only to coherent
logic and only to abducts that are conjunctions of ground atomic formulae. Larus was not implemented
with abduction in mind, yet implementation of support for abduction turned out to be very simple, almost
trivial, and took less than 100 lines of C++ code. In order to find abducts using Larus, we treat them as a
special case of proof steps, in the main proof branch, just after assumptions. We have to add constraints
on what such an abduct can be:
1. the abduct is treated as an assumption;
2. the nesting of the abduct equals 1;
3. the abduct is an atomic formula (no branching);
4. the predicate symbol is one of the predicate symbols in the signature;
5. the arguments are among existing symbols of constants;
6. the abduct is not the goal itself;
7. the abduct is not ⊥.
The given conditions may be written in the following way, assuming that the abduct is placed in i-th
step of the proof:
1. StepKind (i) =ASSUMPTION
2. Nesting (i) = 1
3. Cases (i) = false
4. ContentsPredicate (i,0) < sizeof(Signature)
5. for each argument j (up to maximal arity): ContentsArgument (i,0, j) < sizeof(Constants)
6. Goal (i) = false
7. ContentsPredicate (i,0) ̸= ⊥
One can also choose a number of abducts, each leading to the constraints given above. With such
additional constraints for each abduct (for additional proof steps in specific positions in a proof sought),
with a given set of axioms and a conjecture, and with a concrete proof length, we run Larus as usual.
The solving/proving process is the same as without abducts: the constraint solver finds a way to specify
a full proof, including the abducts, i.e., under-specified assumptions.
In the above list of conditions, the last two do not follow the basic definition of abduct. Like in some
other variants of the definition, the abduct may not be equal to the goal atom because such abducts are
trivial. Also, the abduct may not be equal to ⊥, since it is inconsistent. It is important to discard other
Gonzalez & Janiˇci´c & Narboux
27
inconsistent abducts early, so we add one more restriction: the proof of T,A |= G should not end with
QEDBYEFQ. Some constructed abducts may still be inconsistent with other assumptions, and we use
an external, more efficient automatic theorem prover, Vampire [16], to discard such abducts.
Example 1 For the first inverse problem (Problem 2 from Section 1), Larus produces two consistent,
symmetric abducts (the proof obtained with the first abduct is presented in Appendix 7.2):
• “H is the midpoint of AD”
• “H is the midpoint of DA”
Example 2 For the second inverse problem (Problem 3 from Section 1), Larus produces more than 150
consistent abducts, most of which give degenerate cases, hence are less interesting. Apart from such
abducts, we obtained the following abducts and their symmetric variants (the proof with the first abduct
is presented in Appendix 7.3):
• “the diagonals HF and EG are congruent”
• “̸ FGH is a right angle”
• “̸ EHG is a right angle”
• “̸ HEF is a right angle”
• “̸ EFG is a right angle”
4
Deducts and Completing Goals
Non-trivial first-order logic theories have infinite number of theorems. Approaches based on refutation
cannot be used with under-specified goals and, hence, cannot be used for completing them. In principle,
a controlled forward-reasoning (for instance, based on some kind of breadth-first search) can enumerate
all theorems of a theory. However, such a systematic approach can be hardly useful for some practical
applications, like looking for possible conjectures of a specific form. Our framework allows (but does not
require) specifying partially the form of the goal: for instance, one may specify the dominant predicate
symbol in the goal atom, or some of its arguments.
Finding Deducts in Synthetic Euclidean Geometry.
In the field of geometry, deduct candidates can
also be guessed based on an illustration, giving a concrete model. However, these deduct candidates still
have to be verified i.e., proved. Potential deducts could also be listed as large disjunctions of atomic
formulae, but this method does not scale when the list of potential deducts is too long.
Finding Deducts using Larus.
In Larus, if the goal is given i.e., fully specified, corresponding con-
straints are added to the full constraint representing a proof sought. Let us assume that the final step
of the proof is (some fixed) n and, for simplicity, let us assume that the goal is just a single atom. The
corresponding constraint then includes:
1. Nesting (n) = 1
2. Cases (n) = false
3. ContentsPredicate (n,0) = the goal predicate symbol
28
Automated Completion of Statements and Proofs in Synthetic Geometry
4. for each argument j (except for existentially quantified variables):
ContentsArgument (n,0, j) = the argument from the given goal instantiated.
If the goal is under-specified, for instance, if the predicate symbol is not given (it is given as _ in the
TPTP file), the third condition is just ignored. The same holds for the arguments.4 During the solving
process, if there is a model, these slots are filled-in by some concrete values, giving a concrete goal.
Overall, support for finding under-specified deducts is very simple. The current implementation finds
one possible deduct, but it can be extended to list all possible deducts, similarly as for abducts (as
explained in Section 3).
Example 3 For Problem 4 from Section 1, Larus produces the deduct “EF ∥ GH”. The proof obtained
with such deduct is presented in Appendix 7.4.
5
Hints and Completing Proofs
Informal proofs, for instance from textbooks, are often partial and incomplete. They may even provide
only a part of a full proof, or some instructions like for filling gaps by analogy. Reconstructing proofs
using such hints is very important task, as discussed by Gowers and Hales [13]: “One dream was to
develop an automated assistant that would function at the level of a helpful graduate student. The senior
mathematician would suggest the main lines of the proof, and the automated grad student would fill in
the details.”
Completing Proofs in Synthetic Euclidean Geometry.
In the context of geometry, completing proofs
could be interesting either as a way to render the formalization process simpler (automation would bring
in all the details that are overlooked in pen and paper proofs), or as a tool working behind the scene
for providing guidance for what could be the next step in the proof. This objective has been studied
by Richard et al. [24]. In geometry, hints can also be based on some observations from an associated
illustration.
Completing Proofs using Larus.
Larus can be instructed to look for a proof of a given conjecture
(also possibly only partially specified) meeting some conditions (that we call “hints”) [14]. Therefore,
Larus can try, for instance, to reconstruct a proof given only in outline (like proofs in textbooks). Larus
use hints in a much more general way than just splitting the problem into sub-problems: for instance,
some hint may be used in just one proof branch and cannot be proved itself. Hints do not have to be
ordered (one can ask for a proof using X and Y in no particular order), they can be vague, imposed only
by partial constraints (“find a proof that uses this particular predicate symbol”, or “find a proof using
some specific axiom”, without the way it is instantiated, etc.).
Completing proofs in Larus is supported similarly as for abducts and under-specified goals – by
modifying the corresponding constraints. The main difference is that abducts and incomplete goals
are under-specified, so some constraints have to be omitted, while partial proof introduce additional
constraints, on top of the common constraints that must be met by all proof steps. For expressing hints,
we slightly extended the language TPTP/fof to allow a simple but still quite expressible semantics. Some
kinds of hints (not all) are illustrated below.
4Actually, underspecified arguments can be also handled using existential quantification.
Gonzalez & Janiˇci´c & Narboux
29
fof(hintname0, hint, r(_,_), _ , _).
fof(hintname1, hint, r(_,_), 5 , _).
fof(hintname2, hint, _, 5, ax2(_,_)).
The first hint specifies that some proof step will have an atom of the form r(...,...). The second hint
specifies that the 5th proof step will have atom of the form r(...,...). The third hint specifies that in the
5th proof step the axiom ax2 is applied.
Example 4 For Problem 5 from Section 1, Larus was able to find a proof around 20% faster with a
suitable hint presented in Appendix 7.5.
6
Conclusion and Future Work
In this paper we have shown how a prover using the “theorem proving as constraint solving” paradigm
can be extended such that it can complete partially specified conjectures and partially specified proofs.
This extension is simple, and the implementation update is very small. The completion algorithm is uni-
form, since all three completion tasks (completing assumptions, completing goals, completing proofs) are
handled in the same spirit – in terms of adding or deleting some constraints. To our knowledge, this ap-
proach is new, and we are not aware of other systems that can address all three sorts of completion tasks.
The presented approach is flexible as different variations of completion tasks can be supported. The
strength of this approach is also that it can generate both proofs that are human-readable and machine-
checkable. The proposed framework has two main limitations. First, in current stage, it can deal only
with coherent logic, hence the theories cannot involve function symbols, which excludes geometry proof
that use (non-trivial) arithmetic. Second, the framework cannot deal with conjectures whose proofs are
long (say, longer than 50 proof steps).
To our knowledge, there is only one other approach in which some kind of proof is encoded, and
reconstructed from a model for the corresponding set of constraints – the approach in which rigid con-
nection tableaux are encoded as SAT and SMT instances [9, 5, 18]. However, in this line of research,
neither machine verifiable or readable proofs, nor any of completion tasks are considered.
The presented work can be extended in several directions. One of our goals is to use our framework to
help transfer geometry knowledge from informal sources to proof assistants and between proof assistants,
while keeping its high-level structure. In informal sources, statements of theorems may be incomplete,
while proofs may be given just in outline. Still, using our approach such contents can be, at least in some
cases, completed and turned into a verifiable form. For transferring knowledge from a proof assistant,
one would need to go into its specifics, but only to grab (some) proof steps and make hints out of them.
We are still to explore these ideas on a larger scale, like one geometry textbook. In the same spirit as
the work proposed by Jiang et al. [15], our approach could be combined with large language models
to perform automatic formalization by extracting data from natural language proofs. More specific to
abduction, we are planning to make an in-depth comparison (both qualitative and quantitative) of our
tool to other tools for generating abducts.
Acknowledgement.
The work related to this paper has been partially supported by the European Cost
project CA20111 EUROProofNet. The second author has been partially supported by the Ministry of
Science of Serbia contract 451-03-47/2023-01/200104.
30
Automated Completion of Statements and Proofs in Synthetic Geometry
References
[1] Aliseda Atocha (2006): ABDUCTIVE REASONING. Synthese Library 330, Kluwer Academic Publishers,
Dordrecht, doi:10.1007/1-4020-3907-7.
[2] Haniel Barbosa, Chantal Keller, Andrew Reynolds, Arjun Viswanathan, Cesare Tinelli & Clark Barrett
(2023): An Interactive SMT Tactic in Coq using Abductive Reasoning.
In: EPiC Series in Computing,
94, EasyChair, pp. 11–22, doi:10.29007/432m. ISSN: 2398-7340.
[3] Michael Beeson, Julien Narboux & Freek Wiedijk (2019): Proof-checking Euclid. Annals of Mathematics
and Artificial Intelligence 85(2-4), pp. 213–257, doi:10.1007/s10472-018-9606-x.
[4] Marc Bezem & Thierry Coquand (2005): Automating Coherent Logic. In Geoff Sutcliffe & Andrei Voronkov,
editors: 12th International Conference on Logic for Programming, Artificial Intelligence, and Reasoning —
LPAR 2005, Lecture Notes in Computer Science 3835, Springer, pp. 246–260, doi:10.1007/11591191 18.
[5] Jeremy Bongio, Cyrus Katrak, Hai Lin, Christopher Lynch & Ralph Eric McGregor (2008):
En-
coding First Order Proofs in SMT.
Electron. Notes Theor. Comput. Sci. 198(2),
pp. 71–84,
doi:10.1016/j.entcs.2008.04.081.
[6] Gabriel Braun & Julien Narboux (2012): From Tarski to Hilbert. In Tetsuo Ida & Jacques Fleuriot, ed-
itors: Post-proceedings of Automated Deduction in Geometry 2012, LNCS 7993, Springer, pp. 89–109,
doi:10.1007/978-3-642-40672-0 7.
[7] XueFeng Chen & DingKang Wang (2004): The Projection of Quasi Variety and Its Application on Geo-
metric Theorem Proving and Formula Deduction. In Automated Deduction in Geometry, 4th International
Workshop, ADG 2002, Lecture Notes in Computer Science 2930, Springer, pp. 21–30, doi:10.1007/978-3-
540-24616-9 2.
[8] Marc Denecker & Antonis C. Kakas (2002): Abduction in Logic Programming. In Computational Logic:
Logic Programming and Beyond, Essays in Honour of Robert A. Kowalski, Part I, Lecture Notes in Computer
Science 2407, Springer, pp. 402–436, doi:10.1007/3-540-45628-7 16.
[9] Todd Deshane, Wenjin Hu, Patty Jablonski, Hai Lin, Christopher Lynch & Ralph Eric McGregor (2007):
Encoding First Order Proofs in SAT. In Automated Deduction - CADE-21, 21st International Conference on
Automated Deduction, Lecture Notes in Computer Science 4603, Springer, pp. 476–491, doi:10.1007/978-
3-540-73595-3 35.
[10] Isil Dillig & Thomas Dillig (2013): Explain: A Tool for Performing Abductive Inference. In Computer Aided
Verification, Lecture Notes in Computer Science, Springer, pp. 684–689, doi:10.1007/978-3-642-39799-8 -
46.
[11] Viviane Durand-Guerrier, Paolo Boero, Nadia Douek, Susanna S. Epp & Denis Tanguay (2012): Examining
the Role of Logic in Teaching Proof. In Proof and Proving in Mathematics Education, New ICMI Study
Series 15, Springer, pp. 369–389, doi:10.1007/978-94-007-2129-6 16.
[12] Roy Dyckhoff & Sara Negri (2015): Geometrization of first-order logic. The Bulletin of Symbolic Logic 21,
pp. 123–163, doi:10.1017/bsl.2015.7.
[13] Thomas Hales (2019): An argument for controlled natural languages in mathematics. Available at https:
//jiggerwit.wordpress.com/2019/06/20/an-argument-for-controlled-natural-languages
-in-mathematics/.
[14] Predrag Janiˇci´c & Julien Narboux (2022): Theorem Proving as Constraint Solving with Coherent Logic.
Journal of Automated Reasoning 66(4), pp. 689–746, doi:10.1007/s10817-022-09629-z.
[15] Albert Q. Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timoth´ee Lacroix,
Yuhuai Wu & Guillaume Lample (2023): Draft, Sketch, and Prove: Guiding Formal Theorem Provers with
Informal Proofs, doi:10.48550/arXiv.2210.12283. ArXiv:2210.12283 [cs].
[16] Laura Kov´acs & Andrei Voronkov (2013): First-Order Theorem Proving and Vampire. In Computer Aided
Verification - 25th International Conference, CAV 2013, Lecture Notes in Computer Science 8044, Springer,
pp. 1–35, doi:10.1007/978-3-642-39799-8 1.
Gonzalez & Janiˇci´c & Narboux
31
[17] P. Marquis (1991): Extending abduction from propositional to first-order logic. In Fundamentals of Artificial
Intelligence Research, Springer, doi:10.1007/3-540-54507-7 12.
[18] Ralph Eric McGregor (2011): Automated Theorem Proving Using SAT. PhD Thesis, Clarkson University.
Available at https://search.proquest.com/openview/b87467cab0987f591010cf19dc554fa3/1?
pq-origsite=gscholar&cbl=18750&diss=y.
[19] Julien Narboux & Viviane Durand-Guerrier (2022): Combining pencil/paper proofs and formal proofs, a
challenge for Artificial Intelligence and mathematics education. In: Mathematics Education in the Age of
Artificial Intelligence, Mathematics Education in the Digital Era 17, Springer, doi:10.1007/978-3-030-86909-
0 8.
[20] Charles Peirce (1932): Collected papers of Charles Sanders Peirce. Belknap Press.
[21] Andrew Polonsky (2011): Proofs, Types and Lambda Calculus. Ph.D. thesis, University of Bergen.
[22] T. Recio & M. P. V´elez (1999): Automatic Discovery of Theorems in Elementary Geometry. J. Autom.
Reason. 23(1), pp. 63–82, doi:10.1023/A:1006135322108.
[23] Andrew Reynolds, Haniel Barbosa, Daniel Larraz & Cesare Tinelli (2020): Scalable Algorithms for Ab-
duction via Enumerative Syntax-Guided Synthesis.
In Automated Reasoning - 10th International Joint
Conference, IJCAR 2020, Part I, Lecture Notes in Computer Science 12166, Springer, pp. 141–160,
doi:10.1007/978-3-030-51074-9 9.
[24] Philippe R. Richard, Josep Maria Fortuny, Michel Gagnon, Nicolas Leduc, Eloi Puertas & Mich`ele Tessier-
Baillargeon (2011): Didactic and theoretical-based perspectives in the experimental development of an in-
telligent tutorial system for the learning of geometry. ZDM 43(3), pp. 425–439, doi:10.1007/s11858-011-
0320-y.
[25] Alessandra Russo & Bashar Nuseibeh (2001): On The Use Of Logical Abduction In Software Engineering.
In Handbook of Software Engineering and Knowledge Engineering, doi:10.1142/9789812389718 0037.
[26] Wolfram Schwabh¨auser, Wanda Szmielew & Alfred Tarski (1983): Metamathematische Methoden in der
Geometrie. Springer. doi:10.1007/978-3-642-69418-9.
[27] Sana Stojanovi´c, Julien Narboux, Marc Bezem & Predrag Janiˇci´c (2014): A Vernacular for Coherent Logic.
In Intelligent Computer Mathematics, Lecture Notes in Computer Science 8543, Springer, pp. 388–403,
doi:10.1007/978-3-319-08434-3 28.
[28] Wen-Tsun Wu (1978): On the Decision Problem and the Mechanization of Theorem-Proving in Elementary
Geometry. 21, Scientia Sinica, pp. 157–179.
7
Appendix
In this appendix, we provide a complete list of lemmas and axioms (in coherent logic form) used in
our examples, and the results obtained using Larus. The results were obtained on a PC computer with
Intel(R) Core(TM) i7-8565U CPU @ 1.80GHz processor running under Linux (the time spent should
give just a general picture of the efficiency of the system).
7.1
Problem 1: Varignon’s Theorem
The TPTP file used for Problem 1 is the following:
fof(triangle_mid_par_strict, axiom, (! [A, B, C, P, Q] : ( ((~ col(A,B,C)) & midpoint(B,P
,C) & midpoint(A,Q,C)) => par(A,B,Q,P)))).
fof(lemma_par_trans, axiom, (! [A, B, C, D, E, F] : ((par(A,B,C,D) & par(C,D,E,F) & (~col
(A,B,E))) => par(A,B,E,F)))).
32
Automated Completion of Statements and Proofs in Synthetic Geometry
fof(defparallelogram2,axiom, (! [A,B,C,D] : ((par(A,B,C,D) & par(A,D,B,C)) => ((pG(A,B,C,
D)))))).
fof(lemma_parallelNC,axiom, (! [A,B,C,D] : ((par(A,B,C,D)) => ((~ (col(A,B,C)) & ~ (col(A
,C,D)) & ~ (col(B,C,D)) & ~ (col(A,B,D))))))).
fof(lemma_parallelflip,axiom, (! [A,B,C,D] : ((par(A,B,C,D)) => ((par(B,A,C,D) & par(A,B,
D,C) & par(B,A,D,C)))))).
fof(lemma_parallelsymmetric,axiom, (! [A,B,C,D] : ((par(A,B,C,D)) => ((par(C,D,A,B)))))).
fof(midpoint_sym, axiom, (! [A, B, I] : (midpoint(A,I,B) => midpoint(B,I,A)))).
fof(lemma_tP_trans, axiom,
(! [A, B, C, D, E, F] : ((tP(A,B,C,D) & tP(C,D,E,F)) => tP(A,
B,E,F)))).
fof(th_varignon,conjecture,(! [A,B,C,D,E,F,G,H] : (( (~(col(B,D,A))) & (~(col(B,D,C))) &
(~(col(A,C,B))) & (~(col(A,C,D))) & (~ (col(E,F,G))) & midpoint(A,E,B) & midpoint(B,F
,C) & midpoint(C,G,D) & midpoint(A,H,D)) => pG(E,F,G,H) ))).
If Larus is invoked as: ./larus -l100 -m8 (-l100 means the time limit is 100s, -m8 means that
we look for a proof with 8 or fewer steps), it produces the following proof in 2s:
Consider arbitrary a, b, c, d, e, f, g, h such that:
• ¬col(b,d,a),
• ¬col(b,d,c),
• ¬col(a,c,b),
• ¬col(a,c,d),
• ¬col(e, f,g),
• b ̸= d,
• a ̸= c,
• midpoint(a,e,b),
• midpoint(b, f,c),
• midpoint(c,g,d),
• midpoint(a,h,d).
It should be proved that pG(e, f,g,h).
1. par(a,c,h,g) (by MP, from ¬col(a,c,d), midpoint(c,g,d), midpoint(a,h,d) using axiom triangle mid par strict; instantiation:
A 7→ a, B 7→ c, C 7→ d, P 7→ g, Q 7→ h)
2. par(b,d, f,g) (by MP, from ¬col(b,d,c), midpoint(c,g,d), midpoint(b, f,c) using axiom triangle mid par strict; instantiation:
A 7→ b, B 7→ d, C 7→ c, P 7→ g, Q 7→ f)
3. par(a,c,e, f) (by MP, from ¬col(a,c,b), midpoint(b, f,c), midpoint(a,e,b) using axiom triangle mid par strict; instantiation:
A 7→ a, B 7→ c, C 7→ b, P 7→ f, Q 7→ e)
4. par(b,d,e,h) (by MP, from ¬col(b,d,a), midpoint(a,h,d), midpoint(a,e,b) using axiom triangle mid par strict; instantiation:
A 7→ b, B 7→ d, C 7→ a, P 7→ h, Q 7→ e)
5. par(e, f,g,h) (by MP, from par(a,c,e, f), par(a,c,h,g), ¬col(e, f,g) using axiom lemma par trans; instantiation: A 7→ e, B 7→
f, C 7→ a, D 7→ c, E 7→ g, F 7→ h)
Gonzalez & Janiˇci´c & Narboux
33
6. par(f,g,h,e) (by MP, from par(b,d, f,g), par(b,d,e,h), par(e, f,g,h) using axiom lemma par trans; instantiation: A 7→ f, B
7→ g, C 7→ d, D 7→ b, E 7→ h, F 7→ e)
7. pG(e, f,g,h) (by MP, from par(e, f,g,h), par( f,g,h,e) using axiom defparallelogram2; instantiation: A 7→ e, B 7→ f, C 7→ g,
D 7→ h)
8. Proved by assumption! (by QEDas)
7.2
Problem 2: First Inverse Problem
The list of axioms used for the first inverse problem (Problem 2) is the same as in Section 7.1. Only the
conjecture is different – the assumption midpoint(A,H,D) is ommitted:
fof(th_varignon,conjecture,(! [A,B,C,D,E,F,G,H] : (( (~(col(B,D,A))) & (~(col(B,D,C))) &
(~(col(A,C,B))) & (~(col(A,C,D))) & (~ (col(E,F,G))) & (B != D) & (A != C) & midpoint
(A,E,B) & midpoint(B,F,C) & midpoint(C,G,D)) => pG(E,F,G,H) ))).
If Larus is invoked as: ./larus -l100 -m8 -b1 (-l100 means the time limit is 100s, -m8 means
that we look for a proof with 8 or fewer steps, -b1 means that we look for one atomic formula as an
abduct), it finds a first consistent abduct (after two inconsistent ones) and produces the following human-
readable proof in 3.26 seconds (the abduct found is highlighted):
Consider arbitrary a, b, c, d, e, f, g, h such that:
• ¬col(b,d,a),
• ¬col(b,d,c),
• ¬col(a,c,b),
• ¬col(a,c,d),
• ¬col(e, f,g),
• b ̸= d,
• a ̸= c,
• midpoint(a,e,b),
• midpoint(b, f,c),
• midpoint(c,g,d).
It should be proved that pG(e, f,g,h).
Abducts found:
• midpoint(d,h,a)
1. par(a,c,e, f) (by MP, from ¬col(a,c,b), midpoint(b, f,c), midpoint(a,e,b) using axiom triangle mid par strict; instantiation:
A 7→ a, B 7→ c, C 7→ b, P 7→ f, Q 7→ e)
2. par(b,d, f,g) (by MP, from ¬col(b,d,c), midpoint(c,g,d), midpoint(b, f,c) using axiom triangle mid par strict; instantiation:
A 7→ b, B 7→ d, C 7→ c, P 7→ g, Q 7→ f)
34
Automated Completion of Statements and Proofs in Synthetic Geometry
3. par(b,d,e,h) (by MP, from ¬col(b,d,a), midpoint(d,h,a), midpoint(a,e,b) using axiom triangle mid par strict; instantiation:
A 7→ b, B 7→ d, C 7→ a, P 7→ h, Q 7→ e)
4. par(a,c,h,g) (by MP, from ¬col(a,c,d), midpoint(c,g,d), midpoint(d,h,a) using axiom triangle mid par strict; instantiation:
A 7→ a, B 7→ c, C 7→ d, P 7→ g, Q 7→ h)
5. par(e, f,g,h) (by MP, from par(a,c,e, f), par(a,c,h,g), ¬col(e, f,g) using axiom lemma par trans; instantiation: A 7→ e, B 7→
f, C 7→ a, D 7→ c, E 7→ g, F 7→ h)
6. par(e,h,g, f) (by MP, from par(b,d,e,h), par(b,d, f,g), par(e, f,g,h) using axiom lemma par trans; instantiation: A 7→ e, B
7→ h, C 7→ b, D 7→ d, E 7→ g, F 7→ f)
7. pG(e, f,g,h) (by MP, from par(e, f,g,h), par(e,h,g, f) using axiom defparallelogram2; instantiation: A 7→ e, B 7→ f, C 7→ g,
D 7→ h)
8. Proved by assumption! (by QEDas)
7.3
Problem 3: Second Inverse Problem
The list of axioms used for the second inverse problem (Problem 3) is the same as in section 7.1, extended
with the following axioms.
fof(defmidpoint,axiom, (! [A,B,C] : ((midpoint(A,B,C)) => ((betS(A,B,C) & cong(A,B,B,C)))
))).
fof(defmidpoint2,axiom, (! [A,B,C] : ((betS(A,B,C) & cong(A,B,B,C)) => ((midpoint(A,B,C))
)))).
fof(midpoint_NC, axiom, (! [A, B, I] : ((midpoint(A,I,B) & (A != B)) => ( (A != I) & ( B
!= I))))).
fof(defrectangle,axiom, (! [A,B,C,D] : ((rectangle(A,B,C,D)) => ((pG(A,B,C,D) & per(A,B,C
) & per(B,C,D) & per(C,D,A) & per(D,A,B)))))).
fof(defrectangle2a,axiom, (! [A,B,C,D] : ((pG(A,B,C,D) & per(A,B,C)) => rectangle(A,B,C,D
)))).
fof(defrectangle2b,axiom, (! [A,B,C,D] : ((pG(A,B,C,D) & per(B,C,D)) => rectangle(A,B,C,D
)))).
fof(defrectangle2c,axiom, (! [A,B,C,D] : ((pG(A,B,C,D) & per(C,D,A)) => rectangle(A,B,C,D
)))).
fof(defrectangle2d,axiom, (! [A,B,C,D] : ((pG(A,B,C,D) & per(D,A,B)) => rectangle(A,B,C,D
)))).
fof(defrectangle2e,axiom, (! [A,B,C,D] : ((per(A,B,C) & per(B,C,D) & per(C,D,A) & per(D,A
,B)) => rectangle(A,B,C,D)))).
%fof(defrectangle3a,axiom, (! [A,B,C,D] : (? [X] : ((rectangle(A,B,C,D)) => cong(A,C,B,D)
& midpoint(A,X,C) & midpoint(B,X,D))))).
fof(defrectangle3b,axiom, (! [A,B,C,D,X] : ((cong(A,C,B,D) & midpoint(A,X,C) & midpoint(B
,X,D)) => rectangle(A,B,C,D)))).
fof(defrectangle4a,axiom, (! [A,B,C,D] : ((rectangle(A,B,C,D)) => (pG(A,B,C,D) & cong(A,C
,B,D))))).
fof(defrectangle4b,axiom, (! [A,B,C,D] : ((pG(A,B,C,D) & cong(A,C,B,D)) => rectangle(A,B,
C,D)))).
fof(lemma_8_2,axiom, (! [A,B,C] : ((per(A,B,C)) => ((per(C,B,A)))))).
fof(varignon_th,axiom,(! [A,B,C,D,E,F,G,H] : (( (~(col(B,D,A))) & (~(col(B,D,C))) & (~(
col(A,C,B))) & (~(col(A,C,D))) & (~ (col(G,F,E))) & (B != D) & (A != C) & midpoint(A,
E,B) & midpoint(B,F,C) & midpoint(C,G,D) & midpoint(A,H,D)) => pG(E,F,G,H) ))).
Gonzalez & Janiˇci´c & Narboux
35
The conjecture is also different – the goal is to find under which assumption the quadrilateral EFGH
is a rectangle.
fof(th_varignon_rect,conjecture,(! [A,B,C,D,E,F,G,H] : (( (~(col(B,D,A))) & (~(col(B,D,C)
)) & (~(col(A,C,B))) & (~(col(A,C,D))) & (~ (col(G,F,E))) & (B != D) & (A != C) &
midpoint(A,E,B) & midpoint(B,F,C) & midpoint(C,G,D) & midpoint(A,H,D)) => rectangle(E
,F,G,H) ))).
If Larus is invoked as: ./larus -l100 -m8 -b1, it produces the following human-readable proof
in 14s (the abduct found is highlighted):
Consider arbitrary a, b, c, d, e, f, g, h such that:
• ¬col(b,d,a),
• ¬col(b,d,c),
• ¬col(a,c,b),
• ¬col(a,c,d),
• ¬col(f,g,e),
• b ̸= d,
• a ̸= c,
• midpoint(a,e,b),
• midpoint(b, f,c),
• midpoint(c,g,d),
• midpoint(a,h,d).
It should be proved that rectangle(e, f,g,h).
Abducts found:
• cong(e,g,h, f)
1. midpoint(b,e,a) (by MP, from midpoint(a,e,b), midpoint(a,e,b) using axiom defmidpoint2; instantiation: A 7→ b, B 7→ e, C
7→ a)
2. pG(e, f,g,h) (by MP, from ¬col(b,d,a), ¬col(b,d,c), ¬col(a,c,b), ¬col(a,c,d), ¬col( f,g,e), b ̸= d, a ̸= c, midpoint(b,e,a),
midpoint(b, f,c), midpoint(c,g,d), midpoint(a,h,d) using axiom varignon th; instantiation: A 7→ a, B 7→ b, C 7→ c, D 7→ d, I 7→ e,
J 7→ f, K 7→ g, L 7→ h)
3. rectangle(e, f,g,h) (by MP, from pG(e, f,g,h), cong(e,g,h, f) using axiom defrectangle4b; instantiation: A 7→ e, B 7→ f, C
7→ g, D 7→ h)
4. rectangle(e, f,g,h) (by MP, from rectangle(e, f,g,h), rectangle(e, f,g,h), rectangle(e, f,g,h), rectangle(e, f,g,h) using
axiom defrectangle2e; instantiation: A 7→ e, B 7→ f, C 7→ g, D 7→ h)
5. Proved by assumption! (by QEDas)
36
Automated Completion of Statements and Proofs in Synthetic Geometry
7.4
Problem 4: Partially Specified Goal
The list of axioms used for Problem 4 is the same as presented in Section 7.1. Only the conjecture is
different: the goal does not have the predicate symbol specified:
fof(th_varignon,conjecture,(! [A,B,C,D,E,F,G,H] : (( (~(col(B,D,A))) & (~(col(B,D,C))) &
(~(col(A,C,B))) & (~(col(A,C,D))) & (~ (col(E,F,G))) & (B != D) & (A != C) & midpoint
(A,E,B) & midpoint(B,F,C) & midpoint(C,G,D) & midpoint(A,H,D)) => _(E,F,G,H) ))).
If Larus is invoked as ./larus -l100 -m8, it produces the following human-readable proof (for the
goal par(e, f,g,h), highlighted in the proof) in 2s:
Consider arbitrary a, b, c, d, e, f, g, h such that:
• ¬col(b,d,a),
• ¬col(b,d,c),
• ¬col(a,c,b),
• ¬col(a,c,d),
• ¬col(e, f,g),
• b ̸= d,
• a ̸= c,
• midpoint(a,e,b),
• midpoint(b, f,c),
• midpoint(c,g,d),
• midpoint(a,h,d).
It should be proved that (e, f,g,h).
1. par(a,c,e, f) (by MP, from ¬col(a,c,b), midpoint(b, f,c), midpoint(a,e,b) using axiom triangle mid par strict; instantiation:
A 7→ a, B 7→ c, C 7→ b, P 7→ f, Q 7→ e)
2. par(a,c,h,g) (by MP, from ¬col(a,c,d), midpoint(c,g,d), midpoint(a,h,d) using axiom triangle mid par strict; instantiation:
A 7→ a, B 7→ c, C 7→ d, P 7→ g, Q 7→ h)
3.
par(e, f,g,h)
(by MP, from par(a,c,e, f), par(a,c,h,g), ¬col(e, f,g) using axiom lemma par trans; instantiation: A
7→ e, B 7→ f, C 7→ a, D 7→ c, E 7→ g, F 7→ h)
4. Proved by assumption! (by QEDas)
7.5
Problem 5: Partially Specified Proof
The list of axioms used for Problem 5 is as presented in Section 7.3 (with the axiom defrectangle3a
deleted). The conjecture is the same plus the abduct as an assumption, but we add the following hint:
Gonzalez & Janiˇci´c & Narboux
37
fof(hint1,hint,_,_,defrectangle4b(4,5,6,7)).
If Larus is invoked as ./larus -l100 -m8, it produces the same proof as in Section 7.3 in 4s, while if
the hint is omitted, it takes 5s.
"
"Proof scripts help mathematicians conduct proofs, however, they remain overly technical and difficult to reuse. We present a framework for conducting automated proof transformations that take the form of a post-processing step after a proof has been completed.","Coq proof assistants are popular among mathematicians, but formal proofs are highly complex and difficult to reuse. We propose a tool to post-process proof scripts for improvement, clarity, and reusability.","Previous work has focused on fixing issues after they occur. Our approach aims to improve proof scripts to reduce the likelihood of issues, even after updates.nannannan","We transform large proof scripts into single-step proof scripts by identifying and combining tactics using tacticals. This is achieved by using Coq's serialisation mechanism, anonymous pipes, and tactics aggregation. We handle commands and tactics by assuming specific naming conventions.nan","We successfully transformed proof scripts of the Coq Standard Library, such as Cantor.v2 and GeoCoq library files. The tool also refactored proof scripts generated by our prover for projective incidence geometry.",We developed a successful tool that transforms arbitrary large proof scripts into single-step Coq tactic proof scripts. This tool was tested on significant library files from the Coq ecosystem. We plan to extend it to integrate tactic languages and design new proof script transformations to enhance reliability and maintainability.,Towards Automatic Transformations of Coq Proof Scripts,Nicolas Magaud,"P. Quaresma and Z. Kovács (Ed.): Automated Deduction
in Geometry 2023 (ADG 2023).
EPTCS 398, 2024, pp. 4–10, doi:10.4204/EPTCS.398.4
Towards Automatic Transformations of Coq Proof Scripts
Nicolas Magaud
Lab. ICube UMR 7357 CNRS Université de Strasbourg, France
magaud@unistra.fr
Proof assistants like Coq are increasingly popular to help mathematicians carry out proofs of the
results they conjecture. However, formal proofs remain highly technical and are especially difficult
to reuse. In this paper, we present a framework to carry out a posteriori script transformations. These
transformations are meant to be applied as an automated post-processing step, once the proof has
been completed. As an example, we present a transformation which takes an arbitrary large proof
script and produces an equivalent single-line proof script, which can be executed by Coq in one single
step. Other applications, such as fully expanding a proof script (for debugging purposes), removing
all named hypotheses, etc. could be developed within this framework. We apply our tool to various
Coq proof scripts, including some from the GeoCoq library.
1
Motivations
Proof assistants like Coq [1, 4] are increasingly popular to help mathematicians carry out proofs of the
results they conjecture. However, formal proofs remain highly technical and are especially difficult to
reuse. Once the proof effort is done, the proof scripts are left as they are and they often break when
upgrading to a more recent version of the prover. To reduce the burden of maintaining the proof scripts
of Coq, we propose a tool to post-process the proof scripts to make them cleaner and easier to reuse.
The first transformation that we focused on consists in compacting a several-step proof script into a
single-step proof script. Even though our framework can be used to implement other proof script trans-
formations, this one is of special interest to us. Indeed, we recently designed a prover for projective
incidence geometry [3, 12] which relies on the concept of rank to carry out proofs of geometric theorems
such as Desargues or Dandelin-Gallucci automatically. This prover produces a trace (a large Coq proof
script containing several statements and their proofs). We hope to use the proof transformation tool to
shape up the automatically generated proofs and make them easier to reuse and integrate in larger proof
repositories.
More generally, proof maintenance and reuse tools have been studied extensively by Talia Ringer et
al. [11, 10]. Contrary to our approach, their tools aim at fixing the issues when they occur. In our setting,
we think it is better to try and improve the proof scripts so that they are less likely to break, even after
several years and numerous updates of the components.
Outline of the paper
The paper is organized as follows. In Sect. 2, we present a simple example of a
proof script transformation. In Sect. 3, we describe the implementation of our tool as well as the future
extensions we currently develop. In Sect. 4, we present some concluding remarks and the perspectives
of this work.
N. Magaud
5
Lemma foo : forall A B C : Prop, A ∨ (B ∧ C) → (A∨ B)∧ (A∨ C).
Proof.
intros; destruct H.
split.
left; assumption.
left; assumption.
destruct H.
split.
right; assumption.
right; assumption.
Qed.
Proof.
intros; destruct H;
[ split;
[ left; assumption
| left; assumption ]
| destruct H ;
split;
[ right; assumption
| right; assumption ] ].
Qed.
Figure 1: A user-written script (left) and the equivalent single-step script (right)
2
Transforming Large Proof Scripts into One-line Scripts
The Coq tactic language [5] features tacticals to execute some tactics in a sequence tac1;tac2;tac3
or to try and execute different tactics on the same goal solve [ tac1 | tac2 | tac3 ]. Moreover
these tacticals can be combined. E.g. tac0 ; [tac1 | tac2 | tac3] runs the first tactic tac0 which
should yield 3 subgoals. The first one is solved using tac1, the second one using tac2 and the third one
using tac3. Once a proof script is written (as several steps) by the user, we can use these tacticals to
build an equivalent proof script, which can be executed in a single step.
Let us consider a simple example, proving the distributivity of the connective or (∨) over the con-
nective and (∧) as shown in the statement of figure 1.
The left-hand side presents the proof script that one may expect from a master student, factorizing
some parts but still decomposing the reasoning in several steps. On the right-hand side, we propose a
one-line script to carry out exactly the same proof.
In Coq, writing directly the right-hand side is almost impossible, whereas it is fairly easy to generate
it automatically from the left-hand side. In the Coq standard library, several lemmas are proved using a
single one-line tactic. The main advantage is that it provides concise and structured proofs but it has the
drawback that, when something goes wrong, it is hard to debug and fix it.
3
Experiments, Limitations and Results
3.1
Implementation
We choose to implement our tool in OCaml, using the serialisation mechanism serapi [6] developed by
Emilio Gallego Arias for communication with the Coq proof assistant. Our tool uses anonymous pipes
to communicate with serapi, which itself sends requests to Coq and retrieves the answers. Commands
are kept as in the input file. Tactics are aggregated using tacticals such as ;, [ and ]. At each step of the
proof, we compare the current number of subgoals to the number of subgoals right before the execution
of the current tactic. If it is the same, we simply concatenate the tactics with a ; between them. If the
number of goals increases, we open a square bracket [ and push into the stack the previous number of
goals. Each time a goal is solved, we check whether some goals remain to be proved at this level. If yes,
we add another ; and then focus on the next subgoal. If there are no more subgoals at this level, we pop
6
Towards Automatic Transformations of Coq Proof Scripts
the 0 from the top of the stack, thus closing the current level with a ] and carry on with subgoals of the
previous level.
The source code1 as well as some examples are freely available online. It is developped using Coq
8.17.0 and the corresponding serapi version 8.17.0+0.17.0.
3.2
Limitations
So far, commands and tactics are told apart simply by assuming commands start with a capital letter
[A-Z] and tactics with a small letter [a-z]. This convention is well-known in Coq, however in some
developments (e.g. GeoCoq), some ad-hoc user tactics may start with a capital letter. Handling this
properly requires additional developments and is currently under way.
To make the transformation easier, a first phase could be added to our proof script transformer to
remove all commands which lay among the proof steps (e.g. Check, Print or Locate) and make sure
all tactics names start with a small letter.
Finally, Coq proof scripts can be structured using bullets (+, -, *) as well as curly brackets to identify
some subproofs. In addition, one can direct work on a goal which is not the current one using the 2:
tac. notation which performs the tactic tac on the second goal of the subgoals. We still need to devise
a way to deal properly with such partially-structured proof script.
3.3
Successful Transformations
In addition to our test suite examples, we consider more challenging proof scripts. We successfully
transformed a library file from the Standard Library of Coq: Cantor.v2 from the Arith library as well as
some large files from the GeoCoq library [2, 8] (e.g. orthocenter.v3). As the tool gets more mature, we
plan to transform more files, and we shall especially focus on the GeoCoq library which features several
different proof styles and thus shall allow us to evaluate the robustness of our tool.
3.4
Refactoring Proof Scripts Automatically Generated by our Prover for Projective In-
cidence Geometry
We recently developed a new way [3, 12], based on ranks, to automatically prove statements in projective
incidence geometry. Our approach works well but produces proof scripts which are very large and often
feature several auxiliary lemmas. Figure 2 presents a very simple example LABC, which is formally
proven by our tool but yields a fairly verbose proof script using one intermediate lemma LABCD (see
appendix A for details).
We plan to use our script transformation tool to refactor automatically generated proof scripts, in-
lining auxiliary lemmas and thus making proof scripts more concise and hopefully more readable for
humans.
3.5
Next steps
To fully evaluate the tool, we need to handle larger examples, outside of the standard library of Coq. The
next step consists in improving Coq options handling (e.g. -R) to our script transformation tool to tackle
other formal proof libraries.
1https://github.com/magaud/coq-lint
2https://github.com/coq/coq/blob/master/theories/Arith/Cantor.v
3https://github.com/GeoCoq/GeoCoq/blob/master/Highschool/orthocenter.v
N. Magaud
7
A
D
B
C
• Informal statement
Assume that ABD is a triangle,
Assume that C is a point on AD, such that C̸= A and C̸= D,
Then ABC is a triangle.
• Expressed using ranks
∀A,B,C,D : Point,
rk{A,D,B} = 3 → rk{A,C,D} = 2 →
rk{C,A} = 2 → rk{C,D} = 2 →
rk{A,C,B} = 3.
Figure 2: An example of a statement in projective geometry, formalized using ranks
We also plan to propose the reciprocal script transformation, turning a single-step proof script into a
more detailed (easier to debug) proof script. This could especially be useful when porting formal proofs
from one version of Coq to the next one.
Other applications of interest could be to remove the names of all variables or hypotheses from the
scripts, or at least to force them to be explicitly introduced. The script snippet intros; apply H could
be replaced by a more precise one intros n p H; apply H. This way, we could ensure that the proofs
are not broken when the names of automatic variables change. From a reliability point of view, it would
be even better to use the tactic intros; assumption. Although its cost is higher (because we need to
search the correct hypothesis among all of them every time we run the tactic), it does not depend on some
arbitrary variable names.
Finally, regarding our current implementation, it would be interesting to benchmark the transforma-
tion to see whether transforming the whole standard library of Coq into single-step proof scripts could
improve the compilation time of this library.
4
Conclusions and perspectives
We build a proof script transformation tool, which transforms an arbitrary large proof script into a single-
step one-Coq-tactic proof script. This tool has been successfully experimented on some significant
library files from the Coq ecosystem.
This first example shows that the approach is sound and we plan to extend it to integrate tactic
languages such as ssreflect [7], Ltac2 [9] or Mtac [13] in our framework. In the longer term, we expect
to design some new proof script transformations and combine them in order to build more reliable proof
developments which can last longer and would be easier to maintain. Among these transformations,
we shall start with a mechanism to transform a proof script into a sequence of atomic proof steps (to
make debugging easier when the proof breaks). We may also study how to transform proofs carried
out automatically by their actual traces, avoiding recomputing the proof search each time the proof is
re-runned.
8
Towards Automatic Transformations of Coq Proof Scripts
References
[1] Bertot, Y., Castéran, P.: Interactive Theorem Proving and Program Development, Coq’Art : The Calculus
of Inductive Constructions. Texts in Theoretical Computer Science, An EATCS Series, Springer-Verlag,
Berlin/Heidelberg (May 2004), 469 pages
[2] Boutry, P., Gries, C., Narboux, J., Schreck, P.:
Parallel postulates and continuity axioms:
a
mechanized study in intuitionistic logic using Coq. Journal of Automated Reasoning p. 68 (2017).
https://doi.org/10.1007/s10817-017-9422-8
[3] Braun, D., Magaud, N., Schreck, P.:
Two new ways to formally prove dandelin-gallucci’s the-
orem. In:
Chyzak,
F.,
Labahn,
G. (eds.) ISSAC ’21:
International Symposium on Symbolic
and Algebraic Computation, Virtual Event, Russia, July 18-23, 2021. pp. 59–66. ACM (2021).
https://doi.org/10.1145/3452143.3465550
[4] Coq development team: The Coq Proof Assistant Reference Manual, Version 8.13.2. INRIA (2021), http:
//coq.inria.fr
[5] Delahaye, D.: A Tactic Language for the System Coq. In: Parigot, M., Voronkov, A. (eds.) Logic for Pro-
gramming and Automated Reasoning, 7th International Conference, LPAR 2000, Reunion Island, France,
November 11-12, 2000, Proceedings. Lecture Notes in Computer Science, vol. 1955, pp. 85–95. Springer
(2000). https://doi.org/10.1007/3-540-44404-1_7
[6] Gallego Arias, E.J.: SerAPI: Machine-Friendly, Data-Centric Serialization for Coq. Tech. rep., MINES Paris-
Tech (Oct 2016), https://hal-mines-paristech.archives-ouvertes.fr/hal-01384408
[7] Gonthier, G., Mahboubi, A.: An introduction to small scale reflection in Coq. J. Formaliz. Reason. 3(2),
95–152 (2010). https://doi.org/10.6092/issn.1972-5787/1979
[8] Narboux, J.: Mechanical Theorem Proving in Tarski’s geometry. In: Eugenio Roanes Lozano, F.B. (ed.)
Automated Deduction in Geometry 2006. LNCS, vol. 4869, pp. 139–156. Francisco Botana, Springer, Pon-
tevedra, Spain (Aug 2006). https://doi.org/10.1007/978-3-540-77356-6
[9] Pédrot, P.M.: Ltac2: Tactical Warfare. In: Krebbers, R., Sergey, I. (eds.) Proceedings of the CoqPL workshop
2019 (2019)
[10] Ringer, T., Palmskog, K., Sergey, I., Gligoric, M., Tatlock, Z.:
QED at large:
A survey of
engineering of formally verified software. Found. Trends Program. Lang. 5(2-3), 102–281 (2019).
https://doi.org/10.1561/2500000045
[11] Ringer, T., Yazdani, N., Leo, J., Grossman, D.: Adapting proof automation to adapt proofs. In: Andron-
ick, J., Felty, A.P. (eds.) Proceedings of the 7th ACM SIGPLAN International Conference on Certified
Programs and Proofs, CPP 2018, Los Angeles, CA, USA, January 8-9, 2018. pp. 115–129. ACM (2018).
https://doi.org/10.1145/3167094
[12] Schreck, P., Magaud, N., Braun, D.: Mechanization of incidence projective geometry in higher dimensions, a
combinatorial approach. In: Janicic, P., Kovács, Z. (eds.) Proceedings of the 13th International Conference on
Automated Deduction in Geometry, ADG 2021, Hagenberg, Austria/virtual, September 15-17, 2021. EPTCS,
vol. 352, pp. 77–90 (2021). https://doi.org/10.4204/EPTCS.352.8
[13] Ziliani, B., Dreyer, D., Krishnaswami, N.R., Nanevski, A., Vafeiadis, V.: Mtac: A monad for typed tactic
programming in coq. J. Funct. Program. 25 (2015). https://doi.org/10.1017/S0956796815000118
A
Proof Script for our basic example
Lemma LABCD : forall A B C D ,
rk(A:: C:: nil) = 2 → rk(A:: B:: D:: nil) = 3 →
rk(C:: D:: nil) = 2 → rk(A:: C:: D:: nil) = 2 →
rk(A:: B:: C:: D:: nil) = 3.
N. Magaud
9
Proof.
intros A B C D
HACeq HABDeq HCDeq HACDeq .
assert(HABCDm2 : rk(A:: B:: C:: D:: nil) >= 2).
{
assert(HACmtmp : rk(A:: C:: nil) >= 2)
by (solve_hyps_min HACeq HACm2).
assert(Hcomp : 2 <= 2) by (repeat constructor).
assert(Hincl : incl (A:: C:: nil) (A:: B:: C:: D:: nil))
by (repeat clear_all_rk;my_inO).
apply (rule_5 (A:: C:: nil) (A:: B:: C:: D:: nil) 2 2 HACmtmp Hcomp Hincl).
}
assert(HABCDm3 : rk(A:: B:: C:: D:: nil) >= 3).
{
assert(HABDmtmp : rk(A:: B:: D:: nil) >= 3)
by (solve_hyps_min HABDeq HABDm3).
assert(Hcomp : 3 <= 3)
by (repeat constructor).
assert(Hincl : incl (A:: B:: D:: nil) (A:: B:: C:: D:: nil))
by (repeat clear_all_rk;my_inO).
apply (
rule_5 (A:: B:: D:: nil) (A:: B:: C:: D:: nil) 3 3 HABDmtmp Hcomp Hincl
).
}
assert(HABCDM : rk(A:: B:: C:: D:: nil) <= 3)
by (solve_hyps_max HABCDeq HABCDM3).
assert(HABCDm : rk(A:: B:: C:: D:: nil) >= 1)
by (solve_hyps_min HABCDeq HABCDm1).
intuition.
Qed.
Lemma LABC : forall A B C D ,
rk(A:: C:: nil) = 2 → rk(A:: B:: D:: nil) = 3 →
rk(C:: D:: nil) = 2 → rk(A:: C:: D:: nil) = 2 →
rk(A:: B:: C:: nil) = 3.
Proof.
intros A B C D
HACeq HABDeq HCDeq HACDeq .
assert(HABCm2 : rk(A:: B:: C:: nil) >= 2).
{
assert(HACmtmp : rk(A:: C:: nil) >= 2)
by (solve_hyps_min HACeq HACm2).
assert(Hcomp : 2 <= 2)
by (repeat constructor).
assert(Hincl : incl (A:: C:: nil) (A:: B:: C:: nil))
by (repeat clear_all_rk;my_inO).
apply (
rule_5 (A:: C:: nil) (A:: B:: C:: nil) 2 2 HACmtmp Hcomp Hincl
).
}
assert(HABCm3 : rk(A:: B:: C:: nil) >= 3).
10
Towards Automatic Transformations of Coq Proof Scripts
{
assert(HACDMtmp : rk(A:: C:: D:: nil) <= 2)
by (solve_hyps_max HACDeq HACDM2).
assert(HABCDeq : rk(A:: B:: C:: D:: nil) = 3)
by
(apply LABCD with (A := A) (B := B) (C := C) (D := D) ; assumption).
assert(HABCDmtmp : rk(A:: B:: C:: D:: nil) >= 3)
by (solve_hyps_min HABCDeq HABCDm3).
assert(HACmtmp : rk(A:: C:: nil) >= 2)
by (solve_hyps_min HACeq HACm2).
assert( Hincl :
incl (A:: C:: nil)
(list_inter (A:: B:: C:: nil) (A:: C:: D:: nil)))
by (repeat clear_all_rk;my_inO).
assert( HT1 :
equivlist (A:: B:: C:: D:: nil)
(A:: B:: C:: A:: C:: D:: nil))
by (clear_all_rk;my_inO).
assert( HT2 :
equivlist (A:: B:: C:: A:: C:: D:: nil)
(( A:: B:: C:: nil) ++ (A:: C:: D:: nil))
) by (clear_all_rk;my_inO).
rewrite HT1 in HABCDmtmp;rewrite HT2 in HABCDmtmp.
apply (
rule_2
(A:: B:: C:: nil) (A:: C:: D:: nil) (A:: C:: nil)
3 2 2 HABCDmtmp HACmtmp HACDMtmp Hincl
).
}
assert(HABCM : rk(A:: B:: C:: nil) <= 3)
by (solve_hyps_max HABCeq HABCM3).
assert(HABCm : rk(A:: B:: C:: nil) >= 1)
by (solve_hyps_min HABCeq HABCm1).
intuition.
Qed.
"
"We consider Maxwell eigenvalue problems on uncertain shapes with perfectly conducting TESLA cavities being the driving example. Due to the shape uncertainty, the resulting eigenvalues and eigenmodes are also uncertain and it is well known that the eigenvalues may exhibit crossings or bifurcations under perturbation. We discuss how the shape uncertainties can be modelled using the domain mapping approach and how the deformation mapping can be expressed as coefficients in Maxwell’s equations. Using derivatives of these coefficients and derivatives of the eigenpairs, we follow a perturbation approach to compute approximations of mean and covariance of the eigenpairs. For small perturbations, these approximations are faster and more accurate than Monte Carlo or similar sampling-based strategies. Numerical experiments for a three-dimensional 9-cell TESLA cavity are presented.",Radio-frequency resonators are devices in which electromagnetic fields oscillate at gigahertz frequencies.,"nanIn [26], a linear representation of the first Fréchet derivative of eigenspaces without the need for second derivatives was derived and exploited for a perturbation-based uncertainty quantification approach.nannan","To this end, we use the derivatives of the coefficients (3.4) to calculate the derivatives of the eigenpairs with respect to each deformation mode using equation (4.4). Equation (5.4) then yields a representation of the covariances of the first derivatives.nan","The first 9 of these are nondegenerate eigenpairs, while the next 9 eigenvalues are all degenerate with multiplicity m = 2. To evaluate the efficiency of our derivative-based approach, we compare it to a Quasi-Monte Carlo method using Halton points.",We considered the uncertainty quantification of the Maxwell eigenvalue problem on random domains by means of a perturbation approach.,Shape uncertainty quantification of Maxwell eigenvalues and -modes with application to TESLA cavities,"Jürgen Dölz, David Ebert, Sebastian Schöps, Anna Ziegler","SHAPE UNCERTAINTY QUANTIFICATION OF MAXWELL EIGENVALUES
AND -MODES WITH APPLICATION TO TESLA CAVITIES∗
J¨URGEN D¨OLZ†, DAVID EBERT†, SEBASTIAN SCH¨OPS‡, AND ANNA ZIEGLER‡
Abstract. We consider Maxwell eigenvalue problems on uncertain shapes with perfectly conducting TESLA
cavities being the driving example. Due to the shape uncertainty, the resulting eigenvalues and eigenmodes are also
uncertain and it is well known that the eigenvalues may exhibit crossings or bifurcations under perturbation. We
discuss how the shape uncertainties can be modelled using the domain mapping approach and how the deformation
mapping can be expressed as coefficients in Maxwell’s equations. Using derivatives of these coefficients and deriva-
tives of the eigenpairs, we follow a perturbation approach to compute approximations of mean and covariance of the
eigenpairs. For small perturbations, these approximations are faster and more accurate than Monte Carlo or similar
sampling-based strategies. Numerical experiments for a three-dimensional 9-cell TESLA cavity are presented.
Key words. Maxwell eigenvalue problem, shape uncertainty, uncertainty quantification, Fr´echet derivatives,
local sensitivity analysis, perturbation approach
AMS subject classifications. 47A75, 65J10, 65C05, 65C20, 35Q61
1. Introduction.
1.1. Motivation. Radio-frequency resonators are devices in which electromagnetic fields
oscillate at gigahertz frequencies.
They are used, for example, to accelerate particles, where
the aim is to increase the magnitude of the excitation.
A specific example which will be the
driving force of our article is the superconducting TESLA cavity, following the design by [3]. The
TESLA cavity, designed for linear accelerators, is a 9-cell superconducting cavity, manufactured
from niobium sheets. It is cooled with superfluid helium to 2 K in operation and the accelerating
eigenmode resonates at 1.3 GHz. The standing wave structure has a total length of approximately
1 m and consists of nine elliptic cells, i.e. is designed from circles and ellipses, with an equator
diameter of approximately 21 cm, see Figure 1.1.
beampipes
equator
iris
cell
Figure 1.1: The 9-cell TESLA cavity with attached elongated beampipes.
During operation, a harmonic time-varying electric field is excited in the cavity whose field
distribution and resonating frequency are governed by Maxwell’s equations.
Considering the
∗Submitted to the editors January 23, 2024.
Funding: The work of SSc and AZ is partially supported by the Graduate School CE within the Centre for
Computational Engineering at TU Darmstadt. The work of JD and DE was partially funded by the Deutsche
Forschungsgemeinschaft (DFG, German Research Foundation) – project number 501419255.
JD and DE also
thankfully acknowledge the support by the DFG under Germany’s Excellence Strategy – project number 390685813.
We thank Jacopo Corno for providing us with the code and data to analyze the misaligned TESLA cavities. We
also thank Christian Schmitt for his frequent support with running our code on the compute server.
†Institute for Numerical Simulation, Friedrich-Hirzebruch-Allee 7, 53115 Bonn, Germany (doelz@ins.uni-
bonn.de, ebert@ins.uni-bonn.de).
‡Computational Electromagnetics Group, Technische Universit¨at Darmstadt, Schloßgartenstr. 8, 64289 Darm-
stadt, Germany (schoeps@temf.tu-darmstadt.de, anna.ziegler@tu-darmstadt.de).
1
arXiv:2401.11890v1  [math.NA]  22 Jan 2024
source-free, time harmonic and lossless case, the magnetic field strength H and the electric field
strength E are related by Faraday’s law
∇ × E = −jωµ0H,
where µ0 denotes the magnetic permeability of vacuum, and by Amp`ere’s law
∇ × H = jωε0E,
with ε0 indicating the electric permittivity in vacuum. In both equations ω denotes the unknown
angular resonance frequency of the cavity. The magnetic field strength H can be eliminated to
obtain the Maxwell eigenvalue problem in terms of the electric field strength
∇ × (∇ × E) = λE
with λ = ω2µ0ε0 and complemented with the boundary conditions of a perfect electric conductor
n × E = 0, with n being the outward pointing normal vector. Thus, determining the eigenpair
(λ, E), i.e., solving the eigenvalue problem allows to determine the resonance frequencies and their
modes.
Cavities are carefully designed to accelerate bunched particles in the longitudinal direction by
transferring energy to the bunch, which travels at velocities close to the speed of light. A precise
synchronisation between the electric field and the particle bunch is necessary for the successful
and efficient energy transmission and specific eigenmodes of the cavity need to be excited. The
electromagnetic field is characterised by the resonating frequency and the field quality, indicating
an even distribution of the field within the cavity. Amongst other factors, the eigenfrequency and
the field distribution are strongly coupled to the geometry of the domain and even small deviations
and uncertainties can have a non-negligible effect on the field and thus on the final efficiency of the
acceleration. Since manufacturing inaccuracies or electromagnetic pressure on the domain wall in
operation inevitably result in deviations from the design geometry and thus shift the resonating
frequency and affect the performance, a careful investigation of sensitivity and a shape uncertainty
quantification of the eigenmodes and fields of the eigenvalue problem is necessary.
1.2. Related Work.
1.2.1. Uncertainty quantification for Eigenvalue problems. The mathematical theory
for uncertainty quantification of eigenvalue problems seems to be still a relatively little explored
field. Early works mainly were developed in the fields of structural analysis and aerospace en-
gineering, see, e.g., the review paper [1] for investigations in structural dynamics. These works
are mostly limited to eigenvalues of single multiplicity and rather small algebraic systems. To
overcome the computational challenges of systems involving partial differential equations, [2] pro-
pose a sparse grid approach and [29, 30, 31, 45, 13] apply quasi-Monte Carlo methods. While
these works are restricted to the case of a single, isolated eigenvalue which does not cross, the
trajectories of several eigenvalues may generally show crossings or bifurcations when being per-
turbed [42]. Rellich showed in [47] that eigenpairs remain locally analytic even in this case if the
perturbation depends on a single, real parameter and gave a characterization of the first (and
higher) derivatives. This characterization relies on second derivatives to deal with the occurring
rank-defects in the characterization and was later exploited in the algorithmic approach of Nelson
and Dailey [19, 44]. Their algorithm was extended in [41] for higher derivatives of degenerate
eigenpairs. These derivatives were used in a tracking algorithm to identify eigenvalues where the
crossings occur only in one parameters dimension in a stochastic collocation scheme in [27]. For
eigenvalue problems depending on more than a single scalar parameter, Rellich showed in [47] that
the eigenpairs belonging to an eigenvalue of higher multiplicity are not Fr´echet differentiable in
general. However, [32] and [26] showed that the eigenspaces remain locally analytic. In [32], this
was employed in the uncertainty quantification of eigenspaces based on a stochastic collocation
approach. In [26], a linear representation of the first Fr´echet derivative of eigenspaces without
2
the need for second derivatives was derived and exploited for a perturbation-based uncertainty
quantification approach.
However, to the best of our knowledge, the influence of shape uncertainties onto the eigenpairs
has not been systematically investigated in the mathematical community.
This is in contrast
to shape uncertainties onto the solution of partial differential equations, where essentially two
approaches have emerged. The first, the domain mapping approaches, see [12, 34, 37, 53], transfer
the uncertainty in the domain onto a partial differential equation with random coefficients on
a fixed reference domain and are able to deal with large deformations. For the computation of
statistical quantities of interest such as the mean or the variance of the solutions, high-dimensional
quadrature rules such as Monte Carlo are usually employed. The second class of approaches are
perturbation approaches, see [5, 14, 22, 35, 39], which use local sensitivity analyses of solutions
to partial differential equations to compute statistical quantities of interest. Generally speaking
the domain mapping approach is best suited for large perturbations, whereas the perturbation or
sensitivity-based approach is best suited for smaller perturbations.
1.2.2. Uncertainty Quantification for Cavities. Within the electromagnetics commu-
nity, the effect of uncertainties in cavities on the field quality or resonating frequency have been
studied in several works, such as [18, 28, 16, 55, 51, 48].
Deformations arising in cavities in-
clude deviations from the design parameters, non axis-symmetric deformations, such as bumps or
kinks or mechanical deformations due to Lorentz forces [16]. Furthermore, the welding causes a
shrinkage at the welding point and a random misalignment of the cells with respect to the ideal
cavity axis [18, 15]. Sensitivity analyses are carried out on cavities in order to state the impact of
deviations from the design introduced in the production process. For the TESLA cavity, details
of the manufacturing procedure are given in [3, 18], describing the effects of the deep drawing of
the half-cells from niobium sheets, the machining, welding and trimming. Furthermore, the pro-
cesses to control and balance misalignment due to welding and the necessity for cleaning, chemical
treatments, grinding and final tuning are illustrated. Additionally, [3] state the requirements on
the niobium sheets. The works [16, 18] investigate an uncertainty quantification for the cavity
design parameters. Also in [48], the authors assume deviations of 13 uncertain design parameters
of the TESLA mid-cell and compare a Monte Carlo simulation and uni-variate and multi-variate
generalized Polynomial Chaos expansions to estimate some stochastic properties. In [7], a series
expansion from the eigenmodes of an unperturbed geometry based on matrix perturbation theory
is proposed, in order to avoid computationally expensive parameter studies for cavity perturba-
tions. Investigating the impact of parameter uncertainties on the dipole mode frequencies and the
external quality factor, the authors of [52] use a mesh distortion method. A stochastic response
surface model is proposed in [21], using the derivatives of the system matrices with respect to
geometric parameters.
In [55], a perturbation is applied to the system matrices of the eigen-
value problem and eigenpair derivatives at the unperturbed geometry are calculated. These are
employed for an efficient calculation of the eigenvalues and eigenvectors of the perturbed geometry.
Aside from production inaccuracies, [17] investigates the effects due to electromagnetic radia-
tion pressure and considers the frequency shift of the accelerating mode in the pillbox cavity and
in the 1-cell TESLA cavity applying a linear elasticity problem on the cavity walls. The authors
of [28] extend uncertainty investigations due to manufacturing imperfections to the more realistic
case of misalignment of the cells due to welding. A truncated Karhunen–Lo`eve expansion is per-
formed on measurement data of perturbed geometries and a stochastic collocation method based
on sparse grids at predefined collocation points is employed. For this approach, an eigenvalue
tracking procedure is proposed to ensure consistency of the solutions.
1.3. Contributions. As shape uncertainties in TESLA cavities are usually small, a perturba-
tion-based uncertainty quantification approach can be expected to be beneficial [6, 7, 8]. The
purpose of this article is to derive such an approach for the Maxwell eigenvalue problem. The
contributions are as follows:
1. We provide a characterization of the shape sensitivities of the eigenpairs for the Maxwell
eigenvalue problem in terms of partial differential equations.
In contrast to previous
approaches based on matrix perturbation theory, this provides larger flexibility for the
3
used solvers.
2. We extend the perturbation approach to uncertainty quantification for eigenvalue prob-
lems from [26] to the case of random domains on the example of the Maxwell eigenvalue
problem.
3. We provide closed-form characterizations of the deformation coefficients arising from dis-
cretized domain deformations using the framework from [58].
4. We demonstrate the feasibility and effectiveness of our approach on TESLA cavities with
deformation fields obtained from real world data.
1.4. Outline. The paper is structured as follows. In section 2, we introduce the problem
formulation and our method of modelling the geometry deformations. We review the formulation
of the domain mappings as well as the computation of their derivatives. On this basis, in section 3,
we present the stochastic model and the efficient computation of the covariances of the derivatives.
In section 4, we state the discretized version of the Maxwell eigenvalue problem, of its derivative
and of the covariance equation. We discretize the probability space using a truncated Karhunen-
Lo`eve style expansion in section 5 and specify our chosen discretization method, the isogeometric
analysis. We demonstrate our work in section 6, where we perform an uncertainty quantification
for the TESLA cavity. To this end, we assume a misalignment of the cavity cells with respect to
the ideal axis. Finally, we conclude our work in section 7.
2. Shape perturbations of Maxwell’s eigenvalue problem.
2.1. Maxwell’s Eigenvalue Problem. We consider the Maxwell eigenvalue problem on
the bounded Lipschitz domain D ⊂ R3 with Dirichlet boundary data in the source-free and
homogeneous case with lossless materials, i.e. the problem to find (λ, E) ∈ R×H0 (curl; D) , E ̸= 0
such that
∇ × (∇ × E) = λE
in D,
(2.1a)
∇ · E = 0
in D,
(2.1b)
E × n = 0
on ∂D,
(2.1c)
(a) f = 1.2777 GHz
(b) f = 1.2798 GHz
(c) f = 1.2830 GHz
(d) f = 1.2870 GHz
(e) f = 1.2912 GHz
(f) f = 1.2952 GHz
(g) f = 1.2985 GHz
(h) f = 1.3006 GHz
(i) f = 1.3014 GHz
0
0.2
0.4
0.6
0.8
1
|E| (V/m)
Figure 2.1: Nine smallest eigenvalues of a 9-cell TESLA cavity and respective eigenmodes (sliced
through lengthwise, color map according to the normalized magnitude of the electric field strength).
5
(a) Reference domain, t = 0
(b) t = 0.5
(c) t = 1
Figure 2.2: Physical reference domain of the TESLA cavity (top) and domain according to a
sampled deformation direction z, (t = {0, .5, 1}). On the left, the deformation in the yz-plane.
On the right, the deformation in the xz-plane. The deformations are displayed increased by factor
250. The red line indicates the deformation of the central axis.
with
Gt : D0 → R3,
x 7→ Gt(x; z),
(2.4)
and
(2.5)
G0(x; z) = x = Gt(x; 0).
To avoid technicalities we assume that the deformation field Gt is twice totally continuously
differentiable in (t, x).
In practice, the domain deformations are often modelled by means of
Karhunen-Lo`eve type expansions as discussed in subsection 5.1. These models satisfy the above
assumption (2.5), but our approach can also deal with more general domain deformation models.
An example of such a deformation for the TESLA cavity from Figure 1.1 can be seen in
Figure 2.2, where the deformation is visualized amplified by a factor 250 and the middle axis of
the cavity is depicted in red. We note that we define the reference geometry as the sample mean
geometry of a set of measured cavities. This is the reason why we observe a slightly curved central
axis in Figure 2.2a which obviously deviates from the nominal design. In the following section, we
show how the transformation can be translated into an eigenvalue problem involving coefficients.
2.3. Maxwell’s Eigenvalue Problem on Perturbed Domains. We start by formulating
the eigenvalue equation (2.1) on the perturbed domain Dt to obtain
∇ × (∇ × Et) = λtEt
in Dt,
(2.6a)
∇ · Et = 0
in Dt,
(2.6b)
Et × nt = 0
on ∂Dt,
(2.6c)
problem back to D0, and the isomorphy between H0 (curl; Dt) and H0 (curl; D0) [40, Lemma 2.2.],
we recover a strong formulation on the physical reference domain
∇ × (Ct∇ × E0,t) = λ0,tAtE0,t
in D0,
(2.7a)
∇ · (AtE0,t) = 0
in D0,
(2.7b)
E0,t × n0 = 0
on ∂D0,
(2.7c)
2.5. Derivatives of Eigenpairs. The derivatives of eigenpairs of single multiplicity where
originally characterized in [46]. While the characterization for eigenpairs of higher multiplicity in
[46] was a nonlinear mapping, [26] resolved this anomaly by deriving a linear characterization which
is consistent to the case of single multiplicity. Moreover, it was discussed that for perturbation
theories of eigenpairs to eigenvalues with multiplicity m > 1 one must resort to a perturbation
theory of the full eigenspace, i.e., we need to consider the eigenvalue problem
∇ × (Ct∇ × E0,t) = AtE0,tλ0,t
in D0,
(2.9a)
∇ · (AtE0,t) = 0
in D0,
(2.9b)
E0,t × n0 = 0
on ∂D0,
(2.9c)
mation field yield that (λ0,t, E0,t) satisfies the expansions
E0,t = E0,0 + t(dtE0,t) + O(t2),
λ0,t = λ0,0 + t(dtλ0,t) + O(t2).
Most notably, (λ0,t, E0,t) satisfies (2.9) in a neighbourhood of t = 0, with λ0,t not necessarily
being diagonal for t ̸= 0.
In the next section, we shall consider a stochastic model of the geometric deformations. We
can then use the derivatives of the eigenpair to approximate the mean and covariance of the
eigenpair.
3. Shape Uncertainty Quantification of Eigenvalues and -modes.
3.1. Maxwell’s Eigenvalue Problem on Randomly Deformed Shapes. We now go on
to consider the case where the deformations are realizations of a stochastic model. To this end,
let (Ω, F, P) be a probability space and let z(ω) ∈ U, ω ∈ Ω, be a random variable. In complete
analogy to (2.3), Dt(ω) = Gt(x, z(ω)) defines a randomly perturbed domain. Thus, we obtain a
new eigenvalue problem, i.e., find (λt(ω), Et(ω)) ∈ (R × H0 (curl; Dt(ω))), such that
∇ × (∇ × Et(ω)) = λt(ω)Et(ω)
in Dt(ω),
∇ · Et(ω) = 0
in Dt(ω),
Et(ω) × nt(ω) = 0
on ∂Dt(ω),
Lemma 3.1. Assume that dtGt(x; z) is linear in z and z is centered, i.e. E[z] = 0, then
the mean of the derivatives of the deformation coefficients evaluated at the reference t = 0 is also
centered, i.e.,
E[dtCt] = E[dtAt] = 0.
Proof. We first note that the assumptions (2.5) on the domain transformations, i.e., G0(x; z)=
x yield ∂G0(x; z) = I and C0, A0 = I. So when considering (2.8) at the reference t = 0, we get
dtCt = − tr ((dt∂Gt) I) I +
1
det(I) (dt∂Gt)⊤ I +
1
det(I)

(dt∂Gt)⊤ I
⊤
(3.4a)
= − tr (dt∂Gt) + (dt∂Gt)⊤ + dt∂Gt,
(3.4b)
and
dtAt = tr ((dt∂Gt) I) I − I (dt∂Gt) I − (I (dt∂Gt) I)⊤
(3.4c)
= tr (dt∂Gt) − dt∂Gt − (dt∂Gt)⊤
(3.4d)
= −dtCt,
(3.4e)
implying
E[dtCt] = − tr (E[dt∂Gt]) + (E[dt∂Gt])⊤ + E[dt∂Gt] = − E[dtAt].
Moreover, Gt(x; 0) = x from the assumptions (2.5) and total differentiability imply
dt∂Gt = ∂dtGt = ∂(dtGt|z=0) + ∂(DzdtGt) · z = ∂(DzdtGt) · z.
Taking the mean yields E[dt∂Gt] = ∂(dzdtGt) · E[z] = 0 and thus the assertion.
Lemma 3.2. Let λ0,0 be an eigenvalue of multiplicity m of (3.2) at t = 0 with orthonor-
mal eigenbasis E0,0. Assume that dtGt(x; z) is linear in z and z is centered, i.e. E[z] = 0.
Let (t, z) 7→ (E0,t, λ0,t) be the unique local, analytic trajectory for which it holds (E0,t, λ0,t) =
(E0,0, λ0,0I) at t = 0. Then it holds
E[λ0,t] = λ0,0 + O(t2),
(3.5a)
E[E0,t] = E0,0 + O(t2),
(3.5b)
Cov[λ0,t] = t2 Cov
h
dtλ0,t
i
+ O(t3),
(3.5c)
Cov[E0,t] = t2 Cov
h
dtE0,t
i
+ O(t3).
(3.5d)
Proof. Lemma 3.1 yields that the first derivatives of the coefficients are centered, i.e., it holds
E[dtCt] = 0,
E[dtAt] = 0.
The rest of the proof follows [26, Theorem 3.4] with only minor modifications.
The approximations of the covariances in (3.5c), (3.5d) involve the covariances of the first
derivatives of the eigenpair. To this end, those derivatives can be characterized by tensorizing
(3.3) and taking the mean to obtain
(3.6)
4. Discretization.
4.1. Conforming Discretization of Maxwell’s Eigenvalue Problem. Starting from the
problem formulation (2.7), let us derive the discrete Maxwell eigenvalue problem. To this end, we
pursue a Ritz-Galerkin discretization, but any other discretization method with similar properties
could also be used. The weak formulation of the Maxwell eigenvalue problem reads: find λ ∈ R
and E0,t ∈ H0 (curl; D0), such that
(4.1)

Ct(ω)∇×E0,t(ω), ∇×N

D0 = λ0,t(ω)

At(ω)E0,t(ω), N

D0
for all N ∈ H0 (curl; D0) .
Choosing an N-dimensional subspace W ⊂ H0 (curl; D0), W = spani=1,...,N{N i}, for discretiza-
tion of the electromagnetic field, a Galerkin discretization of (4.1) yields a discrete eigenvalue
problem
(4.2)
K
Unfortunately, the solution of such a system of equations can be computationally infeasible since
covariance matrices are generally dense, and their storage requirements grow prohibitively. To
this end, the solution of such covariance equations has been the topic of many articles, see [11, 24,
33, 36, 49] to mention a few. To keep the implementation simple, we outline in the next section
a strategy which is based on a (possibly approximate) Karhunen-Lo`eve type representation of the
deformation field.
5. Computational Considerations.
5.1. Closed-form Representations for Karhunen-Lo`eve type Deformation Fields.
In order to compute the derivatives of the deformation coefficients efficiently, we follow the domain
mapping approach to model the transformation Gt in the form of
Gt(x; z) = x + tV (x; z),
(5.1)
where x ∈ D0, with a sufficiently smooth deformation field V depending on z. For the purpose
of this article, we follow a Karhunen-Lo`eve type approach, i.e., we assume that the deformation
field can be decomposed as
V (x; z) =
M
X
i=1
ziV i(x)
(5.2)
and zi uniformly distributed i.i.d. random variables on [−1, 1]. For M = ∞ we assume that the
V i are such that the sum converges for almost all z and yields a Lipschitz continuous vector field.
The following lemma gives closed-form expressions for the parametric derivative of the deformation
coefficients for this case, which happen to be again of Karhunen-Lo`eve type.
Lemma 5.1. Let Gt be given through (5.1) and (5.2). Then the parametric derivatives of the
deformation coefficients (2.7e) and (2.7f) are given through
dtCt = −
M
X
i=1
zi
h
dtBt
i
i,
dtAt =
M
X
i=1
zi
h
dtBt
i
i,
where
(5.3)
h
dtBt
i
i = tr(∂V i) − ∂V i − ∂V ⊤
i .
Proof. The specific form of Gt due to (5.1) and (5.2) yields
dt∂Gt =
M
X
i=1
zi∂V i,
which, inserted into (3.4) yields the assertion.
Considering system of equations (2.10) we also observe that dtC, dtA are only plugged in on
the right-hand side of the equation. Since (2.10) is linear, by plugging in [dtC]i, [dtA]i instead,
we obtain derivatives
i.e., we have a solution representation for (3.6). This way of construction transfers to the discrete
case such that we have an explicit way to solve the covariance equation (4.5). Since we observe in
practice M ≪ ∞ without significant loss in accuracy this solution approach is also efficient, but
requires the computation of the [dtBt]i-terms in the discrete setting.
5.2. Geometry Description. A common challenge when dealing with random domains is
that the geometry perturbations need to be realized, e.g. in the classical Monte Carlo method.
This is not the case in our derivative-based approach. Here, the challenge is to determine the
deformation coefficients, for which we need to compute the [dtBt]i-terms given in (5.3). Only to
validate our approach, we compare with the classical Monte Carlo method which requires actual
geometry deformations. The framework of isogeometric analysis (IGA) [38] has been successfully
used for geometry deformations in the past for this purpose, see, e.g., [23, 28, 56], so we will also
follow it here. To this end, for the geometry perturbations we use the same parametric framework
as also used for the representation of geometries in computer-aided design.
For the geometry description itself, we start with the one-dimensional B-spline spaces of
piecewise polynomial degree p ≥ 0. For k > p, a locally quasi uniform p-open knot vector is a
tuple Ξ =

ξ0 = · · · = ξp ≤ · · · ≤ ξk = · · · = ξk+p

∈ [0, 1]k+p+1 with ξ0 = 0 and ξk+p = 1 such
that there exists a constant θ ≥ 1 with θ−1 ≤ hj ·h−1
j+1 ≤ θ for all p ≤ j < k, where hj = ξj+1 −ξj.
The B-spline basis {Bj,p}0≤j<k is then recursively defined according to
Bj,p(x) =
(
1[ξj,ξj+1)
if p = 0,
x−ξj
ξj+p−ξj bp−1
j
(x) +
ξj+p+1−x
ξj+p+1−ξj+1 bp−1
j+1(x)
else,
where 1A refers to the indicator function of the set A.
For further concepts and algorithmic
realization of B-splines, we refer to [20] and the references therein.
We assume the usual isogeometric setting for the domain representation of the physical refer-
ence domain, i.e., denoting □ = [0, 1]3, we assume that the reference domain can be decomposed
into several smooth patches
D0 =
M
[
i=1
D(i)
0 .
Therein, the intersection D(i)
0
∩ D(j)
0
consists at most of a common vertex, a common edge, or a
common face for i ̸= j. In particular, each patch is given as D(i)
0
= gi(□), where gi is an invertible
NURBS mapping gi : □ → D(i)
0
given through
gi(x, y, z) =
k1
X
i1=0
k2
X
i2=0
k3
X
i3=0
Bi1,p1(x)Bi2,p2(y)Bi3,p3(z)wi1,i2,i3
Pk1
j1=0
Pk2
j2=0
Pk3
j3=0 Bj1,p1(x)Bj2,p2(y)Bj3,p3(z)wj1,j2,j3
Pi1,i2,i3,
for control points Pi1,i2,i3 ∈ R3 and weights wi1,i2,i3 ∈ R.
5.3. Geometry Deformation and Spline Spaces. In most cases, the geometry deforma-
tion as given through (5.1) and (5.2) will not be available in closed-form representations. In-
stead, the V i will need to be approximated numerically, for which we need to define suitable
finite-dimensional function spaces on ∂D0. To this end, we define the spline spaces spanned by
the one-dimensional B-splines Sp(Ξ) = span({Bj,p}j<k) constructed from a given knot vector Ξ.
Spline spaces in higher spatial dimensions are defined through tensor-product constructions, i.e.,
for a tuple of knot vectors Ξ = (Ξ1, Ξ2, Ξ3) and polynomial degrees p = (p1, p2, p3), we define
Sp
0 (Ξ) = Sp1(Ξ1) ⊗ Sp2(Ξ2) ⊗ Sp3(Ξ3).
One of the requirements to obtain a reasonably deformed domain is that the deformation field is
at least continuous. Thus, given a tuple of knot vectors Ξ and polynomial degrees p, a natural
choice for the discretization of the perturbation field V is thus given by the vector valued spline
space
SSSp
0 (D0) =

Sp
0 (D0)
3
13
where
Sp
0 (D0) = {f ∈ C(D0): f ◦ gi ∈ Sp
0 (Ξ) for 1 ≤ i ≤ M} .
Of course, the knot vectors and polynomial degrees could vary in each component and on each
patch, but for better readability we opt for using the same knots and degrees. For remarks on
the approximation property of these spaces we refer to [9]. An efficient approach to compute
a discrete approximation to a Karhunen-Lo`eve expansion with vanishing mean and prescribed
covariance was described in [23].
Given an approximation of (5.2) in SSSp
0 (D0) we can represent the deformed random domains
Dt(ω) = Gt(x, z(ω)) as Dt = ∪M
i=1D(i)
t (ω), where D(i)
t (ω) = gi(□, ω) and
gi(□, ω) = Gt(gi(□), z(ω)) = gi(□) + t
M
X
i=1
ziV i(gi(□))
With gi(·) being a NURBS mapping, V i(gi(·)) being a B-spline space, and the sum of these
being a NURBS mapping, we note that gi(·, ω) is again a NURBS mapping. As such, the random
domains Dt(ω) can be dealt with in the same isogeometric framework as D0. We remark that,
despite small deformations being the primary interest of this work, this modelling approach also
permits for large shape deformations, see, e.g., [23, 25, 28, 56].
5.4. Isogeometric discretization of Maxwell’s eigenvalue problem. For the isogeo-
metric H0 (curl; Dt(ω))-conforming discretization of the Maxwell eigenvalue problem we follow
[10] to define
SSSp
1 (Ξ) = Sp1−1(Ξ′
1) ⊗ Sp2(Ξ2) ⊗ Sp3(Ξ3)
× Sp1(Ξ1) ⊗ Sp2−1(Ξ′
2) ⊗ Sp3(Ξ3)
× Sp1(Ξ1) ⊗ Sp2(Ξ2) ⊗ Sp3−1(Ξ′
3),
where Ξ′
i, i = 1, 2, 3, denotes the truncated knot vector Ξi, i.e., Ξi without the first and the last
element. This allows to define H0 (curl; Dt(ω))-conforming spaces by
SSSp
1 (D0) = {f ∈ H0 (curl; Dt(ω)) : f ◦ gi ∈ Sp
1 (Ξ) for 1 ≤ i ≤ M} .
For approximation properties of this space we refer to [9].
6. Numerical Examples.
6.1. Example Problem: Shape Deformation of a TESLA Cavity. We demonstrate
our results investigating the shape deformation of the 9-cell TESLA cavity, where the cell centers
are misaligned with respect to the ideal axis cavity. We focus on the issue of misaligned cells with
respect to the ideal cavity axis, which are inherently introduced during the production processes
of the dumbbells and at the welding at the equators, see Figure 6.1.
For the shape deformations, we use a Karhunen-Lo`eve decomposition with respect to the mean
geometry (including the deviations from the nominal design) such that the random realizations
have zero mean. The decomposition uses M = 7 terms which was derived in [28] from real-world
data from the Deutsches Elektronen-Synchrotron (DESY) database [54]. To this end, the authors
represent the displacement of the nine cell centers in the transversal planes, i.e. in the x- and y-
direction, while keeping the position along the longitudinal z-axis fixed. The deformation between
the cell centers is smoothly interpolated by splines. The deformation of the mean geometry as
well as the deformation modes are illustrated in Figure 6.2, where for each deformation mode, the
deformed cavity axis is displayed in two planes, the yz- and xz-plane, respectively and the original
cell centers on the ideal cavity axis are marked with red crosses.
6.2. Computational Setup. All computations are carried out in MATLAB®, using the
GeoPDEs [50] package for the discretization with IGA. We perform all computations on a compute
server with eight Intel® Xeon® Platinum 8160 CPUs with twelve 2.10 GHz cores each, hyper-
threading enabled and 1.5 TB RAM.
14
EGL DB1
DB2
DB3
DB4
DB5
DB6
DB7
DB8 EGR
equator
welds
Figure 6.1: The TESLA cavity and the manufacturing from dumbbells (DB) and endgroups (EGL
= endgroup left, EGR = endgroup right).
The 9-cell TESLA cavity with attached beampipes was discretized by splines of degree two,
yielding 25, 744 degrees of freedom.
For the numerical examples we want to consider the 27
eigenpairs of smallest magnitude. The first 9 of these are nondegenerate eigenpairs, while the
next 9 eigenvalues are all degenerate with multiplicity m = 2. To evaluate the efficiency of our
derivative-based approach, we compare it to a Quasi-Monte Carlo method using Halton points. To
this end, we choose 17 different perturbation amplitudes (i.e., t = {2−5, 2−4.5, 2−4, . . . , 22, 5, 6}),
for each of which we perform a Monte Carlo simulation using 10, 000 samples.
The sampling
process is parallelized using MATLAB® parfor loops.
6.3. Verification of the perturbation approach. For the calculation of the eigenpair
derivatives we first solve for the unperturbed eigenvalue.
We then use the derivatives of the
coefficients (3.4) to calculate the derivatives of the eigenpairs with respect to each deformation
mode using equation (4.4). Equation (5.4) then yields a representation of the covariances of the
first derivatives. These are then used to calculate first-order approximations of the variances of
the eigenvalues as well as the variances of the associated modes.
To validate our perturbation approach, we compare our approximations against mean and
variance computed with a Quasi-Monte Carlo estimate using nMC = 10, 000 Halton points for the
smallest 27 eigenmodes. The use of quasi-random numbers is appropriate for small deformations
since the eigenspaces and corresponding modes are locally analytic, see [26]. For the eigenvalues
the Euclidean norm of the error over λ is calculated, while for the eigenmodes we consider the
L2-norm of the error of the variances of the eigenspaces. Due to the construction of the stochastic
deformation and Lemma 3.2 we predict an order of convergence of two for the mean and three for
the variance. We plot the resulting errors of the eigenvalues and eigenspaces for the nine smallest
eigenvalues, which are non-degenerate and the following eighteen eigenvalues which are degenerate
with multiplicity m = 2. The results found in Figure 6.3 and Figure 6.4 confirm the predicted
order of convergence, although for the variances the order of convergence can only be seen for the
largest deformations. For small deformations the error rate can be traced back to the Monte Carlo
error being larger than the error of the series approximation, which has also been observed in [26].
An improved Monte Carlo estimate with more samples is out of reach computationally.
6.4. Shape Uncertainty Quantification for TESLA Cavities. The resulting variance
maps of the first nine modes are shown in Figure 6.5. Based on these plots, we can make some
15
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5 ·10−4
z axis (m)
x/y axis, resp., (m)
yz plane
xz plane
ideal cell centers
(a) Deformation of the mean geometry
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5 ·10−4
z axis (m)
x/y axis, resp., (m)
(b) Deformation mode 1
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5 ·10−4
z axis (m)
x/y axis, resp., (m)
(c) Deformation mode 2
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5 ·10−4
z axis (m)
x/y axis, resp., (m)
(d) Deformation mode 3
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5 ·10−4
z axis (m)
x/y axis, resp., (m)
(e) Deformation mode 4
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5 ·10−4
z axis (m)
x/y axis, resp., (m)
(f) Deformation mode 5
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5 ·10−4
z axis (m)
x/y axis, resp., (m)
(g) Deformation mode 6
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5 ·10−4
z axis (m)
x/y axis, resp., (m)
(h) Deformation mode 7
Figure 6.2: The mean deformation and the seven deformation modes displayed for the cavity axis.
The red crosses mark the cell centers when the axis is ideal and undeformed. The blue and black
lines indicate the deformed axes in the mean geometry and for the deformation modes. The blue
lines correspond to the deformation seen in the yz plane and the black lines to the deformation
seen in the xz plane.
16
10−2
10−1
100
101
10−8
10−6
10−4
10−2
Scaling parameter t
Error
Expected Value of Eigenvalues
EW 1
EW 2
EW 3
EW 4
EW 5
EW 6
EW 7
EW 8
EW 9
O(t2)
10−2
10−1
100
101
10−6
10−4
10−2
Scaling parameter t
Error
Expected Value of Eigenvectors
EW 1
EW 2
EW 3
EW 4
EW 5
EW 6
EW 7
EW 8
EW 9
O(t2)
10−2
10−1
100
101
10−14
10−12
10−10
10−8
10−6
10−4
Scaling parameter t
Error
Variances of Eigenvalues
EW 1
EW 2
EW 3
EW 4
EW 5
EW 6
EW 7
EW 8
EW 9
O(t3)
10−2
10−1
100
101
10−8
10−6
10−4
10−2
100
Scaling parameter t
Error
Variances of Eigenvectors
EW 1
EW 2
EW 3
EW 4
EW 5
EW 6
EW 7
EW 8
EW 9
O(t3)
Figure 6.3: Convergence of error between approximation and Monte Carlo estimate for mean and
variance, computed for the first nine eigenmodes, belonging to eigenvalues of simple multiplicity.
observations: First of all, we note a strong connection of the variance to the distribution of the
electric field strength, see Figure 2.1. When also considering the orientation of the lines indicating
the variances, we note that the z-component is particularly prominent. This makes sense, also
in regard to the first observation, as we can see in Figure 6.6, where we display the electric field
strength, exemplary for the case of the accelerating mode. Here, we plot the components and
the magnitude of the electric field strength of the ninth mode. Considering the transversal x-
and y-components, we notice that they are close to zero in most parts of the domain and that
this also does not change significantly when we deform the cavity. In contrast, the longitudinal
17
10−2
10−1
100
101
10−7
10−5
10−3
10−1
Scaling parameter t
Error
Expected Value of Eigenvalues
ES 10
ES 11
ES 12
ES 13
ES 14
ES 15
ES 16
ES 17
ES 18
O(t2)
10−2
10−1
100
101
10−6
10−4
10−2
100
Scaling parameter t
Error
Expected Value of Eigenspaces
ES 10
ES 11
ES 12
ES 13
ES 14
ES 15
ES 16
ES 17
ES 18
O(t2)
10−2
10−1
100
101
10−12
10−10
10−8
10−6
10−4
10−2
Scaling parameter t
Error
Variances of Eigenvalues
ES 10
ES 11
ES 12
ES 13
ES 14
ES 15
ES 16
ES 17
ES 18
O(t3)
10−2
10−1
100
101
10−7
10−5
10−3
10−1
101
Scaling parameter t
Error
Variances of Eigenspaces
ES 10
ES 11
ES 12
ES 13
ES 14
ES 15
ES 16
ES 17
ES 18
O(t3)
Figure 6.4: Convergence of error between approximation and Monte Carlo estimate for mean and
variance, computed for the eigenspaces 10 to 18, each belonging to eigenvalues of multiplicity 2.
z-component, which has the characteristic alternating field pattern, changes vastly during the
deformation.
Notably, the most significant field deformations occur in the center of the cells
rather than in regions distant from the central axis. These observations are clearly reflected in
the variance map Figure 6.5i of the accelerating mode. We also observe that the variance of the
accelerating mode in the outer cells is much larger than in the mid-cells. This also is in accordance
with the fact that when deforming the cavity, the field strength of the accelerating mode changes
most significantly in the outer cells, see Figure 6.6.
18
6.5. Computational Wall Clock Time. For the perturbation approach and the Monte
Carlo estimate, we initialize the computations by assembling the system matrices of the unper-
turbed cavity and solving the corresponding eigenvalue problem. Next, when applying our novel
approach, we compute the derivatives of system matrices with respect to the deformation using
the software published in [57] for each of the seven deformation modes. This step can be fully
parallelized and then takes 2 min, averaged over 10 runs. Then, for each deformation mode, we
assemble and solve system (4.4) and, given the derivatives [DtEt]i and [Dtλt]i for deformation
modes i = 1, . . . , M, compute the covariance (5.4). Since again, the contribution of each defor-
mation mode can be computed independently, this task can be fully parallelized and is completed
after 15 s, on average.
In contrast, for the Monte Carlo approach, we loop over 17 iterations. In each, we evaluate
10, 000 samples, i.e. we assemble the system matrices at 10, 000 quasi-random values, solve the
eigenvalue problem and determine the expected value and the covariance. The samples are com-
puted in a parfor loop. When distributing the task among 90 workers, the computations per
iteration take on average 2 h 13 min, i.e. 37 h 45 min in total. Therefore, the derivative-based un-
certainty quantification, where the full computation is completed after 2 min 17 s, is dramatically
more efficient. The precise timing values are however subject to the prototype character of the
code and are therefore to be understood rather as points of reference.
7. Conclusion. We considered the uncertainty quantification of the Maxwell eigenvalue
problem on random domains by means of a perturbation approach.
The considered eigenval-
ues can be of higher but finite multiplicity and can thus also deal with crossings or bifurcations
in the eigenvalue trajectories. To this end, we used the domain mapping approach to convert the
eigenvalue problem on random domains into an eigenvalue problem with uncertain coefficients.
The uncertainty in this eigenvalue problem was quantified using an extension of the approach
of [26], for which the derivatives of the domain transformation coefficients were required. These
derivatives were taken from [56]. Using isogeometric analysis for the representation of the geom-
etry allows for a straightforward modelling of the deformations and computation of the required
derivatives. We applied our theory for the uncertainty quantification of a three-dimensional 9-cell
TESLA cavity with a random deformation model obtained from real-world data from the DESY
database.
REFERENCES
[1] S. Adhikari and M. I. Friswell, Random matrix eigenvalue problems in structural dynamics, International
Journal for Numerical Methods in Engineering, 69 (2007), pp. 562–591, https://doi.org/10.1002/nme.
1781.
[2] R. Andreev and C. Schwab, Sparse tensor approximation of parametric eigenvalue problems, Numerical
Analysis of Multiscale Problems, 83 (2012), pp. 203–241, https://doi.org/10.1007/978-3-642-22061-6 7.
[3] B. Aune, R. Bandelmann, D. Bloess, B. Bonin, A. Bosotti, M. Champion, C. Crawford, G. Deppe,
B. Dwersteg, D. A. Edwards, H. T. Edwards, M. Ferrario, M. Fouaidy, P.-D. Gall, A. Gamp,
A. G¨ossel, J. Graber, D. Hubert, M. H¨uning, M. Juillard, T. Junquera, H. Kaiser, G. Kreps,
M. Kuchnir, R. Lange, M. Leenen, M. Liepe, L. Lilje, A. Matheisen, W.-D. M¨oller, A. Mosnier,
H. Padamsee, C. Pagani, M. Pekeler, H.-B. Peters, O. Peters, D. Proch, K. Rehlich, D. Reschke,
H. Safa, T. Schilcher, P. Schm¨user, J. Sekutowicz, S. Simrock, W. Singer, M. Tigner, D. Trines,
K. Twarowski, G. Weichert, J. Weisend, J. Wojtkiewicz, S. Wolff, and K. Zapfe, Superconducting
TESLA cavities, Physical Review Accelerators and Beams, 3 (2000), p. 092001, https://doi.org/10.1103/
PhysRevSTAB.3.092001.
[4] D. Boffi, Finite element approximation of eigenvalue problems, Acta Numerica, 19 (2010), pp. 1–120, https:
//doi.org/10.1017/S0962492910000012.
[5] F. Bonizzoni and F. Nobile, Perturbation analysis for the Darcy problem with log-normal permeability,
SIAM/ASA Journal on Uncertainty Quantification, 2 (2014), pp. 223–244, https://doi.org/10.1137/
130949415.
[6] K. Brackebusch, H.-W. Glock, and U. van Rienen, Calculation of high frequency fields in resonant cavities
based on perturbation theory, in Proceedings of IPAC 2011, San Sebasti´an, Spain, 2011, pp. 2235–2237.
[7] K. Brackebusch and U. van Rienen, Eigenmode computation for cavities with perturbed geometry based
on a series expansion of unperturbed eigenmodes, in Proceedings of IPAC2012, New Orleans, Louisiana,
USA, V. Suller, ed., IEEE, May 2012, pp. 277–279.
[8] K. Brackebusch and U. van Rienen, Eigenmode computation for elliptical cavities subject to geometric
19
variation using perturbative methods, in Proceedings of IPAC 2013, Shanghai, China, 2013, pp. 900–902.
[9] A. Buffa, J. D¨olz, S. Kurz, S. Sch¨ops, R. V´azquez, and F. Wolf, Multipatch approximation of the de
Rham sequence and its traces in isogeometric analysis, Numerische Mathematik, 144 (2020), pp. 201–236,
https://doi.org/10.1007/s00211-019-01079-x.
[10] A. Buffa, J. Rivas, G. Sangalli, and R. V´azquez, Isogeometric discrete differential forms in three dimen-
sions, SIAM Journal on Numerical Analysis, 49 (2011), pp. 818–844.
[11] H. Bungartz and M. Griebel, Sparse grids, Acta Numerica, 13 (2004), pp. 147–269.
[12] J. E. Castrill´on-Cand´as, F. Nobile, and R. F. Tempone, Analytic regularity and collocation approxima-
tion for elliptic PDEs with random domain deformations, Computers & Mathematics with Applications,
71 (2016), pp. 1173–1197.
[13] A. Chernov and T. Le, Analytic and Gevrey class regularity for parametric elliptic eigenvalue problems and
applications, Aug. 2023, https://arxiv.org/abs/2306.07010.
[14] A. Chernov and C. Schwab, First order k-th moment finite element analysis of nonlinear operator equations
with stochastic data, Mathematics of Computation, 82 (2013), pp. 1859–1888, https://doi.org/10.1090/
S0025-5718-2013-02692-0.
[15] J. Corno, Numerical Methods for the Estimation of the Impact of Geometric Uncertainties on the Perfor-
mance of Electromagnetic Devices, dissertation, Technische Universit¨at Darmstadt, June 2017, https:
//arxiv.org/abs/urn:nbn:de:tuda-tuprints-70389.
[16] J. Corno, C. De Falco, H. De Gersem, and S. Sch¨ops, Isogeometric Analysis simulation of TESLA
cavities under uncertainty, in 2015 International Conference on Electromagnetics in Advanced Ap-
plications (ICEAA), IEEE, Sept. 2015, pp. 1508–1511, https://doi.org/10.1109/ICEAA.2015.7297375,
http://ieeexplore.ieee.org/document/7297375/.
[17] J. Corno, C. de Falco, H. De Gersem, and S. Sch¨ops, Isogeometric simulation of lorentz detuning in
superconducting accelerator cavities, Computer Physics Communications, 201 (2016), pp. 1–7, https:
//doi.org/https://doi.org/10.1016/j.cpc.2015.11.015.
[18] J. Corno, N. Georg, S. G. Zadeh, J. Heller, V. Gubarev, T. Roggen, U. R¨omer, C. Schmidt,
S. Sch¨ops, J. Schultz, A. Sulimov, and U. van Rienen, Uncertainty modeling and analysis of the
European X-ray free electron laser cavities manufacturing process, Nuclear Instruments and Methods
in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, 971
(2020), p. 164135, https://doi.org/https://doi.org/10.1016/j.nima.2020.164135.
[19] R. L. Dailey, Eigenvector derivatives with repeated eigenvalues, AIAA Journal, 27 (1989), pp. 486–491,
https://doi.org/10.2514/3.10137.
[20] C. de Boor, A Practical Guide to Splines, vol. 27 of Applied Mathematical Sciences, Springer, rev. ed., 2001.
[21] J. Deryckere, H. De Gersem, B. Masschaele, and T. Roggen, Stochastic Response Surface Method for
Studying Microphoning and Lorenz Detuning of Accelerator Cavities, in 11th International Computational
Accelerator Physics Conference, 10 2012, p. WEP11.
[22] J. D¨olz, A Higher Order Perturbation Approach for Electromagnetic Scattering Problems on Random Do-
mains, SIAM/ASA Journal on Uncertainty Quantification, 8 (2020), pp. 748–774, https://doi.org/10.
1137/19M1274365.
[23] J. D¨olz, H. Harbrecht, C. Jerez-Hanckes, and M. Multerer, Isogeometric multilevel quadrature for for-
ward and inverse random acoustic scattering, Computer Methods in Applied Mechanics and Engineering,
388 (2022), p. 114242, https://doi.org/10.1016/j.cma.2021.114242.
[24] J. D¨olz, H. Harbrecht, and C. Schwab, Covariance regularity and H-matrix approximation for rough
random fields, Numerische Mathematik, 135 (2017), pp. 1045–1071.
[25] J. D¨olz, W. Huang, and M. Multerer, $p$-multilevel Monte Carlo for acoustic scattering from large
deviation rough random surfaces, Nov. 2023, https://arxiv.org/abs/2311.12565.
[26] J. D¨olz and D. Ebert, On uncertainty quantification of eigenvalues and eigenspaces with higher multiplicity,
SIAM Journal on Numerical Analysis (to appear), (2024).
[27] N. Georg, W. Ackermann, J. Corno, and S. Sch¨ops, Uncertainty quantification for Maxwell’s eigenprob-
lem based on isogeometric analysis and mode tracking, Computer Methods in Applied Mechanics and
Engineering, 350 (2019), pp. 228–244, https://doi.org/10.1016/j.cma.2019.03.002.
[28] N. Georg, W. Ackermann, J. Corno, and S. Sch¨ops, Uncertainty Quantification for Maxwell’s Eigenprob-
lem based on Isogeometric Analysis and Mode Tracking, Computer Methods in Applied Mechanics and
Engineering, 350 (2019), pp. 228–244, https://doi.org/10.1016/j.cma.2019.03.002. arXiv: 1802.02978.
[29] A. D. Gilbert, I. G. Graham, F. Y. Kuo, R. Scheichl, and I. H. Sloan, Analysis of quasi-Monte Carlo
methods for elliptic eigenvalue problems with stochastic coefficients, Numerische Mathematik, 142 (2019),
pp. 863–915, https://doi.org/10.1007/s00211-019-01046-6.
[30] A. D. Gilbert and R. Scheichl, Multilevel quasi-Monte Carlo for random elliptic eigenvalue problems I:
Regularity and error analysis, Jan. 2022, https://arxiv.org/abs/2010.01044.
[31] A. D. Gilbert and R. Scheichl, Multilevel quasi-Monte Carlo for random elliptic eigenvalue problems II:
Efficient algorithms and numerical results, Jan. 2022, https://arxiv.org/abs/2103.03407.
[32] L. Grubiˇsi´c, M. Saarikangas, and H. Hakula, Stochastic collocation method for computing eigenspaces
of parameter-dependent operators, Numerische Mathematik, 153 (2023), pp. 85–110, https://doi.org/10.
1007/s00211-022-01339-3.
[33] H. Harbrecht, M. Peters, and R. Schneider, On the low-rank approximation by the pivoted Cholesky
decomposition, Applied Numerical Mathematics, 62 (2012), pp. 28–440.
[34] H. Harbrecht, M. Peters, and M. Siebenmorgen, Analysis of the domain mapping method for elliptic
20
diffusion problems on random domains, Numerische Mathematik, 134 (2016), pp. 823–856.
[35] H. Harbrecht, R. Schneider, and C. Schwab, Sparse second moment analysis for elliptic problems in
stochastic domains, Numerische Mathematik, 109 (2008), pp. 385–414.
[36] H. Harbrecht, R. Schneider, and Ch. Schwab, Multilevel frames for sparse tensor product spaces, Nu-
merische Mathematik, 110 (2008), pp. 199–220, https://doi.org/10.1007/s00211-008-0162-x.
[37] R. Hiptmair, L. Scarabosio, C. Schillings, and Ch. Schwab, Large deformation shape uncertainty quan-
tification in acoustic scattering, Advances in Computational Mathematics, 44 (2018), pp. 1475–1518,
https://doi.org/10.1007/s10444-018-9594-8.
[38] T. Hughes, J. Cottrell, and Y. Bazilevs, Isogeometric analysis: CAD, finite elements, NURBS, exact
geometry and mesh refinement, Computer Methods in Applied Mechanics and Engineering, 194 (2005),
pp. 4135–4195, https://doi.org/https://doi.org/10.1016/j.cma.2004.10.008.
[39] C. Jerez-Hanckes and C. Schwab, Electromagnetic wave scattering by random surfaces: uncertainty quan-
tification via sparse tensor boundary elements, IMA Journal of Numerical Analysis, 37 (2016), pp. 1175–
1210.
[40] C. Jerez-Hanckes, C. Schwab, and J. Zech, Electromagnetic wave scattering by random surfaces: Shape
holomorphy, Mathematical Models and Methods in Applied Sciences, 27 (2017), pp. 2229–2259, https:
//doi.org/10.1142/S0218202517500439.
[41] P. Jorkowski, Zur numerischen Berechnung von parametrischen und nichtlinearen Eigenwertproblemen in
der elektromagnetischen Feldsimulation, dissertation, Technische Universit¨at Berlin, 2020.
[42] T. Kat¯o, Perturbation Theory for Linear Operators, Classics in Mathematics, Springer, Berlin, 1995.
[43] P. Monk, Finite element methods for Maxwell’s equations, Numerical mathematics and scientific computa-
tion, Clarendon Press ; Oxford University Press, Oxford : New York, 2003. OCLC: ocm51109019.
[44] R. B. Nelson, Simplified calculation of eigenvector derivatives, AIAA Journal, 14 (1976), pp. 1201–1205,
https://doi.org/10.2514/3.7211.
[45] V. K. Nguyen, Analyticity of Parametric Elliptic Eigenvalue Problems and Applications to Quasi-Monte
Carlo Methods, May 2022, https://arxiv.org/abs/2202.02530.
[46] F. Rellich, St¨orungstheorie der Spektralzerlegung. IV, Mathematische Annalen, 117 (1940), pp. 356–382.
[47] F. Rellich, Perturbation Theory of Eigenvalue Problems, Gordon and Breach Science Publishers Inc., New
York, 1969.
[48] C. Schmidt, T. Flisgen, J. Heller, and U. van Rienen, Comparison of techniques for uncertainty quantifi-
cation of superconducting radio frequency cavities, in 2014 International Conference on Electromagnetics
in Advanced Applications (ICEAA), 2014, pp. 117–120, https://doi.org/10.1109/ICEAA.2014.6903838.
[49] T. von Petersdorff and C. Schwab, Sparse finite element methods for operator equations with stochastic
data, Applications of Mathematics, 51 (2006), pp. 145–180.
[50] R. V´azquez, A new design for the implementation of isogeometric analysis in Octave and Matlab: GeoPDEs
3.0, Computers and Mathematics with Applications, 72 (2016), pp. 523–554, https://doi.org/10.1016/j.
camwa.2016.05.010.
[51] L. Xiao, C. Adolphsen, V. Akcelik, A. Kabel, K. Ko, L. Lee, Z. Li, and C. Ng, Modeling imperfection
effects on dipole modes in TESLA cavity, in 2007 IEEE Particle Accelerator Conference (PAC), 2007,
pp. 2454–2456, https://doi.org/10.1109/PAC.2007.4441281.
[52] L. Xiao, C. Adolphsen, V. Akcelik, A. Kabel, K. Ko, L. Lee, Z. Li, and C. Ng, Modeling imperfection
effects on dipole modes in TESLA cavity, in 2007 IEEE Particle Accelerator Conference (PAC), 2007,
pp. 2454–2456, https://doi.org/10.1109/PAC.2007.4441281.
[53] D. Xiu and D. M. Tartakovsky, Numerical methods for differential equations in random domains, SIAM
Journal on Scientific Computing, 28 (2006), pp. 1167–1185.
[54] S. Yasar, P. D. Gall, V. Gubarev, J. Iversen, and A. Sulimov, A database for the European XFEL, in
Proceedings of SRF2013, Paris, France, 2013, pp. 205–207.
[55] S. G. Zadeh, T. Flisgen, and U. van Rienen, Eigenmode computation of cavities with perturbed geometry
using matrix perturbation methods applied on generalized eigenvalue problems, Journal of Computational
Physics, 364 (2018), pp. 347–364, https://doi.org/https://doi.org/10.1016/j.jcp.2018.03.012.
[56] A. Ziegler, N. Georg, W. Ackermann, and S. Sch¨ops, Mode recognition by shape morphing for Maxwell’s
eigenvalue problem in cavities, IEEE Transactions on Antennas and Propagation, 71 (2023), pp. 4315–
4325, https://doi.org/10.1109/TAP.2023.3249907.
[57] A. Ziegler, M. Merkel, P. Gangl, and S. Sch¨ops, Software for Computation of Analytic Sensitivities
of Eigenpairs in Isogeometric Analysis, Zenodo, Mar. 2023, https://doi.org/10.5281/zenodo.7687757.
https://doi.org/10.5281/zenodo.7687757.
[58] A. Ziegler, M. Merkel, P. Gangl, and S. Sch¨ops, On the computation of analytic sensitivities of eigen-
pairs in isogeometric analysis, Computer Methods in Applied Mechanics and Engineering, 409 (2023),
p. 115961, https://doi.org/https://doi.org/10.1016/j.cma.2023.115961.
21
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
0
2
4
6
8
10
12
13
Var[E]
Figure 6.5: Variance of the E-field.
22
(a) Ex, normalized to the [−1, 1]-range
(b) Ey, normalized to the [−1, 1]-range
(c) Ez, normalized to the [−1, 1]-range
(d) |E|, normalized to the [0, 1]-range
0
0.2
0.4
0.6
0.8
1
|E|
−1.0
−0.5
0.0
0.5
1.0
Ei
Figure 6.6: Normalized components and magnitude of the E-field of the ninth mode (i.e. the
accelerating mode).
At the top we see the cross-sections in the z-plane at the cutting planes
marked in red. In each subfigure, the field plot of the nominal design (top), the mean geometry
D0 (middle; cf. Figure 6.2, subplot (a)) and a sampled deformed cavity Dt (bottom).
23
"
"In the marketing field, understanding consumer heterogeneity, which is the internal or psychological difference among consumers that cannot be captured by behavioral logs, has long been a critical challenge. However, a number of consumers today usually post their evaluation on the specific product on the online platform, which can be the valuable source of such unobservable differences among consumers. Several previous studies have shown the validity of the analysis on text modality, but on the other hand, such analyses may not necessarily demonstrate sufficient predictive accuracy for text alone, as they may not include information readily available from cross-sectional data, such as consumer profile data. In addition, recent advances in machine learning techniques, such as large-scale language models (LLMs) and multimodal learning have made it possible to deal with the various kind of dataset simultaneously, including textual data and the traditional cross-sectional data, and the joint representations can be effectively obtained from multiple modalities. Therefore, this study constructs a product evaluation model that takes into account consumer heterogeneity by multimodal learning of online product reviews and consumer profile information. We also compare multiple models using different modalities or hyper-parameters to demonstrate the robustness of multimodal learning in marketing analysis.","Deep learning is currently used in various fields. In the field such as image recognition, it is already common to obtain features that affect the target variable by the feature extraction of deep learning, and it is considered one of the most powerful advantages of deep learning. In the field of customer relationship management (CRM) in marketing, it also has been widely employed in various ways of prediction and demonstrated the validity of utilizing deep learning, such as customer segmentation, customer lifetime value (CLV), purchases (in the future or current session), churn, and other tasks.However, within the vast field of data science, a problematic factor, referred as consumer heterogeneity, has been identified, particularly in marketing. In marketing analysis, we combine a variety of behavioral log data; however, all the data recorded in the behavioral logs are the results of behaviors. The Differences in the psychological attributes among consumers that cause those behaviors cannot be obseerved by such logs.
Many previous studies have highlighted the importance of considering heterogeneity.","nanIn terms of combining various data, multimodal learning has become widely popular in machine learning applications in general. It combines and learns different types of multiple data (i.e., modality), for example, audio and its corresponding text, in a state close to the original data. This method allows modeling that considers the relationships between modalities. However, in the marketing field, analysis using such a variety of data is generally conducted by variablizing each data into a single dataset.
For the actual use of these methods in marketing, especially regarding word-of-mouth texts, this study predicts user review (that is, sentiment) for smartphone games using review texts collected from the Google Play Store, which compare several techniques to obtain word representations, including RNN, CNN, LSTM, BERT, mBERT, DistilBERT, and RoBERTa. In addition, a study adopted BERT to predict customers’ ratings of hotels in seven criteria (e.g., overall ratings, value, and service) simultaneously, using online review texts for the recommender system. They indicated that BERT could predict a more accurate rating by considering the context of a review text. Another study regarding social media marketing adopts BERT to capture the social media engagements and comments of influencers of eight categories on Instagram.nannan","Based on previous studies of multimodal learning using time series and cross-sectional data, we present a conceptual model of multimodal deep learning that integrates three smaller neural network components, referred to as sub-networks or subnets. In this study, they are called text-specific subnetworks (X1-subnet), cross-sectional-data-specific subnetworks (X2-subnet), and output subnetworks (output-subnet). The construction of each subnet is described in the following subsections.nan","The best models for each modality are listed in Table 2. First, in both the training and test results, the prediction accuracy improved the most with multimodal learning. Although these results do not allow us to evaluate whether multimodal learning immediately improves the prediction accuracy, multimodal learning with the bert-base-japanese-v3 model shows the highest prediction accuracy for the test data, which indicates that the extension to multimodal learning alone does not improve the prediction accuracy. For multimodal learning, we need to carefully consider factors such as the task to be solved with the multimodal model, the relationship between the modalities, and the quality of the data, because several previous studies have shown that the prediction accuracy in multimodal learning can be influenced by such factors.","This study attempts to construct a multimodal deep learning model that predicts the user ratings of a product using both review text and user profile data simultaneously to account for consumer heterogeneity. First, as academic implications, even when both review and demographic data are relatively small, both the best model and the average score by modality, the prediction accuracy is the best when they are combined, which indicates that multimodal learning that accounts for consumer heterogeneity allows analysis with high robustness and generalizability. Second, it can be shown that, at least when dealing with relatively short sentences such as those used in this study (lenmax = 200), a larger BERT model does not necessarily contribute to an improvement in prediction accuracy. This implies that, particularly in small datasets like those used in this study, converting sentences into word embeddings with BERT is important while the scale of the BERT model is not necessarily critical. Next, the conceptual model presented in this study, as a way to extend review data with cross-sectional data or as a way to extend cross-sectional data with review data, has a potential to be extended to various prediction models in marketing analysis with higher prediction accuracy, compared to conventional methods.",Multimodal Deep Learning of Word-of-Mouth Text and Demographics to Predict Customer Rating: Handling Consumer Heterogeneity in Marketing,Junichiro Niimi,"MULTIMODAL DEEP LEARNING OF WORD-OF-MOUTH TEXT
AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING:
HANDLING CONSUMER HETEROGENEITY IN MARKETING
A PREPRINT
Junichiro Niimi* 1,2
1Meijo University
2RIKEN AIP
ABSTRACT
In the marketing field, understanding consumer heterogeneity, which is the internal or psychological
difference among consumers that cannot be captured by behavioral logs, has long been a critical
challenge. However, a number of consumers today usually post their evaluation on the specific
product on the online platform, which can be the valuable source of such unobservable differences
among consumers. Several previous studies have shown the validity of the analysis on text modality,
but on the other hand, such analyses may not necessarily demonstrate sufficient predictive accuracy
for text alone, as they may not include information readily available from cross-sectional data, such
as consumer profile data. In addition, recent advances in machine learning techniques, such as
large-scale language models (LLMs) and multimodal learning have made it possible to deal with
the various kind of dataset simultaneously, including textual data and the traditional cross-sectional
data, and the joint representations can be effectively obtained from multiple modalities. Therefore,
this study constructs a product evaluation model that takes into account consumer heterogeneity by
multimodal learning of online product reviews and consumer profile information. We also compare
multiple models using different modalities or hyper-parameters to demonstrate the robustness of
multimodal learning in marketing analysis.
Keywords Multimodal Learning · BERT · Customer Relationship Management · Word-of-Mouth · LLM
1
Introduction
Deep learning is currently used in various fields. In the field such as image recognition, it is already common to obtain
features that affect the target variable by the feature extraction of deep learning [1], and it is considered one of the most
powerful advantages of deep learning [2]. In the field of customer relationship management (CRM) in marketing, it
also has been widely employed in various ways of prediction and demonstrated the validity of utilizing deep learning,
such as customer segmentation [3], customer lifetime value (CLV) [4, 5], purchases (in the future or current session)
[6, 7, 8], churn [9, 10], and other tasks [11].
However, within the vast field of data science, a problematic factor, referred as consumer heterogeneity, has been iden-
tified, particularly in marketing. In marketing analysis, we combine a variety of behavioral log data; however, all the
data recorded in the behavioral logs are the results of behaviors. The Differences in the psychological attributes among
consumers that cause those behaviors cannot be obseerved by such logs. Many previous studies have highlighted the
importance of considering heterogeneity [12, 13].
In terms of combining various data, multimodal learning [14, 15] has become widely popular in machine learning
applications in general. It combines and learns different types of multiple data (i.e., modality), for example, audio and
its corresponding text, in a state close to the original data. This method allows modeling that considers the relationships
between modalities. However, in the marketing field, analysis using such a variety of data is generally conducted by
variablizing each data into a single dataset [16].
∗jniimi@meijo-u.ac.jp
arXiv:2401.11888v1  [cs.CE]  22 Jan 2024
Multimodal Learning of Word-of-Mouth and Demographics
A PREPRINT
This paper is organized as follows. The previous studies are reviewed in Section 2, and the methodology (proposed
model and used data) is shown in Section 3. An overview of the analysis is described in Section 4. The results of the
analysis are presented in Section 5. Finally, we present the discussion and conclusions in Section 6.
2
Related Work
2.1
Text Analysis in Marketing
Natural Language Processing (NLP) techniques have been proposed for analyzing text modalities using deep learning.
Among them, bidirectional encoder representations from transformers (BERT) [17] and their extensions [18, 19] are
some of the most popular methods because of their wide applicability, which is not limited to NLP tasks, such as ma-
chine translation and question answering. For pre-training, it has learned a contextual language representations using
large text corpus; therefore, it can be further applied to various downstream tasks through fine-tuning. Many models
have already undergone pretraining with extensive data, enabling effective analysis with relatively small datasets. In
recent years, an extension of BERT into multiple languages, known as multilingual BERT (mBERT), has emerged, and
several models for the Japanese version, which is the focus of this study, have also been proposed (e.g., BERT models
trained by Cyber Agent [20] and Tohoku NLP Group [21]). Before the advent of BERT, vectorization methods such
as word2vec [22] and doc2vec [23] had been used to map sentences to feature vectors. Compared with techniques that
produce a single- word embedding representation, the advantage of BERT is that it is context-wise, which means that
it produces a representation based on other terms in the sentence [17, 24].
For the actual use of these methods in marketing, especially regarding word-of-mouth texts, this study [24] predicts
user review (that is, sentiment) for smartphone games using review texts collected from the Google Play Store, which
compare several techniques to obtain word representations, including RNN [25], CNN [26], LSTM [27], BERT,
mBERT, DistilBERT [18], and RoBERTa [19]. In addition, a study [28] adopted BERT to predict customers’ ratings
of hotels in seven criteria (e.g., overall ratings, value, and service) simultaneously, using online review texts for the
recommender system. They indicated that BERT could predict a more accurate rating by considering the context
of a review text. Another study regarding social media marketing [29] adopts BERT to capture the social media
engagements and comments of influencers of eight categories on Instagram. Thus, several studies have used BERT to
map word-of-mouth documents to feature maps to predict customer evaluations.
In addition, while BERT has two different scale: Base (approx. 110 million parameters) and Large (approx. 340
million parameters), the extent to which such scales of BERT affect the result of the analysis for relatively small text
data is yet to be clarified, particularly in non-English models2.
2.2
Multimodal Learning in Marketing
Multimodal learning involves learning representations from multiple modalities such as images, videos, audio, and
text. A combination of these enables the construction of a robust learner based on the relationships among modalities
that cannot be obtained by learning a single modality [15]. There are a variety of applications, including the integration
of information from multiple sources and interconversion between modalities, applied to a wide range of academic
fields such as medicine, human-computer interaction, biometrics, and remote sensing [30]. In contrast to this rise in
multimodal learning, research on multimodal learning in the marketing field is relatively scarce. One possible reason is
that marketing data analysis involves a mixture of various data in different formats, such as server logs, ID-POS, GPS,
survey responses, and customer information. Although analyses combining various datasets are widely practiced, they
are typically conducted by variablizing multiple data and merging them into a single set for analysis 3.
However, there are a few notable instances, such as studies that construct a multimodal deep learning model to predict
consumer loyalty with the source–target attention mechanism [31], which datasets with different dimensionality are
input simultaneously; however, by using bidirectional LSTM, the time-series data is converted to two-dimensional and
unified to a single representation with cross-sectional data by feature fusion4.
2.3
Consumer Heterogeneity in Marketing
Both the studies mentioned above highlight that it is possible to enhance discriminative power by considering demo-
graphic variables as a context affecting actual behavior. In the CRM context, the problem of consumer heterogeneity
has long been highlighted, where even consumers who perform same behavior have unobservable differences [12],
2The results are comparable between the model scales since both BERT models of Base and Large are designed to handle the
same number of input tokens.
3The information loss occurring in the process of converting behavioral log into cross-sectional data may lead to a decrease
in the accuracy of marketing analysis using deep learning. In other words, the manually variablized features cannot be sufficient
statistics for the task on their own.
4Notably, in both studies, data fusion is conducted twice in one network structure: source-target attention and feature fusion.
2
Multimodal Learning of Word-of-Mouth and Demographics
A PREPRINT
such as demographic and psychographic differences. In general, this kind of difference is unobservable in behavioral
logs such as ID–POS data. Several studies have attempted to capture such differences using statistical modeling, such
as structural equation modeling (SEM) and Bayesian mixture models [32, 33]. When it comes to the evaluation of the
results addressed in this study, even among customers who have the same rating (number of stars), the reasons for the
rating must differ; however, this difference cannot be observed in the cross-sectional data. In recent years, with the
availability of a variety of large-scale data, document data such as word-of-mouth on online platforms have been used
in the aforementioned studies as an important source for understanding consumer preferences [34].
Thus, several studies have utilized BERT and text data to make predictions that account for consumer heterogeneity.
In general, the review text has more or less amount of reason why they evaluate the venue in such rating. In other
words, the data, although it is behavioral log, can be the important source of understanding consumer heterogene-
ity. Therefore, this study utilize product review as a means to capture consumer heterogeneity to predict with better
performance.
On the other hand, relying sorely on the feature extraction of machine learning is not advisable because domain
knowledge is not incorporated into the analysis. In fact, several studies have shown that in multimodal learning, the
combination of extracted features and handcrafted variables achieves the best prediction accuracy [1, 16]. Therefore,
this study constructs a multimodal deep learning model that combines the review text with handcrafted user profile
variables to achieve a robust and precise model.
3
Proposed Model
Based on previous studies of multimodal learning using time series and cross-sectional data [17], we present a concep-
tual model of multimodal deep learning that integrates three smaller neural network components, referred to as sub-
networks or subnets. In this study, they are called text-specific subnetworks (X1-subnet), cross-sectional-data-specific
subnetworks (X2-subnet), and output subnetworks (output-subnet). The construction of each subnet is described in
the following subsections.
3.1
Text-Specific Subnetworks (X1-subnet)
First, we describe the structure of the X1-subnet, which specializes in processing text data. The purpose of this subnet
is to map word-of-mouth texts (whose lengths differ among users) onto a two-dimensional single-feature map. The
actual component included two layers: a tokenization layer and a BERT layer. In this study, BERT was used to obtain
embedded representations of text data; therefore, the X1-subnet (tokenizer and BERT layers) is freezed throughout the
actual training process, which means that the parameters of the subnet are held fixed in the pre-trained state.
The text data contains the review of N users and is processed with tokenizer into fixed-length of (N, lenmax) tensor
(lenmax represents the maximum number of tokens of the text among N sets of data). After processing in the BERT
layer with batch size n, we obtained the feature map as the pooler output of BERT as the (n, lenmax) tensor.5
3.2
Cross-Sectional-Data-Specific Subnetworks (X2-subnet)
Next, we describe the structure of the X2-subnet handling two-dimensional cross-sectional data. This subnet consists
of a typical deep neural network with feed-forward layers (FFLs). An input layer for the X2-subnet receives a 2-
dimensional tensor (n, Jin), with batch size n and Jin variables in the cross-sectional data. After processing, (n, Jout)
feature map is obtained as the output.
3.3
Output Subnetworks (output-subnet)
Textual and cross-sectional data, processed in parallel in each dedicated subnet are sent to the output subnet. In this
subnet, both the feature maps, stemming from X1 and X2, are unified as a joint representation by feature fusion layer6,
and the obtained feature map would be (n, lenmax + Jout) tensor. After the feature fusion, the joint representation
is processed through one or more FFLs, and finally classification is conducted with Softmax layer. All the FFLs
throughout the model employed a hyperbolic tangent function (tanh) for layer activation.
4
Analysis
4.1
Dataset
To conduct this analysis, we randomly selected one product from the women’s cosmetics market. The target product
had to be well-recognized in Japan and already out of production. We collected data from 1040 participants online.
5The maximum length of the text lenmax ≤ 512 since it cannot be exceeded the maximum number of tokens that BERT can
deal with.
6However, a more rigorous call of feature fusion in this study should be ""intermediate fusion"" [30].
3
Multimodal Learning of Word-of-Mouth and Demographics
A PREPRINT
Softmax Layer (n, 2)
Predicted Values (n, 1)
Feed-Forward Layers (n, K)
Serial Feature Fusion (n, len    +J   )
Feed-Forward Layers
(n, J   )
Tokenizer & BERT
X1-subnet
X2-subnet
output-subnet
(n, len    )
Word-of-Mouth Data
Demographic Data
(n, J  )
(n, 1)
max
max
in
out
out
Figure 1: Model Architecture
The actual data contains three kinds of modalities: rating for the product (7-point Likert scale), word-of-mouth texts
for the product, and demographic information 7. Finally, the sample size is 1532 (i.e., N = 1532).
Next, we process the survey text in the general way of preprocessing in NLP. The text contained line breaks, pic-
tographs, emoticons, and other characters that were not appropriate for analysis. Therefore, in the preprocessing stage,
these elements were replaced with periods only when they were placed at the end of the sentence; otherwise, the ele-
ments were removed. Subsequently, all successive punctuation periods are merged into a single period. Consequently,
the maximum length of the text data was set as lenmax = 200.
4.2
Model Evaluation
This study adopts a simple binary classification for the task, similar to previous studies [24]. User ratings were
dichotomized into two classes based on the rating scale. Six and seven stars were classified as Loyalty = 1 (loyalty is
high) and Loyalty = 0 (loyalty is not high), respectively. The obtained dataset was divided into a training set (75%)
and a test set (25%). The model performance was evaluated using both training and test accuracies and the number of
epochs to converge.
This study aims to validate several key points to compare the models. First, to validate the usefulness of multimodal
learning in marketing, we construct three basic models according to their modalities: X1-modal, X2-modal, and
7Since the cosmetic we focus on is a product intended for use by females, the survey was limited to females who purchased the
products themselves.
4
Multimodal Learning of Word-of-Mouth and Demographics
A PREPRINT
Table 1: Model Settings
Parameters
Candidates
Model Parameters
Number of Epochs
200 (with Early-Stopping in 50 Epochs)
Batchsize (n)
64
Optimizer
{Adam, Adamax, Nadam}
Loss Function
Binary Cross-entropy
X1-subnet
Structure
mBERT (Japanese)
Model
{bert-base-japanese-v3,
bert-base-japanese-char-v3,
bert-large-japanese-v2,
bert-large-japanese-char-v2}
X2-subnet
Number of Hidden Layers
2
Number of Neurons in the Layer
10
Activation Function
tanh
output-subnet
Number of Hidden Layers
2
Number of Neurons in the Layer
10
Activation Function
tanh
Note. tanh stands for hyperbolic tangent function.
multimodal. This comparison allowed for the verification of changes in prediction accuracy by combining multiple
modalities. Especially in marketing analysis, review texts can be a valuable source for comprehending consumer
heterogeneity in user ratings.
Next, we examined the change in prediction accuracy using multiple pre-trained models within BERT. Regarding
the Japanese language model, several models of different scales (that is, numbers of parameters), different training
datasets, and tokenization methods (in particular, with regard to Japanese models, some are trained on a word-by-word
basis, whereas others are trained on a character-by-character basis) have already been proposed, and the extent to
which the prediction accuracy differs depending on the use of these models is yet to be clarified. Nowadays, we can
easily switch the pre-trained model in BERT by changing one line of the code, which makes it easy to compare the
accuracies of different pre-trained models. As it has already been shown in the literatures [24, 28, 29] that models
using BERT-like architectures achieve higher accuracy compared to those using Collaborative Filtering, LSTM, CNN,
and other benchmark models, this study sticks to comparing among multiple models using BERT.
The model settings are shown in Table 1. In the case where several candidates are shown in the setting, we utilize a
grid search to explore the settings that maximize the test accuracy. For example, we compared the prediction accuracy
among the four pre-trained models in BERT (bert base/large in word/char) and among three optimizers (Adaptive
Moment Estimation, Adam [35], Adamax [35], and Nesterov-accelerated Adaptive Moment Estimation, Nadam [36]).
The training process was conducted with a maximum of 200 epochs and 64 batch sizes. Early Stopping [37] was
employed with a patience of 50 epochs, which terminates the training if no improvement in the accuracy of the
validation data was observed within 50 epochs.
5
Results
The best models for each modality are listed in Table 2. First, in both the training and test results, the prediction
accuracy improved the most with multimodal learning. Although these results do not allow us to evaluate whether
multimodal learning immediately improves the prediction accuracy, multimodal learning with the bert-base-japanese-
v3 model shows the highest prediction accuracy for the test data, which indicates that the extension to multimodal
learning alone does not improve the prediction accuracy. For multimodal learning, we need to carefully consider
factors such as the task to be solved with the multimodal model, the relationship between the modalities, and the
quality of the data, because several previous studies have shown that the prediction accuracy in multimodal learning
can be influenced by such factors [38].
5
Multimodal Learning of Word-of-Mouth and Demographics
A PREPRINT
Table 2: Result I (Accuracy in Train and Test Data)
Train
Test
Epochs
BERT Model / Modality
Both
X1
X2
Both
X1
X2
Both
X1
X2
cl-tohoku/bert-base-japanese-v3
0.705
0.708
-
0.711
0.703
-
36
38
-
cl-tohoku/bert-base-japanese-char-v3
0.683
0.686
-
0.690
0.678
-
41
42
-
cl-tohoku/bert-large-japanese-v2
0.712
0.698
-
0.695
0.699
-
95
190
-
cl-tohoku/bert-large-japanese-char-v2
0.681
0.658
-
0.690
0.703
-
58
18
-
None
-
-
0.555
-
-
0.623
-
-
29
Note. Numbers in bold represent the best accuracy in training and testing and the best epochs.
Table 3: Result II (Group Average)
Optimizer
Train
Test
Epochs
Modality
Train
Test
Epochs
Adam
0.646
0.659
58.4
Both
0.681
0.683
70.2
Adamax
0.627
0.651
61.7
X1
0.685
0.681
58.9
Nadam
0.646
0.662
46.2
X2
0.546
0.604
35.5
Note. Numbers in bold represent the best accuracy in training and testing and the best epochs.
Second, in the comparison of four pre-trained models, note that upgrading to the BERT Large model did not always
lead to a significant improvement in accuracy. Regarding the scale of the model, there is a tendency for an increase in
the number of epochs required for model training with bert-large-japanese-v2, and the X1-modality has reached 190
epochs. This result might indicate the potential for further improvement with an increase in the number of epochs;
however, large model is not always necessary because other models have shown higher prediction accuracy in fewer
epochs.
In addition, a comparison of the mean accuracies among certain conditions is listed in Table 3. In terms of the
optimizer, although the training accuracy is almost the same between Adam and Nadam, the latter is better in terms
of both test accuracy and the best epoch, which means that, on average, Nadam achieved a higher generalization
performance in a shorter time8. In terms of modality, the X1-modal shows high accuracy, as listed in Table 2, for the
training process; however, multimodal learning, on average, still shows the highest accuracy for the test data. This
result suggests that, on average, multimodal learning improves generalization performance. As expected, the accuracy
of the analysis using only X2-modal remains low for both training and testing, although the model converged early.
6
Conclusions
This study attempts to construct a multimodal deep learning model that predicts the user ratings of a product using
both review text and user profile data simultaneously to account for consumer heterogeneity. First, as academic
implications, even when both review and demographic data are relatively small, both the best model and the average
score by modality, the prediction accuracy is the best when they are combined, which indicates that multimodal
learning that accounts for consumer heterogeneity allows analysis with high robustness and generalizability. Second,
it can be shown that, at least when dealing with relatively short sentences such as those used in this study (lenmax =
200), a larger BERT model does not necessarily contribute to an improvement in prediction accuracy. This implies
that, particularly in small datasets like those used in this study, converting sentences into word embeddings with BERT
is important while the scale of the BERT model is not necessarily critical. Next, the conceptual model presented in
this study, as a way to extend review data with cross-sectional data or as a way to extend cross-sectional data with
review data, has a potential to be extended to various prediction models in marketing analysis with higher prediction
accuracy, compared to conventional methods.
Finally, owing to the constraints of data collection, this study relies on consumer ratings as a proxy for behavioral
loyalty and predicts whether it is high or not using the proposed model. However, this methodology can be extended
to purchase prediction models by incorporating data that include purchase history and more demographics. A few
models in previous studies [31] fused modalities twice within a model through the use of attention mechanisms and
feature fusion, which aims to enhance prediction accuracy and robustness. Moreover, regarding the actual analysis,
further improvements in accuracy can be expected by adopting techniques such as dropout [39]. In addition, although
8Note that the time required for training one epoch did not differ significantly among optimizers.
6
Multimodal Learning of Word-of-Mouth and Demographics
A PREPRINT
the multimodal learning model developed in this study, which utilizes actual review texts as a source of information
for understanding consumer heterogeneity, is based on the assumption that consumer heterogeneity is embedded in the
review texts, the actual causal relationships need to be carefully examined for the presence of potential endogeneity
between the variables.
Acknowledgements
All computations in this study were conducted using RAIDEN, which is a computational infrastructure hosted by
RIKEN AIP. We would like to express our gratitude to all the members of AIP who maintain the system.
References
[1] Loris Nanni, Stefano Ghidoni, and Sheryl Brahnam. Handcrafted vs. non-handcrafted features for computer
vision classification. Pattern Recognition, 71:158–172, 2017.
[2] Yoshua Bengio. Deep learning of representations: Looking forward. In International Conference on Statistical
Language and Speech Processing, pages 1–37. Springer, 2013.
[3] Licheng Zhao, Yi Zuo, and Katsutoshi Yada. Sequential classification of customer behavior based on sequence-
to-sequence learning with gated-attention neural networks. Advances in Data Analysis and Classification, pages
1–33, 2022.
[4] Rafet Sifa, Julian Runge, Christian Bauckhage, and Daniel Klapper. Customer lifetime value prediction in non-
contractual freemium settings: Chasing high-value users using deep neural networks and smote. 2018.
[5] Pei Pei Chen, Anna Guitart, Ana Fernández del Río, and Africa Periánez. Customer lifetime value in video games
using deep learning and parametric models. In 2018 IEEE international conference on big data (big data), pages
2134–2140. IEEE, 2018.
[6] Jan Valendin, Thomas Reutterer, Michael Platzer, and Klaudius Kalcher. Customer base analysis with recurrent
neural networks. International Journal of Research in Marketing, 39(4):988–1018, 2022.
[7] Arthur Toth, Louis Tan, Giuseppe Di Fabbrizio, and Ankur Datta. Predicting shopping behavior with mixture of
rnns. In eCOM@ SIGIR, 2017.
[8] Long Guo, Lifeng Hua, Rongfei Jia, Binqiang Zhao, Xiaobo Wang, and Bin Cui. Buying or browsing?: Predict-
ing real-time purchasing intent using attention-based deep network with multiple behavior. In Proceedings of the
25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1984–1992, 2019.
[9] C Gary Mena, Arno De Caigny, Kristof Coussement, Koen W De Bock, and Stefan Lessmann. Churn prediction
with sequential data and deep neural networks. a comparative analysis. arXiv preprint arXiv:1909.11114, 2019.
[10] Philip Spanoudes and Thomson Nguyen. Deep learning in customer churn prediction: unsupervised feature
learning on abstract company independent feature vectors. arXiv preprint arXiv:1703.03869, 2017.
[11] Mainak Sarkar and Arnaud De Bruyn. Lstm response models for direct marketing analytics: Replacing feature
engineering with deep learning. Journal of Interactive Marketing, 53(1):80–95, 2021.
[12] Peter E Rossi, Robert E McCulloch, and Greg M Allenby. The value of purchase history data in target marketing.
Marketing Science, 15(4):321–340, 1996.
[13] Werner J Reinartz and Vita Kumar. The impact of customer relationship characteristics on profitable lifetime
duration. Journal of marketing, 67(1):77–99, 2003.
[14] Nitish Srivastava and Russ R Salakhutdinov. Multimodal learning with deep boltzmann machines. Advances in
neural information processing systems, 25, 2012.
[15] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multimodal deep
learning. In ICML, 2011.
[16] Junichiro Niimi and Takahiro Hoshino. Predicting purchases with using the variety of customer behaviors -
analysis of the purchase history and the browsing history by deep learning-. Transactions of the Japanese Society
for Artificial Intelligence, 32(2):B–G63_1–9, 2017.
[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
7
Multimodal Learning of Word-of-Mouth and Demographics
A PREPRINT
[18] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of BERT:
smaller, faster, cheaper and lighter. arXiv, 2019.
[19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint
arXiv:1907.11692, 2019.
[20] Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh
Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Wang
Phil, and Samuel Weinbach. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch, 8 2021.
[21] Tohoku NLP Group. cl-tohoku/bert-japanese (github: https://github.com/cl-tohoku/bert-japanese), 2023.
[22] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in
vector space. arXiv preprint arXiv:1301.3781, 2013.
[23] Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In International confer-
ence on machine learning, pages 1188–1196. PMLR, 2014.
[24] Zeynep Hilal Kilimci. Prediction of user loyalty in mobile applications using deep contextualized word repre-
sentations. Journal of Information and Telecommunication, 6(1):43–62, 2022.
[25] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating
errors. nature, 323(6088):533–536, 1986.
[26] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[27] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,
1997.
[28] Yuanyuan Zhuang and Jaekyeong Kim. A bert-based multi-criteria recommender system for hotel promotion
management. Sustainability, 13(14):8039, 2021.
[29] Seungbae Kim, Xiusi Chen, Jyun-Yu Jiang, Jinyoung Han, and Wei Wang. Evaluating audience loyalty and
authenticity in influencer marketing via multi-task multi-relational learning. In Proceedings of the International
AAAI Conference on Web and Social Media, volume 15, pages 278–289, 2021.
[30] Dhanesh Ramachandram and Graham W Taylor. Deep multimodal learning: A survey on recent advances and
trends. IEEE signal processing magazine, 34(6):96–108, 2017.
[31] Maher Ala’raj, Maysam F Abbod, and Munir Majdalawieh. Modelling customers credit card behaviour using
bidirectional lstm neural networks. Journal of Big Data, 8(1):1–27, 2021.
[32] Carsten Hahn, Michael D Johnson, Andreas Herrmann, and Frank Huber. Capturing customer heterogeneity
using a finite mixture pls approach. Schmalenbach Business Review, 54:243–269, 2002.
[33] Thomas Otter, Regina Tüchler, and Sylvia Frühwirth-Schnatter. Capturing consumer heterogeneity in metric
conjoint analysis using bayesian mixture models. International Journal of Research in Marketing, 21(3):285–
297, 2004.
[34] Silvana Aciar, Debbie Zhang, Simeon Simoff, and John Debenham. Recommender system based on consumer
product reviews. In 2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Con-
ference Proceedings)(WI’06), pages 719–723. IEEE, 2006.
[35] Diederik P Kingma and Jimmy Ba.
Adam:
A method for stochastic optimization.
arXiv preprint
arXiv:1412.6980, 2014.
[36] Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
[37] Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55–69. Springer, 1998.
[38] Douwe Kiela and Léon Bottou. Learning image embeddings using convolutional neural networks for improved
multi-modal semantics. In Proceedings of the 2014 Conference on empirical methods in natural language pro-
cessing (EMNLP), pages 36–45, 2014.
[39] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a
simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–
1958, 2014.
8
"
"In this work, we explore the interplay between an elastic sense of subjective identity and its impact on notions of fairness in allocation. An elastic sense of identity in agents is known to lead to responsible decision-making in non-cooperative, non-zero-sum games like Prisoners’ Dilemma, and is a desirable feature to add into agent models. We extend a recently proposed model of responsible agency called Computational Transcendence (CT) which models an elastic identity in autonomous agents and it has been demonstrated to result in emergent responsible behaviour.","The allocation of limited resources amongst individuals or groups with competing needs often creates a dilemma especially when conflicting goals, interests, or values are involved. These dilemmas are relevant in the context of both individuals in human societies as well as autonomous agents in multi-agent systems.","nanIn this section, we discuss in detail the ultimatum game (UG) and elaborate rational human behaviour in such a scenario. We also describe some existing approaches to model fairness in the ultimatum game. Further, we discuss a few frameworks that point to the importance of the subjective notion of fairness in the agents.nannan","Consider the scenario, where the resource R is assigned the value 1, and it is to be divided among two agents with the roles of Allocator and Recipient. The allocator proposes the allocation of the resource valued 1 as x for itself and 1 – x for the recipient.nan","We extended the identity-based model of agents - computational transcendence, to allocation games, and attempted to understand different factors which affect the resultant game states. We explored the interplay of the notion of fairness with subjective identity in the context of allocation games.","Allocation games are important to model scenarios of limited resource allocation. We noted that just incorporating responsible behaviour in agents is not sufficient for fair deals in this scenario. The baseline model of transcendence was found to be insufficient and unfair in the case of the allocation games since the agents factored in the utility of all the aspects they identified with, without considering the variation in the absolute value of utilities.",Modelling the Dynamics of Identity and Fairness in Ultimatum Game,"Janvi Chhabra, Jayati Deshmukh, Srinath Srinivasa","MODELLING THE DYNAMICS OF IDENTITY AND FAIRNESS IN
ULTIMATUM GAME
Janvi Chhabra
International Institute of Information Technology, Bangalore
26/C, Electronics City Phase 1
Bangalore, Karnataka, India
janvi.chhabra@iiitb.ac.in
Jayati Deshmukh
International Institute of Information Technology, Bangalore
26/C, Electronics City Phase 1
Bangalore, Karnataka, India
jayati.deshmukh@iiitb.org
Srinath Srinivasa
International Institute of Information Technology, Bangalore
26/C, Electronics City Phase 1
Bangalore, Karnataka, India
sri@iiitb.ac.in
ABSTRACT
Allocation games are zero-sum games that model the distribution of resources among multiple agents.
In this paper, we explore the interplay between an elastic sense of subjective identity and its impact on
notions of fairness in allocation. An elastic sense of identity in agents is known to lead to responsible
decision-making in non-cooperative, non-zero-sum games like Prisoners’ Dilemma, and is a desirable
feature to add into agent models. However, when it comes to allocation, an elastic sense of identity
can be shown to exacerbate inequities in allocation, giving no rational incentive for agents to act
fairly towards one another. This lead us to introduce a sense of fairness as an innate characteristic
of autonomous agency. For this, we implement the well-known Ultimatum Game between two
agents, where their elastic sense of self (controlled by a parameter called γ) and a sense of fairness
(controlled by a parameter called τ) are both varied. We study the points at which agents find it no
longer rational to identify with the other agent, and uphold their sense of fairness, and vice versa.
Such a study also helps us discern the subtle difference between responsibility and fairness when it
comes to autonomous agency.
Keywords Identity, Allocation, Allocation game, Responsibility, Fairness
1
Introduction
The allocation of limited resources amongst individuals or groups with competing needs often creates a dilemma
especially when conflicting goals, interests, or values are involved. This can be classified under a broad umbrella of
responsibility dilemma, where an agent faces a conflict between an individually optimal state and a collectively or
socially optimal state. These dilemmas are relevant in the context of both individuals in human societies as well as
autonomous agents in multi-agent systems [7].
In such dilemmas, the classical game theory models agents to act rationally i.e. they are modelled to choose actions
which maximise their payoff. However, often human societies don’t demonstrate such selfish behaviour; humans in such
arXiv:2401.11881v1  [cs.GT]  22 Jan 2024
Modelling the Dynamics of Identity and Fairness in Ultimatum Game
dilemmas do consider factors more than just their personal benefit [23]. Responsible behaviour often emerges in human
populations even in times of extreme conflict and oppression [3]. Such behaviour indicates that agents’ decision-making
should be more nuanced, maximising not just their own payoff but also being aware of the consequences of their actions
on others.
The notion of fairness in agents has been widely studied in resource allocation scenarios [1]. The notion of fairness of
an agent attributes the allocation which an agent perceives as fair.
Humans facing the dilemma in allocation games demonstrate a preference towards fairness in addition to personal
benefit [9]. They perceive the allocation through the lens of their notion of fairness and hence the utility function which
they try to maximise is also influenced by their fairness preference.
The notion of fairness in agents has been modelled in diverse ways. In some models, agents’ notion of fairness is
modelled as conforming with norms such as inequity aversion and reciprocity, where adhering to these norms is
incorporated into the agents’ utility function [12]. Another way the notion of fairness has been approached is by
considering the influence of emotion and cognition in agents’ utility functions [24]. Reinforcement learning based
approaches have also been used by agents to learn human strategies in the scenario of an ultimatum game.
We extend a recently proposed model of responsible agency called Computational Transcendence (CT) which models
an elastic identity in autonomous agents and it has been demonstrated to result in emergent responsible behaviour [10].
We introduce a notion of fairness into this model and then agents using this extended model are evaluated in the context
of allocation games.
In this paper, we specifically look at the Ultimatum Game (UG) as an allocation scenario. This game highlights the
different aspects involved in the decision-making process such as the notion of fairness in agents. The decisions
of agents in social dilemmas such as UG have been interpreted in various ways, and in this work, we explore how
responsible autonomous agents with a model of fairness act specifically in the context of the ultimatum game.
2
Related Work
In this section, we discuss in detail the ultimatum game (UG) and elaborate rational human behaviour in such a
scenario. We also describe some existing approaches to model fairness in the ultimatum game. Further, we discuss a
few frameworks that point to the importance of the subjective notion of fairness in the agents. Finally, we introduce the
existing CT model and introduce fairness in this model in the context of UG.
2.1
Ultimatum Game
Figure 1: The Ultimatum Game
The Ultimatum Game (UG) was introduced by three economists Güth, Schmittberger, and Schwarze in 1982 [13]. In
the standard UG between two agents, there exist two roles: Allocator and Recipient. For a limited resource R, the
allocator proposes the allocation of the resource amongst itself and the recipient. Then the recipient can either accept or
reject the proposed allocation.
2
Modelling the Dynamics of Identity and Fairness in Ultimatum Game
• If the recipient accepts the proposed allocation, both allocator and recipient receive the split proposed by the allocator.
• If the recipient rejects the proposed allocation, both allocator and recipient receive nothing.
Figure 1 summarises the ultimatum game between the allocator and the recipient. This game has led to significant
research in analysing the trade-off between personal and comparative outcomes [18]. It has been introduced to test the
theoretical assumption that utility maximisation can be equated to maximising personal monetary payoffs [13]. Also,
it has given rise to numerous other games like the dictator game, trust game, gift exchange game, and public goods
game [25].
2.2
UG in Humans
If the players are assumed to be rational as per the classical rational theory, it means that players will optimise to
maximise their external payoff. Following this assumption, as per the game equilibrium, the allocator should offer the
smallest possible split of the resource to the recipient, as it knows that the recipient can either choose between obtaining
the proposed split by accepting or obtain nothing by rejecting the deal. And it makes rational sense for the recipient to
accept the proposed split as accepting the deal would maximise its payoff.
However, this rational assumption often is not observed in the case of experimental studies conducted on humans. The
experimental results, when humans played the ultimatum game as a part of the experimental setup suggest that the
modal offer that the allocator proposes is often around 50% and offers ≤ 20% have around 50% chance of getting
rejected [4] by the recipient. These findings suggest that humans don’t just maximise their external payoff, but are also
concerned about the comparative payoff, which indicates that they have a notion of fairness. UG became a popular
game in experimental studies to understand the behaviour of allocator and recipient when humans play these games. A
few of the scenarios which were experimentally tested are as follows:
• When the stake of the resource to be allocated is increased: Even a 20 times increase in stake didn’t have a
significant impact on allocator and recipient behavior [19].
• When the players were known or anonymous to each other: When the players were known to each other, the
allocator usually offered a greater split to the recipient as compared to when they were anonymous [2].
• When UG was played across various cultures: It was observed that variation in culture leads to very diverse
behaviour. In most dominant cultures, the game settles in the state as discussed so far. However, in some cultures, the
settled state of the game is different from usual. For example, in the Peruvian tribe in the Amazon, the allocator offers
a very low share (around 26%) to the recipient and faces a very high rejection rate from recipients [22]. In contrast to
this, in the case of Lamelara tribes of Indonesia, the allocator offers a very high share (around 58%) to the recipient
and yet approximately 37% of such deals are rejected by recipients [5].
This evidence from experimental studies suggests that humans in the UG scenario do not behave purely based on the
rational (payoff maximization) assumption. The evidence, especially from cultural studies indicates that the comparative
payoff is interpreted in diverse ways across cultures. This also indicates the effect of culture on the identity of humans,
which influences how they perceive such UG scenarios.
2.3
Modelling Fairness in UG
Next, we discuss various ways in which the modelling of agents’ behaviour has been approached specifically in the UG
scenario.
2.3.1
Inequity Aversion based Model
Fehr and Schmidt [12] proposed an inequity-aversion-based model of fairness in the UG context. An agent is inequity-
averse when it dislikes the inequitable allocation of resources irrespective of which side of the inequity it lies on. This
inequity-averse nature of agents affects their utility function. They assume that being on the disadvantageous side of
inequity gives a more negative utility as compared to being on the advantageous side of the inequity.
2.3.2
Reciprocity based Model
Rabin, Falk and Fischbacher proposed a reciprocity-based model of fairness [21, 11]. Using this model, agents evaluate
the “kindness” of the action of the other agent they are interacting with and respond or reciprocate accordingly. This
evaluation of kindness constitutes factoring in the consequences of the other agent’s actions and the underlying intentions
behind those actions.
3
Modelling the Dynamics of Identity and Fairness in Ultimatum Game
The utility function of the agents is influenced by the kindness of the action and the reciprocation reaction of agents to
that treatment. Agents’ actions are influenced by their first-order and second-order beliefs. They try to maximise their
utility and align their actions with their beliefs.
2.3.3
Emotion and Cognition based Model
Tamarit and Sánchez [24] proposed a model where agents’ behaviour is influenced by a combination of both their
emotions and cognition. It is inspired by Kanheman’s two-system theory [16], which proposes that the decision-making
process is a result of the interaction between two systems; System 1 which is fast, intuitive, associative, and emotionally
driven and System 2 which is slow, cognitively demanding, and analytical.
2.3.4
Social Decision-Making in Ultimatum Game
The Social Utility framework proposed for UG [14] categorises the factors which influence the behaviour of agents in
the game. These broad categorisations are as follows:
• Contextual Factors: Contextual factors in UG are changing the nature of the resource to be allocated, framing the
rules of the game, and the game being played across diverse sets of people.
• Characteristics of Players: This can be attributed to the social distance between the players i.e. whether they are
known or anonymous to each other. This can also account for the power dynamics between the players and the
personality of the players which can influence the motivation of the players.
• Characteristics of the Game: This is attributed to the way in which the game is structured. Different variants of UG,
such as the dictator game can also give an idea about the factors of the game itself which impact certain behaviour of
players.
2.4
Computational Transcendence
Computational Transcendence (CT) [10] models an elastic identity or a sense of self in autonomous agents using which
they can identify with external entities like other agents, groups and notions in the system. The sense of self of an agent
a is represented as S(a) = (Ia, da, γa) where Ia represents the identity set of the agent consisting of aspects it identifies
with, da is the semantic distance of the agent which denotes the perceived logical distance of an agent to each aspect in
its identity set and γa is the transcendence level of the agent which denotes the extent to which it identifies with others.
An agent a, with transcendence level γa identifies with an aspect o whose distance is da(o) with an attenuation factor of
γda(o)
a
. It has been shown that this model leads to emergent responsible behaviour by autonomous agents across diverse
scenarios. In this paper, first, we test transcended agents in the UG setup and then extend the model by incorporating a
notion of fairness in transcended agents.
Identifying with external entities affects how an agent’s internal valuation or utility is computed based on external
rewards or payoffs that may be received by different aspects in its identity set. For any aspect o ∈ Ia, let the term πi(o)
refer to the payoff obtained by aspect o in the game or system state i. Given this, the utility derived by agent a in system
state i is computed as follows:
ui(a) =
1
P
∀o∈I γda(o)
a
X
∀o∈I
γda(o)
a
πi(o)
(1)
3
Baseline Model
Consider the scenario, where the resource R is assigned the value 1, and it is to be divided among two agents with the
roles of Allocator and Recipient. The allocator proposes the allocation of the resource valued 1 as x for itself and 1 − x
for the recipient. Next, the recipient can either accept or reject the proposed allocation. If the recipient accepts this
allocation, both allocator and recipient receive the resource (referred to as payoff) of x and 1 − x respectively. On the
other hand, if the recipient rejects the proposed allocation, then both of them get nothing i.e. the payoff for both agents
is 0.
As discussed, the rational strategy using payoff maximization for the allocator is to propose a minimum possible split of
resource R to the recipient, and the recipient accepts it because rejecting the allocation results in a payoff of 0.
Transcended agents make the decision based on the expected utility of their choices. Suppose the two transcended
agents playing UG are represented as follows: a transcended agent a1 having a transcendence level of γ and semantic
4
Modelling the Dynamics of Identity and Fairness in Ultimatum Game
Figure 2: Utility of a transcended agent for possible splits for various γ values. The blue circles are Maximum Utility
Split and the red circles are Minimum Acceptable Utility
distance d with the other agent a2. The utility of a1, when it receives the split of x and a2 receives the split of 1 − x, is
computed in Equation 2 as follows:
util(a1) = x + γd(1 − x)
1 + γd
(2)
In this scenario,
• If a1 is the allocator: It proposes the maximum utility split, i.e. it computes the utility for all the possible splits and
proposes the split that gives it the maximum utility.
• If a1 is the recipient: It accepts if the proposed split gives ≥ its minimum acceptable utility i.e. it calculates the
utility of the proposed split and if it is ≥ 0 it accepts, else it rejects the proposal.
Figure 2, shows the utility of the transcended agent for all possible splits when the utility is computed using Equation 2
for various transcendence (γ) values. We can infer from the plots that the transcended agent as an allocator takes the
whole resource for itself (except at γ = 1) as it has maximum utility when it allocates the whole resource to itself, as
denoted by the blue circles. Also, as a recipient, the utility accrued is ≥ 0, as observed by red circles in Figure 2, thus it
makes sense to always accept the proposed allocation. Hence, the game state for almost all values of γ values involves
the allocator taking almost the whole resource (in this case, the split of 1) and the recipient accepting the proposed
allocation. According to the definition of transcendence, since the agents identify with each other, they also account
for each other’s payoff. However, in the absence of a notion of fairness, they propose and accept unfair allocations.
Next, we introduce a notion of fairness in transcended agents and evaluate whether it leads to more diverse and fair
allocations.
4
Notion of Fairness
The baseline model of transcendence does not result in fair and diverse deals in the case of allocation games. The
more the agents transcend, the more they identify with the other agent, they find unfair deals acceptable since the other
agent’s utility also contributes to their utility. So we extend this model by introducing a notion of fairness in agents
using a fairness threshold.
Fairness threshold, (τ) is defined as a threshold below which if an agent receives an allocation, it perceives it to be unfair.
The rationale for agents having a fairness threshold has been explored in the literature of UG, where agents reject any
offer below their fairness threshold [15, 20, 6]. The fairness threshold acts as a lens through which the agents perceive
the payoff they receive. The utility computation should account for the perceived payoff based on the fairness threshold
instead of just the payoff received. Thus, the process of utility computation consists of two stages, first calculating the
perceived payoff for the allocation and then computing the utility based on the perceived payoff with respect to the
fairness threshold.
5
Modelling the Dynamics of Identity and Fairness in Ultimatum Game
Figure 3: Perceived Payoff Function
4.1
Perceived Payoff
The risk or loss aversion behaviour [17] is usually demonstrated by humans. For the same value of loss or gain, humans
perceive the absolute value of loss to be greater than the equivalent gain. Risk aversion is relevant, especially for agents
having a notion of fairness and it can be used to compute the perceived payoff. Every agent has a fairness threshold (τ)
and it perceives the payoff it receives through the lens of its fairness threshold. An agent a having a fairness threshold of
τa receives allocation x. Its perceived payoff is computed using a sigmoid function as shown in Figure 3. The function
f(x − τa), which is a S-shaped function denotes the loss aversion behaviour of the agents [8]. The disparity between
the received payoff, x and fairness threshold τa, is δa. Then according to the function as shown in Figure 3, we can
state that the inequality |f(δa)| < |f(−δa)| holds.
4.2
Utility Computation
As discussed, the external payoff agents receive is perceived through a lens of fairness threshold. Further, the agents
compute their utility based on the perceived payoffs. As the utility computation also involves the payoff of the aspects
the agent identifies with, it is also replaced by the perceived payoff. Hence, the agent not only perceives its payoff
through the lens of its fairness threshold but also the split of the other agent it identifies with.
Agents perceive the payoff through the lens of their τ, thus this carries an assumption that other agents’ fairness
threshold is also the same as their fairness threshold. Hence, utility computation is a combination of perceived payoff
(Figure 3) and transcended utility computation (Equation 1). Assuming that the total resource to be allocated is 1, the
utility of the agent a, receiving the split s is described in Equation 3 as follows:
util(a) = f(s − τ) + γd ∗ f((1 − s) − τ)
1 + γd
(3)
So far, the utility computation of the agents with a fairness threshold of τ was discussed. Next, we explore different
possible ways to represent τ in agents. The following two alternatives for representing the fairness threshold in agents
will be elaborated and experimental results for both will be discussed in detail:
• τ as an agent-based characteristic: In this case, τ is a characteristic of the agent itself and it doesn’t vary for
different aspects in the identity set of the agent (similar to transcendence level, γ). Thus, no matter how close
(low semantic distance) or far (high semantic distance) the agent is to some aspect in its identity set, its τ is
the same. This model assumes that the fairness criteria of an agent is universal and same for everyone and thus
it does not vary depending on whom the agent is interacting with.
• τ as an association-based characteristic: In this case, the fairness threshold τ of an agent corresponds to each
aspect in its identity set and it depends on how the agent associates with that aspect (similar to the semantic
distance, d). In this case, there is a correlation between the semantic distance and fairness threshold for every
6
Modelling the Dynamics of Identity and Fairness in Ultimatum Game
Figure 4: The utility curve of the transcended agents (γ = 0.5, τ = 0.5) w.r.t possible splits while varying the semantic
distance of the agent with another player
aspect. This model assumes that fairness is not a universal characteristic but rather a notion that varies on a
case-by-case basis depending on the interaction and perception of individual aspects in the identity set.
5
Agent-based Fairness Threshold
In this section, we discuss in detail the decisions that the allocator and recipient make when they have fixed fairness
threshold τ. We also discuss the effect of varying other agent-specific parameters like τ, γ, and d. As discussed in
Section 3, the allocator proposes a split which gives it maximum utility and the recipient accepts a split if its utility is ≥
its minimum acceptable utility.
5.1
Utility Plots
Figure 4 shows the variation of the utility of a split for agents with γ = 0.5 and τ = 0.5. These utility curves are
plotted for a range of semantic distances, d of the agent with the other agent with whom it is playing the UG. The
yellow-shaded region in the figure shows the spectrum of utility for possible splits when d is varied between 0 to 2.4.
The red circle shows the splits at which the recipient gets the minimum acceptable utility and the blue circles show
the splits at which the allocator gets the maximum utility. From this plot, it can be inferred that with the increase in
semantic distance d, the split which gives the maximum utility to the allocator also increases. Hence, as the allocator
transcends higher, its semantic distances reduce and it is more likely to share the resource to a greater extent with the
recipient. On the other hand, as the transcendence level decreases, the allocator is likely to take the whole resource for
itself.
As the recipient, it is observed that varying the semantic distance does not have an impact on the acceptable split, as for
all values of d, the split that satisfies the minimum acceptable utility is 0.5. An interesting observation can be drawn
from the nature of the curves with varying semantic distance. The curve for utility when d = 0 is symmetric at the split
of 0.5, and as the distance increases it becomes S-shaped and gets positive utility from the being on the advantageous
side of the split. The utility curve for d = 0 shows the characteristic of an inequity-averse agent discussed earlier
as it accrues negative utility being on either side of inequitable outcomes. As the value of d increases, this nature of
inequity-aversion decreases.
Next, to understand the impact of τ and γ, we plot similar utility curves for different combinations of τ and γ, as shown
in Figure 5. The effect of varying τ and γ on the utility of the agent can be inferred as follows:
• Effect of τ on utility:
The nature of utility curves when τ is low is presented in Figure 5a and Figure 5b. We note that the utility increases
till the split of 0.5 and then it decreases (when d is low), this is because the τ is 0.2, and the utility is maximum at 0.5
split of resource because both the agents are benefiting equally. For any split beyond 0.5, the other agent suffers more,
7
Modelling the Dynamics of Identity and Fairness in Ultimatum Game
(a) Low τ and Low γ
(b) Low τ and High γ
(c) High τ and Low γ
(d) High τ and High γ
Figure 5: Utility curves with different combination for extremes values of τ and γ
and as observed for low d, the agent shows inequity-averse behaviour, hence, the utility of the agent decreases. This
inequity-averse nature changes as the semantic distance is increased.
Figure 5c and Figure 5d show the utility plots when the value of τ is high. In the case of low d, the utility decreases
as the split is increased till 0.5 split, where the agent has minimum utility and then it increases. This is because both
agents are equally suffering in case of a 0.5 split and on either side one of them is benefiting. As the value of d is
increased, the agent transcends less, and the utility of the agent increases as the split is increased.
In the case of low τ, the allocator (represented by the blue circles) takes splits for itself which are near 0.5 and they
increase the split as the distance with the recipient increases. In the case of the recipient (represented by the red
circles), when the distance is less, they accept a split of low value, but it increases as the distance increases. Hence, in
this case, many game states exist where the deal between the allocator and recipient is accepted.
In the case of high τ, an allocator is always motivated to take the whole split for itself (except d = 0), (represented
by the blue circles). And as a recipient, the split which gives the minimum acceptable utility is high around 0.7
(represented by the red circles). This indicates that in this case, most of the deals are rejected (except when d of
allocator or recipient is 0 from the other player).
• Effect of γ on utility:
When the γ of the agent is low, as observed from Figure 5a and Figure 5c, the splits which the allocator takes for
itself (represented by the blue circles) increases rapidly with the increase in distance. Similarly, the splits at which
recipients get the minimum acceptable utility (represented by the red circles) also increase rapidly compared to an
agent with a high value of γ.
The effect of γ can also be observed in terms of the extent of the yellow-shaded region. Agents with low γ had more
area of shaded region as compared to high γ when the value of τ is kept constant. This shows that for the agents with
low γ, the nature of the utility curve changes rapidly with varying d as compared to agents with high γ as shown in
Figure 5b and Figure 5d.
8
Modelling the Dynamics of Identity and Fairness in Ultimatum Game
(a) Low τ
(b) Moderate τ
(c) High τ
Figure 6: Acceptable Split Matrix with varying semantic distance of recipient with allocator (recipient with γ = 0.4)
5.2
Acceptable Splits for the Recipient
So far, we discussed the utility plots and the variation in the decisions of the allocator and recipient with the change in γ,
d, and τ. Next, we evaluate the game setup specifically focusing on the conditions which lead to deals being accepted.
We analyse the game from the recipient’s perspective and study the factors which affect the acceptance or rejection of
the proposed deal. As noted earlier, γ does not have a significant impact on the nature of the utility curve, so we choose
a fixed value of γ (0.4 in this case). We vary the semantic distance d of the recipient from the allocator and analyse its
impact on the acceptance or rejection of the split offered to the recipient.
The matrices in Figure 6 represent the split on the x-axis and the variation in the semantic distance of the recipient
with the allocator on the y-axis, for different values of τ. The acceptance of a split is represented by green colour and
rejected splits by red colour. The following inferences can be drawn from the matrices:
• Recipient with low τ: In Figure 6a, when the τ of the agent is low (τ=0.2), it accepts all the deals where the
recipient and allocator are getting ≥ 0.2 up to a certain distance. After that recipient starts accepting deals in
which the allocator is getting less than 0.2.
• Recipient with moderate τ: For the agents with moderate τ (τ=0.5), as shown in Figure 6b, when the
semantic distance of the recipient with the allocator is very low, it only accepts the 0.5 splits, as either way
getting more or less than that is unfair for one of them. As the semantic distance with the allocator increases, it
starts accepting all the splits more than 0.5.
• Recipient with high τ: As observed in the above cases, recipients reject the higher split offers when the
semantic distance with the allocator is less. This is because they also account for the fairness of the allocation
of the allocator. Hence, when the agent’s τ is high (τ = 0.7), and the semantic distance is more, they accept the
extreme splits, where at least one of them is satisfied. As the distance increases, they accept the splits which
are strictly greater than 0.7.
In this section, the fairness threshold (τ) is modelled as a fixed characteristic of the agent. The agents perceive the
allocations through the lens of this threshold (factoring in their share as well as other player’s share). As observed
in Figure 6c, most of the deals are rejected when the agent’s τ is high, even though the extent to which the agent
transcends (γd) is high. Hence, in this model, we observe a contradiction in having a high fairness threshold as the
innate characteristic and also highly transcending with the other agent. For example, a realistic scenario of resource
allocation with high transcendence is sharing food with a family member. With a higher transcendence level, there is a
higher tendency to share since the agent prefers to a higher extent the well-being of others. In this case, it doesn’t make
sense to have a fairness threshold of 0.7 (i.e. taking atleast 70% of the food for oneself). In this example, having a static
τ makes sense only in exceptional cases like when health requirements mandate having a minimum of 70% of the food.
Next, we explore an alternate formulation where the fairness threshold (τ) of an agent depends on the association of the
agent with the other agent it interacts with.
9
Modelling the Dynamics of Identity and Fairness in Ultimatum Game
Figure 7: Association-based fairness threshold curves on varying semantic distance with the identity elements for
different γ values
(a) Low γ
(b) Moderate γ
(c) High γ
Figure 8: Utility plots for the agents with different values of γ
(a) Low γ
(b) Moderate γ
(c) High γ
Figure 9: Acceptable Split Matrix with varying semantic distance of recipient with allocator for different values of γ
10
Modelling the Dynamics of Identity and Fairness in Ultimatum Game
6
Association-based Fairness Threshold
In this section, we introduce the notion of fairness using an association-based fairness threshold for the agents. It is
similar to the semantic distance, d that the agents have with different aspects in their identity set. Agent a, has a fairness
threshold τ, for each element o in its identity set represented as τa(o).
As discussed earlier, transcended agents account for the payoff of the aspects they identify with in their utility
computation as described in Equation 1. Their fairness threshold should thus correlate with the extent of their
transcendence level towards the other agent they are interacting with. The extent of transcendence of an agent with
transcendence level γ and semantic distance d with the other agent is denoted using γd. Using this formulation, we
define association-based fairness threshold (τa(o)) towards aspect o in Equation 4 as follows:
τa(o) = 1 − γda(o)
a
(4)
Equation 4 can be interpreted as– ""The more you identify with an entity in your identity set, the less is your fairness
threshold towards it"". Formally, there is a negative correlation between the fairness threshold and the extent of
identification. Next, using this equation, we plot the association-based fairness threshold in Figure 7 for different values
of transcendence level γ. We observe that as an agent’s semantic distance with an identity element increases, the value
of τ also increases. For different values of γ, we observed that γ contributes to the rate of change of τ with d. For
increasing semantic distance d, τ of an agent with a low γ value steadily increases, whereas τ of an agent with high γ
gradually increases. In this case, the utility of the agents is more significantly influenced by γ and d.
6.1
Utility Plots
We vary γ and d (and in turn τ which depends on these), to understand its effect on the utility of the agent. Figure 8
shows the utility plots for the agents for different values of γ. In this case, the nature of utility curves is more diverse as
compared to Section 5 for agents with agent-based fairness threshold, because in this case, τ is also adapting with the
change in the value of d. This also contributes to a larger yellow-shaded region in these utility plots denoting greater
variation in utility.
Like earlier, blue circles indicate the split at which the allocator gets the maximum utility and the red circles indicate
the split at which the recipient gets its minimum acceptable utility. We observe that with the increase in γ, it takes a
higher d for the allocator to take the whole resource for itself. Also, as the recipient, with the increase in γ, the split that
gives the minimum acceptable utility increases slowly as the distance increases.
6.2
Acceptable Splits for the Recipient
To understand more about the nature of deals that get accepted, we plot the matrices by varying the distance of the
recipient from the allocator on the y-axis and the splits offered to it on the x-axis. The colour of cells in matrices
indicates whether the deal is accepted as green or rejected as red. Figure 9 shows the matrices for different values of γ.
We observe that at a lower distance, the recipient accepts almost all splits. However, as γ increases, the agents are more
tolerant and accept splits greater than 0.5 up to low semantic distances to the allocator.
We also note in Figure 9a that the switch of the recipient to accept splits greater than 0.5 happens rapidly when the γ of
the agent is low, but it takes a higher distance for the recipient with high γ to switch to selfish splits (Figure 9c).
7
Conclusions
Allocation games are important to model scenarios of limited resource allocation. We noted that just incorporating
responsible behaviour in agents is not sufficient for fair deals in this scenario. The baseline model of transcendence
was found to be insufficient and unfair in the case of the allocation games since the agents factored in the utility of
all the aspects they identified with, without considering the variation in the absolute value of utilities. The nature of
allocation games characteristically differs from other game-theoretic scenarios like prisoners’ dilemma, since it involves
the distribution of a fixed resource, which can be split in multiple ways and yet the collective payoff of all the players
always remains the same in all possible game states. Thus the notion of fairness is crucial to be modelled in agents
which operate in scenarios representing the allocation games.
In this work, we focused on the Ultimatum Game (UG), which has been widely studied to better understand human
behaviour in allocation scenarios. Though rational choice theories indicate that agents behave selfishly, empirical
evidence suggests the contrary. We extended the identity-based model of agents - computational transcendence, to
11
Modelling the Dynamics of Identity and Fairness in Ultimatum Game
allocation games, and attempted to understand different factors which affect the resultant game states. We explored the
interplay of the notion of fairness with subjective identity in the context of allocation games. Also, this model can be
used to simulate agents with diverse behaviours and preferences similar to the variations observed in people across
different cultures in the context of allocation games.
Acknowledgement
We would like to thank Karthik Sama and Arpitha Malavalli for engaging in insightful discussions. We would also
like to thank the Machine Intelligence and Robotics (MINRO) Center funded by the Government of Karnataka, India
and the Center for Internet of Ethical Things (CIET) funded by the Government of Karnataka, India and the World
Economic Forum for funding and supporting this work.
References
[1] Hamoud S Bin-Obaid and Theodore B Trafalis. Fairness in resource allocation: Foundation and applications.
Network Algorithms, Data Mining, and Applications: NET, Moscow, Russia, May 2018 8, pages 3–18, 2020.
[2] Gary E Bolton and Rami Zwick. Anonymity versus punishment in ultimatum bargaining. Games and Economic
behavior, 10(1):95–121, 1995.
[3] Rutger Bregman. Humankind: A hopeful history. Bloomsbury Publishing, 2020.
[4] Colin F Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton university press, 2011.
[5] Lisa A Cameron. Raising the stakes in the ultimatum game: Experimental evidence from indonesia. Economic
Inquiry, 37(1):47–59, 1999.
[6] Yu-Han Chang, Tomer Levinboim, and Rajiv Maheswaran. The social ultimatum game. Decision making with
imperfect decision makers, pages 135–158, 2012.
[7] Yann Chevaleyre, Paul E Dunne, Ulle Endriss, Jérôme Lang, Michel Lemaitre, Nicolas Maudet, Julian Padget,
Steve Phelps, Juan A Rodrígues-Aguilar, and Paulo Sousa. Issues in multiagent resource allocation. volume 30,
pages 3–31, 2006.
[8] Raymond Dacey. The s-shaped utility function. Synthese, 135:243–272, 2003.
[9] Steven De Jong, Simon Uyttendaele, and Karl Tuyls. Learning to reach agreement in a continuous ultimatum
game. Journal of Artificial Intelligence Research, 33:551–574, 2008.
[10] Jayati Deshmukh and Srinath Srinivasa. Computational transcendence: Responsibility and agency. Frontiers in
Robotics and AI, 9, 2022.
[11] Armin Falk and Urs Fischbacher. A theory of reciprocity. Games and economic behavior, 54(2):293–315, 2006.
[12] Ernst Fehr and Klaus M Schmidt. A theory of fairness, competition, and cooperation. The quarterly journal of
economics, 114(3):817–868, 1999.
[13] Werner Güth, Rolf Schmittberger, and Bernd Schwarze. An experimental analysis of ultimatum bargaining.
Journal of economic behavior & organization, 3(4):367–388, 1982.
[14] Michel JJ Handgraaf, Eric Van Dijk, and David De Cremer. Social utility in ultimatum bargaining. Social Justice
Research, 16:263–283, 2003.
[15] Joseph Henrich. Does culture matter in economic behavior? ultimatum game bargaining among the machiguenga
of the peruvian amazon. American Economic Review, 90(4):973–979, 2000.
[16] Daniel Kahneman. Maps of bounded rationality: A perspective on intuitive judgement and choice. American
Psychologist, 58(9):697–720, 2002.
[17] Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. In Handbook of the
fundamentals of financial decision making: Part I, pages 99–127. World Scientific, 2013.
[18] Richard P Larrick and Sally Blount. The claiming effect: Why players are more generous in social dilemmas than
in ultimatum games. Journal of Personality and Social Psychology, 72(4):810, 1997.
[19] John A List and Todd L Cherry. Learning to accept in ultimatum games: Evidence from an experimental design
that generates low offers. Experimental Economics, 3:11–29, 2000.
[20] Hessel Oosterbeek, Randolph Sloof, and Gijs Van De Kuilen. Cultural differences in ultimatum game experiments:
Evidence from a meta-analysis. Experimental economics, 7:171–188, 2004.
12
Modelling the Dynamics of Identity and Fairness in Ultimatum Game
[21] Matthew Rabin. Incorporating fairness into game theory and economics. The American economic review, pages
1281–1302, 1993.
[22] Alvin E Roth, Vesna Prasnikar, Masahiro Okuno-Fujiwara, and Shmuel Zamir. Bargaining and market behavior
in jerusalem, ljubljana, pittsburgh, and tokyo: An experimental study. The American economic review, pages
1068–1095, 1991.
[23] Amartya K Sen. Rational fools: A critique of the behavioral foundations of economic theory. Philosophy &
Public Affairs, pages 317–344, 1977.
[24] Ignacio Tamarit and Angel Sánchez. Emotions and strategic behaviour: the case of the ultimatum game. Plos one,
11(7):e0158733, 2016.
[25] Eric Van Damme, Kenneth G Binmore, Alvin E Roth, Larry Samuelson, Eyal Winter, Gary E Bolton, Axel
Ockenfels, Martin Dufwenberg, Georg Kirchsteiger, Uri Gneezy, et al. How werner güth’s ultimatum game shaped
our understanding of social behavior. volume 108, pages 292–318. Elsevier, 2014.
13
"
"As machines increasingly interact with people in human-like ways, facial expression recognition has become vital for imparting social robots with the ability to discern user emotional states. This study explores the feasibility of standard facial expression recognition approaches, which have achieved remarkable performance, for individuals with moderate to severe intellectual disabilities, a previously unexplored domain in the literature. Using twelve distinct convolutional neural networks, we train and test on datasets with and without such individuals. Our findings reveal significant distinctions in facial expressions between these populations and within individuals with disabilities. Furthermore, we demonstrate the feasibility of tailored user-specific training methodologies for effective expression recognition in this population. This research underscores the importance of personalized approaches for facial expression recognition in individuals with intellectual disabilities, thus paving the way for improved communication and quality of life.","Understanding emotional states is crucial for human interaction and communication. Facial expressions are a particularly significant component, providing visual representations of emotions. Automated facial expression recognition (FER) has gained prominence in affective computing, leading to advancements in areas like medical diagnosis and human-computer interaction. Deep learning techniques, particularly convolutional neural networks (CNNs), have shown remarkable performance in FER tasks, owing to their ability to learn from large datasets. However, people with intellectual disabilities often display facial expressions that differ from those without disabilities, posing a challenge for existing FER solutions. Thus, we investigate the applicability of standard FER approaches to this population and explore the need for tailored solutions.","Recent years have witnessed significant developments in FER, with a shift towards deep learning and CNN-based techniques achieving high performance on well-known datasets. However, studies focusing on individuals with intellectual disabilities are scarce, highlighting a gap in the literature. A few existing works have explored basic facial expression recognition using different techniques, but they often lack comprehensive evaluations and do not specifically address deep learning approaches. Therefore, a comprehensive study assessing the suitability of deep learning models for facial expression recognition in individuals with intellectual disabilities is warranted.nannannan","To address the research questions, we employed seven datasets, twelve neural networks encompassing established architectures and FER-specific models, and XAI techniques. Images were preprocessed for face detection, alignment, and cropping, and models were trained and evaluated on various datasets. The XAI technique used localized explanations to obtain global per-class explanations, facilitating comparisons of key facial regions across models and classes in different scenarios. Three experiments were conducted: 1) evaluating the performance of networks trained on FER datasets when tested on individuals with intellectual disabilities; 2) examining the effectiveness of training models directly on a dataset of individuals with intellectual disabilities; 3) analyzing the disparities and commonalities in facial expression recognition between individuals with and without intellectual disabilities.nan","The first experiment revealed that models trained on FER datasets performed well on users without disabilities but exhibited low accuracy on individuals with intellectual disabilities. Similarly, models trained on the intellectual disability dataset showed poor performance on users without disabilities. The second experiment demonstrated that training models directly on the intellectual disability dataset improved accuracy, although performance varied across training scenarios and networks. The third experiment employed heat maps to visualize the relevant facial regions for each model and class. These heat maps showed distinct patterns for models trained on FER datasets and models trained on the intellectual disability dataset, suggesting different strategies for facial expression recognition in these populations.",Our study highlights the challenges associated with applying standard FER approaches to individuals with intellectual disabilities and emphasizes the need for tailored solutions. We demonstrate the feasibility of facial expression recognition in this population through user-specific training methodologies. Future research should focus on addressing the scarcity of data for FER in individuals with intellectual disabilities and developing more personalized approaches.,Evaluating the Feasibility of Standard Facial Expression Recognition in Individuals with Moderate to Severe Intellectual Disabilities,"F. Xavier Gaya-Morey, Silvia Ramis, Jose M. Buades-Rubio, Cristina Manresa-Yee","1
Evaluating the Feasibility of Standard Facial
Expression Recognition in Individuals with
Moderate to Severe Intellectual Disabilities
F. Xavier Gaya-Morey
, Silvia Ramis
, Jose M. Buades-Rubio
, Cristina Manresa-Yee
Abstract—Recent research has underscored the increasing
preference of users for human-like interactions with machines.
Consequently, facial expression recognition has gained signifi-
cance as a means of imparting social robots with the capacity
to discern the emotional states of users. In this investigation, we
assess the suitability of deep learning approaches, known for their
remarkable performance in this domain, for recognizing facial
expressions in individuals with intellectual disabilities, which
has not been yet studied in the literature, to the best of our
knowledge. To address this objective, we train a set of twelve
distinct convolutional neural networks in different approaches,
including an ensemble of datasets without individuals with
intellectual disabilities and a dataset featuring such individuals.
Our examination of the outcomes achieved by the various models
under distinct training conditions, coupled with a comprehensive
analysis of critical facial regions during expression recogni-
tion facilitated by explainable artificial intelligence techniques,
revealed significant distinctions in facial expressions between
individuals with and without intellectual disabilities, as well
as among individuals with intellectual disabilities. Remarkably,
our findings demonstrate the feasibility of facial expression
recognition within this population through tailored user-specific
training methodologies, which enable the models to effectively
address the unique expressions of each user.
Index Terms—Facial expression recognition, explainable ar-
tificial intelligence, computer vision, deep learning, intellectual
disabilities
I. INTRODUCTION
Understanding the emotional state of people is important
both on an individual and societal level. This understanding
allows an effective communication, fosters empathy, promotes
mental health or enhances social dynamics, among others.
Non-verbal communication through body language is a pow-
erful form of communication that offers insights into the indi-
vidual’s emotional state [1]. Within the body language, facial
expressions stand out as a particularly significant component
and although facial expressions are not emotions themselves
[2], they provide a compelling visual representation. There-
fore, research in affective computing has notably focused on
automated facial expression recognition (FER), that is, the
identification and analysis of human facial expressions [3]–
[5]. A significant body of research builds upon Ekman six
fundamental and universally recognized facial expressions:
F. Xavier Gaya-Morey, Silvia Ramis, Jose M. Buades-Rubio and Cristina
Manresa-Yee are with the Computer Graphics and Vision and AI Group
(UGIVIA) and the Research Institute of Health Sciences (IUNICS), from
the Universitat de les Illes Balears, Carretera de Valldemossa, km 7.5, Palma,
07122, Illes Balears, Spain.
anger, happiness, surprise, disgust, sadness, and fear [6],
although their universality has been a topic of debate [7]
and they do not express the wide range of expressions that
humans can do. Examples of applications of automated FER
extend across an array of domains, including medical diagnosis
and treatment [8], the analysis of human behavior [9], [10],
as well as its integration into the fields of Human-Computer
Interaction (HCI) and Human-Robot Interaction (HRI) [11],
[12].
Deep learning, particularly Convolutional Neural Networks
(CNNs), has recently emerged in the FER domain [5]. These
deep learning models have demonstrated remarkable perfor-
mance, often attaining state-of-the-art results in FER tasks.
This transition to deep learning signifies a paradigm shift from
early FER research, which primarily revolved around facial
feature extraction and the analysis of facial appearance [13],
[14]. The adoption of deep learning techniques has achieved
important advancements in FER; however, these techniques
often require large amounts of high-quality labeled data for
training to generalize well to new, unseen cases.
In the case of people with intellectual disability, that is,
people that present limitations in their ability to learn at an
expected level and function in daily life, the identification of
facial expressions is an important challenge [15], especially for
those that present verbal communication limitations [16]. Un-
derstanding how this population feels is of utmost importance
as it can indicate discomfort, sadness or pain. The severity of
their limitations can vary widely [17], and the literature shows
that their facial expressions may differ from those without
intellectual disabilities [18]–[20]. Therefore, while automated
FER has made significant advances, a notable gap persists in
the literature regarding comprehensive studies assessing the
applicability of existing solutions to people with intellectual
disabilities. Furthermore, there is a lack of tailored solutions
specifically addressing the needs of this population, likely due
to the scarcity of high-quality datasets.
Understanding the nuances and idiosyncrasies of FER in
this context will enable the full potential of FER technology
in enhancing the quality of life and communication abilities
of people with intellectual disabilities. To address this topic,
we articulate three central research questions:
1) Can deep learning models, originally trained with stan-
dard datasets of facial expressions, yield proficient per-
formance when applied to individuals with intellectual
disabilities? (Q1)
arXiv:2401.11877v1  [cs.CV]  22 Jan 2024
2
2) Can deep learning models, specifically trained on a
dataset comprising individuals with intellectual disabil-
ities, accurately predict the facial expressions of other
individuals with intellectual disabilities? (Q2)
3) What disparities and commonalities exist in the facial
expressions of individuals with and without intellectual
disabilities, learned by DL models? (Q3)
To answer these questions, we execute a series of three
experiments, each devised to address one of the research
questions. Initially, we train twelve diverse neural networks
on a dataset that lacks individuals with intellectual disabilities
and subsequently test them on a distinct dataset that includes
such individuals, namely the MuDERI dataset [21]. Next, we
train the networks directly on the MuDERI dataset, offering
insights into the second research question. Finally, we harness
eXplainable Artificial Intelligence (XAI) techniques to analyze
and visualize the relevant facial regions for each model. As
far as we know, this work stands as an innovative contribution
to the field, addressing the topic in a unique and pioneering
manner.
The work is organized as follows: commencing with a com-
prehensive review of related works, it elucidates the lack of
FER-related works using deep learning techniques for people
with intellectual disabilities. Subsequently, we provide an in-
depth exposition of our methodological approach, encompass-
ing dataset curation, model selection, data preprocessing, and
an overview of our XAI strategy, culminating in a delineation
of the experiments conducted. We then present our empirical
findings and results, and present a comprehensive discussion
of these outcomes. Finally, we summarize the main findings
and present future work lines.
II. RELATED WORK
Recent years have witnessed remarkable developments in
FER, with a shift towards the application of deep learning
techniques, particularly CNNs [22]. Current research works
show the efficacy of these systems achieving high perfor-
mance recognizing from 6 to 8 facial expressions over well-
known datasets such as CK+ or JAFFE [5]. These datasets,
comprised by actor posed expressions or labelled images,
do not specifically inform about the disabilities. Considering
people with moderate to severe intellectual impairments, who
can present differences when producing facial expressions,
we find a scarce number of works that aim at recognizing
automatically their basic facial expressions, especially with
deep learning techniques.
Working in a similar line addressed to people with profound
intellectual and multiple disabilities (PIMD), Campomanes-
´Alvarez and Campomanes- ´Alvarez [23] designed a recognizer
based on analyzing the changes in the appearance of three
face regions -eyes, mouth and jaw- using different techniques
on each zone (e.g. random forest, k-nearest neighbour, na¨ıve
bayes, logistic regression, neural network). They built four
datasets, labeled with the regions’ appearance, to test each
recognizer: one with 375 samples for the eyes and the eye-
brows which included 6 people with PIMD and 5 other people,
one for the mouth with 375 samples which included 6 people
and examples from CK+; and one for the jaw region with
1378 samples, that included 5 people. Labels for each zone
indicated appearance comprising different states for the eyes
(closed, semi, widened, winking or neutral), the eyebrows
(frown, raising or neutral), the mouth (corners of mouth up,
corner of mouth down, mouth wide open, lips movements
and neutral state) and the jaw (grinding, biting, drooping
and no-movement). Their results were promising, however,
the evaluation does not include the basic facial expressions
identifications and the datasets are lacking in the inclusion of
individuals with PIMD.
Dovgan et al. [24] developed a decision support system
(DSS) to recognize behaviour patterns, including facial ex-
pressions, of two persons with PIMD. They used 8 machine
learning techniques (e.g. random forest, support vector ma-
chine, K-nearest neighbours) and a set of rules informed by
experts (e.g. caregivers). They fused the techniques using an
ensemble approach, achieving 12 ensembles. Their results
were modest when classifying the inner state (neutral, pleasure
or displeasure) of the individuals (max. accuracy of 62.4
for one case and 69.3 for the other one). They commented
limitations such as difficulties in the label annotations (even
for caregivers), unbalanced data, facial expressions not well
recognized by the system due to the conditions of the record-
ings (the experiment took place during normal care activities),
informative behaviours for the caregivers that are not always
recognized by the system or inconsistency in the person’s
behaviour.
Addressing other populations which present intellectual
impairments, whose facial expressions may be similar to
the ones of people without disabilities [25], Paredes et al.
[26] built a system based on CNNs aiming at identifying
anger, happiness, sadness, surprise, and neutrality in people
with Down Syndrome. They compiled a dataset including
1200 images of 8- to 12-year-old Down syndrome individuals
displaying spontaneous emotions with their therapist or tutor
during daily activities and achieved an average accuracy of
91.4%, with happiness being the best recognized expression.
III. METHODS
This section provides a comprehensive overview of the
datasets, image processing procedures, DL models, and XAI
techniques employed in this study. Additionally, it outlines
the various experiments conducted to address the research
questions.
A. Datasets
In this study, a total of seven datasets were employed.,
which can be found in Table I. The initial four datasets
are well-established benchmarks commonly utilized in FER
research, namely the Extended Cohn-Kanade (CK+) [27], BU-
4DFE [28], JAFFE [29], and WSEFEP [30] datasets. The CK+
dataset comprises 593 sequences collected from 123 subjects,
with each sequence labeled with one out of seven facial
expressions, namely anger, contempt, disgust, fear, happiness,
sadness, and surprise. On the other hand, the BU-4DFE dataset
3
TABLE I
DETAILED LIST OF DATASETS USED IN THE STUDY.
Details
Data
Classes
Name
Ref.
Year
Authors
Users
Samples
Data type
Happiness
Sadness
Anger
Disgust
Fear
Surprise
Neutral
Contempt
FEGA
[31]
2022
Ramis et al.
51
2856
Image
✓
✓
✓
✓
✓
✓
✓
✗
FE-Test
[31]
2022
Ramis et al.
210
210
Image
✓
✓
✓
✓
✓
✓
✓
✗
MuDERI
[21]
2016
Shukla et al.
12
24
Video
✓
✓
✓
✗
✗
✗
✗
✗
WSEFEP
[30]
2014
Olszanowski et al.
30
210
Image
✓
✓
✓
✓
✓
✓
✓
✗
CK+
[27]
2010
Lucey et al.
123
593
Video
✓
✓
✓
✓
✓
✓
✗
✓
BU-4DFE
[28]
2006
Yin et al.
101
606
Video
✓
✓
✓
✓
✓
✓
✗
✗
JAFFE
[29]
1998
Lyons et al.
10
213
Image
✓
✓
✓
✓
✓
✓
✓
✗
includes 606 sequences from 101 subjects, each subject con-
tributing six sequences, one for each facial expression (anger,
disgust, fear, happiness, sadness, and surprise). The JAFFE
and WSEFEP datasets also offer a neutral class, in addition
to these six facial expressions. The first one encompasses 213
images taken from 10 female Japanese actresses, while the
second one comprises 210 images from 30 individuals.
Apart from these foundational datasets, two additional
datasets were incorporated into the study: FEGA and FE-
test [31]. The FEGA dataset is noteworthy for its multi-label
annotations, including Facial Expression, Gender, and Age.
This dataset involves 51 subjects, each performing the same
seven facial expressions as JAFFE and WSEFEP, with eight
repetitions for each expression, resulting in multiple snapshots.
Conversely, the FE-test dataset consists of 210 frontal images
captured ”in the wild”. Since for the experiments the BU-
4DFE, JAFFE, WSEFEPF, CK+ and FEGA datasets were used
together as a sole dataset, this union will be referred to with
the name of ”FER-DB5”, for simplicity.
Lastly, the study also made use of the MuDERI dataset
[21]. The MuDERI dataset is a comprehensive multimodal
database featuring 12 participants with intellectual disabilities.
This dataset encompasses two audio-visual recordings for each
participant. In the first recording, participants were exposed to
positive stimuli to elicit positive emotions, while the second
recording involved negative stimuli to evoke negative emo-
tions. These videos are segmented by timestamps, and each
timestamp is annotated with three facial expressions: happi-
ness, sadness, and anger. Additionally, the dataset includes
annotations of EEG signals, EDA signals, and Kinect data
that were synchronized with the audio-visual recordings using
these timestamps.
B. Image preprocessing
To facilitate the FER task to the models, we applied pre-
processing to the images, featuring face detection, alignment,
and cropping. Initially, we conducted face detection using the
”a contrario” framework, a method proposed by Lisani et al.
[32]. Subsequently, we employed the 68 facial landmarks, as
outlined by Sagonas et al. [33], to precisely locate the positions
of the eyes and achieve facial alignment. This process involved
calculating the geometric centroids of each eye to compute the
necessary rotation angle for horizontal alignment of the eyes.
After the facial alignment, we performed cropping to isolate
the facial region of interest and then resized the images to
match the specific dimensions required for compatibility with
the CNNs employed in this research.
C. Models
A suite of twelve distinct neural network models was con-
structed for the classification of three basic facial expressions.
These models encompass both widely recognized architectures
and those tailored explicitly for Facial Expression Recognition
(FER). The nine established models, namely AlexNet [34],
VGG16 and VGG19 [35], ResNet50 and ResNet101V2 [36],
InceptionV3 [37], Xception [38], MobileNetV3 [39], and Effi-
cientNetV2 [40] are joined by three specialized FER models:
SilNet [31], SongNet [41], and WeiNet [42], denoted as such
for convenience.
All of these CNNs underwent training and evaluation using
the datasets detailed in subsection III-A, following the prepro-
cessing steps outlined in subsection III-B. By exploring this
variety of architectures and assessing their performance, we
aim to gain valuable insights into how architectural choices
impact FER tasks. These are their basic characteristics:
• AlexNet, WeiNet, SongNet and SilNet: they are rela-
tively simple architectures, composed by three to five
convolutional layers, max-pooling layers, and different
number of dense layers at the end.
• VGG16 and VGG19: they are known for their simplicity
and uniform architecture, using small 3x3 convolutional
filters. Numbers in the names represent the depth of the
network.
• ResNet50 and ResNet101V2: they use residual blocks to
address the vanishing gradient problem. Numbers in the
names represent the depth of the network.
• InceptionV3: it employs inception modules that use
filters of various sizes within the same layer to capture
different features efficiently.
• Xception: a variation of the Inception architecture, Xcep-
tion (Extreme Inception) emphasizes depth-wise separa-
ble convolutions, enhancing efficiency and reducing the
number of parameters.
• MobileNetV3: it is designed for mobile and edge devices,
aiming for high performance with low computational
cost.
4
• EfficientNetV2: it aims to achieve better performance by
balancing network depth, width, and resolution.
All code was written in Python, and the Keras library was
used to implement the AlexNet, WeiNet, SongNet and SilNet
models layer by layer. The remaining models were directly
accessible using the Keras API, along with their pre-trained
weights on ImageNet.
D. XAI approach
To obtain further insights into the inner workings of the
models when performing the FER task, we employed the XAI
technique introduced in [43], which obtains global per-class
explanations by normalizing LIME explanations [44].
LIME stands for Local Interpretable Model-agnostic Expla-
nations, and consists on the perturbation of different regions
at image level, to infer the relevance of each region for
the outputted prediction. We used SLIC [45] to segment
the images and obtain the regions (approximately 30), dark
color to occlude the regions, and 1,000 samples for each
explanation. For the sake of simplicity, we focused on the
positive relevance, omitting the negative one.
Then, following the procedure described in [43], we de-
formed the explanations to fit a normalized space, where all
face landmarks are located at the same coordinates. This al-
lows for the aggregation of multiple explanations into different
heat maps, by classes and by networks, to better understand
the key regions used by the models in each case.
E. Experiments
In this section, we provide a comprehensive overview of the
three experiments conducted to address the research questions
posed in this study.
1) First experiment (training on FER datasets): In the
initial experiment, we expanded upon the methodology in-
troduced in [46] across the entire set of networks detailed
in Section III-C. The objective was to answer Q1, evaluat-
ing whether diverse networks, trained on extended datasets
designed for the FER task, could accurately classify facial
expressions in individuals with intellectual disabilities from
the MuDERI dataset.
This experiment included k trainings with k = 15 on FER-
DB5, which combines five FER datasets (CK+, BU-4DFE,
JAFFE, WSEFEP, and FEGA, as detailed in Section III-A).
Each training was subjected to two evaluations: one on Google
FE-Test and another on MuDERI. This dual evaluation aimed
to validate the models on dissimilar datasets —one involving
users with intellectual disabilities and the other without them.
Additionally, k trainings, also with k = 15, were performed on
the full MuDERI dataset, followed by evaluations on Google
FE-Test. This second set of trainings allowed to investigate in
more depth the inherent differences between the Google FE-
Test and MuDERI datasets. The value for k was set sufficiently
big to enhance the robustness of the performance assessment
and mitigate the impact of individual training variations.
2) Second experiment (training on MuDERI): The second
experiment targeted Q2 and involved training and evaluat-
ing models directly on the MuDERI dataset. Four distinct
scenarios were explored based on the split between training
and test sets. This granular decomposition of the experiment
sought to control the limitations of the MuDERI dataset and
explore various possibilities when dealing with individuals
with intellectual disabilities. The scenarios were the following:
1) User-based Split: Training was conducted on a partition
of MuDERI where the split was performed by users,
training on some users and evaluating on the remaining
ones.
2) Clip-based Split (Full Exposure): The split was done
by clips, ensuring that models ”saw” all users in the
training, offering full exposure to user-specific facial
expressions.
3) Clip-based Split (Restricted Exposure): Similar to the
second scenario, but with an additional constraint. Clips
from users with only a single clip for a specific class
were exclusively included in the training set, not the test
set. This aimed at assessing the model’s ability to rec-
ognize expressions for a user only if it has encountered
other clips of the same user and class during training.
4) Frame-based Split: The split was performed by frames,
allowing adjacent frames to randomly fall into the train-
ing test subsets.
3) Third experiment (getting explanations): In the final
experiment, we delved into the dissimilarities in regions
employed by the models to identify facial expressions. We
accomplished this by comparing the resulting heat maps by
network and class, from two distinct trainings: one on FER-
DB5 and the other on MuDERI (clip-based split with restricted
exposure). This comparison aimed to discern similarities and
dissimilarities in FER for individuals with and without in-
tellectual disabilities. Furthermore, for the trainings on FER-
DB5, we presented heat maps computed by testing the models
on Google FE-Test and MuDERI. This enabled the observation
of differences in important regions when the same models are
employed for users with and without intellectual disabilities.
IV. RESULTS
In this section, we delve into the outcomes of the experi-
ments detailed in the previous section.
A. First experiment: training on FER datasets
In Figure 1, the results of the evaluation of the trainings
on the FER-DB5 dataset on Google FE-Test and MuDERI
are presented. The box plot illustrates the spread of results
through different trainings, showcasing the median, quartiles,
and outliers for each network.
The figure yields the following observations:
• None of the networks achieved satisfactory results on
the MuDERI dataset, with accuracies consistently below
55%.
• Almost all networks, excluding ResNet50, performed
well on the Google FE-Test, with accuracies exceeding
80% in most cases.
5
Fig. 1.
Box-plot showing the accuracy by network and evaluation dataset
for the k = 15 trainings on FER-DB5. The big gap in accuracy between
evaluating the models on Google FE-Test and on MuDERI can be appreciated.
• ResNet50
exhibited
inconsistent
performance
across
trainings, with one achieving accuracy above 80% on
Google FE-Test.
• Trainings on MuDERI displayed larger box and whiskers
compared to FER-DB5 for eight out of twelve networks,
indicating higher variation in accuracy between iterations.
In Figure 2, a box plot illustrates the accuracy of k = 15
trainings on FER-DB5 and MuDERI, evaluated on Google
FE-Test. Results indicate poor performance for trainings on
MuDERI, with accuracies mostly below 40%.
Fig. 2.
Accuracy on the Google FE-Test dataset of the k (with k = 15)
trainings performed on FER-DB5 and MuDERI, by network. In this case,
the low performance on Google FE-Test of the models trained on MuDERI
should be noted.
To explore the subpar results on MuDERI further, Figure
3 displays the average per-class F1 score on Google FE-
Test. Significantly, the ”happiness” class dominates detections,
leading to low recall and F1 scores for other classes.
B. Second experiment: training on MuDERI
Figures 4 and 5 depict the accuracy and F1 score, respec-
tively, of different training scenarios on MuDERI, alongside
results obtained by FER-DB5 trainings on average.
Fig. 3.
Average per-class F1 score obtained on Google FE-Test by the
trainings on MuDERI. Note the great difference among classes.
The figures provide the following insights:
• The 1st training scenario (user-based split) yields the
worst results, close to those achieved by FER-DB5 train-
ings.
• The 2nd and 3rd scenarios (clip-based split) show similar
accuracy, with the 3rd scenario performing over 10%
better on the F1 score for all networks.
• The 4th scenario (frame-based split) attains the highest
accuracy, but potential overfitting is suspected due to the
similarity of consecutive frames.
• Training scenarios exhibit consistent results across differ-
ent networks, except for MobileNetV3.
• EfficientNetV2 excels in the 1st scenario but underper-
forms in the 4th. MobileNetV3 consistently records the
worst results.
These findings offer insights into the varying effectiveness
of different training scenarios on MuDERI and highlight small
network-specific performance variations.
C. Third experiment: getting explanations
In Figure 6 we display heat maps for all classes and
networks, computed for the training on FEER-DB5 tested on
Google FE-Test and MuDERI, and for the training and test on
MuDERI. The following aspects can be observed:
• Training Set Influence: The choice of the training set
significantly impacts the resulting heat maps, leading
to notable distinctions between trainings on FER-DB5
and MuDERI. Heat maps generated on MuDERI appear
less intuitive and, in some instances, exhibit a degree of
complexity.
• Test Set Influence: There are many similarities between
heat maps for a specific network, class, and training set
across various test sets.
• Network Influence: Considerable variation exists in the
learned features across models, particularly notable for
”Sadness” and ”Anger” expressions and for the four
networks without pre-trained weights (SilNet, WeiNet,
AlexNet, and SongNet). However, some common patterns
emerge, contingent on the training set, with clearer trends
observed for the ”Happiness” class trained on FER-DB5.
6
Fig. 4. Accuracy of the different training scenarios on MuDERI, by network. We have added the average results obtained by the 15 trainings on FER-DB5
for comparison.
Fig. 5. F1 score of the different training scenarios on MuDERI, by network. We have added the average results obtained by the 15 trainings on FER-DB5
for comparison.
• Class Influence: Differences between classes are more
pronounced in models trained on the MuDERI dataset.
Conversely, models trained on FER-DB5 tend to empha-
size certain facial features such as the mouth (especially
for ”Happiness”), nose, eyes, and, in some cases, even
the cheeks, chin, and forehead.
V. DISCUSSION
A. Answer to research questions
Q1: Can deep learning models, originally trained with
standard datasets of facial expressions, yield proficient
performance when applied to individuals with intellectual
disabilities?
The outcomes derived from the initial experiment indi-
cate the impracticality of achieving proficient performance
with deep learning models initially trained on conventional
datasets of facial expressions when applied to individuals
with intellectual disabilities. After training on an ensemble of
five datasets for facial expression recognition, the accuracy
achieved by the different models remained high (approxi-
mately 90%) despite changing the dataset used for evaluation
to Google FE-Test, showcasing a proper generalization of the
networks, and unlocking their use for unseen users and data.
However, this adaptability did not extend to the MuDERI
dataset, exclusively comprising individuals with intellectual
disabilities, as evidenced by a significant drop in accuracy
to below 50% across all networks. This suggests that the
facial expressions of individuals with intellectual disabilities
exhibit unique characteristics that are not well captured by the
standard datasets.
Furthermore, the opposite case was also found to hold true:
training deep learning models solely on a dataset of individuals
with intellectual disabilities did not lead to good generalization
performance on standard facial expression datasets. When
models trained on the MuDERI dataset were evaluated on the
Google FE-Test, the accuracy was also poor, approximately
40%. This suggests that the facial expressions of individuals
without intellectual disabilities also exhibit unique character-
istics that are not well captured by the MuDERI dataset.
7
Fig. 6.
Computed heat maps for the different training on FEER-DB5 tested on Google FE-Test and MuDERI, and for the training and test on MuDERI,
grouped by model (by rows), by classes (three major columns), and by training-test sets used (four sub-columns, for each expression).
The discernible dissimilarity in facial expressions between
individuals with and without intellectual disabilities poses a
considerable challenge for the twelve tested networks, hin-
dering their efficacy in both directions. These results under-
score the imperative for a more tailored training approach,
specifically attuned to the nuanced variations in facial ex-
pressions exhibited by individuals with intellectual disabilities.
Additionally, research on the physiological and psychological
factors that contribute to facial expressions in individuals with
intellectual disabilities can provide insights for designing more
effective training datasets and models.
Q2: Can deep learning models, specifically trained on a
dataset comprising individuals with intellectual disabilities,
accurately predict the facial expressions of other individ-
uals with intellectual disabilities?
Looking at the shown figures in section IV-B, it is clear
that it is not possible for any of the models to properly
recognize the facial expression of users with intellectual dis-
abilities unseen during the training phase, since the results
in these cases are similar to that achieved when they are
trained on users without disabilities: below 50% accuracy for
almost all networks, and below 30% F1 score in all cases.
This discrepancy suggests a considerable divergence among
users, somewhat analogous to the distinction between users
from the MuDERI dataset and those from FER-DB5, without
intellectual disabilities.
However, the results also demonstrate that user-specific fine-
tuning can significantly improve the recognition performance.
When individuals to be tested are included in the training
set, allowing the models to learn the unique facial expression
patterns of each person, the accuracy increases considerably.
8
This improvement is particularly evident when at least one
clip per class is present for each individual during training,
contributing to an approximate 10% increase in the F1 score.
Altogether, deep learning models trained solely on a dataset
of individuals with intellectual disabilities struggle to gener-
alize well to unseen individuals within the same population.
This highlights the need for more personalized approaches,
involving user-specific fine-tuning, to achieve accurate facial
expression recognition for individuals with intellectual dis-
abilities. Such fine-tuning can be achieved by incorporating
additional training data from the target individuals, enabling
the models to learn their unique facial expression patterns.
Q3: What disparities and commonalities exist in the fa-
cial expressions of individuals with and without intellectual
disabilities, learned by DL models?
To address this question, the third experiment employed a
XAI technique to generate heat maps highlighting the facial
regions that significantly influence the models’ predictions.
These heat maps (shown in Figure 6) were analyzed across
various model architectures, training datasets, and test datasets.
Firstly, the dependence of heat maps on the training set is
notable, overshadowing the influence of the test set, for which
the explanations were computed. This effect can be observed
on the heat maps for the Google FE-Test and MuDERI
datasets, computed for the models trained on FER-DB5, and
the logical inference is that the networks focus on the patterns
they have learned in the training phase. Nonetheless, the
disparate accuracy outcomes (approximately 90% on Google
FE-Test to less than 50% on MuDERI) imply inadequacy
of these learned patterns for facial expression recognition on
MuDERI, which comprises users with intellectual disabilities.
Secondly, heat maps exhibit variation across different net-
works, with more pronounced differences arising as architec-
tures diverge. For instance, minimal disparity is observed be-
tween VGG16 and VGG19 models or among SilNet, SongNet,
and WeiNet, whereas more substantial differences emerge
between these two groups. Moreover, variability across models
is more prominent when explaining the models trained on
MuDERI, indicative of reduced consensus in finding solutions.
Lastly, attention is drawn to the first and third columns for
each class in the figure, allowing a comparison of learned
regions between the FER-DB5 and MuDERI datasets. Heat
maps computed for FER-DB5 highlight different regions of the
face, which vary depending on the model and the expression:
the mouth and nose are especially important across models
and classes, but also the chin, forehead and cheeks for some
cases, roughly aligning with human expectations. In contrast,
MuDERI’s heat maps appear more erratic, exhibiting greater
variance across models and highlighting less intuitive regions.
These less interpretable heat maps, coupled with increased
variation between models, suggest MuDERI poses a more
challenging dataset, demanding solutions that are both less
generalizable and less intuitive.
B. Limitations of the study
In this study, we trained twelve distinct neural networks for
facial expression recognition tasks on two datasets: FER-DB5,
representing individuals without intellectual disabilities, and
MuDERI, encompassing individuals with intellectual disabili-
ties. However, several limitations associated with the MuDERI
dataset warrant careful consideration when interpreting the
results, particularly before generalizing findings to the broader
population of individuals with intellectual disabilities. These
limitations include:
• Size: While the dataset size was sufficient for training
various models, increased data volume is essential for
enhanced generalization and prediction stability.
• Class imbalance: The dataset exhibits an imbalance, with
more samples for the ”happiness” class than for others.
Although this imbalance does not severely compromise
predictions, it can impact performance, which is why
the F1 score, considering both precision and recall, was
utilized in this study to measure the models’ performance.
• Variety: All dataset images originate from recordings
of twelve participants, potentially leading to overfitting
if not carefully addressed, as observed in the second
experiment results (Section IV-B). To mitigate this, a
more extensive range of users and recordings is preferable
to diversify the dataset and reduce dependence on specific
users, scenarios, or lighting conditions.
• Quality: In some instances, the camera perspective is
suboptimal, either due to users not facing the camera or
the camera being positioned too high.
A significant challenge faced in this study lies in the scarcity
of data for the facial expression recognition task concern-
ing individuals with intellectual disabilities. Consequently,
we utilized only one dataset as a representative sample for
this population, contrasting with an ensemble of datasets for
individuals without intellectual disabilities. Future endeavors
should prioritize addressing this data scarcity by proposing
new datasets that specifically include individuals with intel-
lectual disabilities.
VI. CONCLUSION
This study delved into the challenges associated with
applying facial expression recognition to individuals with
moderate to severe intellectual disabilities. Twelve different
deep learning models were trained, exploring various dataset
combinations and splits, including the use of the MuDERI
dataset, comprised solely of users from this demographic.
Additionally, explainability techniques were used to provide
insights into the internal mechanisms of the models concerning
users with and without intellectual disabilities.
Results underscored the inadequacy of models trained on
generic FER datasets that exclude individuals with intellectual
disabilities. The findings emphasized the necessity for tai-
lored training inclusive of this specific user group. Moreover,
substantial variations were observed even within this demo-
graphic, emphasizing the importance of incorporating target
users in the training set for optimal model performance. A
detailed exploration using Explainable Artificial Intelligence
(XAI) techniques uncovered significant differences in facial
regions employed by the models for expression recognition
when trained on users with and without intellectual disabilities.
9
The patterns identified were more intricate for the former,
featuring less intuitive regions.
Future research should address the existing data scarcity for
FER in individuals with intellectual disabilities, a challenge
for current state-of-the-art methods primarily reliant on data-
intensive deep learning approaches.
ACKNOWLEDGMENTS
Grant
PID2019-104829RA-I00
funded
by
MCIN/AEI/10.13039/ 501100011033, project EXPLainable
Artificial INtelligence systems for health and well-beING
(EXPLAINING). Grant PID2022-136779OB-C32 funded by
MCIN/AEI/ 10.13039/501100011033 and by ERDF A way of
making Europe, project Playful Experiences with Interactive
Social Agents and Robots (PLEISAR): Social Learning and
Intergenerational Communication. F. Xavier Gaya-Morey
was supported by an FPU scholarship from the Ministry of
European Funds, University and Culture of the Government
of the Balearic Islands.
REFERENCES
[1] B. De Gelder and J. Van den Stock, “The bodily expressive action
stimulus
test
(beast).
construction
and
validation
of
a
stimulus
basis
for
measuring
perception
of
whole
body
expression
of
emotions,” Frontiers in Psychology, vol. 2, 2011. [Online]. Available:
https://www.frontiersin.org/articles/10.3389/fpsyg.2011.00181
[2] L. F. Barrett, R. Adolphs, S. Marsella, A. M. Martinez, and S. D. Pollak,
“Emotional expressions reconsidered: Challenges to inferring emotion
from human facial movements,” Psychological Science in the Public
Interest, vol. 20, no. 1, pp. 1–68, 2019.
[3] B. C. Ko, “A brief review of facial emotion recognition based on visual
information,” Sensors, vol. 18, no. 2, 2018.
[4] I. Revina and W. S. Emmanuel, “A survey on human face expression
recognition techniques,” Journal of King Saud University - Computer
and Information Sciences, vol. 33, no. 6, pp. 619–628, 2021.
[5] S. Li and W. Deng, “Deep facial expression recognition: A survey,”
IEEE Transactions on Affective Computing, vol. 13, no. 3, pp. 1195–
1215, 2022.
[6] P. Ekman, “An argument for basic emotions,” Cognition and Emotion,
vol. 6, no. 3-4, pp. 169–200, 1992.
[7] L. F. Barrett, R. Adolphs, S. Marsella, A. M. Martinez, and S. D. Pollak,
“Emotional Expressions Reconsidered: Challenges to Inferring Emotion
From Human Facial Movements,” Psychological Science in the Public
Interest, vol. 20, no. 1, pp. 1–68, 2019.
[8] K. Grabowski, A. Rynkiewicz, A. Lassalle, S. Baron-Cohen, B. Schuller,
N. Cummins, A. Baird, J. Podg´orska-Bednarz, A. Pienia¸˙zek, and
I. Łucka, “Emotional expression in psychiatric conditions: New tech-
nology for clinicians,” Psychiatry and Clinical Neurosciences, vol. 73,
no. 2, pp. 50–62, 2019.
[9] A. M. Barreto, “Application of facial expression studies on the field of
marketing,” Emotional expression: the brain and the face, vol. 9, no.
June, pp. 163–189, 2017.
[10] J. Shen, H. Yang, J. Li, and Z. Cheng, “Assessing learning engagement
based on facial expression recognition in mooc’s scenario,” Multimedia
Systems, vol. 28, no. 2, pp. 469–478, 2022.
[11] S. Medjden, N. Ahmed, and M. Lataifeh, “Adaptive user interface design
and analysis using emotion recognition through facial expressions and
body posture from an rgb-d sensor,” PLoS ONE, vol. 15, no. 7, p.
e0235908, 2020.
[12] S. Ramis, J. M. Buades, and F. J. Perales, “Using a social robot to
evaluate facial expressions in the wild,” Sensors, vol. 20, no. 23, 2020.
[13] M. Pantic and L. J. M. Rothkrantz, “Automatic analysis of facial
expressions: the state of the art,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 22, no. 12, pp. 1424–1445, 2000.
[14] B. Fasel and J. Luettin, “Automatic facial expression analysis: a survey,”
Pattern Recognition, vol. 36, no. 1, pp. 259–275, 2003.
[15] G. Murray, K. McKenzie, A. Murray, K. Whelan, J. Cossar, K. Murray,
and J. Scotland, “The impact of contextual information on the emotion
recognition of children with an intellectual disability,” Journal of
Applied Research in Intellectual Disabilities, vol. 32, no. 1, pp.
152–158, 2019. [Online]. Available: https://onlinelibrary.wiley.com/doi/
abs/10.1111/jar.12517
[16] D. Adams and C. Oliver, “The expression and assessment of emotions
and internal states in individuals with severe or profound intellectual
disabilities,” Clinical Psychology Review, vol. 31, no. 3, pp. 293–
306, 2011. [Online]. Available: https://www.sciencedirect.com/science/
article/pii/S0272735811000080
[17] World Health Organization (WHO), International Classification of Func-
tioning, Disability and Health (ICF), 2018.
[18] R.
H.
Zaja
and
J.
Rojahn,
“Facial
emotion
recognition
in
intellectual disabilities,” Current Opinion in Psychiatry, vol. 21,
no. 5, 2008. [Online]. Available: https://journals.lww.com/co-psychiatry/
fulltext/2008/09000/facial emotion recognition in intellectual.3.aspx
[19] T. Rayworth, “Teaching Children With Mild to Moderate Intellectual
Disabilities to Select and Produce Facial Expressions of Emotion Using
Modelling and Feedback,” Ph.D. dissertation, Edith Cowan University,
2997.
[20] F. L. Wilczenski, “Facial emotional expressions of adults with
mental retardation,” Education and Training in Mental Retardation,
vol.
26,
no.
3,
pp.
319–324,
1991.
[Online].
Available:
http:
//www.jstor.org/stable/23878619
[21] J. Shukla, M. Barreda- ´Angeles, J. Oliver, and D. Puig, “Muderi: Mul-
timodal database for emotion recognition among intellectually disabled
individuals,” 11 2016.
[22] W. Mellouk and W. Handouzi, “Facial emotion recognition using deep
learning: review and insights,” Procedia Computer Science, vol. 175, pp.
689–694, 2020, the 17th International Conference on Mobile Systems
and Pervasive Computing (MobiSPC),The 15th International Conference
on Future Networks and Communications (FNC),The 10th International
Conference on Sustainable Energy Information Technology.
[23] C. Campomanes- ´Alvarez and B. R. Campomanes- ´Alvarez, “Automatic
facial expression recognition for the interaction of individuals with
multiple disabilities,” in 2021 International Conference on Applied
Artificial Intelligence (ICAPAI), 2021, pp. 1–6.
[24] E. Dovgan, J. Valiˇc, G. Slapniˇcar, and M. Luˇstrek, “Recognition of
behaviour patterns for people with profound intellectual and multiple
disabilities,” in Adjunct Proceedings of the 2021 ACM International
Joint
Conference
on
Pervasive
and
Ubiquitous
Computing
and
Proceedings of the 2021 ACM International Symposium on Wearable
Computers, ser. UbiComp/ISWC ’21 Adjunct.
New York, NY, USA:
Association for Computing Machinery, 2021, p. 523–527. [Online].
Available: https://doi.org/10.1145/3460418.3479370
[25] M. C. Smith and D. G. Dodson, “Facial expression in adults with Down’s
Syndrome.” US, pp. 602–608, 1996.
[26] N. Paredes, E. Caicedo-Bravo, and B. Bacca, “Emotion recognition in
individuals with down syndrome: A convolutional neural network-based
algorithm proposal,” Symmetry, vol. 15, no. 7, p. 1435, Jul. 2023.
[Online]. Available: http://dx.doi.org/10.3390/sym15071435
[27] P. Lucey, J. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews,
“The extended cohn-kanade dataset (ck+): A complete dataset for action
unit and emotion-specified expression,” 07 2010, pp. 94–101.
[28] L. Yin, X. Wei, Y. Sun, J. Wang, and M. Rosato, “A 3d facial expression
database for facial behavior research,” vol. 2006, 05 2006, pp. 211–216.
[29] M. Lyons, M. Kamachi, and J. Gyoba, “The Japanese Female Facial
Expression (JAFFE) Dataset,” Apr. 1998, The images are provided at
no cost for non- commercial scientific research only. If you agree to the
conditions listed below, you may request access to download.
[30] M. Olszanowski, G. Pochwatko, K. Kuklinski, M. Scibor-Rylski,
P. Lewinski, and R. Ohme, “Warsaw set of emotional facial expression
pictures: A validation study of facial display photographs,” Frontiers in
Psychology, vol. 5, 12 2014.
[31] S. Ramis, J. M. Buades, F. J. Perales, and C. Manresa-Yee, “A novel
approach to cross dataset studies in facial expression recognition,”
Multimedia Tools Appl., vol. 81, no. 27, p. 39507–39544, nov 2022.
[32] J.-L. Lisani, S. Ramis, and F. J. Perales, “A contrario detection of faces:
A case example,” SIAM Journal on Imaging Sciences, vol. 10, no. 4,
pp. 2091–2118, 2017.
[33] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic, “300 faces
in-the-wild challenge: The first facial landmark localization challenge,”
12 2013, pp. 397–403.
[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classifica-
tion with deep convolutional neural networks,” in Advances in Neural
10
Information Processing Systems, F. Pereira, C. Burges, L. Bottou, and
K. Weinberger, Eds., vol. 25.
Curran Associates, Inc., 2012.
[35] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” 2015.
[36] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” 2015.
[37] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” 2015.
[38] F. Chollet, “Xception: Deep learning with depthwise separable convolu-
tions,” in Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
[39] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang,
Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le, and H. Adam, “Searching for
mobilenetv3,” 2019.
[40] M. Tan and Q. V. Le, “Efficientnetv2: Smaller models and faster
training,” 2021.
[41] I. Song, H.-J. Kim, and P. B. Jeon, “Deep learning for real-time
robust facial expression recognition on a smartphone,” in 2014 IEEE
International Conference on Consumer Electronics (ICCE), 2014, pp.
564–567.
[42] W. Li, M. Li, Z. Su, and Z. Zhu, “A deep-learning approach to
facial expression recognition with candid images,” in 2015 14th IAPR
International Conference on Machine Vision Applications (MVA). IEEE,
2015, pp. 279–282.
[43] C. Manresa-Yee, S. Ramis, and J. M. Buades, “Analysis of Gender
Differences in Facial Expression Recognition Based on Deep Learn-
ing Using Explainable Artificial Intelligence,” International Journal of
Interactive Multimedia and Artificial Intelligence (In press).
[44] M. T. Ribeiro, S. Singh, and C. Guestrin, “”why should i trust you?”:
Explaining the predictions of any classifier,” 2016.
[45] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. S¨usstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 34,
no. 11, pp. 2274–2282, 2012.
[46] S. Ramis, C. Manresa-Yee, J. M. Buades-Rubio, and F. X. Gaya-Morey,
“Explainable facial expression recognition for people with intellectual
disabilities,” in XXIII International Conference on Human Computer In-
teraction (Interaccion 2023). Lleida, Spain: Association for Computing
Machinery, September 2023.
"
"We dive into the complexity of elliptic normal bases introduced by Couveignes and Lercier. We analyze these bases and derive upper and lower bounds on their complexity. Additionally, we analyze weight of vectors related to the basis's multiplication table. Our research sets the stage for future inquiries on the search for low complexity normal bases from elliptic periods.","Arithmetic in finite fields underpins the security of modern communications technology. Customary methods are no longer viable for larger fields, necessitating using explicit isomorphisms to transform fields into vector spaces. Within these spaces, normal bases present significant advantages, including efficient exponentiation. However, multiplication in this context can be more challenging, and the complexity of elliptic normal bases has been a subject of interest.","nanStudies on Gauss periods show that normal bases don't always exist. In an attempt to expand possible cases, Couveignes and Lercier introduced elliptic curves, significantly increasing the number of cases where efficient multiplication is possible. However, their construction lacked analysis of the multiplication table's complexity.nannan","We analyze the complexity of elliptic normal bases constructed using the approach of Couveignes and Lercier. We present bounds for this complexity based on special vectors defined from elliptic curves. Our Magma package, with practical examples, is available online.nan","Our main result is a theorem that provides bounds on the complexity of elliptic normal bases in terms of two vectors derived from elliptic curve points. We also analyze the weight of these special vectors, showing that they are generally dense, leading to high complexity bounds. However, the weight of convolutions involving these vectors remains challenging to predict.",The complexity of elliptic normal bases depends heavily on the vectors derived from elliptic curve points. Predicting the weights of the convolutions involving these vectors remains an open problem. Obtaining points that lead to low weight vectors would be a valuable step towards low complexity normal bases.,The complexity of elliptic normal bases,"Daniel Panario, Mohamadou Sall, Qiang Wang","arXiv:2401.11872v1  [cs.IT]  22 Jan 2024
The complexity of elliptic normal bases
Daniel Panario1, Mohamadou Sall2, and Qiang Wang3
1Carleton University, Canada
daniel@math.carleton.ca
2Universit´e Cheikh Anta Diop of Dakar, Senegal
msallt12@gmail.com
3Carleton University, Canada
wang@math.carleton.ca
Abstract
We study the complexity (that is, the weight of the multiplica-
tion table) of the elliptic normal bases introduced by Couveignes and
Lercier. We give an upper bound on the complexity of these elliptic
normal bases, and we analyze the weight of some special vectors re-
lated to the multiplication table of those bases. This analysis leads us
to some perspectives on the search for low complexity normal bases
from elliptic periods.
1
Introduction
Finite ﬁeld arithmetic is at the heart of modern information and communi-
cations technology. Indeed the latest generations of Intel processors support
a hardware instruction for multiplication in F2n, the ﬁnite ﬁeld of 2n el-
ements [10]. The main reason behind modern general purpose processors
to support such arithmetic operations is that ﬁnite ﬁelds these days ap-
pear almost everywhere in information processing and telecommunication
engineering, particularly in error correcting codes and cryptography.
When performing ﬁnite ﬁeld arithmetic, the cost of various operations
(e.g., addition, multiplication, exponentiation) depends on the choice of rep-
resentation for the elements. For small ﬁelds, look-up tables can be employed
and for mid-sized ﬁelds Zech logarithms can be used; see [12, Chapter 10,
Table B] for an example. However, for larger ﬁnite ﬁelds used in modern
public key cryptography, these methods are no longer feasible. Thus, for a
ﬁnite ﬁeld extension Fqn/Fq, it is usual to exploit an explicit isomorphism
between the ﬁeld Fqn and the corresponding vector space Fn
q by means of a
suitably chosen basis. In this paper, we are interested in normal bases.
Let K be a degree n cyclic extension of a ﬁeld k, and σ a generator of
the Galois group Gal(K/k). Then, a normal basis of K over k is a basis
(α, σ(α), . . . , σn−1(α)) generated by some α in K∗, the units of K.
Such
an α is a normal element and the normal basis theorem [8, Theorem 1.4.1]
ensures its existence for all ﬁnite Galois extension K/k.
Normal bases of Fqn/Fq have a special property that exponentiation by
q corresponds to a cyclic shift, and it can be computed in negligible time.
This is a huge computational advantage but multiplication in this context is
much more diﬃcult. Let (xi)0≤i≤n−1 and (yi)0≤i≤n−1 be the representation
of x and y in a normal basis N of Fqn/Fq. Let (zi)0≤i≤n−1 be the coordinates
of the product xy in N. Because of the structure of N, the multiplication of
basis elements αqi ·αqj are cyclic shifts of each other. It is enough to consider
the products αq0 · αq0, . . . , αq0 · αqn−1 written in the normal basis.
This
deﬁnes the matrix multiplication of the normal basis. The total number of
nonzero elements in this matrix is the complexity C(N) of N. The lower the
complexity is, the more interesting these bases become. Indeed, in practice,
the number of fanout of cells decreases with this complexity. In hardware
implementations, normal bases are often preferred; see [15, Section 16.7] for
more details.
There are diﬀerent ways of doing eﬃcient ﬁnite ﬁeld arithmetic using
normal bases. For example, the bases described in [5, 6] give the product
xy in a normal basis from the coordinates of x and y without using the
multiplication table of this basis. One can also extend appropriate normal
bases or use trace computation to take advantage of their structures. We
shall not go into details into those studies, so we just refer to [4, 7, 18] for
these approaches.
A well-known method to construct eﬃcient normal bases is using Gauss
periods; see [1, 9]. However, as shown by Wassermann [19], Gaussian normal
bases do not always exist. Couveignes and Lercier [5] show how these meth-
ods can be generalized using elliptic curve groups. Since there are many
elliptic curves, they enlarge signiﬁcantly the number of cases where a nor-
mal basis with fast multiplication exists. However, the number of nonzero
elements of the multiplication table, from their construction is not well un-
derstood.
In this paper we proceed to the analysis of the complexity of
normal bases from elliptic periods as designed in [5]. We give some bounds
on this complexity using special vectors deﬁned from elliptic curves.
In Section 2, we review some background material on elliptic curves and
normal elements.
Our results on the complexity of elliptic normal bases
2
are presented in Section 3. In Subsection 3.2, we prove the main theorem
that gives upper and lower bounds on this complexity. In Subsection 3.3,
we analyze the weight of some special vectors related to the multiplication
tensor of elliptic normal bases. In Section 4, we support our studies with
practical examples using the computational algebra system Magma [2]. For
these experiments, we develop a Magma package that deﬁnes all the param-
eters of an elliptic normal basis, and then gives bounds on its complexity.
This Magma package, with test and concrete examples, is available online
[3]. We conclude, in Section 5, with some problems for further research.
2
Background material
In this section, we brieﬂy recall deﬁnitions and properties of elliptic curves
and normal bases over ﬁnite ﬁelds as required in this paper.
2.1
Elliptic curves
Elliptic curves are originally from algebraic geometry which deals with the
solutions of polynomial equations. Their subject encompasses a vast amount
of mathematics. Our aim in this section is to summarize just enough of the
basic theory as needed for the normal elements construction in [5].
An elliptic curve E deﬁned over a ﬁeld K is given by a Weierstrass
equation of the form
E : y2 + a1xy + a3y = x3 + a2x2 + a4x + a6,
with a1, a2, a3, a4, a6 ∈ K. It can be shown, that there is a group associated
with the points on this curve; see [17], Chapter III, Section 2, for the proof.
For any ﬁeld extension L/K the group of L-rational points of E is the set
E(L) = {(x, y) ∈ L2 : E(x, y) = 0} ∪ {O},
endowed with the usual group law, where O is the identity element. An
important concept related to elliptic curves is ﬁnite dimensional linear spaces
given by a special divisor on this curve. A divisor D on an elliptic curve E
is a formal sum
D =
X
P ∈E
nP(P),
where nP ∈ Z and nP = 0 for all but ﬁnitely many P ∈ E. For two divisors
D1 and D2 over E, we have that D1 ≥ D2 if nP ≥ 0 for every P in the
3
formal sum given by D1 − D2. To every divisor D, we can associate a set of
functions
L(D) = {f ∈ ¯K(E)∗ : div(f) ≥ −D} ∪ {O},
where ¯K is the algebraic closure of K. Here, the formal sum div(f) is the
divisor associated to the function f, that is,
div(f) =
X
P ∈E(¯K)
νP (f)(P),
where ν(f) is the order of the vanishing of f at the point P. We have that
L(D) is a ﬁnite-dimensional ¯K-vector space, and its importance lies in that
it gives information about functions on E having prescribed zeros and poles.
Couveignes and Lercier [5] made use of relations between elliptic functions
in these linear spaces which result in nice multiplication formulas for their
elliptic basis. However, the number of nonzero elements in the multiplication
tables (the complexity) of these elliptic normal bases was not analyzed.
2.2
Normal bases over ﬁnite ﬁelds
Let Fqn be a ﬁeld extension of degree n over Fq, and Fn
q an Fq-vector space
of dimension n. Let α0, α1, . . . , αn−1 ∈ Fqn be a basis of this space. Then
for all A ∈ Fqn
A =
n−1
X
i=0
aiαi,
where ai ∈ Fq. We write A = (a0, a1, . . . , an−1). Let B = (b0, b1, . . . , bn−1)
be another element of Fqn and C = (c0, c1, . . . , cn−1) the product
A · B =
X
i,j
aibjαiαj.
Since αiαj ∈ Fqn, one can set, for t(k)
ij ∈ Fq,
αiαj =
n−1
X
k=0
t(k)
ij αk.
Let Tk = (t(k)
ij ) be an n × n matrix over Fq independent of A and B, and let
Bt be the transpose of B. We have that
ck =
X
0≤i,j<n
aibjt(k)
ij = ATkBt,
0 ≤ k < n.
4
A normal basis N of Fqn over Fq is a basis of the form {α, αq, . . . , αqn−1}.
In this case, from one matrix T0 one can get the properties of the remaining
matrices in the collection (Tk)0≤k≤n−1. This is a main advantage of a nor-
mal basis. The total number of nonzero structural constants in T0 is very
important and should be kept as small as possible for eﬃcient software and
hardware implementation; see [1, 8]. The number of nonzero elements of T0
is the complexity of N.
Proposition 1 ([16]). The complexity CN of a normal basis N of Fqn over
Fq is bounded by
2n − 1 ≤ CN ≤ n2 − n + 1.
A normal basis is optimal if it achieves the lower bound in Proposition
1, and a normal basis has a low complexity if it has sub-quadratic bounds,
with respect to n, on their complexity. It turns out that most normal bases
do not have low complexity [13, 14], and ﬁnding normal elements with low
complexity is an interesting research problem [8, Problem 6.2]. For example,
the eﬃciency of multiplication of some (non-normal) bases depends on the
complexity of a well-chosen normal element; for more details, see [7, 18].
2.3
Elliptic normal bases
There are diﬀerent ways of constructing normal elements. Couveignes and
Lercier [5] were the ﬁrst to study normal bases from elliptic periods. Their
construction is derived from elliptic curves properties. Let vl be the valu-
ation associated to l, that is, vl(n) is the degree of divisibility of n by l.
The existence of an appropriate elliptic curve depends on a unique positive
integer nq satisfying for every prime number l





vl(nq) = vl(n) if l is coprime with q − 1,
vl(nq) = 0 if vl(n) = 0,
vl(nq) = max{2vl(q − 1) + 1, 2vl(n)} if l|q − 1 and l|n.
(1)
The following theorem gives a brief summary of their results; for more de-
tails, see [5, Section 5].
Theorem 1 ([5]). With the above notation, let p be a prime number and q
a power of p. Let n ≥ 2 be an integer such that
nq ≤ √q.
5
Then, there exists an elliptic curve E over Fq, a point t ∈ E(Fq) of order
n and a point b ∈ E(¯Fq) such that φ(b) = b + t and nb ̸= 0, where φ is the
Frobenius automorphism.
Furthermore, there exists a point R ∈ E(Fq) such that nR ̸= O, and a
normal basis N of Fqn/Fq with quasi-linear cost of multiplication.
Remark 1. We keep the term “complexity” to refer to the number of non-
zero elements in the multiplication table of a normal basis and use “cost”
for the running time of algorithms. We observe that in [5] the cost of the
algorithm is called the complexity of the operation.
For vector, we use
weight to represent the number of nonzero elements of the vector.
The methods of Couveignes and Lercier allow a fast multiplication al-
gorithm but the complexity of their normal bases, especially needed for
hardware implementations, was not analyzed. The main goal of this paper
is to study the complexity of elliptic normal bases.
3
The complexity of elliptic normal bases
In this section, we give the main results of this paper. It may be convenient
for the reader to have in hand Couveignes and Lercier’s paper [5]. Section 4,
where the multiplication in the elliptic normal basis is given plays a central
role in our paper.
3.1
Notation and previous results
We use the following notation. For P a point on an elliptic curve E, we
denote the x-coordinate of P by xO(P) or simply x(P). For x and y, two
elements of Fqn, we denote by −→x and −→y their coordinates in the elliptic
normal basis. We denote by −→x ⋆ −→y the convolution product and by −→x ⋄
−→y = (xkyk)k the component-wise product of −→x and −→y .
We denote by
σ(−→x ) = (xk−1)k, with (xn−1)0, the cyclic shift of −→x . For a vector −→
R of
length n, we denote by ←−
Rk the vector given by the component-wise product
between −→
R and its k-cyclic shift; we remark the use of left-to-right arrow
for this operation. It is clear that the complexity of ←−
Rk is less than or equal
to the complexity of −→
R.
To diﬀerentiate between the complexity CN of the normal basis and the
weight of a vector, we denote by w(−→
R) the weight, that is, the number of
nonzero elements of a vector −→
R.
We require the following result in our analysis.
6
Lemma 1. [5, Lemma 6] The multiplication tensor for the normal elliptic
basis is
(−→ι a2) ⋆ ((−→x − σ(−→x )) ⋄ (−→y − σ(−→y ))
+ −→
R−1 ⋆ ((−→
R ⋆ −→x ) ⋄ (−→
R ⋆ −→y ) − (−→
Rxa2) ⋆ ((−→x − σ(−→x )) ⋄ (−→y − σ(−→y ))))),
where −→
R = (afO,t(R + jt) + b)0≤j≤n−1, −→
Rx = (xO(R + jt))0≤j≤n−1, and
−→ι represents the coordinates of x(b) in N. The elements a and b are two
scalars in Fq.
In particular, it consists of 5 convolution products, 2 component-wise
products, 1 addition, and 3 subtractions between vectors of size n in Fq.
The scalars a and b have the special property
ac + nb = 1,
where c is also a constant deﬁned by the sum of the elliptic functions
(fkt,(k+1)t)0≤k≤n−1.
It is shown in [5, Lemma 4] that the sum of these
functions is a constant.
Remark 2. We observe that some notation is diﬀerent from [5] and that
in the previous lemma, we combined some of their results for simplicity of
presentation; see [5, Section 4] for more details. In particular, the vector −→
R,
that is central to the complexity analysis is the evaluation at certain special
points of an elliptic function f to be deﬁned in Theorem 2.
3.2
Bounds on the complexity of elliptic normal bases
We show that the complexity of elliptic periods deﬁned in Theorem 1 de-
pends (in some precise way) on a vector −→
R derived from two Fq-rational
points t and R over an elliptic curve E. The point t is of order n and R lies
outside of E[n], the subgroup of n-torsion points of E
E[n] = {P ∈ E(Fq): [n]P = 0}.
The following theorem gives a bound of the whole complexity of the
elliptic normal basis in term of these two vectors.
Theorem 2. With the same notation as before, let p be a prime number
and q a power of p. Let n ≥ 2 be an integer such that nq ≤ √q. Then,
there exists an elliptic curve E over Fq, a Fq-rational point t of order n and
a point R ∈ E(Fq) lying outside of E[n].
7
Furthermore, there exists a normal basis N = {α0, α1, . . . , αn−1} of
Fqn/Fq with complexity lower and upper bounded by
3 +
n−2
X
k=2
w(−→
R−1 ⋆ ←−
Rk) ≤ C(N) ≤ 3n +
n−2
X
k=2
w(−→
R−1 ⋆ ←−
Rk),
where the terms in the sum depend on R and t.
Proof. The existence of the elliptic curve, the ﬁrst part of the theorem,
follows from Theorem 1. We focus next on the nonzero entries of the mul-
tiplication table of the elliptic normal bases.
Let A and B be two distinct points on the elliptic curve E and consider
¯Fq, the algebraic closure of Fq. There exists an elliptic function fA,B having
two poles at A and B given by
fA,B :
E(¯Fq)
−→
¯Fq
P
7−→
yA(P) − y(A − B)
xA(P) − x(A − B)
where yA(P) and xA(P) represent the y-coordinate and the x-coordinate of
the point P − A, respectively. The evaluation of the function fA,B at P is
merely the slope of the line passing through P −A and A−B. Since nq ≤ √q,
it is known from Lemma 1 that the multiplication tensor for the normal
elliptic basis obtained in Theorem 1 consists of 5 convolution products, 2
component-wise products, 1 addition and 3 subtractions between vectors of
size n in Fq.
Indeed, and more precisely, for two vectors −→x and −→y representing two
elements x and y to be multiplied in the elliptic normal basis N, the coor-
dinates of the product x × y are given by
(−→ι a2) ⋆ ((−→x − σ(−→x )) ⋄ (−→y − σ(−→y ))
+ −→
R−1 ⋆ ((−→
R ⋆ −→x ) ⋄ (−→
R ⋆ −→y ) − (−→
Rxa2) ⋆ ((−→x − σ(−→x )) ⋄ (−→y − σ(−→y ))))),
where −→
R = (afO,t(R + jt) + b)0≤j≤n−1, −→
Rx = (xO(R + jt))0≤j≤n−1, and −→ι
represents the coordinates of α2
0 in N. The elements a and b are two scalars
in Fq. For 0 ≤ k ≤ n − 1, the cross-product α0αk is given by
−→ι ⋆
We observe that −→
α 0 = (1, 0, . . . , 0), σ(−→
α 0) = (0, 1, . . . , 0), and αk =
(0, 0, . . . , 1, 0, . . . , 0). Hence, we have
−→
α 0 − σ(−→
α 0)
=
(1, −1, 0, 0, . . . , 0, 0),
−→
α 1 − σ(−→
α 1)
=
(0, 1, −1, 0, . . . , 0, 0),
−→
α 2 − σ(−→
α 2)
=
(0, 0, 1, −1, 0, . . . , 0),
...
−→
α n−1 − σ(−→
α n−1)
=
(−1, 0, 0, 0 . . . , 0, 1).
When 2 ≤ k ≤ n − 2 we have
(−→
α0 − σ(−→
α0)) ⋄ ((−→
αk − σ(−→
αk))
=
(1, −1, 0, . . . , 0) ⋄ (0, 0, . . . , 1, −1, 0, . . . , 0) = (0, . . . , 0).
Since −→
α0 = (1, 0, . . . , 0) can be viewed as the neutral element of the convo-
lution product, the study of the complexity of the normal basis is basically
reduced to the analysis of the vector
−→
R−1 ⋆
−→
R ⋄ (−→
R ⋆ −→
αk)

.
To reduce the vector −→
R−1 ⋆
−→
R ⋄ (−→
R ⋆ −→
αk)

we remark that the convolution
product −→
R ⋆−→
αk is just a k-right cyclic shift of the vector −→
R. In other words,
if −→
R = (r0, r1, . . . , rn−1), then
−→
R ⋆ −→
αk = (rn−k, rn−k+1, rn−k+2, . . . , r2n−k−1),
where indices are computed modulo n. Therefore
−→
R−1 ⋆
−→
R ⋄ (−→
R ⋆ −→
αk)

=
−→
R−1 ⋆ ((r0, r1, . . . , rn−1) ⋄ (rn−k, rn−k+1, rn−k+2, . . . , r2n−k−1))
=
−→
R−1 ⋆ ←−
Rk.
For the n − 3 rows in the middle of the multiplication table of N, we get
the total complexity
n−2
X
k=2
w(−→
R−1 ⋆ ←−
Rk).
It remains to note that the complexity of the ﬁrst two rows and the last
row is lower and upper bounded by 3 and 3n, respectively, since each of the
remaining rows must have at least one nonzero element.
■
9
We observe that from the points R and t, we know the exact complexity
of the matrix derived from the n − 3 rows at the middle of the multiplica-
tion table of N. Next, we study the weight of the vectors involved in this
complexity estimation.
3.3
Weight of special vectors
We study special vectors, that is, vectors constructed from elliptic curve
properties and on which the eﬃciency of the elliptic normal basis depends.
In the previous subsection, we showed that the complexity of the elliptic
normal bases, with fast multiplication algorithm, designed by Couveignes
and Lercier [5] depends heavily on the vectors −→
R, −→
Rx, ←−
Rk, −→
R−1 and −→ι ,
the expression of x(b) in the normal basis.
In this section, we give bounds on the weights of the ﬁrst three vectors
showing that all of them are very close to n. Unfortunately, it is diﬃcult to
predict the weights w(−→
R−1 ∗ ←−
Rk)2≤k≤n−2 needed in Theorem 2. However,
we conjecture that those weights are also very high. We show this with some
examples in the next section.
The vectors −→ι and −→
Rx are involved in the computation of CN when
k = 1 and n − 1. Indeed, if k = 1, then Equation (2) becomes
α0α1 =−→ι ⋆
the order ν(f) of the vanishing of f at P is a negative integer. Hence, the
poles are counted with a negative sign.
Weights w(−→
R) and w(←−
Rk): Let f0(R+jt) := afO,t(R+jt)+b. Since −→
R =
(f0(R + jt))0≤j≤n−1, then the number of nonzero elements in −→
R depends
on the evaluation of f at the points R + jt. Recall that a and b are two
scalars in Fq such that ac + nb = 1. If b = 0, since a is a nonzero element,
f0(R + jt) = 0 if and only if fO,t(R + jt) = 0. Since fO,t has only two
poles at O and t then it has at most two zeroes that might be in the set
(R + jt)0≤j≤n. Hence, we have that if b = 0 then w(−→
R) ≥ n − 2. Following
the same reasoning we obtain that if b ̸= 0 then w(−→
R) ≥ n − 1.
We can easily derive the weight of ←−
Rk from w(−→
R). By deﬁnition ←−
Rk is
the component-wise product between −→
R and its k-cyclic shift. Thus, if −→
R
has one zero element, then w(←−
Rk) = n − 2, and if −→
R has two zeroes, then
w(←−
Rk) ≥ n − 4.
Weight w(−→
Rx): We have that −→
Rx = (xO(R + jt))0≤j≤n−1. This vector
is given by the x-coordinates of n Fq-rational points R + jt over E.
If
char(Fq) ̸= 2, 3, then the Weierstrass form of E can be reduced to the
following equation
y2 = x3 + ax + b,
(5)
with a, b ∈ Fq. Every point P ∈ E on the y-axis has ±
√
b as y-coordinate.
Since we have two possibilities, the number of zero-elements of −→
Rx can not
exceed 2, in other words w(−→
Rx) ≥ n − 2.
We summarize the preceding discussion in the following proposition,
which gives some bounds on the weight of the considered vectors. We sup-
pose the conditions of Theorem 2 are satisﬁed.
Proposition 3. Let E be an elliptic curve over Fq giving rise to the two
vectors −→
R and −→
Rx. Then, w(−→
Rx) ≥ n − 2, w(−→
R) ≥ n − 2, and
w(←−
Rk) ≥



n − 2, if w(−→
R) has one zero element;
n − 4, if w(−→
R) has two zeroes.
The main consequence of this proposition is that most of the vectors in-
volved in the multiplication are highly non-sparse. It remains to understand
the weights of the n − 3 convolutions w(−→
R−1 ∗ ←−
Rk)2≤k≤n−2.
11
4
Examples of elliptic normal basis
In order to have a practical understanding of our results in this paper, we give
three examples of elliptic normal bases using diﬀerent parameters and/or
extensions. For the sake of better illustration, we work with extensions of
small degrees. All these examples were built using a Magma package which
contains two main functions.
1. The function ENBparamsComputation which takes as input a ﬁnite
ﬁeld Fq and an extension degree n and then returns (if possible) all
the necessary parameters to the construction of an elliptic normal basis
of Fqn over Fq.
2. The function ENBcomplexityBounds which computes the weight of the
special vectors and gives a lower and an upper bound on the complexity
of the basis.
These functions make use of several preliminary functions; for more details
on this package, see [3].
An interesting point of working with elliptic curves is that for a given
ﬁnite ﬁeld Fq we can deﬁne many such curves. This can enlarge signiﬁcantly
the number of cases where a normal basis with good properties (fast multi-
plication and/or low complexity) exist. We highlight this fact in Subsection
4.2 by giving two diﬀerent elliptic normal bases on the same extension F76
over F7. The special vectors (on which the complexity depends heavily) of
one of the bases are full of nonzero elements, which leads to a bad complex-
ity. In the last example, the change of an elliptic curve gives some special
vectors with good weight, which leads to a lower bound on the complexity
of the constructed basis.
4.1
First example
Let us consider the ﬁnite ﬁeld F13 and degree extension n = 7.
In this
subsection, we give a practical example of an elliptic normal basis, determine
the weight of its special vectors and give a bound on its complexity. We work
with the elliptic curve
E : y2 + 4xy + 9y = x3 + x2 + 3x + 8,
of order 14 over F13 and the subgroup T generated by the point t = (0, 10)
of order 7. The quotient isogeny E/T is deﬁned by the equation
y2 + 4xy + 9y = x3 + x2 + 6,
12
and its rational point a = (11, 11) generates an irreducible preimage from
which we construct the extension F137. The generic point
b = (τ 5 + 10τ 4 + 9τ 3 + 9τ 2 + 6τ + 8, 12τ 6 + 3τ 5 + 10τ 4 + τ 3 + 10τ 2 + 11τ + 1)
is an F137-rational point of E. The evaluation of the functions afkt,(k+1)t +b
at this point b gives the elliptic normal basis N1 = {α0, α1, . . . , α6} where
α0 = 12τ 6 + 9τ 3 + τ 2 + 12τ + 5,
α1 = 12τ 6 + 3τ 3 + 8τ 2 + 7τ,
α2 = 3τ 5 + 7τ 4 + 8τ 3 + 8τ 2 + 9τ + 5,
α3 = τ 6 + 2τ 5 + 9τ 4 + 8τ 3 + 11τ 2 + 3τ + 3,
α4 = τ 6 + 11τ 5 + 5τ 4 + 9τ 3 + 11τ 2 + 10τ + 3,
α5 = τ 6 + τ 5 + 3τ 4 + 7τ 3 + 7τ 2 + 7τ + 5,
α6 = 12τ 6 + 9τ 5 + 2τ 4 + 8τ 3 + 6τ 2 + 4τ + 11.
In this example, the scalars c, a, and b in Lemma 1 are respectively given
by 11, 6 and 0, and only a is needed in the computations of this section.
To compute the special vectors and determine the bound on the complexity
C(N1) of N1 we deﬁne the F13-rational point R = (9, 0) over E. The point
R is outside of T. The complexity C(N1) of N1 satisﬁes
25 ≤ C(N1) ≤ 43.
These two bounds are derived from the weight of the special vectors in
Table 1.
(−→
R−1 ∗ ←−
Rk)2≤k≤n−2
−→
Rx
(9, 3, 6, 1, 6, 3, 9)
(3, 5, 3, 11, 11, 11, 11)
−→
R
(4, 0, 8, 10, 10, 8, 0)
(6, 1, 1, 6, 0, 0, 0)
−→
R−1
(12, 8, 6, 0, 0, 8, 8)
(6, 0, 0, 0, 6, 1, 1)
(3, 11, 11, 11, 11, 3, 5)
Table 1: Special vectors of N1.
Using our Magma function, ENB3RowsWeight, that we deﬁned for these
toy examples, we can compute the coordinates of α2
0 in N1, the vector
−→ι , and by extension the exact complexity of N1. We have that (
−→
α2
0) =
13
(9, 6, 3, 11, 1, 5, 10) and −→ι = (10, 10, 9, 5, 1, 7, 1). Since a = 6, then by Equa-
tions (3) and (4), we have
α0α1 = (9, 11, 6, 4, 12, 6, 10) and α0αn−1 = (10, 6, 4, 12, 6, 10, 10).
The three remaining rows i.e., α2
0, α0α1, and α0αn−1 are full of non-zero
elements, then N1 achieves the upper bound 43.
We note that the sum
deﬁned by the 4 rows at the middle of the multiplication table is equal to
22.
4.2
Second example
In this subsection, we construct two elliptic normal bases of F76 over F7
using the elliptic curves
E2 : y2 + 3xy + 2y = x3 + x2 + 2x + 4
and
E3 : y2 + 3xy + 4y = x3 + 6x2 + 1
of order 12 over F7. The aim is to show how the equation of the chosen
curve can aﬀect the complexity of the derived basis.
4.2.1
Analysis of the elliptic normal basis from E2
The point t2 = (2, 2) generates a subgroup T2 ⊂ E2 of order 6. With the
quotient isogeny E′
2 = E2/T2 deﬁned by
y2 + 3xy + 2y = x3 + x2 + 2x + 4,
we ﬁnd the point a2 = (4, 3) with irreducible preimage that generates the
ﬁnite ﬁeld F76. We set
b2 = (5τ 5 + 5τ 4 + 2τ 3 + 2τ 2 + τ + 4, 6τ 5 + τ 4 + 2τ 3 + 2τ 2 + 3τ + 1)
and we ﬁnd the elliptic normal basis N2 given by α = 2τ 4 + 2τ 3 + τ and
its conjugates. With the point R = (4, 1), lying outside of E2[6](F7), we
establish Table 2 that contains the special vectors of N2.
Table 2 gives an example where the special vectors, that bound the
complexity, are full of nonzero elements. Thus, without surprise, this leads
to higher bounds on the complexity of N2
21 ≤ C(N2) ≤ 31.
14
(−→
R−1
2
∗ ←−
R2k)2≤k≤n−2
−→
R2x
(4, 4, 3, 1, 1, 3)
(5, 6, 5, 4, 4, 4)
−→
R2
(4, 2, 4, 0, 5, 0)
(4, 3, 3, 4, 3, 3)
−→
R−1
2
(1, 0, 3, 0, 1, 3)
(5, 4, 4, 4, 5, 6)
Table 2: Special vectors of N2.
Using our function ENB3RowsWeight, we found that α2
0 = (4, 2, 6, 0, 2, 0)
and −→ι = (5, 1, 5, 1, 0, 2). Since a = 2, using Equations (3) and (4), we have
α0α1 = (3, 6, 4, 0, 2, 1) and α0αn−1 = (5, 0, 4, 0, 2, 0).
The number of non-zero elements in the three remaining rows is equal to 12.
Thus, the total complexity of N2 is 30. In the next section, using diﬀerent
parameters, we give an elliptic normal basis that has a better lower bound
and a better complexity.
4.2.2
Analysis of the elliptic normal basis from E3
In this example, we deal with the elliptic curve E3. The torsion point of
order 6, the isogenous curve E′
3, the point a3 with irreducible preimage, and
the generic point b3 are respectively given by
t3 = (4, 5),
E′
3 : y2 + 3xy + 4y = x3 + 6x2 + 4x + 5,
a3 = (0, 1),
b3 = (4τ 5 + 3τ 4 + 2τ 3 + 5τ 2 + 3τ + 6, 5τ 4 + 6τ 3 + τ 2 + 2τ + 5).
The above parameters give an elliptic normal basis N3 generated by the
normal element
α3 = 3τ 5 + 3τ 4 + 3τ 3 + 3τ 2 + 1.
Using the point R3 = (1, 1), that lies outside of E3[6](F7), we ﬁnd the special
vectors in Table 3.
We observe that the weights of the special vectors (−→
R−1
3
∗ ←−
R3k)2≤k≤n−2
in Table 3 are sparser than those reported in Tables 1 and 2. This leads to
a better complexity bound for the elliptic normal basis N3
11 ≤ C(N3) ≤ 26.
15
(−→
R−1
3
∗ ←−
R3k)2≤k≤n−2
−→
R3x
(1, 1, 6, 2, 2, 6)
(3, 4, 3, 0, 0, 0)
−→
R3
(2, 5, 2, 1, 4, 1)
(3, 0, 0, 3, 0, 0)
−→
R−1
3
(0, 3, 1, 3, 0, 1)
(3, 0, 0, 0, 3, 4)
Table 3: Special vectors of N3.
For this example, α2
0 = (4, 6, 0, 1, 2, 0) and the precomputed vector −→ι
that corresponds to the x−coordinate of b3 is equal to (1, 3, 1, 0, 0, 1). Then,
by Equations (3) and (4), we have
α0α1 = (6, 0, 2, 1, 5, 0) and α0αn−1 = (6, 2, 1, 5, 0, 0),
and the complexity of N3 is 20. We comment that the sum depending on
R3 and t3 is equal to 8. This example shows that even if the ﬁrst two rows
α2
0 and α0α1, and the last row α0αn−1 have many nonzero elements, we can
end up with a better bound and a better complexity if the sum deﬁned by
the rows in the middle of the multiplication table is small.
5
Conclusions and further work
Looking at our results in Section 3, we observe that it will be very hard to
get a low complexity normal basis, as designed in [5], if we can not ensure
low weight vectors from inverses and convolution products between some
special vectors. This likely involves that vectors with prescribed low weight
are needed in the computations. To have a concrete view of this analysis let
us consider the matrices
M2 =








0
1
1
0
0
0
6
0
2
0
6
0
0
3
0
0
3
0
2
0
6
0
6
0
1
1
0
0
0
0
2
4
2
0
4
0








and
M3 =








2
3
3
2
4
4
1
5
4
5
1
1
2
6
2
2
6
2
4
5
1
1
1
5
3
3
2
4
4
2
4
4
4
1
2
1








.
16
The rows of M2 are equal to the vectors (←−
R2k)1≤k≤n, the component-wise
product between −→
R2 and its k-cyclic shift; the matrix M3 is constructed
similarly. These products play a central role in the proof of Theorem 2 and
the construction of the normal bases N2 and N3 in Section 4.2. We can
apply the same construction on the normal basis N1, of Section 4.1, but
the cases N2 and N3 are suﬃcient for the current analysis. All the given
data are obtained from the Magma package and the practical examples are
available in [3].
The matrices M2 and M3 give rise, respectively, to the bounds of the
elliptic normal bases N2 and N3. Even though, M3 is full of nonzero elements
and M2 has a better complexity, we end up with a good bound for N3, which
is not the case for N2. This example shows clearly that it is hard to predict
the weight of the special vectors (−→
R−1 ∗ ←−
Rk)2≤k≤n−2 from its entries. Since
the complexity of elliptic normal basis depends on these products, it is a
nontrivial and interesting research problem to obtain some points on elliptic
curve that lead to vectors with low weight. We leave this for further research.
Acknowledgements. The authors were funded by the Natural Sciences
and Engineering Research Council of Canada.
References
[1] D. W. Ash, I. F. Blake and S. A. Vanstone, Low complexity normal
bases, Discrete Appl. Math., 25 (1989) 191-210.
[2] J. Canon, W. Bosma, C. Fieker and A. Steel, Handbook of Magma
Functions, http://magma.maths.usyd.edu.au/magma/, Sydney.
[3] Magma Package, EllNormalBasisComplexity,
https://github.com/
sallme/EllNormalBasisComplexity.
[4] M. Christopoulou, T. Garefalakis, D. Panario and D. Thomson, The
trace of an optimal normal element and low complexity normal bases,
Des. Codes Crypt., 49 (2008) 199-215.
[5] J.-M Couveignes and R. Lercier, Elliptic periods for ﬁnite ﬁelds. Finite
Fields Appl., 15 (2009) 1-22.
[6] T. Ezome and M. Sall,
Normal bases from 1-dimensional algebraic
groups. J. Symb. Comput., 101 (2020) 152-169.
17
[7] T. Ezome and M. Sall, On ﬁnite ﬁeld arithmetic in characteristic 2.
Finite Fields Appl., 68 (2020) 101739.
[8] S. Gao. Normal bases over ﬁnite ﬁelds, ProQuest LLC, Ann Arbor,
MI, 1993. Thesis (Ph.D), University of Waterloo (Canada).
[9] S. Gao, J. von zur Gathen, D. Panario and V. Shoup, Algorithms for
exponentiation in ﬁnite ﬁelds, J. Symb. Comput., (2000) 879-889
[10] S. Gueron and M. E. Kounavis. Carry-less multiplication and its usage
for computing the GCM mode. White Paper, Intel Corporation. 2008.
[11] H. W. Lenstra and R. Schoof Primitive normal bases for ﬁnite ﬁelds.
Math. Comp., 48 (1987) 217-231.
[12] R. Lidl and H. Niederreiter, Introduction to Finite Fields and Their
Applications, Cambridge University Press, Cambridge. (1986).
[13] A. M. Masuda, L. Moura, D. Panario and D. Thomson, Low complexity
normal elements over ﬁnite ﬁelds of characteristic two, IEEE Trans.
Comput.. 57 (2008) 990-1001.
[14] L. Moura, D. Panario, D. Thomson, Normal Basis Exhaustive Search:
10 Years Later,
In: Budaghyan, L., Rodriguez-Henriquez, F. (eds)
Arithmetic of Finite Fields. WAIFI 2018. Lecture Notes in Computer
Science., vol 11321, Springer, Cham (2018) 188-206.
[15] G. L. Mullen and D. Panario, Handbook of Finite Fields, CRC Press,
Boca Raton, FL, (2013).
[16] R. C. Mullin, I. M. Onyszchuk, S. A. Vanstone and R. C. Wilson,
Optimal normal bases GF(pn), Discrete Appl. Math., 22 (1988/1989)
149-161.
[17] J. H. Silverman, The arithmetic of elliptic curves, volume 106 of Grad-
uate Texts in Mathematics. Springer, Dordrecht, second edition, 2009.
[18] D. Thomson and C. Weir, Artin-Schreier extensions of normal bases.
Finite Fields Appl. 53 (2018) 267-286.
[19] A. Wassermann, Konstruktion von Normalbasen, Bayreuther Mathe-
matische Schriften., 31 (1990) 155-164.
18
"
"This paper presents a method for comparing approval-based multiwinner voting rules based on the similarity between the committees they select. The method is evaluated on several statistical cultures, and the results indicate that proportional rules, such as PAV and seq-PAV, are more similar to each other than they are to non-proportional rules, such as Minimax AV. The method also reveals that some rules, such as seq-Phragmén and seq-PAV, are not only close to strongly proportional rules (Equal Shares and PAV), but are also indistinguishable from an axiomatic point of view in the dataset.","Multiwinner voting is the process of selecting a fixed number of candidates (a committee) based on the preferences of agents. This general task occurs in a wide range of applications such as group recommendations, blockchain protocols, political elections, and the design of Q&A platforms. As it is apparent from this diverse list of applications, there is a multitude of desiderata for multiwinner voting rules and their desirability depends on the setting in which they are applied.","Among the three aforementioned principles, proportionality has received the most attention and there now exists a hierarchy of proportionality axioms by which the proportionality of a voting rule can be assessed [Lackner and Skowron, 2023, Chapter 4]. Far fewer axioms exist for diversity and individual excellence.nannannan","We give a brief overview of major approval-based multiwinner voting rule. We omit technical details and refer to the survey by Lackner and Skowron [2023] for details. We assume that all voting rules use a tiebreaking mechanism to ensure that they return exactly one winning committee (this property is known as resoluteness).1

Thiele methods are an important class of approval-based multiwinner voting rules that select a committee W maximizing value scw(W) = P
v∈V
P|A(v)∩W|
i=1
w(i) for some score function w. In particular, Multiwinner Approval Voting (AV) is defined by w(i) = i (equivalently, AV selects the k candidates that are approved by the largest number of voters). This rule is considered to be the prime example of the principle of individual excellence.

Chamberlin–Courant (CC) selects the committee W that maximizes the number of voters that approve at least one candidate in W, i.e., w(1) = 1 and w(i) = 0 for all i ≠ 1. CC is an example of a rule designed to achieve diversity.

Proportional Approval Voting (PAV) and Sainte-Laguë Approval Voting (SLAV) are defined by w(i) = 1
i and w(i) = 1
2i−1, respectively.nan","For three of our models, that is, disjoint, 1D-Euclidean, and Pabulib, we present maps of multiwinner rules; see Figure 1 (each map is based on 1000 elections generated for the respective culture).

We start by analyzing the left map for the disjoint model. On the “west” side of the map, we have the CC voting rule; this area can be interpreted as representing diversity: the rules located in this area select committees that consist of diverse candidates. On the “east” side of the map we have AV; this area can be interpreted as representing individual excellence: the rules located here select committees that consist of individually best candidates, not taking into account whether the selected candidates are similar to each other or not. In the central part of the map (highlighted in red), we have numerous rules considered to be proportional, such as the PAV rule or seq-Phragmén. Note that Geometric rules are close to the “cluster” of proportional rules, but their distance increases for larger p-values.","We introduced a framework for comparing multiwinner voting rules based on the committees they select. This framework gives a principled approach to identifying similarity of voting rules. When contrasted with axiomatic analysis, we see that some rules (seq-Phragmén and seq-PAV) are not only close to strongly proportional rules (Equal Shares and PAV), but are also indistinguishable from an axiomatic point of view in our dataset. This indicates that our distance-based approach indeed offers a finer view of multiwinner rules, and thus complements a precise axiomatic analysis.",An Experimental Comparison of Multiwinner Voting Rules on Approval Elections,"Piotr Faliszewski, Martin Lackner, Krzysztof Sornat, Stanisław Szufa","An Experimental Comparison of Multiwinner Voting Rules
on Approval Elections
Piotr Faliszewski1, Martin Lackner2,
Krzysztof Sornat3, and Stanisław Szufa1
1AGH University, Poland
2TU Wien, Austria
3IDSIA, USI-SUPSI, Switzerland
January 23, 2024
Abstract
In this paper, we experimentally compare major approval-based multiwinner voting rules. To
this end, we define a measure of similarity between two equal-sized committees subject to a given
election. Using synthetic elections coming from several distributions, we analyze how similar are the
committees provided by prominent voting rules. Our results can be visualized as “maps of voting
rules”, which provide a counterpoint to a purely axiomatic classification of voting rules. The strength
of our proposed method is its independence from preimposed classifications (such as the satisfaction of
concrete axioms), and that it indeed offers a much finer distinction than the current state of axiomatic
analysis.
1
Introduction
Multiwinner voting is the process of selecting a fixed number of candidates (a committee) based on the
preferences of agents. This general task occurs in a wide range of applications such as group recommenda-
tions Lu and Boutilier [2011]; Gawron and Faliszewski [2022], blockchain protocols Cevallos and Stewart
[2021], political elections Renwick and Pilet [2016]; Brill et al. [2018], and the design of Q&A platforms
Israel and Brill [2021]. As it is apparent from this diverse list of applications, there is a multitude of
desiderata for multiwinner voting rules and their desirability depends on the setting in which they are
applied. Elkind et al. [2017] and Faliszewski et al. [2017b] suggest a classification of multiwinner voting
rules based on three principles: proportionality, diversity and individual excellence. These principles
capture three main goals in multiwinner voting: (i) to find a committee that reflects the voters’ preferences
in a proportional manner, (ii) to find a committee that represents (or, covers) the opinions of as many
voters as possible (diversity), and (iii) to find a committee that contains the objectively “best” candidates
(individual excellence).
In recent years, a large body of work has helped to shape our understanding of multiwinner voting
rules (as surveyed by Faliszewski et al. [2017b] and Lackner and Skowron [2023]). The key method
employed here is axiomatic analysis. Among the three aforementioned principles, proportionality has
received the most attention and there now exists a hierarchy of proportionality axioms by which the
proportionality of a voting rule can be assessed [Lackner and Skowron, 2023, Chapter 4]. Far fewer
axioms exist for diversity and individual excellence Faliszewski et al. [2017a]; Subiza and Peris [2017];
Faliszewski and Talmon [2018]; Lackner and Skowron [2021].
While the axiomatic method has been fundamental in advancing our understanding of multiwinner
rules, significant questions cannot be approached with axioms. First and foremost, the satisfaction of an
1
arXiv:2401.11870v1  [cs.GT]  22 Jan 2024
axiom is a binary fact. If an axiom is not satisfied by a voting rule, this might be due to a fundamental
incompatibility that is evidenced in nearly every election or simply due to an involved counterexample
that hardly occurs in practice (an extreme example is provided in the work of Brandt et al. [2013], which
disproved an over 20-year-old conjecture by giving a counterexample with 10136 candidates; however, a
much smaller example was later reported by Brandt and Seedig [2013]). Secondly, while the axiomatic
approach is useful to highlight differences between voting rules, it is rarely helpful to establish similarities.
Even voting rules that do not share (known) axiomatic properties can behave very similarly on sampled
or real-world preference data. Identifying similar voting rules is important, e.g., if a computationally
demanding rule is infeasible in a given setting and has to be replaced with a faster-to-compute one.
The goal of our paper is to close this gap in our understanding and provide a principled method to
assess the similarity of voting rules. Our proposed method does not rely on preimposed classifications
(such as the proportionality/diversity/individual excellence trichotomy) and, instead, we base our analysis
on comparing the committees provided by the rules. Specifically, given a distance measure between
candidates (which we interpret as a measure of their similarity), we extend it to committees and compare
how close are the committees output by a number of major multiwinner rules on several families of
synthetic elections. For the visualization of our results, we adapt the map framework proposed by Szufa et
al. [2020] and Boehmer et al. [2021] to display voting rules instead of elections.
Our distances between candidates depend solely on which voters approve them. For example, if two
candidates are approved by the same voters, then we view them as being at distance zero, and if the voters
can be partitioned into those approving either one candidate or the other, then we view these candidates as
maximally distant. To measure the distance between two committees, we first build a bipartite graph with
members of one committee on the left and members of the other committee on the right, where each two
candidates (from the other committees) are connected by an edge whose weight is equal to their distance.
The distance of the committees is the weight of the minimum-weight perfect matching in this graph. Our
main findings are as follows:
1. We show that computing two most distant committees in a given election is intractable in many
settings. This is somewhat unfortunate, as such committees would be useful to normalize the
distances between committees. Due to this hardness result, in the remainder of the paper we
normalize by observed maximum distances.
2. We compute the committees output by various multiwinner rules on a number of synthetic elections
and compute the average distances between committees provided by different rules. We find
that committees provided by Multiwinner Approval Voting (AV), Chamberlin–Courant (CC), and
Minimax Approval Voting tend to be the most distinct ones, while those provided by proportional
rules are between those of AV and CC (often, but not always, closer to AV), and far away from
those of Minimax.
3. For a number of proportionality axioms, we report how often the committees output by various
rules satisfy them. Surprisingly, our experiments show that the committees provided by proportional
voting rules (on sampled profiles) generally satisfy stronger proportionality properties than their
axiomatic analysis reveals. In particular, we find that some axiomatic distinctions are not observable
in our data set of 6000 instances.
All in all, we find that all proportional rules—including those that fail some of the stronger proportionality
axioms—are more or less similar to each other and form a well-defined cluster.
2
Preliminaries
For a given graph G = (V (G), E(G)) and a vertex x ∈ V (G), by N(x) we denote the neighbors of x (i.e.,
the set {y ∈ V (G) : (x, y) ∈ E(G)}) and by deg(x) we denote the degree of x (i.e., deg(x) = |N(x)|).
2
2.1
Elections
We consider the approval preference model. An election E = (C, V ) consists of a set C = {c1, . . . , cm}
of candidates and a collection V = (v1, . . . , vn) of voters, where each voter vi is endowed with a set of
candidates that they approve. For a voter v we denote his or her approval set as A(v), and for a candidate
c we write A(c) to denote the set of voters that approve c. Whenever we use this notation, the election in
question will be clear from the context.
2.2
Multiwinner Voting Rules
An approval-based multiwinner voting rule is a function f that given an election E = (C, V ) and
committee size k, k ≤ |C|, provides a family f(E, k) of size-k subsets of C, referred to as the winning
committees.
We give a brief overview of major approval-based multiwinner voting rule. We omit technical details
and refer to the survey by Lackner and Skowron [2023] for details. We assume that all voting rules use a
tiebreaking mechanism to ensure that they return exactly one winning committee (this property is known
as resoluteness).1
Thiele methods are an important class of approval-based multiwinner voting rules that select a
committee W maximizing value scw(W) = P
v∈V
P|A(v)∩W|
i=1
w(i) for some score function w. In
particular, Multiwinner Approval Voting (AV) is defined by w(i) = i (equivalently, AV selects the k
candidates that are approved by the largest number of voters). This rule is considered to be the prime
example of the principle of individual excellence. Chamberlin–Courant (CC) selects the committee W
that maximizes the number of voters that approve at least one candidate in W, i.e., w(1) = 1 and w(i) = 0
for all i ̸= 1. CC is an example of a rule designed to achieve diversity. Proportional Approval Voting
(PAV) and Sainte-Lagu¨e Approval Voting (SLAV) are defined by w(i) = 1
i and w(i) =
1
2i−1, respectively.
Finally, p-Geometric rules are defined by w(i) = p−i for positive constants p. The weights of PAV can
be viewed as the most proportional choice in this spectrum Aziz et al. [2017]; S´anchez-Fern´andez et al.
[2017]; weight functions that decrease more quickly than that of PAV (such as those used by SLAV and
p-Geometric rules) give more importance to small groups (degressive proportionality).
Sequential Thiele methods, also referred to as greedy Thiele rules in the literature, iteratively build a
committee by adding the candidate that increases the score scw most, starting with the empty committee
and iterating until k candidates are selected. We consider seq-PAV, seq-SLAV, and seq-CC. Greedy
Monroe Skowron et al. [2015] is a rule similar to seq-CC, with an additional constraint that each
committee member can represent at most n/k many voters.
Satisfaction Approval Voting (SAV) is defined similarly to AV and selects a committee W maximizing
P
v∈V
|A(v)∩W|
|A(v)| . Minimax Approval Voting (MAV) Brams et al. [2007] selects a committee W that
minimizes maxv∈V |A(v)\W|+|W \A(v)|, i.e., it minimizes the maximum Hamming distance between
a voter and the chosen committee.
Finally, we describe the intuition behind two slightly more complex rules: Sequential Phragm´en
(seq-Phragm´en) Brill et al. [2023] and the method of Equal Shares Peters and Skowron [2020]. Both
can be understood as mechanisms where voters use (virtual) budget to jointly pay for the selection of
candidates in the committee. The cost of adding a candidate is set (arbitrarily) to 1. With seq-Phragm´en,
voters start with a budget of 0, which is increased continuously until a group of voters can pay for the
first candidate. The cost (of 1) is shared among the members of the group and the process repeats until
the committee is filled. With Equal Shares, voters start with a budget of k/n. Each round, a candidate is
selected that requires the least budget per voter. This procedure may result in fewer than k committee
members (since the budget is fixed in advance), in which case seq-Phragm´en is used to fill the committee.
We refer to the works of Lackner et al. [2023] and Peters and Skowron [2020] for details.
1On the technical level, in our experiments we use the implementations of the rules provided in the abcvoting library Lack-
ner et al. [2023], together with their default tie-breaking.
3
2.3
Proportionality
Much of the recent progress in multiwinner voting has been dedicated to the concept of proportionality, to
formally define proportionality and to identify voting rules behaving proportionally. Since our framework
enables us to identify similar voting rules, we are able to ask whether “proportional” rules are indeed
similar. To this end, we consider four proportionality axioms. These axioms apply to committees and
a voting rule is said to satisfy such a property if it is guaranteed to return committees exhibiting this
property.
Definition 1 (S´anchez-Fern´andez et al. 2017). Given an election E = (C, V ), a committee W of
size k satisfies Proportional Justified Representation (PJR) if there is no group of agents N ⊆ V of size
|N| ≥ ℓ · n
k that jointly approves at least ℓ common candidates and
S
v∈N A(v) ∩ W
 < ℓ.
PJR is satisfied by PAV, seq-Phragm´en, Equal Shares, and Greedy Monroe2 (among the rules intro-
duced in Section 2.2).
Definition 2 (Aziz et al. 2017). Given an election E = (C, V ), a committee W of size k satisfies Extended
Justified Representation (EJR) if there is no group of agents N ⊆ V of size |N| ≥ ℓ · n
k that jointly
approves at least ℓ common candidates and for each voter v ∈ N, |A(v) ∩ W| < ℓ.
EJR strengthens PJR and is satisfied only by PAV and Equal Shares. Finally, Justified Representation
(JR) is the special case of PJR (and EJR) restricted to ℓ = 1.
Priceability Peters and Skowron [2020] is a notion of proportionality based on assigning budget to
voters.
Definition 3. Given an election E = (C, V ), a committee W satisfies priceability if there is a budget
b ≥ 0 and for each voter vi a spending function bi : C → R≥0 such that:
1. for each vi ∈ V , P
c∈C bi(c) ≤ b (voters do not spend more than b),
2. for each vi ∈ V , if c /∈ A(vi) then bi(c) = 0 (voters do not pay for candidates they do not approve),
3. if c ∈ W then P
vi∈V bi(c) = 1 and otherwise this value is 0 (payments only for committee
members), and
4. for each c /∈ W, P
vi∈A(c)
Disjoint Model. The disjoint model has three parameters, p ∈ [0, 1], ϕ ∈ [0, 1], and g ∈ N \ {0}. It is
similar to the resampling model, but instead of a single central ballot we have g central ballots. At
the beginning, we sample g disjoint central ballots uniformly at random (each of them approving
⌊pm⌋ candidates, and then proceed in the same manner as before, however before generating each
vote, we uniformly at random select one of our g central ballots on which our vote will be based.
Euclidean Models. The Euclidean model is parameterized by a radius r ∈ R+. For each voter and
each candidate we sample their ideal point in the t-dimensional Euclidean space (uniform in a
t-dimensional cube). Then, each voter approves all the candidates within radius r.
Party-list Model. The party-list model is parameterized by α and g. First, we divide the candidates into
g groups of size ⌊m/g⌋ each; we refer to these groups as the parties. (If m mod g ̸= 0 then some
candidates remain unapproved.) Second, we create an urn which contains a single vote for each
party, approving exactly its members. Then, we iteratively generate votes (one at a time): We draw
a vote from the urn, add its copy to the election, and return the vote to the urn together with αg
copies.
Pabulib Model. We also use real-life participatory budgeting (PB) data from Pabulib Faliszewski et al.
[2023], which we treat as a statistical culture over approval elections (in particular, we omit details
related to PB, such as the costs of the candidates). We selected 21 instances, i.e., all that contain
at least 100 candidates and 100 voters, and where the average number of approved candidates
per voter is at least 3 (for instances with truncated ordinal ballots, we convert them to approval
ones by approving all candidates that were ranked). To sample an election from Pabulib, we first
uniformly at random choose one of the original 21 instances, then we randomly select a subset of
100 candidates and a random subset of 100 voters. We omit voters who cast empty ballots.
The resampling and disjoint models are due to Szufa et al. [2022]. Various Euclidean models are
commonly used in the literature on elections Enelow and Hinich [1984, 1990]; we point to the recent
works of Bredereck et al. [2019] and Godziszewski et al. [2021], which use them in the approval setting.
The party-list model is an adaptation of the Polya-Eggenberger urn model Berg [1985]; McCabe-Dansted
and Slinko [2006].
3
Similarity Between Committees
Our idea of comparing voting rules is based on measuring the similarity between the committees that they
produce.
3.1
Basic Framework
Fix some election E = (C, V ) with candidate set C = {c1, . . . , cm} and voter collection V
=
(v1, . . . , vn). We assume that we have some distance d over the candidates, such that if d(ci, cj) is
small—for whatever “small” means under a given distance—then candidates ci and cj are similar, and
if it is large then they are not. In the most basic setting we could take the discrete distance, where
disc(ci, cj) = 0 exactly if i = j and disc(ci, cj) = 1 for every other case. Later we will discuss two more
distances.
Let X = {x1, . . . , xk} and Y = {y1, . . . , yk} be two size-k committees over C. We extend d to act on
committees as follows: We form a bipartite graph with members of X as the vertices on the left, members
of Y as the vertices on the right, and where for each x ∈ X and each y ∈ Y we have an edge with weight
d(x, y); if some candidate c belongs to both X and Y , then we have two copies of c, one on the left and
one on the right. The distance d(X, Y ) between X and Y is the weight of the minimum-weight matching
in this graph. One can verify that it indeed is a pseudodistance.
5
Proposition 1. For each (pseudo)distance d over the candidates, its above-described extension to
committees is a pseudodistance.
Proof. Let X = {x1, . . . , xk}, Y = {y1, . . . , yk}, and Z = {z1, . . . , zk} be three size-k committees
from the same election. By definition of d, we immediately get that d(X, X) = 0 and d(X, Y ) = d(Y, X).
It remains to show that d(X, Y ) ≤ d(X, Z) + d(Z, X). By reordering the members of X, Y , and Z, we
can assume that:
d(X, Z) = d(x1, z1) + · · · + d(xk, zk), and
d(Z, Y ) = d(z1, y1) + · · · + d(zk, yk).
Since d is a distance, for each i ∈ [k] we have d(xi, yi) ≤ d(xi, zi) + d(zi, yi). Further, we know that
d(X, Y ) ≤ d(x1, yi)+· · ·+d(xk, yk) (because instead of using the lowest-weight matching we use some
fixed one). By putting these inequalities together, we get that, indeed, d(X, Y ) ≤ d(X, Z)+d(Z, Y ).
Let us now consider concrete distance measures between candidates, all of which are well-known
distances (cf. Deza and Deza [2016]). The discrete distance is a rather radical approach as it stipulates that
no two candidates are ever similar to each other. Yet, one could argue that there are cases where some
candidates are clearly more similar to each other than others.
Example 1. Consider an election E = (C, V ) with C = {p, q, r, s} and voter collection V
=
{v1, v2, v3, v4). The approval sets are as follows:
A(v1) = {p},
A(v2) = {q, s},
A(v3) = {p, r, s},
A(v4) = {q}.
One could argue that candidates p and q are completely dissimilar because they are approved by
complementary sets of voters. On the other hand, half of the voters who approve p also approve r, and
every voter who approves r also approves p. The similarity between p and s seems a bit weaker than that
between p and r because there is a voter who approves s but does not approve p.
In addition to the discrete one, we consider the following two distances between the candidates (we
assume some election E with n voters):
1. The Hamming distance between candidates c and d, denoted by ham(c, d), is the number of voters
that approve one of them but not the other. Formally, we have:
ham(c, d) = |A(c) \ A(d)| + |A(d) \ A(c)|.
2. The Jaccard distance between candidates c and d is defined as:
jac(c, d) = 1 − |A(c) ∩ A(d)|
|A(c) ∪ A(d)| =
ham(c, d)
|A(c) ∪ A(d)|
There are two main differences between the Hamming and the Jaccard distances. The less important
one is that the former is not normalized and can assume values between 0 and n, whereas the latter
always assumes values between 0 and 1. Occasionally, we will speak of normalized Hamming distance
nham(c, d) = 1/n · ham(c, d).
The more important difference is in how both distances interpret lack of an approval for a candidate.
Under the Hamming distance, we assume that it is a conscious decision, indicating that a voter disapproves
of a candidate. Hence, if neither candidate c nor d is approved by a voter, then Hamming distance interprets
it as a sign of similarity between them.3 Under the Jaccard distance, we assume that we cannot make any
such conclusions.
3Yet, a voter might dislike c and d for two different reasons—e.g., one is too liberal and the other is not liberal enough, which
makes this approach questionable.
6
Example 2. Let us consider the election from Example 1. We have the following normalized Hamming
and Jaccard distances between the candidates:
nham(p, q) = 1,
jac(p, q) = 1,
nham(p, r) = 1/4,
jac(p, r) = 1/2,
nham(p, s) = 1/2,
jac(p, s) = 2/3,
nham(q, r) = 3/4,
jac(q, r) = 1.
As our intuition from Example 1 suggested, both our distances indicate that candidate p is completely
dissimilar from candidate q, and is more similar to r than to s. However, according to the normalized
Hamming distance q and r are not completely dissimilar, whereas they are at maximum distance according
to Jaccard.
3.2
Hardness of Finding Farthest Committees
Given an election and a committee size, it is convenient to know what is the largest possible distance
between two committees. In particular, one could use this value to normalize distances between committees
provided by various rules. Unfortunately, it turns out that finding two farthest committees under a given
distance is often intractable and in our experiments we will resort to normalizing by largest observed
distances. We define the FARTHEST COMMITTEES problem as follows.
Definition 4. In the FARTHEST COMMITTEES problem (FC) we are given an election (C, V ), an integer
k, and a pseudodistance d over the candidates. The goal is to output two committees, each of size k, which
maximize the distance d between them.
FC can be considered also in its decision variant, where we ask if there are two size-k committees
whose distance is at least a given value.
FC under the discrete distance is trivially polynomial-time solvable. In Theorem 1 we show that FC is
computationally hard in the cases of the Hamming distance and the Jaccard distance. This comes through
a reduction from the BALANCED BICLIQUE problem defined as follows.
Definition 5. In the BALANCED BICLIQUE problem (BB) we are given a bipartite graph G = (AG ∪
BG, EG) and an integer kG. The question is whether there exists a biclique of size kG × kG in G (kG-
biclique), i.e., a set of vertices X ∪ Y such that X ⊆ AG, Y ⊆ BG, |X| = |Y | = kG and for every
x ∈ X, y ∈ Y we have (x, y) ∈ EG.
BB is known to be NP-hard [Garey and Johnson, 1979, p. 196] and W[1]-hard w.r.t. kG Lin [2018].
The idea of the reduction is to encode vertices by candidates and edges by large distances (no-edges
are represented by slightly smaller distances). A technical issue is to define proper votes in order to: 1)
achieve required relation between the distances and, 2) forbid taking candidates to one committee that
come from both parts of the input bipartite graph.
Theorem 1. FARTHEST COMMITTEES is NP-hard and W[1]-hard w.r.t. k (the size of committees), even
in the case of the Hamming distance or the Jaccard distance.
Proof. First we show the theorem for the Jaccard distance. The reduction is from BALANCED BICLIQUE
(BB). We take an instance (G = (AG ∪ BG, EG), kG) of BB and produce an instance of the decision
version of FARTHEST COMMITTEES (FC) with the Jaccard distance.
For every vertex x ∈ AG ∪ BG we define a candidate x ∈ C. Thus, |C| = |AG ∪ BG|. We overload
the notation and by x we mean both a vertex x from the graph G and a candidate x from C. This is due to
the bijection between them.
7
We define the votes as follows. There is one voter vA with an approval set AG and one voter vB with
an approval set BG. Then, for every x ∈ AG we define a voter vx with an approval set {x} ∪ (BG \ N(x))
(recall that N(x) means the neighbors of x). Hence, we have |V | = |AG| + 2.
We ask for an existence of two committees of size k = kG with the Jaccard distance equal to k (indeed,
k is the largest possible Jaccard distance between two size-k committees).
In order to show correctness of the reduction, it suffices to prove the following lemma which implies
that only k-bicliques correspond to committees at Jaccard distance k.
Lemma 1. For every X, Y ⊆ AG ∪ BG, |X| = |Y | = k, the following equivalence holds: (X, Y ) is a
k-biclique in G if and only if jac(X, Y ) = k.
Proof. We will show two implications. First, we assume that X, Y ⊆ AG ∪ BG form a k-biclique in G.
It means that, w.l.o.g., X ⊆ AG and Y ⊆ BG. We will show that the two committees represented by
candidates from X and Y realize the Jaccard distance k.
First, we show that for any two candidates from different committees there is no voter approving both.
Claim 1. For every x ∈ X, y ∈ Y we have A(x) ∩ A(y) = ∅.
Proof. It is enough to show that every voter does not approve one of the candidates: x or y. Recall that
x ∈ AG and y ∈ BG. First, voter vB does not approve x and voter vA does not approve y. Second, as
(X, Y ) is a biclique we have y ∈ N(x), therefore voter vx does not approve y. For every z ∈ AG \ {x},
vz does not approve x.
Therefore, the Jaccard distance between every two candidates x ∈ X and y ∈ Y is equal to
1 − |A(x)∩A(y)|
|A(x)∪A(y)|
Claim1
=
1. Hence, any perfect matching between X and Y (so also the Jaccard distance) has
weight |X| = k, as required.
In order to show the implication in the other direction, let X, Y ⊆ C be two k-sized committees at
the Jaccard distance equal to k. We will show that (X, Y ) is a k-biclique. We will show that X and Y are
subsets of different parts of G. For this we use the following claim.
Claim 2. For every x ∈ X, y ∈ Y we have jac(x, y) = 1.
Proof. By contradiction, assume that for some x ∈ X and y ∈ Y we have jac(x, y) < 1. Then, a perfect
matching between candidates in X and candidates in Y that contains an edge (x, y) (which has weight
strictly smaller than 1) and any other edges (which have weights at most 1) has weight strictly smaller
than |X| = k. This is in contradiction with jac(X, Y ) = k.
Notice that for every x1, x2 ∈ AG we have |A(x1) ∩ A(x2)| = 1 (because of voter vA), hence
jac(x1, x2) < 1. Analogously, we have |A(y1) ∩ A(y2)| ≥ 1 for every y1, y2 ∈ BG (because of voter
vB), hence jac(y1, y2) < 1. It means that, due to Claim 2, one of the sets X and Y is a subset of AG and
the other one is a subset of BG.
It remains to show that (X, Y ) is a biclique, i.e., for every x ∈ X and y ∈ Y we have (x, y) ∈ EG.
Let us assume, by contradiction, that (x, y) /∈ EG, i.e., y /∈ N(x). It means that voter vx approves both
candidates, x and y. Therefore, |A(x) ∩ A(y)| ≥ 1 (actually, this is exactly 1), hence jac(x, y) < 1. This
is in contradiction with Claim 2, so (X, Y ) is truly a k-biclique.
It is straightforward to see that the reduction runs in polynomial time. Hence, NP-hardness of FC
follows from NP-hardness of BB [Garey and Johnson, 1979, p. 196]. Furthermore, BB is W[1]-hard w.r.t.
kG Lin [2018] and we have k = kG, so indeed, FC is W[1]-hard w.r.t. k.
In the case of the Hamming distance we use the same reduction with the following changes: (1)
Instead of one voter vA we add |AG| many voters vA; (2) Instead of one voter vB we add |AG| many
voters vB; (3) We add |AG| many degree-correcting voters who do not approve any candidates from AG
8
but, for every candidate y ∈ BG, exactly deg(y) many of them approve y; (4) We ask for an existence of
two committees of size k with the Hamming distance equal to k · (3|AG| + 1). After such modifications
we still have C = AG ∪ BG, but we have |V | = 4 · |AG|.
In order to show correctness of the reduction it is enough to prove the following lemma.
Lemma 2. For every X, Y ⊆ AG ∪ BG, |X| = |Y | = k, the following equivalence holds: (X, Y ) is a
biclique of size k × k in G if and only if ham(X, Y ) = k · (3|AG| + 1).
Proof. The proof is analogous to that of Lemma 1. The main difference is in calculating pairwise distances
between candidates because we have a different set of voters and we use the Hamming distance instead of
the Jaccard one.
The distances between candidates are as follows (in the calculations below, the first term contains the
Hamming distance realized by all the voters vA and vB, the second one regards all the voters vx, x ∈ AG,
and the third one regards the degree-correcting voters):
1. If x1, x2 ∈ AG then ham(x1, x2) = (0) + (2) + (0) = 2.
2. If y1, y2 ∈ BG then ham(y1, y2) ≤ (0) + (|AG|) + (|AG|) = 2 · |AG|.
3. If x ∈ AG, y ∈ BG, (x, y) /∈ E then ham(x, y) = (2|AG|) + (|AG| − deg(y) − 1) + (deg(y)) =
3 · |AG| − 1.
4. If x ∈ AG, y ∈ BG, (x, y) ∈ E then ham(x, y) = (2|AG|) + (|AG| − deg(y) + 1) + (deg(y)) =
3 · |AG| + 1.
Using the distances calculated above and analogous arguments as in the proof of Lemma 1, we obtain that
two committees corresponding to two parts of a k-biclique realize the required distance k · (3|AG| + 1).
Furthermore, the required distance between committees can be realized only by two committees where
all pairs of candidates (one candidate from one committee) are at the Hamming distance 3 · |AG| + 1. It
means that each such pair defines an edge in the original graph, hence a k-biclique.
This finishes the proof of the theorem.
FC with the normalized Hamming distance is also NP-hard because its objective function is just scaled
compared to the Hamming distance.
3.2.1
Parameterized Hardness
A brute-force algorithm for FARTHEST COMMITTEES (for any distance measure) runs in time m2k ·
poly(n, m) (by evaluating all possible pairs of k-sized committees). Unfortunately, we cannot hope for
large improvements over this as stated below.
Proposition 2. Under a randomized version of the Exponential Time Hypothesis, there is no (n+m)o(
√
k)-
time algorithm for FARTHEST COMMITTEES. This holds even for the Hamming distance and for the
Jaccard distance.
Proof. The result follows from hardness of BALANCED BICLIQUE Lin [2018], which excludes (under the
same hypothesis) existence of |VG|o(
√
k)-time algorithm. Using our reduction from Theorem 1 and the
fact that n = O(|VG|), m = O(|VG|), the theorem follows.
Naturally, FC is FPT w.r.t. m (for any distance measure) by a brute-force algorithm which evaluates
Theorem 2. For a given candidate pseudodistance function d(x, y), which is a function of A(x) and A(y)
only, FARTHEST COMMITTEES under d is FPT w.r.t. n + k.
Proof. We say that two candidates x, y have the same type if they are approved by the same voters, i.e.,
A(x) = A(y). Let us denote the number of candidate types by t. Clearly, we have t ≤ 2n. We find a
solution by guessing how many candidates of every type are in both committees. For one committee
there are at most kt ≤ k2n many choices. Hence, we can find a solution after k2n+1 many checks, each in
polynomial time (by computing the minimum-weight matching).
Using more involved arguments we show that FC is FPT w.r.t. n. Such algorithm can be useful in the
case when the committee size k is much larger than n.
Theorem 3. For a given candidate pseudodistance function d(x, y), which is a function of A(x) and A(y)
only, FARTHEST COMMITTEES under d is FPT w.r.t. n.
The complete proof is in Appendix A. The main technique used to provide this result is formulating
FC as an Integer Linear Program (ILP) with the number of integer variables and the number of constraints
being a function of n. Then, it is enough to apply the result of Lenstra [1983] for solving ILPs. The main
idea for the ILP is to consider types of candidates—defined for every candidate c as a subset of voters
that approve c, i.e., A(c). There are at most 2n types of candidates. Notice that two candidates of the
same type have exactly the same distances to any other candidate. Hence, for each candidate type, by two
integer variables we encode how many candidates of this type are included in both committees. Then,
the objective function is to maximize the weight of a b-matching (a matching in which a vertex can be
matched multiple times) according to the capacities defined by the integer variables. The main technical
issue we faced is ensuring that the achieved matching of maximum weight is also the minimum-weight
matching among the chosen candidates. We overcome this obstacle by adding 25n constraints (one for
every cycle in a bipartite graph in which vertices represent types of candidates).
Rule
Resampling
Disjoint
Party-list
1D
2D
Pabulib
Equal Shares
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
seq-Phragm´en
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
PAV
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
seq-PAV
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
SLAV
1.0
1.0
1.0
1.0
0.95
1.0
1.0
1.0
0.36
1.0
1.0
1.0
0.88
1.0
1.0
1.0
0.99
1.0
1.0
1.0
1.0
1.0
1.0
1.0
seq-SLAV
1.0
1.0
1.0
1.0
0.95
1.0
1.0
1.0
0.36
1.0
1.0
1.0
0.86
1.0
1.0
1.0
0.99
1.0
1.0
1.0
1.0
1.0
1.0
1.0
Greedy Monroe
1.0
1.0
1.0
1.0
0.88
1.0
1.0
1.0
0.29
1.0
1.0
1.0
0.78
1.0
1.0
1.0
0.93
1.0
1.0
1.0
0.95
1.0
1.0
1.0
Geom. 2
1.0
1.0
1.0
1.0
0.95
1.0
1.0
1.0
0.14 0.55 0.55
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
Geom. 3
0.99
1.0
1.0
1.0
0.87 0.99 0.99
1.0
0.03
0.3
0.3
1.0
0.88
1.0
1.0
1.0
0.99
1.0
1.0
1.0
0.97
1.0
1.0
1.0
Geom. 4
0.96
1.0
1.0
1.0
0.82 0.98 0.98
1.0
0.02 0.23 0.23
1.0
0.77
1.0
1.0
1.0
0.96
1.0
1.0
1.0
0.93
1.0
1.0
1.0
Geom. 5
0.93
1.0
1.0
1.0
0.79 0.98 0.98
1.0
0.01
0.2
0.2
1.0
0.68
1.0
1.0
1.0
0.93
1.0
1.0
1.0
0.88
1.0
1.0
1.0
SAV
1.0
1.0
1.0
1.0
0.5
0.64 0.64 0.64 0.06 0.08 0.08 0.08
0.0
0.01 0.01 0.01
0.0
0.06 0.06 0.06 0.84
1.0
1.0
1.0
AV
1.0
1.0
1.0
1.0
0.49 0.63 0.63 0.63 0.06 0.08 0.08 0.08
0.0
0.01 0.01 0.01 0.01 0.12 0.12 0.12 0.91 0.98 0.98 0.98
Minimax AV
0.71 0.99 0.99 0.99 0.46 0.89 0.89 0.98
0.0
0.05 0.05
1.0
0.0
0.09 0.09 0.09 0.02 0.28 0.28 0.28 0.59 0.86 0.86 0.86
seq-CC
0.97
1.0
1.0
1.0
0.62 0.88 0.88
1.0
0.0
0.06 0.06
1.0
0.23 0.99 0.99
1.0
0.47 0.99 0.99
1.0
0.58
1.0
1.0
1.0
CC
0.21 0.61 0.65
1.0
0.62 0.85 0.86
1.0
0.0
0.04 0.04
1.0
0.23 0.98 0.99
1.0
0.42 0.98 0.99
1.0
0.56
1.0
1.0
1.0
Table 1: Fraction of instances satisfying priceability, EJR, PJR, and JR—presented respectively (from left
to right). If 1.0 is printed in bold, the corresponding axiom is guaranteed by the voting rule.
4
Map of Rules
In this section, we present our Map of Multiwinner Voting Rules. The map is constructed in the following
way. First, we generate a number of elections from several statistical cultures (details are described later
10
in the text). Second, we compute winning committees under the considered voting rules. We impose
resoluteness, i.e., each rule outputs exactly one winning committee. Third, we compute the distances
between winning committees using the Jaccard distance (we chose it over Hamming due to the way it
interprets the lack of an approval for a candidate). For each election, we normalize the Jaccard distance
by dividing it by the largest distance between two committees, outputted by our voting rules, for that
election. For each two rules and each statistical culture, we compute the average distance between the rules’
committees; this gives us distance matrices for the cultures. We embed these matrices in 2D-Euclidean
space using a variant of the algorithm of Kamada and Kawai [1989], as implemented by Sapała [2022].
If two rules output similar winning committees, then we view these two rules as similar; the algorithm
places such rules close to one another on the map.4 While different runs of the algorithm (on the same
distance data) may give somewhat different maps, the outputs tend to be similar, and we believe that the
maps give good intuitions regarding the relative distances between the rules.
4.1
Experimental Setup
We generated 6000 instances with 100 candidates and 100 voters from the six following statistical cultures
(1000 elections per culture): 1D-Euclidean with r = 0.05, 2D-Euclidean with r = 0.2, resampling with
p = 0.1 and ϕ ∈ {0,
1
999,
2
999, . . . , 998
999, 1}, disjoint with p = 0.1, ϕ ∈ {0,
1
999,
2
999, . . . , 998
999, 1}, and
g = 10, party-list with g = 10, and the Pabulib model. For all instances, we use a committee size of
k = 10.
4.2
Experimental Results
Results for priceability, EJR, PJR, and JR are presented in Table 1. Even though only Equal Shares and
seq-Phragm´en formally satisfy priceability, for all instances that we generated, the committees provided
by PAV and seq-PAV also satisfied priceability. Moreover, seq-Phragm´en and seq-PAV satisfy EJR on all
instances despite seq-Phragm´en only guaranteeing PJR Brill et al. [2023] and seq-PAV not even JR Aziz
et al. [2017]. These results indicate that instances witnessing that, e.g., seq-PAV fails EJR are rare. In turn,
we can conclude that rules such as seq-PAV have stronger proportionality properties than their axiomatic
analysis reveals.
Let us briefly discuss the existence of cohesive groups in our data, i.e., groups N ⊆ V with |N| ≥ ℓ· n
k
that jointly approves at least ℓ common candidates. If, for some reason, our data did not contain cohesive
groups for ℓ ≥ 2, then the notions of EJR, PJR and JR would coincide; this would render this comparison
meaningless. However, this is not the case. For example, for the Disjoint model, 311 of 1000 instances
have cohesive groups with ℓ = 2; for the Resampling model this holds for 717 of 1000 instances.
Further, for some models, one can see that JR is significantly easier to satisfy than, say, EJR. This is
particularly apparent for CC, which is guaranteed to satisfy JR but fails both PJR and EJR. Finally, we
note that our results also support the conclusion by Bredereck et al. [2019] that—under many statistical
cultures—committees satisfying JR often also satisfy PJR and EJR.
For three of our models, that is, disjoint, 1D-Euclidean, and Pabulib, we present maps of multiwinner
voting rules; see Figure 1 (each map is based on 1000 elections generated for the respective culture).
We start by analyzing the left map for the disjoint model. On the “west” side of the map, we have the
CC voting rule; this area can be interpreted as representing diversity: the rules located in this area select
committees that consist of diverse candidates. On the “east” side of the map we have AV; this area can be
interpreted as representing individual excellence: the rules located here select committees that consist
of individually best candidates, not taking into account whether the selected candidates are similar to
each other or not. In the central part of the map (highlighted in red), we have numerous rules considered
to be proportional, such as the PAV rule or seq-Phragm´en. Note that Geometric rules are close to the
“cluster” of proportional rules, but their distance increases for larger p-values. Finally, in the “south” area
4The code for the experiments is available at https://github.com/Project-PRAGMA/Map-of-Rules-IJCAI-2023.
11
Figure 1: Maps of multiwinner rules. Label “G-p” refers to the p-Geometric rule. The red area highlights
proportional rules.
we have Minimax AV, which is different from all the other rules. Importantly, this separation of diverse,
proportional and individually excellent rules arises naturally in our model.
As for the central map, which is based on 1D-Euclidean elections, the overall location of the rules is
similar. However, the “proportional cluster” is shifted toward CC. Moreover, SAV is more different from
AV than in the previous map. As 1D Euclidean is a simpler model, we generally see fewer details in this
map.
Finally, consider the map based on real-life elections from Pabulib. Here, we see more pronounced
differences (as in the disjoint model). Interestingly, PAV and seq-PAV are almost indistinguishable here.
Generally, it can be observed that the relative position of voting rules does not vary greatly across different
probability models. In particular, the proportional cluster can be observed in all maps.
5
Summary
We introduced a framework for comparing multiwinner voting rules based on the committees they select.
This framework gives a principled approach to identifying similarity of voting rules. When contrasted with
axiomatic analysis, we see that some rules (seq-Phragm´en and seq-PAV) are not only close to strongly
proportional rules (Equal Shares and PAV), but are also indistinguishable from an axiomatic point of view
in our dataset. This indicates that our distance-based approach indeed offers a finer view of multiwinner
rules, and thus complements a precise axiomatic analysis.
Acknowledgements
Martin Lackner was supported by the Austrian Science Fund (FWF), research grant P31890. Krzysztof
Sornat was supported by the SNSF Grant 200021 200731/1. We thank Jannik Peters for helpful feedback
and Pasin Manurangsi for a discussion on the hardness of the BALANCED BICLIQUE problem. This
project has received funding from the European Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme (grant agreement No 101002854).
12
References
H. Aziz, M. Brill, V. Conitzer, E. Elkind, R. Freeman, and T. Walsh. Justified representation in approval-
based committee voting. Social Choice and Welfare, 48(2):461–485, 2017. [→ p. 3, 4, 11]
S. Berg. Paradox of voting under an urn model: The effect of homogeneity. Public Choice, 47(2):377–387,
1985. [→ p. 5]
N. Boehmer, R. Bredereck, P. Faliszewski, R. Niedermeier, and S. Szufa. Putting a compass on the map
of elections. In Proceedings of IJCAI-2021, 2021. [→ p. 2]
S. J. Brams, D. M. Kilgour, and M. R. Sanver. A minimax procedure for electing committees. Public
Choice, 132(3–4):401–420, 2007. [→ p. 3]
F. Brandt and H. G. Seedig. A tournament of order 24 with two disjoint teq-retentive sets. arXiv preprint
arXiv:1302.5592, 2013. [→ p. 2]
F. Brandt, M. Chudnovsky, I. Kim, G. Liu, S. Norin, A. Scott, P. D. Seymour, and S. Thomass´e. A
counterexample to a conjecture of Schwartz. Social Choice and Welfare, 40(3):739–743, 2013. [→ p.
2]
R. Bredereck, P. Faliszewski, A. Kaczmarczyk, and R. Niedermeier. An experimental view on committees
providing justified representation. In Proceedings of IJCAI-2019, pages 109–115, 2019. [→ p. 5, 11]
M. Brill, J.-F. Laslier, and P. Skowron. Multiwinner approval rules as apportionment methods. Journal of
Theoretical Politics, 30(3):358–382, 2018. [→ p. 1]
M. Brill, R. Freeman, S. Janson, and M. Lackner. Phragm´en’s voting methods and justified representation.
Mathematical Programming, 2023. [→ p. 3, 11]
A. Cevallos and A. Stewart. A verifiably secure and proportional committee election rule. In Proceedings
of the 3rd ACM Conference on Advances in Financial Technologies, pages 29–42, 2021. [→ p. 1]
M. M. Deza and E. Deza. Encyclopedia of distances. Springer, fourth edition, 2016. [→ p. 6]
E. Elkind, P. Faliszewski, P. Skowron, and A. Slinko. Properties of multiwinner voting rules. Social
Choice and Welfare, 48(3):599–632, 2017. [→ p. 1]
J. Enelow and M. Hinich. The Spatial Theory of Voting: An Introduction. Cambridge University Press,
1984. [→ p. 5]
J. Enelow and M. Hinich. Advances in the Spatial Theory of Voting. Cambridge University Press, 1990.
[→ p. 5]
P. Faliszewski and N. Talmon. Between proportionality and diversity: Balancing district sizes under the
Chamberlin-Courant rule. In Proceedings of AAMAS-2018, pages 14–22, 2018. [→ p. 1]
P. Faliszewski, P. Skowron, A. Slinko, and N. Talmon. Multiwinner rules on paths from k-Borda to
Chamberlin-Courant. In Proceedings of IJCAI-2017, pages 192–198, 2017. [→ p. 1]
P. Faliszewski, P. Skowron, A. Slinko, and N. Talmon. Multiwinner voting: A new challenge for social
choice theory. In Ulle Endriss, editor, Trends in Computational Social Choice, chapter 2, pages 27–47.
AI Access, 2017. [→ p. 1]
P. Faliszewski, J. Flis, D. Peters, G. Pierczynski, P. Skowron, D. Stolicki, S. Szufa, and N. Talmon.
Participatory budgeting: Data, tools and analysis. pages 2667–2674, 2023. [→ p. 5]
13
M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NP-Completeness.
W. H. Freeman, 1979. [→ p. 7, 8]
G. Gawron and P. Faliszewski. Using multiwinner voting to search for movies. In Proceedings of
EUMAS-2022, pages 134–151, 2022. [→ p. 1]
M. Godziszewski, P. Batko, P. Skowron, and P. Faliszewski. An analysis of approval-based committee
rules for 2D-Euclidean elections. In Proceedings of AAAI-2021, pages 5448–5455, 2021. [→ p. 5]
J. Israel and M. Brill. Dynamic proportional rankings. In Proceedings of IJCAI-2021, pages 261–267,
2021. [→ p. 1]
T. Kamada and S. Kawai. An algorithm for drawing general undirected graphs. Information Processing
Letters, 31(1):7–15, 1989. [→ p. 11]
M. Lackner and P. Skowron. Consistent approval-based multi-winner rules. Journal of Economic Theory,
192:105173, 2021. [→ p. 1]
M. Lackner and P. Skowron. Multi-Winner Voting with Approval Preferences - Artificial Intelligence,
Multiagent Systems, and Cognitive Robotics. Springer Briefs in Intelligent Systems. Springer, 2023.
[→ p. 1, 3]
M. Lackner, P. Regner, and B. Krenn. abcvoting: A Python package for approval-based multi-winner
voting rules. Journal of Open Source Software, 8(81):4880, 2023. [→ p. 3]
H. W. Lenstra. Integer programming with a fixed number of variables. Mathematics of Operations
Research, 8(4):538–548, 1983. [→ p. 10, 21]
B. Lin. The parameterized complexity of the k-biclique problem. Journal of the ACM, 65(5):34:1–34:23,
2018. [→ p. 7, 8, 9]
T. Lu and C. Boutilier. Budgeted social choice: From consensus to personalized decision making. In
Proceedings of IJCAI-2011, pages 280–286, 2011. [→ p. 1]
J. McCabe-Dansted and A. Slinko. Exploratory analysis of similarities between social choice rules. Group
Decision and Negotiation, 15:77–107, 2006. [→ p. 5]
D. Peters and P. Skowron. Proportionality and the limits of welfarism. In Proceedings of EC-2020, pages
793–794, 2020. [→ p. 3, 4]
A. Renwick and J. B. Pilet. Faces on the Ballot: The Personalization of Electoral Systems in Europe.
Oxford University Press, 2016. [→ p. 1]
L. S´anchez-Fern´andez, E. Elkind, M. Lackner, N. Fern´andez, J. A. Fisteus, P. Basanta Val, and P. Skowron.
Proportional justified representation. In Proceedings of AAAI-2017, pages 670–676, 2017. [→ p. 3, 4]
K. Sapała. Algorithms for embedding metrics in Euclidean spaces. Master’s thesis, AGH University of
Science and Technology, 2022. [→ p. 11]
P. Skowron, P. Faliszewski, and A. Slinko. Achieving fully proportional representation: Approximability
result. Artificial Intelligence, 222:67–103, 2015. [→ p. 3]
B. Subiza and J. E. Peris. A representative committee by approval balloting. Group Decision and
Negotiation, 26(5):1029–1040, 2017. [→ p. 1]
S. Szufa, P. Faliszewski, P. Skowron, A. Slinko, and N. Talmon. Drawing a map of elections in the space
of statistical cultures. In Proceedings of AAMAS-2020, pages 1341–1349, 2020. [→ p. 2]
14
S. Szufa, P. Faliszewski, L. Janeczko, M. Lackner, A. Slinko, K. Sornat, and N. Talmon. How to sample
approval elections? In Proceedings of IJCAI-2022, pages 496–502, 2022. [→ p. 5]
15
Appendix
A
Proof of Theorem 3
We will formulate FARTHEST COMMITTEES (FC) as an Integer Linear Program (ILP).
First, we define a set of types of candidates as T = {tV ′ : V ′ ⊆ V }, i.e., every subset of voters
V ′ ⊆ V defines a candidate type tV ′ ∈ T. We have |T| = 2n. Naturally, a candidate c has type tA(c). By
c ∈ t we denote that c has type t and by |t| we denote the number of candidates of type t. Notice that
two candidates of the same type have exactly the same distances to every other candidate. Formally, for
every a, b ∈ t and c ∈ C we have d(a, c) = d(b, c). This holds because of the assumption that d(x, y) is a
function of A(x) and A(y) only.
We define two type-variables yL
t , yR
t for each candidate type t. yL
t and yR
t encode how many candidates
of type t are included in the first committee and in the second committee respectively.
yL
t , yR
t ∈ {0, 1, . . . , min{k, |t|}}
∀t ∈ T.
(1)
There are 2n+1 such type-variables.
We add two constraints meaning that we choose exactly k candidates to each of the committees.
X
t∈T
yR
t =
X
t∈T
yL
t = k.
(2)
Using thus defined and constrained type-variables, we can define corresponding two committees as an
outcome for FC (this is because candidates of the same type are indistinguishable from the objective
function’s point of view).
Next, we need to encode the matching of the candidates. For this, we define edge-variables x(r,s),
for every r, s ∈ T, that encode the number of candidates of type r matched with the same number of
candidates of type s.
xe ∈ {0, 1, . . . , k}
∀e ∈ T × T.
(3)
There are 4n such edge-variables. The values of the edge-variables define a b-matching.
We write the following b-matching constraints in order to relate the number of chosen candidates of a
certain type and the number of matched candidates of this type.
X
r∈T
x(t,r) = yL
t
∀t ∈ T,
(4)
X
r∈T
x(r,t) = yR
t
∀t ∈ T.
(5)
There are 2n+1 such constraints. We say that a b-matching (ˆxe)e∈T×T is consistent with the type-variables
values (ˆyt)t∈T if the b-matching constraints are satisfied.
Next we define an objective function of our ILP (although we will add more constraints later). It is the
weight of a b-matching induced by the edge-variables.
maximize
X
e∈T×T
xe · we,
(6)
where w(r,s), r, s ∈ T is the distance between candidates, where one is of type r and one is of type s.
The ILP defined above is not complete. In its current form, the output would be the maximum weight
matching with exactly k edges, but what we need to achieve is maximum over the choice of the committees
of the minimum weight perfect matching between candidates from these committees. In order to obtain
16
proper output for FC, we keep the objective function exactly as it is defined in Formula (6), but we will add
more constraints in order to make sure that the chosen b-matching between the candidate types (defined
by edge-variables) is a minimum weight b-matching between chosen types of candidates (defined by
type-variables).
It is enough to forbid choosing expensive edges (by restricting edge-variables) in case one could
change them for cheaper ones while keeping the same values of type-variables. We will use Lemma 4 in
order to define proper constraints, but first, we introduce some notation regarding cycles in a bipartite
graph.
A cycle γ in a complete bipartite graph with parts (L, R) = (T, T) is a sequence of alternating distinct
vertices from L and R, i.e.:
γ = (cL
1 , cR
1 , cL
2 , cR
2 , . . . , cL
ℓ , cR
ℓ ),
where cL
i ∈ L, cR
i ∈ R and cX
i ̸= cX
j for every i ̸= j and X ∈ {L, R}.5 A cycle γ can be also represented
through an edge notation, i.e., γ = Mγ
LR ∪ Mγ
RL, where Mγ
LR = {{cL
1 , cR
1 }, {cL
2 , cR
2 }, . . . , {cL
ℓ , cR
ℓ }} and
Mγ
RL = {{cR
1 , cL
2 }, {cR
2 , cL
3 }, . . . , {cR
ℓ−1, cL
ℓ }, {cR
ℓ , cL
1 }}. Notice that both Mγ
LR and Mγ
RL are matchings.
We denote the weight of a matching M as w(M) = P
e∈M w(e). Let Γ be a set of all cycles in (L, R).
Lemma 3. It holds that |Γ| ≤ (2n)(2n+1+1) ≤ 25n.
Proof. In a bipartite graph a cycle has even length and the longest cycle contains all 2n+1 vertices. There
are at most (2n)(2i) many cycles of length 2i, where i ∈ {1, 2, . . . , 2n}. We give an upperbound for |Γ| as
follows.
|Γ| ≤
2n
X
i=1
(2n)(2i) ≤
2n
X
i=1
(2n)(2n+1) ≤ (2n)(2n+1+1).
This shows the first inequality in the lemma statement.
The case n = 1 is polynomial time solvable by taking to one committee the maximum number of
approved candidates and taking to the other committee the maximal number of disapproved candidates.
Therefore n ≥ 2, which implies that (2n)(2n+1+1) ≤ 25n.
In Lemma 3 we gave an upperbound for |Γ| without optimizing constants as the main message of
our result is that our algorithm has running time with double-exponential dependence on n. Notice that
one can use an algorithm from Theorem 2 for n upperbounded by a large constant (e.g. 100) in order to
provide a polynomial time algorithm (with a degree above 2100 in the case of n ≤ 100). Then one can
assume that n > 100 and provide better constants in the exponents immediately. Instead of optimizing
constants it is an open question whether a single-exponential algorithm exists.
In Lemma 4, we show that a b-matching has minimum weight (among b-matchings consistent with
the given type-variables values) if and only if there is no cycle γ ∈ Γ such that the b-matching uses the
more expensive matching among {Mγ
LR, Mγ
RL}.
Lemma 4. For given values of type-variables (ˆyt)t∈T and a b-matching B = (ˆxe)e∈T×T which is
consistent with (ˆyt)t∈T , we define a matching MB = (min{1, ˆxe})e∈T×T . The following two statements
are equivalent:
1. B has minimum weight among b-matchings consistent with (ˆyt)t∈T .
2. For every cycle γ ∈ Γ such that w(Mγ
LR) ̸= w(Mγ
RL) it holds that:
argmaxM∈{Mγ
LR,Mγ
RL}{w(M)} ̸⊆ MB.
5In order to have unique representation of a cycle we assume that, for arbitrary fixed indexing of vertices (types of candidates),
the first (resp. the second) vertex in a cycle is an element from L (resp. R) with the smallest index.
17
Proof. (1. ⇒ 2.) First we show that the first statement implies the second statement. By contraposition,
let us assume that there exists a cycle γ ∈ Γ such that w(Mγ
LR) > w(Mγ
RL) (the other case is analogous)
and Mγ
LR ⊆ MB. We will show that the first statement of the lemma is false by constructing a new
consistent b-matching with smaller weight. The idea is to remove the more expensive matching Mγ
LR
from B and add the less expensive matching Mγ
RL.
Let Mγ
LR = (αe)e∈T×T and Mγ
RL = (βe)e∈T×T , where αe, βe ∈ {0, 1} are b-matching representa-
tions of matchings Mγ
LR, Mγ
RL.
We define B′ as follows:
B′ = (ˆxe − αe + βe)e∈T×T = (ue)e∈T×T ,
and through the following three claims we will show that B′ is the desired b-matching.
Claim 3. B′ is a b-matching.
Proof. We need to show that ue ∈ {0, 1, . . . , k} for every e ∈ T × T.
The value ue = ˆxe − αe + βe is obviously an integer as ˆxe ∈ {0, 1, . . . , k} and αe, βe ∈ {0, 1}.
We have ue ≥ ˆxe − αe ≥ 0, where the last inequality follows from the fact that Mγ
LR ⊆ MB, hence
ˆxe ≥ 1.
We have ue ≤ ˆxe + βe ≤ k + 1. We will show that ˆxe + βe = k + 1 implies a contradiction, hence
ue ≤ k.
Let us assume that ˆxe + βe = k + 1. It means that ˆxe = k. B-matching B is consistent with (ˆyt)t∈T ,
so it satisfies b-matching constraints. Hence we have:
X
t∈T
X
r∈T
ˆx(t,r)
(4)
=
X
t∈T
yL
t
(2)
= k.
It means that all other edge-variables except ˆxe have value 0. It implies that MB contains only one edge,
i.e., e. Moreover, we know that Mγ
LR ⊆ MB, so |Mγ
LR| ≤ 1. A cycle γ has length at least 2, so |Mγ
LR| ≥ 1.
Finally, we can say that Mγ
LR = {e}. But then also Mγ
RL = {e}. Hence we have w(Mγ
LR) = w(Mγ
RL)
and this is in contradiction with the assumption that weights of both matchings are different.
Claim 4. B′ is consistent with (ˆyt)t∈T .
Proof. We need to show that both b-matching constraints, i.e. Constraints (4)–(5), are satisfied by B′. We
will prove it for Constraint (4) and the proof for the other constraint is analogous.
First, we notice that for every t ∈ L the number of edges from Mγ
LR which are incident to t is the
same as the number of edges from Mγ
RL which are incident to t (this number is equal to 0 or 1). Hence we
have:
X
r∈T
(β(t,r) − α(t,r)) = 0.
(7)
Then, for every t ∈ T we have:
X
r∈T
u(t,r) =
X
r∈T
ˆx(t,r) +
X
r∈T
(β(t,r) − α(t,r))
(4),(7)
=
ˆyL
t .
We define the weight of a b-matching A as:
w(A) =
X
e∈T×T
Ae · we.
18
Claim 5. B′ has strictly smaller weight than B.
Proof. The weight of B′ is upperbounded as follows:
w(B′) =
X
e∈T×T
ue · we
=
X
e∈T×T
ˆxe · we −
X
e∈T×T
αe · we +
X
e∈T×T
βe · we
= w(B) − w(Mγ
LR) + w(Mγ
RL) < w(B),
where the last inequality follows from the assumption that w(Mγ
LR) > w(Mγ
RL).
(2. ⇒ 1.) Next, we show that the second statement implies the first one. By contraposition, let us
assume that the first statement is false, i.e., there exists a b-matching B′ = (ue)e∈T×T consistent with
(ˆyt)t∈T such that w(B′) < w(B). We will construct a cycle which shows that the second statement of the
lemma is false.
As both b-matchings B and B′ are consistent with (ˆyt)t∈T , every vertex of (L, R) has the same
number of incident edges in both b-matchings. Formally, for every t ∈ T we have:
X
r∈T
ˆx(t,r)
(4)
= yL
t
(4)
=
X
r∈T
u(t,r),
X
r∈T
ˆx(r,t)
(5)
= yR
t
(5)
=
X
r∈T
u(r,t).
We define two (multi)graphs as follows (for each edge, the value in a vector defines multiplicity of an
edge):
(B − B′)+ = ((ˆxe − ue)+)e∈T×T ,
(B′ − B)+ = ((ue − ˆxe)+)e∈T×T ,
where x+ = max{x, 0}. The symmetric difference of B and B′ is defined as
B△B′ = ((ˆxe − ue)+ + (ue − ˆxe)+)e∈T×T ,
B△B′ is a (multi)graph on (T, T) with the following property.
Claim 6. Every vertex of B△B′ has even degree.
Proof. In a graph B + B′ = (ˆxe + ue)e∈T×T , every vertex t ∈ L has even degree 2yL
t and every vertex
t ∈ R has even degree 2yR
t . We define min{B, B′} = (min{ˆxe, ue})e∈T×T . Obviously, in a graph
2 · min{B, B′} every vertex has even degree. Finally, we observe that B△B′ is equal to (B + B′) −
2 min{B, B′}, so also in B△B′ every vertex has even degree.
Claim 6 implies that B△B′ can be covered by cycles from Γ (possibly using a given cycle multiple
times). Let us fix such a cycle cover Λ.
For a cycle γ ∈ Λ we define two matchings:
γB = {e ∈ γ : ˆxe > ue} = γ ∩ (B − B′)+,
γB′ = {e ∈ γ : ˆxe < ue} = γ ∩ (B′ − B)+.
Notice that {γB, γB′} = {Mγ
LR, Mγ
RL} and γB ̸= γB′. Intuitively, γB is a matching included in b-
matching B and γB′ is a matching included in b-matching B′.
19
We are ready to show that the second statement of the lemma is false. We have the following sequence
of (in)equalities:
0 < w(B) − w(B′) =
X
e∈T×T
(ˆxe − ue) · we
=
X
e∈T×T
ˆxe>ue
(ˆxe − ue) · we −
X
e∈T×T
ˆxe<ue
(ue − ˆxe) · we
=
X
e∈(B−B′)+
we −
X
e∈(B′−B)+
we
=
X
γ∈Λ

X
e∈γ∩(B−B′)+
we −
X
e∈γ∩(B′−B)+
we

=
X
γ∈Λ
 X
e∈γB
we −
X
e∈γB′
we

=
X
γ∈Λ
Now we are ready to define constraints which forbid using more expensive matching if there is a
cheaper one. Let Γ1 = {γ ∈ Γ : w(Mγ
LR) > w(Mγ
RL)} and Γ2 = {γ ∈ Γ : w(Mγ
LR) < w(Mγ
RL)}, i.e.,
they contain cycles with different matching weights.
X
e∈Mγ
LR
ze ≤ |Mγ
LR| − 1
∀γ ∈ Γ1,
(11)
X
e∈Mγ
RL
ze ≤ |Mγ
RL| − 1
∀γ ∈ Γ2.
(12)
There are at most |Γ| many such constraints, i.e., at most 25n (due to Lemma 3).
Due to the new constraints, at least one variable ze, e ∈ Mγ
LR is equal to 0 (in the case of γ ∈ Γ1; the
case γ ∈ Γ2 is analogous). Therefore, due to Lemma 5, at least one variable xe, e ∈ Mγ
LR is equal to 0, so
at least one edge e from a more expensive matching is not taken to a solution. Consequently, such more
expensive matching is excluded from a set of feasible solutions. This finishes the description of our ILP.
Lenstra [1983] showed that an ILP can be solved using O(p2.5p+o(p) ·|I|) arithmetic operations, where
|I| is the input size and p is the number of integer variables.
In our ILP we have: 2n+1 type-variables (Equation (1)); 4n edge-variables (Equation (3)); and 4n
restriction-variables (Equation (8)). Hence, in total, there are O(4n) many variables.
In our ILP we have: 2 cardinality constraints (Equation (2)); 2n+1 b-matching constraints (Equa-
tions (4) and (5)); 2·4n constraints from Inequalities (9) and (10); and 25n constraints from Inequalities (11)
and (12). Hence, in total, there are O(25n) many constraints.
Therefore, using the result of Lenstra [1983] we can find a solution to FC in time upperbounded by:
(O(4n))O(4n) · O(25n) ≤ 2O(n·4n) · 25n ≤ 2O(5n).
This finishes the proof of the theorem.
21
"
"We introduce a family of PCNs (Payment Channel Networks) characterized by a semi-hierarchical topology and a custom set of channel rebalancing strategies. This family exhibits two interesting benefits, if used as a platform for large-scale, instant, retail payment systems, such as CBDCs: Technically, the solution offers state-of-the-art guarantees of fault-tolerance and integrity, while providing a latency and throughput comparable to centralized systems; from a business perspective, the solution perfectly suits the 3-tier architecture of the current banking and payment ecosystem (central banks / commercial banks / retail users), assigning a pivotal and peculiar role to the members of each tier. Furthermore, the cryptographic privacy of payments for retail users—typical of PCNs such as the public Lightning Network—is largely (possibly fully) retained. We study the system by simulating a scaled-down version of a hypothetical European CBDC, exploring the trade-offs among liquidity locked by market operators, payment success rate, throughput, latency, and load on the underpinning blockchain.","Retail instant payment systems—such as credit/debit card circuits or Central Bank Digital Currencies (CBDCs)— manage a very high load of transactions, in the order of 104–105 TPS (see Sect. V-A). Centralized systems are often deployed to ingest and settle all such transactions timely. Performance is not all though: In the special case of CBDCs, further business and technical requirements are put into place1, such as the possibility to set a cap on the amount of liquidity users can amass (wallet cap), strong privacy guarantees for all payment data, small or no fees for citizens (“free for basic use”), and the possibility to be usable by unbanked people. Finally, retail instant payment systems have to be embedded into—and play nice with—the pre-defined, multi-tier structure of the existing banking and payment infrastructure: a pyramid that sees the Central Bank(s) at the top, a set of authorized intermediaries (such as commercial banks and similar financial institutions) in the middle, and retail users (such as citizens and merchants) at the bottom—providing each actor with clear and strong incentives to adopt and use the system itself2.","nanIn this work, we move the first steps towards tentatively answering such a question in the positive (for a CBDC). First essential point: Most blockchains are well-known for their limited throughput, so on-chain settlement of retail payments is very difficult to achieve. We forgo such a perspective entirely and embrace the off-ledger paradigm, whereby scalability is achieved by an additional “payment channel network”, or PCN (called 2nd layer) built on top of the actual blockchain (see Sect. II). In the wild, 2nd layer networks—e.g., the Lightning Network [4]—evolve freely, as peer-to-peer systems: They are unstructured and adopt best-effort payment routing strategies. Unfortunately, their anatomy (i) is not coherent with the 3-tier structure of the monetary system, (ii) is oblivious to most of the business requirements of a CBDC, and (iii) provides a level of scalability that—albeit significant w.r.t. the underpinning blockchains—is still insufficient for a large-scale retail instant payment system.nannan","We address these issues by devising a family of PCNs with a partially constrained topology, called “Semi-Hierarchical Payment Channel Networks”, or SH-PCNs (Sect. III), endowed with a set of custom balancing strategies (Sect. IV). SH-PCNs are designed to play nice with the financial ecosystem and to exhibit performances akin to a retail payment system. Assuming SH-PCNs allow us to achieve the required throughput and latency with sufficient reliability (a question we ponder in Sect. VI-B), we can shift our attention to the benefits of using a distributed, blockchain-based, 2-layer solution: Blockchains introduce tools capable of increasing integrity, availability, fault-tolerance, and verifiability of monetary exchanges; at the 2nd (retail) layer, they (i) accommodate, by design, levels of privacy similar to that of cash; (ii) naturally handle wallet caps; (iii) give the central bank (CB) and the market control on the transaction fees for retail users. We do not model a full-scale SH-PCN system just yet, but a scaled-down version. This reduced model is still capable of exhibiting the key dynamics we want to investigate, such as the trade-offs among the liquidity locked in channels, the success rate of payments, their latency, and the transactional demand on the underlying 1st-layer blockchain (see Sect. VIII). Our preliminary results suggest that the performance of SH-PCN is satisfactory for retail payments; that SH-PCNs map naturally onto the banking/payment ecosystem; and that the most typical CBDC-specific requirements are naturally met. In the following sections, we provide a description of the solution that carefully combines the technical, PCN-oriented angle with the business, CBDC-oriented perspective.nan","In a PCN, two nodes create a bilateral payment channel by “pre-funding” it, i.e., by locking some amount of liquidity, called channel capacity, into a 2-of-2 multisig UTXO, using a single on-chain funding transaction; the sum of nodes’ balance in the channel can never exceed the payment channel capacity. After the channel is created (i.e., the funding transaction is confirmed), the nodes can exchange multiple, instant, off-chain payments, which update the balances of nodes and eliminate the need to constantly execute on-chain transactions.","We introduced a class of self-rebalancing semi-hierarchical PCNs, and showed experimentally—on a small-scale model based on the payment patterns and structure of the euro area—that they seem to (a) ensure good performances (in terms of latency, throughput, and success rate of payments) and (b) suit the existing 3-tier banking/payment architecture naturally.",Self-Balancing Semi-Hierarchical PCNs for CBDCs,"Marco Benedetti, Francesco De Sclavis, Marco Favorito, Giuseppe Galano, Sara Giammusso, Antonio Muci, Matteo Nardelli","Self-Balancing Semi-Hierarchical PCNs for CBDCs
Marco Benedetti, Francesco De Sclavis, Marco Favorito, Giuseppe Galano,
Sara Giammusso, Antonio Muci, Matteo Nardelli∗
Technical Report CFC.CRYPTO.CS/2024/1
Applied Research Team (ART) - IT Department - Bank of Italy†
Abstract—We introduce a family of PCNs (Payment Channel
Networks) characterized by a semi-hierarchical topology and a
custom set of channel rebalancing strategies. This family exhibits
two interesting benefits, if used as a platform for large-scale,
instant, retail payment systems, such as CBDCs: Technically, the
solution offers state-of-the-art guarantees of fault-tolerance and
integrity, while providing a latency and throughput comparable
to centralized systems; from a business perspective, the solution
perfectly suits the 3-tier architecture of the current banking
ecosystem (central banks / commercial banks / retail users),
assigning a pivotal and peculiar role to the members of each tier.
Furthermore, the cryptographic privacy of payments for retail
users—typical of PCNs such as the public Lightning Network—
is largely (possibly fully) retained. We study the system by
simulating a scaled-down version of a hypothetical European
CBDC, exploring the trade-offs among liquidity locked by market
operators, payment success rate, throughput, latency, and load
on the underpinning blockchain.
I. INTRODUCTION
Retail instant payment systems—such as credit/debit card
circuits or Central Bank Digital Currencies (CBDCs)—
manage a very high load of transactions, in the order of
104–105 TPS (see Sect. V-A). Centralized systems are often
deployed to ingest and settle all such transactions timely.
Performance is not all though: In the special case of CBDCs,
further business and technical requirements are put into place1,
such as the possibility to set a cap on the amount of liquidity
users can amass (wallet cap), strong privacy guarantees for all
payment data, small or no fees for citizens (“free for basic
use”), and the possibility to be usable by unbanked people.
Finally, retail instant payment systems have to be embedded
into—and play nice with—the pre-defined, multi-tier structure
of the existing banking and payment infrastructure: a pyramid
that sees the Central Bank(s) at the top, a set of authorized
intermediaries (such as commercial banks and similar financial
institutions) in the middle, and retail users (such as citizens
and merchants) at the bottom—providing each actor with clear
and strong incentives to adopt and use the system itself2.
We ask: Can a blockchain-based payment system address all
these concerns and requirements while bringing to the table
tangible benefits in comparison to a centralized solution?
*E-mail
address
of
the
authors
are
in
the
form
[firstname].[lastname]@bancaditalia.it,
except
for
giuseppe.galano2@bancaditalia.it.
†The views expressed in this paper are those of the authors and do not
necessarily reflect those of the Bank of Italy.
1For the case of the digital euro, see e.g. [1], [2]
2For an introduction to this three-tier structure, see [3].
In this work, we move the first steps towards tentatively
answering such a question in the positive (for a CBDC).
First essential point: Most blockchains are well-known
for their limited throughput, so on-chain settlement of re-
tail payments is very difficult to achieve. We forgo such
a perspective entirely and embrace the off-ledger paradigm,
whereby scalability is achieved by an additional “payment
channel network”, or PCN (called 2nd layer) built on top of
the actual blockchain (see Sect. II). In the wild, 2nd layer
networks—e.g., the Lightning Network [4]—evolve freely, as
peer-to-peer systems: They are unstructured and adopt best-
effort payment routing strategies. Unfortunately, their anatomy
(i) is not coherent with the 3-tier structure of the monetary
system, (ii) is oblivious to most of the business requirements
of a CBDC, and (iii) provides a level of scalability that—
albeit significant w.r.t. the underpinning blockchains—is still
insufficient for a large-scale retail instant payment system.
We address these issues by devising a family of PCNs with a
partially constrained topology, called “Semi-Hierarchical Pay-
ment Channel Networks”, or SH-PCNs (Sect. III), endowed
with a set of custom balancing strategies (Sect. IV). SH-PCNs
are designed to play nice with the financial ecosystem and to
exhibit performances akin to a retail payment system.
Assuming SH-PCNs allow us to achieve the required
throughput and latency with sufficient reliability (a question
we ponder in Sect. VI-B), we can shift our attention to
the benefits of using a distributed, blockchain-based, 2-layer
solution: Blockchains introduce tools capable of increasing
integrity, availability, fault-tolerance, and verifiability of mone-
tary exchanges; at the 2nd (retail) layer, they (i) accommodate,
by design, levels of privacy similar to that of cash; (ii) naturally
handle wallet caps; (iii) give the central bank (CB) and the
market control on the transaction fees for retail users.
We do not model a full-scale SH-PCN system just yet, but
a scaled-down version. This reduced model is still capable of
exhibiting the key dynamics we want to investigate, such as the
trade-offs among the liquidity locked in channels, the success
rate of payments, their latency, and the transactional demand
on the underlying 1st-layer blockchain (see Sect. VIII).
Our preliminary results suggest that the performance of SH-
PCN is satisfactory for retail payments; that SH-PCNs map
naturally onto the banking/payment ecosystem; and that the
most typical CBDC-specific requirements are naturally met.
In the following sections, we provide a description of the
solution that carefully combines the technical, PCN-oriented
angle with the business, CBDC-oriented perspective.
arXiv:2401.11868v1  [cs.DC]  22 Jan 2024
Tier
Role of 
nodes
Node 
owners
Size (magnitude) 
actual / simulated
Sample 
SH-PCN
Sub-network 
topology
Channel 
capacities
Rebalancing
strategy
1
RS, MH
Central 
Banks
 101 / 100
Structured, 
hierarchical
“infinite” 
capacity
N/A
2
LSP 🔵  
CS ⚫
Commercial 
Banks /
Financial 
Institutions
104 / 101  🔵
103 / n.a.  ⚫
Free-form, 
organised 
by market 
operators
Allocated & 
optimised by 
market 
operators
Just-in-time 
rebalancing via 
submarine 
swaps
3
End-Users:
payers, 
payees
Retail users: 
merchants 🔴 
citizens 🟡
106 / 103  🔴
108 / 105  🟡
Fixed topology
(Policy decision)
Fixed capacity
(Policy decision)
Just-in-time
(reverse) 
waterfall
c1
c2
c3
c4
m1
m2
Fig. 1.
Structure and main features of a Semi-Hierarchical Payment Channel Network (SH-PCN), plus three sample payment routes: One Point-of-Sale
payment from retail user c4 (a citizen) to merchant m2; one Peer-to-Peer transaction from c2 to c3; one eCommerce route from c1 to a foreign merchant
m1. As usual with source-based onion-routed protocols, no actor (other than the payer and payee) has full information about any of these payments.
II. THE PAYMENT CHANNEL NETWORK (PCN) MODEL
In a PCN, two nodes create a bilateral payment channel
by “pre-funding” it, i.e., by locking some amount of liquidity,
called channel capacity, into a 2-of-2 multisig UTXO, using a
single on-chain funding transaction; the sum of nodes’ balance
in the channel can never exceed the payment channel capacity.
After the channel is created (i.e., the funding transaction is
confirmed), the nodes can exchange multiple, instant, off-chain
payments, which update the balances of nodes and eliminate
the need to constantly execute on-chain transactions.
The safety of payments within the channel is ensured
through the previously described pre-funding and a crypto-
graphic mechanism called channel revocation: If a cheating
attempt by one party is detected, the other is entitled to
claim the entire channel capacity both as compensation and
as a penalty for the attempted cheating. For a comprehensive
description of how PCNs work and how they relate to the
underpinning blockchain, we refer the reader to [4], [5].
The largest PCN in existence is the public Lightning
Network (LN). Recent studies (e.g., [6], [7]) show that the
topology of the LN exhibits the characteristics of a small-
world and scale-free network. As shown in [8], such scale-free
topologies are sub-optimal in terms of locked liquidity. And
while an alternative star-like network configuration would help
decrease liquidity inefficiencies, it would also reduce fault-
tolerance, security, and privacy of payments [6].
III. STRUCTURE OF A SEMI-HIERARCHICAL PCN
The current monetary system—which is the target deploy-
ment environment for our solution—is based on a three-
tier banking system. The pyramid sees the CB at the top, a
set of authorized intermediaries (such as commercial banks
and similar financial institutions) in the middle, and retail
users (such as citizens and merchants) at the bottom. For an
introduction to this three-tier architecture, see [3].
These different (business) roles will be mapped onto various
different (technical) roles in the PCN subclass we study,
and the hierarchical anatomy of the banking system will be
reflected into the (semi) hierarchical topology of the network.
In SH-PCNs, we make a distinction among four classes of
technical nodes, managed by four types of business actors,
organized into 3 tiers: Monetary Hubs (MHs) at the top,
Lightning Service Providers (LSPs) and Custodian Services
(CSs) in the middle, and End-Users (EUs) at the bottom, as
represented in Fig. 2.
• Monetary Hubs. Tier-1 nodes typically managed by a
CB (or a set of CBs in the same monetary area): At the
on-chain layer, they are fully responsible for the operation
of the ledger where transactions to open/close channels
are stored and where (CB) liquidity is locked. At the
off-ledger layer, they operate nodes with considerable
liquidity, which are largely (or fully) interconnected, and
which act as Routing Service Providers (RSPs) for LSPs
in Tier-2. Thus, MHs act as business-to-business entities
and do not interact with EUs;
• Lightning Service Providers. Tier-2 nodes acting as the
core, distributed “service providers” of the system. LSPs
freely open channels towards each other, forming a “small
world”-like network, ensuring that fault tolerance and
the other benefits of distributed networks are achieved.
Disconnected LSP sub-networks, if they exist, are linked
via RSP(s). From a business perspective, this role is as-
signed to banks and other financial institutions, which are
properly supervised and own large amounts of liquidity.
LSPs offer their service to EUs by opening channels
with them, thus providing them with connectivity and
reachability within the payment network.
• Custodian Services. In addition to off-chain CB liquidity
in the SH-PCN, EUs generally own one or more accounts
at Tier-2 CSs, such as banks or exchanges, which allow
EUs to deposit/withdraw retail CBDC to/from their per-
sonal accounts3, which amass “commercial bank money”.
The CSs are connected to the LSPs network. The EU does
not necessarily have direct channels opened with CSs, as
their monetary interactions happen either out-of-band (see
Sect. IV-B and IV-C) or through LSP intermediaries.
3These account-based ledgers are managed by a (centralized) system
entirely decoupled from the PCN, under the responsibility of the CS.
• End-users. Tier-3 nodes managed by “end-users” (i.e.,
citizens and merchants): They access the network con-
necting to one or more LSPs in a non-custodial way,
thereby preserving the privacy of their payments. Cru-
cially (from a business perspective) we have that (i)
EUs do not open channels with each other4, and (ii) EU
channels—with a known capacity—are only opened with
one or more authorized LSP(s): We can place a cap on
the maximum amount of retail CBDC present in the user
wallet, without sacrificing privacy.
So, Tier-1 is the domain of CBs; Tier-2 is where market
operators act; EUs live at Tier-3, in a hierarchy where MHs
only connect to LSPs and EUs only connect to LSPs and
CSs. The number of EUs is much larger than the number of
LSPs/CSs, which in turn are many more than the MHs.
IV. AUTOMATED CHANNEL REBALANCING TECHNIQUES
Absent any dynamics in network topology (see Sect. V-C),
the performance (success rate, throughput, latency) of SH-
PCNs subject to most types of sustained loads of payments
among end users degrades over time: Channels become more
and more unbalanced, hence unable to participate into the
routing of an increasing percentage of payments.
To avoid such progressive occlusion, we define 3 rebalanc-
ing techniques, aimed at improving channel lifetimes while
minimizing locked liquidity. We make the hypotheses that (i)
LSPs and routing nodes try to keep their channels balanced by
proactively issuing trustless “submarine swaps” between on-
chain and off-chain liquidity; (ii) end users rebalance their
channels in real-time, when needed, by withdrawing funds
from—or depositing funds to—their trusted CS(s)5.
A. Submarine swaps between two LSP/MH nodes
A submarine swap is a transaction that exchanges a given
amount of some on-chain digital asset with the same amount
of the off-chain form of the same asset (e.g., x bitcoin are
moved from A to B on the blockchain and “simultaneously”
x bitcoin go from B to A in the Lightning Network).
Submarine swaps can be trustless: At no point during the
process either party has access to the other party’s fund, and
no third party takes temporary custody of the asset. The swaps
are atomic, with a binary outcome: Either the two parties
successfully exchange their assets, or the swap fails.
2. Send Pmt
LSP2
BLOCKCHAIN
LSP1
3. Rcv Success
4. Claim HTLC
1. Prepare HTLC
Fig. 2. Atomic submarine swap functionality
4This is not a technical limitation, but a policy option: If end users were
allowed to access the Layer 1 blockchain to freely open channels among them,
the level of privacy of payments would quickly approach full anonimity.
5These regulated entities are the points where AML/CFT activities can
be performed, possibly issuing Suspect Transactions Reports (STRs) to the
CB.
To implement the trustless requirement of a submarine
swap, both the on-chain and the off-chain transactions rely
on Hash-Time Locked Contracts (HTLCs). Fig. 2 represents
two LSPs that share an unbalanced channel, with almost all
liquidity held at the LSP1 end. LSP1 wants to rebalance the
channel by transferring assets on the payment channels with
LSP2, who in turn should move the same amount of assets on-
chain to LSP1. They proceed as follows. LSP2, sender of the
on-chain transaction, generates a preimage, whose hash serves
as the foundation for constructing both the on-chain and off-
chain HTLCs, and moves funds to the on-chain HTLC (step 1).
LSP2 also generates an off-chain invoice using the same hash
and asks LSP1, sender of the off-chain transaction, to pay the
invoice. After the chain confirms the locking of funds in the
on-chain HTLC, LSP1 can safely pay the invoice (step 2). To
claim the off-chain funds, LSP2 has to reveal the preimage to
LSP1 (step 3), who in turn can use it to complete the process,
by claiming the funds from the on-chain HTLC (step 4). At
any time, if any of the two parts misbehaves, the other can get
its funds thanks to the unlocking conditions of the HTLCs.
Due to the reliance on on-chain transactions, a submarine
swap takes time (at least the confirmation time of the HTLC
in step 1) and possibly requires a fee (depending on the policy
enacted by the MH). Moreover, the transactional capacity of
the underlying blockchain limits the number of submarine
swaps per unit of time that can be performed by the SH-PCN.
B. Waterfall6 rebalance
The waterfall mechanism allows end-users—in particular
those having high inbound traffic (e.g., merchants)—to always
be able to get paid, even if the amount P to be received raises
the user balance B above the channel capacity limit C (i.e., the
wallet cap). This is achieved by automatically depositing the
amount D = max (B + P − C, LD), where LD is a minimum
amount that the user is willing to deposit, to a linked CS
account. Fig. 3 illustrates the waterfall functionality. When
2. Notify Pmt
5. Rcv Pmt
4. Fwd/Rcv Deposit
END USER
CUSTODIAN
LSP
1. Fwd Pmt
4. Snd Deposit
3. Request Deposit
Fig. 3. Waterfall functionality implementation in a PCN
the LSP receives a payment to forward (step 1), but the end
user’s channel does not have enough outbound liquidity, the
LSP notifies the user about the incoming payment (step 2),
and delays the payment forwarding until the expiration of a
timeout. The user requests a real-time deposit to their CS (step
3), which in turn sends the deposit via the same LSP, thus
rebalancing the channel (step 4). If the channel is successfully
rebalanced within the timeout, the LSP forwards the payment
6The terms “waterfall” / “reverse waterfall” are lifted from [2]. We
present a PCN-oriented, fully compliant implementation of the processes they
designate.
to the user (step 5); otherwise, the payment fails. Messages
exchanged in steps 2 and 3, in bold, are not part of the LN
specification, but are specific to our protocol. From a business
perspective, this mechanism implements a deposit of retail
CBDC liquidity from the wallet of an EU into an account
held by the same EU at some CS, in commercial bank money,
triggered when the CBDC wallet is about to overflow the cap.
C. Reverse waterfall rebalance
The reverse waterfall functionality allows retail users—in
particular those having high outbound traffic (e.g., citizens)—
to automatically fund a payment channel before making trans-
actions too large for the current state of that channel. Fig.
4 illustrates the reverse waterfall functionality. If there are
2. Rcv Withdrawal
2. Snd/Fwd Withdrawal
CUSTODIAN
END USER
LSP
3. Snd Pmt
1. Request Withdrawal
Fig. 4. Reverse waterfall functionality implementation in a PCN
insufficient funds in the payment channel to cover the payment
amount P, users could request a withdrawal (step 1), thus
taking out some money from their CS account (step 2). The
withdrawal amount is W = max (LW − B, P − B), where
LW represents a minimum amount that the user is willing
to keep in its wallet for future use. Once the withdrawal
has successfully transferred liquidity from the linked CS to
the payment channel, the user can send the payment (step
3). The message exchanged in step 1 is not part of the
LN specification, but is specific to our protocol. From a
business perspective, this mechanism implements a withdrawal
of commercial bank money from the account of an EU at their
CS, which is returned as CBDC liquidity put into the wallet of
the same EU; this process is triggered when the CBDC wallet
has insufficient funds to operate.
V. THE REAL WORLD AND OUR SMALL-SCALE MODEL
A. Patterns of real-world retail payments
Retail payment systems manage a very high load of trans-
actions. A commercial, global-scale example is VISA, which
recently disclosed that its network can execute more than
65, 000 transactions per second at its peak7.
For a public retail payment system, such as a prospective
CBDC for the euro area, we can refer to the “Study on the
payment attitudes of consumers in the euro area” (SPACE)
by the ECB [9]. The study reports a survey among a ran-
domly selected sample of the population across 19 euro area
countries. Participants were requested to document their point-
of-sale (POS), peer-to-peer (P2P), and online transactions in
a one-day diary. The survey also includes recurring payments
that are out of the scope of our investigation. The key results
of SPACE 2022 are summarized in Table I.
7https://usa.visa.com/solutions/crypto/deep-dive-on-solana.html
TABLE I
STATISTICS ON NON-RECURRING PAYMENTS. ECB SPACE 2022.
Type
N.
(%)
Distribution (%) of Range Amount (C)
< 5
[5, 10) [10, 20) [20, 30) [30, 50) [50, 100) > 100
PoS
80
21
17
21
13
13
10
5
Online
17
10
11
20
15
17
16
11
P2P
3
(14)∗
(11)∗
(22)∗
(16)∗
(14)∗
(11)∗
(12)∗
∗ Unfortunately, the frequencies of amount ranges in the P2P scenario are
not available in SPACE; these values have been estimated by the authors.
In addition, we consider the Annex 1 of the ECB’s “Market
Research on Potential Technical Approaches for a Digital
Euro” [1], which states that, on average, individuals within the
euro area engage in two financial transactions per day, encom-
passing various payment methods and interaction points. The
“Large” adoption case considered in this report assumes that
70% of the eurozone population will use the CBDC for 35%
of all transactions. This implies approximately 2,000 TPS on
average. Peak values may be larger by an order of magnitude.
These numbers—in the 103–105 range—corroborate our
working hypothesis about the need of an off-ledger layer
to face retail payments. And, they provide us with a clear
reference point to designing proportioned scaled-down models.
The aforementioned SPACE report, as seen in Table I,
provides us with further interesting information about real-
world payments, which we can use to assemble a believable
small-scale model: the distribution of transaction amounts, the
proportion among Point-of-Sale, eCommerce, and Peer-to-Peer
payments, and the tendency of retail payments to be domestic
(within the same country) rather than cross-border.
Finally, it is worth recalling that a payment is defined as
“instant” in this context if it completes within 10 seconds [10].
B. Size of our small-scale model
We study a network on a scale ∼1:1000 relative to the actual
population of the euro area (344 million citizens). Our network
thus consists of 300k individuals (retail users) at Tier 3.
We incorporate a merchant every 100 people, and a service
provider every 10k end-users, in line with the 6.4k inhabitants
per bank branch in the EU [11], for a total of 3k merchants
and 30 service providers in Tier 2. We model 3 countries, rep-
resentative of different scales found in the euro zone: a small
country (such as Cyprus, with ∼1M citizens), one medium-
size country (such as Finland, ∼6M), and a large country
(such as Italy, ∼60M). Each user and each intermediary is
associated with one of these countries, and the distribution of
nodes among countries is proportional to their population.
At Tier 1, each country has a (node representing its) national
Central Bank (NCB). NCBs live in the same monetary area
and are interconnected. Cross-national payments are assumed
to be a fraction of the national payments, namely 5%. Overall,
there are 3 countries (CBs), 3·101 service providers (financial
institutions), 3 · 103 merchants, and 3 · 105 citizens.
The number of payments per second is also scaled 1:1000,
proportionally to the population: Given the average 2,000 TPS
from Sect. V-A, we simulate 2 TPS for 24 hours, or 172, 800
payments, which move 9.75M units of value. When we study
a peak load, we move an order of magnitude up, to 20 TPS
(for 12 hours), thus to 950, 000 payments (53.2M in value).
In our model, we scaled down the number of users, and this
brought about a proportional reduction in both the number of
payments per second they generate, and in the size of the
routing network. We do not scale quantities that do not vary
with the size of the population: The cap on end-user wallets
is taken as-is from what has been hypothesized in [12], [13]
and [14], i.e., C3000; the distribution of payment amounts
is harmonized with the figures in Table I; the definition of
“instant” payment still requires completion within 10s [10].
C. Simplifying assumptions
We adopt several simplifying assumptions to facilitate the
initial study of our SH-PCN under high load. We assume that:
• the topology of the SH-PCN is static, i.e., after the initial
setup, no new channels are opened, nor are existing ones
closed, during the time window of interest;
• each EU, whether a person or a merchant, has only a
single channel, with a single LSP;
• LSPs are connected to their reference MH/RSP in such
a way that full connectivity is ensured;
• the LSP of an EU also acts as their reference, trusted
CS (the roles of LSP and CS are collapsed on the same
actor);
• EUs, LSPs, RSPs, CSs and EUs are always online;
• fees are not imposed to route payments; so the best
payment route is the shortest path between the payer and
the payee (and not the one occasioning the smallest fee);
• the blockchain introduces a constant delay (modeled on
the speed of actual permissioned fault-tolerant systems,
such as [15]) when on-chain transactions are needed.
VI. MODELING AND SIMULATING SH-PCNS
In order to study how SH-PCNs behave if used to support
a retail payment system akin to the one described in Sect. V,
given the rebalancing strategies from Sect. IV, we model
the entire system, and we simulate its execution over a full
business day, under different load and liquidity conditions.
Our architecture is composed of 4 logical sub-models:
A. The
SH-PCN
topology
model
and
generator
(Sect. VI-A), tasked with creating realistic SH-PCNs;
the network it creates is given as input to:
B. The SH-PCN runtime model and simulator (Sect. VI-B),
which simulates the working of actual PCNs to great
detail; this simulator is set in motion by an incoming
stream of payment requests, generated by:
C. The load model and generator for retail payments
(Sect. VI-C), which outputs 24 hours worth of simulated
payments, according to Sect. V-B; as the PCN evolves
under the payment load, it is analyzed and updated by:
D. The PCN realtime rebalancer (Sect. VI-D), which mon-
itors the PCN and applies the strategies from Sect. IV to
keep the system running at full success rate.
Sub-models (A) and (C) are inspired to the structure,
throughput, and functioning of actual European payment
systems/patterns, although we operate at a reduced scale
(Sect. V-B) and under simplifying assumptions (Sect. V-C).
A. SH-PCN topology model and generator
We designed a generator of random SH-PCN instances, such
as the one in Figure 1. The generator takes as an input a list
of parameters—such as the number of entities to place in each
tier, the constraints on their inter-connections, the capacity
of channels among different actors, etc—and generates one
random SH-PCN instance with the given features (in a human-
friendly JSON format). While the generator is capable of con-
structing 1:1 networks, using the parameters from Section V-A,
we use the small scale variant from Section V-B to generate
the SH-PCN we experiment on in Section VII.
Our SH-PCN random model is a composition of various ran-
dom graph models, each representing the connections between
different categories of nodes and within the same category:
1) Central banks are interconnected in a clique formation,
with each link having a capacity of C500 million.
2) The interconnections between LSPs are modeled using
a Watts-Strogatz graph model [16], using an average
degree k = 4 and a rewiring probability p = 0.1; the
capacities associated with channels in this subnetwork
are not fixed: A range of different values for such
capacities is explored in the experimental section.
3) The links between CBs and LSPs are generated by divid-
ing the LSPs into distinct subsets, each corresponding
to a CB/country. Subsequently, a channel is established
from each CB to every LSP in its respective subset. The
subset sizes is log-normally distributed (with µ = 0 and
σ = 1); the capacity of such CB-LSP channels is not
fixed: The simulation explores a range of possibilities.
4) The creation of channels between LSPs and end users,
encompassing both citizens and merchants, is ap-
proached similarly to the CB-LSP connections, albeit
with fixed capacities, which represent the cap on users’
wallets (CBDC requirement). Specifically, channels con-
necting LSPs with citizens have a capacity of C3000.
In contrast, channels linking LSPs with merchants vary
based on the merchant’s size: small (S), medium (M),
and large (L) merchants are assigned channels with
capacities of C5k, C50k, and C500k, respectively.
Overall, here is a summary of the submodels in a SH-PCN:
Connection Type
Random Graph Model
Capacity
Among CBs
Clique
C500M
CBs to LSPs
Log-normal (µ = 0, σ = 1)
several
Among LSPs
Watts-Strogatz (k = 4, p = 0.1)
several
LSPs to Citizens
Log-normal (µ = 0, σ = 1)
C3000
LSPs to Merchants
Log-normal(µ = 0, σ = 1)
C5·103/4/5
(S / M / L)
Payment success
notiﬁed to payer
FORWARD
PAYMENT
GENERATE
PAYMENT
FIND
PATH
SEND
PAYMENT
RECEIVE
FAIL
RECEIVE
PAYMENT
FORWARD
FAIL
FORWARD
SUCCESS
RECEIVE
SUCCESS
NOTIFY
PAYMENT
1/TPS s
1ms
~100ms
500ms
~100ms
~100ms
10ms
~100ms
~100ms
~100ms
~100ms
~100ms
~100ms
~100ms
500ms
~100ms
~100ms
Payment failure
notiﬁed to payer
Payment received
successfully by payee
~ 1 BT
10ms
~300ms
Fig. 5. Event diagram of our ROSS-based, CLoTH-like PCN simulator.
B. SH-PCN runtime model and simulator
Several PCN simulators exist in the literature (see e.g. [17]–
[21]). No one dominates the others along all dimensions, as
different authors rely on the simulation of specific features of
the PCN protocol, but not others. For our purposes, we selected
CLoTH [21], which stands out as an open-source sequential
Discrete Event Simulation (DES) engine that reproduces the
code of a mainstream lightning node implementation8, partic-
ularly focusing on routing and on the HTLC mechanics.
CLoTH features a straightforward simulation run-time and
is used in several papers (see e.g., [22], [23]). However, it lacks
support for parallel processing. We tested its serial engine
and realized it does not perform adequately when very large
networks and large payment volumes are to be analyzed.
Therefore, we developed a CLoTH-like simulator within
ROSS [24], a Parallel DES. ROSS operates on a distributed-
memory architecture, and can run on multiprocessor systems.
We exploit its native advanced features, e.g., parallel execu-
tion, optimistic event scheduling, and load distribution.
CLoTH on ROSS. Within ROSS, we implement and reuse
the same events and states as CLoTH: Fig. 5 contains the
event diagram of our PCN simulator. The elements and edges
in black are part of the core CLoTH simulation. Elements in
color represent extensions to CLoTH that we introduce: One
for dynamic load generation—in green; two for the waterfall
and reverse waterfall functionalities—respectively in red and
blue; one for the submarine swap functionality—purple.
For each event, we implemented all the relevant ROSS
callbacks: the event handler, the reverse event handler used
in optimistic simulations, and the commit event handler that
is used to permanently update the state of LPs. These handlers
call the appropriate (and properly wrapped) CLoTH code.
Executing a payment. When the payment request is re-
ceived, a new FIND PATH event is scheduled for the payment
process (or a withdrawal if the payer does not have enough
liquidity to execute the payment, see Sect. VI-D). The FIND
PATH computes the path between the sender and the receiver
8https://github.com/lightningnetwork/lnd
of the payment. If a path cannot be found, the payment
fails. Otherwise, the payment follows the same life-cycle as
in CLoTH, going through the sequence SEND PAYMENT,
FORWARD PAYMENT (if the payment traverses more than
one edge), RECEIVE PAYMENT, FORWARD SUCCESS (if
any) and RECEIVE SUCCESS for a successful payment.
Conversely, a non-successful payment goes through FORWARD
FAIL (if any) and RECEIVE FAIL. A payment may fail due
to an insufficient balance to forward it along the path selected
by the sender. Once a RECEIVE FAIL notifies the failure to
the sender, the payment can be re-tried by scheduling a new
FIND PATH event. This loop is allotted 10 seconds, after
which the process is aborted and the payment fails definitely.
From SH-PCN nodes to ROSS processes. ROSS uses two
structures to represent processing units:
• A Logical Process (LP), which serves as the fundamental
computation unit. LPs manage their own event queue,
receive messages, process the events they contain, and
optionally send additional messages to other LPs.
• A Physical Process (PE), which are the actual processes
of the distributed MPI architecture, to which LPs run on.
PEs orchestrate the parallel simulation and manage the
communication with other PEs for the LPs they host.
In our simulation we have an LP for each node in the SH-
PCN (all included: Tier 1, 2, and 3 nodes are simulated). We
include a single auxiliary LP for each PE that orchestrates the
payment generation for its own LPs (see Sect. VI-C).
Parallelizing the simulation. In ROSS, simulations can be
executed sequentially or in parallel, using either a conservative
or an optimistic approach. We employ an optimistic simulation,
whereby ROSS activates a synchronization mechanism called
“Time Warp”, which uses a detection-and-recovery protocol to
synchronize the computation: Any time an LP detects out-of-
order events, it rolls those events back and re-executes them.
Load distribution is crucial in a distributed-memory PDES.
Co-locating chatty LPs on the same PE significantly improves
performance, as messages can be exchanged in memory in-
stead of relying on inter-process communication. For our PCN
simulation we devise a domain-specific mapping strategy.
First, we construct a weighted graph of nodes, where edges
are labeled with an estimated communication frequency. Then,
in order to achieve a balanced distribution of citizens, mer-
chants and intermediaries among partitions, we assign weights
to each node. Following the approach outlined in [25], we par-
tition the resulting graph using METIS [26], which minimizes
the sum of the weights of the edge traversing partitions and
equalizes the total node weights among partitions.
C. Model and generator for retail payments
The payment generator schedules a GENERATE PAYMENT
event (green in Fig. 5) with a frequency that depends on
a target global payment rate. As detailed in Sect. V-B, the
global rate in the small scale model is between 2 and 20
PPS (Payments Per Second). Such global rate is apportioned
to PEs, in such a way that each PE generates a local load
that is proportional to the number of retail users co-located
on the PE itself. Then, the payment generator creates a single
random payment between two users that are randomly selected
according to the SPACE model described in Sect. V.
We assume that geographically close users exhibit a greater
tendency to transact with each other, hence users predomi-
nantly conduct domestic transactions within their own country,
while cross-border ones occur infrequently.
The load generator selects the payment sender first, which
is always a retail user. Then, it selects a random payment
scenario according to the SPACE distribution of payment
contexts (POS, eCommerce, P2P) and to the static cross-
border payment probability. Depending on the selected pay-
ment scenario, the generator selects a random receiver among
merchants (for POS and eCommerce payments) or retail users
(for P2P payments). Finally, the payment generator chooses
the transaction amount according to the SPACE conditional
probability distribution of amounts (see Table I).
D. PCN real-time rebalancer
The rebalancer applies the strategies described in Sect. IV
to the payment system simulated via our custom blending of
CLoTH and ROSS, described in Sect. VI-B. The technical
implementation of the strategies is as follows.
Waterfall. Whenever a payment that is about to be re-
ceived by an EU would result in the channel balance ex-
ceeding its cap, the payment is notified to the receiver using
the NOTIFY PAYMENT event, and the processing of the
FORWARD PAYMENT event is delayed. Upon receiving the
notification, the payee initiates another transaction, i.e., a
deposit, to transfer the excess liquidity to a CS. The deposit
process is triggered by the FIND PATH event, using a delay
that takes into account the roundtrip time for the deposit
request. Then, FIND PATH computes the (best) path between
the sender of the deposit (receiver of the payment) and their
CS. If the deposit is completed before the original payment
expires, the FORWARD PAYMENT can be completed and the
payment can be received successfully. Otherwise a FORWARD
FAIL event is generated. See the red state/links in Fig. 5.
Reverse Waterfall. When a payment is generated, and the
sender has an insufficient balance, the simulation initiates a
new transaction, i.e., a withdrawal, from the sender’s asso-
ciated CS to the sender itself, using a FIND PATH event.
The original payment is placed in a queue of payments
awaiting withdrawals, that is managed by the sender. Once
the withdrawal is successfully received (RECEIVE PAYMENT
event), the sender of the original payment triggers it creating
the respective FIND PATH event. See the blue link in Fig. 5.
Submarine Swap. A submarine swap is initiated by either
an LSP or an RSP in response to a FORWARD PAYMENT
event when a payment about to be forwarded would result in
the channel balance exceeding a predefined unbalancedness
threshold (e.g., 80% of its capacity). The simulation then has
the LSP generate a FIND PATH event towards its channel
counterparty, with a delay of 1 block time, as outlined in the
6k
100
50
60k
1k
500
600k
10k
5k
6M
100k
50k
60M
1M
500k
600M
10M
5M
Per Channel Liquidity
10
60
80
90
95
100
Payment Success Rate (%)
Total Network Liquidity
Tier 1 - Tier 2:
Within Tier 2:
Rebalancing mode
Full
(Rev) Waterfall only
None
Fig. 6. Percentage of payments routed instantly by the SH-PCN (y axis) as
a function of the liquidity in the network/channels (x axis). Log-log scale.
purple line of Fig. 2. The off-chain payment simulates the
submarine leg of the swap, depicted in steps 2 and 3 of Fig. 2,
while the block time delay of the event simulates the on-chain
HTLC preparation of step 1. See the purple link in Fig. 5.
VII. EXPERIMENTAL EVALUATION
Payments in PCNs can fail for a number of reasons, the
main one being the insufficient capacity of some link along the
route chosen by the payer/source. When a route fails, another
one is tried, until success—or until some failure criterion
is met, e.g., a timeout expires. As the load on the system
increases, the probability of failure is expected to increase,
both because channels may become unbalanced and because
there is an increasing resource contention from multiple in-
flight concurrent payments with partially overlapping routes.
In this context—fully captured by our simulation—we show
(a) under which conditions our rebalancing strategies are able
to keep the payment success rate at 100% under a constant
load; (b) that payments remain “instant” in all cases; (c) how
the system reacts to a sudden 10x increase in the number of
payments per seconds; (d) how the trade-off between liquidity
locked in channels and their rebalancing plays out.
A. Payment Success Rate
We present our SH-PCN with a constant transaction load as
per Sect. VI-C, exerted for 24 hours (simulation time).
Figure 6 shows the success rate of payments as a function of
the total amount of liquidity Tier 1 and 2 operators are willing
to lock into the system. By “total amount”, we mean the sum
of the capacity of all the channels among Tier 2 operators,
plus the channels between Tier 1 and Tier 29.
9The other capacities are not accounted for, because: Capacities from
Tier 2 to Tier 3 (i.e., wallet caps) are set by policy and are not subject
to adjustments for performance reasons; channels within Tier 1 nodes are
practically infinite (managed by CBs); channels among Tier 3 entities do not
exist in our setting.
0
500
1000
1500
2000
2500
3000
Time (ms)
0
20
40
60
80
100
Percentage of payments
routed successfully
Fig. 7. Time-to-completion of payments: Cumulative distribution.
The success rate is non-zero (∼16%) even for zero total
liquidity, because there is a fraction of payments that happens
between retail customers of the same Tier 2 entity, which can
be routed successfully without traversing internal Tier 2 links.
As the liquidity of the network grows, so does the per-
centage of payments successfully regulated within “instant”
timespans. In our small-scale model, a fully self-rebalancing
network (green line) achieves a 100% success rate after
approximately 600k units of total network liquidity. If we
disable the rebalancing tool internal to Tier 2 (submarine
swaps) and keep active the Tier 2-3 rebalancing (waterfall,
direct and inverse), we are in a scenario where payments can
always be started and received, but they can possibly fail to be
routed due to inner channels becoming permanently (almost)
unbalanced. In this case (blue line), to reach the same success
rate, we need to inject hundreds of times more liquidity
into the network (∼60M units). Finally, the removal of all
rebalancing mechanisms results—as expected—in a failure of
the system to even approach a 100% success rate (orange line).
B. Instant-ness of Payments
Figure 7 reports the cumulative distribution of completion
times for retail transactions, averaged over a full day of
simulated payments, i.e., the curve represents the amount of
payments that succeed within a given amount of time. Any
capacity beyond the 400k threshold identified in Sect. VII-A
produces essentially the same results. The bumpiness of the
curve hinges on our simulation being time discrete, with events
introducing delays as in Fig. 5. We observe how all payments
complete within at most 3 seconds, well under the 10 second
cutoff that identifies instant payments in our context.
C. Adaptive Rebalancing
All the experiments shown so far have been performed
under a constant load of transactions, equal to the average ex-
pected throughput for a payment system at our scale, according
to the payment model described in Sect. V-B. Actual payment
systems must be able to sustain peaks that may increase
the volume of payments by an order of magnitude over the
nominal average throughput (the “Christmas day” effect). To
show how our SH-PCN responds to one such occurrence, we
simulate a day where from 7am to 7pm the load on the system
suddenly increases by an order of magnitude.
Figure 8 shows the system response: All the rebalancing
mechanisms start working harder to maintain the entire pay-
ment network sufficiently well balanced, even under pressure.
As expected, the number of submarine swaps, for a given
routing liquidity grows substantially under pressure. And,
channels of larger capacity require fewer swaps, according to
an approximately linear inverse relation.
In addition, Fig. 8 shows that—as expected—the number
of (reverse) waterfall events is independent of the network
liquidity and of the number of submarine swaps (it only
depends on the load). Also, it is much higher than the number
submarine swaps, because the capacity/cap for retail wallets
is much smaller than the average channel size.
The payment success rate (not shown in the chart) stays
at 100% at all times during the peak window; our simu-
lated small-scale system is capable of maintaining satisfactory
throughput, latency, and performance, also under load.
D. The cost of keeping a channel well balanced
We have seen the inverse correlation between channel
capacity and swaps per unit of time, at the channel level.
At system level, the inverse correlation is between the total
routing liquidity and the transactional demand on Layer 1.
This technical trade-off engenders a related economical
trade-off: Submarine swaps have an economic cost in the
form of Layer-1 transaction fees, which are set by Tier 1
actors (i.e., CBs); at the same time, the cost for Tier 2 market
operators to keep central bank liquidity locked into channels
is again decided by Tier 1 actors, i.e., CBs. The resulting
economic dynamics are out of scope for the present work,
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
0
25
50
75
100
125
(Rev)Waterfall
per Minute
Waterfall
Reverse Waterfall
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
Hour of the Day
0
2
4
6
Swaps
per Minute
Network Liquidity (€)
600k
1.2M
6M
Fig. 8.
In the top chart we plot the number of (reverse) waterfall events
per minute during a 12h stress test (from 7am to 7pm) under peak load. The
(much smaller) number of submarine swaps per minute requested by the Layer
2 payment network to Layer 1 in order to keep itself balanced is reported in
the bottom chart, for three different levels of total routing liquidity.
60k
120k
300k
600k
1.2M
3M
6M
1.2M
Total Network Liquidity
0
200
400
600
800
Daily Cost of Channels Management
Total channel cost
Cost of swaps
Cost of liquidity
97.5%
100.0%
Payment Success Rate
Payment success rate
Fig. 9. Daily cost of locked liquidity and of submarine swaps for different
network capacities. The sum of these components (total cost of channel man-
agement) reaches a minimum at ∼ 800k. We assume the CB lends money11at
4.75%, and charges 0.1 monetary units for each Layer 1 transaction.
and we will not investigate in any detail the policy options
for the CB. We just show that, once the cost parameters have
been set by Tier 1, the market can optimize (statically, or even
dynamically under the current transaction load) the network
liquidity at Tier 2. In particular, the sum of (a) the cost of
liquidity—that increases monotonically with the capacity of
channels, plus (b) the cost of actuating submarine swaps—that
increases monotonically with the number of swaps executed,
is a convex function with a global minimum point. If the
minimum is to the left of the point where the system reaches
100% success rate (gray area in Fig. 9), it means swaps are so
cheap compared to borrowing liquidity that the actual lower
bound for the network capacity is given by the minimum
liquidity necessary to sustain a 100% success rate (approx.
600k in our example). Conversely, if the minimum is to the
right of the point where submarine swaps are no longer needed
(because the initial channel capacity suffices to support all
payments in the 24 hours), it means liquidity is so cheap
compared to executing swaps, that the optimal value for
network liquidity is the minimum value sufficient to process all
payments without swaps (∼ 30M in our example). The only
remaining, interesting case is represented in Fig. 9: Here, the
optimal amount of liquidity to invest by Tier 2 actors for the
purpose of routing retail payments is 800k, which falls in the
non-trivial range [600k; 30M]. This is where our small-scale
SH-PCN requires the smallest running costs.
VIII. RELATED WORK
The PCN topology affects the ability to successfully route
payments, network resilience (against both attacks and random
failures), and the privacy of payments. The unstructured topol-
ogy of LN led to a small-world and scale-free network, with
a few highly-connected nodes that route most of the payments
(e.g., [6], [7]). Avarikioti et al. [8] show that this topology
11This seemengly arbitrary number is the annual Marginal Lending Facility
(MLF) of the European Central Bank (ECB) as of Sept. 20, 2023.
is not optimal: when all transactions are known a-priori, a
star topology can minimize the locked capital and maximize
profits. Such a central node reduces the average path length
and thereby the routing failures due to unavailable balance, at
the price of weakening fault-tolerance, security, and privacy
(e.g., [27], [28]). Rohrer et al. [6] show how the LN can
be subjected to channel exhaustion or node isolation attacks.
Conversely, we resort to (semi) hierarchical topologies, which
promise to combine the benefits of both previous topologies.
A key problem is to determine the initial capacity of chan-
nels: The bigger the channel capacity, the higher the number
of routed transactions, the higher the liquidity costs (e.g.,
interest charges, opportunity costs). To route a transaction,
a channel moves liquidity, getting progressively imbalanced
over time (thus reducing the success probability of routing).
We believe that adopting rebalancing techniques is a key pillar
for highly loaded networks, which otherwise would need large
initial investments to open channels. Shabgahi et al. [29]
propose a model to predict the expected time for a channel
to get unbalanced, considering the channel centrality and its
initial balance. To extend the channels lifespan, they can be
either replenished with on-chain transactions (e.g., [30]) or
rebalanced in-place (e.g., [31]–[33]). Centralized approaches
to rebalance channels requires nodes to disclose their in-
dividual contribution in the channel, thus violating privacy
(e.g., [34]). Avarikioti et al. [32] enhance the protocol using
privacy preserving techniques. Li et al. [35] propose to divide
time in epochs and to use a randomly selected committee of
nodes to estimate the channels’ capacity needed to route the
expected payments in the next epoch. This approach incurs in
a large number of on-ledger transactions. Awathare et al. [31]
propose a decentralized protocol that leverages network cycles
to perform circular payments and rebalance capacity. Focusing
on relay nodes, Papadis et al. [36] use reinforcement learning
to proactively perform submarine swaps aiming to maximize
profit from fees. In this paper, we study the impact of the
initial channel capacity as well as the benefits of three different
approaches for automatic rebelancing of channels. Although
we rely on reactive solutions, we study their feasibility by
considering constraints imposed by the layer-1 blockchain.
Interestingly, Miller et al. [37] propose an alternative to
LN, enabling incremental deposits and withdrawal through
extended off-chain contracts (i.e., with no impact on-ledger).
Nonetheless, this approach is not compatible with LN. We
postpone as a future work the compatibility analysis of our
automated rebalancing techniques with this approach.
So far, a rather large number of LN simulators exist (e.g.,
[17]–[21]), even though a clear comparison among them is
lacking. CLoTH [21] is an open-source simulator of LND,
the most popular LN implementation, which has been used in
different recent research works (e.g., [22], [23]). Aiming to
analyze very large-scale networks, we integrate CLoTH func-
tionalities in ROSS, a general-purpose PDES (see Sect. VI-B).
IX. CONCLUSION AND FUTURE WORK
We introduced a class of self-rebalancing semi-hierarchical
PCNs, and showed experimentally—on a small-scale model
based on the payment patterns and structure of the euro area—
that they seem to (a) ensure good performances (in terms of
latency, throughput, and success rate of payments) and (b) suit
the existing 3-tier banking/payment architecture naturally.
As a future work, we will move towards removing one by
one the simplifying assumptions listed in Sect. V-C, up to the
point of presenting and studying a simulation at scale 1 : 1,
exhibiting a rich set of behaviors and dynamics.
Finally, we will study and characterize the strenght of the
privacy enjoyed by the retail payer and payee in this system,
and compare it to public PCNs such as the Lightning Network.
REFERENCES
[1] ECB Market Research, “Market research on possible technical solutions
for a digital euro,” https://www.ecb.europa.eu/paym/intro/news/html/ecb.
mipnews230113.en.html, 2023.
[2] ECB, “A stocktake on the digital euro,” https://www.ecb.europa.
eu/paym/digital euro/investigation/profuse/shared/files/dedocs/ecb.
dedocs231018.en.pdf, 2023.
[3] F. S. Mishkin, The economics of money, banking, and financial markets.
Pearson education, 2007.
[4] J. Poon and T. Dryja, “The bitcoin lightning network: Scalable off-chain
instant payments,” 2016.
[5] A. Gangwal, H. R. Gangavalli, and A. Thirupathi, “A survey of layer-
two blockchain protocols,” J. Netw. Comput. Appl., vol. 209, p. 103539,
2023.
[6] E. Rohrer, J. Malliaris, and F. Tschorsch, “Discharged payment chan-
nels: Quantifying the lightning network’s resilience to topology-based
attacks,” in Proc. of IEEE EuroS&PW’19, 2019, pp. 347–356.
[7] I. A. Seres, L. Guly´as, D. A. Nagy, and P. Burcsi, “Topological
analysis of bitcoin’s lightning network,” in Mathematical Research for
Blockchain Economy.
Springer, 2020, pp. 1–12.
[8] G. Avarikioti, Y. Wang, and R. Wattenhofer, “Algorithmic channel
design,” in Proc. of ISAAC ’18, vol. 123.
Schloss Dagstuhl-Leibniz-
Zentrum f¨ur Informatik, 2018, pp. 16–1.
[9] ECB Surveys, “Study on the payment attitudes of consumers in the euro
area (SPACE),” https://www.ecb.europa.eu/stats/ecb surveys/space/html/
ecb.spacereport202212∼783ffdf46e.en.html, 2022.
[10] “European central bank. what are instant payments?” https://www.ecb.
europa.eu/paym/integration/retail/instant payments/html/index.en.html,
accessed: 2023-11-10.
[11] “Banking
in
europe:
Ebf
facts
&
figures
2022,”
https://www.ebf.eu/wp-content/uploads/2022/12/
Banking-in-Europe-EBF-Facts-and-Figures-2022.-FINAL.pdf,
accessed: 2023-11-10.
[12] U. Bindseil, “Tiered CBDC and the financial system,” Available at SSRN
3513422, 2020.
[13] U. Bindseil and F. Panetta, “Central bank digital currency remuneration
in a world with low or negative nominal interest rates,” VoxEU,[Internet],
dostupno na: https://voxeu. org/article/cbdc-remuneration-world-low-
or-negative-nominal-interestrates,[15.4. 2022.], 2020.
[14] B. Meller and O. Soons, “Know your (holding) limits: CBDC, financial
stability and central bank reliance,” https://www.dnb.nl/media/iwsban0s/
working paper no-771.pdf, 2023.
[15] M. Benedetti, F. D. Sclavis, M. Favorito, G. Galano, S. Giammusso,
A. Muci, and M. Nardelli, “Certified byzantine consensus with con-
fidential quorum for a bitcoin-derived permissioned dlt,” in Proc. of
the 5th Distributed Ledger Technology Workshop, 2023, available at
https://ceur-ws.org/Vol-3460/papers/DLT 2023 paper 1.pdf.
[16] D. J. Watts and S. H. Strogatz, “Collective dynamics of ‘small-
world’networks,” nature, vol. 393, no. 6684, pp. 440–442, 1998.
[17] F. Beres, I. A. Seres, and A. A. Benczur, “A cryptoeconomic traffic
analysis of bitcoin’s lightning network,” 2019, arXiv:1911.09432.
[18] G. Di Stasi, S. Avallone, R. Canonico, and G. Ventre, “Routing payments
on the lightning network,” in Proc. of IEEE iThings and GreenCom and
CPSCom and SmartData ’18, 2018, pp. 1161–1170.
[19] G. A. F. Rebello, G. F. Camilo, M. Potop-Butucaru, M. E. M. Campista
et al., “PCNsim: A flexible and modular simulator for payment channel
networks,” in Proc. IEEE INFOCOM Workshops ’22, 2022, pp. 1–2.
[20] R. Yu, G. Xue, V. T. Kilari, D. Yang, and J. Tang, “CoinExpress: A
fast payment routing mechanism in blockchain-based payment channel
networks,” in Proc. of ICCCN ’18, 2018, pp. 1–9.
[21] M. Conoscenti, A. Vetr`o, and J. C. De Martin, “CLoTH: A Lightning
Network Simulator,” SoftwareX, vol. 15, p. 100717, 2021.
[22] K. Asgari, A. A. Mohammadian, and M. Tefagh, “Dyfen: Agent-based
fee setting in payment channel networks,” 2022, arXiv:2210.08197.
[23] V. Davis and B. Harrison, “Learning a scalable algorithm for improving
betweenness in the lightning network,” in Proc. of BCCA ’22.
IEEE,
2022, pp. 119–126.
[24] “Ross,” https://ross-org.github.io, 2022.
[25] A. Gill, M. Lalith, S. Poledna, M. Hori et al., “High-performance com-
puting implementations of agent-based economic models for realizing
1:1 scale simulations of large economies,” IEEE Trans. Parallel Distrib
Syst., vol. 32, no. 8, pp. 2101–2114, 2021.
[26] G. Karypis and V. Kumar, “A fast and high quality multilevel scheme
for partitioning irregular graphs,” SIAM J Sci Comput., vol. 20, no. 1,
pp. 359–392, 1998.
[27] G. Kappos, H. Yousaf, A. Piotrowska, S. Kanjalkar et al., “An empirical
analysis of privacy in the lightning network,” in Financial Cryptography
and Data Security.
Springer, 2021, pp. 167–186.
[28] C. Sguanci and A. Sidiropoulos, “Mass exit attacks on the lightning
network,” 2022, arXiv:2208.01908.
[29] S. Z. Shabgahi, S. M. Hosseini, S. P. Shariatpanahi, and B. Bahrak,
“Modeling
Effective
Lifespan
of
Payment
Channels,”
2022,
arXiv:2301.01240.
[30] V. Sivaraman, S. B. Venkatakrishnan, M. Alizadeh, G. Fanti, and
P. Viswanath, “Routing cryptocurrency with the spider network,” in Proc.
of HotNets ’18.
ACM, 2018, p. 29–35.
[31] N. Awathare, Suraj, Akash, V. J. Ribeiro, and U. Bellur, “Rebal: Channel
balancing for payment channel networks,” in Proc. of MASCOTS ’21,
2021, pp. 1–8.
[32] Z. Avarikioti, K. Pietrzak, I. Salem, S. Schmid et al., “Hide & seek:
Privacy-preserving rebalancing on payment channel networks,” in Fi-
nancial Cryptography and Data Security. Springer, 2022, pp. 358–373.
[33] Z. Hong, S. Guo, R. Zhang, P. Li et al., “Cycle: Sustainable off-chain
payment channel network with asynchronous rebalancing,” in Proc. of
IEEE/IFIP DSN ’22, 2022, pp. 41–53.
[34] R. Khalil and A. Gervais, “Revive: Rebalancing off-blockchain payment
networks,” in Proc. of ACM SIGSAC CCS ’17. ACM, 2017, p. 439–453.
[35] P. Li, T. Miyazaki, and W. Zhou, “Secure balance planning of off-
blockchain payment channel networks,” in Proc of IEEE INFOCOM
’20, 2020, pp. 1728–1737.
[36] N. Papadis and L. Tassiulas, “Deep reinforcement learning-based re-
balancing policies for profit maximization of relay nodes in payment
channel networks,” 2023, arXiv:2210.07302.
[37] A. Miller, I. Bentov, S. Bakshi, R. Kumaresan, and P. McCorry, “Sprites
and state channels: Payment networks that go faster than lightning,” in
Financial Cryptography and Data Security.
Springer, 2019, pp. 508–
526.
"
"The modular monolith architecture has attracted the attention of practitioners, as Google proposed ""Service Weaver"" framework to enable developers to write applications as modular monolithic and deploy them as a set of microservices. Google considered it as a framework that has the best of both worlds and it seems to be a trend in software architecture. This paper aims to understand the definition of the modular monolith in industry and investigate frameworks and cases building modular monolith architecture.
We conducted a systematic grey literature review, and the results show that modular monolith combines the advantages of monoliths with microservices. We found three frameworks and four cases of building modular monolith architecture. In general, the modular monolith is an alternative way to microservices, and it also could be a previous step before systems migrate to microservices.","Microservices have been getting more and more popular in the last few years, and a large number of companies are migrating monolithic applications to microservices. Especially in the industry, companies like Amazon and Netflix are looking to take advantage of microservices such as independent development and deployment to help their systems solve problems. These problems are mostly due to the drawbacks of legacy and monolithic systems that the direct tight coupling of their internal components makes them difficult to maintain and increases the development time and effort required.  However, several companies did not get the expected benefits from migrating to microservices, and fell into difficulty because of issues such as high cost and complexity of microservices. Since AWS Re-Invent 2018 started to mention that the migration to microservices blindly was a mistake, more and more practitioners have started discussing the topics that migrating from monolith to microservices. Even there are examples of switching from microservices back to the monolith like Amazon PrimeVideo. In general, monoliths and microservices are not ""perfect"" and they have their own different drawbacks. Recently, the concept of ""Modular Monolith"" has attracted the attention of practitioners, as Google proposed the ""Service Weaver"" framework to enable developers to write applications as modular monolithic and deploy them as a set of microservices. Google explains that it is a framework that has the best of both worlds: the development velocity of a monolith, with the scalability, security, and fault-tolerance of microservices. Traditional monoliths focus on layers and often include three layers: UI, Business and Data. All features are vertically separated into these layers. The business layer is the one that contains the business logic of all features. Each feature knows the business logic of other features, which are tightly coupled.","nanThe concept of modularization is not new knowledge, it has been proposed in the last century that modularization is a mechanism for improving the flexibility and comprehensibility of systems. It differs from a monolithic system in that modularization divides the system into separate modules, and independent teams can work on each module so that it reduces product development time and has greater flexibility and comprehensibility. The proposal of modular monolith is exciting, it has similarities with the traditional monolith and modularization mechanism proposed in the past but is different. It seems to be the ""middle ground"", the combination of monolith and microservices that addresses the issues of monoliths and microservices. The recently proposed Service Weaver framework has triggered even more discussion among practitioners about modular monolithic architectures.nannan","In this paper, we aim to understand the definition of the modular monolith in industry and investigate frameworks and cases in building modular monolith architecture. To such an end, we conducted a systematic grey literature review (SGLR) based on the guidelines defined by [4][5]. Herein, we used the following query in the grey literature search based on the identified keywords: (""serviceweaver"" OR “service weaver” OR“modular monolith” OR  “modular monolithic”) AND (“microservice” OR “microservices”) By searching on Google, we obtained 140 results in total. Each result was read and evaluated by two authors and the inclusion criteria is that the studies mention definitions, frameworks or cases on modular monolith. In the case of disagreement, both authors would discuss together and decide whether to include or exclude these results. We also adopted the ""snowballing"" method in the process to extract more relevant information [12].nan","The following section presents the results of the systematic grey literature review. According to the grey literature review, 64 studies are finally included that mention the definition, frameworks and cases on modular monolith. The included 64 studies(Ss) can be found in APPENDIX A. When dividing these selected 64 studies by the publication year, we can observe the trends of the discussion on modular monolith. As shown in Fig.1, the discussion about the topic of modular monolith started in 2019, and the number of selected studies is stable until 2022. However, its number increases dramatically in 2023. This trend indicates that modular monoliths have become a hot topic and have sparked intense discussion among practitioners.","The modular monolith architecture has attracted the attention of practitioners for the ""Service Weaver"" framework proposed by Google. It seems to be a framework that has the best of both worlds: the development velocity of a monolith, with the scalability, security, and fault-tolerance of microservices[3]. We conducted a systematic grey literature review, which aims to understand the definition of the modular monolith in industry and investigate frameworks and cases in building modular monolith architecture. Starting from 140 results, 64 related studies were selected. The results show that modular monolith is a software architecture pattern that combines the advantages of monolith with microservices architecture and in particular, it can be moved or deployed as microservices later if want. In this architecture, systems are organized into loosely coupled modules, each delineating well-defined boundaries and explicit dependencies on other modules. We found three frameworks: Service Weaver, Spring Modulith and Light-hybrid-4j, and four cases: Shopify, Appsmith, Gusto(TimeTracking) and PlayTech(Casino Backend Team) in building the modular monolith architecture. We conclude that modular monolith is an alternative way to microservices, and it also could be a previous step before systems migrate to microservices. The study helps researchers and practitioners to deeply explore modular monoliths, and provides guidance in software architecture decisions, offering useful directions for future work. In our future work, we shall continue our research on the relationship and migration between modular monoliths and microservices to help organizations enable more effective implementation of architectural changes.",Modular Monolith: Is This the Trend in Software Architecture?,"Ruoyu Su, Xiaozhou Li","Modular Monolith: Is This the Trend in Software Architecture?
Ruoyu Su
M3S, University of Oulu
Oulu, Finland
ruoyu.su@.oulu.fi
Xiaozhou Li
M3S, University of Oulu
Oulu, Finland
xiaozhou.li@oulu.fi
ABSTRACT
Recently modular monolith architecture has attracted the attention
of practitioners, as Google proposed ""Service Weaver"" framework
to enable developers to write applications as modular monolithic
and deploy them as a set of microservices. Google considered it
as a framework that has the best of both worlds and it seems to
be a trend in software architecture. This paper aims to understand
the definition of the modular monolith in industry and investi-
gate frameworks and cases building modular monolith architecture.
We conducted a systematic grey literature review, and the results
show that modular monolith combines the advantages of monoliths
with microservices. We found three frameworks and four cases of
building modular monolith architecture. In general, the modular
monolith is an alternative way to microservices, and it also could
be a previous step before systems migrate to microservices.
KEYWORDS
software engineering, software architecture, modular monolith,
microservices, systematic grey literature review
ACM Reference Format:
Ruoyu Su and Xiaozhou Li. 2024. Modular Monolith: Is This the Trend in
Software Architecture?. In Proceedings of XXX Conference (XXX 2024). ACM,
New York, NY, USA, Article 4, 6 pages. https://doi.org/xx.xxx/xxx_x
1
INTRODUCTION
Microservices have been getting more and more popular in the last
few years, and a large number of companies are migrating mono-
lithic applications to microservices [10]. Especially in the industry,
companies like Amazon and Netflix are looking to take advantage
of microservices such as independent development and deployment
to help their systems solve problems [7]. These problems are mostly
due to the drawbacks of legacy and monolithic systems that the
direct tight coupling of their internal components makes them dif-
ficult to maintain and increases the development time and effort
required [11].
However, several companies did not get the expected benefits
from migrating to microservices, and fell into difficulty because
of issues such as high cost and complexity of microservices [9].
Since AWS Re-Invent 2018 started to mention that the migration
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
XXX 2024, Month 2024, City, Country
© 2024 Association for Computing Machinery.
ACM ISBN xxx-x-xxxx-xxxx-x/YY/MM...$15.00
https://doi.org/xx.xxx/xxx_x
to microservices blindly was a mistake [1], more and more prac-
titioners have started discussing the topics that migrating from
monolith to microservices. Even there are examples of switching
from microservices back to the monolith like Amazon PrimeVideo
[6].In general, monoliths and microservices are not ""perfect"" and
they have their own different drawbacks [9].
Recently, the concept of ""Modular Monolith"" has attracted the
attention of practitioners, as Google proposed the ""Service Weaver""
framework to enable developers to write applications as modular
monolithic and deploy them as a set of microservices [3]. Google
explains that it is a framework that has the best of both worlds: the
development velocity of a monolith, with the scalability, security,
and fault-tolerance of microservices [3].
Traditional monoliths focus on layers and often include three
layers: UI, Business and Data [2]. All features are vertically sepa-
rated into these layers. The business layer is the one that contains
the business logic of all features. Each feature knows the business
logic of other features, which are tightly coupled [2].
The concept of modularization is not new knowledge, it has been
proposed in the last century that modularization is a mechanism
for improving the flexibility and comprehensibility of systems [8].
It differs from a monolithic system in that modularization divides
the system into separate modules, and independent teams can work
on each module so that it reduces product development time and
has greater flexibility and comprehensibility [8].
The proposal of modular monolith is exciting, it has similarities
with the traditional monolith and modularization mechanism pro-
posed in the past but is different. It seems to be the ""middle ground"",
the combination of monolith and microservices that addresses the
issues of monoliths and microservices. The recently proposed Ser-
vice Weaver framework has triggered even more discussion among
practitioners about modular monolithic architectures.
This paper aims to understand the definition of the modular
monolith in industry and investigate frameworks and cases in build-
ing modular monolith architecture. Based on our goals, we define
the following research questions: RQ1 What is Modular Monolith?
RQ2 Are there any frameworks proposed in building modular mono-
lith architecture? RQ3 Are there any cases that use modular monolith
architecture? We conducted a systematic grey literature review and
finally 64 results were included.
The results show that Modular Monolith is a software archi-
tecture pattern that combines the advantages of monolith with
microservices architecture. In this architecture, systems are orga-
nized into loosely coupled modules, each delineating well-defined
boundaries and explicit dependencies on other modules. Especially,
it differs from the previously mentioned modularity in that it can be
moved or deployed as microservices later if want. We found three
frameworks for building the modular monolith: Service Weaver,
Spring Modulith, and Light-hybrid-4j. There are also four cases
arXiv:2401.11867v1  [cs.SE]  22 Jan 2024
XXX 2024, Month 2024, City, Country
R. Su et al.
that use modular monolith rather than microservices: Shopify, App-
smith, Gusto(Time Tracking), and PlayTech(Casino Backend Team).
However, they do not use the frameworks proposed above. Through
these results, we conclude that modular monolith is an alternative
way to microservices, and it also could be a previous step before
systems migrate to microservices.
The remainder of this paper is organized as follows: Section 2
reports the research method employed to conduct a systematic grey
literature review. In Section 3, we analyze the results that address
the goal of the study. Finally, Section 4 concludes the paper and
outlines our future work.
2
METHODOLOGY
In this paper, we aim to understand the definition of the modular
monolith in industry and investigate frameworks and cases in build-
ing modular monolith architecture. To such an end, we conducted
a systematic grey literature review (SGLR) based on the guidelines
defined by [4][5]. Herein, we used the following query in the grey
literature search based on the identified keywords:
(serviceweaver OR “service weaver” OR “modular monolith” OR
“modular monolithic”)
AND
(“microservice” OR “microservices”)
By searching on Google, we obtained 140 results in total. Each
result was read and evaluated by two authors and the inclusion
criteria is that the studies mention definitions, frameworks or cases
on modular monolith. In the case of disagreement, both authors
would discuss together and decide whether to include or exclude
these results. We also adopted the ""snowballing"" method in the
process to extract more relevant information [12].
3
RESULTS
The following section presents the results of the systematic grey
literature review. According to the grey literature review, 64 studies
are finally included that mention the definition, frameworks and
cases on modular monolith. The included 64 studies(Ss) can be
found in APPENDIX A.
When dividing these selected 64 studies by the publication year,
we can observe the trends of the discussion on modular monolith. As
shown in Fig.1, the discussion about the topic of modular monolith
started in 2019, and the number of selected studies is stable until
2022. However, its number increases dramatically in 2023. This
trend indicates that modular monoliths have become a hot topic
and have sparked intense discussion among practitioners.
3.1
RQ1 What is Modular Monolith?
Of the included 64 results, 47 studies answered the RQ1. We ex-
tracted data with four different parts of the modular monolith:
Concept, Characteristics and features, Code structure and Testing.
• Concept
Modular Monolith is a software architecture pattern that strate-
gically combines the simplicity of a monolithic structure with the
advantages of microservices. In this approach, the system is orga-
nized into loosely coupled modules, each delineating well-defined
boundaries and explicit dependencies on other modules. The goal
Figure 1: Selected Studies By Year
is to achieve independence and isolation for each module, allowing
them to be worked on independently while still being deployed
collectively as a single unit. This architectural style seeks to strike
a middle ground between traditional monolithic and microservices
architectures. It emphasizes the interchangeability and potential
reusability of modules, promoting a clear programming interface
between them. The focus on business domains rather than technical
layers, along with the vertical stacking of modules, enhances code
organization and maintainability. Ultimately, a Modular Monolith
represents a holistic and flexible approach to application design,
where each module encapsulates specific functionality, fostering
ease of development, testing, and deployment. Crucially, it can
continue to be migrated to microservices or remain unchanged,
which is more convenient than migrating directly from monolith
to microservice [S1-S44].
• Characteristics and features
We found 6 studies that mention the characteristics, and we sum-
marized 6 characteristics of the modular monolith: (1) Segregation
of modules. Each module is independent with its own layers such
as Domain, Infrastructure and API. Modules are autonomously de-
veloped, tested, and deployed, affording the flexibility to employ
diverse database solutions [S18][S24]. (2) Modularity with loose
coupling and high cohesion. Modules exhibit loose interdependence
and strong internal cohesion. Communication between modules
occurs through APIs, preferably adopting loosely coupled asynchro-
nous communication patterns [S1][S18][S24][S46]. (3) Unified Data-
base Schema. The system adheres to a singular database schema, in
contrast to microservices, where each microservice necessitates an
individual schema [S1]. (4) Monolithic Deployment Structure. All
modules within the modular monolith operate within the same Vir-
tual Machine (VM), or each module may run on dedicated VMs. The
scale of modules renders them impractical to be encapsulated within
containers [S1]. (5) Unified Application Process. The application
functions as a singular process, offering a uniform solution applica-
ble to diverse scales of applications. Notably, there exists no rigid
data ownership delineation among modules [S22][S24][S26]. (6) En-
hanced Maintainability and Scalability. Comparative to traditional
monolithic architectures, the modular monolith model demonstra-
bly enhances both maintainability and scalability, underscoring its
efficacy in managing complexities and facilitating growth [S24].
Modular Monolith: Is This the Trend in Software Architecture?
XXX 2024, Month 2024, City, Country
We also found 9 features of modular monolith in the selected
studies [S4][S5][S11][S17][S62]: (1) A module is never completely
independent. It has dependencies with other modules, but these
should be minimal; (2) Modules are interchangeable. (3) Code is
reusable. (4) Better organization of the dependencies compared to
traditional monolithic apps. (5) Easier to maintain and develop new
versions than traditional monolithic apps. (6) You can keep the
whole project as a single unit, without needing different servers
for deployment. (7) More scalable than traditional monolithic apps.
(8) Less complex than microservices architecture. (9) Have defined
API, allowing access to the logic of each module through public
methods, not internal functions and the logic of each one.
• Code structure
In modular monolith architecture, the code should contain mul-
tiple functional modules, each module having an interface that rep-
resents its public definition [S10]. In the selected studies [S10][S12]
[S13], authors proposed the general modular monolith architecture
structure that exposes module interfaces in two ways: Externally,
the module offers an API via REST HTTP or GRPC, with API calls
managed by a proxy or gateway. Internally, services access the mod-
ule through an abstracted interface, enabling information retrieval
without direct access to the implementation. This upholds a clear
separation of concerns, preserving application processes.
• Testing
Different from traditional monolithic systems but similar to the
microservices systems, the modular monolith should have its own
set of unit testing for each module to ensure that its functionality
works as expected in isolation because it divides the system into
modules [S14]. The interaction between each module should also be
verified by performing integration testing. In addition, developers
also need to review and refactor the modular monolith on a regular
basis to keep the code base neat maintainable and adaptable to
changing business requirements [S14].
3.2
RQ2 Frameworks proposed in building
modular monolith architecture
Of the included 64 results, 17 studies answered the RQ2, and there
are three frameworks proposed for building modular monolith
architecture: Service Weaver, Spring Modulith, and Light-hybrid-4j.
• Service Weaver
Service Weaver is an open-source framework, written by Google,
for building and deploying distributed systems. It provides the idea
of decoupling the code from how code is deployed [S45][S51][S52]
[S53][S54][S55]. The framework is currently only available in Go.
It allows people to write the application as a modular monolith and
deploy it as a set of microservices [S58][S59][S60][S62][S63][S64].
Service Weaver consists of two core pieces: A set of program-
ming libraries that allow developers to write their application as
a single modular binary using only native data structures and
method calls; A set of deployers that allow developers to config-
ure the runtime topology of their application and deploy it as a
set of microservices, either locally or on the cloud of their choice
[S51][S54][S59][S60][S64].
The motivation for building Service Weaver is Google found that
the expense of maintaining multiple different microservice binaries
with separate configuration files, network endpoints, and serial-
izable data formats significantly slowed down the development
of microservice-based applications [S51][S59][S60]. Furthermore,
microservices heightened the difficulty of cross-binary changes
and rendered API modifications very challenging [S54][S59][S60].
Therefore, developers hoped the monolithic binaries would be easy
to write, update, and run locally or in a VM [S54][S63][S64].
The goal of Service Weaver is to improve distributed applica-
tion development velocity and performance [S59][S60][S63][S64].
Its core idea is a modular monolith model that creates a unified
binary using native language data structures and methods, orga-
nized into modules (components) implemented as native types
[S51][S63][S64].
Service Weaver has the following steps: First is to split the appli-
cation into components written as regular Go interfaces. Secondly,
call the components using regular Go method calls and no need for
RPCs or HTTP requests. Then, test the application and deploy it to
the cloud. Finally, run the components wherever in the same or dif-
ferent process and scale up or down to match load [S52][S54][S64].
Service Weaver has more powerful features compared to tradi-
tional modular monoliths. Service Weaver has highly performant
in that co-located components communicate via direct method
call while remote components communicate using highly efficient
custom serialization and RPC protocols [S52][S62]. It has minimal
configuration and deploys to the cloud without extensive boiler-
plate configuration [S52]. Service Weaver provides libraries logging,
metrics, and tracing, automatically integrated into the deployed
cloud environment [S52][S53]. In addition, it can do the effective
sharding that shard requests across various component replicas
for optimized performance [S52][S62]. More importantly, Service
Weaver has flexible scalability, it can easily scale applications hori-
zontally or vertically on demand, whether as a monolithic applica-
tion or a distributed microservice [S52].
However, the Service Weaver framework is still in development
and now only has the v0.1 release which includes: The core Go
libraries used for writing your applications; A number of deployers
used for running your applications locally or on GKE; A set of APIs
that allow you to write your own deployers for any other platform
[S51][S54][S59][S60][S64].
• Spring Modulith
Spring Modulith is an experimental Spring project designed for
modular monolith applications, with its source code organized
based on the module concept [S38][S43][S44][S48]. It provides
conventions and APIs for declaring and validating logical modules
within the Spring Boot application [S44].
Spring Modulith ensures a modular structure for Spring Beans to
grant control over what to expose. The core concept revolves around
application modules, representing units of functionality with ex-
posed APIs. Expressing modules can be done through various meth-
ods, including organizing domain or business modules as direct
sub-packages. Module encapsulation is a key feature, safeguarding
internal implementations through sub-packages of the application
module’s base package. Additionally, it restricts type visibility be-
tween modules, allowing access to module content [S38][S43][S44].
The first release of Spring Modulith introduces advanced fea-
tures, including enhanced package arrangements, flexible module
XXX 2024, Month 2024, City, Country
R. Su et al.
selection for integration tests, a transaction event publication log
for seamless integration, automatic developer documentation gen-
eration with diagrams, runtime observability at the module level,
and Passage of Time Events implementation [S44].
• Light-hybrid-4j
The light-hybrid-4j framework is designed to be a modular mono-
lith framework, built on top of the light-4j for modularized mono-
lithic and serverless architecture from the Light platform [S37]. The
benefit of using this framework when building a modular monolith
system is gives flexibility for deployment and saves production
costs [S37].
The framework involves creating a server with integrated third-
party dependencies and building multiple services with shared or
specific dependencies. Once services are developed, they are com-
piled into independent jar files containing only business handlers.
These jars are placed in a designated folder on the host running the
server, and upon server startup, all services in the folder are loaded.
Traffic is then routed to the appropriate handler in the respective
service for incoming requests. Developers adhere to the principle of
building services into jar files, deploying them to the same Docker
container volume, and loading them into the same JVM. Services
communicate through interfaces, concealing implementation de-
tails. If needed, services experiencing heavy loads can be separated
into individual containers for scalable deployment [S37].
3.3
RQ3 Cases that use modular monolith
architecture
Of the selected 64 studies, 6 studies answered the RQ3, and there
are four cases use modular monolith architecture rather than mi-
croservices(specifically emphasized this): Shopify, Appsmith, Gusto
(Time Tracking) and PlayTech (Casino Backend Team).
Shopify is one of the largest Ruby on Rails codebases in ex-
istence, worked on for over a decade by more than a thousand
developers [S12]. It was initially built as a monolith all the dis-
tinct functionalities were built into the same codebase with no
boundaries between them. To better development, Shopify chose to
change its architecture. Shopify originally considered microservices
but finally chose modular monolith to balance the benefits of a sin-
gle codebase with clear component boundaries. Therefore, Shopify
is a great example that has used this approach as an alternative to
microservice decomposition [S12][S30].
Appsmith is a platform that helps developers build internal apps
[S34].They originally considered a microservice-based architecture,
However, they rejected this alternative due to many considerations
for on-premise deployment of a microservice-based application.
Given their core on-premise deployment as a focal point of their
business, they deemed it imperative to avoid introducing complexity
into the deployment process for end users [S33][S34]. Therefore,
they chose modular monolith because it allowed them to maintain
one large codebase, deploy a single binary, and balance the interests
of their end customers with those of our internal team and open-
source community [S33][S34].
Gusto wanted to build a new feature named ""TimeTracking""
and it would be a separate domain that has its own service [S39].
To achieve this feature, they chose modular monolith, because
for Gusto, their most important is to have a clear API boundary
and make sure the system should not pass rich objects across that
boundary [S39]. They did not choose microservices for it has their
own sets of challenges in testing and deploying and if got the service
boundary wrong, it’d be much more expensive to fix [S39].
PlayTech (Casino Backend Team) discovered their company
has many departments running microservices and cloud-native
applications in the process of continuous refactoring. However,
from the perspective of a single team, this maintenance would
be unnecessary overhead. Therefore, their team chose a modular
monolith as a result [S32].
In general, Shopify, Appsmith, Gusto (Time Tracking) and PlayTech
(Casino Backend Team) are great examples that have used modular
monolith as an alternative to microservice decomposition. However,
they do not use the frameworks proposed in RQ2 above.
4
CONCLUSION
The modular monolith architecture has attracted the attention of
practitioners for the ""Service Weaver"" framework proposed by
Google. It seems to be a framework that has the best of both worlds:
the development velocity of a monolith, with the scalability, se-
curity, and fault-tolerance of microservices[3]. We conducted a
systematic grey literature review, which aims to understand the
definition of the modular monolith in industry and investigate
frameworks and cases in building modular monolith architecture.
Starting from 140 results, 64 related studies were selected. The
results show that modular monolith is a software architecture pat-
tern that combines the advantages of monolith with microservices
architecture and in particular, it can be moved or deployed as mi-
croservices later if want. In this architecture, systems are organized
into loosely coupled modules, each delineating well-defined bound-
aries and explicit dependencies on other modules. We found three
frameworks: Service Weaver, Spring Modulith and Light-hybrid-
4j, and four cases: Shopify, Appsmith, Gusto(TimeTracking) and
PlayTech(Casino Backend Team) in building the modular monolith
architecture. We conclude that modular monolith is an alternative
way to microservices, and it also could be a previous step before
systems migrate to microservices. The study helps researchers and
practitioners to deeply explore modular monoliths, and provides
guidance in software architecture decisions, offering useful direc-
tions for future work. In our future work, we shall continue our
research on the relationship and migration between modular mono-
liths and microservices to help organizations enable more effective
implementation of architectural changes.
REFERENCES
[1] 2018.
AWS
News
Blog
Category:
AWS
re:Invent.
https://aws.amazon.com/cn/blogs/aws/category/events/reinvent/.
[2] 2022. https://www.ahmetkucukoglu.com/en/what-is-modular-monolith.
[3] Kucukoglu Ahmet. 2023. What is Modular Monolith? https://serviceweaver.dev/.
[4] Vahid Garousi, Michael Felderer, and Mika V Mäntylä. 2019. Guidelines for
including grey literature and conducting multivocal literature reviews in software
engineering. Information and software technology 106 (2019), 101–121.
[5] Staffs Keele et al. 2007. Guidelines for performing systematic literature reviews
in software engineering.
[6] Marcin Kolny. 2023.
Scaling up the Prime Video audio/video monitoring
service and reducing costs by 90%. https://www.primevideotech.com/video-
streaming/scaling-up-the-prime-video-audio-video-monitoring-service-and-
reducing-costs-by-90.
[7] Valentina Lenarduzzi, Francesco Lomio, Nyyti Saarimäki, and Davide Taibi. 2020.
Does migrating a monolithic system to microservices decrease the technical
debt? Journal of Systems and Software 169 (2020), 110710.
Modular Monolith: Is This the Trend in Software Architecture?
XXX 2024, Month 2024, City, Country
[8] David Lorge Parnas. 1972. On the criteria to be used in decomposing systems
into modules. Commun. ACM 15, 12 (1972), 1053–1058.
[9] Ruoyu Su, Xiaozhou Li, and Davide Taibi. 2023. Back to the Future: From Mi-
croservice to Monolith. arXiv preprint arXiv:2308.15281 (2023).
[10] Davide Taibi, Valentina Lenarduzzi, and Claus Pahl. 2017. Processes, motivations,
and issues for migrating to microservices architectures: An empirical investiga-
tion. IEEE Cloud Computing 4, 5 (2017), 22–32.
[11] Davide Taibi and Kari Systä. 2019. From monolithic systems to microservices: A
decomposition framework based on process mining. (2019).
[12] Claes Wohlin. 2014. Guidelines for snowballing in systematic literature studies
and a replication in software engineering. In Proceedings of the 18th international
conference on evaluation and assessment in software engineering. 1–10.
APPENDIX A: THE SELECTED STUDIES
[S1] Itiel Maayan. Will Modular Monolith Replace Microservices Archi-
tecture? 2022. https://medium.com/att-israel/will-modular-monolith-
replace-microservices-architecture-a8356674e2ea
[S2] Mehmet Ozkaya. Microservices Killer: Modular Monolithic Architec-
ture. 2023. https://medium.com/design-microservices-architecture-
with-patterns/microservices-killer-modular-monolithic-architecture-
ac83814f6862
[S3] Rahul Garg. When (modular) monolith is the better way to build soft-
ware. 2023. https://www.thoughtworks.com/insights/blog/micro-
services/modular-monolith-better-way-build-software
[S4] Sergio Agamez. Modular Monolithic vs. Microservices. https://www.-
fullstacklabs.co/blog/modular-monolithic-vs-microservices
[S5] Roshan Manjushree Adikari. Is modular monolith enough for your
organization instead of microservices? 2023. https://levelup.gitconne-
cted.com/is-modular-monolith-enough-for-your-organization-inste-
ad-of-microservices-97917fa07f27
[S6] Alex Bolboaca. Modular Monolith Or Microservices? 2019. https://m
ozaicworks.com/blog/modular-monolith-microservices
[S7] Adrian Kodja. Modular Monoliths vs. Microservices. 2023. https://ad
riankodja.com/modular-monoliths-vs-microservices
[S8] Adrain Ababei. Modular Monolith vs Microservices: How Do You
Make a Choice? 2023. https://www.cmsdrupal.com/blog/modular-
monolith-vs-microservices-how-do-you-make-choice
[S9] Priyank Gupta. Understanding the modular monolith and its ideal
use cases. 2020. https://www.techtarget.com/searchapparchitecture/
tip/Understanding-the-modular-monolith-and-its-ideal-use-cases
[S10] Miłosz Lenczewski. What is better? Modular Monolith vs. Microser-
vices. 2022. https://careers.tidio.com/blog-post/what-is-better-modul
ar-monolith-vs-microservices
[S11] Kirsten Westeinde. Deconstructing the Monolith: Designing Soft-
ware that Maximizes Developer Productivity. 2019. https://shopify.en
gineering/deconstructing-monolith-designing-software-maximizes-
developer-productivity
[S12] Chris Richardson. How modular can your monolith go? Part 1 - the
basics. 2023. https://microservices.io/post/architecture/2023/07/31/
how-modular-can-your-monolith-go-part-1.html
[S13] Dennis John. Modular Monolith - primer. 2023. https://www.linkedin.
com/pulse/modular-monolith-primer-dennis-john/
[S14] JRebel. What Is a Modular Monolith? 2020. https://www.jrebel.com/
blog/what-is-a-modular-monolith
[S15] Drawing Boxes. Microservices vs Monolithic Architecture. 2023.
https://www.youtube.com/watch?v=6-Wu178sOEE
[S16] Matthew Freshwaters. Why Companies are Moving their Applica-
tions to Modular Architecture. 2023. https://blog.ippon.tech/why-
companies-are-moving-their-applications-to-modular-architecture/
[S17] Ahmet Kucukoglu. What is Modular Monolith? 2022. https://www.
ahmetkucukoglu.com/en/what-is-modular-monolith
[S18] Tomas Fernandez. 12 Ways to Improve Your Monolith Before Transi-
tioning to Microservices. 2023. https://semaphoreci.com/blog/mono-
lith-microservices
[S19] KAMIL GRZYBEK. MODULAR MONOLITH: A PRIMER. 2019. https:
//www.kamilgrzybek.com/blog/posts/modular-monolith-primer
[S20] Debug Agent. Its Time to go Back to the Monoliths. Use Modu-
lar Monolith, save costs. 2023. https://www.youtube.com/watch?v=
NWu7AJJlLM8
[S21] Fernando Doglio. Domain-Driven Design for a Modular Mono-
lith: Bridging the Gap Between Microservices and Monoliths. 2023.
https://javascript.plainenglish.io/domain-driven-design-for-a-mod-
ular-monolith-bridging-the-gap-between-microservices-and-monol-
iths-2d2521196dd8
[S22] Jinal Desai. Monolith vs Modular Monolith vs SOA vs Microservices
vs MUI vs Nano Services. 2023. https://jinaldesai.com/monolith-vs-
modular-monolith-vs-soa-vs-microservices-vs-mui-vs-nano-services/
[S23] KnowledgeZone Admin. What is Modular Monolith? 2023. https://
knowledgezone.co.in/posts/632d68ee8a9e14d7a6c36e40
[S24] Md Kamaruzzaman. Looking Beyond the Hype: Is Modular Mono-
lithic Software Architecture Really Dead? 2020. https://towardsdata-
science.com/looking-beyond-the-hype-is-modular-monolithic-soft-
ware-architecture-really-dead-e386191610f8
[S25] My experience of using modular monolith and DDD architectures.
2021. https://www.thereformedprogrammer.net/my-experience-of-
using-modular-monolith-and-ddd-architectures/
[S26] Application Developer from ThoughtWorks. Modular Monolith - A
Step Towards Microservice. 2019. https://akshantalpm.github.io/Mo-
dular-Monolith/
[S27] Dzmitry Chubryk. Developing Modular Monolith vs. Traditional
Monolith in Software Engineering, pros and cons. 2023. https://dev.to
/flashblack/developing-modular-monolith-vs-traditional-monolith-
in-software-engineering-pros-and-cons-p61
[S28] Hacker News. https://news.ycombinator.com/item?id=29578712
[S29] Arpit Mohan. Our Journey from SaaS to OSS: Embracing the Modular
Monolith over Microservices. 2023. https://www.appsmith.com/blog/
monoliths-vs-microservices-1
[S30] Thinktecture Team. Modular Monoliths With ASP.NET Core – Prag-
matic Architecture. 2021. https://www.thinktecture.com/en/asp-net-
core/modular-monolith/
[S31] ThanhSangLuskillaug. Modular Monolithic (according to microser-
vice’s principles). 2022. https://www.slideshare.net/ThanhSangLusk-
illaug/modular-monolithic-according-to-microservices-principles
[S32] Light Platform. Modular Monolith. 2021. https://www.networknt.
com/architecture/modular-monolith/
[S33] Jakub Pierzchlewicz. THE MYTHICAL MODULAR MONOLITH.
2020. https://jpintelli.com/2020/09/03/the-mythical-modular-mono-
lith/
[S34] Maciej “MJ” Jedrzejewski. Dillema: Modular monolith or microser-
vices? 2022. https://meaboutsoftware.com/2022/07/18/dillema-1-mo-
nolith-or-microservices/
[S35] Sebastian. Modular monolith in Python. 2020. https://breadcrumbsc-
ollector.tech/modular-monolith-in-python/
[S36] Hamid Reza Sharifi. Introduction to Spring Modulith. 2023. https://
www.baeldung.com/spring-modulith
[S37] Ashley Peacock. Service Weaver: A Framework From Google For Bal-
ancing Monoliths and Microservices. 2023. https://betterprogramm-
ing.pub/service-weaver-a-framework-from-google-for-balancing-m-
onoliths-and-microservices-583e69b274dd
[S38] Dmytro Lazarchuk. How Microservices And APIs Can Make Your
Company Modular. 2022. https://www.forbes.com/sites/forbestechc-
ouncil/2022/02/17/how-microservices-and-apis-can-make-your-co-
mpany-modular/?sh=51a0185ae1cf
[S39] Assis Zang. Creating Good Monoliths in ASP.NET Core. 2022. https://
www.telerik.com/blogs/creating-good-monoliths-aspnet-core
XXX 2024, Month 2024, City, Country
R. Su et al.
[S40] Ruben Casas. Modular Monoliths: Have we come full circle? 2021.
https://www.infoxicator.com/en/modular-monoliths-have-we-come-
full-circle
[S41] Adrian Ochmann. Understanding Architecture: Reasons to Build a
Modular Monolith First? 2023. https://www.wearecogworks.com/
blog/understanding-architecture-reasons-to-build-a-modular-mono-
lith-first/
[S42] Lothar Schulz. Empower modular monolith – never miss these lead-
ership advises. 2023. https://www.lotharschulz.info/2023/10/04/empo-
wer-modular-monolith-never-miss-these-leadership-advises/
[S43] Bilgin Ibryam. Distributed transaction patterns for microservices
compared. 2021. https://developers.redhat.com/articles/2021/09/21/
distributed-transaction-patterns-microservices-compared
[S44] Kevin Urrutia, Justin Trugman. Achieving Monolith Development
Velocity (with a Microservice’s Availability). 2023. https://www.bug-
drivendevelopment.com/p/achieving-monolith-development-velocit
y-with-a-microservice-s-availability
[S45] Arkadiusz Rosloniec. Modular software architecture: advantages and
disadvantages of using monolith, microservices and modular mono-
lith. 2023. https://pretius.com/blog/modular-software-architecture/
[S46] Modular monolith vs microservices for hybrid multi-tenancy. 2021.
https://softwareengineering.stackexchange.com/questions/43236/m-
odular-monolith-vs-microservices-for-hybrid-multi-tenancy
[S47] Admir Mehanovic. Strategic Choices: Microservices vs. Monoliths
for Architectural Success. 2023. https://www.penzle.com/blog/strate-
gic-choices-microservices-vs-monoliths-for-architectural-success
[S48] Aleksei Loos. Case Study: Ditching Microservices in Favor of a Mod-
ular Monolith. 2020. https://playtech.ee/blog/case-study-ditching-
microservices-in-favor-of-a-modular-monolith
[S49] Arpit Mohan. How Adopting a Modular Monolithic Architecture
Enables Our OSS. 2023. https://www.appsmith.com/blog/monoliths-
vs-microservices-3
[S50] Tristan Mahinay. Modular Monolithic in Practice. 2023. https://blog.
rjtmahinay.com/modular-monolithic-in-practice
[S51] Gusto Engineering Blog. Building Toward a Modular Monolith. 2019.
https://engineering.gusto.com/building-toward-a-modular-monolith/
[S52] Spring Blog. Introducing Spring Modulith. 2022. https://spring.io/
blog/2022/10/21/introducing-spring-modulith
[S53] Kamil Kucharski. Modular Monolith in Django. 2023. https://makimo.
com/blog/modular-monolith-in-django/
[S54] Shai Almog. Is it Time to go Back to the Monolith? 2023. https://debu-
gagent.com/is-it-time-to-go-back-to-the-monolith
[S55] Google Open Source Blog. Introducing Service Weaver: A Frame-
work for Writing Distributed Applications. 2023. https://opensource.
googleblog.com/2023/03/introducing-service-weaver-framework-for-
writing-distributed-applications.html
[S56] Service Weaver official website. https://serviceweaver.dev/
[S57] Shiju Varghese. Monolith or Microservices, or Both: Building Mod-
ern Distributed Applications in Go with Service Weaver. 2023. https://
shijuvar.medium.com/monolith-or-microservices-or-both-building-
modern-distributed-applications-in-go-with-service-a096616434fc
[S58] Matt Campbell. Google Service Weaver Enables Coding as a Mono-
lith and Deploying as Microservices. 2023. https://www.infoq.com/
news/2023/03/google-weaver-framework/
[S59] Hector Valls. Service Weaver: Write monolith, deploy microservices.
2023. https://hvalls.dev/posts/service-weaver
[S60] Preslav Rachev. Digging into Service Weaver: Dependency Injection.
2023. https://preslav.me/2023/05/12/golang-dependency-injection-
in-google-service-weaver/
[S61] Kay Ewbank. Google Introduces Service Weaver Framework. 2023.
https://www.i-programmer.info/news/182-frameworks/16154-goog
le-introduces-service-weaver-framework.html
[S62] Katie Dee. Google announces Service Weaver for writing distributed
applications. 2023. https://sdtimes.com/software-development/googl
e-announces-service-weaver-for-writing-distributed-applications/
[S63] Introducing Service Weaver: A Framework for Writing Distributed
Applications. 2023. https://faun.dev/c/links/faun/introducing-service-
weaver-a-framework-for-writing-distributed-applications/
[S64] Google Blogs. Introducing Service Weaver: A Framework for Writing
Distributed Applications. 2023. https://www.googblogs.com/introdu-
cing-service-weaver-a-framework-for-writing-distributed-applicati-
ons/
"
"Researchers have been trying to attain semantic interoperability of Electronic Health Records (EHRs) for quite some time. Using an ontology-based approach that leverages OWL2 ontologies, this research attempts to achieve this goal by concentrating on medical diagnostic statements. The canonical ontology constructed as part of this work includes descriptions of various healthcare-related terms. The proposed framework, which adheres to the notion of full semantic interoperability, includes three primary components: a canonical ontology, an application ontology for each separate health information system, and mapping modules for integrating the two ontologies. Translating information from a source system to a target system with different representations is accomplished by utilizing logic-based descriptions, automated reasoning, and a transfer mechanism. The authors offer a path mapping feature to resolve structural discrepancies between heterogeneous EHR representations, thereby facilitating proper alignment of the data. A discussion on the challenges of implementing this framework in real-world situations rounds out the paper.","The drive to achieve semantic interoperability in the field of healthcare, particularly with regard to Electronic Health Records (EHRs), is a topic of ongoing research and development. The European Community outlined a long-term roadmap in 2009 that laid out the steps necessary for achieving this goal. The introduction of EHRs into healthcare institutions was seen as a positive first step, as they offer advantages such as avoiding legibility issues and facilitating rapid exchange of clinical data between organizations. However, the autonomous development of health information systems by most healthcare institutions has resulted in a proliferation of heterogeneous systems, each with its own proprietary model for representing and storing EHR information. This heterogeneity creates obstacles to interoperability and data exchange among different healthcare organizations.","nanSeveral initiatives are underway to address interoperability challenges in healthcare. Some approaches focus on using knowledge representation standards, such as openEHR, ISO 13606, and HL7-CDA, which provide a dual model-based methodology for representing EHR information. While these standards aim to provide a common framework for data representation, the authors argue that interoperability should not be limited to having a single representation, but rather should allow for semantically equivalent representations. This approach offers several benefits, including promoting semantic interoperability, extensibility to different models, and reducing the need for human intervention due to the use of automated reasoning mechanisms.nannan","The proposed framework for achieving semantic interoperability of EHRs is centered around the use of OWL2 ontologies. The canonical ontology serves as a central conceptual model, with classes and properties representing various types of information found in EHRs, such as observations, evaluations, instructions, actions, and administrative events. The focus of this work is on diagnoses, a specific type of evaluation. A key aspect of the canonical ontology is its inclusion of links to well-known medical terminologies, such as SNOMED and LOINC, to enhance the precision and consistency of the defined terms.nan","To demonstrate the feasibility of the proposed approach, the authors provide an example involving two hypothetical systems, System A and System B. System A uses a relational database schema to store EHR information, while System B follows the HL7 standard. The process of translating information from System A to System B involves several steps, including extracting data from the source system, classifying the data using the application ontology of System A, enriching the local information with inferred knowledge, classifying the data using the canonical ontology, and finally recognizing and storing the data in the target system using its application ontology. Path mappings are utilized during the transfer process to handle structural differences between the ontologies, ensuring proper alignment of information.","The authors conclude by highlighting the main features of the proposed framework, emphasizing its extensibility to both standard and proprietary models, the use of a formal ontology as the canonical conceptual model, the minimization of the need for human intervention through automated reasoning, and the potential for scalability and addressing real-world challenges. They also acknowledge areas for improvement, such as addressing scalability concerns and exploring techniques for dealing with unstructured or semi-structured data sources.",Toward Semantic Interoperability of Electronic Health Records,"Idoia Berges, Jesús Bermúdez, Arantza Illarramendi","arXiv:2401.11865v1  [cs.AI]  22 Jan 2024
1
©2011 IEEE. Personal use of this material is permitted.
Permission from IEEE must be obtained for all other uses, in
any current or future media, including reprinting/republishing
this material for advertising or promotional purposes, creating
new collective works, for resale or redistribution to servers or
lists, or reuse of any copyrighted component of this work in
other works.”
1
Towards Semantic Interoperability of Electronic
Health Records
Idoia Berges, Jes´us Berm´udez and Arantza Illarramendi
Abstract—Although the goal of achieving semantic interop-
erability of Electronic Health Records (EHRs) is pursued by
many researchers, it has not been accomplished yet. In this
paper we present a proposal that smoothes out the way to-
wards the achievement of that goal. In particular our work
focuses on medical diagnoses statements. In summary the main
contributions of our ontology-based proposal are the following:
First, it includes a canonical ontology whose EHR-related terms
focus on semantic aspects. As a result, their descriptions are
independent of languages and technology aspects used in different
organizations to represent EHRs. Moreover, those terms are
related to their corresponding codes in well-known medical
terminologies. Second, it deals with modules that allow obtaining
rich ontological representations of EHR information managed by
proprietary models of health information systems. The features of
one speciﬁc module are shown as reference. Third, it considers the
necessary mapping axioms between ontological terms enhanced
with so-called path mappings. That feature smoothes out struc-
tural differences between heterogeneous EHR representations,
allowing proper alignment of information.
Index Terms—Electronic Health Record, Semantic Interoper-
ability, Ontology.
I. INTRODUCTION
I
N 2009 the European Community presented a long-term
research and deployment roadmap that provides the key
steps for achieving semantic interoperability in the area of
healthcare[1]. The incorporation some years ago of Electronic
Health Records to the healthcare institutions may be seen as
the ﬁrst step towards the achievement of the goal, since, apart
from local advantages over manual records such as avoiding
legibility problems, they favour a fast exchange of clinical data
between different organizations. However, the fact that most
healthcare institutions have developed their health information
systems in an autonomous way has resulted in a proliferation
of heterogeneous health information systems, each one with
its own proprietary model for representing and storing EHR
information, which hinders the task of interoperating with each
other.
In many areas, the adoption of knowledge representation
standards stands out as the most usual approach to solve inter-
operability problems. This happens also in the healthcare area,
where some standards such as openEHR[2], ISO 13606[3] and
HL7-CDA[4] are under development for this purpose. All three
follow a dual model-based methodology for representing EHR
information: the Reference Model deﬁnes basic structures
All authors are with the Department of Languages and Information Systems,
University of the Basque Country, Donostia-San Sebasti´an, 20018 Spain. e-
mail: {idoia.berges, jesus.bermudez, a.illarramendi}@ehu.es.
Manuscript received June 6, 2011.
such as List, Table, etc., while the Archetype Model deﬁnes
knowledge elements (such as Respiration Rate) by using and
constraining the elements of the Reference Model. Although
the idea of using a standard may seem suitable for the consid-
ered goal, we think that interoperability does not mean to have
a unique representation but a semantically acknowledgeable
equivalent one. This would relieve healthcare institutions from
being forced to use one standard in the representation of their
knowledge and moreover, since several standards are being
developed for the same purpose, the interoperability problem
will remain unsolved unless these standards merge into a
single one. Currently, some research is being done on the latter
issue[5].
In this paper we present a proposal to move towards
the notion of full semantic interoperability of heterogeneous
EHRs, which states that when one particular system receives
some EHR information from another healthcare institution,
the received information can be seamlessly integrated into its
underlying repository because the differences in the language,
in the representation of the information and in the storing
systems do not cause any misunderstanding[1]. Two general
approaches for interoperability among systems are described
in [6]: Using a canonical model to which the particular models
are linked or aligning the particular models two by two. The
proposal presented in this paper is sustained in the former
approach. More precisely, it is an ontology-based approach
where OWL2[7] ontologies are used as representation mod-
els. In general, ontologies have been considered relevant for
several purposes such as: enabling reuse of domain knowl-
edge, allowing the analysis of domain knowledge and sharing
common understanding of the meaning of information[8]. Our
approach beneﬁts from the latter advantage and additionally it
provides the following ones:
• It favors the notion of semantic interoperability: The
use of a formal ontology as canonical conceptual model
allows to focus on aspects that are independent of the
languages or technologies used to describe EHRs.
• It favors the notion of extensibility to different models:
The framework comprises two kinds of ontologies which
represent the deﬁnitions of clinical terms that appear in
EHRs at different levels of abstraction. The canonical
contains ontological deﬁnitions of EHR statements and
the application ontologies contain specializations of the
deﬁnitions of the canonical ontology according to the
standards mentioned previously or according to propri-
etary models of healthcare institutions.
• It decreases the need of human intervention: The frame-
2
work relies on a reasoning mechanism that, using axioms
stated in the ontology, infers knowledge that allows the
discovery of more relationships among the heterogeneous
models used by the different health information systems.
Dealing with ontologies, one relevant aspect is the features
of the terms that are part of them. In our scenario those terms
are related to EHRs. Different kinds of information can be
found in an EHR. OpenEHR divides this information into
5 subtypes[9] and we also have adopted that division in the
deﬁnition of our canonical ontology: Observations comprise
the data that can be measured in an objective way, such as the
age of a patient, his respiration rate, etc. Evaluations represent
the evidence obtained from observations, for example the
diagnosis of an illness. Instructions represent actions to be
performed in the future such as the prescription of a medicine
or the request of a laboratory test. Actions are used to model
the information recorded due to the execution of an instruction
and ﬁnally there is one last type to record administrative
events such as admission or discharge information. In this
paper we just focus on one type of evaluations, namely the
diagnoses, but similar ideas to those that will be explained
here for diagnoses could be also applied to the other types of
information. Moreover, the terms of the application ontologies
are obtained from the particular health information systems
and then linked to the terms in the canonical ontology by
using ontology mappings.
A certain number of works related to ours can be found
at present. With regard to the beneﬁts of taking semantics
into account, some works discuss the convenience of using
semantic technologies in several heathcare related issues. In
[10], the handicaps for widespread adoption of semantic
technologies within a care records system are pinpointed.
In [11] the challenges to be addressed in order to be able
to use the so-called Smart Internet to enable reforms on
healthcare information systems are discussed. Lastly, in [12]
the triplespace paradigm is suggested as semantic middleware
to support pervasive access to electronic patient summaries.
The works mentioned next also rely on semantic technologies
for interchanging data, as opposed to other formats such as
XML, which are structure-based. More speciﬁcally, related to
the topic of facilitating semantic interoperability between het-
erogeneous health information systems, the following works
deal only with the interoperability between standard-based
health information systems: [13] provides a solution to achieve
semantic interoperability between systems that have been
developed under the HL7 reference model and which requires
that the source system has some prior knowledge about the
target system. In [14] ontology mappings are proposed be-
tween pairs of archetype-based models. In [15] a model-driven
engineering approach that transforms archetypes of the ISO
13606 standard into OWL models is presented. Finally, authors
in [16] describe an approach to translate from the Archetype
Deﬁnition Language (ADL[17]) to OWL, they also present
some techniques to map archetypes to formal ontologies and
show the convenience of using semantic rules on the resulting
representation in order to guide the execution of primary
care guidelines. In this paper we present a wider approach
since apart from the interoperability of standard-based systems
we deal also with interoperability considering proprietary
models. Some other works that tackle the problem of semantic
interoperability of EHRs from a different perspective are the
following: In [18] a semantic conceptualization model for an
EHR system is presented. This still early work is more oriented
towards the accessibility, use and management of the EHR at
a local level, but it also aims at providing a base in order
to solve the interoperability problem from a semantic point
of view. In [19] the hypothesis that semantic technologies
are potential bridging technologies between the EHRs and
medical terminologies –as well as a possible representation of
the combined semantics of systems to be integrated– is raised
and some experimental study is made on this issue. We also
promote the connection between the semantic representation of
EHR statements and their codes in well-known terminologies.
Finally, in [20] authors discuss how advanced middleware,
such as Enterprise Middleware Bus, and semantic web services
can assist in solving interoperability issues between eHealth
systems.
The rest of the paper is divided as it follows: In Section
II the global architecture of the framework is presented,
and extensive details about the canonical ontology and the
auxiliary modules DB2OntoModule and MappingModule are
given. The feasibility of the solution is shown in Section III,
and ﬁnally, conclusions are discussed in Section IV.
II. GLOBAL ARCHITECTURE
In Fig. 1 the three-layered architecture of the solution can
be found. The lower layer, contains the particular underlying
repository of each healthcare institution, where the information
of the EHRs is stored. Associated to each kind of underlying
repository, there is some kind of ﬁle (e.g. database schema,
set of ADL ﬁles) where information about the structures in
the repository can be found. Then, the middle layer contains
one application ontology for each information system, built on
top of the underlying repository. These application ontologies
are created semi-automatically from the underlying reposito-
ries by some auxiliary modules (e.g. DB2OntoModule and
ADL2OntoModule), or imported from an ontology repository,
and describe semantically each underlying repository. More-
over, they are linked to their corresponding repositories by
some Σ links that specify how to transfer information from
each of the representations to the other. Finally the upper layer
contains one canonical ontology. This ontology will contain
the necessary classes and properties to represent the different
types of information that can be found in an EHR and is
linked to each of the application ontologies by some integra-
tion mappings deﬁned by a MappingModule. Each particular
healthcare institution will have only a partial view of the global
framework, since with our proposal there is no need for that
institution to know anything but its underlying repository, its
application ontology, and the canonical ontology.
The proposed framework allows one healthcare institution
to interpret on the ﬂy clinical statements sent by another one –
even when they use proprietary formats. We support our claim
on the following techniques:
3
Fig. 1.
Global architecture of the solution
• Logic-based descriptions: Representations of diagnoses
considered by particular health information systems, de-
scribed using standards as well as proprietary models,
are expressed in our approach by using OWL2 ontology
axioms. Moreover, terms in those axioms are related with
canonical ontology terms that focus their descriptions
on language and technology independent aspects. This
approach increases the opportunities of solving the in-
teroperability issue since it relies mainly on semantic
aspects.
• Automated reasoning: All ontology descriptions, as well
as the mappings among elements of the ontologies, are
expressed in the same formalism OWL2. This uniform
representation allows the use of well-known reasoners
in order to derive new statements from the existing
ones. Furthermore, the mismatch problem is avoided and
automatic integration is facilitated.
• Transfer mechanism: A process, guided by the previous
two items, is implemented to transform a particular
clinical statement from a healthcare institution into a
corresponding clinical statement for another healthcare
institution. So-called path mappings play a crucial role
during the transfer process, smoothing out the structural
differences between EHR representations.
Finally, we are aware that the messiness of real world
EHRs may sometimes hinder the task of ﬁtting them into the
presented proposal, but in our opinion this does not invalidate
the advantages it can provide in many situations.
In the following subsections, the canonical ontology, the
DB2OntoModule and the MappingModule are described thor-
oughly.
A. Canonical ontology: Representing diagnoses in OWL
The canonical ontology contains the necessary classes and
properties to represent the different types of information that
can be found in an EHR. Following openEHRs classiﬁcation
of EHR entries, we have deﬁned in the ontology ﬁve classes
to represent the general categories: Observation, Evaluation,
Instruction, Action and Admin. Moreover, these ﬁve classes
have been specialized to represent more speciﬁc types of
entries. As we pointed out in the introduction, in this paper we
deal with diagnoses, which are a special case of evaluations.
A diagnosis is deﬁned as the act of identifying a disease
from its signs and symptoms, as well as the decision reached
by that act1. For this reason, in addition to representing
a diagnosis as a subclass of Evaluation, its deﬁnition is
enhanced with two properties: hasFinding, to indicate the
conclusion reached by the physician about what is happening
to the patient, and hasObs, to indicate the information about
the observation(s) which lead to that assessment2.
Diagnosis
≡
Evaluation ⊓ =1 hasFinding.Finding ⊓
∃hasObs.Observation
Speciﬁc diagnoses are deﬁned as subclasses of the class
Diagnosis. For example, the evidence obtained as a result of
an ECG can be described by specializing the range restric-
tions of the properties hasFinding and hasObs. For instance,
the observation that leads to an ECG diagnosis is an ECG
Recording, which is made up of several components3: some of
the components refer to information about the heart’s electrical
axis (i.e. the general direction of the heart’s depolarization
wavefront), while the others refer to information about the
entire ECG.
ECGDiagnosis
≡
Diagnosis ⊓ =1 hasFinding.ECGFinding ⊓
∃hasObs.ECGRecording
ECGRecording
≡
Observation ⊓ ∃comp.P-Axis ⊓
∃comp.QRS-Axis ⊓ ∃comp.T-Axis ⊓
∃comp.PR-Interval ⊓ ∃comp.QT-Interval ⊓
∃comp.QTc-Interval ⊓ ∃comp.QRS-Duration ⊓
∃comp.Heart-Rate
One advantage of working in the medical area is the
existence of medical terminologies, such as SNOMED[21] and
LOINC[22]. These terminologies cover most areas of clinical
information and provide a consistent way to identify medical
terms univocally, which can be very helpful at the time of
gathering and exchanging clinical results. Our system takes
advantage of these terminologies to enhance the deﬁnition of
the classes in the canonical ontology. Thus, whenever is pos-
sible, each term in the ontology is related to its corresponding
code in those terminologies:
ECGDiagnosis
≡
∃loinc.{‘8601-7’}
ECGRecording
≡
∃loinc.{‘34534-8’}
P-Axis
≡
∃loinc.{‘8626-4’}
The use of terminological codes into the deﬁnitions of the
classes in the ontology increases the chances of achieving a
successful communication.
Finally, since building a canonical ontology is not an easy
task, we think that efforts that are being done to deﬁne
archetypes in openEHR could be reused to achieve that task.
1http://www.merriam-webster.com/dictionary/diagnosis
2For the presentation we prefer a logic notation instead of the more verbose
RDF/XML syntax.
3For the sake of brevity, in this example only some components of an ECG
are taken into account. Please refer to [2] for the whole set of components.
4
B. DB2OntoModule
Taking into account the widespread use of relational
databases to store EHR records, we show in this subsection
the main features of the module DB2OntoModule4. This
module takes as input a database schema and after apply-
ing a set of rules based on schema features, it obtains the
ontological representations of those relational databases (i.e.
the application ontology of that system). In the specialized
literature many approaches for translating relational structures
into more expressive formalisms can be found: object models,
description logics and Semantic Web technologies. Some of
them follow the so-called declarative approach, which ﬁrst
convert the relational structure into a declarative language and
then the result is modiﬁed by the user to declare additional
knowledge about the database (e.g [23]). Our proposal also
uses the declarative approach but its novel contribution relies
in the large number of schema properties that it considers,
allowing to make explicit more knowledge, and in the fact
that it associates to the obtained classes their corresponding
codes that appear in well-known medical terminologies.
In order for the DB2OntoModule to accomplish the last fea-
ture, it deals with an element called “Terminology Manager”
or, in short, “TM”, which has an associated function of the
form getX(conceptName), where X is the name of a termi-
nology (LOINC, SNOMED, or any other) and conceptName
is the name of the relation or attribute whose terminological
code is to be retrieved. For example, in the case of a relation
BloodPressure(id, systolic, diastolic) the TM would contain:
Identifying path
LOINC
SNOMED
BloodPressure
18684-1
75367002
BloodPressure.systolic
8480-6
72313002
BloodPressure.diastolic
8462-4
271650006
Concerning schema features, the DB2OntoModule works as
follows:
Relations: Relations of the relational schema are translated
into OWL2 classes. Moreover, if for a given relation R,
TM.getLOINC(R)=LC (being LC a particular LOINC code),
a new axiom R≡ ∃loinc.{’LC’} is added to the ontology
(analogously for other terminological codes).
Attributes: Two options arise: (1) If for a given attribute a in
R (R.a) TM.getLOINC(R.a) returns some code LC, then a
new class A is created (if there is no other class which already
has that code). Moreover, the axioms A≡ ∃loinc.{’LC’}
and A⊑ ∃value.getType(a) are added. Finally, if attribute a
is compulsory in R, the axiom R⊑ ∃hasA.A is added. (2)
If there is no code for R.a in TM, a property a is created in
the ontology, where Domain(a)=R and Range(a)=getType(a).
Moreover, if attribute a is compulsory in R, the axiom R⊑ ∃a
is added.
Integrity
constraints:
An
integrity
constraint
such
as
R.a>30 adds a new axiom of type R⊑ ∃ hasA.(A and
∃value[>30]) if R.a is in TM, and a new axiom of type
R⊑ ∃ a[>30] otherwise.
4Other modules, such as the ADL2Onto module, would be used to perform
the translations between other sources and the ontology
Once the previous steps are accomplished, the next one
involves enriching the obtained descriptions by using several
types of information, such as inclusion, exclusion and func-
tional dependencies:
Inclusion dependencies: Three different situations are con-
sidered (see a previous work [24] from our group for more
details): (1) Dependencies between key (R.K) and non-key
(S.x) attributes, which indicate the existence of a foreign keys
of type S.x⊆R.K. These dependencies are reﬂected by deﬁning
an association between the ontology classes obtained from
those relations (S⊑ ∃x.R); (2) Dependencies between the
keys of two relations (R.K⊆R’.K’); and (3) Dependencies
between a subset of a key and a key (R.subK⊆R’.K’), which
also have the corresponding reﬂection.
Exclusion dependencies: An exclusion dependency between
the keys of two relations (R.K∩R’.K’=∅) creates a new axiom
of the form R⊑ ¬R’ in the ontology. In addition, if there is
no class in the ontology that subsumes both R and R’, such
new class S is created and the axioms R⊑S and R’⊑S are
added.
Functional dependencies: If a functional dependency of the
form R.X→ R.y is detected, with X and y being a non-
key attribute set and a non-key attribute respectively, a new
class X is created. Moreover, two new properties hasX
and hasY are deﬁned and the axioms R⊑ ∃hasX.X and
X⊑ ∃hasY.getType(y) are added to the ontology.
Furthermore, the ontology can be enriched by using domain
information for attribute values, for example, in the case of
properties expressed by enumerating attribute values. For an
attribute R.a whose possible values are either A1 or A2,
if both have a corresponding code in the TM, classes A1
and A2 are created in the ontology. Moreover, one general
class to group those two classes is created (e.g. A0) and
axioms A1⊑A0, A2⊑A0, A1⊑¬A2 and R⊑ ∀a.A0 are added.
However, in the case where A1 and A2 have no terminological
code in the TM, class A0 is created as an enumeration of two
individuals a1 and a2, and axiom R⊑ ∀a.A0 is added too.
All the previous types of considerations are applied in the
following sequence: ﬁrst inclusion dependencies; then when
the input relational schema is not in second or third normal
form, functional dependencies are used to create new classes;
next exclusion dependencies are exploited and last integrity
constraints and domain information for attribute values are
considered. Finally, once the DB2OntoModule has performed
the steps above, a candidate ontology has been created.
However, we feel that it is advisable to allow the health
system administrator to modify the ontology in a ﬂexible way.
For example, some common changes could be substituting ⊑
relationships with ≡ relationships, modifying the names of
the terms that have been created, or adding some missing
terminological code. These changes can be done manually
using any well-known ontology editor.
The DB2OntoModule at work: For example, a particular
registration for an ECG diagnosis may consist of four rela-
tional tables according to the following schema (all attributes
are considered compulsory)
ECGDiagnosis(code, finding, recording)
5
ECGObservation(code, axis, global)
ECGAxis(code, P-Axis, QRS-Axis, T-Axis)
ECGGlobal(code, PR-Interval, QT-Interval,
QTC-Interval, QRS-Duration, Heart-Rate)
and the following inclusion dependencies between non-key
and key attributes:
ECGDiagnosis.recording ⊆ ECGObservation.code
ECGObservation.axis ⊆ ECGAxis.code
ECGObservation.global ⊆ ECGGlobal.code
Moreover, let us consider the bogus case where the attribute
finding of the ECGDiagnosis table must be either “Normal
ECG” or “Abnormal ECG”. As a result of applying the
initial steps for transforming the schema to ontology elements
four new classes are created in the ontology: EGCDiagnosis,
ECGObservation, ECGAxis and ECGGlobal, each with its re-
spective LOINC code. Moreover, since the compulsory at-
tribute P-Axis, whose type is “int”, has also a LOINC code
at the TM, axioms P-Axis≡ ∃loinc.{‘8626-4’}, ECGAxis⊑
∃hasP-Axis.P-Axis and P-Axis⊑ ∃value.xsd:int are created
(same process for the other attributes). Then, the rules for
inclusion dependencies are applied, and, for example, from the
inclusion dependency ECGObservation.axis ⊆ ECGAxis.code,
axiom ECGObservation⊑ ∃axis.ECGAxis is created. Moreover,
information about the allowed values for the finding attribute
is considered and a new class ECGFinding is created as
superclass of two other classes NormalECG and AbnormalECG.
Finally, manual changes are applied. For example, we have
chosen to substitute some of the subclass relationships with
equivalence relationships, so the created ontology has, among
others, the following axioms5:
a:ECGDiagnosis
≡
∃a:finding.a:ECGFinding ⊓
∃a:recording.a:ECGObservation
a:ECGDiagnosis
≡
∃a:loinc.{8601-7}
a:ECGObservation
≡
∃a:hasAxis.a:ECGAxis ⊓
∃a:hasGlobal.a:ECGGlobal
a:ECGObservation
≡
∃loinc.{34534-8}
a:ECGAxis
≡
∃a:hasP-Axis.a:P-Axis ⊓
∃a:hasQRS-Axis.a:QRS-Axis ⊓
∃a:hasT-Axis.a:T-Axis
a:NormalECG
≡
a:ECGFinding ⊓ a:value.{""Normal ECG""}
a:NormalECG
≡
a:snomed.{102593009}
The second task of the DB2OntoModule is to create the Σ
links that indicate how to transfer the information from the
database to the ontology that has been created from it (and
vice versa). This task was previously tackled by our research
group, so we refer to the reader to [25] for further technical
details.
5Throughout the paper, namespaces a: and b: will be used to refer to terms
in the application ontologies of two particular systems A and B. Moreover,
namespace c: or no namespace are used to indicate the terms in the canonical
ontology.
C. MappingModule
Once an application ontology of one particular system
has been generated by the corresponding translator module,
it must be integrated with the canonical ontology, and the
mappings between the terms of that application ontology and
the canonical ontology must be created. A MappingModule has
been implemented for this purpose. Wide research has been
done in the specialized literature about ontology mapping (e.g.
[26]), so working in new techniques for that same issue is out
of the scope of our work. So, our MappingModule takes a
pragmatic approach and receives as input a set of basic map-
ping axioms speciﬁcally deﬁned by the system administrator
(for example, to state that the property a:loinc is equivalent
to the property c:loinc). Then, it incorporates these basic
mappings into the ontologies and, with the help of a reasoner,
it creates an integration mapping that relates the terms of the
application ontology with those of the canonical ontology.
However, our module presents a distinguishing feature,
since it considers mappings between ontology paths, which are
rarely considered in other works. In order to be aware of the
importance of discovering path mappings, let us compare the
deﬁnitions of classes c:ECGRecording and a:ECGObservation
in sections II-A and II-B respectively. Both share the same
LOINC code (34534-8), so their semantics are the same.
Looking at the description of c:ECGRecording , it can be seen
that any individual belonging to that class will be directly
related to an individual of the class c:P-Axis via the property
c:comp (assume the same intuition for the other components).
However, in the case of the descriptions in the application on-
tology of system A, it turns out that classes a:ECGObservation
and a:P-Axis are not directly related, but indirectly: ﬁrst
a:ECGObservation is related to the class a:ECGAxis via the
property a:hasAxis and then the class a:ECGAxis is related
to the class a:P-Axis via the property a:hasP-Axis . Then it
could be stated that there is a simple path between classes
c:ECGRecording and c:P-Axis , while there is a composite
path between classes a:ECGObservation and a:P-Axis .
Intuitively, those two paths could be regarded as equivalent,
since their only difference is from the structural point of view
caused by the heterogeneous origin of the ontologies, not from
a semantic point of view. Let us show how our module deals
with that aspect:
Deﬁnition 1. An ontology path is a regular expression of the
form A.(p.[B])+ where A,B represent class names and p
represents property names, all from the same ontology.
Let us denote equivalences between paths with the symbol
≡p. For instance, the aforementioned example is represented
as:
a:ECGObservation.a:hasAxis[a:ECGAxis].a:hasP-Axis.[a:P-Axis]
≡p
c:ECGRecording.c:comp[c:P-Axis]
Although in this example an equivalence path mapping has
been presented, a corresponding idea is valid for subclass path
mappings (⊑p) and superclass path mappings (⊒p). In order
to determine path mappings, ﬁrst path mapping candidates are
searched:
6
Deﬁnition
2.
Let
PathC
=
C0.p1[C1]....pn[Cn]
and
PathD = D0.q1[D1]....qm[Dm] be two ontology paths. A path
mapping candidate exists between PathC and PathD if any
of the following statements holds:
• C0 ⊑ D0 and Cn ⊑ Dm(represented as PathC
⊑∼
PathD)
• C0 ⊒ D0 and Cn ⊒ Dm(represented as PathC
⊒∼
PathD)
Moreover, if PathC
⊑∼ PathD and PathC
⊒∼ PathD then
PathC
≡∼ PathD
A path mapping candidate becomes a proper path mapping
when the semantics of both paths is found to be the same. Path
mappings are useful at the time of transforming individuals
from one ontology so that they meet the requirements of the
target ontology. The implementation of path mappings is done
by using SWRL[27] rules. SWRL increases the expressivity
of OWL and thus allows to model more domain knowledge
than the one achieved by using OWL in its own. Moreover,
since SWRL can be tightly integrated with OWL, there is no
impedance mismatch between the modelling language and the
rules language: SWRL rules can use directly the classes, prop-
erties and individuals deﬁned in the OWL model. For example,
the path mappings shown before would be implemented using
the following rules (one in each way):
R1
a:ECGObservation(?e)
∧
a:hasAxis(?e,?x)
∧
a:hasP-Axis(?x,?p) → c:comp(?e,?p)
R2
c:ECGRecording(?e)
∧
c:comp(?e,?p)
∧
c:P-Axis(?p)
∧
swrlx:createOWLThing(?e,?x)
→
a:hasAxis(?e,?x)
∧
a:ECGAxis(?x)
∧
a:hasP-Axis(?x,?p)
As looking for all the candidate path mappings between
two large ontologies might be a hard task considering both
time and resources, a threshold can be established to indicate
the maximum length of the paths to be searched. Some other
heuristics could be applied too to discover candidate path
mappings efﬁciently.
So, to sum up, the integration mapping that is generated
between an application ontology and the canonical ontology
can be deﬁned as it follows:
Deﬁnition 3. An integration mapping is a structure I =
⟨O, G, M⟩ where O is a set of OWL2 axioms that comprises
the application ontology corresponding to a healthcare institu-
tion, G is the set of OWL2 axioms for the canonical ontology,
and M is a set of mapping axioms of any of the following
forms:
• Co ⊑ Expg, Co ⊒ Expg or Co ≡ Expg, where Co
is a class name from O, and Expg is a OWL2 class
expression that uses only terms from G.
• po ⊑ pg or po ⊒ pg , where po is a property name from
O, and pg is property name from G.
• sameAs(io, ig) , where io is the name of an individual
from O, and ig is the name of an individual from G.
• Patho ⊑p Pathg, Patho ⊒p Pathg or Patho ≡p
Pathg, where Patho is an ontology path in O and Pathg
is an ontology path in G.
The result of the engineering process of producing the set
M of mapping axioms is the key for the interoperability of
two different health information systems.
III. FRAMEWORK AT WORK
The main contribution of our proposal is the capability of
one system B of interpreting information sent by another
system A on the ﬂy, without prior peer-to-peer agreement
on the semantics and syntax of the interchanged data. In this
example, let us suppose that the database schema of system
A is the one presented in section II-B. Moreover, in the
case of system B, let us consider that it follows the HL7
standard and that different representations are used to represent
ECG information depending on the result of the ECG (e.g.:
ECGNormalDiag for normal ECG results, ECGAbnormalDiag
when abnormalities have been detected). The work of the
ADL2OntoModule and MappingModule led to the following
axioms, with respect to the application ontology of system B:
b:ECGNormalDiag
≡
b:ECGDiagnosis ⊓
=1 b:finding.b:ECGNormalFind ⊓
∃b:component.b:P-Ax ⊓
∃b:component.b:QRS-Ax ⊓ ... ⊓
∃b:component.b:Heart-R
b:ECGDiagnosis
≡
b:loinc.{8601-7}
b:ECGNormalFind
≡
b:snomed.{102593009}
b:loinc
≡
c:loinc
b:component
≡
c:comp
b:snomed
≡
c:snomed
b:finding
≡
c:finding
p1
⊑p
p2
where p1=b:ECGNormalDiag.b:component[b:P-Ax] and p2=
c:ECGDiagnosis.c:hasObs[c:ECGRecording].c:comp[c:P-
Axis].
Moreover, let us suppose that system A wants to send to
system B the following information about the ECGDiagnosis
whose code is ecg01:
σcode=′ecg01′(ECGDiagnosis)= (ecg01, Normal ECG, r01)
σcode=′r01′(ECGRecording)= (r01, ax01, gl01)
σcode=′ax01′(ECGAxis)= (ax01, 27, 88, 49)
σcode=′gl01′(ECGGlobal) = (gl01, 138, 390, 39, 112, 62)
Finally, assume that some of the mapping axioms between
the application ontology of system A and the canonical ontol-
ogy are the following:
a:loinc≡c:loinc
a:snomed≡c:snomed
a:finding≡c:hasFinding
a:recording⊑c:hasObs
a:hasAxis⊑c:comp
a:value≡c:value
The process that needs to be carried out is composed of
several steps:
Step 1: Classiﬁcation of the information in the ap-
plication ontology. In this step the information to be sent
is converted into statements about individuals generated for
7
the application ontology of system A. For example, the
main individual a:ecg01 will be an instance of the class
a:ECGDiagnosis. This is a straightforward process thanks to
the Σ links created by the DB2OntoModule between the
storage system of system A and its application ontology.
Among others, the following OWL statements (represented as
triples) will be created:
(a:ecg01 rdf:type a:ECGDiagnosis)
(a:r01 a:hasAxis a:ax01)
(a:ecg01 a:finding a:f01)
(a:ax01 rdf:type a:ECGAxis)
(a:f01 a:value ""Normal ECG"")
(a:ax01 a:hasP-Axis a:pax01)
(a:ecg01 a:recording a:r01)
(a:pax01 rdf:type a:P-Axis)
(a:r01 rdf:type a:ECGObservation)
(a:pax01 a:value 27)
Step 2: Enrichment of the local information at the
application ontology. In this step implicit information (regard-
ing the individuals) that can be inferred from the application
ontology of system A is made explicit with the help of a
reasoner. For example, in this step each individual inherits a
terminology code from its corresponding class:
(a:ecg01 a:loinc 8601-7)
(a:ax01 a:loinc 8607-4)
(a:f01 a:snomed 102593009)
(a:pax01 a:loinc 8626-4)
(a:r01 a:loinc 34534-8)
Step
3:
Classiﬁcation
of
the
information
in
the
canonical ontology. At this point, thanks to the equiv-
alence, subsumption and path mappings that have been
deﬁned by the MappingModule and the help of a rea-
soner,
the
individuals
are
now
classiﬁed
as
instances
of the concepts of the canonical ontology. For exam-
ple, given that
a:ECGObservation≡∃loinc.{34534-8} and
c:ECGRecording≡∃loinc.{34534-8} it is wise to think that
the MappingModule will infer the equivalence mapping
a:ECGObservation≡c:ECGRecording. Then, as the assertional
box of the application ontology of system A contains the
triple (a:r01 rdf:type a:ECGObservation), the new triple
(a:r01 rdf:type c:ECGRecording) is inferred. Moreover, since
triples (a:r01 rdf:type a:ECGObservation), (a:r01 a:hasAxis
a:ax01) and (a:ax01 a:hasP-Axis a:pax01) exist, path rule R1
is ﬁred and the triple (a:r01 c:comp a:pax01) is generated. The
remaining new triples, some of which are shown next, can be
ﬁgured out accordingly.
(a:ecg01 rdf:type c:ECGDiagnosis)
(a:ecg01 c:hasObs a:r01)
(a:ecg01 c:loinc 8601-7)
(a:r01 c:comp a:pax01)
(a:ecg01 c:hasFinding a:f01)
(a:pax01 rdf:type c:P-Axis)
(a:f01 c:snomed 102593009)
(a:pax01 c:value 27)
(a:r01 rdf:type c:ECGRecording)
Step 4 : Recognition at the receiver’s ontology. The
triples generated up to this moment are sent to system B and,
thanks to the ontological mappings deﬁned for this ontology
by the MappingModule, the individuals will be recognized as
instances of the classes of its application ontology. For exam-
ple, due to (a:f01 c:snomed 102593009), b:snomed≡c:snomed
and the deﬁnition of class b:ECGNormalFind, f01 is classiﬁed
as an individual of class b:ECGNormalFind, and then, due
to the deﬁnition of class b:ECGDiagnosis, now the main
individual a:ecg01 is classiﬁed as an individual of class
b:ECGNormalDiag :
Fig. 2.
Excerpt of the generated HL7 entry
(a:ecg01 b:loinc 8601-7)
(a:ecg01 rdf:type b:ECGDiagnosis) (a:ecg01 b:component a:pax01)
(a:ecg01 b:finding a:f01)
(a:pax01 rdf:type b:P-Ax)
(a:f01 rdf:type b:ECGNormalFind)
(a:pax01 b:value 27)
(a:ecg01 rdf:type b:ECGNormalDiag)
Step 5: Storage at the receiver’s system: At this point, it
is straightforward to store the information into the underlying
repository of system B due to the Σ links that indicate how
to transform the collection of triples into a suitable HL7
document (see Fig.2). Notice that since the main individual
ecg01 has been recognized as of class b:ECGNormalDiag, it is
possible to choose from the HL7 entry templates of system
B the one which represents only information about normal
ECG results –despite in the sender’s system there was only
one table for storing all kind of ECG diagnoses.
IV. CONCLUSIONS AND DISCUSSION
We have presented a semantic-based framework which al-
lows the interoperability of medical diagnoses between health
information systems, including those which were not devel-
oped following EHR standards. The feasibility of the idea has
been proved through an example. To sum up, the main features
of the framework presented in this paper are the following:
(1) It is extensible to both standard and proprietary models,
since any healthcare institution could create its own application
ontology and relate it to the terms of the canonical ontology
via an integration mapping. Two modules are provided in
order to facilitate such a technically demanding skill: one
module that facilitates the task of obtaining the deﬁnitions
of the application ontology from a particular underlying sys-
tem and another module that facilitates the task of linking
deﬁnitions of the application ontology to deﬁnitions of the
canonical ontology; (2) It uses a formal ontology as canonical
conceptual model, which allows to focus on semantic aspects
that are independent of the languages or technologies used to
describe EHRs. As a result, it is not based on peer-to-peer
transformations but on the semantic acknowledgement of one
instance of a class in the source ontology as instance of another
class in the target ontology; (3) The features of any speciﬁc
system remain unknown to the other systems in the framework.
Acknowledging and using the canonical ontology as a shared
model is enough; (4) Reasoning plays a major role in several
8
parts of the framework, which decreases the need of human
intervention.
However, there are still some challenges, such as those
regarding scalability, that need to be addressed in order for
this approach to be accepted widely. In the case of the
DB2OntoModule, the existence of the terminology manager
TM is assumed. The fact that a particular term of a database
has a corresponding terminological code in the TM allows a
more precise deﬁnition of that term in the application ontology.
We are aware that database systems may not provide with such
a set of correspondences, so syntactic and semantic similarity
measures (such as Levenshtein distance6 or WordNet7-based
similarity) between the terms in the database and those in the
terminologies would have to be applied in order to obtain a
set of candidate codes. Moreover, relational databases whose
schema can be consulted have been chosen as underlying
repositories. In the real world data can be far messier and come
from unstructured or semi-structured sources. In general, the
less structured the source is, the more difﬁcult the construction
of the ontology will be. In the case of unstructured sources,
machine learning and text mining algorithms could be used in
order to create an ontology from input documents. For semi-
structured data in XML, XQuery8 and XPath9 could be used
for extraction of relevant information, and moreover, fuzzy
extensions of those languages could be used to enhance that
extraction. Another technique that could be applied in semi-
structured sources is ILP[28]. With respect to the core task
of building an agreed canonical ontology, efforts devoted to
classiﬁcations on standards (e.g. openEHR) or terminology
taxonomies (e.g. SNOMED-CT) can be exploited and oriented
towards the design of such an ontology. Finally, challenges
concerning mappings between the application and canonical
ontologies are diverse (e.g. variable granularity of the infor-
mation, different types of data, etc.). As stated in section II-C,
extensive work has already been made on this area, so the
deﬁnition of a new approach is out of the scope of this paper.
However, we have presented a novel contribution concerning
mapping issues: the deﬁnition of the notion of path mappings
and their implementation using SWRL rules. Additionally, we
suggest that speciﬁc systems publish voluntarily the integration
mappings between their application ontology and the canon-
ical ontology, so that other systems could beneﬁt from this
knowledge at the time of creating their integration mapping.
ACKNOWLEDGMENT
The work of Idoia Berges is supported by a grant of
the Basque Government (Programa de Formaci´on de Inves-
tigadores del Departamento de Educaci´on, Universidades e
Investigaci´on). This work is also supported by the Spanish
Ministry of Education and Science TIN2010-21387-C02-01.
REFERENCES
[1] V. N. Stroetman(ed.), D. Kalra, P. Lewalle, A. Rector, J. M. Rodrigues,
K. A. Stroetmann, G. Surjan, B. Ustun, M. Virtanen, and P. E. Zanstra,
6http://www.levenshtein.net/
7wordnet.princeton.edu/
8http://www.w3.org/TR/xquery/
9http://www.w3.org/TR/xpath20/
“Semantic Interoperability for Better Health and Safer Healthcare,”
European Commission, Tech. Rep., Jan. 2009.
[2] “openEHR,” 2011, available at http://www.openehr.org.
[3] “ISO 13606-1: Electronic Health Record Communication Part 1: Refer-
ence Model,” 2008.
[4] “HL7-CDA,” 2011, available at http://www.hl7.org.
[5] P. Schloeffel, T. Beale, G. Hayworth, S. Heard, and H. Leslie, “The
relationship between cen 13606, hl7 and openehr,” in Health Informatics
Conference, HIC 2006, Sydney, Australia, 2006.
[6] V. Kashyap and A. P. Sheth, “Semantic and schematic similarities
between database objects: A context based approach,” The Very Large
Databases Journal, vol. 5, no. 4, pp. 276–304, 1996.
[7] “OWL2 Web Ontology Language,” World Wide Web Consortium, 2009,
http://www.w3.org/TR/owl2-overview/.
[8] M. Uschold and M. Gruninger, “Ontologies: Principles, methods and
applications,” Knowledge Engineering Review, vol. 11, pp. 93–136,
1996.
[9] T. Beale and S. Heard, “An Ontology-based Model of Clinical Informa-
tion,” in Proceedings of the 12th World Congress on Health (Medical)
Informatics - Building Sustainable Health, MEDINFO 2007, Brisbane,
Australia, 2007, pp. 760–764.
[10] C.
Wroe,
“Is
semantic
web
technology
ready
for
healthcare?”
in
Paper
presented
at
the
3rd
European
Semantic
Web
Conference,
ESWC’06,
Budva,
Montenegro,
jun
2006,
http://ftp.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-194/paper2.pdf.
[11] J. H. Weber-Jahnke and J. Williams, “The smart internet as a catalyst
for health care reform,” in The Smart Internet - Current Research and
Future Applications, 2010, pp. 27–48.
[12] R. Krummenacher, E. P. B. Simperl, D. Cerizza, E. D. Valle, L. J. B.
Nixon, and D. Foxvog, “Enabling the european patient summary through
triplespaces,” Computer Methods and Programs in Biomedicine, vol. 95,
no. 2-S1, pp. 33–43, 2009.
[13] O. Kilic and A. Dogac, “Achieving Clinical Statement Interoperability
using R-MIM and Archetype-based Semantic Transformations,” IEEE
Transactions on Information Technology in Biomedicine, to appear.,
2009.
[14] V. Bicer, O. Kilic, A. Dogac, and G. B. Laleci, “Archetype-Based
Semantic Interoperability of Web Service Messages in the Health Care
Domain,” Int’l Journal on Semantic Web & Information Systems, vol. 1,
no. 4, pp. 1–22, 2005.
[15] C. Mart´ınez-Costa, M. M. Tortosa, and J. T. Fern´andez-Breis, “An ap-
proach for the semantic interoperability of ISO EN 13606 and openEHR
archetypes,” Journal of Biomedical Informatics, vol. 43, no. 5, pp. 736–
746, 2010.
[16] L. Lezcano, M.- ´A. Sicilia, and C. Rodr´ıguez-Solano, “Integrating rea-
soning and clinical archetypes using owl ontologies and swrl rules,”
Journal of Biomedical Informatics, vol. 44, no. 2, pp. 343–353, 2011.
[17] The
openEHR
Foundation,
“Archetype
Def-
inition
Language,”
2007,
available
at
http://www.openehr.org/releases/1.0.2/architecture/am/adl.pdf.
[18] B. Prados-Suarez, C. Molina, M. Prados, and C. Pe˜na, “On the use
of an ontology to improve the interoperability and accesibility of the
electronical health records (ehr),” in International Workshop on Semantic
Interoperability, IWSI 2011, Rome, Italy, jan 2011, pp. 73–81.
[19] R. Hedayat, “Semantic web technologies in the quest for compatible
distributed health records,” Department of Information Technology,
Uppsala University, White Paper, mar 2010.
[20] L. Gonz´alez, G. Llamb´ıas, and P. Pazos, “Towards an e-health integration
platform to support social security services,” in 6th International Policy
and Research Conference on Social Security, Luxembourg, Luxembourg,
sep 2010.
[21] “SNOMED,” 2011, available at http://www.ihtsdo.org/snomed-ct/.
[22] “LOINC,” 2011, available at http://loinc.org/.
[23] P.-A. Champin, G.-J. Houben, and P. Thiran, “Cross: An OWL wrapper
for reasoning on relational databases,” in ER, ser. Lecture Notes in Com-
puter Science, C. Parent, K.-D. Schewe, V. C. Storey, and B. Thalheim,
Eds., vol. 4801.
Springer, 2007, pp. 502–517.
[24] J. M. Blanco, A. Illarramendi, and A. Go˜ni, “Building a federated rela-
tional database system: An approach using a knowledge-based system,”
Int. J. Cooperative Inf. Syst., vol. 3, no. 4, pp. 415–456, 1994.
[25] J. M. Blanco, A. Go˜ni, and A. Illarramendi, “Mapping among knowl-
edge bases and data repositories: Precise deﬁnition of its syntax and
semantics,” Inf. Syst., vol. 24, no. 4, pp. 275–301, 1999.
[26] J. Euzenat and P. Shvaiko, Ontology matching.
Springer-Verlag, 2007.
[27] “SWRL,” 2011, available at http://www.w3.org/Submission/SWRL/.
[28] S. Muggleton, “Inductive logic programming,” New Generation Com-
puting, vol. 8, no. 4, pp. 295–318, 1991.
"
"Large language models (LLMs) exhibit remarkable mathematical reasoning capabilities, yet their deployment efficiency remains a challenge. We propose a novel framework, Mix Thoughts Distillation (MTD), to enhance reasoning performance while democratizing LLMs. MTD combines Equation-of-Thought Distillation (EoTD) and Chain-of-Thought Distillation (CoTD), leveraging the strengths of each method. EoTD translates mathematical reasoning into equations, harnessing deterministic solvers to mitigate calculation errors. CoTD enriches the dataset with structured language rationales, improving SLM learning. We further introduce Program-of-Thought Distillation (PoTD), distilling reasoning into executable programs for comprehensive MTD data generation. We demonstrate that MTD significantly improves SLM reasoning abilities, achieving state-of-the-art results on mathematical reasoning benchmarks. This work advances the accessibility of advanced reasoning models, enabling broader application in resource-constrained settings.","Large language models (LLMs) have captured significant attention for their impressive language understanding and reasoning abilities. However, deploying these models in practical applications poses challenges due to their size and computational demands. To address this, researchers have explored distilling the reasoning capabilities of LLMs into smaller language models (SLMs), enabling deployment on devices with limited resources. This work introduces a novel framework, Mix Thoughts Distillation (MTD), which leverages diverse reasoning paths to enhance the mathematical reasoning performance of SLMs.","Recent studies have made significant progress in distilling LLM reasoning into SLMs. Magister et al. (2023), Shridhar et al. (2023), Ho et al. (2023), and Fu et al. (2023) have demonstrated effective methods for knowledge distillation, achieving promising results on various reasoning tasks. However, these approaches primarily focus on chain-of-thought (CoT) prompting, where intermediate reasoning steps are generated as a sequence of natural language instructions. While CoT has shown promising results, it is susceptible to calculation errors and may not be suitable for all reasoning tasks.nannannan","At the core of our proposed framework, Mix Thoughts Distillation (MTD), lies the concept of Equation-of-Thought Distillation (EoTD). EoTD translates mathematical reasoning into a system of equations, enabling the use of deterministic solvers to obtain the final answer. This approach addresses the limitations of CoT by eliminating calculation errors and simplifying the reasoning process for SLMs. Additionally, we introduce Program-of-Thought Distillation (PoTD), where reasoning is distilled into executable programs. By combining EoTD, CoT, and PoTD, MTD creates a comprehensive dataset that captures diverse reasoning strategies, enhancing the learning of SLMs.nan","Our experimental results demonstrate the effectiveness of MTD in improving the mathematical reasoning capabilities of SLMs. Across four benchmark datasets, MTD consistently outperforms previous state-of-the-art fine-tuned SLMs. Moreover, MTD enables SLMs to achieve results comparable to large language models, showcasing the potential of our framework in democratizing advanced reasoning capabilities. We also conduct ablation studies to analyze the impact of different reasoning paths and data volumes, revealing the importance of diverse reasoning strategies and sufficient training data in enhancing SLM reasoning performance.","In conclusion, this work presents a significant stride towards making the sophisticated reasoning capabilities of large language models more accessible. By introducing Equation-of-Thought Distillation (EoTD) and Mix Thoughts Distillation (MTD), we have demonstrated that it is possible to distill the essence of mathematical reasoning from LLMs into smaller language models with fewer than one billion parameters. Our EoTD approach effectively captures the reasoning process in an equation-based format, facilitating the fine-tuning of SLMs to better understand and generate mathematical reasoning. The MTD framework further augments this by incorporating datasets with varied reasoning paths, leading to a notable improvement in SLMs’ reasoning performance. Our experiments validate that these methodologies not only preserve but also enhance the reasoning abilities of SLMs, enabling them to achieve state-of-the-art performance on reasoning tasks. This advancement opens the door to wider applications of SLMs in resource-constrained environments, bridging the gap between the need for powerful reasoning models and the limitations imposed by computational resources.",Improving Small Language Models' Mathematical Reasoning via Mix Thoughts Distillation,"Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang","Improving Small Language Models’ Mathematical Reasoning via Mix
Thoughts Distillation
Xunyu Zhu1,2, Jian Li1,2*, Yong Liu3, Can Ma1,2, Weiping Wang1,2
1Institute of Information Engineering, Chinese Academy of Sciences
2School of Cyber Security, University of Chinese Academy of Sciences
3Gaoling School of Artificial Intelligence, Renmin University of China
{zhuxunyu, lijian9026, macan, wangweiping}@iie.ac.cn, liuyonggsai@ruc.edu.cn
Abstract
This work addresses the challenge of democ-
ratizing advanced Large Language Models
(LLMs) by compressing their mathematical
reasoning capabilities into sub-billion param-
eter Small Language Models (SLMs) with-
out compromising performance.
We intro-
duce Equation-of-Thought Distillation (EoTD),
a novel technique that encapsulates the rea-
soning process into equation-based represen-
tations to construct an EoTD dataset for fine-
tuning SLMs. Additionally, we propose the
Mix Thoughts Distillation (MTD) framework
to enhance the reasoning performance of SLMs.
This involves creating a reasoning dataset with
multiple thought processes and using it for fine-
tuning. Our experimental findings demonstrate
that EoTD significantly boosts the reasoning
abilities of SLMs, while MTD enables these
models to achieve state-of-the-art reasoning
performance.1
1
Introduction
Large language models (LLMs) like those built
on Transformer architectures mark a leap forward
in natural language processing.
These models,
including prominent ones such as LLaMA (Tou-
vron et al., 2023a), GPT-4 (OpenAI, 2023), and
PaLM (Chowdhery et al., 2023), boast parameter
counts in the hundreds of billions. Trained on vast
text datasets, they demonstrate remarkable profi-
ciency in a wide array of downstream tasks.
Recent studies (Chen et al., 2023; Wang et al.,
2023b,a; Liu et al., 2023) have honed the reasoning
abilities of LLMs through chain-of-thought (CoT)
prompting, which generates intermediate steps to
solve complex problems. However, the deploy-
ment of such models is challenging due to their
size and computational requirements. For exam-
ple, the GPT-175B model (Brown et al., 2020a)
*Corresponding author
1CodeT5+-770m-py fine-tuned by MTD is available at
https://huggingface.co/zhuxunyu/mtd-codet5p-770m-py
Question: Peter plans to go to the movies this week. He always gets a ticket for 
$7 and popcorn for $7. If he has 42 dollars for the week, how many times can he 
go to the movies?
CoTD
Peter spends $7 on a ticket and $7 on popcorn, so he spends a total of $7 + $7 = $12 on tickets 
and popcorn. He has 42 dollars for the week, so he can go to the movies 42 / $12 = 4 times. The 
answer is 4.
PoTD
total_money = 42
ticket_price = 7
popcorn_price = 7
total_revenue = total_money - ticket_price - popcorn_price
times_can_go = total_revenue / total_money_per_movie
ans = times_can_go
EoTD
ticket_price = 7
popcorn_price = 7
total_money = 42
times_go_to_movies = x
ticket_price * times_go_to_movies + popcorn_price * times_go_to_movies = total_money
ans = times_go_to_movies
Calculation Error
Unknown Variable
❎
❎
✅
Answer: 3.0
None
3.0
Figure 1: A particular case where SLMs under CoTD
and PoTD fail to generate the correct answer, but SLMs
under EoTD successfully solve the question.
necessitates at least 320GB of FP16 storage and
multiple A100 GPUs with 80GB of memory each
for efficient inference.
Recent work (Magister et al., 2023; Shridhar
et al., 2023; Ho et al., 2023; Fu et al., 2023) inves-
tigates distilling LLM reasoning into SLMs (un-
der 1B parameters) for broader deployment. This
involves using LLMs to create enriched datasets
with detailed reasoning paths, which then fine-tune
SLMs, endowing them with advanced reasoning
abilities. These distilled SLMs have shown com-
parable performance to LLMs in common sense
and symbolic reasoning tasks. Yet, a notable gap
persists in mathematical problem-solving.
In our research, we introduce Equation-of-
Thought Distillation (EoTD), a framework de-
signed to enhance the mathematical reasoning of
SLMs. EoTD prompts LLMs to generate equa-
tions in response to questions, which are then pro-
cessed by an equation solver. Equations yielding
incorrect solutions are discarded. We compile the
EoTD dataset using this method and employ it to
fine-tune SLMs, thereby boosting their reasoning
arXiv:2401.11864v1  [cs.CL]  22 Jan 2024
capabilities. EoTD outperforms Chain-of-Thought
Distillation (CoTD) by offloading calculations to
an external solver, reducing computational errors
(see Figure 1). Unlike Program-of-Thought Dis-
tillation (PoTD), which requires defined variables,
EoTD assigns solutions to unknown variables, sim-
plifying the problem-solving process for SLMs.
Additionally, we introduce Mix Thoughts Distil-
lation (MTD) to further enhance SLM reasoning.
MTD merges the CoTD, PoTD, and EoTD datasets
into a comprehensive MTD dataset for fine-tuning
SLMs. The diversity of reasoning strategies within
the MTD dataset enriches the reasoning knowledge,
contributing to its effectiveness.
We assessed EoTD and MTD across CodeT5
models from Small (0.06B) to Large (0.77B) on
four mathematical reasoning datasets. Results in-
dicate EoTD significantly boosts SLM reasoning
abilities, while MTD enables SLMs to reach state-
of-the-art (SOTA) performance. For instance, with
EoTD, CodeT5-Small reached 18.87% accuracy
on GSM8K, and MTD elevated CodeT5-Large to
42.45% accuracy. Ablation studies confirm that the
volume and variety of reasoning paths in MTD cor-
relate with improved SLM reasoning performance.
2
Related Work
2.1
Large Language Models (LLMs)
Building on the insights from Wei et al. (2022b);
Huang and Chang (2023); Wei et al. (2022a), our
research investigates the distillation of complex rea-
soning pathways from LLMs, such as GPT-4 (Ope-
nAI, 2023) and PaLM-2 (Anil et al., 2023), into
more manageable models. These LLMs, with their
vast parameter counts exceeding 100 billion, have
demonstrated a profound capacity for navigating
intricate reasoning tasks. They can independently
construct a series of logical steps leading to a con-
clusion, particularly when provided with structured
reasoning examples or when guided by prompts
that encourage stepwise thinking. Our work aims
to capture this advanced reasoning in smaller mod-
els, thus reducing the computational overhead and
making such capabilities more widely accessible.
The formidable reasoning skills of LLMs on
complex tasks are offset by their extensive size
and computational demands. Deploying models
like GPT-175B (Brown et al., 2020b) for inference,
for example, demands at least 320GB of storage
for FP16 parameters and no fewer than five A100
GPUs with 80GB of memory each for efficient
functioning. These requirements pose significant
challenges, particularly for resource-limited set-
tings. Our research addresses these issues by distill-
ing the reasoning capabilities of LLMs into smaller,
more computationally feasible models.
Our work addresses these limitations by focusing
on the distillation of reasoning abilities from LLMs
into smaller models. This process aims to retain
the advanced reasoning capabilities of LLMs while
significantly reducing resource requirements. Con-
sequently, our approach facilitates the democrati-
zation of cutting-edge NLP technologies, enabling
the use of powerful reasoning tools in settings with
constrained computational resources.
2.2
Mathematical Reasoning
Mathematical Reasoning tasks, highlighted by
benchmarks like GSM8K (Cobbe et al., 2021) and
SVAMP (Patel et al., 2021), pose a significant chal-
lenge for LLMs. To improve LLMs’ performance
in this domain, researchers have pinpointed two
main strategies.
Chain-of-Thought Reasoning
LLMs’ reason-
ing can be enhanced by prompting them to ar-
ticulate intermediate steps towards a solution, as
demonstrated by Wei et al. (2022b). This insight
has led to various advancements(Chen et al., 2023;
Wang et al., 2023b,a; Liu et al., 2023) that refine
reasoning paths. For example, Chen et al. (2023)
prompt LLMs to generate executable code, Wang
et al. (2023b) use multiple reasoning paths with a
voting mechanism for the correct answer, Wang
et al. (2023a) have LLMs create a plan before rea-
soning, and Liu et al. (2023) employ diverse rea-
soning prompts for problem-solving. Building on
these methods, our work introduces Equation-of-
Thought Distillation (EoTD) to further improve
SLMs’ mathematical reasoning.
Finetuning-based Reasoning refines LLMs like
Llama2 (Touvron et al., 2023b), Qwen (Bai et al.,
2023), and Baichuan2 (Yang et al., 2023) by draw-
ing on techniques from advanced models such as
GPT-4 (OpenAI, 2023) and PaLM-2 (Anil et al.,
2023). Notably, Yuan et al. (2023) employ Re-
jection Sampling Fine-Tuning (RFT) to enhance
LLMs’ mathematical reasoning, while Wizard-
Math(Luo et al., 2023) uses Reinforcement Learn-
ing from Evolved Instructions Feedback (RLEIF)
to improve LLaMA-2’s reasoning abilities. MAm-
moTH (Yue et al., 2023) combines CoT and PoT
rationales for more effective instruction-tuning of
LLMs in math problem-solving. Despite their ef-
Question: Natalia sold clips to 48 of her 
friends in April, and then she sold half as many 
clips in May. How many clips did Natalia sell 
altogether in April and May?
System of linear equations: (Do not simplify)
april_sales = x
may_sales = april_sales / 2
april_sales = 48
ans = april_sales + may_sales
Question: Jack is stranded on a desert island. 
He wants some salt to season his fish. He 
collects 2 liters of seawater in an old bucket. If 
the water is 20% salt, how many ml of salt will 
Jack get when all the water evaporates?
System of linear equations: (Do not simplify)
LLMs
amount_of_salt = x
amount_of_water = 2000
salt_percentage = 20 / 100
amount_of_salt = amount_of_water * salt_percentage
ans = amount_of_salt
Data Filtering
Saving
 Generating Correct Answer
Equations Solver
Dropping
 Can’t Solve
 Generating Wrong Answer
Figure 2: Detailed data generation of our framework. Firstly, we manually construct some contextualized
examples, and combine these contextualized examples, the question, and the prompt ""System of linear equations:
(Do not simplify)"" to prompt LLMs to generate EoT based on the question. This equations system is sent to a
deterministic equation solver, if there are compile errors or if it produces wrong answer, we will drop the EoT.
Finally, we get a high-quality reasoning dataset.
fectiveness, the large size of these LLMs limits
their deployment efficiency. Our research explores
fine-tuning methods that balance performance with
deployment practicality, aiming to democratize ad-
vanced reasoning across diverse computational set-
tings.
2.3
Knowledge Distillation
Knowledge Distillation optimizes LLMs for prac-
tical use by transferring knowledge from larger
models to smaller, efficient ones (Zhu et al., 2023).
Research (Magister et al., 2023; Shridhar et al.,
2023; Ho et al., 2023; Fu et al., 2023) has aimed
to endow compact models like T5 (Raffel et al.,
2020) and GPT-2 (Radford et al., 2019) with the
advanced reasoning of LLMs such as GPT-4 (Ope-
nAI, 2023) and PaLM-2 (Anil et al., 2023). For
example, Ho et al. (2023) fine-tune student mod-
els using the most accurate reasoning paths from
LLMs. Shridhar et al. (2023) train a dual-model
system on sub-questions and solutions, while Fu
et al. (2023) suggest scaling down general com-
petencies of smaller models to boost task-specific
performance. Our work presents a novel distilla-
tion approach that encodes mathematical reasoning
as equations and introduces Mix Thoughts Distil-
lation, combining CoT, EoT, and PoT to create
a diverse fine-tuning dataset. Our results demon-
strate state-of-the-art performance in mathematical
reasoning.
3
Methodology
In this work, we introduce a novel distilla-
tion framework for mathematical reasoning tasks,
termed Equation-of-Thought Distillation (EoTD),
which translates mathematical reasoning into equa-
tions for fine-tuning SLMs. EoTD stands out in rea-
soning tasks involving unknown variables, teach-
ing SLMs to frame problems as systems of linear
equations. Unlike Chain-of-Thought Distillation
(CoTD), which can lead to calculation errors, and
Program-of-Thought Distillation (PoTD), which
requires all variables to be predefined, EoTD gen-
erates solvable equation systems and employs a
deterministic solver, mitigating calculation inaccu-
racies. As shown in Figure 1, while CoTD guides
SLMs through correct reasoning steps that may
still result in errors, and PoTD models questions
as executable programs limited by variable defi-
nition, EoTD consistently formulates and solves
equations. EoTD’s approach to handling unknowns
complements PoTD’s precision, combining their
strengths to enhance SLMs’ mathematical reason-
ing. We now detail Equation-of-Thought Distilla-
tion (EoTD).
3.1
Equation-of-Thought Distillation
3.1.1
Data Generation from LLMs
Our EoTD framework commences by creating a
dataset from LLMs, which precedes SLM fine-
tuning. As illustrated in Figure 2, we employ in-
context learning (Dong et al., 2023; Min et al.,
2022; Rubin et al., 2022) to prompt LLMs for
reasoning data. Within a mathematical dataset D,
each entry (x, y) pairs a question x with its answer
y. We select k samples {(x1, y1), . . . , (xk, yk)}
from D and manually craft rationales e in EoT
format.
These form contextualized instances
{(x1, e1, y1), . . . , (xk, ek, yk)}, compiled into a
demonstration set DD. We then prompt LLMs with
a question and the instruction ""System of linear
equations: (Do not simplify)"" to generate ratio-
nales. The EoT generation is formalized as:
ei = fM(xi, DD),
where M denotes the LLM, f the decoding func-
tion, and i the index in D. This yields the EoT
dataset DE, composed of triplets (x, e, y).
Data Filtering—After LLMs generate EoT, we
validate each equation system using an external
Equation Solver to ensure the accuracy of our initial
dataset DE. Any equation system that cannot be
solved or produces an incorrect result is excluded.
This rigorous filtering process removes errors, en-
hancing the dataset’s quality. Consequently, this
refinement directly improves the performance of
fine-tuned SLMs by providing cleaner and more
reliable training data.
3.1.2
Fine-tuning SLMs
After assembling the reasoning dataset DE, we fine-
tune SLMs on it. For each training instance (x, e, y)
from DE, we prepend the prompt pe ""System of
linear equations: (Do not simplify)"" to the question
x and employ instruction tuning to guide the SLM
in generating the corresponding equations. The
fine-tuning loss function is:
L = −
N
X
i=1
T
X
t=1
log P(ei
t | ei
<t, xi, pe),
where N is the number of examples in DE, pe is
the prompt, and e:T is the sequence of equations.
Post fine-tuning, the SLM can generate equations
for complex questions, which are then solved by an
external Equation Solver to obtain the final answer.
3.2
Mix Thoughts Distillation
Recognizing that Equation-of-Thought (EoT),
Chain-of-Thought (CoT), and Program-of-Thought
(PoT) encapsulate mathematical reasoning knowl-
edge, we observe that CoT’s unstructured language
format poses challenges for SLMs in learning math-
ematical reasoning, often leading to calculation
errors that degrade performance. Conversely, the
structured nature of EoT and PoT facilitates SLM
learning, with the added benefit of external solvers
or interpreters mitigating calculation errors. How-
ever, PoT’s requirement for predefined variables
can be restrictive. LLMs like gpt-3.5-turbo, when
employing EoT, underperform in solving mathe-
matical questions, which can translate to subopti-
mal SLM performance under EoTD. Each thought
process has its merits and limitations, which are
largely complementary. Moreover, the diversity of
thoughts captures a broader spectrum of mathemat-
ical reasoning knowledge, enhancing the efficiency
of SLMs in achieving superior reasoning perfor-
mance. To harness these synergies, we introduce
Mix Thought Distillation (MTD), an amalgamation
of CoTD, EoTD, and PoTD, aimed at bolstering
the mathematical reasoning capabilities of SLMs.
MTD leverages the strengths of each thought pro-
cess while compensating for their weaknesses, re-
sulting in a more robust and comprehensive ap-
proach to mathematical reasoning. We now detail
the MTD framework and its implementation.
In parallel with EoTD, we construct a Program-
of-Thought (PoT) dataset Dp, with each entry for-
matted as a triplet (x, p, y). The PoT data gener-
ation mirrors the EoT process described in Sec-
tion 3.1.1. We utilize in-context learning to prompt
LLMs to generate programmatic solutions for given
questions. These programs are then executed by
an external Python interpreter to obtain the final
answers. Programs that fail to compile or yield
incorrect answers are discarded, ensuring the PoT
dataset Dp is of high quality. Similarly, we compile
a Chain-of-Thought (CoT) dataset Dc, with each
instance also structured as a triplet (x, c, y). The
comprehensive methodologies for CoTD and PoTD
are delineated in Appendix A and B, respectively.
As depicted in Figure 3, we amalgamate the
EoT, CoT, and PoT datasets to form the new MTD
dataset Dm. For EoT entries, we append the prompt
pe ""System of linear equations: (Do not simplify)""
to each question. For PoT entries, we add the
prompt pr ""Let’s break down the code step by
step,"" and for CoT entries, we include the prompt
pc ""Let’s think step by step."" These prompts are
designed to guide the generation of thoughts in
their respective formats. The combined datasets,
now enriched with diverse thoughts and instruc-
tions, constitute the MTD dataset. We then apply
instruction tuning to fine-tune SLMs on Dm. The
Question: Jack is stranded on a desert 
island. He wants some salt to season 
his fish. He collects 2 liters of 
seawater in an old bucket. If the water 
is 20% salt, how many ml of salt will 
Jack get when all the water evaporates?
System of linear equations: (Do not 
simplify)
amount_of_salt = x
amount_of_water = 2000
salt_percentage = 20 / 100
amount_of_salt = amount_of_water * salt_percentage
ans = amount_of_salt
Question: Weng earns $12 an hour for 
babysitting. Yesterday, she just did 50 
minutes of babysitting. How much did 
she earn?
Let’s break down the code step by 
step
minutes_babysitting = 50
hourly_rate = 12
hours_babysitting = minutes_babysitting / 60
earnings = hourly_rate * hours_babysitting
ans = earnings
SLMs
🤖
Question: James and John combine 
their friends lists.  James has 75 
friends.  John has 3 times as many 
friends as James.  They share 25 
friends.  How many people are on the 
combined list?
Let’s think step by step
John has 3 times as many friends as James, so he has 
3*75 = <<3*75=225>>225 friends.\nTogether, James 
and John have 75+225 = <<75+225=300>>300 
friends.\nThey share 25 friends, so the combined list 
has 300-25 = <<300-25=275>>275 people.\nThe
answer is 275
Figure 3: Detailed overview of Mix Thought Distillation. Firstly, we combine a CoT dataset, a PoT dataset and
a EoT dataset to build a new MTD dataset. The MTD dataset has diverse thoughts and prompts. Then, we use
the MTD dataset to fine-tune SLMs. After fine-tuning, we use the prompt ""System of linear equations: (Do not
simplify)"" to instruct SLMs to generate equations, the prompt ""Let’s break down the code step by step"" to instruct
SLMs to generate programs, and the prompt ""Let’s think step by step"" to instruct SLMs to generate chains to solve
questions.
loss function for this fine-tuning process is:
L = −
N
X
i=1
T
X
t=1
log P(ri
t | ri
<t, xi),
where x is the input comprising both the question
and its associated prompt, and r represents the gen-
erated thoughts conditioned on the input. This
approach aims to enhance the SLMs’ ability to
process and generate a variety of thought patterns,
thereby improving their mathematical reasoning
performance.
Post fine-tuning, the SLMs are primed to gener-
ate different types of reasoning outputs based on
the given prompts. When presented with a question,
the prompt ""System of linear equations: (Do not
simplify)"" elicits equation generation, ""Let’s break
down the code step by step"" induces program gen-
eration, and ""Let’s think step by step"" prompts the
creation of chains of thought. The SLMs’ outputs
are then used to derive the final answers. As evi-
denced in Section 4.5, PoT outperforms the others
in reasoning, with EoT in second place, and CoT
trailing. Consequently, the PoT-generated answer
is initially considered as the final answer. If the
PoT-generated program fails to compile correctly,
the SLMs’ EoT-generated answer is then evaluated.
Should the EoT-generated equations be unsolvable,
the CoT-generated result is finally considered. This
hierarchical approach to reasoning ensures the most
reliable answer is selected. The detailed reasoning
process of MTD is illustrated in Figure 9.
4
Experiments
4.1
Dataset
Our
training
dataset
is
derived
from
the
GSM8K (Cobbe et al., 2021) training set.
We construct separate EoT, PoT, and CoT datasets,
which are then amalgamated to form the MTD
dataset.
This mixed dataset features a variety
of prompts and thought processes, on which we
fine-tune SLMs.
The mathematical reasoning
capabilities of the SLMs are evaluated using the
GSM8K (Cobbe et al., 2021) test set, as well as
ASDiv (Miao et al., 2020), SVAMP (Patel et al.,
2021), and MultiArith (Roy and Roth, 2015).
4.2
Implementation
We employ ChatGPT (gpt-3.5-turbo) as the teacher
LLM to construct our training dataset and utilize
CodeT5 models—Small (60M), Base (220M), and
Large (770M) (Wang et al., 2021)—as student
SLMs. We manually create 8 examples to guide
ChatGPT in generating 4 reasoning paths for each
dataset (EoT, PoT, and CoT). Fine-tuning of all
student SLMs is conducted using the Huggingface
library (Wolf et al., 2020) on an NVIDIA 3090
GPU with 24 GB RAM. The learning rate for fine-
tuning is set to 5e-4, with a total of 10 fine-tuning
epochs.
4.3
Baselines
Proprietary
Large
Language
Models
We
present CoT prompting results from an array of
Models
#Params
GSM8K
ASDiv
SVAMP
MultiArith
AVG
Proprietary Large Language Models
GPT-4 (OpenAI, 2023)
-
92.0
91.3
93.1
-
92.13
ChatGPT
-
80.8
87.3
83.0
-
83.7
Claude-2 (Anthropic, 2023)
-
85.2
-
-
-
85.2
PaLM-2 (Anil et al., 2023)
540B
80.7
-
-
-
80.7
Open-Source Large Language Models
Llama-2 (Touvron et al., 2023b)
7B
13.3
50.7
38.0
-
34
CodeLLaMA (Rozière et al., 2023)
7B
34.0
61.4
59.0
-
51.46
Platypus-2 (Lee et al., 2023)
7B
14.4
47.9
36.7
-
33
WizardMath (Luo et al., 2023)
7B
54.9
59.1
57.3
-
57.1
TORA (Gou et al., 2023)
7B
68.8
73.9
68.2
-
70.3
Fine-tuned Small Language Models
Ho et al. (2023)
0.3B
3.11
-
-
-
3.11
Fu et al. (2023)
0.76B
20.2
23.8
20.4
38.5
25.72
Fu et al. (2023)
0.25B
13.4
20.9
14.2
29.7
19.55
Shridhar et al. (2023)
0.77B
17.89
-
18.14
-
18.01
Our fine-tuned Small Language Models
CodeT5-Small
0.06B
1.1
0.3
0.2
0.6
0.55
(-) EoTD
18.87
29.24
31.5
24.66
26.06
(-) MTD
33.58
49.09
42.8
67.83
48.14
CodeT5-Base
0.22B
0.8
0.2
0.0
0.0
0.25
(-) EoTD
27.21
38.26
38.8
41.66
36.48
(-) MTD
40.63
51.66
48.8
81
55.52
CodeT5-Large
0.77B
2.9
3.6
0.0
0.0
1.62
(-) EoTD
33.13
44.03
46.1
57.33
45.14
(-) MTD
42.45
52.81
49.59
85.5
57.58
Table 1: Overall test set performance. We use EoTD and MTD to fine-tune SLMs, and evaluate them on four
mathematical reasoning datasets, i.e., GSM8K, ASDiv, SVAMP, and MultiArith. The experiment results show that
EoTD can effectively improve SLMs’ reasoning performance, and MTD makes SLMs achieve SOTA reasoning
performance.
SoTA LLMs, such as OpenAI’s GPT-4, ChatGPT
(gpt-3.5-turbo), Google’s PaLM-2, and Anthropic’s
Claude-2.
Open-Source Large Language Models
We
present mathematical reasoning performance of
Llama-2-7B, CodeLLaMA-7B, and their fine-tuned
versions, such as Platpus-2, WizardMath, TORA.
Fine-tuned Small Language Models
We
present some works that try to fine-tune SLMs un-
der 1B, such as Ho et al. (2023) fine-tune GPT-3-
ada, Fu et al. (2023) fine-tune FlanT5, and Shrid-
har et al. (2023) fine-tune GPT-2.
4.4
Main Results
Table 1 showcases our method’s performance on
four mathematical datasets, revealing key insights:
(1) EoTD significantly enhances the mathematical
reasoning of SLMs, with absolute improvements
ranging from 25.51% to 43.52% across tasks. (2)
MTD outperforms previous state-of-the-art fine-
tuned SLMs at all scales, with absolute improve-
ments between 48.14% and 57.58% across tasks.
(3) MTD’s accuracy is approximately 20% higher
than that of EoTD, underscoring the advantage of
diverse prompts and thoughts in bolstering SLMs’
reasoning capabilities. (4) Model size is crucial for
reasoning distillation efficacy in SLMs; larger mod-
els assimilate more reasoning knowledge, trans-
lating to superior performance. For instance, un-
der MTD, CodeT5-Small attains 33.58% accuracy
on GSM8K, CodeT5-Base reaches 40.63%, and
CodeT5-Large achieves 42.45%.
4.5
MTD Enhances Thoughts Distillation
In this subsection, we examine if MTD can en-
hance the reasoning abilities of SLMs across dif-
ferent thought processes. Initially, we generate
CoTD, PoTD, and EoTD datasets from the GSM8K
training set, each containing one reasoning path
per question. These datasets are then merged to
create the MTD dataset. Subsequently, we fine-
tune SLMs, including CodeT5-Small, Base, and
Large, using the MTD dataset. The reasoning per-
formance of these SLMs is assessed on the GSM8K
test dataset, as well as ASDiv, SVAMP, and Multi-
Arith.
Figure 4 illustrates the outcomes of our exper-
iments, from which we deduce that: (1) MTD
Small
Base
Large
Models
5
10
15
20
25
30
Acc
GSM8K
Small
Base
Large
Models
10
15
20
25
30
35
40
45
Acc
ASDiv
Small
Base
Large
Models
5
10
15
20
25
30
35
40
45
Acc
SVAMP
Small
Base
Large
Models
10
20
30
40
50
60
70
Acc
MultiArith
CoTD
PoTD
EoTD
MTD-CoT
MTD-PoT
MTD-EoT
Figure 4: Effect of MTD. We fine-tune SLMs on the
MTD dataset, the CoTD dataset, the PoTD dataset, and
the EoTD dataset to study the effect of MTD. The exper-
iment results shows that MTD can improve reasoning
performance of SLMs under different thoughts.
enhances SLMs’ reasoning performance. SLMs
fine-tuned with MTD outperform those trained on
CoTD, PoTD, and EoTD in CoT, PoT, and EoT
tasks, respectively. For instance, CodeT5-Base
achieves a 26.38% PoT accuracy on the GSM8K
test dataset under MTD, surpassing the 22.44%
PoT accuracy under PoTD. Similarly, CodeT5-
Small reaches a 24.09% EoT accuracy on SVAMP
with MTD, compared to 17.59% with EoTD. (2)
SLMs gain more valuable reasoning knowledge
from PoTD, reflected in superior reasoning perfor-
mance, with EoTD and CoTD following in that
order. This pattern persists with MTD, leading to
a hierarchical approach where the PoT-generated
answer is preferred, followed by EoT if PoT fails,
and CoT as a last resort. (3) Scaling up the size
of student models consistently improves the per-
formance across CoTD, PoTD, EoTD, and MTD,
indicating that larger models benefit more from our
method.
4.6
The Effect of Different Thoughts in MTD
In this subsection, we investigate the impact of
different reasoning paths within the MTD frame-
work. We begin by fine-tuning CodeT5-Base on in-
dividual datasets—CoTD, PoTD, and EoTD—each
containing a unique reasoning path per question.
We then extend our fine-tuning to combinations
of these datasets: CoTD with PoTD, CoTD with
EoTD, PoTD with EoTD, and the full MTD which
integrates CoTD, PoTD, and EoTD. This approach
allows us to assess the influence of each reasoning
Methods
GSM8K
ASDiv
SVAMP
MultiArith
CoTD
8.11
11.59
8.6
15.66
PoTD
22.44
37.16
31.4
46.5
EoTD
15.01
23.13
25.90
16.83
CoTD + PoTD
CoT
7.96
13.12
11.3
14.16
PoT
25.01
39.59
35.6
53.16
CoTD + EoTD
CoT
8.49
13.02
8.79
15.16
EoT
17.13
26.62
28.19
20.5
PoTD + EoTD
PoT
26.23
39.59
34.8
59
EoT
18.65
29.10
31.5
25.5
CoTD + PoTD + EoTD
CoT
9.4
14.16
9
18.5
PoT
26.38
42.6
40.9
57.33
EoT
20.84
31.87
34
29.16
Table 2: Effect of Thoughts in MTD. We fine-tune
CodeT5-Base on MTD datasets which own different
thoughts in it to analyse the effect of thoughts in MTD.
The experiment results show that when the MTD dataset
own more thoughts, SLMs fine-tuned on it can achieve
better reasoning performance.
path and their synergistic effects on the model’s
performance.
Table 2 presents the results of our experiments,
from which we observe that: (1) SLMs exhibit
improved reasoning performance with an increas-
ing number of thought processes incorporated into
MTD. For instance, CodeT5-Base, when trained
on CoTD and PoTD combined, achieves a 25.01%
PoT accuracy on the GSM8K test dataset, which
further increases to 26.38% when trained on the
full combination of CoTD, PoTD, and EoTD. (2)
SLMs trained on the PoTD and EoTD combination
outperform those trained on either the CoTD and
PoTD or the CoTD and EoTD combinations. This
suggests that the structured nature of PoTD and
EoTD contributes to SLMs’ ability to assimilate
more valuable knowledge effectively.
4.7
More Data Improves Reasoning Ability of
SLMs
In this subsection, we explore the influence of
dataset size on the reasoning capabilities of SLMs.
We create subsets of varying sizes from our reason-
ing datasets and utilize these to fine-tune CodeT5-
Base. This analysis helps determine the relation-
ship between the amount of data and the model’s
performance in reasoning tasks.
Figure 5 depicts the outcomes of our experi-
ments, indicating that larger datasets enhance the
0.5K
1K
1.5K
2K
Data Size
2
4
6
8
10
12
14
16
Acc
GSM8K
0.5K
1K
1.5K
2K
Data Size
5
10
15
20
25
30
Acc
ASDiv
0.5K
1K
1.5K
2K
Data Size
5
10
15
20
25
30
Acc
SVAMP
0.5K
1K
1.5K
2K
Data Size
5
10
15
20
25
30
Acc
MultiArith
CoTD
PoTD
EoTD
MTD-CoT
MTD-PoT
MTD-EoT
Figure 5: Effect of Data Scale. We fine-tune CodeT5-
Base under different data sizes to evaluate the effect of
data scale. The experiment results show that larger data
size make SLMs better reasoning performance.
reasoning performance of SLMs. For instance,
CodeT5-Base, when fine-tuned on a 1K MTD
dataset, attains a 24.42% PoT accuracy on the AS-
Div test set, whereas training on a smaller 0.5K
MTD dataset results in a lower PoT accuracy of
17.17% on the same test set. This trend underscores
the positive correlation between dataset size and
the model’s reasoning proficiency.
4.8
Diverse Reasoning Paths Improve SLMs’
reasoning performance
1
2
3
4
The Number of Reasoning Paths
10
15
20
25
30
35
40
Acc
GSM8K
1
2
3
4
The Number of Reasoning Paths
10
15
20
25
30
35
40
45
50
Acc
ASDiv
1
2
3
4
The Number of Reasoning Paths
10
15
20
25
30
35
40
45
Acc
SVAMP
1
2
3
4
The Number of Reasoning Paths
20
30
40
50
60
70
80
Acc
MultiArith
CoTD
PoTD
EoTD
MTD-CoT
MTD-PoT
MTD-EoT
Figure 6: Effect of Reasoning Paths. We fine-tune
CodeT5-Base with different reasoning paths to analyse
the effect of reasoning paths. The experiment results
shows that diverse reasoning paths can improve SLMs’
reasoning performance.
In this subsection, we fine-tune CodeT5-Base
on our reasoning datasets, which are differentiated
by the number of reasoning paths they contain, to
analyze the effect of reasoning path multiplicity on
the reasoning performance of SLMs. This examina-
tion aims to discern how the diversity and quantity
of reasoning paths in training data influence the
model’s ability to perform reasoning tasks.
Figure 6 presents the results of our experiments,
which demonstrate that a variety of reasoning paths
can bolster the reasoning performance of SLMs.
For instance, CodeT5-Base, when trained on an
MTD dataset featuring four reasoning paths, attains
a 38.89% PoT accuracy on the GSM8K test dataset
and a 44.13% EoT accuracy on ASDiv. In contrast,
CodeT5-Base trained on an MTD dataset with only
one reasoning path achieves the same 38.89% PoT
accuracy on GSM8K but a lower 31.87% EoT ac-
curacy on ASDiv. This suggests that the inclusion
of multiple reasoning paths in training data can sig-
nificantly enhance the model’s performance, partic-
ularly in tasks requiring explanation generation.
5
Conclusion
In conclusion, our research presents a significant
stride towards making the sophisticated reasoning
capabilities of Large Language Models (LLMs)
more accessible.
By introducing Equation-of-
Thought Distillation (EoTD) and Mix Thoughts
Distillation (MTD), we have demonstrated that it
is possible to distill the essence of mathematical
reasoning from LLMs into Small Language Mod-
els (SLMs) with fewer than one billion parameters.
Our EoTD approach effectively captures the rea-
soning process in an equation-based format, facili-
tating the fine-tuning of SLMs to better understand
and generate mathematical reasoning. The MTD
framework further augments this by incorporating
datasets with varied reasoning paths, leading to a
notable improvement in SLMs’ reasoning perfor-
mance.
Our experiments validate that these methodolo-
gies not only preserve but also enhance the reason-
ing abilities of SLMs, enabling them to achieve
state-of-the-art performance on reasoning tasks.
This advancement opens the door to wider appli-
cations of SLMs in resource-constrained environ-
ments, bridging the gap between the need for pow-
erful reasoning models and the limitations imposed
by computational resources. Future work will fo-
cus on refining these distillation techniques and
exploring their applicability to other domains of
reasoning beyond mathematics, with the aim of
broadening the accessibility and utility of SLMs
across various fields.
References
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, Eric Chu, Jonathan H. Clark, Laurent El
Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau-
rav Mishra, Erica Moreira, Mark Omernick, Kevin
Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,
Yuanzhong Xu, Yujing Zhang, Gustavo Hernández
Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,
Jan A. Botha, James Bradbury, Siddhartha Brahma,
Kevin Brooks, Michele Catasta, Yong Cheng, Colin
Cherry, Christopher A. Choquette-Choo, Aakanksha
Chowdhery, Clément Crepy, Shachi Dave, Mostafa
Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,
Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxi-
aoyu Feng, Vlad Fienber, Markus Freitag, Xavier
Garcia, Sebastian Gehrmann, Lucas Gonzalez, and
et al. 2023.
Palm 2 technical report.
CoRR,
abs/2305.10403.
Anthropic. 2023. Model card and evaluations for claude
models. Anthropic blog.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong
Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang
Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi
Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,
Yichang Zhang, Zhenru Zhang, Chang Zhou, Jin-
gren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.
Qwen technical report. CoRR, abs/2309.16609.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020a.
Language models are few-shot learners.
In Ad-
vances in Neural Information Processing Systems,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020b. Language models are few-shot learn-
ers. CoRR, abs/2005.14165.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W. Cohen. 2023.
Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. Transactions on
Machine Learning Research.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2023. Palm: Scaling language mod-
eling with pathways. J. Mach. Learn. Res., 24:240:1–
240:113.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. CoRR, abs/2110.14168.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong
Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and
Zhifang Sui. 2023. A survey for in-context learning.
CoRR, abs/2301.00234.
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and
Tushar Khot. 2023. Specializing smaller language
models towards multi-step reasoning. In Interna-
tional Conference on Machine Learning, ICML 2023,
23-29 July 2023, Honolulu, Hawaii, USA, volume
202 of Proceedings of Machine Learning Research,
pages 10421–10430. PMLR.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,
Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu
Chen. 2023.
Tora: A tool-integrated reasoning
agent for mathematical problem solving.
CoRR,
abs/2309.17452.
Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023.
Large language models are reasoning teachers. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 14852–14882, Toronto, Canada.
Association for Computational Linguistics.
Jie Huang and Kevin Chen-Chuan Chang. 2023. To-
wards reasoning in large language models: A survey.
In Findings of the Association for Computational
Linguistics: ACL 2023, pages 1049–1065, Toronto,
Canada. Association for Computational Linguistics.
Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2023.
Platypus: Quick, cheap, and powerful refinement of
llms. CoRR, abs/2308.07317.
Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun
Hu, Yue Zhang, Xipeng Qiu, and Zheng Zhang. 2023.
Plan, verify and switch: Integrated reasoning with
diverse X-of-thoughts. In Proceedings of the 2023
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2807–2822, Singapore. As-
sociation for Computational Linguistics.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-
guang Lou, Chongyang Tao, Xiubo Geng, Qingwei
Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-
ardmath: Empowering mathematical reasoning for
large language models via reinforced evol-instruct.
CoRR, abs/2308.09583.
Lucie Charlotte Magister, Jonathan Mallinson, Jakub
Adamek, Eric Malmi, and Aliaksei Severyn. 2023.
Teaching small language models to reason. In Pro-
ceedings of the 61st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 1773–1781, Toronto, Canada. Associ-
ation for Computational Linguistics.
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
2020. A diverse corpus for evaluating and developing
English math word problem solvers. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 975–984, Online.
Association for Computational Linguistics.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022. Rethinking the role of demonstrations:
What makes in-context learning work? In Proceed-
ings of the 2022 Conference on Empirical Methods in
Natural Language Processing, pages 11048–11064,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
OpenAI. 2023.
GPT-4 technical report.
CoRR,
abs/2303.08774.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 2080–2094, Online.
Association for Computational Linguistics.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res., 21:140:1–140:67.
Subhro Roy and Dan Roth. 2015. Solving general arith-
metic word problems. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1743–1752, Lisbon, Portu-
gal. Association for Computational Linguistics.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom
Kozhevnikov, Ivan Evtimov, Joanna Bitton, Man-
ish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori,
Wenhan Xiong, Alexandre Défossez, Jade Copet,
Faisal Azhar, Hugo Touvron, Louis Martin, Nico-
las Usunier, Thomas Scialom, and Gabriel Synnaeve.
2023. Code llama: Open foundation models for code.
CoRR, abs/2308.12950.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2022. Learning to retrieve prompts for in-context
learning. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 2655–2671, Seattle, United States.
Association for Computational Linguistics.
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya
Sachan. 2023. Distilling reasoning capabilities into
smaller language models. In Findings of the Asso-
ciation for Computational Linguistics: ACL 2023,
pages 7059–7073, Toronto, Canada. Association for
Computational Linguistics.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models. CoRR,
abs/2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurélien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models. CoRR, abs/2307.09288.
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,
Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.
2023a. Plan-and-solve prompting: Improving zero-
shot chain-of-thought reasoning by large language
models. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 2609–2634, Toronto,
Canada. Association for Computational Linguistics.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.
Le, Ed H. Chi, Sharan Narang, Aakanksha Chowd-
hery, and Denny Zhou. 2023b.
Self-consistency
improves chain of thought reasoning in language
models. In The Eleventh International Conference
on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023. OpenReview.net.
Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.
Hoi. 2021. CodeT5: Identifier-aware unified pre-
trained encoder-decoder models for code understand-
ing and generation.
In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 8696–8708, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai, and Quoc V Le. 2022a. Finetuned language
models are zero-shot learners. In International Con-
ference on Learning Representations.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022b. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language processing.
In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, pages 38–45, Online. Association
for Computational Linguistics.
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang,
Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,
Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng
Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao,
Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu,
Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei
Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma,
Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie,
Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng
Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xian-
grong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin
Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding
Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yu-
peng Zhang, Zenan Zhou, and Zhiying Wu. 2023.
Baichuan 2:
Open large-scale language models.
CoRR, abs/2309.10305.
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting
Dong, Chuanqi Tan, and Chang Zhou. 2023. Scaling
relationship on learning mathematical reasoning with
large language models. CoRR, abs/2308.01825.
Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao
Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023.
Mammoth: Building math generalist models through
hybrid instruction tuning. CoRR, abs/2309.05653.
Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping
Wang. 2023. A survey on model compression for
large language models. CoRR, abs/2308.07633.
A
Chain-of-Thought Distillation
A.1
Data Generation from LLMs
The CoTD process commences with the gener-
ation of a dataset from LLMs, which lays the
groundwork for subsequent fine-tuning of SLMs.
As illustrated in Figure 7, we employ in-context
learning strategies (Dong et al., 2023; Min et al.,
2022; Rubin et al., 2022) to elicit the genera-
tion of rationales from LLMs within a mathe-
matical reasoning dataset D, where each entry is
a tuple (x, y)—x being the question and y the
correct answer.
To generate CoT, we select k
samples {(x1, y1), (x2, y2), . . . , (xk, yk)} from D
and manually craft corresponding rationales c in
CoT format. These form contextualized examples
{(x1, c1, y1), (x2, c2, y2), . . . , (xk, ck, yk)}, which
are compiled into a demonstration set DD. We then
prompt the LLM with a new question appended
with ""Let’s think step by step"" and feed it the
demonstration set to generate a rationale for the
question. The CoT generation formula is:
ci = fM(xi, DD),
where M denotes the LLM, f is the greedy decod-
ing function, and i indexes the example (x, y) in
D. This procedure results in a CoT dataset DP ,
composed of triplets (x, c, y).
Data Filtering
Upon generating CoT with
LLMs, we validate the rationale against the gold
answer, a crucial step to ensure the quality of our
initial reasoning dataset DC. Discrepancies be-
tween the rationale’s answer and the gold answer
Question: Natalia sold clips to 48 of her friends in 
April, and then she sold half as many clips in May. 
How many clips did Natalia sell altogether in April 
and May?
Let’s think step by step
In April, Natalia sold 48 clips.\nIn May, she sold half 
as many clips, which is 48/2 = 24 clips.\nAltogether, 
Natalia sold 48 + 24 = 72 clips in April and 
May.\nThe answer is 72
Question: Jack is stranded on a desert island. He 
wants some salt to season his fish. He collects 2 liters 
of seawater in an old bucket. If the water is 20% salt, 
how many ml of salt will Jack get when all the water 
evaporates?
Let’s think step by step
LLMs
If the seawater is 20% salt, then 2 liters of seawater contains 2 * 
1000 * 0.20 = 400 ml of salt.\nTherefore, Jack will get 400 ml of 
salt when all the water evaporates.\nThe answer is 400 ml
Data Filtering
Saving
 Generating Correct Answer
Gold Answer
Dropping
 Generating Wrong Answer
Figure 7: Detailed data generation of CoTD. Firstly, we manually construct some contextualized examples, and
combine these contextualized examples, the question, and the prompt ""Let’s think step by step"" to prompt LLMs to
generate CoT based on the question. Then, We extract the answer from this rationale. If the answer doesn’t agree
with the gold answer, we will drop the CoT. Finally, we get a high-quality reasoning dataset.
result in exclusion from DC. This filtering metic-
ulously purges incorrect instances, enhancing the
dataset’s quality. Consequently, this refinement
directly contributes to the enhanced performance
of fine-tuned SLMs, attributable to the increased
accuracy and dependability of the training data.
A.2
Fine-tuning SLMs
After assembling the reasoning dataset DC, we
fine-tune SLMs on it. For each training instance
(x, c, y) from DC, we prepend the prompt pc ""Let’s
think step by step"" to the question x to form the
input. Instruction tuning is then applied to the SLM
to generate the corresponding rationale. The loss
function for fine-tuning is:
L = −
N
X
i=1
T
X
t=1
log P(ci
t | ci
<t, xi, pc),
where N is the number of examples in DC, pc is the
prompt guiding the SLM to generate the rationale c,
and c:T represents the sequence of rationale steps
{c1, c2, . . . , cT }.
After fine-tuning, the SLM becomes proficient
at initiating a reasoning process for complex ques-
tions, generating the corresponding rationale. The
final answer is then extracted from this rationale.
B
Program-of-Thought Distillation
B.1
Data Generation from LLMs
The initial phase in our PoTD entails creating
a dataset from LLMs, setting the stage for
SLM fine-tuning.
Figure 8 outlines this data
generation process. We utilize in-context learning
methods (Dong et al., 2023; Min et al., 2022;
Rubin et al., 2022) to induce LLMs to produce
reasoning data.
Within the mathematical rea-
soning dataset D, each entry is a tuple (x, y),
where x is the question and y the gold-standard
answer.
For PoT generation, we choose k
samples {(x1, y1), (x2, y2), . . . , (xk, yk)} from
D and manually create rationales p in PoT
format.
These form contextualized instances
{(x1, p1, y1), (x2, p2, y2), . . . , (xk, pk, yk)},
which are compiled into a demonstration set DD.
We then prompt the LLM with a new question
accompanied by ""Let’s break down the problem
step by step"" and input the demonstration set to
generate a rationale for the question. The PoT
generation is formalized as:
pi = fM(xi, DD),
where M is the LLM, f the greedy decoding func-
tion, and i the index of the instance (x, y) in D.
This yields a PoT dataset DP , organized as triplets
(x, p, y).
Data Filtering
Following PoT generation
by LLMs, each program undergoes validation using
an external Python interpreter, a vital step to ensure
the quality of our initial dataset DP . Programs
that fail to compile or produce incorrect results
are immediately discarded. This rigorous filtering
process removes flawed instances, thus improving
the dataset’s quality. The removal of these errors
significantly enhances the performance of the fine-
tuned SLMs due to the increased accuracy and
dependability of the training data.
Question: Natalia sold clips to 48 of her friends in 
April, and then she sold half as many clips in May. 
How many clips did Natalia sell altogether in April 
and May?
Let's break down the code step by step 
clips_sold_April = 48
clips_sold_May = clips_sold_April / 2
total_clips_sold = clips_sold_April + clips_sold_May
ans = total_clips_sold
Question: Jack is stranded on a desert island. He 
wants some salt to season his fish. He collects 2 liters 
of seawater in an old bucket. If the water is 20% salt, 
how many ml of salt will Jack get when all the water 
evaporates?
Let's break down the code step by step
LLMs
seawater_volume_liters = 2
salt_concentration = 0.20
salt_volume_liters = seawater_volume_liters * salt_concentration
salt_volume_ml = salt_volume_liters * 1000
ans = salt_volume_ml
Data Filtering
Saving
 Generating Correct Answer
Python Interpreter
Dropping
 Compile Error
 Generating Wrong Answer
Figure 8: Detailed data generation of PoTD. Firstly, we manually construct some contextualized examples, and
combine these contextualized examples, the question, and the prompt ""Let’s break down the code step by step"" to
prompt LLMs to generate PoT based on the question. This program is sent to a extra python interpreter, if there
are compile errors or if it produces wrong answer, we will drop the PoT. Finally, we get a high-quality reasoning
dataset.
B.2
Fine-tuning SLMs
After generating the reasoning dataset DP , we fine-
tune SLMs on it. For each instance (x, p, y) from
DP , we append the prompt pp ""Let’s break down
the code step by step"" to the question x for input,
and apply instruction tuning to the SLM to produce
the corresponding program. The fine-tuning loss
function is:
L = −
N
X
i=1
T
X
t=1
log P(pi
t | pi
<t, xi, pp),
where N is the count of examples in DP , pp is the
prompt guiding the SLM to generate program p,
and p:T represents the sequence of program steps
{p1, p2, . . . , pT }.
After fine-tuning, the SLM excels at initiating a
reasoning process for complex questions, produc-
ing actionable programs. These are then executed
by an external Python interpreter to obtain the final
answer.
Question
SLMs
PoT
Program
Python 
Interpreter
Answer
Compile Correct
SLMs
EoT
Equations 
Solver
Solve Successfully
SLMs
CoT
Equations
Rationale
Compile Error
Can’t Solve
Figure 9: The detail reasoning process of MTD. When given a question, we first prompt SLMs to generate
program in PoT, and send the program to an python interpreter to get the final answer. If the program fails to
compile, we will prompt SLMs to generate equations in EoT, and send these equations to a equations solver to
solve these equations to get the final answer. If these equations can’t be solved, we will prompt SLMs to generate
rationale in CoT, and extract the final answer from rationale.
"
"We present a fixed-parameter study of the satisfiability problem for propositional logic and propositional dynamic logic. We establish that the satisfiability problem for propositional logic with a fixed number of Boolean negations is fixed-parameter tractable, and provide similar results for certain restricted fragments of propositional dynamic logic.","Propositional dynamic logic is a well-studied system for reasoning about the behavior of programs and has been used for verifying the properties of programs. Its satisfiability problem is EXPTIME-complete, making it impractical for large-scale applications. This motivates us to study fixed-parameter algorithms for propositional logic and propositional dynamic logic.","Related work on propositional dynamic logic includes the introduction of various variants of the logic, such as propositional dynamic logic with intersection, propositional dynamic logic with parallel composition, and propositional dynamic logic with converse. The time complexity of the satisfaction problem for these logics has also been studied, with most variants being EXPTIME-complete or above.nannannan","We use fixed-parameter algorithms to study propositional logic and propositional dynamic logic. Fixed-parameter algorithms focus on reducing the time complexity of a problem by restricting one of its parameters. In our case, we focus on the number of Boolean negations that can appear in a formula.nan","We present several fixed-parameter tractability results for propositional logic and propositional dynamic logic. In particular, we show that:

* The satisfiability problem for propositional logic with a fixed number of Boolean negations is PTIME.
* The satisfiability problem for propositional dynamic logic with a fixed number of Boolean negations is FPT.

Furthermore, we provide several additional variants of propositional logic and propositional dynamic logic for which the satisfiability problem is fixed-parameter tractable.","Our results demonstrate that fixed-parameter algorithms can be used to reduce the time complexity of the satisfiability problem for propositional logic and propositional dynamic logic. This opens up the possibility of using these logics for larger-scale applications, such as program verification.",A Fixed-Parameter Study on Propositional Dynamic Logic,"Mohammad Javad Hosseinpour, Farzad Didehvar","A Fixed-Parameter Study on Propositional 
Dynamic Logic 
Mohammad Javad Hosseinpour𝑖 
𝐹𝑎𝑟𝑧𝑎𝑑 𝐷𝑖𝑑𝑒ℎ𝑣𝑎𝑟𝑖𝑖 
i Amirkabir University of Technology, m.j.hosseinpour@aut.ac.ir  
ii Amirkabir University of Technology, didehvar@aut.ac.ir  
Abstract 
 Since its establishment, propositional dynamic logic (PDL) has been a subject of intensive 
academic research and frequent use in the industry. We have studied the complexity of 
some PDL problems and in this paper, we show results for some special cases of PL and 
PDL. 
Keywords: Propositional Dynamic Logic, Fixed-Parameter, Schaefer’s Dichotomy 
Theorem, Propositional Logic 
1. Introduction 
Dynamic logic is a family of logic systems for studying the behavior of programs and can 
be used to verify the properties of terminating programs. While dynamic logic and closely 
related logics can be used in a variety of fields and problems, including knowledge 
representation (De Giracomo & Lenzerini, 1994) and linguistics (Kracht, 1995), the high 
complexity of its satisfiability problem (EXPTIME-complete (Fischer, 1979)) may cause 
problems in the utilization of the power of dynamic logic. 
1.1. PDL Syntax And Semantics 
Syntax of PDL can be described by defining two sets: a set of formulas (Φ) and a set of 
programs (Π). These sets are defined by the following rules (Harel, 2000): 
Rules for formulas: 
I) 
Every atomic formula is a formula. 
II) 
0 is a formula. 
III) 
If 𝜑 and 𝜓 are formulas, then ∽ 𝜑, 𝜑 ∧ 𝜓, and 𝜑 ∨ 𝜓 are formulas. 
Rules for programs: 
IV) 
Every atomic program is a program. 
V) 
If 𝛼 and 𝛽 are programs, then 𝛼; 𝛽, 𝛼 ∪ 𝛽, and 𝛼∗ are programs. 
Rules for deriving programs from formulas and vice versa: 
VI) 
If 𝜑 is a formula and 𝛼 is a program, then [𝛼]𝜑 is a formula. 
VII) If 𝜑 is a formula, then 𝜑? is a program. 
The semantics of propositional dynamic logic can be defined via Kripke frames . A Kripke 
frame is the pair 𝔎 = (K, 𝑚𝔎) where K is the set of states and 𝑚𝔎 is the meaning function 
from each atomic formula to a subset of K and from each atomic program to a subset of 
𝐾2. Intuitively, we can say that 𝑚𝔎 contains the states that satisfy a certain formula and all 
possible input/output pairs of a certain program. We can define the semantics of every 
formula and program in PDL using induction. If 𝜑 and 𝜓 are formulas and 𝛼 and 𝛽 are 
programs, then: 
I) 
𝑚𝔎(0) ≝ ∅ 
II) 
𝑚𝔎(𝜑 →  𝜓) ≝ (𝐾 − 𝑚𝔎(𝜑)) ∪ 𝑚𝔎(𝜓) 
III) 
𝑚𝔎(𝛼; 𝛽) ≝ 𝑚𝔎(𝛼) ∘ 𝑚𝔎(𝛽) 
IV) 
𝑚𝔎(𝛼 ∪ 𝛽) ≝ 𝑚𝔎(𝛼) ∪ 𝑚𝔎(𝛽) 
V) 
𝑚𝔎(𝛼∗) ≝ ⋃
𝑚𝔎(𝑎)𝑛
𝑛≥0
 
VI) 
𝑚𝔎([𝛼]𝜑) ≝ 𝐾 − (𝑚𝔎(𝛼) ∘ (𝐾 − 𝑚𝔎(𝜑))) 
VII) 𝑚𝔎(𝜑? ) ≝ {(𝑢, 𝑢)|𝑢 ∈ 𝑚𝔎(𝜑)} 
 
1.2. Previous Works 
A substantial amount of work on logics designed to verify the properties of programs can 
be contributed to Hoare’s work and his logic, aptly named Hoare Logic (Hoare, 1969). His 
work was continued by Salwicki’s Algorithmic Logic (Salwicki, 1977) and Pratt’s 
Dynamic Logic (Pratt, 1976). In 1979, Fischer and Ladner introduced the propositional 
version of Dynamic Logic (Fischer, 1979). Since then, many others variants of PDL have 
been introduced. PDL with intersection (IPDL) (Lange & Lutz, 2005), PDL with parallel 
composition (RSPDL) (Balbiani & Tinchev, 2014), and PDL with converse (CPDL) (M, 
1985) are just a few examples of these new logics. 
The time complexity of the satisfaction problem for these logics has also been subject to 
study. Propositional dynamic logic is known to be EXPTIME-complete, while IPDL is 
2EXPTIME-complete, CPDL is EXPTIME and RSPDL is undecidable. 
Since the time complexity of different variants of PDL is high (mostly EXPTIME-complete 
and above), it is worthwhile to search for different techniques to reduce their time 
complexity. The technique used in this paper is trying to study special cases of PL and PDL 
using fixed-parameter algorithms. In this method, one of the parameters of the algorithm 
(like k) input is focused on, and the complexity of the algorithms is computed as a 
parameter of input size and k (Gurevich, Stockmeyer, & Vishkin, 1984). This paper uses 
this technique by restricting the number of Boolean negations that can appear in a formula. 
 
 
2. Fixed-Parameter Results for PL And PDL 
In this section, special cases of PL and PDL are studied. But before introducing these two 
algorithms, a few theorems will be stated and proved, to provide a foundation for the fixed-
parameter algorithms. 
 
Definition 2.1. Shaefer’s Dichotomy Theorem (SDT) (Shaefer, 1978) is a theorem that 
partitions Boolean satisfaction problems into two categories: 𝑃 and 𝑁𝑃 (assuming 𝑃 ≠
𝑁𝑃). This theorem states that a Boolean satisfaction problem 𝐹 can be solved in polynomial 
time, if and only if it satisfies at least one of the six following conditions: 
I) 
𝐹 is satisfied if all variables are false. 
II) 
𝐹 is satisfied if all variables are true. 
III) 
𝐹 is definable by a CNF formula where each conjunct has at most one negated 
variable. 
IV) 
𝐹 is definable by a CNF formula where each conjunct has at most one unnegated 
variable. 
V) 
𝐹 is definable by a CNF formula where each conjunct has at most two literals. 
VI) 
𝐹 is the set of solutions of a system of linear equations over the two-element 
field {0, 1}. 
Using this tool, we can prove our first theorem. 
Theorem 2.1. The satisfiability problem in positive propositional logic is PTIME. 
Proof. SDT’s second condition. 
Definition 2.1. A valuation of a formula like 𝑓 is the set of positive variables in the 
valuation, and is shown as 𝑇(𝑓). 
Lemma 2.1. If 𝑓 is a formula in positive propositional logic, 𝑇1(𝑓) ⊆ 𝑇2(𝑓) and 𝑇1(𝑓) 
satisfies 𝑓, then 𝑇2(𝑓) satisfies 𝑓. 
Proof. Induction on formula length. If 𝛼 is an atomic formula, then {𝛼} is the only 
satisfying valuation and therefore the lemma stands. Now, suppose 𝑎 and 𝑏 are formulas 
for which the lemma is true. There are only two ways to make a formula with 𝑎 and 𝑏 in 
positive propositional logic: 
Case 1. 𝑎 ∨ 𝑏: If 𝑇1(𝑎 ∨ 𝑏) satisfies 𝑎 ∨ 𝑏, then it either satisfies 𝑎 or 𝑏; we know that 𝑇2 
satisfies at least one of 𝑎 or 𝑏, so it satisfies 𝑎 ∨ 𝑏. 
Case 2. 𝑎 ∧ 𝑏: If 𝑇1(𝑎 ∧ 𝑏) satisfies 𝑎 ∧ 𝑏, then it satisfies both 𝑎 and 𝑏. Then if 𝑇1(𝑎 ∧
𝑏) ⊆ 𝑇2(𝑎 ∧ 𝑏), based on the assumption, 𝑇2 satisfies both 𝑎 and 𝑏, so it satisfies 𝑎 ∧ 𝑏. 
Theorem 2.2. The satisfiability problem in propositional logic with one Boolean negation 
is PTIME. 
Proof. If a Boolean formula is transformed to the equal CNF formula (in polynomial time), 
there can be four possible ways to put a Boolean negation operator in the formula: 
Case 1. Before a variable. 
By SDT’s third condition, the problem belongs to 𝑃. 
Case 2. Before a group of variables in a conjunction. 
After using De Morgan’s law and distributing positive literals into negative literals, a 
formula satisfying SDT’s third condition will be obtained. 
Case 3. Before a conjunction. 
After applying De Morgan’s law, each negative literal will be a separate conjunction, 
satisfying SDT’s third condition. 
Case 4. Before a group of conjunctions. 
For this case, the following algorithms can be used. A proof of the correctness of this 
algorithm will also be provided. 
Algorithm 2.1. 
1- Apply De Morgan’s law twice. A formula of the form ⋀ ⋁ 𝑥𝑖𝑗
𝑗
 
𝑖
∧ ⋁
⋀ ∼ 𝑦𝑚𝑛
𝑛
𝑚
 
will be reached. 
2- For every conjunction ⋀ ∼ 𝑦𝑛
𝑛
: 
2-1- Define the valuation 𝑇(𝑓) = 𝑉𝑡𝑜𝑡𝑎𝑙 − {𝑦|𝑦 is present in ⋁ ∼ 𝑦𝑛
𝑛
}. 
2-2- If 𝑇(𝑓) satisfies the formula, return 𝑇(𝑓) and exit. 
3- Return 𝑛𝑜𝑛𝑒. 
It is obvious that if Algorithm 1 returns a valuation, the valuation satisfies the formula. To 
show that if the formula has at least a satisfying valuation, the algorithm returns one. 
Suppose after testing valuations 𝑇1, … , 𝑇𝑛, no satisfying valuation has been found, but there 
is a valuation 𝑇𝑡𝑟𝑢𝑒 that satisfies the formula. There are only two possibilities for 𝑇𝑡𝑟𝑢𝑒: 
Case 1. For every 𝑇𝑖, 𝑇𝑡𝑟𝑢𝑒 − 𝑇𝑖 ≠ ∅: it is obvious that 𝑇𝑡𝑟𝑢𝑒 does not satisfy ⋁
⋀ ~𝑦𝑚𝑛
𝑛
𝑚
 
and therefor does not satisfy the formula, reaching a contradiction. 
Case 2. There is a 𝑇𝑖 such that 𝑇𝑡𝑟𝑢𝑒 ⊆ 𝑇𝑖: we know that 𝑇𝑖 satisfies the negative part of the 
formula, and following lemma 2.1, it also satisfies the positive part of the formula, therefor 
satisfying the entire formula and reaching a contradiction. 
Theorem 2.3. The satisfiability problem with two Boolean negations is PTIME. 
Proof. All possible combinations of two Boolean negations can be reduced to either 
instances of formulas with one negation, or directly to one of the conditions of Shaefer’s 
Dichotomy Theorem. 
Definition 2.2. A propositional formula is AND-only if it only contains atomic 
propositions, the Boolean AND operator and the Boolean NOT operator. 
Definition 2.3. A propositional formula is OR-only if it only contains atomic propositions, 
the Boolean OR operator and the Boolean NOT operator. 
Theorem 2.4. The satisfiability problem for AND-only formulas with a fixed number of 
Boolean negations is PTIME. 
Proof. After distributing the Boolean negations to atomic propositions, a formula with 
alternating AND and OR operators is achieved. This formula can be represented as a tree, 
with each leaf being an atomic proposition, and inner nodes being AND or OR operators. 
The height of this tree is at most 𝑘, and so the number of its leaves is at most 2𝑘. Therefor, 
there are at most 2𝑘 valuations for the formula and so the complexity of the satisfiability 
problem is 𝑂(𝑛2𝑘). 
Theorem 2.5. The satisfiability problem for OR-only formulas with a fixed number of 
Boolean negations is PTIME. 
Proof. The only non-trivial form of OR-only formulas has the form ¬ ⋁ Φ𝑖
𝑖
 that after 
distributing the negation, creates an AND-only formula. Since the number of Boolean 
negations is fixed, 𝑖 ≤ 𝑘 and therefore the problem is PTIME. 
Theorem 2.6. Every PDL formula has an equivalent formula with Boolean negations 
applied only on propositions. 
Proof. If a non-atomic formula such as 𝜓 is negated in the original formula, it can be 
written as [𝜓? ](𝜎 ∧ ¬𝜎) where 𝜎 is an atomic formula, The two formulas are equivalent: 
𝑚𝔎([𝜓? ](𝜎 ∧ ¬𝜎)) = 𝑚𝔎([𝜓? ]0) = 𝐾 − (𝑚𝔎(𝜓? ) ∘ (𝐾 − 𝑚𝔎(0)))
= 𝐾 − (𝑚𝔎(𝜓? ) ∘ 𝐾) = 𝐾 − ({(𝑢, 𝑢)|𝑢 ∈ 𝑚𝔎(𝜓)} ∘ 𝐾)
= 𝐾 − {𝑢|𝑢 ∈ 𝑚𝔎(𝜓)} = 𝐾 − 𝑚𝔎(𝜓) = 𝑚𝔎(¬𝜓) 
 
3. Conclusion 
PL and PDL, due to their different properties, don’t seem to react to fixing parameters 
similarly. Fixing the number of atomic formulas is such an example [n]. Providing fixed-
parameter algorithms for PL is an easier task than doing so for PDL. However, since PL 
can be fully expressed in PDL, obtained results in PDL can almost always apply to PL, 
making these results much stronger. 
4. References 
 
Balbiani, P., & Tinchev, T. (2014). Definability and computability for PRSPDL. 
Advances in Modal Logic, 16-33. 
De Giracomo, G., & Lenzerini, M. (1994). Boosting the Correspondence Between 
Description Logics And Propositional Dynamic Logics. Proceedings of The 
Twelfth National Conference on Artificial Intelligence (pp. 205-212). AAI Press. 
Fischer, M. J. (1979). Propositional dynamic logic of regular programs. Journal of 
computer and system sciences, 194-211. 
Gurevich, Y., Stockmeyer, L., & Vishkin, U. (1984). Solving NP-hard problems on 
graphs that are almost trees and an application to facility location problems. 
Journal of the ACM, 459-473. 
Harel, D. &. (2000). Dynamic Logic. Massachusetts: MIT Press. 
Hoare, C. (1969). An axiomatic basis for computer programming. Communications of the 
Association of Computing Machinery, 576-580. 
Kracht, M. (1995). Syntactic Codes and Grammar Refinement. Journal of Logic, 
Language and Information. 
Lange, M., & Lutz, C. (2005). 2-Exp Time lower bounds for propositional dynamic 
logics. The Journal of Symbolic Logic, 1072-1086. 
M, V. (1985). The taming of converse: Reasoning about two-way computations. 
Workshop on Logic of Programs (pp. 413-424). Berlin: Springer, Berlin, 
Heidelberg. 
Pratt, V. (1976). Semantical considerations on Floyd-Hoare logic. Proceedings of the 
17th IEEE Symposium on Foundations of Computer Science. Los Alamitos, CA. 
Salwicki, A. (1977). Algorithmic Logic, a Tool for Investigations of Programs. Logic, 
Foundations of Mathematics, and Computability Theory: Part One of the 
Proceedings of the Fifth International Congress of Logic, Methodology and 
Philosophy of Science.  
Shaefer, T. J. (1978). The complexity of satisfiability problems. Proceedings of the tenth 
annual ACM symposium on Theory of computing.  
 
 
 
 
"
"With the increasing applications of infrared technology in diverse fields, there is a growing interest in image super-resolution techniques for infrared images using deep learning. This paper proposes a novel Transformer model, termed Large Kernel Transformer (LKFormer), to address the challenges of capturing effective feature information from infrared images, which often exhibit uniform pixel distribution and limited gradient range. The LKFormer comprises a Large Kernel Residual Depth-wise Convolutional Attention (LKRDA) module with linear complexity and a Gated-Pixel Feed-Forward Network (GPFN) to enhance the model's capacity for information flow management. Extensive experimental results demonstrate the superiority of LKFormer in infrared image super-resolution tasks, surpassing the performance of state-of-the-art techniques with fewer parameters and significantly higher visual quality.","Image Super-Resolution (SR) aims to reconstruct high-quality images from their degraded, low-resolution counterparts. Convolutional Neural Networks (CNNs) have been widely adopted in SR models due to their ability to represent abstract and high-level semantic information. However, CNN-based methods are limited by their local structural information modeling capability, making them less suitable for addressing long-range degenerate patterns present in low-resolution infrared images. This paper addresses these challenges by introducing a novel Transformer model, termed Large Kernel Transformer (LKFormer), which leverages a large receptive field to foster increased feature interactions and a reconfigured, highly effective feedforward network.","The literature review section of the paper provides an overview of deep learning-based single-image SR methods, focusing on CNN-based and Transformer-based approaches. It highlights the challenges faced by existing CNN-based methods in modeling degraded distributions over extensive distances and discusses the recent successes of Transformer models, particularly the Vision Transformer (ViT) and Swin Transformer, in image SR tasks. However, it also points out the limitations of Transformer models in considering images as one-dimensional sequences, which neglects their inherent 2D structure. Additionally, the paper reviews the limited research on SR methods specifically designed for infrared images, emphasizing the need for addressing the challenges of low contrast and poor perceptual quality in low-resolution infrared images.nannannan","The methodology section describes the proposed LKFormer model, which consists of three main parts: a shallow feature extraction layer, a deep feature extraction module, and an image reconstruction module. The deep feature extraction module utilizes residual Transformer blocks (RTB) stacked with multiple Transformer layers (TL) and a final convolutional layer. Jump connections are incorporated to aid in the recovery process and stabilize training. The LKFormer introduces a Large Kernel Residual Depth-wise Convolutional Attention (LKRDA) module as an alternative to the standard multi-head self-attention (MSA) module. The LKRDA employs several residual depth-wise convolution blocks that progressively expand the receptive field during feature extraction, enabling both long-range dependency modeling and local content enhancement. The residual connections within LKRDA contribute to network convergence and optimization. To further enhance the suitability of the Transformer architecture for dense pixel prediction, a Gated-Pixel Feed-Forward Network (GPFN) is proposed. The GPFN adds a branch of pixel attention to the classical feed-forward neural network to control the forward flow of features.nan","The results section presents extensive experimental evaluations of the LKFormer against various state-of-the-art SR methods on three public datasets: IR700, results-A, and ESPOL FIR. Quantitative comparisons in terms of PSNR and SSIM metrics demonstrate the superior performance of LKFormer, achieving higher PSNR and SSIM values for both scale factors 2 and 4. Visual comparisons of SR results on infrared images illustrate the effectiveness of LKFormer in reconstructing distant objects, preserving structural integrity, and enhancing texture details. Ablation studies are conducted to investigate the contributions of different components of the LKFormer, including the LKRDA module, the GPFN module, and the number of TLs and RTBs. These experiments confirm the effectiveness of each component in improving the SR performance of infrared images.","The conclusion section summarizes the main contributions of the paper and highlights the superior performance of the LKFormer in infrared image SR tasks. It also acknowledges potential improvements, such as introducing a multi-scale architecture to handle images at different resolutions and a multi-branch structure to capture various features at multiple levels. The authors express their intention to explore these possibilities in future research.",LKFormer: Large Kernel Transformer for Infrared Image Super-Resolution,"Feiwei Qin, Kang Yan, Changmiao Wang, Ruiquan Ge, Yong Peng, Kai Zhang","LKFormer: Large Kernel Transformer for Infrared Image
Super-Resolution
Feiwei Qin1, Kang Yan1, Changmiao Wang2, Ruiquan Ge1,∗, Yong Peng1,
Kai Zhang3,∗
Abstract
Given the broad application of infrared technology across diverse fields, there is an increasing empha-
sis on investigating super-resolution techniques for infrared images within the realm of deep learning.
Despite the impressive results of current Transformer-based methods in image super-resolution tasks,
their reliance on the self-attentive mechanism intrinsic to the Transformer architecture results in
images being treated as one-dimensional sequences, thereby neglecting their inherent two-dimensional
structure. Moreover, infrared images exhibit a uniform pixel distribution and a limited gradient
range, posing challenges for the model to capture effective feature information. Consequently, we
suggest a potent Transformer model, termed Large Kernel Transformer (LKFormer), to address
this issue. Specifically, we have designed a Large Kernel Residual Depth-wise Convolutional Atten-
tion (LKRDA) module with linear complexity. This mainly employs depth-wise convolution with
large kernels to execute non-local feature modeling, thereby substituting the standard self-attentive
layer. Additionally, we have devised a novel feed-forward network structure called Gated-Pixel Feed-
Forward Network (GPFN) to augment the LKFormer’s capacity to manage the information flow
within the network. Comprehensive experimental results reveal that our method surpasses the most
advanced techniques available, using fewer parameters and yielding considerably superior performance.
Keywords: Infrared image, Super-Resolution, Deep learning, Large kernel convolution
1 Introduction
Image Super-Resolution (SR) is a prevalent low-
level vision issue that has risen to prominence in
recent years. The goal of image SR is to recon-
struct high-quality images from their degraded,
low-resolution counterparts by eliminating degra-
dation artifacts. While numerous learning-based
methods have been previously developed for vis-
ible image SR, there is a burgeoning need to
improve the resolution of infrared images. Infrared
images can offer invaluable information in scenar-
ios where visibility is compromised due to severe
weather or challenging environments. They are
extensively deployed in sectors such as security,
medicine, construction, energy, and environmen-
tal studies [1–3]. Despite the great potential of
infrared images, creating high-quality infrared
image datasets through image SR networks is a
challenge due to low contrast and poor perceptual
quality in low-resolution infrared images.
Convolutional Neural Networks (CNNs) have
become a fundamental component in image SR
models and have showcased significant reconstruc-
tion success. However, these CNN-based methods
[4–7] are confined to local structural information
due to the limited receptive field of convolutional
operators. This limitation makes them less capa-
ble of addressing long-range degenerate patterns
present in low-resolution infrared images. To sur-
mount these limitations, researchers have begun
to explore the Transformer structure [8],a preva-
lent model in natural language processing. With
the notable work of SwinIR [9], which incorporates
the self-attentive (SA) mechanism, the Trans-
former structure has demonstrated remarkable
success in single-image SR tasks. Unlike CNNs,
SA mechanisms model non-local information, ren-
dering them ideal for high-quality image restora-
tion. However, while these mechanisms achieve
superior results, the quadratic growth of compu-
tational complexity in relation to spatial resolu-
tion restricts their application to infrared termi-
nal devices with limited computational resources.
Moreover, SA mechanisms neglect the adaptation
of channel dimensions and prohibit modeling of
the local invariant property harnessed by CNNs.
To maximize the powerful feature extraction
capabilities of CNNs and achieve global infor-
mation modeling, we propose an innovative and
efficient Transformer model, termed the Large
Kernel Transformer (LKFormer). This model cap-
italizes on a large receptive field to foster increased
feature interactions, thereby overcoming the limi-
tations of previous CNN-based SR methods, which
struggled to model long distances. Specifically,
we introduce a Large Kernel Residual Depth-wise
Convolutional Attention (LKRDA) block with
linear complexity to supplant the standard multi-
head self-attention. The LKRDA block encodes
local structural information and long-range pixel
dependencies by combining depth-wise convolu-
tion with variable kernel size, while also utilizing
1 × 1 convolution to facilitate information inter-
action between channels. Furthermore, given the
absence of prior information in low-resolution
infrared images, it is crucial to fully leverage the
extracted features during SR while also exploring
their underlying relationships. For this purpose,
the residual connection is incorporated multiple
times within the LKRDA block.
In the Transformer architecture, the feed-
forward network (FN) operates as a critical com-
ponent of the model. The standard FN consists
of two fully connected layers that encompass
a non-linear layer, facilitating local non-linear
transformations of features. This study intro-
duces an advanced FN design termed ’gated-pixel
FN’ (GPFN), specifically engineered to enhance
the Transformer structure for the intricate pixel
prediction task associated with image SR. This
enhancement is achieved by fine-tuning the orig-
inal FN through the incorporation of a pixel
attention branch subsequent to the non-linear
layer. This extra branch generates an attention
map, denoted as M, possessing identical dimen-
sions to the current feature F, followed by exe-
cuting a dot product operation on F and M.
This methodology effectively amplifies the weight
of pertinent content, thereby enabling succeed-
ing network layers to concentrate more keenly
on the finer characteristics of the image. Collec-
tively, these modifications contribute to superior
performance and high-quality output.
The main contributions of this work can be
summarized as follows:
• We introduce a unique Transformer model,
termed
as
LKFormer,
aimed
at
achieving
explicit modeling of both local and global range
dependencies in infrared images.
3
• We
propose
an
inventive
module
named
LKRDA that enables the aggregation of both
local and non-local pixel interactions. This is
primarily accomplished by integrating separable
residual depth-wise convolutional blocks with
varying receptive fields.
• We present a reconfigured and highly effective
feedforward network, founded on pixel atten-
tion. This network is capable of further adjust-
ing the weights of pertinent features to more
efficiently facilitate the SR reconstruction of
infrared images.
• Extensive
experimental
results
demonstrate
that the LKFormer outperforms existing state-
of-the-art methods in terms of performance.
2 Related Work
2.1 Deep Learning-based
Single-image SR
CNN-based methods. Owing to the robust
representational capabilities of CNNs, and their
faculty to utilize the feature hierarchy in learning
mapping relationships between input and target
images, several image SR models [10–12] were
based on CNN architectures during the initial
rise of deep learning. To harness the capacity
of CNN architectures to represent abstract and
high-level semantic information, some methodolo-
gies [13, 14] began experimenting with residual
connectivity. This approach allowed CNNs to cap-
ture more information while retaining efficient
training speed and accuracy. Furthermore, some
techniques [15, 16] employed dense connectivity to
aggregate hierarchical features for superior image
reconstruction. As the exploration of attention
mechanisms in CNN architectures gained trac-
tion, an increasing number of image SR methods
[17–19] began incorporating either channel atten-
tion or spatial attention modules, thus allowing
selective focus on relevant information. However,
since most of these previous methods are based
on small kernel (e.g., 3×3) CNNs, they encounter
challenges in modeling degraded distributions over
extensive distances. To address this, Mei et al.
[20] proposed a novel convolutional neural net-
work structure termed non-local sparse attention.
This structure is capable of globally capturing tex-
ture information by introducing non-local blocks
and a sparse attention mechanism, despite the
significant computation resources it requires.
Transformer-based methods. The Trans-
former model, originally developed for sequential
data processing in natural language tasks, has
been successfully extended to vision tasks due
to its impressive performance in high-dimensional
tasks. Notable examples of this extension include
the Vision Transformer (ViT) [21] and Swin
Transformer [22], which have been recently uti-
lized in image SR. These methods [9, 23, 24] have
exceeded the performance of previous CNN-based
models, thanks to the self-attention mechanism
within the Transformer structure that captures
non-local features. However, it should be noted
that the self-attention mechanism was initially
designed to extract relationships between sequen-
tial data in natural language processing tasks.
When applied to images, the mechanism treats
them as 1D sequences, failing to consider their
inherent 2D structure. Furthermore, while the
Swin Transformer’s window-based self-attention
offers greater computational efficiency than global
self-attention, it still presents a significant com-
putational and memory burden. Although Zamir
et al. [25] employ channel attention to encode
global context information for reducing computa-
tional consumption, their method overlooks the
significance of local features in image restoration.
SR methods for infrared images. Infrared
imagery, facilitated by technological advance-
ments, has become indispensable in various fields
such as aircraft maintenance and object mea-
surement [26–28]. However, due to high hard-
ware costs and complex environmental factors,
the acquired infrared images often exhibit low
resolution. As a result, the reconstruction of
low-resolution infrared images and the genera-
tion of high-quality equivalents have emerged as
paramount research focuses. Traditionally, most
infrared image SR methods relied predominantly
on the frequency domain or dictionary-based algo-
rithms [29]. Recently, however, the potent feature
extraction capabilities of CNN have accelerated
the development of deep learning-based infrared
image SR algorithms. For instance, He et al.
[30] suggested a novel cascaded depth network
approach that utilizes multiple receptive fields
to enhance the SR of infrared images. In con-
trast to visible images, infrared images often
exhibit blurred contours and edges due to varia-
tions in imaging systems. To address these chal-
lenges, Zou et al. [31] designed a U-net-based
model that employs a residual network to capture
high-frequency and low-frequency information in
infrared images. Drawing inspiration from the suc-
cess of Generative Adversarial Networks (GANs)
in SR tasks for visible images, scientists have
leveraged GAN models to enhance the resolution
of infrared images. A case in point is Huang et
al. [32], who introduced a highly effective model,
PSRGAN, that incorporates migration learning
and knowledge refinement to achieve superior
SR performance for infrared images using fewer
parameters. Furthermore, they proposed HetSR-
WGAN [33], which enhances training stability by
integrating a new loss function. In an effort to
address the issues of low resolution and loss of
thermal information in infrared images, Wu et
al. [34] recommended a lightweight SR method
based on Meta-Transfer Learning to achieve high-
resolution reconstruction of infrared images.
2.2 Convolution for no-local range
modelling
AlexNet [35] is the classical CNN that gained
significant attention in the field. It represents
an
early
model
that
employed
convolutional
operations with large kernels. Another classical
approach that leverages large kernel convolution
for enhancing model performance is the Global
Convolutional Network [36]. The method utilizes
large, symmetric, and separable convolutional ker-
nels to improve tasks like semantic segmentation,
which involve pixel-wise predictions. This allows
for the computation of weights across the entire
feature map. Large convolutional kernels were pre-
viously less preferred due to their high computa-
tional cost and the extensive parameter space they
require. However, there has been a recent resur-
gence of interest in them owing to advancements
in efficient convolutional techniques and the appli-
cation of Transformers in vision tasks. For exam-
ple, convMixer [37] utilizes depth-wise convolution
with large convolutional kernels to replace the
mixer component in ViT [21] or MLP [38]. Con-
versely, ConvNeXt [39] reimagines a robust base
network model, inspired by ResNet [40], through
7 × 7 depth-wise convolution and other opti-
mization strategies, yielding results comparable
to or even surpassing those achieved by Swin
Transformer [9]. Acknowledging the significance of
receptive fields for downstream tasks, RepLKNet
[41] develops a pure convolutional model that
expands the convolutional kernel size to a record-
breaking 31 × 31. In a bid to push the bounds
of convolutional kernel size, SLaK [42] incorpo-
rates an unmatched 51 × 51 convolution. Unlike
RepLKNet, SLaK employs two complementary
non-square kernels (M × N and N × M) when
decomposing a large kernel, thereby enhancing the
training stability and memory scalability of large
convolutional kernels. In this study, we adopt a
similar approach to decompose large kernel con-
volution, thus expanding the receptive field whilst
avoiding excessive computational consumption.
3 Method
In this section, we first provide an overview of
our network structure’s overall pipeline. Subse-
quently, we delve into the specifics of the proposed
Transformer layer, which forms the cornerstone of
our approach. This layer primarily comprises two
critical components: the LKRDA and the GPFN.
3.1
Overall pipeline
Fig. 1 illustrates the overall network structure.
The network receives a low-resolution infrared
image as an input and outputs a high-quality
infrared image after processing. More specifically,
the network consists of three main parts: First,
a 3 × 3 convolutional layer performs shallow
feature extraction, converting the low-resolution
infrared image into a feature map. The deep fea-
ture extraction module then applies N residual
Transformer blocks (RTB) to extract spatially
varying reconstructed target information distri-
bution, while each RTB is composed by stacking
multiple Transformer layers (TL) and a final con-
volutional layer. Jump connections are introduced
in the RTB and the entire feature representation
learning part in order to aid in the recovery pro-
cess and stabilize training. Furthermore, to mine
different levels of latent feature information and
to achieve fine structural and textural detail in
the reconstructed infrared images, the output of
each RTB is concatenated prior to being input to
the image reconstruction module. Finally, the rich
features computed by the previous operations are
5
C
RTB
RTB
RTB
TL
TL
TL
TL
TL
Conv3
LN
LKRDA
LN
GPFN
Conv1
Conv1
DWC7
RDB11
RDB21
RDB31
Conv1
Conv1
SiLU
Conv1
Conv1
DWC3
Conv1
(a) LKRDA
(c) GPFN
Attention Map
Element-wise sum
Element-wise product
Conv
Depth-wise conv
Pixel Shuffle
Conv1
DWCK
SiLU
(b) RDB
Concat operation
C
Fig. 1 The architecture of the proposed LKFormer for infrared image SR. Here, RDB, RTB, and TL denote the residual
depth-wise convolution block, residual Transformer block, and Transformer layer respectively. LKRDA stands for large
kernel residual depth-wise convolutional attention block, which utilizes multiple convolutional layers of different sizes to
extract features and then generates an attention map as a way to achieve encoding both local structural information and
long-range dependencies of the input features. GPFN refers to a module called gated-pixel feed-forward network, which
adds a branch of pixel attention to the classical feed-forward neural network as a way to control the forward flow of features.
Additionally, LN and DWC3 represent the layer normalization and the 3 × 3 depth-wise convolution.
used to estimate the recovered image in the image
reconstruction module.
3.2 Large kernel residual depth-wise
convolution attention
The
Transformer
architecture
has
exhibited
remarkable advancement in image restoration,
owing to its potent representation abilities. Nev-
ertheless, current Transformer-based techniques
insufficiently account for the local context in
image
restoration.
Ideally,
image
restoration
methods should be insensitive to shifts in degra-
dation, ensuring the removal of undesirable effects
independent of their spatial distribution across
the image. Hence, maximizing the information
extracted from local regions is indispensable in
restoring clean images. Furthermore, the stan-
dard Transformer architecture, featuring a self-
attention layer, exhibits a quadratic surge in time
and memory complexity concerning key-query dot
product interactions with input spatial resolution.
To circumvent these problems, we present the lin-
early complex LKRDA as an alternative to the
standard multi-head self-attention (MSA) mod-
ule. Assuming that the input image has a size
of h × w, the computational complexity of the
global MSA module and the LKRDA module are
as follows:
Ω(MSA) = 4hwC2 + 2(hw)2C,
Ω(LKRDA) = Pn
i=1 C
Table 1 Quantitative comparison (average PSNR/SSIM) with other compared methods for scale factor 2 and 4 on the
public datasets IR700, results-A, and ESPOL FIR.
Method
Scale
Params (M)
FLOPs (G)
IR700
results-A
ESPOL FIR
PSNR / SSIM
PSNR / SSIM
PSNR / SSIM
EDSR[4]
×2
40.73
166.84
39.35 / 0.9481
37.81 / 0.9346
43.17 / 0.9643
RRDB[5]
26.78
110
39.40 / 0.9479
37.83 / 0.9349
43.18 / 0.9649
PSRGAN[32]
0.31
2.18
39.13 / 0.9307
37.29 / 0.9212
42.84 / 0.9572
NLSN[20]
41.80
171.20
39.41 / 0.9485
37.85 / 0.9353
43.22 / 0.9647
SwinIR[9]
11.75
51.33
39.49 / 0.9489
37.85 / 0.9351
43.23 / 0.9650
HAT[24]
9.47
52.39
39.46 / 0.9486
37.90 / 0.9353
43.11 / 0.9645
LKFormer
6.38
25.81
39.70 / 0.9505
37.91 / 0.9355
43.28 / 0.9655
EDSR[4]
×4
43.09
205.83
32.69 / 0.8583
33.15 / 0.8312
38.89 / 0.9320
RRDB[5]
26.93
114
32.76 / 0.8594
33.17 / 0.8316
38.89 / 0.9322
PSRGAN[32]
0.35
6.50
32.19 / 0.8379
32.74 / 0.8136
38.27 / 0.9229
NLSN[20]
44.16
210.20
32.76 / 0.8589
33.24 / 0.8323
39.01 / 0.9329
SwinIR[9]
11.90
53.83
32.78 / 0.8602
33.28 / 0.8337
39.01 / 0.9329
HAT[24]
9.62
54.89
32.83 / 0.8595
33.32 / 0.8343
39.04 / 0.9334
LKFormer
6.52
28.31
32.95 / 0.8616
33.36 / 0.8348
39.10 / 0.9339
practicality when dealing with images of higher
resolution and detail.
As shown in Fig. 1 a, LKRDA incorporates
several residual depth-wise convolutional blocks
that progressively expand the receptive field dur-
ing feature extraction, facilitating not only long-
range dependency modeling but also local content
enhancement. To generate more detailed and high-
quality image textures and attain superior visual
outcomes, we have integrated the residual concate-
nation operation multiple times within LKRDA.
This operation ensures the retention of as many
features as possible across the entire hierarchy.
Infrared images are inherently susceptible to envi-
ronmental factors, which often results in them
containing less usable information. Therefore, dur-
ing the SR process of infrared images, it becomes
imperative to maximize the utilization of the avail-
able input feature information and minimize any
potential loss of information during the feature
transmission process. Additionally, the incorpo-
ration of residual connections can significantly
aid network convergence during training and sim-
plify the process of network optimization. In con-
trast to the standard MSA module that involves
acquiring the query and key feature followed by
employing the softmax function for attention map
computation. To capture the relationships among
information from various locations in the feature
map, LKRDA utilizes a depth-wise convolution
with progressively expanding convolution kernels
to establish correlations and generate attention
map. This approach mitigates the issue of sig-
nificantly increased complexity resulting from the
processing of high-resolution images.
Given a normalized tensor H of layer, we
first encode the cross-channel information and
local information by applying a 1 × 1 convo-
lution followed by a pair of 7 × 1 and 1 × 7
depth-wise convolution. Subsequently, large kernel
7
residual depth-wise convolution blocks are uti-
lized for capturing long-range dependencies, in
order to generate attention maps at a global level.
Mathematically, our LKRDA can be written as:
f rdwc
k×k (H) = σ
IR700: 3 (×2)
IR700: 196 (×2)
IR700: 10 (×4)
IR700: 12 (×4)
HR
EDSR [4]
RRDB [5]
PSRGAN [32]
PSNR/SSIM
40.94/0.9878
40.50/0.9875
40.37/0.9679
NLSN [20]
SwinIR [9]
HAT [24]
LKFormer
40.87/0.9875
40.91/0.9876
40.75/0.9873
41.17/0.9886
HR
EDSR [4]
RRDB [5]
PSRGAN [32]
PSNR/SSIM
31.04/0.9065
30.79/0.9060
30.67/0.9048
NLSN [20]
SwinIR [9]
HAT [24]
LKFormer
31.16/0.9091
31.20/0.9096
30.68/0.9045
31.40/0.9110
HR
EDSR [4]
RRDB [5]
PSRGAN [32]
PSNR/SSIM
31.32/0.9051
31.65/0.9065
31.19/0.9051
NLSN [20]
SwinIR [9]
HAT [24]
LKFormer
31.53/0.9070
31.59/0.9073
31.61/0.9113
31.97/0.9135
HR
EDSR [4]
RRDB [5]
PSRGAN [32]
PSNR/SSIM
28.50/0.8843
28.65/0.8876
28.43/0.8839
NLSN [20]
SwinIR [9]
HAT [24]
LKFormer
28.33/0.8832
28.94/0.8913
29.04/0.8921
29.06/0.8938
Fig. 2
Visual comparisons of LKFormer with other SR methods on IR700 dataset.
similarity index (SSIM) [46]. Moreover, to bet-
ter understand the proposed novel Transformer
structure, we introduced the local attribution map
(LAM) [47] for overall model analysis.
4.2 Results on Infrared Image SR
We compare the proposed LKFormer with state-
of-the-art SR methods: EDSR [4], RRDB [5],
PSRGAN [32], NLSN [20], SwinIR [9], and HAT
[24]. Table 1 shows the quantitative compar-
ison results for different upscale factors. Our
LKFormer achieves superior SR performance with
a reduced parameter count compared to previous
state-of-the-art methods. On the IR700 dataset,
the maximum gains of PSNR and SSIM access
are 0.76dB and 0.0237 for ×4, separately. Despite
the significantly lower number of parameters in
PSRGAN compared to our proposed LKFormer,
this reduction also hampers its feature extrac-
tion capability, resulting in subpar SR results for
infrared images relative to other models. Fur-
thermore, the utilization of a GAN in PSRGAN
9
Table 2 Ablations of LKRDA for infrared image SR (×4) on the IR700 dataset. The abbreviation DWC7 refers to the
7 × 7 depth-wise convolution, followed by the abbreviation RDB referring to residual depth-wise convolution block.
DWC7
RDB11
RDB21
RDB31
RDB41
Residual
Params (M)
PSNR (dB)
!
%
%
%
%
!
4.13
32.71
%
!
!
!
%
!
4.48
32.83
!
%
!
!
%
!
4.00
32.82
!
!
%
%
%
!
3.02
32.83
!
!
!
%
%
!
3.82
32.91
!
!
!
!
%
%
6.52
32.80
!
!
!
!
!
!
7.50
32.95
!
!
!
!
%
!
6.52
32.95
Table 3 Ablation study on influence of the pixel
attention branch.
FN architecture
Params (M)
PSNR (dB)
FN [8]
4.71
32.79
GPFN (Ours)
6.52
32.95
Table 4 Ablation study on influence of the number of
TL in each RTB.
Number of TL
Params (M)
PSNR (dB)
n = 2
2.56
32.76
n = 4
4.54
32.91
n = 6 (default)
6.52
32.95
n = 8
8.50
32.95
Table 5 Ablation study on influence of the number of
RTB, each RTB contains 6 TLs.
Number of RTB
Params (M)
PSNR (dB)
n = 2
2.40
32.74
n = 4
4.46
32.89
n = 6 (default)
6.38
32.95
n = 8
8.58
32.95
necessitates the implementation of supplemental
tricks to ensure training stability, thereby poten-
tially widening the disparity between PSRGAN
and other SR models. In addition, Fig. 3 illus-
trates the LAM results obtained by various models
employing distinct mechanisms. Fig. 4 displays
the PSNR results of various models trained on
the IR700 dataset, plotted against the number of
training epochs.
Fig. 2 shows a visual comparison between
LKFormer and the other competitor methods on
×2 and ×4. The original images are selected from
the test dataset of IR700. As we can observe from
the zoomed-in view, the infrared images recon-
structed by LKFormer are more closely aligned to
the ground truth than their competitors regarding
edge and texture information. Specifically, Fig. 2
demonstrates that LKFormer excels in recon-
structing distant objects compared to other meth-
ods in the SR reconstruction comparison images
labeled 3 and 196. Moreover, in the comparison
images labeled 10 and 12, LKFormer significantly
preserves the structural integrity of the railings.
The visual comparison therefore also further con-
firms the effectiveness of our proposed LKFormer
for the SR reconstruction of infrared images.
4.3 Ablation Study
Ablation on LKRDA design. The LKRDA
module is critical for integrating features from
both local and no-local sources in the infrared
image SR model. Here, we re-evaluate the method
of combining different convolutions within the
LKRDA module and conducted experiments vary-
ing the size of the convolution kernel. Our exper-
imental results, shown in Table 2, revealed that
the lack of a depth-wise convolution kernel of 7×7
prevented the model from focusing effectively on
local texture information, resulting in degraded
SR performance. Similarly, removing depth-wise
convolution kernels of 11 × 11 cause significant
variation in kernel size within LKRDA, which also
let to decreased SR performance. Additionally, the
removal of 21×21 and 31×31 depth-wise convolu-
tion limits the model’s receptive field and results
in a loss of SR performance. To further investigate
IR700: 18
SwinIR DI: 0.91
HAT DI: 1.29
LKFormer(Ours) DI: 2.96
result-A: fused2
SwinIR DI: 0.79
HAT DI: 1.18
LKFormer(Ours) DI: 1.66
ESPOL FIR: 2
SwinIR DI: 0.72
HAT DI: 1.61
LKFormer(Ours) DI: 2.87
Fig. 3 LAM results of SwinIR, HAT, and LKFormer (Ours) in ×4 infrared image SR. When reconstructing the patches
highlighted in red boxes, the red-marked areas represent informative pixels contributing to the reconstruction. Darker colors
indicate a higher degree of contribution. The diffusion index (DI) reflects the range of pixels utilized in image reconstruction,
with a higher DI showing a wider range and the involvement of more pixels. The results demonstrate that our method
possesses a broader receptive domain, enabling a wider utilization of pixels for image reconstruction.
1000
2000
3000
4000
5000
6000
Number of training epochs
38.0
38.2
38.4
38.6
38.8
39.0
39.2
39.4
39.6
39.7
PSNR (dB)
EDSR
RRDB
PSRGAN
NLSN
SwinIR
HAT
LKFormer
Number of training epochs
PSNR (dB)
(a) PSNR vs. Epochs (×2)
1000
2000
3000
4000
5000
6000
Number of training epochs
31.4
31.6
31.8
32.0
32.2
32.4
32.6
32.8
33.0
PSNR (dB)
EDSR
RRDB
PSRGAN
NLSN
SwinIR
HAT
LKFormer
Number of training epochs
PSNR (dB)
(b) PSNR vs. Epochs (×4)
Fig. 4 The effect of PSNR measure of the different models with respect to training epochs for scaling factors of 2 and 4.
the contribution of reduced information loss to
improve SR in infrared images, we remove only the
residual connections inside LKRDA and find that
the lack of residual structure inside the LKRDA
module resulted in degraded SR performance.
Finally, we incorporate a 41 × 41 depth-wise con-
volution into the original structure. However, this
addition doesn’t improve the SR results since the
receptive field of the LKRDA module is already
capable of processing the current input infrared
image effectively.
11
Gated-pixel for FN. The FN module is a
crucial component of the Transformer architec-
ture, primarily utilized to enhance the model’s
nonlinear capability to learn more intricate fea-
ture representations. To adopt the Transformer
architecture for SR tasks, an extra pixel attention
branch is integrated into the standard FN mod-
ule. Table 3 illustrates that the gating mechanism
present in the GPFN module, which regulates the
backward flow of pertinent features, provides an
improvement of 0.16 dB over the vanilla FN.
Impact of the TL number and RTB num-
ber. In order to investigate how the number of
TLs and RTBs affects the final SR performance of
the model, we conduct an ablation study in Table
4 and 5. The ablation experiments show that if
the number of TLs and RTBs is too small, the
depth and feature representation ability of the net-
work will be compromised. When there are six TLs
and RTBs, the model performs optimally in terms
of SR for infrared images. Increasing the number
of TLs and RTBs beyond six leads to informa-
tion redundancy within the network. In order to
achieve a trade-off between performance and com-
plexity, the final model structure is comprised of
six RTBs, with six TLs nested within each RTB.
5 Conclusion
We present a novel Transformer network, called
LKFormer, to solve infrared image SR. Specifi-
cally, we design a large kernel residual convolution
structure to replace the vanilla SA layer to achieve
local and non-local feature modeling. In addition,
the proposed module can process high-resolution
infrared images more efficiently and does not
exhibit a quadratic increase in computational
complexity with increasing image resolution. Fur-
thermore, to enhance the suitability of the pro-
posed Transformer architecture for the task of
dense pixel prediction, we develop a novel module,
named the GPFN. The GPFN module improves
the information flow within the network by incor-
porating pixel attention branching. Experimen-
tal results show that our LKFormer outperforms
competing SR methods while maintaining fewer
parameters. However, we only present an intu-
itive structure until now, there are still many
possibilities for improving it. For example, a multi-
scale architecture could be introduced to handle
images at different resolutions, while a multi-
branch structure could enable the network to
capture various features at multiple levels. These
potential improvements should be explored in
future research.
Funding
This
work
was
supported
by
National
Key
Research and Development Program of China
(No.
2023YFE0114900),
Aeronautical
Science
Foundation
of
China
(No.
2022Z0710T5001),
GuangDong Basic and Applied Basic Research
Foundation (No. 2022A1515110570), Innovation
teams of youth innovation in science and tech-
nology of high education institutions of Shandong
province (No. 2021KJ088), the Open Project Pro-
gram of the State Key Laboratory of CAD&CG
(No. A2304), Zhejiang University. The authors
would like to thank the reviewers in advance for
their comments and suggestions.
Data availability
The authors confirm that the data supporting
the findings of this study are available in a public
repository. These data were derived from the fol-
lowing resources available in the public domain
(https://figshare.com/s/2121562561211c0a8101,
https://github.com/rafariva/ThermalDatasets).
Ethics declarations
Source code
The source code will be available at https://
github.com/sad192/large-kernel-Transformer.
Conflicts of interest
The authors declare that they have no conflict of
interest.
References
[1] Sousa, E., Vardasca, R., Teixeira, S., Seixas,
A., Mendes, J., Costa-Ferreira, A.: A review
on the application of medical infrared ther-
mal imaging in hands. Infrared Physics &
Technology 85, 315–323 (2017)
[2] Lopez-Perez, D., Antonino-Daviu, J.: Appli-
cation of infrared thermography to failure
detection in industrial induction motors:
case stories. IEEE Transactions on Industry
Applications 53(3), 1901–1908 (2017)
[3] Kirimtat,
A.,
Krejcar,
O.:
A
review
of
infrared
thermography
for
the
investiga-
tion of building envelopes: Advances and
prospects. Energy and Buildings 176, 390–
406 (2018)
[4] Lim, B., Son, S., Kim, H., Nah, S., Mu Lee,
K.: Enhanced deep residual networks for
single image super-resolution. In: Proceed-
ings of the IEEE Conference on Computer
Vision and Pattern Recognition Workshops,
pp. 136–144 (2017)
[5] Wang, X., Yu, K., Wu, S., Gu, J., Liu, Y.,
Dong, C., Qiao, Y., Change Loy, C.: ESR-
GAN: Enhanced super-resolution generative
adversarial networks. In: Proceedings of the
European Conference on Computer Vision
(ECCV) Workshops, pp. 0–0 (2018)
[6] Zhang, Y., Li, K., Li, K., Wang, L., Zhong,
B., Fu, Y.: Image super-resolution using very
deep residual channel attention networks.
In: Proceedings of the European Conference
on Computer Vision (ECCV), pp. 286–301
(2018)
[7] Zhang, K., Li, Y., Zuo, W., Zhang, L.,
Van Gool, L., Timofte, R.: Plug-and-play
image restoration with deep denoiser prior.
IEEE
Transactions
on
Pattern
Analysis
and Machine Intelligence 44(10), 6360–6376
(2021)
[8] Vaswani, A., Shazeer, N., Parmar, N., Uszko-
reit, J., Jones, L., Gomez, A.N., Kaiser,  L.,
Polosukhin, I.: Attention is all you need.
Advances in neural information processing
systems 30 (2017)
[9] Liang, J., Cao, J., Sun, G., Zhang, K.,
Van Gool, L., Timofte, R.: SwinIR: Image
restoration using swin transformer. In: Pro-
ceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 1833–
1844 (2021)
[10] Dong, C., Loy, C.C., He, K., Tang, X.:
Image super-resolution using deep convolu-
tional networks. IEEE transactions on pat-
tern analysis and machine intelligence 38(2),
295–307 (2015)
[11] Kim, J., Lee, J.K., Lee, K.M.: Deeply-
recursive convolutional network for image
super-resolution. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern
Recognition, pp. 1637–1645 (2016)
[12] Zhang, K., Zuo, W., Gu, S., Zhang, L.:
Learning deep CNN denoiser prior for image
restoration. In: Proceedings of the IEEE Con-
ference on Computer Vision and Pattern
Recognition, pp. 3929–3938 (2017)
[13] Kim, J., Lee, J.K., Lee, K.M.: Accurate
image super-resolution using very deep con-
volutional networks. In: Proceedings of the
IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1646–1654 (2016)
[14] Cavigelli, L., Hager, P., Benini, L.: CAS-
CNN:
A
deep
convolutional
neural
net-
work for image compression artifact suppres-
sion. In: 2017 International Joint Conference
on Neural Networks (IJCNN), pp. 752–759
(2017). IEEE
[15] Zhang, Y., Tian, Y., Kong, Y., Zhong, B.,
Fu, Y.: Residual dense network for image
super-resolution. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern
Recognition, pp. 2472–2481 (2018)
[16] Zhang, Y., Tian, Y., Kong, Y., Zhong, B., Fu,
Y.: Residual dense network for image restora-
tion. IEEE Transactions on Pattern Analysis
and Machine Intelligence 43(7), 2480–2495
(2020)
[17] Dai, T., Cai, J., Zhang, Y., Xia, S.-T., Zhang,
L.: Second-order attention network for single
image super-resolution. In: Proceedings of the
IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 11065–11074
(2019)
[18] Niu, B., Wen, W., Ren, W., Zhang, X.,
Yang, L., Wang, S., Zhang, K., Cao, X.,
13
Shen, H.: Single image super-resolution via
a holistic attention network. In: Computer
Vision–ECCV 2020: 16th European Confer-
ence, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part XII 16, pp. 191–207 (2020).
Springer
[19] Zhao, H., Kong, X., He, J., Qiao, Y., Dong,
C.: Efficient image super-resolution using
pixel attention. In: Computer Vision–ECCV
2020 Workshops: Glasgow, UK, August 23–
28, 2020, Proceedings, Part III 16, pp. 56–72
(2020). Springer
[20] Mei, Y., Fan, Y., Zhou, Y.: Image super-
resolution with non-local sparse attention. In:
Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition,
pp. 3517–3526 (2021)
[21] Dosovitskiy, A., Beyer, L., Kolesnikov, A.,
Weissenborn, D., Zhai, X., Unterthiner, T.,
Dehghani, M., Minderer, M., Heigold, G.,
Gelly, S., et al.: An image is worth 16x16
words: Transformers for image recognition
at scale. arXiv preprint arXiv:2010.11929
(2020)
[22] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei,
Y.,
Zhang,
Z.,
Lin,
S.,
Guo,
B.:
Swin
transformer: Hierarchical vision transformer
using shifted windows. In: Proceedings of
the IEEE/CVF International Conference on
Computer Vision, pp. 10012–10022 (2021)
[23] Fang, J., Lin, H., Chen, X., Zeng, K.: A
hybrid network of CNN and Transformer for
lightweight image super-resolution. In: Pro-
ceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition,
pp. 1103–1112 (2022)
[24] Chen, X., Wang, X., Zhou, J., Qiao, Y., Dong,
C.: Activating more pixels in image super-
resolution transformer. In: Proceedings of the
IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 22367–22377
(2023)
[25] Zamir, S.W., Arora, A., Khan, S., Hayat,
M., Khan, F.S., Yang, M.-H.: Restormer:
Efficient
transformer
for
high-resolution
image restoration. In: Proceedings of the
IEEE/CVF Conference on Computer Vision
and
Pattern
Recognition,
pp.
5728–5739
(2022)
[26] Si, T., He, F., Li, P., Gao, X.: Tri-modality
consistency optimization with heterogeneous
augmented images for visible-infrared person
re-identification. Neurocomputing 523, 170–
181 (2023)
[27] Tang, W., He, F., Liu, Y.: Tccfusion: An
infrared and visible image fusion method
based on transformer and cross correlation.
Pattern Recognition 137, 109295 (2023)
[28] Ge, S., Li, C., Zhao, S., Zeng, D.: Occluded
face recognition in the wild by identity-
diversity inpainting. IEEE Transactions on
Circuits and Systems for Video Technology
30(10), 3387–3397 (2020)
[29] Wang, J., Ralph, J.F., Goulermas, J.Y.: An
analysis of a robust super resolution algo-
rithm for infrared imaging. In: 2009 Pro-
ceedings of 6th International Symposium on
Image and Signal Processing and Analysis,
pp. 158–163 (2009). IEEE
[30] He, Z., Tang, S., Yang, J., Cao, Y., Yang,
M.Y., Cao, Y.: Cascaded deep networks with
multiple receptive fields for infrared image
super-resolution. IEEE transactions on cir-
cuits and systems for video technology 29(8),
2310–2322 (2018)
[31] Zou, Y., Zhang, L., Liu, C., Wang, B., Hu, Y.,
Chen, Q.: Super-resolution reconstruction of
infrared images based on a convolutional neu-
ral network with skip connections. Optics and
Lasers in Engineering 146, 106717 (2021)
[32] Huang, Y., Jiang, Z., Lan, R., Zhang, S.,
Pi, K.: Infrared image super-resolution via
transfer learning and PSRGAN. IEEE Signal
Processing Letters 28, 982–986 (2021)
[33] Huang, Y., Jiang, Z., Wang, Q., Jiang, Q.,
Pang, G.: Infrared image super-resolution
via Heterogeneous Convolutional WGAN.
In: Pacific Rim International Conference on
Artificial Intelligence, pp. 461–472 (2021).
Springer
[34] Wu, W., Wang, T., Wang, Z., Cheng, L.,
Wu, H.: Meta transfer learning-based super-
resolution infrared imaging. Digital Signal
Processing 131, 103730 (2022)
[35] Krizhevsky, A., Sutskever, I., Hinton, G.E.:
Imagenet classification with deep convolu-
tional neural networks. Communications of
the ACM 60(6), 84–90 (2017)
[36] Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J.:
Large kernel matters–improve semantic seg-
mentation by global convolutional network.
In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition,
pp. 4353–4361 (2017)
[37] Asher, T., Zico, K.J.: Patches are all you
need? In: Proceedings of the IEEE Interna-
tional Conference on Learning Representa-
tions (ICLR) (2022)
[38] Tolstikhin, I.O., Houlsby, N., Kolesnikov, A.,
Beyer, L., Zhai, X., Unterthiner, T., Yung, J.,
Steiner, A., Keysers, D., Uszkoreit, J., et al.:
Mlp-mixer: An all-mlp architecture for vision.
Advances in neural information processing
systems 34, 24261–24272 (2021)
[39] Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer,
C., Darrell, T., Xie, S.: A convnet for the
2020s. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern
Recognition, pp. 11976–11986 (2022)
[40] He, K., Zhang, X., Ren, S., Sun, J.: Deep
residual learning for image recognition. In:
Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp.
770–778 (2016)
[41] Ding, X., Zhang, X., Han, J., Ding, G.: Scal-
ing up your kernels to 31x31: Revisiting large
kernel design in CNNs. In: Proceedings of the
IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 11963–11975
(2022)
[42] Liu, S., Chen, T., Chen, X., Chen, X., Xiao,
Q., Wu, B., Pechenizkiy, M., Mocanu, D.,
Wang, Z.: More convnets in the 2020s: Scal-
ing up kernels beyond 51x51 using sparsity.
arXiv preprint arXiv:2207.03620 (2022)
[43] Zou, Y., Zhang, L., Liu, C., Wang, B., Hu, Y.,
Chen, Q.: Super-resolution reconstruction of
infrared images based on a convolutional neu-
ral network with skip connections. Optics and
Lasers in Engineering 146, 106717 (2021)
[44] Liu, Y., Chen, X., Cheng, J., Peng, H.,
Wang, Z.: Infrared and visible image fusion
with convolutional neural networks. Interna-
tional Journal of Wavelets, Multiresolution
and Information Processing 16(03), 1850018
(2018)
[45] Danaci, K.I., Akagunduz, E.: A survey on
infrared image and video sets. arXiv preprint
arXiv:2203.08581 (2022)
[46] Wang, Z., Bovik, A.C., Sheikh, H.R., Simon-
celli, E.P.: Image quality assessment: from
error visibility to structural similarity. IEEE
transactions on image processing 13(4), 600–
612 (2004)
[47] Gu,
J.,
Dong,
C.:
Interpreting
super-
resolution networks with local attribution
maps. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern
Recognition, pp. 9199–9208 (2021)
"
"With the rapid advancements in personalized speech generation techniques, protecting speakers' voices from malicious misuse has become a pressing concern. This paper introduces a novel approach for speaker protection against voice privacy threats. We propose a method to generate adversarial speech samples that perturb the target speaker's voice while preserving its naturalness. We attack the speaker encoder of the YourTTS model using gradient-based adversarial perturbation methods to obtain these adversarial speech samples. Evaluations on the YourTTS-generated speech demonstrate the effectiveness of the generated adversarial speech in preventing voice imitation.","In the era of deep learning, personalized speech generation techniques have witnessed remarkable progress. These techniques, including text-to-speech (TTS) and voice conversion (VC), offer the capability of synthesizing speech with high speaker similarity, raising security concerns regarding voice privacy. Specifically, given the speech utterances of a target speaker, TTS and VC technologies can be exploited to generate the speaker's speech with fabricated content, potentially leading to voice spoofing attacks on speaker authentication systems and manipulation of public opinions.","To address voice privacy concerns, researchers have explored various approaches. These include developing synthetic speech detection techniques to identify generated speech in speaker authentication systems, as exemplified by the ASVspoof challenges. Additionally, anonymization methods have been proposed to protect voice privacy by concealing speaker attributes in speech utterances, thus preventing their utilization in malicious speech generation. While these approaches offer valuable insights, we aim to proactively protect voice privacy by preventing speech from being used in personalized speech generation, pursuing a strategy that protects the voice attributes while preserving the naturalness of the speaker's voice.nannannan","Our proposed method for adversarial speech generation builds upon the concept of adversarial attacks on neural networks. Adversarial perturbation, a technique discovered in deep learning, involves modifying inputs to neural network models in a manner that misleads the models into making incorrect predictions. Our approach targets the speaker encoder of personalized speech generation models, which is responsible for extracting speaker attributes from speech utterances. By applying adversarial perturbation to the speaker encoder, we aim to generate adversarial speech samples that disrupt the model's ability to extract accurate speaker attributes, effectively preventing the model from generating the speech of the target speaker.nan","We conducted experiments using the YourTTS model, an open-source pre-trained personalized speech generation model, to evaluate the effectiveness of our adversarial speech generation method. We utilized the I-FGSM adversarial perturbation method to generate adversarial speech samples by attacking the speaker encoder of the YourTTS model. To assess the voice protection capability of the adversarial speech, we carried out automatic speaker verification (ASV) evaluations on the YourTTS-generated speech. The ASV evaluations measured the ability of the model to distinguish between the original speech and the synthesized speech. Our experimental results demonstrated that the adversarial speech generated using the I-FGSM method achieved significantly higher equal error rates (EERs) in the ASV evaluations, indicating that the model was unable to accurately generate the speech of the target speaker.","Through our research, we have introduced a novel method for speaker protection against voice privacy threats in the context of personalized speech generation. Our method utilizes adversarial speech generation to perturb the speaker's voice while preserving its naturalness, effectively preventing the model from imitating the target speaker. This proactive approach to voice privacy protection offers a promising avenue for safeguarding speakers' voices from malicious misuse in personalized speech generation applications.",Adversarial speech for voice privacy protection from Personalized Speech generation,"Shihao Chen, Liping Chen, Jie Zhang, KongAik Lee, Zhenhua Ling, Lirong Dai","ADVERSARIAL SPEECH FOR VOICE PRIVACY PROTECTION FROM PERSONALIZED
SPEECH GENERATION
Shihao Chen1, Liping Chen1,∗, Jie Zhang1, KongAik Lee2, Zhenhua Ling1, Lirong Dai1
1NERC-SLIP, University of Science and Technology of China (USTC), China
2Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University, Hong Kong
shchen16@mail.ustc.edu.cn lipchen@ustc.edu.cn
ABSTRACT
The rapid progress in personalized speech generation technology,
including personalized text-to-speech (TTS) and voice conversion
(VC), poses a challenge in distinguishing between generated and
real speech for human listeners, resulting in an urgent demand in
protecting speakers’ voices from malicious misuse. In this regard,
we propose a speaker protection method based on adversarial at-
tacks. The proposed method perturbs speech signals by minimally
altering the original speech while rendering downstream speech
generation models unable to accurately generate the voice of the
target speaker.
For validation, we employ the open-source pre-
trained YourTTS model for speech generation and protect the target
speaker’s speech in the white-box scenario.
Automatic speaker
verification (ASV) evaluations were carried out on the gener-
ated speech as the assessment of the voice protection capability.
Our experimental results show that we successfully perturbed the
speaker encoder of the YourTTS model using the gradient-based
I-FGSM adversarial perturbation method. Furthermore, the adver-
sarial perturbation is effective in preventing the YourTTS model
from generating the speech of the target speaker.
Audio sam-
ples can be found in https://voiceprivacy.github.io/
Adeversarial-Speech-with-YourTTS.
Index Terms— Personalized speech generation, text-to-speech,
voice conversion, voice privacy, adversary attack.
1. INTRODUCTION
In the era of deep learning, techniques in speech generation have
developed rapidly, greatly improving the quality and naturalness of
the artificial speech. Among them, the personalized speech gener-
ation techniques, including text-to-speech synthesis (TTS) [1] and
voice conversion (VC) [2] facilitate generating the speech of a target
speaker in high speaker similarity, raising the potential security risks
concerning the voice privacy. To be specific, given the speech utter-
ances of a target speaker for reference, multi-speaker TTS and VC
technologies can be utilized to generate its speech utterances with
fabricated content, which can potentially be used to spoof a speaker
authentication system or manipulate public opinions.
How to protect the voice privacy in the speech utterances from
being utilized to generate speech for malicious purposes has at-
tracted the interest of the research community. In this field, several
approaches have been explored.
To name a few, driven by the
ASVspoof challenges [3], the synthesized speech detection tech-
niques have developed remarkably, relieving the threats of synthe-
sized speech on speaker authentication systems. In addition, the
∗Corresponding author
concept of anonymization provides another voice privacy protection
method by concealing the speaker attributes in the speech utterances
before they are used for malicious speech generation. In this manner,
the attributes of the original speaker will not be utilized by speech
modeling techniques. Moreover, it also hides the original speaker
attributes from human listening. Between these two approaches, the
former works on passive protection where the detection is carried out
after the artificial speech of the target speaker is generated; the latter
operates proactively by preventing the artificial speech generation.
This paper follows the route of proactively protecting voice pri-
vacy and aims at preventing speech from being utilized in personal-
ized speech generation. In contrast to existing anonymization meth-
ods that modify speaker attributes and alter human perception, our
study pursues that the voices in both the original and anonymized
speech utterances sound alike to human listeners. To achieve this
goal, the adversarial attack concept will be applied and the adver-
sarial speech will be generated, within which the voice attributes
are protected. Adversarial perturbation on neural networks was dis-
covered in [4] where the neural network models were found to be
misled by adversarial samples. Based on the findings, adversarial
perturbation has been exploited to attack the models of face image
recognition [5], speech recognition [6], speaker identification [7,8],
speaker verification [9, 10] and so on, successfully misleading the
models to make wrong predictions without altering the perceptions
of the original face images and speech utterances.
Currently, in personalized speech generation models, the speaker
modeling is mainly built on extracting speaker attributes with a sep-
arate speaker encoder, e.g., [1, 11]. Given the speech utterances of
a target speaker, the adversarial attack will be carried out on the
speaker encoder to obtain the adversarial speech utterances. In this
manner, the speaker encoder will become incapable of extracting
the speaker attributes of the target speaker from the adversarial ut-
terances. As a result, the speech generation model cannot generate
the speech of the target speaker using the adversarial speech, finally,
protecting the voice privacy. As a preliminary work, we will focus
on white-box scenario, i.e., preventing a known speech generation
model from generating the speech of a target speaker. YourTTS [1]
is selected to be our white-box speech generation model, which can
perform both zero-shot TTS and VC. Given a well-trained YourTTS
model, the adversarial attack methods based on fast gradient step
method (FGSM) [12] are applied to attack the speaker encoder and
thus generate the adversarial speech. To this end, a loss function
is defined on the speaker encoder as the cosine similarity between
the speaker embeddings extracted from the original utterance and
its adversarial sample. Evaluations are carried out on the generated
speech of the target speaker using the adversarial speech as a refer-
ence for speaker attributes extraction. The equal error rate (EER)
arXiv:2401.11857v1  [eess.AS]  22 Jan 2024
HiFi-GAN
Generator
Posterior
Encoder
Generated Speech
Source Speech
Speaker Encoder
Reference Speech
TTS-Related
Modules
Input Text
Language ID
TTS
VC
Flow-Based
Decoder
Fig. 1. The inference flowchart of the YourTTS model. The blue and
red boxes represent the flows of TTS and VC, respectively.
obtained in automatic speaker verification (ASV) is adopted as the
performance metric, measuring the effectiveness of the adversarial
speech utterance in protecting the true speaker attributes from being
perceived by the speaker encoder.
The rest of this paper is organized as follows. In Section 2,
we briefly introduce the background of our work, including the
YourTTS model and the FGSM methods. Section 3 presents the
proposed method for adversarial speech generation.
The experi-
ments will be presented in Section 4. Finally, Section 5 concludes
this work.
2. BACKGROUND
In this section, we will first briefly introduce the inference process of
YourTTS model for personalized speech generation. Then we will
describe the adversarial attack approaches that will be used in our
work, including the FGSM and iterative FGSM (I-FGSM).
2.1. YourTTS
Fig. 1 illustrates the inference process of YourTTS. As shown in
Fig. 1, YourTTS is an end-to-end model that can be applied to gen-
erate personalized speech in both TTS and VC manners. The TTS
flow takes the raw text and the language ID as input. The inputs
are processed by a sequence of TTS-related modules. To be spe-
cific, as detailed in [1], the TTS-related module is composed of an
encoder followed by an alignment generation module. On the VC
side, the model takes the source speech as input and processes it
with a posterior encoder. Finally, a flow-based decoder followed by
a HiFi-GAN [13] generator is applied to generate the speech in both
the TTS and VC flows. Additionally, in both the TTS and VC flows,
a reference speech spoken by the target speaker is applied to pro-
vide the information of the target speaker. The speaker attributes are
extracted with the speaker encoder and utilized in the TTS-related
modules and the HiFi-GAN module to generate the speech of the
target speaker. For details, readers are suggested to read [1].
In our work, the open-source YourTTS model1 will be used
whose speaker encoder utilizes the H/ASP [14] structure, which is
an improved version of ResNet-34 [15]. The H/ASP model was
trained on the VoxCeleb2 dataset [16] and obtained an EER of
1.967% on the test subset of the multi-language LibriSpeech (MLS)
dataset [17].
1https://github.com/Edresson/YourTTS
Error 
Backpropagation
STFT Extraction
Filterbank
Speaker Encoder
Frozen
Original Speech
Fig. 2. The flowchart of generating adversarial samples based on I-
FGSM. The x denotes the STFT feature extracted from the original
reference speech; δi and ˜xi represent the adversarial perturbation
and the resultant adversarial STFT in the i-th iteration, respectively.
When i = 0, ˜x0 is initialized with x.
.
2.2. FGSM & I-FGSM
FGSM [12] is an adversarial attack method based on the optimiza-
tion of neural networks. Denote the loss function as L
STFT ˜x0 is initialized with x. In the i-th iteration, the filterbank
feature vectors are first extracted from x and ˜xi, based on which the
speaker embedding vectors are extracted with the speaker encoder,
denoted as e and ˜ei, respectively. As FGSM methods are based on
error backpropagation, a loss function is exerted on the speaker em-
bedding vectors. Mathematically, it is defined as the cosine distance
between e and ˜ei as follows:
L (e,˜ei) = −
eT˜ei
∥e∥2∥˜ei∥2 .
(3)
A single FGSM step is implemented via loss backpropagation to ˜xi
to increase the cosine distance between e and ˜ei. When the pertur-
bation with respect to ˜xi is obtained as δi, the adversarial STFT is
updated by adding δi to ˜xi as follows:
˜xi+1 ← ˜xi + δi.
(4)
After I iterations, the adversarial STFT can be generated as ˜xI.
When I = 1, the I-FGSM degrades to FGSM.
Upon ˜xI, the inverse STFT (iSTFT) is used to generate the
adversarial speech using the phase as extracted from the original
speech. Since the I-FGSM is carried out on the speaker encoder
of the YourTTS model, the adversarial speech is expected to prevent
it from extracting the correct speaker attributes. When used as ref-
erence speech, it makes the YourTTS model unable to generate the
speech of the target speaker.
4. EXPERIMENTS
Our experiments were conducted on the test-clean subset of the Lib-
rispeech [19] dataset, denoted as libri-test in the following. All the
recordings were resampled to 16kHz. For adversarial speech genera-
tion, 512 frequency bins were extracted with Hanning window, with
a 10 ms frameshift and a 25 ms window size.
In our white-box protection experiments, the YourTTS model
trained with the configuration of Exp.4 as illustrated in [1] was
employed for both adversarial speech generation and evaluation.
Specifically, the utilized YourTTS model was firstly trained using
the multi-lingual corpus composed of VCTK [20] and their internal
datasets, including TTS-Portuguese [21], and M-AILABS French
(trilingual) [22]. Then, the model was finetuned on the train-clean-
100 and train-clean-360 datasets of LibriTTS [23]. In our experi-
ments, three types of perturbed speech were evaluated, including the
Gaussian white noise and the adversarial speech generated with our
proposed method where both FGSM and I-FGSM are applied. For
FGSM, the attack intensity ϵ was set to 0.02. In the experiments of
I-FGSM, the number of iterations I was set to 50 with α = 0.0004.
The effectiveness of the adversarial speech was examined in
both the zero-shot TTS and VC. To evaluate the speaker similar-
ity among the generated speech utterances, the automatic speaker
verification (ASV) evaluations were carried out using the equal er-
ror rate (EER) as the performance metric. In our ASV evaluations,
the official trials on the libri-test dataset as provided by VoicePri-
vacy Challenge 2022 [24] were adopted. The ASV evaluations were
conducted on the speech generated by the YourTTS model. In TTS
speech evaluation, given a pair of a recording and its transcript, the
speech was synthesized with the transcript to be the input text and
the recording as the reference. In the experiments on VC, the record-
ing was used as the reference while the source speech was randomly
selected from a different speaker. Similarly, in the experiments of
perturbed speech, the perturbed utterances were used as references
in YourTTS. The higher EER infers better voice privacy capability
of the perturbation.
ECAPA-TDNN [25] combined with cosine distance was ap-
plied in our ASV evaluation.
The open-source toolkit ASV-
subtools toolkit2 was used to train the ECAPA-TDNN model on
the Librispeech-360-clean dataset. In our experiments, we took into
account the domain mismatch between the original recording and
the generated speech. For evaluation purposes, we utilized two ASV
models: one trained on the original recording, denoted as ASVrec,
and another trained on the TTS speech of Librispeech-360-clean,
generated with the TTS function of the YourTTS model, denoted as
ASVtts. During the training of the two models, data augmentation
using the MUSAN corpus [26] and the RIR dataset [27] was applied.
Firstly, the speech quality of the perturbed speech was evalu-
ated on the libri-test evaluation dataset, with the results as presented
in Table 1. The metrics of perceptual evaluation of speech quality
(PESQ) [28] and signal-to-noise ratio (SNR) were used to assess the
distortion level of the perturbed samples. The results are presented
in Table 1. Besides, the cosine distance between the speaker em-
bedding vectors as extracted with the speaker encoder in YourTTS
model from the recording and the adversarial speech was computed
according to (3). The averaged cosine distance across the utterances
in libri-test is shown in Table 1, denoted as ∆CosD, which ranges
from −1 to 1. A higher value of ∆CosD indicates a lower level of
similarity. It can be observed from Table 1 that I-FGSM achieved
the highest SNR and PESQ, inferring its superiority to the Gaus-
sian white noise and FGSM in preserving the perceptual quality of
the recording. In the ∆CosD metric, I-FGSM also achieved the
highest cosine distance between the speaker embedding vectors ex-
tracted from the recording and the perturbed speech, inferring the
best speaker protection capability.
Table 1. The PESQ, SNR and ∆CosD results on the perturbed
speech obtained with Gaussian white noise, FGSM and I-FGSM.
Method
SNR(dB)(↑)
PESQ(↑)
∆CosD(↑)
Gaussian
32.02
3.79
-0.96
FGSM [12]
32.04
3.78
-0.80
I-FGSM [18]
37.00
4.17
0.06
Next, in order to evaluate the voice protection capability, ASV
evaluations were carried out on the generated speech, where both
TTS and VC were applied for speech generation. Table 2 shows the
EERs obtained on the utterances as generated using YourTTS with
the reference speech being the recording and perturbed speech utter-
ances, respectively. In the evaluations, both the models trained on
recording (ASVrec) and TTS speech (ASVtts) were used. In Table
2, the terms tts and vc refer to the speech generated when the original
recording is used as a reference in the TTS and VC speech genera-
tion processes, respectively. On the other hand, tts∗ and vc∗ indi-
cate the speech generated when the perturbed speech was utilized
as a reference. The Gaussian white noise, adversarial perturbation
generated with FGSM and I-FGSM were examined. For compari-
son, the original recording was also experimented. Regarding the
enrollment utterance in the evaluation trials, the recording (rec), the
speech generated with recording as reference (tts and vc) and the
speech generated with perturbed speech as reference (tts∗ and vc∗)
were applied. tts∗ and vc∗ were used as the test utterances.
From the results as presented in Table 2, we can see that de-
spite the domain difference, ASVrec and ASVtts provided consis-
2https://github.com/Snowdar/asv-subtools
Table 2. The speaker verification EER (%) on recording (rec) , voice conversion data (vc), text-to-speech data (tts), adversarial voice
conversion data (vc∗) and adversarial text-to-speech data (tts∗). The performances are presented in the enrollment-trial format. Four kinds
of speech utterances used for reference in personalized speech generation are included: recording, recording with Gaussian white noise
(Gaussian), adversarial speech obtained by FGSM (FGSM) and adversarial speech obtained by I-FGSM (I-FGSM).
Method
ASVrec
ASVtts
Male
rec-tts∗ tts-tts∗ tts∗-tts∗ rec-vc∗ vc-vc∗ vc∗-vc∗ rec-tts∗ tts-tts∗ tts∗-tts∗ rec-vc∗ vc-vc∗ vc∗-vc∗
recording
5.79
3.79
3.79
10.24
8.46
8.46
5.12
2.45
2.45
8.69
5.35
5.35
Gaussian
9.13
5.12
3.79
12.03
9.80
9.13
9.35
4.90
2.90
11.80
8.24
6.46
FGSM
10.91
6.01
5.79
15.14
12.69
13.81
10.91
5.35
4.90
14.92
9.80
10.24
I-FGSM
48.11
45.21
21.83
49.89
48.55
28.95
52.12
48.11
16.04
52.12
51.22
24.94
Female
recording
9.31
7.48
7.48
10.04
10.58
10.58
8.03
7.48
7.48
10.22
9.12
9.12
Gaussian
8.57
7.64
6.02
11.68
11.13
10.40
8.21
4.93
4.92
11.68
8.76
7.30
FGSM
10.58
9.67
9.12
13.50
13.69
13.87
11.86
7.48
4.93
14.60
11.31
11.86
I-FGSM
51.28
47.26
23.91
50.73
49.27
28.65
52.55
53.01
21.53
52.37
51.64
28.28
tent EERs among the recording and the three types of perturbed ut-
terances. It is noteworthy that the generated speech utterances tts
and tts∗ given recording as reference speech are identical. Both in-
fer that the speech was generated by YourTTS with the recording
being used as reference speech. This is also applicable to the vc
and vc∗ cases. Compared with the recording, when the perturbed
utterances as obtained by the three methods were used for refer-
ence in YourTTS speech generation, higher EERs can be obtained
in both TTS and VC flows in all trial conditions. This suggests
that the perturbation techniques effectively perturbed the speaker at-
tributes extraction by the speaker encoder, thus impairing the ability
of the YourTTS model to generate the voice of the reference speaker.
The adversarial speech generated with I-FGSM achieved the highest
EERs in all trial conditions. Specifically, the rec-tts∗ trials resulted
in approximately 50% EERs, indicating that using the adversarial
speech for reference is unable to generate the speech of the target
speaker. Likewise, the nearly 50% EERs observed in the tts-tts∗
trials suggest that the voices are different between the utterances as
generated using the recording and adversarial speech for the refer-
ence of the target speaker. The same findings can be seen from the
VC evaluations, i.e., the rec-vc∗ and vc-vc∗ scenarios. With respect
to the tts∗-tts∗ and vc∗-vc∗ evaluations, EERs around 20% were
obtained by I-FGSM, which are lower than 50%. This infers that
since the adversarial utterances were generated without considering
the speaker distinction within them, the speaker attributes as per-
ceived by the model still present speaker clustering characteristics.
Finally, given the speech generated using recording and adver-
sarial speech as the reference speech, respectively, the speaker sim-
ilarity with the recording is visualized as given in Fig. 3. In the
visualization, the open-source toolkit Resemblyzer [29] was applied
where the cosine similarity between the speaker embedding vectors
were computed as the similarity score. Five male and five female
speakers were randomly selected from the libri-test dataset, with five
randomly selected utterances for each speaker. The speaker similar-
ity was visualized in utterances and speakers, respectively. The first
and second rows of Fig. 3 show the similarity among utterances and
speakers, respectively. In computing the speaker similarity among
the speakers, the embedding vector for each speaker was obtained
by averaging that of the five utterances. In Fig 3, the speaker simi-
larity between the recording and the generated utterances including
tts, tts∗, vc and vc∗ are presented, respectively. It can be observed
that tts and vc exhibit a relatively high similarity along the diago-
nal with respect to rec. This is consistent with the EERs observed
in the rec-tts∗ and rec-vc∗ conditions when recording is utilized as
reference speech in Table 2. When the I-FGSM method is applied
to protect the voice attributes, the speaker identity in the generated
speech becomes unrelated to the speaker in the recording. This re-
veals the efficacy of the proposed method in protecting the speaker
privacy when being used for personalized speech generation.
rec-tts
rec-tts*
rec-vc
rec-vc*
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Fig. 3. The similarity among utterances (first row) and speakers (sec-
ond row). The speaker similarity between the recording (rec) and the
speech generated using the recording as the reference speech (tts
and vc), using adversarial speech for reference (tts∗ and vc∗) are
presented, respectively.
5. CONCLUSION
In this study, we introduced a method to protect speaker privacy
through adversarial speech generation, aiming at preventing the use
of speaker attributes for generating speech imitating a specific tar-
get speaker. We investigated adversarial sample generation meth-
ods based on FGSM in a white-box protection scenario. The open-
source pre-trained YourTTS model was used for personalized speech
generation, whose speaker encoder was attacked to generate adver-
sarial speech utterances. Our experiments, conducted on the test-
clean dataset of LibriSpeech, demonstrated the effectiveness of our
approach in preventing the YourTTS model from imitating a target
speaker when provided with adversarial speech as a reference.
6. REFERENCES
[1] E. Casanova, J. Weber, C. D. Shulby, A. C. Junior, E. G¨olge,
and M. A. Ponti, “YourTTS: Towards zero-shot multi-speaker
tts and zero-shot voice conversion for everyone,”
in Inter-
national Conference on Machine Learning. PMLR, 2022, pp.
2709–2720.
[2] B. Sisman, J. Yamagishi, S. King, and H. Li, “An overview of
voice conversion and its challenges: From statistical modeling
to deep learning,” IEEE/ACM Transactions on Audio, Speech,
and Language Processing, vol. 29, pp. 132–157, 2021.
[3] X. Liu, X. Wang, M. Sahidullah, J. Patino, H. Delgado, T. Kin-
nunen, M. Todisco, J. Yamagishi, N. Evans, A. Nautsch,
et al., “ASVspoof 2021: Towards spoofed and deepfake speech
detection in the wild,”
IEEE/ACM Transactions on Audio,
Speech, and Language Processing, 2023.
[4] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus, “Intriguing properties of neu-
ral networks,” in 2nd International Conference on Learning
Representations, ICLR 2014, 2014.
[5] S. Aneja, L. Markhasin, and M. Nießner, “TAFIM: Targeted
adversarial attacks against facial image manipulations,” in Eu-
ropean Conference on Computer Vision. Springer, 2022, pp.
58–75.
[6] P. Neekhara, S. Hussain, and P. Pandey,
“Universal adver-
sarial perturbations for speech recognition systems,” CoRR,
abs/1905.03828, 2019.
[7] J. Yao, X. Chen, X.-L. Zhang, W.-Q. Zhang, and K. Yang,
“Symmetric saliency-based adversarial attack to speaker iden-
tification,” IEEE Signal Processing Letters, vol. 30, pp. 1–5,
2023.
[8] A. S. Shamsabadi, F. S. Teixeira, A. Abad, B. Raj, A. Cav-
allaro, and I. Trancoso, “FoolHD: Fooling speaker identifica-
tion by highly imperceptible adversarial disturbances,” in Proc.
ICASSP . IEEE, 2021, pp. 6159–6163.
[9] W. Zhang, S. Zhao, L. Liu, J. Li, X. Cheng, T. F. Zheng, and
X. Hu, “Attack on practical speaker verification system using
universal adversarial perturbations,” in Proc. ICASSP . IEEE,
2021, pp. 2575–2579.
[10] F. Kreuk, Y. Adi, M. Cisse, and J. Keshet, “Fooling end-to-
end speaker verification with adversarial examples,” in Proc.
ICASSP . IEEE, 2018, pp. 1962–1966.
[11] J. Li, W. Tu, and L. Xiao,
“FreeVC: Towards high-quality
text-free one-shot voice conversion,” in Proc. ICASSP . IEEE,
2023, pp. 1–5.
[12] I. J. Goodfellow, J. Shlens, and C. Szegedy,
“Explain-
ing and harnessing adversarial examples,”
arXiv preprint
arXiv:1412.6572, 2014.
[13] J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative adversar-
ial networks for efficient and high fidelity speech synthesis,”
Advances in Neural Information Processing Systems, vol. 33,
pp. 17022–17033, 2020.
[14] H. S. Heo, B.-J. Lee, J. Huh, and J. S. Chung, “Clova baseline
system for the voxceleb speaker recognition challenge 2020,”
arXiv preprint arXiv:2009.14153, 2020.
[15] H. Zeinali, S. Wang, A. Silnova, P. Matˇejka, and O. Plchot,
“BUT system description to voxceleb speaker recognition chal-
lenge 2019,” in Proceedings of The VoxCeleb Challange Work-
shop, 2019, pp. 1–4.
[16] J. Chung, A. Nagrani, and A. Zisserman, “VoxCeleb2: Deep
speaker recognition,” Interspeech 2018, 2018.
[17] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert,
“MLS: A large-scale multilingual dataset for speech research,”
2020.
[18] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial ex-
amples in the physical world,” in Artificial intelligence safety
and security, pp. 99–112. Chapman and Hall/CRC, 2018.
[19] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
rispeech:
An ASR corpus based on public domain audio
books,” in Proc. ICASSP . IEEE, 2015, pp. 5206–5210.
[20] J. Yamagishi, C. Veaux, and K. MacDonald, “CSTR VCTK
Corpus: English multi-speaker corpus for CSTR voice cloning
toolkit (version 0.92),” 2019.
[21] E. Casanova, A. C. Junior, C. Shulby, F. S. d. Oliveira, J. P.
Teixeira, M. A. Ponti, and S. Alu´ısio, “TTS-Portuguese cor-
pus: a corpus for speech synthesis in brazilian portuguese,”
Language Resources and Evaluation, vol. 56, no. 3, pp. 1043–
1055, 2022.
[22] M-ailabs,
“The m-ailabs speech dataset – caito,” [On-
line],
Available: https://www.caito.de/2019/01/
the-m-ailabs-speech-dataset, 2017.
[23] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia,
Z. Chen, and Y. Wu, “LibriTTS: A corpus derived from lib-
rispeech for text-to-speech,” Interspeech 2019, 2019.
[24] X. Chen, G. Li, H. Huang, W. Zhou, S. Li, Y. Cao, and Y. Zhao,
“System description for voice privacy challenge 2022,”
in
Proc. 2nd Symposium on Security and Privacy in Speech Com-
munication, 2022.
[25] B. Desplanques, J. Thienpondt, and K. Demuynck, “ECAPA-
TDNN: Emphasized channel attention, propagation and aggre-
gation in TDNN based speaker verification,” in Proc. Inter-
Speech, 2020, pp. 3830–3834.
[26] D. Snyder, G. Chen, and D. Povey,
“MUSAN: A music,
speech, and noise corpus,” arXiv preprint arXiv:1510.08484,
2015.
[27] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur,
“A study on data augmentation of reverberant speech for robust
speech recognition,” in Proc. ICASSP . IEEE, 2017, pp. 5220–
5224.
[28] Y. Hu and P. C. Loizou, “Evaluation of objective quality mea-
sures for speech enhancement,” IEEE Transactions on audio,
speech, and language processing, vol. 16, no. 1, pp. 229–238,
2007.
[29] L. Wan, Q. Wang, A. Papir, and I. L. Moreno, “Generalized
end-to-end loss for speaker verification,” in Proc. ICASSP .
IEEE, 2018, pp. 4879–4883.
"
"With the increasing clinical applications of medical image segmentation, deep learning-based models have emerged as the mainstream solutions. Segmentation of volumetric medical images remains a challenging task due to high computational costs and low spatial perception capabilities of 2D-based models, while 3D-based methods tend to perform poorly on anisotropic voxel spacing images and suffer from overfitting risks with limited data sizes. Some 2.5D-based models try to overcome these limitations by fusing inter-slice features with 2D-based ones. However, most existing 2.5D-based methods focus on designing architectures to enhance the performances of intra-slice representations. They overlook the importance of exploiting inter-slice information, and encounter difficulties in distinguishing features of each slice during inter-slice fusion, leading to suboptimal segmentation performances. In this work, we propose a novel 2.5D medical image segmentation model, MOSformer, which can make full use of inter-slice information while addressing the above-mentioned issues. To improve the feature distinguishability and consistency, MOSformer incorporates dual encoders, where one is for target slices and the other for neighborhood slices. A momentum update is adopted for the neighborhood slice encoder to maintain the consistency of slice representations. Furthermore, an IF-Swin transformer is developed for efficient inter-slice fusion at multi-scale feature maps. Experimental results on three benchmarks demonstrate the superiority and competitiveness of our proposed MOSformer.","Medical image segmentation is a pivotal technique in various clinical applications, such as computer-aided diagnoses, therapy planning, image-guided interventions, and medical robotics. U-Net and its variants have achieved tremendous success in different medical imaging modalities. However, accurate and efficient segmentation of 3D medical images still remains a challenging task. To overcome the limitations of 2D-based and 3D-based segmentation methods, 2.5D-based models have been developed to explore the inter-slice information. However, prevailing 2.5D-based methods fail to effectively learn and utilize inter-slice information, resulting in suboptimal segmentation performances. To address this issue, we propose a novel 2.5D-based segmentation model, namely MOSformer, which can make full use of inter-slice information for accurate medical image segmentation. MOSformer employs dual encoders to enhance feature distinguishability and consistency among different slices. An IF-Swin transformer module is designed to efficiently establish relationships among inputs at the feature level via inter-slice self-attention.","In the past decade, CNN-based models, notably U-Net, have dominated various medical image tasks. Milletari et al. introduced V-Net for MRI segmentation. Schlemper et al. proposed an attention gate model that can learn to focus on target structures of varying shapes and sizes. Isensee et al. developed nnUNet, which can automatically configure itself to learn features for segmentation. Chen et al. suggested a 2.5D segmentation framework combining with k-UNet and bi-directional convolutional LSTM (BDC-LSTM) to integrate inter-slice information. Zhang et al. proposed an attention fusion module to refine segmentation results by fusing the information of adjacent slices. Li et al. adopted a 2.5D coarse-to-fine architecture, which benefits from the inter-slice context knowledge. Recently, transformer-based segmentation models are also explored. Chen et al. combined U-Net and transformer, where transformer encodes feature maps from CNN encoder to extract global contexts for the decoder to generate segmentation results. Cao et al. are the first to employ a pure transformer architecture for medical image segmentation. Huang et al. introduced MISSformer, which incorporates an encoder-decoder architecture built on enhanced transformer blocks. You et al. presented CASTformer with a class-aware transformer module to better capture discriminative regions of target objects. UNETR pioneered the use of a transformer-based encoder to learn global contexts from volumetric data. CoTr introduced a deformable self-attention mechanism to reduce computational complexity. nnFormer is an interleave architecture, where convolution layers encode precise spatial information and transformer layers fully explore global dependencies. Guo et al. adopted 2D U-Net as the backbone and fused inter-slice information via a transformer at the bottom layer of the encoder. Yan et al. proposed AFTer-UNet with an axial fusion mechanism based on transformer to fuse intra- and inter-slice contextual information. Hung et al. introduced a novel cross-slice attention mechanism based on transformer to learn cross-slice information at multiple scales.nannannan","The proposed MOSformer model consists of dual encoders with a momentum update, an IF-Swin transformer module, and a segmentation head. The dual encoders are employed to extract multi-scale features of input slices, where one encoder is for the target slice and the other is for neighborhood slices. A momentum update approach is adopted for the neighborhood slice encoder to maintain the consistency of slice representations. The IF-Swin transformer module is designed to efficiently capture inter-slice information. It consists of two successive IF-Swin transformers with different window partitioning configurations. The window-based self-attention is expanded to the inter-slice dimension, promoting target slice pixels to learn intra- and inter-slice contexts. Finally, the segmentation head is utilized to generate segmentation maps of target slices.nan","The proposed MOSformer model is evaluated on three benchmark datasets: Synapse, ACDC, and AMOS. The experimental results demonstrate the superiority of MOSformer over state-of-the-art methods. On the Synapse dataset, MOSformer achieves an average DSC of 85.63% and an average HD95 of 13.40 mm. It outperforms the best 2D-based model, CASTformer, by a large margin (+3.08% DSC and -9.33 mm HD95). MOSformer also offers at least +4.61% DSC gains over the 2.5D-based model, AFTer-UNet. Compared to the 3D-based model, nnFormer, MOSformer still has competitive performances, surpassing four of the most widely recognized models and achieving comparable performances to nnFormer. It is worth noting that MOSformer obtains better DSC than nnFormer in four organs (half of the categories), including gallbladder (+1.73%), left kidney (+3.75%), spleen (+1.78%), and stomach (+1.04%). On the ACDC dataset, MOSformer achieves an average DSC of 92.19%, which is the best result among all the compared methods. It is also superior to the best 2D-based model, UCTransNet, by 1.98% in DSC. On the AMOS dataset, MOSformer achieves an average DSC of 85.43%, which is the best result among all the compared methods. It outperforms the best 2D-based model, UCTransNet, by 3.11% in DSC. Moreover, MOSformer offers at least +2.75% DSC gains over the 2.5D-based model, CAT-Net. Compared to the 3D-based model, nnFormer, MOSformer has a 6.77% improvement in DSC.","In this work, we have proposed a novel 2.5D-based medical image segmentation model, MOSformer, which can effectively make use of inter-slice information for accurate segmentation. MOSformer employs dual encoders with a momentum update to enhance feature distinguishability and consistency among different slices. An IF-Swin transformer module is designed to efficiently establish relationships among inputs at the feature level via inter-slice self-attention. The experimental results on three benchmark datasets demonstrate the superiority and competitiveness of our proposed MOSformer. In the future, we plan to extend MOSformer to other downstream medical analysis tasks, such as organ detection and tracking.",MOSformer: Momentum encoder-based inter-slice fusion transformer for medical image segmentation,"De-Xing Huang, Xiao-Hu Zhou, Xiao-Liang Xie, Shi-Qi Liu, Zhen-Qiu Feng, Mei-Jiang Gui, Hao Li, Tian-Yu Xiang, Xiu-Ling Liu, Zeng-Guang Hou","JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
1
MOSformer: MOmentum Encoder-based
Inter-Slice Fusion Transformer for
Medical Image Segmentation
De-Xing Huang, Xiao-Hu Zhou, Member, IEEE, Xiao-Liang Xie, Member, IEEE, Shi-Qi Liu, Zhen-Qiu Feng,
Mei-Jiang Gui, Hao Li, Tian-Yu Xiang, Xiu-Ling Liu, and Zeng-Guang Hou, Fellow, IEEE
Abstract—Medical image segmentation takes an important
position in various clinical applications. Deep learning has
emerged as the predominant solution for automated segmentation
of volumetric medical images. 2.5D-based segmentation models
bridge computational efficiency of 2D-based models and spatial
perception capabilities of 3D-based models. However, prevailing
2.5D-based models often treat each slice equally, failing to
effectively learn and exploit inter-slice information, resulting
in suboptimal segmentation performances. In this paper, a
novel MOmentum encoder-based inter-Slice fusion transformer
(MOSformer) is proposed to overcome this issue by leveraging
inter-slice information at multi-scale feature maps extracted by
different encoders. Specifically, dual encoders are employed to
enhance feature distinguishability among different slices. One
of the encoders is moving-averaged to maintain the consis-
tency of slice representations. Moreover, an IF-Swin transformer
module is developed to fuse inter-slice multi-scale features.
The MOSformer is evaluated on three benchmark datasets
(Synapse, ACDC, and AMOS), establishing a new state-of-the-art
with 85.63%, 92.19%, and 85.43% of DSC, respectively. These
promising results indicate its competitiveness in medical image
segmentation. Codes and models of MOSformer will be made
publicly available upon acceptance.
Index Terms—Medical image segmentation, transformer, inter-
slice, momentum encoder.
I. INTRODUCTION
M
EDICAL image segmentation plays a vital role in
modern clinical applications, such as computer-aided
This work was supported in part by the National Natural Science Foun-
dation of China under Grant 62373351, Grant 62222316, Grant U20A20224,
U1913601, Grant 62073325, Grant 61720106012, Grant 62003198; in part
by the Youth Innovation Promotion Association of Chinese Academy of
Sciences (CAS) under Grant 2020140; and in part by the CIE-Tencent
Robotics X Rhino-Bird Focused Research Program. (Corresponding author:
Xiao-Hu Zhou and Zeng-Guang Hou)
D.-X. Huang, X.-H. Zhou, X.-L. Xie, S.-Q. Liu, M.-J. Gui, H. Li, and
T.-Y. Xiang are with the State Key Laboratory of Multimodal Artificial
Intelligence Systems, Institute of Automation, Chinese Academy of Sciences,
Beijing 100190, China, and also with the School of Artificial Intelligence,
University of Chinese Academy of Sciences, Beijing 100049, China (e-mail:
huangdexing2022@ia.ac.cn; xiaohu.zhou@ia.ac.cn).
Z.-G. Hou is with the State Key Laboratory of Multimodal Artificial
Intelligence Systems, Institute of Automation, Chinese Academy of Sciences,
Beijing 100190, China, also with the CAS Center for Excellence in Brain
Science and Intelligence Technology, Beijing 100190, China, also with the
School of Artificial Intelligence, University of Chinese Academy of Sciences,
Beijing 100049, China, and also with the Joint Laboratory of Intelligence
Science and Technology, Institute of Systems Engineering, Macau University
of Science and Technology, Taipa, Macau (e-mail:zengguang.hou@ia.ac.cn).
X.-L. Liu is with the Key Laboratory of Digital Medical Engineering of
Hebei Province, College of Electronic and Information Engineering, Hebei
University, Baoding 071002, China.
Slice i
Slice i+1
Slice i-1
Fig. 1.
Explanation of inter-slice fusion. Due to the significant intra-class
variance of stomach (labeled in dark blue), it is difficult for models to
categorize the orange pixel of slice i accurately. By introducing inter-slice
information, the pixel can obtain richer contexts.
diagnoses [1], [2], [3] therapy planning [4], [5], image-guided
interventions [6], [7], and medical robotics [8], [9]. U-Net [10]
and its variants [11], [12], [13] have been widely used in this
field and have achieved tremendous success in different med-
ical imaging modalities [14]. However, accurate and efficient
segmentation of 3D medical images still remains a non-trivial
task [15].
Current mainstream segmentation methods can be classified
into two categories: 2D-based and 3D-based methods [16]. 2D-
based methods split 3D images into 2D slices and segment
them individually, while 3D-based methods directly generate
segmentation results of entire 3D images. Despite impressive
performances achieved by state-of-the-art methods [17], they
still exhibit some limitations. Most 2D-based methods focus on
architecture design to enhance intra-slice representations for
better performances, such as incorporating attention modules
[18] or adopting transformers [19], [20]. However, these meth-
ods overlook inter-slice cues, which is also crucial for accurate
segmentation. In contrast, 3D-based methods can capture
intra- and inter-slice information for segmentation but demand
substantial GPU memory and computational resources. Addi-
tionally, they tend to perform poorly in images with anisotropic
voxel spacing since they are primarily designed for 3D images
with nearly isotropic voxel spacing [21], [22]. Furthermore,
due to the limited size of 3D medical image datasets, the data
distribution is often sparse, making 3D-based methods have a
arXiv:2401.11856v1  [eess.IV]  22 Jan 2024
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
2
higher risk of overfitting compared to 2D-based methods [16].
In order to combine advantages of 2D-based and 3D-based
methods, some works have been done to explore 2.5D-based
segmentation models [16]. The main idea of these methods is
fusing inter-slice information into 2D-based models. Fig. 1
is an example. It is difficult to classify the orange circle
according to intra-slice information of slice i due to significant
intra-class variance between stomach filled with water and
air. By expanding views to slice i − 1 and i + 1, partial 3D
structures of the stomach can be perceived, facilitating 2D
models accurately locate and categorize the orange circle to
stomach. The most direct way to achieve inter-slice fusion
is by concatenating slices as multi-channel inputs. However,
it is inefficient, making models challenging to extract useful
features for target slices [16]. Therefore, some studies focus
on exploring “smart” ways of inter-slice fusion. Most of them
formulate 2D slices as time sequences and adopt recurrent neu-
ral network (RNN) [23], transformers [24], [25] or attention
mechanisms [26] to fuse inter-slice information.
While current 2.5D-based methods have achieved impres-
sive segmentation results, they encounter difficulties in distin-
guishing each slice during inter-slice fusion and cannot learn
reliable inter-slice representations for segmentation [16]. This
issue arises because these methods utilize a single encoder for
processing all input slices, leading to the same distribution in
the embedding space, as shown in Fig. 4 (a).
To address the above issue, a novel 2.5D-based segmen-
tation model, MOSformer, MOmentum encoder-based inter-
Slice fusion transformer is proposed to effectively leverage
inter-slice information for 3D medical image segmentation.
MOSformer follows the design of U-shape architecture [10].
In order to enhance feature distinguishability of each slice,
dual encoders are utilized in our model. One for target slices
and the other for neighborhood slices. Parameters of the target
slice encoder are updated by back-propagation, and parameters
of the neighborhood slice encoder are updated using a mo-
mentum update. Therefore, features can hold distinguishability
and consistency, promoting inter-slice fusion. Furthermore,
building upon Swin transformer [27], an efficient Inter-slice
Fusion Swin transformer (IF-Swin) is proposed for capturing
inter-slice cues at multi-scale feature maps.
The main contributions of this work are summarized as
follows:
• A novel 2.5D-based model MOSformer is proposed to
fully exploit inter-slice information for 3D medical image
segmentation.
• To make slice features distinguishable and consistent,
dual encoders with a momentum update are introduced.
Moreover, IF-Swin transformer is developed to efficiently
establish relationships among inputs at feature level via
inter-slice self-attention.
• State-of-the-art segmentation performances have been
achieved by our model on three benchmark datasets,
including Synapse, ACDC, and AMOS.
The remainder of this paper is organized as follows: Sec-
tion II briefly reviews current segmentation methods. Sec-
tion III depicts the proposed model in detail. Section IV in-
troduces model configurations and datasets. The experimental
results are presented in Sec V. Section VI discusses the factors
that affect segmentation performances of our MOSformer.
Finally, Section VII concludes this article.
II. RELATED WORK
In this section, methodologies used in 3D medical im-
age segmentation are briefly reviewed. These methods are
categorized into two categories based on whether they use
transformer blocks.
A. CNN-based segmentation models
In the past decade, CNN-based models, especially U-Net
[10], have taken dominant positions in various medical image
tasks [28], [29]. The encoder-decoder architecture with multi-
scale skip connections of U-Net fully uses low-level and high-
level features for accurate segmentation. Many models have
been designed for 3D medical image segmentation. Milletari
et al. [30] proposed a 3D CNN model, V-Net, for MRI seg-
mentation. With the residual connections at each scale, it can
converge quickly [30]. Schlemper et al. [18] introduced an at-
tention gate (AG) model that can learn to focus on target struc-
tures of varying shapes and sizes. Isensee et al. [31] proposed a
generalized framework nnUNet, which is able to automatically
configure itself to learn features for segmentation. Chen et
al. [23] suggested a 2.5D segmentation framework combining
with k-UNet and bi-directional convolutional LSTM (BDC-
LSTM) to integrate inter-slice information. Zhang et al. [26]
proposed an attention fusion module to refine segmentation
results by fusing the information of adjacent slices. Li et al.
[32] adopted a 2.5D coarse-to-fine architecture, which bene-
fits from the inter-slice context knowledge from consistency
context similarity and discrepancy context. Although these
methods have improved the abilities of context modeling to
some extent, their performances are stranded by CNN, which
has limited receptive fields [17], [19], [33].
B. Transformer-based segmentation models
Recently, with the tremendous success of vision transformer
(ViT) [34] in various computer vision tasks [34], [35], [36],
many works have explored using transformers in medical
image segmentation. Compared with CNNs, transformers can
capture long-range dependencies by sequence modeling and
multi-head self-attention (MHSA) [34], achieving better seg-
mentation performances. Chen et al. [19] proposed TransUNet,
combing U-Net and transformer, where transformer encodes
feature maps from CNN encoder to extract global contexts
for the decoder to generate segmentation results. Cao et al.
[20] are the first to employ a pure transformer architecture for
medical image segmentation. Convolutional layers in U-Net
are all replaced by Swin transformer blocks [27]. However,
this architecture does not obtain better performances [37].
Huang et al. introduced MISSformer [38], which incorporates
an encoder-decoder architecture built on enhanced transformer
blocks. These blocks are connected through the ReMixed
transformer context bridge, enhancing the model’s ability
to capture discriminative details. You et al. [37] presented
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
3
Target slice
Neighborhood slices
…
Momentum encoder
Encoder
IF-Swin
transformer
IF-Swin
transformer
IF-Swin
transformer
IF-Swin
transformer
/16
/8
/4
/2
Output
Multi-scale features
Upsample
Conv+BN+ReLU *2
Segmentation head
Deep supervision
Concentrate
Fig. 2. The architecture of MOSformer. It consists of dual encoders: the momentum encoder for extracting neighborhood slice information and the encoder
for extracting target slice information. IF-Swin transformer is designed to achieve inter-slice fusion at multi-scale features. After that, fused features are fed
to the CNN decoder to generate segmentation maps of target slices.
CASTformer with a class-aware transformer module to better
capture discriminative regions of target objects. Moreover,
they utilized adversarial learning to boost segmentation ac-
curacies. However, the 2D-based methods mentioned above
face limitations in leveraging inter-slice information, which
hinders their potential for further performance improvements.
Some attempts have been made to build 3D-based transformer
segmentation models. UNETR [39] pioneered the use of a
transformer-based encoder to learn global contexts from volu-
metric data. CoTr [33] introduced a deformable self-attention
mechanism to reduce computational complexity. However,
simplifying self-attention may cause contextual information
loss [24]. nnFormer [40] is an interleave architecture, where
convolution layers encode precise spatial information and
transformer layers fully explore global dependencies. Similar
to Swin transformer [27], a computationally-efficient way to
calculate self-attention is proposed in nnFormer. Recently,
2.5D-based transformer models are also explored. Guo et al.
[41] adopted 2D U-Net as the backbone and fused inter-
slice information via a transformer at the bottom layer of
the encoder. Yan et al. [24] proposed AFTer-UNet with an
axial fusion mechanism based on transformer to fuse intra- and
inter-slice contextual information. Hung et al. [25] introduced
a novel cross-slice attention mechanism based on transformer
to learn cross-slice information at multiple scales.
Different from the above 2.5D-based methods which use one
encoder to process all slices, dual encoders with a momentum
update are utilized in our model for slice feature extraction to
facilitate the model’s ability to distinguish each slice during
inter-slice fusion.
III. METHOD
A. Overall architecture
The detailed architecture of MOSformer is shown in Fig. 2.
It is a hybrid encoder-decoder model, combining the advan-
tages of CNNs and transformers [42]. x0 ∈ RC×H×W is the
input of the encoder and is the target slice for segmentation,
where C, H, W denote the channel, height, and width
of the input image. {xi}k ∈ RC×H×W are inputs of the
momentum encoder and are neighborhood slices of x0, where
k ∈ [−s, s]/{0} and s represents the s-th neighborhood of x0.
Finally, the model outputs the segmentation map of x0.
ResNet50 [43] is selected as the encoder to extract multi-
scale features of input slices. Dual encoders with a momentum
update utilized in MOSformer can strengthen the feature dis-
tinguishability and maintain feature consistency. This enables
IF-transformer to capture more precise inter-slice contexts,
facilitating the learning of discriminative representations for
segmentation. Furthermore, IF-Swin transformer is adopted
at different scales (1/2, 1/4, 1/8, 1/16) of encoder outputs
to learn multi-scale features. Then these fused features are
provided to the decoder via skip connections. The final seg-
mentation predictions are derived via a segmentation head
(1 × 1 convolutional layer).
B. Dual encoders with a momentum update
Conventional 2.5D-based methods utilize one encoder to
process all input slices and then fuse inter-slice information at
feature level. This may make models challenging to distinguish
each slice [16] since they are from the same distribution, as
illustrated in Fig. 4 (a). Models cannot focus on capturing
discriminative inter-slice representations for target slices. A
simple idea is to use two independently updated encoders
to process neighborhood slices and target slices, respectively.
However, this approach achieves suboptimal performances in
experiments conducted in Section V-B. We hypothesize that
such unsatisfactory performances are caused by two indepen-
dently update encoders that reduce slice features’ consistency.
Inspired by [44], a momentum update approach is adopted
to overcome this issue. Parameters of target slice encoder
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
4
LN
W-MSA
MLP
LN
LN
SW-MSA
MLP
LN
Target slice
Adjacent slice 2
Adjacent slice 1
Target slice
Adjacent slice 2
Adjacent slice 1
𝒇!""#
$%#, ⋯ , 𝒇!""&
$%#
𝒇!%&
$%#, ⋯ , 𝒇!%#
$%#
𝒇!
$%#
$𝒇!
$
𝒇!
$
$𝒇!
$""#
𝒇!
$""#
Fig. 3. Schematic of IF-Swin transformer module. It has two successive IF-Swin transformers with different window partitioning configurations. The window-
based self-attention is expanded to the inter-slice dimension, promoting target slice pixels to learn intra- and inter-slice contexts.
Enc.
Mom.
Enc.
(a) Conventional paradigm
(b) Our proposed paradigm  
Neighborhood slice
Target slice
Fig. 4.
Comparison between conventional feature extraction paradigm of
2.5D-based segmentation models and our proposed paradigm. (a) Conven-
tional approaches use one encoder to encode all input slices. Therefore, distri-
butions of target slices and neighborhood slices are the same. (b) Our proposed
paradigm adopts two encoders to process target and neighborhood slices,
respectively. Momentum update is used in the neighborhood slice encoder.
Hence, distributions of target and neighborhood slices are distinguishable and
consistent. (Enc.: Encoder; Mom.: Momentum encoder)
θ1 are updated by standard back-propagation. Parameters of
neighborhood slice encoder θ2 are updated by:
θ2 ← m ∗ θ2 + (1 − m) ∗ θ1
(1)
where m ∈ [0, 1) is a momentum coefficient. m should be
relatively small to make features consistent (m is set to 0.1
in our default configuration). Under this circumstance, slice
features are distinguishable and consistent, as shown in Fig. 4
(b) and Fig. 8 (c), facilitating the model in effectively capturing
inter-slice information.
C. Inter-slice fusion transformer
Inspired by [27], [42], Inter-slice Fusion Swin transformer
(IF-Swin) is proposed to fuse inter-slice information, as shown
in Fig. 3. Unlike the vanilla swin transformer [27], windows of
IF-Swin are expanded to inter-slice dimension. Additionally,
IF-Swin is applied at multiple scales to achieve inter-slice
fusion, which is different to [42].
Inputs of the proposed IF-Swin transformer are feature
maps {fi−s, · · · , fi, · · · , fi+s}k extracted by the encoder and
the momentum encoder, where k represents the k-th scale
(k = 1, 2, 3, 4). s is set to 1 in our default configuration.
Therefore, the model uses adjacent slices of target slices x0
as additional inputs. Different from standard self-attention [45]
with quadratic complexity, our approach only calculates self-
attention within the local window. As shown in the left part of
Fig. 3, feature maps are portioned to several non-overlapping
windows1. As mentioned before, windows are expanded to
inter-slice dimension. Therefore, the orange pixel in the target
slice can capture not only intra-slice information (yellow
pixels) but also perceives inter-slice information (green pixels).
However, the local window-based self-attention lacks con-
nections across windows, degrading its feature formulation
power. Following [27], a shifted window partitioning strategy
is introduced, allowing each pixel to receive broader views
from intra- and inter-slices, as shown in Fig. 3. The first trans-
former module adopts a regular window partition approach,
and the feature map is evenly divided into 2 × 2 windows of
size 2 × 2 (M = 2). The second transformer module uses a
different partitioning configuration. Windows of the preceding
layer are displaced by
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
5
d and d0 are embedding dimensions of query/key and value.
In practice, d is equal to d0. B represents the position
embedding matrix, and values are taken from the bias matrix
ˆ
B ∈ R(2M−1)×(2M−1).
D. Loss function
Following previous methods [33], [39], [40], our model is
trained end-to-end using the deep supervision strategy [46]. As
illustrated in Fig. 2, final segmentation results are generated by
the segmentation head (1×1 convolutional layer). Additionally,
two smaller resolutions of decoder outputs are selected as aux-
iliary supervision signals. The deep supervision path in Fig. 2
consists of an upsample layer and a 1 × 1 convolutional layer.
Therefore, the loss function can be formulated as follows:
Lseg = λ1L{H,W } + λ2L{ H
2 , W
2 } + λ3L{ H
4 , W
4 }
(4)
where λ1, λ2, and λ3 are 1
2, 1
4, and 1
8, respectively. L{h,w}
represents the loss function on h × w resolution. It is a linear
combination of cross-entropy loss LCE and Dice loss LDSC:
L{h,w} = α1LCE + α2LDSC
(5)
where α1 and α2 are 0.8 and 1.2, respectively.
IV. EXPERIMENTAL SETUP
A. Datasets
To thoroughly compare MOSformer to previous methods,
we conduct experiments on three challenging benchmarks: the
Synapse multi-organ segmentation dataset [47], the automated
cardiac diagnosis challenge (ACDC) dataset [48], and the
abdominal organ segmentation (AMOS) dataset [49].
1) Synapse for multi-organ segmentation:
This dataset
consists of 30 abdominal CT scans with 8 organs (aorta,
gallbladder, left kidney, right kidney, liver, pancreas, spleen,
and stomach). Each volume has 85
∼
198 slices of
512 × 512
pixels,
with
a
voxel
spatial
resolution
of
([0.54 ∼ 0.54] × [0.98 ∼ 0.98] × [2.5 ∼ 5.0]) mm3. Follow-
ing the splits adopted in TransUNet [19], the dataset is divided
into 18 training cases and 12 evaluation cases.
2) ACDC for automated cardiac diagnosis challenge: The
ACDC dataset includes cardiac MRI images of 100 patients
from real clinical exams with manual annotations of left
ventricle (LV), right ventricle (RV), and myocardium (Myo).
Consistent with TransUNet [19], the dataset is split into 70
training cases, 10 validation cases, and 20 evaluation cases.
3) AMOS for abdominal organ segmentation: The AMOS
dataset is a comprehensive abdominal organ segmentation
dataset that includes patient annotations of 15 abdominal or-
gans (aorta, bladder, duodenum, esophagus, gallbladder, infe-
rior vena cava, left adrenal gland, left kidney, liver, pancreas,
prostate/uterus, right adrenal gland, right kidney, spleen, and
stomach) from different centers, modalities, scanners, phases,
and diseases. Only CT scans are utilized in our experiments,
consisting of 200 training cases and 100 evaluation cases.
B. Implementation details
All experiments are implemented based on PyTorch 1.12.0,
Python 3.8, and Ubuntu 18.04. Our models are trained on
a single Nvidia A6000 GPU with 48GB of memory. The
same model configurations are utilized on three datasets. Input
medical images are resized into 224×224 for fair comparison.
SGD optimizer with momentum of 0.9 and weight decay of
1e−4 is adopted to train our model for 300 epochs. The batch
size is set to 24. A cosine learning rate scheduler with five
epochs of linear warm-up is used during training, and the
maximum and minimum learning rates are 3e−2 and 5e−3,
respectively.
C. Evaluation metrics
Segmentation performances of models are measured based
on two metrics: Dice similarity score (DSC), and 95% Haus-
dorff distance (HD95).
DSC is utilized to evaluate overlaps between ground truths
and segmentation results and are defined as follows:
DSC(P, G) = 2 × |P ∩ G|
|P| + |G|
(6)
where P refers to model predictions and G refers to ground
truths.
HD95 is adopted to measure the 95% distance between
boundaries of model predictions and ground truths. It is
defined as follows:
HD95 = max {dP G, dGP }
(7)
where dP G is the maximum 95% distance between model
predictions and ground truths. dGP is the maximum 95%
distance between ground truths and model predictions.
V. RESULTS
A. Comparison with state-of-the-arts
1) Multi-organ segmentation (Synapse): The quantitative
results of state-of-the-art models and our MOSformer are
presented in Table I. Our MOSformer achieves 85.63%
DSC and 13.40 mm HD95 on this dataset. By leveraging
inter-slice information, MOSformer is able to surpass the
best 2D-based model, i.e., CASTformer [37], by a large
margin (+3.08% DSC and -9.33 mm HD95). Enjoying the
benefits of distinguishable and consistent inter-slice features,
MOSformer offers at least +4.61% DSC gains over the
2.5D-based model, AFTer-UNet [24]. Compared to the 3D-
based model, MOSformer still has competitive performances,
surpassing four of the most widely recognized models and
achieving comparable performances to nnFormer [40]. It
should be noted that MOSformer obtains better DSC than
nnFormer in four organs (half of the categories), including
gallbladder (+1.73%), left kidney (+3.75%), spleen (+1.78%),
and stomach (+1.04%). Among these organs, gallbladder and
stomach are two of the most difficult organs to segment since
gallbladder is very small and boundaries between gallbladder
and liver is unclear while stomach has a significant intra-
class variance, as illustrated in Fig. 1. This reveals that our
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
6
TABLE I
COMPARISON WITH THE STATE-OF-THE-ART MODELS ON THE MULTI-ORGAN SEGMENTATION (SYNAPSE) DATASET. THE BEST RESULTS ARE
HIGHLIGHTED IN BLUE AND THE SECOND-BEST RESULTS ARE HIGHLIGHTED IN RED. THE EVALUATION METRICS ARE DSC AND HD95, CONSISTING
WITH TRANSUNET [19]. MOREOVER, DSC OF EACH ORGAN IS REPORTED IN THIS TABLE. ‡ MEANS THE RESULTS ARE BORROWED FROM [40].
Dimension
Method
Average
Aotra
Gallbladder
Kidney (L)
Kidney (R)
Liver
Pancreas
Spleen
Stomach
DSC (%) ↑
HD95 (mm) ↓
2D
UNet [10] [MICCAI’15]
76.85
39.70
89.07
69.72
77.77
68.60
93.43
53.98
86.67
75.58
AttnUNet [18] [MedIA’19]
77.77
36.02
89.55
68.88
77.98
71.11
93.57
58.04
87.30
75.75
TransUNet [19] [arXiv’21]
77.48
31.69
87.23
63.13
81.87
77.02
94.08
55.86
85.08
75.62
MISSFormer [38] [TMI’22]
81.96
18.20
86.99
68.65
85.21
82.00
94.41
65.67
91.92
80.81
SwinUNet [20] [ECCVW’22]
79.12
21.55
85.47
66.53
83.28
79.61
94.29
56.58
90.66
76.60
MT-UNet [50] [ICASSP’22]
78.59
26.59
87.92
64.99
81.47
77.29
93.06
59.46
87.75
76.81
UCTransNet [51] [AAAI’22]
78.23
26.75
88.86
66.97
80.19
73.18
93.17
56.22
87.84
79.43
CASTformer [37] [NeurIPS’22]
82.55
22.73
89.05
67.48
86.05
82.17
95.61
67.49
91.00
81.55
HiFormer [52] [WACV’23]
80.39
14.70
86.21
65.69
85.23
79.77
94.61
59.52
90.99
81.08
3D
V-Net [30] [3DV’16]
68.81
-
75.34
51.87
77.10
80.75
87.84
40.05
80.56
56.98
CoTr‡ [33] [MICCAI’21]
80.78
19.15
85.42
68.93
85.45
83.62
93.89
63.77
88.58
76.23
UNETR‡ [39] [WACV’22]
79.56
22.97
89.99
60.56
85.66
84.80
94.46
59.25
87.81
73.99
SwinUNETR‡ [53] [MICCAIW’22]
83.51
14.78
90.75
66.72
86.51
85.88
95.33
70.07
94.59
78.20
nnFormer [40] [TIP’23]
86.57
10.63
92.04
70.17
86.57
86.25
96.84
83.35
90.51
86.83
2.5D
AFTer-UNet [24] [WACV’22]
81.02
-
90.91
64.81
87.90
85.30
92.20
63.54
90.99
72.48
MOSformer [Ours]
85.63
13.40
88.95
71.90
90.32
83.58
95.96
74.14
92.29
87.87
Aorta
Gallbladder 
Left kidney
Right kidney
Liver
Pancreas
Spleen
Stomach
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
Fig. 5.
Visual comparisons with current state-of-the-art methods on the multi-organ segmentation (Synapse) dataset. (a) Ground truth; (b) UNet [10]; (c)
TransUNet [19]; (d) MISSformer [38]; (e) SwinUNet [20]; (f) UCTransNet [51]; (g) HiFormer [52]; (h) UNETR [39]; (i) nnFormer [40]; (j) MOSformer
(Ours).
MOSformer has strong abilities to deal with unclear bound-
aries and comprehensive understandings of organ structures.
The qualitative results of some models on the multi-organ
segmentation (Synapse) dataset are shown in Fig. 5. For
case 1, due to the significant intra-class variance of stomach,
many baseline methods cannot locate the stomach precisely
(e.g., UCTransNet, UNTER, and nnFormer) and have some
misclassified pixels (e.g., TransUNet, MISSformer, and
SwinUNet). For cases 2 and 3, our MOSformer can produce
more clear boundaries than other models and reduce the
number of false positive predictions. For example, in case
3, boundaries of the gallbladder and the liver predicted by
nnFormer are blurry, and UNETR [39] produces a large
number of wrong spleen pixels outside the spleen.
2) Automated cardiac diagnosis challenge (ACDC): To
further prove generalizing performances, our model is eval-
uated on the automated cardiac diagnosis challenge (ACDC)
dataset. It should be noted that MRI images in this dataset
can be considered anisotropic since they have high in-plane
image resolution (e.g., 1.37∼1.68 mm) and low through-
plane resolution (e.g., 5 mm) [48]. Experimental results are
summarized in Table II. Compared to state-of-the-art methods
(2D, 2.5D, and 3D-based), our MOSformer achieves the best
performances with 92.19% DSC. Thus, it reveals that our
2.5D-based MOSformer is more advantageous to process
anisotropic data compared to 3D-based models. Fig. 7 shows
qualitative comparisons for different methods on this dataset.
It can be observed that our MOSformer can locate anatom-
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
7
Spleen
Right kidney
Left kidney
Gallbladder
Esophagus
Liver
Stomach
Aotra
Inferior vena cava
Pancreas
Right adrenal gland
Left adrenal gland 
Duodenum
Bladder
Prostate/uterus
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 6. Visual comparisons with current state-of-the-art methods on the abdominal organ segmentation (AMOS) dataset. (a) Ground truth; (b) UNet [10]; (c)
TransUNet [19]; (d) UNETR [39]; (e) nnFormer [40]; (f) MOSformer (Ours).
TABLE II
COMPARISON WITH THE STATE-OF-THE-ART MODELS ON THE
AUTOMATED CARDIAC DIAGNOSIS CHALLENGE (ACDC) DATASET. THE
BEST RESULTS ARE HIGHLIGHTED IN BLUE AND THE SECOND-BEST
RESULTS ARE HIGHLIGHTED IN RED. WE ONLY REPORT DSC IN THIS
TABLE, FOLLOWING THE EVALUATION SETTING OF TRANSUNET [19].
MOREOVER, DSC OF EACH ANATOMICAL STRUCTURE IS REPORTED IN
THIS TABLE. ‡ MEANS THE RESULTS ARE BORROWED FROM [40]. ∗ MEANS
THE BASELINES ARE IMPLEMENTED BY OURSELVES.
Dimension
Method
Average DSC (%) ↑
RV
Myo
LV
2D
UNet [10] [MICCAI’15]
87.60
84.62
84.52
93.68
AttnUNet [18] [MedIA’19]
86.90
83.27
84.33
93.53
TransUNet [19] [arXiv’21]
89.71
86.67
87.27
95.18
MISSFormer [38] [TMI’22]
91.19
89.85
88.38
95.34
SwinUNet [20] [ECCVW’22]
88.07
85.77
84.42
94.03
MT-UNet [50] [ICASSP’22]
90.43
86.64
89.04
95.62
UCTransNet∗ [51] [AAAI’22]
91.98
90.06
89.87
96.02
HiFormer∗ [52] [WACV’23]
90.40
88.24
87.63
95.30
3D
UNETR‡ [39] [WACV’22]
88.61
85.29
86.52
94.02
nnFormer [40] [TIP’23]
92.06
90.94
89.58
95.65
2.5D
CAT-Net∗ [25] [TMI’22]
90.02
86.05
88.75
95.27
MOSformer [Ours]
92.19
90.86
89.65
96.05
ical structures more accurately. Specifically, in case 3, many
models mistakenly classify regions outside the myocardium
into the right ventricle while MOSformer donot produce any
false positive predictions.
(a)
(b)
(c)
(d)
(e)
(f)
(g)
Right ventricle
Myocardium
Left ventricle
Fig. 7.
Visual comparisons with current state-of-the-art methods on the
automatic cardiac diagnosis challenge (ACDC) dataset. (a) Ground truth; (b)
UNet [10]; (c) TransUNet [19]; (d) MISSformer [38]; (e) UCTransNet [51];
(f) CAT-Net [25]; (g) MOSformer (Ours).
3) Abdominal organ segmentation (AMOS): Additionally, a
large dataset with 200 training cases and 100 evaluation cases
is adopted in our experiments. Overall results and individual
DSC on 15 organs are reported, as shown in Table III. Our
MOSformer maintains the first position with the best 14
organs and the second-best DSC in 1 organ. It is surprising that
our MOSformer offers 6.77% DSC improvements over 3D-
based nnFormer [40] while they have close performances on
the multi-organ segmentation (Synapse) dataset. Based on the
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
8
TABLE III
COMPARISON WITH THE STATE-OF-THE-ART MODELS ON THE ABDOMINAL ORGAN SEGMENTATION (AMOS) DATASET. THE BEST RESULTS ARE
HIGHLIGHTED IN BLUE AND THE SECOND-BEST RESULTS ARE HIGHLIGHTED IN RED. DSC IS UTILIZED AS EVALUATION METRIC. MOREOVER, DSC OF
EACH ORGAN IS REPORTED IN THIS TABLE. ∗ MEANS THE BASELINES ARE IMPLEMENTED BY OURSELVES.
Dimension
Method
Average DSC (%) ↑
Spleen
Kid. (R)
Kid. (L)
Gall.
Eso.
Liver
Stom.
Aotra
IVC
Panc.
Adr. (R)
Adr. (L)
Duo.
Blad.
Pros.
2D
UNet∗ [10] [MICCAI’15]
82.53
92.25
92.45
92.50
81.85
79.98
94.73
84.80
92.20
82.94
77.35
67.13
69.34
72.77
82.40
75.31
TransUNet∗ [19] [arXiv’21]
80.10
91.26
92.47
91.90
78.01
77.00
94.93
80.04
91.98
82.99
74.30
63.66
53.84
71.65
81.37
76.03
MISSFormer∗ [38] [TMI’22]
78.16
93.13
91.98
91.88
75.89
71.87
94.27
80.14
88.74
77.53
71.39
60.65
59.32
64.43
77.97
73.16
UCTransNet∗ [51] [AAAI’22]
82.34
93.37
92.32
91.90
77.09
79.77
94.78
85.95
91.77
82.84
77.44
65.88
68.98
71.36
83.93
77.71
HiFormer∗ [52] [WACV’23]
80.03
92.73
92.79
92.01
79.44
76.42
94.55
82.65
90.56
80.16
73.59
61.14
58.73
68.12
82.01
75.64
3D
UNETR∗ [39] [WACV’22]
78.07
93.38
93.00
92.28
73.17
69.72
94.86
73.25
90.82
80.20
73.44
65.19
60.69
65.46
74.10
71.49
nnFormer∗ [40] [TIP’23]
78.66
91.43
92.39
92.08
76.74
69.16
94.95
84.84
89.53
82.06
75.91
62.56
60.36
68.50
74.74
64.61
2.5D
MOSformer [Ours]
85.43
95.26
94.68
94.54
81.53
82.05
96.55
89.07
92.81
86.16
80.28
73.28
73.19
75.05
86.92
80.05
TABLE IV
ABLATION STUDIES OF EACH COMPONENT ON THE MULTI-ORGAN
SEGMENTATION (SYNAPSE) AND THE AUTOMATED CARDIAC DIAGNOSIS
CHALLENGE (ACDC) DATASETS. ENC-S: SINGLE ENCODER; ENC-D:
DUAL ENCODERS; ENC-MOM: DUAL ENCODERS WITH A MOMENTUM
UPDATE. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD. † MEANS THE
INPUT OF THE MODEL IS A SINGLE SLICE.
Model
Module
Synapse
ACDC
Enc-S
Enc-D
Enc-Mom
IF-Swin
DSC (%) ↑
DSC (%) ↑
Model-1†
!
82.42 (-3.21)
91.61 (-0.58)
Model-2
!
!
84.23 (-1.40)
92.04 (-0.15)
Model-3
!
!
84.93 (-0.70)
92.10 (-0.09)
MOSformer
!
!
85.63
92.19
above observation, it can be concluded that our MOSformer
is better than nnFormer. The visualization results are shown
in Fig. 6. Our MOSformer is able to accurately segment
organs of diverse shapes and sizes.
B. Ablation study
Extensive ablation studies are conducted on the multi-organ
segmentation (Synapse) and the automated cardiac diagnosis
challenge (ACDC) datasets to verify the effectiveness of the
momentum encoder and IF-Swin transformer. DSC is selected
as the default evaluation metric. Quantitative results are shown
in Table IV. It should be noted that the baseline, Model-1,
is a 2D-based model.
The efficacy of the momentum encoder: Two variants of
MOSformer are employed in this experiment: a) Model-2:
the encoder with a momentum update is removed, using a
single encoder to encode all input slices; b) Model-3: the mo-
mentum encoder is replaced by a normal encoder and two en-
coders are independently updated via back-propagation. From
the quantitative results presented in Table IV, it is evident that
our MOSformer outperforms two variants on two datasets.
These improvements stem from distinguishable and consistent
features produced by dual encoders with a momentum update
in MOSformer. Notably, both feature distinguishability and
consistency are essential. While Model-3 can also make slice
features distinguishable, independent-updated encoders disrupt
consistency among slices, impending inter-slice fusion. It can
also be discovered that feature distinguishability seems more
important than feature consistency, since Model-3 performs
better than Model-2 on two datasets.
TABLE V
MODEL PARAMETERS, FLOATING-POINT OPERATIONS PER SECOND
(FLOPS), AND THE AVERAGE TIME REQUIRED FOR SEGMENTING
INDIVIDUAL CASES. THE INPUT SIZE OF 2(.5)D-BASED AND 3D-BASED
MODELS ARE SET TO 224 × 224 AND 96 × 96 × 96, RESPECTIVELY. ∗
MEANS THE EXPERIMENTS ARE CONDUCTED ON THE test SET OF THE
MULTI-ORGAN SEGMENTATION (SYNAPSE) DATASET AND REPEATED FIVE
TIMES.
Dimension
Method
#params (M)
FLOPs (G)
Time∗ (s)
2D
U-Net [10] [MICCAI’15]
17.26
30.74
0.67
TransUNet [19] [arXiv’21]
93.23
24.73
5.69
MISSformer [38] [TMI’22]
35.45
7.28
7.20
3D
UNETR [39] [WACV’22]
92.62
82.63
5.39
nnFormer [40] [TIP’23]
149.13
246.10
10.13
2.5D
CAT-Net [25] [TMI’22]
220.16
121.83
21.34
MOSformer [Ours]
77.09
100.06
5.10
The strength of IF-Swin transformer: We remove IF-Swin
transformer module and obtain a baseline model (Model-1).
Compared with the baseline model, models incorporating IF-
Swin transformer (Model-2 and MOSformer) offer substan-
tial improvements on the Synapse (+1.81% and +3.21%) and
the ACDC dataset (+0.43% and +0.58%). With the help of IF-
Swin transformer, the model can learn richer representations
from inter-slice, enhancing feature discrimination.
C. Model complexity
Table V presents a comparison between five medical image
segmentation models and our MOSformer across various
dimensions, including model parameters, floating-point oper-
ations per second (FLOPs), and the average time required for
segmenting individual cases. MOSformer maintains a smaller
size (77.09 M) than that of 3D-based and 2.5D-based models.
Furthermore, MOSformer exhibits an inference speed only
half that of nnFormer [40], surpassing both TransUNet
[19] and MISSformer [38]. This indicates our MOSformer
achieves a favorable trade-off between model complexity and
segmentation performances.
VI. DISCUSSION
This article aims to develop an efficient model for robust 3D
medical image segmentation. Extensive experimental results
in Section V demonstrate the superiorities of MOSformer
and the effectiveness of each component. In this section,
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
9










JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
10
bility and consistency, beneficial for inter-slice fusion. Besides,
rich contexts can be captured via inter-slice self-attention in
the IF-Swin transformer module. The superior performances
to the state-of-the-art on three benchmarks have demonstrated
the MOSformer’s effectiveness and competitiveness. It will
be extended to other downstream medical analysis tasks in our
subsequent works.
REFERENCES
[1] E. Karami, M. S. Shehata, and A. Smith, “Adaptive polar active contour
for segmentation and tracking in ultrasound videos,” IEEE Trans.
Circuits Syst. Video Technol., vol. 29, no. 4, pp. 1209–1222, 2018.
[2] J. Ma et al., “AbdomenCT-1K: Is abdominal organ segmentation a
solved problem?” IEEE Trans. Pattern Anal. Mach. Intell., vol. 44,
no. 10, pp. 6695–6714, 2022.
[3] Z. Liu et al., “The devil is in the boundary: Boundary-enhanced
polyp segmentation,” IEEE Trans. Circuits Syst. Video Technol., 2024,
DOI:10.1109/TCSVT.2023.3348598.
[4] S. Nikolov et al., “Deep learning to achieve clinically applicable segmen-
tation of head and neck anatomy for radiotherapy,” arXiv:1809.04430,
2018.
[5] Y.-J. Zhou, X.-L. Xie, X.-H. Zhou, S.-Q. Liu, G.-B. Bian, and Z.-G. Hou,
“A real-time multifunctional framework for guidewire morphological
and positional analysis in interventional X-ray fluoroscopy,” IEEE Trans.
Cognit. Dev. Syst., vol. 13, no. 3, pp. 657–667, 2020.
[6] D.-X. Huang et al., “Real-time 2D/3D registration via CNN regres-
sion and centroid alignment,” IEEE Trans. Autom. Sci. Eng., 2024,
DOI:10.1109/TASE.2023.3345927.
[7] R.-Q. Li et al., “A unified framework for multi-guidewire endpoint
localization in fluoroscopy images,” IEEE Trans. Biomed. Eng., vol. 69,
no. 4, pp. 1406–1416, 2021.
[8] X.-H. Zhou, G.-B. Bian, X.-L. Xie, Z.-G. Hou, R.-Q. Li, and Y.-
J. Zhou, “Qualitative and quantitative assessment of technical skills
in percutaneous coronary intervention: In vivo porcine studies,” IEEE
Trans. Biomed. Eng., vol. 67, no. 2, pp. 353–364, 2019.
[9] X.-H.
Zhou
et
al.,
“Learning
skill
characteristics
from
ma-
nipulations,”
IEEE
Trans.
Neural
Networks
Learn.
Syst.,
2022,
DOI:10.1109/TNNLS.2022.3160159.
[10] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional net-
works for biomedical image segmentation,” in Proc. MICCAI, 2015, pp.
234–241.
[11] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++:
Redesigning skip connections to exploit multiscale features in image
segmentation,” IEEE Trans. Med. Imaging, vol. 39, no. 6, pp. 1856–
1867, 2019.
[12] H. Huang et al., “UNet 3+: A full-scale connected UNet for medical
image segmentation,” in Proc. ICASSP, 2020, pp. 1055–1059.
[13] H. Li, D.-H. Zhai, and Y. Xia, “Erdunet: An efficient residual double-
coding Unet for medical image segmentation,” IEEE Trans. Circuits
Syst. Video Technol., 2023, DOI:10.1109/TCSVT.2023.3300846.
[14] R. Wang et al., “Medical image segmentation using deep learning: A
survey,” IET Image Proc., vol. 16, no. 5, pp. 1243–1267, 2022.
[15] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, and X. Ding,
“Embracing imperfect datasets: A review of deep learning solutions for
medical image segmentation,” Med. Image Anal., vol. 63, p. 101693,
2020.
[16] Y. Zhang, Q. Liao, L. Ding, and J. Zhang, “Bridging 2D and 3D
segmentation networks for computation-efficient volumetric medical
image segmentation: An empirical study of 2.5D solutions,” Comput.
Med. Imaging Graphics, p. 102088, 2022.
[17] R. Azad et al., “Medical image segmentation review: The success of
U-Net,” arXiv:2211.14830, 2022.
[18] J. Schlemper et al., “Attention gated networks: Learning to leverage
salient regions in medical images,” Med. Image Anal., vol. 53, pp. 197–
207, 2019.
[19] J. Chen et al., “TransUNet: Transformers make strong encoders for
medical image segmentation,” arXiv:2102.04306, 2021.
[20] H. Cao et al., “Swin-Unet: Unet-like pure transformer for medical image
segmentation,” arXiv:2105.05537, 2021.
[21]
¨O. C¸ ic¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,
“3D U-Net: learning dense volumetric segmentation from sparse anno-
tation,” in Proc. MICCAI, 2016, pp. 424–432.
[22] L. Yu et al., “Automatic 3D cardiovascular MR segmentation with
densely-connected volumetric convnets,” in Proc. MICCAI, 2017, pp.
287–295.
[23] J. Chen, L. Yang, Y. Zhang, M. Alber, and D. Z. Chen, “Combining fully
convolutional and recurrent neural networks for 3D biomedical image
segmentation,” in Proc. NeurIPS, vol. 29, 2016.
[24] X. Yan, H. Tang, S. Sun, H. Ma, D. Kong, and X. Xie, “AFTer-UNet:
Axial fusion transformer UNet for medical image segmentation,” in
Proc. WACV, 2022, pp. 3971–3981.
[25] A. L. Y. Hung, H. Zheng, Q. Miao, S. S. Raman, D. Terzopoulos,
and K. Sung, “CAT-Net: A cross-slice attention transformer model
for prostate zonal segmentation in MRI,” IEEE Trans. Med. Imaging,
vol. 42, no. 1, pp. 291–303, 2022.
[26] Y. Zhang, L. Yuan, Y. Wang, and J. Zhang, “SAU-Net: Efficient 3D spine
MRI segmentation using inter-slice attention,” in Proc. MIDL, 2020, pp.
903–913.
[27] Z. Liu et al., “Swin Transformer: Hierarchical vision transformer using
shifted windows,” in Proc. ICCV, 2021, pp. 10 012–10 022.
[28] C. You, Y. Zhou, R. Zhao, L. Staib, and J. S. Duncan, “SimCVD: Simple
contrastive voxel-wise representation distillation for semi-supervised
medical image segmentation,” IEEE Trans. Med. Imaging, vol. 41, no. 9,
pp. 2228–2237, 2022.
[29] K. Chaitanya, E. Erdil, N. Karani, and E. Konukoglu, “Contrastive
learning of global and local features for medical image segmentation
with limited annotations,” in Proc. NeurIPS, vol. 33, 2020, pp. 12 546–
12 558.
[30] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-Net: Fully convolutional
neural networks for volumetric medical image segmentation,” in Proc.
3DV, 2016, pp. 565–571.
[31] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-
Hein, “nnU-Net: A self-configuring method for deep learning-based
biomedical image segmentation,” Nat. Methods, vol. 18, no. 2, pp. 203–
211, 2021.
[32] L. Li, S. Lian, Z. Luo, S. Li, B. Wang, and S. Li, “Learning consistency-
and discrepancy-context for 2D organ segmentation,” in Proc. MICCAI,
2021, pp. 261–270.
[33] Y. Xie, J. Zhang, C. Shen, and Y. Xia, “CoTr: Efficiently bridging cnn
and transformer for 3D medical image segmentation,” in Proc. MICCAI,
2021, pp. 171–180.
[34] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers
for image recognition at scale,” in Proc. ICLR, 2020.
[35] Z. Li et al., “SDTP: Semantic-aware decoupled transformer pyramid
for dense image prediction,” IEEE Trans. Circuits Syst. Video Technol.,
vol. 32, no. 9, pp. 6160–6173, 2022.
[36] J. Yuan, A. Zhu, Q. Xu, K. Wattanachote, and Y. Gong, “Ctif-
net: A CNN-Transformer iterative fusion network for salient ob-
ject detection,” IEEE Trans. Circuits Syst. Video Technol., 2023,
DOI:10.1109/TCSVT.2023.3321190.
[37] C. You et al., “Class-aware generative adversarial transformers for
medical image segmentation,” arXiv:2201.10737, 2022.
[38] X. Huang, Z. Deng, D. Li, X. Yuan, and Y. Fu, “MISSFormer: An
effective transformer for 2D medical image segmentation,” IEEE Trans.
Med. Imaging, 2022, DOI:10.1109/TMI.2022.3230943.
[39] A. Hatamizadeh et al., “UNETR: Transformers for 3D medical image
segmentation,” in Proc. WACV, 2022, pp. 574–584.
[40] H.-Y. Zhou et al., “nnFormer: Volumetric medical image segmentation
via a 3D transformer,” IEEE Trans. Image Process., vol. 32, pp. 4036–
4045, 2023.
[41] D. Guo and D. Terzopoulos, “A transformer-based network for
anisotropic 3D medical image segmentation,” in Proc. ICPR, 2021, pp.
8857–8861.
[42] Y. Jin, Y. Yu, C. Chen, Z. Zhao, P.-A. Heng, and D. Stoyanov, “Exploring
intra- and inter-video relation for surgical semantic scene segmentation,”
IEEE Trans. Med. Imaging, vol. 41, no. 11, pp. 2991–3002, 2022.
[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proc. CVPR, 2016, pp. 770–778.
[44] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast
for unsupervised visual representation learning,” in Proc. CVPR, 2020,
pp. 9729–9738.
[45] A. Vaswani et al., “Attention is all you need,” in Proc. NeurIPS, vol. 30,
2017.
[46] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, “Deeply-supervised
nets,” in Proc. AISTATS, 2015, pp. 562–570.
[47] B. Landman, Z. Xu, J. Igelsias, M. Styner, T. Langerak, and A. Klein,
“MICCAI multi-atlas labeling beyond the cranial vault–workshop and
challenge,” in Proc. MICCAIW, vol. 5, 2015, p. 12.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
11
[48] O. Bernard et al., “Deep learning techniques for automatic MRI cardiac
multi-structures segmentation and diagnosis: Is the problem solved?”
IEEE Trans. Med. Imaging, vol. 37, no. 11, pp. 2514–2525, 2018.
[49] Y. Ji et al., “AMOS: A large-scale abdominal multi-organ benchmark
for versatile medical image segmentation,” in Proc. NeurIPS, vol. 35,
2022, pp. 36 722–36 732.
[50] H. Wang et al., “Mixed transformer U-Net for medical image segmen-
tation,” in Proc. ICASSP, 2022, pp. 2390–2394.
[51] H. Wang, P. Cao, J. Wang, and O. R. Zaiane, “UCTransNet: Rethinking
the skip connections in U-Net from a channel-wise perspective with
transformer,” in Proc. AAAI, vol. 36, no. 3, 2022, pp. 2441–2449.
[52] M. Heidari et al., “HiFormer: Hierarchical multi-scale representations
using transformers for medical image segmentation,” in Proc. WACV,
2023, pp. 6202–6212.
[53] A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H. R. Roth, and D. Xu,
“Swin UNETR: Swin transformers for semantic segmentation of brain
tumors in MRI images,” in Proc. MICCAIW, 2021, pp. 272–284.
[54] L. Van der Maaten and G. Hinton, “Visualizing data using t-SNE.” J.
Mach. Learn. Res., vol. 9, no. 11, 2008.
"
"This paper's main objective is to solve an optimization problem with classification of DNA samples on PCR plates for Sanger sequencing. An integer linear programming model is designed to achieve this. Instances involve classifying thousands of samples, and due to computational limitations of linear models, heuristics are implemented. The proposed heuristic is based on simulated annealing and provides satisfactory solutions in short amounts of time.","DNA sequencing reveals the order of nucleotides Adenine (A), Cytosine (C), Guanine (G) and Thymine (T). Sanger sequencing is a commonly used method, but it is slow and expensive. Therefore, optimizing the organization of DNA samples in plates used for a key step in the Sanger method, Polymerase Chain Reaction (PCR), is of commercial interest. To perform PCR, DNA is deposited in PCR plates placed into thermocyclers with variable temperatures regulated by cycles. Wells in a plate must satisfy several conditions based on their strip, and the main objective is to minimize the number of plates while keeping them as full as possible.","nanPreviously developed optimization techniques cannot handle this distinctive problem. Sample organization in plates resembles a bin packing problem, which is an NP-hard optimization problem. Typical heuristic strategies to solve bin packing problems try to minimize slack in each bin, but reagent properties and temperature requirements complicate the problem here. Despite computational challenges, an ILP model and a heuristic algorithm with simulated annealing were developed to provide satisfactory solutions in short amounts of time.nannan","The ILP model seeks to minimize occupied plates while considering weight, temperature, and reagent constraints. Each strip has a temperature, and samples and reagents must be assigned to strips while satisfying temperature conditions. The simulated annealing heuristic initializes with a sorted group order and sequentially fills plates. Then, two movement types are implemented to improve solutions. The grouping movement joins samples in different plates, and the strip-exchange movement swaps strips between plates. Simulated annealing is used to find the optimal solution based on the objective function, which considers the number of plates, the number of wells, and occupancy rates.nan","Real data was used to evaluate the solution methods. Instances ranged from 174 to 4063 samples divided into groups with 11 to 201 groups and 10 to 18 temperatures. The ILP model could not guarantee optimality within a reasonable time frame for most instances. Instead, the heuristic provided optimal solutions for smaller instances and outperformed a commercial software, Labware, for all instances. The average number of plates saved by the heuristic over LabWare was 10.54 plates. Furthermore, computational times for the heuristic were reasonable for laboratory use.","A real-world problem in genetics was addressed by designing an ILP model and a simulated annealing heuristic to optimize the organization of DNA samples in PCR plates for Sanger sequencing. The heuristic algorithm consistently outperformed a commercial software, reducing the number of plates needed for DNA sequencing while also providing solutions quickly. This has potential implications for reducing costs and turnaround times in genetic testing.",Optimization in Sanger Sequencing,"Luisa Carpente, Ana Cerdeira-Pena, Silvia Lorenzo-Freire, Ángeles S. Places","arXiv:2401.11854v1  [cs.DS]  22 Jan 2024
Optimization in Sanger sequencing✩
Luisa Carpenteb, Ana Cerdeira-Penaa, Silvia Lorenzo-Freireb,∗, Ángeles S. Placesa
aDatabase Laboratory, Department of Computer Science, Faculty of Computer Science, University of A Coruña,
Spain
bMODES Research Group, Department of Mathematics, Faculty of Computer Science, University of A Coruña,
Spain
Abstract
The main objective of this paper is to solve the optimization problem that is associated with the
classiﬁcation of DNA samples in PCR plates for Sanger sequencing. To achieve this goal, we design
an integer linear programming model. Given that the real instances involve the classiﬁcation of
thousands of samples and the linear model can only be solved for small instances, the paper includes
a heuristic to cope with bigger problems.
The heuristic algorithm is based on the simulated annealing technique. This algorithm obtains
satisfactory solutions to the problem in a short amount of time.
It has been tested with real
data and yields improved results compared to some commercial software typically used in (clinical)
laboratories. Moreover, the algorithm has already been implemented in the laboratory and is being
successfully used.
Keywords:
optimization, Sanger sequencing, integer linear programming, simulated annealing
1. Introduction
Recent developments in the study of DNA will have a substantial impact on various ﬁelds of
science, such as clinical medicine and forensic studies. Increasingly many companies and laboratories
are processing DNA samples for various purposes and competing in terms of the time and cost that
are required for processing the results. Because optimization techniques can be used to reduce the
time or cost of any process, these techniques are of great interest for DNA sample processing.
✩Link to published version: https://doi.org/10.1016/j.cor.2019.05.011. Under the CC-BY-NC-ND 4.0 li-
cense (https://creativecommons.org/licenses/by-nc-nd/4.0/).
∗Corresponding author. Postal code: 15071, A Coruña. Phone: +34 981167000. Fax: +34 981167160.
Email addresses: luisa.carpente@udc.es (Luisa Carpente), acerdeira@udc.es (Ana Cerdeira-Pena),
silvia.lorenzo@udc.es (Silvia Lorenzo-Freire), asplaces@udc.es (Ángeles S. Places)
In this article, we will address a scheduling problem that arises within Health in Code1, which
is a company that specializes in the genetic diagnosis of inherited cardiovascular diseases. Every
day, thousands of DNA samples are processed in their labs.
DNA sequencing is the process that is used to determine the order of the four nucleotides,
namely, Adenine (A), Cytosine (C), Guanine (G) and Thymine (T), that make up the DNA
molecule. Currently, there are several methods for DNA sequencing. Health in Code is a com-
pany that has extensive experience with the Sanger method [7]. The method, which is also referred
to as dideoxynucleotide sequencing and chain termination sequencing, consists of the following
steps:
• DNA extraction (from samples of tissue, saliva, blood, etc.) and dilution treatment;
• Polymerase chain reaction (PCR): an eﬀective process for making copies of segments of DNA;
• PCR product puriﬁcation: to remove elements that are used in the PCR process to obtain
high-quality DNA samples for sequencing;
• DNA sequencing: to sort the DNA fragments by size in a sequencing machine so that the
original piece of DNA can be decoded.
Although all these steps are essential in the Sanger method, this paper focuses on the PCR
process. The polymerase chain reaction is a method for amplifying DNA to generate millions of
copies of one or several pieces of DNA. To perform this reaction, the DNA is deposited into PCR
plates. Then, the plates are placed into thermocyclers2 to regulate temperature during cyclical
programmes.
In this framework, we are interested in optimizing the organization of DNA samples in the plates
for performing the PCR process, which is not an easy task since many aspects must be considered
to achieve the optimal organization of DNA samples. Each DNA sample is deposited into the well
of a plate; however, depending on the components that are used in the procedure, each well from
a PCR plate can be used to obtain copies of a single piece of DNA or copies of a group of pieces
of DNA. The components that are used with the DNA sample determine not only the piece or
1http://www.healthincode.com
2A thermocycler, which is also known as a thermal cycler, is a laboratory machine that allows several temperatures
to be set in a block of a plate.
2
pieces to be copied but also the necessary temperature for processing the well in the thermocycler.
However, due to the technological characteristics of thermocyclers, wells in the same plate should
satisfy several conditions. According to their characteristics, each plate is divided into several strips
and all the wells in the same strip should be processed at the same temperature. Moreover, the
temperatures of the consecutive strips in the same plate should belong to a speciﬁed range.
Since processing each plate in the laboratory is slow and expensive, the main objective of our
optimization problem is to order the DNA samples such that the number of plates is minimized
under the restrictions that are imposed by the characteristics of the thermocycler. It is also desirable
to obtain plates that are as full as possible.
To the best of our knowledge, the optimization literature has not addressed this topic. Charac-
teristics of the problem remind us of a bin packing problem in which samples must be packed in the
plates. The bin packing problem is a well-known NP-hard optimization problem (see, for instance,
Delorme et al. [3]). For this reason, it is not always possible to obtain the optimal solution of this
problem in a reasonable computational time. Some of the heuristic proposals for solving the bin
packing problem use strategies that minimize the slack of each bin (see, for instance, Fleszar and
Hindi [5]). However, in our setting, if we consider various temperature ranges and the constraints
that are associated with the reagent that is necessary for amplifying a speciﬁed region of DNA, we
will be dealing with a diﬀerent optimization problem in the end.
Due to the characteristics of our problem, although we have developed an ILP model to solve
this problem, we have also implemented a heuristic algorithm based on the simulated annealing
procedure that provides satisfactory solutions in short computational times.
This algorithm is
being successfully used in the laboratory.
The remainder of the paper is organized as follows: In Section 2, the problem is described in
detail. Section 3 addresses both the optimization model and the heuristic algorithm. In Section 4,
we provide a detailed explanation of the numerical experiments that are necessary for justifying an
interest in the results. For this purpose, we have considered real instances from the laboratory.
2. Problem
In this problem, the available DNA samples are organized in PCR plates to be processed in
thermocyclers. To perform this task properly, several details must be considered:
3
The laboratory uses 96-well PCR plates and the wells are organized in 8 rows and 12 columns.
Each plate is composed of six strips. Each strip has 24 wells, which are arranged in 8 rows and
2 columns. According to the characteristics of the thermocyclers, all the wells in the same strip
will be processed at the same temperature. In addition, the diﬀerence in temperature between two
consecutive strips cannot exceed 5 degrees centigrade. Although the strips in a plate must satisfy
these temperature conditions, the temperature in the strips is not ﬁxed, but rather is selected by
the user. The conﬁguration of a 96-well PCR plate is shown in Figure 1 (a).
1
2
3
4
5
6
7
8
9
10
11
12
A
B
C
D
E
F
G
H
Strip 1
Strip 2
Strip 3
Strip 4
Strip 5
Strip 6
(a) empty plate
520 C
550 C
600 C
610 C
620 C
660 C
1
2
3
4
5
6
7
8
9
10
11
12
A
B
C
D
E
F
G
H
(b) plate with DNA samples
Figure 1: Example of a PCR plate
Each DNA sample, together with all the chemical components that are necessary for amplifying
a speciﬁed region of DNA, are deposited in a well. If we are interested in amplifying the same
region of DNA for multiple DNA samples, we must include the same chemical components in their
corresponding wells. In this case, we say that these DNA samples belong to the same group. All
DNA samples in a group must be processed at the same temperature, which is ﬁxed. However, it
is possible to have multiple groups with the same temperature.
For instance, let us suppose that the laboratory is interested in studying the risk of genetic
diseases. In this particular case, we can think of a group as being associated with a disease. Thus,
the laboratory must determine, using the DNA samples of the clients, whether the clients will
develop this disease. However, since the disease will be detected by evaluating a speciﬁed region of
DNA, each DNA sample will be mixed with a reagent and processed at the necessary temperature
for amplifying this region.
In addition, one well of the plate should be reserved for the isolated reagent that is associated
4
with a group, which is necessary to verify that the experiment is performed correctly.
Figure 1 (b) shows an example of one ﬁlled plate. The 12 groups are represented by colour.
We know the number of DNA samples in each group and the temperatures that correspond to
the groups.
There are six temperatures for the groups.
With this information, we can assign
the temperatures in increasing order to the 6 strips of the plate because the temperature diﬀerence
between any two consecutive strips does not exceed 50 C. With this conﬁguration, we have 6 groups
that have the same temperature in Strip 4. Moreover, for each group, we have added the reagent,
which is represented by a circle of the same colour (but with a black border), as the group samples;
see, for instance, the “red” group, which is placed in two wells, one of which contains the DNA
sample mixed with the reagent, whereas the other contains only the reagent.
In summary, the laboratory initially has multiple samples, each of which has its own code. The
samples belong to groups that are determined by the region of DNA that will be ampliﬁed. The
laboratory also knows the temperature associated with each group. After solving the optimization
problem with this information and the restrictions that are imposed by the characteristics of the
PCR plates, we obtain the number of samples of each group in every strip of a plate and whether
the reagent of each group is present in any strip. We will also know the temperature of every strip,
which will be the only information provided by the optimization problem since in the next phase,
the software will place each sample or reagent at random but according to this distribution. Then,
for each DNA sample or reagent, we will know the code of the plate and the position where it will
be placed, the type of analysis (the group to which it belongs) and the temperature at which it will
be processed.
3. Solution methods
3.1. Model
We have designed an integer linear programming (ILP) problem that takes into account all the
requirements that were discussed. The main objective of this problem is to minimize the number of
occupied plates. We know the number of samples and the temperature for each group. With this
information, the ILP problem will assign the samples and the temperature in each strip. Below, we
list the parameters and the decision variables that are used in the model.
Parameters:
5
• {1, . . . , p} is the set of available plates, with index q. Because each plate is divided into six
strips, the set of strips can be represented by {1, . . . , 6 · p}, with index l.
• wq is the weight assigned to each plate q ∈ {1, . . . , p} in the objective function.
• {1, . . . , n} is the set of groups with index i.
• Ni indicates the number of samples in each group i ∈ {1, . . . , n}.
• Ti is the temperature used in the process for each group i ∈ {1, . . . , n}. Several groups can
be processed at the same temperature. Then, we will consider {T 1, . . . , T m} ⊆ {T1, . . . , Tn}
as the set of temperatures that are used in the process.
Decision variables:
For each strip l ∈ {1, . . . , 6 · p}, we deﬁne the following variables:
• tl is the temperature selected for l.
• For each group i ∈ {1, . . . , n},
⋆ nil represents the number of samples of group i that are placed in strip l.
⋆ xil is a binary variable such that
xil =



1,
if the reagent of group i is in strip l;
0,
otherwise.
• For each j ∈ {1, . . . , m},
⋆ yjl is a binary variable such that
yjl =



1,
if strip l has been assigned temperature T j;
0,
otherwise.
Objective function and constraints:
min z =
n
X
i=1
p
X
q=1
wq
6·q
X
l=6·(q−1)+1
nil
(1)
6
subject to
n
X
i=1
(nil + xil) ≤ 16
∀l ∈ {1, . . . , 6 · p}
(2)
tl − tl+1 ≤ 5
∀l ∈ {6 · (q − 1) + 1, . . . , 6 · q − 1}, ∀q ∈ {1, . . . , p}
(3)
tl+1 − tl ≤ 5
∀l ∈ {6 · (q − 1) + 1, . . . , 6 · q − 1}, ∀q ∈ {1, . . . , p}
(4)
m
X
j=1
yjl · T j = tl
∀l ∈ {1, . . . , 6 · p}
(5)
m
X
j=1
yjl ≤ 1
∀l ∈ {1, . . . , 6 · p}
(6)
yjl ≥
X
i:Ti=T j
nil
n
X
i=1
Ni
∀l ∈ {1, . . . , 6 · p}, ∀j ∈ {1, . . . , m}
(7)
6·p
X
l=1
nil = Ni
∀i ∈ {1, . . . , n}
(8)
nil − xil ≥ 0
∀l ∈ {1, . . . , 6 · p}, ∀i ∈ {1, . . . , n}
(9)
6·q
X
l′=6·(q−1)+1
Nixil′ − nil ≥ 0
∀i ∈ {1, . . . , n}, ∀l ∈ {6 · (q − 1) + 1, . . . , 6 · q},
∀q ∈ {1, . . . , p}
(10)
6·q
X
l=6·(q−1)+1
xil ≤ 1
∀i ∈ {1, . . . , n}, ∀q ∈ {1, . . . , p}
(11)
The objective function (1) minimizes the number of occupied plates. To attain this objective,
the number of samples in each plate is multiplied by a weight. Although the laboratory is mainly
interested in minimizing the number of plates, it would also be desirable to minimize the total
number of occupied cells3 and maximize, according to the lexicographical order4, the occupancy
rate of the plates. The three objectives can be accomplished by penalizing the plates in the last
positions using the weights. Then, the weights will be considered in increasing order to ﬁll in the
cells of the ﬁrst plates. In practice, we use wq = q for all q ∈ {1, . . . , p}.
3The total number of occupied cells is composed of the number of cells that are occupied by all the samples, which
is ﬁxed, and the number of cells that are necessary for placing the corresponding reagents, which is variable.
4The laboratory prefers to have the ﬁrst plates occupied to the detriment of the last ones to be able to place new
samples in the last plates while the ﬁrst ones are being processed. Note that those new samples are allocated once
the solution has been obtained and already the ﬁrst plates have been processed.
7
Constraint (2) sets the number of available cells in each strip. Constraints (3) and (4) guarantee
that the absolute value of the diﬀerence in temperature between two consecutive strips does not
exceed 5 degrees centigrade. Constraints (5), (6) and (7) ensure that the temperature of each strip
is the same as the temperature of the groups with samples in the strip. Therefore, if
m
X
j=1
yjl = 0,
strip l is an empty strip and will not be processed. Otherwise,
m
X
j=1
yjl = 1 and all the cells in strip l
will be processed at the same temperature. Constraint (8) is used to check that all samples are in
the cells of the plates, whereas (9), (10) and (11) are associated with the reagent. These last three
conditions require that if one or more samples of group i is in plate q, the isolated reagent related to
this group should be deposited in exactly one well in the plate; otherwise,
6·q
X
l=6·(q−1)+1
xil = 0 implies
that there are no samples of group i in plate q and the corresponding reagent will not appear in
the plate.
3.2. Heuristic algorithm
Because it is necessary to provide feasible and reasonable solutions to the problem in a short
computational time, we have designed a heuristic algorithm. The heuristic algorithm is based on the
simulated annealing method, which was introduced by Kirkpatrick et al. [6]. The main motivation
of the simulated annealing method comes from the analogy between the physical annealing of solids
and combinatorial optimization problems. A thorough review of this method can be found in Aarts
et al. [1] or Eglese [4].
To implement this method, it is necessary to deﬁne the problem in terms of a solution space
with a deﬁned neighbourhood and cost function.
The algorithm starts with an initial solution
and moves randomly through the solution space, selecting a neighbour of the current solution
and comparing the diﬀerence in cost between the two solutions. If the new solution provides a
better result in terms of the cost function, we choose it as the new solution. Otherwise, although it
provides a poorer result, there is still a probability of accepting the new solution. The probability of
accepting a move is typically given by exp(−β/T), where β is the cost increment and T is a control
parameter that corresponds to temperature in the analogy with physical annealing. According to
this probability function, small increases in the cost function are more likely to be accepted than
large increases. Moreover, when T is high, most moves are accepted, whereas when T approaches
zero, the probability of accepting moves is very small. To avoid local optima, the initial value of T
8
is relatively high and is gradually decreased when new movements are made.
3.2.1. Initial solution
The initial solution is obtained by sorting the groups in ascending order according to their
processing temperature. The samples will be deposited by columns in the wells according to this
group order. In addition, we must reserve one well for the reagent of the group in all the plates
in which at least one sample of the group is deposited. The reagent of a group in a plate will be
deposited immediately after the last sample of the group.
Once the samples and reagent of a group have been deposited in the plate, we can deposit the
samples of the next group as close as possible, under the restriction that samples of both groups
can only be in the same strip if both groups are going to be processed at the same temperature;
otherwise, the samples of the two groups cannot be in the same strip.
If the samples of two consecutive groups must be processed at diﬀerent temperatures, once the
samples of the ﬁrst group have been deposited in the last strip, we deposit the samples of the
next group in the consecutive strip if the diﬀerence in the processing temperature between the two
groups does not exceed 50 C. Otherwise, we leave as many empty strips as necessary to satisfy the
constraint on the diﬀerence in temperature between consecutive strips.
The pseudocode of the algorithm for obtaining the initial solution is presented as Algorithm 1.
The main body of the algorithm describes the initial sorting method. Then, the process of assigning
a sample to existing plates is performed according to the constraints that are speciﬁed above. This
process is reﬂected in the pseudocode in Algorithm 2. If a sample cannot be assigned to an existing
plate, a new one is created.
3.2.2. Movements
To improve upon the initial solution and obtain the ﬁnal positions of the samples in the plates,
the main algorithm explores the search space by creating new solutions from a current solution via
movements. We have devised two movements, from which a neighbouring solution is generated:
1. Grouping movement. First, a group is randomly selected. If there are samples of the same
group in diﬀerent plates, we randomly choose two plates where at least one sample of the
group has been deposited. Then, we try to join all the samples of the group in one of the
two plates. If the empty wells in the plate are not suﬃcient for this, it is possible to move
to the plate samples of other groups that have similar processing temperatures. The groups
9
Algorithm 1: Initial solution
1.
Input: samples, list of samples to be processed; maxT empDiff, maximum allowable temperature
diﬀerence between plate strips
Output: initialSol, initial solution (list of plates)
2. plates ← ø
// Samples are sorted according to their temperature (that is, to the temperature of the group
// each of them belongs to)
3. sort(samples)
4. for i = 0 to samples.size do
5.
posPlate ← 0
// Existing plates are traversed to allocate each sample
6.
while posPlate < plates.size do
7.
currentPlate ← plates(posPlate)
8.
isSampleAssigned ← assignSample (samples(i),currentPlate, maxT empDiff)
9.
if ! isSampleAssigned then
10.
posPlate + = 1
11.
else
12.
break
// Whenever a sample cannot be assigned to existing plates, a new plate is created to contain it
13.
if ! isSampleAssigned then
14.
newPlate ← new Plate()
15.
assignSample (samples(i), newPlate, maxT empDiff)
16.
addPlate(newPlate,plates)
17. initialSol ← plates
18. return initialSol
that have samples in the other plate are preferred to other groups. Moreover, we move the
minimum number of plates that are necessary to satisfy the feasibility constraints.
This
movement also allows all the samples of one group to be moved from one plate to another
plate where there is no sample in the group.
2. Strip-exchange movement. We select two strips in two plates and exchange them. Although
the temperature constraint between consecutive strips should be satisﬁed for the movement
to be permitted, the temperatures of the strips can be diﬀerent.
The movements will be selected according to a ﬁxed parameter that denotes the probability of
each movement. If a movement is not feasible, a new movement will be made. If no movement is
feasible, the algorithm terminates and the solution that was obtained in the last iteration is selected
as the ﬁnal solution of the algorithm5.
5This does not limit the search capacity, as we are already considering an escape mechanism from local optima
based on accepting solutions that may be worse than the current one. Instead, this selection avoids repeated iterations
of the algorithm in a rare scenario.
10
Algorithm 2: Sample assignment
1.
Input: sample, sample to be assigned; plate, plate for holding the sample; maxT empDiff, maximum
allowable temperature diﬀerence between plate strips
Output: true if sample is ﬁnally allocated in plate; false otherwise
2. isSampleAssigned ← false
// First, we attempt to allocate the sample in some of the non-empty strips (that is, strips that already
// hold other samples)
3. nonEmptyStrips ← plate.getNonEmptyStrips()
4. posStrip ← 0
5. while (! isSampleAssigned && posStrip < nonEmptyStrips.size) do
6.
currentStrip ← nonEmptyStrips(posStrip)
// A sample can be assigned to a non-empty strip if their temperatures match. There must also be
// enough space remaining to hold the sample and the reagent of the group (if needed)
7.
. if (checkTemperature (sample, currentStrip) && checkSpace (sample,currentStrip)) then
8.
addSample(sample,currentStrip)
9.
isSampleAssigned ← true
10.
else
11.
posStrip + = 1
// If none of the non-empty strips can hold the sample, we try to assign it to some of the
// empty strips (if any)
12. if ! isSampleAssigned then
13.
emptyStrips ← plate.getEmptyStrips()
14.
posStrip ← 0
15.
while (! isSampleAssigned && posStrip < emptyStrips.size) do
16.
currentStrip ← emptyStrips(posStrip)
// To assign a sample to an empty strip, we must guarantee that the maximum allowable
// temperature diﬀerence between the plate strips is not exceeded
17.
if validateTemperatureDiﬀ (sample, currentStrip, plate, maxT empDiff) then
18.
addSample(sample,currentStrip)
19.
isSampleAssigned ← true
20.
else
21.
posStrip + = 1
22. return isSampleAssigned
3.2.3. Cost function
The cost function for comparing the current and new solutions at each iteration of the algorithm
is obtained by applying the lexicographical order to the following objectives:
• First objective: minimize the number of non-empty plates.
• Second objective: minimize the number of non-empty wells.
We only used two objectives because the deﬁnition of the implemented movements and the
second objective indirectly improve the occupancy rate of the plates. Furthermore, we could have
considered a single objective, as in the ILP model. However, the use of a single aggregated objective
11
often results in the heuristic algorithm ﬁnding local minima and in a high degree of variability in
the results. Thus, we have ruled out this strategy.
The pseudocode of the algorithm for obtaining the ﬁnal solution is presented as Algorithm 3.
Algorithm 3: Simulated annealing
1.
Input: initialSol, initial solution; maxIter, maximum number of algorithm iterations; T max, initial
temperature; T min, minimum temperature; α, cooling parameter; SEProb, probability of applying
a Strip − exchange movement (thus, a Grouping movement will be applied with probability
1 − SEProb)
Output: bestSol, best global solution that has been found
2. bestSol ← initialSol
3. currentSol ← initialSol
4. for i = 0 to maxIter do
5.
T ← T max
6.
while T ≥ T min do
// A new solution is created from the current one by applying a movement. If
// no movement can be applied, the algorithm terminates
7.
newSol ← applyMovement (currentSol, SEProb)
8.
if newSol = ø then
9.
return bestSol
10.
else
// We must always keep updated the best global solution that has been found so far
11.
updateBestSol (newSol, bestSol)
// If newSol improves currentSol (that is, if it uses fewer plates or
// if it minimizes the number of non-empty wells), it will replace the current solution
// for the next iteration
12.
case numPlates (newSol) < numPlates (currentSol) do
13.
currentSol ← newSol
14.
case numPlates (newSol) = numPlates (currentSol) do
15.
if nonEmptyWells (newSol) ≤ nonEmptyWells (currentSol) then
16.
currentSol ← newSol
17.
else
18.
threshold ← e
nonEmptyW ellsDiff(newSol,currentSol)
T
19.
acceptWorseSolution (generateRandom(),threshold, newSol, currentSol)
// Even when newSol is worse than currentSol, it still may replace the current solution
20.
otherwise do
21.
threshold ← e
numPlatesDiff(newSol,currentSol)
T
22.
acceptWorseSolution (generateRandom(),threshold, newSol, currentSol)
23.
T ← α ∗ T
24. return bestSol
3.2.4. Example
To demonstrate how the algorithm operates, we consider the following example: We must
distribute 69 samples that are organized in 14 groups (and the corresponding reagents) in the
12
plates. The initial solution of the algorithm is represented in the ﬁrst part of Figure 2. The groups
are ordered according to their temperatures in increasing order. According to this order, we ﬁx the
temperatures of the strips. To organize the samples under this criterion, we need two plates and
the number of occupied cells is 84 (69 samples and 15 reagents). There are several strips that have
the same temperature since in these cases, one strip is not suﬃcient to hold all the samples that
are processed at the corresponding temperature. Moreover, the samples of the “violet” group are
distributed in both plates. Then, the grouping movement could consist of joining all the samples
of the group in the ﬁrst plate and moving the “pink” group to the ﬁrst plate.
Both solutions (the initial solution and the solution obtained after 1 iteration of applying the
grouping movement of the algorithm) require two plates. However, whereas the number of occupied
cells in the initial solution is 84, this number is reduced to 83 after 1 iteration since the number of
reagents has been reduced from 15 to 14. Thus, the solution has been improved after 1 iteration.
510 C
530 C
550 C
570 C
580 C
590 C
1
2
3
4
5
6
7
8
9
10 11 12
A
B
C
D
E
F
G
H
1
2
3
4
5
6
7
8
9
10 11 12
A
B
C
D
E
F
G
H
590 C
600 C
600 C
630 C
640 C
510 C
530 C
550 C
570 C
580 C
590 C
1
2
3
4
5
6
7
8
9
10 11 12
A
B
C
D
E
F
G
H
1
2
3
4
5
6
7
8
9
10 11 12
A
B
C
D
E
F
G
H
590 C
600 C
600 C
630 C
640 C
Figure 2: Grouping movement
Suppose we accept this new solution and perform another iteration. Now, we could apply the
strip-exchange movement on the fourth strip of the ﬁrst plate and the second strip of the second
plate. After this movement, the number of occupied cells is again 83; however, the solution is
improved because the occupancy rates have changed from (51.04,35.42) to (65.63,20.83).
This
movement is illustrated in Figure 3.
13
510 C
530 C
550 C
570 C
580 C
590 C
1
2
3
4
5
6
7
8
9
10 11 12
A
B
C
D
E
F
G
H
1
2
3
4
5
6
7
8
9
10 11 12
A
B
C
D
E
F
G
H
590 C
600 C
600 C
630 C
640 C
510 C
530 C
550 C
600 C
580 C
590 C
1
2
3
4
5
6
7
8
9
10 11 12
A
B
C
D
E
F
G
H
1
2
3
4
5
6
7
8
9
10 11 12
A
B
C
D
E
F
G
H
590 C
570 C
600 C
630 C
640 C
Figure 3: Strip-exchange movement
4. Experimental results
4.1. Data
The solution methods have been tested with real data. We have considered 36 ﬁles, which are
divided into two sets:
• The ﬁrst set contains 30 ﬁles (Id 1-30), each of which contains information on one working
session in the laboratory.
• The second set contains 6 ﬁles (Id 31-36). Each one of these ﬁles corresponds to 2 or 3 plates
that have been randomly chosen from one working session.
Table 1 summarizes the main characteristics of the ﬁles (numbers of samples, groups and group
temperatures).
In both sets, the ﬁles have been ordered according to the number of samples.
The number of temperatures remains stable in all scenarios. However, the number of groups and,
especially, the number of samples vary considerably. Hence, the characteristics of each working
session are highly particular.
14
Id
1
2
3
4
5
6
7
8
9
10
11
12
Samples
174
193
233
285
290
315
358
368
432
434
501
551
Groups
80
68
31
147
87
99
35
27
32
44
82
37
Temperatures
13
14
12
18
10
16
10
11
14
13
15
15
Id
13
14
15
16
17
18
19
20
21
22
23
24
Samples
612
647
747
797
876
918
963
1128
1270
1309
1398
1473
Groups
107
27
37
27
120
184
37
192
167
201
200
197
Temperatures
17
12
15
11
13
17
15
16
17
17
18
16
Id
25
26
27
28
29
30
31
32
33
34
35
36
Samples
1944
2071
2248
2496
2703
3783
40
44
50
65
77
84
Groups
151
162
165
179
200
171
24
31
45
34
43
68
Temperatures
15
15
17
17
17
17
11
11
14
15
13
16
Table 1: Description of the ﬁles that were obtained from real data
4.2. Comparison between LabWare, the ILP model and the heuristic algorithm
This section is devoted to comparing the results that were obtained in the laboratory with
LabWare software6 and the results that were obtained via our solution methods.
Our solution
methods have been evaluated using a computer with an Intel(R) Core(TM) i7-3770 CPU of 3.40
GHz and 16 GB of RAM. Moreover, the ILP problem was solved using Gurobi Optimizer (version
6.0.5), whereas the heuristic algorithm was implemented in Java.
Each working session in the laboratory involves many samples and groups.
Thus, the ILP
problem for Files 1-30, which are related to real working sessions, will be large and cannot be
solved in a reasonable amount of time. To demonstrate this, Table 2 lists the results that were
obtained with ten examples from the ﬁrst set. The ILP problem solutions were obtained after 60
hours. According to Table 2, after this time, we cannot guarantee the optimality of the solutions.
The table also indicates the best solution that was obtained and the size7 of the ILP problem, the
gap of the solution8 and the minimum time needed to obtain the solution. The results demonstrate
the necessity of developing a tool to identify satisfactory solutions in a reasonable computational
time.
However, it is possible to ﬁnd the optimal solutions in few seconds for problems that are asso-
ciated with a small number of plates. For this reason, we have considered 6 small examples with 2
6Since LabWare is a comercial software, we do not know exactly how the optimization tool of this software works.
7When we refer to the size of an ILP problem, we ﬁrst indicate the number of constraints, followed by the number
of variables.
8The gap is computed with the following formula: 100 ∗ |objective.lower.bound−incumbent.objective|
|incumbent.objective|
.
15
Id
ILP problem solution
Plates
Occupancy rates
Size
Gap (%)
Minimal time
1
4
100/98.96/44.79/22.92
4664×4176
2.17
2.07 h
2
5
100/97.92/55.21/20.83/2.08
5048×4530
3.14
0.21 h
3
4
100/94.79/64.58/26.04
2043×1800
4.17
54.87 h
4
7
98.96/97.92/96.88/95.83/39.58/17.71/6.25
14476×13146
5.43
59.61 h
5
5
100/100/100/72.92/22.92
6182×5550
1.18
12.89 h
6
6
100/100/96.87/84.37/46.87/6.25
7989×7452
1.02
3.46 h
7
5
100/100/100/80.20/36.46
2750×2430
0.99
58.21 h
8
5
100/100/98.96/86.46/31.25
2252×1980
1.90
53.8 h
9
7
100/100/100/98.96/65.63/23.96/2.08
3364×3318
1.90
41.05 h
10
7
100/100/98.96/98.96/75.00/28.13/4.17
4790×4284
1.79
45.92 h
Table 2: Solutions for Files 1-10 that were obtained by solving the ILP problem
or 3 plates that are conﬁgured in the laboratory with LabWare. Despite not being representatives
of a typical working session, these examples will be useful for comparing optimal solutions with
the solutions that are provided by the heuristic algorithm. Thus, in Table 3 we can compare the
occupancy rates of the three methods: LabWare, the ILP model and the heuristic algorithm. The
table also lists the computational times for the ILP model and the heuristic algorithm.
Id
Occupancy rates
Computational times
LabWare
ILP model
Heuristic
ILP model
Heuristic
31
37.50/29.17
50.00/16.67
50.00/16.67
0.01 s
0.006 s
32
29.17/28.13/20.83
57.29/20.83
57.29/20.83
0.25 s
0.07 s
33
54.17/37.50/7.29
71.88/20.83/6.25
71.88/20.83/6.25
0.88 s
0.065 s
34
41.67/39.58/21.88
73.96/20.83/8.33
73.96/20.83/8.33
4.04 s
0.26 s
35
47.92/47.92/30.21
86.46/32.29/6.25
86.46/32.29/6.25
4.76 s
0.55 s
36
67.71/50.00/40.63
91.67/45.83/20.83
91.67/45.83/20.83
4.76 s
0.62 s
Table 3: Examples with 2 or 3 plates
We have obtained the optimal solutions of the ILP problem in Files 31-36.
However, the
solution provided by LabWare never coincides with the optimal solution. In the second example,
the LabWare solution requires an additional plate and in the other cases, the distribution of the
samples in the plates is worse. However, after determining the best values for the input parameters
in the algorithm (a study of the parameters of the algorithm will be performed next), we have
veriﬁed that for these 2 and 3 plate ﬁles, the heuristic algorithm reaches the optimal value provided
by the ILP model in less time. Since Files 31-36 have been randomly selected from the same working
session, these results enable us to guarantee the satisfactory performance of the heuristic algorithm.
16
4.3. Study of the parameters in the heuristic algorithm
In this section, we focus on obtaining the appropriate parameters for Algorithm 3 according to
our experimental environment.
The algorithm depends on several input parameters, as discussed above. To ensure its proper
performance in our scenario, it is necessary to adjust these parameters according to our data. Next,
we justify the values of these input parameters:
• Initial temperature. The initial temperature was set to 100. According to Ben-Ameur [2],
this value ensures a low probability of acceptance of poor solutions with respect to the ﬁrst
objective and higher acceptance ratio with respect to the second objective. Thus, when the
number of plates is minimized, there is greater ﬂexibility for ﬁlling them in the best possible
way.
• Minimum temperature. The ﬁnal temperature at each simulated annealing iteration was set
to 1E − 10. This low value permits an exhaustive search at each iteration.
• Cooling parameter (α). Our annealing procedure initially considers a high temperature. Then,
the temperature is lowered incrementally by a constant factor. It is important to consider
suﬃciently many steps at each temperature to keep the system close to equilibrium until the
system approaches the minimum temperature. We have considered α = 0.9.
• Maximum number of iterations.
Once the above parameters were set, we performed an
exhaustive study of the execution times by varying the number of iterations. According to
the results of this study, a value of 1000 iterations has been chosen to ensure a reasonable
execution time in all scenarios. In addition, the objective function values of the problem do
not improve after 1000 iterations.
• Probability of the Strip-exchange movement, which is the parameter that most inﬂuences the
result of the algorithm. We have studied the results that were obtained for all ﬁles by varying
the probability of choosing this movement from 0 to 1 in increments of 0.1. According to this
empirical study, this movement is the more eﬀective of the two implemented movements as it
obtains only the best solutions if a high probability has been assigned to it.
For simplicity, Table 4 lists the results for probability values of 0.8, 0.9 and 1. To summarize
the results for each instance, Table 4 lists the minimum number of plates that are required
17
for placing the samples, the total number of full plates and the ﬁll rate of the ﬁrst plate that
is not ﬁlled. The Grouping movement plays no role in small instances, as poorer results are
always obtained when it is involved. However, for larger ﬁles, it is important to apply it with
low probability to obtain the best results. For instance, in File 27, we can obtain 24 full plates
out of a total of 27 by assigning probability 0.2 to the Grouping movement and probability
0.8 to the Strip-exchange movement.
 0
 1
 2
 3
 4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Number of full plates
Percentage of the Strip-exchange movement
 0
 20
 40
 60
 80
 100
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fill rate
Percentage of the Strip-exchange movement
Figure 4: Study of the Strip-exchange movement for small instances (Files 1-10)
 0
 2
 4
 6
 8
 10
 12
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Number of full plates
Percentage of the Strip-exchange movement
 0
 20
 40
 60
 80
 100
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fill rate
Percentage of the Strip-exchange movement
Figure 5: Study of the Strip-exchange movement for medium instances (Files 11-20)
 5
 10
 15
 20
 25
 30
 35
 40
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Number of full plates
Percentage of the Strip-exchange movement
 0
 20
 40
 60
 80
 100
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fill rate
Percentage of the Strip-exchange movement
Figure 6: Study of the Strip-exchange movement for large instances (Files 21-30)
18
Figures 4, 5 and 6 show the ﬁll rates that were obtained for various probabilities of the Strip-
exchange movement in small, medium and large instances, respectively. Figures 4, 5 and
6 support the following observation from Table 4: the probabilities of 0.8 and 0.9 for the
Strip-exchange movement yield the best ﬁlling rates for medium and large working sessions.
Id
Plates
Full plates
Percentage
0.8
0.9
1
0.8
0.9
1
1
4
1
1
1
96.88
97.92
98.96
2
5
1
1
1
91.67
86.46
95.83
3
4
0
1
1
97.92
80.21
80.21
4
7
1
1
2
97.92
97.92
97.92
5
5
2
2
3
91.67
91.67
75
6
6
3
2
2
85.42
93.75
98.96
7
5
3
3
3
80.21
80.21
81.25
8
5
3
3
3
85.42
84.38
86.46
9
7
3
3
4
94.79
96.88
39.58
10
7
3
3
4
95.83
95.83
63.54
11
8
5
5
5
54.17
79.17
76.04
12
8
5
5
5
84.38
79.17
73.96
13
9
4
4
5
98.96
98.96
91.67
14
9
6
6
6
83.33
85.42
78.12
15
10
6
6
6
98.96
98.96
91.67
16
10
7
7
6
94.79
89.58
97.92
17
12
8
8
8
98.96
98.96
97.92
18
13
8
8
8
95.83
98.96
95.83
19
12
9
9
8
89.58
94.79
93.75
20
15
12
12
10
86.46
96.88
96.88
21
17
13
13
11
85.42
93.75
97.92
22
18
12
13
12
98.96
98.96
98.96
23
19
13
13
13
95.83
97.92
88.54
24
19
14
15
14
97.92
82.29
97.92
25
23
20
19
19
93.75
98.96
83.33
26
25
21
21
21
98.96
98.96
93.75
27
27
24
23
21
76.04
97.92
85.42
28
30
26
25
25
98.96
94.79
93.75
29
32
29
28
28
87.5
96.88
86.46
30
44
39
38
37
98.96
86.46
96.98
Table 4: Results of applying the Strip-exchange movement with probabilities 0.8, 0.9 and 1
4.4. Final results
Once the parameters of the algorithm have been chosen, we will compare the results that were
obtained by applying the heuristic algorithm with those that were obtained in the laboratory using
19
the LabWare software for Files 1-30.
The ﬁrst and most important objective is the reduction of the number of plates necessary to
process the samples. Figure 7 shows the number of plates that are saved by the heuristic algorithm
compared to the solution that was provided by the LabWare software. In view of Figure 7, it is
evident that the heuristic algorithm is able to provide better solutions in most of the ﬁles. The
savings are more signiﬁcant in large instances.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
Id1
Id2
Id3
Id4
Id5
Id6
Id7
Id8
Id9
Id10
Id11
Id12
Id13
Id14
Id15
Id16
Id17
Id18
Id19
Id20
Id21
Id22
Id23
Id24
Id25
Id26
Id27
Id28
Id29
Id30
Plates
Laboratory
Algorithm
Figure 7: Comparison between the total number of plates that were obtained via the LabWare software and the
heuristic algorithm
In addition, the heuristic algorithm achieves the solutions in reasonable times for the laboratory,
as shown in Figure 8. It is also provided a comparison of the times necessary for the heuristic
algorithm to obtain the initial and ﬁnal solutions. The initial solution is calculated in few seconds
for all working sessions, whereas the computational time of the ﬁnal solution depends to a large
extent on the number of samples of each ﬁle.
As it has been previously mentioned, it is also interesting for the laboratory to have a certain
distribution of the plate occupancy rates, so that the ﬁrst plates are more occupied than the last
plates. This would allow new samples to be placed on the most empty plates, which have not
yet been processed, as these are the ones that occupy the last positions. Figure 9 and Figure 10
show the distribution of the plate occupancy with the algorithm and Labware in small and medium
instances, respectively.
In the case of both ﬁgures, each row represents the best solution obtained for the corresponding
instance by the proposed algorithm and by LabWare. The coloured squares represent the occupied
20
 0
 500
 1000
 1500
 2000
 2500
 3000
Id1
Id2
Id3
Id4
Id5
Id6
Id7
Id8
Id9
Id10
Id11
Id12
Id13
Id14
Id15
Id16
Id17
Id18
Id19
Id20
Id21
Id22
Id23
Id24
Id25
Id26
Id27
Id28
Id29
Id30
Execution time (in seconds)
Initial solution
Final Solution
Figure 8: Comparison of the times in which the initial and ﬁnal solutions of the algorithm are obtained
Id 1
Id 2
Id 3
Id 4
Id 5
Id 6
Id 7
Id 8
Id 9
Id 10
100%
75-99%
50-75%
25-50%
0-25%
Plates
1
2
3
4
5
6
7
Algorithm
Id 1
Id 2
Id 3
Id 4
Id 5
Id 6
Id 7
Id 8
Id 9
Id 10
100%
75-99%
50-75%
25-50%
0-25%
Plates
1
2
3
4
5
6
7
LabWare
8
9
Figure 9: Distribution of the plate occupancy obtained with the algorithm and LabWare in small instances
plates and the occupancy level (100%, [75,100)%, [50,75)%, [25,50)% and [0,25)%) is indicated by
the diﬀerent colours. It can be clearly seen that both the number of plates used and the number of
full plates is worse in the solution obtained by LabWare.
To highlight the eﬃciency of the heuristic algorithm against the solution that was obtained
with LabWare, Figure 11 compares, by means of box-plots, the ﬁll rates that were obtained by
the initial and ﬁnal solutions of the algorithm with the ones provided by LabWare for small and
medium instances (Files 1-20). In the case of the medium instances, the solutions of the algorithm
yield much higher ﬁll rates than LabWare’s solution.
Finally, Figure 12 compares the ﬁlling distributions of initial and ﬁnal solutions that were
obtained via the heuristic algorithm with the solution provided by LabWare for large instances
(Files 21-30). As a summary of the obtained results, the number of plates that correspond to each
21
Id 11
Id 12
Id 13
Id 14
Id 15
Id 16
Id 17
Id 18
Id 19
Id 20
100%
75-99%
50-75%
25-50%
0-25%
LabWare
Plates
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
Id 11
Id 12
Id 13
Id 14
Id 15
Id 16
Id 17
Id 18
Id 19
Id 20
100%
75-99%
50-75%
25-50%
0-25%
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Plates
Algorithm
Figure 10: Distribution of the plate occupancy obtained with the algorithm and LabWare in medium instances
range of ﬁlling percentages (100, [75,100), [50,75), [25,50) and [0,25)) is displayed for each ﬁle.
Thus, for instance, in the case of the largest ﬁle (Id 30), the heuristic algorithm ﬁnds a scheme that
ﬁlls 39 plates out of a total of 44 plates, while the LabWare solution ﬁlls 11 plates out of a total of
90.
Concluding remarks
In this paper, a real problem that was proposed by Health in Code is studied. This company
specializes in genetic diagnosis services for cardiovascular diseases. One of the diagnosis procedures
is based on the Sanger method. The Sanger method consists of several phases. In one phase, DNA
samples are placed into PCR plates to be processed in thermocyclers. The problem addressed in
this paper focusses on the organization of samples in plates such that the minimum number of plates
are used since plate processing is expensive. Minimization of plate use would also enable Health in
Code to oﬀer more competitive prices in the market, as an increasing number of laboratories are
dedicated to DNA sequencing.
The scheduling problem considered in this paper is diﬃcult to solve since to organize the samples
on the plates, a series of constraints must be satisﬁed in such a way that this problem diﬀers from
22
 0
 20
 40
 60
 80
 100
 120
Id
I
Id2
Id3
Id4
Id5
Id6
Id7
Id8
Id9
Id
I  0
 5
 10
 15
 20
 25
 30
 35
 40
0-25%
25-50%
50-75%
75-99
100%
Plates
File Id 21
Lab:DUH
Initial
Final
 0
 5
 10
 15
 20
 25
 30
 35
 40
0-25%
25-50%
50-75%
75-99%
100%
Plates
File Id 22
Lab:DUH
Initial
Final
 0
 5
 10
 15
 20
 25
 30
 35
 40
0-25%
25-50%
50-75%
75-99%
100%
Plates
File Id 23
Lab:DUH
Initial
Final
 0
 5
 10
 15
 20
 25
 30
 35
 40
0-25%
25-50%
50-75%
75-99%
100%
Plates
File Id 24
Lab:DUH
Initial
Final
 0
 5
 10
 15
 20
 25
 30
 35
 40
0-25%
25-50%
50-75%
75-99%
100%
Plates
File Id 25
Lab:DUH
Initial
Final
 0
 5
 10
 15
 20
 25
 30
 35
 40
0-25%
25-50%
50-75%
75-99%
100%
Plates
File Id 2

Lab:DUH
Initial
Final
 0
 5
 10
 15
 20
 25
 30
 35
 40
0-25%
25-50%
50-75%
75-99%
100%
Plates
File Id 27
Lab:DUH
Initial
Final
 0
 5
 10
 15
 20
 25
 30
 35
 40
0-25%
25-50%
50-75%
75-99%
100%
Plates
File Id 2

Lab:DUH
Initial
Final
 0
 5
 10
 15
 20
 25
 30
 35
 40
0-25%
25-50%
50-75%
75-99%
100%
Plates
File Id 29
Lab:DUH
Initial
Final
 0
 5
 10
 15
 20
 25
 30
 35
 40
0-25%
25-50%
50-75%
75-99%
100%
Plates
File Id 30
Lab:DUH
Initial
Final
Figure 12: Distribution of the plate occupancy obtained with the initial and ﬁnal solutions of the algorithm and
LabWare in large instances
24
algorithm obtains satisfactory solutions in short amounts of time and, even in small problems
for which the ILP model can be used, provides solutions of similar quality in a much shorter
computational time, as the company demands. Moreover, these solutions substantially outperform
the solutions that are obtained by the LabWare software, which was used previously at Health in
Code. In most cases, it is possible to substantially reduce the number of plates needed for the
samples.
The algorithm presented in this paper has been successfully implemented in the laboratories of
Health in Code and corresponds to software registration 03/2017/560, which is entitled ""SimPCR:
librería para la optimización del proceso de llenado de placas PCR en secuenciación Sanger"".
Acknowledgements
This work has been supported by MINECO: MTM2014-53395-C3-1-P, MINECO: MTM2017-
87197-C3-1-P, Xunta de Galicia/FEDER-UE ERDF: ED431C-2016-015, Xunta de Galicia/FEDER-
UE ERDF: ED431G/01, FEDER-UE ESF, Xunta de Galicia Conecta Peme-2014: IN852A-2014/9,
Xunta de Galicia/FEDER-UE CSI: ED431G/01, Xunta de Galicia/FEDER-UE GRC: ED431C
2017/58, MINECO-CDTI/FEDER-UE CIEN LPS-BIGGER: IDI-20141259, MINECO-CDTI/FEDER-
UE INNTERCONECTA uForest: ITC-20161074, MINECO-AEI/FEDER-UE eDSalud: RTC-2016-
5143-1, MINECO-AEI/FEDER-UE Datos 4.0: TIN2016-78011-C4-1-R and MINECO-AEI/FEDER-
UE ETOME-RDFD3: TIN2015-69951-R.
The interesting and constructive comments made by three anonymous referees are also gratefully
acknowledged.
References
[1] E.H.L. Aarts, J.H.M. Korst, P.J.M. van Laarhoven, 1997. Simulated Annealing, in: E. Aarts and J.K. Lenstra
(Eds.), Local Search in Combinatorial Optimization, John Wiley & Sons, pp. 91-120.
[2] W. Ben-Ameur, 2004. Computing the initial temperature of simulated annealing. Computational Optimization
and Applications 29(3), 369-385.
[3] M. Delorme, M. Iori, S. Martello, 2016. Bin packing and cutting stock problems: Mathematical models and
exact algorithms. European Journal of Operational Research 255(1), 1-20.
[4] R.W. Eglese, 1990. Simulated Annealing: A tool for Operational Research. European Journal of Operational
Research 46, 271-281.
[5] K. Fleszar, K. S.Hindi, 2002. New heuristics for one-dimensional bin-packing. Computers & Operations Research
29(7), 821-839.
[6] S. Kirkpatrick, C.D. Gelatt, M.P. Vecchi, 1983. Optimization by simulated annealing. Science 220, 671-680.
[7] F. Sanger, S. Nicklen, R. Coulson, 1977. DNA sequencing with chain-terminating inhibitors. Proceedings of the
National Academy of Sciences 74, 5463-5467.
25
"
"Multi-label classification (MLC) tasks are common in the legal domain, where one document can have multiple labels. In this work, we compare different MLC methods using two public legal datasets, POSTURE50K and EURLEX57K, varying training data size and label quantity to explore the advantages offered by different approaches. Results show DistilRoBERTa and LegalBERT have consistent performance and reasonable computational demands.","Multi-label classification (MLC) tasks are common in the legal domain as a document can have multiple labels. Previous research has explored various MLC methods, from traditional approaches to transformer-based models. Domain-specific models trained on legal text often exhibit enhanced performance compared to general-purpose models. The choice of method depends on factors like dataset size, number of labels, and resource availability. This study aims to compare several established MLC approaches for researchers to explore baseline models.","nanMLC is a common legal task with diverse applications. Legal MLC often faces high label imbalance and label co-occurrence patterns. The existence of extremely large label sets, incomplete annotation, and scarcity of data are challenges in the legal domain. Domain-specific transformer-based models have shown superior performance compared to generic models. Several machine learning methods can be employed for MLC problems. Recent systems increasingly rely on Transformers, such as encoder-only approaches or encoder-decoder architectures. Loss functions designed to address label imbalances, like focal loss, may improve the training process. Sentence similarity tasks can be utilized in cases of frequently evolving label sets using SBERT-based approaches.nannan","To study the impact of dataset size and label quantity on the performance of different MLC algorithms, we experiment with two legal MLC datasets, namely POSTURE50K and EURLEX57K. These datasets are sampled to simulate various data scenarios. We choose three sparse vector similarity methods, namely ClassTFIDF, DocTFIDF, and BM25. We compare these with various Transformer architectures. Experiments are conducted on both datasets, varying training set sizes and label quantities. To gain insights on each model's strengths, we examine the performance of the models trained on different-sized training sets.nan","Our experiments show that DistilRoBERTa and LegalBERT perform the best for both datasets in terms of micro-F1 score, except when the training data is insufficient. T5 is a close contender and is suitable for changing label sets. It can be fine-tuned on a smaller dataset with new labels or even be used in a zero-shot setup. DocTFIDF and BM25 also show reasonable performance throughout. The CrossEncoder can significantly improve the macro-F1 score for POSTURE50K when dealing with numerous labels but not for EURLEX57K. In terms of cost-performance trade-off, DistilRoBERTa and LegalBERT have a reasonable balance while T5 is worth trying as it is fast and cheap to run. CrossEncoders are the most expensive but can yield higher macro-F1 performance.","Our experiments explore established MLC approaches in the legal domain to provide insights for the legal community. DistilRoBERTa and LegalBERT generally have a reasonable cost-performance trade-off for legal MLC, yielding superior results. T5 performs well and offers flexibility in the case of changing label sets. DocTFIDF is worth considering due to its speed and low cost. CrossEncoders can provide significant performance gains in macro-F1 score in some cases, but at a higher price. Future work will involve testing algorithms that can handle long documents and experimenting with LLMs.",The Right Model for the Job: An Evaluation of Legal Multi-Label Classification Baselines,"Martina Forster, Claudia Schulz, Prudhvi Nokku, Melicaalsadat Mirsafian, Jaykumar Kasundra, Stavroula Skylaki","The Right Model for the Job: An Evaluation of Legal Multi-Label
Classification Baselines
Martina Forster and Claudia Schulz and Prudhvi Nokku and
Melicaalsadat Mirsafian and Jaykumar Kasundra and Stavroula Skylaki
Thomson Reuters Labs
Abstract
Multi-Label Classification (MLC) is a common
task in the legal domain, where more than one
label may be assigned to a legal document. A
wide range of methods can be applied, ranging
from traditional ML approaches to the latest
Transformer-based architectures. In this work,
we perform an evaluation of different MLC
methods using two public legal datasets, POS-
TURE50K and EURLEX57K. By varying the
amount of training data and the number of la-
bels, we explore the comparative advantage
offered by different approaches in relation to
the dataset properties. Our findings highlight
DistilRoBERTa and LegalBERT as performing
consistently well in legal MLC with reasonable
computational demands. T5 also demonstrates
comparable performance while offering advan-
tages as a generative model in the presence
of changing label sets. Finally, we show that
the CrossEncoder exhibits potential for notable
macro-F1 score improvements, albeit with in-
creased computational costs.
1
Introduction
Multi-label classification (MLC) is a common task
in the legal domain, where more than one label may
be assigned to a legal document (Chalkidis et al.,
2022). One of the challenges of legal MLC is the
long-tail label distribution, i.e., few labels occur
frequently, whereas the majority of labels occurs
very rarely in the data (Chalkidis et al., 2019b).
In the legal domain, a number of studies have ex-
plored the performance of various MLC methods,
ranging from traditional ML approaches based on
sparse vectors such as TF-IDF (Xiao et al., 2018) to
the latest state-of-the-art Transformer based mod-
els (Kementchedjhieva and Chalkidis, 2023). Ad-
ditionally, domain-specific models trained on legal
text may exhibit enhanced performance compared
to general-purpose models (Chalkidis et al., 2020).
For practical MLC applications, the choice of an
appropriate method depends on the data properties
such as dataset size, number of labels and stable
vs. changing label set, the model performance, and
technical considerations, such as resource availabil-
ity, cost, and runtime. It is therefore important for
applied researchers to rapidly establish an effec-
tive experimentation strategy by examining a set of
appropriate baseline models.
We aim to provide an informative starting collec-
tion of baselines by comparing several established
MLC approaches. To elucidate the advantages and
weaknesses of the different approaches, we anaylse:
1) the effect of label quantity on the performance
of the various methods, 2) the effect of dataset size
on the performance of the various methods, 3) the
performance of domain-specific vs generic models,
4) the time, cost, and performance trade-offs of
different methods.
2
Related Work
In the legal domain, MLC is a common task with
a broad range of applications such as legal motion
detection (Vacek et al., 2019), case outcome pre-
diction (Medvedeva et al., 2020), EU legislative
documents categorization (Chalkidis et al., 2019c),
user right violations in terms of services (Lippi
et al., 2019), and legal procedural posture classifi-
cation (Song et al., 2022).
Especially in real-world legal scenarios, MLC
tasks are often challenging due to high label im-
balance as well as patterns of label co-occurrences.
In the legal domain, this phenomenon is exacer-
bated in the presence of extremely large label sets
(Chalkidis et al., 2019b), incomplete or unreliable
annotation data (Braun, 2023), and scarcity of volu-
minous and high quality datasets (Xiao et al., 2018;
Chalkidis et al., 2019a, 2021; Song et al., 2022). Le-
gal documents present additional challenges in the
form of domain specific vocabulary,which makes
domain-adapted Transformer-based language mod-
els outperform out-of-the-box variants (Chalkidis
et al., 2020).
arXiv:2401.11852v1  [cs.CL]  22 Jan 2024
There are several machine learning methods
that can be adopted for MLC problems. Early
approaches were based on TF-IDF vectors (Xiao
et al., 2018), while recent systems increasingly rely
on Transformers, either encoder-only approaches,
such as legal-domain BERT (Chalkidis et al., 2020;
Henderson et al., 2022; Zheng et al., 2021) and
RoBERTa (Song et al., 2022), or encoder-decoder
architectures such as T5 (Liu et al., 2021; Ke-
mentchedjhieva and Chalkidis, 2023). In addition,
loss functions specifically designed to address the
imbalanced label distributions, such as focal loss
(Lin et al., 2020), may be used to improve the train-
ing process. Finally, the formulation of the MLC
task as a sentence similarity task between text and
label using SBERT-based approaches (Reimers and
Gurevych, 2019) may be applied in the case of
frequently evolving label sets.
3
Experiments
To study the effect of data set size and label quantity
on the performance of different MLC algorithms,
we use two legal MLC datasets to simulate various
data scenarios, POSTURE50K (Song et al., 2022)
and EURLEX57K (Chalkidis et al., 2019c) (for
detailed dataset description see A.1).
3.1
Dataset Construction
For model training, we retain the original train-dev-
test splits of the dataset and take data samples from
within each split, i.e., original train data will never
be part of the test data in any of our data samples.
To investigate the effect of label quantity
on model performance, we experiment with us-
ing only the k most frequent labels for k
∈
[5, 20, 50, 100, 200, 1000]1 in the train, dev, and
test sets. This is done by removing labels not in
the k most frequent ones from each data point and
discarding data points without labels. The effect
of dataset size on model performance is accounted
for by downsampling the number of training data
points m to m ∈ [1000, 2000, 5000, 10000] (this
is done after label sampling) (for details see A.2).
The different choices of k and m result in 20
sampled training sets for POSTURE50K and 24
for EURLEX57K.
11000 is only used for EURLEX57K since POSTURE50K
has less than 1000 labels.
3.2
MLC Models
We choose three sparse vector similarity methods,
namely ClassTFIDF, DocTFIDF, and BM25 (de-
tails can be found in A.3). We compare these
with different types of Transformer architectures:
We experiment with two frequently used language
models fine-tuned for MLC, DistilRoBERTa (Sanh
et al., 2019) and the domain-specific LegalBERT
(Zheng et al., 2021), both fine-tuned using focal
loss (Lin et al., 2020) to mitigate the impact of
label imbalances. T5 is chosen as a generative ap-
proach (Raffel et al., 2020), while BiEncoder and
CrossEncoder architectures compare semantic sim-
ilarity between label and text within a language
model (Reimers and Gurevych, 2019). We choose
LegalBERT (Zheng et al., 2021) as the model for
the BiEncoder and CrossEncoder architectures.
Note that we use default parameters for all mod-
els as we aim to provide solid baselines rather than
optimised solutions. Details about the model pa-
rameters can be found in A.4. All Transformer
models in our experiments use only the first 512
tokens of input text, a common way of handling
longer legal documents (Mamakas et al., 2022).
4
Results
To gain insights on the strengths of each model
on the legal MLC datasets, we examine the per-
formance of the different models trained on the
various size trainsets. Figure 1 shows the micro-
and macro-F1 scores on the POSTURE50K dataset.
Similar trends are observed on the EURLEX57K
dataset, as illustrated in Figure 2 in A.5.
4.1
What is the influence of dataset size?
Figure 1 shows that some models can handle a
small training set better than others. LegalBERT
and DistilRoBERTa are unable to learn at all2 when
there is not enough training data compared to the
number of labels, which is the case for 2000 or
less data points with 200 labels, and 5000 or less
data points with 1000 labels (also see Figures 2
and 3 in A.5). For EURLEX57K these models are
able to already learn with slightly smaller training
sets. Leite et al. (2020) and Lee et al. (2020) also
observe that BERT models may not be able to learn
with very small training sets.
2The F1 score is close to 0 due to precision being close to
0, whereas recall is high, indicating that the model predicts
most labels for almost all data points.
Figure 1: Micro- and macro-F1 scores of multi-label classifiers on POSTURE50K data with top 20 and top 200
labels for different training set sizes.
All other models are less affected by a small
training set, and performance increases only
slightly when training with more than 5000 data
points. We observe that the macro-F1 benefits more
from additional training data than the micro-F1, es-
pecially with larger numbers of labels, indicating
that models can improve performance on minority
labels when more data becomes available.
4.2
What is the influence of label quantity?
We observe that the performance of all models de-
creases with a higher number of labels. All models
show a sharp drop in performance from 5 to 20
labels and a more gradual decrease with increased
label quantity thereafter (also see Figure 4 in A.5).
TF-IDF model performance is particularly affected
by the increase in labels. Similar to dataset size, la-
bel quantity affects macro-F1 more than micro-F1.
4.3
Are domain-specific models better?
Comparing the performance of DistilRoBERTa and
LegalBERT, we observe on-par or slightly better
performance of LegalBERT for POSTURE50K,
while the opposite holds for EURLEX57K. This
may be because labels in POSTURE50K are from
the legal domain (e.g., ""On Appeal"" or ""Motion to
Dismiss"") while labels in EURLEX57K are more
general (e.g., ""animal product"" or ""tropical fruit""),
thus requiring less legal understanding.
4.4
What are the best legal MLC baselines?
In Figure 1 we can see that DistilRoBERTa and
LegalBERT are the top performing algorithms for
both datasets in terms of micro-F1 score, except if
there is not enough training data. T5 is a close con-
tender, and, since it is a generative model, suitable
for changing label sets. It can simply be fine-tuned
on a smaller dataset with new labels or even be used
in a zero-shot setup, whereas RoBERTa and Legal-
2k - 20
2k - 200
10k - 20
10k - 200
Model
GPUs
CPUs
GiB
h
$
h
$
h
$
h
$
ClassTFIDF
–
4
16
0.4
0.1
0.6
0.2
0.5
0.1
0.8
0.3
DocTFIDF
–
4
16
0.3
0.1
0.3
0.1
0.5
0.1
0.5
0.1
BM25
–
4
16
1.5
0.7
1.6
0.8
7.4
4.1
6.8
3.8
T5
1 T4
4
16
3.2
2.9
2.8
2.5
4.7
4.1
4.4
3.9
BiEncoder
1 Tesla V100
8
61
0.6
2.3
0.8
3.3
2.0
8.1
2.5
10.0
CrossEncoder
1 Tesla V100
8
61
1.2
7.4
6.1
44.7
2.6
12.8
7.6
50.4
DistilRoBERTa
1 T4
8
32
1.5
1.7
0.6
0.7
2.4
2.6
5.6
5.9
LegalBERT
1 T4
8
32
2.3
2.5
1.0
1.1
4.1
4.4
12.0
12.7
Table 1: Time (h) and cost ($) for training and evaluating models using different sampled training sets (2k, 10k) and
numbers of labels (20, 200) of POSTURE50K and specification of compute resources used.
BERT need to be trained from scratch on the full
data to enable new label prediction. The best vec-
tor similarity methods are DocTFIDF and BM25,
showing reasonable performance throughout.
In terms of macro-F1, DistilRoBERTa and Legal-
BERT are mostly in the top range as well. However,
for larger numbers of labels, the CrossEncoder can
significantly improve the macro-F1 score for POS-
TURE50K: when dealing with 200 labels it outper-
forms the other methods by a large margin. For
EURLEX57K we do not observe this behaviour,
with the CrossEncoder performing worse than Dis-
tilRoBERTa and LegalBERT in terms of macro-
F1. This may be because the legal labels in POS-
TURE50K have higher semantic similarity with
the legal text being classified than the more general
labels in EURLEX57K. Similar to T5 the CrossEn-
coder is suitable for changing label sets, including
zero-shot predictions.
4.5
What is the cost-performance trade-off?
Table 1 shows the time and cost of training and
evaluating the different models when varying label
quantity and training set size for POSTURE50K
(similar trends are observed for EURLEX57K) 3.
Larger training sets and label quantities increase
training and evaluation time across the board.
CrossEncoder, DistilRoBERTa, and LegalBERT
become significantly slower with increasing num-
ber of labels, whereas the speed of T5 is not af-
fected by more labels at all.
TF-IDF methods are significantly faster than
3Note that the BiEncoder and CrossEncoder were run on
a more powerful, and thus faster but more expensive, GPU
instance than the other Transformer models. Training time on
the T4 GPU would be much slower. Also note that training
time and cost is deceptively low for DistilRoBERTa and Legal-
BERT with 2000 training data points and 200 labels since they
are unable to learn and thus training stops early.
BM25.
Since DocTFIDF also has reasonable
prediction performance, it has an excellent cost-
performance trade-off. The most expensive algo-
rithm by a large margin, especially for large label
sets, is the CrossEncoder, incurring 10x the cost
of DistilRoBERTa for the training set with 10,000
data points and 200 labels. Whether the increased
cost is worth the increased macro-F1 performance
will depend on the application. DistilRoBERTa is
nearly twice as fast (and thus half as expensive) as
LegalBERT due to its lower number of model pa-
rameters. Since the performance of DistilRoBERTa
and LegalBERT is very similar, DistilRoBERTa has
a very good cost-performance trade-off.
5
Conclusions and Future Work
In this work, we explore a number of established
MLC approaches in the legal domain. Our aim is
to provide insights for the legal community by ex-
perimenting with a collection of common baselines
in relation to various data and hardware properties.
In summary, our experiments show that Distil-
RoBERTa and LegalBERT generally have a rea-
sonable cost-performance trade-off for legal MLC,
yielding superior results, unless the training set size
is too small. T5 also gives reasonable performance,
while offering flexibility in the case of changing
label sets. DocTFIDF is worth trying, as it is fast
and cheap to run. CrossEncoders can yield signifi-
cant performance gains in macro-F1 score in some
cases, although at a higher price.
In future work, we will experiment with algo-
rithms able to handle long documents, such as
Longformer (Mamakas et al., 2022; Chalkidis et al.,
2023; Niklaus and Giofré, 2023) and LLMs (Traut-
mann et al., 2022; Koco´n et al., 2023; Gema et al.,
2023).
References
Daniel Braun. 2023. I beg to differ: how disagreement
is handled in the annotation of legal machine learning
data sets. Artificial Intelligence and Law.
Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Ale-
tras. 2019a.
Neural legal judgment prediction in
English. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
pages 4317–4323, Florence, Italy. Association for
Computational Linguistics.
Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos
Malakasiotis, Nikolaos Aletras, and Ion Androut-
sopoulos. 2019b. Extreme multi-label legal text clas-
sification: A case study in EU legislation. In Proceed-
ings of the Natural Legal Language Processing Work-
shop 2019, pages 78–87, Minneapolis, Minnesota.
Association for Computational Linguistics.
Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos
Malakasiotis, and Ion Androutsopoulos. 2019c.
Large-scale multi-label text classification on EU leg-
islation. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
pages 6314–6322, Florence, Italy. Association for
Computational Linguistics.
Ilias Chalkidis, Manos Fergadiotis, and Ion Androut-
sopoulos. 2021. MultiEURLEX - a multi-lingual and
multi-label legal document classification dataset for
zero-shot cross-lingual transfer. In Proceedings of
the 2021 Conference on Empirical Methods in Natu-
ral Language Processing, pages 6974–6996, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Ilias Chalkidis, Manos Fergadiotis, Prodromos Malaka-
siotis, Nikolaos Aletras, and Ion Androutsopoulos.
2020. LEGAL-BERT: The muppets straight out of
law school. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020, pages 2898–
2904, Online. Association for Computational Lin-
guistics.
Ilias Chalkidis, Nicolas Garneau, Catalina Goanta,
Daniel Katz, and Anders Søgaard. 2023. LeXFiles
and LegalLAMA: Facilitating English multinational
legal language model development. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 15513–15535, Toronto, Canada. Association
for Computational Linguistics.
Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael
Bommarito, Ion Androutsopoulos, Daniel Katz, and
Nikolaos Aletras. 2022. LexGLUE: A benchmark
dataset for legal language understanding in English.
In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 4310–4330, Dublin, Ireland.
Association for Computational Linguistics.
Aryo Pradipta Gema, Luke Daines, Pasquale Minervini,
and Beatrice Alex. 2023. Parameter-efficient fine-
tuning of llama for the clinical domain.
CoRR,
abs/2307.03042.
Peter Henderson, Mark S. Krass, Lucia Zheng, Neel
Guha, Christopher D. Manning, Dan Jurafsky, and
Daniel E. Ho. 2022. Pile of law: Learning respon-
sible data filtering from the law and a 256gb open-
source legal dataset. In NeurIPS.
Yova Kementchedjhieva and Ilias Chalkidis. 2023. An
exploration of encoder-decoder approaches to multi-
label classification for legal and biomedical text.
In Findings of the Association for Computational
Linguistics: ACL 2023, pages 5828–5843, Toronto,
Canada. Association for Computational Linguistics.
Jan Koco´n, Igor Cichecki, Oliwier Kaszyca, Mateusz
Kochanek, Dominika Szydło, Joanna Baran, Julita
Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil
Kanclerz, Anna Koco´n, Bartłomiej Koptyra, Wik-
toria Mieleszczenko-Kowszewicz, Piotr Miłkowski,
Marcin Oleksy, Maciej Piasecki, Łukasz Radli´nski,
Konrad Wojtasik, Stanisław Wo´zniak, and Prze-
mysław Kazienko. 2023. Chatgpt: Jack of all trades,
master of none. Information Fusion, 99:101861.
Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.
2020. Mixout: Effective regularization to finetune
large-scale pretrained language models. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net.
João Augusto Leite, Diego Silva, Kalina Bontcheva,
and Carolina Scarton. 2020. Toxic language detec-
tion in social media for Brazilian Portuguese: New
dataset and multilingual analysis. In Proceedings of
the 1st Conference of the Asia-Pacific Chapter of the
Association for Computational Linguistics and the
10th International Joint Conference on Natural Lan-
guage Processing, pages 914–924, Suzhou, China.
Association for Computational Linguistics.
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,
and Piotr Dollár. 2020. Focal loss for dense object
detection. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 42(2):318–327.
Marco Lippi, Przemyslaw Palka, Giuseppe Con-
tissa, Francesca Lagioia, Hans-Wolfgang Mick-
litz, Giovanni Sartor, and Paolo Torroni. 2019.
CLAUDETTE: an automated detector of potentially
unfair clauses in online terms of service. Artificial
Intelligence and Law, 27(2):117–139.
Frederick Liu, Siamak Shakeri, Hongkun Yu, and Jing
Li. 2021. Enct5: Fine-tuning T5 encoder for non-
autoregressive tasks. CoRR, abs/2110.08426.
Dimitris Mamakas, Petros Tsotsi, Ion Androutsopou-
los, and Ilias Chalkidis. 2022. Processing long legal
documents with pre-trained transformers: Modding
LegalBERT and longformer. In Proceedings of the
Natural Legal Language Processing Workshop 2022,
pages 130–142, Abu Dhabi, United Arab Emirates
(Hybrid). Association for Computational Linguistics.
Masha Medvedeva, Michel Vols, and Martijn Wieling.
2020. Using machine learning to predict decisions
of the european court of human rights. Artificial
Intelligence and Law, 28:237–266.
Joel Niklaus and Daniele Giofré. 2023. Can we pre-
train a SotA legal language model on a budget from
scratch?
In Proceedings of The Fourth Workshop
on Simple and Efficient Natural Language Process-
ing (SustaiNLP), pages 158–182, Toronto, Canada
(Hybrid). Association for Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research,
21(140):1–67.
Nils Reimers and Iryna Gurevych. 2019.
Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
3982–3992, Hong Kong, China. Association for Com-
putational Linguistics.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of BERT: smaller, faster, cheaper and lighter. CoRR,
abs/1910.01108.
Dezhao Song, Andrew Vold, Kanika Madan, and Frank
Schilder. 2022. Multi-label legal document classifi-
cation: A deep learning-based approach with label-
attention and domain-specific pre-training. Informa-
tion Systems, 106:101718.
Dietrich Trautmann, Alina Petrova, and Frank Schilder.
2022. Legal prompt engineering for multilingual
legal judgement prediction. CoRR, abs/2212.02199.
Andrew Trotman, Antti Puurula, and Blake Burgess.
2014. Improvements to bm25 and language models
examined. In Proceedings of the 19th Australasian
Document Computing Symposium, ADCS ’14, page
58–65, New York, NY, USA. Association for Com-
puting Machinery.
Thomas Vacek, Dezhao Song, Hugo Molina-Salgado,
Ronald Teo, Conner Cowling, and Frank Schilder.
2019. Litigation analytics: Extracting and querying
motions and orders from US federal courts. In Pro-
ceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics (Demonstrations), pages 116–121, Min-
neapolis, Minnesota. Association for Computational
Linguistics.
Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu,
Zhiyuan Liu, Maosong Sun, Yansong Feng, Xianpei
Han, Zhen Hu, Heng Wang, and Jianfeng Xu. 2018.
CAIL2018: A large-scale legal dataset for judgment
prediction. CoRR, abs/1807.02478.
Hui Ye, Zhiyu Chen, Da-Han Wang, and Brian D. Davi-
son. 2020.
Pretrained generalized autoregressive
model with adaptive probabilistic label clusters for
extreme multi-label text classification. In Proceed-
ings of the 37th International Conference on Ma-
chine Learning, ICML 2020, 13-18 July 2020, Vir-
tual Event, volume 119 of Proceedings of Machine
Learning Research, pages 10809–10819. PMLR.
Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai,
Hiroshi Mamitsuka, and Shanfeng Zhu. 2019. At-
tentionxml: Label tree-based attention-aware deep
model for high-performance extreme multi-label text
classification. In Advances in Neural Information
Processing Systems 32: Annual Conference on Neu-
ral Information Processing Systems 2019, NeurIPS
2019, December 8-14, 2019, Vancouver, BC, Canada,
pages 5812–5822.
Lucia Zheng, Neel Guha, Brandon R. Anderson, Peter
Henderson, and Daniel E. Ho. 2021. When does
pretraining help? assessing self-supervised learning
for law and the casehold dataset of 53,000+ legal
holdings. In Proceedings of the Eighteenth Inter-
national Conference on Artificial Intelligence and
Law, ICAIL’21, page 159–168, New York, NY, USA.
Association for Computing Machinery.
A
Appendix
A.1
Datasets
A.1.1
POSTURE50K
POSTURE50K (Song et al., 2022) is a multi-label
dataset consisting of 50,000 legal cases from US
courts that were manually labeled with 256 legal
procedural postures. On average, there are 2,901
words per document. The training set contains
31,944, the validation set 7,991, and the test set
10,065 legal cases, respectively. To the best of
our knowledge, state-of-the-art performance on
the POSTURE50K dataset is 0.272 macro-F1 by
LAMT_MLC (Song et al., 2022), a custom pre-
trained RoBERTa architecture with label-attention,
and 0.812 micro-F1 by APLC_XLNet (Ye et al.,
2020), a fine-tuned XLNet model with clustering,
as reported by Song et al. (2022).
A.1.2
EURLEX57K
EURLEX57K (Chalkidis et al., 2019c) comprises
57,000 English EU legislative documents from EU-
RLEX, which were annotated with 4,271 concepts
from the European Vocabulary EUROVOC. The
average number of words per document is 727,
and the number of documents is 45,000 for train,
6,000 for validation, and 6,000 for test, respectively.
To the best of our knowledge, state-of-the-art per-
formance on the EURLEX57K dataset is 0.284
macro-F1 by LAMT_MLC (Song et al., 2022) and
AttentionXML (You et al., 2019), and 0.762 micro-
F1 by LAMT_MLC (Song et al., 2022), as reported
by Song et al. (2022).
A.2
Sampling Strategy
To investigate the effect of dataset size on
model performance, we downsample the num-
ber
of
training
data
points
m
to
m
∈
[1000, 2000, 5000, 10000] (this is done after label
sampling). The sampling first picks one data point
for each label combination (out of the k labels)
occurring in the original train split to ensure that
each label combination is also represented in the
sampled training set4. The remaining data points
are sampled randomly. We ensure that smaller sam-
pled training sets are sub-sets of larger ones, i.e.
the training set with 1000 data points is contained
in the training set with 2000 data points. This en-
sures that model performance differences observed
on sampled training sets with different sizes are
indeed due to the size rather than the exact data
points included in the sets.
Note that for comparability we use the original
dev and test sets when experimenting with the sam-
pled training sets. However, since we do not aim to
evaluate zero-shot learning here, we limit the dev
and test sets to the k most frequent labels.
A.3
Sparse vector similarity-ranking
ClassTFIDF is a similarity-based approach, where
a TF-IDF vector is obtained for each label (or
""class"") from the concatenation of all documents
in the training set with that label. To obtain predic-
tions for a new document, the cosine-similarity be-
tween the new document’s TF-IDF vector and each
of the label vectors is calculated. Labels whose sim-
ilarity is over a pre-defined threshold are assigned
to the new document. After initial experimentation
we assign the top N most similar labels instead of
using a similarity threshold, where N is the median
number of labels for each data point in the training
set.
DocTFIDF refers to the approach where TF-IDF
vectors for each document in the training set are
4If the number of label combinations is > m, not all label
combinations will be included in the sampled training set.
calculated and then compared the TF-IDF vector of
a new test document using cosine similarity. The
new document gets assigned the set of labels that
belong to the nearest training data point.
Finally, BM25 is the classic ranking function that
ranks a set of documents based on the query terms
appearing in each document, where the query terms
is the BM25 index of the test document (Trotman
et al., 2014). Labels are then assigned as described
for the DocTFIDF approach. Again, we assign the
top N labels, where N is the median number of
labels per document in the training set.
A.4
Hyperparameters
We use the following hyperparameters for model
training:
• DistilRoBERTa
– Huggingface
model:
https:
//huggingface.co/
distilroberta-base
– Epochs: 35
– Batch size: 64
– Early stopping patience: 8
– Learning rate: 0.0001
– Learning rate scheduler: cosine
– Warm-up Ratio: 0.1
• LegalBERT
– Huggingface
model:
https:
//huggingface.co/casehold/
custom-legalbert
– Epochs: 35
– Batch size: 64
– Early stopping patience: 8
– Learning rate: 0.0001
– Learning rate scheduler: cosine
– Warm-up Ratio: 0.1
• T5
– Huggingface
model:
https:
//huggingface.co/t5-small
– Task prefix: ""summarize:""
– Epochs: 20
– Batch size: 16
– Early stopping patience: 3
– Learning rate: 1e-4
– Weight Decay: 0.01
• CrossEncoder
– Huggingface
model:
https:
//huggingface.co/casehold/
custom-legalbert
– Epochs: 1
– Batch size: 8
– Number of negative samples created for
each positive sample: 10
– The top N labels with highest predictions,
where N is the median number of labels
per document in the training set, are as-
signed at inference time
• BiEncoder
– Huggingface
model:
https:
//huggingface.co/casehold/
custom-legalbert
– Epochs: 1
– Batch size: 8
– Loss: CosineSimilarityLoss
– The top N labels with highest predictions,
where N is the median number of labels
per document in the training set, are as-
signed at inference time
A.5
Supplementary Figures
Figure 2: Micro- and macro-F1 scores of multi-label classifiers on EURLEX57K data with top 20 and top 200
labels for different training set sizes.
Figure 3: Micro- and macro-F1 scores of multi-label classifiers on EURLEX57K data with top 1000 labels for
different training set sizes.
Figure 4: Performance of multi-label classifiers on the full POSTURE50K (31,944 data points) and EURLEX57K
(45,000 data points) data with varying label quantities.
"
"We develop a computation flow abstraction method for binary Transformers to enhance the efficiency of QMM execution by optimizing the computation order. We also propose BETA, a binarized energy-efficient Transformer accelerator, to boost efficient deployment at the edge. BETA features a configurable QMM engine, supporting diverse activation precisions of binary Transformers and providing high parallelism and high speed for QMMs with impressive energy efficiency. Experimental results evaluated on ZCU102 FPGA show BETA achieves an average energy efficiency of 174 GOPS/W, which is 1.76–21.92× higher than prior FPGA-based accelerators, demonstrating BETA’s good potential for edge Transformer acceleration.","Binary Transformers are gaining popularity for edge deployment due to their compact model size, low computational complexity, and favorable inference accuracy. However, several challenges hinder their efficient execution on existing processors.
We tackle these challenges by presenting a computation flow abstraction method that optimizes the computation order and BETA, a binarized energy-efficient Transformer accelerator featuring a configurable QMM engine. Our method effectively reduces the number of full-precision operations. BETA achieves an average energy efficiency of 174 GOPS/W, outperforming prior FPGA-based accelerators due to its high parallelism, high-speed QMM engine, and activation precision flexibility.","Various quantization approaches have been proposed to address the computational and memory demands of Transformer models, with binary Transformers receiving particular attention due to their 32× or 16× compression ratios compared to full-precision or moderately quantized models. Despite these advantages, deploying binary Transformers on edge devices faces challenges due to the inefficient execution of QMMs on existing processors and the energy consumption overhead caused by multi-precision activations. Moreover, to meet different edge scenarios, deploying binary Transformers with diverse activation precisions is necessary. Prior efforts have focused on optimizing Transformers for full-precision or moderately quantized models, while binary Transformer accelerators that support diverse activation precisions and offer high energy efficiency are lacking.nannannan","Our approach involves adjusting the computation order and fusing full-precision coefficients and offsets to reduce computational complexity without impacting model accuracy.
BETA comprises a QMM engine, a vector process unit, non-linear function modules, and on-chip buffers. The QMM engine features dynamic configuration and high computational efficiency. It consists of N-parallel dot product units, a compute buffer, a binary weight buffer, and control logics. The dot product units utilize unfolding techniques to exploit parallelism within a single vector. Additionally, a compressor tree loop mitigates carry chain propagation and limits the loop delay to logarithmic relationship with the number of elements.
BETA's PE sequence can flexibly perform computations by configuring packing format and accumulation times according to the combination of activation precision and QMM type.nan","Implementing BETA on the Xilinx ZCU102 FPGA platform, we conduct functional simulation, extract actual data from benchmarks, and measure inference latency. We also estimate power consumption by incorporating the switching activity interchange format (SAIF) file into the Vivado Power Analysis Tool. Additionally, we perform inference on an Intel i7-10510U CPU and an NVIDIA RTX 3090 GPU for cross-platform comparison.
BETA exhibits 91.86× and 17.21× improvement in throughput and energy efficiency, respectively, compared to FP-32 and FIX-16 baselines implemented on the same FPGA.
In comparison with previous FPGA-based works and commercial products, BETA demonstrates 1.76–21.92× higher energy efficiency improvement. Moreover, BETA achieves 643.32× and 124.93× energy efficiency improvement compared to CPU and GPU, respectively.","We develop a computation flow abstraction method and propose BETA, a binary Transformer accelerator, to enable flexible and efficient deployment of binary Transformers at the edge. BETA's configurable QMM engine supports diverse activation precisions and offers high parallelism and speed for QMMs with impressive energy efficiency. Experimental results show BETA's potential for edge Transformer acceleration by achieving an average energy efficiency of 174 GOPS/W, significantly outperforming prior FPGA-based accelerators.",BETA: Binarized Energy-Efficient Transformer Accelerator at the Edge,"Yuhao Ji, Chao Fang, Zhongfeng Wang","BETA: Binarized Energy-Efficient Transformer
Accelerator at the Edge
Yuhao Ji1, Chao Fang1, and Zhongfeng Wang1,2(B)
1School of Electronic Science and Engineering, Nanjing University, Nanjing, China
2School of Integrated Circuits, Sun Yat-sen University, Shenzhen, China
Email: {201180131, fantasysee}@smail.nju.edu.cn, zfwang@nju.edu.cn
Abstract—Existing binary Transformers are promising in edge
deployment due to their compact model size, low computational
complexity, and considerable inference accuracy. However, de-
ploying binary Transformers faces challenges on prior processors
due to inefficient execution of quantized matrix multiplication
(QMM) and the energy consumption overhead caused by multi-
precision activations. To tackle the challenges above, we first
develop a computation flow abstraction method for binary Trans-
formers to improve QMM execution efficiency by optimizing
the computation order. Furthermore, a binarized energy-efficient
Transformer accelerator, namely BETA, is proposed to boost
the efficient deployment at the edge. Notably, BETA features
a configurable QMM engine, accommodating diverse activation
precisions of binary Transformers and offering high-parallelism
and high-speed for QMMs with impressive energy efficiency.
Experimental results evaluated on ZCU102 FPGA show BETA
achieves an average energy efficiency of 174 GOPS/W, which is
1.76∼21.92× higher than prior FPGA-based accelerators, show-
ing BETA’s good potential for edge Transformer acceleration.
I. INTRODUCTION
In recent years, large language models (LLMs) [1] have seen
a surge in popularity, with applications ranging from natural
language understanding [2] and generation [3] to computer
vision [4] and robotics [5]. Transformer-based neural networks
[6] have become the backbone of many LLMs. However,
deploying Transformers on resource-constrained edge devices,
such as mobile phones and wearables, remains challenging due
to their computational and memory demands.
To address this issue, various quantization approaches [7]–
[14] have been proposed, which partially use lower numerical
precision for calculations while maintaining satisfying model
accuracy. Notably, when model parameters are quantized to 1-
bit, also known as binarization, computations can be reduced
to bitwise operations, minimizing both parameter storage and
computational complexity. Compared to 32-bit floating-point
(FP-32) or 16-bit fixed-point (FIX-16) full-precision models,
binary Transformers theoretically offer a 32× or 16× com-
pression ratio, respectively, alleviating the computational and
storage requirements significantly for deploying models on
edge devices. For instance, BiT [11] have achieved a model
compression ratio of 31.2× with a negligible accuracy loss of
only 5.4% for edge deployment. However, edge deployment
of binary Transformers still presents challenges. First, prior
processors or accelerators [15]–[25] are mostly optimized for
full-precision or moderately quantized models, and the key cal-
culations required for binary Transformers, two types of quan-
tized matrix multiplication (QMM), i.e. activation×weight
and activation×activation, cannot be efficiently executed on
them. Second, to meet different edge scenarios with distinct
energy efficiency and accuracy demands, it is necessary to
deploy binary Transformers of different activation precisions
[10], [11]. Multi-precision activations multiplication with no
binary parameter involved potentially increases the energy
consumption overhead.
To tackle the above challenges, in this paper, we first
develop a general computation flow abstraction method for
binary Transformers to reduce the number of full-precision
operations by optimizing the computation order. On top of
that, we propose a binarized energy-efficient Transformer ac-
celerator, namely BETA, enabling efficient binary Transformer
deployment at the edge. To improve the performance of QMM,
which are the dominated operations in binary Transformers,
we design a high-throughput QMM engine in BETA. This
engine leverages the unfolding technique to achieve high
parallelism and optimizes the accumulation structure to reduce
datapath latency. Additionally, we propose a configurable PE
design, flexibly processing diverse activation precisions of
binary Transformers with impressive energy efficiency.
To summarize, our contributions are as follows.
1) We abstract the computation process involved in binary
Transformers by optimizing the computation order and
fusing full-precision coefficients and offsets, which re-
sults in reduced computational complexity and signifi-
cant energy savings without impacts on model accuracy.
2) We propose BETA, a novel architecture to efficiently
deploy binary Transformers. To the best of our knowl-
edge, BETA is the first dedicated accelerator to support
diverse activation precisions of binary Transformers. It
achieves an average energy efficiency of 174 GOPS/W
on ZCU102 FPGA, which is 1.76∼21.92× higher than
prior FPGA-based accelerators [18], [19], [21], [26].
3) We design a high-parallelism, high-speed QMM engine
that performs two types of QMM and accommodates
various activation precisions, enabling dynamic adjust-
ment between computational efficiency and model accu-
racy to meet different application demands at the edge.
II. BACKGROUND AND MOTIVATION
The main structure of a Transformer is a stack of Trans-
former blocks, each of which consists of multi-head attention
arXiv:2401.11851v1  [cs.AR]  22 Jan 2024
Accepted by 2024 IEEE International Symposium on Circuits and Systems (ISCAS 2024)
(MHA) blocks and feed-forward network (FFN) blocks. Fig. 1
presents the details of MHA and FFN blocks in vanilla and
binary Transformers. Compared to the vanilla Transformer, the
binary Transformer incorporates binary weights and quantized
activations, resulting in low parameter storage and computa-
tional complexity.
(a) Vanilla Block
(b) Binarized Block
Softmax
Add & Norm
Binary
Linear
Quant.
Binary
Linear
Binary
Linear
Binary
Linear
Quant.
Quant.
Quant.
Quant.
Quant.
MHA
Binary
Linear
Binary
Linear
Activation
Add & Norm
Quant.
Quant.
FFN
Linear
Linear
Linear
Softmax
Add & Norm
Linear
Linear
Activation
Linear
Add & Norm
MHA
FFN
low parameter 
storage & 
computational 
complexity
Fig. 1.
Overview of MHA and FFN blocks in (a) vanilla Transformer and
(b) binary Transformer, respectively.
Currently, most Transformer hardware accelerators are op-
timized for full-precision [15], [16], [25] or moderately quan-
tized [17]–[23] Transformers. For instance, ViA [16] presents
a novel hardware architecture tailored for accelerating Vision
Transformers (ViT) in the FP-16 format. STA [18] develops
an algorithm-hardware co-optimized framework that enables
flexible and efficient deployment of FIX-16 Transformers by
harnessing general N:M sparsity patterns. EFA-Trans [19]
proposes a hardware design for FIX-8 Transformer models,
which is compatible with both dense and sparse configurations.
Deploying binary Transformers on these accelerators leads to
a huge waste of resources, resulting in low energy efficiency.
Notably, VAQF [26] presents a binary ViT accelerator gener-
ator that fully exploits the speedup potential of binarization
by turning multiplication into bit-wise operation. However,
the generated accelerator only supports one activation preci-
sion in each compilation and does not consider the QMM
of activation×activation. BETA differs from previous works
mainly in two aspects: 1) BETA is dedicated for binary Trans-
formers, and a general computation flow abstraction method
is proposed to further reduce the computational complexity.
2) BETA theoretically supports all binary Transformers, in-
cluding two types of QMM equipped with various activation
precisions, which can be flexibly configured on-the-fly.
III. HARDWARE ACCELERATION
A. Computation Flow Abstraction
In binary Transformers, weights and activations are in the
format of αx + β, where α and β are coefficient and offset
under full-precision, and x is a n-bit integer (INT) number.
When performing multiplication (α1x1 +β1)×(α2x2 +β2) in
Transformer inference on CPU or GPU [9]–[12], full-precision
multiplication is executed instead of integer operation, result-
ing in heavy energy consumption. Also, existing quantized
Transformer accelerators are either designed for the deploy-
ment of fully quantized Transformers without coefficients and
offsets [27], [28], or tailored for quantized Transformers that
solely consider coefficients without accounting for offsets
[26]. This limitation makes them uncompatible with binary
Transformers like BiT [11], BinaryBERT [10] and BiBERT
[12]. To fully leverage the energy-efficient potential of binary
Transformers, a general computation flow abstraction method
is proposed, which involves adjusting the computation orders
and fusing full-precision coefficients and offsets to reduce
computational complexity without impacting model accuracy.
α+γ
γ
α+γ
γ
α+γ
α+γ
α+γ
α+γ
γ
−β
β
−β
β
−β
β
β
β
β
N3 Op
(4N2+N3)Op
α
1
0
1
0
1
1
1
1
0
β
−1
1
−1
1
−1
1
1
1
1
γ
1
1
1
1
1
1
1
1
1
2N3 Iop+(3N2+2)Op
α
β
1
0
1
0
1
1
1
1
0
1
1
1
1
1
1
1
1
1
−1
1
−1
1
−1
1
1
1
1
−1
1
−1
1
−1
1
1
1
1
γ
β
High
Low
Energy 
Consumption
Op
Iop
Operation Type
Fig. 2. An example of binary activation×weight operation (αA + γ · 1) ×
βW and its computation flow abstraction process together with corresponding
computational complexity. Full-precision number α, β serve as coefficients,
γ serves as offset, and A, W are binary matrices. Op denotes full-precision
operation and Iop denotes integer operation.
Assume one binary activation×weight operation is formu-
lated as (αA + γ · 1) × βW, as is shown Fig. 2. Based on
matrix arithmetic, we adjust the computation order, turning the
expression into A×W ×(αβ)+1×W ×(γβ). In this case, a
full-precision matrix multiplication (MM) is transformed into
a combinational operation of integer MM and multiplication
by full-precision coefficients, reducing time complexity from
N 3 Op to 2N 3 Iop + (3N 2 +2) Op. Noting that both αβ and
γβ can be performed offline, yielding two new coefficients.
Considering the energy savings of Iop compared to FP-32 or
FIX-16 Ops, which can be several tens or even hundreds of
times [29], the abstract computation flow significantly reduces
energy consumption overhead compared to the origin full-
precision MM.
B. Overall Hardware Architecture
Fig. 3 (a) presents the architecture of the proposed BETA,
which comprises a QMM engine, a vector process unit (VPU),
several non-linear function modules, and on-chip buffers.
QMMs, the dominant operations of binary Transformers, are
performed by QMM engine with dynamic configuration and
high computational efficiency. VPU is responsible for the
implementation of full-precision operations involved in the
abstract computation flow including coefficient multiplication,
and offset addition, with vectorized inputs and outputs. As
non-linear functions, including Softmax, Layer Normalization,
and GELU, are not as computationally intensive as QMM,
their operations are maintained with full precision to preserve
the model accuracy. The host MCU is utilized for quantization
2
Accepted by 2024 IEEE International Symposium on Circuits and Systems (ISCAS 2024)
4:2 
Comp.
Softmax
LayerNorm
GELU
Vector Buffer
Vector Process Unit
Binary Weight 
Buffer
Serial to 
Parallel
Parallel to 
Serial
Compute
Buffer
Read Control
Addr. Generator
N
QMM 
Engine
Dot Product
Unit
Dot Product
Unit
Dot Product
Unit
SoC Interface
MCU
Off-Chip 
Memory
Dot Product
Unit
(a)
(b)
PE
X(Jn+J−1)
W(Jn+J−1)
PE
X(Jn+1)
W(Jn+1)
W(Jn)
PE
X(Jn)
…
PE Sequence
… …
Carry Select Adder
4:2 Compressor
XOR
XOR
XOR
XOR
sum
carry
cout
cin
Compressor Tree Loop
4:2 
Comp.
4:2 
Comp.
…
4:2 
Comp.
4:2 
Comp.
…
…
J
out1
out2
Transpose
out2
activation
weight
6:2 
Comp.
4:2 
Comp.
Fig. 3.
(a) Hardware architecture of BETA, where the orange arrows pass
control signals, and the black arrows transfer data. (b) Detailed structure of
dot product unit, which consists of the PE sequence and compressor tree loop.
functions, incurring minimal latency overhead for the inference
of binary Transformers.
C. QMM engine
To improve the overall hardware efficiency and support
different types of QMM, QMM engine is designed with
a focus on high throughput and configurability. As shown
in Fig. 3 (a), it consists of N-parallel dot product units
(DPUs), a compute buffer, a binary weight buffer, and various
control logics. Binary weights are stored in the on-chip buffer
before inference. When performing QMM, the entire matrices
involved in the computation are pre-loaded to the compute
buffer from off-chip memory or weight buffer, which enhances
data reuse and minimizes the required data access bandwidth.
Fig. 3 (b) shows the detailed structure of DPU. Each
DPU is composed of a PE sequence and a compressor tree
loop. In addition to replicating DPUs for N times to process
dot product operations simultaneously, we further leverage
unfolding techniques to exploit parallelism within a single
vector. A DPU can process J elements from one vector
at a time after unfolding. Both replication and unfolding
techniques increase the parallelism of QMM engine. Note that
the factor of N and J can be flexibly adjusted based on the
desired level of parallelism. To reduce the circuit delay of
unfolding structure, we design a compressor tree loop for dot
product accumulation. Compressor-based adder tree is built
to aggregate the J computed results and two accumulation
partial results, thereby mitigating the carry chain propagation
and limiting the loop delay to logarithmic relationship with J,
as is illustrated in Fig. 3 (b). The two outputs of compressor
tree loop are sent to a carry select adder to generate the final
result of dot product. The parallelism improvement and circuit
delay reduction result in high throughput.
PE PE PE PE
1 psums (8b×1b) / cycle
repeat X
times
(8b×Xb)
Number of PEs, i.e., J
&
&
&
&
&
&
&
&
Crossbar
PE
PE
PE PE
&
&
&
&
&
&
&
&
Crossbar
PE
PE
PE PE
&
&
&
&
&
&
&
&
Crossbar
…
&
&
&
&
&
&
&
&
PE PE
PE
PE
Crossbar
2 psums (4b×1b) / cycle
repeat X
times
(4b×Xb)
4 psums (2b×1b) / cycle 
repeat X
times
(2b×Xb)
8 psums (1b×1b) / cycle
repeat X
times
(1b×Xb)
W1A8
W1A4
W1A2
W1A1
Comp.
Tree Loop
Fig. 4. Operation modes of configurable PE sequence, which combines data-
packing and bit-serial to enable flexible configuration to process different
workload. Note that a network with weights quantized to bw bits and
activations quantized to ba bits is denoted as WbwAba [11].
As shown in Fig. 4, PE sequence in DPU can flexibly
perform computations by configuring packing format and
accumulation times according to the combination of activation
precision and QMM type. For example, when performing
binary weight × 4-bit activation in W1A4 mode, two multipli-
cations are executed simultaneously and the results are packed
in 8-bit output of one PE, with one cycle needed. Furthermore,
when the QMM type is 4-bit activation×activation, one input
operand is traversed on bit-level within four cycles to generate
the results.
TABLE I
FPGA RESOURCE BREAKDOWN OF BETA
LUT
FF
BRAM
DSP
QMM Engine
Dot Product
Unit
154K
49K
-
-
Compute&Weight
Buffer
-
-
456
Others
21K
25K
-
-
VPU
4K
-
-
64
Others
12K
14K
87
-
Total
191K
88K
543
64
Utilization
274K
(69.71%)
548K
(16.06%)
912
(59.54%)
2520
(2.54%)
IV. EXPERIMENTAL RESULTS
A. Experimental Setup
BETA is implemented using Vivado 2022.2 on the Xilinx
ZCU102 FPGA platform and evaluated under the benchmarks
embracing state-of-the-art binary Transformers [10]–[12]. We
conduct the functional simulation with the extracted actual
data from benchmarks and measure the inference latency of
BETA. Meanwhile, we generate the annotated toggle rate from
the simulator and dump it into the switching activity inter-
change format (SAIF). Then, power consumption is estimated
by incorporating the SAIF file into the Vivado Power Analysis
Tool. Moreover, for cross-platform comparison, we perform
the inference of the benchmark models on an Intel i7-10510U
CPU and an NVIDIA RTX 3090 GPU, respectively. Note that
3
Accepted by 2024 IEEE International Symposium on Circuits and Systems (ISCAS 2024)
TABLE II
COMPARISON OF BETA WITH PREVIOUS WORKS AND COMMERCIAL PRODUCTS
Platform
CPU
GPU
FPGA
ViA [16]
STA [18]
EFA-Trans [19]
VAQF [26]
Our Work
Baseline1
Baseline2
BETA
i7-10510U
RTX 3090
Alveo U50
ZC702
ZCU102
ZCU102
ZCU102
Technology
14nm
8nm
16nm
16nm
16nm
16nm
16nm
Frequency (Hz)
1.8G
1.7G
300M
150M
N/A
150M
190M
Test Network
BiT
Swin
Transformer
N:M Sparse
Transformer
Sparse
Transformer
Quantized Vision
Transformer
BiT
BiT
BinaryBERT
BiBERT
Computation Abstraction
%
%
%
%
%
""
%
%
""
""
""
BiT Precision
FP-32
FP-32
FP-16
FIX-16
FIX-8
INT&FIX-16†
FP-32
FIX-16
INT&FIX-16†
W/A Precision
W1A1
W1A1
W16A16
W16A16
W8A8
W1A8
W1A1
W1A1
W1A1
Throughput (GOPS)
6.69
484.26
309.60
109.45
279.80
861.20
13.51
72.09
1240.98
1387.59
1436.07
Power (W)
25.00
350.00
38.99
2.71
5.48
8.70
11.64
3.91
7.18
7.95
8.20
Energy Efficiency (GOPS/W)
0.27
1.38
7.94
40.39
51.06
98.98
1.16
18.42
172.41
174.59
175.23
† According to the abstract computation flow in Fig. 2, BETA performs integer (INT) operations in QMMs. And here FIX-16 format is used as full-precision to perform coefficient multiplications
and offset additions.
0
200
400
600
800
1000
1200
1400
1600
Throughput (GOPS)
0
25
50
75
100
125
150
175
200
Energy Efficiency (GOPS/W)
Model: BiT
W1A8
84.30%
W1A4
83.60%
W1A2
82.10%
W1A1
79.50%
0
200
400
600
800
1000
1200
1400
1600
Throughput (GOPS)
0
25
50
75
100
125
150
175
200
Model: BinaryBERT
W1A8
84.20%
W1A4
83.90%
W1A2
62.70%
W1A1
35.60%
Fig. 5. Tradeoff between hardware efficiency and model accuracy on BETA.
data at the edge are usually mini-batch, and therefore the cross-
platform comparison is performed with a single batch size on
various platforms.
B. Hardware Consumption
The running frequency of BETA is 190MHz, and the con-
figuration of parallelism is N=2 and J=256. Table I presents
the FPGA resource breakdown of BETA. DPUs, including
PE sequences and compressor tree loops, dominate the LUT
consumption since they are the computing core of QMM. Most
of BRAMs are occupied by compute buffer and weight buffer
to store inputs and binary weights, respectively.
C. Dynamic Adjustment between Efficiency and Accuracy
We evaluate BiT and BinaryBERT with different activation
precisions on BETA, collecting throughput, energy efficiency,
and model accuracy on the MNLI-m dataset [30] to understand
the tradeoff between hardware efficiency and model accuracy.
As shown in Fig. 5, when the activation precision of the
deployed model decreases, there is a notable improvement in
both throughput and energy efficiency on BETA, while con-
versely, the model accuracy gradually drops. This experiment
demonstrates that BETA enables dynamic adjustment between
model inference efficiency and accuracy, which allows it to
meet deployment requirements in various edge scenarios with
different constraints.
D. Comparison with Baselines and Other Architectures
We first compare BETA with FP-32 and FIX-16 baselines.
Both baselines are implemented on the same FPGA as BETA
with close resource consumption, but use traditional FP-32
or FIX-16 computing units instead of BETA’s computation
flow abstraction DPUs. As shown in Table II, compared
with FP-32 and FIX-16 baselines, BETA exhibits 91.86× and
17.21× improvement on throughput and 148.63× and 9.36×
improvement on energy efficiency, respectively.
Moreover, we compare BETA with other previous FPGA-
based works and commercial CPU and GPU products. VAQF
[26] turns multiplication involved in MM to bit-wise opera-
tion and presents excellent experimental results. In contrast,
BETA further supports multi-precision activation×activation
operations in a unified computation engine, such as 8-bit
query×key in W1A8 self-attention. ViA [16] deploys FP-16
networks without quantization, resulting in much more energy
consumption relative to our low-bit design. STA [18] and EFA-
Trans [19] are both dedicated on deploying another kind of
compressed Transformers, namely sparse Transformers, and
also achieve considerable hardware performance. Compared to
the FPGA-based accelerators mentioned above, BETA presents
1.76∼21.92× higher energy efficiency improvement. In addi-
tion, compared to CPU and GPU, BETA achieves 643.32×
and 124.93× energy efficiency improvement, respectively.
V. CONCLUSION
In this paper, we develop a computation flow abstraction
method and propose a binary Transformer accelerator called
BETA to enable flexible and effcient deployment of bina-
rized Transformers at the edge. BETA features a configurable
quantized matrix multiplication (QMM) engine that supports
diverse activation precisions and offers high parallelism and
speed for QMMs with impressive energy efficiency. Experi-
mental results show that BETA achieves an average energy
efficiency of 174 GOPS/W, which is 1.76∼21.92× higher
than prior FPGA-based accelerators, demonstrating BETA’s
potential for Transformer acceleration at the edge.
ACKNOWLEDGMENT
This work was supported by the National Key R&D Pro-
gram of China under Grant 2022YFB4400604.
4
Accepted by 2024 IEEE International Symposium on Circuits and Systems (ISCAS 2024)
REFERENCES
[1] T. Brown, B. Mann, N. Ryder et al., “Language models are few-shot
learners,” Advances in neural information processing systems (NeurIPS),
vol. 33, pp. 1877–1901, 2020.
[2] J. Devlin, M. Chang, K. Lee et al., “BERT: pre-training of deep
bidirectional transformers for language understanding,” in Proceedings
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL-HLT). ACL, 2019,
pp. 4171–4186.
[3] A. Zhou, K. Wang, Z. Lu et al., “Solving challenging math word prob-
lems using GPT-4 code interpreter with code-based self-verification,”
arXiv preprint arXiv:2308.07921, 2023.
[4] J. Chen, H. Guo, K. Yi et al., “VisualGPT: data-efficient adaptation of
pretrained language models for image captioning,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), 2022, pp. 18 030–18 040.
[5] I. Singh, V. Blukis, A. Mousavian et al., “ProgPrompt: generating
situated robot task plans using large language models,” in IEEE In-
ternational Conference on Robotics and Automation (ICRA).
IEEE,
2023, pp. 11 523–11 530.
[6] A. Vaswani, N. Shazeer, N. Parmar et al., “Attention is all you need,”
Advances in neural information processing systems (NeurIPS), vol. 30,
2017.
[7] O. Zafrir, G. Boudoukh, P. Izsak et al., “Q8BERT: quantized 8bit BERT,”
in Fifth Workshop on Energy Efficient Machine Learning and Cognitive
Computing-NeurIPS Edition (EMC2-NeurIPS), 2019, pp. 36–39.
[8] W. Zhang, L. Hou, Y. Yin et al., “TernaryBERT: distillation-aware ultra-
low bit BERT,” in Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
ACL, 2020, pp.
509–521.
[9] J. Tian, C. Fang, H. Wang et al., “BEBERT: Efficient and robust
binary ensemble BERT,” in IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP).
IEEE, 2023, pp. 1–5.
[10] H. Bai, W. Zhang, L. Hou et al., “BinaryBERT: pushing the limit of
BERT quantization,” in Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing (ACL/IJCNLP).
ACL, 2021, pp. 4334–4348.
[11] Z. Liu, B. Oguz, A. Pappu et al., “BiT: robustly binarized multi-distilled
transformer,” in Advances in neural information processing systems
(NeurIPS), vol. 35, 2022, pp. 14 303–14 316.
[12] H. Qin, Y. Ding, M. Zhang et al., “BiBERT: accurate fully bina-
rized BERT,” in International Conference on Learning Representations
(ICLR), 2022.
[13] P.-H. C. Le and X. Li, “BinaryViT: pushing binary vision Transformers
towards convolutional models,” in Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition (CVPR), 2023, pp.
4664–4673.
[14] H. Song, Y. Wang, M. Wang et al., “Ucvit: Hardware-friendly vision
transformer via unified compression,” in IEEE International Symposium
on Circuits and Systems (ISCAS).
IEEE, 2022, pp. 2022–2026.
[15] S. Lu, M. Wang, S. Liang et al., “Hardware accelerator for multi-head
attention and position-wise feed-forward in the Transformer,” in IEEE
33rd International System-on-Chip Conference (SOCC).
IEEE, 2020,
pp. 84–89.
[16] T. Wang, L. Gong, C. Wang et al., “ViA: A novel vision-Transformer
accelerator based on FPGA,” IEEE Trans. Comput. Aided Des. Integr.
Circuits Syst. (TCAD), vol. 41, no. 11, pp. 4088–4099, 2022.
[17] A. Marchisio, D. Dura, M. Capra et al., “SwiftTron: an efficient
hardware accelerator for quantized Transformers,” in International Joint
Conference on Neural Networks (IJCNN).
IEEE, 2023, pp. 1–9.
[18] C. Fang, A. Zhou, and Z. Wang, “An algorithm-hardware co-optimized
framework for accelerating N:M sparse Transformers,” IEEE Trans. Very
Large Scale Integr. Syst. (TVLSI), vol. 30, no. 11, pp. 1573–1586, 2022.
[19] X. Yang and T. Su, “EFA-Trans: an efficient and flexible acceleration
architecture for Transformers,” Electronics, vol. 11, no. 21, 2022.
[20] B. Li, S. Pandey, H. Fang et al., “FTRANS: energy-efficient acceleration
of Transformers using FPGA,” in Proceedings of the ACM/IEEE Inter-
national Symposium on Low Power Electronics and Design (ISLPED).
ACM, 2020, pp. 175–180.
[21] S. Nag, G. Datta, S. Kundu et al., “ViTA: A vision Transformer inference
accelerator for edge applications,” in IEEE International Symposium on
Circuits and Systems (ISCAS).
IEEE, 2023, pp. 1–5.
[22] A. H. Zadeh, I. Edo, O. M. Awad et al., “GOBO: Quantizing attention-
based NLP models for low latency and energy efficient inference,” in
53rd Annual IEEE/ACM International Symposium on Microarchitecture
(MICRO).
IEEE, 2020, pp. 811–824.
[23] T. J. Ham, S. Jung, S. Kim et al., “A3: Accelerating attention mech-
anisms in neural networks with approximation,” in IEEE Interna-
tional Symposium on High Performance Computer Architecture (HPCA).
IEEE, 2020, pp. 328–341.
[24] C. Fang, S. Guo, W. Wu et al., “An efficient hardware accelerator for
sparse transformer neural networks,” in IEEE International Symposium
on Circuits and Systems (ISCAS).
IEEE, 2022, pp. 2670–2674.
[25] R. Rizk, D. Rizk, F. Rizk et al., “A resource-saving energy-efficient
reconfigurable hardware accelerator for bert-based deep neural network
language models using FFT multiplication,” in IEEE International
Symposium on Circuits and Systems (ISCAS).
IEEE, 2022, pp. 1675–
1679.
[26] M. Sun, H. Ma, G. Kang et al., “VAQF: fully automatic software-
hardware co-design framework for low-bit vision Transformer,” arXiv
preprint arXiv:2201.06618, 2022.
[27] Z. Liu, G. Li, and J. Cheng, “Hardware acceleration of fully quantized
BERT for efficient natural language processing,” in Design, Automation
& Test in Europe Conference (DATE).
IEEE, 2021, pp. 513–516.
[28] G. Islamoglu, M. Scherer, G. Paulin et al., “ITA: an energy-efficient
attention and softmax accelerator for quantized Transformers,” in Pro-
ceedings of the ACM/IEEE International Symposium on Low Power
Electronics and Design (ISLPED).
IEEE, 2023, pp. 1–6.
[29] H. You, H. Shi, Y. Guo et al., “ShiftAddViT: mixture of multiplication
primitives towards efficient vision Transformer,” in Advances in neural
information processing systems (NeurIPS), vol. 36, 2023.
[30] A. Wang, A. Singh, J. Michael et al., “GLUE: A multi-task benchmark
and analysis platform for natural language understanding,” in Interna-
tional Conference on Learning Representations (ICLR), 2019.
5
"
"We propose an approach that describes standard communication protocols using Semantic Web Technology (specifically, OWL-DL and SWRL). Those protocols are made up of communication acts. Our proposal describes those communication acts as terms in a communication acts ontology, called CommOnt. The semantics associated with our described communication acts is expressed through social commitments provided as fluents in Event Calculus.","With the emergence of the Web, administrators of Information Systems are encouraged to expose the functionalities of those systems through agents that represent them or through Web Services. Agent technology and Web Services technology have been developed separately, but work is in progress to consolidate their approaches into a common specification. Our work concentrates on inter-agent communication.","nanIn our approach, each agent is associated with a standard communication protocol. When two agents of different Information Systems want to interoperate, our proposal will reason about the protocols embedded in the agents to discover relationships such as equivalence or restriction. Thus, allowing both agents to use the same protocol by replacing dynamically in one agent the protocol supported by the other.nannan","Our approach uses STS as model of protocols. We restrict to deterministic STS and OWL-DL to represent protocols. To model protocol descriptions we have defined five different classes: Protocol, State, Transition, Fluent, and Commitment. Throughout the paper, we describe protocol semantics and reasoning using OWL-DL and SWRL. We present five protocol relationships: equivalence, restriction, specialization, specialized-equivalence, and specialized-restriction.nan","As results, we present three use cases to demonstrate our approach. In the first we describe a protocol for asking for time. In the second, we present how customization of a protocol can be achieved by considering the use of particular communication acts. Finally, in the third, we describe two protocols used in a specific Information System and demonstrate how those protocols can be related.","Our proposal favors the management of semantics aspects, the customization of standard communication protocols, the use of Semantic Web tools to describe protocols, and the discovery of different kinds of relationships between protocols.",Semantic Web Technology for Agent Communication Protocols,"Idoia Berges, Jesús Bermúdez, Alfredo Goñi, Arantza Illarramendi","arXiv:2401.11841v1  [cs.MA]  22 Jan 2024
Semantic Web Technology for Agent
Communication Protocols
Idoia Berges⋆, Jes´us Berm´udez, Alfredo Go˜ni, and Arantza Illarramendi⋆⋆
University of the Basque Country
{iberges003,jesus.bermudez,alfredo,a.illarramendi}@ehu.es
http://siul02.si.ehu.es
Abstract. One relevant aspect in the development of the Semantic Web
framework is the achievement of a real inter-agents communication ca-
pability at the semantic level. The agents should be able to communi-
cate and understand each other using standard communication protocols
freely, that is, without needing a laborious a priori preparation, before
the communication takes place.
For that setting we present in this paper a proposal that promotes to
describe standard communication protocols using Semantic Web technol-
ogy (speciﬁcally, OWL-DL and SWRL). Those protocols are constituted
by communication acts. In our proposal those communication acts are
described as terms that belong to a communication acts ontology, that
we have developed, called CommOnt. The intended semantics associated
to the communication acts in the ontology is expressed through social
commitments that are formalized as ﬂuents in the Event Calculus.
In summary, OWL-DL reasoners and rule engines help in our proposal
for reasoning about protocols. We deﬁne some comparison relationships
(dealing with notions of equivalence and specialization) between proto-
cols used by agents from diﬀerent systems.
Key words: Protocol, Communication acts, agents.
1
Introduction
In the scenario that promotes the emergent Web, administrators of existing In-
formation Systems, that belong to nodes distributed along the Internet network,
are encouraged to provide the functionalities of those systems through agents
that represent them or through Web Services. The underlying idea is to get a
real interoperation among those Information Systems in order to enlarge the
beneﬁts that users can get from the Web by increasing the machine processable
tasks.
⋆ The work of Idoia Berges is supported by a grant of the Basque Government.
⋆⋆ All authors are members of the Interoperable DataBases Group. This work is also
supported by the University of the Basque Country, Diputaci´on Foral de Gipuzkoa
(cosupported by the European Social Fund) and the Spanish Ministry of Education
and Science TIN2007-68091-C02-01.
Although agent technology and Web Services technology have been devel-
oped in a separate way, there exists a recent work of several members from both
communities trying to consolidate their approaches into a common speciﬁcation
describing how to seamlessly interconnect FIPA compliant agent systems [1] with
W3C compliant Web Services. The purpose of specifying an infrastructure for
integrating these two technologies is to provide a common means of allowing
each to discover and invoke instances of the other [2]. Considering the previ-
ous approach, in the rest of this paper we will only concentrate on inter-agent
communication aspects.
Communication among agents is in general based on the interchange of com-
munication acts. However, diﬀerent Information Systems have incorporated dif-
ferent classes of communication acts as their Agent Communication Language
(acl) to the point that they do not understand each other. Moreover, protocols
play a relevant role in agents communication. A protocol speciﬁes the rules of
interaction between agents by restricting the range of allowed follow-up commu-
nication acts for each agent at any stage during a communicative interaction. It
is widely recognized the interest of using standard communication protocols.
We advocate so that the administrators of the Information Systems proceed
in the following way. When they wish to implement the agents that will represent
their systems, they ﬁrst select, from a repository of standard protocols (there
can exist one or more repositories), those protocols that fulﬁll the goals of their
agents. Sometimes a single protocol will be suﬃcient and other times it will
be necessary to design a protocol as a composition of some other protocols.
Next, they can customize the selected protocols before they incorporate them to
the agents. In that setting, when agents of diﬀerent Information Systems want
to interoperate it will be relevant to reason about the protocols embedded in
the agents in order to discover relationships such as equivalence or restriction
between them. Moreover, once those relationships are discovered both agents
can use the same protocol by replacing dynamically in one agent the protocol
supported by the other. Finally, in our opinion it will be desirable to use a formal
language to represent the protocols.
In this paper we present a proposal that promotes to describe standard com-
munication protocols using Semantic Web Technology (OWL-DL and SWRL).
In addition, communication acts that take part of the protocols are described as
terms that belong to a communication acts ontology, that we have developed,
called CommOnt (see more details about the ontology in [3]). The use of that
ontology favours on the one hand, the explicit representation of the meaning of
the communication acts and on the other hand, the customization of existing
standard protocols by allowing the use of particular communication acts that
can be deﬁned as specializations of existing standard communication acts.
Terms of the CommOnt ontology are described using OWL-DL and we have
adopted the so called social approach [4,5] for expressing the intended semantics
of the communication acts included in the protocols. According to the social
approach, when agents interact they become involved in social commitments or
obligations to each other. Those commitments are public, and therefore they
are suitable for an objective and veriﬁable semantics of agent interaction. So-
cial commitments can be considered as ﬂuents in the Event Calculus, which is
a logic-based formalism for representing actions and their eﬀects. Fluents are
propositions that hold during time intervals. A formula in the Event Calculus
is associated to a communication act for describing its social eﬀects. The set of
ﬂuents that hold at a moment describes a state of the interaction. DL axioms
and Event Calculus formulae apply to diﬀerent facets of communication acts.
DL axioms describe static features and are principally used for communication
act interpretation purposes. Event Calculus formulae describe dynamic features,
namely the social eﬀects of communication acts, and are principally used for
communication act operational contexts such as supervising conversations.
In summary the main contributions of the proposal presented in this paper
are:
– It favours a ﬂexible interoperation among agents of diﬀerent systems by using
standard communication protocols described through tools promoted by the
W3C.
– It facilitates the customization of those standard communication protocols
allowing to use communication acts in the protocols that belong to spe-
ciﬁc acl of Information Systems. The particular communication acts are
described in an ontology.
– It provides a basis to reason about relationships between two protocols in
such a way that the following relations can be discovered: equivalence or
restriction (and also considering a notion of specialization). Moreover, no-
tice that our approach allows to get protocols classiﬁcation in terms of the
intended semantics of communication acts that appear in the protocols.
– It allows modeling the communication among agents without regarding only
to the lower level operational details of how communication acts are inter-
changed but taking also into account the meaning of those acts.
The rest of the paper is organized as follows: Section 2 provides background
on the communication ontology, that contains terms corresponding to commu-
nication acts that appear in the protocols, and on the semantics associated to
those acts. Section 3 explains how protocols are described using Semantic Web
Technology and presents the deﬁnitions of the relationships considered between
protocols. Section 4 discusses diﬀerent related works, and conclusions appear in
the last section.
2
Two basic supports for the proposal: the CommOnt
Ontology and the representation of the semantics of
communication acts
Among the diﬀerent models proposed for representing protocols one which stands
out is that of State Transition Systems (STS).
Deﬁnition 1. A State Transition System is a tuple (S, s0, L, T , F), where
S is a ﬁnite set of states, s0 ∈ S is an initial state, L is a ﬁnite set of labels,
T ⊆ S × L × S is a set of transitions and F ⊆ S is a set of ﬁnal states.
In our proposal we use STS where transitions are labeled with communication act
classes described in a communication acts ontology called CommOnt. That is to
say, the set of labels L is a set of class names taken from that ontology. Moreover,
as mentioned before, the intended semantics associated to the communication
acts in the ontology is expressed through predicates in the Event Calculus that
initiate or terminate ﬂuents. In our case, each state is associated to the set of
ﬂuents that holds at that moment.
In the following two subsections we present the main features of the Com-
mOnt ontology and of the intended semantics associated to communication acts,
respectively.
2.1
Main features of the CommOnt Ontology
The goal of the CommOnt ontology is to favour the interoperation among agents
belonging to diﬀerent Information Systems. The leading categories of that on-
tology are: ﬁrst, communication acts that are used for interaction by actors
and that have diﬀerent purposes and deal with diﬀerent kinds of contents; and
second, contents that are the sentences included in the communication acts.
The main design criteria adopted for the communication acts category of
the CommOnt ontology is to follow the speech acts theory [6], a linguistic the-
ory that is recognized as the principal source of inspiration for designing the
most familiar standard agent communication languages. Following that theory
every communication act is the sender’s expression of an attitude toward some
possibly complex proposition. A sender performs a communication act which is
expressed by a coded message and is directed to a receiver. Therefore, a commu-
nication act has two main components. First, the attitude of the sender which
is called the illocutionary force (F), that expresses social interactions such as
informing, requesting or promising, among others. And second, the propositional
content (p) which is the subject of what the attitude is about. In CommOnt
this F(p) framework is followed, and diﬀerent kinds of illocutionary forces and
contents leading to diﬀerent classes of communication acts are supported. More
speciﬁcally, specializations of illocutionary forces that facilitate the absorption
of aspects of the content into the illocutionary force are considered.
CommOnt is divided into three interrelated layers: upper, standards and
applications, that group communication acts at diﬀerent levels of abstraction.
Classes of the CommOnt ontology are described using the Web Ontology Lan-
guage OWL-DL. Therefore, communication acts among agents that commit to
CommOnt have an abstract representation as individuals of a shared universal
class of communication acts.
In the upper layer, according to Searle’s speech acts theory, ﬁve upper classes
of communication acts corresponding to Assertives, Directives, Commissives, Ex-
pressives and Declaratives are speciﬁed. But also the top class CommunicationAct1
is deﬁned, which represents the universal class of communication acts. Every
particular communication act is an individual of this class. In CommOnt, com-
ponents of a class are represented by properties. The most immediate properties
of CommunicationAct are the content and the actors who send and receive the
communication act. There are some other properties related to the context of a
communication act such as the conversation in which it is inserted or a link to
the domain ontology that includes the terms used in the content.
A standards layer extends the upper layer of the ontology with speciﬁc terms
that represent classes of communication acts of general purpose agent communi-
cation languages, like those from KQML or FIPA-ACL. Although the semantic
framework of those agent communication languages may diﬀer from the seman-
tic framework adopted in CommOnt, in our opinion enough basic concepts and
principles are shared to such an extent that a commitment to ontological rela-
tionships can be undertaken in the context of the interoperation of Information
Systems.
With respect to FIPA-ACL, we can observe that it proposes four primitive
communicative acts [1]: Conﬁrm, Disconﬁrm, Inform and Request. The terms
FIPA-Confirm, FIPA-Disconfirm, FIPA-Inform and FIPA-Request are used to
respectively represent them as classes in CommOnt. Furthermore, the rest of
the FIPA communicative acts are derived from those mentioned four primi-
tives. Analogously, communication acts from KQML can be analyzed and the
corresponding terms in CommOnt speciﬁed. It is of vital relevance for the inter-
operability aim to be able of specifying ontological relationships among classes
of diﬀerent standards.
Finally, it is often the case that every single Information System uses a limited
collection of communication acts that constitute its particular agent communi-
cation language. The applications layer reﬂects the terms describing communica-
tion acts used in such particular Information Systems. The applications layer of
the CommOnt ontology provides a framework for the description of the nuances
of such communication acts. Some of those communication acts can be deﬁned
as particularizations of existing classes in the standards layer and maybe some
others as particularizations of upper layer classes. Interoperation between agents
of two systems using diﬀerent kinds of communication acts will proceed through
these upper and standard layer classes.
Following we show some axioms in the CommOnt ontology. For the presen-
tation we prefer a logic notation instead of the more verbose owl/xml syntax.
CommunicationAct ⊑ =1 hasSender.Actor ⊓ ∀hasReceiver.Actor ⊓
∀hasContent.Content
Request ⊑ Directive ⊓ ∃hasContent.Command
Accept ⊑ Declarative
Responsive ⊑ Assertive ⊓ ∃inReplyTo.Request
1 This type style refers to terms speciﬁed in the ontology.
2.2
Semantics associated to Communication Acts
Formal semantics based on mental concepts such as beliefs, desires and inten-
tions have been developed for specifying the semantics of communication acts.
However, they have been criticized on their approach [4] as well as on their an-
alytical diﬃculties [7]. We have adopted the so called social approach [5,8,9] to
express the intended semantics of communication acts described in the Com-
mOnt ontology. According to the social approach, when agents interact they
become involved in social commitments or obligations to each other.
Deﬁnition 2. A base-level commitment C(x, y, p) is a ternary relation repre-
senting a commitment made by x (the debtor) to y (the creditor) to bring about
a certain proposition p.
Sometimes an agent accepts a commitment only if a certain condition holds
or, interestingly, only when a certain commitment is made by another agent.
This is called a conditional commitment.
Deﬁnition 3. A conditional commitment CC(x, y, p, q) is a quaternary relation
representing that if the condition p is brought out, x will be committed to y to
bring about the proposition q.
Moreover, the formalism we use for reasoning about commitments is based on
the Event Calculus. The basic ontology of the Event Calculus comprises actions,
ﬂuents and time points. It also includes predicates for saying what happens
when (Happens), for describing the initial situation (Initially), for describing
the eﬀects of actions (Initiates and Terminates), and for saying what ﬂuents
hold at what times (HoldsAt). See [10] for more explanations.
Commitments (base-level and conditional) can be considered ﬂuents, and
semantics of communication acts can be expressed with predicates. For example:
– Initiates(Request(s,r,P), CC(r, s, accept(r,s,P), P), t)
A Request from s to r produces the eﬀect of generating a conditional com-
mitment expressing that if the receiver r accepts the demand, it will be
commited to the proposition in the content of the communication act.
– Initiates(Accept(s,r,P), accept(s,r,P), t)
The sending of an Accept produces the eﬀect of generating the accept ﬂuent.
Furthermore, some rules are needed to capture the dynamics of commitments.
Commitments are a sort of ﬂuents typically put in force by communication acts
and that become inoperative after the appearance of other ﬂuents. In the fol-
lowing rules e(x) represents an event caused by x. The ﬁrst rule declares that
when a debtor of a commitment that is in force causes an event that initiates
the proposition committed, the commitment ceases to hold.
Rule 1: HoldsAt( C(x, y, p), t) ∧ Happens(e(x), t) ∧ Initiates(e(x), p, t) →
Terminates(e(x),C(x, y, p), t).
The second rule declares that a conditional commitment that is in force dis-
appears and generates a base-level commitment when the announced condition
is brought out by the creditor.
Rule 2: HoldsAt( CC(x, y, c, p), t) ∧ Happens(e(y), t) ∧ Initiates(e(y), c,
t) → Initiates(e(y),C(x, y, p), t) ∧ Terminates(e(y),CC(x, y, c, p), t).
Following we state some predicates that describe the semantics asociated to
some of the communication acts of the upper level of the CommOnt ontology.
This semantics is determined by the ﬂuents that are initiated or terminated as
a result of the sending of a message between agents.
– Initiates(Assertive(s,r,P), P, t)
– Initiates(Commissive(s,r,C,P), CC(s,r,C,P), t)
– Initiates(Responsive(s,r,P, RA), P, t)
Terminates(Responsive(s,r,P, RA),C(s,r,RA), t)
Eﬀects of these predicates can be encoded with SWRL rules. For instance, the
predicate Initiates(Request(s, r, P), CC(r, s, accept(r, s, P), P), t) can be en-
coded as follows:
Request(x) ∧ hasSender(x,s) ∧ hasReceiver(x,r) ∧ hasContent(x,p) ∧ has-
Commit(x,c) ∧ isConditionedTo(c,a) ∧ atTime(x, t) → initiates(x,c) ∧ has-
Debtor(c,r) ∧ hasCreditor(c,s) ∧ hascondition(c,p) ∧ Acceptance(a) ∧ hasSig-
natory(a,r) ∧ hasAddressee(a,s) ∧ hasObject(a,p) ∧ atTime(c, t)
3
Protocol Description
As mentioned in the introduction, our proposal promotes to describe standard
protocols using Semantic Web technology. We use STS as models of protocols.
More speciﬁcally, we restrict to deterministic STS (i.e. if (s, l, s′) ∈ T and
(s, l, s′′) ∈ T then s′ = s′′). In order to represent protocols using OWL-DL,
we have deﬁned ﬁve diﬀerent classes: Protocol, State, Transition, Fluent
and Commitment, which respectively represent protocols, states, transitions in
protocols, ﬂuents and commitments associated to states.
We model those class descriptions with the following guidelines: A state has
ﬂuents that hold in that point and transitions that go out of it. A transition
is labelled by the communication act that is sent and is associated to the state
that is reached with that transition. A ﬂuent has a time stamp that signals
the moment it was initiated. An actual conversation following a protocol is an
individual of the class Protocol. Following are some of the ontology axioms:
Protocol ≡ ∃hasInitialState.State ⊓
∀hasInitialState.State
State ≡ ∀hasTransition.Transition ⊓
∃hasFluent.Fluent ⊓
∀hasFluent.Fluent
Transition ≡ =1 hasCommAct.CommunicationAct ⊓
=1.hasNextState.State
FinalState ⊑ State ⊓
∀hasFluent.(Fluent ⊓ ¬Commitment)
Fluent ⊑
=1 atTime
Commitment ⊑ Fluent ⊓ =1 hasDebtor.Actor ⊓
=1 hasCreditor.Actor ⊓
=1 hasCondition.Fluent
ConditionalCommitment ⊑ Fluent ⊓ =1 hasDebtor.Actor ⊓
=1 hasCreditor.Actor ⊓
=1 hasCondition.Fluent ⊓
=1 isConditionedTo.Fluent
The OWL-DL description of protocols reﬂects their static features and can
be used to discover structural relationships between protocols. For instance, in
Fig. 1 we show a simple protocol where agent A asks for time to agent B. The
protocol description appears in the following:
Fig. 1. Protocol AskTime.
Asktime ≡ Protocol ⊓ ∃hasInitialState.S0
S0 ≡ State ⊓ ∃hasTransition.T01 ⊓ ∃hasFluent.F0
S1 ≡ State ⊓ ∃hasTransition.T12 ⊓ ∃hasFluent.F1
S2 ≡ State ⊓ ∃hasTransition.T23 ⊓ ∃hasFluent.F2
S3 ≡ FinalState ⊓ ∃hasFluent.F3
T01 ≡ Transition ⊓ ∃hasCommAct.TimeRequest ⊓ ∃hasNextState.S1
T12 ≡ Transition ⊓ ∃hasCommAct.TimeAccept ⊓ ∃hasNextState.S2
T23 ≡ Transition ⊓ ∃hasCommAct.TimeInform ⊓ ∃hasNextState.S3
TimeRequest ≡ Request ⊓ =1 hasContent.TimeReq
TimeAccept ≡ Accept ⊓ =1 hasContent.TimeReq
TimeInform ≡ Responsive ⊓ =1 hasContent.TimeInfo ⊓ =1 inReplyTo.TimeRequest
However, dealing only with structural relationships is too rigid if a ﬂexible
interoperation among agents that use diﬀerent standard protocols is promoted.
For that reason, we propose to consider what we call protocol traces.
Deﬁnition 4. A protocol trace is a sequence of time stamped ﬂuents sorted in
increasing order of time stamp.
Notice that protocol traces are deﬁned in terms of the semantics of communica-
tion acts, not in terms of the communication acts themselves; in contrast with
many other related works (see section 4) that consider messages as atomic acts
without considering their content, neither their semantics.
During a simulation of a protocol run we apply the SWRL rules that encode
the semantics of the communication acts (see section 2.2) appearing in the run.
Then, we can consider the sorted set of time stamped ﬂuents that hold at a
ﬁnal state of the protocol. That set represents the eﬀects of the protocol run.
Following we show an example of the application of the rules to a run of protocol
AskTime in Fig. 1.
In Fig. 2 we show which are the ﬂuents associated to the states of the protocol
and how they vary as a consequence of the communication acts that are sent and
the rules described in section 2.2. We depart from a situation where the set of ﬂu-
Fig. 2. Protocol ﬂuents
ents is empty (F0). When the TimeRequest message is sent, due to the predicate
Initiates(Request(s, r, P), CC(r, s, accept(r,s,P), P), t) the conditional commit-
ment CC1 is initiated, which states that if agent B accepts to give information
about the time, then it will be committed to do so; t1 is the time stamp asso-
ciated. By convention we sort time stamps by their subindexes, that is: ti < tj
if i < j. Then agent B agrees to respond by sending the TimeAccept message,
and due to the predicate Initiates(Accept(s,r,P), accept(s,r,P), t), the ﬂuent ac-
cept(B, A, TimeReq) is initiated at time t2. At this point, Rule 2 (see section 2.2)
can be applied, so CC1 is terminated and the base commitment C1 is initiated at
time t3. Finally, agent B sends the TimeInform message, and because of the pred-
icates Initiates(Responsive(s,r,P, RA), P, t) and Terminates(Responsive(s,r,P,
RA), C(s,r,RA), t), C1 is terminated and a new ﬂuent, TimeInfo, is initiated at
time t4. So, at this point we can say that the ﬂuents that hold at the ﬁnal state
of the protocol are (accept(B, A, TimeReq), t2) and (TimeInfo, t4).
Then, we say that the protocol trace [(accept(B, A, TimeReq), t2), (TimeInfo,
t4)] is generated by the protocol. We denote T (A) to the set of all protocol traces
generated by a protocol A.
Now, we proceed with the deﬁnitions of relationships between protocols we
are considering. Our relationships are not structure-based but eﬀect-based. In-
tuitively, two protocols are equivalent if the same eﬀects take place in the same
relative order. Runs of a protocol are made up of communication acts, and ﬂuents
are the eﬀects they leave.
Deﬁnition 5. Protocol A is equivalent to protocol B if T (A) = T (B).
Sometimes, a protocol is deﬁned by restrictions on the allowable communication
acts at some states of a more general protocol. In those situations the application
of those restrictions is reﬂected in the corresponding eﬀects.
Deﬁnition 6. Protocol A is a restriction of protocol B if T (A) ⊂ T (B).
Protocols for speciﬁc Information Systems may use specialized communication
acts. Specialization can also be applied also to domain actions that can be rep-
resented by specialized ﬂuents.
Deﬁnition 7. A protocol trace t is a specialization of a protocol trace s, written
t≪s, if ∀i. t(i) ⊑ s(i) in an ontology of ﬂuents.
Deﬁnition 8. Protocol A is a specialized-equivalent of protocol B if ∀t ∈ T (A).
∃s ∈ T (B). t≪s and ∀s ∈ T (B). ∃t ∈ T (A). t≪s.
Deﬁnition 9. Protocol A is a specialized-restriction of protocol B if ∀t ∈ T (A).
∃s ∈ T (B). t≪s.
Notice that all those relationships can be easily discovered by straightfor-
ward algorithms supported by OWL-DL reasoners. Those reasoners deal with
the ontology descriptions and rule engines that consider our semantic rules for
generating protocol traces.
Moreover, sometimes we may be interested in comparing protocol traces inde-
pendently of time stamps. That is, we may be interested in knowing if a protocol
produces the same ﬂuents as another, in whatever order. For example, in Fig. 3
we show two protocols that can be used to ask for information related to the
vital signs temperature and pulse. In fact, for that purpose it is irrelevant the
order in which the two requests are done.
Fig. 3. Specialization of protocols.
In protocol P1 we can ﬁnd a general protocol, in which agent A makes a re-
quest about the temperature using the communication act RequestTemp for that
purpose. Then, agent B accepts and replies with a TempInform message, which
is used to give information about the temperature. Once agent A receives this
information, it asks agent B information about the pulse using a RequestPulse.
Finally agent B accepts and replies with a PulseInform message and the ﬁnal
state F is reached. On the other hand, in protocol P2 we can ﬁnd the spe-
ciﬁc protocol used by the agents of a speciﬁc system, called Aingeru2, to ex-
change information about vital signs. Protocol P2 may be a specialization of
an standard protocol. First, agent A asks for the pulse, using the communica-
tion act A-RequestPulse. Then, agent B accepts and responds to the request
using the A-PulseInform message. Next, agent A sends a A-RequestTemp mes-
sage to ask about the temperature. Finally, agent B accepts and replies using
the A-TempInform message and reaches state F’. Following we show the OWL
speciﬁcation for the communication acts used in this example.
RequestTemp ≡ Request ⊓ =1 hasContent.TempReq
AcceptTemp ≡ Accept ⊓ =1 hasContent.TempReq
TempInform ≡ Responsive ⊓ =1 hasContent.TempInfo ⊓ =1 inReplyTo.RequestTemp
RequestPulse ≡ Request ⊓ =1 hasContent.PulseReq
AcceptPulse ≡ Accept ⊓ =1 hasContent.PulseReq
PulseInform ≡ Responsive ⊓ =1 hasContent.PulseInfo ⊓ =1 inReplyTo.RequestPulse
A-RequestTemp ≡ RequestTemp ⊓ =1 theSystem.Aingeru ⊓ =1 hasContent.A-TempReq
A-AcceptTemp ≡ AcceptTemp ⊓ =1 theSystem.Aingeru ⊓ =1 hasContent.A-TempReq
A-TempInform ≡ TempInform ⊓ =1 theSystem.Aingeru ⊓ =1 hasContent.A-TempInfo ⊓
=1 inReplyTo.A-RequestTemp
A-RequestPulse ≡ RequestPulse ⊓ =1 theSystem.Aingeru ⊓ =1 hasContent.A-PulseReq
A-AcceptPulse ≡ AcceptPulse ⊓ =1 theSystem.Aingeru ⊓ =1 hasContent.A-PulseReq
A-PulseInform ≡ PulseInform ⊓ =1 theSystem.Aingeru ⊓ =1 hasContent.A-PulseInfo ⊓
=1 inReplyTo.A-RequestPulse
Notice that every communication act in protocol P2 is a subclass of its coun-
terpart in protocol P1 (i.e. A-RequestPulse ⊑ RequestPulse, etc.) and corre-
spondingly A-PulseInfo ⊑ PulseInfo, etc., is also satisﬁed.
Through a reasoning procedure analogous to that explained with the example
of the AskTime protocol, we get the following sets of protocol traces:
T (P1) = {[(accept(B, A, TempReq), t2), (TempInfo, t4), (accept(B, A,
PulseReq), t6), (PulseInfo, t8)] }
T (P2) = {[(accept(B, A, A-PulseReq), t2), (A-PulseInfo, t4), (accept(B, A,
A-TempReq), t6), (A-TempInfo, t8)] }
Even if the structure of the protocols is not exactly the same, we can relate
both protocols by a shallow notion of specialization from the following point
of view. If we get abstracted from time stamps, we can see protocol traces as
multi-sets. Let us denote abstract-time(t) to the multi-set formed by the ﬂuents
appearing in the protocol trace t, without any time stamp associated. Now, we
deﬁne
S(A) = {abstract-time(t)|t ∈ T (A)}
Then, we are in condition to deﬁne analogous relationships to the previous
ﬁve, but in a shallow mood.
Deﬁnition 10.
1. Protocol A is shallow-equivalent to protocol B if S(A) =
S(B).
2 The A- preﬁx intends to label the AINGERU terminology
2. Protocol A is a shallow-restriction of protocol B if S(A) ⊂ S(B).
3. A protocol trace t is a shallow-specialization of a protocol trace s, written
t ≪s s, if there is a map φ from abstract-time(t) to abstract-time(s) such
that ∀f ∈ abstract-time(t).f ⊑ φ(f) in an ontology of ﬂuents.
4. Protocol A is a shallow-specialized-equivalent of protocol B if ∀t ∈ S(A). ∃s ∈
S(B). t ≪s s and ∀s ∈ S(B). ∃t ∈ S(A). t ≪s s.
5. Protocol A is a shallow-specialized-restriction of protocol B if ∀t ∈ S(A). ∃s ∈
S(B). t ≪s s.
Finally, using our proposal, we can conclude that protocols P1 and P2 are
shallow-specialized-equivalent, although they use diﬀerent communications acts
and have diﬀerent structure.
4
Related Works
Among the diﬀerent related works that we can ﬁnd in the specialized literature,
the closer work is [11], where protocols are represented as transition systems
and subsumption and equivalence of protocols are deﬁned with respect to three
state similarity funtions. We share some goals with that work, but the protocol
description formalism used by them is not considered in the paper and there is no
references to how protocol relationships are computed. In contrast, we describe
protocols with a description logic language and protocol relationships can be
computed by straightforward algorithms. It is worth mentioning that protocol
relationships considered in that paper deserve study in our framework.
The works of [12] and [13] are quite similar one to each other. Both capture
the semantics of communication acts through agents’ commitments and represent
communication protocols using a set of rules that operate on these commitments.
Moreover those rule sets can be compiled as ﬁnite state machines. Nevertheless,
they do not consider the study of relationships between protocols. In addition, in
[14], protocols are also represented with a set of rules with terms obtained from
an ontology, but their main goal is protocol development and, in order to reason
about protocol composition, they formalize protocols into the π-calculus. Then,
equivalence through bisimulation is the only process relationship considered. In
[15], they also consider commitment protocols; however, their main focus is on
combining them with considerations of rationality on the enactment of protocols.
Our proposal could be complemented with their approach.
An alternative way to describe ﬁnite state machines with a description logic
language is to take advantage of the relationship of that logic with Deterministic
Propositional Dynamic Logic, see [16] for an example in the context of Web Ser-
vices composition. The approach of that paper is very diﬀerent in purpose from
ours. Their states and transitions descriptions are not prepared to be confronted
in a comparison. In constrast, our state and transition descriptions are care-
fully modelled as class descriptions such that semantics relationships between
protocols can be captured.
Also in the context of Web Services, state transition systems are used in [17]
for representing dynamic behaviour of services and they deﬁne some notions of
compatibility and substitutability of services that can be easily translated to the
context of compatibility of protocols. Relationships between their compatibility
relations and our deﬁned relationships deserve study.
In [18] protocols are deﬁned as a set of permissions and obligations of agents
participating in the communication. They use an OWL ontology for deﬁning the
terms of the speciﬁcation language, but their basic reasoning is made with an
ad hoc reasoning engine. We share their main goal of deﬁning protocols in a
general framework that allows reutilization. Nevertheless, they do not consider
relationships between protocols.
The problem of determining if an agent’s policy is conformant to a protocol
is a very important one, but we are not treating that topic in this paper. Never-
theless, the topic is close to ours and it is worth mentioning the following papers
that consider diﬀerent notions of conformance: In [19], deterministic ﬁnite state
machines are the abstract models for protocols, which are described by simple
logic-based programs. Three levels of conformance are deﬁned: weak, exhaustive
and robust. They consider communication acts as atomic actions, in contrast
to our semantic view. In [20]a nondeterministic ﬁnite state automata is used to
support a notion of conformance that guarantees interoperabiliy among agents
conformant to a protocol. Their conformance notion considers the branching
structure of policies and protocols and applies a simulation-based test. Commu-
nication acts are considered atomic actions, without considering their semantics.
In [21], communication acts semantics is described in terms of commitments but
it is not used for the conformance notion. A third diﬀerent notion of conformance
is deﬁned and, moreover, it is proved orthogonal to their proposed notions of cov-
erage and interoperability.
Finally, [22] and [23] use ﬁnite state machines and Petri nets, respectively,
but without taking into account the meaning of the communication acts inter-
changed, neither considering relationships between protocols.
5
Conclusions
Increasing machine-processable tasks in the Web is a challenge considered at
present. In this line we have presented in this paper a proposal that favours the
communication among agents that represent to diﬀerent Information Systems
accessible through the Web. The main contributions of the proposal are:
– The management of the semantics aspects when dealing with agent commu-
nication protocols.
– The provision of the possibility of customizing standard communication pro-
tocols and management of them.
– The use of standard Semantic Web tools to describe protocols.
– The support for discovering diﬀerent kinds of relationships between proto-
cols.
References
1. FIPA:
FIPA
communicative
act
library
speciﬁcation
(July
2005)
http://www.ﬁpa.org/specs/ﬁpa00037/SC00037J.html.
2. Greenwood, D., M.Lyell, A. Mallya, H.S.: The IEEE FIPA approach to integrating
software agents and web services. In: International Conference on Autonomous
Agents and Multiagent Systems AAMAS, Hawaii USA (2007) 14–18
3. Berm´udez, J., Go˜ni, A., Illarramendi, A., Bag¨u´es, M.I.:
Interoperation among
agent-based information systems through a communication acts ontology.
Inf.
Syst. 32(8) (2007) 1121–1144
4. Singh, M.P.: Agent Communication Languages: Rethinking the Principles. IEEE
Computer 31(12) (December 1998) 40–47
5. Singh, M.P.: A social semantics for agent communication languages. In: Issues in
Agent Communication, Springer-Verlag (2000) 31–45
6. Austin, J.L., ed.: How to do things with words. Oxford University Press (1962)
7. Wooldridge, M.: Semantic Issues in the Veriﬁcation of Agent Comunication Lan-
guages. Journal of Autonomous Agents and Multi-Agent Systems 3(1) (February
2000) 9–31
8. Venkatraman, M., Singh, M.P.: Verifying compliance with commitment protocols.
Autonomous Agents and Multi-Agent Systems 2(3) (1999) 217–236
9. Fornara, N., Colombetti, M.:
Operational speciﬁcation of a commitment-based
agent communication language. In: AAMAS ’02: Proceedings of the ﬁrst interna-
tional joint conference on Autonomous agents and multiagent systems, New York,
NY, USA, ACM Press (2002) 536–542
10. Shanahan, M.: The event calculus explained. Lecture Notes in Computer Science
1600 (1999) 409–430
11. Mallya, A.U., Singh, M.P.: An algebra for commitment protocols. Autonomous
Agents and Multi-Agent Systems 14(2) (2007) 143–163
12. Yolum, P., Singh, M.P.: Flexible protocol speciﬁcation and execution: Applying
event calculus planning using commitments. In: Proceedings of the 1st Interna-
tional Joint Conference on Autonomous Agents and MultiAgent Systems (AA-
MAS), ACM Press (July 2002) 527–534
13. Fornara, N., Colombetti, M.: Deﬁning interaction protocols using a commitment-
based agent communication language. In: AAMAS ’03: Proceedings of the second
international joint conference on Autonomous agents and multiagent systems, New
York, NY, USA, ACM Press (2003) 520–527
14. Desai, N., Mallya, A.U., Chopra, A.K., Singh, M.P.: Interaction protocols as design
abstractions for business processes. IEEE Trans. Softw. Eng. 31(12) (2005) 1015–
1027
15. Yolum, P., Singh, M.: Enacting protocols by commitment concession. In: Inter-
national Conference on Autonomous Agents and Multiagent Systems AAMAS,
Hawaii USA (2007) 116–123
16. Berardi, D., Calvanese, D., Giacomo, G.D., Lenzerini, M., Mecella, M.: Automatic
service composition based on behavioral descriptions. Int. J. Cooperative Inf. Syst.
14(4) (2005) 333–376
17. Bordeaux, L., Sala¨un, G., Berardi, D., Mecella, M.: When are two web services
compatible? In: TES. (2004) 15–28
18. Kagal, L., Finin, T.: Modeling conversation policies using permissions and obliga-
tions. Autonomous Agents and Multi-Agent Systems 14(2) (2007) 187–206
19. Endriss, U., Maudet, N., Sadri, F., Toni, F.: Logic-based agent communication
protocols. In: Workshop on Agent Communication Languages. (2003) 91–107
20. Baldoni, M., Baroglio, C., Martelli, A., Patti, V.: A priori conformance veriﬁcation
for guaranteeing interoperability in open environments. In: ICSOC. (2006) 339–351
21. Chopra, A.K., Singh, M.P.: Producing compliant interactions: Conformance, cov-
erage, and interoperability. In: DALT. (2006) 1–15
22. d’Inverno, M., Kinny, D., Luck, M.: Interaction protocols in agentis. In: In Proceed-
ings of the Third International Conference on Multi-Agent Systems (ICMAS98).
(1998) 261–268
23. Mazouzi, H., Seghrouchni, A.E.F., Haddad, S.: Open protocol design for complex
interactions in multi-agent systems. In: AAMAS ’02: Proceedings of the ﬁrst in-
ternational joint conference on Autonomous agents and multiagent systems, New
York, NY, USA, ACM Press (2002) 517–526
"
"Real-world autonomous agents have seen increased adoption in environments such as homes, offices, and public spaces, yet natural human-robot interaction remains a challenge. By combining the capabilities of large language models (LLMs) and multimodal vision-language models (VLMs), we introduce an approach that allows natural language interactions between humans and autonomous robots. We utilize the LLMs to decode high-level natural language instructions, translating them into precise robot actions. The VLMs provide visual and semantic understanding of the robot's environment. Our results, with a command recognition accuracy of 99.13% and a command execution success rate of 97.96%, demonstrate the effectiveness of our approach in real-world applications.","Human-robot interaction (HRI) has been an active area of research with the goal of integrating autonomous robots into real-world scenarios. Traditional approaches for controlling robots have involved complex teleoperation controllers, teach pendant interfaces, or rigid command protocols. As tasks become more intricate and environments grow more unpredictable, there is a need for more natural and intuitive interaction mechanisms. Prior works have utilized techniques like reinforcement and imitation learning, but these methods can be computationally expensive and require specialized training or fine-tuning. To overcome these challenges, we investigate the use of large language models (LLMs) and multimodal vision-language models (VLMs) to enable natural conversation-based interaction between humans and autonomous robots.","Recent advancements in natural language processing (NLP), notably large language models (LLMs) and multimodal vision-language models (VLMs), have opened up new avenues for human-robot interaction. These models can understand and generate human-like text and visual information. Researchers have successfully incorporated LLMs and VLMs into robotic systems, allowing LLMs to interpret and execute complex commands. However, these approaches often rely on the LLM's ability to plan and execute the robot's actions, which can introduce inconsistencies and randomness. Our approach differs by using LLMs as a linguistic decoder, while utilizing a classical robot operating system (ROS) navigation planner to handle the physical actions of the robot.nannannan","We present a framework that leverages the capabilities of LLMs and VLMs to enable natural language interactions between humans and real-world autonomous robots. Our framework consists of three main components: (a) integration of LLMs and VLMs, (b) development of the robot execution mechanism (REM) node, and (c) development of the chat graphical user interface (ChatGUI). The LLMNode component decodes natural language inputs and maps them to robot actions or information requests. The CLIPNode component provides visual and semantic understanding of the robot's environment. The REM node translates high-level instructions into executable commands for the robot, handling navigation goals and custom movement commands. The ChatGUI serves as the user's primary interaction point with the robot, facilitating natural language conversations. We demonstrate the effectiveness of our framework through real-world and simulated experiments.nan","To evaluate the performance of our framework, we conducted real-world and simulated experiments. We assessed command recognition accuracy (CRA), object identification accuracy (OIA), navigation success rate (NSR), and average response time (ART). Our results showed a high CRA of 99.13%, indicating accurate interpretation of natural language commands. OIA was 55.20%, highlighting the need for further improvement in CLIPNode integration. The NSR of 97.96% demonstrated the effectiveness of the REM node in navigating the robot to designated locations. The ART of approximately 0.45 seconds indicated a quick response time from the framework. Additionally, participants' feedback revealed that 80.9% and 76.2% rated the ease of interaction and intuitiveness of the framework favourably, respectively, indicating positive user experience.","We introduced a framework that utilizes the capabilities of large language models (LLMs) and multimodal vision-language models (VLMs) to enable natural language interactions between humans and autonomous robots. Our real-world experiments demonstrated high command recognition accuracy and effective task execution, highlighting the potential of our approach for simplifying human-robot interaction. Future work will focus on improving the framework across several dimensions, including refining the CLIPNode for broader object recognition and fine-tuning the LLMNode with domain-specific data. We aim to create a more intuitive and adaptive chat interface, enhancing the user experience and facilitating seamless communication between humans and robots.",The Conversation is the Command: Interacting with Real-World Autonomous Robot Through Natural Language,"Linus Nwankwo, Elmar Rueckert","The Conversation is the Command: Interacting with Real-World
Autonomous Robot Through Natural Language
Linus Nwankwo
linus.nwankwo@unileoben.ac.at
Chair of Cyber-Physical Systems, Montanuniversität
Leoben, Austria
Elmar Rueckert
Chair of Cyber-Physical Systems, Montanuniversität
Leoben, Austria
ABSTRACT
In recent years, autonomous agents have surged in real-world en-
vironments such as our homes, offices, and public spaces. How-
ever, natural human-robot interaction remains a key challenge. In
this paper, we introduce an approach that synergistically exploits
the capabilities of large language models (LLMs) and multimodal
vision-language models (VLMs) to enable humans to interact nat-
urally with autonomous robots through conversational dialogue.
We leveraged the LLMs to decode the high-level natural language
instructions from humans and abstract them into precise robot
actionable commands or queries. Further, we utilised the VLMs to
provide a visual and semantic understanding of the robot’s task
environment. Our results with 99.13% command recognition ac-
curacy and 97.96% commands execution success show that our
approach can enhance human-robot interaction in real-world ap-
plications. The video demonstrations of this paper can be found at
https://osf.io/wzyf6 and the code is available at our repository1.
CCS CONCEPTS
• Human-centered computing → Human computer interac-
tion (HCI); Interaction paradigms; Natural language inter-
faces;
KEYWORDS
Human-robot interaction, LLMs, VLMs, ChatGPT, ROS, autonomous
robots, natural language interaction.
ACM Reference Format:
Linus Nwankwo and Elmar Rueckert. 2024. The Conversation is the Com-
mand: Interacting with Real-World Autonomous Robot Through Natural
Language. In Companion of the 2024 ACM/IEEE International Conference
on Human-Robot Interaction (HRI ’24 Companion), March 11–14, 2024, Boul-
der, CO, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/
3610978.3640723
1
INTRODUCTION
The exploration of human-robot interaction (HRI) [29], [32] and
its advancement into real-world applications has been a topic of
significant research over the past decades [30]. Current approaches
for controlling and interacting with autonomous robots in the real
1https://github.com/LinusNEP/TCC_IRoNL.git
This work is licensed under a Creative Commons Attribution
International 4.0 License.
HRI ’24 Companion, March 11–14, 2024, Boulder, CO, USA
© 2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0323-2/24/03.
https://doi.org/10.1145/3610978.3640723
world have been dominated by complex teleoperation controllers
[13], teach pendants [2], and rigid command protocols [16], where
the robots execute predefined tasks based on specialized program-
ming languages. As the challenges we present to these robots be-
come more intricate and the environments they operate in grow
more unpredictable [18], there arises an unmistakable need for
more natural and intuitive interaction mechanisms.
Prior works have seen a tilt towards techniques like reinforce-
ment [21] and imitation learning [2]. By leveraging iterative learn-
ing and human demonstrations, these strategies have shown a
capacity for fostering nuanced robot behaviours, as demonstrated
in [28]. However, the often computational burdens [15], and the
high costs associated with reward specification [25], task-specific
training, or fine-tuning, commonly observed in the reinforcement
and imitation learning frameworks, have limited the practical ap-
plicability of these methods, especially for simpler robotic tasks.
Prompted by these challenges, we turned our focus to the re-
cent advancement in large language models (LLMs) [23], [6] and
multi-modal vision-language models (VLMs) [22], [24] to foster
an intuitive human-robot collaboration. This paper introduces an
innovative approach that exploits the inherent natural language
capabilities of pre-trained LLMs and VLMs to enable humans to in-
teract with autonomous robots through natural language dialogues.
As demonstrated in Figure 1, we aim to realize a new approach
to human-robot interactions—one where the conversation is the
command (refer to Sections 3 & 4 for more details).
Our contributions are therefore threefold: (a) we introduced a
framework that can leverage independent pre-trained LLMs (e.g.,
OpenAI GPT-2 [23] & GPT-3 [6], Google BERT [8], Meta AI LLaMA
[31], etc), and VLMs (e.g., CLIP[22]) to enable real-world autonomous
robots to interact with humans or other entities using natural lan-
guage dialogues. (b) we performed real-world experiments with
our developed framework to ensure that the robot’s actions are
always aligned with the user’s instructions, thereby reducing the
likelihood of erroneous operations. (c) we have made our code and
associated resources available to the public. This allows for easy
reproducibility of our results.
2
RELATED WORK
The recent rise of natural language processing (NLP) [33], marked
by large language models (LLMs) like OpenAI GPT-3 [6], Google
BERT [8], HuggingFace distilBERT [26], EleutherAI GPTNeoX [3],
Meta AI LLaMA [31], Facebook RoBERTa [14], multi-modal vision-
language models (VLMs) e.g., CLIP [22], DALL-E [24], and their
successors, has opened new avenues for human-robot interaction.
The inherent capacity of these models to understand and generate
arXiv:2401.11838v1  [cs.RO]  22 Jan 2024
HRI ’24 Companion, March 11–14, 2024, Boulder, CO, USA
Linus Nwankwo and Elmar Rueckert
Figure 1: Example demonstration of our framework. We demonstrated these results in the real world as shown in the summary
video at https://osf.io/wzyf6. In (a), our framework decodes the high-level instructions such as “move in a circular pattern”,
“move forward, go right, etc.” from humans, and abstracts them to the robot’s physical actions. In (b), we leveraged our framework
for the robot’s task environment understanding, information requests, and goal navigation.
human-like text as well as visual observations has led to several in-
teresting applications [4], [5]. Recent works such as [1], [5], [10] and
[12] have successfully incorporated LLMs and VLMs into robotic
systems, allowing the LLMs to interpret and execute complex com-
mands. Similarly, Wangchunshu et al.[35], Kaiwen Zhou et al. [34],
and Miguel et al. [9] in their work demonstrated how LLMs could
be used to facilitate real-time feedback, zero-shot object navigation
and cognitive learning in autonomous robots.
While these works are exceptional, they have focused solely on
a step-by-step task description [12], and rely completely on the
LLM’s ability to plan the robot’s actions and act. In most complex
real-world scenarios, especially as LLMs can sometimes hallucinate
[7] or generate inconsistent data, their approach may introduce
inconsistencies and randomness in the robot’s actions.
On the contrary, our approach draws inspiration from the work
of Yagi Xie et al. [33]. Instead of relying completely on the LLMs’
ability to plan and execute the robot’s actions, we employ a bidi-
rectional approach, simply using the LLMs as a linguistic decoder
[33], and a classical robot operating system (ROS) [20] navigation
planner to plan the robot’s actions. We provide the LLMs with a
dictionary of task descriptions and action patterns. We then use
the ROS planner2 to plan the actual physical actions of the robot
(e.g., path planning, localisation, obstacle avoidance, mapping, etc.),
as shown in Figure 1b.
3
METHODS
An overview of our framework’s architecture is shown in Figure 2.
One of our core objectives is to develop a framework that enables
real-world autonomous robots to interact with humans or other
entities using natural language dialogues. To achieve this objective,
we decompose the task into three subtasks: (a) the integration of
LLMs and VLMs, (b) the development of the robot execution mech-
anism (REM) node, and (c) the development of the chat graphical
2https://github.com/ros-planning/navigation
Figure 2: Overview of our framework’s architecture. The
LLMNode decodes the natural language conversations. The
CLIPNode provides a visual and semantic understanding of
the robot’s task environment. The REM node abstracts the
high-level understanding from the LLMNode to actual robot
actions. The ChatGUI serves as the user’s primary interaction
point. See Subsections 3.1, 3.2, and 3.3 for more details.
user interface (ChatGUI). An overview is shown in Figure 2. This
section provides details of each of the above three subtasks.
3.1
Integration of LLMs and VLMs
To decode the natural language conversation and abstract them
to the robot’s actions, we developed a ROS node, LLMNode (light
green block in Figure 2) to establish communication between the
pre-trained LLMs and the rest interfaces within ROS ecosystem
[20]. The LLMNode subscribes to topics that provide essential data,
e.g., odometry for spatial sensing and outputs from the CLIPNode
(light purple block in Figure 2) for visual observation and object
recognition. We used the LLMNode to handle incoming natural
language inputs from the ChatGUI (Subsection 3.3) by first passing
The Conversation is the Command: Interacting with Real-World Autonomous Robot Through Natural Language
HRI ’24 Companion, March 11–14, 2024, Boulder, CO, USA
them through the pre-trained LLM [23]. The output is then mapped
to the robot’s actionable commands or queries.
In the mapping process, we leverage pattern matching to align
the generated text with predefined actions or information requests.
For example, navigation commands are translated into goals for
the robot to pursue within its environment, while queries Q about
the robot’s status or surroundings are addressed with information
derived from the robot’s sensor data. The LLMNode also oversees
the execution and feedback process. We added this function to
provide real-time feedback through the publishing of messages,
which not only inform the user but also log the interaction data for
subsequent analysis (see Subsection 4.1 for more details).
Summarily, the LLMNode function can be described as a mapping
from natural language inputs to robot actions, i.e., LLMNode : L ↦→
A where L represents the space of natural language inputs and
A denotes the set of possible robot actions. This mapping is a
composition of several functions, as depicted in Eq. 1.
LLMNode(𝑙) = REM(LM(𝑙), Sen(Data)), 𝑙 ∈ L
(1)
From Eq. 1, LM(𝑙) is the language model’s interpretation of the
input 𝑙𝑖 ∈ L and Sen(Data) represents the sensor data that informs
the context of the command. The REM node (Section 3.2) then
translates this into an executable command for the robot.
Furthermore, to provide a visual and semantic understanding
of the task environment (e.g., Figure 1b), we used the OpenAI con-
trastive language image pretraining (CLIP) model [22]. CLIP model
consists of language and image encoders trained on a staggering
400 million image-text pairs [27]. Thus, we used it to encode the
stream of RGB images from our observation source (Intel Realsense
D435i) alongside the textual descriptions of objects in the image.
Formally, given an image I𝑡 at time 𝑡, we consider a set of pre-
defined textual descriptions D = {𝑑1,𝑑2,...,𝑑𝑛}. Each description
𝑑𝑖 ∈ D is mapped to a tokenized representation, forming a set T =
{𝑡1,𝑡2,...,𝑡𝑛}. This set encompasses human-readable labels for com-
mon office objects such as “table”, “chair”, “person”, and so on. Using
the CLIP model [22], we extract the feature vector of the image,
i.e., 𝑓I = CLIP_encode_image(I𝑡). For each tokenized description
𝑡𝑖 ∈ T, its feature vector is obtained as 𝑓T𝑖 = CLIP_encode_text(𝑡𝑖).
Subsequently, for the image feature and every text feature, we com-
pute the similarity scores using S𝑖 = 𝑓I · 𝑓 T
T𝑖 . Thus, we determine
the recognized object within the image by selecting the textual
description that yields the highest similarity score, as depicted in
Eq. 2.
Recognized Object = 𝑡𝑘; 𝑘 = arg
𝑛
max
𝑖=1 S𝑖
(2)
Additionally, our model uses the bounding boxes from YOLO
V8 [11] to determine regions of interest (ROI) within the image.
Notably, the centres of these bounding boxes are employed as the
spatial coordinates for the recognized objects, capturing both the
identity and the location of the objects in the scene.
We embodied the entire process within a ROS node, CLIPNode
(light purple block in Figure 2). The output from Eq. 2, representing
the recognized objects along with their respective spatial coordi-
nates, is published as a ROS [20] topic. These are subsequently
subscribed to by the LLMNode to handle the natural language com-
mands, generate responses, and decide on actions for the robot to
take. For instance, based on the prompt used by the robot’s user,
it can direct the robot to navigate to a detected object or provide
information about detected objects and their positions.
3.2
Robot Execution Mechanism (REM)
To abstract the high-level language understanding and environment
sensing from the LLMNode to actual robot actions, we developed
the robot execution mechanism (REM) node. This node translates
intents extracted from the LLMNode into actionable tasks for phys-
ical execution by the robot. Central to the REM node’s function-
ality is processing navigation goals, G𝑛 (e.g., Figure 1b). When a
textual description of a goal destination G𝑑, such as “navigate to
the Secretary’s office” is provided, the REM node translates this
into precise goal coordinates (𝑥𝑙,𝑦𝑙,𝑧𝑙,𝑤𝑙) within the robot’s op-
erational environment via a mapping process that correlates the
descriptive labels with their corresponding spatial coordinates i.e.,
G𝑑 ↦→ (𝑥𝑙,𝑦𝑙,𝑧𝑙,𝑤𝑙). To navigate the robot to the goal, we used the
MoveBase package of the ROS navigation planner, which provides
an action server for handling navigation goals. REM node sends
the goal to this server and monitors its progress.
Besides navigation goals, the REM node also handles custom
movement commands C (e.g., Figure 1a) which are not tied to
specific goal locations, but rather to particular motion patterns,
such as “rotate in place”, “move forward” etc. We encoded these
patterns into the robot’s YAML configuration files, allowing for a
flexible command set 𝑐𝑖 ∈ C that can be expanded or modified as
required. We use the REM node to translate the commands into
Twist messages W with linear (𝑣𝑥,𝑣𝑦,𝑣𝑧) and angular (𝜔𝑥,𝜔𝑦,𝜔𝑧)
velocity components as W(𝑐) =𝑇𝑤𝑖𝑠𝑡(𝑣𝑥,𝑣𝑦,𝑣𝑧,𝜔𝑥,𝜔𝑦,𝜔𝑧).
In addition to handling movement, we integrated a security
measure to halt the robot when an unrecognized command (e.g.,
the last command in Figure 1a) is received, issuing zero velocities
to stop all motion, ensuring safe operation.
REM𝑙 =


G𝑛(𝑙),
if 𝑙 ∈ G𝑑
W(𝑐),
if 𝑙 ∈ C
Sen(Data),
if 𝑙 ∈ Q
𝑆𝑡𝑜𝑝(),
if 𝑙 ∈ stop or unknown command
(3)
Formally, as summarised in Eq. 3, the REM node abstracts the com-
plexity of the robot navigation and command execution, translating
the high-level instructions into physical actions.
3.3
Chat Interface Development
To provide an intuitive conversational platform that would facili-
tate natural language interaction between the robot and its human
users, we developed a simple and user-friendly chat graphical user
interface (ChatGUI) which serves as the user’s primary interaction
point with the robot through textual communication. We designed
the ChatGUI using Tkinter libraries3 and integrated it within ROS
[20] for message passing. We employed the standard ROS pub-
lish/subscribe communication mechanisms for the ChatGUI devel-
opment, specifically, a bidirectional message exchange approach,
i.e., ChatGUI : UserInputs ↔ LLMNodeOutputs. User natural lan-
guage inputs are published to the LLMNode, and the responses are
subscribed to and displayed to the user on the ChatGUI interface.
3https://docs.python.org/3/library/tkinter.html
HRI ’24 Companion, March 11–14, 2024, Boulder, CO, USA
Linus Nwankwo and Elmar Rueckert
We developed the ChatGUI with event-driven architecture to
ensure that user actions, such as sending a message or issuing a
command, trigger corresponding updates in the ChatGUI or result in
the publishing of commands to the LLMNode. We encapsulated this
process in a function that translates user actions into corresponding
LLMNode responses.
4
PRELIMINARY RESULTS
We conducted real-world and simulated experiments to demon-
strate the applicability and adaptability of our framework. For simu-
lation, we used the Unitree Go1 Gazebo packages4 and a ROS-based
open-source mobile robot adapted from [17]. We ran all the simula-
tions with a ground station PC with Nvidia Geforce RTX 3060 Ti
GPU, 8GB memory running Ubuntu 20.04, ROS Noetic distribution.
In real-world experiments, we used a Lenovo ThinkBook Intel
Core i7 with Intel iRIS Graphics running Ubuntu 20.04, ROS Noetic
distribution. Unitree Go1 quadruped robot was used. The robot is
equipped with Intel Realsense D435i RGB-D camera and Ouster 3𝐷
LiDAR for both visual and spatial observations of the task envi-
ronment. All the real-world experiments were performed in our
laboratory office (11 rooms) and outside corridor environment, mea-
suring approximately 18×20 𝑚 and 6×120 𝑚 respectively.
We experimented with different pre-trained open-source LLMs
like OpenAI GPT-2 [23], Google BERT [8], HuggingFace distilBERT
[26], EleutherAI GPTNeoX [3], Meta AI LLaMA [31], and Facebook
RoBERTa [14]. OpenAI GPT-3 [6] and GPT-4 [19] are also adaptable
to our framework. However, due to their API access limitations,
we mostly utilised the open-access and free versions of the LLMs
(GPT-2 [23] specifically) in our experiments.
4.1
Initial Evaluation / Participants
In our initial evaluation, we invited 21 participants (mostly students)
with an average age of 23 (±5) and gender distribution, 61.9% male,
28.6% female and 9.5% others to assess the intuitiveness of our
framework by interacting with the robots using natural language.
We instructed the participants to command the robots to navigate
to locations, identify objects, and inquire about their status. We
meticulously logged the interaction data which includes the partic-
ipant’s input text, the LLM’s output, the true label, the LLMNode
predicted label, etc. To quantitatively evaluate the performance of
our framework, we established four key metrics:
(a) Command Recognition Accuracy (CRA): With the CRA,
we assess how accurately the LLMNode interprets the natural lan-
guage commands. This aids us in pinpointing instances where the
predicted label diverged from the true label, providing insights
into potential areas for improvement. (b) Object Identification
Accuracy (OIA): We employed this metric to assess the precision
of the CLIPNode in identifying and localizing objects within the
robot’s task environment. (c) Navigation Success Rate (NSR): We
utilised this metric to determine the effectiveness of the REM node
in successfully navigating the robot to the designated locations. (d)
Average Response Time (ART): We logged in ROS Unix epoch
clock standard, the time a message is sent from the ChatGUI, the
time it is received by the LLMNode, and the time the robot responds.
With the ART, we compute the average duration from receiving
4https://github.com/unitreerobotics/unitree_guide
the user’s chat command to initiating the robot’s movement.
Figure 3 presents our preliminary statistical results obtained from
the interaction data analysis. The top row of Figure 3 shows the
performance metrics and the confusion matrix (for selected labels)
of the LLMNode. The CRA, with a prediction accuracy of 99.13%
(i.e., how often the “Predicted labels” matched the “True labels”),
indicates a high level of accuracy in the command interpretation.
This reflects the robustness of the LLMNode in processing the nat-
ural language inputs. The OIA on the other hand, achieved 55.20%
accuracy, indicating room for improvement in our CLIPNode inte-
gration. Further, the NSR at 97.96%, indicates good performance
in the REM’s ability to abstract the high-level understanding from
the LLMNode to the actual robot’s navigation actions. Also, the
Figure 3: Performance and variability measures illustrating
CRA, OIA, and NSR (top) and the participants’ feedback (bot-
tom) based on the logged interaction data.
overall ART across all the selected commands (refer to the figure at
https://osf.io/ufctx) is approximately 0.45 seconds. This indicates
that, on average, the robot takes less than half a second from re-
ceiving a chat command to initiating movement, which suggests a
relatively quick response time for our framework.
Furthermore, the bottom row of Figure 3 shows the participants’
feedback (refer to the questionnaire at https://osf.io/dgbtr). With
4 and 5 ratings as favourable benchmarks, 80.9% and 76.2% of the
participants respectively rated the ease of interaction and the intu-
itiveness of our framework as favourable, while 85.7% are satisfied
with the response of the robot to their natural language commands.
5
CONCLUSION AND FUTURE WORK
We introduced a framework that leverages the inherent capabilities
of large language models (LLMs) and multimodal vision-language
models (VLMs) to enhance human-robot interaction through natu-
ral conversation. Our evaluation from the logged interaction data
and participants’ feedback was overwhelmingly positive. The high
command recognition accuracy and effective task execution, show
that our framework can simplify human-robot interaction. Looking
ahead, we aim to refine the framework across several dimensions,
not just for ROS-based autonomous robots. The CLIPNode will be
further improved for broader object recognition, and the LLMNode
will be fine-tuned with domain-specific data for better contextual
and voice understanding. User experience will be a priority, with a
focus on creating a more intuitive and adaptive chat interface.
The Conversation is the Command: Interacting with Real-World Autonomous Robot Through Natural Language
HRI ’24 Companion, March 11–14, 2024, Boulder, CO, USA
REFERENCES
[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,
Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-
man, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan,
Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi,
Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine,
Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka
Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton
Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun
Xu, Mengyuan Yan, and Andy Zeng. 2022. Do As I Can, Not As I Say: Grounding
Language in Robotic Affordances. arXiv:2204.01691 [cs.RO]
[2] Baris Akgun, Maya Cakmak, Jae Wook Yoo, and Andrea Lockerd Thomaz. 2012.
Trajectories and Keyframes for Kinesthetic Teaching: A Human-Robot Inter-
action Perspective. In Proceedings of the Seventh Annual ACM/IEEE Interna-
tional Conference on Human-Robot Interaction (Boston, Massachusetts, USA)
(HRI ’12). Association for Computing Machinery, New York, NY, USA, 391–398.
https://doi.org/10.1145/2157689.2157815
[3] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler,
USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben
Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autore-
gressive Language Model. arXiv:2204.06745 [cs.CL]
[4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen,
Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn,
Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan,
Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex
Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal,
Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski,
Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia
Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu
Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan
Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe
Yu, and Brianna Zitkovich. 2023. RT-2: Vision-Language-Action Models Transfer
Web Knowledge to Robotic Control. arXiv:2307.15818 [cs.RO]
[5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis,
Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine
Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J
Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei
Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch,
Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell
Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin
Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran,
Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun
Xu, Tianhe Yu, and Brianna Zitkovich. 2023. RT-1: Robotics Transformer for
Real-World Control at Scale. arXiv:2212.06817 [cs.RO]
[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
arXiv:2005.14165 [cs.CL]
[7] Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge Fan, Dayiheng Liu, Dong-
mei Zhang, Zhixu Li, and Yanghua Xiao. 2023. Hallucination Detection: Robustly
Discerning Reliable Answers in Large Language Models. In Proceedings of the 32nd
ACM International Conference on Information and Knowledge Management (Birm-
ingham, United Kingdom) (CIKM ’23). Association for Computing Machinery,
New York, NY, USA, 245–255. https://doi.org/10.1145/3583780.3614905
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv:1810.04805 [cs.CL]
[9] Miguel Á. González-Santamarta, Francisco J. Rodríguez-Lera, Ángel Manuel
Guerrero-Higueras, and Vicente Matellán-Olivera. 2023. Integration of Large
Language Models within Cognitive Architectures for Autonomous Robots.
arXiv:2309.14945 [cs.RO]
[10] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan-
guage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em-
bodied Agents. arXiv:2201.07207 [cs.LG]
[11] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. 2023. Ultralytics YOLOv8. https:
//github.com/ultralytics/ultralytics
[12] Teyun Kwon, Norman Di Palo, and Edward Johns. 2023. Language Models as
Zero-Shot Trajectory Generators. arXiv:2310.11604 [cs.RO]
[13] Tsung-Chi Lin, Achyuthan Unni Krishnan, and Zhi Li. 2023. Perception-Motion
Coupling in Active Telepresence: Human Behavior and Teleoperation Interface
Design. J. Hum.-Robot Interact. 12, 3, Article 31 (mar 2023), 24 pages.
https:
//doi.org/10.1145/3571599
[14] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]
[15] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker,
Robert Baruch, Travis Armstrong, and Pete Florence. 2022. Interactive Language:
Talking to Robots in Real Time. arXiv:2210.06407 [cs.RO]
[16] Debasmita Mukherjee, Kashish Gupta, and Homayoun Najjaran. 2022. A Critical
Analysis of Industrial Human-Robot Communication and Its Quest for Natural-
ness Through the Lens of Complexity Theory. Frontiers in Robotics and AI 9
(2022). https://doi.org/10.3389/frobt.2022.870477
[17] Linus Nwankwo, Clemens Fritze, Konrad Bartsch, and Elmar Rueckert. 2023.
ROMR: A ROS-based open-source mobile robot. HardwareX 14 (2023), e00426.
https://doi.org/10.1016/j.ohx.2023.e00426
[18] Linus Nwankwo and Elmar Rueckert. 2023. Understanding Why SLAM Algo-
rithms Fail in Modern Indoor Environments. In Advances in Service and Industrial
Robotics, Tadej Petrič, Aleš Ude, and Leon Žlajpah (Eds.). Springer Nature Switzer-
land, Cham, 186–194.
[19] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[20] Morgan Quigley, Ken Conley, Brian Gerkey, Josh Faust, Tully Foote, Jeremy Leibs,
Rob Wheeler, and Andrew Ng. 2009. ROS: an open-source Robot Operating
System. ICRA Workshop on Open Source Software 3.
[21] Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa, and Hiroshi
Ishiguro. 2018. Intrinsically motivated reinforcement learning for human–robot
interaction in the real-world. Neural Networks 107 (2018), 23–33.
https://doi.
org/10.1016/j.neunet.2018.03.014 Special issue on deep reinforcement learning.
[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models
From Natural Language Supervision. arXiv:2103.00020 [cs.CV]
[23] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language Models are Unsupervised Multitask Learners. https:
//api.semanticscholar.org/CorpusID:160025533
[24] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Rad-
ford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation.
arXiv:2102.12092 [cs.CV]
[25] Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, and David
Lindner. 2023. Vision-Language Models are Zero-Shot Reward Models for Rein-
forcement Learning. arXiv:2310.12921 [cs.LG]
[26] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020.
DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.
arXiv:1910.01108 [cs.CL]
[27] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.
2021. LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs.
arXiv:2111.02114 [cs.CV]
[28] Saurav Singh and Jamison Heard. 2022. Human-Aware Reinforcement Learning
for Adaptive Human Robot Teaming. In Proceedings of the 2022 ACM/IEEE Inter-
national Conference on Human-Robot Interaction (Sapporo, Hokkaido, Japan) (HRI
’22). IEEE Press, 1049–1052.
[29] Hang Su, Wen Qi, Jiahao Chen, Chenguang Yang, Juan Sandoval, and Med Amine
Laribi. 2023. Recent advancements in multimodal human–robot interaction.
Frontiers in Neurorobotics 17 (2023). https://doi.org/10.3389/fnbot.2023.1084000
[30] Andrea Thomaz. 2023. Robots in Real Life: Putting HRI to Work. In Proceedings of
the 2023 ACM/IEEE International Conference on Human-Robot Interaction (Stock-
holm, Sweden) (HRI ’23). Association for Computing Machinery, New York, NY,
USA, 3. https://doi.org/10.1145/3568162.3578810
[31] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guil-
laume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.
arXiv:2302.13971 [cs.CL]
[32] Katie Winkle, Erik Lagerstedt, Ilaria Torre, and Anna Offenwanger. 2023. 15 Years
of (Who)Man Robot Interaction: Reviewing the H in Human-Robot Interaction.
J. Hum.-Robot Interact. 12, 3, Article 28 (apr 2023), 28 pages. https://doi.org/10.
1145/3571718
[33] Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, and Harold Soh. 2023.
Translating Natural Language to Planning Goals with Large-Language Models.
arXiv:2302.05128 [cs.CL]
[34] Kaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen, Hongxia Jin, Lise Getoor,
and Xin Eric Wang. 2023. ESC: Exploration with Soft Commonsense Constraints
for Zero-Shot Object Navigation. In Proceedings of the 40th International Confer-
ence on Machine Learning (Honolulu, Hawaii, USA) (ICML’23). JMLR.org, Article
1806, 14 pages.
[35] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang,
Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu
Chen, Wentao Zhang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya
Sachan. 2023. Agents: An Open-source Framework for Autonomous Language
Agents. arXiv:2309.07870 [cs.CL]
"
"This paper introduces a real-time end-to-end visuomotor learning algorithm for multi-instance reaching, where a monocular RGB image and the manipulator’s joint angles are the input to a light-weight fully-convolutional network (FCN) to produce control candidates, and the optimal control candidate is identified by regressing a control-Lyapunov function (cLf) value. The multi-instance capability emerges naturally from the stability analysis associated with the cLf formulation. The proposed algorithm is demonstrated to effectively reach and grasp objects from different categories on a table-top amid distractors and visually similar objects using an over-the-shoulder monocular RGB camera. The network can run up to ∼160 fps during inference on one GTX 1080 Ti GPU.","Scenes that contain multi-instances are common in our daily life, such as cutlery sets on dining tables, books and stationary on desks, fruit hanging on trees, etc. As robotic systems transition into more real-world and shared autonomy environments, the ability to reach and grasp objects amid distracters and in the presence of visually similar objects becomes critical. Pipeline approaches follow the “detect, decide, then servo” paradigm. Grasping algorithms such as [1], [2] first detect objects and estimate 6DoF poses for each detected object. The selection module chooses which object to grasp and the estimated grasp pose is provided to the servo module. The grasp action is undertaken using the standard pose control for robotic manipulators.","nanConvolutional neural networks have become the default algorithm for detection and pose estimation from visual data. Algorithms that first regress an intermediate representation, such as image keypoints, and then compute object pose by solving a PnP problem [3], [4], [5], have achieved impressive performance on popular monocular pose estimation datasets including LINEMOD [6] and Occlusion LINEMOD [7]. However, existing algorithms are trained on single instance datasets and it is unclear how the underlying architecture will adapt to multi-instance.nannan","The control action is generated from a control Lyapunov function formulation, and separately regress the value of the Lyapunov function alongside the corresponding control. The algorithm then uses the regressed Lyapunov function value to select the control action associated with the lowest Lyapunov value, ensuring that the closed-loop system is drawn to the object that is easiest to grasp.nan","In summary, the key contributions of this paper are:

• We demonstrate a real-time, closed-loop, image-to-control fully-convolutional network for robotic reaching in cluttered and dynamic environments. The proposed network achieves consistent high grasp success rate for different object categories regardless of the presence of simultaneous instances and visual clutter.

• We showcase the simplicity and efficiency of utilising a control Lyapunov function approach to deal with visual ambiguity associated with multi-instance grasping.

• We demonstrate that the proposed approach can be trained entirely on simulated data and transfer effectively to real world scenarios.","We propose a fully-convolutional, image-to-control network for a multi-instance robotic reaching task. In particular, we formulate control actions based on a control Lyapunov function and regress both the proposed control and the associated Lyapunov value output at a grid cell level in an end-to-end manner. This provides us a natural structure to implement the proposed effective and efficient non-optimal suppression strategy and reach and grasp robustly in highly complex dynamic scenes. Our system is trained entirely on synthetic data yet robust against the sim-to-real domain gap. Real-world experiments on three different object categories demonstrate the system is able to reach and with high accuracy of 92.8% amid distractors and an arbitrary number of object instances.",End-to-end Multi-Instance Robotic Reaching from Monocular Vision,"Zheyu Zhuang, Xin Yu, Robert Mahony","End-to-end Multi-Instance Robotic Reaching from Monocular Vision
Zheyu Zhuang1, Xin Yu2, Robert Mahony1
Abstract— Multi-instance scenes are especially challenging
for end-to-end visuomotor (image-to-control) learning algo-
rithms. “Pipeline” visual servo control algorithms use separate
detection, selection and servo stages, allowing algorithms to
focus on a single object instance during servo control. End-to-
end systems do not have separate detection and selection stages
and need to address the visual ambiguities introduced by the
presence of arbitrary number of visually identical or similar
objects during servo control. However, end-to-end schemes
avoid embedding errors from detection and selection stages
in the servo control behaviour, are more dynamically robust
to changing scenes, and are algorithmically simpler. In this
paper, we present a real-time end-to-end visuomotor learning
algorithm for multi-instance reaching. The proposed algorithm
uses a monocular RGB image and the manipulator’s joint
angles as the input to a light-weight fully-convolutional network
(FCN) to generate control candidates. A key innovation of the
proposed method is identifying the optimal control candidate by
regressing a control-Lyapunov function (cLf) value. The multi-
instance capability emerges naturally from the stability analysis
associated with the cLf formulation. We demonstrate the
proposed algorithm effectively reaching and grasping objects
from different categories on a table-top amid other instances
and distractors from an over-the-shoulder monocular RGB
camera. The network is able to run up to ∼160 fps during
inference on one GTX 1080 Ti GPU.
I. INTRODUCTION
Scenes that contain multi-instances are common in our
daily life, for example; cutlery sets on dining tables, books
and stationary on desks, fruit hanging on trees, etc. As
robotic systems transition into more real-world and shared
autonomy environments, the ability to reach and grasp ob-
jects amid distracters and in the presence of visually similar
objects becomes critical. Pipeline approaches follow the “de-
tect, decide, then servo” paradigm. Grasping algorithms such
as [1], [2] first detect objects and estimate 6DoF poses for
each detected object. The selection module chooses which
object to grasp and the estimated grasp pose is provided
to the servo module. The grasp action is undertaken using
the standard pose control for robotic manipulators. Such
a pipeline can be run in real-time if the computational
requirements can be met and a heuristic is used to ensure
the algorithm does not jump between different grasp targets
unintentionally.
Convolutional neural networks have become the default
algorithm for detection and pose estimation from visual data.
This research was supported by the Australian Research Council through
the “Australian Centre of Excellence for Robotic Vision” CE140100016.
1 Zheyu Zhuang, Robert Mahony are with “Australian Centre for Robotic
Vision”, Research School of Engineering, The Australian National Univer-
sity. first.last@anu.edu.au
2 Xin Yu is with School of Computer Science, The University of
Technology Sydney. first.last@uts.edu.au
 
Coordinate Tensor
RGB Frame
Joint Angles
bu
klQHDSDcbXhd+9J1JRwe/0JCF+jIacRhQjbaSBfegFgoVqEpsv8x5oSEZIZ2meD+yaU3emgIvELUkNlGgN7C8vFDiNCdeYIaX6rpNoP0NSU8xIXvVSRKEx2hI+oZyFBPlZ9MLcnhilBGQprHNZyqvzsyFKtiSVMZIz1S814h/uf1Ux1d+hnlSaoJx7NBUcqgFrCIA4ZUEqzZxBCEJTW7QjxCEmFtQquaENz5kxdJ56zuNurnt41a86qMowKOwDE4BS64AE1wA1qgDTB4BM/gFbxZT9aL9W59zEqXrLnAPyB9fkD8gmX3Q=</latexit><latexit sha1_base64=""bDlJ4Dkv/uCFfsCztpJ6iC9V2k0="">ACAXicbVDLSsNAFJ34rPUVdSO4GSyCq5JIRZdFNy4r2Ac0oUwmk3boZCbMTJQS4sZfceNCEbf+hTv/xkmbhbYeGOZwzr3ce0+QMKq043xbS8srq2vrlY3q5tb2zq69t9RIpWYtLFgQvYCpAijnLQ1Yz0E
bVmin
hI3eIohnbI+kr0BGdopU6R+veiCwOGqR8yHAm03qWdXyEB0xDobJOseSW3QnoIvFmpERmqHaKX3434kICrlkxrQ8N8Z2yjQKLiEr+ImBmPEh60PLUsVCMO108klGj6zSpb1I21JIJ+rviZSFxozCwHaOzXz3lj8z2sl2Ltop0LFCYLi0W9RFKM6DgW2hUaOMqRJYxrYW+lfMA042jDK9gQvPmXF0n9pOydls9uTkuVy1kceXJADskx8cg5qZBrUiU1wskjeSav5M15cl6cd+dj2pzZjP75A+czx9jaJt</latexit><latexit sha1_base64=""KhWYSFXwUIyR1A+hIgP7pdR460="">ACXicbVA9SwNBEN2LXzF+RS1tFoNgFe5E0TJoYxnBfEAuhL3NJFmyt3fszqnhuNbGv2JjoYit/8DOf+Pmo9DEBwOP92aYmRfEUh03W8nt7S8srqWXy9sbG5t7xR39+omSjSHGo9kpJsBMyCFghoKlNCMNbAwkNAIhldjv3EH2o
Control Proposals
FCN
Execute Control      , and Update Scene 
bu
<latexit sha1_base64=""bDlJ4Dkv/uCFfsCztpJ6iC9V2k0="">ACAXicbVDLSsNAFJ34rPUVdSO4GSyCq5JIRZdFNy4r2Ac0oU
wmk3boZCbMTJQS4sZfceNCEbf+hTv/xkmbhbYeGOZwzr3ce0+QMKq043xbS8srq2vrlY3q5tb2zq69t9RIpWYtLFgQvYCpAijnLQ1Yz0EklQHDSDcbXhd+9J1JRwe/0JCF+jIacRhQjbaSBfegFgoVqEpsv8x5oSEZIZ2meD+yaU3emgIvELUkNlGgN7C8vFDiNCdeYIa
X6rpNoP0NSU8xIXvVSRKEx2hI+oZyFBPlZ9MLcnhilBGQprHNZyqvzsyFKtiSVMZIz1S814h/uf1Ux1d+hnlSaoJx7NBUcqgFrCIA4ZUEqzZxBCEJTW7QjxCEmFtQquaENz5kxdJ56zuNurnt41a86qMowKOwDE4BS64AE1wA1qgDTB4BM/gFbxZT9aL9W59zEqXrLnAP
yB9fkD8gmX3Q=</latexit>
Non-optimal
Suppression
Network Input
7x1x1
Fig. 1: Architecture of the proposed closed-loop reaching
algorithm. A fully-convolutional network densely predicts
a control Lyapunov function (cLf) value bV and control bu
associated to each foreground image grid cell. Non-optimal
suppression is achieved by selecting the control associated
with the grid cell corresponding to the lowest cLf value.
The control is updated in real-time as the image and joint
angles are updated. The reaching trajectory terminates when
the regressed Lyapunov value is lower than a threshold.
Algorithms that first regress an intermediate representation,
such as image keypoints, and then compute object pose by
solving a PnP problem [3], [4], [5], have achieved impressive
performance on popular monocular pose estimation datasets
including LINEMOD [6] and Occlusion LINEMOD [7].
However, existing algorithms are trained on single instance
datasets and it is unclear how the underlying architecture will
adapt to multi-instance. Tremblay et al. [8] propose a real-
time pose estimation network with multi-instance capability
and showcase repeatable experiments of robotic grasping.
To authors understanding, this work [8] is the state-of-the-
art result in multi-instance visual reaching and grasping.
While separating the computer vision pose estimation from
servo control is conceptually simple, this approach can lead
to undesirable error propagation between modules [8]. Fur-
thermore, in dynamic scenes, the decision module needs to
associate and track the estimated poses between frames [9].
In contrast, end-to-end approaches are known to be robust
to dynamic scene changes and model errors [10]. Levine et
al. [11] demonstrate a robot accomplishing real-world tasks
such as hanging clothes and screwing on a bottle cap by
executing control learnt from RGB inputs. James et al. [12]
transfer a recurrent network that learns a multi-stage task
from simulation to real-world with no real-samples. This
work shows the robot sequentially reach, grasp and place a
red cube amid distracters. Zhang et al. [13] achieve closed-
arXiv:2401.11834v1  [cs.RO]  22 Jan 2024
loop reaching towards one unique target in a clutter from
monocular RGB images by learning a visuomotor policy
from a pose-based controller. To increase the network’s accu-
racy, [12] learns the target pose from the image feature vector
as an auxiliary task, and [13] pre-trains the CNN feature
extractor by regressing the target pose. These techniques
apply only to the single instance scenario. For multi-instance
image-to-control learning algorithms, Zhuang et al. [14] use
similar fully-connected network architecture as in [13], [12].
However, a hot-swap scheme is proposed to attend network’s
focus onto a single instance during the final stages of servo
control.
In this paper, we propose a fully-convolutional network
that densely predicts control action over a grid of image
cells, associating the dominant visible instance of the desired
object category with the cell. This naturally gives the network
the ability to generalise over an arbitrary number of object
instances visible in different grid cells. A key innovation is
that we generate the desired control action from a control
Lyapunov function formulation, and separately regress the
value of the Lyapunov function alongside the corresponding
control. The algorithm then uses the regressed Lyapunov
function value to select the control action associated with the
lowest Lyapunov value, ensuring that the closed-loop system
is drawn to the object that is easiest to grasp. This behaviour
is encoded in the Lyapunov control function design, and the
selected control acts to continuously decrease the Lyapunov
function value, leading to successful grasp actions.
In summary, the key contributions of this paper are:
• We demonstrate a real-time, closed-loop, image-to-
control fully-convolutional network for robotic reaching
in cluttered and dynamic environments. The proposed
network achieves consistent high grasp success rate for
different object categories regardless of the presence of
simultaneous instances and visual clutter.
• We showcase the simplicity and efficiency of utilising a
control Lyapunov function approach to deal with visual
ambiguity associated with multi-instance grasping.
• We demonstrate that the proposed approach can be
trained entirely on simulated data and transfer effec-
tively to real world scenarios.
II. FORMULATION
This section presents the formulation of the control Lya-
punov function and corresponding velocity control for a
reaching task.
The 6 DoF poses of target frame {G} and end-effector
frame {H} are represented by elements of the Special
Euclidean Group SE(3). Denote the pose of a frame {B}
with respect to a reference frame {A} as AXB, and its
rotation matrix and translation vector as ARB ∈ SO(3) and
ApB ∈ R3 respectively. The left superscript is omitted if the
pose is defined with respect to the world reference frame.
The absolute end-effector pose XH = XH(θ) is a function
of joint angles θ ∈ R6×1, i.e., the forward kinematics model
of the manipulator.
A. Symmetry-aware Control Lyapunov Function
A control Lyapunov function (cLf) for a reaching task is
a continuously differentiable scalar-valued positive-definite
function V(θ) of the joint angles. V(θ) is zero only at the
joint coordinates for the desired goal pose {Gi} of an object
i. We formulate the cLf as:
V(XH) := 1
2∥ XH − XGi∥2
kF ,
(1)
where ∥ · ∥kF denotes a preferentially weighted Frobenius
norm. In particular, we preferentially weight the translation
component of the pose matrix to balance the relative sensi-
tivity of the homogeneous transform to rotational and trans-
lation displacements. In this work, a preferential weighting
of 5-to-1, translation to rotation weighting, is determined
empirically.
Many objects of interest have geometric symmetries and
there are a continuum of equally valid grasp poses XGi ∈ Gi
for an object i described by a set Gi ⊂ SE(3). To address this
we allow the pose XGi in (1) to vary within the constraint
set Gi depending on the end-effector pose XH. That is, for
a given object i, then XGi := XGi(XH) is chosen as a
function of the end-effector pose
XGi := arg min
XG∈Gi
Tiling
Coordinate Tensor
RGB Image
Joint Angles
4x up
2x up
bu
<latexit sha1_base64=""bDlJ4Dkv/uCFfsCztpJ6iC9V2k0="">ACAXicbVDLSsNAFJ34rPUVdSO4GSyCq5JIRZdFNy4r2Ac0oUwmk3boZCbMTJQS4sZfceNCEbf+hTv/xkmbhbYeGOZwzr3ce0+QMKq043xbS8srq2vrlY3q5tb2
texit>zq69t9RIpWYtLFgQvYCpAijnLQ1Yz0EklQHDSDcbXhd+9J1JRwe/0JCF+jIacRhQjbaSBfegFgoVqEpsv8x5oSEZIZ2meD+yaU3emgIvELUkNlGgN7C8vFDiNCdeYIaX6rpNoP0NSU8xIXvVSRKEx2hI+oZyFBPlZ9MLcnhilBGQprHNZyqvzsyFKtiSVMZIz1S814h/uf1Ux1d+hnlSaoJx7NBUcqgFrCIA4ZUEqzZxBCEJTW7QjxCEmFtQquaENz5kxdJ56zuNurnt41a86qMowKOwDE4BS64AE1wA1qgDTB4BM/gFbxZT9aL9W59zEqXrLnAPyB9fkD8gmX3Q=</la
bVmin
<latexit sha1_base64=""KhWYSFXwUIyR1A+hIgP7pdR460="">ACXicbVA9SwNBEN2LXzF+RS1tFoNgFe5E0TJoYxn
BfEAuhL3NJFmyt3fszqnhuNbGv2JjoYit/8DOf+Pmo9DEBwOP92aYmRfEUh03W8nt7S8srqWXy9sbG5t7xR39+omSjSHGo9kpJsBMyCFghoKlNCMNbAwkNAIhldjv3EH2ohI3eIohnbI+kr0BGdopU6R+veiCwOGqR8yHAm03qWdXyEB0xDobJOseSW3
QnoIvFmpERmqHaKX3434kICrlkxrQ8N8Z2yjQKLiEr+ImBmPEh60PLUsVCMO108klGj6zSpb1I21JIJ+rviZSFxozCwHaOzXz3lj8z2sl2Ltop0LFCYLi0W9RFKM6DgW2hUaOMqRJYxrYW+lfMA042jDK9gQvPmXF0n9pOydls9uTkuVy1kceXJADsk
x8cg5qZBrUiU1wskjeSav5M15cl6cd+dj2pzZjP75A+czx9jaJt</latexit>
Segmentation
Masking
Control Proposals
⊕
<latexit sha1_base64=""RXk+KwpaEpWqH3obcskjrHYEvkA="">AB+HicbVDLS
sNAFL2pr1ofrbp0M1gEVyURZdFNy4r2Ac0oUymk3boZBLmIdTQL3HjQhG3fo7/8ZJm4W2Hhg4nHMv98wJU86Udt1vp7S2vrG5Vd6u7Ozu7VdrB4cdlRhJaJskPJG9ECv
KmaBtzTSnvVRSHIecdsPJbe53H6lULBEPeprSIMYjwSJGsLbSoFb1Y6zHYZT5ScqNmg1qdbfhzoFWiVeQOhRoDWpf/jAhJqZCE46V6ntuqoMS80Ip7OKbxRNMZngEe1bK
nBMVZDNg8/QqVWGKEqkfUKjufp7I8OxUtM4tJN5TLXs5eJ/Xt/o6DrImEiNpoIsDkWGI52gvAU0ZJISzaeWYCKZzYrIGEtMtO2qYkvwlr+8SjrnDe+icXl/UW/eFHWU4Rh
O4Aw8uIm3EL2kDAwDO8wpvz5Lw4787HYrTkFDtH8AfO5w90OJOe</latexit>
Skip Connection
Max Pooling
Conv + BN + Activation
Residual Block ( With            Conv )
Linear + Activation
Bilinear Upsampling
Residual Block ( ResNet )
Concatenation
⊕
<latexit sha1_base64=""RXk+KwpaEpWqH3obcskjrHYEvkA="">AB+HicbVDLS
sNAFL2pr1ofrbp0M1gEVyURZdFNy4r2Ac0oUymk3boZBLmIdTQL3HjQhG3fo7/8ZJm4W2Hhg4nHMv98wJU86Udt1vp7S2vrG5Vd6u7Ozu7VdrB4cdlRhJaJskPJG9ECv
KmaBtzTSnvVRSHIecdsPJbe53H6lULBEPeprSIMYjwSJGsLbSoFb1Y6zHYZT5ScqNmg1qdbfhzoFWiVeQOhRoDWpf/jAhJqZCE46V6ntuqoMS80Ip7OKbxRNMZngEe1bK
nBMVZDNg8/QqVWGKEqkfUKjufp7I8OxUtM4tJN5TLXs5eJ/Xt/o6DrImEiNpoIsDkWGI52gvAU0ZJISzaeWYCKZzYrIGEtMtO2qYkvwlr+8SjrnDe+icXl/UW/eFHWU4Rh
O4Aw8uIm3EL2kDAwDO8wpvz5Lw4787HYrTkFDtH8AfO5w90OJOe</latexit>
1 ⇥ 1
<latexit sha1_base64=""1OSIeQLI/nz7S7nLCJ5nwtQj3nM="">AB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseiF48V7Ae2oWy2m3bpZhN2J0IJ/Rde
PCji1X/jzX/jts1BWx8MPN6bYWZekEh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRI3g7GtzO/cS1EbF6wEnC/YgOlQgFo2ilR4/0UETcEK9frhVdw6ySrycVCBHo1/+6g1ilkZcIZPUmK7nJuhnVKNgk9LvdTwhLIxHfKupYraNX42v3hKzqwyIGsbSkc/X3REYjYyZRYDsjiOz
7M3E/7xuiuG1nwmVpMgVWywKU0kwJrP3yUBozlBOLKFMC3srYSOqKUMbUsmG4C2/vEpaF1WvVr28r1XqN3kcRTiBUzgHD6gDnfQgCYwUPAMr/DmGOfFeXc+Fq0FJ585hj9wPn8ASgKQBg=</latexit>
(a)
0
t = 0s
t = 3s
t = 6s
t = 9s
1
2
1
2
2
4
3
3
5
6
2
4
3
5
6
5
(b)
Fig. 2: (a) The proposed network architecture. (b) Visualisation of the dynamic robustness of the reaching performance.
The robot is undertaking a real-time reaching trajectory, however, it is stopped every 3s and the scene is rearranged. The
Lyapunov value of the image grid cell is coded by colour as shown by the colour-bar. Initially, target instance 1 has the
lowest Lyapunov value and the reaching trajectory is focusing on this instance. The control is unchanged with the addition of
another target instance 3. After target 1 is removed, the reaching trajectory refocuses on target instance 2. The introduction
of any extra instances or distractors makes no impact on the successful grasp achieved at time t = 9s.
(·)∨ denote the linear readout mapping that takes a matrix in
se(3) and forms the associated (ω, v) ∈ R6 vector of angular
and linear velocities. The proposed joint velocity control is
u := −J(θ)−1(∇V(XH))∨,
(2)
where θ are joint angles associated to XH. This velocity
controller design guarantees, in a closed-loop system, execut-
ing the velocity control continuously decreases the Lyapunov
value, i.e.
˙V = −tr(∇V⊤∇V) = −∥∇V∥2
kF < 0,
where ˙V denotes the time derivative of V.
III. LEARNING THE CONTROL LYAPUNOV FUNCTION
The proposed network divides the input image into square
grid cells. Each cell predicts a binary foreground visibility
score, a Lyapunov value and a control vector. All grid cells
corresponding to the same visible instance share identical
supervision labels. In this work, the resolution of the grid
cell is set to 1/8 of the input image size.
A. Network Architecture
The network architecture is illustrated in Fig. 2a. We adopt
an image feature extractor as the backbone of this network;
the extracted image feature is shared between the two output
regressors. The architecture is built upon ResNet18 [15]
by an auto-encoder with skip connections. The Joint auto-
encoder comprises two fully-connected layers. It learns a
higher dimensional latent representation of the joint angles
θ ∈ R6×1. The latent joint representation is then tiled over
the spatial dimension and concatenated with image features
and the coordinate tensor. The segmentation regressor per-
forms binary classification to separate the foreground from
the background, and the control regressor infers the cLf
value bV and velocity control bu. Residual block with 1 × 1
convolution in [16] is used in two output regressors.
The spatial invariance property of convolutional kernels
grants fully-convolutional architectures the potential of gen-
eralising to an arbitrary number of object instances. However,
for an image-to-control task, the network needs access to
precise spatial information. Inspired by “CoordConv” in [17],
[18], we introduce a coordinate tensor input. This is a two-
channel tensor, that carries the normalised UV coordinates
of each grid cells. Coordinate tensor is concatenated with the
joint features and image features as the input to the control
regressor.
B. Loss Functions
Binary cross entropy loss is used for the segmentation
branch:
Lseg = 1
N
N
X
n=1

yn logS( bCn) + (1 − yn) log(1 − S( bCn))

,
where n ∈ {1, . . . , N} denotes grid cell index, S(·), bC
and y represent the sigmoid function, segmentation regressor
output and binary visibility label respectively.
A weighted L1 loss Lctrl is used for the control branch
and this should not be confused with the Lyapunov function
V. Only the loss generated by foreground grid cells are
penalised. The control regressor’s loss function is formulated
as:
Lctrl =
1
Npos
N
X
n=1
yn
Vn − bVn
 + 1
6 |un − bun|

,
(3)
where Npos represents the total number of grid cells in the
foreground. The final loss function is: L = Lseg + Lctrl.
C. Non-optimal Suppression
Our network predicts a tuple of values (bV, bu) at each
grid cell associated with targets present in that grid cell.
An essential step in the algorithm is to select the single
input bu that is the optimal control action proposal for the
robot to act on. In the pipeline algorithms, this control choice
is determined by the instance the algorithm is targeting to
grasp, a decision process that must be explicitly coded. For
end-to-end methods such as our system, this decision process
emerges from the dynamic behaviour of the system.
Since we have derived a control action from a global
control Lyapunov function formulation and regress the actual
value of the Lyapunov function along with the control
proposal, these values provide an ideal metric of optimality
for choosing the control action. By executing the action
associated with the minimal Lyapunov value chosen across
the segmentation masks, the control proposal is adapted to
maximally decrease that particular instance of the cLf. It
follows, that the closed-loop motion of the robot reinforces
the initial preference for a given instance. This motion
decreases the cLf value associated to this instance more
quickly than the cLf value associated with other instances.
This increases the likelihood that the same instance will
generate the minimal Lyapunov regressor for the next image
input. Indeed, in the absence of errors and for static scenes,
the choice of instance is locked in by the initial minimisation.
For real-world dynamic scenarios, the closed-loop motion
of the robot can be seen as an analogue solver for a stochastic
gradient descent algorithm that both selects an instance to
target and then computes the grasp pose. We refer to this
process as non-optimal suppression since the evolution of the
closed-loop acts to increase confidence in the chosen instance
while the closed-loop trajectory is tracked. The progressive
nature of the decision process and its integration into the
servo task makes the formulation naturally robust to dynamic
variations in the scene. A reaching example with visualised
non-optimal suppression process is shown in Fig. 2b.
Image grid cells associated with the same object instance
share identical regression targets. In practice, it is nearly
impossible for these cells to produce identical values. Se-
lecting the lowest Lyapunov value amongst those cells in a
given instance segmentation provides the proposed method to
suppressing similar control action proposals. Statical meth-
ods like clustering and averaging may increase the accuracy
of the inference. However, a frame-by-frame clustering and
averaging algorithm adds significant computation overhead.
Thus, we do use a cross-frame exponentially weighted
moving averaging term, known as momentum in machine
learning literature, to filter sudden changes in the closed-
loop control action inference. The momentum term is define
as:
¯ut = η ¯ut−1 + (1 − η)but,
(4)
where η is a tuneable constant ∈ [0, 1] and but is a current
raw velocity control prediction from the network. This is
equivalent to applying a low pass filter to the raw control
signal generated by the network. The constant η is set to 0.5
in our experiments. It leads to smother reaching trajectories
especially under high controller gain.
IV. IMPLEMENTATION
We perform robotic grasping as the ultimate test for the
proposed reaching algorithm. A two-finger parallel gripper is
attached to a UR5 manipulator. The finger tips of the parallel
gripper are padded with textured soft-silicone to increase the
contact area. A Realsense D435 is place over the shoulder
of the manipulator (see Fig.3). Only the RGB camera of
Realsense is used for the experiments. The camera’s frame
rate is set to 60Hz and the resolution is resized to 512×384.
A. Data Collection
Generating large-scale, pre-labeled data with simulators
is almost cost-free compared to collecting the real-world
data. Deploying a network trained purely on synthetic data
to the real-world has been proven feasible [8], [12]. In this
work, we replicate the real-world setting in Coppeliasim. The
camera extrinsics are calibrated to the manipulator base by
observing a checkerboard attached to the end-effector.
For each sample, the number of simultaneous instances is
equally sampled between one and three, and instances are
simulated on the tabletop amongst a collection of random
(a) Simulation
(b) Real-world
Fig. 3: Lab and simulation environments: The first-person
camera is positioned as shown in Fig. 3b (marked with
the white circle), pointing towards the table workspace.
The simulated environment is geometrically identical to the
physical layout in the lab. The simulated camera is calibrated
to simulate the real camera.
distractors. The end-effector is positioned at a random 6 DoF
pose within the workspace of the robot. The end-effector’s
initial simulated translation component is sampled on the
surface of a quarter sphere, whose radius is sampled from
uniform distribution and constrained by the manipulator’s
usable workspace. The initial simulated rotation is sampled
based on axis-angle representation; the direction axis is
sample inside a downward-facing cone, and the angle is
sample in [−π, π]. Domain randomisation [19] is only
applied to all visible, non-object entities. In order to increase
data efficiency and improve the local convergence of the
closed-loop system, we increase the sampling density while
the manipulator approaches the neighbourhood of a target.
In this work, we collect three datasets for mugs, IKEA
LACK Table Legs (table leg for short), and potted meat
(Spam) cans. Mug and Spam are from the YCB dataset [20].
The “multi-table leg” dataset contains up to two simultaneous
instances; the “multi-spam” and “multi-mug” dataset contain
up to three instances. No spam was consumed in the devel-
opment of this paper.
B. Network Training Details
Our training dataset contains approximately 55k simulated
samples for each object category. We train our networks with
90% of the training dataset while using the remaining 10%
for evaluation. The brightness, saturation, contrast and hue of
input images are randomly jittered at 10% of their maximum
ranges to alleviate the domain gap between simulated and
real data. The weights of ResNet18 backbone are randomly
initialised. We use ADAM optimiser with the batch size 64
for learning. The learning rate is initialised as 10−3 with a
decay rate 0.8 for every 5 epochs. The maximum training
epoch is set to 80.
V. GRASPING EXPERIMENTS
In this section, we demonstrate the performance and
robustness of the proposed reaching method through multi-
instance real-world grasping experiments. Mugs, Table Legs,
and Spam cans are used as testing objects.
Coord-
Conv
Grasp
Success
1
2
3
4
%
Spam
with
13
14
15
14
93.3
w/o
12
11
12
11
76.7
LACK
Leg
with
14
14
14
12
90.0
w/o
9
10
9
9
61.7
Mug
with
14
14
15
14
95.0
w/o
13
13
13
14
88.3
Avg. (%)
with
-
-
-
-
92.8
w/o
-
-
-
-
75.6
TABLE I: Statistical results of real-world grasping exper-
iments using with and without CoordConv. 60 reaching
experiments (15 for one, two, three and four simultaneous
instances) are conducted for each object category.
For each object category, we separately conduct 60 reach-
ing experiments (15 for one, two, three and four simultaneous
instances). The gripper closes when the Lyapunov value is
below a designated threshold. We set this threshold to 0.005
for all our experiments. In each scene, the end-effector pose
is randomly initialised in the workspace, and target instances
and distractors are shuffled. IKEA table legs have larger
grasping error tolerance due to their elongated geometric
shape. Hence, for IKEA table legs, we only register a
successful grasp when the gripper closes within ±3 cm from
the designated grasp point. The grasping strategy for mugs
is designed as opening the gripper when its tip is inside the
cavity of mugs. That is we grasp from the inside out for
mugs.
As shown in Tab.I, the proposed method achieves on
average 92.8% grasp success rate over 3 different object
categories without leveraging additional sim-to-real transfer
techniques apart from domain randomisation. More impor-
tantly, the proposed algorithm maintains its high accuracy
when an arbitrary number of simultaneous instances are
present. The network also generalises beyond the number
of instances included in the training dataset for three object
categories. The proposed algorithm exhibits strong false
positive rejection capability. Most of false positives in the
vision pipeline correspond to Lyapunov values larger than the
current minimum and are automatically rejected by our non-
optimal suppression algorithm. Occasional non-persistent
false positives with lower Lyapunov values do disrupt the
control, however, this effect is not significant and is strongly
mitigated by the momentum term added to the closed-loop
controller design formulated in Eq.(4).
The proposed method achieves comparable overall grasp
success rate to the state-of-the-art pose estimation based
multi-instance grasping [8]. Note that, in our experiments the
degrees of freedom of the object poses are constrained since
they lie on the table. However, our system regresses a full 6
DoF control and the relative pose of the initial end-effector
(a) With CoordConv
(b) Without CoordConv
Fig. 4: Visualisation of 32 reaching trajectories for two networks trained with and without CoordConv for the “multi-spam”
dataset. For a fair comparison, experiments with each network share the same pre-sampled random initial end-effector poses.
The test scene is static, and contains one Spam Can and distractors. The vertical axis indicates the control regression loss
Lctrl defined in Eq. (3) .
with respect to the targets are unconstrained. A further key
point is that the proposed network can run at up to 160 fps
on a single GTX 1080 Ti GPU. The camera that we use
only runs at 60Hz reducing the control bandwidth used the
experimental studies reported, however, the computational
complexity of the approach is not a limiting factor for higher
fps than 60Hz. The real-time computational performance of
the proposed algorithm is significantly ahead of state-of-the-
art methods that use pipeline architectures (approximately 10
fps in [8]).
VI. ABLATION STUDY
The CoordConv module that tiles the grid cell repre-
sentation with coordinates is a crucial component of the
proposed network, enhancing stable and reliable real-world
reaching. Islam et al. [21] provide evidence to support that
spatial information can be implicitly learnt by applying zero-
padding operations. However, this implicitly learnt spatial
information has proved to be insufficient for accurate visuo-
motor reaching task. Including the coordinate labels into the
input to the following convolutional layers provides a simple
way to compute geometric information from the image.
We observe the two networks with and without CoordConv
have nearly identical training and evaluation convergence for
the three synthetic datasets. However, in the real-world, with-
out the CoordConv module, grasping success rate drops from
92.8% to 75.6% (Tab.I). The major cause of failures is the
imprecise and unstable final reaching trajectory convergence.
That is, the network without the CoordConv module is able
to identify targets, roughly servo control towards the goal,
but lacks more accurate geometric information for reliable
grasping.
For further investigation, we additionally perform 32 sets
of reaching experiments on one static scene with a single
instance. In each set of experiments, the networks with and
without CoordConv separately servo the robot from identical
initial end-effector pose to the target instance. The end-
effector pose is randomised among different sets. Trajectories
of tool-centre point are visualised in Fig.4a and Fig.4b. It is
clear that the network with CoordConv produces significantly
smoother trajectories and more stable and precise local
reaching convergence.
The joint auto-encoder is another important source of
information for the network regression. This allows the
network to learn a higher dimensional latent representa-
tion of lower dimensional input (i.e. joint angles in R6)
prior to being concatenated with image feature and coor-
dinate tensor. Although two networks with and without the
joint auto-encoder exhibit similar training convergence on
three synthetic datasets, the inclusion of joint auto-encoder
achieves better generalisation over the evaluation datasets;
approximately 47.9% 30.6% and 6.0% relative performance
improvements are observed on the “multi-spam”, “multi-
table leg”, and“multi-mug” datasets respectively.
VII. CONCLUSION
We propose a fully-convolutional, image-to-control net-
work for a multi-instance robotic reaching task. In particular,
we formulate control actions based on a control Lyapunov
function and regress both the proposed control and the
associated Lyapunov value output at a grid cell level in an
end-to-end manner. This provides us a natural structure to
implement the proposed effective and efficient non-optimal
suppression strategy and reach and grasp robustly in highly
complex dynamic scenes. Our system is trained entirely on
synthetic data yet robust against the sim-to-real domain gap.
Real-world experiments on three different object categories
demonstrate the system is able to reach and with high
accuracy of 92.8% amid distractors and an arbitrary number
of object instances.
REFERENCES
[1] A. Zeng, K.-T. Yu, S. Song, D. Suo, E. Walker, A. Rodriguez, and
J. Xiao, “Multi-view self-supervised deep learning for 6d pose esti-
mation in the amazon picking challenge,” in 2017 IEEE international
conference on robotics and automation (ICRA).
IEEE, 2017, pp.
1386–1383.
[2] D. Morrison, A. W. Tow, M. Mctaggart, R. Smith, N. Kelly-Boxall,
S. Wade-Mccue, J. Erskine, R. Grinover, A. Gurman, T. Hunn,
et al., “Cartman: The low-cost cartesian manipulator that won the
amazon robotics challenge,” in 2018 IEEE International Conference
on Robotics and Automation (ICRA).
IEEE, 2018, pp. 7757–7764.
[3] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, “Pvnet: Pixel-wise
voting network for 6dof pose estimation,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2019, pp.
4561–4570.
[4] C. Song, J. Song, and Q. Huang, “Hybridpose: 6d object pose estima-
tion under hybrid representations,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2020, pp.
431–440.
[5] S. Zakharov, I. Shugurov, and S. Ilic, “Dpod: 6d pose object detector
and refiner,” in Proceedings of the IEEE International Conference on
Computer Vision, 2019, pp. 1941–1950.
[6] E. Brachmann, A. Krull, F. Michel, S. Gumhold, J. Shotton, and
C. Rother, “Learning 6d object pose estimation using 3d object
coordinates,” in European conference on computer vision.
Springer,
2014, pp. 536–551.
[7] S. Hinterstoisser, V. Lepetit, S. Ilic, S. Holzer, G. Bradski, K. Konolige,
and N. Navab, “Model based training, detection and pose estimation of
texture-less 3d objects in heavily cluttered scenes,” in Asian conference
on computer vision.
Springer, 2012, pp. 548–562.
[8] J. Tremblay, T. To, B. Sundaralingam, Y. Xiang, D. Fox, and S. Birch-
field, “Deep object pose estimation for semantic robotic grasping of
household objects,” in Conference on Robot Learning, 2018, pp. 306–
316.
[9] P. I. Corke and S. A. Hutchinson, “Real-time vision, tracking and
control,” in Proceedings 2000 ICRA. Millennium Conference. IEEE
International Conference on Robotics and Automation. Symposia
Proceedings (Cat. No. 00CH37065), vol. 1. IEEE, 2000, pp. 622–629.
[10] S. Hutchinson, G. D. Hager, and P. I. Corke, “A tutorial on visual servo
control,” IEEE transactions on robotics and automation, vol. 12, no. 5,
pp. 651–670, 1996.
[11] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training
of deep visuomotor policies,” The Journal of Machine Learning
Research, vol. 17, no. 1, pp. 1334–1373, 2016.
[12] S. James, A. J. Davison, and E. Johns, “Transferring end-to-end
visuomotor control from simulation to real world for a multi-stage
task,” in Conference on Robot Learning, 2017, pp. 334–343.
[13] F. Zhang, J. Leitner, Z. Ge, M. Milford, and P. Corke, “Adversar-
ial discriminative sim-to-real transfer of visuo-motor policies,” The
International Journal of Robotics Research, vol. 38, no. 10-11, pp.
1229–1245, 2019.
[14] Z. Zhuang, J. Leitner, and R. Mahony, “Learning real-time closed
loop robotic reaching from monocular vision by exploiting a control
lyapunov function structure,” in 2019 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems (IROS).
IEEE, 2019, pp.
4752–4759.
[15] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer
vision and pattern recognition, 2016, pp. 770–778.
[16] A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for
human pose estimation,” in European conference on computer vision.
Springer, 2016, pp. 483–499.
[17] R. Liu, J. Lehman, P. Molino, F. P. Such, E. Frank, A. Sergeev, and
J. Yosinski, “An intriguing failing of convolutional neural networks and
the coordconv solution,” in Advances in Neural Information Processing
Systems, 2018, pp. 9605–9616.
[18] X. Wang, T. Kong, C. Shen, Y. Jiang, and L. Li, “Solo: Segmenting
objects by locations,” arXiv preprint arXiv:1912.04488, 2019.
[19] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel,
“Domain randomization for transferring deep neural networks from
simulation to the real world,” in 2017 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems (IROS).
IEEE, 2017, pp.
23–30.
[20] B. Calli, A. Singh, A. Walsman, S. Srinivasa, P. Abbeel, and A. M.
Dollar, “The ycb object and model set: Towards common benchmarks
for manipulation research,” in 2015 international conference on ad-
vanced robotics (ICAR).
IEEE, 2015, pp. 510–517.
[21] M. A. Islam, S. Jia, and N. D. Bruce, “How much position infor-
mation do convolutional neural networks encode?” in International
Conference on Learning Representations, 2019.
"
"This research introduces a personalized time-domain method (pGTFF0) to enhance the intelligibility of noisy speech in an Autism Spectrum Disorder (ASD) context. To achieve this, harmonic features extracted from speech frames are designated as the center frequencies of Gammatone auditory filterbanks, followed by applying a gain factor to the output of the filtered samples. The primary goal is to emulate an external noise filtering tailored to individuals with ASD. A listening test demonstrates that ASD individuals generally had lower intelligibility rates compared to Neurotypical (NT) participants. The proposed pGTFF0 solution outperforms three alternative methods across four acoustic noises and different signal-to-noise ratios (SNRs). Objective measures (ESTOI and PESQ) are also employed for evaluation. Experimental findings indicate that the personalized solution outperforms the competing approaches in terms of intelligibility and quality improvement.","Speech communication in urban environments is commonly affected by acoustic interferences, the most typical one being the noise masking effect. This can be a significant challenge, especially for individuals with Autism Spectrum Disorder (ASD), as the auditory hypersensitivity characteristic of autism can exacerbate the distortion of speech components and signal harmonic information. Recently, investigating harmonic components from noisy speech signals has gained attention, with various strategies developed to enhance intelligibility. However, there is a need to address the presence of intense internal noise (HIN) in ASD individuals, as its combination with external noise further affects speech formants. Thus, this research aims to develop a personalized method to improve intelligibility for ASD individuals by highlighting the harmonic frequencies of noisy speech signals.","nanSeveral approaches have been developed to enhance speech intelligibility in noisy environments. One study shifted formant center frequencies away from the noise region to emulate the Lombard effect, resulting in improved intelligibility of speech signals in car noise environments. Another research employed amplitude and phase estimation filters with harmonic models to improve the signal-to-noise ratio of reconstructed speech signals. Additionally, a F0-based Gammatone Filtering (GTFF0) method was introduced to achieve intelligibility enhancement by using F0 estimates as center frequencies of the auditory filterbank, with gain factors applied to the filtered samples to mitigate the noise masking effect. However, these methods do not specifically address the unique challenges faced by individuals with ASD.nannan","This research proposes a personalized Gammatone Filtering (pGTFF0) method to enhance intelligibility for individuals with ASD. The target noisy signal is initially split into voiced/unvoiced frames, and for each voiced frame, a set of Gammatone filters are implemented and aligned in time. Subsequently, the filtered signals are amplified by gain factors derived empirically to emphasize the presence of harmonics of the fundamental frequency. The final step involves reconstructing the entire speech signal by combining the reconstructed voiced frames and the remaining frames. The optimal gain values that lead to the highest intelligibility are determined using the ESTOI objective measure.nan","A listening test was conducted to evaluate the perceptual differences between NT and ASD groups under noisy conditions. Results showed that ASD volunteers attained lower intelligibility rates than NT participants, further supporting the hypothesis of using perceptual tests as an auxiliary tool for ASD detection. The proposed pGTFF0 method outperformed competitive methods in terms of intelligibility and quality improvement, demonstrating its effectiveness in enhancing speech intelligibility for ASD individuals in noisy environments.","This research introduced a novel personalization method to improve the intelligibility of noisy speech for individuals with ASD, in particular, to assist in mitigating the effects of noise and internal interference on speech perception. Perceptual tests were found to be useful for assisting in the detection of ASD. The pGTFF0 method outperformed baseline techniques under a variety of acoustic noise conditions, showing potential as an assistive tool for people with ASD in noisy environments.",Intelligibility Enhancement of Acoustic Noisy Speech for Autism Spectrum Disorder Condition,"M. Pillonetto, A. Queiroz, R. Coelho","arXiv:2401.11832v1  [eess.AS]  22 Jan 2024
IEEE SIGNAL PROCESSING LETTERS
1
Intelligibility Enhancement of Acoustic Noisy
Speech for Autism Spectrum Disorder Condition
M. Pillonetto, A. Queiroz, Student Member, IEEE, and R. Coelho, Senior Member, IEEE
Abstract—This work introduces a time domain personalized
method (pGTFF0) to achieve intelligibility improvement of noisy
speech for Autism Spectrum Disorder (ASD) situation. For
this proposal, harmonic features estimated from speech frames
are considered as center frequencies of Gammatone auditory
ﬁlterbanks. A gain factor is further applied to the output of the
ﬁltered samples. The key goal is the emulation of an external
noise ﬁltering tailored for individuals with ASD. A perceptual
listening test demonstrates that ASD volunteers attained lower
intelligibility rates than Neurotypical (NT). The proposed solution
is compared to three competing approaches considering four
acoustic noises at different signal-to-noise ratios. Two objective
measures (ESTOI and PESQ) are also adopted for evaluation.
The experimental results show that the personalized solution
outperformed the competing approaches in terms of intelligibility
and quality improvement.
Index Terms—Acoustic Noises, Autism Spectrum Disorder,
Intelligibility Improvement.
I. INTRODUCTION
S
PEECH communication in urban environments is subject
to several acoustic interferences. The most typical one is
the noise masking effect. This is a crucial cause of impairment
and a key challenge for the acoustic signal processing research
area. This concern underlies several applications and systems
such as robot audition, hearing aids, speech and speaker
recognition. Furthermore, some individuals are especially sen-
sitive to this type of effects. This is the case of people with
Autism Spectrum Disorder (ASD). Auditory hypersensitivity
is a key factor that characterize autism [1][2]. This behavior
is challenging in urban environments, where noises cause
distortions on speech components i.e., the formants of the
fundamental frequency (F0) [3][4][5][6], and signal harmonic
region which concentrates the intelligibility information [7][8].
Recently, the investigation of harmonic components from
noisy speech signals has gained signiﬁcant attraction [9][10].
Particularly, in strategies developed to achieve intelligibility
gain. In [11] the formant center frequencies from voiced
speech segments are shifted away from the region of noise
[12], emulating the Lombard effect [13]. Results showed
that the Smoothed Shifting of Formants for Voiced segments
(SSFV) is able to improve the intelligibility of speech signals
in car noise environment. A different approach in [14] used the
amplitude and phase estimation ﬁlter [15] with the harmonic
The authors are with the Laboratory of Acoustic Signal Processing
(lasp.ime.eb.br), Military Institute of Engineering (IME), Rio de Janeiro,
Brazil (email: coelho@ime.eb.br). This work was partially supported by
the National Council for Scientiﬁc and Technological Development (CNPq)
305488/2022-8 and Fundac¸˜ao de Amparo Pesquisa do Estado do Rio
de Janeiro (FAPERJ) 200518/2023. This work is also supported by the
Coordenac¸˜ao de Aperfeic¸oamento de Pessoal de N´ıvel Superior - Brasil
(CAPES) - Grant Code 001.
models (APESHARM). This procedure led to improved signal-
to-noise ratio (SNR) of the reconstructed speech signals.
Finally, the F0-based Gammatone Filtering (GTFF0) method
[16] is described to attain intelligibility enhancement. This
scheme uses F0 estimates [17] as center frequencies of audi-
tory ﬁlterbank. Then, output samples are ampliﬁed by a gain
factor attenuating the noise masking effect, which achieves
intelligibility improvement.
However, in the speciﬁc case of people with ASD a key
issue must be addressed: the presence of intense internal
noise (High Internal Noise - HIN) [2]. The HIN is a crucial
impairment for the daily social communication of individuals
with ASD [6]. It is characterized by the ampliﬁcation of noise
interference in the inner ear. The composition of HIN with
the external noise affects the F0 and formant of the speech
signal in the ASD context [4]. Thus, a strategy to improve
intelligibility must mitigate the effects of external noise by
highlighting the harmonic frequencies of speech signal.
This Letter proposes a personalized Gammatone Filtering
pGTFF0 method especially to attain intelligibility enhancement
for ASD individuals. In this solution, the ﬁltered samples are
ampliﬁed by gain factors emphasizing the F0 estimated with
HHT-Amp [17] and its harmonics from the speech signal. This
strategy aims to increase the robustness of external noise ﬁlter-
ing for ASD condition leading to intelligibility improvement.
This work also includes perceptual tests with Neurotypical
(NT) and ASD volunteers as an auxiliary instrument to detect
Autism Spectrum Disorder.
Extensive experiments are conducted to evaluate the pGTFF0
for speech intelligibility and quality improvement. ESTOI [18]
and PESQ [19] objective measures are used to compare the
proposed and baseline techniques in terms of intelligibility and
quality assessment. The acoustic noisy scenario is composed
of four background acoustic noises and four SNR values
between -10 dB and 5 dB. The results show that the proposed
solution outperforms the competitive methods in terms of
speech intelligibility and quality.
The main contributions of this study are as follows:
• Introduction of the personalized pGTFF0 method to im-
prove the intelligibility of acoustic noisy speech for ASD
situation;
• Deﬁnition of an auxiliary strategy for ASD diagnosis
based on perceptual tests;
• Adoption of personalized gain factors in the Gammatone
ﬁltered samples to emphasize the harmonic components
of noisy speech signals;
• Interesting intelligibility and quality improvement even
for challenging noisy conditions with low SNR values.
IEEE SIGNAL PROCESSING LETTERS
2
Fig. 1. Perceptual Threshold versus Noise (TvN) for ASD versus NT [2].
II. INTELLIGIBILITY FOR AUTISM SPECTRUM DISORDER
Autism is characterized by deﬁcit in the behavioral/social
and sensory ﬁelds1 and acoustic perceptual skills [6]. This
condition is typical in the syndrome, and audiometric tests are
already adopted as an auxiliary instrument for the disorder
detection. It is important to highlight that population with
ASD are considered normal-hearing, i.e., it is assumed that
individuals are not affected by hearing impairment [1]. An
emerging hypothesis considers that a High Internal Noise
(HIN) proﬁle [2] plays an important role for the degradation
of acoustic perception in ASD. In addition, experimental
results [2] indicate this HIN proﬁle with ﬁltering of external
noisy signals. Therefore, the equivalent degradation can be
quantiﬁed by the sum of internal and external interference.
Perceptual sensitivity is measured in [2] at various levels of
additive external noise. These levels are presented as measures
of TvN (Threshold versus Noise), which allow the effects of
noise to be quantiﬁed in NT and ASD individuals. Fig. 1
shows TvN functions [2], whose curves in these graphs follow
a pattern: a segment parallel to the horizontal axis followed by
an ascending segment. The contrast threshold is represented on
the vertical axis, while the level of external noise is represented
on the horizontal axis. In the upper left subplot of Fig. 1, the
additive internal noise without ﬁltering of autistic is compared
with neurotypicals. Observe that ASD people have a greater
presence of naturally additive internal noise when compared
to the rest of the population.
In the graph on the top right, it can be seen the TvN of the
external noise ﬁltering for the NT and ASD cases. This feature
in combination with HIN should yield increased thresholds
across all levels of external noise (bottom left). A similar
pattern is expected if internal multiplicative noise is elevated
for different difﬁculty levels (bottom right). Either HIN and ex-
ternal ﬁltering may cause more negative end-of-sentence pitch
slopes and less typical harmonics-to-noise ratio [4]. Therefore,
individuals with ASD have lower perceptual intelligibility at
loud external noise when compared to neurotypical in the same
conditions.
1Resolution on Autism Specter Disorder (WHA 67.8)
III. THE PROPOSED PERSONALIZED GAMMATONE FILTER
METHOD: PGTFF0
The pGTFF0 introduces personalized gain factors in the
Gammatone ﬁltered [16] harmonic components to emulate
an external noise ﬁltering for people with ASD [2]. The
target noisy signal x(t) is ﬁrst split into Q overlapping 32
ms voiced/unvoiced (V/UV) frames xq(t), q = 1, 2, . . . , Q,
with 50% overlapping. In this work V/UV separation was
previously applied two deﬁne two disjoint sets: Sv is formed
by frames that contain voiced speech, and Su is composed of
the remaining segments, i.e., unvoiced speech and noise only.
For each voiced frame q ∈ Sv, a set of Gammatone ﬁlters
are used to ﬁlter the sample sequence xq(t). The time-domain
impulse response of the Gammatone ﬁlter [20] is deﬁned as
g(t) = atn−1 cos(2πfct + φ)e−2πbt , t ≥ 0 ,
(1)
where a is the amplitude, n is the ﬁlter order, fc is the center
frequency, φ is the phase, and b is the bandwidth. The center
frequencies are set to
fc = ˆF0, 2 ˆF0, . . . , L ˆF0
(2)
where ˆF0 is the pitch value estimated by the HHT-Amp
method [17].
In the proposed method, each ﬁlter hk(t) is implemented2
considering order n = 4 and bandwidth b = 0.25 ˆF0. In order
to align the impulse response functions, phase compensation
is applied to all ﬁlters, which correspond to the non-causal
ﬁlters
hk(t) = a(t + tc)n−1 cos(2πfct)e−2πb(t+tc) , t ≥ −tc , (3)
where tc = n−1
2πb , which ensures that peaks of all ﬁlters occur
at t = 0.
Let x0
q(t) = xq(t), the ﬁltered signals yk
q (t), k = 1, . . . , L,
are recursively computed by
( yk
q (t) = xk−1
q
(t) ∗ hk(t)
xk
q(t) = xk−1
q
(t) − yk
q (t)
,
k = 1, . . . , L ,
(4)
with residual signal deﬁned as rq(t) = xL
q (t) to guarantee the
completeness of the input sequence.
After the Gammatone ﬁltering, the amplitude of the output
samples yk
q (t), k = 1, . . . , L, are ampliﬁed by a gain factor
Gk ≥ 1. The idea is to emphasize the presence of harmonics
of the fundamental frequency, which will lead to speech
intelligibility improvement, without introducing any noticeable
distortion to the speech signal. The reconstruction of the
voiced frame q ∈ Sv leads to the sample sequence
ˆxq(t) =
L
X
k=1
Gk yk
q (t) + rq(t) .
(5)
The ﬁnal step of the pGTFF0 method is the reconstruction
of the entire speech signal. For this purpose, the reconstructed
voiced frames in Sv and all the remaining frames in Su are
joined together keeping the original frames indices. Thus,
2Code available at http://staffwww.dcs.shef.ac.uk/people/N.Ma/
IEEE SIGNAL PROCESSING LETTERS
3
2
4
6
8
10
Gain
28.0
30.0
32.0
34.0
36.0
38.0
ESTOI [x10
−2
]
F1
F2
F3
F4
F5
1
2
3
4
5
Gain
37.8
38.2
38.6
39.0
ESTOI [x10
−2
]
F6
F7
F8
F9
F10
(a)
2
4
6
8
10
Gain
40.0
44.0
48.0
52.0
ESTOI [x10
−2
]
F1
F2
F3
F4
F5
1
2
3
4
5
Gain
51.6
52.0
52.4
52.8
ESTOI [x10
−2
]
F6
F7
F8
F9
F10
(b)
2
4
6
8
10
Gain
52.0
56.0
60.0
64.0
ESTOI [x10
−2
]
F1
F2
F3
F4
F5
1
2
3
4
5
Gain
64.4
64.8
65.2
65.6
ESTOI [x10
−2
]
F6
F7
F8
F9
F10
(c)
Fig. 2. ESTOI scores to attain the personalized gains empirically for speech
signals corrupted by SSN with SNR values: (a) -5 dB, (b) 0 dB and (c) 5 dB.
all frames are overlap and added to reconstruct the modiﬁed
version ˆx(t) of the target speech signal.
In this work, the gains are personalized to overcome the
external noise ﬁltering issue noticed in ASD individuals. The
values are empirically determined considering a training subset
of 72 speech signals of the TIMIT database deﬁned in [21].
The speech utterances are corrupted by Speech Shaped Noise
(SSN) from DEMAND [22] database with three SNR values:
-5dB, 0dB and 5dB.
Fig. 2 illustrates the ESTOI [18] objective intelligibility
scores for noisy speech signals. The gain conﬁguration starts
from the ﬁrst ﬁlter (F1), and its value is incremented until
ESTOI reaches its maximum value (highlighted point). This
gain is ﬁxed, and the process is repeated for the subsequent ﬁl-
ters. Observe that the same set of gains is attained for the three
SNR values. After the process, the personalized gains that lead
to the highest intelligibility ESTOI scores for the pGTFF0 are
deﬁned as Gk = {10, 10, 4.5, 3.5, 2.5, 2, 1.75, 1.75, 1.5, 1.25}.
This set turns the harmonic components of speech more
prominent when compared to the noisy signal. This effect may
reduce the impact of the acoustic noise, and consequently,
improve speech intelligibility.
IV. EXPERIMENTS AND DISCUSSION
This Section presents a listening test [23] conducted to
verify the perceptual differences between NT and ASD groups
under noisy conditions. Following, the proposed pGTFF0 and
competitive methods are compared using objective measures.
The ESTOI is here adopted to examine the predicted intelli-
gibility while PESQ [19] is used to evaluate the quality rates
of noisy speech signals.
TABLE I
PERCEPTUAL TESTS RESULTS [%] FOR NT AND ASD VOLUNTEERS.
NT
ASD
SNR (dB)
-5
0
5
Av.
-5
0
5
Av.
Male
UNP
52.0
67.5
81.8
67.1
27.3
55.8
71.4
51.5
SSFV
26.0
57.1
77.9
53.7
16.9
46.8
74.0
45.9
APESHARM
18.2
29.9
61.0
36.4
10.4
23.4
45.5
26.4
GTFF0
68.8
79.2
93.5
80.5
27.3
55.8
71.4
51.5
Female
UNP
54.6
68.2
83.8
68.9
51.5
54.6
72.7
59.6
SSFV
24.2
66.7
90.9
60.6
15.2
48.5
66.7
43.4
APESHARM
15.2
21.2
66.7
34.3
12.1
36.4
48.5
32.3
GTFF0
81.8
84.9
100
88.9
57.6
72.7
87.9
72.7
Overall
UNP
52.7
67.7
82.4
67.6
34.6
55.5
71.8
53.9
SSFV
25.5
60.0
81.8
55.8
16.4
47.3
71.8
45.2
APESHARM
17.3
27.3
62.7
35.8
10.9
27.3
46.4
28.2
GTFF0
72.7
80.9
95.5
83.0
56.4
71.8
92.7
73.6
A. Perceptual Evaluation
Three methods for intelligibility improvement are examined
in the perceptual tests: SSFV [12], APESHARM [14] and GTFF0
[16]. The test is conducted for 10 Brazilian volunteers (7 male
and 3 female) for the NT group (ages between 19-57) and ASD
(16-25), considering a scenario of phonetic balanced words3.
The SSN noise is adopted with SNRs of -5 dB, 0 dB and 5
dB. Ten words are applied for each of the 12 test conditions,
i.e., three SNR levels and three methods plus the unprocessed
(UNP) case. Participants are introduced to the task in a training
session with 4 words. The material is diotically presented using
a pair of Roland RH-200S headphones. Listeners hear each
word once in an arbitrary presentation order and are asked to
indicate the word in a sheet list.
The intelligibility results for each method are presented in
Table I. For all cases, GTFF0 [16] outperforms the competitive
approaches. Furthermore, female volunteers presented higher
intelligibility rates than male, with scores 8.4 p.p. and 21.2
p.p. higher for NT and ASD, respectively. The overall average
score of UNP and GTFF0 presented with NT listeners was
67.6% and 83.0%. Note that in the same cases ASD presented
lower scores (53.9% and 73.6%). Observe that this difference
between groups increases for lower SNR values. Since these
evident differences between NT and ASD, it can be concluded
that the subjective listening test might be an auxiliary tool to
detect ASD behavior.
B. Objective Evaluation: ESTOI and PESQ
Several evaluation experiments are conducted considering
the test subset attained from [21], which also provides the
V/UV segments. This set is composed of 192 speech utterances
from TIMIT database [24] sampled at 16 kHz, spoken by
24 speakers (16 male and 8 female). Each speech segment
has on average 3 seconds. The training and test subsets are
independent in terms of speakers and speech content. Four
additive background noises are selected: Babble from RSG-10
[25] database, Cafeteria and Helicopter from Freesound.org4
3The complete test database is available at lasp.ime.eb.br.
4Available at www.freesound.org.
IEEE SIGNAL PROCESSING LETTERS
4
Babble
Cafeteria
Helicopter
SSN
Noises
0.1
0.3
0.5
ESTOI
SNR = -10dB
Babble
Cafeteria
Helicopter
SSN
Noises
0.2
0.4
0.6
ESTOI
SNR = -5dB
Babble
Cafeteria
Helicopter
SSN
Noises
0.3
0.5
0.7
ESTOI
SNR = 0dB
Babble
Cafeteria
Helicopter
SSN
Noises
0.4
0.6
0.8
ESTOI
SNR = 5dB
Fig. 3. ESTOI boxplots for UNP noisy speech and competitive methods: SSFV, APESHARM, GTFF0 and pGTFF0.
TABLE II
PESQ OBJECTIVE SCORES FOR NOISY CONDITIONS AT DIFFERENT SNR VALUES
Babble
Cafeteria
Helicopter
SSN
SNR(dB)
-10
-5
0
5
Av.
-10
-5
0
5
Av.
-10
-5
0
5
Av.
-10
-5
0
5
Av.
Overall
UNP
0.67
1.58
1.95
2.38
1.65
1.06
1.64
2.06
2.47
1.72
1.54
1.86
2.30
2.72
2.10
1.35
1.51
1.86
2.31
1.76
1.81
SSFV
1.11
1.60
1.97
2.39
1.77
1.33
1.69
2.07
2.48
1.89
1.58
1.88
2.31
2.73
2.12
1.48
1.52
1.88
2.32
1.80
1.90
APESHARM
1.09
1.95
2.37
2.72
2.03
1.51
2.05
2.47
2.80
2.21
1.95
2.34
2.70
2.97
2.49
1.56
1.88
2.33
2.68
2.11
2.21
GTFF0
1.84
2.32
2.71
3.09
2.49
2.01
2.44
2.83
3.21
2.62
2.29
2.65
3.05
3.52
2.88
1.97
2.22
2.63
3.05
2.47
2.61
pGTFF0
2.16
2.59
2.98
3.33
2.77
2.33
2.72
3.08
3.44
2.89
2.57
2.94
3.32
3.64
3.12
2.32
2.50
2.91
3.30
2.76
2.88
and SSN [22]. Four SNR values are considered varying from
-10 dB up to 5 dB. In summary, considering 192 signals,
4 noises, 4 SNR values and 4 comparative methods, it is
conducted a total of 12,288 experiments.
Fig. 3 depicts the ESTOI intelligibility results achieved with
the proposed and baseline solutions. Note that the pGTFF0
attained the highest average scores in all of the 16 cases. For
instance, for the Babble noise with SNR=-10dB the pGTFF0
accomplished 8.8 p.p. on ESTOI score, compared to 7.1 p.p.,
–0.5 p.p. and –2.8 p.p. of GTFF0, SSFV and APESHARM
approaches, respectively. Moreover, for the Helicopter scenario
with SNR=-5dB UNP signals have the ESTOI incremented
by the proposed method from 0.40 to 0.54. That is, the
poor values became a regular intelligibility according [26][27].
Furthermore, for SNR=5dB, the pGTFF0 attains ESTOI scores
> 0.75, which is related to be a excellent intelligibility.
The APESHARM is outperformed by the other approaches
in all cases. Finally, the proposal also reaches the highest
ESTOI scores for the SSN noise, outperforming in 2.1p.p. the
competitive GTFF0. Since this noise is used in subjective tests,
this improvement also might reﬂect in perceptual experiments.
The predicted quality rates computed with PESQ [19] are
shown in Table II. The PESQ score is here computed from
30% of the most relevant voiced frames of noisy speech. These
frames are selected from those with the lowest signal-to-noise
ratio values. Note that pGTFF0 outperforms the competing
approaches for most of the noisy speech conditions in terms
of quality assessment. For the Babble scenario, this method
presents average PESQ of 2.77, followed by 2.49, 2.03 and
1.77 for GTFF0, APESHARM and SSFV, respectively. The
pGTFF0 is the only method that accomplished more than 1
point in the Mean Opinion Score (MOS) scale [28] in the
overall average results. These results reinforces the robustness
of the personalized solution in terms of noise effects attenua-
tion. Consequently, it will be reﬂected in improvements in both
acoustic intelligibility and quality, providing better acoustic
perception mainly in ASD individuals.
V. CONCLUSION
This Letter introduced a novel personalized method to
improve intelligibility for ASD situation under urban noisy
environment. In the proposed solution, gains of the Gamma-
tone ﬁltered samples are personalized in order to emphasize
the harmonic features of speech. Perceptual tests proved to
be a useful tool to assist in detection of ASD. Four acoustic
noises were considered to compose the noisy scenarios. Two
objective measures were used to evaluate the competitive so-
lutions. Results showed that pGTFF0 outperforms the baseline
methods in terms of intelligibility and quality scores for all
the acoustic noises. Further research includes the exploitation
of the identiﬁed impairment due to the composition of HIN
and urban acoustic noise solution to serve as an additional aid
to ASD diagnosis.
IEEE SIGNAL PROCESSING LETTERS
5
REFERENCES
[1] W. A. Dunlop, P. G. Enticott, and R. Rajan, “Speech discrimination
difﬁculties in high-functioning autism spectrum disorder are likely inde-
pendent of auditory hypersensitivity,” Frontiers in Human Neuroscience,
vol. 10, pp. 1–16, 2016.
[2] W. J. Park, K. B. Schauder, L. Bennedetto, and D. Tadin, “High internal
noise and poor external noise ﬁltering characterize perception in autism
spectrum disorder,” Scientiﬁc Reports, vol. 7, no. 1, pp. 1–12, 2017.
[3] J. F. Santos, N. Brosh, T. H. Falk, L. Zwaigenbaum, S. E. Bryson,
W. Roberts, I. M. Smith, P. Szatmari, and J. A. Brian, “Very early
detection of autism spectrum disorders based on acoustic analysis
of pre-verbal vocalizations of 18-month old toddlers,” in 2013 IEEE
International Conference on Acoustics, Speech and Signal Processing,
pp. 7567–7571, 2013.
[4] D. Bone, T. Chaspari, and S. Narayanan, Autism Imaging and Devices,
ch. Behavioral signal processing and autism: Learning from multimodal
behavioral signals, p. 26. CRC Press, 2017.
[5] A. Queiroz and R. Coelho, “Noisy speech bsed temporal decomposition
to improve fundamental frequency estimation,” IEEE/ACM Trans. Audio,
Speech, Lang. Process., vol. 30, pp. 2504–2513, 2022.
[6] A. B, V. Narayan, and J. Shukla, “Spasht: Semantic and pragmatic
speech features for automatic assessment of autism,” in ICASSP 2023 -
2023 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 1–5, 2023.
[7] M. Cooke and Y. Lu, “The contribuition of changes in f0 and spectral
tilt to increased intelligibility of speech produced in noise,” Speech
Communication, vol. 51, pp. 1253–1262, 2009.
[8] H. Hong, Z. Zhao, X. Wang, and Z. Tao, “Detection of dynamic
structures of speech fundamental frequency in tonal languages,” IEEE
Signal Process. Lett., vol. 17, no. 10, pp. 843–846, 2010.
[9] D. Ealey, H. Kelleher, , and D. Pearce, “Harmonic tunnelling: Tracking
nonstationary noises during speech,” in Proc. 7th Eur. Conf. Speech
Commun. Technol., pp. 437–440, 2001.
[10] L. Wang and F. Chen, “Factors affecting the intelligibility of low-pass
ﬁltered speech,” in Proc. Interspeech, pp. 563–566, 2017.
[11] K. Nathwani, M. Daniel, G. Richard, B. David, and V. Roussarie,
“Formant shifting for speech intelligibility improvement in car noise
environment,” IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pp. 5375–5379, 2016.
[12] K. Nathwani, G. Richard, B. David, P. Prablanc, and V. Roussarie,
“Speech intelligibility improvement in car noise environment by voice
transformation,” Speech Communication, vol. 91, pp. 17–27, 2017.
[13] E. Lombard, “Le signe de l’elevation de la voix,” Maladies Oreille,
Larynx, Nez, Pharynx, vol. 37, no. 25, pp. 101–119, 1911.
[14] S. Norholm, J. Jensen, and M. Christensen, “Enhancement and noise
statistics estimation for non-stationary voiced speech,” IEEE Trans-
actions on Audio, Speech, and Language Processing, vol. 24, no. 4,
pp. 645–658, 2016.
[15] P. Stoica, H. Li, and J. Li, “A new derivation of the apes ﬁlter,” IEEE
Signal Processing Letters, vol. 6, no. 8, pp. 205–206, 1999.
[16] A. Queiroz and R. Coelho, “F0-based gammatone ﬁltering for intelligi-
bility gain of acoustic noisy signals,” IEEE Signal Process. Lett., vol. 28,
pp. 1225–1229, 2021.
[17] L. Z˜ao and R. Coelho, “On the estimation of fundamental frequency
from nonstationary noisy speech signals based on hilbert-huang trans-
form,” IEEE Signal Process. Lett., vol. 25, no. 2, pp. 248–252, 2018.
[18] J. Jensen and C. H. Taal, “An algorithm for predicting the intelligibility
of speech masked by modulated noise maskers,” IEEE/ACM Trans.
Audio, Speech, Lang. Process., vol. 24, no. 11, pp. 2009–2022, 2016.
[19] A. Rix, J. Beerends, M. Hollier, and A. Hekstra, “Perceptual evaluation
of speech quality (pesq)-a new method for speech quality assessment
of telephone networks and codecs,” in Proc. IEEE Int. Conf. Acoust.,
Speech Signal Process., vol. 2, pp. 749–752, 2001.
[20] P. Johannesma, “The pre-response stimulus ensemble of neuron in the
cochlear nucleus,” in Proc. of the Symposium of Hearing Theory, pp. 58–
69, 6 1972.
[21] S. Gonzalez, “Pitch of the core timit database set,” 2014.
[22] J. Thiemann, N. Ito, and E. Vincent, “Demand: a collection of multi-
channel recordings of acoustic noise in diverse environments,” Proc.
Meetings Acoust., 2013.
[23] S. Ghimire, “Speech intelligibility measurement on the basis of itu-t
recommendation p.863,” 2012.
[24] J. Garofolo, L. Lamel, W. Fischer, J. Fiscus, D. Pallett, N. Dahlgren, and
V. Zue, “Timit acoustic-phonetic continuous speech corpus,” in Linguist.
Data Consortium, Philadelphia, PA, USA, 1993.
[25] H. J. Steeneken and F. W. Geurtsen, “Description of the rsg-10 noise-
database,” report IZF, vol. 3, 1988.
[26] “American national standard: Methods for calculation of the speech
intelligibility index,” in New York, MY, USA: Amer. Nat. Standards Inst.,
1997.
[27] B. Sauert and P. Vary, “Near end listening enhancement: Speech intel-
ligibility improvement in noisy environments,” in Proc. IEEE Int. Conf.
Acoust., Speech Signal Process., vol. 1, pp. 493–496, 2006.
[28] ITU-T, “Rec. p.10/g.100 (2017) vocabulary for performance, quality of
service and quality of experience,”
"
"Binarization of document images is a primary step in document analysis. Deep learning-based techniques are able to produce binarized versions of the images by learning context-dependent features. There have been many studies conducted that employ such methods. However, it is difficult to identify which among these models perform best when compared to one another because most of these models have never been trained and tested on a common dataset using the same evaluation protocol. Therefore, this work focuses on the evaluation of different deep learning-based methods under the same evaluation protocol. The results show that all four evaluation datasets have a different winner. Overall, DE-GAN ranks best across the four chosen DIBCO test datasets while metric-wise, the 2-Stage GAN outperforms the other models.","Binarization of document images is an important pre-processing step in the field of document analysis. It is the process that converts a color or grayscale image into an image whose pixels can have only two different values, usually black and white. Traditional image binarization techniques usually rely on histograms or local statistics. Deep learning techniques are able to generate binarized versions of the images by learning context-dependent features.","nanIn recent years, rather than relying on traditional image binarization techniques, many studies have been conducted that employ deep learning models to binarize document images. Although many approaches using deep learning for document binarization have been put forward, it is difficult to identify which among these models perform best when compared to one another. The root cause of this problem is the fact that most of these models have never been trained and tested on a common dataset using the same evaluation protocol.nannan","In this work, seven deep learning-based methods were evaluated in a fair evaluation where we fixed the data and augmentation used. We evaluated the methods using all ten available DIBCO datasets. We evaluated the methods using four metrics: (1) F-measure (FM), (2) pseudo F-measure (pFM), (3) peak signal to noise ratio (PSNR), and (4) distance reciprocal distortion (DRD).nan","Our evaluations show that the results are very diverse on the four different tested datasets and no clear winner could be established. Overall, the DE-GAN approach achieved the best rank averaged over all four different datasets followed by SauvolaNet. When we compare the metrics individually, then the 2-Stage GAN approach performed best followed by the DE-GAN. In the very different DIBCO2019 dataset, however, the SauvolaNet outperformed these methods.","In this paper, we thoroughly evaluated seven deep learning-based methods in a fair evaluation where we fixed the data and augmentation used. We evaluated the methods using all ten available DIBCO datasets. Therefore, we used six datasets for training and the remaining four datasets for testing. Our evaluations show that the results are very diverse on the four different tested datasets and no clear winner could be established. Overall, the DE-GAN approach achieved the best rank averaged over all four different datasets followed by SauvolaNet. When we compare the metrics individually, then the 2-Stage GAN approach performed best followed by the DE-GAN. In the very different DIBCO2019 dataset, however, the SauvolaNet outperformed these methods. For future work, we would like to evaluate the methods also with a different protocol.",A Fair Evaluation of Various Deep Learning-Based Document Image Binarization Approaches,"Richin Sukesh, Mathias Seuret, Anguelos Nicolaou, Martin Mayr, Vincent Christlein","A Fair Evaluation of Various Deep
Learning-based Document Image Binarization
Approaches
Richin Sukesh[0000−0002−1845−820X], Mathias Seuret[0000−0001−9153−1031],
Anguelos Nicolaou[0000−0003−3818−8718], Martin Mayr[0000−0002−3706−285X], and
Vincent Christlein[0000−0003−0455−3799]
Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg, Erlangen, Germany
richin.sukesh@fau.de
Abstract. Binarization of document images is an important pre-
processing step in the field of document analysis. Traditional image
binarization techniques usually rely on histograms or local statistics to
identify a valid threshold to differentiate between different aspects of the
image. Deep learning techniques are able to generate binarized versions
of the images by learning context-dependent features that are less error-
prone to degradation typically occurring in document images. In recent
years, many deep learning-based methods have been developed for docu-
ment binarization. But which one to choose? There have been no studies
that compare these methods rigorously. Therefore, this work focuses on
the evaluation of different deep learning-based methods under the same
evaluation protocol. We evaluate them on different Document Image
Binarization Contest (DIBCO) datasets and obtain very heterogeneous
results. We show that the DE-GAN model was able to perform better
compared to other models when evaluated on the DIBCO2013 dataset
while DP-LinkNet performed best on the DIBCO2017 dataset. The 2-
StageGAN performed best on the DIBCO2018 dataset while SauvolaNet
outperformed the others on the DIBCO2019 challenge. Finally, we make
the code, all models and evaluation publicly available1 to ensure repro-
ducibility and simplify future binarization evaluations.
Keywords: binarization · deep learning · evaluation
1
Introduction
Image binarization is a process that converts a color or grayscale image into
an image whose pixels can have only two different values, usually black and
white. In the domain of document image analysis, binarization typically consists
in separating the text (foreground) from its support (background), e. g., the
paper. While it became less popular for text recognition, it remains an important
pre-processing step in many other tasks, such as writer identification [4,5], word
1 https://github.com/RichSu95/Document Binarization Collection
arXiv:2401.11831v1  [cs.CV]  22 Jan 2024
2
R. Sukesh et al.
spotting or optical character recognition (OCR) [10]. In traditional global bina-
rization, the grayscale intensity frequency histogram of an image is analyzed and
an appropriate threshold is set, e. g., Otsu’s thresholding [17]. Alternatively, bi-
narization is applied locally using statistics such as mean and standard deviation
like the popular Sauvola method [27]. However, these methods have problems
with ink bleed-through artifacts and other artifacts such as stains, blurring, faint
characters and noise [15]. An error that may be generated through incorrect bina-
rization may propagate forward and lead to performance reduction in subsequent
tasks. Document binarization also acts as a means to filter out these undesirable
features. A thorough overview of binarization techniques, datasets, and metrics
is given in a survey by Tensmeyer and Martinez [31].
In recent years, rather than relying on traditional image binarization tech-
niques, many studies have been conducted that employ deep learning models to
binarize document images. The advent of deep learning has brought a multitude
of changes to the domain of computer vision and image processing. Convolutional
neural networks (CNNs) identify features automatically by learning from training
data. The image features are discovered at multiple layers and are learned gradu-
ally from lower-levels to higher-levels. This multi-layered architecture performs a
series of convolutions on the input image. A training process is implemented to
adjust the parameters of the network to achieve the desired output.
In the past decade, there have been immense progress in the field of binariza-
tion of contemporary and historical documents using deep learning techniques.
Although many approaches using deep learning for document binarization have
been put forward, it is difficult to identify which among these models perform best
when compared to one another. The root cause of this problem is the fact that
most of these models have never been trained and tested on a common dataset
using the same evaluation protocol. This paper aims to resolve this disparity by
training and testing some well-known binarization models [28,10,13,8,29,2,32] on
common datasets from the well-known Document Image Binarization Contests
(DIBCO) [6,18,19,20,21,16,23,24,22,25]. While we evaluated the results of the
models using four metrics, we omitted investigations on the relationship between
result quality and processing time as Lins et al. did [11]. Our evaluations draw a
very heterogeneous picture. All four evaluation datasets have a different winner.
Overall, DE-GAN ranks best across the four chosen DIBCO test datasets while
metric-wise, the 2-Stage GAN outperforms the other models.
The following section 2 of the paper provides a brief overview on the network
architectures and methodologies used in the different binarization models that
would be compared against one another. Section 3 gives a detailed description on
the various datasets, validation metrics and on how all the models were trained.
Section 4 shows the results of evaluating all models on the various test datasets
and provides a brief discussion on the outcome of the experiments.
A Fair Evaluation of Deep Learning-based Binarization Methods
3
2
Overview of Evaluated Binarization Methods
2.1
Document Enhancement Generative Adversarial Network
The work presented by Souibgui et al. [28] models the document binarization
problem as an image-to-image translation task. The Document Enhancement
Generative Adversarial Network (DE-GAN) model basically consists of a generator
and a discriminator. The generator follows a U-Net architecture [26] and its
objective is to generate a clean image given the original degraded image. The
goal of the discriminator is then to determine if the image shown is a fake image
generated by the generator or the original binarized ground truth. An adversarial
loss function is employed for training the model [28]:
LGAN(ϕG, ϕD) = EIW ,IGT log[DϕD(IW , IGT )]
+ EIW ,IGT log[1 − DϕD(IW , GϕG(IW ))],
(1)
where GϕG and DϕD are the generator and discriminator functions respectively,
IW is the degraded image and IGT is the ground truth. After a few epochs, the
network is able to generate images similar to the ground truth. To maintain a
good text quality and to improve training speed an additional log loss function
is added. The objective is that the text output from the generator is identical to
the ground truth text [28]:
Llog(ϕG) = EIGT ,IW [−(IGT log(GϕG(IW )) + ((1 − IGT ) log(1 − GϕG(IW )))].
(2)
The overall loss of the network is denoted as [28]:
Lnet(ϕG, ϕD) = min
ϕG max
ϕD LGAN(ϕG, ϕD) + λLlog(ϕG),
(3)
where LGAN is the adversarial loss function used to train the cGAN and λ is
a hyper-parameter that is set to 500 for document binarization. The generator
follows an encoder-decoder structure. The encoder performs down-sampling of
the given input up to a certain layer and the decoder then up-samples the encoder
output. The discriminator used is a simple Fully-Connected Network (FCN) with
6 convolutional layers. To train the DE-GAN model, overlapped patches of size
256 × 256 pixel are obtained from the degraded images and fed as input to the
generator.
2.2
SauvolaNet
Inspired by the traditional Sauvola thresholding algorithm [27], the work by Li
et al. [10] presents a deep learning approach to learn the Sauvola parameters,
called the “SauvolaNet”. The network aims to making the model computationally
efficient. The model also comprises of an attention mechanism that aims to
estimate the required Sauvola window sizes for each pixel location. One main
4
R. Sukesh et al.
drawback of the traditional Sauvola thresholding approach is that the algo-
rithm achieves its highest performance only when the right hyperparameters
are manually tuned for each input image (window size, estimated level of docu-
ment degradation and dynamic range of input image intensity). SauvolaNet uses
three modules, the Multi-Window Sauvola (MWS), Pixelwise Window Attention
(PWA), and Adaptive Sauvola Threshold (AST) to learn an auxiliary threshold
estimation function.
The MWS module takes an image as input and uses the Sauvola algorithm
to estimate the local thresholds for different window sizes. The PSA module also
takes the same image as input to estimate the window sizes for each pixel location.
The AST module then predicts the final threshold for each pixel location by
fusing the thresholds of different windows from the MWS and weights from the
PWA modules. The SauvolaNet function is modelled as [10]:
T = gSauvolaNet(D),
(4)
where, T is the output, gSauvolaNet is the auxiliary threshold estimation
function and D is the input image. The PWA uses instance normalization instead
of batch normalization in order to avoid overfitting when training with a small
dataset. When training the SauvolaNet, the input image D is normalized to
values in the range (0,1) and a modified hinge loss was developed [10]:
loss[i, j] = max(1 − α · (D[i, j] − T[i, j]) · B[i, j], 0),
(5)
where B is the binarization ground truth with values -1 for foreground and +1
for the background. i and j are indices that specify the location of a pixel. α is a
parameter to control the margin of the decision boundary and only the pixels
close to the decision boundary are used in gradient-backpropagation.
2.3
Two-Stage GAN
The work presented by Suh et al. proposes a two-stage color document image
binarization deep learning architecture using generative adversarial neural net-
works (GANs) [29]. The GAN architecture generally consists of two networks,
i.e., the generator and the discriminator. For this model, the EfficientNet [30]
was used as the generator on account of its efficiency in the domain of image
classification. In the case of the discriminator, the discriminator network from
the PatchGAN [9] was implemented.
The first part of the network consists of four color independent generators
that are trained with the red, green, blue, and gray channels in order to generate
an enhanced image by removing background color information. The resulting
channel images and corresponding ground truths first concatenated and then fed
to the discriminator network. The binarization in the first stage is performed
using local predictions in small patches. In order to cater to regions with larger
backgrounds, the second stage of the network performs global binarization with
the resized original input image and local binarization using the first stage output.
Except for the input image channels, the structure for the generators in the
A Fair Evaluation of Deep Learning-based Binarization Methods
5
second stage is identical to that of the first stage. During training, the images
are divided into patches of 256 × 256 pixels resolution without scaling. When
training GANs in general, it is common to observe an instability in loss function
convergence [29]. To solve this issue, the Wasserstein GAN with penalty was
used which implements the Wasserstein K-distance as the loss function. Further,
instead of the typically used L1 loss, pixel-wise binary cross-entropy is defined as
the additional loss term for the generator update.
2.4
Robin U-Net model
The implementation by Mikhail Masyagin [13] presents the Robust Documentation
Binarization (ROBIN) tool. ROBIN makes use of a simple U-Net model [26] to
perform document binarization. The U-Net model was originally developed for
the purpose of semantic segmentation of medical images. The U-Net architecture
can be described as an encoder-decoder network. The input image is first fed
into the encoder network, where multiple convolution blocks are applied followed
by a maxpool downsampling layer. The idea here is to encode the input image
into feature representations at multiple levels. The output from the encoder is
then sent to the decoder where the activation map undergoes upsampling or
deconvolution. Skip connections are also introduced between the encoder and
decoder structure such that the deep and shallow features can be combined.
When training the model, the input images are split into patches of 128×128 px
resolution. The learning rate was set to 0.0001 with the Adam optimizer. The
training is trained using the dice coefficient loss and run for 250 epochs with an
early stopping criteria.
2.5
DP-LinkNet
The DP-LinkNet is a segmentation model introduced by Xiong et al.. It makes
use of the D-LinkNet [33] and LinkNet [3] models with a pre-trained encoder as
the backbone.
The model consists of: 1) an encoder, 2) a hybrid dilated convolution module,
3) a spatial pyramid pooling (SPP) module, and 4) a decoder [32]. Firstly, the
input image is fed to the encoder where the text stroke features are extracted.
The series of convolutions and down-sampling occurring at the encoder causes a
reduction in the resolution of the obtained feature map. To counter this effect,
dilated convolutions are introduced into the model. Dilated convolutions help
in exponentially increasing the size of the receptive field without affecting the
spatial resolution. An issue that still persists here is the fact that the dilated
convolution module may still find it difficult to identify objects of different sizes
with a fixed-sized field-of-view. To counter this effect, the spatial pyramid pooling
is employed. This helps to present the input feature maps at different scales.
Lastly, the decoder performs transposed convolution. Skip connections between
the decoder and encoder structure are present to combine the shallow-level and
high-level features, helping to compensate any loss encountered by convolution
and pooling operations. When training the model, the binary cross entropy and
6
R. Sukesh et al.
dice coefficient losses are used. The input images were split into patches of size
128×128 px. The adam optimizer was set with an initial learning rate of 2×10−4.
The model was trained for 500 epochs with an early stopping criteria to avoid
overfitting.
2.6
Selectional Auto-Encoder
The work presented by Calvo-Zaragoza et al. [2] uses an auto-encoder network
topology to perform an image-to-image processing task. Such a task results in
higher computational efficiency since all pixels in the input image are processed
at the same time. Generally, an auto-encoder network is trained to learn the
identity function. However, in the selectional auto-encoder (SAE), the network
is trained to learn a selectional map over a w × h image, preserving the input
shape. The activation of each pixel depends on whether the pixel belongs to
the foreground or the background. When training the SAE, the images along
with their corresponding ground-truth (binarized image) are fed as input to
the network. Auto-encoders are feed-forward networks and generally consist of
two sections, i.e., the encoder and decoder. The encoder learns to extract the
latent representation given an input image, downsampling the image until an
intermediate representation is achieved. The output from the encoder is then
upsampled and reconstructed to the original input image dimensions by the
hidden layers of the decoder. The last layer consists of a set of neurons and a
sigmoid activation layer which then gives an output prediction between the range
of 0 and 1.
Since the binarized output image should consist of pixel values being 0 or 1
and not in between, a thresholding process is implemented to decide whether the
certain pixel belongs to the background or foreground. The encoder and decoder
both consisted of 5 layers each and the sampling operators were fixed at 2 × 2.
Network weights were initialized using Xavier initialization [7]. Optimization is
handled with stochastic gradient descent and a mini-batch size of 10. The initial
learning rate is set to 0.001 and the network is trained for 200 epochs with an
early stopping criteria kept in place.
2.7
DeepOtsu
The work presented by He et al. [8] proposes an iterative deep learning approach
to obtain binarized images called the DeepOtsu model. However, unlike the
aforementioned methods in this section, the deep learning network in this case
aims to remove artifacts and generate a non-degraded version of the input image.
The degraded input image x is modeled as:
x = xu + e,
(6)
where xu is the latent uniform image and e is the degradation. The aim of
the deep learning network is to ultimately obtain xu.
A Fair Evaluation of Deep Learning-based Binarization Methods
7
The network was trained with images split into patches of size 256 × 256. The
patches are first fed to the CNN model and the obtained output is then compared
to the ground truth, which in this case should be representative of the uniform,
clean version of the input image. To obtain this ground truth, the degraded input
image is compared to the already available binarized images from the dataset.
Then, the ground truth image is computed as the average pixel value with the
same label within the image patch. Once the non-degraded, uniform version of the
input image is obtained, the binarized version of the image can be easily obtained
using Otsu thresholding [17]. The basic U-Net model [26] is used for learning the
degradation. The down-sampling path of the network consisted of 5 convolutional
layers with a 3 × 3 kernel size, followed by a leaky-ReLU activation [12] and 2 × 2
max pooling. The batch size was set to 8 and the learning rate set to 10−4.
3
Materials and Methods
3.1
Datasets
All models mentioned in the previous section are trained and tested on document
images from the DIBCO dataset. To keep the comparison between the models
fair and precise, the training set and validation set remain the same for all
models. The training set consists of the DIBCO2009, DIBCO2010, DIBCO2011,
DIBCO2012, DIBCO2014, and DIBCO2016 datasets. The models are evaluated
on DIBCO2013, DIBCO2017, DIBCO2018, and DIBCO2019 datasets. The four
test sets were chosen based on the unique properties present in the three sets.
DIBCO2013 consists of both handwritten and printed documents. The images
from DIBCO2017 had more textual content in them. The DIBCO2018 dataset
consisted of images of textual content present towards the borders or corners
of the papers and higher intensity of bleed-through artifacts. The DIBCO2019
dataset had large variations in the types of images. Note that we used only
track A since track B, containing text content on papyri, are not present in any
training data which lead to rather poor learning-based results. Evaluations based
on these four datasets give an idea of how well the models are able to generalize
on different types of unseen images. Figure 1 shows some samples of images that
belong to the DIBCO datasets used for validating the models.
3.2
Metrics
Our evaluation of the various models is based on the standard evaluation metrics
used in the DIBCO challenges: (1) F-measure (FM), (2) pseudo F-measure (pFM),
(3) peak signal to noise ratio (PSNR), and (4) distance reciprocal distortion
(DRD). The FM and pFM reach their best value at 1 and worst at 0 (Eqs. (7)
and (8)). PSNR describes how close the binarized and ground truth images are
(Eq. (9)). The higher the PSNR, the better is the binarized result. The DRD is
based on the reciprocal of distance, matching well to subjective evaluation by
human visual perception (Eq. (10)).
8
R. Sukesh et al.
(a) DIBCO2013
(b) DIBCO2017
(c) DIBCO2018
(d) DIBCO2019
Fig. 1: Image examples from the different DIBCO datasets used for testing the
models.
FM = 2 × Recall × Precision
Recall + Precision
,
(7)
where, Recall =
T P
T P +F N and Precision =
T P
T P +F P . TP, FP and FN denote
true positive, false positive and false negative values respectively.
pFM = 2 × pRecall × pPrecision
pRecall + pPrecision
,
(8)
where, pRecall and pPrecision, respectively the pseudo-recall and the pseudo-
precision, are metrics weighted based on the distance to the contours of the
foreground in the ground truth. For the pseudo-recall, pixels around strokes have
weights starting from 1, and reaching 0 at a distance corresponding to the stroke’s
width, and pixels inside of the strokes have a weight of 1. For the pseudo-precision,
pixels outside strokes but not further than the stroke’s thickness have a weight
of 1, and inside the stroke their weight increase toward the center, where they
reach a value of 2.
A Fair Evaluation of Deep Learning-based Binarization Methods
9
PSNR = log10
 C2
MSE

,
(9)
where, MSE =
Pm
x=1
Pn
y=1(L(x,y)−L′(x,y))2
mn
. The terms m and n denote the
dimensions of the image. C denotes the difference present between the text and
background.
DRD =
P
k DRDk
NUBN
,
(10)
where DRDk is the distortion of the kth flipped pixel and NUBN is the
number of non-uniform 8 × 8 blocks in the ground truth image.
3.3
Training
All models are trained on the DIBCO datasets as mentioned in the previous
sections. Based on the configuration of the models, the degraded images along
with the accompanying ground truths are first split into patches of size 256 × 256
pixel or 128 × 128 pixel resolutions. The patches are further augmented by
random horizontal flipping, vertical flipping and rotations. The number of epochs
for training each model is set based on the recommendation of the authors for
each model, along with an early stopping criteria to monitor any possibility of
overfitting the models. If the validation loss of the model does not show significant
changes for 15 consecutive epochs, the training would stop and the model would
be saved. Certain pre-processing and post-processing operations on the images
exclusive to specific models have also been implemented. Such an example is
the application of Otsu’s thresholding on the output of the DeepOtsu method.
The hyper-parameters for the models are optimized using the python library
“optuna” [1].
4
Evaluation
The results of testing each model on the different test DIBCO datasets are
as shown in the following tables. Table 1a shows the results of validating the
models on the DIBCO2013 dataset. The DIBCO2013 dataset contains images
that have a good representation of the training data, without any major artifacts
or degradation present. All methods display comparable performance with the
DE-GAN performing best. For reference, we also show the DIBCO winners of
the respective challenge. Note that the participants of 2017 and later potentially
used more data for training.
Table 1b shows the results of validating the models on the DIBCO2017 dataset.
Here, the performance of the models start to fluctuate more when compared to
Table 1a. This might be due to the fact that the DIBCO2017 dataset contains
more images that have more densely packed textual content. The DP-LinkNet
model outperforms the other models in terms of PSNR, FM and DRD whereas
10
R. Sukesh et al.
(a) Input image
(b) Ground Truth
(c) DE-GAN
(d) Robin (U-Net)
(e) DeepOtsu
(f) 2-Stage GAN
(g) DP-LinkNet
(h) SAE
(i) SauvolaNet
Fig. 2: Illustration of some results for an image from DIBCO-2017. Pixels in cyan
are false positives. The few pixels in orange are false negatives. Pixels in white
or black match the ground truth.
the DE-GAN model has a higher performance in terms of pFM. However, it
can be observed that the DRD value for DE-GAN is quite high, indicating
that the resulting binarized images have higher rate of distortions. This may
be attributed to the training process of the DE-GAN model, which may have
introduced distortions to the generated images. Qualitative results for a randomly
chosen sample from DIBCO2017 can be seen in Fig. 2.
The results for the DIBCO2018 dataset is shown in Table 1c. The winner is
clearly the 2-Stage GAN approach, outperforming all other methods in each metric.
A Fair Evaluation of Deep Learning-based Binarization Methods
11
Table 1: Results of different image binarization methods on the (a) DIBCO2013,
(b) DIBCO2017, (c) DIBCO2018, and (d) DIBCO2019 datasets. Note that the
winners of the respective DIBCO2017, DIBCO2018 and DIBCO2019 challenge
had more data available.
Model
PSNR↑ FM↑ pFM↑ DRD↓
DE-GAN
24.08 97.68 98.09 1.11
Robin (U-Net)
22.81
95.07 95.82
1.99
DeepOtsu
21.19
93.46 95.99
2.25
2-Stage GAN
22.60
95.75 96.40
1.46
DP-LinkNet
23.63
96.49 97.24
1.10
SAE
20.88
93.35 94.44
3.17
SauvolaNet
23.41
96.31 97.53
1.28
Winner [21,24]
20.68
92.12 94.19
3.10
(a) DIBCO2013
PSNR↑ FM↑ pFM↑ DRD↓
18.31
96.23 98.10
3.22
19.99
92.05 94.06
2.23
18.02
89.01 91.84
3.50
20.89
95.56 96.54
1.33
22.84 97.92 97.94
0.77
16.73
87.59 90.41
5.60
19.40
93.33 96.26
2.20
18.28
91.04 92.86
3.40
(b) DIBCO2017
Model
PSNR↑ FM↑ pFM↑ DRD↓
DE-GAN
15.98
76.21 83.29
8.01
Robin (U-Net)
15.78
78.80 81.11 12.20
DeepOtsu
12.72
66.60 68.83 42.52
2-Stage GAN
19.93 92.40 94.90
2.67
DP-LinkNet
15.73
78.56 80.70 13.72
SAE
14.48
73.45 76.33 15.45
SauvolaNet
16.03
77.94 81.92 10.41
Winner [22,25]
19.11
88.34 90.24
4.92
(c) DIBCO2018
PSNR↑ FM↑ pFM↑ DRD↓
15.12
70.86 70.69
6.23
14.39
65.55 65.34
7.36
14.82
70.81 70.91
7.59
12.87
65.09 65.72 12.71
14.20
61.84 61.55
7.58
12.50
62.17 61.90 13.43
15.83 72.04 71.59
5.55
14.48
72.88 72.15 16.24
(d) DIBCO2019
For the pFM and the DRD metrics, the DE-GAN ranks second. Interestingly,
the DP-LinkNet struggles with black page borders, see Fig. 3b. While it wins for
the 2017 dataset that does not have borders, it performed poorly on images that
have borders that are present in the DIBCO2018 dataset, cf. Fig. 1c.
While SauvolaNet ranks behind these two methods in the DIBCO2018 chal-
lenge, it outperforms both methods on the DIBCO2019 dataset, see Table 1d.
The 2-Stage GAN, which performs very well for the 2013 to 2018 datasets had
some difficulties to deal with the squared paper (check paper, quadrille paper) of
the 2019 dataset, which can be observed in Fig. 3d. When we average all metrics
for all different evaluated datasets, see Table 2a, the 2-Stage GAN seems to be
on average the most suitable binarization method appearing to be consistent in
terms of performance. Interestingly, computing the average rank over all metrics,
i. e., the average over all 16 ranks for each method, it falls behind DE-GAN and
SauvolaNet, cf. Table 2b.
12
R. Sukesh et al.
(a)
(b)
(c)
(d)
Fig. 3: Qualitative examples of failure modes: (b) shows that DP-LinkNet binarizes
the large black borders present in images of DIBCO2018 to white; (d) shows that
the 2-Stage GAN struggles with the squared paper given in images of DIBCO2019,
and additionally produces halo-artifacts.
Table 2: Average over (a) all metrics and (b) all ranks. Runtimes evaluated using
an NVIDIA RTX 2060 GPU (12 GB RAM). Note that DeepOtsu and 2-Stage
GAN were limited by the available memory.
Model
PSNR↑ FM↑ pFM↑ DRD↓ img/sec↑
DE-GAN
18.37
85.25 87.54
4.64
0.67
Robin (U-Net)
18.24
82.87 84.08
5.95
1.99
DeepOtsu
16.69
79.97 81.89 13.96
0.01
2-Stage GAN
19.07 87.20 88.39
4.54
0.01
DP-LinkNet
19.10 83.70 84.36
5.79
0.49
SAE
16.15
79.14 80.77
9.41
0.68
SauvolaNet
18.67
84.91 86.83
4.86
0.37
(a) Average metrics
Avg. rank↓
2.44
4.19
5.50
3.25
3.38
6.63
2.63
(b) Average ranks
We also evaluated the runtime, reported as throughput, i. e., images per second
in the last column of Table 2a. The best throughput has the Robin binarization
method. Note, however that we evaluated the methods on a small-sized GPU
(NVIDIA RTX 2060) with 12 GB GPU-RAM. Unfortunately, this affected the
throughput of DeepOtsu and 2-Stage GAN because multiple images of the DIBCO
2013 dataset contain very large images, e. g., image sizes of 4161 × 1049.
A Fair Evaluation of Deep Learning-based Binarization Methods
13
5
Conclusion
In this paper, we thoroughly evaluated seven deep learning-based methods in a
fair evaluation where we fixed the data and augmentation used. We evaluated the
methods using all ten available DIBCO datasets. Therefore, we used six datasets
for training and the remaining four datasets for testing. Our evaluations show
that the results are very diverse on the four different tested datasets and no clear
winner could be established. Overall, the DE-GAN approach achieved the best
rank averaged over all four different datasets followed by SauvolaNet. When we
compare the metrics individually, then the 2-Stage GAN approach performed
best followed by the DE-GAN. In the very different DIBCO2019 dataset, however,
the SauvolaNet outperformed these methods.
For future work, we would like to evaluate the methods also with a different
protocol. In particular, we would like to simulate the DIBCO scenario of each
year’s challenge to be comparable with the single DIBCO papers, i. e., training
with the datasets 2015–2016, then evaluating with 2017, adding 2017 to the
training set, re-train and evaluate on 2018, and so on. The use of additional
augmentation techniques as well as additional training datasets is also worth
investigating and might have huge impact on the overall performance of the
binarization methods. Furthermore, pixel-based evaluation is not optimal [31].
While the pFM metric incorporates the distance to the script contour, it might
be worth investigating indirect measures, such as OCR/HTR accuracy or purely
skeleton-based metrics [14]. From a practical point of view, the inference time is
also worth investigating. This has been mainly studied in the competitions on
time-quality document image binarization
References
1. Akiba, T., Sano, S., Yanase, T., Ohta, T., Koyama, M.: Optuna: A next-generation
hyperparameter optimization framework. In: 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. p. 2623–2631. KDD ’19,
Association for Computing Machinery, New York, NY, USA (2019) 9
2. Calvo-Zaragoza, J., Gallego, A.J.: A selectional auto-encoder approach for document
image binarization. Pattern Recognition 86, 37–47 (2019) 2, 6
3. Chaurasia, A., Culurciello, E.: Linknet: Exploiting encoder representations for
efficient semantic segmentation. 2017 IEEE Visual Communications and Image
Processing (VCIP) pp. 1–4 (2017) 5
4. Christlein, V., Bernecker, D., H¨onig, F., Maier, A., Angelopoulou, E.: Writer
identification using gmm supervectors and exemplar-svms. Pattern Recognition 63,
258–267 (2017) 1
5. Christlein, V., Gropp, M., Fiel, S., Maier, A.: Unsupervised feature learning for
writer identification and writer retrieval. In: 2017 14th IAPR International Confer-
ence on Document Analysis and Recognition (ICDAR). vol. 01, pp. 991–997 (Nov
2017) 1
6. Gatos, B., Ntirogiannis, K., Pratikakis, I.: Icdar 2009 document image binarization
contest (dibco 2009). In: 2009 10th International Conference on Document Analysis
and Recognition. pp. 1375–1382 (2009) 2
14
R. Sukesh et al.
7. Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward
neural networks. In: Teh, Y.W., Titterington, M. (eds.) Proceedings of the Thir-
teenth International Conference on Artificial Intelligence and Statistics. Proceedings
of Machine Learning Research, vol. 9, pp. 249–256. PMLR, Chia Laguna Resort,
Sardinia, Italy (13–15 May 2010) 6
8. He, S., Schomaker, L.: Deepotsu: Document enhancement and binarization using
iterative deep learning. Pattern Recognition 91, 379–390 (Jul 2019) 2, 6
9. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with con-
ditional adversarial networks. 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) pp. 5967–5976 (2017) 4
10. Li, D., Wu, Y., Zhou, Y.: Sauvolanet: Learning adaptive sauvola network for de-
graded document binarization. In: The 16th International Conference on Document
Analysis and Recognition (ICDAR). p. 538–553 (2021) 2, 3, 4
11. Lins, R.D., Bernardino, R.B., Smith, E.B., Kavallieratou, E.: Icdar 2021 competition
on time-quality document image binarization. In: International Conference on
Document Analysis and Recognition. pp. 708–722. Springer (2021) 2
12. Maas, A.L.: Rectifier nonlinearities improve neural network acoustic models (2013)
7
13. Masyagin,
M.:
Robust
document
image
binarization,
https://github.com/
masyagin1998/robin, last accessed: April 1st, 2022 2, 5
14. Monteiro Silva, A.C., Hirata, N.S.T., Jiang, X.: Skeletal similarity based structural
performance evaluation for document binarization. In: 2020 17th International
Conference on Frontiers in Handwriting Recognition (ICFHR). pp. 37–42 (Sep
2020) 13
15. Mustafa, W.A., Kader, M.M.M.A.: Binarization of document images: A compre-
hensive review. Journal of Physics: Conference Series 1019, 012023 (jun 2018)
2
16. Ntirogiannis, K., Gatos, B., Pratikakis, I.: Icfhr2014 competition on handwritten
document image binarization (h-dibco 2014). In: 2014 14th International Conference
on Frontiers in Handwriting Recognition. pp. 809–813 (2014) 2
17. Otsu, N.: A Threshold Selection Method from Gray-Level Histograms. IEEE Trans-
actions on Systems, Man, and Cybernetics 9(1), 62–66 (jan 1979) 2, 7
18. Pratikakis, I., Gatos, B., Ntirogiannis, K.: H-dibco 2010 - handwritten document
image binarization competition. In: 2010 12th International Conference on Frontiers
in Handwriting Recognition. pp. 727–732 (2010) 2
19. Pratikakis, I., Gatos, B., Ntirogiannis, K.: Icdar 2011 document image binarization
contest (dibco 2011). In: 2011 International Conference on Document Analysis and
Recognition. pp. 1506–1510 (2011) 2
20. Pratikakis, I., Gatos, B., Ntirogiannis, K.: Icfhr 2012 competition on handwritten
document image binarization (h-dibco 2012). In: 2012 International Conference on
Frontiers in Handwriting Recognition. pp. 817–822 (2012) 2
21. Pratikakis, I., Gatos, B., Ntirogiannis, K.: Icdar 2013 document image binarization
contest (dibco 2013). In: 2013 12th International Conference on Document Analysis
and Recognition. pp. 1471–1476 (2013) 2, 11
22. Pratikakis, I., Zagori, K., Kaddas, P., Gatos, B.: Icfhr 2018 competition on hand-
written document image binarization (h-dibco 2018). In: 2018 16th International
Conference on Frontiers in Handwriting Recognition (ICFHR). pp. 489–493 (2018)
2, 11
23. Pratikakis, I., Zagoris, K., Barlas, G., Gatos, B.: Icfhr2016 handwritten document
image binarization contest (h-dibco 2016). In: 2016 15th International Conference
on Frontiers in Handwriting Recognition (ICFHR). pp. 619–623 (2016) 2
A Fair Evaluation of Deep Learning-based Binarization Methods
15
24. Pratikakis, I., Zagoris, K., Barlas, G., Gatos, B.: Icdar2017 competition on document
image binarization (dibco 2017). In: 2017 14th IAPR International Conference on
Document Analysis and Recognition (ICDAR). vol. 01, pp. 1395–1403 (2017) 2, 11
25. Pratikakis, I., Zagoris, K., Karagiannis, X., Tsochatzidis, L., Mondal, T., Marthot-
Santaniello, I.: Icdar 2019 competition on document image binarization (dibco
2019). In: 2019 International Conference on Document Analysis and Recognition
(ICDAR). pp. 1547–1556 (2019) 2, 11
26. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical
image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F. (eds.)
Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015.
pp. 234–241. Springer International Publishing, Cham (2015) 3, 5, 7
27. Sauvola, J., Pietik¨ainen, M.: Adaptive document image binarization. Pattern Recog-
nition 33(2), 225–236 (2000) 2, 3
28. Souibgui, M.A., Kessentini, Y.: De-gan: A conditional generative adversarial network
for document enhancement. IEEE Transactions on Pattern Analysis and Machine
Intelligence 44(3), 1180–1191 (March 2022) 2, 3
29. Suh, S., Kim, J., Lukowicz, P., Lee, Y.O.: Two-stage generative adversarial networks
for document image binarization with color noise and background removal. CoRR
abs/2010.10103 (2020), https://arxiv.org/abs/2010.10103 2, 4, 5
30. Tan, M., Le, Q.: EfficientNet: Rethinking model scaling for convolutional neural
networks. In: Chaudhuri, K., Salakhutdinov, R. (eds.) Proceedings of the 36th
International Conference on Machine Learning. Proceedings of Machine Learning
Research, vol. 97, pp. 6105–6114. PMLR (09–15 Jun 2019) 4
31. Tensmeyer, C., Martinez, T.: Historical Document Image Binarization: A Review.
SN Computer Science 1, 173 (May 2020) 2, 13
32. Xiong, W., Jia, X., Yang, D., Ai, M., et al.: Dp-linknet: A convolutional network
for historical document image binarization. KSII Transactions on Internet and
Information Systems 15(5), 1778–1797 (2021) 2, 5
33. Zhou, L., Zhang, C., Wu, M.: D-linknet: Linknet with pretrained encoder and
dilated convolution for high resolution satellite imagery road extraction. In: 2018
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
(CVPRW). pp. 192–1924 (2018) 5
"
"This paper presents a theoretical perspective on Centered Kernel Alignment (CKA) in Knowledge Distillation (KD). The authors derive a novel relationship between CKA and Maximum Mean Discrepancy (MMD), showing that CKA is an upper bound of MMD with a constant term. To exploit this insight, they propose Relation-Centered Kernel Alignment (RCKA), a practical framework for CKA that significantly improves previous distillation methods. Additionally, they introduce Patch-based CKA (PCKA) for object detection, allowing knowledge transfer in instance-level tasks. Extensive experiments on CIFAR-100, ImageNet-1k, and MS-COCO demonstrate the effectiveness of the proposed approaches, achieving state-of-the-art performance in almost all quantitative comparisons.","Knowledge Distillation (KD) is a powerful technique for knowledge transfer in deep learning models, enabling smaller and more efficient models to learn from larger and more accurate teacher models. Traditional KD methods primarily focus on aligning the output distributions or logits between the teacher and student models. However, these methods often fail to capture the essential characteristics of the teacher's representations, hindering effective knowledge transfer. Centered Kernel Alignment (CKA) has emerged as a valuable metric for measuring representation similarity between neural networks, showing promise in improving KD performance. This paper aims to theoretically analyze CKA's role in KD and propose novel approaches that leverage CKA for effective knowledge transfer.","nannanThe authors provide a comprehensive review of the existing KD methods, categorizing them into two groups: logit-based and feature-based distillation. They highlight the advantages and limitations of previous approaches and discuss the potential of CKA in addressing the challenges associated with KD. Furthermore, they acknowledge the computational cost of CKA and emphasize the need for efficient and scalable alternatives.nan","nanThe authors propose a novel Relation-Centered Kernel Alignment (RCKA) framework for effective knowledge transfer in KD. RCKA is based on a theoretical insight into the relationship between CKA and MMD. They derive a formula showing that CKA can be upper-bounded by MMD with a constant term. This relationship allows them to reformulate the objective of CKA minimization into an optimization problem that minimizes the upper bound of MMD. To make RCKA practical, they introduce several innovations, including a scaling factor to balance the contributions of different loss terms and a strategy for dynamically customizing the application of CKA based on the characteristics of each task.","The authors conduct extensive experiments on CIFAR-100, ImageNet-1k, and MS-COCO datasets to evaluate the proposed approaches. They compare their methods with a range of existing KD algorithms and demonstrate significant improvements in accuracy and performance across various tasks. Notably, the proposed methods achieve state-of-the-art results on several challenging benchmarks, including image classification and object detection. Furthermore, ablation studies and visualizations provide insights into the effectiveness of the proposed components and the characteristics of the learned representations.","The authors conclude that their theoretical analysis of CKA and the proposed Relation-Centered Kernel Alignment (RCKA) framework significantly improve knowledge transfer in deep learning models. They highlight the effectiveness of the proposed approaches in both image classification and object detection tasks. Additionally, they emphasize the scalability and expansion of their methods, making them applicable to a wide range of KD scenarios. The authors suggest future research directions in exploring the relationship between all similarity metric-based distillation methods and investigating the reasons behind the performance boost observed when averaging on the channel dimension with patches.",Rethinking Centered Kernel Alignment in Knowledge Distillation,"Zikai Zhou, Yunhang Shen, Shitong Shao, Huanran Chen, Linrui Gong, Shaohui Lin","Rethinking Centered Kernel Alignment in Knowledge Distillation
Zikai Zhou1 , Yunhang Shen2 , Shitong Shao3 , Huanran Chen4 , Linrui Gong5 , Shaohui Lin1∗
{choukai003,shenyunhang01,shaohuilin007}@gmail.com,
shaoshitong@seu.edu.cn,
huanranchen@bit.edu.cn,
ttrr2021@sjtu.edu.cn
Abstract
Knowledge distillation has emerged as a highly
effective method for bridging the representa-
tion discrepancy between large-scale models and
lightweight models. Prevalent approaches involve
leveraging appropriate metrics to minimize the di-
vergence or distance between the knowledge ex-
tracted from the teacher model and the knowl-
edge learned by the student model.
Centered
Kernel Alignment (CKA) is widely used to mea-
sure representation similarity and has been applied
in several knowledge distillation methods. How-
ever, these methods are complex and fail to un-
cover the essence of CKA, thus not answering
the question of how to use CKA to achieve sim-
ple and effective distillation properly. This paper
first provides a theoretical perspective to illustrate
the effectiveness of CKA, which decouples CKA
to the upper bound of Maximum Mean Discrep-
ancy (MMD) and a constant term. Drawing from
this, we propose a novel Relation-Centered Ker-
nel Alignment (RCKA) framework, which practi-
cally establishes a connection between CKA and
MMD. Furthermore, we dynamically customize the
application of CKA based on the characteristics
of each task, with less computational source yet
comparable performance than the previous meth-
ods. The extensive experiments on the CIFAR-100,
ImageNet-1k, and MS-COCO demonstrate that our
method achieves state-of-the-art performance on
almost all teacher-student pairs for image classifi-
cation and object detection, validating the effective-
ness of our approaches.
1
Introduction
Tremendous efforts have been made in compressing large-
scale models into lightweight models. Representative meth-
ods include network pruning [Frankle and Carbin, 2019],
model quantization [Wu et al., 2016], neural architec-
ture search [Wan et al., 2020] and knowledge distilla-
tion (KD) [Hinton et al., 2015]. Among them, KD has re-
cently emerged as one of the most flourishing topics due
to its effectiveness [Liu et al., 2021b; Huang et al., 2022;
Gong et al., 2023; Shao et al., 2023a] and wide applica-
tions [Chong et al., 2022; Chen et al., 2023; Shao et al.,
2023b].
Particularly, the core idea of KD is to transfer
the acquired representations from a large-scale and high-
performing model to a lightweight model by distilling the
learning representations in a compact form, achieving pre-
cision and reliable knowledge transfer. Out of the consen-
sus of researchers, there are two mainstream approaches to
distilling knowledge from the teacher to the student.
The
first approach is the logit-based distillation, which aims to
minimize the probabilistic prediction (response) scores be-
tween the teacher and student by leveraging appropriate met-
rics [Zhao et al., 2022; Hinton et al., 2015]. The other is
feature-based distillation, which investigates the knowledge
within intermediate representations to further boost the dis-
tillation performance [Yang et al., 2022b; Liu et al., 2021a;
Chen et al., 2021; Ahn et al., 2019]. Among them, the design
of metrics is essential in knowledge transfer and has been
attractive to scholars. Kornblith at al [2019] proposes the
Centered Kernel Alignment (CKA) for the quantitative under-
standing of representations between neural networks. CKA
not only focuses on model predictions but also emphasizes
high-order feature representations within the models, provid-
ing a comprehensive and enriched knowledge transfer.
Recent studies [Qiu et al., 2022; Saha et al., 2022] intro-
duce CKA to quantitatively narrow the gap of learned repre-
sentations between the teacher model and the student model.
Undeniably, these methods have achieved significant success.
However, their designs are excessively complex and need a
large amount of computational resources, making it challeng-
ing to achieve fine-grained knowledge transfer and leading
to low scalability. Moreover, these methods fail to uncover
the essence of CKA, lacking an in-depth analysis of CKA in
knowledge distillation. The reason why CKA is effective has
not been explored. Therefore, we focus on the theoretical
analysis of CKA and rethink a more reasonable architecture
design that ensures simplicity and effectiveness while gener-
alizing well across various tasks.
In this paper, we provide a novel perspective to illustrate
the effectiveness of CKA, where CKA is regarded as the up-
per bound of Maximum Mean Discrepancy (MMD) with a
constant term, specifically. Drawing from this, we propose
a Relation-Centered Kernel Alignment (RCKA) framework,
which practically establishes a connection between CKA and
arXiv:2401.11824v1  [cs.CV]  22 Jan 2024
Figure 1: The overall framework of the proposed Relation-based Centered Kernel Alignment (RCKA). We first transform the feature map
from the shape of (B, C, HW) into (B, CHW) and then compute the CKA similarity of feature maps between the teacher and the student.
Besides, we compute the inter-class and intra-class CKA similarity of logits between teacher and student. Here, N refers to the number of
samples, and P refers to the corresponding probability of class to which this sample belongs.
MMD. Besides, we dynamically customize the application
of CKA on instance-level tasks, and introduce Patch-based
Centered Kernel Alignment (PCKA), with less computational
source yet competitive performance when compared to pre-
vious methods. Our method is directly applied not only to
logit-based distillation but also to feature-based distillation,
which exhibits superior scalability and expansion. We uti-
lize CKA to compute high-order representation information
both between and within categories, which better motivates
the alleviation of the performance gap between the teacher
and student.
To validate the effectiveness of our approaches, we con-
duct extensive experiments on image classification (CIFAR-
100 [Krizhevsky and Hinton, 2009] and ImageNet-1k [Rus-
sakovsky et al., 2015]), and object detection (MS-COCO [Lin
et al., 2014]) tasks. As a result, our methods achieve state-of-
the-art (SOTA) performance in almost all quantitative com-
parison experiments with fair comparison.
Moreover, fol-
lowing our processing architecture, the performance of the
previous distillation methods is further boosted in the object
detection task.
Our contribution can be summarized as follows:
• We rethink CKA in knowledge distillation from a novel
perspective, providing a theoretical reason for why CKA
is effective in knowledge distillation.
• We propose a Relation Centered Kernel Alignment
(RCKA) framework to construct the relationship be-
tween CKA and MMD, with less computational source
yet comparable performance than previous methods,
which verifies our theoretical analysis correctly.
• We further dynamically customize the application of
CKA for instance-level tasks and propose a Patch-based
Centered Kernel Alignment (PCKA) architecture for
knowledge distillation in object detection, which further
boosts the performance of previous distillation methods.
• We conduct plenty of ablation studies to verify the ef-
fectiveness of our method, which achieves SOTA perfor-
mance on a range of vision tasks. Besides, we visualize
the characteristic information of CKA and discover new
patterns in it.
2
Related Work
Vanilla Knowledge Distillation [Hinton et al., 2015] proposes
aligning the output distributions of classifiers between the
teacher and student by minimizing the KL-divergence, dur-
ing training the emphasis on negative logits can be fine-tuned
through a temperature coefficient, which serves as a form of
normalization during the training process of a smaller stu-
dent network. Tremendous efforts [Tung and Mori, 2019;
Huang et al., 2022; Qiu et al., 2022; Zagoruyko and Ko-
modakis, 2016a; Park et al., 2019] have been made on how
to design a good metric to align the distribution between the
teacher and student.
Design an suitable alignment method for KD can start
from two typical types: Drawing on representations, numer-
ous methods have made significant strides by aligning the in-
termediate features [Zagoruyko and Komodakis, 2016a], the
samples’ correlation matrices [Tung and Mori, 2019], and the
output logits between the teacher and student [Huang et al.,
2022]. From a mathematical standpoint, some measure the-
ories are introduced to illustrate the similarity between the
teacher and student, such as mutual information [Ahn et al.,
2019]. Among these, Centered Kernel Alignment (CKA) is a
valuable function for measuring similarity. It simultaneously
considers various properties during similarity measures, such
as invariance to orthogonal transformations. While the ef-
fectiveness of CKA in KD has been demonstrated in some
works [Qiu et al., 2022; Saha et al., 2022], the essence of
CKA has not been thoroughly explored, and the unavoid-
able additional computational costs also limit its application
prospect.
In this paper, we will revisit CKA in KD and provide a
novel theoretical perspective to prove its effectiveness and an-
alyze how it functions across various distillation settings.
3
Methodolgy
In this section, we first revisit the paradigm of knowledge
distillation and then introduce the formula of Centered Ker-
nel Alignment (CKA). Specifically, we derive the formula of
the relationship between CKA and Maximum Mean Discrep-
ancy (MMD), where CKA can be decoupled as the upper
bound of MMD with a constant term. In light of the above
deduction, we outline the methodology of our paper. We ap-
ply the proposed methods in image classification and object
detection, dynamically customizing CKA for each task.
3.1
The Paradigm of Knowledge Distillation
The existing KD methods can be categorized into two groups.
Particularly, the logits-based KD methods narrow the gap be-
tween the teacher and student models by aligning the soft
targets between them, which is formulated as following loss
term:
Llogits = Dlogits(Ts(σ(zs; τ)), Tt(σ(zt; τ))),
(1)
where zs and zt are the logits from students and teachers, re-
spectively. And σ(·) is the softmax function that produces
the category probabilities from the logits, and τ is a non-
negative temperature hyper-parameter to scale the smooth-
ness of the predictive distribution.
Specifically, we have
σi(z; τ) = softmax(exp(zi/τ)). Dlogits is a loss function to
capture the discrepancy distributions, e.g. Kullback-Leibler
divergence. And Ts and Tt denote the transformation func-
tions in students and teachers, respectively, which usually
refer to the identity mapping in Vanilla KD [Hinton et al.,
2015].
Similarly, the feature-based KD methods, which aim to
mimic the feature representations between teachers and stu-
dents, are also represented as a loss item:
Lfeat = Dfeat(Ts(Fs), Tt(Ft)),
(2)
where Fs and Ft denote feature maps from students and
teachers, respectively. Transformation modules Ts and Tt
align the dimensions of Fs and Ft. Dfeat computes the dis-
tance between two feature maps, such as ℓ1- or ℓ2 norm.
Therefore, the KD methods can be represented by a generic
paradigm. The final loss is the weighted sum of the cross-
entropy loss Lce, the logits distillation loss, and the feature
distillation loss:
L = Lce + αLlogits + βLfeat,
(3)
where α and β are hyper-parameters controlling the trade-off
between these three losses.
3.2
Distilling with the Upper Bound
Centered Kernel Alignment (CKA) has been proposed as a
robust way to measure representational similarity between
neural networks. We first prove that CKA measures the co-
sine similarity of the gram matrix between teachers and stu-
dents.
Theorem 1 (Proof in Appendix C.1). Let X and Y be N ×P
matrices. The CKA similarity ∥Y ⊤X∥2
F is equivalent to the
cosine similarity of XX⊤ and Y Y ⊤, which denote the gram
matrix of X and Y , respectively. In other words,
SCKA(X, Y ) =
∥Y ⊤X∥2
F
∥XT X∥F ∥Y T Y ∥F
=
vec(XX⊤)⊤vec(Y Y ⊤)
∥vec(XX⊤)∥2∥vec(Y Y ⊤)∥2
,
where vec operator represents reshaping the matrix to a vec-
tor.
We then derive the formula of the relationship between
CKA and MMD, where CKA can be regarded as the upper
bound of MMD with a constant term.
Theorem 2 (Proof in Appendix C.2). Maximizing CKA sim-
ilarity is equivalent to minimizing the upper bound of MMD
distance:
∥Y ⊤X∥2
F
∥XT X∥F ∥Y T Y ∥F
= −NEi,j[⟨xi, xj⟩ − ⟨yi, yj⟩]2 + 2
≤ −N (Ei,j[⟨xi, xj⟩] − Ei,j[⟨yi, yj⟩])2 + 2,
where the inequality is given by Jesen’s inequality.
According to Jesen’s inequality, CKA can be decoupled as
the upper bound of MMD with a constant term. The first
term corresponds to minimizing the upper bound of MMD
distance with the RKHS kernel. In contrast, the latter constant
term acts as a weight regularizer, enhancing the influence of
MMD, where it promotes the similarity between features of
the same batch, not only instances in the same class but also
in different classes. On one hand, optimizing the upper bound
of MMD, which has additional stronger constraints, allows it
to converge to the optimal solution more quickly and stably.
On the other hand, the latter term serves as a weight scaling
mechanism, effectively avoiding the challenges of optimiza-
tion caused by excessively small MMD values, which result
in small gradients.
According to the deduction, we successfully transformed
our optimization objective from maximizing CKA to mini-
mizing the upper bound of MMD, which makes our method
more intuitive and concise. Building upon these findings, we
propose our methods, which are more effective than previous
methods.
3.3
Relation-based Centered Kernel Alignment
As illustrated in Fig. 1, we propose a Relation Centered
Kernel Alignment (RCKA) framework in image classifica-
tion. In this framework, we leverage CKA as a loss func-
tion to ensure that the centered similarity matrix is distilled
rather than forcing the student to mimic the teacher‘s simi-
larity matrix with a different scale. This is very important
Figure 2: The overall framework of PCKA. We dynamically customize the framework of proposed method based on the characteristics of
object detection. In this framework, we first patch the featuremap of the teacher and student with the patchsize (PH, PW ), then transform
the featuremap in order to get the gram matrix between each patch. Finally, we calculate the loss LP CKA, and get average from dimension
C. Here, B, C, H, W refer to the batchsize, channels, height and width of the featuremap, respectively. NPH , NPW denote the number of
patches cutting along the height and width, respectively.
because a model‘s discriminative capability is dependent on
the distribution of its features rather than its scale, which is
inconsequential for class separation [Nguyen et al., 2020;
Orhan and Pitkow, 2017].
Assume we have a large-scale teacher model t and a
lightweight student model s. The activation map from layer
l of the teacher is denoted as F (l)
t
∈ Rb×c×h×w, whereas
the activation map of layer l′ of the student is denoted as
F (l′)
s
∈ Rb×c′×h′×w′. c, h, and w denote the channel, height,
and width of the teacher, whereas c′, h′ and w′ denote that of
the student. The mini-batch size is denoted by b. The logits
of the teacher and student are denoted as zt ∈ RN×P and
zs ∈ RN×P ′, where N and P (or P ′) refer to the number
of samples and the corresponding probability for which class
this sample belongs to. Therefore, the formula of our method,
similar to Eqn. 3, is represented as:
LRCKA = LCE + αLFCKA
(4)
+ β(LIntra−LCKA + LInter−LCKA)
(5)
where α and β are hyper-parameters controlling the trade-
off between the features loss LFCKA and logits loss
(LIntra−LCKA + LInter−LCKA).
The LFCKA, LIntra−LCKA and LInter−LCKA are repre-
sented as:
LFCKA = SCKA(T(Ft), T(Fs)),
(6)
LIntra−LCKA = SCKA(zt, zs),
(7)
LInter−LCKA = SCKA(zT
t , zT
s ),
(8)
where T in Eqn. 6 refers to the transformation module
Rb×c×h×w → Rb×chw.
Compared with the previous methods, our method has su-
perior scalability and expansion and can be directly applied to
both feature and logits distillation. We calculate the gram ma-
trix to collect high-order inter-class and intra-class represen-
tations, encouraging the student to learn more useful knowl-
edge. Also, we provide the relationship between CKA and
MMD in Appendix C.2 to better demonstrate the theoretical
support of our method.
Because the value of CKA ranges from [0, 1], at the begin-
ning of the training process, LCE plays a more important role
than all CKA losses to drive the optimization of the student,
which helps the student avoid matching extremely complex
representations.
3.4
Patch-based Centered Kernel Alignment
In this subsection, we further adapt the proposed RCKA to
instance-level tasks such as object detection. However, di-
rectly applying RCKA to instance-level tasks may deteriorate
performance, as the above tasks are usually trained with a
small size of mini-batches (e.g. 2 or 4 per GPU), causing
the failure of the gram matrix to collect enough knowledge.
Besides, increasing the mini-batch size requires a significant
amount of computational resources, making it infeasible in
practice. Thus, we dynamically customize our RCKA method
for object detection.
Recent works [Shu et al., 2021; Heo et al., 2019] find that
distilling the representations of intermediate layers is more
effective than distilling the logits in object detection. There-
fore, we adjust our method to only target intermediate lay-
ers. Still, we follow our core idea in the classification task,
which calculates the similarities between different instances
by using CKA. So, we divide the image feature maps into
several patches and compute the similarities between differ-
ent patches.
Our redesigned method is illustrated in Fig. 2.
In this
Table 1: Results on the CIFAR-100 test set. “Same” and “Different” in the first row refer to whether the model architecture is the same for
teachers and students. Combined distillation type means that this method transfers the knowledge both on the features and logits. Our methods
surpass almost all algorithms with the same distillation type. “RN”, “WRN”, “SN”, and “MN” denote ResNet, Wide ResNet, ShuffleNet, and
MobileNet, respectively.
Architecture
Same
Different
Distillation
Type
Teacher
RN-110
RN-110
WRN-40-2
WRN-40-2
RN-32×4
VGG-13
WRN-40-2
RN-32×4
VGG-13
74.31
74.31
75.61
75.61
79.42
74.64
75.61
79.42
74.64
Student
RN-20
RN-32
WRN-40-1
WRN-16-2
RN-8×4
VGG-8
SN–V1
SN–V1
MN–V2
69.06
71.14
71.98
73.26
72.50
70.36
70.50
70.50
64.60
Feature-based
FitNet [2014]
68.99
71.06
72.24
73.58
73.50
71.02
73.73
73.59
64.14
ATKD [2016a]
70.22
70.55
72.77
74.08
73.44
71.43
73.32
72.73
59.40
SPKD [2019]
70.04
72.69
72.43
73.83
72.94
72.68
74.52
73.48
66.30
CCKD [2019]
69.48
71.48
72.21
73.56
72.97
70.71
71.38
71.14
64.86
RKD [2019]
69.25
71.82
72.22
73.35
71.90
71.48
72.21
72.28
64.52
VID [2019]
70.16
70.38
73.30
74.11
73.09
71.23
73.61
73.38
65.56
CRD [2020]
71.46
73.48
74.14
75.48
75.51
73.94
76.05
75.11
69.73
OFD [2019]
-
73.23
74.33
75.24
74.95
73.95
75.85
75.98
69.48
ReviewKD [2021]
-
71.89
75.09
76.12
75.63
74.84
77.14
76.93
70.37
ICKD-C [2021a]
71.91
74.11
74.63
75.57
75.48
73.88
75.19
74.34
67.55
DPK [2022]
72.44
74.89
75.27
76.42
-
74.96
74.43
76.00
68.63
FCKA(ours)
71.49
73.64
74.70
75.53
74.93
74.35
75.98
75.67
68.97
Logit-based
KD [2015]
70.67
73.08
73.54
74.92
73.33
72.98
74.83
74.07
67.37
DKD [2022]
-
74.11
74.81
76.24
76.32
74.68
76.70
76.45
69.71
DIST [2022]
69.94
73.55
74.42
75.29
75.79
73.74
75.23
75.23
68.48
IKL-KD [2023]
-
74.26
74.98
76.45
76.59
74.88
77.19
76.64
70.40
NKD [2023]
71.26
73.79
75.23
76.37
76.35
74.86
76.59
76.90
70.22
LCKA(ours)
70.87
73.64
74.63
75.78
75.12
74.35
76.12
76.43
69.37
Combined
SRRL [2021]
71.51
73.80
74.75
75.96
75.92
74.40
76.61
75.66
69.14
RCKA(ours)
72.26
74.31
75.34
76.51
76.11
74.97
77.21
76.97
70.12
Table 2: Results on the ImageNet validation set. We use ResNet-34 and ResNet-50 released by Torchvision [Marcel and Rodriguez, 2010] as
our teacher’s pre-training weight.
Architecture
Accuracy
Feature-based
Logit-based
Combined
Teacher
Student
Teacher Student OFD CRD ReviewKD ICKD-C MGD [2022b]
KD
RKD DKD DIST SRRL Ours
ResNet-34
ResNet-18
Top-1
73.31
69.76
71.08 71.17
71.61
72.19
71.80
70.66 70.34 71.70 72.07 71.73 72.34
Top-5
91.42
89.08
90.07 90.13
90.51
90.72
90.40
89.88 90.37 90.41 90.42 90.60 90.68
ResNet-50 MobileNet-V1 Top-1
76.16
70.13
71.25 71.37
72.56
-
72.59
70.68
-
72.05 73.24 72.49 72.79
Top-5
92.86
89.49
90.34 90.41
91.00
-
90.74
90.30
-
91.05 91.12 90.92 91.01
framework, we first patch the feature maps of the teacher and
student with a patch size of (PH, PW ), then transform the fea-
ture maps to get the gram matrix between each patch. Finally,
we calculate the loss LPCKA and get the average from dimen-
sion C. Here, NPH and NPW denote the number of patches
cutting along the height and width, respectively. Therefore,
the Patch-based CKA loss is represented as:
LPCKA = γSCKA(N s
PH · N s
PW , N t
PH · N t
PW ),
(9)
where N s
PH · N s
PW and N t
PH, N t
PW are denoted as the num-
ber of the student patches and the teacher patches, respec-
tively. Usually, N s
PH · N s
PW = N t
PH, N t
PW . γ refers to the
loss weight factor.
4
Experiments
We conduct extensive experiments on image classification
and object detection benchmarks. The image classification
datasets include CIFAR-100 [Krizhevsky and Hinton, 2009]
and ImageNet-1k [Russakovsky et al., 2015] and the ob-
ject detection dataset includes MS-COCO [Lin et al., 2014].
Moreover, we present various ablations and analyses for the
proposed methods. More details about these datasets are in
Appendix A. We apply a batch size of 128 and an initial
learning rate of 0.1 for the SGD optimizer on CIFAR-100.
And we follow the settings in [Huang et al., 2022] for the
ResNet34-ResNet18 pair and the ResNet50-MobileNet pair
on ImageNet-1k. The settings of other classification and de-
tection tasks are in Appendix B. Our code will be publicly
available for reproducibility.
4.1
Image Classification
Classification on CIFAR-100.
We compare state-of-the-
art (SOTA) feature-based and logit-based distillation algo-
rithms on 9 student-teacher pairs. Among them, 6 pairs have
the same structure for teachers and students, and the rest of
them have different architectures. The results are presented
in Tab. 1. Our proposed method outperforms all other al-
gorithms on 4 student-teacher pairs and achieves compara-
ble performance on the rest of them, meanwhile requiring
extremely less computational resources and time consump-
tion than the SOTA methods DPK [Qiu et al., 2022] and Re-
viewKD [Chen et al., 2021]. The comparisons of computa-
tional cost are in Appendix D.
Table 3: Results on the COCO validation set (T→S refers to the distillation from T to S). Here, the content in brackets to the right of “Ours”
refers to the methods applied in the distillation process. In addition, CM RCNN-X101 stands for Cascade Mask RCNN-X101.
T→S
CM RCNN-X101 [2017]→Faster RCNN-R50 [2015]
RetinaNet-X101→RetinaNet-R50 [2017]
T→S
FCOS-R101→FCOS-R50 [2019]
Type
Two-stage detectors
One-stage detectors
Type
Anchor-free detectors
Method
AP
AP50
AP75
APS
APM
APL
AP
AP50
AP75
APS
APM
APL
Method
AP
AP50
AP75
APS
APM
APL
Teacher
45.6
64.1
49.7
26.2
49.6
60.0
41.0
60.9
44.0
23.9
45.2
54.0
Teacher
40.8
60.0
44.0
24.2
44.3
52.4
Student
38.4
59.0
42.0
21.5
42.1
50.3
37.4
56.7
39.6
20.0
40.7
49.7
Student
38.5
57.7
41.0
21.9
42.8
48.6
KD [2015]
39.7
61.2
43.0
23.2
43.3
51.7
37.2
56.5
39.3
20.4
40.4
49.5
KD [2015]
39.9
58.4
42.8
23.6
44.0
51.1
COFD [2019]
38.9
60.1
42.6
21.8
42.7
50.7
37.8
58.3
41.1
21.6
41.2
48.3
FitNet [2014]
39.9
58.6
43.1
23.1
43.4
52.2
FKD [2021]
41.5
62.2
45.1
23.5
45.0
55.3
39.6
58.8
42.1
22.7
43.3
52.5
GID [2021]
42.0
60.4
45.5
25.6
45.8
54.2
DIST [2022]
40.4
61.7
43.8
23.9
44.6
52.6
39.8
59.5
42.5
22.0
43.7
53.0
FRS [2021]
40.9
60.3
43.6
25.7
45.2
51.2
DIST+mimic [2022]
41.8
62.4
45.6
23.4
46.1
55.0
40.1
59.4
43.0
23.2
44.0
53.6
FGD [2022a]
42.1
-
-
27.0
46.0
54.6
Ours
41.4
62.1
45.2
23.5
45.6
54.9
40.3
59.9
43.0
23.3
44.2
54.9
Ours
39.8
59.0
42.4
22.2
43.6
52.5
Ours + mimic
42.4
63.3
46.1
24.3
46.7
56.1
40.7
60.4
43.4
23.9
44.7
55.1
Ours + mimic
40.7
60.5
43.1
23.4
44.8
53.1
Table 4: Results on the COCO validation set (T→S refers to the
distillation from T to S). Here, the content in brackets to the right
of “Ours” refers to the methods applied in the distillation process.
In addition, “Inside GT Box” means we use the GT boxes with the
same stride on the FPN layers as the feature imitation regions. “Main
Region” means we imitate the features within the main distillation
region.
T→S
GFL-R101→GFL-R50 [2020]
Method
AP
AP50
AP75
APS
APM
APL
Teacher
44.9
63.1
49.0
28.0
49.1
57.2
Student
40.2
58.4
43.3
23.3
44.0
52.2
FT [2014]
40.7
58.6
44.0
23.7
44.4
53.2
Inside GT Box
40.7
58.6
44.2
23.1
44.5
53.5
DeFeat
40.8
58.6
44.2
24.3
44.6
53.7
Main Region
41.1
58.7
44.4
24.1
44.6
53.6
FGFI [2019]
41.1
58.8
44.8
23.3
45.4
53.1
FGD [2022a]
41.3
58.8
44.8
24.5
45.6
53.0
GID [2021]
41.5
59.6
45.2
24.3
45.7
53.6
SKD [2022]
42.3
60.2
45.9
24.4
46.7
55.6
Our
42.8
61.2
46.3
24.8
47.1
55.4
Classification on ImageNet-1k.
We also conduct exper-
iments on the large-scale ImageNet to evaluate our meth-
ods. Our RCKA achieves comparable results with other algo-
rithms, even outperforms them, as shown in Tab. 2. We find
that with the increasing of categories and instances, it is more
challenging for the student to mimic the high-order distribu-
tion of the teacher. Moreover, in Appendix 8, we We explore
the feature distillation for ViT-based models on ImageNet-1k.
It is noted that our method outperforms other methods, which
means that our method has good scalability and good perfor-
mance.
4.2
Object Detection
Detection on MS-COCO.
Comparison experiments are
run on three kinds of different detectors, i.e., tow-stage de-
tectors, one-stage detectors, and anchor-free detectors. As
shown in Tab. 3, PCKA outperforms the precious meth-
ods almost on all three kinds of metrics, by aligning the
high-order patch-wise presentations. We believe that align-
ing feature maps of the student and teacher in low-order
could also improve the performance of PCKA, driven by
mimicking low-order representations in the early stage and
then learning high-order and complex representations gradu-
ally. Thus, we follow [Huang et al., 2022] by adding aux-
iliary mimic loss, i.e., translating the student feature maps
from the teacher feature map by a convolution layer and su-
pervising them utilizing LMSE, to the detection distillation
task. Ultimately, we conclude from Tab. 3 that PCKA-based
mimic loss achieves the best performance on Cascade RCNN-
X101-Cascade RCNN-R50 and RetinaNet-X101-RetinaNet-
R50 pairs. We also conduct experiments on the other four
architectures, as shown in Tab. 4 and Tab. 9. These results
further validate the effectiveness of our proposed method.
4.3
Ablations and Visualizations
We conduct ablation studies in three aspects: (a) the effect of
hyper-parameters. (b) effectiveness of the proposed modules.
(c) unexplored phenomenon during training.
Ablation studies on hyperparameters.
As shown in
Tab. 12, Tab. 11 and Tab. 13 in Appendix, we conduct the
ablation studies on the size of mini-batch, loss scaling fac-
tor γ on LP CKA and the number of intermediate layers for
distilling. We find the local optima values are the mini-batch
size 12, loss scaling factor 10, and 3 layers of distillation.
The upper bound of MMD.
In Theorem 2, we derive the
relationship between CKA and MMD, where CKA is the up-
per bound of MMD with a constant term. To validate this,
We conduct the experiment, which is shown in Tab. 6, we no-
tice that CKA, which is the upper bound of MMD, has addi-
tional stronger constraints. Because of this, CKA converges
to the optimal solution more quickly and stably, compared
with MMD.
The dimension to average.
In PCKA framework, we cut
the activations of the teacher and student in the shape of
(C, NPH · NPW , B · PH · PW ). We also carry out the experi-
ments of averaging on different dimensions, shown in Tab. 5.
We find averaging on channel dimension is the optima.
Patch distillation.
We explore the effectiveness of cutting
activations into patches. As shown in Tab. 7, several standard
distillation methods [Hinton et al., 2015; Zagoruyko and Ko-
modakis, 2016a; Huang et al., 2022] all perform well with
patch cutting, validating the effectiveness of cutting patches.
With the smaller representation distribution in patches, it is
easier to align the teacher and student. Thus, the proposed
PCKA architecture amazingly boosts the previous methods.
Visualize the CKA value.
We present some visualizations
to show that our method does bridge the teacher-student gap
in logit-level. In particular, we visualize the logit similarity
for 6 teacher-student pairs in Appendix E. We find that our
method significantly improves the logit-similarity.
Table 5: The ablation study on the COCO validation set (T→S refers
to the distillation from T to S). Here, we explore which dimension
we should choose to get better results. ”Mix-up” means the 1st dis-
tilling layer uses Batch avg. method, the 2nd distilling layer uses
Spatial avg. method and the final distilling layer uses Channel avg.
method
T→S
RetinaNet-X101→RetinaNet-R50
Method
AP
AP50
AP75
APS
APM
APL
Teacher
41.0
60.9
44.0
23.9
45.2
54.0
Student
37.4
56.7
39.6
20.0
40.7
49.7
Batch avg.
38.5
57.9
40.8
20.7
41.5
52.5
Spatial avg.
39.3
58.7
41.9
21.4
41.3
50.9
Mix-up avg.
38.2
58.1
40.4
21.3
41.9
50.9
Channel avg.
40.3
59.9
43.0
23.3
44.2
54.9
Visualize the training process.
We further visualize the
training process of different detectors and the patch effect on
the RetinaNet-X101-RetinaNet-R50 pair. The results are in
Tab. 4 and Tab. 3 in the Appendix.
Table 6: Experiments on the upper bound of MMD. We derive the
formula that CKA is the upper bound of MMD with a constant term.
From these experiments, we can prove that optimizing the upper
bound of MMD can better improve the performance, compared with
MMD.
T→S
RetinaNet-X101→RetinaNet-R50
Method
AP
AP50
AP75
APS
APM
APL
Teacher
41.0
60.9
44.0
23.9
45.2
54.0
Student
37.4
56.7
39.6
20.0
40.7
49.7
MMD w/ patch
38.5
57.7
40.9
22.2
42.8
51.3
PCKA
40.3
59.9
43.0
23.3
44.2
54.9
Visualize the inference outputs.
We first visualize the con-
fusion matrix of the proposed method in Fig. 6, and then vi-
sualize the annotated images of training with/without patches
averaging different dimensions in Fig. 7, respectively. These
figures reveal that our method can collect the similarities be-
tween different classes, and also show the effectiveness of our
method on the object detection task.
5
Discussion
PCKA in image classification.
We apply PCKA to the
classification task, and it also outperforms well on the meth-
ods with the same distillation type, as shown in Tab. 14. How-
ever, PCKA performs very badly on the teacher-student pairs
with different architectures. As cutting activations of differ-
ent architectures contain more dissimilar and harmful rep-
resentations, bringing difficulty in transferring knowledge to
the student.
Average on the channel, boosting the performance.
The
results in Tab. 7 reveal an interesting phenomenon, where the
performance of the previous distillation methods is boosted
by averaging the loss on channel dimension after the acti-
vations are cut into patches.
Instead of directly matching
the whole representation distribution in the activations, cut-
ting patches makes the alignment between the teacher and
student easier with a smaller representation distribution in
Table 7: Ablation study of distillation methods with(w/) or with-
out(w/o) patch on the COCO validation set (T→S refers to the dis-
tillation from T to S). Here, we surprisingly notice that previous dis-
tillation method performance can be improved by image patching.
T→S
RetinaNet-X101→RetinaNet-R50
Method
AP
AP50
AP75
APS
APM
APL
Teacher
41.0
60.9
44.0
23.9
45.2
54.0
Student
37.4
56.7
39.6
20.0
40.7
49.7
KD
37.2
56.5
39.3
20.4
40.4
49.5
KD w/ patch
39.3
58.7
41.9
21.4
41.3
50.9
AT
34.4
52.3
36.4
17.7
37.2
47.8
AT w/ patch
37.4
56.6
39.9
20.8
40.6
49.8
DIST
39.8
59.5
42.5
22.0
43.7
53.0
DIST w/ patch
40.2
59.6
43.2
22.7
44.8
53.9
PCKA w/o patch
36.4
55.8
38.7
20.6
39.8
48.7
PCKA(ours)
40.3
59.9
43.0
23.3
44.2
54.9
patches. Besides, cutting into patches follows the idea in the
framework of classification proposed by us, thus PCKA cal-
culates the inter-class similarities and intra-class similarities
in patches. Moreover, due to the superiority of cosine sim-
ilarity over distance-based losses [Boudiaf et al., 2020] and
high-order distribution representations collected by the gram
matrix, PCKA outperforms DIST and AT.
Positional information loss.
In PCKA, we cut the activa-
tion of the teacher and student into patches, and then flatten
them into a vector. Although this operation damages the orig-
inal positional information, performance does not deteriorate.
We suppose that CKA ensures the focus of the optimization
is on the shape of the distribution, rather than the raw values
in the Gram matrix, which is vital because a model‘s discrim-
inative capability is dependent on the distribution of its fea-
tures rather than its scale. Besides, at the beginning, the effect
brought by PCKA is smaller, compared with CE loss. There-
fore, CE loss motivates the optimization of the student model
steadily, and starting from a certain moment, PCKA drives
the student model to align complex and high-order represen-
tations, improving the generalization ability.
6
Conclusion
In this paper, we provide a novel theoretical perspective of
CKA in knowledge distillation, which can be simplified as
the upper bound of MMD with a constant term. Besides, we
dynamically customize the application of CKA based on the
characteristics of each task, with less computational source
yet comparable performance than previous methods. Further-
more, we propose a novel processing architecture for knowl-
edge distillation in object detection task, which can further
boost the performance of previous distillation methods. Our
experimental results, including both qualitative and quantita-
tive ones, demonstrate the effectiveness of our methods. In
future research, we will further explore the relationship be-
tween all similarity metric-based distillation methods, and ex-
plore the theoretical reason why averaging on the channel di-
mension with patches can boost the performance of previous
methods.
References
[Ahn et al., 2019] Sungsoo Ahn, Shell Xu Hu, et al. Varia-
tional information distillation for knowledge transfer. In
CVPR, pages 9163–9171, Long Beach, CA, USA, Jun.
2019. IEEE.
[Boudiaf et al., 2020] Malik Boudiaf, J´erˆome Rony, Im-
tiaz Masud Ziko, Eric Granger, Marco Pedersoli, Pablo
Piantanida, and Ismail Ben Ayed. A unifying mutual in-
formation view of metric learning: Cross-entropy vs. pair-
wise losses. In ECCV, 2020.
[Cai and Vasconcelos, 2017] Zhaowei Cai and Nuno Vas-
concelos. Cascade r-cnn: Delving into high quality object
detection. CVPR, pages 6154–6162, 2017.
[Chen and Wang, 2019] Kai Chen and Jiaqi Wang. MMDe-
tection: Open mmlab detection toolbox and benchmark.
arXiv preprint arXiv:1906.07155, 2019.
[Chen et al., 2021] Pengguang Chen, Shu Liu, et al.
Dis-
tilling knowledge via knowledge review. In CVPR, pages
5008–5017, Virtual Event, Jun. 2021. IEEE.
[Chen et al., 2022] Huanran
Chen,
Shitong
Shao,
Ziyi
Wang, Zirui Shang, Jin Chen, Xiaofeng Ji, and Xinxiao
Wu. Bootstrap generalization ability from loss landscape
perspective. In European Conference on Computer Vision,
pages 500–517. Springer, 2022.
[Chen et al., 2023] Huanran Chen, Yinpeng Dong, Zhengyi
Wang, Xiao Yang, Chengqi Duan, Hang Su, and Jun Zhu.
Robust classification via a single diffusion model. arXiv
preprint arXiv:2305.15241, 2023.
[Chong et al., 2022] Zhiyu
Chong,
Xinzhu
Ma,
Hong
Zhang, Yuxin Yue, Haojie Li, Zhihui Wang, and Wanli
Ouyang.
Monodistill:
Learning spatial features for
monocular 3d object detection.
ArXiv, abs/2201.10830,
2022.
[Contributors, 2021] MMRazor Contributors.
Openmmlab
model compression toolbox and benchmark, 2021.
[Cui et al., 2023] Jiequan Cui,
Zhuotao Tian,
Zhisheng
Zhong, Xiaojuan Qi, Bei Yu, and Hanwang Zhang.
Decoupled kullback-leibler divergence loss.
ArXiv,
abs/2305.13948, 2023.
[Dai et al., 2021] Xing Dai, Zeren Jiang, et al. General in-
stance distillation for object detection. In CVPR, pages
7842–7851, Virtual Event, Jun. 2021.
[de Rijk et al., 2022] Philip de Rijk, Lukas Schneider, Mar-
ius Cordts, and Dariu M. Gavrila. Structural knowledge
distillation for object detection. ArXiv, abs/2211.13133,
2022.
[Frankle and Carbin, 2019] Jonathan Frankle and Michael
Carbin.
The lottery ticket hypothesis: Finding sparse,
trainable neural networks.
In ICLR, New Orleans, LA,
USA, May 2019.
[Gong et al., 2023] Linrui Gong, Shaohui Lin, Baochang
Zhang, Yunhang Shen, Ke Li, Ruizhi Qiao, Bohan Ren,
Muqing Li, Zhou Yu, and Lizhuang Ma.
Adaptive
hierarchy-branch fusion for online knowledge distillation.
In AAAI, 2023.
[He et al., 2016] Kaiming He, Xiangyu Zhang, et al. Deep
residual learning for image recognition. In CVPR, pages
770–778, Las Vegas, NV, USA, Jun. 2016. IEEE.
[Heo et al., 2019] Byeongho Heo, Jeesoo Kim, and Sangdoo
et al. A comprehensive overhaul of feature distillation. In
ICCV, pages 1921–1930, Seoul, Korea (South), Oct.-Nov.
2019. IEEE.
[Hinton et al., 2015] Geoffrey Hinton, Oriol Vinyals, Jeff
Dean, et al. Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531, 2(7), 2015.
[Huang et al., 2022] Tao Huang, Shan You, et al.
Knowl-
edge distillation from a stronger teacher. In NeurIPS, New
Orleans, LA, USA, Nov.-Dec. 2022. NIPS.
[Kornblith et al., 2019] Simon
Kornblith,
Mohammad
Norouzi, Honglak Lee, and Geoffrey E. Hinton.
Simi-
larity of neural network representations revisited. ArXiv,
abs/1905.00414, 2019.
[Krizhevsky and Hinton, 2009] Alex Krizhevsky and Geof-
frey Hinton. Learning multiple layers of features from tiny
images.
Handbook of Sys temic Autoimmune Diseases,
2009.
[Li et al., 2020] Xiang Li, Wenhai Wang, Lijun Wu, Shuo
Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang.
Generalized focal loss: Learning qualified and distributed
bounding boxes for dense object detection.
ArXiv,
abs/2006.04388, 2020.
[Lin et al., 2014] Tsung-Yi Lin, Michael Maire, et al. Mi-
crosoft coco: Common objects in context. In ECCV, pages
740–755, Zurich, Switzerland, Sept. 2014. Springer.
[Lin et al., 2017] Tsung-Yi Lin, Priya Goyal, Ross B. Gir-
shick, Kaiming He, and Piotr Doll´ar. Focal loss for dense
object detection. ICCV, pages 2999–3007, 2017.
[Liu et al., 2021a] Li Liu, Qingle Huang, Sihao Lin, Hong-
wei Xie, Bing Wang, Xiaojun Chang, and Xiao-Xue
Liang. Exploring inter-channel correlation for diversity-
preserved knowledge distillation.
ICCV, pages 8251–
8260, 2021.
[Liu et al., 2021b] Songhua Liu, Tianwei Lin, Dongliang
He, Fu Li, Ruifeng Deng, Xin Li, Errui Ding, and Hao
Wang. Paint transformer: Feed forward neural painting
with stroke prediction. In ICCV, pages 6598–6607, Octo-
ber 2021.
[Marcel and Rodriguez, 2010] S´ebastien Marcel and Yann
Rodriguez.
Torchvision the machine-vision package of
torch. In ACM MM, pages 1485–1488, Firenze, Italy, Oct.
2010. ACM.
[Nguyen et al., 2020] Thao Nguyen, Maithra Raghu, and Si-
mon Kornblith. Do wide and deep networks learn the same
things? uncovering how neural network representations
vary with width and depth. ArXiv, abs/2010.15327, 2020.
[Orhan and Pitkow, 2017] Emin Orhan and Xaq Pitkow.
Skip connections eliminate singularities. In ICLR, 2017.
[Park et al., 2019] Wonpyo Park, Dongju Kim, et al. Rela-
tional knowledge distillation. In CVPR, pages 3967–3976,
Long Beach, CA, USA, Jun. 2019. IEEE.
[Peng et al., 2019] Baoyun Peng, Xiao Jin, et al. Correlation
congruence for knowledge distillation.
In ICCV, pages
5007–5016, 2019.
[Qiu et al., 2022] Zengyu Qiu, Xinzhu Ma, Kunlin Yang,
Chunya Liu, Jun Hou, Shuai Yi, and Wanli Ouyang. Bet-
ter teacher better student: Dynamic prior knowledge for
knowledge distillation. ArXiv, abs/2206.06067, 2022.
[Ren et al., 2015] Shaoqing Ren, Kaiming He, Ross B. Gir-
shick, and Jian Sun. Faster r-cnn: Towards real-time ob-
ject detection with region proposal networks. IEEE PAMI,
39:1137–1149, 2015.
[Romero et al., 2014] Adriana Romero, Nicolas Ballas, et al.
Fitnets:
Hints for thin deep nets.
arXiv preprint
arXiv:1412.6550, 2014.
[Russakovsky et al., 2015] Olga Russakovsky,
Jia Deng,
et al. Imagenet large scale visual recognition challenge.
IJCV, 115(3):211–252, 2015.
[Saha et al., 2022] Aninda Saha, Alina Bialkowski, and Sara
Khalifa. Distilling representational similarity using cen-
tered kernel alignment (cka). In BMVC, 2022.
[Shao et al., 2023a] Shitong Shao, Huanran Chen, Zhen
Huang, Linrui Gong, Shuai Wang, and Xinxiao Wu.
Teaching what you should teach: a data-based distillation
method. In Proceedings of the Thirty-Second International
Joint Conference on Artificial Intelligence, pages 1351–
1359, 2023.
[Shao et al., 2023b] Shitong Shao, Xu Dai, Shouyi Yin, Lu-
jun Li, Huanran Chen, and Yang Hu. Catch-up distilla-
tion: You only need to train once for accelerating sam-
pling. arXiv preprint arXiv:2305.10769, 2023.
[Shu et al., 2021] Changyong Shu, Yifan Liu, et al. Channel-
wise knowledge distillation for dense prediction. In ICCV,
pages 5311–5320, Montreal, Canada, Oct. 2021. IEEE.
[Tian et al., 2019] Zhi Tian, Chunhua Shen, Hao Chen, and
Tong He. Fcos: Fully convolutional one-stage object de-
tection. ICCV, pages 9626–9635, 2019.
[Tian et al., 2020] Yonglong Tian,
Dilip Krishnan,
and
Phillip Isola.
Contrastive representation distillation.
In ICLR, Addis Ababa, Ethiopia, Apr. 2020. OpenRe-
view.net.
[Touvron et al., 2020] Hugo
Touvron,
Matthieu
Cord,
Matthijs Douze, Francisco Massa, Alexandre Sablay-
rolles, and Herv’e J’egou. Training data-efficient image
transformers & distillation through attention.
In ICML,
2020.
[Tung and Mori, 2019] Frederick Tung and Greg Mori.
Similarity-preserving knowledge distillation.
In ICCV,
pages 1365–1374, Seoul, Korea (South), Oct-Nov. 2019.
IEEE.
[Wan et al., 2020] Alvin
Wan,
Xiaoliang
Dai,
Peizhao
Zhang, Zijian He, Yuandong Tian, Saining Xie, Bichen
Wu, Matthew Yu, Tao Xu, Kan Chen, P´eter Vajda, and
Joseph Gonzalez. Fbnetv2: Differentiable neural archi-
tecture search for spatial and channel dimensions. CVPR,
pages 12962–12971, 2020.
[Wang et al., 2019] Tao Wang, Li Yuan, Xiaopeng Zhang,
and Jiashi Feng.
Distilling object detectors with fine-
grained feature imitation. CVPR, pages 4928–4937, 2019.
[Wu et al., 2016] Jiaxiang Wu, Cong Leng, Yuhang Wang,
Qinghao Hu, and Jian Cheng.
Quantized convolutional
neural networks for mobile devices. In CVPR, Las Vegas,
NV, USA, Jun. 2016. IEEE.
[Yang et al., 2019] Ze Yang, Shaohui Liu, Han Hu, Liwei
Wang, and Stephen Lin. Reppoints: Point set representa-
tion for object detection. ICCV, pages 9656–9665, 2019.
[Yang et al., 2021] Jing Yang, Brais Martinez, et al. Knowl-
edge distillation via softmax regression representation
learning. In ICLR. OpenReview.net, 2021.
[Yang et al., 2022a] Zhendong Yang, Zhe Li, et al.
Focal
and global knowledge distillation for detectors. In CVPR,
pages 4643–4652, New Orleans, LA, USA, Jun. 2022.
IEEE.
[Yang et al., 2022b] Zhendong Yang, Zhe Li, Mingqi Shao,
Dachuan Shi, Zehuan Yuan, and Chun Yuan. Masked gen-
erative distillation. In ECCV, 2022.
[Yang et al., 2022c] Zhendong Yang, Zhe Li, Ailing Zeng,
Zexian Li, Chun Yuan, and Yu Li.
Vitkd:
Practical
guidelines for vit feature knowledge distillation. ArXiv,
abs/2209.02432, 2022.
[Yang et al., 2023] Zhendong Yang, Ailing Zeng, Zhe Li,
Tianke Zhang, Chun Yuan, and Yu Li. From knowledge
distillation to self-knowledge distillation: A unified ap-
proach with normalized loss and customized soft labels.
ArXiv, abs/2303.13005, 2023.
[Zagoruyko and Komodakis, 2016a] Sergey Zagoruyko and
Nikos Komodakis. Paying more attention to attention: Im-
proving the performance of convolutional neural networks
via attention transfer. In ICLR, 2016.
[Zagoruyko and Komodakis, 2016b] Sergey Zagoruyko and
Nikos Komodakis. Wide residual networks. In BMVC,
pages 1–15, York, UK, Sept. 2016. BMVA.
[Zhang and Ma, 2021] Linfeng Zhang and Kaisheng Ma.
Improve object detection with feature-based knowledge
distillation: Towards accurate and efficient detectors. In
ICLR, pages 1–14. OpenReview.net, 2021.
[Zhao et al., 2022] Borui Zhao, Quan Cui, Renjie Song,
Yiyu Qiu, and Jiajun Liang. Decoupled knowledge distil-
lation. In CVPR, pages 11953–11962, New Orleans, LA,
USA, Jun. 2022. IEEE.
[Zhixing et al., 2021] Du Zhixing, Rui Zhang, Ming Chang,
Shaoli Liu, Tianshi Chen, Yunji Chen, et al.
Distilling
object detectors with feature richness. NeurIPS, 34:5213–
5224, Dec. 2021.
[Zhou and Chen, 2023] Zikai Zhou and Huanran Chen. Afn:
Adaptive fusion normalization via an encoder-decoder
framework. In ArXiv preprint, 2023.
A
Datasets
CIFAR-100.
Dataset CIFAR-100 [Krizhevsky and Hinton,
2009] is the subsets of the tiny image dataset and consists of
60,000 images with the size 32×32. Specifically, the train-
ing set contains 50,000 images, and the testing set contains
10,000 images.
ImageNet-1k.
Dataset ImageNet-1k [Russakovsky et al.,
2015], also commonly referred to as ILSVRC 2012, has 1000
classes, and the benchmark is trained using the training set
and tested using the validation set. Its training and validation
sets contain 1281,167 and 50,000 images, respectively.
MS-COCO.
Dataset MS-COCO [Lin et al., 2014] is a
large-scale object detection dataset. The benchmark is the
same as ImageNet-1k, using the training set for training and
the validation set for testing. The training/validation split was
changed from 83K/41K to 118K/5K in 2017. Researchers
commonly apply the 2017 version for experiments.
B
Hyperparameter Settings
Classification.
For
the
classification
experiments
on
CIFAR-100, the batch size is 128, the total number of epochs
is 240, and the learning rate is initialized to 0.1 and scheduled
by ALRS [Chen et al., 2022]. In addition, we employ an
SGD optimizer for training and set the weight decay and
momentum as 5e-4 and 0.9, respectively. For the classifica-
tion experiments on ImageNet-1k (ResNet34-ResNet18 pair
and ResNet50-MobileNet pair), the total batch size is 512,
the total number of epochs is 240, the batch size in every
GPU is 128, the number of GPUs is 4 and the learning rate
is initialized to 0.1 and scheduled by ALRS. Besides, we
employ an SGD optimizer for training and set the weight
decay and momentum as 1e-4 and 0.9, respectively. The loss
weight scaling factors α, β are both 5.
Detection.
For the detection experiments on MS-COCO,
we utilize mmdetection [Chen and Wang, 2019] and mm-
razor [Contributors, 2021] for both training and testing.
Following [Shu et al., 2021; Park et al., 2019; Zhou and
Chen, 2023], we use the same standard training strategies on
the Cascade RCNN-X101-Faster RCNN-R50 and RetinaNet-
X101-RetinaNet-R50 pairs. To be specific, the total batch
size is 12, the total number of epochs is 24, the batch size in
every GPU is 3, the number of GPUs is 4 and the learning
rate is divided by 10 at 16 and 22 epochs. The initial learning
rate is set as 0.02 and 0.01 on Cascade RCNN-X101-Faster
RCNN-R50 and RetinaNet-X101-RetinaNet-R50 pairs, re-
spectively. Besides, the setting on the FCOS-R101-FCOS-
R50 pair is following [Yang et al., 2022a]. Compared with the
RetinaNet-X101-RetinaNet-R50 pair, the only difference is
we apply a warm-up learning rate on the FCOS-R101-FCOS-
R50 pair. Furthermore, the settings on GFL-R101-GFL-R50,
RepPoints-X101-RepPoints-R5, RetinaNet-R101-RetinaNet-
R50 and Faster RCNN-R101-Faster RCNN-R50 pairs are
the same with the setting on RetinaNet-X101-RetinaNet-R50
pair.
C
Derivations
C.1
Revisit the formula of CKA
We prove CKA measures the cosine similarity between
teacher‘s gram matrix and student gram matrix.
Proof.
∥Y ⊤X∥2
F
∥XT X∥F ∥Y T Y ∥F
= tr((Y ⊤X)⊤Y ⊤X)
∥XT X∥F ∥Y T Y ∥F
=
tr(X⊤Y Y ⊤X)
∥XT X∥F ∥Y T Y ∥F
=
tr(XX⊤Y Y ⊤)
∥XT X∥F ∥Y T Y ∥F
=
vec(XX⊤)⊤vec(Y Y ⊤)
∥vec(XX⊤)∥2∥vec(Y Y ⊤)∥2
.
C.2
Connection between CKA and MMD
In Theorem 1, we already prove the relationship between
CKA distance and cosine similarity. Denote the i-th row of
matrix
X
√
||XXT ||F as xi and i-th row of matrix
Y
√
||Y Y T ||F as
yi, we can get:
∥Y ⊤X∥2
F
∥XT X∥F ∥Y T Y ∥F
=
vec(XX⊤)⊤vec(Y Y ⊤)
∥vec(XX⊤)∥2∥vec(Y Y ⊤)∥2
= − ∥
vec(XX⊤)
∥vec(XX⊤)∥2
−
vec(Y Y ⊤)
∥vec(Y Y ⊤)∥2
∥2
2
+∥
vec(XX⊤)
∥vec(XX⊤)∥2
∥2
2 + ∥
vec(Y Y ⊤)
∥vec(Y Y ⊤)∥2
∥2
2
= − ∥
vec(XX⊤)
∥vec(XX⊤)∥2
−
vec(Y Y ⊤)
∥vec(Y Y ⊤)∥2
∥2
2 + 2
= −

X
i,j
[⟨xi, xj⟩ − ⟨yi, yj⟩]2

 + 2
= − NEi,j[⟨xi, xj⟩ − ⟨yi, yj⟩]2 + 2
≤ − N (Ei,j[⟨xi, xj⟩] − Ei,j[⟨yi, yj⟩])2 + 2
Consequently, maximizing the Centered Kernel Alignment
(CKA) similarity is tantamount to minimizing the upper
bound of Maximum Mean Discrepancy (MMD) distance.
D
Compare the average running time
We show the superior average training time cost than other
methods.
E
CKA curve in training.
To qualitatively analyze the proposed method, we visualize
the CKA similarities in the training phase.
As shown in
Fig. 5, we can find the CKA values increase with training,
which demonstrates that our method does narrow the gap of
teacher-student models at the logit level.
Table 8: Results on the ImageNet validation set. ∗ indicates the teacher is pre-trained on ImageNet-21K. We evaluate our method on DeiT
architectures.
Architecture
Accuracy
Methods
Teacher
Student
Teacher Student
KD VIT-KD [2022c] VIT-KD+RCKA
DeiT III-Small* [2020] DeiT-Tiny Top-1
82.76
74.42
76.01
76.06
77.23
Top-5
-
92.29
93.26
93.16
93.67
DeiT III-Base*
DeiT-Small Top-1
85.48
80.55
82.52
81.95
83.12
Top-5
-
95.12
96.30
95.64
96.41
Table 9: Results on the COCO validation set (T→S refers to the distillation from T to S). Here, the content in brackets to the right of “Ours”
refers to the methods applied in the distillation process.
T→S
Faster RCNN-R101→Faster RCNN-R50
RetinaNet-R101→RetinaNet-R50
T→S
RepPoints-X101→RepPoints-R50 [2019]
Method
AP
AP50
AP75
APS
APM
APL
AP
AP50
AP75
APS
APM
APL
Method
AP
AP50
AP75
APS
APM
APL
Teacher
39.9
60.1
43.3
23.5
44.2
51.5
38.9
58.0
41.5
21.0
42.8
52.4
Teacher
44.2
65.5
47.8
26.2
48.4
58.5
Student
38.4
59.0
42.0
21.5
42.1
50.3
37.4
56.7
39.6
20.0
40.7
49.7
Student
38.6
59.6
41.6
22.5
42.2
50.4
FGFI [2019]
39.3
59.8
42.9
22.5
42.3
52.2
38.6
58.7
41.3
21.4
42.5
51.5
FKD [2021]
40.6
-
-
23.4
44.6
55.3
GID [2021]
40.2
60.7
43.8
22.7
44.0
53.2
39.1
59.0
42.3
22.8
43.1
52.3
Our
41.0
62.2
44.0
23.6
44.6
55.6
FGD [2022a]
40.4
-
-
22.8
44.5
53.5
39.6
-
-
22.9
43.7
53.6
Our
40.1
60.7
43.8
22.7
44.1
52.6
38.6
57.8
41.3
20.7
42.3
51.9
Table 10: Average throught (batches / second) of training ResNet-
18 student with ResNet-34 teacher on ImageNet. The speed is tested
based on our implementations on 8 NVIDIA V100 GPUs.
Method
KD
RKD
SRRL
CRD
DIST
RCKA
Throughout
14.28
11.11
12.98
8.33
14.19
11.31
Figure 3: Training process visualization of all experimented detec-
tors.
Table 11: Ablation study of loss weight γ.
T→S
RetinaNet-X101→RetinaNet-R50
Loss Weight
AP
AP50
AP75
APS
APM
APL
Teacher
41.0
60.9
44.0
23.9
45.2
54.0
Student
37.4
56.7
39.6
20.0
40.7
49.7
γ = 5
39.8
59.4
42.5
22.3
43.5
53.9
γ = 10
40.3
59.9
43.0
23.3
44.2
54.9
γ = 15
38.5
58.0
41.0
20.7
41.8
51.3
Figure 4: The effect of patch on different dimensions. This experi-
ment is conducted on the RetinaNet-X101-RetinaNet-R50 pair
Table 12: Ablation study of Batchsize. Noted that ”4 × 1” means 4
GPUs, each batchsize is 1, totally batchsize is 4 × 1 = 4
T→S
RetinaNet-X101→RetinaNet-R50
AP
AP50
AP75
APS
APM
APL
Teacher
41.0
60.9
44.0
23.9
45.2
54.0
Student
37.4
56.7
39.6
20.0
40.7
49.7
Batchsize 4×1
39.9
59.5
42.8
21.7
43.9
53.3
Batchsize 4×2
40.3
59.8
43.1
22.5
44.2
54.4
Batchsize 4×3
40.3
59.9
43.0
23.3
44.2
54.9
Batchsize 4×4
40.0
59.8
42.6
22.9
43.8
54.1
F
PCKA in image classification
We apply PCKA to the classification task, and it also outper-
forms well on the methods with the same distillation type,
as shown in Table 14. However, PCKA performs very badly
on the teacher and student pairs with different architectures.
The possible reason is that the cutting activations of differ-
ent architectures contain more dissimilar and harmful repre-
Table 13: Ablation study of number of distillation layers. Noted that
”Layer=1” means choosing 1 layer to distill.
T→S
RetinaNet-X101→RetinaNet-R50
AP
AP50
AP75
APS
APM
APL
Teacher
41.0
60.9
44.0
23.9
45.2
54.0
Student
37.4
56.7
39.6
20.0
40.7
49.7
Layer=1
39.9
59.7
42.8
21.9
43.5
54.1
Layer=2
40.2
59.9
42.6
21.9
44.0
54.6
Layer=3
40.3
59.9
43.0
23.3
44.2
54.9
Table 14: Results on the CIFAR-100 test set. We apply PCKA to
the classification task, and it also outforms some of the previous
methods.
Distillation
Type
Teacher
ResNet110
ResNet110
WRN-40-2 [2016b]
VGG13
74.31
74.31
75.61
74.64
Student
ResNet20 [2016]
ResNet32
WRN-16-2
VGG8
69.06
71.14
73.26
72.50
FitNet [2014]
68.99
71.06
73.58
71.02
ATKD [2016a]
70.22
70.55
74.08
71.43
SPKD [2019]
70.04
72.69
73.83
72.68
CCKD [2019]
69.48
71.48
73.56
70.71
RKD [2019]
69.25
71.82
73.35
71.48
VID [2019]
70.16
70.38
74.11
71.23
KD [2015]
70.67
73.08
74.92
73.33
DKD [2022]
-
74.11
76.24
76.32
DIST [2022]
69.94
73.55
75.29
75.79
PCKA(ours)
70.97
74.11
75.61
74.98
sentations, causing difficulty in transferring knowledge to the
student.
Figure 5: CKA curve in training phase on CIFAR-100. We visualize the CKA similarities in the training for six teacher-student pairs.
Figure 6: The confusion matrix is from the RetinaNet-R50, distilled by PCKA with teacher model RetinaNet-X101, averaging on channel.
Figure 7: Top 5 inference results with RetinaNet-R50, distilled by PCKA without patch or with patch.
"
This paper investigates the performance of backscatter communications (BC) over fluid antenna (FA) technology. We obtain analytical expressions for the outage probability (OP) and delay outage rate (DOR) in closed-form at the FA receiver and examine their asymptotic behavior at the high SNR regime. Numerical results demonstrate that the FA reader can significantly enhance the BC system performance compared with a single-antenna reader.,"We introduce Fluid Antenna (FA) systems, a novel technology for future mobile communications that can improve diversity and multiplexing advantages. We highlight the benefits of integrating FA systems with BC, such as adaptability, reduced channel state information requirements, and cost-effectiveness. We emphasize the lack of previous research combining FA systems and BC and motivate our work to evaluate the performance of wireless BC when backscatter devices utilize FA systems.","We review recent works on the application of FA systems in different wireless communication scenarios, such as channel modeling, performance analysis, channel estimation, and implementation. We note the absence of studies combining FA systems with BC and identify the potential advantages of FA-aided BC for the next generation of wireless technology.nannannan","We propose a system model for FA-aided BC, where a single-antenna source sends data to a reader equipped with a FA through forward and backscatter channels. We derive the cumulative distribution function (CDF) of the equivalent channel at the reader using the copula-based approach. We obtain closed-form expressions for the OP and DOR under correlated Rayleigh fading channels and derive their asymptotic expressions in the high SNR regime. We present numerical results to evaluate the considered system performance.nan","The numerical results show that increasing the FA size or the number of FA ports improves the performance of OP and DOR. The FA reader outperforms the single-antenna reader in terms of OP and DOR, providing lower values. Moreover, the OP and DOR decrease as the average SNR increases. Additionally, the FA reader enables the transmission of a large amount of data with a small delay when the FA size and number of ports are sufficiently large.","We conclude that the FA reader can provide remarkable performance in terms of the OP and DOR compared with the single-antenna reader over BC. The FA reader can enhance diversity, spatial multiplexing, and channel capacity, leading to improved overall link quality.",Performance Analysis of Fluid Antenna-aided Backscatter Communications Systems,"Farshad Rostami Ghadi, Masoud Kaveh, Kai-Kit Wong","1
Performance Analysis of Fluid Antenna-aided Backscatter
Communications Systems
Farshad Rostami Ghadi, Member, IEEE, Masoud Kaveh, Member, IEEE, Kai-Kit Wong, Fellow, IEEE
Abstract—This paper studies the performance of backscatter
communications (BC) over emerging fluid antenna (FA) tech-
nology. In particular, a single-antenna source sends information
to a FA reader through the wireless forward (i.e., source-to-tag)
and backscatter (tag-to-reader) channels. For the considered BC,
we first derive the cumulative distribution function (CDF) of the
equivalent channel at the FA receiver, and then we obtain closed-
form expressions of the outage probability (OP) and delay outage
rate (DOR) under a correlated Rayleigh distribution. Moreover,
in order to gain more insights into the system performance, we
present analytical expressions of the OP and DOR at the high
SNR regime. Numerical results indicate that considering the FA
at the reader can significantly improve the performance of BC
in terms of the OP and DOR compared with a single-antenna
reader.
Index Terms—Backscatter communication, fluid antenna sys-
tem, correlated fading channel
I. INTRODUCTION
Regarding the importance of massive connectivity in sixth-
generation (6G) wireless technology and the escalating intri-
cacy associated with system design in the context of ultra-
massive multiple-input multiple-output (MIMO), there is an
ever-growing demand for a more tractable approach that can
enhance the efficiency of wireless communication systems [1]–
[3]. To this end, fluid antenna (FA) systems have recently
emerged as a cutting-edge technology for future mobile com-
munications, which can enhance diversity and multiplexing
advantages by utilizing novel dynamic radiating structures [4].
In particular, a FA includes a pixel-based structure or dielectric
conductive [5] that can switch its position (i.e., ports) in a
pre-defined small space, where this unique feature can be
especially exploited in mobile phones due to the physical
limitations of antenna deployment. Moreover, compared with
traditional multi-antenna systems, FA multiple access systems
(FAMA) are able to eliminate the necessity for channel state
information (CSI) at base stations (BSs) concerning precoding,
user clustering, and power control; thereby, user equipment
(UEs) are only required to perform single-user decoding [6]–
[8].
This work is supported by the Engineering and Physical Sciences Research
Council (EPSRC) under Grant EP/W026813/1. For the purpose of open
access, the authors will apply a Creative Commons Attribution (CC BY)
licence to any Author Accepted Manuscript version arising.
Farshad Rostami Ghadi and Kai-Kit Wong are with the Department of
Electronic and Electrical Engineering, University College London, WC1E
6BT London, UK. (e-mail:{f.rostamighadi, kai-kit.wong}@ucl.ac.uk).
Masoud Kaveh is with the Department of Information and Com-
munication
Engineering,
Aalto
University,
02150
Espoo,
Finland.
(e-
mail:masoud.kaveh@aalto.fi)
Digital Object Identifier 10.1109/XXX.2021.XXXXXXX
Furthermore, given the practical applications of evolving
technologies such as Radio Frequency Identification (RFID)
systems and the Internet of Things (IoT) in realistic 6G
wireless networks, considerable emphasis has been paid to
backscatter communication (BC) in the recent years [9]. Par-
ticularly, BC is a cost-effective wireless approach that enables
low-power devices to sent data by reflecting or modulating
existing radio frequency (RF) signals in the propagation
environment [10]. In other words, backscatter devices are
designed to consume minimal energy since they do not require
generating their own signals. Therefore, integrating the reflec-
tive capabilities of BC with the dynamic and reconfigurable
properties of FA systems can potentially provide a synergistic.
FA systems are able to adaptively modify their radiating struc-
tures based on environmental conditions or network demands;
this adaptability, when coupled with BC, allows for dynamic
adjustments in the reflection and modulation of RF signals.
Consequently, a highly flexible communication system that can
optimize signal propagation, enhance spectral efficiency, and
enable cost-effective wireless connectivity is provided.
Great efforts have recently been carried out to develop the
application of FA systems in different wireless communication
scenarios from various aspects, e.g., channel modeling [11],
[12], performance analysis [13]–[17], channel estimation [18],
[19], and implementation [20], [21]. However, to the best of
the author’s knowledge, there have been no previous works
that combine the FA system with BC. Hence, motivated by the
potential advantages of FA systems and the unique features of
BC for the next generation of wireless technology, we evaluate
the performance of wireless BC when backscatter devices take
advantage of FA systems. In particular, we consider a single-
antenna source that aims to send data to a FA reader through
the forward (i.e., source-to-tag) and backscatter (tag-to-reader)
channels. For this scenario, (i) We derive the cumulative
distribution function (CDF) of the equivalent channel at the
FA reader, i.e., the CDF of the maximum of K random
variables (RVs) such that each is the product of forward and
backscatter channels, by using the copula-based approach; (ii)
We obtain the outage probability (OP) and delay outage rate
(DOR) in closed-form expressions under correlated Rayleigh
fading channels; (iii) We derive the asymptotic expressions of
the OP and DOR in the high SNR regime; (iv) We present
numerical results to evaluate the performance considered for
FA-aided BC, where the results indicate that the FA reader
can significantly enhance the system performance compared
with the single-antenna reader, namely, lower values of OP
and DOR are achieved.
arXiv:2401.11820v1  [cs.IT]  22 Jan 2024
2
II. SYSTEM MODEL
We consider a wireless FA-aided BC as illustrated in Fig.
1, where a single-antenna source aims to send information x
to a reader that is equipped by a FA through the forward and
backscatter channels. Thus, the instantaneous received signal
power at the tag is given by
Pt = PsLsgf,
(1)
in which Ps is the transmitted power by the source, Ls includes
the gains of the transmit and receive antennas and frequency-
dependent propagation losses, and gf = |hf|2 defines the
fading channel gain between the source and the tag, where hf
is the corresponding forward fading channel coefficient. On
the reader side, we assume that the FA can freely move along
K pre-set positions1 (i.e., ports), which are equally distributed
on a linear space of length Wλ where λ is the wavelength of
propagation. Additionally, we suppose that the FA consists of
only one radio frequency (RF) chain, and thus, only one port
can be activated for communication. Under such assumptions,
the received signal at the k-th port of the reader can be defined
as
yk = hfhb,kx + zk,
(2)
where hb,k denotes the backscatter channel coefficient be-
tween the tag and k-th port of the FA reader with the respective
fading channel gain gb,k = |hb,k|2 and zk is the independent
identically distributed (i.i.d.) additive white Gaussian noise
(AWGN) with zero mean and variance σ2 at each port. Without
loss of generality, we assume that the fading coefficients are
normalized, i.e., E [gf] = E [gf] = 1, where E [·] denotes the
expectation operator.
Furthermore, we assume that the FA is able to always switch
to the best port with the strongest signal for communication,
i.e.,
gFA = max {gp,k . . . , gp,k} ,
(3)
in which gp,k = gfgb,k denotes the product channel gain of
the forward and backscatter links. It is worth noting that gp,k
for k ∈ {1, . . . , K} are spatially correlated since they can be
arbitrarily close to each other so that such a spatial correlation
between FA ports can be characterized by Jake’s model as [23]
µk = ωJ0
2π (k − 1)
K − 1
W

,
(4)
where µk denotes the correlation parameter that can control the
dependency between gp,k, ω is the large-scale fading effect,
and J0 (.) represents the zero-order Bessel function of the first
kind. By doing so, the received signal-to-noise ratio (SNR) at
the reader can be defined as
γ = PtgFA
σ2
= ¯γgFA,
(5)
in which ¯γ = Pt
σ2 is the average SNR.
1In this paper, the switching delay is assumed to be negligible, which is a
reasonable assumption for the pixel-based FA [22].
Fig. 1. The system model represents FA-aided BC.
III. PERFORMANCE ANALYSIS
Here, we first derive the CDF of the equivalent fading
channel gain at the FA reader, and then the closed-form
expressions of the OP and DOR are obtained. Moreover, we
derive the asymptotic expressions of the OP and DOR in the
high SNR regime.
A. Statistical Characterization
From (3), we can see that the CDF of the equivalent fading
channel gain at the reader is defined as the CDF of the
maximum of K correlated RVs that each includes the product
of two independent RVs. Assuming that all fading channels
undergo Rayleigh distribution, the CDF of gFA is derived as
the following proposition.
Proposition 1. The CDF of gFA = max {gp,k . . . , gp,k} for
the considered FA-aided BC is given by
FgFA (r) =
"" K
X
k=1
h3
Rayleigh fading channels, we can derive the CDF of gp,k =
gfgb,k as follows
Fgp,k (r) = Pr (gfgb,k ≤ r)
(11)
=
Z ∞
0
fgf (gf) FGb,k
 r
gf

dgf
(12)
= 1 −
Z ∞
0
e
−

gf+ r
gf

dgf
(13)
(b)
= 1 − 2√rK1
4
D. Asymptotic Analysis
Although the derived OP and DOR in Propositions 2 and 3
are in simple closed-form expressions, we are interested in the
asymptotic behavior of the obtained metrics at the high SNR
regime (i.e., γ → ∞) to gain more insights into the system
performance. To do so, we exploit the series expansion of the
Bessel function K1 (r) when r → 0 as follow
K1 (r) ≈ 1
r + r
4 (2ζ − 1) + r
2 log
r
2

,
(23)
where ζ is the Euler-Mascheroni constant [28]. Hence, the
asymptotic expressions of the OP and DOR for the considered
system model can be obtained in the following corollary.
Corollary 1. The asymptotic expressions of the OP and DOR
for the considered FA-aided BC at the high SNR regime, i.e.,
¯γ → ∞ are respectively given by (24) and (25).
Proof. For the high SNR regime (i.e., ¯γ → ∞), we have
ω
¯γ → 0, where ω ∈
n
γth, ˆTth
o
. Hence, by utilizing (23),
2ω
¯γ K1

2ω
¯γ

can be approximated as
2ω
¯γ K1
2ω
¯γ

≈ 1 + ω
¯γ

2ζ − 1 + 2 log
rω
¯γ

.
(26)
Next, by substituting (26) into (18) and (20), the proof is
accomplished.
IV. NUMERICAL RESULTS
In this section, we present numerical results to evaluate the
considered system performance in terms of the OP and DOR,
which are double-checked by the Monte-Carlo simulation
method. To this end, we set the parameters as γth = 0dB,
R = 5Kbits, B = 2GHz, Tdt = 3ms, ¯γ = 20dB, W =
{0.5, 1, 2, 4, 6}, and N = {2, 4, 6, 8, 10}.
Figs. 2(a) and 2(b) respectively illustrate the behavior of
OP and DOR in terms of the average SNR ¯γ for given values
of FA size W under correlated Rayleigh fading channels. As
expected, the OP and DOR decrease as ¯γ increases, which is
reasonable since the channel condition improves. Moreover,
it can be seen that by increasing the FA size W for a
fixed number of ports K, the performance of OP and DOR
improves. The reason for this behavior is that increasing the
spatial separation between the FA ports by increasing W for a
fixed K can reduce the spatial correlation between FA ports.
Additionally, we can clearly observe that such an improvement
is more noticeable when K is large. The performance of OP
and DOR in terms of ¯γ for given values of K under correlated
Rayleigh fading channels is presented in Figs. 3(a) and 3(b),
respectively. We can see that as the number of FA ports K
grows, lower values of the OP and DOR are provided. The
main reason for such a behavior is that although increasing K
for a fixed value of W raises the spatial correlation between FA
ports, it can potentially improve the channel capacity, diversity
gain, and spatial multiplexing at the same time. Hence, this
can help mitigate fading and improve the overall link quality.
Furthermore, as we can see in both Figs. 2 and 3, considering
a FA reader instead of a single-antenna reader can significantly
enhance the performance of BC in terms of OP and DOR. In
order to evaluate how the FA reader affects the DOR perfor-
mance in terms of transmitted data R over BC, we present Fig.
4 for selected values of W and K. First, we can observe that
as W and K increase simultaneously, the spatial correlation
between FA ports becomes balanced; thereby, lower values of
the OP and DOR are reached. Furthermore, as expected, it can
be seen that as R increases, the DOR performance becomes
worse, such that transmitting a high amount of data (e.g.,
R = 3Kbits) with low delay is almost impractical when a
single-antenna reader or a FA reader with small W and K are
considered. However, thanks to the FA reader, a large amount
of information with a small delay can be sent when the W
and K are large enough.
V. CONCLUSION
In this paper, we investigated the performance of BC in
the presence of FA system. Particularly, we assumed that a
single-antenna source aims to send information to a reader via
wireless forward and backscatter channels. We also supposed
that the reader includes a FA, where only one port can be
activated for communication. Under such assumptions, we first
derive the CDF of the equivalent channel (i.e., the maximum of
K correlated RVs such that each is the product of forward and
backscatter channels) for the reader by exploiting the copula
technique. Then, we derived the closed-form expressions of the
OP and DOR under correlated Rayleigh fading channels. Fur-
thermore, we obtained the asymptotic expressions of the OP
and DOR in the high SNR regime. Eventually, our analytical
results revealed that the FA reader can provide a remarkable
performance in terms of the OP and DOR compared with the
single-antenna reader over BC.
P ∞
o
≈


K
X
k=1


γth
¯γ

1 − 2ζ − 2 log
rγth
¯γ

4µk
2µk−3
− 1

 + 1


2µk−3
4µk
.
(24)
P ∞
dor ≈


K
X
k=1



 ˆTth
¯γ

1 − 2ζ − 2 log


s
ˆTth
¯γ






4µk
2µk−3
− 1

 + 1


2µk−3
4µk
.
(25)
5
(a) OP
(b) DOR
Fig. 2. Performance of (a) OP and (b) DOR versus average SNR ¯γ for selected values of FA size W.
(a) OP
(b) DOR
Fig. 3. Performance of (a) OP and (b) DOR versus average SNR ¯γ for selected values of FA ports K.
REFERENCES
[1] Z. Wang, J. Zhang, H. Du, E. Wei, B. Ai, D. Niyato, and M. Debbah,
“Extremely large-scale MIMO: Fundamentals, challenges, solutions, and
future directions,” IEEE Wirel. Commun., 2023.
[2] D. A. Urquiza Villalonga, H. OdetAlla, M. J. Fern´andez-Getino Garc´ıa,
and A. Flizikowski, “Spectral efficiency of precoded 5G-NR in single
and multi-user scenarios under imperfect channel knowledge: A com-
prehensive guide for implementation,” Electronics, vol. 11, no. 24, p.
4237, 2022.
[3] T. L. Marzetta, “Noncooperative cellular wireless with unlimited num-
bers of base station antennas,” IEEE Trans. Wirel. Commun., vol. 9,
no. 11, pp. 3590–3600, 2010.
[4] K.-K. Wong, A. Shojaeifard, K.-F. Tong, and Y. Zhang, “Fluid antenna
systems,” IEEE Trans. Wirel. Commun., vol. 20, no. 3, pp. 1950–1962,
2020.
[5] Y. Huang, L. Xing, C. Song, S. Wang, and F. Elhouni, “Liquid antennas:
Past, present and future,” IEEE Open J. Antennas Propag., vol. 2, pp.
473–487, 2021.
[6] K.-K. Wong and K.-F. Tong, “Fluid antenna multiple access,” IEEE
Trans. Wirel. Commun., vol. 21, no. 7, pp. 4801–4815, 2021.
[7] K.-K. Wong, D. Morales-Jimenez, K.-F. Tong, and C.-B. Chae, “Slow
fluid antenna multiple access,” IEEE Trans. Commun., 2023.
[8] K.-K. Wong, C.-B. Chae, and K.-F. Tong, “Compact ultra massive
antenna array: A simple open-loop massive connectivity scheme,” IEEE
Trans. Wirel. Commun., 2023.
[9] H. Stockman, “Communication by means of reflected power,” Proc. IRE,
vol. 36, no. 10, pp. 1196–1204, 1948.
[10] N. Van Huynh, D. T. Hoang, X. Lu, D. Niyato, P. Wang, and D. I. Kim,
“Ambient backscatter communications: A contemporary survey,” IEEE
Commun. Surv. Tutor., vol. 20, no. 4, pp. 2889–2922, 2018.
[11] K.-K. Wong, K.-F. Tong, Y. Chen, and Y. Zhang, “Fast fluid antenna
multiple access enabling massive connectivity,” IEEE Commun. Lett.,
vol. 27, no. 2, pp. 711–715, 2022.
6
Fig. 4. DOR versus amount of data R for selected of values W and K.
[12] M. Khammassi, A. Kammoun, and M.-S. Alouini, “A new analytical
approximation of the fluid antenna system channel,” IEEE Trans. Wirel.
Commun., 2023.
[13] K. K. Wong, A. Shojaeifard, K.-F. Tong, and Y. Zhang, “Performance
limits of fluid antenna systems,” IEEE Commun. Lett., vol. 24, no. 11,
pp. 2469–2472, 2020.
[14] W. K. New, K.-K. Wong, H. Xu, K.-F. Tong, and C.-B. Chae, “Fluid
antenna system: New insights on outage probability and diversity gain,”
IEEE Trans. Wirel. Commun., 2023.
[15] F. Rostami Ghadi, K.-K. Wong, F. J. L´opez-Mart´ınez, and K.-F. Tong,
“Copula-Based Performance Analysis for Fluid Antenna Systems Under
Arbitrary Fading Channels,” IEEE Commun. Lett., vol. 27, no. 11, pp.
3068–3072, 2023.
[16] J. D. Vega-S´anchez, A. E. L´opez-Ram´ırez, L. Urquiza-Aguiar, and
D. P. M. Osorio, “Novel Expressions for the Outage Probability and
Diversity Gains in Fluid Antenna System,” IEEE Wirel. Commun. Lett.,
pp. 1–1, 2023.
[17] F. R. Ghadi, K.-K. Wong, F. J. Lopez-Martinez, C.-B. Chae, K.-F.
Tong, and Y. Zhang, “A Gaussian Copula Approach to the Performance
Analysis of Fluid Antenna Systems,” arXiv preprint arXiv:2309.07506,
2023.
[18] C. Skouroumounis and I. Krikidis, “Fluid antenna with linear MMSE
channel estimation for large-scale cellular networks,” IEEE Trans. Com-
mun., vol. 71, no. 2, pp. 1112–1125, 2022.
[19] R. Wang, Y. Chen, Y. Hou, K.-K. Wong, and X. Tao, “Estimation of
channel parameters for port selection in millimeter-wave fluid antenna
systems,” in 2023 IEEE/CIC Int. Conf. Commun. in China (ICCC
Workshops).
IEEE, 2023, pp. 1–6.
[20] C. Borda-Fortuny, L. Cai, K. F. Tong, and K.-K. Wong, “Low-cost 3D-
printed coupling-fed frequency agile fluidic monopole antenna system,”
IEEE Access, vol. 7, pp. 95 058–95 064, 2019.
[21] L. Jing, M. Li, and R. Murch, “Compact Pattern Reconfigurable Pixel
Antenna With Diagonal Pixel Connections,” IEEE Trans. Antennas
Propag., vol. 70, no. 10, pp. 8951–8961, 2022.
[22] S. Song and R. D. Murch, “An efficient approach for optimizing
frequency reconfigurable pixel antennas using genetic algorithms,” IEEE
Trans. Antennas Propag., vol. 62, no. 2, pp. 609–620, 2013.
[23] G. L. St¨uber, Principles of mobile communication.
Springer, 2001,
vol. 2.
[24] R. B. Nelsen, An introduction to copulas.
Springer, 2006.
[25] I. S. Gradshteyn and I. M. Ryzhik, Table of integrals, series, and
products.
Academic, 7th ed., 2007.
[26] F. R. Ghadi, K.-K. Wong, F. J. L´opez-Mart´ınez, C.-B. Chae, K.-F. Tong,
and Y. Zhang, “Fluid Antenna-Assisted Dirty Multiple Access Channels
over Composite Fading,” IEEE Commun. Lett., pp. 1–1, 2023.
[27] H.-C. Yang, S. Choi, and M.-S. Alouini, “Ultra-reliable low-latency
transmission of small data over fading channels: A data-oriented analy-
sis,” IEEE Commun. Lett., vol. 24, no. 3, pp. 515–519, 2019.
[28] E. W. Weisstein, “Euler-mascheroni constant,” https://mathworld. wol-
fram. com/, 2002.
"
"We introduce SuperCLUE-Math6 (SC-Math6), a novel Chinese dataset designed to evaluate the mathematical reasoning abilities of large language models. SC-Math6 consists of over 2000 mathematical word problems requiring multi-step reasoning and presents them in a native Chinese context. It introduces a novel scoring scheme based on problem-solving steps and overall accuracy to provide interpretable and fair reasoning levels. Experiments on 12 representative Chinese models reveal clear reasoning level stratification, with top models like GPT-4 exhibiting superior performance. SC-Math6 contributes as the first comprehensive Chinese mathematical reasoning benchmark, advancing the development of intelligent Chinese language models.","Large language models such as GPT-4 have sparked interest in assessing their proficiency in solving reasoning problems. While benchmarks like GSM8K have been influential, they are limited to English and do not sufficiently test multi-step inference. To overcome these limitations and assess Chinese models' mathematical reasoning, we introduce SC-Math6, a Chinese version of GSM8K with enhanced difficulty, diversity, and application scope. SC-Math6 includes 1072 unique problems, natural language solutions, and follow-up questions for interaction assessment. We propose a scoring scheme combining performance over problems with different reasoning steps and overall accuracy to provide interpretable reasoning levels from 1 to 5.","nanExisting benchmarks for language model reasoning skills focus primarily on English, including GSM8K for mathematical reasoning and MATH for complex mathematical problems. There are also general LLM benchmarks like MT-bench and AlpacaEval, as well as reasoning benchmarks for NLP like the WinoGrad Schema Challenge for commonsense reasoning and ARC for scientific question answering. Our work aims to address the gap for Chinese through a systematically designed mathematical reasoning benchmark, complementing these methods and inspiring new model designs and training strategies for enhanced mathematical and general reasoning.nannan","SuperCLUE-Math6 dataset construction involved curating a large pool of Chinese math problems from school exams and books, followed by manual selection based on reasoning requirements and error-free natural language solutions. A total of 1072 unique arithmetic problems were obtained. Multi-turn follow-up questions were designed for each problem, bringing the total size to 2144. Quality control and inspection ensured the accuracy of the questions and solutions. The distribution of reasoning steps was controlled to prevent biases and test various capabilities, with varying textual lengths for problems and solutions.nan","We evaluated 12 major Chinese models on SC-Math6, covering capacities from 13B to Proprietary APIs. Model performance was assessed based on overall accuracy, reasoning steps score, comprehensive score, and resulting reasoning level. Results revealed a clear stratification of capabilities, with top models like GPT-4 demonstrating remarkably high performance on multi-step problems. Lower-level models exhibited large performance gaps. Furthermore, analysis of the step-by-step scores revealed declining performance as problem complexity increased from 1 to 5 reasoning steps, highlighting the need to improve model capabilities in tackling more challenging problems. Additionally, we observed a decline in model accuracy during multi-turn interaction, suggesting the importance of model stability and adaptability in sustained tasks.","SuperCLUE-Math6, the first native Chinese benchmark for assessing multi-step mathematical reasoning skills in language models, introduces diverse problems and reasoning patterns to advance model designs and training strategies. Our work aims to catalyze progress in Chinese models' ability to support real-world applications, providing valuable data for model training and inspiring new methods to enhance reasoning capabilities.",SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese,"Liang Xu, Hang Xue, Lei Zhu, Kangkang Zhao","SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs
in Chinese
Liang Xu, Hang Xue, Lei Zhu, Kangkang Zhao
SuperCLUE team
clues@cluebenchmarks.com
Abstract
We introduce SuperCLUE-Math6(SC-Math6), a new bench-
mark dataset to evaluate the mathematical reasoning abilities
of Chinese language models. SC-Math6 is designed as an up-
graded Chinese version of the GSM8K dataset with enhanced
difficulty, diversity, and application scope. It consists of over
2000 mathematical word problems requiring multi-step rea-
soning and providing natural language solutions. We propose
an innovative scheme to quantify the reasoning capability of
large models based on performance over problems with dif-
ferent reasoning steps. Experiments on 12 representative Chi-
nese models demonstrate a clear stratification of reasoning
levels, with top models like GPT-4 showing superior perfor-
mance. SC-Math6 fills the gap in Chinese mathematical rea-
soning benchmarks and provides a comprehensive testbed to
advance the intelligence of Chinese language models. 1.
1
Introduction
Recent advances in large language models like GPT-4 [1]
have sparked great interest in evaluating their proficiency
in solving reasoning problems. While benchmarks like
GSM8K [2] have been influential, they are limited to English
and do not sufficiently test multi-step inference. To over-
come these limitations and systematically assess the math-
ematical reasoning of Chinese models, we introduce SC-
Math6 as an upgraded Chinese version of GSM8K.
SC-Math6 has 1072 unique problems covering a diverse
range of grade school math topics. Each problem is pre-
sented in a native Chinese context and accompanied by a
detailed natural language solution walkthrough. Moreover,
SC-Math6 provides a follow-up question for each initial
query to assess the model’s continuous reasoning ability dur-
ing interaction with users (As shown in Figure 1. For more
examples, please refer to the appendix A). We also propose a
novel scoring scheme that combines performance over prob-
lems with different reasoning steps and overall accuracy to
produce interpretable and fair reasoning levels from 1 to 5.
Disparities and Correlations of SC-Math6 and GSM8K is
presented in Table 1
Preprint.
1Our
benchmark
can
be
found
at
https://www.
CLUEbenchmarks.com
Comparison Item
SC-Math6
GSM8K
Mathematical Logic Reasoning
YES
YES
Natural Language Solutions
YES
YES
Elementary Mathematical Knowledge
YES
YES
Multi-step Reasoning
YES
YES
Native Chinese Context
YES
NO
Multi-round In-depth Reasoning
YES
NO
Reasoning Steps in Problems
YES
NO
Interpretable Reasoning Level for LLMs
YES
NO
Number of Test Questions
2144 (1072 Pairs)
1300
Table 1: SC-Math6 and GSM8K: Disparities and Correla-
tions
Our experiments on 12 major Chinese models demon-
strate a clear stratification of reasoning capabilities. Ad-
vanced models like GPT-4 exhibit remarkably high accu-
racy on multi-step problems, while lower-level models show
large performance gaps. The diversified grading scheme
provides a reference for model selection and evaluation.
SC-Math6 thus contributes the first comprehensive Chinese
benchmark to assess and improve the mathematical reason-
ing abilities of Chinese language models.
0
20
40
60
80
Comprehensive Score in Math6
0
20
40
60
80
GSM8k Score
GPT_4_1106_Preview
GPT-4
ERNIE-Bot 4.0
GPT-3.5-turbo
ChatGLM-turbo
Qwen-14B-Chat
Baichuan2-13B-Chat
ChatGLM3-6B
Xinghuo 3.0
ERNIE-3.5-turbo
Chinese-Alpaca-2-13B
Likely trained on GSM8k
Pretrained
Figure 2: Trend analysis between GSM8k and SC-Math6.
SC-Math6 aligns with GSM8K yet demands in-depth rea-
soning, whereas some models may struggle with SC-Math6.
arXiv:2401.11819v1  [cs.CL]  22 Jan 2024
Follow-up question ：如果张伟家中每位成年人均多收到一个红包，而红包的金额不变，那么张伟家需要增加多少红包发放金额？
If each adult in the Zhang Wei family were to receive one additional red envelope, with the amount in each envelope remaining unchanged, how much additional money would the 
Zhang Wei family need to allocate for the distribution of red envelopes?
Solution :
从原来的问题中我们知道每个成年人的每个红包包含60元。如果张伟家中每位成年人多收到一个红包，我们需要计算这会导致多出多少额外的发放金额。
既然已知张伟家中有8个成年人，那么新增的红包总数就是8个。
因为每个红包的金额是60元，所以新增的红包金额就是8个红包乘以每个红包的金额。
8个红包乘以60元每个红包可以计算出总的新增金额：8*60=<<8*60=480>>480元。
这480元就是张伟家需要增加的红包发放金额来满足每位成年人多收一个60元的红包。
Initial Question ：
张伟家族春节聚会时，每位家庭成员收到了5个红包，每个红包内有相同数量的现金。如果张伟家共有8个成年人和6个小孩，而成年人收到的钱是小孩的两倍，且张伟家只用了
整数人民币，小孩每个人共收到了150元，那么张伟家共发放了多少元红包
From the original problem, we know that each red envelope for an adult contains 60 Yuan. If each adult in the Zhang Wei family were to receive one additional red envelope, we need to 
 calculate the extra amount of money this would require for distribution.
Given that there are 8 adults in the Zhang Wei family, the total number of additional red envelopes would be 8.
Since each red envelope is valued at 60 Yuan, the total additional amount is calculated by multiplying the number of additional red envelopes by the amount in each envelope.
Multiplying 8 additional red envelopes by 60 Yuan per envelope gives us the total additional amount: ( 8*60=<<8*60=480>>480 ) Yuan.
This 480 Yuan is the additional amount the Zhang Wei family would need to allocate to satisfy one extra red envelope containing 60 Yuan for each adult.
Solution :
首先，计算一个小孩收到的单个红包的金额。既然每个小孩一共收到了150元，并且收到了5个红包，那么每个红包里有150/5=<<150/5=30>>30元。
既然成年人收到的钱是小孩的两倍，那么每个成年人的每个红包里有30*2=<<30*2=60>>60元。
接着计算家中所有成年人收到的总金额。一共有8个成年人，每个成年人收到了5个红包，每个红包60元，所以成年人总共收到8*5*60=<<8*5*60=2400>>2400元。
最后，把成年人和小孩的金额加起来。小孩一共有6个，每个小孩收到了150元，所以小孩总共收到6*150=<<6*150=900>>900元。
成年人和小孩收到的金额加起来就是张伟家共发放的红包金额，即2400+900=<<2400+900=3300>>3300元。
First, calculate the amount of money in a single red envelope received by a child. Since each child received a total of 150 Yuan and they received 5 red envelopes, each envelope contains 150/5 
=<<150/5=30>>30 Yuan.
Given that the adults received twice as much money as the children, each adult's red envelope contains 30*2=<<30*2=60>>60 Yuan.
Next, calculate the total amount received by all the adults in the family. With 8 adults each receiving 5 red envelopes containing 60 Yuan each, the total amount the adults received is 8*5*60=<
<8*5*60=2400>>2400 Yuan.
Finally, add together the amounts received by the adults and the children. There are 6 children, each receiving 150 Yuan, so the total amount received by the children is 6*150=<<6*150=900>>
900 Yuan.
The sum of the amounts received by both the adults and the children is the total amount of red envelope money distributed by the Zhang Wei family, which equals 2400+900=<<2400+900=330 
0>>3300 Yuan.
?
During the Zhang Wei family's Spring Festival gathering, each family member received 5 red envelopes, each containing an equal amount of cash. With 8 adults and 6 children in the 
Zhang Wei family, and with adults receiving twice as much money as the children, and considering that only integer amounts of Chinese Yuan were used, if each child received a total 
of 150 Yuan, how much money in total did the Zhang Wei family give out in red envelopes?
Final Answer: 3300
Reasoning Steps: 
Final Answer: 480
5
Reasoning Steps: 2
Figure 1: An example of a problem in SC-Math6
This work pioneers the systematic evaluation and bench-
marking of mathematical reasoning capabilities of major
Chinese language models. The key contributions are three-
fold:
First, the construction of SuperCLUE-Math6, the first na-
tive Chinese multi-turn, multi-step mathematical reasoning
dataset for assessing model logical thinking and reasoning
skills.
Second, the proposal of a novel transparent and consis-
tent framework to parse and evaluate model reasoning levels,
providing quantifiable metrics of model intellectual capabil-
ities.
Third, comprehensive benchmarking and analysis of lead-
ing Chinese models on SuperCLUE-Math6, offering valu-
able insights into current model strengths, weaknesses and
factors impacting reasoning performance.
Overall, this research fills the gap in Chinese mathemati-
cal reasoning evaluation and establishes an important bench-
mark for advancing the reasoning abilities of Chinese lan-
guage models. The benchmark and insights lay a solid foun-
dation for developing models with more human-like reason-
ing.
2
SuperCLUE-Math6
Data Collection
We first curated a large pool of Chinese math problems from
elementary school exams and books and altered it manually
to ensure it was unique. Two criteria were then applied to
select problems requiring: 1) At least one step of reasoning,
and 2) Error-free natural language solutions. This yielded
1072 unique arithmetic problems.
To evaluate the model’s proficiency in sustained infer-
ential reasoning throughout the interactive engagement, we
designed multi-turn follow-up questions for each problem,
bringing the total size to 2144.
Quality Control and Inspection of Questions. In the sec-
ond round process, all questions were subjected to manual
verification, which required the annotators to solve the ques-
tions themselves and record their answers. These were then
compared with the provided reference answers and solu-
tion steps. If any inconsistencies were discovered, the prob-
lem was pinpointed and corrected. Corrections were made
if there were issues with the answer or solution steps; if the
question itself was ambiguous, it required clarification. Once
consistency was confirmed, the process advanced to the next
question. After the manual verification, a final round of ran-
dom sampling checks was conducted. Out of 50 pairs of
questions, 1 pair was found to potentially have ambiguities
and needed to be corrected, giving us a sampling accuracy
2
of 98%.
The distribution of reasoning steps is controlled to prevent
biases and test varied capabilities: 15-20% with 1 step, 15-
20% with 2 steps, 45-50% with 3 steps, and 5-10% with
4-5 steps. The textual lengths of problems and solutions also
exhibit high variability.
Scoring Scheme and Reasoning Level
To produce interpretable and fair quantification of reasoning
capabilities, we propose a scoring scheme that combines:
• Reasoning Steps Score: Higher weight assigned for more
steps based on the insight that longer reasoning chains
are more difficult. For example, we assign 5 as the weight
of a problem if the reasoning steps of the problem are 5.
Then, the final Reasoning Steps Score for each model is
the weighted average score of all test problems.
• Overall Accuracy Score: The Overall Average Score is
derived as the mean value of the Mean Accuracy and
the Strict Interaction Accuracy. Mean Accuracy is com-
puted by considering each question and its correspond-
ing follow-up question as two separate items, thus calcu-
lating the average accuracy across 2144 questions. Con-
versely, Strict Interaction Accuracy is calculated by treat-
ing the question and its follow-up question as a unified
interactive pair, with a point awarded only if both the
question and the follow-up question are correctly an-
swered, demonstrating proper reasoning. This Strict In-
teraction Accuracy is evaluated over a cohort of 1072 test
pairs to establish the average interaction accuracy.
• Comprehensive Score: Calculated as the weighted sum
of the Reasoning Steps Score and the Overall Accuracy
Score, each component contributing equally to the final
score.
• Reasoning Level: The Reasoning Level of a language
model is based on the Comprehensive Score, with lev-
els ranging from 1 to 5, where level 5 is the highest and
level 1 is the lowest. A threshold of 0.05 points is used to
determine the levels. If the composite scores of the two
models differ by less than 0.05 points, they are consid-
ered to be within the same level. This provides a trans-
parent system to classify model capabilities.
The accuracy score is a commonly used evaluation metric
to quantify the mathematical reasoning abilities of language
models, but it fails to account for the varying difficulty levels
of individual questions. Solving a more challenging problem
should be awarded more points than solving an easier one,
and the number of reasoning steps involved usually corre-
lates with the difficulty level. Therefore, we incorporate the
Reasoning Steps Score into our assessment framework.
The advantage of the Reasoning Step Score lies in its
ability to account for the varying difficulty levels of differ-
ent questions. This score is validated by manual problem-
solving, ensuring a high level of accuracy in the reasoning
steps involved. Given that mathematical problems may have
multiple solution methods, each with a potentially different
number of reasoning steps, the reasoning step count is not
necessarily unique. Therefore, the calculation of Reasoning
Step Scores cannot eliminate the discrepancies in weighted
precision.
On the other hand, the Overall Accuracy Score, while
not considering the difficulty level of each question, ensures
fairness and avoids bias that might be introduced during the
weighting process. Therefore, we have not completely dis-
carded the Overall Accuracy Score. Instead, we employ a
weighted summation of Reasoning Steps Score and Overall
Accuracy Score to calculate the unified score. This method
balances the precision given by the Reasoning Steps Score
with the fairness ensured by the Overall Accuracy Score,
aiming to provide a more comprehensive evaluation of math-
ematical reasoning performance.
Experiments and Analysis
We evaluated 12 major Chinese models on SC-Math6 cov-
ering capacities from 13B to Proprietary APIs. Table
2
presents the overall accuracy, Reasoning Steps Score, Com-
prehensive Score, and resulting Reasoning Level. Informa-
tion on models is shown in Table 3.
Model Name
R Level
Comp. Score
Reas. Steps Score
OvrAcc Score
GPT-4-1106-Preview
5
87.76
88.60
86.92
GPT-4
5
83.86
83.60
84.12
GLM-4
5
81.72
82.67
80.76
Ernie-bot 4.0
5
79.64
80.73
78.55
GPT-3.5-Turbo
4
53.75
54.87
52.64
ChatGLM-Turbo
4
52.55
53.60
51.49
Qwen-14B-Chat
4
49.05
49.73
48.37
Xinghuo 3.0
3
37.39
40.87
33.91
Baichuan2-13B-Chat
3
36.76
38.40
35.12
ChatGLM3-6B
3
33.03
34.13
31.92
Ernie-3.5-turbo
2
21.01
22.20
19.82
Chinese-Alpaca2-13B
2
18.09
18.67
17.51
Table 2: SC-Math6 Model Reasoning Level. ’R Level’ for
Reasoning Level, ’Comp. Score’ stands for Comprehensive
Score, ’Reas. Steps Score’ stands for Reasoning Steps Score,
’OvrAcc Score’ stands for Overall Accuracy Score.
Model Name
Organization
Access
GPT-4-1106-Preview
OpenAI
API
GPT-4
OpenAI
API
GLM-4
ZhiPu
Web Page
Ernie-bot 4.0
Baidu
API
GPT-3.5-Turbo
OpenAI
API
ChatGLM-Turbo
ZhiPu
API
Qwen-14B-Chat
Alibaba
API
Baichuan2-13B-Chat
Baichuan
Weight
ChatGLM3-6B
ZhiPu
Weight
Xinghuo 3.0
Iflytek
API
Ernie-3.5-turbo
Baidu
Weight
Chinese-Alpaca2-13B
Yiming Cui
Weight
Table 3: Model information
Top models like GPT-4 exhibit remarkably high perfor-
mance on multi-step problems, demonstrating advanced
reasoning skills. There is also a clear stratification of
capabilities,
with
higher-scoring
models
significantly
outperforming lower ones. The diverse levels allow the
selection of appropriate models based on application
requirements.
3
We highlight several observations:
1) Comparison between GSM8k Score and Comprehen-
sive Score in SC-Math6
The comparative analysis of performance on the GSM8k
and SC-Math6 benchmark. SC-Math6 aligns with GSM8K
yet demands more advanced reasoning, whereas models ex-
celling on GSM8K may struggle on SC-Math6. It suggests
that the SC-Math6 benchmark presents a greater level of
difficulty. It was observed that, across the board, models
tend to score lower on the SC-Math6 benchmark compared
to GSM8k, with this trend being particularly pronounced
for the ChatGLM3-6B and Baichuan2-13B-Chat models(As
shown in Figure 2).
2) Performance Declining during Multi-turn Interaction
In all models observed, the accuracy rate of the second it-
eration generally falls below that of the first, indicating a de-
cline in model performance with increasing task complexity
from the first to the second iteration(As shown in Table 4).
This trend is ubiquitous across all models, suggesting that
special attention should be given to the stability and adapt-
ability of models in sustained tasks during their design and
optimization processes.
Model Name
Accuracy of Turn 1
Accuracy of Turn 2
Difference
GPT-4-1106-Preview
94.22
86.10
-8.12
GPT-4
91.70
83.77
-7.93
GLM-4
92.35
78.64
-13.71
Ernie-bot 4.0
89.74
77.80
-11.94
GPT-3.5-Turbo
72.48
50.19
-22.29
ChatGLM-Turbo
71.92
49.16
-22.76
Qwen-14B-Chat
72.31
44.06
-28.25
Baichuan2-13B-Chat
59.24
31.06
-28.18
ChatGLM3-6B
56.72
28.44
-28.28
Xinghuo 3.0
69.12
25.47
-43.65
Chinese-Alpaca2-13B
33.21
16.32
-16.89
Ernie-3.5-turbo
43.00
17.26
-25.74
Table 4: SC-Math6 Accuracy during interaction
For instance, the GPT-4-1106-Preview model exhibited
a first iteration accuracy rate of 94.22%, which decreased
to 86.10% in the second iteration, marking an 8.12% reduc-
tion in accuracy. Similarly, the ERNIE 35 Turbo model’s
accuracy rate declined from 43.00% in the first iteration to
17.26% in the second, constituting a 25.74% decrease.
3) Correlation between Instruction Compliance Ratio and
Comprehensive Score
Models with high compliance to the instructed output
formats also tend to achieve higher Comprehensive Scores,
suggesting instruction understanding as an important indi-
cator(As shown in Figure 3).
4) The Potential Relationship between Mathematical Rea-
soning Proficiency and Response Length
It has been observed that models yielding longer average
response lengths tend to receive higher evaluation scores(As
shown in Figure 4). In certain models, such as GPT-4-1106-
Preview, a higher accuracy rate is accompanied by a longer
average response length, which may suggest that these mod-
els are more precise when generating comprehensive re-
0
20
40
60
80
100
Instruction Compliance Ratio
10
20
30
40
50
60
70
80
90
Comprehensive Score of Math6
Pearson Score: 0.72
P_value: 0.008
Figure 3: Correlation between instruction compliance ratio
and Comprehensive Score
sponses. However, this trend is not consistently observed
across all models, indicating that the relationship between
response length and accuracy rate may be influenced by a
multitude of factors, including the design of the model and
the training data employed.
50
100
150
200
250
300
Average Length of Model Answer
10
20
30
40
50
60
70
80
90
Comprehensive Score of Math6
Pearson Score: 0.78
P_value: 0.003
Figure 4: Relation between response length and Comprehen-
sive Score
The GPT-4-1106-Preview exhibits an average response
length of 173.48, correlating with a higher accuracy rate,
while the ChatGLM3-6B shows a comparatively shorter
average response length of 56.29, with a corresponding
lower accuracy rate. This implies that, in certain instances,
there may be a correlation between response length and
accuracy rate.
5) Performance Declining with Reasoning Steps Increas-
4
ing
1
2
3
4
5
6
Reasoning Steps
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Mean Accuracy of Model
GPT_4_1106_Preview
GPT-4
GLM-4
ERNIE-Bot 4.0
GPT-3.5-turbo
ChatGLM-turbo
Qwen-14B-Chat
Baichuan2-13B-Chat
ChatGLM3-6B
Xinghuo 3.0
ERNIE-3.5-turbo
Chinese-Alpaca-2-13B
Figure 5: Relation between reasoning steps and Mean Accu-
racy
Analysis of the step-by-step scores reveals declining
performance as problem complexity increases from 1 to
5 reasoning steps(As shown in Figure 5). This highlights
the need to improve the model’s capability to tackle more
challenging problems requiring more reasoning steps. The
results provide comprehensive insights to guide further
progress on mathematical and general reasoning for Chinese
language models.
3
Related Work
Benchmarks to evaluate the reasoning skills of language
models have gained increasing research attention. Existing
datasets mostly focus on English, including GSM8K for
mathematical reasoning [2], MATH for complexity mathe-
matical problem [3]. For general LLMs benchmark, we can
find MT-bench [4], AlpacaEval [5]. We can find reasoning
benchmarks for NLP, such as WinoGrad Schema Challenge
for commonsense reasoning
[6], and ARC for scientific
question answering [7]. Our work aims to close the gap for
the Chinese through a systematically designed mathematical
reasoning benchmark. Our focus is to provide a benchmark
to evaluate the general reasoning skills of Chinese language
models. The diverse problems and reasoning patterns in SC-
Math6 complement these methods to inspire new model de-
signs and training strategies targeting enhanced mathemati-
cal intelligence.
4
Conclusion
We present SC-Math6 as the first native Chinese benchmark
dataset for assessing the multi-step mathematical reasoning
skills of language models in a multi-turn interaction. Devel-
oping human-like intelligence requires rich, diverse datasets
like SC-Math6 that test sophisticated capabilities beyond
pattern recognition. Our work aims to catalyze advances in
Chinese models to better support real-world applications.
The human-annotated natural language solutions also pro-
vide valuable data for training. We hope SC-Math6 will in-
spire exciting new model designs and training strategies tar-
geting enhanced reasoning.
References
[1] OpenAI.
2023.
GPT-4
Technical
Report.
arXiv:2303.08774.
[2] Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mo-
hammad and Chen, Mark and Jun, Heewoo and Kaiser,
Lukasz and Plappert, Matthias and Tworek, Jerry
and Hilton, Jacob and Nakano, Reiichiro and others,
2021. Training verifiers to solve math word problems.
arXiv:2110.14168.
[3] Hendrycks, Dan and Burns, Collin and Kadavath,
Saurav and Arora, Akul and Basart, Steven and Tang,
Eric and Song, Dawn and Steinhardt, Jacob, 2021. Mea-
suring mathematical problem solving with the math
dataset. arXiv:2103.03874.
[4] Zheng, Lianmin and Chiang, Wei-Lin and Sheng,
Ying and Zhuang, Siyuan and Wu, Zhanghao and
Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and
Li, Dacheng and Xing, Eric and others, 2023. Judging
LLM-as-a-judge with MT-Bench and Chatbot Arena.
arXiv:2306.05685.
[5] Li, Xuechen and Zhang, Tianyi and Dubois, Yann and
Taori, Rohan and Gulrajani, Ishaan and Guestrin, Car-
los and Liang, Percy and Hashimoto, Tatsunori B, 2023.
Alpacaeval: An automatic evaluator of instruction-
following models. GitHub repository.
[6] Keisuke Sakaguchi and Ronan Le Bras and Chan-
dra Bhagavatula and Yejin Choi, 2019. An Ad-
versarial
Winograd
Schema
Challenge
at
Scale.
arXiv:1907.10641.
[7] Peter Clark and Isaac Cowhey and Oren Etzioni
and Tushar Khot and Ashish Sabharwal and Carissa
Schoenick and Oyvind Tafjord, 2018. Think you have
Solved Question Answering? Try ARC, the AI2 Rea-
soning Challenge. arXiv:1803.05457.
A
Examples for problem in SC-Math6
Below are examples of problems in SuperCLUE-Math6.
5
Follow-up question ：如果突然来了3个客人，妈妈打算只做鸡翅，请问她还需要做几个鸡翅。
If three more guests suddenly arrive and mom plans to prepare only chicken wings, how many more chicken wings does she need to make?
Solution :
每个鸡翅可以供1个人吃，来了3个客人，3/1=<<3/1=3>>3个。
Each chicken wing is enough for one person. With 3 guests arriving, that would be 3/1=3 chicken wings needed.
Solution :
First, let's calculate how many people can eat the steamed buns.
Mom prepared 10 steamed buns, and each bun can be shared by 2 people, so in total the buns can serve 102=20 people.Next, let's calculate how many people can eat the chicken wings.Mom 
prepared 5 chicken wings, and each wing is enough for 1 person, so the chicken wings in total can serve 51=5 people.
Finally, by adding together the number of people who can be served with the buns and the chicken wings, we get the maximum number of people who can be served with the food mom 
prepared.
首先，计算一下包子一共能够供多少人食用。
妈妈准备了10个包子，每个包子可以供2个人吃，所以包子总共可以供10*2=<<10*2=20>>20个人吃。然后，计算一下鸡翅一共能够供多少人食用。
妈妈准备了5个鸡翅，每个鸡翅可以供1个人吃，所以鸡翅总共可以供5*1=<<5*1=5>>5个人吃。
最后，将包子和鸡翅能供食用的人数相加，就是妈妈准备的食材最多可以供多少个人吃饭。
包子和鸡翅一共可以供20+5=<<20+5=25>>25个人吃饭。
Together, the steamed buns and chicken wings can serve 20+5=25 people.
For the Spring Festival family dinner at home, mom prepared 10 steamed buns, each of which can be shared by 2 people. She also prepared 5 chicken wings, with each wing enough 
Initial Question ：
在家里安排春节聚餐，妈妈准备的食材是10个包子，每个包子可以供2个人吃，她还准备了5个鸡翅，每个鸡翅只能供1人吃，那么妈妈准备的食材最多可以供多少个人吃饭？
for 1 person. How many people in total can be served with the food prepared by mom?
Final Answer: 25
Final Answer: 3
Reasoning Steps: 3
Reasoning Steps: 1
Figure 6: An example of a problem in SC-Math6
 
 
 
首先找出张伟剩余假期阅读的页数，他计划在剩余的45天每天阅读30页，所以这45天他将阅读45 * 30 =<<45 * 30 =1350>>1350页。
然后，我们将这1350页与他在前15天阅读的600页相加，也就是说他的总阅读数量将是1350 + 600 =<<1350 + 600 =1950>>1950页。
最后，我们将这个数量与他原计划的1800页相比，也就是说他的阅读数量将超过他原先的计划1950 - 1800 =<<1950 - 1800 =150>>150
Follow-up question ：如果张伟决定在剩下的假期里，每天阅读30页的书，他的总阅读数量将超过他原先的计划多少页？
If Zhang Wei decides to read 30 pages a day for the rest of the vacation, by how many pages will his total reading amount exceed his original plan?
页。
 
 
首先，我们需要计算张伟整个暑假计划要阅读的总页数，60天乘以每天30页，得到：60*30=<<60*30=1800>>1800页。
然后，计算前15天他实际阅读了多少页，15天乘以每天40页，得到：15*40=<<15*40=600>>600页。
接着我们需要找出剩下的假期天数，总天数60天减去已经过去的15天，得到：60-15=<<60-15=45>>45天。
现在，我们知道剩下的天数里张伟还需要阅读多少页，就是总计划阅读页数减去前15天已经阅读的页数，得到：1800-600=<<1800-600=1200>>1200页。
最后，将剩下需要阅读的页面数1200页平均分配到剩下的45天里，计算每天应该阅读多少页：1200/45=<<1200/45=26.67>>26.67页。
因为我们需要的答案是正整数，所以将小数点后面的数四舍五入，得到每天应该阅读27
First, we find out how many pages Zhang Wei will read during the remaining vacation days. He plans to read 30 pages each day for the remaining 45 days, which means he will read 45 
* 30 = 1350 pages in those 45 days.
Next, we add these 1350 pages to the 600 pages he read in the first 15 days. This means his total reading amount will be 1350 + 600 = 1950 pages.
Finally, we compare this total with his original plan of 1800 pages. This means that his reading amount will exceed his initial plan by 1950 - 1800 = 150 pages
Solution :
页。
First, we need to calculate the total number of pages Zhang Wei plans to read during the entire summer vacation, which is 60 days times 30 pages per day, resulting in: 60*30=1800 pages.
Next, we calculate how many pages he actually read in the first 15 days, which is 15 days times 40 pages per day, resulting in: 15*40=600 pages.
Then we need to find out the number of remaining vacation days, which is the total of 60 days minus the 15 days that have passed, giving us: 60-15=45 days.
Now, we know how many more pages Zhang Wei needs to read in the remaining days, which is the total planned pages minus the pages already read in the first 15 days, resulting in: 1800-600=
1200 pages.
Finally, we distribute the remaining 1200 pages to be read over the remaining 45 days, calculating how many pages should be read per day: 1200/45=26.67 pages.
Since we need the answer to be a whole number, we round the number after the decimal point and get that he should read 27 pages per day.
Zhang Wei plans to read 30 pages of a book every day during the 60 days of summer vacation. If Zhang Wei reads 40 pages each day for the first 15 days of the vacation, how many 
Initial Question ：
张伟计划在暑假的60天内，每天阅读30页的书。如果张伟在假期的前15天每天阅读40页的书，那么他在剩下的假期天数里，每天应阅读多少页的书才能达到他的阅读计划?
pages per day should he read during the remaining days of the vacation to meet his reading goal?
Final Answer: 27
Reasoning Steps: 5
Final Answer: 150
Reasoning Steps: 3
Solution :
Figure 7: An example of a problem in SC-Math6
6
"
"We present SYMBRAIN, a novel annotated dataset of MRI images comprising over 3000 axial slices extracted from the Developing Human Connectome Project (dHCP) dataset, designed to promote research and development in automated brain symmetry analysis specific to neonatal MRI. The data set's primary focus is detecting midline structures and anomalies in newborn brain scans, facilitating more accurate diagnoses. These annotations enable the training of deep learning models for anomaly detection, leading to improved diagnostic assistance for radiologists, more precise treatment planning, and the potential for earlier detection of developmental disorders or diseases.","Brain asymmetry and its impact on diagnosing neonatal patients are of increasing interest in the field of medical imaging. Common visual assessment scales for magnetic resonance imaging (MRI) of the brain lack standardized quantitative measures, hindering automated analysis. The proposed dataset addresses this need by providing annotated MRI slices with marked midline structures, promoting the development of robust and accurate algorithms for detecting brain symmetry.","The literature review covers research exploring brain symmetry and asymmetry, highlighting the importance of analyzing brain hemispheres and regions of interest for disease diagnosis. It also discusses the challenges of neonatal MRI, including rapid developmental changes and low image quality, necessitating the creation of a specialized dataset catered to this unique population.nannannan","The methodology section outlines the data acquisition process from the dHCP, focusing on T1-weighted and T2-weighted MRI scans. It explains the slicing procedure to extract cross-sectional views from the 3D MRI volumes and details the manual annotation process using the V7lab annotator software. The two types of annotations include curved and straight lines representing the midline.nan","The results section provides dataset statistics and demonstrates the utility of the SYMBRAIN dataset in assisting radiologists with diagnosing anomalies in neonatal brain MRI scans. It illustrates the potential of deep learning models trained on the dataset in identifying and classifying anomalies, aiding in the detection of potential disease indicators.","The conclusion reiterates the significance of the SYMBRAIN dataset in advancing research and development efforts related to brain symmetry analysis in neonatal MRI. It emphasizes the dataset's potential to improve diagnostic accuracy, assist clinical decision-making, and contribute to a more precise evaluation of brain abnormalities, ultimately leading to better patient outcomes.",Symbrain: A large-scale dataset of MRI images for neonatal brain symmetry analysis,"Arnaud Gucciardi, Safouane El Ghazouali, Francesca Venturini, Vida Groznik, Umberto Michelucci","arXiv:2401.11814v1  [cs.CV]  22 Jan 2024
SYMBRAIN: A LARGE-SCALE DATASET OF MRI IMAGES
FOR NEONATAL BRAIN SYMMETRY ANALYSIS
Arnaud Gucciardi∗1,2, Safouane El Ghazouali2, Francesca Venturini2,3, Vida Groznik1,4, and Umberto Michelucci2
1University of Ljubljana, Faculty of Computer and Information Science, Ljubljana, Slovenia
2TOELT llc, Machine Learning Research and Development LAB, Dübendorf, Switzerland
3Institute of Applied Mathematics and Physics, Zurich University of Applied Sciences, Winterthur, Switzerland
4Faculty of Mathematics, Natural Sciences and Information Technologies, University of Primorska, Koper, Slovenia
ABSTRACT
This paper presents an annotated dataset of brain MRI images designed to advance the ﬁeld of brain
symmetry study. Magnetic resonance imaging (MRI) has gained interest in analyzing brain symme-
try in neonatal infants, and challenges remain due to the vast size differences between fetal and adult
brains. Classiﬁcation methods for brain structural MRI use scales and visual cues to assess hemi-
sphere symmetry, which can help diagnose neonatal patients by comparing hemispheres and anatom-
ical regions of interest in the brain. Using the Developing Human Connectome Project dataset, this
work presents a dataset comprising cerebral images extracted as slices across selected portions of
interest for clinical evaluation . All the extracted images are annotated with the brain’s midline. All
the extracted images are annotated with the brain’s midline. From the assumption that a decrease
in symmetry is directly related to possible clinical pathologies, the dataset can contribute to a more
precise diagnosis because it can be used to train deep learning model application in neonatal cerebral
MRI anomaly detection from postnatal infant scans thanks to computer vision. Such models learn
to identify and classify anomalies by identifying potential asymmetrical patterns in medical MRI
images. Furthermore, this dataset can contribute to the research and development of methods using
the relative symmetry of the two brain hemispheres for crucial diagnosis and treatment planning.
Keywords Brain MRI · Image symmetry · Image analysis · Anomaly detection
1
Introduction
The asymmetry of the brain presents an intriguing paradox. In general, the bodies and brains of most organisms,
including humans, exhibit pronounced bilateral symmetry (right and left). This bilateral symmetry is essentially the
normative state [5]. The human brain, on the other hand, exhibits a distinct bilateral symmetry in shape, but a stunning
asymmetry in function [10, 13, 19]. The brain’s left hemisphere is predominantly responsible for logical reasoning,
language processing, and analytical tasks, while the right hemisphere excels in creative thinking, spatial ability, and
holistic processing [5]. In general, brain images obtained from the axial planes of the MRI images show a pronounced
bilateral symmetry, a property used in multiple studies to diagnose various brain conditions, such as dementia [16], to
study cognitive processes [22], neural disorders [18], and developmental abnormalities or diseases [7].
Magnetic resonance imaging (MRI) is one of the most crucial techniques [11, 20] used to study the brain. In particular,
MRI has been used to study brain disorders and development issues in term and preterm infants [21, 11, 14]. Neonatal
cerebral MRI presents several practical and technical challenges. Speciﬁcally, the brains of babies and adults differ
enormously in terms of size, with the fetal and neonatal brain covering a volume in the range of 100 to 600 ml,
in contrast to the average volume of an adult brain of more than 1 liter [3]. Additionally, MRI imaging of infants’
brains presents unique challenges due to the rapid developmental changes and higher water content, which modify the
contrast and clarity of images. Furthermore, the constant motion of infants, including breathing and slight movements,
∗arnaud.gucciardi@toelt.ai
can lead to motion artifacts, complicating image acquisition and analysis. In contrast, adult brain MRI faces challenges
primarily related to age-related changes such as brain atrophy and the presence of pathologies, which require a more
nuanced interpretation of the images. For these reasons, despite its importance, the availability of data sources for
brain volume MRI for infants is scarce. Among the few available images, there remains a need for comprehensive
datasets dedicated to brain symmetry detection, hindering the development and evaluation of automated algorithms
for this task. This work addresses this need.
To describe the structures of the brain in newborns and children in development, the classiﬁcation methods of brain
structural magnetic resonance imaging rely on scales and cues that qualitatively assess the symmetry of the brain
hemispheres. Scales combine manual subscore assessments on symmetrical regions of the axial view of the brain
volume to provide a ﬁnal evaluation score. The stable reliability of the scale subscores makes it suitable for disease-
speciﬁc classiﬁcation questions [12], such as cerebral palsy. Such visual semi-quantitative scales for the classiﬁcation
of brain MRI are applied to children’s clinical data as consistent methods to quantify imaging ﬁndings in terms of
brain lesion severity [4]. Inspired by standardized scales with detailed quantitative neuroanatomical characterization
to examine the relationship between structure and function in children [12], comparison analysis of the hemispheres
and anatomical regions of interest in the brain can help diagnose neonatal patients. Such scales have moderately high
interrater reliability, supporting their use for further evaluation of automatic symmetry methods and examining the
relationship between brain structure and function [12].
Despite the signiﬁcance of brain symmetry, existing magnetic resonance datasets primarily focus on healthy adults
and the general understanding of brain structure. The properties of healthy young adults’ brain have been examined
and used to describe how the brain typically grows and connects during childhood and the transition through puberty
to adolescence and young adulthood. Some popular large-scale projects and datasets, such as the Human Connectome
Project (HCP) [24] and the Nathan Kline Institute (NKI) [23], provide extensive MRI data for studying brain networks
and organizational patterns. However, they only concern adults or developing children; they do not speciﬁcally target
brain symmetry at birth. The largest available volume dataset for neonatal cerebral images and the one used as the
source for this dataset is the Developing Human Connectome Project (dHCP) [17], based on the principles of the HCP,
with differences in protocols to adapt to neonatal patients.
This void in the availability of large-scale brain MRI datasets dedicated to brain symmetry detection motivates the cre-
ation of a comprehensive repository of annotated magnetic resonance images designed to facilitate the development
and evaluation of automated algorithms for detecting the symmetry axis within brain MRI data. The proposed anno-
tated data set presents midline annotations in two-dimensional volume slices. This data set signiﬁcantly contributes to
various applied areas of computer science and clinical areas. The data set is conveniently accessible to train and val-
idate machine learning algorithms to automatically detect midlines in brain MRI images, speciﬁcally newborn MRIs.
The detection of anomalies or outliers can help radiologists in their diagnosis and save time in image interpretation.
Algorithms trained on such datasets can serve as a decision support system, potentially reducing diagnostic errors. ,
the dataset can be used to study variations in midline structures across different populations, ages, and health condi-
tions, contributing to a better understanding of anatomical variability. It can help medical research, diagnosis, and
treatment. Furthermore, the dataset can support research in various medical ﬁelds, fostering a deeper understanding
of diseases affecting midline structures and facilitating the development of new diagnostic and treatment methods in
which researchers can use the dataset to innovate and develop new techniques for image analysis, potentially leading
to improvements in automated medical imaging.
2
Dataset Description
As previously stated, the dataset created in this paper stems from the dHCP [17], a large-scale effort to map the neural
connections in the human brain during development. The project leveraged cutting-edge MRI techniques, including
diffusion, structural, and functional MRI, to gather rich datasets from hundreds of participants across different age
ranges. The annotations concern the structural MRI available as T1-weighted (T1w) and T2-weighted (T2) MRIs.
T1w and T2w are the most commonly used MRI sequence modalities in the clinical visualization ﬁeld. The T1w and
T2w modalities represent two fundamental types of magnetic resonance imaging (MRI) sequences that are pivotal for
capturing and visualizing various characteristics of brain tissue and its developmental processes. T1w MRI enhances
the signal of fatty tissue, such as brain matter, and suppresses the signal of the water, such as the cerebrospinal ﬂuid
located between brain matter and skull. T2w MRI, on the other hand, enhances the signal of the water. The original
dHCP data set comprises a combined set of 1050 fetal and neonatal T1w and T2w volumetric scans, precisely 492 T1w
and 558 T2w image volumes are accessed. These scans have been completed following a similar protocol in each case
and have been collected from diverse populations, including healthy term-born infants, preterm infants, and fetuses
with known congenital anomalies. The scans were performed at multiple sites using state-of-the-art MRI machines
and protocols speciﬁcally designed for neonatal and fetal imaging. All anatomical images for all dHCP subjects have
2
had motion-corrected reconstruction [9]. The volume information from the original dataset can be visualized and
extracted using different software. This work used the Python package NiBabel [6]. Its API gives full access to header
information (metadata) and image volume data as three-dimensional arrays. The raw image volume data from each
of the T1w and T2w scans is retrieved as three-dimensional arrays. From the entire array of data, we select the depth
of interest for visualization in the coronal view, as these are the views typically used in clinical evaluations [12]. The
volumes are of shape (290, 290, 203) pixels, with the third dimension being the z-axis, the coronal plane along which
the cross-sectional slices are selected.
3
Materials and methods
3.1
Slicing
To perform the slicing process on the three-dimensional dHCP volumes, three distinct depths along the vertical (also
named frontal or coronal) axis are selected. A two-dimensional cross-section from the three-dimensional image vol-
ume is extracted for each depth level, resulting in three two-dimensional cross-sectional slices, per subject (Fig. 1).
Figure 1: Representative visualization of the slices on the T1w and T2 volumes, at the three different depths selected.
D1: 76px 1, D2: 101px 2, D3: 126px.
The three chosen depths on the coronal plane are the D1, D2, and D3 views along the vertical plane, as seen in Fig
1. Since the volume shape is (290, 290, 203), the three selected depths represent the axial view at the 101, 76, and
126 z-axis depth (measured in pixels). Three two-dimensional sections were extracted from each of the 1050 volumes,
creating a total of 3150 two-dimensional axial brain views that can be used for further image analysis. Of the 3150
images, 1476 are T1w-type, and 1674 are T2w-type. Each slice provides a snapshot of the brain’s structure at a
particular depth and can be analyzed separately to identify patterns and abnormalities.
The annotation process for medical images can be complex and often time-consuming. The goal is to provide a clear
and straightforward labeling process that helps computer vision models learn to identify and classify different features
within the images. In the case of brain anomalies, measuring and identifying potential symmetrical patterns in medical
MRI images is crucial for diagnosis and treatment planning.
To annotate the images, the V7lab annotator software [2] was used to manually draw lines and curves on the images
to highlight areas of interest. In the v7lab, the Polyline tool is used to draw a straight line, composed of 2 points,
on each slice. As illustrated in Fig.2, these annotations provide a visual representation of the areas of interest within
the images. They serve as a rich and detailed reference for the deep learning model, offering precise guidance on the
anatomical structures to focus on during the learning process.
3
T2w
T2w
T1w
T1w
Line annotation
Curved annotation
Figure 2: Comparison of straight and curved midline manual annotation on T1w and T2w slice samples. Left: straight
line annotation with two points. Center: curved annotation made of nine control points. Right: visual comparison of
the two annotations. Even on a seemingly symmetric brain image, minor curvature changes are visible.
4
3.2
Annotations
Types of annotations:
• Lines of Symmetry: The ﬁrst type of annotation involves drawing lines that connect two points on opposite
sides of the image, indicating a line of symmetry. This is done by selecting two points on the image, one
on each side, and then drawing a straight line connecting them. The coordinates of these two points are
recorded along with the corresponding line segment. These line segments serve as a rough approximation of
the midline. By comparing the coordinates of the two points, the model can calculate the angle of rotation
and other geometric properties of the midline, helping it to better understand the underlying structure.
• Curved linear paths: The second type of annotation involves drawing a curved linear path that adapts to
the curvature of the midline, providing a more accurate representation of its shape and symmetry. This is
achieved by clicking multiple points, with a maximum of ten, along the midline, creating a polyline that
closely approximates the true curvature of the interhemispheric ﬁssure. The coordinates of these points are
also recorded, allowing the model to analyze the curvature and tortuosity of the midline in greater detail.
By examining the sequence of points that form the curved linear path, the model can gain insights into the
annotated midline’s geometry, such as its radius of curvature, angles of bends, and overall shape.
3.3
Dataset access
The dataset and annotations are available in the HuggingFace Datasets Hub [1]. The Hugginface API allows the
loading of the dataset in a single line of code. Additionally, data processing methods are available to quickly get the
dataset prepared for training in a deep learning model. The dataset separates the two different modalities into two
separate splits. A ﬁrst split of 1476 rows contains the T1w-type images, and the second split, made of 1674 rows,
contains the T2w-type images. Instructions to load the dataset are detailed on the dataset’s repository on Hugginface
[15].
Attributes:
• image: PIL [8] formatted image representing the cross-section, of shape (290, 290).
• line: Straight line annotation coordinates on the image, saved as a Python dictionary. (x:x1, y:y1, x:x2, y:y2).
Where (x1, y1), (x2, y2) are the starting and end points of the line annotation, in image coordinates.
• radscore: Radiology score of the volume the image was extracted from. Refer to dHCP documentation [17]
for scores explanation.
• session: Session-ID of the original dHCP [17] dataset, used for scan identiﬁcation retrieval.
Data Availability
The data presented in this study are openly available in Huggingace dataset at https://www.doi.org/10.57967/hf/1372
[15], accessed on 12 December 2023.
Acknowledgments
Data were provided by the developing Human Connectome Project, KCL-Imperial-Oxford Consortium funded by the
European Research Council under the European Union Seventh Framework Programme (FP/2007-2013) / ERC Grant
Agreement no. [319456]. We are grateful to the families who generously supported this trial.
Funding
This work was supported by the project: “PARENT”, funded by the European Union’s Horizon 2020 Programme
MSCA-ITN-2020 Innovative Training Networks Grant Agreement No. 956394.
Conﬂict of Interest
The authors declare no conﬂicts of interest and no known competing ﬁnancial interests or personal relationships that
could have appeared to inﬂuence the work reported in this paper.
5
Data Availability
The SymBrain dataset is openly available in the HuggingFace Datasets Hub [15].
Abbreviations
MRI
Magnetic Resonance Imaging
T1w
T1 weighted image
T2w
T2 weighted image
MSP
Mid-sagittal plane
HCP
Human Connectome Project
NKI
Nathan Kline Institute
dHCP
Developing Human Connectome Project
References
[1] Huggingface datasets documentation. https://huggingface.co/docs/datasets/index. Accessed: 2023-
11.
[2] V7 data engine. https://www.v7labs.com/. Accessed: 2023-11.
[3] John S Allen, Hanna Damasio, and Thomas J Grabowski. Normal neuroanatomical variation in the human brain:
An mri-volumetric study. American Journal of Physical Anthropology: The Ofﬁcial Publication of the American
Association of Physical Anthropologists, 118(4):341–358, 2002.
[4] Evyn Arnﬁeld, Andrea Guzzetta, and Roslyn Boyd. Relationship between brain structure on magnetic resonance
imaging and motor outcomes in children with cerebral palsy: a systematic review. Research in Developmental
Disabilities, 34(7):2234–2250, 2013.
[5] Mark Bear, Barry Connors, and Michael A Paradiso. Neuroscience: exploring the brain, enhanced edition:
exploring the brain. Jones & Bartlett Learning, 2020.
[6] Matthew Brett, Christopher J Markiewicz, Michael Hanke, Marc-Alexandre Côté, Ben Cipollini, Paul McCarthy,
Dorota Jarecka, CP Cheng, YO Halchenko, M Cottaar, et al. nipy/nibabel: 3.2. 1. Zenodo, 2020.
[7] Monica Laura Cara, Ioana Streata, Ana Maria Buga, and Dominic Gabriel Iliescu. Developmental Brain Asym-
metry. The Good and the Bad Sides. Symmetry, 14(1):128, January 2022. Number: 1 Publisher: Multidisciplinary
Digital Publishing Institute.
[8] Alex Clark. Pillow (pil fork) documentation, 2015.
[9] Lucilio Cordero-Grande, Rui Pedro AG Teixeira, Emer J Hughes, Jana Hutter, Anthony N Price, and Joseph V
Hajnal. Sensitivity encoding for aligned multishot magnetic resonance reconstruction. IEEE Transactions on
Computational Imaging, 2(3):266–280, 2016.
[10] Albert Costa. The Bilingual Brain: And What It Tells Us about the Science of Language. Springer, 2020.
[11] Jessica Dubois, Marianne Alison, Serena J Counsell, Lucie Hertz-Pannier, Petra S Hüppi, and Manon JNL Ben-
ders. Mri of the neonatal brain: a review of methodological challenges and neuroscientiﬁc advances. Journal of
Magnetic Resonance Imaging, 53(5):1318–1343, 2021.
[12] Simona Fiori, Giovanni Cioni, Katrjin Klingels, Els Ortibus, Leen Van Gestel, Stephen Rose, Roslyn N Boyd,
Hilde Feys, and Andrea Guzzetta. Reliability of a novel, semi-quantitative scale for classiﬁcation of structural
brain magnetic resonance imaging in children with cerebral palsy. Developmental Medicine & Child Neurology,
56(9):839–845, 2014.
[13] Michael S. Gazzaniga. Forty-ﬁve years of split-brain research and still going strong. Nature Reviews Neuro-
science, 6(8):653–659, 2005.
[14] Nadine J Girard, Philippe Dory-Lautrec, Mériam Koob, and Anca Melania Dediu. Mri assessment of neonatal
brain maturation. Imaging in Medicine, 4(6):613, 2012.
[15] Arnaud Gucciardi. mri-sym2 (revision 168e48e), 2023.
[16] Nitsa J Herzog and George D Magoulas. Brain asymmetry detection and machine learning classiﬁcation for
diagnosis of early dementia. Sensors, 21(3):778, 2021.
6
[17] E. J. Hughes, T. Winchman, F. Padormo, R. Teixeira, J. Wurie, M. Sharma, M. Fox, J. Hutter, L. Cordero-Grande,
A. N. Price, J. Allsop, J. Bueno-Conde, N. Tusor, T. Arichi, A. D. Edwards, M. A. Rutherford, S. J. Counsell,
and J. V. Hajnal. A dedicated neonatal brain imaging system. Magnetic Resonance Medicine, 78(2):794–804,
2017.
[18] P Kalavathi, M Senthamilselvi, and VB Surya Prasath. Review of computational methods on brain symmetric
and asymmetric analysis from neuroimaging techniques. Technologies, 5(2):16, 2017.
[19] Eric R. Kandel, James H. Schwartz, and Thomas M. Jessell. Principles of neural science. McGraw-Hill, 2000.
[20] Antonios Makropoulos, Serena J Counsell, and Daniel Rueckert. A review on automatic fetal and neonatal brain
mri segmentation. NeuroImage, 170:231–248, 2018.
[21] Ariel Prager and Sudipta Roychowdhury. Magnetic resonance imaging of the neonatal brain. The Indian Journal
of Pediatrics, 74:173–184, 2007.
[22] Lesley J. Rogers. Brain Lateralization and Cognitive Capacity. Animals : an Open Access Journal from MDPI,
11(7):1996, July 2021.
[23] Russell H Tobe, Anna MacKay-Brandt, Ryan Lim, Melissa Kramer, Melissa M Breland, Lucia Tu, Yiwen Tian,
Kristin Dietz Trautman, Caixia Hu, Raj Sangoi, et al. A longitudinal resource for studying connectome develop-
ment and its psychiatric associations during childhood. Scientiﬁc Data, 9(1):300, 2022.
[24] David C Van Essen, Stephen M Smith, Deanna M Barch, Timothy EJ Behrens, Essa Yacoub, Kamil Ugurbil, Wu-
Minn HCP Consortium, et al. The wu-minn human connectome project: an overview. Neuroimage, 80:62–79,
2013.
7
"
"This study develops a finite deformation phase-field formulation to investigate the influence of hygrothermal conditions on the viscoelastic-viscoplastic fracture behavior of epoxy nanocomposites under cyclic loading. The formulation considers contributions from viscoplasticity, equilibrium, non-equilibrium, and volumetric behaviors, with distinct definitions for tension and compression. The proposed phase-field model is implemented in numerical simulations to study the cyclic force-displacement response of both dry and saturated boehmite nanoparticle (BNP)/epoxy samples, considering BNP contents and temperature. Experimental data shows good agreement with numerical results at various BNP contents. Additionally, the phase-field model's predictive capability is evaluated through simulations of single-edge notched nanocomposite plates subjected to monolithic tensile and shear loading.","In various engineering applications, there is a significant emphasis on reducing the weight of structures to enhance their performance and functionality. To achieve this, researchers are concurrently pursuing two strategies: optimizing the structure and developing new materials with superior thermo-mechanical properties and lightweight characteristics. Among these materials, polymer nanocomposites are particularly promising due to their combination of desirable attributes from polymers, such as low weight and high ductility, with the unique features of nanoparticles. Boehmite nanoparticle (BNP) reinforced epoxy composites have exhibited exceptional strength-to-weight ratios, resulting in improved mechanical properties, including strength and fracture toughness, compared to neat epoxies. To further advance material innovation in this area, reliable models are required to predict how external conditions and microstructural parameters influence the material behavior of nanocomposites. Various constitutive models have been developed to capture the nonlinear rate- and temperature-dependent behavior of polymers and their composites. However, predicting the nonlinear stress-strain response of polymer nanocomposites is more challenging due to the heterogeneous distribution of agglomerated nanoparticles in the matrix and complex interactions between the matrix and nanoparticles.","Previous research has made significant contributions to modeling the behavior of polymers and their composites. Phenomenological or physically motivated constitutive models have been developed to elucidate their nonlinear, rate-dependent, and softening behavior. Notably, Boyce et al. proposed a constitutive model based on a composite-type formulation considering the microstructure of semicrystalline polymers. Later, Qi and Boyce utilized this model to capture the nonlinear, rate-dependent, and softening behavior of thermoplastic polyurethanes. Furthermore, Li et al. introduced a physically-based viscoelastic constitutive model for elastomers at large deformation, assuming elastomers to be cross-linked networks with superimposed free chains. Nguyen et al. developed and experimentally calibrated a rate-dependent damage constitutive model for epoxy resins to study the nonlinear behavior of amorphous glassy polymers. Additionally, N'Guyen et al. derived a thermodynamical framework for the thermo-chemo-mechanical couplings in polymer materials at finite deformation based on the definition of Helmholtz free energy.nannannan","The study presents a phase-field formulation for investigating the cyclic viscoelastic-viscoplastic fracture behavior of BNP/epoxy nanocomposites under hygrothermal conditions. This formulation incorporates a definition of the Helmholtz free energy that considers the contributions from nanoparticles, moisture content, and temperature. The free energy is additively decomposed into equilibrium, non-equilibrium, and volumetric components, with distinct definitions for tension and compression. This derivation offers a realistic modeling of damage and viscoplasticity mechanisms in nanocomposites by coupling the phase-field damage model with a modified crack driving force and a viscoelastic-viscoplastic model. Numerical simulations are conducted to analyze the cyclic force-displacement response of both dry and saturated BNP/epoxy samples, taking into account BNP contents and temperature.nan","The study's experimental-numerical comparison demonstrates a good agreement between experimental data and numerical predictions along the entire evolution of displacement. This highlights the proposed phase-field model's capability in predicting the evolution of damage and viscoplasticity mechanisms in epoxy polymers under cyclic loading. The simulations also reveal that the ductility of epoxy increases in the presence of moisture content. Furthermore, dogbone tensile tests and single-edge notched tensile and shear tests are conducted to evaluate the phase-field model's ability to predict the fracture behavior of BNP/epoxy nanocomposites with varying BNP weight fractions and moisture conditions. The numerical results show good agreement with experimental data, evidencing the model's practicability at different BNP weight fractions and its capacity to predict fracture patterns.","In summary, the study presents a comprehensive investigation of the cyclic viscoelastic-viscoplastic fracture behavior of BNP/epoxy nanocomposites under hygrothermal conditions using a finite deformation phase-field fracture model. The proposed model considers the effects of temperature, moisture, and nanoparticle content by incorporating a modified crack driving force and an additively decomposed free energy. Numerical simulations demonstrate the model's ability to accurately predict the damage and viscoplasticity evolution in nanocomposites, as well as the fracture patterns in single-edge notched specimens. These findings contribute to the understanding and modeling of the fracture behavior of polymer nanocomposites under various conditions, which is essential for designing and optimizing lightweight structures in engineering applications.",Cyclic viscoelastic-viscoplastic behavior of epoxy nanocomposites under hygrothermal conditions: A phase-field fracture model,"Behrouz Arash, Shadab Zakavati, Betim Bahtiri, Maximilian Jux, Raimund Rolfes","arXiv:2401.11813v1  [cs.CE]  22 Jan 2024
Cyclic viscoelastic-viscoplastic behavior of epoxy nanocomposites under
hygrothermal conditions: A phase-ﬁeld fracture model
Behrouz Arasha,, Shadab Zakavatia, Betim Bahtirib, Maximilian Juxc, Raimund Rolfesb
aDepartment of Mechanical, Electrical, and Chemical Engineering, Oslo Metropolitan University, 0166 Oslo, Norway
bInstitute of Structural Analysis, Leibniz Universität Hannover, Appelstraße 9A, 30167 Hannover, Germany
cInstitute of Composite Structures and Adaptive Systems, DLR (German Aerospace Center), Lilienthalplatz 7, 38108
Brunswick, Germany
Abstract
In this study, a ﬁnite deformation phase-ﬁeld formulation is developed to investigate the eﬀect of hygrother-
mal conditions on the viscoelastic-viscoplastic fracture behavior of epoxy nanocomposites under cyclic load-
ing. The formulation incorporates a deﬁnition of the Helmholtz free energy, which considers the eﬀect of
nanoparticles, moisture content, and temperature. The free energy is additively decomposed into a devia-
toric equilibrium, a deviatoric non-equilibrium, and a volumetric contribution, with distinct deﬁnitions for
tension and compression. The proposed derivation oﬀers a realistic modeling of damage and viscoplasticity
mechanisms in the nanocomposites by coupling the phase-ﬁeld damage model with a modiﬁed crack driving
force and a viscoelastic-viscoplastic model. Numerical simulations are conducted to study the cyclic force-
displacement response of both dry and saturated boehmite nanoparticle (BNP)/epoxy samples, considering
BNP contents and temperature. Comparing numerical results with experimental data shows good agreement
at various BNP contents. In addition, the predictive capability of the phase-ﬁeld model is evaluated through
simulations of single-edge notched nanocomposite plates subjected to monolithic tensile and shear loading.
Keywords:
Polymer nanocomposite, Moisture eﬀect, Viscoplasticity, Finite deformation, Phase-ﬁeld
modeling
1. Introduction
In the ﬁeld of engineering, a major challenge is reducing the weight of structures to improve their per-
formance and functionality for speciﬁc applications. To achieve this, researchers are focusing on both op-
timizing the structure and developing new materials that have superior thermo-mechanical properties but
are lightweight. One such material is polymer nanocomposites, which combine the desirable attributes of
polymers, such as low weight and high ductility, with the unique features of nanoparticles [1–4]. Studies
have shown that boehmite nanoparticle (BNP) reinforced epoxy composites are among the most promising
composites for lightweight structures due to their high strength-to-weight ratio [5]. Compared to neat epox-
ies, BNP/epoxy nanocomposites have signiﬁcantly improved mechanical properties, including strength and
Email address: behrouza@oslomet.no (Behrouz Arash)
1
fracture toughness [5–7].
To advance the material innovation, reliable models are required to predict how external conditions (such
as loading rate, temperature, and moisture) and microstructural parameters (such as nanoparticle/matrix
interactions) aﬀect the material behavior of nanocomposites. Continuing research activity on polymers and
their composites has led to a variety of phenomenological or physically motivated constitutive models [8–
13] to elucidate their nonlinear rate- and temperature-dependent behavior. Boyce et al. [14] developed a
constitutive model based on a composite-type formulation considering the microstructure of semicrystalline
polymers. In the model, the soft amorphous and stiﬀ crystalline phases are treated as the matrix and ﬁllers,
respectively. Later, based on the model, Qi and Boyce [15] proposed a viscoelastic-viscoplastic constitutive
model to capture the nonlinear, rate-dependent, and softening behavior of thermoplastic polyurethanes. Li
et al. [16] introduced a physically-based viscoelastic constitutive model for elastomers at large deformation,
where elastomers are assumed to be cross-linked networks with superimposed free chains. Nguyen et al. [12]
developed and experimentally calibrated a rate-dependent damage constitutive model for epoxy resins to
study the nonlinear behavior of amorphous glassy polymers.
Based on the deﬁnition of Helmholtz free
energy, N’Guyen et al. [17] derived a thermodynamical framework for the thermo-chemo-mechanical couplings
in polymer materials at ﬁnite deformation.
However, predicting the nonlinear stress-strain response of polymer nanocomposites is more challenging
due to the heterogeneous distribution of agglomerated nanoparticles in the matrix and complex interac-
tions between the matrix and nanoparticles. To overcome these challenges, Fankhänel et al. [18] developed
an atomistically-informed ﬁnite element (FE) model to investigate the material properties of BNP/epoxy
nanocomposites. They used molecular simulations to characterize the interphase properties between BNPs
and epoxy matrices, which were then scaled up to the continuum level. FE simulations of representative
volume elements of the nanocomposites were then performed to homogenize the eﬀective material properties.
Arash et al. [7, 19] proposed a multiscale framework to calibrate a viscoelastic damage model for BNP/epoxy
nanocomposites at ﬁnite deformation. The resulting model was validated through experimental-numerical
testing, demonstrating its ability to accurately capture the stress-strain relationship of the nanocomposites,
including nonlinear hyperelastic, rate-dependent, and softening behavior.
Unger et al. [20, 21] extended
the framework to characterize the thermo-viscoelastic damage behavior of BNP/epoxy nanocomposites and
developed a robust parameter identiﬁcation procedure.
When it comes to computational modeling of fracture in polymer nanocomposites, accurately predicting
crack initiation and propagation is crucial. Research has shown that cracks are related to the generation of
microvoids and microcracks in loaded polymer materials [12, 22]. As loading continues, these microvoids and
microcracks will coalesce, leading to the birth of complete cracks. The damage mechanisms are also aﬀected by
nanoparticle contents and hygrothermal conditions [23–25]. However, FE modeling of the damage mechanisms
based on local continuum description of damage typically suﬀer from an inherent mesh dependence [26, 27].
To overcome this issue and investigate damage and failure in materials, regularized solutions have been
2
proposed in the literature. Various models have been developed based on regularization theories, such as
the gradient-enhanced damage model and its variations [28–32]. These models include the phase-ﬁeld model
(PFM) [33–35], which provides variational fracture models by minimizing potential energy consisting of the
bulk energy, external forces work, and surface energy. PFMs use a scalar phase-ﬁeld parameter to describe the
smooth transition from an intact material to a fully broken state, making them an alternative to discontinuous
crack modeling.
They are capable of predicting complex patterns of crack initiation, propagation, and
branching. PFMs have been used to study brittle, quasi-brittle, and ductile fracture [36–43]. Shanthraj et
al. [44] formulated a PFM for elasto-viscoplastic materials, which gives a physically realistic description of the
material behaviour at the crack tip. Dean at al. [45] developed a PFM for long ﬁber polymer composites by
encompassing the diﬀerentiation of ﬁber and matrix failure phenomena. The formulation incorporates plastic
eﬀects via an invariant-based plasticity model for matrix-dominated deformation states. Msekh et al. [46]
developed a PFM to predict the tensile strength and fracture toughness of clay/epoxy nanocomposites, while
Goswami et al. [47, 48] proposed a neural network algorithm for phase-ﬁeld modeling of fracture in brittle
materials. The simulation results show that the proposed approach can match the crack paths reported in
the literature.
Furthermore, some phase-ﬁeld formulations have been derived to study the rate-dependent fracture of
solids [49–52]. In the case of polymer nanocomposites, Arash et al. [25, 32] developed a phase-ﬁeld formu-
lation to study the nonlinear viscoelastic fracture behavior of BNP/epoxy nanocomposites at ﬁnite defor-
mation, considering the eﬀect of hygrothermal conditions. The formulation takes into account the eﬀect of
temperature, moisture, and nanoparticle contents, and deﬁnes the Helmholtz free energy through an addi-
tive decomposition of energy into equilibrium, non-equilibrium, and volumetric contributions, with varying
deﬁnitions under compressive and tensile loading.
In addition to the above-mentioned inelastic mechanisms (i.e., nonlinear viscoelasticity and damage),
experimental observations suggest the presence of additional irreversible deformation in nanocomposites sub-
jected to cyclic load-unloading [24]. This indicates that the nanocomposites undergo viscoplastic deformation,
which could signiﬁcantly aﬀect their overall behavior. Hence, for the more realistic modeling of these ma-
terials, a phase-ﬁeld formulation, which incorporates both nonlinear viscoelasticity and viscoplasticity, is
necessary.
To address the issue, this study focuses on developing a PFM to investigate the cyclic viscoelastic-
viscoplastic fracture behavior of BNP/epoxy nanocomposites under hygrothermal conditions. For this, a
PFM with a modiﬁed crack driving force is developed, considering the eﬀect of nanoparticles, moisture
content, and temperature. Meanwhile, dogebone tensile tests under dry and saturated conditions are con-
ducted to calibrate and validate the proposed PFM at various BNP contents. The numerical-experimental
comparison reveals that the PFM can accurately predict the damage and viscoplasticity mechanisms in
the nanocomposites. Furthermore, the predictive capability of the model is qualitatively evaluated through
simulations of single-edge notched nanocomposite plates subjected to monolithic tensile and shear loading.
3
This work is organized as follows. Sect. 2 presents a viscoelastic-viscoplastic model describing the rate-
and temperature-dependent behavior of the BNP/epoxy nanocomposites at ﬁnite deformation. In Sect. 3,
the governing equations of the PFM and the corresponding weak form and discretized equations are pro-
vided. In Sect. 4, numerical simulations are presented to validate the proposed PFM and study the eﬀect
of nanoparticle content, moisture and temperature on the fracture evolution in the nanocomposites. Finally,
sect. 5 summarizes the ﬁndings.
2. Constitutive model for nanoparticle/epoxy
In this section, a viscoelastic-viscoplastic model for BNP/epoxy nanocomposites is presented. The stress
response comprises of an equilibrium, two viscous, and a volumetric component to capture the nonlinear
rate-dependent behavior of materials. The eﬀect of BNPs and moisture on the stress-strain relationship is
taken into account by deﬁning an ampliﬁcation factor as a function of the nanoﬁller and moisture contents.
A modiﬁed Kitagawa model is also adopted to account for the eﬀect of temperature.
2.1. Kinematics
The total deformation gradient, containing the mechanical deformation, is multiplicatively split into a
volumetric and deviatoric part as
F = J1/3 ¯F,
(1)
where J = det[F] and ¯F are the volumetric deformation and the deviatoric deformation gradient, respectively.
The volume deformation is further decomposed into two terms: The mechanical compressibility Jm and the
moisture-induced swelling Jw, leading to an overall volumetric deformation as
J = JmJw,
(2)
where
Jw = 1 + αwww.
(3)
In the equation above, αw is the moisture swelling coeﬃcient and ww is the moisture content [25]. Our
model incorporates experimental characteristics by decomposing the material behavior into a viscoelastic and
a viscoplastic part, corresponding to the time-dependent reversible and time-dependent irreversible response,
respectively. We further decompose the viscoelastic stress response into a hyperelastic network and a viscous
network. The hyperelastic spring, associated with the entropy change due to deformations, captures the
equilibrium response, while the viscous network composed of an elastic spring and a viscoelastic dashpot
describes the non-equilibrium behavior of the nanocomposites.
The deviatoric part of the deformation gradient is decomposed into a viscoplastic and a viscoelastic
component [53]:
¯F = ¯Fve.¯Fvp.
(4)
4
Also, the viscoelastic deformation gradient is split into an elastic and an inelastic part as
¯Fve = ¯Fe.¯Fv.
(5)
Accordingly, similar decompositions are obtained for the left Cauchy-Green deformation tensors:
¯B = ¯F.¯FT ,
(6)
¯Be = ¯Fe.¯FT
e ,
(7)
¯Bve = ¯Fve.¯FT
ve.
(8)
The total velocity gradient of the viscoelastic network, ¯Lve = ˙¯Fve
Again, we consider the viscoplastic velocity gradient to be additively decomposed into the symmetric rate
of stretching and the skew-symmetric rate of spinning:
˜Lvp = ˙¯Fvp.¯F−1
vp = ˜Dvp + ˜
Wvp,
(15)
and we take ˜
Wvp = 0 again leading to:
˜Dvp = ˙εvp
τtot
dev [σ] ,
(16)
where dev [σ] is the deviatoric part of the total stress and τtot =∥ dev [σ] ∥F .To characterize the viscoplastic
ﬂow ˙εvp, we implement a simple phenomenological representation as follows:
˙εvp =





0
τtot < σ0
a(ǫ − ǫ0)b ˙ǫ
τtot ≥ σ0
,
(17)
where a, b and σ0 are material parameters.
ǫ0 is the stress at which the viscoplastic ﬂow is activated,
represented by the Frobenius norm of the Green strain tensor ∥ E ∥F , which is derived from the deformation
gradient:
E = 1
2 (FT .F − I),
(18)
and ˙ǫ is the strain rate of the eﬀective strain ∥ E ∥F , thus introducing a simple strain-rate dependency of the
viscoplastic ﬂow. Analogous to 13, the time derivative of the viscoplastic deformation gradient is given by
˙¯Fvp = ¯F−1
ve . ˙εvp
dev [σtot]
τtot
.¯F,
(19)
characterizing the rate kinematics of the viscoplastic ﬂow. We obtain the viscous and viscoplastic deformation
gradients at the end of a time increment using the Euler backward time integration.
In Eqs. 19 and 13, the midpoint method is used to numerically obtain the inelastic deformation gradient
at the end of a time increment, i.e.,
¯F
t+ dt
2
v/vp
=
¯F
t
v/vp + dt
2
˙¯F t
v/vp,
(20)
¯F
t+dt
v/vp
=
¯F
t
v/vp + dt ˙¯F
t+ dt
2
v/vp .
(21)
To calculate the elastic deformation gradient at the midpoint, it is also required to ﬁnd the total defor-
mation gradient at the midpoint. This is done by taking the average of the deformation gradient at the start
and end of the increment
¯F
t+ dt
2
=
¯F
t + ¯F
t+dt
2
.
(22)
6
2.2. Phenomenological Viscoelastic-viscoplastic model coupled with a phase-ﬁeld description
Following the additive decomposition of the free energy proposed in [40], the overall free energy of the
material can be decomposed into an equilibrium ψeq , a non-equilibrium ψneq and a volumetric part ψvol as
ψ Assuming that BNPs are well-dispersed rigid particles in the epoxy matrix, the Guth–Gold model is
adopted by which the eﬀective stiﬀness of particle-ﬁlled solids is obtained by ⟨E⟩ = XEm [55]. The ampliﬁ-
cation factor X is typically a function of ﬁllers’ volume fraction and distribution. So far, some attempts of
various levels of sophistication have been conducted to incorporate the eﬀect of particle/matrix interactions
on the eﬀective modulus of polymer composites. Most of these models suggest a polynomial series expan-
sion for the ampliﬁcation factor. Here, a modiﬁed Guth–Gold model is adopted to account for uniformly
distributed nanoparticles and moisture content as follows.
X
=
3. Phase-ﬁeld model at ﬁnite deformation
To evaluate the predictive capability of the proposed constitutive model, we use the model to present a
formulation of the PFM for nanoparticle/polymer composites. This section presents a variational phase-ﬁeld
formulation for quasi-brittle fracture at ﬁnite deformation, the continuum mechanics incremental scheme and
FE equations are derived to show the procedure of analysis.
3.1. Problem ﬁeld description
The strong form of the boundary value problem in the referential conﬁguration for the coupled displace-
ment u and phase-ﬁeld variable φ can be written as
∇X.P + B
=
0 in Ω0
(39)
P .N
=
¯T on Γ0
(40)
Gc
l0
φ − Gcl0∇X.
Also, a monotonically decreasing degradation function g(φ), satisfying conditions presented in Eq. 48, is
given by
g (φ)
=
(1 − φ)2 + k,
(48)
where k is a small positive parameter introduced for ensuring the stability of the solution [35].
To obtain the weak form of the governing equations, the weighted residual approach is used. Accordingly,
Eqs. (39) and (41) multiplied by weight functions and integrated over Ω0. Using the divergence theorem and
imposing the boundary conditions, the weak form of the governing equations is obtained as follows
Z
Ω0
P .∇X.ηudV −
Z
Ω0
ρ0B.ηudV −
Z
Ω0
¯T .ηudA
=
0
∀
ηu
∈ H1
0 (Ω) ,
(49)
and
Z
Ω0
Å
g′ (φ) Hηφ + Gc
l0
φηφ − Gcl0∇Xφ.C−1.∇Xηφ
ã
dV
=
0
∀
ηφ ∈ H1
0 (Ω) .
(50)
The weak form in the spatial description can then be derived as
Z
Ωt
σ.∇x.ηudv −
Z
Ωt
ρtb.ηudv −
Z
Ωt
¯t.ηuda
=
0
∀
ηu ∈ H1
0 (Ω) ,
(51)
and
Z
Ωt
Å
J−1g′ (φ) Hηφ + J−1 Gc
l0
φηφ − J−1Gcl0∇xφ.∇xηφ
ã
dv
=
0
∀
ηφ ∈ H1
0 (Ω) .
(52)
The Eqs. (51) and (52) in terms of spatial arguments are formulated using dv = JdV and nda = cof (F).
3.2. Consistent incremental-iterative scheme
Assuming that only deformation-independent loads act on the body, Eqs. 49 and 50 can then be expressed
in terms of external and internal nodal forces as
ru
=
f u
int − f u
ext = 0,
(53)
rφ
=
f φ
int − f φ
ext = 0,
(54)
where
f u
int
=
Z
Ω0
P .∇X.ηudV,
(55)
f u
ext
=
Z
Ω0
ρ0B.ηudV −
Z
Ω0
¯T .ηudA,
(56)
f φ
int
=
Z
Ωt
Å
J−1g′ (φ) Hηφ + J−1 Gc
l0
φηφ − J−1Gcl0∇xφ.∇xηφ
ã
dv,
(57)
10
f φ
ext
=
0,
(58)
By linearizing Eqs. (53) and (54) at iteration i + 1 with respect to the previous iteration i, a consistent
tangent stiﬀness is obtained as follows
ru
i+1
=
ru
i + △ru = 0,
(59)
rφ
i+1
=
rφ
i + △rφ,
(60)
where
△ru
=
Duru
i .△u + Dφru
i .△φ
(61)
and
△rφ
=
Durφ
i .△u + Dφrφ
i .△φ.
(62)
The linearization of Eqs. (51) and (52) in spatial formulation can ﬁnally be obtained using a push fowrard
of the linearized equation in the referential conﬁguration
Z
Ωt
(∇x△u.σ.∇xηu + ∇s
xηu.c.∇s
x△u) dv +
Z
Ωt
(∇s
xηu.Dφσ.△φ) dv
=
f u
ext − f u
int,i
(63)
and
Z
Ωt
Å
J−1g′ (φ) 2∂H
∂g .∇x△uηφ
ã
dv
+
Z
Ωt
Å
J−1g′′ (φ) H△φηφ + J−1 Gc
l0
△φηφ + J−1Gcl0∇x△φ.∇xηφ
ã
dv
=
f φ
ext − f φ
int,i
(64)
In the equations above, ˆc = J−1c =
∂S
∂C , where S is the second Piola-Kirchhoﬀ stress. Also,
∂(.)
∂g
=
F ∂(.)
∂C F T .
3.3. Finite element formulation
The linearized equilibrium and phase-ﬁeld equations are ﬁnally summarized in the following system of
equations:

Kuu
i
Kuφ
i
Kφu
i
Kφφ
i



δui+1
δφi+1


=

f u
ext
f φ
ext

 −

f u
int,i
f φ
int,i


(65)
where
Kuu
i
=
Z
Ω
BT
uˆcBudΩ +
Z
Ω
BT
u σBudΩ,
(66)
Kuφ
i
=
Z
Ω
BT
u
Å∂σ
∂φ
ã
NφdΩ,
(67)
11
Kφu
i
=
Z
Ω
NT
φ
Å
J−1g′ 2∂H
∂g
ã
BudΩ,
(68)
Kφφ
i
=
Z
Ω
J−1
Å
NT
φ
Å
g′′H + Gc
l0
ã
Nφ + Gcl0BT
φBφ
ã
dΩ,
(69)
In the equations above, the shape function matrices N u and N φ interpolate the nodal values u and φ,
respectively, and Bu and Bφ are the gradient operators for the displacement and the nonlocal equivalent
strain, respectively. The same shape functions interpolate the nodal values of the weight functions ηu and
ηφ.
In what follows, the two ﬁelds of the phase-ﬁeld fracture problem are solved using a staggered algorithm
proposed by Miehe et al. [36].
The staggered solution, which decouples the equilibrium and phase-ﬁeld
equations, has shown to be computationally eﬃcient and robust through its broad application in the litera-
ture [56, 57]. Accordingly, the coupling terms Kuφ
i
and Kφu
i
are not taken into account.
3.4. Consistent tangent moduli based on the Jaumann–Zaremba stress rate
In order to integrate the viscoelastic model into the incremental-iterative FE framework, the tangent
modulus tensor CσJ = ∂σ
∂ε needs to be explicitly speciﬁed. However, a closed-form calculation of the tangent
tensor is not a straightforward task. Here, we use an eﬃcient numerical approximation of the tangent moduli
proposed by Sun et al. [58]. In this approach, by perturbing the deformation gradient, the tangent moduli for
the Jaumann rate of the Cauchy stress are accurately approximated by a forward diﬀerence of the Cauchy
stresses. The Jaumann rate of the Cauchy stress can be expressed as
▽σ
=
˙σ − W σ − σW T = CσJ : D
(70)
The linearized incremental form of Eq. (70) is then obtained from
∆σ − ∆W σ − σ∆W T
=
CσJ : ∆D
(71)
To numerically calculate components of CσJ, Eq. (71) is perturbed by applying small perturbations to
components of ∆D and ∆W tensors. Here, ∆W ij and ∆Dij tensors with perturbed (i,j) components are
expressed as
∆W ij
=
1
2

∆F ijF −1 −
By substituting Eq. (74) into Eqs. (72) and (73), we have
∆W ij
=
0,
(75)
∆Dij
=
ǫ
2 (ei ⊗ ej + ej ⊗ ei) .
(76)
It is noteworthy that ∆D has six independent components due to its symmetry. Therefore, the choice of
(i,j) would be (1,1), (2,2), (3,3), (1,2), (1,3), and (2,3). The perturbed deformation gradient ˆF ij can then be
written as
ˆF ij
=
F + ∆F ij
(77)
Using Eq. (77), ∆σ is approximated by the forward diﬀerence of the perturbed and unperturbed Cauchy
stresses
∆σ
≈
σ
Ä ˆF ij
ä
− σ (F ) .
(78)
Substituting Eqs. (75), (76) and (78) into Eq. (71) gives
σ
Ä ˆF ij
ä
− σ (F )
≈
σJ
ij : ǫ
2 (ei ⊗ ej + ej ⊗ ei) .
(79)
Using Eq. (79), the numerical approximation of the tangent moduli is ﬁnally obtained as
CσJ
ij
=
1
ǫ
î
σ
Ä ˆF ij
ä
− σ (F )
ó
(80)
where CσJ
ij represents the components of the tangent modulus tensor CσJ calculated by the perturbation of
∆F ij. Table 1 summarizes a step-by-step algorithm adopted from [60] to compute the mechanical tangent
and solve the system of non-linear equations.
4. Fracture experiments and numerical simulations
This section aims to identify and validate the proposed PFM’s parameters.
Firstly, we compare the
numerical results of dogbone tests of BNP/epoxy samples with experimental data.
Then, we study the
inﬂuence of temperature on the fracture evolution. Lastly, we qualitatively evaluate the model’s ability to
predict fracture patterns using the well-known single-edge notched tests.
4.1. Experiments
The samples used for conditioning and mechanical tests are obtained from the panels shown in Figure 1.
In epoxy systems, necking in the tension direction is not a material property but rather a structural instabil-
ity [54]. Therefore, a notch is introduced to mitigate the eﬀects of material imperfections and necking on the
yield. The samples are conditioned at 60 ◦C and 85% relative humidity until they reach the saturated state
at a moisture concentration of 1.0% for the neat epoxy system and 1.2% for the BNPs reinforced epoxy. The
13
Table 1: Summary of step-by-step algorithm used to compute the tangent moduli and solve the system of non-linear equations.
1. Deﬁne a perturbation parameter: ǫ = 10−8.
2. Compute the right Cauchy-Green tensor: C = FT F.
3. Calculate the Cauchy stress tensor (i.e., σ) using equations presented in Sec. 2.
4. for k = 1...3 do
5.
for l = 1...3 do
6.
Initialize ˆF = F
7.
Perturb ˆF kl = ˆF kl + ǫ/2.
8.
Perturb ˆF lk = ˆF lk + ǫ/2.
9.
Calculate the corresponding perturbed stress response ∆σ using Eq. (78).
10.
for i = 1...3 do
11.
for j = 1...3 do
12.
Compute and store CσJ
ijkl and CσJ
ijlk based on the Jaumann stress rate of the Cauchy stress
as presented in Eq. (80).
13.
end
14.
end
15.
end
16. end
17. Determine the material tangent: CσJ.
18. Store the tangent tensor in Voigt-type matrix.
19. Solve the system of equations presented in Eq. (65) using a staggered algorithm [36].
total conditioning time was 115 days. Finally, the samples are subjected to mechanical loading-unloading
tests according to the DIN EN ISO 527–2 testing standard. An extensometer is used to measure the speci-
men’s elongation, and a load rate of 1 mm/min is applied. Before the ﬁnal loading cycle that led to failure,
six cycles were performed by loading the specimen to a speciﬁc amplitude and unloading it until the loading
force reached zero.
4.2. Simulations
The specimen’s symmetry allows for a more eﬃcient FE analysis. By using the symmetries and analyzing
a quarter of the specimen, we can obtain a complete solution of the entire model with less computational
cost. As illustrated in Fig. 2, the model considers the double symmetry of the sample at the mid-length and
width. The uppermost side’s vertical displacement and the left side’s horizontal displacement of the specimen
are restrained, while a horizontal displacement is applied to the right side of the specimen, as shown in the
ﬁgure.
Fig. 2 shows the corresponding mesh with 1563 four-noded quadrilateral (Q4) elements. Due to high-stress
14
Figure 1: Planar dimensions of the specimen for conditioning and mechanical loading-unloading tests with a thickness of 2.3
mm. All dimensions are in millimeters.
concentrations in the right part of the model, the mesh size is reﬁned to around 0.1 mm in the area, while
the length scale parameter is set to 0.5 mm. The load is applied via an imposed displacement at a constant
deformation rate of ˙u = 1 mm/min and Θ = 296 K. The following simulations are performed under plane
strain conditions, with the load applied through constant displacement increments of ∆u = 1 × 10−4 mm.
The simulations have been performed using an in-house parallel MATLAB code.
Figure 2: Loading and boundary conditions imposed on quarter of the specimen because of symmetry, and a two-dimensional
FE model composed of 1563 Q4 elements.
Table 2 provides the identiﬁed parameters of the PFM. The table shows that the material parameters
linked with equilibrium, non-equilibrium, and viscoelastic responses, the modiﬁed Kitagawa model, moisture
swelling, the energy release rate of the neat epoxy, and the length scale have been obtained from the authors’
previous studies [24, 25]. Additionally, the remaining material parameters have been calibrated to achieve
the best ﬁt to the experimental data. The parameters are validated using experimental data for the epoxy
system with 0% and 5% wt BNPs at dry and saturated conditions. The mass density of the neat epoxy and
BNP are 1.2 and 3.0 g/cc, respectively. Accordingly, the corresponding volume fraction can be calculated to
be νnp = 0.0215.
The resulting cyclic loading-unloading force-displacement curves of the dogbone tests for dry and satu-
rated epoxy samples are compared with experimental data in Fig. 3. The experimental-numerical comparison
demonstrates a good agreement between the experimental data and the numerical predictions along the whole
evolution of displacement. It evidences the capability of the proposed PFM in predicting the evolution of
damage and viscoplasticy mechanisms in the epoxy polymer under cyclic loading. Comparing the experi-
mental data in Figs. 3a and 3b show that the ductility of the epoxy increases at the presence of moisture
content.
15
Table 2: Materials parameters of the PFM model.
Parameter
Value
Equation
References
Equilibrium shear modulus
µ0
eq(MPa)
760
37
[24]
Non-equilibrium shear modulus
µ0
neq(MPa)
790
37
[24]
Volumetric bulk modulus
κv(MPa)
1154
38
[24]
Viscoelastic dashpot
˙ε0
(a)
(b)
(c)
Figure 4: Contour plots of damage in the dry epoxy sample for imposed displacements of (a) 3.32 mm, (b) 3.34 mm, and (c)
3.35 mm.
data. The deviation from the experimental results could be attributed to the formulation of the constitutive
model itself as well as the unique set of material parameters used. Nevertheless, the ampliﬁcation approach
employed in this study to account for the eﬀects of moisture and BNP contents has resulted in realistic nu-
merical predictions. Additionally, by calibrating the viscoplastic ﬂow and modifying the crack driving force,
we were able to reasonably predict the evolution of plasticity and damage in the nanocomposites.
0
1
2
3
4
0
500
1,000
1,500
.BbTH+2K2Mi UKKV
6Q`+2 ULV
1tT2`BK2Mi
aBKmHiBQM
(a)
0
1
2
3
4
5
0
500
1,000
1,500
.BbTH+2K2Mi UKKV
6Q`+2 ULV
1tT2`BK2Mi
aBKmHiBQM
(b)
Figure 5: Eﬀect of moisture on the force–displacement response of BNP(5 %wt)/epoxy at room temperature and the deformation
rate of 1 mm/min: (a) dry sample, and (b) saturated sample.
Next, dogbone tensile tests are conducted to evaluate the capability of the PFM model in predicting the
fracture behavior of BNP/epoxy nanocomposites with 10 %wt of BNPs. The simulations are performed using
the calibrated material parameters listed in Tables 2, and the eﬀect of nanoparticle contents is taken into
account using the ampliﬁcation factor presented in Eq. 32. Figs. 6a and 6b show the eﬀect of BNP weight
fraction on the force-displacement response at the deformation rate of ˙u = 1 mm/min and Θ = 296 K. Both
experimental data and numerical predictions are presented in the ﬁgures. In these curves, good agreements
17
are observed between the numerical results and experimental data, evidencing the practicability of the PFM
at diﬀerent BNP weight fractions.
0
1
2
3
4
0
500
1,000
1,500
.BbTH+2K2Mi UKKV
6Q`+2 ULV
1tT2`BK2Mi
aBKmHiBQM
(a)
0
1
2
3
4
5
0
500
1,000
1,500
.BbTH+2K2Mi UKKV
6Q`+2 ULV
1tT2`BK2Mi
aBKmHiBQM
(b)
Figure 6: Eﬀect of moisture on the force–displacement response of BNP(10 %wt)/epoxy at room temperature and the deformation
rate of 1 mm/min: (a) dry sample, and (b) saturated sample.
We then explore how temperature aﬀects the fracture behavior of BNP/epoxy nanocomposites.
The
results of our dogbone simulation tests, presented in Fig. 7, show that specimens with 10 %wt of BNPs
fractures at diﬀerent displacement ranges depending on the temperature: around 2.2 mm at 253 K, around
5 mm at 296 K, and around 7.6 mm at 323 K. These observations can interpreted in two ways. First, as
temperature increases, the shear and bulk modulus associated with the equilibrium, non-equilibrium, and
volumetric responses decrease (see Eqs. 33 and 34), resulting in a less stiﬀ material. Second, the deformation
of the viscoelastic composite is temperature-dependent, meaning that an increase in temperature leads to a
greater displacement at the fracture initiation due to a rise in the viscous ﬂow.
It is important to note that the simulations assume the eﬀect of temperature on the energy release rate
to be negligible. While this assumption is acceptable based on experimental observations, further numerical-
experimental validations are recommended to identify the variation of energy release rate with temperature
accurately.
The proposed model’s ability to predict fracture patterns is evaluated by conducting single-edge notched
tensile and shear tests on pure epoxy samples. The geometry and boundary conditions of the tests are shown
in Figure 8a. A horizontal notch is placed at the center of the specimen’s left outer surface. The bottom
side of the specimen is ﬁxed, while the top side is moved. Both tensile and shear loads are applied at a
deformation rate of ˙u = 1 mm/min with constant displacement increments of 10−6 mm. The simulations are
performed at 296 K under plane strain conditions using the material parameters listed in Table 2, while the
scale parameter is taken to be 0.015 mm. Meshes are reﬁned in areas where cracks are expected to propagate.
For the tensile test, a discretization with 12509 elements and an eﬀective element size of 0.003 mm is
18
0
1
2
3
4
5
6
7
8
0
500
1,000
1,500
.BbTH+2K2Mi UKKV
6Q`+2 ULV
T = 253 E
T = 296 E
T = 323 E
Figure 7: Eﬀect of temperature on the force–displacement curve in dogbone simulation tests of saturated BNP(10 %wt)/epoxy
samples at the deformation rate of 1 mm/min.
generated in the central strip of the specimen. To reach the same element size at the critical zone in the
shear test, 21045 elements with reﬁned meshes in the lower right diagonal strip of the specimen are used. The
fracture patterns for the two cases are shown in Figures 8b and 8c. It can be observed that the crack path
is horizontal for the tensile case, while there is a curved crack path for the pure shear case. From the shear
test in Figure 8c, it can be found that the free energy decomposition presented in Eq. (23) prevents cracking
in compression. The crack patterns obtained are consistent with those reported in the literature [36].
(a)
(b)
(c)
Figure 8: (a) Geometry and boundary conditions of single edge notched specimen, (b) fracture pattern for unidirectional tension
(α = 90◦), and (c) fracture pattern for pure shear deformation (α = 0◦). The deformation rate is 1 mm/min.
19
5. Summary and conclusions
A ﬁnite deformation phase-ﬁeld fracture model has been developed to study the cyclic viscoelastic-
viscoplastic fracture behavior of BNPs reinforced epoxy nanocomposites under hygrothermal conditions. For
this, the PFM with a modiﬁed crack driving force has been coupled to a viscoelastic-viscoplastic constitutive
law. Within the derived formulation, a free energy has additively been decomposed into an equilibrium, a non-
equilibrium, and a volumetric part with positive/negative components, capturing the eﬀect of nanoparticles,
moisture and temperature on the nonlinear material behavior of the nanocomposites. Also, an ampliﬁcation
factora modiﬁed version of the Kitagawa model have been adopted to capture the role played by nanoparticles,
moisture and temperature on the fracture behavior.
The proposed PFM has been implemented in the FE analysis of dogbone tensile tests to demonstrate its
applicability. Numerical simulations were conducted to investigate the impact of nanoparticle content on the
force-displacement response of dry and saturated BNP/epoxy samples subjected to cyclic loading-unloading.
The results indicated that the force-displacement responses obtained from the numerical simulations were
consistent with those of experimental tests at various nanoparticle and moisture contents. The comparison
between numerical and experimental results conﬁrms the capacity of the PFM in predicting the development
of damage and viscoplasticity in BNP/epoxy nanocomposites at diﬀerent nanoparticle and moisture contents.
To evaluate the proposed PFM’s potential for broader applications, it would be worthwhile to compare the
numerical predictions at diﬀerent temperatures and deformation rates with experimental data in the future.
Also, the physical and chemical interactions between nanoparticles and epoxy matrices undergo signiﬁcant
changes under ﬁnite deformation at the micro- and sub-micro-scale.
These changes would impact other
material properties, such as viscosity and energy release rate. However, due to the complex interactions at
the interphase of nanoparticle/epoxy, the mechanisms leading to these changes are not clearly understood. In
the proposed model, the changes are considered by taking the volume fraction of nanoparticles and moisture
contents.
To gain a deeper understanding of the microstructure’s eﬀect on the macroscopic properties,
the PFM can be informed by molecular simulations [61, 62]. It would allow the analysis of the polymer
nanocomposites at the molecular scale.
Furthermore, the aggregation of nanoparticles in the epoxy matrix causes insuﬃcient dispersal [6], which
results in the degradation of material properties due to relatively inferior interactions between the nanopar-
ticles and the matrix. A study on the eﬀect of surface modiﬁcation of BNPs is suggested for future research
to suppress the aggregation and enhance the interfacial properties. The reinforcement of the epoxy with
surface-modiﬁed BNPs can improve the fracture properties of the nanocomposites.
Acknowledgments
This work originates from two research projects: (1) ”Hybrid laminates and nanoparticle-reinforced mate-
rials for improved rotor blade structures” (”LENAH - Lebensdauererhöhung und Leichtbauoptimierung durch
20
nanomodiﬁzierte und hybride Werkstoﬀsysteme im Rotorblatt”), funded by the Federal Ministry of Educa-
tion and Research of Germany, and (2) ”Challenges of industrial application of nanomodiﬁed and hybrid
material systems in lightweight rotor blade construction” (”HANNAH - Herausforderungen der industriellen
Anwendung von nanomodiﬁzierten und hybriden Werkstoﬀsystemen im Rotorblattleichtbau”), funded by the
Federal Ministry for Economic Aﬀairs and Climate Action. The authors wish to express their gratitude for
the ﬁnancial support.
References
[1] Z. Spitalsky, D. Tasis, K. Papagelis, C. Galiotis,
Carbon nanotube–polymer composites: chemistry,
processing, mechanical and electrical properties, Progress in polymer science 35 (2010) 357–401.
[2] S. Zhou, N. Vu-Bac, B. Arash, H. Zhu, X. Zhuang, Interface characterization between polyethylene/silica
in engineered cementitious composites by molecular dynamics simulation, Molecules 24 (2019) 1497.
[3] Y. Li, S. Wang, B. Arash, Q. Wang, A study on tribology of nitrile-butadiene rubber composites by
incorporation of carbon nanotubes: Molecular dynamics simulations, Carbon 100 (2016) 145–150.
[4] A. A. Mousavi, B. Arash, R. Rolfes, Optimization assisted coarse-grained modeling of agglomerated
nanoparticle reinforced thermosetting polymers, Polymer 225 (2021) 123741.
[5] M. Jux, J. Fankhänel, B. Daum, T. Mahrholz, M. Sinapius, R. Rolfes,
Mechanical properties of
epoxy/boehmite nanocomposites in dependency of mass fraction and surface modiﬁcation-an experi-
mental and numerical approach, Polymer 141 (2018) 34–45.
[6] M. G. Z. Khorasani, D. Silbernagl, P. Szymoniak, V.-D. Hodoroaba, H. Sturm, The eﬀect of boehmite
nanoparticles (γ-alooh) on nanomechanical and thermomechanical properties correlated to crosslinking
density of epoxy, Polymer 164 (2019) 174–182.
[7] B. Arash, W. Exner, R. Rolfes, A viscoelastic damage model for nanoparticle/epoxy nanocomposites at
ﬁnite strain: A multiscale approach, Journal of the Mechanics and Physics of Solids 128 (2019) 162–180.
[8] L. Bardella, A phenomenological constitutive law for the nonlinear viscoelastic behaviour of epoxy resins
in the glassy state, European Journal of Mechanics-A/Solids 20 (2001) 907–924.
[9] Y. Zhou, F. Pervin, L. Lewis, S. Jeelani, Experimental study on the thermal and mechanical properties
of multi-walled carbon nanotube-reinforced epoxy, Materials Science and Engineering: A 452 (2007)
657–664.
[10] M. Vogler, R. Rolfes, P. Camanho,
Modeling the inelastic deformation and fracture of polymer
composites–part i: plasticity model, Mechanics of Materials 59 (2013) 50–64.
21
[11] N. Vu-Bac, M. Bessa, T. Rabczuk, W. K. Liu, A multiscale model for the quasi-static thermo-plastic
behavior of highly cross-linked glassy polymers, Macromolecules 48 (2015) 6713–6723.
[12] V.-D. Nguyen, F. Lani, T. Pardoen, X. Morelle, L. Noels,
A large strain hyperelastic viscoelastic-
viscoplastic-damage constitutive model based on a multi-mechanism non-local damage continuum for
amorphous glassy polymers, International Journal of Solids and Structures 96 (2016) 192–216.
[13] H. Park, J. Choi, B. Kim, S. Yang, H. Shin, M. Cho,
Toward the constitutive modeling of epoxy
matrix: Temperature-accelerated quasi-static molecular simulations consistent with the experimental
test, COMPOSITES PART B 142 (2018) 131–141.
[14] M. Boyce, S. Socrate, P. Llana, Constitutive model for the ﬁnite deformation stress–strain behavior of
poly (ethylene terephthalate) above the glass transition, Polymer 41 (2000) 2183–2201.
[15] H. Qi, M. Boyce, Stress–strain behavior of thermoplastic polyurethanes, Mechanics of Materials 37
(2005) 817–839.
[16] Y. Li, S. Tang, M. Kröger, W. K. Liu, Molecular simulation guided constitutive modeling on ﬁnite strain
viscoelasticity of elastomers, Journal of the Mechanics and Physics of Solids 88 (2016) 204–226.
[17] T. N’guyen, S. Lejeunes, D. Eyheramendy, A. Boukamel, A thermodynamical framework for the thermo-
chemo-mechanical couplings in soft materials at ﬁnite strain, Mechanics of Materials 95 (2016) 158–171.
[18] J. Fankhänel, B. Arash, R. Rolfes, Elastic interphase properties of nanoparticle/epoxy nanocomposites:
A molecular dynamics study, Composites Part B: Engineering 176 (2019) 107211.
[19] B. Arash, W. Exner, R. Rolfes, Viscoelastic damage behavior of ﬁber reinforced nanoparticle-ﬁlled epoxy
nanocomposites: Multiscale modeling and experimental validation, Composites Part B: Engineering 174
(2019) 107005.
[20] R. Unger, W. Exner, B. Arash, R. Rolfes, Non-linear viscoelasticity of epoxy resins: Molecular simulation-
based prediction and experimental validation, Polymer 180 (2019) 121722.
[21] R. Unger, B. Arash, W. Exner, R. Rolfes, Eﬀect of temperature on the viscoelastic damage behaviour
of nanoparticle/epoxy nanocomposites: Constitutive modelling and experimental validation, Polymer
191 (2020) 122265.
[22] K. Chowdhury, R. Talreja, A. A. Benzerga, Eﬀects of manufacturing-induced voids on local failure in
polymer-based composites, Journal of engineering materials and technology 130 (2008).
[23] B. Bahtiri, B. Arash, R. Rolfes, Elucidating atomistic mechanisms underlying water diﬀusion in amor-
phous polymers: An autonomous basin climbing-based simulation method, Computational Materials
Science 212 (2022) 111565.
22
[24] B. Bahtiri, B. Arash, S. Scheﬄer, M. Jux, R. Rolfes, A machine learning-based viscoelastic–viscoplastic
model for epoxy nanocomposites with moisture content, Computer Methods in Applied Mechanics and
Engineering 415 (2023) 116293. doi:https://doi.org/10.1016/j.cma.2023.116293.
[25] B. Arash, W. Exner, R. Rolfes, Eﬀect of moisture on the nonlinear viscoelastic fracture behavior of
polymer nanocompsites: a ﬁnite deformation phase-ﬁeld model, Engineering with Computers 39 (2023)
773–790.
[26] A. Needleman,
Material rate dependence and mesh sensitivity in localization problems,
Computer
methods in applied mechanics and engineering 67 (1988) 69–85.
[27] M. Geers, R. De Borst, W. Brekelmans, R. Peerlings, Strain-based transient-gradient damage model for
failure analyses, Computer methods in applied mechanics and engineering 160 (1998) 133–153.
[28] R. H. Peerlings, R. de Borst, W. M. Brekelmans, J. De Vree, Gradient enhanced damage for quasi-brittle
materials, International Journal for numerical methods in engineering 39 (1996) 3391–3403.
[29] S. Saroukhani, R. Vafadari, A. Simone, A simpliﬁed implementation of a gradient-enhanced damage
model with transient length scale eﬀects, Computational Mechanics 51 (2013) 899–909.
[30] L. H. Poh, G. Sun, Localizing gradient damage model with decreasing interactions, International Journal
for Numerical Methods in Engineering 110 (2017) 503–522.
[31] B. Vandoren, A. Simone, Modeling and simulation of quasi-brittle failure with continuous anisotropic
stress-based gradient-enhanced damage models, Computer Methods in Applied Mechanics and Engineer-
ing 332 (2018) 644–685.
[32] B. Arash, R. Unger, W. Exner, R. Rolfes,
A ﬁnite deformation gradient-enhanced damage model
for nanoparticle/polymer nanocomposites: An atomistically-informed multiscale approach, Composite
Structures 258 (2021) 113211.
[33] G. A. Francfort, J.-J. Marigo, Revisiting brittle fracture as an energy minimization problem, Journal
of the Mechanics and Physics of Solids 46 (1998) 1319–1342.
[34] I. Aranson, V. Kalatsky, V. Vinokur, Continuum ﬁeld description of crack propagation, Physical review
letters 85 (2000) 118.
[35] C. Miehe, F. Welschinger, M. Hofacker, Thermodynamically consistent phase-ﬁeld models of fracture:
Variational principles and multi-ﬁeld fe implementations, International journal for numerical methods
in engineering 83 (2010) 1273–1311.
[36] C. Miehe, M. Hofacker, F. Welschinger, A phase ﬁeld model for rate-independent crack propagation:
Robust algorithmic implementation based on operator splits, Computer Methods in Applied Mechanics
and Engineering 199 (2010) 2765–2778.
23
[37] B. Bourdin, G. A. Francfort, J.-J. Marigo, Numerical experiments in revisited brittle fracture, Journal
of the Mechanics and Physics of Solids 48 (2000) 797–826.
[38] J. Fang, C. Wu, T. Rabczuk, C. Wu, G. Sun, Q. Li, Phase ﬁeld fracture in elasto-plastic solids: a
length-scale insensitive model for quasi-brittle materials, Computational Mechanics (2020) 1–31.
[39] J.-Y. Wu, A uniﬁed phase-ﬁeld theory for the mechanics of damage and quasi-brittle failure, Journal of
the Mechanics and Physics of Solids 103 (2017) 72–99.
[40] M. Ambati, R. Kruse, L. De Lorenzis, A phase-ﬁeld model for ductile fracture at ﬁnite strains and its
experimental veriﬁcation, Computational Mechanics 57 (2016) 149–167.
[41] C. Miehe, S. Mauthe, Phase ﬁeld modeling of fracture in multi-physics problems. part iii. crack driving
forces in hydro-poro-elasticity and hydraulic fracturing of ﬂuid-saturated porous media,
Computer
Methods in Applied Mechanics and Engineering 304 (2016) 619–655.
[42] P. K. A. V. Kumar, A. Dean, J. Reinoso, M. Paggi, Nonlinear thermo-elastic phase-ﬁeld fracture of
thin-walled structures relying on solid shell concepts, Computer Methods in Applied Mechanics and
Engineering 396 (2022) 115096.
[43] F. Dammaß, K. A. Kalina, M. Ambati, M. Kästner, Phase-ﬁeld modelling and analysis of rate-dependent
fracture phenomena at ﬁnite deformation, Computational Mechanics (2023) 1–25.
[44] P. Shanthraj, L. Sharma, B. Svendsen, F. Roters, D. Raabe, A phase ﬁeld model for damage in elasto-
viscoplastic materials, Computer Methods in Applied Mechanics and Engineering 312 (2016) 167–185.
[45] A. Dean, P. A. V. Kumar, J. Reinoso, C. Gerendt, M. Paggi, E. Mahdi, R. Rolfes, A multi phase-ﬁeld
fracture model for long ﬁber reinforced composites based on the puck theory of failure,
Composite
Structures (2020) 112446.
[46] M. A. Msekh, M. Silani, M. Jamshidian, P. Areias, X. Zhuang, G. Zi, P. He, T. Rabczuk, Predictions of j
integral and tensile strength of clay/epoxy nanocomposites material using phase ﬁeld model, Composites
Part B: Engineering 93 (2016) 97–114.
[47] S. Goswami, C. Anitescu, S. Chakraborty, T. Rabczuk, Transfer learning enhanced physics informed
neural network for phase-ﬁeld modeling of fracture, Theoretical and Applied Fracture Mechanics 106
(2020) 102447.
[48] S. Goswami, C. Anitescu, T. Rabczuk, Adaptive fourth-order phase ﬁeld analysis using deep energy
minimization, Theoretical and Applied Fracture Mechanics 107 (2020) 102527.
[49] R. Shen, H. Waisman, L. Guo, Fracture of viscoelastic solids modeled with a modiﬁed phase ﬁeld method,
Computer Methods in Applied Mechanics and Engineering 346 (2019) 862–890.
24
[50] P. J. Loew, B. Peters, L. A. Beex,
Rate-dependent phase-ﬁeld damage modeling of rubber and its
experimental parameter identiﬁcation, Journal of the Mechanics and Physics of Solids 127 (2019) 266–
294.
[51] B. Yin, M. Kaliske, Fracture simulation of viscoelastic polymers by the phase-ﬁeld method, Computa-
tional Mechanics 65 (2020) 293–309.
[52] R. Brighenti, T. Rabczuk, X. Zhuang, Phase ﬁeld approach for simulating failure of viscoelastic elas-
tomers, European Journal of Mechanics-A/Solids 85 (2021) 104092.
[53] S. Govindjee, S. Reese, A presentation and comparison of two large deformation viscoelasticity models,
Journal of engineering materials and technology 119 (1997) 251–255.
[54] X. Poulain, A. Benzerga, R. Goldberg,
Finite-strain elasto-viscoplastic behavior of an epoxy resin:
Experiments and modeling in the glassy regime, International Journal of Plasticity 62 (2014) 138–161.
[55] E. Guth, Theory of ﬁller reinforcement, Journal of applied physics 16 (1945) 20–25.
[56] A. Dean, J. Reinoso, N. Jha, E. Mahdi, R. Rolfes, A phase ﬁeld approach for ductile fracture of short
ﬁbre reinforced composites, Theoretical and Applied Fracture Mechanics 106 (2020) 102495.
[57] G. Molnár, A. Gravouil, 2d and 3d abaqus implementation of a robust staggered phase-ﬁeld solution for
modeling brittle fracture, Finite Elements in Analysis and Design 130 (2017) 27–38.
[58] W. Sun, E. L. Chaikof, M. E. Levenston, Numerical approximation of tangent moduli for ﬁnite element
implementations of nonlinear hyperelastic material models, Journal of biomechanical engineering 130
(2008).
[59] C. Miehe, Numerical computation of algorithmic (consistent) tangent moduli in large-strain computa-
tional inelasticity, Computer methods in applied mechanics and engineering 134 (1996) 223–240.
[60] R. Ostwald, E. Kuhl, A. Menzel, On the implementation of ﬁnite deformation gradient-enhanced damage
models, Computational Mechanics 64 (2019) 847–877.
[61] B. Arash, H. S. Park, T. Rabczuk, Tensile fracture behavior of short carbon nanotube reinforced polymer
composites: A coarse-grained model, Composite Structures 134 (2015) 981–988.
[62] B. Arash, H. S. Park, T. Rabczuk, Coarse-grained model of the j-integral of carbon nanotube reinforced
polymer composites, Carbon 96 (2016) 1084–1092.
25
"
"The safety of machine learning modules in decision-making processes depends on their ability to quantify uncertainty. Conformal prediction provides a method to accomplish this by transforming a base predictor into a set predictor with coverage guarantees. Despite its certified coverage, conformal prediction offers no control over the average size of predicted sets. This paper establishes a relationship between the generalization properties of the base predictor and the informativeness of resulting conformal prediction prediction sets. An upper bound is derived for the expected size of the conformal prediction set predictor that builds on generalization error bounds for the base predictor. The derived upper bound offers insights into the dependence of the average size of the conformal prediction set predictor on calibration data, target reliability, and base predictor generalization performance. These insights are validated via numerical regression and classification tasks.","Machine learning models are often required to offer accurate estimates of their uncertainties, especially in domains like healthcare, finance, and engineering. Conformal prediction provides a solution to compute ""error bars"" in the form of set predictions by post-processing the outputs of a fixed, pre-trained, base predictor. While conformal prediction ensures the predicted set contains the target quantity with a user-defined tolerance, it does not control the informativeness of the predicted set, i.e., the average predicted set size.","nanConformal prediction provides distribution-free marginal guarantees of coverage at any user-defined level. Inductive conformal prediction, the most practical variant of conformal prediction, splits the available data into a training set and a calibration set. The training data is used to optimize a base model, and the calibration data determines the ""error bars"" around the decisions produced by the base model. The informativeness of the set predictors produced by conformal prediction depends on the base predictor's quality and the selected non-conformity score, used to determine the prediction sets.nannan","This work investigates the interplay between the efficiency of conformal predictors, measured by the average set size, and the generalization performance of the underlying base predictor. It establishes an upper bound on the expected set size of probabilistic conformal predictors as a function of the generalization performance of the training algorithm, the calibration data set size, and the target reliability level. The bound is derived under the assumption that the approximation error of the training-based estimate meets the standard statistical error scaling, neglecting logarithmic terms.nan","The derived bound provides insights into the relationship between the average set size of conformal predictors and factors like the amount of calibration data, target reliability level, and the generalization performance of the base predictor. The expected set size decreases exponentially with the amount of calibration data and approaches a constant value determined by the target reliability level and the generalization performance of the base predictor. The rate of decrease in the expected set size increases as the target reliability level becomes smaller than the corrected empirical training estimate.","This paper establishes a theoretical connection between the generalization properties of conformal predictors and the informativeness of the resulting prediction sets. The derived bound offers insights into the dependence of the average conformal prediction set size on the dimension of the training set, the amount of calibration data, and the target reliability level. The theoretical results are validated on simple numerical regression and classification tasks.",Generalization and Informativeness of Conformal Prediction,"Matteo Zecchin, Sangwoo Park, Osvaldo Simeone, Fredrik Hellström","Generalization and Informativeness
of Conformal Prediction
Matteo Zecchin, Sangwoo Park, Osvaldo Simeone
Centre for Intelligent Information Processing Systems
Department of Engineering
King’s College London
London, United Kingdom
Email: {matteo.1.zecchin,sangwoo.park,osvaldo.simeone}@kcl.ac.uk
Fredrik Hellström
Centre for Artificial Intelligence
Department of Computer Science
University College London
London, United Kingdom
Email: f.hellstrom@ucl.ac.uk
Abstract—The safe integration of machine learning modules
in decision-making processes hinges on their ability to quantify
uncertainty. A popular technique to achieve this goal is conformal
prediction (CP), which transforms an arbitrary base predictor
into a set predictor with coverage guarantees. While CP certifies
the predicted set to contain the target quantity with a user-
defined tolerance, it does not provide control over the average
size of the predicted sets, i.e., over the informativeness of the
prediction. In this work, a theoretical connection is established
between the generalization properties of the base predictor and
the informativeness of the resulting CP prediction sets. To this
end, an upper bound is derived on the expected size of the CP
set predictor that builds on generalization error bounds for the
base predictor. The derived upper bound provides insights into
the dependence of the average size of the CP set predictor on
the amount of calibration data, the target reliability, and the
generalization performance of the base predictor. The theoretical
insights are validated using simple numerical regression and
classification tasks.
I. INTRODUCTION
Context and motivation: In safety-critical domains such as
health, finance, and engineering [1]–[3], machine learning
models are typically required to provide accurate estimates
of the uncertainty associated with their outputs [4]–[6]. Con-
formal prediction (CP) offers a practical solution to produce
certified “error bars” in the form of set predictions obtained by
post-processing the outputs of a fixed, pre-trained, base pre-
dictor [7]. With CP, uncertainty is captured by the size of the
predicted sets, with smaller sets providing more informative
predictions. While CP is guaranteed to meet a user-defined
coverage level, the informativeness of the predicted set can
usually only be assessed at test time.
The most practical version of CP, known as inductive CP,
splits the available data into a training set and a calibration set.
Training data are used to optimize an arbitrary base model,
while the calibration data are leveraged to determine the
“error bars” around the decisions produced by the base model.
As illustrated in Figure 1, a more accurate base predictor,
providing better generalization outside the training set, tends
to yield smaller prediction sets upon the application of CP. The
goal of this work is to connect the generalization properties
of the base predictor with the informativeness of the set
predictions produced by CP.
Fig. 1: Conformal prediction (CP) set predictors (gray ar-
eas) obtained by calibrating a base predictor with a higher
generalization error on the left and a lower generalization
error on the right. Thanks to CP, both set predictors satisfy a
user-defined coverage guarantee, but the inefficiency, i.e., the
average prediction set size, is larger when the generalization
error of the base predictor is larger.
Related work: CP offers distribution-free marginal guarantees
of coverage at any user-defined level [7], [8]. The infor-
mativeness of the set predictors produced by CP depends
on the quality of the base predictor and on the choice of
criterion, known as non-conformity score, used to determine
the prediction sets [9]. In the asymptotic regime of a large
training and calibration data set, it is known that CP sets
produced using nonparametric density estimators are optimal,
matching the set produced by an oracle that knows the data
distribution [10]–[12]. In the finite-sample regime of interest in
this work, a recent paper has provided methods for estimating
the expected predicted set size based on additional held-out
data [13]. In contrast, our work derives a bound relating the
expected set size to the generalization performance of the base
predictor.
A recent contribution has combined PAC-Bayes generaliza-
tion theory [14], [15] and CP to obtain generalization bounds
on the coverage and informativeness of the set predictions [16].
Similarly to our work, the authors derive an upper bound on
the expected prediction set size. However, their focus is on
arXiv:2401.11810v1  [cs.LG]  22 Jan 2024
deriving a design criterion for a modified CP procedure that
jointly leverages training and calibration data for enhanced
efficiency. Unlike [16], our results pertain to the conventional
inductive CP approach.
Main contributions and organization: In this paper, we provide
an upper bound on the expected size of CP predictions that
depends on the generalization error of the base predictor. The
derived bound provides insights into the dependence of the
CP-based predicted set size on the dimension of the training
set, on the amount of calibration data, and on the target
reliability level. The theoretical results are validated on simple
classification and regression tasks.
The rest of the paper is organized as follows. Section II
describes the problem, Section III presents the main result, and
Section IV describes two numerical examples, with Section V
concluding the paper.
II. PROBLEM DEFINITION
We consider the problem of characterizing the informative-
ness of set predictors obtained using the CP framework [7],
[8]. To this end, we study the standard supervised learning set-
ting in which each data point Z = (X, Y ) ∈ X ×Y consists of
an input feature X ∈ X and a label Y ∈ Y, which are jointly
distributed according to an unknown distribution PZ = PXY .
Throughout, we use capital letters for random variables and
the corresponding lowercase letters for realizations.
In this context, a set predictor Γ maps an input feature x into
a subset of the label space Γ(x) ⊆ Y. For a given reliability
level α, a set predictor Γ is said to be α-reliable if it includes
the true label Y with a probability no smaller than 1 − α, i.e.,
Pr[Y ∈ Γ(X)] ≥ 1 − α.
(1)
Note that the coverage guarantee (1) is marginal in the sense
that it applies on average with respect to the jointly distributed
input variable X and output variable Y .
The coverage guarantee (1) can always be satisfied by a set
predictor that returns the entire label space Y for any input
feature X. Therefore, for a target coverage level α, a key
metric to rank a set predictor Γ is its inefficiency, which is
defined as the average predicted set size
Λ(Γ) = E[|Γ(X)|].
(2)
Given access to a set of n i.i.d data samples D = {Zi}n
i=1 ∼
P ⊗n
Z , a common practice to produce a reliable and efficient,
or informative, set predictor consists in training a machine
learning model on a subset of the data set Dtr, and then to use
the remaining part of the data Dcal = D \ Dtr to calibrate the
resulting model using CP. We denote the sizes of the training
data set Dtr and of the calibration data set Dcal as ntr and
ncal = n − ntr, respectively.
A. Model Training
The first step towards the definition of a reliable set predic-
tor is the training of a base point predictor. To this end, we as-
sume that the learner selects a model class F = {fθ : θ ∈ Θ},
where the prediction function fθ : X → ˆY, such as a neural
network, is parameterized by a vector θ ∈ Θ.
A learning algorithm defines a mapping from the training
data set Dtr to a probability Q(θ|Dtr) over the space of model
parameters Θ. The distribution Q(θ|Dtr) may be implicit, as in
the case of randomized algorithms such as stochastic gradient
descent. Given a distribution Q(θ|Dtr), a decision on an input
X is made by first drawing a model θ ∼ Q(θ|Dtr) and then
using the prediction ˆY = fθ(X) (see, e.g., [17, Chapter 12]).
An important metric to gauge the performance of a model
class F and of a training algorithm Q(θ|Dtr) under the data
distribution PZ is the generalization error. The generalization
error measures the discrepancy between the training and
testing performance of the learning algorithm Q(θ|Dtr). The
generalization error is a function of the training algorithm
Q(θ|Dtr), viewed as a function of the training data set Dtr,
as well as of the training data set Dtr [14], [15].
B. Reliable Set Predictors via Conformal Prediction
CP is a post-hoc calibration technique that converts a point
predictor fθ : X → ˆY into an α-reliable set predictor Γθ. Fix
a bounded function R : ˆY ×Y → [0, Rmax] to measure the loss
of the predictor, e.g., the 0-1 loss or the squared loss. This is
known as the non-conformity (NC) scoring function. For each
i-th sample in the calibration data set Dcal, we define the i-th
calibration NC score, for i = 1, . . . , ncal, as
Ri = R(fθi(Xi), Yi),
where θi ∼ Q(θ|Dtr).
(3)
By (3), the calibration score is obtained by first sampling a
model parameter θi ∼ Q(θ|Dtr) to determine a predictor fθi,
and then evaluating the NC score Ri of the prediction fθi(Xi)
for the calibration data point Zi = (Xi, Yi).
We denote the collection of the calibration NC scores as the
set Rcal = {Ri}ncal
i=1, and the empirical calibration cumulative
distribution function (c.d.f.) of the calibration NC scores as
ˆFθ(r|Dcal) =
1
ncal
ncal
X
i=1
1{R(fθi(xi), yi) ≤ r}.
(4)
We also let Q1−α(Rcal) = ˆF −1
θ
((nα + 1)/ncal|Dcal) denote
the (nα + 1)-th largest element of the calibration NC scores,
where we define
nα = ⌈(ncal + 1)(1 − α)⌉ − 1,
(5)
and we used the notation ˆF −1
θ
(p|Dcal) to represent the smallest
value of r such that the inequality ˆFθ(r|Dcal) ≥ p holds.
For a test input feature X, and given the randomly drawn
model θ ∼ Q(θ|Dtr), the CP set predictor is obtained by
including all values of y ∈ Y with a NC score that is no
larger than Q1−α(Rcal), i.e., [7]
ΓCP
θ (X|Dcal) = {y ∈ Y : R(fθ(X), y) ≤ Q1−α(Rcal)}. (6)
The CP set predictor (6) is known to be α-reliable in the sense
that the inequality
Pr[Y ∈ ΓCP
θ (X|Dcal)|Dtr] ≥ 1 − α
(7)
holds, where the probability is taken over the joint distribution
of the model θ ∼ Q(θ|Dtr), the calibration data Dcal ∼ P ⊗ncal
Z
,
and the test data Z ∼ PZ. Note that the probability (7) is
conditioned on the training data set, and that the random
variables θ, Dcal and Z are conditionally independent given
Dtr. Using (2), we define as
ΛCP(Q|Dtr) = E

|ΓCP
θ (X|Dcal)|

(8)
the inefficiency of the CP set predictor, where the average is
taken over the same distribution as in (7).
C. Size of an NC Score
Let us denote as PR|Y =y,θ the probability density function
(p.d.f) of the NC score R(fθ(X), y) for a given model θ.
Note that the distribution depends solely on the conditional
distribution PX|Y =y = PX,Y =y/PY =y of the data. Therefore,
we can interpret PR|Y =y,θ as the fraction of inputs for which
the score assigned by the model θ to the label y equals r. By
averaging over the trained model θ ∼ Q(θ|Dtr) and over a
uniformly selected label Y ∼ U(Y), we obtain the size of an
NC score level r as [13]
γ(r|Q, Dtr) =
1
|Y|
Z
Y
EQ(θ|Dtr)

PR|Y =y,θ(r)

dy.
(9)
By definition (9), the size of the score r represents the fraction
of data points, drawn as explained, that are associated with an
NC score level r. We make the following assumption.
Assumption 1. The factor γ(r|Q, Dtr) is non-decreasing in r
for any data set Dtr.
This assumption holds for common NC scores such as the
ℓp error for regression with a bounded target domain Y =
[Bl, Bu], for which we have [13, Table 1]
γ(r|Q, Dtr) =
2r
1
p −1
p(Bu − Bl),
(10)
as well as for classification with the 0-1 loss as
γ(r|Q, Dtr) =
( 1
|Y|,
if r = 0,
1 −
1
|Y|,
if r = 1.
(11)
III. INEFFICIENCY AND GENERALIZATION
In this section, we study the interplay between the efficiency
of the CP set predictor, as measured by the average set size
ΛCP(Q|Dtr) in (8), and the generalization performance of the
training algorithm Q(θ|Dtr).
A. Main Result
To start, we define a counterpart of the c.d.f. (4) evaluated
using the training data set. As we will see, this will allow us to
relate the generalization performance of the training algorithm
with the size of the predicted set. Accordingly, the empirical
training c.d.f. of the NC scores at level r is defined as
ˆF(r|Q, Dtr) = 1
ntr
ntr
X
i=1
Pr[R(fθ(xi), yi) < r|Dtr],
(12)
where the probability is evaluated with respect to the random
model θ ∼ Q(θ|Dtr) for a fixed training data set Dtr. We
can view the training c.d.f. (12) as an approximation of the
population c.d.f.
F(r|Q, Dtr) = Pr[R(fθ(X), Y ) < r],
(13)
where the probability is computed over the model θ
∼
Q(θ|Dtr) and the sample (X, Y ) ∼ PZ. We measure the
generalization properties of the training-based estimate (12)
via the metric
∆(Q|Dtr) =
sup
r∈[0,Rmax]
F(r|Q, Dtr)− ˆF(r|Q, Dtr)
.
(14)
Assumption 2. For any δ ∈ (0, 1), there exists a function
β(δ, ntr), which satisfies β(δ, ntr) = O(log(ntr)) for any fixed
δ, such that the inequality
∆(Q|Dtr) ≤ β(δ, ntr)
√ntr
(15)
holds with probability no smaller than 1 − δ with respect to
the distribution of the training data set Dtr.
Note that, due to the supremum in (14), the inequality in
(15) differs from standard generalization bounds. However,
as shown formally in Appendix A, Assumption 2 holds for
many practical learning algorithms, such as Gibbs posteriors,
stochastic gradient Langevin dynamics (SGLD), and differen-
tially private algorithms. Appendix A also reports an explicit
expression for the function β(δ, ntr).
Assumption 2 stipulates that the approximation error (14) of
the model Q(θ|Dtr) meets the standard statistical error scaling
of O(1/√ntr), neglecting logarithmic terms. As reflected by
the main result of this work, summarized in the following
theorem, this generalization error can be directly related to
the efficiency of CP predictions. Henceforth, we denote the
function β(δ, ntr) as β for brevity.
Theorem 1. Under Assumption 1 and Assumption 2, the
expected set size of the probabilistic CP predictor (6) satisfies
the inequality
ΛCP(Q|Dtr)
|Y|
≤
Z Rmax
Rmin
e
−ncaldKL

nα
ncal
 ˆ
F (r|Q,Dtr)−
β
√ntr

γ(r|Q, Dtr)dr
+ γ(Rmin|Q, Dtr)Rmin
(16)
with probability 1 − δ with respect to the random draw of
the training data set Dtr, where dKL(a||b) = a log(a/b) +
(1 − a) log((1 − a)/(1 − b)) is the binary Kullback-Leibler
divergence and we have defined
Rmin =
inf
r∈[0,Rmax]

r : ˆF(r|Q, Dtr) ≥ nα
ncal
+
β
√ntr

.
(17)
B. Discussion
The bound (16) on the expected size of the CP set predictor
presented in Theorem 1 offers insights into the efficiency of
CP as a function of the generalization performance of the
predictor, which is accounted for by the function β > 0 and
Fig. 2: Bound on the average set size (16) for different values
of ntr and ncal as a function of the target reliability level 1−α.
Increasing the number ncal of calibration data points causes the
bound to converge exponentially fast to a function (black line)
that is increasing in 1 − α and decreasing in the amount of
training data ntr.
by the size of the training set ntr via Assumption 2, as well
as of the calibration data set size ncal and the target reliability
level 1 − α.
To start, we observe that the first term in (16) decreases
exponentially fast with an increase in the amount of calibration
data ncal. Consequently, for large values of ncal, the bound
on the set predictor size can be approximated by the second
term, namely γ(Rmin|Q, Dtr)Rmin, which is roughly constant
with respect to ncal, since we have the approximate equality
nα/ncal
≈ 1 − α for ncal sufficiently large. Overall, as
illustrated in Figure 2, as the amount of calibration data ncal
grows, the bound (16) on the efficiency of the CP set size
converges exponentially fast to a quantity, γ(Rmin|Q, Dtr)Rmin,
drawn as a black line, that is increasing in the target reliability
level 1 − α and decreasing in the training set size ntr.
For a finite amount of calibration data ncal, both terms in
(16) are generally non-negligible. In this regard, a key observa-
tion is that the exponent dKL(nα/ncal
 ˆF(r|Q, Dtr) − β/√ntr)
that dictates the decrease rate of the first term grows with the
discrepancy between the target reliability level 1 − α – more
precisely, the fraction nα/ncal ≈ 1−α of calibration NC scores
included in the CP predicted set – and the corrected empirical
training c.d.f. ˆF(r|Q, Dtr)−β/√ntr. Thus, the exponent tends
to increase as the target reliability level 1 − α decreases,
indicating smaller sets for less demanding reliability targets.
The exponential decrease rate of the first term in (16)
justifies the common practice of allocating a larger amount
of training data ntr to train the base model compared to the
size of the calibration set ncal. However, as sketched in Figure
2, the interplay between these two decreasing terms is not
straightforward, as one needs to ensure that enough data points
are allocated to calibration to control the average set size.
IV. EXAMPLES
In this section, we provide two representative examples, one
for classification and one for regression, with the main goal of
instantiating the general bound in Theorem 1 and of comparing
the numerical evaluations of the expected set size with the
behavior predicted by Theorem 1.
A. Multi-class Classification
Consider the multi-class classification problem with |Y|
classes, where we choose as NC score the standard 0-1 loss
R(fθ(x), y) = 1{y ̸= fθ(x)}.
(18)
Using (6), for any reliability level 1 − α, the associated CP
predictor is
ΓCP
θ (X|Dcal) =
(
{fθ(X)},
if Q1−α(Rcal) = 0
Y,
if Q1−α(Rcal) = 1,
(19)
that is, the predicted set includes the entire label set Y if the
number of correctly classified calibration points is less than
nα, and it includes only the point prediction fθ(X) otherwise.
1) Evaluating the bound: The corresponding size (9) of the
NC score r is (11). Accordingly, the empirical training c.d.f.
is given by ˆF(0|Q, Dtr) = 0 and
ˆF(1|Q, Dtr) = 1
ntr
ntr
X
i=1
Pr[fθ(Xi) = Yi|Dtr] = ˆPtr,
(20)
where ˆPtr is the expected average fraction of training data
points that are correctly classified. Evaluating the bound (16),
we obtain that, if ˆPtr ≥ nα/ncal + β/√ntr, i.e., if the base
predictor has a sufficiently high training accuracy, then the
normalized expected set size satisfies the inequality
ΛCP(Q|Dtr)
|Y|
≤ 1
|Y| +

1− 1
|Y|

e
−ncaldKL

nα
ncal
 ˆ
Ptr−
β
√ntr

.
(21)
By the bound (21), increasing the number ncal of calibration
data points reduces the magnitude of the second term in (21)
at an exponential rate, until the expected set-size reaches the
minimum normalized size 1/|Y|. Furthermore, the rate of de-
crease of the prediction set size grows as the largest reliability
level 1 − α becomes smaller than the corrected empirical
training estimate ˆPtr − β/√ntr. Finally, when the accuracy of
the predictor is insufficient, i.e., when ˆPtr < nα/ncal+β/√ntr,
the bound (21) on the expected set size becomes vacuous,
yielding ΛCP(Q|Dtr)/|Y| = 1.
2) Numerical results: We empirically validate the behavior
of the expected set size by studying a simple classification
task. Specifically, we consider the problem of classifying hand-
written digits from the MNIST dataset, which has |Y| = 10
classes. We adopt set predictors obtained by calibrating two
different logistic regression models, one trained on a data set
of size ntr = 100 and one trained on a larger data set of
size ntr = 500. The resulting classifiers are calibrated using
calibration data sets of increasing size ncal ∈ {50, 100, 200}.
In Figure 3, we report the normalized CP prediction set size
ΛCP(Q|Dtr)/|Y| for different reliability values 1 − α.
Comparing the empirical results in Figure 3 with the the-
oretical sketch derived from our theory in Figure 2 confirms
the validity of the general insights presented in the previous
0.70
0.75
0.80
0.85
0.90
0.95
1.00
1 − α
0.2
0.4
0.6
0.8
1.0
Normalized expected set size
ntr = 100
ntr = 500
ncal = 50
ncal = 100
ncal = 200
Fig. 3: Normalized empirical CP set size for a multi-class
classification problem on the MNIST data set as a function
of the reliability level 1 − α and for different sizes of the
calibration and training data sets.
section. First, when increasing the size ncal of the calibration
data set, the performance converges to a function of the
reliability 1 − α (black line) that increases with the training
set size ntr. In this regard, a minor caveat is that, unlike
the prediction of the bound (16), there is a small range of
reliability levels in which the expected set size tends to this
function from the right rather than from the left.
B. Example: ℓp Regression
We now consider a one-dimensional regression problem
with bounded target domain Y = [−Bu, Bl] and with the NC
score being the ℓp-norm
R(x, y) = ∥x − y∥p.
(22)
Based on (6), the corresponding CP set predictor is given by
ΓCP
θ (X|Dcal)={y ∈ Y : ∥fθ(X) − y∥p ≤Q1−α(Rcal)}, (23)
i.e., it includes all values of Y with an ℓp distance from the
point prediction fθ(X) that is no larger than Q1−α(Rcal).
1) Evaluating the bound: For the ℓp NC score, the score
size (9) can be computed in closed form as in (10). The
empirical training NC score c.d.f. is given by
ˆF(r|Q, Dtr) = 1
ntr
ntr
X
i=1
Pr[∥fθ(Xi) − Yi∥p < r|Dtr],
(24)
which corresponds to the expected fraction of training samples
with an ℓp error less than r. Instantiating the bound in (16), we
find that the normalized expected CP satisfies the inequality
ΛCP(Q|Dtr)
Bu − Bl
≤
Z Rmax
Rmin
e
−ncaldKL

nα
ncal
 ˆ
F (r|Q,Dtr)−
β
√ntr
 2r1/p−1
p(Bu − Bl)dr
+
2R1/p
min
p(Bu − Bl).
(25)
0.75
0.80
0.85
0.90
0.95
1.00
1 − α
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Normalized expected set size
ntr = 100
ntr = 500
ncal = 50
ncal = 100
ncal = 200
Fig. 4: Normalized empirical CP set size for an ℓp regression
task with p = 2 on the California housing data set as a function
of the reliability level 1 − α and for different sizes of the
calibration and training data sets.
2) Numerical results: To validate the analysis, we study a
one-dimensional regression problem based on the California
housing data set. To this end, we train a two-hidden-layer
perceptron with 50 neurons per layer to predict the value of
a property based on an 8 dimensional feature vector. We con-
sider two neural networks, one trained using ntr = 100 samples
and one trained using ntr = 500 samples. The resulting models
are calibrated using the ℓp NC score with p = 2 and with
different calibration data set sizes ncal ∈ {50, 100, 200}.
In Figure 4, we again show the normalized empirical CP set
size ΛCP(Q|Dtr)/|Y| as a function of the reliability threshold
1 − α. Like the classification example, the results confirm
the general conclusions produced by our theory. In particular,
increasing the calibration data set size ncal yields a prediction
set size that decreases with the number of data points in the
training set (black lines).
V. CONCLUSION
In this work, we have investigated the informativeness of
set predictors produced by CP by providing an upper bound
on the expected predicted set size. The bound illustrates the
interplay between informativeness of the prediction and the
generalization properties of the base predictor. An interesting
research direction may attempt to translate the derived bound
into a practical training algorithm to produce efficient set
predictors with coverage guarantees [18], [19].
ACKNOWLEDGMENTS
This work was partially supported by the European Union’s
Horizon Europe project CENTRIC (101096379), by the Open
Fellowships of the EPSRC (EP/W024101/1), by the EPSRC
project (EP/X011852/1), by the UK Government under Project
REASON, and by the Wallenberg AI, Autonomous Systems
and Software Program (WASP) funded by the Knut and Alice
Wallenberg Foundation.
REFERENCES
[1] A. L. Beam and I. S. Kohane, “Big data and machine learning in health
care,” JAMA, vol. 319, no. 13, pp. 1317–1318, 2018.
[2] J. W. Goodell, S. Kumar, W. M. Lim, and D. Pattnaik, “Artificial
intelligence and machine learning in finance: Identifying foundations,
themes, and research clusters from bibliometric analysis,” Journal of
Behavioral and Experimental Finance, vol. 32, p. 100577, 2021.
[3] L. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, “Learning-
based model predictive control: Toward safe learning in control,” Annual
Review of Control, Robotics, and Autonomous Systems, vol. 3, pp. 269–
296, 2020.
[4] C. Lu, A. Lemay, K. Chang, K. Höbel, and J. Kalpathy-Cramer, “Fair
conformal predictors for applications in medical imaging,” in Proc. of the
AAAI Conference on Artificial Intelligence, vol. 36, pp. 12008–12016,
2022.
[5] W. Wisniewski, D. Lindsay, and S. Lindsay, “Application of conformal
prediction interval estimations to market makers’ net positions,” in
Conformal and Probabilistic Prediction and Applications, pp. 285–301,
PMLR, 2020.
[6] M. Zecchin, S. Park, and O. Simeone, “Forking uncertainties: Reliable
prediction and model predictive control with sequence models via
conformal risk control,” arXiv preprint arXiv:2310.10299, 2023.
[7] V. Vovk, A. Gammerman, and G. Shafer, Algorithmic learning in a
random world, vol. 29. Springer, 2005.
[8] A. N. Angelopoulos, S. Bates, et al., “Conformal prediction: A gentle
introduction,” Foundations and Trends® in Machine Learning, vol. 16,
no. 4, pp. 494–591, 2023.
[9] Y. Kato, D. M. Tax, and M. Loog, “A review of nonconformity measures
for conformal prediction in regression,” Conformal and Probabilistic
Prediction with Applications, pp. 369–383, 2023.
[10] J. Lei, J. Robins, and L. Wasserman, “Distribution-free prediction sets,”
Journal of the American Statistical Association, vol. 108, no. 501,
pp. 278–287, 2013.
[11] J. Lei and L. Wasserman, “Distribution-free prediction bands for non-
parametric regression,” Journal of the Royal Statistical Society Series
B: Statistical Methodology, vol. 76, no. 1, pp. 71–96, 2014.
[12] M. Sadinle, J. Lei, and L. Wasserman, “Least ambiguous set-valued
classifiers with bounded error levels,” Journal of the American Statistical
Association, vol. 114, no. 525, pp. 223–234, 2019.
[13] G. S. Dhillon, G. Deligiannidis, and T. Rainforth, “On the expected size
of conformal prediction sets,” arXiv preprint arXiv:2306.07254, 2023.
[14] P. Alquier, “User-friendly introduction to PAC-Bayes bounds,” arXiv
preprint arXiv:2110.11216, 2021.
[15] F. Hellström, G. Durisi, B. Guedj, and M. Raginsky, “Generalization
bounds: Perspectives from information theory and PAC-Bayes,” arXiv
preprint arXiv:2309.04381, 2023.
[16] A. Sharma, S. Veer, A. Hancock, H. Yang, M. Pavone, and A. Majumdar,
“PAC-Bayes generalization certificates for learned inductive conformal
prediction,” in Proc. Advances in Neural Information Processing Systems
37, 2023.
[17] O. Simeone, Machine learning for engineers.
Cambridge University
Press, 2022.
[18] S. Park, K. M. Cohen, and O. Simeone, “Few-shot calibration of set pre-
dictors via meta-learned cross-validation-based conformal prediction,”
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.
[19] D. Stutz, K. D. Dvijotham, A. T. Cemgil, and A. Doucet, “Learning
optimal conformal classifiers,” in Proc. International Conference on
Learning Representations, 2021.
[20] A. Pensia, V. Jog, and P.-L. Loh, “Generalization error bounds for noisy,
iterative algorithms,” in Proc. IEEE Int. Symp. Inf. Theory (ISIT), (Vail,
CO, USA), 6 2018.
[21] V. Feldman and T. Steinke, “Calibrating noise to variance in adaptive
data analysis,” in Proc. Conf. Learning Theory (COLT), (Stockholm,
Sweden), 7 2018.
[22] M. Raginsky, A. Rakhlin, and A. Xu, Information-Theoretic Stability
and Generalization, p. 302–329. Cambridge University Press, 2021.
[23] M. B. Marcus and J. Zinn, “The bounded law of the iterated logarithm
for the weighted empirical distribution process in the non-I.I.D. case,”
The Annals of Probability, vol. 12, no. 2, pp. 335–360, 1984.
[24] M. J. Wainwright, High-Dimensional Statistics: a Non-Asymptotic View-
point. Cambridge, U.K.: Cambridge Univ. Press, 2019.
[25] M. Mohri, A. Rostamizadeh, and A. Talwalkar, Foundations of machine
learning. MIT press, 2018.
APPENDIX
A. On Assumption 2
In this section, we establish that Assumption 2 holds for
several common learning algorithms. To this end, we let P
denote the marginal distribution on the model parameter space
Θ induced by the product distribution Q × P n
Z on Θ × (X ×
Y)n. With this definition, the mutual information between the
model parameter and the training data set can be expressed as
the average I(θ; Dtr) = EDtr[KL(Q||P)]. The key observation
is that for Gibbs posteriors, ϵ-differentially private algorithms
with ϵ = O(log(ntr)/ntr), and stochastic gradient Langevin
dynamics (SGLD), the mutual information scales as [20]–[22]
I(θ; Dtr) = O(log ntr).
(26)
In the following lemma, which is a direct consequence of
Markov’s inequality, we show that, if the mutual information
is upper bounded by a logarithmic function of the number of
training points, ntr, as per (26), this implies that the Kullback-
Liebler (KL) divergence KL(Q||P) is also logarithmic in ntr
with high probability.
Lemma 1. Assume that Q is absolutely continuous with
respect to P and that the inequality I(θ; Dtr) ≤ c log(ntr)
holds for some constant c > 0. Then, for any δ ∈ (0, 1), with
probability at least 1 − δ, we have the following inequality
KL(Q||P) ≤ I(θ; Dtr)
δ
≤ c log(ntr)
δ
.
(27)
In the following proposition, we show that Lemma 1 suffices
to motivate Assumption 2 for the listed learning algorithms.
Proposition 1. If the condition in (27) is satisfied, Assump-
tion 2 holds.
Proof. As in (4), we define the empirical training c.d.f. at the
NC score level r for a fixed model θ as
ˆF(r|θ, Dtr) = 1
ntr
ntr
X
i=1
1{R(fθ(xi), yi) < r},
(28)
as well as the corresponding true population c.d.f. as in (13)
F(r|θ) = EX,Y [1{R(fθ(X), Y ) < r}].
(29)
For a given model θ, we denote the maximum absolute
discrepancy between the population c.d.f. and the empirical
training c.d.f. as in (14), i.e.,
∆(θ|Dtr) =
sup
r∈[0,Rmax]
|F(r|θ) − ˆF(r|θ, Dtr)|.
(30)
Following a symmetrization argument [23, Lemma 1.1], we
have the inequality
EDtr[eλ∆(θ|Dtr)] ≤ e
2λ2
ntr log(4),
∀λ ≥ 0.
(31)
Thus, the random variable ∆(θ|Dtr) is 2
p
log(4)/ntr-sub-
Gaussian [24, Thm. 2.6].
Next, the Donsker and Varadhan identity [14] states that for
a measurable and bounded function h(θ), we have
log
h
Eθ∼P
h
eh(θ)ii
= sup
Q∈P(Θ)
[Eθ∼Q[h(θ)] − KL(Q||P)], (32)
where P(Θ) is the set of all probability distributions on Θ.
Applying (32) to (ntr − 1)∆(θ|Dtr)2/(16 log(2)), we obtain
Eθ∼P EDtr[e
(ntr−1)∆(θ|Dtr)2
16 log(2)
]
(33)
= EDtrEθ∼P [e
(ntr−1)∆(θ|Dtr)2
16 log(2)
]
(34)
= EDtr[esupQ
ntr−1
16 log(2) Eθ∼Q[∆(θ|Dtr)2]−KL(Q||P )]
(35)
≤ √ntr,
(36)
where the first equality follows from Fubini’s Theorem and the
inequality follows by [24, Thm. 2.6.(IV)] with λ = 1 − 1/ntr.
From the Chernoff bound, we deduce the inequality
Pr
Dtr

sup
Q∈P(Θ)
ntr − 1
16 log(2)Eθ∼Q[∆(θ|Dtr)2] −KL(Q||P)> s

≤ EDtr[esupQ∈P(Θ)
ntr−1
16 log(2) Eθ∼Q[∆(θ|Dtr)2]−KL(Q||P )]e−s
(37)
≤ elog(√ntr)−s,
(38)
where the last inequality follows from (36).
Rearranging terms in (37)–(38), we get
Pr
Dtr

sup
Q∈P(Θ)
Eθ∼Q[∆(θ|Dtr)2]
> 16 log(2)(KL(Q||P) + s + log √ntr)
ntr − 1

≤ e−s. (39)
By Jensen’s inequality, it follows that Eθ∼Q[∆(θ|Dtr)2] ≥
(Eθ∼Q[∆(θ|Dtr)])2. Furthermore, we have that
Eθ∼Q[∆(θ|Dtr)] = Eθ∼Q

sup
r∈R
|F(r|θ) − ˆF(r|θ, Dtr)|

(40)
≥ sup
r Eθ∼Q
h
|F(r|θ) − ˆF(r|θ, Dtr)|
i
(41)
≥ ∆(Q|Dtr).
(42)
The final result is obtained by choosing s = − log(δ), which
leads to
Pr
Dtr

 sup
Q∈P(Θ)
∆(Q|Dtr) ≤ 4
p
log 2
s
KL(Q||P) + log
√ntr
δ
ntr − 1


≥ 1 − δ.
(43)
Using the union bound, we combine this with the assumed
bound (27), with the inequality ntr − 1 ≤ 2ntr, and we let
δ → δ/2. Thus, Assumption 2 holds with the function
β(δ, ntr) =
s
32 log 2
2c log(ntr)
δ
+ log 2√ntr
δ

.
(44)
B. Proof of Theorem 1
The expected set size is
ΛCP(Q|Dtr) = E

|ΓCP
θ (X|Dcal)|

,
(45)
where the expectation is taken over the calibration data Dcal ∼
P ⊗ncal
Z
, the input X ∼ PX, and the model θ ∼ Q(θ|Dtr). For
a reliability level α, the expected set size is [13]
ΛCP(Q|Dtr)
(46)
=E
Z
y
1{y ∈ Y:R(fθ(X), y)≤Q1−α(Rcal)}dy

(47)
=E
Z
y
Pr[R(fθ(X), y) ≤ Q1−α(Rcal)|Dtr]dy

(48)
=E
Z
y
Pr

Q1−α(Rcal)≥r
R(fθ(X), y)=r, Dtr

dy

(49)
+ E
Z
y
Pr

R(fθ(X), y) = r
Dtr

dy

=
Z
R
Pr

Q1−α(Rcal) ≥ r
Dtr

γ(r|Q, Dtr)|Y|dr,
(50)
where the last equality follows from the independence of Rcal
and R(fθ(X), y), and the definition of the weighting factor
γ(r|Q, Dtr) in (9).
By the definition of Q1−α(Rcal), we have the equivalence
Q1−α(Rcal) ≥ r ⇐⇒
ncal
X
i=1
1{R(fθi(Xi), Yi)<r} ≤ nα, (51)
with nα = ⌈(ncal + 1)(1 − α)⌉ − 1. It then follows that
Pr

Q1−α(Rcal)≥r
Dtr

=Pr
"" ncal
X
i=1
Bern(pi(r))≤nα
Dtr
#
, (52)
where Bern(pi(r)) denotes the Bernoulli random variable of
parameter pi(r) = Pr[R(fθi(Xi), Yi) < r]. By the def-
inition of calibration NC score (3), the random variables
{Bern(pi(r))}ncal
i=1 are independent and identically distributed,
and thus we have the equality pi(r) = F(r|Q, Dtr) for all
i ∈ 1, . . . , ncal with F(r|Q, Dtr) defined in (13). Therefore,
we have the equality
Pr

Q1−α(Rcal)≥r
Dtr

=Pr

Bin(ncal, F(r|Q, Dtr))≤nα
Dtr

,
(53)
with Bin(ncal, F(r|Q, Dtr)) being the binomial random vari-
able with parameters ncal and F(r|Q, Dtr).
From Assumption 2, with probability 1 − δ with respect
to the realization of the training data set Dtr, the probability
F(r|Q, Dtr) can be lower bounded as
F(r|Q, Dtr)
≥ 1
ntr
ntr
X
i=1
EQ(θ|Dtr)[1{R(fθ(xi), yi) < r}]
|
{z
}
= ˆ
F (r|Q,Dtr)
− β
√ntr
.
(54)
Since the binomial c.d.f. Bin(ncal, p(r)) is decreasing in the
probability p(r), we have
Pr

Q1−α(Rcal) ≥ r
Dtr

(55)
≤Pr

Bin

ncal, ˆF(r|Q, Dtr)− β
√ntr

≤nα
Dtr

.
(56)
Having defined Rmin as in (17), for values of r ≥ Rmin we have
the inequality ˆF(r|Q, Dtr) −
β
√ntr ≥ nα
ncal . From the Chernoff
bound, we then have the inequality
Pr

Q1−α(Rcal) ≥ r
Dtr

≤ e
−ncaldKL
 
nα
ncal

 ˆ
F tr(r)− β+µ
√ntr
!
(57)
for r ≥ Rmin, whereas for r < Rmin we use the trivial bound
Pr

Q1−α(Rcal) ≥ r
Dtr

≤ 1.
(58)
Combining (58) and (57) into (50), and using the monotonicity
of the multiplicative factor γ(r|Q, Dtr) (Assumption 1), we
obtain the desired bound.
C. Estimating the Bound
The evaluation of the expected set size bound (16) provided
in Theorem 1 requires calculating the empirical training c.d.f.
ˆF(r|Q, Dtr), which includes an expectation with respect to
the model θ ∼ Q(θ|Dtr). To sidestep this practical difficulty,
we consider estimating the empirical training c.d.f. using a
finite number of draws from the model distribution Q(θ|Dtr),
obtaining an expected set size bound that is easier to evaluate.
Denoting as θi ∼ Q(θ|Dtr) for i = 1, . . . , ntr, the models
drawn independently for each data point i, we define the
doubly empirical training NC score c.d.f. as
ˆFθ(r|Q, Dtr) = 1
ntr
ntr
X
i=1
1{R(fθi(xi), yi) < r}.
(59)
We have the following result.
Corollary 1. Given a learning algorithm Q(θ|Dtr) satisfying
Assumption 1 and Assumption 2, the expected set size of the
probabilistic CP predictor (6) satisfies the following inequality
ΛCP(Q|Dtr)
|Y|
≤
Z Rmax
Rmin,θ
e
−ncaldKL

nα
ncal
 ˆ
Fθ(r|Q,Dtr)− β+µ
√ntr

γ(r|Q,Dtr)dr
+ γ(Rmin,θ|Q, Dtr)Rmin,θ
(60)
with probability 1 − 2δ with respect to the random draw of
the training data set Dtr and of the models {θi}ntr
i=1, where we
have defined µ =
p
log(2/δ)/2 +
p
4 log(ntre/2) and
Rmin,θ =
inf
r∈[0,Rmax]

r : ˆFθ(r|Q, Dtr) ≥ nα
ncal
+ β + µ
√ntr

.
(61)
Proof. Using McDiarmid’s inequality and the fact that de-
cision stumps have VC dimension d = 2, the value of the
function ˆF(r|Q, Dtr) can be uniformly bounded based on the
estimate (59). In particular, with probability 1 − δ, we have
that for all r ∈ [0, Rmax] the inequality
ˆF(r|Q, Dtr) ≥ ˆFθ(r|Q, Dtr) −
µ
√ntr
(62)
holds, where µ =
p
log(2/δ)/2 +
p
4 log(ntre/2) [25, Corol-
lary 3.19]. Defining Rmin,θ as in (61), the final expression
is obtained following the same steps as the proof of Theo-
rem 1.
"
This study examines the Weihrauch degree of the problem of finding a bad sequence in a non-well quasi order (BS) relative to that of finding a descending sequence in an ill-founded linear order (DS). It aims to rectify a previous mistaken claim made in a prior article by demonstrating that the degrees of these problems are distinct.,"The paper establishes that BS is Weihrauch (W)-reducible to DS through the separation of their first-order parts. However, it also provides evidence to the contrary, demonstrating that DS is not W-reducible to KÖnig’s lemma (KL) and that BS is strictly above DS in the Weihrauch hierarchy.","The authors acknowledge and cite their previous work, which provides essential background and notation related to Weihrauch reducibility, DS, BS, and other relevant concepts.nannannan","The authors utilize various techniques to prove their claims, including constructing tree-like partial orders with no infinite descending sequences, demonstrating the existence of a computable procedure for guessing solutions to BS, and utilizing the notion of ACCN × f ≤W DS.nan","The study demonstrates that BS and DS are Weihrauch incomparable and that KL is not W-reducible to DS. It further indicates that if a parallelizable problem f satisfies ACCN ≤W f ≤W DS, then f ≤W lim, explicitly showing that KL ≠W DS.","The paper rectifies a mistaken claim from previous research and establishes a corrected Weihrauch hierarchy involving BS, DS, and KL. The findings contribute to a deeper understanding of the Weihrauch degrees of these problems and shed light on the relationship between descending sequences and ill-founded linear orders.",The weakness of finding descending sequences in ill-founded linear orders,"Jun Le Goh, Arno Pauly, Manlio Valenti","arXiv:2401.11807v1  [math.LO]  22 Jan 2024
The weakness of ﬁnding descending sequences in
ill-founded linear orders
Jun Le Goh & Arno Pauly & Manlio Valenti
We prove that the Weihrauch degree of the problem of ﬁnding a bad sequence in a non-well
quasi order (BS) is strictly above that of ﬁnding a descending sequence in an ill-founded linear
order (DS). This corrects our mistaken claim in [1], which stated that they are Weihrauch
equivalent.
We prove that K¨onig’s lemma KL is not Weihrauch reducible to DS either,
resolving the main open question raised in [1].
For background and notation on Weihrauch reducibility, DS, BS, etc., please refer to our
previous article [1].
1
Separating BS and DS
A key tool used in [1] is the notion of the ﬁrst-order part of a problem. In this section, we shall
separate BS and DS by separating their ﬁrst-order parts.
Theorem 1. 1(BS) ≰W 1(DS) and so DS <W BS.
This corrects our mistaken claim [1, Proposition 4.5] that BS and DS are Weihrauch equiv-
alent. Further results on the Weihrauch degree of BS (and variants thereof) are in preparation.
Recall from [1, Theorem 4.10] that the ﬁrst-order part 1(DS) is Weihrauch equivalent to
Π1
1−Bound, which is the problem of producing an upper bound for a ﬁnite subset of N (given
via a Π1
1 code). Observe that Π1
1−Bound is upwards closed (i.e. if n ∈ g(x), then m ∈ g(x) for
all m > n).
Lemma 2. Let f be a problem with codomain N. The following are equivalent:
1. there exists an upwards closed problem g with codomain N such that f ≤W g;
2. there is a computable procedure which takes as input any x ∈ dom(f) and produces a
sequence px ∈ NN of guesses for f-solutions to x which is correct coﬁnitely often.
Proof. For 1. ⇒ 2., let g be upwards closed and assume f ≤W g via Φ and Ψ. Given x ∈ dom(f),
dovetail the computations (Ψ(x, m))m∈N. Once some Ψ(x, m) halts, we output its result and
cancel Ψ(x, n) for all n < m.
For the converse direction, for every x ∈ dom(f), let px ∈ NN be as in the hypothesis. Deﬁne
M := max{m | px(m) /∈ f(x)} and let g(x) := {n | n > M}. Clearly g is upwards closed. The
fact that f ≤W g follows from the fact that x 7→ px is computable.
In light of Lemma 2, to prove Theorem 1 it suﬃces to show that one cannot computably
“guess” solutions for BS. In other words, given a computable procedure which tries to guess
extendible elements in a non-wqo, we want to construct a non-wqo P on which the procedure
outputs a non-extendible element inﬁnitely often. The non-wqos P we construct will be “tree-
like” in the following sense:
We are grateful to Takayuki Kihara for pointing out the mistake in our previous article. We thank also C´ecilia
Pradic for comments which greatly improved the presentation of this note.
2
The weakness of DS
Deﬁnition 3. A tree decomposition of a partial order (P, <P ) consists of a tree T ⊆ 2<N and a
function ι : T → P such that:
1. P is partitioned into ﬁnite P-intervals, where each interval has the form
(wb] = {v ∈ P | ι(w) <P v ≤P ι(wb)}
for some vertex wb ∈ T (with ﬁnal entry b), or (ε] = {ι(ε)} (where ε denotes the root of
2<N). For v ∈ P let ⌈v⌉ ∈ T be uniquely deﬁned by v ∈ (⌈v⌉].
2. If w1, w2 ∈ T and w1 is a proper preﬁx of w2 (written w1 ⊏ w2), then ι(w1) <P ι(w2).
3. If w1, w2 ∈ T are incompatible, so are ι(w1) and ι(w2) (i.e. they have no common upper
bound in P).
The following lemma is straightforward.
Lemma 4. If ι : T → P is a tree decomposition, then P has no inﬁnite descending sequences.
Moreover, T is wqo (i.e. it has ﬁnite width) if and only if P is wqo. In other words, T has an
inﬁnite antichain iﬀ so does P.
Proof. The fact that every partial order that admits a tree decomposition does not have an
inﬁnite descending sequence follows from the fact that if (vn)n∈N is an inﬁnite descending se-
quence in P, then since every interval (⌈vn⌉] is ﬁnite, up to removing duplicates, the sequence
(⌈vn⌉)(n ∈ N) would be an inﬁnite descending sequence in T.
If (wn)n∈N is an inﬁnite antichain in T then, by deﬁnition of tree decomposition, (ι(wn))n∈N
is an inﬁnite antichain in P. Conversely, if (vn)n∈N is an inﬁnite antichain in P, then for every
n, for all but ﬁnitely m, ⌈vn⌉ is ⊑-incomparable with ⌈vm⌉. In particular, we can obtain an
inﬁnite antichain in T by choosing a subsequence (vni)i∈N such that, for every i ̸= j, ⌈vni⌉ and
⌈vnj⌉ are ⊑-incomparable.
Lemma 5. There is no computable procedure that, given in input a partial order which admits
a tree decomposition, outputs a sequence of elements of that partial order such that if the input
is not wqo, then coﬁnitely many elements in the output are extendible to a bad sequence.
We point out a subtle yet important aspect regarding Lemma 5: The procedure only has
access to the partial order, not to a tree decomposition of it.
Proof. Fix a computable “guessing” procedure g that receives as input a partial order (admitting
a tree decomposition) and outputs a sequence of elements in that partial order. We shall build
a partial order P together with a tree decomposition ι : T → P in stages such that, inﬁnitely
often, g outputs an element of P that does not extend to an inﬁnite bad sequence.
Start with T0 = {ε} and P0 having a single element vε, with ι0(ε) = vε. In stage s, we have
built a ﬁnite tree decomposition ιs : Ts → Ps and wish to extend it to some ιs+1 : Ts+1 → Ps+1.
The tree Ts+1 will always be obtained by giving each leaf in Ts a single successor, and then
adding two successors to exactly one of the new leaves. To decide which leaf gets two successors,
say a ﬁnite extension Q of Ps is suitable for ιs : Ts → Ps if for every v ∈ Q \ Ps, there is exactly
one leaf w ∈ Ts such that ιs(w) <Q v. Pick the left-most leaf σ of Ts with the following property:
There is some suitable extension Q of Ps such that, when given Q, the guessing
procedure g would guess an element of Q which is comparable with ιs(σ).
J.L. Goh, A. Pauly, M. Valenti
3
To see that such σ must exist, consider extending Ps by adding an “inﬁnite comb” (i.e. a copy
of {0n1i | n ∈ N, i ∈ {0, 1}}) above the ιs-image of a single leaf in Ts. The resulting partial
order is non-wqo, admits a tree decomposition (obtained by extending Ts and ιs in the obvious
way), and its ﬁnite approximations (extending Ps) are suitable for ιs. Hence, by hypothesis, g
eventually guesses some element which must be comparable with ιs(σ), for some leaf σ ∈ Ts.
Having identiﬁed σ, we ﬁx any corresponding suitable extension Q of Ps. In order to extend
ιs, we further extend Q to Q′ by adding a new maximal element vw to Q for each leaf w ∈ Ts
as follows: vw lies above all v ∈ Q \ Ps such that ιs(w) <P v, and is incomparable with all other
elements (including the other new maximal elements vw′). To extend Ts, we add a new leaf τ⌢0
to Ts for each leaf τ, obtaining a tree T ′. We extend ιs to yield a tree decomposition ι′ : T ′ → Q′
in the obvious way.
Finally, we add two successors to σ⌢0 in T ′, i.e., deﬁne Ts+1 = T ′ ∪ {σ⌢00, σ⌢01}. We
also add two successors v1, v2 to ι′(σ⌢0) in Q′ to obtain Ps+1, and extend ι′ to ιs+1 by setting
ιs+1(σ⌢0i) = vi. This concludes stage s.
It is clear from the construction that ι : T → P is a tree decomposition. Let us discuss the
shape of the tree T. In stage s, we introduced a bifurcation above a leaf σs of Ts. These are the
only bifurcations in T. Observe that, whenever s′ < s, σs is either above or to the right of σs′,
because every suitable extension of Ps is also a suitable extension of Ps′ and, at stage s′, the
chosen leaf was the left-most. Therefore T has a unique non-isolated inﬁnite path p = lims σs,
and a vertex w in T is extendible to an inﬁnite antichain in T if and only if it does not belong
to p.
We may now apply Lemma 4 to analyze P. First, since T is not wqo, neither is P. Second,
we claim that if v <P ι(σ) for some vertex σ on p, then v is not extendible to an inﬁnite bad
sequence. To prove this, suppose v is extendible. Then so is ι(σ). The proof of Lemma 4 implies
that ⌈ι(σ)⌉ = σ is extendible to an inﬁnite antichain in T. So σ cannot lie on p, proving our
claim.
To complete the proof, observe that our construction of ι ensures that for each s, g(P)
eventually outputs a guess which is below ι(σs⌢0). Whenever σs⌢0 lies along p (which holds
for inﬁnitely many s), this guess is wrong by the above claim.
We may now complete the proof of Theorem 1.
Proof of Theorem 1. Suppose towards a contradiction that 1(BS) ≤W Π1
1−Bound. Since the
problem of ﬁnding an element in a non-wqo which extends to an inﬁnite bad sequence is ﬁrst-
order, it is Weihrauch reducible to Π1
1−Bound as well. Now Π1
1−Bound is upwards closed, so
there is a computable guessing procedure for this problem (Lemma 2). However such a procedure
cannot exist, even for partial orders which admit a tree decomposition (Lemma 5).
2
Separating KL and DS
In this section we shall prove KL ̸≤W DS (Corollary 7), providing a negative answer to [1,
Question 6.1].
Theorem 6. Let f be a problem. The following are equivalent:
1. \
ACCN × f ≤W DS
2. f ≤W lim.
4
The weakness of DS
Proof. The implication from (2) to (1) follows from lim ≤W DS (as shown in [1, Theorem 4.16]).
For the other direction, we consider a name x for an input to f together with witnesses Φ, Ψ
for the reduction. We show that, from them, we can uniformly compute an input q to \
ACCN
together with an enumeration of a set W such that W is the well-founded part of the (ill-founded)
linear order L built by the forward functional Φ on (q, x). We can then use lim to obtain the
characteristic function of W. Having access to this lets us ﬁnd an inﬁnite descending sequence
in L greedily by avoiding ever choosing an element of W. From such a descending sequence the
backward functional Ψ then computes a solution to f for x.
It remains to construct q = ⟨q0, q1, . . .⟩ and W to achieve the above. At the beginning, W is
empty, and we extend each qi in a way that removes no solution from its ACCN-instance. As we
do so, for each i /∈ W (in parallel), we monitor whether the following condition has occurred:
L (as computed by the ﬁnite preﬁx of (q, x) built/observed thus far) contains i and
some (ﬁnite) descending sequence ℓ such that
1. ℓ is L-above i (i.e. i <L minL ℓ);
2. the functional Ψ, upon reading the current preﬁx of (q, x) and ℓ, produces some
output m for the i-th ACCN-instance.
Once the above occurs for i (if ever), we remove m as a valid solution to qi. This means that ℓ
cannot be extendible to an inﬁnite descending sequence in L, so i must be in the well-founded
part of L. Hence we shall enumerate i into W. This completes our action for i, after which we
return to monitoring the above condition for numbers not in W. This completes the construction.
It is clear that each qi is an ACCN-instance (with solution set N if the condition is never
triggered, otherwise with solution set N\{m}). Hence L = Φ(q, x) is an ill-founded linear order.
As argued above, W is contained in the well-founded part of L. Conversely, suppose i lies in the
well-founded part of L. Fix an inﬁnite descending sequence r which lies above i. The backward
functional Ψ will have to produce all ACCN answers upon receiving (q, x) and r, including an
answer to qi. This answer is determined by ﬁnite preﬁxes only, and after having constructed
a suﬃciently long preﬁx of q, some ﬁnite preﬁx ℓ of r will trigger the condition for i (unless
something else triggered it previously), which ensures that i gets placed into W. This shows
that W is exactly the well-founded part of L, thereby concluding the proof.
Corollary 7. If f is a parallelizable problem with ACCN ≤W f ≤W DS, then f ≤W lim. In
particular, KL ≰W DS.
Proof. Since ACCN ≤W f ≤W DS and f is parallelizable, we have \
ACCN × f ≤W f ≤W DS. By
the previous theorem, f ≤W lim. We conclude that KL ≰W DS because ACCN ≤W KL and KL is
parallelizable, yet KL ≰W lim.
References
[1] Jun Le Goh, Arno Pauly & Manlio Valenti (2021): Finding descending sequences through ill-founded
linear orders. Journal of Symbolic Logic 86(2), doi:10.1017/jsl.2021.15.
"
"This work examines the challenge of blind demixing and super-resolution of point sources, where blind super-resolution is concurrently pursued for K point source signals within their superimposed mixture. We formulate this problem as a low-rank matrix demixing problem, taking advantage of the subspace assumption regarding unknown point spread functions. A convex recovery approach is proposed, utilizing the low-rank structure of each vectorized Hankel matrix associated with the target matrix. Our analysis reveals that to achieve precise recovery, the number of samples must satisfy the criterion n ≳ Ksr log(sn). Empirical evaluations validate the recovery capabilities and computational effectiveness of the convex method.","The simultaneous blind demixing and super-resolution of point sources refers to the problem of achieving blind super-resolution for K point source signals within their superimposed mixture. This challenge arises in various applications, including joint radar-communications, multi-user multi-channel estimation, and many others. However, blind super-resolution is inherently ill-posed and without additional assumptions. Therefore, we introduce a subspace assumption, reformulating the problem as a structured low-rank matrix demixing problem. Recent research efforts have successfully leveraged the inherent structure of data matrices to develop a variety of convex relaxation techniques for this task.","In [5], inspired by joint radar and communication systems, an atomic norm minimization (ANM) approach was proposed, specifically for simultaneous blind demixing and super-resolution with K = 2. Furthermore, in [7], a nuclear norm minimization method was designed for the same problem. Additionally, [9] extended ANM to address this problem for arbitrary K, but lacks theoretical analysis. Our focus is on addressing the simultaneous blind demixing and super-resolution problem for arbitrary values of K. We utilize the vectorized Hankel lift technique as introduced in [4] to exploit the low-dimensional structures within the target matrices. This approach enables a convex framework for reconstruction, and the exact recovery guarantees based on standard assumptions are established.nannannan","We start by formulating the problem mathematically, characterizing the superimposed mixture of K point source signals convolved with unknown point spread functions. By taking the Fourier transform and sampling, the problem can be expressed as a superposition of linear observations involving measurements from vectorized Hankel lifting. We then propose a convex approach named Multiple Vectorized Hankel Lift (MVHL) to recover the data matrices {X₂⁄k} from superimposed linear measurements. The key technique underlying MVHL is nuclear norm minimization, which promotes the low rank structure of vectorized Hankel matrices associated with the target matrices.nan","Our theoretical analysis, presented in Theorem III.1, demonstrates that under certain assumptions, MVHL can achieve exact recovery of the target matrices with probability at least 1 − c0(sn)^{-c1}, provided that the number of measurements n satisfies n ≳ Kμm0μm1sr log(sn). Empirical evaluations showcase MVHL's efficacy in recovering the target matrices, and its outperformance compared to ANM. Additionally, robustness tests reveal that MVHL can effectively handle noisy measurements. The application of MVHL in joint radar-communication systems is also demonstrated, with empirical results validating its channel parameter estimation capabilities.","In conclusion, this research comprehensively addresses the simultaneous blind demixing and super-resolution problem. The proposed MVHL method, based on nuclear norm minimization and vectorized Hankel lifting technique, offers a well-founded convex framework for recovery. Theoretical analysis guarantees exact recovery under specific conditions, and extensive simulations validate the effectiveness of MVHL in various scenarios. Future research could explore extending the framework to accommodate more general scenarios, such as unknown data matrix dimensions or time-varying environments.",Simultaneous Blind Demixing and Super-resolution via Vectorized Hankel Lift,"Haifeng Wang, Jinchi Chen, Hulei Fan, Yuxiang Zhao, Li Yu","Simultaneous Blind Demixing and Super-resolution
via Vectorized Hankel Lift
Haifeng Wang∗, Jinchi Chen†, Hulei Fan∗, Yuxiang Zhao‡, Li Yu‡
∗China Mobile (Zhejiang) Research & Innovation Institute, Hangzhou, China
{wanghaifeng40, fanhulei}@zj.chinamobile.com
†School of Mathematics, East China University of Science and Technology, Shanghai, China
{jcchen.phys}@gmail.com
‡China Mobile Research Institute, Beijing, China
{zhaoyuxiang, yuliyf}@chinamobile.com
Abstract—In this work, we investigate the problem of simul-
taneous blind demixing and super-resolution. Leveraging the
subspace assumption regarding unknown point spread functions,
this problem can be reformulated as a low-rank matrix demixing
problem. We propose a convex recovery approach that utilizes the
low-rank structure of each vectorized Hankel matrix associated
with the target matrix. Our analysis reveals that for achieving
exact recovery, the number of samples needs to satisfy the
condition n ≳ Ksr log(sn). Empirical evaluations demonstrate
the recovery capabilities and the computational efficiency of the
convex method.
Index Terms—Blind dimixing, blind super-resolution, vector-
ized Hankel lift.
I. INTRODUCTION
The simultaneous blind demixing and super-resolution of
point sources refers to the problem of concurrently achieving
blind super-resolution [1]–[4] for K point source signals
within their superimposed mixture. This problem arises in a
range of applications, including but not limited to joint radar-
communications [5], multi-user multi-channel estimation [6].
It is well-established that blind super-resolution is intrin-
sically ill-posed without additional assumptions [1]–[4]. The
problem under consideration can be viewed as an extension
of blind super-resolution, which exacerbates its complexity.
Consequently, we introduce a subspace assumption and refor-
mulate the problem of simultaneous blind demixing and super-
resolution as a structured low-rank matrix demixing problem.
Recent research efforts, as demonstrated in [5], [7]–[9],
have harnessed the inherent structure of data matrices to
develop various convex relaxation techniques for addressing
this problem. Specifically, in [5], inspired by joint radar
and communication systems, the authors proposed an atomic
norm minimization (ANM) approach for simultaneous blind
demixing and super-resolution with K = 2. Furthermore, in
[7], a nuclear norm minimization method was designed for the
same problem. Additionally, [9] extended ANM to address this
problem for arbitrary K, but the theoretical analysis is lacking.
In this paper, our focus centers on addressing the si-
multaneous blind demixing and super-resolution problem for
Corresponding authors: Jinchi Chen and Li Yu.
Jinchi Chen was partially supported by National Science Foundation of
China under Grant No. 12001108.
arbitrary values of K. We utilize the vectorized Hankel lift
technique as introduced in [4] to leverage the low-dimensional
structures within the target matrices. This approach enables a
convex framework for reconstruction, and the exact recovery
guarantees based on standard assumptions are established.
II. PROBLEM FORMULATION AND PROPOSED METHOD
A. Problem Formulation
Consider a set of K point source signals denoted as
{xk(t)}K
k=1, where the k-th signal can be expressed in the
following form:
xk(t) =
rk
X
ℓ=1
dk,ℓδ(t − τk,ℓ),
where δ(·) represents the Dirac function, rk indicates the
number of spikes, and {τk,ℓ}rk
ℓ=1 and {dk,ℓ}rk
ℓ=1 represent
the locations and amplitudes of the point source signals,
respectively.
Let y(t) be the summation of point source signals convolved
with unknown point spread functions, given by
y(t) =
K
X
k=1
xk(t) ∗ gk(t) =
K
X
k=1
rk
X
ℓ=1
dk,ℓgk(t − τk,ℓ).
(II.1)
By taking the Fourier transform of (II.1) and subsequently
sampling, we obtain that for j = 0, · · · , n − 1,
y[j] =
K
X
k=1
rk
X
ℓ=1
dk,ℓe−ı2π(j−1)τk,ℓˆgk[j],
(II.2)
where gk :=
ˆgk[0]
· · ·
ˆgk[n − 1]T ∈ Cn are unknown.
The goal of the simultaneous blind demixing and super-
resolution problem is to jointly recover both {dk,ℓ, τk,ℓ} and
{gk} from (II.2).
As previously mentioned, simultaneous blind demixing and
super-resolution is an ill-posed problem without any additional
assumptions. In alignment with prior research [5], [7]–[9], we
adopt a similar approach and make the assumption that each
point spread function gk lies within a known subspace defined
by Bk ∈ Cn×sk, such that:
gk = Bkhk,
k = 1, · · · , K,
arXiv:2401.11805v1  [cs.IT]  22 Jan 2024
where hk ∈ Csk denotes an unknown coefficient vector.
Under the subspace assumption and by leveraging a
lifting approach, we can express the measurements given
in (II.2) as a superposition of linear observations involving
n
X♮
k := Prk
ℓ=1 dk,ℓhkaT
τk,ℓ
oK
k=1:
y[j] =
K
X
k=1
D
ejbH
k,j, X♮
k
E
,
j = 0, · · · , n − 1,
where ej ∈ Rn represents the j-th standard basis vector in
Rn, bk,j denotes the j-th row of Bk, and aτ ∈ Cn stands as
the steering vector, defined as

1
e−ı2π·1·τ
· · ·
e−ı2π·(n−1)·τT ∈ Cn.
Without loss of generality, we assume that s1 = · · · = sK = s
and r1 = · · · = rK = r. Additionally, consider Ak as a linear
operator Ak : Cs×n → Cn, defined as

Ak

X♮
k

[j] =
D
ejbH
k,j, X♮
k
E
,
j = 0, · · · , n − 1.
Consequently, the measurement model can be succinctly ex-
pressed as
y =
K
X
k=1
Ak

X♮
k

.
(II.3)
Hence, the problem of simultaneous blind demixing and super-
resolution can be cast as the task of demixing a sequence of
matrices {X♮
k} from the superimposed linear measurements of
these matrices. Upon successfully recovering the data matrices
X♮
k, it becomes feasible to extract the frequencies τk,ℓ through
spatial smoothing MUSIC [4], [10]–[12].
B. Proposed Method
Let H denote the vectorized Hankel lifting operator, which
transforms a matrix X ∈ Cs×n into a matrix of dimensions
sn1 × n2, defined as follows:
H(X) =


x0
x1
· · ·
xn2−1
x1
x2
· · ·
xn2
...
...
...
...
xn1−1
xn−1
· · ·
xn−1

 ∈ Csn1×n2,
where xi ∈ Cs represents the i-th column of X, and n1 +
n2 = n+1. It has been demonstrated that the rank of H(X♮
k)
is at most r [4]. Consequently, we adopt the nuclear norm
minimization to promote the low rank structure of H(X♮
and consider the following convex approach to recover {X♮k)
k}:
min
{Xk}K
k=1
K
X
k=1
∥H(Xk)∥∗ ,
s.t.
K
X
k=1
Ak(Xk) = y,
(II.4)
which is denoted as the Multiple Vectorized Hankel Lift
(MVHL). Since this convex optimization problem can be
effectively tackled using various existing software packages,
our focus is narrowed down to evaluating the theoretical
performance of (II.4) and investigating when its solution aligns
with {X♮
k}.
III. MAIN RESULTS
Before presenting our main results, we will introduce some
standard assumptions.
Assumption III.1. The matrices {Bk} are independent, and
the column vectors {bk,ℓ}n
ℓ=1 of the subspace matrix Bk are
sampled independently and identically from a population F
which obeys the following properties:
E [bb∗] = Is and
max
0≤k≤s−1 |b[j]|2 ≤ µ0.
(III.1)
Remark III.1. Assumption III.1 is widely used in blind super-
resolution [1]–[4], dual-blind deconvolution [5], [7]–[9], and
can be satisfied by Rademacher random vector or when b is
uniformly sampled from a Discrete Fourier Transform matrix.
Assumption III.2. Let H(X♮) = UΣV H be the singular
value decomposition of H(X♮), where U ∈ Csn1×r, Σ ∈
Rr×r and V ∈ Cn2×r. Denote U H =

U H
0
· · ·
U H
n1−1
H,
where Uℓ = U[ℓs + 1 : (ℓ + 1)s, :] is the ℓ-th block of U for
ℓ = 0, · · · , n1 −1. The matrix X♮ is µ1-incoherence if U and
V obey that
max
0≤ℓ≤n1−1 ∥Uℓ∥2
F ≤ µ1r
n
and
max
0≤j≤n2−1
eT
j V
2
2 ≤ µ1r
n
for some positive constant µ1.
Remark III.2. Assumption III.2 is commonly adopted in blind
super-resolution, and is satisfied when the minimum wrap-up
distance between the locations of point sources is greater than
about 2/n.
Now we state our main results, whose proofs are deferred
to Section V.
Theorem III.1. Under Assumptions III.1 and III.2, the matri-
ces {X♮
k}K
k=1 are the unique optimal solution to the problem
(II.4) with probability at least 1 − c0(sn)−c1, provided that
n ≳ Kµ0µ1sr log(sn), where c0, c1 are absolute constants.
IV. SIMULATION RESULTS
In this section, we evaluate the empirical performance of
MVHL for simultaneous blind demixing and super-resolution
problem. To solve MVHL, we use the CVX optimization
framework [13].
A. Recovery Ability of MVHL
We begin by investigate the recovery performance of MVHL
in comparison to Atomic Norm Minimization (ANM) [9]
using the empirical phase transition framework. To generate
the data matrices {X♮
k}K
k=1, we follow this procedure: the
locations {τk,ℓ}L
ℓ are uniformly sampled from the interval
[0, 1), the amplitudes {dk,ℓ}L
ℓ are set as (1+10cℓ)e−ıΨℓ, with
cℓ uniformly sampled from [0, 1) and Ψℓ uniformly sampled
from [0, 2π); the coefficient hk is drawn from a standard Gaus-
sian distribution with normalization. The subspace matrices
{Bk}K
k=1 are independently sampled from the Discrete Fourier
Transform (DFT) matrix. The experiments are conducted for
K = 2, n = 48, and various values of r and s. We execute
each algorithm 20 times for every combination of r and s.
A successful reconstruction is defined when the relative error
satisfies
s PK
k=1∥X♮
k−Xk∥
2
F
PK
k=1∥X♮
k∥
2
F
≤ 10−3.
Figure 1 (a) and Figure 1 (b) display the phase transition
plots for MVHL and ANM, respectively. The figures reveal
that MVHL exhibits a higher phase transition curve compared
to ANM.
MVHL
1
2
3
4
5
6
Dimension of Subspace: s
1
2
3
4
5
6
Number of Spikes: r
0
0.2
0.4
0.6
0.8
1
(a)
ANM
1
2
3
4
5
6
Dimension of Subspace: s
1
2
3
4
5
6
Number of Spikes: r
0
0.2
0.4
0.6
0.8
1
(b)
Fig. 1. The phase transitions of MVHL and ANM
B. Robustness of MVHL
In this experiment, we aim to illustrate the robustness of
MVHL in the presence of additive noise. Consider the additive
noise model where a true signal y is contaminated by a noisy
vector e, given by:
e = ε · ∥y∥2 · w/ ∥w∥2 ,
where σ represents the noise level, and w follows a standard
multivariate normal distribution. We perform tests with values
of n equal to either 48 or 64, while setting s and r to 2. The
noise level ε is varied from 10−3 to 1, corresponding to a
signal-to-noise ratio (SNR) ranging from 60 dB to 0 dB. For
each combination of (n, ε), we conduct 10 random instances
of the problem. Figure 2 displays the average relative error
as a function of SNR. Notably, the plot clearly demonstrates
a linear relationship between the relative reconstruction error
and the noise level. Furthermore, it is evident that the relative
reconstruction error decreases as the number of measurements
increases.
Fig. 2. Performance of MVHL under different noise levels
C. Channel Parameter Estimation
In this section, we evaluate the performance of MVHL in the
context of joint radar-communication systems. The received
signal is formulated as described in [5]:
yp[n] =
L
X
ℓ=1
[dr]ℓe−2ıπ(n[τr]ℓ+p[νr]ℓ)sp[n]
+
J
X
j=1
[dc]je−2ıπ(n[τc]j+p[νc]j)gp[n],
where L and J represent the number of targets and propagation
paths, respectively. drℓ = 1L and dcj = 1J denote the channel
coefficients, τrℓ = 1L and τcj = 1J are the time delays, and
νrℓ = 1L and νcj = 1J are the Doppler frequencies. sp[n]
represents the discrete values of the Fourier transform of the
p-th pulse s(t−pT), while gp[n] denotes data symbols in the p-
th message. Under the low-dimensional subspace assumption,
and using arguments similar to those in [14], the observed
signal can be reformulated as:
y[i] =
*
eibH
i ,
L
X
ℓ=1
[dr]ℓu[ar]T
ℓ
+
+
*
eidH
i ,
J
X
j=1
[dr]jv[ac]T
j
+
,
(IV.1)
for j = 0, · · · , NP − 1. Here ar and ac are defined as
[ar]ℓ = [e−2ıπ((0)[τr]ℓ+(0)[νr]ℓ), · · · , e−2ıπ((N−1)[τr]ℓ+(P −1)[νr]ℓ)],
[ac]j = [e−2ıπ((0)[τc]j+(0)[νc]j), · · · , e−2ıπ((N−1)[τc]j+(P −1)[νc]j)].
It’s worth noting that (IV.1) is a special case of (II.3). In
the simulation, we set N = P = 10 and L = J = 2.
Each row of the subspace matrices is generated using the form

1
e2πıf
· · ·
e2πı(s−1)f
with f being uniformly sampled
from [0, 1] [1]. The target delays and Dopplers are drawn from
the interval [0, 1] uniformly at random. The target matrices are
recovered by solving problem (II.4). Subsequently, the delays
and Dopplers are estimated using 2D MUSIC. Figure 3 illus-
trates the true and estimated channel parameters for radar and
communications. The results demonstrate that the proposed
method, in conjunction with MUSIC, effectively recovers the
delays and Dopplers.
Fig. 3. Performance of MVHL for channel parameter estimation problem
V. PROOFS OF MAIN RESULT
The proof of Theorem III.1 relies on the dual certificate
technique which has been widely used in analyzing low rank
matrix recovery [15], [16] and blind super-resolution [4]. Let
H∗ be the adjoint of H which maps sn1 × n2 matrices to
matrices of size s × n. We define D2 = H∗H. Given a matrix
X ∈ Cs×n, the matrix D2 (X) ∈ Cs×n is given by
D2 (X) =

w0x0
· · ·
wn−1xn−1

,
where wi(i = 0, · · · , n − 1) is defined as
wi = ♯{(j, k) | j + k = i, 0 ≤ j ≤ n1 − 1, 0 ≤ k ≤ n2 − 1}.
In addition, let the operator G be G = HD−1. The ad-
joint of G, denoted by G∗, is given by G∗ = D−1H∗.
Letting Z
=
H(X)
=
GD(X), one has D(X)
=
G∗(Z) and (I − GG∗)(Z) = 0. Moreover, define D =
diag
where step (a) follows from (V.4) and (V.5), step (b) is due
to Lemma V.2, and the last inequality uses the fact that if
PT ⊥
k (Mk) = 0 for all 1 ≤ k ≤ K, then Mk = 0.
B. Construction of Dual Certificate
Following the well developed route in [16], the dual certifi-
cates {Yk}K
k=1 are constructed as follows: for any k ∈ [K],
we firstly divide the linear measurements Ak into t0 partitions,
denoted {Ωt}t0
t=1, and let m = n
t0 . Define
Ak,t(X) =

bk,jeT
j , X
	
j∈Ωt ∈ C|Ωt|.
(V.8)
Let Y 0
k
= 0 and λ0 := PK
k=1 Ak,1G∗(UkV H
k ). Then the
golfing scheme for constructing {Yk} are expressed as
λt−1 =
K
X
k=1
Ak,tG∗PTk(UkV H
k − Yk,t−1) and
Yk,t = Yk,t−1 + n
mGA∗
k,t(λt−1)
+ (I − GG∗) PTk
Thus, one has
P⊥
Tk(Yk)
 ≲ n
m
t0
X
t=1
K
X
j=1
αj,t−1.
(V.12)
Recall that
Ej,t−1 =

PTjGG∗PTj − n
mPTjGA∗
j,tAj,tG∗PTj

(Ej,t−2)
− n
m
X
k̸=j
PTjGA∗
j,tAk,tG∗PTk(Ek,t−2).
Applying Lemma 3.12 in [4] and inequality V.10 yields that
∥Ej,t−1∥G,F ≲
r
µ1r log(sn)
n
n
m
K
X
k=1
αk,t−2.
(V.13)
Utilizing Lemma 3.13 in [4] and inequality (V.11), the term
∥Ej,t−1∥G,∞ is bounded by
∥Ej,t−1∥G,∞ ≲ µ1r
n
n
m
K
X
k=1
αk,t−2.
(V.14)
After substituting (V.13) and (V.14) into (V.12), we have
K
X
j=1
αj,t−1
≲ nK log(sn)
m
rsµ0µ1r
n
+ sµ0µ1r
n
 K
X
j=1
αj,t−2
(a)
≤ 1
2
K
X
j=1
αj,t−2 ≤
1
2
t−1
K
X
j=1
αj,0.
where inequality (a) is due to m ≳ K√n√sµ0µ1r log(sn).
Finally, noting that Ej,0 = UjV H
j , applying Lemma 3.14
in [4] yields that
P⊥
Tk(Yk)
 ≲ n
m
t0
X
t=1
K
X
j=1
αj,t−1
≲ n
m
t0
X
t=1
1
2
t−1
K
X
j=1
αj,0
≲ nK log(sn)
2m
 r
sµ0µ1r log(sn)
n
+ sµ0µ1r
n
!
≤ 1
2,
when m ≳ √nK√sµ0µ1r log(sn).
VI. CONCLUSION
This paper studies the problem of simultaneous blind demix-
ing and super-resolution. We introduce a convex approach
named MVHL and rigorously establish its recovery perfor-
mance. Our analysis demonstrates that MVHL can achieve
exact recovery of the target matrices provided the sample
complexity satisfying n ≳ Ksr log(sn). Furthermore, we
illustrate the efficacy of MVHL through a series of numerical
simulations.
REFERENCES
[1] Yuejie Chi, “Guaranteed blind sparse spikes deconvolution via lifting
and convex optimization,” IEEE Journal of Selected Topics in Signal
Processing, vol. 10, no. 4, pp. 782–794, 2016.
[2] Dehui Yang, Gongguo Tang, and Michael B Wakin, “Super-resolution
of complex exponentials from modulations with unknown waveforms,”
IEEE Transactions on Information Theory, vol. 62, no. 10, pp. 5809–
5830, 2016.
[3] Shuang Li, Michael B Wakin, and Gongguo Tang, “Atomic norm denois-
ing for complex exponentials with unknown waveform modulations,”
IEEE Transactions on Information Theory, vol. 66, no. 6, pp. 3893–
3913, 2019.
[4] Jinchi Chen, Weiguo Gao, Sihan Mao, and Ke Wei, “Vectorized hankel
lift: A convex approach for blind super-resolution of point sources,”
IEEE Transactions on Information Theory, vol. 68, no. 12, pp. 8280–
8309, 2022.
[5] Edwin Vargas, Kumar Vijay Mishra, Roman Jacome, Brian M Sadler,
and Henry Arguello,
“Dual-blind deconvolution for overlaid radar-
communications systems,” IEEE Journal on Selected Areas in Infor-
mation Theory, 2023.
[6] Xiliang Luo and Georgios B Giannakis, “Low-complexity blind syn-
chronization and demodulation for (ultra-) wideband multi-user ad hoc
access,” IEEE Transactions on Wireless communications, vol. 5, no. 7,
pp. 1930–1941, 2006.
[7] Jonathan Monsalve, Edwin Vargas, Kumar Vijay Mishra, Brian M
Sadler, and Henry Arguello, “Beurling-selberg extremization for dual-
blind deconvolution recovery in joint radar-communications,”
arXiv
preprint arXiv:2211.09253, 2022.
[8] Roman Jacome, Edwin Vargas, Kumar Vijay Mishra, Brian M Sadler,
and Henry Arguello,
“Multi-antenna dual-blind deconvolution for
joint radar-communications via soman minimization,”
arXiv preprint
arXiv:2303.13609, 2023.
[9] Saeed Razavikia, Sajad Daei, Mikael Skoglund, Gabor Fodor, and Carlo
Fischione,
“Off-the-grid blind deconvolution and demixing,”
arXiv
preprint arXiv:2308.03518, 2023.
[10] JE Evans, “High resolution angular spectrum estimation technique for
terrain scattering analysis and angle of arrival estimation,” in 1st IEEE
ASSP Workshop Spectral Estimat., McMaster Univ., Hamilton, Ont.,
Canada, 1981, 1981, pp. 134–139.
[11] James Everett Evans, DF Sun, and JR Johnson,
“Application of
advanced signal processing techniques to angle of arrival estimation in
atc navigation and surveillance systems,” Tech. Rep., Massachusetts Inst
of Tech Lexington Lincoln Lab, 1982.
[12] Zai Yang, Petre Stoica, and Jinhui Tang, “Source resolvability of spatial-
smoothing-based subspace methods: A hadamard product perspective,”
IEEE Transactions on Signal Processing, vol. 67, no. 10, pp. 2543–2553,
2019.
[13] Michael Grant and Stephen Boyd, “Cvx: Matlab software for disciplined
convex programming, version 2.1,” 2014.
[14] Sihan Mao and Jinchi Chen, “Blind super-resolution of point sources via
projected gradient descent,” IEEE Transactions on Signal Processing,
vol. 70, pp. 4649–4664, 2022.
[15] Emmanuel J Cand`es and Benjamin Recht, “Exact matrix completion
via convex optimization,” Foundations of Computational Mathematics,
vol. 9, no. 6, pp. 717, 2009.
[16] David Gross, “Recovering low-rank matrices from few coefficients in
any basis,” IEEE Transactions on Information Theory, vol. 57, no. 3,
pp. 1548–1566, 2011.
[17] Haifeng Wang, Jinchi Chen, Hulei Fan, Yuxiang Zhao, and Li Yu,
“Robust simultaeous blind demixing and blind super-resolution based
on low rank vectorized hankel structures,” in preparation.
"
"Accurate real-time traffic prediction is crucial for reducing transportation time. Spatio-temporal graph neural networks (ST-GNNs) process traffic data as temporal graphs, predicting traffic conditions for each graph node. However, ST-GNNs often encounter challenges in timely predictions due to their computational complexity. Knowledge distillation (KD) enhances execution time by training a smaller (student) network using distilled data from a larger (teacher) network while maintaining accuracy. Pruning removes unnecessary student network parameters, improving performance. We propose a cost function designed to train a student using distilled teacher data and a pruning algorithm utilizing cost function data to calculate pruning scores. Our approach maintains student accuracy similar to the teacher, with a significant reduction in network parameters. We evaluate our method on two real-world datasets, demonstrating its effectiveness in traffic prediction.","Traffic prediction has gained significant importance due to the increasing use of vehicles in various contexts. Efficient real-time traffic prediction is crucial for reducing transportation time. To predict traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-time traffic data as temporal graphs. Despite its capabilities, it often encounters challenges in delivering efficient real-time predictions for real-world traffic data. Recognizing the significance of timely prediction due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to enhance the execution time of ST-GNNs for traffic prediction. In this paper, We introduce a cost function designed to train a network with fewer parameters (the student) using distilled data from a complex network (the teacher) while maintaining its accuracy close to that of the teacher. We use knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable the student to learn the complex patterns perceived by the teacher. However, a challenge arises in determining the student network architecture rather than considering it inadvertently. To address this challenge, we propose an algorithm that utilizes the cost function to calculate pruning scores, addressing small network architecture search issues, and jointly fine-tunes the network resulting from each pruning stage using KD.","nanTraffic prediction has been extensively studied using various techniques. Traditional methods often struggle with complex traffic flow dependencies and neglect spatial and temporal relationships. Deep networks capable of handling temporal and spatial data have been proposed to address these issues. In [22], the authors discuss the significance of traffic network prediction and its applications, such as network monitoring, resource management, and threat detection. Various recurrent neural network architectures, including standard RNN, LSTM, and GRU, are explored for network traffic prediction. In [23], the development and evaluation of short-term traffic prediction models using bidirectional and unidirectional long short-term memory (LSTM) neural networks are examined. Bidirectional LSTM (BiLSTM) models outperform unidirectional LSTM (Uni-LSTM) models, especially for speed and traffic flow prediction. A combination of uni-LSTM and BiLSTM architectures also enhances 15-minute predictions. However, LSTM and RNN networks have high execution times due to numerous parameters. For low-latency traffic prediction, spatio-temporal graph neural networks are introduced. These networks utilize graph convolution layers for spatial processing and convolution layers for capturing temporal correlations, offering reduced execution time and comparable or better accuracy. In [3], the spatio-temporal graph convolutional network (ASTGCN) model is introduced to predict traffic flow. ASTGCN addresses challenges in modeling spatial-temporal correlations in traffic data, using three components to capture recent, daily, and weekly dependencies. Spatial-temporal attention mechanisms and convolution layers effectively capture traffic data patterns, outperforming other methods in traffic flow prediction. In [1], the spatio-temporal graph convolutional networks (STGCN) framework is presented for time series prediction in traffic management. STGCN employs a graph-based convolutional structure, which enhances training efficiency and captures comprehensive spatial-temporal correlations. It consists of two spatio-temporal convolution blocks and a fully connected layer for making predictions. This approach outperforms other methods on real traffic datasets, providing a more efficient solution for traffic forecasting.nannan","We model the traffic network as graphs defined across time steps. The N components of observation Vt at time step t consist of data from N monitoring stations, encompassing information like speed and traffic flow. Each time step generates a new observation vector. In this paper, we model the traffic network as graphs defined across time steps. The N components of observation Vt at time step t consist of data from N monitoring stations, encompassing information like speed and traffic flow. Each time step generates a new observation vector. In this paper, we model the traffic network as graphs defined across time steps. The N components of observation Vt at time step t consist of data from N monitoring stations, encompassing information like speed and traffic flow. Each time step generates a new observation vector. In this paper, we model the traffic network as graphs defined across time steps. The N components of observation Vt at time step t consist of data from N monitoring stations, encompassing information like speed and traffic flow. Each time step generates a new observation vector. In this paper, we model the traffic network as graphs defined across time steps. The N components of observation Vt at time step t consist of data from N monitoring stations, encompassing information like speed and traffic flow. Each time step generates a new observation vector. In this paper, we model the traffic network as graphs defined across time steps. The N components of observation Vt at time step t consist of data from N monitoring stations, encompassing information like speed and traffic flow. Each time step generates a new observation vector. The observation vector Vt ∈ RN at time step t consists of data from N monitoring stations, encompassing information like speed and traffic flow. Each time step generates a new observation vector. Vertices and edges represent the stations and their connections. Weight matrices W ∈ RN×N signify relationships between stations. Our neural architecture, the ST-GCN[1], processes spatial and temporal data. Graph convolution layers capture spatial relationships and node features over time. Convolutional layers model temporal correlations. The architecture comprises two blocks with temporal and spatial layers, and an output block transforms information into the final output. The teacher and student networks share an identical number of blocks and layers. Any variations in parameters result from differences in channel numbers within the temporal and spatial layers of the hidden layer. The complex ST-GCN teacher network achieves high accuracy with 333,604 and 296,426 parameters for PeMSD7 and PeMSD8 datasets (Table 1 and Figure 1). Despite accuracy, its high parameter count results in significant computational cost. The challenge lies in balancing accuracy and computational cost, crucial for selecting models. To address the need for a lighter model, the student network is a scaled-down version of the teacher network. With 10,144 and 7,766 parameters for PeMSD7 and PeMSD8 (Table 1 and Figure 1), it maintains accuracy while reducing complexity and computational requirements. This lightweight solution is efficient for traffic prediction, especially in resource-constrained scenarios.nan","We utilized two knowledge distillation techniques. The initial approach, known as response-based distillation, involves transmitting crucial information from the teacher network's outputs to the student network. This leads to the development of a simpler and faster model while simultaneously preserving performance. The second method, feature-based knowledge distillation, focuses on hidden layers' knowledge, encompassing spatial and temporal correlations among graph nodes (Figure 1). Knowledge distillation proves valuable for transferring knowledge from complex networks to simpler ones, particularly in resource-constrained environments requiring rapid algorithm execution.","We addressed the critical challenge of predicting traffic conditions to reduce transportation time. Our chosen methodology involved leveraging the spatio-temporal graph convolutional network (ST-GCN) for processing traffic prediction data, and we proposed a solution to improve the execution time of ST-GCNs. This involved introducing a novel approach that employed knowledge distillation to train a smaller network (referred to as the student) using distilled data from a more complex network (referred to as the teacher), all while maintaining a high level of prediction accuracy. Additionally, we tackled the challenge of determining the architecture of the student network within the knowledge distillation process, employing Algorithm 1. Our findings demonstrated that a student network derived through Algorithm 1 exhibited enhanced performance during the knowledge distillation training process. Furthermore, by utilizing a simpler model in knowledge distillation, as opposed to the teacher model, as the base model in pruning, we illustrated that the performance of our algorithm was not necessarily contingent on the high accuracy of the base model. We evaluated our proposed concepts using two real-world datasets, PeMSD7 and PeMSD8, demonstrating the effectiveness of our approach in predicting traffic conditions. Overall, our research contributed to the field by presenting a methodology to optimize the execution time of spatio-temporal graph convolutional networks for traffic prediction without compromising accuracy.",Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction,"Mohammad Izadi, Mehran Safayani, Abdolreza Mirzaei","Knowledge Distillation on Spatial-Temporal Graph Convolutional
Network for Traffic Prediction
Mohammad Izadi, Mehran Safayani ⋆, Abdolreza Mirzaei
Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan 84156-83111, Iran
m.izadi@ec.iut.ac.ir, safayani@iut.ac.ir, mirzaei@iut.ac.ir
Abstract. Efficient real-time traffic prediction is crucial for reducing transportation time. To predict
traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-
time traffic data as temporal graphs. Despite its capabilities, it often encounters challenges in delivering
efficient real-time predictions for real-world traffic data. Recognizing the significance of timely prediction
due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to
enhance the execution time of ST-GNNs for traffic prediction. In this paper, We introduce a cost
function designed to train a network with fewer parameters (the student) using distilled data from
a complex network (the teacher) while maintaining its accuracy close to that of the teacher. We use
knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable
the student to learn the complex patterns perceived by the teacher. However, a challenge arises in
determining the student network architecture rather than considering it inadvertently. To address this
challenge, we propose an algorithm that utilizes the cost function to calculate pruning scores, addressing
small network architecture search issues, and jointly fine-tunes the network resulting from each pruning
stage using KD. Ultimately, we evaluate our proposed ideas on two real-world datasets, PeMSD7 and
PeMSD8. The results indicate that our method can maintain the student’s accuracy close to that of
the teacher, even with the retention of only 3% of network parameters.
Keywords: Traffic prediction · Spatial-temporal graph knowledge distillation · Spatial-temporal graph
neural network pruning · Model compression · Teacher-student architecture
1
Introduction
Traffic prediction has gained significant importance due to the increasing use of vehicles in various contexts
such as transportation of goods, city taxis, personal cars, buses, etc. [1,2]. The data used for traffic predic-
tion is obtained by monitoring stations in various urban areas. These stations record various parameters,
including vehicle speed, traffic density, traffic flow, and ..., at specific time periods. These records can be
modeled as temporal graphs through preprocessing [1]. Each graph node represents the traffic data of each
monitoring station and connections between these stations indicate graph edges. With this data in hand,
spatio-temporal graph neural networks will be capable of predicting future traffic conditions for each of the
graph nodes at desired time intervals. In real-world traffic prediction, dealing with a large number of nodes
can be computationally expensive and may demand substantial hardware resources. Our objective is to pro-
pose an idea to enhance the execution time of spatio-temporal graph convolutional networks (ST-GCNs)
within the domain of traffic prediction. These networks serve as robust tools in deep learning and complex
data analysis, dedicated to predicting relationships among various elements within spatio-temporal systems.
Leveraging knowledge distillation [4], we formulate a network that, owing to its reduced number of parame-
ters and subsequently lower execution time, can accurately predict traffic comparable to that of a complex
network. Initially, through knowledge distillation, we introduce a cost function that trains a network with
fewer parameters, denoted as the ”student,” by drawing insights from a comprehensive model known as the
”teacher.” This training occurs under the consideration of spatial-temporal structures and their temporal
relationships in graphs. The objective is to equip the student network with accuracy akin to the teacher
model. While knowledge distillation effectively addresses the challenge of training a compact network, it
falls short in providing guidance on the architecture of the student network. To overcome this, we introduce
a pruning algorithm capable of identifying and eliminating insignificant parameters in a complex teacher
⋆ Corresponding author: Mehran Safayani ( safayani@iut.ac.ir )
arXiv:2401.11798v1  [cs.LG]  22 Jan 2024
network. In leveraging knowledge distillation, we view parameter learnability as a solution for elimination.
The algorithm adeptly prunes the network while jointly training it using knowledge distillation.
We evaluate our proposed cost function and algorithm using two real-world traffic datasets: PeMSD7, com-
prising 228 nodes, and PeMSD8, consisting of 170 nodes. The results demonstrate that the student trained
with our cost function outperforms previous approaches in terms of knowledge distillation and execution
time. Additionally, we showcase that our pruning algorithm effectively determines the architecture of the
student network, enhancing learnability during training with knowledge distillation compared to when the
algorithm does not employ knowledge distillation.
In the following subsection, we will provide further explanation of the proposed knowledge distillation and
pruning algorithm.
1.1
Knowledge Distillation and Pruning
Knowledge distillation (KD) and Pruning are two complementary techniques employed in this study for
model compression and performance enhancement. KD, serving as a model compression approach, involves
transferring knowledge from a complex teacher model to a simpler student model, with the aim of reducing
computational overhead [4,8,9,10]. This process entails training the smaller network in an optimized manner
to mitigate potential accuracy loss resulting from parameter reduction [4].
In the specific context of ST-GNNs for traffic prediction, KD offers advantages such as reducing model size
and the number of parameters, with the teacher model transferring spatial and temporal complexities to the
student model for accelerated learning while maintaining accuracy.
Knowledge distillation (KD) is categorized into response-based, feature-based, and relation-based methods
[11,12,13,14,15,16,17]. Response distillation focuses on the final layer’s output, while feature distillation tar-
gets intermediate layers. This paper adopts both response-based and feature-based knowledge distillation
methods. Additionally, KD can be classified based on distillation strategy into offline, online, and self-
distillation [18,19,20]. The chosen strategy in this work is offline distillation, where both student and teacher
networks are updated. An inherent challenge in KD is the selection of the student network structure, ad-
dressed in this study by intentionally reducing parameters in the student network compared to the teacher
model. The paper utilizes pruning methods to achieve this reduction, and the remaining student network
parameters’ learnability is a key criterion for employing KD.
Pruning, as a separate neural network compression technique, is employed to reduce model complexity and
enhance performance by eliminating unnecessary parameters and connections [5,21].
Specifically, in this study, pruning is applied to filters in hidden layers, involving the removal of weights or
connections with the lowest importance based on specified thresholds derived from methods like gradient
analysis or entropy information [6]. The integration of KD into the pruning process establishes criteria for
assessing the impact of network parameters on model accuracy and their ability to learn from the distilled
data provided by the teacher model. The evaluation of parameter competency becomes a crucial criterion in
the joint application of knowledge distillation and pruning.
The structure of the paper is organized as follows: Section 2 explores related works on knowledge distillation,
network pruning, and spatio-temporal graph neural networks. Section 3 introduces a ST-GNN model and
the proposed knowledge distillation methods. It also discusses a pruning algorithm designed to address chal-
lenges in the student network architecture. Section 4 evaluates the performance of various cost functions on
the PeMSD7 and PeMSD8 datasets, presenting ablation studies and analyzing their results. Finally, Section
5 outlines future works and draws conclusions.
2
Related Works
2.1
Traffic Prediction and Spatio-Temporal Graph Neural Networks
Fast and accurate traffic prediction is essential for urban traffic control and management, particularly for
medium and long-term forecasts. Traditional methods often struggle with these predictions due to the com-
plexities of traffic flows and the neglect of spatial and temporal dependencies. To address this, deep networks
capable of handling temporal and spatial data have been proposed. In [22], the paper discusses the signifi-
cance of traffic network prediction and its applications, such as network monitoring, resource management,
2
and threat detection. Various recurrent neural network architectures, including standard RNN, LSTM, and
GRU, are explored for network traffic prediction. In [23], the development and evaluation of short-term traffic
prediction models using bidirectional and unidirectional long short-term memory (LSTM) neural networks
are examined. Bidirectional LSTM (BiLSTM) models outperform unidirectional LSTM (Uni-LSTM) models,
especially for speed and traffic flow prediction.
A combination of uni-LSTM and BiLSTM architectures also enhances 15-minute predictions. However, LSTM
and RNN networks have high execution times due to numerous parameters. For low-latency traffic predic-
tion, spatio-temporal graph neural networks are introduced. These networks utilize graph convolution layers
for spatial processing and convolution layers for capturing temporal correlations, offering reduced execution
time and comparable or better accuracy.
In [3], the spatio-temporal graph convolutional network (ASTGCN) model is introduced to predict traffic
flow. ASTGCN addresses challenges in modeling spatial-temporal correlations in traffic data, using three
components to capture recent, daily, and weekly dependencies. Spatial-temporal attention mechanisms and
convolution layers effectively capture traffic data patterns, outperforming other methods in traffic flow pre-
diction. In
[1], the spatio-temporal graph convolutional networks (STGCN) framework is presented for
time series prediction in traffic management. STGCN employs a graph-based convolutional structure, which
enhances training efficiency and captures comprehensive spatial-temporal correlations. It consists of two
spatio-temporal convolution blocks and a fully connected layer for making predictions. This approach out-
performs other methods on real traffic datasets, providing a more efficient solution for traffic forecasting.
2.2
Knowledge Distillation
Knowledge Distillation, also known as Teacher-Student Learning, is a method introduced in the paper [4] in
2015. In [4], the authors introduced knowledge distillation, a method that transfers knowledge from a complex
and accurate neural network (the ”teacher”) to a smaller and more efficient neural network (the ”student”).
The aim is to align the behavior and predictions of the student network with the teacher network, allowing
the student to benefit from the rich knowledge of the teacher, leading to improved performance and reduced
computational resources required for predictions. Initially, knowledge distillation was applied to training
smaller models from complex ones, with the teacher network being a complex, accurate neural network with
many parameters, and the student network being trained to align its outputs with the teacher’s, resulting in
improved performance and efficiency. In further research, knowledge distillation extended to graph networks
and spatiotemporal graph networks.
In [25], the focus is on knowledge transfer in Convolutional Neural Networks (CNN) and graph convolu-
tional networks (GCN), introducing a novel approach for knowledge transfer from a GCN teacher model
to a GCN student model, leading to a smaller model with improved execution time, particularly suited for
dynamic graph models. In [26], the objective is to train a simpler network through knowledge distillation
from multiple GCN teacher models with different tasks, extending to distill spatial graph information from
hidden convolutional layers. This approach empowers the student model to excel in various tasks without
additional labeled data. In [27], knowledge distillation is extended to spatio-temporal graph neural networks
for modeling spatial and temporal data in human body position videos, utilizing various techniques, includ-
ing minimizing loss, leveraging spatial-temporal relations, and using the gradient rejuvenation method to
optimize the student model with distilled knowledge from the teacher.
2.3
Pruning and Fine-Tuning
In 1990, the concept of neural network pruning was first presented in the paper by [5]. Pruning involves elimi-
nating unnecessary connections or neurons from trained neural networks to enhance computational efficiency
without compromising network performance significantly. The authors presented a method for identifying
connections safe to prune using the Hessian matrix associated with the network’s error function. This ap-
proach aims to reduce the parameters in large neural networks, lowering the risk of overfitting, decreasing
computational resource requirements, and facilitating deployment on resource-constrained devices. Pruning
connections has become an essential tool for compressing neural networks and optimizing models for efficient
deployment across various applications.
In 2019, [6] introduced a method for estimating the importance of layers and weights in neural networks,
3
enabling the removal of unimportant weights and reducing network size. This method accurately assesses
the importance of each layer and network parameter, followed by the removal of unimportant weights. The
iterative pruning and retraining stages enhance performance and reduce model complexity. Removing unim-
portant weights reduces network size, leading to reduced computational resource consumption and improved
model execution speed. This approach offers a substantial improvement in compressing large network models.
3
Proposed Methods
Traffic prediction, involves predicting future values of key traffic conditions (e.g., speed or traffic flow) for
the next h time steps based on M prior traffic observations. The observation vector Vt ∈ RN at time step
t consists of data from N monitoring stations, encompassing information like speed and traffic flow. Each
time step generates a new observation vector. In this paper, we model the traffic network as graphs defined
across time steps. The N components of observation Vt are interconnected through pairwise connections in
the graph, making the data points Vt graph signals in a directed graph G with weights Wij. At time step
t, data from N monitoring stations is represented as a graph Gt = (Vt, E, W), where Vt contains features
from N nodes (e.g., road segments) with speed as the chosen criterion. The edge set E depicts connections
between stations, defined based on distance criteria, reflecting the influence of stations on each other. Weights
W ∈ RN×N in the weighted adjacency matrix signify the relationships between stations, often determined
by spatial disparities.
Table 1: Network information for knowledge distillation and pruning
PeMSD7
Models
Parameters
Hidden Blocks Channel
Test Time (s)
FLOPS
Teacher
333,604
[1, 32, 64][64, 32, 128]
3.423
49,889,172,087
Pruning Base Model
48,628
[1, 8, 16][16, 8, 32]
1.069
9,113,934,711
Student
10,144
[1, 2, 4][4, 2, 8]
0.547
1,726,990,455
PeMSD7
Models
Parameters
Hidden Blocks Channel
Test Time (s)
FLOPS
Teacher
296,426
[1, 32, 64][64, 32, 128]
2.556
40,636,466,453
Pruning Base Model
39,290
[1, 8, 16][16, 8, 32]
0.810
5,659,617,749
Student
7,766
[1, 2, 4][4, 2, 8]
0.441
1,003,700,933
3.1
Neural Architecture
The ST-GCN[1], designed for traffic prediction, processes spatial and temporal data. Graph convolution layers
capture spatial relationships and node features over time. Convolutional layers model temporal correlations,
focusing on sequential patterns and changes between graphs. The architecture comprises two blocks with
temporal and spatial layers, and an output block transforms information into the final output. The teacher
and student networks share an identical number of blocks and layers. Any variations in parameters result
from differences in channel numbers within the temporal and spatial layers of the hidden layer. The complex
ST-GCN teacher network achieves high accuracy with 333,604 and 296,426 parameters for PeMSD7 and
PeMSD8 datasets ( Table 1 and Figure 1 ). Despite accuracy, its high parameter count results in significant
computational cost. The challenge lies in balancing accuracy and computational cost, crucial for selecting
models. To address the need for a lighter model, the student network is a scaled-down version of the teacher
network. With 10,144 and 7,766 parameters for PeMSD7 and PeMSD8 ( Table 1 and Figure 1 ), it maintains
accuracy while reducing complexity and computational requirements. This lightweight solution is efficient
for traffic prediction, especially in resource-constrained scenarios.
4
LORD
LSTCD
TEACHER
STUDENT
OUTPUT
HIDDEN BLOCK
HIDDEN BLOCK
OUTPUT
Vt+1
OUTPUT
Vt+1
TARGET
Vt+1
Addition using 
coefficients
Summation
INPUT        Vt-M+1
INPUT        Vt-1
INPUT        Vt
Fig. 1: The illustration depicts both our student and teacher models, demonstrating how our cost functions
are applied to the spatio-temporal graph convolutional network (ST-GCN).
3.2
Knowledge Distillation
We utilized two knowledge distillation techniques. The initial approach, known as response-based distillation,
involves transmitting crucial information from the teacher network’s outputs to the student network. This
leads to the development of a simpler and faster model while simultaneously preserving performance. The sec-
ond method, feature-based knowledge distillation, focuses on hidden layers’ knowledge, encompassing spatial
and temporal correlations among graph nodes (Figure 1). Knowledge distillation proves valuable for trans-
ferring knowledge from complex networks to simpler ones, particularly in resource-constrained environments
requiring rapid algorithm execution.
3.2.1
Response-based Distillation
Leveraging the L2 and Kullback-Leibler (KL) divergence metrics, these functions measure the differences be-
tween the teacher and student network outputs [27,28,30,29,25]. The student network endeavors to enhance
its accuracy compared to the teacher by minimizing these error functions. Equations (2) and (3) employ
the L2 error function to quantify the difference between the outputs of the teacher and student networks
for each node. This function assesses accuracy by summing the squared differences between the values of
each network. The KL divergence error function in Equation 3 gauges the disparity between the probability
distributions of the outputs of the teacher and student networks.
5
∀b ∈ B, ∀i ∈ N :
KL(ys
bi, yt
bi) = yt
bi log
yt
bi
ys
bi

(1)
∀b ∈ B, ∀i ∈ N :
LRD(KL)bi = β · KL(ys
bi, yt
bi) + (1 − β) · ∥ys
bi − Tbi∥2
(2)
∀b ∈ B, ∀i ∈ N :
LRD(L2)bi = β · ∥ys
bi − yt
bi∥2 + (1 − β) · ∥ys
bi − Tbi∥2
(3)
In these equations, || · ||2 denotes the L2 norm. The variables N, ys, and yt represent the number of nodes,
outputs of the student network, and outputs of the teacher network, respectively. KL is the Kullback-Leibler
divergence metric (Equation 1), Tbi is the target data value for batch b, and β is an adjustable coefficient.
These cost functions consider both the teacher and target data simultaneously. However, we propose an
alternative approach described by Equations (4) to (6) to decide when to utilize teacher predictions and
when to use target data during student training. This decision is based on evaluating the difference between
teacher predictions and target data. If this difference exceeds a threshold (α1), indicating potential inaccuracy,
we incorporate the teacher’s prediction during student training for that node. Otherwise, we use the target
data itself. This decision is made to account for potential noise in the target data, making the teacher’s
prediction more valuable in such cases.
∀b ∈ B, ∀i ∈ N : Dt
bi =
yt
bi − Tbi

(4)
Dt
bi =
Dt
bi − min(Dt
b)
max(Dt
b) − min(Dt
b)
(5)
LORD+ =
X
b∈B
X
i∈N



∥ys
bi − Tbi∥2 : Dt
bi ≤ α1
∥ys
bi − yt
bi∥2 : Dt
bi > α1
(6)
In these equations, Dt
bi represents the absolute differences between teacher predictions and target data for
each node in batch b. After normalization (Equation 5), each element is compared with the threshold α1 to
identify potentially noisy data. The loss function LORD is then computed by summing these values for each
node across all batches. You can observe the representation of this cost function in Figure 1.
3.2.2
Feature-based Knowledge Distillation in Hidden Layers
Knowledge distillation from hidden layers simplifies the training process for deep and complex neural net-
works, making it less intricate and time-consuming [31]. This method enables the training of simpler models
that preserve meaningful information from the data and leverage the hidden knowledge of more complex
models. Particularly beneficial in resource-constrained scenarios, this approach optimizes model training with
limited resources. In the following subsections, we elaborate on our cost functions designed to capture both
spatial and temporal correlations between the teacher and student networks. These cost functions aim to
align the output of corresponding layers, fostering a close relationship in both spatial and temporal aspects.
By addressing the intricacies of temporal correlation in temporal correlation distillation (described in equa-
tions (7) and (8)), and recognizing the resemblance between spatial and temporal layers in spatial correlation
distillation (explained through equations (9) and (10)), our approach seeks to enhance the performance and
knowledge transferability of the student network.
Temporal Correlation Distillation We introduce the cost function LTCD (as defined in equations (7) and
(8) and illustrated in Figure 1 ) to ensure that the output of temporal layers in the student network closely
aligns with the corresponding layers in the teacher network. The objective of LTCD is to enable the temporal
layers in the student network, which have fewer parameters compared to equivalent layers in the teacher
network, to perform similarly. By quantifying differences in temporal correlation values between nodes, this
cost function guides the student network toward approximating these values in the teacher network.
6
TCDbnij = 1
C
C
X
c=1
|Fbinc − Fbjnc| ,
TCD ∈ RB×N×T ×T
(7)
LTCD =
1
B · N ·
In this equation, a regularization term is incorporated into the equation to reduce the number of parameters.
The symbol λ serves as a scaling factor, and ∥·∥0 denotes the L0 norm, which signifies the count of non-zero
elements. Minimizing the L0 norm poses a challenge due to its non-convex nature. As outlined in the method
presented by [6], Equation 13 is utilized to establish a score I for the parameter m. This involves employing
gradients and weights obtained during the training phase through a first-order Taylor series expansion.
I(1)
m (W, B) =
B
X
b=1
(gm,b wm,b)2
(13)
In this equation, B represents the batch, W is the weight matrix, and g represents gradients. The importance
score Im is straightforward to calculate since the gradient g is obtained through backpropagation.
According to [6], this importance score can be thought of as a set of parameters labeled as ws, similar to a
convolutional filter. It is defined as a contribution to group sparsity in Equation 14:
I(1)
S (W, B)
△=
X
s∈S
I(1)
s (W, B) =
X
s∈S
 B
X
i=1
(gs,i ws,i)2
!
(14)
In this paper, Equation 14 is used to obtain the importance score of parameters. However, instead of using the
gradients and weights obtained from training the network in the standard form, the network is trained using
cost function defined in the section 3.2. The importance score obtained from the gradients and weights of
the network, this time, not only indicates the importance of the parameter in the output but also reflects the
learning ability of the knowledge perceived and extracted by the teacher. The proposed method is outlined
in Algorithm 1.
Algorithm 1 Jointly KD-Pruning Algorithm
1: Input: base model - Pre-trained model
2: Output: Pruned mask M
3: Initialize:
4: minibatch counter ← 0
5: pruning minibatch ← your value here
6: KDIS ← Zero matrix with the shape of hidden block weights
7: M ← 1 matrix with the shape of hidden block weights
8: pruning percentage ← n%
9: for each minibatch do
10:
minibatch counter ← minibatch counter + 1
11:
grads, weights ← Finetune(base model, LSTCD, M)
12:
KDIS ← KDIS + compute KDIS(grads, weights)
13:
if minibatch counter == pruning minibatch then
14:
minibatch counter ← 0
15:
KDIS ← KDIS / pruning minibatch
▷ Update pruning decision
16:
for each layer in layers do
17:
sort(KDIS[l], ASC)
18:
for each index in first Npercent indexes(KDIS) do
19:
M[l][p] ← 0
▷ Prune N% of parameters with the lowest KDIS
20:
end for
21:
end for
22:
end if
23: end for
24: for several epochs do
25:
Finetune(base model, ST-GCN, M)
26: end for
27: return M
8
The algorithm defines a mask matrix M to selectively retain or discard parameters in the network, with
values of 1 or 0, respectively. An importance matrix KDIS is initialized to calculate the importance score of
each parameter. The algorithm specifies minibatch intervals for pruning steps and the percentage of param-
eters to be pruned at each step. During each minibatch, the model undergoes fine-tuning using the LSTCD
loss function (line 11), where gradients and importance scores (KDIS) are computed based on the network
weights. Knowledge distillation (KD) is applied to calculate gradients and weights, denoted as KDIS instead
of I. After a specified minibatch count in line 5, importance values are averaged, and parameters are pruned
in each layer according to the specified percentage (lines 13 to 19). The pruned network is then fine-tuned
over multiple epochs (lines 24 and 25). The resulting mask matrix M contains values of zero (removed
parameters) and one (remaining parameters). Multiplying this matrix by the weight matrix of the student
network’s base architecture yields the final pruned network.
4
Experiments and results
We thoroughly evaluate our model by testing it on two real traffic datasets: PeMSD7, which includes 228
nodes, and PeMSD8, with 170 nodes.
These datasets are collected by the California Department of Transportation and the San Bernardino City
Transportation Commission.
PeMSD7 is collected by over 39,000 sensor stations located throughout the main urban areas of the Cali-
fornia state freeway system [1]. This dataset is gathered at 5-minute intervals from 30-second data samples.
In this paper, we randomly select an average scale from Region 7 of California, consisting of 228 stations,
labeled as PeMSD7. The time range for the PeMSD7 dataset is on weekdays, during the months of May and
June in the year 2012. We choose training, validation, and test sets of 35, 5, and 5 days, respectively.
PeMSD8 is similar to PeMSD7, with the difference that it includes traffic data for the city of San Bernardino
from July to August 2016, collected from 170 monitoring stations on 8 streets at 5-minute intervals.
4.1
Data Preprocessing
A linear interpolation approach is employed to fill in missing values after data cleaning. Additionally, the in-
put data is normalized using the Z-Score approach. The normalization process is represented in Equation 15:
z = x − µ
σ
(15)
In this equation, x represents the point for which we want to calculate its position relative to the dataset’s
mean. µ denotes the mean of the dataset, serving as a reference point. σ is the standard deviation of the
dataset, representing a measure of the data’s dispersion. In PeMSD7 and PeMSD8, the adjacency matrix
of the road graph is calculated based on the distances between stations in the traffic network. The weighted
adjacency matrix W can be formed by Equation 16:
Wij = exp
 
−d2
ij
σ2
!
+ ε
(16)
Here, Wij is the weight of the edge determined by dij (distance between station i and j). σ2 and ε are thresh-
olds controlling the distribution and dispersion of the W matrix, assigned values of 10 and 0.5, respectively.
4.2
Hardware and Execution Details:
All presented results were generated using a TI 1080 GTX graphics card. We consistently used 12 historical
timesteps M to predict 9 future timesteps H. Our model predicts these 9 timesteps sequentially, as illustrated
in Figure 2. In this figure, each prediction (e.g., Vt+1) is utilized as the last graph in the input series to
forecast the next timestep (Vt+2). This sequential approach enables us to predict h future timesteps. With
data collected at 5-minute intervals, selecting 9 time units for prediction allows us to report results for the
9
next 15, 30, and 45 minutes. Throughout all runs, the learning rate decreases by a factor of 0.7 after 5 epochs.
Vt-M+1
Vt-1
Vt
...
Vt-M+2
Vt
Vt+1
...
Vt-M+3
Vt+1
Vt+2
...
Vt+1
Vt+2
...
Vt-M+H
Vt+H-2
Vt+H-1
...
Vt+3
Vt+H
Fig. 2: Sequence of ST-GCN predictions for future time steps. In each step, the output of the current state
is used as the last input graph to predict the next timestep.
Visualization of Hidden Layer Representations All the results depicted in the t-SNE plots illustrate
the hidden layer representation (which functions as the input to the output layer) for 50 test data points,
each represented as an individual data instance, and is subsequently fed into t-SNE for visualization. In
the plots, the red dots signify the student, while the blue dots represent either the teacher or the baseline
pruning network (utilized exclusively for certain pruning results). The data employed in these visualizations
is sourced from two datasets: PeMSD7 (depicted in Figure a) and PeMSD8 (depicted in Figure b). We can
assess the effectiveness of our method by examining the t-SNE plots. In these visual representations, the
proximity or overlap of the red and blue dots serves as an indicator of the model’s success. Ideally, when the
method is effective, the red and blue dots should be close to or overlapping each other.
4.3
Evaluation Metrics and Training Details:
The results of traffic prediction in all tables are reported for the next 15, 30, and 45 minutes. The output
values and calculated errors for these three time units are reported using three error functions: MAPE
(Mean Absolute Percentage Error), MAE (Mean Absolute Error), and RMSE (Root Mean Square Error).
The execution time reported in the tables are based on a batch of 1140 data and average over 100 runs.
4.4
Knowledge Distillation Analysis
All knowledge distillation experiments, except for pruning, employed the teacher and student models outlined
in Table 1 and depicted in Figure 1 which provide comprehensive information on both networks, serving as a
reference for subsequent comparisons and analysis. In the table, the column labeled ’Hidden Blocks Channel’
represents the channels for each spatial and temporal layer in the two hidden blocks of the ST-GCN network.
For a more detailed explanation, we have visualized these channels on each layer in the accompanying Figure.
Additionally, the columns: ”Test Time (s),” denotes the average test time over 100 trials, and ”FLOPS,”
indicating the network’s floating-point operations per second (FLOPS) on a single forward pass. The student
to teacher parameter ratio is approximately 3% in both datasets. The execution time of the student network
has reduced by approximately 83%−84% compared to the teacher. The number of blocks and layers in both
networks is the same, and the difference in the number of parameters is due to the variation in the number
of channels in each layer. The FLOPS ratio of the student network to the teacher is approximately 4% for
the PeMSD7 dataset and 2.5% for the PeMSD8 dataset. We compared our proposed loss function, LORD,
10
with other knowledge distillation loss functions such as LRD(KL), LRD(L2) [27,28,30,29,25]. The results are
shown in Table 2. In all cases, except one (15-minute prediction on the PeMSD7 dataset), our approach
has shown improvement compared to other approaches. For PeMSD7, LORD(ours) demonstrates competi-
tive performance compared to LRD(L2) and LRD(KL) across all evaluated metrics. LORD(ours) consistently
outperforming both LRD(L2) and LRD(KL) in MAE and RMSE for 15 min and 30 min predictions. This
highlights the effectiveness of the overseen knowledge distillation method in capturing underlying patterns
and improving accuracy. In the case of PeMSD8, LORD(ours) exhibits clear advantages over LRD(L2) and
LRD(KL) in various metrics. Specifically, for MAE and RMSE at 15 min and 30 min prediction intervals, it
consistently achieves lower errors, indicating superior predictive capabilities.
Table 2: Comparison of our approach LORD with LRD(L2) and LRD(KL) on PeMSD7 and PeMSD8
PeMSD7
Models
MAPE
MAE
RMSE
15 min
30 min
45 min
15 min
30 min
45 min
15 min
30 min
45 min
Teacher
5.223
7.316
8.739
2.230
3.010
3.565
4.097
5.752
6.834
Student W/O KD
6.423
9.685
12.298
2.666
3.868
4.799
4.649
6.938
8.610
Student LRD(l2)
6.379
9.661
12.474
2.768
4.214
5.527
4.709
7.185
9.178
Student LRD(KL)
6.411
9.527
11.894
2.700
3.938
4.918
4.672
6.984
8.657
Student LORD(ours)
6.411
9.516
12.104
2.762
4.091
5.221
4.645
6.847
8.569
PeMSD8
Models
MAPE
MAE
RMSE
15 min
30 min
45 min
15 min
30 min
45 min
15 min
30 min
45 min
Teacher
2.293
3.239
3.925
1.211
1.665
2.031
2.524
3.501
4.081
Student W/O KD
2.967
4.035
4.734
1.472
1.956
2.294
2.988
4.090
4.706
Student LRD(l2)
2.812
3.976
4.908
1.426
1.953
2.387
2.839
3.978
4.733
Student LRD(KL)
2.964
4.179
5.057
1.507
2.125
2.575
2.895
3.971
4.664
Student LORD(ours)
2.661
3.717
4.553
1.363
1.885
2.285
2.788
3.862
4.573
We conducted a comparative analysis of our final loss function, LSTCD, against other approaches [27,28,30,29,25],
and the results are presented in Table 4 and Figure 3. By reviewing the table, for PeMSD7, LSTCD(ours)
demonstrates significant improvements over the baseline Student W/O KD and outperforms other knowl-
edge distillation methods, including LRD(L2), LRD(KL), and SKD. Specifically, in terms of MAE and RMSE
for the 15-minute and 30-minute predictions, LSTCD(ours) consistently achieves lower errors. Similarly, on
the PeMSD8 dataset, LSTCD(ours) exhibits superior performance across various metrics when compared
to LRD(L2), LRD(KL), and SKD. Noteworthy is the consistent outperformance in MAE and RMSE at the
15-minute and 30-minute prediction intervals, emphasizing the effectiveness of LSTCD(ours) in capturing
spatial-temporal dependencies and enhancing predictive accuracy. These findings highlight the efficacy of
the LSTCD(ours) approach, underscoring its potential as a robust spatial-temporal loss function for knowl-
edge distillation in the context of traffic flow prediction datasets. Figure 3 illustrates better convergence and
pattern imitation of the LSTCD approach compared to other approaches in both knowledge bases. Addition-
ally, we report the ratio of attention to teacher output instead of the dataset target in Table 3.
Table 3: Ratios of attention to teacher prediction to all training data
Models
Ratios of Attention to Teacher Prediction
PeMSD7
PeMSD8
Student LORD
3.009%
0.749%
Student LSCD
1.377%
0.046%
Student LTCD
64.637%
0.503%
Student LSTCD
15.844%
0.103%
11
(a) PeMSD7
(b) PeMSD8
Fig. 3: Comparison of our spatial-temporal correlation distillation loss function LSTCD with other loss func-
tions
Ablation Study Finally, we conducted an ablation study on the LSTCD loss function, and the results are
presented in Table 5. In PeMSD7, comparing various knowledge distillation models, LSTCD(ours) exhibits
competitive performance against other variants, including LORD, LTCD, and LSCD. For instance, in terms
of Mean Absolute Percentage Error (MAPE), Mean Absolute Error (MAE), and Root Mean Squared Error
(RMSE) at different predictions (15 min, 30 min, and 45 min), LSTCD(ours) consistently demonstrates
effectiveness in achieving lower errors compared to the baseline Student W/O KD and other knowledge
distillation methods. Moving to the PeMSD8 dataset, the results in Table 5 further emphasize the robust
performance of LSTCD(ours). When compared to LORD, LTCD, and LSCD, LSTCD(ours) consistently achieves
lower MAPE, MAE, and RMSE across various predictions. Specifically, at 15 min and 30 min intervals,
LSTCD(ours) outperforms its counterparts, showcasing its efficacy in capturing spatial-temporal dependencies
and enhancing predictive accuracy.
12
Table 4: Performance comparison of our spatial-temporal correlation distillation loss function LSTCD with
other loss functions
PeMSD7
Models
MAPE
MAE
RMSE
15 min
30 min
45 min
15 min
30 min
45 min
15 min
30 min
45 min
Teacher
5.223
7.316
8.739
2.230
3.010
3.565
4.097
5.752
6.834
Student W/O KD
6.423
9.685
12.298
2.666
3.868
4.799
4.649
6.938
8.610
Student LRD(l2)
6.379
9.661
12.474
2.768
4.214
5.527
4.709
7.185
9.178
Student LRD(KL)
6.411
9.527
11.894
2.700
3.938
4.918
4.672
6.984
8.657
Student LSKD
6.282
9.503
11.902
2.762
4.092
5.101
4.735
7.161
8.923
Student LSTCD(ours)
6.078
9.043
11.488
2.615
3.776
4.754
4.537
6.678
8.344
PeMSD8
Models
MAPE
MAE
RMSE
15 min
30 min
45 min
15 min
30 min
45 min
15 min
30 min
45 min
Teacher
2.293
3.239
3.925
1.211
1.665
2.031
2.524
3.501
4.081
Student W/O KD
2.967
4.035
4.734
1.472
1.956
2.294
2.988
4.090
4.706
Student LRD(l2)
2.812
3.976
4.908
1.426
1.953
2.387
2.839
3.978
4.733
Student LRD(KL)
2.964
4.179
5.057
1.507
2.125
2.575
2.895
3.971
4.664
Student LSKD
2.726
3.819
4.553
1.409
1.944
2.317
2.909
4.139
4.894
Student LSTCD(ours)
2.491
3.375
4.067
1.281
1.719
2.052
2.716
3.707
4.385
Table 5: Results of LSCD, LTCD, LSTCD, and LORD loss functions ( ablation study )
PeMSD7
Models
MAPE
MAE
RMSE
15 min
30 min
45 min
15 min
30 min
45 min
15 min
30 min
45 min
Teacher
5.223
7.316
8.739
2.230
3.010
3.565
4.097
5.752
6.834
Student W/O KD
6.423
9.685
12.298
2.666
3.868
4.799
4.649
6.938
8.610
Student LORD
6.411
9.516
12.104
2.762
4.091
5.221
4.645
6.847
8.569
Student LTCD
6.320
9.411
11.667
2.730
3.966
4.853
4.678
6.899
8.512
Student LSCD
6.326
9.193
11.380
2.743
3.957
4.866
4.645
6.853
8.476
Student LSTCD
6.078
9.043
11.488
2.615
3.776
4.754
4.537
6.678
8.344
PeMSD8
Models
MAPE
MAE
RMSE
15 min
30 min
45 min
15 min
30 min
45 min
15 min
30 min
45 min
Teacher
2.293
3.239
3.925
1.211
1.665
2.031
2.524
3.501
4.081
Student W/O KD
2.967
4.035
4.734
1.472
1.956
2.294
2.988
4.090
4.706
Student LORD
2.661
3.717
4.553
1.363
1.885
2.285
2.788
3.862
4.573
Student LTCD
2.509
3.497
4.215
1.296
1.747
2.063
2.665
3.716
4.406
Student LSCD
2.619
3.680
4.457
1.355
1.850
2.214
2.722
3.778
4.478
Student LSTCD
2.491
3.375
4.067
1.281
1.719
2.052
2.716
3.707
4.385
4.4.1
Hyperparameters You can refer to Tables 6 for the learning rate, batch size, and hyperparame-
ters related to knowledge distillation. These parameters are crucial in shaping the behavior of the models
during training. In the context of the provided equations, the ”Models” column enumerates various knowl-
edge distillation approaches evaluated for PeMSD7 and PeMSD8 datasets. Each approach, such as LRD(l2),
LRD(KL), LSKD, LORD(ours), and LSTCD(ours), incorporates distinct strategies for transferring knowledge.
The ”Batch Size” column indicates the number of training samples utilized in each iteration for a specific
model and dataset. For instance, SKD employs a batch size of 16 for PeMSD7 and 60 for PeMSD8. LRD(l2),
LRD(KL), LORD(ours), and LSTCD(ours) maintain a batch size of 50 for both datasets, influencing the gran-
ularity of parameter updates during training. The ”Learning Rate” column specifies the step size during
optimization, impacting the convergence of the models. Across various models and datasets, a learning rate
of 1E-03 is consistently chosen, except for SKD on PeMSD8, which employs a learning rate of 1E-02. This
parameter governs the size of the steps taken to reach a minimum in the loss landscape. The hyperparameters
α1, α2, α3, α, and β are pivotal in the knowledge distillation process, as defined in equations (6), (11), (2), and
13
(3). These values determine the contributions of different loss terms and the overall weighting of knowledge
distillation components. For instance, α1 in LORD(ours) plays a role in deciding whether to prioritize teacher
predictions or target labels based on a threshold.
Table 6: Hyperparameters of the proposed knowledge distillation functions
PeMSD7
Models
Batch Size
Learning Rate
α1
α2
α3
α
β
LRD(l2)
50
1E-03
-
-
-
-
0.045
LRD(KL)
50
1E-03
-
-
-
-
0.007
LSKD
16
1E-03
-
-
-
0.333
-
LORD(ours)
50
1E-03
0.593
-
-
-
-
LSTCD(ours)
50
1E-03
0.170
0.047
0.313
-
-
PeMSD8
Models
Batch Size
Learning Rate
α1
α2
α3
α
β
LRD(l2)
50
1E-03
-
-
-
-
0.905
LRD(KL)
50
1E-03
-
-
-
-
0.728
LSKD
60
1E-02
-
-
-
0.005
-
LORD(ours)
50
1E-03
0.541
-
-
-
-
LSTCD(ours)
50
1E-03
0.846
0.465
0.504
-
-
4.5
Pruning and Fine-Tuning Analysis
In this section, the focus is on modifying the student network through a pruning algorithm, intentionally
applying the same algorithm to the teacher network. The objective is to create a student network with
parameters comparable to the one used in knowledge distillation experiments. The goal is to showcase
that a consciously pruned student network, guided by Algorithm 1, exhibits superior learning capabilities
compared to a predetermined student network used in knowledge distillation. The experiment’s outcomes
are summarized in Table 7 and Figures 4 and 5.
Table 7: The results showcase the impact of applying the pruning algorithm in enhancing knowledge distil-
lation.
PeMSD7
Models
Parameter %
MAPE
MAE
RMSE
15 min
30 min
45 min
15 min
30 min
45 min
15 min
30 min
45 min
Teacher
100%
5.223
7.316
8.739
2.230
3.010
3.565
4.097
5.752
6.834
Student W/O pruning algorithm
3%
6.078
9.043
11.488
2.615
3.776
4.754
4.537
6.678
8.344
Student W pruning algorithm
3%
6.148
9.026
11.506
2.563
3.694
4.754
4.398
6.485
8.228
PeMSD8
Models
Parameter %
MAPE
MAE
RMSE
15 min
30 min
45 min
15 min
30 min
45 min
15 min
30 min
45 min
Teacher
100%
2.293
3.239
3.925
1.211
1.665
2.031
2.524
3.501
4.081
Student W/O pruning algorithm
3%
2.491
3.375
4.067
1.281
1.719
2.052
2.716
3.707
4.385
Student W pruning algorithm
3%
2.379
3.307
3.961
1.248
1.679
1.990
2.597
3.594
4.193
14
(a) PeMSD7
(b) PeMSD8
Fig. 4: The left chart in the top row corresponds to the teacher, the right chart in the top row corresponds
to a student without knowledge distillation. The left chart in the second row represents a student with
knowledge distillation without pruning, and the right chart in the second row represents a student with
knowledge distillation and pruning.
Table 7 presents results comparing the performance of the student network with and without pruning through
knowledge distillation. Looking at the PeMSD7 dataset in the table, the teacher model, using all of its pa-
rameters, achieved an RMSE of 4.097 for a 15-minute prediction. On the other hand, the student model
without the pruning algorithm, which retained about 3% of the parameters, had a higher RMSE of 4.537
for the same prediction period. However, when we applied the pruning algorithm to the student model (also
retaining 3% of parameters), the RMSE improved to 4.398. This improvement highlights how pruning posi-
tively influences knowledge distillation by refining the student model’s predictive performance. A significant
improvement is observed in the PeMSD8 dataset. The teacher model achieved a 15-minute MAPE of 2.293
15
with 100% of its parameters. In contrast, the student model without pruning, retaining approximately 3% of
the parameters, had a 15-minute MAPE of 2.491. The application of the pruning algorithm to the student
model (also retaining 3% of parameters) resulted in a decreased 15-minute MAPE of 2.379. These results
vividly demonstrate that the pruning algorithm significantly contributes to enhancing knowledge distilla-
tion, showcasing instances where pruning leads to better model performance compared to the non-pruned
counterparts. The improvements in MAPE values indicate that the pruning algorithm helps the student
models better capture the knowledge from the teacher model. Figure 4 visually represents the performance
enhancement achieved by the pruning algorithm, showing the average predicted values for a random node
on 50 training data points.
(a) PeMSD7
(b) PeMSD8
Fig. 5: The results highlight the impact of employing the pruning algorithm, demonstrating the average
predicted value for a randomly selected node based on 50 data points.
Next, we conduct a comparative analysis between the presented pruning algorithm and a non-distillation sce-
nario from [6]. Previous research has confirmed that incorporating knowledge distillation enhances accuracy
while utilizing a more straightforward model. Building upon this, our current study extends the concept,
16
illustrating that Algorithm 1 yields a student architecture that performs better during training with KD.
We compare Algorithm 1 with [6] using a base pruning model from Table 1. The results depicted in Table
8 underscore that our pruning algorithm consistently outperforms the alternative across various pruning
percentages.
Table 8: Comparison of the performance of Algorithm 1 with the algorithm presented in [6]
PeMSD7
Models
Parameter %
MAPE
MAE
RMSE
15 min
30 min
45 min
15 min
30 min
45 min
15 min
30 min
45 min
Base Model
100%
5.512
8.004
9.988
2.321
3.216
3.896
4.177
6.006
7.341
Pruned W/O KD
25%
6.185
8.981
11.060
2.721
3.889
4.728
4.682
6.947
8.579
Pruned W LSTCD(ours)
25%
5.950
8.654
10.906
2.544
3.705
4.751
4.406
6.424
8.014
Pruned W/O KD
50%
6.571
9.735
12.114
2.975
4.389
5.418
4.858
7.255
8.971
Pruned W LSTCD(ours)
50%
6.232
9.185
11.419
2.584
3.773
4.723
4.524
6.773
8.448
Pruned W/O KD
75%
7.090
11.746
15.940
2.925
4.635
6.034
4.943
7.759
9.967
Pruned W LSTCD(ours)
75%
6.275
9.436
12.261
2.644
3.783
4.708
4.586
6.680
8.270
PeMSD8
Models
Parameter %
MAPE
MAE
RMSE
15 min
30 min
45 min
15 min
30 min
45 min
15 min
30 min
45 min
Base Model
100%
2.206
3.028
3.677
1.187
1.592
1.924
2.479
3.434
4.084
Pruned W/O KD
25%
2.438
3.459
4.338
1.279
1.781
2.227
2.615
3.644
4.359
Pruned W LSTCD(ours)
25%
2.427
3.200
3.782
1.236
1.630
1.927
2.672
3.645
4.303
Pruned W/O KD
50%
2.456
3.514
4.449
1.322
1.890
2.397
2.648
3.797
4.685
Pruned W LSTCD(ours)
50%
2.424
3.310
3.988
1.261
1.693
2.029
2.605
3.550
4.169
Pruned W/O KD
75%
2.530
3.503
4.275
1.335
1.823
2.225
2.715
3.810
4.596
Pruned W LSTCD(ours)
75%
2.348
3.192
3.788
1.228
1.627
1.911
2.595
3.577
4.216
In the PeMSD7 dataset, our proposed pruned W LSTCD (pruned with LSTCD) consistently exhibits superior
performance compared to the pruned W/O KD (without knowledge distillation) approach. This superiority
is evident in metrics such as mean absolute percentage error (MAPE), mean absolute error (MAE), and root
mean square error (RMSE) across different time intervals (15 min, 30 min, and 45 min). For example, at
a pruning level of 25%, pruned W LSTCD achieves lower MAPE values (5.950, 8.654, 10.906) compared to
pruned W/O KD (6.185, 8.981, 11.060) for 15 min, 30 min, and 45 min intervals, respectively. Similar trends
are observed at higher pruning levels of 50% and 75%, where pruned W LSTCD consistently outperforms
pruned W/O KD across all evaluation metrics. The superiority of our algorithm is further emphasized in the
PeMSD8 dataset. At various pruning levels, pruned W LSTCD consistently records lower MAPE, MAE, and
RMSE values compared to pruned W/O KD. Remarkably, even at aggressive pruning levels of 75%, pruned
W LSTCD maintains competitive performance, showcasing the effectiveness and robustness of our proposed
pruning algorithm. Overall, the findings affirm that our introduced pruning algorithm plays a pivotal role
in enhancing learning through knowledge distillation, significantly improving neural network accuracy by
employing fewer parameters and simpler structures. In the executions related to comparing our pruning
algorithm with the one presented in [6], pruning operations are performed after every two cycles, and the
final network cycle undergoes fine-tuning (see Table 8).
4.5.1
Hyperparameters The hyperparameters for the proposed pruning algorithm are presented in Table
9. The table categorizes hyperparameters based on the percentage of pruning applied, as well as batch size,
learning rate, and three alpha values (α1, α2, and α3), specific to the PeMSD7 and PeMSD8 datasets. For
instance, in the PeMSD7 row with a pruning percentage of 97%, only 3% of the model parameters are
retained. The associated hyperparameters include a batch size of 25, a learning rate of 1 × 10−3, and alpha
values α1 = 0.099, α2 = 0.091, and α3 = 0.531. The alpha values play distinct roles in the loss function
17
equations. For α1, it defines the conditions under which different loss terms are applied in LORD as specified
in Equation 6. Meanwhile, α2 and α3 are used as weights in Equation 11, influencing the contributions of
LSCD and LTCD relative to LORD.
Table 9: Hyperparameters of the proposed pruning algorithm
PeMSD7
Pruned %
Batch Size
Learning Rate
α1
α2
α3
97%
25
1E-03
0.099
0.091
0.531
75%
50
1E-03
0.963
0.716
0.081
50%
50
1E-03
0.935
0.981
0.129
25%
50
1E-03
0.971
0.234
0.684
PeMSD8
Pruned %
Batch Size
Learning Rate
α1
α2
α3
97%
25
1E-03
0.746
0.445
0.020
75%
50
1E-03
0.996
0.720
0.405
50%
50
1E-03
0.946
0.516
0.094
25%
50
1E-03
0.748
0.324
0.868
5
Conclusion and Future Works
5.1
Conclusion
We addressed the critical challenge of predicting traffic conditions to reduce transportation time. Our chosen
methodology involved leveraging the spatio-temporal graph convolutional network (ST-GCN) for processing
traffic prediction data, and we proposed a solution to improve the execution time of ST-GCNs. This involved
introducing a novel approach that employed knowledge distillation to train a smaller network (referred to
as the student) using distilled data from a more complex network (referred to as the teacher), all while
maintaining a high level of prediction accuracy. Additionally, we tackled the challenge of determining the
architecture of the student network within the knowledge distillation process, employing Algorithm 1. Our
findings demonstrated that a student network derived through Algorithm 1 exhibited enhanced performance
during the knowledge distillation training process. Furthermore, by utilizing a simpler model in knowledge
distillation, as opposed to the teacher model, as the base model in pruning, we illustrated that the perfor-
mance of our algorithm was not necessarily contingent on the high accuracy of the base model. We evaluated
our proposed concepts using two real-world datasets, PeMSD7 and PeMSD8, demonstrating the effectiveness
of our approach in predicting traffic conditions. Overall, our research contributed to the field by presenting
a methodology to optimize the execution time of spatio-temporal graph convolutional networks for traffic
prediction without compromising accuracy.
5.2
Future Works
As we progress, we have the opportunity to explore our cost function LSTCD and Algorithm 1 on alternative
spatiotemporal graph neural networks, such as ASTGCN. Furthermore, considering the method proposed in
[32] and [26], we can explore whether using multiple teachers in Algorithm 1 and knowledge distillation with
LSTCD can lead to a model with higher accuracy. Alternatively, we can develop a model capable of executing
the tasks performed by all its teachers with increased accuracy and reduced execution time compared to
the model in [26]. It is possible to enhance the concept of knowledge distillation based on the cost function
LSTCD by incorporating relationship-based approaches. This improvement allows for the consideration of
student and teacher networks with varying numbers of layers.
18
References
1. B Yu; H Yin; Z Zhu, Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic
Forecasting, 27th International Joint Conference on Artificial Intelligence, 2018.
2. D. A. Tedjopurnomo; Z. Bao; B. Zheng; F. M. Choudhury; A. K. Qin, A Survey on Modern Deep Neural Network
for Traffic Prediction: Trends, Methods and Challenges, IEEE Transactions on Knowledge and Data Engineering,
2022.
3. S Guo; Y Lin; N Feng; C Song; H Wan, Attention Based Spatial-Temporal Graph Convolutional Networks for
Traffic Flow Forecasting, AAAI conference on artificial intelligence, 2019.
4. G Hinton; O Vinyals; J Dean, Distilling the Knowledge in a Neural Network, NIPS 2014 Deep Learning Workshop,
2015.
5. Y LeCun; J Denker; S Solla, Optimal Brain Damage, Advances in Neural Information Processing Systems 2 (NIPS
1989), 1989.
6. P Molchanov; A Mallya; S Tyree; I Frosio; J Kautz, Importance estimation for neural network pruning, IEEE/CVF
conference on computer vision and pattern recognition. 2019, 2019.
7. Shengjie Min; Zhan Gao; Jing Peng; Liang Wang; Ke Qin; Bo Fang, STGSN — A Spatial–Temporal Graph Neural
Network framework for time-evolving social networks, Knowledge-Based Systems, 2021.
8. Jianping Gou; Baosheng Yu; Stephen John Maybank; Dacheng Tao, Knowledge Distillation: A Survey, Interna-
tional Journal of Computer Vision (2021), 2021.
9. Yu Cheng; Duo Wang; Pan Zhou; Tao Zhang, A Survey of Model Compression and Acceleration for Deep Neural
Networks, IEEE Signal Processing Magazine, 2020.
10. J. Yim; D. Joo; J. Bae and J. Kim, A Gift from Knowledge Distillation: Fast Optimization, Network Minimization
and Transfer Learning, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
11. G Chen; W Choi; X Yu; T Han, Learning efficient object detection models with knowledge distillation, Neural
Information Processing Systems 30 (NIPS 2017), 2017.
12. Feng Zhang; Xiatian Zhu; Mao Ye, Fast Human Pose Estimation, Computer Vision and Pattern Recognition
(CVPR2019), 2019 .
13. Zhong Meng; Jinyu Li; Yong Zhao; Yifan Gong, Conditional Teacher-Student Learning, , 2019 .
14. Yoshua Bengio; Aaron Courville; Pascal Vincent, Representation learning: A review and new perspectives, , 2013.
15. Peyman Passban; Yimeng Wu; Mehdi Rezagholizadeh; Qun Liu, ALP-KD: Attention-Based Layer Projection for
Knowledge Distillation , AAAI 2021, 2021.
16. X Wang; T Fu; S Liao; S Wang; Z Lei; T Mei, Exclusivity-consistency regularized knowledge distillation for face
recognition , Computer Vision–ECCV 2020, 2020 .
17. Seung Hyun Lee; Dae Ha Kim; Byung Cheol Song, Self-supervised Knowledge Distillation Using Singular Value
Decomposition, ECCV 2018, 2018 .
18. Umar Asif; Jianbin Tang; Stefan Harrer, Ensemble Knowledge Distillation for Learning Improved and Efficient
Networks, Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG), 2020.
19. Seyed-Iman Mirzadeh; Mehrdad Farajtabar; Ang Li; Nir Levine; Akihiro Matsukawa; Hassan Ghasemzadeh,
Improved knowledge distillation via teacher assistant, AAAI 2020, 2020.
20. Hossein Mobahi; Mehrdad Farajtabar; Peter L. Bartlett, Self-Distillation Amplifies Regularization in Hilbert
Space , , 2020.
21. Wen W; Wu C; Wang Y; Chen Y; Li H, Learning structured sparsity in deep neural networks, Neural Information
Processing Systems (NIPS), IEEE, 2016.
22. N. Ramakrishnan; T. Soni, Network Traffic Prediction Using Recurrent Neural Networks, 2018 17th IEEE Inter-
national Conference on Machine Learning and Applications (ICMLA), 2018.
23. Tang, Jinjun; Abduljabbar, Rusul L; Dia, Hussein; Tsai, Pei-Wei, Unidirectional and Bidirectional LSTM Models
for Short-Term Traffic Prediction, Journal of Advanced Transportation, 2021.
24. Yaguang Li; Rose Yu; Cyrus Shahabi; Yan Liu, Diffusion Convolutional Recurrent Neural Network: Data-Driven
Traffic Forecasting, ICLR 2018, 2018.
25. Y. Yang; J. Qiu; M. Song; D. Tao; X. Wang, Distilling Knowledge from Graph Convolutional Networks, 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.
26. Y Jing; Y Yang; X Wang; M Song, Amalgamating Knowledge from Heterogeneous Graph Neural Networks, 2021
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
27. C. Bian; W. Feng; L. Wan; S. Wang, Structural knowledge distillation for efficient skeleton-based action recogni-
tion, IEEE Transactions on Image Processing, 2021.
28. Z Huang; N Wang, Like What You Like: Knowledge Distill via Neuron Selectivity Transfer, Computer Vision and
Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE), 2017.
29. Adriana Romero; Nicolas Ballas; Samira Ebrahimi Kahou; Antoine Chassang; Carlo Gatta; Yoshua Bengio,
FitNets: Hints for Thin Deep Nets, Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE),
2014.
19
30. Sergey Zagoruyko; Nikos Komodakis, Paying More Attention to Attention: Improving the Performance of Con-
volutional Neural Networks via Attention Transfer, Computer Vision and Pattern Recognition (cs.CV), 2017.
31. Cheng Yang; Jiawei Liu; Chuan Shi, Extract the Knowledge of Graph Neural Networks and Go Beyond it: An
Effective Knowledge Distillation Framework, Machine Learning (cs.LG); Social and Information Networks (cs.SI),
2021.
32. Sihui Luo; Xinchao Wang; Gongfan Fang; Yao Hu; Dapeng Tao; Mingli Song, Knowledge Amalgamation from
Heterogeneous Networks by Common Feature Learning, the 28th International Joint Conference on Artificial In-
telligence (IJCAI 2019), 2019.
20
"
"This research tackles the issue of devising a safe and generalized end-to-end autonomous driving system. The core of this system revolves around the reinforcement learning and demonstration paradigms, addressing critical concerns like insufficient safety, poor generalization, and inefficient sampling. To overcome these challenges, they propose SGADS (Safe and Generalized end-to-end Autonomous Driving System), which incorporates variational inference with normalizing flows, ensuring accurate prediction of future driving trajectories. Additionally, they formulate robust safety constraints, enhancing the system's safety performance. To further augment the search process of the agent, they ingeniously combine reinforcement learning with demonstrations. The experimental evaluation showcases the remarkable capabilities of SGADS in achieving enhanced safety, robust generalization across diverse testing maps, and improved training efficiency compared to existing methods. Their findings underscore the significant impact of input data types on the safety performance of autonomous vehicles, opening up new avenues for exploration in this domain.","Autonomous driving systems are expected to tackle intricate road geometries, multi-agent interactions with dynamic objects, and accurate adherence to planned routes and obstacle avoidance strategies. Conventional autonomous driving systems often rely on highly modularized, hand-engineered approaches, leading to limitations in accuracy, generality, and efficiency. To address these issues, researchers have turned to imitation learning (IL) and reinforcement learning (RL) techniques, which offer enhanced learning capabilities. However, IL methods suffer from high training costs, limited driving skills transfer from human experts, and performance limitations imposed by human expertise. RL methods, on the other hand, enable autonomous exploration and learning without relying on human expert data, potentially surpassing IL performance. However, RL methods often face challenges in safety, sampling efficiency, and generalization. To address these challenges, researchers have proposed combining RL with demonstrations, leveraging human expertise to augment the learning process. However, existing methods still exhibit limitations in terms of handling complex driving scenarios, high learning costs, and insufficient safety and generalization capabilities. In this paper, the authors introduce a novel SGADS framework to tackle these challenges effectively.","nanThe authors provide a comprehensive overview of existing IL and RL methods for autonomous driving, highlighting their strengths and limitations. They delve into the key challenges faced by these methods, including low safety, poor generalization, and inefficient sampling, emphasizing the need for a more robust and efficient approach. The authors also discuss the integration of RL with demonstrations, acknowledging its potential to enhance learning and safety. They highlight notable contributions in this area, such as the works of Theodorou et al., Van Hoof et al., and Rajeswaran et al., which demonstrate the efficacy of combining demonstrations and RL for improving learning efficiency. Additionally, they reference the work of Huang et al., 2024, which explores an enhanced human-in-the-loop RL method, albeit with limitations in handling complex scenarios.nannan","The proposed SGADS framework incorporates several key components to address the challenges of safety, sampling efficiency, generalization, and accurate prediction of future driving trajectories. Firstly, it utilizes variational inference with normalizing flows to enable accurate prediction of future driving trajectories, a crucial aspect for optimal decision-making. Secondly, it introduces robust safety constraints to enhance the safety performance of the autonomous vehicle. Thirdly, it employs behavior cloning to augment the search process of the agent, leveraging human expert demonstrations to improve learning efficiency and safety. The model learning process is formulated as a partially observable Markov decision process (POMDP), optimizing the evidence lower bound (ELBO) by maximizing the likelihood of observations and minimizing the KL-divergence between prior and posterior distributions. The policy learning component aims to minimize a loss function that balances the cloning policy and deep reinforcement learning policy. The safety constraint is formulated to minimize front and lateral time to collision and ensure smooth steering. To evaluate the effectiveness of SGADS, the authors conduct extensive experiments in CARLA, a widely used simulation platform for autonomous driving research. They compare SGADS with several baseline methods, including DDPG, SAC, TD3, DQN, and Latent SAC, across various performance metrics such as average driving distance, maximum driving distance, and training time.nan","The experimental results demonstrate the superior performance of SGADS compared to existing methods. SGADS achieves significantly higher average and maximum driving distances in both random and fixed complex scenarios, indicating its enhanced safety capabilities. The ablation study reveals that the safety benefits of SGADS primarily arise from the safety constraints and behavior cloning components, with safety further enhanced when combined. SGADS also exhibits strong generalization capabilities, outperforming baseline methods in multiple testing maps with diverse road networks and scenarios. Furthermore, the authors investigate the impact of different input data types on the safety performance of autonomous vehicles. They evaluate 12 different input types, including lidar, camera, semantic segmentation, and combinations thereof. The results show that lidar noground input yields the highest safety performance, but the differences among input types are relatively small, indicating minimal impact on safety. This finding suggests that the choice of input data type is less critical for ensuring the safety of autonomous vehicles.","In conclusion, the authors successfully address the limitations of existing methods in solving safety, sampling efficiency, generalization problems in autonomous driving systems. The proposed SGADS framework incorporates variational inference with normalizing flows, enabling accurate prediction of future driving trajectories. Furthermore, SGADS enhances safety through robust safety constraints and leverages behavior cloning to augment the search process of the agent. The experimental results demonstrate the significant improvements achieved by SGADS in terms of safety, generalization, and training efficiency. Additionally, the authors provide insights into the impact of input data types on the safety performance of autonomous vehicles. Their work contributes to the development of more robust and efficient autonomous driving systems, paving the way for safer and more reliable autonomous vehicles.",Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations,"Zuojin Tang, Xiaoyu Chen, YongQiang Li, Jianyu Chen","Safe and Generalized end-to-end Autonomous Driving System with
Reinforcement Learning and Demonstrations
Zuojin Tang 1 2 Xiaoyu Chen 3 YongQiang Li 4 Jianyu Chen 1 3
Abstract
An intelligent driving system should be capa-
ble of dynamically formulating appropriate driv-
ing strategies based on the current environment
and vehicle status, while ensuring the security
and reliability of the system.
However, exist-
ing methods based on reinforcement learning
and imitation learning suffer from low safety,
poor generalization, and inefficient sampling.
Additionally, they cannot accurately predict fu-
ture driving trajectories, and the accurate pre-
diction of future driving trajectories is a precon-
dition for making optimal decisions. To solve
these problems, in this paper, we introduce a
Safe and Generalized end-to-end Autonomous
Driving System (SGADS) for complex and var-
ious scenarios. Our SGADS incorporates varia-
tional inference with normalizing flows, enabling
the intelligent vehicle to accurately predict future
driving trajectories. Moreover, we propose the
formulation of robust safety constraints. Further-
more, we combine reinforcement learning with
demonstrations to augment search process of the
agent. The experimental results demonstrate that
our SGADS can significantly improve safety per-
formance, exhibit strong generalization, and en-
hance the training efficiency of intelligent vehi-
cles in complex urban scenarios compared to ex-
isting methods.
1. Introduction
An intelligent autonomous driving systems must be able
to handle complex road geometry and topology, complex
multi-agent interactions with dense surrounding dynamic
objects, and accurately follow the planning and obstacle
1Shanghai Qizhi Institute 2College of Computer Science and
Technology, Zhejiang University 3Institute for Interdisciplinary
Information Sciences, Tsinghua University 4Neolix. Correspon-
dence to: Jianyu Chen <jianyuchen@tsinghua.edu.cn>.
Preprint. Copyright 2024 by the author(s)
avoidance. Current, autonomous driving systems in indus-
try are mainly using a highly modularized hand-engineered
approach, for example, perception, localization, behav-
ior prediction, decision making and motion control, etc.
(Thrun et al., 2006) and (Urmson et al., 2008). Particularly,
the autonomous driving decision making systems are focus-
ing on the non-learning model-based methods, which of-
ten requires to manually design a driving policy(Gonz´alez
et al., 2015) and (Paden et al., 2016). However, the man-
ually designed policy could have two several weaknesses:
1) Accuracy. The driving policy of human heuristics and
pre-training model can be suboptimal, which will lead to
either conservative or aggressive driving policies. 2) Gen-
erality. For different scenarios and complicated tasks, we
might need to be redesigned the model policy manually for
each new scenario.
To solve those problems, existing works such as Imita-
tion Learning (IL) is most popular approach, which can
learn a driving policy by collecting the expert driving
data(Bojarski et al., 2016), (Codevilla et al., 2018), (Bansal
et al., 2018) and (Chen et al., 2019a). However, those meth-
ods can suffer from the following shortcomings for imita-
tion learning: (1) High training cost and sample complex-
ity. (2) Conservation of driving policy. Due to the collect
driving data from the human expert, which can only learn
driving skills that are demonstrated in the datasets. (3) Lim-
itation of driving performance. The performance of imita-
tion learning policy is impossible exceed human-level per-
formance.
What’s more, the driving policy based on Reinforcement
Learning (RL) is also popular method in recent years,
which can automatically learn and explore without any hu-
man expert data in various kinds of different driving cases,
and it is possible to have a better performance than imi-
tation learning. (Lillicrap et al., 2015) introduced a con-
tinuous control deep RL algorithm that trains a deep neu-
ral network policy for autonomous driving on a simulated
racing track. (Wolf et al., 2017) used Deep Q-Network to
learn to steer an autonomous vehicle to keep in the track
in simulation. (Chen et al., 2018) developed a hierarchi-
cal deep RL framework to handle driving scenarios with in-
tricate decision-making processes, such as navigating traf-
1
arXiv:2401.11792v1  [cs.RO]  22 Jan 2024
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
Sensor inputs
Reconstruction inputs
Policy
 Latent Encoder Layer
Latent Decoder Layer
Policy updates 
the datasets
Initial datasets
Experience Replay buffers
Control Command
Driving Environment
Safety Constraint
G29 steering wheel
Initial datasets
Expert Replay buffers
Expert collect data offline
Figure 1. Overview of the safe and generalized end-to-end autonomous driving with reinforcement learning and demonstrations
fic lights. (Kendall et al., 2019) marked the first applica-
tion of deep RL in real-world autonomous driving, where
they trained a deep lane-keeping policy using only a sin-
gle front-view camera image as input. (Chen et al., 2021)
proposed an interpretable deep RL method for end-to-end
autonomous driving. (Nehme & Deo, 2023) proposed safe
navigation.(Murdoch et al., 2023) propose a partial end-to-
end algorithm that decouples the planning and control tasks.
(Zhou et al., 2023) proposes a method to identify and pro-
tect unreliable decisions of a DRL driving policy. (Zhang
et al., 2023) a framework of constrained multi-agent rein-
forcement learning with a parallel safety shield for CAVs
in challenging driving scenarios.
(Liu et al., 2022) pro-
pose the Scene-Rep Transformer to enhance RL decision-
making capabilities. (Huang et al., 2023) introduces a pre-
dictive behavior planning framework that learns to predict
and evaluate from human driving data. In addition, com-
bination of RL with Demonstrations are also learned more
complex tasks such as dexterous manipulator. The tech-
niques outlined in (Theodorou et al., 2010), (Van Hoof
et al., 2015) and (Rajeswaran et al., 2017) have proven to
be efficient in merging demonstrations and RL for improv-
ing learning speed. (Huang et al., 2024) propose an en-
hanced human in-the-loop reinforcement learning method,
while they rely on human expert performance and can only
accomplish simple scenario tasks. However, the existing
methods also have some weakness: (1) They can only deal
with the simple driving scenario tasks. (2)High costs of
learning and exploration. (4) The safety and generaliza-
tion of intelligent vehicles need further improvement. (4)
Such these RL frameworks are mostly based on Partially
Observable Markov Decision Process(POMDP)( ˚Astr¨om,
1965),(Hollinger & Sukhatme, 2014),(Agha-Mohammadi
et al., 2014),(Indelman et al., 2015),(Karl et al., 2016).
However, they cannot accurately predict future driving tra-
jectories, and the accurate prediction of future driving tra-
jectories is a precondition for making optimal decisions.
In conclusion, the existing IL and RL methods can not
well solve the safety, sampling efficiency and generaliza-
tion of autonomous vehicles.
To solve above problems,
in this paper, we introduce a Safe and Generalized end-
to-end Autonomous Driving System (SGADS) for com-
plex and various scenarios. Our SGADS incorporates vari-
ational inference with normalizing flows, enabling the in-
telligent vehicle to accurately predict future driving trajec-
tories. Moreover, we design safety constraint for intelli-
gent vehicle.
We further propose a behavior cloning as
to augment search process of agent. We also contribute
a demonstration dataset by having human experts drive
using the G29 steering wheel.
The experimental result
shows our SGADS greatly augments safety performance,
exhibits strong generalization in multiple testing maps, and
enhances the training efficiency of intelligent vehicles in
intricate urban scenarios compared to the existing methods.
Moreover, we designed and explored up to 12 different data
input types to evalutate their impact on the safety perfor-
mance of intelligent vehicles. To sum up, our contributions
are:
• We introduces a novel SGADS designed to address
the limitations of existing methods in solving safety,
sampling efficiency, generalization problems in au-
tonomous driving system.
• SGADS incorporates variational inference with nor-
malizing flows, enabling the intelligent vehicle to ac-
curately predict future driving trajectories.
• The experimental result shows that SGADS is capable
of greatly enhancing safety, generalization, and train-
ing efficiency for intelligent vehicles.
2
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
( )
a
( )
b
( )
c
3ˆa
2ˆa
ts −1
ts +1
ts
ts −1
ts +1
ts
ts −1
ts +1
ts
to +1
to
to −1
to +1
to −1
to
to −1
ˆto +1
ˆto
1
ˆto −
ˆta −1
ˆta
ˆta +1
ˆta +1
ˆtr −1
ˆtr
ˆtr +1
ˆtr −1
ˆtr
ˆtr +1
ˆtv
ˆtv +1
ˆtv −1
ta
ta −1
ta
1
ta −
Figure 2. (a) RL Agent learns potential dynamics from past experience datasets of autonomous vehicle. (b) RL Agent predicts driving
behavior in an imaginary space. (c) RL Agent interacts with driving environment.
• We provide a dataset collected from human expert us-
ing the G29 steering wheel. Moreover, we have de-
signed and explored 12 inputs to evalutate their influ-
ence on the safety performance of intelligent vehicles.
2. Problem formulation
2.1. System Framework
The proposed framework of our SGADS in Figure 1.
Firstly, human expert collects the demonstrations offline
using the Logitech G29 steering wheel. Then, the expert
demonstrations are used as the initial experience replay
buffers for training the entire model. Subsequently, a pre-
training process is conducted to establish a model with hu-
man expert experience that do not update environmental
data during training. The resulting model with human ex-
pert experience is then utilize to initialize network param-
eters for RL policy. Additionally, safety constraint is de-
signed for intelligent vehicle, enhancing their safety per-
formance. In addition, we design and explore 12 different
types of images as the model inputs as shown in Figure 3
and Section 4.7.
2.2. Problem Formulation
Visual control(Thrun, 1999),(Silver & Veness, 2010),
(Bengtsson et al., 2008) can be defined as a POMDP, the
goal of the RL agent is to maximize expected cumulative
reward E[P∞
t=0 γtrt],γ is the discount factor. Inspired by
(Hafner et al., 2019a), (Hafner et al., 2019b), the RL agent
in our SGADS enable the intelligent vehicle to predict fu-
ture driving trajectories, which mainly consists of the fol-
lowing three parts:
(1) RL Agent learns potential dynamics from past experi-
ence datasets of autonomous vehicle to predict future re-
wards from actions and past observations. As shown in Fig-
ure 2(a), the entire learning process is a POMDP decision
chain, using p to represent prior probability, q to represent
posterior probability, agent learns to encode observations o
and actions a into compact latent states s, and ˆot is recon-
structed with probability q(ˆot|st) while st is determined
via probability p(st|st−1, at−1, ot).
(2) RL Agent predicts state values and actions of driv-
ing behaviors in an imaginary space, which maximize
future value predictions by propagating gradients back
through imagined trajectories. As shown in Figure 2(b),
RL Agent is in a close latent state space where it can pre-
dict state ˆvt, reward ˆrt and action ˆat based on current input
ot−1 with probabilities q(ˆvt, ˆrt, ˆat|st), p(st|st−1, ˆat−1),
q(ˆat−1|st−1).
(3) RL Agent encodes the history of the episode to com-
pute the current model state and predict the next action
to execute in the environment. As shown in Figure 2(c),
RL Agent predicts next action values ˆat+1 by encoding his-
torical trajectory information via calculating probabilities
q(ˆat+1|st+1), p(st+1|st, at, ot+1).
3. Methodology
3.1. Model Learning
We introduces a novel SGADS model consists of compo-
nents can be constructed the PGM of POMDP as follow:
State transition model:
pθ(st|st−1, at−1)
Reward model:
pθ(rt|st)
Observation model:
pθ(ot|st)
Belief inference model:
qψ(st|τt, ot)
(1)
Where p is prior distribution, q is posterior distribution,
τt = {o1, a1, · · · , ot−1, at−1, ot, at}. The above of compo-
nents can be optimized by maximizing the Evidence Lower
BOund (ELBO)(Jordan et al., 1998),(De Cao et al., 2020):
Mmodel(θ, ψ, ω) =
T
X
t=1
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
Lidar
Camera
Semantic
Multi-fusion2
Lidar_noground and Multi-fusion3
Lidar_noground and Depth
Lidar_noground and Camera_gray
Camera and Lidar
Birdeye
Multi-fusion1
Lidar_noground
Depth
Figure 3. Multiple types of input images
In the continuous and dynamic space, existing methods
based on Normalizing Flows(NF)(Dinh et al., 2016) ,
(Huang et al., 2018), (Rezende & Mohamed, 2015) can
learn more flexible and generalized beliefs. These methods
provide a solid foundation for RL agents based on POMDP
to accurately predict future driving trajectories. Inspired by
(Chen et al., 2022), we can substitute the probability den-
sity with Normalizing Flow in the KL-divergence term of
Equation 2.
qK(st|τt, ot) = log q0(st|τt, ot) −
K
X
k=1
|det ∂fθk
∂st,k−1 |
pK(st|τt) = log p0(st|τt) −
K
X
k=1
|det ∂fωk
∂st,k−1 |
(3)
Where qK = qψ,θ, pK = qψ,ω. The final inferred belief
is obtained by propagating qψ(st|τt, ot) through a set of
NF mappings denoted fθK . . . fθ1 to get a posterior distri-
bution qψ,θ(st|τt, ot). The final prior is obtained by prop-
agating pψ(st|τt) through a set of NF mappings denoted
fωK . . . fω1 to get a prior distribution pψ,ω(st|τt).
3.2. Policy Learning
If the initialization of the policy is poor, the learning pro-
cess may be very slow and the algorithm will explore states
and actions that are irrelevant to the task. To address this
problem, we use behavior cloning to provide a policy π∗
via expert demonstrations and then to train a model Mexpert
with some expert ability. The loss function for training the
model in the first stage is as follows:
min
η
X
(s′,a′)∈π∗(Dexpert)
− ln π∗
η(a′
t|s′
t)
(4)
In the second stage of the model, we employ Mexpert to
initialize a model trained by deep reinforcement learning
policies, which greatly reduces the sampling complexity of
the deep RL policy intelligent vehicle. The training loss of
the actor model in the second stage is as follows:
Mactor(ϕ, η) = E

N
X
i=1
t+H
X
τ=t

V λ
i,τ
 
+ k ln π∗
η(a′
t|s′
t)
(5)
Where k represents the balance between the cloning policy
and deep reinforcement learning policy, and is set as a con-
stant based on empirical data. Then we can train the critic
to regress the TD(λ) (Sutton & Barto, 2018) target return
via a mean squared error loss:
Mcritic (ξ) = E
h
N
X
i=1
t+H
X
τ=t
1
2

vξ (si,τ) − V λ
i,τ
2 i
(6)
The vξ (sτ) is designed to forecast the cumulative dis-
counted sum of future rewards achievable by the actor, start-
ing from an initial state sτ. V λ
τ can be defined as follow:
V λ
τ = (1 − τ)vξ(sτ+1) + λV λ
τ+1,
τ<t + H
(7)
Where ξ denote the parameters of the critic network and H
is the prediction horizon. Finally, the loss function for the
second stage training of the whole model is as follows:
min
ψ,ξ,ϕ,θ,ω,ηα0Mcritic(ξ) − α1Mactor(ϕ, η) − α2Mmodel(θ, ψ, ω)
(8)
We jointly optimize the parameters of model loss ψ, θ, ω,
critic loss ξ and actor loss ϕ, η, where α0, α1, α2 are coef-
ficients for different components.
3.3. Safety constraint
To keep the intelligent vehicle running safely and smoothly
in complex environments, we add safety and robustness
constraints to the reward function of the algorithm(Chen
et al., 2019b), including time to collision and smooth steer-
ing.
(1)Front time to collision. When around vehicles are within
the distance of ego vehicle (our agent vehicle) head in our
setting, then we can calculate the front time to collision be-
tween ego vehicle and around vehicles. Firstly, the speed
4
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
Ground Truth
5o
  Imaginary trajectories：
Model iuputs
o16
20
o
o15
1o
5o
10
o
o15
o10
1o
o25
30
o
o16
o20
o25
o30
5o
o15
1o
10
o
30
ˆo
o16
o20
o25
30
o
17
ˆo
28
o
13
o
o28
13
o
28
o
13
o
Figure 4. Randomly sample sensor inputs Lidar noground o1, o2, · · · , o15, and then our model can imagine driving behaviors
ˆo16, ˆo17, · · · , ˆo30. The results show that our model can make precise and diverse predictions with no mode mixup and less blur.
and steering vector (sτ, aτ) ∈ S of the ego vehicle are de-
fined, where sτ represents the angle vector of vehicle steer-
ing and aτ represents the acceleration vector of the vehicle
in local coordinate system. Secondly, two waypoints clos-
est to the current ego vehicle are selected from the given
navigation routing as direction vectors ⃗wp for the entire
route progression, where → indicates a vector in world co-
ordinates. The position vectors for both ego vehicle and
around vehicles are represented by (⃗xt
∗, ⃗yt
∗), respectively.
Finally, δe and δa representing angles between position vec-
tors for ego vehicle and around vehicles with respect to ⃗wp
are calculated respectively.
⃗wp =

(
⃗
wx
t+1 − ⃗
wx
t
2
) − ( ⃗
wx
t ), (
⃗
wy
t+1 − ⃗
wy
t
2
) − ( ⃗
wy
t )

δe =
[ ⃗
vx∗
t , ⃗
vy∗
t ] · ⃗wp
∥ ⃗
vx∗
t , ⃗
vy∗
t ∥2
∥ ⃗wp∥2
, δa =
[ ⃗vx
t , ⃗vy
t ] · ⃗wp
∥ ⃗vx
t , ⃗vy
t ∥2
∥ ⃗wp∥2
(9)
Where, l is the length of the set of waypoints W stored. The
variable t ∈ τ, and ⃗wx
t ∈ W1 represents the x coordinate
of the first navigation point closest to the intelligent vehi-
cle on its current route at time t. Similarly,
⃗
wx
t+1 ∈ W2.
Furthermore, it is possible to calculate the Fttc as follows:
Fttc =
∥⃗xt − ⃗x∗
t , ⃗yt − ⃗y∗
t ∥2
∥ ⃗
vx∗
t , ⃗
vy∗
t ∥2sin(δe) − ∥ ⃗vx
t , ⃗vy
t ∥2sin(δa)

(10)
(2)Lateral time to collision. When around vehicles are not
within the distance of ego vehicle head in our setting, we
consider significantly the Lttc. The calculation method for
Lttc and Fttc is the same. However, the collision constraint
effect of Lttc on intelligent vehicle is limited, mainly due to
the slow reaction time of intelligent vehicle to Lttc, lack of
robustness and generalization ability. Therefore, we have
implemented a method of assigning values to different in-
tervals for Lttc as follows:

















min(zτ, cτ + 1.0),
νg ≤ (clttc − 1.5) and µa ≤ (cτ − 0.5).
min(zτ, cτ − 1.8),
νg ≤ (cτ − 3.0) and µa ≤ (cτ − 2.0).
min(zτ, cτ − 3.0),
νg ≤ (cτ − 3.5) and µa ≤ (cτ − 3.0).
(11)
Where cτ is the empirical const of Lttc in our setting at
(5,7), zτ is the ttc based on their combined speed. νg is the
ttc obtained by calculating the longitudinal velocity. µa is
the ttc obtained by calculating the lateral velocity.
(3) Smooth steering is defined as |sδ
t − s∗δ
t | ∈ ec. sδ
t is
the actual steering angle. s∗δ
t is the predicted steering angle
based on policy π. The range of ec can be established based
on empirical data.
4. Experiment
The model training in this paper is deployed on systems
including Python=3.6, Pytorch-gpu=1.5.0, GeForce RTX-
3090 and CARLA=0.9.10 running on Linux. In order to
evaluate the performance of our SGADS in addressing chal-
lenges related to sampling efficiency, safety, generalization
and prediction for future driving trajectories, we respec-
tively designed corresponding experiments to answer each
of these questions.
Additionally, we formulated experi-
ments involving 12 different sets of input data to evaluate
the influence on the safety of autonomous vehicles.
4.1. Training and Evaluation Setting
4.1.1. TRANING ENVIRONMENT
The training and evaluation of our entire autonomous driv-
ing perception-decision-making joint system are conducted
on the CARLA(Dosovitskiy et al., 2017).
In order to
5
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
 Underpass and Overpass with circular roads in Town04
Intersection in Town03
large multilane roads and junctions in Town05
Roundabout in Town03
 Town03
 Town05
 Town02
 Town04
 Town01
 Town06
Figure 5. The road networks of the CARLA include routes for Town01, Town02, Town03, Town04, Town05, and Town06, as well as
more complex scenarios in Town03, Town04, and Town05
Table 1. In the training stage, the steps and times for all methods are compared under different baseline average collision distances in
Town03(max episodes length=500), the number of vehicles is 200, and calculating Average Driving Distance(Avg Dis) with 10 episodes.
Particularly, +∞ means the situation where the training time has exceeded 250hours and the model has still not reached the baseline.
METHOD
INPUT
AVG DIS=50M
AVG DIS=150M
AVG DIS=200M
EPISODES↓
TIMES↓
EPISODES↓
TIMES ↓
EPISODES↓
TIMES ↓
DDPG
LIDAR
+∞
+∞
+∞
+∞
+∞
+∞
SAC
LIDAR
+∞
+∞
+∞
+∞
+∞
+∞
TD3
LIDAR
≥161
≥192H
+∞
+∞
+∞
+∞
DQN
LIDAR
≥163
≥53H
+∞
+∞
+∞
+∞
LATENT SAC
LIDAR
≥167
≥40H
≥300
≥212H
+∞
+∞
SGADS
LIDAR
≥21
≥1.3H
≥121
≥20H
≥321
≥48H
SGADS
LIDAR NG
≥15
≥0.9H
≥139
≥9H
≥211
≥38H
comprehensively evaluate the performance of our SGADS,
we utilized six maps in CARLA ranging from Town01 to
Town06, as illustrated in Figure 5. Town01, a small, sim-
ple town with a river and several bridges. Town02, a small
simple town with a mixture of residential and commercial
buildings. Town04, a small town embedded in the moun-
tains with a special infinite highway. Town05, squared-grid
town with cross junctions and a bridge. Town06, long many
lane highways with many highway entrances and exits. Par-
ticularly, Town03 is the most complex town with a 5-lane
junction, a roundabout, unevenness, a tunnel, and more. To
evaluate more realistically, we adopted the approach of ran-
domly generating driving routes, resetting the coordinates
of all vehicles in the map, and randomizing the driving sce-
narios in each episode.
4.1.2. MEASURE THE SAFETY
Safety is defined as the conditions in which an autonomous
vehicle in its environment can follow a predetermined route
without colliding with surrounding vehicles, avoid major
deviations from a predetermined trajectory, avoid stopping
on the road surface, avoid contact with a roadside wall or
guardrail, and successfully navigate a complex intersection
or roundabout without a collision. In this experimental en-
vironment setting, if the intelligent vehicle fails to comply
with the above regulatory constraints, it will be penalized,
resulting in the termination of the current episode. In ad-
dition, when an episode ends, all scenes and vehicles are
forcibly reset, and the reset of the intelligent car is also
considered as the vehicle experienced a collision, resulting
in the end of the episode. Therefore, we measure the safety
using the Average Driving Distance(Avg Dis), which you
can think of as the average distance a self-driving car can
safely travel in multiple episodes. When intelligent vehicle
travels a longer driving distance within the episode, it indi-
cates that its safety performance is higher. Simultaneously,
as the episode length increases and our testing extends, the
vehicle is more likely to have a longer safe driving distance.
Additionally, we evaluate the ability to safely travel the
maximum distance over multiple episodes using Max Driv-
ing Distance(Max Dis), calculated as the maximum value
of the driving distance across multiple episodes.
6
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
Logitech G29
Tensor(steer)
Tensor(accel,brake)
Autonomous vehicle
Pygame.joystick()
Steering
Accelerator
Brake
CARLA
(a)
(b)
Figure 6. (a) CARLA connects with the G29 steering wheel (b) Human expert collects the datasets via the G29 steering wheel
4.2. Human expert collects teaching datasets
CARLA can be operated and controlled through using the
python API. Figure 6 shows that we establish a connec-
tion between the Logitech G29 steering wheel and the
CARLA, and then human expert can collect the datasets
of teaching via the G29 steering wheel. Specifically, we
linearly map accelerator pedals, brake pedals, and turning
angles into accel[0,3](min,max), brake[-8,0] (min,max),
steer[-1,1](left,right).
The tensors are written into user-
built Python scripts and combined with CARLA built-in
Python API so that users can provide input from their steer-
ing wheels to autonomous driving cars in CARLA simula-
tor for Dexpert collection. Particularly, we contributed a
dataset collected through human expert steering wheel con-
trol.
4.3. Evaluate the prediction of driving trajectories
In order to accurately evaluate our model prediction of
driving behavior for intelligent vehicle in the imagination
space, this problem can be viewed as a special POMDP
problem with the reward value maintained at 0. Then, two
scenarios of original sensor inputs for intelligent vehicle at
time steps 1-15 are randomly sampled as shown in Figure
4, followed by further allowing our model to predict the
driving behaviors of intelligent vehicle at time steps 16-30
under an imagination space length of H = 15. The result
shows that our model can accurately predict future driving
trajectories.
4.4. Evaluate the Sampling efficiency
In order to evaluate the performance of our autonomous
driving system more effectively,
we have conducted
various comparisons with existing methods such as
DDPG(Lillicrap et al., 2015), SAC(Haarnoja et al., 2018),
TD3(Fujimoto et al., 2018), DQN(Mnih et al., 2015) and
Latent SAC(Chen et al., 2021).
These methods achieve
their best performance when using lidar as input:
As
shown in Table 1, compared to other methods, our SGADS
achieves at least a 30 times and 10 times efficiency improve-
ment in training time when reaching baselines of 50 meters
and 150 meters, respectively, with Lidar as input. Particu-
larly, when the baseline is set at 200 meters and the train-
ing time exceeds 250 hours, only our model achieves this
baseline. Additionally, by exploring the input as Lidar ng,
efficiency of the model is further enhanced compared to Li-
dar. This indicates that our SGADS can effectively reduce
training costs and improve sampling efficiency compared
to existing methods.
4.5. Evaluate the Safety
To comprehensively evaluate safety, we chose the most
complex urban map, Town03.
Our SGADS is initially
trained in Town03 and subsequently evaluated in Town03
under random and complex scenarios. In random scenar-
ios, the ego vehicle and around vehicles can appear at any
location on the map. In fixed scenarios, they are restricted
to random appearances within a specific scene range. The
difference between the two lies in the range of random se-
lection, but both follow the randomization mechanisms of
CARLA in every episode during training and evaluation.
As shown in Figure 7, the first row from left to right illus-
trates the three scenarios we evaluated in Town03, includ-
ing random driving scenarios, fixed complex roundabouts,
and fixed intersections. The experimental results show that
SGADS exhibits outstanding Avg Dis and Max Dis in both
random driving scenarios and complex scenarios, signifi-
cantly surpassing existing methods. This further empha-
sizes that our approach has the potential to significantly en-
hance the safety of intelligent vehicles.
Additionally, to clarify the sources of safety benefits, we de-
composed SGADS into three components, variational infer-
ence based on normalizing flows (Base), safety constraints
(SC), and behavior cloning (BC). We then conducted eval-
uations using three comparison settings, Base, Base+SC,
and Base+SC+BC. This is illustrated in the first row of eval-
uations in Figure 7 for the ablation evaluation in Town03.
Under the input conditions of lidar or Lidar noground, the
safety benefits of SGADS primarily result from the design
7
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
Figure 7. All training for SGADS was completed in Town03, followed by separate evaluations from Town01 to Town06. In all evalua-
tions, the calculations for Avg Dis and Max Dis are based on 5 episodes. Due to the smaller size of the map in town02, the number of
vehicles is limited to 50, while in other maps, the number of vehicles is set to 200.
of the SC component, while the BC component can further
provide additional benefits.
4.6. Evaluate the Generalization
To evaluate the generalization performance of SGADS, in
each episode of the experiment, scenes are randomly set,
driving routes are generated randomly, and environmen-
tal vehicles are randomly generated, ensuring the complex-
ity and diversity of the evaluation. As shown in the sec-
ond row of Figure 7, we initially train SGADS and other
methods in Town03 and evaluate respectively in Town01,
Town02, Town04, Town05, and Town06. The experimen-
tal results demonstrate that SGADS exhibits significantly
superior Avg Dis and Max Dis compared to other methods,
and the required training time is relatively short, confirm-
ing its efficient training performance.
Additionally, despite the relatively small size of the
Town02 map, resulting in a smaller upper limit for driving
distance, SGADS still outperforms other methods in terms
of Avg Dis, Max Dis, and training efficiency. This indicates
that SGADS exhibits outstanding generalization capabili-
ties, primarily benefiting from the integration of various as-
pects, including realistic random mechanisms close to the
real world in CARLA, trajectory prediction based on nor-
malizing flows, safety constraints, and the fusion of human
expert demonstrations.
4.7. Multiple types of input images
We compare various input image types for evaluating the
performance of intelligent vehicle based on BC of the
SGADS, as shown in Table 2. The representation of mul-
tiple inputs is described in the section Appendix A.1 Avg
Dis of lidar noground reaches the highest value compared
with all other input types. This is because lidar noground
Table 2. Evaluate the performance of intelligent vehicle under
multiple input images in town03, training steps=100k. Avg Dis
is based on 5 episodes, the number of vehicles is 200.
MULTIPLE INPUT
AVG DIS(M)
BIRDEYE
29.4
LIDAR
38.6
CAMERA
41.5
LIDAR+CAMERA
56.4
SEMANTIC
26.3
DEPTH
29.4
LIDAR NG
64.7
MULTI-FUSION1
52.1
MULTI-FUSION2
47.2
LIDAR+DEPTH
NG
32.5
LIDAR NG+MULTI-FUSION3
36.4
LIDAR NG +CAMERA GRAY
48.8
removes a large amount of redundant information, reduces
the difficulty of world model understanding environment
semantics, and also involves stationary status of intelli-
gent vehicle in experiment. The results show that the li-
dar noground input is relatively optimal. However, it is
worth noting that the effects of these 12 different input
types are relatively small, with the Avg Dis e only varying
between 20 and 40 meters. This shows that different data
types have a minimal impact on the safety performance of
intelligent vehicles.
5. Conclusion
In order to address the challenges of low sample efficiency,
low safety, limited generalization, and accurate prediction
of future driving trajectories in autonomous driving sys-
tems, we propose a safe, efficient, and generalizable end-
to-end autonomous driving system (SGADS). Experimen-
8
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
tal results demonstrate that compared to existing methods,
the safety constraints in SGADS significantly enhance the
safety of intelligent vehicles, and demonstrations further
improve safety and sample efficiency.
Variational infer-
ence based on normalizing flow accurately predicts future
driving trajectories, providing a precondition for optimal
decision making. Finally, the fusion of these three mod-
ules greatly enhances the generalization capabilities of in-
telligent vehicles, significantly surpassing existing meth-
ods. Additionally, we observe that despite differences in
input data types, these variances have minimal impact on
the safety. We contribute a demonstration dataset by having
human experts drive using the G29 steering wheel. How-
ever, there is still space for improvement in aligning our
SGADS system with real-world safety and generalization
requirements.
References
Agha-Mohammadi, A.-A., Chakravorty, S., and Amato,
N. M. Firm: Sampling-based feedback motion-planning
under motion uncertainty and imperfect measurements.
The International Journal of Robotics Research, 33(2):
268–304, 2014.
˚Astr¨om, K. J. Optimal control of markov processes with
incomplete state information. Journal of mathematical
analysis and applications, 10(1):174–205, 1965.
Bansal, M., Krizhevsky, A., and Ogale, A. Chauffeurnet:
Learning to drive by imitating the best and synthesizing
the worst. arXiv preprint arXiv:1812.03079, 2018.
Bengtsson,
T.,
Bickel,
P.,
and Li,
B.
Curse-of-
dimensionality revisited: Collapse of the particle filter
in very large scale systems. In Probability and statistics:
Essays in honor of David A. Freedman, volume 2, pp.
316–335. Institute of Mathematical Statistics, 2008.
Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B.,
Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller,
U., Zhang, J., et al. End to end learning for self-driving
cars. arXiv preprint arXiv:1604.07316, 2016.
Chen, J., Wang, Z., and Tomizuka, M. Deep hierarchical
reinforcement learning for autonomous driving with dis-
tinct behaviors. In 2018 IEEE Intelligent Vehicles Sym-
posium (IV), pp. 1239–1244. IEEE, 2018.
Chen, J., Yuan, B., and Tomizuka, M. Deep imitation learn-
ing for autonomous driving in generic urban scenarios
with enhanced safety. In 2019 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS),
pp. 2884–2890. IEEE, 2019a.
Chen, J., Yuan, B., and Tomizuka, M. Model-free deep re-
inforcement learning for urban autonomous driving. In
2019 IEEE Intelligent Transportation Systems Confer-
ence (ITSC), pp. 2765–2771. IEEE, 2019b.
Chen, J., Li, S. E., and Tomizuka, M. Interpretable end-to-
end urban autonomous driving with latent deep reinforce-
ment learning. IEEE Transactions on Intelligent Trans-
portation Systems, 23(6):5068–5078, 2021.
Chen, X., Mu, Y. M., Luo, P., Li, S., and Chen, J. Flow-
based recurrent belief state learning for pomdps. In Inter-
national Conference on Machine Learning(ICML), pp.
3444–3468. PMLR, 2022.
Codevilla, F., M¨uller, M., L´opez, A., Koltun, V., and Doso-
vitskiy, A. End-to-end driving via conditional imitation
learning.
In 2018 IEEE International Conference on
Robotics and Automation (ICRA), pp. 4693–4700. IEEE,
2018.
De Cao, N., Aziz, W., and Titov, I. Block neural autore-
gressive flow. In Uncertainty in artificial intelligence,
pp. 1263–1273. PMLR, 2020.
Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density esti-
mation using real nvp. arXiv preprint arXiv:1605.08803,
2016.
Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., and
Koltun, V.
Carla: An open urban driving simulator.
In Conference on Robot Learning(CORL), pp. 1–16.
PMLR, 2017.
Fujimoto, S., Hoof, H., and Meger, D. Addressing function
approximation error in actor-critic methods. In Interna-
tional conference on machine learning, pp. 1587–1596.
PMLR, 2018.
Gonz´alez, D., P´erez, J., Milan´es, V., and Nashashibi, F. A
review of motion planning techniques for automated ve-
hicles. IEEE Transactions on Intelligent Transportation
Systems, 17(4):1135–1145, 2015.
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft
actor-critic:
Off-policy maximum entropy deep rein-
forcement learning with a stochastic actor. In Interna-
tional conference on machine learning, pp. 1861–1870.
PMLR, 2018.
Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to
control: Learning behaviors by latent imagination. arXiv
preprint arXiv:1912.01603, 2019a.
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D.,
Lee, H., and Davidson, J. Learning latent dynamics for
planning from pixels. In International conference on ma-
chine learning, pp. 2555–2565. PMLR, 2019b.
9
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
Hollinger, G. A. and Sukhatme, G. S.
Sampling-based
robotic information gathering algorithms. The Interna-
tional Journal of Robotics Research, 33(9):1271–1287,
2014.
Huang, C.-W., Krueger, D., Lacoste, A., and Courville,
A. Neural autoregressive flows. In International Con-
ference on Machine Learning, pp. 2078–2087. PMLR,
2018.
Huang, Z., Liu, H., Wu, J., and Lv, C. Conditional predic-
tive behavior planning with inverse reinforcement learn-
ing for human-like autonomous driving. IEEE Transac-
tions on Intelligent Transportation Systems, 2023.
Huang, Z., Sheng, Z., Ma, C., and Chen, S. Human as
ai mentor: Enhanced human-in-the-loop reinforcement
learning for safe and efficient autonomous driving. arXiv
preprint arXiv:2401.03160, 2024.
Indelman, V., Carlone, L., and Dellaert, F.
Planning in
the continuous domain: A generalized belief space ap-
proach for autonomous navigation in unknown environ-
ments. The International Journal of Robotics Research,
34(7):849–882, 2015.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul,
L. K. An introduction to variational methods for graphi-
cal models. Learning in graphical models, pp. 105–161,
1998.
Karl, M., Soelch, M., Bayer, J., and Van der Smagt, P.
Deep variational bayes filters: Unsupervised learning
of state space models from raw data.
arXiv preprint
arXiv:1605.06432, 2016.
Kendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen,
J.-M., Lam, V.-D., Bewley, A., and Shah, A. Learning
to drive in a day. In 2019 International Conference on
Robotics and Automation (ICRA), pp. 8248–8254. IEEE,
2019.
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T.,
Tassa, Y., Silver, D., and Wierstra, D. Continuous con-
trol with deep reinforcement learning.
arXiv preprint
arXiv:1509.02971, 2015.
Liu, H., Huang, Z., Mo, X., and Lv, C. Augmenting re-
inforcement learning with transformer-based scene rep-
resentation learning for decision-making of autonomous
driving. arXiv preprint arXiv:2208.12263, 2022.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-
land, A. K., Ostrovski, G., et al. Human-level control
through deep reinforcement learning. nature, 518(7540):
529–533, 2015.
Murdoch, A., Schoeman, J. C., and Jordaan, H. W. Partial
end-to-end reinforcement learning for robustness against
modelling error in autonomous racing. arXiv preprint
arXiv:2312.06406, 2023.
Nehme, G. and Deo, T. Y. Safe navigation: Training au-
tonomous vehicles using deep reinforcement learning in
carla. arXiv preprint arXiv:2311.10735, 2023.
Paden, B., ˇC´ap, M., Yong, S. Z., Yershov, D., and Frazzoli,
E. A survey of motion planning and control techniques
for self-driving urban vehicles. IEEE Transactions on
Intelligent Vehicles, 1(1):33–55, 2016.
Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G.,
Schulman, J., Todorov, E., and Levine, S.
Learn-
ing complex dexterous manipulation with deep rein-
forcement learning and demonstrations. arXiv preprint
arXiv:1709.10087, 2017.
Rezende, D. and Mohamed, S. Variational inference with
normalizing flows. In International conference on ma-
chine learning, pp. 1530–1538. PMLR, 2015.
Silver, D. and Veness, J.
Monte-carlo planning in large
pomdps. Advances in neural information processing sys-
tems, 23, 2010.
Sutton, R. S. and Barto, A. G. Reinforcement learning: An
introduction. MIT press, 2018.
Theodorou, E., Buchli, J., and Schaal, S. Reinforcement
learning of motor skills in high dimensions: A path
integral approach.
In 2010 IEEE International Con-
ference on Robotics and Automation(ICRA), pp. 2397–
2403. IEEE, 2010.
Thrun, S. Monte carlo pomdps. Advances in neural infor-
mation processing systems, 12, 1999.
Thrun, S., Montemerlo, M., Dahlkamp, H., Stavens, D.,
Aron, A., Diebel, J., Fong, P., Gale, J., Halpenny, M.,
Hoffmann, G., et al. Stanley: The robot that won the
darpa grand challenge. Journal of field Robotics, 23(9):
661–692, 2006.
Urmson, C., Anhalt, J., Bagnell, D., Baker, C., Bittner, R.,
Clark, M., Dolan, J., Duggins, D., Galatali, T., Geyer, C.,
et al. Autonomous driving in urban environments: Boss
and the urban challenge. Journal of field Robotics, 25(8):
425–466, 2008.
Van Hoof, H., Hermans, T., Neumann, G., and Peters, J.
Learning robot in-hand manipulation with tactile fea-
tures. In 2015 IEEE-RAS 15th International Conference
on Humanoid Robots (Humanoids), pp. 121–127. IEEE,
2015.
10
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
Wolf, P., Hubschneider, C., Weber, M., Bauer, A., H¨artl, J.,
D¨urr, F., and Z¨ollner, J. M. Learning how to drive in
a real world simulation with deep q-networks. In 2017
IEEE Intelligent Vehicles Symposium (IV), pp. 244–250.
IEEE, 2017.
Zhang, Z., Han, S., Wang, J., and Miao, F.
Spatial-
temporal-aware safe multi-agent reinforcement learning
of connected autonomous vehicles in challenging scenar-
ios. In 2023 IEEE International Conference on Robotics
and Automation (ICRA), pp. 5574–5580. IEEE, 2023.
Zhou, W., Cao, Z., Deng, N., Jiang, K., and Yang, D. Iden-
tify, estimate and bound the uncertainty of reinforcement
learning for autonomous driving. IEEE Transactions on
Intelligent Transportation Systems, 2023.
11
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
A. Appendix
A.1. Multiple types of input images
The 12 types of input data we designed are mainly categorized into single-modal and single-image input, single-image and
multimodal fusion, and multiple images and multimodal fusion.
1)Single-modal and input of a single image. As Figure 3 shown, the lidar images, which project the 3D point cloud infor-
mation from lidar onto a 2D point cloud image, with each pixel color determined by whether there is lidar or other relevant
pixel information on the corresponding area. Navigation path is rendered in blue and surrounding road conditions are repre-
sented by green rectangular boxes to indicate participants such as vehicles, pedestrians etc. Particularly, lidar noground is
created to remove redundant ground truth information from the 2D point cloud image. Moreover, we also consider camera,
semantic, birdeye and depth as our sensor inputs.
2)Single-image and multimodal fusion. The input of single-image and multi-modal fusion involve fusing lidar, rgb
forward-facing grayscale image(camera gray), and navigation path into a composite rgb image with three types of
information.
The fused image has three channels, multi-fusion1(lidar,camera gray,routing).
Similarly having multi-
fusion2(lidar,depth,routing) and multi-fusion3(lidar, depth,0).
3)Multiple images and multi-modal fusion. Multiple fusion can complement the shortcomings of a single input source
and provide richer and more effective information. Therefore, we also design several single-modal fusion inputs as shown
on the right side of Figure 3, including lidar noground and multi fusion3, lidar noground and depth, lidar-noground and
camera gray, as well as camera and lidar.
A.2. Reward function
We use the following reward function in our experiments:
R = 200 · rc + 200 · rft + 50 · rlt + 2 · rsc + vlon + 10 · rf + ro − 5α2 + 0.2 · rlat − 0.1
(12)
where rc is the reward related to collision, which is set to -1 if the ego vehicle collides and 0 otherwise. vlon is the
longitudinal speed of the ego vehicle. rf is the reward related to running too fast, which is set to −1 if it exceeds the
desired speed (8 m/s here) and 0 otherwise. ro is set to −1 if the ego vehicle runs out of the lane, and 0 otherwise. α is
the steering angle of the ego vehicle in radians. rlat is the reward related to lateral acceleration, which is calculated by
rlat = −|α| · v2
lon. The last constant term is added to prevent the ego vehicle from standing still. rft represents the time to
collision in the forward direction. rlt represents the time to collision in the lateral direction. rsc represents the smoothness
constraint.
A.3. Hyperparameter settings
Mmodel, the KL regularizer is clipped below 3.0 free nats for imagination range H = 15 using the same trajectories for
updating action and value models separately with λ = 0.99 and λ = 0.95, while k = 1.5. The size of all our trainig and
evaluating images is 128 × 128 × 3. For Algorithm 1, a random seed S = 5 is used to collect datasets for the ego vehicle
before updating the model every C = 100 steps during training process.
Table 3. Hyperparameter settings for the training and evaluation of each baseline
METHOD
BATCH SIZE
MODEL SIZE
EVAL EPISODES
ACTION REPEAT
DDPG
256
32
5
2
SAC
256
32
5
2
TD3
256
32
5
2
DQN
256
32
5
2
LATENT SAC
256
32
5
2
SGADS
32
32
10
1
12
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
Table 4. Hyperparameter settings for the learning rate of each baseline
METHOD
MODEL LEARNING RATE
ACTOR LEARNING RATE
VALUE LEARNING RATE
DDPG
1 × 10−4
3 × 10−4
3 × 10−4
SAC
1 × 10−4
3 × 10−4
3 × 10−4
TD3
1 × 10−4
3 × 10−4
3 × 10−4
DQN
1 × 10−4
3 × 10−4
3 × 10−4
LATENT SAC
1 × 10−4
3 × 10−4
3 × 10−4
SGADS
1 × 10−3
8 × 10−5
8 × 10−5
A.4. The world model reconstructs the input images from the original sensors
We explores the differences between input images from original sensors and the corresponding reconstructed input images
from a world model for 8 types of input. As shown in Figure 8, multiple comparisons are made between the reconstructed
input types generated by the world model and their corresponding original sensor inputs. Among them, multi-fusion2,
lidar noground, lidar+camera and lidar reconstructions are very clear and highly consistent, indicating that q(ot|st) has a
precise decoding capability without causing loss of st. However, birdeye, semantic, (lidar noground and multi-fusion3),
and (lidar noground and camera gray) of reconstructions are not as clear as their sensor input. This suggests that world
model have difficulty understanding large amounts of irrelevant information related to driving tasks resulting in unclear
reconstruction outputs.
Figure 8. Randomly sampled frames to reconstruct the input images from the original sensors of SGADS on 8 types of input. For each
type of image, first row: original sensor inputs. Second row: reconstructed images.
13
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
A.5. Algorithm
we propose the framework of safe and generalized end-to-end autonomous driving system with reinforcement learning and
demonstrations is presented in Algorithm 1.
Algorithm 1 SGADS
Input: batch size B, batch length L, imagination horizon H, step T, trajectory τ, number of trajectories N, initialize
datasets D with seeds.
1: The first stage of training
Collect Dexpert via the G29 steering.
Randomly initialize parameters θ, ϕ, ψ, η, ω.
2: while Mexpert does not converge do
3:
for c = 1, 2, . . . , C do
4:
Randomly sample B sequences {(a′
t, o′
t, r′
t)}k+L
t=k from Dexpert.
5:
Calculating the states pη(s′
t|s′
t−1, a′
t−1, o′
t).
6:
Update parameters ϕ, η.
7:
end for
8: end while
9: The second stage of training
Initialize all neural network parameters using Mexpert.
10: while Mmodel does not converge do
11:
for c = 1, 2, . . . , C do
12:
Ramdomly sample B sequences from D.
13:
Infer belief state st ∼ qK(st|st−1, at−1, ot).
14:
for i = 1, 2, . . . , N do
15:
Rollout imaginary trajectories (si,τ, ai,τ)t+H
τ=t with belief transition model.
16:
end for
17:
Update θ, ϕ, ψ, ξ, η, ω.
18:
end for
19:
for t = 1, 2, . . . , T do
20:
Compute st ∼ qK(st|st−1, at−1, ot) from history.
21:
Compute at ∼ π(at|st) with action model.
22:
rt, ot+1 ← TTC and CSS.
23:
end for
24:
Update D ← D ∪ {(at, ot+1, rt)T
t=1}
25: end while
A.6. Further comparisons were made regarding safety and generalization
In the existing baseline methods, the default number of environment vehicles is set to 100. We also conducted comparisons
in terms of generalization and safety as shown in Figure 9 and 10. The experiments indicate that our model significantly
outperforms existing methods.
A.7. More results regarding predictions of future driving trajectories
The accurate prediction of future driving trajectories is a precondition for making optimal decision making. Random
samples of driving trajectories for the first 15 time steps were collected from the sensor. Subsequently, the model predicted
the driving trajectories for the next 15 time steps, and the ground truths for these trajectories were also provided We provide
additional results regarding predictions of future driving trajectories as shown in Figure 10 ...Figure 18.
14
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
Figure 9. The model is trained in Town03 and then evaluated for generalization in Town01, Town04 and Town05.
Figure 10. The model is trained in Town03 and then evaluated for safety in roundabout of the Town03, evluated for generalization in
Town06.
(a) Randomly sample ground truth of inputs Lidar o1, o2, · · · , o15
(b) Randomly sample ground truth of inputs Lidar o16, o17, · · · , o30
(c) Our model can imagine driving behaviors ˆo16, ˆo17, · · · , ˆo30
Figure 11. We randomly sampled input images, and then SGADS was used to make predictions
(a) Randomly sample ground truth of inputs Lidar o1, o2, · · · , o15
(b) Randomly sample ground truth of inputs Lidar o16, o17, · · · , o30
(c) Our model can imagine driving behaviors ˆo16, ˆo17, · · · , ˆo30
Figure 12. We randomly sampled input images, and then SGADS was used to make predictions
15
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
(a) Randomly sample ground truth of inputs Lidar o1, o2, · · · , o15
(b) Randomly sample ground truth of inputs Lidar o16, o17, · · · , o30
(c) Our model can imagine driving behaviors ˆo16, ˆo17, · · · , ˆo30
Figure 13. We randomly sampled input images, and then SGADS was used to make predictions
(a) Randomly sample ground truth of inputs Lidar o1, o2, · · · , o15
(b) Randomly sample ground truth of inputs Lidar o16, o17, · · · , o30
(c) Our model can imagine driving behaviors ˆo16, ˆo17, · · · , ˆo30
Figure 14. We randomly sampled input images, and then SGADS was used to make predictions
(a) Randomly sample ground truth of inputs Lidar o1, o2, · · · , o15
(b) Randomly sample ground truth of inputs Lidar o16, o17, · · · , o30
(c) Our model can imagine driving behaviors ˆo16, ˆo17, · · · , ˆo30
Figure 15. We randomly sampled input images, and then SGADS was used to make predictions.
16
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations
(a) Randomly sample ground truth of inputs Lidar o1, o2, · · · , o15
(b) Randomly sample ground truth of inputs Lidar o16, o17, · · · , o30
(c) Our model can imagine driving behaviors ˆo16, ˆo17, · · · , ˆo30
Figure 16. We randomly sampled input images, and then SGADS was used to make predictions
(a) Randomly sample ground truth of inputs Lidar o1, o2, · · · , o15
(b) Randomly sample ground truth of inputs Lidar o16, o17, · · · , o30
(c) Our model can imagine driving behaviors ˆo16, ˆo17, · · · , ˆo30
Figure 17. We randomly sampled input images, and then SGADS was used to make predictions
(a) Randomly sample ground truth of inputs Lidar o1, o2, · · · , o15
(b) Randomly sample ground truth of inputs Lidar o16, o17, · · · , o30
(c) Our model can imagine driving behaviors ˆo16, ˆo17, · · · , ˆo30
Figure 18. We randomly sampled input images, and then SGADS was used to make predictions
17
"
"Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using training image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may only capture discriminative image regions of target object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP space to enhance the semantic alignment between the segmented regions and the target object categories.","Semantic segmentation aims to classify every pixel in images to identify object categories and the associated regions, which can benefit various applications in the real world. While promising results have been presented by fully-supervised approaches, collecting pixel-level annotations could be time-consuming and expensive, and therefore limits the scalability and practicality of fully-supervised methods. To address this issue, Weakly-Supervised Semantic Segmentation (WSSS) has emerged as an alternative approach to train segmentation models with only coarse or incomplete annotations such as bounding boxes, scribbles ( [32]), points [3], or image-level labels.","With the rapid growth in the amount of visual and linguistic data in recent years, several vision-language models [10,37,41] have been proposed to bridge the underlying semantics between images and text descriptions. Given that both the images and the associated text labels (category names) are available in the setting of WSSS, the underlying image-text semantics from the CLIP [41] space can be leveraged for pixel-level predictions. Recent approaches focus on manually designing text prompts to refine the produced pseudo masks, including background texts for suppressing co-occurring background regions (e.g., {“a photo of river”, “a photo of lake”, “a photo of water”} for the “boat” category [56]. Despite the effectiveness demonstrated, these approaches require heuristic human knowledge involved with the additional manual effort, as shown in Figure 1 (a). Moreover, manual prompting may not fully exploit the semantic knowledge inside the CLIP space.nannannan","In this paper, we aim to fully exploit the CLIP space to extract semantic information that can benefit the weakly-supervised semantic segmentation problem without manual prompting. To achieve this goal, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework to learn prompts embedded with the semantic knowledge discovered from the CLIP space, as shown in Figure 1 (b), where the learned prompts can enhance the semantic alignment between the segmented regions and the target object categories.nan","To evaluate our proposed SemPLeS framework, we first compare the accuracy of the initial activation maps and also the produced pseudo masks (Seed) with previous works, as shown in Table 1. From the results in this table, our method achieves the best performance compared with existing weakly-supervised segmentation works. Specifically, our initial activation maps (Seed) achieve 68.7% while the pseudo masks (Mask) report 78.3% in mIoU. This verifies that, by advancing vision-language learning on the pre-trained CLIP model, our proposed SemPLeS framework successfully generates pixel-wise predictions from image-level labels, which would benefit the learning of the following segmentation model.For the final segmentation results, our SemPLeS achieves the best performance and reports 74.2% and 74.8% mIoU on the validation and testing set, respectively, as shown in Table 2. Our method outperforms the previous SOTA, Mat-Label [51], by 1.2% and 2.1% mIoU on the validation and testing set, respectively. Moreover, our results are only 5.5% and 4.8% lower than our upper bound, which is the fully-supervised method trained with densely-annotated labels, on the validation and testing set, respectively.","In this paper, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which advances vision-language learning to achieve weakly-supervised semantic segmentation (WSSS). In addition to exploiting the pre-trained CLIP model to perform Segment-Label Matching, we further present Contrastive Prompt Learning and Class-associated Semantic Refinement in the proposed SemPLeS framework to prevent false activation of image backgrounds. With no need to manually define background texts through prompt engineering, our learned prompts properly capture and suppress co-occurring backgrounds for each object category, resulting in precise activation maps for segmentation in a weakly-supervised fashion. Quantitative experiments on the segmentation benchmarks confirm the effectiveness of our proposed SemPLeS framework, and visualization and ablation studies are conducted to demonstrate and verify the interpretability of learned prompts.",SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation,"Ci-Siang Lin, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen","SemPLeS: Semantic Prompt Learning for
Weakly-Supervised Semantic Segmentation
Ci-Siang Lin1,2⋆ Chien-Yi Wang1 Yu-Chiang Frank Wang1,2 Min-Hung Chen1†
1 NVIDIA
2 National Taiwan University
Abstract. Weakly-Supervised Semantic Segmentation (WSSS) aims to
train segmentation models using training image data with only image-
level supervision. Since precise pixel-level annotations are not accessible,
existing methods typically focus on producing pseudo masks for training
segmentation models by refining CAM-like heatmaps. However, the pro-
duced heatmaps may only capture discriminative image regions of target
object categories or the associated co-occurring backgrounds. To address
the issues, we propose a Semantic Prompt Learning for WSSS (Sem-
PLeS) framework, which learns to effectively prompt the CLIP space
to enhance the semantic alignment between the segmented regions and
the target object categories. More specifically, we propose Contrastive
Prompt Learning and Class-associated Semantic Refinement to learn the
prompts that adequately describe and suppress the image backgrounds
associated with each target object category. In this way, our proposed
framework is able to perform better semantic matching between object
regions and the associated text labels, resulting in desired pseudo masks
for training the segmentation model. The proposed SemPLeS framework
achieves SOTA performance on the standard WSSS benchmarks, PAS-
CAL VOC and MS COCO, and demonstrated interpretability with the
semantic visualization of our learned prompts. The codes will be released.
1
Introduction
Semantic segmentation aims to classify every pixel in images to identify object
categories and the associated regions, which can benefit various applications
in the real world [39, 44, 64]. While promising results have been presented by
fully-supervised approaches [5–7,31,36,65,66], collecting pixel-level annotations
could be time-consuming and expensive, and therefore limits the scalability and
practicality of fully-supervised methods. To address this issue, Weakly-Supervised
Semantic Segmentation (WSSS) has emerged as an alternative approach to train
segmentation models with only coarse or incomplete annotations such as bound-
ing boxes [22], scribbles ( [32]), points [3], or image-level labels. Among these
annotation forms, image-level labels which indicate the presence or absence of
⋆ Work done during the internship at NVIDIA Research, Taiwan.
† Project lead.
arXiv:2401.11791v1  [cs.CV]  22 Jan 2024
2
C.-S. Lin et al.
Class-associated
Semantic Knowledge
Prompting
(b) Our SemPLeS framework
Manual
Prompting
(a) Previous CLIP-guide WSSS framework
Human
Knowledge
SemPLeS
Segmentation
Knowledge
CLIP
Space
Segmentation
Model
Segmentation
Knowledge
CLIP
Space
Segmentation
Model
Training data
Class Labels: boat, sheep, aeroplane, …
“boat” seems
related to “river”,
“lake”, “water”, …
“river”
“lake”
“water”
Training data
Class Labels: boat, sheep, aeroplane, …
Fig. 1: An overview of this paper. (a) Previous CLIP-guide WSSS methods require
heuristic human knowledge involved with the additional manual prompt engineering ef-
fort, while (b) our proposed SemPLeS framework automatically learns class-associated
prompts embedded with semantic knowledge discovered from the CLIP representation
space without any manual human effort.
certain object categories are commonly used due to the efficiency in data col-
lection and the availability in various benchmark image datasets. Since precise
annotations of object positions are not observed, learning to localize and seg-
ment object categories from image-level supervision is particularly challenging.
Most existing methods [4, 46, 52, 59] focus on producing pseudo ground truth
masks by refining CAM-like heatmaps [48, 67] with class labels as discrimina-
tive supervision. Despite the shown efficacy, the produced pseudo masks may
still miss relevant regions of target object categories and fail to cover the entire
object. Furthermore, co-occurring backgrounds associated with certain object
categories may also be falsely activated (e.g., roads, or trees in an image of a
car). Consequently, learning precise image regions that align with the semantics
of target objects from weak supervision remains a challenging task.
With the rapid growth in the amount of visual and linguistic data in recent
years, several vision-language models [10,37,41] have been proposed to bridge the
underlying semantics between images and text descriptions. Given that both the
images and the associated text labels (category names) are available in the set-
ting of WSSS, the underlying image-text semantics from the CLIP [41] space can
be leveraged for pixel-level predictions. Recent approaches focus on manually de-
signing text prompts to refine the produced pseudo masks, including background
texts for suppressing co-occurring background regions (e.g., {“a photo of river”,
“a photo of lake”, “a photo of water”} for the “boat” category [56]. Despite the
effectiveness demonstrated, these approaches require heuristic human knowledge
involved with the additional manual effort, as shown in Figure 1 (a). Moreover,
manual prompting may not fully exploit the semantic knowledge inside the CLIP
space.
In this paper, we aim to fully exploit the CLIP space to extract semantic
information that can benefit the weakly-supervised semantic segmentation prob-
lem without manual prompting. To achieve this goal, we propose a Semantic
Prompt Learning for WSSS (SemPLeS) framework to learn prompts embed-
ded with the semantic knowledge discovered from the CLIP space, as shown in
SemPLeS: Semantic Prompt Learning for WSSS
3
Figure 1 (b), where the learned prompts can enhance the semantic alignment
between the segmented regions and the target object categories with image-
level labels. More specifically, we perform image-text contrastive learning un-
der the guidance of CLIP and train a mask generator to generate class ac-
tivation maps. However, such produced masks might not be sufficiently pre-
cise, and the co-occurring backgrounds associated with the object categories
may be falsely activated. To alleviate this problem, we uniquely present Con-
trastive Prompt Learning and Class-associated Semantic Refinement to suppress
class-associated background regions. In Contrastive Prompt Learning, we learn
prompts to capture co-occurring backgrounds from images and labels. Without
manually defining the background texts, our learned prompts would properly
describe the backgrounds associated with target object categories. Under the
guidance of our class-associated background prompts, we further suppress co-
occurring backgrounds from the activation maps via Class-associated Semantic
Refinement. With the above-introduced learning strategy, we will be able to en-
hance the semantic matching between object regions and the associated text
labels, resulting in desired activation maps for segmentation purposes. The pro-
posed SemPLeS framework achieves SOTA performance on the standard WSSS
benchmarks, PASCAL VOC and MS COCO. Moreover, qualitative experiments
demonstrate that our learned prompts would show better interpretability than
manual prompts.
In summary, our contributions are three-fold:
– We propose a Semantic Prompt Learning for WSSS (SemPLeS) frame-
work, which performs weakly-supervised semantic segmentation via enforcing
semantic alignment between the object categories and the associated image
regions.
– In SemPLeS, we present Contrastive Prompt Learning to learn prompts that
can capture co-occurring background regions. With no need to manually define
background texts, our learned prompts would represent backgrounds associ-
ated with the object categories.
– Under the guidance of the derived prompts, our Class-associated Semantic
Refinement learns to suppress co-occurring backgrounds while enhancing the
semantic matching between object regions and the associated class labels,
resulting in precise pseudo masks for WSSS.
2
Related Works
2.1
Weakly-Supervised Semantic Segmentation (WSSS)
The existing WSSS approaches typically follow a three-stage learning process.
Firstly, the image-level labels are utilized as supervision to generate initial Class
Activation Maps (CAMs) [48,67]. Secondly, the initial CAMs are refined by using
dense CRF [23] or pixel affinity-based methods [1, 2] to obtain pseudo masks.
Lastly, the refined pseudo masks are further used as ground truth to train a
segmentation network. Among all the stages, producing precise CAMs is the
4
C.-S. Lin et al.
main focus of WSSS, and various approaches have been proposed to improve the
quality of CAMs to benefit the learning of the segmentation networks [9,11,15,
17,21,25,28,29,54,57,63]. On the other hand, there are end-to-end WSSS works
without the need for multiple training steps [47, 53], but their performances
are worse than standard 3-stage methods. With the rapid development and the
success of vision transformers [14], recent approaches [12, 40, 45–47, 59, 60, 71]
generate finer activation maps based on the patch-level affinity learned from the
attention layers. In general, most WSSS methods take class labels as discrimi-
native supervision to generate CAMs without considering the textual meaning
of class names. Instead, our method exploits vision-language models to learn
class-associated semantic knowledge for segmentation.
2.2
CLIP for Semantic Segmentation
Recently, the Contrastive Language-Image Pretraining (CLIP) model [41] has
been adopted in semantic segmentation tasks thanks to the generalized knowl-
edge learned from a large corpus of image-text pairs. Given the generalization ca-
pability, several zero-shot approaches [13,18,27,30,38,42,58,61,62] exploit CLIP
to segment the classes which are unseen during training. However, these meth-
ods still require mask annotations during training, causing additional manual
effort. CLIP has also been adopted to improve unsupervised methods [19,49,68].
While minimizing the annotation effort, the segmentation performance is still
unsatisfactory since there is no ground truth to guide the training. Recently,
CLIP has also been utilized to benefit WSSS by comprehending the meaning
of class-related texts using the language model [34, 56]. However, they mainly
design text prompts with manual effort. Instead, our proposed SemPLeS frame-
work automatically learns the class-associated prompts embedded with semantic
knowledge, producing precise pseudo masks for segmentation purposes.
2.3
Prompt Learning
In natural language processing (NLP), prompting [35] involves giving a text-
based input such as a sentence or phrase to obtain desired responses from lan-
guage models. Driven by the recent success of pre-trained vision-language models
(e.g., CLIP [41]), there has been an increasing interest to identify proper prompts
for computer vision tasks. Early work relies on prompt engineering to identify
text templates (e.g., “a photo of ”) describing classes of interest to obtain under-
lying knowledge. However, such a trial and error approach generally takes a large
amount of time and effort and also requires expertise about the task. To tackle
the problem, prompt learning methods [20,69,70] replace the manually-defined
text prompts with a set of learnable context vectors preceding the class names
to automate the prompting process. Different from the above methods, our Con-
trastive Prompt Learning aims to capture class-associated semantic knowledge
for segmentation purposes rather than replacing the text template like “a photo
of {}” for classification purposes.
SemPLeS: Semantic Prompt Learning for WSSS
5
3
Method
𝑣!
""
𝑢!
""
𝑢!
#
Learnable
Prompts 𝑝!
“a photo of train""
Predicted
background 𝑋!
""
𝐸$
Image Encoder
𝐸%
Text Encoder
𝐸%
Text Encoder
Mask Generator
𝑆
Background
𝑋
𝑀
Predicted
foreground 𝑋!
#
Foreground
𝑋!
#
“a photo of train""
𝑋!
""
𝐸$
Image Encoder
𝐸%
Text Encoder
𝑣!
#
𝑢!
""
repel
attract
repel
𝑋!
#′
𝑀′
attract
repel
(a) Segment-Label Matching
(update 𝑆)
(b) Contrastive Prompt Learning
(update 𝑝! )
(c) Class-associated
Semantic Refinement
(update 𝑆)
Output
pseudo mask 𝑀′
&
foreground region 𝑋!
#′
Fixed & Shared CLIP Encoders
𝐸$
Image Encoder
𝐸%
Text Encoder
Text Prompt 𝑡!
Fig. 2: An overview of the proposed SemPLeS framework. We first introduce
Segment-Label Matching as a warm-up stage, which leverages image-text contrastive
learning to produce initial object masks M from the mask generator S (Sec. 3.2).
We then propose Contrastive Prompt Learning (Sec. 3.2) and Class-associated Seman-
tic Refinement (Sec. 3.2) to automatically learn prompts pk embedded with semantic
knowledge, which can help produce refined object masks M ′.
3.1
Problem Formulation and Model Overview
We first define the problem setting and notations used in this paper. In weakly-
supervised semantic segmentation (WSSS), we assume that there is a set of N
images X with the associated image-level labels y, where X ∈ RH×W ×3 and
y ∈ {0, 1}K is a multi-hot vector indicating the presence or absence of K object
categories. Without access to pixel-wise annotations, our goal is to learn precise
class activation maps for segmentation purposes. To perform segmentation from
image labels, we propose a novel Semantic Prompt Learning for WSSS (Sem-
PLeS) framework to derive prompts from visual and textual representations of
CLIP [41], which would be exploited to enhance the semantic alignment between
the segmented regions and the target object categories.
As shown in Figure 2, our framework takes Segment-Label Matching as a
warm-up stage, leveraging image-text contrastive learning to produce initial ob-
ject masks M from our mask generator S (Sec. 3.2). To suppress falsely activated
backgrounds in such masks (e.g., Xf
k in the red box), we present Contrastive
Prompt Learning (Sec. 3.2) and Class-associated Semantic Refinement (Sec. 3.2)
to tackle this problem. The former learns class-associated prompts pk to cap-
ture co-occurring backgrounds from images and labels, while the latter takes the
6
C.-S. Lin et al.
derived prompts to disregard co-occurring backgrounds from the object masks
(e.g., Xf
k
′ in the green box). By jointly enforcing vision-language matching and
suppression objectives, our proposed framework would enhance the semantic
alignment between object regions and the associated text labels, resulting in
precise activation maps for the objects of interest.
3.2
Semantic Prompt Learning for WSSS (SemPLeS)
Segment-Label Matching: Given an input image X, our mask generator
S is designed to produce soft foreground masks M = S(X) for target object
categories. Since pixel-wise annotations are not available, we choose to leverage
vision-language models to guide the learning of our mask generators from image-
level supervision. To be more precise, we exploit the joint latent space for images
and texts from CLIP to match the object regions and the associated text labels.
To achieve this, an image-text triplet (i.e., foreground-background-text) would
be formulated to perform contrastive learning, as illustrated in Figure 2 (a).
For the kth ground truth category which presents in the input image X (i.e.,
yk = 1), we derive the foreground image Xf
k = Mk · X by applying the kth
predicted mask Mk to the original image X. Similarly, we reverse the predicted
mask to obtain the background regions Xb
k = (1−Mk)·X. As for the text input
tk, we adopt the common prompt template “a photo of {}” filled with the kth
class name to describe the category of interest. With the triplet [Xf
k , Xb
k, tk]
serving as the input of the image encoder EI and text encoder ET pre-trained
by CLIP, we perform image-text contrastive learning to maximize the cosine
similarity between Xf
k and tk for the foreground, while the similarity of Xb
k and
tk would be minimized to repel the background. Therefore, our matching loss
Lmatch would be formulated as follows:
Lmatch = EX −
h
log(sim(vf
k, uf
k)) + λb · log(1 − sim(vb
k, uf
k))
i
,
where
vf
k = EI(Xf
k ),
vb
k = EI(Xb
k)
and
uf
k = ET (tk).
(1)
Here, λb is the loss weight for repelling backgrounds and sim refers to cosine sim-
ilarity. Note that we keep the image encoder EI and the text encoder ET frozen
during training and preserve the latent space learned from CLIP to avoid poten-
tial overfitting. With the above Segment-Label Matching step, our mask genera-
tor S is encouraged to segment object regions that align with the associated text
labels. However, as noted above, such masks learned from image-level supervi-
sion are still coarse, and may falsely include co-occurring backgrounds associated
with certain object categories. Therefore, the above image-text matching is not
sufficient for segmentation.
Contrastive Prompt Learning: To address the coarse mask issues, the previ-
ous language-guided approach [56] exploits vision-language models to refine the
masks with manual prompting techniques. However, these methods require ad-
ditional prompt engineering efforts with human knowledge involved. Moreover,
SemPLeS: Semantic Prompt Learning for WSSS
7
manual prompting may not be able to fully exploit vision-language represen-
tation space. To tackle these problems, we propose Contrastive Prompt Learn-
ing (Figure 2 (b)) to learn prompts embedded with semantic knowledge from
vision-language models, facilitating the following object mask refinement. Dif-
ferent from the previous work, we employ a sequence of learnable prompts pk
as the input of the text encoder ET to describe backgrounds for the kth cate-
gory. Specifically, to align the prompts pk with the background image Xb
k, we
maximize the similarity of their representations in the latent space by proposing
LI
prompt. On the other hand, to avoid describing the foreground objects, we en-
courage the similarity between ub
k and uf
k to be low with the proposed LT
prompt.
Thus, we propose Lprompt as below:
Lprompt = LI
prompt + λT · LT
prompt
= EX
h
−log(sim(ub
k, vb
k)) + λT · log(sim(ub
k, uf
k))
i
,
where
ub
k = ET (pk),
vb
k = EI(Xb
k)
and
uf
k = ET (tk).
(2)
Here, λT is the loss weight for minimizing the similarities to the object cate-
gories. Note that the mask generator S is fixed and pk is the only trainable part
in this step. Once the above learning is complete, our prompts pk can repre-
sent backgrounds for the kth categories. It is worth noting that, our Contrastive
Prompt Learning aims to capture class-associated backgrounds for segmenta-
tion purposes rather than replacing the text template like “a photo of {}” for
classification purposes.
Class-associated Semantic Refinement: Finally, to suppress co-occurring
background regions from the object mask M, our SemPLeS framework exploits
the previously derived background prompts pk to perform Class-associated Se-
mantic Refinement (Figure 2 (c)). More specifically, we encourage our mask
generator S to produce refined masks M ′ by excluding the semantic knowledge
embedded in the background prompts pk, while the objectives introduced in 3.2
are retained to match the class labels. Hence, the refinement loss Lrefine and
the total loss function Ltotal are defined as follows:
Ltotal = Lmatch + λ · Lrefine,
where
Lrefine = EX
h
−log(1 − sim(vf
k, ub
k))
i
.
(3)
Here, λ is the weight for the refinement loss. It can be seen that, with the derived
background prompts pk (fixed in this step) and the introduced refinement loss
Lrefine, the class-associated background regions would be suppressed from the
foreground mask M, preventing possible false activation. More importantly, by
jointly applying the matching and refinement objectives with image-level super-
vision, our Semantic Prompt Learning for WSSS framework advances vision-
language learning to enhance the semantic alignment between the segmented
regions and the target object categories, resulting in compact and complete ob-
ject masks M ′ desired for segmentation in a weakly-supervised fashion, where M ′
can be adopted in the WSSS pipeline to obtain the final segmentation results.
8
C.-S. Lin et al.
4
Experiments
4.1
Datasets, Implementation, and Evaluation Metrics
We train and validate our proposed framework on the standard benchmark se-
mantic segmentation datasets, PASCAL VOC 2012 [16] and MS COCO 2014 [33].
The PASCAL VOC 2012 dataset contains 20 object categories along with a
background category. The original training, validation, and testing set consists
of 1464, 1449, and 1456 images, respectively, with only image-level labels for
training. The testing set results are obtained from the official evaluation web-
site. As for the MS COCO 2014 dataset, the training and validation set contains
82081 and 40137 images from 80 object categories, respectively. We adopt the
recent SOTA Transformer-based backbone model [71] for our mask generator S,
and perform online retraining [71] to obtain the final segmentation model. For
CLIP [41], we use ViT-B/32 [14] as the image encoder. The learnable prompts are
randomly initialized with the sequence length K = 30, and the prompt embed-
ding dimension D = 512. Following the common protocol in previous works [56],
we train the proposed framework with an augmented training set of 10582 im-
ages (e.g., flipped, cropped, etc.). The mean Intersection over Union (mIoU) is
used as the evaluation metric for all experiments.
4.2
Quantitative Comparisons
Table 1: Quantitative comparisons of the
initially generated CAMs (Seed) and the
pseudo masks on PASCAL VOC 2012 train-
ing set. The best and second results are in
bold and underline, respectively. †: repro-
duced from the public codes.
Seed
Mask
CLIMS CVPR’22 [56]
57.5
72.8
MCTformer CVPR’22 [59] 61.7
69.1
Spatial-BCE ECCV’22 [54] 65.3
66.3
ViT-PCM ECCV’22 [45]
67.1
71.4
ESOL NeurIPS’22 [29]
53.6
68.7
ToCo CVPR’23 [47]
-
72.2
CLIP-ES CVPR’23 [34]
70.8
75.0
MMCST CVPR’23 [60]
66.3
-
USAGE ICCV’23 [40]
67.7
72.8
WeakTr arXiv’23 [71]†
65.9
74.2
SemPLeS (Ours)
68.7
78.3
To
evaluate
our
proposed
Sem-
PLeS framework, we first compare
the accuracy of the initial activation
maps and also the produced pseudo
masks (Seed) with previous works, as
shown in Table 1. From the results
in this table, our method achieves
the best performance compared with
existing weakly-supervised segmenta-
tion works. Specifically, our initial ac-
tivation maps (Seed) achieve 68.7%
while the pseudo masks (Mask) report
78.3% in mIoU. This verifies that, by
advancing vision-language learning on
the pre-trained CLIP model, our pro-
posed SemPLeS framework success-
fully generates pixel-wise predictions
from image-level labels, which would
benefit the learning of the following
segmentation model.
For the final segmentation results, our SemPLeS achieves the best perfor-
mance and reports 74.2% and 74.8% mIoU on the validation and testing set,
respectively, as shown in Table 2. Our method outperforms the previous SOTA,
SemPLeS: Semantic Prompt Learning for WSSS
9
Table 2: Quantitative comparisons for recent methods on PASCAL VOC 2012 val and
test sets. “Sup.” denotes supervision type. F denotes full supervision. I denotes image-
level supervision. “Seg. Model” denotes the segmentation network. “DL1”, “DL2”, and
“DL3” denote DeepLabV1, V2, and V3+, respectively. “ResX” and “WResX” denote
ResNet-X and WideResNet-X, respectively. “Backbone” denotes the network for CAMs
generation.
Sup. Method
Seg. Model
Backbone val
test
F
Full supervision
DeepLabV2
TPAMI’18 [5]
DL2-Res101
-
77.6 79.7
WResNet38
PR’19 [55]
DL1-WRes38
-
80.8 82.5
Segmenter
ICCV’21 [50]
Seg-DeiT-S
-
79.7 79.6
I
Image-level supervision only
AMN
CVPR’22 [26]
DL2-Res101
Res50
69.5 69.6
W-OoD
CVPR’22 [25]
DL2-Res101
Res50
69.8 69.9
CLIMS
CVPR’22 [56]
DL2-Res101
Res50
69.3 68.7
MCTformer
CVPR’22 [59] DL1-WRes38
DeiT-S
71.9 71.6
Spatial-BCE
ECCV’22 [54] DL1-Res101
WRes38
68.5 69.7
ViT-PCM
ECCV’22 [45]
DL2-Res101 ViT-B/16 70.3 70.9
ESOL
NeurIPS’22 [29]
DL2-Res101
Res50
69.9 69.3
ToCo
CVPR’23 [47]
DeiT-B
DeiT-B
69.8 70.5
CLIP-ES
CVPR’23 [34]
DL2-Res101 ViT-B/16 71.1 71.4
OCR
CVPR’23 [12]
DL1-WRes38
DeiT-S
72.7 72.0
MMCST
CVPR’23 [60]
DL1-WRes38
ViT-B
72.2 72.2
BECO
CVPR’23 [43]
DL3-Res101
Res101
72.1 71.8
ACR
CVPR’23 [24]
DL1-WRes38 WRes38
71.9 71.9
USAGE
ICCV’23 [40]
DL1-WRes38
DeiT-S
71.9 72.8
FPR
ICCV’23 [8]
DL2-Res101
Res50
70.3 70.1
Mat-Label
ICCV’23 [51]
DL2-Res101
Res50
73.0 72.7
MCC
WACV’24 [53]
DeiT-B
DeiT-B
70.3 71.2
SemPLeS (Ours)
Seg-DeiT-S
DeiT-S
74.2 74.8
Mat-Label [51], by 1.2% and 2.1% mIoU on the validation and testing set, respec-
tively. Moreover, our results are only 5.5% and 4.8% lower than our upper bound,
which is the fully-supervised method trained with densely-annotated labels, on
the validation and testing set, respectively. Similarly, our SemPLeS performs
favorably and achieves 44.9% mIoU on MSCOCO, as shown in Table 3. The
above results verify that our method is effective in performing segmentation in
a weakly-supervised fashion.
10
C.-S. Lin et al.
4.3
Qualitative Comparisons
Table 3: Quantitative comparisons of re-
cent methods on MS COCO 2014 validation
set.
Method
Val
MCTformer CVPR’22 [59] 42.0
SIPE CVPR’22 [9]
43.6
Spatial-BCE ECCV’22 [54] 35.2
ESOL NeurIPS’22 [29]
42.6
ToCo CVPR’23 [47]
41.3
OCR CVPR’23 [12]
42.5
USAGE ICCV’23 [40]
42.7
SemPLeS (Ours)
44.9
In addition to quantitative results,
we also provide qualitative compar-
isons of initially generated CAMs
as
shown
in
Figure
3,
and
our
method shows more accurate activa-
tion maps compared with CLIMS [56]
and WeakTr [71]. This validates that,
by advancing image-text contrastive
learning with learnable prompts, our
SemPLeS would enhance the align-
ment between the segment regions
and the target object categories. In
Figure 4, we also visualize the cor-
responding regions of our learnable
background prompts by calculating
the similarities to image patches with
the text and image encoders of CLIP. We see that the manual prompts de-
fined in CLIMS [56] may falsely highlight the foreground objects due to high
co-occurrence when pre-training CLIP. Furthermore, such manual prompts are
limited to specific categories, and therefore the corresponding backgrounds even
do not present in the images (e.g., the prompt “a photo of track"" falsely indicates
the background regions of the first example). In contrast, our learnable prompts
successfully highlight all the background regions associated with each object
category, demonstrating the effectiveness of our Contrastive Prompt Learning. It
is worth noting that our class-associated prompts can also learn class-agnostic
knowledge, such as the grass and tree region in both the first (cow) and third
(dog) image in Figure 4.
4.4
Ablation Studies
Table 4: Quantitative ablation studies of
the loss functions on PASCAL VOC 2012.
Lmatch LT
prompt LI
prompt Lrefine mIoU(%)
✓
67.6
✓
✓
✓
67.7
✓
✓
✓
67.9
✓
✓
✓
✓
68.7
Loss Component Analysis: To an-
alyze the importance of the intro-
duced loss functions, we conduct both
quantitative and qualitative ablation
studies, as shown in Table 4 and Fig-
ure 5. In Table 4, we see that applying
only the matching loss Lmatch would
result in 67.6% mIoU. When learning
our background prompts, considering
only the LT
prompt to repel the text labels may result in trivial solutions with
little improvement. On the other hand, if only LI
prompt is enforced to achieve
alignment with the background images, the prompts are still likely to learn the
semantics of the foreground object categories instead of the background, result-
ing in 67.9% mIoU. Finally, when LI
prompt and LT
prompt are jointly applied to
SemPLeS: Semantic Prompt Learning for WSSS
11
Image
CLIMS
GT
SemPLeS
WeakTr
tvmonitor
diningtable
dog
Label
Fig. 3: Qualitative comparisons of the initial CAMs on PASCAL VOC 2012. Note that
our SemPLeS successfully suppresses the backgrounds and achieves better results than
WeakTr and CLIMS, which relies on manually-defined background prompts with the
heuristic human effort.
“a photo of railway”
“a photo of track”
“a photo of tree”
“a photo of track”
All BG Prompts
Ours
All BG Prompts
All BG Prompts
“a photo of tree”
“a photo of sea”
Manually-Defined Prompts
Image
dog
Label
cow
motorbike
Fig. 4: Visualization of manually-defined/learned background prompts and the corre-
sponding image regions. For manual prompts (determined in CLIMS, we select those
with the highest, lowest similar, and the aggregated one (“All BG Prompts”) from the
second to fourth columns, respectively.
learn the background regions while avoid describing the foreground object cate-
gories, the mIoU would improve to 68.7%. Together with the qualitative results
in Figure 5, we validate that our proposed framework SemPLeS and semantic
prompt learning would prevent false activation of co-occurring backgrounds and
therefore benefit segmentation in a weakly-supervised fashion.
Table 5: Loss weight analysis.
λ
0
0.01 0.02 0.05 0.1
mIoU 67.6 68.3 68.4 68.7 67.8
Loss Weight Analysis: In Table 5, we
perform hyper-parameter analysis on the
refinement loss weight λ using PASCAL
VOC 2012. λ = 0 means only Lmatch is
used. When varying λ from 0 to 0.1, the
12
C.-S. Lin et al.
Image
𝐿𝑚𝑎𝑡𝑐ℎ
GT
train
bottle
+
𝐿𝑝𝑟𝑜𝑚𝑝𝑡
& 𝐿𝑟𝑒𝑓𝑖𝑛𝑒
bird
dog
train
Label
Fig. 5: Qualitative ablation studies of losses. With Lrefine applied with our learned
prompts, our SemPLeS successfully suppresses class-associated backgrounds from class
activation maps.
mIoU gradually increases from 67.6% to 68.7% when λ = 0.05, and then drops
to 67.8% when λ = 0.1. Hence, we set λ = 0.05 by default.
Table 6: Learning scheme variants
analysis.
Scheme
# of rounds mIoU
joint training
-
65.9
seq./alter. training
1
68.7
2
68.5
3
68.4
Learning Scheme Variants:
In Ta-
ble
6,
we
provide
quantitative
com-
parisons when using different learning
schemes in our proposed SemPLeS frame-
work using PASCAL VOC 2012. We see
that, jointly training all the learning
steps (i.e., a, b, and c in Figure 2) in Fig-
ure 2 would result in inferior performance
of 65.9%. This is because that, in the be-
ginning of training, the prompts are not
well learned and therefore would not be desired for our Class-associated Se-
mantic Refinement (c). If we instead perform sequential training (i.e., 1 round),
the mIoU would improve to 68.7%. Since alternate training between Contrastive
Prompt Learning (b) and Class-associated Semantic Refinement (c) results in
similar performance in our SemPLeS framework (and also increase the training
time), we choose to sequentially train the two steps once instead.
5
Conclusion
In this paper, we propose a Semantic Prompt Learning for WSSS (Sem-
PLeS) framework, which advances vision-language learning to achieve weakly-
supervised semantic segmentation (WSSS). In addition to exploiting the pre-
SemPLeS: Semantic Prompt Learning for WSSS
13
trained CLIP model to perform Segment-Label Matching, we further present
Contrastive Prompt Learning and Class-associated Semantic Refinement in the
proposed SemPLeS framework to prevent false activation of image backgrounds.
With no need to manually define background texts through prompt engineering,
our learned prompts properly capture and suppress co-occurring backgrounds
for each object category, resulting in precise activation maps for segmentation
in a weakly-supervised fashion. Quantitative experiments on the segmentation
benchmarks confirm the effectiveness of our proposed SemPLeS framework, and
visualization and ablation studies are conducted to demonstrate and verify the
interpretability of learned prompts.
References
1. Ahn, J., Cho, S., Kwak, S.: Weakly supervised learning of instance segmentation
with inter-pixel relations. In: CVPR. pp. 2209–2218 (2019)
2. Ahn, J., Kwak, S.: Learning pixel-level semantic affinity with image-level super-
vision for weakly supervised semantic segmentation. In: CVPR. pp. 4981–4990
(2018)
3. Bearman, A., Russakovsky, O., Ferrari, V., Fei-Fei, L.: What’s the point: Semantic
segmentation with point supervision. In: ECCV. pp. 549–565 (2016)
4. Chang, Y.T., Wang, Q., Hung, W.C., Piramuthu, R., Tsai, Y.H., Yang, M.H.:
Weakly-supervised semantic segmentation via sub-category exploration. In: CVPR.
pp. 8991–9000 (2020)
5. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Se-
mantic image segmentation with deep convolutional nets, atrous convolution, and
fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence 40(4), 834–848 (2017)
6. Chen, L.C., Papandreou, G., Schroff, F., Adam, H.: Rethinking atrous convolution
for semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017)
7. Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder
with atrous separable convolution for semantic image segmentation. In: ECCV.
pp. 801–818 (2018)
8. Chen, L., Lei, C., Li, R., Li, S., Zhang, Z., Zhang, L.: Fpr: False positive rectification
for weakly supervised semantic segmentation. In: ICCV. pp. 1108–1118 (2023)
9. Chen, Q., Yang, L., Lai, J.H., Xie, X.: Self-supervised image-specific prototype
exploration for weakly supervised semantic segmentation. In: CVPR. pp. 4288–
4298 (2022)
10. Chen, Y.C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., Liu,
J.: Uniter: Universal image-text representation learning. In: ECCV. pp. 104–120
(2020)
11. Chen, Z., Wang, T., Wu, X., Hua, X.S., Zhang, H., Sun, Q.: Class re-activation
maps for weakly-supervised semantic segmentation. In: CVPR. pp. 969–978 (2022)
12. Cheng, Z., Qiao, P., Li, K., Li, S., Wei, P., Ji, X., Yuan, L., Liu, C., Chen, J.: Out-
of-candidate rectification for weakly supervised semantic segmentation. In: CVPR.
pp. 23673–23684 (2023)
13. Ding, J., Xue, N., Xia, G.S., Dai, D.: Decoupling zero-shot semantic segmentation.
In: CVPR. pp. 11583–11592 (2022)
14
C.-S. Lin et al.
14. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth
16x16 words: Transformers for image recognition at scale. In: ICLR (2021)
15. Du, Y., Fu, Z., Liu, Q., Wang, Y.: Weakly supervised semantic segmentation by
pixel-to-prototype contrast. In: CVPR. pp. 4320–4329 (2022)
16. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal
visual object classes (voc) challenge. International Journal of Computer Vision 88,
303–338 (2010)
17. Fan, J., Zhang, Z., Song, C., Tan, T.: Learning integral objects with intra-class
discriminator for weakly-supervised semantic segmentation. In: CVPR. pp. 4283–
4292 (2020)
18. Ghiasi, G., Gu, X., Cui, Y., Lin, T.Y.: Scaling open-vocabulary image segmentation
with image-level labels. In: ECCV. pp. 540–557 (2022)
19. He, W., Jamonnak, S., Gou, L., Ren, L.: Clip-S4: Language-guided self-supervised
semantic segmentation. In: CVPR. pp. 11207–11216 (2023)
20. Jia, M., Tang, L., Chen, B.C., Cardie, C., Belongie, S., Hariharan, B., Lim, S.N.:
Visual prompt tuning. In: ECCV. pp. 709–727 (2022)
21. Jiang, P.T., Yang, Y., Hou, Q., Wei, Y.: L2g: A simple local-to-global knowledge
transfer framework for weakly supervised semantic segmentation. In: CVPR. pp.
16886–16896 (2022)
22. Khoreva, A., Benenson, R., Hosang, J., Hein, M., Schiele, B.: Simple does it: Weakly
supervised instance and semantic segmentation. In: CVPR. pp. 876–885 (2017)
23. Krähenbühl, P., Koltun, V.: Efficient inference in fully connected crfs with gaussian
edge potentials. In: NeurIPS (2011)
24. Kweon, H., Yoon, S.H., Yoon, K.J.: Weakly supervised semantic segmentation via
adversarial learning of classifier and reconstructor. In: CVPR. pp. 11329–11339
(2023)
25. Lee, J., Oh, S.J., Yun, S., Choe, J., Kim, E., Yoon, S.: Weakly supervised semantic
segmentation using out-of-distribution data. In: CVPR. pp. 16897–16906 (2022)
26. Lee, M., Kim, D., Shim, H.: Threshold matters in wsss: manipulating the activation
for the robust and accurate segmentation model against thresholds. In: CVPR. pp.
4330–4339 (2022)
27. Li, B., Weinberger, K.Q., Belongie, S., Koltun, V., Ranftl, R.: Language-driven
semantic segmentation. In: ICLR (2022)
28. Li, J., Fan, J., Zhang, Z.: Towards noiseless object contours for weakly supervised
semantic segmentation. In: CVPR. pp. 16856–16865 (2022)
29. Li, J., Jie, Z., Wang, X., Wei, X., Ma, L.: Expansion and shrinkage of localization
for weakly-supervised semantic segmentation. In: NeurIPS (2022)
30. Liang, F., Wu, B., Dai, X., Li, K., Zhao, Y., Zhang, H., Zhang, P., Vajda, P.,
Marculescu, D.: Open-vocabulary semantic segmentation with mask-adapted clip.
In: CVPR. pp. 7061–7070 (2023)
31. Liang-Chieh, C., Papandreou, G., Kokkinos, I., Murphy, K., et al.: Semantic im-
age segmentation with deep convolutional nets and fully connected crfs. In: ICLR
(2015)
32. Lin, D., Dai, J., Jia, J., He, K., Sun, J.: Scribblesup: Scribble-supervised convolu-
tional networks for semantic segmentation. In: CVPR. pp. 3159–3167 (2016)
33. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV. pp. 740–755
(2014)
SemPLeS: Semantic Prompt Learning for WSSS
15
34. Lin, Y., Chen, M., Wang, W., Wu, B., Li, K., Lin, B., Liu, H., He, X.: Clip is
also an efficient segmenter: A text-driven approach for weakly supervised semantic
segmentation. In: CVPR. pp. 15305–15314 (2023)
35. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and
predict: A systematic survey of prompting methods in natural language processing.
ACM Computing Surveys 55(9), 1–35 (2023)
36. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: CVPR. pp. 3431–3440 (2015)
37. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolin-
guistic representations for vision-and-language tasks. In: NeurIPS (2019)
38. Lüddecke, T., Ecker, A.: Image segmentation using text and image prompts. In:
CVPR. pp. 7086–7096 (2022)
39. Meyer, B.J., Drummond, T.: Improved semantic segmentation for robotic applica-
tions with hierarchical conditional random fields. In: ICRA. pp. 5258–5265. IEEE
(2017)
40. Peng, Z., Wang, G., Xie, L., Jiang, D., Shen, W., Tian, Q.: Usage: A unified seed
area generation paradigm for weakly supervised semantic segmentation. In: ICCV.
pp. 624–634 (2023)
41. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML. pp. 8748–8763. PMLR (2021)
42. Rao, Y., Zhao, W., Chen, G., Tang, Y., Zhu, Z., Huang, G., Zhou, J., Lu, J.:
Denseclip: Language-guided dense prediction with context-aware prompting. In:
CVPR. pp. 18082–18091 (2022)
43. Rong, S., Tu, B., Wang, Z., Li, J.: Boundary-enhanced co-training for weakly su-
pervised semantic segmentation. In: CVPR. pp. 19574–19584 (2023)
44. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-
ical image segmentation. In: MICCAI. pp. 234–241. Springer (2015)
45. Rossetti, S., Zappia, D., Sanzari, M., Schaerf, M., Pirri, F.: Max pooling with vision
transformers reconciles class and shape in weakly supervised semantic segmenta-
tion. In: ECCV. pp. 446–463 (2022)
46. Ru, L., Zhan, Y., Yu, B., Du, B.: Learning affinity from attention: end-to-end
weakly-supervised semantic segmentation with transformers. In: CVPR. pp. 16846–
16855 (2022)
47. Ru, L., Zheng, H., Zhan, Y., Du, B.: Token contrast for weakly-supervised semantic
segmentation. In: CVPR. pp. 3093–3102 (2023)
48. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-
cam: Visual explanations from deep networks via gradient-based localization. In:
ICCV. pp. 618–626 (2017)
49. Shin, G., Xie, W., Albanie, S.: Reco: Retrieve and co-segment for zero-shot transfer.
In: NeurIPS (2022)
50. Strudel, R., Garcia, R., Laptev, I., Schmid, C.: Segmenter: Transformer for seman-
tic segmentation. In: ICCV. pp. 7262–7272 (2021)
51. Wang, C., Xu, R., Xu, S., Meng, W., Zhang, X.: Treating pseudo-labels generation
as image matting for weakly supervised semantic segmentation. In: ICCV. pp.
755–765 (2023)
52. Wang, Y., Zhang, J., Kan, M., Shan, S., Chen, X.: Self-supervised equivariant
attention mechanism for weakly supervised semantic segmentation. In: CVPR. pp.
12275–12284 (2020)
53. Wu, F., He, J., Cheng, L., Yin, Y., Hao, Y., Huang, G.: Masked collaborative
contrast for weakly supervised semantic segmentation. In: WACV (2024)
16
C.-S. Lin et al.
54. Wu, T., Gao, G., Huang, J., Wei, X., Wei, X., Liu, C.H.: Adaptive spatial-bce loss
for weakly supervised semantic segmentation. In: ECCV. pp. 199–216 (2022)
55. Wu, Z., Shen, C., Van Den Hengel, A.: Wider or deeper: Revisiting the resnet
model for visual recognition. Pattern Recognition 90, 119–133 (2019)
56. Xie, J., Hou, X., Ye, K., Shen, L.: Clims: cross language image matching for weakly
supervised semantic segmentation. In: CVPR. pp. 4483–4492 (2022)
57. Xie, J., Xiang, J., Chen, J., Hou, X., Zhao, X., Shen, L.: C2am: contrastive learn-
ing of class-agnostic activation map for weakly supervised object localization and
semantic segmentation. In: CVPR. pp. 989–998 (2022)
58. Xu, J., Liu, S., Vahdat, A., Byeon, W., Wang, X., De Mello, S.: Open-vocabulary
panoptic segmentation with text-to-image diffusion models. In: CVPR. pp. 2955–
2966 (2023)
59. Xu, L., Ouyang, W., Bennamoun, M., Boussaid, F., Xu, D.: Multi-class token
transformer for weakly supervised semantic segmentation. In: CVPR. pp. 4310–
4319 (2022)
60. Xu, L., Ouyang, W., Bennamoun, M., Boussaid, F., Xu, D.: Learning multi-modal
class-specific tokens for weakly supervised dense object localization. In: CVPR. pp.
19596–19605 (2023)
61. Xu, M., Zhang, Z., Wei, F., Hu, H., Bai, X.: Side adapter network for open-
vocabulary semantic segmentation. In: CVPR. pp. 2945–2954 (2023)
62. Xu, M., Zhang, Z., Wei, F., Lin, Y., Cao, Y., Hu, H., Bai, X.: A simple baseline for
open-vocabulary semantic segmentation with pre-trained vision-language model.
In: ECCV. pp. 736–753 (2022)
63. Yoon, S.H., Kweon, H., Cho, J., Kim, S., Yoon, K.J.: Adversarial erasing frame-
work via triplet with gated pyramid pooling layer for weakly supervised semantic
segmentation. In: ECCV. pp. 326–344 (2022)
64. Zendel, O., Schörghuber, M., Rainer, B., Murschitz, M., Beleznai, C.: Unifying
panoptic segmentation for autonomous driving. In: CVPR. pp. 21351–21360 (2022)
65. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:
CVPR. pp. 2881–2890 (2017)
66. Zhao, H., Zhang, Y., Liu, S., Shi, J., Loy, C.C., Lin, D., Jia, J.: Psanet: Point-wise
spatial attention network for scene parsing. In: ECCV. pp. 267–283 (2018)
67. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features
for discriminative localization. In: CVPR. pp. 2921–2929 (2016)
68. Zhou, C., Loy, C.C., Dai, B.: Extract free dense labels from clip. In: ECCV. pp.
696–712 (2022)
69. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional prompt learning for vision-
language models. In: CVPR. pp. 16816–16825 (2022)
70. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language
models. International Journal of Computer Vision 130(9), 2337–2348 (2022)
71. Zhu, L., Li, Y., Fang, J., Liu, Y., Xin, H., Liu, W., Wang, X.: Weaktr: Explor-
ing plain vision transformer for weakly-supervised semantic segmentation. arXiv
preprint arXiv:2304.01184 (2023)
"
"In this paper, the authors propose a novel framework for full-body motion reconstruction with sparse sensing data by representing the human body as a graph (Body Pose Graph) and formulate the task as a prediction problem of graph missing nodes. They first endow node features with characteristics corresponding to the joints properties by generating motion features with temporal and spatial properties. Then, a Graph Neural Network with expressive edges is applied to updated nodes. The authors demonstrate that their approach achieves state-of-the-art performance on full-body avatar estimation from sparse inputs and provides insights into learned motion relationships and its prowess in mesh-scale representations.","Existing methods for full-body motion reconstruction from sparse sensor data, which typically use six inertial measurement units (IMUs) on the human body, can be impractical and costly for consumer-grade virtual reality systems. Therefore, the authors focus on reconstructing full-body motion from input signals only available in common virtual reality systems (head-mounted display and handheld controllers) where sensors are attached to the head and hands.","nanThe authors provide an overview of previous work in this area, which can be broadly categorized into studies that use six IMUs and studies that use sensors available in consumer-grade virtual reality systems. They discuss the limitations of existing methods, particularly their reliance on joint motion relations and the lack of consideration of the human body as a graph.nannan","The proposed framework, which is termed ""full-body motion reconstruction with sparse sensing from graph perspective"", consists of the following steps:

1. **Problem Formulation**: The task is formulated as a prediction problem of missing nodes in an established graph. The graph represents the human body, where nodes correspond to joints and edges represent the relationships between joints.

2. **Node Feature Initialization**: The authors extract features from the sparse sensor data and assign them to the nodes in the graph. This is done by integrating positional and angular measurements using dual interactive learning and generating motion features with temporal and spatial properties.

3. **Node Feature Updating**: The node features are updated using a graph convolutional network (GCN) with expressive edges. This allows the nodes to capture diverse joint relationships and contextual information.

4. **Full Body Motion Generation**: The final full-body motion is generated from the updated node features. This is done by referring to a human body model.nan","The authors evaluate their proposed framework on a publicly available dataset and compare it with several baseline methods. The results show that the proposed framework achieves state-of-the-art performance, particularly in predicting unseen lower body joints. It outperforms baseline methods in terms of mean per joint rotation error (MPJRE), mean per joint position error (MPJPE), and mean per joint velocity error (MPJVE).","The authors conclude that their proposed framework represents a significant advancement in full-body motion reconstruction from sparse sensor data. By representing the human body as a graph and modeling spatial-temporal relations among joints, their method surpasses baseline methods, notably in predicting unseen lower body joints.",Full-Body Motion Reconstruction with Sparse Sensing from Graph Perspective,"Feiyu Yao, Zongkai Wu, Li Yi","Full-Body Motion Reconstruction with Sparse Sensing from Graph Perspective
Feiyu Yao1, Zongkai Wu2*, Li Yi3,4,5*
12012 Lab, Huawei Technologies Co., Ltd
2Fancy Technology
3Tsinghua University
4Shanghai Artificial Intelligence Laboratory
5Shanghai Qi Zhi Institute
yaofeiyu1@huawei.com, wuzongkai@fancy.tech, ericyi@mail.tsinghua.edu.cn
Abstract
Estimating 3D full-body pose from sparse sensor data is a
pivotal technique employed for the reconstruction of realis-
tic human motions in Augmented Reality and Virtual Reality.
However, translating sparse sensor signals into comprehen-
sive human motion remains a challenge since the sparsely
distributed sensors in common VR systems fail to capture
the motion of full human body. In this paper, we use well-
designed Body Pose Graph (BPG) to represent the human
body and translate the challenge into a prediction problem
of graph missing nodes. Then, we propose a novel full-body
motion reconstruction framework based on BPG. To estab-
lish BPG, nodes are initially endowed with features extracted
from sparse sensor signals. Features from identifiable joint
nodes across diverse sensors are amalgamated and processed
from both temporal and spatial perspectives. Temporal dy-
namics are captured using the Temporal Pyramid Structure,
while spatial relations in joint movements inform the spatial
attributes. The resultant features serve as the foundational ele-
ments of the BPG nodes. To further refine the BPG, node fea-
tures are updated through a graph neural network that incor-
porates edge reflecting varying joint relations. Our method’s
effectiveness is evidenced by the attained state-of-the-art per-
formance, particularly in lower body motion, outperforming
other baseline methods. Additionally, an ablation study vali-
dates the efficacy of each module in our proposed framework.
Introduction
Continuously full-body motion reconstruction from sparse
motion sensing is crucial for applications in Augmented Re-
ality and Virtual Reality (AR/VR), which demands highly
accurate human motion poses to render vivid avatars in
the digital world and do interactions. Common VR systems
are composed by head-mounted displays and handheld con-
trollers. These devices can provides resourceful abundant
upper body motion information, yet they are unable to pro-
vide corresponding lower body motion data. The signifi-
cant sparsity inherent in known data distribution makes the
generation of realistic full-body motion a particularly chal-
lenging endeavor for conventional methods based on human
kinematics (Company 2018) and matching motions (Ahuja
et al. 2021).
*Corresponding authors.
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Various learning-based methods have been made to gen-
erate full-body avatars from sparse inputs in AR/VR (Dit-
tadi et al. 2021) (Du et al. 2023) (Jiang et al. 2022) (Jiang
et al. 2022). These methods in these diverse studies essen-
tially entail the extraction of features from sparse sensor
data, devoid of considerations for human body joint rela-
tionships. Subsequently, these extracted features are inte-
grated into various network architectures that similarly also
lack a profound consideration of the interdependence among
human body joints. The homogenization of these method-
ologies confines the development of reconstructing human
motion from sparse inputs to the realm of network structure
updates. Also, the absence of sufficient human body infor-
mation contributes to a notable disparity between the recon-
structed outcomes of the lower human body and actual mo-
tion dynamics.
To solve the problems mentioned above, we consider the
human body from graph perspective and propose BPG to
represent full body. The task is then transformed to be pre-
dicting missing nodes in established BPG. Considering the
limited information available on missing nodes, the BPG
is initialized and updated referring to node properties. The
first stage is processing node features. Position feature and
angle feature are fused since they share different transfor-
mation law and distribution. Temporal Pyramid Structure
is proposed on fusing frame-level and clip-level features
to build temporal properties for feature representation. To
model spatial properties, features of limb joints and trunk
joints are generated separately referring to the human skele-
ton dynamic. The generated motion features are assigned to
be initial features in BPG. In Node Feature Updating stage,
the nodes in BPG is updated referring to joint relations.
We split the node relations into static skeleton relations, dy-
namic skeleton relations and latent relations. Then the node
features in BPG are updated in Graph Convolution Network
with expressive edges generated from node relations.
Our main contributions are summarized as follows:
• We are the first to conduct research on full body pose re-
construction with sparse sensing from graph perspective.
The task is viewed as predicting missing nodes in an es-
tablished graph.
• We propose a framework to reconstruct full body motions
via Body Pose Graph (BPG). Motion features with tem-
poral and spatial properties are generated and assigned
arXiv:2401.11783v1  [cs.CV]  22 Jan 2024
Feature Integration
Interactive learning
P
F
Truck motion
Limb motion
Node Property Generation
Temporal 
Pyramid
Temporal 
Pyramid
Node Feature Updating
Static 
skeleton 
edges
Dynamic
skeleton 
edges
Dynamic 
latent
edges
Body Pose Graph
A
F
P 
F c
A
F c
T
F
L
F



P 
exp
F
\



A 
exp
F
I






P 
A
exp
F
F
K
I
 

exp I F
e






A
P
exp
F
F
U
\ 
 
 
 
 
exp
e



T 
exp
I F






T
L
exp
F
F
U
\ 
 
 
 
 

expp
Figure 1: Illustration of our proposed structure. Inputs are sparse sensor position and rotational signals from VR system. Feature
Integration module integrates position feature and rotation feature with different physical properties with interactive learning. In
Node Property Generation module, motion temporal property is achieved through Temporal Pyramid module. To gain motion
spatial property, the limb motion feature is composed by trunk motion features and limb local motion features. The trunk and
limb feature then serve as initial node features in Body Pose Graph. In Node Feature Updating, graph convolution network with
different edges modeling different joint relations is applied to update nodes.
to be initial node features. Then a Graph Neural Net-
work with expressive edges is applied to updated nodes.
Full body motion sequence is generated from the node-
associated joint movements.
• Experiment results demonstrate that our framework
achieves state of the art performance on on full-body
avatar estimation from sparse inputs. Further analysis
shows the contribution of each component to the perfor-
mance improvement, especially in the lower body joints.
Related Work
Full-Body Motion Reconstruction From Sparse
Inputs
Primary researches on this area make attempts on fully-body
motion reconstruction from 6 IMU sensors on human body
(head, arms, pelvis and legs). (von Marcard et al. 2017) pro-
poses a joint optimization framework based on statistic body
models. (Huang et al. 2018) applies learning method BiRNN
with body models to do the estimation. (Yi, Zhou, and Xu
2021) proposes a multi-stage learning based method where
multiple subtasks and losses are designed to restrain pose
generation. (Yi et al. 2022) relies on physical models to re-
fine the poses generated from learning methods.
However, requiring six IMUs is still excessive, as they
are costly and logistically inconvenient to deploy. Recon-
structing full body motion from common VR system will
be more advantageous and flexible. (Ahuja et al. 2021) first
utilizes sparser inputs from current consumer-grade VR sys-
tems (with headset and hand controllers) to estimate full-
body motion. It makes full body motion reconstruction much
easier. However, it estimates poses based on matching from
a dataset with only 5 types of activities. It can hardly han-
dle diverse activities out of dataset. (Yang, Kim, and Lee
2021) proposes a Gated Recurrent Unit - based method to
estimate lower-body pose while achieving upper-body with
IK solver. It queries the confidence of lower-body motion
reconstruction especially when upper-body and lower-body
have weak correlations. Thus apart from sensor data from
VR devices, it also requires a sensor on human waist. (Dit-
tadi et al. 2021) proposes a VAE(Variance AutoEncoders)-
based method to generate poses from VR devices. However,
it assumes the directions of pelvis in each frame should be
the same. (Jiang et al. 2022) proposes a Transformer-based
structure to generate global orientation and local joint ori-
entations. Orientations will then be input to body models to
generate joint poses. In the domain of human body recon-
struction utilizing only head-mounted devices, several meth-
ods have also been developed and explored. (Li, Liu, and Wu
2023) estimates full-body human motions from only ego-
centric video for diverse scenarios. (Winkler, Won, and Ye
2022) proposes a reinforcement learning based framework
and, together with physical simulator, can generate vivid
leg motions even when the input is only the 6D transforma-
tions of the HMD. Despite these methods’ promising perfor-
mance, leveraging joint motion relations in the human body
can likely yield better results. Hence, our work introduces
this prior knowledge through a graph-based approach.
Graph Neural Networks
Graph Neural Networks (Kipf and Welling 2017)(Zhang and
Chen 2018) process data that can be represented as graph.
Nodes representations will be iteratively updated by mes-
sages passed from their neighbors. Typical message pass-
ing methods include convolution-based methods (Kipf and
Welling 2017) and attention-based (Veliˇckovi´c et al. 2018).
One research field related to sparsity is handling graphs with
missing nodes. (Chen et al. 2020) develops a distribution
match based GNN Transformer-like method for attribute-
missing graph. (Taguchi, Liu, and Murata 2018) introduces
Gaussian mixture model to represent missing data in Graph
Convolutional Network. (Jiang and Zhang 2020) utilizes a
partial message-passing method to transmit observed fea-
tures observed features in GCN-based model. (Rossi et al.
2022) handles missing features in graph by minimizing
Dirichlet energy and leading to a diffusion-type differential
equation on graph.
Although they handle the graphs missing data, however,
the graphs that these methods focus on have three char-
acteristics. First, the node features have great similitudes.
Also, node relations tend to be qualitative and simple. And
the downstream tasks (for example node prediction) do not
rely much on the accurate quantity of node features. A typi-
cal example will be graph in bibliographic data. While full-
body motion reconstruction from sparse inputs needs accu-
rate quantity node features. The joints also have their own
characteristics and the relations between them have motion
meanings. The methods mentioned above are unsuitable for
handling joint missing human joint graph.
Graph Neural Networks in Human Pose
Estimation
While there are currently no methods solving full-body mo-
tion estimation from sparse inputs utilizing Graph Neural
Networks (GNN), GNN, renowned for their heightened in-
terpretability, have found widespread application in tasks as-
sociated with human pose. For example, action recognition
(Sofianos et al. 2021; Liu et al. 2020c; Cheng et al. 2020b,a;
Chen et al. 2021b; Si et al. 2019; Zhang, Xu, and Tao 2020;
Li et al. 2019; Shi et al. 2019b; Chen et al. 2021a; Shi et al.
2019a; Duan et al. 2022; Shi et al. 2020; Ye et al. 2019) and
3D human pose estimation from 2D (Azizi et al. 2022; Liu
et al. 2020b,a; Kundu et al. 2021; Zhao et al. 2019; Zou and
Tang 2021; Li et al. 2020; Liu, Zou, and Tang 2020).
(Sofianos et al. 2021) is the first work that applies GNN
in action recognition. After that, various researches have
been proposed on GNN-based method in action recogni-
tion. Some focus on improving the graph structure itself.
For example, (Cheng et al. 2020b) proposes shift operations
and lightweight point-wise convolutions to provide flexible
receptive field for graph. (Si et al. 2019) integrates GNN
with attention mechanism and lstm to increase representa-
tion ability. Some focus on empowering node relations to be
more expressive. (Shi et al. 2019b) generates graph edges
with directions based on kinematics while (Shi et al. 2020;
Ye et al. 2019) generates edges containing temporal infor-
mation or spatial information by learning methods.
Different from action recognition tasks which focus on
pose classification, 3D human pose estimation aims to re-
duce the estimation errors on all joints in pose. In order
to solve this harder task, various methods focus on more
powerful graph structure and more contextual edges. The
more powerful graph includes graph in Non-euclidean space
(Azizi et al. 2022), hypergraph neural networks (Liu et al.
2020b), and so on. More advanced graph updating methods
are also proposed, for example (Liu et al. 2020a; Kundu et al.
2021; Zhao et al. 2019; Zou and Tang 2021; Yan, Xiong,
and Lin 2018). Also, more human priors are utilized. For
example, (Li et al. 2020) establishes dynamic GNN based
on human motion prediction. (Liu, Zou, and Tang 2020) re-
veals the importance of decoupling global information from
joints. (Zeng et al. 2021), (Lee and KIM 2022) analyzes the
multi-hop relations between human graph nodes and models
then in updating methods.
The various methods mentioned above motivate us to in-
troduce graph in fully-body pose estimation from sparse in-
puts. However, these methods mainly focus on tasks where
sparsity hardly exists. Features for almost all joints are sup-
plied as input. While our task only provides sensor data on 3
joints as inputs and expects accurate estimation in 22 joints.
Above mentioned methods in related human pose methods
can hardly be applied to our task.
Methods
In this section, we first formalize the process of full body
motion reconstruction with sparse sensing and understand
the task from graph perspective. Then building process of
BPG is introduced. After node initialization, BPG is updated
referring to several joint relations and all joint motions are
generated.
Problem Formulation
This work focuses on full-body motion reconstruction with
measurements from one headset and two hand controllers,
a common configuration in commercial VR device. The in-
puts are cartesian coordinates p1×3 and orientations in axis-
angle representation Φ1×3 of headset and hand controllers.
The outputs are local rotation angles between joints and their
parent joints θ. considering real-time requirements in appli-
cation scenarios, the issue is formalized an online problem:
θ1:F
N = f

{pw, Φw}1:S
(N-K):N

,
(1)
in which S = 3 corresponds to the number of joints
tracked by the VR system, F = 22 is the number of joints
used to represent the full-body motion. The movements of
joints in current frame (N) are generated from sensor data
in previous K frames ((N-K):N). The final full human body
can be rendered from the outputs θ with human body model.
From graph perspective, we view the full-body as a graph
with 22 nodes. For N-th frame full-body motion reconstruc-
tion, motions of 3 nodes in graph are known and used. They
are the positional and angular motions of head and two
hands in previous K frames. Thus the task is transformed
to be completion of the missing 19 nodes in graph. Feature
Integration module and Node Property Generation module
extract various features from movement sequences of three
known nodes, assign features to different nodes and endow
these node features with characteristics corresponding to the
joints properties.
Feature Integration module integrates different sensor sig-
nals as normalization. Node Property Generation module
generates features with temporal and spatial properties for
nodes as initial value. In Node Feature Updating module, the
node features are updated by GCN with expressive edges.
Node Feature Initialization
Given the constraints of a limited number of known nodes
and the valuable information associated with them, the
sparse sensor data from sensors undergoes an abstraction
process. This process leads to the extraction of features re-
lated to the whole body motions. Subsequently, these ex-
tracted features are assigned to all nodes, thus serving as the
initialization for the graph structure.
Feature Integration
The angular measurements and po-
sitional measurements have totally different distribution and
follows different math laws for transformation, we propose
Feature Integration module to fuse them.
The measurements from each VR system device are joint
position p ∈ R1×3 and joint angular representation θ ∈
R1×6 (elements in rotation matrix R3×3 first row and sec-
ond row). We augment the features by differentiate the mea-
surements and get joint velocity v ∈ v1×3 and joint angular
velocity representation ωt ∈ R1×6(elements in rotation ve-
locity matrix R−1
t−1Rt first row and second row ). Joint posi-
tion and joint velocity compose translation feature FPsensor .
Joint angular representation and joint angular velocity rep-
resentation compose joint rotation feature FAsensor .
F S×6
Psensor =


(p1
t, v1
t )1×6
. . .
(ps
t, vs
t)1×6


(2)
F S×12
Asensor =


(θ1
t , ω1
t )1×12
. . .
(θs
t , ωs
t )1×12


(3)
FPsensor and FAsensor describes the joint motion from
different perspective and shares totally different geometric
properties. Thus, to enhance the feature representation abil-
ity and eliminate feature geometric differences, we utilize
dual interactive learning to generate new feature referring to
(Zhu et al. 2020).
F ′
P = FP ⊙ exp (ϕ (FA)) − ρ (FA ⊙ exp (ψ (FP )))
F ′
A = FA ⊙ exp (ψ (FP )) + η (FP ⊙ exp (ϕ (FA))) (4)
ψ, ρ are 1D convolutional layers. exp is used to map dif-
ferent features onto similar distribution spaces. The Feature
Integration module is applied to integrate the motion infor-
mation of nodes and output fused node features that incor-
porate both positional and angular information.
Node Temporal Motion Property Generation
As stated
in the Problem Formulation, the framework’s input com-
prises motion information from k preceding frames, while
the output entails the motion of the current frame. Thus this
module is designed to mitigate temporal disparities in fea-
tures originating from different frames. Also, As highlighted
in (Martinez, Black, and Romero 2017), motion continuity
stands out as a distinct characteristic of human motion. Mo-
tion details within each frame can be inferred from the sur-
rounding contextual frames (clip). In this section, we apply
the modeling of motion continuity as a guidance to mitigate
temporal disparities.
For node temporal properties, to better capture the joint
motion temporal properties, we design Temporal Pyramid
ĊĊ
ĊĊ
ĊĊ
ĊĊ
Feature Extractor
Feature Extractor
Previous K-frame Sequence
K/2-frame Clip Level 
 Feature Sequence
Feature Extractor
K/2-frame Frame Level 
 Feature Sequence
K/2-Frames Interleaved 
Concatenated Feature Sequence
Current Frame Feature
Figure 2: Temporal Pyramid Structure
Structure in Figure 2. The inputs are k previous frame fea-
tures and the outputs are high dimensional temporal features
of current frame. The feature extractor is based on SCI-
Block (Liu et al. 2022), which is a CNN-based time series
model with output dimension adjustable. In Temporal Pyra-
mid Structure, three Feature extractor are applied. First one
and second one extract frame level and clip level features.
The two level features are concatenated in an interleaved
manner. The third extractor is applied to generate motion
features for current frame.
Node Spatial Motion Property Generation
Human mo-
tions in different joints have different spatial properties.
(Leteneur et al. 2013) claims the important impact of trunk
on human body motion and reveals the different property of
trunk and limb joints. As the human skeleton kinematic (Shi
et al. 2019b) (Hu et al. 2021) reveals, joints farther from the
center of the human body are always physically controlled
by an adjacent joint which is closer to the center. In this con-
text, limb joints act as child joints relative to trunk joints, re-
sulting in limb joint motions being composed of both local
limb joint movements and trunk joint motions. To address
the challenge of predicting joints located distantly from the
body’s center and to capture this directed control relation-
ship, we propose a unidirectional interactive learning ap-
proach. This method guides the extraction of limb motion
features by leveraging the guidance from trunk motion fea-
tures. This module’s mechanism is described as followed.
FL = FL ⊙ exp (ϕ (FT)) + ρ (FT ⊙ exp (ψ (FL)))
(5)
FT and FL are trunk motion features and limb motion
features generated by temporal pyramid separately. ϕ and
ψ are convolutional networks for generating sub-structure
level features. The interactive learning mechanism used here
is similar to Feature Integration module. The trunk and limb
motion features generated are then assigned to correspond-
ing trunk nodes (joint 0,1,2,3,4,5,6,9,12,13,14 in Figure 3)
and limb nodes (joint 7,8,10,11,15,16,17,18,19,20,21 in Fig-
ure 3) as initial features.
Node Feature Updating
Node Feature Updating aims to capture diverse joint rela-
tionships through a Graph Convolutional Network with ex-
pressive edges. In Section 1, we initially revisit the vanilla
12
15
17
19
21
9
6
3
0
18
20
16
13
10
11
1
2
4
5
7
8
14
Figure 3: Index of human body joints
Graph Convolutional Network, updating joint features con-
sidering the static human skeleton as edges. Subsequently,
Section 2 introduces Node Updating using a graph convolu-
tional network with expressive edges.
Vanilla GCN
We will first review the vanilla graph con-
volution network. Given a graph G = (V, E), it consists of
the nodes V and the edges E. We revisit a generic GCN layer
defined as follows:
X
′ = σ(WXA)
(6)
where A ∈ RN×N is an adjacency matrix with N nodes,
indicating the connections between nodes. In 2D-3D hu-
man pose estimation, a task predicts 3d coordinates from
pictures, the adjacency matrix is often established referring
to human skeleton. If there is a bone connection between
joint j and the joint i, then aij = 1. Otherwise, the value
will be set to zero aij = 0. We denote the input node
features as X ∈ RN×Cin. Each node corresponding to a
Cin-dimensional feature vector. The learnable weight ma-
trix W ∈ RCin×Cout is set to adjust feature’s dimension to
be expected. The σ(·) is common activation function.
Consider i-th node, the node feature of node i is Xi. Cor-
responding adjacency matrix slice will be Ai, with j-th ele-
ment being aij. Si represents the joints have bone connec-
tions with joint i. aij = 1 if j ∈ Si and aij = 0 if j ̸∈ Si.
The update of i-th node in vanilla GCN is expressed as:
X′
i = σ

X
j∈Si
WXjaij


(7)
Although lots of improvements has been made in past, the
Graph updating method updates each node synchronously,
which assumes that useful information is evenly distributed
and the confidence of joint features is at the same level.
Node Updating With Expressive Edges
Vanilla graph
convolution network has limited representation ability. It as-
sumes that features in each node is reliable and node can
be represented well by updates referring to constant graph
edges built by human skeletons. Also, other strong hidden
relationships among joint nodes exist and change with ac-
tions, these relations can hardly be modeled by vanilla GCN.
For example, when the human is running, there is a strong
relation between hand joint and foot joint. But when the hu-
man is sitting, there is no such strong relation. Consider-
ing above two limitations, we proposed GCN with multiple
kinds of edge learned. To be specific, the edges in graph are
dynamic corresponding to the current joint state instead of
being constant. When human action changes, the edges can
change simultaneously to better represent the node relations.
In our task, edges are represented as an adjacency matrix
Ah ∈ R22×22.
Ah = As + Al
(8)
As = Ass + Ads
(9)
As ∈ R22×22 is skeleton relation adjacency matrix, it de-
scribes the relations exist in human skeleton (to be specific,
all the edges drawn in Figure 3). Al ∈ R22×22 is latent re-
lation adjacency matrix, it describes potential links between
nodes (node links not exist in Figure 3 ). Static skeleton rela-
tion adjacency matrix Ass ∈ R22×22 is built referring to the
human skeleton of SMPL model. Joints connected in human
skeleton will have edges with non-zero constant value in the
corresponding place. Dynamic skeleton relation adjacency
matrix Ads ∈ R22×22 models the relations among joints
in skeleton. It is also built referring to human skeleton of
SMPL. However, the value of the edges will be determined
by the features of nodes in graph. The values in Ads and Al
are learned by MLP structure seperately.
A = W1ϕ(W0X + B0) + B1
(10)
in which, X ∈ Rb×nf, W0 ∈ Rh×nf, B0 ∈ Rh, W1 ∈
Ro×h, B1 ∈ Ro. X is joint feature. b is batch size, n is
number of nodes and f is the dimension of feature. h is the
dimension of the hidden layer. o is the dimension of output.
ϕ is the ReLU activation function.
The nodes are updated with above adjacency matrix. The
final output of BPG are the axis-angle of each joint, which,
togther with SMPL human model (Pavlakos et al. 2019), will
be referred to generate the position of each joint .
Training and Loss
The loss function is composed of rotational loss, positional
loss and bone symmetric loss.
Lfinal = Lrot + Lpos + Lbone
(11)
Lrot is absolute error loss on all joint axis-angles. Lpos
is absolute error loss on all joint positions. The accuracy
of axis-angle and position of each joint is both crucial for
full body reconstruction. (Jiang et al. 2022). Lbone is human
skeleton symmetric loss. It emphasises the relative position
relations among joints and introduce human body priors for
optimization.
Lbone =
X
il,jl,ir,jr
(|| ˆY pos
il
− ˆY pos
jl
||−|| ˆY pos
ir
− ˆY pos
jr ||) (12)
Methods
MPJRE
MPJPE
MPJVE
Final IK
16.77
18.09
59.24
CoolMoves
5.20
7.83
100.54
LoBSTr
10.69
9.02
44.97
AvatarPoser
3.21
4.18
29.40
AvatarPoser *
3.01
4.11
27.79
Our method *
2.49
3.34
22.84
Table 1: Performance Comparison among our method and
baselines in AMASS dataset. Notice that * means the results
are trained in our machine.
Here, ˆY pos
i
represents the predicted position of joint i.
(il, jl) ∈ setr are right human skeleton bones shown as blue
lines in Figure 3, (ir, jr) ∈ setl are left human skeleton
bones shown as red lines in Figure 3.
Experiemnt
Data Preparation and Evaluation Metrics
CMU (Lab 2000), BMLrub (Troje 2002) and HDM05
(M¨uller et al. 2007) in AMASS (Mahmood et al. 2019)
dataset are employed . The datasets are randomly partitioned
into training and testing subsets, comprising 90% and 10%
of the data respectively, following the same setting as (Jiang
et al. 2022).
The metrics utilized for overall performance comparison
are MPJRE (Mean Per Joint Rotation Error [◦] ), MPJPE
(Mean Per Joint Position Error [cm] ), and MPJVE (Mean
Per Joint Velocity Error [cm/s] ). In ablation study, to reveal
the effect of each component on motion reconstruction, we
list the estimated position error on each lower body joint.
Performance Comparison With Baseline Method
We compare our method with baseline methods in Table
1. To be specific, there are Final IK, CoolMoves(Ahuja
et al. 2021), LoBSTr (Yang, Kim, and Lee 2021), VAE-
HMD (Dittadi et al. 2021), AvatarPoser (Jiang et al. 2022).
The results are referring to (Jiang et al. 2022). To be fair,
we retrained AvatarPoser on our platform. Our method at-
tains superior results across all three metrics, outperforming
all other methods. By representing human body as Graph
and modeling spatial-temporal relations among joints, our
method surpasses than baseline method, notably in predict-
ing unseen lower body joints, as illustrated in Table 3.
Performance Comparison With Offline Method
Offline methods refer to methods outputting n length hu-
man pose sequence instead single frame in each inference.
AGRoL (Du et al. 2023) is the state-of-art Offline method.
In our method, we use 41 frame sensor sequence as input
and ouput 1 frame in each inference. In Table 2, AGRoL41
represents that the lengths of input sequence and output se-
quence are both 41. Thanks to the feature generation method
and graph based architecture, which are special designed for
human body, our method performs better than AGRoL in
Methods
MPJRE
MPJPE
MPJVE
VAE-HMD
4.11
6.83
37.99
AGRoL 41
2.59
3.64
23.24
AGRoL 196
2.66
3.71
18.59
Our method
2.49
3.34
22.84
Table 2: Performance Comparison with offline methods
Index
AvatarPoser
Our method
Improvement
Joint 1
3.8
3.1
18.84%
Joint 2
3.8
3.2
15.79%
Joint 4
6.9
5.5
20.29%
Joint 5
6.9
5.5
20.29%
Joint 7
10.1
7.9
21.78%
Joint 8
10.1
8.0
20.80%
Joint 10
10.8
8.4
22.22%
Joint 11
11.0
8.7
20.91%
All Joints
4.11
3.34
18.73%
Table 3: Joint Position Error performance comparison be-
tween our method and AvatarPoser in lower body joints [cm]
all criteria in same condition. When extending the output
sequence length to 192, AGRoL demonstrates commend-
able performance in MPJVE metric. However, this enhance-
ment of the MPJVE metrics did not translate into supe-
rior results for the MPJRE and MPJPE metrics, which are
more important for the restructure task. In contrast, our pro-
posed methodology exhibits superior performance in both
the MPJRE and MPJPE metrics, further substantiating its
efficacy.
Ablation Study
To dissect individual component functions, we conduct ab-
lation studies across various cases. Findings are outlined in
Table 4. Given our method’s targeted focus on mitigating
substantial estimation errors in lower body joints, we em-
ploy MPJPE-lower-body (MPJPE on joints 1, 2, 4, 5, 7, 8,
10, 11) to directly exemplify performance.
• No Bone Symmetric Loss: The bone symmetric loss is
not utilized in the framework.
• No Spatial Property: The node features are not generated
separately for trunk joints and limb joints and the rela-
tions between trunk and limb is not considered.
• No Temporal Property: The temporal pyramid structure
is replaced by temporal feature extractor.
• No Feature Initialization: The Feature Integration pro-
cess is replaced by simple MLP structure.
• Vanilla GCN: The nodes in BPG are updated through
Vanilla GCN instead of the GCN with expressive edges.
As evident from Table 4, the absence of modules induces
notable performance declines, particularly in the lower body
region. This proves the efficacy of each component.
Figure 4: Visualization of estimated poses on an avatar involves a series of frames portraying a human front kick action. It
encompasses three rows: the top row showcases avatars with ground truth (GT) poses, while the subsequent two rows display
avatars generated by our approach and AvatarPoser. These avatars are color-coded to denote errors in each mesh.
Figure 5: Left diagram depicts a 0-1 adjacency matrix repre-
sentation of the skeletal connectivity within the human body.
Conversely, right diagram showcases an adjacency matrix
generated by the GCN with expressive edges. The deeper the
color, the stronger the relationship between the nodes. Red
indicates positive correlation, while blue indicates negative
correlation.
Configuration
MPJPE
MPJPE-lower-body
No Bone Symmetric Loss
3.53
6.75
No Spatial Property
3.60
6.88
No Temporal Property
3.71
7.10
No Feature Initialization
3.53
6.80
Vanilla GCN
3.58
6.88
Default
3.34
6.29
Table 4: Ablation study
Visualization of Estimated Pose on Avatar
In order to better analyze the estimation performance, we vi-
sualize the estimated poses on the whole avatar in figure 4.
Each mesh triangle in avatar is rendered referring to the error
of each estimated mesh vertex. Red represents large mesh
vertex estimation error. The avatars in the first row show the
ground truth poses. The avatars in second row and the third
row are generated by our proposed method and baseline.
As can be seen, the avatar generated by our method accom-
plishes the whole process of lifting and lowering the leg with
little mesh error while the one generated by AvatarPoser ac-
complishes the action with errors and stiffness. Especially in
frames (5) (6) (7) (8) (9), avatars generated by AvatarPoser
can hardly even raise left leg as high as the ground truth.
Analysis of Expressive Edges
As shown in Figure 5, the adjacency matrix generated by
GCN with expressive edges (right-hand figure) shows more
joint relations than the static 0-1 adjacency matrix gener-
ated from the quantification of the human skeletal structure
(left-hand figure). This indicated that, benefiting from the
potent expressive capabilities of of GCN with expressive
edges, our approach has yielded more comprehensive joint
relationships in than human skeleton.
Conclusion
In this study, we approach the task of full-body motion
reconstruction from sparse sensor input through a graph-
based perspective, introducing the Body Pose Graph to rep-
resent the human body. In Node Feature Initialization step,
different kind of VR system device features are first inte-
grated. The new generated features are then processed to
achieve spatial properties and temporal properties of joint
motions before serving as initial node features in Body Pose
Graph. Temporal property is generated by Temporal Pyra-
mid Structure and Spatial property is generated referring to
joint motion spatial relations. In the Node Feature Updat-
ing stage, we employ GNN with expressive edges to up-
date node features within the Body Pose Graph. Our ap-
proach demonstrates exceptional estimation performance as
evidenced by comprehensive evaluations. Ablation studies
validate the effectiveness of individual components. Visu-
alizations of learned edges and estimated poses on avatars
provide insights into learned motion relationships and our
method’s prowess in mesh-scale representations.
References
Ahuja, K.; Ofek, E.; Gonzalez-Franco, M.; Holz, C.; and
Wilson, A. D. 2021. CoolMoves: User Motion Accentuation
in Virtual Reality. In Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies, volume 5,
1–23.
Azizi, N.; Possegger, H.; Rodol`a, E.; and Bischof, H. 2022.
3D Human Pose Estimation Using M¨obius Graph Convolu-
tional Networks. In Proceedings of the European Confer-
ence on Computer Vision, 160–178.
Chen, X.; Chen, S.; Yao, J.; Zheng, H.; Zhang, Y.; and
Tsang, I. W. 2020. Learning on Attribute-Missing Graph.
IEEE transactions on pattern analysis and machine intelli-
gence, 44(2): 740–757.
Chen, Y.; Zhang, Z.; Yuan, C.; Li, B.; Deng, Y.; and Hu, W.
2021a. Channel-Wise Topology Refinement Graph Convo-
lution for Skeleton-Based Action Recognition. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision, 13359–13368.
Chen, Z.; Li, S.; Yang, B.; Li, Q.; and Liu, H. 2021b.
Multi-scale spatial temporal graph convolutional network
for skeleton-based action recognition. In Proceedings of the
AAAI conference on artificial intelligence, volume 35, 1113–
1122.
Cheng, K.; Zhang, Y.; Cao, C.; Shi, L.; Cheng, J.; and Lu,
H. 2020a. Decoupling GCN with DropGraph Module for
Skeleton-Based Action Recognition. In Proceedings of the
European Conference on Computer Vision, 536–553.
Cheng, K.; Zhang, Y.; He, X.; Chen, W.; Cheng, J.; and
Lu, H. 2020b.
Skeleton-based action recognition with
shift graph convolutional network.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 183–192.
Company, R. 2018. Final IK. https://assetstore.unity.com/
packages/tools/animation/final-ik-14290.
Dittadi, A.; Dziadzio, S.; Cosker, D.; Lundell, B.; Cashman,
T.; and Shotton, J. 2021. Full-Body Motion from a Single
Head-Mounted Device: Generating SMPL Poses from Par-
tial Observations. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, 11687–11697.
Du, Y.; Kips, R.; Pumarola, A.; Starke, S.; Thabet, A.;
and Sanakoyeu, A. 2023. Avatars Grow Legs: Generating
Smooth Human Motion From Sparse Tracking Inputs With
Diffusion Model. In Proceedings of the IEEE conference on
computer vision and pattern recognition, 481–490.
Duan, H.; Zhao, Y.; Chen, K.; Lin, D.; and Dai, B. 2022.
Revisiting Skeleton-Based Action Recognition. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2969–2978.
Hu, W.; Zhang, C.; Zhan, F.; Zhang, L.; and Wong, T.-T.
2021. onditional Directed Graph Convolution for 3D Hu-
man Pose Estimation. In Proceedings of the 29th ACM In-
ternational Conference on Multimedia, 602–611.
Huang, Y.; Kaufmann, M.; Aksan, E.; Black, M. J.; Hilliges,
O.; and Pons-Moll, G. 2018. Deep inertial poser: Learning to
reconstruct human pose from sparse inertial measurements
in real time. ACM Transactions on Graphics (TOG), 37(6):
1–15.
Jiang, B.; and Zhang, Z. 2020.
Incomplete graph repre-
sentation and learning via partial graph neural networks.
arXiv:2003.10130.
Jiang, J.; Streli, P.; Qiu, H.; Fender, A.; Laich, L.; Snape,
P.; and Holz, C. 2022. AvatarPoser: Articulated Full-Body
Pose Tracking from Sparse Motion Sensing. In Proceedings
of the European Conference on Computer Vision, 443–460.
Kipf, T. N.; and Welling, M. 2017. Semi-supervised classi-
fication with graph convolutional networks. In International
Conference on Learning Representations.
Kundu, J. N.; Seth, S.; Jamkhandi, A.; YM, P.; Jampani, V.;
Chakraborty, A.; and R, V. B. 2021. Non-local Latent Re-
lation Distillation for Self-Adaptive 3D Human Pose Esti-
mation. In Advances in Neural Information Processing Sys-
tems, volume 34, 158–171.
Lab, C. G. 2000.
CMU Graphics Lab Motion Capture
Database. http://mocap.cs.cmu.edu/.
Lee, J. Y.; and KIM, I. 2022. Multi-hop Modulated Graph
Convolutional Networks for 3D Human Pose Estimation. In
British Machine Vision Conference.
Leteneur, S.; Simoneau, E.; Gillet, C.; Dessery, Y.; and Bar-
bier, F. 2013. Trunk’s natural inclination influences stance
limb kinetics, but not body kinematics, during gait initiation
in able men. PloS one, (1): e55256.
Li, J.; Liu, K.; and Wu, J. 2023.
Ego-Body Pose Esti-
mation via Ego-Head Pose Estimation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 17142–17151.
Li, M.; Chen, S.; Chen, X.; Zhang, Y.; Wang, Y.; and Tian,
Q. 2019. Actional-structural graph convolutional networks
for skeleton-based action recognition.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 3595–3603.
Li, M.; Chen, S.; Zhao, Y.; Zhang, Y.; Wang, Y.; and Tian,
Q. 2020. Dynamic Multiscale Graph Neural Networks for
3D Skeleton Based Human Motion Prediction. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 214–223.
Liu, K.; Ding, R.; Zou, Z.; Wang, L.; and Tang, W. 2020a.
A Comprehensive Study of Weight Sharing in Graph Net-
works for 3D Human Pose Estimation. In Proceedings of
the European Conference on Computer Vision, 318–334.
Liu, K.; Zou, Z.; and Tang, W. 2020. Learning Global Pose
Features in Graph Convolutional Networks for 3D Human
Pose Estimation. In Proceedings of the Asian Conference
on Computer Vision, 1429–1442.
Liu, M.; Zeng, A.; Chen, M.; Xu, Z.; Lai, Q.; Ma, L.; and
Xu, Q. 2022. SCINet: Time Series Modeling and Forecast-
ing with Sample Convolution and Interaction. In Advances
in Neural Information Processing Systems, 5816–5828.
Liu, S.; Lv, P.; Zhang, Y.; Fu, J.; Cheng, J.; Li, W.; Zhou, B.;
and Xu, M. 2020b. Semi-Dynamic Hypergraph Neural Net-
work for 3D Pose Estimation. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence, 782–788.
Liu, Z.; Zhang, H.; Chen, Z.; Wang, Z.; and Ouyang, W.
2020c. Disentangling and unifying graph convolutions for
skeleton-based action recognition.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 143–152.
Mahmood, N.; Ghorbani, N.; Troje, N. F.; Pons-Moll, G.;
and Black, M. J. 2019. AMASS: Archive of motion capture
as surface shapes. Proceedings of the IEEE/CVF interna-
tional conference on computer vision, 5442–5451.
Martinez, J.; Black, M. J.; and Romero, J. 2017. On hu-
man motion prediction using recurrent neural networks. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, 2891–2900.
M¨uller, M.; R¨oder, T.; Clausen, M.; Eberhardt, B.; Kr¨uger,
B.; and Weber, A. 2007. Documentation Mocap Database
HDM05. Technical Report CG-2007-2, Institut f¨ur Infor-
matik II, Universit¨at Bonn.
Pavlakos, G.; Choutas, V.; Ghorbani, N.; Bolkart, T.; Osman,
A. A. A.; Tzionas, D.; and Black, M. J. 2019. Expressive
body capture: 3d hands, face, and body from a single image.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, 10975–10985.
Rossi, E.; Kenlay, H.; Gorinova, M. I.; Chamberlain, B. P.;
Dong, X.; and Bronstein, M. M. 2022. On the Unreasonable
Effectiveness of Feature Propagation in Learning on Graphs
With Missing Node Features. In Learning on Graphs Con-
ference, volume 198, 11:1–11:16.
Shi, L.; Zhang, Y.; Cheng, J.; and Lu, H. 2019a. Two-Stream
Adaptive Graph Convolutional Networks for Skeleton-
Based Action Recognition. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
12026–12035.
Shi, L.; Zhang, Y.; Cheng, J.; and Lu, H. 2019b. Skeleton-
based action recognition with directed graph neural net-
works.
In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 7912–7921.
Shi, L.; Zhang, Y.; Cheng, J.; and Lu, H. 2020. Skeleton-
Based Action Recognition With Multi-Stream Adaptive
Graph Convolutional Networks. IEEE Transactions on Im-
age Processing, 29: 9532–9545.
Si, C.; Chen, W.; Wang, W.; Wang, L.; and Tan, T. 2019.
An attention enhanced graph convolutional lstm network
for skeleton-based action recognition.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 1227–1236.
Sofianos, T.; Sampieri, A.; Franco, L.; and Galasso, F. 2021.
Space-time-separable graph convolutional network for pose
forecasting. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, 11209–11218.
Taguchi, H.; Liu, X.; and Murata, T. 2018. Graph convolu-
tional networks for graphs containing missing features. Fu-
ture Generation Computer Systems, 117: 155–168.
Troje, N. F. 2002.
Decomposing biological motion: A
framework for analysis and synthesis of human gait patterns.
Journal of vision, 2(5): 2–2.
Veliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Li`o,
P.; and Bengio, Y. 2018.
Graph Attention Networks.
In
International Conference on Learning Representations.
von Marcard, T.; Rosenhahn, B.; Black, M. J.; and Pons-
Moll, G. 2017. Sparse Inertial Poser: Automatic 3D Human
Pose Estimation from Sparse IMUs. Computer graphics fo-
rum, 36(2): 349–360.
Winkler, A.; Won, J.; and Ye, Y. 2022.
QuestSim: Hu-
man Motion Tracking from Sparse Sensors with Simulated
Avatars. In SIGGRAPH Asia 2022 Conference Papers, 1–8.
Yan, S.; Xiong, Y.; and Lin, D. 2018. Spatial temporal graph
convolutional networks for skeleton-based action recogni-
tion. In Proceedings of the AAAI conference on artificial
intelligence, volume 32.
Yang, D.; Kim, D.; and Lee, S.-H. 2021. Lobstr: Real-time
lower-body pose prediction from sparse upper-body tracking
signals. Computer Graphics Forum, 40(2): 265–275.
Ye, F.; Pu, S.; Zhong, Q.; Li, C.; Xie, D.; and Tang, H.
2019.
Dynamic gcn: Context-enriched topology learning
for skeleton-based action recognition. In Proceedings of the
28th ACM international conference on multimedia, 55–63.
Yi, X.; Zhou, Y.; Habermann, M.; Shimada, S.; Golyanik, V.;
Theobalt, C.; and Xu, F. 2022. Physical Inertial Poser (PIP):
Physics-aware Real-time Human Motion Tracking from
Sparse Inertial Sensors. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
13167–13178.
Yi, X.; Zhou, Y.; and Xu, F. 2021. TransPose: Real-time
3D Human Translation and Pose Estimation with Six Inertial
Sensors. ACM Transactions on Graphics (TOG), 40(4): 1–
13.
Zeng, A.; Sun, X.; Yang, L.; Zhao, N.; Liu, M.; and Xu, Q.
2021. Learning skeletal graph neural networks for hard 3d
pose estimation. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, 11436–11445.
Zhang, M.; and Chen, Y. 2018. Link Prediction Based on
Graph Neural Networks. In Advances in Neural Information
Processing Systems, 5165–5175.
Zhang, X.; Xu, C.; and Tao, D. 2020. Context aware graph
convolution for skeleton-based action recognition. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 14333–14342.
Zhao, L.; Peng, X.; Tian, Y.; Kapadia, M.; and Metaxas,
D. N. 2019. Semantic graph convolutional networks for 3d
human pose regression. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
3425–3435.
Zhu, S.; Pan, S.; Zhou, C.; Wu, J.; Cao, Y.; and Wang, B.
2020. Graph geometry interaction learning. In Advances in
Neural Information Processing Systems, volume 33, 7548–
7558.
Zou, Z.; and Tang, W. 2021. Modulated Graph Convolu-
tional Network for 3D Human Pose Estimation. In Proceed-
ings of the International Joint Conference on Artificial Intel-
ligence, 11477–11487.
"
"This research examines the impact of temporal interactions on the evolution of cooperative behaviors in social dilemmas. Traditional research on pairwise interactions has overlooked the effects of interaction frequency and synchronicity on cooperation. This study introduces two temporal interaction mechanisms that model the stochastic or periodic participation of individuals in games, recognizing the impact of exogenous temporal factors and geographical time differences. The results indicate that maximum game participation frequency is not optimal under a stochastic interaction mechanism; instead, an intermediate range of activation probability produces the highest cooperation level, especially under strong dilemma conditions. Local synchronization of interactions within specific areas is beneficial for cooperation because time differences hinder the spread of cross-structures but promote the formation of dense cooperative clusters with smoother boundaries.","Cooperation is an essential phenomenon for the success of biological ecosystems and human societies. Understanding its emergence and persistence is critical in addressing global challenges. However, cooperation often requires individual sacrifices for collective gain, leading to the tragedy of the commons. The Public Goods Game (PGG) is a fundamental paradigm for cooperation research, and in pairwise interactions, it can be simplified into the more specific framework of the Prisioner's Dillema Game (PDG). Evolutionary game theory has been bolstered by advancements in network science, statistical physics, and related fields, providing a robust mathematical framework for studying cooperation in social dilemmas. Complex structures, reputation, punishment, migration, and volunteering mechanisms have paved the way for cooperation to flourish.","nanResearch on evolutionary games on networks often assumes continuous interaction and participation in iterated games. However, in reality, connections between individuals are not always active, leading to intermittent interactions even when there are links. Accounting for this temporal activation of interactions, research in network science has extended to the time dimension and examines how the time-varying nature of the interaction topology influences the propensity of coupled systems to syncronize. Moreover, synchronization is crucial for maintaining cooperation. There have been efforts to incorporate temporal progression into evolving games, such as coevolution games, game transitions, stochastic interactions, etc. However, these works often ignore the temporal feature of interactions. For example, individuals with various circadian rhythms may have different periods of idle time, and nodes with different positions are rarely able to participate in games simultaneously due to jet lag. Thus, the question arises: How do the interaction frequency and synchronicity affect the evolution of cooperation in the spatial public goods game (SPGG)?nannan","This study combines settings of volunteering and stochastic interaction, proposing temporal interaction mechanisms to explore the effect of intermittent engagement on cooperation in group interactions. Individuals have two strategies and two states in PGGs: cooperation or defection, and active or inactive. Different states capture how the social enviornment or present physical factors affect individual participation, payoffs, and the study. The interaction state is a random variable that varies with time and space, obeying heterogeneous distribution. Despite inactive individuals taking free-rides on collective benefits, temporal interactions actually enhance the evolution of cooperation, and varying active patterns have different micro-mechanisms to promote cooperation. Simulations are performed according to Monte Carlo methods, and parameters such as multiplication factor, activation probability, and time lag are varied to explore their impact on cooperation levels.nan","The results show that temporal interaction mechanism promotes cooperation in SPGG, with a lower threshold for cooperators to defeat defectors compared to the null model. However, the fixed payoff of inactive individuals undermines cooperation and shrinks the area of the C+D phase. The interplay between multiplication factor and activation probability reveals a nonlinear effect, with an optimal range of activation probability  that evolves to an all C equilibrium with the smallest multiplication factor. Stochastic interaction features a negative correlation between multiplication factor and activation probability at low activation probability, but a positive correlation at frequent interactions. Periodic interaction has a slightly stronger positive effect on cooperation, showing a plateau in the cooperation level as activation probability increases. The temporal difference leads to the phenomenon of periodic shift of the active center at the macro level, with activity patterns similar among individuals in the same region. At the micro level, inactive neighbors reduce cross-like structures extended by active cooperators, leading to the formation of perfect C clusters, especially when activation probability is small.","Temporal interactions can enhance cooperation, and the level of cooperation is influenced by interaction patterns and activation probability. An intermediate region of activation probability provides a favorable environment for cooperators to thrive under stochastic interactions. With periodic interaction mechanism, synchronized interactions within a small region have a more positive effect on cooperation. The findings evidence temporal interaction as an alternative mechanism to promote cooperation and open new avenues for investigating the evolution of cooperation and other emergent behaviors from a time-varying dimension.",Impact of temporal interaction on the evolution of cooperation,"Yujie He, Tianyu Ren, Junjun Zheng, Huawen Liang","DRAFT MANUSCRIPT
1
Impact of temporal interaction on the evolution of
cooperation
Yujie He, Tianyu Ren, Junjun Zheng, Huawen Liang.
Abstract—This research investigates the impact of dynamic
interactions with time-varying topologies on the evolution of
cooperative behaviours in social dilemmas. Traditional research
has focused on deterministic rules governing pairwise interac-
tions, yet the impact of interaction frequency and synchronicity
on cooperation remains underexplored. Addressing this gap,
our work introduces two temporal interaction mechanisms to
model the stochastic or periodic participation of individuals in
these games, acknowledging real-life variances due to exogenous
temporal factors and geographical time differences. We consider
that the interaction state significantly influences both game payoff
calculations and the strategy updating process, offering new
insights into the emergence and sustainability of cooperation. Our
results indicate that maximum game participation frequency is
suboptimal under a stochastic interaction mechanism. Instead, an
intermediate region of activation probability yields the highest
cooperation level, especially under strong dilemma conditions.
This suggests that a balance between inactivity security and inter-
action frequency is crucial. Furthermore, local synchronization of
interactions within specific areas is shown to be beneficial, as time
differences hinder the spread of cross-structures but promote the
formation of dense cooperative clusters with smoother bound-
aries. Our findings provide an intuitive understanding of node-
based temporality and probabilistic interactions, contributing to
the broader discourse on resolving social dilemmas.
Index Terms—Temporal interactions, Time-varying network,
Public goods game, Evolutionary game theory.
I. INTRODUCTION
C
Ooperation is a fundamental pro-social behaviour es-
sential for the successful evolution of both biological
ecosystems and human societies [1]–[3]. The imperative to un-
derstand its origins and persistence has become more pressing
in the context of global challenges such as climate change [4]
and pandemics [5]. Notwithstanding, cooperation typically
requires individual sacrifices for collective gain. It presents
a dilemma where selfish individuals may be tempted to defect
and exploit the efforts of others, contributing to the tragedy of
the commons scenario [6]. To illustrate the contrast between
individual and collective optimal behaviours, the Public Goods
Game (PGG) serves as a fundamental paradigm in cooperation
research. Particularly in contexts where interactions occur
in pairwise patterns, the PGG can be simplified into the
more specific framework of the Prisoner’s Dilemma Game
(PDG) [7].
Y. He is with the Party School of the Guizhou Provincial Committee of
C.P.C, Guiyang 550025, China (e-mail: yujie he@whu.edu.cn)
T. Ren is with the Department of Computer Science, the University of
Manchester, Manchester M13 9PL, U.K. (email: tianyu.ren@manchester).
J. Zheng is with the Economics and Management school, the Wuhan
University, Wuhan 430072, China (email: 99zhengjunjun@163.com).
H. Liang is with the Department of Physics, University of Science and Tech-
nology of China, Hefei 230026, China (email: lhw233@mail.ustc.edu.cn).
In recent decades, the advancement of evolutionary game
theory (EGT) has been bolstered by developments in network
science, statistical physics, and other interconnected fields [8]–
[10]. These contributions have furnished a robust mathematical
framework essential for examining the emergence of coopera-
tion in social dilemmas. By using nodes to represent individu-
als and links to correspond to interactions, evolutionary games
on graphs have proven that certain population structures, such
as lattice [11], small-world [12], scale-free [13], etc., can sup-
port cooperation where well-mixed populations cannot [14],
[15], which is known as network reciprocity [16]. Besides
complex structures, numerous heuristic mechanisms based on
reputation [17]–[19], punishment [20]–[22],migration [23]–
[25], and volunteering [26]–[28] (to name but a few) have
also paved the way for cooperation to flourish. We recommend
referring to several surveys [10], [29]–[31] for a further
comprehensive understanding.
A common simplifying assumption in most research of evo-
lutionary games on networks is that individuals continuously
interact and participate in iterated games. However, in the
real world, connections between individuals are not always
in effect; instead, these pairwise interactions are activated
intermittently, even if there are links among neighboring
individuals [32]. Accounting for this temporal activation of
interactions, research in network science has extended to the
time dimension and discusses how the time-varying nature
of the interaction topology influences the propensity of the
coupled systems to synchronize [33], [34]. Furthermore, syn-
chronization is the basis for the emergence of coherent global
behaviors and also plays a crucial role in maintaining cooper-
ation. There have been efforts to incorporate a temporal pro-
gression in evolving games, such as coevolution games [35]–
[37], game transitions [38]–[40], stochastic interactions [41]–
[43], etc. Although these works discuss the type of games or
connections changing over time, to our knowledge, they ignore
the temporal feature of interactions. For example, individuals
with various circadian rhythms [44] may have different periods
of idle time between rounds of games; nodes with different
positions are rarely able to participate in games simultaneously
because of jet lag. Therefore, our work raises the question:
How do the interaction frequency and synchronicity affect
the evolution of cooperation in spatial public goods game
(SPGG)?
Previous studies have tried to discuss evolutionary games
with discontinuous interactions, but some research gaps re-
main to be explored. As presented in Refs.
[26]–[28], the
volunteering mechanism in SPGG represents that the so-called
loners refuse to participate in games by default in exchange
arXiv:2401.11782v1  [physics.soc-ph]  22 Jan 2024
DRAFT MANUSCRIPT
2
for a small but fixed income. However, it does not make
sense in real life that loners still receive benefits when there
are no collaborative contributions to public goods. According
to research on interaction stochasticity (Refs.
[41]–[43]),
individuals stochastically play PDGs with their direct neigh-
bors with certain probabilities. The intensity of interaction
is heterogeneous, and it only affects the game participation
between paired individuals but not includes strategy updates.
Here we combine the settings of volunteering and stochastic
interaction, proposing the temporal interaction mechanisms to
explore the effect of intermittent engagement on cooperation in
group interactions. Specifically, individuals have two strategies
and two states in PGGs: cooperation or defection, and active or
inactive. Different states capture how the social environment or
present physical affects individual participation, payoffs, and
study. The interaction state is a random variable that varies
with time and space, obeying heterogeneous distribution.
As we will show, despite inactive individuals take free-rides
on collective benefits, temporal interactions actually enhance
the evolution of cooperation and varying active patterns have
different micro-mechanisms to promote cooperation. The re-
mainder of the paper proceeds as follows. Section II describes
the model setting under stochastic and periodic interaction
mechanisms. Section III present in detail our findings of nu-
merical simulations and corresponding explanations. Section
IV contains the summary and discussions. An extended pair
approximation approach is provided in Appendix to theoreti-
cally analyzes the conditions facilitating cooperation.
II. MODEL
A. Public Goods Game
We consider the spatial public goods game on a L2 square
lattice with periodic boundary conditions (the degree k = 4 ).
Each individual occupies a node and interact within the von
Neumann neighborhood, i.e., G = k+1 participants in a PGG
group. Two strategies are available to choose: cooperation (C)
and defection (D). Specifically, cooperator invests a quantity c
to the common pool, while defector contribute nothing but take
a free ride on the public goods. Without the loss of generality,
we set the investment cost equal to one.
B. Temporal Interaction
In view of the situations where individuals are often absent
from games due to various temporal factors, here, we set
an exogenous time-dependent state variable ax(t) ∈ {0, 1}
to represent the interaction state of x at time t.Active in-
dividuals (ax(t) = 1) decides whether to invest in public
goods and alter strategies based on the payoff difference
with their neighbors. Conversely, for ax(t) = 0, inactive
individuals cannot participate in games and modify strategies,
but they can free-ride a small but fixed share σ of the public
resources, because public goods have non-excludability and
positive externalities, i.e., it is not possible to prevent those
who have not paid for them from having access to them.
For each PGG group, the total benefits only come from
the contributions of active cooperators, after deducting the
incomes of inactive individuals, the remainder is distributed
equally among all active individuals. Particularly, unlike the
setting of loners [27], inactive individuals get nothing when
there are no active cooperators.
Different activity patterns bring a certain degree of hetero-
geneity and randomness on the evolutionary time scale, which
may have different impacts on evolutionary dynamics. In this
work, two typical distributions are considered as activation
interaction rules, Fig. 1 schematize the individual activity
patterns. Wherein, the population activation rate is equal to the
number of activated individuals at time t divided by the total
number of the population. Stochastic interaction represents
(b)
(a)
(c)
Fig. 1. schematic of individual activity patterns under two temporal mecha-
nisms. (a) stochastic interaction, (b) periodic interaction. (c) is an example of
time lag effect.
situations that human behavior is random at any time, as
shown in Fig. 1(a). Set the probability that each individual
to become active in group interactions as p. For p ∈ [0, 1],
the number of activations Ab that any individual participates
in games during the whole simulation time T is subject to a
binomial distribution denoted by Ab ∼ B(T, p). p = 1 means
full interactions, and our model recovers to the null model —
the spatial public goods game — has been studied in detail in
[11]. Independent and identically distributed among different
individuals.
DRAFT MANUSCRIPT
3
Periodic interaction is a mode that considers human cir-
cadian rhythms and location, manifesting as periods of en-
gagement and periods of rest in Fig. 1(b). Let the number of
activations Ap for any individual in time [t, t + τ) obeys a
Poisson distribution with parameter λ , denoted Ap ∼ P(λ).
Here τ is a unit time such as a day or week, and the
active rate λ designates the average number of individual
participations per interval. The probability that Ap takes any
value is P(Ap = n) =
λne−λ
n!
, n = 0, 1, 2, .... Thus,
the average activation probability ¯p = λ/τ, and it is simply
represented by p in the following. In addition, time lag has a
strong effect on the interaction state of individuals in different
regions, resulting in heterogeneous distribution among differ-
ent individuals. We introduce a random variable ε that follows
a normal distribution to represent the time difference, denoted
ε ∼ N(µ, σ2). Arbitrarily chose a node in the network as a
criterion site, and denote its time as the simulation time. µ
is the mean value of ε and is proportional to the Euclidean
distance d between the individual and the criterion node, as
shown in Eq. 1
d(i, j) =
q
(xi − xj)2 + (yi − yj)2
(1)
To ensure that the time difference between the farthest indi-
viduals does not exceed the time interval, it should be satisfied
2µd(max) = τ. The local time of any agent equal to t − ε,
as the example given in Fig. 1(c). Note that ε is randomly
generated in each interval, so that interactions are dynamic
and heterogeneous.
C. Evolutionary dynamics
Simulations are performed according to Monte Carlo meth-
ods. Initially, each individual on site is randomly assigned a
strategy with equal probability, and all actively participate in
games. Then, a schedule of activation participation is respec-
tively generated for each individual based on the temporal
interaction rule, which determines the state of the individual
at any time in the evolutionary process. Especially, we take
τ = 24 , µ = 12
√
2L and σ = 2 in periodic interaction
to model the activities of people in a real day. After initial
settings, the game is iterated forward with the following
elementary steps.
Step 1 (Game Interaction):An individual x is randomly
selected as the focal player to play the PGGs taking place
on its site and all neighboring sites. In each group g, let
ic ( id) be the number of active cooperators (defectors) in
the neighborhood, and i = ic + id represents the number of
active neighbors. For ax + i ≤ 1, aligned with volunteering
mechanism, the game cannot occur so that the payoff for all
individuals is 0. For the other cases, the multiplication factor
r ∈ (1, G) denotes dilemma weakness, which amplified the
sum of cooperators’ contributions. An increase in r leads to
a lower dilemma strength. Accordingly, the payoff of active
cooperator (AC) and active defector (AD) in one game are
given as
πAC = r(ic + 1) − σ(k − i)
i + 1
− 1,
(2)
πAD = ric − σ(k − i)
i + 1
.
(3)
Inactive cooperator (IC) and inactive defector (ID) receive the
same payoff
πI =
(
σ, ic ≥ 1
0, ic = 0
,
(4)
where σ ∈ (0, r − 1) represents the inactivity payoff has a
lower performance than mutual cooperation (ic = 4) but better
than mutual defection (ic = 0) . The overall payoff at time
step t is accumulated from all the involved games, which is
given as Πx = PG
g=1 πg
x.
Step 2 (Update of Strategy and State):After playing
games, individuals update strategies asynchronously in a ran-
dom sequential order. First, individual x chooses one of its
neighbors y at random, and y acquires its payoff Πy in the
same way. Next, the strategy of y is adopted by x with the
probability calculated by the Fermi function [29]:
f(sx → sy) =
ax(t)
1 + exp[−(Πy − Πx)/K],
(5)
where K quantifies the noise intensity. If K → 0, higher-
payoffs individual could certainly enforce its strategy on the
neighbor with a lower payoff. Conversely, the case K → ∞
represents the limit of weak selection [45], where imitation
is random regardless of the differences in payoffs of both
players. As stipulated in previous studies, we set K = 0.1.
Thus, strategy with higher payoffs could be readily imitated,
but it is not impossible to adopt the strategy of a player
performing worse because of the noise. In addition, it is easy
to know from Eq. 5 that only active individuals can update
strategies. Since no actual interactions occur, the inactive ones’
strategies remain unchanged, but are possibly imitated. Finally,
individual x updates its state as ax(t + 1) according to the
schedule of activation participation to prepare for the next
round of game.
Repeating aforementioned steps L2 times until all individ-
uals have a chance to adjust their strategy and state once
on average, and it constitutes a full Monte Carlo simulation
(MCS).
III. RESULTS
In what follows, we will show the simulation results conduct
on a square lattice of size 10000. The key value ρc character-
izing the frequency of cooperators in population is computed
by averaging over the last 103 generations after the iteration of
more than 104 time steps. To ensure robustness and minimize
variability, the final steady states are obtained by conducting
up to 10 independent realizations.
We start by providing a comprehensive view of the sys-
tem transition phenomenon with multiplication factor r and
activation probability p. Fig. 2 presents the full r − p phase
diagrams of cooperation level ρc under the stochastic and pe-
riodic interaction mechanisms respectively. Overall, although
a portion of the investment returns in public goods has been
allocated to inactive individuals, the threshold for cooperators
to defeat defectors is significantly lower than the null model. In
DRAFT MANUSCRIPT
4
Fig. 2. r − p phase diagrams of the spatial public goods game as obtained for different inactive income σ = 0.5 (left column) and σ = 1 (right column).
Subfigures (a) and (b) represent stochastic interaction, (c) and (d) represent periodic interaction. The red (blue) line signify the phase transitions between the
mixed C+D and homogeneous D (C) states, and solid (dashed) lines signify the continuous (discontinuous) phase transitions. The inserted colormap (a-1) and
(b-1) of each panel shows the equilibrium fraction of cooperators with r and p. From red to blue, the color bar indicates that the cooperation level changes
from 0 to 1 accordingly. All results are obtained for K = 0.1 and t = 105.
other words, temporal interaction mechanism actually allows
cooperation to flourish. Moreover, for σ = 1, p = 0.5, the
proportion of defectors is lower with the same multiplier r
compared with volunteering [27], thus the group is more likely
to reach a steady state of global cooperation. However, the
fixed payoff σ of inactive individuals undermines cooperation.
As σ grows from 0.5 to 1, the transition lines between all
D, C+D and all C shrink, and the area of the C+D phase
decreases. Interestingly, the phase transitions between all C
and all D is discontinuous with small p, while at frequent
interactions the evolving system switches to the mixed C+D
state via a discontinuous phase transition.
Further, there exists a non-linear interplay between r and
p to promote cooperation, but different activity patterns have
dissimilar effects on the survival and prevalent thresholds of
cooperators. Fig. 2(a) and (b) depict the stochastic interaction
mechanism. On the one hand, r and p are negatively correlated
at low activation probability p < 0.5. As p decreases, the
threshold r that transforms the system from D to C state
gradually increases. Regardless of how r varies, cooperators
cannot survive when p < 0.1, suggesting that sufficient
interaction is the basis for cooperation.On the other hand, in
the case of frequent activation (p ≥ 0.5), the transition line
between all D and C+D phase is not sensitive to changes in
p. But the multiplier r required to reach all C phase increases
with p, showing a positive correlation. Thus, game interactions
are not as frequent as possible in case of stochastic interaction,
and there is an optimal activation probability p that evolves
to all C equilibrium with the smallest multiplication factor
r. Moreover, the positive effect of periodic interaction on
DRAFT MANUSCRIPT
5
cooperation is slightly stronger, as shown in Fig. 2 (c) and (d),
the areas of C+D and all C are larger than stochastic interac-
tion. Interestingly, the threshold for cooperative emergence is
independent of p, with all D threshold around r = 3.62 in
Fig. 2 (c) and r = 3.75 in (d). However, as the activation
probability p increased from 0.01 to 1, the threshold of r
required to reach all C state decreases and then increases. With
appropriate values of r, cooperation can emerge and prevail
even when p < 0.1 (Fig. 2(c)). This suggests that small-scale
activation is more likely to develop into global cooperation
due to spatio-temporal factors, which will later be specifically
analyzed in Fig. 6.
To quantify the role of stochastic interaction and find the
critical exponent for promoting cooperation more precisely,
we then study the dependence of the cooperation level ρc on
p for different values of r, as shown in Fig. 3. For fixed r,
the proportion of cooperators first rises and then falls as p
gradually increases from 0.1 to 1. Obviously, there exists an
optimal range of p, resulting in full cooperation. That means
only at an intermediate temporality can cooperators under
stochastic interaction mechanism outspread defectors, and the
similar results have been observed in [42], [46] studying
spatial PDGs, termed the Goldilocks effect of temporality.
When p exceeds this region, ρc monotonically decreases with
p, reaching the C+D mixed strategy equilibrium (r = 4 and
4.25). At fixed p and σ, ρc is higher at evolutionary equilibrium
with increasing r. Notably, the length of plateau increases with
r but decreases with σ. In Fig. 3(b) with σ = 1, the optimal
p-value to promote cooperation is significantly larger than the
case of σ = 0.5 in Fig. 3(a). This phenomenon is reflected in
the reality that when the basic security that each individual can
obtain from public goods is high, the public needs relatively
frequent interactions in order to reach a cooperative consensus.
In this condition, the difference in gains between individuals
is small and the probability of learning strategy calculated by
5 in each MSC is quiet low, thus only sufficient interactions
can sustain cooperation.
In order to intuitively understand the effect of stochastic
interaction on cooperation at the micro level, we visualize the
yellow curve in Fig. 3(a) for activation probability p = 0.6 and
plot some typical snapshots, see Fig. 4. Overall, the proportion
of cooperative strategy declines to a lower value and then
rises. Refer to [47], this process can be categorized into two
period: the enduring (END) period and the expanding (EXP).
The END period (about t ≤ 50) features a rapid downfall of
cooperation and the formation of large-scale defector clusters.
But cooperators can survive in the interstices of D clusters
because inactive individuals protect active ones from copying
defection. In the EXP period, the small C groups surrounded
by D clusters gradually expands, as observed in Fig.4 (d) to
(e). When individuals at the rough (concavo-convex) boundary
become active, defection is converted to cooperation since the
income within C clusters is significantly higher than that of D
clusters. But inactivity leads to a high barrier to the formation
of perfect C clusters, i.e., with smoother boundary and no
defectors inside, and the evolutionary trail is difficult to be
absorbed by all C state. As a result, cooperation level increases
quickly and then fluctuates around the C+D equilibrium.
(a)
(b)
Fig. 3.
The cooperation level ρc as a function of p for different values of
r under stochastic interaction mechanism. The results are obtained for (a)
σ = 0.5 and (b) σ = 1. The four curves in the subgraph correspond to
r=3.5,3.75,4 and 4.25, respectively.
Strategy fraction
Time
r=3.75
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 4. Evolutionary dynamics and spatial distributions of different strategies
and states under stochastic interaction mechanism. (a) depicts the fractions
of the two strategies change over time at r = 3.75, p = 0.6, σ = 0.5 and
t = 25000. (b-f) take the snapshots at five iteration times (t=1,50,250,500
and 10000). Dark(light) red and dark(light) blue represent the active(inactive)
cooperators and active(inactive) defectors respectively.
Recalling Fig. 2 (c) and (d), periodic interaction mechanism
enables cooperation to prevail with a small average active
probability. Fig.5 plots how ρc varies as a function of p
under different values of r and reveals that cooperation is
maximized at the combination of intermediate r and low p.
When r exceeds all D threshold in Fig.2, the proportion of
cooperation decreases from 1 with increasing p, reaching C+D
mixed equilibrium. Counterintuitively, inactivity payoff σ only
effect the evolution of cooperation but has no impact on the
final cooperation level. As r get larger, the downward trend of
ρc slows down and stabilizes at a higher level than in Fig.3.
One exception is the yellow line in Fig.5(b), which shows a
positive correlation between ρc and p, because r = 3.75 is the
critical value for cooperation evolution under σ = 1.
To further explore the reason why periodic interaction shows
the significant positive effect when activation frequency is
small, Fig.6 records snapshots of the spatial distribution at
different simulation times. As shown, the time difference leads
to the phenomenon of periodic shift of the active center at
the macro level, and the activity patterns are similar between
individuals located in the same region. When one region is
active, nearly all individuals within a certain region participate
in games, but others far away are inactive. Comparing Fig.6
(a-*) and (b-*), we find that higher p forms many C clusters of
small sizes with bumpy boundaries, while lower p produces
DRAFT MANUSCRIPT
6
(a)
(b)
Fig. 5.
The cooperation level ρc as a function of p for different values of
r under stochastic interaction mechanism. The results are obtained for (a)
σ = 0.5 and (b) σ = 1. The four curves in the subgraph correspond to
r=3.5,3.75,4 and 4.25, respectively.
(b-1)
(b-2)
(b-3)
(a-1)
(a-2)
(a-3)
(c-1)
(c-2)
(c-3)
(c-4)
(c-5)
Fig. 6.
The cooperation level ρc as a function of p for different values of
r under stochastic interaction mechanism. The results are obtained for (a)
σ = 0.5 and (b) σ = 1. The four curves in the subgraph correspond to
r=3.5,3.75,4 and 4.25, respectively.
several C clusters with smooth boundaries. The fraction of
cooperation decreases in the END period and rises up to 0.55
in Fig.6(a-1) and 1 in (b-1) during the EXP period. With
p = 0.1 , there are fewer strategy updating and it takes longer
to reach a stochastic stable equilibrium. At the period [t, t+24)
level, the trend of ρc is same as the population activity rate in
Fig.1(b), which is rising and then falling. Fig.6(c-*) depicts
this process of change in C clusters over a period at the
micro level. Within each active region, the individuals on
the boundary of the C clusters can expand outward in cross-
like structures because of the payoff settings, as shown in
Fig.6(c-1) to (c-3). However, as the active center shifts and
inactive neighbors increase, the active cooperators at the end
of the cross-branch are free-ridded by defectors. As a result,
they decrease in number (Fig.6(c-4)), and shrink into larger
and compact C clusters with smooth boundaries (Fig.6(c-5)),
which can better resist defection the next time the region
is active. Therefore, frequent interactions are detrimental to
cooperation under periodic interaction mechanism, but sparse
activity contributes to the formation and integration of perfect
C clusters. In Fig.6(b-2) and (b-3), these compact C clusters
gradually connected and surrounded the D clusters, and finally
reach all C equilibrium.
IV. CONCLUSION
In the real society, individuals have different active patterns,
sometimes active and sometimes resting. Due to diversity
of biological clocks and geographic locations, those in dif-
ferent states rarely interact at the same time. In this sense,
the actual partnership networks are dynamical in nature, i.e.
time-varying ones. Here, we have proposed two temporal
mechanisms for people to participate in spatial public goods
games stochastically or periodically. Individual state affects
not only the payoffs from the game, but also the strategy
updating process. Besides, considering the non-exclusive na-
ture of public goods, we constrain the collective benefits
sharing rule that inactive individuals can free-ride only if
there are contributions from active cooperators. The results
have indicated that temporal interaction can largely promote
cooperation, and the enhancement of the cooperation level is
highly influenced by interaction patterns and the activation
probability. Interestingly, the transition line of C+D→C is
quasiconcave and there exists an optimal combination of
p and r resulting in massive cooperation. Under stochastic
interactions, we find an intermediate region of p provides a
favorable environment for cooperators to thrive. That is to
say, the frequency of games is not always better, appropriate
absences are more likely to promote cooperation. However,
with periodic interaction mechanism, the synchronized inter-
actions within a small region have more positive effect on
the evolution of cooperation. From a macro perspective, the
time difference causes the shift of active center and impedes
to the expansion of cooperation to inactive regions. But from
a micro level, the inactivity of neighbors reduces cross-like
structures extended by active cooperators, thus it is more
conducive to form the perfect C clusters, especially when p is
small. In summary, we have studies how interaction frequency
and synchronicity influence the evolution of cooperation in
spatial public PGG. By means of Monte Carlo simulations
and theoretical calculations, it was shown that interactions
need appropriate frequency and local synchrony to maintain
cooperation. Our work may evidence temporal interaction as
an alternative mechanism to enhance cooperation, and open a
new avenue for investigating the evolution of cooperation and
other emergent behaviors from a time-varying dimension.
REFERENCES
[1] A. S. Griffin and S. A. West, “Kin discrimination and the benefit of
helping in cooperatively breeding vertebrates,” Science, vol. 302, no.
5645, pp. 634–636, 2003.
[2] E. Fehr and U. Fischbacher, “The nature of human altruism,” Nature,
vol. 425, no. 6960, pp. 785–791, 2003.
[3] M. Nowak and R. Highfield, Supercooperators: Altruism, evolution, and
why we need each other to succeed.
Simon and Schuster, 2011.
[4] O. Hoegh-Guldberg, D. Jacob, M. Taylor, T. Guill´en Bola˜nos, M. Bindi,
S. Brown, I. A. Camilloni, A. Diedhiou, R. Djalante, K. Ebi et al., “The
human imperative of stabilizing global climate change at 1.5 c,” Science,
vol. 365, no. 6459, p. eaaw6974, 2019.
DRAFT MANUSCRIPT
7
[5] J. J. V. Bavel, K. Baicker, P. S. Boggio, V. Capraro, A. Cichocka,
M. Cikara, M. J. Crockett, A. J. Crum, K. M. Douglas, J. N. Druckman
et al., “Using social and behavioural science to support covid-19
pandemic response,” Nature human behaviour, vol. 4, no. 5, pp. 460–
471, 2020.
[6] G. Hardin, “The tragedy of the commons: the population problem has
no technical solution; it requires a fundamental extension in morality.”
science, vol. 162, no. 3859, pp. 1243–1248, 1968.
[7] J. Tanimoto, “Difference of reciprocity effect in two coevolutionary
models of presumed two-player and multiplayer games,” Physical Re-
view E, vol. 87, no. 6, p. 062136, 2013.
[8] J. M. Smith, “Evolution and the theory of games,” in Did Darwin get
it right? Essays on games, sex and evolution.
Springer, 1982, pp.
202–215.
[9] D. Y. Kenett, M. Perc, and S. Boccaletti, “Networks of networks–an
introduction,” Chaos, Solitons & Fractals, vol. 80, pp. 1–6, 2015.
[10] M. Perc, J. J. Jordan, D. G. Rand, Z. Wang, S. Boccaletti, and A. Szol-
noki, “Statistical physics of human cooperation,” Physics Reports, vol.
687, pp. 1–51, 2017.
[11] A. Szolnoki, M. Perc, and G. Szab´o, “Topology-independent impact of
noise on cooperation in spatial public goods games,” Physical Review
E, vol. 80, no. 5, p. 056109, 2009.
[12] G. Abramson and M. Kuperman, “Social games in a social network,”
Physical Review E, vol. 63, no. 3, p. 030901, 2001.
[13] F. C. Santos, M. D. Santos, and J. M. Pacheco, “Social diversity
promotes the emergence of cooperation in public goods games,” Nature,
vol. 454, no. 7201, pp. 213–216, 2008.
[14] M. A. Nowak, C. E. Tarnita, and T. Antal, “Evolutionary dynamics in
structured populations,” Philosophical Transactions of the Royal Society
B: Biological Sciences, vol. 365, no. 1537, pp. 19–30, 2010.
[15] B. Allen, G. Lippner, Y.-T. Chen, B. Fotouhi, N. Momeni, S.-T. Yau,
and M. A. Nowak, “Evolutionary dynamics on any population structure,”
Nature, vol. 544, no. 7649, pp. 227–230, 2017.
[16] M. A. Nowak, “Five rules for the evolution of cooperation,” science,
vol. 314, no. 5805, pp. 1560–1563, 2006.
[17] M. Milinski, D. Semmann, and H.-J. Krambeck, “Reputation helps solve
the ‘tragedy of the commons’,” Nature, vol. 415, no. 6870, pp. 424–426,
2002.
[18] F. Fu, C. Hauert, M. A. Nowak, and L. Wang, “Reputation-based partner
choice promotes cooperation in social networks,” Physical Review E,
vol. 78, no. 2, p. 026117, 2008.
[19] T. Ren and X.-J. Zeng, “Reputation-based interaction promotes cooper-
ation with reinforcement learning,” IEEE Transactions on Evolutionary
Computation, 2023.
[20] A. Szolnoki, G. Szab´o, and M. Perc, “Phase diagrams for the spatial
public goods game with pool punishment,” Physical Review E, vol. 83,
no. 3, p. 036101, 2011.
[21] D. G. Rand and M. A. Nowak, “The evolution of antisocial punishment
in optional public goods games,” Nature communications, vol. 2, no. 1,
p. 434, 2011.
[22] A. Szolnoki and M. Perc, “Effectiveness of conditional punishment for
the evolution of public cooperation,” Journal of theoretical biology, vol.
325, pp. 34–41, 2013.
[23] A. Cavagna, A. Cimarelli, I. Giardina, G. Parisi, R. Santagati, F. Ste-
fanini, and M. Viale, “Scale-free correlations in starling flocks,” Pro-
ceedings of the National Academy of Sciences, vol. 107, no. 26, pp.
11 865–11 870, 2010.
[24] T. Ren and J. Zheng, “Evolutionary dynamics in the spatial public goods
game with tolerance-based expulsion and cooperation,” Chaos, Solitons
& Fractals, vol. 151, p. 111241, 2021.
[25] A. K. Fahimipour, F. Zeng, M. Homer, A. Traulsen, S. A. Levin, and
T. Gross, “Sharp thresholds limit the benefit of defector avoidance in
cooperation on networks,” Proceedings of the National Academy of
Sciences, vol. 119, no. 33, p. e2120120119, 2022.
[26] C. Hauert, S. De Monte, J. Hofbauer, and K. Sigmund, “Volunteering as
red queen mechanism for cooperation in public goods games,” Science,
vol. 296, no. 5570, pp. 1129–1132, 2002.
[27] G. Szab´o and C. Hauert, “Phase transitions and volunteering in spatial
public goods games,” Physical review letters, vol. 89, no. 11, p. 118101,
2002.
[28] D. Semmann, H.-J. Krambeck, and M. Milinski, “Volunteering leads
to rock–paper–scissors dynamics in a public goods game,” Nature, vol.
425, no. 6956, pp. 390–393, 2003.
[29] G. Szab´o and G. Fath, “Evolutionary games on graphs,” Physics reports,
vol. 446, no. 4-6, pp. 97–216, 2007.
[30] M. Perc and A. Szolnoki, “Coevolutionary games—a mini review,”
BioSystems, vol. 99, no. 2, pp. 109–125, 2010.
[31] M. Perc, J. G´omez-Gardenes, A. Szolnoki, L. M. Flor´ıa, and Y. Moreno,
“Evolutionary dynamics of group interactions on structured populations:
a review,” Journal of the royal society interface, vol. 10, no. 80, p.
20120997, 2013.
[32] D. Ghosh, M. Frasca, A. Rizzo, S. Majhi, S. Rakshit, K. Alfaro-
Bittner, and S. Boccaletti, “The synchronized dynamics of time-varying
networks,” Physics Reports, vol. 949, pp. 1–63, 2022.
[33] P. Holme and J. Saram¨aki, Temporal network theory.
Springer, 2019,
vol. 2.
[34] P. Holme and J. Saramki, “Temporal networks,” Physics reports, vol.
519, no. 3, pp. 97–125, 2012.
[35] J. M. Pacheco, A. Traulsen, and M. A. Nowak, “Coevolution of strategy
and structure in complex networks with dynamical linking,” Physical
review letters, vol. 97, no. 25, p. 258103, 2006.
[36] D. Melamed, A. Harrell, and B. Simpson, “Cooperation, clustering, and
assortative mixing in dynamic networks,” Proceedings of the National
Academy of Sciences, vol. 115, no. 5, pp. 951–956, 2018.
[37] J. Zheng, Y. He, T. Ren, and Y. Huang, “Evolution of cooperation in
public goods games with segregated networks and periodic invasion,”
Physica A: Statistical Mechanics and its Applications, vol. 596, p.
127101, 2022.
[38] C. Hilbe, ˇS. ˇSimsa, K. Chatterjee, and M. A. Nowak, “Evolution of
cooperation in stochastic games,” Nature, vol. 559, no. 7713, pp. 246–
249, 2018.
[39] Q. Su, A. McAvoy, L. Wang, and M. A. Nowak, “Evolutionary dy-
namics with game transitions,” Proceedings of the National Academy of
Sciences, vol. 116, no. 51, pp. 25 398–25 404, 2019.
[40] A. Szolnoki and M. Perc, “Seasonal payoff variations and the evolution
of cooperation in social dilemmas,” Scientific reports, vol. 9, no. 1, p.
12575, 2019.
[41] J. Li, C. Zhang, Q. Sun, Z. Chen, and J. Zhang, “Changing the intensity
of interaction based on individual behavior in the iterated prisoner’s
dilemma game,” IEEE Transactions on Evolutionary Computation,
vol. 21, no. 4, pp. 506–517, 2016.
[42] A. Li, L. Zhou, Q. Su, S. P. Cornelius, Y.-Y. Liu, L. Wang, and
S. A. Levin, “Evolution of cooperation on temporal networks,” Nature
communications, vol. 11, no. 1, p. 2259, 2020.
[43] X. Li, G. Hao, Z. Zhang, and C. Xia, “Evolution of cooperation in
heterogeneously stochastic interactions,” Chaos, Solitons & Fractals,
vol. 150, p. 111186, 2021.
[44] R. S. Edgar, E. W. Green, Y. Zhao, G. Van Ooijen, M. Olmedo, X. Qin,
Y. Xu, M. Pan, U. K. Valekunja, K. A. Feeney et al., “Peroxiredoxins
are conserved markers of circadian rhythms,” Nature, vol. 485, no. 7399,
pp. 459–464, 2012.
[45] H. Ohtsuki, C. Hauert, E. Lieberman, and M. A. Nowak, “A simple rule
for the evolution of cooperation on graphs and social networks,” Nature,
vol. 441, no. 7092, pp. 502–505, 2006.
[46] X. Chen, F. Fu, and L. Wang, “Interaction stochasticity supports cooper-
ation in spatial prisoner’s dilemma,” Physical Review E, vol. 78, no. 5,
p. 051120, 2008.
[47] Z. Wang, S. Kokubo, J. Tanimoto, E. Fukuda, and K. Shigaki, “Insight
into the so-called spatial reciprocity,” Physical Review E, vol. 88, no. 4,
p. 042145, 2013.
"
"This study reviews the impact of personalization on human-robot interaction. Various strategies used to achieve personalization are briefly described. Effects of personalization known to date are discussed along with the personalized parameters, personalized features, used technology, and use case they relate to. It is noticed that various positive effects have been discussed in the literature while possible negative effects seem to require further investigation.","Socially assistive robots (SAR) should be capable of social interactions and are presented as a potential solution for the growing need for assistance in domains such as healthcare, education, and elderly care. They should, for example, perform various household tasks, provide companionship to the elderly, assist children in learning, deliver meals, or guide people in tours of shopping or museums.
As robots are expected to become increasingly integrated into people’s daily lives, it seems important to create more natural and personalized interactions between humans and robots. Indeed, they should become companions that provide both mental and physical support.","Mehdi Hellou et al. (2021) [9] presented a review of the employed methods, to date, to create robot personalization. Personalization can influence differently the user perception of the quality of the interaction and of the system trustworthiness. Many strategies for achieving personalization have been investigated.nannannan",Nonenan,"Various personalization strategies are used to achieve personalization, ranging from the most straightforward ones to more intricate multimodal ones that have recently been developed.
Some strong advantages of personalized robots have been found, such as creating favorable emotional impulses for users or increasing the effectiveness of tutoring. Some possible drawbacks of personalization have been examined, such as privacy issues, trust between people and robots, and moral and ethical considerations.","This study on the impact of robot personalization on human-robot interaction (HRI) presented various personalization techniques, ranging from simple to sophisticated multimodal methods. It highlighted several substantial advantages of personalization that have been extensively researched in recent years. Finally, it examined potential negative effects of personalization and noted the need for more study on this topic. Overall, it appears that the effects of personalization need more investigation.",On the impact of robot personalization on human-robot interaction: A review,"Jinyu Yang, Camille Vindolet, Julio Rogelio Guadarrama Olvera, Gordon Cheng","On the impact of robot personalization on human-robot
interaction: A review
Jinyu Yang
jinyu.yang@tum.de
Technical University of Munich
Germany
M. Sc. Camille Vindolet
camille.vindolet@tum.de
Institute for Cognitive Systems
Technical University of Munich
Germany
Dr.-Ing. Julio Rogelio Guadarrama Olvera
rogelio.guadarrama@tum.de
Institute for Cognitive Systems
Technical University of Munich
Germany
Prof. Dr. Gordon Cheng
gordon@tum.de
Institute for Cognitive Systems
Technical University of Munich
Germany
ABSTRACT
This study reviews the impact of personalization on human-robot
interaction. Firstly, the various strategies used to achieve personal-
ization are briefly described. Secondly, the effects of personalization
known to date are discussed. They are presented along with the
personalized parameters, personalized features, used technology,
and use case they relate to. It is observed that various positive ef-
fects have been discussed in the literature while possible negative
effects seem to require further investigation.
CCS CONCEPTS
• Human-centered computing → Collaborative and social
computing; • Security and privacy → Database and storage
security; • Computing methodologies → Artificial intelligence; •
Applied computing;
KEYWORDS
Human-robot interaction, robot personalization, socially assistive
robotics, social robotics, human-centred design
ACM Reference Format:
Jinyu Yang, M. Sc. Camille Vindolet, Dr.-Ing. Julio Rogelio Guadarrama
Olvera, and Prof. Dr. Gordon Cheng. 2023. On the impact of robot personal-
ization on human-robot interaction: A review. In Proceedings of (ACM/IEEE
HRI 23, workshop CONCATENATE). ACM, New York, NY, USA, 6 pages.
1
INTRODUCTION
Nowadays, robots are expected to evolve alongside humans and
to interact more closely with them. In particular, socially assistive
robots (SAR) should be capable of social interactions and are pre-
sented as a potential solution for the growing need for assistance in
domains such as healthcare, education, and elderly care [19]. They
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ACM/IEEE HRI 23, workshop CONCATENATE, March 13, 2023, Stockholm, Sweden
© 2023 Association for Computing Machinery.
should, for example, perform various household tasks, provide com-
panionship to the elderly, assist children in learning [22], deliver
meals [6], or guide people in tours of shopping [15] or museums.
As robots are expected to become increasingly integrated into
people’s daily lives, it seems important to create more natural and
personalized interactions between humans and robots. Indeed, they
should become companions that provide both mental and physical
support. This type of interactions require an emotional intelligence
from the robot side with a capacity to adapt to the user’s state
[7]. Thus, personalization is one of the current field of research
in HRI and more specifically in SAR. It aims to enable robots to
understand human needs and preferences and to correctly adapt
to them. For example, personalized robots should recognize and
respond accordingly to the users social interactions, nonverbal
behaviors, or culture. Personalization is said to improve the user
experience, encourage social acceptance of the robot, and lead to
more effective social interactions by adapting the behavior, appear-
ance and interaction patterns of robots to the individuals needs and
preferences.
Many strategies for achieving personalization have been inves-
tigated. Mehdi Hellou et al. (2021) [9] presented a review of the
employed methods, to date, to create robot personalization.
Personalization can influence differently the user perception of
the quality of the interaction and of the system trustworthiness. It
can help improving the match between the user expectations and
the robot behavior. This paper reviews the findings on the impact of
personalization in HRI, in order to acquire a more comprehensive
understanding of how personalization affects the interaction ex-
perience and provided service that better fulfills the requirements
of the user. Examples of current personalization strategies are pro-
vided in section 2. Personalizing a robot for different scenarios and
work contexts is possible in a variety of ways. The consequences
of various personalization are studied in section 3. Section 4 dis-
cusses possible directions for further research into the effects of
personalization.
arXiv:2401.11776v1  [cs.RO]  22 Jan 2024
ACM/IEEE HRI 23, workshop CONCATENATE, March 13, 2023, Stockholm, Sweden
Yang, et al.
Analysed
Parameters
Personalized
Features
Technology,
Hardware and
Sensors
Use Cases/
Purpose
The Effects of
personalization
M. K. Lee et al.
(2012) [10]
Users’ snack
selection,
service-patterns and
previous behaviour
of the robot
Social interactions
and small talk topics
by robot
Snack ordering
website, Snackbot,
SICK LIDAR, etc.
Delivering snacks
Reinforcement of
people’s rapport,
cooperation and
engagement with
robot
D. Portugal et al.
(2015) [18]
Users’ emotional
state and personal
preferences
Robot’s service:
movement,
navigation,
emotional
companionship, etc.
Service-Oriented
Architecture (SOA),
service-oriented
architecture, etc.
Home care of the
elder
Not mentioned
G. Gordon et al.
(2016) [8]
Emotional and game
state
Game-related
responses
(instructions,
prompts) and
emotional responses
Tega robot platform,
Robot Operating
System (ROS), etc.
Regulating the
student’s emotional
state while
providing them with
second language
tutoring
Significant increase
in children’s
engagement and
valence
C. E. Clabaugh
(2017) [5]
Frequency of
elicitation (asking
learning-sensitive
questions)
Robotic tutor’s
elicitation of
learning-sensitive
information
Not mentioned
Robot tutor for
personalized
education
Improved
stimulation and
increased trust
towards the robot
N. Churamani et al.
(2017) [3]
User utterance and
personal
information
Utterance by robot
NICo, Humanoidly
Speaking Scenario,
Face and Speech
Recognition, etc.
Teaching the robot
to recognize
different objects
Improved
impressions of the
robot for the users
D. Leyzberg et al.
(2018) [11]
Participants’ level of
skill acquisition
Ordering of the
interchangeable
lesson chapters
The extended
Bayesian
Knowledge Tracing
(BKT) family of
student models with
camera and
microphone
Rearrange
curriculum while
completing an
English Language
Learning
educational task
Increased learning
gains
C. Clabaugh et al.
(2019) [4]
Children’s
behaviour during
the game (Whether
mistakes were made,
help requested, etc.)
Level of challenge
and feedback
Tega robot platform,
electrodermal
activity sensors, etc.
Social and
educational
development of
children with ASD
Improved learning
and evidence of
longer-term
engagement
H. W. Park et al.
(2019) [16]
Users’ behavior and
affective arousal
state
Lexical and
syntactic complexity
in storytelling
Tega robot platform,
electrodermal
activity sensors, etc.
Learning
companion
Increase of
Children’s
engagement,
improved learning,
increased use of the
target words, etc.
S. Schneider and
Franz Kummert
(2021) [20]
Level of automation
Robot’s service:
propose different
activities
Database of
different exercises,
session controller,
3D depth sensor, etc.
Assisting the user in
exercising
Higher trust and
perceived as more
competent
M. E. U. Ligthart et
al. (2022) [13]
Children’s interest,
preferences and
choices
Serial narrative
dialog
Google’s dialog flow,
onboard
microphone, Redis
database, etc.
Communicating
with children in
dialogue
Longer interest for
the robot, better
engagement
Table 1: Summary of the papers analyzed in this review.
On the impact of robot personalization on human-robot interaction: A review
ACM/IEEE HRI 23, workshop CONCATENATE, March 13, 2023, Stockholm, Sweden
2
PERSONALIZED ROBOT STRATEGIES AND
USE CASES
As discussed in the previous section, there are various use cases
of SAR, such as visiting of and caring for the elderly, helping dis-
abled people with rehabilitation training, assisting and instructing
children in their studies, among others. As a result, there is not
a single solution for personalizing a robot. The robot’s personal-
ization and the researchers’ design can vary from simple methods,
such as choosing between several pre-defined dialogue or actions
the one that fit the best the user needs and preferences, to more
complex algorithms and technology that, based on multisensory
integration and complex models, generate appropriate robot behav-
iors. The examples studied in this article are listed in Table 1. The
table presents in order of the columns, the article, the experiment
parameters taken into account for the personalization, the person-
alized robot features, the encountered technology, the use case and
purpose of the personalization and its effects.
Personalization can be, for example, pre-programming the ro-
bot’s speech or actions based on user data. To investigate how to
build and maintain rapport between robots and people, Min Kyung
Lee et al. (2012) [10] designed a snack delivering robot and created
a personalization strategy that involved developing pre-defined
discussions and responses, later selected based on the user’s prior
interactions with the robot and with its delivery service. The ro-
bot will present multiple discussions to the user depending on the
user’s snack selection pattern, and service usage pattern, and on the
robot’s past behavior (as indicated in figure 1). Based on a record
of defects and faults maintained in the service database, the robot
also apologizes for any errors that have happened.
More recently, researchers have developed complex algorithms
and techniques to create personalized robot strategies. These al-
gorithms frequently require a combination of data, such as how
the participant interacts with the robot, the responses given to the
robot’s activities (e.g., facial expressions, etc.), and the history of
previous interactions.
David Portugal et al. (2015) [18] provided a summary of the
techniques developed in the SocialRobot project. In this project,
a mobile robot platform was created in conjunction with virtual
social care technologies in order to enhance elder’s quality of life
and fulfill their individual needs. They created a model based on
service-oriented architecture (SOA) and used social care community
network to manage and coordinate user profiles for proactive and
individualized care. The database could dynamically be adjusted in
order to update user preferences.
Goren Gordon et al. [8] created an integrated system of affective
policy learning and autonomous social robots to investigate how
effectively personalized robots regulate students’ affective states in
one-on-one tutorials. Various states were specified as parameters
in the affective policy: the child’s valence, engagement, as well as
information about whether they correctly answered the questions
and if they had previously interacted with the robot. A state-action
reward state-action (SARSA) algorithm and an epsilon-greedy al-
gorithm are used to determine the robot’s action states.
Figure 1: An overview of the personalized topics proposed
by Min Kyung Lee et al. [10]
Hae Won Park et al. [16] studied how robots can assist young
learners in optimizing their learning results. To manage the selec-
tion of robot behaviors, they created an affective personalization
policy. The user’s behavior (their question answering behavior dur-
ing the robot’s story narration), their affective arousal state (their
facial muscle activation illustrating their expressiveness and arousal
level), the robot’s actions (the lexical and syntactic complexity of
a given sentence in storytelling), the rewards the robot earned for
its activities (the new lexical and syntax learning), and the changes
in the user’s state were organized into different episodes that were
trained by Q-Learning, resulting in a personalized policy for each
user.
Caitlyn Clabaugh et al. [4] developed a hierarchical framework
for Human Robot Learning to manage the behavior of personal-
ized robots in their study of assistive robots serving children with
autism spectrum disorders (ASD). In this framework, the robot’s
interventions were subdivided into defined state-action sub-spaces,
with five controllers each responsible for a subset of actions and
describing abstract action categories such as disclosure, promise, in-
struction, feedback, and inquiry. These controllers were combined
with algorithms including Q-Learning to ensure that the robot’s
feedback and instruction challenge levels could be personalized to
each child’s particular learning style.
3
THE IMPACT OF PERSONALIZATION
Personalization, as discussed in the preceding section, has become
an increasingly important component of HRI. Additionally, it has
created new possibilities for SAR by enabling more efficient and
satisfying user experiences. However, it is necessary to examine the
potential impacts of personalization. This section examines both
positive and negative effects of personalization including potential
long-term ones.
3.1
Positive Impact
Caitlyn E. Clabaugh et al. (2017) [5] analysed the elicitation of
learning-sensitive information by a robotic tutor that would be
ACM/IEEE HRI 23, workshop CONCATENATE, March 13, 2023, Stockholm, Sweden
Yang, et al.
utilised by an interactive machine learning approach for person-
alized education. They focused on the relationship between the
frequency of elicitation and human learners’ impressions of tuto-
rial interactions. The study showed that more frequent elicitation
of learning-sensitive information by the robot tutor led to a sig-
nificant increase in participants’ interaction with the robot tutor,
and that the robot tutor was perceived to be more interactive and
affectionate, and was better trusted.
A study on the impact of various SAR automation levels on user
perception of the system in an exercise scenario was presented
by Sebastian Schneider et al. (2021) [20]. According to the study,
adaptive robots (where an adaptive process can be initiated auto-
matically by the robot, such as when the robot chooses exercises for
the user) are perceived as warmer and more competent than adapt-
able robots (where the user adapts the system themselves, such
as when the user selects their own exercises). Users trusted the
adaptive robot substantially more than the adaptable robot. More-
over, users who were exposed to an adaptive robot more frequently
volunteered to repeat the task.
According to Daniel Leyzberg et al. [11] evaluated the advantages
personalized social robots can offer in a learning environment. They
developed a strategy to organize lessons based on a model for evalu-
ating students’ skill competency, with the content of the lessons re-
lated to educational activities for native Spanish-speaking students
learning English. The study’s findings demonstrated that, assuming
that all students had roughly the same amount of prior knowledge,
the group receiving the personalized program learned more than
the group receiving the non-personalized program, which was rep-
resented in a large increase in learning gains. When compared
to learners who received non-personalized education, those who
took personalized courses produced a higher percentage of correct
answers on tests.
Additionally, Mike E.U. Ligthart et al. [13], a memory-based per-
sonalization strategy was developed and deployed in combination
with a continuous narrative interaction structure to address de-
mands and keep children from losing interest in the robot once
the novelty wore off. The research indicates that in the classic
control condition, as the number of interaction sessions increased,
children’s interest in continuing the contact began to decrease
over time. However in the personalization strategy-using condition,
children’s interest in continuing the interaction remained strong.
Moreover, the children’s proximity to the robot was higher after
multiple interaction sessions when the personalization strategy
was deployed, and the strategy encouraged a closer relationship
between the child and the robot. Positive social cues were elicited
more often.
The benefits of personalization have been extensively studied.
Many of these effects are connected to specific use cases and imple-
mentation contexts. They have shown to improve the user experi-
ence and generate positive emotions.
3.2
Possible negative Impacts
Personalization has recently been a popular study topic. Numer-
ous research focused on the benefits of personalization, such as
increasing the user engagement. Those studies usually address the
effects of personalization with regard to some hypothesis and dis-
cuss whether or not personalization has a positive impact on the
features they want to study or no impact at all. For example, Mike
E.U. Ligthart et al. [13] notes that personalization does not have
a substantial impact on the need for familiarity and similarity, in
addition to the obvious positive effects. However, little research has
thoroughly investigated the negative impact of personalization. As
a result, this chapter presents the existing research on the topic, as
well as discusses other features of personalization that are likely to
have a negative influence.
Research and discussions on the privacy policies provided by
various consumer robot companies were performed by Anna Chatz-
imichali et al. (2021) [2]. According to the study, the structure,
context, and clarity of privacy provisions differ considerably be-
tween companies. This highlights the topic of how personalization
may have a negative impact on personal privacy, user rights, and
consumer rights in a context where consumer robotics businesses
are not now uniformly considering privacy issues. As personaliza-
tion depends on private data such as users’ individual preferences,
behaviors, and interaction history, the question of how to handle
this information so that privacy and user rights meet users’ re-
quirements needs to be addressed. This is also mentioned in Jim
Torresen’s work (2021) [23]. He examines how sensor data collec-
tion should be limited in uncommon and urgent situations and
establishes the argument that privacy requires a trade-off on this.
In the case of robot-assisted care for the elderly, for example, the
robot is only permitted to transmit data about the elderly in order
to alert caretakers, etc., if an abnormal incident occurs.
The research by Nikhil Churamani et al. [3] presents an addi-
tional hypothesis regarding negative impacts. They investigated
robots’ social acceptability, perceived intelligence, and likability
in HRI scenarios. Since more complicated interactions may raise
certain user concerns, the study indicated that the personalized
interactive system scored worse in terms of safety. Furthermore, be-
cause the personalized robot was equipped with speech technology
that was insufficient to support its capacity to reliably recognize the
user’s words in a noisy setting, participants did not recommend the
system for usage in the real world. This raises the possibility that
personalization might have a negative impact, since a personalized
robot may be unable to deal with the different disturbances and
emergencies that come in real-life work environments before the
technology is developed, leading people to distrust the robot and
choose a more general and safe robot. In addition to human trust
in the robot, whether the robot models the human as trustworthy
can have an effect on the process and experience of HRI. According
to research by Dylan P. Losey et al. [14], modeling the human as
a trusting agent can increase HRI effectiveness, however trusting
humans in this scenario are rewarded less than humans treated as
rational.
In addition to the user experience, considering the robot’s ethical
and moral feasibility is essential. For instance, the system needs
to have the ability to guarantee the user’s right as well as the fair-
ness and verifiability of the decision-making process [23]. Kathrin
Pollmann et al. [17] also discusses the necessity of maintaining an
equilibrium between ethical considerations and user experience
design concepts in their research. In order to avoid manipulating
On the impact of robot personalization on human-robot interaction: A review
ACM/IEEE HRI 23, workshop CONCATENATE, March 13, 2023, Stockholm, Sweden
the user and infringing on their autonomy, it is crucial to strive to
avoid personalization methods that could result in affective mech-
anisms (ie. ""guilt trip, emotional blackmail, erotic attraction,etc.""
[17]).
Research on the potential negatives effects of personalization is
currently lacking. The impact of personalization on security of user
data privacy, social acceptability, trust and ethical considerations
of the robots should be further researched in the future.
4
DISCUSSION AND CONCLUSION
Whereas much research has shown the positive advantages of per-
sonalization in various ways and for different use cases, such as
creating positive emotional impulses for users or improving tutor-
ing efficiency, the negative implications have not yet been properly
investigated. In the previous section, some of the potential negative
features of personalization were briefly discussed: privacy concerns,
trust between human and robot, and consideration for ethics. Fur-
thermore, there are a variety of issues that must be addressed in a
future study on personalization.
Cong Li (2016) noted the difference between actual personal-
ization and perceived personalization in his work on personaliza-
tion [12]. He tested the personalization effect through the message
sender’s actual personalization process and identified that it may
have misleading effects. According to his research, a real person-
alization process does not always result in beneficial results, and
participants only reacted positively to messages that they thought
were more personalized, even though these messages may not have
been produced through the personalization process. He suggested
that there may be certain conditions under which a message is
interpreted as personalized or non-personalized. Unquestionably,
when a message contains incorrect information, the receiver of the
message and the message are mismatched. Such a message will
cause the receiver to perceive it as non-personalized, resulting in a
negative attitude.
Another concern is human acceptance or trust towards robots.
This is a complex, multifaceted problem. The previously indicated
negative human evaluations due to the robot’s lack of competency
can be linked to trust and acceptance of the robot. Leila Takayama et
al. (2009) [21] investigated factors influencing proxemic behaviors
in HRI. It was discovered that the size of the participant’s personal
space when interacting with a robot was related to a number of
parameters. By examining the comfortable distance between the hu-
man and the robot during HRI, S. M. Bhagya et al. (2019) [1] studied
human affinity preferences for service robots. They discovered that
happy and sad facial expressions on the robot had closer proxemics,
whereas anger and disgust had the opposite effect. The comfort
distance of HRI is alternatively impacted by the robot’s internal
sounds and human-like appearance. Additionally, Dylan P. Losey
et al. (2019) [14] discovered that the user experience is impacted
by the human trustworthiness or untrustworthiness settings when
the robot models people. Generally, in addition to the influence of
personalization, variables such as perceived personalization and
people’s acceptance of and trust in robots need to be considered
and further investigated.
In conclusion, this review on the impact of robot personalization
on HRI presented various personalization strategies, from the most
simple ones to some more recent complex multimodal ones. It high-
lighted some strong personalization benefits that have been studied
in research in the past few years. Finally this review discussed some
possible negative impacts of personalization and informed on the
lack of research on this topic. Overall, the effects of personalization
seem to still required further investigation.
REFERENCES
[1] S. M. Bhagya, P. Samarakoon, M. A. Viraj, J. Muthugala, A. G. Buddhika, P.
Jayasekara, and Mohan R. Elara. 2019. An Exploratory Study on Proxemics
Preferences of Humans in Accordance with Attributes of Service Robots. In 2019
28th IEEE International Conference on Robot and Human Interactive Communication
(RO-MAN). 1–7. https://doi.org/10.1109/RO-MAN46459.2019.8956297 ISSN: 1944-
9437.
[2] Anna Chatzimichali, Ross Harrison, and Dimitrios Chrysostomou. 2021. Toward
privacy-sensitive human–robot interaction: Privacy terms and human–data in-
teraction in the personal robot era. Paladyn, Journal of Behavioral Robotics 12,
1 (Jan. 2021), 160–174.
https://doi.org/10.1515/pjbr-2021-0013 Publisher: De
Gruyter Open Access.
[3] Nikhil Churamani, Paul Anton, Marc Brügger, Erik Fließwasser, Thomas Hummel,
Julius Mayer, Waleed Mustafa, Hwei Geok Ng, Thi Linh Chi Nguyen, Quan
Nguyen, Marcus Soll, Sebastian Springenberg, Sascha Griffiths, Stefan Heinrich,
Nicolás Navarro-Guerrero, Erik Strahl, Johannes Twiefel, Cornelius Weber, and
Stefan Wermter. 2017. The Impact of Personalisation on Human-Robot Interaction
in Learning Scenarios. In Proceedings of the 5th International Conference on Human
Agent Interaction. ACM, Bielefeld Germany, 171–180. https://doi.org/10.1145/
3125739.3125756
[4] Caitlyn Clabaugh, Kartik Mahajan, Shomik Jain, Roxanna Pakkar, David Becerra,
Zhonghao Shi, Eric Deng, Rhianna Lee, Gisele Ragusa, and Maja Matarić. 2019.
Long-Term Personalization of an In-Home Socially Assistive Robot for Children
With Autism Spectrum Disorders. Frontiers in Robotics and AI 6 (2019). https:
//www.frontiersin.org/articles/10.3389/frobt.2019.00110
[5] Caitlyn E. Clabaugh. 2017. Interactive Personalization for Socially Assistive
Robots. In Proceedings of the Companion of the 2017 ACM/IEEE International
Conference on Human-Robot Interaction (HRI ’17). Association for Computing Ma-
chinery, New York, NY, USA, 339–340. https://doi.org/10.1145/3029798.3034813
[6] Francesca Dino, Rohola Zandie, Hojjat Abdollahi, Sarah Schoeder, and Moham-
mad H. Mahoor. 2019. Delivering Cognitive Behavioral Therapy Using A Con-
versational Social Robot. In 2019 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS). 2089–2095. https://doi.org/10.1109/IROS40897.2019.
8968576 ISSN: 2153-0866.
[7] Dalila Durães, Javier Bajo, and Paulo Novais. 2018. Characterize a Human-
Robot Interaction: Robot Personal Assistance. In Personal Assistants: Emerging
Computational Technologies, Angelo Costa, Vicente Julian, and Paulo Novais (Eds.).
Springer International Publishing, Cham, 135–147. https://doi.org/10.1007/978-
3-319-62530-0_8
[8] Goren Gordon, Samuel Spaulding, Jacqueline Kory Westlund, Jin Joo Lee, Luke
Plummer, Marayna Martinez, Madhurima Das, and Cynthia Breazeal. 2016. Af-
fective Personalization of a Social Robot Tutor for Children’s Second Language
Skills. Proceedings of the AAAI Conference on Artificial Intelligence 30, 1 (March
2016). https://doi.org/10.1609/aaai.v30i1.9914
[9] Mehdi Hellou, Norina Gasteiger, Jong Yoon Lim, Minsu Jang, and Ho Seok Ahn.
2021. Personalization and Localization in Human-Robot Interaction: A Review
of Technical Methods. Robotics 10, 4 (Dec. 2021), 120. https://doi.org/10.3390/
robotics10040120 Number: 4 Publisher: Multidisciplinary Digital Publishing
Institute.
[10] Min Kyung Lee, Jodi Forlizzi, Sara Kiesler, Paul Rybski, John Antanitis, and
Sarun Savetsila. 2012. Personalization in HRI: A longitudinal field experiment. In
2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI).
319–326. ISSN: 2167-2148.
[11] Daniel Leyzberg, Aditi Ramachandran, and Brian Scassellati. 2018. The Effect of
Personalization in Longer-Term Robot Tutoring. ACM Transactions on Human-
Robot Interaction 7, 3 (Dec. 2018), 19:1–19:19. https://doi.org/10.1145/3283453
[12] Cong Li. 2016. When does web-based personalization really work? The distinction
between actual personalization and perceived personalization. Computers in
Human Behavior 54 (Jan. 2016), 25–33. https://doi.org/10.1016/j.chb.2015.07.049
[13] Mike E.U. Ligthart, Mark A. Neerincx, and Koen V. Hindriks. 2022. Memory-Based
Personalization for Fostering a Long-Term Child-Robot Relationship. In Proceed-
ings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction
(HRI ’22). IEEE Press, Sapporo, Hokkaido, Japan, 80–89.
ACM/IEEE HRI 23, workshop CONCATENATE, March 13, 2023, Stockholm, Sweden
Yang, et al.
[14] Dylan P. Losey and Dorsa Sadigh. 2019. Robots that Take Advantage of Human
Trust. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS). 7001–7008. https://doi.org/10.1109/IROS40897.2019.8968564 ISSN: 2153-
0866.
[15] S. Nithya Priya, G. Swadesh, K.M Thirivikraman, Mohammed Vazeer Ali, and
M. Ranjith Kumar. 2021. Autonomous Supermarket Robot Assistance using
Machine Learning. In 2021 7th International Conference on Advanced Computing
and Communication Systems (ICACCS), Vol. 1. 996–999. https://doi.org/10.1109/
ICACCS51430.2021.9441833 ISSN: 2575-7288.
[16] Hae Won Park, Ishaan Grover, Samuel Spaulding, Louis Gomez, and Cynthia
Breazeal. 2019. A model-free affective reinforcement learning approach to per-
sonalization of an autonomous social robot companion for early literacy edu-
cation. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelli-
gence and Thirty-First Innovative Applications of Artificial Intelligence Confer-
ence and Ninth AAAI Symposium on Educational Advances in Artificial Intelli-
gence (AAAI’19/IAAI’19/EAAI’19). AAAI Press, Honolulu, Hawaii, USA, 687–694.
https://doi.org/10.1609/aaai.v33i01.3301687
[17] Kathrin Pollmann, Wulf Loh, Nora Fronemann, and Daniel Ziegler. 2023. Enter-
tainment vs. manipulation: Personalized human-robot interaction between user
experience and ethical design. Technological Forecasting and Social Change 189
(April 2023), 122376. https://doi.org/10.1016/j.techfore.2023.122376
[18] David Portugal, Luís Santos, Paulo Alvito, Jorge Dias, George Samaras, and
Eleni Christodoulou. 2015. SocialRobot: An interactive mobile robot for elderly
home care. In 2015 IEEE/SICE International Symposium on System Integration (SII).
811–816. https://doi.org/10.1109/SII.2015.7405084
[19] Natacha Rouaix, Laure Retru-Chavastel, Anne-Sophie Rigaud, Clotilde Monnet,
Hermine Lenoir, and Maribel Pino. 2017. Affective and Engagement Issues in
the Conception and Assessment of a Robot-Assisted Psychomotor Therapy for
Persons with Dementia. Frontiers in Psychology 8 (2017). https://www.frontiersin.
org/articles/10.3389/fpsyg.2017.00950
[20] Sebastian Schneider and Franz Kummert. 2021. Comparing Robot and Human
guided Personalization: Adaptive Exercise Robots are Perceived as more Compe-
tent and Trustworthy. International Journal of Social Robotics 13, 2 (April 2021),
169–185. https://doi.org/10.1007/s12369-020-00629-w
[21] Leila Takayama and Caroline Pantofaru. 2009. Influences on proxemic behaviors
in human-robot interaction. In 2009 IEEE/RSJ International Conference on Intelli-
gent Robots and Systems. 5495–5502. https://doi.org/10.1109/IROS.2009.5354145
ISSN: 2153-0866.
[22] Tanio K. Tanev and Anna Lekova. 2022. Implementation of Actors’ Emotional
Talent into Social Robots Through Capture of Human Head’s Motion and Basic
Expression. International Journal of Social Robotics 14, 7 (Sept. 2022), 1749–1766.
https://doi.org/10.1007/s12369-022-00910-0
[23] Jim Torresen. 2021. Ethical Considerations in User Modeling and Personalization.
In Proceedings of the 29th ACM Conference on User Modeling, Adaptation and
Personalization. ACM, Utrecht Netherlands, 321–322. https://doi.org/10.1145/
3450613.3457121
Received 27 January 2023
"
"This paper approaches the problem of position reasoning in referring image segmentation (RIS). We propose a collaborative positioning reasoning network (CPRN) that utilizes two parallel pathways (Row-and-Column interactive (RoCo), Guided holistic interactive (Holi)) to capture accurate local and global information for object localization and precise segmentation. The RoCo module aggregates the visual features into the row- and column-wise features in different directions. In addition, it integrates the linguistic features and performs position reasoning over a hierarchical space. The Holi module preserves the holistic feature map to guarantee the integrity of global information. A global guidance path is designed to enhance the localization of Holi by incorporating the RoCo's positioning information. The RoCo and Holi outputs are merged via a Feed Forward Network (FFN). A multi-scale decoder is employed to aggregate multi-level features for accurate referring segmentation. Extensive experiments are conducted on three challenging datasets, demonstrating the superiority of the proposed CPRN over state-of-the-art methods.","Referring image segmentation (RIS) aims to segment the foreground masks of the entities referred by the natural language expression. Existing methods mainly focus on the interactive learning between vision and language to enhance the multi-modal representations for global context reasoning. However, directly predicting the result in pixel-level space can lead to inaccurate positioning and poor segmentation. The main challenge is how to explicitly model entity localization, especially for non-salient entities. In this paper, we tackle this problem by executing a collaborative positioning reasoning network (CPRN) via the proposed novel row-and-column interactive (RoCo) and guided holistic interactive (Holi) modules.","nanSince RIS involves visual and linguistic domains, it is challenging especially in modeling the fine-grained interactions between the two modalities. As shown in Fig. 2, existing works can be roughly divided into four types according to the network structure. (a) A straightforward way to extract contextual knowledge to produce the final result via a simple concatenation-convolution scheme such as dynamic filters, LSTM and attention mechanism. This solution aggregates the visual and linguistic features without a deep understanding, which could not effectively explore the relationships between the two modalities. (b) Another line of works process each word in the referring expression to learn cross-modal interaction in a sequential manner. However, they consider each word as an equal contribution. This may have trouble distinguishing the target with long referring expressions. (c) Alternative works establish several attributes (object, location and relationship to other objects) of referring expressions to improve the scores of cross-modal matching. This design help refine the segmentation results but lacks global context information and relies on the proposals generated by object detectors. (d) At last, a series of works have been proposed to progressively integrate contextual information at multiple levels. LAVT fuses the linguistic and visual features into each stage of the network and captures segmentation masks with a lightweight decoder. BRINet considers the interaction through a bi-directional cross-modal attention module that uses both visual and textual guidances to capture their dependencies, realizing the compatibility between vision and textural features. They focuses on utilizing the inference module to enhance the visual and textual interaction achieves the best results. Nevertheless, all these methods do not directly focus on the issue of position reasoning and fail to locates the referred entity from background.nannan","Fig. 4 illustrates the overall architecture of our collaborative positioning reasoning network (CPRN). We take an image I and a referring expression E as input, the visual features Vi and linguistic features L are extracted by Swin Transformer and language encoder BERT, respectively. They are fed into our proposed CPRN block which consists of two modules, for highlighting the referent entities. One is the Row-and-Column interactive (RoCo) module, the other is the Guided Holistic interactive (Holi) module. After that, the two pathways are merged using a Feed Forward Network (FFN) to enhance the reasoning features. Finally, the Multi-Scale Decoder module is used to perform the different stage feature fusion and refine the final segmentation mask.nan","The proposed CPRN achieves an average improvement of 7.53% over ReSTR on all three datasets. In detail, CPRN yields an average improvement of 1.36%-4.71% compared to CRIS, which had previously achieved the best performance. In particular, our method also greatly outperforms LAVT using the same backbone, which fully illustrates the advantages of our proposed CPRN block. The gains are more obvious on other datasets than on the RefCOCO+ dataset, indicating that the ability of the CPRN block to locate referents has a great impact on improving model performance. In the end, the above comparison with existing methods fully demonstrates the superiority of our method.","In this work, we address the problem of position reasoning in RIS and propose a collaborative positioning reasoning network (CPRN) for leveraging the hierarchical context of images for position reasoning. As illustrated in Fig. 3, in our model, the features passed through two parallel pathways can capture the local and global information for accurate localization and fine-grained segmentation. In detail, the row-and-column interactive (RoCo) module generates the correlation between horizontal and vertical feature maps with linguistic features. The guided holistic interactive (Holi) module keeps the holistic feature map to ensure the integrity of global information. Meanwhile, a global guidance path is designed to enhance the localization of Holi by incorporating the RoCo's positioning information. The output features of RoCo and Holi are merged via a Feed Forward Network (FFN). Finally, we devise a multi-scale decoder to aggregate multi-level features for accurate referring segmentation. Extensive experiments on all three challenging datasets show that the proposed CPRN plays an important role in improving the positioning performance of referring image segmentation. And our model achieves superior performance compared to state-of-the-art methods.",Collaborative Position Reasoning Network for Referring Image Segmentation,"Jianjian Cao, Beiya Dai, Yulin Li, Xiameng Qin, Jingdong Wang","1
Collaborative Position Reasoning Network for
Referring Image Segmentation
Jianjian Cao, Beiya Dai, Yulin Li, Xiameng Qin and Jingdong Wang, Fellow, IEEE
Abstract—Given an image and a natural language expression
as input, the goal of referring image segmentation is to segment
the foreground masks of the entities referred by the expression.
Existing methods mainly focus on interactive learning between
vision and language to enhance the multi-modal representations
for global context reasoning. However, predicting directly in
pixel-level space can lead to collapsed positioning and poor
segmentation results. Its main challenge lies in how to explicitly
model entity localization, especially for non-salient entities. In
this paper, we tackle this problem by executing a Collaborative
Position Reasoning Network (CPRN) via the proposed novel Row-
and-Column interactive (RoCo) and Guided Holistic interactive
(Holi) modules. Specifically, RoCo aggregates the visual features
into the row- and column-wise features corresponding two
directional axes respectively. It offers a fine-grained matching
behavior that perceives the associations between the linguistic
features and two decoupled visual features to perform position
reasoning over a hierarchical space. Holi integrates features
of the two modalities by a cross-modal attention mechanism,
which suppresses the irrelevant redundancy under the guide of
positioning information from RoCo. Thus, with the incorporation
of RoCo and Holi modules, CPRN captures the visual details of
position reasoning so that the model can achieve more accurate
segmentation. To our knowledge, this is the first work that
explicitly focuses on position reasoning modeling. We also validate
the proposed method on three evaluation datasets. It consistently
outperforms existing state-of-the-art methods.
Index Terms—Referring Image Segmentation, Position Rea-
soning, Transformer.
I. INTRODUCTION
R
EFERRING image segmentation (RIS) aims to predict a
pixel-level segmentation mask in the image correspond-
ing to entity referred by the natural language expression. As
shown in Fig. 1, it can identify the entities of interest by
the description of free-form referring expressions, which are
not restricted to pre-defined object categories. RIS requires
the algorithms to explore the relationship between language
and vision so that the style of referred entity can be more
flexible than traditional segmentation tasks. Hence, RIS can
be regarded as an open-ended task and has a wide range of
potential applications in interactive image editing and human-
robot interaction, etc. It has attracted the attention of many
researchers in the intersection of vision and language.
Since RIS involves visual and linguistic domains, it is chal-
lenging especially in modeling the fine-grained interactions
Jianjian Cao is with School of Information Science and Technology, Fudan
University, Shanghai, China. (Email: jjcao22@m.fudan.edu.cn).
Beiya Dai is with Department of Computer Science, Notional University
of Defense Technology, Changsha, China. (Email: beiya dai@nudt.edu.cn).
Yulin Li, Xiameng Qin and Jingdong Wang are with Baidu Inc., Beijing,
China. (Email: liyulin03@baidu.com, qinxiameng@baidu.com, wangjing-
dong@baidu.com).
“The bowl with a spoon sticking out of it with brown frosting in it”
(b) Others
(a) Image
“grey shirt walking away”
(d) GT
(c) Ours
Fig. 1.
Comparison of visualization results between state-of-the-art
methods and our proposed approach. The first row shows that the existing
methods are prone to positioning errors on some Non-Salient targets with
the small-scale. The second row shows that for some complex referring
expressions, the reasoning ability of the previous models is not enough to
position the target accurately.
and aligning implicit relationships among the two modalities.
As shown in Fig. 2, existing works can be roughly divided into
four types according to the network structure. (a) A straight-
forward way to extract contextual knowledge to produce the
final result via a simple concatenation-convolution scheme
such as dynamic filters, LSTM and attention mechanism.
This solution [5], [6] aggregates the visual and linguistic
features without a deep understanding, which could not effec-
tively explore the relationships between the two modalities.
(b) Another line of works [9], [10] process each word in
the referring expression to learn cross-modal interaction in
a sequential manner. However, they consider each word as
an equal contribution. This may have trouble distinguishing
the target with long referring expressions. (c) Alternative
works [52], [53] establish several attributes (object, location
and relationship to other objects) of referring expressions
to improve the scores of cross-modal matching. This design
help refine the segmentation results but lacks global context
information and relies on the proposals generated by object
detectors. (d) At last, a series of works have been proposed
to progressively integrate contextual information at multiple
levels. LAVT [66] fuses the linguistic and visual features
into each stage of the network and captures segmentation
masks with a lightweight decoder. BRINet [34] considers
the interaction through a bi-directional cross-modal attention
module that uses both visual and textual guidances to capture
their dependencies, realizing the compatibility between vision
and textural features. They focuses on utilizing the inference
module to enhance the visual and textual interaction achieves
the best results. Nevertheless, all these methods do not directly
focus on the issue of position reasoning and fail to locates the
arXiv:2401.11775v1  [cs.CV]  22 Jan 2024
2
𝐼
𝐸
Encoder
Encoder
𝑉
𝐿
Fuse
Pre Mask
(a)
(b)
𝐼
Encoder
𝑉
𝐸
𝑤𝑜𝑟𝑑!
𝑤𝑜𝑟𝑑""
⋮
Encoder
𝐿!
𝐿""
⋮
Fuse
Fuse
⋮
Pre Mask
𝐼
𝐸
Encoder
Encoder
𝐿
Pre Mask
Fuse
𝐼
𝐸
Encoder
Encoder
𝑉!
𝐿!
𝑉#
⋮
𝐿#
⋮
Match
Pre Mask
⋮
(c)
(d)
𝑉#
𝑉!
⋮
Fuse
Fig. 2.
Conceptual Comparisons of typical architectures. (a) Simply combining the two modals of information. (b) Dividing the referring expression
and fusing it with visual features progressively. (c) Exploring the relationship between visual and textual features by matching their attributes. (d) Adopting
inference module in a multi-semantic-level progressively-fusion network architecture. I, E, V and L denotes image, natural language expression, visual
feature and texture feature, respectively.
referred entity from background.
Most previous works tackle the referring problem utilizing
efficient cross-modality feature interaction to explore semantic
contextual representations. Specifically, mainstream frame-
works firstly extract visual and linguistic features respectively,
and then introduce diverse operations to solve the interactive
learning. Although these methods have achieved remarkable
performance, the limitation of them is that the global context
modelings still lack sufficient fine-grained visual concepts
which is essential for position reasoning. Fig. 1 shows the vi-
sualization examples in which the segmentation masks remain
unsatisfactory because of the distraction of background and
non-salient objects. From the perspective of human cognition,
the RIS model focuses on positioning the entity regions that
well match the expressions, and then refine the precise seg-
mentation. The fine-grained semantic features help the model
distinguish the referred entity from other analogs.
Recently, the Transformer has achieved great success in the
area of Natural Language Processing and Computer Vision.
The state-of-the-art RIS methods introduce Transformer archi-
tectures to strengthen the ability of multi-modal feature fusion
and global information modeling. VLT [60] uses a transformer
to build a network with an encoder-decoder attention mecha-
nism to enhance global contextual information. LAVT [66]
utilizes the multi-stage design in the Swin Transformer to
form a hierarchical language-aware visual coding scheme.
CRIS [64] leverages the pre-trained model CLIP [59] and
contrastive learning strategy to achieve text-pixel alignment.
Although the transformer can bring a certain performance
improvement to the RIS model, the challenge of position
reasoning still exists and has not been well solved.
In this paper, we address the problem of position reasoning
in RIS and propose a Collaborative Positioning Reasoning
Network (CPRN) for leveraging the hierarchical context of
images for position reasoning. As illustrated in Fig. 3, in our
model, the features passed through two parallel pathways can
capture the local and global information for accurate local-
ization and fine-grained segmentation. In detail, the Row-and-
Cross-modal
Attention
RoCo Module
Holi Module
skier with googles and black out outfit
Input Image
Referring Expression
Segmentation Mask
(a)
(b)
Guided
Attention
Fig. 3. Illustration of the difference between (a) previous methods and
(b) our model. Previous works apply the holistic feature map in Cross-modal
Attention. Differently, we use two parallel branches. The above one divides the
holistic feature map into horizontal and vertical maps, and the lower one keeps
the holistic feature map. They are fused with the textual feature, respectively,
getting a better location for the referent.
Column interactive (RoCo) module generates the correlation
between horizontal and vertical feature maps with linguistic
features. The Guided Holistic interactive (Holi) module keeps
the holistic feature map to ensure the integrity of global infor-
mation. Meanwhile, a global guidance path directs the RoCo’s
positioning information into the Holi module to enhance entity
perception reasoning and suppress the irrelevant redundancy
from background. The output features of RoCo and Holi are
merged via a Feed Forward Network (FFN). Finally, we devise
a Multi-Scale decoder to aggregate multi-level features for
accurate referring segmentation.
In summary, this paper makes the following contributions:
• We propose a novel Collaborative Positioning Reasoning
Network (CPRN) to explicitly settle the position rea-
soning issue in RIS. And the proposed CPRN can be
used as a flexible block adaptable to any inference-based
framework.
• We propose a Row-and-Column interactive (RoCo) mod-
3
ule to explicitly locate the referent by dividing the holistic
feature map into row- and column-wise maps and inte-
grating them separately with textual features.
• We propose a Guided Holistic interactive (Holi) module
to retain a comprehensive perception of all pixels in an
image, for fine-grained segmentation. Furthermore, the
global guidance path is designed to enhance the local-
ization of Holi by incorporating the RoCo’s positioning
information.
• Extensive experiments on all three challenging datasets
show that the proposed CPRN plays an important role
in improving the positioning performance of referring
image segmentation. And our model achieves superior
performance compared to state-of-the-art methods.
II. RELATED WORK
A. Referring Image Segmentation.
Given an image and a natural language expression, the goal
of Referring image segmentation is to produce a segmen-
tation mask in the image corresponding to entities referred
by the natural language expression. The RIS task is firstly
introduced in [5], which directly concatenates both visual and
textual features to generate the final mask. RRN [6] considers
the multi-scale semantics in the visual encoding step and
employs ConvLSTM [16] in the feature fusion step. Later,
word attention [7] extracts keywords in the image regions to
suppress noises in the referring expression and highlight the
target object. RMI [9] directly combines visual features with
each word feature from a language LSTM to recurrently refine
segmentation results. DMNet [10] utilizes a dynamic filter for
each word to further enhances this interaction. Further, relation
inference is applied to capture visual and textual modalities.
With the application of the attention mechanism more and
more widely, some work uses the attention mechanism to
extract visual content corresponding to language expression.
STEP [8] emphasizes the attention from image to word by
computing dependencies between each visual region and each
word, to guide the segmentation recurrently. CMSA [15] is
exploited in respectively to capture global interaction infor-
mation between image regions and words via Cross-modal
self-attention. CMPC [58] firstly employs entity and attribute
words to perceive all the related entities. Then, the relational
words are adopted to highlight the correct entity, as well as
suppress other irrelevant ones by multi-modal graph reason-
ing. BRINet [34] uses both visual and linguistic guidances
to capture the dependencies between multi-modal features.
LSCM [29] models interaction between visual and textural
information under the guidance of DPT-WG [43]. ReSTR [63]
is the first convolution-free architecture for RIS, unifying
two different modal network topologies with Transformer.
CRIS [64] uses the pre-trained model CLIP [59] and contrast
learning strategies to achieve text pixel alignment. MaIL [65]
introduces a new modal information mask mode and designs a
simpler encoder-decoder pipeline and a mask-image-language
three-mode encoder. LAVT [66] fuse the linguistic and visual
features into each stage of the network and captures segmen-
tation masks with a lightweight decoder.
However, these works merely adopt holistic visual informa-
tion in multi-modal interaction, leading to inaccurate object
location. In this work, we introduce a collaborative position
reasoning method by row-and-column interaction, in addition
to holistic multi-interactive inference, and achieve satisfied
segmentation results.
B. Multi-modal Interaction.
A lot of multi-modal interaction researchers are interested
in combining natural language processing with visual under-
standing. At first, [44] demonstrates that if relevant data from
different modalities is available at training time, better features
can be learned. TFN [45], LMF [46], and T2FN [47] are
proposed to capture both intra- and inter-modal dynamics si-
multaneously. MulT [48] aligns data from different modalities
implicitly, which leverages cross-modal attention modules for
each modality on a high level, and each of them is responsible
for aligning the target modality vector with the complementary
modal vector. [42] introduces Auto-Fusion and GAN-Fusion
learning to compress information from different modalities
while preserving the context and GAN-Fusion regularizes
the learned latent space given context from complementing
modalities, making the network decide the fusion manner.
MCF [49] puts forward reshaping feature vectors into circulant
matrices and defining two types of interaction operations
between vectors and matrices. [50] realizes bidirectional multi-
layer fusion from both channel-level and pixel-level through
two fusion operations, which can strengthen the multi-modal
feature interactions across channels as well as enhance the
spatial feature discrimination. For a given query (image or
language),
[57] simply considers the keys and values from
all input tokens, it just merges the input from both ways,
this multi-modal attention is called Merge attention.
[32]
approach is that given a query from one modality (e.g.,
image), keys and values can only be obtained from another
modality (e.g., language), this multi-modal attention is called
co-attention. These methods for multi-modal interaction are
based on holistic feature maps.
In this paper, we designed a Row-and-Column interactive
(RoCo) module, which decompose the holistic feature map
and used row- and column-wise information to interact with
textual feature, respectively, to establish the local association
between visual and linguistic patterns.
C. Location Mechanism.
In semantic segmentation tasks, in addition to multimodal
feature fusion, the problem of locating reference images cannot
be ignored. At present, some work has been done in locating
target reference objects. [67] uses the prior extractor to extract
the prior, and then uses the before to generate a prior region
map of the query image, which is used to locate objects.
MCN [12] jointly learns two tasks, Citation Representation
Comprehension (REC) and Segmentation (RES). To address
the problem of conflicting predictions between the two tasks,
he proposed an adaptive soft non-localization suppression
(ASNLS) design, a post-processing method that suppresses
responses in irrelevant regions in the RES based on the
4
𝐹!
CPRN Block
“a baseball 
player about 
to hit a ball”
CPRN Block
……
Stage 1
𝑉!
Stage N
𝐿
𝑉""
𝐿
Multi-
Scale
Decoder
Cross-
Attention
Row-
Column
Aggregation
Roco
Holi
𝑉$
𝐿
𝑉$
𝐹$
Cross-
Attention
Q
K
V
Q
K
V
Guided
Attention
V
K
Q
Feed Forward
CPRN Block
⨂ Matrix Multiplication
⨁ Matrix Addition
⨀ Dot Product
Row Feature
Column Feature
𝐹""
𝑉: Image Feature
𝐿: Language Feature
𝐹: Fused Feature
⨁
⨂
⨀
…
𝐿
𝐿
Fig. 4.
The overall architecture of our method. The CPRN block directly fuses features from two modal inputs of image Vi and text L to generate
cross-modal feature representations. It includes RoCo interactive module and Holi interactive module. The multi-scale decoder generates segmentation results
based on the interactive representation of multi-modal features at different stages.
predictions of RECs. LTS [62] proposes a localization module
to obtain the corresponding visual content of the expression
and uses the obtained object prior as the visual localization
guidance for the subsequent segmentation module. Unlike the
localization module, which includes two forms of simple filters
and Transformers, the proposed CPRN block locates the visual
area that responds to the linguistic expression by the row-
column position information. There is a class of methods
to obtain object localization information by using additional
external sources, such as MAttNet [38] and lang2seg [13].
These two methods use Mask R-CNN [40] to pre-process
and post-process the image when segmenting the image. Al-
though Mask R-CNN provides localization and segmentation
of objects in images, greatly improving the performance of
the model, our collaborative positioning and reasoning net-
work performs much better on the three benchmark datasets,
demonstrating the superiority of our approach in positioning.
In addition, the idea of CCNet [14] is somewhat similar to
ours and it obtains dense contextual information for semantic
segmentation through two recurrent cross-attention (RCCA)
modules, which aggregates associated information via rows
and columns. Different from CCNet, we design two parallel
interactive modules, Roco and Holi, where Roco leverages the
row and column information to explicitly locate the referent,
and Holi utilizes the global image information for fine seg-
mentation.
III. METHODOLOGY
Fig. 4 illustrates the overall architecture of our Collaborative
Positioning Reasoning Network, which integrates the proposed
CPRN block for referring image segmentation. We first elabo-
rate on the motivation of our approach in Sec. III-A. Given an
image and a natural language expression as input, we extract
the visual and linguistic features on different semantic levels,
respectively (Sec. III-B). Then, they are fused and fed into
the CPRN inference block (Sec. III-C), which is composed
of two modules, to highlight the referent entities. One is the
Row-and-Column interactive (RoCo) module, the other is the
Guided Holistic interactive (Holi) module. After that, the two
pathways are merged using a Feed Forward Network (FFN)
to enhance the reasoning features. Finally, the Multi-Scale
Decoder module (Sec. III-D) is used to perform the different
stage feature fusion and refine the final segmentation mask.
A. Motivation
It is essential for RIS task to mine relation information
between vision and language via feature interaction. Some
works [6], [15], [29] consider the multi-scale information
to find the referent. Since they only consider holistic visual
information, inappropriate segmentation results exist. A main
problem is that they cannot accurately locate the object.
We propose the Collaborative Positioning Reasoning Network
(CPRN) utilizing two parallel pathways to sufficiently aggre-
gate object position (RoCo module) while capturing holistic
information (Holi module) between visual and textual modali-
ties, as illustrated in Fig. 4. For solving the referent positioning
issue, we decompose the visual feature into row- and column-
wise features, which will interact with the textual features
separately, to locate the object in both horizontal and vertical
directions. The multi-modal features of the two directions
will assign the location of the referent object. Meanwhile, the
positioning effect of the RoCo module will also guide the Holi
module, helping it to more accurately locate and segment the
referent. By the mutual enhancement between the RoCo and
Holi modules, our method can perform reliable joint reasoning,
5
⨂
𝑀𝑎𝑠𝑘!""#""
⨂
Row-Column
Aggregation
Roco
Q
K
V
conv
conv
𝐿
conv
conv
𝐿
Q
K
V
𝑉!
𝑣%
Holi
𝑉!
conv
conv
𝐿
Q
K
softmax
…
…
𝐹!
Global Guidance
𝑣&
𝑊×𝑇
𝜎(𝐻)
𝜎(𝑇)
𝜎(𝑇)
𝜎(𝑊)
Expand
𝐻×𝑇
Expand
Feed Forward
⨁
⨂
⨂
⨂
⨂
⨂
⨀
𝑀𝑎𝑠𝑘$""%&
𝑀𝑎𝑠𝑘!""$""
Fig. 5. Illustration of our Collaborative Positioning Reasoning Network (CPRN). First, the visual features Vi generate vertical features and horizontal
features via the Row-Column Aggregation. These two visual features are obtained through cross attention layers and it generates the multi-modal features
with semantic information vhw
h
and vhw
w . Then, the final output of RoCo module are obtained through expand and addition operations. At the same time,
maskroco with positioning information is calculated, which guides the Holi module through the global guidance pathway for fine-grained segmentation. The
Holi module utilizes Maskroco and Maskholi to generate the final output via a designed Guided Attention layer. Expand represents bilinear interpolation
operation, and for convenience of representation, we ignore some residual connections.
which greatly improves the localization and segmentation of
referent entities.
B. Feature Extraction
Taking an image I and a referring expression E with T
words as input, we firstly use the Swin Transformer [23] to ex-
tract visual features at different stages. Let Vi ∈ RHi×Wi×Ci,
i ∈ {2, 3, 4, 5}, denotes the visual features, corresponding
to the 1st, 2nd, 3rd and 4th stages of Swin Transformer
network, where Hi, Wi and Ci are the dimensions of height,
width, and visual feature channels, respectively. Besides, the
spatial coordinate features are used to capture more spatial
information. For each stage, we also define an 8-D spatial
coordinate feature denoted as Pi ∈ RHi×Wi×8, i ∈ {2, 3, 4, 5}
at each spatial position as the implementation in [9]. Then, a
new fused visual feature Vi is obtained by concatenating the
visual feature Vi and the spatial coordinate feature Pi followed
by a 1×1 convolution layer. We denote a single level of fused
visual features as V for ease. Next, the linguistic features
L = {L1, L2, ..., LT }, Li ∈ Rdl, i ∈ {1, 2, ..., T} is extracted
with a language encoder BERT [24], where dl and T denote
the number of channels and the number of words. After that,
the visual features V and linguistic features L are fed into
our proposed Collaborative Positioning Reasoning Network
(CPRN) which depicted in Sec. III-C.
C. Collaborative Positioning Reasoning Network
Rather than fusing the visual and linguistic features directly
in previous works, the proposed Collaborative Positioning
Reasoning Network (CPRN) pays more attention to the posi-
tioning of referent entities and designed the Row-and-Column
interactive (RoCo) module, which realizes the positioning
target by perceiving the row- and column-wise local features
of the image. As illustrate in Fig. 5, it also designs the Guided
Holistic interactive (Holi) module, which realizes the accurate
segmentation of referents by perceiving the global features of
the image. Furthermore, the Feed Forward Network (FFN) is
designed to merge the two parallel pathways, enabling joint
reasoning, making the features used for final segmentation
more reliable.
1) Row-and-Column interactive module: In the positioning
path, the network firstly decomposes the visual feature map
Vi into two parts, i.e. a row-wise feature and a column-
wise feature, corresponding to the horizontal and the vertical
directions, respectively. After that, it leverages to interact
among the two visual features and the linguistic features, as
shown in Fig. 5. For the convenience of representation, we
remove the i subscript of all variables.
Specifically, the row-wise and column-wise visual features
are obtained by the Row-Colunm Aggregation operations
which execute average pooling on V, with pooling kernel
of size 1 × W and H × 1. And each of these two features
after pooling is implemented with 1×1 convolution layer and
followed by the GeLU function adding nonlinearity:
vh = GeLU
6
RW ×Cw denotes the column-wise feature, Ch = Cw = C. H
is the height of vertical feature, W represents the width of
horizontal feature, Ch and Cw define the number of channels.
From the linguistic feature L, it generates word vectors,
wordhk ∈ RT ×dh, wordhv ∈ RT ×dh, wordwk ∈ RT ×dw,
wordwv ∈ RT ×dw, through four 1×1 convolution layers:
wordhk = w2
hk (L) + b2
hk,
wordhv = w2
hv (L) + b2
hv,
wordwk = w2
wk (L) + b2
wk,
wordwv = w2
wv (L) + b2
wv.
(2)
Then, those word vectors are passed to the row-wise infer-
ence and the column-wise inference branches, respectively, to
fully capture the two directional interactions. In detail, it feeds
the row-wise and column-wise visual features and the corre-
sponding word vectors into two cross-attention mechanisms
separately, to calculate the language perception of the row-and-
column level pixels in the image. Since the implementations
of these two cross-attention mechanisms are the same, for
simplicity, we only take the row-wise inference as an exam-
ple. Specifically, the common cross-attention mechanisms are
utilized to learn the row-wise influence by feeding the vertical
visual feature vh to query the linguistic feature wordhk
and generate the vertical linguistic feature. Attention is the
simple Scaled Dot-Product Attention mechanism and can be
expressed by:
Attention(Q, K, V ) = softmax
QK⊤
√dk

V.
(3)
After obtaining the vertical linguistic feature which have
the same shape as vh, we combine them to produce a set
of vertical multi-modal feature maps vAtt
h
via element-wise
multiplication. Formally,
vAtt
h
= Attention (vh, wordhk, wordhv) ⊙ vh,
(4)
where vAtt
h
∈ RH×Ch, ⊙ denotes element-wise multi-
plication. In the same way, we use another cross-attention
layer to generate the horizontal multi-modal feature maps
vAtt
w
∈ RW ×Cw. Finally, we use the Bilinear Interpolation to
resize vh, vw, vAtt
h
, vAtt
w
to the scale of the original image,
which are added up as the output of the Row-and-Column
interactive module for further fusion:
vall
hw = B (vh) + B (vw) + B
7
where ∗ denotes matrix multiplication and ⊙ denotes element-
wise multiplication.
3) Merging two pathways: In the following steps, a Feed
Forward Network is utilized to fuse the outputs of these
two parallel branches. Firstly, we use two convolution layers
followed by ReLU nonlinearity to perform feature projection
on vall
hw and vall
g
and mathematically described as follows
Fhw = ReLU
8
TABLE I
COMPARISON WITH STATE-OF-THE-ART METHODS ON THREE BENCHMARK DATASETS USING overall IoU AS METRIC. U: THE UMD SPLIT. G: THE
GOOGLE SPLIT.
RefCOCO
RefCOCO+
Gref
val
test A
test B
val
testA
testB
val (U)
test (U)
val(G)
RRN [6]
55.33
57.26
53.93
39.75
42.15
36.11
-
-
36.45
CSMA [15]
58.32
60.61
55.09
43.76
47.60
37.89
-
-
39.98
BRINet [34]
60.98
62.99
59.21
48.17
52.32
42.11
-
-
48.04
CMPC [58]
61.36
64.53
59.64
49.56
53.44
43.23
-
-
49.05
LSCM [29]
61.47
64.99
59.55
49.34
53.12
43.50
-
-
48.05
EFN [11]
62.76
65.69
59.67
51.50
55.24
43.01
-
-
51.93
BUSNet [61]
63.27
66.41
61.39
51.76
56.87
44.13
-
-
50.56
VLT [60]
65.65
68.29
62.73
55.50
59.20
49.36
52.99
56.65
49.76
LTS [62]
65.43
67.76
63.08
54.21
58.32
48.02
54.40
54.25
-
ReSTR [63]
67.22
69.30
64.45
55.78
60.44
48.27
54.48
-
-
CRIS [64]
70.47
73.18
66.10
62.27
68.08
53.68
59.87
60.36
-
LAVT [66]
72.73
75.82
68.79
62.14
68.38
55.10
61.24
62.09
60.50
CPRN (Ours)
73.42
76.65
70.84
63.58
69.44
55.84
62.81
64.25
60.92
TABLE II
ABLATION STUDIES ON REFCOCO VALIDATION SET. “&” AND “∥” REPRESENT THE SERIES AND PARALLEL CONNECTION OF ROCO MODULE AND HOLI
MODULE, RESPECTIVELY. ”HOLI*” REPRESENTS A SIMPLE CROSS ATTENTION MECHANISM, AND ”HOLI” REPRESENTS OUR PROPOSED GUIDED HOLI
INTERACTIVE MODULE.
Method
P@0.5
P@0.6
P@0.7
P@0.8
P@0.9
Overall IoU
Mean IoU
baseline (Holi*)
83.26
79.31
73.38
62.29
32.36
71.99
73.10
RoCo
79.27
74.68
67.35
50.45
15.11
66.63
67.61
RoCo & Holi*
80.61
75.81
67.92
50.98
17.58
68.57
69.02
RoCo ∥ Holi*
84.56
80.65
75.10
64.21
33.90
72.79
74.29
RoCo ∥ Holi
84.58
81.21
75.91
64.28
34.04
72.96
74.48
+FFN
84.66
81.49
76.21
65.23
35.00
73.12
74.60
+ape (CPRN)
85.09
81.71
76.54
65.53
35.26
73.42
75.00
Pre@X as our evaluation metrics. Given the predicted seg-
mentation mask and the ground truth, the Overall IoU metric
is the ratio between the intersection and the union of the two,
which is calculated by dividing the total intersection area by
the total union area. Both intersection area and union area are
accumulated over all test samples. The Pre@X measures the
percentage of test examples that have IoU score higher than the
threshold X. In our experiments, X ∈ {0.5, 0.6, 0.7, 0.8, 0.9}.
Implementation Details: Given an input image, we resize
it to 480 × 480 and adopt Swin Transformer [23] pre-trained
on ImageNet-22K dataset [54] as our backbone, following
previous works [66]. On all three benchmark datasets, we keep
the maximum length of query expression as 20. The language
model we use is a BERT [24] model with 12 layers, a hidden
size of 768, and is initialized with official pre-trained weights.
The number of inference stage N is equal to 4. Our model
is optimized with a binary cross-entropy loss, and we employ
the AdamW optimizer [37] with a weight decay of 0.01. We
employ a learning rate schedule with an initial learning rate
set to 5e−5 and a polynomial learning rate decay. We use the
batch size of 32 and train on 8 Tesla V100 with 16 GPU
VRAM. During inference, we upsample the prediction results
back to the original image size and use argmax to select the
index on the channel dimension of the score map, no other
post-processing operations are required.
B. Quantitative Results
Comparison with State-of-the-arts: To demonstrate the
superiority of our CPRN, we compare it with state-of-the-
art methods, including RRN [6], CSMA [15], BRINet [34],
CMPC [58], LSCM [29], EFN [11], BUSNet [61], VLT [60],
LTS [62], ReSTR [63], CRIS [64] and LAVT [66] on three
RIS benchmarks. The results of the comparison with other
methods on three datasets using overall IoU as metrics are
9
Expression:   A dog with brown fur, with its head up, laying on a gray sheet.
Expression:   The bowl with a spoon sticking out of it with brown frosting in it
Expression:   A boy with a white shirt with teal sleeves
Expression:   A skier wearing green pants and blue jacket
(a) Original Image
(b) RoCo_atten_map
(c) Holi_atten_map
(e) Segmentation
(f) Ground Truth
(d) RoHo_atten_map
Fig. 6.
Visualization of the attention map of the Guided Attention layer. (a) Original image. (b) RoCo Attention map. (c) Holi Attention map. (d) RoHo
Attention map. (e) Segmentation. (f) Ground-truth
（a) Original Image
(b) LAVT
(c) CPRN
(d) Ground Truth
“The man whipping the elephant”
“The black cat only”
“A woman in a black dress with a red shawl standing next to a man”
（a) Original Image
(b) LAVT
(c) CPRN
(d) Ground Truth
“A boy in a yellow t-shirt runs with a frisbee in a farm's empty field”
“A man performing skateboard trick”
“a boy wearing blue t-shirt standing near a table with his friends celebrating birthday party”
Fig. 7. Visualization of comparisons between LAVT and CPRN segmentation results on the small-scale object and complex language set from the Gref-umd
validation set. (a) Original image. (b) LAVT. (c) CPRN. (d) Ground-truth.
displayed in Tab. I. It can be seen that our method consistently
outperforms all previous methods on three benchmarks. In
detail, CPRN yields an average improvement of 7.53% over
ReSTR on all three datasets. Similarly, it achieves a signifi-
cant improvement in the range of 1.36%-4.71% compared to
CRIS, which had previously achieved the best performance. In
particular, our method also greatly outperforms LAVT using
the same backbone, which fully illustrates the advantages
of our proposed CPRN block. Specifically, on the refcoco+
dataset, our method exceeds LAVT by 1.44% on val split,
1.06% on testA split, and 0.74% on testB split. Since this
dataset does not contain location information in referring
expressions, it turns out that our CPRN block can enhance the
ability to locate referents more than other methods that lack
explicit modeling of positioning. According to Tab. I, the gains
are more obvious on other datasets than on the RefCOCO+
dataset, indicating that the ability of the CPRN block to locate
language-responsive regions has a great impact on improving
model performance. More importantly, compared to the sota
model, our network achieves 1.57% and 2.16% improvement
on Gref-umd validation and test sets. Note that the referring
expressions in the Gref dataset are generally longer, indicating
that CPRN can better handle complex long sentences by
positioning interactions between visual and linguistic features.
10
“Person in orange coat standing beside a truck with two men standing in front of him”
“A skateboarder riding a pipe”
Stage4
Stage3
Stage2
Stage1
“A dog with brown fur, with its head up, laying on a gray sheet”
Ground Truth
Segmentation
Original Image
“A Krispy Creme coffee cup that is filled nearly to the brim”
Fig. 8.
Visualization of segmentation and feature map at different stages from the Gref-umd validation set. The left-most column shows the original image,
and the right-most column illustrates the predicted mask and the ground truth mask.
(c) LAVT
“man on tv”
“back to us white 
shirt”
“balding man with vest”
（a) Original Image
(b) LSCM
(d) CPRN
(e) Ground Truth
“skier with googles and black out outfit”
(c) LAVT
（a) Original Image
(b) LSCM
(d) CPRN
(e) Ground Truth
“baby with brush”
“black coat looking at suitcase black hat”
“man in jeans out of focus”
“the umpire in blue shirt standing”
“man with skateboard”
“man on the floor”
Fig. 9. Visualization of comparisons between LSCM, LAVT and CPRN segmentation results from the RefCOCO+ testA set. (a) Original image. (b) LSCM.
(c) LAVT. (d) CPRN. (e) Ground-truth.
11
TABLE III
COMPARISON WITH LAVT ON THE GREF-UMD VALIDATION SET ON A SMALL DATASET OF SMALL SCALE OBJECTS AND COMPLEX LANGUAGES.
Methods
P@0.5
P@0.6
P@0.7
P@0.8
P@0.9
Overall IoU
Mean IoU
small scale
LAVT
43.96
36.24
28.32
18.81
2.57
28.54
41.63
CPRN
48.91
40.20
33.27
20.99
1.78
31.65
45.45
complex language
LAVT
69.08
63.77
57.00
44.69
21.74
58.26
61.65
CPRN
72.46
70.29
64.73
52.66
25.12
61.88
64.77
TABLE IV
EXPERIMENTS WITH FIVE FEATURE COMBINATION WAY OF ROCO
MODULE ON REFCOCO VALIDATION SET.
Methods P@0.5 P@0.7 P@0.9 Overall IoU Mean IoU
f1
83.78
74.81
34.54
72.46
73.90
f2
84.55
75.00
34.49
73.21
74.28
f3
84.83
76.38
34.08
72.93
74.75
f4
85.07
76.66
34.93
73.17
74.83
CPRN
85.09
76.54
35.24
73.42
75.00
In the end, the above comparison with existing methods fully
demonstrates the superiority of our method.
C. Ablation Studies
To investigate the relative contribution of each component in
the proposed modules and the localization ability of the CPRN
block, we conduct a series of ablation experiments on the
RefCOCO dataset and evaluate it in the validation set, which
is illustrated in Tab. II. In addition, we study the combination
of row- and column-wise features in RoCo module, such as
Tab. IV. Furthermore, to verify that our CPRN block is more
effective on some non-salient objects with small scale and
complex language expressions, we also conduct a plenty of
experiments on the Gref-umd dataset and use overall IoU,
mean IoU and Pre@X as evaluation metrics, such as Tab. III.
Effective of RoCo module and Holi module.
To in-
vestigate the contribution of the Row-and-Column interactive
(RoCo) module and Guided Holistic interactive (Holi) module
to the overall model performance, we design four sets of
ablation experiments. The baseline network is built with a
simple holistic module (Holi* module) which only contains
a cross-attention layer. We analyze: (1) Holi* module(Holi*),
(2) RoCo module(RoCo), (3) RoCo module and Holi* module
are combined in series(RoCo & Holi*), (4) RoCo module and
Holi* module are combined in parallel(RoCo ∥ Holi*), (5)
RoCo module and Holi module merged in two pathways(RoCo
∥ Holi). The results in Tab. II show that since Holi* only
uses a simple cross-attention mechanism, the segmentation
performance of the model is not excellent. Due to the RoCo
module is only responsible for learning the modeling of
positioning referents and lacks the perception of the global
information of images, the result of the individual Roco
module is poor. In addition, we try the combination of the
RoCo module and Holi* module in simple series and simple
parallel, and both of the model performances do not improve
greatly. We also analyzed the reasons for the poor performance
of serial combination. This is because the visual features are
multi-modal interactively fused with language features in the
RoCo module, and then sent into the Holi* module, the global
information of the original visual features will be destroyed.
For the parallel method, the model performance can already
exceed using Holi* module alone. Based on Holi module and
RoCo module in parallel, our proposed CPRN has a better
ability to localize referring entities than the scheme only
using single Holi* module, scheme only using single Roco
module, simple series scheme, and simple parallel scheme.
In order to further improve the performance, we have made
some improvements, adding absolute postion embedding(ape)
to the visual features and adding the FFN network layer after
the combination of the RoCo module and the Holi module.
Through experimental verification, these improvements will
bring certain improvements to the model performance.
Performance on small-scale objects and complex lan-
guage expressions. In order to demonstrate the effectiveness
of our CPRN block, especially the positioning ability of
small-scale non-salient objects, and the joint reasoning ability
of complex language expressions, we constructed two small
datasets on the Gref-umd dataset for further ablation studies.
In detail, we use thresholds of 0.03 and 18 as criteria for
segmenting small-scale objects and complex language expres-
sions, separately. We consider data with the mask rate less than
0.03 as small-scale objects, and the proportion of this small
dataset is 10.31%. Besides, We consider data with language
expression length longer than 18 after tokenizer as complex
language queries, and this small dataset accounts for 8.46%.
The result can be seen from Tab. III that CPRN outperforms
the sub-optimal method LAVT by absolute advantage on
those two reconstructed datasets with small-scale objects and
complex languages, and is higher than the Overall IoU 3.11%
and 3.62%. The visualization of these two small datasets
also shows the advantage of our model in Fig. 7. The left
column is the visualization of small-scale objects, and the right
column is the visualization of complex language expressions.
The visualization clearly illustrate that our model has great
advantages, especially in solving the segmentation problem of
small-scale objects and complex language expressions in the
RIS task.
The combination of row- and column-wise features. In
order to verify the combination of row- and column-wise muti-
12
modal features, we designed four different fusion ways to
generate vall
hw, which are the functions f1, f2, f3 and f4 as as
follows:
f1 =
13
[13] Chen, Y. W. and Tsai, Y. H. and Wang, T. and Lin, Y. Y. and Yang,
M. H, “Referring Expression Object Segmentation with Caption-Aware
Consistency,” in BMVC, 2019.
[14] Huang, Zilong and Wang, Xinggang and Huang, Lichao and Huang,
Chang and Wei, Yunchao and Liu, Wenyu, “Ccnet: Criss-cross attention
for semantic segmentation,” in ICCV, 2019.
[15] Ye, Linwei and Rochan, Mrigank and Liu, Zhi and Wang, Yang, “Cross-
Modal Self-Attention Network for Referring Image Segmentation,” in
CVPR, 2019.
[16] Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit
Yan and Wong, Wai Kin and Woo, Wang Chun, “Convolutional LSTM
Network: A Machine Learning Approach for Precipitation Nowcasting,”
in CoRR, 2015.
[17] Long, Jonathan and Shelhamer, Evan and Darrell, Trevor, “Fully Convo-
lutional Networks for Semantic Segmentation,” in IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 39, no. 4, pp. 640-651,
2015.
[18] Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian,
“RefineNet: Multi-path Refinement Networks for High-Resolution Se-
mantic Segmentation,” in IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.
[19] Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto, “Seg-
Net: A Deep Convolutional Encoder-Decoder Architecture for Image
Segmentation,” in IEEE Transactions on Pattern Analysis & Machine
Intelligence, pp. 1-1, 2017.
[20] Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang,
Xiaogang and Jia, Jiaya, “Pyramid Scene Parsing Network,” in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
[21] Hang Zhang and Kristin J. Dana and Jianping Shi and Zhongyue Zhang
and Xiaogang Wang and Ambrish Tyagi and Amit Agrawal, “Context
Encoding for Semantic Segmentation,” in CoRR, 2018.
[22] Chen, L. C. and Papandreou, G and Kokkinos, I and Murphy, K
and Yuille, A. L, “DeepLab: Semantic Image Segmentation with Deep
Convolutional Nets, Atrous Convolution, and Fully Connected CRFs,”
in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.
40, no. 4, pp. 834-848, 2018.
[23] Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and
Zhang, Zheng and Lin, Stephen and Guo, Baining, “Swin transformer:
Hierarchical vision transformer using shifted windows,” in ICCV, 2021.
[24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,
“BERT: pre-training of deep bidirectional transformers for language
understanding,” in NAACL, 2019.
[25] Chen, Liang Chieh and Papandreou, George and Schroff, Florian and
Adam, Hartwig, “Rethinking Atrous Convolution for Semantic Image
Segmentation,” in CoRR, 2017.
[26] Hochreiter and Sepp and Schmidhuber and Jurgen, “Long short-term
memory,” in Neural Computation, 1997.
[27] Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu,
Oana and Murphy, Kevin, “Generation and Comprehension of Unam-
biguous Object Descriptions,” in IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016.
[28] Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg,
Tamara, “ReferItGame: Referring to Objects in Photographs of Natural
Scenes,” in Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), 2014.
[29] Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang,
and Jizhong Han , “Linguistic structure guided context modeling for
referring image segmentation,” in European Conference on Computer
Vision, 2020.
[30] Yu, Licheng and Poirson, Patric and Yang, Shan and Berg, Alexander
C. and Berg, Tamara L, “Modeling Context in Referring Expressions,”
in ECCV, 2016.
[31] Escalante, Hugo Jair and Hernandez, Carlos A. and Gonzalez, Jesus A.
and Lopez-Lopez, A. and Montes, Manuel and Morales, Eduardo F. and
Sucar, L Enrique and Villasenor, Luis and Grubinger, Michael, “The
segmented and annotated IAPR TC-12 benchmark,” in Computer Vision
and Image Understanding, vol. 114, no. 4, pp. 419-42, 2010.
[32] Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan, “Vilbert:
Pretraining task-agnostic visiolinguistic representations for vision-and-
language tasks,” in Advances in neural information processing systems,
2019.
[33] Hui, Tianrui and Liu, Si and Huang, Shaofei and Li, Guanbin and Han,
Jizhong, “Linguistic Structure Guided Context Modeling for Referring
Image Segmentation,” in ECCV, 2020.
[34] Hu, Zhiwei and Feng, Guang and Sun, Jiayu and Zhang, Lihe and Lu,
Huchuan, “Bi-Directional Relationship Inferring Network for Referring
Image Segmentation,” in CVPR, 2020.
[35] Everingham, Mark and Gool, Luc Van and Williams, Christopher K.
I. and Winn, John and Zisserman, Andrew, “The Pascal Visual Object
Classes (VOC) Challenge,” in International Journal of Computer Vision,
vol. 88, no. 2, pp. 303-338, 2010.
[36] Krhenb¨uhl, Philipp and Koltun, Vladlen, “Efficient Inference in Fully
Connected CRFs with Gaussian Edge Potentials,” in Curran Associates
Inc., 2012.
[37] Loshchilov, I. and Hutter, F., “Decoupled Weight Decay Regularization,”
in ICLR, 2019.
[38] Yu, Licheng and Lin, Zhe and Shen, Xiaohui and Yang, Jimei and Lu,
Xin and Bansal, Mohit and Berg, Tamara L, “MAttNet: Modular Atten-
tion Network for Referring Expression Comprehension,” in IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2018.
[39] Ben-Younes, Hedi and Cadene, R´emi and Cord, Matthieu and Thome,
Nicolas, “MUTAN: Multimodal Tucker Fusion for Visual Question
Answering,” in ICCV, 2017.
[40] He, Kaiming and Gkioxari, Georgia and Piotr Doll´ar and Girshick, Ross,
“Mask R-CNN,” in IEEE Transactions on Pattern Analysis & Machine
Intelligence, 2017.
[41] Hou, Qibin and Zhang, Li and Cheng, Ming Ming and Feng, Jiashi,
“Strip Pooling: Rethinking Spatial Pooling for Scene Parsing,” in CVPR,
2020.
[42] Sahu, Gaurav and Vechtomova, Olga, “Adaptive Fusion Techniques for
Multimodal Data,” in EACL, 2021.
[43] Chen, Danqi and Manning, Christopher, “A Fast and Accurate De-
pendency Parser using Neural Networks,” in Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing
(EMNLP), 2014.
[44] Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan
and Ng, Andrew Y, “Multimodal Deep Learning,” in International
Conference on Machine Learning, 2009.
[45] Amir Zadeh and Minghai Chen and Soujanya Poria and Erik Cambria
and Louis-Philippe Morency, “Tensor Fusion Network for Multimodal
Sentiment Analysis,” in CoRR, 2017.
[46] Liu, Zhun and Shen, Ying and Lakshminarasimhan, Varun Bharadhwaj
and Liang, Paul Pu and Zadeh, Amir and Morency, Louis Philippe,
“Efficient Low-rank Multimodal Fusion with Modality-Specific Factors,”
in Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), 2018.
[47] Liang, Paul Pu and Liu, Zhun and Tsai, Yao Hung Hubert and Zhao,
Qibin and Salakhutdinov, Ruslan and Morency, Louis Philippe, “Learn-
ing Representations from Imperfect Time Series Data via Tensor Rank
Regularization,” in CoRR, 2019.
[48] Tsai, Yao Hung Hubert and Bai, Shaojie and Liang, Paul Pu and
Kolter, J. Zico and Salakhutdinov, Ruslan, “Multimodal Transformer
for Unaligned Multimodal Language Sequences,” in CoRR, 2019.
[49] Aming Wu , Yahong Han, “Multi-modal Circulant Fusion for Video-to-
Language and Backward,” in International Joint Conference on Artificial
Intelligence, 2018.
[50] Yikai Wang and Fuchun Sun and Ming Lu and Anbang Yao, “Learning
Deep Multimodal Feature Representation with Asymmetric Multi-layer
Fusion,” in ACMMM, 2020.
[51] Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and
Polosukhin, Illia, “Attention Is All You Need,” in CoRR, 2017.
[52] Licheng Yu and Zhe Lin and Xiaohui Shen and Jimei Yang and Xin Lu
and Mohit Bansal and Tamara L. Berg, “MAttNet: Modular Attention
Network for Referring Expression Comprehension,” in CoRR, 2018.
[53] Chen, Yi Wen and Tsai, Yi Hsuan and Wang, Tiantian and Lin, Yen
Yu and Yang, Ming Hsuan, “Referring Expression Object Segmentation
with Caption-Aware Consistency,” in BMVC, 2019.
[54] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-
Fei, “ImageNet: A Large-Scale Hierarchical Image Database,” in CVPR,
2009.
[55] Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas, “U-Net:
Convolutional Networks for Biomedical Image Segmentation,” in IEEE
Access,vol. PP, no. 99, pp. 1-1, 2015.
[56] Escalante, Hugo Jair and Hern´andez, Carlos A and Gonzalez, Jesus A
and L´opez-L´opez, Aurelio and Montes, Manuel and Morales, Eduardo
F and Sucar, L Enrique and Villasenor, Luis and Grubinger, Michael,
“The segmented and annotated IAPR TC-12 benchmark,” in Computer
vision and image understanding,vol. 114, no. 4, pp. 419–428, 2010.
[57] Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed
and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing,
“Uniter: Universal image-text representation learning,” in ECCV, 2020.
14
[58] Shaofei Huang and Tianrui Hui and Si Liu and Guanbin Li and Yunchao
Wei and Jizhong Han and Luoqi Liu and Bo Li, “Referring Image
Segmentation via Cross-Modal Progressive Comprehension,” in CVPR,
2020.
[59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel
Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin,
Jack Clark, “Learning transferable visual models from natural language
supervision,” in ICML, 2021.
[60] Ding, Henghui and Liu, Chang and Wang, Suchen and Jiang, Xudong,
“Vision-language transformer and query generation for referring seg-
mentation,” in ICCV, 2021.
[61] Yang, Sibei and Xia, Meng and Li, Guanbin and Zhou, Hong-Yu
and Yu, Yizhou, “Bottom-up shift and reasoning for referring image
segmentation,” in ICCV, 2021.
[62] Jing, Ya and Kong, Tao and Wang, Wei and Wang, Liang and Li, Lei
and Tan, Tieniu, “Locate then segment: A strong pipeline for referring
image segmentation,” in CVPR, 2021.
[63] Kim, Namyup and Kim, Dongwon and Lan, Cuiling and Zeng, Wenjun
and Kwak, Suha, “ReSTR: Convolution-free Referring Image Segmen-
tation Using Transformers,” in CVPR, 2022.
[64] Wang, Zhaoqing and Lu, Yu and Li, Qiang and Tao, Xunqiang and
Guo, Yandong and Gong, Mingming and Liu, Tongliang, Suha, “Cris:
Clip-driven referring image segmentation,” in CVPR, 2022.
[65] Li, Zizhang and Wang, Mengmeng and Mei, Jianbiao and Liu, Yong,
“MaIL: A Unified Mask-Image-Language Trimodal Network for Refer-
ring Image Segmentation,” in arXiv, 2021.
[66] Yang, Zhao and Wang, Jiaqi and Tang, Yansong and Chen, Kai and
Zhao, Hengshuang and Torr, Philip HS, “LAVT: Language-Aware Vision
Transformer for Referring Image Segmentation,” in CVPR, 2022.
[67] Liu, Weide and Wu, Zhonghua and Ding, Henghui and Liu, Fayao and
Lin, Jie and Lin, Guosheng, “Few-shot segmentation with global and
local contrastive learning,” in arXiv, 2021.
"
"This research paper presents an open-source python package called voice-cloning for aiding people with speech disorders in communicating more effectively, as well as assisting professionals integrating voice cloning into their projects. The package consists of a speaker verification system, synthesizer, and vocoder. The speaker verification system was trained on a diverse set of speakers to optimize generalization performance. The synthesizer was trained using a combination of audio, transcriptions to generate Mel spectrogram from text. Finally, the vocoder converts the generated Mel Spectrogram into a corresponding audio signal. The performance of synthesized speech from unseen speakers was evaluated using subjective and objective evaluation methods. The results demonstrated that the model can create speech in different voices by incorporating speaker characteristics chosen randomly.","Voice cloning is a process of creating a synthetic or artificial version of someone's voice, often using text-to-speech (TTS) technology and deep learning algorithms. Advances in TTS have made it possible to generate speech in the voice of different speakers, offering potential benefits for individuals with speech disorders, those seeking to automate tasks like customer service, and professionals integrating voice cloning capabilities into their projects.","nanVoice cloning techniques typically fall into one of two main categories: speaker adaptation approaches and speaker encoder approaches. Speaker adaptation involves refining a multi-speaker speech synthesis model with the speech of a new speaker. However, this approach can lead to overfitting with limited data. Research has proposed addressing this issue by breaking down the adaptation process into multiple modules. Other work has explored conditional layer normalization to reduce the number of fine-tuning parameters. In addition to speaker adaptation and encoder methods, there have been advancements in neural network-driven TTS synthesis systems, including combining RNNs and CNNs to produce synthetic speech from textual input.nannan","The proposed neural TTS system comprises three essential models: the encoder, synthesizer, and vocoder. The encoder, utilizing WaveNet, performs speaker verification and captures distinct voice characteristics. It was trained on various datasets covering different accents, including Indian and Western accents. The synthesizer generates Mel spectrograms from input text and conditions them with speaker embeddings. The vocoder converts Mel spectrograms into waveforms using a suitable generative model. All three models were trained on data from both Indian and Western accents. The trained models are packaged into a Python package named ""Voice-Cloning 0.0.9,"" allowing users to engage in voice cloning and speech synthesis with ease.nan","The evaluation of the synthesized speech involved subjective measures such as Mean Opinion Score (MOS), as well as objective measures such as Gross Pitch Error (GPE) and Spectral Distortion (SD). The subjective evaluation was conducted by human listeners, who rated the similarity of the speaker's voices on a scale ranging from ""not at all similar"" to ""very similar."" The objective evaluation was applied to unseen speakers, having Western and Indian accents. The results showed that the proposed system achieved comparable or better naturalness and similarity compared to existing work. The proposed work also demonstrated lower GPE and SD scores, indicating improved accuracy in replicating pitch and spectral characteristics of the original speech.","The paper concludes by highlighting the development of a speech synthesis and voice cloning package consisting of a speaker verification system, synthesizer, and vocoder. The package enables users to synthesize speech, perform real-time voice cloning, and utilize multi-accent support. The incorporated noise reduction algorithm further enhances the quality of the output audio signal. The subjective and objective evaluation results indicate that the proposed system closely resembles a person's natural voice, providing support for those with speech disorders and new possibilities for professionals integrating voice cloning capabilities into their projects. Future work in voice cloning could explore enhancing the expressiveness and emotional range of synthesized voices.",Advancing Accessibility: Voice Cloning and Speech Synthesis for Individuals with Speech Disorders,"Vinotha R, Hepsiba D, L. D. Vijay Anand, Deepak John Reji"," 
 
Advancing Accessibility: Voice Cloning and Speech Synthesis for 
Individuals with Speech Disorders 
Vinotha R 1, Hepsiba D 2*, L. D. Vijay Anand 3, Deepak John Reji 4 
1 Department of Robotics Engineering 
1 vinothar@karunya.edu.in  
2 Department of Biomedical Engineering 
2 hepsiba@karunya.edu 
3 Department of Robotics Engineering 
3 vijayanand@karunya.edu 
4 Environmental Resources Management 
4 deepak.reji@erm.com 
 
Abstract 
Neural Text-to-speech (TTS) synthesis is a powerful technology that can generate 
speech using neural networks. One of the most remarkable features of TTS synthesis is its 
capability to produce speech in the voice of different speakers. This paper introduces voice 
cloning and speech synthesis https://pypi.org/project/voice-cloning/ an open-source python 
package for helping speech disorders to communicate more effectively as well as for 
professionals seeking to integrate voice cloning or speech synthesis capabilities into their 
projects. This package aims to generate synthetic speech that sounds like the natural voice of 
an individual, but it does not replace the natural human voice. The architecture of the system 
comprises a speaker verification system, a synthesizer, a vocoder, and noise reduction. Speaker 
verification system trained on a varied set of speakers to achieve optimal generalization 
performance without relying on transcriptions. Synthesizer is trained using both audio and 
transcriptions that generate Mel spectrogram from a text and vocoder which converts the 
generated Mel Spectrogram into corresponding audio signal. Then the audio signal is processed 
by a noise reduction algorithm to eliminate unwanted noise and enhance speech clarity. The 
performance of synthesized speech from seen and unseen speakers are then evaluated using 
subjective and objective evaluation such as Mean Opinion Score (MOS), Gross Pitch Error 
 
 
(GPE), and Spectral distortion (SD). The model can create speech in distinct voices by 
including speaker characteristics that are chosen randomly. 
Keywords: Speech Synthesis, Voice Cloning, Speaker Characteristics, MOS, Speech 
Disorders 
1. Introduction 
Voice cloning is a process of creating a synthetic or artificial version of someone's 
voice. It involves using TTS technology and deep learning algorithms to train a computer 
model to mimic the speech patterns, intonation, and other unique characteristics of a specific 
individual's voice. The resulting synthetic voice can then be used to generate new speech that 
sounds like the original speaker. Voice cloning has become increasingly important due to its 
practical applications in various fields. For speech-impaired individuals, voice cloning can be 
a powerful tool that helps them communicate more effectively. People who have lost the ability 
to speak due to conditions such as stroke, Amyotrophic Lateral Sclerosis (ALS), or vocal cord 
damage can use voice cloning to generate speech that sounds like their own voice with the help 
of voice bank. This can be particularly important for these individuals as it helps them maintain 
a sense of identity and connection with others. The technology of voice cloning can be 
employed to generate artificial voices for individuals who are born with conditions like cerebral 
palsy or autism, and have never been able to speak. Through the utilization of voice cloning, 
such people can express their thoughts more comprehensively and communicate effectively 
with others. In addition, voice cloning can also save time and money by automating processes 
such as customer service and technical support, making it a more efficient tool.  
Voice cloning is the process of creating a digital replica of a person's voice and building 
a TTS model. Through advanced deep learning techniques, this is accomplished by analysing 
and modelling a person's unique speech characteristics, such as tone, pitch, and accent. The 
model can be trained using a diverse set of speech datasets to generate synthetic speech that 
sounds similar to the voice of the original speaker.  In order to generate a synthetic voice that 
emulates the distinctive qualities of an individual's natural voice, deep learning algorithms 
analyse a substantial amount of recorded speech data from that individual. Through this 
analysis, the algorithms learn the patterns and characteristics of the voice, including factors 
such as tone, pitch, and pacing that contribute to its distinctiveness. Once these features have 
been learned, the algorithms are able to construct a digital model of the individual's voice, 
which can then be utilized to produce new speech that closely resembles that of the original 
 
 
person. The quality and accuracy of the synthetic speech generated are determined by the 
calibre and quantity of the training data employed to train the deep learning algorithms. In 
general, the more high-quality training data available, the more accurate and natural-sounding 
the synthetic voice will be.  
Speaker adaptation has been studied and implemented in TTS models for several 
decades [1], and has been incorporated into both Hidden Markov Model (HMM) based 
parametric TTS models and Deep Neural Network (DNN) based TTS models. In HMM-based 
models, speaker adaptation is often achieved using speaker-dependent Gaussian mixture 
models (GMMs) [2, 26, 27], which capture the unique characteristics of a particular speaker's 
voice. In DNN-based models, speaker adaptation is achieved through various techniques such 
as speaker embeddings [3] and adversarial training [4], which aim to learn a speaker-specific 
representation that can be used to generate personalized speech. Regarding voice cloning, TTS 
and voice conversion can be seen as comparable systems that utilize different inputs to create 
speech with a desired voice [5]. To separate the speaker content and voice information, U-net 
structure and the Vector Quantization (VQ) technology are utilized [6]. Voice cloning can be 
challenging due to low-quality recordings and noisy speech [7]. It requires technical expertise, 
specialized tools and software, and a significant amount of time and effort [8]. The quality of 
the audio source is crucial for creating an accurate clone, and the difficulty level depends on 
factors such as the complexity of the voice and the availability of training data. Unique accents 
or speech patterns may pose additional challenges. 
Voice cloning technology is accent-based because it relies on the characteristics of a 
specific accent to create a voice that sounds like a particular individual with that accent. The 
role of accent in voice cloning is significant as it defines the distinct way in which people speak 
and their individual vocal traits. An open-source project called CorentinJ/Real-Time-Voice-
Cloning was created by Corentin Jemine, a machine learning engineer [9]. The project employs 
deep learning technology to facilitate real-time voice cloning, allowing users to clone and 
synthesize their own or others voices in real-time. However, the system is trained using the 
LibriSpeech dataset [22], which is not tailored to Indian accents. Different accents have unique 
phonetic features, such as intonation patterns, stress, and rhythm [28-30]. For example, British 
English has different vowel sounds than American English, and speakers of different accents 
may place emphasis on different syllables or words [31]. To create an accurate voice clone, the 
network needs to be trained on recordings from speakers with the same accent as the person 
being cloned [11]. Therefore, this paper aims to build upon this project by developing a voice 
 
 
cloning system for Indian accents using a large training dataset consisting of 400 hours of 
Indian speech. 
2. Related Work 
The area of voice cloning focuses on generating speech that has the timbre of a new 
speaker, using only a small amount of speech from the new speaker [14-16]. In the context of 
voice cloning, two primary methodologies are employed: speaker adaptation techniques and 
speaker encoder techniques. Speaker adaptation involves refining the multi-speaker speech 
synthesis model by incorporating the speech of the new speaker. However, this approach can 
lead to overfitting when trained with limited data. To address this, a two-cascade module 
approach is suggested in [32], in which the first module predicts the acoustic characteristics of 
the speech, and the second module models the speaker's timbre. This approach permits the fine-
tuning of only the modules associated with the speaker's timbre, which reduces overfitting. 
Adaspeech [33] utilizes conditional layer normalization to reduce the number of fine-tuning 
parameters, enabling the model to acquire knowledge of the speaker's distinct timbre without 
sacrificing voice quality. In addition to speaker encoder and adaptation methods, there have 
been advancements in neural network-driven TTS synthesis systems, i.e., blend of RNNs and 
CNNs. This system produces synthetic speech from textual input, capable of scaling to large 
datasets, handling multiple languages and accents [10].  
Deep Voice 1, an advanced TTS system, is built upon a sequence-to-sequence (seq2seq) 
model with an attention mechanism [37]. The system comprises an encoder that converts the 
textual input into a continuous representation, and the decoder leverages this representation to 
generate the speech waveform. Deep Voice 2 improves upon Deep Voice 1 by introducing a 
new architecture called ClariNet [38], which combines the seq2seq model with a Generative 
Adversarial Network (GAN) that produces high-quality audio. Deep Voice 3 uses a 
Transformer architecture [20], which synthesizes speech directly from text and audio data, 
achieving state-of-the-art results on LJSpeech dataset and multiple languages. It employs 
several techniques including teacher forcing, attention mechanisms, and layer normalization to 
predict Mel spectrograms from text input and convert them into speech using a neural vocoder. 
Yi Ren et.al [12] proposed FastSpeech 2, a feed-forward Transformer architecture that enables 
parallel processing and significantly increases the speed of the TTS process. This approach is 
complementary to Deep Voice 3's sequence-to-sequence model, as both aim to improve the 
speed and quality of the TTS process using different techniques. Using transfer learning 
 
 
techniques and speaker embeddings extracted from a speaker verification system, Jiaqi Su et 
al. [13] developed a TTS system that synthesize speech in multiple speaker voices. When 
developing a TTS system targeted towards speakers with a limited quantity of training data, it 
is advantageous to employ a pretrained model such as Deep Voice or FastSpeech [34-36]. 
Tacotron2 implements the Global Style Token (GST) in conjunction with a seq2seq model to 
directly generate speech from textual characters. Furthermore, it incorporates an attention 
mechanism to enhance the quality of the speech [18]. In contrast, Attentron employs an 
attentive feed-forward network to predict Mel-spectrograms based on textual input. These 
spectrograms are subsequently converted into speech utilizing a WaveNet vocoder [39]. 
3. Neural TTS System 
The proposed Neural TTS system comprises three vital models: the encoder, the 
synthesizer, and the vocoder. Each model plays a critical role in the process of voice cloning 
and speech synthesis. The encoder, which utilizes the WaveNet framework, serves the purpose 
of speaker verification and capturing distinct voice characteristics. By verifying speakers' 
identities and encoding their unique vocal traits, the encoder enables precise voice cloning and 
speech synthesis. Operating in a seq2seq manner, the synthesizer generates Mel spectrograms 
from input text. This process seamlessly converts text into corresponding speech 
representations, ensuring high-fidelity speech synthesis. On the other hand, the vocoder 
transforms Mel spectrograms into waveforms in the time domain. This transformation is 
essential for generating natural and expressive speech output. To create a diverse and 
comprehensive system, all three models are trained using data from both Indian and Western 
accents. With a specific focus on incorporating more Indian accent data, the models underwent 
training with a 400-hours of Indian accent dataset.  
Once these models are trained, they are assembled into a Python package named 
""Voice-Cloning 0.0.9."" This comprehensive package enables users to engage in voice cloning 
and speech synthesis with ease. Notably, it offers users the unique capability to replicate both 
Indian and Western accents, granting a diverse array of voice cloning options. Users can input 
any desired text and combine it with a reference or existing voice for the desired output. 
Moreover, the ""Voice-Cloning 0.0.9"" package empowers users to develop their own 
personalized TTS systems. Additionally, it facilitates the replication of their own voices, 
allowing them to create custom speech models for a personalized touch. This package's 
versatility and potential make it an invaluable resource for individuals who have lost their 
 
 
voices due to medical conditions or surgeries. Moreover, it serves as a powerful tool for 
professionals seeking to integrate voice cloning or speech synthesis capabilities into their 
projects. The package's extensive capabilities, combined with the comprehensive training of 
the encoder, synthesizer, and vocoder models, ensure exceptional performance in speech 
synthesis and voice cloning tasks.  
Section IV of the manuscript discusses the speech corpora employed in the research. In 
Section V, the paper outlines process involved in speech synthesis system, encompassing the 
training of three distinct models: the speaker encoder, synthesizer, and vocoder. In Section VI, 
the evaluation process which includes subjective evaluation such as MOS by human listeners 
and objective evaluation on unseen speakers are described. Section VII explains the process of 
packaging the neural TTS model and releasing the Python package. 
4. Speech Corpora 
The speech corpus chosen to train the speech synthesis system carefully covers both 
Indian and Western accents with 3,000 speakers representing a mix of male and female voices. 
This enables the system to accurately synthesize speech in both accents. The training data of 
the encoder model includes a diverse set of datasets, namely LibriSpeech, NPTEL2020-Indian-
English-Speech-Dataset [23], LJSpeech [25], and VCTK [40]. It encompasses a wide range of 
dialects, such as Tamil, Odiya, Telugu, Malayalam, Bangla, as well as American, Welsh, 
British, and Australian accents. The overall training data of encoder covers a total duration of 
600 hours, equally distributed with 300 hours dedicated to Indian accents and 300 hours to 
Western accents. To train the synthesizer effectively, both audio recordings and transcriptions 
are crucial. AccentDB [24] dataset is utilized for training the synthesizer consists of Indian 
accents such as Odiya, Telugu, Malayalam, and Bangla excluded Tamil accent. To address this 
gap, the authors developed an Indian-Tamil accent speech database, which includes speech 
data from 10 speakers each uttered 250 utterances. The recordings are made using a Condenser 
Studio XLR microphone in a laboratory environment at a sampling rate of 16 kHz. Western 
accents such as British, American, and Welsh from the LJ speech [25] and LibriSpeech [22] 
datasets are also included in the training data.  The synthesizer will output generated audios 
and spectrograms to its model directory when training. Using this synthesized output, training 
data for the vocoder has been generated.  
 
 
 
 
5. Process involved in speech synthesis system  
 
Fig.1 Architecture of speech synthesis system 
 
Below section elaborates on the training methodology for the Speaker Encoder model, 
focusing on data pre-processing and the encoding of unique speaker characteristics. The 
Synthesizer model's training encompasses TTS model training, conditioning with speaker 
embeddings, and optimizing mel-spectrogram predictions. The Vocoder model is 
comprehensively investigated, including selecting a suitable generative model, precise mel-
spectrogram and audio waveform alignment, and generation of high quality synthesized audio. 
      5.1 Speaker Verification System (Encoder) 
The primary function of the speaker encoder is to capture the distinctive characteristics 
exhibited by different speakers. WaveNet framework is utilized for text-independent speaker 
verification in the speaker encoder model. By processing log-mel spectrogram frames from 
speech utterances of varying lengths, the WaveNet generates fixed-dimensional embedding 
vectors, commonly referred to as d-vectors. These d-vectors effectively encapsulate the unique 
attributes of individual speakers, enabling the system to accurately represent and differentiate 
speaker identities in the input data. Throughout the training process, the network is optimized 
to generate embeddings that exhibit a high cosine similarity for utterances originating from the 
 
 
same speaker. Simultaneously, the embeddings of different speakers are deliberately positioned 
far apart in the embedding space. The outputs of the embeddings are averaged and normalized 
to generate the ultimate utterance embedding, ensuring a consistent and standardized 
representation for each speech segment. 
The encoder utilizes a training method called Generalized End-to-End (GE2E), which 
can handle a considerable amount of speech samples as a batch. These batches consist of N 
speakers, with an average of M utterances from each speaker [17]. The GE2E Loss is a loss 
function that helps in the training of speaker verification systems. These systems are 
specifically designed to identify whether two speech utterances belong to the same speaker or 
not. This loss considers a group of utterances from the same speaker, referred to as an 
""utterance group"". The loss function computes the distances between the embeddings of all the 
utterances in the group and a weighted centroid embedding, which represents the average 
acoustic characteristics of the speaker. The weights assigned to each utterance in the group are 
learned during training and reflect the relative importance of each utterance in capturing the 
speaker's acoustic characteristics. 
In order to create a batch, a set of 𝑁 × 𝑀 utterances are utilized with each utterance 
being spoken by a 𝑁 distinct speakers and each speaker contributes M utterances. Each feature 
vector 𝑥𝑗𝑖(1 ≤ j ≤ N and 1 ≤ i ≤ M) represents the extracted features corresponding to speaker 
‘j’ and utterance ‘i’. Then the features extracted from each utterance 𝑥𝑗𝑖 are feed into an LSTM 
network. 
The embedding vector (d-vector) is calculated by applying L2 normalization to the 
network output. This normalization process is mathematically expressed as Equation (1). 
                               𝑒𝑗𝑖 = (𝑓(𝑥𝑗𝑖; 𝑤)) ⁄ ‖𝑓(𝑥𝑗𝑖; 𝑤)‖2                                            (1) 
Therefore, 𝑒𝑗𝑖 denotes the embedding vector that pertains to the ith utterance of the jth 
speaker. The resulting output of the system can be expressed as 𝑓(𝑥𝑗𝑖; 𝑤), where both 𝑤 and 
𝑥𝑗𝑖 correspond to the parameters of the LSTM layer and the linear layer, respectively.  
The similarity matrix 𝑆𝑗𝑖,𝑘 is defined as the scaled cosine similarities between each 
embedding vector 𝑒𝑗𝑖 to all centroids  𝑐𝑘 (1 ≤  𝑗, 𝑘 ≤  𝑁, 𝑎𝑛𝑑 1 ≤  𝑖 ≤  𝑀) is given in 
Equations (2) and (3). 
                                  𝑠𝑗𝑖,𝑘 =  𝑤 ·  𝑐𝑜𝑠 (𝑒𝑗𝑖, 𝑐𝑘) +  𝑏                                                (2) 
 
 
                                    𝑐𝑘 = 𝐸𝑚[𝑒𝑘𝑚] = 
1
𝑀   ∑
𝑒𝑘𝑚
𝑀
𝑚=1
                                                (3) 
 
Where ‘w’ and ‘b’ are learnable parameter and ‘k’ represents the index of the centroid.  
The LSTM's sequence-to-vector mapping produces a fixed-size embedding vector represented 
by 'e', while the L2 normalized response is indicated by {𝑒𝑗 ∼, 𝑒𝑘1, . . . , 𝑒𝑘𝑀)}. 
Algorithm: Speaker verification training 
• Pre-processing the input speech signal to extract MFCC features. 
• Using UMAP projection to reduce the dimensionality of the MFCC features. 
• Feeding the projected features into a neural network. 
• Training the network using the GE2E loss function for learning the speaker's 
characteristics. 
• During training, randomly sample a batch of speech segments from different speakers 
and compute the GE2E loss by comparing the embeddings of all speakers in the 
batch. 
• At inference time, taking a new spoken phrase as input. 
• Using the trained system to map the features of the input to an identity vector. 
• Comparing the input vector to a set of pre-defined identity vectors that corresponds 
to authorized speakers. 
• If the input vector is similar to one of the authorized identity vectors, the speaker as 
valid. 
The batch of speakers are grouped by colours to indicate different speakers based on 
the similarity of their speech patterns and acoustic features. The grouping of speakers improved 
step by step with each colour representing a different speaker in the batch. Each dot on the plot 
represents an embedding for a specific speech segment. By grouping the embeddings for each 
speaker together in the same colour represents how well the model is able to separate 
embeddings for different speakers as shown in Fig.2. Ideally, the embeddings for each speaker 
should cluster together in a tight group, while also being well-separated from embeddings for 
other speakers. The embeddings for each speaker are grouped and visualized using UMAP. 
The resulting clusters demonstrate that the embeddings capture distinct speaker characteristics, 
enabling speaker encoding tasks as shown in Fig. 2. 
 
 
 
 
 
 
 
 
 
Fig.2. UMAP projection of speaker embeddings 
 
 
 
 
 
 
5.2 Synthesizer 
The synthesizer utilizes the text-to-mel model that takes textual input and predicts a 
corresponding Mel spectrogram. The Mel spectrogram captures the frequency content of the 
speech over time. To enhance the pronunciation of less common words and proper nouns, the 
text is transformed into a sequence of phonemes within the text-to-mel model. The system uses 
transfer learning with pretrained speaker encoder to extract a speaker embedding from the 
target audio. The target audio serves as the speaker reference signal during training. Once the 
text-to-mel model has predicted the Mel spectrogram, it is used to condition the WaveNet 
vocoder. The WaveNet model takes the desired Mel spectrogram representation as input and 
learns to predict the corresponding waveform. Finally, the WaveNet vocoder takes the 
predicted Mel spectrogram as input and generates the corresponding raw audio signal. During 
the training process, the system utilizes pairs of text transcripts and target audio. Transfer 
learning is employed with a frozen, pretrained speaker encoder to extract a speaker embedding 
from the target audio. The target audio serves as the speaker reference signal during training. 
The WaveNet model takes the target Mel spectrogram representation as input and learns to 
predict the corresponding waveform, resulting in the generation of the predicted Mel 
spectrogram. Few illustrations of the target and predicted Mel spectrograms from multiple 
speakers is shown in Fig. 3. 
 
Algorithm: Synthesizer training 
• Converting the input text transcript to phonemes or graphemes using a text 
normalization method. 
• Generating a sequence of phoneme or grapheme embeddings using an encoder. 
• Transmitting the embeddings through a pre-net to reduce the dimensionality and 
improve conditioning. 
• Feeding the pre-net output through a text-to-mel model, which predicts a mel-
spectrogram representation of the speech signal. 
• Passing the predicted mel-spectrogram through the WaveNet model, which generates 
the corresponding raw audio signal. 
 
 
 
 
 
 
 
 
Fig.3. Comparison of Target and Predicted Mel Spectrograms for Multi- Speaker Synthesis 
5.3 Vocoder 
The speaker vocoder is used to generate high-quality audio waveforms directly from 
the digitized raw audio using autoregressive WaveNet architecture. This task is challenging 
due to the complexity and high dimensionality of raw audio signals. The autoregressive 
WaveNet architecture takes Mel spectrogram as input to generate high-quality speech that 
resembles the voices of various individuals. To create a Mel spectrogram from a raw audio 
signal, a Fourier transform is applied to obtain the frequency content of the signal. The resulting 
spectrum is then filtered using a filter bank to produce a set of filter bank outputs. These outputs 
are then transformed logarithmically to generate the Mel spectrogram. Then the synthesized 
Mel spectrogram is converted into time-domain waveform samples by modelling the 
 
 
conditional distribution of the next waveform sample, given the previous samples and the Mel 
spectrogram input.  This enables the network to generate realistic and high-quality speech that 
clone the voices of various individuals, resulting in a natural-sounding output. To create a 
multi-speaker vocoder, the network is trained on data from many different speakers.  
To train the vocoder, the initial step involves pre-processing the training data to acquire 
the corresponding Mel spectrograms. Subsequently, these Mel spectrograms serve as the input 
to the neural vocoder for further training and processing. WaveNet architecture is used because 
of its effectiveness for modelling the complex patterns in raw audio signals. In WaveNet 
architecture, the system is conditioned on the input Mel spectrogram by concatenating it to the 
input waveform at each layer of the network. The system is then trained to predict the 
corresponding waveform given the Mel spectrogram as the input. To train the system, a dataset 
of speech recordings and corresponding Mel spectrograms is used. The system is trained using 
a Mean Squared Error (MSE) loss function to quantify the disparity between the predicted and 
target waveforms. To minimize this loss, a Stochastic Gradient Descent (SGD) optimizer is 
employed. During training, a technique called teacher forcing is used, which involves feeding 
the correct waveform as input to the system during each time step. Once the system has been 
trained, it is used to synthesize speech from new Mel spectrograms by feeding them into the 
system and generating the corresponding audio signal. After obtaining the synthesized 
speech/cloned voice, a process called noise reduce [21] is applied to remove unwanted noise 
and make the speech clearer.  
Algorithm: Vocoder training 
• Obtaining the training data from synthesizer output generated audios and 
corresponding Mel spectrograms 
• Using a WaveNet architecture, input Mel spectrogram is conditioned by 
concatenating it to the input waveform at each layer of the network. 
• Training the network using the following steps: 
a. Initializing the network weights. 
b. Feeding the Mel spectrogram and waveform as inputs to the network using 
teacher forcing method. 
c. Computing the disparity between the predicted and actual waveforms by 
using the MSE loss function. 
d. Using an optimizer such as SGD to minimize the loss. 
 
 
e. Repeating steps b-d for a predetermined number of epochs. 
• Using the trained neural vocoder to synthesize speech from new Mel spectrograms 
by feeding them into the network and generating the corresponding audio signal 
• Applying noise reduction technique to remove unwanted noise and improve the 
overall quality of the speech. 
 
6. Evaluation Metrics 
To evaluate the effectiveness of voice cloning and speech synthesis system, three 
evaluation metrics are utilized: MOS is an important tool for evaluating the effectiveness of 
speech synthesis systems, as it provides a measure of speech perceived as high-quality and 
natural-sounding by human listeners. GPE is used to evaluate the accuracy of speech synthesis 
systems by measuring the disparity between the pitch of the input signal and synthesized signal. 
SD measures the spectral difference between the original speech signal and the synthesized 
speech signal. The evaluation of the synthesized speech involves subjective measures such as 
MOS, as well as objective measures such as GPE and SD. The objective evaluation are applied 
on unseen speakers (Excluded from training), having Western accents (VCTK and 
LibriSpeech) and Indian accents (Tamil, Bangla, Telugu, and Malayalam).  
6.1 Speaker similarity evaluation by human listeners 
In this interface, listeners are presented with original and synthesized audio samples as 
shown in Fig 4 (a). The listeners are requested to rate the similarity of the speaker’s voices on 
a rating scale that ranges from ""not at all similar"" to ""slightly similar,"" ""moderately similar,"" 
and ""very similar. MOS is subsequently calculated by aggregating the ratings provided by all 
participants. The responses of the participants are recorded and visualized in Fig. 4 (b), where 
the ""very similar"" category corresponds to MOS values between 5 and 4, ""moderately similar"" 
corresponds to MOS values between 4 and 3, ""slightly similar"" corresponds to MOS values 
between 3 and 2, and ""not at all similar"" includes MOS values less than 2. The subjective 
speaker similarity MOS evaluation, conducted by human listeners holds significant importance 
in the assessment of speech synthesis system.  
 
 
(a) 
 
                                           (b) 
 
Fig. 4 (a) illustrates the MOS evaluation interface, while Fig. 4 (b) visually represents the 
responses provided by human listeners. 
Table. 1 Objective evaluation on unseen speakers. (A downward decreasing arrow 
signifies that as the metric value decreases, the performance improves) 
Accent 
Dataset Speaker 
_ID 
Gender 
Existing Work [13] 
Proposed Work  
 
 
 
 
Natural 
ness 
 
Similarity  
Natural 
ness 
 
Similarity  GPE 
↓ 
SD ↓ 
 
 
 
Western 
VCTK 
[32] 
p230 
F 
4.22 
4.65 
4.57 
4.64 
1.95 
3.38 
P240 
F 
4.57 
4.67 
4.63 
4.69 
1.69 
2.25 
P300 
F 
4.60 
4.87 
4.72 
4.76 
2.23 
1.43 
P340 
M 
4.44 
4.71 
4.65 
4.78 
1.23 
2.23 
P260 
M 
4.56 
4.31 
4.61 
4.58 
1.54 
2.67 
P270 
M 
4.29 
4.77 
4.48 
4.82 
2.33 
1.32 
LibriSp
eech 
[22] 
7021 
M 
4.71 
4.55 
4.63 
4.52 
0.43 
2.92 
7729 
M 
4.55 
4.48 
4.59 
4.73 
3.89 
1.04 
8230 
M 
4.65 
4.70 
4.71 
4.79 
0.67 
0.78 
3575 
F 
4.59 
4.36 
4.57 
4.45 
1.98 
2.67 
 
 
4970 
F 
3.77 
4.16 
4.27 
4.12 
0.34 
2.78 
 
4992 
F 
4.40 
3.81 
4.54 
4.46 
2.12 
3.23 
 
 
 
 
 
Indian 
 
 
Tamil 
accent 
dataset 
by 
authors 
Tamil_
02 
F 
- 
- 
4.32 
4.23 
0.51 
1.45 
Accent
DB 
[24] 
Bangla
_01 
F 
- 
- 
4.65 
4.47 
1.97 
5.32 
Telugu
_02 
M 
- 
- 
4.32 
4.26 
1.38 
4.42 
Malaya
lam_03 
F 
- 
- 
4.52 
4.17 
1.55 
3.79 
 
6.2 Comparison with existing work 
The naturalness and similarity of the proposed work are compared with existing work 
as mentioned in Table 1. While the existing work solely relied on subjective evaluation, the 
proposed approach goes a step further and incorporates objective evaluations such as GPE and 
SD for both Western and Indian accents. Conducting the evaluation on unseen speakers enables 
a thorough analysis of the synthesized speech quality across a diverse range of accents and 
speakers. The proposed work demonstrates superior naturalness and similarity when compared 
with existing work [13]. The naturalness metric represents the average MOS score given by 
human listeners to the original speech in the dataset. The similarity metric represents proposed 
system's ability to produce speech that is similar to the original speech. A higher similarity 
score indicates that the synthesized speech closely resembles the original speech, which is 
desirable for a good voice cloning system. A low GPE value indicates that the speech synthesis 
system has accurately replicates the pitch of the original speech signal. Similarly, a low value 
for SD indicates a close match between the spectral characteristics of the synthesized speech 
and the original speech, suggesting a high level of accuracy in the cloning process. Overall, the 
table suggests that the proposed speech synthesis system is able to produce synthesized speech 
that closely matches the naturalness and similarity of the original speech, indicating a good 
level of performance.  
 
 
7. Release of Voice-Cloning 0.0.9 python package  
  
The trained neural system has demonstrated promising outcomes through both 
subjective and objective evaluations. As a result, the trained encoder, synthesizer, and vocoder 
models have been integrated into a Python package. By simply installing the voice cloning 
package, users gain the ability to effortlessly synthesize speech. This package holds the 
potential to be seamlessly integrated into assistive technologies, offering a transformative 
solution for individuals with speech disorders. By harnessing the power of voice banks, this 
technology enables individuals to restore a semblance of their natural voice. Moreover, this 
innovation is poised to be an helpful asset for professionals immersed in the field of speech 
synthesis technology. Voice-Cloning 0.0.9 python package https://pypi.org/project/Voice-
Cloning/ has been released, which empowers users to generate audio from text, design their 
own text-to-speech systems, and even create a personalized speech model by cloning their own 
voice. The entire Voice-Cloning package is conveniently available via the Python Package 
Index (PyPI), making installation effortless. Simply execute the following command to install 
Voice-Cloning package using pip: 
• Use the package manager pip to install Voice-Cloning 0.0.9 
• pip install voice-cloning 
The availability of the package on PyPI ensures that the user can easily incorporate Voice-
Cloning into their projects, enabling them to harness its powerful speech synthesis and 
voice cloning features without hassle. To reproduce the experiments and explore the 
functionalities offered by Voice-Cloning, users can refer to the comprehensive 
documentation provided alongside the package. This documentation explains the usage of 
functions, parameters, and examples, facilitating a smooth and effective utilization of the 
package's capabilities. Fig. 5 illustrates the functionality of the voice cloning package, 
which accepts any text input and a reference voice or existing voice. Fig. 6 explains the 
flow of speech synthesis model used in the package.  Giving users the option to select 
between cloning via an external reference voice or speech synthesis using an existing voice 
from the sound library. The sound library features a collection of 31 speaker voices 
supporting both ""Western"" and ""Indian"" accents. The package tasks are summarized in 
Table 2. 
 
 
 
Fig.5. Pipeline of speech synthesis and voice cloning model 
Table 2. Voice-Cloning tasks 
Feature 
Description 
Speech synthesis 
Users can utilize the package to generate synthesized speech 
by entering text, with access to pre-loaded speakers, similar to 
TTS system. 
Real-time voice cloning 
 
The package offers the capability to perform real-time voice 
cloning by analysing either a reference voice clip or the user's 
speech input.  
Multi-Accent support 
Supports Western and Indian accents for speech synthesis and 
voice cloning. 
Noise reduction 
The package incorporates noise reduction to reduce noise in 
recorded audio, resulting in an overall improvement in the 
quality of the synthesized or cloned speech.  
 
 
 
 
8. Conclusion 
A speech synthesis and voice cloning package has been developed, consisting of 
Speaker verification system, a synthesizer, and a vocoder. These three systems are trained using 
huge amount of open source dataset which includes accents like American, Australian, Welsh, 
Tamil, Telugu, Malayalam, Odiya, Bangla. The speaker encoder uses a WaveNet framework 
to capture the unique characteristics of different speakers and conditioning the synthesis 
network on the desired target speaker's reference speech signal. Synthesizer consists of two 
main parts: the text-to-mel model and the WaveNet vocoder. The text-to-mel model takes the 
text as input and predicts a corresponding Mel spectrogram, which captures the frequency 
content of the speech over time. The WaveNet vocoder then uses this predicted Mel 
spectrogram to generate the corresponding raw audio signal. Vocoder trained on data from 
many different speakers i.e. the multispeaker vocoder learns to generate high-quality audio 
waveforms directly from raw audio signals  that sounds like different people, making it a 
valuable tool in various fields such as speech synthesis and voice cloning. Noise reduce 
algorithm have been incorporated to further enhance the quality of the output audio signal. The 
MOS subjective evaluation involves assessing the ground truth and synthesized speech 
(proposed model) for unseen speakers of both Western and Indian accents. The lower the GPE 
score, the better the accuracy of the speech synthesis system in reproducing the pitch of the 
original speech signal. A low SD value indicates that the synthesized speech closely matches 
the spectral characteristics of the original speech, which indicates a high level of accuracy in 
the cloning process. However, voice cloning systems encounter challenges when attempting to 
reproduce the intricate emotional cues present in human speech, resulting in synthesized 
voices. Hence the package employs  speaker encoder, synthesizer and vocoder  to produce 
synthesized speech that closely resembles a person's natural voice and providing  a support for 
those with speech disorders to communicate more effectively. This technology also opens up 
new possibilities for professionals seeking to integrate voice cloning or speech synthesis 
capabilities into their projects. Future work in voice cloning could explore enhancing the 
expressiveness and emotional range of synthesized voices, enabling them to convey a wider 
spectrum of human feelings and tones. 
 
 
 
 
 
 
Declarations  
Conflict of interest  
The corresponding author on behalf of all authors, affirms that there are no conflicts of 
interest to disclose in relation to this work.  
Acknowledgment 
The authors express their sincere gratitude to the Science for Equity Empowerment and 
Development Division (SEED) under the Department of Science and Technology (DST), 
Government of India, for their financial support towards this project (Ref No: 
SEED/TIDE/2019/431/G). The team extends their sincere gratitude to Karunya Institute of 
Technology and Sciences, Coimbatore, Tamil Nadu, for providing the research facilities and 
the infrastructure. 
References 
[1] Sari, Leda, Mark Hasegawa-Johnson, and Samuel Thomas. ""Auxiliary networks for joint 
speaker adaptation and speaker change detection."" IEEE/ACM Transactions on Audio, Speech, 
and Language Processing 29 (2020): 324-333. 
[2] Xue, Shaofei, Ossama Abdel-Hamid, Hui Jiang, and Lirong Dai. ""Direct adaptation of 
hybrid DNN/HMM model for fast speaker adaptation in LVCSR based on speaker code."" In 
2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 
6339-6343. IEEE, 2014. 
[3] Wu, Zhizheng, Pawel Swietojanski, Christophe Veaux, Steve Renals, and Simon King. ""A 
study of speaker adaptation for DNN-based speech synthesis."" In Sixteenth Annual Conference 
of the International Speech Communication Association. 2015. 
[4] Doddipatla, Rama, Norbert Braunschweiler, and Ranniery Maia. ""Speaker Adaptation in 
DNN-Based Speech Synthesis Using d-Vectors."" In Interspeech, pp. 3404-3408. 2017. 
[5] Luong, Hieu-Thi, and Junichi Yamagishi. ""Nautilus: a versatile voice cloning system."" 
IEEE/ACM Transactions on Audio, Speech, and Language Processing 28 (2020): 2967-2981. 
[6] Van Den Oord, Aaron, and Oriol Vinyals. ""Neural discrete representation learning."" 
Advances in neural information processing systems 30 (2017). 
 
 
[7] Hsu, Wei-Ning, Yu Zhang, Ron J. Weiss, Yu-An Chung, Yuxuan Wang, Yonghui Wu, and 
James Glass. ""Disentangling correlated speaker and noise for speech synthesis via data 
augmentation and adversarial factorization."" In ICASSP 2019-2019 IEEE International 
Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5901-5905. IEEE, 
2019. 
[8] Cong, Jian, Shan Yang, Lei Xie, Guoqiao Yu, and Guanglu Wan. ""Data efficient voice 
cloning from noisy samples with domain adversarial training."" arXiv preprint 
arXiv:2008.04265 (2020). 
[9] Jemine, Corentin. ""Real-time-voice-cloning."" University of Liége, Liége, Belgium (2019). 
[10] Wang, Yuxuan, Daisy Stanton, Yu Zhang, RJ-Skerry Ryan, Eric Battenberg, Joel Shor, 
Ying Xiao, Ye Jia, Fei Ren, and Rif A. Saurous. ""Style tokens: Unsupervised style modeling, 
control and transfer in end-to-end speech synthesis."" In International Conference on Machine 
Learning, pp. 5180-5189. PMLR, 2018. 
[11] Shen, Jonathan, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng 
Yang, Zhifeng Chen et al. ""Natural tts synthesis by conditioning wavenet on mel spectrogram 
predictions."" In 2018 IEEE international conference on acoustics, speech and signal processing 
(ICASSP), pp. 4779-4783. IEEE, 2018. 
[12] Ren, Yi, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. 
""Fastspeech 2: Fast and high-quality end-to-end text to speech."" arXiv preprint 
arXiv:2006.04558 (2020). 
[13] Jia, Ye, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, Patrick Nguyen, 
Ruoming Pang, Ignacio Lopez Moreno, and Yonghui Wu. ""Transfer learning from speaker 
verification to multispeaker text-to-speech synthesis."" Advances in neural information 
processing systems 31 (2018). 
[14] Valle, Rafael, Jason Li, Ryan Prenger, and Bryan Catanzaro. ""Mellotron: Multispeaker 
expressive voice synthesis by conditioning on rhythm, pitch and global style tokens."" In 
ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal 
Processing (ICASSP), pp. 6189-6193. IEEE, 2020. 
[15] Mohammadi, Seyed Hamidreza, and Alexander Kain. ""Voice conversion using deep 
neural networks with speaker-independent pre-training."" In 2014 IEEE Spoken Language 
Technology Workshop (SLT), pp. 19-23. IEEE, 2014. 
 
 
[16] Zhao, Li, and Feifan Chen. ""Research on voice cloning with a few samples."" In 2020 
International Conference on Computer Network, Electronic and Automation (ICCNEA), pp. 
323-328. IEEE, 2020. 
[17] Wan, Li, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. ""Generalized end-to-end 
loss for speaker verification."" In 2018 IEEE International Conference on Acoustics, Speech 
and Signal Processing (ICASSP), pp. 4879-4883. IEEE, 2018. 
[18] Shen, Jonathan, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng 
Yang, Zhifeng Chen et al. ""Natural tts synthesis by conditioning wavenet on mel spectrogram 
predictions."" In 2018 IEEE international conference on acoustics, speech and signal processing 
(ICASSP), pp. 4779-4783. IEEE, 2018. 
[19] Oord, Aaron van den, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex 
Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. ""Wavenet: A generative 
model for raw audio."" arXiv preprint arXiv:1609.03499 (2016). 
[20] Ping, Wei, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan 
Narang, Jonathan Raiman, and John Miller. ""Deep voice 3: Scaling text-to-speech with 
convolutional sequence learning."" arXiv preprint arXiv: 1710.07654 (2017). 
[21] Sainburg, Tim. ""timsainb/noisereduce: v1. 0."" Zenodo, Jun (2019). 
[22] https://www.openslr.org/12 
[23] https://github.com/AI4Bharat/NPTEL2020-Indian-English-Speech-Dataset 
[24] https://accentdb.org/ 
[25] https://keithito.com/LJ-Speech-Dataset/ 
[26] Jung, Chi-Sang, Young-Sun Joo, and Hong-Goo Kang. ""Waveform interpolation-based 
speech analysis/synthesis for HMM-based TTS systems."" IEEE Signal Processing Letters 19, 
no. 12 (2012): 809-812. 
[27] Yamagishi, Junichi, Bela Usabaev, Simon King, Oliver Watts, John Dines, Jilei Tian, 
Yong Guan et al. ""Thousands of voices for HMM-based speech synthesis–Analysis and 
application of TTS systems built on various ASR corpora."" IEEE Transactions on Audio, 
Speech, and Language Processing 18, no. 5 (2010): 984-1004. 
 
 
[28] Qian, Yanmin, Xun Gong, and Houjun Huang. ""Layer-Wise Fast Adaptation for End-to-
End Multi-Accent Speech Recognition."" IEEE/ACM Transactions on Audio, Speech, and 
Language Processing 30 (2022): 2842-2853. 
[29] Ghorbani, Shahram, and John HL Hansen. ""Domain Expansion for End-to-End Speech 
Recognition: Applications for Accent/Dialect Speech."" IEEE/ACM Transactions on Audio, 
Speech, and Language Processing (2022). 
[30] William, Freddy, Abhijeet Sangwan, and John HL Hansen. ""Automatic accent assessment 
using phonetic mismatch and human perception."" IEEE transactions on audio, speech, and 
language processing 21, no. 9 (2013): 1818-1829. 
[31] Seong, Jiwon, WooKey Lee, and Suan Lee. ""Multilingual speech synthesis for voice 
cloning."" In 2021 IEEE International Conference on Big Data and Smart Computing 
(BigComp), pp. 313-316. IEEE, 2021. 
[32] Wang, Tao, Jianhua Tao, Ruibo Fu, Jiangyan Yi, Zhengqi Wen, and Rongxiu Zhong. 
""Spoken Content and Voice Factorization for Few-Shot Speaker Adaptation."" In 
INTERSPEECH, pp. 796-800. 2020. 
[33] Chen, Mingjian, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, Sheng Zhao, and Tie-Yan Liu. 
""Adaspeech: Adaptive text to speech for custom voice."" arXiv preprint arXiv:2103.00993 
(2021). 
[34] Fan, Yuchen, Yao Qian, Frank K. Soong, and Lei He. ""Multi-speaker modeling and 
speaker adaptation for DNN-based TTS synthesis."" In 2015 IEEE international conference on 
acoustics, speech and signal processing (ICASSP), pp. 4475-4479. IEEE, 2015. 
[35] Huang, Zhiying, Heng Lu, Ming Lei, and Zhijie Yan. ""Linear networks based speaker 
adaptation for speech synthesis."" In 2018 IEEE International Conference on Acoustics, Speech 
and Signal Processing (ICASSP), pp. 5319-5323. IEEE, 2018. 
[36] Inoue, Katsuki, Sunao Hara, Masanobu Abe, Tomoki Hayashi, Ryuichi Yamamoto, and 
Shinji Watanabe. ""Semi-supervised speaker adaptation for end-to-end speech synthesis with 
pretrained models."" In ICASSP 2020-2020 IEEE International Conference on Acoustics, 
Speech and Signal Processing (ICASSP), pp. 7634-7638. IEEE, 2020. 
 
 
[37] Arık, Sercan Ö., Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, 
Yongguo Kang, Xian Li et al. ""Deep voice: Real-time neural text-to-speech."" In International 
conference on machine learning, pp. 195-204. PMLR, 2017. 
[38] Gibiansky, Andrew, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, 
Jonathan Raiman, and Yanqi Zhou. ""Deep voice 2: Multi-speaker neural text-to-speech."" 
Advances in neural information processing systems 30 (2017). 
[39] Choi, Seungwoo, Seungju Han, Dongyoung Kim, and Sungjoo Ha. ""Attentron: Few-shot 
text-to-speech utilizing attention-based variable-length embedding."" arXiv preprint 
arXiv:2005.08484 (2020). 
[40] https://www.kaggle.com/datasets/showmik50/vctk-dataset. 
 
 
"
"Property prediction plays a major role in crystal material research for the discovery of new materials. Graph learning-based methods have achieved significant progress by representing atoms and structures as graphs. Most existing works only consider bond distances and neglect bond angles. This is mainly because bond angles are challenging to handle due to their infinite nature. Here, we propose a dual scale modeling method for the crystal structure, which uses a larger scale cutoff for edge neighbors and a smaller scale cutoff for angle neighbors. We also propose the Atom-Distance-Angle Graph Neural Network (ADA-GNN) for property prediction tasks. Extensive experiments on two large-scale material benchmark datasets validate the effectiveness of the ADA-GNN, achieving state-of-the-art results.","Property prediction is a crucial task in crystal research and plays a key role in discovering new materials with desired characteristics. Predicting crystal properties is challenging since they are dictated by the chemical element types and structural details, including bond distances and angles. Therefore, modeling both atomic attributes and structural information accurately is essential. Prior works have mainly focused on optimizing representations based on multi-graph modeling of crystals, overlooking the role of bond angles despite their crucial contribution to structural information. Incorporating bond angles has been hindered by the infinite number of angles associated with each edge. To address this issue, we propose a dual scale modeling approach for the crystal structure and a novel Atom-Distance-Angle Graph Neural Network (ADA-GNN) for property prediction tasks.","nanIn this section, we discuss related work on crystal property prediction, including the state-of-the-art PotNet model. We highlight the key differences between our approach and existing methods, particularly in incorporating bond angle information and the independent embedding of atoms and structures. Table 1 summarizes the comparison between our method and other baselines in terms of their consideration of atomic features, bond distances, bond angles, and the use of an independent cutoff for obtaining bond distances and bond angles.nannan","In this section, we introduce our proposed methods, including the dual scale modeling for crystal materials, the embedding module, the message passing module, and the readout module. We describe the overall architecture of the ADA-GNN and provide a time cost analysis to theoretically demonstrate the efficiency of our approach.nan","Our experimental results demonstrate the effectiveness of the proposed ADA-GNN in property prediction tasks on two large-scale material benchmark datasets: The Materials Project-2018.6 and JARVIS. The ADA-GNN achieves state-of-the-art performance, outperforming seven representative baselines, including the latest PotNet model, by 2.04% to 21.82% in terms of test Mean Absolute Error (MAE). Additionally, we provide a comprehensive comparison of prediction time and ablation studies to highlight the importance of including bond angle information.","We address the infinite bond angle problem by proposing a dual scale modeling for crystal materials and a novel Atom-Distance-Angle Graph Neural Network (ADA-GNN) for property prediction tasks. The ADA-GNN processes atom features and structural information independently, enhancing training stability and improving prediction accuracy. Extensive experiments on two real-world datasets validate the ADA-GNN's superior performance, achieving state-of-the-art results. Additionally, the ADA-GNN exhibits faster inference time compared to other related methods, meeting the demands of downstream tasks.",ADA-GNN: Atom-Distance-Angle Graph Neural Network for Crystal Material Property Prediction,"Jiao Huang, Qianli Xing, Jinglong Ji, Bo Yang","ADA-GNN: ATOM-DISTANCE-ANGLE GRAPH NEURAL
NETWORK FOR CRYSTAL MATERIAL PROPERTY PREDICTION ∗
Jiao Huang
Jilin University
Changchun, Jilin
haungjiao20@mails.jlu.edu.cn
Qianli Xing ∗
Jilin University
Changchun, Jilin
qianlixing@jlu.edu.cn
Jinglong Ji
Jilin University
Changchun, Jilin
jijl22@mails.jlu.edu.cn
Bo Yang ∗
Jilin University
Changchun, Jilin
ybo@jlu.edu.cn
ABSTRACT
Property prediction is a fundamental task in crystal material research. To model atoms and structures,
structures represented as graphs are widely used and graph learning-based methods have achieved
significant progress. Bond angles and bond distances are two key structural information that greatly
influence crystal properties. However, most of the existing works only consider bond distances and
overlook bond angles. The main challenge lies in the time cost of handling bond angles, which
leads to a significant increase in inference time. To solve this issue, we first propose a crystal
structure modeling based on dual scale neighbor partitioning mechanism, which uses a larger scale
cutoff for edge neighbors and a smaller scale cutoff for angle neighbors. Then, we propose a novel
Atom-Distance-Angle Graph Neural Network (ADA-GNN) for property prediction tasks, which
processes node information and structural information separately. The accuracy of predictions and
inference time are improved with the dual scale modeling and the specially designed architecture of
ADA-GNN. The experimental results validate that our approach achieves state-of-the-art results in
two large-scale material benchmark datasets on property prediction tasks.
Keywords Graph Neural Network · Crystal Property Prediction
1
Introduction
Property prediction is a fundamental task in crystal research and crucial for discovering new materials with desired
characteristics [1, 2, 3]. The properties of a crystal are dictated by the types of atoms and the structural details. The
atoms have intrinsic features, and structural details encompass the bond distances and bond angles between each atom
[4, 5, 6]. Thus, the key challenge of predicting crystal properties lies in effectively modeling both atomic attributes and
structural information.
To address this challenge, CGCNN [7] initially represents the crystal structure as an undirected multigraph, where
nodes signify atoms within the unit cell and edges denote connections between atoms. Multigraph modeling of crystals
has been validated in crystal property prediction tasks. Subsequent research has predominantly focused on refining
representations based on this foundation [2, 8]. Among them, PotNet [9] achieves state-of-the-art performance by
encompassing the complete set of potentials among all atoms through the infinite summation of distances.
These prior efforts have made substantial progress by investigating in atom attributes and bond distances. However,
they overlook the bond angle, which is crucial for structural information. According to the potential energy equation
between atoms [4], it is imperative to consider not only bond distances but also the structural information conveyed by
∗Corresponding authors: Qianli Xing (qianlixing@jlu.edu.cn) and Bo Yang (ybo@jlu.edu.cn)
arXiv:2401.11768v1  [cs.LG]  22 Jan 2024
(d)
(b)
Bond  
Bond 
Angle
(a)
Infinite Structure
Bond 
Distance 
(c)
Figure 1: Illustrations of the crystal structure of NaCl and the bond distances and bond angles. (a): The structural
diagram of NaCl. The green sphere signifies the atom Cl, while the purple sphere represents the atom Na. The central
cube serves as a visual representation of the unit cell. (b): Bonds in NaCl. (c): Bond distances of bonds. (d): Bond
angles formed by the bonds.
bond angles, as shown below:
E = Edistance + Eangle + Eothers.
(1)
Edistance models on bond distances, Eangle captures the effects of angles between bonds and Eothers encompasses
interaction forces involving multiple atoms or atoms situated at a distance. Considering that crystal properties, which
are related to crystal energy, are intricately linked to these angles, it becomes imperative to incorporate angles into the
modeling of the crystal structure for accuracy.
The primary difficulty in modeling bond angles arises from the infinite number of angles associated with each edge. For
instance, as illustrated in Figure 1, considering the infinite extension of the crystal structure, each bond depicted in
Figure 1 (b) will have approximately an infinite number of associated angles, as shown in Figure 1 (d). Consequently,
integrating angles would lead to a significant augmentation in the number of inputs, thereby impacting the inference
time. ALiGNN [1] divides the overall structure into point and edge graphs to solve the infinite angle problem. However,
this model is time-consuming as the processing of bond angles significantly increases inference time. As a result, its
application in various scenarios for downstream tasks is effectively blocked. Thus, an effective method for processing
bond angles is in need.
To solve the infinite bond angle problems, we first propose a dual scale modeling for the crystal structure consisting
of the potential energy equation. The modeling involves utilizing a larger scale truncation for edge neighbors and a
smaller scale truncation for angle neighbors, which can effectively reduce input complexity and inference time. Then,
we propose a novel Atom-Distance-Angle Graph Neural Network (ADA-GNN) for property prediction tasks based on
the modeling. Specifically, we decouple atom attributes from structural attributes to ensure the independent embedding
of atoms and structures. This approach enhances training stability and improves the accuracy of model predictions. The
experimental results validate that our model delivers highly competitive performance in terms of inference time and
achieves state-of-the-art (SOTA) results in two real-world datasets. Our contribution mainly includes the following
three parts:
• We propose a dual-scale modeling for crystal materials to choose edge neighbors and angle neighbors, which
can leverage the information from bond angles and effectively reduce the inference time.
• We propose a novel approach named ADA-GNN for property prediction tasks with a specially designed
embedding process. The ADA-GNN embeds atom features and structural information separately which can
help in better understanding these two kinds of information.
• Experiments in two real-world large scale material benchmark datasets show that our approach outperforms
previous state-of-the-art crystal property prediction methods by 2.04% − 21.82% in terms of MAE.
2
Related work
Predicting crystal properties is crucial for discovering materials with ideal properties [10, 11, 12]. Unlike organic
molecules such as small molecules [13] and proteins [14, 15], the structure of crystal molecules is a periodic structure
that extends the unit cell infinitely, which is more challenging to model [16, 17, 18]. Thus, aside from addressing
the atomic types pertinent to the molecular domain, the pivotal factor in accurately predicting the quality of crystal
properties lies in effectively harnessing structural information.
Considering the unique challenges of crystal molecules, CGCNN [7] introduces an innovative approach that incorporates
both multi-graph modeling and cutoff mechanisms. In multigraph modeling, the neighbors of the target atom are
selected based on the values of the bond distance and cutoff. An atom neighbor is selected when the bond distance
2
Table 1: Comparison between our method and other baselines.
The columns labeled ""Atom,"" ""Distance,"" and ""Angle"" indicate
whether the model incorporates atomic features, bond distance,
and bond angle information, respectively. The final column,
denoted as ""In. Cutoff,"" specifies whether an independent cutoff
has been established to obtain bond distances and bond angles.
Method
Atom
Distance
Angle
In. Cutoff
CGCNN [2018]
✓
✓
×
×
SchNet [2017]
✓
✓
×
×
MEGNET [2019]
✓
✓
×
×
GATGNN [2020]
✓
✓
×
×
ALiGNN [2021]
✓
✓
✓
×
Matformer [2022]
✓
✓
×
×
PotNet [2023]
✓
✓
×
×
ADA-GNN
✓
✓
✓
✓
between the neighbor atom and the target atom is smaller than the cutoff. Consequently, the originally infinite-node
graph structure is condensed into a finite-node graph structure.
There are a number of methods that employ the graph-structure and cutoff mechanism after the CGCNN. We compare
seven representative methods, including: CGCNN [7], SchNet [19], MEGNET [2], GATGNN [3], ALiGNN [1],
Matformer [8], and PotNet [9]. All of these methods achieved the best performance during their publication period, and
PotNet shows state-of-the-art performance today.
As demonstrated in Table 1, four dimensions are employed to show their difference including atom, bond distance, bond
angle, and cutoff. At present, our model and ALiGNN are the only two methods that incorporate angle information. To
incorporate angle details into structural information, ALiGNN uses one cutoff that partitions the entire graph structure
into point and edge graphs. However, this approach exhibits noticeable limitations. Firstly, it overlooks the challenge
of managing the quantity of bond angle information and utilizes the same scale as bond distance to gather neighbor
information, resulting in a significant increase in inference time. Secondly, the continuous updating of node attributes
during the modeling of structural features may impact training stability, potentially diminishing the competitiveness of
the final model’s predictive performance.
A significant distinction lies in our approach, where we employ two distinct cutoffs for obtaining edge and angle
information, in contrast to ALiGNN, which relies solely on a single cutoff type. Additionally, unlike ALiGNN’s
consideration of node attributes during the modeling of structural features, our embedding module processes atom
features and structure information independently.
3
Dual Scale Modeling of Crystal Materials
In this section, we introduce how to use graph structure to represent the crystal materials. Firstly, the existing
representation of crystal materials is shown. Then, we propose a dual scale neighbor partitioning mechanism to
effectively select the edge neighbors and angle neighbors. Finally, we give a problem definition of crystal property
prediction tasks with the selected neighbors.
3.1
Crystal Materials
Crystal material is initially expressed as M = {A, X, L}. A = [a1, ..., aN]⊤ ∈ AN represents atom varieties, with
A symbolizing the set of chemical elements. X = [x1, ..., xN]⊤ ∈ RN×3 specifies the spatial coordinates of atoms
in the Cartesian coordinate system. L = [l1, l2, l3]⊤ ∈ R3×3 embodies the recurrent lattice, indicating the extending
directions of unit cells in three-dimensional space. In this work, we represent the M by using graph structures. The
atoms are points and the interaction forces between atoms are edges. Note that the modeling of crystals must satisfy
four types of invariances [11], including permutation invariance, translation invariance, rotation invariance, and periodic
invariance. To fulfill these requirements, absolute coordinates denoted by variables such as X and L must undergo a
transformation into relative quantities, such as distance and angle, which are shown in the following section.
3
 Area determined 
by Angle Cutoff
 Area determined 
by Edge Cutoff
Figure 2: Illustrations of the partitioning mechanism for edge neighbors and angle neighbors.
3.2
Dual Scale Neighbor Partitioning Mechanism
In this section, we proposed a dual scale neighbor partitioning mechanism to determine the edge neighbors and angle
neighbors of atoms as shown in Figure 2. Through this mechanism, we are able to represent bond distances and bond
angles in a relative quantity manner.
When selecting the value of edge cutoff Ce, we opt for a larger scale. This choice is driven by two main factors:
firstly, larger scales adequately fulfill the graph’s connectivity prerequisites, and secondly, a sufficient number of edge
neighbors at larger scales are necessary to address periodicity concerns. Aligning with the scale used in prior works
[7, 1] helps fulfill these criteria effectively.
For bond distances, the bond distance dij between atom i and atom j is computed by the following formula:
dij = ||xj − xi||2,
(2)
where atom i is the atom within the unit cell and atom j is atom i’s edge neighbor. If the distance dij between atom j
and atom i is less than edge cutoff Ce, then atom j is the edge neighbor of atom i. As shown in Figure 2, the red circle
represents the area determined by edge cutoff Ce. For atom i, the atoms inside the red circle are all its edge neighbors.
When selecting the value of angle cutoff Ca, we opt for a smaller scale due to two primary considerations. Firstly, a
smaller cutoff for angle neighbors can significantly reduce the input data volume to the network, thereby impacting
inference time. We have verified this in both theoretical and experimental aspects in Section 4.4 and Section 5.3,
respectively. Secondly, empirical findings indicate that employing smaller-scale cutoffs can still enhance algorithm
performance to a state-of-the-art level.
For each edge ij, the angles related to the edge are divided into angles αi, with atom i as the vertex and angles αj, with
atom j as the vertex. They are respectively computed as follows:
αi = {αiik|αiik = angle(−→
iik, ⃗ij)},
(3)
and
αj = {αjjk|αjjk = angle(−→
jjk, ⃗ji)}.
(4)
Where atom ik and atom jk are respectively the angle neighbors of atom i and atom j. The angle neighbors of atoms are
determined by the smaller cutoff called angle cutoff Ca, and the distance between atoms less than the angle cutoff Ca is
defined as the angle neighbors. As shown in Figure 2, the cyan circle represents the area determined by angle cutoff Ca.
For atom i, the atoms inside the cyan circle, including atom i1, atom i2, and atom i3, are all its angle neighbors.
Specifically, the relationship between the angle neighbor’s cutoff Ca and the edge neighbor’s cutoff Ce satisfies:
Ce = C2
a.
(5)
Through the limitation of angle neighbor cutoff, we’ve successfully enhanced the model’s predictive performance by
leveraging angle information. Simultaneously, this restriction has effectively minimized the time impact on predictions
caused by incorporating angle information.
4
A
 j
ij
d
Node
Embedding
Interaction Block
Interaction Block
Interaction Block
Embedding
Message
Passing
Average Pooling
Readout
Output
(
1)
l
ih

( )
( )
l
l
i
j
ij
h
h
s


MLP 
MLP 
LN
ReLu
j NBi

(1)
h
(2)
h
(3)
h

i

Interaction 
Block
Structure
Embedding
Structure
Embedding
Node
Embedding
A
MLP 
MLP 
Number
Group
||
MLP 
(0)
h
(0)
h
 j
dij
i

SBF
RBF
MLP 
MLP 
MLP 

ijs
ijs
AC
Add
MLP 
(a)
(c)
(d)
(b)
( )l
ih
Figure 3: Illustrations of detailed Architecture of ADA-GNN. (a): The overall structure of ADA-GNN. (b): An
illustration of the node embedding module. (c): An illustration of the structure embedding module. (d): An illustration
of the interaction block.
3.3
Crystal Material Property Prediction
Utilizing the previously acquired edge distances d and angles α, the graph structure GM for the crystal material M is
expressed as follows:
GM = {Ai, dij, αi, αj}∀i∈I,
(6)
where I encompasses all atoms within the unit cell, and j denotes the edge neighbor of atom i. The calculations for αi
and αj are presented in formulas (3) and (4).
For a given crystal structure M, our goal is to predict the value of crystal energy-related properties, such as formation
energy, band gap, and energy hull (ehull). Consequently, the crystal property prediction is represented as GM → y ∈ R
in regression tasks, where y denotes the crystal property value.
4
ADA-GNN
To leverage the dual scale modeling of crystal in graphs, we propose Atom-Distance-Angle Graph Neural Network
(ADA-GNN), which is composed of embedding module, message passing module and readout module as shown in
Figure 3. Different from existing works, we process the atom features and structure information separately in the
embedding module. Furthermore, the time cost analysis is provided to theoretically show the efficiency of the proposed
approach.
4.1
Embedding Module
In this section, we describe how to embed the elements in GM. The elements can be classified into atom attributes
and structural information. Thus, we design node embedding blocks to process the atom features (Ai), and structure
embedding blocks to process the bond distance (dij) and bond angles (αi, αj).
5
4.1.1
Node Embedding Block
As depicted in Figure 3(b), the initial atom representation is computed based on their respective atom numbers and
corresponding groups, which is shown as follows:
AN = MLP(NumA),
AG = MLP(GrpA),
h(0) = MLP(AN∥AG),
(7)
Here, NumA represents the atom number, GrpA denotes the group number, and ∥ signifies concatenation. Additionally,
AN and AG stand for their corresponding embeddings. The assignment of group numbers is conducted manually,
aligning with the configuration outlined in CGCNN [7]. The initial atom embeddings, denoted as h(0), are obtained
through the concatenation of AN and AG.
4.1.2
Structure Embedding Block
In this section, we integrate both bond distance and angle information. This involves modeling the edge length
information for each edge alongside the corresponding angle formed by that specific edge.
As shown in Figure 3(c), the initial structure embedding in architecture is computed as:
sij = MLP(MLP(SBF(αi, αj)) ⊙ MLP(RBF(dij)))
(8)
where the formulas for RBF and SBF are
RBF(dij) = jl
ϕln
Ce
dij

,
SBF(αi, αj) = Y m
l (αi, αj).
(9)
Here, jl is a spherical Bessel function of order l, and ϕln is the n-th root of the l-order Bessel function. The edge
neighbor cutoff is represented by Ce. Y m
l
is a spherical harmonic function of degree m and order l.
Note that the existing methods use one embedding block to process the atom features and bond distances [7, 19, 2, 3, 8, 9].
Our approach goes beyond by modeling atom features and structural information separately. There are two benefits of
such design: (1) each embedding block can focus on one kind of information; (2) our approach can be easily extended
to incorporate additional structural information.
4.2
Message Passing Module
Based on the outputs from the embedding module, the message passing module with several interaction blocks in series
connection is designed. The message passing module aggregates the feature embedding and structure embedding of
neighbors, which is shown as follows:
h(l)
i
= f (l)(h(l−1)
i
, h(l−1)
j
, sij)j∈NBi,
(10)
where f (l) represent the l-th interaction block, and NBi is the edge neighbors of atom i, which are determined by the
edge neighbor cutoff value. The primary goal of the formula is to convolve the features of all neighboring atoms j onto
atom i.
4.2.1
Interaction Block
The pipeline of interaction block is shown in Figure 3(d), which shows the process of updating the feature embedding
h(l)
i
for atom i is. The fusion of node embedding is computed as follows:
e(l)
ij = MLP((h(l)
i ∥h(l)
j ∥sij)),
e(l)′
ij
= Active(e(l)
i ),
ti =
X
j∈NBi
(s(l)′
ij
⊙ s(l)
ij ),
h(l+1)
i
= ReLu(h(l)
i
+ LNorm(ti)).
(11)
6
e(l)
ij represents the concatenated embedding of h(l)
i , h(l)
j , and sij, while e(l)′
ij
denotes its activated form. Symbols ⊙ and
∥ respectively denote the Hadamard Product and concatenation operations. The variable ti aggregates all neighbor
information of node i, serving as an intermediate aggregation. Ultimately, the updated node embedding h(l+1)
i
is
obtained by applying activation to the sum of h(l)
i
and layer normalization to ti. Note that the first interaction block
takes the atom embedding h(0) and structural embedding sij as inputs.
4.3
Readout Module
Lastly, the readout module utilizes average pooling to compile and consolidate the atom features from all atoms within
the unit cell. The final prediction is computed as
ˆy = MLP( 1
N
N
X
n=1
h(L)
n ),
(12)
where h(L)
n
represents the final embedding acquired as atom n undergoes a total of L interaction blocks. The overarching
loss function is founded on the principle of minimizing the average error between predicted and actual values. This
calculation method is widely adopted in related literature [7, 2, 3, 1, 9].
Based on the ˆy, we introduce the loss of ADA-GNN. We denote the parameters of ADA-GNN as θ for simplicity. For
each episode, θ is updated through gradient descent:
θ′ = θ − λ∇Lθ(f(θ)),
(13)
where λ is the learning rate, and the L is the loss function of function f(θ) with respect to the parameter θ.
The loss Lθ is computed across the mini-batch:
Lθ(f(θ)) = 1
B
B
X
b=1
( ˆyb − yb)2,
(14)
where B represents the batch size, ˆyb is the predicted value calculated from formula (12), and yb is the ground truth.
4.4
Training Algorithm and Time Cost Analysis
The training process of the ADA-GNN is outlined in Algorithm 1. As the ADA-GNN is specially designed based on the
GM, we incorporate the calculation of GM as part of the training process. Thus, the training algorithm has two phases:
the calculation of GM and the training of ADA-GNN.
The first phase involves computing node attributes, edge attributes, and angle attributes. The distance d between atoms is
directly calculated using the formula in the third row. Following this, lines 4-9 of the algorithm illustrate the calculation
of the angle α. Once all the inputs are gathered, we have the second phase. Within our architecture, rows 16-18 handle
the computation of structural attributes, while row 15 deals with node features. The message passing operation within
rows 20-22 amalgamates structural attributes and neighboring information, consequently updating node attributes. In
row 23, predictions are generated utilizing the updated node attributes. Subsequently, the loss function is computed in
lines 25 and 26, respectively, and the network parameters are updated accordingly. This entire training process iterates
until reaching the maximum number of training rounds.
After training, the inference time for crystal materials is strongly affected by the volume of the inputs. Thus, we provide
an analysis of the input volume of ADA-GNN. The volume of bond distance is O(NM), where N is the number of
atoms in the crystal cell, and M is the average number of edge neighbors for each atom. On the other hand, the volume
of angle information is O(NMK), where K is the average number of angle neighbors for each node. Notably, as
per Formula (5), the number of edge length neighbors M is a square multiple of the number of angle neighbors K.
Consequently, the overall volume of both edge and angle information is O(NM log M). As the ALiGNN is the only
one considering the bond angle information, we take ALiGNN as a comparison. The volume of inputs in ALiGNN is
O(NM 2). Thus, we make a notable advancement, as compared to the O(NM 2) with one kind of cutoff, signifying a
reduction in computational complexity by one logarithmic dimension. The inference time is also compared in Section
5.3.
7
Algorithm 1: The Training Algorithm of ADA-GNN
Input: A = [a1, ..., aN]⊤ ∈ AN, X = [x1, ..., xN]⊤ ∈ RN×3, L = [l1, l2, l3]⊤ ∈ R3×3
Output: θ
1 Part I: Graph Structure Acquisition
2 for i = 1 to N do
3
for j = 1 to mi do
4
Compute dij via: dij = ||xj − xi||2
5
for k = 1 to ri do
6
Get αiik via: αiik = angle(−→
iik, ⃗ij)
7
end
8
for k = 1 to rj do
9
Get αjjk via: αjjk = angle(−→
jjk, ⃗ji)
10
end
11
end
12 end
13 Part II: Model Training
14 while not done do
15
for b = 1 to B do
16
for i = 1 to N do
17
Compute h0 via formulate (7). for j = 1 to mi do
18
Compute embij via formulate (8).
19
end
20
end
21
for i = 1 to N do
22
Update hi via formulate (10)
23
end
24
Compute ˆy via formulate (12)
25
end
26
Compute Lθ(f(θ)) via formulate (14) Update θ via formulate (13)
27 end
5
Experiments
The following research questions guide the remainder of the paper: (Q1) can our proposed ADA-GNN outperform
SOTA baselines; (Q2) is there any improvement in prediction time compared to similar algorithms; (Q3) how does
angel information affect the performance?
5.1
Experimental Setup
The evaluation of ADA-GNN’s efficacy and inference speed spans across two extensive material benchmark datasets:
The Materials Project-2018.6 [20] and JARVIS [21]. The number of data entries corresponding to each dataset is
presented in Table 2, enhancing the transparency and consistency of our evaluations against established benchmarks.
Seven representative baseline methods are selected: CGCNN, SchNet, MEGNET, GATGNN, ALiGNN, Matformer,
and PotNet. The main differences between each algorithm are shown in the Table 1.
Our computational setup involves the utilization of a single NVIDIA GeForce RTX 24G 3090 GPU for all tasks. In
terms of implementation, all models undergo training using the Adam optimizer [22] with a one-cycle learning rate
scheduler [23], and the learning rate is configured at 0.001, with a batch size of 64 and a training epoch count of 500
serving as our chosen training parameters.
5.2
Experimental Results
In this section, our primary goal is to respond to Q1 by conducting a comprehensive assessment of the ADA-GNN’s
performance. The evaluation metric employed here is the test Mean Absolute Error (MAE), consistent with prior studies
[1, 9, 8, 7]. As outlined in Table 3, ADA-GNN showcases remarkable performance across all tasks on both benchmark
8
Table 2: Statistics of datasets.
Dataset
Tasks
# train
# valid
# test
JARVIS
F. Energy
44578
5572
5572
T. energy
44578
5572
5572
B. Gap MBJ
14537
1817
1817
Ehull
44296
5537
5537
Materials
F. Energy
60000
5000
4239
Project
B. Gap
60000
5000
4239
Table 3: Comparison between our method and other baselines in terms of test MAE on JARVIS dataset and The Materials
Project dataset. The best results are shown in bold. The last line shows the improvement rate compared to PotNet.
Method
JARVIS
Materials Project
Formation Energy
Total energy
Bandgap(MBJ)
Ehull
Formation Energy
Band Gap
eV/atom
eV/atom
eV
eV
eV/atom
eV
CGCNN
0.063
0.078
0.41
0.17
0.031
0.292
SchNet
0.045
0.047
0.43
0.14
0.033
0.345
MEGNET
0.047
0.058
0.34
0.084
0.030
0.307
GATGNN
0.047
0.056
0.51
0.12
0.033
0.280
ALiGNN
0.0331
0.037
0.31
0.076
0.0221
0.218
Matformer
0.0325
0.035
0.30
0.064
0.0210
0.211
PotNet
0.0294
0.032
0.27
0.055
0.0188
0.204
ADA-GNN
0.0288
0.031
0.25
0.043
0.0182
0.197
(2.04% ↑)
(3.13% ↑)
(7.41% ↑)
(21.82% ↑)
(3.19% ↑)
(3.43% ↑)
datasets. When benchmarked against the latest state-of-the-art method, PotNet [9], our algorithm achieves a notable
performance improvement of 2.04% to 21.82%. This underscores the effectiveness of our network architecture in fully
harnessing the role of bond angles, achieving performance levels unattainable solely through the utilization of bond
distances.
Specifically, our approach demonstrates superior performance, with enhancements ranging from 9.63% to 43.32%
across diverse tasks compared with ALiGNN [1]. While both our architecture ADA-GNN and the ALiGNN approach
incorporate angle information, there are two notable differences between our ADA-GNN and ALiGNN. Firstly, our
network employs a dual-scale cutoff, effectively leveraging less critical angle information compared to ALiGNN.
Secondly, there is another difference that lies in the construction of the structure embedding: ALiGNN uses one
embedding block to incorporate the information of edge length, edge angle, and node features. In contrast, we introduce
the node embedding block and structure embedding block that segregates structural embeddings from node embeddings,
thereby mitigating the impact of node feature updates on structural attributes. The results reveal that our network utilizes
less angle information than the ALiGNN while outperforming it. This outcome underscores the robust effectiveness of
our proposed ADA-GNN architecture.
5.3
Prediction Time Cost Analysis
In this section, we aim to answer question Q2 by analyzing the prediction time cost of ADA-GNN. In the realm of new
material discovery, employing machine learning algorithms for crystal property prediction primarily aims to expedite
the screening process for newly generated crystal stabilities [10]. Consequently, beyond the precision of crystal property
predictions, the time invested in forecasting these properties serves as a crucial metric for assessing the efficacy of the
methodology.
Based on this, we compare the predicating time using the JARVIS formation energy dataset with the most related
method, ALiGNN, which also computes angles, and the most recent method, MatFormer, and PotNet. The column
labeled ""Total Prediction Time"" denotes the cumulative time expended in predicting 5572 molecules within the test
set, while, the ""Per Crystal Time"" column articulates the average time allocated for predicting each individual crystal
molecule.
9
Table 4:
Prediction time compared with Matformer,
ALiGNN, and PotNet on JARVIS formation energy predic-
tion.
Method
Prediction Time
Total Prediction Time
Per Crystal Time
Matformer
2.05min
23ms
Potnet
1.08min
12ms
ALiGNN
5.41min
61ms
ADA-GNN
1.8min
19ms
Table 5: Ablation studies for the effects of ignoring an-
gles.
Tasks
Ours
Ours without angles
F. Energy (eV/atom)
0.0288
0.0301 (4.51% ↓)
T. energy (eV/atom)
0.031
0.033 (6.45% ↓)
B. Gap(MBJ) (eV)
0.25
0.32 (28.0% ↓)
Ehull (eV)
0.043
0.070 (62.8% ↓)
F. Energy (eV/atom)
0.0182
0.0189 (3.85% ↓)
B. Gap (eV)
0.197
0.200 (1.52% ↓)
Compared with ALiGNN, our ADA-GNN is more efficient. As shown in Table 4, our ADA-GNN is nearly three
times faster in inference time for the whole test set. This result indicates that our dual scale neighbor acquisition
mechanism can significantly reduce prediction time, thereby meeting the needs of downstream tasks. Compared to
methods exclusively targeting bond distance, our algorithm sustains competitive prediction times. While ADA-GNN
falls behind the efficiency-focused bond distance-based PotNet, it outperforms PotNet in performance. Notably, our
method excels in prediction time compared to MatFormer, another bond distance-focused approach, underscoring the
efficiency gains achieved by our algorithm.
5.4
The Affection of Missing Angle Information
In this section, our objective is to address Q3 by emphasizing the pivotal role played by angle information within
the ADA-GNN. We conduct experiments on The Materials Project-2018.6 and JARVIS tasks, employing test Mean
Absolute Error (MAE) as the evaluation metric.
To underscore the significance of angle information, we compare the ADA-GNN model with ‘Ours without angles’, a
variant that exclusively considers distances while disregarding angle information. The initial structure embedding in
‘Ours without angles’ is computed as the following formula :
sij = MLP(MLP(RBF(dij))).
(15)
For a fair comparison, the two architectures have the same settings for all tasks except for structural embedding.
As illustrated in Table 5, excluding angle information results in a performance decline, reducing the accuracy by a
range of 1.52% to 62.8%. This stark contrast underscores the pivotal role that angle information plays within the
representation, highlighting its impact on predictive accuracy.
6
Conclusion
The infinite bond angle problem hinders the utilization of bond angle in modeling crystal materials. To alleviate this
issue, we propose a dual scale modeling in selecting neighbors: a larger scale truncation for edge neighbors and a smaller
scale truncation for angle neighbors. Then, we propose a novel approach named ADA-GNN for property prediction
tasks based on the dual scale modeling. The atom features and structure information are processed independently, which
can enhance training stability and improve the accuracy of model predictions. Extensive experiments are conducted on
two large-scale real-world datasets compared with the seven graph-learning-based methods. The results show that the
ADA-GNN achieves state-of-the-art (SOTA) results. Furthermore, the ADA-GNN is 3.21 times faster than ALiGNN in
terms of inference time per crystal.
10
References
[1] Kamal Choudhary and Brian DeCost. Atomistic line graph neural network for improved materials property
predictions. npj Computational Materials, 7(1):185, 2021.
[2] Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, and Shyue Ping Ong. Graph networks as a universal machine
learning framework for molecules and crystals. Chemistry of Materials, 31(9):3564–3572, 2019.
[3] Steph-Yves Louis, Yong Zhao, Alireza Nasiri, Xiran Wang, Yuqi Song, Fei Liu, and Jianjun Hu.
Graph
convolutional neural networks with global attention for improved materials property prediction. Physical Chemistry
Chemical Physics, 22(32):18141–18148, 2020.
[4] Johannes Gasteiger, Janek Groß, and Stephan Günnemann. Directional message passing for molecular graphs.
arXiv preprint arXiv:2003.03123, 2020.
[5] William D Callister Jr. Materials science and engineering an introduction. 2007.
[6] Charles Kittel. Introduction to solid state physics. John Wiley & sons, inc, 2005.
[7] Tian Xie and Jeffrey C Grossman. Crystal graph convolutional neural networks for an accurate and interpretable
prediction of material properties. Physical review letters, 120(14):145301, 2018.
[8] Keqiang Yan, Yi Liu, Yuchao Lin, and Shuiwang Ji. Periodic graph transformers for crystal material property
prediction. Advances in Neural Information Processing Systems, 35:15066–15080, 2022.
[9] Yuchao Lin, Keqiang Yan, Youzhi Luo, Yi Liu, Xiaoning Qian, and Shuiwang Ji. Efficient approximations of
complete interatomic potentials for crystal property prediction. arXiv preprint arXiv:2306.10045, 2023.
[10] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk.
Scaling deep learning for materials discovery. Nature, pages 1–6, 2023.
[11] Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, and Tommi S Jaakkola. Crystal diffusion variational
autoencoder for periodic material generation. In International Conference on Learning Representations, 2021.
[12] Yong Zhao, Edirisuriya M Dilanga Siriwardane, Zhenyao Wu, Nihang Fu, Mohammed Al-Fahdi, Ming Hu, and
Jianjun Hu. Physics guided deep learning for generative design of crystal materials with symmetry constraints.
npj Computational Materials, 9(1):38, 2023.
[13] Zhengyang Wang, Meng Liu, Youzhi Luo, Zhao Xu, Yaochen Xie, Limei Wang, Lei Cai, Qi Qi, Zhuoning Yuan,
Tianbao Yang, et al. Advanced graph and sequence neural networks for molecular property prediction and drug
discovery. Bioinformatics, 38(9):2579–2586, 2022.
[14] Alex Morehead, Chen Chen, and Jianlin Cheng. Geometric transformers for protein interface contact prediction.
arXiv preprint arXiv:2110.02423, 2021.
[15] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn
Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al.
Highly accurate protein structure
prediction with alphafold. Nature, 596(7873):583–589, 2021.
[16] Logan Ward, Ankit Agrawal, Alok Choudhary, and Christopher Wolverton. A general-purpose machine learning
framework for predicting properties of inorganic materials. npj Computational Materials, 2(1):1–7, 2016.
[17] Paul Raccuglia, Katherine C Elbert, Philip DF Adler, Casey Falk, Malia B Wenny, Aurelio Mollo, Matthias Zeller,
Sorelle A Friedler, Joshua Schrier, and Alexander J Norquist. Machine-learning-assisted materials discovery using
failed experiments. Nature, 533(7601):73–76, 2016.
[18] Anton O Oliynyk, Erin Antono, Taylor D Sparks, Leila Ghadbeigi, Michael W Gaultois, Bryce Meredig, and
Arthur Mar. High-throughput machine-learning-driven synthesis of full-heusler compounds. Chemistry of
Materials, 28(20):7324–7331, 2016.
[19] Kristof Schütt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and
Klaus-Robert Müller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions.
Advances in neural information processing systems, 30, 2017.
[20] Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen Dacek, Shreyas
Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, et al. The materials project: A materials genome approach
to accelerating materials innovation, apl mater. 2013.
[21] Kamal Choudhary, Kevin F Garrity, Andrew CE Reid, Brian DeCost, Adam J Biacchi, Angela R Hight Walker,
Zachary Trautt, Jason Hattrick-Simpers, A Gilad Kusne, Andrea Centrone, et al. The joint automated repository
for various integrated simulations (jarvis) for data-driven materials design. npj computational materials, 6(1):173,
2020.
11
[22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
[23] Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large learning
rates. In Artificial intelligence and machine learning for multi-domain operations applications, volume 11006,
pages 369–386. SPIE, 2019.
12
"
