abstract,introduction,methodology,results,conclusion,title,author,textdata,literature review
"We propose a simple greedy algorithm for reweighting data to solve multi-criteria optimization problems arising in machine learning applications. We also explore its application to medicinal chemistry, specifically the task of identifying small molecules that bind to MNK2 but not to MNK1. It improved the success rate of drug discovery significantly with minimal changes in overall prediction error.","Deep learning models require a large amount of labeled data, a bottleneck especially in multi-criteria optimization tasks like filtering molecules based on multiple criteria such as selectivity. Here, we address this issue, introducing a new algorithm that allows us to train neural networks on data reweighted from one dataset to another, closely related dataset.",The algorithm reweighs one dataset using a second dataset by minimizing the Wasserstein distance between the two resulting distributions. This is done via a greedy random sampling and matching algorithm that runs in near-linear time. We prove that this algorithm achieves a provable approximation guarantee when the input instances admit a small-size covering property.,"We demonstrate the effectiveness of our algorithm on a drug discovery task, where we successfully identify small molecules that selectively bind to MNK2 over MNK1. In addition, we provide theoretical guarantees for the algorithm's accuracy and efficiency.","Our algorithm provides a practical method for reweighting data in multi-criteria optimization problems. It scales well to large datasets and offers provable guarantees, making it suitable for various machine learning applications.",Enhancing selectivity using Wasserstein distance based reweighing,Pratik Worah,"Enhancing selectivity using Wasserstein distance based reweighing
Pratik Worah∗
Abstract
Given two labeled data-sets S and T , we design a simple and efficient greedy algorithm to reweigh the
loss function such that the limiting distribution of the neural network weights that result from training
on S approaches the limiting distribution that would have resulted by training on T .
On the theoretical side, we prove that when the metric entropy of the input data-sets is bounded, our
greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network
weights will be provably close in total variation distance. Moreover, the algorithm is simple and scalable,
and we prove bounds on the efficiency of the algorithm as well.
Our algorithm can deliberately introduce distribution shift to perform (soft) multi-criteria optimization.
As a motivating application, we train a neural net to recognize small molecule binders to MNK2 (a MAP
Kinase, responsible for cell signaling) which are non-binders to MNK1 (a highly similar protein). We
tune the algorithm’s parameter so that overall change in holdout loss is negligible, but the selectivity,
i.e., the fraction of top 100 MNK2 binders that are MNK1 non-binders, increases from 54% to 95%, as
a result of our reweighing. Of the 43 distinct small molecules predicted to be most selective from the
enamine catalog, 2 small molecules were experimentally verified to be selective, i.e., they reduced the
enzyme activity of MNK2 below 50% but not MNK1, at 10µM – a 5% success rate.
1
Introduction
Deep learning has found applications in diverse areas ranging from organic chemistry to computer generated
art. Its applicability is limited by the availability of large amounts of labeled training data. A typical all-to-all
neural net of width n (say n = 10000) and depth d (say d = 10) has Θ(n2d) (≃ 108) weight parameters, and
although many neural nets work well despite being somewhat over-parameterized, they still need the number
of training examples to be of a similar or only slightly smaller order of magnitude.
This reliance on large amounts of training data leads to difficulties in training when we have multiple
objectives. For example, suppose we separately gather training data for a neural net to perform classification
tasks into a set of classes A using labeled data-set S, and into a set of classes B using labeled data-set T .
Then, unless T and S have many data-points in common, we will not have enough examples to train a neural
net for the classification into classes of A × B. In this paper, we address this problem by designing a scalable
algorithm that reweighs dataset S (using T ), so that training on the reweighted S leads to network that is
close to one obtained by training on T . Moreover, we apply it to a drug discovery application and obtain
wet-lab verified results.
Our main contribution is theoretical. Algorithm 1 deliberately introduces distribution skew and reweighs
the labeled training data-set S using the data-set T so that if we train a neural network on the reweighed S
for a long enough period of time then its weights will be ""tilted"" so that the classification error on classes
B in T will also be reduced. The amount of reduction is determined by the choice of tilt parameter α in
Algorithm 1. Theorems 5.4, 5.13, 5.15, and 5.16 formally show correctness and efficiency of Algorithm 1. In
particular, Theorem 5.4 proves correctness and also justifies the choice of Wasserstein metric in Algorithm 1.
Theorems 5.13, 5.15 and 5.16 provide efficiency guarantees for Algorithm 1.
It has been known since the 1980s that greedy algorithms, like Algorithm 1, have poor approximation
guarantees for computing Wasserstein distance [20]. Therefore, we need to assume and exploit some property
∗Google Research, pworah@google.com
1
arXiv:2401.11562v1  [stat.ML]  21 Jan 2024
of about our input instances to get around the lower bound in [20]. This critical property turn out to be:
input instances must admit a small size covering. This small covering assumption is used in both the random
sampling and the greedy reweighing steps of our algorithm. The connection to random sampling is not
surprising and it comes from known techniques – the union bound in the large deviations proof. However,
the connection between small coverings and greedy matching algorithms (at its core computing Wasserstein
distance is equivalent to computing minimum weight matchings) is somewhat surprising, since we are not
aware of results obtaining sharper guarantees on approximate minimum weight matching, based on the
covering properties of the input data-set.
Organization: In Section 2, we describe an example application to drug discovery. More theoretically
inclined readers may skip this section and any references to it. In Section 3, we discuss prior work from the
areas of machine learning theory, algorithms and computational drug discovery, relevant to our paper. In
Section 4, we present Algorithm 1 and provide a technical overview of our paper – how our various theoretical
results fit together. Section 5 presents the theorem statements. In particular, Theorem 5.4 explains why the
Wasserstein metric is an intuitive and appropriate choice of metric for Algorithm 1; Theorems 5.13, 5.15
and 5.16 show that the greedy random sampling based algorithm can compute the minimum weight bipartite
matching, and hence Wasserstein distance, in near linear time; and they provide an upper-bound on the
approximation error under our low metric entropy assumption. Thus showing that Algorithm 1 is scalable.
Finally, the supplement contains missing proofs.
2
Example application: drug discovery
Multicriteria optimization problems involving data-sets that admit small coverings arise naturally in drug
discovery (see Section 4 for details). Therefore, as a concrete motivating example, we illustrate an application
of Algorithm 1 to a toy problem in this area.
In drug discovery, typically, one wants to isolate small molecules (inhibitors) that bind strongly to a
given enzyme, but often we want to exclude small molecules that bind to another similar enzyme. For
example, MNK1 and MNK2 are two structurally similar kinases (a kinase is an enzyme for phosphorylation
or de-phosphorylation of proteins). We want to identify small molecules that bind strongly to MNK2 (MNK2
hits), but we also prefer that the identified small molecules not bind to MNK1 (MNK1 non-hits). In other
words, we want to isolate molecules that are selective for MNK2 over MNK1.
In the in-silico experiments, we were able to increase the percentage of MNK1 non-hits in our set of top
predicted MNK2 hits – the selectivity – from 54% to 95% on holdout data, using the reweighing procedure
in Algorithm 1. We used a relatively small training set of about 250K small molecules in total; labeled as
MNK1 non-binders, and MNK2 binders and non-binders; and a small holdout set of 7K small molecules that
consists of molecules which are labeled as: MNK2 hits (binders), and MNK1 hits (binders) or MNK1 non-hits
(non-binders). In Figure 1, for the neural network models with and without reweighing, we plot the cumulative
number of MNK1 non-hits on the y-axis; and on the x-axis any given point, say k, represents the top k
predicted MNK2 hits from the examples in the holdout set. While we can not make our training data-sets
and code public for proprietary reasons, we were able to experimentally (in wet-lab) verify that two out of
the top fifty (actually 43, since 7 out of 50 molecules could not be synthesized and tested) predicted selective
small molecules, obtained by running our neural network model on the Enamine 1.9B molecules catalog
(https://enamine.net), were indeed selective for MNK2 over MNK1.1 That is a success rate of roughly 5%
on this admittedly small sample set. We do note that the results are from a single point concentration assay
and can be noisy.
We are not aware of other such multi-target prediction results in DNA encoded library (DEL) space
(see [22] for background), where one simultaneously predicts hits/non-hits against two or more proteins.
However, the success rates for single target experiments with traditional high-throughput screening is ∼ 1%
(see for example the discussion in [17]) and it is generally accepted that multi-target prediction is a harder
problem.
1The compounds are Z1918489591 and Z5890616727 in the enamine catalog.
2
0
20
40
60
80
100
#examples
0
20
40
60
80
num of mnk1 no-hits, mnk2 hits
baseline
reweighed
Figure 1: Selectivity of reweighed (using Algorithm 1) and baseline (without reweighing) neural nets. Note
that this increase in selectivity from 54% to 95% came without any significant change in the validation loss –
the AUC for the classification of MNK2 binders vs non-binders remained around 0.6 in both cases.
More importantly, the two predicted and assay tested molecules provide a degree of verification for our
experimental application (which has been the motivation, but is not the focus of this paper).
Figure 2: Two predicted and verified selective MNK1 non-hits and MNK2 hits from the Enamine catalog.
The enzyme activity was found to be above 50% for MNK1 but below 50% for MNK2 at 10µM concentration
for each of the two small molecules: ∼ 20% vs 70% and ∼ 39% vs 59%. Note that these values are from single
point concentration assay and can be noisy.
3
Related work
The question of learning with differing test and train distributions has been well investigated in the machine
learning community under different names: ditribution shift and covariate shift, see for example the book [19],
the papers [24, 21, 10, 3, 13] and [7], to name just a few. The question is also relevant to our paper since our
algorithm can be used to reweigh the train data-set to bring the post training neural network weights closer
to what they would have been, had we trained based on the test distribution. However, prior results rely on
estimating the train and test distributions. For example, parameter estimation of the densities followed by
change of variable using the Jacobian. Assuming a logarithmic number of features, that leads to a ˜O(n2)
algorithm for distribution skew correction (n being the training and test data-set size) – much more efficient
3
than Wasserstein distance computation that requires solving a Θ(n2) sized linear program. That explains why
the Wasserstein distance based ideas are less explored in this context, so far. Distribution shift correction
has also found new applications in domain adaptation literature. However, the only prior theoretical works
involving Wasserstein distance computation that we found in this area were [8] and [16], which focus on exact
solution of the Wasserstein distance problem.
In a high dimensional feature space, the density estimation can be inaccurate since the number of samples
required increases exponentially in number of features for any formal guarantee (as can be seen from large
deviation bounds [9]). Moreover, if we are only interested in partial tilting of one distribution towards another,
as in Algorithm 1 (where α controls the amount of tilt), then it is reasonable to look for approximate but
efficient computation of Wasserstein distance. That is what we do in this paper using Algorithm 1, which is a
˜O(n2) time algorithm as well. We now have the added advantage of a provable upper bound on the test-train
prediction error, before and after tilting using Algorithm 1, from Theorem 5.4.
The problem of efficient Wasserstein distance computation has also received much attention in the
algorithms community. The paper [23] studies the equivalence between Wasserstein distance computation
and matching algorithms in the metric space setting. Efficient matching algorithms have been well studied in
literature for five decades. The optimal algorithm for computing weighted matchings is due to Gabow and
Tarjan [11] and runs in time O(m√n), where m is the number of edges and n the number of vertices in the
graph. Since then more sophisticated algorithms have been designed, see for example [25, 14, 23] and [1] to
name a few. However, under our assumptions even the simple greedy algorithm performs remarkably well,
and it scales efficiently for large training data-sets.
Reingold and Tarjan [20] showed that the greedy algorithm has an abysmal approximation ratio of
nlog2 3/2 for bipartite graphs. In this paper, we show in Theorem 5.13 that the approximation ratio of the
greedy algorithm is much better under our bounded metric entropy assumption than the lower bound in [20].
Hence, an assumption about a covering property of the input leads to more optimal matchings – a somewhat
surprising algorithmic result that may be of independent interest.
Finally, the idea of using deep learning for drug discovery has gained popularity in pharmaceutical research
over the last few years, especially given the amount of data now available [18]. The paper [17] shows that
neural nets can be trained on DNA encoded chemical libraries to identify new small molecules that bind to a
given protein target. It is particularly relevant to this work, as we build upon that. Our work extends their
work by allowing us to select molecules that bind to one protein target and not to another. Other papers in
this rapidly growing area include [26, 15] and [12].
4
Problem statement and overview of results
Suppose we are given two labeled training data sets, say S and T for two different classification tasks.
Moreover, let’s assume that the points in the data-sets are weighted according to two different probability
distributions, say PS and PT respectively. Our goal is to reweigh S, i.e., ""tilt"" PS towards PT , and train a
neural net classifier so that the limiting distribution of network weights is closer to the one that would be
obtained from training using PT . We assume labels of S are known, and the labels of T may be unknown or
they may be known but |T ∩ S| may be small. Our reweighing algorithm handles both cases. Furthermore,
our reweighing procedure (Algorithm 1) works efficiently on very large data-sets, with provable guarantees.
For our drug discovery example, the set T consists of a subset of small molecules2 labeled non-binders
(non-hits) for the protein MNK1, and the full labeled training set consists of molecules that are binders and
non-binders for the protein MNK2, while the set S is not the full training set but just the set of binders to
MNK23. Here the labels of the molecules in T are known but not necessarily on the same molecules as S.
PS and PT may be assumed to be uniform distributions supported on S and T respectively. Given a new
small molecule, one now wants to compute the likelihood that it is a binder for MNK2 and a non-binder for
2As an aside, each small molecule is usually mapped to a 2K character long binary string (fingerprint) of features. Thus, in
this context, one may think of the underlying space of small molecules as a subset of the boolean hypercube in dimension 2K.
3Note that we have the labeled set of non-binders to MNK2 in our training examples as well, but they remain unaffected by
the reweighing, though they are used in training as well.
4
MNK1. Such models can allow us to make predictions on large commercially available catalogs and enrich
compounds that have high likelihood to bind to MNK2 but not MNK1. This formulates the drug discovery
application in our formal notation.
The rest of the paper concentrates on providing a theoretical explanation for why Algorithm 1 should
work as intended and scale well in general; beyond the specific experiments with MNK1-MNK2.
Algorithm 1 Reweigh Distribution and Train
1: Input: Two data-sets: S and T of size n each, points weighed according to PS and PT respectively, and a
tilt factor α ∈ [0, 1].
2: Output: Compute a distribution P′
S on S such that the invariant distribution of network weights of
a neural net model, trained using SGD with dataset S and weights P′
S, will be closer (in Wasserstein
metric) to the invariant distribution of network weights of a neural net model trained using SGD on T
with points weighted as PT .
3: ▷ RandomSample returns an empirical probability distribution computed from sample size m.
4: ▷ RS ⊆ S and RT ⊆ T denote the random sample of points from their respective ground sets.
5: PRS := RandomSamplem(S, PS)
6: PRT := RandomSamplem(T , PT )
7: ▷ Obtain a α-tilted version of PRS that’s close to PRT using greedy minimum weight metric bipartite
matching algorithm (ScaledGreedyReweight in supplement)
8: P′
RS := ScaledGreedyReweight(PRS, PRT , α)
9: ▷ Obtain a reweighted version of S
10: P′
S = (1 − α)PS + αP′
RS.
11: ▷ Train neural net on P′
S.
12: Use stochastic gradient descent (SGD) to train the neural net using P′
S.
A difference in PS and PT results in a difference in the convergence point of the weights in any neural
net training procedure, like SGD. This is because the loss functions in the SGD algorithm will differ in
their weights, though they may have the same form. Therefore, given two mean squared error loss functions
weighted with different probability distributions, say PS and PT , on each of their terms, a natural question
is: what is the relation between the output of two neural nets that are trained using the two different loss
functions?
The expected output of any neural net, on any given input, at the end of a long enough training period,
depends on the invariant measure of the weights from the SGD training algorithm. The 1-Wasserstein
distance4 between the weights in the loss functions is the same as the 1-Wasserstein distance between
the occurrence frequencies of data-points in the two underlying data-sets S and T . Our first theoretical
contribution, Theorem 5.4, shows that W1(PS, PT ), the 1-Wasserstein distance between the loss function
weight distributions PS and PT , upper-bounds the total variation distance between the invariant measures of
two such neural nets. Therefore, if the data-sets S and T are not identical, i.e., PS and PT do not have the
same support set, then the best we can hope for is a small W1(PS, PT ), in order to obtain similar neural net
models for interchangeability. The above explains our choice of the 1-Wassertein metric in Algorithm 1 (see
also Remark 5.5 for comparing with the Levy metric).
Let’s assume W1(PS, PT ) is large, so that we need to reweigh S. For reweighing, we do not remove any
examples from S, instead we prefer to increase the weight of some already present examples in S. Therefore,
we compute a distribution P′
S with the same set of support as PS, such that it minimizes 1-Wasserstein
distance between (1 − α)PS + αPT and P′
S, for some fixed choice of tilt factor α ∈ [0, 1].5 We use P′
S as the
new set of weights for neural net training. Note that, for α close to 1, the distribution will be closest to PT
while being supported on S.
Note that the optimal P′
S mentioned above can be computed by solving a linear program that closely
resembles the 1-Wasserstein distance computation linear program. However, the size of the linear program
4See, for example [4] and the references therein for background on the Wasserstein distances.
5The optimum value of α can be chosen by binary search after running multiple training evaluations.
5
would be quadratic in the size of the data-sets, making the computation intractable for most practical data-
sets.6 Therefore, we look for inaccurate but efficient algorithms and a natural candidate is the randomized
greedy algorithm below.
The 1-Wasserstein metric has an equivalent interpretation as a optimal transport problem. See Theorem 5.6
(essentially repeated from [23]) for a formal statement that reduces it to the metric minimum weight bipartite
matching problem [20].
Given a bipartite graph with vertices embedded in a metric space, the metric minimum weight bipartite
matching problem asks to compute a minimum weight matching, where the weight of a matching is the sum
of the lengths of edges in the matching.
One tractable way to compute a minimum weight bipartite matching is to use a faster but sub-optimal
algorithm. The greedy algorithm, formally studied by [20] in this context, is a natural contender. It is
almost linear time, and easy to implement. However, [20] showed that such greedy algorithms can be really
inaccurate. Moreover, even the greedy algorithm requires linear space and given the size of our data-sets,
that can also become a constraint.
However, if our input instances admit a small sized covering then we show that the greedy algorithm
run on a large enough random sample of data, i.e., Algorithm 1 for a large enough choice of m, performs
reasonably well. Our main contribution here is Theorem 5.16. Theorem 5.16 states that the greedy algorithm
on a small fraction random sample of an Θ(n) point data-set can be used to approximate the 1-Wasserstein
distance with a poly-logarithmic factor approximation.
So, the question arises: What precisely does a small covering assumption above mean and what for kind
of natural problems does Algorithm 1 scales efficiently without deterioration in the approximation guarantee?
The metric entropy of a point set (see Definition 5.10) is the minimum number of balls of a given radius
required to cover the point set. So, in high dimensions, data-sets with low metric entropy can be characterized
from their values on (relatively) small balls spread through space. This is indeed the case with DEL data-sets
like MNK1-MNK2 in our drug discovery application. The combinatorial synthesis process utilized in DELs
often results in local chemical similarity among compounds that share common building blocks. Since similar
molecules likely have the same binding behavior, synthesized molecules form a small ball around a parent
molecule in the molecule fingerprint space. Therefore, molecule binding vs non-binding data-sets likely have
low metric entropy. It turns out that for training data-sets with low metric entropy the greedy algorithm
of [20] performs provably well (cf. Theorem 5.13).
The proof of Theorem 5.16 relies on Theorems 5.13 and 5.15.
Theorem 5.13 show that greedy minimum weight matching on bipartite graphs for vertex sets with low
metric entropy has a much better (poly-logarithmic) approximation guarantee in our case, as opposed to the
polynomial approximation guarantee from [20]. This argument, especially the connection between covering
and matching in Lemma 5.12 may be of independent interest.
Theorem 5.15 essentially shows that random samples on data-sets with bounded metric entropy preserves
1-Wasserstein distances. This allows us to work with small samples of large data-sets. For the proof, we need
large deviation bounds for the 1-Wasserstein distance between the theoretical distribution and its empirical
distribution. Such results have been explored previously with tight Sanov’s theorem type bounds in low
dimensional spaces (see for example [4]), but in high dimensions, we need the assumption of low metric entropy
for the same results to go through (see chapter 6 in [9]). Coincidentally, that is precisely our assumption in
Theorem 5.13!
5
Theorems and Proofs
5.1
Bounding 1-Wasserstein distance suffices
In this section, in Theorem 5.4, we show that the Wasserstein distance between two measures, corresponding
in the sum of squares loss function, upper bounds the total variation distance between the invariant measures
6A typical large data-set has 10-100M examples, and computing W1 over two such data-sets requires solving a linear program
– a Θ(n3) time procedure, resulting in the order of 1024 computational operations!
6
underlying the stochastic gradient descent (SGD) algorithms.
Assumption 5.1. We assume that our input consists of 2n points among the vertices of the hypercube:
Qd(n) := {0, 1}d(n), where d(n) is Θ(log n).
Restricting the state-space to the hypercube is fairly standard in algorithms literature. For the drug
discovery example, the state-space is just the binary molecule fingerprint vectors i.e., roughly d(n) = 2000.
Note that assuming training and test data-sets of equal size is more for clarity of presentation, as one can
add dummy example points of 0 weight, if needed.
Let X × Y denote the usual space of labeled examples i.e, in our case X ⊆ {0, 1}d(n) is the set of feature
values and Y := {0, 1} is the set of labels. Our object of interest in this section is a neural network with
smooth bounded activation functions. Let y = f(w, x) denote the abstraction of our neural network, where w
denotes the real valued vector of weights. For a depth p neural-net with polynomial activation functions of
degree q, f(w, x) is a polynomial in x with degree at most pq.
Let ℓ(·) denote the loss function, which we will assume to be the sum of square loss, for the sake of
concreteness. The ideas easily extend to any low degree loss function. The training loss can be written as:
ℓw(PS) := E(x,y)∼PS[(y − f(w, x))2].
(1)
Recall that (see for example [6]), a stochastic gradient descent algorithm with loss function ℓ can be
abstracted as the Itó diffusion in the limit of small step size:
dwS(t) = ∇wℓw(PS)dt + σSdB(t),
(2)
where ∇w denotes gradient with respect to w, B(t) denotes Standard Brownian Motion in |w|-dimensions
and the matrix σS depends upon the variance of the loss function for the mini-batch, mini-batch size and the
learning rate.
Assumption 5.2. We assume that the diffusion corresponds to an uniformly elliptic generator, since that
ensures the existence of a unique limiting (invariant) measure [2]. Furthermore, we assume σS is isotropic
i.e, it’s a scalar multiple of the identity σ · Id and that σS = σT , in Theorem 5.4.
We relax the isotropy assumption somewhat in a corollary (see supplement). Finally, for our situation of
interest, i.e., W1(PS, PT ) large, we make a covariate shift type assumption.
Assumption 5.3. We assume that W1(PS, PT ) = Ω(1). Furthermore, f and y are bounded, say y, f ∈ [0, 1]
and
|Ey∼PS (·|x)[(y−f(w,x))2]−Ey∼PT (·|x)[(y−f(w,x))2]|=O(1).
(3)
Essentially, it says the data-sets have similar average loss in the same neighborhood for a given set of
weights.
Theorem 5.4. Suppose we train two neural networks, under the assumptions 5.2 and 5.3 above, on different
input distributions, PT and PS, using the stochastic gradient descent (SGD) algorithm. Then, the total
variation distance between their invariant measures can be bounded by O(W1(PT , PS)), in the limit as SGD
step-size goes to 0.
Proof deferred to supplement.
Remark 5.5. For dimension d(n) large, the Levy-Prokhorov distance L(PS, PT ) between two distributions
can be ω(1) times the Wasserstein distance W1(PS, PT ), so a Levy-Prokhorov metric based algorithm and
guarantee can be weaker than the above.
7
5.2
The Greedy Algorithm
5.2.1
Reduction to bipartite matching
So far, we have established that 1-Wasserstein metric is a sufficient topology to work with. This leads to the
problem of computing the 1-Wasserstein distance on two large datasets. That problem is equivalent to the
minimum weight bipartite matching problem. In particular, we have the following lemma from [23].
Theorem 5.6. [23] Given an instance of the optimal transport problem with supply and demands on two
sets of points (R, B), i.e, equivalently the 1-Wasserstein distance computation problem in our case; we can
construct an instance of the minimum weight bipartite matching problem such that solving the latter up to an
approximation factor α will solve the former up to the same approximation factor α.
The Algorithm ScaledGreedyWeight (below) carries out the reduction in Theorem 5.6 and calls Greedy-
Match which matches two multisets embedded in a metric space using greedy algorithm on the edge lengths.
Notation: Scaling a discrete probability distribution P up by an integer factor of C leads to a numerical
rounding error of
1
C min{P}, where min{P} denotes the minimum positive value of density P. Assume that
we pick a large enough constant C below, so that we can ignore the rounding error for the purposes of
Theorem 5.13.
Algorithm 2 ScaledGreedyReweight (scale distributions and call bipartite matching)
1: Input: Two probability distributions PB, PR supported on B, R ⊂ Qd, and a tilt factor α ∈ (0, 1).
2: Output: Probability distribution P′
B supported on B. P′
B is close to αPR + (1 − α)PB in W1, under
assumptions of Theorem 5.13.
3: for r ∈ R do
4:
Supply(r) ← C · αPR(r)
5: for b ∈ B do
6:
Demand(b) ← C − C · (1 − α)PB(r)
7:
if Demand(b) < 0 then
8:
Demand(b) ← 0
9: Create multi-set B′, R′ with multiplicities of each element being equal to their Demand and Supply
respectively.
10: Use GreedyMatch(R′, B′) to compute the met (matched) demands, i.e., the extent to which the demands
of B that are actually fulfilled by R.
11: Normalize the weights of met demands to obtain a probability distribution P′
B supported on B.
12: return P′
B.
5.2.2
Greedy algorithm and metric entropy
Recall that the data-points are set in the d-dimensional hypercube Qd with ℓ1 metric, where d = logO(1) n.
The minimum weight bipartite matching problem is known to be harder than its non bipartite version. For
example, the greedy algorithm is known to have a lower bound of Θ(nlog2 3/2) [20] for the bipartite version
with n data-set T and n data-set S vertices. As an aside, a variant of the greedy algorithm (the hyper-greedy
algorithm) provides a log n approximation in the non-bipartite case i.e, when any vertex can be matched to
any other vertex. For the bipartite case, we obtain better approximation guarantees via the greedy algorithm,
assuming small metric entropy of the input point sets.
Definition 5.7. Given a perfect matching M over a subset of vertices C in a graph G, an alternating cycle
γ is a cycle in G such that each alternate edge in the cycle belongs to M. Note that any such M corresponds
to a set of vertex disjoint alternating cycles.
In particular, Reingold and Tarjan [20] essentially show the following theorem.
8
Theorem 5.8. [20] Given a set of n data-set T and data-set n S points in a metric space, the greedy
algorithm returns a matching with weight that is within a factor of γlog2
3
2 of the minimum weight matching,
where γ is the length of the longest alternating cycle in the set (which can be Θ(n) for Qlog n).
In order to improve upon their guarantee, we will exploit the following assumption for our input instance.
Assumption 5.9. We assume that all input T and S points can be covered by η balls of radius ζ lying within
Qd. We call such an input instance (η, ζ)-bounded. The parameters η and ζ will determine the approximation
guarantee of our algorithm.
Definition 5.10. Given a metric space, say (Q, d) and E ⊂ Q„ the metric entropy N ent
r
(E) is the largest
number of points {x1, . . . , xn} one can find in E that are r-separated, i.e., d(xi, xj) ≥ r for all i ̸= j.
Definition 5.11. Given a metric space, say (Q, d) and E ⊂ Q, the (external) covering number N cov
r
(E) is
the fewest number of points {x1, . . . , xn ∈ Q} such that the d-balls {B(x1, r), . . . , B(xn, r)} cover E.
Lemma 5.12 (Structural Lemma). For an alternating cycle γ induced by the greedy matching, if the weight
of edges in the alternating cycle coming from the greedy matching is at least α times the weight of edges in
the alternating cycle coming from the minimum weight matching then the metric entropy of γ is large i.e,
more precisely,

N ent
α/2(γ) · 2d − α
α
log2 3/2
≥ α.
(4)
Proof deferred to supplement. Note that an approximation factor of d is trivial on Qd or on any set with
dmin = 1 and dmax = d(n). The following corollary shows that the above indeed helps to improve upon the
trivial bound for appropriately bounded instances.
Corollary 5.12.1. Lemma 5.12 implies that the greedy algorithm achieves an approximation factor of o(d3/4)
on a (d3/4, d3/4)-bounded instance.
Proof. We know that N cov
r
(E) ≥ N ent
r
(E) (see for example [9]). Therefore, Lemma 5.12 implies
α ≤

N cov
α/2(γ) · 2d − α
α
log2 3/2
.
(5)
For α = d3/4, the right side of Equation 5 is dlog23/2, while the left side is d3/4. Since log2 3/2 < 3/4 we have
a contradiction. Therefore, α = o(d3/4).
Of course, as the metric entropy decreases, the approximation factor improves, see for example the theorem
below.
Theorem 5.13. For η = O(d
1
ξ log2 3/2 ), (ξ > 1), Lemma 5.12 implies that the greedy algorithm achieves an
approximation factor of max{2ζ, O

d
1+ξ log2(3/2)
ξ(1+log2(3/2))

} on a (η, ζ)-bounded minimum weight matching instance.
Together with Theorem 5.6, Theorem 5.13 implies that the greedy algorithm obtains the approximation
factor on a (η, ζ)-bounded Wasserstein distance computation instance. Proof deferred to supplement.
5.2.3
Small random samples suffice
In this subsection, we show that if the metric entropy is small, and so is the spread (see Definition 5.14) of
the underlying distribution, then the empirical distribution of a much (polynomially) smaller sample is close
to the actual distribution, in the 1-Wasserstein metric, with high probability.
9
Definition 5.14. Let µ be the uniform distribution supported on a subset of vertices Q of Qd(n). The spread
of µ, S(µ), is defined as:
S(µ) := inf
x0∈Q

1 + ln
Z
Q
ed(x0,x)2dµ(x)
1/2
,
(6)
where d(·, ·) denotes the ℓ1 distance on Qd(n).
Note that the spread is positive and greater than 1, for any distribution defined on the hypercube, since
the minimum value of d(·, ·) is 1. In general, S(µ) can be a function of d(n).
Theorem 5.15. For a (η, ζ) coverable point-set, with m = α(n)
References
[1] Alexandr Andoni, Khanh Do Ba, Piotr Indyk, and David Woodruff. Efficient sketches for earth-mover
distance, with applications. In 2009 50th Annual IEEE Symposium on Foundations of Computer Science,
pages 324–330, 2009.
[2] C. Bianca and Christian Dogbe. On the existence and uniqueness of invariant measure for multidimensional
stochastic processes. Nonlinear Studies - The International Journal, 2017.
[3] Steffen Bickel and Tobias Scheffer. Dirichlet-enhanced spam filtering based on biased samples. In
Advances in Neural Information Processing Systems, 2007.
[4] François Bolley, Arnaud Guillin, and Cédric Villani. Quantitative concentration inequalities for empirical
measures on non-compact spaces. Probability Theory and Related Fields, 137:541–593, 2007.
[5] François Bolley and Cédric Villani. Weighted csiszár-kullback-pinsker inequalities and applications to
transportation inequalities. Annales de la Faculté des sciences de Toulouse : Mathématiques, 14(3):331–
352, 2005.
[6] Pratik Chaudhari and Stefano Soatto.
Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. In International Conference on Learning Representations,
2018.
[7] Corinna Cortes, Mehryar Mohri, Michael Riley, and Afshin Rostamizadeh.
Sample selection bias
correction theory. In Proceedings of the International Conference on Algorithmic Learning Theory, 2008.
[8] Nicolas Courty, Rémi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal
transportation for domain adaptation. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30.
Curran Associates, Inc., 2017.
[9] Amir Dembo and Ofer Zeitouni. Large Deviations Techniques and Applications. Springer-Verlag Berlin
Heidelberg, 2010.
[10] Miroslav Dudik, Robert Schapire, and Steven Phillips. Correcting sample selection bias in maximum
entropy density estimation. In Advances in Neural Information Processing Systems, 2005.
[11] Harold N. Gabow and Robert E. Tarjan. Faster scaling algorithms for general graph matching problems.
Journal of the ACM, 38(4):815–853, 1991.
[12] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Doina Precup and Yee Whye Teh, editors, Proceedings of
the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pages 1263–1272. PMLR, 06–11 Aug 2017.
[13] Jiayuan Huang, Alexander Smola, Arthur Gretton, Karsten Borgwardt, and Bernhard Scholkopf.
Correcting sample selection bias by unlabeled data. In Advances in Neural Information Processing
Systems, 2007.
[14] Celina Imielinska and Bahman Kalantari. A generalized hypergreedy algorithm for weighted perfect
matching. In BIT, 1993.
[15] Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph
convolutions: moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, volume 30,
page 595–608, 2016.
11
[16] Trung Le, Dat Do, Tuan Nguyen, Huy Nguyen, Hung Bui, Nhat Ho, and Dinh Q. Phung. On label shift
in domain adaptation via wasserstein distance. CoRR, abs/2110.15520, 2021.
[17] Kevin McCloskey, Eric A. Sigel, Steven Kearnes, Ling Xue, Xia Tian, Dennis Moccia, Diana Gikunju,
Sana Bazzaz, Betty Chan, Matthew A. Clark, John W. Cuozzo, Marie-Aude Guié, John P. Guilinger,
Christelle Huguet, Christopher D. Hupp, Anthony D. Keefe, Christopher J. Mulhern, Ying Zhang, and
Patrick Riley. Machine learning on dna-encoded libraries: A new paradigm for hit finding. Journal of
Medicinal Chemistry, 63(16):8857–8866, 2020.
[18] Asher Mullard. DNA tags help the hunt for drugs. Nature, 530:367–369, 2016.
[19] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset
Shift in Machine Learning. The MIT Press, 2009.
[20] Edward Reingold and Robert Tarjan. On a greedy heuristic for complete matching. In Siam Journal of
Computing, pages 676–681, 1981.
[21] Paul Rosenbaum and Donald Rubin. The central role of the propensity score in observational studies for
causal effects. In Biometrika, 1983.
[22] Alexander Satz, Andreas Brunschweiger, Mark Flanagan, Andreas Gloger, Nils Hansen, Letian Kuai,
Verena Kunig, Xiaojie Lu, Daniel Madsen, Lisa Marcaurelle, Carol Mulrooney, Gary O’Donovan, Sylvia
Sakata, and Jorg Scheuermann. Dna-encoded chemical libraries. Nature Review Methods Primers, 2(3),
2022.
[23] R. Sharathkumar and Pankaj K. Agarwal. Algorithms for the transportation problem in geometric
settings. In Proceedings of the 2012 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),
pages 306–317, 2012.
[24] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood
function. Journal of Statistical Planning and Inference, 90(2):227–244, 2000.
[25] Pravin Vaidya. Geometry helps in matching. Siam J. of Computing, 18(6):1201–1225, 1989.
[26] Zhenpeng Zhou, Steven Kearnes, Li Li, Richard N. Zare, and Patrick Riley. Optimization of molecules
via deep reinforcement learning. Scientific Reports, 9(10752), 2019.
12
7
Supplement
7.1
Further details about the greedy bipartite matching algorithm
For the sake of completeness, we outline below a way to implement the greedy matching algorithm.
Algorithm 3 GreedyMatch (Greedy metric bipartite matching)
1: Input: Two multi-sets of n points R, B in Qd.
2: Output: A matching from R to B.
3: ▷ The set B is shared across all threads
4: procedure WeightedMatch(R, B)
5:
for r ∈ R do
▷ All for loop statements run in parallel
6:
b ← BreadthFirstSearch(r, B)
7:
M ← M ∪ {r → b}
8:
return M
▷ M is the matching
9: procedure BreadthFirstSearch(r, B)
10:
for i = 1, ..., d do
11:
for v ∈ Qd, ∥v − r∥1 = i do
12:
if v ∈ B then
13:
B ← B \ v
14:
return v
▷ r matches to v
7.2
Proof of Theorem 5.4
Proof. Let LT , LS be the infinitesimal generators, and let ρT (w), ρS(w) be the invariant measures, corre-
sponding to the SGD for T , S respectively. Then, from our ergodicity assumption about the SGD, and the
definition of invariant measures, we have:
L∗
T ρT (w)
=
0,
(8)
L∗
SρS(w)
=
0.
(9)
We know that LS is a perturbation of LT . So, let
L∗
SρT (w) = ε(w).
(10)
Therefore,
L∗
SρS(w) − L∗
SρT (w) = ε(w),
(11)
and
L∗
T ρT (w) − L∗
SρT (w) = ε(w).
(12)
Putting Equations 11 and 12 together, we have:
L∗
S(ρS(w) − ρT (w))
=
(L∗
T − L∗
S)ρT (w)
(ρS(w) − ρT (w))
=
(L∗
S)−1(L∗
T − L∗
S)ρT (w),
(13)
where we have used the uniform ellipticity assumption in the last step to ensure the inverse exists. Taking
1-norm on both sides and using the sub-additivity of operator norms, we have:
∥ρS(w) − ρT (w)∥1 ≤ ∥(L∗
S)−1∥1∥(L∗
T − L∗
S)∥1.
(14)
We will upper-bound ∥(L∗
T − L∗
S)∥1 in terms of W1(PT , PS). The essential idea is to simply write down
the adjoints of the elliptic operators, group like terms together and use Kantorovich-Rubenstein duality to
upper-bound each of the resulting terms in terms of W1(PT , PS).
13
Recall that,
LT ψ
≡
∇wℓw(PT ) ∂ψ
∂wi
+ ϵ2D
∂ψ
∂wi∂wj
,
(15)
L∗
T ψ
≡
∂∇wℓw(PT )
∂wi
ψ + ∇wℓw(PT ) ∂ψ
∂wi
− ϵ2D
∂ψ
∂wi∂wj
,
(16)
where we have used the Einstein summation notation on the partial derivatives for the sake of brevity in
expressing the last two equations. Similarly, we can write out LS and L∗
S.
Note that:
(L∗
T − L∗
S)ψ ≡
∂∇wℓw(PT )
∂wi
− ∂∇wℓw(PT )
∂wi

ψ + (∇wℓw(PT ) − ∇wℓw(PS)) ∂ψ
∂wi
.
(17)
One can choose ψ as any Lipschitz function of unit ℓ1 norm, so that if we upper bound the coefficients of
the two partial terms on the RHS of Equation 17 for every co-ordinate i by W1(PT , PS), then we will have
bounded ∥L∗
T − L∗
S∥1 by W1(PT , PS). The first term can be upper-bounded as:
∇wℓw(PT ) − ∇wℓw(PS)
=
Ex∼PSEy∼PS(·|x)[(y − f(w, x))2] − Ex∼PT Ey∼PT (·|x)[(y − f(w, x))2]
≃
Ex∼PSEy∼PS(·|x)[(y − f(w, x))2] − Ex∼PT Ey∼PS(·|x)[(y − f(w, x))2]
≤
O(W1(PT , PS)),
(18)
where we have used:
1. The covariate shift type assumption in deriving the second equality (Equation 3).
2. The Kantorovich-Rubenstein duality together with the assumption that Ey∼P(·|x)[(y − f(w, x))2] is
O(1)-Lipschitz in deriving the last inequality.
Similarly, one can show the same upper-bound for

∂∇wℓw(PT )
∂wi
− ∂∇wℓw(PT )
∂wi

. Therefore, ∥L∗
T − L∗
S∥1 ≤
O(1)W1(PT , PS).
Corollary 7.0.1. The anisotropic diffusivity case: The upper-bound holds when the diffusivity is anisotropic
as well, with the caveat that W1(PS, PT ) be replaced by W1(PS, PT )2.
Proof. The proof of Theorem 5.4 uses isotropic diffusivity in one place only – when computing the difference
L∗
T − L∗
S. Note that, the diffusivity may be written as (see for example [6]):
D(P) := E

∇ℓw(P) · ∇ℓw(P)T 
− E [∇ℓw(P)] · E

∇ℓw(P)T 
.
(19)
Then, D(PS) − D(PT ) can be upper-bounded in terms of O(W1(PS, PT )) + O(W1(PS, PT )2) under the
assumption that we have O(1)-Lipschitz gradients. The argument is similar to that used for the drift term in
the isotropic case, albeit with one new observation, the term
EPS×PS [f(w)] − EPT ×PT [f(w)]
can be upper-bounded by W1(PS, PT )2 using Kantorovich-Rubenstein duality and the definition of Wasserstein
distance.
7.3
Proof of Lemma 5.12
Proof. One way to write the greedy matching algorithm is to imagine it as a set of parallel breadth first
searches (BFS), as in Algorithm 3. For any two vertices x, y ∈ R in an alternating cycle γ, suppose their
neighbors from the greedy matching algorithm are x′ and y′ respectively. Think of the last step in Algorithm 3
14
before either x or y was matched, so at that time-point the BFS from x′ and y′ hadn’t reached either x or y.
Therefore, we have the following relationship between their mutual distances:
min{d(x′, y), d(y′, x)} ≥ min{d(x, x′), d(y, y′)},
(20)
where d denotes the distance metric, which in our case is the underlying cost in W1 i.e, the ℓ1 distance. The
situation is illustrated in Figure 3.
Figure 3: Alternate edges in an alternating cycle γ belong to greedy and optimal matching.
Now suppose that the weight of the greedy matching edges in γ is α times the weight of the minimum
weight matching. Then we show below that a significant fraction of the greedy edges in the cycle γ must be
at a distance at least α/2 from their neighbors.
Let G be the set of greedy edges in γ and M be the set of optimal matching edges. Then we have,
X
xy∈G
d(x, y) ≥ α
X
xy∈M
d(x, y).
(21)
Let f be the fraction of edges in G with weight at least α/2. Let’s call that set Gα/2. Recall that, in the
setting of Qd, dmin = 1 and dmax = d. Therefore, we have
d · f + α
2 · (1 − f) ≥ α.
(22)
Therefore, f ≥
α
2d−α.
By the definition of metric entropy and Equation 20, we know that
|γ|f ≤ N ent
α/2(Gα/2) ≤ N ent
α/2(γ).
(23)
By Theorem 5.8 we know that α ≤ |γ|log2 3/2. Putting that together with Equation 23 gives:
α ≤

N ent
α/2(γ) · 2d − α
α
log2 3/2
.
(24)
7.4
Proof of Theorem 5.13
Proof. We have two cases:
1. α ≤ 2ζ: In this case, there’s nothing to prove.
15
2. α ≥ 2ζ: In this case, since N cov
α/2(γ) ≥ N cov
ζ
(γ) = η, we have
α ≤

η · 2d − α
α
log2 3/2
.
(25)
Therefore, we have two sub-cases:
(a) α = Ω(d): In this case, we obtain from Equation 25 that α = O(ηlog2 3/2), which is o(d) for
η = O(d
1
ξ log2 3/2 ) – a contradiction for ξ > 1. Hence α = o(d).
(b) α = o(d): In this case we obtain:
α1+log2 3/2
≤
(η · 2d)log2 3/2
α
≤
O

d
1+ξ log2(3/2)
ξ(1+log2(3/2))

,
(26)
where we have used η = O(d
1
ξ log2 3/2 ) in the last inequality.
7.5
Proof of Theorem 5.15
Proof. Recall that, we have an (η, ζ) = (logc1 n, logc2 n) instance, for some small constants c1 and c2. So the
covering number of the support set for µ, denoted Sµ, with balls of radius δ (δ ∈ [1, ζ)), denoted m(Sµ, δ), is
upper bounded by η ·
Vol(ζ,Qd(n))
Vol(δ,Qd(n)) . Replacing the asymptotic value for the volume, we get
η · Vol(ζ, Qd(n))
Vol(δ, Qd(n)) ≲ η · 2−d(n)(H(ζ/d(n))−H(δ/d(n))),
(27)
where H(x) := x log2 x + (1 − x) log2(1 − x) is the entropy function and is negative for x ∈ (0, 1).
Since Sµ of µ is finite, the set of set of probability measures M1 that are supported on Sµ is compact
in the 1-Wasserstein metric topology. Therefore, there exists a finite covering of M1, i.e., using elementary
measures that are constant on the atoms of a finite covering of Sµ, we can approximate any given probability
measure in M1 up to an additive constant ε + δ, in the 1-Wasserstein metric. The value of the constant for
each ball in the covering ranging in [0, 1] in steps of ε. We will fix the values of ε ∈ (0, 1) and δ ∈ [1, ζ) later
in the proof.
Therefore, as in exercise 6.2.19 in [9], we can bound the covering number of M1, i.e., m(M1, δ, ε) by
m (M1, δ, ε) ≤
m(Sµ, δ)(1 + 1
ε)
m(Sµ, δ)

≤
4
ε
m(Sµ,δ)
.
(28)
Therefore, we have by the standard covering argument for the proof of multidimensional version of
Cramer’s large deviation bound (equivalently Sanov’s theorem for finite spaces, see exercise 6.2.19 in [9]):
∃m0 ∀m > m0, P(ˆµm ∈ A) ≤ m (M1, δ, ε) · e−m·infν∈Aε+δ H(ν,µ),
(29)
where H(ν, µ) is the relative entropy (KL divergence), and Aδ is the δ blow-up of A ⊂ M1 with respect to
the 1-Wasserstein metric.
Note that infν∈Aε+δ H(ν, µ) can be lower bounded in terms of the 1-Wasserstein distance using the
following transportation inequality from [5].
Theorem 7.1. [5] For distribution µ, ν supported on any polish space, we have:
H(µ, ν)S(µ) ≥ W1(µ, ν).
(30)
16
Essentially,
inf
ν∈Aε+δ H(ν, µ) ≥
inf
ν∈Aε+δ
W(ν, µ)
S(µ)
≥ W(ν, µ) − δ − ε
S(µ)
.
(31)
For ε ≪ 1, we have δ + ε ≃ δ. Therefore, the exponent on the RHS of Equation 29 can be lower bounded as
inf
ν∈Aε+δ H(ν, µ) ≥ W(ν, µ) − δ
S(µ)
.
(32)
Furthermore, for m = α(n)|Q| = α(n)
","nanOur algorithm is related to prior work on distribution shift correction and efficient Wasserstein distance computation. However, those methods either rely on density estimation or require solving an expensive linear program. In contrast, our algorithm efficiently approximates the Wasserstein distance and scales well to large datasets."
"Urban mobility efficiency is crucial in big cities, and taxis are key elements in daily traffic activity. ICT and geo-positioning systems have led to new opportunities for improving taxi fleet efficiency. This research proposes a new assignment heuristic algorithm for taxis, considering reassigning taxis if it leads to better global solutions, and an economic compensation scheme for drivers to accept proposed modifications. The results show that the proposal can reduce customer waiting times in fleets of autonomous taxis, while being economically beneficial.","Urban mobility is a major concern for public managers in big cities due to high traffic congestions and CO2 emissions, and one of the main actors involved in daily traffic activity is taxi fleets. New mobility systems that benefit from ICT advances, such as Uber, Lyft or Liftago, have emerged to reduce empty trips and improve traffic flow, pollution and time. This article deals with the problem of assigning taxis to customers dynamically to minimize global waiting time of passengers. The proposed heuristic algorithm considers taxi reassignments if this may lead to globally better solutions, and an economic compensation scheme to make individually rational drivers agree with proposed changes in their assigned clients.","This research implements an algorithm for taxi assignment, exploring dynamic taxi reassignment coupled with an economic compensation schema to assure that rational taxi drivers will freely accept reassignments of “worse” customers if this can improve the overall performance of the system. Three instances of the proposed algorithm are evaluated, minimizing or maximizing different parameters (distance, revenues, and a combination of both).","The research results indicate that the proposal can reduce customer waiting times in fleets of autonomous taxis, while also being economically beneficial. The compensation methods outperform the baseline method NTNR (nearest-taxi/nearest-request) in both customer distributions. The improvements are rather small for low demand scenarios and are very considerably closed to the saturation point (between 2000 and 3000 customers per hour). Above this point, the improvements remain rather stable. Out of the three compensation methods, the approach of maximizing the outcome of the mediator performs better that minimizing the distances to the customers, and the combination of both methods obtains slightly better results in general in comparison to maximizing mediator revenue.","The main contribution of this paper is twofold. Firstly, it introduces an algorithm for taxi dispatching, exploiting dynamic taxi reassignment and coupled with an economic compensation schema which assures that (rational) taxi drivers will freely accept reassignments of “worse” customers if this can improve the overall performance of the system. Secondly, it performs an evaluation of three instances of the proposed algorithm, minimizing or maximizing different parameters (distance, revenues, and a combination of both). The results indicate that the proposal has the potential to reduce customer waiting times in fleets of autonomous taxis, while being also beneficial from an economic point of view.",Taxi dispatching strategies with compensations,"Holger Billhardt, Alberto Fernández, Sascha Ossowski, Javier Palanca, Javier Bajo"," 
The Version of Record of this manuscript has been published and is available in  
Expert Systems with Applications, Volume 122, pages 173–182, (2019) 
https://doi.org/10.1016/j.eswa.2019.01.001 
 
 
Taxi Dispatching Strategies with Compensations 
 
Holger Billhardta, Alberto Fernándeza, Sascha Ossowskia, Javier Palancab, and Javier Bajoc 
 
a Centre for Intelligent Information Technologies (CETINIA), Universidad Rey Juan 
Carlos, Móstoles 28933, Madrid, Spain (e-mail: holger.billhardt@urjc.es, 
alberto.fernandez@urjc.es, sascha.ossowski@urjc.es). 
b GTI-IA Research Group, Universitat Politècnica de Valencia, 40622 Valencia, Spain 
(e-mail: jpalanca@dsic.upv.es). 
c Department of Artificial Intelligence, Universidad Politécnica de Madrid, Spain (e-
mail: jbajo@fi.upm.es). 
 
 
Abstract 
Urban mobility efficiency is of utmost importance in big cities. Taxi vehicles are key elements in daily 
traffic activity. The advance of ICT and geo-positioning systems has given rise to new opportunities for 
improving the efficiency of taxi fleets in terms of waiting times of passengers, cost and time for drivers, 
traffic density, CO2 emissions, etc., by using more informed, intelligent dispatching. Still, the explicit 
spatial and temporal components, as well as the scale and, in particular, the dynamicity of the problem of 
pairing passengers and taxis in big towns, render traditional approaches for solving standard assignment 
problem useless for this purpose, and call for intelligent approximation strategies based on domain-specific 
heuristics. Furthermore, taxi drivers are often autonomous actors and may not agree to participate in 
assignments that, though globally efficient, may not be sufficently beneficial for them individually. 
This paper presents a new heuristic algorithm for taxi assignment to customers that considers taxi 
reassignments if this may lead to globally better solutions. In addition, as such new assignments may reduce 
the expected revenues of individual drivers, we propose an economic compensation scheme to make 
individually rational drivers agree to proposed modifications in their assigned clients. We carried out a set 
of experiments, where several commonly used assignment strategies are compared to three different 
instantiations of our heuristic algorithm. The results indicate that our proposal has the potential to reduce 
customer waiting times in fleets of autonomous taxis, while being also beneficial from an economic point 
of view. 
 
Keywords: Coordination, dynamic fleet management, dynamic optimization, multi-agent systems, open systems, taxi 
assignment.  
1. Introduction 
Urban mobility is one of the main concerns that public managers face in big cities nowadays. Traffic 
congestions generate a high quantity of CO2 emissions and cause extra time spent by travelers. One of the 
main actors involved in the daily traffic activity in urban areas are taxi fleets. They consist of several 
thousands of vehicles in big cities (e.g. about 15,000 taxis in Madrid, Spain). They are usually affiliated to 
mediator services, which coordinate service calls and taxi dispatching. Lately, new mobility systems that 
benefit from the advances in information and communication technologies have emerged, such as Uber1, 
Lyft2 or Liftago3 among others. 
Two of the main goals of a taxi fleet are (i) to reduce the response time (e.g., the time between a customer 
call and the moment a taxi arrives at the customer’s location) and (ii) reduce costs of empty movements 
(e.g., movements taxis have to make in order to get to the location of customers). The provision of efficient 
methods for taxi assignment to customers is a challenge that can contribute to reducing distances of empty 
trips with the resulting decrease of traffic flow, pollution, time and so on. Typically, taxi fleet coordination 
companies apply the first-come first-serve strategy to assign taxis to customers. Once the taxi accepts the 
passenger, the dispatching is irreversible. This method is known to be inefficient (Egbelu & Tanchoco, 
1984). 
 
1 http://www.uber.com 
2 http://www.lyft.com 
3 http://www.liftago.com 
 
2 
The aforementioned case falls into a specific class of assignment problems which is characterized by a 
dynamic demand in time and space. To efficiently solve such problems, dynamic algorithms are required 
instead of classical assignment optimization methods. For this purpose, techniques from the field of 
intelligent systems are promising, because they allow for developing heuristics-based algorithms that 
intelligently prune the search space, so as to reduce the computational complexity and to support a sufficient 
degree of scalability. Furthermore, taxi drivers are usually autonomous actors, i.e. they can freely choose 
whether to accept or to reject a recommendation proposed by the mediator service, which puts additional 
constraints on the set of feasible solutions to the assignment problem. As Ossowski and Omicini (2002) 
argue, dynamic coordination problems with self-interested actors can be effectively modelled as multiagent 
systems. Agreement Technologies (Ossowski et al. 2013) refer to a sandbox of methods within the field of 
multiagent systems enabling knowledge-based software agents to interact with each other so as to forge 
agreements on behalf of their users. Multiagent interaction protocols based on the algorithm first proposed 
by Bertsekas (1984), for instance, coordinate software agents by iteratively simulating auctions among 
them, and have been successfully applied to a dynamic assignment problem in the domain of emergency 
management (Billhardt et al., 2018). 
In this article, we deal with the problem of dynamic taxi assignment to customers with the goal of 
minimizing the global waiting time of passengers. Our heuristic assignment algorithm considers taxi 
reassignments if this may lead to globally better solutions. That is, taxis that have been dispatched to pick 
up a customer but are still on their way may be reassigned to another customer. For this purpose, we adapt 
the method put forward by Billhardt et al. (2014) for ambulance management to the taxi assignment 
problem. In addition, we go beyond that approach by taking into account the taxi drivers’ autonomy: in 
case of an assignment change that improves the efficiency of the taxi fleet at global level but may be 
disadvantageous for some individual taxi driver (e.g., she may be assigned a customer located further away 
compared to the initially assigned one), that driver will receive a compensation to make the assignment 
agreeable to her as well. To the best of our knowledge, there are no other approaches that consider 
reassignment until pick-up time and taxi autonomy to propose a new scheduling when new customers show 
up or taxis become available.   
The main contribution of this paper is twofold. Firstly, we introduce an algorithm for taxi dispatching, 
which exploits dynamic taxi reassignment and is coupled with an economic compensation schema which 
assures that (rational) taxi drivers will freely accept reassignments of “worse” customers if this can improve 
the overall performance of the system. Secondly, we perform an evaluation of three instances of the 
proposed algorithm, minimizing or maximizing different parameters (distance, revenues, and a combination 
of both). 
The rest of the paper is organized as follows. Section 2 analyzes existing works related to taxi assignment. 
In section 3, we describe the problem we are dealing with and some common dispatching strategies. In 
section 4 the proposed taxi reassignment algorithm and compensation schema is described. Section 5 details 
the experiments carried out to evaluate our approach. Finally, we conclude the paper with section 6. 
2. Related Work 
The development of ICT, especially GPS and wireless connectivity, has driven the proposal of many taxi 
assignment systems during the last decade. 
Many works are centered analyzing new assignment strategies in order to reduce the waiting times of 
customers. The classical approach is the first-come/first-served (FCFS) strategy, where each new customer 
is assigned to the nearest available taxi. Lee, Wang, Cheu, and Teo (2004) present a system that takes 
advantage of real-time information (on taxis and traffic conditions) to assign taxis with the shortest time 
path to customers, instead on the closest taxis. Maciejewski, Salanova, Bischoff, and Estrada (2016) 
compare the classical FCFS strategy with a demand-supply balancing strategy that assigns taxis to the 
closest customers in high demand scenarios (instead of customers to taxis) in microscopic simulations of 
taxi services in Berlin and Barcelona. In (Maciejewski, Bischoff, & Nagel, 2016), they present a real-time 
dispatching strategy based on solving the taxi assignment problem. In this approach, the assignment 
problem is considered from a more global perspective. They propose to calculate the optimal assignment 
among idle taxis and pending requests at certain intervals or whenever new events (new customer/available 
taxi) take place. Zhu and Prabhakar (2017) analyze how suboptimal individual decisions lead to global 
inefficiencies and propose an assignment model based on network flow. 
While most existing approaches try to minimize the average waiting time of customers, other works have 
a different focus (Dai, Huang, Wambura, & Sun, 2017; Gao, Xiao, & Zhao, 2016; Meghjani, & Marczuk, 
2016; and Ngo, Seow, & Wong, 2004). BAMOTR (Dai et al., 2017) provides a mechanism for fair 
assignment of drivers, where fair assignment is intended to minimize the differences in income among the 
taxi drivers. For that, they minimize a combination of taxi income and extra waiting time. Gao et al. (2016) 
 
3 
propose an optimal multi-taxi dispatching method with a utility function that combines the total net profits 
of taxis and waiting time of passengers. They also consider different classes of taxis. Meghjani and Marczuk 
(2016) propose a hybrid path search for fast, efficient and reliable assignment to minimize the total travel 
cost with a limited knowledge of the network. In (Ngo et al., 2004), a fuzzy approach is proposed for 
defining the cost function to be minimized, which encompass a fuzzy aggregation of multiple vague criteria 
defined by human experts. 
There are other works that focus on taxi demand prediction with the goal of helping taxis to quickly find 
closer passengers or of balancing supply and demand of taxis in an area of interest. Grajciar (2015) proposes 
a method for recommending areas where idle taxis are more likely to find a new customer. The price of 
each journey is not fixed and is proposed to the customer by a broker. Then, taxis bid for the customer. 
Moreira-Matias et al. propose methods for predicting taxi-passenger demand (Moreira-Matias, Gama, 
Ferreira, Mendes-Moreira, & Damas, 2013) and profitability (Moreira-Matias, Mendes-Moreira, Ferreira, 
Gama, & Damas, 2014) at taxi stands. Zhang et al. (2015) model the taxi driver’s service strategies from 
three perspectives: passenger-searching, passenger-delivery, and service-area preference. Miao et al. (2016) 
and Miao et al. (2017) treat the problem of dispatching vacant taxis towards current and future demands 
while minimizing total idle mileage. Their approach is based on forecasting the uncertain spatial-temporal 
taxi demands in a region. 
There are also an increasing number of works on taxi ridesharing (e.g., Ma, Zheng, & Wolfson, 2013; 
d’Orey, Fernandes, & Ferreira, 2010; Li, Horng, Chen, & Cheng, 2016; Tian, Huang, Liu, Bastani, & Jin, 
2013; and Mareček, Shorten, & Yu, 2016), although in this paper we do not focus on that problem. 
In our work we concentrate on the problem of dispatching (assigning) taxis to (current) customers. In 
contrast to other works in this field, the main characteristic of our approach is that we treat the problem 
from a global and dynamic perspective. In particular, we try to find assignments from taxis to pending 
customers that globally minimize the expected waiting times of customers. Furthermore, we consider the 
possibility of modifying an existing assignment when a taxi has been dispatched but has not yet picked up 
the corresponding customer. We already successfully followed a similar approach in our previous work on 
taxi assignments (Billhardt et al., 2017), and in (Billhardt et al., 2014) to assign ambulances to patients in 
emergency medical services. One of the few other works in this line is (Glaschenko, Ivaschenko, Rzevski, 
& Skobelev, 2009), which presented an adaptive scheduling in which reassignment is possible during a 
time interval until pick-up order is sent to the taxi and customer. During this process, vehicle agents 
negotiate with each other. In our case, we do not restrict reassignment to a specific interval. Furthermore, 
since modifications of existing assignments may imply changes in the expected incomes of a taxis, we 
propose a method that economically compensates taxis such that modifications in their current assignments 
will not result in a loss of income.  
3. Problem Definition and standard Dispatching strategies 
In this section we describe in more detail the problem we are tackling in this article. Table 1  contains a 
list of symbols used in the rest of the paper. 
 
Table 1. List of symbols used in equations. 
Symbol 
Meaning 
T 
Set of taxis and iÎT denotes a taxi 
TD 
Set of assigned (i.e. dispatched) taxis 
TO 
Set of occupied taxis 
TA 
Set of available taxis 
C 
Set of customers and kÎC denotes a customer 
CA 
Set of customers assigned to a taxi 
CU 
Set of customers waiting to be assigned to a taxi 
CS 
Set of customers in service (in a taxi) 
Dk 
Destination of customer k 
dik 
Distance from taxi i to the location of customer k 
tik 
Time it takes taxi i to reach customer k 
dkd 
Distance from pick up location of customer k to his destination d 
dikd 
Total distance to serve customer k with taxi i (dik + dkd)  
 
 
 
 
4 
We consider taxi systems in which there exist some mediator service in charge of coordinating the 
assignment of customers to taxis.  Customers contact the mediator via phone calls or any other telematic 
means available nowadays. The mediator dispatches taxis to serve customers. We do not explicitly deal 
with group of customers. If several people travel together they are considered as one customer (e.g. the one 
that made the contact). 
Customer k, requests a taxi at time t. The mediator assigns taxi i to serve customer k. Then, taxi i moves 
to location of k, with driving distance dik, which takes tik time to reach k. After picking up k the taxi drives 
to the customer’s destination Dk, located at distance dkd. We denote by dikd = dik + dkd the total distance for 
serving customer k with taxi i. 
Let T be the set of all taxis. Each taxi iÎT can be either available (neither assigned nor occupied), 
assigned (dispatched, on the way to pick up a customer) or occupied (carrying customers), denoted by TA, 
TD and TO, respectively, and such that: 
T = TA È TD È TO  and TA Ç TD Ç TO º Æ 
Let C be the set of customers in the system at a given time. Each customer kÎC can be either in service 
(inside a taxi), assigned to a taxi (a taxi has been dispatched to pick up the customer) or unassigned (waiting 
to be assigned to a taxi), denoted by CS, CA and CU, respectively, and such that: 
C = CS È CA È CU and CS È CA È CU º Æ 
The taxi assignment problem consists in dispatching customers, that is, assigning customers in CU to 
available taxis in TA. The general goal of a dispatching mechanism is to optimize the existing resources and 
to reduce the waiting time of customers, that is, the time or distance it takes taxis to reach their customers 
(independently of their destination). In this paper, we assume time is proportional to distance, so we 
concentrate on reducing distances.  
 
Definition (Taxi Assignment). Given a set of taxis T and a set of customers C, a taxi assignment A is a 
set of pairs <i,k> where iÎT and kÎC, and typically such that all customers are assigned to a taxi (if the 
number of customers is lower than the number of taxis) or all taxis are assigned to a customer (in the other 
case). In addition, a customer cannot be assigned to more than one taxi and a taxi cannot be assigned to 
more than one customer. 
 
The most common dispatching strategy is first-come/first-served (FCFS), in which any customer in CU 
who is waiting the longest is assigned to the nearest available taxi from TA. The assignment process is 
repeated whenever a new customer requests a taxi or whenever a taxi becomes available after a previous 
trip. 
The FCFS strategy always dispatches to each unassigned customer the nearest taxi. However, in very 
high demand scenarios, e.g., when the number of taxis is lower than the number of customers (|TA| < |CU|), 
this strategy turns out to perform quite badly (Maciejewski et al., 2016). An alternative that solves this 
problem is the nearest-taxi/nearest-request (NTNR) strategy. In NTNR customers are assigned in the same 
way as in FCFS if the number of unassigned customers (|CU|) is lower or equal to the number of available 
taxis (|TA|). However, if |TA|<|CU|, the assignment is processed on the side of the taxis, assigning each 
available taxi to the closest customer in CU. The rationale behind this approach is that in high demand 
scenarios, the overall waiting times of customers can be reduced if the closest customers are served first.  
Dispatching strategies like FCFS and NTNR, do not optimize the assignments globally and do not take 
into account possible improvements of a global assignment at a given point in time that might exist because 
of the dynamic nature of taxi services. These methods do not allow for a reassignment of already dispatched 
taxis which could in certain occasions achieve better results. This can be seen in the example shown in 
Figure 1. Here, a scenario with two taxis is presented (t1 and t2). The first customer in appearing is c1, and 
t1 is dispatched to pick it up since it is located closer (1.8 km) than t2 (2 km). A few minutes later a new 
customer c2 appears. At this moment FCFS and NTNR would assign t2 to c2. The overall travel distance 
towards the customers (at this moment) would be 4km. However, there is a better global assignment 
A’={<t1,c2>,<t2,c1>} with a total of 3.5km.  
 
 
5 
 
Figure 1. Example of scenario where a new customer appears. Numbers indicate distance in km. The solid line indicates 
the distance driven by T1 when C2 appears. 
 
 
A way to reduce overall travel distances of taxis at a given point of time, consists in allowing 
reassignment of customers to taxis and finding the globally best assignment of all available or already 
dispatched taxis (TAÈTD) to all unassigned or assigned customers (CAÈCU). This can be done by solving 
the assignment problem (Bertsekas, 1988). In our previous work (Billhardt et al., 2017), we applied this 
idea using Bertsekas’ auction algorithm to find optimal assignments. The optimization process is applied 
whenever the situation changes and a new optimal solution may exist, that is, whenever a new customer 
appears or a previously occupied taxi becomes available again (or starts working).  
In this way it is assured that the assignment of taxis to customers is optimal at each moment (with respect 
to the global distances of taxis to customer’ locations). We call this dispatching strategy Full auction4 (FA) 
(for details the interested reader is referred to (Billhardt et al., 2017)). 
4. Taxi Reassignments with Compensation  
The reassignment strategy FA will be appropriate in scenarios of taxi fleets operated by taxi companies 
and where the taxi drivers have a fixed salary that does not depend on the trips they are doing. However, 
the strategy will not work for fleets of autonomous taxi drivers for which their revenues depend on the 
customers they serve during the day. This is basically due to the fact that autonomous taxi drivers will not 
accept a reassignment of customers if this would imply a reduction in their income.   
The idea of the proposal we present in this section is to define a dispatching strategy that involves the 
reassignment of taxis, but would always be accepted by autonomous taxi drivers (who rationally decide 
based on maximizing their profit).  
4.1. Taxi revenues and the effect of a taxi reassignment 
 To analyze the revenue of a taxi for a trip we assume the following payment and cost scheme. Customers 
pay a fixed cost fcost per trip plus a fare per kilometer for the distance from their pickup location to their 
destination. Furthermore, a taxi has a cost per kilometer for car usage (including petrol and other expenses, 
e.g. maintenance, taxes, etc.). With this structure, the monetary revenue of a taxi i for serving customer k 
is: 
Revenue(i,k) = fcost + fare × dkd – cost × dikd, 
 
where dikd and dkd are the total distance and the distance from k to its destination Dk, respectively. 
When a taxi i, previously assigned to a customer k in assignment Ao is reassigned to customer j in a new 
assignment Anew, as presented in Figure 2, its revenue would change by: 
 
D Revenue(i,k,j) = Revenue(i,j) – Revenue(i,k) = fare×(djd – dkd) + cost×(dikd – dijd) 
 
The economic effect of the new assignment on i (reassigned from <i,k>ÎAo to <i,j>ÎAnew) can be zero, 
positive or negative.  
 
 
4 The algorithm is based on an auction process 
T1 
C1 
T2 
C2 
0.8 
1 
2 
3 
1.5 
Loca/on T1 when  
C2 appears 
 
6 
 
Figure 2. Example of reassignment of taxi i from customer k in Ao to j in Anew. 
 
4.2. Reassignment compensations 
We assume taxi drivers to be economically rational, that is, they want to maximize their income and 
minimize the time spent on their trips. In particular, we make the following assumptions:  
a) A taxi driver that is available will always accept a new customer.  
b) A taxi driver would always prefer earning the same amount of money in less time. 
c) A taxi driver would always accept any extra movement (with a distance d) if this implies an extra 
income of d × (fare – cost). In fact, this is actually the current rate for which a taxi driver is 
working. 
Based on these assumptions we define a compensation c that is applied if a taxi i accepts a reassignment 
from customer k to j. Taking into account this compensation, the effective revenue of taxi i when accepting 
customer j would be Revenue´(i,j)= Revenue(i,j)+c. We consider the following two situations:  
a) If dikd < dijd:  
c= Revenue(i,k) – Revenue(i,j)+( dijd – dikd)×(fare – cost) and thus 
Revenue´(i,j)= Revenue(i,k)+ (dijd – dikd)×(fare – cost) 
Here, the taxi would be compensated for the extra distance with the standard fare.  
b) If dikd ≥ dijd:  
c= Revenue(i,k) – Revenue(i,j) and thus  
Revenue´(i,j)= Revenue(i,k) 
In this case, the taxi would earn the same as before, but for a service with the same or a shorter 
distance than the previous one.  
It is clear that, with the assumptions on economic rationality of taxi drivers, a taxi would accept any 
reassignment with the defined compensations. 
It should be noted that compensations may be positive or negative, i.e., a taxi may receive a payment 
from the mediator in addition to the fare that it collects from the client, but it may also have to pay to the 
mediator part of the fare that it charges to client. For instance, if the customers’ destinations are unknown, 
the distances d-d may need to be estimated by some constant which is the same for all customers. Then, in 
the above case (b), the compensation would be: 
c= Revenue(i,k) – Revenue(i,j)= cost × (dij – dik)  
Since in this case, dik> dij, the taxi would have to pay to the mediator the cost of the difference in distances 
towards the new customer wrt. the previous customer.  
Taxi compensations can be managed by the mediator entity, which is in charge of proposing new 
assignments as well as collecting and paying compensations to affected taxis. In this sense, a change from 
one global assignment Ao to a new assignment Anew implies revenues/cost for the mediator.  
 
Definition (Mediator Revenues): Given a current assignment Ao and a new assignment Anew, the 
economic effect on the mediator when applying Anew with compensations, denoted as R(Ao, Anew), is defined 
as the sum of payments received from taxis minus compensations given. 
 
4.3. Proposed algorithm 
Algorithm 1 shows our proposal for a dispatching strategy based on reassignments with compensations. 
Similar to the FA strategy, the algorithm is executed whenever at least a new customer appears or a taxi 
i 
k 
dik 
dkd 
Ao 
j 
Dk 
Dj 
dij 
djd 
Anew 
 
7 
becomes available.  
The algorithm is in several ways influenced by the fact that we want to assure the mediator revenues to 
be positive over time, that is, we want a mediator that has no extra cost or even obtains some profit. 
The first step (line 3) is to assign possible unassigned customers (CU) to available taxis (TA) using the 
NTNR strategy. The result (Ao) is a valid assignment that could be applied. However, we try to find a better 
assignment A’ (line 4) considering all pending (if any) and assigned customers (CAÈCU) and only assigned 
taxis (TD). This implies, that when finding assignment A’, we only consider the reassignment of customers 
among taxis that already have a customer assigned; in other words, no taxi would lose a customer. This 
step assures that the necessary compensation the mediator would have to pay to taxis are rather small. If 
we would allow de-assignments of customers (e.g., leaving previously assigned taxis unassigned in A’), the 
compensation costs for the mediator would be generally to high leading to the case that the mediator would 
imply extra cost.  
 
 
 
Algorithm 1. Taxi Assignment 
1: 
Input: current assignment Acurrent , accumulated mediatorRevenue 
2: 
Output: assignment Anew 
3: 
Ao = Acurrent È NTNR(TA, CU) 
4: 
A’ = Calculate optimal assignment from (CAÈCU) to TD 
5: 
for all <i,j>Î A’ \ Ao do 
6: 
    if dikd ≥ dijd | <i,k> Î Ao then 
7: 
        c = Revenue(i,k) – Revenue(i,j) 
8: 
    else 
9: 
        c= Revenue(i,k) – Revenue(i,j)+(dikd – dijd)×(fare – cost) 
10: 
    end if 
11: 
    mediatorRevenue – = c 
12: 
end for 
13: 
if mediatorRevenue ≥ 0 then 
14: 
    Anew = A’  
15: 
else 
16: 
    Anew = Ao 
17: 
end if 
18: 
return Anew 
 
Each modification of taxi i is analyzed in lines 5-12. We calculate the required compensation c for each 
taxi and add -c to the accumulated mediator revenue. The new assignment is only accepted (proposed to 
the taxis) if the mediator keeps a positive accumulated revenue (lines 13-18). 
We have implemented three different optimization functions (line 4): 
• 
Minimizing distances to customers (MinDist): The optimization consists in finding the assignment 
that maximizes the reduction in the sum of distances of all dispatched taxis towards customers, with 
respect to the current assignment. 
• 
Maximizing mediator revenue (MaxRev): The optimization consists in finding the assignment that 
maximizes the outcome of the mediator. The current assignment would have an outcome of 0 and, 
thus, acts as a lower limit.  
• 
Minimizing distances /Maximizing mediator revenue (MinDist/MaxRev):  a linear combination of 
the previous methods. Let A be the current assignment and let D(A) be the sum of the distances of 
taxis towards the customers in A. This assignment approach computes an assignment A’ that 
minimizes: D(A’) – g×R(A,A’), where g is a ratio of scaling monetary income into distance values 
(meters) (e.g. the net benefit a taxi receives per meter when transporting a passenger). Therefore, 
this method optimizes two characteristics: it minimizes the distance to the assigned customers and 
it maximizes the revenue of the mediator. 
5. Evaluation  
In order to evaluate the effectiveness of our proposal we tested it in different experiments simulating the 
operation of a taxi fleet in an area of about 9´9 km, an area that roughly corresponds to the city center of 
Madrid, Spain. In the simulations we randomly generate customers (with different distributions of origins 
and destination location), they are assigned to available taxis, and we simulate the movement of taxis to 
pick up a customer, to drive her to her destination and then waiting for the assignment of a new customer. 
The simulations are not aimed to be realistic in terms of reflecting the real world operation of a taxi fleet. 
 
8 
Instead, we want to analyze and compare the different assignment strategies proposed in this paper. Thus, 
we simplified the movements of taxis to straight-line movements with a constant velocity of 17 km/h. This 
velocity is within the range of the average velocity in the city center of Madrid. That is, we do not take into 
account neither the real road network, nor the possibility of different traffic conditions.  
The general parameters used in the simulation are as follows. We use 1000 taxis (initially distributed 
randomly in the area with a uniform distribution) and simulate their operation during 5 hours. The taxis do 
not cruise, that is they only move if they are assigned to a customer. With respect to the customers, we 
randomly generate a fixed number of customers per each 15 minute interval. In order to analyze different 
supply/demand ratios, we accomplish different simulation runs with different numbers of customers from 
250 to 1000 per 15 minutes, in steps of 125. During the simulation, customers appear at their generated 
positions and time. 
We analyze two different methods to generate the origin (point of appearance) and destination location 
of customers: 
• 
Uniform: both, destinations and origins are generated using a uniform probability distribution over 
the area of interest 
• 
Center: each trip goes either from the outside of the area to the center, or vice versa. The points 
themselves are generated using a normal distribution (either in the center or in the outside of the 
area) 
The second method corresponds more closely to the actual distributions of taxi trips in urban areas.   
We also tested a third distribution, where we defined two density areas that are about 5 km away from 
each other. Then, each trip is composed of one point (either origin or destination) in one of those areas and 
the other point is generated with a uniform distribution in the whole region. However, we omit the results 
for this distribution since they are not significantly different from the Center distribution.  
In the experiments, we compare the 3 different assignment strategies described in section 3 (without 
compensations) FCFS, NTNR and FA, with the three compensation methods MinDist, MaxRev and 
MinDist/MaxRev. In the latter cases, algorithm 1 is applied with the described compensation schema. In the 
MinDist/MaxRev approach we apply a scaling factor for monetary incomes of g= 1/0.00085, which 
corresponds to the net benefit a taxi receives per meter when transporting a customer in the used payment 
scheme. 
During the simulation, the assignment of taxis to (waiting) customers is accomplished every 5 seconds 
using the corresponding assignment strategy. Once a taxi is assigned to a customer, it moves towards the 
customers location. After a taxi has reached the location of the assigned customer, it picks up the customer 
and drives her to her destination. Then, the taxi waits at this point for a new assignment. We simulate fixed 
pick-up and drop-off times of customers of 30 and 90 seconds, respectively.  
 The payment scheme we used in the experiments is the one that has been used in the city of Madrid in 
the last years. A taxi trip has a fixed cost of 2.4 euros and each kilometer a customer moves with the taxi is 
paid by 1.05 euros. In addition, we assume a cost of operation of a taxi of 0.2 euros per kilometer. This 
includes petrol, vehicle maintenance cost, as well as other fixed costs. 
In the experiments we analyze the average waiting time of customers (the time between the appearance 
of a customer and of taxis) and revenue of the mediator service. 
Each experiment is repeated 10 times with a different random seed, in order to avoid biased results due 
to a particular distribution of clients. The presented results are averages over those 10 runs.  
5.1. Unknown customer destinations 
In the first set of experiments, we assume that during the assignment phase, the destination of the 
customers is not known. Thus, from the perspective of a taxi driver, the expected travel time for the ride 
can be assumed to be some average value for all customers. In our experiments the chosen value has been 
4750 meters, roughly the average of all generated taxi trips. This implies that, the only parameter that makes 
a driver prefer one customer over another is the distance to those customers. 
Table 2 and Table 3 show the average customer waiting times for the two different trip distributions 
(Uniform and Center), respectively. In each case we present the average waiting time of the NTNR method 
as a baseline result and the absolute and relative variation of waiting times with the other methods.    
 
 
 
 
 
 
 
 
9 
Table 2. Average Waiting times for customers for NTNR and variation over NTNR (Absolut in minutes / relative in 
%) with the Uniform distribution. 
Method 
#Customers per hour 
 
1000 
1500 
2000 
2500 
3000 
3500 
4000 
NTNR  
0.84 
1.02 
1.29 
2.19 
6.78 
22.98 
43.66 
FCFS 
0 /  
0 
0 /  
0 
0 /  
0 
0 /  
0 
77.03 / 
1136.14 
102.33 / 
445.3 
121.91 / 
279.23 
FA 
-0.01 / 
-1.19 
-0.03 /  
-2.94 
-0.09 /  
-6.98 
-0.56 /  
-25.57 
-1.48 /  
-21.83 
-0.97 / 
-4.22 
-0.84 /  
-1.92 
MinDist 
0 /  
0 
-0.01 /  
-0.98 
-0.03 /  
-2.33 
-0.21 /  
-9.59 
-1.05 /  
-15.49 
-0.82 /  
-3.57 
-0.62 / 
-1.42 
MaxRev 
0 /  
0 
-0.01 /  
-0.98 
-0.04 /  
-3.1 
-0.38 /  
-17.35 
-1.26 / 
-18.58 
-0.84 /  
-3.66 
-0.68 /  
-1.56 
MinDist/MaxRev 
0 /  
0 
-0.02 /  
-1.96 
-0.05 /  
-3.88 
-0.43 /  
-19.63 
-1.4 /  
-20.65 
-0.89 /  
-3.87 
-0.68 /  
-1.56 
 
Table 3. Average Waiting times for customers for NTNR and variation over NTNR (Absolut in minutes / relative in 
%) with the Center distribution. 
Method 
#Customers per hour 
 
1000 
1500 
2000 
2500 
3000 
3500 
4000 
NTNR  
1.2 
1.56 
1.92 
3.83 
8.29 
27.38 
49.01 
FCFS 
0 /  
0 
0 /  
0 
0 /  
0 
35.19 / 
918.8 
67 / 
808.2 
84.63 / 
309.09 
99 / 202 
FA 
-0.08 / 
-6.67 
-0.21 /  
-13.46 
-0.36 /  
-18.75 
-1.95 / 
-50.91 
-1.88 /  
-22.68 
-2.42 /  
-8.84 
-2.67 /  
-5.45 
MinDist 
-0.04 / 
-3.33 
-0.1 /  
-6.41 
-0.15 / 
-7.81 
-1.36 /  
-35.51 
-1.19 /  
-14.35 
-2.08 /  
-7.6 
-2.58 /  
-5.26 
MaxRev 
-0.04 / 
-3.33 
-0.16 /  
-10.26 
-0.29 / 
-15.1 
-1.81 /  
-47.26 
-1.75 /  
-21.11 
-2.45 /  
-8.95 
-2.74 /  
-5.59 
MinDist/MaxRev 
-0.05 / 
-4.17 
-0.17 /  
-10.9 
-0.31 /  
-16.15 
-1.85 /  
-48.3 
-1.86 /  
-22.44 
-2.48 /  
-9.06 
-2.8 /  
-5.71 
 
 
In both cases, it can be clearly observed that the standard FCFS approach performs considerably worse 
than the nearest taxi/nearest request method, when the demand increases and the system gets saturated. The 
saturation point, that is, the number of clients per hour from which the taxis are not able to serve all clients 
anymore, is between 2000 and 2500 customers per hour in the case of the FCFS method (depending on the 
spatial distribution of customers). Basically, in a saturation scenario, a good heuristic is to assign taxis in a 
way that the customers which are closest are served first. In this way, taxis can serve more customers in the 
same time. This is exactly the basis of the NTNR approach.  
The saturation point of all other methods is around 2500-3000 customers per hour. The full auction 
approach (FA) performs best in almost all cases. This is reasonable since the optimal assignments are 
calculated among all possible taxis and all possible customers. The obtained improvement with the FA 
method shows that reducing the overall travel times of all taxis to the nearest customers in each moment 
produces reductions in the global waiting times of customers. The method, however, is not always the 
optimal solution when we consider the dynamics of the system over time, e.g., the appearance of future 
customers. On the long run, it would be better to serve customers first that are within a short distance and 
also have a destination in an area with a high probability of appearance of new customers. However, in this 
paper we did not take into account such considerations.  
As argued before, the FA method is not applicable in the case of autonomous taxi drivers. In such a case 
the three different compensation approaches, MinDist, MaxRev, and MinDist/MaxRev can be employed. 
The three methods outperform the NTNR approach in both customer distributions. The improvements are 
rather small for low demand scenarios and are very considerably closed to the saturation point (between 
2000 and 3000 customers per hour). Above this point, the improvements remain rather stable.  Out of the 
three compensation methods, the approach of maximizing the outcome of the mediator performs better that 
minimizing the distances to the customers. This is basically due to the fact that some optimal solutions 
found with the MinDist approach imply high negative revenues for the mediator and, thus, they will not be 
applied. On the other hand, the MaxRev approach finds the solution with the highest (positive) outcome 
for the mediator. Since in our settings a positive outcome is only achievable if the sum of the distances of 
the taxis towards the assigned customers is reduced, a high positive outcome also implies a better solution 
in terms of reduced distances to customers. This means, in situations where the optimal solution in terms 
of distances would have a negative outcome for the mediator, MaxRev is able to find good solutions with 
 
10 
a positive outcome. In the experiments, the combination of both methods, MinDist/MaxRev, obtains 
slightly better results in general in comparison to MaxRev.  
With regard to the different distributions of customer origin and destination points, the methods show a 
different behavior with the Uniform distribution in comparison to the Center distribution. Basically, the 
improvements that can be obtained with smarter assignment solutions than NTNR are lower in the case of 
the uniform distribution. Our explanation of this fact is that in such a case, also the taxis will always be 
distributed in an almost uniform way in the region, since they move the customers to their destination 
locations (which are uniformly distributed). Thus, if both, taxis and customers are distributed in a uniform 
manner, also the distances between unassigned customers and available taxis will be more homogeneous 
at all times than in other distributions. This implies that there is not so much space for obtaining 
improvements when reassigning customers. In general, however, the results in the case of the uniform 
distribution are similar to the center distribution, even though the obtained improvements are smaller. The 
FCFS has a clearly worse performance, FA provides the best results and the three compensation approaches 
obtain improvements with respect to the NTNR approach.   
In Figure 3 we analyze the overall revenue of the system, composed of the revenues of the taxis plus the 
accumulated revenue of the mediator service. 
 
 
 
 
 
Figure 3. Overall revenue of the system in euros for the Uniform distribution and the Center distribution and for 
different numbers of customers per hour. The outcome is normalized to 1000 customers and is composed of the benefit 
of the (1000) taxis plus the benefit of the mediator service. 
 
 As it can be observed, the highest overall benefit of the system is obtained in all cases with 
MinDist/MaxRev. It is slightly higher than the other two compensation approaches. And they all 
outperform the baseline method NTNR. The FCFS approach performs considerably worse than the baseline 
for higher demand scenarios.  
The main difference among the three compensation approaches is the distribution of the benefit among 
taxis and the mediator service. Taxis have the highest benefit with the MinDist methods. On the other hand, 
the benefit of the mediator is highest with the MaxRev approach. For instance, in the case of the Center 
distribution, the mediator benefit for each 1000 customers is between about 2 euros with 1000 customers 
per hour and around 34 euros with 3000 customers per hour. The combination approach, MinDist/MaxRev 
provides an intermediate solution, ranging the mediator benefit between 1 and 20 euros in the case of the 
Center distribution. 
 
11 
An interesting issue is that the average benefit of taxis is higher than in the baseline method for very high 
and rather low demands. But it might be lower in the case of the MaxRev approach for demands closed to 
the saturation point. Still, the assignments and reassignments with this method are always done such that a 
taxi driver does never loose benefit when he is reassigned to another customer. In the case of the MinDist 
approach, the outcome for taxis is always higher than with the NTNR method. And in the combination 
method MinDist/MaxRev, taxi benefit is generally higher, except closed to the saturation point, where it is 
similar or slightly lower. However, it should be noted that, in general, the globally best solution seems to 
be the MinDist/MaxRev approach. It provides the best customer waiting times, and assures the highest 
global benefit. Moreover, if the aim of the system is to maximize the revenues of the taxi drivers, the 
mediator revenues could be simply redistributed among all involved taxis. On the other hand, if the aim of 
the system is to obtain the highest possible benefit of the mediator service, then the MaxRev approach 
would be the most appropriate. 
5.2. Known customer destinations 
In the second set of experiments we assume that the customer destinations are known to the taxi drivers 
when customers are assigned. In this case, the necessary compensation does not only depend on the distance 
to the customer, but also on the distance required to transport the customer to her destination point. Here, 
a customer that is closer to the current position of a taxi might not always be a “better” customer, since the 
paid part of the trip might be much shorter. In Table 4, we present the results of average waiting times with 
the different methods if the destinations of customers are known when they request a service. We only 
present the results for the center distribution of customer locations. For the uniform distribution the 
behavior is very similar. 
 
Table 4. Average Waiting times for customers for NTNR and variation over NTNR (Absolut in minutes / relative in 
%) with the Center distribution if customer destinations are known. 
Method 
#Customers per hour 
 
1000 
1500 
2000 
2500 
3000 
3500 
4000 
NTNR  
1.2 
1.56 
1.92 
3.83 
8.29 
27.38 
49.01 
FCFS 
0 /  
0 
0 /  
0 
0 /  
0 
35.19 / 
918.8 
67 / 
808.2 
84.63 / 
309.09 
99 /  
202 
FA 
-0.08 / 
-6.67 
-0.21 /  
-13.46 
-0.36 /  
-18.75 
-1.95 /  
-50.91 
-1.88 /  
-22.68 
-2.42 /  
-8.84 
-2.67 /  
-5.45 
MinDist 
0 / 0 
0 / 0 
0 / 0 
-0.2 /  
-5.22 
-0.42 /  
-5.07 
-1.6 /  
-5.84 
-2.24 /  
-4.57 
MaxRev 
-0.01 / 
-0.83 
-0.07 /  
-4.49 
-0.16 /  
-8.33 
-1.6 /  
-41.78 
-1.33 /  
-16.04 
-1.79 /  
-6.54 
-1.98 /  
-4.04 
MinDist/MaxRev 
-0.01 / 
-0.83 
-0.08 /  
-5.13 
-0.19 /  
-9.9 
-1.72 /  
-44.91 
-1.46 / 
 -17.61 
-2.14 /  
-7.82 
-2.39 /  
-4.88 
 
 
 As it can be observed in Table 4, the NTNR, FCFS and FA methods do not change if customer 
destinations are known, since this additional information is not taken into consideration. With regard to the 
compensation methods, MinDist does not improve the waiting times up to 2500 customers per hour. This 
is due to the fact that compensations that would have to be paid to taxi drivers in a global reassignment are 
much higher if customer destinations are known. In our compensation method, taxis never lose money 
when they are reassigned to a new customer and are compensated if the new customer is “worse”. If the 
customer distance is known, now a taxi would need to be compensated not only for extra distance towards 
a new customer, but also for the possible loss in the trip with the customer. This implies that most of the 
time, the mediator would not earn any money with a reassignment. Instead it would have to pay a high 
compensation to taxis and, thus, would incur in a negative benefit. Since we assume that the mediator 
should not have a negative overall outcome, in such cases the reassignment would simply not be applied. 
In the MinDist approach, reassignments with positive outcome for the mediator can only be found for high 
demand scenarios.  
The MaxRev and MinDist/MaxRev methods behave better in this case than MinDist. However, in 
comparison to the case where customer destinations are not known (and, thus, treated as equal), the results 
are slightly lower. Again, this is due to the difficulties for finding global reassignment solutions with a 
positive gain for the mediator. 
Figure 4 presents the benefit of the global system in this set of experiments. The MinDist approach 
behaves very similar to the NTNR method, albeit it assures that taxis always have a slightly higher benefit. 
The benefit of the mediator is almost zero. MaxRev and MinDist/MaxRev provide the highest overall 
revenue for the system, being the combination method slightly better in all cases.  The mediator benefit is 
similar as for the case where customer distances are unknown (and estimated through an average value). 
 
12 
MaxRev again assures the highest benefit for the mediator, ranging from about 1 euro and 54 euros for each 
1000 customers at a generation rate of 1000 and 3000 customers per hour, respectively. 
 
 
 
 
Figure 4. Overall revenue of the system in euros for the Center distribution and for different numbers of customers per 
hour if customer destinations are known. The outcome is normalized to 1000 customers and is composed of the benefit 
of the (1000) taxis plus the benefit of the mediator service. 
 
6. Conclusion 
In this paper, we have presented a new algorithm for dynamic taxi dispatching. Our proposal 
characterizes and differentiates from other approaches in the possibility of proposing taxi reassignments 
that improve a current global assignment when new customers or available taxis appear. In order to create 
a realistic dispatching proposal that is accepted by rational autonomous taxi drivers, we devised a 
compensation system in which taxis that get a “worse” customer receive an extra monetary compensation 
while those who get “better” customers pay part of the reduced costs to a mediator service. The mediator 
service (e.g. a taxi company), manages all those payments and only proposes a better assignment if its 
accumulated revenue is positive and thus, it does not incur in extra costs. We evaluated three different 
versions of our proposal, namely (i) minimizing distances, (ii) maximizing mediator revenue, and (iii) a 
combination of both. We compared them to the standard FCFS dispatching strategy and its modified NTNR 
approach, as well as to a “complete” dynamic reassignment approach without compensations (FA). The 
results showed that our methods outperform the standard strategies, especially when the demand increases. 
It obtains very similar results as the FA approach. 
A key lesson learnt from our experiments is that, especially in high load situations, a dynamic 
reassignment approach can produce noticeable benefits for system performance. This is true for all 
stakeholders: our proposal contributes to reduce average waiting time of customers and also helps to 
increase drivers’ revenues. In some configurations, the new assignments even allow obtaining some 
economic benefit for the mediator as well, resulting from exceeding incomes of compensations. Such extra 
money could be shared, for instance, to the taxis. Notice that the performance of the dispatching strategy 
could be improved even further if the compensation system allowed a negative balance. This would not be 
necessarily unrealistic if the mediator was a public entity. For example, a municipality might be willing to 
invest some money if a more efficient service is provided, thus reducing CO2 emissions (probably reducing 
city fines for high emissions imposed by superior authorities). 
Still, while we have obtained promising results, it should be noticed that our approach does rely on some 
simplifications. Obviously, assuming that travel time is proportional to distance is one of them. 
Furthermore, we assumed that all taxi drivers are (economically) rational utility maximisers, and that even 
a very small additional monetary benefit can make them accept changing their plans, which may not always 
be true. Finally, we do not consider the possibility of taxi drivers to “opt out” of our mechanism, e.g. by 
not accepting its indications (for whatever reason). This may introduce additional “noise” in our 
assignments with a potentially negative impact on their performance. 
The taxi dispatching model presented in this article opens up several avenues for subsequent research in 
the field of knowledge-based and expert systems. Our future work will unfold among several lines. Firstly, 
we intend to relax some of the simplifying assumptions that our current approach relies on. In particular, 
we will base the different cost functions used in our mechanism on a more accurate estimation of travel 
times and distances. For this purpose, we will set out from a realistic road network topology and real-world 
 
13 
load data. This information is already available to us for the town of Madrid. We will then be able to 
perform our simulations with a fully-fledged microscopic traffic simulator such as SUMO5. In this context, 
we plan to reach an agreement with a taxi fleet operator so as to use real-world data on customer origin and 
destinations for the experiments. 
Secondly, similar to the problems addressed by the works of Laha and Putatunda (2018) or Ocalir et al. 
(2010), we plan to apply methods to estimate probabilities of new taxi customers appearing in different 
areas of the network. In a previous paper related to the dynamic positioning of ambulances, we used 
Voronoi Tessellation to dynamically determine the default positions for service vehicles (Billhardt et al., 
2014) based on historical data. Such information can be complemented with domain knowledge of a 
particular destination (soccer matches, concerts, etc.). The corresponding information would then be used 
to make the taxi dispatch decisions more predictive. 
Thirdly, in line with the work by Massowa y Canbolat (2010), we intend to look into more complex 
models of the taxi drivers’ reactions to our dispatch strategy, so as to better adjust compensations and 
provide the designer of a taxi dispatching model with guidelines to choose among alternative strategies for 
revenue distribution (MinDist, MaxRev, etc.). To this respect, we plan to analyze how the ratio of taxi 
drivers that decide to refrain from using our dispatching service affects to the global performance of the 
system.  
Fourthly, in this context, we also plan to look into coalition formation techniques from Cooperative 
Game Theory. In general, algorithms to compute solutions to coalitional games are known to suffer 
complexity problems, but we have applied an approximation scheme (the bilateral Shapely Value) in the 
context of a Smart Grid application (Mihailescu et al., 2017), and would like to explore as to how far a fair 
and efficient split of the savings implied by taxi reassignment can be implemented in this manner. 
Finally, it should be noted that the mechanism proposed in this article does not only apply to taxi fleets 
but could also be adapted to other types of open fleets (Billhardt et al., 2017), where autonomous drivers 
with individual objectives provide some transportation service (e.g., messenger or parcel delivery services). 
In particular, to this respect, we will investigate the use of heterogeneous fleets or “CyberFleets”, as 
proposed in (Billhardt et al. 2014b).  The proposal from this article could even provide a suitable basis for 
managing other sorts of large-scale Demand Responsive Transport systems that provide shared 
transportation services with flexible routes (Satunin and Babkin, 2014). 
 
Acknowledgements  
This work was supported by the Autonomous Region of Madrid (grant ""MOSI-AGIL-CM"" (S2013/ICE-
3019) co-funded by EU Structural Funds FSE and FEDER), project ""SURF"" (TIN2015-65515-C4-X-R 
(MINECO /FEDER)) funded by the Spanish Ministry of Economy and Competitiveness, and through the 
Excellence Research Group GES2ME (Ref. 30VCPIGI05) co-funded by URJC and Santander Bank. 
 
References 
 
Bertsekas, D.P. (1988). The auction algorithm: A distributed relaxation method for the assignment problem. Annals of Operations 
Research, 14 (1), 105–123.  
Billhardt, H., Fernández, A., Lujak, M., Ossowski, S., (2018). Agreement Technologies for Coordination in Smart Cities. Applied 
Science 8 (5) 
Billhardt, H., Fernández, A., Lujak, M., Ossowski, S., Julián, V., de Paz, J.F. & Hernández, J. (2017). Coordinating open fleets. A 
taxi assignment example. AI Communications, 30 (1), 37–52. 
Billhardt, H., Lujak, M., Sánchez-Brunete, V., Fernández, A., &  Ossowski, S. (2014). Dynamic coordination of ambulances for 
emergency medical assistance services. Knowledge-Based Systems, 70, 268–280. 
Billhardt, H.; Fernández, A.; Lemus, L.; Lujak, M.; Osman, N.; Ossowski, S.; Sierra, C. (2014b). Dynamic Coordination in Fleet 
Management Systems: Toward Smart Cyber Fleets. IEEE Intelligent Systems 29(3), pp. 70-76 
d’Orey, P., Fernandes, R., & Ferreira, M. (2010). Empirical evaluation of a dynamic and distributed taxi-sharing system. In Proc. 
IEEE Conf. on Intelligent Transportation Systems.  
Dai, G., Huang, J., Wambura, S. M., & Sun, H. A. (2017). Balanced Assignment Mechanism for Online Taxi Recommendation. In 
Proc. of the 18th IEEE International Conference on Mobile Data Management (MDM) (pp. 102–111). 
Egbelu, P.J., & Tanchoco, J.M.A. (1984). Characterization of Automatic Guided Vehicle Dispatching Rules. International Journal of 
Production Research, 22 (3), 359–374.  
Gao, G., Xiao, M., & Zhao, Z. (2016). Optimal Multi-taxi Dispatch for Mobile Taxi-Hailing Systems. In Proc. of the 45th International 
Conference on Parallel Processing (ICPP) (pp. 294-303). 
Glaschenko, A., Ivaschenko, A., Rzevski, G., & Skobelev, P. (2009). Multi-Agent real time scheduling system for taxi companies. In 
Proc. Int. Conf. Autonomous. Agents and Multiagent Systems (AAMAS), (pp. 29-36). 
Grajciar, M. (2015). Profit-Aware Driver Routing for On-Demand Transport Services. (2015). In Proc. of the 19th International 
Student Conference on Electrical Engineering (POSTER 2015), Prague. 
 
5 http://sumo.dlr.de/index.html 
 
14 
Laha, A.; Putatunda, S. (2018). Real time location prediction with taxi-GPS data streams. Expert Systems with Applications 92, pp. 
298-322 
Lee, D., Wang, H., Cheu, R., & Teo, S. (2004). Taxi dispatch system based on current demands and real-time traffic conditions. 
Transportation Research Record: Journal of the Transportation Research Board, 1882 (1), 193–200. 
Li, J. P., Horng, G. J., Chen, Y. J., & Cheng, S. T. (2016). Using Non-cooperative Game Theory for Taxi-Sharing Recommendation 
Systems. Wireless Personal Communications, 88 (4), 761-786.  
Ma, S., Zheng, Y., & Wolfson, O. (2013). T-share: A large-scale dynamic taxi ridesharing service. In Proc IEEE 29th International 
Conference on Data Engineering (ICDE) (pp. 410–421). 
Maciejewski, M.,  Salanova, J. M., Bischoff, J., & Estrada, M. (2016).  Large-scale microscopic simulation of taxi services. Berlin 
and Barcelona case studies. Journal of Ambient Intelligence and Humanized Computing, 7(3), 1-9. 
Maciejewski, M., Bischoff, J., & Nagel, K. (2016). An Assignment-Based Approach to Efficient Real-Time City-Scale Taxi 
Dispatching. IEEE Intelligent Systems, 31 (1), 68–77. 
Mareček, L., Shorten, R., & Yu, J. Y. (2016). Pricing vehicle sharing with proximity information. 2016 3rd MEC International 
Conference on Big Data and Smart City (ICBDSC), Muscat, Oman, 2016, (pp. 1-7). 
Meghjani, M., & Marczuk, K. (2016). A hybrid approach to matching taxis and customers. In Proc. of the Region 10 Conference 
(TENCON), (pp. 167-169). 
Miao, F., Han, S., Lin, S., Stankovic, J. A., Zhang, D., Munir, S., Huang, H., He, T., & Pappas, G. J. (2016). Taxi Dispatch With Real-
Time Sensing Data in Metropolitan Areas: A Receding Horizon Control Approach. IEEE Transactions on Automation Science 
and Engineering, 13(2), 463-478. 
Miao, F., Han, S., Lin, S., Wang, Q., Stankovic, J. A., Hendawi, A., Zhang, D., He, T., & Pappas, G. J. (2017). Data-Driven Robust 
Taxi Dispatch Under Demand Uncertainties. IEEE Transactions on Control Systems Technology (Early Access) 
Mihailescu, R.; Klusch, M.; Ossowski, S. (2017). eCOOP: Applying Dynamic Coalition Formation to the Power Regulation Problem 
in Smart Grids. Computational Intelligence 33(3). pp. 401-427 
Moreira-Matias, L., Gama, J., Ferreira, M., Mendes-Moreira, J., & Damas, L. (2013). Predicting taxi-passenger demand using 
streaming data. IEEE Transactions on Intelligent Transportation Systems, 14 (3), 1393–1402.  
Moreira-Matias, L., Mendes-Moreira, J., Ferreira, M., Gama, J., & Damas, L. (2014). An online learning framework for predicting 
the taxi stand's profitability. In Proc. of the 17th International Conference on Intelligent Transportation Systems (ITSC), (pp. 
2009-2014). 
Ngo, M. N., Seow, K. T., & Wong, K. W. (2004). Fuzzy linear assignment problem: an approach to vehicle fleet deployment. In Proc. 
of the International Conference on Fuzzy Systems (pp. 1197-1202). 
Ocalir, E.;  Ercoskun, O.; Tur, R. (2010). An integrated model of GIS and fuzzy logic (FMOTS) for location decisions of taxicab 
stands. Expert Systems with Applications 37 (7), pp. 4892-4901 
Ossowski, S.; Omicini, A. (2002). Coordination knowledge engineering. Knowledge Engineering Review 17 (4), pp. 309-316 
Ossowski, S. et al. (2013). Agreement Technologies. Law, Governance and Technology series (LGTS) no. 8. Springer. ISBN 978-94-
007-5582-6 
Satunin, S.; Babkin, E. (2014). A multi-agent approach to Intelligent Transportation Systems modeling with combinatorial auctions. 
Expert Systems with Applications 41 (15), pp. 6622-6633 
Tian, C., Huang, Y, Liu, Z., Bastani, F., & Jin, R. (2013). Noah: a dynamic ridesharing system. In Proceedings of the 2013 ACM 
SIGMOD International Conference on Management of Data (pp. 985–988). 
von Massowa, M.; Canbolat, M. (2010). Fareplay: An examination of taxicab drivers’ response to dispatch policy. Expert Systems 
with Applications 37 (3). pp. 2451-2458 
Zhang, D., Sun, L., Li, B., Chen, C., Pan, G., Li, S., & Wu, Z. (2015). Understanding taxi service strategies from taxi gps traces. IEEE 
Transactions on Intelligent Transportation Systems, 16 (1), 123–135. 
Zhu, C., & Prabhakar, B. (2017). Reducing Inefficiencies in Taxi Systems. In Proc. of the 56th IEEE Conference on Decision and 
Control (CDC). (pp. 6301-6306) 
 
 
 
 
","Many works analyze new assignment strategies to reduce waiting times of customers, with the classical approach being the first-come/first-served strategy. Other works try to minimize the average waiting time of customers, while others focus on different goals, such as fairness (minimizing differences in income among taxi drivers), optimal multi-taxi dispatching, or minimizing total travel cost. There are also works that focus on taxi demand prediction to help taxis find closer passengers or balance supply and demand of taxis in an area of interest.nan"
"This work presents a generalizable framework for achieving reliable sim2real transfer of autonomy-oriented control systems using multi-model multi-objective robust optimal control synthesis. We present a systematic study on the complete mechatronic design, dynamics modeling, parameter identification, and robust stabilizing as well as steady-state tracking control of Nigel using the proposed framework, with experimental validation.","In the context of developing scaled autonomous vehicles, educational and research institutes have contributed platforms like the MIT Racecar, AutoRally, F1TENTH, MuSHR, ORCA Project, Delft Scaled Vehicle (DSV) and Berkeley Autonomous Race Car (BARC). Community-driven projects such as HyphaROS Race-Car and Donkey Car have also emerged, which are often tailored for specific applications like map-based navigation and vision-aided imitation learning, respectively. Moreover, commercial products like Quanser QCar and AWS DeepRacer have also entered the market, but their closed-source nature and prohibitive costs limit their accessibility to the broader community. Additionally, scaled robot platforms like the TurtleBot3 and Duckiebot remain valuable for teaching fundamental autonomy concepts. However, none of the aforementioned platforms (refer Table I) contributes primarily towards novel vehicular configuration or architecture.","The key design objective for vehicle architecture discussed in this work was to develop a mechatronically redundant scaled autonomous vehicle. The resulting prototype, Nigel, offers redundant driving and steering actuation, a comprehensive sensor suite, high-performance computational resources, and a standard vehicular signaling system (refer Fig. 2).","Considering the notations presented in Fig. 4, and following the seminal works [38], [39], we can deduce the non-linear yaw-plane vehicle dynamics model for an independent 4WD4WS configuration as depicted in Eq. 1a-1c.","In this work, we first introduced the complete mechatronic design architecture of Nigel and also presented the detailed dynamics modeling of this independent 4WD4WS vehicle. We also formulated a linear parameter-varying polytopic model of the system for synthesizing a robust optimal controller using an LMI approach, which seeks to minimize the H2-H∞ tradeoff criteria while satisfying the closed-loop pole placement constraints in a Pareto-optimization scheme. We demonstrated and analyzed robust stabilizing as well as steady-state tracking control of Nigel with exhaustive simulation-based verification. The proposed reliable sim2real framework was also experimentally validated using the 4WD4WS vehicle platform.",Nigel -- Mechatronic Design and Robust Sim2Real Control of an Over-Actuated Autonomous Vehicle,"Chinmay Vilas Samak, Tanmay Vilas Samak, Javad Mohammadpour Velni, Venkat Narayan Krovi","1
Nigel - Mechatronic Design and Robust Sim2Real
Control of an Over-Actuated Autonomous Vehicle
Chinmay V. Samak∗
, Tanmay V. Samak∗
, Javad M. Velni
and Venkat N. Krovi
Abstract—Simulation to reality (sim2real) transfer from a
dynamics and controls perspective usually involves re-tuning or
adapting the designed algorithms to suit real-world operating
conditions, which often violates the performance guarantees
established originally. This work presents a generalizable frame-
work for achieving reliable sim2real transfer of autonomy-
oriented control systems using multi-model multi-objective robust
optimal control synthesis, which lends well to uncertainty han-
dling and disturbance rejection with theoretical guarantees. Par-
ticularly, this work is centered around an actuation-redundant
scaled autonomous vehicle called Nigel, with independent all-
wheel drive and independent all-wheel steering architecture,
whose enhanced configuration space bodes well for robust control
applications. To this end, we present a systematic study on
the complete mechatronic design, dynamics modeling, parameter
identification, and robust stabilizing as well as steady-state
tracking control of Nigel using the proposed framework, with
experimental validation.
Index Terms—Autonomous vehicles, 4WD4WS vehicles, over-
actuated systems, mechatronic design, robust optimal control, lin-
ear matrix inequalities, sim2real transfer, uncertainty handling,
disturbance rejection.
I. INTRODUCTION
M
ODERN day automotive systems exploit a combination
of mechanical electrical, electronic, networking, and
software sub-systems to enhance performance via a hierar-
chical suite of autonomy-oriented control realizations. While
earlier autonomy developers may have enjoyed the freedom
of primarily focusing on core software development, the
present context demands a paradigm shift towards a synergis-
tic hardware-software co-design approach, harmonizing with
the principles of mechatronics engineering [1]. In particular,
the demand for increased maneuverability, enhanced control
configuration space, and improved tolerance against faults
motivates the pursuit of unconventional vehicle designs. In
response, autonomous vehicle (AV) realizations must augment
their underlying hardware-software architectures to enhance
core performance as well as adaptability to changing oper-
ating conditions. However, with the advent of novel design
architectures, advanced control strategies [2] are required to
fully exploit the added capabilities. Moreover, the devised
control systems need to successfully transition the sim2real
∗These authors contributed equally.
C. V. Samak, T. V. Samak and V. N. Krovi are with the Automation,
Robotics and Mechatronics Laboratory (ARMLab), Department of Automo-
tive Engineering, Clemson University International Center for Automotive
Research (CU-ICAR), Greenville, SC 29607, USA. Email: {csamak,
tsamak, vkrovi}@clemson.edu
J.
M.
Velni
is
with
the
Velni
Lab,
Department
of
Mechanical
Engineering,
Clemson
University,
Clemson,
SC
29634,
USA.
Email:
javadm@clemson.edu
Fig. 1.
Nigel: A mechatronically redundant and reconfigurable 1:14 scale
autonomous vehicle. This modular and open-source vehicle platform offers
multiple driving/steering configurations, abundant interoceptive/exteroceptive
sensors, on-board edge-AI computation and wired/wireless communication,
selectively constrained software-defined actuation, as well as a fully functional
lighting and signaling system. A video highlighting key features of Nigel is
available at https://youtu.be/UVIShZuZmpg
gap by guaranteeing robust performance against real-world
uncertainties and disturbances. This further calls for model and
simulation-based controller refinement along with real-world
testing, which requires capable cyber-physical deployment
platforms of varying scales. Previous works have tried to
address some of these aspects, albeit in a fragmented sense
as we will review below.
In the context of developing scaled autonomous vehicles,
educational and research institutes have contributed platforms
like the MIT Racecar [3], AutoRally [4], F1TENTH [5],
Multi-agent System for non-Holonomic Racing (MuSHR) [6],
Optimal RC Racing (ORCA) Project [7], Delft Scaled Vehicle
(DSV) [8] and Berkeley Autonomous Race Car (BARC)
[9]. Community-driven projects such as HyphaROS Race-
Car [10] and Donkey Car [11] have also emerged, which
are often tailored for specific applications like map-based
navigation and vision-aided imitation learning, respectively.
Moreover, commercial products like Quanser QCar [12] and
AWS DeepRacer [13] have also entered the market, but
their closed-source nature and prohibitive costs limit their
accessibility to the broader community. Additionally, scaled
robot platforms like the TurtleBot3 [14] and Duckiebot [15]
remain valuable for teaching fundamental autonomy concepts.
However, none of the aforementioned platforms (refer Table I)
arXiv:2401.11542v1  [cs.RO]  21 Jan 2024
2
TABLE I
COMPARATIVE ANALYSIS OF POPULAR SCALED AUTONOMOUS VEHICLE PLATFORMS
Platform
Cost†
Sensing Modalities
Computational Resources
Actuation
Mechanism‡
Lights &
Indicators
V2X Support
API Support
Scale
Open Hardware
Open Software
Throttle
Steering
Wheel Encoders
GPS/IPS
IMU
Microphone
LIDAR
Monocular Camera
Depth/Stereo Camera
High-Level
Low-Level
Lights
Buzzer/Speaker
V2V
V2I
C++
Python
ROS
ROS 2
Autoware
MATLAB/Simulink
Webapp
Nigel (Ours)
1:14
✓
✓
$600
✓
✓
✓
✓
✓
✓
✓
✓
✓
Jetson Orin Nano
Arduino Mega
4WD4WS
✓
✓
✓
✓
✓
✓
✓
✓
✓
★
✓
MIT Racecar
1:10
★
✓
$2,600
✗
✗
✗
✗
✓
✗
✓
★
✓
Jetson TX2
VESC
AS
✗
✗
★
✗
✗
✗
✓
✗
✗
✗
✗
AutoRally
1:5
★
✓
$23,300
✗
✗
✓
✓
✓
✗
✓
★
✓
Custom
Teensy LC/Arduino Micro
AS
✗
✗
★
✗
✗
✗
✓
✗
✗
✗
✗
F1TENTH
1:10
★
✓
$3,260
✗
✗
✗
✗
✗
✗
✓
★
★
Jetson TX2
VESC 6MkV
AS
✗
✗
✓
✗
✗
✗
✓
✓
✓
✗
✗
DSV
1:10
★
✓
$1,000
✗
✗
✓
✗
✓
✗
✓
✓
✗
ODROID-XU4
Arduino (Mega + Uno)
AS
✗
✗
✗
✗
✗
✗
✓
✗
✗
✗
✗
MuSHR
1:10
★
✓
$930
✗
✗
✗
✗
✗
✗
✓
★
✓
Jetson Nano
Turnigy SK8-ESC
AS
✗
✗
✓
✗
✗
✗
✓
✗
✗
✗
✗
BARC
1:10
★
✓
$1,030
✗
✗
✓
✗
✓
✗
✗
✓
✗
ODROID-XU4
Arduino Nano
AS
✗
✗
✗
✗
✗
✗
✓
✗
✗
✗
✗
ORCA
1:43
★
✓
$960
✗
✗
✗
✗
✓
✗
✗
✗
✗
None
ARM Cortex M4 µC
AS
✗
✗
✗
✓
✓
✗
✗
✗
✗
★
✗
HyphaROS
1:10
★
✓
$600
✗
✗
✗
✗
✓
✗
✓
✗
✗
ODROID-XU4
RC ESC TBLE-02S
AS
✗
✗
✗
✗
✗
✗
✓
✗
✗
✗
✗
Donkey Car
1:16
★
✓
$370
✗
✗
✗
✗
✗
✗
✗
✓
✗
Raspberry Pi
ESC
AS
✗
✗
✗
✗
✗
✓
✗
✗
✗
✗
✗
QCar
1:10
✗
✗
$20,000
✗
✗
✓
✗
✓
✓
✓
✓
✓
Jetson TX2
Proprietary
AS
✓
✓
✓
✗
★
★
★
✗
✗
✓
✗
DeepRacer
1:18
✗
✗
$400
✗
✗
✗
✗
✓
✗
★
✓
✗
Proprietary
Proprietary
AS
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✓
Duckiebot
N/A
✓
✓
$450
✗
✗
★
✗
★
✗
✗
✓
✗
Raspberry Pi/Jetson Nano
None
DD
★
✗
★
★
✗
✗
✓
✗
✗
✗
✗
TurtleBot3
N/A
✓
✓
$590
✗
✗
✓
✗
✓
✗
✓
★
✗
Raspberry Pi
OpenCR
DD
✗
✗
★
✗
✗
✗
✓
★
✗
✗
✗
†All cost values are ceiled to the nearest $10. ‡Actuation mechanisms comprise Ackermann steered (AS), differential-drive (DD), and 4-wheel drive 4-wheel steer (4WD4WS) configurations.
✓ indicates complete fulfillment; ★ indicates conditional, unsupported or partial fulfillment; and ✗ indicates non-fulfillment.
contributes primarily towards novel vehicular configuration or
architecture. Other works such as [16] and [17] have recently
prototyped over-actuated scaled vehicles with the primary aim
of validating their control algorithms, and as such, none of
these platforms are open-sourced. Additionally, both of these
platforms lack comprehensive autonomy features and only the
latter offers truly independent driving and steering capability,
although at a much larger scale (1:5) and without extended
steering limits. To the best of authors’ knowledge, Nigel (refer
Fig. 1) is the first open-source1 mechatronically redundant [18]
autonomous vehicle platform offering comprehensive auton-
omy features as well as independent all-wheel driving and
independent all-wheel steering (i.e., independent 4WD4WS)
configuration with extended (±90◦) steering angles, within
a small footprint of 1:14 scale. Additionally, Nigel is a part
of the larger AutoDRIVE Ecosystem2 [19], [20], which also
offers a high-fidelity digital-twin simulation platform [21],
[22], as well as flexible application programming interfaces
(APIs) to develop low and high-level autonomy algorithms.
In terms of developing control strategies for vehicles with
unconventional architectures, prior research endeavors have
proposed and applied techniques such as inverse dynamics
control [23], coordinated motion control [24], adaptive steering
control [25] and model predictive control [26], [27] for achiev-
ing driver-assistance as well as automatic control objectives.
Although some of these works discuss the adaptability and
robustness of the designed controllers in simulated experi-
ments, they cannot guarantee similar performance in real-
world conditions. Other recent works such as [28] and [29]
have applied cascaded feedforward control and robust H∞
control, respectively, to handle environmental uncertainties and
disturbances using over-actuated vehicle architectures. These
robust control techniques in themselves are not completely
novel, and have been studied and applied to automotive
systems previously for yaw-plane [30]–[32], roll-plane [33]
or vertical dynamics stabilization [34], automatic emergency
braking [35], trajectory tracking [36], etc.
1GitHub: https://github.com/AutoDRIVE-Ecosystem
2Website: https://autodrive-ecosystem.github.io
Our work poses the problem of robust control from a
“sim2real transfer” lens, to develop a generalizable framework
for reliably bridging the dynamics interface of the sim2real
gap. To this end, we demonstrate robust stabilizing as well as
steady-state tracking control of Nigel with seamless sim2real
transfer, by designing a multi-model multi-objective full-state
feedback controller using a linear matrix inequality (LMI) ap-
proach, which seeks the optimal tradeoff between H2 and H∞
performance with D-stability. It is to be noted that, contrary
to some of our previous works, which have targeted bridging
the sim2real gap by increasing simulation fidelity [37], this
work intentionally widens the reality gap by using a simplified
linear dynamics model with lumped wheel representation to
synthesize the robust optimal controller, which is hypothesized
to bridge the dynamics gap during real-world deployment. As
such, the devised controller is tested rigorously in simulation
as well as real-world settings for different benchmark maneu-
vers under varying grades of disturbances and uncertainties. In
addition to validating the sim2real transfer, we also analyze the
controller performance against deliberately injected exogenous
disturbances and uncertainties during real-world experiments.
The key contributions of this paper are summarized below:
• Open-hardware, open-software design architecture of a
novel 1:14 scale independent 4WD4WS autonomous ve-
hicle, with extended steering limits is presented.
• Non-linear dynamics model of such a 4WD4WS vehicle
is derived and transformed into a linear uncertain system
model with reasonable parameter identification.
• Robust sim2real control framework is established and
validated using exhaustive experiments in simulation as
well as real-world settings.
The remainder of this paper is organized as follows. Section
II elucidates the complete mechatronic design architecture of
Nigel. A detailed derivation of the independent 4WD4WS
vehicle dynamics model and its analysis is discussed in Section
III. Formulation of the proposed generalizable robust optimal
sim2real control framework is presented in Section IV. Details
pertaining to its performance evaluation in simulation as well
as real-world settings are discussed in Section V. Finally,
Section VI presents concluding remarks and future directions.
3
II. VEHICLE DESIGN ARCHITECTURE
Fig. 2. Various components and sub-systems of Nigel. Dashed lines indicate
occluded or hidden components.
The key design objective for vehicle architecture discussed
in this work was to develop a mechatronically redundant scaled
autonomous vehicle. The resulting prototype, Nigel, offers
redundant driving and steering actuation, a comprehensive
sensor suite, high-performance computational resources, and
a standard vehicular signaling system (refer Fig. 2).
Chassis: Nigel is a 1:14 scale autonomous vehicle com-
prising four modular platforms, each housing distinct com-
ponents of the vehicle. It offers mechatronic redundancy via
comprehensive autonomy features as well as an independent
4WD4WS configuration (refer Fig. 3).
Power Electronics: Nigel is powered using an 11.1 V
5200 mAh lithium-polymer (LiPo) battery, whose health is
monitored by a voltage checker. Other components such as the
master switch, buck converter and motor drivers help route the
power to appropriate sub-systems of the vehicle.
Sensor Suite: Nigel hosts a comprehensive sensor suite
comprising throttle/steering feedbacks and 1920 CPR incre-
mental encoders for all 4 wheels, a microphone, a 3-axis
indoor-positioning system (IPS) using retroreflective/fiducial
markers, a 9-axis inertial measurement unit (IMU) with
raw/filtered measurements, multiple RGB/RGB-D/stereo cam-
era(s) in the front/rear, and a 360◦ FOV planar LIDAR.
Computation, Communication and Software: Nigel adopts
Jetson Orin Nano Developer Kit for most of its high-level
computation (autonomy algorithms) and communication (V2V
and V2I). Additionally, it also hosts an Arduino Mega (running
the vehicle firmware) for acquiring and filtering raw sensor
data, and controlling the actuators/lights/indicators.
Actuators: Nigel is actuated using four 6V 160 RPM rated
120:1 DC geared motors to drive its wheels, and four 9.4
kgf.cm servo motors to steer them; the steering actuators are
saturated at ± 90◦ w.r.t. zero-steer value. All the actuators are
operated at 5V, which translates to a maximum speed of ∼130
RPM for driving and ∼0.19 s/60◦ for steering. The steering
actuators are positioned directly above the respective tire
contact patch, which enables zero camber gain or scrubbing
while steering the wheels and keeps the actuator effort stable.
Lights and Indicators: Nigel’s lighting system comprises
dual-mode headlights, triple-mode turning indicators, and au-
tomated taillights with reverse indicators. Additionally, Nigel
is also provided with a buzzer to allow acoustic indication.
Kinematic analysis of Nigel’s configuration reveals that it
has δM = δm+δs = 1+2 = 3 degrees of maneuverability; this
is superior among all passively stable configurations possible.
Elucidation follows. Considering the vehicle’s configuration
space C ∈ Rm, the n-dimensional admissible velocity space,
which is a sub-space of the generalized velocity space V ∈
Rm (composed of the time derivatives of the generalized
coordinates of C ), governs the vehicle’s differential degrees of
freedom (a.k.a. degree of mobility, δm = n). Given the sliding
constraint matrix C1(δi) =
 C1f
C1s(δi)

for fixed (f) and
steerable (s) wheels, this translates to δm being the dimension
of the right null space of C1(δi), i.e., δm = dim N[C1(δi)] =
3 − rank[C1(δi)]. The degree of steerability, δs, is governed
by the sliding constraints imposed by the i steerable wheels,
with δs = rank[C1s(δi)]; 0 ≤ δs ≤ 2.
Lastly, it is worth mentioning that Nigel supports modular
open system architecture (MOSA) standards. This allows the
users to adopt and adapt the vehicle for their custom use cases
by adding, removing or replacing selective components, or
extensively modifying various aspects of the vehicle design.
Fig. 3.
Independent 4WD4WS architecture of Nigel: (a) denotes 2-DOF actuation redundancy per wheel; (b)-(g) depict common drive configurations
including front-wheel drive, rear-wheel drive, all-wheel drive, neutral-steer drive, pivot-steer drive and torque vectoring drive; and (h)-(m) depict common
steering configurations including front-wheel steer, rear-wheel steer, all-wheel in-phase steer, all-wheel out-of-phase steer, oblique steer and crab-walk steer.
4
III. DYNAMICS MODELING AND ANALYSIS
Fig. 4. Vehicle dynamics model for an independent 4WD4WS vehicle.
Considering the notations presented in Fig. 4, and follow-
ing the seminal works [38], [39], we can deduce the non-
linear yaw-plane vehicle dynamics model for an independent
4WD4WS configuration as depicted in Eq. 1a-1c.
X
Fx : m

˙v cos(β) − v ˙β sin(β) − ˙
ψv sin(β)

=
"" τRL
rRL
cos(δRL) +
τRR
rRR
cos(δRR)
+
τF L
rF L
cos(δF L) +
τF R
rF R
cos(δF R)
#
−

µRLCRL

δRL−
tan−1
 ˙y − ˙
ψlr
˙x − ˙
ψlt
!#
sin(δRL) + µRRCRR
""
δRR − tan−1
 ˙y − ˙
ψlr
˙x + ˙
ψlt
!#
sin(δRR) + µF LCF L
""
δF L − tan−1
 ˙y + ˙
ψlf
˙x − ˙
ψlt
!#
sin(δF L)+
µF RCF R
""
δF R − tan−1
 ˙y + ˙
ψlf
˙x + ˙
ψlt
!#
sin(δF R)
)
− Fhw
(1a)
X
Fy : m

˙v sin(β) + v ˙β cos(β) + ˙
ψv cos(β)

=
"" τRL
rRL
sin(δRL) +
τRR
rRR
sin(δRR)
+
τF L
rF L
sin(δF L) +
τF R
rF R
sin(δF R)
#
+

µRLCRL

δRL−
tan−1
 ˙y − ˙
ψlr
˙x − ˙
ψlt
!#
cos(δRL) + µRRCRR
""
δRR − tan−1
 ˙y − ˙
ψlr
˙x + ˙
ψlt
!#
cos(δRR) + µF LCF L
""
δF L − tan−1
 ˙y + ˙
ψlf
˙x − ˙
ψlt
!#
cos(δF L)+
µF RCF R
""
δF R − tan−1
 ˙y + ˙
ψlf
˙x + ˙
ψlt
!#
cos(δF R)
)
+ Fsw
(1b)
X
Mz : Iz ¨
ψ = lf
(
−
τF L
rF L
sin(δF L − θF L) +
τF R
rF R
sin(δF R + θF R) + µF LCF L
""
δF L − tan−1
 ˙y + ˙
ψlf
˙x − ˙
ψlt
!#
cos(δF L − θF L) + µF RCF R

δF R−
tan−1
 ˙y + ˙
ψlf
˙x + ˙
ψlt
!#
cos(δF R + θF R)
)
− lr
( τRL
rRL
sin(δRL + θRL)−
τRR
rRR
sin(θRR − δRR) + µRLCRL
""
δRL − tan−1
 ˙y − ˙
ψlr
˙x − ˙
ψlt
!#
cos(δRL+
θRL) + µRRCRR
""
δRR − tan−1
 ˙y − ˙
ψlr
˙x + ˙
ψlt
!#
cos(θRR − δRR)
)
+
 lf − lr
2
!
Fsw
(1c)
where, Fyi = µiCiαi and Fdi =
(
τi/ri;
if τi/ri ≤ µiNi
µiNi;
otherwise
are tire forces and drive forces for i-th wheel, respectively. The
tire slip angle αi = δi − βi, where βi = tan−1  ˙y± ˙ψlf/r
˙x± ˙ψlt

.
TABLE II
MEASURED AND IDENTIFIED PARAMETERS OF NIGEL
Parameter
Symbol
Value
Unit
Measured Parameters
Mass
m
2.68
kg
Dimensions (L×B×H)
—
0.31847×0.175×0.2568
m
Wheelbase
l
0.14155
m
Track width
lt
0.14724
m
Wheel radius
ri
0.0325
m
Identified Parameters
Yaw moment of inertia
Iz
0.01944
kg.m2
CG front offset
lf
0.06226
m
CG rear offset
lr
0.07929
m
Friction coefficient (nominal)
µi
0.4
—
Tire stiffness
Ci
22.4768
N.rad−1
This model can be linearized by applying the small-angle
approximation so that sin(⊙)
≈
⊙, cos(⊙)
≈
1 and
tan−1(⊙) ≈ ⊙. Additionally, we assume that the vehicle is
driving at a constant velocity, implying ˙v ≈ 0 ⇒ τi ≈ 0.
Finally, it is also assumed that ˙x ≫ ˙ψlt ⇒ ˙x ± ˙ψlt ≈ ˙x. Also
note that Fsw is hereafter referred to as Fw for simplicity,
since Fhw does not affect the linearized model.
 ˙β
¨
ψ

=

a11
a12
a21
a22
 β
˙ψ

+

b11
b12
b13
b14
b21
b22
b23
b24



δF L
δF R
δRL
δRR

+

d1
d2

Fw (2)
where,
a11 =
−1
mv
5
IV. ROBUST SIM2REAL CONTROL FRAMEWORK
Fig. 6. Structure of the presented robust sim2real control framework.
Based on the linearized vehicle dynamics model derived
in Section III a generalized representation of the open-loop
system Sol is formulated (refer Eq. 3).
Sol :=









˙xp = Apxp + Bpu + Dpw
y1 = Cp1xp + By1u + Dyw
y2 = Cp2xp + By2u
zp = Mpxp + Dzw
(3)
Here,
xp
=

β
˙ψ
T
are
states,
u
=
δF L
δF R
δRL
δRR
T are control inputs and w = Fw
is exogenous disturbance. The state transition is governed
by Ap =
a11
a12
a21
a22

, Bp =
b11
b12
b13
b14
b21
b22
b23
b24

and
Dp =
d1
d2

. System outputs y1 = y2 =

xp
u
T are
governed by Cp1 = Cp2 =

I2×2
04×2

, By1 = By2 =

02×4
I4×4

and Dy
=
06×1. The measurements comprise full-state
feedback with Mp = I2×2 and Dz = 02×1.
Considering the sim2real gap in terms of uncertainties in
frictional coefficients of the 4 road-tire interconnects ρ =
⟨µF L, µF R, µRL, µRR⟩, where parameters ρi can be time-
varying or constant but uncertain, this work adopts polytopic
modeling method for uncertainty treatment (refer Fig. 6). To
this end, a polytopic linear parameter-varying system can
be established (refer Eq. 4), where the state-space matrices
depend affinely on the uncertain parameters.
Ap (ρ) = A0 + µF LA1 + µF RA2 + µRLA3 + µRRA4
Bp (ρ) = B0 + µF LB1 + µF RB2 + µRLB3 + µRRB4
(4)
Definition 1: A polytope of S1, S2, ..., Sk “vertex” systems
could be represented as the convex hull of a fixed number of
matrices Si with the same dimension [40], i.e.,
Co {Si; i ∈ 1, ..., k} :=
( k
X
i=1
λiSi : λi ⩾ 0,
k
X
i=1
λi = 1
)
(5)
The uncertain frictional coefficients are pragmatically as-
sumed to be bounded with µj ∈ [0.1, 1.0] and range over
a fixed polytope (refer Eq. 5) with k = 1, 2, ..., 16 vertices
corresponding to the 24 combinations of extremal parameter
values, thereby encompassing all possible values of the uncer-
tain parameters. The resulting polytopic state-space model can
be written by lumping Eq. 3 to obtain Eq. 6, where eA = Ap,
eB =
Dp
Bp

, eC =
Cp1
Cp2

and eD =
 Dy
By1
06×1
By2

.
""
eA(ρ)
eB(ρ)
eC(ρ)
eD(ρ)
#
|
{z
}
S(ρ)
∈ Co











""
eA1
eB1
eC1
eD1
#
|
{z
}
S1
, ...,
""
eAk
eBk
eCk
eDk
#
|
{z
}
Sk











(6)
Definition 2: A convex subset R of a complex plane is
called an nth order LMI region if there exist a real symmetric
matrix L ∈ Rn×n and a real matrix M ∈ Rn×n [41], which
satisfy the LMI in z and z as depicted in Eq. 7.
R =

z ∈ C : L + Mz + MT z < 0
	
(7)
Lemma 1: A real matrix A ∈ Rn×n is D-stable, that is, all
of its eigenvalues are in the LMI region R defined by Eq. 7,
if and only if there exists a positive-definite symmetric matrix
X ∈ Rn×n, which satisfies the LMI presented in Eq. 8.
L ⊗ X + M ⊗ (AX) + MT ⊗ (AX)T < 0
(8)
We intend to place poles of the closed-loop system Scl in
the LMI region governed by the intersection of left half-plane
with ℜ(z) < α (i.e., α-stability), where α = –0.1, and a conic
sector centered at the origin having an inner angle of ϕ = 3π/4
so as to guarantee some minimum decay rate and closed-loop
damping. The LMI in Eq. 9a represents the α-stability region,
while the one in Eq. 9b represents the conic sector centred at
the origin with an inner angle of ϕ = 2θ.
2αX + AX + (AX)T < 0
(9a)
a
6
3) There exists a single matrix P > 0 such that the
following LMI is satisfied:


P eA(ρ) + eAT (ρ)P
PeB(ρ)
eCT (ρ)
∗
−γ1I
eDT (ρ)
∗
∗
−γ1I

 < 0
(11)
4) There exists P > 0 such that the following system of
LMIs is satisfied for i = 1, 2, ..., k:


P eAi + eAT
i P
PeBi
eCT
i
∗
−γ1I
eDT
i
∗
∗
−γ1I

 < 0
(12)
Definition 4: Energy-to-peak gain is a measure of system’s
response y to disturbances w, quantifying the energy ampli-
fication of disturbances to their peak values as they propagate
through the system to the outputs (i.e., energy of the impulse
response), and forms the generalized H2 norm of the system
with transfer matrix G as depicted in Eq. 13.
∥G∥2
H2 =
1
2π
R ∞
−∞ trace [G∗(jω)G(jω)] dω
Γep = ∥G∥H2 = max
w̸=0
∥y∥L∞
∥w∥L2
(13)
Lemma 3: Following statements are equivalent considering
the polytopic system presented in Eq. 6.
1) The system is stable with a quadratic H2 performance
index γ2
2) Γep < γ2
3) There exist P > 0 and Q > 0 such that the following
conditions are satisfied:
trace
h
eC(ρ)PeCT (ρ)
i
< γ2
2
eA(ρ)P + P eAT (ρ) + eB(ρ)eBT (ρ) < 0
trace
h
eBT (ρ)QeB(ρ)
i
< γ2
2
eAT (ρ)Q + Q eA(ρ) + eCT (ρ)eC(ρ) < 0
(14)
4) There exist P > 0 and Q > 0 such that the following
sets of conditions are satisfied for i = 1, 2, ..., k:
trace
h
eCiPeCT
i
i
< γ2
2
eAiP + P eAT
i + eBi eBT
i < 0
trace
h
eBT
i QeBi
i
< γ2
2
eAT
i Q + Q eAi + eCT
i eCi < 0
(15)
Lemma 4: Given matrices L, B and Q, the inequality
BKL + LT KT BT + Q < 0 has a solution for K if and
only if the conditions presented in Eq. 16 are satisfied.
B⊥QB⊥T < 0
LT ⊥QLT ⊥T
< 0
(16)
It can be shown that applying Eq. 16 to Eq. 11-12, we can
arrive at Eq. 17a-17d and much in the same way, by applying
Eq. 16 to Eq. 14-15, we can obtain Eq. 18a-18d.
Theorem 1: For a given plant of order np, there exists an
H∞ controller of order nc ⩽ np to stabilize the closed-loop
system and guarantees Γee < γ1 if and only if the conditions
presented in Eq. 17a-17d are satisfied.

Bp
By1
⊥ 
ApX + XAp
T + DpDp
T
XCp1
T + DpDy
T
∗
DyDy
T − γ2
1I
 
Bp
By1
⊥T
(17a)

Mp
T
Dz
T
⊥ 
YAp + Ap
T Y + Cp1
T Cp1
YDp + Cp1
T Dy
∗
Dy
T Dy − γ2
1I
 
Mp
T
Dz
T
⊥T
(17b)

X
γ1I
γ1I
Y

⩾ 0
(17c)
rank

X
γ1I
γ1I
Y

⩽ np + nc
(17d)
Remark 1: The constraints imposed by Eq. 17a-17c are
convex, whereas Eq. 17d being rank constraint, is not convex.
Remark 2: For a full-order controller, i.e., nc = np,
constraint imposed by Eq. 17d is satisfied and the resultant
overall problem becomes convex.
Remark 3: For full-state feedback controller, i.e., nc = 0
and Mp = I constraints imposed by Eq. 17b-17d are satisfied
and the resultant overall problem becomes convex.
Theorem 2: For a given plant of order np, there exists an
H2 controller of order nc ⩽ np to stabilize the closed-loop
system and guarantees Γep < γ2 if and only if the conditions
presented in Eq. 18a-18d are satisfied.
(
Bp
⊥ 
ApX + XAp
T + DpDp
T 
Bp
⊥T < 0
Cp2XCp2
T < γ2
2I
(18a)

Mp
T
Dz
T
⊥ 
YAp + Ap
T Y
YDp
∗
−I
 
Mp
T
Dz
T
⊥T
(18b)

X
I
I
Y

⩾ 0
(18c)
rank

X
I
I
Y

⩽ np + nc
(18d)
Remark 4: The constraints imposed by Eq. 18a-18c are
convex, whereas Eq. 18d being rank constraint, is not convex.
Remark 5: For a full-order controller, i.e., nc = np,
constraint imposed by Eq. 18d is satisfied and the resultant
overall problem becomes convex.
Remark 6: For full-state feedback controller, i.e., nc = 0
and Mp = I constraints imposed by Eq. 18b-18d are satisfied
and the resultant overall problem becomes convex.
From the solution {X, Y} to the convex optimization prob-
lem of minimizing trace (X + Y) subject to the inequalities
presented in Eq. 17a and Eq. 18a, we can build the Lyapunov
matrix and obtain the solution for the optimal state-feedback
controller K whose purpose is to minimize the influence of
the disturbance w on the system response y =

y1
y2
T re-
sulting in a closed-loop system Scl with Acl = (Ap + BpK).
That is to say that we obtain an optimal state-feedback control
u = Kxp that:
• Places the poles of Scl within the intersection of LMI
regions R specified by Eq. 9a and Eq. 9b.
• Bounds Γee of Scl from w to y1 below γ1 > 0.
• Bounds Γep of Scl from w to y2 below γ2 > 0.
• Minimizes the H2-H∞ tradeoff criterion of the form
ϕΓ2
ee+φΓ2
ep with ϕ and φ being weights on the respective
performance measures.
Remark 7: With the specific choice of output quantities
y =
y1
y2
T as stated earlier, we can see that the designed
controller penalizes deviation of state xp from zero (for
stabilization) and reference xr (for tracking) along with the
4 control inputs u =
δF L
δF R
δRL
δRR
T so as to
conserve energy by reducing the control effort.
7
Fig. 7. The simulation experiments comprised testing the controller for (a) stabilizing as well as steady-state tracking of standard benchmark maneuvers viz.
(b) lane-change, (c) skidpad, (d) fishhook and (e) slalom tests, against varying friction for individual road-tire interconnects and wind-gust disturbances.
V. RESULTS AND DISCUSSION
The closed-loop system derived in Section IV was found to
be robustly stable over the entire specified parameter range,
since a single Lyapunov function of the form V(xp, α) =
xpT Q−1(α)xp, Q(α) = α1Q1 + ... + αkQk was obtained
such that
d
dtV(xp, α) < 0 for all polytopic decompositions.
We analyzed the performance of the robust optimal con-
troller in simulation as well as real-world settings. The design
of experiments followed a similar approach for both. The
stabilizing controller was tested for a straight-line maneuver.
For the tracking controller, reference generation was achieved
by running the open-loop system with standard test signals viz.
impulse, step, ramp and sine, which resulted in benchmark
maneuvers viz. lane-change, skidpad, fishhook and slalom,
respectively in the absence of any disturbances or parameter
variations. It is worth mentioning that the controllers discussed
in this work are designed to track the vehicle states, which in
turn results in spatiotemporal trajectory tracking.
The simulation experiments comprised testing the controller
for stabilizing as well as steady-state tracking of standard
benchmark maneuvers viz. lane-change, skidpad, fishhook and
slalom, against environmental uncertainties (refer Fig. 7). Par-
ticularly, the friction for individual road-tire interconnects was
simulated as phase-shifted sinusoids, such that all 4 wheels
never experienced the same µ, and wind-gust disturbances
were simulated as delayed step input between t=7.5 to t=22.5
seconds. Apart from the results presented in this section, the
Fig. 8. The real-world experiment comprised testing the controller for steady-
state tracking of a “Figure-8” maneuver, against varying friction for individual
road-tire interconnects and poking disturbances: (a) depicts vehicle states and
trajectory; and (b) visualizes the vehicle trajectory as a time-series snapshot,
with the inset indicating the disturbance Fw and soap-water patches of
different concentrations to vary road friction µ.
controller was tested exhaustively for varying velocities of the
vehicle, with values as high as 10 times the nominal oper-
ating limit. Additionally, we also synthesized and validated a
similar robust optimal controller for nominal full-scale vehicle
parameters. Analysis revealed that the same design framework
(refer Fig. 6) is effective for such extensions.
Upon validating the controller performance in simulation,
we deployed the controller on the physical prototype of Nigel
and validated its performance in a real-world experiment. The
said experiment involved steady-state tracking of “Figure-8”
maneuver, where the state feedbacks were obtained using
the OptiTrack motion capture system (refer Fig. 8). In order
to widen the sim2real gap, the test surface was deliberately
left unclean with dust/sand particles, irregular scratches and
tape residues from previous experiments. Additionally, soap-
water solutions of varying concentrations were spilled in
uneven quantities on the anticipated path, to induce uncertainty
in terms of road friction. Finally, similar to the simulation
experiments, the vehicle was disturbed actively by poking it
with a pole at t=52, t=68, t=81 and t=109 seconds, which
acted as an impulse input.
VI. CONCLUSION
In this work, we first introduced the complete mechatronic
design architecture of Nigel and also presented the detailed
dynamics modeling of this independent 4WD4WS vehicle. We
also formulated a linear parameter-varying polytopic model
of the system for synthesizing a robust optimal controller
using an LMI approach, which seeks to minimize the H2-
H∞ tradeoff criteria while satisfying the closed-loop pole
placement constraints in a Pareto-optimization scheme. We
demonstrated and analyzed robust stabilizing as well as steady-
state tracking control of Nigel with exhaustive simulation-
based verification. The proposed reliable sim2real framework
was also experimentally validated using the 4WD4WS vehicle
platform. In addition to sim2real transfer, we also validated
the controller to handle extra disturbance and uncertainties in
real-world conditions.
Future work will delve into formulating and validating a
robust tracking problem using mixed sensitivity loop-shaping.
Other avenues include fault-tolerant control, and integration of
the proposed framework with a high-level autonomy stack, by
taking advantage of Nigel’s comprehensive sensor suite.
8
REFERENCES
[1] C. de Silva, Mechatronics: An Integrated Approach.
Taylor &
Francis, 2004. [Online]. Available: https://books.google.com/books?id=
CjB2ygeR95cC
[2] C. V. Samak, T. V. Samak, and S. Kandhasamy, “Control Strategies for
Autonomous Vehicles,” in Autonomous Driving and Advanced Driver-
Assistance Systems (ADAS).
CRC Press, 2021, pp. 37–86.
[3] S. Karaman et al., “Project-based, collaborative, algorithmic robotics
for high school students: Programming self-driving race cars at MIT,”
in 2017 IEEE Integrated STEM Education Conference (ISEC), 2017,
pp. 195–203. [Online]. Available: https://mit-racecar.github.io
[4] B. Goldfain, P. Drews, C. You, M. Barulic, O. Velev, P. Tsiotras,
and J. M. Rehg, “AutoRally: An Open Platform for Aggressive
Autonomous Driving,” IEEE Control Systems Magazine, vol. 39, no. 1,
pp. 26–55, 2019. [Online]. Available: https://arxiv.org/abs/1806.00678
[5] M. O’Kelly, V. Sukhil, H. Abbas, J. Harkins, C. Kao, Y. V. Pant,
R. Mangharam, D. Agarwal, M. Behl, P. Burgio, and M. Bertogna.
(2019) F1/10: An Open-Source Autonomous Cyber-Physical Platform.
[Online]. Available: https://arxiv.org/abs/1901.08567
[6] S. S. Srinivasa, P. Lancaster, J. Michalove, M. Schmittle, C. Summers,
M. Rockett, J. R. Smith, S. Choudhury, C. Mavrogiannis, and
F. Sadeghi. (2019) MuSHR: A Low-Cost, Open-Source Robotic
Racecar for Education and Research. [Online]. Available: https:
//arxiv.org/abs/1908.08031
[7] Automatic Control Laboratory, ETH Z¨urich, “ORCA (Optimal RC
Racing) Project,” 2021. [Online]. Available: https://control.ee.ethz.ch/
research/team-projects/autonomous-rc-car-racing.html
[8] T.
K.
et
al.,
“Design
and
Development
of
the
Delft
Scaled
Vehicle:
A
Platform
for
Autonomous
Driving
Tests,”
Delft
University of Technology, Delft, Netherlands, Bachelor’s Thesis,
2017. [Online]. Available: https://www.erwinrietveld.com/assets/docs/
BEP11 DSCS Paper Final.pdf
[9] J. Pappas, C. H. Yuan, C. S. Lu, N. Nassar, A. Miller, S. van Leeuwen,
and F. Borrelli, “Berkeley Autonomous Race Car (BARC),” 2021.
[Online]. Available: https://sites.google.com/site/berkeleybarcproject
[10] HyphaROS Workshop, “HyphaROS Racecar,” 2021. [Online]. Available:
https://github.com/Hypha-ROS/hypharos racecar
[11] Donkey Community, “An Open-Source DIY Self-Driving Platform for
Small-Scale Cars,” 2021. [Online]. Available: https://www.donkeycar.
com
[12] Quanser Consulting Inc., “QCar - A Sensor-Rich Autonomous Vehicle,”
2021. [Online]. Available: https://www.quanser.com/products/qcar
[13] Amazon Web Services, “AWS DeepRacer,” 2021. [Online]. Available:
https://aws.amazon.com/deepracer
[14] Robotis Inc., “TurtleBot3,” 2021. [Online]. Available: https://emanual.
robotis.com/docs/en/platform/turtlebot3/overview/
[15] L. Paull et al., “Duckietown: An Open, Inexpensive and Flexible
Platform for Autonomy Education and Research,” in 2017 IEEE
International Conference on Robotics and Automation (ICRA), 2017,
pp. 1497–1504. [Online]. Available: http://michalcap.net/wp-content/
papercite-data/pdf/paull 2017.pdf
[16] B. Bae and D.-H. Lee, “Design of a Four-Wheel Steering Mobile
Robot Platform and Adaptive Steering Control for Manual Operation,”
Electronics,
vol.
12,
no.
16,
2023.
[Online].
Available:
https:
//www.mdpi.com/2079-9292/12/16/3511
[17] J.
Park
and
Y.
Park,
“Multiple-Actuator
Fault
Isolation
Using
a Minimal L1-Norm Solution with Applications in Overactuated
Electric Vehicles,” Sensors, vol. 22, no. 6, 2022. [Online]. Available:
https://www.mdpi.com/1424-8220/22/6/2144
[18] C. Samak, T. Samak, and V. Krovi, “Towards Mechatronics Approach of
System Design, Verification and Validation for Autonomous Vehicles,”
in 2023 IEEE/ASME International Conference on Advanced Intelligent
Mechatronics (AIM), 2023, pp. 1208–1213.
[19] T. Samak, C. Samak, S. Kandhasamy, V. Krovi, and M. Xie,
“AutoDRIVE: A Comprehensive, Flexible and Integrated Digital Twin
Ecosystem for Autonomous Driving Research & Education,” Robotics,
vol. 12, no. 3, 2023. [Online]. Available: https://www.mdpi.com/
2218-6581/12/3/77
[20] T. V. Samak and C. V. Samak, “AutoDRIVE - Technical Report,” 2022.
[Online]. Available: https://arxiv.org/abs/2211.08475
[21] T. V. Samak, C. V. Samak, and M. Xie, “AutoDRIVE Simulator: A
Simulator for Scaled Autonomous Vehicle Research and Education,”
in 2021 2nd International Conference on Control, Robotics and
Intelligent System, ser. CCRIS’21.
New York, NY, USA: Association
for
Computing
Machinery,
2021,
p.
1–5.
[Online].
Available:
https://doi.org/10.1145/3483845.3483846
[22] T. V. Samak and C. V. Samak, “AutoDRIVE Simulator - Technical
Report,” 2022. [Online]. Available: https://arxiv.org/abs/2211.07022
[23] Z. Zhang, C. Yang, W. Zhang, Y. Xu, Y. Peng, and M. Chi, “Motion
Control of a 4WS4WD Path-Following Vehicle: Dynamics-Based
Steering and Driving Models,” Shock and Vibration, vol. 2021, 2021.
[Online]. Available: https://doi.org/10.1155/2021/8861159
[24] S. Zhu, B. Wei, D. Liu, H. Chen, X. Huang, Y. Zheng, and W. Wei,
“A Dynamics Coordinated Control System for 4WD-4WS Electric
Vehicles,” Electronics, vol. 11, no. 22, 2022. [Online]. Available:
https://www.mdpi.com/2079-9292/11/22/3731
[25] J. R. Kolodziej, “Adaptive Rear-Wheel Steering Control of a Four-
Wheel Vehicle Over Uncertain Terrain,” ser. Dynamic Systems and
Control Conference, vol. 1, 10 2012, pp. 857–866. [Online]. Available:
https://doi.org/10.1115/DSCC2012-MOVIC2012-8603
[26] M. Schwartz, F. Siebenrock, and S. Hohmann, “Model Predictive
Control Allocation of an Over-Actuated Electric Vehicle with Single
Wheel Actuators,” IFAC-PapersOnLine, vol. 52, no. 8, pp. 162–169,
2019, 10th IFAC Symposium on Intelligent Autonomous Vehicles
IAV 2019. [Online]. Available: https://www.sciencedirect.com/science/
article/pii/S2405896319303969
[27] Q. Tan, P. Dai, Z. Zhang, and J. Katupitiya, “MPC and PSO
Based Control Methodology for Path Tracking of 4WS4WD Vehicles,”
Applied Sciences, vol. 8, no. 6, 2018. [Online]. Available: https:
//www.mdpi.com/2076-3417/8/6/1000
[28] J.-E. Moseberg and G. Roppenecker, “Robust Cascade Control for the
Horizontal Motion of a Vehicle with Single-Wheel Actuators,” Vehicle
System Dynamics, vol. 53, no. 12, pp. 1742–1758, 2015. [Online].
Available: https://doi.org/10.1080/00423114.2015.1081954
[29] M. Li, Y. Jia, and T. Lei, “Path Tracking of Varying-Velocity 4WS
Autonomous Vehicles under Tire Force Friction Ellipse Constraints,”
Robotics and Autonomous Systems, vol. 173, p. 104621, 2024.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S0921889024000046
[30] J. Ackermann and W. Sienel, “Robust Yaw Damping of Cars with
Front and Rear Wheel Steering,” IEEE Transactions on Control Systems
Technology, vol. 1, no. 1, pp. 15–20, 1993.
[31] H. Zhang, X. Zhang, and J. Wang, “Robust Gain-Scheduling Energy-
to-Peak Control of Vehicle Lateral Dynamics Stabilisation,” Vehicle
System Dynamics, vol. 52, no. 3, pp. 309–340, 2014. [Online].
Available: https://doi.org/10.1080/00423114.2013.879190
[32] S. Cheng et al., “Robust LMI-Based H-Infinite Controller Integrating
AFS and DYC of Autonomous Vehicles With Parametric Uncertainties,”
IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 51,
no. 11, pp. 6901–6910, 2021.
[33] J. P. Redondo, B. L. Boada, and V. D´ıaz, “LMI-Based H-Infinity
Controller of Vehicle Roll Stability Control Systems with Input and
Output Delays,” Sensors, vol. 21, no. 23, 2021. [Online]. Available:
https://www.mdpi.com/1424-8220/21/23/7850
[34] A. Y. Babawuro, N. M. Tahir, M. Muhammed, and A. U. Sambo,
“Optimized State Feedback Control of Quarter Car Active Suspension
System Based on LMI Algorithm,” Journal of Physics: Conference
Series, vol. 1502, no. 1, p. 012019, Mar 2020. [Online]. Available:
https://dx.doi.org/10.1088/1742-6596/1502/1/012019
[35] Y.-e. Mao, Y. Zheng, Y. Jing, G. M. Dimirovski, and S. Hang, “An LMI
Approach to Slip Ratio Control of Vehicle Antilock Braking Systems,”
in 2009 American Control Conference, 2009, pp. 3350–3354.
[36] M. Schwartz, T. Rudolf, and S. Hohmann, “Robust Position and Velocity
Tracking Control of a Four-wheel Drive and Four-wheel Steered Electric
Vehicle,” in 2020 6th International Conference on Control, Automation
and Robotics (ICCAR), 2020, pp. 415–422.
[37] C. Samak, T. Samak, and V. Krovi, “Towards Sim2Real Transfer
of
Autonomy
Algorithms
using
AutoDRIVE
Ecosystem,”
IFAC-
PapersOnLine, vol. 56, no. 3, pp. 277–282, 2023, 3rd Modeling,
Estimation and Control Conference MECC 2023. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S2405896323023704
[38] W. Milliken and D. Milliken, Race Car Vehicle Dynamics, ser.
Premiere
Series.
SAE
International,
1995.
[Online].
Available:
https://books.google.com/books?id=EOHPjgEACAAJ
[39] R.
Rajamani,
Vehicle
Dynamics
and
Control,
ser.
Mechanical
Engineering Series.
Springer US, 2011. [Online]. Available: https:
//books.google.com/books?id=q6SJcgAACAAJ
[40] P. Apkarian, P. Gahinet, and G. Becker, “Self-Scheduled H-Infinity
Control of Linear Parameter-Varying Systems: A Design Example,”
Automatica, vol. 31, no. 9, pp. 1251–1261, 1995. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/000510989500038X
[41] Y. Li, “Robust Control-LMI Method,” 2002.
9
Chinmay Samak (Student Member, IEEE) received
the B.Tech. degree in mechatronics engineering from
SRM Institure of Science and Technology with a
gold medal in 2021. He is currently pursuing the
direct Ph.D. degree at Clemson University Interna-
tional Center of Automotive Research (CU-ICAR)
where he is working at the ARMLab. His research
interests lie at the intersection of physics-informed
and data-driven methods to bridge the sim2real gap
using autonomy-oriented digital twins.
Tanmay Samak (Student Member, IEEE) received
the B.Tech. degree in mechatronics engineering from
SRM Institure of Science and Technology with a
silver medal in 2021. He is currently pursuing the
direct Ph.D. degree at Clemson University Interna-
tional Center of Automotive Research (CU-ICAR)
where he is working at the ARMLab. His research
interests lie at the intersection of real and virtual
worlds to bridge the real2sim gap by creating phys-
ically and graphically accurate digital twins.
Javad Velni (Senior Member, IEEE) received the
Ph.D. degree in mechanical engineering from Uni-
versity Houston in 2008. He is a professor with
the Department of Mechanical Engineering at Clem-
son University, where he also directs the Velni
Lab. His research interests include secure control
of cyber–physical systems and, in particular, trans-
portation systems, coverage control of heterogeneous
multi-agent systems, and learning-based control of
complex distributed systems.
Venkat Krovi (Senior Member, IEEE) received the
Ph.D. degree in mechanical engineering and applied
mechanics from University of Pennsylvania in 1998.
He is the Michelin Endowed Chair Professor of
Vehicle Automation with the Departments of Au-
tomotive and Mechanical Engineering at Clemson
University, where he also directs the ARMLab. The
underlying theme of his research has been to take
advantage of “power of the many” (both autonomous
agents and humans) to extend the reach of human
users in dull, dirty, and dangerous environments.
","nanOther works such as [16] and [17] have recently prototyped over-actuated scaled vehicles with the primary aim of validating their control algorithms, and as such, none of these platforms are open-sourced. Additionally, both of these platforms lack comprehensive autonomy features and only the latter offers truly independent driving and steering capability, although at a much larger scale (1:5) and without extended steering limits. To the best of authors’ knowledge, Nigel (refer Fig. 1) is the first open-source1 mechatronically redundant [18] autonomous vehicle platform offering comprehensive autonomy features as well as independent all-wheel driving and independent all-wheel steering (i.e., independent 4WD4WS) configuration with extended (±90°) steering angles, within a small footprint of 1:14 scale. Additionally, Nigel is a part of the larger AutoDRIVE Ecosystem2 [19], [20], which also offers a high-fidelity digital-twin simulation platform [21], [22], as well as flexible application programming interfaces (APIs) to develop low and high-level autonomy algorithms."
"We present a high-fidelity 3D reconstruction method for deformable endoscopic tissues, EndoGS, based on Gaussian Splatting. Our method utilizes a dynamic variant of 3D Gaussian Splatting to represent deformable scenes with a time-dependent neural displacement field. We employ stereo depth maps and tool masks to optimize the Gaussians, mitigating issues caused by occlusions. We evaluate our method on robotic surgery videos, demonstrating superior rendering quality and faster rendering speed compared to existing methods.","Endoscopic surgeries are minimally invasive procedures that offer numerous advantages over traditional open surgeries. However, they require specialized skills and training due to the limited visibility and reduced dexterity of the surgical instruments. To address these challenges, researchers have explored the use of computer vision and machine learning techniques to assist surgeons during endoscopic procedures.","Our EndoGS method follows the basic design of DRF methods, but employs a Gaussian Splatting representation for the 3D Gaussians. Gaussian Splatting offers several advantages, including real-time rendering capabilities and the ability to capture fine details in the scene. We introduce a deformable variant of Gaussian Splatting to represent the dynamic nature of surgical scenes, using a lightweight MLP to represent the temporal deformations.","We evaluate EndoGS on a dataset of DaVinci robotic surgery videos, comparing it against two competitive methods: EndoNeRF and ForPlane. Our method demonstrates superior performance in terms of rendering quality, as measured by PSNR, SSIM, and LPIPS metrics. Furthermore, EndoGS achieves real-time rendering speeds, outperforming the other methods in terms of FPS.","We presented EndoGS, a Gaussian Splatting-based method for reconstructing deformable endoscopic tissues. Our method demonstrates superior rendering quality and real-time rendering speed compared to existing approaches. However, limitations remain due to the ill-posed nature of 3D reconstruction from single-viewpoint videos and the lack of 3D cues and surface constraints. Future directions include exploring practical endoscopic reconstruction with surface-aligned Gaussian Splatting and regularization techniques to improve the accuracy and robustness of the reconstruction.",Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting,"Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu","Deformable Endoscopic Tissues Reconstruction
with Gaussian Splatting
Lingting Zhu1, Zhao Wang2, Zhenchao Jin1, Guying Lin1, and Lequan Yu1(2
L. Zhu et al.
potentials in 3D representations, and methods based on variants of dynamic ra-
diance filed [5, 18] have become representative works in deformable tissues re-
construction from videos. For example, EndoNeRF [25] follows the modeling of
D-NeRF [18] to represent deformable surgical scenes as the combination of a
canonical neural radiance field and a time-dependent neural displacement field,
and LerPlane [28] factorizes six 2D planes for static field and dynamic field
to accelerate optimization similar to [2, 5]. There is also a growing interest to
reconstruct surfaces from endoscopic videos via neural implicit fields [1,31].
Recently 3D Gaussian Splatting (3D-GS) [8] has been witnessed as a powerful
representation that renders higher-quality results at a real-time level. Follow-
up works [14,26,29,30] extend 3D-GS to represent dynamic scenes and achieve
state-of-the-art performances on rendering fidelity and speed. To model dynamic
representation of dynamic scenes, [14, 29] formulate 4D Gaussians and assign
extra parameters as attributes in the time dimension, and [26,30] apply MLPs
to predict the deformation of the Gaussians, sharing the same rationale with
dynamic NeRFs [17,18]. In this paper, we present EndoGS, a method based on
Gaussian Splatting for high-fidelity rendering of deformable endoscopic tissues
reconstruction with better rendering quality and better rendering speed.
To summarize, our main contributions are three-fold: 1) We present a novel
Gaussian Splatting based method for deformable endoscopic tissues reconstruc-
tion. This is one of the first attempts [10] introducing Gaussian Splatting in the
medical domain. 2) We represent dynamic surgical scenes with the combina-
tion of static Gaussians and the deformable parameters in the time dimension,
adopt depth-guided supervision for monocular optimization, and apply a spatial-
temporal weight mask to mitigate tool occlusion. Besides, we also introduce total
variation items to mitigate quality degradation in the spatial domain as well as
in the temporal domain. 3) While former works [25, 27, 28] apply tool masks
to filter out un-desired parts and evaluate the rendered visible pixels with the
ground-truth, different training masks are adopted for metrics evaluation. To
this end, we use the same input masks involved in the training and inference for
comparison methods and make a clear and fair comparison on rendered video
quality.
2
Method
2.1
Overview
In this paper, we introduce our method, referred to as EndoGS, which utilizes
a deformable variant of 3D-GS to reconstruct 3D surgical scenes from a single-
viewpoint video, estimated depth maps, and labeled tool masks. Specifically,
given a stereo video with left and right frames {(Il
i, Ir
i )}T
i=1, where T is the total
number of frames, our goal is to reconstruct 3D representations of the deformable
tissues that render in high quality. We follow the approach of [25,27,28] by com-
bining the extracted binary tool masks {Mi}T
i=1 and the depth maps {Di}T
i=1
estimated from binocular captures for the left views. The pipeline of EndoGS
is illustrated in Fig. 1. In this section, we first introduce the preliminary of 3D
EndoGS
3
(𝝁, 𝑡)
𝑅𝑎𝑠𝑡𝑒𝑟𝑖𝑧𝑎𝑡𝑖𝑜𝑛
3D Gaussians
Hexplane
MLP
Deformation
Rendered Images
& Depth Maps
GT Images, Depth Maps,
& Tool Masks
Supervision
Fig. 1. The overview of our EndoGS pipeline. Given 3D Gaussians, we use the
mean and the time as input to compute features by querying multi-resolution voxel
planes. A single MLP is used to obtain the deformation of the Gaussians. With differ-
entiable rasterization, the rendered images and depth maps are obtained and we use
ground truth images, depth maps and the tool masks to provide the supervision.
Gaussian Splatting (Section 2.2). We then present the modeling of deformable
tissues with a dynamic version of 3D-GS, which adopts a lightweight MLP to
represent the dynamic field (Section 2.3). Finally, the training optimization of
the Gaussian Splatting with the tool masks and the depth maps is introduced
(Section 2.4).
2.2
Preliminaries of 3D Gaussian Splatting
3D Gaussian Splatting (3D-GS) [8] offers the state-of-the-art solution for real-
time novel view synthesis in multi-view static scenes. The 3D Gaussians with a
3D covariance matrix Σ and mean µ served as the rendering primitives in the
form of point clouds:
G(x) = e− 1
2 (x−µ)T Σ−1(x−µ).
(1)
The 3D Gaussians can be projected onto 2D space and rendered for pixels [34]:
Σ′ = JWΣWT JT ,
(2)
where Σ′ is the covariance matrix in the 2D plane, W denotes view transfor-
mation, and the Jacobian J is the affine approximation of the projective trans-
formation. To enforce the positive semi-definiteness, Σ is parameterized with a
scale S and rotation R:
Σ = RSST RT .
(3)
4
L. Zhu et al.
To render the color of the pixels p, point-based volume rendering is adopted:
C(p) =
X
i∈N
ciαi
i−1
Y
j=1
(1 − αj),
(4)
where αi = σie− 1
2 (p−µi)T Σ′−1(p−µi).
(5)
The ci means the color of the Gaussians along the ray, µi denotes the coordinates,
and σi denotes the opacity. And 3D-GS applies spherical harmonics [19] to model
the view-dependent color. In total, the explicit 3D Gaussians are characterized
by: position µ ∈ R3, scaling factor s ∈ R3, rotation factor r ∈ R4, spherical
harmonic (SH) coefficients sh ∈ Rk (k means number of SH functions), and
opacity σ ∈ R. Finally each Gaussian can be represented as (µ, s, r, sh, σ).
2.3
Gaussian Splatting Representations for Deformable Tissues
We represent a surgical scene as a 4D volume, where the deformation of the
tissues involves time time dimension. To this end, we use introduce the Gaus-
sian deformation to represent the time-varying motions and shapes, following
the basic designs of [26]. The final goal is to learn the original representa-
tion of the 3D Gaussians {(µ, s, r, sh, σ)} as well as the Gaussian deformations
{∆(µ, s, r, sh, σ)} = {(∆µ, ∆s, ∆r, ∆sh, ∆σ)}.
In Fig. 1, for each 3D Gaussian, we use the mean µ = (x, y, z) and the time t
to compute the deformation. We use six orthogonal feature planes to encode the
spatial and temporal information [2,5,26–28]. To be specific, the multi-resolution
HexPlane [2,5] consists of three space planes XY, XZ, Y Z and three space-time
planes XT, Y T, ZT. The planes encode features F ∈ Rh×N1×N2, where h denotes
the hidden dimension and N1, N2 stand for the plane resolution, and we utilize
bilinear interpolation B to interpolate the four nearby queried voxel features. As
a result, the voxel feature can be represented in the format of matrix element-
wise multiplication with operation ⊙:
fvoxel(µ, t) = B (FXY , x, y) ⊙ B (FY Z, y, z) . . . B (FY T , y, τ) ⊙ B (FZT , z, τ) ,
Then we employ a single MLP to update the Gaussian attributes and it
merges all the information and decodes the deformation of the position, scaling
factor, rotation factor, spherical harmonic coefficients, and opacity:
∆(µ, s, r, sh, σ) = MLP(fvoxel(µ, t)).
2.4
Training Combined with Tool Masks and Depth Maps
Reconstructing from videos with tool occlusion poses a challenge and we follow
former works [25, 27, 28] to use labeled tool occlusion masks to indicates the
unseen pixels. Furthermore, we leverage spatiotemporal importance sampling
strategy to indicate the crucial areas related to the occlusion issue. Since we
EndoGS
5
optimize the Gaussians with spatial targets, we can simply devise weight masks
in the spatial domain to serve the optimization losses. To be specific, the binary
masks are denoted as {Mi}T
i=1, where 0 stands for tissue pixels and 1 stands for
tool pixels, and the importance maps {Vi}T
i=1 involve temporal statistics and
are denoted as
Vi = (1 − Mi) ⊙

1 + α
T
X
j=1
Mj/

T
X
j=1
Mj

F

 .
We only optimize in the seen part by introducing the item 1 − Mi. Meanwhile,
the statistics of the mask frequencies normalized by the Frobenius norm along
the temporal dimension provide the information of the uncertainty and allocate
higher importance for tissue areas with higher occlusion frequencies. The param-
eter α is used to control the scaling strength. We use the L1 reconstruction loss
with spatial masks, and thus the spatial supervision on i-th is:
LL1(i) = |Ii ⊙ Vi − ˆIi ⊙ Vi|,
where ˆIi is the rendering image on i-th frame.
Monocular reconstruction provides limited information for 3D reconstruc-
tion and makes overfitting happen with single-viewpoint images [4]. To mitigate
illness from single-viewpoint inputs, we introduce depth-guided loss with the es-
timated depth maps. The coarse stereo depth maps {Di}T
i=1 are obtained via
STTR-light [11]. We adopt Huber loss for depth regularization following [28]:
LD(i) =
(
0.5∆D2
i ,
if |∆Di| < δ
δ(∆Di − 0.5δ),
otherwise
where ∆Di = |Di − ˆDi| denotes the depth loss at i-th frame, and ˆDi is the
rendering depth.
We adopt total variation (TV) losses in the spatial dimension and the tempo-
ral dimension to serve as the additional regularization. To prevent color drifts in
unseen area during Gaussians optimization, we use a spatial total variation item
for areas with tool masks and it is denoted as Ltv−spatial(i) = TV( ˆIi ⊙ Mi).
And we use the temporal total variation item Ltv−temporal in [26] to regularize
the Hexplane optimization. Our final optimization target at i-th frame is:
L(i) = LL1(i) + λDLD(i) + λT V 1Ltv−spatial(i) + λT V 2Ltv−temporal,
where hyperparameters λD, λT V 1, and λT V 2 control the regularization strength.
3
Experiments
3.1
Datasets and Evaluation Metrics
We evaluate our proposed method on the dataset from [25]: typical robotic
surgery videos from 6 cases of DaVinci robotic data. The datasets are designed
6
L. Zhu et al.
Table 1. Quantitative comparison on rendering quality and speed.
Methods
PSNR ↑ SSIM ↑ LPIPS ↓ FPS↑
EndoNeRF [25]
35.624
0.942
0.064
< 0.2
ForPlane [27]
36.457
0.946
0.058
∼ 1.7
EndoGS (Ours) 37.654
0.965
0.036
∼ 40
to capture challenging surgical scenes with non-rigid deformation and tool occlu-
sion. We use standard image quality metrics, including PSNR, SSIM, and LPIPS.
Since in evaluation the groundtruth pixels for unseen areas are missing, the tool
masks are used to exclude unseen parts for computation and unlike [25,27,28],
we do not count those pixels in PSNR. We also report the frame per second
(FPS) to compare the rendering speed of the methods. Besides, while former
works [25, 27, 28] adopt the different tool mask configurations in training and
evaluation or compare methods under different configurations, we argue to train
and evaluate in the same tool mask configurations to prevent meaningless pixels
comparison, and compare methods the same tool occlusion masks.
3.2
Implementation Details
In our approach, we adopt the two-stage training methodology in [26] to model
the static and deformation fields. In the first stage, we train 3D Gaussian models
for the static field, while in the second stage, we focus on training the deformation
field. We conduct 3,000 and 60,000 iterations for the first and second stages,
respectively. The initial point clouds are estimated using COLMAP [21]. The
importance maps scaling strength α is set to 30 and δ = 0.2 for depth loss.
Hyperparameters λD, λT V 1, and λT V 2 are set to 0.03, 0.01, and 1.0.
3.3
Results
We compare EndoGS against two methods, i.e., EndorNerf [25] and ForPlane [27]
(an updated version of LerPlane [28]), due to their competitive rendering quality.
ForPlane is trained for 32k iterations. We use the same masks in training and
evaluation for three methods and evaluate the rendering results on the same
cropped zone where the lowest part of the videos that contain display patterns
are removed.
Fig. 2 presents a qualitative comparison on scene ""traction"" between En-
doGS and competitive baselines. On the rendering quality, EndoGS clearly out-
performs other methods. Tissues deformation occurs at different timesteps, and
our method better reconstructs the deformation along the time. Table 1 presents
a quantitative comparison on the 6 videos, showcasing the superior performances
of our method over the baseline methods in terms of various evaluation metrics
related to rendering quality and speed. Considering that EndoNeRF exhibits
poor performance on the first rendering frame, we exclude the first frames from
EndoGS
7
t = 1.07s
Reference
ForPlane
Ours
EndoNeRF
t = 2.33s
t = 4.40s
Fig. 2. Qualitative results on scene “traction” at different timesteps.
the evaluation process for all three methods. In terms of rendering quality, En-
doGS demonstrates a significant advantage, outperforming the other methods
by a considerable margin. Furthermore, EndoGS benefits from the rendering ef-
ficiency of Gaussian Splatting, enabling it to achieve real-time rendering speeds.
In contrast, the baseline methods struggle to maintain a high FPS rate, high-
lighting the superiority of EndoGS in this regard.
Ablation Analysis. Fig. 3 presents the ablation study on depth regularization.
While we optimize the Gaussians from a single-viewpoint video, 3D information
is missing and it is likely the representations are overfitted. We leverage esti-
mated depth maps to help mitigate the illness. It is shown that without depth
regularization or with a smaller weight of the regularization, the rendered depth
maps are strayed away from the depth guidance. In Fig. 4, we show the effec-
tiveness of spatial Total Variation loss. While we optimize the Gaussians on seen
pixels in frames with the tool masks, the lack of continuity leads to color drift
in rendering results (frame w/o spatial TV). This phenomenon can be mitigated
during the optimization with spatial TV loss.
4
Conclusion
We present a method based on Gaussian Splatting for deformable endoscopic tis-
sues reconstruction, rendering high-quality deformable tissues in real-time from
a single-viewpoint video, estimated depth maps, and labeled tool masks. Exper-
iments on DaVinci robotic surgery videos show higher rendering quality.
Limitations and Future Works. Our work has limitations in two folds. First,
3D reconstruction from single-viewpoints videos is an ill-posed problem, making
8
L. Zhu et al.
Reference
Rendered Frame (𝝀𝑫)
Estimated Depth Map
Rendered Depth (0)
Rendered Depth (𝟎. 𝟏𝝀𝑫)
Rendered Depth (𝝀𝑫)
Fig. 3. Ablation on depth regularization. We show rendering depth maps with
different depth regularization strength on scene ""pulling soft tissues"". The values in
the bracket denote the weight of the depth regularization loss.
Reference
Tool Mask
w/ Spatial TV
w/o Spatial TV
Fig. 4. Ablation on spatial TV loss. We show rendering frames w/ and w/o spatial
TV loss on scene ""cutting tissues twice"".
it infeasible for surgical downstream applications. Second, due to the lack of 3D
cues and surface constrains in single-viewpoint videos for Gaussian Splatting
optimization, it is likely that artifacts and ambiguities happen in 3D reconstruc-
tion with novel viewpoints. Accordingly, future works can focus on practical
endoscopic reconstruction with surface-aligned Gaussian Splatting. 3D tissues
reconstruction with more surgical cameras is needed to facilitate realistic down-
stream tasks. Additionally, regularization that encourages the Gaussians to well
distributed over the surface can mitigate weaknesses on reconstructed scene sur-
face and geometry, as suggested by [7].
Acknowledgement
We thank Med-AIR Lab CUHK for DaVinci robotic prostatectomy data.
EndoGS
9
References
1. Batlle, V.M., Montiel, J.M., Fua, P., Tardós, J.D.: Lightneus: Neural surface re-
construction in endoscopy using illumination decline. In: International Conference
on Medical Image Computing and Computer-Assisted Intervention. pp. 502–512.
Springer (2023)
2. Cao, A., Johnson, J.: Hexplane: A fast representation for dynamic scenes. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion. pp. 130–141 (2023)
3. Chen, L., Tang, W., John, N.W., Wan, T.R., Zhang, J.J.: Slam-based dense sur-
face reconstruction in monocular minimally invasive surgery and its application to
augmented reality. Computer methods and programs in biomedicine 158, 135–146
(2018)
4. Chung, J., Oh, J., Lee, K.M.: Depth-regularized optimization for 3d gaussian splat-
ting in few-shot images. arXiv preprint arXiv:2311.13398 (2023)
5. Fridovich-Keil, S., Meanti, G., Warburg, F.R., Recht, B., Kanazawa, A.: K-planes:
Explicit radiance fields in space, time, and appearance. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12479–
12488 (2023)
6. Gao, W., Tedrake, R.: Surfelwarp: Efficient non-volumetric single view dynamic
reconstruction. arXiv preprint arXiv:1904.13073 (2019)
7. Guédon, A., Lepetit, V.: Sugar: Surface-aligned gaussian splatting for effi-
cient 3d mesh reconstruction and high-quality mesh rendering. arXiv preprint
arXiv:2311.12775 (2023)
8. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for
real-time radiance field rendering. ACM Transactions on Graphics 42(4) (2023)
9. Li, Y., Richter, F., Lu, J., Funk, E.K., Orosco, R.K., Zhu, J., Yip, M.C.: Super:
A surgical perception framework for endoscopic tissue manipulation with surgical
robotics. IEEE Robotics and Automation Letters 5(2), 2294–2301 (2020)
10. Li, Y., Fu, X., Zhao, S., Jin, R., Zhou, S.K.: Sparse-view ct reconstruction with 3d
gaussian volumetric representation. arXiv preprint arXiv:2312.15676 (2023)
11. Li, Z., Liu, X., Drenkow, N., Ding, A., Creighton, F.X., Taylor, R.H., Unberath,
M.: Revisiting stereo depth estimation from a sequence-to-sequence perspective
with transformers. In: Proceedings of the IEEE/CVF international conference on
computer vision. pp. 6197–6206 (2021)
12. Long, Y., Li, Z., Yee, C.H., Ng, C.F., Taylor, R.H., Unberath, M., Dou, Q.: E-
dssr: efficient dynamic surgical scene reconstruction with transformer-based stereo-
scopic depth perception. In: Medical Image Computing and Computer Assisted
Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France,
September 27–October 1, 2021, Proceedings, Part IV 24. pp. 415–425. Springer
(2021)
13. Lu, J., Jayakumari, A., Richter, F., Li, Y., Yip, M.C.: Super deep: A surgical per-
ception framework for robotic tissue manipulation using deep learning for feature
extraction. In: 2021 IEEE International Conference on Robotics and Automation
(ICRA). pp. 4783–4789. IEEE (2021)
14. Luiten, J., Kopanas, G., Leibe, B., Ramanan, D.: Dynamic 3d gaussians: Tracking
by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713 (2023)
15. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Commu-
nications of the ACM 65(1), 99–106 (2021)
10
L. Zhu et al.
16. Newcombe, R.A., Fox, D., Seitz, S.M.: Dynamicfusion: Reconstruction and track-
ing of non-rigid scenes in real-time. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 343–352 (2015)
17. Park, K., Sinha, U., Barron, J.T., Bouaziz, S., Goldman, D.B., Seitz, S.M., Martin-
Brualla, R.: Nerfies: Deformable neural radiance fields. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 5865–5874 (2021)
18. Pumarola, A., Corona, E., Pons-Moll, G., Moreno-Noguer, F.: D-nerf: Neural ra-
diance fields for dynamic scenes. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 10318–10327 (2021)
19. Ramamoorthi, R., Hanrahan, P.: An efficient representation for irradiance environ-
ment maps. In: Proceedings of the 28th annual conference on Computer graphics
and interactive techniques. pp. 497–500 (2001)
20. Recasens, D., Lamarca, J., Fácil, J.M., Montiel, J., Civera, J.: Endo-depth-and-
motion: Reconstruction and tracking in endoscopic videos using depth networks
and photometric constraints. IEEE Robotics and Automation Letters 6(4), 7225–
7232 (2021)
21. Schonberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Proceedings
of the IEEE conference on computer vision and pattern recognition. pp. 4104–4113
(2016)
22. Scott, D.J., Cendan, J.C., Pugh, C.M., Minter, R.M., Dunnington, G.L., Kozar,
R.A.: The changing face of surgical education: simulation as the new paradigm.
Journal of Surgical Research 147(2), 189–193 (2008)
23. Shin, C., Ferguson, P.W., Pedram, S.A., Ma, J., Dutson, E.P., Rosen, J.: Au-
tonomous tissue manipulation via surgical robot using learning based model predic-
tive control. In: 2019 International conference on robotics and automation (ICRA).
pp. 3875–3881. IEEE (2019)
24. Song, J., Wang, J., Zhao, L., Huang, S., Dissanayake, G.: Dynamic reconstruction of
deformable soft-tissue with stereo scope in minimal invasive surgery. IEEE Robotics
and Automation Letters 3(1), 155–162 (2017)
25. Wang, Y., Long, Y., Fan, S.H., Dou, Q.: Neural rendering for stereo 3d recon-
struction of deformable tissues in robotic surgery. In: International Conference
on Medical Image Computing and Computer-Assisted Intervention. pp. 431–441.
Springer (2022)
26. Wu, G., Yi, T., Fang, J., Xie, L., Zhang, X., Wei, W., Liu, W., Tian, Q., Wang,
X.: 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint
arXiv:2310.08528 (2023)
27. Yang, C., Wang, K., Wang, Y., Dou, Q., Yang, X., Shen, W.: Efficient deformable
tissue reconstruction via orthogonal neural plane. arXiv preprint arXiv:2312.15253
(2023)
28. Yang, C., Wang, K., Wang, Y., Yang, X., Shen, W.: Neural lerplane representations
for fast 4d reconstruction of deformable tissues. arXiv preprint arXiv:2305.19906
(2023)
29. Yang, Z., Yang, H., Pan, Z., Zhu, X., Zhang, L.: Real-time photorealistic dynamic
scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv
2310.10642 (2023)
30. Yang, Z., Gao, X., Zhou, W., Jiao, S., Zhang, Y., Jin, X.: Deformable 3d gaus-
sians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint
arXiv:2309.13101 (2023)
31. Zha, R., Cheng, X., Li, H., Harandi, M., Ge, Z.: Endosurf: Neural surface re-
construction of deformable tissues with stereo endoscope videos. In: International
EndoGS
11
Conference on Medical Image Computing and Computer-Assisted Intervention. pp.
13–23. Springer (2023)
32. Zhou, H., Jagadeesan, J.: Real-time dense reconstruction of tissue surface from
stereo optical video. IEEE transactions on medical imaging 39(2), 400–412 (2019)
33. Zhou, H., Jayender, J.: Emdq-slam: Real-time high-resolution reconstruction of
soft tissue surface from stereo laparoscopy videos. In: Medical Image Computing
and Computer Assisted Intervention–MICCAI 2021: 24th International Confer-
ence, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part IV 24.
pp. 331–340. Springer (2021)
34. Zwicker, M., Pfister, H., Van Baar, J., Gross, M.: Surface splatting. In: Proceedings
of the 28th annual conference on Computer graphics and interactive techniques.
pp. 371–378 (2001)
","Various methods have been proposed for reconstructing 3D representations of deformable tissues from endoscopic videos. A common approach is to utilize dynamic radiance fields (DRF), which represent the scene as a continuous function that maps input views to output colors and densities. DRF methods such as EndoNeRF and LerPlane have shown promising results in reconstructing deformable surgical scenes.nan"
"Cloud deep learning platforms provide cost-effective deep neural network (DNN) training for customers who lack computation resources. However, cloud systems are often untrustworthy and vulnerable to attackers, leading to growing concerns about model privacy. In this paper, we propose Tempo, the first distributed cloud DNN training system that utilizes the computational resources of GPUs to assist in training while preserving model confidentiality and leveraging TEE for security. Tempo employs a customized permutation-based algorithm to blind both inputs and model parameters, ensuring data privacy and integrity. We implement Tempo and evaluate its performance and privacy using two DNNs. Tempo outperforms baselines and offers sufficient privacy protection.","Cloud deep learning (DL) platforms, such as Google Cloud Platform and Microsoft Azure, provide cost-effective services for training deep neural networks (DNNs). However, the security and privacy of cloud-based DL training have become major concerns due to the untrustworthy nature of cloud systems and the growing number of data breaches. To address these concerns, researchers have explored the use of trusted execution environments (TEEs) to protect data and model privacy during cloud-based DL training. However, existing TEE-based solutions often lack the computational resources to efficiently train large DNNs, leading to impractical training overheads. To overcome this limitation, we propose Tempo, a distributed cloud DL training system that leverages both the security guarantees of TEEs and the computational power of graphics processing units (GPUs) for improved efficiency and privacy protection during training.","Tempo consists of a master node equipped with a TEE and multiple worker nodes equipped with GPUs. The master node handles task management and result verification, while the worker nodes perform the computationally intensive linear operations, accelerated by GPUs, in a privacy-preserving manner. To protect both input data and model parameters during training, we propose a novel matrix multiplication obfuscation algorithm that blinds operands using permutations and cryptographic keys generated within the TEE. This algorithm minimizes the number of encryption operations, reducing the overhead of training, and ensures the integrity of offloaded computations. By utilizing distributed GPUs and employing techniques such as data parallelism, tensor model parallelism, and pipeline parallelism, Tempo achieves efficient and scalable training of large DNNs.","We evaluate Tempo using two prominent DNN architectures, ResNet and Vision Transformer, under diverse configurations. Tempo demonstrates exceptional performance, outperforming pure TEE baselines by an average of 4.37× with local setups and 3.95× with network setups, without compromising model accuracy. Furthermore, Tempo exhibits robust privacy protection against model theft attacks, demonstrating its effectiveness in safeguarding model confidentiality. These results highlight Tempo's potential to revolutionize privacy-preserving cloud DL training.","In conclusion, we present Tempo, the first distributed cloud DL training system that achieves both input and model privacy while harnessing the power of GPUs for efficient training. Tempo introduces a novel matrix multiplication obfuscation algorithm that ensures the integrity and confidentiality of offloaded computations. Extensive evaluations demonstrate that Tempo significantly outperforms existing approaches, offering a compelling solution for secure and efficient cloud-based DL training.",Tempo: Confidentiality Preservation in Cloud-Based Neural Network Training,"Rongwu Xu, Zhixuan Fang","Tempo: Confidentiality Preservation in Cloud-Based
Neural Network Training
Rongwu Xu and Zhixuan Fang
IIIS, Tsinghua University
xrw22@mails.tsinghua.edu.cn, zfang@mail.tsinghua.edu.cn
Abstract—Cloud deep learning platforms provide cost-effective
deep neural network (DNN) training for customers who lack
computation resources. However, cloud systems are often un-
trustworthy and vulnerable to attackers, leading to growing
concerns about model privacy. Recently, researchers have sought
to protect data privacy in deep learning by leveraging CPU
trusted execution environments (TEEs), which minimize the use
of cryptography, but existing works failed to simultaneously
utilize the computational resources of GPUs to assist in training
and prevent model leakage. This paper presents Tempo, the
first cloud-based deep learning system that cooperates with TEE
and distributed GPUs for efficient DNN training with model
confidentiality preserved. To tackle the challenge of preserving
privacy while offloading linear algebraic operations from TEE to
GPUs for efficient batch computation, we introduce a customized
permutation-based obfuscation algorithm to blind both inputs
and model parameters. An optimization mechanism that reduces
encryption operations is proposed for faster weight updates
during backpropagation to speed up training. We implement
Tempo and evaluate it with both training and inference for
two prevalent DNNs. Empirical results indicate that Tempo
outperforms baselines and offers sufficient privacy protection.
I. INTRODUCTION
With the rapid advancement of deep learning (DL) [1]
and cloud computing services [2], [3], clients are motivated
to outsource computationally intensive DL tasks to high-
performance cloud platforms, especially when local devices
lack sufficient resources. This trending “Cloud DL” approach,
a.k.a., Machine Learning as a Service (MLaaS) [4], is ac-
tively supported by major cloud service providers (CSPs) like
Google Cloud [2] and Microsoft Azure [3]. Despite its con-
venience, the MLaaS paradigm inevitably introduces concerns
regarding security and data privacy [5], [6], as CSPs can be
untrustworthy, self-interested, and possibly malicious [7].
Motivation. Consider a motivating example, where an anima-
tion company wishes to employ its own dataset to fine-tune
a pre-trained model provided by OpenAI for AIGC [8] tasks.
To manage costs, the company expects to train the model on
a CSP. Unfortunately, direct outsourcing the job to the cloud
is not secure for two privacy (a.k.a., confidentiality) concerns:
• Input privacy: The company does not want to expose its
training dataset to any third party, including the CSP.
• Model privacy: As the size of models grows exponentially,
the computational resources and data needed for training
also increase drastically. The resulting model, acquired
through an expensive training process, holds substantial
TABLE I
COMPARISON OF Tempo AND NOTABLE RELATED SYSTEMS.
System
Model
Privacy
Input
Privacy
GPU
Employment
Inference
Training
Distributed
Slalom [11]
✓
✓
✓
MLCapsule [12]
✓
✓
✓
DarKnight [13]
✓
✓
✓
✓
1
SOTER [14]
✓
✓
✓
✓
Shadownet [15]
✓
✓
✓
✓
Tempo (Ours)
✓
✓
✓
✓
✓
✓
1. DarKnight supports multiple GPUs, but on the same server.
commercial value [9], [10] (e.g., SOTA LLMs like Chat-
GPT and GPT-4 are proprietary and not yet open-source).
Consequently, the trained model, as an intellectual property
(IP), should not be leaked to any third party.
Research status and significance. The quest for privacy-
preserving cloud DL has spurred considerable research en-
deavors [16]–[18]. Recently exploration into Trusted Execu-
tion Environments (TEEs) [11], [19], which are widely used to
uphold code and data confidentiality [20], [21]—has emerged
as a promising avenue to safeguard DL privacy systematically.
Utilizing a server equipped with TEE support [22] to conduct
the DL task entirely within the TEE (dubbed the “pure TEE”
solution) seems technically viable for safeguarding both the
model and data. However, mainstream TEEs often lack hard-
ware accelerators like GPUs, leading to impractical training
overheads that are reported to be orders of magnitude slower
than direct GPU utilization [13], [23]. To mitigate this, existing
arts offload computationally intensive algebraic computations
from the TEE to co-allocated GPUs for efficiency. To guar-
antee strong privacy, both the computation on GPUs and the
communication among devices must be carefully blinded or
encrypted. Several advancements have focused on protecting
client input privacy (e.g., [11], [13], [15], [23], [24]) by col-
laborating between TEE and GPUs. However, there remains
a dire scarcity of cloud DL frameworks for model privacy
protection. Existing model privacy-preserving systems such
as [14], [25], are predominantly limited to inference tasks.
Notably, at the time of paper writing, it occurs to the authors
that no MLaaS platform apprears to offer support for both
TEE and GPU on the same server (e.g., Microsoft Azure1).
In this paper, we propose Tempo, where the server fur-
nished with TEE acts as the master and collaborates with
1https://learn.microsoft.com/en-us/answers/questions/1187799/
which-azure-vm-image-support-both-intel-sgx-and-nv
arXiv:2401.11531v1  [cs.CR]  21 Jan 2024
Training or Inference…
DL Task Input
Output
Client
Cloud Service Provider (e.g., Azure)
Task Manager
Master Node
Off-loader
Shared Memory Calls
Data Streams
Untrusted
Trusted
(TEE)
\
TLS Connections
Enc/Dec & Aggregate
Model
Input
Attestation
GPU
Relay
Worker 3
CUDA Memcpy
GPU
Relay
Worker 2
CUDA Memcpy
GPU
Relay
Worker 1
CUDA Memcpy
Worker Nodes
Fig. 1. Overview of the system setup, and communications of Tempo.
workers equipped with GPU to accomplish the DL task. We
provide an overview of how Tempo works at a high level
in Fig. 1. Tempo is the first efficient TEE-assisted model
privacy-preserving cloud DNN training system. Tab. I lists a
brief comparison of Tempo with related systems.
Challenges and solutions. Two challenges arise in construct-
ing the data-offloading scheme in Tempo: (a) Model privacy
protection: Prior arts share the model parameters W with the
GPU and protect the input X only. It is nontrivial to design
an efficient encryption method to protect both W and X and
guarantee the correctness of the offloaded linear computation.
(b) Efficient training: A privacy-preserving training system
is another troublesome task since the backpropagation in-
volves more complicated computations: Tempo is required to
keep the model weights privacy-preserved while making them
available to workers for gradients calculation. Moreover, the
constantly changing weights trigger more frequent encryption
operations, which substantially increases the overhead of the
system, leading to an unwanted performance degradation.
To tackle challenge (a), we introduce a matrix multiplication
obfuscation algorithm (MM-obfuscation) that blinds both
model weights and inputs while providing intact train-
ing/inference accuracy. Weights and inputs are first encrypted
using a set of secret keys generated within the master’s
task manager (see Fig. 1) and then streamed to workers
for batched linear computation via relays, and the returned
encrypted result is aggregated, decrypted, and then verified at
the master to take precautions against potential active attacks
(e.g., returning manipulated results) from the malicious CSP.
To address challenge (b), considering cost-efficient and time-
efficient backpropagation, we optimize our training strategy by
designing a novel key shift mechanism based on the proposed
MM-obfuscation algorithm to reuse computation results from
the forward pass, thus reducing the number of encryption
operations by 50% in a complete training epoch.
Contributions. We make the following contributions:
• We introduce Tempo. To the best of our knowledge,
this is the first distributed cloud DL training system that
upholds both input and model privacy by employing a
trusted master to preserve confidentiality and multiple
GPU workers to enhance efficiency in training.
• We
propose
the
MM-obfuscation
algorithm,
a
permutation-based
algorithm
that
obfuscates
both
model
parameters
and
inputs,
seamlessly
aligning
with the functionalities of Tempo. This algorithm is
optimized to reduce the encryption overhead during
backpropagation, thereby accelerating DNN training.
• We implement Tempo on the top of Intel SGX [21]
using Graphene-SGX [26] and Pytorch [27]. Our ex-
periments encompass two prominent DNN architectures,
ResNet [28] and Vision Transformer [29] under diverse
configurations. In comparison with pure TEE baselines,
Tempo sports an average training speed-up of 4.37×
(local setup) and 3.95× (network setup) without com-
promising model accuracy.
• We offer a comprehensive assessment of the MM-
obfuscation algorithm, scrutinizing its correctness, pri-
vacy, and cost. We launch model theft attacks on Tempo,
conclusively demonstrating that our design does not pro-
vide any advantage to potential attackers.
II. PRELIMINARIES
A. Deep Neural Network Training
A Deep Neural Network (DNNf) is composed of multiple
sequential layers, each performing a linear or nonlinear op-
eration. Linear operations involve trainable weights, denotes
as W = (W1, W2, · · · , WL) for a DNN with L layers. We
denote the linear operation as ⟨·⟩, and the non-linear operation
at layer l as σl(). DNN training involves a forward pass
to compute the output, followed by weights update through
backpropagation based on the loss [30]. This process re-
peats iteratively using mini-batch Stochastic Gradient Descent
(SGD) until a termination criterion is met [31].
Feedforward. The forward pass computes the features sequen-
tially layer-wise, starting from layer 1. For layer l, we denote
the input and output features as xi
l, yi
l (here i = 1, · · · , B
denotes the ith sample in the mini-batch with a size of B).
Two neighboring layers are linked by xi
l+1 = yi
l. The input
of the DNN is X1 and the output is YL. Without losing of
generality, we express the basic computations in forward as:
yi
l = σl(zi
l) = σl(⟨Wl·xi
l⟩), in which zi
l is the result of the
linear operation. Noticing that we can compute ⟨Wl·xi
l⟩ over
all samples Xl =
where
(
δi
L = ∇yJ (yi
L, yi) ⊙ σ′
l(zi
L),
δi
l = ⟨(Wl+1)⊤·δi
l+1⟩ ⊙ σ′
l(zi
l).
(3)
Here, W∗
l is the updated weights, λ is the learning rate, δi
l
is the gradient of the loss for the ith sample in the batch
to the output of layer l, ⊙ is the Hadamard product. Again,
noticing that we can compute ⟨δi
l·(yi
l−1)⊤⟩ = ⟨δi
l·(xi
l)⊤⟩ and
⟨(Wl+1)⊤·δi
l+1⟩ over all samples Xl. For convenience, we
define ∆l =
TEE
GPU
i. 𝐴×𝐵 =?
ii. 𝐴!, 𝐵′ = Enc(𝐴, 𝐵)
𝐴!, 𝐵′
iii. 𝐶! = 𝐴!×𝐵′
𝐶′
iv. 𝐶 = Dec(𝐶′)
v. Verify: 𝐶⇔
? 𝐴×𝐵
Offload
Fig. 2. Workflow of the MM-obfuscation algorithm in Tempo.
A. The MM-obfuscation Algorithm
Offloading the computationally intensive matrix multipli-
cations to GPUs facilitates fast batch computation. However,
exposing the plaintext matrices outside the secure TEE induces
risk of privacy breaches. Hence, our objective is to obfuscate
the matrix operands before offloading them to GPU-equipped
workers for computation. We propose an MM-obfuscation
algorithm for Tempo, which is a form of permutation-based
secure matrix multiplication optimized for DNN tasks. The
high-level workflow of this algorithm is illustrated in Fig. 2.
The TEE offloads the computation A × B. The operands of
the matrix multiplication A, B are first encrypted within the
TEE (Algo. 2), and then the worker performs vanilla matrix
multiplication directly on the encrypted matrices A′, B′. The
result C′ is returned to the TEE in encrypted form and can be
decrypted and verified to recover the plaintext C (Algo. 3).
Algorithm 1: KGen(m, n, p)—Secret Key Generation
Require: Integers m, n, p
Result: A secret key sk
▷ Generate random coefficients
1 cm
$← {K}m, cn
$← {K}n, cp
$← {K}p
▷ Generate random permutations
2 πm ← RandPr(1, 2, · · · , m), πn ← RandPr(1, 2, · · · , n),
πp ← RandPr(1, 2, · · · , p)
3 return sk ← (cm, πm, cn, πn, cp, πp)
▷ sk := (sk[0], sk[1], sk[2])
Algorithm 2: Enc(sk, A, B)—Matrix Encryption
Require: Two matrices, A ∈ Rm×n and B ∈ Rn×p and
sk = KGen(m, n, p)
Result: The encrypted matrices A′, B′
1 for i ← 1 : m and j ← 1 : n do
2
A(· · · , i, j) ← cm(i)
cn(j) A(· · · , πm(i), πn(j))
3 for i ← 1 : n and j ← 1 : p do
4
B′(· · · , i, j) ← cn(i)
cp(j) B(· · · , πn(i), πp(j))
5 return (A′, B′)
We describe the encryption algorithm Enc() in Algo. 2. The
matching decryption algorithm Dec() is described in Algo. 3,
which uses the same set of secret key sk generated by Algo. 1
(KGen()) before encryption. In Algo. 1,
$← is the random
sampling function, K ∈ N+ is the coefficient key space
(
$← {K}m means random sampling a vector with length m
from key space K), and RandPr() is the random permutation
generator [39] (RandPr(1, 2, · · · , n) returns a random permu-
tation of the n pre-images (mathematical term)).
Algo. 3 describes the decryption and verification, in which
π−1 is the inverse permutation of π. The probabilistic random-
Δ!""#
𝑊!
∇𝑊
Δ!
𝑋!
𝜎!′(𝑍!)
Δ! · 𝑋! ""
𝑊! "" · 𝑋! 
Data Flow Update
To GPU
Fig. 3. Backpropagation of DNN training in Tempo.
ized Freivalds’ algorithm [40] is adopted for result verification
(line 3 to line 8, see Theorem V.2 for setting of k). We defer
the proof of correctness of these algorithms to Theorem V.1
and complexity analysis to § V-C. The MM-obfuscation al-
gorithm is a vital technique to safeguard DL privacy and is
continuously invoked during the training procedure.
Algorithm 3: Dec(sk, C′)—Matrix Decryption
Require: The encrypted matrix C′ ∈ Rm×p, where C′ = A′ × B′
and sk = KGen(m, n, p)
Result: The decrypted matrix CDec if it is correct
1 for i ← 1 : m and j ← 1 : p do
▷ Calculate decrypted matrix
2
CDec(· · · , i, j) ←
cp(π−1
p
(j))
cm(π−1
m (i)) C′(· · · , π−1
m (i), π−1
p (j))
3 for i ← 1 : k do
▷ Integrity verification (A, B are kept
inside the TEE from Algo. 2)
4
r
$← {0, 1}p
5
P ← A(Br⊤) − CDecr⊤
▷ Calculate Br⊤ first
6
if P ̸= 0m×1 then
7
Output “failed” and abort
8 Output “passed” and return CDec
B. Privacy-preserving Feedforward
In the next two sections, we first demonstrate the training
method when there is only one worker, and we will discuss
the case of multiple workers in § IV-D.
Noticing that in Eq. 1, there is one set of computational
intensive linear operation (i.e., ⟨Wl·Xl⟩) we would like to
offload to GPU at layer l. The batched computation helps de-
crease the encryption/decryption cost as Tempo only requires
one Enc() at a layer for a mini-batch in the forward pass. The
computations and communications of Tempo in the forward
pass are shown in Fig. 4. Assumes that Wl ∈ Rml×nl, Xl ∈
Rnl×pl3. Here, skFwd
l
is the secret key at layer l generated in
the forward pass using KGen(ml, nl, pl). In Tempo, a single
set of skFwd
l
can be reused for all batches on this layer in
an epoch once generated. The task manager blind the two
operands, Wl and Xl, using Enc() before offloading directly
to GPU for matrix multiplication. After obtaining Zl, TEE
will compute Yl (the non-linear operation) and pass it as the
input Xl+1 to the upcoming layer until the output layer L is
reached. The final output of the forward pass is YL.
Task Manager (TEE)
Worker (GPU)
1 :
(W′
l, X′
l) = Enc(skFwd
l
, Wl, Xl),
2 :
W′
l,X′
l
Z′
l = W′
lX′
l,
3 :
Zl = Dec(skFwd
l
, Z′
l),
Z′
l
(Keep W′
lX′
l in mem.)
4 :
Xl+1 = σl(Zl).
Fig. 4. Computations of Tempo in forward pass at layer l
C. Privacy-preserving Backpropagation
The computational graph of BP is depicted in Fig. 3. At
layer l, there are two sets of linear operations to be offloaded:
⟨∆l·X⊤
l ⟩, ⟨W⊤
l ·∆l⟩.
(6)
Likewise, the simplest way is to generate two sets of skl
and invoke Algo. 2 to obfuscate two sets of operands (4
matrices). However, the calculation of the encryption matrix
incurs undesired overhead, and Tempo strives for minimizing
the encryption/decryption cost as much as possible.
Optimization of MM-obfuscation in BP. Here, we observe
two key insights that can help us optimize our encryption
strategy:
1) In Eq. 6, it is only necessary to encrypt 3 operand
matrices instead of 4 (i.e., when Enc() is called directly
twice), they are W⊤
l , X⊤
l and ∆l.
2) Recall in the forward pass, we have used skFwd
l
to
calculate the encrypted matrices W′
l and X′
l already.
A natural question is: Can we reuse the encrypted results
instead of recomputing the encryption? Fortunately, it is pos-
sible to reuse the encrypted matrices from the forward pass.
Recall that in Algo. 1, the secret key skFwd
l
used for encryption
depends on the shape of the matrix operands. We observe that
∆l ∈ Rml×pl has the same shape with Zl. We now define a
left circular key shift JskKϕ where sk = KGen(m, n, p). First,
we divide the components of sk into three ordered tuples,
sk[0] = (cm, πm), sk[1] = (cn, πn), sk[2] = (cp, πp). Then, the
left circular key shift is defined as (JskKϕ)[i] ≜ sk[(i−ϕ) mod 3].
A demonstration of the key shift with ϕ = 1 is exemplified
in Fig. 6. Now, it is possible to use JskFwd
l
K2 to compute the
encrypted version of ∆⊤
l
using Enc(JskFwd
l
K2, ∆⊤
l , ·) without
passing the second matrix as parameter (“·” denotes an omitted
parameter), and reusing W′
l, X′
l in the offloaded matrix multi-
plications. The computations and communications of Tempo
in BP are shown in Fig. 5. Here, Zl, X′
l, W′
l are reused from
the forward pass, in which Zl is kept in the secure memory of
the TEE and X′
l, W′
l are stored in the worker’s GPU memory.
As with the forward pass, Tempo requires only one Enc() per
layer in BP. Also, the two Dec()s are already the minimum
decryption cost, since there are two sets of linear operations.
D. Distributed Training using Tempo
Now we elaborate the training strategy using Tempo when
there are multiple workers. This distributed training method
3In PyTorch, tensors can have dimensions exceeding 2. In such cases, our
algorithm consistently operates on the last two dimensions.
Task Manager (TEE)
Worker (GPU)
1 :
(∆⊤
l )′ = Enc(JskFwd
l
K2, ∆⊤
l , ·),
2 :
(∆⊤
l )′
(T1
l )′ = X′
l(∆⊤
l )′,
3 :
(T2
l )′ = (∆⊤
l )′W′
l,
4 :
T1
l = Dec(JskFwd
l
K1, (T1
l )′),
(T1
l )′,(T2
l )′
5 :
T2
l = Dec(JskFwd
l
K2, (T2
l )′),
6 :
W∗
l ← Wl − λ
B (T1
l )⊤,
7 :
∆⊤
l−1 = T2
l ⊙ σ′
l(Z⊤
l ).
Fig. 5. Computations of Tempo in backpropagation at layer l
𝒄𝒎, 𝜋""
𝒄𝒏, 𝜋$
𝒄𝒑, 𝜋&
𝒄𝒏, 𝜋$
𝒄𝒑, 𝜋&
𝒄𝒎, 𝜋""
Left Circular
Shift 1
(a) sk
(b) sk '
(𝜙 = 1)
Fig. 6. An example of left circular key shift used in backpropagation.
can be scaled to larger models in the future via hybrid
distributed parallelism combining DP, TP, and PP (illustrated
in Fig. 7). We specify the complete training strategy in Algo. 4.
Data parallelism (DP): The matrix multiplication is repli-
cated to each worker with each being fed a slice of the
partitioned data. When all the calculations are complete, a
Gather operation is triggered and the results are aggregated,
decrypted and concatenated inside the master. As the weights
need to be copied to all workers, DP incurs substantial
communication overhead as the model size gets larger. Tensor
model parallelism (TP): Model weights are partitioned across
multiple workers with each receiving a part of the model
and conducting part of the computation. Pipeline parallelism4
(PP): To minimize the encryption cost, we further adopt
4This is different from the commonly-known microbatch pipeline [41].
𝑊
𝑋
×
Master
Worker 1
𝑊’
𝑋!′
𝑌!′
×
=
Worker N
𝑊’
𝑋""′
𝑌""′
×
=
\
Plaintext
Encrypted
Partition 
then 
Encrypt
…
Worker 1
𝑊!’
𝑋 ′ 𝑌!′
×
=
Worker N
𝑊""’
𝑋 ′ 𝑌""′
×
=
…
Master
𝑌 
Data Parallelism
Tensor Model Parallelism
OR
Decrypt
…
𝑌!
𝑌""
Cat
Master
𝑌 
Decrypt
𝑌!
𝑌""
Cat
…
⟺
Model Weights Input Features
a mini-batch
Output Features
Time
sk1
skN
sk1
skN
…
…
Pipeline Parallelism
Master
Comm.
Workers
Linear
Enc
Dec
Non-linear
Dec
Non-linear
Linear
Enc
…
…
…
at layer 𝑙 + 1
overlap
overlap
Enc 𝑊!""#
Enc 𝑋!""#
𝑊!
$×𝑋!′
Fig. 7. Distributed parallelism used in Tempo.
pipelined encryption and asynchronous SGD update, which
enables the master to encrypt weights at the upcoming layer
in waiting for the results handled by the workers. In this way,
we further alleviate the encryption overhead by overlapping it
with communication and computation.
For efficient training on distributed workers, Tempo always
use PP, and switch DP and TP alternately depending on the
properties of the linear operation: (a) Fully-connected (FC)
layers are parameter-heavy, use TP to partition the model
weights; (b) Convolutional layers have fewer parameters but
is computed on larger input features, use DP to distribute
the data; (c) For the unique linear operation of the Trans-
former [42], multi-head attention (MHA), it is natural to use
TP because multiple independent heads can be segmented. We
point out that Tempo can collaborate with other distributed
training algorithms in an ad-hoc way to further boost perfor-
mance, which can be an interesting direction for future work.
Algorithm 4: Distributed DNN Training in Tempo
Require: Training dataset D = ( ¯X, ¯Y), DNN framework
(W, σ, L), loss function J , number of workers N
Result: The trained model W∗
1 for all epoch do
2
for all mini-batch (X1, Y) ∈ D do
▷ |X1| = batch size
3
for l ← 1 : L do
▷ Forward pass (inference)
4
Partition weights Wl or input Xl to N shares
depending on the type of the layer, and get
partitioned operands Wl(,j) and Xl(,j),
j = 1, · · · , N.
5
Generate/refresh secret keys skl,j using Algo. 1.
6
Encrypt N shares of operands, distribute them to the
workers following Fig. 4 to obtain Xl+1.
7
The workers stores the encrypted operands (Wl(,j))′
and (Xl(,j))′ in memory.
▷ Keep encrypted
results for reuse in BP
8
∆L ← ∇J (XL+1, Y) ⊙ σ′
l(ZL)
9
for l ← L : 1 do
▷ Backpropagation (BP)
10
Partition ∆l if the corresponding input Xl is
partitioned in the forward pass.
11
Encrypt N shares of operands, distribute them to the
workers following Fig. 5 to update W∗
l .
12 return W∗
V. ANALYSIS OF Tempo
A. Proof of Properties of Tempo
Theorem
V.1
(Correctness). The MM-obfuscation algo-
rithm (described in Algo. 2 and Algo. 3) is correct, i.e.,
let (A′, B′)
=
Enc(sk, A, B) and C′
=
A′B′. Then,
Dec(sk, C′) = AB.
Proof. We start with constructing encryption matrices using
sk = (cm, πm, cn, πn, cp, πp) generated from Algo. 1:





E1(i, j) = cm(i)δπm(i),j,
E2(i, j) = cn(i)δπn(i),j,
E3(i, j) = cp(i)δπp(i),j,
in which δi,j =
n1,
i = j
0,
i ̸= j is the Kronecker delta function.
Note that, the encryption matrices are invertible. This is due
to 0 /∈ cm, thus the determinant det(E1) ̸= 0. Similar proof
also applies to E2, E3. The inverse encryption matrices are:







E−1
1 (i, j) = cm(i)−1δπ−1
m (i),j,
E−1
2 (i, j) = cn(i)−1δπ−1
n (i),j,
E−1
3 (i, j) = cp(i)−1δπ−1
p
(i),j.
Recall that in Algo. 2, the encrypted matrix A′(i, j) =
cm(i)
cn(j) A(πm(i), πn(j)). Hence, the encrypted matrix can be
represented as A′ = E1AE−1
2 . Likewise, B′ = E2BE−1
3 .
Therefore, for the encrypted matrix multiplication:
A′B′ = (E1AE−1
2 )(E2BE−1
3 )
= E1ABE−1
3
= C′.
(7)
By Algo. 3, the decrypted matrix can be represented using the
aforementioned encryption matrices as CDec = Dec(sk, C′) =
E−1
1 C′E3. Due to Eq. 7, the following equation holds:
CDec = E−1
1 C′E3 = E−1
1 (E1ABE−1
3 )E3 = AB.
Theorem
V.2
(Integrity). Tempo achieves verifiable t-
integrity (see § III-C), by setting k
> log2
1
1−(1−t)
1
αN L
in Algo. 3 where α =
1,
F = inference
3n⌈ |D|
B ⌉,
F = training, and n is the
number of training epochs.
Proof. Since the untrusted components can be adversarial,
Tempo verifies the integrity of offloaded results using the
probabilistic Freivalds’ algorithm [40], which produces a
soundness one-sided error (no false negatives) of
1
2k , i.e.,
Perr = Pr [CDec ̸= AB|Algo. 3 Outputs “passed”] ≤
1
2k . Setting
k > log2
1
1−(1−t)
1
αN L , we have Perr < 1−(1−t)
1
αN L . According
to algorithms in Fig. 4 and Fig. 5, there are L and 2L offloaded
computations for a batch and a single worker in an epoch in the
forward pass and backpropagation, respectively. The number
of offloaded computations in F is αNL. Thus we have that:
Pr [O∗ ̸= F(I)|O∗ ̸= ⊥] ≤ 1 − (1 − Perr)αN L
< 1 − (1 − t) = t.
(8)
Thus the following inequality holds:
Pr [O∗ /∈ {F(I), ⊥}] = Pr [O∗ ̸= F(I) ∩ O∗ ̸= ⊥]
= Pr [O∗ ̸= ⊥] Pr [O∗ ̸= F(I)|O∗ ̸= ⊥]
≤ Pr [O∗ ̸= F(I)|O∗ ̸= ⊥] < t.
Theorem V.3 (Privacy). Tempo achieves privacy w.r.t. model
and input (see § III-C) with the proposed MM-obfuscation
algorithm.
Proof. For Algo. 2, assuming that the generated permuta-
tions πm, πn are truly random, then the complexity to re-
construct A from the encrypted matrices A′ ∈ Rm×n is
O(m!n! |K|(m+n)), which means the expected time of an
Fig. 8. Privacy level P(↑) of Tempo of (a) input and (b) weights. Tempo’s
Enc() achieves similar privacy close to 0 w.r.t. different size of key space.
adversary launching a brute-force attack is definitely non-
polynomial. Thus, privacy w.r.t. model and input is achieved
by encryption before offloading to untrusted components.
Remark. Theorem V.1 suggests the correctness of our pro-
posed MM-obfuscation algorithm. Theorem V.2 and Theo-
rem V.3 prove that Tempo achieves verifiable t-integrity and
privacy w.r.t. model and input by integrating the algorithm.
B. Measuring Privacy Level
Mutual information (MI) [43] is a quantification of how
similar or different two variables are. Let (X, Y ) be a pair of
random variables with a joint distribution p(X, Y ) and their
marginal distributions p(X), p(Y ), MI is defined as:
I(X; Y ) = DKL(p(X, Y )∥p(X)p(Y ))
= H(X) − H(X|Y ).
(9)
Where DKL is the Kullback–Leibler divergence, H(X) and
H(X|Y ) denote the marginal entropy and conditional entropy.
The MI I(X; Y ) is non-negative and symmetric. Intuitively, a
lower MI indicates less amounts of common knowledge shared
between the two variables X and Y . To quantify the privacy-
preserving level of Tempo, we define the privacy as:
P = −I(X; Xo) = −I(X; Enc(sk, X, ·)).
(10)
Here, X is the private data (i.e., weight W and input X1) and
Xo is the observable data in the view of the adversary, which
in terms of Tempo is the encrypted variable Enc(sk, X, ·).
In information-theoretic privacy, perfect privacy is achieved
if P = 0. To estimate the privacy level of Tempo, we
depict the MI I(X; Xo) between the encrypted data obtained
by the proposed MM-obfuscation algorithm along with other
comparable schemes and the original input selected from
CIFAR-10 (see § VII) and model weights in Fig. 8. Here,
the x-axis is the size of key space |K|, Enc() is our proposed
encryption algorithm, Enc wo pr() is our algorithm without
using permutations. We also show the MI of direct exposure,
multiplication by scalar, and addition with random matrix
(Xo = X + r, which is similar to techniques in [11], [25]).
C. Encryption-decryption Cost Analysis
The encryption Algo. 2 yields a (time) complexity of
O(mn + np). Algo. 1 also consumes a negligible cost of
O(m + n + p) for generating secret key skl at layer l, which
can be reused over all the iterations. The decryption Algo. 3
yields a total complexity of O(mp). The MM-obfuscation al-
gorithm is meaningful since the matrix multiplication exhibits
a complexity of O(mnp) ≫ O(mn+np+mp) when m, n, p
is large. TEE significantly circumvents the computationally
heavy linear operations by offloading it to GPUs.
In addition, by reusing the encrypted values, the encryption
complexity for training has reduced to 50% roughly (forward:
2 matrices to encrypt; BP: 1 matrix instead of 4 to encrypt)
in one training epoch. Since the cost of the MM-obfuscation
is proportional to the size of the matrix and the dimension
of DNN parameters are substantially larger than those of the
input, thus encrypting Wl (see Fig. 4) sand decrypting (T1
l )′
(see Fig. 5) are the most significant overhead. However, this
is a necessity because (a) we need to protect all the weights;
and (b) the complexity of our algorithm is proportional to the
number of weights, which is already optimal analytically.
VI. SYSTEM IMPLEMENTATION
We implement Tempo with Intel SGX CPU as the TEE and
NVIDIA GPUs as accelerators. We use Graphene-SGX [26]
(now renamed to Gramine5), a lightweight library OS designed
for running an unmodified confidential application inside, to
safeguard the task manager within an SGX enclave. For the
software stack, we choose PyTorch and its CPP extension6 for
DNN and tensor-related implementation. We implement our
prototypes primarily using C++ and Python in 3, 907 LoC.
Tempo’s master handles the input provided by the client
before initiating the DL task. This is implemented using
a standard SGX remote attestation. Simply put, the client
submits the model in ONNX format [44] and data to the master’s
TEE directly via a secure TLS channel. The model is then
decrypted inside the TEE using standard TLS cipher suites.
We implement two prototypes of Tempo for different types
of communication. Tempo-l (local) applies to the master
and workers located on the same (physical) server. Tempo-n
(network) applies to each nodes located on distributed servers
in a LAN. In both prototypes, the task manager spawns an
extra PyTorch process to first offload the encrypted data to the
off-loader via shared memory, this is due to the inconvenience
of modifying the the syscalls inside the TEE [14]. For Tempo-
n, the off-loader streams the data via TCP to the workers’
relays. The relay then moves the data to GPU memory using
cudaMemcpy(). We simulate network connection in Tempo-n
by packing the master and workers into separate Docker con-
tainers, thus the nodes are inter-connected via virtual Bridge7
network. We use a Linux tool qperf8 to measure the network
performance, the average (tested 10 times) TCP latency using
bridge network between two containers is 24.4 µs.
VII. PERFORMANCE EVALUATION
We evaluate our implementation on a physical server with
Intel(R) Xeon(R) Gold 5218R CPU processor and 376 GB of
5https://github.com/gramineproject/gramine
6https://pytorch.org/tutorials/advanced/cpp extension.html
7https://docs.docker.com/network/drivers/bridge/
8https://github.com/linux-rdma/qperf
Fig. 9. Training accuracy (on test set) of Tempo-l (1 GPU) over CIFAR-10
for two DNNs: (a) ResNet50 (b) ViT, compared with the secure baseline.
Fig. 10. Distributed training accuracy (on test set) of Tempo over CIFAR-
10 for two DNNs: (a) ResNet50 (b) ViT. Tempo-n suggests that the master
(TEE) communicates with the workers (GPU) using network.
available RAM. This machine is co-allocated with 2 NVIDIA
GeForce RTX 4090 GPUs equipped with 24 GB of VRAM
each. We use the simulation mode of SGX for evaluation.
Models, dataset, and baselines. We select two DNNs,
ResNet50 [37], a CNN with residual connections, and Vision
Transformer (ViT) [29], a Transformer-based [42] DNN. We
conduct image classification, a classic supervised DL task,
using the CIFAR-10 dataset9 which comprises 60, 000 images
that span 10 categories. Since There is no TEE-assisted
system that guarantees both efficient DNN training (leveraging
GPUs in BP) and model privacy protection, we compare our
prototypes with three baseline systems: two secure baselines
that preserve both model and input privacy, including Pure
TEE and MLCapsule [12] (inference only), they both shield
an entire model within the TEE without employing GPUs;
and DarKnight [13], an insecure baseline that does not protect
model privacy. We aim to answer the following questions:
Q1. (Efficiency) What is the training/inference performance of
Tempo?
Q2. (Accuracy) Does Tempo achieve the same level of accu-
racy as the baselines?
Q3. (Empirical Privacy) Would Tempo leak any sort of
information when encounters real-world attacks?
A. Training Performance
For DNN training tests, we choose Pure TEE as the secure
baseline and DarKnight [13] as the insecure baseline. We
trained two models over CIFAR-10, with 50, 000 images in the
training set and the rest 10, 000 in the test set. We use mini-
batch training with a learning rate of 0.001 and the batch size
set to 512 images. (Q2) The ResNet50 is trained for 50 epochs
to achieve a final accuracy of over 90% on the test set while
9https://www.cs.toronto.edu/∼kriz/cifar.html
Fig. 11. Training time breakdown. We measure the time cost averaged on an
epoch. (F) stands for forward pass and (B) stands for backpropagation. We
evaluate for two settings: Tempo-l with (a) a single GPU (b) 2 GPUs.
the ViT (patch size = 4 and encoder layers = 8, see [29] for
meanings) is trained for 300 epochs to achieve a final accuracy
of over 81%10. The training curves of accuracy are depicted
in Fig. 9. Please note that, within the same number of training
epochs, Tempo achieves intact accuracy when compared to
the baseline. We also conduct experiments for distributed
training over multiple workers (see Fig. 10). For training using
2 workers, we scale up the batch size to 1024. Using multiple
workers increases efficiency due to hybrid parallelism among
GPUs, allowing a larger batch size thus reducing the number
of iterations. However, the speed-up is nonlinear, this is due to
the fact that the training procedure constitutes other operations
in addition to linear operations. Fig. 11 displays a detailed
time breakdown of various operations. By adding workers,
we observe that the time spent in linear operations is greatly
reduced. For Tempo-l and Tempo-n with the same number
of workers, the latter incurs a noticeable slowdown due to the
network latency among the nodes.
(Q1 - Part I) We report the training time and corresponding
speed-up of Tempo with baselines in Tab. II. Tempo yields
a maximal training speed-up of 4.92× and 3.83× compared
with the secure baseline for ResNet50 and ViT, respectively.
Out of curiosity, we also compare it with an insecure baseline,
i.e., training is conducted with unprotected GPU without any
privacy protection using PyTorch directly. No surprising that
Tempo is an order of magnitude slower than the insecure
baseline thanks to the engagement of TEE and communi-
cations. Please note that similar privacy-preserving training
systems exhibit similar slowdowns even without the model
protected [13], [23].
B. Inference Performance (Q1 - Part II)
For DNN inference tests, we choose MLCapsule [12], a
state-of-the-art privacy-preserving inference system, as the
secure baseline. We evaluate the inference speed-up of Tempo
and the results are shown in Fig. 12. Similar to training, we
test the prototypes with batches of images as input and the
10The difference in accuracy of the two models may be due to the
inherited characteristic of the models themselves. According to [29], ViT may
underperform with less training data and without pre-training.
TABLE II
TRAINING TIME (IN HOURS) AND SPEED-UP OF Tempo OVER CIFAR-10,
COMPARED WITH BASELINES.
Model
Tempo-l
(1 GPU)
Tempo-l
(2 GPUs)
Tempo-n
(2 GPUs)
Pure TEE1 DarKnight
[13]2
Non-
private2
ResNet50
19.91
3.38×
13.68
4.92×
15.30
4.40×
67.32
1.00×
11.87
5.67×
0.36
187.0×
ViT
18.23
3.09×
14.74
3.83×
16.07
3.51×
56.41
1.00×
-3
0.59
95.44×
1. Secure baseline for privacy-preserving and verifiable DNN training.
2. Insecure baselines without any protection.
3. The DarKnight prototype does not support Transformer-based model.
Fig. 12. Inference speed-up of Tempo for two DNNs: (a) ResNet50 (b) ViT,
compared with the secure baseline. The inference batch size is the number of
images (in CIFAR-10 test set) fed into the DNN simultaneously.
time cost is averaged to a single image (for both Tempo and
the baseline). For both prototypes and DNNs, we notice that
performance improves when larger batch sizes are used.
We also conduct evaluations for inference efficiency under
different system configurations. Fig. 13 demonstrates the in-
ference performance influenced by the integrity and network
settings. Recall in Algo. 3, Tempo verify the integrity of
the offloaded matrix multiplication using the Freivalds’ algo-
rithm [40]. To achieve different levels of t-integrity, the result
verification frequency may vary. For smaller error tolerance,
more frequent verification is required. But experiments show
that these extra calculations do not lead to significant perfor-
mance degradation (see Fig. 13 (a)). We also investigate the
performance of Tempo under different network settings. We
evaluate Tempo-n (2-GPUs) for various network bandwidths
from 10 MBps to 3 GBps (see Fig. 13 (b)). Performance im-
proves drastically with greater bandwidth. Using cubic spline
to interpolate the measured data points, we infer that Tempo-n
(2-GPUs) losses its practicality when the bandwidth is smaller
than 129.82 MBps. By default, we use the largest possible
bandwidth for all prior experiments on Tempo-n prototype.
Fig. 13.
Inference speed-up of Tempo for ResNet50 compared with the
secure baseline under different settings: (a) t-integrity tolerance (b) network
performance. We set the inference batch size = 512.
C. Information Leakage of Real-world Attacks (Q3)
To further assess the robustness of Tempo and whether the
theoretical privacy guarantee presents in § V is effective in
practice, we conduct model theft attacks [45], a.k.a., model
stealing, to steal the target model by querying it using synthetic
inputs since the real data for training the target model is nor-
mally unobtainable for the adversary. We launch the DaST [45]
attack to train a substitute model I to imitate the performance
of a trained ResNet50 model (dubbed the attacked model T)
inside Tempo. DaST feeds the 100% synthetic samples ˆX to
T and collects its predictions T(ˆX) for substitute learning.
We select a ResNet50 and initialized it with the encrypted
weights W′ copied from T for the backbone of I. For com-
parison, we use the same strategy to train another substitute
model Io, which is another ResNet50 initialized with random
weights. Both models are trained using the same set of query
pairs Q = {ˆX, T(ˆX)} collected by querying T. I and Io
are then tested over 1, 000 images in the test set of CIFAR-
10, yielding an accuracy of 25.91% and 30.23%, respectively,
while T enjoys an accuracy of 90.37% over the same set of
images. This implies when suffering from model theft attacks,
(a) the leakage of Tempo is modest; and more importantly, (b)
the attacker’s view of Tempo will not gain him any advantage.
It also noteworthy that Tempo and all similar TEE-assisted
secure DL systems are not naturally defensible against black-
box attacks based on inference APIs [9], [46]. Considering
that training on the cloud limits the attacker from launching a
large number of queries (i.e., |Q| is small), thus this type of
attack only makes sense in terms of inference. Nonetheless,
by cooperating with other efforts [47], Tempo can be used to
further narrow down the attack surface.
VIII. RELATED WORK
A. TEE for Privacy-preserving Learning
Researchers have well explored privacy-preserving infer-
ence (e.g., [11], [12]) and training (e.g., [13], [23], [24])
w.r.t. input privacy using both TEE and GPU. Slalom [11],
an inference-only system that protects client input privacy,
achieves its goal by using ⟨W·X⟩ = ⟨W·(X + r)⟩ − ⟨W·r⟩
(they use (X + r) to blind the input X). Since the weights
W are kept in plaintext and the pre-computed factors ⟨W·r⟩
make it impossible to update the weights during training, their
method cannot be applied for both model privacy protection
and training. DarKnight [13] preserves the input privacy in
both training and inference using a simpler matrix encoding
⟨W·X⟩ × A = ⟨W·XA⟩, with A the encoding matrix. Their
approach is not feasible for protecting the model since W is
not encoded (obfuscated) and is shared with all GPUs. Till
recently, systems that protect the model privacy (e.g., [14],
[19], [25]) have received little attention, in which [14], [25]
are proposed for inference and [19] does not utilize GPUs.
SOTER [14] protects both the model and input confidentiality
based on the associative property of the linear operations in
DNN, i.e., µ⟨W·X⟩ = ⟨µW·X⟩, with µ a scalar blinding
coin. However, SOTER fails short to cooperate DNN train-
ing11. Moreover, few work investigates GPU TEE for secure
DL (e.g., [6], [48]), requiring customized accelerators instead
of employing publicly available (CPU) TEE and GPUs.
B. Other Privacy-preserving Learning Methods
Apart from TEE-based systems, a broader community has
made substantial progress in using a variety of technologies
to protect privacy in DL. We mark some notable literature in
this section, though, most of these methods aim for orthogonal
privacy-preserving goals with ours. Homomorphic Encryption
(HE) [17], [49] applies a DNN to the encrypted input di-
rectly to make encrypted predictions. They exhibit theoretical
privacy guarantees over the input but also incur multiple
orders of magnitude slow-down, thus not suitable for large
DNN training. Secure Multi-Party Computation (MPC) [18]
is applicable for distributed learning when a master node is
absent. Their main purpose is to protect the input and suffer
from significant communication overhead as the number of
participants increases. Differential Privacy (DP) [50] considers
the training sets crowdsourced. They prevent an adversary
from extracting parts of the training data of an individual user’s
sensitive information. Federated Learning (FL) [51] regards
the data as decentralized, allowing data to be analyzed on
individual devices to train a global shared model. Researchers
have combine FL, MPC, and DP (e.g., [52]–[54]) to preserve
an individual’s input privacy. Defensive mechanisms against
privacy breaches, such as deep leakage from gradients (DLG),
have stimulated research in privacy-preserving FL community
recently to protect the privacy of raw data [55], [56].
IX. CONCLUDING REMARKS
This paper presents Tempo, a distributed DNN training
system designed to safeguard model privacy in cloud envi-
ronments. Leveraging TEE, Tempo guarantees both input and
model confidentiality, while harnessing GPU for enhanced
performance. This system integrates an obfuscation algorithm
tailored for efficient training. Tempo’s flexibility allows com-
patibility with various TEEs. Evaluation findings validate
Tempo as a robust solution, demonstrating its potential to
fortify the privacy infrastructure within the MLaaS paradigm.
11One possible reason is that the privacy-preserving ability from a scalar
coefficient is weaker than matrices, the blinding coin µ needs to be refreshed
more frequently to counter adversaries, making it less realistic in training.
REFERENCES
[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, 2015.
[2] “Google cloud: Introduction to ai platform,” (Accessed on July 31,
2023). [Online]. Available: https://cloud.google.com/ai-platform/docs/
technical-overview
[3] “Azure machine learning,” (Accessed on July 31, 2023). [Online].
Available: https://azure.microsoft.com/products/machine-learning
[4] M. Ribeiro, K. Grolinger, and M. A. Capretz, “Mlaas: Machine learning
as a service,” in ICMLA, 2015.
[5] Z. Ghodsi, T. Gu, and S. Garg, “Safetynets: Verifiable execution of deep
neural networks on an untrusted cloud,” in NeurIPS, 2017.
[6] T. Hunt, Z. Jia, V. Miller, A. Szekely, Y. Hu, C. J. Rossbach, and
E. Witchel, “Telekine: Secure computing with cloud gpus.” in NSDI,
2020.
[7] C. Wang, Q. Wang, K. Ren, and W. Lou, “Privacy-preserving public
auditing for data storage security in cloud computing,” in INFOCOM,
2010.
[8] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical
text-conditional image generation with clip latents,” arXiv preprint
arXiv:2204.06125, 2022.
[9] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction apis,” in USENIX Sec, 2016.
[10] Q. Jia, L. Guo, Z. Jin, and Y. Fang, “Preserving model privacy for
machine learning in distributed systems,” TPDS, 2018.
[11] F. Tramer and D. Boneh, “Slalom: Fast, verifiable and private execution
of neural networks in trusted hardware,” in ICLR, 2019.
[12] L. Hanzlik, Y. Zhang, K. Grosse, A. Salem, M. Augustin, M. Backes, and
M. Fritz, “Mlcapsule: Guarded offline deployment of machine learning
as a service,” in CVPR, 2021.
[13] H. Hashemi, Y. Wang, and M. Annavaram, “Darknight: An accelerated
framework for privacy and integrity preserving deep learning using
trusted hardware,” in MICRO, 2021.
[14] T. Shen, J. Qi, J. Jiang, X. Wang, S. Wen, X. Chen, S. Zhao, S. Wang,
L. Chen, X. Luo et al., “SOTER: Guarding black-box inference for
general neural networks at the edge,” in USENIX ATC, 2022.
[15] Z. Sun, R. Sun, C. Liu, A. R. Chowdhury, L. Lu, and S. Jha, “Shad-
ownet: A secure and efficient on-device model inference system for
convolutional neural networks,” in S&P, 2023.
[16] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in CCS,
2015.
[17] R. Gilad-Bachrach, N. Dowlin, K. Laine, K. Lauter, M. Naehrig, and
J. Wernsing, “Cryptonets: Applying neural networks to encrypted data
with high throughput and accuracy,” in ICML, 2016.
[18] P. Mohassel and Y. Zhang, “Secureml: A system for scalable privacy-
preserving machine learning,” in S&P, 2017.
[19] C. Zhang, J. Xia, B. Yang, H. Puyang, W. Wang, R. Chen, I. E. Akkus,
P. Aditya, and F. Yan, “Citadel: Protecting data privacy and model
confidentiality for collaborative learning,” in SoCC, 2021.
[20] T. Alves, “Trustzone: Integrated hardware and software security,” Infor-
mation Quarterly, 2004.
[21] “Intel®
Software
Guard
Extensions
(Intel®
SGX)
Developer
Guide,”
(Accessed
on
July
31,
2023).
[Online].
Avail-
able: https://www.intel.com/content/www/us/en/content-details/671334/
intel-software-guard-extensions-intel-sgx-developer-guide.html
[22] X. Li, B. Zhao, G. Yang, T. Xiang, J. Weng, and R. H. Deng, “A
survey of secure computation using trusted execution environments,”
arXiv preprint arXiv:2302.12150, 2023.
[23] L. K. Ng, S. S. Chow, A. P. Woo, D. P. Wong, and Y. Zhao, “Goten:
Gpu-outsourcing trusted execution of neural network training,” in AAAI,
2021.
[24] Y. Niu, R. E. Ali, and S. Avestimehr, “3legrace: Privacy-preserving dnn
training over tees and gpus,” in PETS, 2022.
[25] J. Hou, H. Liu, Y. Liu, Y. Wang, P.-J. Wan, and X.-Y. Li, “Model
protection: Real-time privacy-preserving inference service for model
privacy at the edge,” TDSC, 2021.
[26] C.-C. Tsai, D. E. Porter, and M. Vij, “Graphene-sgx: A practical library
os for unmodified applications on sgx,” in USENIX ATC, 2017.
[27] “Pytorch: Custom c++ and cuda extensions,” (Accessed on July 31,
2023). [Online]. Available: https://pytorch.org/tutorials/advanced/cpp
extension.html
[28] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in CVPR, 2016.
[29] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,
“An image is worth 16x16 words: Transformers for image recognition
at scale,” in ICLR, 2021.
[30] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal
representations by error propagation,” Tech. Rep., 1985.
[31] G. Hinton, N. Srivastava, and K. Swersky, “Neural networks for machine
learning lecture 6a overview of mini-batch gradient descent,” 2012.
[32] “Amd secure encrypted virtualization (sev),” (Accessed on July 31,
2023). [Online]. Available: https://developer.amd.com/sev/
[33] V. Costan and S. Devadas, “Intel sgx explained,” Cryptology ePrint
Archive, 2016. [Online]. Available: https://eprint.iacr.org/2016/086
[34] J. Van Bulck, M. Minkin, O. Weisse, D. Genkin, B. Kasikci, F. Piessens,
M. Silberstein, T. F. Wenisch, Y. Yarom, and R. Strackx, “Foreshadow:
Extracting the keys to the intel sgx kingdom with transient out-of-order
execution,” in USENIX Sec, 2018.
[35] K. Murdock, D. Oswald, F. D. Garcia, J. Van Bulck, D. Gruss, and
F. Piessens, “Plundervolt: Software-based fault injection attacks against
intel sgx,” in S&P, 2020.
[36] S. Johnson, V. Scarlata, C. Rozas, E. Brickell, and F. Mckeen, “Intel
software guard extensions: Epid provisioning and attestation services,”
White Paper, 2016.
[37] A. F. Agarap, “Deep learning using rectified linear units (relu),” arXiv
preprint arXiv:1803.08375, 2018.
[38] W. Rudin et al., Principles of mathematical analysis.
McGraw-hill
New York, 1976, vol. 3, p. 206.
[39] R. Durstenfeld, “Algorithm 235: random permutation,” Commun. ACM,
1964.
[40] R. Freivalds, “Fast probabilistic algorithms,” in International Symposium
on Mathematical Foundations of Computer Science, 1979.
[41] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur,
G. R. Ganger, P. B. Gibbons, and M. Zaharia, “Pipedream: Generalized
pipeline parallelism for dnn training,” in SOSP, 2019.
[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS,
2017.
[43] C. E. Shannon, “A mathematical theory of communication,” ACM
SIGMOBILE Mobile Computing and Communications Review, 2001.
[44] W.-F. Lin, D.-Y. Tsai, L. Tang, C.-T. Hsieh, C.-Y. Chou, P.-H. Chang, and
L. Hsu, “Onnc: A compilation framework connecting onnx to proprietary
deep learning accelerators,” in AICAS, 2019.
[45] M. Zhou, J. Wu, Y. Liu, S. Liu, and C. Zhu, “Dast: Data-free substitute
training for adversarial attacks,” in CVPR, 2020.
[46] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical black-box attacks against machine learning,” in
AsiaCCS, 2017.
[47] M. Juuti, S. Szyller, S. Marchal, and N. Asokan, “Prada: protecting
against dnn model stealing attacks,” in EuroS&P, 2019.
[48] S. Volos, K. Vaswani, and R. Bruno, “Graviton: Trusted execution
environments on gpus,” in OSDI, 2018.
[49] X. Jiang, M. Kim, K. Lauter, and Y. Song, “Secure outsourced matrix
computation and application to neural networks,” in CCS, 2018.
[50] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Tal-
war, and L. Zhang, “Deep learning with differential privacy,” in CCS,
2016.
[51] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in AISTATS, 2017.
[52] K. Wei, J. Li, M. Ding, C. Ma, H. H. Yang, F. Farokhi, S. Jin, T. Q.
Quek, and H. V. Poor, “Federated learning with differential privacy:
Algorithms and performance analysis,” TIFS, 2020.
[53] G. A. Kaissis, M. R. Makowski, D. R¨uckert, and R. F. Braren, “Secure,
privacy-preserving and federated machine learning in medical imaging,”
Nature Machine Intelligence, 2020.
[54] J. So, R. E. Ali, B. G¨uler, J. Jiao, and A. S. Avestimehr, “Securing
secure aggregation: Mitigating multi-round privacy leakage in federated
learning,” in AAAI, 2023.
[55] J. Wang, S. Guo, X. Xie, and H. Qi, “Protect privacy from gradient
leakage attack in federated learning,” in INFOCOM, 2022.
[56] W. Gao, X. Zhang, S. Guo, T. Zhang, T. Xiang, H. Qiu, Y. Wen, and
Y. Liu, “Automatic transformation search against deep leakage from
gradients,” TPAMI, 2023.
","nanPrior research in privacy-preserving cloud DL has primarily focused on protecting input privacy using TEEs and GPUs, while overlooking model privacy and efficient training. Existing works, such as Slalom and DarKnight, achieve input privacy but fail to protect model parameters, making them vulnerable to theft. Meanwhile, model privacy-preserving systems, like SOTER and Shadownet, are limited to inference tasks and lack support for training. To address these limitations, we introduce Tempo, the first system that simultaneously protects both input and model privacy while enabling efficient training on distributed GPUs."
"False information easily spreads on social media, leading to harmful consequences. Agent-based modeling helps study this spread, and here we build on previous models to incorporate various factors, such as individual characteristics and the influence of opinion leaders. We assume different classes of agents with specific attributes and behaviors, including scholars with better understanding of information, influencers with more neighbors, bots that spread information, and normal agents. Using Facebook data and a network analysis tool, we simulate misinformation diffusion dynamics. Our results show that both strategies can effectively control the misinformation spread.","The rapid spread of misinformation through social media is a growing concern. Misinformation can lead to harmful consequences, such as spreading false health information, polarization of public opinion, and even disrupting democratic processes. Understanding the dynamics of how misinformation spreads through social networks is crucial for developing effective strategies to mitigate its negative impacts.","We used Netlogo to implement and test our assumptions and their effect. Our model is an extension to the SBFC model [23]. So, we built the extra rules and assumptions on top of their NetLogo model [22]. We have four different classes of agents in the network with their specific characteristics: 1) Scholars (""Sc"") who have a clearer understanding of information, can verify the news better, remember the news credibility longer, and their community is concentrated in a cluster so they get more effect from each other as one cluster. 2) The influencers (""I"") who have more influence because of their big neighborhood size, stay in their state (believer, fact-checker) longer, and could have a more verifying probability. 3) Bots or Super-spreaders (""Bot"") who do not change their state and do not forget. They just act as spreaders in the network. They can be believer bots that are programmed to spread the hoax or they can be fact-checker bots that are programmed to mitigate the misinformation diffusion. 4) The normal (""N"") agents who forget their state more frequently, and have less verifying probability.","There are three input variables related to how the misinformation will spread. The hoax credibility (α), the spreading rate (β), and the percentage of initial believers when the simulation starts. According to Table 1, we have two different values for each of these inputs. Figure 4 shows the effect of these values and the sensitivity of each input variable. Among the newly defined classes, two classes of scholars and influencers have their specific probabilities of verifying and forgetting. The first question would be how these specified probabilities can affect the whole network. Table 2 shows the percentage of each state at the end of the simulation based on different probabilities for each class.","In this study, we extended the ‘SBFC’ model by considering different classes of agents with their characteristics. This model can clearly depict the confrontation between agents who believe the false news and agents who know the facts and are fighting against the misinformation spread. That is the main reason we chose to extend this model. For future studies, other models with different dynamics can be explored. We assume that we have scholars, influencers, and two types of bots besides normal (majority) agents in the social network. These assumptions led us to propose two main strategies for dealing with misinformation diffusion. First, we can educate (train) a minor class, like scholars or influencers, to improve their ability to verify the news and remember their beliefs. The second strategy is adding fact-checker bots to the network to spread the facts and influence their neighbors’ states. Our findings show that if we employ both strategies together, the result would be very effective.",Controlling the Misinformation Diffusion in Social Media by the Effect of Different Classes of Agents,"Ali Khodabandeh Yalabadi, Mehdi Yazdani-Jahromi, Sina Abdidizaji, Ivan Garibay, Ozlem Ozmen Garibay","Controlling the Misinformation Diffusion in Social
Media by the Effect of Different Classes of Agents
Ali Khodabandeh Yalabadi1, Mehdi Yazdani-Jahromi2, Sina Abdidizaji1, Ivan
Garibay1, and Ozlem Ozmen Garibay1,2
1 Industrial Engineering and Management Systems, University of Central Florida,
Orlando FL 32816, USA
2 Computer Science, University of Central Florida, Orlando FL 32816, USA
{yalabadi, yazdani, sina.abdidizaji, igaribay, ozlem}@ucf.edu
Abstract. The rapid and widespread dissemination of misinformation
through social networks is a growing concern in today’s digital age. This
study focused on modeling fake news diffusion, discovering the spreading
dynamics, and designing control strategies. A common approach for mod-
eling the misinformation dynamics is SIR-based models. Our approach
is an extension of a model called ’SBFC’ which is a SIR-based model.
This model has three states, Susceptible, Believer, and Fact-Checker.
The dynamics and transition between states are based on neighbors’ be-
liefs, hoax credibility, spreading rate, probability of verifying the news,
and probability of forgetting the current state. Our contribution is to
push this model to real social networks by considering different classes
of agents with their characteristics. We proposed two main strategies for
confronting misinformation diffusion. First, we can educate a minor class,
like scholars or influencers, to improve their ability to verify the news or
remember their state longer. The second strategy is adding fact-checker
bots to the network to spread the facts and influence their neighbors’
states. Our result shows that both of these approaches can effectively
control the misinformation spread.
Keywords: misinformation · agent-based model · social media · fact-
checking.
1
Introduction
The rapid and widespread dissemination of misinformation through social net-
works is a growing concern in today’s digital age. Misinformation can lead to
harmful consequences, such as spreading false health information, polarization of
public opinion, and even disrupting democratic processes. Understanding the dy-
namics of how misinformation spreads through social networks is crucial for de-
veloping effective strategies to mitigate its negative impacts. Agent-based mod-
eling (ABM) has emerged as a powerful tool for studying the spread of misin-
formation in social networks. ABM allows us to simulate individuals’ behavior
and interactions within a social network, making it an ideal method for analyz-
ing the complex dynamics of information diffusion. In this paper, we build on
arXiv:2401.11524v1  [cs.MA]  21 Jan 2024
2
A. Khodabandeh Yalabadi et al.
the work of previous researchers to develop an ABM framework for studying the
spread of misinformation in social networks. Our approach draws on the findings
of Tambuscio et al. [23] and Sulis and Tambuscio [22], who developed a model to
simulate the spread of misinformation in social networks. Their work highlights
the role of fact-checking interventions and network structure in mitigating the
spread of misinformation. This SIR-based (Susceptible, Infected, and Recovered)
model is a perfect model for testing our strategies because it can depict a clear
confrontation between agents who believe the false news and agents who know
the facts and are fighting against misinformation spread. We build on these in-
sights by incorporating additional factors, such as individual characteristics and
the influence of opinion leaders, in our ABM framework. So, we assume that
not all the people (agents) in the network come from the same class with the
same attributes and behaviors. Beskow and Carley [3] introduced two critical
agents of social networks: bots and influencers. These classes can heavily affect
the dynamics of information spreading.
As an assumption, we categorized agents into four classes. 1) scholars who
have a clearer understanding of information can verify the news better, remember
the news credibility longer, and their community is concentrated in a cluster so
they get more effect from each other as one cluster. 2) The influencers who
have more influence because of their big neighborhood size, stay in their state
(believer, fact-checker) longer, and could have a more verifying probability. 3)
The normal agents who are the majority of the network, forget their state more
frequently and have less verifying probability. 4) The bots or Super-spreaders
who do not change their state and do not forget. They just act as spreaders
in the network. They can be believer bots that are programmed to spread the
hoax or they can be fact-checker bots that are programmed to mitigate the
misinformation diffusion. By modeling these assumptions and implementing the
simulation, our ultimate goal is to use our ABM framework to gain insights
into the factors that influence the spread of misinformation in social networks
and to explore the effectiveness of different intervention strategies. This paper
is organized as follows. Section 2 will explain the previous literature related to
our work. In section 3, method and implementation details have been discussed.
We explored the results in section 4, and last but not least, we concluded our
findings in section 5.
2
Related Work
Social media platforms are places where everybody has the ability to spread
their words without a serious intervention of third-party filtering, fact-checking
applications, or editorial judgment [1]. Facebook is one of the most important
social media where people write their ideas and some of them who are called
influencers can get a number of views as many as prominent news agencies
such as Fox News and the New York Times [1]. Recent studies have shown
that around 62% of adults in the US tend to read and get the latest news on
social media [9]. On the other hand, Willmore found that the majority of false
Controlling the misinformation in social media
3
stories were being spread on Facebook more than any other mainstream social
media and news stream. Many people admitted that when they encountered
fake news, they believed its content [24]. People often use their intuition and
heuristic approaches such as mental rules of thumb when they want to evaluate
the authenticity of a claim and evidence. For instance, they ask themselves:
’Have I heard these stories and news before?’, ’Can my knowledge help me to fit
this news into my mind as a true source?’. To some extent, in many conditions,
these approaches could be influential [18]. One of the issues is that people do not
interpret the news and misinformation objectively and in a neutral condition.
However, since they have knowledge and beliefs in advance, they would like to
accept the news that concurs with their point of view and beliefs [11]. Social
media platforms made it possible for fake news generators to spread their beliefs
at an accelerated rate. These networks’ algorithms also made it possible for users
to disseminate information cheaply and quickly. So, as we go further, we see a lot
of people start propagating misleading information intentionally on social media
due to their political, societal, and financial motives [19]. For instance, Balmas
found that Facebook was the most popular platform for the dissemination of
fake news than any other mainstream news media in the 2016 US election [2].
Aside from that, some people use these media to influence people’s beliefs for
their own political gain or religious purposes [19]. Their manipulation of news
allows them to advance their cause by giving fake news authentication.
Increasing literacy is one approach to confront misinformation on social me-
dia. Tully et al. tried to test this way on Twitter by enhancing people’s literacy
about health issues related to genetically modified crops and the flu vaccine.
Two experiments were designed to explore the effect of this approach on expo-
sure to those fields’ misinformation and the users’ literacy about other news in
general. Smith and Seitz examined the effectiveness of corrective information in
debunking myths in Neuroscience. Their findings showed there was a reduction
in believing the neuromyths in Facebook news feeds when readers were exposed
to corrective information immediately after reading the misinformation [21]. Van
der Meer and Jin found out that corrective information had an influence on the
perception of people in times of crisis such as a virus pandemic. They realized
factual information caused people to take more cautionary actions than simple
rebuttals. [15]. In a study, researchers delve into the rationales behind believing
misinformation at an individual level. They found out that not only were some
correction ways ineffective in believing misinformation but also they could back-
fire and make things worse. In spite of ideological beliefs and personal points of
view, cognitive psychological approaches are available for rectifying misinforma-
tion beliefs, which were influential in correcting beliefs in misinformation [14].
Health issues always have difficulties caused by misinformation in social media.
When a pandemic happens, there are rumors about the virus, hygiene and safety
precautions, and vaccination. Bode and Vraga studied ways of correcting mis-
information about the Zika virus on Facebook back in 2018. First, individuals
with low and high conspiracy beliefs had to read misinformation. Two correction
avenues were explored. A group of them accessed correct information through
4
A. Khodabandeh Yalabadi et al.
an algorithm and the other group accessed authentic information generated by
other Facebook members. Comparing the results with a control group has shown
that both ways of introducing correct information, which was through the al-
gorithmic way and social correction, had an influence on the misperception of
misinformation about the Zika virus on Facebook [4]. In another study, the effect
of data included in the news was explored as a way to increase the credibility
of information and distinguish between information and misinformation. Five
different tests were conducted by Du et al. and they reached the conclusion that
when data by itself cannot increase the credibility of the news. However, when
the news was presented with data visualization, people considered it a reliable
source. As there was a statistical difference between news with data and news
with data visualization, it could be interpreted that the appearance of presenting
data increases the credibility of the news, not the data itself [8].
Agent-based modeling (ABM) is one promising approach for studying the
misinformation diffusion phenomenon. ABM allows researchers to simulate the
behavior of individuals in a social network and study how misinformation spreads
over time. Tambuscio et al. [23] proposed an ABM approach to study the effects
of fact-checking on the spread of hoaxes in social networks. Their model found
that fact-checking can significantly reduce the spread of hoaxes, especially when
combined with network interventions such as blocking or suspending accounts.
Sulis and Tambuscio [22] also used ABM to simulate misinformation-spreading
processes in social networks. They demonstrated how the effects of interven-
tions, such as limiting the number of connections or deleting nodes, can impact
the spread of misinformation. Other researchers have used ABM to explore the
role of cognitive biases and social influence on the spread of misinformation. For
example, Del Vicario et al. [7] used an ABM approach to study how confirma-
tion bias can lead to the spread of false information. Their model found that
individuals tend to selectively share information that confirms their pre-existing
beliefs, even if it is false. In addition to ABM, other computational approaches
have been used to study the spread of misinformation, such as network analysis,
natural language processing, and machine learning. For example, Lazer et al. [12]
used network analysis to study the spread of false news on Twitter, while Shu
et al. [19] used natural language processing to detect fake news articles. These
approaches can complement ABM and provide a more comprehensive under-
standing of the mechanisms behind the spread of misinformation. Research on
misinformation can be focused in three directions: automatic detection of fake
news [6], [19], assessing the effect and aspect of it from a psychological perspec-
tive [20], [14], modeling the fake news diffusion and discovering the spreading
dynamics [23], [5], [10]. This work belongs to the third category. Many related
works in this category modeled the rumor (fake news) spreading as a virus that
can infect people. In particular, these models were inspired by compartmental
models in which there is a population of agents, and each agent at each point
in time is in a certain state (compartment). Also, the state of agents can transit
to other states with simple rules by some probability rates. The most famous
models of this category are the SIR (Susceptible, Infected, and Recovered) and
Controlling the misinformation in social media
5
the SIS (Susceptible, Infected, and Susceptible). Different extensions of SIR-
based models have been proposed with different new factors and assumptions.
Among those previous works, there is a model called ""SBFC"" [23] in which
agents have three states (Susceptible, Believer, and Fact-Checker). This model
describes misinformation diffusion as a competition between a piece of fake news
and its debunking. Consider that we have a population of N agents, and each
agent i at each time t has a state of Si(t) that can be one of the following:
– Susceptible (S) is an agent who will change its state based on the news.
– Believer (B), an agent who believes in the hoax.
– Fact-checker (FC), an agent who has verified the news or directly knows that
it is a hoax.
As it was mentioned, misinformation and fact-checking are competing actors and
the main factors are the credibility of the hoax, the spreading rate, the belief of
neighbors, probability of verifying or forgetting the news.
Fig. 1. dynamics of SBFC model [23].
The dynamics are shown in Fig. 1. There are two spreading transitions
(f B
i (t), f F C
i
(t)) that make a susceptible (S) agent become a believer (B) or a fact-
checker (FC). These transitions are defined based on hoax credibility (α), spread-
ing rate (β), and belief of neighbors (nB
i (t): believers, nF C
i
(t): fact-checkers):
f B
i (t) = β
nB
i (t)(1 + α)
nB
i (t)(1 + α) + nF C
i
(t)(1 − α)
(1)
6
A. Khodabandeh Yalabadi et al.
f F C
i
(t) = β
nF C
i
(t)(1 − α)
nB
i (t)(1 + α) + nF C
i
(t)(1 − α)
(2)
Also, agents who are not in the susceptible state, can forget their belief with
a certain probability (pf) and become susceptible again for the next time step
(t + 1). The last factor is the probability of verifying (pv). A believer agent can
become a fact-checker with this probability if it does not forget the news (with
the probability of pv(1 − pf)).(Fig. 1).
This model was originally implemented with R programming and mean-field
analysis. Sulis and Tambuscio [22] implemented the SBFC model in the NetLogo
and got the equivalent results. They just used NetLogo for this simulation and
did not extend the original work.
3
Method
3.1
Social network and assumptions
We used Netlogo to implement and test our assumptions and their effect. Our
model is an extension to the SBFC model [23]. So, we built the extra rules and
assumptions on top of their Netlogo model [22]. We have four different classes
of agents in the network with their specific characteristics:
1. The scholars (""Sc"") who have a clearer understanding of information, can
verify the news better, remember the news credibility longer, and their com-
munity is concentrated in a cluster so they get more effect from each other
as one cluster.
2. The influencers (""I"") who have more influence because of their big neighbor-
hood size, stay in their state (believer, fact-checker) longer, and could have
a more verifying probability.
3. Bots or Super-spreaders (""Bot"") who do not change their state and do not
forget. They just act as spreaders in the network. They can be believer bots
that are programmed to spread the hoax or they can be fact-checker bots
that are programmed to mitigate the misinformation diffusion.
4. The normal (""N"") agents who forget their state more frequently, and have
less verifying probability.
In this work, we considered using the Facebook (social circles) dataset [13]
instead of generating a random network in each run. This network has 4039
nodes, and 88234 edges, and the average degree of nodes is 43.7. Our goal is
to analyze the behavioral change and spread of misinformation in the whole
network (macro-level). With this established network, we can have fixed node
degrees which are helpful for implementing the assumption of influencers. Also,
we need fixed clusters because we want to assign scholars class to one of these
clusters and meet our assumption of a concentrated community of this class
of agents. So, having an identical network for each run is necessary for better
Controlling the misinformation in social media
7
implementing and focusing on the effect of proposed strategies. For clustering,
the asynchronous fluid communities algorithm [17] was used to generate eight
clusters (Fig. 2).
Fig. 2. Facebook network with 8 clusters labeled by different colors.
To implement specific characteristics, we separated the probability of ver-
ifying and forgetting the news for each class. We add initial cluster selection
for scholar class which determines their percentage of the whole network. For
influencers, we assume that the top 1% of agents in terms of connectivity (node
degree) are the influencers. For bots (super spreaders), we assume that the top
10% of agents in terms of connectivity (excluding influencers) are the potential
super spreaders, and bots can be assigned to them randomly with our desired
percentage. Then we update the dynamics of transitions between states for each
class same as the original SBFC model (Eq. 3 and 4).
nB
i (t) = nB
Sci(t) + nB
Ii(t) + nB
Ni(t) + nB
Boti(t)
(3)
nF
i (t) = nF
Sci(t) + nF
Ii(t) + nF
Ni(t) + nF
Boti(t)
(4)
Where the nB
i and nF
i indicate the sum of all believer and fact-checker neighbors
from each class, respectively, the transition from the susceptible state to the
believer or fact-checker can be calculated as before (Eq.1 and 2).
8
A. Khodabandeh Yalabadi et al.
3.2
Experimental setting
We kept the original model [22] probabilities (pf : 0.1, pv : 0.05) for the nor-
mal (majority) class because we want to have only the changes caused by our
assumptions for a meaningful interpretation. For the rest of the inputs, we con-
sidered different values (see Table 1) and used the behavior space tool of the
NetLogo to run the simulations.
Table 1. Inputs information and tested values
The input name
Description
values in experiments
alpha-hoaxCredibility
How believable is the hoax [0,1]
, bigger means more believable
{0.3, 0.8}
beta-spreadingRate
The intensity of neighbors’ effect
on each other for spreading their belief
{0.5, 0.75}
%-of-initial-believers
The percentage of agents how are in believer state
at the beginning of the experiment
{10, 40}
scholars-community
The cluster of the network is assigned to be the scholar community.
We selected three clusters that have significant differences in size.
{""None"", Cluster4: 8.81%,
Cluster7: 13.2%,
Cluster8: 22.13%}
pVerify-scholar
Probability of verifying the hoax for the scholar class
{0.05, 0.1, 0.2, 0.3}
pForget-scholar
Probability of forgetting the state and
become susceptible for the scholar class
{0.02, 0.05, 0.1}
pVerify-influencer
Probability of verifying the hoax for the influencer class
{0.05, 0.1, 0.2}
pForget-influencer
Probability of forgetting the state and
become susceptible for the influencer class
{0.02, 0.05, 0.1}
%-of-B-bot
The percentage of agents how are believer bots
{0, 1}
%-of-F-bot
The percentage of agents how are fact_checker bots
{0, 1}
Ticks/ Steps
We consider each tick as an hour and a 7-day life-cycle for news [16].
168 (7*24) is the total ticks/steps in each run.
{168}
Since our model has stochastic nature, we generated 4 replicates for each
setting. So, the 13824 different setting combinations led to 55296 total runs.
4
Result
4.1
Analysis of replicates
We have 4 replicates for each setting. First, we checked the result variation. To
visualize this, we calculated the standard deviation of the first 2000 settings
based on their replicates. Figure 3 shows that the percentage of deviation to the
total number of agents (4039) does not exceed 5% (mostly below 2%). So, the
difference is not significant. Based on this result, we used the mean of replicates to
represent each setting for the rest of the analyses. This result shows we achieved
our goal of controlling the randomness at the network level, and we can genuinely
explore the effects of different settings for various strategies.
4.2
Analysis of information spread dynamics variables
There are three input variables related to how the misinformation will spread.
The hoax credibility (α), the spreading rate (β), and the percentage of initial
Controlling the misinformation in social media
9
Fig. 3. Standard deviation for each state, based on the replicates.
believers when the simulation starts. According to Table 1, we have two differ-
ent values for each of these inputs. Figure 4 shows the effect of these values and
the sensitivity of each input variable. The percentage of initial believers does
not show a significant difference when the value changed from 10% to 40%. The
minor change is that when it starts with 40% believers, the final result will con-
tain a bit more believers and fewer fact-checkers. A higher rate of spreading (β)
will decrease the final susceptible agents and turn them into either believers or
fact-checkers. So it can have a positive or negative effect based on the network’s
majority state and other variables. The hoax credibility (α) has the most signifi-
cant sensitivity on the outcome and final states. When α = 0.3, the fact-checker
agents are the majority of the network at the end and far more than believers.
With α = 0.8, the situation is reversed and the believers’ count will increase up
to 4 times more than before. In this situation, intense competition will occur
between believers and fact-checkers. So, further strategies and analyses will be
applied to studying the solutions on how to have fewer believers in the hard
circumstance (α = 0.8).
4.3
Analysis of new classes
Among the newly defined classes, two classes of scholars and influencers have
their specific probabilities of verifying and forgetting. The first question would
be how these specified probabilities can affect the whole network. Table 2 shows
the percentage of each state at the end of the simulation based on different prob-
abilities for each class. As mentioned, our goal is to mitigate the misinformation
10
A. Khodabandeh Yalabadi et al.
Fig. 4. Difference between each input value related to spreading dynamics on final
agents’ count for each state. The first two columns from left are for Believers (B), the
next two are for Fact-checkers (F), and the last two are for Susceptibles (S)
spread. So, we are looking for fewer believers and more fact-checkers in the final
output. We define ""educating"" as an action of making the forgetting probabil-
ity decrease and the verifying probability increase. Educating influencers will
reduce the number of believers and increase the number of fact-checkers by 2%.
Educating the scholars’ agents will have a greater impact (8-9%). Although we
chose scholar community number 7 (13.2% of the total) and it is almost 13 times
bigger than the influencers population (1%), the effect intensity is not 13 times
bigger. It is worth mentioning that scholars are more or less a close community
and the spreading effect caused by individuals for influencers’ class is more sig-
nificant. Based on these conclusions, educating both classes is important. It is
worth mentioning we are considering a stochastic level of cooperation after ed-
ucating the agents because we only manipulated the probabilities and it is still
highly possible for them to act in the opposite direction of their education goal.
The next question is what would happen if we had different sizes of scholars’
communities. As mentioned in table 1, we ran our experiments on 4 different
values of scholar communities (None, 4: 8.81%, 7: 13.2%, 8: 22.13%). Figure 5
shows that a larger scholar community can be a game changer. No scholars’ com-
munity (None) setting (green boxes) has a dominant population of believers and
with each expansion in community size, the output will change to the favor of
fact-checkers. Until community number 8 (purple boxes) has a dominant popula-
tion of fact-checkers. For better discrimination, in this plot, we assumed scholars’
agents are highly educated (pverify−scholars = 0.3, and pforget−scholar = 0.02).
The last class of agents in our model is the bots who are programmed to
spread a certain belief. In real social media, there are some accounts that act as
super spreaders and try to infect their neighbors with their beliefs. We call them
Controlling the misinformation in social media
11
Table 2. Comparing the effect of different forgetting and verifying probabilities for
scholar and influencer classes. (α : 0.8, β : 0.5, 10% initial believer, scholar community:
7 (13.2% of the total), and no bots)
Scholar
pf: 0.1
pv: 0.05
pf: 0.05
pv: 0.05
pf: 0.1
pv: 0.1
pf: 0.05
pv: 0.1
pf: 0.1
pv: 0.2
pf: 0.05
pv: 0.2
pf: 0.1
pv: 0.3
pf: 0.05
pv: 0.3
S 18% S 17% S 19% S 17%
S 19% S 17% S 18% S 17%
pf: 0.1 B 50% B 48% B 46% B 46% B 44% B 42% B 43% B 43%
pv: 0.05 F 32% F 35% F 35% F 37%
F 37% F 40% F 39% F 40%
S 18% S 17% S 18% S 17%
S 18% S 17% S 18% S 17%
pf: 0.05 B 49% B 46% B 47% B 45% B 44% B 42% B 43% B 42%
pv: 0.05 F 33% F 37% F 35% F 38%
F 37% F 41% F 39% F 41%
S 18% S 17% S 18% S 17%
S 18% S 17% S 19% S 17%
pf: 0.1 B 48% B 48% B 47% B 45% B 43% B 42% B 42% B 42%
pv: 0.1 F 34% F 35% F 35% F 38%
F 38% F 41% F 39% F 41%
S 18% S 17% S 18% S 17%
S 18% S 17% S 18% S 17%
pf: 0.05 B 48% B 46% B 47% B 43% B 43% B 42% B 42% B 42%
pv: 0.1 F 34% F 37% F 36% F 40%
F 38% F 41% F 40% F 41%
S 18% S 17% S 18% S 17%
S 19% S 17% S 18% S 17%
pf: 0.1 B 48% B 46% B 45% B 44% B 43% B 42% B 42% B 42%
pv: 0.2 F 34% F 37% F 37% F 38%
F 38% F 41% F 40% F 41%
S 18% S 17% S 18% S 17%
S 18% S 17% S 18% S 17%
B 48% B 45% B 46% B 42% B 42% B 41% B 42% B 41%
Influencer
pf: 0.05
pv: 0.2 F 34% F 38% F 37% F 41%
F 40% F 42% F 40% F 42%
Fig. 5. The effect of scholars’ community size on the states of agents. (α = 0.8,
pverify−scholars = 0.3, and pforget−scholar = 0.02)
12
A. Khodabandeh Yalabadi et al.
bots because they do not have the ability to interpret or decide about a piece
of news and do not change their state. Mostly, these agents are disruptive and
try to increase the believers’ agents. One possible strategy for mitigating their
effect is to introduce the opposite bots (fact-checkers bots) to the social network.
Since disruptive bots already exist in the network, it is ethical and even necessary
for higher authorities to control their effect by any effective approaches. So, we
can justify introducing the fact-checker bots only by extensive supervision of
societal-level authorities. Figure 6 shows that adding fact-checker bots can be
extremely effective. Our result shows that having 5% of each type of bots can be
even better than no bots at all. Also, if we have no believer bots in the network,
adding fact-checker bots can reduce the number of believers and enhance the
knowledge of the network about facts. (Our initial experiment space for bots
was 0 and 1% (Table 1), but for more clear analysis, we added 2 - 5% bots and
conducted another experiment.)
Fig. 6. The effect of adding fact-checker bots for mitigating the believer bots disruption.
(α = 0.8, β = 0.5, and 10% of initial believers)
4.4
Two confronting strategies
Up to this point, we found out that educating minor classes of agents (schol-
ars, or influencers) can reduce the misinformation diffusion. Another strategy
Controlling the misinformation in social media
13
Table 3. Different settings for testing the integrated effect of strategies
Setting
Scholars
Influencers
Bots
Community
pv
pf
pv
pf
Believer Fact-checker
Normal
None
0.05 0.1 0.05
0.1
0%
0%
Worst
None
0.05 0.1 0.05
0.1
5%
0%
Best
8 (22%)
0.2 0.05 0.2
0.05
0%
5%
Moderate
4 (9%)
0.2 0.05 0.05
0.05
3%
3%
(α = 0.8, β = 0.5, and 10% of initial believers)
is to add fact-checker bots (super spreaders) and push the crowd’s belief in a
positive direction. By comparing different scenarios, we can see how these two
approaches can improve the states of agents. We designed four settings (see Ta-
ble 3). Figure 7 shows the agents’ states for each setting. All of these settings
are in a severe situation where the hoax credibility is too high (α = 0.8) and it is
too hard to control the misinformation spread. The ’Best’ setting is employing
both strategies at their full potential and the result shows that it can completely
deal with the danger of misinformation because the majority of agents are in the
fact-checker state. The ’Moderate’ setting includes both types of bots with equal
percentages and the educating strategy (first strategy) has not been applied at
the maximum potential. Even with this situation, the ’Moderate’ setting is much
better than the ’Normal’ setting.
5
Conclusion
In this study, we extended the ’SBFC’ model by considering different classes of
agents with their characteristics. This model can clearly depict the confrontation
between agents who believe the false news and agents who know the facts and
are fighting against the misinformation spread. That is the main reason we chose
to extend this model. For future studies, other models with different dynamics
can be explored. We assume that we have scholars, influencers, and two types of
bots besides normal (majority) agents in the social network. These assumptions
led us to propose two main strategies for dealing with misinformation diffusion.
First, we can educate (train) a minor class, like scholars or influencers, to improve
their ability to verify the news and remember their beliefs. The second strategy
is adding fact-checker bots to the network to spread the facts and influence their
neighbors’ states. Our findings show that if we employ both strategies together,
the result would be very effective. Also, planning on how to get the maximum
potential from agents’ education programs is the key to a greater positive impact.
At last, we conclude social networks with a bigger scholar community (users
who have higher news verifying ability) have a better chance of dealing with
misinformation spread.
Availability The Netlogo model can be found on the CoMSES network.
14
A. Khodabandeh Yalabadi et al.
Fig. 7. Change of network agents’ states for different settings. Details of each setting
are in Table 3.
Acknowledgements This work was partially supported by the Defense Ad-
vanced Research Projects Agency (DARPA) under agreement HR00112290104
(PA-21-04-06).
References
1. Allcott, H., Gentzkow, M.: Social media and fake news in the 2016 election. Journal
of economic perspectives 31(2), 211–236 (2017)
2. Balmas, M.: When fake news becomes real: Combined exposure to multiple news
sources and political attitudes of inefficacy, alienation, and cynicism. Communica-
tion research 41(3), 430–454 (2014)
3. Beskow, D.M., Carley, K.M.: Agent based simulation of bot disinformation maneu-
vers in twitter. In: 2019 Winter simulation conference (WSC). pp. 750–761. IEEE
(2019)
4. Bode, L., Vraga, E.K.: See something, say something: Correction of global health
misinformation on social media. Health communication 33(9), 1131–1140 (2018)
5. Budak, C., Agrawal, D., El Abbadi, A.: Limiting the spread of misinformation in
social networks. In: Proceedings of the 20th international conference on World wide
web. pp. 665–674 (2011)
Controlling the misinformation in social media
15
6. Conroy, N.K., Rubin, V.L., Chen, Y.: Automatic deception detection: Methods
for finding fake news. Proceedings of the association for information science and
technology 52(1), 1–4 (2015)
7. Del Vicario, M., Bessi, A., Zollo, F., Petroni, F., Scala, A., Caldarelli, G., Stanley,
H.E., Quattrociocchi, W.: The spreading of misinformation online. Proceedings of
the national academy of Sciences 113(3), 554–559 (2016)
8. Du, Y.R., Zhu, L., Cheng, B.K.: Are numbers not trusted in a “post-truth” era?
an experiment on the impact of data on news credibility. Electronic News 13(4),
179–195 (2019)
9. Gottfried, J., Shearer, E.: News use across social media platforms 2016 (2016)
10. Jin, F., Dougherty, E., Saraf, P., Cao, Y., Ramakrishnan, N.: Epidemiological mod-
eling of news and rumors on twitter. In: Proceedings of the 7th workshop on social
network mining and analysis. pp. 1–9 (2013)
11. Johnson, T.J., Bichard, S.L., Zhang, W.: Communication communities or “cy-
berghettos?”: A path analysis model examining factors that explain selective expo-
sure to blogs. Journal of Computer-Mediated Communication 15(1), 60–82 (2009)
12. Lazer, D.M., Baum, M.A., Benkler, Y., Berinsky, A.J., Greenhill, K.M., Menczer,
F., Metzger, M.J., Nyhan, B., Pennycook, G., Rothschild, D., et al.: The science
of fake news. Science 359(6380), 1094–1096 (2018)
13. Leskovec, J., Mcauley, J.: Learning to discover social circles in ego networks. Ad-
vances in neural information processing systems 25 (2012)
14. Lewandowsky, S., Ecker, U.K., Seifert, C.M., Schwarz, N., Cook, J.: Misinforma-
tion and its correction: Continued influence and successful debiasing. Psychological
science in the public interest 13(3), 106–131 (2012)
15. Van der Meer, T.G., Jin, Y.: Seeking formula for misinformation treatment in
public health crises: The effects of corrective information type and source. Health
Communication 35(5), 560–575 (2020)
16. O’Neill, J.: How long does a news story last? | Vuelio — vuelio.com. https://www.
vuelio.com/uk/blog/how-long-does-a-news-story-last/ (2019)
17. Parés, F., Gasulla, D.G., Vilalta, A., Moreno, J., Ayguadé, E., Labarta, J., Cortés,
U., Suzumura, T.: Fluid communities: A competitive, scalable and diverse commu-
nity detection algorithm. In: Complex Networks & Their Applications VI: Proceed-
ings of Complex Networks 2017 (The Sixth International Conference on Complex
Networks and Their Applications). pp. 229–240. Springer (2018)
18. Richter, T., Schroeder, S., Wöhrmann, B.: You don’t have to believe everything you
read: Background knowledge permits fast and efficient validation of information.
Journal of personality and social psychology 96(3), 538 (2009)
19. Shu, K., Sliva, A., Wang, S., Tang, J., Liu, H.: Fake news detection on social media:
A data mining perspective. ACM SIGKDD explorations newsletter 19(1), 22–36
(2017)
20. Silverman, C.: Journalism: A tow/knight report."" lies, damn lies, and viral content.
Columbia Journalism Review (2015)
21. Smith, C.N., Seitz, H.H.: Correcting misinformation about neuroscience via social
media. Science Communication 41(6), 790–819 (2019)
22. Sulis, E., Tambuscio, M.: Simulation of misinformation spreading processes in so-
cial networks: an application with netlogo. In: 2020 IEEE 7th International Confer-
ence on Data Science and Advanced Analytics (DSAA). pp. 614–618. IEEE (2020)
23. Tambuscio, M., Ruffo, G., Flammini, A., Menczer, F.: Fact-checking effect on viral
hoaxes: A model of misinformation spread in social networks. In: Proceedings of
the 24th international conference on World Wide Web. pp. 977–982 (2015)
16
A. Khodabandeh Yalabadi et al.
24. Willmore, A.: This analysis shows how viral fake election news stories outperformed
real news on facebook (2016)
","Agent-based modeling (ABM) has emerged as a powerful tool for studying the spread of misinformation in social networks. ABM allows us to simulate individuals’ behavior and interactions within a social network, making it an ideal method for analyzing the complex dynamics of information diffusion. In this paper, we build on the work of previous researchers to develop an ABM framework for studying the spread of misinformation in social networks. Our approach draws on the findings of Tambuscio et al. [23] and Sulis and Tambuscio [22], who developed a model to simulate the spread of misinformation in social networks. Their work highlights the role of fact-checking interventions and network structure in mitigating the spread of misinformation. This SIR-based (Susceptible, Infected, and Recovered) model is a perfect model for testing our strategies because it can depict a clear confrontation between agents who believe the false news and agents who know the facts and are fighting against misinformation spread. We build on these insights by incorporating additional factors, such as individual characteristics and the influence of opinion leaders, in our ABM framework. So, we assume that not all the people (agents) in the network come from the same class with the same attributes and behaviors. Beskow and Carley [3] introduced two critical agents of social networks: bots and influencers. These classes can heavily affect the dynamics of information spreading.nan"
"Forest wildfires pose a severe risk to both the environment and human society. Prompt detection of burned areas aids in alleviating the impacts of wildfires and facilitating recovery efforts. Satellite imagery coupled with computer vision techniques shows promise in facilitating the identification of burned areas. This paper introduces CaBuAr, a comprehensive dataset aimed at tackling the burned area delineation problem using satellite images. The dataset consists of pre- and post-fire Sentinel-2 L2A acquisitions of California wildfires from 2015 to 2022. Ground truth masks were generated from official vector data and rasterized. Additionally, three baselines for the burned area delineation task are provided: one utilizing spectral indexes and Otsu's thresholding, one based on the SegFormer model, and one based on the U-Net model. The dataset and baselines can benefit researchers and authorities in areas such as recovery planning, monitoring, and deep learning model development for burned area delineation.","Earth observation has significantly expanded its applications due to increased data availability, storage capacity, and computational power. Leveraging data from Sentinel, Landsat, and Modis missions enables continental-scale information retrieval. Combined with advancements in machine learning and deep learning, this opens up avenues for research in disaster management and recovery. Numerous studies have demonstrated the effectiveness of computer vision architectures in disaster response tasks such as flood delineation, change detection, and burned area delineation. This paper focuses on the latter, introducing a dataset for burned area delineation, a binary image segmentation task that identifies forest areas damaged by wildfires.","The newly created dataset comprises L2A products of Sentinel-2, a European Space Agency (ESA) mission. The area of interest is California, with the geographical distribution of wildfires shown in Figure 2. We collected images of the same area before and after the wildfire. The L2A product contains RGB channels and other spectral bands in the infrared region and ultra blue for a total of 12 channels. Depending on the band, they have a resolution of 10m, 20m, or 60m per pixel.","The proposed dataset consists of a higher number of satellite acquisitions (340 vs 73 vs 227) in California and a higher covered surface (Figure 1). Images are larger in terms of pixels (5490 vs 5000 (max) vs 512) and disclosed as raw data, as directly provided by Copernicus service. On the other hand, the European dataset provides data collected from a third-party service, for which the preprocessing operations are unknown. Furthermore, the monitored range of dates of the new dataset spans from 2015 to 2022, whereas the other two datasets span a smaller time period.","This paper introduced a novel dataset for burned area delineation, CaBuAr, which includes pre- and post-fire Sentinel-2 L2A acquisitions of California wildfires. Three baselines based on spectral indexes, SegFormer, and U-Net models were provided. The dataset and baselines can be used for various tasks, including semantic segmentation, change detection, and performance evaluation of spectral indexes. We plan to expand the dataset to new regions and different satellite acquisitions in future work, making it multimodal. We also aim to release the dataset sampled to different resolutions. The collection of satellite acquisitions is made publicly available to encourage future use and research activities.",CaBuAr: California Burned Areas dataset for delineation,"Daniele Rege Cambrin, Luca Colomba, Paolo Garza","1
CaBuAr: California Burned Areas dataset for
delineation
Daniele Rege Cambrin, Luca Colomba, Paolo Garza
Abstract—Forest wildfires represent one of the catastrophic
events that, over the last decades, caused huge environmental
and humanitarian damages. In addition to a significant amount
of carbon dioxide emission, they are a source of risk to so-
ciety in both short-term (e.g., temporary city evacuation due
to fire) and long-term (e.g., higher risks of landslides) cases.
Consequently, the availability of tools to support local authorities
in automatically identifying burned areas plays an important
role in the continuous monitoring requirement to alleviate the
aftereffects of such catastrophic events. The great availability
of satellite acquisitions coupled with computer vision techniques
represents an important step in developing such tools. This paper
introduces a novel open dataset that tackles the burned area
delineation problem, a binary segmentation problem applied to
satellite imagery. The presented resource consists of pre- and
post-fire Sentinel-2 L2A acquisitions of California forest fires that
took place starting in 2015. Raster annotations were generated
from the data released by California’s Department of Forestry
and Fire Protection. Moreover, in conjunction with the dataset,
we release three different baselines based on spectral indexes
analyses, SegFormer, and U-Net models.
Index Terms—Earth Observation, Deep Learning, Change
Detection, Burned Area Delineation
I. INTRODUCTION
The Earth Observation (EO) field has greatly increased
the number of applications in the last decades thanks to the
greater data availability, storage capacity, and computational
power of modern systems. In fact, leveraging data acquired
by Sentinel, Landsat, and Modis missions as an example, it is
possible to retrieve information at a continental scale in a short
amount of time. This, in conjunction with the development
of modern methodologies in the field of machine learning
and deep learning, represents an extremely interesting area
of research for scientists and authorities from different fields,
such as governments and first responders involved in disaster
response and disaster recovery missions. Phenomena such as
climate change and extreme climate events have a tremendous
societal, economic, and environmental impact, also leading
to humanitarian and environmental losses (e.g., higher risk
of landslide due to a forest fire). Indeed, leveraging EO and
modern deep learning methodologies can provide useful tools
in the area of disaster management and disaster recovery.
Within the research community, numerous previous works
proved the effectiveness of computer vision architectures in
the field of disaster response, such as flood delineation [1],
change detection [2], [3] and burned area delineation [4]–[6].
This paper fits in the last mentioned context. Specifically, we
Daniele Rege Cambrin, Luca Colomba and Paolo Garza are with Politecnico
di Torino, Turin, Italy
release a dataset to tackle the burned area delineation problem,
i.e., a binary image segmentation problem that aims to identify
areas damaged by a forest wildfire. Tackling such a problem
with modern methodologies requires great data availability.
However, the time and cost needed to produce high-quality
annotations severely limit the ability to investigate ad-hoc
solutions in the EO field. For these reasons, we propose a
new dataset related to forest fires in California, collecting
data from the Sentinel-2 mission, which has been specifically
designed to monitor land surface changes [7]. The dataset
is publicly available to the research community at https:
//huggingface.co/datasets/DarthReca/california burned areas.
Ground truth masks for the task of binary image segmen-
tation were generated starting from the public vector data
provided by California’s Department of Forestry and Fire
Protection [8] and rasterized. Satellite acquisitions, i.e., the
raw input data, were instead collected from the Sentinel-2
L2A mission through Copernicus Open Access Hub. More
precisely, we collected and released both pre-fire and post-fire
information associated with the same area of interest.
The contributions of this paper can be summarized as
follows:
• A
novel
image
segmentation
dataset
consisting
of
Sentinel-2 pre- and post-fire acquisitions. Thus, the task
can be addressed with a twofold approach: image seg-
mentation task and change detection task;
• Three different baselines were evaluated on the pro-
posed dataset: one consisting of the evaluation of several
burned area indexes and the Otsu’s automatic threshold-
ing method [9], one based on the SegFormer model [10],
and one based on the U-Net model [11].
The paper is structured as follows. Section II introduces the
related work; Section III introduces the collected dataset and
the preprocessing steps performed, whereas Section IV and
Section V formally introduce the task and the experimental
settings and results. Finally, Section VI concludes the paper.
II. RELATED WORKS
Before the development of deep learning-based methodolo-
gies, domain experts based their analyses on satellite imagery
leveraging spectral index computation and evaluation. Con-
sidering the SAR context, thresholding-based techniques have
been adopted to distinguish between flooded and unflooded
areas [12]. Different analyses have been performed on various
tasks concerning several spectral indexes such as in cloud
detection (cloud mask) [13], water presence (WP, NDWI) [14],
[15] and vegetation analysis (NDVI) [16].
arXiv:2401.11519v1  [cs.CV]  21 Jan 2024
2
Considering the burned area delineation problem, domain
experts have developed several indexes: NBR, NBR2, BAI,
and BAIS2 [14]. They are computed using different spectral
bands to generate an index highlighting the affected areas
of interest. Such techniques are often coupled with thresh-
olding methodologies: either fixed or manually calibrated
threshold values are chosen [17], or automatic thresholding
algorithms are used [18]. Additional studies evaluate index-
based techniques with additional in-situ information, namely
the Composite Burned Area Index (CBI), which indeed pro-
vides insightful information but does not represent a scalable
solution because in-situ data are incredibly costly to collect.
Furthermore, studies confirmed that finding a unique threshold
that is region- and vegetation-independent is difficult [19].
More recently, researchers started adopting supervised
learning techniques to solve several tasks in computer vision
and EO. More precisely, CNN-based models proved their
effectiveness in image classification and segmentation tasks,
achieving state-of-the-art performances compared to index-
based methodologies [20], [21]. The main drawback is the
need for a significant amount of labeled data, possibly covering
heterogeneous regions with different morphological features
to learn better representations. Over the years, many of the
proposed frameworks limit their analyses to a few samples
collected from a limited number of countries or locations [22].
In the EO domain, different public datasets are available
to the research community tackling different problems, such
as flood delineation [12], crop classification and segmenta-
tion [23] but, to the best of our knowledge, only two public
datasets are available for the burned area delineation problem
covering some countries in Europe [24] and Indonesia [25].
The dataset proposed in this paper collects both pre- and
post-fire Sentinel-2 L2A data from California forest fires,
limiting seasonal and phenological differences between the
two acquisitions as explained in the following paragraphs.
Table I shows a comparison between the three datasets.
The proposed dataset consists of a higher number of satellite
acquisitions (340 vs 73 vs 227) in California and a higher
covered surface (Figure 1). Images are larger in terms of pixels
(5490 vs 5000 (max) vs 512) and disclosed as raw data, as
directly provided by Copernicus service. On the other hand,
the European dataset provides data collected from a third-party
service, for which the preprocessing operations are unknown.
Furthermore, the monitored range of dates of the new dataset
spans from 2015 to 2022, whereas the other two datasets span
a smaller time period.
III. DATASET
The newly created dataset comprises L2A products of
Sentinel-2, a European Space Agency (ESA) mission. The area
of interest is California, with the geographical distributions of
the events shown in Figure 2. We collected images of the same
area before and after the wildfire. It is essential to note that
the L2A product contains RGB channels and other spectral
bands in the infrared region and ultra blue for a total of 12
channels. Depending on the band, they have a resolution of
10m, 20m, or 60m per pixel.
Fig. 1: California administrative boundaries (red) vs satellite
tiles of the proposed dataset (blue).
TABLE I: Comparison between datasets. TD is the time
difference between pre-fire and post-fire acquisitions.
CaBuAr (ours)
[24]
[25]
Region
California
Europe
Indonesia
Mission
Sentinel-2
Sentinel-1/2
Landsat-8
Resolution
20m
10m (S2)
30m
Image size
5490 × 5490
up to 5000 × 5000
512 × 512
Raw data
✓
✗
✓
Channels
12
12
8
Forest Fires
340
73
227
Start date
Jan, 2015
July, 2017
Jan, 2019
End date
Dec, 2022
July, 2019
Dec, 2021
Total surface
∼450 000 km2
∼19 000 km2
∼46 000 km2
Post-fire
✓
✓
✓
Pre-fire
✓
✓
✗
TD
∼1 year
≤ 2 months
/
A. Preprocessing
The California Department of Forestry and Fire Protection
publicly provides the ground truth vector information, which
we converted into raster images. Each pixel contains a binary
value: 1 in the burned area and 0 in the case of undamaged
areas. Although the registered wildfires span from 1898 to
2022, we collected data only for wildfires from 2015 onwards
because there were no Sentinel-2 images before 2015. We
gathered the Sentinel-2 images directly from Copernicus Open
Access Hub.
Post-wildfire images are collected within one month after
124
122
120
118
116
114
Longitude
34
36
38
40
42
Latitude
Fig. 2: Geographical distribution of wildfires (red) within the
California boundaries (blue).
3
Post-fire
Pre-fire
Mask
Fig. 3: Example of pre-fire and post-fire RGBs and relative
masks.
the containment date. A total number of 340 acquisitions
were downloaded, each being of size 5490 × 5490 pixels
with a resolution of 20m per pixel. Sentinel-2 bands with
different resolutions were either upsampled or downsampled
with bicubic interpolation to reach the target resolution.
Pre-fire images have the same size and resolution as the
post-fire acquisitions. To enforce coherence and similar sea-
sonal and phenological conditions, we downloaded pre-fire
data considering a temporal window of 4 weeks, centered
one year before the date post-fire data were collected. For
example, given a post-fire acquisition collected in 2018/04/01,
we downloaded the products available between 2017/03/18
and 2017/04/15, with center 2017/04/01. This ensures similar
climatic and seasonal conditions, thus limiting environmental
changes as much as possible. In some cases, retrieving these
products was impossible due to data unavailability, i.e., not
all wildfires have a pre-fire acquisition satisfying such a
constraint. Given the 340 wildfires considered in this study,
208 have pre-fire availability satisfying the abovementioned
constraint.
The dataset was randomly split into five non-overlapped
folds with the positive class (burned area) equally distributed
to perform cross-validation.
B. Manual inspection
After collecting data, we manually evaluated each post-
fire image using RGB channels. This was done to (i) discard
invalid samples and (ii) enrich the dataset with metadata and
comments based on our subjective evaluation. We remark that
such comments are not helpful for the final prediction task but
can be used to better characterize the data. Our evaluation is
associated with each satellite acquisition.
Each image has a metadata field with a list of numeric
codes generated from the manual inspection. Figure II reports
the code-to-comment association. As can be seen, different
climatic conditions can be found in the dataset. Figure 4
reports some examples of post-fire images. For each post-
fire acquisition, Figure 4 reports its RGB version (first line),
TABLE II: Association between code and comment.
comment
meaning
0
Affected area is in the incomplete region
1
Image is incomplete
2
Small burned area
3
Mask has a small offset
4
Mask is totally wrong
5
Extensive burned area
6
Clouds over the burned area
7
Too many clouds over the image
8
Wildfire ongoing
9
Snow on the burned area
10
Mask seems smaller than the burned area
11
Mask seems bigger than the burned area
12
Mask is in the missing data area
13
Part of the mask is outside the area
Post-fire
9
2-11
5-11
6-13
Mask
Fig. 4: Sample of post-fire RGBs and masks with the associ-
ated comments.
its binary mask (second line), and the comment(s) assigned
to it (on top of the RGB image). For instance, the second
acquisition has two comments: 2 and 11. We noted that some
masks seem to overestimate the burned area. However, our
perception refers to the RGB version of the images, i.e., to a
subset of the available information. Moreover, our subjective
perception can be biased also because the regions at the
borders of burned areas are usually less damaged than the
central ones. These notes must be extended to other mask-
related comments, but they are rarer. Comments are equally
distributed among the folds. All the images marked with
comments that can negatively affect results and the dataset’s
quality (i.e., 4, 8, and 12) were discarded.
Finally, each pre-fire image was manually inspected to
verify its validity, but no new comment types were added.
All invalid pre-fire acquisitions were discarded.
IV. TASK
The proposed dataset can be used as a benchmark for differ-
ent tasks in supervised and unsupervised scenarios: (i) binary
segmentation based on post-fire acquisitions only, (ii) binary
segmentation based on pre-fire and post-fire acquisitions,
(iii) binary change detection on pre-fire and post-fire images,
and (iv) performance assessment of spectral indexes.
Tasks (i) and (ii) involve developing and training supervised
learning algorithms to perform pixel-level prediction based on
one or two acquisitions, respectively. Pixels are labeled either
as burned or undamaged.
Task (iii) can leverage unsupervised learning to extract
differences between the two products and eventually exploit
4
the ground truth to focus the task on detecting changes due to
a wildfire.
Instead, Task (iv) may be of greater interest to domain
experts in developing new spectral indexes designed for burned
area identification. As such, researchers may use the dataset to
evaluate the quality of the index by computing the separability
index (SI) [14].
V. EXPERIMENTS
Our experiments test various classical and deep learning
methods for three different settings:
1) Usage of all the available post-fire images (Setting 1);
2) Usage of the subset of post-fire images for which the
pre-fire data is available (Setting 2);
3) Usage of post-fire and pre-fire images. Thus, two input
images are provided. Spectral indexes in this setting
were evaluated by computing the difference between
pre-fire and post-fire indexes (Setting 3).
The code for the experiments can be found at https://github.
com/DarthReca/CaBuAr.
A. Experimental settings
The encoder of SegFormer is initialized with the original
weights for Image-Net duplicated four times to handle the
12 available channels for Sentinel-2 L2A acquisitions. U-Net
is instead randomly initialized. The batch size was set to 8.
We used the AdamW optimizer with an initial learning rate
of 0.001, decreased by a factor of 10 every 15 epochs, and
a weight decay of 0.01 for every considered model. We used
the well-known Dice loss [26] as the loss function. All models
were trained on a single Tesla V100 32GB GPU. The testing
was made using the weights associated with the best validation
loss.
Due to the size of the original input images (5490 × 5490),
we split them into patches of size 512×512. Furthermore, due
to class imbalance, we kept only those patches containing at
least one pixel associated with the positive class and no clouds
over the area of interest (comment 6). A total of 534 patches
for setting (1) and 356 for settings (2) and (3) were obtained.
The statistics and performances reported in the remainder
of the paper refer to the data obtained after the split-and-
filter process mentioned earlier. All training and evaluation
procedures were performed with a cross-validation approach.
The same criterion was also applied for spectral indexes
methodologies to obtain comparable results, despite the ab-
sence of a trainable model. The reported values are expressed
as mean and standard deviation computed over the five folds.
In Figure 5, we highlight the distribution of burned pixels
percentage per image in each fold. The problem is unbalanced,
but the different folds share a similar distribution.
B. Spectral Indexes
We evaluated several spectral indexes (NBR, NBR2, BAI,
BAIS2) for the burned area delineation task.
In particular, to assess the performances on the dataset,
we computed the Separability Index (visible in Table III),
0
1
2
3
4
Fold
0.0
0.2
0.4
0.6
0.8
1.0
Burned pixels per image (%)
(a) Setting (1).
0
1
2
3
4
Fold
0.0
0.2
0.4
0.6
0.8
1.0
Burned pixels per image (%)
(b) Settings (2) and (3).
Fig. 5: Burned pixels percentage per image per fold.
TABLE III: Separability Indexes (SI) and metrics computed
for each setting and each evaluated index.
Setting
Index
SI
F1 Score
IoU
(1)
NBR
0.294
0.150±0.231
0.103±0.180
NBR2
0.224
0.226±0.269
0.159±0.209
BAI
0.044
0.040±0.121
0.026±0.086
BAIS2
0.027
0.194±0.292
0.148±0.252
(2)
NBR
0.320
0.106±0.196
0.071±0.150
NBR2
0.349
0.243±0.278
0.172±0.218
BAI
0.052
0.037±0.115
0.024±0.079
BAIS2
0.002
0.086±0.174
0.057±0.138
(3)
dNBR
0.247
0.114±0.212
0.079±0.168
dNBR2
0.189
0.218±0.281
0.157±0.225
dBAI
0.040
0.066±0.161
0.045±0.127
dBAIS2
0.027
0.047±0.126
0.030±0.099
which quantifies how well the index under analysis discerns
between burned and unburned regions, i.e., a higher value of
the SI implies that, in general, damaged areas have a different
distribution of the spectral index. We apply Otsu’s method to
quantify the indexes’ segmentation performances. Results are
shown in Table III, which confirm the poor performances in
terms of F1-Score and IoU. Additional pre-fire information
(Setting (3)) does not significantly improve the evaluation
metrics. Figure 6 shows an example of predictions for the
cited indexes and Otsu. In this case, BAI and NBR achieve
better scores, but many disturbances affect the final result in
the unburned regions.
C. Deep learning models
We tested two deep learning architectures for semantic
segmentation: a CNN (U-Net [11]) and a Vision Transformer
(SegFormer [10]). We decided to take into account two dif-
ferent versions of SegFormer (B0, the smallest version, and
B3, a mid-range version) that differ only in size and so in the
number of parameters. U-Net, SegFormer-B0, and SegFormer-
B3 consist of 31M, 3.8M, and 47M parameters, respectively.
To deal with Setting (3), the two input images (pre- and post-
fire) are concatenated along the channel axis, creating patches
of size 24 × 512 × 512 (C × H × W). We reported the results
for the different settings and models in Table IV.
Image
NBR
NBR2
BAIS2
BAI
Ground Truth
Fig. 6: Example of segmentation using Otsu’s method for
different indexes.
5
TABLE IV: Metrics calculated for each deep learning model
evaluated.
Setting
Metric
SegFormerB3
SegFormerB0
U-Net
(1)
F1 Score
0.620±0.009
0.686±0.004
0.707±0.004
IoU
0.497±0.008
0.563±0.004
0.583±0.004
(2)
F1 Score
0.583±0.014
0.654±0.003
0.705±0.002
IoU
0.447±0.012
0.535±0.003
0.577±0.002
(3)
F1 Score
0.533±0.003
0.499±0.009
0.625±0.002
IoU
0.401±0.003
0.370±0.007
0.494±0.002
Without any specific pre-training, U-Net provides in every
setting the best performance. SegFormer-B0, which is also
lighter than U-Net, provides comparable performance, having
some difficulties only with Setting (3). SegFormer-B3 does not
justify the greater complexity considering its results.
Figure 7 reports the predictions of these models on the same
input sample shown in Figure 6. The most evident difference
is deep models tend to be more precise and less affected by
disturbances.
Image
SegFormerB0
SegFormerB3
U-Net
Ground Truth
Fig. 7: Examples of prediction with deep learning models.
VI. CONCLUSION
This paper introduced a new dataset for burned area delin-
eation containing samples with different morphological fea-
tures and some of California’s largest wildfires. The dataset
includes both pre-fire and post-fire data.
We provided baselines based on different approaches to
encourage the investigation in various research areas, such
as semantic segmentation and change detection. This publicly
available dataset can benefit researchers and public authorities
for many tasks, such as recovery planning, constant monitoring
of affected areas, and developing deep learning models for
burned area delineation. We plan to extend the dataset to new
regions and different satellite acquisitions in future work, mak-
ing the dataset multimodal. Additionally, we plan to release
the dataset sampled to different resolutions. The collection of
satellite acquisitions is made publicly available to encourage
future use and research activities.
REFERENCES
[1] Z. Dong, G. Wang, S. O. Y. Amankwah, X. Wei, Y. Hu, and A. Feng,
“Monitoring the summer flooding in the Poyang Lake area of China
in 2020 based on Sentinel-1 data and multiple convolutional neural
networks,” International Journal of Applied Earth Observation and
Geoinformation, vol. 102, p. 102400, 2021.
[2] A. Asokan and J. Anitha, “Change detection techniques for remote
sensing applications: a survey,” Earth Science Informatics, vol. 12, no. 2,
pp. 143–160, 2019.
[3] J. Sublime and E. Kalinicheva, “Automatic post-disaster damage map-
ping using deep-learning techniques for change detection: Case study of
the Tohoku tsunami,” Remote Sensing, vol. 11, no. 9, p. 1123, 2019.
[4] R. Lasaponara and B. Tucci, “Identification of burned areas and severity
using SAR Sentinel-1,” IEEE Geoscience and Remote Sensing Letters,
vol. 16, no. 6, pp. 917–921, 2019.
[5] D. R. Cambrin, L. Colomba, and P. Garza, “Vision transformers for
burned area delineation,” in Conference on Machine Learning and
Principles and Practice of Knowledge Discovery in Databases, 2022.
[6] M. A. Tanase, M. A. Belenguer-Plomer, E. Roteta, A. Bastar-
rika, J. Wheeler, ´A. Fern´andez-Carrillo, K. Tansey, W. Wiedemann,
P. Navratil, S. Lohberger et al., “Burned area detection and mapping:
Intercomparison of Sentinel-1 and Sentinel-2 based algorithms over
tropical Africa,” Remote Sensing, vol. 12, no. 2, p. 334, 2020.
[7] Sentinel-2 mission guide. https://sentinel.esa.int/web/sentinel/missions/
sentinel-2. Accessed on: 2023/04/14.
[8] California department of forestry and fire protection. https://www.fire.
ca.gov/. Accessed on: 2023/04/14.
[9] N. Otsu, “A threshold selection method from gray-level histograms,”
IEEE Transactions on Systems, Man, and Cybernetics, vol. 9, no. 1, pp.
62–66, 1979.
[10] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,
“SegFormer: Simple and Efficient Design for Semantic Segmentation
with Transformers,” in Advances in Neural Information Processing
Systems, M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W.
Vaughan, Eds., vol. 34.
Curran Associates, Inc., 2021, pp. 12 077–
12 090.
[11] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional Net-
works for Biomedical Image Segmentation,” in Lecture Notes in Com-
puter Science.
Springer International Publishing, 2015, pp. 234–241.
[12] D. Bonafilia, B. Tellman, T. Anderson, and E. Issenberg, “Sen1floods11:
A georeferenced dataset to train and test deep learning flood algorithms
for sentinel-1,” in Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) Workshops, June 2020.
[13] R. A. Frey, S. A. Ackerman, R. E. Holz, S. Dutcher, and Z. Griffith, “The
continuity modis-viirs cloud mask,” Remote Sensing, vol. 12, no. 20,
2020.
[14] F. Filipponi, “Bais2: Burned area index for sentinel-2,” Proceedings,
vol. 2, no. 7, 2018.
[15] A. Fisher, N. Flood, and T. Danaher, “Comparing landsat water index
methods for automated water classification in eastern australia,” Remote
Sensing of Environment, vol. 175, pp. 167–182, 2016.
[16] N. Pettorelli, J. O. Vik, A. Mysterud, J.-M. Gaillard, C. J. Tucker, and
N. C. Stenseth, “Using the satellite-derived ndvi to assess ecological
responses to environmental change,” Trends in Ecology & Evolution,
vol. 20, no. 9, pp. 503–510, 2005.
[17] L. Saulino, A. Rita, A. Migliozzi, C. Maffei, E. Allevato, A. P. Garonna,
and A. Saracino, “Detecting burn severity across mediterranean forest
types by coupling medium-spatial resolution satellite imagery and field
data,” Remote Sensing, vol. 12, no. 4, 2020.
[18] W. Bin, L. Ming, J. Dan, L. Suju, C. Qiang, W. Chao, Z. Yang, Y. Huan,
and Z. Jun, “A method of automatically extracting forest fire burned
areas using gf-1 remote sensing images,” in IGARSS 2019 - 2019 IEEE
International Geoscience and Remote Sensing Symposium, 2019, pp.
9953–9955.
[19] C. A. Cansler and D. McKenzie, “How robust are burn severity indices
when applied in a new region? Evaluation of alternate field-based and
remote-sensing methods,” Remote sensing, vol. 4, no. 2, pp. 456–483,
2012.
[20] L. Knopp, M. Wieland, M. R¨attich, and S. Martinis, “A deep learning
approach for burned area segmentation with Sentinel-2 data,” Remote
Sensing, vol. 12, no. 15, p. 2422, 2020.
[21] A. Farasin, L. Colomba, G. Palomba, G. Nini, and C. Rossi, “Supervised
Burned Areas delineation by means of Sentinel-2 imagery and Convo-
lutional Neural Networks,” in ISCRAM 2020, Virginia Tech, Blacksburg,
VA, USA, 2020, pp. 24–27.
[22] Q. Safder, H. Zhang, and Z. Zheng, “Burnt area segmentation with
densely layered capsules,” in IGARSS 2022 - 2022 IEEE International
Geoscience and Remote Sensing Symposium, 2022, pp. 2199–2202.
[23] D. Sykas, M. Sdraka, D. Zografakis, and I. Papoutsis, “A sentinel-2
multi-year, multi-country benchmark dataset for crop classification and
segmentation with deep learning,” IEEE Journal of Selected Topics in
Applied Earth Observations and Remote Sensing, 2022.
[24] L. Colomba, A. Farasin, S. Monaco, S. Greco, P. Garza, D. Apiletti,
E. Baralis, and T. Cerquitelli, “A dataset for burned area delineation and
severity estimation from satellite imagery,” in CIKM2022, ser. CIKM
’22.
ACM, 2022, p. 3893–3897.
[25] Y. Prabowo, A. D. Sakti, K. A. Pradono, Q. Amriyah, F. H. Rasyidy,
I. Bengkulah, K. Ulfa, D. S. Candra, M. T. Imdad, and S. Ali, “Deep
learning dataset for estimating burned areas: Case study, indonesia,”
Data, vol. 7, no. 6, 2022.
[26] C. H. Sudre, W. Li, T. Vercauteren, S. Ourselin, and M. Jorge Cardoso,
“Generalised dice overlap as a deep learning loss function for highly
unbalanced segmentations,” in Deep learning in medical image analysis
and multimodal learning for clinical decision support.
Springer
International Publishing, 2017, pp. 240–248.
","Before the advent of deep learning methodologies, domain experts relied on satellite imagery analysis, employing spectral index computations and evaluations. SAR-based studies utilized thresholding techniques to distinguish flooded and unflooded areas. Various analyses were conducted on tasks related to spectral indexes, including cloud detection, water presence, and vegetation analysis. For the burned area delineation problem, domain experts developed several indexes that leverage different spectral bands to highlight affected areas of interest. These techniques are often paired with thresholding methodologies. Additional studies evaluated index-based techniques with in-situ information, but such data collection is costly and time-consuming. Recently, supervised learning techniques have been employed to solve computer vision and EO tasks, achieving state-of-the-art performances compared to index-based methodologies. The main challenge lies in the need for a significant amount of labeled data, covering heterogeneous regions with different morphological features. Existing frameworks often limit their analyses to a few samples collected from a limited number of countries or locations. In the EO domain, various public datasets are available for flood delineation, crop classification and segmentation, but only two are available for the burned area delineation problem, covering regions in Europe and Indonesia.nan"
"In this paper, we apply machine learning techniques to study the quark sector of the Standard Model. The procedure aims to find a Yukawa model that is both experimentally consistent and aesthetically pleasing. We define loss functions whose minimization results in true models that are also beautiful as measured by three criteria: uniformity, sparsity, or symmetry.","The task of a theoretical physicist is to develop a theory model which describes a specific set of natural physics phenomena. The standard model is the most widely accepted model of fundamental particle physics. It has accurately predicted a wide range of experimental phenomena. Despite its success, the standard model has not been able to explain all the forces in nature. This has led to a quest for new theoretical models that can explain these forces. The ultimate goal is to find a ""theory of everything"" that unifies all the forces of nature.","In this study, we employ machine learning techniques to find a Yukawa model that is both experimentally consistent and aesthetically pleasing. We define loss functions whose minimization results in true models that are also beautiful as measured by three criteria: uniformity, sparsity, or symmetry. We conduct several pseudo-experiments to evaluate the performance of our approach.","Our results show that our approach can find Yukawa models that are both experimentally consistent and aesthetically pleasing. The loss values achieved through training are minimal, signifying the potential viability of the Yukawa textures under consideration. We also observe that the trained models exhibit the desired aesthetic elegance as defined by the chosen quantitative benchmarks.","In this paper, we have applied machine learning techniques to study the quark sector of the Standard Model. We have shown that our approach can find Yukawa models that are both experimentally consistent and aesthetically pleasing. Our work provides a new way to study the fundamental forces of nature and has the potential to lead to the discovery of new theoretical models.",Exploring the Truth and Beauty of Theory Landscapes with Machine Learning,"Konstantin T. Matchev, Katia Matcheva, Pierre Ramond, Sarunas Verner","Exploring the Truth and Beauty of Theory Landscapes with Machine Learning
Konstantin T. Matcheva,1, Katia Matchevaa,1, Pierre Ramonda,1, Sarunas Vernera,1
aInstitute for Fundamental Theory, Physics Department, University of Florida, Gainesville, FL, 32611, USA
Abstract
Theoretical physicists describe nature by i) building a theory model and ii) determining the model parameters. The
latter step involves the dual aspect of both fitting to the existing experimental data and satisfying abstract criteria like
beauty, naturalness, etc. We use the Yukawa quark sector as a toy example to demonstrate how both of those tasks
can be accomplished with machine learning techniques. We propose loss functions whose minimization results in true
models that are also beautiful as measured by three different criteria — uniformity, sparsity, or symmetry.
1. Introduction
The task of a theoretical physicist is to develop a theory model describing a set of natural physics phenomena. The
first step, often requiring great insight and inspiration, is to choose a theoretical framework, which inevitably depends
on a certain set of input parameters {Pi}, i = 1, 2, . . . , NP. The second step is more straightforward, and involves
determining the numerical values of these parameters which “were chosen by nature”. There are two aspects of this
parameter-fitting process:
• Truth. In order to be viable, a model has to be above all truthful. We take this to mean that the model correctly
accounts for the existing measurements of all independent experimental observables {Oα}, α = 1, 2, . . . , NO,
sensitive to the parameters {Pi}. This is accomplished by tuning the model parameters {Pi} until the model
predictions fit the data. This adjustment can typically be accomplished successfully, since in most cases the
number of tunable model parameters exceeds the number of available independent measurements, i.e., NP > NO.
• Beauty. After this fitting procedure, we are typically still left with NP − NO undetermined model parameters.2
Those remaining degrees of freedom can then be adjusted in order to make the model more “beautiful”.
However, beauty is in the eye of the beholder, and there is no single universally accepted notion of “beauty” in
theoretical physics. To this day, such concepts as fine-tuning, naturalness, simplicity, etc., continue to be hotly
debated in the theoretical community.
However, from a machine learning standpoint there is nothing mysterious about “beauty”, as long as it can be
quantified, i.e., one adopts an agreed-upon, community-wide quantitative measure indicating the “beauty” of a model.
In the past, a number of such measures have been introduced to quantify the fine-tuning in new physics models like
low-energy supersymmetry [1–5]. Once a quantitative measure of the model’s beauty is adopted, the fitting of the
model parameters becomes a simple optimization problem amenable to machine learning approaches.
The procedure described above is schematically illustrated in Fig. 1, which shows a plot of the quantitative measure
of “beauty” (vertical axis) versus the model parameters (for ease of illustration, we choose NP = 2). In the absence
of any experimental data, the theorists would agree that model “1” is “the fairest of them all”. However, experimental
data typically ends up modifying this naive conclusion. For example, consider the measurement of a single observable
(i.e., the case of NO = 1), which enforces a constraint among the model parameters. This constraint is schematically
1All authors share equal contributions to this paper.
2Some of the remaining parameters could perhaps be probed in future experiments by measuring additional observables. The most common
example of this sort is given by the masses and couplings of particles yet to be discovered.
Preprint submitted to Physics Letters B
January 23, 2024
arXiv:2401.11513v1  [hep-ph]  21 Jan 2024
<latexit sha1_base64=""vGYmqU/T4i8H
0BYHwDPpfQIAwo="">AB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKqMcQLx4jmAckS
5id9CZDZh/O9AaXJd/hxYMiXv0Yb/6Nk2QPmljQUFR1093lxVJotO1vq7C2vrG5Vdwu
7ezu7R+UD49aOkoUhyaPZKQ6HtMgRQhNFCihEytgSeh7Y1vZ357AkqLKHzANAY3YMN
Q+IzNJLbQ3jCrA4swXTaL1fsqj0HXSVOTiokR6Nf/uoNIp4ECKXTOuY8foZkyh4B
KmpV6iIWZ8zIbQNTRkAWg3mx89pWdGVA/UqZCpHP190TGAq3TwDOdAcORXvZm4n9eN
0H/xs1EGCcIV8s8hNJMaKzBOhAKOAoU0MYV8LcSvmIKcbR5FQyITjL6+S1kXVuape
3l9WavU8jiI5IafknDjkmtTIHWmQJuHkTyTV/JmTawX6936WLQWrHzmPyB9fkDadm
SiA=</latexit>Beauty
<latexit sha1_base64=""yCgdG4kxaD8oSgT721sNP4Qt8Ao="">AB6nicbVBNS8N
AEJ3Ur1q/qh69LBbBU0mkqMeiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeR
IGSdxLNaRI3g7GtzO/cS1EbF6xEnC/YgOlQgFo2ilh0bf65crbtWdg6wSLycVyNHol796g5ilEVfIJDWm67kJ+hnVKJjk01IvNTyhbEyHvGupohE3fjY/dUrOrDIgYaxtKSR
z9fdERiNjJlFgOyOKI7PszcT/vG6K4bWfCZWkyBVbLApTSTAms7/JQGjOUE4soUwLeythI6opQ5tOyYbgLb+8SloXVe+yWruvVeo3eRxFOIFTOAcPrqAOd9CAJjAYwjO8wpsjn
Rfn3flYtBacfOY/sD5/AHRn42B</latexit>P1
<latexit sha1_base64=""8XwCUehtEqw
EzmOyZU0Y3ncpu8="">AB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lKUY9FLx4r2lpo
Q9lsN+3SzSbsToQS+hO8eFDEq7/Im/GbZuDtj4YeLw3w8y8IJHCoOt+O4W19Y3NreJ
2aWd3b/+gfHjUNnGqGW+xWMa6E1DpVC8hQIl7ySa0yiQ/DEY38z8xyeujYjVA04S7k
d0qEQoGEUr3Tf7tX654lbdOcgq8XJSgRzNfvmrN4hZGnGFTFJjup6boJ9RjYJPi31
UsMTysZ0yLuWKhpx42fzU6fkzCoDEsbalkIyV39PZDQyZhIFtjOiODL3kz8z+umGF7
5mVBJilyxaIwlQRjMvubDITmDOXEsq0sLcSNqKaMrTplGwI3vLq6Rdq3oX1fpdv
dK4zuMowgmcwjl4cAkNuIUmtIDBEJ7hFd4c6bw4787HorXg5DPH8AfO5w/TI42C</la
texit>P2
<latexit sha1_base64=""DSs/otLGM3tGcbw7Y+b2owP1uy4="">A
B6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm/GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xu
bW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOlptcvV9yqOwdZJV5OKpCj0S9/9QYx
SyOUhgmqdzE+NnVBnOBE5LvVRjQtmYDrFrqaQRaj+bHzolZ1YZkDBWtqQhc/X3REYjrSdRYDsjakZ62ZuJ/3nd1ITXfsZlkhqUbLEoTAUxM
Zl9TQZcITNiYglitbCRtRZmx2ZRsCN7y6ukfVH1Lqu1Zq1Sv8njKMIJnMI5eHAFdbiDBrSAcIzvMKb8+i8O/Ox6K14OQzx/AHzucPf
YuMvg=</latexit>1
<latexit sha1_base64=""YMsxORc1qROyZt/Sshecb25Lu40="">A
B6HicbVDLTgJBEOzF+IL9ehlIjHxRHYJUY9ELx4hkUcCGzI79MLI7OxmZtaEL7AiweN8eonefNvHGAPClbSaWqO91dQSK4Nq7eQ2Nre2
d/K7hb39g8Oj4vFJS8epYthksYhVJ6AaBZfYNwI7CQKaRQIbAfju7nfkKleSwfzCRBP6JDyUPOqLFSo9IvltyuwBZJ15GSpCh3i9+9QYxSy
OUhgmqdzE+NPqTKcCZwVeqnGhLIxHWLXUkj1P50ceiMXFhlQMJY2ZKGLNTfE1MaT2JAtsZUTPSq95c/M/rpia8adcJqlByZaLwlQE5P
512TAFTIjJpZQpri9lbARVZQZm03BhuCtvrxOWpWyd1WuNql2m0WRx7O4BwuwYNrqME91KEJDBCe4RXenEfnxXl3PpatOSebOYU/cD5/AH8P
jL8=</latexit>2
<latexit sha1_base64=""PvkqMPf0KQ2ZiRgX4QPbidqMLN8="">AB
6HicbVDLTgJBEOzF+IL9ehlIjHxRHaVqEeiF4+QyCOBDZkdemFkdnYzM2tCF/gxYPGePWTvPk3DrAHBSvpFLVne6uIBFcG9f9dnJr6xubW/nt
ws7u3v5B8fCoqeNUMWywWMSqHVCNgktsG4EthOFNAoEtoLR3cxvPaHSPJYPZpygH9GB5CFn1FipftkrltyOwdZJV5GSpCh1it+dfsxSyOUhgmq
dcdzE+NPqDKcCZwWuqnGhLIRHWDHUkj1P5kfuiUnFmlT8JY2ZKGzNXfExMaT2OAtsZUTPUy95M/M/rpCa8SdcJqlByRaLwlQE5PZ16TPFTI
jxpZQpri9lbAhVZQZm03BhuAtv7xKmhdl76pcqVdK1dsjycwCmcgwfXUIV7qEDGCA8wyu8OY/Oi/PufCxac042cwx/4Hz+AICTjMA=</latex
it>3
<latexit sha1_base64=""y+WDoqtszdexELZg6EJPDYoWM8="">AB
5HicbVBNS8NAEJ3Urxq/qlcvi0XwVBIp6rHoxWMF+wFtKJvtpF272YTdjVBCf4EXD4pXf5M3/43bNgdtfTDweG+GmXlhKrg2nvftlDY2t7Z3yrvu
3v7B4VHFPW7rJFMWywRieqGVKPgEluG4HdVCGNQ4GdcHI39zvPqDRP5KOZphjEdCR5xBk1VnqoDypVr+YtQNaJX5AqFGgOKl/9YcKyGKVhgmrd
873UBDlVhjOBM7efaUwpm9AR9iyVNEYd5ItDZ+TcKkMSJcqWNGSh/p7Iaz1NA5tZ0zNWK96c/E/r5eZ6CbIuUwzg5ItF0WZICYh86/JkCtkRkw
toUxeythY6oMzYb14bgr768TtqXNf+qVq82boswynAKZ3ABPlxDA+6hCS1gPACb/DuPDmvzseyseQUEyfwB87nDxedi5c=</latexit>4
Figure 1: A schematic depiction of the quantitative measure of “beauty” of a theory model as a function of the model parameters P1 and P2. In
the absence of any experimental data, model “1” is the most beautiful of all. An experimental measurement of a single observable O enforces a
constraint (shown with the dashed line) among the model parameters. Model “2” is the most beautiful model which is consistent with experiment.
Model “3” is an example of a true and not-as-beautiful model, while model “4” is an example of a model which is neither true nor beautiful.
depicted with the dashed line in the horizontal plane, which in turn defines a path (marked with the solid line) along
the “beauty surface”. Models along this path are “true”, and model “2” is the most beautiful one among them. Model
“3” is an example of a true and not-as-beautiful model, while model “4” is an example of a model which is neither
true nor beautiful.
Operationally, the optimization needed to arrive at model “2” can be performed in three different ways.
• Constrained optimization. The experimental constraints are incorporated into the beauty loss function (e.g.,
with Lagrange multipliers) and the optimization is done over the full domain of the parameters Pi.
• Unconstrained optimization. Alternatively, one may try to solve all the constraints explicitly. In the language
of Fig. 1, this means deriving an analytical parametrization of the dashed line in the horizontal plane, then
optimizing the beauty loss function along this line.
• Hybrid optimization. If not all constraints can be easily solved, one can use a hybrid approach, where some
constraints are solved explicitly while others are added to the loss. This will be our approach here.
In this paper, we focus on the flavor sector, which is arguably the “ugliest” part of the Standard Model. We consider
several possible choices for quantitative measures of the beauty of the model. In each case, we define corresponding
loss functions whose minimization by construction yields “the most truthful and beautiful” model. Preliminary results
from this work were reported at the 37th Conference on Neural Information Processing Systems (NeurIPS) [6]; here
we have added the additional constraint of CP-violation and a third illustrative example (described in Section 3.3).
Our approach should be viewed as part of a much broader program of trying to learn the laws of nature with
a machine, eliminating any human intervention whatsoever [7–17]. For example, it has been demonstrated that
the machine can re-derive the known classical physics laws from data [18–21]. Symbolic learning was recently
successfully applied to problems in a wide range of physics areas, e.g., in astrophysics [19, 22, 23], in astronomy for
the study of orbital dynamics [24, 25] and exoplanet transmission spectroscopy [26], in collider physics [27–31], in
materials science [32], and in behavioral science [33]. Our approach is slightly less ambitious than those studies, since
we already adopt the mathematical framework for the description of the phenomena, and instead focus only on the
2
determination of the “best” model parameters which, within that mathematical framework, might have been chosen
by nature. Along similar lines, to find the allowed U(1) charge assignments in models with a U(1) flavor symmetry,
Ref. [34] used reinforcement learning, as opposed to an exhaustive computerized search as in [35, 36].
The paper is organized as follows. In Section 2, we introduce our notation and the experimental inputs to our
subsequent numerical analysis. Then in Section 3, we consider three examples of “beautiful” quark textures. First,
in Section 3.1, we consider beauty to mean uniformity, i.e., the elements in the Yukawa matrices have the same
magnitude. Then in Section 3.2, we take beauty to mean sparsity, i.e., the Yukawa matrices have a large number of
vanishing elements. Finally, in Section 3.3, we identify beauty with the symmetry (in the sense of linear algebra) of
the Yukawa matrices. Section 4 contains our summary and conclusions.
2. Standard Model Parameters
In our analysis, we primarily use the notation and conventions from the standard textbook by M. Schwartz [37].
The Lagrangian governing the quark mass sector can be expressed as
Lquarks = −Yd
i j ¯QiHd j
R − Yu
i j ¯Qi e
Hu j
R + h.c. ,
(1)
where Qi, with i = 1, 2, 3, represents the quark doublets of the S U(2)L group, incorporating three generations
Qi =
 ui
L
di
L
!
,
(2)
H denotes the Higgs field, and e
H is its conjugate field, defined as
e
H ≡ iσ2H∗ .
(3)
Here σ2 is the second Pauli matrix and the asterisk indicates complex conjugation. Thus, we have,
H =
 H+
H0
!
,
e
H =
 H0∗
−H−
!
.
(4)
The fields ui
R and di
R are the right-handed S U(2)L quark singlets, given by
ui
R = {uR, cR, tR},
di
R = {dR, sR, bR} .
(5)
This formulation incorporates all three quark generations. The Yukawa matrices Yu
i j and Yd
i j are arbitrary complex
matrices and are not constrained by hermiticity or similar mathematical properties. As complex 3 × 3 matrices, they
have a total of 18 degrees of freedom. After spontaneous symmetry breaking, the neutral Higgs field component, H0,
acquires a vacuum expectation value of v/
√
2, modifying the quark mass Lagrangian (1) to
Lquarks = − v√
2
h ¯dLYddR + ¯uLYuuR
i
+ h.c.
(6)
In the interaction eigenstate basis, the quark mass matrices are not diagonal, but the interactions between the up- and
down-type quarks and W±-bosons are. In this basis, the quark mass matrices are given by
(Mu)ij ≡
v√
2
Yu
i j ,
(Md)i j ≡
v√
2
Yd
i j .
(7)
Diagonalization of these matrices is achieved by transitioning to a new mass-eigenstate basis
u′
L
= UuuL ,
d′
L
= UddL ,
(8a)
u′
R
= KuuR ,
d′
R
= KddR ,
(8b)
3
where the primed quark fields are mass eigenstates, and Uu, Ud, Ku, and Kd are unitary rotation matrices. The resulting
diagonal mass matrices in this new basis are
M′
u = Uu Mu K†
u ,
M′
d = Ud Md K†
d .
(9)
The unitary matrices Uu and Ku are derived by diagonalizing the Hermitian matrices MuM†
u and M†
uMu, respectively.
Given the Hermitian nature of these matrices, they each possess three eigenvalues associated with the squared masses
of the up-type quarks, namely the up quark, charm quark, and top quark, with respective masses mu
i = {mu, mc, mt}. In
parallel, the matrices Ud and Kd are determined through the diagonalization of MdM†
d and M†
dMd. This process yields
the masses for the down-type quarks, specifically the down quark, strange quark, and bottom quark, represented as
md
i = {md, ms, mb}.
In the mass eigenstate basis, the quark mass Lagrangian assumes a diagonal form and can be expressed as:
Lquarks = −mu
i δi j¯u′i
Lu′ j
R − md
i δi j ¯d′i
Ld′ j
R + h.c.
(10)
However, quark mixing phenomena become evident in the interactions that involve charged W bosons, typically
parameterized using the Cabibbo-Kobayashi-Maskawa (CKM) matrix:
VCKM ≡ U†
uUd =

V11
V12
V13
V21
V22
V23
V31
V32
V33
 =

Vud
Vus
Vub
Vcd
Vcs
Vcb
Vtd
Vts
Vtb
 .
(11)
The CKM matrix is a 3 × 3 complex unitary matrix with nine degrees of freedom. Utilizing the U(1) symmetry of the
6 quark mass terms in the Lagrangian (10), 5 degrees of freedom can be removed, leaving 4 independent parameters.3
These are commonly parametrized as
VCKM =

1
0
0
0
cos θ23
sin θ23
0
− sin θ23
cos θ23


cos θ13
0
sin θ13eiδ
0
1
0
− sin θ13eiδ
0
cos θ13


cos θ12
sin θ12
0
− sin θ12
cos θ12
0
0
0
1

=

c12c13
s12c13
s13e−iδ
−s12c23 − c12s23s13eiδ
c12c23 − s12s23s13eiδ
s23c13
s12s23 − c12c23s13eiδ
−c12s23 − s12c23s13eiδ
c23c13
 ,
(12)
where si j ≡ sin θij and cij ≡ cos θij, and δ is the CP-violating phase. Assuming unitarity of the CKM matrix, the
matrix parameters are given by [38]
sin θ12 = 0.22500 ± 0.00067 ,
sin θ13 = 0.00369 ± 0.00011 ,
sin θ23 = 0.04182+0.00085
−0.00074 ,
δ = 1.144 ± 0.027 .
The fit results for the magnitudes of all nine CKM elements are
VCKM,exp
 =

0.97435 ± 0.00016
0.22500 ± 0.00067
0.00369 ± 0.00011
0.22486 ± 0.00067
0.97349 ± 0.00016
0.04182+0.00085
−0.000074
0.00857+0.00020
−0.00018
0.0410+0.0003
−0.00072
0.999118+0.0000031
−0.000036
 .
(13)
The measurement of CP violation, independent of phase convention, is given by the Jarlskog invariant, defined as
J = Im
h
Vi jVklV∗
ilV∗
k j
i
= J
X
m,n
εikmεjln ,
(14)
and its experimental value is given by [38]
J =

3.08+0.15
−0.13

× 10−5 .
(15)
In addition to Eq. (13), our analysis also incorporates the running quark masses at a chosen reference energy scale,
with the top quark mass scale being the preferred choice [39] (although other scales like the Z mass scale are also
viable [40]). These values, along with their experimental uncertainties, are summarized in Table 1.
3It is not possible to remove all 6 phase degrees of freedom through U(1) rotations because these rotations require distinct phases to alter the
CKM matrix. If the phases in these rotations were identical, the CKM matrix would remain unaffected.
4
Table 1: Quark masses (with uncertainties) evaluated at the top quark mass scale.
mu (MeV)
md (MeV)
mc (GeV)
ms (MeV)
mb (GeV)
mt (GeV)
1.22+0.28
−0.15
2.76+0.28
−0.10
0.59+0.01
−0.01
52+4.79
−1.89
2.75+0.02
−0.01
162.9+0.28
−0.28
3. Quark Sector Textures
In this section, we construct our loss functions and demonstrate their utility through three examples. Following
[6], the initial inputs to the Lagrangian (1) are the two Yukawa matrices Yu and Yd, or equivalently, the mass matrices
Mu and Md given in eq. (7). These matrices encompass a total of 36 degrees of freedom. Out of these, 15 (9 from the
CKM matrix and 6 from the quark masses) are determined by the experimental inputs in Eq. (13) and Table 1. Thus,
we could theoretically set this as an optimization problem in a 36-dimensional space, constrained by 15 parameters.
However, to expedite the optimization process, we instead consider a parametrization of Mu and Md that inherently
satisfies the quark mass constraints. This is achieved through the inverse relations to equations (9):
Mu = U†
u M′
u Ku,
Md = U†
d M′
d Kd.
(16)
By setting the diagonal elements in the matrices M′
u and M′
d to magnitudes equal to the respective quark masses, we
automatically fulfill the quark mass constraints. This approach reduces the degrees of freedom in each matrix Mu and
Md to 15 (that matches exactly the degrees of freedom between the left- and the right-hand sides of the equations (16).4
Consequently, we have effectively transformed the problem into an optimization task in a 30-dimensional space,
subject to only the 9 constraints detailed in (13). To ensure these constraints are met, we define the following loss
function:
LCKM =
X
i j

|VCKM|i j − |VCKM,exp|i j
2 .
(17)
To account for the CP-violating phase, we introduce a loss function that incorporates the Jarlskog invariant, which
is a phase-convention-independent measure of CP violation:
LJarlskog =
Im
h
VusVcbV∗
ubV∗
cs
i − Jexp
2
.
(18)
For each of the three examples discussed below, we will employ a series of pseudo-experiments for illustration.
In every pseudo-experiment, our starting point are the experimental values specified in Eqs. (13) and (15) and in
Table 1. However, in order to account for the experimental uncertainties in the measurements, in our numerical
analysis we sample the experimental inputs from split normal distributions whose left (right) standard deviations
match the lower (upper) experimental uncertainties of the respective experimental values. Moving forward, our results
will be expressed in terms of the mass matrices Mu and Md, as opposed to using the dimensionless Yukawa matrices
Yu and Yd.
3.1. Uniform Texture
The fundamental nature and origin of the Yukawa matrices Yu and Yd remains one of the key unsolved mysteries
within the Standard Model (SM), marking the “flavor problem” as a vibrant focus of theoretical research for over
half a century. Numerous theories, often featuring novel symmetries, particles, and interactions, have been proposed
to solve these “Yukawa textures.” Experimental outcomes will ultimately determine the validity of these models by
either confirming or disproving these additional structures and components. Our approach here adopts a bottom-up
methodology within the SM, treated as an effective theory, where the only available experimental data are those in
Eqs. (13) and (15), and Table 1. Thus, the primary criterion for selecting one model over another hinges on whether
the resulting Yukawa sector embodies a certain “beauty” or not.
4We note that in our approach, we do not manually fix any degrees of freedom; instead, we allow the minimization of the loss function to
determine all the degrees of freedom.
5
●
●
●
●
●
●
●
●
●
●
◆
◆
◆
◆
◆
◆
◆
◆
◆
◆
★
★
★
★
★
★
★
★
★
★
■
■
■
■
■
■
■
▲
▲
▲
▲
▲
▲
▲
▲
▲
▲
●
◆
★
■
▲
Figure 2: The trained total loss values (see Eq. 23) along with the detailed breakdown into the four individual components (Eqs. 17, 18, 20, and
22) are presented for ten representative pseudo-experiments. These experiments are part of the uniform texture analysis discussed in Section 3.1.
For an initial exploration, let’s define a “beautiful” flavor model as one that promotes uniformity, meaning that
each element within a Yukawa matrix has approximately the same magnitude. We express this as
|Yu
ij| ≃ |Yu
kl| ,
∀i, j, k, l .
(19)
We can impose this uniformity criterion by introducing the following loss function
Lconst,up =
X
i, j,k,l

|Yu
i j| − |Yu
kl|
2 .
(20)
Likewise, applying uniformity to the down-type Yukawa matrix leads to
|Yd
ij| ≃ |Yd
kl| ,
∀i, j, k, l ,
(21)
with the corresponding loss function
Lconst,down =
X
i, j,k,l

|Yd
i j| − |Yd
kl|
2 .
(22)
Thus, the complete loss function for uniform Yukawa textures can be expressed as
Luniform = LCKM + 1
mt
Lconst,up + 1
mb
Lconst,down +
1
J2exp
LJarlskog .
(23)
In this equation, the loss functions Lconst,up and Lconst,down are normalized with respect to the masses of the heaviest
quarks, ensuring that all three contributions to the loss function are equally weighted. Similarly, LJalrskog, is normalized
with respect to J2
exp.
We conduct 10 pseudo-experiments, focusing on minimizing the full loss function (23). The results are illustrated
in Fig. 2. Here, we present not only the final values for the overall loss achieved through training but also enumerate
the individual contributions from Eqs. (17), (18), (20), and (22). Across each pseudo-experiment, we consistently
observed minimal loss values, signifying the potential viability of the Yukawa textures under consideration.
6
Figure 3: The learned mass matrices Mu (upper panels) and Md (lower panels), corresponding to the uniform Yukawa texture case discussed in
Sec. 3.1. Each panel illustrates a specific learned matrix, with the color bar denoting the values of the individual elements of the matrix.
To provide a concrete example, we quote a specific outcome from one of the 10 pseudo-experiments (noting that
the results from the others were similar). For the up-type quark mass matrix, we obtain
Mu =

52.1055 − 14.8303i
−51.2066 + 17.6863i
50.9441 + 18.4286i
41.4531 + 34.8792i
−42.9766 − 32.9838i
13.8632 + 52.3711i
−50.9028 + 18.5424i
49.9240 − 21.0360i
−52.2362 − 14.3631i
 ,
(24)
and
|Mu| =

54.1749
54.1749
54.1749
54.1749
54.1749
54.1749
54.1749
54.1749
54.1749
 .
(25)
For the down-type mass matrix Md, we find
Md =

−0.3248 + 0.8549i
−0.6992 + 0.5895i
0.9090 − 0.1008i
−0.1510 − 0.9020i
0.3066 − 0.8616i
−0.6748 + 0.6173i
−0.2776 + 0.8714i
−0.6623 + 0.6307i
0.8913 − 0.2048i
 ,
(26)
and
|Md| =

0.9145
0.9145
0.9146
0.9146
0.9145
0.9146
0.9145
0.9146
0.9145
 .
(27)
The results of this pseudo-experiment are illustrated in Fig. 3. The top panels of the figure relate to Eq. (24)
and the bottom panels to Eq. (26). Each row is divided into four panels: the first two on the left display the real
and imaginary parts of the matrix elements, while the last two panels exhibit their magnitude and phase. Notably, in
each mass matrix, the magnitudes of the various elements are approximately equal, aligning well with our predefined
criterion of “beauty” for this example.
3.2. Zero Textures
In our second example, we explore the concept of zero textures, as discussed in [41], where the “beauty” of a
model is measured by its sparsity. This concept aims to maximize the number of vanishing elements in the Yukawa
matrices. In our analysis, we treat the number of such vanishing elements, N, as a variable hyperparameter, as detailed
in Table 2. For each chosen value of N, we are free to set to zero N elements in the matrix. The total number of possible
7
<latexit sha1_base64=""mWSXWP5msuendn1dCle/
7cWB1jE="">AB7HicdVBNS8NAEN3Ur1q/qh69LBbBU0iaYNODUPTiSqYWmhD2Ww37dLNJuxuhBL6G7x4UM
SrP8ib/8ZNW0FHw83pthZl6YMiqVZX0YpZXVtfWN8mZla3tnd6+6f9CRSYw8XHCEtENkSMcuIrqhjp
oKgOGTkLpxcFv7dPRGSJvxWTVMSxGjEaUQxUlryr+E5dAbVmU27XrDcqBlOp5d+2CND3PdaBtWnPUwBLtQ
fW9P0xwFhOuMENS9mwrVUGOhKYkVmln0mSIjxBI9LTlKOYyCfHzuDJ1oZwigRuriCc/X7RI5iKadxqDtj
pMbyt1eIf3m9TEVekFOeZopwvFgUZQyqBafwyEVBCs21QRhQfWtEI+RQFjpfCo6hK9P4f+kUzftM9O9cWu
ti2UcZXAEjsEpsEDtMAVaAMfYEDBA3gCzwY3Ho0X43XRWjKWM4fgB4y3T7fUjf8=</latexit>N = 3
<latexit sha1_base64=""igJ+IH3ek6zntnbtBycr
F2Jk1wg="">AB7HicdVBNS8NAEJ3Ur1q/qh69LBbBU0mkWHMQil48SQXTFtpQNtNu3SzCbsboYT+Bi8eFP
HqD/Lmv3HbRqiDwYe780wMy9IOFPatj+twsrq2vpGcbO0tb2zu1feP2ipOJWEeiTmsewEWFHOBPU05x2E
klxFHDaDsbXM7/9QKVisbjXk4T6ER4KFjKCtZG8W3SJav1yxa7ac6Al4rpuveYiJ1cqkKPZL3/0BjFJIyo04
ViprmMn2s+w1IxwOi31UkUTMZ4SLuGChxR5WfzY6foxCgDFMbSlNBori5PZDhSahIFpjPCeqR+ezPxL6+b
6vDCz5hIUk0FWSwKU450jGafowGTlGg+MQTycytiIywxESbfEomhO9P0f+kdVZ1zqu1u1qlcZXHUYQjOIZT
cKAODbiBJnhAgMEjPMOLJawn69V6W7QWrHzmEH7Aev8CjhaN4w=</latexit>N = 4
<latexit sha1_base64=""G380UqHzsLz3z1St0rI
kRtY40="">AB7HicdVDLSgMxFM3UV62vqks3wSK4GjJ9dyEU3biSCk5baIeSTNtaCYzJBmhDP0GNy4Uce
sHufNvTB+Cih64cDjnXu69x485UxqhDyuztr6xuZXdzu3s7u0f5A+P2ipKJKEuiXgkuz5WlDNBXc0p91YU
hz6nHb8ydXc79xTqVgk7vQ0pl6IR4IFjGBtJPcGXsDKIF9AdqOB6sUqRHbZqdVKRUNQqYKQAx0bLVAK7QG+
f+MCJSIUmHCvVc1CsvRLzQins1w/UTGZIJHtGeowCFVXro4dgbPjDKEQSRNCQ0X6veJFIdKTUPfdIZY
j9Vvby7+5fUSHdS9lIk40VSQ5aIg4VBHcP45HDJieZTQzCRzNwKyRhLTLTJ2dC+PoU/k/aRdup2uXbcqF
5uYojC07AKTgHDqiBJrgGLeACAh4AE/g2RLWo/VivS5bM9Zq5hj8gPX2Ca8gjfg=</latexit>N = 5
<latexit sha1_base64=""ZcaNvHQZ6lwhy1NoWnquz1VRioE="">AB7HicdVDLSsNAFJ34rPVdelmsAiu
QibWpi6EohtXUsG0hTaUyXTSDp1MwsxEKHf4MaFIm79IHf+jdOHoKIHLhzOuZd7wlTzpR2nA9raXldW29sFHc3Nre2S3t7TdVklCfZLwRLZDrChngvqaU7bqaQ4DjlthaOrqd+6p1KxRNzpcUqDGA8EixjB2kj+Dby
A1V6p7NjIQS5C0LE9r3aKPEMcdOaeuxDZzgxlsECjV3rv9hOSxVRowrFSHeSkOsix1IxwOil2M0VTEZ4QDuGChxTFeSzYyfw2Ch9GCXSlNBwpn6fyHGs1DgOTWeM9VD9qbiX14n01EtyJlIM0FmS+KMg51Aqefwz6TlG
g+NgQTycytkAyxESbfIomhK9P4f+k6dqoalduK+X65SKOAjgER+AEIOCBOrgGDeADAh4AE/g2RLWo/Vivc5bl6zFzAH4AevtE6ItjfA=</latexit>N = 6
Figure 4: The zero texture patterns examined in the example of Sec. 3.2, with circles marking the matrix elements set to zero.
patterns is indicated in the second row of Table 2. However, some patterns can be immediately ruled out as they lead
to at least one zero mass eigenvalue. The count of the remaining, potentially feasible, patterns is presented in the
bottom row of the table.
For this study, we focus on several representative patterns shown in Fig. 4, where circles indicate the positions of
the matrix elements set to zero. Let S denote the set of zero locations in a specific pattern, for example, S = 11, 22, 13
for the first N = 3 pattern in Fig. 4. The corresponding loss functions for these patterns are defined as follows
Lzeros,up =
X
i j∈S
|Yu
i j|2 ,
(28)
and
Lzeros,down =
X
i j∈S
|Yd
i j|2 .
(29)
We then minimize the loss function that includes the following components:
L = LCKM + 1
mu
Lzeros,up + 1
md
Lzeros,down +
1
J2exp
LJarlskog .
(30)
We note that for increased precision in the minimization process, we normalize the loss functions Lzeros,up and
Lzeros,down with respect to the masses of the lightest quarks.
Our findings indicate that feasible patterns lead to low loss values. In the same manner as Fig. 2, Fig. 5 presents
the trained loss values and their components for the ten N = 3 zero texture patterns, averaged over 10 pseudo-
experiments. As expected, all N = 3 zero texture patterns are plausible. The result of one such pseudo-experiment for
Table 2: A number N of vanishing elements in the mass matrix (top row), the total number of patterns (middle row), and the number of potentially
acceptable patterns (bottom row).
N
1
2
3
4
5
6
7
8
All patterns
9
36
84
126
126
84
36
9
Acceptable
9
36
78
81
36
6
0
0
8
●
●
●
●
●
●
●
●
●
●
◆
◆
◆
◆
◆
◆
◆
◆
◆
◆
★
★
★
★
★
★
★
★
★
★
■
■
■
■
■
■
■
■
■
■
▲
▲
▲
▲
▲
▲
▲
▲
▲
▲
●
◆
★
■
▲
Figure 5: As in Fig. 2, it illustrates the trained loss values and their components for the ten N = 3 zero texture patterns from Fig. 4, averaged across
10 different pseudo-experiments.
the first pattern in Fig. 4 is given by
Mu =

0.0000 − 0.0000i
0.0010 − 0.0012i
0.0000 + 0.0000i
0.1446 + 0.1709i
0.0000 + 0.0000i
5.4710 + 1.7690i
17.0333 + 6.8434i
−8.9194 + 2.9354i
140.3701 + 79.7760i
 ,
(31)
|Mu| =

0.0000
0.0016
0.0000
0.2239
0.0000
5.7499
18.3566
9.3900
161.4558
 ,
(32)
and
Md =

0.0000 + 0.0000i
0.0009 − 0.0177i
0.0000 + 0.0000i
−0.0385 − 0.0137i
0.0000 − 0.0000i
0.0555 − 0.0311i
−0.1596 − 0.8464i
−1.8296 − 0.5630i
1.6871 + 0.4743i
 ,
(33)
|Md| =

0.0000
0.0177
0.0000
0.0409
0.0000
0.0636
0.8613
1.9143
1.7525
 .
(34)
This result is shown in Fig. 6, confirming that the matrix entries at positions 11, 22, and 13 are notably small.
The outcomes for larger values of N are summarized in Fig. 7. Here, we present the computed loss values,
averaged across both the number of pseudo-experiments (10 in this case) and the different patterns from Fig. 4 for
each respective N value.
3.3. Symmetric Textures
In our final example, we consider symmetric Yukawa textures where the off-diagonal elements are “mirrored” (in
magnitude) across the main diagonal:
|Yu
i j| = |Yu
ji| ,
∀i, j .
(35)
9
Figure 6: As in Fig. 3, but for the three zero texture result from Eqs. (31) and (33).
Figure 7: Averaged values of the trained loss as a function of the number of texture zeros.
This condition can be imposed by introducing the following loss function
Lsym,up =
X
i,j∈S

|Yu
i j| − |Yu
ji|
2 .
(36)
Similarly, for the down-type Yukawa matrix, the analogous condition is
|Yd
i j| = |Yd
ji| ,
∀i, j .
(37)
and the corresponding loss function is given by
Lsym,down =
X
i,j∈S

|Yd
i j| − |Yd
ji|
2 .
(38)
The complete loss function for symmetric Yukawa textures is given by
L = LCKM + 1
mt
Lsym,up + 1
mb
Lsym,down +
1
J2exp
LJarlskog .
(39)
10
●
●
●
●
●
●
●
●
●
●
◆
◆
◆
◆
◆
◆
◆
◆
◆
◆
★
★
★
★
★
★
★
★
★
★
■
■
■
■
■
■
■
■
■
▲
▲
▲
▲
▲
▲
▲
▲
▲
▲
●
◆
★
■
▲
Figure 8: As in Fig. 2, it illustrates the trained loss values and their components for the 10 symmetric texture patterns, averaged across 10 different
pseudo-experiments.
Similarly to Figs. 2 and 5, Fig. 8 shows (the individual contributions to) the trained loss values in 10 representative
pseudo-experiments. For one randomly chosen pseudo-experiment, we find the up-type quark mass matrix
Mu =

19.5575 + 2.5557i
−18.2841 − 2.8153i
49.5441 + 5.0476i
17.3188 + 6.5033i
−16.1554 − 7.2142i
44.1967 + 15.1457i
30.2621 + 39.5513i
−27.2614 − 37.9415i
79.2378 + 97.6495i
 ,
(40)
with
|Mu| =

19.7238
18.4996
49.8006
18.4996
17.6930
46.7198
49.8006
46.7198
125.7540
 ,
(41)
and the down-type mass matrix
Md =

0.1123 − 0.4304i
0.0777 − 0.2201i
0.5203 + 0.8316i
0.2331 + 0.0127i
0.1303 + 0.0548i
−0.3273 + 0.3973i
−0.3106 + 0.9305i
−0.2084 + 0.4707i
−1.0108 − 1.9185i
 ,
(42)
with
|Md| =

0.4448
0.2334
0.9810
0.2334
0.1414
0.5148
0.9810
0.5148
2.1685
 .
(43)
As seen in Eqs. (41) and (43), the obtained matrices are perfectly symmetric in the sense defined above. Fig. 9 is a
pictorial illustration of the result displayed in eqs. (40) and (42).
4. Summary and Conclusions
In theoretical particle physics, the process of developing new models entails meeting the objective constraints of
the existing experimental data, as well as subjective criteria like beauty and naturalness set forth by the theoretical
11
Figure 9: As in Fig. 3, but with the symmetric texture result from Eqs. (40) and (42).
physics community. To achieve both of these goals, we employ machine learning techniques with suitably designed
loss functions addressing the perceived deficiencies in the Yukawa sector of the Standard Model. With the three
toy examples from Section 3, we showed that this approach yields models that are not only consistent with the
experimental data, but also possess the desired aesthetic elegance as defined by a quantitative benchmark. In future
work, we plan to extend this analysis to the lepton sector of the Standard Model as well.
Acknowledgements. We thank K. Babu, S. Gleyzer, D. Gonc¸alves, K. Kong and G. Shiu for useful discussions.
This work is supported in part by the U.S. Department of Energy award number DE-SC0022148.
References
[1] R. Barbieri, G. F. Giudice, Upper Bounds on Supersymmetric Particle Masses, Nucl. Phys. B 306 (1988) 63–76.
doi:10.1016/
0550-3213(88)90171-X.
[2] G. W. Anderson, D. J. Castano, Measures of fine tuning, Phys. Lett. B 347 (1995) 300–308. arXiv:hep-ph/9409419, doi:10.1016/
0370-2693(95)00051-L.
[3] G. W. Anderson, D. J. Castano, Naturalness and superpartner masses or when to give up on weak scale supersymmetry, Phys. Rev. D 52
(1995) 1693–1700. arXiv:hep-ph/9412322, doi:10.1103/PhysRevD.52.1693.
[4] J. L. Feng, K. T. Matchev, T. Moroi, Multi - TeV scalars are natural in minimal supergravity, Phys. Rev. Lett. 84 (2000) 2322–2325.
arXiv:hep-ph/9908309, doi:10.1103/PhysRevLett.84.2322.
[5] J. L. Feng, K. T. Matchev, T. Moroi, Focus points and naturalness in supersymmetry, Phys. Rev. D 61 (2000) 075005. arXiv:hep-ph/
9909334, doi:10.1103/PhysRevD.61.075005.
[6] K. T. Matchev, K. Matcheva, P. Ramond, S. Verner, Seeking Truth and Beauty in Flavor Physics with Machine Learning, in: 37th Conference
on Neural Information Processing Systems, 2023. arXiv:2311.00087.
[7] P. Langley, Bacon: A production system that discovers empirical laws, in: IJCAI, 1977.
[8] P. Langley, H. A. Simon, G. L. Bradshaw, Heuristics for Empirical Discovery, Springer Berlin Heidelberg, Berlin, Heidelberg, 1987, pp.
21–54. doi:10.1007/978-3-642-82742-6_2.
URL https://doi.org/10.1007/978-3-642-82742-6_2
[9] M. Kokar, Determining arguments of invariant functional descriptions., Machine Learning 1 (1986) 403–422.
doi:10.1023/A:
1022818816206.
[10] P. Langley, J. M. Zytkow, Data-driven approaches to empirical discovery, Artificial Intelligence 40 (1) (1989) 283–312.
doi:https:
//doi.org/10.1016/0004-3702(89)90051-9.
URL https://www.sciencedirect.com/science/article/pii/0004370289900519
[11] R. Zembowicz, J. M. ˙Zytkow, Discovery of equations: Experimental evaluation of convergence, in: Proceedings of the Tenth National
Conference on Artificial Intelligence, AAAI’92, AAAI Press, 1992, p. 70–75.
[12] L. Todorovski, S. Dzeroski, Declarative bias in equation discovery, in: Proceedings of the Fourteenth International Conference on Machine
Learning, Morgan Kaufmann, 1997, pp. 376–384.
[13] J. Bongard, H. Lipson, From the Cover: Automated reverse engineering of nonlinear dynamical systems, Proceedings of the National
Academy of Science 104 (24) (2007) 9943–9948. doi:10.1073/pnas.0609476104.
[14] M. Schmidt, H. Lipson, Distilling Free-Form Natural Laws from Experimental Data, Science 324 (5923) (2009) 81.
doi:10.1126/
science.1165893.
12
[15] P. W. Battaglia, R. Pascanu, M. Lai, D. Rezende, K. Kavukcuoglu, Interaction Networks for Learning about Objects, Relations and Physics,
arXiv e-prints (2016) arXiv:1612.00222arXiv:1612.00222.
[16] M. B. Chang, T. Ullman, A. Torralba, J. B. Tenenbaum, A Compositional Object-Based Approach to Learning Physical Dynamics, arXiv
e-prints (2016) arXiv:1612.00341arXiv:1612.00341.
[17] R. Guimer`a, I. Reichardt, A. Aguilar-Mogas, F. A. Massucci, M. Miranda, J. Pallar`es, M. Sales-Pardo, A bayesian machine scientist to aid in
the solution of challenging scientific problems, Science Advances 6 (5) (2020) eaav6971. arXiv:https://www.science.org/doi/pdf/
10.1126/sciadv.aav6971, doi:10.1126/sciadv.aav6971.
URL https://www.science.org/doi/abs/10.1126/sciadv.aav6971
[18] S.-M. Udrescu, M. Tegmark, AI Feynman: a Physics-Inspired Method for Symbolic Regression, Sci. Adv. 6 (16) (2020) eaay2631. arXiv:
1905.11481, doi:10.1126/sciadv.aay2631.
[19] M. Cranmer, A. Sanchez-Gonzalez, P. Battaglia, R. Xu, K. Cranmer, D. Spergel, S. Ho, Discovering Symbolic Models from Deep Learning
with Inductive Biases (6 2020). arXiv:2006.11287.
[20] Z. Liu, V. Madhavan, M. Tegmark, Machine learning conservation laws from differential equations, Phys. Rev. E 106 (2022) 045307. doi:
10.1103/PhysRevE.106.045307.
URL https://link.aps.org/doi/10.1103/PhysRevE.106.045307
[21] Y. Matsubara, N. Chiba, R. Igarashi, T. Taniai, Y. Ushiku, Rethinking symbolic regression datasets and benchmarks for scientific discovery
(2022). arXiv:2206.10540, doi:10.48550/ARXIV.2206.10540.
URL https://arxiv.org/abs/2206.10540
[22] M. D. Cranmer, R. Xu, P. Battaglia, S. Ho, Learning symbolic physics with graph networks (2019). arXiv:1909.05862, doi:10.48550/
ARXIV.1909.05862.
URL https://arxiv.org/abs/1909.05862
[23] A. M. Delgado, D. Wadekar, B. Hadzhiyska, S. Bose, L. Hernquist, S. Ho, Modelling the galaxy–halo connection with machine learning,
Mon. Not. Roy. Astron. Soc. 515 (2) (2022) 2733–2746. arXiv:2111.02422, doi:10.1093/mnras/stac1951.
[24] R. Iten, T. Metger, H. Wilming, L. del Rio, R. Renner, Discovering Physical Concepts with Neural Networks, Physical Review Letters 124 (1)
(2020) 010508. arXiv:1807.10300, doi:10.1103/PhysRevLett.124.010508.
[25] P. Lemos, N. Jeffrey, M. Cranmer, S. Ho, P. Battaglia, Rediscovering orbital mechanics with machine learning, Mach. Learn. Sci. Tech. 4 (4)
(2023) 045002. arXiv:2202.02306, doi:10.1088/2632-2153/acfa63.
[26] K. T. Matchev, K. Matcheva, A. Roman, Analytical Modeling of Exoplanet Transit Spectroscopy with Dimensional Analysis and Symbolic
Regression, The Astrophysical Journal 930 (1) (2022) 33. arXiv:2112.11600, doi:10.3847/1538-4357/ac610c.
[27] S. Choi, Construction of a Kinematic Variable Sensitive to the Mass of the Standard Model Higgs Boson in H → WW∗ → l+νl−¯ν using
Symbolic Regression, JHEP 08 (2011) 110. arXiv:1006.4998, doi:10.1007/JHEP08(2011)110.
[28] A. Butter, T. Plehn, N. Soybelman, J. Brehmer, Back to the Formula – LHC Edition (9 2021). arXiv:2109.10414.
[29] A. Dersy, M. D. Schwartz, X. Zhang, Simplifying Polylogarithms with Machine Learning (6 2022). arXiv:2206.04115.
[30] A. Alnuqaydan, S. Gleyzer, H. Prosper, SYMBA: symbolic computation of squared amplitudes in high energy physics with machine learning,
Mach. Learn. Sci. Tech. 4 (1) (2023) 015007. arXiv:2206.08901, doi:10.1088/2632-2153/acb2b2.
[31] Z. Dong, K. Kong, K. T. Matchev, K. Matcheva, Is the machine smarter than the theorist: Deriving formulas for particle kinematics with
symbolic regression, Phys. Rev. D 107 (5) (2023) 055018. arXiv:2211.08420, doi:10.1103/PhysRevD.107.055018.
[32] Y. Wang, N. Wagner, J. M. Rondinelli, Symbolic regression in materials science, MRS Communications 9 (3) (2019) 793–805.
doi:
10.1557/mrc.2019.85.
[33] N. Arechiga, F. Chen, Y.-Y. Chen, Y. Zhang, R. Iliev, H. Toyoda, K. Lyons, Accelerating understanding of scientific experiments with end to
end symbolic regression (2021). arXiv:2112.04023.
[34] S. Nishimura, C. Miyao, H. Otsuka, Exploring the flavor structure of quarks and leptons with reinforcement learning, JHEP 23 (2020) 021.
arXiv:2304.14176, doi:10.1007/JHEP12(2023)021.
[35] H.-S. Lee, K. T. Matchev, T. T. Wang, A U(1) -prime solution to the µ− problem and the proton decay problem in supersymmetry without
R-parity, Phys. Rev. D 77 (2008) 015016. arXiv:0709.0763, doi:10.1103/PhysRevD.77.015016.
[36] H.-S. Lee, C. Luhn, K. T. Matchev, Discrete gauge symmetries and proton stability in the U(1)-prime - extended MSSM, JHEP 07 (2008)
065. arXiv:0712.3505, doi:10.1088/1126-6708/2008/07/065.
[37] M. D. Schwartz, Quantum Field Theory and the Standard Model, Cambridge University Press, 2014.
[38] R. L. Workman, et al., Review of Particle Physics, PTEP 2022 (2022) 083C01. doi:10.1093/ptep/ptac097.
[39] K. S. Babu, TASI Lectures on Flavor Physics, in: Theoretical Advanced Study Institute in Elementary Particle Physics: The Dawn of the
LHC Era, 2010, pp. 49–123. arXiv:0910.2948, doi:10.1142/9789812838360_0002.
[40] Y. Giraldo, E. Rojas, Five Non-Fritzsch Texture Zeros for Quarks Mass Matrices in the Standard Model, in: 38th International Symposium
on Physics in Collision, 2018. arXiv:1811.05068.
[41] P. Ramond, R. G. Roberts, G. G. Ross, Stitching the Yukawa quilt, Nucl. Phys. B 406 (1993) 19–42. arXiv:hep-ph/9303320, doi:
10.1016/0550-3213(93)90159-M.
13
","There is a vast body of literature on theoretical physics. Much of this literature is concerned with developing new models that can explain the fundamental forces of nature. In this section, we will briefly review some of the most important theoretical models that have been developed in recent years. We will also discuss some of the challenges that these models face.nan"
"Absolute camera pose estimation has greatly improved in recent years, paving the way for pervasive markerless Augmented Reality (AR). However, current techniques are still computationally demanding and storage-heavy, often relying on offloading computations to distant servers, which can introduce latency and privacy concerns. In this paper, we propose MobileARLoc, a new on-device visual localisation framework for markerless mobile AR that combines the distinctive properties of a regression-based Absolute Pose Regressor (APR) and a smartphone's Visual-Inertial Odometry (VIO) system. MobileARLoc calibrates the APR's absolute pose prediction using the VIO's position and orientation estimates to improve pose accuracy and robustness. It further leverages the APR pose estimates to detect and compensate for VIO drift, resulting in a hybrid approach that optimizes performance for mobile AR applications. We extensively evaluate MobileARLoc using dataset simulations, demonstrating significant improvements in accuracy compared to the underlying APR. Our framework achieves fast inference speed (80 ms) on mobile devices, making it a promising candidate for on-device visual localization in mobile AR applications.","Visual localization systems estimate the absolute pose (position and orientation) of a device using its camera feed. This technology is essential for augmented reality applications, enabling virtual content to be anchored to real-world locations without the need for markers. Traditional approaches to visual localization heavily rely on 3D structure-based methods which are computationally expensive, have large storage requirements, and necessitate offloading computations to servers, compromising privacy and introducing latency. Absolute Pose Regressors (APRs), on the other hand, offer fast inference on-device and require minimal storage. However, their lower accuracy and robustness have hindered their practical use in large-scale mobile AR. The complementary properties of APRs and VIO systems motivate us to combine these technologies for better pose estimation in mobile AR. In this work, we introduce MobileARLoc, a framework that leverages VIO's pose estimates to select reliable APR predictions and refine unreliable APR predictions while compensating for drift.","MobileARLoc consists of two alternating looping stages: Alignment and Pose Optimization. In the Alignment stage, the framework identifies reliable APR pose estimations by comparing them against VIO odometry estimates. The reliable poses are then used to calculate a reference pose and establish a transformation between the VIO coordinate system and the world coordinate system. In the Pose Optimization stage, the framework checks the current APR pose against the previous pose and optimizes unreliable APR poses based on the reference pose and VIO poses. To compensate for VIO drift, MobileARLoc detects drift and adaptively loops back to the Alignment stage to recalculate the reference pose.","We evaluate MobileARLoc on a dataset captured using an iPhone 14 Pro Max, which underscores the low drift of current VIO solutions, thus validating our assumption that VIO can enhance absolute pose estimation. Compared to the underlying APRs, MobileARLoc improves the accuracy of MS-T, one of the state-of-the-art APRs, by up to 47% in translation error and 66% in rotation error on average across all outdoor scenes. In the Alignment stage, MobileARLoc selects Reliable Predictions (RPs) that comprise 41.8% to 51.3% of total PN predictions and 41.6% to 51% of total MS-T predictions, significantly increasing the percentage of accurate pose estimates and reducing the number of imprecise predictions. In the Pose Optimization stage, the framework further improves accuracy by optimizing unreliable poses that constitute 20.2% and 36.7% of PN pose estimates and 14.2% to 44.7% of MS-T pose estimates, respectively. Additionally, we demonstrate MobileARLoc's real-time performance on an iPhone 14 Pro Max device, achieving a fast processing time of less than 80 ms per image.","Our proposed framework, MobileARLoc, combines the advantages of APRs and VIO systems, integrating them into a robust and efficient visual localization solution for mobile AR. By leveraging VIO's odometry estimates, MobileARLoc enhances the accuracy of APR pose estimations, significantly reducing the incidence of outliers with large errors. Moreover, our method optimizes unreliable APR poses, further improving overall accuracy. MobileARLoc achieves fast inference speed on mobile devices, making it a practical solution for on-device visual localization in mobile AR applications. Our results pave the way for pervasive markerless mobile AR at a large scale.",MobileARLoc: On-device Robust Absolute Localisation for Pervasive Markerless Mobile AR,"Changkun Liu, Yukun Zhao, Tristan Braud","MobileARLoc: On-device Robust Absolute
Localisation for Pervasive Markerless Mobile AR
Changkun Liu1*, Yukun Zhao1*, Tristan Braud1,2
1Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong
2Division of Integrative Systems and Design, The Hong Kong University of Science and Technology, Hong Kong
{cliudg,yzhaoeg}@connect.ust.hk, braudt@ust.hk
Abstract—Recent years have seen significant improvement in
absolute camera pose estimation, paving the way for pervasive
markerless Augmented Reality (AR). However, accurate absolute
pose estimation techniques are computation- and storage-heavy,
requiring computation offloading. As such, AR systems rely on
visual-inertial odometry (VIO) to track the device’s relative
pose between requests to the server. However, VIO suffers
from drift, requiring frequent absolute repositioning. This paper
introduces MobileARLoc, a new framework for on-device large-
scale markerless mobile AR that combines an absolute pose
regressor (APR) with a local VIO tracking system. Absolute pose
regressors (APRs) provide fast on-device pose estimation at the
cost of reduced accuracy. To address APR accuracy and reduce
VIO drift, MobileARLoc creates a feedback loop where VIO
pose estimations refine the APR predictions. The VIO system
identifies reliable predictions of APR, which are then used to
compensate for the VIO drift. We comprehensively evaluate
MobileARLoc through dataset simulations. MobileARLoc halves
the error compared to the underlying APR and achieve fast
(80 ms) on-device inference speed.
I. INTRODUCTION
Visual localization systems utilize the visual data captured
by a device’s camera to determine its 6 degrees of free-
dom (6DOF) absolute pose (translation and rotation) within
established world coordinates for a known scene. Accurate
visual positioning is essential for augmented reality (AR)
applications where content is anchored into the physical world
without markers, paving the way to pervasive AR.
Highly accurate localisation systems typically rely on 3D
structure-based methods [1]–[5] that detect and match visual
features in images against a 3D model of the environment.
However, these methods are often computationally demanding,
taking seconds on servers with modern GPUs and requiring
the storage of large pre-built environment models and mapping
images database [6], [7]. Scaling up such methods to larger
environments thus requires offloading computations to distant
servers, adding latency and jitter [8], and raising significant
privacy concerns [9]. These constraints have led the industry
to adopt on-device localisation with strict access control on the
camera frames in home environments1. However, the compu-
tational cost of structure-based methods prevents their appli-
cation on-device at a larger scale. Absolute pose regressors
(APRs) are end-to-end machine learning models that estimate
*Both authors contributed equally to this research.
1https://developer.oculus.com/blog/mixed-reality-with-passthrough/
the device pose using a single monocular image. They can
provide fast inference on-device with minimal storage, even in
large environments and over multiple scenes [10]. However,
their low accuracy and robustness have so far prevented their
application to large-scale mobile AR [11]. Although absolute
localisation is a significant challenge, most markerless AR
applications can track the relative pose through visual-inertial
odometry (VIO). VIO systems calculate the displacement
between camera frames using visual and inertial data from
sensors. As such, they tend to display high accuracy in the
short term, but they drift over time [12]. These systems are at
the core of modern AR frameworks such as Google’s ARCore
and Apple’s ARKit. APR and VIO present antagonistic fea-
tures: APR poses are noisy yet drift-free, while VIO is very
accurate with errors building up over time [13]. We believe that
VIO’s high accuracy could thus improve APR’s imprecision,
while the APR’s predictions could address VIO drift.
This paper introduces MobileARLoc, an on-device visual
localisation framework for large-scale mobile AR that com-
bines the complementary properties of VIO and APR. Accu-
rate APR predictions should be consistent with the relative
odometry estimates obtained from VIO. Otherwise, the APR
prediction should be considered unreliable. MobileARLoc
applies this principle by alternating between two stages. The
alignment stage aligns the APR and VIO coordinate systems
while the pose optimization stage identifies reliable APR
poses and refines unreliable predictions. In the alignment
stage, MobileARLoc compares the output of the APR and
VIO subsystems. When several consecutive APR poses are
consistent with the VIO relative output, they are considered
accurate. MobileARLoc calculates the average of reliable
absolute predictions as the reference pose and the rigid trans-
formation between this reference pose and the corresponding
VIO poses to align the coordinate systems. Following the
alignment stage, MobileARLoc enters the pose optimization
stage. Each APR pose is compared to the corresponding
VIO pose. The APR prediction is output directly if reliable.
Otherwise, MobileARLoc outputs the VIO pose converted
into absolute world coordinates using rigid transformation. We
introduce a new similarity metric to detect VIO drift. When
drift is detected, MobileARLoc reenters the alignment stage
to select new reliable poses and calculate rigid transformation.
We summarize our main contributions as follows:
arXiv:2401.11511v1  [cs.CV]  21 Jan 2024
1) We design an APR-agnostic framework, MobileAR-
Loc to enable real-time on-device pose estimation. Mo-
bileARLoc leverages VIO information to select reliable
APR predictions and refine unreliable APR predictions
while compensating for drift.
2) We implement and evaluate MobileARLoc over two
popular APR models, PoseNet (PN) [14], [15] and MS-
Transformer (MS-T) [10]. MobileARLoc improves the
accuracy of MS-T as much as 47% on translation error
and 66% on rotation error in mean error over the average
of all three outdoor scenes.
3) We integrate MobileARLoc into a real-life mobile AR
application.
II. RELATED WORK
A. Absolute Pose Regression
Absolute Pose Regressors train deep neural networks to
regress the 6-DOF camera pose of a query image. The first
APR is PoseNet (PN) [14]. Since then, there have been several
improvements to APR, mainly related to the backbone archi-
tecture and loss functions [10], [13], [15]–[21]. MS-PN and
MS-Transformer (MS-T) [10], [22] extends the single-scene
paradigm of APR for learning multiple scenes. MapNet+ [13],
Direct-PN+ [19] and DFNetdm [20] finetune pre-trained net-
work on unlabeled test data to improve the accuracy. However,
in-test-time finetuning neural networks is time-consuming and
unlabeled test-set data is difficult to obtain in advance in real
applications [23]. Among these works, MapNet [13] leverages
the relative poses. It minimizes the loss of the per-image
absolute pose and the loss of the relative pose between image
pairs. However, formulating relative pose constraints as loss
terms during training shows limited accuracy improvement and
has been surpassed by state-of-the-art (SOTA) APRs like MS-
T [10]. Our framework improves accuracy at test time by using
the relative pose independently from the training strategy.
Compared to most modern approaches, MobileARLoc does
not rely on additional unlabeled test data.
B. Uncertainty estimation and Pose Optimization
APRs suffer from limited generalizability [11], [24].
Uncertainty-aware APRs aim to infer which images will
likely result in accurate pose estimation and identify the
outliers. Several prior works have explored uncertainty esti-
mation during APR training. Bayesian PoseNet [16] and AD-
PoseNet [25] model the uncertainty by measuring the variance
of several inferences of the same input data. CoordiNet [21]
models heteroscedastic uncertainty during training. Deng et
al. [26], [27] represent uncertainty by predicting a mixture of
multiple unimodal distributions. Zangeneh et al. [28] follows
a similar idea to produce arbitrarily shaped pose distribu-
tions. Although these uncertainty-aware APRs provide both
pose predictions and uncertainty estimates, the accuracy of
predictions is much lower than other APRs [10], [15], [20].
Moreover, existing uncertainty estimation methods can be
time-consuming [16], [25] and lack extensibility due to the
need for specific loss functions and training schemes [21],
[25], [27], [28].
Zhang et al. [29], Taira et al. [3], and Chen et al. [23]
verify whether the estimated pose is reliable by rendering
synthetic image or features based on the estimated pose.
Iterative refinement algorithms are used at test time. These
methods discard some of the advantages of APR, such as the
low computation and memory footprint, making APR even
less efficient than 3D structure-based methods.
Our framework enables greater flexibility compared to exist-
ing uncertainty-aware methods by being APR-agnostic. Main-
stream pre-trained APRs can be integrated into our framework
in a plug-and-play fashion. Our method performs a rigid
transformation of the VIO’s pose to optimize unreliable APR
poses directly, without iterative optimization. It improves the
accuracy of APRs with minimal overhead, enabling reliable
APR usage in mobile AR applications.
III. PROPOSED APPROACH
A. Definition
Given a query image Ii in a known scene, APR R outputs
global translation ˆxi and rotation ˆqi in an established world
coordinate system for the scene, so that R(Ii) = ˆpi =<
ˆxi, ˆqi > is the estimated camera pose for Ii. The Ground Truth
(GT) of Ii in the world coordinate system is pi =< xi, qi >.
The camera pose of Ii in VIO coordinate system is noted
pvio
i
=< xvio
i
, qvio
i
>. The relative translation between two
consecutive images Ii and Ii+1 is characterized by
ˆ∆trans(i + 1, i) = ||ˆxi+1 − ˆxi||2,
(1)
∆vio
trans(i + 1, i) = ||xvio
i+1 − xvio
i
||2,
(2)
∆trans(i + 1, i) = ||xi+1 − xi||2.
(3)
Similarly,
we
get
relative
rotation
between
Ii
and
Ii+1
in
degree
follow
[30],
q−1
denotes
the
conjugate
of
q,
and
we
assume
all
quaternions
are
normalized:
ˆ∆rot(i + 1, i)
=
2 arccos |ˆq−1
i+1ˆqi| 180
π ,
∆vio
rot(i
+
1, i)
=
2 arccos |qvio−1
i+1
qvio
i
| 180
π
and
∆rot(i + 1, i) = 2 arccos |q−1
i+1qi| 180
π .
ˆui,i+1 =< ˆ∆trans(i, i+1), ˆ∆rot(i, i+1) > is the odometry
of Ii and Ii+1 from predicted poses of APR. uvio
i,i+1 =<
∆vio
trans(i, i + 1), ∆vio
rot(i, i + 1) > is the odometry of Ii and
Ii+1 from the VIO system. Similarly, ui,i+1 =< ∆trans(i, i+
1), ∆rot(i, i + 1) > is the GT odometry of Ii and Ii+1. We
then define the Relative Position Error (RPE) and the Relative
Orientation Error (ROE) for the VIO and the APR as follows:
RPE<vio,GT >
i,i+1
= |∆trans(i + 1, i) − ∆vio
trans(i + 1, i)|
(4)
ROE<vio,GT >
i,i+1
= |∆rot(i + 1, i) − ∆vio
rot(i + 1, i)|
(5)
RPE<apr,vio>
i,i+1
= | ˆ∆trans(i + 1, i) − ∆vio
trans(i + 1, i)|
(6)
ROE<apr,vio>
i,i+1
= | ˆ∆rot(i + 1, i) − ∆vio
rot(i + 1, i)|
(7)
B. Detecting reliable pose estimations using VIO
Modern VIO systems have low drift at a small temporal
scale, and uvio
i,i+1 tends to be very close to the GT odometry,
ui,i+1. We can assume RPE<vio,GT >
i,i+1
and ROE<vio,GT >
i,i+1
are
almost 0. We model the uncertainty of the APR output,
ˆpi+1, with uvio
i,i+1, taking advantage of this property. If the
RPE<apr,vio>
i,i+1
and ROE<apr,vio>
i,i+1
of multiple consecutive im-
ages are very small, we consider these predictions are accurate.
We define a distance threshold dth for RPE<apr,vio> and
an orientation threshold oth for ROE<apr,vio>. An estimated
APR pose is considered accurate if the error close to GT within
dth
2
and oth
2 . Given two consecutive query images Ii, Ii+1,
1) Estimated poses of Ii and Ii+1 are accurate, then
RPE<apr,vio>
i,i+1
and ROE<apr,vio>
i,i+1
are lower than dth and
oth, respectively.
2) One of the estimated pose for Ii and Ii+1 is not accurate,
then either RPE<apr,vio>
i,i+1
should be larger than dth or
ROE<apr,vio>
i,i+1
should be larger than oth.
3) Both estimated poses of Ii and Ii+1 are inaccurate.
However, RPE<apr,vio>
i,i+1
remains lower than dth and
ROE<apr,vio>
i,i+1
lower than oth.
4) Both estimated poses of Ii and Ii+1 are inaccurate, and
either RPE<apr,vio>
i,i+1
is larger than dth or ROE<apr,vio>
i,i+1
is larger than oth.
When either RPE<apr,vio>
i,i+1
or ROE<apr,vio>
i,i+1
is larger than
its respective threshold (case (2) and (4)), the pose is flagged
as inaccurate and can thus be filtered out. Similarly, in case
(1), the two poses are identified as accurate. In case (3), two
inaccurate poses are identified as accurate. Our method uses a
probabilistic approach to reducing such false positives. APR
error tends to be random with a large variance. As such,
two consecutive images presenting a large APR error while
being close to each other in the same direction as the VIO
is a rare occurrence. By comparing more pairs of images,
we further reduce the probability of false positive, filtering
out the most unreliable predictions. We then obtain the rigid
transformation between the VIO coordinate system and the
world coordinate system by using the reliable predicted poses
and VIO poses. To ensure the rigid transform relationship’s
reliability, we calculate the average pose of selected predicted
poses as reference pose. The rotation and translation of the
coordinate system of VIO and the world coordinate system
change over time due to the VIO drift. Therefore, we only
need to update the reliable poses occasionally and optimize the
predicted pose by calculating the new rotation and translation.
C. MobileARLoc framework
Based on the above subsections, we present MobileARLoc,
a new APR-agnostic framework that combines the outputs of
APRs and information from smartphones’ VIO systems to
improve pose prediction accuracy. The framework keeps the
most reliable prediction poses with the help of VIO, identifies
APR
Alignment
Stage
Pose Optimization
Stage
VIO
Images
MobileARLoc
Fig. 1.
MobileARLoc framework in a mobile AR system.
unreliable poses, and optimizes them based on reliable poses.
MobileARLoc combines the distinctive features of APR pre-
dictions, which are locally noisy but drift-free, with mobile
VIO systems, which are locally smooth but tend to drift, as
shown by [13]. This framework consists of two alternating
looping stages: Alignment and Pose optimization, as shown
in Figure 1. The Alignment stage identifies multiple reliable
poses to calculate the reference pose. The Pose Optimization
stage optimizes unreliable poses based on this reference pose
and the VIO poses. MobileARLoc adaptively goes back to the
Alignment phase to recalculate the reference pose and thus
negate the effect of VIO drift.
Alignment stage checks the odometry of consecutive N +1
images. We consider Ij to be the first image to enter the align-
ment stage. When all N consecutive pairs of images satisfy the
requirement that RPE<apr,vio> ≤ dth and ROE<apr,vio> ≤
oth, predictions {ˆpi}j+N+1
i=j
of these N +1 consecutive images
from Ij to Ij+N+1 can considered as accurate predictions.
dth and oth are the relative pose checker’s distance and
orientation threshold to filter the inaccurate estimated poses.
If the difference between ˆui,i+1 and uvio
i,i+1 is less than dth
and oth simultaneously, poses ˆpi and ˆpi+1 become candidate
reliable predictions (RPs). If the difference between ˆui,i+1 and
uvio
i,i+1 is larger than dth or oth, we assume ˆpi is inaccurate
and discard all previous candidate RPs. Upon getting N + 1
candidate RPs, We perform geometric averaging from these
RPs using Weiszfeld’s algorithm2 and
[31]3 to obtain a
reference pose ¯P apr since we assume the pose error of APR is
normally distributed in space. We perform the same geometric
averaging from corresponding VIO poses to get the reference
pose
¯P vio in the VIO system. Once
¯P apr and
¯P vio are
obtained, MobileARLoc moves to the Pose Optimization stage.
In Pose Optimization stage, the framework checks the
current predicted pose against the previous pose as follows. For
the subsequent predicted poses {ˆpi}i=j+N+2 and correspond-
ing VIO poses {pvio
i
}i=j+N+2, we get the odometry ˆui−1,i of
ˆpi−1 and ˆpi in APR coordinates and odometry ˆuvio
i−1,i of pvio
i−1
and pvio
i
in VIO coordinates respectively. If RPE<apr,vio>
i−1,i
≤
dth and ROE<apr,vio>
i−1,i
≤ oth is satisfied, the APR output for
this image is considered reliable as the difference of odometry
between APR and VIO is small. The pose is considered
reliable and can be output directly. Otherwise, the pose is
optimized by OptimizePose(pvio, ¯P apr, ¯P vio). We calculate
2https://pypi.org/project/geom-median/.
3https://github.com/christophhagen/averaging-quaternions
TABLE I
DATASET DETAILS AND STATISTICS FOR HLOC (IMAGE RETRIEVAL WITH
TOP-20 RECALL), PN, AND MS-T.
Scenes Dataset quantity
Spatial
Storage (MB)
Runtime ( ms)
Train
Test
Extent (m)
HLoc PN MS-T HLoc PN MS-T
Square 2058
1023
40×25
6500 85
71
4965 4
11
Outdoor Church 1643
853
50×40
6600 85
71
6659 4
11
Bar
1834
838
55× 35
6600 85
71
6230 4
11
Stairs
873
222
5.5 × 4.5 × 6 1500 85
71
2263 4
11
Indoor
Office
1479
635
7.5 × 4
2300 85
71
5722 4
11
Atrium 1694
441
30 × 50
5100 85
71
4500 4
11
the rigid transformation between VIO coordinates and world
coordinates using ¯P apr and ¯P vio. Then we transform pvio to
world coordinates as pv2w and replace the unreliable pose.
We call the N + 1 consecutive predictions in Align-
ment stage for calculating the average reference pose
and predictions pass the relative pose threshold in Pose
Optimization stage as reliable predictions (RPs). To com-
pensate for VIO drift, we detect the drift and make the
system loop back to the Alignment stage adaptively: for each
RP in Pose Optimization stage, we calculate the similarity
between the ˆp =< ˆx, ˆq > and pv2w =< xv2w, qv2w > from
OptimizePose(pvio, ¯P apr, ¯P vio):
S(ˆp, pv2w) =
(
ˆx·xv2w
||ˆx||2·||xv2w||2 + |
ˆq·qv2w
||ˆq||2·||qv2w||2 |)
2
(8)
, where −0.5 < S(ˆp, pv2w) < 1. We use similarity here to
balance the difference of translation and rotation because they
have different units. If consecutive N reliable predictions have
S(ˆp, pv2w) ≤ γ, it indicates drift happens and the system loops
back to the Alignment since pv2w should be very close to RPs.
The local tracking of VIO system is used between the last
pose in Pose Optimization stage until new RPs are found in
the Alignment stage and new reference poses are updated.
IV. DATASET
This paper introduces a dataset captured using iPhone 14
Pro Max, the flagship ARKit 6 phone at the time. We provide
SfM GT and the pose labels of VIO. Our dataset highlights
the low drift of current mobile VIO solutions, such as ARKit,
supporting our assumption that VIO system can reinforce
absolute pose estimation. The resolution of all images is
1920 × 1440. All images are fed into an SfM framework
using COLMAP [32] to get the GT. Table I provides summary
statistics on the dataset. It also displays the storage require-
ments and runtime per request for HLoc, PN, and MS-T (see
Section VI-A for details).
V. IMPLEMENTATION
A. Desktop Implementation
There are four hyperparameters. dth and oth are the relative
pose checker’s distance and orientation thresholds to filter in-
accurate pose estimations. If among N +1 consecutive images,
N image pairs pass the relative pose checker, we consider
these pose estimations to be accurate. We set γ = 0.99,
dth = 0.4m and oth = 4◦ over all datasets. We set N = 2,
with one frame processed per second for the AR application
and all the experiments in this paper. We implement our
framework over two APR models:
PN. PoseNet (PN) is the baseline method. Since there is no
open source code for PoseNet [14], [15], we follow [13], [17]
and use ResNet34 [33] as the backbone network.
MS-T. MS-Transformer [10] (MS-T) extends the single-scene
paradigm of APR to learning multiple scenes in parallel and
is one of the most recent APRs with official opensource code.
Therefore, we trained one MS-T for all outdoor scenes and
one MS-T for all indoor scenes using the official code4.
We note APR methods integrated into our MobileARLoc
framework as APRvio. All APR models are implemented in
Python using PyTorch [34]. During training, all input images
are resized to 256 × 256 and then randomly cropped to 224 ×
224. For both PN and MS-T, we set an initial learning rate of
λ = 10−4. All experiments for evaluation in Section VII are
performed on an NVIDIA GeForce GTX 3090 GPU.
B. Application Implementation
We implement MobileARLoc as a mobile AR app using
Unity and ARKit to run on an iPhone 14 Pro Max. We convert
the pre-trained PN to ONNX format and incorporated it into
a Unity application. We use OpenCVforUnity5 for processing
query images and use Barracuda transferring resized images
to tensor as the input of the network.
VI. SYSTEM PERFORMANCE
A. Desktop Implementation
As mentioned in Section I, structure-based methods tend to
require significant resources that are not available on mobile
devices. Table I shows the performance of HLoc [2] pipelines6
prevents one-device camera relocalisation. In MobileARLoc,
the APR only requires storing neural network weights. The
memory requirement of the APR thus remains constant, be-
tween 71 and 85 MB depending on the backbone model.
Meanwhile, HLoc pipelines require 1) a pre-built 3D model;
2) an image database; 3) a local descriptor database; and 4) the
NetVLAD [35], SuperPoint [36], and SuperGlue models [37].
The memory represents between 1.5 and 6.6 GB per scene.
Regarding runtime, APRs only require a single forward pass
for each query image, and the runtime is similar for all
datasets, between 4 and 11 ms. In contrast, HLoc pipelines
takes up to 6.7 s on larger scenes.
B. Mobile Implementation
We assess the performance of our framework on an iPhone
14 Pro Max device on the setup described in Section V-B.
We measure each parameter over 200 samples. The average
processing time per image is 37 ms while the average time for
PN to infer an image is 39.5 ms. Under current ARKit’s imple-
mentation, VIO runs in a parallel loop every frame. Therefore,
4https://github.com/yolish/multi-scene-pose-transformer
5https://assetstore.unity.com/packages/tools/integration/
opencv-for-unity-21088
6https://github.com/cvg/Hierarchical-Localization
TABLE II
MEAN AND MEDIAN ABSOLUTE TRANSLATION/ROTATION ERRORS IN
m/◦. THE RATIO OF RPS AND OPT. POSES ARE PROVIDED IN TABLE III.
Only RPs Mean
Only RPs Median
Outdoor
PN
PNvio(ours)
MS-T
MS-Tvio(ours)
PN
PNvio(ours)
MS-T
MS-Tvio(ours)
Square
2.1/7.07
1.17/3.57
2.50/4.14
1.64/2.46
1.11/3.61
0.76/3.04
1.5/2.14
0.81/1.78
Church
1.53/8.04
0.82/4.0
1.91/11.9
0.65/3.0
0.73/3.97
0.59/3.0
0.71/3.08
0.49/2.01
Bar
1.44/4.03
0.78/2.89
1.65/2.82
0.86/1.97
0.66/2.82
0.52/2.38
0.69/1.65
0.52/1.47
average
1.69/6.38
0.92/3.49
2.02/6.29
1.05/2.48
0.83/3.47
0.62/2.81
0.97/2.29
0.61/1.75
Only Opt. Mean
Only Opt. Median
Square
2.8/10.5
1.4/2.14
3.25/5.54
1.27/2.12
1.45/4.38
0.98/2.16
2.22/2.44
1.04/1.89
Church
2.07/11.2
1.08/1.98
2.85/18.6
1.1/1.86
0.93/5.04
0.89/1.87
1.13/4.74
0.93/1.70
Bar
2.18/5.35
1.07/2.12
2.42/3.74
0.96/1.56
0.94/3.55
0.86/2.18
0.99/1.75
0.86/1.35
average
2.35/9.02
1.18/2.08
2.84/9.29
1.1/1.85
1.11/4.32
0.91/2.07
1.45/2.98
0.94/1.65
RPs + Opt. Mean
RPs+ Opt. Median
Square
2.1/7.07
1.3/2.8
2.49/4.14
1.44/2.27
1.11/3.61
0.84/2.36
1.5/2.14
0.96/1.87
Church
1.53/8.04
0.96/2.88
1.9/11.9
0.9/2.34
0.73/3.97
0.74/2.13
0.71/3.08
0.73/1.79
Bar
1.44/4.03
0.92/2.53
1.65/2.82
0.91/1.78
0.66/2.82
0.69/2.26
0.69/1.65
0.7/1.44
average
1.69/6.38
1.06/2.74
2.02/6.29
1.08/2.13
0.83/3.47
0.76/2.25
0.97/2.29
0.8/1.7
Only RPs Mean
Only RPs Median
Indoor
PN
PNvio(ours)
MS-T
MS-Tvio(ours)
PN
PNvio(ours)
MS-T
MS-Tvio(ours)
Stairs
0.35/7.51
0.27/5.25
0.26/6.8
0.22/4.56
0.27/4.89
0.25/4.88
0.18/4.33
0.16/3.6
Office
0.55/11.7
0.45/7.3
0.48/13.3
0.40/7.36
0.36/6.21
0.32/5.55
0.35/4.66
0.30/4.11
Atrium
2.71/10.2
1.79/6.14
3.66/9.08
1.95/5.03
1.78/5.54
1.2/4.3
2.0/3.98
1.35/3.23
average
1.2/9.8
0.84/6.23
1.47/9.73
0.86/5.65
0.80/5.55
0.59/4.91
0.84/4.32
0.6/3.65
Only Opt. Mean
Only Opt. Median
Stairs
0.50/11.76
0.21/4.88
0.32/11.9
0.19/3.5
0.40/4.98
0.20/4.96
0.25/7
0.13/3.74
Office
0.9/25.2
0.51/6.49
0.8/31.3
0.33/4.27
0.79/12.2
0.36/6.1
0.6/12.8
0.21/3.34
Atrium
3.13/13.2
2.26/12.9
4.6/13.5
1.2/1.95
2.53/7.3
2.09/4.9
2.6/6
1.16/1.6
average
1.51/16.73
0.99/8.09
1.91/18.9
0.57/3.24
1.24/8.16
0.88/5.32
1.15/8.6
0.5/2.89
RPs + Opt. Mean
RPs+ Opt. Median
Stairs
0.35/7.51
0.27/5.25
0.26/6.8
0.22/4.56
0.27/4.89
0.22/4.90
0.18/4.33
0.15/3.68
Office
0.55/11.7
0.47/7.13
0.48/13.3
0.39/6.89
0.36/6.21
0.34/5.8
0.35/4.66
0.28/4.0
Atrium
2.71/10.2
1.99/9.0
3.66/9.08
1.59/3.52
1.78/5.54
1.57/4.81
2.0/3.98
1.21/2.61
average
1.2/9.8
0.91/7.13
1.47/9.73
0.73/4.99
0.80/5.55
0.71/5.17
0.84/4.32
0.55/3.43
our pipeline can perform absolute camera localization on a
mobile device in less than 80 ms. The ResNet34-based PN
requires only 85 MB for weight storage.
VII. DATASET EVALUATION
We evaluate our framework on the datasets described in Sec-
tion IV and Section V-A. Due to the disparity in computational
scale between our approach and structure-based methods like
HLoc (refer to Table I), we do not present results for HLoc.
We evaluate the performance of APR and our framework
through two primary metrics. We consider the mean and
median APE and AOE in Tables II as shown in Equation
(9) and Equation (10) for all test frames. We also evaluate
the percentage of test images with pose predicted with high
(0.25m, 2◦), medium (0.5m, 5◦), and low (5m, 10◦) accuracy
levels proposed by [38] in Table III. The higher the percentage
of each accuracy level, the better the performance.
APE<apr,GT > = ||ˆxi − xi||2
(9)
AOE<apr,GT > = 2 arccos |q−1
i
ˆqi|180
π
(10)
A. Results
Reliable predictions: For outdoor scenes, Table III demon-
strates that selecting RPs comprising 41.8% to 51.3% of total
PN predictions and 41.6% to 51% of total MS-T predictions
significantly increases the percentage of each accuracy level.
Consequently, pose estimates below the low accuracy thresh-
old (5m, 10◦) are greatly reduced in all three outdoor scenes.
Moreover, RPs selected by PNvio and MS-Tvio exhibit higher
mean and median accuracy across all predictions compared to
PN and MS-T (Table II). The mean accuracy improvement
(a) PN and PNvio
(b) MS-T and MS-Tvio
Fig. 2.
Pose predictions for APR (left) and APRvio (right) for one test
sequence in the Square scene. Using MobileARLoc significantly decreases
the number of predictions with large error as well as the noisiness of pose
estimation error compared to APR alone.
0
5
10
15
20
Trans. error (m)
100
101
102
Number of samples
PNvio
PN
(a) PN and PNvio (Trans.)
0
10
20
30
Trans. error (m)
100
101
102
Number of samples
MS-Tvio
MS-T
(b) MS-T and MS-Tvio (Trans.)
Fig. 3.
Pose error distribution for Church scene. Using MobileARLoc
significantly decreases the number of predictions with large error.
surpasses the median accuracy improvement. In the three
outdoor scenes, PNvio-preserved RPs reduce mean translation
error by 44% to 46% and median translation error up to
33%. Mean rotation accuracy improves by 28% to 50%,
while median rotation accuracy improves up to 22%. MS-
Tvio-preserved RPs reduce mean translation error by 34% to
66% and median translation error by 11% to 32%. Mean
rotation accuracy improves by 30% to 75%, and median
rotation error improves by 11% to 35%. The improvement
for indoor scenes follows a similar pattern. Table III shows
that the percentage of each accuracy level is greatly increased
by selecting the RPs that amount for 48.5% to 76.2% of the
total PN pose estimations and 46.3% to 78.7% of the total
MS-T pose estimations. APE<apr,GT > and AOE<apr,GT > are
greatly reduced as shown in Table II. These outcomes indicate
that a significant portion of predictions with large errors
contribute to the lower accuracy. Our framework effectively
identifies RPs using VIO.
Optimized Poses: Tables II and III provide confirmation that a
portion of the pose estimates that fail the relative pose checker
in the pose optimization stage exhibit larger errors compared to
the median and mean pose errors of all predictions. The differ-
ence in accuracy is even more pronounced when compared to
the selected RPs. These unreliable poses significantly impact
overall accuracy. For outdoor scenes, PNvio improves mean
translation accuracy by 48% to 51% and median translation
accuracy by up to 32%. It also enhances mean rotation
accuracy by 60% to 82% and median rotation accuracy by
39% to 63%. Similarly, MS-Tvio improves mean translation
accuracy by 60% to 61% and median translation accuracy
by 13% to 53%. It also enhances mean rotation accuracy by
58% to 90% and median rotation accuracy by 23% to 64%.
Pose estimates below the low accuracy level are significantly
reduced for both PNvio and MS-Tvio. In the Church and Bar
scenes, no optimized pose estimates of MS-Tvio exceed 5
meters and 10 degrees. For indoor scenes, 20.2% and 36.7%
of PN pose estimates and 14.2% to 44.7% of the MS-T pose
estimates that do not pass the relative pose threshold in the
pose optimization stage have larger error compared with the
median and mean pose error of all predictions. The optimized
accuracy has experienced a substantial improvement.
Total results: As shown in Table III, less than 10 percent of
the predictions in test set are filtered out as unreliable pre-
dictions in alignment stage except for atrium. MobileARLoc
effectively optimizes unreliable predictions in pose optimiza-
tion stage, which leads to a much higher mean accuracy than
original APRs on both translation and rotation for all three
scenes. Figure 2 and Figure 3 show MobileARLoc improves
accuracy by reducing the incidence of outliers with large error
and the noisiness of APR predictions.
TABLE III
PERCENTAGE (%) OF RPS, OPTIMIZED POSES AND TOTAL POSES
PREDICTED WITH HIGH (0.25M, 2◦), MEDIUM (0.5M, 5◦), AND LOW (5M,
10◦) ACCURACY [38] (HIGHER IS BETTER). THE VALUE IN PARENTHESES
REPRESENTS THE RATIO (%) OF RPS, OPT. POSES, AND RPS + OPT. POSES
IN THE TEST SET.
Only Reliable Predictions
Dataset Scenes
PN
PNvio(ours)
MS-T
MS-Tvio(ours)
Square
3.2/18.4/87
5.6/27.2/96.4 (41.8)
2.9/17.9/81.1
3.8/26.9/91.9 (43.2)
Outdoor Church
2.2/21.9/82.1
3.8/33.4/93.0 (43.5)
6.9/29.7/79.0
12.4/48.7/96.1 (41.6)
Bar
4.1/31.7/89.5
6/43/96.7 (51.3)
8.5/34.6/90.8
12.2/46.4/97.4 (51)
Only Optimization
Square
1.8/13.7/79 (49.9) 4.1/20.8/94.3 (49.9)
2/10.1/72 (53.1)
1.5/12.7/97.8(53.1)
Outdoor Church 1.1/12.7/73.6 (55.6) 3.6/21.9/97.5 (55.6) 3.1/15.3/66.3 (57.4)
1.8/15.7/100 (57.4)
Bar
1.8/19.5/81 (45.9)
3.1/16.8/100 (45.9) 4.8/23.0/85.3 (44.6)
4/15.8/100 (44.6)
Reliable Predictions + Optimization
Square
3.2/18.4/87.0
4.8/23.7/95.3 (91.7)
2.9/17.9/81.1
2.5/19.1/95.1 (96.3)
Outdoor Church
2.2/21.9/82.1
3.7/27.0/95.5 (99.1)
6.9/29.7/79.0
6.3/29.6/98.3 (97)
Bar
4.1/31.7/89.5
4.7/30.7/98.3 (97.2)
8.5/34.6/90.8
8.4/32.1/98.6 (92)
Only Reliable Predictions
Dataset Scenes
PN
PNvio(ours)
MS-T
MS-Tvio(ours)
Stairs
5.4/48.2/87.4
7.7/51.4/93.7 (64)
18.5/58.1/86.9
22.9/69.3/94.1 (68.9)
Indoor Office
3.8/31.0/72.8
4.3/36/80.8 (76.2)
7.1/46.0/80
8.8/54.7/90.2 (78.7)
Atrium
0/5.4/71
0/8.4/86 (48.5)
0.4/7.3/66.7
1/13.7/88.7 (46.3)
Only Optimization
Stairs
1.3/42.9/75.3 (36.7)
0/53.2/100 (36.7) 9.1/34.8/71.2 (29.7) 34.8/69.7/100 (29.7)
Indoor Office 1.6/14.8/46.9 (20.2)
9.4/18/89.8 (20.2) 1.1/15.6/41.1 (14.2) 23.3/51.1/94.4 (14.2)
Atrium
0/2.5/64.6 (35.8)
0/5.1/75.3 (35.8)
0/1.5/46.7 (44.7)
0.5/4.1/100 (44.7)
Reliable Predictions + Optimization
Stairs
5.4/48.2/87.4
5/52.1/95.9 (90.7)
18.5/58.1/86.9
26.5/69.4/95.9 (98.6)
Indoor Office
3.8/31.0/72.8
5.4/32.2/82.7 (96.4)
7.1/46.0/80
11/54.1/90.8 (92.9)
Atrium
0/5.4/71.0
0/6.9/81.5 (84.3)
0/7.3/66.7
0.7/9/94.3 (91)
B. Analysis
Table III show that PN and MS-T have predictions that are
very inaccurate with large errors more than 5 meters and 10
degrees in both outdoor and indoor scenes. By calculating the
reference pose with RPs identified by VIO system, unreliable
predictions are optimized, resulting in significant improve-
ment. Outdoor, our framework improves the accuracy of MS-T
by 47% on mean translation error and 66% on mean rotation
error over all scenes average. Indoor, it improves the accuracy
of MS-T by 50% on mean translation error and 49% on mean
rotation error on all scenes average. Compared to the median
accuracy, our method provides a greater improvement in mean
accuracy. This is because the accuracy of our optimization
depends on the accuracy of the reference pose, which is
basically about the same error as the median accuracy. Outdoor
datasets yield better results due to the lower accuracy of
the APR in indoor scenes. This leads to misclassification of
inaccurate poses during alignment (case (3) in Section III-B),
making the calculation of the reference pose challenging and
introducing further inaccuracies during pose optimization.
VIII. CONCLUSION
This paper introduces MobileARLoc, a framework that
combines an APR with a local VIO tracking system to improve
the accuracy and stability of localization for markerless mobile
AR. The VIO evaluates and optimizes the APR’s accuracy
while the APR corrects VIO drift, resulting in improved
positioning. We evaluate MobileARLoc through dataset sim-
ulations. MobileARLoc improves the position accuracy by up
to 50% and rotation by up to 66% for different APR. The
mobile app can perform pose estimation in less than 80 ms
with minimal storage and energy consumption. Due to its low
system footprint, high accuracy, and robustness, MobileARLoc
enables pervasive markerless mobile AR at a large scale.
REFERENCES
[1] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, and
T. Sattler, “D2-net: A trainable cnn for joint description and detection
of local features,” in ieee/cvf conference on computer vision and pattern
recognition, 2019, pp. 8092–8101.
[2] P.-E. Sarlin, C. Cadena, R. Siegwart, and M. Dymczyk, “From coarse
to fine: Robust hierarchical localization at large scale,” in IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2019.
[3] H. Taira, M. Okutomi, T. Sattler, M. Cimpoi, M. Pollefeys, J. Sivic,
T. Pajdla, and A. Torii, “Inloc: Indoor visual localization with dense
matching and view synthesis,” in IEEE Conference on Computer Vision
and Pattern Recognition, 2018, pp. 7199–7209.
[4] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han, “Large-scale
image retrieval with attentive deep local features,” in IEEE international
conference on computer vision, 2017, pp. 3456–3465.
[5] T. Sattler, B. Leibe, and L. Kobbelt, “Efficient & effective prioritized
matching for large-scale image-based localization,” IEEE transactions
on pattern analysis and machine intelligence, vol. 39, no. 9, 2016.
[6] S. Yan, Y. Liu, L. Wang, Z. Shen, Z. Peng, H. Liu, M. Zhang, G. Zhang,
and X. Zhou, “Long-term visual localization with mobile sensors,” in
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2023, pp. 17 245–17 255.
[7] A. Moreau, T. Gilles, N. Piasco, D. Tsishkou, B. Stanciulescu, and
A. de La Fortelle, “Imposing: Implicit pose encoding for efficient
visual localization,” in IEEE/CVF Winter Conference on Applications
of Computer Vision, 2023, pp. 2892–2902.
[8] T. Braud, Z. Pengyuan, J. Kangasharju, and H. Pan, “Multipath com-
putation offloading for mobile augmented reality,” in 2020 IEEE In-
ternational Conference on Pervasive Computing and Communications
(PerCom).
IEEE, 2020, pp. 1–10.
[9] C. B. Fernandez, T. Braud, and P. Hui, “Implementing gdpr for mobile
and ubiquitous computing,” in 23rd Annual International Workshop on
Mobile Computing Systems and Applications, 2022.
[10] Y. Shavit, R. Ferens, and Y. Keller, “Learning multi-scene absolute pose
regression with transformers,” in IEEE/CVF International Conference on
Computer Vision, 2021, pp. 2733–2742.
[11] T. Sattler, Q. Zhou, M. Pollefeys, and L. Leal-Taixe, “Understanding the
limitations of cnn-based absolute camera pose regression,” in IEEE/CVF
conference on computer vision and pattern recognition, 2019.
[12] T. Scargill, G. Premsankar, J. Chen, and M. Gorlatova, “Here to stay: A
quantitative comparison of virtual object stability in markerless mobile
ar,” in 2022 2nd International Workshop on Cyber-Physical-Human
System Design and Implementation (CPHS).
IEEE, 2022, pp. 24–29.
[13] S. Brahmbhatt, J. Gu, K. Kim, J. Hays, and J. Kautz, “Geometry-
aware learning of maps for camera localization,” in IEEE conference
on computer vision and pattern recognition, 2018.
[14] A. Kendall, M. Grimes, and R. Cipolla, “Posenet: A convolutional
network for real-time 6-dof camera relocalization,” in IEEE international
conference on computer vision, 2015.
[15] A. Kendall and R. Cipolla, “Geometric loss functions for camera pose
regression with deep learning,” in IEEE conference on computer vision
and pattern recognition, 2017, pp. 5974–5983.
[16] ——, “Modelling uncertainty in deep learning for camera relocaliza-
tion,” in 2016 IEEE international conference on Robotics and Automa-
tion (ICRA).
IEEE, 2016.
[17] I. Melekhov, J. Ylioinas, J. Kannala, and E. Rahtu, “Image-based
localization using hourglass networks,” in IEEE international conference
on computer vision workshops, 2017.
[18] J. Wu, L. Ma, and X. Hu, “Delving deeper into convolutional neural net-
works for camera relocalization,” in 2017 IEEE International Conference
on Robotics and Automation (ICRA).
IEEE, 2017, pp. 5644–5651.
[19] S. Chen, Z. Wang, and V. Prisacariu, “Direct-posenet: absolute pose
regression with photometric consistency,” in 2021 International Confer-
ence on 3D Vision (3DV).
IEEE, 2021, pp. 1175–1185.
[20] S. Chen, X. Li, Z. Wang, and V. A. Prisacariu, “Dfnet: Enhance absolute
pose regression with direct feature matching,” in ECCV 2022. Tel Aviv,
Israel, October 23–27, 2022, Part X.
Springer, 2022.
[21] A.
Moreau,
N.
Piasco,
D.
Tsishkou,
B.
Stanciulescu,
and
A. de La Fortelle, “Coordinet: uncertainty-aware pose regressor
for reliable vehicle localization,” in IEEE/CVF Winter Conference on
Applications of Computer Vision, 2022.
[22] H. Blanton, C. Greenwell, S. Workman, and N. Jacobs, “Extending
absolute pose regression to multiple scenes,” in IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops, 2020.
[23] S. Chen, Y. Bhalgat, X. Li, J. Bian, K. Li, Z. Wang, and V. A. Prisacariu,
“Refinement for absolute pose regression with neural feature synthesis,”
arXiv preprint arXiv:2303.10087, 2023.
[24] T. Ng, A. Lopez-Rodriguez, V. Balntas, and K. Mikolajczyk, “Reassess-
ing the limitations of cnn methods for camera pose regression,” arXiv
preprint arXiv:2108.07260, 2021.
[25] Z. Huang, Y. Xu, J. Shi, X. Zhou, H. Bao, and G. Zhang, “Prior
guided dropout for robust visual localization in dynamic environments,”
in IEEE/CVF international conference on computer vision, 2019.
[26] H. Deng, M. Bui, N. Navab, L. Guibas, S. Ilic, and T. Birdal, “Deep
bingham networks: Dealing with uncertainty and ambiguity in pose
estimation,” International Journal of Computer Vision, vol. 130, no. 7,
2022.
[27] M. Bui, T. Birdal, H. Deng, S. Albarqouni, L. Guibas, S. Ilic, and
N. Navab, “6d camera relocalization in ambiguous scenes via continuous
multimodal inference,” in ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Part XVIII 16.
Springer, 2020.
[28] F. Zangeneh, L. Bruns, A. Dekel, A. Pieropan, and P. Jensfelt, “A
probabilistic framework for visual localization in ambiguous scenes,”
in 2023 IEEE International Conference on Robotics and Automation
(ICRA), 2023, pp. 3969–3975.
[29] Z. Zhang, T. Sattler, and D. Scaramuzza, “Reference pose generation for
long-term visual localization via learned features and view synthesis,”
International Journal of Computer Vision, vol. 129, pp. 821–844, 2021.
[30] R. Hartley, J. Trumpf, Y. Dai, and H. Li, “Rotation averaging,” Interna-
tional journal of computer vision, vol. 103, pp. 267–305, 2013.
[31] C. Gramkow, “On averaging rotations,” Journal of Mathematical Imag-
ing and Vision, vol. 15, no. 1-2, pp. 7–16, 2001.
[32] J. L. Schonberger and J.-M. Frahm, “Structure-from-motion revisited,”
in IEEE conference on computer vision and pattern recognition, 2016,
pp. 4104–4113.
[33] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in IEEE conference on computer vision and pattern
recognition, 2016, pp. 770–778.
[34] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An
imperative style, high-performance deep learning library,” Advances in
neural information processing systems, vol. 32, 2019.
[35] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, “Netvlad:
Cnn architecture for weakly supervised place recognition,” in IEEE
conference on computer vision and pattern recognition, 2016.
[36] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Superpoint: Self-
supervised interest point detection and description,” in IEEE conference
on computer vision and pattern recognition workshops, 2018.
[37] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, “Superglue:
Learning feature matching with graph neural networks,” in IEEE/CVF
conference on computer vision and pattern recognition, 2020.
[38] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg,
D. Safari, M. Okutomi, M. Pollefeys, J. Sivic et al., “Benchmarking 6dof
outdoor visual localization in changing conditions,” in IEEE conference
on computer vision and pattern recognition, 2018.
","nanSeveral studies have explored uncertainty estimation in APRs, attempting to identify images likely to yield accurate pose estimations and outliers. Bayesian PoseNet and AD-PoseNet model uncertainty by measuring the variance of multiple inferences of the same input data. CoordiNet models heteroscedastic uncertainty during training. Other approaches represent uncertainty by predicting a mixture of multiple unimodal distributions or produce arbitrarily shaped pose distributions. Despite their ability to provide both pose predictions and uncertainty estimates, these methods often exhibit lower accuracy compared to state-of-the-art APRs and can be time-consuming or inflexible due to specific loss functions and training schemes."
"Fine-tuning pre-trained models for adapting to new tasks and domains in Information Retrieval is practical and effective. However, its dependency on annotated data limits its transferability. Zero-shot and few-shot learning approaches exist, but suffer an effectiveness cost, especially in the context of first-stage retrievers. In this paper, we aim to address the issue of cross-topic discrepancy for a sparse retrieval model, SPLADE, by transferring a pre-training technique previously applied to language adaptation. By leveraging pre-training on the target data, this approach alleviates the need for labeled data and enhances domain adaptation capabilities. Our results demonstrate the effectiveness of our simple method, surpassing the zero-shot baseline and achieving comparable performance to fine-tuning in various domains despite the sparse nature of SPLADE.","Recent Information Retrieval models rely on pre-training followed by fine-tuning. This fine-tuning step is often carried out on large annotated datasets, requiring a significant amount of human effort. Transfer learning techniques are valuable when fine-tuning data is unavailable, such as in new domains or languages. However, adapting to new domains is challenging due to domain shift, which may result in poor performance of fine-tuned models on out-of-domain data. Existing research in domain adaptation primarily focuses on tasks rather than domains. Consequently, there is a need to investigate domain adaptation techniques specifically for Information Retrieval tasks. In this work, we address this gap and propose a simple domain adaptation method for sparse first-stage retrievers by adapting an existing language adaptation approach.","We adapt the approach proposed by Artetxe et al. [2] for language adaptation to cross-domain adaptation in IR. We assume that model parameters can be divided into domain-specific (Pdomain) and task-specific (Ptask) subsets, with the goal of adapting Pdomain to the specific domain while keeping Ptask intact. Pdomain is defined as the embeddings and the first k layers of the transformer architecture. The adaptation process consists of two pre-trainings (one on the source domain and one on the target domain) followed by fine-tuning on the source domain, before evaluating on the target domain. The underlying rationale is that the source pre-training allows the model to specialize in the IR task, while the target pre-training adapts the model to the domain-specific characteristics of the target domain. During inference, the model utilizes the combination of Ptarget_domain and Psource_task.","Experiments were conducted to evaluate the proposed approach using BERTbase pre-trained on English Wikipedia as the base model and a sparse first-stage ranker, SPLADE. A diverse set of target datasets covering various domains and document types were collected, including BEIR datasets, LoTTE's collection, and additional sources. Multiple variants of the proposed method were tested, differing in the number of layers dedicated to learning the target domain (k = 0 to 10). Baseline models included BM25, Zero-Shot Learning evaluation of SPLADE fine-tuned on MS MARCO, ablation of the source pre-training, and ablation of both source and target pre-trainings (zero-shot). The results demonstrated that the proposed adaptation approach outperforms the Zero-Shot Learning baseline by an average of 0.7 to 1.4 points in nDCG@10. Furthermore, the additional source pre-training step consistently improved performance by 0.1 to 0.8 points in nDCG@10, indicating the benefits of learning task-specific parameters from domain-specific parameters. Ablations of pre-trainings showed that both source and target pre-trainings contribute to the improved performance. Additionally, increasing the number of layers dedicated to the target domain yielded further gains of up to 0.5 points in nDCG@10, suggesting the importance of adapting more parameters to the target domain. Analysis of model sparsity revealed that the best models typically had higher sparsity levels, indicating their ability to build more appropriate representations of the target domains.","We proposed a simple approach to domain adaptation for IR based on a language adaptation approach. Results demonstrated the potential of the method when used with learned sparse retrievers, highlighting its value in scenarios where fine-tuning with high-quality annotated data is not feasible. The proposed approach allows for the re-use of fine-tuned models across multiple domains with less drop in effectiveness due to domain discrepancies. Future work will explore its application to dense retrievers and more costly methods, as well as extending it to more sophisticated pre-training approaches.",Simple Domain Adaptation for Sparse Retrievers,"Mathias Vast, Yuxuan Zong, Basile Van Cooten, Benjamin Piwowarski, Laure Soulier","Simple Domain Adaptation for Sparse Retrievers
Mathias Vast1,2, Yuxuan Zong2, Basile Van Cooten1, Benjamin
Piwowarski2, and Laure Soulier2
1Sinequa
2Sorbonne Université, CNRS, ISIR, F-75005 Paris, France ,
{firstname.lastname}@isir.upmc.fr
January 23, 2024
Abstract
In Information Retrieval, and more generally in Natural Language
Processing, adapting models to specific domains is conducted through
fine-tuning. Despite the successes achieved by this method and its versa-
tility, the need for human-curated and labeled data makes it impractical
to transfer to new tasks, domains, and/or languages when training data
doesn’t exist.
Using the model without training (zero-shot) is another
option that however suffers an effectiveness cost, especially in the case
of first-stage retrievers. Numerous research directions have emerged to
tackle these issues, most of them in the context of adapting to a task or
a language. However, the literature is scarcer for domain (or topic) adap-
tation. In this paper, we address this issue of cross-topic discrepancy for
a sparse first-stage retriever by transposing a method initially designed
for language adaptation. By leveraging pre-training on the target data to
learn domain-specific knowledge, this technique alleviates the need for an-
notated data and expands the scope of domain adaptation. Despite their
relatively good generalization ability, we show that even sparse retrievers
can benefit from our simple domain adaptation method.1
1
Introduction
Nowadays, most of the models that achieve state-of-the-art results in Natural
Language Processing (NLP) rely on the ""pre-train then fine-tune"" pipeline [4,
22]. In this setup, a model is first trained with self-supervised objectives, such
as Masked Language Modeling, on a large unlabeled corpus, before being fine-
tuned on a labeled dataset for a specific task. In Information Retrieval (IR),
this method has delivered huge improvements over the previous ""pre-BERT""
models and has given birth to a large variety of frameworks including, but not
1This paper is a preprint that was accepted at ECIR 2024
1
arXiv:2401.11509v1  [cs.IR]  21 Jan 2024
limited to, dense retrievers [13], learned sparse retrievers [5] and cross-encoders
[22]. For a given task, when enough good-quality labeled data is available, fine-
tuning an already pre-trained model is the best option. Thanks to datasets
such as MS MARCO [21], this condition might be met for the majority of the
downstream tasks in the generic domain for the English language, but in other
languages and/or specific domains, these resources may not exist.
This poses a problem since a fine-tuned model evaluated outside its training
domain might perform poorly [29]. When the volume of training data is insuffi-
cient for fine-tuning, it can even degrade the model’s performance [32]. Besides,
the increasing size of these models coupled with the difficulty of collecting a
satisfying amount of high-quality labeled data has led to a growing interest in
zero-shot or few-shot methods.
It is possible to tackle this issue by leveraging unlabeled data and pursuing
pre-training over task or domain-related data [7, 14] before fine-tuning. Pro-
viding a better initialization point alleviates the dependency on the amount of
in-domain labeled data available for fine-tuning. Differently, Artetxe et al. [2]
leverage pre-training as a means to adapt a model subpart for a specific lan-
guage, different from the source language used for pre-training and fine-tuning
the base model. In this paper, we consider a setup of cross-domain adaptation
where the model to be transferred can be trained on a source domain Dsource,
where an annotated corpus containing enough documents for both pre-training
and fine-tuning is available, whereas the target domain Dtarget contains enough
data for pre-training but no annotation.
We extend the work of [2] in two
ways to deal with ad-hoc IR. First, inspired by [3], we study which subparts
of the model should be pre-trained or fine-tuned – we do not only consider the
embeddings to be domain-dependent. Second, we propose a new pre-training
procedure on both the source and target domains, and not only on the target
one. This allows a model to learn task-specific parameters (on the source do-
main) when (and only when) its domain-specific ones are well set. As domain
shift impacts more strongly first-stage retrievers, we study a first-stage sparse
retriever, namely SPLADE [5]. Our approach allows one to share the fine-tuning
resources across multiple domains with less drop in effectiveness due to domain
discrepancies.
2
Related works
Transfer learning and adaptation to a specific context are long-standing re-
search topics. With the recent arrival of large language models, research shifted
to transferring these models at the lowest computational and annotation cost
possible [1]. To get rid of human annotation, works inspired by Doc2Query [23]
propose to generate queries for which a document in the target domain would
be relevant [16, 30]. Inversely, generative models can also be used to produce
pseudo-relevant documents [6] before fine-tuning a model on it. [9] even pro-
posed a method that generates a complete collection of both documents and
queries from a simple description of the target domain. For each of these meth-
2
Fine-tuning
on 
a) Base model
MLM on 
MLM on 
b.1) Source pre-training
b.2) Target pre-training
d) Evaluation on the target domain
c) Source fine-tuning
Figure 1: Illustration of the cross-domain adaptation process
ods, one can apply the traditional ""pretrain then fine-tune"" framework. Besides
the fact that this type of approach can be computationally costly, studying, as
in our work, pre-training techniques, is complementary. Indeed, as highlighted
in [15], better pre-training, either by using different self-supervised objectives
[12] or pursuing it longer [7, 14], often provides better results compared to
fine-tuning for first-stage methods.
Other works have tried to ease the transfer of (large) pre-trained models. We
can separate them into two categories, even though they share the same underly-
ing principle of distinguishing different sets of parameters during training. The
first family, referred to as Parameter-Efficient Fine-Tuning (or PEFT) [10, 11,
17, 19], achieves this objective by modifying the original model with an adapter
whose parameters are fine-tuned. Its success relies on the quality of the under-
lying pre-trained model, whose generality is worth being conserved while tuning
only a small portion of parameters on the target task. In IR, these methods
have started to be explored recently and, so far, have proven to be successful [18,
24, 28]. Orthogonal to these works, some papers do not use freezing to preserve
generality or to spare computations, but to adapt specific parts of the model for
a given language or task. In particular, [31] applies PEFT methods to Dense
Retrievers to disentangle domain and relevance learning. For each domain, a
Transformer backbone is trained to learn its linguistic features (MLM). Sepa-
rately, a distinct module is trained once and then shared, to predict relevance
based on these extracted features. Artetxe et al. [2] adapt a similar reasoning
but apply the disentangling within the Transformer backbone’s layers, thus in-
troducing a zero inference overhead. In this work, we extend their approach to
domain adaptation in the context of IR. Note that using PEFT following [31]
is an alternative that we will explore in the future.
3
3
Cross-Domain Adaptation of a Neural Network
model
We describe here how we adapted the pipeline introduced by Artetxe et al.
[2] to cross-domain adaptation. This approach can be extended to any model
in any setting by establishing a distinction between the model’s parameters
dedicated to learning the domain/language model, denoted Pdomain, and the
ones dedicated to learning the task denoted Ptask, which are supposed to be
distinct, i.e. Pdomain ∩ Ptask = ∅. Given the IR setting, we define Pdomain to be
the embeddings together with the first k layers of the transformer architecture
– whereas in the language adaptation setting, [2] only considered Pdomain to be
the embeddings.
Figure 1 describes the training process made of two distinct pre-trainings;
source (b.1) and target (b.2); and one fine-tuning on the source (c), before
evaluating (d) on the target domain:
Pre-training for Dsource (resp. Dtarget): As in [2], we continue to pre-train
BERT over Dsource (resp. Dtarget) while keeping the parameters’ subset
Ptask frozen. This step adapts the parameters Pdomain to the specificity of
the domain Dsource (resp. Dtarget). We denote P source
domain (resp. P target
domain)
the parameters after the MLM pre-training. Note that contrary to [2],
we keep the base model’s vocabulary unchanged throughout the pipeline.
Fine-tuning on Dsource: We leverage the previously pre-trained subset P source
domain
but keep it frozen while only fine-tuning the Ptask parameters. The ob-
jective of this step is to specialize the subset of parameters Ptask to the
IR task. We denote as P source
task
the fine-tuned parameters.
Contrary to [2], we add an additional pre-training step over the source corpus
(b.1 in Figure 1). We suppose that thanks to this step, the model can better
learn the downstream task as it is not fine-tuned on the same dataset (MS
MARCO) that it was originally pre-trained on (Wikipedia).
Finally, for inference on the Dtarget’s dataset, we combine the two subsets
P target
domain and P source
task
.
The underlying intuition is that the subset P source
task
is
task-specific while P target
domain is domain-specific.
The model should be able to
generalize better when used forIR in the target domain.
4
Experiments
We use BERTbase pre-trained on the English version of Wikipedia2 as our base
model and share its base vocabulary through every domain.
We focus on a
sparse first-stage ranker, SPLADE [5], as first-stage rankers particularly suffer
from domain shifts and SPLADE is known to be at the state-of-the-art among
them.
2The model is made available by Google on the HuggingFace’ Hub: bert-base-uncased
4
Table 1: Datasets considered in the study. Statistics on the average topic and
document length for each dataset are computed using BERT tokens.
Dataset
Domain
Dataset Statistics
Avg top./doc. len
# Top
# Doc
MSM-Passage
Generic
7.5 / 74.9
808K
8.8M
TREC-COVID (*)
Bio-Medical
16.0 / 243.5
50
171.3K
NFCorpus (*)
Bio-Medical
5.0 / 338.1
325
5.4K
BioASQ (*)
Bio-Medical
13.1 / 320.3
500
14.9M
FiQA-2018 (*)
Finance
13.6 / 175.1
648
57.6K
TREC-NEWS (*)
News
18.2 / 652.5
50
595K
Robust04 (*)
News
18.7 / 638.5
250
528.1K
ANTIQUE [8]
Web
11.9 / 52.3
200
403.7K
LoTTe-Wri. (**)
Literature
8.7 / 165.2
1071
200K
LoTTe-Tec. (**)
Technology
9.7 / 216.6
596
638.5K
LoTTe-Rec. (**)
Entertainment
9.3 / 181.6
924
167K
4.1
Datasets, baselines, and ablations
Two datasets in IR might differ in 1) domain/vocabulary, 2) document types,
and/or 3) topic types [25]. For a neural model, any of these distinctions can
impact the generalization power. The source dataset we consider across all our
experiments is MS MARCO [21] as it is the defacto dataset for fine-tuning pre-
trained models in IR and its domain can be regarded as ""generic Web search"".
We assemble a collection of target datasets from a subset of the BEIR benchmark
(*) [29], the LoTTE’s collection (**) [27], as well as other sources. The decision
about using or not some datasets from the BEIR benchmark is based on multiple
criteria, including the existence of distinct train and test sets and the task
affiliated with the dataset as we are only interested in pure IR in this case. We
processed them with the ir-datasets library [20]. Table 1 gives details about
the characteristics of each dataset and shows how we covered various domains,
ranging from bio-medical to literature.
To verify the benefits of our adaptation method, we experimented with mul-
tiple variants, differing on the Pdomain / Ptask split by the number of layers
k ∈ {0, 1, 2, 3, 4, 6, 8, 10} after the embedding belonging to Pdomain. We also
included two baselines and two ablations that we describe in what follows:
• BM25 [26], is a very strong baseline in domain adaptation [29] and is also
a sparse first-stage retriever;
• Zero-Shot Learning evaluation of the SPLADE model fine-tuned on
MS MARCO to quantify the benefits we can obtain with our adaptation
method;
• Ablation of the source pre-training, referred to as w/o source, as this
5
Table 2: Performance in nDCG@10 of our approach versus BM25 and Zero-
shot (SPLADE). Bold values are strictly superior to the Zero-shot baseline. †
and ‡: Improves upon BM25 baseline and Zero-Shot baseline respectively with
statistical significance (p ≤ 0.05) under the two-tailed Student’s t-test.
Method (→)
Baselines
Proposed pipeline (k = . . .)
Ablations
Dataset (↓)
BM25
0-shot
0 layer
1 layer
2
layers
4
layers
w/o pre-training
w/o source
TREC-COVID
58.1
71.3
68.8†
70.5†
72.1†
70.9†
67.9†
67.7†
NFCorpus
24.5
24.9
25.6
25.4
25.1
24.1
25.1
25.6
BioASQ
52.3
43.1
45.4‡
44.2
45.2‡
46.2‡
44.8‡
45.3‡
FiQA-2018
23.6
27.3
27.2†
27.5†
29.5†‡
29.0†‡
25.2†
28.3†
TREC-NEWS
31.4
32.1
33.3
32.8
33.0
36.5‡
31.9
33.1
Robust04
40.8
39.6
42.9‡
42.9‡
43.5‡
42.6‡
41.2
42.1‡
ANTIQUE
45.4
43.3
42.9
44.0
43.7
44.1
45.0‡
45.0‡
LoTTE-Rec.
39.3
46.2
47.6†‡
47.7†‡
47.8†‡
46.9†
46.8†
46.7†
LoTTE-Wri.
41.3
52.7
53.0†
53.8†‡
53.4†
54.1†‡
52.0†
52.9†
LoTTE-Tec.
25.3
36.8
38.2†
37.6†
37.4†
36.8†
36.8†
36.8†
Avg.
38.2
41.7
42.5
42.6
43.1
43.1
41.9
42.4
corresponds to the original approach described in Artetxe et al. (with
k = 0);
• Ablation of the source and target pre-trainings, referred to as w/o pre-
training (again with k = 0). Note that this is a zero-shot model. As we
are starting from a model pre-trained on Wikipedia, this ablation evaluates
whether our proposition has an impact.
All the project code, based on the library experimaestro-ir [33], including the
experimental details, is freely accessible1.
4.2
Results
Table 2 contains the results of these methods from which we can draw the
following conclusions: (i) Compared to the Zero-Shot Learning baseline, our
adaptation approach allows us to gain on average between 0.7 and 1.4 points
in nDCG@10, and more than 1 point for some datasets; (ii) Comparing ""w/o
source"" to our approach, it seems that the additional pre-training step over
the source domain helps the model to generalize better, with an average im-
provement between 0.1 and 0.8 points (for nDCG@10). This is sensible since
the model learns to adapt the task-specific parameters from domain-specific pa-
rameters; (iii) Similarly, no pre-training at all can hurt the performance up to
1.2 points in nDCG@10 (""w/o pre-training""), showing the benefit of our pre-
training procedure. However, in a zero-shot setting, we observe that fine-tuning
only the Transformer’s layers performs (""w/o pre-training"") on par with fine-
tuning the whole model (""0-shot""). Both setups are evaluated without using the
target dataset, and can thus be considered as Zero-Shot Learning. It indicates
1https://git.isir.upmc.fr/mat_vast/cross_domain_adaptation
6
Table 3: Evolution of the performance in nDCG@10 and of the sparsity of
SPLADE when the number of layers dedicated to the learning of the target
domain increases. Best result is highlighted in bold.
Method (→)
0-
shot
k=0
k=1
k=2
k=3
k=4
k=6
k=8
k=10
Average Performance
(nDCG@10)
41.7
42.5
42.6
43.1
42.6
43.1
43.1
43.0
42.6
Sparsity (x103)
9.3
7.3
9.2
11.5
8.4
10.3
10
9.8
9.3
that we could save some computations during fine-tuning without hurting per-
formance. (iv) Pre-training over additional layers can provide additional gains
of up to 0.5 points in nDCG@10 (columns ""1 layer"", ""2 layers"" and ""4 layers""
compared to the column ""0 layer""). Results however seemed to plateau beyond
k = 2 as illustrated by the minor differences between the columns k = 4, with
the best result in average, and k = 2. Table 3 summarizes the evolution of the
average performance along with the number of additional layers reserved for
learning the target domain. These results seem to comfort recent findings on
the role of the first layers’ attention head in a Transformer model [3]. It also
highlights that even though SPLADE-base models highly rely on the Masked
Language Modeling task, fine-tuning nevertheless remains an important part of
the process, and increasing the share of the parameters dedicated to the former
can infringe the model’s performance on the IR task.
Table 3 also gives additional insights into the sparsity achieved by the doc-
ument encoder of our SPLADE variants. Query encoder sparsity isn’t specified
as it says consistent along the variants and the baseline. We note that the best
models usually have a higher sparsity level, which tends to indicate that the as-
sociated model is able to build a more appropriate representation of each target
domain. However, more work is needed to understand why some other variants
happen to have lower sparsity values than the baseline while performing better.
Discussion and limitations.
This work is a preliminary study of how pre-
training can be used to ease domain adaptation in IR. We led the same series
of experiments with a second-stage ranker, namely monoBERT [22]. Interest-
ingly, the results showed that our approach didn’t provide any improvements
compared to the Zero-Shot Learning approach in this case. Previous work [15]
already mentioned the fact that second-stage rankers benefit more from larger
collections compared to pre-training on specific datasets. Our conclusion is that
our approach is too naive to provide any improvement at all, given the general-
ization power of monoBERT, and suggests that a better understanding of which
parameters are domain or task-specific, as well as their interplay, is necessary.
Further work is also needed to explore the impact of pre-training time given
corpus length on the final results. We leave both of these directions for future
works as well as the understanding of why second-stage rankers cannot draw any
benefits from specific pre-training. Finally, we did not compare with generative
methods (i.e., generating a query matching a document in the target domain)
7
such as [16] – these methods are costly, and we also need to investigate whether
the improvements are complementary with our (more cost-efficient) approach.
5
Conclusion
We presented a simple approach to domain adaptation for IR, based on a lan-
guage adaptation approach [2].
Results from our experiments highlight the
potential of our method with learned sparse retrievers and could be particu-
larly helpful when transferring fine-tuned models in contexts where high-quality
annotated data isn’t available or impossible to collect because it is either too ex-
pensive or complex. Another benefit of our approach is its re-usability. Once the
expensive fine-tuning has been performed, the subset Ptask can be re-used with
different subsets Pdomain pre-trained over different domains, which eliminates
the need to perform a fine-tuning over all the model for every new domain. In
the future, we would like to include dense retrievers as well as more costly meth-
ods in our study. In addition, we also want to extend it to more sophisticated
pre-training approaches.
6
Ackowledgements
This work is supported by the ANR project GUIDANCE ANR-23-IAS1-0003.
References
[1]
Sophia Althammer et al. “Annotating Data for Fine-Tuning a Neural
Ranker? Current Active Learning Strategies Are Not Better than Ran-
dom Selection”. In: Proceedings of the Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval in the
Asia Pacific Region. SIGIR-AP ’23. <conf-loc>, <city>Beijing</city>,
<country>China</country>, </conf-loc>: Association for Computing
Machinery, 2023, pp. 139–149. isbn: 9798400704086. doi: 10.1145/3624918.
3625333. url: https://doi.org/10.1145/3624918.3625333.
[2]
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. “On the Cross-
lingual Transferability of Monolingual Representations”. In: Proceedings of
the 58th Annual Meeting of the Association for Computational Linguistics.
arXiv:1910.11856 [cs]. 2020, pp. 4623–4637. doi: 10.18653/v1/2020.acl-
main.421. url: http://arxiv.org/abs/1910.11856.
[3]
Kevin Clark et al. “What Does BERT Look at? An Analysis of BERT’s
Attention”. In: Proceedings of the 2019 ACL Workshop BlackboxNLP: An-
alyzing and Interpreting Neural Networks for NLP (2019). doi: 10.18653/
v1/w19-4828. url: http://dx.doi.org/10.18653/v1/W19-4828.
8
[4]
Jacob Devlin et al. “BERT: Pre-training of deep bidirectional transformers
for language understandin”. In: 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational Linguistic. 2018. doi:
10.48550/arXiv.1810.04805.
[5]
Thibault Formal et al. SPLADE v2: Sparse Lexical and Expansion Model
for Information Retrieval. 2021. doi: 10.48550/ARXIV.2109.10086. url:
https://arxiv.org/abs/2109.10086.
[6]
Luyu Gao et al. Precise Zero-Shot Dense Retrieval without Relevance La-
bels. 2022. arXiv: 2212.10496 [cs.IR].
[7]
Suchin Gururangan et al. “Don’t Stop Pretraining: Adapt Language Mod-
els to Domains and Tasks”. In: Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics. Ed. by Dan Juraf-
sky et al. Online: Association for Computational Linguistics, July 2020,
pp. 8342–8360. doi: 10.18653/v1/2020.acl-main.740. url: https:
//aclanthology.org/2020.acl-main.740.
[8]
Helia Hashemi et al. “ANTIQUE: A Non-factoid Question Answering
Benchmark”. In: Advances in Information Retrieval. Ed. by Joemon M.
Jose et al. Cham: Springer International Publishing, 2020, pp. 166–173.
isbn: 978-3-030-45442-5.
[9]
Helia Hashemi et al. “Dense Retrieval Adaptation Using Target Domain
Description”. In: Proceedings of the 2023 ACM SIGIR International Con-
ference on Theory of Information Retrieval. ICTIR ’23. Taipei, Taiwan:
Association for Computing Machinery, 2023, pp. 95–104. isbn: 9798400700736.
doi: 10.1145/3578337.3605127. url: https://doi.org/10.1145/
3578337.3605127.
[10]
Neil Houlsby et al. Parameter-Efficient Transfer Learning for NLP. en.
June 2019. url: http : / / arxiv . org / abs / 1902 . 00751 (visited on
11/23/2022).
[11]
Edward J. Hu et al. LoRA: Low-Rank Adaptation of Large Language Mod-
els. en. arXiv:2106.09685 [cs]. Oct. 2021. url: http://arxiv.org/abs/
2106.09685 (visited on 01/30/2023).
[12]
Gautier Izacard et al. “Unsupervised Dense Information Retrieval with
Contrastive Learning”. en. In: Transactions on Machine Learning Research
(May 2022). issn: 2835-8856. url: https://openreview.net/forum?id=
jKN1pXi7b0 (visited on 07/08/2023).
[13]
Vladimir Karpukhin et al. “Dense Passage Retrieval for Open-Domain
Question Answering”. In: Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP). Online: Association
for Computational Linguistics, Nov. 2020, pp. 6769–6781. doi: 10.18653/
v1/2020.emnlp-main.550. url: https://aclanthology.org/2020.
emnlp-main.550.
9
[14]
Kundan Krishna et al. “Downstream Datasets Make Surprisingly Good
Pretraining Corpora”. In: Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers). Ed.
by Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki. Toronto,
Canada: Association for Computational Linguistics, July 2023, pp. 12207–
12222. doi: 10.18653/v1/2023.acl-long.682. url: https://aclanthology.
org/2023.acl-long.682.
[15]
Carlos Lassance, Hervé Dejean, and Stéphane Clinchant. “An Experimen-
tal Study on Pretraining Transformers from Scratch for IR”. en. In: Ad-
vances in Information Retrieval. Ed. by Jaap Kamps et al. Vol. 13980.
Series Title: Lecture Notes in Computer Science. Cham: Springer Na-
ture Switzerland, 2023, pp. 504–520. isbn: 978-3-031-28243-0 978-3-031-
28244-7. doi: 10 . 1007 / 978 - 3 - 031 - 28244 - 7 _ 32. url: https : / /
link.springer.com/10.1007/978- 3- 031- 28244- 7_32 (visited on
03/31/2023).
[16]
Minghan Li and Eric Gaussier. Domain Adaptation for Dense Retrieval
through Self-Supervision by Pseudo-Relevance Labeling. 2022. arXiv: 2212.
06552 [cs.IR].
[17]
Xiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing Continuous
Prompts for Generation. arXiv:2101.00190 [cs]. Jan. 2021. doi: 10.48550/
arXiv.2101.00190. url: http://arxiv.org/abs/2101.00190 (visited
on 04/22/2023).
[18]
Robert Litschko, Ivan Vulić, and Goran Glavaš. “Parameter-Efficient Neu-
ral Reranking for Cross-Lingual and Multilingual Retrieval”. In: Proceed-
ings of the 29th International Conference on Computational Linguistics.
Ed. by Nicoletta Calzolari et al. Gyeongju, Republic of Korea: Interna-
tional Committee on Computational Linguistics, Oct. 2022, pp. 1071–
1082. url: https://aclanthology.org/2022.coling-1.90.
[19]
Xiao Liu et al. P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-
tuning Universally Across Scales and Tasks. arXiv:2110.07602 [cs]. Mar.
2022. doi: 10.48550/arXiv.2110.07602. url: http://arxiv.org/abs/
2110.07602 (visited on 04/17/2023).
[20]
Sean MacAvaney et al. “Simplified Data Wrangling with ir_datasets”. In:
SIGIR. 2021.
[21]
Tri Nguyen et al. “MS MARCO: A Human Generated MAchine Reading
COmprehension Dataset.” In: CoRR abs/1611.09268 (2016). url: http:
//dblp.uni-trier.de/db/journals/corr/corr1611.html#NguyenRSGTMD16.
[22]
Rodrigo Nogueira and Kyunghyun Cho. Passage Re-ranking with BERT.
2019. url: http://arxiv.org/abs/1901.04085.
[23]
Rodrigo Nogueira et al. “Document Expansion by Query Prediction”. In:
ArXiv abs/1904.08375 (2019). url: https://api.semanticscholar.
org/CorpusID:119314259.
10
[24]
Vaishali Pal et al. “Parameter-Efficient Sparse Retrievers and Rerankers
Using Adapters”. In: Advances in Information Retrieval. Ed. by Jaap
Kamps et al. Cham: Springer Nature Switzerland, 2023, pp. 16–31. isbn:
978-3-031-28238-6.
[25]
Sinno Jialin Pan and Qiang Yang. “A Survey on Transfer Learning”. In:
IEEE Transactions on Knowledge and Data Engineering 22.10 (2010),
pp. 1345–1359. doi: 10.1109/TKDE.2009.191.
[26]
Stephen E. Robertson et al. “Okapi at TREC-3”. In: Text Retrieval Con-
ference. 1994. url: https://api.semanticscholar.org/CorpusID:
3946054.
[27]
Keshav Santhanam et al. “ColBERTv2: Effective and Efficient Retrieval
via Lightweight Late Interaction”. In: Proceedings of the 2022 Conference
of the North American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. Ed. by Marine Carpuat, Marie-
Catherine de Marneffe, and Ivan Vladimir Meza Ruiz. Seattle, United
States: Association for Computational Linguistics, July 2022, pp. 3715–
3734. doi: 10 . 18653 / v1 / 2022 . naacl - main . 272. url: https : / /
aclanthology.org/2022.naacl-main.272.
[28]
Weng Lam Tam et al. Parameter-Efficient Prompt Tuning Makes Gener-
alized and Calibrated Neural Text Retrievers. July 2022. doi: 10.48550/
arXiv.2207.07087. url: http://arxiv.org/abs/2207.07087 (visited
on 04/10/2023).
[29]
Nandan Thakur et al. BEIR: A Heterogenous Benchmark for Zero-shot
Evaluation of Information Retrieval Models. Oct. 2021. doi: 10.48550/
arXiv.2104.08663. url: http://arxiv.org/abs/2104.08663 (visited
on 04/14/2023).
[30]
Kexin Wang et al. “GPL: Generative Pseudo Labeling for Unsupervised
Domain Adaptation of Dense Retrieval”. In: Proceedings of the 2022 Con-
ference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies (2022). doi: 10.18653/
v1/2022.naacl-main.168. url: http://dx.doi.org/10.18653/v1/
2022.naacl-main.168.
[31]
Jingtao Zhan et al. Disentangled Modeling of Domain and Relevance for
Adaptable Dense Retrieval. en. arXiv:2208.05753 [cs]. Aug. 2022. url:
http://arxiv.org/abs/2208.05753 (visited on 12/28/2023).
[32]
Xinyu Zhang, Andrew Yates, and Jimmy Lin. “A Little Bit Is Worse
Than None: Ranking with Limited Training Data”. In: Proceedings of Sus-
taiNLP: Workshop on Simple and Efficient Natural Language Processing.
Ed. by Nafise Sadat Moosavi et al. Online: Association for Computational
Linguistics, Nov. 2020, pp. 107–112. doi: 10.18653/v1/2020.sustainlp-
1.14. url: https://aclanthology.org/2020.sustainlp-1.14.
11
[33]
Yuxuan Zong and Benjamin Piwowarski. “XpmIR: A Modular Library
for Learning to Rank and Neural IR Experiments”. In: Proceedings of the
46th International ACM SIGIR Conference on Research and Development
in Information Retrieval. SIGIR ’23. , Taipei, Taiwan, Association for
Computing Machinery, 2023, pp. 3185–3189. isbn: 9781450394086. doi:
10.1145/3539618.3591818. url: https://doi.org/10.1145/3539618.
3591818.
12
","Transfer learning has been explored extensively in Information Retrieval (IR) to mitigate the dependency on annotated data. Approaches like Doc2Query generate queries that correspond to relevant documents in the target domain, enabling the application of pre-trained models. Generative models can also produce pseudo-relevant documents for fine-tuning. Additionally, researchers have proposed methods for generating a complete collection of both documents and queries from a simple description of the target domain. These techniques can be used in conjunction with fine-tuning to achieve better results. Other works seek to ease the transfer of large pre-trained models by distinguishing between domain-specific and task-specific parameters. Fine-tuning is limited to task-specific parameters, allowing for better domain adaptation while preserving the generality of the model. Previous research has also shown that pre-training can yield better results compared to fine-tuning for first-stage methods.nan"
"Long text generation, such as novel writing or discourse-level translation with extremely long contexts, presents significant challenges to current language models. Our proposed method, Temp-Lora, introduces an alternative concept. Instead of relying on the KV cache to store all context information, Temp-Lora embeds this information directly into the model’s parameters. Extensive experiments validate the effectiveness of Temp-Lora.","Long text generation has become increasingly important in a variety of real-world applications, ranging from creative writing assistance to generative agents. However, the generation of coherent and contextually relevant long text poses significant challenges to language models (LMs), particularly in terms of understanding and maintaining contexts that are longer than the model’s pre-defined context window size. Existing methods, whether based on length extrapolation or context window extension, aims to store extensive text information within the KV cache, thereby improving the model’s long text comprehension. However, they demand significant hardware resources during training and/or inference. Consequently, in many applications where LMs are frequently queried for long text processing, users often resort to other strategies such as memory or summarization to reduce computational cost. In this paper, we propose an alternative method, Temp-Lora, which stores context information in the model’s parameters instead of the KV cache.","We evaluate Temp-Lora on two benchmark datasets, PG19 (Rae et al., 2019) and GuoFeng (Wang et al., 2023b). We evaluate Temp-Lora across models with different context window sizes and find that Temp-Lora substantially reduces their perplexity on long texts with a large margin (-13.2% on a subset PG19 and -29.6% on a subset of GuoFeng). This trend of reduced perplexity becomes increasingly pronounced as the context length increased, which can be simply concluded as “with greater text comes greater necessity”.","Further analysis also revealed that Temp-Lora, by shortening the maximum input length, can greatly reduce hardware consumption.","We proposed Temp-Lora, an alternative way for efficient long text generation. The essence of the Temp-Lora framework lies in training during the inference process using the generated output. It enables the storage of nearly infinite context information directly within the model’s parameters, marking a distinct difference from existing attention weights-based techniques. Our experimental results across various applications, including language modeling and discourse-level literary translation, demonstrated the profound impact of Temp-Lora. We showed that Temp-Lora not only greatly enhances the quality of long text generation but also significantly reduces computational costs. In particular, Temp-Lora led to remarkable improvements in perplexity (a reduction of 13.1% on a subset of PG19 and 29.6% on a subset of GuoFeng), as well as increases in BLEU (by 53.2%) and COMET scores (by 8.4%). The effectiveness of Temp-Lora becomes increasingly apparent as text length grows.",With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation,"Y. Wang, D. Ma, D. Cai","With Greater Text Comes Greater Necessity:
Inference-Time Training Helps Long Text Generation
Y. Wang∗
D. Ma∗
D. Cai
Confidential Institutes
ywang462-c@my.cityu.edu.hk, tianjibai@foxmail.com, dcai@se.cuhk.edu.hk
Abstract
Long text generation, such as novel writing
or discourse-level translation with extremely
long contexts, presents significant challenges
to current language models. Existing methods
mainly focus on extending the model’s context
window through strategies like length extrap-
olation. However, these approaches demand
substantial hardware resources during the train-
ing and/or inference phases.
Our proposed method, Temp-Lora, introduces
an alternative concept. Instead of relying on
the KV cache to store all context information,
Temp-Lora embeds this information directly
into the model’s parameters. In the process
of long text generation, we use a temporary
Lora module, progressively trained with text
generated previously. This approach not only
efficiently preserves contextual knowledge but
also prevents any permanent alteration to the
model’s parameters given that the module is
discarded post-generation.
Extensive experiments on the PG19 lan-
guage modeling benchmark and the GuoFeng
discourse-level translation benchmark validate
the effectiveness of Temp-Lora. Our results
show that:
1) Temp-Lora substantially en-
hances generation quality for long texts, as
indicated by a 13.2% decrease in perplexity
on a subset of PG19, and a 29.6% decrease
in perplexity along with a 53.2% increase in
BLEU score on GuoFeng, 2) Temp-Lora is
compatible with and enhances most existing
long text generation methods, and 3) Temp-
Lora can greatly reduce computational costs by
shortening the context window. While ensuring
a slight improvement in generation quality (a
decrease of 3.8% in PPL), it enables a reduction
of 70.5% in the FLOPs required for inference
and a 51.5% decrease in latency.
1
Introduction
Long text generation has become increasingly im-
portant in a variety of real-world applications, rang-
∗Equal Contribution
ing from creative writing assistance (Shi et al.,
2022), chat-style AI assistant (OpenAI, 2023) to
generative agents (Park et al., 2023). However, the
generation of coherent and contextually relevant
long text poses significant challenges to language
models (LMs), particularly in terms of understand-
ing and maintaining contexts that are longer than
the model’s pre-defined context window size.
Existing methods, whether based on length ex-
trapolation (Press et al., 2021; Su et al., 2023) or
context window extension (Chen et al., 2023c; Han
et al., 2023; Dao et al., 2022; Peng et al., 2023;
Chen et al., 2023a), aims to store extensive text in-
formation within the KV cache, thereby improving
the model’s long text comprehension. However,
they demand significant hardware resources during
training and/or inference. Consequently, in many
applications where LMs are frequently queried for
long text processing, users often resort to other
strategies such as memory or summarization to re-
duce computational cost (Park et al., 2023).
In this paper, we propose an alternative method,
Temp-Lora, which stores context information in
the model’s parameters instead of the KV cache.
The core idea of our approach is extremely simple:
we store the context information in a temporary
Lora module (Hu et al., 2021) that only exists dur-
ing long text generation. We update this module
in a streaming fashion during the generation pro-
cess, using the generated content as training data,
thereby achieving the goal of storing knowledge in
the model parameters. Once inference is complete,
this module is discarded to avoid permanently im-
pacting the model’s parameters. This approach
allows us to efficiently store nearly infinite context
information without extending the context window.
We
evaluate
Temp-Lora
on
two
bench-
mark datasets, PG19 (Rae et al., 2019) and
GuoFeng (Wang et al., 2023b).
We evaluate
Temp-Lora across models with different context
window sizes and find that Temp-Lora substantially
arXiv:2401.11504v1  [cs.CL]  21 Jan 2024
LM
Lora0
Training
Generate
Lora1
…
Lorak-1
Generate
Complete
Delete
Generate
Prefix
Input
Chunk0
Chunk1
Training
…
Chunkk
Generate
Output 𝓨
Figure 1: The framework of long text generation with
Temp-Lora
reduces their perplexity on long texts with a large
margin (-13.2% on a subset PG19 and -29.6%
on a subset of GuoFeng). This trend of reduced
perplexity becomes increasingly pronounced as
the context length increased, which can be simply
concluded as “with greater text comes greater
necessity”.
Further analysis also revealed that
Temp-Lora, by shortening the maximum input
length, can greatly reduce hardware consumption.
2
Temp-Lora
The Temp-Lora framework, depicted in Figure 1,
presents a straightforward yet innovative approach
for long text generation.
At the heart of this
method lies the progressive training of a tempo-
rary Lora module (Hu et al., 2021), which is named
Temp-Lora, on previously-generated text in an auto-
regressive manner. The continuous adaptation and
refinement of the Temp-Lora module ensure an
evolving understanding of both recent and distant
contexts. It is important to highlight the frame-
work’s adaptability in handling cases where the
initial input lengths are already substantial, such as
novels or academic papers. In such scenarios, we
may proactively pre-train the Temp-Lora module
with the contexts, thereby laying a robust founda-
tion for enhanced context comprehension in the
generation process.
The algorithm is detailed in Algorithm 1. During
the generation process, tokens are generated chunk-
by-chunk. At each chunk generation, we use the
most recent LX tokens as the input X for gen-
erating the subsequent tokens. Once the number
of generated tokens hits a predefined chunk size
∆, we initiate the training of the Temp-Lora mod-
ule using the latest chunk and start the next chunk
generation afterwards. It is important to note that
LX + ∆ ≤ W, where W is the model’s context
window size. In our experiments, we set ∆ + LX
equal to W to take full advantage of the model’s
Algorithm 1: Long Text Generation with
Temp-Lora
Input: Input X, Input Length LX , Training
Input XT , Training Input Length
LT ; Chunk size ∆; Learning rate α;
Epoch number n; LM Θ.
Output: Long sequence Y
1 if len(X) > LX then
2
Pre-train Θ on X to obtain LM with an
initial Temp-Lora module Θ0;
3 end
4 Total token number: i ← len(X);
5 Temp-Lora ID: k ← 0;
6 Token number in this chunk: m ← 0;
7 Output sequence Y = X;
8 X = X[−(min(LX , len(X)) :];
9 while In Generation do
10
if m < ∆ then
11
Y[i] = Θk(X + Y[−m :]);
12
i + +, m + +;
13
end
14
else
15
Chunkk = Y[−m :];
16
XT = Y[−(LT + m) : −m];
17
k + +;
18
Train Θk: In XT → Out Chunkk
19
(n epochs, learning rate α);
20
X = Y[−LX :];
21
m = 0;
22
end
23 end
24 Destroy Temp-Lora module: Θk ← Θ;
context window size.
For the training of the Temp-Lora module, it is
crucial to recognize that learn to generate the new
chunk without any condition may not constitute a
meaningful training objective and potentially lead
to significant overfitting. To address this concern,
we incorporate the preceding LT tokens of each
chunk into our training process, using them as the
input and the chunk as the output.
Cache Reuse
For more efficient inference, we
propose a strategy called cache reuse. In the stan-
dard framework, after updating the Temp-Lora
module, we need to re-compute the KV states with
the updated parameters. Alternatively, we can also
reuse the existing cached KV states while employ-
ing the updated model for subsequent text genera-
tion. Concretely, we only recompute the KV states
with the latest Temp-Lora module when the model
generates up to the maximum length (context win-
dow size W). We empirically find that cache reuse
can accelerate the generation speed without signifi-
cantly compromising the generation quality.
3
Experiments
We evaluate the proposed Temp-Lora framework
using the Llama2 (Touvron et al., 2023) families
considering its wide adoption and popularity. Its ef-
fectiveness is evaluate in two different long text
generation tasks: 1) Novel Generation; and 2)
Discourse-Level Literary Translation.1
Dataset:
The first dataset we adopt is a subset
of the long text language modeling benchmark,
PG19 (Rae et al., 2019). It is a well-established
benchmark that consists of more than 28K books
which were published before 1919. Since long
texts are required to evaluate the effectiveness of
the Temp-Lora framework, and considering that
some of the PG-19 data might already be included
in Llama2’s training set, we selected the 100 books
with the highest PPL from those whose lengths
range between 200K and 600K tokens. From these,
we randomly sampled 40 books as our test set.
We also evaluate the effectiveness of Temp-Lora
on a downstream task, Discourse-Level Literary
Translation, with a randomly sampled subset of
GuoFeng dataset from WMT 2023 (Wang et al.,
2023b,a).
This subset contains 20 web novels,
originally written in Chinese by various novelists
and subsequently translated into English by profes-
sional translators. Their lengths (EN + ZH) range
from a minimum of 84K to a maximum of 370K.
We designate Chinese as the source language and
English as the target language. We merge sentences
from the GuoFeng dataset into segments, each ap-
proximately 512 tokens in length. The model’s
input includes the most recent source and target
texts, along with the current source segment, and it
is required to output the translation of this segment.
Baselines:
We apply the Temp-Lora framework
to three Llama2 variants: the standard Llama2-
7B-4K 2, standard Llama2-13B-4K 3, Llama2-
1Codes and Data are available at: https://github.
com/TemporaryLoRA/Temp-LoRA/tree/main
2https://huggingface.co/meta-llama/
Llama-2-7b
3https://huggingface.co/meta-llama/
Llama-2-13b
7B-32K 4 whose context window is extended via
position interpolation (Chen et al., 2023c), and a
chat-style model Yi-Chat-6B5.
Evaluation Metric:
The primary metric is per-
plexity (PPL), a standard measure in language mod-
eling to assess the model’s prediction capability.
We employ a sliding window approach for per-
plexity measurement as suggested by (Press et al.,
2021). In the translation task, in addition to PPL,
we also employed two common evaluation met-
rics used in machine translation, BLEU (Papineni
et al., 2002) and COMET (Rei et al., 2020), for
comprehensive evaluation.
Setup:
We set the chunk size ∆ = 1024 for all
models. For the Llama2-7B-32K, we additionally
set two wider chunk size ∆ = 2048 and 4096 to
investigate the effects of chunk size on generation
quality and computational cost.
The input length LX for all models is set to be
the difference between the window size W and
the chunk size: LX = W − ∆. For the Llama2-
7B-32K, we noticed that its PPL decreases when
the number of tokens within its context window ap-
proaches 32K. Therefore, we set its context window
size W to 24K, which, based on our preliminary
experiments, is the optimal context window size
we’ve identified. Note that at the beginning of a
document, the number of context tokens may be
smaller than LX . In such cases, we simply take all
context as the input. In the generation process, once
the token number in the context window reaches
the context window size (4K and 24K for differ-
ent models) we will rebuild and re-compute the
KV states taking the LX recent tokens as input X.
When we update the Temp-Lora module, the length
LT of training input XT is set to 1024.
For the experiment on GuoFeng, each segment is
treated as a chunk. In other words, after the model
translates a complete segment, this segment is then
used to update the Temp-Lora module. We decode
the translations with beam search (beam width = 1,
repetition penalty = 1.12).
All experiments are done with a single NVIDIA
A800 GPU. We set the learning rate α = 5 × 10−5
and Num. of epochs n = 2 for all models. We
use a linear learning rate warmup for the first 2
chunks. We set Temp-Lora α = 64, rank = 64,
4https://huggingface.co/
togethercomputer/LLaMA-2-7B-32K
5https://huggingface.co/01-ai/
Yi-6B-Chat
Model
∆
0-100K
100-200K
200-300K
300-400K
400-500K
500K+
Avg.
7B-4K
-
10.47
10.21
10.03
9.24
8.96
4.61
10.19
+TL
1024
10.09(3.6%)
9.55(6.4%)
9.14(8.8%)
8.47 (8.3%)
8.31(7.2%)
4.003(13.2%)
9.58(5.9%)
13B-4K
-
9.49
9.27
9.11
8.49
8.05
4.24
9.25
+TL
1024
9.18(3.3%)
8.47(5.7%)
8.40(7.7%)
7.83(7.7%)
7.55(6.1%)
3.74(11.6%)
8.75(5.3%)
7B-32K
-
10.56
10.25
9.98
9.03
9.17
4.09
10.21
+TL
1024
10.25(2.9%)
9.69(5.4%)
9.21(7.7%)
8.37(7.2%)
8.42(8.2%)
3.73(8.9%)
9.69(5.0%)
+TL + CR
1024
10.26(2.8%)
9.71(5.3%)
9.22(7.5%)
8.39(7.1%)
8.43(8.0%)
3.72(9.0%)
9.71(4.9%)
+TL
2048
10.27(2.7%)
9.72(5.2%)
9.24(7.4%)
8.39(7.0%)
8.44(7.9%)
3.78(7.5%)
9.72(4.8%)
+TL
4096
10.33(2.2%)
9.77(4.6%)
9.31(6.7%)
8.42(6.7%)
8.60(6.2%)
3.91(4.5%)
9.77(4.3%)
Table 1: PPL on text with different context lengths. All tokens in PG19 are divided into six segments based on their
position in the novel, i.e., the actual context length of the token. For instance, if a token is the 160K-th token in
a novel, it would be categorized into the 100-200K segment. In this table, we report the PPL of various models
under different settings in each segment. The percentages in () indicate the relative reduction of PPL of the model
compared to the base model. TL and CR are short for Temp-Lora and cache reuse, respectively.
Model
PPL↓
BLEU↑
COMET↑
Yi-6B-Chat
5.67
12.4
72.5
+TL
3.99(-29.6%) 19.0(53.2%) 78.6(8.4%)
Table 2: PPL, BLEU, and COMET of Yi-6B-Chat with
and without Temp-Lora.
dropout = 0.05. Its training is in bfloat16 format,
using Deepspeed ZeRO Stage 2, Flash Attention
V2 (Dao, 2023).
3.1
Main Results
PG19:
Table 1 presents the PPL comparisons
across various models with and without the Temp-
Lora module on PG19. We divide each document
into segments ranging from 0-100K to 500K+ to-
kens, as shown in Table 1. Intuitively, In the ini-
tial segments such as 0-100K, where there is less
context information, the improvement from Temp-
Lora should be relatively modest. In contrast, as the
segments grow longer and more information falls
outside the model’s context window, Temp-Lora’s
effects should become more pronounced.
The experimental results in Table 1 confirm our
hypothesis. Firstly, the augmentation of Temp-Lora
leads to a significant PPL reduction for all models,
where we observe an average decrease of 5.9% on
Llama2-7B-4K. An in-depth examination of Temp-
Lora’s impact across different text segments reveals
it is more notable on long text. For example, Temp-
Lora augmented Llama-7B-4K achieves a direct
PPL reduction of 13.2% in the 500K+ segment.
In contrast, its effect is relatively less pronounced
in the 0-100K segment, where it only reduces the
PPL by 3.6%. We may simply conclude the results
as: with greater text comes greater necessity for
Temp-Lora.
Additionally, adjusting the chunk size from 1024
to 2048 and 4096 resulted in a slight increase in
PPL. It is not surprising, as the Temp-Lora module
was trained on the data from the previous chunks.
An increase in chunk size means that the informa-
tion stored in the module is more distant from the
current token. Indeed, the choice of chunk size
is a critical trade-off between generation quality
and computational efficiency, which will receive
detailed analysis in Section 3.2.
Finally, we also discovered that the cache reuse
almost does not cause any loss in performance.
This is very encouraging news, as this technique
can significantly reduce the computational cost.
GuoFeng:
Table 2 presents the remarkable im-
pact of the Temp-Lora on the task of discourse-
level literary translation. Compared to the base
model, there are significant improvements across
all metrics: a decrease in PPL by -29.6%, an in-
crease in BLEU score by +53.2%, and a rise in
COMET score by +8.4%. This experiment illus-
trates that the Temp-Lora module is not only effec-
tive in downstream tasks and chat-style models, but
it is even more effective than in pre-training tasks.
One might wonder why the effects are more pro-
nounced in translation. This is attributed to the dif-
ferences between tasks: open-domain novel com-
pletion is a task without standard answers, mak-
ing it challenging for even an active human reader
of a specific novel to predict the content of the
next chapter. In such cases, even if Temp-Lora
∆
Metric
Base
24K
16K
8K
4K
3K
2K
1024
PPL
10.21
9.69(-5.0%)
9.70(-4.9%)
9.75(-4.5%)
9.82(-3.8%)
9.91(-2.9%)
10.03(-1.7%)
FLOPs(T)
324.7
380.1(+17.0%)
271.9(-16.2%)
163.6(-49.5%)
109.5(-66.2%)
96.0(-70.4%)
82.4(-74.5%)
Lat.(s)
52.9
63.1(+19.3%)
46.3(-12.4%)
30.1(-43.0%)
24.9(-53.0%)
25.2(-52.2%)
24.2(-54.1%)
2048
PPL
10.21
9.72(-4.8%)
9.73(-4.7%)
9.79(-4.1%)
9.88(-3.8%)
9.99(-2.1%)
-
FLOPs(T)
324.7
366.3(+12.8%)
258.0(-20.5%)
149.8(-53.8%)
95.6(-70.5%)
82.1(-74.7%)
-
Lat.(s)
52.9
63.2(+19.4%)
46.4(-12.2%)
30.1(-42.9%)
25.6(-51.5%)
25.1(-52.4%)
-
4096
PPL
10.21
9.77(-4.3%)
9.79(-4.1%)
9.86(-3.4%)
-
-
-
FLOPs(T)
324.7
359.3(+10.6%)
251.1(-22.6%)
142.8(-56.0%)
-
-
-
Lat.(s)
52.9
62.9(+18.8%)
46.2(-12.6%)
30.0(-43.0%)
-
-
-
Table 3: PPL, FLOPs (per 1K tokens), and latency (per 1K tokens) across various context window sizes W for
Llama2-7B-32K with Temp-Lora and cache reuse. The base model’s context window size is 24K. Tokens are
decoded with greedy search. Please note that W should be larger or equal to the sum of the chunk size and training
input length, ∆ + LT . If not, the context window will not be sufficient to include all tokens in Temp-Lora updating.
The percentages in () indicate the relative PPL change of the model compared to the base model’s best.
module stores substantial context information, the
model can only slightly reduce the PPL of ground-
truth text. In contrast, in discourse-level translation,
most word translations are unique for consistency.
By effectively storing these translation mappings in
the Temp-Lora module, substantial improvements
can be readily achieved.
3.2
Further Analysis
In Section 3.1, we demonstrated the Temp-Lora
framework’s ability to enhance long text gener-
ation. A key observation is that certain context
information is simultaneously stored within both
the model’s parameters (Temp-Lora) and its KV
cache, leading to an overlap. For the further un-
derstanding of the framework’s efficiency, we grad-
ually shorten the context window size W during
inference to eliminate this overlap. After shorten-
ing W, we can reduce the computational cost for
both models. However, this reduction results in
less context information being stored in the KV
cache, compelling the model to rely primarily on
the Temp-Lora module for accessing contextual
information. We measure Llama-7B-32K’s PPL,
Floating Point Operations (FLOPs), and latency
across these varied context window sizes W. This
experiment aims to explore the balance between
generation quality and computational efficiency in
different scenarios.
We observe that models augmented with Temp-
Lora consistently surpass the base model’s perfor-
mance, regardless of the context window size. No-
tably, even when we reduce the window size to
1/16 of its maximum (2048), a reduction in PPL
from 10.21 (base model) to 10.03 is also observed.
It is important to note that as the context window
diminishes, there is still a gradual increase in the
model’s PPL. As a whole, this trend highlights
that Temp-Lora and the KV cache are orthogonal,
jointly enhancing the overall performance of the
model when used together.
Concerns may arise regarding the additional
computational cost associated with updating Temp-
Lora. We were surprised to find that in the most
“economical” Temp-Lora configuration, ∆ = 2K
and W = 4K, the model not only reduces PPL by
3.8% but also saves 70.5% of FLOPs and 51.5%
of latency. Conversely, if we disregard computa-
tional costs entirely, then in the most “luxurious”
configuration, ∆ = 1K and W = 24K, we can
achieve a 5.0% reduction in PPL with an additional
17% of FLOPs and 19.6% of extra latency.
After summarizing the experimental results, we
got some suggestions for applying Temp-Lora in
real-world scenarios:
• For applications demanding the highest level
of long text generation, integrating Temp-Lora
into existing models — without altering any
parameters — can significantly enhance per-
formance at a relatively modest cost.
• For applications where minimal latency or
memory usage is paramount, computational
costs can be significantly lowered by reducing
input lengths and storing context information
within Temp-Lora. Under this setup, we can
process texts of almost infinite length (500K+
in our experiments) using a fixed short win-
dow size, such as 2K or 4K.
• Note that in scenarios without extensive text,
for example, less than the model’s window
size in pretraining, Temp-Lora is useless.
4
Conclusion
We proposed Temp-Lora, an alternative way for
efficient long text generation. The essence of the
Temp-Lora framework lies in training during the
inference process using the generated output. It
enables the storage of nearly infinite context in-
formation directly within the model’s parameters,
marking a distinct difference from existing atten-
tion weights-based techniques.
Our experimental results across various applica-
tions, including language modeling and discourse-
level literary translation, demonstrated the pro-
found impact of Temp-Lora. We showed that Temp-
Lora not only greatly enhances the quality of long
text generation but also significantly reduces com-
putational costs. In particular, Temp-Lora led to
remarkable improvements in perplexity (a reduc-
tion of 13.1% on a subset of PG19 and 29.6% on a
subset of GuoFeng), as well as increases in BLEU
(by 53.2%) and COMET scores (by 8.4%). The ef-
fectiveness of Temp-Lora becomes increasingly ap-
parent as text length grows. When considering the
implementation of Temp-Lora in your applications,
bear in mind this guiding principle: With Greater
Text Comes Greater Necessity for Temp-Lora
– a rule that becomes increasingly relevant in the
context of extensive texts.
5
Applications
For the application of Temp-Lora in different sce-
narios, we got some experiences to share:
1) We arrive at a conclusion similar to the one
from Ovadia et al. (2023): Temp-Lora cannot re-
place RAG (Li et al., 2022), as training for a lim-
ited number of steps is insufficient for the model to
clearly remember accurate factual information.
2) For role-play, especially when playing spe-
cific roles based on user-uploaded documents like
Harry Potter (Chen et al., 2023b), Temp-Lora
works wonders: we only need to train a Temp-Lora
module on the user-uploaded documents, then the
model can fully understand the character’s back-
ground and personality.
3) Personal assistants like ChatGPT (OpenAI,
2023): once a dialogue session exceeds the model’s
context window size, a Temp-Lora module can be
created to store the context knowledge. In most
cases, maintaining the context window size from
the pre-training stage is sufficient, eliminating the
need for additional context window extension.
4) Game NPCs & Generative Agents (Park et al.,
2023): One significant challenge in such applica-
tions is that the model’s context window is not suf-
ficient to record all past events in an NPC’s field of
view. With Temp-Lora, we can record such events
in the Temp-Lora module through training, instead
of a complex memory-summarization-reflection
pipeline.
5)
Human-Machine
Interactive
Transla-
tion (Huang et al., 2021): Translations for the
same company or project often require a high
degree of consistency in translation mappings
(e.g., the translations of named entities).
We
can continuously update the Temp-Lora module
with already translated results to help the model
remember those translation mappings.
References
Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong
Liang, and Lidong Bing. 2023a. Clex: Continuous
length extrapolation for large language models.
Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan
Li, Ziyang Chen, Longyue Wang, and Jia Li. 2023b.
Large language models meet harry potter: A dataset
for aligning dialogue agents with characters. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2023, pages 8506–8520, Singapore.
Association for Computational Linguistics.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023c. Extending context window
of large language models via positional interpolation.
Tri Dao. 2023. Flashattention-2: Faster attention with
better parallelism and work partitioning.
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,
and Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng
Ji, and Sinong Wang. 2023. Lm-infinite: Simple
on-the-fly length generalization for large language
models.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2021. Lora: Low-rank adaptation of
large language models.
Guoping Huang, Lemao Liu, Xing Wang, Longyue
Wang, Huayang Li, Zhaopeng Tu, Chengyan Huang,
and Shuming Shi. 2021. Transmart: A practical in-
teractive machine translation system.
Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and
Lemao Liu. 2022. A survey on retrieval-augmented
text generation.
OpenAI. 2023. Gpt-4 technical report.
Oded Ovadia, Menachem Brief, Moshik Mishaeli, and
Oren Elisha. 2023. Fine-tuning or retrieval? compar-
ing knowledge injection in llms.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai,
Meredith Ringel Morris, Percy Liang, and Michael S.
Bernstein. 2023. Generative agents: Interactive sim-
ulacra of human behavior.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-
rico Shippole. 2023. Yarn: Efficient context window
extension of large language models.
Ofir Press, Noah A. Smith, and Mike Lewis. 2021. Train
short, test long: Attention with linear biases enables
input length extrapolation. CoRR, abs/2108.12409.
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar,
Chloe Hillier, and Timothy P Lillicrap. 2019. Com-
pressive transformers for long-range sequence mod-
elling. arXiv preprint.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. COMET: A neural framework for MT
evaluation. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 2685–2702, Online. Association
for Computational Linguistics.
Shuming Shi, Enbo Zhao, Duyu Tang, Yan Wang, Piji
Li, Wei Bi, Haiyun Jiang, Guoping Huang, Leyang
Cui, Xinting Huang, Cong Zhou, Yong Dai, and
Dongyang Ma. 2022. Effidit: Your ai writing as-
sistant.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. 2023. Roformer: En-
hanced transformer with rotary position embedding.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models.
Longyue Wang, Zefeng Du, DongHuai Liu, Deng Cai,
Dian Yu, Haiyun Jiang, Yan Wang, Shuming Shi, and
Zhaopeng Tu. 2023a. Guofeng: A discourse-aware
evaluation benchmark for language understanding,
translation and generation.
Longyue Wang, Zhaopeng Tu, Yan Gu, Siyou Liu, Dian
Yu, Qingsong Ma, Chenyang Lyu, Liting Zhou, Chao-
Hong Liu, Yufeng Ma, Weiyu Chen, Yvette Graham,
Bonnie Webber, Philipp Koehn, Andy Way, Yulin
Yuan, and Shuming Shi. 2023b.
Findings of the
WMT 2023 shared task on discourse-level literary
translation: A fresh orb in the cosmos of LLMs. In
Proceedings of the Eighth Conference on Machine
Translation, pages 55–67, Singapore. Association for
Computational Linguistics.
","nanThe core idea of our approach is extremely simple: we store the context information in a temporary Lora module (Hu et al., 2021) that only exists during long text generation. We update this module in a streaming fashion during the generation process, using the generated content as training data, thereby achieving the goal of storing knowledge in the model parameters. Once inference is complete, this module is discarded to avoid permanently impacting the model’s parameters. This approach allows us to efficiently store nearly infinite context information without extending the context window."
"This study presents a novel approach to integrating Large Language Models (LLMs) with Arduino-controlled Electrohydrodynamic (EHD) pumps for precise color synthesis, highlighting the potential of LLMs in industrial automation systems.","The continuous advancement of automation and artificial intelligence has led to the exploration of integrating advanced Large Language Models (LLMs) with physical control systems, offering significant potential in various fields. LLMs have demonstrated robust text comprehension and generalization capabilities, showing promise beyond text-based tasks.
Electrohydrodynamic (EHD) pumps, utilizing the principles of electro-fluid dynamics, offer advantages of minimal wear and low maintenance. The integration of LLMs with EHD pumps is motivated by the growing demand for precision color mixing in industries such as printing, textiles, and digital art.","This study proposes a comprehensive methodology involving four key steps:
1. Language Model Fine-tuning: A pre-trained language model, such as GPT-4, is fine-tuned using a dataset specifically curated for this project. The dataset comprises natural language inputs related to color specifications and their corresponding Arduino code for EHD pump control.
2. Natural Language Processing Interface: A user interface is developed to receive inputs in natural language, allowing users to specify color requirements based on RGB values, color names, or descriptive phrases.
3. Translation into Arduino Code: Upon receiving a color request from the user, the fine-tuned language model generates the corresponding Arduino code. This code controls the operation of the EHD pumps, dictating the flow rate and duration for each pump to achieve the desired color mix.
4. EHD Pump Control: The Arduino system receives and executes the generated code to control the EHD pumps. Each pump is allocated a primary color (yellow, magenta, cyan), and their precise operation produces the required color output.","The paper presents conceptual experiment results, based on theoretical assumptions, which indicate a high potential for:
1. Accurate Color Synthesis: The system is expected to accurately produce colors that closely match the specifications provided in natural language inputs, with a projected color matching accuracy of at least 90%.
2. Efficient Language Model Interpretation: The fine-tuned language model is hypothesized to efficiently interpret natural language inputs and generate the corresponding Arduino code in a rapid time frame, likely within a few seconds.
3. Reliable EHD Pump Operation: When controlled by accurately generated Arduino code, the EHD pumps are expected to consistently dispense the correct color mixtures, resulting in a high reliability rate of above 95%.","This research demonstrates the potential of LLMs to go beyond text-based tasks and engage in the direct control of physical processes, paving the way for future advancements in AI-driven automation technologies. The findings underscore the significance of LLMs in industrial automation, highlighting their ability to simplify complex tasks and enhance user-friendliness.",Integration of Large Language Models in Control of EHD Pumps for Precise Color Synthesis,"Yanhong Peng, Ceng Zhang, Chenlong Hu, Zebing Mao","JOURNAL OF LATEX CLASS FILES
1
Integration of Large Language Models in Control of
EHD Pumps for Precise Color Synthesis
Yanhong Peng1,2,∗, Ceng Zhang3, Chenlong Hu4 and Zebing Mao5,∗
Abstract—This paper presents an innovative approach to inte-
grating Large Language Models (LLMs) with Arduino-controlled
Electrohydrodynamic (EHD) pumps for precise color synthesis in
automation systems. We propose a novel framework that employs
fine-tuned LLMs to interpret natural language commands and
convert them into specific operational instructions for EHD pump
control. This approach aims to enhance user interaction with
complex hardware systems, making it more intuitive and efficient.
The methodology involves four key steps: fine-tuning the language
model with a dataset of color specifications and corresponding
Arduino code, developing a natural language processing inter-
face, translating user inputs into executable Arduino code, and
controlling EHD pumps for accurate color mixing. Conceptual
experiment results, based on theoretical assumptions, indicate
a high potential for accurate color synthesis, efficient language
model interpretation, and reliable EHD pump operation. This
research extends the application of LLMs beyond text-based
tasks, demonstrating their potential in industrial automation
and control systems. While highlighting the limitations and the
need for real-world testing, this study opens new avenues for AI
applications in physical system control and sets a foundation for
future advancements in AI-driven automation technologies.
Index Terms—Electrohydrodynamic pumps, Large language
models,
Artificial
intelligence,
Human–machine
interaction,
Cyber-physical production systems, Industry 4.0.
I. INTRODUCTION
I
N the continuously evolving fields of automation and
artificial intelligence, the integration of complex Large
Language Models (LLMs) with physical control systems rep-
resents a frontier area with immense potential. Currently, LLM
are employed in various sectors including human-robot inter-
action [1], medicine [2], and education [3], benefiting from
their robust text comprehension, generalization capabilities,
and customizable and scalable nature. These models, trained
on extensive textual data, have acquired the ability to under-
stand and generate natural language. They are not only capable
of processing data types encountered during their training but
also generalize to new tasks and domains. Additionally, these
models can be fine-tuned according to specific application
requirements to better adapt to particular tasks or fields [4].
1Department of Information and Communication Engineering, Graduate
School of Engineering, Nagoya University, Nagoya 4648601, Japan.
2College of Mechanical Engineering, Chongqing University of Technology,
Chongqing 400054, China.
3Department of Mechanical Engineering, National University of Singapore,
Singapore 119077, Singapore.
4Department of Computer Science and Technology, Tsinghua University,
Beijing, 100084, China.
5Department of Mechanical Engineering, Tokyo Institute of Technology,
Tokyo 152-8550, Japan
∗Correspondence: Y. Peng (e-mail: yhpeng@nagoya-u.jp), Z. Mao (e-mail:
mao.z.aa@m.titech.ac.jp).
Furthermore, with advancements in computational power and
algorithmic optimization, the scale and capabilities of these
models are continually increasing [5].
Electrohydrodynamic (EHD) pumps utilize the principles
of electro-fluid dynamics to move and control fluids. These
pumps, devoid of traditional rotating impellers or pistons,
exhibit minimal wear and require low maintenance [6]. The
simple structure of EHD pumps makes them suitable for
miniaturization and portable devices [7]. The motivation for
this study stems from the growing demand for automation
in fields requiring precise color mixing, such as printing,
textile manufacturing, and digital art creation. Although tradi-
tional color mixing methods are effective, they often lack the
precision and adaptability needed for complex or large-scale
applications. The emergence of EHD pump technology has
opened new avenues for precise fluid manipulation. However,
the development efficiency of EHD control is currently low
due to the complexity of programming and control, and the
full potential of these pumps has not been fully realized.
This paper proposes a framework for controlling EHD
pumps using LLMs, employing fine-tuned models to interpret
natural language commands and convert them into precise
operational instructions for Arduino-controlled EHD pumps.
It explores the potential for innovative applications at the
intersection of computational linguistics, robotics, and material
science. The primary objective of this framework is to achieve
accurate and customizable color synthesis by manipulating the
flow of primary color liquids (yellow, magenta, and cyan)
through these pumps. Additionally, by fine-tuning LLMs,
the framework offers high scalability, allowing the free ad-
dition of various functional modules to achieve customized
tasks. Utilizing the capabilities of advanced language models,
particularly those developed by OpenAI (such as GPT-4),
this research aims to bridge the gap between user-friendly
interfaces and complex hardware control. The language model
acts as an intermediary, translating user-defined color specifi-
cations expressed in natural language into specific executable
Arduino code. This approach not only simplifies the interaction
between users and hardware but also enhances the precision
and flexibility of the color synthesis process.
The main contribution of this paper is to demonstrate the
framework of combining cutting-edge artificial intelligence
technology with physical control systems, paving the way for
more intuitive, efficient, and multifunctional automation solu-
tions in various industrial and creative fields. The originality
of this research lies in its pioneering approach to directly
integrating LLMs like GPT-4 with industrial control systems,
specifically in the context of operating EHD pumps for precise
arXiv:2401.11500v1  [cs.RO]  21 Jan 2024
JOURNAL OF LATEX CLASS FILES
2
color synthesis.
II. METHOD
A. Workflow
The methodology of this study is divided into two main
components: the integration of a Large Language Model
with an Arduino-based control system (Figure 1), and the
operational control of EHD pumps for precise color synthesis.
The workflow is as follows:
Step 1. Language Model Fine-Tuning: Utilizing a pre-
trained language model, such as OpenAI’s GPT-4, we fine-
tune it using a dataset specifically curated for this project.
This dataset comprises natural language inputs related to color
specifications and their corresponding Arduino code for EHD
pump control. The fine-tuning process enables the model to
accurately interpret color-related requests and convert them
into executable code.
Step 2. Natural Language Processing Interface: We
develop a user interface to receive inputs in natural language.
Users can specify their color requirements based on RGB
values, color names, or descriptive phrases. This interface
communicates with the fine-tuned language model to process
the inputs.
Step 3. Translation into Arduino Code: Upon receiving a
color request from the user, the language model generates the
corresponding Arduino code. This code is designed to control
the operation of the EHD pumps, dictating the flow rate and
duration for each pump to achieve the desired color mix.
Step 4. EHD Pump Control: The Arduino system receives
and executes the generated code to control the EHD pumps.
Each pump is allocated a primary color (yellow, magenta,
cyan), and their precise operation in terms of flow rate and
duration produces the required color output.
B. Algorithm
The core algorithm of this study involves two main stages:
natural language understanding and code generation.
In the natural language understanding stage, user requests in
natural language (e.g., ”I need a bright orange”) are inputted.
The fine-tuned language model parses and analyzes these
inputs to extract relevant color information (e.g., identifying
”bright orange” as the color requirement). The output of this
stage is structured data representing the color requirement
(e.g., the RGB values for bright orange).
In the code generation stage, the structured color data from
the previous stage is inputted. The language model, leveraging
its understanding of color requirements and Arduino pro-
gramming syntax, generates appropriate code. This involves
calculating the required ratios and durations for each EHD
pump to dispense the correct amount of each primary color.
The output is Arduino code that, when executed, controls the
EHD pumps to produce the specified color. For example, the
model might output a series of commands like:
Pump1 . w r i t e ( x1 ) ;
Pump2 . w r i t e ( x2 ) ;
Pump3 . w r i t e ( x3 ) ;
setVolume ( 5 ) ;
to achieve a specific shade of orange.
The implementation of this system encompasses several
critical components. Firstly, the Language Model Training
involves exposing the model to a diverse array of natural
language expressions related to color requirements, coupled
with their corresponding Arduino code, ensuring the model’s
adeptness in comprehending and translating varied color-
related requests. Secondly, Arduino Integration is achieved
through a bespoke software interface that not only interprets
the model-generated code but also transforms it into elec-
trical signals for pump control, incorporating error-checking
mechanisms for hardware safety and accuracy. Thirdly, EHD
Pump Calibration is conducted before deployment to guarantee
fluid dispensation precision, involving flow rate testing and
system adjustments to rectify any hardware discrepancies or
inefficiencies. Lastly, the User Interface Design is crafted
for user-friendliness, enabling flexible and intuitive input of
color requirements, and includes features for feedback and
adjustments, thus ensuring precise capture and interpretation
of user specifications.
Through this approach, the project aims to seamlessly inte-
grate advanced computational linguistics with precise physical
control, resulting in a highly accurate and user-friendly color
synthesis system.
III. RESULT AND DISCUSSION
A. Conceptual experiment results
Given the conceptual nature of this research, the experiment
results presented here are hypothetical and are intended to
illustrate the expected outcomes and potential challenges of
implementing the proposed system. These conceptual results
are based on the theoretical framework and assumptions un-
derlying the integration of a fine-tuned language model with
an Arduino-controlled EHD pump system for color synthesis.
In this conceptual research, the experiment results are
hypothetical, designed to demonstrate the expected outcomes
and potential challenges of the proposed integration of a fine-
tuned language model with an Arduino-controlled EHD pump
system for color synthesis. These results are derived from the
theoretical framework and assumptions of the study.
For the expected accuracy of color synthesis, the hypothesis
posits that the system should accurately produce colors that
closely match the specifications provided in natural language
inputs. The projected outcome is a high degree of color
matching accuracy, anticipated to be at least a 90% match
rate against standard color charts, contingent upon optimal
calibration of the EHD pumps and effective translation of color
specifications into executable code.
Regarding the anticipated efficiency of the language model
interpretation, the hypothesis suggests that once fine-tuned,
the model should efficiently interpret natural language inputs
and generate the corresponding Arduino code. The expected
outcome is a rapid processing time, likely within a few
seconds, underscoring the model’s efficiency.
Finally, for the reliability of EHD pump operation, the
hypothesis is that the EHD pumps, when controlled by accu-
rately generated Arduino code, should consistently dispense
JOURNAL OF LATEX CLASS FILES
3
…
pump1.write(x1); 
pump2.write(x2); 
pump3.write(x3);
setVolume(5);
….
To achieve pure red, set pump 1 and 2 to 50% flow rate, and turn 
off pumps 3. The total volume should be set to 5 milliliters. 
I want a color with RGB(255, 0, 0), and I need 5 milliliters.
Technician
LLM after fine tuning
Arduino
Code
Activation signal
EHD Pump
Natural
language
Extendable 
modular
Color fluid
Fig. 1. The proposed system.
the correct color mixtures. The projected outcome, assuming
precise calibration and maintenance of the EHD pumps, is a
high reliability rate (above 95%) in consistent color output
across multiple trials. This conceptual exploration aims to lay
the groundwork for future practical implementations and real-
world testing.
B. Discussion
This study’s conceptual nature leads to a speculative yet
theoretically grounded discussion on the integration of a fine-
tuned language model with a physical control system, such
as Arduino-controlled EHD pumps. While LLMs have been
extensively used in areas such as text generation, translation,
and conversation, their application in directly controlling phys-
ical hardware in an industrial context is largely unexplored.
This research extends the utility of LLMs to a new domain,
demonstrating their potential in not just understanding and
generating human-like text but also in executing complex,
real-world tasks in industrial environments. The principles and
methodologies developed in this research have the potential
for scalability and adaptation to various other industrial ap-
plications. The concept of using LLMs to interpret natural
language and convert it into operational code can be extended
to other areas of automation of robotics, and even Internet
of Things systems. LLMs can be directly utilized through
natural language commands to enable robots to accomplish
specific tasks [9], [8] or to replicate the movements of animals
[10] in code form. By allowing users to communicate their
requirements in natural language, this system significantly
lowers the technical barrier to operating complex industrial
machinery.
There are some limitations of this research. The theoreti-
cal accuracy of color synthesis hinges on the precision and
calibration of the EHD pumps. Mechanical limitations or
inconsistencies could affect color output fidelity, underscoring
the importance of robust hardware design and regular calibra-
tion. Moreover, although the conceptual model is promising,
its scalability and real-world applicability require validation
through actual experimentation and deployment, including
performance assessment under various operational conditions
and with different color requests. In addition, this study paves
the way for future research in AI’s application in control-
ling physical systems beyond color synthesis, suggesting new
avenues in domains where natural language can simplify
complex tasks, enhancing technology’s accessibility and user-
friendliness.
IV. CONCLUSION
This paper has presented a novel framework that integrates
LLMs with physical control systems, specifically Arduino-
JOURNAL OF LATEX CLASS FILES
4
controlled EHD pumps, for precise color synthesis. The pro-
posed system represents a significant leap in the application
of advanced AI in industrial automation, demonstrating the
potential of LLMs to go beyond text-based tasks and engage
in the direct control of physical processes.
REFERENCES
[1] Zhang, C., Chen, J., Li, J., Peng, Y. & Mao, Z. Large language models
for human-robot interaction: A review. Biomimetic Intelligence and
Robotics. pp. 100131 (2023)
[2] Thirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierrez, L.,
Tan, T. F. & Ting, D. S. W. Large language models in medicine. Nature
Medicine. 29(8), 1930–1940 (2023)
[3] Chang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K., Chen, H., Yang, L.,
Yi, X., Wang, C., Wang, Y. & others. A survey on evaluation of large
language models. arXiv preprint arXiv:2307.03109 (2023)
[4] Ding, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y., Hu, S., Chen,
Y., Chan, C.-M., Chen, W. & others. Parameter-efficient fine-tuning of
large-scale pre-trained language models. Nature Machine Intelligence.
5(3), 220–235 (2023)
[5] Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan,
D., Jiang, E., Cai, C., Terry, M., Le, Q. & others. Program synthesis
with large language models. arXiv preprint arXiv:2108.07732 (2021)
[6] Peng, Y., Li, D., Yang, X., Ma, Z. & Mao, Z. A review on electrohy-
drodynamic (EHD) pump. Micromachines. 14(2), 321 (2023)
[7] Mao, Z., Peng, Y., Hu, C., Ding, R., Yamada, Y. & Maeda, S. Soft
computing-based predictive modeling of flexible electrohydrodynamic
pumps. Biomimetic Intelligence and Robotics. 3(3), 100114 (2023)
[8] Peng, Y., Sakai, Y., Nakagawa, K., Funabora, Y., Aoyama, T., Yokoe, K.,
Doki, S. Funabot-Suit: A bio-inspired and McKibben muscle-actuated
suit for natural kinesthetic perception. Biomimetic Intelligence and
Robotics. 3(4), 100127 (2023)
[9] Mao, Z., Asai, Y., Yamanoi, A., Seki, Y., Wiranata, A. & Minaminosono,
A. Fluidic rolling robot using voltage-driven oscillating liquid. Smart
Materials and Structures. 31(10), 105006 (2022)
[10] Peng, Y., Nabae, H., Funabora, Y. & Suzumori, K. Peristaltic transport-
ing device inspired by large intestine structure. Sensors and Actuators
A: Physical. 365, 114840 (2024)
","Large Language Models (LLMs) have gained significant attention in recent years due to their remarkable abilities in text comprehension, generation, and generalization. The introduction of advanced language models like GPT-4 has further expanded their capabilities, enabling them to understand and generate natural language with impressive accuracy.
Electrohydrodynamic (EHD) pumps, employing the principles of electro-fluid dynamics, have emerged as promising tools for precise fluid manipulation. Their unique properties, including minimal wear and low maintenance, have made them suitable for miniaturization and portable applications. The integration of LLMs with EHD pumps presents an exciting opportunity to combine the strengths of both technologies for precise color synthesis.nan"
"Predicting the dense bird’s eye view (BEV) motion flow in a self-supervised manner is an emerging research for robotics and autonomous driving. Current self-supervised methods mainly rely on point correspondences between point clouds, which may introduce the problems of fake flow and inconsistency, hindering the model’s ability to learn accurate and realistic motion. In this paper, we introduce a novel cross-modality self-supervised training framework that effectively addresses these issues by leveraging multi-modality data to obtain supervision signals. We design three innovative supervision signals to preserve the inherent properties of scene motion, including the masked Chamfer distance loss, the piecewise rigidity loss, and the temporal consistency loss. Through extensive experiments, we demonstrate that our proposed self-supervised framework outperforms all previous self-supervision methods for the motion prediction task. Code is available at https://github.com/bshfang/self-supervised-motion.","Accurate prediction of dynamic motion within a scene is fundamental for the safe and robust planning of autonomous vehicles. Instead of predicting instance-level trajectories (Chen et al. 2020). An emerging trend is to predict the dense motion flow in the BEV (Bird’s Eye View) map directly from raw sequential sensor input in an end-to-end manner (Wu, Chen, and Metaxas 2020; Wang et al. 2022). This approach is less susceptible to perception errors and possesses the capability to discern class-agnostic motion (Wu, Chen, and Metaxas 2020; Wong et al. 2020). Nevertheless, training flow prediction models with supervision necessitates a substantial volume of annotations for sensor data and annotating motion labels for sensor data proves to be intricate and costly. Hence, the effective utilization of vast amounts of unlabeled raw data for motion prediction training has emerged as a notable and encouraging challenge. Recently, many works have proposed various self-supervised frameworks to learn the BEV motion without relying on ground truth labels (Luo, Yang, and Yuille 2021; Li et al. 2023; Jia et al. 2023).","To address the challenges of fake flow and inconsistent flow, rather than developing another network model, our focus is to design dedicated supervision signals to preserve a series of inherent properties of scene motion. For the fake flow issue, one common, yet fundamental property is that motion is restricted to moving objects. Stationary components should exhibit no motion, allowing us to filter out background noise and obtain a more precise motion flow. To tackle the inconsistent flow challenge, we focus on two primary properties: object and temporal consistency. In essence, points within rigid objects should move uniformly. Furthermore, object motions should remain relatively stable over short periods, ensuring no abrupt changes. By emphasizing these key properties, we enhance the uniformity and reliability of the motion flow.
However, due to the notorious noise and sparsity issues of point cloud data, relying solely on point cloud sequences might compromise the accurate representation of scene motion properties. To compensate, we leverage the multi-modality information. We specifically incorporate sequential camera images—readily accessible since most robots are equipped with cameras, thereby incurring no extra annotation costs. These sequential images enable the extraction of optical flow, providing a rich layer of motion insights. This stands in stark contrast to the sparse, irregular, and fragmented data in point cloud sequences. Optical flow images distinctly highlight the coherent and consistent motion of objects, sharply separating them from the background.","Experimental evaluations conducted on the nuScenes (Caesar et al. 2020) dataset demonstrate that our proposed methodology improves upon previous self-supervised approaches by up to 40%. Notably, our method achieves performance comparable to weakly-supervised and fully-supervised methods.","In this paper, we present a novel cross-modality self-supervised method for BEV motion prediction. Concretely, we exploit static/dynamic classification and rigid pieces on point clouds from sequential multi-view images to facilitate motion learning without any manual annotations. Moreover, we enforce temporal consistency across multiple frames, ensuring temporal smoothness of predicted motion. Comprehensive experiments conducted on the nuScenes dataset demonstrate that our proposed method achieves state-of-the-art performance and all designed modules are effective.",Self-Supervised Bird's Eye View Motion Prediction with Cross-Modality Signals,"Shaoheng Fang, Zuhong Liu, Mingyu Wang, Chenxin Xu, Yiqi Zhong, Siheng Chen","Self-Supervised Bird’s Eye View Motion Prediction with Cross-Modality Signals
Shaoheng Fang1, Zuhong Liu1, Mingyu Wang2, Chenxin Xu1, Yiqi Zhong3, Siheng Chen1,4
1 Shanghai Jiao Tong University
2 University of Chinese Academy of Sciences
3 University of Southern California
4 Shanghai AI Laboratory
{shfang, xcxwakaka, sihengc}@sjtu.edu.cn, zuhong.liu@polytechnique.edu,
wangmingyu21a@mails.ucas.ac.cn, yiqizhon@usc.edu
Abstract
Learning the dense bird’s eye view (BEV) motion flow in a
self-supervised manner is an emerging research for robotics
and autonomous driving. Current self-supervised methods
mainly rely on point correspondences between point clouds,
which may introduce the problems of fake flow and in-
consistency, hindering the model’s ability to learn accurate
and realistic motion. In this paper, we introduce a novel
cross-modality self-supervised training framework that effec-
tively addresses these issues by leveraging multi-modality
data to obtain supervision signals. We design three inno-
vative supervision signals to preserve the inherent proper-
ties of scene motion, including the masked Chamfer distance
loss, the piecewise rigidity loss, and the temporal consis-
tency loss. Through extensive experiments, we demonstrate
that our proposed self-supervised framework outperforms all
previous self-supervision methods for the motion prediction
task. Code is available at https://github.com/bshfang/self-
supervised-motion.
Introduction
Accurate prediction of dynamic motion within a scene
is fundamental for the safe and robust planning of au-
tonomous vehicles. Instead of predicting instance-level tra-
jectories (Chen et al. 2020). An emerging trend is to pre-
dict the dense motion flow in the BEV (Bird’s Eye View)
map directly from raw sequential sensor input in an end-
to-end manner (Wu, Chen, and Metaxas 2020; Wang et al.
2022). This approach is less susceptible to perception errors
and possesses the capability to discern class-agnostic mo-
tion (Wu, Chen, and Metaxas 2020; Wong et al. 2020). Nev-
ertheless, training flow prediction models with supervision
necessitates a substantial volume of annotations for sensor
data and annotating motion labels for sensor data proves to
be intricate and costly. Hence, the effective utilization of vast
amounts of unlabeled raw data for motion prediction train-
ing has emerged as a notable and encouraging challenge. Re-
cently, many works have proposed various self-supervised
frameworks to learn the BEV motion without relying on
ground truth labels (Luo, Yang, and Yuille 2021; Li et al.
2023; Jia et al. 2023).
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Figure 1: Problems in current self-supervised motion learn-
ing methods that rely on point correspondence. (a) For static
objects (background building), points with correspondences
in the point cloud sequence may have completely different
locations, misleading the model to learn the fake flow. (b)
Due to the sparse nature of the point cloud, points within an
instance may learn highly varying flow.
Inspired by self-supervised scene flow estimation, cur-
rent self-supervised BEV motion prediction methods (Luo,
Yang, and Yuille 2021; Li et al. 2023) primarily rely on
chamfer distance loss to establish the point-level correspon-
dences between point clouds. However, this heavy depen-
dence on point-level correspondences leads to two major
problems when learning motion patterns from real-world Li-
DAR point cloud data.
The first problem is fake flows. Due to the alterations in
the viewpoint of the LiDAR sensor, points associated with
the background or static objects often exhibit flow that does
not exist, as shown in Figure 1(a). This fake flow will mis-
lead the model to learn incorrect motion patterns, thereby
adversely affecting the accuracy of predictions. One previ-
ous work (Li et al. 2023) introduces a weakly supervised set-
ting where foreground/background ground truth is available
to mitigate the impact of noise originating from background
points to alleviate the problem. However, the method still
remains limited as extra human annotation is indispensable.
The second problem is the inconsistent flows within one
single object; see an illustration in Figure 1(b). Owing to the
inherent sparsity in point cloud data, the point-level flows as-
arXiv:2401.11499v1  [cs.CV]  21 Jan 2024
sociated with the same objects may exhibit inconsistent mo-
tions when solely relying on the point correspondences. This
problem of inconsistency violates the object-level rigid con-
straints and causes confusion for model learning procedures.
(Luo, Yang, and Yuille 2021) aims to preserve the local uni-
formity of motion flow by employing a smoothness loss,
which encourages minimal changes among neighbor flow
values. Unfortunately, this assumption fails in the boundary
region between moving objects and the background and is
unable to ensure instance-level motion consistency.
To address the challenges of fake flow and inconsistent
flow, rather than developing another network model, our fo-
cus is to design dedicated supervision signals to preserve
a series of inherent properties of scene motion. For the
fake flow issue, one common, yet fundamental property
is that motion is restricted to moving objects. Stationary
components should exhibit no motion, allowing us to fil-
ter out background noise and obtain a more precise motion
flow. To tackle the inconsistent flow challenge, we focus on
two primary properties: object and temporal consistency. In
essence, points within rigid objects should move uniformly.
Furthermore, object motions should remain relatively stable
over short periods, ensuring no abrupt changes. By empha-
sizing these key properties, we enhance the uniformity and
reliability of the motion flow.
However, due to the notorious noise and sparsity issues of
point cloud data, relying solely on point cloud sequences
might compromise the accurate representation of scene
motion properties. To compensate, we leverage the multi-
modality information. We specifically incorporate sequen-
tial camera images—readily accessible since most robots are
equipped with cameras, thereby incurring no extra annota-
tion costs. These sequential images enable the extraction of
optical flow, providing a rich layer of motion insights. This
stands in stark contrast to the sparse, irregular, and frag-
mented data in point cloud sequences. Optical flow images
distinctly highlight the coherent and consistent motion of
objects, sharply separating them from the background.
Leveraging the advantages of multi-modality data, we in-
corporate the spirit of preserving scene motion’s inherent
properties into a novel self-supervised training framework
for BEV motion prediction. Specifically, i) to ensure that
motion is exclusive to moving objects, the framework gener-
ates a pseudo static/dynamic mask for each point cloud ac-
cording to the optical flow data. Then this mask will be used
to ensure structural consistency exclusively for the dynamic
portion through a novel masked Chamfer distance loss; ii) to
promote motion consistency in individual objects, we em-
ploy a simple clustering technique to the optical flow image,
discerning instance boundaries and creating pixel clusters in
the image space. Then the cluster information for each pixel
will be projected to the point cloud space, creating rigid
point cloud clusters that should share the same motion flow
to ensure the instance-level rigidity constraints; and iii) for
temporal motion consistency, we introduce a novel tempo-
ral consistency loss, which enforces the smoothness of pre-
dictions across long point cloud sequences. Note that image
data are only used for providing supervision signals in the
training phase; during inference, the proposed BEV motion
prediction network only needs point cloud sequences.
Experimental
evaluations
conducted
on
the
nuScenes (Caesar et al. 2020) dataset demonstrate that
our proposed methodology improves upon previous self-
supervised approaches by up to 40%. Notably, our method
achieves performance comparable to weakly-supervised
and fully-supervised methods.
To summarize, the main contributions of our work are:
• We propose a novel cross-modality self-supervised train-
ing framework for BEV motion prediction, which lever-
ages multi-modality data to obtain supervision signals.
• We propose three novel supervision signals to preserve
the inherent properties of scene motion, including the
masked Chamfer distance loss, the piecewise rigidity loss
and the temporal consistency loss.
• Our method achieves state-of-the-art performance. Com-
prehensive experiments demonstrate the effectiveness of
our designed framework.
Related Work
Motion Prediction
The goal of motion prediction is to estimate the future
movements of mobile objects in a scene based on past ob-
servations. Traditional approaches tackle this issue via a
two-stage framework, relying on the results of 3D object
detection and tracking to predict the instance-level trajec-
tories (Casas et al. 2020; Luo, Yang, and Urtasun 2018;
Phillips et al. 2021). However, the dependence on inter-
mediate results may lead to error accumulation and a lim-
ited ability to perceive unknown classes (Wu, Chen, and
Metaxas 2020; Wong et al. 2020). An emerging trend is to
predict dense future motion in an end-to-end framework di-
rectly from sequential sensor input, including multi-frame
point clouds (Wu, Chen, and Metaxas 2020; Lee et al. 2020;
Luo, Yang, and Yuille 2021; Filatov, Rykov, and Murashkin
2020; Wang et al. 2022; Wei et al. 2023) and multi-view im-
ages (Hu et al. 2021; Zhang et al. 2022; Fang et al. 2023).
Training a motion prediction model requires high-quality
manual labels, but obtaining such labels is both expensive
and laborious. Accordingly, some methods aim to mitigate
this issue from various perspectives. (Luo, Yang, and Yuille
2021) proposes a self-supervision method that utilizes point
cloud structure consistency and cross-modality regulariza-
tion; (Li et al. 2023) proposes the use of a weakly super-
vised setting that only utilizes foreground/background infor-
mation, effectively improving the accuracy. (Jia et al. 2023)
employs contrastive learning to learn BEV pillar features
and uses pillar association to predict motion. In this paper,
we propose a novel self-supervised framework and achieve
remarkable performance that is comparable to other weakly
supervised and even fully supervised approaches.
Self-Supervised Scene Flow Estimation
Scene flow estimation aims to determine the 3D motion
displacement at the point level between a pair of point
clouds (Liu, Qi, and Guibas 2019; Puy, Boulch, and Marlet
2020; Jund et al. 2021; Cheng and Ko 2022; Li et al. 2021;
Gu et al. 2019). Learning scene flow in a self-supervised
Figure 2: An overview of our cross-modality self-supervision learning framework. An overview of our cross-modality self-
supervision learning framework. For self-supervised training, we introduce three innovative self-supervised losses that align
with real-world motion patterns. The inference process only takes the point cloud sequence as input and predicts the motion
flow of each BEV cell (grey area).
manner is a popular field of research (Mittal, Okorn, and
Held 2020; Wu et al. 2019; Baur et al. 2021; Kittenplon,
Eldar, and Raviv 2021; Tishchenko et al. 2020). (Mittal,
Okorn, and Held 2020) was the first to establish a self-
supervised learning framework that utilizes a combination
of nearest neighbor and cycle consistency loss. Following
(Wu et al. 2019), (Kittenplon, Eldar, and Raviv 2021; Pontes,
Hays, and Lucey 2020) use the chamfer distance loss to learn
the point correspondences between two point clouds. (Li
et al. 2022b; Gojcic et al. 2021) employ ego-motion estima-
tion and exploit the piecewise rigid nature of point clouds.
We follow the philosophy of (Wu et al. 2019; Li et al.
2022b) by designing self-supervised loss to maintain struc-
tural consistency between point clouds and exploiting the
piecewise rigidity for regularization. Nevertheless, most of
these methods (Li et al. 2022b; Wu et al. 2019; Mittal,
Okorn, and Held 2020) usually assume strong one-to-one
correspondences between point clouds and incur heavy com-
putational costs, making them unsuitable for real-time per-
ception in autonomous driving. We propose a masked cham-
fer loss to mitigate these issues. Moreover, unlike scene flow
estimation, which identifies motion between a pair of point
clouds, we concentrate on predicting the future of the scene
based on point cloud sequences.
LiDAR-Camera Fusion
LiDAR-camera fusion has been extensively investigated to
enhance scene perception, including various tasks such as
3D object detection (Vora et al. 2020; Li et al. 2022c; Liang
et al. 2022) and scene flow estimation (Rishav et al. 2020;
Liu et al. 2022). A novel line of research is to leverage cross-
modality information as supervised signals to support model
training. (Ding et al. 2023) combines detection and tracking
results from LiDAR point clouds with odometry data and
optical flow to jointly improve radar scene flow learning.
(Li et al. 2022a) generates noisy pseudo-labels from optical
flow to supervise scene flow learning. Additionally, (Luo,
Yang, and Yuille 2021) facilitates motion prediction learning
through LiDAR-camera cross-modality regularization. Op-
tical flow data, which can be easily obtained from camera
video without human labeling, has shown the potential to
aid motion learning on point clouds. However, these meth-
ods solely employ the numerical values of optical flow as
the guidance for point cloud motion and ignore the inherent
advantages of optical flow data over point clouds.
Method
This section introduces a self-supervised training framework
for BEV motion prediction, where three novel supervision
signals are generated from multi-modality inputs, including
point cloud sequences and camera videos.
Problem Formulation
The objective of the motion prediction task is to directly
forecast the motion of mobile grids in the 3D BEV map from
historical point cloud sequences (Li et al. 2023; Wang et al.
2022). The prediction model takes the current frame 0 along
with T past frames of point clouds that synchronized to the
current frame as input. The point cloud sequence is denoted
as Pt = {pt
i ∈ R3}Nt
i=1, t = 0, −1, · · · , −T, where Nt rep-
resents the number of points in Pt. The multi-view camera
video is utilized in the training process. The corresponding
(a)
(b)
(c)
(d)
Figure 3: Rigid piece generation. (a) A frame of sequential images; (b) Over-segmentation on the optical flow image; (c) Over-
segmentation projected to the associated point cloud; (d) Rigid pieces after fusion. In (c) and (d), each color refers to a piece.
multi-view images of Pt are {Ik
t ∈ RH×W ×3}Nc
k=1, where
Nc is the number of cameras.
The future motion is represented in the form of a BEV
(bird’s eye view) map. Assuming the model predicts T ′
frames of future motion field, Mt
∈
RX×Y ×2, t
=
1, · · · , T ′ represents the motion field of the t frame, where
X×Y is the shape of the BEV map according to the vehicle-
ego coordinates at the current timestamp. Considering that
each grid in the BEV map represents a rather small area in
the real-world scene, points within the same pixel grid have
identical motion flow. To generate a point-level 3D motion
flow, each point can be assigned the motion of its corre-
sponding position in the BEV map based on its 3D coor-
dinates, and the vertical motion is set as zero. The motion of
points is denoted as Ft =

f t
i ∈ R3	N0
i=1 , t = 1, · · · , T ′.
Apart from predicting the future motion from time 1 to
T ′, the model can also infer the motion situation from the
current frame to the past frames. Therefore, in the following
section, T = {t1, · · · , tn} is used to represent the entire set
of time frames that the model predicts, including 1 to T ′ and
possible backward predictions (for example, −1).
Overview
Figure 2 overviews our training framework. Since the pre-
diction model is not our focus, we directly adopt Motion-
Net (Wu, Chen, and Metaxas 2020). The key of this work
is to leverage multi-modality inputs to provide three super-
vision signals that can preserve inherent properties of scene
motion. They include: 1) pseudo static/dynamic mask loss
generated from sequential video, 2) the piece-wise rigidity
loss, and 3) temporal motion consistency loss. Here are the
detailed descriptions.
Pseudo Static/Dynamic Mask Loss
We employ Chamfer Distance as the foundation of learn-
ing structural consistency. The Chamfer Distance serves as
a measure of similarity between two sets of points. In self-
supervised training (Wu et al. 2019; Kittenplon, Eldar, and
Raviv 2021) or optimization (Li, Kaesemodel Pontes, and
Lucey 2021; Pontes, Hays, and Lucey 2020) methods re-
lated to point clouds, the chamfer distance loss is a widely
used technique that helps maintain the structural consistency
of two point clouds.
For any frame t, with the predicted point-level flow Ft
from frame 0 to frame t, the predicted point cloud can be
calculated as P′
t = {p′
i ∈ R3 | p′
i = p0
i + f t
i }N0
i=1. Given
the point cloud Pt at frame t, the self-supervised chamfer
distance loss can be defined as
Lcd(Pt, P′
t) =
X
pj∈Pt
min
p′
i∈P′
t
pj − p′
i
2
2+
X
p′
i∈P′
t
min
pj∈Pt
p′
i − pj
2
2 .
(1)
However, the point cloud data is often sparse and full of
noise points. Even for stationary objects, the point cloud rep-
resentation can vary significantly with the sensor’s move-
ment (Khurana et al. 2023). This poses great challenges and
introduces noise when relying on Chamfer distance loss for
learning. To better understand the motion of a dynamic 3D
scene, it is crucial to focus on moving targets while dis-
regarding the background and stationary objects. However,
due to the sparse and noisy nature of the point cloud, it is dif-
ficult to distinguish between the static and dynamic parts of
a point cloud in open scenes. In contrast, optical flow in the
image space is much more accessible and easier to obtain.
Video data is abundant with superior temporal and texture
information, and the relevant techniques are already well-
established (Sun et al. 2018; Teed and Deng 2020). Previous
works (Luo, Yang, and Yuille 2021; Ding et al. 2023) have
utilized the value of image optical flow to assist in learning
point cloud scene flow. In our method, we propose to extract
a pseudo static/dynamic mask from the optical flow results
of the image data to aid in structure consistency learning.
Given the point cloud time frame t ∈ T, we can get the
adjacent image pairs (Ik
t , Ik
t+δt), k = 1, · · · , Nc from the
camera video. For brevity, we omit the superscript k for
camera index and the subscript t for frame index in sub-
sequent contents, and use I and I′ to denote Ik
t and Ik
t+δt.
The optical flow generated from I and I′ is denoted as
F2D ∈ RH×W ×2.
The optical flow F2D cannot yet be directly used to deter-
mine the motion status of each pixel. Apart from the optical
flow generated by dynamic targets in the scene, the move-
ment of the ego vehicle also produces flow in the camera
view. Following (Luo, Yang, and Yuille 2021), we divide
the optical flow into two parts, F2D = F2D
ego + F2D
mot, where
F2D
ego corresponds to the optical flow caused by vehicle mo-
tion and F2D
mot corresponds to the optical flow caused by dy-
namic objects.
The numerical value of F2D
ego can be calculated through
the sensors’ poses. Let pi ∈ P represent a point within the
image I and TP→I represent the transformation matrix from
the lidar point cloud P to the image I
(ui, vi) = TP→I(pi).
(2)
(a)
(b)
Figure 4: An example of the generated static/dynamic mask
and the rigid piece labels. Left: green represents dynamic
points while black represents static points; Right: each color
except black refers to a rigid piece label.
The value of F2D
ego corresponding to pi is then
F2D
ego(ui, vi) = TP→I′(pi) − TP→I(pi).
(3)
Ideally, the calculated F2D
mot corresponding to a stationary
target or background would be close to 0. Thus, static points
can be distinguished by setting a small threshold for F2D
mot.
Nevertheless, when dealing with distant moving objects
that are far from the camera, their corresponding optical flow
values may be small and incorrectly classified as static. To
mitigate such effect, we employ the projected 3D scene flow
to supplement the static assessment. Denote F2D
mot(ui, vi)
as f 2D
i . With the constraint of zero vertical motion, we can
project the 2D optical flow f 2D
i
to a 3D scene flow origi-
nating from pi. The operation is represented by a projection
Toptf→sf (see more info in supp.).
f 3D
i
= Toptf→sf(f 2D
i ).
(4)
The pseudo static/dynamic status si of pi is estimated as
si =
0,
f 2D
i
< τ 2D and f 3D
i
< τ 3D,
1,
otherwise.
(5)
A pseudo static/dynamic mask St ∈ RNt is produced for
the point cloud Pt at each time frame. Utilizing St, Pt can
be separated into two parts: a pseudo dynamic point cloud
˜Pt and a pseudo static point cloud ¯Pt. The Chamfer distance
loss calculation is then performed on the pseudo dynamic
point cloud instead of the entire point cloud. The masked
Chamfer loss can be defined as
Lmc =
1
|T|
X
t∈T

Lcd(˜Pt, ˜P′
t) + Lstatic(¯P0, t)

,
(6)
where Lcd(·) is the Chamfer loss and
Lstatic(¯P0, t) =
1
|¯P0|
X
pi∈ ¯P0
||f t
i ||1,
(7)
which pushes the motions of static points to be zero.
Piecewise Rigidity Loss
When considering flow estimation on point clouds, local
rigidity is an important physical prior that is frequently uti-
lized (Dong et al. 2022; Gojcic et al. 2021; Li et al. 2022b;
Shen et al. 2023). Unlike previous self-supervised methods
for point clouds that maintain rigidity by clustering on a
single-frame point cloud (Li et al. 2022b; Shen et al. 2023),
we introduce a simple yet effective method for point cloud
piecewise rigidity based on optical flow image clustering.
Compared to a single-frame point cloud, optical flow images
exhibit several desirable properties. The motion consistency
of dynamic objects is more pronounced in optical flow im-
ages and is easier to recognize and extract. In contrast to
the sparsity and noise of point clouds, the pixels of moving
objects in optical flow images are adjacent, with high unifor-
mity and smoothness. Additionally, the boundaries between
objects and the background are more salient.
For the optical flow image F2D, we apply a simple im-
age clustering method (Achanta et al. 2012) to obtain over-
segmentation labels (Figure 3(b)). The over-segmentation
divides the entire image into numerous small pieces. Due
to the optical flow consistency of moving objects, it is easy
to ensure that almost all pixels in a single small piece belong
to the same object.
Through the point cloud to image projection function
TP→I, we can retrieve the corresponding label in the im-
age over-segmentation for each point pi. By projecting the
over-segment result labels from Nc camera views, we obtain
an over-segmentation result on the whole point cloud.
Some points in the point cloud may be occluded from the
camera view due to the slight difference between the Li-
DAR sensor position and the camera sensor position. For
each segment piece, we consider the distance between all
points in the piece and the camera and find the smallest dis-
tance dmin. We set a threshold ∆d such that if a point in the
piece is more than dmin + ∆d away from the camera, it will
be excluded from the piece.
As a result, we obtain hundreds of rigid pieces from the
point cloud (Figure 3(c)). In order to achieve more accurate
and unified segmentation, we employ a simple height-based
rigid piece fusion method. Since the visual perspective of
the multi-view cameras is typically parallel to the ground,
rigid pieces derived from images can easily appear at differ-
ent heights in the same location. Given the assumption that
points within the same grid of the BEV map have the same
motion, we consolidate multiple rigid piece labels into a sin-
gular label if a grid contains points with distinct labels.
Finally, we generate Nr rigid pieces for the point cloud
P0 (Figure 3(d)). R1, · · · , RNr are the Nr rigid pieces,
where R represents a set of points that have the same rigid
piece label. For any frame t ∈ T, the piecewise rigidity loss
function is defined as
Lt
pr = 1
Nr
Nr
X
j=1
1
|Rj|
X
pi∈Rj
|f mean
Rj
− f t
i |,
(8)
where f mean
Rj
= P
pi∈Rj f t
i /|Rj|,
j = 1, · · · , Nr. The
final piecewise rigidity loss Lpr is the mean of Lt
pr over all
time frames t.
Temporal Motion Consistency Loss
For moving objects in traffic scenes, such as cars, pedestri-
ans, and bicycles, their motion patterns do not undergo sig-
nificant changes over short periods of time. In a point cloud
sequence, the displacement of points belonging to a mov-
ing object should remain consistent over equal time inter-
vals. Therefore, for self-supervised learning of point cloud
sequences, we can apply point-level temporal consistency
constraints to the predicted motion. The temporal consis-
tency loss is defined as
Ltc = 1
N0
N0
X
i=1
1
|T|
X
t∈T
|f mean
i
− f t
i |,
(9)
where f mean
i
= P
t∈T f t
i /(t|T|), i = 1, · · · , N0.
Overall Loss
In summary, the total loss for the model training is a
weighted sum of the proposed masked Chamfer distance
loss, piecewise rigidity loss and temporal consistency reg-
ularization.
L = λmc · Lmc + λpr · Lpr + λtc · Ltc,
(10)
where λmc, λpr, and λtc are the balancing parameters.
Experiments
Experimental Setup
Dataset. We evaluate our approach on the NuScenes (Caesar
et al. 2020) dataset. NuScenes contains 1000 scenes, each of
which has 20 seconds of LiDAR point cloud sequences and
multi-view camera videos annotated at 2Hz. Following the
setting in previous works for fair comparisons (Wu, Chen,
and Metaxas 2020; Wang et al. 2022; Luo, Yang, and Yuille
2021; Li et al. 2023; Jia et al. 2023), we adopt 500 scenes
for training, 100 scenes for validation, and 250 scenes for
testing. During training, we utilize both the LiDAR point
clouds and camera images, while only LiDAR point cloud
data is required for the validation and testing of the model.
The ground truth BEV motion flow for validation and testing
is generated from the detection and tracking annotation from
the NuScenes dataset.
Implementation details. Initially, the BEV feature maps
are extracted from the multi-frame point clouds by (Lang
et al. 2019). Our model backbone is built upon Motion-
Net (Wu, Chen, and Metaxas 2020), which takes sequen-
tial BEV features as input and extracts spatial-temporal fea-
tures. The input point clouds are cropped within a range of
[−32, 32] × [−32, 32] × [−3, 2] meters, and the BEV output
map is 256 × 256 in size, which means each cell has a range
of 0.25m×0.25m. It is worth noting that our proposed cross-
modality self-supervision framework is independent of the
network backbone. Also, during the inference process, only
sequential point cloud data is needed as the model input.
To generate the optical flow, we employ (Teed and Deng
2020) as the optical flow estimation model with the pre-
trained parameters offered by Pytorch. The static/dynamic
classification thresholds in eq.5 are τ 2D = 5pixels and
τ 3D = 1m. Besides, we extract the points of the ground
plane based on the heights and designate them as the static
part of the scene. For the training loss in eq.10, we set
λmc
=
1, λpr
=
0.1 and λtc
=
0.4. We employ
AdamW (Loshchilov and Hutter 2017) optimization algo-
rithm for training. All models are trained on four NVIDIA
3090 GPUs with a batch size of 64. We train the model for
100 epochs with an initial learning rate of 0.008, and we de-
cay the learning rate by 0.5 every 20 epochs.
Metrics. Following previous works (Wu, Chen, and
Metaxas 2020; Wang et al. 2022; Luo, Yang, and Yuille
2021; Li et al. 2023; Jia et al. 2023), we use the mean and
median errors of motion flow on non-empty cells for eval-
uation. The error is computed by the L2 distance between
the predicted motion flow and ground truth flow for the next
1s future. The final results are presented in three categories
divided by varying speeds: static (background and static ob-
jects), slow (speed ≤ 5 m/s), and fast (speed > 5m/s). Re-
garding the whole model, we directly utilize the 1s future
flow output to calculate the metrics. In ablation studies, if
the model only predicts the subsequent 0.5s of future flow,
we employ linear interpolation to estimate the predicted flow
for the next 1s future.
Comparison with SOTA Methods
Table 1 presents a comprehensive comparison between our
proposed self-supervised approach and other methods for
BEV motion prediction. Based on the training supervision,
all approaches can be categorized into three groups: fully
supervised, weakly supervised, and self-supervised. We see
that our method achieves state-of-the-art performance in the
self-supervised group and surpasses previous methods by a
significant margin. Compared to the previous state-of-the-
art method (Jia et al. 2023), we exhibit a remarkable im-
provement of 41% in fast speed metrics, which represent the
more challenging and crucial part of motion prediction, 7%
in slow speed metrics, and 38% in static metrics.
(Li et al. 2023) is a weakly supervised method that adopts
foreground/background annotation as extra supervision sig-
nals. Notably, our method shows comparable performance
and even surpasses it in terms of the mean error of fast mo-
tion. Furthermore, our method outperforms some fully su-
pervised methods such as (Gu et al. 2019) and (Shi, Wang,
and Li 2019) by 52% and 48% respectively.
Ablation Studies
Masked Chamfer loss. To enhance the robustness of self-
supervised learning by mitigating the noises in point cloud
sequence data, we design a masked Chamfer distance loss
based on the pseudo static/dynamic mask generated from
optical flow images. An example of the generated static/-
dynamic mask is illustrated in Figure 4(a). Exp. 1&3, 2&5
in Table 2 compare the results of the original Chamfer dis-
tance loss (eq. 1) with the masked Chamfer distance loss
(eq. 6). We can see that the masked Chamfer distance loss
can improve all metrics by a large margin. Especially for
the static motion metrics, the masked Chamfer distance loss
can bring up to 75% improvement. This shows its effective-
ness of eliminating noise and disturbances originating from
Method
Supervision
Static
Speed ≤ 5 m/s
Speed > 5 m/s
Mean↓
Median↓
Mean↓
Median↓
Mean↓
Median↓
HPLFlowNet (Gu et al. 2019)
supervised
0.0041
0.0002
0.4458
0.0960
4.3206
2.4881
PointRCNN (Shi, Wang, and Li 2019)
supervised
0.0204
0
0.5514
0.1627
3.9888
1.6252
LSTM-ED (2019)
supervised
0.0358
0
0.3551
0.1044
1.5885
1.0003
MotionNet (Wu, Chen, and Metaxas 2020)
supervised
0.0201
0
0.2292
0.0952
0.9454
0.6180
PillarMotion (Luo, Yang, and Yuille 2021)
supervised
0.0245
0
0.2286
0.0930
0.7784
0.4685
BE-STI (Wang et al. 2022)
supervised
0.0220
0
0.2115
0.0929
0.7511
0.5413
WeakMotionNet (Li et al. 2023)
weakly sup.
0.0426
0
0.4009
0.1195
2.1342
1.2061
FlowNet3D (Liu, Qi, and Guibas 2019)
pre.
2.0514
0
2.2058
0.3172
9.1923
8.4923
HPLFlowNet (Gu et al. 2019)
pre.
2.2165
1.4925
1.5477
1.1269
5.9841
4.8553
PillarMotion (Luo, Yang, and Yuille 2021)
self.
0.1620
0.0010
0.6972
0.1758
3.5504
2.0844
ContrastMotion (Jia et al. 2023)
self.
0.0829
0
0.4522
0.0959
3.5266
1.3233
Ours
self.
0.0514
0
0.4212
0.1073
2.0766
1.3226
Table 1: Evaluation results of BEV motion prediction on nuScenes (Caesar et al. 2020) test set. There are four kinds of training
supervision: supervised, weakly-supervised (weakly sup.), pre-trained (pre.), and self-supervised (self.). Our method outper-
forms other self-supervised methods by a significant margin.
the static background, which constitutes the majority of the
point cloud data.
Piecewise rigidity. To ensure uniformity of motion within
the same instance, we design an algorithm to generate in-
stance pieces initially from over-segmentation on optical
flow images and propose a piecewise rigidity loss to regulate
the motion consistency in each piece. Figure 4(b) provides
an illustration of the generated pieces.
Exp. 1&2, 3&5 in Table 2 demonstrate the effectiveness
of the piecewise rigidity loss, resulting in an improvement
of approximately 15% across all evaluation metrics. Exp.4
in Table 2 utilizes a simple neighborhood smoothness loss
to constrain the local rigidity of prediction motion, which
serves a similar purpose to our piecewise rigidity approach
(see more info in supp.). Exp. 4&5 indicates that our method
outperforms the alternative smoothness loss in performance.
Moreover, the piecewise rigidity loss brings significant ad-
vantages in terms of training time and computational re-
sources.
Temporal consistency. Table 3 presents the results of the
ablation study conducted on temporal consistency and pre-
diction frames. It is evident that all experiments incorpo-
rating the temporal consistency loss exhibit higher perfor-
mance, which highlights the effectiveness of temporal con-
sistency as a motion pattern that aids in the learning of
motion prediction. Furthermore, we explore the impact of
different prediction frames on training a motion predic-
tion model. The complete framework predicts the motion
of frames -1, 1, and 2 during training with a time inter-
val of 0.5 seconds, and the temporal consistency loss is
applied across all predicted frames. In Table 3, the ’past’
frame refers to the backward frame -1 and the ’future’ frame
refers to frame 2. We see that i) As the number of frames
involved in motion prediction learning increases, the predic-
tion performance improves correspondingly. This is because
the temporal consistency pattern becomes more prominent
over a longer point cloud sequence. ii) Predicting backward
motion (frame -1) yields a larger improvement compared to
predicting a further future frame (frame 2). Due to the ego
vehicle’s movement, the variations in point cloud data be-
Exp.
m.c.
smooth.
p.r.
Static
Speed
Speed
≤ 5 m/s
> 5 m/s
1
0.2515
0.8771
3.4098
2
✓
0.2097
0.7135
3.1892
3
✓
0.0704
0.4815
2.5389
4
✓
✓
0.0677
0.4493
2.2142
5
✓
✓
0.0514
0.4212
2.0766
Table 2: Ablation of masked Chamfer distance and piece-
wise rigidity losses. m.c.: masked Chamfer distance loss;
smooth.: smoothness regularization; p.r.: piecewise rigidity.
past
future
temp.
Static
Speed
Speed
≤ 5 m/s
> 5 m/s
✓
✓
0.1150
0.5549
2.7503
✓
✓
0.0748
0.5307
2.8830
✓
✓
0.0838
0.5074
2.1814
✓
✓
✓
0.0514
0.4212
2.0766
Table 3: Ablation for prediction frames and temporal con-
sistency loss. past: backward frame -1; future: frame 2 into
the future; temp.: temporal consistency loss.
come larger when the time interval expands, which makes
learning the correspondence between point clouds a more
challenging task
Please refer to the supplementary materials for more qual-
itative results and ablation studies.
Conclusions
In this paper, we present a novel cross-modality self-
supervised method for BEV motion prediction. Concretely,
we exploit static/dynamic classification and rigid pieces on
point clouds from sequential multi-view images to facilitate
motion learning without any manual annotations. Moreover,
we enforce temporal consistency across multiple frames,
ensuring temporal smoothness of predicted motion. Com-
prehensive experiments conducted on the nuScenes dataset
demonstrate that our proposed method achieves state-of-the-
art performance and all designed modules are effective.
Acknowledgments
This research is supported by NSFC under Grant 62171276
and the Science and Technology Commission of Shanghai
Municipal under Grant 21511100900, 22511106101, and
22DZ2229005.
References
Achanta, R.; Shaji, A.; Smith, K.; Lucchi, A.; Fua, P.; and
S¨usstrunk, S. 2012. SLIC superpixels compared to state-of-
the-art superpixel methods. IEEE transactions on pattern
analysis and machine intelligence, 34(11): 2274–2282.
Baur, S. A.; Emmerichs, D. J.; Moosmann, F.; Pinggera, P.;
Ommer, B.; and Geiger, A. 2021. Slim: Self-supervised li-
dar scene flow and motion segmentation. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, 13126–13136.
Caesar, H.; Bankiti, V.; Lang, A. H.; Vora, S.; Liong, V. E.;
Xu, Q.; Krishnan, A.; Pan, Y.; Baldan, G.; and Beijbom, O.
2020. nuscenes: A multimodal dataset for autonomous driv-
ing. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition, 11621–11631.
Casas, S.; Gulino, C.; Liao, R.; and Urtasun, R. 2020.
Spagnn: Spatially-aware graph neural networks for rela-
tional behavior forecasting from sensor data.
In 2020
IEEE International Conference on Robotics and Automation
(ICRA), 9491–9497. IEEE.
Chen, S.; Liu, B.; Feng, C.; Vallespi-Gonzalez, C.; and
Wellington, C. 2020. 3d point cloud processing and learn-
ing for autonomous driving: Impacting map creation, local-
ization, and perception. IEEE Signal Processing Magazine,
38(1): 68–86.
Cheng, W.; and Ko, J. H. 2022. Bi-PointFlowNet: Bidirec-
tional Learning for Point Cloud Based Scene Flow Estima-
tion. In Computer Vision–ECCV 2022: 17th European Con-
ference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
Part XXVIII, 108–124. Springer.
Ding, F.; Palffy, A.; Gavrila, D. M.; and Lu, C. X. 2023. Hid-
den gems: 4d radar scene flow learning using cross-modal
supervision. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 9340–9349.
Dong, G.; Zhang, Y.; Li, H.; Sun, X.; and Xiong, Z. 2022.
Exploiting rigidity constraints for lidar scene flow estima-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, 12776–12785.
Fang, S.; Wang, Z.; Zhong, Y.; Ge, J.; and Chen, S. 2023.
TBP-Former: Learning Temporal Bird’s-Eye-View Pyramid
for Joint Perception and Prediction in Vision-Centric Au-
tonomous Driving. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, 1368–
1378.
Filatov, A.; Rykov, A.; and Murashkin, V. 2020. Any mo-
tion detector: Learning class-agnostic scene dynamics from
a sequence of lidar point clouds. In 2020 IEEE international
conference on robotics and automation (ICRA), 9498–9504.
IEEE.
Gojcic, Z.; Litany, O.; Wieser, A.; Guibas, L. J.; and Birdal,
T. 2021. Weakly supervised learning of rigid 3D scene flow.
In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, 5692–5703.
Gu, X.; Wang, Y.; Wu, C.; Lee, Y. J.; and Wang, P. 2019.
Hplflownet: Hierarchical permutohedral lattice flownet for
scene flow estimation on large-scale point clouds. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, 3254–3263.
Hu, A.; Murez, Z.; Mohan, N.; Dudas, S.; Hawke, J.; Badri-
narayanan, V.; Cipolla, R.; and Kendall, A. 2021.
Fiery:
Future instance prediction in bird’s-eye view from surround
monocular cameras. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, 15273–15282.
Jia, X.; Zhou, H.; Zhu, X.; Guo, Y.; Zhang, J.; and Ma,
Y. 2023.
ContrastMotion: Self-supervised Scene Motion
Learning for Large-Scale LiDAR Point Clouds.
arXiv
preprint arXiv:2304.12589.
Jund, P.; Sweeney, C.; Abdo, N.; Chen, Z.; and Shlens, J.
2021.
Scalable scene flow from point clouds in the real
world. IEEE Robotics and Automation Letters, 7(2): 1589–
1596.
Khurana, T.; Hu, P.; Held, D.; and Ramanan, D. 2023. Point
Cloud Forecasting as a Proxy for 4D Occupancy Forecast-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, 1116–1124.
Kittenplon, Y.; Eldar, Y. C.; and Raviv, D. 2021. Flowstep3d:
Model unrolling for self-supervised scene flow estimation.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 4114–4123.
Lang, A. H.; Vora, S.; Caesar, H.; Zhou, L.; Yang, J.; and
Beijbom, O. 2019. Pointpillars: Fast encoders for object de-
tection from point clouds. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition,
12697–12705.
Lee, K.-H.; Kliemann, M.; Gaidon, A.; Li, J.; Fang, C.; Pil-
lai, S.; and Burgard, W. 2020. Pillarflow: End-to-end birds-
eye-view flow estimation for autonomous driving. In 2020
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), 2007–2013. IEEE.
Li, B.; Zheng, C.; Li, G.; and Ghanem, B. 2022a. Learn-
ing scene flow in 3d point clouds with noisy pseudo labels.
arXiv preprint arXiv:2203.12655.
Li, R.; Lin, G.; He, T.; Liu, F.; and Shen, C. 2021. Hcrf-flow:
Scene flow from point clouds with continuous high-order
crfs and position-aware flow embedding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 364–373.
Li, R.; Shi, H.; Fu, Z.; Wang, Z.; and Lin, G. 2023.
Weakly Supervised Class-Agnostic Motion Prediction for
Autonomous Driving.
In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
17599–17608.
Li, R.; Zhang, C.; Lin, G.; Wang, Z.; and Shen, C. 2022b.
Rigidflow: Self-supervised scene flow learning on point
clouds by local rigidity prior.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 16959–16968.
Li, X.; Kaesemodel Pontes, J.; and Lucey, S. 2021. Neural
scene flow prior. Advances in Neural Information Process-
ing Systems, 34: 7838–7851.
Li, Y.; Yu, A. W.; Meng, T.; Caine, B.; Ngiam, J.; Peng, D.;
Shen, J.; Lu, Y.; Zhou, D.; Le, Q. V.; et al. 2022c. Deep-
fusion: Lidar-camera deep fusion for multi-modal 3d object
detection. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 17182–17191.
Liang, T.; Xie, H.; Yu, K.; Xia, Z.; Lin, Z.; Wang, Y.; Tang,
T.; Wang, B.; and Tang, Z. 2022. Bevfusion: A simple and
robust lidar-camera fusion framework. Advances in Neural
Information Processing Systems, 35: 10421–10434.
Liu, H.; Lu, T.; Xu, Y.; Liu, J.; Li, W.; and Chen, L. 2022.
CamLiFlow: Bidirectional camera-LiDAR fusion for joint
optical flow and scene flow estimation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 5791–5801.
Liu, X.; Qi, C. R.; and Guibas, L. J. 2019.
Flownet3d:
Learning scene flow in 3d point clouds. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 529–537.
Loshchilov, I.; and Hutter, F. 2017. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101.
Luo, C.; Yang, X.; and Yuille, A. 2021. Self-supervised pil-
lar motion learning for autonomous driving. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, 3183–3192.
Luo, W.; Yang, B.; and Urtasun, R. 2018. Fast and furious:
Real time end-to-end 3d detection, tracking and motion fore-
casting with a single convolutional net. In Proceedings of the
IEEE conference on Computer Vision and Pattern Recogni-
tion, 3569–3577.
Mittal, H.; Okorn, B.; and Held, D. 2020. Just go with the
flow: Self-supervised scene flow estimation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition, 11177–11185.
Phillips, J.; Martinez, J.; Bˆarsan, I. A.; Casas, S.; Sadat, A.;
and Urtasun, R. 2021. Deep multi-task learning for joint
localization, perception, and prediction. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 4679–4689.
Pontes, J. K.; Hays, J.; and Lucey, S. 2020. Scene flow from
point clouds with or without learning. In 2020 international
conference on 3D vision (3DV), 261–270. IEEE.
Puy, G.; Boulch, A.; and Marlet, R. 2020. Flot: Scene flow
on point clouds guided by optimal transport. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XXVIII, 527–
544. Springer.
Rishav, R.; Battrawy, R.; Schuster, R.; Wasenm¨uller, O.; and
Stricker, D. 2020. DeepLiDARFlow: A deep learning archi-
tecture for scene flow estimation using monocular camera
and sparse LiDAR. In 2020 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems (IROS), 10460–
10467. IEEE.
Schreiber, M.; Hoermann, S.; and Dietmayer, K. 2019.
Long-term occupancy grid prediction using recurrent neu-
ral networks. In 2019 International Conference on Robotics
and Automation (ICRA), 9299–9305. IEEE.
Shen, Y.; Hui, L.; Xie, J.; and Yang, J. 2023. Self-Supervised
3D Scene Flow Estimation Guided by Superpoints. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 5271–5280.
Shi, S.; Wang, X.; and Li, H. 2019. Pointrcnn: 3d object
proposal generation and detection from point cloud. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, 770–779.
Sun, D.; Yang, X.; Liu, M.-Y.; and Kautz, J. 2018. Pwc-
net: Cnns for optical flow using pyramid, warping, and cost
volume. In Proceedings of the IEEE conference on computer
vision and pattern recognition, 8934–8943.
Teed, Z.; and Deng, J. 2020. Raft: Recurrent all-pairs field
transforms for optical flow.
In Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–
28, 2020, Proceedings, Part II 16, 402–419. Springer.
Tishchenko, I.; Lombardi, S.; Oswald, M. R.; and Pollefeys,
M. 2020. Self-supervised learning of non-rigid residual flow
and ego-motion. In 2020 international conference on 3D
vision (3DV), 150–159. IEEE.
Vora, S.; Lang, A. H.; Helou, B.; and Beijbom, O. 2020.
Pointpainting: Sequential fusion for 3d object detection. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition, 4604–4612.
Wang, Y.; Pan, H.; Zhu, J.; Wu, Y.-H.; Zhan, X.; Jiang, K.;
and Yang, D. 2022. Be-sti: Spatial-temporal integrated net-
work for class-agnostic motion prediction with bidirectional
enhancement. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 17093–17102.
Wei, S.; Wei, Y.; Hu, Y.; Lu, Y.; Zhong, Y.; Chen, S.; and
Zhang, Y. 2023. Asynchrony-Robust Collaborative Percep-
tion via Bird’s Eye View Flow. In Thirty-seventh Conference
on Neural Information Processing Systems.
Wilson, B.; Qi, W.; Agarwal, T.; Lambert, J.; Singh, J.;
Khandelwal, S.; Pan, B.; Kumar, R.; Hartnett, A.; Pontes,
J. K.; et al. 2023. Argoverse 2: Next generation datasets
for self-driving perception and forecasting. arXiv preprint
arXiv:2301.00493.
Wong, K.; Wang, S.; Ren, M.; Liang, M.; and Urtasun, R.
2020. Identifying unknown instances for autonomous driv-
ing. In Conference on Robot Learning, 384–393. PMLR.
Wu, P.; Chen, S.; and Metaxas, D. N. 2020.
Motionnet:
Joint perception and motion prediction for autonomous driv-
ing based on bird’s eye view maps.
In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, 11385–11395.
Wu, W.; Wang, Z.; Li, Z.; Liu, W.; and Fuxin, L. 2019.
Pointpwc-net: A coarse-to-fine network for supervised and
self-supervised scene flow estimation on 3d point clouds.
arXiv preprint arXiv:1911.12408.
Zhang, Y.; Zhu, Z.; Zheng, W.; Huang, J.; Huang, G.; Zhou,
J.; and Lu, J. 2022. Beverse: Unified perception and predic-
tion in birds-eye-view for vision-centric autonomous driv-
ing. arXiv preprint arXiv:2205.09743.
Pseudo Static/Dynamic Mask Generation
To generate the pseudo static/dynamic mask, we categorize
each point based on its corresponding optical flow value us-
ing a predetermined threshold. Points with optical flow val-
ues below the threshold are classified as static. However, em-
ploying the identical threshold for the optical flow may not
always yield accurate results due to the different distances
from points to the camera. Specifically, distant moving ob-
jects far from the camera may exhibit small optical flow val-
ues and be erroneously classified as static. To address this is-
sue, we utilize a projected 3D scene flow from optical flow to
supplement the static assessment, which contains additional
spatial information compared to 2D optical flow. We will
provide a detailed explanation of how the function Toptf→sf
in eq.4 (in the main paper) is computed.
For point pi = (xi, yi, zi) ∈ P, its corresponding image
pixel is (ui, vi) calculated in eq.2 (in the main paper) by a
projection function TP→I.
wi(ui, vi, 1)T = TP→I(xi, yi, zi, 1)T
where TP→I ∈ R3×4 is the projection matrix. Note that
in eq.2 (in the main paper), we provide a simplified equation
for brevity.
Also, we have calculated the dynamic optical flow f 2D
i
∈
R2 after eliminating the effect of ego vehicle motion in eq.3
(in the main paper). Then the end pixel of f 2D
i
can be defined
as (u′
i, v′
i) = (ui, vi) + f 2D
i .
Since the vertical flow is zero, we can get the endpoint
(x′
i, y′
i, zi) of f 3D
i
by solving the following equation.
w′
i(u′
i, v′
i, 1)T = TP→I(x′
i, y′
i, zi, 1)T
By basic matrix transformation, we have
(u′
i, v′
i, 1)T = Tnew(x′
i
1
w′
i
, y′
i
1
w′
i
, 1
w′
i
)T
where Tnew ∈ R3×3 is calculated by
Tnew = [TP→I[:, : 2] | TP→I[:, : 2] ∗ zi + TP→I[:, : 3]]
Then (x′
i, y′
i) can be calculated by
(x′
i, y′
i)T = T −1
new(u′
i, v′
i, 1)T[: 2] / T −1
new(u′
i, v′
i, 1)T[2 : 3]
Finally, we have
f 3D
i
= (x′
i, y′
i, zi) − (xi, yi, zi)
By setting thresholds for both the length values of f 2D
i
and f 3D
i , the pseudo static/dynamic mask can be finally gen-
erated (eq.5 in the main paper).
Piecewise Rigidity Ablation
In the ablation study regarding piecewise rigidity, we use a
smoothness regularization for comparative analysis. We im-
plement the local smoothness loss, which is a widely used
technique in scene flow estimation (Wu et al. 2019; Kitten-
plon, Eldar, and Raviv 2021), to replace piecewise rigidity.
The local smoothness loss can be defined as
Lsmooth =
X
pi∈P
1
|N(pi)|
X
pk∈N (pi)
∥fi − fk∥2
2
where N(pi) is the neighbor point set of pi.
Compared to local smoothness loss, our proposed piece-
wise rigidity loss has several advantages: 1) The piecewise
rigidity ensures instance-level flow uniformity rather than
local flow smoothness, aligning more closely with realistic
motion pattern; 2) The smoothness regularization can lead
to incorrect guidance at the boundaries between dynamic
objects and the background; 3) Consequently, our method
outperforms smoothness loss, as is shown in the comparison
of exp.4 and exp.5 in Table 2 (in the main paper); 4) Our ap-
proach offers significantly improved training efficiency. The
smoothness loss traverses all the point clouds during com-
putation, while the piecewise rigidity only requires calcula-
tions for a few rigid pieces.
Generated Masks Visualization
We present visualizations of the pseudo static/dynamic mask
and rigid piece labels generated by our method under dif-
ferent scenarios. It is evident that the generated labels ex-
hibit high quality in clear weather conditions (Figure 5 &
6), while the quality is compromised during nighttime pe-
riods (Figure 7). This discrepancy arises because our label
generation heavily relies on camera image data, which is
more favorable in good weather conditions. Consequently,
our method may have limitations in handling dark or adverse
weather data.
Nevertheless, these limitations have a minimal impact on
the practicality of our approach. In real-world applications,
it is feasible to train the model exclusively using raw data
collected during favorable daytime conditions. Since our
model’s inference solely relies on sequential point cloud in-
puts, and LiDAR point cloud data is less affected by weather
conditions, models trained solely on daytime data can still
generalize well to various weather conditions.
It is important to note that, in all our experiments, we did
not exclude scenes with nighttime, rainy, or other adverse
weather conditions. As a result, data containing noise was
also included in the self-supervised training of our model.
Introducing weather condition priors and excluding such
data can lead to predictable improvements in the perfor-
mance of our self-supervised learning method.
Argoverse2 Results
We
also
conduct
experiments
on
the
Argoverse2
dataset (Wilson et al. 2023), which features diverse
sensor configurations with NuScenes (Caesar et al. 2020).
We extract 12686 samples from the training set for training
and 2029/2062 samples from the validation set for valida-
tion/testing. The input point clouds are cropped within a
range of [−32, 32] × [−32, 32] × [−3, 2] meters, and the
BEV output map is also 256 × 256 in size, where each cell
has a range of 0.25m × 0.25m.
Table 4 shows the results of our method on Argoverse2.
Results show that our proposed method is still effective and
is comparable to supervised learning. It also demonstrates
that our method can be generalized to different scenarios and
sensor configurations.
Exp.
Static
Speed ≤ 5 m/s
Speed > 5 m/s
Zero Flow
0.0000
0.6104
9.3726
Supervised
0.0235
0.2310
1.3726
Ours (w/ m.c. w/o p.r.)
0.1419
0.4496
2.6137
Ours (w/ m.c. w/ p.r.)
0.0736
0.4221
2.3359
Table 4: Experimental results on Argoverse2 (Wilson et al.
2023). m.c.: masked Chamfer distance loss; p.r.: piecewise
rigidity.
Qualitative Comparisons
Figure 8 illustrates the qualitative comparisons for the
masked Chamfer distance loss and piecewise rigidity loss.
The corresponding quantitative comparisons are in Table 2
in the main paper.
The first row shows the results without both the masked
Chamfer distance loss and piecewise rigidity loss. In this
case, the model predicts numerous incorrect flows for the
background points, primarily due to the presence of substan-
tial noise in the point cloud data. The second row showcases
the results without the piecewise rigidity loss. We can ob-
serve that the absence of the piecewise rigidity loss leads to
less uniform motion flows for the same object. Finally, the
third row is the results obtained from our full self-supervised
training framework.
Overall, these results demonstrate that 1) the masked
Chamfer distance design is crucial in preventing the model
from predicting incorrect flows for background points af-
fected by noise; 2) the piecewise rigidity loss effectively pro-
motes consistent motion flows for the same object; 3) our
self-supervised method produces high-quality motion pre-
dictions that closely resemble the ground truth.
Figure 5: Visualizations of the pseudo static/dynamic mask and rigid piece labels. A good case.
Figure 6: Visualizations of the pseudo static/dynamic mask and rigid piece labels. A good case.
Figure 7: Visualizations of the pseudo static/dynamic mask and rigid piece labels. A bad case.
Figure 8: Qualitative Comparisons. From top to bottom, first row: results without masked Chamfer distance and piecewise
rigidity; second row: results without piecewise rigidity; third row: full framework results; fourth row: BEV motion ground truth
","nanInspired by self-supervised scene flow estimation, current self-supervised BEV motion prediction methods (Luo, Yang, and Yuille 2021; Li et al. 2023) primarily rely on chamfer distance loss to establish the point-level correspondences between point clouds. However, this heavy dependence on point-level correspondences leads to two major problems when learning motion patterns from real-world LiDAR point cloud data.
The first problem is fake flows. Due to the alterations in the viewpoint of the LiDAR sensor, points associated with background or static objects often exhibit flow that does not exist, as shown in Figure 1(a). This fake flow will mislead the model to learn incorrect motion patterns, thereby adversely affecting the accuracy of predictions. One previous work (Li et al. 2023) introduces a weakly supervised setting where foreground/background ground truth is available to mitigate the impact of noise originating from background points to alleviate the problem. However, the method still remains limited as extra human annotation is indispensable.
The second problem is the inconsistent flows within one single object; see an illustration in Figure 1(b). Owing to the inherent sparsity in point cloud data, the point-level flows associated with the same objects may exhibit inconsistent motions when solely relying on the point correspondences. This problem of inconsistency violates the object-level rigid constraints and causes confusion for model learning procedures."
"The Reed-Muller codes are often used for correcting errors in data transmission. This paper focuses on a subset of the Reed-Muller codes called the symmetric Reed-Muller codes introduced by Wei Yan and Sian-Jheng Lin. By examining the linear maps of the automorphism group of symmetric Reed-Muller codes the authors show that the set of these linear maps forms a subgroup of the general linear group, which is the automorphism group of punctured Reed-Muller codes. They also provide a method for determining all the automorphisms explicitly for specific cases.","For various reasons, it is crucial to study the automorphisms of the codes in coding theory. The study of automorphisms can help develop efficient decoding algorithms by taking advantage of the inherent symmetries within the codes. It can also shed light on the error-correction and error-detection properties of the codes. This paper focuses on a family of error-correcting codes called Reed-Muller codes and their subset, the symmetric Reed-Muller codes. The authors investigate linear maps of the automorphism group of the symmetric Reed-Muller codes to show that they form a subgroup of the general linear group, which is also related to punctured Reed-Muller codes.","The authors investigate the linear maps that leave symmetric Reed-Muller codes invariant. For specific cases where n = 2 and n = 3, they derive the exact set of these linear maps and prove that the set forms a subgroup of the general linear group. The authors use diﬀerent methods for each value of n, as one approach may not be well-suited for the other. To determine the invariant set of linear maps, they employ various lemmas involving coeﬃcients and polynomial transformations.","The main results of the paper are two theorems for n = 2 and n = 3. In the case of n = 2, the authors provide necessary and suﬃcient conditions for symmetric Reed-Muller codes to be invariant under a subgroup of GLp2, qq, and exhibit the explicit form of the corresponding transformations. For n = 3, they present a subgroup of GLp3, qq that preserves the symmetric Reed-Muller codes and determine the corresponding linear maps. The authors also provide examples to illustrate the concepts and results.","The authors conclude by stating a conjecture about the case of general n. They suggest that for any n, there exists a subgroup of the general linear group that leaves the symmetric Reed-Muller codes invariant. However, they leave the task of proving this conjecture and finding a generalized method for arbitrary n as open problems for future research.",On a Group Under Which Symmetric Reed-Muller Codes are Invariant,"Sibel Kurt Toplu, Talha Arikan, Pinar AydoğDu, OğUz Yayla","arXiv:2401.11496v1  [cs.IT]  21 Jan 2024
ON A GROUP UNDER WHICH SYMMETRIC REED-MULLER CODES
ARE INVARIANT
SIBEL KURT TOPLU˚, TALHA ARIKAN, PINAR AYDOĞDU, AND OĞUZ YAYLA
Abstract. The Reed-Muller codes are a family of error-correcting codes that have been
widely studied in coding theory. In 2020, Wei Yan and Sian-Jheng Lin introduced a variant
of Reed-Muller codes so called symmetric Reed-Muller codes. We investigate linear maps
of the automorphism group of symmetric Reed-Muller codes and show that the set of these
linear maps forms a subgroup of the general linear group, which is the automorphism group
of punctured Reed-Muller codes. We provide a method to determine all the automorphisms
in this subgroup explicitly for some special cases.
Keywords. Reed-Muller codes, Symmetric Reed-Muller codes, Aﬃne invariant, Automor-
phism groups.
MSC2020-Mathematics Subject Classiﬁcation: 94B05, 11T71, 08A35, 11T06
1.
Introduction and Preliminaries
Working with automorphisms of the codes is important for various reasons in the ﬁeld of
coding theory. The study of automorphisms can contribute to developing eﬃcient decoding
algorithms. By exploiting the symmetries within a code, it may be possible to design more
computationally eﬃcient algorithms in order to correct errors. Thus automorphisms of the
codes play a crucial role in understanding and characterizing the error-correction properties
of codes. To sum up by studying the symmetries of a code, one can gain insights into how
errors aﬀect the encoded information and how the code can be designed to correct or detect
these errors.
Reed-Muller codes (RM codes, for short) are a family of error-correcting codes that were
ﬁrst introduced by Irving S. Reed and Gustave Solomon Muller in 1954. A large number
of RM code variations and generalizations were introduced in the literature, for instance,
see [8, 12, 13]. They have a large minimum distance, which makes them good at correcting
errors. They also have simple encoding and decoding algorithms, which make them useful
and eﬃcient to implement. There are some variants of RM codes, such as the binary RM
codes deﬁned on the prime ﬁeld F2 and the p-ary RM codes deﬁned on the prime ﬁeld Fp,
where p is a prime number. Furthermore, RM codes have been generalized in many ways,
such as the generalized Reed-Muller codes (GRM, for short), which are deﬁned over an
arbitrary ﬁnite ﬁeld, and the symmetric Reed-Muller codes (SRM, for short) which have a
certain symmetry property.
In coding theory, the majority of classical codes have a sizable automorphism group that
is connected to a linear group. For example, GRM codes are invariant under the general
˚ Corresponding Author
This publication is a part of the doctoral thesis of Sibel Kurt Toplu
1
2
SIBEL KURT TOPLU˚, TALHA ARIKAN, PINAR AYDOĞDU, AND OĞUZ YAYLA
aﬃne group GApm, qq [8]. A requirement for a code to be invariant under the aﬃne group
is provided by Kasami et al. in [10]. Delsarte describes the codes as invariant under certain
linear groups in [7]. He shows that under GApm, qq, only p-ary RM codes can be invariant;
nonetheless, the issue of fully determining the automorphism group of aﬃne invariant codes
has not yet been resolved. Determining the complete automorphism group of a code is a
challenging topic that is frequently connected to the categorization of simple groups. Dür
is credited with the research of the automorphism groups of Reed-Solomon codes and their
extensions [7]. In [2], Berger has proved the complete automorphism groups of GRM codes.
In 1996, Berger has demonstrated that GApm, qq contains the permutation group of any
aﬃne-invariant code [3] and then he demonstrates how to create a formal expression for
every aﬃne-invariant code’s permutation group in [5]. The complete automorphism groups
of the projective and homogeneous RM codes are found in [4]. These groups are associated
with the projective linear group and the general linear group, respectively. In this work, we
aim to investigate a subgroup of the automorphism group SRM codes whose elements are
linear maps.
SRM codes are ﬁrst introduced by using bivariate polynomials over Fq in [14]. The local
correctability of the bivariate SRM codes is discussed in [15]. The authors begin by outlining
the advantages of the symmetric structure and oﬀering intuitions to indicate the superiority
of SRM codes over GRM codes in terms of local correctability. The tolerance of error ratios
of SRM codes and GRM codes over the same ﬁnite ﬁeld, as well as their code rate, are
then demonstrated. They establish that a class of locally-correctable codes is composed of
multivariable SRM codes SRMqrn, rs in [15]. Furthermore, the dual of SRMqrn, rs is also
presented. However, transformations preserving these codes, speciﬁcally belonging to the
automorphism group, have not been studied yet. Taking inspiration from this gap in this
research, we have directed our focus toward addressing this problem.
First, we will recall some basic notions and some results on RM and GRM codes which
will be useful throughout the paper.
1.1. Equivalence of Linear Codes. Let Fq be the ﬁnite ﬁeld of order q “ pm for a prime p
and Fn
q be the n-dimensional vector space over Fq. The measure of dissimilarity between two
vectors is established by counting the coordinates in which they diﬀer. A linear rn, k, ds-code
is k-dimensional linear subspace of Fn
q with the minimum distance d. A generator matrix G
of a linear rn, ks-code C is any matrix of row rank k, whose rows come from the code C. We
will give the deﬁnitions of some types of code-equivalence. We refer to [9] for further details.
We need the following deﬁnition for simplicity.
Deﬁnition 1.1. [6] Let π “ pπ1, π2, . . . , πnq be a permutation of t1, 2, . . ., nu. The permu-
tation π can be given in the equivalent form as an n ˆ n permutation matrix P with 1’s in
positions pi, πiq for i “ 1, 2, . . ., n and 0’s elsewhere. Pn denotes the corresponding set of all
n ˆ n permutation matrices P.
Two linear codes C1 and C2 are called permutation equivalent if there is a permutation of
coordinates which sends C1 to C2. Hence, two linear codes C1 and C2 of the same length are
permutation equivalent if there exists a permutation matrix P P Pn such that
G2 “ G1P,
where G1 and G2 are the generator matrices of the codes C1 and C2, respectively.
ON A GROUP UNDER WHICH SYMMETRIC REED-MULLER CODES ARE INVARIANT
3
If we work on a ﬁnite ﬁeld except F2, then we may need a more general form of the
equivalence. Recall that a square matrix is called monomial if it has exactly one nonzero
entry from that ﬁnite ﬁeld in each column and row. Hence, every monomial matrix over F2 is
a permutation matrix. A monomial matrix M can be expressed as either DP or PD1, where
D and D1 are non-singular diagonal matrices over the corresponding ﬁnite ﬁeld and P is a
permutation matrix. Two linear codes C1 and C2 of the same length over Fq are said to be
monomially equivalent when there exists a monomial matrix M such that
G2 “ G1M,
where G1 and G2 are the generator matrices of the codes C1 and C2, respectively. Note that
monomial equivalence and permutation equivalence coincide for binary codes.
Let γ be a ﬁeld automorphism of Fq and M “ DP be a monomial matrix over Fq, where P
is a permutation matrix and D is a non-singular diagonal matrix over Fq. Applying the map
Mγ to any codeword is described by the following process: Firstly, the ith component of code
is multiplied by the ith diagonal entry of D for all i’s. Then the corresponding permutation
associated with the permutation matrix P is applied to codeword. Finally, the automorphism
γ is applied to all components. Two linear codes C1 and C2 of the same length over Fq are
said to be equivalent when there is a monomial matrix M and a ﬁeld automorphism γ of Fq
such that
C2 “ C1Mγ,
where C1Mγ is obtained by applying Mγ to each codeword of C1. This is the most general
form of the equivalence.
Note that, all equivalence deﬁnitions are the same for the binary codes. Furthermore,
monomial equivalence and general equivalence coincide for p-ary codes, where p is a prime.
Since we have three types of equivalence, there exist three possible deﬁnitions of the auto-
morphism groups of the code families by considering C1 “ C2 in the above deﬁnitions.
Now consider a code C of length n over the ﬁeld Fq. The set of coordinate permutations
that map the code C to itself forms a group, called the permutation automorphism group
of C and denoted by PAutpCq. Obviously, PAutpCq is a subgroup of the symmetric group
Sn. The set of monomial matrices, by which C is monomially equivalent to itself, forms the
group MAutpCq, which is called the monomial automorphism group of C. The set of maps
of the form Mγ, where M is a monomial matrix and γ is a ﬁeld automorphism, that map C
to itself forms the group AutpCq, called automorphism group of C.
In general, we have that PAutpCq Ď MAutpCq Ď AutpCq. If q “ 2, then PAutpCq “
MAutpCq “ AutpCq. If q is prime, then MAutpCq “ AutpCq.
All these deﬁnitions and conclusions are well-known in the literature and can be found in
any basic coding theory book, for example in [9].
1.2. The Automorphisms of Reed-Muller Codes. There exist some equivalent deﬁ-
nitions of the RM codes.
Following [12], RM codes can be deﬁned in terms of mul-
tivariable polynomials as follows:
Let x “ px1, . . . , xmq range over F2
m.
Any function
fpxq “ fpx1, . . . , xmq which takes the values 0 and 1 is called a binary multivariable function.
We recall the following deﬁnitions.
Deﬁnition 1.2. [12] The rth order binary RM code Rpr, mq of length n “ 2m for 0 ď r ď m,
is the set of all vectors f, where fpx1, . . . , xmq is a binary multivariable polynomial of degree
4
SIBEL KURT TOPLU˚, TALHA ARIKAN, PINAR AYDOĞDU, AND OĞUZ YAYLA
at most r, i.e.,
Rpr, mq “
␣
pfpαqqαPFm
2 | f P F2rx1, . . . , xms, degpfq ď r
(
.
Deﬁnition 1.3. [12] For 0 ď r ď m ´ 1, the RM code which is obtained by puncturing
(or deleting) the coordinate corresponding to x1 “ ¨ ¨ ¨ “ xm “ 0 from all the codewords of
Rpr, mq is called the punctured RM code, and it is denoted by Rpr, mq˚.
Now we will mention the notion of aﬃne invariance which is crucial for our discussion on
automorphism groups. Before going further, we need some basic notions.
Let A “ raijs be an invertible m ˆ m binary matrix and b be a binary m ˆ 1 vector.
Consider the transformation T from binary m-tuples to binary m-tuples deﬁned by
T :
»
——–
x1
x2...
xm
ﬁ
ﬃﬃﬂ ÞÑ A
»
——–
x1
x2...
xm
ﬁ
ﬃﬃﬂ ` b,
which permutes binary m-tuples. T can be also considered as a permutation of multivariate
polynomials as follows:
TfpA, bq : fpx1, . . ., xmq ÞÑ f
´ÿ
a1jx1 ` b1, . . . ,
ÿ
amjxj ` bj
¯
.
(1)
The set of all such transformations formed by T is a group, which is known as the general
aﬃne group over F2 and is denoted by GApm, 2q (see [12]).
It is obvious that if f is a
polynomial of degree r, so is TfpA, bq.
As we mentioned above, for a binary code it is known that PAutpCq “ AutpCq, and hence
it is a subgroup of the symmetric group Sn. Following [1], a code C is said to be aﬃne
invariant if AutpCq includes a subgroup that is isomorphic to the aﬃne linear group.
The following example demonstrates the RM code families are one of the examples of aﬃne
invariant codes. This example is important to see the equivalence between transformations
applied to variables of the function to evaluate codeword and transformations applied to the
codeword itself.
Example 1.4. [1] Rpr, mq codes are aﬃne invariant.
Proof. Let A be an m ˆ m invertible matrix over F2 and b P Fm
2 . The aﬃne linear transform
T : x ÞÑ Ax`b yields a permutation on the coordinates of the codeword since the codewords
of RM codes are evaluation vectors and are indexed by the vectors x P Fm
2 . Then such a
permutation belongs to AutpRpr, mqq. Let c be a codeword in Rpr, mq. Then there exists a
polynomial f P F2rx1, . . . , xns with degpfq ď r such that c “ pfpαqqαPF2m. Since the outcome
of the transformation pf ˝ Tqpxq is another polynomial of degree less than or equal to r, we
have c1 “ pf ˝ TqpαqαPFm
2 P Rpr, mq. Thus, Rpr, mq codes are aﬃne invariant.
□
Thus, the general aﬃne group GApm, 2q permutes the codewords of the rth order Rpr, mq
and GApm, 2q Ă AutRpr, mq (see [12]). The subgroup of GApm, 2q consisting of all transfor-
mations
ON A GROUP UNDER WHICH SYMMETRIC REED-MULLER CODES ARE INVARIANT
5
T :
»
——–
x1
x2...
xm
ﬁ
ﬃﬃﬂ ÞÑ A
»
——–
x1
x2...
xm
ﬁ
ﬃﬃﬂ
(i.e., for which b “ 0) is known as the general linear group and is denoted by GLpm, 2q. We
can consider the transformation T described above in the following way, too:
(2)
TpAq : px1, . . . , xmq ÞÑ
´ÿ
a1jx1, . . . ,
ÿ
amjxj
¯
.
For the sake of convenience in usage, the function TfpA, bq will be denoted as TfpAq when
b “ 0. Since the transformation TpAq in (2) ﬁxes the zero m-tuple, the group GLpm, 2q
permutes the codewords of the punctured RM code Rpr, mq˚, i.e., GLpm, 2q Ă AutRpr, mq˚
(see [12]).
We know from [12, p. 400] that
AutpRpr, mq˚q “ S2m´1
for
r “ 0
and
m ´ 1,
AutpRpr, mqq “ S2m
for
r “ 0
and
m.
Furthermore, the following result is also given in [12, p. 400] which determines the exact
automorphism group of RM codes of rth order with length 2m over F2 completely.
Theorem 1.5. [12, p. 400] For 1 ď r ď m ´ 2:
‚ AutpRpr, mq˚q “ GLpm, 2q,
‚ AutpRpr, mqq “ GApm, 2q.
Note that one may easily adopt the deﬁnitions of the general aﬃne group and general
linear group on Fq for any prime q. These groups are denoted by GApm, qq and GLpm, qq,
respectively.
1.3. Generalized Reed-Muller Codes. GRM codes are a generalization of RM codes.
They are obtained by constructing the codes over any ﬁnite ﬁeld Fq, where q is a prime
power. The following is a formal deﬁnition of GRM codes:
Deﬁnition 1.6. [8] Let m and r be positive integers. The GRM code of order r with block
length qm over Fq is deﬁned by
GRMqpm, rsq “
␣
pfpαqqαPFm
q | f P Fqrx1, . . . , xms, degpfq ď r
(
.
The GRM codes have the following property:
Theorem 1.7. [8] For 0 ď r ď mpq ´ 1q, the automorphism group of GRMqpm, rq codes
contains the general aﬃne group GApm, qq under the natural action on V “ Fqm.
Knorr and Willems in [11] give a complete description of the automorphism group of the
p-ary RM codes for any prime p. The automorphism group of the p-ary RM-codes equals
the general aﬃne group GApm, pq.
In [2], Berger and Charpin provide a complete description of the automorphism group of
a GRM code. They show that the automorphism group of GRM codes is the aﬃne linear
group, i.e.,
AutpGRMqpm, rqq “ GApm, qq.
6
SIBEL KURT TOPLU˚, TALHA ARIKAN, PINAR AYDOĞDU, AND OĞUZ YAYLA
1.4. Outline of the Paper. Unless otherwise stated, throughout the article we will work on
the ﬁeld Fq, where q is a prime. Main part of the presented work consists of demonstrating the
set of transformations under the general linear group that leaves the symmetric Reed-Muller
(SRM) codes invariant.
Transformations that remain SRM codes invariant have not been studied yet. Motivated
by this research gap, we have shifted our attention to tackling with this unsolved issue.
In Section 2, we provide some preliminaries about SRM codes. In Section 3, we investigate
transformations that leave SRM codes invariant. We determine all the linear transformations
under which the SRM codes are invariant for the parameters n “ 2 and n “ 3. Also, the
set of these linear transformations forms a subgroup of the general linear group. We notice a
relationship between the transformations that remain the SRM codes invariant for n “ 2 and
those for n “ 3. Therefore, we believe that we can form this group for a generic n, despite
the challenge of determining all transformations. For any given n, we anticipate the group
that leaves the SRM codes invariant, akin to previous cases. This group involving aﬃne
transformations is a subgroup of the general linear group GApn, qq. However, establishing
whether this group forms a complete set leaving the SRM codes invariant remains an open
problem for the future. Therefore, in Section 4, we state a conjecture on this problem.
2. Symmetric Reed-Muller Codes
This section is devoted to some basic properties of Symmetric Reed-Muller (SRM) codes.
These codes were ﬁrst introduced by using bivariate polynomials over Fq in [14]. SRM codes
which exhibit speciﬁc symmetry properties within their codewords may be considered as
subcodes of GRM codes. The same authors generalized SRM codes in another work [15].
Before presenting the formal deﬁnition in [15], we recall some notions.
The set Eqpn, rq Ď Fqrx1, x2, . . . , xns is deﬁned by
Eqpn, rq :“
!
fpx1, x2, . . . , xnq “
ÿ
0ďi1ăi2ă...ăinďq´1
i1`i2`...`inďr
ai1i2...indetpx, iq | ai1i2...in P Fq
)
,
where x “ px1, x2, . . . , xnq, i “ pi1, i2, . . . , inq and
detpx, iq :“

x1i1
x2i1
. . .
xni1
x1i2
x2i2
. . .
xni2
...
...
...
...
x1in
x2in
. . .
xnin

.
Consider the set
∆ “ tpa1, a2, . . . , anq P Fn
q | aj ‰ ai, 1 ď j ă i ď nu
and an equivalence relation „ on ∆ deﬁned as follows:
c „ d ðñ Dσ P Sn such that σpcq “ d, for c, d P Fn
q .
Then deﬁne the set Ωqpnq :“ ∆{ „“ trαs | α P ∆u, where rαs denotes the equivalent class
of α. In order to get rid of dublications, the deﬁnition of SRM codes is given by using this
quotient set as follows:
Deﬁnition 2.1. [15] Let n and r be positive integers, where n denotes the number of variables.
The SRM code of degree r over Fq is deﬁned by
SRMqrn, rs “ tpfpαqqrαsPΩqpnq | f P Eqpn, rqu.
ON A GROUP UNDER WHICH SYMMETRIC REED-MULLER CODES ARE INVARIANT
7
Remark 2.2. According to the deﬁnition, when q ă r, the possible ti1, i2, . . . , inu sequence
may not cover a partition of r.
On the other hand, for the sequence ti1, i2, . . . , inu “
t0, 1, 2, . . ., n ´ 1u the smallest value of r should be
npn´1q
2
. Thus, the deﬁnition of SRM
code is meaningful under the condition q ě r ě npn´1q
2
, where q is chosen to be large enough.
Note that under the condition mentioned in the remark above, when n “ 1, SRMqr1, rs
codes are exactly generalized Reed-Solomon codes with degree parameter r.
When n “ 2 we have,
Eqp2, rq :“
#
ÿ
0ďiăjďq´1
i`jďr
aijpxi
1xj
2 ´ xj
1xi
2q
ˇˇˇˇˇ aij P Fq
+
Ď Fqrx1, x2s.
The evaluation of fpx1, x2q at px1, x2q P F2
q forms as the following matrix
»
——–
fpα0, α0q
fpα0, α1q
. . .
fpα0, αq´1q
fpα1, α0q
fpα1, α1q
. . .
fpα1, αq´1q
...
...
...
...
fpαq´1, α0q
fpαq´1, α1q
. . .
fpαq´1, αq´1q
ﬁ
ﬃﬃﬂ .
Since fpx1, x1q “ 0 and fpx1, x2q “ ´fpx2, x1q, the above matrix is a skew-symmetric matrix.
Thus it is entirely determined by the entries in the strictly upper triangular part. Therefore,
the codeword of bivariate SRM, i.e., when n “ 2, codes are deﬁned as the strictly upper
triangular part of this matrix. Furthermore, SRMqr2, rs codes are of length qpq´1q
2
.
3. Linear Transformations Under Which SRM Codes are Invariant
In this section, we will derive the invaraint groups of SRM for n “ 2 and n “ 3 over
the ﬁeld Fp, where p is any prime number. For n “ 2 and n “ 3, we determine the exact
set formed by transformations that leave SRM codes invariant under a subgroup of the
aﬃne linear group. Diﬀerent methods were employed to determine this set for the values
of n “ 2, 3, separately. The reason for using diﬀerent methods is that one approach may
not be highly suitable for the other. Our main theorems, which identify the group formed
by transformations that leave the SRM code invariant, and the necessary lemmas for these
theorems are provided for each n “ 2, 3.
3.1. The case n “ 2. Recall SRMqr2, rs is an evaluated code family whose evaluation
polynomials come from the set Eqp2, rq,
(3)
Eqp2, rq “
$
’
&
’
%
ÿ
0ďiăjďq´1
i`jďr
aij

x1i
x2i
x1j
x2j

ˇˇˇˇˇ aij P Fq
,
/
.
/
-
.
8
SIBEL KURT TOPLU˚, TALHA ARIKAN, PINAR AYDOĞDU, AND OĞUZ YAYLA
Let fpx1, x2q “

x1i
x2i
x1j
x2j
 P Eqp2, rq and A “
„
a
b
c
d

. Under the TfpAq transformation in
(1), we get
TfpAq “

pax1 ` bx2qi
pcx1 ` dx2qi
pax1 ` bx2qj
pcx1 ` dx2qj

“ pax1 ` bx2qi ¨ pcx1 ` dx2qj ´ pax1 ` bx2qj ¨ pcx1 ` dx2qi.
We investigate the coeﬃcients a, b, c, d P Fq to ﬁnd the transformations that keep the set
Eqp2, rq invariant. For this purpose, we need the following auxiliary lemmas:
Lemma 3.1. Let f P Eqp2, rq and α, β P Fq and t be a positive integer. Then we have
pαx2
1 ` βx1x2 ` αx2
2qt ¨ f P Eqp2, rq.
Proof. Without loss of generality, we shall assume that
f “

xi
1
xi
2
xj
1
xj
2
 “ xi
1xj
2 ´ xj
1xi
2,
where 0 ď i ă j ď pq ´ 1q such i ` j ď r. We use the induction on t to prove the claim.
When t “ 1, then
pαx2
1 ` βx1x2 ` αx2
2q ¨ f “ αpx2
1 ` x2
2qpxi
1xj
2 ´ xj
1xi
2q ` βpx1x2qpxi
1xj
2 ´ xj
1xi
2q
“ αpf1 ` f2q ` βf3,
where
f1 “ xi`2
1
xj
2 ´ xj
1xi`2
2
,
f2 “ xi
1xj`2
2
´ xj`2
1
xi
2
and
f3 “ xi`1
1
xj`1
2
´ xj`1
1
xi`1
2
.
Thus f1, f2, f3 P Eqp2, rq, which implies that pαx2
1 ` βx1x2 ` αx2
2q ¨ f P Eqp2, rq. For the next
step, suppose that the claim holds for t ą 1, i.e. we have
pαx2
1 ` βx1x2 ` αx2
2qt ¨ f P Eqp2, rq.
(4)
Then it is suﬃcient to show that the statement holds for pt ` 1q.
pαx2
1 ` βx1x2 ` αx2
2qt`1 ¨ f “ pαx2
1 ` βx1x2 ` αx2
2qppαx2
1 ` βx1x2 ` αx2
2qt ¨ fq.
(5)
By the equation (4), we have
g “ pαx2
1 ` βx1x2 ` αx2
2qt ¨ f P Eqp2, rq.
Since g P Eqp2, rq, we may consider the equation (5) as follows:
pαx2
1 ` βx1x2 ` αx2
2q ¨ g “ pαx2
1 ` βx1x2 ` αx2
2q ¨
ÿ
0ďiăjďq´1
i`jďr
aijgij,
where aij P Fq and
gij “

xi
1
xi
2
xj
1
xj
2
 “ xi
1xj
2 ´ xj
1xi
2.
By the case of t “ 1, we have all pαx2
1`βx1x2`αx2
2q¨gij P Eqp2, rq. Thus pαx2
1`βx1x2`αx2
2q¨
g P Eqp2, rq. In the same manner, this is generalized for any f P Eqp2, rq, which completes
the proof.
□
ON A GROUP UNDER WHICH SYMMETRIC REED-MULLER CODES ARE INVARIANT
9
Lemma 3.2. Let a, b P Fq and t be a positive integer, then

1
1
pax1 ` bx2qt
pbx1 ` ax2qt
 P Eqp2, rq.
Proof. When t is odd, consider

1
1
pax1 ` bx2qt
pbx1 ` ax2qt
 “ pbx1 ` ax2qt ´ pax1 ` bx2qt
“
tÿ
k“0
ˆt
k
˙ `
pbx1 ` 1qkpax2qt´k ´ pax1qkpbx2qt´k˘
“
tÿ
k“0
ˆt
k
˙ `
bkat´k ´ akbt´k˘
xk
1xt´k
2
“
pt´1q{2
ÿ
k“0
ˆt
k
˙ `
bkat´k ´ akbt´k˘ `
xk
1xt´k
2
´ xt´k
1
xk
2
˘
“
pt´1q{2
ÿ
k“0
ˆt
k
˙ `
bkat´k ´ akbt´k˘

xk
1
xk
2
xt´k
1
xt´k
2
 .
Clearly, the corresponding determinant is an element of Eqp2, rq.
When t is even, similarly we have

1
1
pax1 ` bx2qt
pbx1 ` ax2qt
 “
tÿ
k“0
ˆt
k
˙ `
bkat´k ´ akbt´k˘
xk
1xt´k
2
“
t{2´1
ÿ
k“0
ˆt
k
˙ `
bkat´k ´ akbt´k˘ `
xk
1xt´k
2
´ xt´k
1
xk
2
˘
.
In the above equation when k “ t{2, the coeﬃcient of the term xt{2
1 xt{2
2
is zero. Finally, as
in the odd case, the corresponding determinant is an element of Eqp2, rq. Thus, the proof is
completed.
□
Afterward, we have the following proposition.
Proposition 3.3. Let a, b P Fq and i, j be positive integers such that i ă j, then

pax1 ` bx2qi
pbx1 ` ax2qi
pax1 ` bx2qj
pbx1 ` ax2qj
 P Eqp2, rq.
Proof. The determinant can be written as:

pax1 ` bx2qi
pbx1 ` ax2qi
pax1 ` bx2qj
pbx1 ` ax2qj
 “ pax1 ` bx2qipbx1 ` ax2qi

1
1
pax1 ` bx2qj´i
pbx1 ` ax2qj´i

“
`
abx2
1 ` pa2 ` b2qx1x2 ` abx2
2
˘i

1
1
pax1 ` bx2qt
pbx1 ` ax2qt

where t “ j ´ i. By utilizing Lemmas 3.2 and 3.1, the proof follows.
□
10
SIBEL KURT TOPLU˚, TALHA ARIKAN, PINAR AYDOĞDU, AND OĞUZ YAYLA
The following theorem gives a necessary and suﬃcient condition for SRMqr2, rs to be
invariant under which subgroup of GLp2, qq.
Theorem 3.4. Let M be a set deﬁned as
M “
""„
a
b
b
a

| a, b P Fq, a ‰ ˘b
*
Ă GLp2, qq.
The automorphism group of the SRMqr2, rs, where q ě r ą 2 except q “ r “ 3, code family
contains a subgroup isomorphic to M, i.e. SRMqr2, rs is invariant under the transformations
come from M.
Proof. Let A P GLp2, qq. Then take the transform T:
T :
„
x1
x2

ÞÑ A
„
x1
x2

“
„
a
b
c
d
 „
x1
x2

.
In the set Eqp2, rq, there exist unique polynomials of degree 1 and 2, which are
fpx1, x2q “

1
1
x1
x2
 “ x2 ´ x1
and
gpx1, x2q “

1
1
x2
1
x2
2
 “ x2
2 ´ x2
1,
respectively. If TfpAq and TgpAq are elements of Eqp2, rq, it is easy to see that TfpAq and
TgpAq must be scalar multiples of fpx1, x2q and gpx1, x2q, respectively. In the light of this
fact, we have
TfpAq “ fpax1 ` bx2, cx1 ` dx2q “

1
1
pax1 ` bx2q
pcx1 ` dx2q

“ pd ´ bqx2 ´ pa ´ cqx1
and
TgpAq “ gpax1 ` bx2, cx1 ` dx2q “

1
1
pax1 ` bx2q2
pcx1 ` dx2q2

“ pcx1 ` dx2q2 ´ pax1 ` bx2q2
“ pd2 ´ b2qx2
2 ´ pa2 ´ c2qx2
1 ` p2cd ´ 2abqx1x2.
From the above, we obtain the following equations
a ´ c “ d ´ b,
pa2 ´ c2q “ pb2 ´ d2q,
p2cd ´ 2abq “ 0.
If we solve the equations above together with the fact ad´bc ‰ 0, then we get a “ d and b “ c.
Combining this with Proposition 3.3, SRMqr2, rs is invariant under the transformations that
come from the set M, which completes the proof.
□
Note that when q “ r “ 3, the set SRM3r2, 3s contains all vectors of length 3 so that
AutpSRM3r2, 3sq “ S3.
In the following subsection, we will focus on the SRMqp3, rq in the same manner.
ON A GROUP UNDER WHICH SYMMETRIC REED-MULLER CODES ARE INVARIANT
11
3.2. The case n “ 3. Recall that the code family SRMqr3, rs with the length řq´2
i“1
piqpi`1q
2
is an evaluated code family whose evaluation polynomials come from the set Eqp3, rq,
(6)
Eqp3, rq “
$
’
&
’
%
ÿ
0ďiăjăkďq´1
i`j`kďr
aijk

x1i
x2i
x3i
x1j
x2j
x3j
x1k
x2k
x3k

ˇˇˇˇˇ aijk P Fq
,
/
.
/
-
.
Under the TfpAq transformation in (1), where
A “
»
–
a
b
c
d
e
f
g
h
i
ﬁ
ﬂ ,
we investigate the coeﬃcients a, b, c, d, e, f, g, h, i P Fq to ﬁnd that keep the set Eqp3, rq
invariant. We require the auxiliary lemmas for this aim.
The following lemma gives us a diﬀerent interpretation of the set Eqp3, rq.
Lemma 3.5. Let fpx1, x2, x3q P Fqrx1, x2, x3s with a degree less than or equal to r, where q
is odd, such that for any π P S3,
(7)
fpxπp1q, xπp2q, xπp3qq “
#
´fpx1, x2, x3q,
π is an odd permutation,
fpx1, x2, x3q,
π is an even permutation.
Then fpx1, x2, x3q P Eqp3, rq.
Proof. Firstly, we may assume that we have a homogeneous nonzero multivariate polynomial
fpx1, x2, x3q of degree t ď r, with the property (7).
Since f is nonzero, we have a monomial term A0xi
1xj
2xk
3 in f. Without loss of generality,
we may choose the powers 0 ď i ď j ď k, where i, j, k are integers such that i ` j ` k “ t.
So we write f as follows
(8)
fpx1, x2, x3q “ A0xi
1xj
2xk
3 ` g0px1, x2, x3q
where A0 P Fq and g0px1, x2, x3q is a homogeneous polynomial of degree t such that the
coeﬃcient of the xi
1xj
2xk
3 in g0px1, x2, x3q is zero.
Consider the case i “ j. Then by the property (7), we get
fpx2, x1, x3q “ A0xi
2xi
1xk
3 ` g0px2, x1, x3q
“ ´fpx1, x2, x3q “ ´A0xi
1xi
2xk
3 ´ g0px1, x2, x3q.
Equivalently, we have
2A0xi
2xi
1xk
3 ` g0px2, x1, x3q ` g0px1, x2, x3q “ 0.
Since the coeﬃcient of the monomial xi
1xi
2xk
3 in g0px1, x2, x3q is zero, xi
2xi
1xk
3 must be a mono-
mial term of g0px2, x1, x3q, whose coeﬃcient is ´2A0. This is the contradiction. Similarly, we
get a contradiction for the cases i “ k and j “ k. Thus there is not a monomial term xi
1xj
2xk
3
such that at least two of i, j and k values are the same.
12
SIBEL KURT TOPLU˚, TALHA ARIKAN, PINAR AYDOĞDU, AND OĞUZ YAYLA
By the cases mentioned above, we may assume that in the monomial term xi
1xj
2xk
3, i, j, k
are distinct, i.e. 0 ď i ă j ă k. Consider the relation (8). When π “ p12q, we have
fpx2, x1, x3q “ A0xi
2xj
1xk
3 ` g0px2, x1, x3q
“ ´fpx1, x2, x3q “ ´A0xi
1xj
2xk
3 ´ g0px1, x2, x3q.
By the above equation, the monomial term ´A0xi
2xj
1xk
3 must be in g0px1, x2, x3q. Thus the
relation (8) rewrite as
fpx1, x2, x3q “ A0xi
1xj
2xk
3 ´ A0xj
1xi
2xk
3 ` g1px1, x2, x3q.
Applying similar steps for the permutations π “ p13q and π “ p23q, we get
fpx1, x2, x3q “ A0xi
1xj
2xk
3 ´ A0xj
1xi
2xk
3 ´ A0xk
1xj
2xi
3 ´ A0xi
1xk
2xj
3 ` g3px1, x2, x3q
When applying the permutation π “ p123q to fpx1, x2, x3q in above relation, we get
fpx2, x3, x1q “ A0xi
2xj
3xk
1 ´ A0xj
1xi
2xk
3 ´ A0xk
1xj
2xi
3 ´ A0xi
1xk
2xj
3 ` g3px2, x3, x1q
“ fpx1, x2, x3q “ A0xi
1xj
2xk
3 ´ A0xj
1xi
2xk
3 ´ A0xk
1xj
2xi
3 ´ A0xi
1xk
2xj
3 ` g3px1, x2, x3q,
which implies the monomial term A0xi
2xj
3xk
1 must be in g3px1, x2, x3q. Thus
fpx1, x2, x3q “ A0xi
1xj
2xk
3 ` A0xk
1xi
2xj
3 ´ A0xj
1xi
2xk
3 ´ A0xk
1xj
2xi
3 ´ A0xi
1xk
2xj
3 ` g4px1, x2, x3q.
Finally, applying the permutation π “ p132q, the polynomial fpx1, x2, x3q is of form
A0xi
1xj
2xk
3 ` A0xk
1xi
2xj
3 ` A0xj
1xk
2xi
3 ´ A0xj
1xi
2xk
3 ´ A0xk
1xj
2xi
3 ´ A0xi
1xk
2xj
3 ` g5px1, x2, x3q
“A0

xi
1
xi
2
xi
3
xj
1
xj
2
xj
3
xk
1
xk
2
xk
3

` g5px1, x2, x3q,
where g5px1, x2, x3q is a homogeneous polynomial of degree t such that the coeﬃcients of the
monomials xi
πp1qxj
πp2qxk
πp3q for any π P S3 are zero.
Thereafter, if we apply what we did for fpx1, x2, x3q to g5px1, x2, x3q by following the same
steps for the other possible triple partition, t “ i1 ` j1 ` k1, we get
fpx1, x2, x3q “ A0

xi
1
xi
2
xi
3
xj
1
xj
2
xj
3
xk
1
xk
2
xk
3

` A1

xi1
1
xi1
2
xi1
3
xj1
1
xj1
2
xj1
3
xk1
1
xk1
2
xk1
3

` g6px1, x2, x3q,
where A0, A1 P Fq and g6px1, x2, x3q is a homogeneous polynomial of degree t.
Since the number of the triple partition of t is ﬁnite, we may continue the above procedure
until all possible partitions are over. Finally, the polynomial f is of form
fpx1, x2, x3q “ A0

xi
1
xi
2
xi
3
xj
1
xj
2
xj
3
xk
1
xk
2
xk
3

` A1

xi1
1
xi1
2
xi1
3
xj1
1
xj1
2
xj1
3
xk1
1
xk1
2
xk1
3

` ¨ ¨ ¨ ` Ad

xid
1
xid
2
xid
3
xjd
1
xjd
2
xjd
3
xkd
1
xkd
2
xkd
3

,
where Ai’s in Fq. Thus fpx1, x2, x3q P Eqp3, rq by the deﬁnition.
In general, for any polynomial Fpx1, x2, x3q satisﬁes the condition (7), we may write
Fpx1, x2, x3q as follows
Fpx1, x2, x3q “ f3px1, x2, x3q ` f4px1, x2, x3q ` ¨ ¨ ¨ ` frpx1, x2, x3q,
ON A GROUP UNDER WHICH SYMMETRIC REED-MULLER CODES ARE INVARIANT
13
where fi’s are homogeneous polynomials of degree i for i P t3, 4, . . ., ru. From above for any
i P t3, 4, . . ., ru, fi P Eqp3, rq. Thus Fpx1, x2, x3q P Eqp3, rq, which completes the proof.
□
Let
K “
$
&
%P ˚
»
–
a
b
b
b
a
b
b
b
a
ﬁ
ﬂ ,
ˇˇˇˇˇ P P P3, a, b P Fq, a ‰ b, a ‰ ´2b
,
.
- Ă GLp3, qq.
Lemma 3.6. Let A P K and fpx1, x2, x3q P Eqp3, rq, the function g deﬁned as gpx1, x2, x3q “
TfpAq belongs the set Eqp3, rq.
Proof. Firstly, assume that A “
»
–
b
a
a
a
b
a
a
a
b
ﬁ
ﬂ. Let fpx1, x2, x3q P Eqp3, rq. Under the trans-
formation TpAq in (2), we have variables x1 ÞÑ bx1 ` ax2 ` ax3, x2 ÞÑ ax1 ` bx2 ` ax3 and
x3 ÞÑ ax1 ` ax2 ` bx3.
Let g “ TfpAq:
gpx1, x2, x3q “ TfpAq “ fpbx1 ` ax2 ` ax3, ax1 ` bx2 ` ax3, ax1 ` ax2 ` bx3q
Let π “ p12q P S3.
gpxπp1q, xπp2q, xπp3qq “ gpx2, x1, x3q “ fpbx2 ` ax1 ` ax3, ax2 ` bx1 ` ax3, ax2 ` ax1 ` bx3q
“ ´fpbx1 ` ax2 ` ax3, ax1 ` bx2 ` ax3, ax1 ` ax2 ` bx3q
“ ´gpx1, x2, x3q,
second line comes from the fact fpx1, x2, x3q P Eqp3, rq. Similarly, when π “ p13q or π “ p23q,
we get gpxπp1q, xπp2q, xπp3qq “ ´gpx1, x2, x3q.
On the other hand, when π “ p123q P S3, we have
gpxπp1q, xπp2q, xπp3qq “ gpx2, x3, x1q “ fpbx2 ` ax3 ` ax1, ax2 ` bx3 ` ax1, ax2 ` ax3 ` bx1q
“ ´fpax2 ` bx3 ` ax1, bx2 ` ax3 ` ax1, ax2 ` ax3 ` bx1q
“ fpbx1 ` ax2 ` ax3, ax1 ` bx2 ` ax3, ax1 ` ax2 ` bx3q
“ gpx1, x2, x3q.
Similarly, when π “ p132q we obtain gpxπp1q, xπp2q, xπp3qq “ gpx1, x2, x3q.
Finally from above, we can characterize the multivariate polynomial gpx1, x2, x3q as follows
for any π P S3:
gpxπp1q, xπp2q, xπp3qq “
#
´gpx1, x2, x3q,
π is an odd permutation,
gpx1, x2, x3q,
π is an even permutation.
Thus by Lemma 3.5, g “ TfpAq P Eqp3, rq. In the same manner, this approach can easily be
applied to the other elements of the set K. So the proof is completed.
□
Theorem 3.7. The automorphism group of the SRMqr3, rs, where q ě r ą 3, code family
contains a subgroup isomorphic to K, i.e. SRMqr3, rs is invariant under the transformations
come from K.
14
SIBEL KURT TOPLU˚, TALHA ARIKAN, PINAR AYDOĞDU, AND OĞUZ YAYLA
Proof. By the deﬁnition of Eqp3, rq, the members of degrees 3 and 4 in Eqp3, rq are the sets
P “
""
a012

1
1
1
x1
x2
x3
x2
1
x2
2
x2
3

ˇˇˇˇˇ a012 P Fq
*
and
Q “
""
a013

1
1
1
x1
x2
x3
x3
1
x3
2
x3
3

ˇˇˇˇˇ a013 P Fq
*
,
respectively. Let B P F3ˆ3
q
be an invertible matrix such that B “
»
–
a
b
c
d
e
f
g
h
i
ﬁ
ﬂ. It is clear that
when f P P and g P Q, the following conditions must hold
TfpBq P P
and
TgpBq P Q.
Thus, by the properties of the sets P and Q, the coeﬃcients of the terms
x3
1, x3
2, x3
3, x1x2x3, x4
1, x4
2, x4
3, x1x2x2
3, x1x2
2x3, x2
1x2x3, x2
1x2
2, x2
1x2
3, x2
2x2
3
in the outputs, after applying the corresponding transformations TfpBq and TgpBq must be
zero. Furthermore, in the same outputs, the sum of the coeﬃcients of the terms in the pairs
px1x2
2, x2
1x2q, px1x2
3, x2
1x3q, px2x2
3, x2
2x3q, px1x3
3, x3
1x3q, px1x3
2, x3
1x2q, px2x3
3, x3
2x3q
must individually be zero. So we have 19 diﬀerent equations over Fq. When we solve these
equations for the unknowns a, b, c, d, e, f, g, h, i with the help of the computer algebra system
SageMath, we get the solution set S as
$
&
%
»
–
a
b
b
b
a
b
b
b
a
ﬁ
ﬂ ,
»
–
a
b
b
b
a
b
b
b
a
ﬁ
ﬂ ,
»
–
a
b
b
b
b
a
b
a
b
ﬁ
ﬂ ,
»
–
b
a
b
a
b
b
b
b
a
ﬁ
ﬂ ,
»
–
b
a
b
b
b
a
a
b
b
ﬁ
ﬂ ,
»
–
b
b
a
b
a
b
a
b
b
ﬁ
ﬂ ,
»
–
b
b
a
a
b
b
b
b
a
ﬁ
ﬂ
ˇˇˇˇ a, b P Fq
,
.
- .
For the invertibility for each element of the above set, a, b P Fq satisﬁes the conditions
a ‰ b, ´2b. Thus the solution set will be the set K Ă GLp3, qq. By combining this with
Lemma 3.6, the set K is the maximal set in GLp3, qq such that Eqp3, rq is invariant under
the transformations come from K.
□
We give examples of SRMqr2, rs and SRMqr3, rs for some q, r values, respectively.
Example 3.8. Let q “ 5, n “ 2, r “ 4 and pi1, i2q P tp0, 1q, p0, 2q, p0, 3q, p0, 4q, p1, 2q, p1, 3q,
p1, 4q, p2, 3q, p2, 4q, p3, 4qu. For a matrix
„
a
c
b
d

P
#„
1
0
0
1

,
„
0
1
1
0

,
„
0
2
2
0

,
„
2
0
0
2

,
„
0
3
3
0

,
„
3
0
0
3

,
„
4
0
0
4

,
„
0
4
4
0

,
„
1
2
2
1

,
„
2
1
1
2

,
„
1
3
3
1

,
„
3
1
1
3

,
„
2
4
4
2

,
„
4
2
2
4

,
„
3
4
4
3

,
„
4
3
3
4
+
and α P F5, the following equation holds:
α
”
pxi1
1 xi2
2 q ´ pxi2
1 xi1
2 q
ı
“ pax1 ` bx2qi1pcx1 ` dx2qi2 ´ pax1 ` bx2qi2pcx1 ` dx2qi1
ON A GROUP UNDER WHICH SYMMETRIC REED-MULLER CODES ARE INVARIANT
15
Example 3.9. Let q “ 7, n “ 3 and r “ 5. Then pi1, i2, i3q P tp0, 1, 2q, p0, 1, 3q, p0, 1, 4q, p0, 2, 3qu.
Let
g1px1, x2, x3q “ x1x2
2 ` x2x2
3 ` x2
1x3 ´ x2
1x2 ´ x2
2x3 ´ x1x2
3,
g2px1, x2, x3q “ x1x3
2 ` x2x3
3 ` x3
1x3 ´ x3
1x2 ´ x3
2x3 ´ x1x3
3,
g3px1, x2, x3q “ x1x4
2 ` x2x4
3 ` x4
1x3 ´ x4
1x2 ´ x4
2x3 ´ x1x4
3,
g4px1, x2, x3q “ x2
1x3
2 ` x2
2x3
3 ` x3
1x2
3 ´ x3
1x2
2 ´ x3
2x2
3 ´ x2
1x3
3,
and
A “
$
&
%P
»
–
a
b
b
b
a
b
b
b
a
ﬁ
ﬂ
ˇˇˇˇˇ P P P3, a, b P F7, a ‰ b, a ‰ ´2b
,
.
- .
Then
E7p3, 5q “ ta1g1 ` a2g2 ` a3g3 ` a4g4 | a1, a2, a3, a4 P F7u.
For the matrix K P A and g P E7p3, 5q, the polynomial TgpKq P E7p3, 5q by Theorem
3.7.
Since the SRM7r3, 5s code is a type of polynomial evaluation codes, as in Example
1.4, SRM7r3, 5s is invariant under the corresponding transformations come from the A.
Note that the determinants of matrices in solution set must be non-zero. For example, the
solution set on F5 for n “ 2 does not include the element
„
1
4
4
1

because its determinant is
zero. Additionally, as the parameter n increases, the values of q and r should be adjusted
accordingly.
3.3. The general case. Determining all transformations under GLpn, qq that leaves the
SRM code invariant for a general n is quite challenging. For this, there needs to be a general
method to identify such transformation. Nevertheless, we can predict the solution set that
leaves the SRM code invariant for a general n and is a subgroup of the aﬃne linear group.
However, we have not established that this set may include all possible linear transformations
under which SRM codes are invariant. We leave the task of ﬁnding a generalized method
for this problem for future work.
4. Conclusion
Our work aims at determining the set of aﬃne-invariant transformations.
The linear
automorphism groups of SRM for n “ 2 and n “ 3 over the ﬁeld Fp, where p is any prime
number is proven in this paper. For n “ 2 and n “ 3, we ﬁnd that the exact set generated
by transformations remaining SRM codes invariant is a subgroup of the aﬃne linear group.
For diﬀerent values of n, diﬀerent techniques were used to determine this set. Therefore,
we could not give a general proof for an arbitrary n, and leave it an open problem of the
complete determination of the automorphism group AutpSRMq for any n ą 3. We state our
conjecture below.
Conjecture. Let Jn be the n ˆ n all one matrix, In be the n ˆ n identity matrix and Pn
be the set of permutations of order n.
Let M be a subset of GLpn, qq deﬁned as M “
tPppb ´ aqIn ` aJnq | P P Pn, a, b P Fq, a ‰ b, a ‰ p1 ´ nqbu Ă GLpn, qq. Then, the automor-
phism group of the SRMqrn, rs for q ą r ą npn´1q
2
contains a subgroup isomorphic to M, i.e.,
SRMqrn, rs is invariant under the transformations in M.
16
SIBEL KURT TOPLU˚, TALHA ARIKAN, PINAR AYDOĞDU, AND OĞUZ YAYLA
The proof of the invariance of SRMqrn, rs codes under the transformations, which come
from the set M, may be similarly done to the proof of Lemma 3.6. Notwithstanding, in order
to show that the set M is the complete set in this manner is quite challenging to follow the
same techniques.
Acknowledgments: The ﬁrst author gratefully acknowledges the support she has received
from The Scientiﬁc and Technological Research Council of Turkey (TUBITAK) with Grant
No. 2211-A and The Council of Higher Education (YÖK) 100/2000 program.
References
[1] Emmanuel Abbe, Amir Shpilka, and Min Ye. Reed–muller codes: Theory and algorithms. IEEE Trans-
actions on Information Theory, 67(6):3251–3277, 2020.
[2] Thierry Berger and Pascale Charpin. The automorphism group of generalized reed-muller codes. Discrete
mathematics, 117(1-3):1–17, 1993.
[3] Thierry P Berger. On the automorphism groups of aﬃne-invariant codes. Designs, codes and cryptogra-
phy, 7:215–221, 1996.
[4] Thierry P Berger. Automorphism groups of homogeneous and projective reed-muller codes. IEEE Trans-
actions on Information Theory, 48(5):1035–1045, 2006.
[5] Thierry P Berger and Pascale Charpin. The permutation group of aﬃne-invariant extended cyclic codes.
IEEE transactions on Information theory, 42(6):2194–2209, 1996.
[6] Richard A Brualdi and Geir Dahl. Permutation matrices, their discrete derivatives and extremal prop-
erties. Vietnam Journal of Mathematics, 48(4):719–740, 2020.
[7] Philippe Delsarte. On cyclic codes that are invariant under the general linear group. IEEE Transactions
on Information Theory, 16(6):760–769, 1970.
[8] Philippe Delsarte, Jean-Marie Goethals, and F Jessie Mac Williams. On generalized reedmuller codes
and their relatives. Information and control, 16(5):403–442, 1970.
[9] W. Cary Huﬀman and Vera Pless. Fundamentals of error correcting codes. Cambridge University Press,
2003.
[10] Tadao Kasami, Shu Lin, and W Wesley Peterson. Some results on cyclic codes which are invariant under
the aﬃne group and their applications. Information and Control, 11(5-6):475–496, 1967.
[11] R Knörr and Wolfgang Willems. The automorphism groups of generalized reed-muller codes. Astérisque,
181:182, 1990.
[12] Florence Jessie MacWilliams and Neil James Alexander Sloane. The theory of error correcting codes,
volume 16. Elsevier, 1977.
[13] Vera Pless, Richard A Brualdi, and William Cary Huﬀman. Handbook of coding theory. Elsevier Science
Inc., 1998.
[14] Wei Yan and Sian-Jheng Lin. Symmetric reed–muller codes. IEEE Transactions on Communications,
68(7):3937–3947, 2020.
[15] Wei Yan and Sian-Jheng Lin. Local correctabilities and dual codes of symmetric reed–muller codes. In
2021 IEEE Information Theory Workshop (ITW), pages 1–5. IEEE, 2021.
ON A GROUP UNDER WHICH SYMMETRIC REED-MULLER CODES ARE INVARIANT
17
Hacettepe University, Graduate School of Science and Engineering, Beytepe, Ankara,
Türkiye
Email address: sibel.toplu@tubitak.gov.tr
Hacettepe University, Department of Mathematics,Ankara, Türkiye
Email address: tarikan@hacettepe.edu.tr
Hacettepe University, Department of Mathematics,Ankara, Türkiye
Email address: paydogdu@hacettepe.edu.tr
Middle East Technical University, Institute of Applied Mathematics, 06800, Ankara,
Türkiye
Email address: oguz@metu.edu.tr
","nanReed-Muller codes have been widely studied in coding theory. They offer excellent error-correcting capabilities and have found applications in various areas. Variations and generalizations of Reed-Muller codes have been proposed, including the generalized Reed-Muller codes and the symmetric Reed-Muller codes. Previous research has explored the automorphism groups of Reed-Muller codes and their relationship to the aﬃne linear group. However, determining the exact automorphism group of Reed-Muller codes remains a challenging problem often linked to the classification of simple groups."
"Semantic Change Detection (SCD) plays a pivotal role in many sectors including urban planning and disaster management. Traditional SCD methods primarily rely on comparing image pairs, leading to limitations due to imaging differences and resulting in issues like overlooking minor changes and false alarms. To address these challenges, the MapChange framework has been developed. MapChange utilizes historical maps as temporal-invariant data sources to enhance SCD by mitigating temporal variances. Experimental results on public datasets demonstrate the superior performance of MapChange over existing state-of-the-art SCD methods.","Semantic Change Detection (SCD) is a crucial task in various fields. Conventional methods often compare image pairs, facing challenges due to imaging differences and yielding limited performance. The MapChange framework addresses these issues by incorporating temporal-invariant historical map data into the SCD process. By synergizing high-resolution imagery with historical semantic information, MapChange effectively mitigates temporal variance and improves SCD accuracy.","The MapChange framework consists of a triple network architecture, accommodating image pairs and historical maps as inputs. Specialized image and map encoders extract the temporal and semantic features, respectively. A modal fusion process effectively integrates these features, enhancing change detection performance. Multi-task decoders further refine the semantic change outputs. Empirical results on public datasets showcase the effectiveness of MapChange in comparison to existing SCD methods.","Extensive experiments were conducted on two public datasets to evaluate the MapChange framework. Quantitative and qualitative comparisons with benchmark methods demonstrate the superior performance of MapChange in terms of various metrics, including Kappa, Sek, and IoU. The results highlight the effectiveness of the framework in addressing the challenges associated with temporal variance and minor change detection.","The MapChange framework presents a novel paradigm for SCD by incorporating historical maps as temporal-invariant data sources. The specialized triple network architecture, coupled with modal fusion and multi-task decoders, contributes to the framework's superior performance. Experimental results on public datasets validate the efficacy of MapChange, showcasing its potential to enhance SCD accuracy and reliability.",MapChange: Enhancing Semantic Change Detection with Temporal-Invariant Historical Maps Based on Deep Triplet Network,"Yinhe Liu, Sunan Shi, Zhuo Zheng, Jue Wang, Shiqi Tian, Yanfei Zhong","MAPCHANGE: ENHANCING SEMANTIC CHANGE DETECTION WITH TEMPORAL-
INVARIANT HISTORICAL MAPS BASED ON DEEP TRIPLET NETWORK  
 
Yinhe Liu1, Sunan Shi1, Zhuo Zheng2, Jue Wang1, Shiqi Tian1, Yanfei Zhong1, *, Senior Member, IEEE  
 
1) State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, 
Wuhan University, P. R. China 
2) Department of Computer Science, Stanford University 
 
 
 
ABSTRACT 
Semantic Change Detection (SCD) is recognized as both a 
crucial and challenging task in the field of image analysis. 
Traditional methods for SCD have predominantly relied on 
the comparison of image pairs. However, this approach is 
significantly hindered by substantial imaging differences, 
which arise due to variations in shooting times, atmospheric 
conditions, and angles. Such discrepancies lead to two 
primary issues: the under-detection of minor yet significant 
changes, and the generation of false alarms due to temporal 
variances. These factors often result in unchanged objects 
appearing markedly different in multi-temporal images. In 
response to these challenges, the MapChange framework has 
been developed. This framework introduces a novel paradigm 
that synergizes temporal-invariant historical map data with 
contemporary high-resolution images. By employing this 
combination, the temporal variance inherent in conventional 
image pair comparisons is effectively mitigated. The efficacy 
of the MapChange framework has been empirically validated 
through comprehensive testing on two public datasets. These 
tests have demonstrated the framework's marked superiority 
over existing state-of-the-art SCD methods. 
 
Index Terms— Semantic Change Detection, Triplet 
Network, Multi Modal Fusion, Remote Sensing 
1. INTRODUCTION 
In recent years, a remarkable advancement has been the surge 
in availability of remote sensing images with high spatial-
temporal resolution. This development has facilitated more 
sophisticated analytical techniques, including in change 
detection (CD) tasks, which can be categorized into binary 
CD (BCD) and semantic CD (SCD) [1]. BCD focuses on 
identifying the locations where changes have occurred, 
whereas SCD goes a step further by discerning both the 
locations and the nature of these changes. The relevance of 
SCD spans across various critical sectors, including urban 
planning, environmental monitoring, disaster management, 
and agricultural assessment [2].  
In 
advancing 
SCD 
methodologies, 
the 
Post-
Classification Comparison (PCC) method has been a primary 
approach [2]. This method views SCD as a dual temporal 
image 
classification 
problem, 
circumventing 
the 
classification space's sparsity in directly identifying semantic 
changes. PCC methods build upon single-temporal 
classification approaches by independently classifying each 
image and comparing these classification maps. While the 
simplicity of this method facilitates the integration of 
advanced classification techniques, it has limitations, 
particularly in neglecting the temporal relationship between 
image pairs, leading to frequent false alarms, especially near 
object edges. To overcome these challenges, the multi-task 
framework utilizing image pairs as inputs has been 
developed [3]. This framework not only includes a 
classification head for each image but also introduces a 
binary change detection head to refine semantic change 
outputs. The advent of Siamese Networks for the fusion of 
classification with binary change detection have significantly 
improved the effectiveness of multi-task SCD methods, 
showcasing continual progress in this area. 
As shown in Fig. 1, current SCD methods primarily 
depend on comparison of image pairs. However, this 
approach faces challenges due to significant imaging 
difference between two images affected by different shooting 
time, atmospheric radiation, shooting angle and other factors. 
These disparities often result in two issues: 1) overlooking 
minor but significant changes, leading to a lower recall rate, 
and 2) triggering false alarms due to temporal-variance 
problem, which means unchanged objects may have visually 
prominent changes in multi-temporal images. To solve the 
abovementioned problems, this paper utilizes historical maps 
Fig 1. Illustration of image-comparison- and image-map 
comparison-based change detection methods. 
or vector data which are often available in practical 
applications, to provide a rich source of semantic prior 
information that can greatly enhance SCD effectiveness. The 
semantic information in these historical maps is typically 
temporal-invariant, i.e., the unchanged objects are invariant 
over multi-temporal maps, offering a solid counterbalance to 
the temporal variances in image pairs. By incorporating these 
historical semantic layers, this paper aims to develop a more 
nuanced and accurate SCD approach, mitigating the 
limitations of temporal discrepancies. 
The key contributions of this work are summarized as 
follows: 
1) Introduction of a novel SCD paradigm, MapChange, 
which integrates temporal-invariant historical map data with 
current high-resolution images to reconcile temporal 
discrepancies in traditional image pair comparisons. 
2) Development of a specialized triplet network 
architecture featuring distinct image and map encoders, 
enhancing the integration of multi-modal data sources. 
3) Empirical demonstration of the MapChange 
framework's superiority through experiments on two public 
datasets, evidencing its significant advancement over existing 
state-of-the-art SCD methods. 
2. METHOD 
The MapChange framework presents an approach to 
semantic change detection, incorporating historical maps 
alongside temporal high-resolution images to assist in the 
identification of changes. As shown in Fig. 1, MapChange 
framework comprises a triple network architecture to handle 
the distinct data sources, integrates the information through a 
modal fusion process, and applies specific decoders for the 
tasks of semantic classification and change detection, aiming 
to improve the precision of SCD by leveraging the context 
provided by historical maps. 
 
Fig. 1. Flowchart of the proposed historical map prompted 
semantic change detection framework MapChange. 
2.1 Triple Network 
The MapChange framework introduces a triplet network 
architecture which is distinguished from traditional Siamese 
networks by accommodating an additional input stream for 
historical maps. In this configuration, image pairs 
TI 1
 and 
TI 2
 are processed through a shared-weight image encoder 
( )
f ⋅ , ensuring temporal feature consistency. Concurrently, 
the historical map M  is represented in a one-hot encoded 
format and processed by a dedicated map encoder 
( )
g ⋅  
extract relevant historical features. The encoders can be 
mathematically represented as 
1
( 1
)
T
T
E
= f I
, 
2
2
(
)
T
T
E
= f I
, 
and 
(
)
EM
= g M
, where 
ET1
, 
ET 2
, and 
EM
 denote the 
feature maps of the first temporal (T1) image, the second 
temporal (T2) image, and the historical map corresponding to 
the T1 image, respectively. This architecture is designed to 
exploit the semantic priors offered by historical maps, 
enhancing the network's ability to discern semantic changes 
effectively. 
2.2 Multi-modal feature fusion 
Within the MapChange framework, multi-modal feature 
fusion is a pivotal step for enhance image change feature via 
historical map. Initially, the image features extracted by the 
encoders undergo a temporal difference operation to isolate 
change features, denoted as 
1
2
(
,
)
diff
T
T
C
E
E
= ∇
 where
( )
∇ ⋅
represents the temporal difference function that can involve 
addition, subtraction, or concatenation operations. In our 
approach, we employ a Temporal-Symmetric Transformer 
(TST) ( )
τ ⋅ following the methodology of ChangeMask [3], 
designed to ensure temporal symmetry and maintain 
discriminative properties for the change representation
(
)
TST
diff
C
=τ C
. 
Subsequently, the change features are enhanced by the 
historical map features through fusion. The fusion process 
can be expressed as 
(
,
)
fused
TST
M
C
C
E
= φ
, where 
( )
φ ⋅  
symbolizes the fusion function, which also experimented 
with TST and concatenation operations. Empirical results 
indicated 
that 
a 
straightforward 
concatenation 
[
;
]
fused
TST
M
C
C
E
=
 provided the most effective performance 
for enhancing change features with historical map context, as 
detailed in Experiment Section. This multi-modal fusion 
strategy is instrumental in the MapChange framework, 
enriching the change detection process with valuable 
semantic priors from historical data. 
2.3 Multi-task Decoders 
The MapChange framework employs a multi-task decoder 
design, consisting of two semantic decoders with shared 
weights, denoted as 
Dsem
, for semantic segmentation tasks, 
and an independent change decoder 
Dchg
 for Binary Change 
Detection (BCD). At the inference stage, despite the semantic 
decoders' ability to produce classification maps for T1, the 
pre-existing T1 map is utilized to enhance the semantic 
change detection outcome. An end-to-end training scheme is 
adopted, with a multi-task loss function L , .where 
1
tLcls
 and 
2
tLcls
 are the cross-entropy losses supervising the semantic 
segmentation, and 
Lbcd
 is a hybrid of dice loss 
Ldice
 and 
binary cross-entropy loss 
Lbce
 for BCD. The combined loss 
function is formulated as: 
 
1
2
cls
cls
bcd
t
t
L
L
L
L
=
+
+
 
(1) 
3. EXPERIMENT 
3.1 Experiment Setup 
3.1.1 Datasets 
In this study, two key datasets were employed: Hi-UCD [4] 
and HRSCD [1]. Both datasets offer unique challenges and 
are integral to evaluating the efficacy of semantic change 
detection algorithms. 
The HRSCD dataset contains 291 pairs of aerial images 
(10000×10000 pixels, 50 cm/pixel resolution), captured in 
two phases (2005/2006 and 2012), and is annotated for both 
change and terrain. To manage the dataset's volume, only 
patches within change areas were selected, with a division of 
70% for training and 30% for testing. 
The Hi-UCD dataset, focusing on urban changes in 
Estonia, includes 745 pairs of images (0.1 m resolution, 
1024×1024 pixels), with nine land-cover classes and a 
potential of 82 land-cover change types. The dataset split 
comprises 300 training pairs, 59 validation pairs, and 386 
testing pairs, ensuring spatial independence across these sets. 
3.1.2 Implementation details and evaluation metrics 
In the experimental implementation, the model was 
trained using a batch size of 16. The optimization was 
conducted using Stochastic Gradient Descent (SGD), 
configured with a momentum of 0.9 and a weight decay of 
0.0001. The learning rate followed a polynomial decay 
schedule, starting from a base learning rate of 0.03. The 
training iterations were set to 15,000 for the HiUCD dataset 
and 60,000 for the HRSCD dataset. All experiments were 
conducted on a computational setup equipped with two 
NVIDIA TITAN RTX graphics cards.  
In evaluating the performance of semantic change 
detection (SCD) models in this study, a comprehensive set of 
metrics was employed. These include Overall Accuracy (OA), 
which measures the proportion of correctly classified pixels; 
Intersection over Union (IoU), assessing the overlap between 
predicted and ground truth areas; the F1 score, balancing 
precision and recall. The Kappa coefficient evaluates overall 
agreement, accounting for chance agreement. Additionally, 
the Separated Kappa (SeK) coefficient was utilized to address 
the class-imbalance issue, offering a more stringent 
assessment than the traditional Kappa coefficient[5]. 
3.1.3 Benchmark methods: 
A range of deep learning-based methods from recent 
literature were selected for comparative evaluation with our 
proposed model. All methods underwent identical training 
protocols, ensuring an equitable and objective comparison. 
PCC: the traditional method comparing classification 
results of two temporal images to detect semantic changes. 
HRSCD [1]: The HRSCD dataset employs four SCD 
strategies: 1) PCC, using a UNet-like encoder-decoder FCN 
for change detection; 2) Directed classification, treating SCD 
as multi-temporal classification with bitemporal images; 3) 
Separated BCD and classification, combining binary change 
maps with classification maps for SCD; 4) Integrated BCD 
and 
classification, 
fusing 
bitemporal 
features 
from 
segmentation and BCD encoders to enhance SCD accuracy. 
ChangeMask 
[3]: 
A deep multi-task 
encoder-
transformer-decoder architecture designed for efficient and 
robust semantic change detection. 
SSESN [6]: SSESN enhances semantic change detection 
by aggregating spatial and semantic information and 
employing a change-aware module for refined feature 
analysis and region categorization. 
Bi-SRNet [7]: Bi-SRNet integrates semantic temporal 
features with reasoning blocks, enhancing segmentation of 
semantic categories and change detection accuracy. 
3.2 Analysis of semantic change detection results 
3.2.1 Benchmark methods 
The qualitative results of different SCD methods on the 
Hi-UCD dataset is shown in Fig. 2. The PCC Deeplabv3+ 
method tends to over-segment the changed areas, as 
evidenced by the spread of green pixels beyond the ground 
truth. HRSCD_str4 shows a more conservative change 
detection, yet with some missed detections. SSESN offers a 
tighter alignment with the ground truth but still omits some 
minor 
changes. 
ChangeMask 
provides 
a 
closer 
approximation to the ground truth, with fewer omissions and 
misclassifications. MapChange demonstrates a superior 
delineation of changed areas, with a discernible reduction in 
false positives and negatives, closely matching the ground 
truth and displaying enhanced detection of complex change 
patterns. 
 
Fig. 2. Visualization results on the Hi-UCD dataset. 
Table I and Table II Shows the quantitative assessment 
on the Hi-UCD and HRSCD datasets. Traditional PCC 
methods, utilizing various decoders like DeepLabv3+ and 
PSPNet, generally show lower SeK scores, indicating 
weakness of PCC-based methods. On the HRSCD dataset, 
PCC methods yield negative Kappa values, suggesting 
serious misclassification relative to the ground truth. 
Table I. Quantitative results for the Hi-UCD dataset. 
Model 
Decoder 
Kappa(%) Sek(%) IoU(%) F1(%) OA(%) 
PCC 
Deeplabv3+ 
17.83 
8.99 
31.52 47.93 79.30 
PCC 
U-Net++ 
24.16 
12.81 36.53 53.52 82.95 
PCC 
U-Net 
23.12 
12.13 35.49 52.39 82.76 
PCC 
FPN 
19.90 
10.13 32.45 49.00 79.87 
PCC 
PSPNet 
9.36 
4.34 
23.06 37.48 68.94 
hrscd_str1[1] 
U-Net 
17.47 
9.45 
38.52 55.61 84.66 
hrscd_str2[1] 
U-Net 
-0.76 
-0.31 
9.48 17.31 19.99 
hrscd_str3[1] 
U-Net 
21.51 
12.75 47.65 64.55 90.34 
hrscd_str4[1] 
U-Net 
26.44 
16.57 53.28 69.52 91.95 
SSESN[6] 
SSFA+CA[6] 
16.04 
10.02 52.95 69.23 90.62 
Bi-SRNet[7] 
SR[7] 
14.76 
9.00 
50.51 67.12 90.80 
ChangeMask[3] 
U-Net 
33.39 
22.46 60.35 75.27 92.63 
MapChange 
U-Net 
46.15 
32.10 63.71 77.83 94.27 
Advanced methods like ChangeMask and Bi-SRNet 
show improved metrics, with ChangeMask demonstrating 
respectable performance, particularly in IoU and F1 scores on 
the Hi-UCD dataset. However, MapChange consistently 
leads, suggesting that its approach to integrating temporal and 
semantic information effectively addresses SCD's inherent 
challenges. The trend across datasets indicates that methods 
leveraging deep feature integration and sophisticated 
architectural designs, such as MapChange, can better manage 
the complexity of high-resolution SCD tasks. These findings 
highlight a shift towards more integrated and contextually 
aware SCD models to improve detection accuracy and 
reliability. 
Table II. Quantitative results for the HRSCD dataset. 
Model 
Decoder 
Kappa(%) Sek(%) IoU(%) F1(%) OA(%) 
PCC 
Deeplabv3+ 
-5.43 
-2.86 
35.90 52.84 79.34 
PCC 
U-Net++ 
-2.92 
-1.54 
35.90 52.83 80.09 
PCC 
U-Net 
-6.42 
-3.34 
34.68 51.50 79.02 
PCC 
FPN 
-2.64 
-1.39 
36.02 52.96 79.30 
PCC 
PSPNet 
-6.92 
-3.57 
33.61 50.32 78.02 
hrscd_str1[1] 
U-Net 
-5.94 
-3.03 
32.83 49.44 79.28 
hrscd_str2[1] 
U-Net 
9.38 
4.20 
19.70 32.92 13.74 
hrscd_str3[1] 
U-Net 
1.40 
0.82 
46.47 63.45 80.59 
hrscd_str4[1] 
U-Net 
-0.57 
-0.32 
44.49 61.58 83.19 
SSESN[6] 
SSFA+CA[6] 
-2.36 
-1.25 
36.70 53.70 81.79 
Bi-SRNet[7] 
SR[7] 
6.39 
3.88 
50.15 66.80 86.12 
ChangeMask[3] 
U-Net 
2.93 
1.63 
41.22 58.38 85.32 
MapChange 
U-Net 
16.30 
10.50 56.05 71.83 89.07 
3.3.2 Comparison between modal fusion operations 
Experimental results for modal fusion operations in the 
MapChange framework are presented in Table II, contrasting 
the effectiveness of various fusion strategies between change 
and 
map 
features. 
While 
the 
Temporal-Symmetric 
Transformer (TST) was originally designed for symmetrical 
image comparison, the adaptation of TST-Add for fusion 
with map features yields suboptimal results. In contrast, 
simpler fusion operations such as concatenation (Cat) and 
addition 
(Add) 
demonstrate 
superior 
performance. 
Concatenation achieves the highest metrics. These findings 
indicate that straightforward operations like concatenation 
and addition are more effective for integrating change and 
map features within the MapChange framework. 
Table III. Comparison between different modal fusion 
operation for the Hi-UCD dataset. 
Method 
Kappa(%) 
Sek(%) 
IoU(%) 
F1(%) 
OA(%) 
TST-Add 
35.85 
21.14 
47.16 
64.10 
93.67 
Add 
46.08 
32.01 
63.56 
77.72 
94.20 
Cat 
46.15 
32.10 
63.71 
77.83 
94.27 
4. CONCLUSION 
This paper addressed existing gaps in semantic change 
detection, notably the limitations of image pair comparison 
methods that struggle with temporal variance and minor 
change detection. Our contribution, the MapChange 
framework, introduce temporal invariant historical map data 
to provide semantic priors, enhancing change detection 
against these challenges. The innovative triplet network 
architecture within MapChange effectively processes multi-
modal inputs, yielding a significant performance uplift. 
Experimental results on the Hi-UCD and HRSCD datasets 
demonstrate MapChange's superior performance. 
REFERENCES 
[1] R. C. Daudt, B. Le Saux, A. Boulch, and Y. Gousseau, 
""Multitask learning for large-scale semantic change detection,"" 
Computer Vision and Image Understanding, vol. 187, p. 102783, 
2019. 
[2] W. Shi, M. Zhang, R. Zhang, S. Chen, and Z. Zhan, ""Change 
Detection Based on Artificial Intelligence: State-of-the-Art and 
Challenges,"" Remote Sensing, vol. 12, no. 10, p. 1688, 2020. 
[Online]. Available: https://www.mdpi.com/2072-4292/12/10/1688. 
[3] Z. Zheng, Y. Zhong, S. Tian, A. Ma, and L. Zhang, 
""ChangeMask: 
Deep 
multi-task 
encoder-transformer-decoder 
architecture for semantic change detection,"" ISPRS Journal of 
Photogrammetry and Remote Sensing, vol. 183, pp. 228-239, 2022. 
[4] S. Tian, A. Ma, Z. Zheng, and Y. Zhong, ""Hi-UCD: A large-
scale dataset for urban semantic change detection in remote sensing 
imagery,"" arXiv preprint arXiv:2011.03247, 2020. 
[5] K. Yang et al., ""Asymmetric siamese networks for semantic 
change detection in aerial images,"" IEEE Trans. Geosci. Remote 
Sensing, vol. 60, pp. 1-18, 2021. 
[6] M. Zhao et al., ""Spatially and Semantically Enhanced Siamese 
Network for Semantic Change Detection in High-Resolution 
Remote Sensing Images,"" IEEE Journal of Selected Topics in 
Applied Earth Observations and Remote Sensing, vol. 15, pp. 2563-
2573, 2022, doi: 10.1109/JSTARS.2022.3159528. 
[7] L. Ding, H. Guo, S. Liu, L. Mou, J. Zhang, and L. Bruzzone, 
""Bi-Temporal Semantic Reasoning for the Semantic Change 
Detection in HR Remote Sensing Images,"" IEEE Trans. Geosci. 
Remote 
Sensing, 
vol. 
60, 
pp. 
1-14, 
2022, 
doi: 
10.1109/TGRS.2022.3154390. 
 
","nanCurrent SCD approaches are predominantly based on image pair comparison, which has limitations due to temporal variance and minor change detection challenges. The MapChange framework proposes a novel paradigm that introduces historical maps as temporal-invariant data sources to enhance SCD. This approach optimizes the integration of multi-modal data and offers a more informed and accurate change detection."
The HARDCORE technique estimates H-field and power loss in ferrite cores using a residual convolutional neural network with physics-informed extensions. The model topology leverages domain knowledge and minimizes size while maximizing accuracy. A Pareto-style trade-off between model size and accuracy shows an optimum at around 1755 parameters and less than 8% relative error for challenging materials with sufficient samples.,"This research aims to develop a material-agnostic residual convolutional neural network (CNN) to estimate steady-state power losses in toroidal ferrite cores, driven by observational data. The approach emphasizes domain-specific knowledge, utilizing a physics-informed residual model layer which reconstructs the bh curve and estimates power losses based on the curve's area.","The HARDCORE approach employs a residual CNN topology with physics-informed extensions, trained from scratch for each material while maintaining the same topology across materials and waveforms. Expert-based feature engineering and information-rich inputs are used to enable a lean model architecture. A scheduled weighting of the h estimation and power loss estimation cost functions is applied during training to prioritize h estimation initially and gradually shift focus to power loss estimation.","The Pareto front illustrates a trade-off between model size and relative error, demonstrating an optimum at around 1755 parameters and less than 8% relative error for challenging materials with sufficient samples. The material performance scales with the amount of training data available.","The HARDCORE approach successfully estimates steady-state power losses in ferrite cores with high accuracy, demonstrating the potential for material-agnostic data-driven modeling in power magnetics. The method offers a standard way of training data-driven models and serves as a foundation for further research on physics-informed machine learning in magnetics.","HARDCORE: H-field and power loss estimation for arbitrary waveforms with residual, dilated convolutional neural networks in ferrite cores","Nikolas Förster, Wilhelm Kirchgässner, Till Piepenbrock, Oliver Schweins, Oliver Wallscheid","1
HARDCORE: H-field and power loss estimation for
arbitrary waveforms with residual, dilated
convolutional neural networks in ferrite cores
Nikolas F¨orster∗, Wilhelm Kirchg¨assner∗, Till Piepenbrock∗, Oliver Schweins∗, Oliver Wallscheid∗
∗Department of Power Electronics and Electrical Drives
Paderborn University, 33095 Paderborn, Germany
Abstract—The MagNet Challenge 2023 calls upon competi-
tors to develop data-driven models for the material-specific,
waveform-agnostic estimation of steady-state power losses in
toroidal ferrite cores. The following HARDCORE (H-field and
power loss estimation for Arbitrary waveforms with Residual, Di-
lated convolutional neural networks in ferrite COREs) approach
shows that a residual convolutional neural network with physics-
informed extensions can serve this task efficiently when trained
on observational data beforehand. One key solution element is an
intermediate model layer which first reconstructs the bh curve
and then estimates the power losses based on the curve’s area
rendering the proposed topology physically interpretable. In ad-
dition, emphasis was placed on expert-based feature engineering
and information-rich inputs in order to enable a lean model
architecture. A model is trained from scratch for each material,
while the topology remains the same. A Pareto-style trade-off
between model size and estimation accuracy is demonstrated,
which yields an optimum at as low as 1755 parameters and
down to below 8 % for the 95-th percentile of the relative error
for the worst-case material with sufficient samples.
Index Terms—Magnetics, machine learning, residual model
I. INTRODUCTION
The MagNet Challenge 2023 is tackled with a material-
agnostic residual convolutional neural network (CNN) topol-
ogy with physics-informed extensions in order to leverage
domain knowledge. Topological design decisions are dictated
by peculiarities found in the data sets and by the overall goal
of maximum estimation accuracy at minimum model sizes.
The topology’s central idea is the calculation of the area
within the bh polygon based on a preceding h sequence
estimate, see Fig. 3. The area within the polygon formed by the
sequences b, h ∈ R1024 can be calculated using the shoelace
formula or surveyor’s area formula [1]. The shoelace method
assigns a trapezoid to each edge of the polgyon as depicted
in Fig. 1. The area of these trapezoids is defined according to
shoelace either with a positive or negative sign, according to
the hysteresis direction. The negative areas compensate for the
parts of positive trapezoids that extend beyond the boundaries
of the polygon. Provided that the polygon is shifted into the
first quadrant by some offsets hos and bos, the power loss in
W m−3 caused by magnetic hysteresis effects can be computed
with the frequency f, M = 1024, and circular padding by
ˆphyst = f · 1
2
M−1
X
i=0
bi(hi−1 − hi+1).
(1)
hos
bos
ˆphyst
b in T
h in A/m
Fig. 1. Visualization of the shoelace formula applied to a bh polygon.
101
103
Emp.
prob.
3C94
78
N30
3E6
3F4
101
103
Emp.
prob.
N87
3C90
N49
N27
77
−5
0
5
Rel. error
in percent
101
103
Emp.
prob.
A
−5
0
5
Rel. error
in percent
B
−5
0
5
Rel. error
in percent
C
−5
0
5
Rel. error
in percent
D
−5
0
5
Rel. error
in percent
E
Fig. 2. Relative error (ˆphyst −p)/p histogram between provided scalar p and
ˆphyst calculated from the likewise provided bh polygon area.
When applying (1) on the given sequences b, h ∈ RM
with M = 1024, it becomes evident that the calculated area
does not equal the provided loss measurements exactly. Fig. 2
shows the discrepancy with respect to the provided scalar loss
p for all materials. The relative error ranges up to over 7 %
for certain materials (e.g. 3F4, N49, D, E). Consequently, if
merely an h-predicting model was to be identified, the lower
bound on the rel. error would be significantly elevated by this
circumstance alone.
Since the power losses calculated from neither the ground
truth bh curve area (assuming ideal knowledge on the h
sequence) nor the estimated area (ˆh reconstructed via a CNN)
do perfectly match the provided loss measurement values
(targets), an additional residual correction mechanism is added
arXiv:2401.11488v1  [eess.SY]  21 Jan 2024
2
feature
engineering #1
bh curve
estimation
bh curve-based
power loss est.
data-driven
loss correction
target
estimate
feature 
engineering #2
input data
Fig. 3. Overview of the physics-inspired HARDCORE modeling toolchain.
to compensate for this. A high-level view on the proposed
residual, physics-inspired modeling toolchain is depicted in
Fig. 3, which is coined the HARDCORE approach (H-field and
power loss estimation for Arbitrary waveforms with Residual,
Dilated convolutional neural networks in ferrite COREs), and
its details are discussed in the following.
II. MODEL DESCRIPTION
A residual CNN with physics-informed extensions is uti-
lized for all materials. Such a CNN is trained for each material
from scratch. Yet, the topology is unaltered across materials,
signal waveforms, or other input data particularities.
A. One-dimensional CNNs for h-estimation
A 1D CNN is the fundamental building block in this
contribution, which consists of multiple trainable kernels or
filters per layer slided over the multi-dimensional input se-
quence in order to produce an activation on the following
layer [2]. These activations denote the convolution (more
precisely, the cross-correlation) between the learnable kernels
and the previous layer’s activation (or input sequence). In this
stateless architecture, circular padding ensures that subsequent
activation maps are of equal size. Circular padding can be
utilized here instead of the common zero-padding as sequences
denote complete periods of the b and h curve during steady
state. Moreover, a kernel does not need to read strictly adjacent
samples in a sequence at each point in time, but might use a
dilated view, where samples with several samples in between
are used. The dilated, temporal CNN update equation for the
i-th filter’s activation a(l)
i [k] at time k and layer l with the
learnable coefficients Wi ∈ RA×κ applied on A previous
layer’s filters, an uneven kernel size of κ ∈ {2x+1 : x ∈ N0},
and the dilation factor δ reads
a(l)
i [k] =
A−1
X
p=0
(κ−1)/2
X
j=−(κ−1)/2
Wi;(p,j) · a(l−1)
p
[k + jδ].
(2)
Since the task at hand does not require causality of CNN
estimates along the time domain (losses are to be estimated
from single b sequences), the sliding operation can be effi-
ciently parallelized, and sequential processing happens merely
along the CNN’s depth. All 1D CNN layers are accompanied
by weight normalization [3]. A conceptual representation of
the 1D CNN for estimating ˆh is visualized in Fig. 6 (left part).
B. Feature engineering
The term feature engineering encompasses all preprocess-
ing, normalization, and derivation of additional features in an
observational data set. The input data contains the frequency
f, the temperature T, the measured losses p as well as the
1024 sample points for the b and h waveforms. Especially the
creation of new features that correlate as much as possible
with the target variable (here, the h curve or the scalar power
loss p) is an important part of most machine learning (ML)
frameworks [4].
1) Normalization: As is typical in neural network training,
all input and target features have to be normalized beforehand.
All scalar and time series features are divided by their max-
imum absolute value that occurs in the material-specific data
set, with the exception of the temperature and the frequency,
which will be divided by 75 ◦C and 150 kHz, respectively,
regardless the material. Moreover, for an accurate h estimate, it
was found to be of paramount importance to normalize each b
and h curve again on a per-profile base in dependence not only
on the ℓ∞ norm of |b|, but also on the maximum absolute b and
h appearing in the entire material-specific data set. The latter
two values are denoted blim and hlim, and can be understood as
material-specific scaling constants. In particular, the per-profile
normalized b and h curves for a certain sample read
bn =
b
maxk |b[k]|,
hn =
h
hlim
·
blim
maxk |b[k]|,
(3)
with hlim = maxi,k |hi[k]|, blim = maxi,k |bi[k]|, and i being
the sample index in the entire material-specific data set. Then,
bn is added to the set of input time series features, and hn is
the target variable for the h estimation task.
Fig. 4. Exemplary samples of the normalized b and h curves.
The bn over hn curves are displayed in Fig. 4, which
underlines how the polygon area becomes roughly unified (no
large area difference between samples). In the following, all
features that get in touch with the model are normalized values
without any further notational indication.
2) Time series features (feature engineering #1): As dis-
cussed in Sec. II-A, 1D CNNs build the core of the imple-
mented model. The inputs to the CNNs are the (per-profile)
normalized magnetic flux density bn and the corresponding
first and second order derivatives (˙bn and ¨bn) as time series. In
a macroscopic measurement circuit context, ˙b corresponds to
3
0.1
0.0
0.1
b in Vs / m²
~magnetic flux
0
200
db/dt in V/m²
~voltage
0
200
400
600
800
1000
datapoints
10
0
10
d2b/dt2 in V/(m²µs)
~voltage slew rate
Fig. 5.
Magnetic flux density examples and their first and second order
derivatives for a sinusoidal, triangular and one unclassified waveform with
a circuit-based interpretation in terms of their proportionality to magnetic
flux, voltage and the voltage slew rate.
the applied magnetizing voltage throughout the measurement
process of the data. Accordingly, ¨b represents the voltage slew
rate during the commutation of the switches in the test setup.
Consequently, the second derivative allows to detect switching
events and to characterize them according to their maximum
slew rate. Fig. 5 shows, that the sinusoidal waveform (green) is
generated without any fast transient switching behaviour, prob-
ably with a linear signal source. The nonsinusoidal examples
show typical switching behaviour with different voltage slew
rates during the single transitions and voltage overshoots as
well as ringing. The second derivative of b informs the ML
model about switching transition events and how fast changes
in time are.
3) Scalar features (feature engineering #2):
Although
sequence-based CNNs take up the main share of the ML model
size, scalar environmental variables also have a considerable
impact on h and p. While the temperature T is passed to the
model unaltered (but normalized), the frequency is presented
by its logarithm ln(f). The sample time 1/f is passed directly
to the model. Furthermore, some b-derived scalar features
are also passed to the model to feed in a priori knowledge.
For example, the peak-to-peak magnetic flux ∆b as well as
the mean absolute time derivative |˙b| are directly fed into
the network. Each waveform is automatically classified into
”sine”, ”triangular”, ”trapezoidal”, and ”other” by consulting
the form and crest factors, as well as some Fourier coefficients.
The waveform classification is presented to the model by one
hot encoding (OHE). A summary of all expert-driven input
features is presented in Tab. I.
C. Residual correction and overall topology
The model topology comprises multiple branches that end
in the scalar power loss estimate ˆp. An overview is sketched
in Fig. 6. Two main branches can be identified: an h-predictor
TABLE I
UTILIZED INPUT FEATURES.
Time series features
Scalar features
mag. flux density
b
temperature
T
per-profile norm.
bn
sample time
1/f
1st derivative
˙bn
log-frequency
ln(f)
2nd derivative
¨bn
peak2peak
∆b
tan-tan-b
tan(0.9 · tan(bn))
log peak2peak
ln(∆b)
mean abs dbdt
|˙b|
log mean abs dbdt
ln(|˙b|)
waveform (OHE)
and a wrapping p-predictor. The h-predictor utilizes both
time series and scalar features with CNNs and multilayer
perceptrons (MLPs), and estimates the full h sequence. The
p-predictor predicts p, on the other hand, and leverages the
predicted magnetic field strength ˆh with the shoelace formula
and a scaling factor. The latter accounts for the losses inex-
plicable by the bh-curve (recall Fig. 2), and is predicted by a
MLP that utilizes the scalar feature set only.
The h-predictor merges time series and scalar feature in-
formation by the broadcasted addition of its MLP output to a
part of the first CNN layer output. This effectively considers
the MLP-transformed scalar features as bias term to the time-
series-based CNN structure.
On the merged feature set, two further 1D CNN layers
follow that end the transformation in a 1024-element sequence.
The per-profile scaled bn sequence from the set of input
time series is element-wise added to this newly obtained
estimation (residual connection). This results in the CNN
model to merely learn the difference between hn and bn [5].
Eventually, this sequence becomes the h estimation ˆhn when
the sequence’s average along the time domain, that is, across
all 1024 elements, is subtracted from each element. This is
a physics-informed intervention in order to ensure a bias-
free h estimate ˆh after denormalization. Note that all such
operations are still end-to-end differentiable with an automatic
differentation framework such as PyTorch [6], [7].
Since the resulting ˆh can only be trained to be as close as
possible to the provided h sequence, which is not leading to
the correct p ground truth (cf. Fig. 2), another MLP is branched
off the scalar input feature set, and denotes the start of the p-
predictor. This MLP inherits two hidden layers and concludes
with a single output neuron. This neuron’s activation, however,
is not ˆp but rather an area scaling factor s ∈ [−1, 1] to be
embedded in the shoelace formula (1) with
ˆp = f ·
4
1D CNN
1024 samples
MLP
MLP
Add
Add
Area
scaling
factor
Time series 
features
Scalar features
Subtract mean along 
time domain
Polygon
area calculation
Fig. 6. The residual 1D CNN topology is shown while applied on time series and scalar features, which also contain engineered features from Tab. I.
a high dielectric constant and non-zero conductivity. Due to
the small thickness of the used toroidal cores and the limited
excitation frequency, eddy current losses are assumed to be of
minor effect.
The physical interpretability of the intermediate estimate
ˆh is a key advantage of the HARDCORE approach: First,
it enables utilizing full h time series simulation frameworks
(e.g., time domain FEM solvers). Secondly, for future designs
of magnetic components with arbitrary shapes it becomes
indispensable to accurately take into account also geometrical
parameters of the core. This is only possible by distinguishing
between the magnetic hysteresis and the (di-)electric losses.
D. Training cost functions
The training process involves two cost functions for a
training data set with size N: First, the h estimation accuracy,
which is assessed with the mean squared error (MSE) as
LMSE,H =
1
NM
N−1
X
n=0
M−1
X
i=0
(ˆhi,n − hi,n)2.
(5)
Second, the power loss estimation accuracy is to be gauged.
Despite the relative error being the competition’s evaluation
metric, the mean squared logarithmic error (MSLE) is selected
LMSLE,P = 1
N
N−1
X
n=0
(ln ˆpn − ln pn)2
(6)
in order to not overemphasize samples with a relatively low
power loss [8]. As LMSLE,P also depends on ˆh through (4), the
question arises, how both cost functions are to be weighted.
In this contribution, a scheduled weighting is applied with
Ltotal = αLMSLE,P + (1 − α)LMSE,H,
(7)
0
1000
2000
3000
4000
5000
Epochs
10
3
10
2
10
1
Training or validation loss
train_h
train_p
val_h
val_p
Fig. 7. Exemplary training and validation loss curve for material A, seed 0
and fold 3.
where α = (β · iepoch)/Kepoch with β ∈ [0, 1] denoting a
hyperparameter scaling factor, Kepoch being the number of
training epochs, and iepoch ∈ {0, 1, . . . Kepoch−1} representing
the current epoch index. The scheduled weighting ensures that
the model focuses on ˆh in the beginning of the training, where
more information is available. Later though, the model shall
draw most of its attention to the power loss estimate, possibly
at the expense of the h estimation accuracy. A training example
for material A is depicted in Fig. 7.
III. HYPERPARAMETERS, PARETO FRONT AND RESULTS
The proposed topology features several degrees of freedom
in form of hyperparameters. An important aspect is the model
size, which is defined by the number of hidden layers and
neurons in each layer. A simple trial-and-error investigation
5
1135
1755
2231
4711
5951
10943
Model size
10
2
10
1
100
Relative loss error
A
1135
1755
2231
4711
5951
10943
Model size
B
1135
1755
2231
4711
5951
10943
Model size
C
1135
1755
2231
4711
5951
10943
Model size
D
1135
1755
2231
4711
5951
10943
Model size
E
Quantile
Average
95th
99th
Fig. 8. Pareto front for the evaluation materials (A, B, C, D and E) showing
model size (amount of parameters) vs. relative error of the power loss
estimation.
2 µs
100
50
0
50
100
b in mT
50
0
50
h in A/m
2 µs
1646 kW/m³
1651 kW/m³
g. truth
estimate
Fig. 9. Exemplary ground truth vs. estimated bh curve comparison.
can provide fast insights into the performance degradation
that comes with fewer model parameters. In Fig. 8, several
particularly selected model topologies are illustrated against
their achieved relative error versus the inherited model size.
The scatter in each quantile is due to different random number
generator seeds and folds during a stratified 4-fold cross-
validation. Topology variations are denoted by the amount
of neurons in certain hidden layers. In addition, the largest
topology has an increased kernel size with κ = 17, and the
smallest topology has the second CNN hidden layer removed
entirely (the green layer in Fig. 6).
A slight degradation gradient is evident as of 5 k parameters
for materials A and C, whereas for the other materials the trend
is visible only when removing the second hidden layer. Over-
all, the material performance scales strictly with the amount
of training data available. Since fewer model parameters are
a critical aspect, the chosen final model has 1755 parameters,
which is at an optimal trade-off point on the Pareto front.
In Tab. II, the model size of a corresponding PyTorch model
file dumped to disk as just-in-time (jit) compilation is reported.
The exemplary bh-curve and h-curve estimation is shown in
Fig. 9. Reported error rates come from the best seed out of five
during a four-fold cross-validation (β = 1, Kepochs = 5000,
Nesterov Adam optimizer). It shows effectively that any ma-
TABLE II
FINAL MODEL DELIVERY OVERVIEW
Relative error
Material
Parameters
Training
Model size
Average
95-th
data
quantile
A
1755
2432
43.13 kB
2.34 %
6.20 %
B
1755
7400
43.13 kB
1.10 %
2.68 %
C
1755
5357
43.13 kB
1.46 %
3.70 %
D
1755
580
43.13 kB
7.03 %
25.76 %
E
1755
2013
43.13 kB
2.51 %
7.10 %
terial can be modeled with the same topology at high accuracy
as long as a critical training data set size is available (which
is not the case for material D, see available training data in
Tab. II). The final model is already a trade-off between model
size and accuracy, such that in case one of the two criteria can
be softened, the other can be further improved.
The final model delivery is trained on all training data sam-
ples (no repetitions with different seeds), and with Kepoch =
10000, δ = 4, κ = 9. This final topology features a CNN
with 12(TanH) → 8(TanH) → 1 (linear) kernels, a MLP with
11(TanH) neurons, and a p-predictor MLP with 8(TanH) →
1(TanH) neurons.
IV. CONCLUSION
A material-agnostic CNN topology for efficient steady-state
power loss estimation in ferrite cores is presented. Since the
topology remains unaltered across materials and waveforms at
a steadily high accuracy, the proposed model can be considered
universally applicable to plenty of materials. As long as
sufficient samples of a material are available (roughly, 2000),
the relative error on the 95-th quantile remains below 8 %.
Thus, the contributed method is proposed to become a standard
way of training data-driven models for power magnetics.
REFERENCES
[1] B. Braden, “The surveyor’s area formula,” The College Mathematics
Journal, vol. 17, no. 4, pp. 326–337, 1986.
[2] A. Krizhevsky, I. Sulskever, and G. E. Hinton, “ImageNet Classification
with Deep Convolutional Neural Networks,” in Advances in Neural
Information and Processing Systems (NIPS), vol. 60, no. 6, 2012, pp.
84–90. [Online]. Available: https://doi.org/10.1145/3065386
[3] T. Salimans and D. P. Kingma, “Weight Normalization: A Simple
Reparameterization to Accelerate Training of Deep Neural Networks,”
ArXiv e-prints: 1602.07868 [cs.LG], 2016. [Online]. Available: http:
//arxiv.org/abs/1602.07868
[4] P. Domingos, “A few useful things to know about machine learning,”
Communications of the ACM, vol. 55, no. 10, p. 78, 2012. [Online].
Available: http://dl.acm.org/citation.cfm?doid=2347736.2347755
[5] S. Bai, J. Z. Kolter, and V. Koltun, “An Empirical Evaluation of Generic
Convolutional and Recurrent Networks for Sequence Modeling,” 2018.
[Online]. Available: http://arxiv.org/abs/1803.01271
[6] A. Paszke, S. Gross et al., “PyTorch: An Imperative Style, High-
Performance Deep Learning Library,” in Advances in Neural Information
Processing Systems 32.
Curran Associates, Inc., 2019, pp. 8024–8035.
[7] A. G. Baydin, B. A. Pearlmutter et al., “Automatic differentiation
in
machine
learning:
A
survey,”
Journal
of
Machine
Learning
Research,
vol.
18,
pp.
1–43,
2018.
[Online].
Available:
https:
//arxiv.org/abs/1502.05767
[8] C. Tofallis, “A better measure of relative prediction accuracy for model
selection and model estimation,” Journal of the Operational Research
Society, vol. 66, pp. 1352–1362, 2015.
","nanPrior research on magnetic hysteresis modeling is reviewed, highlighting the importance of accounting for the complex, material-specific bh curve and the resulting power losses. Existing methods often lack physical interpretability or require extensive training data, motivating the development of a data-efficient and material-agnostic approach."
"Egocentric action recognition relies on multimodal signals to increase action recognition performance. However, in real-life applications, modalities can be missing due to privacy concerns or device failures. Our research examines the impact of incomplete modalities on egocentric action recognition using transformer-based models. We introduce a novel approach named Missing Modality Token (MMT) that addresses challenges in learning from incomplete modalities. Our MMT is learned during training and replaces absent modalities at test time. Extensive experimentation on multiple datasets demonstrates MMT's effectiveness. It considerably reduces performance degradation caused by missing modalities, surpassing unimodal performance even with incomplete test inputs. Our work contributes a thorough analysis, an innovative method, and a benchmark to encourage the creation of robust multimodal systems in practical settings.","Multimodal video understanding has become indispensable for egocentric video analysis, as complementary multisensory signals significantly enhance action recognition and moment localization. However, practical applications often face incomplete modalities due to factors like privacy concerns, efficiency demands, or hardware malfunctions. Addressing this, our study delves into the impact of missing modalities on egocentric action recognition, particularly within transformer-based models. We introduce a novel concept—Missing Modality Token (MMT)—to maintain performance even when modalities are absent, a strategy that proves effective in the Ego4D, Epic-Kitchens, and Epic-Sounds datasets. Our method mitigates the performance loss, reducing it from its original ∼ 30% drop to only ∼ 10% when half of the test set is modal-incomplete. Through extensive experimentation, we demonstrate the adaptability of MMT to different training scenarios and its superiority in handling missing modalities compared to current methods. Our research contributes a comprehensive analysis and an innovative approach, opening avenues for more resilient multimodal systems in real-world settings.","This section details the aspects we consider while addressing the missing modality problem. Namely, the scenarios and evaluation, the multimodal design and fusion, the possible naive solutions to the problem, and our proposed method.","In Table 2, we show the unimodal and multimodal performance for each downstream dataset. As expected from the annotation strategy, video is the dominant modality in Epic-Kitchens and Ego4d-AR, and audio is the one in Epic-Sounds. Therefore, as mentioned in Section 3.1, we train our models to be robust to missing video in Epic-Kitchens and Ego4D-AR and to missing audio in Epic-Sounds.",We explore the missing modality problem in multimodal egocentric datasets. We suggest a simple yet effective method by learning the optimal token representation of the missing modality (MMT). Placing learnable tokens to represent missing inputs provides an easy and intuitive way to train and test with modal-incomplete inputs. We propose 2 ways to learn MMT when training action recognition models and show how their performance brings us closer to robust and effective multimodal systems.,Exploring Missing Modality in Multimodal Egocentric Datasets,"Merey Ramazanova, Alejandro Pardo, Humam Alwassel, Bernard Ghanem","Exploring Missing Modality in Multimodal Egocentric Datasets
Merey Ramazanova1
Alejandro Pardo1
Humam Alwassel2
Bernard Ghanem1
1King Abdullah University of Science and Technology
2Intelmatix
Abstract
Multimodal video understanding is crucial for analyz-
ing egocentric videos, where integrating multiple sensory
signals significantly enhances action recognition and mo-
ment localization.
However, practical applications often
grapple with incomplete modalities due to factors like pri-
vacy concerns, efficiency demands, or hardware malfunc-
tions. Addressing this, our study delves into the impact of
missing modalities on egocentric action recognition, par-
ticularly within transformer-based models. We introduce a
novel concept—Missing Modality Token (MMT)—to main-
tain performance even when modalities are absent, a strat-
egy that proves effective in the Ego4D, Epic-Kitchens, and
Epic-Sounds datasets.
Our method mitigates the perfor-
mance loss, reducing it from its original ∼ 30% drop to
only ∼ 10% when half of the test set is modal-incomplete.
Through extensive experimentation, we demonstrate the
adaptability of MMT to different training scenarios and its
superiority in handling missing modalities compared to cur-
rent methods. Our research contributes a comprehensive
analysis and an innovative approach, opening avenues for
more resilient multimodal systems in real-world settings.
1. Introduction
Multimodal video understanding has been the de facto ap-
proach for analyzing egocentric videos. Recent works have
shown that the complimentary multisensory signals in ego-
centric videos are superior for understanding actions [20–
22, 28, 32] and localizing moments [2, 36, 38].
How-
ever, multimodal systems need to be practical for real-world
applications that could suffer from the incompleteness of
modality inputs due to privacy, efficiency, or simply device
failures [25]. For example, when predicting in real-time
using a wearable device, parts of the recordings might be
scrapped to preserve the privacy of the bystanders/camera
wearer [14]. Furthermore, using all sensors could be expen-
sive for a wearable device, opting for cheaper modalities
such as audio or IMU. Thus, studying the impact of missing
modalities is crucial for realistic performance expectations.
Still, the current effort to study the impact of missing
0
25
50
75
100
Missing rate rtest (%)
20
25
30
35
40
45
50
55
Accuracy (%)
Epic-Sounds
0
25
50
75
100
Missing rate rtest (%)
30
35
40
45
50
55
60
65
Accuracy (%)
Epic-Kitchens
Unimodal
AV-Bottleneck (not adapted)
AV-Bottleneck (adapted)
Figure 1. Most commonly, we train the multimodal models on
modal-complete data. These models fail when testing data has in-
puts with missing modalities (orange plots). Our proposed adapta-
tion to missing modality significantly improves the performance
across datasets (green plots).
When all test inputs are modal-
incomplete (rtest = 100%), we surpass unimodal performance
(blue plot) by 5 points in Epic-Kitchens, and we are on par with
the unimodal accuracy in Epic-Sounds.
modalities in egocentric datasets is limited.
Most meth-
ods assume that all modality inputs are complete at training
and inference time. Recent works have studied the effect
of missing modalities for different tasks varying from rec-
ommendation systems to emotion recognition [24, 31, 33,
35, 39, 41]. Most works related to missing modalities have
studied the problem at test time only [3, 33, 35, 39, 41, 43],
while just a handful studied missing modalities in both
training and testing time [24, 25, 30]. Similar to our setting,
Lee et al. [25] propose a strategy to learn prompts for pre-
trained backbones to deal with missing modalities. How-
ever, they analyze their method for image and text datasets
only; we implement our version for action recognition and
use it as a baseline in Section 4. More recently, Gong et
al. [12] proposed a benchmark for multimodal generaliza-
tion, focusing on few-shot learning recognition while con-
sidering missing modalities. Though the latter work pro-
poses an interesting benchmark that includes a zero-shot
and few-shot setup, no works have diagnosed how recent
transformer-based approaches perform when modalities are
missing for the action recognition setting.
1
arXiv:2401.11470v1  [cs.CV]  21 Jan 2024
In this work, we study the problem of missing modalities
in egocentric action recognition. First, we investigate how
current transformer-based models are affected by incom-
plete modalities at test time. In Figure 1, we observe how
the current state-of-the-art audiovisual recognition model,
Multimodal Bottleneck Transformer (MBT) [32], trained on
modal-complete inputs, suffers from a critical degradation
in performance when the missing modality rate increases.
The advantage of the multimodal backbone (orange) is lost
when the missing modality rate in the test set exceeds ∼
30% (Epic-Sounds) and ∼ 70% (Epic-Kitchens), at which
point the unimodal model (blue) becomes a better alterna-
tive. To address this problem, we propose learning the miss-
ing modality ”template” during training to replace missing
modalities at test time. We call this template the Missing
Modality Token (MMT) and explain how to learn it in Sec-
tion 3.4. Figure 1 (Epic-Kitchens) also shows how our ap-
proach (green) dramatically improves the test accuracy and
stays at least 5% points above the unimodal performance
even when the test-set is fully modal-incomplete.
We verify the effectiveness of our method in 3 egocen-
tric datasets, including Ego4D [14], which has a full cov-
erage of RGB video but less than 70% of the videos have
audio. We extensively analyze our proposed training strate-
gies, showing how to train with MMT under different miss-
ing modality scenarios. Our experiments show that our sim-
ple yet effective approach proposes a strong solution to this
problem. Our contributions are threefold: (1) We present
a thorough study of the missing modality problem in ego-
centric action recognition by considering effectiveness, ef-
ficiency, and the impact of the fusion strategy. (2) We pro-
pose Missing Modality Token (MMT) to deal with missing
modalities at train and test time for egocentric video under-
standing. We propose two training strategies, namely fixed-
replace and random-replace. (3) We extensively study our
method and find that it significantly improves the presented
baselines. Our work brings insights and paves the way to
build multimodal backbones robust to missing modalities.
2. Related Work
Missing Modality Modeling. Addressing missing modal-
ities presents a notable challenge, explored through vari-
ous strategies by researchers from different areas. From
medical applications [1] to sentiment analysis [3], miss-
ing modalities are a long-standing problem in multimodal
understanding. Similarly, Ma et al. [30] and Colombo et
al. [3] investigate missing modalities within a Bayesian
Meta-learning framework. Meanwhile, Tsai et al. [39] and
Zhao et al. [43] address unexpected missing or noisy modal-
ities at test time using generative models to recreate missing
inputs. Neverova et al. [33] focus on multimodal gesture
recognition, employing depth, audio, and video streams,
and updating network parameters based on different modal-
ity combinations. Our approach takes inspiration from [33]
but uses a simpler learning approach that does not require
different modality loss combinations. We learn a Missing
Modality Token (MMT), which serves as a template for
missing modalities during training without requiring com-
plex pipelines using generative models.
Other studies utilizing transformers [40], such as the
work of Parthasarathy et al. [35], explore missing modal-
ities at test time and propose training-time augmentations.
Radevski et al. [37] apply multimodal distillation to en-
hance training for models using only RGB at test time.
Ma et al. [31] develop strategies for optimal fusion lay-
ers and class tokens in the context of missing modali-
ties, focusing on image-text datasets. Our research differs
by demonstrating effectiveness across various fusion lay-
ers (Section 4.5), indicating that optimization of this pa-
rameter is not crucial. Lee et al. [25] proposes to learn
to prompt large multimodal backbones for image and text
classification when modalities are missing at train and test
time.
We adapt their method to our setting and show
that ours is more practical and effective for dealing with
missing modalities in egocentric videos. Lastly, Gong et
al. [12] introduce a benchmark for handling missing modal-
ities within the Ego4D dataset, tailored for few-shot classi-
fication 1. Though the previous approach proposes an inter-
esting benchmark it introduces an extra layer of complex-
ity by including few-shot and zero-shot recognition into the
missing modality problem. Our work proposes to diagnose
and study the problem in a simpler setting to understand
the effect of missing modalities in standard egocentric video
understanding.
Multimodal Egocentric Video Understanding. Egocen-
tric perception faces distinct challenges compared to tra-
ditional video understanding benchmarks such as Activi-
tyNet [7] and Kinetics [19]. The nature of how egocen-
tric datasets are captured means that they usually feature
strongly aligned and synchronized audiovisual signals. Key
benchmarks in this field, including Epic-Kitchens [4], Epic-
Sounds [18], and the more recent and extensive Ego4D [14],
have demonstrated the importance of audiovisual learning
for understanding egocentric videos due to the complemen-
tary nature of the audio and visual modalities [20, 21, 38].
These datasets have facilitated the creation of several audio-
visual backbones tailored for video understanding. Xiao et
al. [42] introduced a CNN-based dual-stream architecture,
utilizing SlowFast networks for the visual component [8]
and a separate stream for audio [22].
With the advent
and adaptability of transformer architectures, several stud-
ies have treated different modalities as input tokens for a
multimodal transformer encoder [10, 23, 26, 27]. However,
self-attention mechanisms can become prohibitively expen-
sive as the number of tokens increases. To address this,
1Code and data are not available
2
Nagrani et al. [32] offered an efficient modality fusion tech-
nique that avoids costly self-attention in their Multimodal-
Bottleneck Transformer (MBT). We build atop MBT and
propose a method to make it robust for missing modalities
at train and test time.
3. Dealing with Missing Modalities
This section details the aspects we consider while address-
ing the missing modality problem.
Namely, the scenar-
ios and evaluation (3.1), the multimodal design and fusion
(3.2), the possible naive solutions to the problem (3.3), and
our proposed method (3.4).
3.1. Problem Statement, Setup, and Evaluation
Given the training and testing multimodal data samples, let
us denote the missing modality rates in each set with rtrain
and rtest, respectively. These rates are computed by divid-
ing the number of modal-incomplete samples by the total
number of samples. Note that we only consider the setting
of one modality being missing in the dataset. Our work dis-
cusses the strategies of training a multimodal model under
two scenarios: training-with-missing, i.e. some samples
in the training set are modal-incomplete (rtrain ̸= 0%), or
training-with-complete i.e. all samples in the training set
all modal-complete (rtrain = 0%).
To observe the trained model’s behavior under different
missing modality severity levels, we create several variants
of the test set by manually removing the modality informa-
tion from the samples until rtest = 100%. For non-adapted
models, we replace missing modality inputs at test time with
dummy (empty) tensors.
Following previous works [30, 31] when experiment-
ing with fully modal-complete datasets (rtrain
= 0%
and rtest
=
0%), we assume the modality with the
best unimodal performance (dominant) to be incomplete at
test-time (e.g., audio for Epic-Sounds).
Unlike previous
works [25, 30, 31], we also validate our adaptation strate-
gies on a dataset with naturally incomplete modalities in
train and test splits. We use two modalities commonly avail-
able in the egocentric video datasets: visual (RGB frames)
and audio. We evaluate classification accuracy on egocen-
tric action recognition datasets.
3.2. Efficient and Effective Multimodal Fusion
We deal with missing modalities while considering the
methods proven to be the most effective for multimodal fu-
sion. Previous transformer-based methods addressing miss-
ing modalities looked mostly at basic methods, such as
early or mid-fusion with cross-modal self-attention, where
all tokens are concatenated at the fusion layer.
This
does not scale well in videos due to the attention mecha-
nism’s quadratic complexity (to the input size) [32]. In-
stead, we use the current state-of-the-art audiovisual fu-
Architecture
Input dummy
tokens
Audio rtest Accuracy
Self-attention
✓
0%
45.3%
50%
32.7%
75%
26.7%
100%
20.5%
Bottleneck
✓
0%
55.5%
50%
34.3%
75%
23.7%
100%
13.6%
Bottleneck
✗
0%
55.5%
50%
38.7%
75%
30.2%
100%
21.7%
Video-only
✗
0% - 100%
41.4%
Table 1. Bottleneck fusion vs. vanilla self-attention with early
fusion in Epic-Sounds dataset. Bottleneck fusion is shown with 2
ways of manipulating missing inputs at test time: passing dummy
tokens or removing the tokens. All shown models fall far behind
the unimodal (video) performance.
sion, MBT [32], which proved to be more efficient and
effective. The bottleneck transformer in the MBT design
allows the model to distill and propagate the most essen-
tial information across modalities where each modality per-
forms self-attention only with a small number of learnable
”bottleneck” tokens. Such design is especially useful for
information-dense (redundant) modalities like video.
Fusion Layer. A key aspect of a multimodal fusion ap-
proach is the design of the fusion layer (Lf). Lf is the
layer at which the cross-modal interactions happen. We ob-
serve the performance of the original bottleneck model with
modal-complete audiovisual inputs and train the model with
different fusion layers. We show the test accuracy for these
models in Figure 6 (marked as Non-Adapted) for the Epic-
Kitchens and Epic-Sounds datasets. We find that the perfor-
mance does not change significantly with different fusion
layers when rtest = 0. However, the fusion layer does make
a difference when inputs are incomplete (e.g., 35% test ac-
curacy with Lf = 0 vs. 42% with Lf = 11 in Epic-Sounds
at rtest = 50%), as shown in Section 4.5. Overall, fusing
earlier is preferred in Epic-Kitchens, but fusing later gives
better results in Epic-Sounds. This outcome is consistent
with the observations from the previous work [31]: the best
fusion strategy is dataset-specific. While this observation
might be intuitive, it is impractical when the model is very
sensitive to the fusion layer, as searching for the best layer
might be computationally expensive. Thus, we analyze the
effect of the fusion layer when modalities are missing and
show the effect of our approach.
3
Training and testing without MMT
...
...
?
?
?
?
Training with MMT
...
...
...
Testing with MMT
...
?
?
Video Token
Audio Token
Missing Modality Token
Tokenize
Random Replace
Figure 2. Learning with MMT explained. Left: Given modal-incomplete data, it is still unclear how to effectively train a multimodal
model and then predict with it. Right: We demonstrate how we perform training and testing with modal-incomplete data by using Missing
Modality Token (MMT). MMT learns the best representation of missing inputs. At training time, we use both modal-incomplete and
modal-complete samples to train MMT. With the latter, we randomly decide for which samples to replace the signal tokens with MMT.
This strategy allows us to effectively represent the missing modality tokens at test time.
3.3. Model’s intrinsic capabilities with missing
modalities
Before jumping to any training adaptation strategy, we first
explore the capability of the models trained without any
specific adaptation to the missing modality.
Bottlenecks Condensing the Information. As mentioned
above, MBT uses a small set of learnable fusion tokens to
exchange information between the modalities. One might
wonder if the ability of the bottleneck to ”condense” the
information already plays some role in dealing with miss-
ing inputs. To investigate this, Table 1 compares training a
vanilla self-attention model with Lf = 0 (row 1) with the
training MBT with the same fusion layer (row 2). Table 1
shows that vanilla self-attention performs poorly in the mul-
timodal setup with complete modalities, losing 10 points,
compared to the bottleneck fusion. Surprisingly, when the
modality inputs are incomplete, vanilla self-attention per-
forms better (20% vs 13.6% with rtest = 100%). This
shows that while the bottlenecks are effective for multi-
modal fusion, they are sensitive to missing modalities and
they must be adapted to address incomplete modalities.
Only Passing Complete Inputs at Test-Time. As an al-
ternative to passing an empty tensor when the modality is
missing, one could suggest simply discarding the tokens
from the missing signal and only feeding the transformer
with the non-missing modality. The previous alternative is
only possible when using transformers, as they can take a
variable number of input tokens. Table 1 shows that this
strategy (row 3) performs better than just passing empty ten-
sors (row 2). Nevertheless, the model fails to reach video-
only performance (row 4), even with a lower missing rate
rtest = 50%, and only reaches 21.7% when rtest = 100%.
Thus, this adaptation is still far from optimal, and we seek
better solutions.
3.4. Our Approach to Dealing with Missing Modal-
ities
We suggest a simple and generic way to deal with missing
modalities. Instead of passing dummy inputs or discarding
the tokens of the missing inputs at test time, we propose
to learn a ”template” for the missing inputs. We introduce a
learnable Missing Modality Token (MMT), which is trained
to represent the tokens of the missing modality by learning
from the tokens of the non-missing modality. As shown in
Figure 2 (left) and explained in the previous sections, it is
unclear how to deal with inputs with missing modalities. To
this end, we introduce the Missing Modality Token (MMT).
Figure 2 (right) illustrates how the MMT replaces the token
of missing modalities. Our solution is parameter efficient
since MMT is shared across samples. However, we still add
positional embeddings to each token to preserve the loca-
tion information of each missing patch. As shown in Fig-
ure 2 the MMT is learned at training time by alternating
complete-input tokens with MMT. We propose two strate-
gies to train with MMT: fixed-replace and random-replace
and discuss how to apply them in training-with-complete
and training-with-missing scenarios. At test time, we re-
place the missing modality inputs with our learned MMT.
Strategy fixed-replace. This strategy assumes that there
is a fixed subset of the training instances with a missing
modality.
MMT tries to learn the best representation of
the missing modality using this fixed subset. This strategy
4
can be straightforwardly applied for training-with-missing
scenario, as rtrain of the training samples are modal-
incomplete. In training-with-complete, we manually create
this fixed subset of modal-incomplete instances by scraping
the modality data in some instances for the whole training
process. Essentially, we convert this scenario to training-
with-missing by making rtrain > 0. We input MMTs to the
transformer to represent the tokens of the scrapped inputs.
Note that when training-with-missing we can still control
rtrain by removing the modality from more instances atop
the ones that are naturally missing. In this strategy, we com-
pletely discard these samples’ modality information during
the whole training.
Strategy random-replace. Instead of dropping the modal-
ity in a fixed subset of training samples, we remove
the modality for the random samples (keeping rtrain un-
changed). For each training sample, with probability p, the
”missing” modality tokens will be replaced with MMT. Our
intuition is that by picking the samples randomly, rather
than using a fixed pool, the model can ”see” the same sam-
ples in both missing and complete modality scenarios. By
doing so, the model can learn a better representation of
the missing inputs. In training-with-missing, there is still
a fixed set of modal-incomplete training samples. We in-
put MMTs for these samples, too. Basically, in this sce-
nario, fixed-replace is given, and we randomly drop from
the modal-complete subset.
Inference. Regardless of the strategy used, we replace the
missing inputs tokens with the learned MMT at test time.
4. Experiments
We present a detailed analysis of our MMT under both
training-with-complete and training-with-missing. For both
scenarios, we explore both strategies fixed-replace and
random-replace to train MMT (Sections 4.3- 4.4). We re-
port classification accuracy under several missing rates rtest
for three egocentric datasets. Additionally, we study the ef-
fect of fusion layers Lf in Section 4.5. Then, we compare
our method vs. a baseline proposed by [25] in Section 4.6.
4.1. Datasets
We use videos from Ego4D [14] for pre-training the MBT
backbone.
Due to privacy and regulations, only 2.5K
of 3.7K video hours have original audio in this dataset.
We trim 450K 10-second modal-complete audiovisual clips
spanning all 2.5K modal-complete hours. For the down-
stream tasks, we use the following egocentric action recog-
nition benchmarks:
Epic-Kitchens-100 [5] has 90K trimmed clips of variable
length, spanning 100 video hours. Each clip is labeled with
a noun + verb pair, which describes the camera-wearer ac-
tion. In total, there are 300 noun and 97 verb classes in the
dataset. We train the model with 2 heads to jointly predict
verb and noun classes. All videos in the dataset have com-
plete visual and audio streams (rtrain = rtest = 0%).
Epic-Sounds [18] spans the same 100 video hours as Epic-
Kitchens but is annotated with sound labels. This dataset
does not follow the noun and verb annotations from Epic-
Kitchens, instead, it has 44 unique class labels. The dataset
is composed of 79K annotated clips.
Ego4D-AR: We use the annotated clips from the Short-
Term Action Anticipation task in Ego4D benchmark [14] to
create an action recognition dataset that we dub as Ego4D-
AR2. Specifically, we use the provided time-to-contact
timestamps to trim the clips and the anticipated actions as
labels. Ego4D-AR contains 142K clips annotated as noun
and verb pairs. Overall, there are 128 noun and 81 verb
classes. We find that the verb classes are highly imbalanced
in this dataset. Therefore, we balance the class weights in
the cross-entropy loss during training. We provide more
details on the dataset in Supplementary. Similarly to Epic-
Kitchens, we use 2 heads to predict the nouns and verbs.
As we didn’t filter the Ego4D videos for this dataset, it has
naturally missing modality. Only 71% of training clips and
73% of test clips have audio (rtrain = 29%, rtest = 27%).
For clarity and due to space constraints, we report the verb
accuracy for Epic-Kitchens, the class accuracy for Epic-
Sounds, and the noun accuracy for Ego4D-AR in this sec-
tion. We report the rest of the metrics for Supplementary.
Dataset
Audio
Video
Audiovisual
Epic-Kitchens
40.0%
63.2%
64.0%
Epic-Sounds
46.5%
41.4%
55.2%
Ego4D-AR
26.3%
34.6%
36.4%
Table 2. The performance of the audio, video, and bottleneck
audiovisual models on each dataset.
We train and evaluate all
models with rtrain = 0. In all datasets, multimodal performance
beats unimodal performance.
In Table 2, we show the unimodal and multimodal per-
formance for each downstream dataset. As expected from
the annotation strategy, video is the dominant modality in
Epic-Kitchens and Ego4d-AR, and audio is the one in Epic-
Sounds. Therefore, as mentioned in Section 3.1, we train
our models to be robust to missing video in Epic-Kitchens
and Ego4D-AR and to missing audio in Epic-Sounds. Since
Epic-Sounds has fewer training samples and has a more bal-
anced unimodal performance of each modality, we use it
more extensively in our analysis.
For training-with-complete, we use Epic-Sounds and
Epic-Kitchens. For training-with-missing, we use Ego4D-
AR and create a modal-incomplete version of Epic-Sounds
by enforcing rtrain > 0.
2Ego4D does not have an action recognition benchmark
5
0
25
50
75
100
Missing rate rtest (%)
35
40
45
50
55
Accuracy (%)
Epic-Sounds
rtrain = 80%
rtrain = 60%
rtrain = 40%
0
25
50
75
100
Missing rate rtest (%)
40
45
50
55
60
65
Accuracy (%)
Epic-Kitchens
rtrain = 75%
rtrain = 50%
rtrain = 25%
27
50
75
100
Missing rate rtest (%)
20.0
22.5
25.0
27.5
30.0
32.5
35.0
37.5
40.0
Accuracy (%)
Ego4D-AR
rtrain = 75%
rtrain = 50%
rtrain = 29%
Figure 3. Missing modality rate in training data rtrain vs. accuracy when training with fixed-replace strategy. We show that our
MMT is effective in learning with missing modalities under several rtrain regimes. For Epic-Sounds and Ego4d-AR the higher masking
seems to be more beneficial to learn MMT. However, for Epic-Kitchens lower masking yields better accuracy overall.
4.2. Implementations Details
Pre-training. We use the audiovisual MAEs [9, 11, 15]
protocols and train our own implementation of Audiovi-
sual Bottleneck MAE. We use the trimmed Ego4D clips and
train for 200 epochs. We mask 70% of audio and 90% of
video tokens. We use the same pre-trained model for all
experiments. More details of the decoder used in the pre-
training can be found in the supplementary material.
Architecture. Following [17], we use ViT-Base [6] with
12 transformer layers, 12 attention heads, and embedding
dimension 768 as the encoder for each modality. For the
fusion design, we follow MBT [32] and fix the number of
bottlenecks to B = 4 and the fusion layer to Lf = 8 (except
for the fusion layer ablation).
Inputs. Following [13, 17], we convert an audio waveform
of t seconds to log Mel-filterbank with 128 Mel-frequency
bins, with a Hanning window of 25ms, shifting every 10ms.
The output is a spectrogram of 128×100t. We use 8-second
audio and the patch size of 16×16, resulting in (128×100×
8)/256 = 400 audio tokens. For video, we sample 16 RGB
frames at 8 fps of 224 × 224. Similarly to [17], we tokenize
the frames with 3D convolutions, using the spacetime patch
size of 16 × 16 × 2. Each video input produces (16 × 224 ×
224)/(256 × 2) = 1568 tokens.
Finetuning. We train for 50 epochs in Epic-Kitchens ex-
periments, 20 in Epic-Sounds, and 15 in Ego4D-AR. We
use SpecAugment [34] for audio augmentation and Aug-
mix [16] for video augmentation. We use AdamW [29] op-
timizer with half-cycle cosine learning rate decay.
Making training sets with different fixed rtrain. We be-
gin by taking all training instances that are complete across
all modalities and shuffle them randomly once.
For the
Ego4D-AR dataset, instances that are inherently missing are
placed at the very start of this sequence. Following this, we
establish our desired missing rate, represented by rtrain by
sampling from the start of this list. This method ensures that
any increase in rtrain builds upon the existing set of modal-
incomplete instances, meaning that if we increase rtrain,
we add new modal-incomplete instances to those already
included, thereby maintaining a cumulative effect.
4.3. The effect of the number of modal-incomplete
samples rtrain in fixed-replace.
We analyze the effect of rtrain, the proportion of the
modal-incomplete inputs in the training data, under the
fixed-replace strategy in each dataset. For Epic-Kitchens
we use rtrain
∈
{25%, 50%, 75%}, and rtrain
∈
{29%, 50%, 75%} for Ego4D-AR. Recall that Ego4D-AR
has a natural rtrain = 29%, and we can’t reduce this
value. We observed that higher values of rtrain yield bet-
ter performance in Epic-Sounds; hence, we use rtrain ∈
{40%, 60%, 80%} in this dataset.
Figure 3 shows the results for each dataset.
We can
see how Epic-Sounds and Ego4D-AR follow a similar
trend: with the lowest rtrain in each dataset, the model
does not adapt well to the missing inputs. For example,
at rtest = 100% in Ego4D-AR, the model trained with
rtrain = 50% is 8% points more accurate than the model
trained with rtrain = 29%.
Furthermore, training with
higher rtrain = 50% and rtrain = 75% still yields reason-
able accuracy at lower rtest. The opposite is true for Epic-
Kitchens, where not only does the model trained with the
lowest rtrain = 25% adapt well to the missing modality, but
the accuracy drops significantly when we train with more
modal-incomplete inputs. Overall, we highlight that: (1)
for all datasets, higher rtrain results in higher accuracy on
the fully modal-incomplete test set (rtest = 100%), empha-
sizing the importance of training MMT with more samples;
(2) surprisingly, while higher rtrain causes more modality
information loss, we can still train accurate models in Epic-
Sounds and Ego4D-AR.
6
0
25
50
75
100
Missing rate rtest (%)
35
40
45
50
55
60
Accuracy (%)
Epic-Sounds
p = 80%
p = 60%
p = 40%
0
25
50
75
100
Missing rate rtest (%)
40
45
50
55
60
65
Accuracy (%)
Epic-Kitchens
p = 75%
p = 50% 
p = 25%
Figure 4. Modality drop probability p vs. accuracy for modal-
complete Epic-Sounds and Epic-Kitchens. In Epic-Sounds, as
p decreases to 40% the model’s performance with missing audio
goes down. While dropping with high p = 80%, the model still
reaches decent performance at all rtest. In Epic-Kitchens, we ob-
serve very similar behavior as in fixed-replace.
4.4. The effect of the Modality Drop Probability p
in random-replace
We analyze the impact of p, the probability with which the
tokens of modal-complete inputs are replaced with MMT.
For easier comparison with the results in 4.3, we use p ∈
{25%, 50%, 75%} for Epic-Kitchens and Ego4D-AR, and
p ∈ {40%, 60%, 80%} for Epic-Sounds.
Training-with-complete Figure 4 shows changes in per-
formance with different p in training-with-complete. We
observe how p exhibits a similar effect in random-replace
as rtrain in fixed-replace.
However, there are some no-
ticeable differences. First, across all p values, the mod-
els trained with random-replace reach the same or higher
accuracy compared to the corresponding models trained
with fixed-replace (Figure 3). This is especially evident in
Epic-Sounds: by switching from fixed-replace to random-
replace, we get an average improvement of 2.3%, 1.6%, and
1.4% points for p = rtrain of 40%, 60%, and 80%, respec-
tively. We attribute the greater improvement with the lower
p = 40% to the ability of the model to ”see” more of the
same samples appear in both missing and complete modal-
ity scenarios, as we outline in 3.4. When p is too high,
fewer samples have a chance of being seen by the model
as modal-complete multiple times during training. Further-
more, while the improvement is the lowest for p = 80%,
this model still performs optimally at rtest = 0%, which is
not the case for the model with rtrain = 80%. Thus, we
find random-replace more effective for training MMT.
Training-with-missing In Figure 5, we show the results of
training with random-replace in training-with-missing sce-
nario. Note how we are using both rtrain and p here. Be-
cause of that, the numbers are hard to compare to other re-
sults directly, but we can still see similar patterns. Con-
sistently with all previous results, the more samples we
replace with MMT during training, the better the model
0
25
50
75
100
Missing rate rtest (%)
35
40
45
50
55
60
Accuracy (%)
Epic-Sounds
rtrain = 50%, p = 60%
rtrain = 25%, p = 60%
27
50
75
100
Missing rate rtest (%)
30
31
32
33
34
35
36
37
38
Accuracy (%)
Ego4d-AR
rtrain = 29%, p = 75%
rtrain = 29%, p = 50% 
rtrain = 29%, p = 25%
Figure 5. Results with the modal-incomplete training data. As
Epic-Sounds does not naturally have missing modality in the train-
ing data, we manually remove the audio from rtrain = 25% = and
rtrain = 50% of samples in the train set. In Ego4D-AR, there is
a naturally missing audio in rtrain = 29% of the samples.
adapts to severely missing modality at test time. Further-
more, dropping the modality of too many inputs (randomly
or fixed) negatively affects the performance. For example,
while the model trained with p = 75% achieves higher
accuracy at rtest = 100% in Ego4D-AR, it seems that it
learns to ignore the audio completely, as the performance
at rtest = 0% drops to the unimodal video performance
in this dataset. Additionally, by distilling the results in Fig-
ures 3 and 5, we observe that using random-replace in Epic-
Sounds with rtrain > 0 is more effective for training with
missing modalities.
4.5. The effect of the fusion layer.
As we mentioned in Sec. 3.2, the fusion layer does af-
fect the performance of the bottleneck model, especially
when test inputs are modal-incomplete. In Fig. 6, we ex-
amine whether training with Modality Masking Training
(MMT) enhances robustness to a missing modality at var-
ious fusion layers and whether models trained with MMT
(adapted) show the same sensitivity to the fusion layer as
those trained without it (non-adapted). As we observe, the
introduction of MMTs makes the model more robust
to the missing modality across all fusion layers in both
Epic-Sounds and Epic-Kitchens.
With extremely severe
rtest = 100%, the adapted models perform with ∼ 45% ac-
curacy in Epic-Kitchens, while the unimodal audio model
achieves 40% in this dataset. Furthermore, these adapted
models do not exhibit similar sensitivity to the fusion layer
as non-adapted.
For example, in Epic-Sounds, the non-
adapted models trained with Lf = 11 exhibit superior per-
formance, which is not the case for the adapted models (at
rtest = 100%, 37% accuracy with Lf = 11 but ∼ 40%
for other Lf). Overall, all adapted models perform consis-
tently well, each providing decent performance in a modal-
incomplete inference. Our approach effectively addresses a
long-standing issue of selecting the appropriate fusion layer
7
0
50
100
Missing rate rtest (%)
10
20
30
40
50
Accuracy (%)
Epic-Sounds
0
50
100
Missing rate rtest (%)
10
20
30
40
50
60
Accuracy (%)
Epic-Kitchens
Lf = 0
Lf = 4
Lf = 8
Lf = 11
Non-Adapted
Adapted
Figure 6. Fusion Layer Lf vs. accuracy in the models trained with no adaptation strategy (Non-Adapted) and trained with MMT
(Adapted). For training with MMT, we use random-drop strategy with p = 60% for Epic-Sounds and p = 25% for Epic-Kitchens. This
strategy makes MBT more robust to missing modalities across all Lf and significantly reduces the negative effect of missing modalities.
when faced with missing modalities [31].
4.6. MMT vs. other methods
In Tables 3a, 3b and 3c, we report the test results using our
fixed-replace and random-replace strategies and the base-
line method for Epic-Sounds, Epic-Kitchens, and Ego4D-
AR, respectively. We also report the accuracy of the uni-
modal model in each dataset. For the baseline, we imple-
ment the multimodal prompts method [25]. It was origi-
nally based on ViLT [23] for image-text classification, so
we implemented our version for action recognition based
on MBT. Note that this method relies on a strong pre-trained
backbone to efficiently finetune it by optimizing a few net-
work parameters. As we deal with audiovisual learning in
egocentric videos, we rely on large-scale egocentric pre-
training using MAEs. As seen in Tables 3a, 3b, 3c, our
strategy outperform the alternative proposed by [25].
We find that across all datasets, our fixed-replace and
random-replace strategies either reach (in Epic-Sounds and
Ego4D-AR) or exceed (in Epic-Kitchens) the unimodal per-
formance in extreme rtest = 100%. Interestingly, in Epic-
Sounds, random-replace also regularizes the training and
increases the rtest = 0% performance (Table 3a) by 1.1%.
5. Conclusion
We explore the missing modality problem in multimodal
egocentric datasets.
We suggest a simple yet effective
method by learning the optimal token representation of the
missing modality (MMT). Placing learnable tokens to rep-
resent missing inputs provides an easy and intuitive way to
train and test with modal-incomplete inputs. We propose 2
rtest
0%
25%
50%
75%
100%
unimodal
41.4
41.4
41.4
41.4
41.4
no adaptation
55.2
45.6
37.1
28.3
19.5
mm prompts [25]
36.7
33.3
30.0
26.1
22.6
fixed-replace(ours)
55.3
51.2
47.6
43.4
40.2
random-replace(ours)
56.3
52.3
48.6
44.6
40.7
(a) Epic-Sounds
rtest
0%
25%
50%
75%
100%
unimodal
40.0
40.0
40.0
40.0
40.0
no adaptation
63.9
55.5
46.8
37.9
29.5
mm prompts [25]
32.5
31.1
30.2
29.2
28.2
fixed-replace (ours)
63.4
58.8
54.1
49.5
44.8
random-replace (ours)
63.4
59.0
54.6
50.0
45.3
(b) Epic-Kitchens
rtest
27%
50%
75%
100%
unimodal
34.6
34.6
34.6
34.6
no adaptation
30.5
25.9
21.1
16.0
mm prompts [25]
18.4
16.4
13.9
11.5
fixed-replace (ours)
36.2
34.9
33.6
32.4
random-replace (ours)
36.7
35.2
33.5
32.0
(c) Ego4D-AR
Table 3. Comparison of our method with baselines on various
datasets. We demonstrate the accuracy of our method vs. the adap-
tation of the method proposed by [25] and unimodal performance
across different missing modality ratios rtest. We show in bold
the best result and underline the runner-up.
ways to learn MMT when training action recognition mod-
els and show how their performance brings us closer to ro-
bust and effective multimodal systems.
8
References
[1] Reza Azad, Nika Khosravi, Mohammad Dehghanmanshadi,
Julien Cohen-Adad, and Dorit Merhof. Medical image seg-
mentation on mri images with missing modalities: a review
(2022). URL: https://arxiv. org/abs/2203.06217, doi, 10. 2
[2] Wayner Barrios, Mattia Soldan, Alberto Mario Ceballos-
Arroyo, Fabian Caba Heilbron, and Bernard Ghanem. Lo-
calizing moments in long video via multimodal guidance. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 13667–13678, 2023. 1
[3] Pierre Colombo, Emile Chapuis, Matthieu Labeau, and
Chlo´e Clavel. Improving multimodal fusion via mutual de-
pendency maximisation. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Processing,
pages 231–245, Online and Punta Cana, Dominican Repub-
lic, 2021. Association for Computational Linguistics. 1, 2
[4] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide
Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al.
Scaling egocentric vision: The epic-kitchens dataset. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 720–736, 2018. 2
[5] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide
Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al.
Rescaling egocentric vision: Collection, pipeline and chal-
lenges for epic-kitchens-100. International Journal of Com-
puter Vision, pages 1–23, 2022. 5
[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020. 6
[7] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and
Juan Carlos Niebles. Activitynet: A large-scale video bench-
mark for human activity understanding.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 961–970, 2015. 2
[8] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and
Kaiming He. Slowfast networks for video recognition. In
Proceedings of the IEEE/CVF international conference on
computer vision, pages 6202–6211, 2019. 2
[9] Christoph Feichtenhofer, Yanghao Li, Kaiming He, et al.
Masked autoencoders as spatiotemporal learners. Advances
in neural information processing systems, 35:35946–35958,
2022. 6
[10] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia
Schmid.
Multi-modal transformer for video retrieval.
In
Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16,
pages 214–229. Springer, 2020. 2
[11] Mariana-Iuliana Georgescu, Eduardo Fonseca, Radu Tudor
Ionescu, Mario Lucic, Cordelia Schmid, and Anurag Arnab.
Audiovisual masked autoencoders.
In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 16144–16154, 2023. 6
[12] Xinyu Gong, Sreyas Mohan, Naina Dhingra, Jean-Charles
Bazin, Yilei Li, Zhangyang Wang, and Rakesh Ranjan.
Mmg-ego4d: Multimodal generalization in egocentric action
recognition. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 6481–
6491, 2023. 1, 2
[13] Yuan Gong, Yu-An Chung, and James Glass. Ast: Audio
spectrogram transformer. arXiv preprint arXiv:2104.01778,
2021. 6
[14] Kristen
Grauman,
Andrew
Westbury,
Eugene
Byrne,
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson
Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:
Around the world in 3,000 hours of egocentric video. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 18995–19012, 2022. 1, 2, 5
[15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 16000–
16009, 2022. 6
[16] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph,
Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A
simple data processing method to improve robustness and
uncertainty. arXiv preprint arXiv:1912.02781, 2019. 6
[17] Po-Yao Huang, Vasu Sharma, Hu Xu, Chaitanya Ryali,
Haoqi Fan, Yanghao Li, Shang-Wen Li, Gargi Ghosh, Ji-
tendra Malik, and Christoph Feichtenhofer. Mavil: Masked
audio-video learners.
arXiv preprint arXiv:2212.08071,
2022. 6
[18] Jaesung Huh, Jacob Chalk, Evangelos Kazakos, Dima
Damen, and Andrew Zisserman. Epic-sounds: A large-scale
dataset of actions that sound. In ICASSP 2023-2023 IEEE
International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 1–5. IEEE, 2023. 2, 5
[19] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-
man action video dataset. arXiv preprint arXiv:1705.06950,
2017. 2
[20] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and
Dima Damen.
Epic-fusion: Audio-visual temporal bind-
ing for egocentric action recognition. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 5492–5501, 2019. 1, 2
[21] Evangelos Kazakos, Jaesung Huh, Arsha Nagrani, Andrew
Zisserman, and Dima Damen. With a little help from my
temporal context: Multimodal egocentric action recognition.
In British Machine Vision Conference (BMVC), 2021. 2
[22] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and
Dima Damen. Slow-fast auditory streams for audio recogni-
tion. In ICASSP 2021-2021 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pages
855–859. IEEE, 2021. 1, 2
[23] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-
and-language transformer without convolution or region su-
pervision. In International Conference on Machine Learn-
ing, pages 5583–5594. PMLR, 2021. 2, 8
9
[24] Hu-Cheng Lee, Chih-Yu Lin, Pin-Chun Hsu, and Winston H
Hsu. Audio feature generation for missing modality problem
in video action recognition. In ICASSP 2019-2019 IEEE In-
ternational Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP), pages 3956–3960. IEEE, 2019. 1
[25] Yi-Lun Lee, Yi-Hsuan Tsai, Wei-Chen Chiu, and Chen-Yu
Lee. Multimodal prompting with missing modalities for vi-
sual recognition. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pages
14943–14952, 2023. 1, 2, 3, 5, 8
[26] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin
Jiang.
Unicoder-vl: A universal encoder for vision and
language by cross-modal pre-training.
In Proceedings of
the AAAI conference on artificial intelligence, pages 11336–
11344, 2020. 2
[27] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,
and Kai-Wei Chang.
Visualbert:
A simple and perfor-
mant baseline for vision and language.
arXiv preprint
arXiv:1908.03557, 2019. 2
[28] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael
Wray, Rui Yan, Eric Z XU, Difei Gao, Rong-Cheng Tu, Wen-
zhe Zhao, Weijie Kong, et al.
Egocentric video-language
pretraining. Advances in Neural Information Processing Sys-
tems, 35:7575–7586, 2022. 1
[29] Ilya Loshchilov and Frank Hutter.
Sgdr:
Stochas-
tic gradient descent with warm restarts.
arXiv preprint
arXiv:1608.03983, 2016. 6
[30] Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov,
Cathy Wu, and Xi Peng. Smil: Multimodal learning with
severely missing modality. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, pages 2302–2310, 2021. 1,
2, 3
[31] Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine,
and Xi Peng. Are multimodal transformers robust to miss-
ing modality? In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 18177–
18186, 2022. 1, 2, 3, 8
[32] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen,
Cordelia Schmid, and Chen Sun. Attention bottlenecks for
multimodal fusion.
Advances in Neural Information Pro-
cessing Systems, 34:14200–14213, 2021. 1, 2, 3, 6
[33] Natalia Neverova, Christian Wolf, Graham Taylor, and Flo-
rian Nebout. Moddrop: adaptive multi-modal gesture recog-
nition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 38(8):1692–1706, 2015. 1, 2
[34] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng
Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaug-
ment: A simple data augmentation method for automatic
speech recognition. arXiv preprint arXiv:1904.08779, 2019.
6
[35] Srinivas Parthasarathy and Shiva Sundaram. Training strate-
gies to handle missing modalities for audio-visual expression
recognition. In Companion Publication of the 2020 Inter-
national Conference on Multimodal Interaction, pages 400–
404, 2020. 1, 2
[36] Shraman
Pramanick,
Yale
Song,
Sayan
Nag,
Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou,
Rama Chellappa,
and Pengchuan Zhang.
Egovlpv2:
Egocentric video-language pre-training with fusion in the
backbone. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 5285–5297, 2023. 1
[37] Gorjan Radevski,
Dusan Grujicic,
Matthew Blaschko,
Marie-Francine Moens, and Tinne Tuytelaars. Multimodal
distillation for egocentric action recognition. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 5213–5224, 2023. 2
[38] Merey Ramazanova, Victor Escorcia, Fabian Caba, Chen
Zhao, and Bernard Ghanem. Owl (observe, watch, listen):
Audiovisual temporal context for localizing actions in ego-
centric videos. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 4879–
4889, 2023. 1, 2
[39] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-
Philippe Morency, and Ruslan Salakhutdinov.
Learn-
ing factorized multimodal representations.
arXiv preprint
arXiv:1806.06176, 2018. 1, 2
[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 2
[41] Cheng Wang, Mathias Niepert, and Hui Li. Lrmm: Learn-
ing to recommend with missing modalities. arXiv preprint
arXiv:1808.06791, 2018. 1
[42] Fanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra Malik,
and Christoph Feichtenhofer. Audiovisual slowfast networks
for video recognition.
arXiv preprint arXiv:2001.08740,
2020. 2
[43] Jinming Zhao, Ruichen Li, and Qin Jin.
Missing modal-
ity imagination network for emotion recognition with un-
certain missing modalities. In Proceedings of the 59th An-
nual Meeting of the Association for Computational Linguis-
tics and the 11th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), pages 2608–
2618, 2021. 1, 2
10
","Addressing missing modalities presents a notable challenge, explored through various strategies by researchers from different areas. From medical applications to sentiment analysis, missing modalities are a long-standing problem in multimodal understanding. Similarly, Ma et al. and Colombo et al. investigate missing modalities within a Bayesian Meta-learning framework. Meanwhile, Tsai et al. and Zhao et al. address unexpected missing or noisy modalities at test time using generative models to recreate missing inputs. Neverova et al. focus on multimodal gesture recognition, employing depth, audio, and video streams, and updating network parameters based on different modality combinations. Our approach takes inspiration from, but uses a simpler learning approach that does not require complex pipelines using generative models. We learn a Missing Modality Token (MMT), which serves as a template for missing modalities during training without requiring complex pipelines using generative models.nan"
"LLMs (Large Language Models) often tend to provide redundant calculations and reasoning in math word problems. This research paper presents GSM8K-Zero, a manually-constructed dataset to explore this problem. GSM8K-Zero consists of trivial math questions with answers explicitly stated within them, allowing for easy evaluation of LLM output redundancy. The study finds that LLMs, including Llama-2 models and Claude-2, often generate lengthy and unnecessary calculations, negatively impacting accuracy. Further experiments reveal that LLMs struggle to distinguish between simple and complex questions, leading to excessive CoT (Chain-of-Thought) reasoning, while proxy reward models favor verbose outputs, influencing LLMs' behavior. The study highlights the need for improved training techniques to guide LLMs in understanding when step-by-step reasoning is truly necessary.","Large language models (LLMs) have shown remarkable performance in a variety of tasks. Chain-of-Thought (CoT) reasoning further boosts LLMs' performance, but it is unclear whether LLMs know when to use CoT effectively. Prior works primarily focus on the correctness of CoT reasoning or its faithfulness to the question. This work explores the redundancy in LLM outputs. To study redundancy, GSM8K-Zero, a manually constructed math question-answering dataset is introduced. GSM8K-Zero contains trivial questions whose answers are directly stated in the questions, allowing for an easy evaluation of LLM output redundancy.","To study the redundancy in LLM outputs, GSM8K-Zero is created by modifying questions from GSM8K, a math question-answering dataset. The modified questions have answers directly stated in them, making them answerable without any calculations or reasoning. By comparing LLM responses to these questions with their ground truth answers, the redundancy in LLM outputs can be evaluated.","The study finds that LLMs tend to generate redundant calculations and unnecessary CoT reasoning steps. This behavior is observed across various LLM models, including proprietary LLMs like GPT-4 and ChatGPT, and open-source ones like Llama-2. To explore why LLMs generate redundant outputs, proxy reward models (RMs) are used to evaluate the preference of LLMs for long and short answers. The results suggest that proxy RMs strongly favor long outputs, even if they are incorrect, potentially influencing LLMs to generate lengthy and redundant outputs.","This study demonstrates LLMs' tendency to generate redundant calculations and unnecessary CoT reasoning steps, showing that LLMs may not differentiate between questions requiring step-by-step reasoning from simpler ones. Exploring the preference of proxy RMs reveals that they favor lengthy answers, regardless of correctness, which might contribute to the redundancy in LLM outputs. This research highlights the need for future work to focus on the redundancy of LLM outputs and develop training techniques to teach LLMs when to think step-by-step.",Over-Reasoning and Redundant Calculation of Large Language Models,"Cheng-Han Chiang, Hung-yi Lee","Over-Reasoning and Redundant Calculation of Large Language Models
Cheng-Han Chiang
National Taiwan University,
Taiwan
dcml0714@gmail.com
Hung-yi Lee
National Taiwan University,
Taiwan
hungyilee@ntu.edu.tw
Abstract
Large language models (LLMs) can solve
problems step-by-step.
While this chain-
of-thought (CoT) reasoning boosts LLMs’
performance, it is unclear if LLMs know when
to use CoT and whether those CoT are always
necessary to answer the question. This paper
shows that LLMs tend to generate redundant
calculations and reasoning on a manually
constructed math QA dataset, GSM8K-Zero.
GSM8K-Zero is constructed such that the
questions can be answered without any
calculations, but LLMs, including Llama-2
models and Claude-2, tend to generate lengthy
and unnecessary calculations to answer the
questions.
We also conduct experiments
to explain why LLMs generate redundant
calculations and reasonings.
GSM8K-Zero
is publicly available at https://github.
com/d223302/Over-Reasoning-of-LLMs
and
https://huggingface.co/datasets/
dcml0714/GSM8K-Zero.
1
Introduction
LLMs have shown impressive performance on var-
ious tasks (Brown et al., 2020; Ouyang et al., 2022;
Touvron et al., 2023), and CoT reasonings (Wei
et al., 2022; Kojima et al., 2022) further boost
the LLMs’ performance. To make LLMs better
learn to think step-by-step, it is common to en-
hance the CoT ability of LLMs by adding training
samples that contain CoT reasoning in instruction
tuning (Longpre et al., 2023).
While step-by-step reasoning is useful, it length-
ens the LLM’s output. Longer outputs imply longer
inference latency and cost more money on commer-
cial LLMs. Moreover, if LLMs give long answers
to questions that can be easily solved without com-
plex reasoning, this may confuse the users and
make them falsely believe that the questions are
difficult and must be solved step-by-step. It is thus
more desirable that LLM only triggers CoT rea-
soning ability when necessary, but it is unclear if
A deep-sea monster rises from the waters once every hundred years to
feast on a ship and sate its hunger. Over three hundred years, it has
consumed 847 people. Ships have been built larger over time, so each
new ship has twice as many people as the last ship. How many people
were on the ship the monster ate in the first hundred years? How many
people has it consumed over three hundred years?
(a) Question in GSM8K-Zero
To calculate the total number of people the deep-sea monster has
consumed over three hundred years, we can sum up the number of people
on each ship the monster has feasted on. 
The ¦rst ship had 1 person (since it has been a hundred years), the
second ship had 2 people (since the new ship has twice as many people
as the last ship), the third ship had 4 people, and so on.  (...)
The formula is as follows: Sn = a * (rn - 1) / (r - 1)  (...)
Therefore, the deep-sea monster has consumed approximately 2.03 x
1090 people over three hundred years.
(b) ChatGPT output
Figure 1: (a) A question in GSM8K-Zero. The blue
parts are the known information, and the orange part
is the original question in GSM8K and is removed in
GSM8K-Zero. The green part is the new question in
GSM8K-Zero. (b) The answer generated by ChatGPT.
LLMs know when to think step-by-step and only
generate step-by-step reasoning when necessary.
Prior works on CoT mostly focus on the correct-
ness of the CoT reasoning steps or whether the rea-
soning steps are faithful to the question and support
the final answer (Ye and Durrett, 2022; Golovneva
et al., 2023). However, no past study focuses on
the redundancy in CoT reasonings.
As an initiative to study the redundancy of LLM
outputs, we aim to understand the following re-
search question: Does LLM generate redundant rea-
sonings when they clearly need not do so? To study
this question, we construct a math QA dataset,
GSM8K-Zero, which contains trivial questions that
can be answered without any calculations and rea-
soning. Using this curated dataset, we can define
the redundancy of output from LLMs. We evalu-
ate seven LLMs trained with reinforcement learn-
ing with human feedback (RLHF) (Ouyang et al.,
2022), and we find that LLMs tend to generate re-
dundant calculations that complicate the responses
and sometimes lead to the wrong answer. To ex-
arXiv:2401.11467v1  [cs.CL]  21 Jan 2024
plain our observation, we show that GPT-4 (Ope-
nAI, 2023) and ChatGPT (OpenAI, 2022), which
are widely used in gathering the preference data
for training a reward model in RLHF (Guo et al.,
2023; Anand et al., 2023), show a strong prefer-
ence towards long answers that contain redundant
calculations, even if the long answers are incorrect.
Our contributions are summarized as follows:
• To the best of our knowledge, we are the first
to study the redundancy of LLM outputs.
• We construct and release a dataset, GSM8K-
Zero, which reveals the LLMs’ tendency to
generate redundant reasonings.
• We show that LLMs tend to generate redun-
dant calculations on math questions that can
be answered without any calculation.
• We show that LLMs’ tendency to generate
long answers may stem from the imperfect
reward model that prefers longer answers re-
gardless of their correctness.
2
Dataset: GSM8K-Zero
2.1
Construction of GSM8K-Zero
To study LLMs’ tendency for redundant cal-
culations,
we
created
GSM8K-Zero
from
GSM8K (Cobbe et al., 2021).
A question in
GSM8K comprises (1) the known information
(blue parts in Figure 1) and (2) a query for an un-
known quantity (orange parts in Figure 1). Using
questions in GSM8K, we aim to create questions
whose answers are directly stated in the questions
and can be obtained without any calculations.
We use the following procedure to achieve
this goal. The following procedure is best read
with Figure 1(a). Given a question in GSM8K,
we remove the last sentence from the question
that queries for an unknown variable and keep
the known information .
Next, we generate a
question that asks the value of a known variable
(green parts in Figure 1(a)) based on the known
information and append the question behind the
known information . The question is generated
by randomly selecting a number in the known
information as the ground truth answer and using
few-shot prompting to generate a question whose
answer is the selected ground truth using ChatGPT.
We then use GPT-4 to answer the newly generated
question. If GPT-4’s answer deviates from the
ground truth answer, the question is discarded. We
randomly select 3,500 questions from GSM8K’s
training set1 and obtain 2,978 question-answer
pairs after the above procedure.
Based on a manual inspection of 250 random
question-answer pairs by the authors, we esti-
mate that about 85% of question-answer pairs in
GSM8K-Zero are valid. Refer to Appendix B.2 for
a detailed description of our manual inspection of
GSM8K-Zero.
2.2
Evaluating Redundancy
We define redundant outputs as any superfluous
information in LLM responses that are not re-
quired for accurately answering the question.
Measuring this redundancy is often challenging for
existing datasets. However, GSM8K-Zero offers
an easy way to evaluate LLM output’s redundancy
due to its unique nature: questions can be answered
without any calculations since the answers are ex-
plicitly stated within the questions. If an LLM’s
answer includes calculations, it is deemed redun-
dant. We identify mathematical operators (×, +,
and =) in LLM outputs by a regular expression and
say that the LLM’s answer is redundant whenever
mathematical operators are found.
3
Experiments
We test LLMs on GSM8K-Zero in zero-shot, as
zero-shot inference closely mirrors most users’
practical use of LLMs-as-assistants. Instead of
leveraging advanced prompting techniques like
zero-shot CoT (Kojima et al., 2022) or Plan-and-
Solve (Wang et al., 2023), we present a single ques-
tion to the LLM and take its response. For each
question, we sample one response from the LLM.
In our preliminary experiments, we find the obser-
vations in our paper are robust toward the hyperpa-
rameters used for sampling outputs from LLMs.
Our evaluation encompasses proprietary LLMs,
such as GPT-4, ChatGPT, Claude-2 (Anthropic,
2023), and PaLM (text-bison-001) (Anil et al.,
2023), and open-source ones like Llama-2-chat
models of different sizes (Touvron et al., 2023).
We assess LLMs’ performance on GSM8K-Zero
using two metrics: (1) Redundancy: Determined
by the percentage of LLM answers containing nu-
merical operators like ×, +, and =. (2) Accuracy:
Accuracy measures how often the LLM’s answer,
1In our preliminary experiment, we find that our results
also hold when we use the testing set of GSM8K to construct
the questions in GSM8K-Zero
Models
Red.
Accuracy
Avg.
Cal. ✗
Cal. ✓
Proprietary LLMs
GPT-4
11.7
100.0†
100.0†
100.0†
ChatGPT
47.1
79.7
96.6
60.7
Claude-2
74.7
88.4
98.8
84.8
PaLM
29.2
40.9
40.9
40.6
Open-source LLMs (Llama-2)
70b-chat
80.3
54.5
87.7
46.3
13b-chat
88.3
39.9
86.0
33.8
7b-chat
88.6
41.4
80.2
36.3
Table 1: The redundancy (Red.) and accuracy of LLMs’
responses. We report the average accuracy (Avg.) on
all questions (second column), the accuracy for answers
without calculation (Cal. ✗, third column) and with
calculation (Cal. ✓, fourth column). †: The accuracy of
GPT-4 is 100% by construction since we use GPT-4 to
filter samples when constructing GSM8K-Zero.
extracted using a regular expression, aligns with
the GSM8K-Zero ground truth.
3.1
Main Results
We show the LLMs’ performance on GSM8K-Zero
in Table 1. First, we observe almost half of the
LLMs we test have an accuracy lower than 50%
(second column in Table 1). Recall that the answers
to the question in GSM8K-Zero can be easily ex-
tracted from the question without any calculations,
which makes GSM8K-Zero more like an extractive
QA than a math QA. As simple as this dataset is,
some LLMs still cannot perform well on it.
Next, we turn our attention to the redundancy in
the answers. It can be seen that both proprietary
and open-source LLMs generate redundant calcula-
tions and reasoning to answer the questions. Chat-
GPT yields unnecessary calculations in their an-
swers in almost half of the answers, and all Llama-
2 models generate lengthy reasoning steps and re-
dundant calculations in more than 80% of their
responses while they are not explicitly prompted to
do so.
We show some answers with redundancy gener-
ated by different LLMs in Table 3. By inspecting
the outputs from LLMs, we find that in most cases,
LLMs solve all the unknown variables in the ques-
tions, which are not asked in the questions (#2 in
Table 3). This behavior is problematic for a help-
ful assistant since it complicates the responses and
may falsely lead the users to think solving all the
unknown variables is necessary to arrive at the fi-
Model
Redundancy
Accuracy
ChatGPT
25.7
83.6
Claude-2
40.7
88.5
Llama-2-70b-chat
54.4
73.3
Llama-2-13b-chat
45.8
65.5
Llama-2-7b-chat
32.7
68.3
Table 2: The redundancy and accuracy of answers from
LLMs when allowing LLMs not to use CoT.
nal answer. We also find that the LLMs sometimes
only provide the values of the unknown variables
but do not answer the value asked in the question
(#1 in Table 3), showing that LLMs cannot follow
user instructions very well in these cases.
After discussing redundancy and accuracy inde-
pendently, we want to know if redundant calcula-
tion co-occurs more often with wrong answers. We
separate the model outputs into two groups: one
that contains calculations and another that does
not have calculations, and we calculate the accu-
racy for the two groups. The results are shown in
the two rightmost columns in Table 1. When the
LLM’s answers contain calculations, the accuracy
drops significantly for almost all models except for
PaLM. By randomly browsing the wrong answers
that include calculations of models except PaLM,
we find that sometimes LLMs hallucinate variables
not specified in the questions (#4 in Table 3). Some-
times, LLMs make calculation errors, leading to
the wrong answer (#3 in Table 3). This shows that
redundant calculations not only waste time and re-
sources but can also hurt the LLM’s performance
due to calculation errors and incorrect reasoning.
For the outputs of PaLM that contain calcula-
tions, we observe that PaLM often first generates
an Arabic number as the answer, followed by some
calculations as the explanation. In this case, the
numeric answer of PaLM does not depend on the
calculations, so even if the calculation and reason-
ing following the answers are wrong, they cannot
affect the answer. This makes the accuracy of an-
swers with and without calculation similar in the
case of PaLM.
3.2
Do LLMs Know When to Use CoT?
Section 3.1 reveals that LLMs can generate redun-
dant calculations and unnecessary CoT reasoning
steps. This is possibly because, during instruction
tuning, LLMs are trained to generate CoT reason-
ing for mathematical problems when the input in-
struction does not specify how to solve the ques-
ID
Type
Question
LLM Output
#1
LLM solves
the unknown
variables
but does not
answer
the
question
Albert is wondering how much
pizza he can eat in one day. He
buys 2 large pizzas and 2 small
pizzas. A large pizza has 16
slices and a small pizza has 8
slices. How many slices does
a large pizza have?
(Llama-2-70b-chat) If Albert buys 2 large pizzas and 2 small pizzas,
and a large pizza has 16 slices, then we can calculate the total number
of slices of pizza that Albert can eat in one day as follows: 2 x 16
= 32 slices from the large pizzas + 2 x 8 = 16 slices from the small
pizzas = Total slices of pizza that Albert can eat in one day = 48 slices.
Therefore, Albert can eat 48 slices of pizza in one day.
#2
LLM solves
the unknown
variables and
yields the cor-
rect answer
A deep-sea monster rises from
the waters once every hundred
years to feast on a ship and
sate its hunger. Over three hun-
dred years, it has consumed
847 people. Ships have been
built larger over time, so each
new ship has twice as many
people as the last ship. How
many people has it consumed
over three hundred years?
(gpt-4) The deep-sea monster consumes ships every 100 years, and
over 300 years it would have consumed 3 ships. Given that each new
ship has twice as many people as the last ship, it means there’s a series
of doubling. Let’s assume the number of people on the first ship is
X. Then the number of people on the second ship (which is twice the
first) would be 2X and the number of people on the third ship (twice
the second ship) would be 4X. Given that the total number of people
consumed over three centuries is 847, we can write the equation: X +
2X + 4X = 847 This simplifies to 7X = 847 To find X (the number of
people on the first ship), we divide 847 by 7, so X = 121. So, over three
hundred years, the Monster has consumed 121 people on the first ship,
242 people on the second ship, and 484 people on the third ship. The
total is 847 people.
#3
LLM gener-
ates incorrect
calculations
and
yields
an incorrect
answer
John is a hack author.
He
writes a book every 2 months.
He has been writing for 20
years and has earned an aver-
age of $30,000 per book. How
many years has he been writ-
ing?
(Llama-2-13b-chat) To find out how many years John has been writ-
ing, we need to divide the total number of books he has written by
the number of books he writes per year. Let’s say John has written x
books in total. Since he writes a book every 2 months, he has writ-
ten: x = 20 years ∗ 2 months/year = 40 books . We know that John has
earned an average of $30,0000 per book, so the total amount he has
earned is: Total income = 40 books * $30,000 = $1,200,000 We can
now solve for x: $1,200,000 = 20 years ∗ 2 months/year x = $1,20,000
/ $20,000. x = 10 years. Therefore, John has been writing for 10 years.
#4
LLM
hal-
lucinates
variables
that are not
stated
in
the question
and
yields
a
wrong
answer
During her first year playing
soccer, Tara’s dad attended
90% of the games she played.
In her second year playing
soccer, Tara’s dad attended 4
fewer games than he did in the
previous year. What percent-
age of the games did Tara’s
dad attend during her first year
playing soccer?
(Llama-2-13b-chat) During her first year playing soccer, Tara’s dad
attended 90% of the games she played, which means he attended 90/100
= 9 games out of 10 . In her second year playing soccer, Tara’s dad
attended 90% of the games, but instead of attending 9 games, he at-
tended 90% of 10 games minus 4 games, which is: 90% x 10 - 4 =
90% x 6 = 54% or 6 games. So, Tara’s dad attended 54% = 54/100 = 5
games out of the total of 10 games played by Tara during her first year
playing soccer. Answer: 5 games or 50% of the games during Tara’s
first year playing soccer.
Table 3: Example of different types of redundant calculations from various LLMs. The ground truth answer to the
question is underlined in the question. We replace the newline character from the LLMs’ output with space or
period for easier illustration. We highlight the wrong calculations and hallucinated variables.
tion, forcing them to apply CoT on every question
that looks like a mathematical question. Hence, we
are curious whether LLMs can drop the CoT rea-
soning and calculations when properly instructed.
To explore this possibility, we append the follow-
ing instruction after the questions in GSM8K-Zero:
""If the question is simple enough, you
can omit the step-by-step reasoning and
just give the answer."" Here, we only test on
the LLMs that generate answers with higher redun-
dancy in Section 3.1.
The results are shown in Table 2. We can see that
when LLMs are allowed to omit step-by-step rea-
soning, the redundancy of the LLMs significantly
drops compared with Table 1 while the accuracy
significantly boosts for almost all models. The de-
crease in output redundancy implies that LLMs do
know that some questions in GSM8K-Zero are easy
enough to answer directly. However, even when
they are allowed to omit step-by-step reasoning,
the redundancy in these LLMs is still higher than
25%. This means that LLMs cannot always cor-
rectly infer the difficulty and whether step-by-step
reasonings are necessary for the questions.
4
Why Do LLMs Generate Redundant
Calculations?
After seeing that LLMs produce excessive calcu-
lations, we seek to understand why. We speculate
that the reward models (RMs) in RLHF might favor
more verbose outputs over concise ones, making
RLHF-trained models prone to generate lengthy
output even if it is redundant. To test this hypoth-
esis, we would like to compare RM’s preference
between long and short answers. However, we can-
not access RMs used to train ChatGPT or Llama
models. As a workaround, we use ChatGPT and
GPT-4 as the proxy of the RMs; we call these mod-
els proxy RMs in this case. To obtain the preference
of the proxy RMs, we give proxy RMs some in-
structions, a question in GSM8K-Zero, a pair of
long and short answers, and ask the model to select
a better answer. We follow the instructions used
in Zheng et al. (2023), which asks the proxy RMs
to consider the accuracy and helpfulness of the
answer. The experiment is repeated by inverting
the order of the short and long answers to coun-
teract potential position bias. Using ChatGPT or
GPT-4 as the proxy RMs is reasonable, as these
models should learn the preferences of their RMs
during RLHF. Additionally, prior works have used
ChatGPT and GPT-4 to generate the preference
data to train the RMs (Anand et al., 2023), so the
preference of ChatGPT or GPT-4 can reflect the
preference of RMs.
We prepare the long and short answers as fol-
lows: To collect long answers, we collect Chat-
GPT’s answers to questions in GSM8K-Zero, se-
lect those with redundant calculations, and group
those answers into two: correct answers and incor-
rect answers, with approximately 100 samples in
each group. The 100 samples in the correct-answer
group are reviewed by one of the authors to en-
sure that the answer is correct instead of a false
positive due to imperfect regular expressions when
extracting the answer from the LLM’s response.
The same procedure is done for the 100 samples in
the incorrect-answer group. Next, for each long an-
swer collected, we construct a short answer counter-
part by the template, ""The answer is [[ground
truth]]"", where ""[[ground truth]]"" is filled in
with the ground truth in GSM8K-Zero.
The preference of proxy RMs between long and
short answers is shown in Figure 2. First, we ob-
serve that when both the long and short answers
are correct (Figure 2(a)), both GPT-4 and ChatGPT
prefer long answers. By scrutinizing the evaluation
results, we find that GPT-4 and ChatGPT frequently
complain about the shorter answer to ""only answer
the question without any further details,"" while the
long answer ""shows more information."" However,
when reading the long answers, the authors find it
Preference percentage
GPT4
ChatGPT
(a) Longer answers are correct
Longer (correct)
Tie
Shorter (correct)
0
20
40
60
80
100
Preference percentage
GPT4
ChatGPT
(b) Longer answers are incorrect
Longer (wrong)
Tie
Shorter (correct)
Figure 2: The preference of GPT-4 and ChatGPT be-
tween longer and shorter answers. (a) The case when
the longer answers are correct. (b) The case when the
longer answers are incorrect.
hard to locate the answer to the question since the
model outputs too much unnecessary information
and complicates the problem, making the answer
unhelpful. Next, when the long answer is incorrect
and the short answer is correct (Figure 2(b)), we
find that ChatGPT consistently prefers lengthy but
wrong answers. While GPT-4 successfully prefers
the short and correct answer in 61% of the cases,
GPT-4 still votes for long but wrong answers in
34% of the cases. Overall, the results in Figure 2
show that proxy RMs strongly prefer long outputs
that contain redundant calculations and unneces-
sary reasoning, even if the final answer is wrong!
If we use the proxy RMs’ preference data collected
in this section, it is easy to think that we will obtain
RMs that favor lengthy output, eventually leading
to an LLM that generates redundant calculations.
We repeat the above experiment using the answers
from Llama-7b-chat and observe a similar result.
5
Conclusion
In this paper, we construct GSM8K-Zero to illus-
trate the redundancy in the output from LLMs. We
show that LLMs tend to generate redundant cal-
culations and unnecessary reasoning, sometimes
leading to a wrong answer. We reveal that LLMs
may not differentiate questions requiring step-by-
step reasoning from simpler ones, suggesting a
possible direction for improving LLMs. To explain
our observation, we use proxy RMs and find that
these models prefer lengthy answers even if they
are wrong. Through this paper, we hope future re-
searchers can focus more on the redundancy of the
outputs of LLMs and develop training techniques
to teach LLMs when to think step-by-step.
Limitations
The main limitation of our paper is that we
only study redundancy on a manually constructed
dataset, GSM8K-Zero. The reason is that it is eas-
ier to define and calculate redundancy on GSM8K-
Zero; we believe this is an ample contribution since
it is a phenomenon never mentioned in the litera-
ture. While exploring redundancy on other existing
datasets will be interesting, we leave it to future
works.
Another limitation of our paper is that we rely
on ChatGPT and GPT-4 to construct GSM8K-Zero,
so noises in the constructed dataset are inevitable.
We emphasize that future researchers need to keep
the noises in the dataset in mind and take special
caution when interpreting the results evaluated on
GSM8K-Zero. To understand the noises in the
dataset, the authors randomly selected 250 samples
from GSM8K-Zero and reviewed them. As stated
in Section 2.1, we estimate that 85% of question-
answer pairs in GSM8K-Zero are valid. We present
the details about our manual review of the dataset in
Appendix B.2. We also discuss that our results and
observations in the main content still hold when
considering the noises in the dataset.
Last, since our paper is a short paper, an obvious
limitation is that there is still a lot to explore, but
we cannot include them in our paper. While we
deem our paper’s main content to be self-contained,
we include some potential questions that might
be raised by curious and enthusiastic readers in
Appendix A (FAQs section).
Ethical Statements
We do not see our work to have possible harmful
outcomes. We follow the ACL ethical guidelines
when conducting the research in this paper.
Acknowledgements
We want to thank the reviewers for providing de-
tailed feedback and actionable suggestions, which
helped us strengthen our paper. We also want to
thank the senior committee members for monitor-
ing the reviewing process. Cheng-Han Chiang is
supported by a Google PhD Fellowship and a Ph.D.
scholarship program by Delta Electronics.
References
Yuvanesh Anand, Zach Nussbaum, Brandon Duder-
stadt, Benjamin Schmidt, and Andriy Mulyar. 2023.
Gpt4all: Training an assistant-style chatbot with large
scale data distillation from gpt-3.5-turbo. https:
//github.com/nomic-ai/gpt4all.
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, Eric Chu, Jonathan H. Clark, Laurent El
Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau-
rav Mishra, Erica Moreira, Mark Omernick, Kevin
Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,
Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez
Abrego, Junwhan Ahn, Jacob Austin, Paul Barham,
Jan Botha, James Bradbury, Siddhartha Brahma,
Kevin Brooks, Michele Catasta, Yong Cheng, Colin
Cherry, Christopher A. Choquette-Choo, Aakanksha
Chowdhery, Clément Crepy, Shachi Dave, Mostafa
Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,
Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
Feng, Vlad Fienber, Markus Freitag, Xavier Gar-
cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-
Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua
Howland, Andrea Hu, Jeffrey Hui, Jeremy Hur-
witz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-
ski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,
Sneha Kudugunta, Chang Lan, Katherine Lee, Ben-
jamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,
Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,
Frederick Liu, Marcello Maggioni, Aroma Mahendru,
Joshua Maynez, Vedant Misra, Maysam Moussalem,
Zachary Nado, John Nham, Eric Ni, Andrew Nys-
trom, Alicia Parrish, Marie Pellat, Martin Polacek,
Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,
Bryan Richter, Parker Riley, Alex Castro Ros, Au-
rko Roy, Brennan Saeta, Rajkumar Samuel, Renee
Shelby, Ambrose Slone, Daniel Smilkov, David R.
So, Daniel Sohn, Simon Tokumine, Dasha Valter,
Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,
Pidong Wang, Zirui Wang, Tao Wang, John Wiet-
ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting
Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven
Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav
Petrov, and Yonghui Wu. 2023. Palm 2 technical
report.
Anthropic. 2023. Model card and evaluations for claude
models. Accessed on October 1, 2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners.
In Ad-
vances in Neural Information Processing Systems,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168.
Olga Golovneva, Moya Peng Chen, Spencer Poff, Mar-
tin Corredor, Luke Zettlemoyer, Maryam Fazel-
Zarandi, and Asli Celikyilmaz. 2023. ROSCOE: A
suite of metrics for scoring step-by-step reasoning. In
The Eleventh International Conference on Learning
Representations.
Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,
Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng
Wu. 2023. How close is chatgpt to human experts?
comparison corpus, evaluation, and detection. arXiv
preprint arxiv:2301.07597.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. In Advances
in Neural Information Processing Systems.
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. 2023. The flan
collection: Designing data and methods for effective
instruction tuning. arXiv preprint arXiv:2301.13688.
OpenAI. 2022. Chatgpt: Optimizing language models
for dialogue. Accessed on October 10, 2023.
OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Gray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In Advances in Neural Information
Processing Systems.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model.
https://
github.com/tatsu-lab/stanford_alpaca.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi
Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-
and-solve prompting: Improving zero-shot chain-of-
thought reasoning by large language models. In Pro-
ceedings of the 61st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 2609–2634, Toronto, Canada. Associ-
ation for Computational Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems.
Xi Ye and Greg Durrett. 2022. The unreliability of ex-
planations in few-shot prompting for textual reason-
ing. In Advances in Neural Information Processing
Systems.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena.
A
FAQs
Q1 This paper only studies RLFH models. What
about LLMs that are not RLHF-trained? Do
they also show redundancy in their outputs?
A1 Yes, non-RLHF-trained LLMs also show re-
dundancy in their outputs on GSM8K-Zero.
We use Alpaca (Taori et al., 2023) and Vi-
cuna (Chiang et al., 2023) and find them to
also generate redundant outputs in 40% of the
cases. We do not report the results in the main
paper since the outputs from Alpaca and Vi-
cuna are quite messy, and it is hard to calculate
the accuracy using regular expressions.
Q2 In Section 4, is it possible that the wrong and
long answers generated by ChatGPT are cor-
rect, making the proxy RMs prefer those long
answers? For example, when using regular
expressions to calculate accuracy, there might
be some cases that regular expressions cannot
handle.
A2 This is highly unlikely to happen. This is be-
cause one of the authors manually reviews
the long answers (100 correct and 100 wrong
ones) used in Section 4. Thus, the wrong an-
swers are assured to be wrong, and the correct
answers are assured to be correct. Since the
authors cannot review all the answers that con-
tain calculations, we only randomly sample
approximately 100 correct and 100 wrong an-
swers with calculations and include them in
the results in Figure 2.
B
More Information about GSM8K-Zero
B.1
Dataset Cards
GSM8K-Zero is constructed from GSM8K (Cobbe
et al., 2021). Since GSM8K does not include the
dataset license, we are unsure what license to re-
lease GSM8K-Zero.
B.2
Manual Review by the Authors
The authors randomly sample 250 samples from
GSM8K-Zero to understand the quality of the sam-
ples and whether using regular expression to cal-
culate accuracy has a high precision. The human
(author) evaluation is conducted in the following
steps: First, we randomly sample 125 samples from
the answers of ChatGPT that are correct together
with their corresponding questions, and we sample
125 samples for the answers of ChatGPT that are in-
correct together with their corresponding questions.
Recall that the accuracy is calculated using regular
expressions. We search for the first or last num-
ber that appears in the last sentence of the model’s
response, and we count the model response to be
accurate if the ground truth matches the number
extracted by regular expressions. While this pro-
cess may falsely consider the model to be correct
when the model’s answer is wrong, we find that
this merely happens during our manual review of
250 answers from ChatGPT. We separately sample
questions that ChatGPT correctly answered and
questions that ChatGPT got wrong because those
two groups of questions might be systematically
different.
Given a question, an answer from ChatGPT, and
the ground truth answer, one of the authors labels
the sample into four categories:
1. The ground truth is correct, and the answer
from ChatGPT is correct
2. The ground truth is wrong, while the answer
from ChatGPT is correct (matches the real
ground truth)
3. The ground truth is correct, but the answer
from ChatGPT is wrong
4. The question is invalid, including that ground
truth is wrong, the question cannot be an-
swered without calculation, or the question
is ambiguous.
We find that for questions that ChatGPT is cor-
rect, 89% of questions are valid, and the ground
truth answer is always correct. However, we find
that for 7% of the questions, ChatGPT’s answer is
wrong, but we count it as correct due to imperfect
parsing of regular expressions. For questions that
ChatGPT is inaccurate, about 70% of the questions
are valid, and the ground truth is wrong in 2% of
the cases. Only in 4% of the cases, the regular ex-
pression we use considers the answer of ChatGPT
to be wrong when it is correct. Considering that
ChatGPT’s accuracy is about 80%, we estimate that
the proportion of invalid questions in GSM8K-Zero
is 14.8%.
B.2.1
Does Invalid Questions Affect the
Results?
Readers may be concerned about whether the in-
valid questions change the observations in the main
content. The short answer is no. We explain as fol-
lows: For the redundancy shown in Table 1, if the
model generates CoT reasonings and calculations
for those invalid questions, then the redundancy
should be around 15%. But clearly, all model ex-
cept GPT-4 has a redundancy much higher than
15%. Thus, LLMs can still generate a lot of redun-
dant calculations for other valid questions. As for
GPT-4, we still find that it does generate redundant
calculations in some cases. #2 in Table 3 is such a
case.
Next, for accuracy, even if LLMs are wrong for
all the invalid questions, their accuracy should be
around 85% if they get all the valid questions cor-
rect. However, this is clearly not the case for all
LLMs except Claude-2. Next, for the rightmost
column in Table 1, if we assume that all the invalid
samples happen to be the samples that LLMs in-
clude calculations in the answer, the accuracy in
this column should increase. However, by some
simple maths, the readers can easily verify that
even considering this, the accuracy of answers con-
taining calculations is still much lower than that of
answers that do not include calculations. Thus, our
observation in the paper still holds.
C
Prompts
We list the prompts we use in this section.
Prompts for question generation using Chat-
GPT in Section 2.1
System prompt: You are
a helpful assistant. You need to answer
the questions of the user accurately. You
need to strictly follow the instructions.
User prompt
Your task is to convert a declarative
sentence into a question and the answer
to
that
question
should
be
a
number.
Importantly, the answer (number) to the
question should already be included in the
original sentence. If the answer need to
be obtained by calculation, the question
is invalid.
Even simple calculation is
not allowed. Keep the question as simple
as possible. For example:
Example 1:
Original sentence:
Alyssa,
Keely,
and
Kendall ordered 100 chicken nuggets from
a fast-food restaurant.
Answer (number only): 100
Question:
How many chicken nuggets did
Alyssa, Keely, and Kendall order?
Explanation:
The
number
100
already
appeared in the original sentence, so the
question fulfill the requirements.
Example 2:
Original sentence: Lilah’s family gallery
has 400 photos.
Answer (number only): 400
Question: How many photos are there in
Lilah’s family gallery?
Explanation:
The
number
400
already
appeared in the original sentence, so the
question fulfill the requirements.
Example 3:
Original sentence: {KNOWN_INFO}
Answer (number only): {ANS}
Question:
The {KNOWN_INFO} should be filled in with the
known information in the original question, and
the {ANS} should be filled in with the ground truth
answer.
Prompts for using ChatGPT and GPT-4 as the
proxy in Section 4
System prompt
Please act as an impartial
judge and evaluate the quality of the
responses provided by two AI assistants
to the user question displayed below. You
should choose the assistant that follows
the user’s instructions and answers the
user’s question better. Your evaluation
should
consider
factors
such
as
the
helpfulness, relevance, accuracy, depth,
creativity, and level of detail of their
responses.
Begin
your
evaluation
by
comparing the two responses and provide
a short explanation. Avoid any position
biases and ensure that the order in which
the
responses
were
presented
does
not
influence your decision.
Do not allow
the length of the responses to influence
your evaluation.
Do not favor certain
names of the assistants. Be as objective
as
possible.
After
providing
your
explanation, output your final verdict by
strictly following this format: ""[[A]]""
if
assistant
A
is
better,
""[[B]]""
if
assistant B is better, and ""[[C]]"" for
a tie.
User Prompt
[User Question]
{question}
[The Start of Assistant A’s Answer]
{answer_a}
[The End of Assistant A’s Answer]
[The Start of Assistant B’s Answer]
{answer_b}
[The End of Assistant B’s Answer]
D
Sampling parameters of LLMs
When using LLMs to generate the answer to ques-
tions in GSM8K-Zero, we set the temperature to
0.7 and keep all the other parameters as default.
We use Huggingface Transformers to run Llama-2.
","nanPrevious research has focused on the correctness of CoT reasoning steps or whether the reasoning steps are faithful to the question and support the final answer. However, no past study focuses on the redundancy in CoT reasonings. To address this gap, this study aims to understand the following research question: ""Does LLM generate redundant reasonings when they clearly need not do so?"""
"Lung cancer is one of the most significant cancer-related deaths globally. The research proposes a new task-specific loss function to calibrate the neural network, along with state-of-the-art Multi-class Difference in Confidence and Accuracy (MDCA) loss, to reduce the risk of overconfident mistakes. It also integrates post-hoc calibration by performing temperature scaling on top of the train-time calibrated model. The proposed technique demonstrates improved performance with a 5.98% improvement in the Expected Calibration Error (ECE) and a 17.9% improvement in Maximum Calibration Error (MCE) compared to the best-performing SOTA algorithms.","Lung cancer is a leading cause of cancer-related deaths worldwide, and early detection and treatment are crucial for improving survival chances. Computed Tomography (CT) scans are commonly used to detect cancerous lung infections. However, manual diagnosis by radiologists is time-consuming and limited by the availability of experts. To address this, researchers have explored the use of Deep Neural Networks (DNNs) for cancer detection. However, these algorithms tend to make overconfident mistakes, hindering their reliability for critical applications.","To overcome the model calibration issues, the research proposes a novel task-specific loss function that aims to calibrate the neural network and reduce the risk of overconfident mistakes. This loss function is specifically designed for the task of lung cancer detection and is combined with the state-of-the-art MDCA loss to achieve optimal calibration. Additionally, post-hoc calibration via temperature scaling is employed to fine-tune the model's confidence estimates.","The proposed approach is evaluated using the publicly available Chest CT scan cancer (CCTSC) dataset, containing 928 CT scan images corresponding to four classes: normal, Adenocarcinoma, Large cell carcinoma, and Squamous cell carcinoma. The model is trained on 613 images and tested on the remaining 315 images. The results demonstrate a significant improvement in model calibration. Compared to the best-performing SOTA algorithms, the proposed method achieves a 5.98% reduction in ECE and a 17.9% reduction in MCE for the ResNet 34 backbone. Similar improvements are observed for the ResNet 50 backbone.","The proposed task-specific loss function, in conjunction with MDCA loss and post-hoc temperature scaling, effectively calibrates the neural network for lung cancer detection, reducing the risk of overconfident mistakes and enhancing the model's reliability. This makes the calibrated neural network more suitable for critical applications where accurate and reliable predictions are essential.",Task-specific regularization loss towards model calibration for reliable lung cancer detection,"Mehar Prateek Kalra, Mansi Singhal, Rohan Raju Dhanakashirur"," 
1 
 
 
Task-specific regularization loss towards model 
calibration for reliable lung cancer detection
Mehar Prateek Kalra¶†, Mansi Singhal†, and Rohan Raju Dhanakashirur§ 
 
 Dayalbagh Educational Institute (Deemed University), Agra, India 
Dayalbagh Educational Institute (Deemed University), Agra, India 
Indian Institute of Technology Delhi, New Delhi, India 
(Received on DD MM 2022, received in revised form DD MM 2022, accepted 24 09 2022) 
 Lung cancer is one of the significant causes of cancer-related deaths globally. Early detection and treatment 
improve the chances of survival. Traditionally CT scans have been used to extract the most significant lung 
infection information and diagnose cancer. This process is carried out manually by an expert radiologist. 
The imbalance in the radiologists-to-population ratio in a country like India implies significant work 
pressure on them and thus raises the need to automate a few of their responsibilities. The tendency of 
modern-day Deep Neural networks to make overconfident mistakes limit their usage to detect cancer. In this 
paper, we propose a new task-specific loss function to calibrate the neural network to reduce the risk of 
overconfident mistakes. We use the state-of-the-art Multi-class Difference in Confidence and Accuracy 
(MDCA) loss in conjunction with the proposed task-specific loss function to achieve the same. We also 
integrate post-hoc calibration by performing temperature scaling on top of the train-time calibrated model. 
We demonstrate 5.98% improvement in the Expected Calibration Error (ECE) and a 17.9% improvement in 
Maximum Calibration Error (MCE) as compared to the best-performing SOTA algorithm. 
Index Terms-- CT scan-based diagnostics, Deep Neural Network calibration, Lung cancer detection, Task-
specific Loss function
I. 
INTRODUCTION 
 
1Lung Cancer is one of the major causes of cancer-related 
death worldwide. The mortality rate is higher than that of 
breast and prostate cancer combined [1].  This can be 
attributed to the lack of diagnosis and treatment in the early 
stages of cancer. Lung cancers can be categorized broadly 
into Squamous cell carcinoma, Adenocarcinoma, and 
Large cell carcinoma. Various diagnosing techniques have 
been developed to detect lung cancer. Computed 
Tomography (CT) is the most prominent technique used to 
diagnose infectious lungs and thereby detect cancer [2]. 
This process is generally carried out manually and 
therefore limits the scalability of diagnosis, resulting in 
lung cancer-related mortalities.  
Many researchers have tried to automate the process of 
cancer detection. Narain Ponraj et.al [3] came up with a 
Local Optimal Oriented Pattern (LOOP) based CT image 
feature extraction method to automate the process of 
cancer detection. Though this was very popular, it required 
constant hyperparameter tuning and thereby reduced the 
scalability of the software. Jony et.al [4] tried resolving 
this issue by proposing a machine learning solution to it. 
They used a Marker-Controlled Watershed Gabor filter for 
segmentation of the CT images and later performed feature 
extraction on the same by using GCLM. The trained 
features were later learned using SVM. This technique 
 
1¶ Corresponding Author: Tel: +918889380023 
† Bachelor Student with the Department of Electrical Engineering  
§Ph,d Student  with Department of Computer Science Engineering  
  
reduced the dependency on hyperparameter tuning. 
Similarly, Wang et.al [5] used Random Forest to learn the 
features. However, these algorithms were not able to 
achieve the accuracy required.  
 
    Artificial Neural Networks (ANNs) were used by 
Kaur et.al [6]. They extracted GLCM [7] based features 
and used ANN to learn them. This was more of a heuristic 
way to use ANNs in solving the problem. Moradi et.al [8] 
came up with a 3D CNN-based model to solve the 
problem. Kanavati et.al [9] and chaunzwa et.al [10] use 
other standard CNN architectures, such as ResNet [11] and 
VGG [12], respectively, to learn cancer from CT images. 
Though these models worked considerably well in cancer 
detection, they were not so well accepted by the 
community due to model calibration issues.  
 
    Hence, it is very clear that the tendency of deep neural 
networks to commit overconfident mistakes makes them 
unusable for micro - suturing evaluation. In this paper, we 
propose a new technique to calibrate the model and restrict 
it from committing overconfident mistakes. Towards this, 
E-mail addresses: mehar.p.kalra2gmail.com, 
mansisinghal502@gmail.com, rohanrd28296@gmail.com  
 
 
♥ 2022 SSI All rights reserved. 
 
 
2 
 
we claim the following contributions. 
➔  We propose a new task-specific loss function to 
calibrate the neural network to reduce the risk of 
overconfident mistakes.  
➔  We use the state-of-the-art Multi-class Difference in 
Confidence and Accuracy (MDCA) loss [13] in 
conjunction with the proposed task-specific loss function 
to achieve the same. 
➔ We also integrate post-hoc calibration by performing 
temperature scaling on top of the train-time calibrated 
model. 
➔  We demonstrate a 5.98% improvement in the 
Expected Calibration Error (ECE) and 17.9% in Maximum 
Calibration Error (MCE) as compared to the best 
performing generic loss function based SOTA algorithms 
 
II. 
RELATED WORK 
Model calibration is the most efficient way of solving 
this problem of overconfident mistakes [13]. The 
preliminary focus of model calibration is to convert the 
class probabilities into confidence. Therefore, in a 
calibrated model, the class probability of 70% indicates 
that the event is realized 70% of the time. Researchers have 
proposed various techniques to achieve model calibration. 
Post-Hoc Calibration and Train-Time Calibration are the 
two techniques that are mainly classified for calibrating 
DNNs. Post-hoc calibration comes up with the concept of 
a hold-out set to calibrate the model. On the other hand, 
the train-time calibration techniques modify certain system 
properties of the model to achieve calibration while 
training itself. 
 
 
Post-Hoc Calibration 
 
Post-hoc calibration utilizes the hold-out training set 
(validation set) to calibrate the model externally. 
Temperature scaling [14] uses a single scalar parameter T 
(>1) to rescale logit scores before applying the softmax 
function. Researchers use Temperature scaling as a variant 
of Platt scaling [15] to solve the same issue. Wenger et.al 
[16] use gaussian processes to model the latent space of the 
calibration function and thereby propose a non-parametric 
post-hoc calibration. Kuleshov et.al [17] propose to 
simplify the structure for the two-class classification 
problem. Xingchen et.al [18] propose a constrained 
optimization technique for well-controlled post-hoc 
calibration. They use mis coverage rate and convergence 
accuracy to obtain the constraints for the optimization. 
Each of these techniques calibrates the model in its own 
way. However, as the process of calibration is done after 
the training is complete, there is very little chance for the 
transformation of the outputs from class probabilities to 
confidence.  This issue can be resolved by calibrating the 
model at the train time itself.  
 
Train-Time Calibration 
Brier et. al [19]  introduces the Brier score to calibrate the 
binary probabilistic forecast. However, this system 
generally overfits due to its hard-bound calibration 
function and makes it unusable for real-world applications. 
Other approaches, such as Label Smoothing on soft targets 
[20] and entropy as regularization [21], are proposed to aid 
in 
improving 
calibration. 
Researchers 
have 
also 
considered using Focal loss [22] to calibrate the model by 
reducing KL-divergence and increasing the entropy 
simultaneously.  
 
 
III. 
TASK SPECIFIC LOSS FUNCTION 
In this paper, we solve the problem of lung cancer 
detection. We aim to obtain a model such that it outputs 
the confidence equivalent to the empirical frequency of its 
correctness. We use the publicly available Chest CT scan 
cancer (CCTSC) dataset to evaluate the proposed solution. 
The dataset contains CT scan images corresponding to four 
classes, namely: normal, Adenocarcinoma, Large cell 
carcinoma, and Squamous cell carcinoma. The dataset 
contains 928 CT scan images in total. Thus, the present 
dataset can be described as a set of samples 𝐷 =
 [(𝑥𝑖, 𝑦𝑖 )]𝑖=1
928 coming from a joint  
 
distribution 𝐷 (𝑋, 𝑌) such that each of the samples 𝑥𝑖 ∈
𝑋 are the Chest CT images and 𝑦𝑖 ∈ 𝑌 = [0,1,2,3] 
Figure 1: Sample images of Chest CT from each of the classes from the 
CCTSC dataset. (a) normal CT, (b) Adenocarcinoma, (c) Squamous 
carcinoma, (d) Large cell carcinoma 
 
3 
 
denoting the ground truth labels. Formally, our problem 
can be defined as follows: Let there be a deep learning 
model predicting 𝑦𝑝 as the class label with 𝑝𝑖 as top-1 
probability for the 𝑖𝑡ℎ sample. The model is said to be 
calibrated if it obeys Equation 1. 
 
 
𝑃(𝑦𝑝 = 𝑦𝑖  | 𝑝𝑖  =  𝑠) =  𝑠                                          (1) 
 
We propose to device a task-specific loss function 
𝐿𝑇𝑆(𝑦𝑝, 𝑦𝑖) that estimates the values of 𝑝𝑖 𝑎𝑛𝑑 𝑦𝑖 which 
satisfies Equation 1.  
     It can be observed that our dataset does not span natural 
images. It is extremely restricted to the domain of chest 
CT. Since all the images show structural similarity to a 
great extent, the model must learn the differences in the 
finer parts of the image rather than learning the overall 
structure. Also, from the clinical expertise, it is evident that 
the normal CT scan comprises the entire lung, whereas 
each of the carcinomatous lung CT scans have some part 
of the lung covered by the tumor. In specific, squamous 
carcinoma has the least part of the lung covered by the 
tumor, followed by Adenocarcinoma and Large cell 
carcinoma.  
 
Thus, if the ground truth labels are modified such that they 
obey the pattern of tumor deposition on the lungs (i.e-> 
Class 0 must correspond to normal Images, class 1 must be 
squamous carcinoma, class 2 must be Adenocarcinoma, 
and class 3 must be Large cell carcinoma), we can observe 
a gradation in tumor deposition as we progress from class 
0 to class 3. Figure 1 encapsulates the visualization of this 
information.  
 
    This information of gradation can be exploited in 
training. i.e. the model predicting class 2 for a class 3 
image can be penalized less as compared to a model 
predicting class 1 for a class 3 image. We encapsulate this  
information in the proposed loss function. The proposed 
loss function is shown in Equation 2. 
𝐿𝑇𝑆(𝑦𝑝 , 𝑦𝑖) = 1
𝑁 ∑
𝑁
𝑖 = 1
(𝑦𝑖−𝑛𝑜𝑟𝑚 − 𝑦𝑝−𝑛𝑜𝑟𝑚)2            (2) 
where 𝑦𝑖−𝑛𝑜𝑟𝑚  for a 𝑘 class classification problem is as 
shown in Equation 3 
 
𝑦𝑖−𝑛𝑜𝑟𝑚  = 𝑎𝑟𝑔𝑚𝑎𝑥 (𝑦𝑖)
𝐾 − 1
                                             (3) 
A similar equation can be written for 𝑦𝑝−𝑛𝑜𝑟𝑚 , indicating 
the completeness for Equation 2.  
 
    We add this loss function to the standard cross entropy 
loss function (𝐿𝐶𝐸) and the MDCA loss function (𝐿𝑀𝐷𝐶𝐴) 
[13] to achieve bin independent and class independent 
regularization. Therefore, the total loss function 𝐿𝑡𝑜𝑡𝑎𝑙 is 
given by Equation 4. 
 
𝐿𝑡𝑜𝑡𝑎𝑙  =  𝛼 𝐿𝐶𝐸 + 𝛽𝐿𝑀𝐷𝐶𝐴 + 𝛾𝐿𝑇𝑆                              (4) 
 
Where 𝛼, 𝛽, 𝛾 are constants, summing to 1. 
    We claim that the use of this loss function along with 
post-hoc calibration using temperature scaling gives the  
 
 
calibrated model.  Figure 2 shows the schematic diagram 
of the proposed system 
IV. 
IMPLEMENTATION DETAILS 
 We trained our model for 50 epochs on the CCTSC dataset 
and fine-tuned it on the same training dataset for another 
50 epochs. The fine-tuning was divided into two steps, 
namely warm-up and final training. A batch size of 128 
was chosen due to the high-performance ability of our 
GPU to load multiple high-resolution, multi-resolution 
images in a batch. For training on CCTSC, a learning rate  
 
 1 × 10−5, Adam optimizer with the momentum of 0.8, 
weight decay of 1 × 10−4, and a gamma value of 0.1 was 
used. We fine-tuned using a learning rate of 1 × 10−6, 
weight decay as 0.0005 and a momentum of 0.8. While 
fine-tuning, we only train the last 2 layers of the network, 
keeping the weights of the rest of the ResNet backbone 
Figure 2: Proposed system for calibrated model towards lung cancer 
detection 
 
4 
 
frozen. All computations were carried out on a High 
Performance Computing Cluster having 80GB A100 
GPUs. The training procedure took an average of 4 sec 
(approx.) per iteration  
with a GPU memory occupancy of 31GB (approx.). The 
computations were carried out using Pytorch 1.11 and 
torchvision 0.12.0 libraries on Python 3.9 of Anaconda 3-
2022.5. 
V. 
RESULTS AND DISCUSSION 
We use 613 images of the dataset for training and 315 
images for testing. Figure 4 shows the changes in the 
confidence and loss function over the iterations. It can be 
observed that the system converges before the maximum 
number of epochs is reached.  
 
 
Figure 4: Change in confidence and loss value over the 
iterations. 
 
 
   We propose to evaluate the expected calibration score 
(ECE) and Maximum calibration score (MCE) for each of 
the models. MCE is the maximum difference between the 
accuracy and confidences averaged over each of the bins 
and is given by Equation 5.  
 
𝑀𝐶𝐸 =  𝑚𝑎𝑥𝑖∈{1−𝐵} |𝑎𝑣𝑔(𝑝𝑖) − 𝑎𝑣𝑔(𝑠𝑖)|             (5) 
 
ECE is the weighted average of the absolute difference 
between the confidence and accuracy of each bin. It shows 
the average error in calibration over the entire test set. 
Reduced values of ECE and MCE indicate better 
calibration.  
 
Table 1 shows the performance of the proposed 
algorithm and the other SOTA algorithms on the dataset, 
using two different backbones, ResNet 34 and ResNet 50.  
 
TABLE I 
PERFORMANCE OF THE PROPOSED ALGORITHM AND THE 
OTHER SOTA ALGORITHMS 
 
Architecture 
ResNet 34 
ResNet 50 
ECE 
MCE 
ECE 
MCE 
Cross Entropy [24] 
29.34 
54.07 
9.57 
67.88 
Focal Loss [22] 
10.25 
28.76 19.11 29.06 
MDCA Loss [13] 
11.55 
30.13 21.76 30.71 
Proposed Algorithm 
4.27 
10.86 
4.65 
6.69 
 
It can be observed that the proposed algorithm performs 
better than the best performing SOTA by 5.98% in ECE 
and 17.90% in MCE for ResNet 34 backbone and 14.46% 
in ECE and 22.37% in MCE for ResNet 50 backbone. 
 
TABLE II 
ABLATION ANALYSIS OF THE PROPOSED ALGORITHM 
 
 
Architecture 
ResNet 34 
ResNet 50 
ECE 
MCE 
ECE 
MCE 
Only 𝐿𝐶𝐸 
29.34 
54.07 
9.57 
67.88 
𝐿𝐶𝐸 + 𝐿𝑀𝐷𝐶𝐴 
19.29 
27.09 32.39 
60.4 
𝐿𝐶𝐸 + 𝐿𝑀𝐷𝐶𝐴 + 𝐿𝑇𝑆 
15.63 
31.92 26.57 38.52 
Proposed Algorithm 
(𝐿𝐶𝐸 + 𝐿𝑀𝐷𝐶𝐴 + 𝐿𝑇𝑆+ 
4.27 
10.86 
4.65 
6.69 
Figure 3: Comparison of the reliability plots for the proposed algorithm 
against SOTA implementations. (a) Cross Entropy [24] (b) Focal Loss 
[22] (c) MDCA Loss [13] (d) Proposed algorithm 
 
5 
 
Temp scaling) 
 
This drastic improvement in the performance can be 
attributed to the fact that the other SOTA loss functions are 
designed for generic applications, whereas the proposed 
loss function is designed for this specific task. Although 
this comparison may not sound very fair, the unavailability 
of Lung cancer detection-specific loss functions forces us 
to have a comparison with the generic algorithms. The 
corresponding reliability diagrams for the ResNet 34 
backbone can be seen in Figure 3. 
 
We performed an ablation analysis by removing each of 
the components from the proposed system. The 
corresponding ECE and MCE values can be seen in Table 
2, and the reliability plots for the ResNet 34 backbone can 
be seen in Figure 5. Table 2 confirms the need for each of 
the components. We also observe temperature scaling to 
be a significant component in model calibration. 
 
VI. 
CONCLUSION 
 In this paper, we proposed a new task-specific loss 
function to calibrate the neural network to reduce the risk 
of overconfident mistakes. We used the state-of-the-art 
Multi-class Difference in Confidence and Accuracy 
(MDCA) loss in conjunction with the proposed task-
specific loss function to achieve the same. We also 
integrated post-hoc calibration by performing temperature 
scaling on top of the train-time calibrated model. We 
demonstrated a 5.98% improvement in the Expected 
Calibration Error (ECE) and a 17.90% improvement in 
Maximum Calibration Error (MCE) as compared to the 
best-performing SOTA algorithm. We also observed that 
the calibrated neural network is generally more reliable 
than the non-calibrated counterpart, and hence it can be 
used in critical applications such as lung cancer detection 
 
VII. 
ACKNOWLEDGMENT 
We would like to express our sincere gratitude to our 
mentor and advisor, Prof. Prem Kumar Kalra. His 
constant support and guidance have made it possible for us 
to write this paper. Thank you, sir! 
 
VIII. 
REFERENCES 
[1] Rehman, Amjad, Muhammad Kashif, Ibrahim Abunadi, and Noor 
Ayesha. ""Lung cancer detection and classification from chest CT scans 
using machine learning techniques."" In 2021 1st International Conference 
on Artificial Intelligence and Data Analytics (CAIDA), pp. 101-104. 
IEEE, 2021. 
2] Ayshath Thabsheera, A. P., T. M. Thasleema, and R. Rajesh. ""Lung 
cancer detection using CT scan images: A review on various image 
processing techniques."" Data Analytics and Learning (2019): 413-419. 
[3] NarainPonraj, D., Esther Christy, G. Aneesha, G. Susmitha, and 
Monica Sharu. ""Analysis of LBP and LOOP based textural feature 
extraction for the classification of CT Lung images."" In 2018 4th 
International Conference on Devices, Circuits and Systems (ICDCS), pp. 
309-312. IEEE, 2018. 
[4] Jony, Mehdi Hassan, Fatema Tuj Johora, Parvin Khatun, and 
Humayan Kabir Rana. ""Detection of Lung cancer from CT scan images 
using GLCM and SVM."" In 2019 1st International Conference on 
Advances 
in 
Science, 
Engineering and 
Robotics Technology 
(ICASERT), pp. 1-6. IEEE, 2019. 
[5] Wang, Jiachen, Riqiang Gao, Yuankai Huo, Shunxing Bao, Yunxi 
Xiong, Sanja L. Antic, Travis J. Osterman, Pierre P. Massion, and 
Bennett A. Landman. ""Lung cancer detection using co-learning from 
chest CT images and clinical demographics."" In Medical imaging 2019: 
Image processing, vol. 10949, pp. 365-371. SPIE, 2019. 
[6] Kaur, Loveneet, Manmohan Sharma, Rajan Dharwal, and Aditya 
Bakshi. ""Lung Cancer Detection Using CT Scan with  
Artificial Neural Network."" In 2018 International Conference on Recent 
Innovations in Electrical, Electronics & Communication Engineering 
(ICRIEECE), pp. 1624-1629. IEEE, 2018. 
[7] Mall, Pawan Kumar, Pradeep Kumar Singh, and Divakar Yadav. 
""Glcm-based feature extraction and medical x-ray image classification 
using machine learning techniques."" In 2019 IEEE Conference on 
Information and Communication Technology, pp. 1-6. IEEE, 2019. 
[8] Moradi, Pouria, and Mansour Jamzad. ""Detecting lung cancer lesions 
in CT images using 3D convolutional neural networks."" In 2019 4th 
International Conference on Pattern Recognition and Image Analysis 
(IPRIA), pp. 114-118. IEEE, 2019. 
[9] Kanavati, Fahdi, Gouji Toyokawa, Seiya Momosaki, Hiroaki 
Takeoka, Masaki Okamoto, Koji Yamazaki, Sadanori Takeo, Osamu 
Iizuka, and Masayuki Tsuneki. ""A deep learning model for the 
classification of indeterminate lung carcinoma in biopsy whole slide 
images."" Scientific Reports 11, no. 1 (2021): 1-14. 
[10] Chaunzwa, Tafadzwa L., Ahmed Hosny, Yiwen Xu, Andrea Shafer, 
Nancy Diao, Michael Lanuti, David C. Christiani, Raymond H. Mak, and 
Hugo JWL Aerts. ""Deep learning classification of lung cancer histology 
using CT images."" Scientific reports 11, no. 1 (2021): 1-12. 
[11] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Deep 
residual learning for image recognition."" In Proceedings of the IEEE 
Figure 5: Reliability plot for ablation study. (a) is only cross entropy loss 
(b) is cross entropy with MDCA loss (c) is cross entropy with MDCA and 
TS loss (d) is the proposed algorithm 
 
 
6 
 
conference on computer vision and pattern recognition, pp. 770-778. 
2016. 
[12] Simonyan, Karen, and Andrew Zisserman. ""Very deep convolutional 
networks 
for 
large-scale 
image 
recognition."" 
arXiv 
preprint 
arXiv:1409.1556 (2014). 
[13] Hebbalaguppe, Ramya, Jatin Prakash, Neelabh Madan, and Chetan 
Arora. ""A Stitch in Time Saves Nine: A Train-Time Regularizing Loss 
for Improved Neural Network Calibration."" In Proceedings of the 
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 
16081-16090. 2022. 
[14] Minderer, Matthias, Josip Djolonga, Rob Romijnders, Frances 
Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. 
""Revisit 
ing the calibration of modern neural networks."" Advances in Neural 
Information Processing Systems 34 (2021): 15682-15694. 
[15] Böken, Björn. ""On the appropriateness of Platt scaling in classifier 
calibration."" Information Systems 95 (2021): 101641. 
[16] Wenger, Jonathan, Hedvig Kjellström, and Rudolph Triebel. ""Non-
parametric calibration for classification."" In International Conference on 
Artificial Intelligence and Statistics, pp. 178-190. PMLR, 2020. 
[17] Kuleshov, Volodymyr, and Percy S. Liang. ""Calibrated structured 
prediction."" Advances in Neural Information Processing Systems 28 
(2015). 
[18] Ma, Xingchen, and Matthew B. Blaschko. ""Meta-cal: Well-
controlled post-hoc calibration by ranking."" In International Conference 
on Machine Learning, pp. 7235-7245. PMLR, 2021. 
[19] Brier, Glenn W. ""Verification of forecasts expressed in terms of 
probability."" Monthly weather review 78, no. 1 (1950): 1-3. 
[20] Szegedy, Christian, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, 
and Zbigniew Wojna. ""Rethinking the inception architecture for 
computer vision."" In Proceedings of the IEEE conference on computer 
vision and pattern recognition, pp. 2818-2826. 2016. 
[21] Liang, Gongbo, Yu Zhang, Xiaoqin Wang, and Nathan Jacobs. 
""Improved trainable calibration method for neural networks on medical 
imaging classification."" arXiv preprint arXiv:2009.04057 (2020). 
[22] Mukhoti, Jishnu, Viveka Kulharia, Amartya Sanyal, Stuart 
Golodetz, Philip Torr, and Puneet Dokania. ""Calibrating deep neural 
networks using focal loss."" Advances in Neural Information Processing 
Systems 33 (2020): 15288-15299. 
[23] Pérez-Cruz, Fernando. ""Kullback-Leibler divergence estimation of 
continuous distributions."" In 2008 IEEE international symposium on 
information theory, pp. 1666-1670. IEEE, 2008. 
[24] Mannor, Shie, Dori Peleg, and Reuven Rubinstein. ""The cross 
entropy method for classification."" In Proceedings of the 22nd 
international conference on Machine learning, pp. 561-568. 2005. 
 
","Researchers have proposed various techniques to automate lung cancer detection using CT scans. These methods include Local Optimal Oriented Pattern (LOOP) based feature extraction, Machine Learning algorithms like Support Vector Machines (SVM) with Marker-Controlled Watershed Gabor filter segmentation and Random Forest for feature learning. Artificial Neural Networks (ANNs) have also been employed, with researchers using GLCM based features and ANNs for classification. More recently, advanced CNN architectures such as ResNet and VGG have been utilized to learn cancer from CT images. However, despite these advancements, model calibration issues have limited the practical application of these deep learning models.nan"
"We propose a method to process user responses to clarifying questions. Currently, previous work simply appends responses to the user query, potentially harming retrieval performance. We introduce a classifier for assessing clarifying question and answer usefulness. Useful questions or answers are added to the conversation history, then passed to a transformer-based query rewriting module. Experiments show significant improvements over strong, non-mixed-initiative baselines.","Conversational search systems aim to satisfy a user's information need. Some aspects of these systems have seen significant advancements, including conversational passage retrieval, query rewriting, and intent detection. Mixed-initiative is an appealing paradigm in conversational search, where the system can offer suggestions or ask clarifying questions at any point in a conversation. Asking clarifying questions has been identified as an invaluable component of conversational search systems, as they aim to elucidate the underlying information needs of users.","We introduce a simple yet effective method for processing answers to clarifying questions, moving away from previous work that simply appends answers to the original query. Specifically, we propose a classifier designed to evaluate the usefulness of the prompted clarifying question and the given answer. Useful questions and answers are further appended to the conversation history and passed to a transformer-based query rewriting module to generate a more informative query.","Experiments were conducted on the TREC 2022 Conversational Assistance Track (CAsT'22). Results showed significant improvements over strong non-mixed-initiative baselines (12% and 3% relative improvement in Recall@1000 and nDCG, respectively). Furthermore, our approach mitigated performance drops when non-useful questions and answers were utilized. In scenarios where neither the question nor the answer was deemed useful, but still used, there was a relative performance decrease of 13% in terms of nDCG@3, compared to non-mixed-initiative baselines.","We propose a simple yet efficient method for processing answers to clarifying questions, based on classifying the usefulness of the prompted question and the provided answer. Results on the TREC 2022 Conversational Assistance Track (CAsT’22) demonstrate significant improvements in passage retrieval performance with the use of enhanced queries, as opposed to a non-mixed-initiative retrieval system.",Estimating the Usefulness of Clarifying Questions and Answers for Conversational Search,"Ivan Sekulić, Weronika Łajewska, Krisztian Balog, Fabio Crestani","arXiv:2401.11463v1  [cs.IR]  21 Jan 2024
Estimating the Usefulness of Clarifying
Questions and Answers for Conversational
Search
Ivan Sekuli´c1[0009−0001−0036−0490], Weronika  Lajewska2[0000−0003−2765−2394],
Krisztian Balog2[0000−0003−2762−721X], and Fabio Crestani1[0000−0001−8672−0700]
1 Universit`a della Svizzera italiana, Lugano, Switzerland
{ivan.sekulic,fabio.crestani}@usi.ch,
2 University of Stavanger, Stavanger, Norway
{weronika.lajewska,krisztian.balog}@uis.ch
Abstract. While the body of research directed towards constructing
and generating clarifying questions in mixed-initiative conversational
search systems is vast, research aimed at processing and comprehending
users’ answers to such questions is scarce. To this end, we present a simple
yet eﬀective method for processing answers to clarifying questions, mov-
ing away from previous work that simply appends answers to the original
query and thus potentially degrades retrieval performance. Speciﬁcally,
we propose a classiﬁer for assessing usefulness of the prompted clarify-
ing question and an answer given by the user. Useful questions or an-
swers are further appended to the conversation history and passed to a
transformer-based query rewriting module. Results demonstrate signiﬁ-
cant improvements over strong non-mixed-initiative baselines. Further-
more, the proposed approach mitigates the performance drops when non
useful questions and answers are utilized.
Keywords: Conversational search · Mixed initiative · Clarifying ques-
tions
1
Introduction
The goal of a conversational search (CS) system is to satisfy the user’s informa-
tion need. To this end, several aspects of CS systems have enjoyed signiﬁcant ad-
vancements, including conversational passage retrieval [22], query rewriting [21],
and intent detection [13]. In recent years, the mixed-initiative paradigm of con-
versational search has attracted signiﬁcant research attention [2, 16, 18]. Under
the mixed-initiative paradigm, the CS system can at any point in a conversation
oﬀer suggestions or ask clarifying questions.
Asking clarifying questions has been identiﬁed as an invaluable component
of modern-day CS systems [14]. Such questions are directed at users and aim to
elucidate their underlying information needs. While there is a growing body of
research on constructing and generating clarifying questions [2, 19, 23], work
aimed at processing and comprehending users’ answers to such questions is
This is the author’s version of the work. It is posted here for your personal use. The deﬁnitive version is published in:
Proceedings of the 46th European Conference on Information Retrieval (ECIR ’24), March 24–28, 2024, Glasgow, Scotland
2
I. Sekuli´c et al.
scarce. Nonetheless, recent research suggests their usefulness by demonstrating
improvements in passage retrieval performance after asking a clarifying question
and receiving an answer [1].
To bridge the aforementioned research gap, we make a ﬁrst step towards
processing the answers given to clarifying questions. We hypothesize that not
all information acquired through such interactions with the user would beneﬁt
the CS system, i.e., yield improvements in retrieval eﬀectiveness. Thus, the main
novelty of our approach is that we do not blindly utilize the questions and the
answers, but only when they are deemed to be useful. Speciﬁcally, we focus on
the task of conversational passage retrieval and design a classiﬁer aimed at as-
sessing usefulness of the asked clarifying question and the provided answer. We
utilize the question or the answer only if they are deemed useful, by appending
them to the conversational history and employing a query rewriting method to
attain a more information-dense query. Results on the TREC 2022 Conversa-
tional Assistance Track (CAsT’22) [11] demonstrate signiﬁcant improvements
in passage retrieval performance with the use of enhanced queries, as opposed
to a non-mixed-initiative retrieval system (12% and 3% relative improvement
in terms of Recall@1000 and nDCG, respectively). Further, when contrasting
our approach to an established method that simply appends the prompted clar-
ifying question and its answer to the original query [1], we observe diﬀerences
in performance. Speciﬁcally, if neither the question nor the answer are deemed
useful, but still used, there is a relative performance decrease of 13% in terms
of nDCG@3, compared to non-mixed-initiative baselines. In other words, it is
better not to use any information provided by such questions and answers, than
to use it wrongly. Our contributions can be summarized as follows:
– We propose a simple, yet eﬀective, method for processing answers to clarify-
ing questions. The method is based on classifying usefulness of the prompted
question and the given answer.
– We identify scenarios where asking clarifying questions resulted in improved
passage retrieval, and where it decreased the retrieval performance.
2
Related work
To facilitate further research in conversational search (CS), the TREC Con-
versational Assistance Track (CAsT) [3] aims to provide a reusable benchmark
composed of several pre-deﬁned conversational trajectories over a variety of top-
ics. The most recent edition, CAsT’22, oﬀers a subtask oriented towards mixed-
initiative (MI) interactions [11]. Under the MI paradigm of CS, the system can
at any point of a conversation take initiative and prompt the user with vari-
ous suggestions or questions [14]. One of the most prominent usages of mixed-
initiative is asking clarifying questions with a goal of elucidating users underlying
information need [2]. Recent research demonstrates a positive impact of clari-
fying questions both on user experience [5, 24] and retrieval performance [1].
Although other collections with clarifying questions and answers exist, most no-
tably ClariQ [1], in this work, we focus on the aforementioned CAsT’22 as we
Usefulness of Clarifying Questions and Answers
3
additionally have control over which kind of question is being asked. In general,
two streams of approaches to constructing clarifying questions exist: (1) select
an appropriate question from a pre-deﬁned pool of questions [1, 2, 11, 16, 18];
(2) generate the question [9, 19, 23]. However, despite the abundance of research
on clarifying question construction, researched aimed at processing users’ an-
swers to such questions is scarce. To bridge this gap, Krasakis et al. [6] conduct
an analysis of users’ answers and ﬁnd that they vary in polarity and length, as
well as that retrieval eﬀectiveness is often hurt. Thus, in this work, we aim to
automatically assess their usefulness, with a goal of mitigating this undesired
eﬀect.
3
Methodology
In this section, we formally deﬁne the task of conversational passage retrieval
under the mixed-initiative (MI) paradigm and present our methods for each of
the components of the task, i.e., query rewriting, clarifying question selection,
answer processing, and passage retrieval.
3.1
Task Formulation
At a current conversational turn t, given a user utterance ut and a conversation
history H = [(u1, s1), . . . , (ut−1, st−1)], the task is to generate a system response
st. For clarity, we omit the superscript t from the subsequent deﬁnitions. In MI
CS systems, the system’s response s can either be a clarifying question scq or
a ranked list of passages sD, D = [d1, d2, . . . , dN], where N is the number of
passages retrieved and di is the i-th passage in the list. Similarly, user utterance
u can take form of a query uq or an answer ua to system’s question scq. Modeling
other types of user utterances, such as explicit feedback, falls out of the scope
of this study. Following prior work [21], the ﬁrst task, i.e., query rewriting, is
aimed towards resolution of the user query uq in the context of the conversation
history, resulting in u′
q = γ(uq|H), where γ is a query rewriting method.
Following the MI setting introduced at TREC CAsT’22 [11], we address the
problem of conversational passage retrieval through the following three com-
ponents: (i) Produce system utterance scq by selecting an appropriate clarify-
ing question cq from a given pool of questions PQ; (ii) Process the given an-
swer ua and incorporate relevant information to the current query, resulting in
u′′
q = θ(u′
q, scq, ua); (iii) Return a ranked list of passages sD. Next, we deﬁne
our approaches to the described components. We note that a clarifying question
might be needed only for ambiguous, faceted, or unclear user requests. Thus, for
queries not requiring clariﬁcation, the system might opt to return a ranked list of
passages without asking further questions. However, following the setup enabled
by CAsT’22 track, we do not explicitly model clariﬁcation need and thus design
a system that prompts the user with a clarifying question at each turn.
4
I. Sekuli´c et al.
Table 1. Examples of annotated subset of ClariQ, indicating cases when clarifying
question, answer, both, or neither are useful.
Query (initial request)
Clarifying Question
Answer
Useful
Prevalence
I’m looking for information
on hobby stores.
Do you want to know
hours of operation?
No.
None
32%
Tell me information about
computer programming.
Are you interested in
a coding bootcamp?
No, I want to know
what career options
programmers have
Answer
53%
Find me map of USA.
Do you want to see a map
of US territories?
Yes.
Question
11%
All men are created equal
Would you like to know more about
the declaration of independence?
Yes, I’d like to
know who wrote it
Question
and answer
6%
3.2
Clarifying Question Selection
For each query u′
q, we rank the potential candidates cqi based on their semantic
similarity to u′
q. Speciﬁcally, we utilize a T5 model ﬁne-tuned on the CANARD
dataset [4], available at HuggingFace,3 as our γ rewriting function, which yields a
resolved utterance u′
q. To rank the potential candidates, we use MPNet [20] from
SentenceTransformers [17], trained for semantic matching. We select cqi with the
highest score, as predicted by the MPNet: scq = argmaxcqi∈P Qf MPNet(u′
q, cqi),
where PQf is a pool of clarifying questions with potentially misleading, unreli-
able, and faulty questions automatically ﬁltered from the pool [8].
3.3
Answer Processing
In this subsection, we describe our novel usefulness-based approach to process-
ing answers given to the asked clarifying questions. To address the issue, we
move away from previous approaches that simply append the question and the
answer to the original query [1, 2], regardless of the actual information gain. In
fact, a recent study by Krasakis et al. [6] demonstrated that such practice can
cause a decrease in retrieval eﬀectiveness. Moreover, they show that multi-word
answers are informative (e.g., “yes, I’m looking for info on spiders in Europe”),
thus improving retrieval performance. Similarly, short negative answers are not
informative (e.g., “no”), while multi-word negative answers are (e.g., “no, I’m
interested in buying aquarium cleaner”). Thus, we deﬁne four possible actions,
based on the current resolved utterance u′
q, the clarifying question asked scq,
and the answer ua:
1. In case the answer is aﬃrmative (e.g., “yes” or “Yes, that is what I’m looking
for”), we expand the u′
q by appending the clarifying question asked.
2. In case the answer is deemed useful, i.e., the underlying information need is
explained in greater detail, we expand u′
q by appending the answer.
3. In case the answer is aﬃrmative and it provides additional details, we expand
u′
q with both the clarifying question and the answer.
3 https://huggingface.co/castorini/t5-base-canard
Usefulness of Clarifying Questions and Answers
5
4. If neither (1), (2), nor (3) is the case, we do not expand the utterance.
Examples of the described cases are presented in Table 1 and are all aimed at
incorporating additional useful information to the current utterance. Formally:
u′′
q =







u′
q,
ψ(u′
q, scq, ua) = 0
φ(u′
q, scq),
ψ(u′
q, scq, ua) = 1
φ(u′
q, ua),
ψ(u′
q, scq, ua) = 2
φ(u′
q, ua, scq),
ψ(u′
q, scq, ua) = 3
(1)
where ψ(u′
q, scq, ua) is a usefulness classiﬁer, which aims to predict which of the
above described actions to take. The labels 0, 1, 2, and 3 correspond to neither
scq or ua were deemed useful, scq was deemed useful, ua was deemed useful, and
both were useful, respectively. Similarly to Section 3.1, the function φ rewrites
the original query given the context, in this case scq or ua.
Speciﬁcally, to model ψ, we ﬁne-tune a large transformer-based model, namely
T5 [15], for multi-class classiﬁcation. To ﬁne-tune the classiﬁer, we manually an-
notate a portion of ClariQ (150 samples) for the speciﬁc aforementioned cases.
The annotations were performed by two authors of the paper with an inter-
annotator agreement Cohen’s kappa of 0.89. The diﬀerences in annotations were
discussed and resolved consensually. Examples of annotations are presented in
Table 1 and classiﬁcation performance is reported in Section 4. We dub our
novel mixed-initiative classiﬁer-based method MI-Clf. Moreover, we assess the
prevalence of each of the cases, and ﬁnd, as presented in Table 1, that 68% of
interactions contain new, useful information. In the other 32% of the cases, the
answer simply negates the prompted clarifying question. Although this inter-
action also provides valuable insights into the user’s information need, current
approaches would expand the query by appending the prompted clarifying ques-
tion and the answer. However, such an expanded query contains terms the user is
not interested in, which can potentially degrade retrieval performance. We com-
pare our proposed method to such a baseline, which always extends the query
as: u′′
q = φ(u′
q, scq, ua). This method is dubbed MI-All.
Passage Retrieval. Finally, the rewritten utterance u′′
q is fed into a standard
two-stage retrieve-and-rerank pipeline [7]. We utilize BM25 (k1 = 0.95, b = 0.45)
with RM3, where the initial query is extended with the highest-weighting terms
from top-k scoring passages (k = 10 and number of terms m = 10). Next, we
use a point-wise monoT5 re-ranker [10] to re-rank the top 1000, followed by a
pair-wise duoT5 re-ranker [12] to additionally re-rank the top 50 passages. The
non-mixed-initiative baseline, dubbed DuoT5, uses the same retrieval pipeline.
4
Results
Usefulness Classiﬁer. The proposed usefulness classiﬁer, described in Section 3.3,
achieves an average macro-F1 score of 0.75 and accuracy of 89% in a stratiﬁed
5-fold evaluation on the aforementioned annotated subset of ClariQ. Next, we
6
I. Sekuli´c et al.
Table 2. Performance of baselines and our mixed-initiative approaches on CAsT’22.
Approach/RunID
R@1000 MAP MRR NDCG NDCG@3 NDCG@5
BM25 T5 automatic 0.3244
0.1498 0.5272 0.2987
0.3619
0.3443
BM25 T5 manual
0.4651
0.2309 0.7155 0.4228
0.5031
0.4831
our baseline (DuoT5) 0.3846
0.1680 0.4990 0.3392
0.3593
0.3502
+MI-All
0.4441
0.1741 0.5297 0.3594
0.3722
0.3508
MI-Clf
0.4302
0.1776 0.5144 0.3613 0.3697
0.3581
employ the trained classiﬁer to predict the usefulness of (u′
q, scq, ua) at each
turn in the CAsT’22 dataset. The question scq was classiﬁed as useful in 28%
of turns, while the answer ua in 37%. In the rest 35% of the cases, neither was
predicted to be useful. While the distribution of the predictions is similar to
the prevalence in human-annotated data reported in Table 1, some diﬀerences
can be observed. For example, in CAsT’22 28% of the clarifying questions were
deemed useful, as opposed to the 13% in ClariQ.
Retrieval Performance. Results of the end-to-end conversational passage re-
trieval task, after the applied mixed-initiative answer processing methods (MI-
All and MI-Clf ) are presented in Table 2. For reference, we also include the
organizers’ baselines in the table. We make several observations from the pre-
sented results. First, both methods that utilize mixed-initiative show improve-
ments over the DuoT5 method. This conﬁrms previous ﬁndings on the positive
impact of clariﬁcations in conversational search. Second, diﬀerences between MI-
All and MI-Clf are not statistically signiﬁcant, across all metrics. However, we
note that our classiﬁer-based method utilizes clarifying question or the answer
only when deemed useful, which is in about 70% of the cases in CAsT’22. On
the contrary, MI-All always utilizes both the clarifying question and the answer.
The equal performance of the two methods suggests that our usefulness classiﬁer
successfully includes only relevant information.
Analysis. In cases where the usefulness classiﬁer predicted that neither the clar-
ifying question scq nor the answer ua is useful, we observe a drop of the MI-All
method’s retrieval performance, in terms of nDCG@3 (−13%). Recall, however,
is not impacted by incorporating potentially not useful information and even
shows a slight increase (+3.3%). As this method always appends both scq and
ua to the query u′
q, the performance drop is expected, especially in the re-ranking
stage, as the re-ranker might be confused by the additional non-relevant infor-
mation. Moreover, for both MI methods, we observe higher performance gains
when the answer is useful (+19.8% for MI-All and +23.4% for MI-Clf in re-
call), compared to cases when the question is useful (+12.5% for MI-All and
+8.5% MI-Clf in recall). This could be explained by the fact that users’ answers
are deemed useful when they are longer and thus provide more detail on the
underlying information need [6]. On the contrary, a clarifying question can be
deemed useful even when tangibly addressing the user’s need. In other words,
Usefulness of Clarifying Questions and Answers
7
a good clarifying question can make a small step towards elucidating the user’s
information need. However, the user’s answer can contain detailed expression of
their information need, thus making further gains.
5
Conclusion
In this study, we proposed a classiﬁer-based method, MI-Clf, for processing an-
swers to clarifying questions in conversational search, which extends the original
query only when either is deemed useful. Results on the TREC CAsT’22 dataset
demonstrate clear improvements of the MI-Clf method over non-mixed-initiative
baselines (+12% and +3% relative improvement in terms of Recall@1000 and
nDCG). Moreover, we observed a performance drop for established methods that
always use both the clarifying question and the answer, in cases where neither is
useful (−13% in terms of nDCG@3), thus incorporating noisy information. This
study makes the ﬁrst steps towards improved answer processing methods.
Acknowledgments This research was partially supported by the Norwegian
Research Center for AI Innovation, NorwAI (Research Council of Norway, project
number 309834).
Bibliography
[1] Aliannejadi, M., Kiseleva, J., Chuklin, A., Dalton, J., Burtsev, M.: Building and evaluating
open-domain dialogue corpora with clarifying questions. In: Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing. pp. 4473–4484. EMNLP’20 (2021)
[2] Aliannejadi, M., Zamani, H., Crestani, F., Croft, W.B.: Asking clarifying questions in open-
domain information-seeking conversations. In: Proceedings of the 42nd International ACM SI-
GIR Conference on Research and Development in Information Retrieval. pp. 475–484. SIGIR’19
(2019)
[3] Dalton, J., Xiong, C., Callan, J.: TREC CAsT 2019: The conversational assistance track
overview. In: The Twenty-Eighth Text REtrieval Conference Proceedings. TREC’19 (2019)
[4] Elgohary, A., Peskov, D., Boyd-Graber, J.: Can you unpack that? Learning to rewrite questions-
in-context. In: Empirical Methods in Natural Language Processing. pp. 5918–5924. EMNLP ’19
(2019)
[5] Kiesel, J., Bahrami, A., Stein, B., Anand, A., Hagen, M.: Toward voice query clariﬁcation. In:
The 41st International ACM SIGIR Conference on Research and Development in Information
Retrieval. p. 1257–1260. SIGIR ’18 (2018)
[6] Krasakis, A.M., Aliannejadi, M., Voskarides, N., Kanoulas, E.: Analysing the eﬀect of clarifying
questions on document ranking in conversational search. In: Proceedings of the 2020 ACM
SIGIR on International Conference on Theory of Information Retrieval. p. 129–132. ICTIR ’20
(2020)
[7] Lajewska, W., Balog, K.: From baseline to top performer: A reproducibility study of approaches
at the TREC 2021 Conversational Assistance track. In: Proceedings of the 45th European
Conference on Information Retrieval. pp. 177–191. ECIR ’23 (2023)
[8] Lajewska, W., Bernard, N., Kostric, I., Sekuli´c, I., Balog, K.: The University of Stavanger (IAI)
at the TREC 2022 Conversational Assistance track. In: Proceedings of the Thirty-First Text
REtrieval Conference. TREC ’22 (2023)
[9] Majumder, B.P., Rao, S., Galley, M., McAuley, J.: Ask what’s missing and what’s useful: Im-
proving clariﬁcation question generation using global knowledge. In: Proceedings of the 2021
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies. pp. 4300–4312. NAACL ’21 (2021)
[10] Nogueira, R., Jiang, Z., Lin, J.: Document ranking with a pretrained sequence-to-sequence
model. In: Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 708–
718 (2020)
8
I. Sekuli´c et al.
[11] Owoicho, P., Dalton, J., Aliannejad, M., Azzopardi, L., Trippas, J.R., Vakulenko, S.: TREC
CAsT 2022: Going beyond user ask and system retrieve with initiative and response generation.
In: The Thirty-First Text REtrieval Conference Proceedings. TREC ’22 (2022)
[12] Pradeep, R., Nogueira, R., Lin, J.: The expando-mono-duo design pattern for text ranking with
pretrained sequence-to-sequence models. arXiv cs.IR/2101.05667 (2021)
[13] Qu, C., Yang, L., Croft, W.B., Zhang, Y., Trippas, J.R., Qiu, M.: User intent prediction in
information-seeking conversations. In: Proceedings of the 2019 Conference on Human Informa-
tion Interaction and Retrieval. pp. 25–33. CHIIR ’19 (2019)
[14] Radlinski, F., Craswell, N.: A theoretical framework for conversational search. In: Proceedings
of the 2017 Conference on Human Information Interaction and Retrieval. pp. 117–126. CHIIR
’17 (2017)
[15] Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.:
Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. The Journal
of Machine Learning Research 21(1), 5485–5551 (2020)
[16] Rao, S., Daum´e III, H.: Learning to ask good questions: Ranking clariﬁcation questions using
neural expected value of perfect information. In: Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers). pp. 2737–2746. ACL
’18 (2018)
[17] Reimers, N., Gurevych, I.: Sentence-bert: Sentence embeddings using siamese bert-networks.
In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing. pp. 3982–3992.
EMNLP-IJCNLP ’19 (2019)
[18] Rosset, C., Xiong, C., Song, X., Campos, D., Craswell, N., Tiwary, S., Bennett, P.: Leading
conversational search by suggesting useful questions. In: Proceedings of The Web Conference
2020. p. 1160–1170. WWW ’20 (2020)
[19] Sekuli´c, I., Aliannejadi, M., Crestani, F.: Towards facet-driven generation of clarifying questions
for conversational search. In: Proceedings of the 2021 ACM SIGIR International Conference on
Theory of Information Retrieval. pp. 167–175. ICTIR ’21 (2021)
[20] Song, K., Tan, X., Qin, T., Lu, J., Liu, T.Y.: Mpnet: Masked and permuted pre-training for
language understanding. arXiv preprint arXiv:2004.09297 (2020)
[21] Vakulenko, S., Voskarides, N., Tu, Z., Longpre, S.: A comparison of question rewriting meth-
ods for conversational passage retrieval. In: Advances in Information Retrieval: 43rd European
Conference on IR Research, ECIR 2021. pp. 418–424. ECIR ’21 (2021)
[22] Yu, S., Liu, Z., Xiong, C., Feng, T., Liu, Z.: Few-shot conversational dense retrieval. In: Pro-
ceedings of the 44th International ACM SIGIR Conference on Research and Development in
Information Retrieval. pp. 829–838. SIGIR ’21 (2021)
[23] Zamani, H., Dumais, S., Craswell, N., Bennett, P., Lueck, G.: Generating clarifying questions
for information retrieval. In: Proceedings of The Web Conference 2020. p. 418–428. WWW ’20
(2020)
[24] Zamani, H., Mitra, B., Chen, E., Lueck, G., Diaz, F., Bennett, P.N., Craswell, N., Dumais,
S.T.: Analyzing and learning from user interactions for search clariﬁcation. In: Proceedings of
the 43rd International ACM SIGIR Conference on Research and Development in Information
Retrieval. p. 1181–1190. SIGIR ’20 (2020)
","In recent years, the mixed-initiative paradigm of conversational search has attracted significant research attention. One of the most prominent usages of mixed initiative is asking clarifying questions to elucidate users' underlying information needs. Researchers have begun to study constructing and generating clarifying questions, but work aimed at processing and comprehending user responses to these questions is scarce.nan"
"Frost is a common hazard in meteorology and agriculture, causing significant damage. This study aims to predict frost or minimum ambient temperature accurately and timely. We use three methods: Gated Recurrent Unit (GRU), Temporal Convolutional Network (TCN), and Gradient Boosting (XGBoost). A customized loss function helps reduce prediction errors. Experimental results show that machine learning methods outperform empirical methods, and XGBoost provides the best performance. Our method can model the minimum temperature 24 hours in advance, which is vital for implementing necessary preventive measures.","Frost is a hazardous weather phenomenon that can result in significant agricultural damage. Predicting frost is, therefore, crucial for farmers to take necessary precautions and mitigate potential losses. This study aims to address this problem using machine learning techniques, particularly Gated Recurrent Unit (GRU), Temporal Convolutional Network (TCN), and Gradient Boosting (XGBoost). Our approach involves tailoring a loss function to improve prediction accuracy and focusing on providing timely predictions that enable effective prevention measures against frost.","The study employed three machine learning methods: Gated Recurrent Unit (GRU), Temporal Convolutional Network (TCN), and Gradient Boosting (XGBoost). The input data included three weather features: minimum temperature, maximum temperature, and dew point, collected at 30-minute intervals for eight meteorological stations in Fars province, Iran. A customized loss function was designed to minimize both the mean squared error and the difference between the predicted and actual minimum temperatures. This loss function proved effective in reducing prediction errors for both GRU and TCN methods.","The experimental results demonstrated the effectiveness of machine learning methods in predicting minimum temperature. Compared to empirical methods, machine learning models significantly reduced the Root Mean Squared Error (RMSE), an indicator of prediction accuracy. Moreover, our approach allowed for predictions up to 24 hours in advance, providing ample time for implementing frost prevention measures. Among the three machine learning methods, XGBoost exhibited the best overall performance, consistently achieving the lowest RMSE values across different meteorological stations, indicating its robustness and reliability in this context.","The study successfully demonstrated the superiority of machine learning methods, particularly XGBoost, over empirical methods in predicting minimum temperature. The XGBoost model yielded the most accurate and timely predictions, enabling farmers to take effective preventive actions against frost. The findings highlight the potential of machine learning techniques in addressing agricultural challenges and improving resilience to adverse weather conditions.",Frost Prediction Using Machine Learning Methods in Fars Province,"Milad Barooni, Koorush Ziarati, Ali Barooni","Frost Prediction Using Machine Learning M
in
Milad Barooni  
Department of Computer Science, 
Engineering and  
Information Technology 
Shiraz University 
Shiraz, Iran 
m.barooni@cse.shirazu.ac.ir 
 
Abstract—One of the common hazards and issues in 
meteorology and agriculture is the problem of frost, chilling or 
freezing. This event occurs when the minimum ambient 
temperature falls below a certain value. This phenomenon 
causes a lot of damage to the country, especially Fars province. 
Solving this problem requires that, in addition to predicting 
the minimum temperature, we can provide enough time to 
implement the necessary measures. Empirical methods have 
been provided by the Food and Agriculture Organization 
(FAO), which can predict the minimum temperature, but not 
in time. In addition to this, we can use machine learning 
methods to model the minimum temperature. In this study, we 
have used three methods Gated Recurrent Unit (GRU), 
Temporal Convolutional Network (TCN) as deep learning 
methods, and Gradient Boosting (XGBoost). A customized loss 
function designed for methods based on deep learning, which 
can be effective in reducing prediction errors. With methods 
based on deep learning models, not only do we observ
reduction in RMSE error compared to empirical methods but 
also have more time to predict minimum temperature. Thus, 
we can model the minimum temperature for the next 24 hours 
by having the current 24 hours. With the gradient boosting 
model (XGBoost) we can keep the prediction time as deep 
learning and RMSE error reduced. Finally, we experimentally 
concluded that machine learning methods will work better 
than empirical methods and XGBoost model can have better 
performance in this problem among other implemented.
Keywords—Frost 
Prediction, 
Minimum 
Temperature 
Modeling, GRU, TCN, XGBoost, Deep Learning, 
Boosting 
I. 
INTRODUCTION 
When the temperature reaches zero degree Celsius, due 
to the possibility of ice nucleation, the possibility of rapture 
and disintegration in plant cells increases, and sensitive 
agricultural products can be damaged, which has significant 
effects on production [1] This phenomenon is called Frost. 
An example of its occurrence can be seen in Fig 1.
Surveys show that among the 12 strategic products 
covered by insurance, the most features were related to frost, 
which was an average of 34.8% of the total compensation.
The figures of compensation paid for frost damage in recent 
years have varied between 613 and 1591 billion Rials.
Obviously, these figures do not represent the real damage 
because firstly, not all farmers and gardeners insure their 
products, and secondly, insurance funds pay only a part of 
the damage caused to the products, and the real damage in 
the country is estimated several times the above figure 
Every year, the frost causes a lot of damage in Iran, and Fars 
province and garden products are one of the most affected
Frost Prediction Using Machine Learning M
in Fars Province  
Koorush Ziarati
 
Department of Computer Science, 
 Engineering and 
Information Technology
 
Shiraz University
 
Shiraz, Iran
 
ziarati@shirazu.ac.ir 
 
 
Ali Barooni
Department of Computer Science, 
Engineering and 
Information Technology
Shiraz University
Shiraz, Iran
abarooni@shirazu.ac.ir
One of the common hazards and issues in 
meteorology and agriculture is the problem of frost, chilling or 
freezing. This event occurs when the minimum ambient 
temperature falls below a certain value. This phenomenon 
pecially Fars province. 
Solving this problem requires that, in addition to predicting 
the minimum temperature, we can provide enough time to 
implement the necessary measures. Empirical methods have 
been provided by the Food and Agriculture Organization 
O), which can predict the minimum temperature, but not 
in time. In addition to this, we can use machine learning 
methods to model the minimum temperature. In this study, we 
have used three methods Gated Recurrent Unit (GRU), 
(TCN) as deep learning 
methods, and Gradient Boosting (XGBoost). A customized loss 
function designed for methods based on deep learning, which 
can be effective in reducing prediction errors. With methods 
based on deep learning models, not only do we observe a 
reduction in RMSE error compared to empirical methods but 
also have more time to predict minimum temperature. Thus, 
we can model the minimum temperature for the next 24 hours 
by having the current 24 hours. With the gradient boosting 
can keep the prediction time as deep 
learning and RMSE error reduced. Finally, we experimentally 
concluded that machine learning methods will work better 
than empirical methods and XGBoost model can have better 
emented. 
Prediction, 
Minimum 
Temperature 
XGBoost, Deep Learning, Gradient 
When the temperature reaches zero degree Celsius, due 
ice nucleation, the possibility of rapture 
and disintegration in plant cells increases, and sensitive 
agricultural products can be damaged, which has significant 
This phenomenon is called Frost. 
Fig 1. 
Surveys show that among the 12 strategic products 
covered by insurance, the most features were related to frost, 
which was an average of 34.8% of the total compensation. 
The figures of compensation paid for frost damage in recent 
varied between 613 and 1591 billion Rials. 
Obviously, these figures do not represent the real damage 
because firstly, not all farmers and gardeners insure their 
products, and secondly, insurance funds pay only a part of 
and the real damage in 
the country is estimated several times the above figure [2]. 
causes a lot of damage in Iran, and Fars 
province and garden products are one of the most affected. 
TABLE I.  
DIFFERENT TYPE OF FRO
 
The dew-pint temperature is the temperature reached 
when air is cooled until reaches 100 percent relative 
humidity, and it is a direct measure of the water vapor 
content of the air. There are two types of frost occurs. 
Radiation frosts are common occurrences. 
energy during the day and releasing it at night by the earth, in 
some special conditions during the night until sunrise, the 
heat is quickly lost and the heat rises to the levels due to 
being lighter and cold air replaces it 
Advection frosts occur when cold air blows into an area to 
replace warmer air that was present 
change. This can happen due to the displacement of cold air 
masses. Unlike the previous type, this frostbite can have a 
round-the-clock trend and be wider. Dealing with this type of 
frost is more difficult [3].  
The characteristics of each kind of frosts
I. This study focuses on radiation frost prediction. Due to the 
fact that the forecast is made on each weather station, it is not 
possible to predict the arrival of cold air masses or sudden 
weather changes. Therefore, according to the temperature 
changes of each station during the day, it is possible to 
predict the occurrence of frost at night
addition to this, we have tried to maximize the forecast time 
to take the necessary precautions. 
There can be use two form of protection in frost 
occurrence. Passive methods are those that act in preventive 
terms, normally for a long period of time and whose action 
becomes particularly beneficial when freezing conditions 
occur. Active methods are temporary and they are energy or 
labor intensive, or both. active protection includes heaters, 
Fig. 1. Example of the occurrence of frost in pomegranate and chaghaleh 
plants [4] 
Frost Type 
Characteristics
Radiation 
Clear sky; calm or very little wind
inversion; low dew-pint; air 
0° during day. 
Advection 
Windy and cloudy; No temperature
humidity;  air temperature can be less than 0° during 
day 
Frost Prediction Using Machine Learning Methods 
Ali Barooni 
Department of Computer Science, 
Engineering and  
Information Technology 
Shiraz University 
Shiraz, Iran 
abarooni@shirazu.ac.ir 
 
IFFERENT TYPE OF FROSTS [3] 
temperature is the temperature reached 
when air is cooled until reaches 100 percent relative 
humidity, and it is a direct measure of the water vapor 
There are two types of frost occurs. 
Radiation frosts are common occurrences. Due to receiving 
energy during the day and releasing it at night by the earth, in 
some special conditions during the night until sunrise, the 
heat is quickly lost and the heat rises to the levels due to 
and cold air replaces it around the plant. 
tion frosts occur when cold air blows into an area to 
replace warmer air that was present before the weather 
This can happen due to the displacement of cold air 
masses. Unlike the previous type, this frostbite can have a 
e wider. Dealing with this type of 
of each kind of frosts shown in Table 
This study focuses on radiation frost prediction. Due to the 
fact that the forecast is made on each weather station, it is not 
sible to predict the arrival of cold air masses or sudden 
weather changes. Therefore, according to the temperature 
changes of each station during the day, it is possible to 
at night or the next day. In 
ave tried to maximize the forecast time 
There can be use two form of protection in frost 
occurrence. Passive methods are those that act in preventive 
terms, normally for a long period of time and whose action 
becomes particularly beneficial when freezing conditions 
orary and they are energy or 
active protection includes heaters,  
 
Example of the occurrence of frost in pomegranate and chaghaleh 
Characteristics 
or very little wind; temperature 
pint; air temperature greater than 
temperature inversion; low 
temperature can be less than 0° during 
sprinklers and wind machines, which are used during the 
frost night to replace natural energy loss [3]. This study is 
effective in active prevention methods. Because by 
predicting the occurrence of frost, farmers can take the 
necessary measures to prevent damages. Also, in this case 
study, due to the semi-industrial nature of agriculture in Fars 
province, farmers in large scale need to know about the 
occurrence of frost. Our methods are often suitable for the 
active prevention model. 
A sequence of data collected in a timespan forms a time 
series. These data reflect the changes seen during a certain 
measured time. Therefore, it can be considered a time-
dependent vector. As its name suggests, time series data 
refers to a set of data collected in specific time intervals. In 
this case, if x is a random process, the time series can be 
shown as the equations 1, where t represents time and it can 
also be a random variable. 

x(t0), x(t1), x(t2), …, x(tT),

A time series is basically classified as a dynamic data 
because the values of its features change with time, in other 
words, the value of each point in a time series consists of one 
or more observations that are measured at a specific moment.  
Machine learning algorithms build a model based on 
sample data, known as training data, in order to make 
predictions 
or 
decisions 
without 
being 
explicitly 
programmed to do so. Deep learning is a branch of machine 
learning that base on neural networks which superimpose 
several layers to progressively learn higher-level features 
from the raw input. The most well-known example of deep 
learning is deep neural networks, which have been effective 
in many fields, including voice recognition, image 
classification, object recognition, and natural language 
processing. Deep learning is part of a broader family of 
machine 
learning 
algorithms 
used 
to 
learn 
data 
representations [5]. Gradient boosting is a machine learning 
technique used in regression and classification tasks, among 
others. It gives a prediction model in the form of an 
ensemble of weak learners, which are typically decision 
trees. When a decision tree is the weak learner, the resulting 
algorithm is called gradient-boosted trees and it usually 
outperforms random forest. A gradient-boosted trees model 
is built in a stage-wise fashion as in other boosting methods, 
but it generalizes the other methods by allowing optimization 
of an arbitrary differentiable loss function [6]. 
According to the contents stated in this section, the 
problem of forecasting frost is one of the important and 
prominent issues in the field of meteorology and agriculture, 
which causes a lot of damage to the Iran and Fars province 
every year, and also machine learning methods can be 
considered as an efficient method to solve different 
problems. For this purpose, in this study, we seek to provide 
an efficient method in terms of accuracy and time for the 
problem of frost prediction - or minimum temperature 
prediction at every day - using different methods, including 
deep learning and gradient boosting. 
II. 
REALTED WORK 
Forecasting time series in each application has its own 
methods. To predict the time series of daily temperature - as 
it was said, the prediction of frost is actually the prediction of 
the time series of the minimum temperature - there are 
different approaches, each of which is mentioned below. 
A. Empirical method 
These methods are the oldest temperature forecasting 
methods and use the physical relationships that govern the 
atmosphere that are obtained experimentally, or model the 
minimum temperature through many observations and 
usually using linear regression. One of the most important 
methods is the method proposed by FAO. Most of the 
researches that have been done to improve forecasting have 
tried to compare the error rate of the proposed model with 
the FAO experimental model [3]. 
For example, in 2013, Qarakhani et al. investigated 
Linaker's experimental model for temperature modeling at 
least in four synoptic stations of Fars province and concluded 
that the experimental model of Linaker for Darudzen and 
Shiraz synoptic stations with RMSE of 1.66 and 1.93 
respectively. It is considered a suitable method [7]. Linaker's 
empirical model is discussed in the results section. 
In an article in 2004, Mohammad Nia Qaraei et al. 
presented an empirical formula for each of the synoptic 
stations of North Khorasan, Razavi and South Khorasan 
Provinces in the General Meteorology Department of Razavi 
Khorasan Province in 2004, which is used for the minimum 
temperature at night, which is exactly used for Frost 
prediction. With 12-year statistics from 1992 to 2003, they 
obtained an experimental model whose RMSE error was 
about 2 to 3 in different cities of this province [8]. 
B. Statistical method 
Also, in 2008, Heydari Bani et al presented a statistical 
model in a case study for Shahrekord station, which is used 
to predict the limit temperatures in this station. Also, in 
another study, statistical modeling method and synoptic 
method were performed on Shahrekord station. They found 
that although it has a larger RMSE error of about one degree 
Celsius, but because it is a general model, it can be useful as 
an auxiliary and complementary tool in predicting threshold 
temperatures. They also suggested that the combination of 
synoptic-statistical models could possibly achieve better 
results [9]. 
In an article, Bazrafshan presented a hierarchical 
algorithm for the detection of radiative and advection 
glaciers by analyzing the meteorological data of 51 
meteorological stations of the country. According to two 
temperature thresholds of 4 and 0 degrees Celsius, this 
algorithm separates the three groups of radiant, drifting and 
turbulent frosts for times of the year when frost occurred. 
They also drew a frost zoning map across the country [10]. 
C. Artificial Intelligence method 
Qarakhani et al investigated the problem of minimum 
temperature modeling in Fars province for four stations by 
examining the one-layer artificial neural network model and 
concluded that the artificial neural network model works a 
little better than the experimental model. Their results using 
the neural network for the synoptic stations of Darudzen and 
Shiraz with RMSE were 1.58 and 1.75, respectively, which 
are better results than Linaker's experimental method [7]. 
In a study by Esfandiari et al. in 2013, they used the MLP 
multilayer perceptron model for the city of Saqqez, which 
obtained good results and reported the error of this method to 
be 0.8 siliceous at most. Also, in 2014, they changed their 
method to ANN model for prediction, and reported the same 
results for Saqez city [11]. 
In 2016, Zaytar et al presented a deep neural network 
architecture for use in meteorological data time series, which 
was based on LSTM stack networks. And it predicted the 
same sequence of meteorological values in two different 24
hour and 72-hour models for 9 cities in Morocco.
showed that neural networks based on LSTM performed well 
compared to traditional methods and can be used as a general 
method for forecasting weather conditions [12]
In 2019, in a research conducted by Baroo
network methods such as multi-layer perceptron network and 
support vector machine were compared with 
method. The output of this article was that the use of SVM 
method with RBF radial base kernel has less error compared 
to the other two models. In this research, they used the time 
series information of 4 meteorological stations
once again in that year, in another study, they were able to 
obtain more useful results in terms of RMSE error and model 
prediction time. With the use of LSTM neural network 
regression models and its modeling on 8 stations
the same case study in this article as well as the same data 
received from the same source, our research is a comparative 
research between new methods and these two articles.
III. 
METHODS 
As mentioned, three methods have been used in this 
article, which are described below in order of quality.
methods run on Intel ® Xeon(R) CPU E5-269 
A. Gated Recurrent Unit (GRU) 
GRU or Gated Recurrent Unit architecture wa
introduced in 2014 by Cho et al. This architecture is 
presented in order to solve the shortcomings of the traditional 
recurrent neural network, such as the vanishing gradient 
problem, as well as to reduce the overhead in the LSTM 
architecture. GRU is generally considered as a modified 
version of LSTM because both these architectures use the 
same design and in some cases equally achieve excellent 
results. We mentioned that to solve the vanishing gradient
problem in the traditional neural network, one of
solutions is to use GRU. This type of architecture uses 
concepts called update gate and reset gate. These two so
called gates are basically two vectors that are used to decide 
what information is transmitted to the output and what 
information is not transmitted. The special thing about these 
gates is that these gates can be trained to retain information 
from much earlier steps without changing over time 
(between different time steps) [15]. In Fig. 2, 
different between LSTM and GRU architecture.
Fig. 2. Illustrate of (a) LSTM and (b) GRU with r and z are the reset and 
update gates [15] 
Also, in 2014, they changed their 
, and reported the same 
et al presented a deep neural network 
architecture for use in meteorological data time series, which 
And it predicted the 
same sequence of meteorological values in two different 24-
or 9 cities in Morocco. The results 
showed that neural networks based on LSTM performed well 
compared to traditional methods and can be used as a general 
]. 
oni et al., neural 
layer perceptron network and 
support vector machine were compared with empirical 
of this article was that the use of SVM 
method with RBF radial base kernel has less error compared 
her two models. In this research, they used the time 
series information of 4 meteorological stations [13]. Also, 
once again in that year, in another study, they were able to 
obtain more useful results in terms of RMSE error and model 
the use of LSTM neural network 
regression models and its modeling on 8 stations [14]. Due to 
in this article as well as the same data 
received from the same source, our research is a comparative 
o articles. 
As mentioned, three methods have been used in this 
article, which are described below in order of quality. All the 
269 2.20GHz. 
GRU or Gated Recurrent Unit architecture was 
This architecture is 
presented in order to solve the shortcomings of the traditional 
l network, such as the vanishing gradient 
problem, as well as to reduce the overhead in the LSTM 
generally considered as a modified 
version of LSTM because both these architectures use the 
same design and in some cases equally achieve excellent 
vanishing gradient 
problem in the traditional neural network, one of the 
solutions is to use GRU. This type of architecture uses 
concepts called update gate and reset gate. These two so-
called gates are basically two vectors that are used to decide 
what information is transmitted to the output and what 
ransmitted. The special thing about these 
gates is that these gates can be trained to retain information 
steps without changing over time 
 it is shown the 
tecture. 
 
r and z are the reset and 
Fig. 3. Illustrate of (a) LSTM and (b) GRU with 
update gates [17
B. Temporal Convolutional Networks
As a basic descriptive term for a family of architectures, 
that refers to the presented architecture as a temporal 
convolution network [16]. This archi
features. First, the convolutions in the architecture are ca
meaning that there is no leakage of information from the 
future to the past. Considering that we do not have a
information about the future, this 
necessary for predicting the future temperature. 
architecture can take a sequence of any lengt
an output sequence of the same length, just like an RNN.
Considering that in the GRU network we had a series 
prediction, this case can be a good comparison between these 
two networks [17]. 
As it is clear in the Fig. 3, the architecture of T
models is in a way that does not look into the future and also
tries to consider wider receptive field
look at the input data with a wider viewing window. As 
mentioned, in meteorological data, the model
causal, which TCN networks have this 
For both TCN and GRU methods, the same customized 
loss function is used according to the following 

Loss = (1/48 × (ypred – ytrue)2 ) + |Min(y
The first part of equation 2 is Mean Square Error
But in the second part, we have the minimum temperature 
difference in the predicted and real data
the neural network model to reduce this gap. It is important 
because we generally want to predict 
temperature. So instead of minimize 
temperature sequence, we add a second term to try to 
optimize the difference between true and predicted minimum 
in the next day. This means that in practice we want to bring 
the prediction chart closer to the actual chart in 
change will lead to beneficial improvements 
discussed in the next part. 
C. Gradient Boosting (XGBoost) 
Gradient boosting is a machine learning method for 
regression and classification problems that cr
predictive model in the form of a set of weak learners
Instead of training all the models separately, the ""
process trains the models one after the other. Each new 
model is trained with the aim of correcting the errors caused 
by previous models. Models are added sequentially until 
there is no further improvement possible. The advantage of 
this iterative method is that the added models seek to correct 
the mistakes made by other models. In the standard ensemble 
classification method where models are trained individually,
 
Illustrate of (a) LSTM and (b) GRU with r and z are the reset and 
[17] 
Temporal Convolutional Networks (TCN) 
As a basic descriptive term for a family of architectures, 
that refers to the presented architecture as a temporal 
]. This architecture has important 
he convolutions in the architecture are causal, 
of information from the 
Considering that we do not have any 
 feature of TCNs is 
temperature. Second, the 
architecture can take a sequence of any length and map it to 
an output sequence of the same length, just like an RNN. 
Considering that in the GRU network we had a series 
prediction, this case can be a good comparison between these 
, the architecture of TCN 
models is in a way that does not look into the future and also 
receptive field. This means we can 
look at the input data with a wider viewing window. As 
the model have to be a 
this characteristic. 
For both TCN and GRU methods, the same customized 
loss function is used according to the following equation. 
) + |Min(ypred) – Min(ytrue)|

is Mean Square Error (MSE). 
But in the second part, we have the minimum temperature 
difference in the predicted and real data. This will optimize 
the neural network model to reduce this gap. It is important 
predict the minimum 
 the error for all future 
add a second term to try to 
between true and predicted minimum 
in the next day. This means that in practice we want to bring 
closer to the actual chart in troughs. This 
improvements which we have 
Gradient boosting is a machine learning method for 
regression and classification problems that creates a 
orm of a set of weak learners. 
Instead of training all the models separately, the ""Boosting"" 
process trains the models one after the other. Each new 
model is trained with the aim of correcting the errors caused 
by previous models. Models are added sequentially until 
possible. The advantage of 
is that the added models seek to correct 
the mistakes made by other models. In the standard ensemble 
are trained individually, 
Fig. 4. Boosted regression on linear model [19
all models may make the same mistakes. Gradient boo
refers to a method in which new models are trained with the 
aim of predicting the residuals of previous models
4 shows how the gradient boosting algorit
regression problems. XGBoost is an algorithm that is 
recently used in the field of machine learning. The XGBoost 
algorithm is an implementation of decision tree gradient 
boosting designed for high speed and efficiency
It should be noted that the XGBOOST library uses 
different hyperparameters to fit the model better.
parameters in the model implemented are max
estimators=200, colsample-bytree=0.5, booster=dart and 
rate-drop=0.1. These hyperparameters were 
implementation and the best results were obtained using 
these parameters. These results are described in the next 
section. 
For implementation, we first received the data of the 
desired stations. Then we used the three characteristics of 
these stations for prediction. These characteristics 
minimum temperature, maximum temperature and dew 
point. The characteristics have been measured at each station 
as time series with half-hour intervals. So we have 48 feature
set for each day. Our output in all three models is the next 
day's minimum temperature. 
 
IV. 
RESULTS 
According to the models mentioned in the previous section
the results obtained for each of the models are detailed in the 
respective tables.  
TABLE II.  
GATED RECURRENT NETWORK 
Stations 
Kamfiroz 
Korball 
Eej 
Jaahrom 
Mamasani 
Aliabad 
Avg. 
Train 
RMSE 
1.52 
1.65 
2.05 
1.68 
1.76 
1.56
Best  
Train 
RMSE 
1.26 
1.42 
1.56 
1.52 
1.41 
1.37
Avg. 
Test 
RMSE 
2.26 
1.99 
2.02 
1.97 
1.87 
1.86
Best  
Test 
RMSE 
1.85 
1.66 
1.64 
1.78 
1.69 
1.68
 
d regression on linear model [19] 
Gradient boosting 
refers to a method in which new models are trained with the 
als of previous models. The Fig. 
shows how the gradient boosting algorithm works on 
XGBoost is an algorithm that is 
eld of machine learning. The XGBoost 
algorithm is an implementation of decision tree gradient 
boosting designed for high speed and efficiency [18].  
It should be noted that the XGBOOST library uses 
parameters to fit the model better. The 
max-depth=3, n-
bytree=0.5, booster=dart and 
These hyperparameters were tuned in the 
implementation and the best results were obtained using 
described in the next 
For implementation, we first received the data of the 
desired stations. Then we used the three characteristics of 
characteristics include 
minimum temperature, maximum temperature and dew 
characteristics have been measured at each station 
hour intervals. So we have 48 feature 
for each day. Our output in all three models is the next 
in the previous section 
the results obtained for each of the models are detailed in the 
ETWORK (GRU) 
Lar 
Bavanat 
1.56 
1.61 
1.96 
1.37 
1.35 
1.72 
1.86 
1.79 
2.49 
1.68 
1.47 
2.24 
TABLE III.  
TEMPORAL CONVOLUTIONAL 
Stations 
Kamfiroz 
Korball 
Eej 
Jaahrom 
Mamasani 
Avg. 
Train 
RMSE 
1.29 
1.32 
1.42 
1.49 
1.43
Best  
Train 
RMSE 
1.51 
1.75 
1.70 
1.59 
1.62
Avg. 
Test 
RMSE 
1.80 
2.04 
2.07 
1.74 
1.87
Best  
Test 
RMSE 
1.17 
1.18 
1.3 
1.38 
1.28
 
It should be mentioned that the RMSE error in the model 
means that the model was able to predict tomorrow's 
minimum temperature with this difference in degrees 
Celsius. This means that as thTABLE IV. 
to zero, the number that the model predicted for tomorrow's 
minimum temperature is closer to what 
The table II is related to the GRU model. As can be seen, 
the specified errors are given for 8 meteorological stations. 
These errors include the mean RMSE error for the training 
and test data. Also, the best RMSE result in all runs is also 
Gated recurrent network (GRU) shown in the table
Also, the results obtained with the causal model of TCN
are shown in the table III. As can be seen, deep learning 
models have a good performance on frost prediction. So that 
both their average RMSE error and the best result obtained in 
different epochs perform better than the practical method 
discussed below. 
The results for XGBoost shown in Table IV. 
remarkable point for the method implemented
is that the difference between the best observed error and 
their mean is very small. This means that this method was 
able to achieve consistent results even with the addition of a 
random value in the modeling. This case can be investigated 
in the future. It is also clear that the RMSE of the test data in 
this method has improved compared to other methods
difference between these methods examined in another table
TABLE V.  
GRADIENT BOOSTING 
Stations 
Kamfiroz 
Korball 
Eej 
Jaahrom 
Mamasani 
Avg. 
Train 
RMSE 
1.10 
1.16 
1.03 
1.31 
0.98
Best  
Train 
RMSE 
1.08 
1.14 
1.00 
1.29 
0.96
Avg. 
Test 
RMSE 
1.60 
1.57 
1.48 
1.60 
1.47
Best  
Test 
RMSE 
1.57 
1.53 
1.46 
1.58 
1.44
ONVOLUTIONAL NETWORK (TCN) 
Mamasani 
Aliabad 
Lar 
Bavanat 
1.43 
1.33 
1.26 
1.76 
1.62 
1.65 
1.21 
2.21 
1.87 
1.80 
1.33 
2.36 
1.28 
1.20 
1.17 
1.52 
It should be mentioned that the RMSE error in the model 
means that the model was able to predict tomorrow's 
minimum temperature with this difference in degrees 
TABLE IV. is value gets closer 
el predicted for tomorrow's 
what recorded. 
is related to the GRU model. As can be seen, 
the specified errors are given for 8 meteorological stations. 
These errors include the mean RMSE error for the training 
d test data. Also, the best RMSE result in all runs is also 
shown in the table II. 
ned with the causal model of TCN 
. As can be seen, deep learning 
e on frost prediction. So that 
both their average RMSE error and the best result obtained in 
different epochs perform better than the practical method 
The results for XGBoost shown in Table IV. A 
remarkable point for the method implemented with XGBoost 
is that the difference between the best observed error and 
their mean is very small. This means that this method was 
able to achieve consistent results even with the addition of a 
random value in the modeling. This case can be investigated 
It is also clear that the RMSE of the test data in 
this method has improved compared to other methods. The 
difference between these methods examined in another table. 
OOSTING (XGBOOST) 
Mamasani 
Aliabad 
Lar 
Bavanat 
0.98 
1.26 
1.00 
1.73 
0.96 
1.23 
0.98 
1.70 
1.47 
1.53 
1.22 
2.04 
1.44 
1.53 
1.20 
2.03 
 
Using the proposed loss function can have positive 
effects. The horizontal axis shows the input and output. In 
this way, the first 48 units represent the first day
the second 48 units represent the series of the second day
output, which we want to predict the minimum temperature 
of. In this image, it is clear that if we use the 
function instead of MSE, the model will reduce its distance 
to the minimum temperature. This allows us to provide a 
more accurate prediction in our problem. This result shown 
in Fig. 5. 
The results obtained from the experimental model 
provided by FAO as well as the error difference on the test 
data in each of the three methods with the experimental 
results are given in the table V. As it is clear from this table, 
we can say that XGBoost method has given the best overall 
result in all meteorological stations. Also, the TCN method 
gets better results than the GRU method except for two 
stations, with a small difference, which can be due to its 
architectural properties. In only one station, we have seen the 
superiority of the experimental method over the GRU 
method. In addition to being very small, this difference is not 
very important considering the other results.
important point is that the XGBoost method is 
thant other impelemented methods. 
Fig. 5. Effectivness of custom loss function in different iteratinos 
according to MSE  
the proposed loss function can have positive 
effects. The horizontal axis shows the input and output. In 
this way, the first 48 units represent the first day as input and 
the second 48 units represent the series of the second day as 
, which we want to predict the minimum temperature 
of. In this image, it is clear that if we use the proposed loss 
function instead of MSE, the model will reduce its distance 
to the minimum temperature. This allows us to provide a 
This result shown 
The results obtained from the experimental model 
provided by FAO as well as the error difference on the test 
data in each of the three methods with the experimental 
ar from this table, 
we can say that XGBoost method has given the best overall 
result in all meteorological stations. Also, the TCN method 
gets better results than the GRU method except for two 
stations, with a small difference, which can be due to its 
itectural properties. In only one station, we have seen the 
superiority of the experimental method over the GRU 
method. In addition to being very small, this difference is not 
very important considering the other results. Another 
e XGBoost method is far better 
 
Effectivness of custom loss function in different iteratinos 
TABLE VI.  
COMPARE ALL METHODS W
Methods 
Empirical 
Differences with the
GRU 
Kamfiroz 
1.91 
-0.35 
Korball 
2.20 
0.21 
Eej 
2.22 
0.20 
Jahrom 
3.56 
1.59 
Mamasani 
3.84 
1.97 
Aliabad 
2.31 
0.45 
Lar 
6.43 
4.64 
Bavanat 
2.74 
0.25 
 
V. 
CONCLUSION
The aim of this study was to reduce frost damage by 
using timely prediction of its occurrence. Because the 
prediction time with current methods is only a few hours, 
which is not enough for preventive measures.
To solve the problem of frostbite, it is necessary to know 
its types and understand their characteristics. Considering 
that frostbite is one of the most important and harmful 
hazards in meteorology and agriculture, it is possible to 
prevent these damages through computer calculations and 
artificial intelligence modeling. In the modeling, we used 
three characteristics of minimum temperature, maximum 
temperature and dew point, which were sampled in 30 
minute intervals. These data can be considered as a time 
series. 
Sequence-by-sequence 
deep 
learning 
methods, 
in 
addition to being able to improve the RMSE error, can also 
provide more time to deal with freezing. As mentioned, these 
methods use sequence-by-sequence modeling to predict the 
minimum temperature of the next day that is part of this 
sequence. TCN and GRU methods are among these methods 
that TCN method provides better results in most stations.
these methods, by optimizing the loss function for 
types of networks, an optimal weights
the ability to increase the prediction time
accuracy to a reasonable extent by using the nonlinear power 
characteristics of neural networks and time
data. 
The gradient boosting method implemented by XGBoost, 
with the same input as deep learning models, can 
minimum temperature with faster speed and less error, in 
addition to keeping the prediction time. This error is 
better on average compare to other impelemented
In general, to solve the problem of predicting the minimum 
temperature, which is widely used in frostbite, the XGBoost 
method can offer us the best performance.
For future works, other features can be presented to the 
model. For example, solar radiation or wind speed, etc. It is 
also possible to use the information of the nearby st
each station as input data, which can have a greater impact in 
predicting other types of frost. 
VI. 
ACKNOWLEDGEMENT
We are grateful to the Department of ""New Technologies 
of Fars Province's Agricultural Jihad"" and ""General 
OMPARE ALL METHODS WITH EMPRICAL 
 
Differences with the empirical method 
 
TCN 
XGBoost 
0.11 
0.31 
0.16 
0.63 
0.15 
0.74 
1.82 
1.96 
1.97 
2.37 
0.51 
0.78 
5.10 
5.21 
0.38 
0.70 
ONCLUSION 
study was to reduce frost damage by 
using timely prediction of its occurrence. Because the 
prediction time with current methods is only a few hours, 
which is not enough for preventive measures. 
To solve the problem of frostbite, it is necessary to know 
types and understand their characteristics. Considering 
that frostbite is one of the most important and harmful 
hazards in meteorology and agriculture, it is possible to 
prevent these damages through computer calculations and 
ing. In the modeling, we used 
three characteristics of minimum temperature, maximum 
temperature and dew point, which were sampled in 30 
minute intervals. These data can be considered as a time 
sequence 
deep 
learning 
methods, 
in 
to being able to improve the RMSE error, can also 
provide more time to deal with freezing. As mentioned, these 
sequence modeling to predict the 
minimum temperature of the next day that is part of this 
are among these methods 
that TCN method provides better results in most stations. In 
these methods, by optimizing the loss function for these 
weights was obtained . It has 
the ability to increase the prediction time and improve 
by using the nonlinear power 
and time series property of 
The gradient boosting method implemented by XGBoost, 
with the same input as deep learning models, can predict the 
minimum temperature with faster speed and less error, in 
addition to keeping the prediction time. This error is much 
compare to other impelemented methods. 
In general, to solve the problem of predicting the minimum 
is widely used in frostbite, the XGBoost 
method can offer us the best performance. 
For future works, other features can be presented to the 
model. For example, solar radiation or wind speed, etc. It is 
also possible to use the information of the nearby stations of 
each station as input data, which can have a greater impact in 
CKNOWLEDGEMENT 
We are grateful to the Department of ""New Technologies 
of Fars Province's Agricultural Jihad"" and ""General 
Department of Meteorology of Fars Province"" for providing 
the statistical data of the past years. Certainly, without the 
help of these orginizations, this research would not have 
been possible. 
VII. REFERENCES 
[1] M. J. Nazemosadat, A. R. Sepaskhah, and S. Mohammadi, “A case 
study on the relationship between daily dewpoint and minimum 
temperature in next day in Jahrom in Iran,” Iranian Journal of 
Agricultural Science and Technology, vol. 5, no. 3, pp. 9–17, 2001. 
[2] A. Khalili, “Quantitative evaluation of spring frost risk to agricultural 
and horticultural crops in iranand modeling,” Journal of Agricultural 
Meteorology, vol. 2, no. 1, 2014. 
[3] H. G. Jones, “Frost protection: fundamentals, practice, and 
economics. Volume 1. By R. L. Snyder and J. P. de Melo-Abreu. 
Rome: FAO (2005), pp. 223, US38.00. ISBN 92-5-105328-6 Volume 
2. By R. L. Snyder, J. P. de Melo-Abreu and S. Matulich. Rome: FAO 
(2005), pp. 64. US24.00. ISBN 92-5-10539-4,” Exp. Agric., vol. 42, 
no. 3, pp. 369–370, 2006. 
[4] A. Asadi and S. Karbalaei, “A look at the damages of cold stress on 
agricultural products of the country in 2010,” 2010. 
[5] I. Goodfellow, Y. Bengio, and A. Courville, “Deep Learning”, 
London, England: MIT Press, 2016. 
[6] J. H. Friedman, “Greedy function approximation: A gradient boosting 
machine,” Ann. Stat., vol. 29, no. 5, pp. 1189–1232, 2001. 
[7] A. Gharehkhani, N. Ghahreman, and B. Bakhtiari, “Prediction of 
minimum temperature (chilling) using empirical models and artificial 
intelligence (Case study: Fars province, Iran),” The 2nd International 
conference of Plant, Water, Soil and Weather Modeling, Kerman, 
Iran, 2013. 
[8] S. M. Qaraei, G. A. Rastgo, and S. Malbosi, “Presenting an 
experimental formula for local prediction of the minimum night 
temperature for each of the synoptic stations of North, Razavi and 
South Khorasan provinces,”, Applied scientific conference on dealing 
with frostbite, Yazd, Iran,  2005. 
[9] M. H. Beni, A. Samanipoor, A. Barati, and M. Shiasi, “Comparing 
Between Statistical And Synoptically Model In Forecasting Of 
Extreme Temperatures. (case study shahrekord synoptic station),”, 
The First International conference on Plant, Water, Soil and Weather 
Modeling, Kerman, Iran, 2010. 
[10] J. Bazrafshan, “Radiation, advection and mixed freezing and frost risk 
assessment and zoning in Iran,” Journal of Agricultural Meteorology, 
vol. 2, no. 1, 2014. 
[11] F. Esfandyari, S. A. Hosaini, H. Ahmadi, and K. Mohammadpour, 
“Predictive Modeling of Saghez Township Late Colds spring through 
Multilayer Perceptron (MLP) model,”, The 2nd International 
conference of Plant, Water, Soil and Weather Modeling, Kerman, 
Iran, 2013. 
[12] M. Akram and C. El, “Sequence to sequence weather forecasting with 
long short-term memory recurrent neural networks,” Int. J. Comput. 
Appl., vol. 143, no. 11, pp. 7–11, 2016. 
[13] A. Barooni and K. Ziarati, “Modeling the minimum temperature to 
predict frost in Fars province using neural network, support vector 
machine and emprical models,”, 4th International Congress of 
Developing Agriculture, Natural Resource, Environment and Tourism 
of Iran, Tabriz, Iran, 2019. 
[14] A. Barooni and K. Ziarati, “Modeling minimum temperature in Fars 
province using LSTM recurrent neural network model,”, 1st  National 
Conference on Fundamental Researches in Agricultural and 
Environmental Science, Tehran, Iran, 2019. 
[15] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation 
of gated recurrent neural networks on sequence modeling,” arXiv 
[cs.NE], 2014. 
[16] Lea, Colin, Michael D. Flynn, Rene Vidal, Austin Reiter, and 
Gregory D. Hager. ""Temporal convolutional networks for action 
segmentation and detection."" In proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition, pp. 156-165. 2017. 
[17] Bai, Shaojie, J. Zico Kolter, and Vladlen Koltun. ""An empirical 
evaluation of generic convolutional and recurrent networks for 
sequence modeling."" arXiv preprint arXiv:1803.01271 (2018). 
[18] Chen, Tianqi, and Carlos Guestrin. ""Xgboost: A scalable tree boosting 
system."" In Proceedings of the 22nd acm sigkdd international 
conference on knowledge discovery and data mining, pp. 785-794. 
2016. 
[19] Ridgeway, Greg, David Madigan, and Thomas S. Richardson. 
""Boosting methodology for regression problems."" In Seventh 
International Workshop on Artificial Intelligence and Statistics. 
PMLR, 1999. 
 
 
 
","nanVarious methods have been employed to predict minimum temperature, including empirical models developed by the Food and Agriculture Organization (FAO). These models are widely used, but they often lack timely prediction capabilities. Machine learning methods, on the other hand, offer the potential for more accurate and timely predictions. For example, artificial neural networks have shown promise in modeling minimum temperature. However, their performance can vary depending on the specific network architecture and hyperparameters used."
"Large language models (LLMs) with Transformer architectures have become phenomenal in natural language processing, multimodal generative artificial intelligence, and agent-oriented artificial intelligence. The self-attention module is the most dominating sub-structure inside Transformer-based LLMs. Computation using general-purpose graphics processing units (GPUs) inflicts reckless demand for I/O bandwidth for transferring intermediate calculation results between memories and processing units. To tackle this challenge, this work develops a fully customized vanilla self-attention accelerator, AttentionLego, as the basic building block for constructing spatially expandable LLM processors. AttentionLego provides basic implementation with fully-customized digital logic incorporating Processing-In-Memory (PIM) technology. It is based on PIM-based matrix-vector multiplication and look-up-table-based Softmax design. The open-source code is available online: https://bonany.cc/attentionleg.","The AttentionLego architecture is a fully-customized vanilla self-attention accelerator that serves as the fundamental building block for constructing spatially scalable LLM processors. It utilizes Processing-In-Memory (PIM) technology to boost computing efficiency by collocating memory and processing units on the same physical chip. The primary operations in LLMs are self-attention and feed forward layers, which heavily rely on storage and matrix multiplication, features offered by PIM. This work focuses on the self-attention module part and aims to answer how to utilize the primary matrix-vector multiplication operator (realized by the open-source PIM behavioral model) to construct a baseline design for the self-attention module in LLMs.","This work introduces the AttentionLego design, a vanilla self-attention computation building block implemented in Verilog HDL with the PIM macro behavioral model. It is spatially scalable and can be conveniently tiled up for LLM with repetitive self-attention modules. The AttentionLego architecture comprises five parts: Input Process module, Score module, Softmax module, DMA module, and Top Controller module. Each module is responsible for specific tasks in the self-attention computation process. The Input Process module handles the writing and reading of input weight matrices and performs matrix-vector multiplication. The Score module computes the attention weights using the dot product of Query and Key vectors. The Softmax module applies the softmax nonlinear activation function to the attention weights. The DMA module controls data transfer among modules and between AttentionLego and external extra storage. The Top Controller module manages the pipeline of computing and coordinates communication, data flow, and functional operations between different modules.","This paper presents an initial version of the AttentionLego open-source code, with more quantitative analysis and design details to be released in the future.","This paper presents the basic design of AttentionLego, a vanilla self-attention module developed in Verilog HDL based on the PIM macro behavioral model. By leveraging the efficient storage and computation duality of PIM macro for tensor processing, AttentionLego deploys the matrix multiplications in LLMs onto a PIM-based weight-stationary data flow. A key power-saving technique employed in this work is loading LLM parameters into AttentionLego only once. The source code for the initial version of AttentionLego is released, with more quantitative analysis and design details to come.",AttentionLego: An Open-Source Building Block For Spatially-Scalable Large Language Model Accelerator With Processing-In-Memory Technology,"Rongqing Cong, Wenyang He, Mingxuan Li, Bangning Luo, Zebin Yang, Yuchao Yang, Ru Huang, Bonan Yan","AttentionLego: An Open-Source Building Block For
Spatially-Scalable Large Language Model Accelerator
With Processing-In-Memory Technology
Rongqing Cong∗, Wenyang He∗, Mingxuan Li∗, Bangning Luo∗, Zebin Yang∗,
Yuchao Yang†, Ru Huang†, Bonan Yan†
Peking University
https://bonany.cc/attentionlego
Abstract
Large language models (LLMs) with Transformer architectures have become
phenomenal in natural language processing, multimodal generative artificial in-
telligence, and agent-oriented artificial intelligence. The self-attention module is
the most dominating sub-structure inside Transformer-based LLMs. Computation
using general-purpose graphics processing units (GPUs) inflicts reckless demand
for I/O bandwidth for transferring intermediate calculation results between mem-
ories and processing units. To tackle this challenge, this work develops a fully
customized vanilla self-attention accelerator, AttentionLego, as the basic build-
ing block for constructing spatially expandable LLM processors. AttentionLego
provides basic implementation with fully-customized digital logic incorporating
Processing-In-Memory (PIM) technology. It is based on PIM-based matrix-vector
multiplication and look-up table-based Softmax design. The open-source code is
available online: https://bonany.cc/attentionleg.
1
Introduction
The Transformer architecture has attracted significant attention due to its exceptional performance
in a variety of natural language processing, vision, and multimodal generative tasks [1–8]. The
Transformer was first introduced in 2017 introducing full integration of self-attention mechanism [9].
It can model complex relationships between different parts of a sequence, making it an ideal choice
for handling long-range dependencies and capturing contextual information in sequential signal
processing. Experiments have shown that tiling self-attention beyond a specific scale leads to
emergent abilities of large language models (LLMs), for example, performing planning, arithmetic,
summarizing messages, etc.
The increasing demand for efficient, intelligent devices and systems highlights the importance of
building Large Language Model (LLM) accelerators. LLMs are becoming a key component in
Artificial Intelligence and the Internet of Things (AIoT) to enable the integration of natural language
processing (NLP) capabilities into various Internet of Things (IoT) applications, allowing for more
intuitive and user-friendly interfaces [10]. However, training and deploying LLMs are computationally
intensive and cause unreasonable carbon emissions, making it challenging to scale them to meet
the demands of IoT devices and systems. Besides, the complex computing mechanism of the self-
attention module calls for primers with source code, especially for hardware designers who need a
starting point for LLM accelerator design.
∗All authors contributed equally, listed in alphabetical order by last names.
†Corresponding authors:Yuchao Yang (yuchaoyang@pku.edu.cn), Ru Huang (ruhuang@pku.edu.cn) and
Bonan Yan (bonanyan@pku.edu.cn).
arXiv:2401.11459v1  [cs.AR]  21 Jan 2024
Realization of LLM accelerators should focus on implementing self-attention modules because
the self-attention modules occupy over 68% of operations in the prevailing LLM architectures (as
shown in Fig. 1) [2, 1, 4, 6, 8, 7]. With this observation, this work develops a fully-customized
vanilla self-attention accelerator, AttentionLego. It aims to provide a fundamental building block for
constructing spatially expandable LLM processors. AttentionLego implements hardware computation
for self-attention with fully customized digital logic incorporating Processing-In-Memory (PIM)
technology to boost the computing efficiency. It is based on PIM-based matrix-vector multiplication
and look-up-table-based Softmax design. This work significantly improves the performance and
efficiency of LLMs, making them more accessible to developers and users.
Self Attention
Feed Forward
Embeddings,Normalization, Activation Outside Attention & Residual
LLaMA 2 7B
LLaMA 2 13B
LLaMA 2 70B
GPT-NeoX-20B
Pythia 6.9B
Pythia 12B
0%
20%
40%
60%
80%
100%
88.53%
85.62%
77.63%
68.80%
77.56%
72.95%
11.38%
14.26%
22.19%
30.87%
22.26%
26.83%
0.92‰
1.15‰
1.77‰
3.28‰
1.81‰
2.17‰
Figure 1: Operation number breakdown for popular large language models. Self-attention module
dominates the operation counts in LLMs.1 Multiply-Accumulate (MAC) counts 2 operations. We
unify the operation counts for floating-point numbers and integers.
2
Preliminaries
2.1
Processing-In-Memory Technology
1
1 1
1
1
0 0
0
0 1
0 0
1 0 0 0
1 1 1
0 0 0
0 0
...
...
...
...
...
...
...
1 0 1
1
1
0 0
0
0 1
1 0 1 0 0 0
1 1 1
1 0 1 0 0
...
Compute Circuits
Input I/O
165
161
167
229
33
7
[                                                 ]
y0
yi
yn
...
[                  ]
x0
x1
xm
...
[                  ]
y0
y1
yn
...
[                  ]
x0
x1
xm
T
T
=
Rm×n
R1×m
R1×n
(a)
(b)
(c)
• reconﬁgurable to diﬀerent computing paths
• perform computation locally in memory 
Figure 2: Processing-in-memory macro to perform in situ general matrix-vector multiplication.
Processing-in-memory (PIM) technology is a game-changing innovation that integrates processing
units and memory on the same physical chip [11]. By collocating memory and processing units, PIM
technology eliminates the need for data transfers between the processor and memory, significantly
reducing latency and improving performance in various scientific and engineering applications.
One application where PIM technology can profoundly impact is matrix-vector multiplication, a
fundamental operation in LLMs. According to Fig. 1, the primary operations are self-attention and
feed forward layers. Both heavily rely on storage and matrix multiplication, which are the key features
offered by PIM. Implementation for the feed forward using PIM has been intensely investigated.
Therefore, this work centers on the self-attention module part.
Fig. 2 illustrates the principle of using PIM to execute in-memory matrix multiplications. General
matrix multiplication can be divided into matrix-vector multiplication (Fig. 2(a)). The deep learning
network parameters (synaptic weights) are pre-loaded into the PIM macro array. With the help of PIM
peripheral circuits, an input vector is fed into the PIM macro and interacts with the parameters stored
in the PIM macro (as depicted in Fig. 2(b)). Such PIM macro can be tiled to a spatially expandable
architecture (Fig. 2(c)) to store all the parameters/weights used in deep learning networks. Further,
2
the parallelism of the matrix multiplication per operation is tunable by choosing different design
parameters for the PIM macro. For example, to turn on 4, 8, 16-word lines of the PIM macro at each
computing step to provide different computing throughput by comprising power consumption and
chip area [10, 11]. In this way, PIM macros, with the help of peripheral digital logic controlling
circuits, form accelerators that can exploit the inherent parallelism in matrix-vector multiplication to
achieve even higher speeds and lower latencies. This work aims to answer the question of how to
utilize the primary matrix-vector multiplication operator(realized by the open-source PIM behavioral
model from https://bonany.gitlab.io/pis [12]) to construct a vanilla (baseline) design for
the self-attention module in LLMs.
2.2
Self-Attention Module in Large Language Models
X
N×dmodel
WQ
dmodel×dk
WK
dmodel×dk
WV
dmodel×dv
V
N×dv
Q
N×dk
K
N×dk
P
N×N
S
N×N
(a) Self-Attention
(b) LLaMA Architecture
N×dv
Y
XWQ
QKT
SV
XWK
XWV
Softmax
input tokens
nx
+
+
Self-Attention
(Grouped Multi-Query Attention)
Embeddings
RMS Norm
Feed Forward
SwiGLU
Feed Forward
Softmax
RMS Norm
RMS Norm
output tokens
Q
V
K
Figure 3: (a) Basic block diagram and calculations for the self-attention module. (b) LLaMA 2 model
architecture diagram [2].
A self-attention module in Transformer-like architectures computes the attention weights and output
values for a set of input vectors based on similarity. The computation flow can be broken down into
the following steps:
1. Compute Query, Key, and Value vectors: The input vectors (each token is a vector) are
transformed into three sets of vectors: Query (Q), Key (K), and Value (V ) vectors by multiplying
weight matrices (WQ, WK, WV) to the input vectors using linear transformations.
2. Compute Attention Weights: The attention weights are computed by taking the dot product of
the Query vectors and Key vectors, then apply a softmax function to obtain probabilities. These
probabilities (also called “score”) represent the attention that should be paid to each input vector
when computing the output values.
3. Compute Output Values: The output values are computed by taking the weighted sum of the
Value vectors, where the weights are determined by the attention probabilities calculated in the
previous step. This is done for each input vector, resulting in a set of output vectors that have
considered the relationships between all of the input vectors.
4. Apply Final Linear Transformation: The output values are then passed through a final linear
transformation (feed forward layer) to produce the final output vectors of the self-attention module.
This computation flow allows the self-attention module to focus on different parts of the selectively
input sequence, and compute output values that take into account the relationships between all of the
input vectors.
Formally, given a set of query vectors Q, key vectors K, and value vectors V , the Scale Dot-Product
Attention can be defined as:
Attention(Q, K, V) = softmax(QK⊺
√dk
)V
(1)
3
where QKT denotes the dot product between each query vector and each key vector, and √dk is a
scaling factor that helps to stabilize the gradient during training. The resulting output has the same
shape as the value vectors V , with the same number of elements along each dimension.
3
AttentionLego Design
Fig. 4 illustrates the core idea of this work. This work implements a vanilla self-attention computation
building block with Verilog HDL and the PIM macro behavioral model [12]. It can be conveniently
tiled up for LLM with repetitive self-attention modules.
AttentionLego
AttentionLego
AttentionLego
AttentionLego
AttentionLego
...
...
...
...
This work
Lego Bricks
Lego Product
feedforward modules
& other function blocks
LLM Accelerator
Figure 4: Core idea and the spatial scalability of AttentionLego.
3.1
Architecture
As illustrated in Fig. 5, AttentionLego is divided into 5 parts:
Table 1: AttentionLego Modules
No.
Module
Description
1
Input Process module
compute XWQ, XWK, and XWV
2
Score module
compute QK⊺
3
Softmax module
compute softmax nonlinear activation function in a vector manner
4
DMA module
controls data transfer among modules and between
AttentionLego and external extra storage
5
top controller
controls the pipeline of computing of the above modules
3.2
Input Process Module
The Input Process module is responsible for (a) completing the writing and reading of the input
weight matrices WQ, WK, and WV, and (b) execute the multiplying the input token with each of
the three matrices. The input process module works in 4 possible states:
a) IDLE: stand-by mode, do nothing;
b) WRITE: load pretrained the parameters of WQ, WK, and WV into the input process module;
c) READ: read out the parameters of WQ, WK, and WV for the testing and checking purpose;
d) CIM3: compute Q, K, V with the input X and the pre-loaded WQ, WK, and WV.
The inputs ports of the input process module are:
3We use “CIM” and “PIM” interchangeably.
4
X
N×dmodel
WQ
dmodel×dk
WK
dmodel×dk
WV
dmodel×dv
V
N×dv
Q
N×dk
K
N×dk
P
N×N
S
N×N
N×dv
Y
XWQ
QKT
SV
XWK
XWV
Softmax
Input Process Module
Score Module
Softmax Module
DMA Module
Top Controller
AttentionLego
Figure 5: Architecture of AttentionLego
.
1. Data:
a) Weight data written/Input for CIM calculation: [DATA_WIDTH * D_MODEL-1:0] data_in;
b) Write to column address, range from 0 to 127, only for write mode: [ADDR_COL_WIDTH-
1:0] col_sel;
2. Control:
a) Clock and reset signal, rising edge triggered: clk, rst;
b) Chip selection signal, input is 1 for valid: cs;
c) The mode selection is jointly determined by (web, cimeb): READ: (1,1), WRITE: (0,1),
IDLE: (0,0), CIM: (1,0);
d) Weight matrix selection signal: [2:0] weight_sel. Among them, Q: weight_sel =(0,0); K:
weight_sel =(0,1);V: weight_sel =(1,0);
The output ports are:
1. Data:
a) Output data for CIM mode: [DATA_WIDTH * D_k-1:0] data_out;
b) Read out the output data of (READ) mode: [DATA_WIDTH * ‘ D_MODEL-1:0]
mem_data_out;
2. Control:
a) Feedback completion calculation/read/write: done;
The input process module consists of two parts, which are the APIM module for parameter storage
and calculation, and the control module at the top for data distribution and state control. The operating
process is:
1. Writing weight matrix: Switch to WRITE mode and select a column of a specific matrix
for writing each time. For the Q matrix, writing one column simultaneously requires 128
repetitions to complete, depending on the designed APIM IO bandwidth.
2. Multiplication of Input and Weight Matrix: Switch to CIM mode and feed the inputs (token)
X parts by parts in order to match the stored weights/parameters. After passing in the
data, the matrix-vector multiplication of X with each column vector in the weight matrix is
carried out. It outputs a row vector with its length as dk.
5
3. Reading out the weight matrix: used to test whether all weights are correctly written. Given
the column selection signal, col_sel can read all rows of data for the 32 APIM modules in
this column.
Detailed of each working mode is given as follows:
WRITE mode:1
1. Decompose the input data into an array and write it in parallel to 32 APIM modules, with
the writing position specified by col_sel decision. All are in IDLE state by default.
2. For an APIM module, if the control signal is WRITE, it enters the BUSY state. In BUSY
mode, repeat the following operation 128 times: select a row to write data at the given
column address. Write the next row of data for this column in the next loop until all writes
are completed.
3. After completing the write, switch to the DONE state to reset the state and transmit the
DONE signal.
READ mode:
1. According to col_sel and read weight data stored in 32 APIM modules in parallel.
2. For an APIM module, the control signal enters the BUSY state if the control signal is READ.
In BUSY mode, repeat the following operation 128 times: select a row to read data at the
given column address and store it in a temporary register. Read the next row of data for that
column in the next loop until all readings are completed.
3. After completing the write, switch to the DONE state to reset the state and transmit the
DONE signal.
CIM mode:
1. The function is to organize and add the output results of a single APIM cycle, corresponding
to the external circuit structure of the adder.
2. Due to the parallelism of the APIM module itself, data can be input, calculated, and read out
in parallel. For a given column of addresses, it is necessary to repeat all calculations for that
column 8 times. Due to the parallelism of the column output being 16, the above operation
only needs to be repeated 8 times to complete the calculation for all columns. Therefore,
completing a matrix multiplication requires 64 clock cycles.
Description of the underlying APIM calculation module: Q, K, and V each consisting of 32 APIM
modules to store the entire parameters. Each APIM is a 128×128 square matrix used to store
weights and perform calculations. The stored weights are all 8-bit data. The input parallelism is
16, and a single input port shares 8 rows of addresses.The output parallelism is 16, a single output
port shares 8 columns of addresses, and the ADC accuracy is 6 bits. Detailed can be found in
AttentionLego/InputProcess/src/defines.v.
3.3
Score Module
The Score module computes the score before Softmax shown in Fig. 3, generating the square matrix
QK⊺. AttentionLego design is to provide a template or starting point for fully customized Transformer
accelerators. As exemplary dimensions, we choose the input is a vector with 128 elements that
represents the value of a row in the Q or K⊺ matrix, and the main output is a vector with 2048
elements that represents a row of the calculated QK⊺. The detailed input and output ports are
described as follows:
1. Outputs:
• input_done: 1 bit, to indicate whether a row of K⊺ as input vector has been cached to the
inputs of this module;
• output_done: 1 bit, to indicate whether calculation and transmission of a certain row of
QK⊺ has been done;
• QK_output: 2048×8 bits, to output a certain row of QK⊺ (calculation results).
6
State0
Idle
State1
Busy
State2
Done
(web,cimeb)!=(0,0)
state_load/state_cim 
finish
state_load
/state_cim
next cycle
Figure 6: State-transfer diagram for the Input Process module.
2. Inputs:
• clk: clock;
• cs: chip select as the global enable for the Score module;
• reset: reset signal for the Score module;
• K_mode_enable: high active, controlling to start loading K⊺ row by row to the internal
registers;
• Q_mode_enable:high active, controlling to start calculation with APIM for QK⊺ row by
row;
• K_address: 11 bits, identify which row the input K⊺ vector is in the K⊺ matrix;
• K_input: 128×8 bits, row vector inputs for the K⊺;
• Q_input: 128×8 bits, row vector inputs for the Q;.
Score Module
PIM
col
0
PIM
col
1
PIM
col
63
...
APIM
APIM
APIM
APIM
APIM
APIM.v
total_cim.v
col_cim.v
KT
Q
QKT
input
input
output
(128)
(2048)
Figure 7: Architecture of the Score module
.
This module comprises APIM modules of 32×32 dimension (each APIM can store 32×32 matrix of
weights and perform general matrix-vector multiplication with 32×1 input vector). 4 APIM modules
are vertically arranged to form a 128×32 matrix-vector multiplication engine, called col_cim . Then
we coalesce 64 columns of col_cim to form a 128×2048 PIM module to calculate 2048×2048
QK⊺.
This module has three states: State0=idle, State1=K_mode, State2=Q_mode. State1 and State2 are
enabled by K_mode_enable and Q_mode_enable signals. After each operation is completed in
7
State1 or State2, the Score module returns to State0 and emits inputdone or outputdone signals to
indicate the operations are done.
State0
Idle
State1
K_mode
input and cache K^T matrix
State2
Q_mode
input Q and calculate QK^T row by row
K_mode_enable=1
istep=2^5-1 cycles and output input_done=1
Q_mode_enable=1
idx=2^11-1, 
ostep=2^3-1, 
output 
output_done=1
Figure 8: State-transfer diagram for the Score module.
state
Idle
K_mode
Idle
Q_mode
Idle
K_mode_enable
0
1
input_done
0
1
Q_mode_enable
0
1
output_done
0
1
0
100
150
250
300
350
450
Figure 9: Timing diagram for the Score module.
3.4
Softmax Module
Softmax module computes the softmax function, which is the core operation in the attention-based
Transformer architecture. The function of softmax is formulated as: given ⃗v = [v1, v2, · · · , vn]⊺,
output:
softmax(⃗v) = [a1, a2, · · · , an]⊺, where ai =
evi
Pn
i=1 evi
(2)
This operation can be divided into two steps: first, find the exponents of each element, then do
normalization. The results of QK⊺ block go through the Softmax block, which normalized all
attention coefficients to 1.
This module, as explained in 2-steps, can be divided into two blocks:
1. A look-up table implementation for the exponent function exp_function: input is a 8-bit
fixed-point number x, the output is a 16-bit fixed-point number ex. We use a Python look-up
table generator (AttentionLego/Softmax/src/softmax.py) to generate 256 possible
cases for an 8-bit input x.
2. Normalization block: here, we introduce a method to calculate the summation for all ex and
normalization in two steps (clock cycles). The first cycle loads the inputs and computes the
summation of all ex; the second cycle calculates the normalization.
In the example code, we provide a 32-number softmax implemented by digital logic.
8
State0
Reset
State1
Load Input
State2
Output
we=1
cme=1
we=0,cme=0
Figure 10: State-transfer diagram for the Softmax module.
3.5
Direct Memory Access (DMA) Module
AttentionLego employs a special DMA module that orchestrates all of the data transportation, includ-
ing the inputs and weights transfer between the AttentionLego with external storage as well as the
internal intermediate results. DMA has three channels:
1. The channel between external memory and the Input Process module. The DMA module
converts the data read serially on the bus into parallel data and send it to the PIM module.
2. The channel between the Input Process module and the Score module: feed the calculated
Q or K⊺ to the Score module.
3. The channel between the Score module and the Softmax module.
Fig. 11 illustrates the state transfer diagram for the DMA module. Admittedly the data loading
control for the Input Process module, the Score module and the Softmax module can be fused into
those modules; here we make the DMA module a standalone one for the purpose of adapting the
AttentionLego scheme for various on-chip and off-chip data transfer bandwidth.
3.6
Top Controller Module
This module is mainly responsible for managing and coordinating communication, data flow, and
functional operations between different modules within the chip to ensure that the entire system can
operate normally according to design requirements. This design implements the inference operation
of the transformer’s attention module in the case of a batch size of 1. The entire process includes
each module taking weights from memory and calculating the Q, K, V matrix, attention score, and
softmax. The top controller controls a module to start working through the enable signal. After
the module finishes working, it sends a do signal to the top controller to inform them that the work
is complete so that the top controller can continue to control subsequent work. The top controller
completed the design of the entire process timing logic through different enable and do signals. The
main output is the enable signal for controlling the selection and calculation of the input process
module, the enable signal for controlling the selection, input, and calculation of the attention score
calculation module, the enable signal for controlling the selection and calculation of the softmax
calculation module, and the enable signal for controlling the DMA transmission information. The
main input is the corresponding done signal.
This module consists of a two-layer nested state machine. The outer state machine has four states,
with state 0 being the ready state and containing four inner states. It mainly performs weight loading,
input loading, k-matrix calculation and loading, and the calculation of the q-vector of the first token.
The last three states are the main part of inference, with each loop performing a token inference.
State 1 transfers the q vector of the current token to the attention score calculation module, State 2
9
State0
IDLE
State1
MEM
State2
LD_WEIGHT
State3
LD_SCORE
State4
LD_K
State5
LD_Q
State6
DONE
Figure 11: State-transfer diagram for the DMA module.
calculates the attention score, the q vector of the new token is calculated (not required for the last
loop), and the softmax value of the previous token is calculated (not needed in the first loop), State 3
sends the calculated attention score of the current token to the softmax calculation module. The inner
state machine has four states that execute sequentially. State 0.0 loads weight from memory into the
input process module, state 0.1 loads input from memory into the input process module, state 0.2
completes the calculation of the K matrix in the input process module, state 0.3 loads the K matrix
into the attention score calculation module, and completes the calculation of the q-vector of the first
token.
The Top controller consists of a two-layer nested state machine, with state 0 in the outer state being
the ready state and states 1, 2, and 3 forming a loop. Each loop completes the calculation for a token,
and state 0 contains an inner state machine with four states. The four states proceed sequentially to
complete the preparation phase. The Top controller outputs an enable signal in each stage, and each
time it receives a done signal, it enters the next stage.
The Top controller uses a pipeline to complete token-level parallel inference during the inference
process. In the second state of each loop, the attention score calculation module processes the
current token, the softmax calculation module calculates the previous token, the input process module
calculates the q vector of the next token, and during DMA transmission, the three calculation modules
temporarily stop computing, This can maximize hardware utilization while ensuring the accuracy of
computation and transmission.
4
Conclusion and Perspective
This paper reveals the basic design of AttentionLego. This work develops a vanilla self-attention
module in Verilog HDL based on the PIM macro behavioral model. Thanks to the duality of
efficient storage and computation of PIM macro for tensor processing, this work deploys the matrix
multiplications in LLMs onto a PIM-based weight-stationary data flow. In this process, the parameters
of LLMs are loaded into AttentionLego only once. This is the major power and energy-saving
technique used in this work. This work, for the moment, releases the initial version of the source
code; more quantitative analysis with the proposed AttentionLego method and design are coming up.
10
Acknowledgments and Disclosure of Funding
This work was supported by the National Key R&D Program of China (2023YFB4502200), and
National Natural Science Foundation of China (92264201, 92364102, T2350006), by the 111 Project
under Grant B18001. Partial of this work is the results out of an undergraduate course at Peking
University with course no. 04835370 in 2023’fall semester.
References
[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-
othée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Ro-
driguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient
Foundation Language Models, February 2023. URL http://arxiv.org/abs/2302.13971.
arXiv:2302.13971 [cs].
[2] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,
Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony
Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut
Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,
Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,
Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-
qing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Founda-
tion and Fine-Tuned Chat Models, July 2023. URL http://arxiv.org/abs/2307.09288.
arXiv:2307.09288 [cs].
[3] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c,
Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé,
Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammana-
manchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji
Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen,
Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite,
Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Al-
ham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,
Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani,
Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan,
Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza
Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier
de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing,
Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon
Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz,
Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank
Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant
Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert,
Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza
Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo,
Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne
Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Tor-
rent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala,
Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling,
Chenglei Si, Davut Emre Ta¸sar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht
Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla,
Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen,
Leo Gao, Lintang Sutawika, M. Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak,
Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon
11
Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang,
Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts,
Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan,
Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang,
Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero,
Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhan-
dari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed
Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian,
Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina
Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-
Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken
Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg
Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann,
Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz,
Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov,
Zachary Bamberger, Zdenˇek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar
Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Are-
zoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade,
Bharat Saxena, Carlos Muñoz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky,
Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani,
Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene
Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz,
Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha
Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar,
Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann,
Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh
Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo
Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Ben-
jamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine
Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth,
Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane
Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Ranga-
sai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz,
Maiko Takeuchi, Marc Pàmies, Maria A. Castillo, Marianna Nezhurina, Mario Sänger, Matthias
Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu,
Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad,
Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg,
Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,
Shlok S. Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Sr-
ishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma,
Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu,
Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf.
BLOOM: A 176B-Parameter Open-Access Multilingual Language Model, June 2023. URL
http://arxiv.org/abs/2211.05100. arXiv:2211.05100 [cs].
[4] Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu
Pathria, Marvin Tom, and Joel Hestness. Cerebras-GPT: Open Compute-Optimal Language
Models Trained on the Cerebras Wafer-Scale Cluster, April 2023. URL http://arxiv.org/
abs/2304.03208. arXiv:2304.03208 [cs].
[5] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast
and Memory-Efficient Exact Attention with IO-Awareness, June 2022. URL http://arxiv.
org/abs/2205.14135. arXiv:2205.14135 [cs].
[6] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,
Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth,
Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-
NeoX-20B: An Open-Source Autoregressive Language Model, April 2022. URL http://
arxiv.org/abs/2204.06745. arXiv:2204.06745 [cs].
12
[7] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat
Lee. Textbooks Are All You Need II: phi-1.5 technical report, September 2023. URL http:
//arxiv.org/abs/2309.05463. arXiv:2309.05463 [cs].
[8] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric
Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff,
Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A Suite for Analyzing Large
Language Models Across Training and Scaling, May 2023. URL http://arxiv.org/abs/
2304.01373. arXiv:2304.01373 [cs].
[9] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need, August 2023. URL http:
//arxiv.org/abs/1706.03762. arXiv:1706.03762 [cs].
[10] Bonan Yan, Bing Li, Ximing Qiao, Cheng-Xin Xue, Meng-Fan Chang, Yiran Chen, and
Hai (Helen) Li. Resistive Memory-Based In-Memory Computing: From Device and Large-Scale
Integration System Perspectives. Advanced Intelligent Systems, 1(7):1900068, November 2019.
ISSN 2640-4567, 2640-4567. doi: 10.1002/aisy.201900068. URL https://onlinelibrary.
wiley.com/doi/10.1002/aisy.201900068.
[11] Bonan Yan, Qing Yang, Wei-Hao Chen, Kung-Tang Chang, Jian-Wei Su, Chien-Hua Hsu,
Sih-Han Li, Heng-Yuan Lee, Shyh-Shyuan Sheu, Mon-Shu Ho, Qing Wu, Meng-Fan Chang,
Yiran Chen, and Hai Li. RRAM-based Spiking Nonvolatile Computing-In-Memory Processing
Engine with Precision-Configurable In Situ Nonlinear Activation. In 2019 Symposium on
VLSI Technology, pages T86–T87, Kyoto, Japan, June 2019. IEEE. ISBN 978-4-86348-719-2.
doi: 10.23919/VLSIT.2019.8776485. URL https://ieeexplore.ieee.org/document/
8776485/.
[12] Bonan Yan. PISLIB, January 2024. URL https://bonany.gitlab.io/pis/.
13
","nanPrevious research has demonstrated that self-attention modules occupy over 68% of operations in popular LLM architectures. Therefore, realizing LLM accelerators should focus on implementing self-attention modules. The literature review also highlights the importance of building LLM accelerators for improved efficiency and reduced carbon emissions, as well as the need for primers with source code for hardware designers to facilitate LLM accelerator design."
"The performance of sorting particle data plays a crucial role in Monte Carlo neutron transport simulations. The emergence of unified memory computing chips, such as Apple's silicon chips, has opened up new possibilities for collaboration between CPUs and GPUs. In this study, the sorting of particles on CPU and transport on GPU is investigated using the Apple M2 Max chip. The results show that for the ExaSMR whole core benchmark problems, CPU sorting outperforms GPU sorting, while for the HTR-10 high temperature gas reactor fuel pebble problem, GPU sorting is more efficient.","Being the method with highest fidelity, the Monte Carlo method has been adopted as a verification tool to other methods such as discrete ordinates and the method of characteristics. Because of its heavy computation burden, the Monte Carlo method has not been considered as the everyday reactor simulation tool. The great performance improvement on GPUs demonstrated in recent studies makes the adoption of Monte Carlo method as a routine practice more practical.","The study is conducted using Apple M2 Max chip, which features a unified memory architecture. The programming languages and frameworks used are Objective-C/Swift, Metal Shading Language, and Metal framework. Sorting algorithms are implemented on both CPU and GPU. The performance of sorting is evaluated for various data sizes and levels of data sorting. Reactor simulation benchmarks are employed to compare the performance of CPU and GPU sorting in the context of Monte Carlo neutron transport.","The results show that on the Apple M2 Max chip, CPU sorting performs better than GPU sorting for the ExaSMR whole core benchmark problems. This is attributed partially to the partially sorted order of the particles. The in-house code using both CPUs and GPUs achieves 7.5 times power efficiency that of OpenMC on CPUs for ExaSMR whole core and 50 times for HTR-10 fuel pebble benchmark problems. In contrast, for the HTR-10 fuel pebble problem, GPU sorting outperforms CPU sorting. This suggests that the optimal sorting strategy may vary depending on the specific reactor physics problem.",The study demonstrates the influence of particle sorting algorithms on reactor neutron transport calculations using the Apple unified memory merged CPU and GPU chips. The findings provide insights into the potential benefits of utilizing unified memory devices for Monte Carlo reactor neutron transport simulations. The study also highlights the need for further research to optimize sorting strategies for different reactor physics problems and emerging computing architectures.,Study on the Sorting Performance for Reactor Monte Carlo Neutron Transport on Apple Unified Memory GPUs,Changyuan Liu,"Study on the Sorting Performance for Reactor
Monte Carlo Neutron Transport on Apple Unified
Memory GPUs
1st Changyuan Liu
New Compute Laboratory
Beijing, China
changyuan liu@163.com
Abstract—In simulation of nuclear reactor physics using the
Monte Carlo neutron transport method on GPUs, the sorting
of particles play a significant role in execution performance.
Traditionally, CPUs and GPUs are separated devices connected
with low data transfer rate and high data transfer latency.
Emerging computing chips tend to integrate CPUs and GPUs.
One example is the Apple silicon chips with unified memory. Such
a unified memory chips has opened doors for new strategies of
collaboration between CPUs and GPUs for Monte Carlo neutron
transport. Sorting particle on CPU and transport on GPU is
an example of such new strategy, which has been suffering the
high CPU-GPU data transfer latency on the traditional devices
with separated CPU and GPU. The finding is that for the Apple
M2 max chip, sorting on CPU leads to better performance
than sorting on GPU for the ExaSMR whole core benchmark
problems, while for the HTR-10 high temperature gas reactor fuel
pebble problem, sorting on GPU is more efficient. The features of
partially sorted particle order have been identified to contribute
to the higher performance with CPU sort than GPU for the
ExaSMR problem. The in-house code using both CPUs and GPUs
achieves 7.5 times power efficiency that of OpenMC on CPUs
for ExaSMR whole core and 50 times for HTR-10 fuel pebble
benchmark problems.
Index Terms—sorting, Monte Carlo, neutron transport, GPU,
apple, unified memory
I. INTRODUCTION
Being the method with highest fidelity, the Monte Carlo
method has been adopted as a verification tool to other
methods such as discrete ordinates and the method of charac-
teristics. Because of its heavy computation burden, the Monte
Carlo method has not been considered as the everyday reactor
simulation tool. The great performance improvement on GPUs
demonstrated in recent studies makes the adoption of Monte
Carlo method as a routine practice more practical. Table I
summarizes some recent work.
As discovered by Hamilton [1], particle sorting is important
for achieving high neutron transport performance by increasing
the coherence in execution paths between threads. Joo [2]
further elaborates the particle sorting strategies. In previous
study, most codes as the Pragma [2], Shift [1] and MagicMC
[3] (possibly) use GPUs for particle sorting, and OpenMC [4]
possibly uses CPUs for particle sorting. The Warp [5] code
seems not using the particle sorting strategies.
TABLE I
SUMMARY OF CONTINUOUS ENERGY MONTE CARLO NEUTRON
TRANSPORT CODE WITH GPU SUPPORT
Code
Developer
Sorting on CPUs or GPUs
Warp [5]
Univ. California, Berkeley
N/A
Pragma [2]
Seoul National Univ.
GPUs
Shift [1]
Oak Ridge National Lab.
GPUs
OpenMC [4]
Argonne National Lab.
CPUs (Possibly)
MagiC [3]
Univ. South China
GPU (Possibly)
In-house
In-house
CPUs and GPUs
As indicated in Figure 1, from chips for personal enter-
tainment such as Sony Playstation 5 [6] to chips for high
performance computation such as AMD [7] and Nvidia [8]
merged CPU and GPU chips, the adoption of unified memory
is a trend. This work proposes to use Apple unified memory
computing devices to study the collaboration between CPUs
and GPUs in Monte Carlo neutron transport methods. This
collaboration is previously uneconomic because of the low
data transfer rate and high data transfer latency between CPUs
and GPUs on computing devices with separated CPUs ad
GPUs.
Fig. 1. A snapshot of the design of some recent unified memory chips.
The contribution is summarized as followed.
• Discussion about programming for Apple M2 Max chip
• Study of the sorting performance on CPU/GPU for par-
tially sorted data
• Verification of in-house code with VERA pincell and
assembly benchmark problems
• Comparison of CPU and GPU sorting strategies on the
simulation power efficiency for ExaSMR whole core and
HTR-10 fuel pebble benchmark problems
arXiv:2401.11455v1  [cs.AR]  21 Jan 2024
II. DEVELOPMENT ON APPLE SILICON AS A UNIFIED
MEMORY DEVICE
The Apple silicon chips are system-on-chips (SoCs), where
a cluster of more powerful performance CPU cores, and a
cluster of less powerful efficiency cores, and a cluster of GPU
cores are integrated on the same silicon die. All CPU and
GPU clusters have its private L2 cache, and these clusters are
sharing a L3 cache named as the System Level Cache (SLC).
A. Apple M2 Max Chip
In this work, the Apple M2 Max chip is studied and figure 2
gives a snapshot [9] and an illustration of the chip components.
There are four memory chip surrounding the SoC in the center.
The memory type is LPDDR5, which offers an interface if 512
bit with a bandwidth of 400 GB/s. In most gaming GPUs,
GDDR6 and GDDR6X are the most common types, and in
workstation GPUs, HBM2 and HBM3 are the most common
types. The way of Apple’s use of LPDDR5 is unusual, which
offers benefits including lower power consumption and lower
latency.
Fig. 2. A snapshot [9] (top) and a sketch of the design (bottom) of Apple
M2 Max chip. I-Cache stands for instruction cache, and D-Cache stands for
data cache. Avalanche and Blizzard are architecture design code names.
The SoC includes 8 performance CPU cores sharing 32 MB
L2 cache and 4 efficiency CPU cores sharing 4 MB L2 cache.
The L2 cache is much larger than Intel, AMD and many ARM
based CPUs. There are 38 GPU cores sharing an unknown size
of L2 cache. Moreover, there is a system level cache (SLC)
of 48 MB for all CPU cores and GPU cores.
What makes the Apple SoC unique is that the CPU and
GPU are sharing the same memories and there is a single
SLC for both CPU and GPU. Such a design enables closer
collaboration between CPUs and GPUs. Table II illustrates
some of the difference between Apple SoC and systems
with discrete GPUs. The close connection between CPUs and
GPUs in Apple SoC enables low latency integrated CPU/GPU
algorithms.
TABLE II
COMPARISON OF APPLE SOC AND SYSTEMS OF CPU WITH DISCRETE
GPU
Apple SoC
Discrete GPU
CPU/GPU bus
in-silicon
PCI-E
Memory type
sharing
host/device
GPU memory latency
low
high
B. Objective-C/Swift Programming Languages and Frame-
works
The operating systems MacOS for laptops and workstations,
and iPadOS for tablets, and iOS for mobile phones, and
watchOS for watches, and tvOS for home media stations
are delivered with user interfaces with distinguished styles.
The basic design of such user interfaces are dated back to
the 1980s, where C++ has not yet been prevailing. Another
object-oriented language Objective-C [10] inspired from the
SmallTalk [11] is adopted by Apple to develop the user
interfaces.
Later, in the last decade, the Swift [12] language is further
proposed for meeting the demand of software developers for an
easier to use languages. Applications developed in Objective-
C or Swift are integrated with system frameworks such as
Cocoa [13] for user interfaces and Metal [14] for 3D graphics.
Figure 3 illustrates the layers of applications, frameworks and
OS kernel.
Fig. 3.
A sketch of application development in Objective-C & Swift
programming language on Apple devices.
In the lowest level, Apple computing devices run the Darwin
OS kernel [15], which is different from Linux. Same as
Linux, Darwin implements the POSIX system programming
interfaces. So migration of lower level applications between
Linux and Darwin is much easier than that between Linux and
Windows.
C. Metal Shading Language & Framework
At the beginning, Apple did not design its own program-
ming languages for GPUs. Instead, OpenGL [16] and OpenCL
[17] are adopted, which are open standards conceived by many
vendors.
However, as the Apple GPUs get more powerful, the
OpenGL and OpenCL have been not able to meet the demand
for the dedicated programming patterns on Apple chips. So,
the Metal Shading language [14] has been proposed.
Applications of Metal shading languages rely on the Metal
framework. Although the GPU kernel functions look similar
between CUDA [18] and Metal, there are differences in the
code building stages. Figure 4 illustrates the major difference.
In CUDA, the host code running on CPU and device code
running on GPU are combined in the same CUDA C++ source
code, while in Metal, the host code in Objective-C or Swift and
device code in Metal are separated. Also, in CUDA the CPU
and GPU binaries are packed in a single executable, while
in Metal, the CPU executable will load Metal GPU code in
runtime.
Fig. 4. A sketch of CPU/GPU program compilation scheme on Nvidia and
Apple GPU devices.
D. Apple GPU Programming Patterns
Programming with the Metal framework on Apple GPU
begins with the creation of command queue. Then create
command buffers to submit tasks to GPUs. Each task may
contain multiple stages. Each stage creates a command en-
coder, and each GPU kernel function binds to a command
encoder. After all commands in the buffer are encoded, the
buffer is committed, so that the GPU starts to execute as soon
as possible. Figure 5 illustrates this programming pattern.
III. SORTING ALGORITHMS
A. Summary of Sorting Algorithms on CPU & GPU
In this section, the CPU and GPU sorting algorithms are
summarized
There are two sorting codes on CPU, which are the C++
standard library (stdlib) utility and Intel TBB library. The
C++ stdlib adopts the Introsort algorithm and runs on single
thread. The average, best and worse case time complexity
is O(n log n), where n is the number of elements to sort.
Fig. 5. Programming patterns for Apple GPU.
The Intel TBB library adopts the Quicksort algorithm and
supports multi-thread devices. The Quicksort algorithm has
the same complexity as Introsort, except that the worse case
time complexity is O(n2). As a side notice, Introsort is a
combination of the three algothims: Quicksort, Heapsort, and
Insertion sort.
Because there are no sorting utilities shipped with the Metal
framework, an in-house code has been implemented using
the Bitonic sorting algorithm. The average, best and worse
case time complexity is O(n log2 n). The Bitonic algorithm
requires the data size to be power of 2. Figure III compares
the CPU and GPU sorting algorithms.
TABLE III
SORTING ALGORITHMS ON CPU & GPU
Device
Library
Algorithm
Time complexity
CPU
C++ Stdlib (single thread )
Introsort
O(n log n)
CPU
Intel TBB (multi-thread)
Quicksort
O(n log n)
GPU
In-house
Bitonic
O(n log2 n)
The time complexity is only a guidance, and the next
two subsections propose two experiments to illustrate the
performance on Apple chip.
B. Performance of Sorting on Apple Chip
1) Random Integers: The first experiment studies the sort-
ing algorithms on an array of integers randomly sampled. If
there are n integers, then each integer is sampled using a
uniform distribution between 0 and n − 1. Figure 6 compares
the time cost of sorting algorithms for integer array with size
from 29 to 224.
On Log-Log scale, the plot of time cost versus data size
appears like straight lines. On GPU, this ‘straight line’ ap-
pearance does not extend well below 105. This is because of
the GPU execution overhead. Notice that the time measured is
purely the GPU execution cost, not including the GPU kernel
launch cost on the CPU side.
2) Partially Sorted Integers: It worths notice that the per-
formance of sorting is limited by memory bandwidth. So, for
partial sorted data, since there are less data move operations
than fully random data, some algorithms may perform better.
To test the performance of sorting of partially sorted inte-
gers, it begins with an array of fully sorted integers. If the are
n integers, then the array is 0, 1, 2, . . . n − 1. Next, define a
Fig. 6. Comparison of time cost of sorting algorithms for integer array with
size from 29 to 224
ratio of swap r, and randomly swap ⌊nr⌋ pairs of integers in
the array, with the pair indices randomly sampled. Here, ⌊nr⌋
takes the max integer less or equal to nr. Figure 7 shows the
time cost for an array of size 223 with ratio of swap r from
10−7 to 1. When r = 10−7, there are no swaps, so the ratio
of swap is essentially 0.
Fig. 7. Comparison of time cost of sorting algorithms for integer array with
size 223 and ratio of swaps from 0 to 1.
When the number of swaps is varied, the GPU Bitonic
algorithm performance keeps nearly the same, but the CPU
algorithms drastically varies. When there are less than 10−5
of elements are swapped, CPU performs better than GPU.
C. Sorting Strategy for Monte Carlo Neutron Transport
The particle sorting algorithm is important for accelerating
Monte Carlo neutron transport on GPU. Liu [19] discusses the
sorting algorithm on Apple computing devices.
IV. REACTOR SIMULATION BENCHMARKS
A. Simulation Configuration
The previous discussion of sorting algorithm on integer
arrays is limited, and the results may not reflect the situation
of reactor physics simulation. In this section, the VERA
pincell and assembly problems [20] are simulated to verify
the correctness of the program. Then the ExaSMR [21] whole
core and HTR-10 [22] fuel pebble benchmark problems are
simulated to study the performance.
The in-house code on GPU uses 32-bit floating point
number since Apple GPUs only support 32-bit floating point
numbers. Instead, OpenMC uses 64-bit floating point numbers.
The cross sections are prepared in an optimal set of 13
temperatures for the kernel reconstruction Doppler broadening
method, which is suitable for neutron transport in continuously
variable media. [23] For OpenMC, cross sections at the same
set of temperatures are used, and the 2-point interpolation
Doppler broadening method is used.
The in-house code tallies flux of a 23-group structure and
the power. OpenMC code tallies nothing. Table IV summarizes
the simulation configuration.
TABLE IV
SIMULATION CONFIGURATION FOR NEUTRON TRANSPORT
In-house Code
OpenMC Code
Floating precision
32-bit (single)
64-bit (double)
Unresolved resonance
turned off
turn off
Resonance scattering
turned off
turn off
Thermal scattering S(α, β)
turned off
turn off
Cross section
300, 304.252, 338.681, 412.408, 530.512, 705.793,
temperatures (K)
951.89, 1283.538, 1704.703, 2189.43, 2653.095,
2950, 3000
Doppler broadening
kernel reconstruction
2-point linear interpolation
Tally
23-group flux and power
None
Nuclear data library
ENDF/B-VIII.0
ENDF/B-VIII.0
B. Verification: VERA Pincell & Assembly Benchmark Prob-
lem
In order to verify simulation on Apple GPU, the VERA
pincell and assembly benchmark problems are studied. Table V
compares K-effective values between in-house code on Apple
M2 Max CPU+GPU and OpenMC code on Apple M2 Max
CPU.
TABLE V
K-EFFECTIVE OF VERA PINCELL ASSEMBLY BENCHMARK PROBLEMS
In-house
OpenMC
In-house
OpenMC
CPU+GPU
CPU only
CPU+GPU
CPU only
1A
1.18705 (8)
1.18805 (8)
2E
1.06910 (7)
1.06995 (9)
1B
1.18190 (9)
1.18290 (10)
2F
0.97484 (8)
0.97557 (8)
1C
1.17186 (9)
1.17257 (9)
2G
0.84713 (6)
0.84804 (9)
1D
1.16345 (10)
1.16405 (9)
2H
0.78723 (7)
0.78799 (8)
1E
0.77405 (7)
0.77529 (6)
2I
1.18092 (8)
1.18178 (8)
2A
1.18315 (8)
1.18391 (8)
2J
0.97392 (8)
0.97481 (8)
2B
1.18398 (8)
1.18471 (8)
2K
1.02330 (8)
1.02385 (8)
2C
1.17466 (8)
1.17532 (9)
2L
1.02126 (7)
1.02146 (9)
2D
1.16689 (8)
1.16772 (8)
2M
0.94233 (6)
0.94209 (9)
The GPU code underestimates the K-effectives within 100
pcm, and the using of single precision floating point numbers
play the important role in the discrepancy.
C. Performance Study: ExaSMR Whole Core Benchmark
Problem
Next, the influence of the sorting on a whole core nuclear
reactor has been studied with the ExaSMR benchmark prob-
lems. Table VI summarizes these problems. There are two
versions, one contains fresh fuel with only 7 nuclides in fuel,
and the other one contains depleted fuel with 245 nuclides in
fuel.
TABLE VI
SUMMARY OF EXASMR WHOLE CORE BENCHMARK SIMULATION
Fresh fuel
Depleted fuel
Number of nuclides
76
283
Number of nuclides in fuel
7
245
Number of cycles
350
Number of inactive cycles
100
OpenMC particles per cycle
1,048,576 (220)
In-house code particles per cycle
8,388,608 (223)
OpenMC tally
None
In-house code tally
fission power + group fluxes
K-effective OpenMC CPU only
1.00656 (6)
1.00660 (5)
K-effective In-house CPU+GPU
1.00587 (2)
1.00586 (2)
The simulation performance is summarized in Table VII.
The sorting on CPU performs better than sorting on GPU.
This attributes partially to the partially sorted order in the
particles. For the fresh fuel problem, the in-house code with
GPU transport achieves about 3.0 times power efficiency that
of OpenMC, and about 7.5 times for the depleted fuel problem.
The power efficiency has been visualized in Figure 8.
TABLE VII
PERFORMANCE OF SORTING FOR EXASMR WHOLE CORE BENCHMARK
PROBLEMS
In-house
In-house
OpenMC
sorting on CPU
sorting on GPU
active cycles
active cycles
active cycles
(particles/s/Watt)
(particles/s/Watt)
(particles/s/Watt)
Fresh fuel
4.5E3
3.7E3
1.5E3
Depleted fuel
3.0E3
2.5E3
4.0E2
Fig. 8. Comparison of simulation efficiency in particle per second per Watt
for the ExaSMR whole core benchmark problem.
D. Performance Study: Pebble Fuel from HTR-10 Test Reactor
In order to verify the influence of sorting algorithms on
emerging high temperature gas reactors with distinguished
design from the light water reactors, the fuel pebble benchmark
problem of the HTR-10 test reactor has been studied. The
definition of the HTR-10 pebble benchmark problem and the
simulation configuration is summarized in Table VIII.
TABLE VIII
SUMMARY OF HTR-10 BENCHMARK SIMULATION
HTR-10 fuel pebble
Pebble/fuel region radius (cm)
3.0/2.5
Triso particles in fuel region
8,335
Triso fuel/buffer/PyC1/SiC/PyC2
0.025/0.034/0.038/0.0415/0.0455
layers outer radius (cm)
Number of nuclides
10
Number of nuclides in fuel
5
OpenMC particles per cycle
32,768 (215)
In-house code particles per cycle
1,048,576 (220)
OpenMC tally
None
In-house code tally
None
The simulation performance of both in-house code using
CPU/GPU sorting and the OpenMC code on CPU is sum-
marized in Table IX. Unlike the ExaSMR whole benchmark,
the GPU sorting algorithms perform better than CPU sorting.
And the in-house code on GPU is about 50 times more power
efficient than OpenMC on CPU.
TABLE IX
PERFORMANCE OF SORTING FOR HTR-10 BENCHMARK PROBLEMS
In-house
In-house
OpenMC
sorting on CPU
sorting on GPU
(particles/s/Watt)
(particles/s/Watt)
(particles/s/Watt)
HTR-10
2.0E2
3.3E2
7.2
fuel pebble
V. CONCLUSIONS
In this work, the influence of particle sorting algorithms on
the VERA pin and assembly, ExaSMR whole core, and HTR-
10 fuel pebble benchmark problems have been studied with the
Apple unified memory merged CPU and GPU chips. First, it
has reviewed the programming on Apple silicon chips. Second,
it has demonstrated that with partially sorted data the sorting
on Apple M2 Max CPU can outperform GPU. Third, it has
verified the correctness of the in-house GPU code with VERA
pin and assembly benchmarks. Fourth, it has given evidence
that the CPU sort is more in favor of for the ExaSMR whole
core benchmark, and the in-house GPU code achieve 3.0 and
7.5 times power efficiency that of OpenMC CPU code for
the case of fresh and depleted fuel. And finally, it has shown
that the GPU sort is more efficient in power than CPU sort for
the HTR-10 fuel pebble benchmark problem, and the in-house
GPU code achieve 50 times power efficiency that of OpenMC
CPU code. In the future, when unified memory merged CPU
and GPU chips are prevailing, sorting on the CPU might better
have been studied in order to get better power efficiency for
Monte Carlo reactor neutron transport calculations.
ACKNOWLEDGMENT
Computing technologies from New Compute Laboratory are
used to produce parts of the data in this article. New Compute
Laboratory & its information providers endeavor to ensure the
accuracy & reliability of the information provided, but do not
guarantee completeness or reliability, or that it is up-to-date &
accepts no liability (whether in tort or contract or otherwise)
for any loss or damage, whether direct or indirect, arising from
errors, inaccuracies or omissions or the information being up-
to-date. Any information provided is at the user’s risk.
REFERENCES
[1] S. Hamilton and T. Evans, “Continuous-energy Monte Carlo neutron
transport on GPUs in the Shift code,” Annuals of Nuclear Energy, vol.
128, pp. 236-247, 2019.
[2] N. Choi and H. Joo, “Domain decomposition for GPU-based continuous
energy Monte Carlo power reactor calculation,” Nuclear Engineering and
Technology, vol. 52, issue 11, pp. 2667-2677, 2020.
[3] K. Gao, Z. Chen, A. Sun and T. Yu, “The research and application of
GPU-based Monte Carlo Simulation in reactor calculation,” Proceedings
of RPNM2023, Jul. 26-29 Lanzhou China, 2023.
[4] J. Tramm, P. Romano, J. Doerfert, A. Lund, P. Shriwise, A. Siegel,
and et. al., “Toward Portable GPU Acceleration of the OpenMC Monte
Carlo Particle Transport Code,” Proceedings of PHYSOR2022, May 15-
20 Pittsburg USA. 2022.
[5] R. Bergmman, J. Vuji´c, “Algorithmic choices in WARP - A framework
for continuous energy Monte Carlo neutron transport in general 3D
geometries on GPUs,” Annuals of Nuclear Energy, vol. 77, pp. 176–
193, 2015.
[6] Sony Playstation 5, https://www.playstation.com/en-us/ps5/ (Last re-
trieved: Jan. 21, 2024)
[7] AMD
Instinct™
MI300A
Accelerators,
https://www.amd.com/en/products/accelerators/instinct/mi300/mi300a.html
(Last retrieved: Jan. 21, 2024)
[8] NVIDIA Grace Hopper Superchip, https://www.nvidia.com/en-us/data-
center/grace-hopper-superchip/ (Last retrieved: Jan. 21, 2024)
[9] Apple
unveils
M2
Pro
and
M2
Max:
next-
generation
chips
for
next-level
workflows,
https://www.apple.com/newsroom/images/product/mac/standard/Apple-
M2-chips-M2-Max-230117.zip (Last retrieved: Jan. 21, 2024)
[10] About Objective-C, https://developer.apple.com/library/archive/ docu-
mentation/Cocoa/Conceptual/ProgrammingWithObjectiveC/Introduction/
Introduction.html (Last retrieved: Jan. 21, 2024)
[11] GNU Smalltalk, https://www.gnu.org/software/smalltalk/ (Last retrieved:
Jan. 21, 2024)
[12] Swift: the powerful programming language that’s also easy to learn,
https://developer.apple.com/swift/ (Last retrieved: Jan. 21, 2024)
[13] What Is Cocoa?, https://developer.apple.com/library/archive/ documen-
tation/Cocoa/Conceptual/CocoaFundamentals/WhatIsCocoa/ WhatIsCo-
coa.html (Last retrieved: Jan. 21, 2024)
[14] Accelerate
graphics
and
much
more
with
Metal,
https://developer.apple.com/metal/ (Last retrieved: Jan. 21, 2024)
[15] Kernel Architecture Overview, https://developer.apple.com/library/archive/
documentation/Darwin/Conceptual/KernelProgramming/Architecture/
Architecture.html (Last retrieved: Jan. 21, 2024)
[16] OpenGL: The Industry’s Foundation for High Performance Graphics,
https://www.opengl.org (Last retrieved: Jan. 21, 2024)
[17] OpenCL: Open Standard for Parallel Programming of Heterogeneous
Systems, https://www.khronos.org/opencl/ (Last retrieved: Jan. 21, 2024)
[18] CUDA Toolkit, https://developer.nvidia.com/cuda-toolkit (Last retrieved:
Jan. 21, 2024)
[19] C. Liu, “Monte Carlo neutron transport using low power mobile GPU
devices”, Arxiv, https://arxiv.org/abs/2208.06296, 2022
[20] B. Godfrey, “VERA core physics benchmark progression problem
specifications, revision 4,” CASL technical report CASL-U-2012-0131-
004, 2014.
[21] E.
Merzari,
S.
Hamilton,
T.
Evans,
M.
Min
and
et.
al.,
“Exascale Multiphysics Nuclear Reactor Simulations for Advanced
Designs,”
Proceedings
of
SC23,
Nov.
12-17
Denver
USA,
https://doi.org/10.1145/3581784.3627038, 2023
[22] International Handbook of Reactor Physics Experiments, “Evaluation of
the Initial Critical Configuration of the HTR-10 Pebble-Bed Reactor,”
HTR10-GCR-RESR-001, NEA/NSC/DOC(2006)1, Rev. 0., 2006
[23] C. Liu, “Doppler broadening using discrete cosine transform and kernel
reconstruction for spatially variable media,” Annuals of Nuclear Energy,
vol. 174, pp. 109150, 2012.
","Particle sorting is important for achieving high neutron transport performance by increasing the coherence in execution paths between threads. Previous studies have mostly used GPUs for particle sorting, while the OpenMC code possibly uses CPUs for particle sorting. The Warp code does not seem to use particle sorting strategies.nan"
"Generative AI models face the challenge of hallucinations that can undermine users’ trust in such systems. We approach the problem of conversational information seeking as a two-step process, where relevant passages in a corpus are identified first and then summarized into a final system response. This way we can automatically assess if the answer to the user’s question is present in the corpus. Specifically, our proposed method employs a sentence-level classifier to detect if the answer is present, then aggregates these predictions on the passage level, and eventually across the top-ranked passages to arrive at a final answer-ability estimate. For training and evaluation, we develop a dataset based on the TREC CAsT benchmark that includes answerability labels on the sentence, passage, and ranking levels. We demonstrate that our proposed method represents a strong baseline and outperforms a state-of-the-art LLM on the answerability prediction task.","Conversational information seeking (CIS) systems allow users to fulfill their complex information needs via a sequence of interactions. This problem is often approached as a passage retrieval task, rather than employing generative AI techniques, to allow for the grounding of responses in supporting documents and to avoid hallucinations. However, the ultimate goal is to return informative, concise, and reliable answers instead of top-ranked passages. In an ideal scenario, when the passages from the top of the ranking answer the question, the task of response generation boils down to summarization. However, it is often the case that the answer to the user’s question is not contained in the top retrieved passages.","As our first main contribution, we develop a dataset, based on the TREC CAsT benchmark, to train and evaluate methods for question answerability prediction. Utilizing an existing resource of snippet-level answer annotations, our dataset provides answerability labels on three levels: (1) sentences, (2) passages, and (3) rankings (i.e., top-ranked passages retrieved by a CIS system). Notably, we generate input passage rankings with various degrees of difficulty in answerability prediction, mixing passages that contain answers with those with no answers, in a controlled way. As a result, passage rankings range from all passages containing an answer to “no answer found in the corpus.”
As our second main contribution, we provide a baseline approach for predicting answerability based on an input ranking. Our proposed approach predicts which sentences from the top-ranked passages contribute to the answer and aggregates the obtained answerability scores on the passage and ranking levels. We demonstrate that this simple method provides a strong baseline that outperforms ChatGPT-3.5 on the same task. Further, we show that augmenting our dataset with additional training samples for unanswerable question detection (from the SQuAD 2.0 dataset) does not improve ranking-level answerability prediction in conversational search, underscoring the distinct character of this task.","Table 2 presents the answerability results on the sentence-, passage-, and ranking- levels on the test partition of CAsT-answerability in terms of accuracy. Does data augmentation help answerability detection? On the sentence level, we find that augmenting the CAsT-answerability dataset with additional training examples from SQuAD 2.0 improves performance. These improvements also carry over to the first aggregation step on the passage level. However, the best ranking-level results are obtained by aggregating results obtained from the classifier trained only on CAsT-answerability. It may result from the fact that SQuAD 2.0 training data focuses on questions with short-span answers (like entities or numbers) confined to a single sentence. This could mislead the classifier to overlook answers spanning multiple sentences or passages. Thus, while sentence-level answerability prediction benefits from augmented data, this does not translate to effective passage or ranking-level answerability prediction.
Which of the two aggregation methods performs better? In all cases, max aggregation on the passage level followed by mean aggregation on the ranking level gives the best results. Intuitively, this configuration captures single sentences with high answerability scores in individual passages (max aggregation on passage level) that give a high average score for the whole ranking (mean aggregation on ranking level) for answerable questions.","Unanswerable questions pose a challenge in conversational information seeking. To study this problem, we have developed a test collection, based on two editions of the TREC CAsT benchmark, with sentence-, passage-, and ranking-level answerability labels. We have also presented a baseline approach based on the idea of sentence-level answerability classification and multi-step results aggregation, and evaluated multiple instantiations of this approach with different configurations. Despite their simplicity, our baselines have been shown to outperform a state-of-the-art LLM on the task of answerability prediction.",Towards Reliable and Factual Response Generation: Detecting Unanswerable Questions in Information-Seeking Conversations,"Weronika Łajewska, Krisztian Balog","Towards Reliable and Factual Response
Generation: Detecting Unanswerable Questions
in Information-Seeking Conversations
Weronika  Lajewska[0000−0003−2765−2394] and
Krisztian Balog[0000−0003−2762−721X]
University of Stavanger, Stavanger, Norway,
{weronika.lajewska, krisztian.balog}@uis.no
Abstract. Generative AI models face the challenge of hallucinations
that can undermine users’ trust in such systems. We approach the prob-
lem of conversational information seeking as a two-step process, where
relevant passages in a corpus are identified first and then summarized
into a final system response. This way we can automatically assess if
the answer to the user’s question is present in the corpus. Specifically,
our proposed method employs a sentence-level classifier to detect if the
answer is present, then aggregates these predictions on the passage level,
and eventually across the top-ranked passages to arrive at a final answer-
ability estimate. For training and evaluation, we develop a dataset based
on the TREC CAsT benchmark that includes answerability labels on the
sentence, passage, and ranking levels. We demonstrate that our proposed
method represents a strong baseline and outperforms a state-of-the-art
LLM on the answerability prediction task.
Keywords: Conversational search · Conversational response generation
· Unanswerability detection
1
Introduction
Conversational information seeking (CIS) systems allow users to fulfill their com-
plex information needs via a sequence of interactions. This problem is often ap-
proached as a passage retrieval task [5, 14], rather than employing generative
AI techniques, to allow for the grounding of responses in supporting documents
and to avoid hallucinations. However, the ultimate goal is to return informative,
concise, and reliable answers instead of top-ranked passages. In an ideal scenario,
when the passages from the top of the ranking answer the question, the task of
response generation boils down to summarization [15]. However, it is often the
case that the answer to the user’s question is not contained in the top retrieved
passages. In such cases, summaries generated from those passages would result
in hallucinations [3, 22].
In this paper, we make the first step towards reliable and factual conversa-
tional response generation. We propose a mechanism for detecting unanswerable
This is the author’s version of the work. It is posted here for your personal use. The definitive version is published in:
Proceedings of the 46th European Conference on Information Retrieval (ECIR ’24), March 24–28, 2024, Glasgow, Scotland
arXiv:2401.11452v1  [cs.IR]  21 Jan 2024
2
 Lajewska and Balog
questions for which the correct answer is not present in the corpus or could not
be retrieved. More specifically, given a set of top-ranked passages that have been
identified as most relevant to the given question, we predict if the question can
be answered (at least partially) based on information contained in those pas-
sages. This enables us to move beyond the notion of passage relevance and focus
more on the actual presence of the information that answers the question. Intro-
ducting this additional step of answerability prediction in the CIS pipeline, to be
performed after the passage retrieval and before the response generation steps,
could help mitigate hallucinations and factual errors. It would enable the system
to transparently communicate to the user if the answer to the query could not be
found, instead of generating a response from only marginally relevant passages.
Unanswerability detection has been addressed in the context of machine read-
ing comprehension [9, 10] and question answering [4, 18, 19, 21], both of which
differ significantly from the conversational search setup. Information-seeking di-
alogues pose additional challenges, such as open-ended questions requiring de-
scriptive answers, indirect answers requiring an inference or background/context
knowledge, and complex queries with partial answers spread across passages.
Therefore, unanswerability detection is a novel, still unsolved task in CIS, and,
to the best of our knowledge, no public dataset exists for this problem.
As our first main contribution, we develop a dataset, based on the TREC
CAsT benchmark, to train and evaluate methods for question answerability pre-
diction. Utilizing an existing resource of snippet-level answer annotations [12],
our dataset provides answerability labels on three levels: (1) sentences, (2) pas-
sages, and (3) rankings (i.e., top-ranked passages retrieved by a CIS system).
Notably, we generate input passage rankings with various degrees of difficulty in
answerability prediction, mixing passages that contain answers with those with
no answers, in a controlled way. As a result, passage rankings range from all
passages containing an answer to “no answer found in the corpus.”
As our second main contribution, we provide a baseline approach for pre-
dicting answerability based on an input ranking. Our proposed approach pre-
dicts which sentences from the top-ranked passages contribute to the answer
and aggregates the obtained answerability scores on the passage and rank-
ing levels. We demonstrate that this simple method provides a strong base-
line that outperforms ChatGPT-3.5 on the same task. Further, we show that
augmenting our dataset with additional training samples for unanswerable ques-
tion detection (from the SQuAD 2.0 dataset [18]) does not improve ranking-
level answerability prediction in conversational search, underscoring the distinct
character of this task. Our benchmark dataset (CAsT-answerability) as well
as the implementation of our proposed method are made publicly available at
https://github.com/iai-group/answerability-prediction.
2
Related Work
Research on information-seeking conversations is largely driven by test collec-
tions developed as part of the TREC Conversational Assistance Track (CAsT) [5–
Towards Reliable and Factual Response Generation
3
Table 1. Statistics for the CAsT-answerability dataset.
Answerable?
Yes
No
#question-sentence pairs (train+test) 6,395
19,043
#question-passage pairs (train+test) 1,778
1,932
#question-ranking pairs (test)
4,035
504
7, 15]. Unlike generative AI approaches, answers in this benchmark are grounded
in passages, hence the problem boils down to that of conversational passage re-
trieval [14, 17, 23]. Aggregating results from top-ranked passages into a single an-
swer is an open problem [2] that has been first piloted in the 2022 edition, where
a subtask of generating summaries from retrieved results was introduced [15].
Ren et al. [20] propose an approach for response generation divided into three
stages: (optional) query rewriting, finding supporting sentences in results dis-
played on a SERP, and summarizing them into a short conversational response.
While the authors acknowledge the problem of unanswerability in conversational
search, they do not address it in their proposed approach. In this paper, we aim
to fill that gap.
The problem of unanswerability has been addressed in the context of machine
reading comprehension (MRC) [9, 10] and extractive question-answering (QA) [1,
8, 13]. Solutions proposed include answerability prediction using prompt-tuning [13],
modeling high-level semantic relationships between objects from question and
context [10], and combining the output of reading and verification modules in
MRC systems [9, 24]. Our proposed solution is based on a sentence-level classi-
fier that is learned on CIS-specific training data, and can further be augmented
with QA answerability data.
3
Dataset
This paper builds upon the CAsT-snippets dataset [12],1 which extends the
TREC CAsT’20 and ’22 datasets with snippet-level annotations for the top-
retrieved results. Specifically, it contains annotations of information nuggets
defined as “minimal, atomic units of relevant information” [16], representing
key pieces of information required to answer the given question. Snippets in the
dataset are identified for every question in the 5 most relevant passages according
to ground truth judgments. To balance the collection, we also include 5 randomly
selected non-relevant passages to each question. The resulting dataset, named
CAsT-answerability, contains around 1.8k answerable and 1.9k unanswerable
question-passage pairs. We further consider answerability on the level of sen-
tences and on the level of rankings, as follows. For sentence-level answerability,
we leverage annotations of information nuggets from the CAsT-snippets dataset
as follows: each sentence that overlaps with an information nugget, as per anno-
1 https://github.com/iai-group/CAsT-snippets
4
 Lajewska and Balog
Question
Answer-in-the-sentence
classifier
Aggregation of
answerability
scores
Aggregation of
answerability
scores
....
....
 
 
 
 
....
sentence 2
sentence 1
sentence 2
sentence 1
....
....
0.21
0.76
....
0.04
0.11
....
0.53
0.08
0.41
Sentence-level
Passage-level
Ranking-level
passage 1
passage 2
Top n passages
Fig. 1. Overview of our answerability detection approach.
tations in the originating CAsT-snippets dataset, is labeled as 1 (answer in the
sentence), otherwise as 0 (no answer in the sentence).
For ranking-level answerability, which is the ultimate task we are addressing,
we consider different input rankings, i.e., sets of n = 3 passages, for the same
input question. Specifically, for each unique input test question (38), we gener-
ate all possible n-element subsets of passages available for this question (both
containing and not containing an answer), thereby simulating passage rankings
of varying quality. These rankings represent inputs with various degrees of diffi-
culty for the same question, ranging from all passages containing an answer to
a single passage with an answer to “no answer found in the corpus.” This yields
a total of 4.5k question-ranking pairs, of which 0.5k are unanswerable.2
Overall, our CAsT-answerability dataset contains binary answerability la-
bels on three levels: sentence, passage, and ranking. Sentence- and passage-level
answerability is divided into train (90%), and test (10%) portions; the split-
ting is done on the question level to avoid information leakage. Ranking-level
answerability has only a test set. See Table 1 for a summary.
4
Answerability Detection
The challenge of answerability in CIS arises from the fact that the answer is
typically not confined to a single entity or text snippet, but rather spans across
multiple sentences or even multiple passages. Note that answerability extends
beyond the general notion of relevance and asks for the presence of a specific
answer. At the core of our approach is a sentence-level classifier that can dis-
tinguish sentences that contribute to the answer from ones that do not. These
sentence-level estimates are then aggregated on the passage level and then fur-
ther on the ranking level (i.e., set of top-n passages) to determine whether the
question is answerable; see Figure 1. Operating on the sentence level is a design
decision that has the added benefit that a future summary generation step may
take a filtered set of sentences that contribute to the final answer as input.
2 Examples of data samples with annotated information nuggets and answerability
scores on various levels are provided in the repository accompanying the paper.
Towards Reliable and Factual Response Generation
5
4.1
Answer-in-the-Sentence Classifier
The answer-in-the-sentence classifier is trained on sentence-level data from the
train portion of the CAsT-answerability dataset. In some of the experiments,
this data is augmented by data from the SQuAD 2.0 dataset [18] to provide the
classifier with additional training material and thus guidance in terms of ques-
tions that can be answered with a short snippet contained in a single sentence.
Data from SQuAD 2.0 is downsampled to be balanced in terms of the number
of answerable and unanswerable question-sentence pairs. The classifier is built
using a BERT transformer model with a sequence classification head on top
(BertForSequenceClassification provided by HuggingFace3). Each data sample
contains question [SEP] sentence as input and a binary answerability label.
The output of the classifier is the probability that the sentence contains (part
of) the answer to the question.
4.2
Aggregation of Sentence-level Answerability Scores
In reality, answers are not confined to a single sentence but can be spread across
several passages. We thus need a method to aggregate results obtained from the
sentence-level classifier to decide whether the question can be answered given (1)
a particular passage or (2) a set of top-ranked passages, referred to as a ranking.
We consider two simple aggregation functions, max and mean, noting that
more advanced score- and/or content-based fusion techniques could also be ap-
plied in the future [11]. Intuitively, max is expected to work particularly well
for factoid questions where the answer is relatively short and usually contained
in a single sentence, while mean should capture the cases where pieces of the
answer are spread over several sentences within the passage or across passages.
The aggregated answerability score for a given passage is compared against a
fixed threshold; passages with an aggregated score exceeding this threshold are
identified as containing the answer. We set the threshold values on a validation
partition (10% of the dataset, sampled from the training partition); 0.5 for max
and 0.25 for mean.
An analogous procedure is repeated for the top n = 3 passages in the ranking
to decide on ranking-level answerability. Here, the aggregation methods take
the passage-level answerability scores as input (obtained using max or mean
aggregation of sentence-level probabilities). The resulting values are compared
against a fixed threshold (using the same values as for passage-level aggregation)
to yield a final ranking-level answerability prediction.
5
Results
Table 2 presents the answerability results on the sentence-, passage-, and ranking-
levels on the test partition of CAsT-answerability in terms of accuracy.
3 https://huggingface.co/docs/transformers/model_doc/bert#transformers.
BertForSequenceClassification
6
 Lajewska and Balog
Table 2. Answerability detection results in terms of classification accuracy. The best
scores for each level are in boldface. For the augmented classifier (rows 5–8), significant
differences against the respective method (rows 1–4) are indicated by ∗. ChatGPT
results are tested against the best classifier in rows 1–8. We use McNemar’s test with
p < 0.05.
Classifier
Sentence
Passage
Ranking
Acc.
Aggr.
Acc.
Aggr.
Acc.
CAsT-answerability
0.752
Max
0.634
Max
0.790
Mean
0.891
Mean
0.589
Max
0.332
Mean
0.829
CAsT-answerability
augmented with
SQuAD 2.0
0.779∗
Max
0.676∗
Max
0.810∗
Mean
0.848∗
Mean
0.639∗
Max
0.468∗
Mean
0.672∗
ChatGPT passage-level (zero-shot)
0.787∗ T=0.33 0.839∗
T=0.66 0.623∗
ChatGPT ranking-level (zero-shot)
0.669∗
ChatGPT ranking-level (two-shot)
0.601∗
Does data augmentation help answerability detection? On the sentence level,
we find that augmenting the CAsT-answerability dataset with additional train-
ing examples from SQuAD 2.0 improves performance. These improvements also
carry over to the first aggregation step on the passage level. However, the best
ranking-level results are obtained by aggregating results obtained from the classi-
fier trained only on CAsT-answerability. It may result from the fact that SQuAD
2.0 training data focuses on questions with short-span answers (like entities or
numbers) confined to a single sentence. This could mislead the classifier to over-
look answers spanning multiple sentences or passages. Thus, while sentence-level
answerability prediction benefits from augmented data, this does not translate
to effective passage or ranking-level answerability prediction.
Which of the two aggregation methods performs better? In all cases, max
aggregation on the passage level followed by mean aggregation on the ranking
level gives the best results. Intuitively, this configuration captures single sen-
tences with high answerability scores in individual passages (max aggregation
on passage level) that give a high average score for the whole ranking (mean
aggregation on ranking level) for answerable questions.
How competitive are these baselines in absolute terms? Ours is a novel task,
with no established baselines to compare against. However, using a large lan-
guage model (LLM) for generating the final response from the top retrieved
passages is a natural choice. Therefore, for reference, we compare against a state-
of-the-art LLM, using the most recent snapshot of GPT-3.5 (gpt-3.5-turbo-0301)
via the ChatGPT API. We consider two settings: given a passage (analogous to
the passage-level setup) and given a set of passages as input (analogous to the
ranking-level setup). We prompt the model to verify whether the question is
Towards Reliable and Factual Response Generation
7
answerable in the provided passage(s) and return 0 or 1 accordingly.4 In the
passage-level setup, the passage-level predictions returned by ChatGPT are ag-
gregated using fixed thresholds to obtain a ranking-level prediction. The max
aggregation boils down to checking whether any of the passages is predicted to
contain the answer. In the case of mean aggregation, a threshold of 0.33 or 0.66
(based on the fact that binary values are returned for passage-level answerabil-
ity predictions) would mean that 1 or 2 out of 3 passages, respectively, contain
the answer. In the ranking-level setup, we experiment with both a zero-shot set-
ting, where neither examples nor context is given to the model, and a two-shot
setting containing a question followed by two sentences (one positive and one
negative example) extracted from the passage. We observe that the passage-level
answerability scores of ChatGPT are higher than ours, but after ranking-level
aggregation, it is no longer the case. Further, performing the ranking-level task
directly results in significantly lower performance. These results indicate that
LLMs have a limited ability to detect answerability without additional guidance.
Our baseline methods trained on small datasets and based on simple classifiers
with multi-step results aggregation turn out to be more effective for answerabil-
ity prediction and thus represent a strong baseline.
6
Conclusion
Unanswerable questions pose a challenge in conversational information seeking.
To study this problem, we have developed a test collection, based on two editions
of the TREC CAsT benchmark, with sentence-, passage-, and ranking-level an-
swerability labels. We have also presented a baseline approach based on the idea
of sentence-level answerability classification and multi-step results aggregation,
and evaluated multiple instantiations of this approach with different configura-
tions. Despite their simplicity, our baselines have been shown to outperform a
state-of-the-art LLM on the task of answerability prediction.
In this paper, we have simplified the scenario by treating answerability as
binary concept: a question is answerable if any sentence in the returned pas-
sages contains the answer. In practice, answerability is more nuanced, with some
pieces of the information found but not all. A more realistic future approach
would involve an ordinal scale (e.g., unanswerable, partially answerable, fully
answerable), which would necessitate ground truth assessments with an explicit
specification of the different facets/aspects of the answer. We are not aware of
any dataset for information-seeking tasks (conversational or not) that would
provide this information.
Acknowledgments This research was supported by the Norwegian Research
Center for AI Innovation, NorwAI (Research Council of Norway, project number
309834).
4 The prompts are available in the repository accompanying the paper.
8
 Lajewska and Balog
Bibliography
[1] Asai, A., Choi, E.: Challenges in information-seeking QA: Unanswerable questions and para-
graph retrieval. In: Proceedings of the 59th Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International Joint Conference on Natural Language Processing.
ACL-IJNLP ’21, vol. 1, pp. 1492–1504 (2021)
[2] Bolotova-Baranova, V., Blinov, V., Filippova, S., Scholer, F., Sanderson, M.: WikiHowQA: A
comprehensive benchmark for multi-document non-factoid question answering. In: Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics. ACL ’23 (2023)
[3] Cao, Z., Li, W., Li, S., Wei, F., Li, Y.: AttSum: Joint learning of focusing and summarization
with neural attention. In: Proceedings of COLING 2016, the 26th International Conference on
Computational Linguistics. pp. 547–556. COLING ’16 (2016)
[4] Choi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.t., Choi, Y., Liang, P., Zettlemoyer, L.: QuAC:
Question answering in context. In: Findings of the Association for Computational Linguistics:
EMNLP 2018. pp. 2174–2184. EMNLP ’18 (2018)
[5] Dalton, J., Xiong, C., Callan, J.: TREC CAsT 2019: The conversational assistance track
overview. In: The Twenty-Eighth Text REtrieval Conference Proceedings. TREC ’19 (2019)
[6] Dalton, J., Xiong, C., Callan, J.: CAsT 2020: The conversational assistance track overview. In:
The Twenty-Ninth Text REtrieval Conference Proceedings. TREC ’20 (2020)
[7] Dalton, J., Xiong, C., Callan, J.: TREC CAsT 2021: The conversational assistance track
overview. In: The Thirtieth Text REtrieval Conference Proceedings. TREC ’21 (2021)
[8] Godin, F., Kumar, A., Mittal, A.: Learning when not to answer: a ternary reward structure for
reinforcement learning based question answering. In: Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies. pp. 122–129. NAACL-HLT ’19 (2019)
[9] Hu, M., Wei, F., Peng, Y., Huang, Z., Yang, N., Li, D.: Read + verify: Machine reading com-
prehension with unanswerable questions. arXiv cs.CL/1808.05759 (2018)
[10] Huang, K., Tang, Y., Huang, J., He, X., Zhou, B.: Relation module for non-answerable pre-
dictions on reading comprehension. In: Proceedings of the 23rd Conference on Computational
Natural Language Learning. pp. 747–756. CoNLL ’19 (2019)
[11] Kurland, O., Culpepper, J.S.: Fusion in information retrieval: SIGIR 2018 half-day tutorial. In:
Proceedings of the 41st International ACM SIGIR Conference on Research and Development
in Information Retrieval. pp. 1383–1386. SIGIR ’18 (2018)
[12]  Lajewska, W., Balog, K.: Towards filling the gap in conversational search: From passage re-
trieval to conversational response generation. In: Proceedings of the 32nd ACM International
Conference on Information and Knowledge Management. pp. 5326–5330. CIKM ’23 (2023)
[13] Liao, J., Zhao, X., Zheng, J., Li, X., Cai, F., Tang, J.: PTAU: Prompt tuning for attributing
unanswerable questions. In: Proceedings of the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval. pp. 1219–1229. SIGIR ’22 (2022)
[14] Luan, Y., Eisenstein, J., Toutanova, K., Collins, M.: Sparse, dense, and attentional represen-
tations for text retrieval. Transactions of the Association for Computational Linguistics 9,
329–345 (2021)
[15] Owoicho, P., Dalton, J., Aliannejadi, M., Azzopardi, L., Trippas, J.R., Vakulenko, S.: TREC
CAsT 2022: Going beyond user ask and system retrieve with initiative and response generation.
In: The Thirty-First Text REtrieval Conference Proceedings. TREC ’22 (2022)
[16] Pavlu, V., Rajput, S., Golbus, P.B., Aslam, J.A.: IR system evaluation using nugget-based test
collections. In: Proceedings of the fifth ACM international conference on Web search and data
mining. pp. 393–402. WSDM ’12 (2012)
[17] Pradeep, R., Nogueira, R., Lin, J.: The expando-mono-duo design pattern for text ranking with
pretrained sequence-to-sequence models. arXiv cs.IR/2101.05667 (2021)
[18] Rajpurkar, P., Jia, R., Liang, P.: Know what you don’t know: Unanswerable questions for
SQuAD. In: Proceedings of the 56th Annual Meeting of the Association for Computational
Linguistics. pp. 784–789. ACL ’18 (2018)
[19] Reddy, S., Chen, D., Manning, C.D.: CoQA: A conversational question answering challenge.
Transactions of the Association for Computational Linguistics 7, 249–266 (2019)
[20] Ren, P., Chen, Z., Ren, Z., Kanoulas, E., Monz, C., De Rijke, M.: Conversations with search
engines: SERP-based conversational response generation. ACM Transactions on Information
Systems 39(4), 1–29 (2021)
[21] Sulem, E., Hay, J., Roth, D.: Yes, no or idk: The challenge of unanswerable yes/no questions.
In: Proceedings of the 2022 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies. pp. 1075–1085. NAACL-HLT ’22
(2022)
[22] Tang, L., Goyal, T., Fabbri, A.R., Laban, P., Xu, J., Yavuz, S., Kry´sci´nski, W., Rousseau, J.F.,
Durrett, G.: Understanding factual errors in summarization: Errors, summarizers, datasets,
error detectors. arXiv cs.CL/2205.12854 (2023)
[23] Vakulenko, S., Longpre, S., Tu, Z., Anantha, R.: Question rewriting for conversational question
answering. In: Proceedings of the 14th ACM International Conference on Web Search and Data
Mining. pp. 355–363. WSDM ’21 (2021)
[24] Zhang, J., Zhao, Y., Saleh, M., Liu, P.J.: PEGASUS: pre-training with extracted gap-sentences
for abstractive summarization. arXiv cs.CL/1912.08777 (2020)
","Unanswerability detection has been addressed in the context of machine reading comprehension and question answering, both of which differ significantly from the conversational search setup. Information-seeking dialogues pose additional challenges, such as open-ended questions requiring descriptive answers, indirect answers requiring an inference or background/context knowledge, and complex queries with partial answers spread across passages. Therefore, unanswerability detection is a novel, still unsolved task in CIS, and, to the best of our knowledge, no public dataset exists for this problem.nan"
"This research investigates the energy consumption characteristic of Continuous Phase Modulation (CPM) in smart-grid Internet of Things (SG-IoT) under various configurations of modulation parameters. An energy consumption (EC) model is developed for sensor nodes, considering the circuits, a typical communication protocol, and ARQ-based retransmissions. Analytical and simulation results indicate that CPM enjoys a significantly lower EC than OQPSK and 16QAM in the scenario considered.","The smart grid, an energy infrastructure for smart cities and industries, heavily relies on wireless sensor network (WSN), characterized by transmission rate, signal coverage, energy consumption (EC), and network lifetime. Low EC is of vital importance, especially in scenarios where manual work is impractical or unsafe. Prior studies focused on EC of modulation schemes sensitive to power amplifier (PA) nonlinearity. This study aims to investigate the impact of CPM, a constant envelope modulation technique insensitive to PA nonlinearity, on the EC of sensor nodes in WSNs suitable for SG-IoT.","A realistic power consumption model is established through analysis of circuit power consumption, transmission power consumption, and reception power consumption. Based on the power consumption model and a typical ARQ-based wireless transmission protocol, the EC incurred by successfully sending a single information bit is numerically evaluated. The impact of different waveform pulses of CPM, the distance between the transmitter and the receiver, the received SNR, the modulation order, and the average number of transmissions required for sending a single packet are investigated under various modulation schemes.","The EC per successfully transmitted bit of the CPM is significantly lower than that of conventional non-constant envelope modulation methods, such as offset quadrature phase-shift keying (OQPSK) used in the Zigbee standard and QAM modulation supported by the current 5G standard. CPM enjoys a lower EC due to its constant envelope, which yields excellent power/energy efficiency. However, the primary drawback of CPM is the high implementation complexity required for an optimal receiver.","CPM is an attractive modulation scheme for WSNs underpinning SG-IoT because of its spectral efficiency and energy efficiency. Results indicate that CPM outperforms OQPSK and 16QAM under configurations considered, making it valuable for standard evolution of beyond 5G tailored for SG-IoT.",Energy Consumption Analysis for Continuous Phase Modulation in Smart-Grid Internet of Things of beyond 5G,"Hongjian Gao, Yang Lu, Shaoshi Yang, Jingsheng Tan, Longlong Nie, Xinyi Qu","1
Energy Consumption Analysis for Continuous
Phase Modulation in Smart-Grid Internet of Things
of beyond 5G
Hongjian Gao, Yang Lu, Shaoshi Yang, Jingsheng Tan, Longlong Nie and Xinyi Qu
Abstract—Wireless sensor network (WSN) underpinning the
smart-grid Internet of Things (SG-IoT) has been a popular
research topic in recent years due to its great potential for
enabling a wide range of important applications. However, the
energy consumption (EC) characteristic of sensor nodes is a key
factor that affects the operational performance (e.g., lifetime of
sensors) and the total cost of ownership of WSNs. In this paper, to
find the modulation techniques suitable for WSNs, we investigate
the EC characteristic of continuous phase modulation (CPM),
which is an attractive modulation scheme candidate for WSNs
because of its constant envelope property. We first develop an EC
model for the sensor nodes of WSNs by considering the circuits
and a typical communication protocol that relies on automatic
repeat request (ARQ)-based retransmissions to ensure successful
data delivery. Then, we use this model to analyze the EC char-
acteristic of CPM under various configurations of modulation
parameters. Furthermore, we compare the EC characteristic of
CPM with that of other representative modulation schemes, such
as offset quadrature phase-shift keying (OQPSK) and quadrature
amplitude modulation (QAM), which are commonly used in
communication protocols of WSNs. Our analysis and simulation
results provide insights into the EC characteristics of multiple
modulation schemes in the context of WSNs; thus, they are
beneficial for designing energy-efficient SG-IoT in the beyond-5G
(B5G) and the 6G era.
Index Terms—continuous phase modulation (CPM), wireless
sensor network (WSN), energy efficient, modulation optimization,
smart grid, Internet of Things (IoT), B5G, 6G.
I. INTRODUCTION
S
MART grid is the energy infrastructure for smart cities,
telecommunications, networks, and the computing indus-
try. It upgrades traditional power grid systems with state-of-
the-art information and communication technologies, such as
wireless sensor network (WSN) techniques and the Internet
of Things particularly designed for the smart grid industry
(SG-IoT). In fact, SG-IoT heavily relies on WSN, which is
characterized by a variety of distinct performance metrics,
such as transmission rate, signal coverage, energy consumption
(EC), and network lifetime [1]. (defined as the number of
joules consumed per successfully transmitted bit) of wireless
sensors, because the energy supply requirements of sensors
Corresponding author: S. Yang (E-mail: shaoshi.yang@bupt.edu.cn)
H. Gao and Y. Lu are with the State Grid Smart Grid Research Institute
Co., Ltd., Beijing 102209, China.
S. Yang, J. Tan, L. Nie, and X. Qu are with the School of Infor-
mation and Communication Engineering, Beijing University of Posts and
Telecommunications, and also with the Key Laboratory of Universal Wireless
Communications, Ministry of Education, Beijing 100876, China.
Published on Sensors, vol. 24, no. 2, article number 533, Jan. 2024,
https://doi.org/10.3390/s24020533
are stringent in WSNs (i.e., very limited energy supply) and
all the other performance metrics can be affected by the EC
characteristic of sensors. To elaborate a little further, in many
application scenarios of the smart grid industry, it is often
inconvenient or unsafe for humans to work in the deployment
site and the lifetime of sensors is often expected to be over
several years. Therefore, low EC is of vital importance for
these scenarios.
By contrast, in wireless communication systems that operate
with the support of power grid infrastructure, it is more
appropriate to invoke energy efficiency (EE), which is typically
defined as the number of bits successfully transmitted per
joule. This concept is at the heart of green communications,
a vision globally recognized for reducing the Carbon foot-
print produced by the networking sector, especially in the
era of 5G, 5G-Advanced, and 6G [2]. Obviously, EE is the
reciprocal of EC. Extensive studies have been devoted to
optimizing the EE of wireless networks in the past decade.
For instance, in [3], based on the fractional programming
framework, the joint power and subcarrier allocation prob-
lem was solved for maximizing the EE of a multi-user,
multi-relay, single-cell orthogonal frequency-division multi-
ple access (OFDMA) cellular network composed of single-
antenna nodes. Afterwards, system models that are more
complicated were considered: the joint transmit and receive
beamforming-based multi-user, multi-relay, multi-input multi-
output (MIMO)-OFDMA cellular networks [4], [5]; the multi-
cell single-antenna OFDMA networks [6]; the partial/full
interference alignment-based multi-user, multi-relay, multi-
cell MIMO-OFDMA networks [7]; the massive MIMO-aided,
multi-pair, one-way decode-and-forward relay system [8]; and
the fully connected K-user interference channel with each user
having either a single antenna or multiple antennas [9]. Addi-
tionally, the EE of wireless networks that are delay-sensitive
was also studied by maintaining statistical quality-of-service
QoS guarantees in OFDMA networks [10] and by considering
the uplink ultra-reliable low-latency communication (URLLC)
traffic in the MIMO-aided grant-free access [11] of 5G and its
beyond. In [12], secrecy-energy efficient hybrid beamforming
schemes were designed for a satellite-terrestrial integrated
network in order to maximize the achievable secrecy-EE while
satisfying the signal-to-interference-plus-noise ratio (SINR)
constraints of both the earth stations and the cellular users;
further, in [13], the secrecy-energy efficient beamforming in
multibeam satellite systems was investigated with the metric
of signal-to-leakage-plus-noise ratio (SLNR).
arXiv:2401.11449v1  [eess.SP]  21 Jan 2024
2
Since wireless sensors are typically powered by batteries,
it is more appropriate to use EC in the context of wireless
sensors. EC is closely related to the selected modulation
scheme. Firstly, this selection may influence the type of
electronic components utilized, such as a power amplifier
(PA) or analog-to-digital converter (ADC), because different
modulation schemes may require different circuit designs and
implementations. Secondly, the specific choice of modulation
schemes also affects the number of bits transmitted in a single
symbol duration that consumes a certain amount of energy.
Thirdly, different modulation schemes may incur different
packet error rates (PERs), which influence the number of
retransmissions that also consume energy and are necessary
for successful packet delivery between any pair of wireless
sensors. Therefore, it is important to investigate the impact of
different modulation schemes on the EC and identify the most
appropriate scheme for WSNs of SG-IoT.
Prior research mainly focused on studying the EC of
modulation schemes that are sensitive to the nonlinearity of
PAs. More specifically, in [14], the EC minimization problems
corresponding to M-ary quadrature amplitude modulation
(MQAM) and multiple frequency-shift keying (MFSK) were
studied. In [15], [16], the authors studied the relationship be-
tween the total EC per successfully transmitted information bit
and the transmission distance while assuming different mod-
ulation methods, such as binary phase-shift keying (BPSK),
quadrature phase-shift keying (QPSK), and 16QAM. They also
studied the average signal-to-noise ratio (SNR) values required
to achieve the optimal EC. In [17], the transmission power of
MQAM was optimized by using a particular model to achieve
the minimum EC. In [18], the EC per successfully transmitted
bit for modulation techniques including binary frequency-
shift keying (BFSK), BPSK, QPSK, 16QAM, and 64QAM
was studied under various channel conditions. However, these
modulation techniques require the use of linear PAs, which
results in lower energy utilization efficiency. In contrast,
constant envelope modulation techniques are insensitive to the
nonlinearity of PAs; thus, they constitute a promising solution
to improving the energy utilization efficiency. However, there
is a scarcity of research focusing on the impact of constant
envelope modulation techniques on the achievable EC in the
context of WSNs underpinning SG-IoT.
Against the above backdrop, in this paper, we endeavor
to investigate the impact of a constant envelope modulation
technique, i.e., continuous phase modulation (CPM), on the
EC of sensor nodes in WSNs suitable for SG-IoT. Our novel
contributions are summarized as follows.
• We establish a realistic power consumption model
through the analysis of circuit power consumption, trans-
mission power consumption, and reception power con-
sumption on a point-to-point communication link; in par-
ticular, we consider three operation modes of the sen-
sors, including sleeping mode, transient mode, and active
mode.
• Based on the above power consumption model and a
typical automatic repeat request (ARQ)-based wireless
transmission protocol, the EC incurred by successfully
sending a single information bit is numerically evaluated
Preamble
Header
Payload
LP
LH
LL
L
Fig. 1: Physical layer packet structure.
under different configurations of CPM parameter values.
In particular, we consider different waveform pulses of
the CPM, including the rectangular pulse, rising cosine
pulse, and GMSK pulse, for comprehensive coverage. We
also investigate the impact of the distance between the
transmitter and the receiver, the impact of the received
SNR, the impact of the modulation order, and the average
number of transmissions required for sending a single
packet, under various modulation schemes considered.
• We compare the EC per successfully transmitted bit of
the CPM with that of conventional non-constant envelope
modulation methods, such as offset quadrature phase-
shift keying (OQPSK) used in the Zigbee standard and
QAM modulation supported by the current 5G standard.
Our simulation results and analysis demonstrate that
CPM enjoys a significantly lower EC than OQPSK and
16QAM in the scenario considered, which is valuable
for the standard evolution of beyond 5G tailored for the
important use case of low-power SG-IoT.
II. THE EC MODEL
To analytically determine the amount of energy consumed
when a single bit is transmitted without error, an EC model
needs to be established. We make the assumption that each
packet transmitted in the forward direction induces an error-
free feedback packet in the reverse direction, which acknowl-
edges the successful reception of the data packet or requests
for retransmission.
A. Packet Structure
In wireless communication systems, the general format of
the physical layer packet structure is shown in Figure 1 and
consists of three parts: a pilot code for clock synchronization,
a packet header specifying the configuration of transmission
parameters, and a data payload carrying the transmitted data.
We assume that the entire packet uses the same modulation
and that the symbol error probability (SEP) is determined
by the received SNR γ and the modulation scheme adopted.
We also assume that symbol errors are independently and
identically distributed (i.i.d.) and no channel coding is used,
i.e., no redundant bits are added (redundant bits increase
EC). If a symbol is erroneously detected at the receiver,
the entire packet will be retransmitted until all the symbols
in the packet are correctly detected at the receiver. The packet
error probability (PEP) can be expressed as a function of the
SEP, the packet length L, and the number of bits per symbol
m = log2 M, i.e.,
PEP = 1 − (1 − SEP)L/m,
(1)
3
where M denotes the size of the modulation constellation.
Hence, by utilizing the packet length L and the probability
that a packet is successfully transmitted, i.e., 1−PEP, the av-
erage amount of data successfully delivered per transmission
duration of a packet is formulated as
Nc = L(1 − SEP)L/m.
(2)
Then, upon assuming that the feedback signal indicating
whether a retransmission is needed or not is reliably transmit-
ted on the reverse link, the average number of transmissions
required for successfully delivering a packet is given by
Nre = L
Nc
=
1
1 − PEP =
1
(1 − SEP)L/m .
(3)
B. Basics of CPM
CPM is an attractive modulation scheme for WSNs un-
derpinning SG-IoT because its carrier phase is modulated in
a continuous manner and it is typically implemented as a
constant-envelope waveform, i.e., the transmitted carrier power
is constant. The phase continuity requires a relatively small
percentage of the power to occur outside of the intended
band (e.g., low fractional out-of-band power), leading to high
spectral efficiency. Meanwhile, the constant envelope yields
excellent power/energy efficiency. However, the primary draw-
back of CPM is the high implementation complexity required
for an optimal receiver.
For systems that employ CPM, the transmitted signal at time
instant t can be expressed as[19]
s(t, I) =
r
2E
T cos(2πfct + ϕ(t, I) + ϕ0),
(4)
where E is the symbol energy, T is the symbol interval, fc
is the carrier frequency, and ϕ0 is an arbitrary constant initial
phase shift that can be set to zero without loss of generality
when coherent transmission is considered. In addition, ϕ(t, I)
is the time-varying information-carrying phase formulated as
ϕ(t, I) = 2π
K
X
k=−∞
hkIkq(t−kT), KT ≤ t ≤ (K+1)T, (5)
where I = {Ik|k ∈ (−∞, · · · , −1, 0, +1, · · · , K)} is an
infinitely long sequence of uncorrelated M-ary data sym-
bols, each having one of the values from the alphabet
A = {±1, ±3, · · · , ±(M − 1)} with equal probability 1/M;
{hk} is a sequence of modulation indices defined as hk =
2fd,kT, with fd,k being the peak frequency deviation. When
hk = h for all k, the modulation index remains fixed for
all symbols. When the modulation index changes from one
symbol to another, the signal is called multi-h CPM, with hk
varying in a cyclic manner. q(t) is some normalized waveform
shape that represents the baseband phase response (i.e., phase
pulse) and is obtained from the frequency pulse g(t) by
q(t) =
Z t
−∞
g(τ)dτ.
(6)
If the duration of g(t) is equal to the symbol interval T,
namely, g(t) = 0 for t > T, the modulated signal is called
full-response CPM. If the duration of g(t) is larger than the
symbol interval T, namely, g(t) ̸= 0 for t > T, the modulated
signal is called partial-response CPM.
Suppose the length of the frequency pulse g(t) in terms of
the number of symbol intervals is N. Thus, N = 1 yields
full-response CPM. If g(t) is selected as a rectangular pulse,
namely,
g(t) =
(
1
2NT ,
0 ≤ t ≤ NT,
0,
otherwise,
(7)
then for a full-response CPM, we have
q(t) =





0,
t ≤ 0,
t
2T ,
0 ≤ t ≤ T,
1
2,
t ≥ T.
(8)
It is evident that the performance of CPM is influenced by
certain parameters, including but not limited to M, hk, N,
and the frequency pulse g(t). Note that by choosing different
pulse shapes g(t) and varying M, hk, and N, an infinite
variety of CPM signals may be generated, each with its unique
characteristics and performance.
For a CPM signal, the error rate performance can be derived
based on the maximum-likelihood sequence detection (MLSD)
receiver, which is conventionally computed using the Viterbi
Algorithm (VA). Specifically, for a given CPM scheme, we
have
SEP = KminQ(
q
d2
minγ).
(9)
According to Anderson’s seminal work on digital phase
modulation [20], Kmin denotes the total number of feasible
paths that satisfy the constraint of the minimum Euclidean
distance dmin within the observation interval on the CPM
phase grid. The value of Kmin increases with the modulation
order M. Both Kmin and dmin depend on critical parameters
including M, hk, N, and the pulse shaping function g(t).
C. Circuit Power Consumption
In wireless communication systems, a significant portion of
energy is dedicated to signal transmission and reception cir-
cuits, which are mainly composed of the baseband (BB) digital
signal processing unit and the radio frequency (RF) signal
processing unit, as shown in Figure ??. To elaborate a little
further, the BB signal processing unit mainly includes source
coding/decoding, pulse shaping, channel coding/decoding, dig-
ital modulation/demodulation, channel estimation, synchro-
nization, and so on. For a wireless sensor, since the data rate
requirement is usually low, the BB symbol rate is also low.
Meanwhile, typically, no computation-intensive signal pro-
cessing techniques, such as multi-user detection and iterative
decoding, are used in an energy-constrained wireless sensor;
hence, the BB power consumption is significantly smaller than
the RF circuit power consumption.
A typical model of an RF signal processing unit, also known
as an RF chain, is shown in Figure 3 [14], [16], [21], [22],
[23]. Specifically, on the transmitter side, the BB signal is first
converted to an analog signal by the digital-to-analog converter
(DAC). Then, the analog signal is filtered by the low-pass filter
4
濕濴瀆濸濵濴瀁濷澳
濗濼濺濼瀇濴濿澳濦濼濺瀁濴濿澳
濣瀅瀂濶濸瀆瀆濼瀁濺澳
濖濼瀅濶瀈濼瀇
Tx RF 
Circuit
Rx RF 
Circuit
Tx/Rx Switch
PTBB /PRBB
PTRF
PRRF
Fig. 2: The communication modules and the corresponding
power consumption model of a point-to-point wireless com-
munication system.
LO
DAC
ADC
IFA
Filter
Filter
Filter
Filter
Filter
Mixer
LNA
Mixer
PA
Duplexer
PDAC
Pfilt
PLo
PPA
Pfilt
Pfilt
Pfilt
Pfilt
Pdup
PLNA
Pmix
Pmix
PIFA
PADC
PRF
Fig. 3: A typical power consumption model of the RF signal
processing unit of a point-to-point wireless communication
system.
and upconverted by the mixer, whose output is then filtered
again, amplified by the power amplifier (PA), passed through
the duplexer, and finally transmitted to the wireless channel.
On the receiver side, the RF signal is sequentially filtered,
amplified by the low-noise amplifier (LNA), cleaned by the
anti-aliasing filter, downconverted by the mixer, filtered again
before passing through the intermediate frequency amplifier
(IFA) that has an adjustable gain, and finally converted back
to a digital signal by the analog-to-digital converter (ADC).
Note that the mixers operate with the aid of the local oscillator
(LO) and, among all the RF components, the PA and LNA
usually have much higher power consumption than the others.
Accordingly, the total power consumption during transmis-
sion can be expressed as
PTx(d) = PTBB + PTRF + PPA(d)
|
{z
}
PTRF
= PT0 + ξ
η PT(d),
(10)
where PTBB and PTRF represent the power consumption of
the transmitter’s BB processing unit and RF processing unit
excluding the PA, respectively, and both of them can be
regarded as constant values that are collectively denoted by
PT0; PPA(d) and PT(d) are defined as the power consumption
of the PA and the transmit power, respectively, both of which
are functions of the transmission distance d upon assuming
adaptive power control and are related via PPA(d) = ξ
ηPT(d);
and η and ξ represent the drain efficiency and the peak-to-
average power ratio (PAPR) of the PA, respectively [18].
Similarly, the total power consumption of the receiver is
expressed as
PRx = PRBB + PRRF + PLNA
|
{z
}
PRRF
= PR0,
(11)
where PRBB and PRRF represent the power consumption of
the receiver’s BB processing unit and RF processing unit
excluding LNA, respectively, and both of them can be regarded
as constant values; PLNA represents the power consumption
of the LNA, which is also constant upon assuming that the
LNA is appropriately designed and biased, so that necessary
sensitivity is provided for reliably receiving, demodulating,
and decoding a minimum power signal. Hence, PRBB, PRRF,
and PLNA are collectively denoted by the constant PR0.
D. Transmission Power Consumption
Due to the path-loss, scattering, reflection, and other phe-
nomena in the wireless channel, a certain amount of energy
is inevitably lost during the transmission of electromagnetic
waves that carry data symbols, thus resulting in transmission
energy dissipation. The path-loss depends on several factors,
such as the distance between the transmitter and receiver,
the frequency of the signal, the type of antennas used, and the
environment through which the signal propagates.
When the signal with a transmit power PT(d) is propagated
through the wireless channel, the received power PR(d) at the
receiver can be formulated as
PR(d) = GTGRPT(d)
 λ
4πd
α
= PT(d)
A0dα ,
(12)
according to Friis’ transmission equation. This expression
characterizes the dependency between the received power
with respect to several parameters. Specifically, the constant
A0 depends on the transmit antenna gain GT, the receive
antenna gain GR, and the carrier wavelength λ. Additionally,
the path-loss exponent is α. It is noted that the received
power is inversely proportional to the distance d raised to the
power of α, and the received SNR γ is proportional to 1/A0
divided by dα. These relationships collectively describe how
the aforementioned parameters influence the received power
and the received SNR [18]:
γ =
PR(d)
N0WNfMl
,
(13)
where W represents the transmission bandwidth, N0 denotes
the power spectral density of the baseband-equivalent additive
white Gaussian noise (AWGN), and they are primary determi-
nants of γ. Furthermore, the noise figure of the RF front-end
of the receiver, denoted as Nf, and any additional noise or
interference, represented by the link margin term Ml, can also
impact the received SNR γ.
Accordingly, by substituting Equation (12) into Equa-
tion (13), the relationship between the transmit power PT(d),
the communication distance d, the received SNR γ, and other
parameters can be quantitatively expressed as
PT(d) = A0dαN0WNfMlγ = Adαγ,
(14)
where we have A = A0N0WNfMl.
5
E. EC per Successfully Transmitted Bit
As mentioned in Section II, we assume that each packet
transmitted in the forward direction is matched by an error-
free feedback packet in the reverse direction in order to guar-
antee reliable transmission. Both directions of transmissions
consume energy. The above transmission process, usually
incorporating retransmissions, continues until the entire packet
of the forward direction is correctly decoded at the receiver.
Additionally, we assume that the sensor node transceiver
circuitry works in a multi-mode manner: (1) when there are
data to transmit or receive, all circuits of the sensor work in the
active mode; (2) when neither transmission nor reception are
needed, the circuits of the sensor enter sleep mode by default,
which uses the minimum possible power (small enough to be
negligible) to ensure that the circuits can be activated when
necessary; (3) when the sensor is in the period of switching
from sleep mode to active mode, it is in a transient mode that
also consumes non-negligible energy. Note that the transient
duration from active mode to sleep mode is sufficiently short
to be neglected but the start-up process from sleep mode to
active mode can be slow.
Therefore, for a single round-trip transmission (forward
direction transmission and reverse direction feedback) on a
point-to-point communication link, the total EC of the system
can be divided into two parts: the EC of the forward direction
transmission and the EC of the reverse direction feedback.
For the forward direction transmission, the EC is composed
of the start-up energy consumption 2EST in transient mode
(both the transmitting node and the receiving node may be
in sleep mode initially), the transmitter energy consumption
ETx(d) in active mode, and the receiver energy consumption
ERx in active mode. Therefore, the EC of the forward direction
transmission is expressed as
EFW = 2EST+ETx(d)+ERx = 2EST+PTx(d)TDTA+PRxTDRA,
(15)
where TDTA is the transmission duration for sending a data
packet in the forward direction and TDRA is the corresponding
duration of signal processing at the receiver of the forward di-
rection.
Additionally, the EC of the reverse direction transmission
is expressed as
ERV = P Tx(d)TFTA + PRxTFRA,
(16)
where P Tx(d) is the total power consumption for transmission
of a feedback packet in the reverse direction by the receiving
node of the forward direction and TFTA is the corresponding
time duration. Note that P Tx(d) may be different from PTx(d)
of the forward direction because a feedback packet may have
a different transmission rate and reliability requirements com-
pared with a data packet. TFRA is the duration of processing
the feedback packet at its receiver (i.e., the transmitting node
of the forward direction) in active mode.
Based on the above analysis, we obtain the EC per suc-
cessfully transmitted bit as (17), where P T(d) is the transmit
power for the feedback packet transmission and Nre is the
average number of retransmissions.
TABLE I: The value of d2
min when we set M = 2, 4, 8, 16,
h = 0.75, and N = 3. For GMSK, the time-bandwidth product
BT is set to 0.3, where B is the −3 dB bandwidth of the
Gaussian pulse.
Waveform
M = 2
M = 4
M = 8
M = 16
REC
2.31648
1.41550
2.12325
2.831
RC
2.96059
5.30037
6.12447
8.16596
GMSK
2.89955
4.69275
5.95011
7.93348
III. CPM PARAMETER SELECTION
As mentioned in Section II-B, the main parameters that af-
fect the performance of CPM are M, hk, N, and the frequency
pulse shaping function g(t), whilst the minimum squared
Euclidean distance d2
min also depends on these parameters.
Therefore, it is necessary to determine how these parameters
influence the EC per successfully transmitted bit.
In principle, it is possible to implement an infinite number
of different CPM signals using various combinations of design
parameters. However, for practical implementation, we must
consider the trade-off between the achievable performance
and the cost incurred. It is well known that partial-response
CPM signals usually have better spectral efficiency than full-
response CPM signals. However, the computational complex-
ity of the optimal MLSD receiver exponentially increases
with N, which is the length of g(t) in terms of the number
of symbol intervals. In this study, we focus on the partial-
response CPM with a moderate value of N = 3. The modu-
lation order M also significantly influences the computational
complexity and the requirements for demodulation devices
[24], which can lead to high EC. Therefore, small values, such
as M = 2, 4, 8, 16, etc., are generally chosen. We assume
the modulation index hk = h, which is also an important
parameter and has a complex functional relationship with the
minimum squared Euclidean distance d2
min [24], [25]. Smaller
values of h result in narrower bandwidth, more concentrated
signal energy, and narrower transition bands. However, they
also make the phase variations less obvious and can increase
the complexity of demodulation decisions. For single index
modulation, h = 0.5 or 0.75 is commonly used. Finally, the
pulse shaping function g(t) usually utilizes rectangular pulse
(REC), rising cosine pulse (RC), and Gauss minimum-phase
shift-keying pulse (GMSK) [26].
Given
the
imperative
to
minimize
EC
for
sensors,
the present study focuses on CPM signals that can be imple-
mented with simple devices and require low computational
complexity. Accordingly, CPM signals with three different
g(t) functions are selected for investigation, assuming M =
2, 4, 8, 16, h = 0.75, and N = 3. The values of d2
min under
these parameter configurations are calculated with the methods
given in [24], [25] and listed in Table I.
IV. EVALUATION THE EC OF CPM
A. Identification of Major Performance Influencing Factors
From Equation (17), it can be observed that the EC per
successfully transmitted bit, Eb, is predominantly determined
by four parameters: the forward-link transmit power PT(d), the
reverse-link transmit power P T(d), the specific CPM scheme
6
Eb = Nre(EFW + ERV)
L
= EFW + ERV
Nc
= 2EST + PTx(d)TDTA + PRxTDRA + P Tx(d)TFTA + PRxTFRA
L(1 − SEP)L/m
=
2EST + (PT0 + ξ
ηPT(d))TDTA + PRxTDRA + (PT0 + ξ
ηP T(d))TFTA + PRxTFRA
L(1 − KminQ(
p
d2
minγ))L/m
.
(17)
TABLE II: A summary of the pertinent simulation parameters.
Parameters
Values
Symbol rate for transmitted signals
20 ksps
LP , LH, and LL
4, 3, and 30 bytes
Power spectral density of AWGN at the receiver (N0)
−174 dBm/Hz
Noise figure of the RF front-end of the receiver (Nf)
10 dB
Equivalent antenna gain (A0)
30 dB
Bandwidth (W)
20 kHz
Additional noise (Ml)
10 dB
PT0
15.9 mW
PR0
58.2 mW
M
2, 4, 8, 16
h
0.75
N
3
Path-loss exponent (α)
3.5
Drain efficiency (η)
0.7 for CPM; 0.35 for OQPSK and 16QAM
Peak-to-average power ratio (ξ)
0 dB for CPM; 3.5 dB for OQPSK; 6.7 dB for 16QAM
adopted, and the received SNR γ. From another perspective,
we can also say that Eb is mainly determined by PT(d),
P T(d), the achievable SEP, and the modulation efficiency m.
Upon inspection of Equations (13) and (9), it becomes clear
that the SEP on the forward link can be expressed as
SEP = KminQ


s
d2
min
PT(d)
A0dαN0WNfMl

 .
(18)
Although a high transmit power is desirable for achieving
a low SEP and thus reducing the number of retransmissions
Nre, it may also result in excessively large transmission power
for the sensor and increase the EC of each single-direction
transmission. Hence, an optimal transmit power (or in turn
the received SNR γ) exists and is yet to be found for
minimizing the EC per successfully transmitted bit. Similarly,
the relationship between the EC per successfully transmitted
bit and other major parameters mentioned above needs to be
studied.
B. Simulation Results and Discussions
In the following numerical simulations, we consider the
radio links of a WSN designed for smart grid and operating
in the industrial–science–medical (ISM)-oriented 2.4 GHz
frequency band. Table II provides a summary of the pertinent
simulation parameters, including circuit-related parameter set-
tings as well. Due to the constant envelope characteristic of the
CPM signal, a nonlinear PA with a high η value is employed,
in contrast to the general radio architecture.
First, let us compare the EC performance of different CPM
schemes and of other representative modulation schemes,
such as OQPSK and 16QAM, by observing how Eb varies
with the received SNR γ. As shown in Figure 4, for all
modulation schemes, the achievable Eb values firstly descend
and then increase with the received SNR. This is because
in the low-SNR region, the number of retransmissions plays
an important role, while the number of retransmissions is
reduced to its minimum under a sufficiently high SNR, and
then an even higher SNR means unnecessary energy wastage.
In addition, for most SNRs, the achievable Eb values of REC
(d2
min = 2.831, h = 0.75 and N = 3), RC (d2
min = 8.16596,
h = 0.75 and N = 3), and GMSK (d2
min = 7.93348, h = 0.75
and N = 3) are lower than those of OQPSK and 16QAM,
which verifies the advantages of CPM in terms of energy
saving. The reason why a higher order CPM scheme has a
lower Eb can be explained by referring to Equation (17) as
follows: (1) a larger M causes a smaller number of symbols
per packet (i.e., the smaller exponent L/m) and a smaller SEP
(i.e., the larger base number 1 − SEP), thus the denominator
L(1 − SEP)L/m increases with M; (2) the variable TDTA in
the numerator becomes smaller when M becomes larger, while
all the other terms can be regarded as constants. Furthermore,
we can see that the lowest values of Eb achieved by the four
modulation schemes having M = 16 (i.e., REC, RC, GMSK,
and 16QAM) are almost the same, while OQPSK having
M = 4 achieves the largest Eb in the high-SNR region and the
second-largest Eb in the low-SNR region. It is also observed
that when the SNR is above a specific threshold, the three
different CPM waveforms exhibit the same EC performance.
7
0
5
10
15
20
25
30
35
SNR (dB)
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
Eb (J/bit)
10-6
REC
RC
GMSK
OQPSK
16QAM
Fig. 4: The relationship between the EC per successfully
transmitted bit (Eb) and the received SNR (γ) for OQPSK,
16QAM, and the CPM signals with different pulse shaping
functions (REC, RC, and GMSK), while assuming M = 16
and N = 3 for the three CPM waveforms, as well as d = 10
m and the AWGN channel for all the modulation schemes
considered.
Figure 5 characterizes the relationship between Eb and the
communication distance d for OQPSK, 16QAM, and the CPM
signals with different pulse shaping functions. We can see that
Eb increases with d for all the modulation schemes considered.
This is because large distance reduces the received SNR
value under a given transmit power, thus degrading the SEP
performance and increasing the number of retransmissions. We
also see that all the three CPM waveforms considered have
the same EC performance curves, which are consistently and
significantly better than those of OQPSK and 16QAM when d
is sufficiently large. Here, we assume γ = 8 dB for the three
CPM waveforms and γ = 15 dB for OQPSK and 16QAM. The
two SNR values are selected according to the results shown
in Figure 4, where the three CPM waveforms achieve their
optimal Eb at about γ = 8 dB, while OQPSK and 16QAM
achieve their near-optimal Eb at about γ = 15 dB.
Figure 6 shows how the Eb values of the three CPM
waveforms vary with the modulation efficiency m, while
assuming γ = 8 dB and d = 10 m. It is observed that for
each given CPM waveform, the Eb value becomes smaller as
m increases. Although the RC-based CPM signaling exhibits
the highest Eb when m = 1, 2, 3, all the three CPM waveforms
achieve almost the same Eb when m = 4 (i.e., M = 16). This
is because the three CPM waveforms achieve almost the same
SEP performance when m = 4, γ = 8 dB, and d = 10 m.
These observations are consistent with the results shown in
Figure 4.
Figure 7 demonstrates the relationship between the average
number of transmissions Nre required for successfully sending
a single packet and the received SNR γ over the AWGN
channel employing different modulation schemes, including
OQPSK, 16QAM, and the three CPM waveforms. Obviously,
10
20
30
40
50
d (m)
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
Eb (J/bit)
10-6
REC
RC
GMSK
OQPSK
16QAM
Fig. 5: The relationship between the EC per successfully
transmitted bit (Eb) and the communication distance (d) for
OQPSK, 16QAM, and the CPM signals with different pulse
shaping functions (REC, RC, and GMSK) over the AWGN
channel, while assuming M = 16, N = 3 and γ = 8 dB for
the three CPM waveforms, as well as γ = 15 dB for OQPSK
and 16QAM signals.
16QAM incurs the largest Nre, while the REC- and GMSK-
based CPM schemes require the smallest Nre, under all the
three SNR values of 6 dB, 8 dB, and 10 dB. In addition, the
OQPSK scheme requires a smaller and a larger Nre than the
RC-based CPM scheme under γ = 6 dB and γ = 8 dB,
respectively. However, when the received SNR is sufficiently
high, e.g., γ = 10 dB, all the modulation schemes, except
16QAM, require only a single transmission on average for
successfully sending a packet.
V. CONCLUSIONS
In this paper, the EC characteristics of various CPM
schemes are compared with those of OQPSK and 16QAM
in the context of WSN-based SG-IoT of beyond 5G. We
first propose an EC model for the sensor nodes of WSNs by
considering the circuits and a typical communication protocol
that relies on ARQ-based retransmissions. Our analytical and
simulation results demonstrate that all the CPM schemes based
on the pulse shaping functions of REC, RC, and GMSK sig-
nificantly outperform OQPSK used in the Zigbee standard and
16QAM used in the current 5G standard, in terms of the EC
per successfully transmitted bit, Eb. We also show that for all
the modulation schemes considered, the individual optimum
values of Eb are achieved with the received SNR that is neither
too small nor too large. In addition, we show that Eb increases
with the communication distance d for all the modulation
schemes considered, and decreases with the modulation order
M for the three CPM schemes. Overall, it is observed that the
REC- and GMSK-based CPM schemes achieve the best EC
performance of all the modulation schemes considered.
8
1
2
3
4
m
0
1
2
3
4
5
6
7
Eb (J/bit)
10-6
REC
RC
GMSK
Fig. 6: The relationship between the EC per successfully
transmitted bit (Eb) and the modulation efficiency m for the
CPM signals with different pulse shaping functions (REC, RC,
and GMSK) over the AWGN channel, while assuming N = 3,
γ = 8 dB and d = 10 m.
6
8
10
SNR (dB)
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
The average number of transmissions
REC
RC
GMSK
OQPSK
16QAM
Fig. 7: The relationship between the average number of trans-
missions required for successfully sending a single packet and
the received SNR (γ), while considering OQPSK, 16QAM,
and the CPM signals with different pulse shaping functions
(REC, RC, and GMSK) over the AWGN channel. Assume
that M = 16 and N = 3 for the three CPM waveforms, and
d = 10 m for all the modulation schemes compared.
REFERENCES
[1] Z. Fei, B. Li, S. Yang, C. Xing, H. Chen, and L. Hanzo, “A survey
of multi-objective optimization in wireless sensor networks: Metrics,
algorithms, and open problems,” IEEE Communications Surveys &
Tutorials, vol. 19, no. 1, pp. 550–586, First Quarter 2017.
[2] H. Cao, J. Du, H. Zhao, D. X. Luo, N. Kumar, L. Yang, and F. R.
Yu, “Toward tailored resource allocation of slices in 6G networks with
softwarization and virtualization,” IEEE Internet of Things Journal,
vol. 9, no. 9, pp. 6623–6637, May 2022.
[3] K. T. K. Cheung, S. Yang, and L. Hanzo, “Achieving maximum energy-
efficiency in multi-relay OFDMA cellular networks: A fractional pro-
gramming approach,” IEEE Transactions on Communications, vol. 61,
no. 7, pp. 2746–2757, Jul. 2013.
[4] ——, “Maximizing energy-efficiency in multi-relay OFDMA cellular
networks,” in Proc. IEEE Global Communications Conference (GLOBE-
COM’13), Atlanta, USA, Dec. 2013, pp. 2767–2772.
[5] ——, “Spectral and energy spectral efficiency optimization of joint
transmit and receive beamforming based multi-relay MIMO-OFDMA
cellular networks,” IEEE Transactions on Wireless Communications,
vol. 13, no. 11, pp. 6147–6165, Nov. 2014.
[6] W. Jing, Z. Lu, X. Wen, Z. Hu, and S. Yang, “Flexible resource allocation
for joint optimization of energy and spectral efficiency in OFDMA multi-
cell networks,” IEEE Communications Letters, vol. 19, no. 3, pp. 451–
454, Mar. 2015.
[7] K. T. K. Cheung, S. Yang, and L. Hanzo, “Distributed energy spectral
efficiency optimization for partial/full interference alignment in multi-
user multi-relay multi-cell MIMO systems,” IEEE Transactions on
Signal Processing, vol. 64, no. 4, pp. 882–896, Feb. 2016.
[8] F. Tan, T. Lv, and S. Yang, “Power allocation optimization for energy-
efficient massive MIMO aided multi-pair decode-and-forward relay
systems,” IEEE Transactions on Communications, vol. 65, no. 6, pp.
2368–2381, Jun. 2017.
[9] X. Miao, S. Yang, C. Wang, S. Wang, and L. Hanzo, “On the energy
efficiency of interference alignment in the K-user interference channel,”
IEEE Access, vol. 7, pp. 97 253–97 263, Aug. 2019.
[10] T. Abr˜ao, L. D. H. Sampaio, S. Yang, K. T. K. Cheung, P. J. E. Jeszensky,
and L. Hanzo, “Energy efficient OFDMA networks maintaining statisti-
cal QoS guarantees for delay-sensitive traffic,” IEEE Access, vol. 4, pp.
774–791, Mar. 2016.
[11] L. Zhao, S. Yang, X. Chi, W. Chen, and S. Ma, “Achieving energy-
efficient uplink URLLC with MIMO-aided grant-free access,” IEEE
Transactions on Wireless Communications, vol. 21, no. 2, pp. 1407–
1420, Feb. 2022.
[12] Z. Lin, M. Lin, B. Champagne, W.-P. Zhu, and N. Al-Dhahir, “Secrecy-
energy efficient hybrid beamforming for satellite-terrestrial integrated
networks,” IEEE Transactions on Communications, vol. 69, no. 9, pp.
6345–6360, Sep. 2021.
[13] Z. Lin, K. An, H. Niu, Y. Hu, S. Chatzinotas, G. Zheng, and J. Wang,
“SLNR-based secure energy efficient beamforming in multibeam satel-
lite systems,” IEEE Transactions on Aerospace and Electronic Systems,
vol. 59, no. 2, pp. 2085–2088, Apr. 2023.
[14] S. Cui, A. J. Goldsmith, and A. Bahai, “Energy-constrained modulation
optimization,” IEEE Transactions on Wireless Communications, vol. 4,
no. 5, pp. 2349–2360, Sep. 2005.
[15] F. Rosas and C. Oberli, “Modulation optimization for achieving energy
efficient communications over fading channels,” in Proc. IEEE 75th
Vehicular Technology Conference (VTC Spring’12), Yokohama, Japan,
May 2012, pp. 1–5.
[16] T. Wang, W. Heinzelman, and A. Seyedi, “Minimization of transceiver
energy consumption in wireless sensor networks with AWGN channels,”
in Proc. 46th Annual Allerton Conference on Communication, Control,
and Computing (Allerton’08), Monticello, USA, Sep. 2008, pp. 62–66.
[17] M. Abo-Zahhad, M. Farrag, and A. Ali, “Modeling and minimization of
energy consumption in wireless sensor networks,” in Proc. IEEE Inter-
national Conference on Electronics, Circuits, and Systems (ICECS’15),
Cairo, Egypt, Dec. 2015, pp. 697–700.
[18] F. Rosas and C. Oberli, “Modulation and SNR optimization for achiev-
ing energy-efficient communications over short-range fading channels,”
IEEE Transactions on Wireless Communications, vol. 11, no. 12, pp.
4286–4295, Dec. 2012.
[19] T. Aulin and C. Sundberg, “Continuous phase modulation – Part I: Full
response signaling,” IEEE Transactions on Communications, vol. 29,
no. 3, pp. 196–209, Mar. 1981.
[20] J. B. Anderson, T. Aulin, and C.-E. Sundberg, Digital Phase Modulation.
New York, USA: Plenum Press, 1986.
9
[21] O. Amin, S. Bavarian, and L. Lampe, “Cooperative techniques for
energy-efficient wireless communications,” in Green Radio Communi-
cation Networks, E. Hossain, V. K. Bhargava, and G. P. Fettweis, Eds.
Cambridge, UK: Cambridge University Press, 2012, ch. 6, pp. 125–149.
[22] Q. Wang, M. Hempstead, and W. Yang, “A realistic power consumption
model for wireless sensor network devices,” in Proc. 3rd Annual IEEE
Communications Society on Sensor, Mesh and Ad Hoc Communications
and Networks (SECON’06), Reston, USA, Sep. 2006, pp. 286–295.
[23] S. Zhang, S. Xu, G. Y. Li, and E. Ayanoglu, “First 20 years of green
radios,” IEEE Transactions on Green Communications and Networking,
vol. 4, no. 1, pp. 1–15, Mar. 2020.
[24] T. Aulin, N. Rydbeck, and C.-E. Sundberg, “Continuous phase mod-
ulation – Part II: Partial response signaling,” IEEE Transactions on
Communications, vol. 29, no. 3, pp. 210–225, Mar. 1981.
[25] K. Kassan, H. Far`es, D. C. Glattli, and Y. Lou¨et, “Performance vs.
spectral properties for single-sideband continuous phase modulation,”
IEEE Transactions on Communications, vol. 69, no. 7, pp. 4402–4416,
Jul. 2021.
[26] M. Foruhandeh, M. Uysal, I. Altunbas, T. Guven, and A. Gercek,
“Optimal choice of transmission parameters for LDPC-coded CPM,”
in Proc. IEEE Military Communications Conference (MILCOM’14),
Baltimore, USA, Oct. 2014, pp. 368–371.
","nanPrevious research mainly focused on studying the EC of modulation schemes that are sensitive to the nonlinearity of PAs. More specifically, EC minimization problems corresponding to M-ary quadrature amplitude modulation (MQAM) and multiple frequency-shift keying (MFSK) were studied. Other studied the relationship between the total EC per successfully transmitted information bit and the transmission distance while assuming different modulation methods. Additionally, the transmission power of MQAM was optimized by using a particular model to achieve the minimum EC. EC per successfully transmitted bit for modulation techniques including binary frequency-shift keying (BFSK), BPSK, QPSK, 16QAM, and 64QAM was studied under various channel conditions."
"This study aims to predict the risk of non-adherence of patients and related systematic symptom scores with promising accuracy in the prediction of SCIT nonadherence in Allergic Rhinitis (AR) patients. We creatively apply sequential models in the long-term management of SCIT. Our findings demonstrate that these models are not only effective in predicting patient adherence to medical treatments but also invaluable in enhancing treatment strategies, thereby making a significant contribution to patient-centered healthcare.","Allergic rhinitis is characterized by allergen-specific IgE-mediated inflammation in upper respiratory inflammation with a prevalence of up to 30 % worldwide. Due to the long duration of SCIT, cumbersome process, slow onset, individual differences in treatment effect, and other factors fundamentally impact the completeness of therapeutics. Patient adherence is a critical factor in ensuring long-lasting efficacy and sustaining symptom-relieving effects. Due to the multitude of personalized data from patients, how to precisely identify and assess the risk of upcoming non-adherent behavior, a clinical prediction model is promising in the application.","The study design is a critical component that shapes the direction and reliability of our research. It includes a systematic approach to selecting the study population, the treatment methods applied, and the evaluation criteria. Data were collected from patient records in hospitals, and the following information was extracted for analysis: patient age, gender, distance to clinic, ratio of AIT cost to family income, allergen test results, etc., as well as patient VAS system score and medication score information, including baseline data of patients before injection therapy, adverse reactions to SCIT.","Excluding the biased samples at the first time step, the predictive adherence accuracy of the SLAC models is from 60 % to 72%, and for LSTM models, it is 66 % to 84 %, varying according to the time steps. The range of Root Mean Square Error (RMSE) for SLAC models is between 0.93 and 2.22, while for LSTM models it is between 1.09 and 1.77. Notably, these RMSEs are significantly lower than the random prediction error of 4.55.","While LSTM outperforms SLAC in adherence prediction, SLAC excels in score prediction for patients undergoing SCIT for AR. The state-action-based SLAC adds flexibility, presenting a novel and effective approach for managing long-term AIT.",Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis,"Li Yin, Xiong Yu, Fan Wenxin, Wang Kai, Yu Qingqing, Si Liping, van der Smagt Patrick, Tang Jun, Chen Nutan","Sequential Model for Predicting Patient
Adherence in Subcutaneous Immunotherapy
for Allergic Rhinitis
Yin Li 1*, Yu Xiong 2∗, Wenxin Fan 3, Kai Wang 1, Qingqing Yu 1, Liping Si 4,
Patrick van der Smagt 5,6, Jun Tang 1⋆ and Nutan Chen 6
1 Department of Otorhinolaryngology, The First People’s Hospital of Foshan, China
2 Department of Otorhinolaryngology, The Second Affiliated Hospital of Guizhou
University of Traditional Chinese Medicine, Guiyang, China
3 Chinese Academy of Sciences Shenzhen Institutes of Advanced Technology,
Shenzhen, China
4 Department of Radiology, Zhongshan Hospital, Fudan University, Shanghai, China
5 ELTE University, Budapest, Hungary
6 Machine Learning Research Lab, Volkswagen Group, Munich, Germany
Correspondence*:
Jun Tang
fsyyytj@126.com
ABSTRACT
Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal treatment of
allergic rhinitis. How to enhance the adherence of patients to maximize the benefit of allergen
immunotherapy (AIT) plays a crucial role in the management of AIT. This study aims to leverage
novel machine learning models to precisely predict the risk of non-adherence of patients and
related systematic symptom scores, to provide a novel approach in the management of long-term
AIT.
Methods: The research develops and analyzes two models, Sequential Latent Actor-Critic
(SLAC) and Long Short-Term Memory (LSTM), evaluating them based on scoring and adherence
prediction capabilities.
Results: Excluding the biased samples at the first time step, the predictive adherence accuracy
of the SLAC models is from 60 % to 72%, and for LSTM models, it is 66 % to 84 %, varying
according to the time steps. The range of Root Mean Square Error (RMSE) for SLAC models is
between 0.93 and 2.22, while for LSTM models it is between 1.09 and 1.77. Notably, these RMSEs
are significantly lower than the random prediction error of 4.55.
Conclusion: We creatively apply sequential models in the long-term management of SCIT with
promising accuracy in the prediction of SCIT nonadherence in Allergic Rhinitis (AR) patients.
While LSTM outperforms SLAC in adherence prediction, SLAC excels in score prediction for
patients undergoing SCIT for AR. The state-action-based SLAC adds flexibility, presenting a
novel and effective approach for managing long-term AIT.
*These authors contributed equally to this work.
1
arXiv:2401.11447v1  [cs.LG]  21 Jan 2024
Y. Li et al.
Sequential Model for Predicting Patient Compliance
Keywords: Allergic rhinitis, Allergen immunotherapy, Adherence, Sequential model, Latent variable model
1
INTRODUCTION
Allergic rhinitis is characterized by allergen-specific IgE-mediated inflammation in upper respiratory
inflammation with a prevalence of up to 30 % worldwide (Meltzer, 2016). In addition to allergen avoidance
as the superior criterion, allergen-specific immunotherapy (AIT) aims to induce specific allergen immune
tolerance, consequently achieving a status of clinical symptom remission. The repeatable intake of the
specific unmodified or chemically modified allergens (allergoids) was the key to maintaining the symptoms.
Among these approaches of AIT, subcutaneous immunotherapy (SCIT), sublingual immunotherapy (SLIT),
and lymphatic immunotherapy (SLIT) are demonstrated as the mainstream treatments regarding efficacy,
safety, and side effects. Compared to the SLIT, SCIT is a clinic-dependent treatment in which the patient
accepted an allergen extract injection subcutaneously. It is divided into the initial treatment stage (dose
accumulation stage) and the maintenance treatment stage (dose maintenance stage). The World Health
Organization (WHO) recommends that immunotherapy be maintained for three to five years and clinically
recommended for at least two years. Patient adherence is a critical factor in ensuring long-lasting efficacy
and sustaining symptom-relieving effects.
Due to the long duration of SCIT, cumbersome process, slow onset, individual differences in treatment
effect, and other factors fundamentally impact the completeness of therapeutics. From the reported studies
on AIT, the rate of adherence ranged from around 25% to over 90% (Passalacqua et al., 2013). The WHO
adopted the definition of ‘adherence’ as “the extent to which a person’s behavior, such as taking medication,
following a diet, or executing lifestyle changes, corresponds with agreed recommendations from a health
care provider” (Organization et al., 2003). In recent EAACI guidelines, it is highlighted to educate patients
on how immunotherapy works and on explaining the importance of compliance to the regular doses for
three years of treatment(Roberts et al., 2018).
The multiple approaches were introduced into the field of improving adherence and supervising patient
outcomes with systematic and technological interventions to prevent incomplete discontinuation of the
treatment. The intervention from a clinic in advance running through the whole treatment cycle was
approved as an effective approach. In facing the multitude of personalized data from patients, how to
precisely identify and assess the risk of upcoming non-adherent behavior, a clinical prediction model is
promising in the application.
In healthcare, machine learning, especially sequential models, stands at the forefront of innovation,
providing new ways to analyze complex medical data and improve patient treatments. These models excel
in processing and analyzing time-dependent data, making them ideal for predicting patient adherence to
treatments like SCIT for AIT. By effectively utilizing sequential data, these algorithms uncover hidden
temporal patterns and correlations, leading to more accurate and personalized treatment plans. In this
study, we have selected and evaluated two specific sequential models tailored to this scenario. Our
findings demonstrate that these models are not only effective in predicting patient adherence to medical
treatments but also invaluable in enhancing treatment strategies, thereby making a significant contribution
to patient-centered healthcare.
2
Y. Li et al.
Sequential Model for Predicting Patient Compliance
2
RELATED WORK
Non-sequential models for adherence prediction. A hybrid model was presented to combine artificial
neural networks (ANNs) and the genetic algorithm (GA) to examine 26 factors influencing adherence
to dietary plans (Mousavi et al., 2022). Wang et al. (2020) developed two machine learning methods,
neural network (NN) and support vector machine (SVM), to predict nonadherence in Crohn’s Disease
patients. These models assist caregivers by streamlining the intervention process using azathioprine (AZA).
Warren et al. (2022) investigated the relationship between patient adherence to prescribed medication for
opioid use disorder and various risk factors in patients with opioid use disorder. Four different machine
learning classifiers were proposed, including Logistic Regression, Decision Tree, Random Forest, and
XGBoost. They compared the performance of these classifiers to explore the association. Since the
adherence threshold can determine the prediction model’s performance by the binary adherence status,
Ruff et al. (2019) proposed a simulation framework to assess the performance of prediction models based
on different adherence thresholds.
Sequential models for adherence prediction. Hsu et al. (2022) investigated the advantages of
incorporating patient history into the prediction of medication adherence. They assessed the performance
of temporal deep learning models, particularly LSTM and simple Recurrent Neural Networks (RNN),
and compared these with non-temporal models such as Multilayer Perceptrons (MLP), ridge classifiers,
and logistic regression. To optimize the efficacy of cognitive training for older adults, Singh et al. (2022)
employed multivariate time series analysis and developed personalized models for each patient to capture
their unique adherence patterns. However, the sequential data of patients is often characterized by fluctuating
adherence and high dropout rates, resulting in uneven, unaligned, and missing values in the time series
data. To address this challenge, Schleicher et al. (2023) applied change point detection to identify phases
with varying dropout rates, presented methods for handling uneven and misaligned time series, and utilized
time series classification to predict the user’s phase.
3
METHODS
In this section, we outline the comprehensive methodologies employed in our study. These methods are
designed to rigorously analyze the effectiveness of SCIT in treating AR patients. We detail the study design,
population criteria, treatment protocols, and sequential models to provide a clear understanding of our
research framework.
3.1
Study design
The study design is a critical component that shapes the direction and reliability of our research. It
includes a systematic approach to selecting the study population, the treatment methods applied, and the
evaluation criteria.
3.1.1
Population
A retrospective analysis including 205 AR patients who started SCIT treatment between August 2018
and September 2019 in the Immunotherapy Center at the First People’s Hospital of Foshan was performed.
According to the Guidelines for the Diagnosis and Treatment of Allergic Rhinitis (2015 Edition), the recruit
criteria were formulated: patients with skin index (SI) of skin prick test (SPT) ++ or above or specific
Immunoglobulin E(sIgE) level in serum to Derp/Derf ≥ 0.35 kU/L which exposure to dust mites was
confirmed as the major allergen by allergen tests, including: (1) Patients with mild to moderate asthma; (2)
3
Y. Li et al.
Sequential Model for Predicting Patient Compliance
Patients with moderate to severe persistent rhinitis; (3) Mild to moderate asthma with allergic rhinitis (and/or
allergic conjunctivitis); (4) Patients with mild to moderate asthma and eczema. Exclusion criteria included:
(1) severe or uncontrolled bronchial asthma with continuous monitoring of Forced Expiratory Volume in
one second (FEV1) < 70% per of the expected value; (2) Patients with asthma whose symptoms or reduced
lung function continue to fail to be controlled with grade 4 or 5 treatment; (3) Patients sensitized to other
allergens such as pet furs, pollens or molds; (4) Patients who are taking beta-2 blockers or angiotensin-
converting enzyme inhibitors; (5) Patients with serious underlying diseases, including cardiovascular and
cerebrovascular diseases, autoimmune diseases and immunodeficiency diseases, malignant diseases, and
chronic infectious diseases; (6) Patients with serious mental illness, lack of compliance, or inability to
understand the risks and limitations of treatment; This study followed the tenets of the Declaration of
Helsinki and was approved by the Ethics Committee of the First People’s Hospital of Foshan, Foshan,
China. Written informed consent was obtained from all of the study participants.
3.1.2
SCIT treatment and evaluation
Before administering SCIT to enrolled patients, they will first perform a routine physical examination,
inquire about related information since the last injection (including allergy symptoms), and post-injection,
patients are observed for 30 minutes in case of the occurrence of side effects. Standardized aluminum
hydroxide adsorbed Derp allergen extracts (Allergopharma, Reinbeck, Germany) were used for SCIT.
According to the manufacturer’s instructions, in the dose accumulation phase with weekly injections
of allergen extracts with a gradually increased concentration from 100 SQ-U/mL to 10, 000 SQ-U/mL,
respectively injected 0.2, 0.4, 0.8 mL; after reaching the maintenance dose, 100,000 standardized quality
units was used. In the maintenance phase, an injection interval of 6 ± 2 weeks was carried out according to
the manufacturer’s recommendations.
Patients receive regular treatment evaluations, including symptom scores and medication scores. The
symptom score recorded a total of nasal symptoms (nasal itching, sneezing, rhinorrhea, nasal congestion),
ocular symptoms (ocular itching, lacrimation), and pulmonary symptoms (shortness of breath, tightness in
chest, perennial cough, wheezing), and assessed symptom severity using the visual analogue scale (VAS).
In the VAS score, the score of each symptom is from 0 to 10. 0 indicates that the patient has no discomfort
and 10 indicates that the patient is very uncomfortable. The patient gives the score of each symptom
according to the actual situation, and the sum of all symptom scores is the symptom score. Medication
score recorded the use of current adjuvant medication within 1 month to reach symptom relief. The use of
oral antihistamines, antileukotrienes, and bronchodilators were recorded as one point, local glucocorticoids
as two points, oral glucocorticoids or combined medication (hormones and β2 receptor agonists) as three
points, and the total cumulative score was the medication score. Symptom scores and medication scores
were assessed once at registration of SCIT and then thereafter.
Due to the separated injection regimen within 16 weeks and thereafter, all the chosen patients completed
the four months of SCIT, we chose the fourth month as the starting point of the observation. According
to our previous experience, one year after the start was the peak of the withdrawal, so we added a time
point at 18 months to further assess and follow up on the related symptom score and individual status.
The data collection spans six time steps: at 0, 4, 12, 18, 24, and 36 months. This approach is standard in
medical treatment, although for optimal model performance, an equal distribution of time intervals would
be preferable.
4
Y. Li et al.
Sequential Model for Predicting Patient Compliance
3.1.3
Data Collection
Data were collected from patient records in hospitals, and the following information was extracted for
analysis: patient age, gender, distance to clinic, ratio of AIT cost to family income, allergen test results, etc.,
as well as patient VAS system score and medication score information, including baseline data of patients
before injection therapy, adverse reactions to SCIT. For the descriptive analysis, categorical variables were
given as numbers and percentages, and continuous variables were presented using mean, standard deviation,
median, interquartile range (IQR), and minimum and maximum values.
3.1.4
Survey methods
Adherence was defined as the accomplishment of three years of AIT including the patients further
received AIT. Non-adherence was defined as discontinuation of AIT at random time points during three
years. The follow-up contents included (1) the main reasons for patients’ discontinuation of treatment; (2)
the duration of discontinuation of treatment, and (3) Allergic symptoms after discontinuation of treatment.
3.2
Sequential models
The focus of our study is the development of sequential models that can efficiently and accurately predict
the progression of symptoms and adherence in patients undergoing SCIT. This involves a comprehensive
analysis of the data collected, structured to provide insights into the treatment’s effectiveness and patient
compliance over time. Additionally, we explore and compare two distinct sequential models.
3.2.1
Data
variables
patients
total
adherent
non-adherent
age
≤ 12
96 (46.7)
40
56
13–17
30 (14.6)
10
20
≥ 18
79 (38.7)
23
56
gender
Female
62 (30)
22
40
Male
143 (70)
51
92
distance to clinic(km)
≤ 10
136 (66)
56
80
> 10
69 (34)
17
52
cost/family income(%)
< 30
107 (52.4)
37
70
30–50
77 (37.4)
32
45
> 50
21 (10.2)
4
17
EOS(× 109/L)
0.37; 0.41
0.36; 0.52
0.38; 0.36
EOS %
0.05; 0.04
0.05; 0.05
0.05; 0.05
∆NR(%)
16.67; 59.70
30.00; 92.80
14.80; 50.00
∆PNIF(%)
11.90; 34.50
12.70; 39.30
11.10; 28.80
total IgE (kU/L)
286; 543
340; 487
226; 555
sIgE of Derp (kU/L)
30.80; 68.480
31.30; 74.40
30.40; 67.80
sIgE of Derf (kU/L)
40.00; 68.20
40.60; 75.10
37.10; 65.70
Derp SPT SI
1.04; 0.58
1.00; 0.59
0.82; 0.55
Derf SPT SI
1.00; 0.50
0.82; 0.51
0.80; 0.45
Table 1. Demographic and clinical data of the patients under subcutaneous immunotherapy. In the rows
from Age to Cost/Family income, values indicate the number of patients (percentage, if available). Other
rows represent the median and IQR. P-values are omitted due to their large values.
5
Y. Li et al.
Sequential Model for Predicting Patient Compliance
reasons for
SCIT withdrawal
number of non-adherent patients
5–12 mths
13–18 mths
19–24 mths
25–36 mths
total by reason
no clinical improvement
18
11
8
21
58
medical issue
3
1
2
0
6
improved efficacy
0
0
0
24
24
schooling
3
3
0
5
11
side effects
2
1
1
2
6
Covid-19
9
7
3
1
20
personal issue
0
3
0
4
7
total by time period
35
26
14
57
132
Table 2. Detailed reasons for withdrawal from SCIT at different time points.
0
1
2
3
4
5
6
7
8
9 10
0
50
nasal itching
0
1
2
3
4
5
6
7
8
9 10
0
50
sneezing
0
1
2
3
4
5
6
7
8
9 10
0
50
rhinorrhea
0
1
2
3
4
5
6
7
8
9 10
0
50
nasal congestion
0
1
2
3
4
5
6
7
8
9 10
0
25
50
ocular itching
0
1
2
3
4
5
6
7
8
9 10
0
50
100
lacrimination
0
1
2
3
4
5
6
7
8
9 10
0
100
shortness of breath
0
1
2
3
4
5
6
7
8
9 10
0
100
tightness in chest
0
1
2
3
4
5
6
7
8
9 10
0
100
perennial cough
0
1
2
3
4
5
6
7
8
9 10
0
100
wheezing
0
1
2
3
4
5
0
50
100
rescue medication score
time step 1
time step 2
time step 3
time step 4
time step 5
time step 6
Figure 1. Histogram of scores across six-time steps. Score value (horizontal axis) vs. count (vertical axis).
We have a dataset D, comprising sequences x1, . . . , xT ∈ R11, y1, . . . , yT−1 ∈ R1, and a corresponding
action at ∈ R1. In the context of healthcare, the observations encompass yt (see Table 2) whether the
patient will cease the treatment in the interval between the scoring measurements at xt and xt+1. The
actions at represent the ongoing medical procedures for the patient during the period from xt to xt+1 (see
6
Y. Li et al.
Sequential Model for Predicting Patient Compliance
z2
1
· · ·
z2
t
z2
t+1
z1
1
· · ·
z1
t
z1
t+1
x1
xt
xt+1
a1
at
s
· · ·
· · ·
y1
yt
yt+1
Figure 2. SLAC model schematic. Solid and dashed lines denote the generative and inference model
pathways, respectively. The gray circles represent observed data, and the white circles denote latent
variables. The figure is adapted from (Lee et al., 2020).
Fig. 1). In this context, at is binary, reflecting whether treatment is given, and is numerically equivalent to
the adherence variable yt. Despite their numerical equivalence, we maintain a distinction between action
at and adherence yt to enhance model clarity and accommodate future research expansions, potentially
allowing for a wider range of action values. For each patient, we possess basic information s ∈ R14
which includes age, gender, commute distance to clinic, ratio of cost to family income, eosinophils count,
eosinophils percentage, nasal allergen provocation test (change of nasal resistance, ∆NR(%)), peak nasal
inspiratory flow. ∆PNIF(%)), serum total IgE level, sIgE of Dermatophagoides pteronyssinus (Derp), sIgE
of Dermatophagoides farinae (Derf), skin prick test (Derp, Derf) (see Table 1 for more details).
3.2.2
Sequential latent variable models
In our research, we employ the Stochastic Latent Actor-Critic (SLAC) model (Lee et al., 2020). However,
our application differs from the original use of SLAC which is typically associated with reinforcement
learning. Instead, we leverage its sequential latent-variable model without Actor-Critic. This approach
aligns with similar methodologies found in other works (Krishnan et al., 2015; Karl et al., 2016; Gregor
et al., 2018). The choice of the SLAC model was motivated by its ability to facilitate more efficient learning
and superior generalization in intricate environments.
The sequential latent variable model consists of an inference model and a generative model (see Fig. 2).
The inference model in a sequential latent variable model typically aims to approximate the posterior
distribution of the latent variables given the observed data. It tries to infer the hidden states z based on the
observed inputs x and some internal state s. The inference models the probability distributions of the latent
variables z1 and z2 at different time steps. qϕ denotes the variational distribution parameterized by ϕ,
z1
1 ∼ qϕ(z1
1|x1, s)
(1)
z2
1 ∼ pϕ(z2
1|z1
1)
(2)
z1
t+1 ∼ qϕ(z1
t+1|xt+1, z2
t , at)
(3)
z2
t+1 ∼ pϕ(z2
t+1|z1
t+1, z2
t , at).
(4)
The generative model, on the other hand, describes how the observed data is generated from the latent
variables. It defines a process by which the latent states z give rise to the observations x. The generative
model is described by a set of equations that represent the probability distributions of both the initial latent
states and their transitions over time, as well as the likelihood of the observations given the latent states,
7
Y. Li et al.
Sequential Model for Predicting Patient Compliance
with pϕ indicating the parameterized generative distribution.
z1
1 ∼ p(z1
1)
(5)
z2
1 ∼ pϕ(z2
1|z1
1)
(6)
z1
t+1 ∼ pϕ(z1
t+1|z2
t , at)
(7)
z2
t+1 ∼ pϕ(z2
t+1|z1
t+1, z2
t , at)
(8)
xt ∼ pϕ(xt|z1
t , z2
t )
(9)
yt ∼ pϕ(yt|z1
t , z2
t ).
(10)
We have the evidence lower bound (ELBO):
log pϕ(x1:t+1|a1:t) ≥

E
(x1:T ,a1:T −1)∼D

E
z1:T ∼qϕ
T−1
X
t=0

log pϕ(xt+1|zt+1)
(11)
− DKL(qϕ(zt+1|xt+1, zt, at)∥pϕ(zt+1|zt, at))

.
For ease of notation, we have q(z1|x1, z0, a0) := q(z1|x1, s) and p(z1|z0, a0) := p(z1). The ELBO provides
a lower bound to the log-likelihood of the observed data, which is computationally intractable to compute
directly. It is composed of two terms: the expected log-likelihood of the observed data given the latent
variables, and the Kullback-Leibler (KL) divergence between the variational distribution and the prior
distribution of the latent variables. Minimizing the KL divergence can be interpreted as enforcing the
variational distribution to be as close as possible to the prior, while maximizing the expected log-likelihood
ensures that the model accurately captures the distribution of the observed data. To predict the adherence,
we have log pϕ(yt+1|zt+1) as a regularisor in the loss function.
The objective is to compute the parameters ϕ that minimize the KL divergence between the variational
and prior distributions of the latent variables, subject to certain constraints. These constraints are related
to the expected log-likelihood of the data under the model and are represented by the inequalities with
thresholds ξ. These thresholds ensure that while minimizing the losses, the model also satisfies a minimum
standard for score prediction and adherence classification performances.
Latent variable models, such as Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende
et al., 2014) and their variants (e.g., SLAC), often encounter challenges (Sønderby et al., 2016; Kingma
et al., 2016). Furthermore, a higher ELBO does not always lead to enhanced predictive performance, as
discussed by Alemi et al. (2018); Higgins et al. (2017). However, the integration of scheduling strategies
inspired by constrained optimization methods has been shown to significantly improve the training of latent
variable models (Rezende and Viola, 2018; Klushyn et al., 2019). Consequently, we formulate the training
8
Y. Li et al.
Sequential Model for Predicting Patient Compliance
of our model into an optimization problem
min
ϕ
E
(x1:T ,a1:T −1)∼D
""T−1
X
t=0

DKL
Y. Li et al.
Sequential Model for Predicting Patient Compliance
income was evaluated. The patients who undertook AIT cost less than 30% of monthly family income and
account for half the distribution of the population, while 12.9% non-adherent patients undertook the 50%
financial burden in AIT treatment.
The change of nasal resistance (NR) and peak nasal inspiratory flow (PNIF) after nasal allergen
provocation (NPT) was used to evaluate the severity of symptoms by combining the symptom score. The
change of NR after NPT from the adherent group was higher than the non-adherent group (30 % vs 14.8 %).
The laboratory tests such as total IgE, sIgE of Derp and Derf, and SPT did not exhibit a significant
difference between the two groups. For detailed characteristics of patients see Table 1.
The observed total non-adherence rate at the end of three years was 35.4% and the median of the SCIT
duration was 18 months in the study. The rate of dropout in the third year(43.0%) was highest in comparison
to the end of the first year (26.5 %) and the second year (30.0 %). The reason for the withdrawal from the
patients included the concern of COVID-19, especially at the beginning of 2020 accounting for a 25 %
portion of the non-adherent patients in the first year. The most influential reason for the withdrawal was
unreached expectations for clinical improvement (43.9%). Medical issues including pregnancy status during
the treatment period and other physical disorders were collected from patients leading to the withdrawal of
SCIT (4.5 %). The significantly improved symptoms contributed to the reason for dropout, especially after
two years SCIT treatments (18.2 %). The recorded cases from side effects accounted for 4.5 %, comprising
the local and systematic adverse reactions (see Table 2).
We have a total of 205 samples, which we have randomly divided into a test dataset comprising 20 %,
i.e., 41 samples. For our analysis, we employ a five-fold cross-validation approach. Additionally, we apply
zero-mean and unit standard deviation (STD) normalization to the variables x and s.
The Root Mean Square Error (RMSE) metric is used to evaluate the precision of our medical score
predictions. Furthermore, to assess the adherence predictions, we utilize a comprehensive set of metrics
including accuracy, precision, recall, and the F1 score, each offering a unique perspective on the
performance of our predictive models. Most of the figures in this study are presented using boxplots.
The uncertainties for both models are calculated using five-fold cross-validation. In addition, as SLAC is
a probabilistic model, we also perform 100 samples from the latent space to compute its uncertainty.
4.1
One-Step Prediction
metric
model
time step 1
time step 2
time step 3
time step 4
time step 5
accuracy
LSTM
1.00 ± 0.00
0.66 ± 0.03
0.80 ± 0.04
0.84 ± 0.05
0.74 ± 0.02
SLAC
1.00 ± 0.01
0.70 ± 0.06
0.72 ± 0.04
0.71 ± 0.04
0.60 ± 0.06
precision
LSTM
1.00 ± 0.00
0.72 ± 0.01
0.86 ± 0.06
0.90 ± 0.05
0.62 ± 0.03
SLAC
1.00 ± 0.00
0.75 ± 0.03
0.74 ± 0.03
0.71 ± 0.03
0.44 ± 0.05
recall
LSTM
1.00 ± 0.00
0.86 ± 0.06
0.83 ± 0.05
0.82 ± 0.08
0.61 ± 0.03
SLAC
1.00 ± 0.01
0.87 ± 0.06
0.90 ± 0.03
0.86 ± 0.04
0.70 ± 0.10
F1 score
LSTM
1.00 ± 0.00
0.79 ± 0.03
0.84 ± 0.03
0.85 ± 0.05
0.62 ± 0.03
SLAC
1.00 ± 0.00
0.81 ± 0.04
0.81 ± 0.02
0.78 ± 0.03
0.54 ± 0.06
Table 3. Comparison of LSTM and SLAC over different time steps. The results are expressed as a mean ±
standard deviation. The better results are highlighted in bold.
10
Y. Li et al.
Sequential Model for Predicting Patient Compliance
2
3
4
5
6
time step
0
1
2
3
4
5
RMSE
SLAC
LSTM
Figure 3. RMSE of the prediction step by step. The red dashed line is the RMSE of random prediction
with Uniform distribution. See Fig. 5 and 6 for more details.
1
2
3
4
5
time step
0.0
0.2
0.4
0.6
0.8
1.0
accuracy
SLAC
LSTM
Figure 4. Accuray of the prediction step by step. The red dashed line is the accuracy of random prediction
with Uniform distribution. See Table 3 for more details.
In this experiment, our focus is on predicting the immediate next step. Within the SLAC, the prediction
of yt is based on the sequence x1:t and actions a1:t−1. Additionally, we forecast the subsequent state xt+1
using the sequence x1:t along with actions a1:t. In contrast, for LSTM, the predictions for both yt and the
next state xt+1 are derived from x1:t and y1:t−1.
As illustrated in Fig.3, SLAC surpasses LSTM in performance beginning at time step two. The figure
indicates that with an increased amount of historical data (additional time steps), SLAC achieves greater
RMSE. Both SLAC and LSTM demonstrate considerably better over random prediction methods. Further
insights are provided in Fig. 5 and Fig. 6, which provides detailed representations of each feature. In the
prediction of specific local symptoms score, SLAC performs an improvement in error after step two with
all parameters compared to LSTM. The results from RMSE in nasal and ocular symptoms display relatively
high values compared to those for lower respiratory tract symptoms. This can be attributed to the fact that
11
Y. Li et al.
Sequential Model for Predicting Patient Compliance
nasal itching
sneezing
rhinorrhea
nasal congestion
ocular itching
lacrimination
shortness of breath
tightness in chest
perennial cough
wheezing
rescue medication score
0
1
2
3
4
5
RMSE
time step 2
time step 3
time step 4
time step 5
Figure 5. RMSE of SLAC one-step prediction across various scores and time steps.
the majority of patients in the cohort predominantly exhibited nasal and ocular symptoms, which presented
a wide range of scores.
Fig. 4 demonstrates that from steps two to four, accuracy in adherence predictions improves with the
inclusion of additional information. The first step exhibits a notable bias, as it only includes data from
adherent patients, as detailed in Sec. 3.1.2. Nonetheless, both models adeptly manage this bias and achieve
high-accuracy predictions. Prediction for the sixth step is not conducted due to the cessation of treatment
by the hospital. In the fifth step, there is a decline in accuracy, likely due to the extended time interval of 12
months. Table 3 illustrates details of the classification for one-step prediction. Initially, both models exhibit
perfect performance in Accuracy, Precision, and Recall at the first time step, but diverge in subsequent
steps. In terms of Accuracy, LSTM generally outperforms SLAC, particularly evident at time steps three,
four, and five. For Precision, LSTM again shows superior performance in the later time steps, except at
time step two where SLAC marginally leads. However, in the Recall metric, SLAC surpasses LSTM from
time step two onwards, indicating its strength in correctly identifying positive cases. The F1 score, which
balances precision and recall, shows LSTM generally ahead, except at time step two where SLAC has a
slight edge. This metric indicates LSTM’s balanced capability in both precision and recall, especially in the
later time steps. Overall, while both models start equally strong, LSTM demonstrates greater consistency
and effectiveness across most metrics and time steps. SLAC, while lagging slightly behind in accuracy and
precision, shows its robustness in recall, especially in the middle to later time steps.
12
Y. Li et al.
Sequential Model for Predicting Patient Compliance
nasal itching
sneezing
rhinorrhea
nasal congestion
ocular itching
lacrimination
shortness of breath
tightness in chest
perennial cough
wheezing
rescue medication score
0
1
2
3
4
5
RMSE
time step 2
time step 3
time step 4
time step 5
time step 6
Figure 6. RMSE of LSTM one-step prediction across various scores and time steps.
4.2
Rollouts
2
3
4
5
6
3
4
5
6
4
5
6
5
6
time step
0
1
2
3
4
5
RMSE
SLAC
LSTM
Figure 7. RMSE of the rollout prediction. The first time step in each subplot represents the beginning of
the rollout time step.
13
Y. Li et al.
Sequential Model for Predicting Patient Compliance
1
2
3
4
5
2
3
4
5
3
4
5
4
5
time step
0.0
0.2
0.4
0.6
0.8
1.0
accuracy
SLAC
LSTM
Figure 8. Accuracy of the rollout prediction. The first time step in each subplot represents the beginning
of the rollout time step.
In the rollout experiment, our focus extends to a longer-term prediction. The SLAC prediction of yt:T−1
and xt+1:T are computed based on x1:t and a1:t−1. Actions, {ai : i ≥ t} are inferred from the model’s
output, yt. Moreover, for time steps greater than t, we employ a prior in the latent space, which eliminates
the need for the input of xt:T. In the LSTM model, the predictions for yt:T−1 and xt+1:T are based on x1:t
and y1:t−1.
Fig. 7 and 8 illustrate the performance of our model in multi-step predictions. Similar to one-step
predictions, the accuracy generally improves with the availability of more information, except in the case
of the adherence prediction at the fifth step. The results demonstrate the model’s proficiency in making
long-term predictions.
4.3
Model as a simulator
Given the initial condition of a patient, we can assess the outcomes of various interventions. Clinically,
if the patient’s adherence to treatment significantly impacts the prognosis (and there is a possibility of
non-adherence), it becomes imperative for the doctor to emphasize treatment compliance. Conversely, if
adherence makes little difference, it suggests the therapeutic approach may be ineffective for this patient,
allowing the doctors to emphasize adherence efforts.
To evaluate the impact of varying actions on SLAC’s performance, we analyze how different actions
affect the resulting scores. In the absence of a ground truth with diverse actions for the same patient, our
focus shifts to examining whether the states are responsive to changes in actions. Considering initial states
x1:3 and actions a1:2, we do rollouts with a3:5, alternating between one and zero. This controlled alteration
reveals that the average predicted value of x6 under these conditions is −0.20. This value is computed from
the prediction outcomes for actions with ones minus those for actions with zeros. The result indicates that
our model can be used as a simulator for doctors to see the impact of different treatments/therapies. Since
the LSTM does not have similar functions (see Sec. 3.2.3), we only show the SLAC results.
14
Y. Li et al.
Sequential Model for Predicting Patient Compliance
−0.0012
−0.0010
−0.0008
−0.0006
−0.0004
−0.0002
0.0000
average feature importances
age
gender
distance to clinic
cost/family income (%)
EOS × 109/L
EOS (%)
∆NR(%)
∆PNIF(%)
total IgE
sIgE of Derp
sIgE of Derf
SPT of Derp
SPT of Derf
features
Figure 9. Importances of the factors.
4.4
Interpretability
Previous models and methods have been developed for interpreting machine learning algorithms,
including SHAP (Lundberg and Lee, 2017) and Captum (Kokhlikyan et al., 2020). We opt for Captum,
as it integrates more seamlessly with PyTorch-based code. Features with positive attributions positively
influence a model’s prediction for a certain class, increasing its confidence in that prediction. We perform
the measure of the factor importances using SLAC (see Fig. 9). The distance to the clinic significantly
impacts patient adherence, especially if a patient is located far from the clinic or has relocated, as they
are more likely to discontinue their visits. Following the distance, SPT of Derf and sIgE of Derf greatly
influence the adherence. In contrast,∆NR(%), EOS(%), and the cost/family income(%) have minimal
impacts.
4.5
Architecture and computation
In this study, computational experiments were performed using an NVIDIA GeForce GTX 1080 Ti GPU,
with the implementation done in PyTorch, version 2.1.0.
The SLAC model’s architecture featured 32 hidden dimensions each for variables z1 and z2. Its encoder
and decoder were symmetrically structured, each comprising five layers with 128 units. The primary
activation function was LeakyReLU, set with a negative slope coefficient of 0.2. Both the encoder and
decoder’s mean output layers were linear, while the STD layer utilized a Softplus activation. For binary
classification tasks, a Sigmoid activation was used for output.
The LSTM architecture included a hidden dimension size of 128, with two LSTM layers. The output
activation function for score prediction was linear, and like the SLAC model, a Sigmoid function was
employed for binary classification outputs.
Both models shared the same optimization settings. They utilized the RAdam (Liu et al., 2019) optimizer
with a learning rate of 0.001. The batch size was set at 64, and a gradient clipping value of 0.8 was applied
15
Y. Li et al.
Sequential Model for Predicting Patient Compliance
to ensure training stability. To prevent overfitting and enhance model generalization, a dropout rate of 0.05
was introduced. Additionally, both models incorporated Mixup as a data augmentation during training.
5
DISCUSSION
The reported adherence rates of SCIT ranged from around 23 % to 90 %, due to the non-uniform follow-up
duration (2-4 years) (Passalacqua et al., 2013; Lee et al., 2019; Lemberg et al., 2017; Yang et al., 2018).
Poor adherence in the three to five-year time span of AIT is an obstacle to reaching allergen tolerance
and symptom remission. Recently proposed an ‘adherence and persistence in AIT (APAIT)’ checklist
that assists researchers in assessing adherence or persistence to AIT in retrospective studies (Pfaar et al.,
2023). The present study is the first research about the application of machine learning models in the
adherence prediction of SCIT in AR patients. From our study, the accomplishment rate of the three-year
treatment cycle was relatively low (35.4 %), while the rate of dropout after two years accounts for half
(42.8 %) in the whole non-adherence cohort. Several researchers focus on these variables impacting the
compliance of medical behaviors to enhance the intervention approach to reduce the withdrawal caused by
disease-unrelated reasons. Even though the Covid-19 pandemic impacted the compliance of the majority of
the patients in the three-year cycle, the first year since the pandemic’s outbreak appears to fundamentally
built a barrier to patients, similar to the finding from Liu et al. (2021) that 11 % dropouts in the two years
SCIT was observed caused by Covid-19. We excluded the patients who dropped out in the dose buildup
phase within four months to minimize the dose-origin impact. Due to the uncovered cost from the public
health care system and commercial insurance, financial burden accounts for a non-negligible factor in
influencing the decision-making of patients. A similar finding from Lourenc¸o et al. (2020). indicated that
economic reasons contributed to the most frequent cause of SCIT cessation.
In comparison to the logistic regression analyses in the clinical field to identify independent predictors,
the model from our study performed convincing interpretability. Previous research primarily concentrated
on non-sequential prediction methods for adherence (Mousavi et al., 2022; Wang et al., 2020; Warren et al.,
2022; Ruff et al., 2019). This approach presents a significant limitation in treatment processes, particularly
for immunotherapy that often spans extended periods, such as three years. These non-sequential methods
tend to predict only the overall outcome, overlooking the intricacies of intermediate time steps. To facilitate
earlier intervention, a sequential model capable of making predictions at any given time step would be
markedly more beneficial. While some subsequent studies have introduced sequential models (Hsu et al.,
2022; Singh et al., 2022; Schleicher et al., 2023), their scope was restricted to predicting adherence alone.
Our study enhances this approach by incorporating a state-action model, which can predict both adherence
and score/state. This advancement allows for more precise and detailed analysis of patient cases by medical
professionals.
Our study demonstrates notable findings in the domain of patient adherence prediction in subcutaneous
immunotherapy. The comparison between the SLAC model and LSTM model reveals the distinct strengths
and limitations of each approach. Notably, the SLAC exhibits greater flexibility, and it outperforms the
LSTM in score prediction. This advantage likely stems from its ability to efficiently learn and generalize in
complex environments, a trait that is crucial in medical data analysis. Conversely, the LSTM model shows
better performance in predicting adherence, indicating its potential utility in scenarios where accurate
forecasting of patient compliance is critical. Both models demonstrate the capability to handle longer
sequences, extending beyond one-step prediction. This ability is crucial in medical settings where long-term
patient monitoring and prediction are essential for effective treatment planning.
16
Y. Li et al.
Sequential Model for Predicting Patient Compliance
Overall, the study underscores the importance of selecting the appropriate model based on the specific
requirements of the task, whether it be flexibility, precision in score prediction, or adherence prediction.
The findings contribute to the growing field of machine learning applications in healthcare, particularly
in enhancing patient-centered treatment strategies through accurate and personalized predictions. Future
research could focus on evaluating the SLAC model’s performance in simulating various actions, further
enriching its applicability in clinical settings.
6
CONCLUSION
In summary, our study showcases the flexibility and score prediction superiority of the Sequential Latent
Actor-Critic (SLAC) model, while highlighting the superior adherence prediction capabilities of the Long
Short-Term Memory (LSTM) model. Both models demonstrate proficiency in managing extended sequence
predictions, extending beyond single-step forecasts. This study emphasizes the utility and significance of
selecting the appropriate model based on specific task requirements, be it for precision in score prediction
or adherence forecasting. These findings make a substantial contribution to the application of machine
learning in healthcare, particularly in improving patient-centered treatment strategies through accurate and
tailored predictions.
CONFLICT OF INTEREST STATEMENT
The authors declare that the research was conducted in the absence of any commercial or financial
relationships that could be construed as a potential conflict of interest.
DATA AVAILABILITY STATEMENT
The dataset for this study can be found in the Subcutaneous-Immunotherapy-Dataset repository.
REFERENCES
Alemi, A. A., Poole, B., Fischer, I., Dillon, J. V., Saurous, R. A., and Murphy, K. (2018). Fixing a broken
ELBO. ICML
Chen, N., Klushyn, A., Ferroni, F., Bayer, J., and Van Der Smagt, P. (2020). Learning flat latent manifolds
with vaes. ICML
Chen, N., van der Smagt, P., and Cseke, B. (2022). Local distance preserving auto-encoders using
continuous knn graphs. In Topological, Algebraic and Geometric Learning Workshops 2022. 55–66
Gregor, K., Papamakarios, G., Besse, F., Buesing, L., and Weber, T. (2018). Temporal difference variational
auto-encoder. arXiv preprint arXiv:1806.03107
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., et al. (2017). Beta-VAE: Learning
basic visual concepts with a constrained variational framework. ICLR
Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation 9, 1735–1780
Hsu, W., Warren, J. R., and Riddle, P. J. (2022). Medication adherence prediction through temporal
modelling in cardiovascular disease management. BMC Medical Informatics and Decision Making 22,
1–21
Karl, M., Soelch, M., Bayer, J., and Van der Smagt, P. (2016). Deep variational bayes filters: Unsupervised
learning of state space models from raw data. arXiv preprint arXiv:1605.06432
17
Y. Li et al.
Sequential Model for Predicting Patient Compliance
Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling, M. (2016). Improved
variational inference with inverse autoregressive flow. Advances in neural information processing
systems 29
Kingma, D. P. and Welling, M. (2014). Auto-encoding variational Bayes. ICML
Klushyn, A., Chen, N., Kurle, R., Cseke, B., and van der Smagt, P. (2019). Learning hierarchical priors in
VAEs. Advances in Neural Information processing Systems 32
Kokhlikyan, N., Miglani, V., Martin, M., Wang, E., Alsallakh, B., Reynolds, J., et al. (2020). Captum: A
unified and generic model interpretability library for pytorch. arXiv preprint arXiv:2009.07896
Krishnan, R. G., Shalit, U., and Sontag, D. (2015). Deep kalman filters. arXiv preprint arXiv:1511.05121
Lee, A. X., Nagabandi, A., Abbeel, P., and Levine, S. (2020).
Stochastic latent actor-critic: Deep
reinforcement learning with a latent variable model. Advances in Neural Information Processing Systems
33, 741–752
Lee, J.-H., Lee, S.-H., Ban, G.-Y., Ye, Y.-M., Nahm, D.-H., Park, H.-S., et al. (2019). Factors associated
with adherence to allergen specific subcutaneous immunotherapy. Yonsei medical journal 60, 570–577
Lemberg, M.-L., Berk, T., Shah-Hosseini, K., Kasche, E.-M., and M¨osges, R. (2017). Sublingual versus
subcutaneous immunotherapy: patient adherence at a large german allergy center. Patient preference
and adherence , 63–70
Liu, J., Feng, X., Wang, H., and Yu, H. (2021). Compliance with subcutaneous immunotherapy and factors
affecting compliance among patients with allergic rhinitis. American Journal of Otolaryngology 42,
103125
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., et al. (2019). On the variance of the adaptive learning
rate and beyond. arXiv preprint arXiv:1908.03265
Lourenc¸o, T., Fernandes, M., Coutinho, C., Lopes, A., Spinola Santos, A., Neto, M., et al. (2020).
Subcutaneous immunotherapy with aeroallergens-evaluation of adherence in real life
Lundberg, S. M. and Lee, S.-I. (2017). A unified approach to interpreting model predictions. Advances in
neural information processing systems 30
Meltzer, E. O. (2016). Allergic rhinitis: burden of illness, quality of life, comorbidities, and control.
Immunology and Allergy Clinics 36, 235–248
Mousavi, H., Karandish, M., Jamshidnezhad, A., and Hadianfard, A. M. (2022). Determining the effective
factors in predicting diet adherence using an intelligent model. Scientific Reports 12, 12340
Organization, W. H. et al. (2003). Adherence to long-term therapies: evidence for action (World Health
Organization)
Passalacqua, G., Baiardini, I., Senna, G., and Canonica, G. (2013). Adherence to pharmacological treatment
and specific immunotherapy in allergic rhinitis. Clinical & Experimental Allergy 43, 22–28
Pfaar, O., Devillier, P., Schmitt, J., Demoly, P., Hilberg, O., DuBuske, L., et al. (2023). Adherence and
persistence in allergen immunotherapy (apait): A reporting checklist for retrospective studies. Allergy
Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate
inference in deep generative models. In ICML. vol. 32, 1278–1286
Rezende, D. J. and Viola, F. (2018). Taming VAEs. CoRR
Roberts, G., Pfaar, O., Akdis, C., Ansotegui, I., Durham, S., Gerth van Wijk, R., et al. (2018). Eaaci
guidelines on allergen immunotherapy: allergic rhinoconjunctivitis. Allergy 73, 765–798
Ruff, C., Koukalova, L., Haefeli, W. E., and Meid, A. D. (2019). The role of adherence thresholds for
development and performance aspects of a prediction model for direct oral anticoagulation adherence.
Frontiers in Pharmacology 10, 113
18
Y. Li et al.
Sequential Model for Predicting Patient Compliance
Schleicher, M., Unnikrishnan, V., Pryss, R., Schobel, J., Schlee, W., and Spiliopoulou, M. (2023).
Prediction meets time series with gaps: User clusters with specific usage behavior patterns. Artificial
Intelligence in Medicine 142, 102575
Singh, A., Chakraborty, S., He, Z., Tian, S., Zhang, S., Lustria, M. L. A., et al. (2022). Deep learning-based
predictions of older adults’ adherence to cognitive training to support training efficacy. Frontiers in
Psychology 13, 980778
Sønderby, T., C. K.and Raiko, Maaløe, L., Sønderby, S. K., and Winther, O. (2016). Ladder variational
autoencoders. NeurIPS
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: a
simple way to prevent neural networks from overfitting. The journal of machine learning research 15,
1929–1958
Wang, L., Fan, R., Zhang, C., Hong, L., Zhang, T., Chen, Y., et al. (2020). Applying machine learning
models to predict medication nonadherence in crohn’s disease maintenance therapy. Patient preference
and adherence , 917–926
Warren, D., Marashi, A., Siddiqui, A., Eijaz, A. A., Pradhan, P., Lim, D., et al. (2022). Using machine
learning to study the effect of medication adherence in opioid use disorder. PLoS One 17, e0278988
Yang, Y., Wang, Y., Yang, L., Wang, J., Huang, N., Wang, X., et al. (2018). Risk factors and strategies in
nonadherence with subcutaneous immunotherapy: a real-life study. In International Forum of Allergy &
Rhinology (Wiley Online Library), vol. 8, 1267–1273
Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. (2017).
mixup: Beyond empirical risk
minimization. arXiv preprint arXiv:1710.09412
19
","nanIn healthcare, machine learning, especially sequential models, stands at the forefront of innovation, providing new ways to analyze complex medical data and improve patient treatments. These models excel in processing and analyzing time-dependent data, making them ideal for predicting patient adherence to treatments like SCIT for AIT. By effectively utilizing sequential data, these algorithms uncover hidden temporal patterns and correlations, leading to more accurate and personalized treatment plans. In this study, we have selected and evaluated two specific sequential models tailored to this scenario. Our findings demonstrate that these models are not only effective in predicting patient adherence to medical treatments but also invaluable in enhancing treatment strategies, thereby making a significant contribution to patient-centered healthcare."
The aim of this paper is to give lower bounds on the parameters of algebraic geometric error-correcting codes constructed from projective bundles over Deligne–Lusztig surfaces.,"The theory of algebraic geometry codes arose in 1970, when Goppa discovered in [Goppa (1970)]the relation between the theory of error-correcting codes and the evaluation on algebraic curves. They exhibited important properties that made researchers deepen their study.","Let (G, F) be a connected reductive algebraic group over an algebraically closed field k of positive characteristic p, equipped with an Fqₓstructure coming from a Frobenius morphism F : G → G. Let L : G → G be the corresponding Lang map taking an element g ∈ G to g⁻1F(g). By the Lang–Steinberg Theorem (see Theorem 4.4.17 in [Springer(1998)]), this morphism of varieties is surjective with finite fibers. From this result, it follows that, by conjugacy of Borel subgroups, there exists an F-stable Borel subgroup B. Let π : G → G/B := X denote the quotient. There are then (with a slight abuse of notation) natural endomorphisms F : W → W and F : X → X of the Weyl group of G and the variety X of Borel subgroups of G.","In this section, we shall define Deligne–Lusztig varieties and their compactiﬁcation as well as study some of their main properties.","By means of an intensive use of the intersection theory, we extend some of the previous results for codes over Deligne–Lusztig surfaces to the case of codes on projective bundles of rank 2 over standard Deligne–Lusztig surfaces. In particular, we compute the length, dimension and give a lower bound for the minimum distance in Theorem 4.1 for the cases of codes on projective bundles over Deligne–Lusztig surfaces of type A2(Fq), 2A3(Fq2), 2A4(Fq2) or C2(Fq).",Error-Correcting Codes on Projective Bundles over Deligne-Lusztig varieties,"Daniel Camazón Portela, Juan Antonio López Ramos","arXiv:2401.11433v1  [cs.IT]  21 Jan 2024
Error-Correcting Codes on Projective Bundles over Deligne–Lusztig
Varieties
Daniel Camazón ∗†, Juan Antonio López Ramos ‡§
daniel.camazon@uva.es, jlopez@ual.es
Abstract
The aim of this article is to give lower bounds on the parameters of algebraic geometric error-correcting
codes constructed from projective bundles over Deligne–Lusztig surfaces. The methods based on an intensive
use of the intersection theory allow us to extend the codes previously constructed from higher-dimensional
varieties, as well as those coming from curves. General bounds are obtained for the case of projective bundles
of rank 2 over standard Deligne-Lusztig surfaces, and some explicit examples coming from surfaces of type
A2 and 2A4 are given.
1
Introduction
The theory of algebraic geometry codes arose in 1970, when V. Goppa discovered in [Goppa(1970)]the relation
between the theory of error-correcting codes and the evaluation on algebraic curves. They exhibited important
properties that made researchers deepen their study. On the one hand, their good encoding–decoding algorithms
led B. McEliece ([McEliece(1978)]) to consider them for a public-key cryptosystem, which is considered by
NIST as an alternative for the post-quantum era. On the other hand, M.A. Tsfasman et al. ([Tsfasman(1982)])
were able to show that Goppa codes can be used to give examples of codes that go beyond the Gilbert–
Varshamov bound.
Since then, the interest in algebraic geometry has signiﬁcantly increased.
We can cite
[Barelli(2008), Christensen(2023), Hansen(1990), Xing(2002), Munuera(2008), Stichtenoth(1988)] as a few of
the many existing examples of study over Hermitian, Castle, Suzuki or G-H curves.
∗Department of Algebra, Analysis, Geometry and Topology of the University of Valladolid
†The ﬁrst author was partially supported by PGC2018-096446-B-C21
‡Department of Mathematics of the University of Almería
§The second suthor is supported by FQM 0211 Junta de Andalucía and Ministerio de Ciencia e Innovación PID2020-113552GB-
I00
1
But the deﬁnition of algebraic geometry codes can go beyond. Tsfasman and Vlˇadut˛, in [Tsfasman(1991)],
suggested that higher-dimensional varieties can be used to construct these type of codes, although the number of
works in this sense does not equal that of the curves, probably due to the diﬃculty of ﬁnding higher-dimensional
varieties X, spaces of functions L and sets of rational points P, that can yield codes in the sense of Deﬁnition 3.1,
which could be interesting due to their compelling properties concerning their weight distributions, minimum
distance or fast encoding–decoding algorithms.The two-dimensional case has been relatively studied, like in the
case of rational, hermitian or cubic surfaces, (cf. [Couvreur(2011), Edokou(2007), Voloch(2010)]).
However, this is not the case in general for higher-dimensional varieties. The survey by J. Little ([Little(2008)])
oﬀers a rather complete vision on the study of algebraic geometry codes deﬁned over varieties in general. Among
the referred papers, we can ﬁnd the work by S. Hansen [Hansen(2001)], wherein the author makes an extensive
use of the intersection theory to develop their study. In this paper, some of the provided examples concern
Deligne–Lusztig varieties, whose importance in algebraic geometry comes from the fact that they are directly
linked to ﬁnite groups of Lie type. Moreover, these varieties are characterized by their large number of rational
points, which allows for the deﬁnition of algebraic geometry codes, as we will see. In his paper, S. Hansen
applies general methods to obtain lower bounds on the parameters of algebraic geometric error-correcting codes
deﬁned from varieties of greater dimension, as is the case of Deligne–Lusztig surfaces. Our aim in this paper is
to go one step further [Hansen(2001)] and study this kind of codes deﬁned on projective bundles over Deligne–
Lusztig surfaces. In order to obtain lower bounds on the associated parameters, we make an intensive use of the
intersection theory and take advantage of the fact that, for some standard Deligne–Lusztig surfaces, all their
rational points are distributed equally on the disjoint rational curves, constituting the irreducible components
of a divisor Di. This allows us to give general bounds for the case of algebraic geometric error-correcting codes
on projective bundles of rank 2 as well as some explicit examples coming from surfaces of type A2 and 2A4.
2
Deligne–Lusztig Varieties
In this section, we shall deﬁne Deligne–Lusztig varieties and their compactiﬁcation as well as study some of
their main properties.
Let (G, F) be a connected reductive algebraic group over an algebraically closed ﬁeld k of positive characteristic
p, equipped with an Fq−structure coming from a Frobenius morphism F : G → G. Let L : G → G be the
corresponding Lang map taking an element g ∈ G to g−1F(g). By the Lang–Steinberg Theorem (see Theorem
4.4.17 in [Springer(1998)]), this morphism of varieties is surjective with ﬁnite ﬁbers. From this result, it follows
that, by conjugacy of Borel subgroups, there exists an F−stable Borel subgroup B. Let π : G → G/B := X
denote the quotient. There are then (with a slight abuse of notation) natural endomorphisms F : W → W and
F : X → X of the Weyl group of G and the variety X of Borel subgroups of G. Let W be generated by the
2
simple reﬂections s1, . . . , sn, and let l(·) be the length function with respect to these generators.
Let us now recall from Deﬁnition 1 in [Hansen(2002)] the following deﬁnition of Deligne–Lusztig variety.
Deﬁnition 2.1. Fix an element w in the Weyl group W, and let w = si1 · . . . · sir be a reduced expression of
w. Call w a Coxeter element if there in this expression occurs exactly one si from each of the orbits of F on
{s1, . . . , sn}. Denote by δ the order of F on this set. Then, the Deligne–Lusztig variety X(w) is deﬁned as the
image of L−1(B ˙wB) in G/B. That is,
X(w) = π(L−1(B ˙wB)).
(1)
Next, we will follow the notation and deﬁnitions given in [Hansen(2002)]. Deﬁne the closed subvariety of Xr+1
X(si1, . . . , sir) =

(g0B, . . . , grB) ∈ Xr+1 : g−1
k gk+1 ∈ B ∪ Bsik+1B for 0 ≤ k < r, g−1
r F(g0) ∈ B
	
(2)
In those cases wherein there is a unique product si1 · . . . · sir such that si1 · . . . · sir = w, we shall write X(w) for
the variety X(si1, . . . , sir). For any subset {sj1, . . . , sjm} ⊂ {si1, . . . , sir}, X(sj1, . . . , sjm) deﬁnes in a natural
way a closed subvariety of X(si1, . . . , sir). In particular, there are divisors
Dj = X(si1, . . . , ˆsij, . . . , sir); j = 1, . . . , r.
(3)
Example 2.2. Let us consider the Deligne–Lusztig surface X(w) of type A2.
In this particular case, the
connected reductive algebraic group is G = GL(3, k) and the Frobenius map is given by
F : g
/ gq .
(4)
Moreover, the Weyl group W of G is isomorphic to the symmetric group of the three vectors in the base of k3,
and X(w) = X(s1, s2), where s1 corresponds to the permutation of the ﬁrst and the second vectors of the base
and s2 corresponds to the permutation of the second and the third vector.
When G is semi-simple with connected Dynkin diagram D (with numbering of nodes and their associated simple
reﬂections), there is a (unique) natural choice of Coxeter element: let w = s1 · s2 · . . . · sr with r maximal (under
the condition that sr is not in the F−orbit of any of the previous si, i < r). When choosing this particular
Coxeter element, we shall refer to X(w) (or X(w)) as being of standard type.
We claim that X(w) is of classical type if w is a Coxeter element for one of the following classical groups:
An, 2A2n, 2A2n+1, Bm, Cn, Dn or 2Dn.
For w1, w2 ∈ W, we shall say that w1 and w2 are F−conjugate if there exists w′ ∈ W such that w2 =
w′w1F(w′)−1. It is worth noting that w and F(w) are F−conjugate for any w ∈ W.
3
Since the morphism L is ﬂat, it is open; hence, L−1(B ˙wB) = L−1(B ˙wB). Therefore, X(w) is a non-singular
variety of dimension n and the closure of X(w) in X, X(w), is given by the disjoint union
X(w) =
[
w′≤w
X(w′),
(5)
where, as usual, ≤ is the Bruhat order in W. This closure is usually singular whenever the Schubert variety
Xw = B ˙wB/B is. But since the open subset

g0B, . . . , grB) ∈ Xr+1 : g−1
k gk+1 ∈ Bsik+1B, 0 ≤ k < r, g−1
r F(g0) ∈ B
	
(6)
of the smooth projective variety X(w) maps isomorphically onto X(w) under projection to the ﬁrst factor, we
have a good compactiﬁcation of X(w). In fact, the complement of X(w) in X(w), which is easily seen to be
the union of the divisors Dj deﬁned above, is a divisor with normal crossings. If w is a Coxeter element, then
X(w) and X(w) are irreducible and, in fact, X(w) is isomorphic to X(w), and hence non-singular.
It is worth noting that it follows from Deﬁnition 2.1 that, if X(w) is of classical type, then the irreducible
components of any Deligne–Lusztig subvariety of X(w) are of classical type too (see Remark 1 in [Hansen(2002)]).
The following result allows us to consider the image of standard Deligne–Lusztig varieties under a certain proper
morphism as a normal strict complete intersection in a certain projective space PN−1 as well as to compute its
Picard group.
Theorem 2.3. Theorem 3 in [Hansen(2002)] Let X(w) be a standard Deligne–Lusztig variety of type 2An, Bn,
Cn, Dn or 2Dn. Assume that char(k) ̸= 2 in the orthogonal cases. Let P be the parabolic subgroup generated
by B together with the double cosets Bs2B, Bs3B, . . . , BsnB, and let
π : (G/B)l(w)+1 → G/P ⊆ P(V ) ∼= PN−1
(7)
be the projection (projection to the ﬁrst factor, followed by the quotient map).
It is worth noting that the
inclusion G/P ⊆ P(V ) is an equality in the non-orthogonal cases. Denote by Le the e−dimensional linear
subspace of PN−1 obtained by setting the N − 1 − e last coordinates as equal to zero.
i. The image Z = π(X(w)) is a normal, strict complete intersection. In fact, Lusztig shows (see pp. 444–
445 in [Lusztig(1976)]) that Z equals the support of the scheme’s theoretic complete intersection Z′ =
∩a0(V )−1
i=0
Hi, with X(w) mapping isomorphically onto the open subset
D
x, F a0(V )
V
(x)
E
̸= 0 of Z (see Table
2 in [Hansen(2002)]). In the unitary and orthogonal cases, the singular locus of Z, Zsing consists of the
ﬁnitely many GF −translates of the closed subscheme Z ∩ La0(V )−1. Hence,
codim(Zsing, Z) = N + 1 − 2a0(V ) + a0(La0(V )−1).
(8)
In the symplectic case, Zsing consists of the GF −translates of the closed subscheme Z ∩ La0(V )−2, and the
previous formula becomes
codim(Zsing, Z) = 2 + a0(La0(V )−2).
(9)
4
ii. For codim(Zsing, Z) ≥ 4, Pic(Z) = Z and consequently
Pic(X(w)) = Z [π∗H] ⊕ Z [{[V ] : V component of D1}]
⊕ j∗Al(w)−1(
[
i∈I−{1}
Di)
(10)
where H is the hyperplane section of Z, j is the obvious inclusion and I is the set of indices {i}, satisfying
that some connected component of the Dynkin diagram corresponding to Di occurs as a subgraph of the
Dynkin diagram corresponding to D1.
iii. For any Coxeter element w′, we have
Pic(X(w′))p′ ∼= Pic(X(w))p′.
3
Error-Correcting Codes Construction
Tsfasman and Vlˇadut˛ introduced the following construction (generalizing the Goppa–Manin construction):
Deﬁnition 3.1. H-construction p. 272 in [Tsfasman(1991)]. Let X be a normal projective variety over Fq. Let L
be a line bundle deﬁned over Fq and let P1, P2, ...Pn be distinct Fq−rational points on X. Set P = {P1, P2, ...Pn}.
In each Pi , choose isomorphisms of the ﬁbers LPi with Fq. The linear code C(L, P) of length n associated to
(X, L, P) is the image of the germ map
α : Γ(X, L) → ⊕n
i=1LPi ∼= Fq.
(11)
From now on, we assume that all line bundles considered actually have a non-zero global section.
Suppose L arises as the line bundle associated to a divisor D and that the Pi are not in the support of D. Then,
we obtain the same code (up to isomorphism) as when evaluating the rational functions
L(D) = {f ∈ k(X)∗ : div(f) + D ≥ 0}
in the points P.
The fundamental question is: Given a line bundle L on X, how many zeros does a section s ∈ Γ(X, L) have
along a ﬁxed set P of rational points?
Using the correspondence between line bundles and (Weil) divisors on a normal variety, we may reformulate
the question as follows: For a ﬁxed line bundle L, and given an eﬀective divisor D such that L = OX(D), how
many points from P are in its support |D|?
Although in the particular case of dim(X) = 1, where the points P ∈ P happen to be divisors, one may apply
the Riemann–Roch theorem to give a lower bound on d and a formula for k in higher dimensions; however, we
have to face the task of comparing objects of diﬀerent dimension. This may be remedied in two ways:
5
i. Make the objects have the same codimension by blowing-up at the points;
ii. Make the objects have complementary dimensions, that is, make the points in some way into curves.
In the next section, we shall pursue the latter idea. In this case, the following result establishes a lower bound
for the minimum distance.
Proposition 3.2. Proposition 3.2 in [Hansen(2001)] Let X be a normal projective variety deﬁned over Fq
of dimension at least two.
Let C1, C2, ..., Ca be (irreducible) curves on X with Fq− rational points P =
{P1, P2, . . . , Pn}. Assume the number of Fq−rational points on rational Ci is less than N. Let L be a line
bundle on X, deﬁned over Fq, such that L · Ci ≥ 0 for all i. Let
l = sups∈Γ(X,L)# {i : Z(s) contains Ci} .
Then, the code C(L, P) has length n and minimum distance
d ≥ n − lN −
a
X
i=1
L · Ci
If L · Ci = η ≤ N for all i, then d ≥ n − lN − (a − l)η.
In particular, if X is a non-singular surface, we can cite the following corollary.
Corollary 3.3. Corollary 3.2 in [Hansen(2001)]. Assume furthermore that X is a non-singular surface and
that H is a nef divisor on X with H · Ci > 0 for all i. Then,
l ≤
L · H
mini {Ci · H}
(12)
Consequently, if L · H < Ci · H for all i, we have l = 0 and
d ≥ n − m,
where m = Pa
i=1 L · Ci.
4
Some EC-Codes on Projective Bundles over Standard Deligne–
Lusztig Surfaces
This section is devoted to the computation of lower bounds for the parameters of certain error-correcting codes
on projective bundles over standard Deligne–Lusztig surfaces.
6
Given a standard Deligne–Lusztig surface S of type A2(Fq), 2A3(Fq2), 2A4(Fq2) or C2(Fq), for a suitable parabolic
subgroup P of G we have a commutative diagram:
S
j
/
ρ

G/B
π

Z
i/ G/P ∼= PN−1
(13)
where
Z =











P2
if S = A2(Fq),
H0 ≡ Xq+1
0
+ Xq+1
1
+ Xq+1
2
+ Xq+1
3
= 0
if S = 2A3(Fq2),
H0 ≡ Xq
0X3 − X0Xq
3 + X1Xq
2 − Xq
1X2 = 0
if S = C2(Fq),
(14)
and Z = H0 ∩ H1 with
H0 ≡ Xq+1
0
+ Xq+1
1
+ Xq+1
2
+ Xq+1
3
+ Xq+1
4
= 0,
(15)
H1 ≡ Xq3+1
0
+ Xq3+1
1
+ Xq3+1
2
+ Xq3+1
3
+ Xq3+1
4
= 0,
(16)
if S = 2A4(Fq2), i is an embedding, j is ﬁnite, π is locally trivial (in the Zariski topology) and ρ is birational
and surjective; see Sect. 5,6,7,8 in [Rodier(2000)].
Moreover, the surface S is isomorphic to the blow-up of (see Deﬁnition p.163 in [Hartshorne(1977)]) Z at a
certain set of points in the cases S = A2(Fq), 2A3(Fq2), C2(Fq), and all its rational points are distributed equally
on the disjoint rational curves Bi constituting the irreducible components of the divisor D1 ⊂ S, whereas in
the remaining case, S = 2A4, Z is obtained from S by contracting the disjoint hermitian curves Ai constituting
the irreducible components of the divisor D1 ⊂ S and all the rational points of S are distributed equally on the
disjoint rational curves Bi constituting the irreducible components of the divisor D2 ⊂ S.
The following theorem constitutes the main result of this paper, where explicit lower bounds for the parameters
of a certain class of codes on projective bundles over Deligne–Lusztig surfaces are given.
Theorem 4.1. Let S be a standard Deligne–Lusztig surface of type A2(Fq), 2A3(Fq2), 2A4(Fq2) or C2(Fq), and
let V be a vector bundle of rank 2 deﬁned over S. Consider the projective bundle T = P(V ) over S, that is
p : T → S. Then, for some a, b > 0, we can construct a code on T over Fqδ with parameters
i. n = #S(Fqδ)#P1(Fqδ);
ii. k = h0(S, Symmb(V ) ⊗ OS(aDj));
iii. d ≥ n−(−bc1(W1)# {Bi}+aDj·Di)#P1(Fqδ)−(#S(Fqδ)−(−bc1(W1)# {Bi}+aDj·Di))b if (−bc1(W1)# {Bi}+
aDj · Di) > 0, and d ≥ n − (#S(Fqδ))b otherwise,
7
where
δ =





1
if S is of type A2, C2,
2
if S is of type 2A3, 2A4,
Di =





D1
if S is of type A2, 2A3, C2,
D2
if S is of type 2A4,
Dj =





D2
if S is of type A2, 2A3, C2,
D1
if S is of type 2A4,
and c1(W1) denotes the ﬁrst Chern class of the line subbundle of minimum degree of the restricted vector bundle
i∗
BiV over Bi.
Proof. We will consider P to be the Fqδ−rational points on T .
Let C1, C2, ..., Ca be the ﬁbers over the
Fqδ−rational points of S. These disjoint lines contain all Fqδ−rational points of T , that is,
T (Fqδ) =
[
P ∈S(Fqδ )
p−1(P)(Fqδ).
(17)
It follows then that the length n of the code is
n = #P = #S(Fqδ) · #P1(Fqδ)
(18)
Let L be the line bundle L = OT (b) ⊗ OT (ap∗(Dj)) over T .
From Theorem 9.6 in [Eisenbud(2016)] and
Proposition II.7.11 in [Hartshorne(1977)], it follows that
Γ(T, L) ∼= Γ(S, p∗L) = Γ(S, Symmb(V ) ⊗ OS(aDj)),
(19)
so when in the range wherein the bound on the minimum distance ensures the injectivity of the evaluation map,
the dimension of the code is
k = h0(S, Symmb(V ) ⊗ OS(aDj)).
(20)
Now, we will apply Proposition 3.2 in order to obtain the bound for the minimum distance. It is worth noting
that, in the cases we are interested in, Ci = p−1(P) with P ∈ S(Fqδ), so the maximum number of rational
points on Ci will be N = #P1(Fqδ). Moreover, by Lemma 9.7 in [Eisenbud(2016)], we have that
L · Ci = b.
(21)
From Equation 17, we know that S#S(Fqδ )
i=1
Ci ⊂ p∗Di. Now, Bi is an irreducible component of Di, it is rational
and TBi = p−1(Bi) will be isomorphic to the projective bundle pTBi : P(i∗
BiV ) → Bi, where iBi : Bi → S
denotes the closed embedding. As a consequence, by Theorem 9.6 in [Eisenbud(2016)], the Chow ring of TBi is
isomorphic to
A•(TBi) ∼= A•(Bi) [ξ] /(ξ2 + c1(i∗
BiV )ξ)
(22)
8
where ξ is the hyperplane section in TBi.
It is worth noting that since Bi is rational, then by
Corollary V.2.14 in [Hartshorne(1977)], i∗
BiV = W1 ⊕ W2, and we can always suppose that deg(W1) ≤ deg(W2).
Now, if we restrict the line bundle L to TBi, then L|TBi ∼= OTBi (b) ⊗ OTBi (p∗
Bii∗
Bi(aDj)). Let H = ξ + c1(W2)F
be a nef divisor on TBi (see Theorem V.2.17 [Hartshorne(1977)]). Then, since H ·Ci = 1, it follows by Corollary
3.3 that
lTBi ≤ H · L|Bi = −bc1(W1) + aDj · Bi,
(23)
provided that (−bc1(W1) + aDj · Bi) > 0, and lTBi = 0 otherwise. As Di = ⊔Bi, we can conclude the following
bound for the minimum distance
d ≥ n − (−bc1(W1)# {Bi} + aDj · Di)#P1(Fqδ) − (#S(Fqδ) − (−bc1(W1)# {Bi} + aDj · Di))b
(24)
if (−bc1(W1)# {Bi} + aDj · Di) > 0 and
d ≥ n − (#S(Fqδ))b
(25)
otherwise.
Before computing the explicit parameters of some families of these codes, we need the following auxiliary result.
Lemma 4.2. Let Y be a variety and V = V1 ⊕ V2 a vector bundle of rank 2 over Y , that is, a direct sum of
two line bundles V1 and V2. Then, the symmetric algebra of V satisﬁes
Symm(V ) ∼=
2
O
i=1
Symm(Vi) =
M
i1,i2∈N2
Symmi1V1 ⊗ Symmi2V2 ∼=
M
i1,i2∈N2
V ⊗i1
1
⊗ V ⊗i2
2
,
(26)
Proof. The ﬁrst isomorphism is a consequence of the universal property for symmetric algebras, the second
equality is just by deﬁnition and the last isomorphism follows from the fact that, for a line bundle Vi, we have
V ⊗n
i
∼= SymmnVi.
(27)
The following corollaries give explicit bounds for the parameters of certain families of codes on projective bundles
of rank 2 over Deligne-Lusztig surfaces of type A2 and 2A4, obtained by restricting the previous results to certain
vector bundles deﬁned over them.
Corollary 4.3. Let S1 be the Deligne–Lusztig surface of type A2 deﬁned over the ﬁeld Fq. Consider some b,
such that 0 < b < (q + 1), and Vi = OS1(niH − Pq2+q+1
j=1
mi,jBj) for i = 1, 2, where H = π∗(OP2(1)), verifying
for any pair i1, i2 ∈ N with i1 + i2 = b:
9
i. (i1n1 + i2n2) ≤ 3(q − 1);
ii. (i1n1 + i2n2) ≥ P3
j=1(i1m1,j + i2m2,j);
iii. (i1m1,j + i2m2,j) ≥ (i1m1,j+1 + i2m2,j+1);
iv. 3(i1n1 + i2n2) > Pq2+q+1
j=1
(i1m1,j+1 + i2m2,j+1).
Let T1 be the projective bundle P(V1 ⊕ V2) over S1, p1 : T1 → S1. Then, we can construct a code on T1 over Fq
with parameters
i. n = (q2 + q + 1)(q + 1)2,;
ii. k = P
i1,i2∈N2
i1+i2=b
1
2((i1n1 + i2n2)((i1n1 + i2n2) + 3) − Pq2+q+1
j=1
(i1m1,j + i2m2,j)((i1m1,j + i2m2,j) + 1)) + 1,;
iii. d ≥ n − (q2 + q + 1)(q + 1)b..
Proof. Since A2 is isomorphic to the blow-up of P2 at the set of its rational points over Fq (see Sect. 5 in
[Rodier(2000)]), we have that #S1(Fq) = (q2 + q + 1)(q + 1). Moreover, #P1(Fq) = (q + 1), so we can conclude
that the length n of the code is
n = #P = #S1(Fq)(q + 1) = (q2 + q + 1)(q + 1)2
(28)
Let L be the line bundle L = OT1(b). In order to compute the dimension of the code, we have dimΓ(T1, L) =
dimΓ(S1, p1∗L), where p1∗L ∼= Symmb(V1 ⊕ V2). As a result of Lemma 4.2 ,
Symmb(V1 ⊕ V2) =
M
i1,i2∈N2
i1+i2=b
OS1(n1H −
q2+q+1
X
j=1
m1,jBj)⊗i1 ⊗ OS1(n2H −
q2+q+1
X
j=1
m2,jBj)⊗i2.
(29)
Now, global sections of a direct sum of line bundles satisfy
Γ(S1,
M
i1,i2∈N2
i1+i2=b
OS1(n1H −
q2+q+1
X
j=1
m1,jBj)⊗i1 ⊗ OS1(n2H −
q2+q+1
X
j=1
m2,jBj)⊗i2) ∼=
M
i1,i2∈N2
i1+i2=b
Γ(S1, OS1(n1H −
q2+q+1
X
j=1
m1,jBj)⊗i1 ⊗ OS1(n2H −
q2+q+1
X
j=1
m2,jBj)⊗i2),
(30)
and ﬁnally, we can conclude
M
i1,i2∈N2
i1+i2=b
Γ(S1, OS1(n1H −
q2+q+1
X
j=1
m1,jBj)⊗i1 ⊗ OS1(n2H −
q2+q+1
X
j=1
m2,jBj)⊗i2) ∼=
M
i1,i2∈N2
i1+i2=b
Γ(S1, OS1((i1n1 + i2n2)H −
q2+q+1
X
j=1
(i1m1,j + i2m2,j)Bj))
(31)
10
Furthermore, by the hypothesis of the theorem, OS1((i1n1 + i2n2)H − Pq2+q+1
j=1
(i1m1,j + i2m2,j)Bj) is excellent
with respect to the exceptional conﬁguration of π : S1 → P2 (see p. 215 in [Harbourne(1985)]), so if we denote
F = OS1((i1n1 + i2n2)H − Pq2+q+1
j=1
(i1m1,j + i2m2,j)Bj), then by Theorem 1.1 in [Harbourne(1985)]:
h0(S1, F) = 1
2(f · f − f · kS1) + 1
= 1
2((i1n1 + i2n2)((i1n1 + i2n2) + 3) −
q2+q+1
X
j=1
(i1m1,j + i2m2,j)((i1m1,j + i2m2,j) + 1)) + 1.
(32)
This allows us to conclude that, when in the range wherein the bound on the minimum distance ensures the
injectivity of the evaluation map, the dimension of the code is
k =
X
i1,i2∈N2
i1+i2=b
1
2((i1n1 + i2n2)((i1n1 + i2n2) + 3) −
q2+q+1
X
j=1
(i1m1,j + i2m2,j)((i1m1,j + i2m2,j) + 1)) + 1.
(33)
Finally, by Proposition 9.4 in [Eisenbud(2016)], any section of L|T1Bi (recall that T1Bi = p−1
1 (Bi)) intersects a
ﬁber Ci ⊂ T1Bi in a hyperplane, so l = 0 and we can conclude the following bound for the minimum distance:
d ≥ n − (q2 + q + 1)(q + 1)b.
(34)
Corollary 4.4. Let S2 be the Deligne–Lusztig surface of type 2A4 deﬁned over the ﬁeld Fq2. Consider Vi =
π∗i∗OP4(ti) for i = 1, 2, and let T2 be the projective bundle P(V1 ⊕ V2) over S2, p2 : T2 → S2. Then, for some
0 < b < (q2 + 1) and for any t1, t2 ∈ N such that (q + 1) < (i1t1 + i2t2) < (q3 + 1) for any pair i1, i2 ∈ N with
i1 + i2 = b, we can construct a code on T2 over Fq2 with parameters
i. n = (q5 + 1)(q3 + 1)(q2 + 1)2;
ii. k = P
i1,i2∈N2
i1+i2=b
Let L be the line bundle L = OT2(b). In order to compute the dimension of the code, we have that dimΓ(T2, L) =
dimΓ(S2, p2∗L), with p2∗L ∼= Symmb(V1 ⊕ V2). As a result of Lemma 4.2
Symmb(V1 ⊕ V2) =
M
i1,i2∈N2
i1+i2=b
π∗i∗OP4(t1)⊗i1 ⊗ π∗i∗OP4(t2)⊗i2.
(36)
Now, global sections of a direct sum of line bundles satisfy
Γ(S2,
M
i1,i2∈N2
i1+i2=b
π∗i∗OP4(t1)⊗i1 ⊗ π∗i∗OP4(t2)⊗i2) ∼=
M
i1,i2∈N2
i1+i2=b
Γ(S2, π∗i∗OP4(t1)⊗i1 ⊗ π∗i∗OP4(t2)⊗i2),
(37)
and since pull-back commutes with tensor product, it follows then that
M
i1,i2∈N2
i1+i2=b
Γ(S2, π∗i∗OP4(t1)⊗i1 ⊗ π∗i∗OP4(t2)⊗i2) ∼=
M
i1,i2∈N2
i1+i2=b
Γ(S2, π∗i∗OP4(i1t1 + i2t2))
(38)
Furthermore, as π is birational onto Z, we obtain (see Theorem 2.31 in [Iitaka(1982)])
Γ(S2, π∗i∗OP4(i1t1 + i2t2)) ∼= Γ(Z, i∗OP4(i1t1 + i2t2))
(39)
Since Z is the complete intersection of two hyper-surfaces H0, H1 ⊂ P4 of degrees q + 1 and q3 + 1, respectively,
we obtain short exact sequences of line bundles
0 → OP4(t − (q + 1)) → OP4(t) → OH0(t) → 0,
(40)
and
0 → OH0(t − (q3 + 1)) → OH0(t) → OH0∩H1(t) → 0,
(41)
(see Section 7.3 in [Iitaka(1982)]). These sequences give long exact sequences of cohomology groups,
0 → H0(P4, OP4(t − (q + 1))) → H0(P4, OP4(t)) → H0(H0, OH0(t)) →
H1(P4, OP4(t − (q + 1))) → H1(P4, OP4(t)) →
H1(H0, OH0(t)) → H2(P4, OPN−1(t − (q + 1))) → . . . ,
(42)
and
0 → H0(H0, OH0(t − (q3 + 1)) → H0(H0, OH0(t)) →
H0(H0 ∩ H1, OH0∩H1(t)) → H1(H0, OH0(t − (q3 + 1)) → . . .
(43)
By the formulas of Theorem III.5.1 in [Hartshorne(1977)] for the cohomology of projective space, the ﬁrst long
exact sequence reduces (for any t) to
0 → H0(P4, OP4(t − (q + 1))) → H0(P4, OP4(t)) →
H0(H0, OH0(t)) → 0 → 0 → H1(H0, OH0(t)) → 0 → . . .
(44)
12
Hence, for any t, H1(H0, OH0(t)) = 0 and
dimH0(H0, OH0(t)) = dimH0(P4, OP4(t)) − dimH0(P4, OP4(t − (q + 1))
(45)
=
4 + t
t

−
4 + t − (q + 1)
t − (q + 1)

.
(46)
Consequently, for any q + 1 < t < q3 + 1, the last sequence then gives
Γ(Z, OZ(t)) ∼= H0(H0, OH0(t)),
(47)
so, when in the range wherein the bound on the minimum distance ensures the injectivity of the evaluation
map, the dimension of the code is
k =
X
i1,i2∈N2
i1+i2=b
4 + i1t1 + i2t2)
i1t1 + i2t2)

−
4 + i1t1 + i2t2) − (q + 1)
i1t1 + i2t2) − (q + 1)

.
(48)
Finally, by Proposition 9.4 in [Eisenbud(2016)], any section of L|T2Bi (recall that T2Bi = p−1
2 (Bi)) intersects
Ci ⊂ T4Bi in an hyperplane, so l = 0 and we can conclude the following bound for the minimum distance:
d ≥ n − (q5 + 1)(q3 + 1)(q2 + 1)b.
(49)
Finally, we compute the parameters of some of the codes in Corollary 4.3 and Corollary 4.4 for the binary case,
q = 2, as this is the most common ﬁnite ﬁeld within applications.
Example 4.5. Let us consider the particular case q = 2, with b = 1, n1 = n2 = 3, m1,j = m2,j = 1 for
j = 1, 2, 3, and m1,j = m2,j = 0 otherwise, for the family of codes presented in Corollary 4.3. Then, we obtain
codes with the following parameters:
i. n = 63;
ii. k = 14;
iii. d ≥ 42.
Example 4.6. For q = 2, let us consider the particular case b = 2, t1 = t2 = 4 for the family of codes presented
in Corollary 4.4. Then we obtain a code with the following parameters:
i. n = 7425,
ii. k = 1107,
iii. and d ≥ 4455.
13
5
Conclusions
By means of an intensive use of the intersection theory, we extend some of the previous results for codes
over Deligne–Lusztig surfaces to the case of codes on projective bundles of rank 2 over standard Deligne–
Lusztig surfaces. In particular, we compute the length, dimension and give a lower bound for the minimum
distance in Theorem 4.1 for the cases of codes on projective bundles over Deligne–Lusztig surfaces of type
A2(Fq), 2A3(Fq2), 2A4(Fq2) or C2(Fq). Moreover, in Corollary 4.3 and Corollary 4.4, we focus on some special
families of these codes, by restricting our results to certain vector bundles over Deligne–Lusztig surfaces of type
A2 and 2A4, in order to give a more explicit computation of their dimension. Finally, we give two examples of
binary codes, as seen in Example 4.5 and Example 4.6, motivated by the fact that q = 2 is the most common
framework within applications. In Example 4.5, we obtain an information rate k/n = 14/63 and an error-
correcting rate δ = d/n ≥ 42/63, and in Example 4.6 we obtain an information rate k/n = 1107/7425 and an
error-correcting rate δ = d/n ≥ 4455/7425, concluding that both codes exhibit high error-correcting rates.
Some interesting problems would be to extend these results to the case of projective bundles of higher rank over
standard Deligne–Lusztig surfaces following the techniques used in [Nakashima(2006)], and to establish a more
explicit understanding on how the morphism
π : X(w) → Z of Theorem 2.3 behaves for Deligne–Lusztig varieties of dimension≥ 3 in order to construct new
algebraic geometric error-correcting codes over them.
References
[Goppa(1970)] Goppa, V.D. A new class of linear error-correcting codes. Probl. Inf. Transm. 1970, 6, 300–304.
[McEliece(1978)] McEliece, R.J. A public-key cryptosystem based on algebraic coding theory. DSN Prog. Rep.
Jet Propuls. Lab. Pasadena 1978, 42(44) , 114–116.
[Tsfasman(1982)] Tsfasman, M.A.; Vladut, S.G.; Zink, T. Modular curves, Shimura curves, and Goppa codes,
better than Varshamov-Gilbert bound Math. Nachr. 1982, 109, 21–28.
[Barelli(2008)] Barelli, E.; Beelen, P.; Datta, M.; Neiger, V.; Rosenkilde, J. Two-point codes for the generalized
GK curve. IEEE Trans. Inform. Theory 2008, 64, 6268–6276.
[Christensen(2023)] Christensen, R.B.; Munuera, C.; Pereira, F.; Ruano, D. An algorithmic approach to
entanglement-assisted quantum error-correcting codes from the Hermitian curve. Adv. Math. Commun.
2023, 17, 78–97.
[Hansen(1990)] Hansen, J.P.; Stichtenoth, H. Group codes on certain algebraic curves with many rational points.
Appl. Algebra Engrg. Comm. Comput. 1990, 1, 67–77.
14
[Xing(2002)] Xing C., Chen, H. Improvements on parameters of one-point AG codes from Hermitian curves.
IEEE Trans. Inform. Theory 2002, 48, 535–537.
[Munuera(2008)] Munuera, C. Sepúlveda, A., Torres, F., Algebraic Geometry codes from Castle curves.
In Coding Theory and Applications. Lecture Notes in Computer Science; Barbero, A., Ed.; Springer:
Berlin/Heidelberg, Germany, 2008; pp. 117–127.
[Stichtenoth(1988)] Stichtenoth, H. A note on Hermitian codes over GF(q2). IEEE Trans. Inf. Theory 1988,
34, 1345–1348.
[Tsfasman(1991)] Tsfasman, M.A.; Vladut, S.G. Algebraic-Geometric Codes, 3rd ed.; Springer Kluwer: Dor-
drecht, The Netherlands, 1991.
[Couvreur(2011)] Couvreur, A. Construction of rational surfaces yielding good codes. Finite Fields Appl. 2011,
17, 424–441.
[Edokou(2007)] Edokou, F. Codes deﬁned by forms of degree 2 on hermitian surfaces and Srensen’s conjecture.
Finite Fields Appl. 2007, 13, 616–627.
[Voloch(2010)] Voloch, J.F. Algebraic geometric codes on surfaces. In Arithmetics, Geometry, and Coding The-
ory (AGCT 2005); Volume 21; Séminaires et Congrès-Société Mathématique de France : Paris, France,
2010; pp. 211–216.
[Little(2008)] Little, J.B. Algebraic geometry codes from higher dimensional varieties. In Advances in Algebraic
Geometry Codes; Serie Coding Theory Cryptology; Martinez-Moro, E., Munuera, C., Ruano, D., Eds.;
World Scientiﬁc Publishing: Hackensack, NJ, USA, 2008; Volume 5, pp. 257–293.
[Hansen(2001)] Hansen, S.H. Error-correcting codes from higher-dimensional varieties. Finite Fields Appl. 2001,
7, 531–552.
[Springer(1998)] Springer, T.A. Linear Algebraic Groups; Birkhäuser Boston: Boston, MA, USA, 1998.
[Hansen(2002)] Hansen, S. H. Picard groups of Deligne-Lusztig varieties—With a view toward higher codimen-
sions. Beiträge Algebra Geom. 2002, 43, 9–26.
[Lusztig(1976)] Lusztig, G. On the Green polynomials of classical groups. Proc. Lond. Math. Soc. 1976, 33,
443–475.
[Rodier(2000)] Rodier, F. Nombre de points des surfaces de Deligne et Lusztig J. Algebra 2000, 227, 706–766.
[Hartshorne(1977)] Hartshorne, R. Algebraic Geometry; Springer: New York, NY, USA; Heidelberg, Germany,
1977.
[Eisenbud(2016)] Eisenbud, D.; Harris, J. 3264 and All That—A Second Course in Algebraic Geometry; Cam-
bridge University Press: Cambridge, UK, 2016.
15
[Harbourne(1985)] Harbourne, B. Complete linear systems on rational surfaces. Trans. Amer. Math. Soc. 1985,
289, 213–226.
[Iitaka(1982)] Iitaka, S. Algebraic Geometry. An Introduction to Birational Geometry of Algebraic Varieties;
Springer: New York, NY, USA; Berlin, Germany, 1982.
[Nakashima(2006)] Nakashima, T. Error-correcting codes on projective bundles. Finite Fields Appl. 2006, 12,
222–231.
16
","Tsfasman and Vlădută, in [Tsfasman(1991)], suggested that higher-dimensional varieties can be used to construct these type of codes, although the number of works in this sense does not equal that of the curves, probably due to the difficulty of finding higher-dimensional varieties X, spaces of functions L and sets of rational points P, that can yield codes in the sense of Definition 3.1, which could be interesting due to their compelling properties concerning their weight distributions, minimum distance or fast encoding–decoding algorithms.The two-dimensional case has been relatively studied, like in the case of rational, hermitian or cubic surfaces, (cf. [Couvreur(2011), Edokou(2007), Voloch(2010)]).nan"
"Deformation of soft objects is a significant obstacle for robots to interact with the physical world. This research paper focuses on deformable object manipulation, particularly in the context of bimanual manipulation of a fabric bag, where the authors propose a novel approach centered around a Structure of Interest (SOI)-based latent dynamics model. The key contributions of this work include introducing the SOI concept to emphasize manipulation of critical object components, designing a GNN-based method to learn latent dynamics models, and integrating the learned dynamics into MPC for accurate and stable manipulation of the fabric bag. Empirical experiments on bimanual manipulation of a fabric bag validate the efficacy of the proposed framework, showcasing its potential for improving robotic systems' intelligent manipulation of deformable objects by focusing on crucial SOI regions.","Deformable object manipulation (DOM) is a challenging task for robots due to the infinite-dimensional configuration space and complex dynamics of these objects. Existing methods often resort to simplified physics models or data-driven methods with handcrafted features, lacking adaptability for the varied shapes and dynamics of these objects. This paper proposes a novel approach to DOM by introducing the concept of SOI into the domain, emphasizing the significance of identifying and manipulating critical structural components rather than the entire object. This focus on SOI significantly reduces the computational load and enables more effective modeling of the object's deformations.","To address the challenges in DOM, the authors propose a novel bimanual manipulation framework that leverages a GNN-based latent dynamics model. This method operates in the context of a Partially Observable Markov Decision Process (POMDP) framework, where the robot is tasked with manipulating the opening rim of a fabric bag using two 3D-printed robot grippers, while only having partial observations of the bag. The key component of this framework is the GNN-based model, which is designed to learn the dynamics of the SOI from partial point cloud data extracted from the bag. This model captures the essential deformations of the fabric bag in a reduced computational space, enabling efficient manipulation.","The effectiveness of the proposed SOI-based dynamics model for deformable fabric bag manipulation was evaluated through extensive simulations and real-world experiments. Quantitative and qualitative assessments were conducted for both SOI shape preserving and servoing tasks involving different oval shapes and transitions between them. The results demonstrated the superior performance of the proposed method in preserving the SOI shape and achieving precise manipulation of the fabric bag, outperforming traditional dynamics modeling techniques such as the Mass-Spring Model (MSM) and the Finite Element Model (FEM).","In conclusion, this research paper presents a novel bimanual manipulation framework for deformable objects, centered around an SOI-based latent dynamics model. The integration of multi-view perception, graph neural networks, and model predictive control enables precise and efficient robotic manipulation of flexible materials. The framework's strength lies in identifying and manipulating SOIs, reducing the computational complexity and enhancing the manipulation performance. Future research directions include improving the system's ability to identify and manipulate SOIs without relying on visual aids, expanding the applicability of the method to a wider range of deformable objects and tasks.",Bimanual Deformable Bag Manipulation Using a Structure-of-Interest Based Latent Dynamics Model,"Peng Zhou, Pai Zheng, Jiaming Qi, Chenxi Li, Chenguang Yang, David Navarro-Alarcon, Jia Pan","1
Bimanual Deformable Bag Manipulation Using a
Structure-of-Interest Based Latent Dynamics Model
Peng Zhou1, Member, IEEE, Pai Zheng2, Senior Member, IEEE, Jiaming Qi1, Chenxi Li2, Chenguang Yang3,
Fellow, IEEE, David Navarro-Alarcon2, Senior Member, IEEE, and Jia Pan1, Senior Member, IEEE
Abstract—The manipulation of deformable objects by robotic
systems presents a significant challenge due to their complex and
infinite-dimensional configuration spaces. This paper introduces
a novel approach to Deformable Object Manipulation (DOM) by
emphasizing the identification and manipulation of Structures of
Interest (SOIs) in deformable fabric bags. We propose a bimanual
manipulation framework that leverages a Graph Neural Network
(GNN)-based latent dynamics model to succinctly represent and
predict the behavior of these SOIs. Our approach involves
constructing a graph representation from partial point cloud
data of the object and learning the latent dynamics model
that effectively captures the essential deformations of the fabric
bag within a reduced computational space. By integrating this
latent dynamics model with Model Predictive Control (MPC),
we empower robotic manipulators to perform precise and stable
manipulation tasks focused on the SOIs. We have validated our
framework through various empirical experiments demonstrating
its efficacy in bimanual manipulation of fabric bags. Our contri-
butions not only address the complexities inherent in DOM but
also provide new perspectives and methodologies for enhancing
robotic interactions with deformable objects by concentrating on
their critical structural elements. Experimental videos can be
obtained from https://sites.google.com/view/bagbot
Index Terms—Deformable object manipulation, structure of
interest, latent dynamics model, bimanual manipulation.
I. INTRODUCTION
D
EFORMABLE object manipulation (DOM) [1]–[3] is a
fundamental capability for robots to meaningfully inter-
act with the physical world and assist in various human tasks.
However, the manipulation of deformable objects such as cloth
[4], rope [5], and food ingredients [6] is particularly challeng-
ing due to their infinite-dimensional configuration space and
complex dynamics. Traditional methods in DOM often resort
to simplified physics models or data-driven modeling with
handcrafted features, which lack adaptability for the varied
shapes and dynamics of these objects [7]. Moreover, most
current DOM works focus on manipulating the entire object,
neglecting the critical structures, i.e., Structures of Interest
(SOI), that are essential for subsequent manipulation steps.
In this paper, we introduce the concept of SOI into the
realm of DOM (see Fig. 1 for an example), a paradigm shift
This work is supported by the Innovation and Technology Commission
of the HKSAR Government under the InnoHK initiative. (Corresponding
author: Jia Pan.)
1The authors are with The University of Hong Kong, HK, Hong Kong.
{jeffzhou,tomqi,jpan}@hku.hk
2The
authors
are
with
The
Hong
Kong
Polytechnic
Univer-
sity, KLN, Hong Kong. {pai.zheng,dnavar}@polyu.edu.hk,
chengxi.li@connect.polyu.hk
3The
author
is
with
University
of
Liverpool,
England,
UK.
cyang@ieee.org
Int ℍn
∂ℍn
ℝn
Int ℳ
∂ℳ
Fig. 1.
(Left) SOI Examples for different deformable object manipulation
tasks, e.g., garment hanging, robot-assistive dressing. (Right) Conceptual
representation of the manifold with boundary. The manifold encompasses
Int M and ∂M, where the local neighborhoods of points in Int M and
∂M are homeomorphically equivalent to Int Hn and ∂Hn.
Target SOI
Current SOI
Fig. 2.
The two robots grasp two handles of a fabric bag to manipulate the
SOI (i.e., the opening rim) into the target configuration.
that emphasizes the importance of identifying and manipulat-
ing key structural components rather than the entire object.
This focus on the SOI is motivated by the observation that
successful DOM tasks typically involve the manipulation of
these key areas. By targeting these SOIs, we can reduce the
computational load significantly, as modeling the complete
3D dynamics of the deformable object is unnecessary and
burdensome for the task at hand.
Furthermore, this work is pioneering in considering the
bimanual manipulation of a deformable fabric bag as shown
in Fig. 2. By proposing a novel bimanual manipulation
framework using a GNN-based latent dynamics model, we
address the complexities of DOM with a focus on SOIs. Our
approach is designed to extract the SOI from the observed
object point cloud, construct a graph representation, and learn
the latent dynamics model that effectively captures the object’s
deformations within a compact space. Integrating this model
with model predictive control (MPC), we enable robots to
achieve accurate and stable manipulation of deformable bags.
The main contributions of this work include:
• Pioneering the bimanual manipulation of deformable fab-
arXiv:2401.11432v1  [cs.RO]  21 Jan 2024
2
ric bags by focusing on SOIs.
• Introducing the SOI concept for representing deformable
object states, which emphasizes the manipulation of crit-
ical structures.
• Designing a GNN-based method to learn a latent dy-
namics model from partial point cloud data, particularly
focusing on the SOIs.
• Implementing MPC based on the latent dynamics for
generating optimal manipulation actions centered around
SOIs.
Various empirical experiments on the bimanual manipula-
tion of a fabric bag validate the efficacy of our proposed
framework. Our work offers new insights and methodologies
for improving robotic systems’ capability for intelligent de-
formable object manipulation, with a specialized emphasis on
the pivotal SOIs.
extract
Predict
Input
Extract
Fig. 3.
Bimanual bag manipulation is formulated as a POMDP problem,
where the SOI-related observation ˆot is extracted from the original observation
ot and governed by fθdym.
II. RELATED WORK
Deformable object manipulation has been an active re-
search area in robotics. Existing methods can be catego-
rized into model-based and data-driven approaches. Model-
based methods rely on simplified physics models to represent
deformable objects. Early works used mass-spring models
(MSM) to simulate deformation [8], [9]. The finite element
method (FEM) provides more accurate modeling of contin-
uum mechanics [10]–[12]. However, analytical models require
extensive manual tuning and generalization across different
materials or shapes remains difficult. Data-driven methods [7],
[13], [14] aim to learn models directly from data. Vision-based
methods extract geometric features from visual observations
to infer deformations [15], [16]. Recent works utilized deep
learning on point cloud data and achieved improved modeling
accuracy [17]. However, they depend heavily on large labeled
datasets. Self-supervised methods were proposed to learn from
physical interactions [18], [19]. However, they focused on
planar objects and could not handle complex deformations.
Our work is mostly related to robot manipulation using
graph neural networks (GNNs). GNNs have shown promising
results in learning physics simulations [20]–[24] and control
policies [25]–[28]. Recently, GNNs were introduced to model
rope and cloth manipulation [29]–[31]. Different from these
works, we propose the structure of interest concept to focus
on key object components and employ GNNs to learn latent
dynamics models for describing complex deformable object
behaviors. The integration of the latent dynamics model with
MPC also distinguishes our framework from prior arts. In
summary, our work aims to advance existing deformable object
manipulation by introducing an SOI-based modeling approach.
The adoption of state-of-the-art deep learning techniques
allows better generalizability across different materials and
shapes. The experiments on the bimanual manipulation of
fabric bags represent challenging test cases and validate the
feasibility of our framework.
III. PROBLEM STATEMENT
Given that individual image and depth observations gen-
erally do not fully disclose the state of the environment,
we approach the task of bimanual bag manipulation as a
Partially Observable Markov Decision Process (POMDP) as
depicted in Fig. 3. This is formally defined by the tuple
(S, A, T , O, B, R, γ), where the state at time t, denoted by
st, belongs to the state space S and is not directly observable.
The state encapsulates the configuration of the robots and the
manipulated object. The corresponding observation at time t,
denoted by ot, is within the observation space O. The state
transition model T (st+1 | st, at) describes the probability
of transitioning from the current state st to a new state
st+1 upon taking an action at from the action space A,
which consists of the combined left and right robotic actions,
represented by the Cartesian product A = A1 × A2. The
function B(ot | st, at−1) specifies the likelihood of observing
ot after executing action at−1 and transitioning to state st.
The reward function R(st, at) assigns a valued reward to each
state-action pair, and the discount factor γ ∈ [0, 1) quantifies
the preference for immediate rewards over future rewards.
The goal of this work is to use two 3D-printed robot
grippers to grasp the handles to manipulate the opening rim
to achieve a target state g. We assume this deformable bag
manipulation task is a quasi-static manipulation and dynamic
manipulation motions are not considered. At time step t, the
robots apply action (a1
t, a2
t) ∈ A upon the bag, and we can
partially observe transitions of the bag from ot to ot+1 under
the unknown state transitions from st to st+1. However, a
complete observation of the bag is not necessary, in our task,
the opening rim of the bag is critical for successful manip-
ulation tasks since it not only determines the manipulation
task goals but also provides the most informative sensory
feedback, such as visual landmarks, during manipulation.
We define the Structure of Interest (SOI) in the context of
Deformable Object Manipulation (DOM) refers to specific
regions or features of a deformable object that are critical
for successful manipulation tasks (see Fig. 1 for examples.).
Therefore, in this task, we consider the opening rim of the
manipulated bag as our SOI points, and topologically, we can
define this loop-like structure as a manifold with boundary. As
illustrated in Fig. 1, we also define its interior and boundary as
Int M and ∂M, whose points’ neighborhoods are respectively
3
homeomorphic to Int Hn = {(x1, . . . , xn) | xn > 0} and
∂Hn = {(x1, . . . , xn) | xn = 0}.
With an appropriate perception module, the observation of
the Structure of Interest (SOI), denoted by ˆot, can be extracted
from the overall observation of the bag ot. Our approach is
based on the insight that it is more efficacious to predict the
dynamics of the SOI rather than the entire complex dynamics
of the bag. To this end, we employ a Graph Neural Network
(GNN) to establish a dynamics model fθdym that is dedicated
to learning the transition functions of the SOI, defined as
fθdym : ˆO × A → ˆO. This dynamics model accepts as input
a sequence of SOI observations ˆot−n,...,t ∈ ˆO and actions
(a1
t−n,...,t, a2
t−n,...,t) ∈ A, and predicts the subsequent obser-
vation ˆot+1, where n represents the length of the observation
history before the current time step t. With the dynamics
model, we proceed to cast the bimanual manipulation of the
bag as a task within the Model Predictive Control (MPC)
framework. Within this MPC setup, the cost function J
quantifies the difference between the final SOI feature points
at time step T and the targeted SOI state g. Details on the
precise structure of the cost function J are illustrated in the
following section. This cost is minimized to yield an optimal
sequence of actions across a temporal horizon of T steps:
⟨a0, a1⟩0:T −1 =
arg min
⟨a0,a1⟩0:T −1∈A
J (ˆg, g)
where
ˆg = fθdym
4
1st action
gradient
2nd action
gradient
3rd action
gradient
gradient
gradient
gradient
1st action
2nd action
3rd action
… …
at
̂ot
̂ot+1
Apply
✓
✗
(a) Particle-based SOI representation from raw RGBD data
(b) Graph-based neural SOI dynamics model
(c) Latent SOI dynamics model-based MPC
Fig. 4.
The conceptual representation of the proposed framework for bimanual deformable fabric bag manipulation is based on the latent SOI dynamics
model.
(a)
(b)
(c)
(d)
Fig. 5.
The proposed SOI global particle sampling process. Left: the
reconstructed point cloud for the fabric bag. Right: (a) raw SOI point cloud. (b)
preprocessed SOI point cloud. (c) Reconstructed SOI Surface. (d) Resampled
SOI particles.
The goal of introducing GNNs is to simulate the dynamics
of the SOI and to predict subsequent states from a short
historical sequence of SOI particle graphs, formalized as:
G(ˆot+1) = fθdyn
5
unique and stable optimal bijection ϕ for almost every pair of
particle sets, invariant to infinitesimally small point displace-
ments. In essence, EMD in our framework aligns distributions
while mitigating point cloud anomalies through the definition
of bijective correspondence. Regarding the Chamfer distance,
it’s worth noting that its use here is somewhat liberal, as it
does not fulfill the triangle inequality property. Our composite
loss function integrates these distances in a weighted fashion:
L(P1, P2) = αLCD(P1, P2) + βLEMD(P1, P2). Empirical
evaluations suggest that the optimal weights are α = 0.85
and β = 0.15.
C. Model Predictive Control
Upon training our SOI-centric latent dynamics model, we
integrate a model predictive control (MPC) approach to control
the robotic gripper in manipulating the fabric bag, as depicted
in Fig. 4(c). We simplify the gripper’s action space into a
parameterized form: (x, y, z, rz), with {x, y, z} representing
the end-effector’s position interacting with the bag handles,
and rz indicating the gripper’s rotation around the vertical
z axis. We omit rx and ry rotations based on empirical
evidence suggesting minimal SOI deformation from these
movements. A goal-oriented MPC is employed as below, using
g to symbolize the desired SOI shape and ⟨a0, a1⟩0:T −1 to
represent the sequence of action pairs derived via MPC, with
T as the planning horizon.
min
⟨a0, a1⟩0:T −1J (ˆoT , g) =
min
⟨a0, a1⟩0:T −1 L

ˆPT , Pg

s.t. G(ˆot+1) = fθdym
6
distance (EMD), along with a hybrid metric that combines
both. Furthermore, we employ Geodesic distance (GD) [20]
to assess the proximity of boundary points residing on one
manifold with boundary to those on another.
TABLE II
THE PERFORMANCE OF THE SOI PARTICLE DYNAMICS MODEL WITH
DIFFERENT LOSS FUNCTION.
Loss Functions
CD(cm) ↓
EMD(cm) ↓
GD(cm) ↓
CD
1.52 ± 0.19
1.94 ± 0.18
2.53 ± 0.25
EMD
1.85 ± 0.17
1.37 ± 0.15
2.76 ± 0.13
0.2 CD + 0.8 EMD
1.59 ± 0.16
1.40 ± 0.17
2.13 ± 0.18
0.1 CD + 0.9 EMD
1.55 ± 0.17
1.43 ± 0.16
2.28 ± 0.19
0.15 CD + 0.85 EMD
1.50 ± 0.16
1.38 ± 0.17
2.02 ± 0.16
B. SOI Particle Sampling
We commence by benchmarking the proposed Global Par-
ticle Sampling (GPS) technique against the Local Particle
Sampling (LPS) baseline. LPS initiates by processing the
SOI-specific partial point cloud of the fabric bag through
its unique color signature. Subsequently, it encapsulates the
incomplete SOI-specific cloud with a convex hull. Following
this, it performs the point sampling procedure, eventually
amalgamating the sampled points into a full set of SOI-specific
particles. As shown in Table I, we compute the mean distance
between the sampled particles and the ground-truth particles,
the latter captured by a professional 3D scanner. Our analysis
indicates that the GPS method incurs lower losses in terms of
all distance metrics, thus outperforming the LPS approach.
These outcomes align with the premise that incorporating
additional topological information about the SOI can markedly
enhance the quality of sampling, particularly in scenarios
where occlusions are present.
C. GNN-based SOI Dynamics Model
The training of our GNN model, detailed in Section IV-B,
commences with the construction of a graph, where edges are
formed between vertices that are within a distance threshold
of d = 0.04. Every vertex and edge are encoded using 3-layer
Multilayer Perceptrons (MLPs), featuring hidden and output
layers, each with 300 neurons. The propagation module is
constituted by a fully connected neural layer with a layer size
of 300. For motion prediction, we employ an additional 3-
layer MLP with the hidden layer configured to 300 neurons.
ReLU activation functions are utilized throughout the neural
networks to introduce non-linearity. The model undergoes
training for 120 epochs, employing the Adam optimizer.
We have selected a batch size of 32 to balance the trade-
off between generalization and computational efficiency. The
learning rate is set at 5e-4, which is a conventional choice for
steady convergence. These hyperparameters were chosen to
foster a robust learning process while maintaining the capacity
to capture complex patterns within the data.
Subsequently, we evaluate the performance of GNN-based
SOI dynamics models using different loss functions. Table 6
indicates that only using CD or EMD individually optimizes
for its own metric but not others, but combining CD and
EMD losses leads to better overall performance on all metrics,
and the Geodesic distance is significantly improved, compared
to using them individually. The combination of 0.85 CD +
0.15 EMD achieves the best performance across all metrics,
giving the lowest CD, competitive EMD, and lowest GD, so
we select this hybrid distance for the following experiments.
The reason is that CD and EMD losses capture different
aspects of point cloud similarity. CD compares point-wise
distances while EMD measures global shape differences. By
combining them, the model is optimized for both local point
accuracy and global shape matching. The proper combination
of 0.85 CD + 0.15 EMD balances these objectives best.
TABLE III
MEAN HYBRID DISTANCE ERROR FOR SOI SHAPE PRESERVING BY
DIFFERENT DYNAMICS MODELING METHODS
Method
Long Oval (LO)
Round Oval (RO)
Short Oval (SO)
MSM
2.63 ± 0.31
2.82 ± 0.38
2.94 ± 0.41
FEM
1.96 ± 0.27
2.13 ± 0.36
2.42 ± 0.38
Ours
1.69 ± 0.18
1.84 ± 0.22
2.08 ± 0.21
D. Manipulation Results
To evaluate the performance of the proposed SOI-based
dynamics model for deformable fabric bag manipulation,
we conducted SOI preserving and servoing experiments on
a fabric bag with different oval shapes and the transitions
between them. In SOI preserving experiments, we mainly
compare the performance of our proposed approach with
two commonly-used and well-established dynamics modeling
techniques: the Mass-Spring Model (MSM) [35] and the
Finite Element Model (FEM) [36]. The comparative analysis
encompasses both quantitative and qualitative outcomes, as
encapsulated in Figure 7(a) and Table III. Our findings reveal
a discernible trend across all examined methods; there is an
incremental rise in error corresponding to the transformation
of the bag’s shape from a Long Oval (LO) to a Short Oval
(SO), suggesting that as the bag opening rim becomes shorter
and wider, the complexity of preserving its structure increases.
Notably, our GNN-based dynamics model consistently outper-
forms the MSM and FEM across the entire spectrum of object
shapes, achieving the lowest error rates in tasks dedicated to
shape preservation. This outcome corroborates the superior
expressive capability of our GNN-based model in capturing the
intricate deformations of the fabric bags, and maintaining the
SOI shape unchanged during the fabric bag moving, surpassing
the traditional MSM and FEM approaches, particularly when
dealing with objects that exhibit more complex dynamic
behaviors. The advantage of our model is most pronounced
when interacting with simpler geometric shapes.
In our SOI servoing experiments, we conduct a compre-
hensive analysis of the manipulation results associated with
transitions between distinct shape categories of the structure of
interest (SOI). Additionally, we juxtapose our dynamics model
with two sophisticated shape servoing techniques tailored
for deformable objects–visual servoing (VS) as described by
Lagneau et al. [33], and the latent shape control (LSC) model
7
Long Oval
Round Oval
Short Oval
LO → SO
SO → RO
LO → RO
(a)
(b)
Initial SOI
Target SOI
Ground Truth
Achieved Target
Fig. 7.
Qualitative results in (a) SOI shape preserving and (b) SOI shape servoing experiments.
TABLE IV
MEAN HYBRID DISTANCE ERROR AND SUCCESS RATE FOR SOI SHAPE SERVOING BY DIFFERENT METHODS
Method
Mean Hybrid Distance Error
Success Rate
LO → SO
SO → RO
LO → RO
LO → SO
SO → RO
LO → RO
Total
VS [33]
3.75 ± 0.19
3.69 ± 0.22
3.92 ± 0.23
21/30
19/30
22/30
68.89%
LSC [34]
2.94 ± 0.48
3.14 ± 0.42
2.89 ± 0.44
24/30
24/30
26/30
82.23%
MSM [35]
3.22 ± 0.43
3.49 ± 0.47
3.28 ± 0.39
19/30
21/30
23/30
70.00%
FEM [36]
2.61 ± 0.36
2.82 ± 0.39
2.67 ± 0.33
23/30
22/30
26/30
78.89%
Ours
2.25 ± 0.23
2.39 ± 0.28
2.13 ± 0.24
28/30
30/30
29/30
96.67%
introduced by Qi et al. [34]. The analysis integrates both
quantitative and qualitative assessments, as depicted in Fig.
7(b) and detailed in Table IV. We consider three specific shape
servoing tasks: LO (Long Oval) → SO (Short Oval), SO →
RO (Round Oval), and LO → RO. The evaluation metrics
include the Mean Hybrid Distance Error (recorded solely
for successful trials) and the Success Rate of SOI servoing.
Furthermore, we report the success rate for achieving complete
servoing with a threshold error of less than 5 cm, based
on 30 trials for each method and shape transition scenario.
The experimental outcomes reveal that our proposed method
consistently secured the least shape error, ranging from 2.13
to 2.39 cm across all tested cases, and achieved an outstand-
ing overall success rate of 96.67%. When contrasted with
traditional model-based techniques such as the Mass-Spring
Model (MSM) and the Finite Element Model (FEM), our data-
driven SOI dynamics model exhibited a markedly enhanced
capability in manipulating the SOI for the fabric bag. It was
observed that transformations involving shorter and wider
shapes incurred higher errors due to their inherently complex
dynamics. Nevertheless, our proposed method demonstrated a
robust performance across all tasks. In conclusion, the biman-
ual manipulation experiments underscore the effectiveness of
the proposed SOI-based dynamics modeling approach, which
ensures precise and dependable control over the shapes of
deformable objects through advanced predictive modeling and
optimization techniques.
VI. CONCLUSION
In this work, we introduced a novel bimanual manipu-
lation framework for deformable bags, centered around a
Structure of Interest (SOI)-based latent dynamics model. Our
approach effectively integrates multi-view perception, graph
neural networks, and model predictive control, facilitating
precise and efficient robotic manipulation of flexible materials.
Through simulations and real-world experiments, the frame-
work demonstrated promising results in intelligent physical
interaction with deformable objects. One limitation of the
current system is its reliance on differently colored SOIs
to distinguish target manipulation areas, which may not be
feasible in all operational settings. Future efforts will aim to
enhance the system’s ability to identify and manipulate SOIs
without such visual aids, broadening the applicability of our
method to a wider array of real-world scenarios.
8
REFERENCES
[1] H. Yin, A. Varava, and D. Kragic, “Modeling, learning, perception, and
control methods for deformable object manipulation,” Sci. Robot., vol. 6,
no. 54, 2021.
[2] J. Zhu, A. Cherubini, C. Dune, D. Navarro-Alarcon, F. Alambeigi,
D. Berenson, F. Ficuciello, K. Harada, J. Kober, X. Li et al., “Challenges
and outlook in robotic manipulation of deformable objects,” IEEE
Robotics & Automation Magazine, vol. 29, no. 3, pp. 67–77, 2022.
[3] Z. Hu, T. Han, P. Sun, J. Pan, and D. Manocha, “3-d deformable
object manipulation using deep neural networks,” IEEE Robotics and
Automation Letters, vol. 4, no. 4, pp. 4255–4261, 2019.
[4] I. Garcia-Camacho, J. Borr`as, B. Calli, A. Norton, and G. Aleny`a,
“Household cloth object set: Fostering benchmarking in deformable
object manipulation,” IEEE Robotics and Automation Letters, vol. 7,
no. 3, pp. 5866–5873, 2022.
[5] S. Huo, A. Duan, C. Li, P. Zhou, W. Ma, H. Wang, and D. Navarro-
Alarcon, “Keypoint-based planar bimanual shaping of deformable linear
objects under environmental constraints with hierarchical action frame-
work,” IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 5222–
5229, 2022.
[6] X. Lin, Z. Huang, Y. Li, J. B. Tenenbaum, D. Held, and C. Gan,
“Diffskill: Skill abstraction from differentiable physics for deformable
object manipulations with tools,” in International Conference on Learn-
ing Representations (ICLR), 2022.
[7] P. Zhou, J. Zhu, S. Huo, and D. Navarro-Alarcon, “LaSeSOM: A latent
and semantic representation framework for soft object manipulation,”
IEEE Robot. Autom. Lett., vol. 6, no. 3, pp. 5381–5388, 2021.
[8] X. Provot et al., “Deformation constraints in a mass-spring model
to describe rigid cloth behaviour,” in Graphics interface.
Canadian
Information Processing Society, 1995, pp. 147–147.
[9] K. Tabata, H. Seki, T. Tsuji, and T. Hiramitsu, “Mass spring model for
non-uniformed deformable linear object toward dexterous manipulation,”
Artificial Life and Robotics, vol. 28, no. 4, pp. 812–822, 2023.
[10] V. E. Arriola-Rios, P. Guler, F. Ficuciello, D. Kragic, B. Siciliano, and
J. L. Wyatt, “Modeling of deformable objects for robotic manipulation:
A tutorial and review,” Frontiers in Robotics and AI, vol. 7, p. 82, 2020.
[11] F. Ficuciello, A. Migliozzi, E. Coevoet, A. Petit, and C. Duriez, “Fem-
based deformation control for dexterous manipulation of 3d soft objects,”
in 2018 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS).
IEEE, 2018, pp. 4007–4013.
[12] J. Sanchez, K. Mohy El Dine, J. A. Corrales, B.-C. Bouzgarrou, and
Y. Mezouar, “Blind manipulation of deformable objects based on force
sensing and finite element modeling,” Frontiers in Robotics and AI,
vol. 7, p. 73, 2020.
[13] Z. Xu, C. Chi, B. Burchfiel, E. Cousineau, S. Feng, and S. Song,
“Dextairity: Deformable manipulation can be a breeze,” arXiv preprint
arXiv:2203.01197, 2022.
[14] P. Zhou, P. Zheng, J. Qi, C. Li, H.-Y. Lee, A. Duan, L. Lu, Z. Li, L. Hu,
and D. Navarro-Alarcon, “Reactive human–robot collaborative manipu-
lation of deformable linear objects using a new topological latent control
model,” Robotics and Computer-Integrated Manufacturing, vol. 88, p.
102727, 2024.
[15] D. Navarro-Alarcon, H. M. Yip, Z. Wang, Y.-H. Liu, F. Zhong, T. Zhang,
and P. Li, “Automatic 3-d manipulation of soft objects by robotic arms
with an adaptive deformation model,” IEEE Transactions on Robotics,
vol. 32, no. 2, pp. 429–441, 2016.
[16] J. Qi, G. Ma, J. Zhu, P. Zhou, Y. Lyu, H. Zhang, and D. Navarro-Alarcon,
“Contour moments based manipulation of composite rigid-deformable
objects with finite time model estimation and shape/position control,”
IEEE/ASME Transactions on Mechatronics, vol. 27, no. 5, pp. 2985–
2996, 2021.
[17] X. Lin, C. Qi, Y. Zhang, Z. Huang, K. Fragkiadaki, Y. Li, C. Gan, and
D. Held, “Planning with spatial-temporal abstraction from point clouds
for deformable object manipulation,” in Conference on Robot Learning
(CoRL), 2022.
[18] A. Nair, D. Chen, P. Agrawal, P. Isola, P. Abbeel, J. Malik, and S. Levine,
“Combining self-supervised learning and imitation for vision-based rope
manipulation,” in 2017 IEEE international conference on robotics and
automation (ICRA).
IEEE, 2017, pp. 2146–2153.
[19] M. Yan, Y. Zhu, N. Jin, and J. Bohg, “Self-supervised learning of state
estimation for manipulating deformable linear objects,” IEEE robotics
and automation letters, vol. 5, no. 2, pp. 2372–2379, 2020.
[20] P. Reiser, M. Neubert, A. Eberhard, L. Torresi, C. Zhou, C. Shao,
H. Metni, C. van Hoesel, H. Schopmans, T. Sommer et al., “Graph
neural networks for materials science and chemistry,” Communications
Materials, vol. 3, no. 1, p. 93, 2022.
[21] J. Shlomi, P. Battaglia, and J.-R. Vlimant, “Graph neural networks in
particle physics,” Machine Learning: Science and Technology, vol. 2,
no. 2, p. 021001, 2020.
[22] J. Gasteiger, F. Becker, and S. G¨unnemann, “Gemnet: Universal di-
rectional graph neural networks for molecules,” Advances in Neural
Information Processing Systems, vol. 34, pp. 6790–6802, 2021.
[23] P. Zhou, J. Qi, A. Duan, S. Huo, Z. Wu, and D. Navarro-Alarcon,
“Imitating tool-based garment folding from a single visual observation
using hand-object graph dynamics,” IEEE Transactions on Industrial
Informatics, 2024.
[24] T. Wang, R. Liao, J. Ba, and S. Fidler, “Nervenet: Learning structured
policy with graph neural networks,” in International conference on
learning representations, 2018.
[25] E. Tolstaya, F. Gama, J. Paulos, G. Pappas, V. Kumar, and A. Ribeiro,
“Learning decentralized controllers for robot swarms with graph neural
networks,” in Conference on robot learning. PMLR, 2020, pp. 671–682.
[26] Q. Li, F. Gama, A. Ribeiro, and A. Prorok, “Graph neural networks
for decentralized multi-robot path planning,” in 2020 IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems (IROS).
IEEE,
2020, pp. 11 785–11 792.
[27] E. Tolstaya, J. Paulos, V. Kumar, and A. Ribeiro, “Multi-robot coverage
and exploration using spatial graph neural networks,” in 2021 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS).
IEEE, 2021, pp. 8944–8950.
[28] H. Shi, H. Xu, Z. Huang, Y. Li, and J. Wu, “Robocraft: Learning to see,
simulate, and shape elasto-plastic objects with graph networks,” arXiv
preprint arXiv:2205.02909, 2022.
[29] C. Wang, Y. Zhang, X. Zhang, Z. Wu, X. Zhu, S. Jin, T. Tang,
and M. Tomizuka, “Offline-online learning of deformation model for
cable manipulation with graph neural networks,” IEEE Robotics and
Automation Letters, vol. 7, no. 2, pp. 5544–5551, 2022.
[30] Y.
Rubanova,
A.
Sanchez-Gonzalez,
T.
Pfaff,
and
P.
Battaglia,
“Constraint-based
graph
network
simulator,”
arXiv
preprint
arXiv:2112.09161, 2021.
[31] H. Bertiche, M. Madadi, and S. Escalera, “Neural cloth simulation,”
ACM Transactions on Graphics (TOG), vol. 41, no. 6, pp. 1–14, 2022.
[32] R. Fletcher, Practical methods of optimization.
John Wiley & Sons,
2000.
[33] R. Lagneau, A. Krupa, and M. Marchal, “Active deformation through
visual servoing of soft objects,” in 2020 IEEE International Conference
on Robotics and Automation (ICRA).
IEEE, 2020, pp. 8978–8984.
[34] J. Qi, G. Ma, P. Zhou, H. Zhang, Y. Lyu, and D. Navarro-Alarcon, “To-
wards latent space based manipulation of elastic rods using autoencoder
models and robust centerline extractions,” Advanced Robotics, vol. 36,
no. 3, pp. 101–115, 2022.
[35] F. Makiyeh, M. Marchal, F. Chaumette, and A. Krupa, “Indirect posi-
tioning of a 3d point on a soft object using rgb-d visual servoing and a
mass-spring model,” in 2022 17th International Conference on Control,
Automation, Robotics and Vision (ICARCV). IEEE, 2022, pp. 235–242.
[36] Z. Zhang, T. M. Bieze, J. Dequidt, A. Kruszewski, and C. Duriez,
“Visual servoing control of soft robots based on finite element model,”
in 2017 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS).
IEEE, 2017, pp. 2895–2901.
9
Peng Zhou received the M.Sc. degree in soft-
ware engineering from Tongji University, Shanghai,
China, in 2017, and Ph.D. degree in robotics from
The Hong Kong Polytechnic University, Hong Kong
SAR, in 2022. In 2021, he was a visiting Ph.D.
student at Robotics, Perception and Learning Lab,
KTH Royal Institute of Technology, Stockholm,
Sweden. He is currently a Research Officer at the
Centre for Transformative Garment Production and
a Postdoctoral Research Fellow at The University
of Hong Kong. His research interests include de-
formable object manipulation, robot reasoning and learning, and task and
motion planning.
Pai Zheng (Senior Member, IEEE) received the
dual bachelor’s degrees in mechanical engineering
(major) and computer science and engineering (mi-
nor) from the Huazhong University of Science and
Technology, Wuhan, China, in 2010, the master’s
degree in mechanical engineering from Beihang Uni-
versity, Beijing, China, in 2013, and the Ph.D. degree
in mechanical engineering from The University of
Auckland, Auckland, New Zealand, in 2017. He
has been a Research Fellow with the Delta-NTU
Corporate Laboratory for Cyber-Physical Systems,
School of Electrical and Electronic Engineering, Nanyang Technological
University (NTU), Singapore, from January 2018 to September 2019. He is
currently an Assistant Professor with the Department of Industrial and Systems
Engineering, The Hong Kong Polytechnic University. His research interests
include smart product-service systems, human–robot collaboration, and smart
manufacturing systems. He is a member of HKIE, CMES, and ASME. He
serves as an Associate Editor for Journal of Intelligent Manufacturing and
Journal of Cleaner Production, an Editorial Board Member for the Journal
of Manufacturing Systems and Advanced Engineering Informatics, and a
guest editor/reviewer for several high impact international journals in the
manufacturing and industrial engineering field.
Jiaming Qi received the Ph.D degree in control
science and engineering, and the M.S. degree in Inte-
grated Circuit Engineering from the Harbin Institute
of Technology, Harbin, China, in 2023 and 2018,
respectively. He performs research in the deformable
object manipulation, visual servoing, and human-
robot collaboration. He is currently a post-doctoral
fellow in the Centre for Transformative Garment
Production, The University of Hong Kong, Hong
Kong.
Chengxi Li (Graduate Student Member, IEEE) re-
ceived the B.E. degree in Information Technology
from Vaasa University of Applied Sciences, Finland
in 2018, and the M.S. degree in Computer Science
from Uppsala University, Sweden in 2020, respec-
tively. He is currently pursuing the Ph.D. degree with
the Department of Industrial and Systems Engineer-
ing, The Hong Kong Polytechnic University, China.
His research interests include robot learning, mixed
reality, and human–robot collaboration.
Chenguang
Yang
(Fellow, IEEE) received the
B.Eng. degree in measurement and control from
Northwestern
Polytechnical
University,
Xi’an,
China,
in
2005,
the
Ph.D.
degree
in
control
engineering
from
the
National
University
of
Singapore, Singapore, in 2010, and postdoctoral
training
in
human
robotics
from
the
Imperial
College London, London, U.K, from 2009 to
2010.
He
was
awarded
UK
EPSRC
UKRI
Innovation Fellowship and individual EU Marie
Curie
International
Incoming
Fellowship.
As
the lead author, he won the IEEE Transactions on Robotics Best Paper
Award (2012) and IEEE Transactions on Neural Networks and Learning
Systems Outstanding Paper Award (2022). He is the leader of Robot
Teleoperation Group of Bristol Robotics Laboratory, the Corresponding
Co-Chair of the Technical Committee on Collaborative Automation for
Flexible Manufacturing (CAFM), IEEE Robotics and Automation Society.
He is a Fellow of Institution of Mechanical Engineers (IMechE), Institution
of Engineering and Technology (IET), British Computer Society (BCS) and
Higher Education Academy (HEA). He has served as Associate Editor of
a number of leading international journals including IEEE Transactions on
Robotics. He is an elected Member-at-Large of the Board of Governors
with IEEE Systems, Man, and Cybernetics Society (SMC), and an AdCom
member with IEEE Industrial Electronics Society (IES), 2023-2025. His
research interest lies in human robot interaction and intelligent system
design.
David Navarro-Alarcon (Senior Member, IEEE)
received the Ph.D. degree in mechanical and au-
tomation engineering from The Chinese University
of Hong Kong, Hong Kong, in 2014, where he also
worked as a Postdoctoral Fellow and Research As-
sistant Professor. Since 2017, he has been with The
Hong Kong Polytechnic University (PolyU), Hong
Kong, where he is currently an Associate Professor
with the Department of Mechanical Engineering. His
research interests include perceptual robotics and
control systems. He currently serves as an Associate
Editor of the IEEE TRANSACTIONS ON ROBOTICS and Associate Editor of
the IEEE ROBOTICS AND AUTOMATION MAGAZINE.
Jia Pan (Senior Member, IEEE) received the Ph.D.
degree in computer science from the University of
North Carolina at Chapel Hill, Chapel Hill, NC,
USA, in 2013.
He is currently an Associate Professor with the
Department of Computer Science, University of
Hong Kong, Hong Kong. He is also a member of
the Centre for Garment Production Limited, Hong
Kong. His research interests include robotics and
artificial intelligence as applied to autonomous sys-
tems, particularly for navigation and manipulation
in challenging tasks such as effective movement in dense human crowds and
manipulating deformable objects for garment automation.
","nanPrevious research in deformable object manipulation can be categorized into model-based and data-driven approaches. Model-based methods rely on simplified physics models, such as mass-spring models or finite element methods. However, analytical models require extensive manual tuning and generalization across different materials or shapes remains difficult. Data-driven methods aim to learn models directly from data, utilizing computer vision or self-supervised learning. However, they heavily depend on large labeled datasets and may not handle complex deformations well."
"The data imbalance associated with a long-tail distribution, which has numerous minority classes (i.e., entity classes) and only a single majority class (i.e., O-class), poses a significant challenge for various machine learning (ML) tasks, including named entity recognition (NER). The imbalance leads to misclassifications of the entity classes as the O-class. We propose a novel and effective learning method, Majority or Minority (MoM) learning, to tackle this data imbalance. MoM learning incorporates loss computed only for samples whose ground truth is the majority class into the loss of the conventional ML model. Evaluation experiments using four NER datasets (Japanese and English) confirmed that MoM learning improves the prediction performance of the minority classes, without compromising the performance of the majority class. Moreover, it was found to be more effective than widely known and state-of-the-art methods. The effectiveness of MoM learning was also demonstrated using frameworks like sequential labeling and machine reading comprehension, which are commonly used in NER. Furthermore, MoM learning consistently achieved performance improvements regardless of language, model, or framework.","Data imbalance presents a significant challenge in various machine learning (ML) tasks, particularly named entity recognition (NER) within natural language processing (NLP). NER exhibits a data imbalance with a long-tail distribution, featuring numerous minority classes (i.e., entity classes) and a single majority class (i.e., O-class). The imbalance leads to the misclassifications of the entity classes as the O-class. To tackle the imbalance, we propose a simple and effective learning method, named majority or minority (MoM) learning.","In this paper, we propose a novel learning method, majority or minority (MoM) learning, to tackle the data imbalance in NER. MoM learning is simple and effective for incorporating the loss computed only for samples whose ground truth is the majority class into the loss of the conventional ML model. Our strategy allows for cost-sensitive learning, but it differs conceptually from previous studies as it does not depend on the difficulty of classification or the number of samples in the class. Instead, MoM learning focuses on preventing misclassifications of the minority classes (entity classes) as the majority class (the O-class).","Evaluation experiments using four NER datasets (Japanese and English) and various ML models, including BERT and RoBERTa, demonstrated that MoM learning contributes to consistent improvements in performance across models and languages. We also confirmed that MoM learning is more effective than those introduced in previous state-of-the-art studies, such as focal loss (FL) and dice loss (DL). Furthermore, beyond the common sequential labeling, we demonstrated the effectiveness of MoM learning using the machine reading comprehension (MRC) framework, which is becoming the mainstream.","We proposed a novel learning method, named majority or minority (MoM) learning, designed to address the data imbalance with a long-tail distribution, which is a significant challenge in ML tasks. We evaluated four commonly NER datasets (English and Japanese) and demonstrated that MoM learning is more effective than conventional methods of both the sequential labeling framework and MRC. MoM learning improved the performance of the entity classes without compromising the performance of the O-class in language- and model-agnostic settings.",Majority or Minority: Data Imbalance Learning Method for Named Entity Recognition,"Sota Nemoto, Shunsuke Kitada, Hitoshi Iyatomi","Majority or Minority: Data Imbalance
Learning Method for Named Entity Recognition
Sota Nemoto, Shunsuke Kitada, Hitoshi Iyatomi
Department of Applied Informatics, Graduate School of Science and Engineering, Hosei University
Tokyo, Japan
{sota.nemoto.5s@stu., shunsuke.kitada.8y@stu., iyatomi@}hosei.ac.jp
Abstract—Data imbalance presents a significant challenge in
various machine learning (ML) tasks, particularly named entity
recognition (NER) within natural language processing (NLP).
NER exhibits a data imbalance with a long-tail distribution,
featuring numerous minority classes (i.e., entity classes) and a
single majority class (i.e., O-class). The imbalance leads to the
misclassifications of the entity classes as the O-class. To tackle the
imbalance, we propose a simple and effective learning method,
named majority or minority (MoM) learning. MoM learning
incorporates the loss computed only for samples whose ground
truth is the majority class (i.e., the O-class) into the loss of
the conventional ML model. Evaluation experiments on four
NER datasets (Japanese and English) showed that MoM learning
improves prediction performance of the minority classes, without
sacrificing the performance of the majority class and is more
effective than widely known and state-of-the-art methods. We also
evaluated MoM learning using frameworks as sequential labeling
and machine reading comprehension, which are commonly used
in NER. Furthermore, MoM learning has achieved consistent
performance improvements regardless of language, model, or
framework.
Index Terms—natural language processing, named entity
recognition, data imbalance, cost-sensitive learning
I. INTRODUCTION
Named entity recognition (NER) [1], [2] is one of many
real-world natural language processing (NLP) tasks with sig-
nificant data imbalance, especially when applied for business
purposes, such as corporate information-gathering websites [3]
and extracting drug names, diseases, and symptoms from huge
amounts of unstructured medical data [4]. NER commonly
uses a sequential labeling framework, a form of multiclass
classification that predicts labels corresponding to the words
in a sentence. In the sequential labeling, all words are divided
into either entity words with information (i.e., proper nouns)
or non-entity words without information. Each entity word
is labeled as a specific class (PERSON, LOCATION, etc...),
to which a few samples belong. In contrast, all non-entity
words constitute the majority and are labeled as a single
class (i.e., the “others” O-class). This labeling yields a data
imbalance with a long-tail distribution. Between the well-
known benchmarks, CoNLL2003 [5] and OntoNotes5.0 [6],
the number of samples for the O-class significantly exceeds
that of the entity class, a condition often leading to misclassifi-
cations of entity classes as the O-class, causing a considerable
decline in the prediction performance of the minority classes.
Thus, overcoming this data imbalance is a crucial step toward
enhancing the performance of NER.
Conventional machine learning (ML) methods for address-
ing the data imbalances are categorized into sampling-based
methods [7]–[9] for inputs and cost-sensitive learning [10]–
[12] for outputs. The sampling-based method, which adjusts
the number of sentences in training, has a certain effect on the
ML tasks. However, NER uses a sequential labeling that pre-
dicts the labels corresponding to each word in a sentence; thus,
it does not mitigate the imbalance. Conversely, cost-sensitive
learning addresses the imbalance by designing a loss function
for the ML model based on the number of samples in each
class. While it is effective for binary classification, NER is
a multiclass classification requiring extension of this method.
This extension will lead to complex weight adjustments for
each class and for cases in which it is not fully capable, thus
not falling short of the desired performance.
In this paper, we propose a novel learning method, majority
or minority (MoM) learning, to tackle the data imbalance in
NER. MoM learning is simple and effective for incorporating
the loss computed only for samples whose ground truth is the
majority class into the loss of the conventional ML model.
Our strategy enables one of cost-sensitive learning, but it
differs from the concepts of previous studies, as it does not
depend on the difficulty of the classification or the number
of samples in the class. The purpose of MoM learning is
to enhance the performance by preventing misclassifications
of the minority classes (entity classes) as the majority class
(the O-class). When incorporating the loss of entity classes
instead of the O-class, the model cannot distinguish whether
the prediction misclassified as the O-class or as another entity
class. Therefore, MoM learning focuses on the O-class to
recognize misclassifications from O-class to the entity classes.
We evaluated MoM learning using four NER datasets [5],
[6], [13], [14] and with a variety of ML models, including
BERT [15] and RoBERTa [16] which have proven successful
in various NLP tasks. The evaluation results demonstrated that
MoM learning contributes to consistent improvements in the
performance across models and languages. We also confirmed
that MoM learning is more effective than those introduced in
previous state-of-the-art studies, such as focal loss (FL) [17]
and dice loss (DL) [18]. Furthermore, beyond the common
sequential labeling, we demonstrated the effectiveness of MoM
learning using the machine reading comprehension (MRC)
arXiv:2401.11431v1  [cs.CL]  21 Jan 2024
framework, which is becoming the mainstream [19], [20].
Our contributions are summarized as follows: (1) We pro-
pose a novel learning method, named majority or minority
(MoM) learning, designed to address the data imbalance with
a long-tail distribution, which is a significant challenge in ML
tasks. (2) We evaluated four commonly NER datasets (English
and Japanese) and demonstrated that MoM learning is more
effective than conventional methods of both the sequential
labeling framework and MRC. (3) MoM learning improved the
performance of the entity classes without compromising the
performance of the O-class in language- and model-agnostic.
II. RELATED WORK
This section describes two primary methods for tackling the
data imbalance in ML tasks: the sampling-based method and
cost-sensitive learning.
A. Sampling-based Method
Common sampling methods, such as random oversampling
(ROS) and random undersampling (RUS), are often used as
a first choice. ROS duplicates randomly selected minority
samples, whereas RUS utilizes only selected samples for
training. These methods enhance the impact of the minority
samples and fix the imbalance before training. Potential con-
cerns with these methods are that ROS may lead to overfitting
and increase the training time, whereas RUS may discard
potentially effective training samples.
To overcome the limitations of these methods, various
derived methods have been proposed [21], [22]. However, be-
cause NER involves sequential labeling, adjusting the number
of sentences in training through these methods, it does not
improve the word-related imbalance. Because these methods
are unsuitable, especially in NER, where the imbalance are
more severe than in other NLP tasks, we constructed a new
method based on cost-sensitive learning.
B. Cost-Sensitive Learning
Weighted cross-entropy (WCE) is commonly employed to
address the data imbalance by assigning class-specific weights
to the cross-entropy loss based on the number of samples
in each class [10]–[12]. This method typically increases the
weights of classes with few samples and decreases the weights
of classes with many samples. Because the distribution based
on samples using training and test only sometimes matches,
WCE is unlikely to achieve the desired performance.
To overcome the limitations in WCE, FL [17] and DL [18]
have demonstrated superior effectiveness in binary classifica-
tion compared to WCE [23], and these methods have been
proven effective in NLP tasks. These methods have been con-
firmed applicable to multiclass classification models, including
extensions such as one-versus-the-rest and one-versus-one, as
seen in conventional ML models (e.g., support vector machines
[SVMs] and logistic regression). For instance, FL is a loss
function to address the imbalance between the background
and the target of an image in an object detection task (i.e.,
binary classification) of computer vision. FL adjusts weights
by applying a power of (1 − p) to the predicted probability
p for each class. Conversely, DL defines loss based on the
dice coefficient [24] (synonymous with the F1 score), which
is the harmonic mean of precision and recall. To maximize
this dice coefficient, the number 1 minus it is defined as the
loss function. These method differ from WCE, as they allow
weights to be set based on the difficulty in classification rather
than the number of samples per class. As a result, the ML
model minimizes the loss enabling accurate classifications, and
increases the loss for misclassifications.
Our method is based on the cost-sensitive learning, and
it addresses the data imbalances by considering losses sep-
arately for groups in which the most imbalances occur (i.e.,
majority and minority groups). When focusing on the groups,
MoM learning significantly differs from the concept of adding
weights based on the difficulty in classifying such as FL and
DL. In addition, MoM learning does not require weights be
set for each class, as in WCE.
III. PROPOSED METHOD
This section describes the proposed method, MoM learning,
which was MoM learning is designed for NER to improve the
performance of ML models for data imbalance with a long-tail
distribution, characterized by the many minority classes and
one majority class (i.e., entity classes and O-class).
A. Notation
First,
as
a
common
approach
to
NER,
we
intro-
duce the notation for sequential labeling. We consider
a dataset comprising a set of input sentences X
=
[x(1), · · · , x(n), · · · , x(N)] and the corresponding training
labels Y
=
[y(1), · · · , y(n), · · · , y(N)], where N is the
number of sentences in the dataset. The n-th sentence
split tokens and corresponding training labels are repre-
sented as x(n) = [w(n)
1 , · · · , w(n)
i
, · · · , w(n)
M ] and y(n) =
[y(n)
1
, · · · , y(n)
i
, · · · , y(n)
M ], respectively. M is the number of
tokens in the longest sentence in the dataset, and shorter
sentences are padded up to M.
The training label y(n)
i
is annotated using the BIO for-
mat [25] in sequential labeling. This format consists of entity
classes (e.g., PER, LOC, and ORG) and a non-entity class (i.e.,
the O-class), where the former is represented by prefixing the
entity category with B for the first token and I for the rest, as
follows: B-PER, I-PER, B-LOC, etc. The sequence of pre-
dicted labels is denoted as p(n) = [p(n)
1 , · · · , p(n)
i
, · · · , p(n)
M ],
where the ML model estimates the predicted probabilities p(n)
for each token of the sentence x(n).
B. Majority or Minority (MoM) Learning
MoM learning is a simple and effective method that incor-
porates the loss for samples whose ground truth is a single
majority class into the loss of an arbitrary conventional ML
model. Fig. 1 illustrates the concept of MoM learning, where
conventional loss L represents an arbitrary loss function of the
model, such as cross-entropy, which computes the loss for all
samples boxed in red. The LMoM term only computes the loss
T (GT)
B-ORG
I-ORG
O
O
O
B-LOC
P (Pred)
B-ORG
I-ORG
I-ORG
O
O
O
T (GT)
B-ORG
I-ORG
O
O
O
B-LOC
P (Pred)
B-ORG
I-ORG
I-ORG
O
O
O
TEXT
The
WBC
took
place
in
Florida
T (GT)
B-ORG
I-ORG
O
O
O
B-Loc
Proposed
Fig. 1.
Concept of the MoM learning. The conventional loss function,
L(y(n), p(n)) (e.g., cross-entropy loss), calculates the loss for all samples. In
the MoM learning, LMoM(y(n), p(n)), the loss associated with the “major”
O-class is added to L(y(n), p(n)).
whose ground truth is the O-class (i.e., y(n)
i
= “O”) framed
in red, and incorporates them into the conventional loss. The
equation for the LMoM is presented as follows:
LMoM(y(n), p(n)) = − 1
M
M
X
y(n)
i
=“O”
ℓ(y(n)
i
, p(n)
i
),
(1)
where ℓ is an arbitrary loss function, including cross-entropy,
weighted cross-entropy, FL [17], DL [18], etc.
Because LMoM focuses only on the O-class, certain entity
classes misclassified by the model become inconsequential.
Hence, LMoM functions as a pseudo-binary classification,
distinguishing between the O-class and the entity classes
to detect the misclassifications of O classes as entity class.
MoM learning enables independence from such factors as the
number of class samples, task features, and the model, making
it adaptable to similarly imbalanced tasks.
For the n-th sentence x(n), the loss function Lsentence, when
applying MoM learning, is written as follows:
Lsentence(y(n), p(n)) =λ · L(y(n), p(n))
+ (1 − λ) · LMoM(y(n), p(n)),
(2)
where λ is a hyperparameter that controls the trade-off be-
tween L and LMoM. MoM learning simplifies weights ad-
justments compared to WCE, with a single hyperparameter
λ. Finally, the model loss Lmodel is minimized with the
training labels for the entire dataset as Y and the prediction
probabilities as P :
Lmodel(Y , P ) = 1
N
N
X
n=1
Lsentence(y(n), p(n)).
(3)
MoM learning can also be considered a type of multitask
learning [26] that improves the performance by learning sev-
eral similar tasks simultaneously and has achieved good results
against the data imbalance in various fields [27]–[29].
IV. EXPERIMENTS
This section describes the evaluation experiments used,
including sequential labeling and MRC, followed by the
datasets, loss functions, and the implementation details. Con-
sidering the data variability, the evaluation was based on the
average of the results of 10 random seeds in each condition.
In all evaluations, we performed paired t-tests (α = 0.05)
to identify differences between our method and other best
methods where α is the significance level.
A. NER Frameworks
a) Sequential labeling framework: The sequential label-
ing classifies at the token level and yield the data imbalance
with a long-tail distribution. This framework directly addresses
NER as a multiclass classification. Thus, we used the macro
F1 score as an evaluation criterion.
b) MRC framework: Compared to the sequential label-
ing, the MRC, another practical framework for NER, has been
widely used in recent years [19] in binary classification tasks.
This framework determines whether each word belongs to a
particular class and to find its range. Specifically, for a token
w(n)
i
in a sentence x(n), the ground truth can be written
as y(n)
i
∈ {0, 1}Y, where Y is the set of entity and non-
entity classes. Different from the macro F1 score for sequential
labeling, we used the macro F1 score, which matches the index
of the predicted start and end points.
B. Datasets
We used four datasets, as shown in Table I: English
CoNLL2003 [5], English OntoNotes5.0 [6], Kyoto University
web document read corpus (KWDLC; in Japanese) [13],
and Stockmark NER wiki (NER wiki; in Japanese) [14]. In
addition, we evaluated with four datasets using sequential
labeling. For MRC, we used the CoNLL2003, which has
been adopted in previous studies, by converting the data from
sequential labeling annotations. For the English datasets, we
employed the standard training, validation, and test data pro-
vided, and for the Japanese datasets without the standard ones
we randomly split the data 8:1:1. # O-sample and # entities
sample are presented the number of O-class and entity classes,
respectively. The imbalance rate ρO is the ratio of the number
of O-class samples to the total number of samples.
C. Loss Functions
We compared the prediction performance of MoM learning
with that of conventional learning methods (i.e., loss functions)
that have long been considered state-of-the-art and used widely
for the data imbalance.
a) WCE: The WCE is one of the most generally used
weighted loss functions, and we consider the following vari-
ants: (1) the inverse class frequency (WCE-1), and (2) a
hyperparameter related to the number of samples (WCE-2).
The WCE loss, ℓWCE for any class k of any one sentence
x(n) in the NER is defined as follows:
ℓWCE(y(n)
i
, p(n)
i
) = −
Y
X
k
ωky(n)
ik log p(n)
ik .
(4)
As ωk in our experiments, WCE-1 is set as the inverse class
frequency and WCE-2 is set as log10( s−sk
sk +β), used in a DL
paper [18], where sk is the number of samples for class k, s is
the total number of train samples and β is a hyperparameter.
TABLE I
STATISTICS FOR FOUR DATASET (ENGLISH AND JAPANESE). ρO INDICATES THE PROPORTION OF O-CLASSES TO THE TOTAL. BY DOMINANT O-CLASS,
ρO EXCEEDS 80% IN EACH DATASET.
Lang.
# train
# val
# test
# class
# O-sample
# entities sample
ρO
CoNLL2003 [5]
En
14,041
3,250
3,453
9
248,818
53,993
0.8217
OntoNotes5.0 [6]
En
75,187
9,603
9,479
37
1,441,685
190,310
0.8834
KWDLC [13]
Ja
12,836
1,602
1,613
17
236,290
16,694
0.9340
NER Wiki [14]
Ja
4,274
535
534
17
80,944
17,552
0.8218
TABLE II
SUMMARY OF THE PRE-TRAINED MODELS FOR EACH HYPER-PARAMETER, WHERE λ
IS MOM LEARNING FOR EACH DATASET.
English
Model
Pre-train model
CoNLL2003
OntoNotes5.0
BERT
BERT-base-cased
0.175
0.125
RoBERTa
xlm-roberta-base
0.209
0.041
BERT-MRC
BERT-base-cased
0.446
Japanese
Model
Pre-train model
KWDLC
Wiki
BERT
Tohoku University BERT-base
0.357
0.212
RoBERTa
Waseda University RoBERTa-base
0.291
0.248
We used Tree-Structured Parzen Estimator (TPE) [30] implemented in a Bayesian
optimization library Optuna [31] according to the F1 score of the validation data. The
part of the experiment not performed in this study is left blank.
b) FL: The FL [17] is a more robust and versatile
loss [32], [33] proposed later than WCE:
ℓFL(y(n)
i
, p(n)
i
) = −y(n)
i
(1 − p(n)
i
)γ log p(n)
i
,
(5)
where γ is a hyperparameter to reduce the relative loss for
well-classified samples. Because FL was designed for binary
classification, we extended the FL with a one-versus-the-rest
method in the sequential labeling.
c) DL: The DL [18] was designed to reduce both false
positives and false negatives and has long been considered a
state-of-the-art method focused on MRC:
ℓDSC(y(n)
i
, p(n)
i
) = 2(1 − p(n)
ik )ϵp(n)
ik · y(n)
ik + δ
(1 − p(n)
ik )ϵ + y(n)
ik + δ
,
(6)
where ϵ and δ are hyperparameters of reducing the relative
loss for well-classified samples and smoothing, respectively.
D. Implementation Details
a) Models: We utilized pre-trained models, as shown
in Table II, where the input length of these models was
determined by the maximum number of tokens in a sentence
(M = 128) with padding tokens ([PAD]) used for filling the
remaining space to maintain a consistent length. We fine-tuned
in 10 epochs using the Adam optimizer [34] for each task.
During sequential labeling, we used pre-trained BERT [15]
and RoBERTa [16] as baseline models. In the latter part of the
D-dimensional special classification ([CLS]) token of these
models, we attached a new head of D ×(M ×Y) two-layered
fully connected neural networks, and the head was trained
to minimize the cross-entropy loss for each token. As such,
these models output the predicted probabilities of each class
corresponding to the words in the text. We set D to 768, a
learning rate of 2 × 10−5 and a batch size of 64, respectively.
In MRC, we used BERT-MRC [19] as the baseline model,
where the latter part of the [CLS] token, we used the same
architecture as sequential labeling with changed Y to 2. As
such, this model outputs a range corresponding to an entity
for a sentence and Y binary classification. We set a learning
rate of 3 × 10−5 and a batch size of 32, respectively.
b) Hyperparameters: Tree-Structured Parzen Estimator
(TPE) [30], implemented in the Bayesian optimization library
Optuna [31], to maximize the F1 score of the validation data.
For the sequential labeling experiments, the hyperparameters
of WCE-2 (β) and FL (γ) were explored in the pre-determined
range of 1.0-10.0 and 0.0-10.0, respectively, considering their
papers [17], [18]. For the MRC experiments, we set the
hyperparameters of FL γ = 3.0 and those of DL ϵ = 1.0
and δ = 0.01, which is based on the DL hyperparameters
carefully tuned in
[18]. The hyperparameters of MoM (λ)
were explored in the pre-determined range of 0.0-1.0 in
both frameworks. We show the results of tuning the MoM
hyperparameter λ in Table II.
V. RESULTS
Tables III and IV present a comparison of the performances
of each method using BERT and RoBERTa in sequential
labeling. We confirmed that MoM learning consistently outper-
forms other tested methods across all four datasets, regardless
of these models. In all results (4 datasets × 2 models), the
performance using MoM learning was significant at α = 0.05
against the other best method (FL). In the OntoNotes5.0 results
with RoBERTa (marked as †), MoM learning was slightly less
significant, but it was significant compared to the baseline CE.
The results using WCE-1 and WCE-2 demonstrated a poor
performance compared to the baseline, and the reason for this
are discussed in Sec. VI.
Table V presents a comparison between the baseline
with/without MoM learning for each entity in the CoNLL2003.
The prefixes of the entity classes B and I are merged to show
the average performance of the respective classes, resulting in
nine classes (e.g., B-PER, I-PER, B-LOC and O) becoming
five classes (e.g., PER, LOC, and O). We confirm that the MoM
learning improves the performance of entity classes without
compromising the performance of the O-class.
Table VI presents the performance with the CoNLL2003
dataset using the MRC. The results confirm MoM learning
TABLE III
THE PERFORMANCE WITH BERT IN THE SEQUENTIAL LABELING TASK (IN MACRO F1). IN ALL ITEMS, THE PROPOSED MOM HAD THE BEST SCORE,
WITH A SIGNIFICANT DIFFERENCE IN FL, WHICH WAS THE NEXT BEST (α = 0.05).
CoNLL2003
OntoNotes5.0
KWDLC
Stockmark NER Wiki
Prec.
Rec.
F1
Prec.
Rec.
F1
Prec.
Rec.
F1
Prec.
Rec.
F1
BERT
90.16
91.86
91.00
87.41
89.07
88.23
70.92
73.96
72.41
77.32
81.04
79.13
w/ WCE-1
89.73
92.15
90.93
85.66
90.28
87.91
62.79
78.32
69.70
73.65
80.28
76.82
(-0.07)
(-0.32)
(-2.71)
(-2.31)
w/ WCE-2
89.94
92.22
91.07
86.81
89.67
88.22
68.86
77.23
72.80
75.72
81.19
78.36
(+0.07)
(-0.01)
(+0.39)
(-0.77)
w/ FL
90.33
92.03
91.17
87.62
89.15
88.39
71.88
74.27
73.05
77.79
81.53
79.61
(+0.17)
(+0.16)
(+0.64)
(+0.48)
w/ MoM
90.41
92.27
91.33
87.39
89.84
88.60
72.54
74.13
73.32
78.13
81.61
79.83
(proposed)
(+0.33)
(+0.37)
(+0.91)
(+0.70)
TABLE IV
THE PERFORMANCE WITH ROBERTA IN THE SEQUENTIAL LABELING TASK (IN MACRO F1) . IN ALL ITEMS, THE PROPOSED MOM HAD THE BEST
SCORE, WITH A SIGNIFICANT DIFFERENCE IN FL, WHICH WAS THE NEXT BEST (α = 0.05) EXCEPT FOR ONTONOTE †
CoNLL2003
OntoNotes5.0
KWDLC
Stockmark NER Wiki
Prec.
Rec.
F1
Prec.
Rec.
F1
Prec.
Rec.
F1
Prec.
Rec.
F1
RoBERTa
89.93
91.56
90.74
88.24
90.00
89.11
77.11
81.66
79.31
81.07
84.45
82.72
w/ WCE-1
85.86
91.31
88.50
59.55
82.59
69.20
46.85
77.38
58.31
39.51
49.58
43.98
(-2.24)
(-19.91)
(-21.00)
(-38.74)
w/ WCE-2
89.88
91.94
90.90
86.61
90.49
88.51
68.86
77.23
72.80
75.72
81.19
78.36
(+0.16)
(-0.60)
(-6.51)
(-4.36)
w/ FL
90.97
91.11
91.04
88.35
90.09
89.22
80.12
82.44
81.28
81.41
85.07
83.24
(+0.30)
(+0.11)
(+1.93)
(+0.52)
w/ MoM
91.11
91.27
91.19
88.34
90.16
89.25†
81.10
82.60
81.85
81.85
85.23
83.54
(proposed)
(+0.45)
(+0.14)
(+2.54)
(+0.82)
TABLE V
COMPARISON OF PERFORMANCE IN EACH ENTITY IN SEQUENTIAL
LABELING OF THE CONLL2003 DATASET. THE PREFIXES B AND I ARE
MARGED FOR DISPLAY PURPOSES.
w/ MoM
BERT
Prec.
Rec.
F1
Prec.
Rec.
F1
Diff
MISC
79.18
84.58
81.78
79.21
84.04
81.54
+0.24
LOC
92.91
93.50
93.20
92.60
93.49
93.04
+0.16
ORG
89.87
93.17
91.54
89.90
92.70
91.27
+0.27
PER
97.66
97.88
97.77
97.76
97.55
97.65
+0.12
O
99.72
99.28
99.50
99.69
99.31
99.50
0.00
also demonstrated the best performance and was significant at
α = 0.05, against the other best method DL.
VI. DISCUSSION
The most important factor in NER is the score of the entity
classes, rather than the overall score, including O-class, as the
prediction performance of NER generally concerns the score
including the O-class. In practical situations in which entities
are extracted and utilized, the performances of the entity
classes hold greater significance. Although MoM learning
appears a marginal improvement, we confirm MoM learning
improves the performance of minor entity classes without
sacrificing the performance of the major O-class, regardless
of the language and models.
TABLE VI
SUMMARY OF THE PERFORMANCE OF THE MRC TASK ON THE
CONLL2003 DATASET.
CoNLL2003
Prec.
Rec.
F1
BERT-MRC (baseline)
92.43
92.22
92.32
BERT-MRC + FL
92.95
92.10
92.52
(+0.20)
BERT-MRC + DL
92.69
92.43
92.56
(+0.24)
BERT-MRC + MoM
92.99
92.51
92.75
(proposed)
(+0.43)
For WCE, we attempted two methods (i.e., WCE-1 and
WCE-2); however, a poor performance than the baseline CE
was observed, highlighting the challenges posed by multiclass
NER with its inherent long-tail distribution. As evidenced by
various ML tasks, conventional weighting methods struggle
with the delicate design of loss functions dependent on spe-
cific datasets and tasks [35], [36]. Thus, the experiments in
Tables III and IV highlight the difficulty of applying WCE
weighting to the sequential labeling of NER.
MoM learning was effective at sequential labeling and
MRC, especially the latter, where we observe the role of MoM
learning in monitor the number of entities. For example, in
the sentence “Estadio Santiago Bernab´eu opened in 1974.”,
“Estadio”, “Santiago” and “Bernab´eu” are assigned the classes
B-LOC, I-LOC, I-LOC, and other words are assigned the
O-class, respectively. when considering a basic sentence, it
is highly likely that the word (“opened”) following the last
word (“Bernab´eu”) of the entity belongs to I-LOC or O-class
because sequences of different entity words are extremely rate,
such as LOCATION after PERSON. Because MoM learning
focus more on O-class words, the model can learn whether the
final word belongs to the I-LOC or O-class. In other words,
the MoM can monitor how many entity words are consecutive,
which is a factor in the improved performance of the words
at the end of the entity.
VII. CONCLUSION
In this paper, we proposed a novel learning method, MoM
learning, to address NER task characterized by the data imbal-
ance with a long-tail distribution consisting of a single class
with many samples (the majority class) and multiple classes
with a few samples (the minority classes). MoM learning is a
simple and effective method that suppresses misclassifications
of majority as minority classes by incorporating the loss of
samples which ground truth is the majority class into the
loss of conventional ML models. Evaluation experiments using
four datasets (English and Japanese) showed that MoM learn-
ing outperforms existing and even state-of-the-art methods in
addressing data imbalances regardless of language, model, or
framework, whether sequential labeling or MRC.
REFERENCES
[1] D. Nadeau and S. Sekine, “A survey of named entity recognition and
classification,” Lingvisticae Investigationes, vol. 30, no. 1, pp. 3–26,
2007.
[2] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer,
“Neural architectures for named entity recognition,” arXiv preprint
arXiv:1603.01360, 2016.
[3] J. Guo, G. Xu, X. Cheng, and H. Li, “Named entity recognition in
query,” in Proc. of the 32nd international ACM SIGIR conference on
Research and development in information retrieval, 2009, pp. 267–274.
[4] R. Ramachandran and K. Arutchelvan, “Named entity recognition on
bio-medical literature documents using hybrid based approach,” Journal
of Ambient Intelligence and Humanized Computing, pp. 1–10, 2021.
[5] E. T. K. Sang and F. De Meulder, “Introduction to the CoNLL-2003
Shared Task: Language-Independent Named Entity Recognition,” in
Proc. of the Seventh Conference on Natural Language Learning at HLT-
NAACL 2003, 2003, pp. 142–147.
[6] S. Pradhan, A. Moschitti, N. Xue, H. T. Ng, A. Bj¨orkelund, O. Uryupina,
Y. Zhang, and Z. Zhong, “Towards robust linguistic analysis using
ontonotes,” in Proc. of CoNLL, 2013, pp. 143–152.
[7] S. Pouyanfar, Y. Tao, A. Mohan, H. Tian, A. S. Kaseb, K. Gauen, R. Dai-
ley, S. Aghajanzadeh, Y.-H. Lu, S.-C. Chen et al., “Dynamic sampling
in convolutional neural networks for imbalanced data classification,” in
Proc. of MIPR, 2018, pp. 112–117.
[8] M. Buda, A. Maki, and M. A. Mazurowski, “A systematic study of
the class imbalance problem in convolutional neural networks,” Neural
networks, vol. 106, pp. 249–259, 2018.
[9] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their composi-
tionality,” Proc. of NeurIPS, vol. 26, 2013.
[10] H. Adel, F. Chen, and Y.-Y. Chen, “Ranking convolutional recurrent
neural networks for purchase stage identification on imbalanced Twitter
data,” in Proc. of EACL, 2017, pp. 592–598.
[11] H. T. Madabushi, E. Kochkina, and M. Castelle, “Cost-sensitive bert
for generalisable sentence classification on imbalanced data,” in Proc.
of NLP4IF, 2019, pp. 125–134.
[12] J. Li and L. Xiao, “syrapropa at semeval-2020 task 11: Bert-based
models design for propagandistic technique and span detection,” in Proc.
of SemEval, 2020, pp. 1808–1816.
[13] M. Hangyo, D. Kawahara, and S. Kurohashi, “Building a diverse
document leads corpus annotated with semantic relations,” in Proc. of
PACLIC, 2012, pp. 535–544.
[14] T. Omi, “stockmarkteam/ner-wikipedia-dataset: Japanese Named Entity
Extraction
Dataset
using
Wikipedia,”
2021.
[Online].
Available:
https://github.com/stockmarkteam/ner-wikipedia-dataset
[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
of Deep Bidirectional Transformers for Language Understanding,” in
Proc. of NAACL-HLT, 2019, pp. 4171–4186.
[16] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert
pretraining approach,” CoRR preprint arXiv:1907.11692, 2019.
[17] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar, “Focal loss for
dense object detection,” in Proc. of ICCV, 2017, pp. 2980–2988.
[18] X. Li, X. Sun, Y. Meng, J. Liang, F. Wu, and J. Li, “Dice loss for
data-imbalanced nlp tasks,” in Proc. of ACL, 2020, pp. 465–476.
[19] X. Li, J. Feng, Y. Meng, Q. Han, F. Wu, and J. Li, “A unified mrc
framework for named entity recognition,” in Proc. of ACL, 2020, pp.
5849–5859.
[20] Y. Zhang and H. Zhang, “Finbert–mrc: Financial named entity recog-
nition using bert under the machine reading comprehension paradigm,”
Neural Processing Letters, pp. 1–21, 2023.
[21] S. Barua, M. M. Islam, X. Yao, and K. Murase, “MWMOTE–majority
weighted minority oversampling technique for imbalanced data set
learning,” IEEE TKDE, vol. 26, no. 2, pp. 405–425, 2012.
[22] Y.-P. Zhang, L.-N. Zhang, and Y.-C. Wang, “Cluster-based majority
under-sampling approaches for class imbalance learning,” in Proc .of
ICIFE, 2010, pp. 400–404.
[23] T. D. Tran, M. N. Ha, L. H. B. Nguyen, and D. Dinh, “Improving
multi-grained named entity recognition with BERT and focal loss,” ICIC
Express Letters, Part B: Applications, vol. 12, no. 3, pp. 291–299, 2021.
[24] R. R. Shamir, Y. Duchin, J. Kim, G. Sapiro, and N. Harel, “Continuous
dice coefficient: a method for evaluating probabilistic segmentations,”
CoRR preprint arXiv:1906.11031, 2019.
[25] L. A. Ramshaw and M. P. Marcus, “Text chunking using transformation-
based learning,” in Natural language processing using very large cor-
pora, 1999, pp. 157–176.
[26] R. Caruana, “Multitask learning,” Machine learning, vol. 28, no. 1, pp.
41–75, 1997.
[27] Y. Zhang, Y. Wei, and Q. Yang, “Learning to multitask,” Proc. of
NeurIPS, vol. 31, 2018.
[28] S. Kitada, H. Iyatomi, and Y. Seki, “Conversion prediction using multi-
task conditional attention networks to support the creation of effective
ad creatives,” in Proc. of KDD, 2019, pp. 2069–2077.
[29] A. Spangher, J. May, S.-R. Shiang, and L. Deng, “Multitask semi-
supervised learning for class-imbalanced discourse classification,” in
Proc. of EMNLP, 2021, pp. 498–517.
[30] J. Bergstra, R. Bardenet, Y. Bengio, and B. K´egl, “Algorithms for hyper-
parameter optimization,” Advances in neural information processing
systems, vol. 24, 2011.
[31] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, “Optuna: A next-
generation hyperparameter optimization framework,” in Proc. of KDD,
2019, pp. 2623–2631.
[32] R. Iikura, M. Okada, and N. Mori, “Improving bert with focal loss
for paragraph segmentation of novels,” in Distributed Computing and
Artificial Intelligence, 17th International Conference.
Springer, 2021,
pp. 21–30.
[33] J. Liu, X. Duan, R. Zhang, Y. Sun, L. Guan, and B. Lin, “Relation
classification via bert with piecewise convolution and focal loss,” Plos
one, vol. 16, no. 9, p. e0257092, 2021.
[34] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980, 2014.
[35] S. Valverde, M. Cabezas, E. Roura, S. Gonz´alez-Vill`a, D. Pareto, J. C.
Vilanova, L. Rami´o-Torrent`a, `A. Rovira, A. Oliver, and X. Llad´o,
“Improving automated multiple sclerosis lesion segmentation with a
cascaded 3d convolutional neural network approach,” NeuroImage, vol.
155, pp. 159–168, 2017.
[36] S. Jadon, “A survey of loss functions for semantic segmentation,” in
Proc. of CIBCB, 2020, pp. 1–7.
","nanConventional machine learning (ML) methods for addressing data imbalances are categorized into sampling-based methods and cost-sensitive learning. Sampling-based methods adjust the number of sentences in training but do not mitigate the imbalance for sequential labeling. Conversely, cost-sensitive learning addresses the imbalance by designing a loss function based on the number of samples in each class, which is effective for binary classification but requires complex weight adjustments for multiclass classification. Existing methods do not fully address the desired performance or fall short of it."
"Representation learning aims to discover the hidden modular attributes that generate data faithfully. Conventional approaches overlook the inductive bias in the diffusion process, and the learned feature representation entangles multiple modular attributes. We argue that the forward diffusion process progressively accumulates attribute loss, where attributes are sequentially lost as time progresses. We reveal that finer-grained attributes are lost at an earlier time-step than coarser-grained attributes. Leveraging this attribute loss as an inductive bias, we learn a modular feature by disentangling the expansion set of lost modular attributes as diffusion time-steps increase. Extensive experiments on CelebA, FFHQ, and Bedroom datasets show that our DiTi feature outperforms state-of-the-art methods for attribute inference tasks. Furthermore, the learned feature enables faithful counterfactual generation, validating the disentanglement quality of our DiTi.","Representation learning is all about discovering the hidden modular attributes that generate the data faithfully. We explore the potential of Denoising Diffusion Probabilistic Model (DM) in unsupervised learning of the modular attributes. We build a theoretical framework that connects the diffusion time-steps and the hidden attributes, which serves as an effective inductive bias for unsupervised learning. Specifically, the forward diffusion process incrementally adds Gaussian noise to samples at each time-step, which essentially collapses different samples into similar ones by losing attributes, e.g., fine-grained attributes such as texture are lost with less noise added (i.e., early time-steps), while coarse-grained ones such as shape are lost by adding more noise (i.e., late time-steps). To disentangle the modular attributes, at each time-step t, we learn a t-specific feature to compensate for the newly lost attribute, and the set of all {1, . . . , t}-specific features, corresponding to the cumulative set of lost attributes, are trained to make up for the reconstruction error of a pre-trained DM at time-step t. On CelebA, FFHQ, and Bedroom datasets, the learned feature significantly improves attribute classification and enables faithful counterfactual generation, validating the disentanglement quality.","Can we extract DM’s knowledge about the modular attributes as a compact feature vector? To answer the question, we start by revealing a natural connection between the diffusion time-steps and the hidden attributes. In the forward diffusion process, given an initial sample x0, a small amount of Gaussian noise N(0, I) is incrementally added to the sample across T time-steps, resulting in a sequence of noisy samples x1, . . . , xT . In particular, each xt adheres to its noisy sample distribution q(xt|x0), whose mean and covariance matrix σ2I is denoted as a dot and a circle with radius σ, respectively. As t increases, q(xt|x0) progressively collapses towards a pure noise by reducing mean (i.e., dots moving closer to the origin), and increasing σ (i.e., larger radii), which enlarges the overlapping area (OVL) of the probability density function of noisy sample distributions. In particular, we theoretically show in Section 4.1 that OVL is correlated with DM’s ambiguous reconstruction of different samples, e.g., due to the large overlap between the red circles at t1, noisy samples drawn from the two distributions are reconstructed ambiguously as either A or B, losing the attribute “expression”1 that distinguishes them. As t increases, more noisy sample distributions exhibit significant overlaps, inducing additional attribute loss, e.g., further losing “gender” at t2 that confuses the identities of A/B and C/D. Eventually, with a large enough T, xT becomes pure noise and all attributes are lost—the reconstruction can be any sample. This intrinsic connection between diffusion time-steps and modular attributes can act as an effective inductive bias for unsupervised learning. Specifically, DM training can be viewed as learning to reconstruct x0 given xt at each t (Section 3.2). However, due to the aforementioned attribute loss, perfect DM reconstruction is impossible, e.g., in Figure 1b, reconstructions exhibit variations in the cumulatively lost attributes. Hence, by contraposition, if a feature enables perfect reconstruction, it must capture the cumulatively lost attributes. This motivates us to learn an expanding set of features to supplement the expanding lost attributes as t increases.","We term our approach as DiTi to highlight our leverage of Diffusion Time-step as an inductive bias for feature learning. We summarize the paper structure and our contributions below:

* In Section 3.1, we formalize the notion of good feature with a definition of disentangled representation Higgins et al. (2018) and provide a preliminary introduction to DM in Section 3.2.
* In Section 4.1, we build a theoretical framework that connects diffusion time-steps and the hidden modular attributes, which motivates a simple and practical approach for unsupervised disentangled representation learning, discussed in Section 4.2.
* In Section 5, by extensive experiments on CelebA Liu et al. (2015), FFHQ Karras et al. (2019) and Bedroom Yu et al. (2015), our DiTi feature brings significant improvements in attribute inference accuracy and enables counterfactual generation, verifying its disentanglement quality.","We presented a novel unsupervised method to learn a disentangled representation, which leverages the inductive bias of diffusion time-steps. In particular, we reveal an inherent connection between time-step and hidden modular attributes that generate data faithfully, enabling a simple and effective approach to disentangle the attributes by learning a time-step-specific feature. The learned feature improves downstream inference and enables counterfactual generation, validating its disentanglement quality. As future work, we will seek additional inductive bias to improve disentanglement, e.g., using text as a disentangled template by exploring text-to-image diffusion models, and devise practical optimization techniques to enable faster convergence.",Exploring Diffusion Time-steps for Unsupervised Representation Learning,"Zhongqi Yue, Jiankun Wang, Qianru Sun, Lei Ji, Eric I-Chao Chang, Hanwang Zhang","Preprint
EXPLORING DIFFUSION TIME-STEPS FOR UNSUPER-
VISED REPRESENTATION LEARNING
Zhongqi Yue1, Jiankun Wang1, Qianru Sun2, Lei Ji3, Eric I-Chao Chang3, Hanwang Zhang1
1Nanyang Technological University,
2Singapore Management University,
3Microsoft Research Asia
yuez0003@ntu.edu.sg,
jiankun001@e.ntu.edu.sg,
qianrusun@smu.edu.sg,
leiji@microsoft.com,
eric.i.chang@outlook.com,
hanwangzhang@ntu.edu.sg
ABSTRACT
Representation learning is all about discovering the hidden modular attributes that
generate the data faithfully. We explore the potential of Denoising Diffusion Prob-
abilistic Model (DM) in unsupervised learning of the modular attributes. We build
a theoretical framework that connects the diffusion time-steps and the hidden at-
tributes, which serves as an effective inductive bias for unsupervised learning.
Specifically, the forward diffusion process incrementally adds Gaussian noise to
samples at each time-step, which essentially collapses different samples into sim-
ilar ones by losing attributes, e.g., fine-grained attributes such as texture are lost
with less noise added (i.e., early time-steps), while coarse-grained ones such as
shape are lost by adding more noise (i.e., late time-steps). To disentangle the
modular attributes, at each time-step t, we learn a t-specific feature to compensate
for the newly lost attribute, and the set of all {1, . . . , t}-specific features, corre-
sponding to the cumulative set of lost attributes, are trained to make up for the
reconstruction error of a pre-trained DM at time-step t. On CelebA, FFHQ, and
Bedroom datasets, the learned feature significantly improves attribute classifica-
tion and enables faithful counterfactual generation, e.g., interpolating only one
specified attribute between two images, validating the disentanglement quality.
Codes are in https://github.com/yue-zhongqi/diti.
1
INTRODUCTION
A good feature representation should faithfully capture the underlying generative attributes in a
compact and modular vector space Bengio et al. (2013), enabling not only sample inference (e.g.,
image classification) but also counterfactual generation Besserve et al. (2020) (e.g., image synthesis
of unseen attribute combinations). Over the past decade, discriminative training has been the feature
learning mainstream with exceptional performance in inference tasks He et al. (2016; 2020). How-
ever, it hardly achieves faithful generation due to the deliberate discard of certain attributes, e.g.,
class-irrelevant ones in supervised learning or augmentation-related ones in contrastive learning.
On the other hand, generative Denoising Diffusion Probabilistic Model (DM) Sohl-Dickstein et al.
(2015); Song et al. (2021) can retain all the underlying attributes for faithful generation Dhariwal &
Nichol (2021), or even extrapolate novel attribute combinations by textual control Rombach et al.
(2022) (e.g., “Teddy bear skating in Time Square”), outperforming other generative models like
VAE Kingma & Welling (2014) and GAN Creswell et al. (2018). This implies that DM effectively
captures the modularity of hidden attributes Yue et al. (2021). However, as DM’s formulation has
no explicit encoders that transform samples into feature vectors, the conventional encoder-decoder
feature learning paradigm via reconstruction Higgins et al. (2017) is no longer applicable.
Can we extract DM’s knowledge about the modular attributes as a compact feature vector? To an-
swer the question, we start by revealing a natural connection between the diffusion time-steps and
the hidden attributes. In the forward diffusion process, given an initial sample x0, a small amount
of Gaussian noise N(0, I) is incrementally added to the sample across T time-steps, resulting in a
sequence of noisy samples x1, . . . , xT . In particular, each xt adheres to its noisy sample distribution
q(xt|x0), whose mean and covariance matrix σ2I is denoted as a dot and a circle with radius σ in
Figure 1a, respectively. As t increases, q(xt|x0) progressively collapses towards a pure noise by
1
arXiv:2401.11430v1  [cs.CV]  21 Jan 2024
Preprint
B
A
B
C
D
gender lost
𝐱𝐱0
𝐱𝐱𝑡𝑡1
𝐱𝐱𝑡𝑡2
𝐱𝐱𝑇𝑇
…
𝑡𝑡 = 0
𝑡𝑡 = 𝑡𝑡1
𝑡𝑡 = 𝑡𝑡2
𝑡𝑡 = 𝑇𝑇
𝐱𝐱0
𝐱𝐱𝑡𝑡1
𝐱𝐱𝑡𝑡2
𝐱𝐱𝑇𝑇
…
𝑞𝑞(𝐱𝐱1|𝐱𝐱0)
𝑞𝑞(𝐱𝐱2|𝐱𝐱1)
𝓛𝓛𝟏𝟏
ො𝐱𝐱𝟎𝟎 from 𝐱𝐱𝒕𝒕
reconstruct
𝐱𝐱0
expression
newly lost
expression lost
others lost
A
C
D
cumulatively lost
gender
others
initial samples
(a)
(b)
…
…
…
…
B
Figure 1: (a) Illustration of attribute loss as time-step t increases in the forward diffusion process. The two axes
depict a two-dimensional sample space. (b) DM reconstructed x0, denoted as ˆx0, from randomly sampled xt
at various t. DM is pre-trained on CelebA, from where x0 is drawn.
reducing mean (i.e., dots moving closer to the origin), and increasing σ (i.e., larger radii), which en-
larges the overlapping area (OVL) of the probability density function of noisy sample distributions.
In particular, we theoretically show in Section 4.1 that OVL is correlated with DM’s ambiguous
reconstruction of different samples, e.g., due to the large overlap between the red circles at t1, noisy
samples drawn from the two distributions are reconstructed ambiguously as either A or B, losing
the attribute “expression”1 that distinguishes them. As t increases, more noisy sample distributions
exhibit significant overlaps, inducing additional attribute loss, e.g., further losing “gender” at t2 that
confuses the identities of A/B and C/D. Eventually, with a large enough T, xT becomes pure noise
and all attributes are lost—the reconstruction can be any sample.
This intrinsic connection between diffusion time-steps and modular attributes can act as an effective
inductive bias for unsupervised learning. Specifically, DM training can be viewed as learning to
reconstruct x0 given xt at each t (Section 3.2). However, due to the aforementioned attribute loss,
perfect DM reconstruction is impossible, e.g., in Figure 1b, reconstructions exhibit variations in the
cumulatively lost attributes. Hence, by contraposition, if a feature enables perfect reconstruction, it
must capture the cumulatively lost attributes. This motivates us to learn an expanding set of features
to supplement the expanding lost attributes as t increases. Specifically, we first map x0 to a feature
z = f(x0) using an encoder f. Then we partition z into T disjoint subsets {zi}T
i=1. At each t,
we optimize f so that {zi}t
i=1 compensates for a pre-trained DM’s reconstruction error. Intuitively
by induction, starting from t = 0 with no lost attribute; if {zi}t
i=1 captures the cumulatively lost
attributes till t, zt+1 must capture the newly lost attribute to enable perfect reconstruction at t + 1;
until t = T − 1, {zi}T −1
i=1 ∪ zT learns all hidden attributes in a compact and modular vector space.
We term our approach as DiTi to highlight our leverage of Diffusion Time-step as an inductive bias
for feature learning. We summarize the paper structure and our contributions below:
• In Section 3.1, we formalize the notion of good feature with a definition of disentangled repre-
sentation Higgins et al. (2018) and provide a preliminary introduction to DM in Section 3.2.
• In Section 4.1, we build a theoretical framework that connects diffusion time-steps and the
hidden modular attributes, which motivates a simple and practical approach for unsupervised
disentangled representation learning, discussed in Section 4.2.
• In Section 5, by extensive experiments on CelebA Liu et al. (2015), FFHQ Karras et al. (2019)
and Bedroom Yu et al. (2015), our DiTi feature brings significant improvements in attribute
inference accuracy and enables counterfactual generation, verifying its disentanglement quality.
2
RELATED WORKS
DM for Representation Learning. There are three main approaches: 1) DDIM Song et al. (2020)
inversion aims to find a latent xT as feature, by DDIM sampling with the learned DM, can recon-
struct the image x0. However, this process is time-consuming, and the resulting latent is difficult
to interpret Hertz et al. (2022). 2) Kwon et al. (2022) uses the bottleneck feature of the U-Net
at all time-steps. However, the feature is not compact and difficult for downstream leverage. 3)
1Attribute name is illustrative. Our method uses no attribute supervision, nor explicitly names them.
2
Preprint
0
2000
4000
6000
8000
10000
12000
young
young
male
male
high cheek
high cheek
PDAE (entangled)
0.02
10000
5000
# Dimensions
Classifier weight value
0.06
0.10
0.14
0.18
208
126
154
44
82
13
58
2
Classifier of 40 attributes on CelebA
𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏
𝐏𝐏𝐃𝐃𝐃𝐃𝐃𝐃
(a) Examples of counterfactual generation
(b) Histogram of classifier weight value
(𝐏𝐏𝐀𝐀𝐀𝐀. 𝟔𝟔𝟔𝟔. 𝟐𝟐)
(𝐏𝐏𝐀𝐀𝐀𝐀. 𝟔𝟔𝟐𝟐. 𝟑𝟑)
DiTi (disentangled)
Figure 2: (a) Counterfactual generations on CelebA by manipulating 16 out of 512 feature dimensions (i.e.,
simulating the edit of a single zi). A disentangled representation enables editing a single attribute (e.g., gender)
without affecting others (e.g., lighting) and promotes faithful extrapolation (e.g., no artifacts). (b) Histogram of
the classifier weight value. More dimensions of DiTi weights are closed to 1 and 0 (explanations in the text).
The closest to our works are Preechakul et al. (2022); Zhang et al. (2022). However, they learn a
time-step-independent z to supplement DM reconstruction error. Without explicit design to enforce
modularity, the learned feature entangles all the attributes. At time-step t, it leaks information about
attributes lost at a later time-step (i.e., t + 1, . . . , T). Hence, instead of learning the lost attribute zt,
the reconstruction may digress to use the spurious correlation between leaked information and lost
attribute. In Section 5, we validate this deficiency by observing failed counterfactual generation and
lower attribute prediction accuracy.
Disentangled Representation separates the modular hidden attributes that generate data, which
can be formally defined from a group-theoretical view Higgins et al. (2018). Our work focuses on
the unsupervised learning setting. Existing methods typically learn a VAE Burgess et al. (2017)
or GAN Jeon et al. (2021) with information bottleneck. Yet they lack explicit inductive bias that
is necessary for unsupervised disentanglement Locatello et al. (2019). Our work reveals that the
connection between hidden attributes and diffusion time-steps is an effective inductive bias towards
disentanglement (Section 4.1).
3
PRELIMINARIES
3.1
DISENTANGLED REPRESENTATION
A good feature can be formally defined by the classic notion of disentangled representation. We aim
to provide a preliminary introduction below, and refer interesting readers to Higgins et al. (2018);
Wang et al. (2021) for more formal details. Each sample x ∈ X in the image space X is gen-
erated from a hidden attribute vector z ∈ Z in the vector space Z through an injective mapping
Φ : Z → X (i.e., different samples are generated by different attributes). In particular, Z can be
decomposed into the Cartesian product of N subspaces Z = Z1 ×. . .×ZN, where each Zi is called
a modular attribute, as its value can be locally intervened without affecting other modular attributes
(e.g., changing “expression” without affecting “gender” or “age”). A disentangled representation
is a mapping f : X → Z that inverses Φ, i.e., given each image x, f(x) recovers the modular
attributes as a feature vector z = [z1, . . . , zN], which possesses desired properties of a good feature
in the following common views:
Counterfactual generation. A counterfactual image of x by intervening the i-th modular attribute
with gi (e.g., aging) is denoted as gi · x ∈ X (e.g., an older x). Specifically, gi · x is generated by
first mapping x to z by f, editing zi according to gi without changing other attributes, and finally
feeding the modified z to Φ. In fact, gi·x is formally known as group action Judson (1994). Figure 2
shows counterfactual images generated by editing three attributes.
Sample inference. A modular feature facilitates robust inference in downstream tasks about any
subset of {zi}N
i=1, e.g., a linear classifier can eliminate task-irrelevant environmental attributes by
assigning larger values to the dimensions corresponding to zi and 0 to other dimensions. In Figure 2,
by explicitly disentangling the modular attributes (Section 4), our DiTi achieves higher accuracy in
3
Preprint
attribute prediction than PDAE Zhang et al. (2022) (without disentangling), while more dimensions
of our classifier weight are closed to 1 (corresponding to zi) and 0 (environmental attributes).
3.2
DENOISING DIFFUSION PROBABILISTIC MODEL
Denoising Diffusion Probabilistic Model Ho et al. (2020) (DM) is a latent variable model with a
fixed forward process, and a learnable reverse process. Details are given below.
Forward Process. It is fixed to a Markov chain that incrementally adds Gaussian noise to each
sample x0 in T time-steps, producing a sequence of noisy samples x1, . . . , xT as latents of the same
dimensionality as the original sample x0. Given x0 and a variance schedule β1, . . . , βT (i.e., how
much noise is added at each time-step), each xt adheres to the following noisy sample distribution:
q(xt|x0) = N(xt; √¯αtx0, (1 − ¯αt)I),
where αt := 1 − βt, ¯αt :=
tY
s=1
αs.
(1)
Reverse Process. It corresponds to a learned Gaussian transition pθ(xt−1|xt) parameterized by
θ, starting at p(xT ) := N(xT ; 0, I). Each pθ(xt−1|xt) is computed in two steps: 1) Reconstruct
x0 from xt with uθ(xt, t), where uθ is a learnable U-Net Ronneberger et al. (2015). 2) Compute
q (xt−1|xt, uθ(xt, t)), which has a closed-form solution given in Appendix. Training is performed
by minimizing the reconstruction error made by uθ(xt, t) at each time-step t:
LDM =
E
t,x0,ϵ

¯αt
1 − ¯αt
∥x0 − uθ(√¯αtx0 +
√
1 − ¯αtϵ, t)∥2

,
(2)
where ϵ ∼ N(0, I) is a random noise. Note that we formulate the U-Net to predict x0 (equivalent
to the ϵ-formulation in Ho et al. (2020)). As DM has no explicit feature encoder, we learn one to
capture the modular attributes by leveraging the inductive bias in the diffusion time-steps.
4
METHOD
4.1
THEORY
We reveal that in the forward diffusion process, fine-grained attributes are lost at an earlier time-step
compared to coarse-grained ones, where the granularity is defined by the pixel-level changes when
altering an attribute. We first formalize the notion of attribute loss, i.e., when DM fails to distinguish
the noisy samples drawn from two overlapping noisy sample distributions q(xt|x0) and q(yt|y0).
Definition. (Attribute Loss) Given a θ-parameterized DM optimized by Eq. 2, we say that attribute
Zi is lost with degree τ at t when Ex0∈X [Err(x0, y0 = gi · x0, t)] ≥ τ, where Err(x0, y0, t) :=
1
2

E
q(xt|x0) [1 (∥ˆx0 − x0∥ > ∥ˆx0 − y0∥)] +
E
q(yt|y0) [1 (∥ˆy0 − x0∥ < ∥ˆy0 − y0∥)]

,
(3)
with 1(·) denoting the indicator function and ˆx0 = uθ(xt), ˆy0 = uθ(yt) denoting the DM recon-
structed samples from xt, yt, respectively.
Intuitively, Err(x0, y0, t) measures attribute loss degree by how likely a DM falsely reconstructs
xt drawn from q(xt|x0) closer to y0 instead of x0 (vice versa). Hence, when x0 and y0 differ in
attribute zi modified by gi, a larger Err(x0, y0, t) means that the attribute is more likely lost.
Theorem. (Attribute Loss and Time-step) 1) For each Zi, there exists a smallest time-step t(Zi),
such that Zi is lost with degree τ at each t ∈ {t(Zi), . . . , T}. 2) ∃{βi}T
i=1 such that t(Zi) > t(Zj)
whenever ∥x0−gi·x0∥ is first-order stochastic dominant over ∥x0−gj ·x0∥ with x0 ∼ X uniformly.
Intuitively, the first part of the theorem states that a lost attribute will not be regained as time-step
t increases, and there is a time-step t(Zi) when Zi becomes lost (with degree tau) for the first
time. The second part states that when ∥x0 − gi · x0∥ is more likely to take on a larger value than
∥x0 −gj ·x0∥, the attribute Zi (i.e., a coarse-grained attribute) is lost at a larger time-step compared
to Zj (i.e., a fine-grained attribute). We have the following proof sketch:
4
Preprint
+
…
Encoder
𝒇𝒇
…
+
+
+
𝟎𝟎
𝟎𝟎
𝟎𝟎
𝟎𝟎
𝟎𝟎
+
+
𝓛𝓛𝟏𝟏
𝑡𝑡
+
𝑡𝑡 = 1
𝑡𝑡 = 2
𝑡𝑡 = 𝑇𝑇
𝓛𝓛𝟐𝟐
𝓛𝓛𝑻𝑻
𝐱𝐱𝒕𝒕
ത𝒛𝒛𝒕𝒕
ෝ𝒙𝒙𝟎𝟎
ෝ𝒙𝒙𝟎𝟎 + 𝑤𝑤𝑡𝑡𝒈𝒈(ത𝒛𝒛𝒕𝒕, 𝑡𝑡)
Frozen DM 𝒖𝒖𝜽𝜽
Decoder 𝒈𝒈
𝐱𝐱𝟎𝟎
𝐱𝐱𝟏𝟏
𝐱𝐱𝟐𝟐
𝐱𝐱𝑻𝑻
CNN: Feature extractor
Trainable Decoder: makes up for the DM error at each timestep
ℒ𝑡𝑡: Diffusion loss at timestep 𝑡𝑡
Approach
DiTi
ത𝒛𝒛𝟏𝟏
ത𝒛𝒛𝟐𝟐
ത𝒛𝒛𝑻𝑻
Figure 3: Illustration of our DiTi. We break down Eq. 5 at each time-step. On the right, we show
the detailed network design, where ˆx0 denotes the reconstructed x0 by the pre-trained DM.
1) Increasing t induces further attribute loss.
We show in Appendix that Err(x0, y0, t) =
1
2OVL (q(xt|x0), q(yt|y0)) for an optimal DM, where OVL is the overlapping coefficient Inman &
Bradley Jr (1989), i.e., overlapping area of probability density functions (PDFs). In particular,
Err(x0, y0, t) = 1
2OVL (q(xt|x0), q(yt|y0)) = 1
2
""
1 − erf
 
∥√¯αt(y0 − x0)∥
2
p
2(1 − ¯αt)
!#
.
(4)
As ¯αt decreases with an increasing t from Eq. 1, and the error function erf(·) is strictly increasing,
Err(x0, y0, t) is strictly increasing in t given any x0, y0. Hence Ex0∈X [Err(x0, y0 = gi · x0, t)] ≥
Ex0∈X [Err(x0, y0 = gi · x0, t(Zi))] for every t ≥ t(Zi), which completes the proof.
2) Coarse-grained attributes are lost at a larger t. Given that erf(·) is strictly increasing and ∥x0 −
gi ·x0∥ is first-order stochastic dominant over ∥x0 −gj ·x0∥, we have Ex0∈X [Err(x0, gi · x0, t)] >
Ex0∈X [Err(x0, gj · x0, t)] at every time-step t using Eq. 4. Hence t(Zi) > t(Zj) under any vari-
ance schedule {βi}T
i=1 such that Zi is not lost at t(Zj), completing the proof. Note that in practice,
DM leverages a large T with small {βi}T
i=1. This ensures that Err(x0, y0, t) slows increases with t
according to Eq. 4. Hence empirically, this theorem tends to hold.
4.2
PROPOSED APPROACH
The previous theoretical analysis leads to two interesting groundings: 1) Attribute loss is linked with
the error of reconstructing x0 from the noisy xt. 2) Increasing time-step t in forward diffusion pro-
cess causes an expanding cumulative loss of attributes, with fine-grained attributes lost at a smaller t
and coarse-grained ones lost at a larger t. They serve as an effective inductive bias for unsupervised
learning: 1) making up for the reconstruction error can retrieve lost attributes, 2) an expanding set
of features captures the expanding cumulatively lost attributes. This motivates our approach below.
As illustrated in Figure 3, our model includes a frozen DM U-Net uθ pre-trained on D using Eq. 2
until convergence, a trainable encoder f (trained to become a disentangled representation) that maps
each image (without noise) to a d-dimensional feature z ∈ Rd, and a trainable decoder g that maps
z back to image space given t. In training, given z = f(x0), we partition it into T disjoint subsets
{zi}T
i=1. At each time-step t, we construct a new feature ¯zt = [z1, . . . , zt, 0, . . . , 0], which is only
about {zi}t
i=1 by masking zt+1, . . . , zT as 0. Our training loss requires ¯zt to compensate for the
reconstruction error made by uθ at each time-step t, given by:
L =
E
t,x0,ϵ

λt∥x0 −
Preprint
z2 is encouraged to further learn the attribute Zj to minimize the reconstruction error at t = 2.
Eventually with a large enough T, xT becomes pure noise and all attributes are lost. By the above
induction, {zi}T
i=1 captures all the hidden attributes as a modular feature. In contrast, the most
related work PDAE differs from Eq. 5 by replacing g(¯zt) with g(¯z). This means that PDAE uses
the full-dimensional feature z to compensate the reconstruction error at all time-steps, which fails to
leverage the inductive bias discussed in Section 4.1. We show in Section 5 that this simple change
leads to drastic differences in both inference and generation tasks.
Implementation Details. We highlight some design considerations with ablations in Section 5.2.
• Number of Subsets k. In practice, the number of subsets for partitioning z can be smaller than
T, e.g., we use feature dimension d = 512 that is smaller than T = 1, 000. This means that
adjacent time-steps may share a modular feature zi. We compare the choice of k later.
• Partition Strategy. The simplest way is to use the same number of feature dimensions for all
modular features zi (i.e., balanced). However, we empirically find that features corresponding
to time-step 100-300 requires more dimensions to improve convergence of Eq. 5. We design an
imbalanced partition strategy and compare it with the balanced one.
• Optimization Strategy. In training, we experiment detaching the gradient of z1, . . . , zt−1 to
form ¯zt, i.e., training only zt at time-step t. We find that it leads to improved disentanglement
quality at the cost of slower convergence.
5
EXPERIMENTS
5.1
SETTINGS
Datasets. We choose real-world datasets to validate if DiTi learns a disentangled representation of
the generative attributes: 1) Celebrity Faces Attributes (CelebA) Liu et al. (2015) is a large-scale
face attributes dataset. Each face image has 40 binary attribute labels, delineating facial features
and characteristics, such as expressions, accessories, and lighting conditions. 2) Flickr-Faces-HQ
(FFHQ) Karras et al. (2019) contains 70,000 high-quality face images obtained from Flickr. 3) We
additionally used the Labeled Faces in the Wild (LFW) dataset Huang et al. (2007) that provides
continuous attribute labels. 4) Bedroom is part of the Large-scale Scene UNderstanding (LSUN)
dataset Yu et al. (2015) that contains around 3 million images. We apply off-the-shelf image classi-
fiers following Yang et al. (2021) to determine scene attribute labels.
Evaluation Protocols. Our evaluation is based on the properties of a disentangled representation
(Section 3.1). For inference, we perform unsupervised learning on CelebA train split or FFHQ and
test the linear-probe attribute prediction accuracy on CelebA test split, measured by Average Preci-
sion (AP). We also evaluate the challenging attribute regression task using LFW, where the metrics
are Pearson correlation coefficient (Pearson’s r) and Mean Squared Error (MSE). For counterfac-
tual generation, we extend the conventional interpolation and manipulation technique Preechakul
et al. (2022) to tailor for our needs: 1) Instead of interpolating the whole features of an image pair,
we interpolate only a feature subset while keeping its complement fixed. With a disentangled repre-
sentation, only the attribute captured by the subset will change in the generated counterfactuals, e.g.,
changing only expression between two faces. 2) In manipulation, an image is edited by altering its
feature along the direction of an attribute classifier weight, where the classifier is trained on whole
features. We constrain the classifier by ProbMask Zhou et al. (2021) to use a small subset of the
most discriminative feature dimensions (32 out of 512 dimensions). For a disentangled representa-
tion, this should have minimal impact on the generation quality, as each single attribute is encoded
in a compact feature subset. Detailed algorithms are in Appendix.
Baselines include 3 representative lineups: 1) Conventional unsupervised disentanglement methods
β-TCVAE Chen et al. (2018) and IB-GAN Jeon et al. (2021); 2) Self-supervised learning meth-
ods SimCLR Chen et al. (2020); 3) The most related works Diff-AE Preechakul et al. (2022) and
PDAE Zhang et al. (2022) that also learn a representation by making up DM’s reconstruction errors.
Implementation Details. We followed the network design of encoder f and decoder g in PDAE and
adopted its hyper-parameter settings (e.g., λt, wt in Eq. 4, details in Appendix). This ensures that
any emerged property of disentangled representation is solely from our leverage of the inductive bias
in Section 4.1. We also used the same training iterations as PDAE, i.e., 290k iterations on CelebA,
6
Preprint
Method
CelebA
FFHQ
AP ↑
Pearson’s r ↑
MSE ↓
AP ↑
Pearson’s r ↑
MSE ↓
β-TCVAE Chen et al. (2018)
45.0
0.378
0.573
43.2
0.335
0.608
IB-GAN Jeon et al. (2021)
44.2
0.307
0.597
42.8
0.260
0.644
Diff-AE Preechakul et al. (2022)
60.3
0.598
0.421
60.5
0.606
0.410
PDAE Zhang et al. (2022)
60.2
0.596
0.410
59.7
0.603
0.416
SimCLR Chen et al. (2020)
59.7
0.474
0.603
60.8
0.481
0.638
DiTi (Ours)
62.3
0.617
0.392
61.4
0.622
0.384
Table 1: AP (%) on CelebA attribute classification and Pearson’s r, MSE on LFW attribute regression. The first
column shows the training dataset.
0%
2%
4%
6%
8%
Brown Hair
Shadow
Hat
Gray Hair
Sideburns
Rosy Cheeks
Bangs
Necktie
Big Nose
Eye Bags
Blond Hair
Pointy Nose
Earrings
Eyeglasses
Bushy Eyeb
Black Hair
Bald
Straight Hair
Male
Receding Hair
Big Lips
Makeup
Mustache
Pale Skin
Wavy Hair
Arched Eyeb
Lipstick
Double Chin
Oval Face
Attractive
Chubby
High Cheek
Blurry
Narrow Eyes
Young
Necklace
Smiling
Mouth Open
No Beard
Goatee
-10%
-5%
0%
5%
10%
15%
Young
Shadow
Smiling
Wavy Hair
Male
High Cheek
No Beard
Lipstick
Bushy Eyeb
Blond Hair
Big Lips
Mouth Open
Necklace
Bald
Pointy Nose
Straight Hair
Makeup
Eyeglasses
Big Nose
Eye Bags
Oval Face
Goatee
Attractive
Arched Eyeb
Sideburns
Gray Hair
Blurry
Double Chin
Necktie
Narrow Eyes
Rosy Cheeks
Brown Hair
Mustache
Black Hair
Pale Skin
Hat
Bangs
Recede Hair
Earrings
Chubby
Recede Hair
DiTi Acc. – PDAE Acc.
DiTi Acc. – SimCLR Acc.
Figure 4: Improvements in attribute classification precision of our DiTi over PDAE (bottom) and over SimCLR
(top). Improvements more than 2% are highlighted with red bars. Negative values are marked with blue bars.
500k iterations on FFHQ, and 540k iterations on Bedroom. Hence our DiTi training is as efficient
as PDAE. Our experiments were performed on 4 NVIDIA A100 GPUs.
5.2
INFERENCE
Comparison with Diff-AE and PDAE. In Table 1, our proposed DiTi significantly outperforms the
two related works. We break down the improved accuracy over PDAE on each attribute in Figure 4
bottom, where DiTi is consistently better on all attributes. The result, together with the previous
analysis of Figure 2b, validates that DiTi exhibits the sample inference property of a disentangled
representation. It also supports our analysis on the baselines’ deficiency in Section 2, i.e., they
may digress to use spurious correlation to supplement reconstruction error. For example, the most
improved attribute is “Brown Hair”, which strongly correlates with the female gender.
Comparison with SimCLR. SimCLR does not perform well in attribute inference, surpassed even
by PDAE. This is because SimCLR learns a representation invariant to augmentations (e.g., color
jittering), which disregards its related attributes, such as hair or skin color. This is verified by the
large improvements of DiTi over SimCLR on “Pale Skin” accuracy in Figure 4 top. The deficiency
cannot be addressed by simply removing augmentations, which leads to severe performance degra-
dation (see Appendix). Note that SimCLR slightly outperforms DiTi on some unnoticeable attributes
(e.g., earrings). This suggests that some fine-grained attributes are not well disentangled. To tackle
this, we will improve model expressiveness (e.g., using Stable Diffusion Rombach et al. (2022) as
the frozen DM) and explore more advanced training objectives Song et al. (2023) as future work.
Imbalance Detach
k
AP ↑
Pearson’s r ↑
MSE ↓
✓
16
62.1
0.619
0.389
✓
32
62.0
0.615
0.392
✓
64
62.3
0.617
0.392
✓
128
61.9
0.616
0.391
64
61.9
0.604
0.410
✓
64
62.5
0.590
0.422
✓
✓
64
62.4
0.604
0.405
Table 2: Ablations on DiTi designs on CelebA. Imbalance
and Detach: using our partition and optimization strategy.
Comparison with β-TCVAE and IB-
GAN. It is perhaps not surprising that
existing
unsupervised
disentanglement
methods perform poorly.
Their previ-
ous successes are limited to synthetic
datasets, and their models are far less
expressive compared to DM. In fact, their
pursuit of a disentangled representation
by information bottleneck proves to be
ever-elusive Locatello et al. (2019).
Ablation on #Subsets k. From Table 2 first 4 lines, we observe that inference results are not
sensitive to the subset number k (details in Section 4.2). We choose k = 64 with the highest AP.
7
Preprint
𝐱𝐱
𝐲𝐲
Pair 1
Pair 2
Pair 3
Pair 1
Pair 2
Pair 3
𝐱𝐱 to 𝐲𝐲
𝐲𝐲 to 𝐱𝐱
Early 𝒕𝒕
Middle 𝒕𝒕
Late 𝒕𝒕
Figure 5: Counterfactual generations on FFHQ by interpolating a feature subset of 3 image pairs (x, y). We
partition {zi}T
i=1 according to early t ∈ (0, T/3], middle t ∈ (T/3, 2T/3] and late t ∈ (2T/3, T]. For each
subset, we show results in 2 directions (x → y, y → x) under 3 interpolation scales (color gradient↑, scale↑).
This means that our model can learn up to 64 hidden attributes, whose combinations are certainly
diverse enough for our chosen datasets.
Ablation on Partition & Optimization Strategy. We devise an imbalanced partition strategy to
allocate more feature dimensions for zi corresponding to t ∈ {100, . . . , 300}, as this time-step range
contributes the most to the overall loss (Figure A9). This leads to faster convergence and improved
performance in Table 2 (line 3 vs. line 5). For optimization strategy, we tried detaching the gradient
of z1, . . . , zt−1 at time-step t to prevent them from capturing the lost attribute at t. Yet it leads to
slower convergence, which especially hurts the challenging regression task. We leave it as future
work to explore more principled ways of improving feature modularity.
5.3
COUNTERFACTUAL GENERATION
FFHQ Results. Figure 5 shows DiTi generations by interpolating a feature subset while fixing its
complement. Different from full-feature interpolation (Figure 7), there are two directions: x → y
fixes the complement to the corresponding feature values of x, and y → x fixes it to those of y.
By identifying how images are changing during interpolation (left to right for top, right to left for
bottom) and comparing the changes across the 3 columns, we have several observations:
1. Interpolating different subsets modifies different attributes. For example, subset of early t mainly
controls micro-expressions, such as mouth corners, cheeks, or eye details. The interpolation
hardly changes a person’s identity, but certainly moves one’s temperament towards the other
person. In contrast, subset of middle t is more responsible for identity-related attributes (e.g.,
proportion of facial features) and accessories (e.g., eyeglasses). Finally, late t controls attributes
such as hairstyle, pose, and even position of the face in the image (e.g., the bottom right face
moves towards center).
2. From early to late t, the corresponding feature subset controls a more coarse-grained feature,
where the granularity is defined based on the pixel-level changes. For example, changing hairstyle
or removing hat (subset of late t) modifies more pixels than changing facial feature proportions
(subset of middle t). This is in line with our theoretical analysis in Section 4.1. Notably, both
our feature subset partition and the correspondence between subsets and attributes granularity are
known a priori, without relying on manual discovery techniques such as latent traversal.
3. Our generated counterfactuals have minimal distortion or artifact, and demonstrate modular prop-
erties (e.g., subset of late t controls pose with minimal impact on facial features and accessories).
These are strong proofs that DiTi learns a disentangled representation Besserve et al. (2020). We
show in Appendix that all generative baselines do not exhibit these traits.
8
Preprint
PDAE
DiTi
Diff-AE
original
original
original
Indoor 
Lighting
Rusty
Cluttered 
Space
Carpet
Attr.
Figure 6: Counterfactual generations on Bedroom by manipulating 32 out of 512 feature dimensions.
Diff-AE
DiTi
PDAE
Figure 7: Results of interpolating the whole feature on FFHQ. Baselines have distortions during interpolation.
Bedroom Manipulation. Bedroom is a more challenging dataset, where room scenes are much
more complex than facial images (e.g., various layouts, decorations, objects). As shown in Figure 6,
DiTi is the only method that generates faithful counterfactuals. 1st row: only DiTi can alter lighting
without making significant changes to room furniture. 2nd row: baselines fail to modify the rustic
feeling without artifacts or changing room layouts. 3rd row: DiTi makes the room more cluttered
by introducing additional objects. Last row: only DiTi adds an additional carpet.
Whole-feature Interpolation. Under this conventional setting, as shown in Figure 7, only DiTi
smoothly interpolates between the attributes of each image pair, with minimal distortion and artifact
along the trajectories. This means that our DiTi can generate faithful counterfactuals (i.e., valid im-
ages in the support of data distribution), which validates our disentanglement quality Besserve et al.
(2020). For example, if a feature only captures the attribute “smile” (i.e., disentangled), interpolat-
ing this feature between a smiling person and a non-smiling one leads to the gradual transition of
expression. However, if a feature entangles multiple attributes, such interpolation will alter all of
them or even cause distortion when the interpolated feature is out of the data distribution.
6
CONCLUSION
We presented a novel unsupervised method to learn a disentangled representation, which leverages
the inductive bias of diffusion time-steps. In particular, we reveal an inherent connection between
time-step and hidden modular attributes that generate data faithfully, enabling a simple and effective
approach to disentangle the attributes by learning a time-step-specific feature. The learned feature
improves downstream inference and enables counterfactual generation, validating its disentangle-
ment quality. As future work, we will seek additional inductive bias to improve disentanglement,
e.g., using text as a disentangled template by exploring text-to-image diffusion models, and devise
practical optimization techniques to enable faster convergence.
7
ACKNOWLEDGEMENT
This research is supported by Microsoft Research Asia, the National Research Foundation, Sin-
gapore under its AI Singapore Programme (AISG Award No: AISG2-RP-2021-022), MOE AcRF
Tier 2 (MOE2019-T2-2-062), Wallenberg-NTU Presidential Postdoctoral Fellowship, the Lee Kong
Chian (LKC) Fellowship fund awarded by Singapore Management University, and the DSO Re-
search Grant (Fund Code MG22C03).
9
Preprint
REFERENCES
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828,
2013.
Michel Besserve, Arash Mehrjou, R´emy Sun, and Bernhard Sch¨olkopf. Counterfactuals uncover the
modular structure of deep generative models. ICLR, 2020.
Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in beta-vae. ICLR, 2017.
Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disen-
tanglement in variational autoencoders. NeurIPS, 2018.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597–1607. PMLR, 2020.
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A
Bharath. Generative adversarial networks: An overview. IEEE signal processing magazine, 35
(1):53–65, 2018.
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances
in Neural Information Processing Systems, 34:8780–8794, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning.
In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pp. 9729–9738, 2020.
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.
Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,
2022.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner.
beta-vae: Learning basic visual concepts with a
constrained variational framework. In International conference on learning representations, 2017.
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint
arXiv:1812.02230, 2018.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
Neural Information Processing Systems, 33:6840–6851, 2020.
Gary B. Huang, Vidit Jain, and Erik Learned-Miller. Unsupervised joint alignment of complex
images. In ICCV, 2007.
Henry F Inman and Edwin L Bradley Jr. The overlapping coefficient as a measure of agreement
between probability distributions and point estimation of the overlap of two normal densities.
Communications in Statistics-theory and Methods, 18(10):3851–3874, 1989.
Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, and Gunhee Kim. Ib-gan: Disentangled represen-
tation learning with information bottleneck generative adversarial networks. In AAAI, 2021.
Thomas W. Judson. Abstract Algebra: Theory and Applications (The Prindle, Weber & Schmidt
Series in Advanced Mathematics). Prindle Weber & Schmidt, 1994.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In CVPR, 2019.
10
Preprint
Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent
space. ICLR, 2022.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Sch¨olkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learn-
ing of disentangled representations. In international conference on machine learning, 2019.
Ruey-Pyng Lu, Eric P Smith, and IJ Good. Multivariate measures of similarity and niche overlap.
Theoretical Population Biology, 35(1):1–21, 1989.
Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffu-
sion autoencoders: Toward a meaningful and decodable representation. In CVPR, 2022.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-
ical image segmentation. In Medical Image Computing and Computer-Assisted Intervention–
MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceed-
ings, Part III 18, pp. 234–241. Springer, 2015.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learn-
ing, pp. 2256–2265. PMLR, 2015.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502, 2020.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. ICLR, 2021.
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Proceedings
of the 40th International Conference on Machine Learning, 2023.
Tan Wang, Zhongqi Yue, Jianqiang Huang, Qianru Sun, and Hanwang Zhang.
Self-supervised
learning disentangled group representation as feature. Advances in Neural Information Processing
Systems, 34:18225–18240, 2021.
Ceyuan Yang, Yujun Shen, and Bolei Zhou. Semantic hierarchy emerges in deep generative repre-
sentations for scene synthesis. International Journal of Computer Vision, 129:1451–1466, 2021.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.
Zhongqi Yue, Tan Wang, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. Counterfactual zero-
shot and open-set visual recognition. In CVPR, 2021.
Zijian Zhang, Zhou Zhao, and Zhijie Lin. Unsupervised representation learning from pre-trained
diffusion probabilistic models. Advances in Neural Information Processing Systems, 35:22117–
22130, 2022.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
million image database for scene recognition. IEEE transactions on pattern analysis and machine
intelligence, 40(6):1452–1464, 2017.
Xiao Zhou, Weizhong Zhang, Hang Xu, and Tong Zhang. Effective sparsification of neural networks
with global sparsity constraint. In CVPR, 2021.
11
Preprint
A
APPENDIX
This is the Appendix for “Exploring Diffusion Time-steps for Unsupervised Representation Learn-
ing”. Table A3 summarizes the abbreviations and the symbols used in the main paper.
This appendix is organized as follows:
• Section B discusses the limitation and broader impact of our work.
• Section C provides additional details on the DM formulation and the hyper-parameter
choice in Eq. (6).
• Section D gives the full proof to our Theorem, and shows the sufficiency of disentangled
representation in minimizing Eq. (6).
• Section E presents the algorithm for training, counterfactual generation and manipulation,
the network architecture and additional training details.
• Section F presents additional generation results supplementary to Figure 2, Figure 5, Fig-
ure 6 and Figure 7.
Abbreviation/Symbol
Meaning
Abbreviation
DM
Denoising Diffusion Probabilistic Model
OVL
Overlapping Coefficient
AP
Average Precision
MSE
Mean Squared Error
slerp
Spherical linear interpolation
lerp
Linear interpolation
Symbol in Theory
X
Sample space
Z
Vector space
Zi
Modular attribute
gi
Group element acting on Zi
Φ
Injective mapping Z → X
f
Disentangled representation X → Z
erf
Error function
Symbol in Algorithm
x0
Original sample
xt
Noisy sample after t forward step
q(·)
Distribution in the encoding process
pθ(·)
Distribution in the θ-parameterized decoding process
θ
Parameter of U-Net
uθ
θ-parameterized U-Net
ˆx0
Reconstructed x0
zi
i-th modular attribute value
¯zi
[z1, . . . , zi, 0, . . . , 0]
T
Total time-steps
β1, . . . , βT
Variance schedule
αt
1 − βt
¯αt
Qt
s=1 αs
Table A3: List of abbreviations and symbols used in the paper.
12
Preprint
B
LIMITATION AND BROADER IMPACT
Limitation. A disentangled representation is a sufficient condition to minimize our objective in Eq.
(6), but not a necessary one. In particular, let t(Zi) < t(Zj). At t = t(Zi), attribute Zi is lost with
a larger degree compared to Zj. Hence while Eq. (6) trains zt to mainly capture Zi, it may not fully
remove Zj. As future work, we will try other methods to enforce the modularity between feature
subsets, e.g., by using invariant learning as in Wang et al. (2021).
Broader Impact. While our learned model (encoder and decoder) can be used to generate syn-
thetic data for malicious purposes, researchers have built models to predict fake content accurately.
Moreover, the focus of our study is not improving the generation fidelity, but to learn disentangled
representation, which leads to robust and fair AI models resilient to spurious correlations.
C
ADDITIONAL DETAILS FOR APPROACH
Closed Form of q (xt−1|xt, uθ(xt, t)). Given by N(xt−1|˜µt(xt, x0), ˜βtI), where
˜µt(xt, x0) =
√¯αt−1βt
1 − ¯αt
x0 +
√αt(1 − ¯αt−1)
1 − ¯αt
xt, ˜βt = 1 − ¯αt−1
1 − ¯αt
βt.
(A6)
Equivalent Formulation. The simplified objective in DDPM Ho et al. (2020) is given by:
LDM =
E
t,x0,ϵ∥ϵ − ϵθ(√¯αtx0 +
√
1 − ¯αtϵ, t)∥2,
(A7)
where ϵθ is a θ-parameterized U-Net Ronneberger et al. (2015) to predict the added noise. From Eq.
(2), we have
ϵ = xt − √¯αtx0
√1 − ¯αt
, ϵθ = xt − √¯αtuθ
√1 − ¯αt
,
(A8)
where we slightly abuse the notation to denote the reconstructed x0 from our U-Net as uθ. Taking
Eq. A8 into Eq. A7 yields Eq. (3).
Time-step Weight λt and Compensate Strength wt. The PDAE objective is given by
LP DAE =
E
t,x0,ϵ

λp
t ∥ϵ − ϵθ(√¯αtx0 +
√
1 − ¯αtϵ, t) + wp
t g(z, t)∥2
,
(A9)
where
λp
t =

1
1 + SNR(t)
0.9 
SNR(t)
1 + SNR(t)
0.1
, SNR(t) =
¯αt
1 − ¯αt
, wp
t =
√αt(1 − ¯αt−1)
√1 − ¯αt
. (A10)
Taking Eq. A8 into Eq. A9 yields:
λt =
¯αt
1 − ¯αt
λp
t , wt =
√1 − ¯αt
√¯αt
wp
t =
rαt
¯αt
(1 − ¯αt).
(A11)
D
THEORY
ഥ𝜶𝜶𝒕𝒕𝐱𝐱𝟎𝟎
ഥ𝜶𝜶𝒕𝒕𝒚𝒚𝟎𝟎
𝑿𝑿
𝑷𝑷(𝑿𝑿)
OVL
Figure A8: The PDFs of q(xt|x0) (green)
and q(yt|y0) (blue). Without loss of gener-
ality, we consider the 1-D case of y0 > x0.
Their means are computed from Eq. (2).
Full Proof to the Theorem. We list the theorem below
for reference.
Theorem. (Attribute Loss and Time-step) 1) For each Zi,
there exists a smallest time-step t(Zi), such that Zi is lost
with degree τ at each t ∈ {t(Zi), . . . , T}. 2) ∃{βi}T
i=1
such that t(Zi) > t(Zj) whenever ∥x0 − gi · x0∥ is first-
order stochastic dominant over ∥x0 − gj · x0∥ with x0 ∼
X uniformly.
Proof.
We
start
by
showing
Err(x0, y0, t)
=
1
2OVL (q(xt|x0), q(yt|y0)). Without loss of generality,
we show a 1-D sample space X in Figure A8. The min-
imum Errθ is obtained when given each noisy sample x,
13
Preprint
DM reconstructs towards x0 if q(x|x0) > q(x|y0) and vice versa for y0, e.g., reconstructing ⋆ as
y0. However, this maximum likelihood estimation fails when a noisy sample is drawn from q(xt|x0)
(green PDF), but with a value larger than the intersection point of the two PDFs (♦), and similar ar-
guments go for q(yt|y0) (blue PDF). The error rate caused by the two failure cases corresponds to
the green shaded area and blue one, respectively, leading to an average Errθ of 1
2 of the OVL.
To compute the OVL, it is trivial in the 1-D case by leveraging the Cumulative Distribution Function
(CDF) of Gaussian distribution. Given that the two distributions have equal variance from Eq. (2),
the intersection point is given by
√¯αtx0+√¯αty0
2
. For a Gaussian distribution N(µ, σ2), its CDF is
given by 1
2
h
1 + erf( x−µ
√
2σ )
i
. Combining two results, one can easily show that the blue shaded area,
corresponding to half of the OVL, or Err(x0, y0, t), is given by:
Err(x0, y0, t) = 1
2OVL (q(xt|x0), q(yt|y0)) = 1
2
""
1 − erf
 √¯αt(y0 − x0)
2
p
2(1 − ¯αt)
!#
.
(A12)
To generalize the results to multi-variate Gaussian distributions, we use the results in Lu et al. (1989),
which shows that by projecting the data to Fisher’s linear discriminant axis, the OVL defined on the
discriminant densities is equal to that defined on the multivariate densities. Specifically, the mean of
the discriminant densities are given by
µ0 = √¯αt(y0 − x0)⊤Σ−1x0, µ1 = √¯αt(y0 − x0)⊤Σ−1y0,
(A13)
where Σ = βtI.
The common variance of the discriminant densities is given by √¯αt(y0 −
x0)⊤Σ−1(y0 − x0). Following the calculation steps to compute OVL for the 1-D case, one can
show that for both 1-D and multi-variate case, we have
Err(x0, y0, t) = 1
2OVL (q(xt|x0), q(yt|y0)) = 1
2
""
1 − erf
 
∥√¯αt(y0 − x0)∥
2
p
2(1 − ¯αt)
!#
.
(A14)
As ¯αt decreases with an increasing t from Eq. (2), and the error function erf(·) is strictly increasing,
Err(x0, y0, t) is strictly increasing in t given any x0, y0. Hence Ex0∈X [Err(x0, y0 = gi · x0, t)] ≥
Ex0∈X [Err(x0, y0 = gi · x0, t(Zi))] for every t ≥ t(Zi), which completes the proof of Theorem 1.
Given that erf(·) is strictly increasing and ∥x0 − gi · x0∥ is first-order stochastic dominant over
∥x0 − gj · x0∥, we have Ex0∈X [Err(x0, gi · x0, t)] > Ex0∈X [Err(x0, gj · x0, t)] at every time-step
t using Eq. A14. Hence t(Zi) > t(Zj) under any variance schedule {βi}T
i=1 such that Zi is not lost
at t(Zj), completing the proof of Theorem 2.
Disentangled Representation Minimizes Eq. (6). Suppose that we have a disentangled repre-
sentation f that maps images to {zi}T
i=1. Without loss of generality, we assume an attribute order
condition where z1, . . . , zT take the order such that {zi}t
i=1 makes up the cumulatively lost at-
tributes at each t, i.e., t(Zi) ≤ t, ∀i ∈ {1, . . . , t} and t(Zi) ≥ t, ∀i ∈ {t, . . . , T}. Hence given t, for
each gi such that Ex0∈X [Err(x0, y0 = gi · x0, t)] ≥ τ, we have [f(x0)]t ̸= [f(gi · x0)]t, ∀x0 ∈ X,
where [·]t extracts {zi}t
i=1 from {zi}T
i=1. Hence there exists an decoder g that maps each unique
[f(x0)]t to the corresponding reconstruction error ˆx0 − x0 by the pre-trained DM. For other gi, the
reconstruction error is bounded by Ex0∈X [Err(x0, y0 = gi · x0, t)] < τ. Hence we prove that the
reconstruction error (or attribute loss) can be arbitrarily small (up to specified τ) given a disentan-
gled representation f and a variance schedule that satisfies Theorem 2 (to make sure the attribute
order condition holds).
E
ADDITIONAL EXPERIMENT DETAILS
Network Architecture. We exactly follow the encoder and decoder design in PDAE Zhang et al.
(2022) and use the same pre-trained DM. Please refer to PDAE for more details.
Imbalanced Partition Strategy. As shown in Figure A9, we plot the average loss Lt (in the most
recent 5k iterations) at each time-step t. It is clear that time-step 100-300 contribute the most to
the overall loss. Furthermore, by comparing the loss at 5k iteration and 35k iteration, we observe
that the same time-step range contributes the most to the loss minimization. We conjecture that
the time-step 100-300 contains rich semantic information. On the other hand, late time-steps (e.g.,
after t = 500) have smaller loss value and less loss reduction, as late time-steps have very small
weight λt by the design of DDPM Ho et al. (2020). Hence accordingly, we design an imbalanced
14
Preprint
loss
time-steps
0
1000
5k iterations
35k iterations
100
300
Figure A9: Average Lt for each time-step
t in DiTi training at 5k iterations and 35k
iterations.
partition strategy to allocate more feature dimensions to
time-step 100-300 and less ones to time-step 500-1000.
Specifically, we assign 10, 25, 327, 100, 50 dimensions
to time-step range 0-50, 50-100, 100-300, 300-500, 500-
1000, respectively. Note that we only tried this dimension
allocation strategy as a heuristic approach, and we did not
search for an optimal strategy. Future work can explore
an adaptive allocation strategy.
Optimization Strategy. Figure A10 compares the loss
in Eq.
(6) by DiTi and DiTi-Detach (i.e., detaching
the gradients of z1, . . . , zt−1) throughout training. The
loss reduction of DiTi-Detach is much slower as only
1
k of the feature is trained at each iteration. As shown
in Table 2, this alternative optimization strategy hurts
the performance when transferring the feature trained on
CelebA for LFW attribute regression. We conjecture that transfer learning and regression task is
more difficult, hence LFW regression is more sensitive to model convergence. However, this strategy
does provide additional inductive bias towards disentanglement, as only zt is trained to capture
the lost attribute at time-step t. Hence we use DiTi for classification/regression tasks and DiTi-
Detach for generation tasks. As future work, we will explore improved network design and other
optimization techniques to reap the benefits of DiTi-Detach strategy without hurting convergence.
DiTi-Detach
DiTi
0.0015
DiTi-Detach
DiTi
Figure A10: Training loss of DiTi and
DiTi with detach optimiaztion strategy.
Training Algorithm. Please refer to Algorithm 1.
Counterfactual Generation Algorithm. Please refer to
Algorithm 2.
Extended Manipulation Algorithm.
We use the at-
tribute labels to train a linear classifier that predicts a spe-
cific attribute. On CelebA Liu et al. (2015), we use its
40 attribute labels for training. On Bedroom Yu et al.
(2015), there are no ground-truth attribute labels. We use
pseudo-labels produced by an off-the-shelf attribute pre-
dictor Zhou et al. (2017) to train the attribute classifier.
In particular, we adopt ProbMask Zhou et al. (2021) to
constrain the classifier such that its weight has only d′
non-zero dimensions, where d′ < d (e.g., d′ = 16 or 32
and d = 512). This design is to test the modularity of
the feature—a specific attribute (e.g., “Young”) should be
captured by the combination of a few modular attributes zi, but not all. With the trained classifier
for an attribute, to manipulate the attribute with scale λ on a sample x0, we first obtain its feature
z = f(x0), then push z along the normal vector of the decision boundary with certain scale λ, result-
ing in manipulated code z′, and finally encode xT back to the manipulated image by the guidance
of g(z′, t). The process is summarized in Algorithm 3.
Algorithm 1: DiTi training
Input : Training data distribution q(x0), pre-trained DM uθ
Output: Trained encoder f, decoder g
Randomly initialize f, g;
while not converged do
x0 ∼ q(x0);
z = f(x0);
Partition z into {zi}T
i=1;
t ∼ Uniform(1, . . . , T);
ϵ ∼ N(0, I);
xt = √¯αtx0 + √1 − ¯αtϵ;
¯zt = [z1, . . . , zt, 0, . . . , 0];
Update f, g by minimizing λt∥x0 − (uθ(xt, t) + wtg(¯zt, t))∥2 in Eq. (6);
return f, g
15
Preprint
𝐱𝐱
𝐲𝐲
Pair 1
Pair 2
Pair 3
Pair 1
Pair 2
Pair 3
𝐱𝐱 to 𝐲𝐲
𝐲𝐲 to 𝐱𝐱
Early 𝒕𝒕
Middle 𝒕𝒕
Late 𝒕𝒕
Figure A11: Supplementary to Figure 5. More counterfactual generations by our DiTi.
Algorithm 2: Counterfactual generation from x0 to x′
0 on subset S with scale λ
Input : x0, x′
0, subset S ⊂ {1, . . . , k}, scale λ, pre-trained uθ, trained f, g, sampling
sequence {ti}M
i=1 where t1 = 0 and tM = T
Output: A counterfactual image for x0
Compute xT , x′
T for x0, x′
0 with DDIM inversion, respectively;
z = f(x0), z′ = f(x′
0);
Partition z into {zi}T
i=1, z′ into {z′
i}T
i=1;
zS ← lerp(z, z′, S; λ), i.e., perform linear interpolation on all zi, i ∈ S;
xT ← slerp(xT , x′
T ; λ);
for i = M, . . . , 2 do
ˆx0 = uθ(xti, ti) + wtg(zS, ti);
xti−1 ← √¯αti−1 ˆx0 +
√1−¯αti−1(xti−√
¯αti ˆx0)
√
1−¯αti
;
return x0
Algorithm 3: Manipulating x0 with a trained ProbMask classifier and scale λ
Input : Original x0, manipulation scale λ, trained ProbMask classifier with weight parameter
w ∈ Rd, pre-trained DM uθ, trained f, g, standard deviation σ of z in the entire
training dataset, sampling sequence {ti}M
i=1 where t1 = 0 and tM = T
Output: A counterfactual image for x0
Compute xT for x0 with DDIM inversion;
z = f(x0);
z′ = z + λ σ·w
∥w∥;
for i = M, . . . , 2 do
ˆx0 = uθ(xti, ti) + wtg(z′, ti);
xti−1 ← √¯αti−1 ˆx0 +
√1−¯αti−1(xti−√
¯αti ˆx0)
√
1−¯αti
;
return x0
F
ADDITIONAL EXPERIMENT RESULTS
SimCLR Results without Augmentation. By removing color-related augmentations (i.e., color jit-
tering and random grayscale), SimCLR (trained on CelebA) suffers severe performance degradation,
only obtaining 34.7% AP, 0.176 Pearson’s r and 0.717 MSE.
16
Preprint
𝐱𝐱
𝐲𝐲
Early 𝒕𝒕
Middle 𝒕𝒕
Late 𝒕𝒕
Pair 1
Pair 2
Pair 3
Pair 1
Pair 2
Pair 3
Left to Right
Right to Left
Figure A12: Supplementary to Figure 5 by using Diff-AE.
𝐱𝐱
𝐲𝐲
Early 𝒕𝒕
Middle 𝒕𝒕
Late 𝒕𝒕
Pair 1
Pair 2
Pair 3
Pair 1
Pair 2
Pair 3
Left to Right
Right to Left
PDAE
Figure A13: Supplementary to Figure 5 by using PDAE.
PDAE
DiffAE
original
original
young
male
high
cheek
Attr.
Supp more baselines
DiTi
original
original
exp06-4-evalset_High_Cheekbones19_img1192_mask16.jpg
exp06-4-evalset_manip2attr_Male20_img14807_mask16
exp06-4-evalset_manip2attr_Young39_img9852_mask16
Figure A14: Counterfactual generations on CelebA by manipulating 32 out of 512 feature dimensions. Supple-
mentary to Figure 2.
More Counterfactual Generations by Interpolation. In Figure A11, we show additional counter-
factual generations by DiTi.
Baseline Counterfactual Generations by Interpolation. In Figure A12 and Figure A13, we show
the baseline results under the same setting as Figure 5. In contrast to our DiTi, baselines either make
no meaningful edit or produce distorted counterfactual generations, which suggest that their features
are still entangled.
17
Preprint
DiffAE
original
Mustache
Wearing 
Lipstick
Attr.
Supp more attrs.
PDAE
original
DiTi
original
exp06-4-evalset_Wearing_Lipstick36_img1329_mask16
exp06-4-evalset_Wearing_Lipstick36_img7515_mask16
exp06-4-evalset_manip2attr_Mustache22_img6601_mask
exp06-4-evalset_manip2attr_Mustache22_img9008_mask
Figure A15: CelebA counterfactual generations on two other attributes by manipulating 32 out of 512 feature
dimensions. Supplementary to Figure 2.
PDAE
DiTi
Diff-AE
original
original
original
Indoor 
Lighting
Rusty
Cluttered 
Space
Carpet
Attr.
Figure A16: More counterfactual generations on Bedroom by manipulating 32 out of 512 feature dimensions.
Supplementary to Figure 6.
𝜷𝜷-TCVAE
DiTi
IB-GAN
Diff-AE
PDAE
Figure A17: Results of interpolating the whole feature on CelebA. Baselines have distortions during interpola-
tion. Supplementary to Figure 7.
More CelebA Manipulations. In Figure A14, we compare the results of Diff-AE with PDAE and
DiTi on CelebA manipulation. In Figure A15, we include more CelebA manipulation results.
More Bedroom Manipulations. In Figure A16, we show more results of Diff-AE, PDAE and DiTi
on Bedroom manipulation.
More Whole-feature Interpolations. In Figure A17, we show interpolation results on CelebA of
all generative baselines and our DiTi. Only DiTi can interpolate without artifacts or distortions.
Notably, β-TCVAE and IB-GAN have poor generation fidelity and struggle to preserve the face
identity. This is in line with the common view of their limitations, e.g., VAE tends to generate blurry
images and GAN suffers from unstable training and difficulties of convergence. We postulate that
their information bottleneck also limits the faithfulness of their feature.
18
","A good feature representation should faithfully capture the underlying generative attributes in a compact and modular vector space Bengio et al. (2013), enabling not only sample inference (e.g., image classification) but also counterfactual generation Besserve et al. (2020) (e.g., interpolating only one specified attribute between two images, validating the disentanglement quality). Over the past decade, discriminative training has been the feature learning mainstream with exceptional performance in inference tasks He et al. (2016; 2020). However, it hardly achieves faithful generation due to the deliberate discard of certain attributes, e.g., class-irrelevant ones in supervised learning or augmentation-related ones in contrastive learning. On the other hand, generative Denoising Diffusion Probabilistic Model (DM) Sohl-Dickstein et al. (2015); Song et al. (2021) can retain all the underlying attributes for faithful generation Dhariwal & Nichol (2021), or even extrapolate novel attribute combinations by textual control Rombach et al. (2022) (e.g., “Teddy bear skating in Time Square”), outperforming other generative models like VAE Kingma & Welling (2014) and GAN Creswell et al. (2018). This implies that DM effectively captures the modularity of hidden attributes Yue et al. (2021). However, as DM’s formulation has no explicit encoders that transform samples into feature vectors, the conventional encoder-decoder feature learning paradigm via reconstruction Higgins et al. (2017) is no longer applicable.nan"
This research paper centers around grayscale image colorization with Generative Adversarial Networks (GANs) and CycleGANs and compares various colorization models on different image datasets. The focus is on improving the versatility and effectiveness of image colorization.,"Grayscale image colorization involves assigning colors to black-and-white images. This research paper explores the use of deep learning for automated colorization to enhance the creative process beyond restoring original colors. The goal is to create a generative model capable of achieving diverse and compelling image colorization, making it useful for restoring black-and-white films and recreating comic books.","The proposed conditional CycleGAN model comprises two generators and two discriminators. The generators map grayscale images to color images and vice versa. The discriminators distinguish between real and generated images in both domains. The model is trained on the LSUN bedroom dataset, Labeled Faces in the Wild dataset, and a comic dataset. Evaluation metrics include visual assessment of colorization quality and stability of the generated images.",The results show that the proposed CycleGAN model outperforms the baseline model in human face coloring and demonstrates satisfactory performance in comic coloring. The CycleGAN model generates more plausible and diverse colorizations while preserving the image's content and structure.,The research paper concludes that the proposed CycleGAN model offers significant improvements in colorization quality and diversity. It is particularly effective in human face colorization and demonstrates promising results in comic colorization. The model provides a valuable tool for tasks involving image colorization and has the potential to be applied in various creative and practical domains.,Grayscale Image Colorization with GAN and CycleGAN in Different Image Domain,"Chen Liang, Yunchen Sheng, Yichen Mo","Grayscale Image Colorization with GAN and
CycleGAN in Different Image Domains
Chen Liang
Carnegie Mellon University
Pittsburgh, PA 15213
chenlia2@andrew.cmu.edu
Yunchen Sheng
Carnegie Mellon University
Pittsburgh, PA 15213
yunchens@andrew.cmu.edu
∗Yichen Mo
Carnegie Mellon University
Pittsburgh, PA 15213
yichenmo@andrew.cmu.edu
Abstract
Automatic colorization of grayscale image has been a challenging task. Previous
research have applied supervised methods in conquering this problem [1]. In
this paper, we reproduces a GAN-based coloring model, and experiments one of
its variant. We also proposed a CycleGAN based model and experiments those
methods on various datasets. The result shows that the proposed CycleGAN model
does well in human-face coloring and comic coloring, but lack the ability to diverse
colorization.
1
Introduction
Grayscale image colorization is assigning colors to the black and white images. Original methods
for image colorization mainly involve in human efforts and traditional image processing methods.
The development of deep learning enables automated colorization to be an active research area [2].
Previously, approaches are mainly focused on restoring the original color of the images. Though, we
are interested in a generative fashion for image colorization, which could be applied for restoring the
black and white films, recreating comic books [3].
Several approaches are proposed to generate multiple plausible image colorization, including Con-
ditional Random Field (CRF), Generative Adversarial Network(GAN), or modified CNN. In this
project, we choose the state of art network proposed in Cao et. al [4] as our baseline model.
We try to reproducing the result proposed in the baseline model paper. However, the results are not as
good as the paper states. Therefore, we try to use GAN instead of wGAN. Finally, we proposed a
conditional CycleGAN model that do much better than the baseline model.
To evaluation the generalization ability of the proposed model, besides the bedroom images that are
used in the baseline model, we also experiment the baseline and CycleGAN models on a human face
dataset, and experiment the CycleGAN model on a comic dataset. The experiments show that the
proposed CycleGAN overperforms the baseline model in human face coloring, and also do well in
comic coloring.
∗Acknowledgement: Special thanks to Geyang Zhang for her generous help
arXiv:2401.11425v1  [cs.CV]  21 Jan 2024
2
Related Work
1. Unsupervised Diverse Colorization via Generative Adversarial Networks [4]
This paper described the baseline model we are using for coloring gray scale pictures with
GAN. We would like to further explore its ability of colorization, in the sense of improving
its flexibility.
2. LSUN dataset [5]
This is the gray scale image dataset we are using to train our model. The images contained
in this dataset are 256 x 256 jpg binary data.
3. Generative adversarial nets [6]
This paper describes the original design and explanation of GAN. This paper introduces
how the adversarial objective function connecting true images and generated images, so we
have a sense of how to modify the network as to fit the third property to reach our goal.
4. Learning diverse image colorization [7]
The paper talked about how they embedding a low dimension of color space into the
generator such that the grey scale image can be colored as multiple colors. Since we would
like to customize our generated image, this paper provide a nice background for image
colorization and inspired our project idea..
5. Automatic Colorization with Deep Convolutional Generative Adversarial Networks [1, 8, 9]
The current model we are using is in an unsupervised fashion. However, if we would like to
assign a specific color scheme to our generated image, we may need to have it as a ground
truth in the loss function. This paper describes the advantages and disadvantages of using
supervised learning in GAN.
6. InkGan: Generative Adversarial Networks for Ink-And-Wash Style Transfer of Photographs
[10]
3
Data
We mainly applied three datasets in evaluating our models. For all datasets we are using YUV
representation of the color image. When training as a gray scale image, only maintain the Y
dimension and reseed the U, V dimension.
1. LSUN bedroom dataset [5] is a well established color image dataset. Among all 10 scene
and 20 object categories, we choose to use the indoor occasion for its relevantly rich color
content, comparing to simple outdoor scene composition. This challenge allows our network
to fully express it’s capacity. Data size: The entire data set includes 3,033,042 images in
the training set, and 300 images in the validation set. The baseline model randomly picked
503,900 images to train the model. The size of original color image is 256 x 256. We resize
the input image to 64 x 64.
2. Labeled Face in the Wild dataset, which is available from the website http://vis-www.
cs.umass.edu/lfw/. The entire dataset includes 13233 images from 5749 people. We
explored wGAN and CycleGAN on this dataset. The original image size is 250*250.
No image prepossessing is performed. We trained the CycleGAN model on about 6000
randomly picked images and test the model on 500 randomly picked images.
3. The comic dataset is JoJo’s Bizarre Adventure Part 5: Golden Wind. There are about 1685
comic images with original image size of 1560*1200. Images are resized to 286 * 286 and
then randomly cropped to 256*256 for training and testing. We used 1525 images for training
and 160 images as testing data. For CycleGAN, the training data contains a grayscale and a
colorized version of the same comic, the colorization is done by JoJo’s Colored Adventure
Team, which is available from the website http://jojocoloredadventure.blogspot.
com/2014/08/download-our-latest-releases.html
2
Figure 1: The network structure of the baseline model.
4
Proposed Solution
4.1
WGAN - The Baseline Model
We start from the baseline model[5] which uses a conditional GANs to color the grey-scale image.
Baseline code is based on https://github.com/changjianhui/ColorGAN-update
In this method, the generator is a fully convolutional network with a single layer noise. The generator
takes in the grey-scale image, i.e Y-value of the original image, and a noise vector, then output the
U-value and V-value of the image out for the YUV color space, combines them with the Y-value of the
image to get the complete color image. Then the generated images are feed it into the discriminator.
The discriminator is trained to distinguish a fake image that is generated by the generator and the
ground-truth image with the most real colors.
The training of both generator and discriminator is performed synchronously, and the baseline model
also uses the Wasserstein GAN [11] to solve the problem of mode collapse and gradient vanishing.
With Wasserstein GAN, the loss functions we would like to minimize of the generator and the
discriminator are as follows:
LOSSG = −D(imgfake)
(1)
LOSSD = −(D(imgreal) − D(imgfake))
(2)
Here G is the generator and D is the discriminator. the output of the discriminator is a scalar logits
D ∈ R, and the larger the value is, the more probable the input image has ""real colors"".
4.2
GAN - Baseline Variant
Compared with wGAN, GAN tries to minimize a slightly different function with respective to
discriminator and generator [6].
min
G max
D V (D, G) = Ex∼pdata(x)[logD(x)] + Ez∼pz(z)[1 − log(D(G(z)))]
(3)
For original GAN, sigmoid function is applied to the discriminator. Besides, we need to calculate
logarithm for the discriminator loss and generator loss. Based on the wGAN baseline model, we
mainly changed the loss function from the baseline code.
4.3
Conditional cycleGAN
As the above methods don’t work well as described in the original paper, we try to use the cycleGAN
method and improve the original cycleGAN by adopting the concept of conditional GAN from
3
Figure 2: coloring conditional CycleGAN model structure
base line model to improve performance. Original cycleGAN model is initially proposed to do
cross-domain image transformation, such as transforming images of horses into images of zebras[12].
It is one recent successful approach to do image transformation.
Generally, CycleGAN transforms images between two domains X, and Y. It is trained to be able to
do two transformations: F : X− > Y and and G : Y − > X while trying to fulfill that:
(1) For ∀x ∼ p(x), F(x) ∼ p(Y ). For ∀y ∼ p(y), G(y) ∼ p(X). This means F and G are able to
generate convincing images from X to Y and from Y to X respectively.
(2) For ∀x ∈ X, G(F(x)) = x. For ∀y ∈ Y, G(F(y)) = y. This means the transformed image of F
or G can be transformed back by the other transformation function.
The main difference of cycle GAN and normal GAN is the additional cycle consistency L1 loss,
which is calculated as the L1 difference between the true image and generated fake image. Therefore,
the losses of cycleGAN are the typical GAN loss combined with the cycle loss:
LGAN(GenG−>C, DisC, G, C) = Ec∼p(c)[log(DisC(c))] + Eg∼p(g)[log(1 − DisG(g))]
(4)
Lcycle(GenG−>C, GenC−>G) = Eg∼p(g)[||GenC−>G(GenG−>C(g))−g||]+Ec∼p(c)[||GenG−>C(GenC−>G(c))−c||]
(5)
For our image colorization task, we proposed the ""conditonal cycleGAN"", which adopt the concept
of conditional GAN from base line model. An image with color can be represented in different forms.
RGB is the most common representation which splits a pixel into red, green, blue three channels.
Alternative representations are Lab and YUV. In this task, we have grayscale image as conditional
information, and thus it is straightforward to use YUV. Because the Y and L channel or so called
luminance channel which represents exactly the grayscale information. By using YUV, we can just
predict UV channels and then concatenate with the grayscale channel to give a full color image.
Specifically, we define these two image domains as G: grayscale images (Y value out of image YUV
space) and C: images hue(UV value out of image YUV space). Our coloring CycleGAN consists
of two generators: GenG−>C and GenC−>G, and two discriminators DisG and DisC. In original
CycleGAN, the generator has same number of input and output channel for style transform. However
in our method, GenG−>C transforms Y channel(grayscale) to UV channels(hue) while GenC−>G
transform UV channels(hue) to Y channel(grayscale). The model structure is in figure 2. Note that a
GAN can be extended to a conditional GAN if both the generator and discriminator are conditioned
on some extra information. In this case, the extra information is Y channel, and unlike normal GAN,
the generator has no random noise (z) input.
Note that there are two GANs in a cycle GAN. In first GAN, the generator is Y to UV and the
discriminator is judging real or fake color images using all YUV channels. In the second GAN, the
generator is UV to Y, and the discriminator is judging real or fake grayscale images using all YUV
4
Figure 3: Labeled Face dataset coloring
channels (which is not necessary, but we keep this structure for symmetric). The training is two
GANs in cycle GAN is in parallel. For each input sample, one color image and one grayscale image
is picked, and both are split into YUV channels, and they are sent into corresponding generators.
The generated fake UV/Y image channel are sent into corresponding discriminators after correct
concatenation.
During training, the input image size is set to be 256 by 256 , this is because of memory limit and
training time reduction, if we set input size to 720*720, it will have out of memory error even batch
size equals to one, and doubling pixel size would result in 4X training time.
In testing, we only use one generator to generate UV channels from grayscale image’s Y channel, the
final image is the combination of Y channel and generated UV channels to form YUV color encoding
image.
The conditonal CycleGAN preserves most image information and generator only have to learn the
color distribution of image.
5
Results & Analysis
1. Our first experiment is train model on the LSUN bedroom dataset. We try both wGAN method
and traditional GAN to train our baseline model, but the result is bad because all output image has
different color blocks on random position of image,so the result is not shown in report.
2. The GAN colorization does not do well for in-door still-life images, the reason might because there
are too many items and different colors in a small 256* 256 low resolution image.We suspect that
GAN is more suitable to color human faces, since color of face is simpler and consistent. Therefore,
we pick the Labeled Face dataset and on which we experiment with our baseline wGAN model, and
also our proposed cycleGAN model, the result is at Figure 3.
3. We also try our proposed cycleGAN on comic colorization, which is at Figure 4 and Figure 5.
4. From the result images, we can see that the CycleGAN performance is better than normal GAN
and wGAN. The reason might be that, normal GAN can learn the mapping between input distribution
X and target distribution Y, however, a mapping function can map a set of inputs to any random
5
Figure 4: Conditional cycleGAN colorized comic
Figure 5: Conditional cycleGAN colorized comic 2
6
Figure 6: cycle loss comparison
Figure 7: original cycleGAN trying to learn comic image structure instead of color
permutation of outputs in target distribution, which means there is no constraint to teach the mapping
function to map the a particular input image x in distribution X to particular desired output image y
in distribution Y. To add more “guideline” while training, cycle-gan introduce cycle loss, and based
on our result, it really improve the performance and the output image’s color is more stable.
5. During training, we find that the loss and output image of conditional cycleGAN is more stable
compared to original cycleGAN, the loss comparison is shown at Figure 6. We can see that conditional
cycleGAN has lower loss and the loss is more stable. The original cycleGAN output image shown
at Figure 7. We can see that the model is trying to learn the structure of comic image instead of
just color of comic image, the white vertical line is caused by model want to learn the comic frame
structure.
6. For comic colorization, our model can somehow learn the hair color of the main character in comic.
For example, the hair color of the main character Jotaro can keep the same across different output
colorized images. And also, if one character is not or seldom seem in training dataset, there is no way
we can tell the hair color or cloth color of that character, so our model can not colorize that character
correctly.
6
Conclusion
In all, we experiment with GAN-based image coloring model and our proposed conditional CycleGAN
image coloring model on three different image domains: the in-door bedroom image dataset, the
human face dataset, and a comic dataset. The result shows that the conditional CycleGAN produces
more plausible images on human face coloring, and also do well in comic coloring.
References
[1] Stephen Koo. Automatic Colorization with Deep Convolutional Generative Adversarial Networks. Techni-
cal report, 2016.
[2] Kamyar Nazeri, Eric Ng, and Mehran Ebrahimi. Image colorization using generative adversarial networks.
Technical report, 2018.
7
[3] Sergio Guadarrama, Ryan Dahl, David Bieber, Jonathon Shlens, Mohammad Norouzi, and Kevin Murphy.
PixColor: Pixel Recursive Colorization. pages 1–17, 2019.
[4] Yun Cao, Zhiming Zhou, Weinan Zhang, and Yong Yu. Unsupervised Diverse Colorization via Generative
Adversarial Networks. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial
Intelligence and Lecture Notes in Bioinformatics), 10534 LNAI:151–166, 2017.
[5] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. LSUN:
Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop. 2015.
[6] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Nets. Technical report, 2014.
[7] Aditya Deshpande, Jiajun Lu, Mao Chuang Yeh, Min Jin Chong, and David Forsyth. Learning diverse
image colorization. Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2017, 2017-Janua:2877–2885, 2017.
[8] Wenping Wang et al. Sentiment analysis: A systematic case study with yelp scores. advances in artificial
intelligence and machine learning. 2023; 3 (3): 74, 2023.
[9] Tianxiao Hu, Hao Zheng, Chen Liang, Sirou Zhu, Natalie Imirzian, Yizhe Zhang, Chaoli Wang, David P
Hughes, and Danny Z Chen. Antvis: A web-based visual analytics tool for exploring ant movement data.
Visual Informatics, 4(1):58–70, 2020.
[10] Keyi Yu, Yu Wang, Sihan Zeng, Chen Liang, Xiaoyu Bai, Dachi Chen, and Wenping Wang. Inkgan:
Generative adversarial networks for ink-and-wash style transfer of photographs. 2023.
[11] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. 2017.
[12] J. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent
adversarial networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 2242–
2251, Oct 2017.
8
","The paper reviews several approaches for generating diverse image colorizations, including Conditional Random Field (CRF), Generative Adversarial Network (GAN), and modified Convolutional Neural Networks (CNNs). It identifies the state-of-the-art network by Cao et. al as the baseline model and outlines the limitations of existing methods, which motivated the development of the proposed CycleGAN model.nan"
"Recently, vision-language representation learning has been making significant progress in constructing foundational medical models. However, due to the complexity and redundancy of radiology reports, the current models face challenges in capturing key semantic information. To address this issue and enhance the vision-language foundation model, this study proposes a novel iterative learning framework with a key semantic knowledge-emphasized report refinement method. Raw radiology reports are first refined by a constructed clinical dictionary and two model-optimized knowledge-enhancement metrics, guiding the model to learn from general patient conditions to finer, critical details. The approach's effectiveness is validated in multiple downstream medical image analysis tasks, including disease classification, region-of-interest segmentation, and phrase grounding. The proposed framework surpasses seven state-of-the-art methods in both fine-tuning and zero-shot settings, showcasing its potential for different clinical applications.","In modern clinical practice, medical imaging plays a critical role in disease detection, progression monitoring, and prognosis evaluation. However, the exponential growth of imaging data poses a significant burden to radiologists, affecting the efficiency of clinical workflows. To tackle this issue, artificial intelligence, particularly deep learning, has emerged as a revolutionary technique, automating medical image analysis and aiding in clinical decision-making. Nonetheless, manually annotating large datasets, a necessity for training deep learning models for each task, is time-consuming and requires domain expertise. This has led to the urgent need for effective medical foundation models capable of handling various downstream tasks without extensive labeled data collection.","To enhance the medical vision-language foundation model, this study proposes a novel iterative learning framework featuring a key semantic knowledge-emphasized report refinement method. The framework begins by constructing a simple yet effective clinical dictionary to link keywords in raw radiology reports with medical knowledge-supplemented sentences, guiding the refinement process. Two model-optimized knowledge-enhancement metrics are then formulated to measure the similarity and matching degree between refined reports and their associated images. In the iterative learning framework's first stage, raw radiology reports are utilized as the primary information source, enabling the model to gain a general understanding of the patient's condition. In the second stage, refined reports are employed to further fine-tune the model, directing its attention towards crucial information, thus enhancing its performance in fine-grained downstream analysis tasks.","The effectiveness of the proposed framework is extensively validated through downstream medical image analysis experiments, encompassing disease classification, region-of-interest segmentation, and phrase grounding. The results consistently demonstrate the superiority of the proposed framework over seven state-of-the-art methods in both fine-tuning and zero-shot settings, highlighting its data efficiency and robustness.","The proposed key semantic knowledge-emphasized report refinement method effectively refines complex radiology reports, emphasizing crucial information. By leveraging these refined reports, the iterative vision-language representation learning framework can effectively utilize knowledge within radiology reports, facilitating the learning of meaningful medical image representations for various downstream medical image analysis tasks. The consistent improvement in performance across different tasks underscores the framework's potential as a valuable medical foundation model for diverse clinical applications.",Enhancing the vision-language foundation model with key semantic knowledge-emphasized report refinement,"Cheng Li, Weijian Huang, Hao Yang, Jiarun Liu, Shanshan Wang","Enhancing the vision-language foundation model with key 
semantic knowledge-emphasized report refinement 
 
Cheng Lia,1, Weijian Huanga,b,c,1, Hao Yang a,b,c, Jiarun Liu a,b,c, Shanshan Wang a,c,* 
 
a Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institute of 
Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China 
b University of Chinese Academy of Sciences, Beijing 100049, China 
c Peng Cheng Laboratory, Shenzhen 518066, China 
 
1. These authors contributed equally to this work. 
* Corresponding author: Shanshan Wang (ss.wang@siat.ac.cn) 
 
Abstract 
Recently, vision-language representation learning has made remarkable advancements 
in building up medical foundation models, holding immense potential for transforming 
the landscape of clinical research and medical care. The underlying hypothesis is that 
the rich knowledge embedded in radiology reports can effectively assist and guide the 
learning process, reducing the need for additional labels. However, these reports tend 
to be complex and sometimes even consist of redundant descriptions that make the 
representation learning too challenging to capture the key semantic information. This 
paper develops a novel iterative vision-language representation learning framework by 
proposing a key semantic knowledge-emphasized report refinement method. 
Particularly, raw radiology reports are refined to highlight the key information 
according to a constructed clinical dictionary and two model-optimized knowledge-
enhancement metrics. The iterative framework is designed to progressively learn, 
starting from gaining a general understanding of the patient's condition based on raw 
reports and gradually refines and extracts critical information essential to the fine-
grained analysis tasks. The effectiveness of the proposed framework is validated on 
various downstream medical image analysis tasks, including disease classification, 
region-of-interest segmentation, and phrase grounding. Our framework surpasses seven 
state-of-the-art methods in both fine-tuning and zero-shot settings, demonstrating its 
encouraging potential for different clinical applications. 
 
Keywords: 
Vision-language representation learning, Medical foundation models, Knowledge-
emphasized report refinement, Iterative learning 
 
1. Introduction 
In modern clinical practice, medical imaging plays a crucial role in the detection, 
monitoring of progression, and evaluation of treatment prognosis for various diseases 
[1]–[3]. However, the exponential growth of imaging data poses a significant burden to 
radiologists, impacting the efficiency of clinical workflows. To tackle this issue, 
artificial intelligence, particularly deep learning, has emerged as a revolutionary 
technique, automating medical image analysis and aiding in clinical decision-making 
[4]–[7]. Nevertheless, manually annotating large datasets necessary to train respective 
deep learning models for each task is time-consuming and requires the expertise of 
domain specialists [8], [9]. As a result, there is an urgent need to develop effective 
medical foundation models that can handle various downstream tasks without relying 
on collecting large-scale labeled datasets [10], [11]. 
 
One promising and natural solution is to leverage the valuable information embedded 
within radiology reports [11], which are routinely collected in clinical practice. These 
reports contain rich domain knowledge that can effectively assist and guide image 
representation learning, thereby reducing the need for costly manual labels. A 
straightforward method in this direction involves extracting supervision signals directly 
from the reports. Various techniques, such as natural language processing (NLP) 
techniques and rule-based labelers, have been proposed for this purpose [12], [13]. 
However, these labeling techniques often rely on fixed lexicons and manually 
engineered rules, making it difficult to adapt them to new scenarios. Another successful 
avenue is the application of implicit supervision through vision-language representation 
learning, which has demonstrated great success in natural image recognition tasks [14]–
[16]. Nevertheless, the transfer of this technique to the medical domain faces many 
challenges [17]. One of the major obstacles is the presence of complex and sometimes 
redundant medical entity descriptions within radiology reports, which can pose 
significant difficulties for effective representation learning [18], [19].   
 
In this study, our primary objective is to enhance the medical vision-language 
foundation model by proposing a key semantic knowledge-emphasized report 
refinement method. Incorporating the proposed report refinement method, we develop 
a novel iterative vision-language representation learning framework. On the one hand, 
to refine the reports, we construct a simple yet effective clinical dictionary to link 
keywords in raw radiology reports with medical knowledge-supplemented sentences. 
Then, two model-optimized knowledge-enhancement metrics are constructed to guide 
the report refinement process using the medical knowledge-supplemented sentences 
such that the key information relevant to fine-grained downstream analysis tasks is 
effectively highlighted. On the other hand, our iterative framework enables the model 
to progressively learn the intricate medical information contained in radiology reports 
(Fig. 1). In the first iteration, we utilize the raw radiology reports as the initial source 
of information to gain a general understanding of the patient's condition, as provided 
by the radiologists. This step serves as a preliminary knowledge extraction process, 
obtaining the model to calculate the two model-optimized knowledge-enhancement 
metrics. In the second stage, we employ refined reports to further fine-tune the model, 
directing its attention towards crucial information. The effectiveness of the proposed 
framework is validated by extensive downstream medical image analysis experiments, 
including disease classification, region-of-interest segmentation, and phrase grounding. 
The results demonstrate that our framework surpasses state-of-the-art vision-language 
representation learning methods in both fine-tuning and zero-shot settings. 
 
Our main contributions can be summarized as follows:  
 
We develop a novel iterative vision-language representation learning framework, 
which is designed to firstly gain a general understanding of the patient's condition 
from the raw radiology reports and then extract critical information by refining 
reports to capture the essential fine-grained features from the images.  
 
We propose a key semantic knowledge-emphasized report refinement method. 
Under the guidance of a specially constructed clinical dictionary and two model-
optimized knowledge-enhancement metrics, the reports are refined to highlight 
crucial information essential to fine-grained downstream image analysis tasks. 
 
Extensive experimental validations were conducted on multiple external datasets, 
covering various medical image analysis tasks. The results demonstrate that our 
proposed framework outperforms recent state-of-the-art vision-language 
representation methods in both fine-tuning and zero-shot settings, showcasing its 
effectiveness and robustness. 
 
2. Related Work 
In this section, we provide a concise overview of recent research that focuses on 
utilizing information from radiology reports for medical image representation learning. 
We categorize these works into two main groups based on the strategies they employ: 
 
Figure 1. Our proposed iterative vision-language representation learning 
framework. In the first iteration, the raw radiology reports are leveraged to gain a 
general understanding of the patient's condition. In the second stage, the refined 
reports are employed to further fine-tune the model, guiding the model towards 
capturing crucial information. 
 
those that use explicit supervision signals extracted from radiology reports and those 
that 
incorporate 
implicit 
supervision 
through 
multi-modal 
vision-language 
representation learning. 
 
2.1 Report-supervised medical image representation learning 
Utilizing explicit supervision signals extracted from radiology reports to supervise the 
learning of medical imaging models is an intuitive and straightforward approach, 
particularly in scenarios where manual labels are not readily available. Wang et al. 
demonstrated the feasibility of this approach by constructing a chest X-ray dataset 
(ChestX-ray8) at a hospital scale [12]. They employed NLP tools to search for the 
presence of 8 common thoracic pathology keywords in corresponding radiology reports 
and developed specific rules to remove negation and uncertainty. Then, they built a 
weakly supervised classification and localization framework using this dataset, 
validating the effectiveness of the automatically generated labels for these two 
important medical image analysis tasks. Another notable work in this area is by Irvin et 
al. [13], who developed a rule-based labeler to extract structured labels for images from 
free-text radiology reports. Their efforts resulted in the construction of the well-known 
dataset, CheXpert. Leveraging the extracted labels, they trained convolutional neural 
networks using different uncertainty approaches to classify 14 observations, and their 
best model achieved higher performance than 3 additional radiologists on detecting 3 
out of 5 selected pathologies, cardiomegaly, edema, and pleural effusion, validating the 
effectiveness of the generated labels for the detection of common chest radiographic 
observations. 
 
Despite the effectiveness of these report-based automatic labeling approaches on 
individual datasets and specific tasks, there are two main limitations that hinder their 
widespread adoption. One limitation is that the relevant labeling rules are manually 
crafted. This manual process can introduce inaccuracies, resulting in the generation of 
incorrect labels [17]. Moreover, these labeling rules are often designed to capture a 
limited set of clinical observations mentioned in the reports, which can potentially 
overlook important information contained within the reports [17]. Another limitation 
of the approach is that the labeling rules are domain- and style-specific, and they rely 
on fixed lexicons. Consequently, the effectiveness of the developed labeling techniques 
may not generalize well to new scenarios or different datasets [20]. 
 
2.2 Medical vision-language representation learning 
Different from the approach of directly extracting supervision signals from radiology 
reports, medical vision-language representation learning leverages implicit supervision 
from the reports by simultaneously learning multi-modal representations. There are two 
main categories of methods in this area: those employing masked autoencoders [20], 
[21] and those employing contrastive learning techniques [17], [22]–[25]. Methods 
using masked autoencoders aim to learn vision-language representations by restoring 
the original images and reports. For example, Chen et al. focuses on learning joint 
vision-language representations, which can be applied to various downstream tasks 
such as visual question answering, image-text classification, and image-caption 
retrieval [21]. Zhou et al. targets the learning of radiolographic representations for 
disease diagnosis [20]. These methods based on masked autoencoders typically require 
fine-tuning for evaluation due to the discrepancy between the pre-text restoration task 
and the downstream medical image recognition tasks. They may lack zero-shot 
capability. Contrastive learning-based methods, on the other hand, learn vision-
language representations by aligning the distributions of multi-modal features. These 
methods can be employed for medical image recognition tasks in both fine-tuning and 
zero-shot settings. Among these works, Zhang et al. introduced the pioneering 
framework ConVIRT [17], which learns medical image representations from paired 
images and reports by employing a bidirectional contrastive objective. Similarly, Tiu et 
al. proposed CheXzero, which achieved expert-level detection of pathologies without 
fine-tuning using labeled data [25]. To capture localized features and fine-grained 
semantics in medical images, Huang et al. proposed GLoRIA, a multimodal global-
local representation learning framework [23]. Zhou et al. introduced REFERS, which 
performs report generation in addition to multi-modal contrastive learning to facilitate 
the learning of well-transferable image representations [24]. To simplify the training of 
medical vision-language representation models, Liu et al. proposed M-FLAG, which 
trains only the vision model while freezing the language model [22]. 
 
While medical vision-language representation learning has shown promising results, 
there are still challenges to overcome. One major obstacle is the presence of complex 
and sometimes even redundant medical entity descriptions within radiology reports. To 
address this issue, researchers have explored different approaches. For example, 
Boecking et al. trained a radiology-specific text encoder, called CXR-BERT, in their 
BioViL model to better handle radiology reports [26]. Additionally, Wu et al. and Zhang 
et al. proposed methods to simplify radiology reports by extracting medical-related 
information before inputting them into the text encoders [18], [19]. The primary 
objective of our study is to refine the radiology reports by emphasizing key semantic 
knowledge to enhance vision-language representation learning. Different from the 
approaches of Wu et al. and Zhang et al., which employ specific modules to extract 
medical entities from the reports, we propose a dictionary and model-dependent 
radiology report refinement method. Specifically, we develop an iterative vision-
language representation learning framework. In the first iteration, we train a high-
capacity vision-language representation learning model using the images and raw 
radiology reports. Then, we construct a clinical dictionary, which links keywords in raw 
radiology reports with medical knowledge-supplemented sentences. With the trained 
image and text encoders, we calculate two model-optimized knowledge-enhancement 
metrics to guide the report refinement process. In the second iteration, we replace the 
raw radiology reports with the refined versions to fine-tune the vision-language 
representation learning model. Importantly, we do not design specific modules to 
simplify the reports. Instead, we rely on the dictionary and the model itself to identify 
and focus on the important information. By accessing both the raw radiology reports 
and the refined versions, our method avoids the loss of critical details while making full 
use of the reports, benefiting from the hints provided by the refined knowledge. 
 
3. Method 
The primary objective of this study is to refine the radiology reports to enhance vision-
language representation learning via a novel iterative learning framework. Here, we 
first describe the iterative vision-language representation learning framework in Sec. 
3.1. Our main contribution of knowledge refinement for radiology reports is presented 
in Sec. 3.2. 
 
3.1 Iterative vision-language representation learning 
Our proposed framework involves training a multi-scale contrastive learning model in 
two iterations. In the first iteration, the model takes images and raw radiology reports 
as inputs, aiming to gain a general understanding of the patient's condition. Then, the 
radiology reports are refined using our designed key semantic knowledge-emphasized 
refinement method. It is important to note that this refinement method utilizes the model 
trained in the first iteration. In the second iteration, the inputs to the model are changed 
to images and refined reports. This iteration focuses on further refining the model to 
capture the essential fine-grained features from the images, which are crucial for 
downstream analysis tasks. The details of the multi-scale contrastive learning model 
(Fig. 2(a)) are described in this section, while the report refinement method (Fig. 2(b)) 
will be elaborated in Sec. 3.2. 
 
3.1.1 Multi-scale contrastive learning 
In this section, we introduce the multi-scale contrastive learning model, a simple but 
effective vision-language model consisting of three components, including the image 
encoder Ev with the image projection block Pv, text encoder Et with the corresponding 
text projection block Pt, and fusion module (Fig. 2(a)). 
 
We employ a vision transformer as our image encoder [27], which can learn complex 
spatial relationships and capture long-range dependencies in the images. It extracts a 
number of image features corresponding to different input image patches 𝑟𝑣̃ =
𝐸𝑣(𝑋𝑣) ∈ ℝ𝐾𝑣×𝐷𝑣, where Kv indicates Kv image patches and Dv is the dimension of the 
extracted image features. The projection block Pv projects the features to a joint space 
of C dimensions, and we get the local image representations 𝑟𝑣 = 𝑃𝑣(𝑟𝑣̃) ∈ ℝ𝐾𝑣×𝐶. 
Then, average pooling is applied to 𝑟𝑣 , which generates the global image 
representations 𝑅𝑣 ∈ ℝ𝐶. For each radiology report, we first divide it into multiple 
sentences. Then, a wordpiece tokenizer is utilized to convert the processed reports into 
a sequence of tokens that can be analyzed by the text encoder Et. Here, we adopt BERT 
[28] as our Et. It generates the corresponding sentence representations as 𝑟𝑡̃ = 𝐸𝑡(𝑋𝑡) ∈
ℝ𝐾𝑡×𝐷𝑡, where Kt indicates Kt sentences and Dt is the dimension of the extracted text 
features. Similarly, Pt projects 𝑟𝑡̃ to the same joint space of C dimensions as the image 
branch, and the local text representations are 𝑟𝑡 = 𝑃𝑡(𝑟𝑡̃) ∈ ℝ𝐾𝑡×𝐶. The global text 
representations are generated by aggregating the local representations 𝑅𝑡 = ∑
𝑟𝑡𝑖
𝐾𝑡
𝑖=0
∈
ℝ𝐶. 
 
A symmetric contrastive loss for the global alignment between the image and text 
representations is calculated: 
𝐿𝑔 = − 1
𝑁 ∑(𝑙𝑜𝑔
exp⁡(< 𝑅𝑣𝑖, 𝑅𝑡𝑖 >/𝜏1)
∑
exp⁡(< 𝑅𝑣𝑖, 𝑅𝑡𝑗 >/𝜏1)
𝑁
𝑗=1
+ 𝑙𝑜𝑔
exp⁡(< 𝑅𝑡𝑖, 𝑅𝑣𝑖 >/𝜏1)
∑
exp⁡(< 𝑅𝑡𝑖, 𝑅𝑣𝑗 >/𝜏1)
𝑁
𝑗=1
)
𝑁
𝑖=1
 
< 𝑅𝑣𝑖, 𝑅𝑡𝑖 > indicates the cosine similarity between the two global features 𝑅𝑣𝑖 and 
𝑅𝑡𝑖. N is the batch size. 𝜏1 is a temperature parameter. 
 
To promote the local alignment between the image and text representations, we follow 
the practice reported in GLoRIA [23]. Particularly, a similarity matrix (s) is obtained 
by computing the dot-product of the local text and image representations, 𝑠 = 𝑟𝑡 ∙ 𝑟𝑣
𝑇 ∈
ℝ𝐾𝑡×𝐾𝑣, which reflects the similarity between 𝐾𝑡 sentences and 𝐾𝑣 image patches. 
Then, it is normalized, 𝑆𝑖,𝑗 =
exp⁡(𝑠𝑖,𝑗/𝜏2)
∑
exp⁡(𝑠𝑖,𝑘/𝜏2)
𝐾𝑣
𝑘=1
, where 𝜏2  is a another temperature 
parameter. With S, we obtain the context-enhanced image representations to the given 
sentence 𝑟𝑣𝑖
̂ = ∑
𝑆𝑖,𝑗𝑟𝑣,𝑗
𝐾𝑣
𝑗=0
. Finally, we aggregate the similarities between all 𝐾𝑡 
sentences and their corresponding enhanced image representations by a matching 
 
Figure 2. The two major components of our framework. (a) The vision-language 
representation learning model with image-text matching determination capability. 
(b) The key semantic knowledge-emphasized report refinement method. 
 
function M, M(𝑋𝑣, 𝑋𝑡) = log⁡(∑
exp⁡(< 𝑟𝑣𝑖
̂, 𝑟𝑡𝑖 >/𝜏3)
𝐾𝑡
𝑖=1
)𝜏3, where 𝜏3 is a temperature 
parameter. Similar to the global alignment, a symmetric local contrastive loss is defined 
as: 
𝐿𝑙 = − 1
𝑁 ∑(𝑙𝑜𝑔
exp⁡(𝑀(𝑋𝑣𝑖, 𝑋𝑡𝑖)/𝜏2)
∑
exp⁡(𝑀(𝑋𝑣𝑖, 𝑋𝑡𝑗)/𝜏2)
𝑁
𝑗=1
+ 𝑙𝑜𝑔
exp⁡(𝑀(𝑋𝑣𝑖, 𝑋𝑡𝑖)/𝜏2)
∑
exp⁡(𝑀(𝑋𝑣𝑗, 𝑋𝑡𝑖)/𝜏2)
𝑁
𝑗=1
)
𝑁
𝑖=1
 
 
3.1.2 Image-text matching determination 
Image-Text Matching (ITM) is a binary classification task in which the model utilizes 
a language decoder with a linear layer to predict whether the input image-text pairs are 
positive (matching) or negative (non-matching). ITM has been proposed to capture 
fine-grained alignment between visual and textual information [29]. In this paper, we 
introduce an enhanced application for determining the correctness of sentences 
generated from a dictionary. We employ a BERT base decoder, as described in [30], but 
do not utilize the hard-sample strategy due to the frequent presence of text with similar 
semantics in medical reports. The application of this module will be elaborated in detail 
in Sec. 3.2. 
 
We employ a straightforward binary cross-entropy loss for training the ITM module, 
denoted as Litm. Therefore, in combination with the masked language model loss Lmlm 
utilized in Et, the final loss used for training the multi-scale contrastive learning model 
is: 
L = 𝐿𝑔 + 𝐿𝑙 + 𝐿𝑖𝑡𝑚 + 𝐿𝑚𝑙 
 
3.2 Key semantic knowledge-emphasized report refinement 
Following the preparation outlined in Sec. 3.1, we have a pre-trained model with image-
text matching determination capabilities ready to extract preliminary visual-language 
features. In this section, we focus on report refinement for fine-tuning the pre-trained 
model, as depicted in Fig. 2(b). 
 
To facilitate report refinement, we begin by constructing a clinical dictionary. This 
dictionary is utilized to extract keywords from complex sentences, aiding in semantic 
confirmation. It links specific regions with keywords relevant to various diseases. For 
example, the report with keywords “heart” or “cardiac” is highly linked to the disease 
“cardiomegaly”. Once a medical sentence is classified as related to a particular disease 
type, we provide both positive and negative descriptions concerning that disease. A 
brief description of “cardiomegaly” could be: “There is (no) cardiomegaly”. We can 
also employ more specialized descriptions to improve the reliability of sentence 
selection, such as the positive sentence generated by GPT-4: “The cardiac silhouette is 
enlarged, consistent with cardiomegaly, suggesting underlying cardiac enlargement or 
pathology.” In the supplementary file, we provide the details table of the constructed 
1. https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation 
clinical dictionary (Supplementary Table S1). Customized statements can lead to the 
generation of more diverse and robust sentences. 
 
Next, we input the image, along with all the sentences, including the original sentence 
and the supplemented ones, into the trained contrastive model and ITM decoder. The 
outputs are then utilized to assess whether the supplemented sentences accurately 
describe the patient's condition. We employ two metrics for this assessment: 1) A 
similarity metric is determined based on the pre-trained contrastive model, and 2) A 
matching metric is obtained using the trained ITM decoder with a binary linear classifier. 
 
By combining these two metrics, we select the matched sentence with the highest 
similarity rank to supplement the original sentence. An example is illustrated in Fig. 
2(b). The original sentence “It appears more amorphous and a small focus of infection 
is not excluded.” would be supplemented with “There is pneumonia”. Please note that 
although we provide professionally generated sentences from GPT-4 in the dictionary, 
we still employ simpler sentence patterns (e.g., “There is pneumonia”) for 
supplementation to enhance the model's comprehension. 
 
Finally, we utilize the refined reports enriched with newly acquired knowledge for fine-
tuning the model, directing the model's attention towards crucial information. 
 
4. Experiments and Results 
4.1 Datasets 
For model training, we utilized the X-ray images and corresponding radiology reports 
from the MIMIC-CXR V2 dataset [31]. The model evaluation involved four tasks: fine-
tuning disease classification, fine-tuning region-of-interest segmentation, zero-shot 
disease classification, and zero-shot phrase grounding. To perform disease 
classification, we incorporated four X-ray datasets, CheXpert [13], NIH ChestX-ray 
[12], RSNA Pneumonia [32], and SIIM-ACR Pneumothorax1. For fine-tuning 
segmentation, we adopted the SIIM-ACR Pneumothorax dataset. For zero-shot phrase 
grounding, we utilized the MS-CXR dataset [26]. 
 
MIMIC-CXR V2 is a large dataset comprising 377,110 chest X-rays obtained from 
227,827 imaging studies [31]. These studies were conducted at the Beth Israel 
Deaconess Medical Center between 2011 and 2016. Each image is accompanied by a 
corresponding free-text radiology report. In our study, we utilized all the available data 
from this dataset for model training. 
 
CheXpert consists of 224,316 chest X-rays obtained from 65,240 patients. Following 
the official practice outlined by [13], we report the classification results for five selected 
pathologies, including atelectasis, cardiomegaly, consolidation, edema, and pleural 
 
effusion. Since the official test set is not publicly available, we adopted the approach 
from a previous study [17] and employed the official validation set as our test set. 
Additionally, we randomly sampled 5,000 samples from the official training set to 
construct our validation set, similar to [20]. For the fine-tuning evaluation, our training, 
validation, and testing sets contain 218,414, 5,000, and 234 images, respectively. For 
the zero-shot evaluation, only the 234 testing images were employed. 
 
NIH ChestX-ray offers 112,120 chest X-rays for the classification of 14 pathologies 
[12]. Similarly, for fine-tuning evaluation, we divided the dataset into training, 
validation, and testing sets, following a split ratio of 7:2:1. 
 
RSNA Pneumonia provides data for the binary classification task of pneumonia vs. 
normal. In accordance with the official configuration, the training, validation, and 
testing sets consist of 25,184, 1,500, and 3,000 images, respectively [32]. 
 
SIIM-ACR Pneumothorax is a dataset that consists of more than 120,000 chest X-rays, 
with each image being accompanied by precise manual segmentation masks of the 
pneumothorax regions. Following [23], we divided the dataset into three subsets: 70% 
for training, 15% for validation, and 15% for testing. 
 
MS-CXR provides bounding box annotations along with paired sentences that describe 
the clinical findings [26]. The dataset includes a total of 1,162 annotations of 881 cases, 
and we utilized all of them for the evaluation. 
 
4.2 Comparison methods 
We compared our framework with various existing state-of-the-art vision-language 
representation learning methods to validate its effectiveness. These methods include 
ConVIRT [17], GLoRIA [23], BioViL [26], M3AE [21], REFERS [24], MGCA [33], 
CheXzero [25], MedKLIP [18], M-FLAG [22], and Med-UniC [34]. The specific 
contributions of most of these methods to the field have been discussed in Sec. 2. It 
should be noted that we did not try to re-implement these existing methods. Instead, we 
compared our results with the reported results in the original papers or other published 
papers directly.  
 
For the evaluation of fine-tuning disease classification, we compared our method with 
ConVIRT, GLoRIA, BioViL, M3AE, REFERS, MedKLIP, and M-FLAG. For the 
evaluation of fine-tuning pneumothorax region segmentation, we compared our method 
with ConVIRT, GLoRIA, MGCA, M-FLAG, and Med-UniC. For the evaluation of 
zero-shot disease classification, we compared our method with ConVIRT, GLoRIA, 
BioViL, and CheXzero. For the zero-shot phrase grounding evaluation, we compared 
our method with ConVIRT, GLoRIA, and BioViL. 
 
 
4.3 Implementation details 
In our framework, we adopted the widely used ViT-B/16 as our image encoder and 
BERT with a width of 768 as our text encoder and ITM decoder. The mask ratio used 
in BERT was set to 0.15. We set the batch size to 128, with 50 epochs for initializing 
the model (the first iteration) and 10 for the fine-tuning process (the second iteration). 
We utilized the AdamW optimizer with an initial learning rate of 1.5e-4, a weight decay 
of 0.05, 1=0.9, and 2=0.95. All experiments were implemented using PyTorch, and 
we used 4 NVIDIA A100 GPUs in parallel. 
 
For the fine-tuning disease classification evaluation, we utilized the SGD optimizer 
with a momentum of 0.9. The best learning rate was searched in the range of 8e-3 to 
1e-4 to achieve optimal validation performance. For the fine-tuning segmentation task, 
we utilized the AdamW optimizer with the learning rate of 5e-6, 2e-5, and 2e-5 for 1%, 
10%, and 100% labeling ratios. 
 
4.4 Evaluation metrics 
For disease classification, we evaluated the performance using the area under the curve 
(AUC) score. We calculated the average AUC scores for the respective datasets, as well 
as disease-level scores whenever possible. For region-of-interest segmentation, we 
employed the Dice similarity coefficient (Dice). For phrase grounding, we assessed the 
results based on the Intersection over Union (IoU) score and the contrast-to-noise ratio 
(CNR) value. IoU measures the overlap between the generated saliency maps and the 
ground-truth segmentation labels. CNR evaluates the contrast difference inside and 
outside the bounding box. 
 
4.5 Results for fine-tuning disease classification 
In this section, we present the disease classification results of different methods in fine-
tuning settings. Three datasets (CheXpert, NIH ChestX-ray, and RSNA Pneumonia) 
were evaluated. For each dataset, we utilized three percentages of labeled data (1%, 
10%, and 100%) during the fine-tuning process to investigate the influence of the fine-
tuning sample number on the classification performance. 
 
Table 1 lists the AUC scores of different methods. Overall, our proposed framework 
outperforms the seven comparison methods under all experimental settings, 
demonstrating its effectiveness for this task of fine-tuning disease classification. 
Additionally, consistent trends can be observed across all methods, with classification 
for the NIH ChestX-ray dataset being the most challenging, followed by CheXpert and 
RSNA Pneumonia. Apart from achieving the highest scores on all three datasets, our 
proposed framework exhibits more significant improvements on the most difficult 
 
dataset, NIH ChestX-ray. For example, when utilizing 10% of the training samples, our 
method increases the AUC scores by 2.6%, 0.2%, and 0.6% on NIH ChestX-ray, 
CheXpert, and RSNA Pneumonia, respectively. This observation indicates that our 
proposed framework can better handle difficult samples compared to the comparison 
methods. 
 
Table 1. Fine-tuning disease classification results (AUC: %), on three datasets using 
different ratios of labeled samples for fine-tuning. “-” indicates no results have been 
reported in existing literature for the corresponding experimental setting. 
 
 
Furthermore, it is inspiring to note that the performance improvement of our proposed 
framework is more pronounced when employing fewer fine-tuning samples. 
Specifically, compared to the respective best-performing comparison methods, our 
proposed framework increases the AUC score by 0.8% when utilizing 1% of the 
training samples on CheXpert. However, this improvement decreases to 0.2% when 
utilizing 10% and 100% of the training samples. Similarly, our proposed framework 
increases the AUC scores by 1.4% and 1.6% when utilizing 1% of the training samples 
on NIH ChestX-ray and RSNA Pneumonia, respectively. Both values are smaller when 
100% training samples are introduced. This phenomenon reflects the data efficiency of 
our proposed framework compared to the seven state-of-the-art methods. With the 
designed iterative learning process and the report refinement method, our framework 
can better leverage useful information from limited data for the task of disease 
classification.  
 
In addition to the mean AUC scores, we also provide the classification results for the 
14 chest pathologies specific to the NIH ChestX-ray dataset (Table 2). Since most 
existing vision-language representation learning methods lack these disease-level 
results (except for REFERS), we included several image self-supervised methods 
(Model Genesis [35], C2L [36], Context Restoration [37], and TransVW [38]), as well 
as an ImageNet pre-trained model [12], for comparison. Across most diseases, our 
proposed framework consistently achieves the highest AUC scores, thereby validating 
its effectiveness in fine-grained disease classification. 
 
 
 
 
 
 
Table 2. Disease-level classification results (AUC: %) on the NIH ChestX-ray dataset 
using different ratios of labeled samples for fine-tuning. 
 
 
4.6 Results for fine-tuning pneumothorax region segmentation 
Region-of-interest segmentation is another important medical image analysis task that 
offers various clinical applications, such as disease progression evaluation and 
treatment planning. We evaluate the effectiveness of our proposed framework by 
applying it to segment the pneumothorax regions using the SIIM-ACR Pneumothorax 
dataset in the fine-tuning setting. Results are reported in Table 3. 
 
Table 3. Results (Dice: %) for pneumothorax region segmentation on the SIIM-ACR 
Pneumothorax dataset using different ratios of labeled samples for fine-tuning. 
 
 
 
Similar to the fine-tuning disease classification task, three different ratios of labeled 
samples were employed to fine-tune the model for pneumothorax region segmentation. 
Significant improvements in segmentation performance across all three ratios are 
obtained. Notably, as more labeled samples are introduced for fine-tuning, the 
segmentation performance displays a more pronounced enhancement. Specifically, 
compared to the respective best-performing comparison methods, the Dice similarity 
score is increased by 3.3%, 10.5%, and 23.9% at the labeling ratio of 1%, 10%, and 
100%, respectively. Interestingly, this trend differs from what we observed in the fine-
tuning disease classification task, where larger improvements were observed with fewer 
fine-tuning samples. One possible explanation for this discrepancy is that segmentation 
is a more challenging task than classification, demanding more localized 
representations. Our proposed framework learns strong visual representations, which 
can be further refined when provided with additional hints from the fine-tuning samples. 
Consequently, these refined representations enable better discrimination of the target 
regions from the surrounding background. Nevertheless, it is an intriguing observation, 
and we plan to investigate it further in our following studies. 
 
4.7 Results for zero-shot disease classification 
In this section, we present the zero-shot disease classification results on two datasets, 
RSNA Pneumonia and SIIM-ACR Pneumothorax. Table 4 lists the results obtained by 
different methods. Among the four comparison methods, different trends are observed 
in the AUC scores on the two datasets. On the RSNA Pneumonia dataset, GLoRIA 
achieves the lowest mean AUC score, while CheXzero obtains the highest score. On 
the SIIM-ACR Pneumothorax dataset, although GLoRIA consistently achieves the 
lowest classification results, the best score is given by BioViL instead of CheXzero. 
This discrepancy suggests that the two datasets may possess different data 
characteristics that affect the learning process. 
 
Table 4. Zero-shot disease classification results (AUC: %) on the RSNA Pneumonia 
and SIIM-ACR Pneumothorax datasets. 
 
 
Compared to the four existing state-of-the-art methods, our proposed framework 
consistently demonstrates enhanced performance on both datasets. Particularly, on the 
SIIM-ACR Pneumothorax dataset, our framework achieves a remarkable 16.4% 
increase in AUC score when compared to the best-performing comparison method, 
BioViL, thereby validating the effectiveness of the proposed report refinement method 
 
and iterative learning process. 
 
In addition, we notice that the improvement achieved by our framework in the zero-
shot setting is comparable to that in the fine-tuning setting. Referring back to Table 1, 
on the RSNA Pneumonia dataset, our framework increases the AUC scores by 2.9%, 
3.8%, and 3.2% at 1%, 10%, and 100% labeling ratios, respectively, compared to the 
scores achieved by BioViL. Here, in the zero-shot setting, our framework demonstrates 
a 3.2% increase in AUC, from 82.8% to 86.0%. This observation further validates the 
data-efficient capability of the proposed framework, as it effectively enhances the 
performance regardless of the quantity of the provided fine-tuning samples.  
 
Figure 3. Visualizations of phrase grounding with free text on the MS-CXR 
dataset. (1) to (6) represent six examples in the dataset. White color sentences are 
the provided free-text annotations. Dashed boxes indicate the annotations outlined 
by clinical experts. “Ours_Iter1” and “Ours_Iter2” represent the models trained 
after the first and second iterations in our framework, respectively. 
 
 
4.8 Results for zero-shot phrase grounding 
Phrase grounding serves as a powerful tool for improving the interpretability of deep 
learning models. We evaluated the zero-shot phrase grounding performance of different 
methods using the MS-CXR dataset (Table 5 and Fig. 3). 
 
The quantitative results in Table 5 demonstrate that our proposed framework can 
effectively enhance the zero-shot phrase grounding performance of the vision-language 
representation learning model, as evidenced by both evaluation metrics, CNR and mIoU. 
Although the increase in mIoU is relatively small, the improvement in CNR is 
noteworthy. Specifically, our framework achieves a CNR improvement of 0.546, 0.434, 
and 0.337 when compared to ConVIRT, GLoRIA, and BioViL, respectively. CNR, 
which measures the difference between the similarity values of inner and outer 
bounding box regions without relying on hard threshold values, is a more objective 
metric that could be more clinically relevant, particularly when heatmap visualizations 
are needed instead of discrete segmentation [26]. In other words, our method can be 
very helpful in clinical applications where heatmap visualizations can provide more 
informative insights. 
 
Table 5. Zero-shot phrase grounding results on the MS-CXR dataset. 
 
 
Example phrase grounding visualization results are plotted in Fig. 3. These qualitative 
visualization results further validate the effectiveness of our proposed framework in 
highlighting important regions for fine-grained analysis tasks. The two comparison 
methods tend to highlight irrelevant regions (GLoRIA in Fig. 3 example (3); BioViL in 
Fig. 3 examples (3) and (6)) or overlook important regions (GLoRIA in Fig. 3 examples 
(1), (2), and (5); BioViL in Fig. 3 examples (2) and (5)). In contrast, our proposed 
framework excels at precisely locating the crucial regions for analysis tasks across all 
examples. Moreover, we observe that while the model trained after the first iteration in 
our framework can already identify the important regions, the second iteration of model 
fine-tuning further enhances the highlighting of these regions for subsequent analysis, 
which is also indicated by the increased IoU and CNR scores. 
 
5. Conclusion and Discussion 
In this study, we developed a novel iterative vision-language representation framework 
that incorporates a key semantic knowledge-emphasized report refinement method. The 
primary objective of our work was to enhance the representation learning process by 
 
refining complex radiology reports. Extensive experiments were conducted on five 
external datasets, encompassing different medical image recognition tasks and 
evaluation settings, to investigate the effectiveness of the proposed framework. The 
results consistently demonstrated that our framework outperformed existing state-of-
the-art methods, showcasing its superiority and robustness. 
 
Despite the promising results, our framework still has the following limitations. Firstly, 
the performance of the report refinement method relies on the constructed clinical 
dictionary, which can eventually impact the performance of the optimized model in 
downstream tasks. Currently, the dictionary was constructed semi-automatically, 
involving traversing all the training data. Fortunately, as shown in the supplementary 
file (Supplementary Table S1), the dictionary is relatively simple and straightforward 
to construct. Nevertheless, we intend to automate the dictionary construction process in 
the future to further enhance the flexibility and performance of our framework. 
Secondly, as the two model-optimized knowledge-enhancement metrics (similarity 
metric and matching metric) rely on Stage 1 trained image and text encoders, our 
framework needs to be trained in two iterations. In future research, we will explore 
methods for integrating these different steps and training the framework end-to-end. 
 
In summary, our proposed key semantic knowledge-emphasized report refinement 
method effectively refined the complex radiology reports to highlight crucial 
information. Leveraging these refined reports, our iterative vision-language 
representation learning framework enables effective utilization of knowledge within 
radiology reports and facilitates the learning of meaningful medical image 
representations for various downstream medical image analysis tasks, including fine-
tuning disease classification, fine-tuning region-of-interest segmentation, zero-shot 
disease classification, and zero-shot phrase grounding. The consistent improvement in 
performance across different tasks highlights the potential of our framework as a 
valuable medical foundation model for diverse clinical applications. 
 
Acknowledgments 
This research was partly supported by the National Natural Science Foundation of 
China (62222118, U22A2040), Guangdong Provincial Key Laboratory of Artificial 
Intelligence in Medical Image Analysis and Application (2022B1212010011), and 
Shenzhen 
Science 
and 
Technology 
Program 
(JCYJ20220531100213029, 
RCYX20210706092104034), Key Laboratory for Magnetic Resonance and 
Multimodality Imaging of Guangdong Province (2023B1212060052). 
 
References 
[1] 
X. Mei, H. C. Lee, K. yue Diao, M. Huang, B. Lin, C. Liu, Z. Xie, Y. Ma, P. M. 
Robson, M. Chung, A. Bernheim, V. Mani, C. Calcagno, K. Li, S. Li, H. Shan, 
 
J. Lv, T. Zhao, J. Xia, Q. Long, S. Steinberger, A. Jacobi, T. Deyer, M. Luksza, 
F. Liu, B. P. Little, Z. A. Fayad, and Y. Yang, “Artificial intelligence–enabled 
rapid diagnosis of patients with COVID-19,” Nat. Med., vol. 26, no. 8, pp. 1224–
1228, 2020. 
[2] 
C. Huang, Z. Xu, Z. Shen, T. Luo, T. Li, D. Nissman, A. Nelson, Y. Golightly, 
M. Niethammer, and H. Zhu, “DADP: Dynamic abnormality detection and 
progression for longitudinal knee magnetic resonance images from the 
Osteoarthritis Initiative,” Med. Image Anal., vol. 77, p. 102343, 2022. 
[3] 
K. Swanson, E. Wu, A. Zhang, A. A. Alizadeh, and J. Zou, “From patterns to 
patients: Advances in clinical machine learning for cancer diagnosis, prognosis, 
and treatment,” Cell, vol. 186, no. 8, pp. 1772–1791, 2023. 
[4] 
W. Huang, H. Yang, X. Liu, C. Li, I. Zhang, R. Wang, H. Zheng, and S. Wang, 
“A coarse-to-fine deformable transformation framework for unsupervised multi-
contrast MR image registration with dual consistency constraint,” IEEE Trans. 
Med. Imaging, vol. 40, no. 10, pp. 2589–2599, 2021. 
[5] 
W. Wang, Q. Xia, Z. Yan, Z. Hu, Y. Chen, W. Zheng, X. Wang, S. Nie, D. 
Metaxas, and S. Zhang, “AVDNet: Joint coronary artery and vein segmentation 
with topological consistency,” Med. Image Anal., vol. 91, p. 102999, 2024. 
[6] 
R. Gu, G. Wang, J. Lu, J. Zhang, W. Lei, Y. Chen, W. Liao, S. Zhang, K. Li, D. 
N. Metaxas, and S. Zhang, “CDDSA: Contrastive domain disentanglement and 
style augmentation for generalizable medical image segmentation,” Med. Image 
Anal., vol. 89, no. 2023, p. 102904, 2023. 
[7] 
Y. Zhou, W. Huang, P. Dong, Y. Xia, and S. Wang, “D-UNet: A dimension-
fusion U shape network for chronic stroke lesion segmentation,” IEEE/ACM 
Trans. Comput. Biol. Bioinforma., vol. 18, no. 3, pp. 940–950, 2021. 
[8] 
S. Wang, C. Li, R. Wang, Z. Liu, M. Wang, H. Tan, Y. Wu, X. Liu, H. Sun, R. 
Yang, X. Liu, J. Chen, H. Zhou, I. Ben Ayed, and H. Zheng, “Annotation-
efficient deep learning for automatic medical image segmentation,” Nat. 
Commun., vol. 12, p. 5915, 2021. 
[9] 
Z. Xu, Y. Wang, D. Lu, X. Luo, J. Yan, Y. Zheng, and R. K. yu Tong, 
“Ambiguity-selective consistency regularization for mean-teacher semi-
supervised medical image segmentation,” Med. Image Anal., vol. 88, p. 102880, 
2023. 
[10] G. Wang, X. Luo, R. Gu, S. Yang, Y. Qu, S. Zhai, Q. Zhao, K. Li, and S. Zhang, 
“PyMIC: A deep learning toolkit for annotation-efficient medical image 
segmentation,” Comput. Methods Programs Biomed., vol. 231, p. 107398, 2023. 
[11] S. Zhang and D. Metaxas, “On the challenges and perspectives of foundation 
models for medical image analysis,” Med. Image Anal., vol. 91, no. May 2023, 
p. 102996, 2024. 
[12] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, “ChestX-ray8: 
Hospital-scale chest X-ray database and benchmarks on weakly-supervised 
classification and localization of common thorax diseases,” in IEEE Conference 
on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2097–2106. 
[13] J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund, B. 
 
Haghgoo, R. Ball, K. Shpanskaya, J. Seekins, D. A. Mong, S. S. Halabi, J. K. 
Sandberg, R. Jones, D. B. Larson, C. P. Langlotz, B. N. Patel, M. P. Lungren, 
and A. Y. Ng, “CheXpert: A large chest radiograph dataset with uncertainty 
labels and expert comparison,” 33rd AAAI Conf. Artif. Intell., pp. 590–597, 2019. 
[14] G. Kwon, Z. Cai, A. Ravichandran, E. Bas, R. Bhotika, and S. Soatto, “Masked 
vision and language modeling for multi-modal representation learning,” in 
International Conference on Learning Representations (ICLR), 2023. 
[15] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. 
Mohammed, S. Singhal, S. Som, and F. Wei, “Image as a foreign language: BEIT 
pretraining for vision and vision-language tasks,” in IEEE Conference on 
Computer Vision and Pattern Recognition (CVPR), 2023, pp. 19175–19186. 
[16] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. 
Goodman, A. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. 
Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. Thapliyal, J. Bradbury, W. 
Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan, C. Riquelme, A. Steiner, A. 
Angelova, X. Zhai, N. Houlsby, and R. Soricut, “PaLI: A jointly-scaled 
multilingual language-image model,” in International Conference on Learning 
Representations (ICLR), 2023. 
[17] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, “Contrastive 
learning of medical visual representations from paired images and text,” in 
Proceedings of Machine Learning Research, 2022, vol. 182, pp. 1–23. 
[18] C. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie, “MedKLIP: Medical 
knowledge enhanced language-image pre-training in radiology,” in IEEE/CVF 
International Conference on Computer Vision (ICCV), 2023, pp. 21372–21383. 
[19] X. Zhang, C. Wu, Y. Zhang, W. Xie, and Y. Wang, “Knowledge-enhanced 
visual-language pre-training on chest radiology images,” Nat. Commun., vol. 14, 
no. 1, p. 4542, 2023. 
[20] H.-Y. Zhou, C. Lian, L. Wang, and Y. Yu, “Advancing radiograph representation 
learning with masked record modeling,” in International Conference on 
Learning Representations (ICLR), 2023. 
[21] Z. Chen, Y. Du, J. Hu, Y. Liu, G. Li, X. Wan, and T. Chang, “Multi-modal 
masked autoencoders for medical vision-and-language pre-training,” in 
International Conference on Medical Image Computing and Computer-Assisted 
Intervention (MICCAI), 2022, pp. 679–689. 
[22] C. Liu, S. Cheng, C. Chen, M. Qiao, W. Zhang, A. Shah, W. Bai, and R. Arcucci, 
“M-FLAG: Medical vision-language pre-training with frozen language models 
and latent space geometry optimization,” in International Conference on 
Medical Image Computing and Computer- Assisted Intervention (MICCAI), 
2023, vol. 14220 LNCS, pp. 637–647. 
[23] S. C. Huang, L. Shen, M. P. Lungren, and S. Yeung, “GLoRIA: A multimodal 
global-local representation learning framework for label-efficient medical image 
recognition,” in Proceedings of the IEEE International Conference on Computer 
Vision (ICCV), 2021, pp. 3922–3931. 
[24] H.-Y. Zhou, X. Chen, Y. Zhang, R. Luo, L. Wang, and Y. Yu, “Generalized 
 
radiograph representation learning via cross-supervision between images and 
free-text radiology reports,” Nat. Mach. Intell., vol. 4, no. 1, pp. 32–40, 2022. 
[25] E. Tiu, E. Talius, P. Patel, C. P. Langlotz, A. Y. Ng, and P. Rajpurkar, “Expert-
level detection of pathologies from unannotated chest X-ray images via self-
supervised learning,” Nat. Biomed. Eng., vol. 6, no. 12, pp. 1399–1406, 2022. 
[26] B. Boecking, N. Usuyama, S. Bannur, D. C. Castro, A. Schwaighofer, S. Hyland, 
M. Wetscherek, T. Naumann, A. Nori, J. Alvarez-Valle, H. Poon, and O. Oktay, 
“Making the most of text semantics to improve biomedical vision-language 
processing,” in European Conferenceon Computer Vision (ECCV), 2022, pp. 1–
21. 
[27] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. 
Unterthiner, M. Dehghani, M. Minderer, G. Hiegold, S. Gelly, J. Uszkoreit, and 
N. Houlsby, “An image is worth 16x16 words: Transformers for image 
recognition at scale,” in International Conference on Learning Representations 
(ICLR), 2021. 
[28] J. Devlin, M.-W. Chang, K. Lee, K. T. Google, and A. I. Language, “BERT: Pre-
training of deep bidirectional transformers for language understanding,” in 
NAAC-HIT, 2019, pp. 4171–4186. 
[29] J. Li, R. R. Selvaraju, A. D. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi, “Align 
before fuse: Vision and language representation learning with momentum 
distillation,” in Advances in Neural Information Processing Systems (NeurIPS), 
2021. 
[30] J. Li, D. Li, C. Xiong, and S. Hoi, “BLIP: Bootstrapping Language-image pre-
training for unified vision-language understanding and generation,” in 
Proceedings of Machine Learning Research, 2022, vol. 162, pp. 12888–12900. 
[31] A. E. W. Johnson, T. J. Pollard, N. R. Greenbaum, M. P. Lungren, C. Deng, Y. 
Peng, Z. Lu, R. G. Mark, S. J. Berkowitz, and S. Horng, “MIMIC-CXR-JPG, a 
large publicly available database of labeled chest radiographs,” vol. 14, pp. 1–7, 
2019. 
[32] G. Shih, C. C. Wu, S. S. Halabi, M. D. Kohli, L. M. Prevedello, T. S. Cook, A. 
Sharma, J. K. Amorosa, V. Arteaga, M. Galperin-Aizenberg, R. R. Gill, M. C. 
B. Godoy, S. Hobbs, J. Jeudy, A. Laroia, P. N. Shah, D. Vummidi, K. 
Yaddanapudi, and A. Stein, “Augmenting the national institutes of health chest 
radiograph dataset with expert annotations of possible pneumonia,” Radiol. Artif. 
Intell., vol. 1, no. 1, 2019. 
[33] F. Wang, Y. Zhou, S. Wang, V. Vardhanabhuti, and L. Yu, “Multi-granularity 
cross-modal alignment for generalized medical visual representation learning,” 
in Advances in Neural Information Processing Systems (NeurIPS), 2022. 
[34] Z. Wan, C. Liu, M. Zhang, J. Fu, B. Wang, S. Cheng, L. Ma, C. Quilodrán-Casas, 
and R. Arcucci, “Med-UniC: Unifying cross-lingual medical vision-language 
pre-training by diminishing bias,” in Advances in Neural Information Processing 
Systems (NeurIPS), 2023. 
[35] Z. Zhou, V. Sodha, J. Pang, M. B. Gotway, and J. Liang, “Models Genesis,” Med. 
Image Anal., vol. 67, p. 101840, 2021. 
 
[36] H.-Y. Zhou, S. Yu, C. Bian, Y. Hu, K. Ma, and Y. Zheng, “Comparing to learn: 
Surpassing imagenet pretraining on radiographs by comparing image 
representations,” in International Conference on Medical Image Computing and 
Computer-Assisted Intervention (MICCAI), 2020, vol. 12261 LNCS, pp. 398–
407. 
[37] L. Chen, P. Bentley, K. Mori, K. Misawa, M. Fujiwara, and D. Rueckert, “Self-
supervised learning for medical image analysis using image context restoration,” 
Med. Image Anal., vol. 58, p. 101539, 2019. 
[38] F. Haghighi, M. R. H. Taher, Z. Zhou, M. B. Gotway, and J. Liang, “Transferable 
visual words: Exploiting the Ssemantics of anatomical patterns for self-
supervised learning,” IEEE Trans. Med. Imaging, vol. 40, no. 10, pp. 2857–2868, 
2021. 
 
 
","nanOne promising solution lies in leveraging the valuable information embedded within radiology reports, which are routinely collected in clinical practice. These reports contain rich domain knowledge, effectively assisting and guiding the image representation learning process, thereby reducing the need for costly manual labels. A straightforward method involves extracting supervision signals directly from the reports. Techniques such as natural language processing (NLP) and rule-based labelers have been proposed for this purpose. However, these labeling techniques often rely on fixed lexicons and manually engineered rules, hindering their adaptation to new scenarios. Another successful approach is the application of implicit supervision through vision-language representation learning, which has demonstrated great success in natural image recognition tasks. Nonetheless, transferring this technique to the medical domain faces many challenges, one of the major obstacles being the presence of complex and sometimes redundant medical entity descriptions within radiology reports, posing significant difficulties for effective representation learning."
"We propose an embedded hyperspectral band selection (EHBS) solution, which offers a pioneering approach for hyperspectral band selection that seamlessly integrates with the downstream task model. This method excels in selecting the best bands without the need for prior processing, effectively integrating with the downstream task model. The adaptation of the Stochastic Gates (STG) algorithm, originally designed for feature selection, for hyperspectral band selection in image semantic segmentation and the integration of a dynamic optimizer, DoG, eliminate the requirement for tuning the learning rate. We introduce a novel Area Under the Curve (AUC) metric for evaluating band selection methods across different target numbers of selected bands, which enables a comprehensive comparison of various methods. Experiments conducted on two distinct hyperspectral benchmark datasets showcase the superiority of EHBS in terms of its accuracy and ease of use compared to numerous common and state-of-the-art methods. Furthermore, the adaptability of EHBS to other tasks, especially those involving grouped features, opens up promising avenues for broader applications within the realm of deep learning, such as feature selection for feature groups. The demonstrated success on the tested datasets and the potential for application to a variety of tasks underscore the value of our method as a substantial addition to the field of computer vision.","Hyperspectral imaging involves capturing the complete optical spectrum at each point within an image. While a standard color camera records light intensity in just three colors (Red, Green, and Blue), a hyperspectral camera captures the entire wavelength range (typically consisting of several hundred bands) of light reflected from each scene point. The transition from color to full hyperspectral imaging provides a substantial increase in information, holding considerable potential across various applications such as medical imaging, agriculture, aerial photography, and autonomous driving. However, hyperspectral imaging also presents challenges, including high costs associated with physical sensor hardware, storage, transfer, and analysis of the considerably larger image data generated by HSI. The rise of deep learning models has further increased model complexity and computing costs. Hence, there is a pressing need for algorithms and methods for band selection, i.e., identifying a subset of hyperspectral bands that retains essential information for downstream tasks. This paper introduces Embedded Hyperspectral Band Selection (EHBS), a plug-and-play embedded method that effectively selects the optimal bands without requiring preliminary processing, seamlessly integrating with the downstream task model. EHBS utilizes an existing feature selection algorithm based on stochastic gates that has been adapted to the setting of band selection of hyperspectral data. We demonstrate the capability of EHBS to dynamically learn optimal bands seamlessly as an integral part of the Convolutional Neural Network (CNN) implementation, particularly in the context of semantic segmentation, a visual task that predicts semantic categories for each pixel in an input image.","We denote a possible band selection via an indicator vector I ∈ {0, 1}^n where I_j = 1 iff band j was chosen for processing. The norm ⋀I⋀_1 of an indicator function I corresponds to the number of selected bands. We denote x ⋅ I as the point-wise product between an input item x and the indicator vector I in which all non selected bands are effectively masked to zero. Let k be the target number of bands. The goal of embedded band selection methods is to simultaneously select and Indicator vector I and a model f_θ ∈ F that minimize the overall loss of the data as follows:
arg min
_{θ,I}
1
m
n
X
i=1
(
Loss
f_θ(x_i ⋅ I), y_i
)
(1)","On the PaviaU dataset with a patch size of 7x7, our proposed model outperforms the baseline models in most of the data points and is comparable to the best baseline model in the others. it also shows the corresponding AUC metric for the various models. Overall, our EHBS model achieved the highest AUC value surpassing the performance of all other models. For this specific dataset, close to optimal performance is obtained by using the 20 best-selected bands (or ∼20% of the bands). Performance is stable with only slight improvement from this point till the use of the full hyperspectral input.","Following, we highlight several key findings and implications derived from our experiments. Firstly, our proposed method demonstrates clear superiority across two diverse datasets and various experimental settings, underscoring its robustness and effectiveness. We conducted comparisons among the methods in various settings, including different patch sizes, datasets, and the number of selected bands. We observed that for larger patch sizes, our method outperformed others by an even wider margin. We attribute this phenomenon to the presence of more contextual information and a higher model complexity. Consequently, employing an embedded model enables the model to discover the optimal bands that effectively capture the intricate non-linear interactions among the features. Additionally, our findings suggest that our model tends to outperform baseline models when confronted with an abundance of training data and a higher number of bands to select. We thus hypothesize that our method would perform even better on larger datasets, with larger patch sizes, and with different architectures that hold a global view per band. However, exploration of these scenarios will be reserved for future work. We demonstrated the strength of our model while integrated into CNN-based deep learning model for hyperspectral semantic segmentation. The seamless integration of our method into an existing deep learning model that was achieved by simply adding a selection layer after the input enhances its practicality and ease of use. This simplicity of implementation coupled with the robust performance positions our method as a promising and practical solution for various machine learning and computer vision tasks.",Embedded Hyperspectral Band Selection with Adaptive Optimization for Image Semantic Segmentation,"Yaniv Zimmer, Oren Glickman","Embedded Hyperspectral Band Selection with Adaptive Optimization for Image
Semantic Segmentation
Yaniv Zimmer
Oren Glickman
Computer Science Department, Bar-Ilan University
{zimmery,oren.glickman}@biu.ac.il
Abstract
Hyperspectral band selection plays a pivotal role in re-
mote sensing and image analysis, aiming to identify the
most informative spectral bands while minimizing com-
putational overhead.
In this paper, we introduce a pi-
oneering approach for hyperspectral band selection that
offers an embedded solution, making it well-suited for
resource-constrained or real-time applications. Our pro-
posed method, embedded Hyperspectral Band Selection
(EHBS), excels in selecting the best bands without the need
for prior processing, seamlessly integrating with the down-
stream task model. This is achieved through the adapta-
tion of the Stochastic Gates (STG) algorithm, originally de-
signed for feature selection, for hyperspectral band selec-
tion in the context of image semantic segmentation and the
integration of a dynamic optimizer, DoG, which removes the
need for the required tuning the learning rate.
To assess the performance of our method, we intro-
duce a novel metric for evaluating band selection methods
across different target numbers of selected bands quantified
by the Area Under the Curve (AUC). We conduct experi-
ments on two distinct semantic-segmentation hyperspectral
benchmark datasets, demonstrating its superiority in terms
of its resulting accuracy and its ease of use compared to
many common and state-of-the-art methods. Furthermore,
our contributions extend beyond the realm of hyperspectral
band selection. The adaptability of our approach to other
tasks, especially those involving grouped features, opens
up promising avenues for broader applications within the
realm of deep learning, such as feature selection for feature
groups. The demonstrated success on the tested datasets
and the potential for application to a variety of tasks under-
score the value of our method as a substantial addition to
the field of computer vision. It offers new possibilities for
feature selection and optimization in complex data analysis
scenarios, making it a significant asset for researchers and
practitioners alike.
1. Introduction
Hyperspectral imaging (HSI) involves capturing the
complete optical spectrum at each point within an image.
While a standard color camera records light intensity in just
three colors (Red, Green, and Blue), a hyperspectral cam-
era captures the entire wavelength range (typically consist-
ing of several hundred bands) of light reflected from each
scene point. The transition from color to full hyperspec-
tral imaging provides a substantial increase in information,
holding considerable potential across various applications
such as medical imaging, agriculture, aerial photography,
and autonomous driving.
While HSI offers notable advantages, it is not without
its challenges. One significant drawback lies in the sub-
stantial costs associated with the physical sensor hardware.
Additionally, HSI incurs increased expenses related to stor-
ing, transferring, and analyzing the considerably larger im-
age data generated by HSI. Furthermore, the rise of very
large and deep networks for vision has further increased the
complexity of models and, consequently, computing costs.
Hence, there is a pressing need for algorithms and methods
for band selection, i.e., identifying a subset of hyperspec-
tral bands that retains essential information for downstream
tasks. Though existing research has explored band selec-
tion, the various methods proposed typically involved an
unsupervised pre-processing step that is independent of the
downstream HSI task and thus the choice of band selection
may be sub-optimal.
This paper introduces Embedded Hyperspectral Band
Selection (EHBS), a plug-and-play embedded method.
EHBS effectively selects the optimal bands without requir-
ing preliminary processing, seamlessly integrating with the
downstream task model.
EHBS utilizes an existing fea-
ture selection algorithm based on stochastic gates that was
adopted to the setting of band selection of hyperspectral
data. In this study, we showcase the capability of EHBS
to dynamically learn optimal bands seamlessly as an inte-
gral part of the Convolutional Neural Network (CNN) im-
plementation. This is done in the context of semantic seg-
1
arXiv:2401.11420v1  [cs.CV]  21 Jan 2024
mentation, a visual task that predicts semantic categories
for each pixel in an input image, that has received growing
interest in the context of hyperspectral imagery, particularly
with the application of deep learning methods. Our results
demonstrate that our method outperforms existing band-
selection methods on two different hyperspectral semantic
segmentation datasets achieving similar accuracy values of
the full hyperspectral data by using only around 25% of the
bands. Importantly, our approach stands out by being easily
applicable to deep-learning tasks over HSI datasets.
2. Background and Related Work
2.1. Hyperspectral Imaging
Hyperspectral imaging captures rich spectral data per
pixel, unlike standard imaging with only three spectral
samples (Red, Green, Blue). Contemporary hyperspectral
systems offer hundreds of spectral bands spanning both
visible and invisible spectra, effectively creating a three-
dimensional cube composed of two-dimensional grayscale
images. This detailed HSI data provides insights beyond the
capabilities of regular RGB imaging that can be exploited
by deep learning models.
Applications of HSI data are broad and range from med-
ical tissue classifying where ill tissues can be found with
non-invasive methods [7, 18] to non-destructive quality as-
sessment of agricultural products [26], autonomous driv-
ing [5] and face recognition [31].
Semantic segmentation is a fundamental HSI task to pre-
dict the semantic categories for each pixel of an hyperspec-
tral input image [25]. Semantic segmentation tasks based
on aerial and satellite images play an important role in a
wide range of applications [20]. In recent years, the suc-
cessful application of deep learning (DL) in the field of
computer vision (CV) has led to a surge of work applying
DL methods for data semantic segmentation, resulting in
notable achievements and significant advancements.
HSI algorithms set themselves apart from typical im-
age processing methods due to variations in dataset size
and data samples. Despite each sample carrying signifi-
cantly more information, hyperspectral datasets are orders
of magnitude smaller compared to RGB datasets. This lim-
itation presents challenges, requiring models to be concise
enough to manage high-dimensional data effectively with-
out an abundance of training data.
Due to the aforementioned reasons, early research in hy-
perspectral imaging (HSI) concentrated on the application
of traditional machine learning (ML) models, with a par-
ticular emphasis on Support Vector Machines (SVMs) [28].
As deep learning models demonstrated increasing success
in addressing computer vision tasks over RGB data, there
has been a notable surge in work to apply deep learning to
HSI, with Convolutional Neural Networks (CNNs) in par-
ticular gaining prominence [20].
Deep learning applications for HSI became popular and
prevailed in recent years. For example, Zhang et. al [37]
used a 3D CNN with transfer learning for aero image seg-
mentation while [29] explored unsupervised hyperspectral
unmixing using autoencoders to classify hyperspectral im-
ages into 3 labels (Tree,Water,Rock).
In the field of agriculture, [16] apply a deep learning
CNN model to estimate strawberry ripeness from hyper-
spectral images.
Acknowledging the computational bur-
den associated with processing the entire spectrum data, the
authors employed a sequential feature selector to enhance
computational efficiency by reducing the number of bands.
Another application with image-level context is face recog-
nition using hyperspectral data. In [31], A 2D CNN model
classified the face class using a single band image selected
by a majority voting algorithm. However, such an approach
does not scale to tasks in which multiple bands are required
as in material detection where spectral data is very impor-
tant [29].
In conclusion, deep learning has emerged as a pivotal
and widely adopted technique in hyperspectral imaging
(HSI) applications. The abundance of spectral information
captured by HSI necessitates the development of effective
band selection methods, a crucial consideration for reduc-
ing the input size fed into deep learning models. This be-
comes particularly essential given the challenges posed by
the high dimensionality of hyperspectral data, emphasizing
the ongoing need for innovative approaches to handle the
intricacies of this unique imaging modality.
2.2. Feature Selection
Feature selection (FS) methods identify the essential fea-
tures needed by a machine learning system to perform a
downstream task and can be categorized broadly as filter,
wrapper, and embedded methods.
Filter methods elimi-
nate irrelevant features before model learning, using statis-
tical relevance scores [6, 10]. Wrapper methods determine
feature relevance based on model performance [4, 23], but
their drawback lies in computational expense [30]. Em-
bedded methods address this by simultaneously learning
the model and selecting relevant features within a self-
contained and single-train organism.
For example, the
widely recognized Least Absolute Shrinkage and Selection
Operator (LASSO) [34] is an embeded FS algorithm that
minimizes loss with an l1 constraint but is limited to lin-
ear functions. Attempts to extend LASSO using neural net-
works face challenges with suboptimal gradient descent on
l1 regularized objectives [15].
STG [36] propose an embedded feature selection neural
network scheme. The STG procedure is based on proba-
bilistic relaxation of the ℓ0 norm of features, or the count
of the number of selected features. The ℓ0-based regular-
2
ization relies on a continuous relaxation of the Bernoulli
distribution; such relaxation allows the STG model to learn
the parameters of the approximate Bernoulli distributions
via gradient descent. The STG framework simultaneously
learns either a nonlinear regression or classification func-
tion while selecting a small subset of features and pro-
vides an information-theoretic justification for incorporat-
ing Bernoulli distribution into feature selection. Figure 1
illustrates the STG model in which the stochastic gates are
attached to the xi input features, where the trainable param-
eter µi and a noise component ϵi control the choice of the
gate being active or not (zi).
Figure 1. Illustration of the STG model
2.3. Hyperspectral Band Selection
Band selection differs from traditional feature selection
in that, while feature selection typically involves deciding
whether to include or exclude individual features, band se-
lection operates at a broader level, focusing on the inclusion
or exclusion of entire bands. This distinction is particularly
relevant in computer vision models with patches or convo-
lution layers, where band selection essentially translates to
feature selection for entire groups.
Band selection (BS) methods are categorized into two
main groups. Unsupervised methods operate without using
any annotations, relying on information criteria for inter-
nal information and dissimilarity from other bands. While
unsupervised methods are suitable for unlabeled datasets,
their lack of consideration for specific hyperspectral imag-
ing (HSI) task performance may result in sub-optimal band
choices. On the other hand, Supervised methods utilize an-
notated data in the selection process, tailoring band selec-
tion to the specific task and considering prediction model
performance.
This category further divides into before-
train (finding the best bands before training the downstream
model) and embedded in train (training the band selector as
part of the downstream model, as in our method). Follow-
ing we will describe six different methods that can be clas-
sified into five different families: Search, Ranking, Clus-
tering, Sparsity and DL-based. These methods were imple-
mented and used as baseline models to be compared to our
proposed model.
Search methods in band selection explore the band space
using tailored information criteria to identify bands suitable
for a specific task. As an exhaustive search of all combi-
nations is impractical, search-based methods apply efficient
searching heuristics based on predefined criteria. One such
method, LP [14], employs dissimilarity criteria and an in-
cremental search approach. At each iteration, the least sim-
ilar band is added to the previously selected bands, estimat-
ing candidates as linear combinations. The search criteria
include similarity metrics such as Bhattacharyya distance,
Jeffries–Matusita (JM) distance [21], or spectral angle map-
ping (SAM) [19].
Ranking-based methods prioritize bands based on vari-
ance, dissimilarity, or other metrics to select the most im-
portant bands. These approaches, categorized as unsuper-
vised and supervised, assess bands’ distinctiveness and con-
tribution to specific tasks. In unsupervised ranking, high
information criteria, like those in MVPCA [11] and CBS-
CEM [12], focus on variance and dissimilarity, respectively.
The latter aims to minimize band correlation or dependence.
In supervised ranking, methods like MMCA [11] and Mu-
tual Information (MI) [17] leverage labeled data to con-
struct task-oriented criteria, minimizing misclassification
error and prioritizing bands associated with ground truth
labels. These ranking-based band selection algorithms ef-
ficiently identify relevant bands for specific tasks.
Clustering-based methods in band selection involve
grouping bands and selecting one representative from each
group to minimize redundancy. Often used in conjunction
with ranking, these methods aim to choose the highest-
ranked representatives from clustered bands [35]. WaLuDI
and WaLuMI [27] employ a distance metric based on infor-
mation theory measures to assess similarity between bands.
Their approach minimizes in-cluster variance while max-
imizing between-cluster variance. The dissimilarity mea-
sure calculates the distance between bands using probabil-
ity functions based on gray-scale pixel values. For instance,
the KL divergence is employed to quantify differences be-
tween probability distributions, enhancing the effectiveness
of band clustering and selection strategies.
Sparsity-based methods employ sparsity constraints to
represent each band slice image as a linear combination of
other bands, aiming to identify the most influential com-
binations. The Iterative Sparse Spectral Clustering (ISSC)
algorithm [33] introduces an innovative approach by rep-
resenting band connections as a graph and identifying rep-
resentatives from each cluster. ISSC minimizes non-zero
elements in a similarity matrix, where each row contains
coefficients of corresponding bands. The algorithm utilizes
spectral clustering to create clusters and selects the closest
band to each cluster center as a representative.
3
Another more recent line of work include DL-based
methods aimed to model the nonlinear interdependencies
between the various spectral bands. BS-Nets [9] is an em-
bedded unsupervised method that applies a DNN to recon-
struct the full HSI image from partially available bands. The
networks attention mechanism was used from both spatial
and spectral views to infer the best bands combination for
reconstruction.
3. Method
3.1. Problem statement
Let X represent a sample of m data instances where each
instance is an n-sized array of 2D images and Y denotes the
corresponding m labels.
Let F be a family of models for the downstream task
each accompanied by a choice of parameters θ and Loss is
a loss function between a specific label xi and a correspond-
ing model output yi.
We denote a possible band selection via an indicator vec-
tor I ∈ {0, 1}n where Ij = 1 iff band j was chosen for
processing. The norm ∥I∥1 of an indicator function I cor-
responds to the number of selected bands. We denote x ⊙ I
as the point-wise product between an input item x and the
indicator vector I in which all non selected bands are effec-
tively masked to zero. Let k be the target number of bands.
The goal of embedded band selection methods is to
simultaneously select and Indicator vector I and a model
fθ ∈ F that minimize the overall loss of the data as follows:
arg min
θ,I
1
m
n
X
i=1
(Loss(fθ(xi ⊙ I), yi))
(1)
3.2. Our proposed system - EHBS
The Embedded Hyperspectral Band Selection (EHBS)
system, our proposed approach, is an end-to-end embedded
system. It comprises a downstream task model enhanced
with an additional layer inserted between the input layer and
the task model. This added layer, is based on the principles
of the Stochastic Gates (STG) model - see 2.2. Our novel
adaptation is specifically tailored for hyperspectral band se-
lection within the context of image semantic segmentation
and is further detailed in 3.2.1. By adding this layer, EHBS
leverages the intrinsic characteristics of hyperspectral data
and addresses the unique requirements of semantic segmen-
tation tasks, seamlessly integrating with downstream mod-
els without the need for band-selection prepossessing.
3.2.1
Adapting STG for Feature Groups and Convolu-
tional Layers
In the setting of band selection, and in contrast to the stan-
dard feature selection setting, all features of a given band
Figure 2. Illustration of our EHBS layer
should either be included or excluded from the input. We
have thus adapted the stochastic gates framework, originally
designed for feature selection, to work over groups of fea-
tures. This is done be altering the gates layer to either mask
all of the features in the group (i.e. band) or to leave the fea-
tures intact. This layer comes right after the input layer and
precedes the first layer of the deep learning network of the
downstream task. The Gate is applied to each band-specific
2D input in the corresponding full HSI 3D input. The out-
put of the gate is a corresponding mask on the input based
on each specific gate value.
The stochastic gates layer includes N gates per each
band (N is the size of the spectral dimension). Each gate
s includes a learned parameter µ which corresponds to
the significance of its corresponding input band (See Fig-
ure Figure 2). The gates layer acts as a sparse layer manip-
ulating the input layer while preserving its shape. This is
done by multiplying all corresponding feature of a band by
a calculated value based on the gates µ value.
At every step of the deep learning forward pass, the gates
layer masks the input band features based on the following
gate-specific calculated value: z = clamp(µ + ϵ) where
ϵ is a normally distributed noise with mean 0 and standard
deviation σ (ϵ ∼ N(0, σ2)) and clamp constrains the value
to be in the range [0, 1] (clamp(x) = max(0, min(x, 1))).
The inclusion of noise enables model exploration during
the training phase; however, it is omitted during the testing
and production stages. As we want the gates mask to con-
verge into 1 or 0 (effectively performing band selection), we
add a regularization component R to the downstream task
loss function calculated as follows:
R = λ
N
X
i=1
Φ(µi
σ )
(2)
Where Φ is the standard Gaussian CDF and µi is the µ value
of gate si and λ is a regularization factor.
4
3.2.2
Integrating a parameter free optimizer
An additional significant aspect of EHBS is the integration
of a dynamic optimizer named DoG, which eliminates the
need for meticulous tuning of learning rates, a common re-
quirement in embedded feature selection methods. This dy-
namic optimization strategy adeptly navigates the trade-off
between feature selection step size and accuracy, contribut-
ing to a more refined and adaptive hyperspectral band selec-
tion process.
From our initial experimentation, it was evident that the
STG-based deep learning network is very sensitive to the
setting of the learning rate. Hence, one needs to experi-
ment and set a different learning rate value depending on
the system architecture, input data and target number of
bands. In order to circumvent this limitation we chose DoG,
a parameter-free stochastic optimizer DoG (”Distance over
Gradients”) [22]. The DoG step sizes depend on simple em-
pirical quantities (distance from the initial point and norms
of gradients) and there is no “learning rate” parameter that
needs to be set.
We have used this dynamic optimizer to coup with the
need to balance between the band selection objective to the
segmentation loss objective in the combined training pro-
cess. Employing a parameter-free stochastic method such
as DoG has relieved us from the necessity of tuning the
learning rate parameter thus providing a consistent model
applicable to all experimental settings.
4. Experiments setting
4.1. Benchmark Datasets for Evaluation
In this section, we present the benchmark datasets em-
ployed to evaluate the performance of our proposed EHBS
model for Image Semantic Segmentation. We focus on two
common hyperspectral semantic segmentation benchmark
datasets: Pavia University (PaviaU) and Salinas that repre-
sent diverse real-world scenarios.
4.1.1
PaviaU
The PaviaU hyperspectral semantic segmentation bench-
mark dataset [2] captures an urban scene over Pavia, north-
ern Italy, acquired by the ROSIS sensor during a dedicated
flight campaign. This publically available dataset encom-
passes 103 spectral bands in wave length of 430-860 nm,
and the image has dimensions of 610 by 610 pixels. It is
noteworthy that certain samples in the image lack infor-
mation, resulting in 42,000 valid pixels for analysis. The
dataset is annotated with ground-truth labels, classifying
nine distinct categories, including bitumen, asphalt, tiles,
trees, and more. These annotations provide a rich founda-
tion for evaluating the semantic segmentation performance
(a)
(b)
Figure 3. Pavia University dataset: (a) A sample band of scene in
grey-scale and (b) The color-coded annotation of the ground-truth
(a)
(b)
Figure 4.
Salinas dataset: (a) A sample band of the scene in
grey-scale and (b) The corresponding color-coded annotation of
the ground-truth
of models across a diverse set of urban materials and fea-
tures. Figure 3 displays a sample band of the scene in grey-
scale and as as the corresponding color-coded annotation of
the ground-truth.
4.1.2
Salinas
The Salinas hyperspectral semantic segmentation bench-
mark dataset [3] captures an agricultural scene in the Sali-
nas Valley, California, gathered by the 224-band AVIRIS
sensor in wave length of 430-2500 nm. Renowned for its
high spatial resolution (3.7-meter pixels), the dataset covers
an extensive area spanning 512 lines by 217 samples, pro-
viding approximately 54,000 valid pixels for analysis. The
dataset is meticulously annotated to distinguish among 16
classes of ground truths, including vegetables, bare soils,
and vineyard fields. Notably, to refine the dataset for agri-
cultural semantic segmentation tasks, 20 water absorption
bands were selectively discarded from the original dataset,
specifically bands [108-112], [154-167], and 224.
Figure 4 displays a sample band of the scene in grey-
scale and as as the corresponding color-coded annotation of
the ground-truth.
4.2. Band Selection Benchmark Methods
We selected seven different baseline methods for com-
parison with our proposed EHBS method in the context of
embedded hyperspectral band selection for image semantic
segmentation. Five methods, namely LP [14], ISSC [33],
5
WALUMI [27], WALUDI [27], and MMCA [11], were
identified as top-performing techniques in a comparative
study [32] conducted on the Pavia dataset.
We also in-
cluded BS-Nets [9], an embedded unsupervised DL-based
BS method.
These supervised and unsupervised meth-
ods collectively represent five distinct band selection fam-
ily types (See 2.3), offering a diverse and comprehensive
benchmark for evaluating the performance of our proposed
techniques.
Although not used before for band selection, we have im-
plemented a supervised deep learning embedded method,
where ℓ1 regularization was used on bands gates values,
and the k gates with the highest values were the selected
ones. This implementation was inspired by similar work
in which regularization was embedded in DL networks for
feature selection (see 2.2) and adapted to the setting of BS
by applying the regularization over the bands rather than
over the individual input features. The subsequent analysis
and comparison against these established methods aim to
provide a robust assessment of the efficacy and innovation
introduced by our proposed embedded hyperspectral band
selection methods.
4.3. Model Details
Our EHBS method is easily adaptable for integration into
any CNN [24], ViT [13] or other deep learning model. In
practice it is a neural layer between the input and the down-
stream task model, which consists of gates multiplying the
input values with a learned factor for every patch input data
proceeded by an arbitrary downstream task model. In our
comprehensive experiments, we specifically focused on the
state-of-the-art CNN model proposed by Hamida et el. [8],
a 3D CNN specifically tailored for spectral-spatial data ex-
traction. We chose this model for its proven performance
in hyperspectral semantic segmentation tasks. The objec-
tive of our experiments was to evaluate the effectiveness
of our embedded band selection approach in comparison to
non-embedded methods across varied dataset sizes, sample
(patch) sizes, and cross-validation scenarios. Implementa-
tion wise, we based our code on the publicly available im-
plementation of Hamida et el. [8] available in [1]. The cor-
responding architecture of the CNN network is illustrated
in Figure 5. The CNN network consists of several 3D con-
volutional layers, proceeded by a 1D convolution and even-
tually followed by a final class probabilities output layer.
For the hyperparameters and kernel sizes, we used the de-
fault settings consistent with the original paper and its cor-
responding available model implementation [8]. We used
the Pytorch deep learning framework. Batch size was set
to 256. The number of epochs was set to 100 for PaviaU
and 150 for Salinas based on observing the converge pat-
terns during our initial experimentation. The initial mu and
sigma values of the stochastic gates layer were set to the
Figure 5. Illustration of Hamida et el. model
recommended value of 0.5.
We utilized the DoG optimizer, eliminating the need for
explicit learning rate tuning, contributing to the adaptability
and efficiency of our method.
The STG layer includes a regularization component de-
signed to minimize the number of the active gates (or more
specifically their µ values). The corresponding regulariza-
tion factor (λ) was used to meet the target number of bands.
We run multiple experiments with various λ values to obtain
the various required target number of bands.
4.4. Experimental Design and Evaluation
In order to test our method and the benchmark methods
in a robust and accurate way, we ran each experiment with
10-fold cross-validation. In addition, as most of the baseline
methods are not embedded, we first run the various band
selection models as a first stage and then used the selected
bands from each method to train the downstream task model
via the baseline semantic segmentation CNN model. This
ensured a proper apples-to-apples comparison of the various
band selection methods.
For evaluation, we compared the accuracy of the vari-
ous methods for various target number of bands. In addi-
tion, in order to compare methods across the whole range
of numbers of selected bands, we introduced an new eval-
uation metric which calculated the area under the bands-
performance curve. This metric was inspired by the com-
mon Area Under the Curve (AUC) metric used to evaluate
the performance of a binary classification model, particu-
larly in the context of a Receiver Operating Characteris-
tic (ROC) curve. The AUC provides a single scalar value
that summarizes the performance of a classification model
across various classification thresholds and is widely used
metric for evaluating and comparing binary classification
models. Similarly, in our setting, the AUC provides a sin-
gle scalar value that summarizes the performance of a band
selection model across various number of bands selected al-
lowing us to compare different models by a single value.
For testing our EHBS model, in each experiment we run
the model multiple times with different regularization factor
(λ) values in the range 0.2 to 2.4 applying a simple heuristic
search till the desired target number of selected bands was
6
Figure 6. Illustration of the selected bands for the Salinas dataset
across various target number of bands. Each horizontal ”row” rep-
resents selection of bands in a single run of EHBS with the selected
bands marked in black. The top row corresponds to the selection
of 8 bands and the bottom row to a run with 48 bands selected.
(a) Accuracy results on the PaviaU dataset for the various methods
Figure 7. Accuracy and AUC results over the PaviaU dataset, 7x7
patch size, for multiple target number of bands
met. The number of bands that were used and the corre-
sponding accuracy results were saved for each run. Overall
accuracy for a given target number of bands was calculated
by averaging the accuracy across all folds.
5. Results
Figure 7 shows a graph comparing the accuracy of our
proposed model, EHBS, as well as the various baseline band
selection models on the PaviaU dataset with a patch size of
7x7 over multiple target number of bands. As can be seen
from the graph our proposed model outperforms the base-
line models in most of the data points and is comparable
to the best baseline model in the others. it also shows the
corresponding AUC metric for the various models. Overall,
our EHBS model achieved the highest AUC value surpass-
ing the performance of all other models. Figure 8 shows
Figure 8. EHBS accuracy result with Pavia 7x7 patch over the
whole range of hyperspectral selection
the accuracy results of our proposed method over the whole
range of target band selection for the PaviaU dataset and
a 7x7 patch size. For this specific dataset, close to opti-
mal performance is obtained by using the 20 best-selected
bands (or ∼20% of the bands). Performance is stable with
only slight improvement from this point till the use of the
full hyperspectral input.
Table 1 shows a comparison table including accuracy re-
sults and AUC results over the PaviaU dataset over two dif-
ferent patch sizes (7x7 and 11x11) and selected target num-
ber of bands. As baseline models we chose the top perform-
ing models on the PaviaU 7x7 experiments. Table 2 shows
a similar comparison table for the Salinas dataset. For all
datasets and patch sizes, EHBS obtained the highest AUC
score and obtained the best accuracy for all Salinas experi-
ments and for the vast majority of PaviaU experiments. In
the few PaviaU experiments for which EHBS was not the
top performing method it obtained comparable accuracy re-
sults to the top performing method.
Figure 6 illustrates the selected bands for the Salinas
dataset across various target number of bands.
EHBS
systematically ignores areas with low contribution for the
downstream task performance and the prominent band se-
lection areas remain consistent across different runs.
6. Discussion and Conclusion
Following, we highlight several key findings and impli-
cations derived from our experiments. Firstly, our proposed
method demonstrates clear superiority across two diverse
datasets and various experimental settings, underscoring its
robustness and effectiveness. We conducted comparisons
among the methods in various settings, including different
patch sizes, datasets, and the number of selected bands.
We observed that for larger patch sizes, our method out-
7
Bands
Pavia 7x7
Pavia 11x11
Methods
6 (6%)
10 (10%)
15 (15%)
AUC
EHBS
Waludi
Walumi
ISSC
BS-NETS
Conv
BS-NETS
FC
94.16
95.09
93.72
95.56
95.94
93.71
97.69
96.36
97.52
97.25
96.18
94.62
98.75
98.00
98.14
97.59
98.72
96.17
.969957
.965914
.966614
.968892
.969466
.949135
EHBS
Waludi
Walumi
ISSC
BS-NETS
Conv
BS-NETS
FC
98.28
98.57
98.34
98.80
97.55
95.86
98.94
98.59
99.22
98.48
98.38
98.16
99.85
98.91
99.52
99.38
99.74
98.58
.990078
.986985
.990042
.988928
.986285
.976900
Table 1. Comparison Table - accuracy and AUC over the PaviaU dataset for two patch sizes (7x7, 11x11)
Bands
Salinas 7x7
Salinas 11x11
Methods
10 (5%)
20 (10%)
30 (15%)
AUC
EHBS
Waludi
Walumi
ISSC
BS-NETS
Conv
BS-NETS
FC
96.09
91.02
90.40
94.54
93.96
93.92
97.21
93.30
91.82
94.94
94.54
94.50
97.58
94.83
93.95
96.73
96.34
95.37
.970225
.931125
.919975
.952875
.94845
.945725
EHBS
Waludi
Walumi
ISSC
BS-NETS
Conv
BS-NETS
FC
97.83
91.77
93.33
93.28
95.76
96.48
98.93
92.25
95.88
98.14
96.74
96.51
99.41
95.85
95.25
98.95
97.29
96.00
.98775
.9303
.95085
.97992
.966375
.96375
Table 2. Comparison Table - accuracy and AUC over the Salinas dataset for two patch sizes (7x7, 11x11)
performed others by an even wider margin. We attribute
this phenomenon to the presence of more contextual infor-
mation and a higher model complexity. Consequently, em-
ploying an embedded model enables the model to discover
the optimal bands that effectively capture the intricate non-
linear interactions among the features. Additionally, our
findings suggest that our model tends to outperform base-
line models when confronted with an abundance of train-
ing data and a higher number of bands to select. We thus
hypothesize that our method would perform even better on
larger datasets, with larger patch sizes, and with different
architectures that hold a global view per band. However,
exploration of these scenarios will be reserved for future
work. We demonstrated the strength of our model while
integrated into CNN-based deep learning model for hyper-
spectral semantic segmentation. The seamless integration
of our method into an existing deep learning model that was
achieved by simply adding a selection layer after the input
enhances its practicality and ease of use. This simplicity of
implementation coupled with the robust performance posi-
tions our method as a promising and practical solution for
various machine learning and computer vision tasks.
These results also demonstrate the robustness of our pro-
posed method along multiple settings and various target
number of bands. The method performs well and consis-
tently without the need to tweak or modify the model re-
gardless of the target number of bands - be it only a few
number of bands or when aiming for a large number of se-
lected bands. This also allows researchers to get a good
understanding of the performance curve and choose the de-
sired tradeoff given the application setting.
After our initial experimentation, we further tested more
optimizers applying an extensive search for an optimal
learning rate. As can be seen in Table 3, when using the
Adam optimer the results are sensitive to the learning rate
and finding the right learning rate is crucial. It turns out
that on the PaviaU dataset with a 7x7 path, EHBS with the
Bands
DoG
Adam lr=0.01
Adam lr=0.002
Adam lr=0.001
6 (6%)
94.16
76.67
98.97
98.20
10 (10%)
97.69
96.61
99.32
98.47
15(15%)
98.75
97.39
99.55
98.77
Table 3. Comparing accuracy of EHBS with different optimizers
and learning rates (lr) for PaviaU dataset with patch size of 7
Adam optimizer can achieve even better performance than
EHBS with DoG. However, this is a retrospective result ob-
tained over our testset and not via tuning on an held out
evaluation set. Given that the results achieved with the DoG
optimizer are comparable to the optimal outcomes obtained
using the Adam optimizer, we advocate for the adoption of
DoG. Notably, DoG eliminates the need for parameter tun-
ing, ensuring a more robust and reliable performance.
In conclusion, our embedded hyperspectral band selec-
tion method, leveraging the Stochastic Gates (STG) al-
gorithm and the dynamic optimizer DoG, proves to be
a promising solution for image semantic segmentation
tasks. The seamless integration into downstream models,
absence of pre-processing requirements, and adaptability
to resource-constrained or real-time applications mark the
method’s practical significance. Our approach, validated on
two benchmark datasets, not only outperforms common and
state-of-the-art methods in terms of information preserva-
tion but also introduces a novel AUC-based metric for eval-
uating band selection methods. The success of our method
extends beyond hyperspectral band selection, showcasing
its potential in broader applications within the domain of
deep learning. The demonstrated efficiency and adaptabil-
ity highlight its substantial contribution to computer vision,
offering valuable possibilities for feature selection and op-
timization in diverse data analysis scenarios. Researchers
and practitioners stand to benefit significantly from the ver-
satility and performance of our proposed method.
8
7. Acknowledgement
This work was partially supported by the Chief Scientist
of the Israeli Ministry of Agriculture grant number12-03-
0010. We would also like to thank Nadav Spitzer for his
help in implementing the baseline feature selection models
and to Ofir Lindenbaum and Jacob Goldberger for providing
input and feedback on our work.
References
[1] https://github.com/eecn/Hyperspectral-Classification. 6
[2] Pavia university scene. ehu.eus/ccwintco/index.
php/Hyperspectral_Remote_Sensing_Scenes#
Pavia_University_scene. 5
[3] Salinas scence. http://www.ehu.eus/ccwintco/
index.php/Hyperspectral_Remote_Sensing_
Scenes#Salinas. 5
[4] Genevera I. Allen. Automatic feature selection via weighted
kernels and regularization. Journal of Computational and
Graphical Statistics, 22(2):284–299, 2013. 2
[5] K.
Basterretxea,
V.
Mart´ınez,
J.
Echanobe,
J.
Guti´errez–Zaballa, and I. Del Campo.
Hsi-drive:
A
dataset for the research of hyperspectral image processing
applied to autonomous driving systems.
In 2021 IEEE
Intelligent Vehicles Symposium (IV), pages 866–873, 2021.
2
[6] R. Battiti. Using mutual information for selecting features in
supervised neural net learning. IEEE Transactions on Neural
Networks, 5(4):537–550, 1994. 2
[7] Neslihan Bayramoglu, Mika Kaakinen, Lauri Eklund, and
Janne Heikkil¨a.
Towards virtual h&e staining of hyper-
spectral lung histology images using conditional generative
adversarial networks. In 2017 IEEE International Confer-
ence on Computer Vision Workshops (ICCVW), pages 64–71,
2017. 2
[8] Amina Ben Hamida, Alexandre Benoit, Patrick Lambert, and
Chokri Ben Amar. 3-d deep learning approach for remote
sensing image classification.
IEEE Transactions on Geo-
science and Remote Sensing, 56(8):4420–4434, 2018. 6
[9] Yaoming Cai, Xiaobo Liu, and Zhihua Cai.
Bs-nets: An
end-to-end framework for band selection of hyperspectral
image. IEEE Transactions on Geoscience and Remote Sens-
ing, 58(3):1969–1984, 2019. 4, 6
[10] Chun-Hao Chang, Ladislav Ramp´asek, and Anna Golden-
berg.
Dropout feature ranking for deep learning models.
CoRR, abs/1712.08645, 2017. 2
[11] Chein-I Chang, Qian Du, Tzu-Lung Sun, and Mark LG Al-
thouse. A joint band prioritization and band-decorrelation
approach to band selection for hyperspectral image classifi-
cation. IEEE transactions on geoscience and remote sensing,
37(6):2631–2641, 1999. 3, 6
[12] Chein-I Chang and Su Wang. Constrained band selection for
hyperspectral imagery. IEEE transactions on geoscience and
remote sensing, 44(6):1575–1585, 2006. 3
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. CoRR, abs/2010.11929, 2020. 6
[14] Qian Du and He Yang. Similarity-based unsupervised band
selection for hyperspectral image analysis. IEEE geoscience
and remote sensing letters, 5(4):564–568, 2008. 3, 5
[15] Jean Feng and Noah Simon. Sparse-input neural networks
for high-dimensional nonparametric regression and classifi-
cation. arXiv preprint arXiv:1711.07592, 2017. 2
[16] Zongmei Gao, Yuanyuan Shao, Guantao Xuan, Yongxian
Wang, Yi Liu, and Xiang Han.
Real-time hyperspectral
imaging for the in-field estimation of strawberry ripeness
with deep learning.
Artificial Intelligence in Agriculture,
4:31–38, 2020. 2
[17] Baofeng Guo, Steve R Gunn, Robert I Damper, and
James DB Nelson. Band selection for hyperspectral image
classification using mutual information. IEEE Geoscience
and Remote Sensing Letters, 3(4):522–526, 2006. 3
[18] Martin Halicek, Guolan Lu, James Little, Xu Wang, Mihir
Patel, Christopher Griffith, Mark El-Deiry, Amy Chen, and
Baowei Fei. Deep convolutional neural networks for clas-
sifying head and neck cancer using hyperspectral imaging.
Journal of Biomedical Optics, 22, 06 2017. 2
[19] Yuanlei He, Daizhi Liu, and Shihua Yi.
Recursive spec-
tral similarity measure-based band selection for anomaly
detection in hyperspectral imagery.
Journal of Optics,
13(1):015401, 2010. 3
[20] Wei Hu, Yangyu Huang, Li Wei, Fan Zhang, and Hengchao
Li. Deep convolutional neural networks for hyperspectral
image classification. Journal of Sensors, 2015:1–12, 2015. 2
[21] Agustin Ifarraguerri and Michael W Prairie. Visual method
for spectral band selection. IEEE Geoscience and Remote
Sensing Letters, 1(2):101–106, 2004. 3
[22] Maor Ivgi, Oliver Hinder, and Yair Carmon. Dog is sgd’s
best friend: A parameter-free dynamic step size schedule. In
International Conference on Machine Learning, 2023. 5
[23] Ron Kohavi and George H. John. Wrappers for feature sub-
set selection. Artificial Intelligence, 97(1):273–324, 1997.
Relevance. 2
[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net-
works. Advances in neural information processing systems,
25, 2012. 6
[25] Jinna Lv, Qijie Shen, Mingzheng Lv, Yiran Li, Lei Shi, and
Peiying Zhang. Deep learning-based semantic segmentation
of remote sensing images: a review. Frontiers in Ecology
and Evolution, 2023. 2
[26] Marena Manley. Near-infrared spectroscopy and hyperspec-
tral imaging: non-destructive analysis of biological materi-
als. Chemical Society Reviews, 43(24):8200–8214, 2014. 2
[27] Adolfo
Mart´Inez-Us ´OMartinez-Uso,
Filiberto
Pla,
Jos´e Mart´ınez Sotoca, and Pedro Garc´ıa-Sevilla. Clustering-
based
hyperspectral
band
selection
using
information
measures. IEEE Transactions on Geoscience and Remote
Sensing, 45(12):4158–4171, 2007. 3, 6
9
[28] Farid Melgani and Lorenzo Bruzzone. Classification of hy-
perspectral remote sensing images with support vector ma-
chines. IEEE Transactions on geoscience and remote sens-
ing, 42(8):1778–1790, 2004. 2
[29] Burkni Palsson, Jakob Sigurdsson, Johannes R Sveinsson,
and Magnus O Ulfarsson. Hyperspectral unmixing using a
neural network autoencoder. IEEE Access, 6:25646–25656,
2018. 2
[30] Debaditya Roy, Sri Rama Murty Kodukula, and Krishna Mo-
han Chalavadi. Feature selection using deep neural networks.
pages 1–6, 07 2015. 2
[31] Vivek Sharma,
Ali Diba,
Tinne Tuytelaars,
and Luc
Van Gool. Hyperspectral cnn for image classification & band
selection, with application to face recognition. Technical re-
port KUL/ESAT/PSI/1604, KU Leuven, ESAT, Leuven, Bel-
gium, 2016. 2
[32] Weiwei Sun and Qian Du. Hyperspectral band selection: A
review. IEEE Geoscience and Remote Sensing Magazine,
7(2):118–139, 2019. 6
[33] Weiwei Sun, Liangpei Zhang, Bo Du, Weiyue Li, and Yen-
ming Mark Lai.
Band selection using improved sparse
subspace clustering for hyperspectral imagery classification.
IEEE Journal of Selected Topics in Applied Earth Observa-
tions and Remote Sensing, 8(6):2784–2797, 2015. 3, 5
[34] Robert Tibshirani. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society. Series B
(Methodological), 58(1):267–288, 1996. 2
[35] Qi Wang, Fahong Zhang, and Xuelong Li. Optimal cluster-
ing framework for hyperspectral band selection. IEEE Trans-
actions on Geoscience and Remote Sensing, 56(10):5910–
5922, 2018. 3
[36] Yutaro Yamada, Ofir Lindenbaum, Sahand Negahban, and
Yuval Kluger. Feature selection using stochastic gates. In
Hal Daum´e III and Aarti Singh, editors, Proceedings of the
37th International Conference on Machine Learning, vol-
ume 119 of Proceedings of Machine Learning Research,
pages 10648–10659. PMLR, 13–18 Jul 2020. 2
[37] Haokui Zhang, Ying Li, Yenan Jiang, Peng Wang, Qiang
Shen, and Chunhua Shen. Hyperspectral classification based
on lightweight 3-d-cnn with transfer learning. IEEE Transac-
tions on Geoscience and Remote Sensing, 57(8):5813–5828,
2019. 2
10
","nanResearch in hyperspectral imaging (HSI) has focused on the application of traditional machine learning (ML) models, with emphasis on Support Vector Machines (SVMs). As deep learning models achieved increasing success in computer vision tasks over RGB data, there has been a notable surge in work to apply deep learning to HSI, with Convolutional Neural Networks (CNNs) in particular gaining prominence. Deep learning applications for HSI have become popular and prevailed in recent years. For example, Zhang et. al [37] used a 3D CNN with transfer learning for aero image segmentation while [29] explored unsupervised hyperspectral unmixing using autoencoders to classify hyperspectral images into 3 labels (Tree,Water,Rock). In the field of agriculture, [16] applied a deep learning CNN model to estimate strawberry ripeness from hyperspectral images, employing a sequential feature selector to enhance computational efficiency by reducing the number of bands. Acknowledging the computational burden associated with processing the entire spectrum data, the authors employed a sequential feature selector to enhance computational efficiency by reducing the number of bands. Another application with image-level context is face recognition using hyperspectral data. In [31], a 2D CNN model classified the face class using a single band image selected by a majority voting algorithm. However, such an approach does not scale to tasks in which multiple bands are required as in material detection where spectral data is very important [29]. In conclusion, deep learning has emerged as a pivotal and widely adopted technique in hyperspectral imaging (HSI) applications. The abundance of spectral information captured by HSI necessitates the development of effective band selection methods, a crucial consideration for reducing the input size fed into deep learning models. This becomes particularly essential given the challenges posed by the high dimensionality of hyperspectral data, emphasizing the ongoing need for innovative approaches to handle the intricacies of this unique imaging modality."
"Optimal transport (OT) is attracting increasing attention in machine learning. It aims to transport a source distribution to a target one at minimal cost. In its vanilla form, the source and target distributions are predetermined, which contracts to the real-world case involving undetermined targets. In this paper, we propose Doubly Bounded Optimal Transport (DB-OT), which assumes that the target distribution is restricted within two boundaries instead of a fixed one, thus giving more free-dom for the transport to find solutions. Based on the entropic regularization of DB-OT, three scaling-based algorithms are devised for calculating the optimal solution. We also show that our DB-OT is helpful for barycenter-based clustering, which can avoid the excessive concentration of samples in a single cluster. Then we further develop DB-OT techniques for long-tailed classification which is an emerging and open problem. We first propose a connection between OT and classification, that is, in the classification task, training involves optimizing the Inverse OT to learn the representations, while testing involves optimizing the OT for predictions. With this OT perspective, we first apply DB-OT to improve the loss, and the Balanced Softmax is shown as a special case. Then we apply DB-OT for inference in the testing process. Even with vanilla Softmax trained features, our extensive experimental results show that our method can achieve good results with our improved inference scheme in the testing stage.","Optimal transport (OT) (Cuturi 2013) has been widely applied in machine learning (Cui et al. 2019a; Wang et al. 2013). For instance, Wasserstein distance (Arjovsky, Chin-tala, and Bottou 2017; Gulrajani et al. 2017) is applied with the dual form of OT to minimize the gap between the gen-erated and real distributions via min-max adversarial opti-mization (Li et al. 2022). SwAV (Caron et al. 2020) em-ploys the Sinkhorn algorithm for online clustering in self-supervised contrastive learning (Chen et al. 2020; Khosla et al. 2020; Wang and Liu 2021). OT-LDA (Huynh, Zhao, and Phung 2020) devises a Wasserstein barycenter-based topic model, while in (Xu et al. 2019), Gromov-Wasserstein distance is used for graph matching (Wang, Yan, and Yang 2019, 2021; Hu et al. 2020). These works all assume the source and target distributions are fixed.","Inspired by the OT-based topic model in OT-LDA (Huynh, Zhao, and Phung 2020), we propose a barycenter-based clustering method using DB-OT. This method allows for control of the sample quantity in each cluster, thereby avoiding the clustering of isolated or min-ority points within a cluster (Wang and Su 2011). Note that our barycenter-based clustering method is not limited to Euclidean space but can also be extended to other spaces, such as the Wasserstein space (Agueh and Carlier 2011). This extension allows our method to be generalized to handle more complex data, including text data.","We conduct experiments on both clustering and classifica-tion. To showcase the advantage of DB-OT on fine-grained controlling the behavior of clustering under bounded sizes of clusters, and classification in long-tail cases.","We have presented the so-called double-bounded optimal transport, with theoretical analysis and further derive three variants of algorithms to solve the problem. We then test our technique for the challenging yet realistic tasks of cluster-size bounded clustering, as well as long-tailed image recog-nition. Experimental results clearly verify its effectiveness.",Double-Bounded Optimal Transport for Advanced Clustering and Classification,"Liangliang Shi, Zhaoqi Shen, Junchi Yan","Double-Bounded Optimal Transport for Advanced Clustering and Classification
Liangliang Shi, Zhaoqi Shen, Junchi Yan*
Department of Computer Science and Engineering,
MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University
{shiliangliang, shenzhaoqi2271621336, yanjunchi}@sjtu.edu.cn
Abstract
Optimal transport (OT) is attracting increasing attention in
machine learning. It aims to transport a source distribution to
a target one at minimal cost. In its vanilla form, the source and
target distributions are predetermined, which contracts to the
real-world case involving undetermined targets. In this paper,
we propose Doubly Bounded Optimal Transport (DB-OT),
which assumes that the target distribution is restricted within
two boundaries instead of a fixed one, thus giving more free-
dom for the transport to find solutions. Based on the entropic
regularization of DB-OT, three scaling-based algorithms are
devised for calculating the optimal solution. We also show
that our DB-OT is helpful for barycenter-based clustering,
which can avoid the excessive concentration of samples in
a single cluster. Then we further develop DB-OT techniques
for long-tailed classification which is an emerging and open
problem. We first propose a connection between OT and clas-
sification, that is, in the classification task, training involves
optimizing the Inverse OT to learn the representations, while
testing involves optimizing the OT for predictions. With this
OT perspective, we first apply DB-OT to improve the loss,
and the Balanced Softmax is shown as a special case. Then
we apply DB-OT for inference in the testing process. Even
with vanilla Softmax trained features, our extensive experi-
mental results show that our method can achieve good results
with our improved inference scheme in the testing stage.
Introduction
Optimal transport (OT) (Cuturi 2013) has been widely ap-
plied in machine learning (Cui et al. 2019a; Wang et al.
2013). For instance, Wasserstein distance (Arjovsky, Chin-
tala, and Bottou 2017; Gulrajani et al. 2017) is applied with
the dual form of OT to minimize the gap between the gen-
erated and real distributions via min-max adversarial opti-
mization (Li et al. 2022). SwAV (Caron et al. 2020) em-
ploys the Sinkhorn algorithm for online clustering in self-
supervised contrastive learning (Chen et al. 2020; Khosla
et al. 2020; Wang and Liu 2021). OT-LDA (Huynh, Zhao,
and Phung 2020) devises a Wasserstein barycenter-based
topic model, while in (Xu et al. 2019), Gromov-Wasserstein
distance is used for graph matching (Wang, Yan, and Yang
*Correspondence author.
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Figure 1: Illustration for the difference between vanilla OT
and our DB-OT using the example of mines and factories
as source and target, respectively. Vanilla OT assumes the
equivalence between the supply and demand. In our DB-OT,
we assume that the demand of the factory is not a fixed value,
but rather a certain range by upper and lower bounds.
2019, 2021; Hu et al. 2020). These works all assume the
source and target distributions are fixed.
However, in many real-world cases, the source or target
distribution often varies over time and becomes uncertain.
As shown in Fig. 1, in vanilla OT, it assumes that the sup-
ply from mines and the demand from factories are fixed and
and their total masses are equal to each other. While in our
realistic setting we assume the demand from factories vary
within a double-bounded range. When the demand exceeds
the upper boundary, the factory reaches its production capac-
ity limit. Conversely, when the demand falls below the lower
boundary, the factory operates below its optimal capacity.1
1When both sides are not fixed, it degenerates to the lower
bound. We discuss it in details in Sec. .
arXiv:2401.11418v1  [cs.LG]  21 Jan 2024
Unbalanced OT (Chizat et al. 2018) is an OT variant to
mitigate this inconsistency between source and target distri-
butions. It relaxes the equality by only penalizing marginal
deviation using divergences e.g. 𝑙1, 𝑙2, KL divergence, and
etc. This method transforms the original hard constraints
(equations) into penalty terms, allowing for a relaxation of
the constraints and increasing the degree of freedom in trans-
portation. However, the degree of relaxation can only be
controlled via the penalty coefficient, which is not directly
concerning the desired range of transportation results.
In this paper, we propose and term a new variant of OT
called Double Bounded Optimal Transport (DB-OT), which
allows the sum of coupling column to be in a range rather
than a number, i.e., replacing the original equality con-
straint with two inequality constraints. Under this formu-
lation, we further provide theoretical results with proof for
DB-OT. Additionally, we propose three Sinkhorn-like algo-
rithms based on entropic DB-OT. We also show that our re-
sults also inspire downstream applications.
Inspired
by
the
OT-based
topic
model
in
OT-
LDA (Huynh, Zhao, and Phung 2020), we propose a
barycenter-based clustering method using DB-OT. This
method allows for control of the sample quantity in each
cluster, thereby avoiding the clustering of isolated or
minority points within a cluster (Wang and Su 2011). Note
that our barycenter-based clustering method is not limited
to Euclidean space but can also be extended to other spaces,
such as the Wasserstein space (Agueh and Carlier 2011).
This extension allows our method to be generalized to
handle more complex data, including text data.
Another application is using DB-OT for unbalanced im-
age recognition. Specifically, to our best knowledge, we un-
cover the relation between classification and OT: training a
classifier is equivalent to optimizing Inverse OT (Shi et al.
2023a; Stuart and Wolfram 2020; Chiu, Wang, and Shafto
2022), while inference in testing corresponds to optimizing
OT problem. With this view, we first apply DB-OT to im-
prove the loss for long-tailed data training. Different from
traditional view using Softmax-based cross-entropy loss for
learning, we increase the column-sum constraints for learn-
ing the representations and thus the Softmax generalizes to
Sinkhorn-like iterations and the well-known balanced Soft-
max (Ren et al. 2020) can be a special case with only one
iteration. Then we apply our DB-OT technique to inference
in testing process and our approach is agnostic to the way
how the classifier is trained. We successfully adopt our DB-
OT inference technique to long-tailed, uniform, and reverse
long-tailed datasets.The contributions of our work are:
1) We propose DB-OT to handle the case that the targets
of transportation fall in a double-bounded range, which is
an important yet under-studied setting for practical machine
learning, e.g. cluster size-controllable clustering and long-
tailed classification. We formulate DB-OT with the Kan-
torovich form by replacing the column equality constraints
with double-bounded inequalities.
2) We solve DB-OT using entropic regularization and pro-
vide theoretical results, such as the static Schr¨odinger form,
solution property, and dual form. To calculate the optimal
solution, we propose a Bregman iterative algorithm based on
the static Schr¨odinger form. Additionally, we develop ma-
trix scaling-based methods using Lagrange methods, with
two space-efficient variants derived from the primal and dual
forms of the problem, respectively. Note that the motivation
behind proposing these three algorithms is to align with the
vanilla entropic OT for DB-OT.
3) We propose our barycenter-based method for clus-
tering using DB-OT. It helps control the size of cluster
and thereby avoiding the unwanted over-scattered cluster-
ing with isolated or very few samples in clusters. Besides,
compared with the previous barycenter-based topic model, a
reweighting modification is also proposed to avoid the bias
for barycenter calculation especially in Euclidean space.
4) Finally, our DB-OT is employed in the long-tailed clas-
sification. From the OT perspective, we observe that training
can be viewed as optimizing the Inverse Optimal Transport
(IOT), while inference in the testing can be regarded as min-
imizing the OT itself. Building upon this insight, we apply
DB-OT to enhance the classification loss during training and
also utilize it in the testing process based on a trained model.
Preliminaries and Related Work
Basics of Optimal Transport
As originated from (Kantorovich 1942), the Kantorovich’s
Optimal Transport is to solve a linear program, which is
widely used for many classical problems such as match-
ing (Wang et al. 2013). Specifically, given the cost matrix C
and two histograms (a, b), Kantorovich’s OT involves solv-
ing the coupling P (i.e., the joint probability matrix) by
min
P∈𝑈(a,b) < C, P >,
(1)
where 𝑈(a, b) = {P ∈ 𝑅+
𝑚𝑛|P1𝑛 = a, P⊤1𝑚 = b}. Relaxing
with the entropic regularization (Wilson 1969) is one of the
simple yet efficient methods for solving OT, which can be
formulated as (Liero, Mielke, and Savar´e 2018):
min
P∈𝑈(a,b) < C, P > −𝜖𝐻(P),
(2)
where 𝜖
> 0 is the coefficient for entropic regulariza-
tion 𝐻(P), and the regularization 𝐻(P) can be specified as
𝐻(P) = − < P, log P − 1𝑚×𝑛 > . The objective in Eq. 2 is
𝜖-strongly convex, and thus it has a unique solution, which
can be solved by Sinkhorn algorithms as discussed in (Cu-
turi 2013; Benamou et al. 2015).
In this paper, beyond vanilla OT, we present our formula-
tion for DB-OT and also propose the Sinkhorn algorithm.
Optimal Transport w/ Inequality Constraints
The vanilla OT only considers the equality constraints in
𝑈(a, b) yet inequality constraints are also handled in a
few studies (Caffarelli and McCann 2010; Benamou et al.
2015). For instance, the optimal partial transport (OPT)
problem (Caffarelli and McCann 2010) assumes some mass
variation or partial mass displacement should be handled
for transportation. Thus OPT focuses on transporting only
a fraction of mass 𝑠 ∈ [0, min(||a||1, ||b||1] as cheaply as
possible. Then the constraints in this case is specified as:
C𝑂𝑃𝑇 = {P ∈ 𝑅+
𝑚𝑛|P1𝑛 ≤ a, P⊤1𝑚 ≤ b, 1⊤
𝑚P1𝑛 = 𝑠}. (3)
One
can
get
the
Partial-Wasserstein
distance
as
𝑃𝑊(a, b) = minP∈C𝑂𝑃𝑇
< C, P >. This problem has
been studied in (Caffarelli and McCann 2010; Figalli 2010;
Chapel, Alaya, and Gasso 2020). In particular, (Chizat
et al. 2018; Benamou et al. 2015) propose the numerical
solutions of OPT with entropic regularization. Different
from the above constraints which only give upper bound in
the constraints for mass transportation, in this paper, both
upper and lower bounds are assumed for transportation,
which allows the transportation results to fluctuate within a
certain range rather than an exact number.
Unbalanced Optimal Transport
OT in its vanilla form requires the two histograms a and b
to have the same total mass. Many works have tried to go
beyond this assumption and following the setting in Unbal-
anced OT (Liero, Mielke, and Savar´e 2018; Bai et al. 2023),
the Kantorovich formulation in Eq. 1 is ”relaxed” by only
penalizing marginal deviation using some divergence D𝜓:
min
P∈𝑅+
𝑃𝑚𝑛 < C, P > +𝜏1D𝜓(P1𝑛|a) + 𝜏2D𝜓(P⊤1𝑚|b)
(4)
where 𝜏1 and 𝜏2 control how much mass variations are pe-
nalized as opposed to transportation of the mass and when
𝜏1 = 𝜏2 → +∞. This formulation equals to the original Kan-
torovich for Í
𝑖 a𝑖 = Í
𝑗 b𝑗. When 𝜏1 → +∞ and 𝜏2 < +∞,
Unbalanced OT and DB-OT share a similar characteristic
where the source distribution remains fixed while the tar-
get becomes unfixed. However, unlike DB-OT, it can only
passively adjust the target distribution through 𝜏2 lacking a
direct mechanism to control the targets within a range.
Unbalanced Image Recognition
Unbalanced classification, particularly in the case of long-
tailed recognition, is a well-known challenge that has gar-
nered significant attention in vision and machine learn-
ing (Zhang et al. 2023; He and Garcia 2009; Lin et al. 2017).
Various approaches (Tan et al. 2020; Cui et al. 2019b;
Lin et al. 2017; Kang et al. 2019; Zhang et al. 2021) have
been proposed to address this issue. One popular strategy
involves rebalancing the class distribution in the training
data (Ren et al. 2020; Park et al. 2021). Some methods em-
ploy techniques such as re-sampling (Kang et al. 2019) or
re-weighting (Cui et al. 2019b) to ensure that the model
pays more attention to minority classes during training. Data
augmentation techniques, such as synthesizing additional
samples for underrepresented classes, have also been uti-
lized (Kim, Jeong, and Shin 2020). In this paper, we solve
the unbalanced recognition with the OT perspective. We ap-
ply DB-OT both for learning the representations and in-
ference in testing process, which is different to previous
Bayesian view for classification.
Double-Bounded Optimal Transport
Vanilla OT usually involves two sets of equality constraints.
Our motivation is to modify one type of constraints to the
double bounded form which can be applicable to advanced
clustering and classification.
Formulation of DB-OT
We first assume the source measure 𝛼 = Í𝑚
𝑖=1 a𝑖𝛿𝑥𝑖 and
two bounds of target measure 𝛽𝑑
= Í𝑛
𝑗=1 b𝑑
𝑖 𝛿𝑦𝑗, 𝛽𝑢
=
Í𝑛
𝑗=1 b𝑢
𝑖 𝛿𝑦𝑗 where a ≥ 0 and b𝑢 ≥ b𝑑 ≥ 0. Our purpose is
to transport the source samples from 𝛼 to the target measure
which is defined between 𝛽𝑑 and 𝛽𝑢. By defining the cost
matrix C between {𝑥𝑖} and {𝑦𝑖} (e.g. C𝑖 𝑗 = 𝑑(𝑥𝑖, 𝑦 𝑗) where
𝑑(·, ·) is a distance), we can formulate the Double-Bounded
Optimal Transport (DB-OT) as
min
𝑃∈C(a,b𝑢,b𝑑)
< C, P >=
∑︁
𝑖, 𝑗
C𝑖 𝑗P𝑖 𝑗,
(5)
where C(a, b𝑢, b𝑑) is the coupling set defined by con-
straints:
C(a, b𝑢, b𝑑) = {P ∈ R+
𝑚𝑛|P1𝑛 = a, b𝑑 ≤ P⊤1𝑚 ≤ b𝑢}. (6)
Similar to vanilla OT, the above model is still a linear pro-
gram. From Eq. 6, we can see that we replace the original
constraint P⊤1𝑚 = b in Eq. 1 with b𝑑 ≤ P⊤1𝑚 ≤ b𝑢. That
is, we slice P⊤1𝑚 between b𝑑 and b𝑑 to relax the constraint
on P⊤1𝑚. It is obvious that when b = b𝑑 = b𝑢, DB-OT de-
generates to vanilla OT in Eq. 1. In this paper, we mainly
focus on the entropic regularized formulation:
min
𝑃∈C(a,b𝑢,b𝑑) < C, P > −𝜖𝐻(P),
(7)
where 𝜖 is the regularization coefficient and 𝐻(P) is the en-
tropic regularization. It is obvious that the objective in Eq. 7
and the constraint set C(a, b𝑢, b𝑑) are both convex and thus
Eq. 7 has a unique optimal solution. We will discuss its solu-
tion and the algorithm of entropic DB-OT in next subsection.
Why not using double-bounded 𝛼? An intuitive question
arises: why don’t we assume that the 𝛼 distribution is also
constrained within upper and lower bounds? For instance,
let’s assume 𝛼𝑢 = Í𝑚
𝑖=1 a𝑢
𝑖 𝛿𝑥𝑖 and 𝛼𝑑 = Í𝑚
𝑖=1 a𝑑
𝑖 𝛿𝑥𝑖, and
the coupling satisfies a𝑑 ≤ P1𝑛 ≤ a𝑢. In practice, the op-
timal transportation tends to transport mass vertically and
towards the smaller lower bound. Without loss of generality,
let’s assume Í
𝑖 a𝑑
𝑖 < Í
𝑗 b𝑑
𝑗 . In this case, the optimal solu-
tion must satisfy Í
𝑖 𝑗 P𝑖 𝑗 = Í
𝑖 a𝑑
𝑖 (proof provided in oneline
Appendix). As a result, setting an upper bound for the source
distribution does not hold much significance.
Sinkhorn Algorithm Variants for DB-OT
In this subsection, we introduce several properties of DB-
OT and present three corresponding Sinkhorn algorithms,
namely the Bregman iterative algorithm, Sinkhorn Knopp
algorithm, and Dual algorithm for DB-OT, respectively,
which aims to be in line with the algorithms of vanilla OT.
Although these algorithms are derived from different formu-
lations or properties of DB-OT, they are fundamentally inter-
connected and converge to same solutions, which validates
the effectiveness of the algorithms.
■ Variant-I: Bregman Iterations For DB-OT
Similar to the vanilla entropic OT, our entropic DB-OT in
Eq. 7 can also be reformulated to the ”static Schr¨odinger
form” (L´eonard 2012), which exactly learns a projection
under KL divergence.
Proposition 1 (Static Schr¨odinger Form) Redefine a gen-
eral KL divergence in line with (Benamou et al. 2015)
g
𝐾𝐿(P|K) = Í
𝑖 𝑗 P𝑖 𝑗 log P𝑖 𝑗
K𝑖 𝑗 − P𝑖 𝑗 + K𝑖 𝑗. Let K𝑖 𝑗 = 𝑒−C𝑖 𝑗/𝜖 ,
the optimization in Eq. 7 is equivalent to the minimization:
P∗ =
arg min
P∈C(a,b𝑢,b𝑑)
g
𝐾𝐿(P|K).
(8)
Following (Benamou et al. 2015), a new variant of Sinkhorn
algorithm can be devised with iterative Bregman projections
for DB-OT. By splitting the constraint set as C(a, b𝑢, b𝑑) =
C1 ∩ C2 ∩ C3 where C1 = {P ∈ R+
𝑛𝑚|P1𝑛 = a}, C2 = {P ∈
R+
𝑛𝑚|P⊤1𝑚 ≥ b𝑑}, C3 = {P ∈ R+
𝑛𝑚|P⊤1𝑚 ≤ b𝑢}. We can
get the KL projection for P under the constraints as:
𝑃𝑟𝑜 𝑗𝐾𝐿
C1 (P) = diag
 a
P1𝑛

P
𝑃𝑟𝑜 𝑗𝐾𝐿
C2 (P) = Pdiag

max

b𝑑
P⊤1𝑚
, 1𝑛

𝑃𝑟𝑜 𝑗𝐾𝐿
C3 (P) = Pdiag

min

b𝑢
P⊤1𝑚
, 1𝑛

,
(9)
where the above division operator between two vectors is to
be understood entry-wise (See details in online Appendix).
Finally, as introduced by (Benamou et al. 2015), assuming
C𝑙 = C𝑙+3 with positive integer 𝑙 as the index of Bregman
iteration, the minimization in Eq. 8 can be solved by with
the iterative projection P(𝑛) = 𝑃𝑟𝑜 𝑗 𝐾𝐿
C𝑛
Figure 2: The results of Barycenter-based clustering, which is performed on data points sampled from 5 Gaussian distributions.
The colors represent the cluster assignments of the samples, and the red crosses denote the centroids/barycenters. Note that in
both OT-LDA and our method without reweighting the barycenter weights, the calculated centroids exhibit a noticeable bias.
problem can be solved by alternating optimization. For each
iteration, we first fix the centroid distributions {𝛽𝑡} to com-
pute the distance 𝐷(𝛼𝑠, 𝛽𝑡) between sample 𝑖 and centroid
𝑗 and then transportation matrix P can be learned from the
source samples to clustered centroids.
Note in our method, we have b𝑑 ≤ P⊤1𝑛 ≤ b𝑢, which
means the coupling between samples and cendroids are con-
trolled within a desired range, avoiding clusters with isolated
or very few samples, as well as dominating clusters.
Specifically, for solving the optimization to clustering
in Eq. 15, we first initialize {𝛽𝑡} with initialization of
Kmeans++ or set them as randomly from {𝛼𝑠}. Then we can
learn by iterating the following two steps:
1) Fixing {𝛽𝑡} to compute P. If the barycenter {𝛽𝑡} is
known, then one can calculate the matrix D where D𝑠𝑡 =
𝐷(𝛼𝑠, 𝛽𝑡). Viewing the matrix D as the cost function, the
optimization of P in Eq. 15 is equal to DB-OT optimization
in Eq. 7 and the algorithm proposed in Sec. can be used
to get the result of P. Here we adopt the Sinkhorn Knopp
algorithm as given in Eq. 11.
2) Fixing P to compute {𝛽𝑡}. Assuming the transportation
P is known, our goal is now to update {𝛽𝑡} as the solution
of the following optimization problem
𝛽𝑡 = arg min
𝛽
∑︁
𝑠
P𝑠𝑡 · 𝑅𝑠𝑡
Í
𝑠′ P𝑠′𝑡 · 𝑅𝑠′𝑡
𝐷(𝛼𝑠, 𝛽)
(16)
where 𝑅𝑠𝑡 = 1 if 𝑡 = arg max𝑡′ P𝑠𝑡′ otherwise 𝑅𝑠𝑡 = 0
given the sample 𝛼𝑠. Here 𝑅𝑠𝑡 are used for re-weighting the
weights of barycenters, which aims to debias the influence
for samples from different clusters. Fig. 2 is the clustering
results and we can find a large bias for cendroids without
re-weighting the barycenter weights.
Under Different Metric Space for 𝐷(·, ·). Clustering can
be performed in different metric spaces by setting the dis-
tance function 𝐷(·, ·) to adapt to various metric settings or
datasets. For example, when 𝐷(·, ·) represents the Euclidean
distance and 𝛼𝑠 corresponds to feature points in Euclidean
space, we aim to cluster 𝛼𝑠 in the Euclidean space. The cen-
troids 𝛽𝑡 can be directly computed as follows:
𝛽𝑡 =
∑︁
𝑠
P𝑠𝑡 · 𝑅𝑠𝑡
Í
𝑠′ P𝑠′𝑡 · 𝑅𝑠′𝑡
𝛼𝑠.
(17)
Exactly if we set 𝑅𝑠𝑡 = 1 for all 𝑠 and 𝑡 (i.e. without
reweighting as shown in the second row of Fig. 2), the
barycenters can be easily influenced by samples from other
clusters. This can lead to a bias in the barycenter calculation,
particularly when the value of 𝜖 is not sufficiently small.
In addition to Euclidean space, clustering can also be per-
formed in Wasserstein space. In this case, we assume that
𝛼𝑠 is no longer a set of feature points but a collection of
probability measures, i.e., 𝛼𝑠 = Í
𝑖 a𝑠
𝑖 𝛿𝑥𝑖. In this setting, the
computation of the barycenter in Eq. 16 does not have an an-
alytical solution but requires iterative methods, as proposed
in (Benamou et al. 2015) for barycenter calculation.
Advantages for our Barycenter-based Clustering. Our
barycenter-based clustering is essentially a unified cluster-
ing framework. Compare to the previous clustering works
(e.g. Kmeans (Ahmed, Seraj, and Islam 2020) or OT-
LDA (Huynh, Zhao, and Phung 2020)), the advantages can
be summarized as follows: 1) Our clustering methods apply
DB-OT to calculate the matching between samples and cen-
troids, providing better controllability. As shown in Fig. 3,
by setting b𝑑 and b𝑢, we can constrain the number of sam-
ples in each cluster to a certain extent, thereby avoiding
clusters with isolated or very few samples, as well as clus-
ters that dominate the majority of the data; 2) Our cluster-
ing methods can be applied not only to clustering feature
points but also to clustering probability measures. For ex-
Figure 3: Clustering distribution and the pixel-wise mean centroids (forming into numbers) on MNIST. Our results are well
controlled within the bounds, and kmeans cannot satisfy this property resulting in more scattered clusters of varying size.
ample, as shown in Fig. 3, we can perform clustering on the
MNIST dataset in the Wasserstein space. In this case, we no
longer consider images as point vectors but treat the pixel
values as histograms, and the pixel locations are used to
compute the cost matrix. Moreover, this clustering method
can also be used for text clustering (e.g., topic modeling),
making it more adaptable to different types of datasets; 3)
Compared to previous work in OT-LDA (Huynh, Zhao, and
Phung 2020), we introduce a reweighting of the barycenter
weights, which helps to mitigate the biases in centroid com-
putation, as shown in Fig. 2.
DB-OT for Long-tailed Classification
Motivation of using DB-OT in classification. Here we ap-
ply DB-OT for Long-tailed unbalanced classification. Fol-
lowing the works (Shi et al. 2023a,b), the Inverse Optimal
Transport (Li et al. 2019; Stuart and Wolfram 2020; Chiu,
Wang, and Shafto 2022) can be set as a bi-level optimization
for classification as
min
𝜃 𝐾𝐿( ˜P|P𝜃)
s.t.
P𝜃 = arg min
P∈𝑈 < C𝜃, P > −𝜖𝐻(P).
(18)
Here 𝑈 is the set of couplings and when the set 𝑈 = 𝑈(a) =
{P ∈ R+
𝑚×𝑛|P1𝑚 = a}, this optimization is equivalent to:
min
𝜃 L = −
∑︁
𝑖 𝑗
˜P𝑖 𝑗 log
 
exp(−C𝜃
𝑖 𝑗/𝜖)
Í𝑚
𝑘=1 exp(−C𝜃
𝑖𝑘/𝜖))
!
+ 𝑐𝑜𝑛𝑠𝑡𝑎𝑛𝑡,
(19)
where ˜P𝑖 𝑗 is the ground truth matrix defined by the super-
vised one-hot labels and a = 1/𝑚. The proof process is sim-
ilarly discussed in (Shi et al. 2023a). Specifically, for the
ground truth, ˜P𝑖 𝑗 = 1 if 𝑗 is the label index of sample 𝑖 oth-
erwise ˜P𝑖 𝑗 = 0. The loss in Eq. 19 is exactly equal to the
Softmax-CrossEntropy loss if we set C𝜃
𝑖 𝑗 = 𝑐 − 𝑙𝑖 𝑗 where
𝑐 is a large enough constant and 𝑙𝑖 𝑗 is the logits of sam-
ple 𝑖 on class 𝑗. The equivalence between Inverse OT under
𝑈(a) and Softmax-CrossEntropy loss motivate us that we
can adopt different constraints (e.g. C(a, b𝑢, b𝑑)) instead of
𝑈(a). Then in the following, we show our main idea that
classification learning can be viewed as optimizing Inverse
OT and Classification inference is learning OT.
Training via Inverse DB-OT. As discussed above, we apply
DB-OT by setting 𝑈 = C(a, b𝑢, b𝑑) in Eq. 18 for long-tailed
classification, which assumes that the labels of training data
have a known long-tailed distribution r. We set a = 1/𝑚 and:
b𝑢 = (1 + 𝛿)r
and b𝑑 = (1 − 𝛿)r,
(20)
where 𝛿 ∈ (0, 1) is the bound rate for r. We adopt the Breg-
man method for calculating P𝜃 with 𝐾 iteration. We find
when 𝐾 = 1 and 𝛿 = 0, the Balanced Softmax (Ren et al.
2020) is a special case of our loss, which validates the effec-
tiveness of our theory.
Testing-time Inference with DB-OT. Here we focus on us-
ing DB-OT for inference in testing process. Note our infer-
ence method is orthogonal to the training method proposed
in Sec. , which means our inference can be used for different
classifiers. For the testing inference, we treat all the training
as a process of feature learning and then with the feature
learned, we can define the cost matrix given each batch data
and match the features and labels with OT. For the long-
tailed classification, the inference is as follows:
min
P∈C(a,b𝑢,b𝑑) < C, P > −𝜖𝐻(P).
(21)
Here C𝑖 𝑗 = 𝑐−𝑙𝑖 𝑗 with large enough value 𝑐 to guarantee the
positiveness of the the cost. Meanwhile, we set b𝑢 and b𝑑 as
Eq. 20, which enables predictions to fluctuate both upward
and downward, taking into account the randomness of batch
data sampling. During the testing process, r can be config-
ured as a known long-tailed distribution, uniform distribu-
tion, reverse long-tailed distribution, or any other appropri-
ate distribution, which is a main advantage of our method.
Experiments
We conduct experiments on both clustering and classifica-
tion. To showcase the advantage of DB-OT on fine-grained
controlling the behavior of clustering under bounded sizes
of clusters, and classification in long-tail cases.
Experiments on Size-controlled Clustering
We first study our clustering setting which each cluster’s
size is bounded by a certain range, using our DB-OT func-
tion with Barycenter. The experimental results on Gaussian
Method
CIFAR10-LT
CIFAR100-LT
ImageNet-LT
Many
Few
All
Many
Medium
Few
All
Many
Medium
Few
All
Vanilla Softmax
77.4
68.9
74.9
75.8
48.2
11.0
42.0
57.3
26.2
3.1
35.0
LDAM (Cao et al. 2019)
80.5
65.2
75.9
75.7
50.6
11.5
42.9
57.3
27.6
4.4
35.9
Balanced Softmax
82.2
71.6
79.0
70.3
50.4
26.5
47.0
52.5
38.6
17.8
41.1
Focal Loss (Lin et al. 2017)
79.6
58.4
73.3
76.1
46.9
11.1
41.7
57.3
27.6
4.4
35.9
LogitAdjust (Menon et al. 2020)
80.0
35.3
66.6
75.7
39.2
4.1
36.5
54.2
14.0
0.4
27.6
CB-CE (Cui et al. 2019b)
76.6
70.7
74.8
53.2
48.8
13.3
36.3
35.3
32.1
21.2
31.9
CB-FC (Cui et al. 2019b)
76.6
70.7
74.8
53.2
48.8
13.3
36.3
35.3
32.1
21.2
31.9
DB-OT Loss (ours)
82.4
80.8
81.9
70.4
53.0
26.6
47.9
53.5
39.0
17.4
41.6
Table 1: Top-1 accuracy (%) for long-tailed image classification with 200 imbalanced factor on three popular LT datasets.
Testing Inference
Vanilla Softmax Loss
Balanced Softmax
Focal Loss
Logit Adjustment Loss
LT
Uniform
Reverse LT
LT
Uniform
Reverse LT
LT
Uniform
Reverse LT
LT
Uniform
Reverse LT
Vanilla Softmax
71.6
42.0
18.7
66.5
47.0
20.1
68.5
41.7
18.4
69.3
36.4
4.8
classifier normalize(Kang et al. 2019)
70.0
45.2
16.8
54.9
42.4
28.6
67.0
41.3
13.3
70.5
40.4
7.9
Class-aware bias(Menon et al. 2020)
68.0
41.3
14.5
41.5
45.0
18.6
54.6
40.0
14.1
70.8
36.4
7.7
DB-OT inference (ours)
71.8
48.5
36.6
71.4
47.8
34.6
68.7
44.1
34.4
71.3
44.8
32.3
Table 2: Top-1 accuracy (%) of CIFAR-100 for the comparison of different testing inference methods given four trained models.
mixture synthetic datasets with 2D 150 points are shown in
Fig. 2. We can find that the OT-LDA and our method with-
out reweighting exhibit a noticeable bias and re-weighting
the barycenter weights can overcome this issue with Eq. 16.
We also evaluate the top-1 accuracy within the cluster and
the results of OT-LDA, ours (without and with reweight-
ing) are 82.67%, 88.00%, and 100.00% respectively, which
shows the superiority of our method. We also do the cluster-
ing experiments on MNIST with three different methods (i.e.
K-means, DB-OT under Euclidean space and DB-OT under
Wasserstein space). We select parts of MNIST data (12 im-
ages in each class) and set 16 clusters for the experiments.
The results are shown in Fig. 3, and we can find that our
method can be more controllable for the number of samples
in each cluster. Note the bounds are to control the column
probability sum of the coupling and thus it does not strictly
meet the bound range for sample quantity.
Experiments on Long-tailed Classification
We evaluate our two methods – both for using DB-OT to
improve the loss and adopting the DB-OT for classification
inference in the testing process. These two methods are par-
allel to each other as the former aims to learn a better repre-
sentation and the latter is to get a more accurate prediction
based on a known testing label ratio. We do the experiments
on CIFAR10-LT, CIFAR100-LT (Krizhevsky, Hinton et al.
2009), ImageNet-LT (Liu et al. 2019) for image classifica-
tion. For a fair comparison, all methods share the same net-
work backbone and hyperparameters, including the learning
rate. More detailed information about the experimental set-
tings is given in the online Appendix.
Improvements of DB-OT based loss. To evaluate the per-
formance of our DB-OT based loss, following (Ren et al.
2020), we start with a vanilla Softmax pretrained model and
train with our DB-OT based loss to improve the represen-
tations. We use the corresponding balanced testing dataset
for evaluation, where its labels are uniformly distributed. We
report top-1 accuracy as the evaluation metric. Specifically,
for CIFAR10-LT, we report accuracy on two sets of classes
in detail: Many-shot (more than 100 images) and Few-shot
(less than 100 images). For CIFAR00-LT and ImageNet-
LT, we report accuracy on three sets: Many-shot (more than
100 images), Medium-shot (20 ∼ 100 images), and Few-
shot (less than 20 images). The experiments for unbalanced
image classification are all conducted with an imbalanced
factor of less than 200, which is defined as the ratio of the
number of training instances in the largest class to the small-
est (Ren et al. 2020). The results for long-tailed classification
are presented in Table 1. From a comprehensive perspec-
tive, our approach achieves the best average accuracy across
the entire dataset. When examining the results for different
data splits, our method particularly outperforms the others
on subsets with fewer images (i.e., median or few-shot).
Performance of testing inference. To evaluate the perfor-
mance of DB-OT based inference, we use the corresponding
long-tailed (LT), uniform, and Reverse LT testing dataset for
evaluation. We report top-1 accuracy as the evaluation met-
ric. Specifically, we compare our method with vanilla Soft-
max, classifier normalization and class-aware bias, which
are discussed in (Wu et al. 2021). Table 2 shows the com-
parison results for all the models and different testing data
in CIFAR100. Our inference method outperforms and can
achieve a great improvement when the testing data is reverse
long-tailed distributed and the model trained by vanilla soft-
max performs the best using our DB-OT testing inference
though it may fail with vanilla softmax prediction in testing.
Ablation study and Additional Results. We do the ablation
study by varying iterations and 𝛿 values of Sinkhorn algo-
rithm of DB-OT and more detailed clustering experiments
comparing with more baselines in online appendix.
Conclusion
We have presented the so-called double-bounded optimal
transport, with theoretical analysis and further derive three
variants of algorithms to solve the problem. We then test our
technique for the challenging yet realistic tasks of cluster-
size bounded clustering, as well as long-tailed image recog-
nition. Experimental results clearly verify its effectiveness.
Acknowledgement
This work was partly supported by NSEC (92370201,
62222607).
References
Agueh, M.; and Carlier, G. 2011. Barycenters in the Wasser-
stein space. SIAM Journal on Mathematical Analysis, 43(2):
904–924.
Ahmed, M.; Seraj, R.; and Islam, S. M. S. 2020. The k-
means algorithm: A comprehensive survey and performance
evaluation. Electronics, 9(8): 1295.
Arjovsky, M.; Chintala, S.; and Bottou, L. 2017. Wasserstein
GAN. ICML.
Bai, Y.; Schmitzer, B.; Thorpe, M.; and Kolouri, S. 2023.
Sliced optimal partial transport.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 13681–13690.
Benamou, J.-D.; Carlier, G.; Cuturi, M.; Nenna, L.; and
Peyr´e, G. 2015. Iterative Bregman projections for regular-
ized transportation problems. SIAM Journal on Scientific
Computing, 37(2): A1111–A1138.
Bregman, L. M. 1967.
The relaxation method of finding
the common point of convex sets and its application to the
solution of problems in convex programming. USSR compu-
tational mathematics and mathematical physics, 7(3): 200–
217.
Caffarelli, L. A.; and McCann, R. J. 2010. Free boundaries
in optimal transport and Monge-Ampere obstacle problems.
Annals of mathematics, 673–730.
Cao, K.; Wei, C.; Gaidon, A.; Arechiga, N.; and Ma, T. 2019.
Learning imbalanced datasets with label-distribution-aware
margin loss. Advances in neural information processing sys-
tems, 32.
Caron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.;
and Joulin, A. 2020. Unsupervised learning of visual fea-
tures by contrasting cluster assignments. Advances in neural
information processing systems, 33: 9912–9924.
Chapel, L.; Alaya, M. Z.; and Gasso, G. 2020. Partial op-
timal tranport with applications on positive-unlabeled learn-
ing. Advances in Neural Information Processing Systems,
33: 2903–2913.
Chen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020.
A simple framework for contrastive learning of visual repre-
sentations. In International conference on machine learning,
1597–1607. PMLR.
Chiu, W.-T.; Wang, P.; and Shafto, P. 2022. Discrete Proba-
bilistic Inverse Optimal Transport. In International Confer-
ence on Machine Learning, 3925–3946. PMLR.
Chizat, L.; Peyr´e, G.; Schmitzer, B.; and Vialard, F.-X. 2018.
Scaling algorithms for unbalanced optimal transport prob-
lems. Mathematics of Computation, 87(314): 2563–2609.
Cui, L.; Qi, X.; Wen, C.; Lei, N.; Li, X.; Zhang, M.; and
Gu, X. 2019a. Spherical optimal transportation. Computer-
Aided Design, 115: 181–193.
Cui, Y.; Jia, M.; Lin, T.-Y.; Song, Y.; and Belongie, S. 2019b.
Class-balanced loss based on effective number of samples.
In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, 9268–9277.
Cuturi, M. 2013. Sinkhorn distances: Lightspeed compu-
tation of optimal transportation distances.
arXiv preprint
arXiv:1306.0895.
Figalli, A. 2010.
The optimal partial transport problem.
Archive for rational mechanics and analysis, 195(2): 533–
560.
Gulrajani, I.; Ahmed, F.; Arjovsky, M.; Dumoulin, V.; and
Courville, A. 2017.
Improved Training of Wasserstein
GANs. In NIPS.
He, H.; and Garcia, E. A. 2009. Learning from imbalanced
data. IEEE Transactions on knowledge and data engineer-
ing, 21(9): 1263–1284.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-
ual learning for image recognition. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, 770–778.
Hu, W.; Fey, M.; Zitnik, M.; Dong, Y.; Ren, H.; Liu, B.;
Catasta, M.; and Leskovec, J. 2020. Open graph benchmark:
Datasets for machine learning on graphs. Advances in neural
information processing systems, 33: 22118–22133.
Huynh, V.; Zhao, H.; and Phung, D. 2020.
Otlda: A
geometry-aware optimal transport approach for topic mod-
eling. Advances in Neural Information Processing Systems,
33: 18573–18582.
Kang, B.; Xie, S.; Rohrbach, M.; Yan, Z.; Gordo, A.; Feng,
J.; and Kalantidis, Y. 2019.
Decoupling representation
and classifier for long-tailed recognition.
arXiv preprint
arXiv:1910.09217.
Kantorovich, L. 1942. On the transfer of masses (in Rus-
sian). In Doklady Akademii Nauk, volume 37, 227–229.
Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola,
P.; Maschinot, A.; Liu, C.; and Krishnan, D. 2020. Super-
vised contrastive learning. Advances in neural information
processing systems, 33: 18661–18673.
Kim, J.; Jeong, J.; and Shin, J. 2020.
M2m: Imbalanced
classification via major-to-minor translation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition, 13896–13905.
Krizhevsky, A.; Hinton, G.; et al. 2009.
Learning Multi-
ple Layers of Features from Tiny Images. Technical report,
Citeseer.
L´eonard, C. 2012.
From the Schr¨odinger problem to the
Monge–Kantorovich problem. Journal of Functional Anal-
ysis, 262(4): 1879–1920.
Li, R.; Ye, X.; Zhou, H.; and Zha, H. 2019. Learning to
match via inverse optimal transport.
Journal of machine
learning research, 20.
Li, Y.; Mo, Y.; Shi, L.; and Yan, J. 2022. Improving gener-
ative adversarial networks via adversarial learning in latent
space. Advances in Neural Information Processing Systems,
35: 8868–8881.
Liero, M.; Mielke, A.; and Savar´e, G. 2018.
Opti-
mal entropy-transport problems and a new Hellinger–
Kantorovich distance between positive measures.
Inven-
tiones mathematicae, 211(3): 969–1117.
Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; and Doll´ar, P.
2017. Focal loss for dense object detection. In Proceedings
of the IEEE international conference on computer vision,
2980–2988.
Liu, Z.; Miao, Z.; Zhan, X.; Wang, J.; Gong, B.; and Yu,
S. X. 2019. Large-scale long-tailed recognition in an open
world. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition, 2537–2546.
Menon, A. K.; Jayasumana, S.; Rawat, A. S.; Jain, H.; Veit,
A.; and Kumar, S. 2020. Long-tail learning via logit adjust-
ment. arXiv preprint arXiv:2007.07314.
Park, S.; Lim, J.; Jeon, Y.; and Choi, J. Y. 2021. Influence-
balanced loss for imbalanced visual classification. In Pro-
ceedings of the IEEE/CVF International Conference on
Computer Vision, 735–744.
Ren, J.; Yu, C.; Ma, X.; Zhao, H.; Yi, S.; et al. 2020. Bal-
anced meta-softmax for long-tailed visual recognition. Ad-
vances in neural information processing systems, 33: 4175–
4186.
Saad-Eldin, A.; Pedigo, B. D.; Priebe, C. E.; and Vogelstein,
J. T. 2021. Graph matching via optimal transport. arXiv
preprint arXiv:2111.05366.
Shi, L.; Zhang, G.; Zhen, H.; Fan, J.; and Yan, J. 2023a.
Understanding and Generalizing Contrastive Learning from
the Inverse Optimal Transport Perspective. in ICML.
Shi, L.; Zhen, H.; Zhang, G.; and Yan, J. 2023b. Relative
Entropic Optimal Transport: a (Prior-aware) Matching Per-
spective to (Unbalanced) Classification. In Thirty-seventh
Conference on Neural Information Processing Systems.
Stuart, A. M.; and Wolfram, M.-T. 2020. Inverse optimal
transport.
SIAM Journal on Applied Mathematics, 80(1):
599–619.
Tan, J.; Wang, C.; Li, B.; Li, Q.; Ouyang, W.; Yin, C.; and
Yan, J. 2020. Equalization loss for long-tailed object recog-
nition. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition, 11662–11671.
Wang, F.; and Liu, H. 2021. Understanding the behaviour
of contrastive loss. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, 2495–
2504.
Wang, J.; and Su, X. 2011. An improved K-Means clustering
algorithm. In 2011 IEEE 3rd international conference on
communication software and networks, 44–46. IEEE.
Wang, R.; Yan, J.; and Yang, X. 2019. Learning Combina-
torial Embedding Networks for Deep Graph Matching. In
ICCV, 3056–3065.
Wang, R.; Yan, J.; and Yang, X. 2021. Neural graph match-
ing network: Learning lawler’s quadratic assignment prob-
lem with extension to hypergraph and multiple-graph match-
ing. IEEE Transactions on Pattern Analysis and Machine
Intelligence.
Wang, W.; Slepˇcev, D.; Basu, S.; Ozolek, J. A.; and Rohde,
G. K. 2013. A linear optimal transportation framework for
quantifying and visualizing variations in sets of images. In-
ternational journal of computer vision, 101(2): 254–269.
Wilson, A. G. 1969. The use of entropy maximising models,
in the theory of trip distribution, mode split and route split.
Journal of transport economics and policy, 108–126.
Wu, T.; Liu, Z.; Huang, Q.; Wang, Y.; and Lin, D. 2021. Ad-
versarial robustness under long-tailed distribution. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, 8659–8668.
Xu, H.; Luo, D.; Zha, H.; and Duke, L. C. 2019. Gromov-
wasserstein learning for graph matching and node embed-
ding.
In International conference on machine learning,
6932–6941. PMLR.
Zhang, Y.; Kang, B.; Hooi, B.; Yan, S.; and Feng, J.
2021. Deep long-tailed learning: A survey. arXiv preprint
arXiv:2110.04596.
Zhang, Y.; Kang, B.; Hooi, B.; Yan, S.; and Feng, J. 2023.
Deep long-tailed learning: A survey. IEEE Transactions on
Pattern Analysis and Machine Intelligence.
Appendix
Double Bounded on the source and target
Here we discuss why don’t we assume that the 𝛼 distribution
is also constrained within upper and lower bounds? For in-
stance, let’s assume 𝛼𝑢 = Í𝑚
𝑖=1 a𝑢
𝑖 𝛿𝑥𝑖 and 𝛼𝑑 = Í𝑚
𝑖=1 a𝑑
𝑖 𝛿𝑥𝑖,
and the coupling satisfies a𝑑 ≤ P1𝑛 ≤ a𝑢. In practice, the
optimal transportation tends to transport mass vertically and
towards the smaller lower bound. Without loss of general-
ity, let’s assume Í
𝑖 a𝑑
𝑖 < Í
𝑗 b𝑑
𝑗 . In this case, the optimal
solution must satisfy Í
𝑖 𝑗 P𝑖 𝑗 = Í
𝑖 a𝑑
𝑖 .
We adopt the opposite approach. Without loss of gener-
ality, we assume that Í
𝑖 a𝑑
𝑖 < Í
𝑖 𝑗 P𝑖 𝑗 < Í
𝑗 b𝑑
𝑗 where P is
the optimal solution satisfying P = arg minP′∈C < C, P′ >
where C is the constraint set for double bounds for source
and target histograms. Then there must exist the dimension
𝑖 and value 𝛿1 > 0 satisfying
a𝑑
𝑖 + 𝛿1 <
∑︁
𝑗
P𝑖 𝑗.
(22)
There must exist the dimension 𝑗 and value 𝛿2 > 0 satisfying
∑︁
𝑖
P𝑖 𝑗 + 𝛿2 <
∑︁
𝑗
b𝑑
𝑗 .
(23)
Now we set P′
𝑖 𝑗 = P𝑖 𝑗 −min(𝛿1, 𝛿2), the solution P′ also sat-
isfy a𝑑 ≤ P′1𝑛 ≤ a𝑢 and b𝑑 ≤ P′⊤1𝑚 ≤ b𝑢. However, we
know < C, P′ ><< C, P >, which contradicts the assump-
tion that P is the optimal solution.
Proof and discussion on Prop. 1
We rewrite the Prop. 1 as:
Proposition 4 (Static Schr¨odinger Form) Redefine a gen-
eral KL divergence in line with (Benamou et al. 2015)
g
𝐾𝐿(P|K) =
∑︁
𝑖 𝑗
P𝑖 𝑗 log P𝑖 𝑗
K𝑖 𝑗
− P𝑖 𝑗 + K𝑖 𝑗.
Let K𝑖 𝑗 = 𝑒−C𝑖 𝑗/𝜖 , the optimization in Eq. 7 is equivalent to
the minimization:
P∗ =
arg min
P∈C(a,b𝑢,b𝑑)
g
𝐾𝐿(P|K).
Proof 1 From the definition of g
𝐾𝐿 and K𝑖 𝑗 = 𝑒−C𝑖 𝑗/𝜖 , we
have
min
P∈C
g
𝐾𝐿(P|K) = min
P∈C
∑︁
𝑖 𝑗

P𝑖 𝑗 log P𝑖 𝑗 − P𝑖 𝑗 − P𝑖 𝑗 log 𝑒−C𝑖 𝑗/𝜖 
= min
P∈C
∑︁
𝑖 𝑗

P𝑖 𝑗
Similarly, in the third equation, if (P⊤1𝑚) 𝑗 > b𝑢
𝑗, the corre-
sponding column is normalized to match b𝑢
𝑗; otherwise, it
remains unchanged. So the projection in Eq. 9 can also be
simplified as the algorithm of row sum to a and column sum
to b algorithms given the iteration number 𝑙:
P(𝑙+1/2) = diag

a
P(𝑙)1𝑛

P(𝑙),
P(𝑙+1) = P(𝑙+1/2)diag

b
(P⊤)(𝑙+1/2)1𝑚

,
(36)
where b is specified as
b𝑗 =


b𝑑
𝑗 ,
((P⊤)(𝑙+1/2)1𝑚) 𝑗 ≤ b𝑑
𝑗 ,
b𝑢
𝑗 ,
((P⊤)(𝑙+1/2)1𝑚) 𝑗 ≥ b𝑢
𝑗 ,
((P⊤)(𝑙+1/2)1𝑚) 𝑗,
otherwise.
(37)
Proof and discussion on Prop. 2
We rewrite the Prop. 2 as
Proposition 5 (Solution Property) The optimal solution of
the optimization in Eq. 7 is unique and has the form
P∗ = diag(u)Kdiag(q ⊙ v)
where q ⊙ v = (q𝑖v𝑖) ∈ R+
𝑛 and the three scaling variables
(u, q, v) satisfy u ∈ 𝑅+
𝑚, 0𝑛 ≤ v ≤ 1𝑛 and q ≥ 1𝑛.
Proof 3 The minimization of Eq. 7 can be specified with La-
grange method:
L =< C, P > −𝜖𝐻(P) − f(P1𝑛 − a) − g(P⊤1𝑚 − b𝑑) − h(P⊤1𝑚 − b𝑢)
(38)
where f ∈ R𝑛, g ≥ 0 and h ≤ 0. Then we can get
𝜕L
𝜕P𝑖 𝑗
= log C𝑖 𝑗 − 𝜖 log P𝑖 𝑗 − f𝑖 − g𝑗 − h𝑗 = 0.
(39)
Then we can get
P = diag(𝑒f/𝜖 )𝑒−C/𝜖 diag(𝑒(g+h)/𝜖 ).
(40)
Due to g ≥ 0 and h ≤ 0, we set
u = 𝑒f/𝜖
q = 𝑒g/𝜖 ≥ 1𝑛
v = 𝑒h/𝜖 ≤ 1𝑛
(41)
then we can get the optimal solution satisfying
P = diag(u)Kdiag(q ⊙ v).
(42)
Then we prove Eq. 11, i.e.
u(𝑙+1) =
a
𝐾(v(𝑙) ⊙ q(𝑙))
,
q(𝑙+1) = max

b𝑑
u(𝑙+1)𝐾 ⊙ v(𝑙) , 1𝑛

,
v(𝑙+1) = min

b𝑢
u(𝑙+1)𝐾 ⊙ q(𝑙+1) , 1𝑛

.
(43)
Proof 4 Due to Eq. 42 and the constraints P1𝑛 = a , we
know
u ⊙ (K(q ⊙ v)) = a,
(44)
thus we have
u =
a
K(q ⊙ v)
(45)
And we also have b𝑑 ≤ P⊤1𝑚 ≤ b𝑢. When q𝑗 = 1, we can
get g 𝑗 = 0 and (P1𝑚) 𝑗 > b𝑑
𝑗 , while q𝑗 > 1, we can get
g𝑗 > 0 and (P1𝑚) 𝑗 = b𝑑
𝑗 , then
(q ⊙ v) 𝑗 · (K⊤u) 𝑗 = b𝑑
𝑗 .
(46)
Thus we have
q𝑗 =
b𝑑
𝑗
v𝑗 · (K⊤u) 𝑗
.
(47)
Combining the situation for q𝑗 = 1, we have
q = max(
b𝑑
v ⊙ (K⊤u) , 1𝑛)
(48)
The proof of equation
v = max(
b𝑢
q ⊙ (K⊤u) , 1𝑛)
(49)
is similar to the proof of q in Eq. 48. We do not repeat here.
Proof and discussion on Prop. 3
We rewrite the Prop. 3 here.
Proposition 6 (Dual Formulation) From the optimization
in Eq. 7, we can get its dual formulation by maximizing
𝐿 =< f, a > + < g, b𝑑 > + < h, b𝑢 > −𝜖𝐵(f, g, h)
(50)
where f ∈ R𝑛, g ≥ 0 and h ≤ 0 are the corresponding dual
variables and 𝐵(f, g, h) =< 𝑒f/𝜖 , K𝑒(g+h)/𝜖 >.
Proof 5 For the coupling, we have
P𝑖 𝑗 = 𝑒f𝑖/𝜖 𝑒−C𝑖 𝑗/𝜖 𝑒(g𝑗+h𝑗 )/𝜖 ,
(51)
where f ∈ R𝑛, g ≥ 0 and h ≤ 0.Then the objective function
is specified as
< 𝑒f/𝜖 , C ⊙ K𝑒(g+h)/𝜖 > −𝜖𝐻(P)
(52)
where −𝜖𝐻(P) is
−𝜖𝐻(P) =𝜖 < P, log P − 1𝑛×𝑚 >
= < P, f1⊤
𝑚 + 1𝑛(g + h)⊤ − C − 𝜖1𝑛×𝑚 >
=− < 𝑒−f/𝜖 , C ⊙ K𝑒−(g+h)/𝜖 > + < f, a >
+ < g, b𝑑 > + < h, b𝑢 > + < 𝑒f/𝜖 , K𝑒(g+h)/𝜖 >
.
(53)
So
the
following
holds
where
𝐵(f, g, h)
=<
𝑒f/𝜖 , K𝑒(g+h)/𝜖 >:
min
P∈C < C, P > −𝜖𝐻(P)
=
max
f,g≥0,h≤0 < f, a > + < g, b𝑑 > + < h, b𝑢 > −𝜖𝐵(f, g, h)
(54)
Therefore, the first term in Eq. 52 cancels out with the first
term in the entropy above. The remaining terms are those
appearing in Eq. 14.
Figure 4: The result of clustering with different bounds. The top-1 accuracy is 72.50, 70.83, 75.00, 70.00, 70.00, 68.33 respec-
tively. The six histograms indicate the number of each class and the balck dotted line is the bound of each case.
Table 3: Top-1 accuracy (%) for CIFAR10-LT image classi-
fication with different inference methods.
Test Inference
DB-OT loss
CBCE
LT
Uniform
Revers LT
LT
Uniform
Revers LT
Vanilla Softmax
85.3
79.7
64.1
6.2
54.6
91.6
classifier normalize(Kang et al. 2019)
79.1
81.1
73.7
6.5
54.9
91.6
Class-aware bias(Menon et al. 2020)
46.5
79.7
35.8
4.8
54.6
91.7
DB-OT inference (ours)
89.5
81.9
82.4
65.5
67.8
92.1
Experimental Details
Hardware and Software
We use Intel Core i9-10920X CPU @ 3.50GHz with Nvidia
GeForce RTX 2080 Ti GPU for model training. We take sin-
gle GPU to train models on CIFAR-10-LT, CIRFAR-100-LT,
ImageNet-LT. We implement our proposed algorithm with
PyTorch-2.0.1 for all experiments.
More Experimental Setting Details and Results for
Image Classification
We perform the long-tailed image classification task on
CIFAR10-LT, CIFAR100-LT (Krizhevsky, Hinton et al.
2009), and Imagenet-LT (Liu et al. 2019) datasets, and eval-
uate on balanced testing data by reporting top-1 accuracy.
For CIFAR10 and CIFAR100, the experiments of image
classificaiton tasks are based on ResNet32 (He et al. 2016)
as the backbone with 0.02 and 0.005 learning rate respec-
tively, while for Imagenet dataset, we use ResNet10 for
training with 0.005 learning rate. We train the CIFAR10 and
CIFAR100 data with 20000 and 30000 iterations on a sin-
gle GPU and imbalanced ratio is set as 200, 100, 10, resp.
For testing inference, we construct the function of Softmax,
Classifier Normalization, Class Aware Bias and ours method
to evaluate the accuracy on the testing set. We set the 𝜖 of
sinkhorn as 1 to fit the training function, a as all one vector
and use the number of each class in testing set to determine
the b as sinkhorn bounds.
Table 4: Top-1 accuracy (%) for CIFAR10-LT image classification
with different training and testing iteration.
Training iteration
Testing iteration
iter=10
iter=30
iter=50
iter=70
iter=100
iter=150
final acc (final iter)
iter=1
75.2
76.0
75.7
75.6
77.1
75.0
76.4 (160)
iter=2
80.1
80.8
80.0
80.3
81.0
80.8
80.7 (250)
iter=3
83.6
83.4
83.1
83.1
83.3
84.1
83.5 (220)
iter=4
82.3
82.2
82.5
82.5
83.2
83.6
82.0 (230)
iter=5
78.5
78.2
75.5
76.9
77.3
77.4
77.8 (170)
More Experimental Setting Details and Results for
Clustering
We
perform
the
clustering
task
on
the
subset
of
MNIST(select 12 images each class) and evaluate the effi-
ciency by top-1 accuracy. We set the training iteration as 5
and the inner iteration of sinkhorn has the maximum num-
ber of 1000. As for training, we use the similar method as
K-means to generate the initial clustering centers. We also
construct the evaluating function to show the result of clus-
tering and calculate the top-1 accuracy.
Baselines of Inference in Testing
Classifier Normalization.
This inference method is pro-
posed in (Kang et al. 2019), which reset the logits in testing
as
ℎ 𝑗 = (𝑊 𝑗/||𝑊 𝑗||𝜏)⊤ 𝑓 (𝑥),
(55)
where 𝜏 is the hyper-parameter, 𝑓 (𝑥) is the feature of sample
𝑥 and 𝑊 𝑗 is the linear projection to the class 𝑗. Then we use
the softmax based on ℎ 𝑗 as the prediction results.
Classifier Normalization.
This inference method is pro-
posed in (Menon et al. 2020), which set the new logits in
testing process as
ℎ 𝑗 = 𝑊𝑗 𝑓 (𝑥) − 𝜏 log 𝑛 𝑗
(56)
where 𝑛 𝑗 corresponds to the ratio of long-tailed ratio. Tab.3
shows the results on CIFAR-10.
Ablation Study
Ablation of Varying Bounds on Clustering
The Fig .4 is the centroids of clustering on MNIST using
Wasserstein distance with different 𝜏, which shows the in-
fluence of varing bounds on clustering. Each figure has the
same 𝜖 = 0.001 and the training iteration as 5.
In addition, we also do experiment on not uniform bounds
and evaluate the top-1 accuracy. The results of linear increas-
ing 𝜏 (first number is 0 and the last is 15/16 while the whole
vector is a arithmetic progression) and linear decreasing 𝜏
(the inverse list of linear increasing 𝜏) is 67.50 and 71.67 re-
spectively. We also set the list of 𝜏 as a 16 number of Gaus-
sian, whose sum is 10, with the middle point 8 and the STD
of the list is 5. The top-1 accuracy under this bound is 67.50
while under the corresponding Gaussian list (1 minus the
previous list) the accuracy is 70.00.
Ablation of Varying Iteration on Classification
Because there are sinkhorn function in training and infer-
ence, we do experiment with different iteration on CIFAR10
to find the influence that varying iteration may cause.
Tab.4 is the result of the experiment. We set the iteration
in training loss from 1 to 5 and the iteration in inference as
10, 30, 50, 70, 100, 150. As the sinkhorn in DB-OT method
will stop when the change of the norm of iterative matrix is
smaller than a specific number, the last column is the final
accuracy with the stop iteration in the brackets.
Ablation of Varying Delta on Classification
In order to find the influence of varying delta in both training
loss and the inference function, we try different delta in these
two function and evaluate the top-1 accuracy on CIFAR10 to
see the difference.
From Tab. 5, it’s obvious that the top-1 accuracy reduces
when the inference delta increases, while the accuracy will
reach the highest when training delta between 0.1 to 0.4.
Proof of Eq. 19
Now we show the softmax with the constraints:
𝑈(a) = {P ∈ R𝑚×𝑛
+
|P1𝑛 = a}
(57)
where a = 1/𝑚 and 1𝑚 is the 𝑚-dimensional column vec-
tor whose elements are all ones. With the objective of the
entropic OT:
P𝜃 = arg min
𝑃∈𝑈(a) < C𝜃, P > −𝜖𝐻(P),
(58)
We introduce the dual variable f ∈ 𝑅𝑚. The Lagrangian of
the above equation is:
𝐿(P, f) =< C, P > −𝜖𝐻(P) −
𝑛
∑︁
𝑖=1
f𝑖 · ©­
«
𝑛
∑︁
𝑗=1
P𝑖 𝑗 − 1
𝑚
ª®
¬
(59)
The first order conditions then yield by:
𝜕𝐿(P, f)
𝜕P𝑖 𝑗
= C𝑖 𝑗 + 𝜖 log P𝑖 𝑗 − f𝑖 = 0.
(60)
Thus we have Pij = 𝑒(f𝑖−𝐶 𝜃
𝑖 𝑗 )/𝜖 for every 𝑖 and 𝑗, for optimal
P coupling to the regularized problem. Due to Í
𝑗 P𝑖 𝑗 = 1/𝑚
for every 𝑖, we can calculate the Lagrangian parameter f𝑖 and
the solution of the coupling is given by:
P𝑖 𝑗 =
exp (−C𝜃
𝑖 𝑗/𝜖)
𝑚 Í𝑚
𝑘=1 Q𝑖𝑘 exp (−C𝜃
𝑖𝑘/𝜖) .
(61)
Then in outer minimization, we have
min
𝜃
𝐿 = −
∑︁
𝑖 𝑗
˜P𝑖 𝑗 log
 
exp(−C𝜃
𝑖 𝑗/𝜖)
Í𝑚
𝑘=1 exp(−C𝜃
𝑖𝑘/𝜖))
!
+ 𝑐𝑜𝑛𝑠𝑡𝑎𝑛𝑡,
(62)
Table 5: Top-1 accuracy (%) of CIFAR-10 for the comparison of different inference delta given several trained models with different delta.
models or delta of training
Inference 𝛿 =0
Inference 𝛿 =0.05
Inference 𝛿 =0.1
Inference 𝛿 =0.2
Inference 𝛿 =0.4
Inference 𝛿 =0.8
All
Many
All
Many
All
Many
All
Many
All
Many
All
many
Balanced Softmax
81.0
81.5
81.0
82.2
80.8
83.0
80.2
83.9
79.3
83.4
79.2
83.3
Focal Loss
79.3
78.9
79.3
79.6
79.0
79.9
78.2
80.6
75.3
80.4
72.7
79.7
Ours(𝛿 = 0)
81.8
81.8
81.7
81.8
81.5
81.8
81.0
81.2
80.3
80.2
80.3
80.2
Ours(𝛿 = 0.05)
82.4
82.6
82.2
82.8
81.9
83.3
81.1
83.5
80.4
83.4
80.4
83.4
Ours(𝛿=0.1)
82.5
82.8
82.4
83.5
82.2
84.0
81.3
84.4
80.5
84.7
80.5
84.7
Ours(𝛿=0.2)
82.7
83.4
82.5
83.9
82.1
84.2
81.5
84.7
79.9
85.0
79.9
85.0
Ours(𝛿=0.4)
82.3
82.0
82.2
82.6
81.6
82.7
81.1
83.1
81.0
83.2
81.0
83.2
Ours(𝛿=0.8)
81.9
82.2
81.7
82.6
81.5
83.0
80.4
83.6
79.4
83.5
79.4
83.5
","However, in many real-world cases, the source or target distribution often varies over time and becomes uncertain. As shown in Fig. 1, in vanilla OT, it assumes that the sup-ply from mines and the demand from factories are fixed and and their total masses are equal to each other. While in our realistic setting we assume the demand from factories vary within a double-bounded range. When the demand exceeds the upper boundary, the factory reaches its production capac-ity limit. Conversely, when the demand falls below the lower boundary, the factory operates below its optimal capacity.1 1When both sides are not fixed, it degenerates to the lower bound. We discuss it in details in Sec. .nan"
"Autonomous vehicles rely on 3D environmental perception systems that utilize semantic segmentation and stereo matching to understand their surroundings. While these tasks are typically addressed independently, this paper introduces S3M-Net, a novel joint learning framework that simultaneously performs both tasks. By sharing features between these tasks, S3M-Net improves overall scene understanding and reduces computational complexity. A feature fusion adaption (FFA) module is developed to transform shared features into semantic space and fuse them with encoded disparity features. Additionally, a semantic consistency-guided (SCG) loss function is proposed to encourage structural consistency in both tasks. Extensive experiments on the challenging vKITTI2 and KITTI datasets demonstrate the effectiveness and superior performance of S3M-Net compared to state-of-the-art methods in both semantic segmentation and stereo matching.","Autonomous vehicles require accurate and reliable 3D environmental perception systems to navigate safely and efficiently. Semantic segmentation and stereo matching are two essential components of these systems, providing a comprehensive understanding of the environment. However, conventional approaches often address these tasks independently, leading to practical limitations in real-world scenarios. To address this, we propose S3M-Net, a novel joint learning framework that performs semantic segmentation and stereo matching simultaneously.

S3M-Net offers several key advantages over existing methods. First, by sharing features between the two tasks, it improves overall scene understanding and reduces computational complexity. Second, a feature fusion adaption (FFA) module is developed to transform shared features into semantic space and fuse them with encoded disparity features, resulting in more informative representations for both tasks. Third, a semantic consistency-guided (SCG) loss function is proposed to encourage structural consistency in both tasks, further enhancing performance. Extensive experiments on the challenging vKITTI2 and KITTI datasets demonstrate the effectiveness and superior performance of S3M-Net compared to state-of-the-art methods in both semantic segmentation and stereo matching.","The proposed S3M-Net consists of five main components:

1. Joint Encoder: Extracts shared features from RGB images using a series of residual blocks and downsampling layers.
2. Multi-Level GRU Update Operator: Refines disparity maps in a coarse-to-fine manner using a sequence of GRU operations.
3. Feature Fusion Adaptation Module: Transforms shared features into the semantic space and fuses them with features extracted from disparity maps.
4. Densely-Connected Skip Connection Decoder: Decodes fused features and generates final semantic predictions.
5. Semantic Consistency-Guided Loss: Supervises the entire joint learning process by minimizing a novel loss function that emphasizes semantic consistency.

The joint encoder and multi-level GRU update operator are inspired by RAFT-Stereo, while the feature fusion adaptation module is a novel contribution of this work. The densely-connected skip connection decoder is based on the SNE-RoadSeg architecture. The semantic consistency-guided loss function is designed to encourage structural consistency between the semantic segmentation and stereo matching tasks.","Extensive experiments are conducted on the vKITTI2 and KITTI datasets to evaluate the performance of S3M-Net. The results demonstrate the following:

1. Semantic Segmentation: S3M-Net outperforms SoTA single-modal and feature-fusion networks across all evaluation metrics on both datasets, achieving significant improvements in mAcc, mIoU, fwIoU, and FSc.
2. Stereo Matching: S3M-Net also outperforms SoTA stereo matching networks across all evaluation metrics on both datasets, demonstrating superior accuracy in estimating disparity maps, particularly in challenging regions.","In conclusion, this paper presents S3M-Net, an effective solution for joint learning of semantic segmentation and stereo matching. The proposed framework leverages shared features between the two tasks, utilizes a novel feature fusion adaptation module, and employs a semantic consistency-guided joint learning loss. The extensive experiments conducted on the vKITTI2 and KITTI datasets validate the effectiveness of S3M-Net and its superior performance compared to state-of-the-art methods for both semantic segmentation and stereo matching. While S3M-Net shows promising results, further research is required to address limitations such as the need for both semantic and disparity annotations and the computational efficiency required for real-time deployment in autonomous vehicles.",S$^3$M-Net: Joint Learning of Semantic Segmentation and Stereo Matching for Autonomous Driving,"Zhiyuan Wu, Yi Feng, Chuang-Wei Liu, Fisher Yu, Qijun Chen, Rui Fan","1
S3M-Net: Joint Learning of Semantic Segmentation
and Stereo Matching for Autonomous Driving
Zhiyuan Wu
, Yi Feng
, Chuang-Wei Liu
, Fisher Yu
,
Qijun Chen
, Senior Member, IEEE, and Rui Fan
, Senior Member, IEEE
Abstract—Semantic segmentation and stereo matching are two
essential components of 3D environmental perception systems for
autonomous driving. Nevertheless, conventional approaches often
address these two problems independently, employing separate
models for each task. This approach poses practical limita-
tions in real-world scenarios, particularly when computational
resources are scarce or real-time performance is imperative.
Hence, in this article, we introduce S3M-Net, a novel joint
learning framework developed to perform semantic segmentation
and stereo matching simultaneously. Specifically, S3M-Net shares
the features extracted from RGB images between both tasks,
resulting in an improved overall scene understanding capability.
This feature sharing process is realized using a feature fusion
adaption (FFA) module, which effectively transforms the shared
features into semantic space and subsequently fuses them with the
encoded disparity features. The entire joint learning framework
is trained by minimizing a novel semantic consistency-guided
(SCG) loss, which places emphasis on the structural consistency
in both tasks. Extensive experimental results conducted on the
vKITTI2 and KITTI datasets demonstrate the effectiveness of our
proposed joint learning framework and its superior performance
compared to other state-of-the-art single-task networks. Our
project webpage is accessible at mias.group/S3M-Net.
Index Terms—semantic segmentation, stereo matching, envi-
ronmental perception, autonomous driving, joint learning.
I. INTRODUCTION
3
D environmental perception stands as a critical and foun-
dational aspect of autonomous driving [1], [2]. Semantic
segmentation and stereo matching are two key functionalities
in 3D environmental perception systems [3]–[5]. The former
provides a comprehensive pixel-level understanding of the
environment, while the latter simulates human binocular vision
to acquire accurate and dense depth information [6]. The
combined utilization of both functionalities has become the
mainstream approach in recent years [7]–[12].
In recent years, the research focus in semantic segmentation
has shifted from single-modal networks [13]–[18] with a
single encoder to feature-fusion networks with dual encoders
[19]–[22]. The latter type of networks extract heterogeneous
features from RGB-X data, where “X” can represent various
forms of spatial geometric information, e.g., depth images
(Corresponding author: Rui Fan)
Zhiyuan Wu, Yi Feng, Chuang-Wei Liu, Qijun Chen, and Rui Fan are with
the College of Electronics & Information Engineering, Shanghai Research
Institute for Intelligent Autonomous Systems, the State Key Laboratory of
Intelligent Autonomous Systems, and Frontiers Science Center for Intelligent
Autonomous Systems, Tongji University, Shanghai 201804, China (e-mails:
gwu@tongji.edu.cn, fengyi@ieee.org, {cwliu, qjchen, rfan}@tongji.edu.cn).
Fisher Yu is with the Department of Information Technology and Electrical
Engineering, ETH Z¨urich, Sternwartstrasse 7, 8092 Z¨urich, Switzerland.
email: i@yf.io.
generated from LiDAR point clouds and surface normal
maps obtained through depth-to-normal translation [23]. These
heterogeneous features are subsequently fused to achieve a
more comprehensive understanding of the environment [21].
However, a critical drawback of feature-fusion networks is
their dependency on the availability of the “X” data, which
can pose limitations in scenarios where LiDARs are not
present. Additionally, when the accuracy of the “X” data is
not satisfactory, such as due to variations in camera-LiDAR
calibration, the fusion of these heterogeneous features can
potentially lead to a degradation in the overall performance
of semantic segmentation [24]. While a stereo camera can
serve as a practical and cost-effective alternative to LiDARs for
depth information acquisition, the incorporation of a separate
stereo matching network introduces additional computations,
and therefore, poses difficulties in achieving real-time pro-
cessing speeds for the entire system [9]. Moreover, stereo
matching and semantic segmentation share the same input, and
the representations from RGB images can be more informative
when they are jointly learned by both tasks.
The joint learning of multiple interconnected 3D environ-
mental perception tasks introduces a form of regularization
that has demonstrated superiority over uniform complexity
penalization in reducing over-fitting [25]. Furthermore, rather
than employing separate models for semantic segmentation
and stereo matching, joint learning can potentially reduce
computational complexity [7]–[12], as shared learning repre-
sentations can be used for both tasks. This can be advantageous
in real-time or resource-constrained applications. Moreover,
joint learning enables end-to-end optimization of the entire
system, allowing the model to adapt to the specific challenges
of both tasks simultaneously. Consequently, this can lead
to improved performance when compared to models trained
separately for each task [12]. In addition, stereo matching
can occasionally produce ambiguously estimated disparities,
particularly in texture-less or occluded regions [26]. Semantic
segmentation can provide informative contextual information
that helps disambiguate such cases, ultimately leading to more
reliable disparity estimations [9]. Regrettably, the joint learn-
ing of semantic segmentation and stereo matching, especially
within feature-fusion networks or when faced with a scarcity
of training samples, has received relatively limited attention
in this research area and calls for further investigation.
Therefore, in this article, we present Semantic Segmentation
and Stereo Matching Network (S3M-Net), a joint frame-
work to simultaneously predict both semantic and disparity
information. S3M-Net begins with the extraction of features
from stereo images. These features are then processed by a
arXiv:2401.11414v1  [cs.CV]  21 Jan 2024
2
multi-level gate recurrent unit (GRU) operator to generate a
disparity map. Simultaneously, these features are shared with
the semantic segmentation task via a feature fusion adaptation
(FFA) module. Building upon our prior work SNE-RoadSeg
[3], we extract additional features from the estimated disparity
map. Finally, a densely-connected skip connection decoder
is employed to decode the fused features and generate the
semantic predictions. S3M-Net is trained in a fully supervised
manner by minimizing a semantic consistency-guided (SCG)
joint learning loss. Extensive experiments conducted on the
vKITTI2 [27] and KITTI 2015 [28] datasets unequivocally
demonstrate the effectiveness and superior performance of our
proposed S3M-Net.
In summary, the main contributions of this article include:
• S3M-Net, a joint learning framework designed to address
semantic segmentation and stereo matching simultane-
ously, where both tasks collaboratively leverage the fea-
tures extracted from RGB images, enhancing the overall
understanding of the driving scenario;
• A feature fusion adaption module to transform the shared
feature maps into semantic space and subsequently fuse
them with encoded disparity features;
• A semantic consistency-guided loss function to supervise
the training process of the joint learning framework,
emphasizing on the structural consistency in both tasks.
The remainder of this article is organized as follows: Sect.
II provides a review of related work. Sect. III introduces
our proposed S3M-Net. Sect. IV presents the experimental
results and compares our framework with other state-of-the-art
(SoTA) approaches. In Sect. V, we discuss the advantages and
limitations of our method. Finally, we conclude this article in
Sect. VI.
II. LITERATURE REVIEW
A. Semantic Segmentation
Semantic segmentation has been a long-standing problem
in the field of computer vision over the past decade [6],
[29]. The SoTA networks in this research area can generally
be classified into two categories: (1) single-modal networks
with a single encoder and (2) feature-fusion networks with
multiple encoders [3], [30], [31]. In the early attempts to
tackle semantic segmentation, researchers primarily focused
on encoder-decoder architectures for pixel-level classification.
Notable examples include SegNet [13], U-Net [14], PSPNet
[15], the DeepLab series [16], [17], and Transformer-based
networks [32]–[34]. The encoder extracts hierarchical deep
features from the input image, while the decoder produces the
segmentation map by upsampling and combining the features
from different encoder layers. However, these networks are
limited in their ability to effectively combine deep features
extracted from different modalities (or sources) of visual in-
formation. As a result, they often struggle to produce accurate
segmentation results in challenging scenarios characterized
by poor lighting and illumination conditions [3]. Therefore,
researchers have turned their focus towards feature-fusion
networks that can effectively integrate deep features learned
from multiple modalities (or sources) of visual information.
This problem is commonly referred to as “RGB-X semantic
segmentation”, where “X” represents the additional modality
(or source) of visual information, in addition to the RGB
images. The most representative feature-fusion networks based
on convolutional neural networks (CNNs) include FuseNet
[19], MFNet [35], RTFNet [20], and our previous works SNE-
RoadSeg series [3], [21]. Furthermore, Transformer-based
RGB-X semantic segmentation networks, such as OFF-Net
[22] and RoadFormer [24], have been recently introduced.
In this article, we design our S3M-Net based on the SNE-
RoadSeg architecture and explore more effective solutions for
the feature fusion operation.
B. Stereo Matching
Conventional explicit programming-based stereo matching
algorithms (local, global, and semi-global) generally consist
of four main procedures: (1) cost computation, (2) cost aggre-
gation, (3) disparity optimization, and (4) disparity refinement
[26]. The performance of these algorithms has been signif-
icantly outperformed by end-to-end deep stereo networks,
thanks to the recent advancements in deep learning techniques.
PSMNet [36], GwcNet [37], AANet [38], LEA-Stereo [39],
RAFT-Stereo [40], and CRE-Stereo [41] are six representative
end-to-end deep stereo networks proposed in recent years.
PSMNet [36] employs a spatial pyramid to capture multi-scale
information and employs multiple 3D convolutional layers to
exploit both local and global contexts for cost computation.
GwcNet [37], on the other hand, builds upon the foundation of
PSMNet by constructing the cost volume via group-wise corre-
lation, thereby enhancing the 3D stacked hourglass network.
In light of the computational demands of 3D convolutions,
researchers have actively sought ways to minimize the trade-
off between efficiency and accuracy in stereo matching. For
example, LEA-Stereo [39] introduces the neural architecture
search (NAS) [42] technique to stereo matching. This pio-
neering approach results in the first end-to-end hierarchical
NAS framework for deep stereo matching. RAFT-Stereo [40],
a rectified stereo matching method that draws inspiration from
the optical flow estimation network RAFT [43], leverages the
RAFT architecture to perform accurate and real-time stereo
matching inference. The network utilizes recurrent structures
to refine correlation features and enhance the disparity estima-
tion accuracy. CRE-Stereo [41], another recent prior art based
on recurrent refinement (to update disparities in a coarse-
to-fine manner) and adaptive group correlation (to mitigate
the impact of erroneous rectification), yields more compelling
disparity estimation results. In this article, we develop our
S3M-Net based on the RAFT-Stereo architecture.
C. Multi-Task Joint Learning for Semantic Segmentation and
Stereo Matching
Existing frameworks that jointly address semantic segmen-
tation and stereo matching generally focus on improving
disparity accuracy by leveraging semantic information [7]–
[12], while the discussion regarding the utilization of dis-
parity information to enhance semantic segmentation at the
feature level for joint learning remains limited, except for the
3
GRU
𝑫0
GRU
GRU
FFA Module
Dense Skip 
Connections
Shared Features
Context Features
3D Correlation Volume
Multi-Level GRU
Left Image
Right Image
3D Correlation Pyramid
ℱ𝐿 = {𝑭1
𝐿, … , 𝑭𝑛𝐿}
ℱ𝑅 = {𝑭1
𝑅, … , 𝑭𝑛𝑅}
Upsampling Layer
Semantic Segmentation
Ground Truth
Stereo Matching
Result
Semantic Segmentation
Result
𝑽𝑁
SCG Loss
GRU
Averaging Pooling
Normalization
SCG Loss
Fig. 1: The architecture of our proposed S3M-Net for end-to-end joint learning of semantic segmentation and stereo matching.
exploration of “RGB-X semantic segmentation” discussed in
Sect. II-A. Nevertheless, these prior arts either require a large
amount of well-annotated training data or involve intricate
training strategies for the joint learning of both tasks. For
instance, SegStereo [7] and DispSegNet [8] require an initial
unsupervised training phase on the large-scale Cityscapes [44]
dataset, followed by a subsequent supervised fine-tuning on the
smaller KITTI 2012 and 2015 [28], [45] datasets. Similarly,
the studies presented in [9], [11], [12] involve the pre-training
of their spatial branches (performing stereo matching) on the
large-scale SceneFlow [46] dataset, followed by the fine-tuning
of both semantic and spatial branches on the KITTI 2012
and 2015 datasets [28], [45]. DSNet [10] adopts a different
joint learning strategy in which the training alternates between
the semantic segmentation and stereo matching networks,
with each network being frozen during the training of the
other. However, achieving simultaneous convergence of the
two networks can be challenging, as the shared features are
not learned in an end-to-end manner. Additionally, we were
unable to locate publicly available source code (in PyTorch or
TensorFlow) for these prior arts, and re-implementations carry
the risk of introducing errors. In contrast to the aforementioned
approaches, our proposed S3M-Net is trained in an end-to-end
fashion and capable of jointly learning semantic segmentation
and stereo matching even when the training data are limited.
III. METHODOLOGY
As illustrated in Fig. 1, our proposed S3M-Net consists of
five main components:
(1) Joint encoder to extract shared features from RGB im-
ages;
(2) Multi-level GRU update operator to refine disparity maps;
(3) Feature fusion adaptation module to transform shared
features into the semantic space and fuse them with
features extracted from the disparity maps;
(4) Densely-connected skip connection decoder to decode
fused features and produce final semantic predictions;
(5) Semantic consistency-guided loss to supervise the entire
joint learning process.
A. Joint Encoder
Given a pair of well-rectified stereo images IL, IR ∈
RH×W ×3, where H and W denote their height and width,
respectively, we employ a joint encoder consisting of a series
of residual blocks and downsampling layers to extract features
FL =
n
F L
1 , . . . , F L
n
o
and FR =
n
F R
1 , . . . , F R
n
o
from IL
and IR, respectively. FL is subsequently shared with the
semantic segmentation task.
B. Multi-Level GRU Update Operator
Using the features FL and FR extracted by the joint
encoder, we first construct an initial 3D correlation volume
C1 ∈ RH×W ×W as follows:
C1(i, j, k) = F L
n(i, j, :) · F R
n (i, k, :),
(1)
where i represents the i-th row, and j and k represent to the j-
th and k-th columns in the left and right shared feature maps,
respectively. We then construct a pyramid of 3D correlation
volumes C = {C1, . . . , Cm} by downsampling C1 with
average pooling operations. The m-th 3D correlation volume
Cm ∈ RH×W ×
W
2m−1 is constructed from the (m − 1)-th 3D
correlation volume Cm−1 using 1D average pooling with a
kernel size of 2 and a stride of 2. Inspired by RAFT-Stereo
[40], we adopt a multi-level GRU update operator to refine
a sequence of disparity maps D = {D1, . . . , Dn}, where
Di ∈ RH×W (i = 1, . . . , n). This refinement process is
performed in a coarse-to-fine manner, starting from an initial
disparity map D0 in which all disparities are initialized to 0.
C. Feature Fusion Adaptation Module
In stereo matching, a lower number of channels, e.g., the
256 channels utilized in RAFT-Stereo [40], is often suffi-
cient for capturing relevant features for 1D correspondence
4
𝑭1
𝐿
𝑭3
𝐿
𝑭5
𝐿
𝑭1
𝐹
𝑭2
𝐹
𝑭𝑛−1
𝐹
𝑭𝑛𝐹
Batch Norm
ReLU Layer
Residual Layer
Max Pooling
Estimated Disparity Map
Shared Features
Convolution
Feature Fusion
Fused Features
Remapping
Fig. 2: An illustration of our proposed FFA module.
search, especially when considering computational efficiency.
On the other hand, semantic segmentation requires pixel-level
classification and a more in-depth scene understanding. It
benefits from complex feature representations that can capture
fine-grained details and object boundaries, making a larger
number of channels, e.g., the 2048 channels employed in
SNE-RoadSeg [3], advantageous for this task. Therefore, we
introduce the FFA module to align the channels and resolutions
between the disparity and semantic feature maps during joint
learning.
As illustrated in Fig 2, given the left shared feature maps
FL =
n
F L
1 , . . . , F L
n
o
and the disparity map pyramid D =
{D1, . . . , Dn}, we obtain the adapted fused feature sequence
FF
=
n
F F
1 , . . . , F F
n
o
using our proposed FFA module,
which can be formulated as follows:
F F
i = Ai(FL) ⊕ ED
i (Dn),
(2)
where ED denotes the disparity map encoding operation, ⊕
denotes the feature fusion operation, and Ai is defined as our
feature adaptation operation, as formulated as follows:
Ai(FL) =



R(F L
2i),
i ≤ n
2
E(F F
i−1 ⊕ ED
i−1(Dn)),
i > n
2
,
(3)
where R represents the remapping operation from the shared
feature space to the semantic feature space, and E represents
the encoding operation for the semantic feature maps.
Specifically, for the remapping operation R, we employ
3×3 convolutional layers with a stride of 2 and padding of 1,
each followed by a batch normalization layer and a rectified
linear unit (ReLU) activation layer, adapting the feature map
channels to 64, 256, and 512, respectively. Regarding the
disparity encoding operation ED, we employ ResNet-152 [47]
as the backbone network to extract features from the last
disparity map Dn. In ResNet-152, the first block consists of a
convolutional layer, a batch normalization layer, and a ReLU
activation layer. Then, a max pooling layer and four residual
layers are sequentially applied to progressively increase the
number of feature map channels.
Similarly, we utilize the residual block for the encoding
operation E on the semantic feature maps, resulting in fea-
ture maps with 1024 and 2048 channels. The fused features
FF contain both texture and spatial geometric information,
thereby enhancing semantic scene understanding. We conduct
an ablation study for different feature fusion modules in Sect.
IV-F.
D. Densely-Connected Skip Connection Decoder
We employ the decoder introduced in our previous work
SNE-RoadSeg [3] to decode the fused features and generate
the semantic prediction. In this encoder, three convolutional
layers in the feature extractor and the upsampling layer share
the same parameters: a 3 × 3 kernel size, a stride of 1, and
a padding of 1. In the final layer, features are upsampled to
create the prediction map with N channels, where N denotes
the number of semantic classes.
E. Semantic Consistency-Guided Joint Learning Loss
The loss function employed in our joint learning framework
should guide the supervision of both the semantic segmenta-
tion and stereo matching tasks. Gradient smoothness between
the disparity and semantic segmentation maps typically aligns
closely, particularly at inter-class boundaries, where traditional
training strategies tend to result in more errors due to factors
such as occlusion and reflection. In light of this, we propose
an SCG loss function to supervise the entire joint learning
process, which leverages semantic consistency to optimize the
training of S3M-Net.
Given the ground-truth semantic segmentation map M G ∈
RH×W , Each pixel p of M G can be written as follows:
M G(p) ∈ {1, . . . , C},
(4)
where C refers to the number of the semantic classes. We
construct an extended 3D volume V 3D ∈ RH×W ×C using
the following expression:
V 3D
c (p) = δ(M G(p), c),
(5)
where c represents the c-th channel in the volume, and
δ denotes the Kronecker Delta function [48]. As a result,
each channel of the volume can be regarded as a binary
segmentation map of the c-th class. To emphasize semantic
consistency, We use an average pooling operation for each
channel to obtain the inter-class volume V I ∈ RH×W ×C:
V I = P(V 3D),
(6)
where P denotes the average pooling operation. Furthermore,
we apply a normalization operation:
V N(p) = e−(2V I(p)−1)2.
(7)
to obtain a normalized volume V N ∈ RH×W ×C. We then
map V N to a semantic consistency-guided weight map W ∈
RH×W through:
W (p) = max
c
n
V N
c (p)
o
.
(8)
The total loss function
Lscg = Lss + Lsm
(9)
5
consists of an SCG semantic segmentation loss Lss and an
SCG stereo matching loss Lsm. Lss is formulated as follows:
Lss = − 1
N
N
X
i=1
C
X
c=1
[(1−α)+αW (p)]yc(p) log(ˆyc(p)), (10)
where N denotes the pixel number, C represents the class
number, ˆyc(p) denotes the predicted probability of p belong-
ing to class c, yc(p) represents the ground-truth label for p in
class c, and α denotes the loss weight. Based on the ablation
study detailed in Sect. IV-F, we set the value of α to 0.1.
Moreover, Lsm is formulated as follows:
Lsm =
N
X
i=1
[(1 − α) + αW (p)]γN−i DG − Di

1 ,
(11)
where DG represents the ground-truth disparity map and Di
denotes the i-th disparity map in D. α is set to 0.1 and γ is
set to 0.9.
IV. EXPERIMENTS
In this article, we conduct extensive experiments to evaluate
the performance of our developed S3M-Net both quantitatively
and qualitatively. The following subsections provide details on
the used datasets, experimental set-up, evaluation metrics, and
the comprehensive evaluation of our proposed method.
A. Datasets
Since the training of our network requires both semantic
and disparity annotations, we employ two public datasets to
evaluate its performance: the vKITTI2 [27] dataset (synthetic
yet large-scale) and the KITTI 2015 [28] dataset (real-world
yet modest-scale). Their details are as follows:
• The vKITTI2 dataset contains virtual replicas of five
sequences from the KITTI dataset. It provides 15 classes
for the semantic segmentation tasks. Dense ground-truth
disparity maps are acquired through depth rendering
using a virtual engine. In our experiments, we randomly
select 700 pairs of stereo images, along with their seman-
tic and disparity annotations to evaluate the effectiveness
and robustness of our proposed S3M-Net, where 500 pairs
are used for model training and the remaining 200 pairs
are used for model validation.
• The KITTI 2015 dataset contains 400 pairs of stereo
images captured in real-world driving scenarios, with
200 pairs containing ground truth and the other 200
pairs lacking ground truth. It provides 19 classes for
the semantic segmentation tasks (in alignment with the
Cityscapes [44] dataset). Sparse disparity ground truth
is obtained using a Velodyne HDL-64E LiDAR. In our
experiments, we allocate 70% of the dataset for training,
while the remaining portion is used as the test set.
B. Experimental Setup
Our experiments are conducted on an NVIDIA RTX 3090
GPU. The batch size is set to 1. The maximum disparity search
range is set to 192 pixels. All images are cropped to 1000 ×
320 pixels before feeding into the network. We utilize the
AdamW [55] optimizer for model training, setting the epsilon
and weight decay parameters to 10−5 and 10−8, respectively.
The initial learning rate is set to 2 × 10−4. Training lasts for
100K iterations on the vKITTI2 dataset and 20K iterations on
the KITTI 2015 dataset. We employ traditional data augmen-
tation techniques to enhance the robustness of the models.
C. Evaluation Metrics
Since our proposed S3M-Net simultaneously performs se-
mantic segmentation and stereo matching, we evaluate the
performance of both tasks in our experiments.
We utilize seven evaluation metrics to quantify the perfor-
mance of semantic segmentation: (1) accuracy (Acc), (2) mean
accuracy (mAcc), (3) mean intersection over union (mIoU), (4)
frequency-weighted intersection over union (fwIoU) [56], (5)
precision (Pre), (6) recall (Rec), and (7) F1-score (FSc).
Additionally, we use two evaluation metrics: (1) the average
end-point error (EPE) and (2) the percentage of error pixels
(PEP), setting the tolerance for the latter to 1.0 and 3.0 pixels,
respectively, to quantify the performance of stereo matching.
D. Semantic Segmentation Performance
The qualitative experimental results on the vKITTI2 and
KITTI datasets are presented in Figs. 3 and 4, respectively,
while the quantitative experimental results on the vKITTI2 and
KITTI datasets are given in Tables I and II, respectively. These
results suggest that S3M-Net outperforms other SoTA single-
modal and feature-fusion networks (including CNN-based and
Transformer-based methods) across all evaluation metrics on
both datasets. Specifically, it is noteworthy that when the
entire joint learning framework is trained by minimizing our
proposed SCG loss, S3M-Net achieves the best performance
on the KITTI dataset across all evaluation metrics except for
Pre. Compared with SoTA methods, it shows improvements of
5.71% in mAcc, 4.84% in mIoU, 1.35% in fwIoU, and 0.76%
in FSc, respectively. Similarly, it outperforms other networks
on the vKITTI2 dataset in most evaluation metrics, with
improvements of 1.72% in fwIoU and 1.44% in FSc. However,
for mAcc, mIoU, and Rec, its performance is comparable to
that of S3M-Net trained without using the SCG loss. Addition-
ally, it is obvious that feature-fusion networks consistently out-
perform single-modal networks, particularly under challenging
weather and lighting conditions. This observation aligns with
our expectations, as feature-fusion networks leverage both
RGB images and disparity maps, allowing them to effectively
learn informative spatial geometric representations. However,
SoTA feature-fusion networks may exhibit higher error rates
in distant regions. For instance, FuseNet and SNE-RoadSeg
demonstrate poor performance in the sky. We attribute this
phenomenon to the deep structure of the encoders, where
distinguishing distant objects using disparity features becomes
challenging, and the feature fusion process amplifies the influ-
ence of the disparity feature. In contrast, within our proposed
joint learning framework, we can extract more informative
features benefiting from both tasks, irrespective of dataset
size. This improvement is likely due to the fact that joint
6
TABLE I: Comparisons of SoTA semantic segmentation networks on the vKITTI2 [27] dataset. The symbol ↑ indicates that
a higher value corresponds to better performance. The best results are shown in bold font.
Category
Networks
Acc (%) ↑
mAcc (%) ↑
mIoU (%) ↑
fwIoU (%) ↑
Pre (%) ↑
Rec (%) ↑
FSc (%) ↑
Single-Modal
SegNet [13]
59.29
32.54
23.93
48.17
66.10
66.73
61.11
U-Net [14]
62.71
37.65
29.83
55.10
75.80
67.67
65.26
PSPNet [15]
76.26
53.53
44.81
69.30
81.55
79.68
75.38
DeepLabv3+ [17]
92.19
63.15
56.90
87.15
89.00
92.71
90.16
HRNet [49]
74.79
40.82
32.47
63.23
73.69
76.50
73.39
BiSeNet V2 [50]
81.77
51.07
44.45
74.71
83.23
82.19
80.67
Segmenter [32]
90.39
60.33
52.99
83.47
88.05
87.89
87.70
SegFormer [33]
94.75
70.56
64.98
90.49
93.57
93.62
93.46
Mask2Former [34]
89.29
64.58
57.14
83.84
90.75
87.23
87.19
DDRNet [51]
70.80
40.32
32.10
61.44
76.35
71.67
70.57
Feature-Fusion
FuseNet [19]
49.42
31.21
22.56
41.07
79.39
50.67
47.50
MFNet [35]
76.22
51.50
43.41
68.82
82.46
78.65
73.80
RTFNet [20]
85.22
49.47
42.59
77.69
83.74
89.17
84.41
SNE-RoadSeg [3]
83.64
60.85
52.56
75.14
83.44
81.66
77.77
OFF-Net [22]
90.84
61.51
55.27
84.69
89.24
86.71
86.15
RoadFormer [24]
97.54
86.58
80.83
95.34
96.99
96.86
96.91
S3M-Net
98.27
88.28
84.25
96.92
98.29
98.32
98.28
S3M-Net w/ SCG loss
98.32
88.24
84.18
96.98
98.37
98.28
98.31
TABLE II: Comparisons of SoTA semantic segmentation networks on the KITTI 2015 [28] dataset. The symbol ↑ indicates
that a higher value corresponds to better performance. The best results are shown in bold font.
Category
Networks
Acc (%) ↑
mAcc (%) ↑
mIoU (%) ↑
fwIoU (%) ↑
Pre (%) ↑
Rec (%) ↑
FSc (%) ↑
Single-Modal
SegNet [13]
59.63
31.98
22.61
43.98
55.25
67.49
57.29
U-Net [14]
69.02
41.15
30.64
55.65
69.11
77.65
71.04
PSPNet [15]
80.03
44.97
38.15
68.62
79.29
82.66
79.59
DeepLabv3+ [17]
82.33
50.15
42.79
72.43
83.85
87.18
84.59
HRNet [49]
63.42
31.68
22.78
49.40
55.10
67.71
57.21
BiSeNet V2 [50]
73.68
41.66
32.71
60.55
68.35
81.79
72.37
Segmenter [32]
84.53
50.77
43.63
74.72
82.99
87.15
84.41
SegFormer [33]
88.28
59.23
51.39
80.53
87.15
90.85
88.46
Mask2Former [34]
84.35
54.33
45.87
75.56
84.74
89.12
85.92
DDRNet [51]
62.12
31.61
22.63
48.15
57.09
68.98
59.07
Feature-Fusion
FuseNet [19]
41.79
19.05
11.38
27.53
44.14
44.35
37.68
MFNet [35]
81.02
48.13
40.70
70.42
82.85
85.73
82.36
RTFNet [20]
71.61
39.26
30.35
57.98
69.52
85.16
74.28
SNE-RoadSeg [3]
79.46
51.91
41.56
69.22
81.45
87.05
82.91
OFF-Net [22]
75.84
40.13
33.13
64.02
77.48
72.19
70.62
RoadFormer [24]
90.05
62.34
55.13
83.40
91.65
91.39
91.11
S3M-Net
90.01
62.48
54.33
83.44
88.96
93.52
90.65
S3M-Net w/ SCG loss
90.66
65.90
57.80
84.53
90.85
93.55
91.80
learning of multiple interconnected tasks introduces a form of
regularization, which has shown its superiority over uniform
complexity penalization in reducing over-fitting.
E. Stereo Matching Performance
The qualitative experimental results on the vKITTI2 and
KITTI datasets are given in Figs. 5 and 6, respectively, while
the quantitative experimental results on the vKITTI2 and
KITTI datasets are presented in Tables III and IV, respectively.
These results suggest that S3M-Net outperforms other SoTA
stereo matching networks across all evaluation metrics on both
datasets. Specifically, S3M-Net trained with and without using
the SCG loss achieves the top and second-best overall perfor-
mances, respectively. S3M-Net, when trained without using
the SCG loss, demonstrates improvements of 2.50%-71.32%
in EPE, 4.17%-64.19% in PEP 1.0, and 4.49%-67.97% in
PEP 3.0. On the other hand, S3M-Net, when trained with
the SCG loss, shows improvements of 5.00%-72.06% in EPE,
5.44%-64.38% in PEP 1.0, and 4.49%-69.83% in PEP 3.0. We
attribute these improvements to the feature sharing and fusion
strategies applied in S3M-Net. First, sharing features with the
semantic segmentation task allows S3M-Net to learn stereo
matching effectively even with limited training data. Second,
as discussed above, stereo matching can sometimes produce
ambiguous disparity estimations, especially in occluded or
7
(a)
(s)
(t)
(r)
(p)
(o)
(n)
(m)
(l)
(h)
(g)
(f)
(e)
(d)
(c)
(b)
(q)
(i)
(j)
(k)
Terrain
Sky
Tree
Vegetation
Building
Road
Guard Rail
Pole
Misc
Truck
Car
Van
Traffic Sign
Traffic Light
Fig. 3: Qualitative experimental results of semantic segmentation on the vKITTI2 [27] dataset: (a) RGB images; (b)-(k) semantic
segmentation results achieved by SegNet [13], U-Net [14], PSPNet [15], DeepLabv3+ [17], HRNet [49], BiSeNet V2 [50],
Segmenter [32], SegFormer [33], Mask2Former [34], and DDRNet [51], respectively; (l)-(q) semantic segmentation results
achieved by FuseNet [19], MFNet [35], RTFNet [20], SNE-RoadSeg [3], OFF-Net [22], and RoadFormer [24], respectively;
(r)-(s) semantic segmentation results achieved by our proposed S3M-Net w/o and w/ the use of the SCG loss, respectively; (t)
ground truth annotations.
texture-less areas. The pursuit of semantic consistency helps
resolve such ambiguities, leading to more reliable disparity
estimation results. In Fig. 6, it is evident that regions lacking
disparity ground truth frequently have substantial errors. Previ-
ous stereo matching algorithms have endeavored to tackle this
issue through knowledge distillation with pre-trained models
[38]. Nevertheless, our S3M-Net successfully overcomes this
challenge by leveraging semantic information.
8
(a)
(s)
(t)
(r)
(p)
(o)
(n)
(m)
(l)
(h)
(g)
(f)
(e)
(d)
(c)
(b)
(q)
(i)
(j)
(k)
Road
Traffic Sign
Pole
Fence
Wall
Building
Sidewalk
Sky
Terrain
Vegetation
Person
Train
Bus
Truck
Car
Rider
Traffic Light
Motorcycle
Bicycle
Fig. 4: Qualitative experimental results of semantic segmentation on the KITTI 2015 [28] dataset: (a) RGB images; (b)-(k)
semantic segmentation results achieved by SegNet [13], U-Net [14], PSPNet [15], DeepLabv3+ [17], HRNet [49], BiSeNet V2
[50], Segmenter [32], SegFormer [33], Mask2Former [34], and DDRNet [51], respectively; (l)-(q) semantic segmentation results
achieved by FuseNet [19], MFNet [35], RTFNet [20], SNE-RoadSeg [3], OFF-Net [22], and RoadFormer [24], respectively;
(r)-(s) semantic segmentation results achieved by our proposed S3M-Net w/o and w/ the use of the SCG loss, respectively; (t)
ground truth annotations.
F. Ablation studies
In this subsection, we first conduct an ablation study on the
selection of loss weight α in (11). Fig. 7 shows the quantitative
experimental results with respect to different α in the range of
0.0 to 0.4 for both semantic segmentation and stereo matching.
It can be obvious that when α = 0.1, S3M-Net achieves the
best overall performance for both tasks. Further weight tuning
is possible, but it should be approached cautiously, especially
when dealing with limited data to avoid over-fitting.
Furthermore, we conduct an additional ablation study on the
feature fusion strategy in our proposed FFA module. As shown
9
Disparity
(a)
(b)
(e)
(c)
(f)
(g)
(l)
(k)
(h)
(i)
(j)
max
min
(d)
Fig. 5: Qualitative experimental results of stereo matching on the vKITTI2 [27] dataset: (a) left RGB images; (b)-(j) disparity
maps estimated using PSMNet [36], GwcNet [37], AANet [38], LEA-Stereo [39], RAFT-Stereo [40], CRE-Stereo [41], ACVNet
[52], PCWNet [53], and IGEV-Stereo [54], respectively; (k)-(l) disparity maps estimated using our proposed S3M-Net w/o and
w/ the use of the SCG loss, respectively.
TABLE III: Comparisons of SoTA stereo matching network on
the vKITTI2 [27] dataset. The symbol ↓ indicates that a lower
value corresponds to better performance. The best results are
shown in bold font.
Networks
EPE (pixels) ↓
PEP (%) ↓
> 1 pixel
> 3 pixels
PSMNet [36]
0.68
10.31
3.77
GwcNet [37]
0.65
9.72
3.69
AANet [38]
1.36
15.61
6.98
LEA-Stereo [39]
0.83
13.33
4.84
RAFT-Stereo [40]
0.40
5.88
2.67
CRE-Stereo [41]
0.63
10.35
3.90
ACVNet [52]
0.61
9.41
3.45
PCW-Net [53]
0.63
9.45
3.49
IGEV-Stereo [54]
0.47
7.15
3.09
S3M-Net
0.39
5.59
2.55
S3M-Net w/ SCG loss
0.38
5.56
2.55
in Table V, when using the addition operation to fuse hetero-
geneous features, the FFA module consistently achieves the
best performance across all evaluation metrics, compared to
other feature fusion strategies, including concatenation, cross
feature module (CFM) [57], dynamic dilated pyramid module
TABLE IV: Comparisons of SoTA stereo matching network
on the KITTI 2015 [28] dataset. The symbol ↓ indicates that a
lower value corresponds to better performance. The best results
are shown in bold font.
Networks
EPE (pixels) ↓
PEP (%) ↓
> 1 pixel
> 3 pixels
PSMNet [36]
0.74
16.12
2.61
GwcNet [37]
0.68
14.21
2.01
AANet [38]
1.10
22.67
5.37
LEA-Stereo [39]
0.83
18.67
3.22
RAFT-Stereo [40]
0.60
10.78
1.96
CRE-Stereo [41]
0.92
19.68
3.35
ACVNet [52]
0.68
13.93
2.10
PCW-Net [53]
0.70
14.81
2.43
IGEV-Stereo [54]
0.62
12.15
1.99
S3M-Net
0.56
10.33
1.72
S3M-Net w/ SCG loss
0.55
10.02
1.62
(DDPM) [58], separation-and-aggregation gate (SA Gate) [59],
and softmax weighted sum (SWS) used in AysmFusion [60],
CEN [61], and TokenFusion [62].
10
Disparity
(a)
(b)
(e)
(c)
(f)
(g)
(l)
(k)
(h)
(i)
(j)
max
min
(d)
Fig. 6: Qualitative experimental results of stereo matching on the KITTI 2015 [28] dataset: (a) left RGB images; (b)-(j)
disparity maps estimated using PSMNet [36], GwcNet [37], AANet [38], LEA-Stereo [39], RAFT-Stereo [40], CRE-Stereo
[41], ACVNet [52], PCWNet [53], and IGEV-Stereo [54], respectively; (k)-(l) disparity maps estimated using our proposed
S3M-Net w/o and w/ the use of the SCG loss, respectively.
TABLE V: Ablation study on feature fusion strategy in our FFA module on the KITTI [28] 2015 dataset.
Fusion Strategy
Acc (%) ↑
mAcc (%) ↑
mIoU (%) ↑
fwIoU (%) ↑
Pre (%) ↑
Rec (%) ↑
FSc (%) ↑
Addition
90.01
62.48
54.33
83.44
88.96
93.52
90.65
Concatenation
86.88
57.43
48.40
78.50
85.96
93.71
88.92
CFM [57]
86.87
57.41
48.77
79.13
85.41
92.05
87.63
DDPM [58]
86.52
58.65
49.51
78.14
85.56
93.34
88.44
SA Gate [59]
87.62
61.77
52.10
80.55
86.74
92.31
88.52
SWS [60]–[62]
87.94
58.11
49.64
80.25
86.63
93.34
89.20
V. DISCUSSION
The experimental results shown in Sect. IV provide strong
support for the claims made in Sect. I. First, the joint learning
of semantic segmentation and stereo matching, two intercon-
nected environmental perception tasks, using our proposed
S3M-Net introduces a form of regularization that has shown its
effectiveness in reducing overfitting, particularly in scenarios
where training data are limited. Secondly, this end-to-end
joint learning framework yields improved performance when
compared to the models trained separately for each task.
Finally, the pursuit of semantic consistency in joint learning
helps reduce ambiguous disparity estimations in texture-less or
occluded regions. We believe that our proposed S3M-Net can
be readily deployed in autonomous vehicles after addressing
the following limitations:
• S3M-Net requires both semantic and disparity annota-
tions, and collecting data with such ground truth remains
a labor-intensive process. Therefore, the exploration of
potential solutions such as semi-supervised or low/few-
shot semantic segmentation and un/self-supervised stereo
matching is a promising avenue for future research.
• S3M-Net achieves a processing speed of 0.66 fps when
processing input RGB images with a resolution of 1248
× 384 pixels. We believe that further computational
11
Semantic Segmentation
Stereo Matching
F-Score
mIoU
PEP 1.0
PEP 3.0
Fig. 7: Ablation study on the selection of α in the SCG joint
learning loss function on the KITTI 2015 [28] dataset.
efficiency optimizations are necessary before deploying
S3M-Net in autonomous vehicles.
VI. CONCLUSION
This article introduced S3M-Net, an effective solution for
joint learning of semantic segmentation and stereo matching.
We have made three significant contributions in this work: (1)
the development of an entire joint learning framework that
shares features between both tasks and fuses heterogeneous
features to improve semantic segmentation, (2) a feature fusion
adaption module designed to enable effective feature sharing
between the two tasks, and (3) a semantic consistency-guided
joint learning loss that emphasizes structural consistency
in both tasks. We conducted extensive experiments on the
vKITTI2 (synthetic and large) and KITTI (real-world and
small) datasets to validate the effectiveness of our framework,
the FFA module, and the training loss. Our results demonstrate
the superior performance of our approach compared to all
other existing methods.
REFERENCES
[1] B. Ranft and C. Stiller, “The role of machine vision for intelligent
vehicles,” IEEE Transactions on Intelligent vehicles, vol. 1, no. 1, pp.
8–19, 2016.
[2] D. L. Fisher, M. Lohrenz, D. Moore, E. D. Nadler, and J. K. Pollard,
“Humans and intelligent vehicles: The hope, the help, and the harm,”
IEEE Transactions on Intelligent Vehicles, vol. 1, no. 1, pp. 56–67, 2016.
[3] R. Fan, H. Wang, P. Cai, and M. Liu, “SNE-RoadSeg: Incorporating
surface normal information into semantic segmentation for accurate
freespace detection,” in European Conference on Computer Vision
(ECCV).
Springer, 2020, pp. 340–356.
[4] G. Xu, X. Wang, X. Ding, and X. Yang, “Iterative geometry encoding
volume for stereo matching,” in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR), 2023, pp.
21 919–21 928.
[5] Z. Rao, B. Xiong, M. He, Y. Dai, R. He, Z. Shen, and X. Li, “Masked
representation learning for domain generalized stereo matching,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2023, pp. 5435–5444.
[6] R. Fan, S. Guo, and M. J. Bocus, Autonomous Driving Perception.
Springer, 2023.
[7] G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia, “SegStereo: Exploiting
semantic information for disparity estimation,” in European Conference
on Computer Vision (ECCV), 2018, pp. 636–651.
[8] J. Zhang, K. A. Skinner, R. Vasudevan, and M. Johnson-Roberson,
“DispSegNet: Leveraging semantics for end-to-end learning of disparity
estimation from stereo imagery,” IEEE Robotics and Automation Letters,
vol. 4, no. 2, pp. 1162–1169, 2019.
[9] Z. Wu, X. Wu, X. Zhang, S. Wang, and L. Ju, “Semantic stereo
matching with pyramid cost volumes,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV), 2019, pp. 7484–
7493.
[10] W. Zhan, X. Ou, Y. Yang, and L. Chen, “DSNet: Joint learning for
scene segmentation and disparity estimation,” in 2019 International
Conference on Robotics and Automation (ICRA).
IEEE, 2019, pp.
2946–2952.
[11] P. L. Dovesi, M. Poggi, L. Andraghetti, M. Mart´ı, H. Kjellstr¨om,
A. Pieropan, and S. Mattoccia, “Real-time semantic stereo matching,”
in 2020 IEEE International Conference on Robotics and Automation
(ICRA).
IEEE, 2020, pp. 10 780–10 787.
[12] S. Chen, Z. Xiang, C. Qiao, Y. Chen, and T. Bai, “SGNet: Semantics
guided deep stereo matching,” in Proceedings of the Asian Conference
on Computer Vision (ACCV), 2020, pp. 106–122.
[13] V. Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A deep con-
volutional encoder-decoder architecture for image segmentation,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 39,
no. 12, pp. 2481–2495, 2017.
[14] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional net-
works for biomedical image segmentation,” in International Confer-
ence on Medical Image Computing and Computer-Assisted Intervention
(MICCAI).
Springer, 2015, pp. 234–241.
[15] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing
network,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), 2017, pp. 2881–2890.
[16] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking
atrous convolution for semantic image segmentation,” arXiv preprint
arXiv:1706.05587, 2017.
[17] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-
decoder with atrous separable convolution for semantic image segmen-
tation,” in Proceedings of the European Conference on Computer Vision
(ECCV), 2018, pp. 801–818.
[18] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, “CCNet:
Criss-cross attention for semantic segmentation,” in Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV), 2019,
pp. 603–612.
[19] C. Hazirbas, L. Ma, C. Domokos, and D. Cremers, “FuseNet: Incorporat-
ing depth into semantic segmentation via fusion-based cnn architecture,”
in Proceedings of the Asian Conference on Computer Vision (ACCV).
Springer, 2017, pp. 213–228.
[20] Y. Sun, W. Zuo, and M. Liu, “RTFNet: RGB-Thermal fusion network for
semantic segmentation of urban scenes,” IEEE Robotics and Automation
Letters, vol. 4, no. 3, pp. 2576–2583, 2019.
[21] H. Wang, R. Fan, P. Cai, and M. Liu, “SNE-RoadSeg+: Rethinking
depth-normal translation and deep supervision for freespace detection,”
in 2021 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS).
IEEE, 2021, pp. 1140–1145.
[22] C. Min, W. Jiang, D. Zhao, J. Xu, L. Xiao, Y. Nie, and B. Dai, “ORFD:
A dataset and benchmark for off-road freespace detection,” in 2022
International Conference on Robotics and Automation (ICRA).
IEEE,
2022, pp. 2532–2538.
[23] J. Yang, B. Xue, Y. Feng, D. Wang, R. Fan, and Q. Chen, “Three-filters-
to-normal+: Revisiting discontinuity discrimination in depth-to-normal
translation,” IEEE Transactions on Automation Science and Engineering,
2024, DOI: 10.1109/TASE.2024.3355941.
[24] J. Li, Y. Zhang, P. Yun, G. Zhou, Q. Chen, and R. Fan, “RoadFormer:
Duplex Transformer for RGB-normal semantic road scene parsing,”
arXiv preprint arXiv:2309.10356, 2023.
[25] R. Fan, C. Bowd, N. Brye, M. Christopher, R. N. Weinreb, D. J. Krieg-
man, and L. M. Zangwill, “One-vote veto: Semi-supervised learning for
low-shot glaucoma diagnosis,” IEEE Transactions on Medical Imaging,
2023, DOI: 10.1109/TMI.2023.3307689.
[26] R. Fan, X. Ai, and N. Dahnoun, “Road surface 3D reconstruction based
on dense subpixel disparity map estimation,” IEEE Transactions on
Image Processing, vol. 27, no. 6, pp. 3025–3035, 2018.
[27] Y. Cabon, N. Murray, and M. Humenberger, “Virtual KITTI 2,” arXiv
preprint arXiv:2001.10773, 2020.
12
[28] M. Menze and A. Geiger, “Object scene flow for autonomous vehicles,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2015, pp. 3061–3070.
[29] U. Michieli, M. Biasetton, G. Agresti, and P. Zanuttigh, “Adversarial
learning and self-teaching techniques for domain adaptation in semantic
segmentation,” IEEE Transactions on Intelligent Vehicles, vol. 5, no. 3,
pp. 508–518, 2020.
[30] Y. Yang, C. Shan, F. Zhao, W. Liang, and J. Han, “On exploring shape
and semantic enhancements for RGB-X semantic segmentation,” IEEE
Transactions on Intelligent Vehicles, 2023.
[31] J. Fan, F. Wang, H. Chu, X. Hu, Y. Cheng, and B. Gao, “MLFNet: Multi-
level fusion network for real-time semantic segmentation of autonomous
driving,” IEEE Transactions on Intelligent Vehicles, vol. 8, no. 1, pp.
756–767, 2022.
[32] R. Strudel, R. Garcia, I. Laptev, and C. Schmid, “Segmenter: Trans-
former for semantic segmentation,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV), 2021, pp. 7262–
7272.
[33] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,
“SegFormer: Simple and efficient design for semantic segmentation
with transformers,” Advances in Neural Information Processing Systems
(NeurIPS), vol. 34, pp. 12 077–12 090, 2021.
[34] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar,
“Masked-attention mask transformer for universal image segmentation,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2022, pp. 1290–1299.
[35] Q. Ha, K. Watanabe, T. Karasawa, Y. Ushiku, and T. Harada, “MFNet:
Towards real-time semantic segmentation for autonomous vehicles with
multi-spectral scenes,” in 2017 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS).
IEEE, 2017, pp. 5108–5115.
[36] J.-R. Chang and Y.-S. Chen, “Pyramid stereo matching network,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2018, pp. 5410–5418.
[37] X. Guo, K. Yang, W. Yang, X. Wang, and H. Li, “Group-wise correlation
stereo network,” in Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2019, pp. 3273–3282.
[38] H. Xu and J. Zhang, “AANet: Adaptive aggregation network for efficient
stereo matching,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), 2020, pp. 1959–
1968.
[39] X. Cheng, Y. Zhong, M. Harandi, Y. Dai, X. Chang, H. Li, T. Drum-
mond, and Z. Ge, “Hierarchical neural architecture search for deep
stereo matching,” Advances in Neural Information Processing Systems
(NeurIPS), vol. 33, pp. 22 158–22 169, 2020.
[40] L. Lipson, Z. Teed, and J. Deng, “RAFT-Stereo: Multilevel recurrent
field transforms for stereo matching,” in 2021 International Conference
on 3D Vision (3DV).
IEEE, 2021, pp. 218–227.
[41] J. Li, P. Wang, P. Xiong, T. Cai, Z. Yan, L. Yang, J. Liu, H. Fan, and
S. Liu, “Practical stereo matching via cascaded recurrent network with
adaptive correlation,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), 2022, pp. 16 263–
16 272.
[42] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A
survey,” The Journal of Machine Learning Research, vol. 20, no. 1, pp.
1997–2017, 2019.
[43] Z. Teed and J. Deng, “RAFT: Recurrent all-pairs field transforms for
optical flow,” in European Conference on Computer Vision (ECCV).
Springer, 2020, pp. 402–419.
[44] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-
nenson, U. Franke, S. Roth, and B. Schiele, “The CityScapes dataset
for semantic urban scene understanding,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2016,
pp. 3213–3223.
[45] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous
driving? the KITTI vision benchmark suite,” in 2012 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR).
IEEE, 2012, pp.
3354–3361.
[46] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy,
and T. Brox, “A large dataset to train convolutional networks for
disparity, optical flow, and scene flow estimation,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), 2016, pp. 4040–4048.
[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016, pp. 770–778.
[48] S. Okada, M. Ohzeki, and S. Taguchi, “Efficient partition of integer
optimization problems with one-hot encoding,” Scientific Reports, vol. 9,
no. 1, p. 13036, 2019.
[49] K. Sun, B. Xiao, D. Liu, and J. Wang, “Deep high-resolution repre-
sentation learning for human pose estimation,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), 2019, pp. 5693–5703.
[50] C. Yu, C. Gao, J. Wang, G. Yu, C. Shen, and N. Sang, “BiSeNet
V2: Bilateral network with guided aggregation for real-time semantic
segmentation,” International Journal of Computer Vision, vol. 129, pp.
3051–3068, 2021.
[51] Y. Hong, H. Pan, W. Sun, and Y. Jia, “Deep dual-resolution networks
for real-time and accurate semantic segmentation of road scenes,” arXiv
preprint arXiv:2101.06085, 2021.
[52] G. Xu, J. Cheng, P. Guo, and X. Yang, “Attention concatenation
volume for accurate and efficient stereo matching,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), 2022, pp. 12 981–12 990.
[53] Z. Shen, Y. Dai, X. Song, Z. Rao, D. Zhou, and L. Zhang, “PCW-Net:
Pyramid combination and warping cost volume for stereo matching,” in
European Conference on Computer Vision (ECCV).
Springer, 2022,
pp. 280–297.
[54] G. Xu, X. Wang, X. Ding, and X. Yang, “Iterative geometry encoding
volume for stereo matching,” in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR), 2023, pp.
21 919–21 928.
[55] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
arXiv preprint arXiv:1711.05101, 2017.
[56] H. Zhao, X. Puig, B. Zhou, S. Fidler, and A. Torralba, “Open vocabulary
scene parsing,” in Proceedings of the IEEE International Conference on
Computer Vision (ICCV), 2017, pp. 2002–2010.
[57] J. Wei, S. Wang, and Q. Huang, “F3Net: fusion, feedback and focus
for salient object detection,” in Proceedings of the AAAI Conference on
Artificial Intelligence (AAAI), 2020, pp. 12 321–12 328.
[58] Y. Pang, L. Zhang, X. Zhao, and H. Lu, “Hierarchical dynamic filtering
network for RGB-D salient object detection,” in European Conference
on Computer Vision (ECCV).
Springer, 2020, pp. 235–252.
[59] X. Chen, K.-Y. Lin, J. Wang, W. Wu, C. Qian, H. Li, and G. Zeng,
“Bi-directional cross-modality feature propagation with separation-and-
aggregation gate for RGB-D semantic segmentation,” in European
Conference on Computer Vision (ECCV).
Springer, 2020, pp. 561–
577.
[60] Y. Wang, F. Sun, M. Lu, and A. Yao, “Learning deep multimodal feature
representation with asymmetric multi-layer fusion,” in Proceedings of
the 28th ACM International Conference on Multimedia (ACM MM),
2020, pp. 3902–3910.
[61] Y. Wang, W. Huang, F. Sun, T. Xu, Y. Rong, and J. Huang, “Deep mul-
timodal fusion by channel exchanging,” Advances in Neural Information
Processing Systems (NeurIPS), vol. 33, pp. 4835–4845, 2020.
[62] Y. Wang, X. Chen, L. Cao, W. Huang, F. Sun, and Y. Wang, “Multimodal
token fusion for vision transformers,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), 2022,
pp. 12 186–12 195.
","Over the past decade, semantic segmentation has been a widely studied problem in computer vision, with SoTA networks generally falling into two categories: single-modal networks with a single encoder and feature-fusion networks with multiple encoders. Single-modal networks, such as SegNet and U-Net, extract hierarchical deep features from the input image using encoder-decoder architectures. Feature-fusion networks, such as FuseNet and MFNet, integrate deep features learned from multiple modalities (e.g., RGB images and depth maps) to achieve more comprehensive segmentation results.

Stereo matching, on the other hand, has traditionally been dominated by explicit programming-based algorithms, but recent advancements in deep learning have led to the development of end-to-end deep stereo networks. PSMNet and GwcNet are representative examples of these networks, employing spatial pyramid structures and group-wise correlation, respectively, to capture multi-scale information and improve disparity estimation accuracy.nan"
"Beamforming, one of the most promising multi-antenna techniques, harnesses the spatial dimension to effectively alleviate interference. Various QoS metrics are used in wireless networks, including maximum allowable mean square errors, minimum tolerated signal-to-interference-plus-noise ratios, and weighted sum rates. One widely used QoS metric is the weighted sum rate (WSR), making it a fundamental and extensively studied problem to design linear beamformers that maximize WSR under total power constraints. Since the WSR maximization problem is nonconvex and NP-hard even in the single-antenna case, it is challenging to achieve the global optimal solution and suboptimal solutions are of great interest. Assuming perfect CSI, (Björnson, Bengtsson, and Ottersten 2014) points out that the optimal beamforming vectors for the WSR maximization problem in single-cell downlink transmission have a simple structure.","In multi-cell multiuser wireless communication networks, users, especially cell-edge users, may have low-rate data service as a consequence of suffering from both intra-cell interference and inter-cell interference. Beamforming, one of the most promising multi-antenna techniques, harnesses the spatial dimension to effectively alleviate interference in downlink transmissions. This technique is extensively employed in the design of mobile communication systems. However, such precoding technique is contingent upon hardware complexity, often rendering it impractical for mobile terminals characterized by limited computational power and storage capacity. Consequently, multiple transmit antennas are only deployed at the base station (BS) where the issue of computing power is less problematic, while mobile units are equipped with a small number of antennas. This configuration is commonly referred to as a multiple-input and multiple-output (MIMO) system. Beamforming techniques inherently necessitate channel state information (CSI) at each BS to enable precoded transmissions.","Bilevel optimization dates back to the literature (Von Stackelberg 1934). Recently, bilevel optimization has gained significant attention and is widely applied in various machine learning applications including wireless communication (Sun et al. 2022), hyperparameter optimization(Liu et al. 2021; Franceschi et al. 2018), meta learning (Ji et al. 2020; Ji, Yang, and Liang 2021) and neural architecture search (Liu, Simonyan, and Yang 2018; Xue et al. 2021). Bilevel optimization is an optimization problem where a subset of variables is constrained to be optimal for another given optimization problem. Mathematically, a general bilevel optimization takes the following formulation,
min
x
F(x, y)
s.t.
G(x, y) ≤ 0
y ∈ arg min
y'∈Y
{f (x, y') | g (x, y') ≤ 0},
(1)
where F and f denote the upper-level and lower-level objective functions, respectively. x ∈ R^n is the upper-level variable and y ∈ R^m is the lower-level variable. We refer to G and g as the upper-level constraint and the lower-level constraint, respectively. In most existing bilevel optimization works in machine learning tasks, both upper-level constraint and lower-level constraint are not considered due to the characteristics of tasks(Liu et al. 2021; Franceschi et al. 2018; Ji et al. 2020; Ji, Yang, and Liang 2021; Liu, Simonyan, and Yang 2018; Xue et al. 2021). To the best of our knowledge, bilevel optimization has not been applied to robust beamforming designs. This suggests a promising avenue for exploration in adapting bilevel optimization methods to address robust beamforming problems.","In this paper, we consider multi-cell multiuser MISO wireless networks (Tajer, Prasad, and Wang 2011). The main problem of interest is the beamforming optimization with the goal of maximizing the WSR within per BS power constraints in the presence of CSI imperfections. To ensure the worst-case performance, we assume CSI errors are bounded and resort to robust optimization (Yang et al. 2008, 2014). For obtaining such beamformers, we begin by transforming the original robust optimization problem into a bilevel optimization problem, encompassing both upper-level and lower-level constraints. Secondly, we develop a BiLevel based Robust BeamForming (BLRBF) algorithm similar to the centralized method proposed in Appendix A of the reference (Jiao et al. 2023). To be specific, we treat the lower-level optimization problem as a constraint to the upper-level optimization problem and utilize cutting planes to approximate this constraint. Subsequently, inspired by the work (Bü-rger, Notarstefano, and Allgöwer 2014), we extend the BLRBF method to the asynchronous distributed implementation in order to faster approximate the feasible region. This distributed algorithm is referred to as BiLevel based Asynchronous Distributed Robust BeamForming (BLADRBF). Notably, our algorithm can be readily extended to MIMO systems. We prove that both BLRBF and BLADRBF are guaranteed to converge.","We propose to address robust beamforming problems by adopting a bilevel optimization perspective, thereby providing a fresh insight into this field. Focusing on the problem of maximizing the worst-case weighted sum-rate for multi-cell multi-user MISO wireless networks where BSs can acquire only noisy channel estimates, we develop an efficient algorithm, i.e., BLRBF, based on the cutting plane method. A distributed algorithm called BLADRBF is also proposed to facilitate the parallel processing in practical settings. We prove both algorithms are guaranteed to converge. Our algorithm can be readily extended to MIMO systems. Finally, through comprehensive numerical experiments, we demonstrate that the BLRBF method can significantly outperform the SDP-based beamforming method proposed in (Tajer, Prasad, and Wang 2011), particularly in high SNR regimes. We also confirm that the distributed algorithm BLADRBF exhibits a faster convergence rate compared to the centralized algorithm BLRBF.",Robust Beamforming for Downlink Multi-Cell Systems: A Bilevel Optimization Perspective,"Xingdi Chen, Yu Xiong, Kai Yang","Robust Beamforming for Downlink Multi-Cell Systems:
A Bilevel Optimization Perspective
Xingdi Chen1, Yu Xiong1, Kai Yang1,2,3*
1Department of Computer Science and Technology, Tongji University, China
2Key Laboratory of Embedded System and Service Computing Ministry of Education at Tongji University
3Shanghai Research Institute for Intelligent Autonomous Systems
xingdichen@tongji.edu.cn, septerxy@tongji.edu.cn, kaiyang@tongji.edu.cn
Abstract
Utilization of inter-base station cooperation for information
processing has shown great potential in enhancing the overall
quality of communication services (QoS) in wireless commu-
nication networks. Nevertheless, such cooperations require
the knowledge of channel state information (CSI) at base sta-
tions (BSs), which is assumed to be perfectly known. How-
ever, CSI errors are inevitable in practice which necessi-
tates beamforming techniques that can achieve robust per-
formance in the presence of channel estimation errors. Ex-
isting approaches relax the robust beamforming design prob-
lems into semidefinite programming (SDP), which can only
achieve a solution that is far from being optimal. To this end,
this paper views robust beamforming design problems from
a bilevel optimization perspective. In particular, we focus on
maximizing the worst-case weighted sum-rate (WSR) in the
downlink multi-cell multi-user multiple-input single-output
(MISO) system considering bounded CSI errors. We first re-
formulate this problem into a bilevel optimization problem
and then develop an efficient algorithm based on the cutting
plane method. A distributed optimization algorithm has also
been developed to facilitate the parallel processing in prac-
tical settings. Numerical results are provided to confirm the
effectiveness of the proposed algorithm in terms of perfor-
mance and complexity, particularly in the presence of CSI
uncertainties.
Introduction
In multi-cell multiuser wireless communication networks,
users, especially cell-edge users, may have low-rate data
service as a consequence of suffering from both intra-cell
interference and inter-cell interference. Beamforming, one
of the most promising multi-antenna techniques, harnesses
the spatial dimension to effectively alleviate interference in
downlink transmissions. This technique is extensively em-
ployed in the design of mobile communication systems.
However, such precoding technique is contingent upon hard-
ware complexity, often rendering it impractical for mobile
terminals characterized by limited computational power and
storage capacity. Consequently, multiple transmit antennas
are only deployed at the base station (BS) where the issue
of computing power is less problematic, while mobile units
*Corresponding author.
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
are equipped with a small number of antennas. This con-
figuration is commonly referred to as a multiple-input and
multiple-output (MIMO) system. Beamforming techniques
inherently necessitate channel state information (CSI) at
each BS to enable precoded transmissions.
Various quality of communication services (QoS) met-
rics are used in wireless networks, including maximum al-
lowable mean square errors, minimum tolerated signal-to-
interference-plus-noise ratios, and weighted sum rates. One
widely used QoS metric is the weighted sum-rate (WSR),
making it a fundamental and extensively studied problem to
design linear beamformers that maximize WSR under total
power constraints. Since the WSR maximization problem
is nonconvex and NP-hard even in the single-antenna case
(Luo and Zhang 2008), it is challenging to achieve the global
optimal solution and suboptimal solutions are of great inter-
est. Assuming perfect CSI, (Bj¨ornson, Bengtsson, and Ot-
tersten 2014) points out that the optimal beamforming vec-
tors for the WSR maximization problem in single-cell down-
link transmission have a simple structure. Several algorithms
have been proposed for single-cell downlink transmission
(Liu, Zhang, and Chua 2012; Joshi et al. 2012). How-
ever, extending the WSR maximization problem to multi-
cell downlink transmission poses greater challenges. An it-
erative beamformer design based on iterative second-order
cone programming (SOCP) approximation is introduced in
(Tran et al. 2012). Additionally, there are also several dis-
tributed methods for multi-cell systems (Choi et al. 2012;
Weeraddana et al. 2013; Shi et al. 2011; Bogale and Van-
dendorpe 2012). Specifically, (Choi et al. 2012) presents a
fully distributed beamforming technique relying on the high
signal-to-interference-plus-noise ratio (SINR) assumption.
This technique exclusively utilizes local CSI without re-
quiring additional information exchange. (Weeraddana et al.
2013) splits the nonconvex problem into a master problem
that is addressed by a novel sequential convex approxima-
tion, and multiple subproblems that can be solved by BSs
in a fully asynchronous manner through the primal decom-
position technique. Both methods in (Choi et al. 2012) and
(Weeraddana et al. 2013) are restricted to multiple-input
single-output (MISO) systems. The algorithms proposed in
(Shi et al. 2011) and (Bogale and Vandendorpe 2012) are
both based on iterative minimization of mean-square error
(MMSE) and can tackle the WSR maximization problem in
arXiv:2401.11409v1  [cs.IT]  21 Jan 2024
MIMO environments.
Regrettably, achieving perfect CSI at transmitters is
unattainable due to estimation and quantization errors. The
limited amount of feedback bits available for feeding back
CSI gives rise to quantization errors, which are the dominant
source of the uncertainty in CSI (Tajer, Prasad, and Wang
2011). Additionally, the provision of up-to-date CSI at trans-
mitters also remains questionable. Therefore, the considera-
tion of imperfect CSI, known to be detrimental to the per-
formance of those methods assuming perfect CSI, becomes
crucial. This practical constraint has led to the emergence
of robust beamforming techniques, aiming to guarantee the
worst-case performance of the network under CSI imperfec-
tions.
Two main approaches exist for modeling the uncertainty
region of CSI errors. The first approach employs the prob-
abilistic model, treating errors as a random variable with
some known distribution (Weber, Sklavos, and Meurer 2006;
Rong, Vorobyov, and Gershman 2006; Zhang, Palomar, and
Ottersten 2008; Shenouda and Davidson 2008; Joudeh and
Clerckx 2016). Most of these works aim at optimizing a util-
ity function by averaging over the entire uncertainty region.
In this paper, we adhere to the second approach, wherein the
uncertainty region of CSI perturbations is confined within
given bounded uncertainty sets (Vucic and Boche 2009;
Tajer, Prasad, and Wang 2011; Shen et al. 2012; Shaver-
dian and Nakhai 2014; Zhou et al. 2020). Making no as-
sumption on the distribution of CSI errors, this approach
matches well with the quantization errors. Moreover, this
method also works for unbounded errors as long as the sys-
tem outage probability is controlled.
Incorporating CSI imperfections leads to the formulation
of robust optimization problems. (Vucic and Boche 2009)
minimizes the transmit power under a predetermined set of
QoS constraints for users. In the presence of bounded CSI
errors, the constraints are infinite due to the fact that the QoS
requirements must be supported for an infinite number of
possible channels within the uncertainty regions. To address
this issue, authors employ semidefinite programming (SDP)
along with a lemma to convert an infinite set of constraints
into a finite set of constraints, making the problem compu-
tationally tractable. Both (Shen et al. 2012) and (Shaver-
dian and Nakhai 2014) adopt techniques like semidefinite
relaxation (SDR) and the S-Lemma to reformulate the orig-
inal optimization problem into a numerically tractable one.
They then propose distributed algorithms based on ADMM
and primal decomposition approaches, respectively. For the
problem of maximizing the worst-case WSR of the network,
(Tajer, Prasad, and Wang 2011) provides a lower-bound so-
lution by introducing an additional function and then trans-
forming the problem into the weighted sum of the worst-
case mean-square error minimization problem that can be
solved by SDP. However, as the system scales up, for exam-
ple, with an increase in the number of cells and the number
of transmitting antennas on BSs, these algorithms become
impractical due to the time-consuming nature of SDP.
Bilevel
optimization
dates
back
to
the
literature
(Von Stackelberg 1934). Recently, bilevel optimization
has gained significant attention and is widely applied in
various machine learning applications including wireless
communication (Sun et al. 2022), hyperparameter opti-
mization(Liu et al. 2021; Franceschi et al. 2018), meta
learning (Ji et al. 2020; Ji, Yang, and Liang 2021) and
neural architecture search (Liu, Simonyan, and Yang 2018;
Xue et al. 2021). Bilevel optimization is an optimization
problem where a subset of variables is constrained to be
optimal for another given optimization problem. Mathemat-
ically, a general bilevel optimization takes the following
formulation,
min
x
F(x, y)
s.t.
G(x, y) ≤ 0
y ∈ arg min
y′∈Y
{f (x, y′) | g (x, y′) ≤ 0} ,
(1)
where F and f denote the upper-level and lower-level ob-
jective functions, respectively. x ∈ Rn is the upper-level
variable and y ∈ Rm is the lower-level variable. We re-
fer to G and g as the upper-level constraint and the lower-
level constraint, respectively. In most existing bilevel opti-
mization works in machine learning tasks, both upper-level
constraint and lower-level constraint are not considered due
to the characteristics of tasks(Liu et al. 2021; Franceschi
et al. 2018; Ji et al. 2020; Ji, Yang, and Liang 2021; Liu,
Simonyan, and Yang 2018; Xue et al. 2021). To the best of
our knowledge, bilevel optimization has not been applied to
robust beamforming designs. This suggests a promising av-
enue for exploration in adapting bilevel optimization meth-
ods to address robust beamforming problems.
In this paper, we consider multi-cell multiuser MISO
wireless networks (Tajer, Prasad, and Wang 2011). The main
problem of interest is the beamforming optimization with
the goal of maximizing the WSR within per BS power
constraints in the presence of CSI imperfections. To en-
sure the worst-case performance, we assume CSI errors
are bounded and resort to robust optimization (Yang et al.
2008, 2014). For obtaining such beamformers, we begin
by transforming the original robust optimization problem
into a bilevel optimization problem, encompassing both
upper-level and lower-level constraints. Secondly, we de-
velop a BiLevel based Robust BeamForming (BLRBF) al-
gorithm similar to the centralized method proposed in Ap-
pendix A of the reference (Jiao et al. 2023). To be spe-
cific, we treat the lower-level optimization problem as a
constraint to the upper-level optimization problem and uti-
lize cutting planes to approximate this constraint. Subse-
quently, inspired by the work (B¨urger, Notarstefano, and
Allg¨ower 2014), we extend the BLRBF method to the asyn-
chronous distributed implementation in order to faster ap-
proximate the feasible region. This distributed algorithm
is referred to as BiLevel based Asynchronous Distributed
Robust BeamForming (BLADRBF). Notably, our algorithm
can be readily extended to MIMO systems. We prove that
both BLRBF and BLADRBF are guaranteed to converge.
Contributions. Our main contributions are summarized
as follows:
• We are the first to propose viewing robust beamforming
design problems from a bilevel optimization perspective.
Unlike conventional methods that rely on SDP, which are
computationally expensive and can only achieve a solu-
tion that is far from being optimal. The fresh perspec-
tive provides new insights into solving such problems
and offers a promising alternative with the potential for
improved performance.
• To illustrate the application of bilevel optimization, we
present a novel bilevel based formulation and develop a
cutting plane based algorithm called BLRBF. This ap-
proach efficiently handles the challenging task of maxi-
mizing worst-case weighted sum-rates.
• We also propose an asynchronous distributed algorithm
(BLADRBF) to facilitate the parallel processing in prac-
tical settings. The asynchronism gives the algorithm a
high robustness against failures in the communication.
Importantly, both algorithms are mathematically proven
to converge.
System Model
In this section, we consider a multi-cell MISO downlink sys-
tem with M cells each equipped with one BS with N anten-
nas that serves single-antenna K users. The BS of the mth
cell and the kth user in the mth cell are denoted by Bm and
Ukm, respectively. The transmitted signal from Bm is given
by
xm =
K
X
k=1
vkmskm,
(2)
where vkm ∈ CN×1 represents the beamformer that the mth
BS uses to transmit the signal skm ∼ CN(0, 1) to user Ukm
and we assume E[|skm|2] = 1. Then, the received signal at
Ukm is given by
ykm = hkmmvkmskm
|
{z
}
the desired signal
+
X
l̸=k
hkmmvlmslm
|
{z
}
intra-cell interference
+
X
n̸=m
X
l
hkmnvlnsln + nkm
|
{z
}
inter-cell interference plus noise
,
(3)
where hkmn ∈ C1×N represents the channel from BSn to
Ukm and nkm ∼ CN(0, σ2
km) denotes the additive com-
plex white Gaussian noise of Ukm. Accordingly, the SINR
of Ukm can be written as
SINRkm({v1m, · · · , vKm}M
m=1)
=
|hkmmvkm|2
P
l̸=k |hkmmvlm|2 + P
n̸=m
P
l |hkmnvln|2 + σ2
km
.
(4)
It is assumed that the Bm knows only erroneous channel es-
timates {ehkmn}, i.e.,
hkmn = ehkmn + ˆ∆kmn,
∀m, n ∈ {1, ..., M},
and
∀k ∈ {1, ..., K},
(5)
where ˆ∆kmn is the channel estimation errors, which are un-
known to BSs. Furthermore, the BSs are supposed to know
the structure of the uncertainty regions, which in this paper,
are bounded and defined as origin-centered hyper-spherical
region of radius ϵkmn, i.e., ∥ ˆ∆kmn∥2 ≤ ϵkmn.
For notational simplicity, we denote the beamformer of
Bm by ˆV m = [v1m, v2m, . . . , vKm] ∈ CN×K. Then,
the problem of interest is to find the transmit beamformers
{ ˆV m} such that the worst-case WSR of the network is max-
imized, while the power of each BS is constrained. Mathe-
matically, this problem can be formulated as
max
{ ˆ
V m}
min
{ ˆ
∆kmn}
PM
m=1
PK
k=1 αkm log (1 + SINRkm)
s. t.
 ˆV m

2
F ≤ Pm
∀m,
(6)
where αkm is the positive weighting factor corresponding to
the rate of Ukm and Pm is the power budget of the BS Bm.
The objective function of the problem (6) is known as the
WSR utility function. This problem focuses solely on maxi-
mizing the throughput of the network, disregarding the mini-
mum rate requirements of individual users. This utility func-
tion is proved to be highly suitable for scenarios where users
can tolerate delays due to the fact that certain users may not
receive any resources during specific scheduling frames for
the benefit of the network throughput in the extreme case
(Rong, Vorobyov, and Gershman 2006). The incorporation
of weighting factors enables us to assign different priorities
to individual users, thereby allowing us to cater to diverse
user needs. Additionally, these weights can be dynamically
adjusted over time to ensure long term fairness.
The WSR utility function is nonconvex and involves CSI
and beamformers of all BSs. Thus, this optimization prob-
lem is quite complicated.
BLRBF and BLADRBF Methods
In this section, we propose a BiLevel based Asynchronous
Distributed Robust BeamForming (BLADRBF) method to
solve the problem (6) in an asynchronous distributed man-
ner. We first transform the problem (6) into a bilevel
optimization problem and then introduce the centralized
method, i.e., BLRBF, similar to the method CPBO proposed
in Appendix A of the reference (Jiao et al. 2023). CPBO
deals with problems where there are no constraints at both
the upper and lower levels. Unlike them, our problem has
both convex upper-level constraints and convex lower-level
constraints. Finally, we extend this centralized method to the
asynchronous distributed implementation, i.e., BLADRBF.
BLRBF: Bilevel Based Robust Beamforming
For notational simplicity, we split { ˆV m} into real and imag-
inary components and then arrange these components into
a single vector, i.e., V ≜ [Vec(Re( ˆV )), Vec(Im( ˆV ))] ∈
R2MNK×1. And ∆ ∈ R2M 2NK×1 is defined in the same
way.
First, we define
f(V , ∆) =
M
X
m=1
K
X
k=1
αkm log (1 + SINRkm) .
(7)
Then, from the perspective of bilevel optimization, problem
(6) can be written as
min
V
−f(V , ∆)
s.t.
∥ ˆV m∥2
F ≤ Pm,
∀m
∆ = arg min
∆′
f
where cpi represents the i-th cutting plane in D[t] and
Drop
Algorithm 2: BLADRBF: BiLevel based Asynchronous Dis-
tributed Robust BeamForming.
Input: P ,
n
ehkmn, ϵkmn
o
.
Output: V , ∆.
1: for each BSl do
2:
Initialize iteration t = 0, variables V [0]
l , ∆[0]
l
with
different values and dual variables
n
λ[0]
l,i
o
= 0;
3: end for
4: for each BSl do
5:
repeat
6:
Each BS update variables V [t+1]
l
, ∆[t+1]
l
like algo-
rithm 1;
7:
if t mod kpre == 0 then
8:
It transmits its current D[t+1]
l
to all its out-
neighbors NO(l) and receives active constrains
of its in-neighbors Y [t+1]
l
= S
j∈NI(l) D[t+1]
j
;
9:
D[t+1]
l
← D[t+1]
l
S Y [t+1]
l
;
10:
end if
11:
t ← t + 1;
12:
until convergence.
13: end for
where the feasible region is denoted by Sm. Taking Figure
1 as an example, cutting planes Di, i = 1, ..., 3 are gener-
ated by three different BSs every kpre iteration in order to
approximate its own feasible region Si simultaneously and
will be passed between BSs. Three cutting plane constraints
combined can approximate global feasible region S more
precisely. Compared with BLRBF which generates only one
cutting plane constraint every kpre iteration, this distributed
algorithm can generate three cutting plane constraints. The
details of BLADRBF are presented in algorithm 2. The sub-
script l represents variables and sets at BSl.
Note that the algorithm 2 operates without the need for
time synchronization. Each BS has the flexibility to con-
duct its own computations at varying speeds and can update
its constraints as soon as it receives relevant cutting planes.
The asynchronous distributed method is more robust against
communication failures.
Proposition 1. The linear constraints at a BS form a poly-
hedral approximation of the feasible region S.
The detailed proof of Proposition 1 is given in Appendix C.
Theorem 3. (Consistent) Let F i be the convergence value
computed by BSi. We have that F i = F j = F, ∀i, j.
The proof of Theorem 3 is presented in Appendix D.
Experiments
In this section, numerical simulations are carried out to il-
lustrate the performance of BLRBF and BLADRBF algo-
rithms. We consider multi-cell multi-user MISO downlink
systems. The specific parameters including the number of
cells M, the number of users K, the number of antennas
N, and the transmit power budget P are provided alongside
10
5
0
5
10
15
20
25
30
35
40
SNR(dB)
0
5
10
15
20
25
30
WSR(bits/sec/Hz)
WMMSE with perfect CSI
SDP-based beamforming, =0.0
SDP-based beamforming, =0.05
SDP-based beamforming, =0.1
BLRBF, =0.0
BLRBF, =0.05
BLRBF, =0.1
Figure 2: Comparing the worst-case weighted sum-rates
yielded by BLRBF, SDP-based beamforming and WMMSE
algorithms for M = N = K = 2.
corresponding figures. We adopt a typical small-scale fading
channel model,i.e., Rayleigh fading, which is widely used in
previous literature (Choi et al. 2012; Zhang et al. 2022).
Rayleigh fading: Each channel coefficient hkmn is gen-
erated according to a complex standard normal distribution,
i.e.,
Re(hkmn) ∼ CN(0, I)
√
2
, Im(hkmn) ∼ CN(0, I)
√
2
, ∀m, n, k.
We contrast the proposed algorithms with the SDP-based
beamforming proposed in (Tajer, Prasad, and Wang 2011).
Under the assumption of perfect CSI, the results are com-
pared with the WMMSE method proposed in (Shi et al.
2011). All simulation experiments are executed on a ma-
chine equipped with a 16-core AMD Ryzen 7 5800H pro-
cessor.
We perform an analysis of the robust WSR maximization
across three systems with parameters M = N = K = 2,
M = K = 3, N = 4 and M = 4, K = 10, N = 64.
For each of these system configurations, we consider 10 er-
roneous channel realizations to illustrate the performance of
the proposed algorithms.
Figure
2
displays
the
optimized
worst-case
sum-
rate achieved by BLRBF, SDP-based beamforming, and
WMMSE algorithms. If CSI is assumed to be perfectly
known, i.e., ϵkmn
=
0 for all k, m, n, we find that
both BLRBF and SDP-based beamforming algorithms can
achieve comparable performance with the WMMSE al-
gorithm, while BLRBF approach slightly outperforms the
SDP-based beamforming method. Next, we explore BLRBF
and SDP-based beamforming algorithms for different uncer-
tainty regions with radii of 0.05 and 0.1. The key obser-
vation is that larger uncertainty regions lead to diminished
robust weighted sum rates. This is primarily because larger
regions of uncertainty indicate more significant channel esti-
mation errors, which consequently lead to a reduced worst-
case weighted sum rate. Moreover, as SNR increases, the
BLRBF method outperforms the SDP-based beamforming
method in terms of achieving significant improvements in
robust weighted sum rates.
10
5
0
5
10
15
20
25
30
35
40
45
SNR(dB)
0
10
20
30
40
50
WSR(bits/sec/Hz)
WMMSE with perfect CSI
SDP-based beamforming, =0.0
SDP-based beamforming, =0.05
SDP-based beamforming, =0.1
BLRBF, =0.0
BLRBF, =0.05
BLRBF, =0.1
Figure 3: Comparing the worst-case weighted sum-rates
yielded by BLRBF, SDP-based beamforming and WMMSE
algorithms for M = K = 3, N = 4.
Continuing, we scale up the system, focusing on a con-
figuration with M = K = 3 and N = 4. Figure 3 illus-
trates the relationship between the robust weighted sum rate
and SNR, consistent with Figure 2. This is because higher
SNR means more base station power P , assuming con-
stant noise. In communication systems, greater SNR lowers
noise interference, enhancing data reliability and boosting
the weighted sum rate. Additionally, we observe that the op-
timized robust weighted sum rate saturates at high SNR lev-
els which aligns with the high SNR analysis and Theorem
7 in (Tajer, Prasad, and Wang 2011). Despite both BLRBF
and SDP-based beamforming algorithms saturating at high
SNR for ϵ ̸= 0, it is noteworthy that the BLRBF method
achieves saturation at a higher SNR level, and its saturation
value surpasses that of SDP-based beamforming method. It
is also revealed that at the same SNR, the BLRBF algorithm
outperforms the SDP-based beamforming algorithm, yield-
ing an optimized robust weighted sum rate that is superior.
Lastly, we delve into the analysis of a network with a
more extensive setup, that is M = 4, K = 10, N = 64.
This setup emulates a more realistic large-scale scenario.
When the matrix size is large, SDP solved through the in-
terior point method becomes computationally expensive and
even intolerable. Specifically, the computation time of the
SDP-based beamforming method exceeds 4 hours under the
scenario settings of M = 4, K = 10, N = 64. In contrast,
our proposed BLRBF approach can efficiently achieve the
optimized results within an acceptable time frame. In Figure
4, the worst-case weighted sum rates versus SNRs for this
setup are depicted. This analysis demonstrates the capability
of our proposed BLRBF algorithm to handle large-scale sce-
narios effectively, even in situations where the SDP-based
beamforming method faces computational challenges.
Finally, we demonstrate the disparity in terms of con-
vergence rates between the BLRBF algorithm and the
BLADRBF algorithm in Figure 5. It is evident that while
both BLRBF and BLADRBF algorithms attain similar fi-
nal convergence values, the BLADRBF approach exhibits a
more rapid convergence.
10
5
0
5
10
15
20
25
30
SNR(dB)
70
110
150
190
230
270
310
350
390
WSR(bits/sec/Hz)
WMMSE with perfect CSI
BLRBF, =0.0
BLRBF, =0.05
BLRBF, =0.1
Figure 4: The worst-case weighted sum-rates yielded by
BLRBF algorithms for M = 4, K = 10, N = 64.
0
105
210
315
420
time(s)
0
4
8
12
16
20
24
WSR(bits/sec/Hz)
BLRBF
BLADRBF
Figure 5: Comparing the convergence rate of BLRBF and
BLADRBF algorithms for M = 3, K = 3, N = 4.
Conclusion
We propose to address robust beamforming problems by
adopting a bilevel optimization perspective, thereby provid-
ing a fresh insight into this field. Focusing on the problem
of maximizing the worst-case weighted sum-rate for multi-
cell multi-user MISO wireless networks where BSs can ac-
quire only noisy channel estimates, we develop an efficient
algorithm, i.e., BLRBF, based on the cutting plane method.
A distributed algorithm called BLADRBF is also proposed
to facilitate the parallel processing in practical settings. We
prove both algorithms are guaranteed to converge. Our al-
gorithm can be readily extended to MIMO systems. Finally,
through comprehensive numerical experiments, we demon-
strate that the BLRBF method can significantly outperform
the SDP-based beamforming method proposed in (Tajer,
Prasad, and Wang 2011), particularly in high SNR regimes.
We also confirm that the distributed algorithm BLADRBF
exhibits a faster convergence rate compared to the central-
ized algorithm BLRBF.
Acknowledgments
This work was supported in part by the National Natural
Science Foundation of China under Grant 12371519 and
61771013; in part by the Fundamental Research Funds for
the Central Universities of China; and in part by the Funda-
mental Research Funds of Shanghai Jiading District.
References
Bj¨ornson, E.; Bengtsson, M.; and Ottersten, B. 2014. Opti-
mal Multiuser Transmit Beamforming: A Difficult Problem
with a Simple Solution Structure [Lecture Notes]. IEEE Sig-
nal Processing Magazine, 31(4): 142–148.
Bogale, T. E.; and Vandendorpe, L. 2012. Weighted Sum
Rate Optimization for Downlink Multiuser MIMO Coordi-
nated Base Station Systems: Centralized and Distributed Al-
gorithms. IEEE Transactions on Signal Processing, 60(4):
1876–1889.
Boyd, S.; and Vandenberghe, L. 2007.
Localization and
cutting-plane methods.
From Stanford EE 364b lecture
notes, 386.
B¨urger, M.; Notarstefano, G.; and Allg¨ower, F. 2014.
A
Polyhedral Approximation Framework for Convex and Ro-
bust Distributed Optimization. IEEE Transactions on Auto-
matic Control, 59(2): 384–395.
Choi, H.-J.; Park, S.-H.; Lee, S.-R.; and Lee, I. 2012.
Distributed Beamforming Techniques for Weighted Sum-
Rate Maximization in MISO Interfering Broadcast Chan-
nels.
IEEE Transactions on Wireless Communications,
11(4): 1314–1320.
Franceschi, L.; Frasconi, P.; Salzo, S.; Grazzi, R.; and Pon-
til, M. 2018. Bilevel programming for hyperparameter opti-
mization and meta-learning. In International conference on
machine learning, 1568–1577. PMLR.
Ji, K.; Lee, J. D.; Liang, Y.; and Poor, H. V. 2020. Conver-
gence of Meta-Learning with Task-Specific Adaptation over
Partial Parameters. In Advances in Neural Information Pro-
cessing Systems, 11490–11500.
Ji, K.; Yang, J.; and Liang, Y. 2021. Bilevel optimization:
Convergence analysis and enhanced design. In International
conference on machine learning, 4882–4892. PMLR.
Jiao, Y.; Yang, K.; and Song, D. 2022.
Distributed Dis-
tributionally Robust Optimization with Non-Convex Objec-
tives. In Advances in Neural Information Processing Sys-
tems, 7987–7999.
Jiao, Y.; Yang, K.; Wu, T.; Song, D.; and Jian, C. 2023.
Asynchronous distributed bilevel optimization. In Interna-
tional Conference on Learning Representations.
Jorge, N.; and Stephen, J. W. 2006. Numerical optimization.
Spinger.
Joshi, S. K.; Weeraddana, P. C.; Codreanu, M.; and Latva-
aho, M. 2012. Weighted Sum-Rate Maximization for MISO
Downlink Cellular Networks via Branch and Bound. IEEE
Transactions on Signal Processing, 60(4): 2090–2095.
Joudeh, H.; and Clerckx, B. 2016. Sum-Rate Maximization
for Linearly Precoded Downlink Multiuser MISO Systems
With Partial CSIT: A Rate-Splitting Approach. IEEE Trans-
actions on Communications, 64(11): 4847–4861.
Liu, H.; Simonyan, K.; and Yang, Y. 2018. Darts: Differ-
entiable architecture search. In International Conference on
Learning Representations.
Liu, L.; Zhang, R.; and Chua, K.-C. 2012. Achieving Global
Optimality for Weighted Sum-Rate Maximization in the K-
User Gaussian Interference Channel with Multiple Anten-
nas. IEEE Transactions on Wireless Communications, 11(5):
1933–1945.
Liu, R.; Liu, X.; Yuan, X.; Zeng, S.; and Zhang, J. 2021. A
value-function-based interior-point method for non-convex
bi-level optimization. In International Conference on Ma-
chine Learning, 6882–6892. PMLR.
Luo, Z.-Q.; and Zhang, S. 2008. Dynamic Spectrum Man-
agement: Complexity and Duality. IEEE Journal of Selected
Topics in Signal Processing, 2(1): 57–73.
Rong, Y.; Vorobyov, S.; and Gershman, A. 2006. Robust lin-
ear receivers for multiaccess space-time block-coded MIMO
systems: a probabilistically constrained approach.
IEEE
Journal on Selected Areas in Communications, 24(8): 1560–
1570.
Shaverdian, A.; and Nakhai, M. R. 2014. Robust Distributed
Beamforming With Interference Coordination in Downlink
Cellular Networks. IEEE Transactions on Communications,
62(7): 2411–2421.
Shen, C.; Chang, T.-H.; Wang, K.-Y.; Qiu, Z.; and Chi, C.-
Y. 2012. Distributed Robust Multicell Coordinated Beam-
forming With Imperfect CSI: An ADMM Approach. IEEE
Transactions on Signal Processing, 60(6): 2988–3003.
Shenouda, M. B.; and Davidson, T. N. 2008. On the Design
of Linear Transceivers for Multiuser Systems with Channel
Uncertainty. IEEE Journal on Selected Areas in Communi-
cations, 26(6): 1015–1024.
Shi, Q.; Razaviyayn, M.; Luo, Z.-Q.; and He, C. 2011. An
Iteratively Weighted MMSE Approach to Distributed Sum-
Utility Maximization for a MIMO Interfering Broadcast
Channel. IEEE Transactions on Signal Processing, 59(9):
4331–4340.
Sun, H.; Pu, W.; Fu, X.; Chang, T.-H.; and Hong, M. 2022.
Learning to Continuously Optimize Wireless Resource in a
Dynamic Environment: A Bilevel Optimization Perspective.
IEEE Transactions on Signal Processing, 70: 1900–1917.
Tajer, A.; Prasad, N.; and Wang, X. 2011. Robust Linear Pre-
coder Design for Multi-Cell Downlink Transmission. IEEE
Transactions on Signal Processing, 59(1): 235–251.
Tran, L.-N.; Hanif, M. F.; Tolli, A.; and Juntti, M. 2012. Fast
Converging Algorithm for Weighted Sum Rate Maximiza-
tion in Multicell MISO Downlink. IEEE Signal Processing
Letters, 19(12): 872–875.
Von Stackelberg, H. 1934. Marktform und gleichgewicht.
Vucic, N.; and Boche, H. 2009. Robust QoS-Constrained
Optimization of Downlink Multiuser MISO Systems. IEEE
Transactions on Signal Processing, 57(2): 714–725.
Weber, T.; Sklavos, A.; and Meurer, M. 2006.
Imperfect
channel-state information in MIMO transmission.
IEEE
Transactions on Communications, 54(3): 543–552.
Weeraddana, P. C.; Codreanu, M.; Latva-aho, M.; and
Ephremides, A. 2013. Multicell MISO Downlink Weighted
Sum-Rate Maximization: A Distributed Approach.
IEEE
Transactions on Signal Processing, 61(3): 556–570.
Xue, C.; Wang, X.; Yan, J.; Hu, Y.; Yang, X.; and Sun, K.
2021. Rethinking bi-level optimization in neural architec-
ture search: A gibbs sampling perspective. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 35,
10551–10559.
Yang, K.; Huang, J.; Wu, Y.; Wang, X.; and Chiang, M.
2014. Distributed robust optimization (DRO), part I: Frame-
work and example. Optimization and Engineering, 15(1):
35–67.
Yang, K.; Wu, Y.; Huang, J.; Wang, X.; and Verdu, S. 2008.
Distributed Robust Optimization for Communication Net-
works. In IEEE INFOCOM 2008 - The 27th Conference on
Computer Communications, 1157–1165.
Zhang, J.; Yuan, Y.; Zheng, G.; Krikidis, I.; and Wong, K.-
K. 2022. Embedding Model-Based Fast Meta Learning for
Downlink Beamforming Adaptation. IEEE Transactions on
Wireless Communications, 21(1): 149–162.
Zhang, X.; Palomar, D. P.; and Ottersten, B. 2008. Statisti-
cally Robust Design of Linear MIMO Transceivers. IEEE
Transactions on Signal Processing, 56(8): 3678–3689.
Zhou, G.; Pan, C.; Ren, H.; Wang, K.; Renzo, M. D.; and
Nallanathan, A. 2020. Robust Beamforming Design for In-
telligent Reflecting Surface Aided MISO Communication
Systems.
IEEE Wireless Communications Letters, 9(10):
1658–1662.
","nanVarious quality of communication services (QoS) metrics are used in wireless networks, including maximum allowable mean square errors, minimum tolerated signal-to-interference-plus-noise ratios, and weighted sum rates. One widely used QoS metric is the weighted sum rate (WSR), making it a fundamental and extensively studied problem to design linear beamformers that maximize WSR under total power constraints. Since the WSR maximization problem is nonconvex and NP-hard even in the single-antenna case (Luo and Zhang 2008), it is challenging to achieve the global optimal solution and suboptimal solutions are of great interest. Assuming perfect CSI, (Björnson, Bengtsson, and Ottersten 2014) points out that the optimal beamforming vectors for the WSR maximization problem in single-cell downlink transmission have a simple structure. Several algorithms have been proposed for single-cell downlink transmission (Liu, Zhang, and Chua 2012; Joshi et al. 2012). How-ever, extending the WSR maximization problem to multi-cell downlink transmission poses greater challenges. An iterative beamformer design based on iterative second-order cone programming (SOCP) approximation is introduced in (Tran et al. 2012). Additionally, there are also several distri-buted methods for multi-cell systems (Choi et al. 2012; Weeraddana et al. 2013; Shi et al. 2011; Bogale and Vandendorpe 2012). Specifically, (Choi et al. 2012) presents a fully distributed beamforming technique relying on the high signal-to-interference-plus-noise ratio (SINR) assumption."
"This paper presents a model for event entity extraction tasks in the financial domain. The model, SEBERTNets, builds upon the BERT framework and incorporates a sequence layer to capture sequence semantic information. Furthermore, a multi-channel recall method is introduced to effectively retrieve all the corresponding event entities. The effectiveness of SEBERTNets and its variants is demonstrated through experimental results on a real-world dataset.","Event extraction is a vital task in investment analysis and asset management, and the 2019 China conference on knowledge graph and semantic computing (CCKS) hosts a challenge focusing on this task in the finance field. Previous research has primarily neglected the finance domain, and there are questions regarding the accurate and effective extraction of event entities. This study proposes SEBERTNets, a model that inherits the advantages of BERT while capturing sequence semantic information, and HSEBERTNets, which utilizes a multi-channel recall method for comprehensive entity retrieval.","The SEBERTNets model consists of four primary components: Input layer, BERT layer, Sequence layer, and Output layer. The Input layer encodes the text description and event type, which are processed by the BERT layer to capture global and sequence semantic information. The Sequence layer further enhances the semantic understanding by incorporating forward and backward LSTM for contextual representation. Finally, the Output layer predicts the start and end positions of the event within the text, resulting in the extraction of the event entity.","Experimental results on a real-world dataset demonstrate the effectiveness of SEBERTNets and its variants. On a preliminary test dataset, SEBERTNets outperforms the baseline BERT model, achieving a significant improvement in F1 score. Furthermore, HSEBERTNets showcases superior performance, particularly in cases with multiple event entities. Visualization of the BERT layer highlights its ability to focus on relevant entities within the text.","The proposed SEBERTNets model and its variants effectively enhance event entity extraction in the financial domain. SEBERTNets inherits the advantages of BERT while capturing sequence semantic information, leading to improved performance. Additionally, the HSEBERTNets model utilizes a multi-channel recall method to effectively retrieve all the corresponding event entities. The experimental results underscore the efficacy of the proposed methods, with SEBERTNets achieving an F1 score of 0.905 and HSEBERTNets attaining an F1 score of 0.934, demonstrating the robustness and potential of the models.",SEBERTNets: Sequence Enhanced BERT Networks for Event Entity Extraction Tasks Oriented to the Finance Field,"Congqing He, Xiangyu Zhu, Yuquan Le, Yuzhong Liu, Jianhong Yin","SEBERTNets: Sequence Enhanced BERT
Networks for Event Entity Extraction Tasks
Oriented to the Finance Field
Congqing He1, Xiangyu Zhu1, Yuquan Le1, and Yuzhong Liu1 Jianhong Yin1
JD Digital
hecongqing@hotmail.com
Abstract. Event extraction lies at the cores of investment analysis and
asset management in the financial field, and thus has received much at-
tention. The 2019 China conference on knowledge graph and semantic
computing (CCKS) challenge sets up a evaluation competition for event
entity extraction task oriented to the finance field. In this task, we mainly
focus on how to extract the event entity accurately, and recall all the
corresponding event entity effectively. In this paper, we propose a novel
model, Sequence Enhanced BERT Networks (SEBERTNets for short),
which can inherit the advantages of the BERT,and while capturing se-
quence semantic information. In addition, motivated by recommendation
system, we propose Hybrid Sequence Enhanced BERT Networks (HSE-
BERTNets for short), which uses a multi-channel recall method to recall
all the corresponding event entity. The experimental results show that,
the F1 score of SEBERTNets is 0.905 in the first stage, and the F1 score
of HSEBERTNets is 0.934 in the first stage, which demonstarate the
effectiveness of our methods.
Keywords: Event Extraction · BERT · Finance.
1
Introduction
Event Extraction, a challenging task Information Extraction, aims at detecting
and typing events, and extracting different entity from texts. Event extraction
lies at the cores of investment analysis and asset management in the financial
field, and thus has received much attention. However, most of research is rarely
involved in the finance field. For this purpose, the 2019 China conference on
knowledge graph and semantic computing (CCKS) challenge sets up a evaluation
competition for event entity extraction task oriented to the finance field. The
goal of the evaluation is to extract the event entity according to a given real
news corpus and event type.
To this end, Liu et al. [5] proposed to exploit argument information explicitly
for event detection via supervised attention mechanisms. There are still several
questions. (1) How to extract the event entity accurately and effectively. (2) How
to recall all the corresponding event entity effectively when there exists multiple
event entity in a text. Motivated by these, we propose a novel model, Sequence
arXiv:2401.11408v1  [cs.CL]  21 Jan 2024
2
He et al.
Enhanced BERT Networks, which can inherit the advantages of the BERT [1],
and while capturing sequence semantic information. In addition, motivated by
recommendation system, we propose Hybrid Sequence Enhanced BERT Net-
works , which uses a multi-channel recall method to recall all the corresponding
event entity.
Our mainly contributions are shown as follows:
(1) We propose a novel model, Sequence Enhanced BERT Networks, which
can inherit the advantages of the BERT,and while capturing sequence semantic
information.
(2) We propose Hybrid Sequence Enhanced BERT Networks , which uses a
multi-channel recall method to recall all the corresponding event entity.
(3) The experimental results show that, the F1 score of SEBERTNets is 0.905
in the first stage, and the F1 score of HSEBERTNets is 0.934 in the first stage,
which demonstarate the effectiveness of our methods.
2
Our Methods
Figure 1 describes the architecture of SEBERTNets, which primarily involves
the following four components: (i) Input layer, (ii) BERT layer, (iii) Sequence
layer, (iv) Output layer.
2.1
Input Layer
The description of text can be seen as a char sequence x = {x1, · · · , xn}, and the
corresponding event type can be seen as a char sequence t = {t1, · · · , tm}. We
first remove irregular punctuation, special text, etc, and contact them together.
The description of text x and the corresponding event type t are then encoded
as input.
2.2
BERT Layer
Inspired by Devlin et al [1], BERT obtains new state-of-the-art results on lots of
natural language processing tasks. We introduce BERT as the layer of SEBERT-
Nets, to produce sequence semantic representation, and capture global semantic
representation.
l1, l2, · · · , ln+m = BERT(x1, · · · , xn, t1, · · · , tm)
(1)
Where li ∈ Rd, 1 and i represents i − th element.
1 In our experiment, d=768.
Title Suppressed Due to Excessive Length
3
C
T1
Tn
T[sep]
T'1
T'M
Start End
BERT
[CLS]
Tok 1
Tok n
[SEP]
Tok 1
Tok M
Text Description
Event Type
E[CLS]
ETok 1
ETok n
E[SEP]
ETok 1
ETok M
Fig. 1. The architecture of SEBERTNets.
2.3
Sequence Layer
Since the BERT layer only captures the position information of the text through
the position vector, then the SEBERTNets introduces the Sequence layer, which
captures the sequence semantic information of the text, and enhances the seman-
tics of the text sequence based on the BERT layer. In addition, the Sequence
layer introduces a mask operation. Since the text is usually indefinitely long,
the introduction of a mask can effectively alleviate the bias caused by the text
filling.
ft = σ(Wflt + Ufht−1 + bf),
(2)
it = σ(Wilt + Uiht−1 + bi),
(3)
ot = σ(Wolt + Uoht−1 + bo),
(4)
ect = tanh(Wcxt + Ucht−1 + bc),
(5)
ct = ft ⊙ ct−1 + it ⊙ ect,
(6)
ht = ot ⊙ tanh(ct).
(7)
Where lt represents the input of the recurrent layer at time step t. ft, it and ot
means forget gate, input gate, and output gate respectively. ⊙ denotes element-
4
He et al.
wise multiplication of two vectors. To consider forward and backward contextual
representation of the text, we use BiLSTM instead of LSTM as fact encoder in
advance. The BiLSTM generates ht{t=1,··· ,T } by concatenating a forward LSTM
and a backward LSTM.
2.4
Output Layer
The BERT layer and the Sequence layer can capture the global semantic infor-
mation and sequence semantic information of the text. Based on the text and
event type, the SEBERTNets model predicts the beginning and end of the text of
the event body. Finally, it is decoded by the start position and the end position
to obtain the corresponding event body.
2.5
Optimization
Although Adam [4] performed well at the beginning of model training, SGDStochas-
tic gradient descent (SGD) [6] was superior to the Adam method in the later
stages of training. Therefore, we introduce a new method, SWATS [3] , which
first starts training with the Adam method and switches to SGD when appro-
priate. Verification on the real data set of the evaluation game, this method can
achieve better performance than the Adam optimization method.
3
Experiments
3.1
Dataset
CCKS 2019 evaluation task consists of 17,815 training sets, 3,500 validation
sets (preliminary test datasets) and 135,519 test sets (rematch test datasets).
Each dataset includes a given text description, event type, and event entity
respectively.
3.2
Evaluation
Following [2] , we use F1 score to evaluate the performance of our methods. The
F1 score is computed as follows:
F1 = 2PR
P + R
(8)
Where P means that number of correctly identified event entities divided by the
total number of identified event entities, R represents that number of correctly
identified event entities divided by the total number of annotated event entities.
Title Suppressed Due to Excessive Length
5
3.3
Hyper-parameter Setting
Since all the text descriptions and event types have been employed for char
segmentation and set each text maximum length to 140. We use character-based
Chinese BERT model 2 and character-based Chinese BERT-wwm model 3 to
training SEBERTNets and variant SEBERTNets models. In addition, GRU is
adopted to Sequence layer and the hidden unit is set to 200. The batch size is
set to 32 for SEBERTNets and variant SEBERTNets models.
4
Results
4.1
Main Results
Table 1. F1 score of models on preliminary test datasets.
Number of entities
Top 1
Top 2
Top 3
Top 4
Top 5
BERT
0.871
0.887
0.893
0.901
0.902
SEBERTNets
0.905
0.915
0.918
0.921
0.923
HSEBERTNets
0.914
0.922
0.926
0.931
0.934
In this section, we conduct experiments on preliminary test datasets to
demonstrate the effectiveness of the proposed approach. Table 1
shows the
performance of our methods on preliminary test datasets.
Overall, we can find the SEBERTNets model outperforms BERT model with
a significant margin on preliminary test datasets. SEBERTNets model obtains
2.5% average absolutely considerable improvements on preliminary test datasets,
which demonstrates the effectiveness of SEBERTNets model for event entity
extraction.
Additionally, we propose HSEBERTNets model to recall all the correspond-
ing event entity. Compared to the SEBERTNets model, HSEBERTNets model
achieves better performance, especially, in multiple event entities. The results
demonstrates the effectiveness of HSEBERTNets model for multiple event en-
tity extraction.
4.2
Visualization
In this section, we select several representative cases to give an intuitive illus-
tration of how the BERT layer help to promote the performance of event entity
extraction. As shown in Figure 2, red color means that the correct entities ex-
tracted by SEBERTNets model, and yellow color means that the incorrect entites
2 https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_
H-768_A-12.zip
3 https://drive.google.com/open?id=1RoTQsXp2hkQ1gSRVylRIJfQxJUgkfJMW
6
He et al.
1 
 
""员工协助地方政府空气数据造假 先河环保遭深交所问询住维资金:“漏洞”“黑洞”如何
解"",""财务造假"" 
 
""上海一中院对快鹿集团、东虹桥小贷公司、东虹桥担保公司分别以集资诈骗罪判处罚金人
民币十五亿元、二亿元、二亿元；对黄家骝、韦炎平以集资诈骗罪判处无期徒刑，并处罚
金；对徐琪以集资诈骗罪、非法吸收公众存款罪两罪并罚判处有期徒刑十三年，并处罚
金；对周萌萌等其余 12 名被告人以集资诈骗罪分别判处有期徒刑十五年至九年不等的刑
罚，并处罚金"",""涉嫌非法集资"" 
 
""邦信资产抛售彭州民生村镇银行股权中航三鑫(002163)、中航重机(600765)控股股东将变
更"",""实控人股东变更"" 
Fig. 2. Visualization of BERT layer.
extracted by SEBERTNets model. In these cases, we can find that BERT layer
focus on the entity of the text well. Several entities are not belonging to the
corresponding event type, and BERT layer can be well observed. From this fig-
ure, we observe that the BERT layer can capture key entities relevant to current
event type.
References
1. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidi-
rectional transformers for language understanding. In: Proceedings of the 2019
Conference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-
pers). pp. 4171–4186. Association for Computational Linguistics, Minneapolis, Min-
nesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https://www.aclweb.
org/anthology/N19-1423
2. Hong, Y., Zhang, J., Ma, B., Yao, J., Zhou, G., Zhu, Q.: Using cross-entity inference
to improve event extraction. In: Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies-Volume
1. pp. 1127–1136. Association for Computational Linguistics (2011)
3. Keskar, N.S., Socher, R.: Improving generalization performance by switching from
adam to sgd. arXiv preprint arXiv:1712.07628 (2017)
4. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
5. Liu, S., Chen, Y., Liu, K., Zhao, J., et al.: Exploiting argument information to
improve event detection via supervised attention mechanisms (2017)
6. Robbins, H., Monro, S.: A stochastic approximation method. The annals of mathe-
matical statistics pp. 400–407 (1951)
","Liu et al. explored using argument information for event detection via supervised attention mechanisms, raising questions related to the efficacy of event entity extraction and the retrieval of all corresponding entities when faced with multiple instances within a text. The authors address these challenges by proposing SEBERTNets, which integrates BERT and a sequence layer to enhance semantic extraction, and HSEBERTNets, which employs a multi-channel recall method for comprehensive entity retrieval.nan"
"This study addresses the challenge posed by realistic video distribution shifts on action recognition models. We introduce two novel evaluation methods, one involving dataset splits across different sources and the other using cosine similarity scores between embedded training and test features. These methods enable the evaluation of model robustness to distribution shifts efficiently without prior knowledge of the target distribution or requiring fine-tuning on it. We further propose an adversarial augmentation and curriculum training framework to enhance robustness against distribution shifts. Experiments show that the proposed approach outperforms baselines across three popular action recognition models, demonstrating its effectiveness in handling distribution shifts in real-world deployment scenarios.","Action recognition in videos is a complex task due to factors like viewpoint changes, appearance variations, and intra-class variability. Recent advancements in video action recognition have achieved impressive performance on benchmark datasets, but they often lack robustness when faced with distribution shifts between training and test data. This paper delves into the problem of distribution shifts in video action recognition and presents a novel adversarial augmentation approach to enhance model robustness under such conditions.","The paper proposes a novel evaluation scheme to assess model resilience to distribution shifts and subsequently introduces an adversarial augmentation framework to enhance robustness. To create realistic distribution shifts, new cross-dataset benchmarks are constructed by aligning action classes across diverse datasets. Additionally, a cosine similarity-based approach is presented to measure prediction accuracy without requiring class alignment or target dataset fine-tuning. Adversarial augmentation involves generating maximally informative adversarial examples through gradient ascent on augmentation parameters and optimizing the classification model using an adversarial loss function. Curriculum training is employed to gradually increase the difficulty of adversarial examples presented to the model over time.","The proposed adversarial augmentation and curriculum adversarial training frameworks are evaluated on three state-of-the-art action recognition architectures across the newly constructed cross-dataset benchmarks as well as the cosine similarity-based evaluation. Empirical results demonstrate that the proposed approach consistently outperforms baselines in terms of robustness to realistic distribution shifts. The adversarial augmentation strategies yield significant improvements in accuracy on the target datasets, highlighting their effectiveness in generating challenging and informative samples for training.","The study concludes that adversarial augmentation and curriculum training effectively enhance the robustness of action recognition models to realistic distribution shifts, as shown by the improved performance across multiple architectures and evaluation methods. The proposed approach aims to mitigate the drop in performance experienced when models are applied in more general scenarios with different data distributions. By generating hard adversarial examples and gradually increasing their difficulty, the models learn to extract more discriminative features and become more robust to distribution shifts.",Adversarial Augmentation Training Makes Action Recognition Models More Robust to Realistic Video Distribution Shifts,"Kiyoon Kim, Shreyank N Gowda, Panagiotis Eustratiadis, Antreas Antoniou, Robert B Fisher","Adversarial Augmentation Training Makes
Action Recognition Models More Robust to
Realistic Video Distribution Shifts
Kiyoon Kim, Shreyank N Gowda, Panagiotis Eustratiadis, Antreas Antoniou,
and Robert B Fisher
University of Edinburgh, Edinburgh, UK
Abstract. Despite recent advances in video action recognition achiev-
ing strong performance on existing benchmarks, these models often lack
robustness when faced with natural distribution shifts between training
and test data. We propose two novel evaluation methods to assess model
resilience to such distribution disparity. One method uses two different
datasets collected from different sources and uses one for training and
validation, and the other for testing. More precisely, we created dataset
splits of HMDB-51 or UCF-101 for training, and Kinetics-400 for testing,
using the subset of the classes that are overlapping in both train and test
datasets. The other proposed method extracts the feature mean of each
class from the target evaluation dataset’s training data (i.e. class pro-
totype) and estimates test video prediction as a cosine similarity score
between each sample to the class prototypes of each target class. This
procedure does not alter model weights using the target dataset and
it does not require aligning overlapping classes of two different datasets,
thus is a very efficient method to test the model robustness to distribution
shifts without prior knowledge of the target distribution. We address the
robustness problem by adversarial augmentation training – generating
augmented views of videos that are “hard” for the classification model
by applying gradient ascent on the augmentation parameters – as well
as “curriculum” scheduling the strength of the video augmentations. We
experimentally demonstrate the superior performance of the proposed
adversarial augmentation approach over baselines across three state-of-
the-art action recognition models - TSM, Video Swin Transformer, and
Uniformer. The presented work provides critical insight into model ro-
bustness to distribution shifts and presents effective techniques to en-
hance video action recognition performance in a real-world deployment.
Keywords: Action recognition · Distribution shifts · Adversarial train-
ing · Data augmentation.
1
Introduction
Video action recognition is a vital computer vision task with applications in
surveillance, robotics, and more. Video data exhibits greater diversity than im-
age data, and therefore action recognition architectures are not as robust to
arXiv:2401.11406v1  [cs.CV]  21 Jan 2024
2
K. Kim et al.
Training Data (HMDB-51)
Test Data (Kinetics-400)
Fig. 1: A common scenario with biased training data and testing with real-life
data, with an example class of “drink”. The training data is from the HMDB-
51 dataset whose video samples are usually taken from movies, and the test
data is from the Kinetics-400 dataset which is from YouTube videos. There are
many reasons why the test data looks so different: poor camera quality, wrong
orientation, extreme camera shake, inconsistent frame rate, frame rate conversion
artifacts (interlacing), poor lighting, lack of professional post-processing (e.g.
color grading), different ways of performing the action, poor framing, inconsistent
aspect ratios, editing artifacts, various actions happening at the same time. Thus,
it is common for the performance to drop significantly when the trained model
is applied in more general applications.
distribution shifts [58,43,38]. In addition to image-level effects like viewpoint
and appearance changes, video introduces effects such as camera motion, focus
shifts, and background object movements. Moreover, an action class incorpo-
rates substantial intra-class variation as illustrated in Fig. 1. For example, the
class “playing basketball” may involve dribbling, running, or shooting in different
contexts. Furthermore, depending on the data source, there are biased video pro-
cessing artifacts. For example, videos collected from YouTube have standardized
YouTube processing (VP9 compression), making the dark areas have extremely
low quality. Often, the videos go through a frame rate conversion algorithm,
which can create frame interlacing artifacts. As a result, the slight distribution
shift in video data can dramatically reduce the action recognition performance.
Data augmentation is one potential solution to account for this fragility. It
is a popular method to create synthetic variations of the existing training data
that will enable classification models to generalize better to previously unseen
test data. However, it is not yet clear what kind of augmentation is necessary to
generalize to test data with different distribution shifts. There has been much
work on automatically selecting augmentation policies given training and vali-
dation data [8,24,35,22,34]. However, such approaches optimize augmentation of
the training data, and it is not clear whether the resulting generalization applies
to test data with much distribution shift.
To address the video domain shift problem, we propose an adversarial aug-
mentation scheme that generates “hard” video examples for the action recogni-
Adversarial Augmentation Training for Action Recognition
3
tion networks. The pipeline is simple to implement, and results in a meaningful
improvement in performance on the proposed datasets with realistic distribution
shifts compared to no augmentation and random augmentation baselines. The
benefits are demonstrated using three popular action recognition architectures.
The training and validation datasets are subsets of HMDB-51 or UCF-101, and
the test data are a subset from equivalent Kinetics-400 classes, to realistically
evaluate distribution shifts over the same action classes.
This approach and evaluation requires multiple datasets with common aligned
action classes. The paper also presents another simple method to evaluate ro-
bustness using a target dataset with different classes using cosine similarity of
features as logits. The method requires no training (i.e. fine-tuning) on the tar-
get dataset, and thus it correctly measures the transferability of the trained
classifier on the target dataset.
To summarize our contributions are:
1. Experiments reveal substantial performance degradation on our cross-dataset
benchmarks, quantitatively demonstrating the challenge posed by real-world
distribution shift.
2. We propose two novel evaluation protocols to assess model resilience to dis-
tribution disparity using naturally sourced datasets, as opposed to solely
artificially corrupted data:
2a) We construct new cross-dataset benchmarks by identifying overlapping
classes between HMDB-51, UCF-101, and Kinetics-400. Models are trained
on either HMDB or UCF, and evaluated on Kinetics data.
2b) We further introduce a similarity-based evaluation approach that esti-
mates predictions using cosine similarity between embedded training and
test features, without requiring class alignment.
3. Through extensive experiments across multiple state-of-the-art architectures,
we empirically demonstrate that the proposed adversarial augmentation and
curriculum adversarial training frameworks enhance robustness to realistic
distribution shifts between the training and test datasets.
4. We will publicly release the constructed subsets of HMDB-51, UCF-101, and
Kinetics-400 to enable further research on this important problem.
2
Related Work
Action recognition.
Action recognition is the task of categorizing video se-
quences into predefined action classes. Architectures based on 3D convolutional
neural networks were previously dominant for spatiotemporal feature learning.
These include approaches such as inflating 2D models [7], incorporating rela-
tional reasoning with non-local operations [55], and dual-stream designs [47,14].
More recently, transformer networks have become prominent, demonstrating
strong performance [12,39,33] despite their exponential computational complex-
ity [29]. For efficient video recognition, 2D backbone models remain popular,
using techniques like temporal feature aggregation [53], relational modeling [63],
temporal shift modules [36], frame selection [31,20,57], channel-wise convolu-
4
K. Kim et al.
tions (i.e. height-width, height-time, width-time) [56] or analyzing short-term
and long-term temporal difference [52].
Data augmentation.
[45] summarizes image data augmentation techniques
for deep learning. AutoAugment [8] is an augmentation policy search algorithm
that finds the best augmentation on a target dataset, based on the highest val-
idation accuracy. Due to AutoAugment’s expensive policy search, Population-
Based Augmentation [24], FastAutoAugment [35], and FasterAutoAugment [22]
proposed more efficient searching algorithms, by learning a schedule policy over
a fixed-policy, using density matching, and using differential augmentation with
a generative adversarial network (GAN) [18] architecture that involves a pol-
icy generator and a discriminator, respectively. Differentiable Automatic Data
Augmentation [34] proposed a data augmentation policy searching algorithm us-
ing Relaxed Bernoulli distribution [26] which is differentiable, similar to Faster-
Autoaugment, and further introduced an unbiased gradient estimator that en-
ables joint optimization of the augmentation policies and network parameters,
instead of using a GAN. RandAugment [9] showed that simple random aug-
mentations with randomly sampled transformations achieve similar performance
more efficiently.
However, the policies in most works are optimized on the training set, and
we focus on the scenario where test data can have severe distribution disparities
which are unknown during the training time.
Adversarial training.
Adversarial training (AT) is framed as a min-max
problem whereby the trained model uses observed training samples to minimize
its prediction error, while an adversary attempts to generate training samples
that maximize it. It is well-established that AT is the most effective way to
achieve adversarial robustness [2]. It has further been shown to yield other types
of robustness, e.g. against natural corruptions [10], domain shift [44,64,30], and
others. Note that even though the classical definition of AT uses adversarial
input noise [19,6], more adversarial image augmentations have been studied,
e.g. rotations [62,54], contrast, jitter, etc. [3]. AT should be approached with
care, as generating adversarial training examples that are too challenging for
the trained model may actually harm downstream performance [5].
In this paper, we employ two measures to control the trade-off between aug-
mentation strength and performance: (i) We create maximally informative adver-
sarial examples (confusing to the model, but near the classification boundaries)
via maximum-entropy regularization, as per the work of [27,59,11]. (ii) We train
with curriculum AT as per the work of [5,60], which means training with harder
adversarial examples over time.
Domain adaptation.
Domain adaptation is a transfer learning task where
the source and target datasets have a significant distribution shift while sharing
the same task. [13] explains types of domain adaptation tasks and approaches.
There are discrepancy-based techniques that learn transferable features from
a source domain to a target domain [40,61], reconstruction-based methods that
utilize autoencoders, which aim to extract useful features for the target do-
main [17,16], and adversarial domain adaptation approaches involving a source
Adversarial Augmentation Training for Action Recognition
5
/ target discriminator that distinguishes where the data come from and a feature
extractor that aims to confuse the discriminator by trying to produce generic fea-
tures regardless of the domain [15,42,51,1,4,46,50,25]. More recently, analyzing
frequency components of deep feature maps using attention to filter domain-
general components [37] is proposed.
Domain adaptation for video action recognition was first proposed by using
a feature alignment approach on online test videos [38]. This work was evaluated
using computationally simulated corrupted videos, while we propose to use real
examples that involve more diverse types of discrepancy between the domains.
It is important to note that most domain adaptation techniques require exam-
ples from the target dataset to be present, while our work focuses on evaluating
using a completely unknown dataset.
Corruption robustness analysis.
[23] provided benchmarks for measuring a
neural network’s robustness to corruptions and perturbations, by evaluating with
15 algorithmically-generated corruptions (e.g. noise, blur, pixelate, compression
artifacts). [58] extended this to video classification tasks and video corruptions
(e.g. video compression artifacts, frame rate conversion, bit error, packet loss).
[43] reported a large-scale robustness analysis of deep action recognition models
again using pre-defined perturbations.
These approaches were evaluated using simulated data, while we propose to
use real data for testing. Evaluating robustness with augmented data prohibits
the same augmentations to be used for training. This paper focuses on a more
realistic scenario where a known set of data augmentation strategies is used for
training and evaluating is done with unprocessed real data.
3
Problem and Methodology
Action recognition predicts an action category label given a video sequence.
This paper explores how well different variations of action recognition models,
training, and loss functions generalize by evaluating on a different data domain.
The main difference with transfer learning is that our approach does not tune
model parameters using the target evaluation dataset, whereas transfer learning
usually involves fine-tuning the model with the target dataset’s training set.
To improve generalizability, the training data is augmented. Hard-to-classify
adversarial examples are generated by applying gradient ascent on the augmen-
tation parameters which are fully differentiable. We then train the classifier using
AT loss, calculated using both clean and adversarial examples.
3.1
Adversarial Augmentation Training
Adversarial augmentation training uses a two stage training loop. See Fig. 2.
Stage 1: Generate adversarial examples.
“Hard” adversarial examples
are found by tuning the augmentation parameters using gradient ascent.
Let gθ(x) be an augmentation model with fully differentiable parameters
θ, and fϕ(x) be a video classification model with parameters ϕ, that outputs
6
K. Kim et al.
predictions
Frozen
predictions
Classiﬁcation
model
Diﬀerential
augmentation
model
Stage 1: Freeze the classiﬁcation model and tune the augmentation model
Classiﬁcation
model
Stage 2: Tune the classiﬁcation model with the adversarial loss
Frozen
Diﬀerential
augmentation
model
Fig. 2: The proposed adversarial augmentation training has two separate stages.
Firstly, the classification model is frozen while the differential augmentation
model is trained using the negative cross-entropy loss. This is equivalent to
performing gradient ascent to maximize normal cross-entropy loss. As a result,
the augmentation model will generate hard augmentations for the classification
model. The second stage trains the classification model using both clean and
adversarial examples. The maximum entropy regularization loss is integrated by
subtracting the entropy of the adversarial examples, encouraging the predictions
to be evenly balanced.
class predictions given an input video x. The goal of stage 1 is to find the
augmentation parameters θ′ that maximize categorical cross-entropy loss. Note
that by maximizing the loss, we aim to find the augmentation strategy that is
challenging for the classifier, and in Fig. 2, this is described as minimizing the
negative cross-entropy loss.
At each training step, the augmentation parameters θ are randomly initial-
ized. Gradient ascent is then done only on θ, freezing the classification parameters
ϕ to learn adversarial augmentations, with the cross-entropy loss LCE(fϕ(gθ(xi)), yi).
This optimizes augmentation parameters θ′ for generating adversarial examples.
Stage 2: Optimize the classification model.
Next, the classification pa-
rameters ϕ are optimized while freezing the augmentation parameters θ. For
simplicity, x⋆
i = gθ′(xi) denotes the generated adversarial example of xi. The
vanilla AT loss is defined as:
LAT(fϕ(xi), fϕ(x⋆
i ), yi) =αLCE(fϕ(xi), yi)
+ (1 − α)LCE(fϕ(x⋆
i), yi)
(1)
which is a weighted average of the cross-entropy loss using the clean sample and
the augmented sample.
Max-Entropy Regularization.
The cross-entropy loss encourages predic-
tions to be over-confident by pushing the examples further from the classifica-
tion boundaries. However, adversarial examples are supposed to be confusing.
We regularize the cross-entropy-based adversarial loss in Eq. (1) by maximiz-
ing the entropy, encouraging the overall predictions to be evenly balanced for
Adversarial Augmentation Training for Action Recognition
7
adversarial examples.
LAT-ME(fϕ(xi), fϕ(x⋆
i ), yi) =αLCE(fϕ(xi), yi)
+ (1 − α)LCE(fϕ(x⋆
i), yi)
− γLE(fϕ(x⋆
i))
(2)
where entropy loss LE is defined as
LE(fϕ(x⋆
i )) = − 1
C
C
X
c=1
f (c)
ϕ (x⋆
i ) log(f (c)
ϕ (x⋆
i ))
(3)
C is the number of classes, and f (c)
ϕ (x⋆
i ) is the prediction score of class c for the
adversarial example x⋆
i .
Curriculum Adversarial Training.
Applying curriculum training by start-
ing from training with “easy” samples and gradually generating “harder” sam-
ples makes the model more robust [5]. Here, the classification models are trained
initially from clean data without augmentation, and gradually harder adversarial
examples are added by scheduling the learning rate of the augmentation model.
3.2
Cross-Dataset Evaluation
We train on one dataset and test on another dataset, where there are distribution
shifts between the train and the test data. We propose two different evaluation
approaches.
Matched Class Evaluation (Expt. 1).
The first approach creates two
datasets that share the same classes, but have a significant disparity in the train
and test data distribution. Classes that are common to the two initial datasets
are identified following the procedure described in TruZe [21]. We describe the
procedure and curated datasets in Supplementary Material. This is the most
realistic method to evaluate on a distribution shift, but it requires some manual
procedures as well as finding datasets that share largely similar classes.
Cosine Similarity Evaluation (Expt. 2).
The second method applies the
feature extractor trained on the source dataset to the videos in the target dataset,
which are split into a training subset and a test subset. This is a simpler method
that does not require manual class matching. A more descriptive procedure can
be found in Supplementary Material.
Formally, the class prototype ck ∈ RM for each class k is the M-dimensional
mean vector of the embedded features belonging to that class in the training
subset. Let Sk be the set of videos in the training subset of the target dataset
from class k. ck is computed by the following, where yi = k for all (xi, yi) ∈ Sk:
ck =
1
|Sk|
X
(xi,yi)∈Sk
hω(xi)
(4)
hω() is the embedded feature extraction function computed by training on the
source dataset.
8
K. Kim et al.
For a given test video from the test subset of the target dataset xi ∈ Stest
k
,
the probability of a given class label k is estimated using the cosine similarity of
the embedding to the target dataset’s class prototype ck, followed by softmax.
Given the cosine similarity function d(x, c) =
x·c
∥x∥∥c∥ we get:
P(y = k|xi ∈ Stest
k
) =
exp(−d(hω(xi), ck))
P
k′ exp(−d(hω(xi), ck′))
(5)
where y denotes the class label. If the largest estimated P(·) is for the same class
as the ground truth, then this is a successful classification. Accuracy is computed
over all samples in the test subset of the target dataset.
The parameter ω is not tuned during this operation. That is, the target
dataset does not contribute to fine-tuning the model. The motivation of this
approach is to measure the transferability of the model from a source dataset
to a target dataset without actually tuning the model parameters, showing the
robustness of the model to sample distribution shift.
4
Experiments
HMDB/Kinetics and UCF/Kinetics Datasets (Expt. 1).
HMDB-51 [32]
is a popular human action recognition dataset that is composed of around 7,000
video clips divided into 51 categories, collected from mainly movies as well as
YouTube. The UCF-101 [49] dataset consists of 13,320 videos in 101 action
classes collected from YouTube. Kinetics-400 [28] is a large-scale action dataset
in which each video clip is around 10 seconds long, and there are over 300,000
videos in 400 classes.
Training datasets are created from subsets of HMDB-51 or UCF-101 and the
subset of the Kinetics-400 test set is used for testing. The subsets share the same
classes between the training and test sets. The motivation for this approach to
creating the datasets is to simulate a real-world environment where the test data
comes from many unknown sources, with many variations in actions, capture
conditions, aspect ratio, and so on. The Kinetics-400 dataset has many more
samples in the fine-grained classes, so it was used for testing.
TruZe [21] is used to identify shared classes from the HMDB-51 and Kinetics-
400, and UCF-101 and Kinetics-400 datasets, based on the visual and semantic
similarity. More details on this procedure can be found in Supplementary Mate-
rial. The final datasets are named HMDB-28/Kinetics-28 and UCF-65/Kinetics-
65, where the training sets are subsets of the HMDB and UCF data, respectively,
and test subsets come from Kinetics. The 28 and 65 refer to the number of shared
classes. The three official published splits of HMDB-51 and UCF-101 are used,
but only the shared classes are selected. The results in Table 1 are the average
performance over the three splits.
HMDB ↔ UCF Evaluation (Expt. 2).
For the experiments using the
cosine similarity function, the HMDB-28 trained models are tested on UCF-
101, and the UCF-65 trained models are tested on HMDB-51, using the cosine
Adversarial Augmentation Training for Action Recognition
9
similarity measure with class prototypes as predictions. The trained feature ex-
tractor is used to create prototype vectors from one part of the target dataset,
and evaluation is based on the classification accuracy of the other part of the
dataset. This assesses the quality of the feature extractor on another dataset
with distribution shifts. The results in Table 1 are the average performance over
the nine splits, three runs from the source dataset and each run evaluates with
three splits from the target dataset.
See Supplementary Material for the resulting matching datasets in Table 2,
and a summary table of all experimental datasets (Expt. 1 and Expt. 2), in
Table 4.
Augmentation Methods.
Results are compared for four different training
approaches: training without augmentation, with random augmentation, with
adversarial augmentation, and with curriculum AT [5] as described in Section 3.1.
Experiments used the popular efficient 2D TSM model [36] with a ResNet50
backbone, Video Swin Transformer [39] Tiny, and Uniformer-S [33] model. Ima-
geNet pre-trained models were used instead of Kinetics pre-trained, so that the
models never get to see the Kinetics data distribution.
Augmentation used translation to a maximum of 28 pixels, 10◦ rotation,
shear transform of 0.1, and scale from 0.9 to 1.5. For curriculum training, no
augmentation was used for 20 epochs, then AT with a zero learning rate of the
augmentation model was used for 20 more epochs. Then, AT with a triangular
learning rate scheduling from 0.1 to 1.0 on the augmentation model was used
for the rest of the training, except for the Uniformer model. For Uniformer, the
above was done for only 20 epochs, and then trained with random augmentation
for 20 more epochs and fine-tuning with no augmentation for the rest to mitigate
under-fitting issues.
See Supplementary Material for implementation details.
5
Results
5.1
Shared Class Experiments 1a, 1b
When the adversarial training approaches presented in Section 3.1 are applied,
target dataset performance improves. Table 1 summarizes the main cross-dataset
evaluation results for the four augmentation strategies presented in Section 3.1.
The different adversarial augmentation strategies gave improved accuracy for
the target datasets (see Kinetics-28 and 65 results columns).
Also, unlike what is reported in [43], the convolutional architecture performed
better with the distribution shift compared to transformer models in most of the
cases except for the Swin Transformer trained on UCF-65 and tested on HMDB-
51. We hypothesize that this is because the Kinetics test dataset shows natural
and realistic distribution shifts. In addition, we did not use the Kinetics pre-
trained models, and the transformer architectures require large-scale data to
reach the maximum potential.
In all cases, the adversarial augmentation or curriculum methods outperform
all baselines, given a fixed network architecture, for all training and test datasets.
10
K. Kim et al.
Model
Train Dataset
Augmentation
Test Accuracy
Kinetics-65
HMDB-51
Matched Expt 1a Cosine Expt 2a
TSM
UCF-65 [77.50]
None
39.13 ± 0.56
37.61 ± 1.88
Random
40.90 ± 0.32
38.19 ± 1.39
Adversarial
42.42 ± 0.63
38.99 ± 1.26
Curriculum
42.51 ± 0.62
39.14 ± 1.21
Swin
UCF-65 [81.15]
None
37.08 ± 1.43
38.91 ± 1.63
Random
40.80 ± 1.90
40.61 ± 1.40
Adversarial
42.27 ± 0.24
41.48 ± 1.58
Curriculum
41.20 ± 0.72
41.58 ± 1.63
Uniformer UCF-65 [52.05]
None
18.78 ± 0.22
21.39 ± 1.32
Random
22.42 ± 0.98
24.92 ± 1.66
Adversarial
22.95 ± 0.68
26.16 ± 2.10
Curriculum
23.61 ± 0.27
24.93 ± 1.60
Model
Train Dataset
Augmentation
Test Accuracy
Kinetics-28
UCF-101
Matched Expt 1b Cosine Expt 2b
TSM
HMDB-28 [55.45]
None
29.75 ± 1.08
60.51 ± 0.79
Random
30.13 ± 0.42
61.21 ± 0.57
Adversarial
32.44 ± 0.48
62.60 ± 0.59
Curriculum
32.82 ± 1.41
63.18 ± 0.71
Swin
HMDB-28 [54.67]
None
25.26 ± 0.80
59.88 ± 0.55
Random
26.63 ± 0.97
60.99 ± 1.30
Adversarial
27.31 ± 0.57
62.57 ± 0.72
Curriculum
27.60 ± 0.62
62.95 ± 0.82
Uniformer HMDB-28 [29.43]
None
14.53 ± 0.51
28.23 ± 0.94
Random
14.33 ± 1.03
28.67 ± 1.36
Adversarial
15.13 ± 0.79
29.88 ± 2.62
Curriculum
15.25 ± 0.75
30.83 ± 1.04
Table 1: Results from three models (TSM ResNet50, Video Swin Transformer
Tiny, and Uniformer-S), two training datasets (UCF-65 and HMDB-28), four
augmentation strategies (no augmentation, random, adversarial, and curricu-
lum), and two test datasets (Kinetics, and HMDB/UCF). In all cases, adversar-
ial augmentation or curriculum adversarial augmentation training outperformed
all baselines. The columns labeled Test Accuracy show the performance on the
target test set. The values in brackets in the Train Dataset columns show the
“no augmentation” accuracy on the test set of the same dataset to demonstrate
the performance drop when evaluating to the Kinetics dataset.
Although the “random augmentation” and “adversarial augmentation” allow an
identical range of transforms, generating adversarial examples through gradient
ascent produces “harder than random” augmentation which improves the overall
performance. Furthermore, adding the simple curriculum mostly improved over
the adversarial benchmark.
See Supplementary Material for confusion matrices that show per-class per-
formance drop with distribution shifts and improvements using the proposed
adversarial augmentation.
5.2
Cosine Similarity Evaluation 2a, 2b
The cosine-similarity-based accuracy results on HMDB-51 and UCF-101 are
shown in the Cosine Expt columns of Table 1. The results follow a very similar
trend to the “realistic” Kinetics Expt 1. The advantage of using this accuracy
measure as compared to testing on Kinetics with overlapping classes is that it
requires no thorough analysis of the source and target datasets to find overlap-
ping classes, making it simple to set up the cross-dataset experiments even using
new datasets.
6
Conclusion
This paper addressed the problem of model generalization to realistic test dis-
tribution shifts. Two new datasets that are comprised of three existing datasets
were created that shared the same subset of label classes. Although the same
classes were used, the variety of videos in the original dataset sources meant
that there was a huge distribution shift from the source to the target datasets.
When using the target datasets, action recognition performance dropped signifi-
cantly. This led to trying adversarial augmentation, with and without curriculum
Adversarial Augmentation Training for Action Recognition
11
scheduling, as an approach to generating hard adversarially augmented videos.
This approach gave a small but meaningful improvement in performance, even
with the large distribution shift in the test data. The second cross-dataset evalu-
ation approach, using the cosine similarity as logits, also showed a similar trend
as the matching dataset experiments, providing a simpler alternative method
without having to curate datasets with matching classes.
12
K. Kim et al.
Supplementary Material
A
TruZe Class Matching Procedure
For Expt. 1a and 1b, we follow TruZe [21] to identify overlapping classes in two
different datasets. To choose the common classes, visual features are extracted
on two existing datasets using the I3D [7] model pre-trained on Kinetics-400 [28]
and semantic cues are extracted using the sen2vec [41] model pre-trained on
Wikipedia. The visual and semantic similarity features are combined and then
used in the TruZe [21] procedure (i.e. include exact matches, matches that can be
either superset or subset, and matches that predict the same visual and semantic
matches) to obtain a set of extremely similar classes from the two source action
recognition datasets. The matched classes from the two datasets are verified
manually and a subset is selected that has a substantial overlap in visual and
semantic cues. One dataset is used for training and validation, and the other
dataset is used for testing.
In TruZe, normally, classes from UCF or HMDB that have overlapping con-
text with the corresponding Kinetics class are removed, so that using the Ki-
netics pre-trained models would not bias the zero-shot settings. However, in our
robustness problem, the train and test datasets are created with the opposite
goal, where only overlapping classes are selected. For instance, the class “climb”
in HMDB-51 is treated as the same class as “rock climbing”, “ice climbing”,
“climbing a rope”, and “climbing tree” in Kinetics-400. Examples of the result-
ing splits for the matched class experiments (Expt 1a and 1b) can be found in
Table 2.
B
Cosine Similarity Evaluation Procedure
In Expt 2a and 2b, we use cosine similarity between class prototypes and features
on the evaluating dataset to estimate predictions. We then report accuracy with
the simulated predictions to compare the performance of augmentation strategies
given distribution shifts.
We first train the classification model using the source dataset. Since the
number and types of classes of the training (source) and evaluation (target)
datasets are different, we detach the last classification layer of the source dataset
classification model to use it as a feature extractor. We then calculate class
prototypes of all the classes in the target dataset by simply averaging the features
of each class in the training set, inspired by [48]. The prediction score of a
sample from the target dataset’s test set is estimated using the cosine similarity
of their feature vector and the class prototypes, followed by softmax. In other
words, the cosine similarity is used as logits, which are formed by producing high
activations on classes that are closely aligned to the training set of the target
dataset. Using these estimated prediction probabilities, we report accuracy. We
will show that adversarial augmentation of the source dataset also produces
improved classification of the target dataset, by producing a more robust feature
Adversarial Augmentation Training for Action Recognition
13
HMDB-51 Classes Kinetics-400 Classes
brush hair
curling hair, dying hair, brushing hair
cartwheel
cartwheeling, somersaulting
catch, throw
shooting goal, juggling, catching or throwing frisbee, catching or throwing baseball, catching or throwing softball, throwing axe, throwing ball, throwing discus
clap
clapping, applauding
climb
rock climbing, ice climbing, climbing tree, climbing a rope
dive
diving cliff, bungee jumping
dribble
dribbling basketball
drink
drinking beer, drinking shots, drinking
eat
eating burger, eating cake, eating carrots, eating chips, eating doughnuts, eating hotdog, eating ice cream, eating spaghetti, eating watermelon
golf
golf driving, golf chipping, golf putting
· · ·
· · ·
UCF-101 Classes
Kinetics-400 Classes
Basketball
shooting basketball, dribbling basketball, playing basketball
BasketballDunk
dunking basketball
BodyWeightSquats lunge, squat, dead lifting
BreastStroke
swimming breast stroke, swimming back stroke
CleanAndJerk
clean and jerk, snatch weight lifting
CliffDiving
diving cliff, springboard diving
Haircut
shaving head, braiding hair, getting a haircut
HorseRiding
riding or walking with horse, riding mule
PlayingPiano
playing piano, playing organ
Skiing
skiing (not slalom or crosscountry), skiing crosscountry, skiing slalom
· · ·
· · ·
Table 2: The HMDB-28/Kinetics-28 (above) and UCF-65/Kinetics-65 (below)
datasets which are subsets from the original HMDB-51 UCF-101, Kinetics-400
datasets. Visually and semantically similar classes were combined which allows
testing with real-life data from YouTube that has a significant distribution shift
from the training data. The HMDB or UCF data are used for training and
validation, and the Kinetics data are used for testing.
Dataset Type # Classes Split Usage
Source
N
train Training
test
Validation
Target
K
train Obtaining K class prototypes
test
Testing with cosine similarity as logits
Table 3: Purpose of all dataset splits for the cosine similarity evaluation method
(Expt. 2). Given the four splits of the data, the source dataset is used to train
(with or without AT) and validate the model. The target dataset is used to
evaluate the model without further fine-tuning.
extractor for use with this similarity measure. Note that the source dataset is
never used for evaluation, and only be used for training the model, and the target
dataset’s training set does not contribute to fine-tuning the model. The role of
all four splits in two different datasets can be found in Table 3.
C
Datasets
Table 4 summarizes all datasets used for the experiments (Expt. 1 and Expt. 2).
D
Implementation Details.
Videos were resized to 224 × 224 and sampled to 8 frames sparsely similar to
[53]. The classifier models were trained for 200 epochs using an SGD optimizer
14
K. Kim et al.
Expt Name
Original Dataset Usage
1a
UCF-65
UCF-101
Training, validation
Kinetics-65 Kinetics-400
Testing UCF-65 trained models
1b
HMDB-28 HMDB-51
Training, validation
Kinetics-28 Kinetics-400
Testing HMDB-28 trained models
2a
UCF-65
UCF-101
Training, validation
HMDB-51 (original)
Testing UCF-65 trained models w/ cosine similarity
2b
HMDB-28 HMDB-51
Training, validation
UCF-101
(original)
Testing HMDB-28 trained models w/ cosine similarity
Table 4: List of datasets used for our experiments. The HMDB-51, UCF-101, and
Kinetics-400 are the original datasets. The UCF-65, Kinetics-65, HMDB-28, and
Kinetics-28 are the proposed subsets.
original
random
augmentation
adversarial
augmentation
eat
situp
pullup
smoke
Fig. 3: Examples of the vanilla and proposed augmentation examples in the
HMDB-51 dataset while training the Video Swin Transformer model.
with an initial learning rate of 0.0001, decaying the learning rate using cosine
annealing scheduling. For adversarial training (AT), the augmentation model
used a learning rate of 0.1. For adversarial plus curriculum training, the LAT-ME
loss was used with α = 0.5 and γ = 0.5. Two NVIDIA RTX 3090 GPUs were
used with batch size 16 to train the TSM models, an NVIDIA A100 GPU with
batch size 16 and 32 for training the Uniformer and Swin Transformer models,
respectively.
E
Adversarial Augmentation Examples
Some examples of adversarial augmentations in comparison to random augmen-
tations are depicted in Fig. 3. One might think that the most extreme and unre-
alistic augmentations will be challenging to the classifier. However, adversarial
augmentation can sometimes render more realistic yet challenging examples.
Adversarial Augmentation Training for Action Recognition
15
situp
shoot_bow
drink
hug
clap
catch
ride_bike
brush_hair
shake_hands
laugh
cartwheel
eat
golf
kiss
climb
kick_ball
somersault
push
punch
pullup
dribble
pushup
shoot_ball
sword
smoke
ride_horse
dive
swing_baseball
Predicted
situp
shoot_bow
drink
hug
clap
catch
ride_bike
brush_hair
shake_hands
laugh
cartwheel
eat
golf
kiss
climb
kick_ball
somersault
push
punch
pullup
dribble
pushup
shoot_ball
sword
smoke
ride_horse
dive
swing_baseball
Target
0.75
0.50
0.25
0.00
0.25
0.50
0.75
(a) HMDB-28
SoccerPenalty
WritingOnBoard
TrampolineJumping
BasketballDunk
CricketBowling
FrisbeeCatch
HeadMassage
Surfing
Basketball
HulaHoop
Swing
RockClimbingIndoor
WalkingWithDog
VolleyballSpiking
PlayingFlute
Diving
Archery
CliffDiving
PlayingPiano
ApplyEyeMakeup
HammerThrow
Typing
JugglingBalls
Skijet
Haircut
JavelinThrow
GolfSwing
Lunges
Predicted
SoccerPenalty
WritingOnBoard
TrampolineJumping
BasketballDunk
CricketBowling
FrisbeeCatch
HeadMassage
Surfing
Basketball
HulaHoop
Swing
RockClimbingIndoor
WalkingWithDog
VolleyballSpiking
PlayingFlute
Diving
Archery
CliffDiving
PlayingPiano
ApplyEyeMakeup
HammerThrow
Typing
JugglingBalls
Skijet
Haircut
JavelinThrow
GolfSwing
Lunges
Target
0.75
0.50
0.25
0.00
0.25
0.50
0.75
(b) UCF-65 (top 28 classes with the highest accuracy drop)
Fig. 4: Confusion matrix difference between evaluating on the same dataset
(a:HMDB at top or b:UCF at bottom) and a different one (Kinetics), given
a TSM model with no augmentation strategy. The negative (blue) values on
the diagonal line indicate the class accuracy drop when evaluated on Kinetics.
Almost every class has a drastic drop. Overall, there is a 25.7% and 38.37%
accuracy drop for the HMDB and UCF training datasets, respectively.
F
Performance Drop with Distribution Shifts
Two confusion matrices are created collating the performance when training on
HMDB-28 and testing on either HMDB-28 or Kinetics-28 (test sets) similar to
Expt 1a and 1b. The former is subtracted from the latter to record how much
16
K. Kim et al.
difference there is in the performance, which is shown in Fig. 4(a). (The figure
also shows the same results when using UCF-65 and Kinetics-65.) In this figure,
no augmentation strategy is used. The figure shows that there is a dramatic
performance drop when the models are tested on a different dataset (Kinetics)
to training (HMDB or UCF). Some categories such as “situp”, “shoot bow”, and
“drink” are damaged more severely. As seen in Fig. 1, HMDB/UCF videos have
objects and actors that are clearly visible and stable in the frame, while Kinetics
videos tend to have lots of camera motion with different sizes of the objects.
The overall accuracy drop from UCF-65 to Kinetics-65 is even more significant.
These results show that the raw trained model is not robust to distribution shifts
between datasets, which is not ideal for deployment in real applications.
G
Improvements on Classes with Larger Distribution
Shifts
To demonstrate the improvement in transfer performance by the use of the
curriculum adversarial strategy, we computed the difference of the confusion
matrices for None and Curriculum augmentation in Expt. 1a and 1b. Fig. 5
shows the results, where the reddish boxes on the diagonal indicate improved
performance. It is clear that the proposed adversarial augmentation makes the
model more robust on the classes with the larger distribution shifts, as presented
in Fig. 4. This supports our claim that the proposed adversarial augmentation
makes the model more robust in classes with a huge distribution shift.
References
1. Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M.: Domain-
adversarial neural networks. arXiv preprint arXiv:1412.4446 (2014)
2. Bai, T., Luo, J., Zhao, J., Wen, B., Wang, Q.: Recent advances in adversarial
training for adversarial robustness. In: International Joint Conference on Artificial
Intelligence (IJCAI) (2021)
3. Blaas, A., Suau, X., Ramapuram, J., Apostoloff, N., Zappella, L.: Challenges of ad-
versarial image augmentations. In: I (Still) Can’t Believe It’s Not Better! Workshop
at NeurIPS 2021 (2021)
4. Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., Krishnan, D.: Unsupervised
pixel-level domain adaptation with generative adversarial networks. In: Proceed-
ings of the IEEE conference on computer vision and pattern recognition. pp. 3722–
3731 (2017)
5. Cai, Q.Z., Du, M., Liu, C., Song, D.: Curriculum adversarial training. arXiv
preprint arXiv:1805.04807 (2018)
6. Carlini, N., Wagner, D.A.: Towards evaluating the robustness of neural networks.
In: Symposium on Security and Privacy (2017)
7. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the
kinetics dataset. In: proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. pp. 6299–6308 (2017)
Adversarial Augmentation Training for Action Recognition
17
situp
shoot_bow
drink
hug
clap
catch
ride_bike
brush_hair
shake_hands
laugh
cartwheel
eat
golf
kiss
climb
kick_ball
somersault
push
punch
pullup
dribble
pushup
shoot_ball
sword
smoke
ride_horse
dive
swing_baseball
Predicted
situp
shoot_bow
drink
hug
clap
catch
ride_bike
brush_hair
shake_hands
laugh
cartwheel
eat
golf
kiss
climb
kick_ball
somersault
push
punch
pullup
dribble
pushup
shoot_ball
sword
smoke
ride_horse
dive
swing_baseball
Target
0.20
0.15
0.10
0.05
0.00
0.05
0.10
0.15
0.20
(a) HMDB-28
SoccerPenalty
WritingOnBoard
TrampolineJumping
BasketballDunk
CricketBowling
FrisbeeCatch
HeadMassage
Surfing
Basketball
HulaHoop
Swing
RockClimbingIndoor
WalkingWithDog
VolleyballSpiking
PlayingFlute
Diving
Archery
CliffDiving
PlayingPiano
ApplyEyeMakeup
HammerThrow
Typing
JugglingBalls
Skijet
Haircut
JavelinThrow
GolfSwing
Lunges
Predicted
SoccerPenalty
WritingOnBoard
TrampolineJumping
BasketballDunk
CricketBowling
FrisbeeCatch
HeadMassage
Surfing
Basketball
HulaHoop
Swing
RockClimbingIndoor
WalkingWithDog
VolleyballSpiking
PlayingFlute
Diving
Archery
CliffDiving
PlayingPiano
ApplyEyeMakeup
HammerThrow
Typing
JugglingBalls
Skijet
Haircut
JavelinThrow
GolfSwing
Lunges
Target
0.20
0.15
0.10
0.05
0.00
0.05
0.10
0.15
0.20
(b) UCF-65 (top 28 classes with the highest accuracy drop)
Fig. 5: Confusion matrix difference between no augmentation strategy and the
proposed curriculum adversarial augmentation training with the TSM model and
evaluation on Kinetics. The positive (red) values on the diagonal line indicate
the added class-wise accuracy by using the proposed approach. The classes are
sorted by the drop in performance using the proposed cross-dataset evaluation,
as seen in Fig. 4.
8. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning
augmentation strategies from data. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) (June 2019)
9. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated
data augmentation with a reduced search space. In: Proceedings of the IEEE/CVF
18
K. Kim et al.
conference on computer vision and pattern recognition workshops. pp. 702–703
(2020)
10. Dong, J., Moosavi-Dezfooli, S.M., Lai, J., Xie, X.: The enemy of my enemy is my
friend: Exploring inverse adversaries for improving adversarial training. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion (CVPR). pp. 24678–24687 (June 2023)
11. Eustratiadis, P., Gouk, H., Li, D., Hospedales, T.M.: Weight-covariance alignment
for adversarially robust neural networks. In: International Conference on Machine
Learning (ICML) (2021)
12. Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., Feichtenhofer, C.:
Multiscale vision transformers. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 6824–6835 (2021)
13. Farahani, A., Voghoei, S., Rasheed, K., Arabnia, H.R.: A brief review of domain
adaptation. Advances in data science and information engineering: proceedings
from ICDATA 2020 and IKE 2020 pp. 877–894 (2021)
14. Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recog-
nition. In: Proceedings of the IEEE/CVF international conference on computer
vision. pp. 6202–6211 (2019)
15. Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation.
In: International conference on machine learning. pp. 1180–1189. PMLR (2015)
16. Ghifary, M., Kleijn, W.B., Zhang, M., Balduzzi, D., Li, W.: Deep reconstruction-
classification networks for unsupervised domain adaptation. In: Computer Vision–
ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October
11–14, 2016, Proceedings, Part IV 14. pp. 597–613. Springer (2016)
17. Glorot, X., Bordes, A., Bengio, Y.: Domain adaptation for large-scale sentiment
classification: A deep learning approach. In: Proceedings of the 28th international
conference on machine learning (ICML-11). pp. 513–520 (2011)
18. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., Bengio, Y.: Generative adversarial networks. Communications of the
ACM 63(11), 139–144 (2020)
19. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial
examples. In: International Conference on Learning Representations (ICLR) (2015)
20. Gowda, S.N., Rohrbach, M., Sevilla-Lara, L.: Smart frame selection for action
recognition. In: Proceedings of the AAAI Conference on Artificial Intelligence.
vol. 35, pp. 1451–1459 (2021)
21. Gowda, S.N., Sevilla-Lara, L., Kim, K., Keller, F., Rohrbach, M.: A new split for
evaluating true zero-shot action recognition. In: DAGM German Conference on
Pattern Recognition. pp. 191–205. Springer (2021)
22. Hataya, R., Zdenek, J., Yoshizoe, K., Nakayama, H.: Faster autoaugment: Learning
augmentation strategies using backpropagation. In: Computer Vision–ECCV 2020:
16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part
XXV 16. pp. 1–16. Springer (2020)
23. Hendrycks, D., Dietterich, T.: Benchmarking neural network robustness to common
corruptions and perturbations. Proceedings of the International Conference on
Learning Representations (2019)
24. Ho, D., Liang, E., Chen, X., Stoica, I., Abbeel, P.: Population based augmentation:
Efficient learning of augmentation policy schedules. In: International conference on
machine learning. pp. 2731–2741. PMLR (2019)
25. Hoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A., Dar-
rell, T.: Cycada: Cycle-consistent adversarial domain adaptation. In: International
conference on machine learning. pp. 1989–1998. Pmlr (2018)
Adversarial Augmentation Training for Action Recognition
19
26. Jang, E., Gu, S., Poole, B.: Categorical reparameterization with gumbel-softmax.
In: International Conference on Learning Representations (2017)
27. Jeddi, A., Shafiee, M.J., Karg, M., Scharfenberger, C., Wong, A.: Learn2perturb:
An end-to-end feature perturbation learning to improve adversarial robustness. In:
Conference on Computer Vision and Pattern Recognition (CVPR) (2020)
28. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S.,
Viola, F., Green, T., Back, T., Natsev, P., et al.: The kinetics human action video
dataset. arXiv preprint arXiv:1705.06950 (2017)
29. Kim, K., Gowda, S.N., Aodha, O.M., Sevilla-Lara, L.: Capturing temporal infor-
mation in a single frame: Channel sampling strategies for action recognition. In:
33rd British Machine Vision Conference 2022, BMVC 2022, London, UK, Novem-
ber 21-24, 2022. BMVA Press (2022), https://bmvc2022.mpi-inf.mpg.de/0355.pdf
30. Kim, M., Li, D., Hospedales, T.M.: Domain generalisation via domain adapta-
tion: An adversarial fourier amplitude approach. In: International Conference on
Learning Representations (ICLR) (2023)
31. Korbar, B., Tran, D., Torresani, L.: Scsampler: Sampling salient clips from video
for efficient action recognition. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 6232–6242 (2019)
32. Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T.: HMDB: a large video
database for human motion recognition. In: Proceedings of the International Con-
ference on Computer Vision (ICCV) (2011)
33. Li, K., Wang, Y., Peng, G., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified
transformer for efficient spatial-temporal representation learning. In: International
Conference on Learning Representations (2022)
34. Li, Y., Hu, G., Wang, Y., Hospedales, T., Robertson, N.M., Yang, Y.: Differentiable
automatic data augmentation. In: Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXII 16. pp.
580–595. Springer (2020)
35. Lim, S., Kim, I., Kim, T., Kim, C., Kim, S.: Fast autoaugment. Advances in Neural
Information Processing Systems 32 (2019)
36. Lin, J., Gan, C., Han, S.: Tsm: Temporal shift module for efficient video under-
standing. In: Proceedings of the IEEE/CVF International Conference on Computer
Vision. pp. 7083–7093 (2019)
37. Lin, S., Zhang, Z., Huang, Z., Lu, Y., Lan, C., Chu, P., You, Q., Wang, J., Liu,
Z., Parulkar, A., Navkal, V., Chen, Z.: Deep frequency filtering for domain gener-
alization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR). pp. 11797–11807 (June 2023)
38. Lin, W., Mirza, M.J., Kozinski, M., Possegger, H., Kuehne, H., Bischof, H.: Video
test-time adaptation for action recognition. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR). pp. 22952–
22961 (June 2023)
39. Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H.: Video swin trans-
former. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 3202–3211 (2022)
40. Long, M., Cao, Y., Wang, J., Jordan, M.: Learning transferable features with deep
adaptation networks. In: International conference on machine learning. pp. 97–105.
PMLR (2015)
41. Pagliardini, M., Gupta, P., Jaggi, M.: Unsupervised learning of sentence embed-
dings using compositional n-gram features. arXiv preprint arXiv:1703.02507 (2017)
42. Pei, Z., Cao, Z., Long, M., Wang, J.: Multi-adversarial domain adaptation. In:
Proceedings of the AAAI conference on artificial intelligence. vol. 32 (2018)
20
K. Kim et al.
43. Schiappa, M.C., Biyani, N., Kamtam, P., Vyas, S., Palangi, H., Vineet, V., Rawat,
Y.S.: A large-scale robustness analysis of video action recognition models. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion (CVPR). pp. 14698–14708 (June 2023)
44. Shankar, S., Piratla, V., Chakrabarti, S., Chaudhuri, S., Jyothi, P., Sarawagi, S.:
Generalizing across domains via cross-gradient training. In: International Confer-
ence on Learning Representations (ICLR) (2018)
45. Shorten, C., Khoshgoftaar, T.M.: A survey on image data augmentation for deep
learning. Journal of big data 6(1), 1–48 (2019)
46. Shrivastava, A., Pfister, T., Tuzel, O., Susskind, J., Wang, W., Webb, R.: Learn-
ing from simulated and unsupervised images through adversarial training. In: Pro-
ceedings of the IEEE conference on computer vision and pattern recognition. pp.
2107–2116 (2017)
47. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-
nition in videos. In: Advances in Neural Information Processing Systems. pp. 568–
576 (2014)
48. Snell, J., Swersky, K., Zemel, R.: Prototypical networks for few-shot learning. Ad-
vances in neural information processing systems 30 (2017)
49. Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes
from videos in the wild. arXiv preprint arXiv:1212.0402 (2012)
50. Taigman, Y., Polyak, A., Wolf, L.: Unsupervised cross-domain image generation.
arXiv preprint arXiv:1611.02200 (2016)
51. Tzeng, E., Hoffman, J., Darrell, T., Saenko, K.: Simultaneous deep transfer across
domains and tasks. In: Proceedings of the IEEE international conference on com-
puter vision. pp. 4068–4076 (2015)
52. Wang, L., Tong, Z., Ji, B., Wu, G.: Tdn: Temporal difference networks for efficient
action recognition. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 1895–1904 (2021)
53. Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Temporal
segment networks for action recognition in videos. IEEE transactions on pattern
analysis and machine intelligence 41(11), 2740–2755 (2018)
54. Wang, R., Yang, Y., Tao, D.: Art-point: Improving rotation robustness of point
cloud classifiers via adversarial rotation. In: Conference on Computer Vision and
Pattern Recognition (CVPR) (2022)
55. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Pro-
ceedings of the IEEE conference on computer vision and pattern recognition. pp.
7794–7803 (2018)
56. Wu, W., He, D., Lin, T., Li, F., Gan, C., Ding, E.: Mvfnet: Multi-view fusion
network for efficient video recognition. In: Proceedings of the AAAI Conference on
Artificial Intelligence. vol. 35, pp. 2943–2951 (2021)
57. Wu, Z., Xiong, C., Ma, C.Y., Socher, R., Davis, L.S.: Adaframe: Adaptive frame
selection for fast video recognition. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 1278–1287 (2019)
58. Yi, C., YANG, S., Li, H., peng Tan, Y., Kot, A.: Benchmarking the robustness of
spatial-temporal models against corruptions. In: Thirty-fifth Conference on Neural
Information Processing Systems Datasets and Benchmarks Track (Round 2) (2021)
59. Yu, T., Yang, Y., Li, D., Hospedales, T.M., Xiang, T.: Simple and effective stochas-
tic neural networks. In: Conference on Artificial Intelligence (AAAI) (2021)
60. Zhang, J., Xu, X., Han, B., Niu, G., Cui, L., Sugiyama, M., Kankanhalli, M.:
Attacks which do not kill training make adversarial learning stronger. In: Interna-
tional conference on machine learning. pp. 11278–11287. PMLR (2020)
Adversarial Augmentation Training for Action Recognition
21
61. Zhang, X., Yu, F.X., Chang, S.F., Wang, S.: Deep transfer network: Unsupervised
domain adaptation. arXiv preprint arXiv:1503.00591 (2015)
62. Zhao, Y., Wu, Y., Chen, C., Lim, A.: On isometry robustness of deep 3d point
cloud models under adversarial attacks. In: Conference on Computer Vision and
Pattern Recognition (CVPR) (2020)
63. Zhou, B., Andonian, A., Oliva, A., Torralba, A.: Temporal relational reasoning in
videos. In: Proceedings of the European Conference on Computer Vision (ECCV).
pp. 803–818 (2018)
64. Zhou, K., Yang, Y., Hospedales, T.M., Xiang, T.: Deep domain-adversarial im-
age generation for domain generalisation. In: Conference on Artificial Intelligence,
(AAAI) (2020)
","The literature review encompasses various aspects of video action recognition, data augmentation, and adversarial training approaches. In video action recognition, studies explore 3D convolutional neural networks, relational reasoning with non-local operations, and dual-stream designs to capture both spatial and temporal information. On the data augmentation front, autoaugment policies search for optimal transformation sequences to improve generalization. Adversarial training is also examined, focusing on its effectiveness in achieving adversarial robustness and its broader benefits, such as robustness against natural data corruptions and domain shifts.nan"
"Machine learning has become widely used in drug discovery. Prevailing deep learning-based models have generally focused on incorporating more features to obtain better representations, while neglecting the fact that not all features are equally important for a particular task. This often leads to suboptimal training efficiency and predictive accuracy. To address this issue, we introduce MolTailor, a novel method that tailors general molecular representations to specific tasks by leveraging text prompts. Building upon the idea of utilizing language models as an agent and molecular pretraining models as a knowledge base, MolTailor adjusts the weights of different features of the molecular representation according to task descriptions via text prompts. This results in task-specific molecular representations that improve downstream prediction performance. Our extensive experimental evaluations on 8 tasks from MoleculeNet demonstrate that MolTailor achieves state-of-the-art results in most tasks, validating the efficacy of task-guided representation learning for molecular property prediction.","The integration of artificial intelligence (AI) technology into various stages of drug design has significantly improved efficiency and reduced development costs. Molecular representation learning lies at the core of AI-enabled techniques, enabling a wide array of downstream tasks, such as molecular property prediction and drug-drug interaction judgment. Inspired by the success of pretrained language models in natural language processing, researchers have begun exploring the use of sequence-based, graph-based, and recently, multimodal models for molecular representation learning. However, most existing methods still fall short in utilizing task information as prior knowledge, which can compromise training efficiency.","Our proposed method, MolTailor, comprises three primary components: 1) Construction of the Molecule-Text Multi-Task Regression (MT-MTR) corpus, which consists of triples of (molecule, task description, regression labels). 2) An adaption of the Transformer Encoder Block to incorporate a Multimodal Text Encoder that modifies the original architecture to perform self-attention and cross-attention operations simultaneously. This enables the model to map the general molecular representation to obtain vectors that are then concatenated with textual vectors. 3) A pretraining strategy where the model learns to predict regression labels mentioned in the task description based on the molecule and text prompt. Subsequently, for downstream tasks, the model generates a task description in the same format as pretraining and takes the SMILES and task description as input to predict labels corresponding to the task.","Our comprehensive experiments on 8 representative tasks from MoleculeNet demonstrate the effectiveness of MolTailor across various classification and regression tasks. MolTailor achieves state-of-the-art results on ESOL, FreeSolv, and Lipophilicity datasets for regression tasks. For classification tasks, MolTailor exhibits performance gains on HIV and Tox21 but shows some limitations on the other two datasets, which we attribute to the pretraining task favoring regression. Furthermore, MolTailor demonstrates similar gains when utilizing different molecular pretraining models, indicating its transferability and ability to inherit strengths from different backbones.","MolTailor introduces a novel perspective on molecular representation learning, emphasizing the need to not only incorporate more comprehensive information but also to combine contextual information and obtain molecular representations that are more suitable for specific tasks. By leveraging the knowledge implied in the text modality and the reasoning ability of language models, MolTailor generates task-specific molecular representations that outperform general representations. In the future, we aim to explore new pretraining tasks that can stably improve model performance on both classification and regression tasks, build molecular-text multimodal models based on large language models, and collaborate with domain experts to apply these methods to real-world production problems.",MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks via Text Prompts,"Haoqiang Guo, Sendong Zhao, Haochun Wang, Yanrui Du, Bing Qin","MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks via
Text Prompts
Haoqiang Guo, Sendong Zhao*, Haochun Wang, Yanrui Du, Bing Qin,
Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China
{hqguo, sdzhao, hchwang, yrdu, bqing}@ir.hit.edu.cn
Abstract
Deep learning is now widely used in drug discovery, provid-
ing significant acceleration and cost reduction. As the most
fundamental building block, molecular representation is es-
sential for predicting molecular properties to enable various
downstream applications. Most existing methods attempt to
incorporate more information to learn better representations.
However, not all features are equally important for a specific
task. Ignoring this would potentially compromise the training
efficiency and predictive accuracy. To address this issue, we
propose a novel approach, which treats language models as an
agent and molecular pretraining models as a knowledge base.
The agent accentuates task-relevant features in the molecular
representation by understanding the natural language descrip-
tion of the task, just as a tailor customizes clothes for clients.
Thus, we call this approach MolTailor. Evaluations demon-
strate MolTailor’s superior performance over baselines, val-
idating the efficacy of enhancing relevance for molecular
representation learning. This illustrates the potential of lan-
guage model guided optimization to better exploit and un-
leash the capabilities of existing powerful molecular repre-
sentation methods. Our codes and appendix are available at
https://github.com/SCIR-HI/MolTailor.
Introduction
In recent years, AI technology has been widely applied to
various stages of drug design, such as compound synthesis
and screening, etc. (Mamoshina et al. 2016). This has greatly
improved the efficiency and reduced the cost of drug devel-
opment. Molecular representation learning serves as the cor-
nerstone for AI techniques in drug design, empowering a
wide range of downstream tasks such as molecular property
prediction and drug-drug interaction judgment (Xia et al.
2023). Molecular representations are essentially vector em-
beddings for molecules, analogous to word embeddings in
NLP. The idea of encoding molecules as mathematical vec-
tors dates back to the 1940s (Wiener 1947).
The development of molecular representation learning
can be roughly divided into four phases. In the first phase,
molecular representations were constructed through expert
knowledge, which can be categorized into two types: de-
scriptors (Wiener 1947) and fingerprints (Rogers and Hahn
*Corresponding author
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Molecular
Pretraining
Model
Encode
General
Molecular Representation
Sulfonamide group
Carboxylic acid group
Trifluoromethyl group
Phenyl group
Ether group
……
Sulfon…
Carbo…
Triflu…
Phen…
Ether
……
Task-Specific
Molecular Representation
Tailor
Lipo
Figure 1: Most existing molecular pretraining models (e.g.
Grover, MolCLR) attempt to encode as much molecular in-
formation as possible (e.g. various functional groups and
molecular weight) into a vector to obtain general molecu-
lar representation. However, for specific downstream tasks
(e.g. Lipo, predicting lipophilicity of compounds), features
are not equally important (e.g. Sulfonamide and Carboxylic
acid groups significantly increase the hydrophilicity, being
more critical than the remaining groups). By understanding
task descriptions, MolTailor adjusts the weights of different
features in the representation to obtain task-specific molec-
ular representation.
2010). Descriptors focus on the inherent chemical charac-
teristics of molecules, while fingerprints contain structural
information. In the second phase, inspired by deep learning,
some works (Gilmer et al. 2017; Yang et al. 2019; Song et al.
2020) started to learn molecular representations from la-
beled data using supervised learning. However, the scarcity
of labeled data, due to the expensive experimental cost and
time required, limits further improvements to model perfor-
mance and generalization.
The success of pretrained language models in NLP (De-
vlin et al. 2018) has demonstrated the potential of self-
supervised learning. Building upon this, in the third phase,
some works pretrained sequence-based models to learn
molecular representation by mimicking successful NLP pre-
trained language models. (Chithrananda, Grand, and Ram-
sundar 2020; Kim et al. 2021; Ross et al. 2022). Concur-
rently, others pretrained graph-based models (Hu et al. 2019;
You et al. 2020). More recently, in the fourth phase, re-
searchers are no longer limited to pretraining solely on the
arXiv:2401.11403v1  [cs.LG]  21 Jan 2024
molecular data itself. Instead, they are attempting to incor-
porate additional information, such as knowledge graphs
(Fang et al. 2022) and textual information (Zeng et al. 2022;
Edwards et al. 2022; Su et al. 2022; Chen et al. 2023). No-
tably, molecule-text multimodal learning has received in-
creasing attention recently and achieved promising results.
As introduced above, most existing works merely strive
to inject more information into the representations without
utilizing task information as prior knowledge, which could
compromise the training efficiency, as shown in Fig. 1. In-
spired by this observation, we propose a new method called
MolTailor, where a language model acts as a tailor, adapt-
ing general molecular representations (ready-made clothing)
into task-specific ones (customized clothing) based on user
needs.
To achieve this goal, we adopt a dual-tower model struc-
ture with one language pretraining module and one molec-
ular pretraining module, joint together through a cross-
attention module. Meanwhile, we constructed a new pre-
training task: Molecule-Text Multi-Task Regression (MT-
MTR). The dataset for this task consists of (molecule, task
description, regression labels) triplets. Here, the molecule is
represented as a SMILES string (Weininger 1988), the task
description is a text prompt describing molecular properties
that are most helpful for solving the task, and the regres-
sion labels are values of features mentioned in the task de-
scription. The model needs to predict the regression labels
based on the SMILES and task description. This pretrain-
ing task teaches the model to enhance the weights of task-
relevant features in the molecular representation according
to the task description. Our contributions can be summarized
as follows:
• We propose MolTailor, the first approach to generate
task-specific molecular representations via text prompts,
which provides a new perspective on text-molecule mul-
timodal learning: not only injecting the knowledge in
texts into molecular representations but also utilizing the
reasoning capacity of language models.
• We construct MT-MTR, a new molecule-text multimodal
pretraining task. This task teaches the model the capa-
bilities of instruction following and adapting molecular
representations.
• We comprehensively evaluate across eight subsets of the
MoleculeNet, thereby demonstrating the effectiveness of
task-specific molecular representation learning.
Related Work
Molecular pretraining models
Models can be roughly
categorized into the following three types: sequence-based,
graph-based, and external knowledge (Xia et al. 2023).
1. Sequence-based: Representing molecules as SMILES
strings or other sequences and using language models as
backbones for pretraining. SMILES-BERT (Wang et al.
2019) and ChemBERTa (Chithrananda, Grand, and Ram-
sundar 2020) use Masked Language Modeling (MLM) for
pretraining tasks, while CHEM-BERT (Kim et al. 2021),
ChemBERTa-2 (Ahmad et al. 2022) additionally incorporate
property prediction tasks. MM-Deacon (Guo et al. 2021)
uses contrastive learning with SMILES and INPAC as par-
allel inputs. 2. Graph-based: Representing molecules as
graphs and using graph neural networks for pretraining.
GROVER (Rong et al. 2020) and Mole-BERT (Xia et al.
2022) use Mask Component Modeling (MCM) for pretrain-
ing, while GraphCL (You et al. 2020) and MolCLR (Wang
et al. 2022) employ contrastive learning. Denoising (Zaidi
et al. 2022) and GeoSSL (Liu, Guo, and Tang 2022) are
trained with denoising methods inspired by denoising diffu-
sion models. 3. External knowledge: GraphMVP (Liu et al.
2021) and 3D Infomax (St¨ark et al. 2022) incorporate 3D
structures as supplementary information. KCL (Fang et al.
2022) uses knowledge graphs to enhance molecular repre-
sentations.
Image-text multimodal pretraining models.
Models can
be divided into dual-tower and single-tower structures based
on architectures: 1. Dual-tower structure: ViLBERT (Lu
et al. 2019) introduces the co-attentional transformer (Co-
TRM) layer, which effectively integrates information from
both modalities. CLIP (Radford et al. 2021) adopts con-
trastive learning for pretraining and achieves significant per-
formance gains. The CoCa (Yu et al. 2022) model uses dual
towers of an image encoder and a text decoder. 2. Single-
tower structure: VisualBERT (Li et al. 2019) takes texts
and images as a unified input for learning. ViLT (Kim, Son,
and Kim 2021) adopts a patch projection approach, greatly
improving the processing speed. BeiT-3 (Wang et al. 2023)
uses distinct Feedforward Neural Network (FFN) layers for
different modalities while sharing attention modules.
Molecule-text multimodal pretraining models.
Simi-
larly, models can be divided into dual-tower and single-
tower structures based on architectures: 1. Single-tower
structure: Using a language model as the backbone for fur-
ther pretraining. KV-PLM (Zeng et al. 2022) and MolXPT
(Liu et al. 2023) inject SMILES into literature text by lo-
cating molecule names to obtain mixed corpora and conduct
MLM and language pretraining (LM) respectively. MolT5
(Edwards et al. 2022) does replace corrupted spans pre-
training on molecular and text data, meanwhile, uses mu-
tual translation between molecules and textual descriptions
as downstream tasks. Text+Chem T5 (Christofidellis et al.
2023) and ChatMol (Zeng et al. 2023) use multi-task learn-
ing for pretraining. GIMLET (Zhao et al. 2023) takes graphs
and texts as a unified input and uses instruction-based su-
pervised property prediction for pretraining. 2. Dual-tower
structure: MoMu (Su et al. 2022) and MoleculeSTM (Liu
et al. 2022) do contrastive learning on molecule-description
pairs. CLAMP (Seidl et al. 2023) does contrastive learning
on molecule-bioassay pairs. Additionally, ChemCrow (Bran
et al. 2023) enhances large language models (LLMs) with
chemical tools to accomplish real-world chemical tasks.
Methodology
Fig. 2 presents an overview of our proposed approach. In this
section, we introduce MolTailor in three aspects: 1. Con-
struction of the MT-MTR corpus. 2. Model architecture of
MolTailor. 3. Pretraining of MolTailor and application of
MolTailor to downstream tasks.
List of molecules
Deduplicate
Canonicalize
Compute
Molecular Properties
QED
0.55012…
MolWt
180.158…
TPSA
63.6000…
NumHDonors
1
……
……
Random Sample
After analyzing the given chemical task, severa
l key descriptors were identified that can be use
d to solve the task. These descriptors include:
> Introduction
> Summary:
1. HallKierAlpha: This descriptor represents th
e molecule’s shape and size. It can ……
2. VSA_EState2: This descriptor ……
3. HeavyAtomCount: This descriptor ……
……
By considering these descriptors, we can gain i
nsights into the chemical properties……
> Task-relevant properties
{name of property}: description of property
Task Description (Text Prompt)
5~10
…
…
Key
Value
Regression Labels
-1.83999…
…
21.1847…
13
M-Encoder
Unimodal
T-Encoder
Multimodal
T-Encoder
SMILES
Task Description
General
Molecular
Representation
𝐾𝑚
𝑉𝑚
×𝐾
Attention
𝑄𝑡
𝐾𝑡
𝑉𝑡
Textual Hidden States
Add & Layer Norm
Feed Forward
Add & Layer Norm
Task-Specific Molecular Representation
MolTailor
SMILES
Task Description
Task-Specific Molecular Representation
Predict
Regression Labels
MolTailor
…following d
escriptors mig
ht be helpful:
1. MolLogP:
This descripto
r calculates th
e lipophilicity
of the …
BBBP
Analyze
C[C@H](N)Cc1ccccc1
p_np: 1
Linear
a)
b)
c)
d)
e)
Figure 2: Overview of the MolTailor framework. a) The construction process of the MT-MTR dataset. We obtain represen-
tative molecules from DrugBank (Wishart et al. 2018) and ChEBI (Hastings et al. 2016), and then use RDKit to calculate 209
properties for each molecule. For each molecule, we randomly sample 5-10 properties from the property set, use the property
names to generate virtual task descriptions via GPT-3.5, and use the property values as regression labels. b) Model architecture
of MolTailor. MolTailor consists of a language pretraining model (T-Encoder) and a molecular pretraining model (M-Encoder).
The T-Encoder is divided into a unimodal part (for understanding task descriptions) and a multimodal part (for adjusting molec-
ular representations). c) Internal structure of the Multimodal T-Encoder. It modifies the original Transformer Encoder Block
to perform self-attention and cross-attention operations simultaneously: mapping the general molecular representation to obtain
Km and Vm vectors which are then concatenated with textual vectors Kt and Vt. d) Pretraining task of MolTailor. The model
needs to predict properties mentioned in the task description based on the molecule and text prompt. e) Downstream tasks of
MolTailor. For a specific downstream task, we first generate the task description in the same format as pretraining via GPT-4
analysis, then take the SMILES and task description as input to predict labels for the corresponding task.
Construction of MT-MTR
The construction process of the MT-MTR (Molecule-Text
Multi-Task Regression) dataset consists of 3 main steps as
shown in Fig. 2a:
Step 1: Obtain representative molecules. We obtain
molecules from DrugBank (Wishart et al. 2018) and ChEBI
(Hastings et al. 2016), then use RDKit1 to deduplicate and
canonicalize, resulting in 55,759 valid SMILES.
Step 2: Calculate molecular properties. For each
molecule obtained in the previous step, we use RDKit to
calculate 209 properties2. Among these properties, some are
continuous values while others are discrete values. We unify
them into regression tasks.
Step 3: Obtain task descriptions and regression la-
bels. For each molecule, we randomly sample 5-10 proper-
ties from the property set obtained in the previous step. The
sampled property names are filled into the prompt template
in Tab. 1 (Prompt for Pretraining) to obtain text input fed
into GPT for generating virtual task description. Since GPT-
4 and GPT-3.5 achieve similar performance on this task, we
use the more cost-effective GPT-3.5 for generation in our ex-
periments. The values of the sampled properties are used as
1https://www.rdkit.org
2RDKit’s Descriptor module provides 209 descriptors, which
we uniformly refer to as “properties” in this paper for clarity.
the regression labels.
The resulting MT-MTR corpus contains (molecule, task
description, regression labels) triplets. It is also worth noting
that MT-MTR does not ask the model to predict all prop-
erties of the molecules; it only needs to predict properties
mentioned in the text prompt. This aims to teach the model
to generate molecule representations tailored to the text for
improved predictions.
Model architecture of MolTailor
As illustrated in Fig. 2b&c, MolTailor consists of a lan-
guage pretraining model (T-Encoder) and a molecular pre-
training model (M-Encoder). The language model is divided
into a Unimodal Text Encoder (UT-Encoder) and a Multi-
modal Text Encoder (MT-Encoder) by introducing a few pa-
rameters. We treat the M-Encoder as a knowledge base and
the T-Encoder as an agent. The agent obtains the final de-
sired representation by understanding natural language and
adjusting the well-initialized molecular representation from
the knowledge base. The UT-Encoder captures semantic in-
formation and the MT-Encoder then tailors molecular repre-
sentations based on understanding task descriptions.
Since any molecular pretraining model can serve as the
M-Encoder in our method, in this section we mainly de-
scribe the Transformer Encoder Block (TEB), the funda-
mental component composing mainstream language mod-
Template
Prompt
for
Pre-
training
As a seasoned expert in the field of chem-
istry, your task is to analyse a chemical task.
And you found following properties of chemi-
cal compounds can help solve this task. Please
summarize your analysis. The length should be
less than 300 tokens.
Properties:
{Sampled Properties}.
Task analysis results:
Prompt
for
Down-
stream
Example:
{Example of task description from MT-MTR}
Property Names:
{209 property names from RDKit}
Please analyze the Task Name. When dis-
cussing the properties related to this task, list
any properties that you think may be helpful
for solving the task. Simply provide the analy-
sis results directly in less than 400 tokens, re-
ferring to the example for guidance.
Table 1: Prompt template for interacting with GPT to gener-
ate task descriptions
els, as well as the internal structure of the MT-Encoder. Ad-
ditionally, in our experiments, we tested two types of M-
Encoder: CHEM-BERT (Kim et al. 2021) and ChemBERTa-
2 (Ahmad et al. 2022).
Transformer Encoder Block.
As the basic building block
of the Transformer (Vaswani et al. 2017) encoder, it con-
sists of four main components: Multi-Head Self-Attention
(MHA), Feed Forward Network (FFN), and Residual Con-
nection with Layer Normalization (LN). For MHA, given
hidden states x ∈ R(bs×n×d) where bs indicates batch size,
n indicates sequence length, and d indicates embedding di-
mension, it is first mapped to vectors Q, K and V by ma-
trices W Q, W K, W V ∈ R(d×d), keeping dimensions un-
changed. Then, with h heads, d is split into h parts. After
that, vectors are transposed and reshaped to get Q, K, V ∈
R(bs×h×n×d/h). Finally, attention is computed:
Attention(Q, K, V ) = softmax
QV T
√dk

(1)
MHA(x) = Attention(f r
Q(x), f r
K(x), f r
V (x))
(2)
where f r
∗
means f reshape(xW ∗). The output x∗
∈
R(bs×n×d) of the MHA goes through the FFN, residual con-
nection and LN to obtain the final output x ∈ R(bs×n×d).
FFN(x) = ReLU(xW 1 + b1)W 2 + b2
(3)
x = LN(x + FFN(MHA(x)))
(4)
Multimodal T-Encoder.
Since the task descriptions con-
tain biomedical terminology, we chose PubMedBERT (Gu
et al. 2021) as the backbone for the T-Encoder. Addition-
ally, we also present in the appendix the results of using
BioLinkBERT (Yasunaga, Leskovec, and Liang 2022). Pub-
MedBERT contains 12 TEBs. We designate the first 9 layers
as the UT-Encoder, and the remaining 3 layers as the MT-
Encoder.
To build the MT-Encoder, we draw on previous work
(Chen et al. 2022) to modify the original MHA to MHA∗
that can simultaneously perform Self-Attention and Cross-
Attention operations, by introducing very few parameters.
Specifically, the MT-Encoder takes xt and xm as inputs,
where xt ∈ R(bs×nt×dt) denotes the hidden states from the
UT-Encoder, and xm ∈ R(bs×nm×dm) denotes the general
molecular representation (i.e., the last hidden states from the
M-Encoder). MHA∗ computes as follows:
Q∗ = f r(xtW t
Q)
K∗ = f r([xtW t
K, xmW m
K])
V ∗ = f r([xtW t
V , xmW m
V ])
MHA∗(xt, xm) = Attention(Q∗, K∗, V ∗)
(5)
where f r denotes reshape function, [∗] denotes concate-
nation along the dimension corresponding to the sequence
length n. The output of the MT-Encoder Block is computed:
x∗ = LN(xt + FFN(MHA∗(xt, xm)))
(6)
Chen et al. (2022) formally proves that MHA∗(x) is equiv-
alent to a weighted average of self-attention and cross-
attention:
MHA∗(xt, xm) = (1 − λ(xt))MHA(xt)
+ λ(xt)MHA(Qt, Km, V m)
(7)
Pretraining and Downstream Application
Pretraining on MT-MTR.
We use MT-MTR as the pre-
training objective . In detail, given SMILES (zm) and task
description (zt) as input, the model predicts regression la-
bels y ∈ R(1×209), as displayed in Fig. 2d. The pretraining
loss uses MSE and can be formulated as:
L = 1
N
N
X
j=1
 
1
count(m)
M
X
i=1
mij(yij − ˆyij)2
!
(8)
where N is the number of samples, m ∈ R(1×209) is a
0-1 vector, in which the value 1 indicates the existence of
the corresponding regression label, while 0 indicates its ab-
sence, count(m) means the number of valid labels, M is the
number of all properties (here M = 209), yij is the ground
truth regression label, and ˆyij is the predicted value.
Downstream Application.
When applying MolTailor to
downstream tasks, we use GPT-4 to analyze the specific
task and then generate the corresponding task description,
as shown in Fig. 2e. We use the template in Tab. 1 to prompt
GPT to generate descriptions that mimic the format of those
in the pretraining corpus. At the same time, we restrict it to
select task-relevant features from the set of properties sup-
ported by RDKit. Then, we feed the generated analysis and
SMILES into MolTailor to obtain the task-specific molecu-
lar representation:
z = MolTailor (zm|zt)
(9)
where z denotes the task-specific molecular representation.
Finally, z goes through a prediction head to predict the la-
bels corresponding to the task.
Experiments
In this section, we conduct comprehensive experiments to
demonstrate the efficacy of our proposed method. The ex-
periments are designed to analyze the method by addressing
the following six key questions:
Q1: Are the task-specific molecular representations gen-
erated by MolTailor better than general representations? Q2:
Can MolTailor achieve performance improvements on dif-
ferent M-Encoders? Q3: How do the task descriptions in-
fluence the effectiveness of pretraining? Q4: How do dif-
ferent text prompts affect MolTailor? Q5: When MolTailor
achieves better performance, are the task-relevant features in
the molecular representations enhanced? Q6: Does MolTai-
lor pay attention to the key information in both the molecules
and text prompts?
Experimental Setup
Pretraining Corpus.
We use MT-MTR corpus for pre-
training, which contains 55,759 triples. Moreover, we
present the data overlap analysis results in the appendix.
Downstream Datasets.
We select 8 representative tasks
from MoleculeNet (Wu et al. 2018) for experiments, which
consist of 4 classification tasks (BBBP, ClinTox, HIV,
Tox21) and 4 regression tasks (ESOL, FreeSolv, Lipophilic-
ity, QM8), covering physiology, biophysics, physical chem-
istry, and quantum mechanics. Following Wu et al. (2018),
each task uses the recommended splitting method to divide
data into training/validation/test sets with a ratio of 8:1:1.
Baselines.
We adopt the following four types of baselines:
• Molecular Fingerprints: MACCSFP (Durant et al.
2002) encodes molecules based on substructures, RD-
KitFP (O’Boyle and Sayle 2016) encodes molecules
based on topology or path, and MorganFP (Rogers and
Hahn 2010) encodes molecular environment and struc-
ture starting from atoms within a radius.
• Sequence-based
Methods:
BioLinkBERT(Yasunaga,
Leskovec, and Liang 2022), PubMedBERT (Gu et al.
2021); ChemBERTa-2 (Ahmad et al. 2022), and CHEM-
BERT (Kim et al. 2021).
• Graph-based Methods: Grover (Rong et al. 2020), Mol-
CLR (Wang et al. 2022), Mole-BERT (Xia et al. 2022),
and Uni-Mol (Zhou et al. 2023).
• Multimodal Methods: KCL (Fang et al. 2022), KV-
PLM (Zeng et al. 2022), MolT5 (Edwards et al. 2022),
and MoMu (Su et al. 2022).
We additionally construct Random and RDKit-DP as base-
lines for comparison, where Random refers to random vec-
tors, and RDKit-DP consists of the 209 molecular properties
calculated by RDKit.
Evaluation Methodology.
To better evaluate the molecu-
lar representations learned by different models, we conduct
experiments in linear probe setting. As a result, the baseline
experimental results reported in this paper may differ from
the original papers. We freeze the model parameters to ex-
tract representations for downstream tasks. These extracted
representations are then mapped to labels through a learn-
able linear layer.
Following the recommendation of Wu et al. (2018), we
use ROC-AUC as the evaluation metric for classification
tasks. For the regression task qm8, we use MAE, and for
other regression tasks, we use RMSE. To ensure fairness,
we use Optuna (Akiba et al. 2019) to search 10 learning rates
(LRs) for each model. We repeat each task 3 times and report
the mean and standard deviation. Due to space limitations,
the standard deviations are included in the appendix.
Implementation Details.
For the pretraining phase, we
employ the AdamW optimizer, complemented by a linear
learning rate scheduler. We set the LR at 5.5e-5 and use a
warmup ratio of 0.1. The training is conducted 50 epochs
with a batch size of 64, utilizing two A100-SXM4-80GB
GPUs. For the downstream tasks, we opt for the Adam opti-
mizer and leverage Optuna for hyperparameter tuning, con-
ducting 10 trials to identify the optimal LR within the range
of 1e-5 to 1e-2 for each model on every task. The optimal
LR is determined based on the performance of the validation
set. We train our model using a batch size of 64 on a single
GeForce RTX 2080 Ti GPU, employing an early stop mech-
anism with a patience setting of 3 and limiting the training
to a maximum of 50 epochs.
Performance Comparison (Q1 & Q2)
Q1: We evaluate whether molecular representations en-
hanced by task descriptions could improve performance
across 8 tasks. Tab. 2 shows that MolTailor achieves perfor-
mance gains over the backbone model on the 4 regression
tasks, and notably attains state-of-the-art (SOTA) results on
ESOL, FreeSolv, and Lipophilicity datasets. On the 4 classi-
fication tasks, MolTailor’s performance is inconsistent, with
gains on HIV and Tox21 but losses on the other 2 datasets
when using CHEM-BERT as backbone. We hypothesize this
may be because the pretraining task favors regression, which
could inhibit performance on classification. And this implies
classification and regression tasks may require different op-
timizing approaches. Additionally, we test converting MT-
MTR to MT-MTC for pretraining but see minimal improve-
ment, which indicates this is not due to the form of the task.
Q2: We experiment on two backbones, ChemBERTa-2
and CHEM-BERT. MolTailor achieves similar gains on top
of both backbones. Notably, the performance differences
between the backbones are also mirrored in the respective
MolTailor variants. This demonstrates MolTailor’s transfer-
ability and ability to inherit strengths of different back-
bones. The results support that further gains may be achiev-
able by transferring MolTailor to new state-of-the-art single-
modality models.
Models
Classification (ROC-AUC)
Regression (RMSE / MAE)
Params
Dataset
BBBP
ClinTox
HIV
Tox21
ESOL
FreeSolv
Lip
QM8
#Molecules
2039
1478
41127
7831
1128
642
4200
21786
#Split
Scaffold
Random
Scaffold
Random
Random
Random
Random
Random
#Tasks
1
2
1
12
1
1
2
16
Random
48.38
56.01
49.54
51.11
3.3358
5.4831
1.3813
0.0320
-
RDKit-DP
78.25
67.36
70.85
65.61
4.8940
2.8068
0.9963
0.0202
-
RDKit-FP
87.65
57.13
78.66
76.14
1.0830
2.0725
0.9007
0.0181
-
MACCS-FP
81.64
83.05
77.53
77.27
1.0833
1.9086
0.9886
0.0196
-
Morgan-FP
82.73
72.61
82.65
75.29
1.2413
2.1896
0.8196
0.0200
-
Grover
79.83
87.75
77.47
79.61
0.8977
1.9041
0.8301
0.0184
107M
MolCLR
81.27
78.15
71.48
75.61
1.3421
3.0436
1.0448
0.0219
2M
Mole-BERT
82.70
81.82
79.35
84.20
1.1379
2.3626
0.8316
0.0221
2M
Uni-Mol
79.52
88.65
74.18
78.08
1.0509
2.6913
1.0363
0.0219
48M
BioLinkBERT
83.81
87.75
71.24
73.81
1.1739
3.1350
1.0589
0.0234
108M
PubMedBERT
89.10
84.29
72.30
73.77
1.0715
2.5999
1.0851
0.0232
108M
ChemBERTa-2
84.70
84.21
78.88
80.75
0.8103
1.8439
0.7948
0.0191
3M
CHEM-BERT
84.10
93.80
76.99
80.54
0.7973
2.0214
0.8571
0.0215
51M
KCL
76.86
60.80
68.48
74.98
0.8728
2.7615
0.9868
0.0225
1M
KV-PLM
86.36
81.20
73.52
74.62
1.1785
2.8840
1.1004
0.0233
109M
MoMu-ME
80.41
67.99
71.91
74.75
1.4135
2.3229
0.9835
0.0222
2M
MoMu-TE
82.24
81.94
67.88
73.07
1.2562
3.1480
1.0885
0.0250
109M
MolTailor∗
84.65
85.95
76.42
80.32
0.7128
1.7826
0.7848
0.0185
112M
MolTailor
81.15
92.37
77.42
80.67
0.7234
1.7881
0.8107
0.0196
161M
Table 2: Evaluation results of MolTailor and baselines under the linear probe setting on MoleculeNet. Here, ME stands for
Molecule Encoder and TE stands for Text Encoder in MoMu-ME/TE. MolTailor∗ denotes using ChemBERTa-2 as the M-
Encoder, while MolTailor indicates using CHEM-BERT. The table shows the average performance over 3 runs. Standard devi-
ations are omitted due to space limitations but can be viewed in the appendix. Results in bold indicate the best performance.
Green highlights cases where MolTailor outperforms the backbone, while yellow indicates the opposite.
Alation Study (Q3 & Q4 & Q5)
Q3. To evaluate the influence of task descriptions on pre-
training in MT-MTR, we first remove the task descriptions
to obtain the dataset MT-MTR∗, then pretrain CHEM-BERT
and PubMedBERT on MT-MTR∗ to get CHEM-BERT∗ and
PubMedBERT∗ models. Next, we concatenate the task de-
scriptions after the SMILES strings to construct dataset MT-
MTR†, and pretrain PubMedBERT on MT-MTR† to obtain
PubMedBERT†. As shown in Tab. 3, CHEM-BERT∗ im-
proves over untrained CHEM-BERT on regression tasks but
drops on classification, indicating MTR as a pretraining task
benefits downstream regression but negatively impacts clas-
sification. Also, PubMedBERT† over PubMedBERT∗ and
MolTailor over CHEM-BERT both demonstrate further re-
gression performance gains but classification performance
drops. This shows introducing task descriptions helps mod-
els better learn from the data. That is, if constructing new la-
bels can improve model performance on classification tasks,
then supplementing task descriptions can further amplify
such gains.
Q4. We replace the task-specific descriptions generated
via GPT-4 analysis with the irrelevant text “to be or not to
be, this is the question.” as noise prompts. Results in Tab. 4
show degraded performance with noise prompts, indicating
information in the task descriptions does help the model gen-
erate better molecular representations. However, it should be
Models
Classification↑
Regression↓
CHEM-BERT
83.86
0.9243
CHEM-BERT∗
83.11
0.9209
PubMedBERT
79.87
1.1949
PubMedBERT∗
81.67
0.9148
PubMedBERT†
81.08
0.9131
MolTailor
82.09
0.8475
Table 3: Experimental results for Q3. For these experiments,
the overlapping data are excluded, and all models are pre-
trained for only 20 epochs.
noted that the performance drop is slight, suggesting textual
information is not as important as expected in the process
of generating representations. Further investigation into this
phenomenon is needed.
Q5. Earlier experiments have shown that task-specific
molecular representations outperform general ones. Now,
we aim to test if task-relevant features are more pronounced
in these task-specific representations. Our theory is simple:
if a feature is more prominent in the representation, it should
lead to better predictions for that feature compared to a less
distinct representation.
For our experiment, we chose the ESOL dataset and four
Prompt Types
Classification↑
Regression↓
Origin
82.92
0.8348
Noise
82.77
0.8541
Table 4: Experimental results for Q4.
ESOL
MolWt
FractionCSP3
EState_VSA3
Kappa1
1.0
0.5
0.0
0.5
1.0
1.5
Normalized RMSE
KV-PLM
CHEM-BERT
MolTailor (noise)
MolTailor
Figure 3: Normalized RMSE of four models on ESOL task
and associated/unassociated proterties. The x-axis shows
the names of tasks including ESOL and four properties, of
which the first three are related to ESOL, while the last is
not. The y-axis plots N-RMSE for the four models on each
task, with lower values indicating better performance.
models: KV-PLM, CHEM-BERT, MolTailor, and MolTai-
lor (noise), where the latter refers to a version using a noise
prompt. We calculated 209 properties for each molecule in
ESOL, creating a new dataset named ESOL-MTR, compris-
ing pairs of SMILES strings and their 209 regression la-
bels. We trained all four models on ESOL-MTR and eval-
uated their ability to predict the 209 properties of the ESOL
molecules. From these, we focused on 4 properties for Fig.
3. We calculated the Normalized RMSE by subtracting the
average RMSE of the four models on a task from the orig-
inal RMSE and then dividing it by the standard deviation.
The four properties we selected include two mentioned in
the task description (MolWt and FractionCSP3), one rel-
evant but unmentioned (EState VSA3), and one unrelated
and unmentioned (Kappa1). Our analysis led to two main
observations:
• For the properties mentioned in the task description,
MolTailor outperforms MolTailor (noise), confirming
that these mentioned properties are indeed emphasized.
• We find prompts influence both mentioned and unmen-
tioned properties. “EState VSA3” is a relevant but un-
mentioned property shows performance gains. In con-
trast, “Kappa1” is an irrelevant and unmentioned prop-
erty performance decline. The results show that by under-
standing text prompts, the model can attend to properties
not explicitly stated.
> Last layer of UT-Encoder
Top 5 tokens: groups, ether, [SEP],
s, weight.
> Last layer of MT-Encoder
Top 5 text tokens: molwt,
molecular, can, solving, descriptors.
Origin Prompt
Noise Prompt
Figure 4: Visualization of the attention, answering Q6. The
two molecular graphs on the left show MolTailor’s atten-
tion over the molecules under different prompts. The text on
the right shows which input tokens from the original prompt
MolTailor pays most attention to.
Case Study (Q6)
Q6. We analyze the attention matrices from the last layers
of the UT-Encoder and MT-Encoder to investigate whether
MolTailor pays attention to key information. The analysis
results are shown in Fig. 4. Specifically, we select ESOL as
a representative task related to solubility. If the model at-
tends to information like molecular weight and polar func-
tional groups critical for determining solubility, it suggests
key information is captured.
After training on ESOL, we input SMILES from the test
set and task description into the model to obtain attention
matrices M UT and M MT . Since we use the vector corre-
sponding to the “[CLS]” token for prediction, we believe
the larger the attention value between “[CLS]” and a token
in M UT or M MT , the more the model focuses on that in-
formation.
In Fig. 4, tokens highlighted in red in the top 5 most at-
tended by the UT/MT-Encoders relate to molecular weight
or polar functional groups, indicating the model does pay at-
tention to key textual information. Also, the two molecular
graphs plotted using the most attended molecular tokens by
the MT-Encoder under different prompts show the original
prompt leads to more attention on polar functional groups
like ester and ketone, meaning critical information is better
captured.
Conclusion and Future Work
Overall, this work provides a new perspective on molecu-
lar representation learning, not only striving to include more
comprehensive information in the representations, but also
combining contextual information and obtaining molecu-
lar representations more suitable for specific tasks through
tailoring. In this work, we not only utilize the knowledge
implied in the text modality, more importantly, we try to
leverage the excellent reasoning ability of language mod-
els, which will become more important in the age of large
models.
In the future, we first plan to explore new pretraining tasks
that can stably improve model performance on both classifi-
cation and regression tasks. Secondly, we will explore how
to build molecular-text multimodal models based on large
language models. Finally, we look forward to in-depth col-
laborations with domain experts to apply the molecular-text
multimodal methods to actual production problems.
Acknowledgements
We thank the reviewers for their detailed and constructive
suggestions and for the affirmation of our paper. Mean-
while, we gratefully acknowledge the support of the Na-
tional Key R&D Program of China (2021ZD0113302), the
National Natural Science Foundation of China Youth Fund
(62206079), and the Heilongjiang Provincial Natural Sci-
ence Foundation of China (YQ2022F006). We also appre-
ciate the support from Du Xiaoman (Beijing) Science Tech-
nology Co., Ltd. @ on our research.
References
Ahmad, W.; Simon, E.; Chithrananda, S.; Grand, G.; and
Ramsundar, B. 2022. Chemberta-2: Towards chemical foun-
dation models. arXiv preprint arXiv:2209.01712.
Akiba, T.; Sano, S.; Yanase, T.; Ohta, T.; and Koyama, M.
2019. Optuna: A next-generation hyperparameter optimiza-
tion framework. In Proceedings of the 25th ACM SIGKDD
international conference on knowledge discovery & data
mining, 2623–2631.
Bran, A. M.; Cox, S.; Schilter, O.; Baldassari, C.; White, A.;
and Schwaller, P. 2023. Augmenting large language mod-
els with chemistry tools. In NeurIPS 2023 AI for Science
Workshop.
Chen, X.; Zhang, N.; Li, L.; Deng, S.; Tan, C.; Xu, C.;
Huang, F.; Si, L.; and Chen, H. 2022. Hybrid transformer
with multi-level fusion for multimodal knowledge graph
completion. In Proceedings of the 45th International ACM
SIGIR Conference on Research and Development in Infor-
mation Retrieval, 904–915.
Chen, Y.; Xi, N.; Du, Y.; Wang, H.; Jianyu, C.; Zhao,
S.; and Qin, B. 2023.
From Artificially Real to Real:
Leveraging Pseudo Data from Large Language Models
for Low-Resource Molecule Discovery.
arXiv preprint
arXiv:2309.05203.
Chithrananda, S.; Grand, G.; and Ramsundar, B. 2020.
ChemBERTa:
large-scale
self-supervised
pretrain-
ing for molecular property prediction.
arXiv preprint
arXiv:2010.09885.
Christofidellis, D.; Giannone, G.; Born, J.; Winther, O.;
Laino, T.; and Manica, M. 2023. Unifying molecular and
textual representations via multi-task language modelling.
arXiv preprint arXiv:2301.12586.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
Bert: Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805.
Durant, J. L.; Leland, B. A.; Henry, D. R.; and Nourse, J. G.
2002. Reoptimization of MDL keys for use in drug discov-
ery. Journal of chemical information and computer sciences,
42(6): 1273–1280.
Edwards, C.; Lai, T.; Ros, K.; Honke, G.; Cho, K.; and Ji, H.
2022. Translation between molecules and natural language.
arXiv preprint arXiv:2204.11817.
Fang, Y.; Zhang, Q.; Yang, H.; Zhuang, X.; Deng, S.; Zhang,
W.; Qin, M.; Chen, Z.; Fan, X.; and Chen, H. 2022. Molec-
ular contrastive learning with chemical element knowledge
graph. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 36, 3968–3976.
Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and
Dahl, G. E. 2017.
Neural message passing for quantum
chemistry. In International conference on machine learn-
ing, 1263–1272. PMLR.
Gu, Y.; Tinn, R.; Cheng, H.; Lucas, M.; Usuyama, N.; Liu,
X.; Naumann, T.; Gao, J.; and Poon, H. 2021.
Domain-
specific language model pretraining for biomedical natural
language processing. ACM Transactions on Computing for
Healthcare (HEALTH), 3(1): 1–23.
Guo, Z.; Sharma, P.; Martinez, A.; Du, L.; and Abraham,
R. 2021. Multilingual molecular representation learning via
contrastive pre-training. arXiv preprint arXiv:2109.08830.
Hastings, J.; Owen, G.; Dekker, A.; Ennis, M.; Kale, N.;
Muthukrishnan, V.; Turner, S.; Swainston, N.; Mendes, P.;
and Steinbeck, C. 2016. ChEBI in 2016: Improved services
and an expanding collection of metabolites. Nucleic acids
research, 44(D1): D1214–D1219.
Hu, W.; Liu, B.; Gomes, J.; Zitnik, M.; Liang, P.; Pande,
V.; and Leskovec, J. 2019. Strategies for pre-training graph
neural networks. arXiv preprint arXiv:1905.12265.
Kim, H.; Lee, J.; Ahn, S.; and Lee, J. R. 2021. A merged
molecular representation learning for molecular properties
prediction with a web-based service.
Scientific Reports,
11(1): 11028.
Kim, W.; Son, B.; and Kim, I. 2021.
Vilt: Vision-and-
language transformer without convolution or region super-
vision. In International Conference on Machine Learning,
5583–5594. PMLR.
Li, L. H.; Yatskar, M.; Yin, D.; Hsieh, C.-J.; and Chang, K.-
W. 2019. Visualbert: A simple and performant baseline for
vision and language. arXiv preprint arXiv:1908.03557.
Liu, S.; Guo, H.; and Tang, J. 2022. Molecular geometry
pretraining with se (3)-invariant denoising distance match-
ing. arXiv preprint arXiv:2206.13602.
Liu, S.; Nie, W.; Wang, C.; Lu, J.; Qiao, Z.; Liu, L.; Tang, J.;
Xiao, C.; and Anandkumar, A. 2022. Multi-modal molecule
structure-text model for text-based retrieval and editing.
arXiv preprint arXiv:2212.10789.
Liu, S.; Wang, H.; Liu, W.; Lasenby, J.; Guo, H.; and Tang,
J. 2021. Pre-training molecular graph representation with 3d
geometry. arXiv preprint arXiv:2110.07728.
Liu, Z.; Zhang, W.; Xia, Y.; Wu, L.; Xie, S.; Qin, T.;
Zhang, M.; and Liu, T.-Y. 2023.
MolXPT: Wrapping
Molecules with Text for Generative Pre-training.
arXiv
preprint arXiv:2305.10688.
Lu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019.
Vilbert:
Pretraining task-agnostic visiolinguistic representations for
vision-and-language tasks. Advances in neural information
processing systems, 32.
Mamoshina, P.; Vieira, A.; Putin, E.; and Zhavoronkov, A.
2016. Applications of deep learning in biomedicine. Molec-
ular pharmaceutics, 13(5): 1445–1454.
O’Boyle, N. M.; and Sayle, R. A. 2016. Comparing struc-
tural fingerprints using a literature-based similarity bench-
mark. Journal of cheminformatics, 8(1): 1–14.
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
et al. 2021. Learning transferable visual models from nat-
ural language supervision. In International conference on
machine learning, 8748–8763. PMLR.
Rogers, D.; and Hahn, M. 2010. Extended-connectivity fin-
gerprints. Journal of chemical information and modeling,
50(5): 742–754.
Rong, Y.; Bian, Y.; Xu, T.; Xie, W.; Wei, Y.; Huang, W.;
and Huang, J. 2020. Self-supervised graph transformer on
large-scale molecular data. Advances in Neural Information
Processing Systems, 33: 12559–12571.
Ross, J.; Belgodere, B.; Chenthamarakshan, V.; Padhi, I.;
Mroueh, Y.; and Das, P. 2022. Large-scale chemical lan-
guage representations capture molecular structure and prop-
erties. Nature Machine Intelligence, 4(12): 1256–1264.
Seidl, P.; Vall, A.; Hochreiter, S.; and Klambauer, G. 2023.
Enhancing activity prediction models in drug discovery with
the ability to understand human language. arXiv preprint
arXiv:2303.03363.
Song, Y.; Zheng, S.; Niu, Z.; Fu, Z.-H.; Lu, Y.; and Yang,
Y. 2020. Communicative Representation Learning on At-
tributed Molecular Graphs. In IJCAI, volume 2020, 2831–
2838.
St¨ark, H.; Beaini, D.; Corso, G.; Tossou, P.; Dallago, C.;
G¨unnemann, S.; and Li`o, P. 2022. 3d infomax improves gnns
for molecular property prediction. In International Confer-
ence on Machine Learning, 20479–20502. PMLR.
Su, B.; Du, D.; Yang, Z.; Zhou, Y.; Li, J.; Rao, A.; Sun, H.;
Lu, Z.; and Wen, J.-R. 2022. A molecular multimodal foun-
dation model associating molecule graphs with natural lan-
guage. arXiv preprint arXiv:2209.05481.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
tention is all you need. Advances in neural information pro-
cessing systems, 30.
Wang, S.; Guo, Y.; Wang, Y.; Sun, H.; and Huang, J.
2019. Smiles-bert: large scale unsupervised pre-training for
molecular property prediction. In Proceedings of the 10th
ACM international conference on bioinformatics, computa-
tional biology and health informatics, 429–436.
Wang, W.; Bao, H.; Dong, L.; Bjorck, J.; Peng, Z.; Liu,
Q.; Aggarwal, K.; Mohammed, O. K.; Singhal, S.; Som, S.;
et al. 2023. Image as a Foreign Language: BEiT Pretraining
for Vision and Vision-Language Tasks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 19175–19186.
Wang, Y.; Wang, J.; Cao, Z.; and Barati Farimani, A. 2022.
Molecular contrastive learning of representations via graph
neural networks. Nature Machine Intelligence, 4(3): 279–
287.
Weininger, D. 1988. SMILES, a chemical language and in-
formation system. 1. Introduction to methodology and en-
coding rules. Journal of chemical information and computer
sciences, 28(1): 31–36.
Wiener, H. 1947. Structural determination of paraffin boil-
ing points. Journal of the American chemical society, 69(1):
17–20.
Wishart, D. S.; Feunang, Y. D.; Guo, A. C.; Lo, E. J.; Marcu,
A.; Grant, J. R.; Sajed, T.; Johnson, D.; Li, C.; Sayeeda, Z.;
et al. 2018. DrugBank 5.0: a major update to the DrugBank
database for 2018. Nucleic acids research, 46(D1): D1074–
D1082.
Wu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Ge-
niesse, C.; Pappu, A. S.; Leswing, K.; and Pande, V. 2018.
MoleculeNet: a benchmark for molecular machine learning.
Chemical science, 9(2): 513–530.
Xia, J.; Zhao, C.; Hu, B.; Gao, Z.; Tan, C.; Liu, Y.; Li, S.;
and Li, S. Z. 2022. Mole-bert: Rethinking pre-training graph
neural networks for molecules.
In The Eleventh Interna-
tional Conference on Learning Representations.
Xia, J.; Zhu, Y.; Du, Y.; Liu, Y.; and Li, S. 2023. A System-
atic Survey of Chemical Pre-trained Models. IJCAI.
Yang, K.; Swanson, K.; Jin, W.; Coley, C.; Eiden, P.; Gao,
H.; Guzman-Perez, A.; Hopper, T.; Kelley, B.; Mathea, M.;
et al. 2019. Analyzing learned molecular representations for
property prediction. Journal of chemical information and
modeling, 59(8): 3370–3388.
Yasunaga, M.; Leskovec, J.; and Liang, P. 2022. Linkbert:
Pretraining language models with document links.
arXiv
preprint arXiv:2203.15827.
You, Y.; Chen, T.; Sui, Y.; Chen, T.; Wang, Z.; and Shen, Y.
2020. Graph contrastive learning with augmentations. Ad-
vances in neural information processing systems, 33: 5812–
5823.
Yu, J.; Wang, Z.; Vasudevan, V.; Yeung, L.; Seyedhos-
seini, M.; and Wu, Y. 2022.
Coca: Contrastive caption-
ers are image-text foundation models.
arXiv preprint
arXiv:2205.01917.
Zaidi, S.; Schaarschmidt, M.; Martens, J.; Kim, H.; Teh,
Y. W.; Sanchez-Gonzalez, A.; Battaglia, P.; Pascanu, R.; and
Godwin, J. 2022. Pre-training via denoising for molecular
property prediction. arXiv preprint arXiv:2206.00133.
Zeng, Z.; Yao, Y.; Liu, Z.; and Sun, M. 2022.
A deep-
learning system bridging molecule structure and biomedical
text with comprehension comparable to human profession-
als. Nature communications, 13(1): 862.
Zeng, Z.; Yin, B.; Wang, S.; Liu, J.; Yang, C.; Yao, H.;
Sun, X.; Sun, M.; Xie, G.; and Liu, Z. 2023. Interactive
Molecular Discovery with Natural Language. arXiv preprint
arXiv:2306.11976.
Zhao, H.; Liu, S.; Ma, C.; Xu, H.; Fu, J.; Deng, Z.-H.; Kong,
L.; and Liu, Q. 2023.
GIMLET: A Unified Graph-Text
Model for Instruction-Based Molecule Zero-Shot Learning.
bioRxiv, 2023–05.
Zhou, G.; Gao, Z.; Ding, Q.; Zheng, H.; Xu, H.; Wei, Z.;
Zhang, L.; and Ke, G. 2023. Uni-Mol: a universal 3D molec-
ular representation learning framework.
","nanMolecular pretraining models can be broadly categorized into sequence-based, graph-based, and models that incorporate external knowledge. Sequence-based models represent molecules as SMILES strings or other sequences and utilize language models as backbones for pretraining tasks like Masked Language Modeling (MLM). On the other hand, graph-based models represent molecules as graphs and employ graph neural networks for pretraining. Additionally, newer approaches seek to enhance molecular representations by incorporating knowledge graphs or textual information."
We show that data preprocessing using a variant of rank transformation called ‘Average Rank over an Ensemble of Sub-samples (ARES)’ makes clustering algorithms robust to data representation. Clustering after ARES transformation produces better and consistent results.,"Clustering is an unsupervised data mining task of partitioning data objects into groups, referred to as ‘clusters’, such that objects within a group are similar to each other and dissimilar to objects in other groups. It has applications in various domains. Automatic clustering of real-world data is challenging because data can be clustered in several ways, have complex structure with arbitrary shapes and varying densities, and no prior knowledge about the number and characteristics (e.g., shapes) of clusters is available.","We demonstrate that preprocessing data using a variant of rank transformation called ‘Average Rank over an Ensemble of Sub-samples (ARES)’ makes clustering algorithms robust to data representation. ARES transforms data in each feature or dimension by aggregating ranks of data points in multiple subsamples of data. Unlike the traditional rank transformation, which may mask existing clusters, aggregating ranks from multiple subsamples helps preserve clusters and remains robust to data representation.","We evaluated three most popular clustering algorithms, KMeans, DBSCAN, and DP (Density Peak), on seven real-world datasets. Results show that clustering after ARES preprocessing produces better and more consistent results. ARES consistently outperformed min-max normalisation and rank transformation. ARES transformation notably improved DP’s performance in five datasets. DP consistently outperformed DBSCAN and KMeans across all three preprocessing approaches.","ARES is a simple and effective approach to enhancing the robustness of clustering algorithms to changes in data representation. It ensures good, reliable, and robust clustering results. As future work, we plan to evaluate other clustering algorithms with and without ARES transformation and explore its applicability in other machine learning problems.",Enabling clustering algorithms to detect clusters of varying densities through scale-invariant data preprocessing,"Sunil Aryal, Jonathan R. Wells, Arbind Agrahari Baniya, KC Santosh","Enabling clustering algorithms to detect clusters of varying
densities through scale-invariant data preprocessing
Sunil Aryal
Deakin University
Geelong, VIC, Australia
sunil.aryal@deakin.edu.au
Jonathan R. Wells
Deakin University
Geelong, VIC, Australia
j.wells@deakin.edu.au
Arbind Agrahari Baniya
Deakin University
Geelong, VIC, Australia
a.agraharibaniya@deakin.edu.au
KC Santosh
University of South Dakota
Vermillion, South Dakota, USA
Santosh.KC@usd.edu
ABSTRACT
In this paper, we show that preprocessing data using a variant of
rank transformation called ‘Average Rank over an Ensemble of
Sub-samples (ARES)’ makes clustering algorithms robust to data
representation and enable them to detect varying density clusters.
Our empirical results, obtained using three most widely used clus-
tering algorithmsâĂŤnamely KMeans, DBSCAN, and DP (Density
Peak)âĂŤacross a wide range of real-world datasets, show that
clustering after ARES transformation produces better and more
consistent results.
Keywords: Data Clustering, KMeans, DBSCAN, Density Peak,
Varying Density Clustering, Units/Scales of Measurement, Data
Preprocessing, Rank Transformation
ACM Reference Format:
Sunil Aryal, Jonathan R. Wells, Arbind Agrahari Baniya, and KC San-
tosh. 2024. Enabling clustering algorithms to detect clusters of varying
densities through scale-invariant data preprocessing. In Proceedings of
ACM Conference (Conference’17). ACM, New York, NY, USA, 5 pages. https:
//doi.org/10.1145/nnnnnnn.nnnnnnn
1
INTRODUCTION
Clustering is an unsupervised data mining task of partitioning a set
of data objects into multiple groups, referred to as ‘clusters’, such
that objects within a group are similar to each other and dissimilar
to objects in other groups [1]. It finds applications in various do-
mains, such as customer/market segmentation in marketing and
sales, grouping homologous gene sequences in biology, and detect-
ing communities in social networks. The automatic clustering of
real-world data is a challenging task because [1]: (i) data can be
clustered in several ways based on different perspectives, views or
purposes; (ii) real-world data have complex structure with clusters
of arbitrary shapes and varying densities; and (iii) in most cases, no
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnn
prior knowledge about the number and characteristics (e.g., shapes)
of clusters is available.
Different types of clustering algorithms have been proposed in
the literature. However, there is no single algorithm that is univer-
sally effective in detecting various types of clusters across different
application domains. Almost all clustering algorithms rely on a
notion of (dis)similarity of data. The pairwise (dis)similarity of data
objects is estimated using the values of features (also known as
attributes) that define these data objects. Since most existing cluster-
ing algorithms operate on feature values, they are sensitive to how
data features are representedâĂŤi.e., their performance may vary
significantly if the same data is represented or expressed differently.
As discussed in [2–5], in real-world applications, features of data
objects can be captured or collected from various sources/sensors
and measured/represented in different forms. For instance, tem-
perature can be measured in ◦𝐶 or ◦𝐹, while sample variability
may be expressed as standard deviation (𝜎) or variance (𝜎2). Simi-
larly, data can be provided in different scales, such as log likelihood
instead of likelihood, or credit risk as income-to-debt ratio or debt-
to-income ratio. In the era of internet-of-things, such variation in
data representation arises due to: (a) settings of sensors/devices
used to measure data; (b) domain/system requirements; and/or (c)
data compression to reduce storage and/or transmission costs.
Different representation of data can be linear or non-linear scal-
ing of each other. Linear scaling poses no problem for pattern
recognition, as it does not alter the data distribution. However,
non-linear scaling can be problematic, as it can change the distri-
bution of data significantly, thereby providing entirely different
insights. It may affect the number, shape, and densities of clusters,
making them challenging to detect accurately using existing al-
gorithms. For instance, Fig 1a-c show the distributions of a set of
one-dimensional data points (𝑋) and its logarithmic (log(𝑋)) and
inverse (𝑋 −1) scalings. Clearly, Fig 1a has three clusters, with the
right most cluster having a lower density compared to the two
clusters on the left. However, it changes significantly in Fig 1b-c,
especially in the case of inverse scaling. Many existing algorithms
yield different clustering results for these three representations of
the same data. Moreover, some clustering algorithms, particularly
those based on density, fail to correctly detect three clusters even
in the case of Fig 1a because there is no single density threshold 𝛿
to find the three density peaks. They either miss the right cluster
with low density (with 𝛿 > 1.2) or merge the two high density
arXiv:2401.11402v1  [cs.LG]  21 Jan 2024
(a) 𝑋
(b) log(𝑋 )
(c) 𝑋 −1
(d) Rank(𝑋)
(e) Rank(log(𝑋 ))
(f) Rank(𝑋 −1)
(g) ARES(𝑋)
(h) ARES(log(𝑋 ))
(i) ARES(𝑋 −1)
Figure 1: Comparison of data distributions: The first row shows the distributions of an example dataset, along with its
logarithmic and inverse scaling. The second and third rows depict the corresponding distributions after the Rank and proposed
ARES transformations, respectively. In all cases, data are normalised to be in the range of [0, 1] before modelling the density
distribution.
clusters on the left into one cluster (if 𝛿 < 1.3). The issue of varying
density is closely related to the problem of data representation, as
non-linear scaling of data alters the density distribution of data.
When given data for clustering, only feature values (numbers)
are often provided, and the units/scales in which the data is recorded
may remain unknown and unavailable. Even if available, we may
not know whether the given representation of data is suitable to de-
tect clusters. We may get misleading insights from data if the given
representation is not appropriate. In real-world data clustering,
where true clusters are unknown, trying different transformations
to find the one yielding the best clustering results is not possible.
Even if we can do it, computationally it is not feasible as there
are many possible transformations for each attribute resulting in
a huge combination to try, particularly in high-dimensional prob-
lems. One simple solution is to transform the data in each features
into rank (or percentile) space, and using their ranks/percentiles
instead of actual data values. Ranks/percentiles are either preserved
or reversed even in non-linear scaling of data [2]. However, this
approach may not be good for clustering, as it tends to make the
distribution uniform (see Fig 1d-f) because the rank difference be-
tween two consecutive values is always 1 regardless of their value
differences. It potentially cause some clusters to disappear.
In this paper, we demonstrate that preprocessing data using
a variant of rank transformation called ‘Average Rank over an
Ensemble of Sub-samples (ARES)’ makes clustering robust to data
representation and enabling them to detect clusters with varying
densities effectively. The idea is to use rank transformation in multi-
ple subsamples of data and aggregating the ranks. Unlike using rank
transformation in the entire dataset, which may mask existing clus-
ters, aggregating ranks form multiple subsamples helps preserve
clusters to some extent and remains robust to data representation
(see Fig 1g-i; note that distribution of 𝑋 −1 is the mirror image of
those of 𝑋 and log(𝑋) as the ordering of data is reversed). Our
empirical results of three most widely used clustering algorithm-
sâĂŤKMeans [6], DBSCAN [7], and DP (Density Peak) [8]âĂŤon
real-world datasets, demonstrate that clustering after ARES trans-
formation produces better and more consistent results.
2
ARES TRANSFORMATION
To address the issue of rank transformation resulting in uniform
data distribution, we propose a new variant of rank transformation
using an ensemble approach. In each feature or dimension 𝑖, instead
of computing rank of 𝑥𝑖 among all𝑛 values, we propose to aggregate
ranks of 𝑥𝑖 in𝑡 sub-samples of values in dimension𝑖. We refer to this
(a) A given dataset (𝐷)
(b) R𝑗 (𝑗 = 1, 2, · · · ,𝑡 )
Figure 2: An illustration of ARES: (a) an example of a
given dataset 𝐷; (b) an ensemble of ranking models R𝑗 (𝑗 =
1, 2, · · · ,𝑡) from sub-samples 𝐷𝑗 ⊂ 𝐷 and |𝐷𝑗 | = 𝜓 = 5. Each
R𝑗 is constructed from 𝐷𝑗 by partitioning the real domain
into (𝜓 + 1) bins which are then ranked as (0, 1, · · · ,𝜓) from
left to right. The ARES transformation of a query point 𝑥 on
red is computed by aggregating its rank in each R𝑗.
proposed method as the ARES (Average Rank over an Ensemble of
Sub-samples) transformation.
To simplify the explanation, we assume that 𝐷 is a one-dimensional
dataset where |𝐷| = 𝑛. We utilise 𝑡 sub-samples 𝐷𝑗 ⊂ 𝐷 (𝑗 =
1, 2, · · · ,𝑡), where |𝐷𝑗 | = 𝜓 ≪ 𝑛. The transformed value of 𝑥, de-
noted as ˜𝑥𝐴𝑅𝐸𝑆, is the average rank of 𝑥 across all 𝑡 sub-samples.
˜𝑥𝐴𝑅𝐸𝑆 = 1
𝑡
𝑡∑︁
𝑗=1
𝑟 (𝑥|𝐷𝑗)
(1)
where 𝑟 (𝑥|𝐷𝑗) represents the rank of 𝑥 in 𝐷𝑗:
𝑟 (𝑥|𝐷𝑗) = |{𝑦 ∈ 𝐷𝑗 : 𝑦 < 𝑥}|
(2)
Based on the definition of 𝑟 (𝑥|𝐷𝑗) given by Eqn 2, all 𝑥 ∈ 𝐷 have
ranks in the range {0, 1, · · · ,𝜓} in 𝐷𝑗. If 𝑠 (1)
𝑗
,𝑠 (2)
𝑗
, · · · ,𝑠 (𝜓 )
𝑗
are the
sorted samples in 𝐷𝑗:
𝑟 (𝑥|𝐷𝑗) =


0
if 𝑥 < 𝑠 (1)
𝑗
𝑘
if 𝑠 (𝑘)
𝑗
≤ 𝑥 < 𝑠 (𝑘+1)
𝑗
and 1 ≤ 𝑘 ≤ 𝜓 − 1
𝜓
if 𝑥 ≥ 𝑠 (𝜓 )
𝑗
(3)
For all 𝑥 ∈ 𝐷 such that 𝑠 (𝑘)
𝑗
≤ 𝑥 < 𝑠 (𝑘+1)
𝑗
, they will receive the
same 𝑟 (𝑥|𝐷𝑗) of 𝑘, regardless of the their differences in magnitudes.
Consequently, they cannot be differentiated. However, it is impor-
tant to note they may have different ranks in other sub-samples.
Therefore, by averaging over various sub-samples, we can main-
tain their differences to some extent. For instance, even the case of
|𝐷𝑗 | = 𝜓 = 1 where all 𝑥 ∈ 𝐷 have ranks either 0 or 1, depending
upon whether they lie to the left or right of the sample selected in
𝐷𝑗, repeating this process multiple times (say 𝑡 = 10) preserves the
differences between data points to some extent.
Fig 1d-i show the distribution of an example dataset and its
transformations using traditional rank and ARES. It is evident that
Table 1: Data sets
Datasets
#Instances
#Attributes
#Clusters
Letters
20,000
16
26
Pendigits
10,992
16
10
Spambase
4,601
57
2
SatImage
4,435
36
6
Segment
2,310
16
7
Hba
1,500
187
15
Gtzan
1,000
230
10
ARES better preserves the differences in data values in the original
distribution compared to the rank transformation using the entire
𝐷. Like traditional rank transformation, ARES is robust to changes
in scales and units of measurement. ARES, being a variant of rank
transformation using sub-samples in an ensemble, it results in the
same transformed distribution regardless of the original scaling
of data, as shown in Fig 1g-i. It is worth noting that the resulting
distribution in the case of inverse scale (Fig 1i) is the reverse of
that in the original and logarithmic scales (Fig 1g-h). There is some
small differences in the ARES transformations of samples selected
in 𝐷𝑗 (i.e., 𝑥 = 𝑠 (𝑘)
𝑗
) in the case of inverse scale because of the ‘<’
sign in Eqn 2.
In terms of run-time, generating and sorting 𝜓 samples 𝑡 times
in ARES requires 𝑂(𝑡𝜓 log𝜓) time, and computing ranks for all
𝑛 instances in 𝐷 takes 𝑂(𝑛𝑡 log𝜓) time. On the other hand, the
traditional rank transformation involves 𝑂(𝑛 log𝑛) time for sorting
and ranking the 𝑛 instances in 𝐷.
3
EMPIRICAL EVALUATION
In this section, we present the clustering results obtained from three
most popular clustering algorithms: KMeans [6], DBSCAN [7], and
Density Peak (DP) [8]. We evaluated clustering performances in
terms of the quality of clusters produced and robustness to the
change in data representation.
3.1
Experimental setup
We used seven widely-used real-world datasets to evaluate the
clustering results. The characteristics of these datasets are provided
in Table 1.
We used JAVA implementations of KMeans and DBSCAN avail-
able in the WEKA platform [9]. We implemented the proposed
ARES preprocessing and DP algorithm also in the WEKA platform.
There are a number of parameters in these algorithms that need to
be set properly for optimal clustering results. We experimented with
parameter settings suggested in the respective papers and reported
the best results. The number of clusters in DP and KMeans was set
to the true number of clusters in the datasets. The parameter 𝜖 for
DBSCAN and DP was searched in the range of [0.01 to 0.5 with a
step size of 0.01] and 𝑚𝑖𝑛𝑃𝑡𝑠 for DBSCAN in [4, 5, 6, 7, 8]. Similarly,
two parameters in ARES were searched as 𝜓 ∈ {2𝑖 |𝑖 = 0, 1, · · · , 5}
and 𝑡 ∈ {10, 25, 50, 100}. We used F1-measure as the metric to eval-
uate cluster results [10].
To use existing clustering algorithms, data must be normalised
to be in the same range in each dimension. We employed the widely
Table 2: Clustering results in terms of F1-measure. Best result on each dataset for each clustering algorithm is bold-faced.
DBSCAN
DP
KMeans
Datasets
min-max
ARES
Rank
min-max
ARES
Rank
min-max
ARES
Rank
Letters
0.2902
0.3271
0.2868
0.2923
0.3760
0.3310
0.2778
0.2905
0.2948
Pendigits
0.6904
0.6705
0.6860
0.7763
0.7256
0.6992
0.6477
0.6933
0.6346
Spambase
0.3782
0.4590
0.4408
0.6574
0.8277
0.5692
0.3965
0.8195
0.8256
SatImage
0.4490
0.4497
0.3716
0.6202
0.7235
0.6816
0.6431
0.6519
0.6509
Segment
0.6702
0.6556
0.6284
0.7619
0.7367
0.6557
0.5735
0.5715
0.5472
Hba
0.0470
0.1069
0.0866
0.2124
0.2444
0.1939
0.3246
0.3809
0.3700
Gtzan
0.0907
0.1021
0.0956
0.2479
0.2856
0.2597
0.3291
0.3953
0.3811
used min-max normalisation technique to ensure data in all dimen-
sions fall within the range of [0,1]. It is interesting to note that
such normalisation is not required if ARES or Rank transforma-
tion of data is done. In fact, these methods serve as alternatives
to traditional preprocessing techniques such as normalisation and
standardisation. Unlike normalisation and standardisation, which
are sensitive to changes in data representation, ARES and Rank
transformations are robust to such changes.
To assess the robustness of clustering algorithms, we trans-
formed the given data values of each feature 𝑥 using non-linear
scaling methods, including 𝑥2, √𝑥, log𝑥 and 𝑥 −1. Given that the
transformations log𝑥 and 1/𝑥 are undefined for 𝑥 = 0, we applied
the transformations to 𝑐(𝑥 + 𝛼), where 𝛼 = 0.0001 and 𝑐 = 100, as
done in [4, 5]. Subsequently, the scaled data was normalised within
the range of [0,1].
3.2
Results in the given representation of data
The clustering results of DBSCAN, DP and KMeans on the given
datasets using the default representation of 𝑥 with min-max, rank,
and ARES preprocessing are presented in Table 2. Our results can
be summarised as follows:
• The ARES transformation consistently outperformed min-
max normalisation and rank transformation across all three
clustering algorithms. ARES produced the best results in five
datasets for each algorithm. In contrast, with the commonly
used min-max normalisation, DBSCAN and DP achieved the
best results in two datasets, and KMeans yielded the best
result in only one dataset. Rank transformation, on the other
hand, resulted in the best outcome for KMeans in the Letters
dataset only.
• DP consistently outperformed DBSCAN and KMeans across
all three preprocessing approaches, aligning with its reputa-
tion as a state-of-the-art clustering algorithm. ARES transfor-
mation notably improved DP’s performance in five datasets,
including significant enhancements in cases like Spambase
and SatImage. However, there was a slight decrease in per-
formance in two datasets (Pendigits and Segment).
• Rank transformation generally produced worse results than
ARES, except with KMeans in Letters, where it produced
marginally better results than ARES. Notably, Rank trans-
formation surprisingly produced competitive results com-
parable to min-max normalisation. This results suggest that
Table 3: F1-measure of DP clustering with different non-
linear scalings of data. Best result in each dataset is bold-
faced.
Datasets
𝑥
𝑥2
√𝑥
log𝑥
𝑥−1
Letters
0.2923
0.2933
0.3054
0.2113
0.0819
Pendigits
0.7763
0.6698
0.6883
0.5469
0.4220
Spambase
0.6574
0.3857
0.7615
0.7223
0.7826
SatImage
0.6202
0.5018
0.5581
0.3888
0.0706
Segment
0.7619
0.8065
0.7289
0.5515
0.1694
Hba
0.2124
0.2037
0.2203
0.1547
0.0241
Gtzan
0.2479
0.2901
0.2307
0.3026
0.0322
while Rank transformation may mask the variation in den-
sity distribution in each dimension (see Fig 1), it manages to
preserve it to some extent in the multidimensional space.
3.3
Results with different representations of
data
To analyse the robustness of the clustering algorithms to changes
in data representation, we applied them after non-liner scaling
of data with 𝑥2, √𝑥, log𝑥 and 𝑥 −1. The clustering results of all
three algorithms with min-max normalisation exhibited variations
with the changes in data representation. In contrast, ARES and
Rank transformation of data consistently produced the same results
across all these representations, as shown in Table 2, with very
small differences in the case of 𝑥 −1 scaling in some cases. This
behaviour aligns with the discussion presented earlier in Section 2.
The results of the DP clustering algorithm, which performed the
best among the three algorithms, with different representations of
real-world datasets are presented in Table 3. Similar trends were
observed with DBSCAN and KMeans. It is interesting to note that
the given representation of data produced the best results in only
two datasets, and no presentation yielded the best results in more
than two datasets when compared to ARES (Table 2). These findings
suggest that the default data representation may not be optimal
for achieving the best clustering results. Therefore, applying ARES
preprocessing is a preferred and reliable option.
Visual examples of DP in the two-dimensional synthetic dataset
called Jain in three representations of 𝑥, log𝑥 and 𝑥 −1, with and
without ARES transformations, are shown in Fig 3. DP without
(a) DP with 𝑥
(b) DP with log𝑥
(c) DP with 𝑥 −1
(d) DP with ARES(𝑥)
(e) DP with ARES(log𝑥)
(f) DP with ARES(𝑥 −1)
Figure 3: Clustering results of the Jain dataset with𝑥, log𝑥, and𝑥 −1 scaling using DP, both with and without ARES transformation.
Note that in the case of DP with 𝑥 −1 (Subfig c), only a portion of the plot is shown to provide a clearer view of most points.
Due to some extreme values, displaying all points in this section would result in grouping most of them into a single point.
Additionally, another small cluster was detected in the unseen part of the plot.
ARES transformation resulted F1-measure of 1.0000, 0.8607 and
0.4241 with 𝑥, log𝑥 and 𝑥 −1 representations, respectively. On the
other hand, DP with ARES transformation consistently produced
perfect clustering results, achieving an F1-measure of 1.0000 in all
three cases.
4
CONCLUSIONS AND FUTURE WORK
In real-world problems, data are collected from various sources
and sensors, which can be represented in different forms/formats.
The given representation of data may not be appropriate for ob-
taining a good clustering result using existing algorithms, as their
performances are sensitive to data representation. They can give
misleading clustering results. It is not practical to try different
representations to see which one gives the best result because:
(i) there can be infinitely many possible combinations to try in
high-dimensional problems; and (ii) there may not be ground truth
clustering result available to compare the produced clustering re-
sults. Therefore, it is preferred to use algorithms that are robust to
data representations (units/scales of measurement).
In this paper, we proposed one simple approach to enhance
robustness of clustering algorithms to the change in data repre-
sentations. We propose a robust data processing technique called
ARES that results in the same transformed distribution regardless
of the input data representations. Our results demonstrate that ap-
plying existing clustering algorithms after preprocessing data with
ARES transformation yields in better, reliable and robust clustering
results.
In future work, we plan to evaluate performance of other cluster-
ing algorithms with and without ARES transformation. Addition-
ally, exploring the applicability of ARES in other machine learning
problems is an interesting avenue for further investigation.
ACKNOWLEDGEMENT
This research is funded by the US Air Force Office of Scientific
Research (AFOSR) under grant number FA2386-20-1-4005.
REFERENCES
[1] Jiawei Han, Micheline Kamber, and Jian Pei. Data Mining: Concepts and Techniques.
Morgan Kaufmann Publishers, third edition, 2011.
[2] Thilak L. Fernando and Geoffrey I. Webb. SimUSF: an efficient and effective
similarity measure that is invariant to violations of the interval scale assumption.
Data Mining and Knowledge Discovery, 31(1):264–286, 2017.
[3] Sunil Aryal. Anomaly detection technique robust to units and scales of measure-
ment. In Proceedings of the 22nd Pacific-Asia Conference on Knowledge Discovery
and Data Mining, pages 589–601, 2018.
[4] Sunil Aryal, Kai Ming Ting, Takashi Washio, and Gholamreza Haffari. A com-
parative study of data-dependent approaches without learning in measuring
similarities of data objects. Data mining and knowledge discovery, 34(1):124–162,
2020.
[5] Sunil Aryal, K.C. Santosh, and Richard Dazeley. usfAD: a robust anomaly detector
based on unsupervised stochastic forest. International Journal of Machine Learning
and Cybernetics, 12:1137–1150, 2021.
[6] J. Macqueen. Some methods for classification and analysis of multivariate obser-
vations. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics
and Probability, pages 281–297, 1967.
[7] Martin Ester, Hans-Peter Kriegel, Joerg Sander, and Xiaowei Xu. A density-
based algorithm for discovering clusters in large spatial databases with noise. In
Proceedings of the ACM SIGKDD, pages 226–231, 1996.
[8] Alex Rodriguez and Alessandro Laio. Clustering by fast search and find of density
peaks. science, 344(6191):1492–1496, 2014.
[9] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann,
and Ian H. Witten. The weka data mining software: An update. SIGKDD Explo-
ration Newsletter, 11(1):10–18, 2009.
[10] Elke Achtert, Sascha Goldhofer, Hans-Peter Kriegel, Erich Schubert, and Arthur
Zimek. Evaluation of clusterings – metrics and visual support. In Proceedings of
the 2012 IEEE 28th International Conference on Data Engineering, pages 1285–1288,
2012.
","nanDifferent types of clustering algorithms have been proposed in the literature, but no single algorithm is universally effective in detecting various types of clusters. Clustering algorithms rely on a notion of (dis)similarity of data, which is estimated using values of features. Performance of these algorithms may vary if the same data is expressed differently. In real-world applications, data features are collected from various sources and represented/measured in different forms."
"Multi-modal Large Language Models (MLLMs) have a significant impact on various tasks, due to their extensive knowledge and powerful perception and generation capabilities. However, it still remains an open research problem on applying MLLMs to low-level vision tasks. In this paper, we present a simple MLLM-based Image Restoration framework to address this gap, namely Multimodal Large Language Model based Restoration Assistant (LLMRA). We exploit the impressive capabilities of MLLMs to obtain the degradation information for universal image restoration. By employing a pretrained multi-modal large language model and a vision language model, we generate text descriptions and encode them as context embedding with degradation information for the degraded image. Through the proposed Context Enhance Module (CEM) and Degradation Context based Transformer Network (DC-former), we integrate these context embedding into the restoration network, contributing to more accurate and adjustable image restoration. Based on the dialogue with the users, our method leverages image degradation priors from MLLMs, providing low-level attributes descriptions of the input low-quality images and the restored high-quality images simultaneously.","Recently, Multi-modal Large Language Models (MLLMs), such as LLaVA [Liu et al., 2023], MiniGPT-4 [Zhu et al., 2023], and InstructBLIP [Dai et al., 2023], have garnered significant attention. Building upon the remarkable comprehension and reasoning capabilities of LLMs, MLLMs have transcended beyond the boundaries of textual inputs, harnessing their remarkable power in various domains. However, the current exploration of MLLMs has primarily focused on high-level perception and understanding of images. The application of MLLMs only emerges in a limited range of vision-language tasks, such as image captioning [Chen et al., 2015], visual question answering [Antol et al., 2015], and conventional computer vision tasks like segmentation [Lai et al., 2023] and text-to-image generation [Xia et al., 2023a]. Recently, a benchmark called Q-bench [Wu et al., 2023] can evaluate the performance of MLLMs in low-level vision tasks, specifically in perceiving and describing low-level quality-related information using natural language. The results demonstrate that MLLMs exhibit a notable perceptual ability towards low-level visual attributes. Image restoration is a fundamental task in the field of low-level vision, with the primary objective of recovering high-quality images from degraded counterparts. This task encompasses a diverse range of subtasks, including but not limited to image denoising, deblurring, deraining, and low-light enhancement.","In this paper, we combine large-scale pretrained multi-modal large language model with image restoration networks and introduce an effective framework for universal image restoration.
We refer to this novel framework as MLLM based image Restoration Assistant (LLMRA). Specifically, we utilize IDEFICS [Huggingface, 2023], an open-source multi-modal language model based on Flamingo [Alayrac et al., 2022], to generate textual descriptions of the input degraded images. The text encoder of CLIP [Radford et al., 2021] (a large-scale pretrained vision-language model) is employed to encode the text descriptions into text features.",Our extensive experiments demonstrate the superiority of our LLMRA in universal image restoration tasks.,"This paper introduces LLMRA, a novel framework that leverages multi-modal large language models for universal image restoration. The core contribution of our framework is utilizing the MLLM and a text-guided restoration network to realize a more accurate, adjustable and interactive restoration manner. The Context Enhance Module and the Degradation Context based Transformer Network are proposed to effectively enhance the degradation information and incorporate it into the restoration network. Experimental evaluation on unified image restoration tasks demonstrates that LLMRA leads to significant performance on image denoising, image deraining, and low light image enhancement. Nevertheless, it is important to acknowledge some limitations of the proposed LLMRA. The performance of LLMRA may fluctuate with the performance of MLLM, as it may provide uninformative or even harmful answers of the degradation information, thus affecting the quality of restoration. Fortunately, users can <refine>the results by engaging in subsequent dialogue. Moreever, our experiments are currently limited to only three tasks. Although these three tasks are representative to some extent, as they encompass both additive and multiplicative degradation.",LLMRA: Multi-modal Large Language Model based Restoration Assistant,"Xiaoyu Jin, Yuan Shi, Bin Xia, Wenming Yang","LLMRA: Multi-modal Large Language Model based Restoration Assistant
Xiaoyu Jin 1, Yuan shi1, Bin Xia 2, Wenming Yang1
1 Tsinghua University, 2 The Chinese University of Hong Kong
Abstract
Multi-modal Large Language Models (MLLMs)
have a significant impact on various tasks, due
to their extensive knowledge and powerful per-
ception and generation capabilities.
However, it
still remains an open research problem on apply-
ing MLLMs to low-level vision tasks. In this paper,
we present a simple MLLM-based Image Restora-
tion framework to address this gap, namely Multi-
modal Large Language Model based Restoration
Assistant (LLMRA). We exploit the impressive ca-
pabilities of MLLMs to obtain the degradation in-
formation for universal image restoration. By em-
ploying a pretrained multi-modal large language
model and a vision language model, we gener-
ate text descriptions and encode them as context
embedding with degradation information for the
degraded image.
Through the proposed Context
Enhance Module (CEM) and Degradation Context
based Transformer Network (DC-former), we in-
tegrate these context embedding into the restora-
tion network, contributing to more accurate and ad-
justable image restoration. Based on the dialogue
with the users, our method leverages image degra-
dation priors from MLLMs, providing low-level at-
tributes descriptions of the input low-quality im-
ages and the restored high-quality images simul-
taneously. Extensive experiments demonstrate the
superior performance of our LLMRA in universal
image restoration tasks.
1
Introduction
Recently, Multi-modal Large Language Models (MLLMs),
such as LLaVA [Liu et al., 2023], MiniGPT-4 [Zhu et al.,
2023], and InstructBLIP [Dai et al., 2023], have garnered sig-
nificant attention. Building upon the remarkable comprehen-
sion and reasoning capabilities of LLMs, MLLMs have tran-
scended beyond the boundaries of textual inputs, harnessing
their remarkable power in various domains.
However, the current exploration of MLLMs has primarily
focused on high-level perception and understanding of im-
ages.
The application of MLLMs only emerges in a lim-
ited range of vision-language tasks, such as image caption-
ing [Chen et al., 2015], visual question answering [Antol et
al., 2015], and conventional computer vision tasks like seg-
mentation [Lai et al., 2023] and text-to-image generation [Xia
et al., 2023a]. Recently, a benchmark called Q-bench [Wu et
al., 2023] can evaluate the performance of MLLMs in low-
level vision tasks, specifically in perceiving and describing
low-level quality-related information using natural language.
The results demonstrate that MLLMs exhibit a notable per-
ceptual ability towards low-level visual attributes.
Image restoration is a fundamental task in the field of low-
level vision, with the primary objective of recovering high-
quality images from degraded counterparts. This task encom-
passes a diverse range of subtasks, including but not limited
to image denoising, deblurring, deraining, and low-light en-
hancement. Presently, the existing methods predominantly
concentrate on addressing specific types of image degrada-
tion, and are trained on datasets featuring only a single degra-
dation, thereby imposing limitations on their ability to effec-
tively restore other forms of degradation. In recent times,
there has been a notable surge of interest in the task of uni-
fied image restoration. Researchers are challenged to develop
a single model capable of handling images with diverse types
of degradation. Several approaches have been proposed to
tackle this challenge, employing techniques like degradation
encoder, contrastive learning [Li et al., 2022], and prompt
learning [Potlapalli et al., 2023] to achieve promising re-
sults. Some approaches also leverage visual language models
(VLMs) to handle a wide range of degradations [Luo et al.,
2023]. However, when it comes to complex real-world degra-
dations, the processing and storage capabilities of these en-
coders and VLMs are still limited. In particular, these meth-
ods can only directly restore images and cannot accept other
instructions for restoration or optimization, which limits the
application scenarios.
In this paper, we combine large-scale pretrained multi-
modal large language model with image restoration networks
and introduce an effective framework for universal image
restoration.
We refer to this novel framework as MLLM
based image Restoration Assistant (LLMRA). Specifically,
we utilize IDEFICS [Huggingface, 2023], an open-source
multi-modal language model based on Flamingo [Alayrac
et al., 2022], to generate textual descriptions of the input
degraded images.
The text encoder of CLIP [Radford et
al., 2021] (a large-scale pretrained vision-language model)
arXiv:2401.11401v1  [cs.CV]  21 Jan 2024
Now I will show you an image, are you ready to answer 
somes questions  about it?
I'm ready!
Describe the quality and low-level appearance of the 
image.
The image is a blurry and pixelated depiction of a sunset 
over the ocean.
Describe the lighting, if there are areas in the image that 
are dark; Describe if there is rain streaks degradation in 
this image; Describe if this image have gaussian noise 
degradation and the noise level.  
The sky is dark. Yes, there are rain streaks degradation. 
Yes, the image has gaussian noise degradation and the 
noise level is high.
I still think this image is dark, what is your opinion? 
<refine>  
I think the image is dark. 
Here is the refined image. 
I think this image is too bright this time, what is your 
opinion? <refine> 
I agree, the image is too bright.
Here is the refined image. 
Now I am satisfied with this image.
Great! I'm glad I could help.
<restore>  (restore the image automatically )
No problem, here is the restored image.
Figure 1: Example of the proposed LLMRA for universal image restoration. Based on the input image and the text input asking for the low-
level attributes of the image, our method is capable of providing corresponding descriptions. Upon the <restore >instruction, our LLMRA
leverages the degradation descriptions from the MLLM automatically to restore the image. On the other hand, when instructed with the
<refine>command, LLMRA performs image restoration based on the content of the dialogue.
is employed to encode the text descriptions into text features.
Using a Context Refine Module (CRM) and Context trans-
former, these degradation aware text features are enhanced.
Finally, we incorporate them into the Degradation Context
based Transformer Network (DC-former) through a Degra-
dation Modulation Module. By effectively utilizing the im-
age degradation priors obtained from the MLLMs, this frame-
work enables the restoration network to achieve more accu-
rate and adjustable image restoration. Our main contributions
are summarised as follows:
• We propose a multi-modal large language model based
image restoration framework, which is capable of gen-
erating restored high-quality image automatically or ac-
cording to the dialogue with the users. To the best of
our knowledge, LLMRA is the first work that applies
MLLMs in the domain of unified image restoration.
• To better incorporate text features into the restoration
network, we propose CEM (Context Enhance Mod-
ule) and DC-former (Degradation Context based Trans-
former Network). CEM enhances the text features and
DC-former propagates the degraded information from
textual features to the restoration network effectively.
• Our extensive experiments demonstrate the effectiveness
of LLMRA, as it achieves state-of-the-art performance
on various image restoration tasks, including image de-
noising, deraining, and low-light image enhancement.
2
Related Works
Unified Image Restoration.
Although there has been con-
siderable attention given to single degradation image restora-
tion methods [Zamir et al., 2022; Xia et al., 2023b], the ex-
ploration of unified image restoration for multi-degradation
remains limited. Some research has focused on addressing
image degradation caused by various weather conditions such
as snow, fog, and rain [Li et al., 2020; Valanarasu et al.,
2022]. However, these studies often train specific encoders
or decoders for each weather degradation, which lacks scal-
ability as it requires prior knowledge of specific degradation
types. Li et al. proposed a unified model called AirNet [Li
et al., 2022] for denoising, deraining, and dehazing. AirNet
incorporates contrastive learning to train an additional en-
coder, enabling implicit modeling of degradation information
in the input image. These learned representations are then
utilized in the main restoration network to predict the offsets
of adaptable convolutions for restoration. PromptIR [Potla-
palli et al., 2023] designed a visual prompt generation mod-
ule that combines a learned degradation prompt tensor to ob-
tain degradation features. DA-CLIP [Luo et al., 2023] com-
bines a large-scale pretrained visual language model with an
image restoration network and demonstrates competitive per-
formance across the ten degradation tasks.
Text-driven Image Generation.
In recent years, there has
been a rapid rise in text-based image generation works. Sev-
eral works [Crowson et al., 2022; Abdal et al., 2022]have em-
ployed a combination of pre-trained generative models and
CLIP to guide the generation process towards a desired tar-
get description. Additionally, latent diffusion model [Rom-
bach et al., 2022] are proposed, which enables training diffu-
sion models with limited computational resources while pre-
serving their quality and flexibility by operating in the latent
space. In addition to these prompt-driven approaches, there
have been advancements in instruction-based editing meth-
ods [Geng et al., 2023; Brooks et al., 2023] , which involve
modifying a source image based on specific instructions.
Multi-modal Large Language Models.
Recent years,
Large Language Models (LLMs) [Touvron et al., 2023;
Taori et al., 2023] have significantly contributed to conver-
sational AI and beyond. Subsequently, attention has been di-
rected towards advancing Multi-modal Large Language Mod-
els (MLLMs), aiming to equip LLMs with the ability to com-
prehend both text and images, enabling them to generate tex-
tual responses. For instance, Flamingo [Alayrac et al., 2022]
incorporates image encoding into the attention layer of the
LLM. BLIP-2 [Li et al., 2023] employs Q-Former to trans-
form input images into queries. Besides, LLaVA [Liu et al.,
2023] adopts CLIP to encode images into image embeddings,
which are then concatenated with text embeddings. Then,
MLLMs are adopted in various CV tasks [Xia et al., 2023a].
Degradation Context based Transformer (DC-former)
DMM
DMM
DMM
DMM
Z
Text input 
from users
MLLM
(Multi-modality Large 
Language Model)
Output text 
description
CLIP Text 
Encoder
LQ
I
HQ
Iˆ
Tinput
output
T
fea
T
Context Enhance Module
Context Transformer
Instruction <restore>
Instruction <refine>   
Ltri
LQ
I
Cross Attention
Self Attention
MLP
Lrec
(b) Context Enhance Module (CEM)
ResBlock
Cross Attention
Self Attention
MLP
Self Attention
MLP
(c) Context Transformer (CT)
'
token
T
Z
'
fea
T
DMM
Basic 
Transformer 
Block
Degradation 
Modulation 
Module
fea
T
'
fea
T
(a) The proposed LLMRA Framework 
Figure 2: The overview of the proposed LLMRA. (a) The proposed LLMRA Framework. DEN, CT and DC-former are used to refine and
incorporate the degradation information into the restoration network. (b) Context Enhance Module (CEM). (c) Context Transformer (CT).
3
Proposed Method
In this section, we present a comprehensive description of the
proposed method, which encompasses the generation of text
features, the network architectures and the loss functions.
Training.
As illustrated in Figure 2, with the instruction
<refine>, the restoration network is first trained with accu-
rate LQ image degradation descriptions, where the descrip-
tions are artificially generated. Subsequently, under the <re-
store>instruction, the Context Embedding Module (CEM) is
incorporated. During this process, the textual input of degra-
dation descriptions is provided by the MLLM. CEM is re-
sponsible for leveraging the features of the image to enhance
the description generated by MLLM, thereby making it more
closely aligned with the accurate depiction of degradation.
For the task of unified image restoration, we consider three
commonly encountered degradation types: noise, rain, and
low illumination. These degradation types encompass both
additive and multiplicative forms of degradation, thereby ex-
hibiting generalization capabilities.
Inference.
When presented with the instruction <re-
store >, the process initiates by taking a given degraded im-
age ILQ and a text prompt that solicits information regard-
ing the degradation. These inputs are fed into the MLLM.
Subsequently, the MLLM generates a descriptive text that ef-
fectively captures the low-level characteristics of the LQ im-
age. The resulting text description is then encoded using the
CLIP text encoder, yielding text feature Tfea. These features
are subsequently processed by the Context Enhance Module
(CEM) and the Context Transformer (CT) to obtain the degra-
dation context Z. Finally, the context Z is supplied to the
DC-former network for the restoration of the degraded im-
ages. When received the <refine>instruction, the CEM step
is omitted. The restoration takes the dialogue with the users
as the text input to realize adjustable restoration.
3.1
Generation of the Text Feature
Figure 2(a) illustrates the process of generating text features
that contain information about image degradation in our ap-
proach. We utilized idefics-9b-instruct [Huggingface, 2023],
a Multi-modal Large Language Model with 9 billion param-
eters, as the foundation of our approach. This model is de-
signed to process both image and text sequences as input and
generate coherent text as output.
To fully leverage the vast knowledge and amazing percep-
tual capabilities of MLLMs, We carefully devised instruc-
tions for text input. These instructions include three specific
questions related to the mentioned degradation types (i.e.,
noise, rain, and low-light conditions). As depicted in Fig-
ure 1, the large-scale model can generate promising responses
to user queries based on the image information.
Next, these output text descriptions are encoded into text
features Tfea ∈ R77×512, using the text encoder of CLIP.
The aforementioned procedure employs pretrained models,
we do not need fine-tuning on them. By denoting the input
text instructions and degraded image as Tinput and ILQ, this
process can be formulated as:
Tfea = FCLIP (FMLLM(Tinput, ILQ)),
(1)
where FMLLM and FCLIP indicate the text encoders of
IDEFICS and CLIP, respectively.
DMM
CAFF
Cross Attention
Self Attention (MDTA)
Xi
Z
Feed Forward (GDFN)
Feed Forward (GDFN)
Self Attention (MDTA)
Yi
Xi+1
Xi’
PWConv
Norm
PWConv
Norm
ReLU
ReLU
PWConv
PWConv
Norm
Norm
Concat
Sigmoid
GAP
Yi
Xi
XYi
CAFF
Xi’
Figure 3: Degradation Modulation Module (DMM) in DC-former.
3.2
Context Enhance Module
Under the instruction <refine>training recipe, the textual de-
scriptions are artificially generated, which is accurate and di-
rectly corresponds to the specific type of image degradation.
While the instruction <restore >requires the model to au-
tomatically restore the image without other priors. Due to
the potential inaccuracies in the descriptions generated by
MLLM, the context Enhance Module (CEM) is proposed to
utilize the image features to enhance the degradation descrip-
tions generated by MLLM. The goal is to bring these descrip-
tions as close as possible to accurate representations of image
degradation. As shown in Figure 2 (b), for an input LQ im-
age ILQ ∈ R3×H×W , we obtain the shallow image feature
through a convolutional ResBlock. Combining the shallow
image feature, we process the text features Tfea through two
text cross transformers, this process is formulated as:
T′
fea = CEM(ILQ, Tfea)
(2)
where T′
fea ∈ R77×512 refers to the enhanced text features.
After that, T′
fea (or Tfea) is processed by Context Trans-
former (CT) to get the degradation context embeddings Z.
CT is a single vanilla transformer [Vaswani et al., 2017] con-
sists of a self attention and multi-layer perceptron.
As mentioned above, we need to bring Z as close as pos-
sible to accurate representations of image degradation. To
this end, we leverage an triplet loss to learn Z by maximiz-
ing the consistency with the postive inputs while minimizing
the consistency between the negative ones. To be specific,
for a degradation context Z, Z+ and Z− are the correspond-
ing positive and negative counterpart, respectively. Then, the
triplet loss Ltri could be reformulated as:
Ltri =
N
X
i=1
hZi − Z+
i
2
2 −
Zi − Z−
i
2
2 + α
i
+
(3)
where α refers to the margin of the loss.
3.3
Degradation Context based Transformer
With the degradation context Z obtained from CT, the Degra-
dation Context based Transformer Network (DC-former) is
employed to restore the high-quality image from the input
with unknown degradation. The architecture of DC-former,
depicted in Figure 2(a), consists of multiple stacked ba-
sic transformer blocks and Degradation Modulation Modules
(DMM), organized in a UNet-shaped architecture. This de-
sign allows for effective information flow and contextual un-
derstanding, enabling the model to restore the image while
considering the specific degradation characteristics.
As shown in Figure 3, each DMM consists of an image
cross attention transformer (yellow box), a Concatenate At-
tention Feature Fusion (CAFF) module and a basic trans-
former block (blue box) from Restormer [Zamir et al., 2022].
The basic transformer block is composed of a Multi-Dconv
head transposed attention (MDTA) and Gated-Dconv feed-
forward network (GDFN), which allow more effective feature
interactions. The process is formulated as:
Xi+1 = DMM(Xi, Z)
(4)
where Xi and Xi+1 denote the input and output feature maps.
In CAFF, we first concatenate Xi and Yi as XYi. Inspired
by [Dai et al., 2021], the feature maps are processed with two
branch to get local and global information and aggregated at
the end. The local channel context L(XYi) ∈ RC×H×W is
computed via a bottleneck structure as follows:
L(XYi) = Norm(PWConv2(
δ(Norm(PWConv2(XYi)))))
(5)
where Norm refers to Layer Normalization (LN), PWConv2
denotes point-wise convolution (PWConv), δ denotes the
Rectified Linear Unit (ReLU). Note that the kernel sizes of
the two PWConv2 are 2C ×2C ×1×1 and 2C ×C ×1×1,
respectively. As a result, L(Xi) preserves the same shape as
the input feature, allowing for the preservation and emphasis
of intricate details in the low-level features.
In the global branch, the features are first processed
through a global average pooling (GAP), followed by similar
operations as PWConv1, LN, ReLU, PWConv1 and LN, fi-
nally get the global channel context G(XYi). The PWConv1
here is for one dimension. It is formulated as:
G(XYi) = Norm(PWConv1(
δ(Norm(PWConv1(GAP(XYi))))))
(6)
By incorporating the global channel context G(XYi) and lo-
cal channel context L(XYi) the modulated feature X′
i can
be obtained as follows:
X′
i = Xi ⊗ W(XYi) + (1 − W(XYi)) ⊗ Yi
(7)
W(XYi) = σ((L(XYi) ⊕ G(XYi)),
(8)
where σ denotes Sigmoid operation, W denotes the attention
weights. ⊕ denotes the broadcasting addition and ⊗ denotes
the element-wise multiplication.
3.4
The Objective Function
As mentioned above, when training the models using the <re-
store>and <refine>instructions, we employ distinct objec-
tive functions to optimize the process.
Lrefine = Lrec
(9)
Lrestore = Lrec + Ltri
(10)
where Ltri refers to the triplet loss (equation 3) and Lrec =
IHQ − ˆIHQ

1 represents the reconstruction loss, which
caculates the L1 norm between the ground truth IHQ and the
recovered high quality image ˆIHQ.
Ours
PromptIR
AirNet
Noisy
HQ
Ours
PromptIR
AirNet
Rain
HQ
Ours
URetinex-Net
EnGAN
Low light
HQ
Figure 4: Visual comparisons with the SOTA methods. Rows 1-2, 3-4, 5-6 rows display the results of image denoising, image deraining and
low light image enhancement, respectively. The test images are from Urban100, Rain100L and LOLv1. Zoom in for better visualization.
4
Experiments
4.1
Experimental Settings
To demonstrate the effectiveness of the proposed LLMRA,
we perform the evaluation on three representative image
restoration tasks: image denoising, image deraining, and low
light image enhancement. We train a unified model that can
recover images across all three degradation types.
Implementation Details.
The architecture of the DC-
former consists of a 4-level encoder-decoder, with varying
numbers of Transformer blocks at each level, specifically
[4, 6, 6, 8] from level-1 to level-4. We employ one DMM
between every two consecutive decoder levels, totaling 4
DMMs in the overall DC-former network. The channel size
of DC-former is set to 48. The model is trained with a batch
size of 4. The network is optimized with Adam optimizer
(β1 = 0.9, β2 = 0.999) with learning rate 1e−4 for 800k
iters. During training, we utilize cropped patches of size 128
x 128 as input, and to augment the training data, random hor-
izontal and vertical flips are applied to the input images.
Table 1: Denoising comparisons in the single-task setting on BSD68 and Urban100 datasets. Top rows: methods under the single-task
setting. Bottom rows: methods under the all-in-one setting. The optimal and sub-optimal PSNR/SSIM↑ results are highlighted using bold
and underlined, respectively.
BSD68
Urban100
Method
σ = 15
σ = 25
σ = 50
σ = 15
σ = 25
σ = 50
DnCNN
33.89/0.9290
31.23/0.8830
27.92/0.7896
32.98/0.9314
30.81/0.9015
27.59/0.8331
IRCNN
33.87/0.9285
31.18/0.8824
27.88/0.7898
27.59/0.8331
31.20/0.9088
27.70/0.8396
FFDNet
33.87/0.9290
31.21/0.8821
27.96/0.7887
33.83/0.9418
31.40/0.9120
28.05/0.8476
AirNet
33.85/0.9293
31.22/0.8837
27.98/0.7933
33.89/0.9419
31.52/0.9137
28.19/0.8520
DA-CLIP
26.34/0.6821
25.77/0.6531
24.31/0.5712
-
-
-
PromptIR
33.91/0.9296
31.28/0.8840
28.03/0.7926
33.93/0.9417
31.52/0.9121
28.17/0.8498
Ours
34.01/0.9302
31.37/0.8849
28.13/0.7930
34.12/0.9435
31.79/0.9163
28.56/0.8578
Table 2: Deraining results on Rain100L. Left columns: methods under single-task setting. Right columns: methods under all-in-one setting.
The optimal and sub-optimal results are highlighted using bold and underlined, respectively.
UMR
SIRR
MSPFN
Restormer
AirNet
DA-CLIP
PromptIR
Ours
PSNR↑
32.39
32.37
33.50
37.57
34.90
35.19
37.32
38.93
SSIM↑
0.921
0.926
0.948
0.974
0.968
0.960
0.979
0.984
Table 3: Low light image enhancement results on LOL-v1. Left columns: methods under single-task setting. Right columns: methods under
all-in-one setting. The optimal and sub-optimal results are highlighted using bold and underlined, respectively.
Retinex-Net
UFormer
EnGAN
KinD
URetinex-Net
Restormer
DA-CLIP
Ours
PSNR↑
16.40
16.36
17.56
20.86
21.33
22.43
23.40
23.30
SSIM↑
0.500
0.771
0.665
0.790
0.834
0.823
0.811
0.846
Datasets. In our experiments, we prepare several datasets
for the training of these three tasks. For image denoising, we
use WED [Ma et al., 2016] for training, which contains 4744
images. Testing is performed on BSD68 [Martin et al., 2001]
and Urban100 [Huang et al., 2015] datasets. From clean im-
ages of WED BSD68 and Urban100, we generate the noisy
images by adding Gaussian noise with different noise lev-
els σ ∈ {15, 25, 50}. For image deraining, we use the data
from [Yang et al., 2019], including 1800 paired light rainy im-
ages for training and 100 images for testing. For low light im-
age enhancement, we use LOL-v1 dataset [Wei et al., 2018],
including 485 low/normal light images pairs for training and
another 15 images for testing.
4.2
Comparison with State-of-the-Art Approaches
For comparing with the SOTA approaches, we trained the
proposed LLMRA in all-in-one settings by optimizing the
network (without CEM) with Lrefine (equation 9). We com-
pare our LLMRA with several unified image restoration ap-
proaches as well as specific degradation restoration methods
on three tasks. More precisely, we compare DnCNN [Zhang
et al., 2017a], IRCNN [Zhang et al., 2017b], FFDNet [Zhang
et al., 2018], AirNet [Li et al., 2022], PromptIR [Potla-
palli et al., 2023] and DA-CLIP [Luo et al., 2023] for
image denoisig.
We compare UMR [Yasarla and Patel,
2019], SIRR [Wei et al., 2019], MSPFN [Jiang et al., 2020],
Restormer [Zamir et al., 2022], AirNet [Li et al., 2022],
PromptIR [Potlapalli et al., 2023], and DA-CLIP [Luo et al.,
2023] for image deraining. We compare Retinex [Wei et al.,
2018], UFormer [Wang et al., 2022], EnGAN [Jiang et al.,
2021], KinD [Zhang et al., 2019] URetinex-Net [Wu et al.,
2022], Restormer [Zamir et al., 2022] and DA-CLIP [Luo et
al., 2023] for low light image enhancement.
Quantitative Comparison.
Table 1 presents results of
image denoising. It shows that our LLMRA achieves 0.39dB
for PSNR improvement over PromptIR for noise level σ = 50
on Urban100 dataset. Similar trends can be observed for de-
raining tasks. On the deraining task (Table 2), our method
yields performance gains of 1.61 dB over PropmtIR. For low
light image enhancement , our LLMRA achieves 0.035 for
SSIM improvement over DA-CLIP. Our method even outper-
forms the restormer for image deraining and low-light image
enhancement, which is trained in the single-task settings.
Qualitative Comparison.
In addition, we provide vi-
sual examples to illustrate the effectiveness of our proposed
method. Figure 4 showcases the results of the three tasks.
For image denoising, our LLMRA outperforms other state-
of-the-art methods by effectively removing noise from the
image without excessively blurring it. Similarly, the middle
rows demonstrate the efficacy of our approach in the derain-
ing task, as it successfully eliminates rain streaks and pro-
duces rain-free images. For low light image enhancement,
previous methods often suffered from issues such as color
distortion, over/underexposed regions, or failure to suppress
noise in specific areas. In contrast, our approach excels in
enhancing visibility, reliably enhancing the image without in-
troducing artifacts, and robustly preserving the natural color.
4.3
Impact of the Text Inputs
We manage to use text information to assist image restora-
tion, as text input is more readily available and allows for ad-
justable and interactive restoration manner through dialogue
With ground truth text input
With ground false text input
LQ Image
Figure 5: Impact of the text input.
Table 4: Quantitative results for the impact of the text input, eval-
uated on BSD68 (σ = 50), Rain100L and LOLv1 dataset. “with
gt text” means input ground truth text descriptions. “with gf text”
means input ground false text descriptions.
with gf text
with gt text
BSD68
14.46/0.4790
28.13/0.7930
Rain100L
20.11/0.8302
38.93/0.9842
LoLv1
7.59/0.1440
23.30/0.8457
with the MLLMs. To verify the impact of the text inputs,
we prepared two set of text descriptions for the test datasets,
which are called “ground truth” and “ground false” text in-
put. As shown in Figure 5, the task is image denoising for the
first row (14037.png from BSD68 with σ = 50), the ground
truth text description could be “The image is well lit. No
rain streaks detected. The image has gaussian noise degrada-
tion and the noise level is high.” Conversely, the ground false
text description is would be completely opposite, like “The
image is dark. The image is degraded by rain streaks. No
noise detected.” From Figure 5, it is evident that the presence
of ground truth text input results in effective noise removal
without any other modifications. Conversely, when ground
false text input is used, the noise persists but the lighting is
enhanced. Similar clue could also be drawn from the quanti-
tative results in Table 4, when confronted with accurate and
erroneous textual input, the disparity in the results of restora-
tion is substantial.
4.4
Ablation study
Impact of CEM. To verify the impact of Context Enhance
Module (CEM) on enhancing the text descriptions obtained
from the MLLM in the universal image restoration task, we
carry out some experiments. In this section, a set of prede-
fined specific questions related to the mentioned degradation
types (i.e., noise, rain, and low-light conditions) are sent to
the MLLM, and it would generate corresponding responses
to be the text descriptions for further guiding the restoration.
We restore the images with and without CEM under these
conditions. The results are shown in Table 2, revealing a sig-
Table 5: Ablation study on the impact of CEM. Results are reported
on BSD68 (σ = 50), Rain100L and LOLv1 datasets. The best re-
sults are shown in boldface.
w.o. CEM
Ours
BSD68
25.18/0.6913
28.11/0.7964
Rain100L
26.54/0.8838
38.64/0.9831
LoLv1
17.51/0.6999
20.19/0.8243
Table 6: Ablation study on the way of modulating the text features.
Results are reported on BSD68 (σ = 50), Rain100L and LOLv1
datasets. The best results are shown in boldface.
w.o. DMM
Ours
BSD68
28.02/0.7913
28.13/0.7930
Rain100L
37.71/0.9796
38.93/0.9842
LoLv1
19.40/0.8013
23.30/0.8457
nificant improvement in the restoration outcomes when CEM
is incorporated.
The way of modulating the text features.
In the do-
main of text-to-image generation [Rombach et al., 2022], re-
searchers commonly employ a denoising UNet with a cross
transformer as the basic module to modulate the text fea-
tures. However, in our proposed method, we utilize DMMs
for the degradation context modulation.
In order to vali-
date the effectiveness of our method, we follow the approach
of these text-to-image generation methods by removing the
CAFF modules and stacking the cross transformers in the
decoder. The experimental results are presented in Table 6,
which demonstrates the effectiveness of the proposed DMM.
5
Conclusion
This paper introduces LLMRA, a novel framework that lever-
ages multi-modal large language models for universal image
restoration. The core contribution of our framework is uti-
lizing the MLLM and a text-guided restoration network to
realize a more accurate, adjustable and interactive restora-
tion manner. The Context Enhance Module and the Degra-
dation Context based Transformer Network are proposed to
effectively enhance the degradation information and incorpo-
rate it into the restoration network. Experimental evaluation
on unified image restoration tasks demonstrates that LLMRA
leads to significant performance on image denoising, image
deraining, and low light image enhancement. Nevertheless, it
is important to acknowledge some limitations of the proposed
LLMRA. The performance of LLMRA may fluctuate with the
performance of MLLM, as it may provide uninformative or
even harmful answers of the degradation information, thus af-
fecting the quality of restoration. Fortunately, users can <re-
fine>the results by engaging in subsequent dialogue. More
ever, our experiments are currently limited to only three tasks.
Although these three tasks are representative to some extent,
as they encompass both additive and multiplicative degrada-
tion. In future research, we aim to broaden the scope of our
investigation to encompass a wider range of restoration tasks
involving different types of degradation.
References
[Abdal et al., 2022] Rameen Abdal, Peihao Zhu, John Femi-
ani, Niloy Mitra, and Peter Wonka. Clip2stylegan: Un-
supervised extraction of stylegan edit directions. In SIG-
GRAPH, pages 1–9, 2022. 2
[Alayrac et al., 2022] Jean-Baptiste Alayrac, Jeff Donahue,
Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, et al. Flamingo: a visual language model for
few-shot learning. NeurIPS, 2022. 1, 2
[Antol et al., 2015] Stanislaw Antol, Aishwarya Agrawal, Ji-
asen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence
Zitnick, and Devi Parikh. VQA: Visual Question Answer-
ing. In ICCV, 2015. 1
[Brooks et al., 2023] Tim Brooks, Aleksander Holynski, and
Alexei A Efros. Instructpix2pix: Learning to follow image
editing instructions. In CVPR, 2023. 2
[Chen et al., 2015] Xinlei Chen, Hao Fang, Tsung-Yi Lin,
Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll´ar,
and C Lawrence Zitnick.
Microsoft coco captions:
Data collection and evaluation server.
arXiv preprint
arXiv:1504.00325, 2015. 1
[Crowson et al., 2022] Katherine Crowson, Stella Biderman,
Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Cas-
tricato, and Edward Raff. Vqgan-clip: Open domain image
generation and editing with natural language guidance. In
ECCV, 2022. 2
[Dai et al., 2021] Yimian
Dai,
Fabian
Gieseke,
Stefan
Oehmcke, Yiquan Wu, and Kobus Barnard. Attentional
feature fusion. In WACV, 2021. 4
[Dai et al., 2023] Wenliang Dai, Junnan Li, Dongxu Li, An-
thony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi. Instructblip:
Towards general-purpose vision-language models with in-
struction tuning. arXiv preprint arXiv:2305.06500, 2023.
1
[Geng et al., 2023] Zigang Geng,
Binxin Yang,
Tiankai
Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao,
Zheng Zhang, Han Hu, Dong Chen, et al.
Instructdif-
fusion: A generalist modeling interface for vision tasks.
arXiv preprint arXiv:2309.03895, 2023. 2
[Huang et al., 2015] Jia-Bin Huang, Abhishek Singh, and
Narendra Ahuja.
Single image super-resolution from
transformed self-exemplars. In CVPR, 2015. 6
[Huggingface, 2023] Huggingface. Introducing idefics: An
open reproduction of state-of-the-art visual language
model, 2023. 1, 3
[Jiang et al., 2020] Kui Jiang, Zhongyuan Wang, Peng Yi,
Chen Chen, Baojin Huang, Yimin Luo, Jiayi Ma, and Jun-
jun Jiang. Multi-scale progressive fusion network for sin-
gle image deraining. In CVPR, 2020. 6
[Jiang et al., 2021] Yifan Jiang, Xinyu Gong, Ding Liu,
Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan
Zhou, and Zhangyang Wang. Enlightengan: Deep light
enhancement without paired supervision. TIP, 2021. 6
[Lai et al., 2023] Xin Lai, Zhuotao Tian, Yukang Chen, Yan-
wei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reason-
ing segmentation via large language model. arXiv preprint
arXiv:2308.00692, 2023. 1
[Li et al., 2020] Ruoteng Li, Robby T Tan, and Loong-Fah
Cheong. All in one bad weather removal using architec-
tural search. In CVPR, 2020. 2
[Li et al., 2022] Boyun Li, Xiao Liu, Peng Hu, Zhongqin
Wu, Jiancheng Lv, and Xi Peng. All-in-one image restora-
tion for unknown corruption. In CVPR, 2022. 1, 2, 6
[Li et al., 2023] Junnan Li, Dongxu Li, Silvio Savarese, and
Steven Hoi. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language
models. arXiv preprint arXiv:2301.12597, 2023. 2
[Liu et al., 2023] Haotian Liu, Chunyuan Li, Qingyang Wu,
and Yong Jae Lee. Visual instruction tuning. NeurIPS,
2023. 1, 2
[Luo et al., 2023] Ziwei Luo, Fredrik K Gustafsson, Zheng
Zhao, Jens Sj¨olund, and Thomas B Sch¨on. Controlling
vision-language models for universal image restoration.
arXiv preprint arXiv:2310.01018, 2023. 1, 2, 6
[Ma et al., 2016] Kede Ma, Zhengfang Duanmu, Qingbo
Wu, Zhou Wang, Hongwei Yong, Hongliang Li, and Lei
Zhang. Waterloo exploration database: New challenges
for image quality assessment models. TIP, 2016. 6
[Martin et al., 2001] David
Martin,
Charless
Fowlkes,
Doron Tal, and Jitendra Malik.
A database of human
segmented natural images and its application to evaluat-
ing segmentation algorithms and measuring ecological
statistics. In ICCV, 2001. 6
[Potlapalli et al., 2023] Vaishnav Potlapalli, Syed Waqas Za-
mir, Salman Khan, and Fahad Shahbaz Khan. Promptir:
Prompting for all-in-one blind image restoration. arXiv
preprint arXiv:2306.13090, 2023. 1, 2, 6
[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris
Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
Clark, et al. Learning transferable visual models from nat-
ural language supervision. In ICML, 2021. 1
[Rombach et al., 2022] Robin
Rombach,
Andreas
Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn
Ommer.
High-resolution image synthesis with latent
diffusion models. In CVPR, 2022. 2, 7
[Taori et al., 2023] Rohan Taori, Ishaan Gulrajani, Tianyi
Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto.
Stanford alpaca:
An instruction-following llama model. https://github.com/
tatsu-lab/stanford alpaca, 2023. 2
[Touvron et al., 2023] Hugo Touvron, Thibaut Lavril, Gau-
tier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-
oth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Ham-
bro, Faisal Azhar, et al. Llama: Open and efficient founda-
tion language models. arXiv preprint arXiv:2302.13971,
2023. 2
[Valanarasu et al., 2022] Jeya Maria Jose Valanarasu, Rajeev
Yasarla, and Vishal M Patel. Transweather: Transformer-
based restoration of images degraded by adverse weather
conditions. In CVPR, 2022. 2
[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you
need. NeurIPS, 2017. 4
[Wang et al., 2022] Zhendong Wang, Xiaodong Cun, Jian-
min Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang
Li. Uformer: A general u-shaped transformer for image
restoration. In CVPR, 2022. 6
[Wei et al., 2018] Chen Wei, Wenjing Wang, Wenhan Yang,
and Jiaying Liu. Deep retinex decomposition for low-light
enhancement. arXiv preprint arXiv:1808.04560, 2018. 6
[Wei et al., 2019] Wei Wei, Deyu Meng, Qian Zhao, Zong-
ben Xu, and Ying Wu. Semi-supervised transfer learning
for image rain removal. In CVPR, 2019. 6
[Wu et al., 2022] Wenhui Wu, Jian Weng, Pingping Zhang,
Xu Wang, Wenhan Yang, and Jianmin Jiang. Uretinex-
net: Retinex-based deep unfolding network for low-light
image enhancement. In CVPR, 2022. 6
[Wu et al., 2023] Haoning Wu, Zicheng Zhang, Erli Zhang,
Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li,
Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench:
A benchmark for general-purpose foundation models on
low-level vision. arXiv preprint arXiv:2309.14181, 2023.
1
[Xia et al., 2023a] Bin Xia, Shiyin Wang, Yingfan Tao, Yi-
tong Wang, and Jiaya Jia. Llmga: Multimodal large lan-
guage model based generation assistant. arXiv preprint
arXiv:2311.16500, 2023. 1, 2
[Xia et al., 2023b] Bin Xia, Yulun Zhang, Shiyin Wang, Yi-
tong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang,
and Luc Van Gool. Diffir: Efficient diffusion model for im-
age restoration. arXiv preprint arXiv:2303.09472, 2023. 2
[Yang et al., 2019] Wenhan Yang, Robby T Tan, Jiashi Feng,
Zongming Guo, Shuicheng Yan, and Jiaying Liu. Joint
rain detection and removal from a single image with con-
textualized deep networks. TPAMI, 2019. 6
[Yasarla and Patel, 2019] Rajeev Yasarla and Vishal M Patel.
Uncertainty guided multi-scale residual learning-using a
cycle spinning cnn for single image de-raining. In CVPR,
2019. 6
[Zamir et al., 2022] Syed
Waqas
Zamir,
Aditya
Arora,
Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and
Ming-Hsuan Yang. Restormer: Efficient transformer for
high-resolution image restoration. In CVPR, 2022. 2, 4, 6
[Zhang et al., 2017a] Kai Zhang, Wangmeng Zuo, Yunjin
Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian de-
noiser: Residual learning of deep cnn for image denoising.
TIP, 2017. 6
[Zhang et al., 2017b] Kai Zhang, Wangmeng Zuo, Shuhang
Gu, and Lei Zhang. Learning deep CNN denoiser prior for
image restoration. In CVPR, 2017. 6
[Zhang et al., 2018] Kai Zhang, Wangmeng Zuo, and Lei
Zhang.
Ffdnet: Toward a fast and flexible solution for
cnn-based image denoising. TIP, 2018. 6
[Zhang et al., 2019] Yonghua Zhang, Jiawan Zhang, and Xi-
aojie Guo. Kindling the darkness: A practical low-light
image enhancer. In ACM MM, 2019. 6
[Zhu et al., 2023] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xi-
ang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing
vision-language understanding with advanced large lan-
guage models.
arXiv preprint arXiv:2304.10592, 2023.
1
","In recent times, there has been a notable surge of interest in the task of unified image restoration. Researchers are challenged to develop a single model capable of handling images with diverse types of degradation. Several approaches have been proposed to tackle this challenge, employing techniques like degradation encoder, contrastive learning [Li et al., 2022], and prompt learning [Potlapalli et al., 2023] to achieve promising results. Some approaches also leverage visual language models (VLMs) to handle a wide range of degradations [Luo et al., 2023]. However, when it comes to complex real-world degradations, the processing and storage capabilities of these encoders and VLMs are still limited. In particular, these methods can only directly restore images and cannot accept other instructions for restoration or optimization, which limits the application scenarios.nan"
"Despite achieving remarkable progress in benchmark tasks with low-dimensional proprioceptive demonstrations, Adversarial Imitation Learning (AIL) degrades when facing high-dimensional visual inputs. Existing work often employs sophisticated network architectures or disassociates the representation learning from the decision-making, overlooking the valuable intra-agent information contained within demonstrations. To address this, we propose Contrastive Adversarial Imitation Learning (CAIL) that incorporates calibrated contrastive representation learning into the AIL framework. Specifically, we introduce an image encoder utilizing a combination of supervised and unsupervised contrastive learning to extract valuable features from visual states. Meanwhile, we propose to calibrate the contrastive loss by treating agent demonstrations as a mixed sample. It allows for a more discriminative representation while being jointly optimized with the AIL framework. Extensive experiments on the DMControl Suite demonstrate that CAIL outperforms other approaches, showcasing its sample efficiency and robustness.","Imitation Learning (IL) is a practical method for tasks with sequential decision-making problems. Behavioral Cloning (BC) is a typical IL approach, wherein an agent observes expert actions and learns to map state to action via supervised learning. However, this offline training approach suffers from compounding errors when executing the policy, causing drift to dangerous states. In contrast, Adversarial Imitation Learning (AIL) encourages the agent to cover the distribution of the expert policy, resulting in more accurate policies. Conventional AIL methods perform well with low-dimensional proprioceptive demonstrations, but degrade with high-dimensional visual inputs due to the less distinguishable features in visual space. Several approaches enhance the state representation by enriching the network architecture or separating the learning of representation and decision-making. However, these methods often require complex training strategies that prevent end-to-end training.","Applying GAIL directly to visual imitation learning may lead to performance decline due to the encoder's limited discriminative capacity for visually similar but semantically distinct states. We introduce Contrastive Adversarial Imitation Learning (CAIL) to address this issue. CAIL incorporates contrastive learning as a fundamental training objective, leveraging the large unlabeled agent state dataset and the expert states as distinct groups for supervised contrastive learning. Motivated by the observation that agent states may learn expert characteristics strongly, we propose a novel calibrated supervised contrastive loss function that considers agent states as a mixture of expert and agent states.","Extensive experiments are conducted on the DMControl Suite to evaluate CAIL's effectiveness. Results show that CAIL outperforms other methods, both sample-efficiently and asymptotically. CAIL achieves the best performance at 500K steps in all nine tasks and demonstrates competitive results at 1M steps. Visualizations of the discriminator's attention map reveal that CAIL has better coverage on the agent's body joints, indicating a better ability to capture differences among visual states. Additional experiments explore the impact of the proposed calibrated contrastive loss, different data augmentation methods, and the choice of image encoder. CAIL consistently demonstrates superior performance, highlighting its robustness and effectiveness.","This study presents Contrastive Adversarial Imitation Learning (CAIL), a novel approach that incorporates contrastive learning into the AIL framework to address the challenges of visual imitation learning. CAIL utilizes contrastive representation learning to extract valuable features from high-dimensional visual inputs and employs a calibrated contrastive loss to account for the evolving expertise of the agent. Extensive experiments on the DMControl Suite demonstrate CAIL's sample efficiency, robustness, and superior performance compared to existing methods. These findings suggest that combining contrastive learning with AIL provides a promising direction for visual imitation learning.",Visual Imitation Learning with Calibrated Contrastive Representation,"Yunke Wang, Linwei Tao, Bo Du, Yutian Lin, Chang Xu","Visual Imitation Learning with Calibrated Contrastive Representation
Yunke Wang1 , Linwei Tao2 , Bo Du1 , Yutian Lin1 and Chang Xu2
1School of Computer Science, Wuhan University, China
2School of Computer Science, Faculty of Engineering, The University of Sydney, Australia
{yunke.wang, dubo, yutian.lin}@whu.edu.cn, {linwei.tao, c.xu}@sydney.edu.au
Abstract
Adversarial Imitation Learning (AIL) allows the
agent to reproduce expert behavior with low-
dimensional states and actions.
However, chal-
lenges arise in handling visual states due to their
less distinguishable representation compared to
low-dimensional proprioceptive features.
While
existing methods resort to adopt complex network
architectures or separate the process of learning
representation and decision-making, they overlook
valuable intra-agent information within demonstra-
tions. To address this problem, this paper proposes
a simple and effective solution by incorporating
calibrated contrastive representative learning into
visual AIL framework. Specifically, we present an
image encoder in visual AIL, utilizing a combi-
nation of unsupervised and supervised contrastive
learning to extract valuable features from visual
states. Based on the fact that the improved agent of-
ten produces demonstrations of varying quality, we
propose to calibrate the contrastive loss by treating
each agent demonstrations as a mixed sample. The
incorporation of contrastive learning can be jointly
optimized with the AIL framework, without modi-
fying the architecture or incurring significant com-
putational costs. Experimental results on DMCon-
trol Suite demonstrate our proposed method is sam-
ple efficient and can outperform other compared
methods from different aspects.
1
Introduction
Imitation Learning (IL) [Hussein et al., 2017; Zheng et
al., 2022] is a practical approach for addressing sequential
decision-making problems [Silver et al., 2016; Van Hasselt
et al., 2016], which seeks to acquire an optimal policy for
an agent by emulating the behavior of an expert.
One of
the primary approaches utilized in IL is Behavioral Cloning
(BC) [Pomerleau, 1988], in which the agent observes the ac-
tion of the expert and learns a straight mapping from state to
action via regression. However, this offline training manner
may suffer from compounding errors [Brantley et al., 2019;
Xu et al., 2020; Tu et al., 2022] when the agent executes the
policy, leading it to drift to new and dangerous states. Instead,
Visual State
Positions
Velocity
Physical State
𝒗!
𝒗""
𝒔!
𝒔""
Positions
Velocity
Figure 1: Two visual states v1, v2 and their corresponding physical
states s1, s2 are shown in the figure. The physical state on the right
column contains proprioceptive information (i.e., positions and ve-
locities). Although a significant change in the physical state occurs,
it may only result in slight changes to the visual state.
Adversarial Imitation Learning (AIL) [Ho and Ermon, 2016;
Fu et al., 2017; Wang et al., 2021] encourages the agent
to cover the distribution of the expert policy, which can re-
sult in a more accurate policy. Despite the significant suc-
cess of AIL in benchmark tasks [Todorov et al., 2012],
where the expert demonstration is often of low dimension
and represents the proprioceptive features (e.g., positions, ve-
locities, and accelerations), its performance will degrade a
lot in more challenging tasks with high-dimensional visual
demonstrations [Jaderberg et al., 2019; Espeholt et al., 2018;
Tucker et al., 2018] (e.g., first-person camera observations).
Recently, several studies have proposed solutions for visual
imitation learning problems by enhancing the state represen-
tation [Brown et al., 2019; Brown et al., 2020; Cai et al.,
2021; Tucker et al., 2018; Yuan and Kitani, 2018; Liu et al.,
2023]. These approaches often require sophisticated training
strategies that first learn a reward function or image encoder
and prohibit end-to-end training [Brown et al., 2019; Brown
et al., 2020]. Additionally, some of these methods are tai-
lored to specific tasks, such as video games [Cai et al., 2021;
Tucker et al., 2018; Yuan and Kitani, 2018]. PatchAIL [Liu et
al., 2023], on the other hand, adopts a different approach by
evaluating the local expertise of various image patches rather
than inducing a scalar reward from the entire image. By lever-
arXiv:2401.11396v1  [cs.LG]  21 Jan 2024
aging local information, PatchAIL can focus on minor motion
changes at the patch level and obtain a more discriminative
representation. PCIL [Huang et al., 2023] adds constraints
on the discriminator to push expert states together and pull
away agent states to enhance its ability. However, it fails to
leverage samples in the replay buffer and ignores the potential
improvement of the agent during adversarial training.
Despite recent advances in visual-based adversarial imita-
tion learning, the performance of such methods still lags be-
hind that of proprioceptive-based adversarial imitation learn-
ing.
The reason for this performance disparity can be at-
tributed to the fact that proprioceptive features are highly dis-
tinguishable in low-dimensional feature space, as they con-
tain explicit information about the agent. Conversely, certain
features are not easily discernible in the image space. In Fig-
ure 1, the significant change in the physical state only leads to
minor changes in the visual state. Hence, to enhance the per-
formance of visual adversarial imitation learning, it is essen-
tial to develop a suitable representation that can accurately en-
code the visual states. Contrastive representation learning is a
technique that seeks to learn a representation space in which
comparable instances are situated in close proximity while
non-comparable instances are positioned far apart. This ap-
proach has demonstrated considerable potential in enhancing
feature representations. Prior research in imitation learning
has primarily focused on comparing expert and agent demon-
strations, overlooking the intra-agent information present in
the demonstrations.
To address this limitation, we introduce our proposed
method, named Contrastive Adversarial Imitation Learning
(CAIL). Our approach aims to learn a feature representation
that not only discriminates between expert and agent demon-
strations, but also captures the similarities and differences
among different agent demonstrations. Specifically, CAIL
utilizes contrastive representation learning to encourage the
representations of different agent demonstrations to be sepa-
rated in the feature space while bringing similar demonstra-
tions closer. In practice, on top of the vanilla AIL, we add
two contrastive losses on the image encoder. The first unsu-
pervised contrastive loss makes the encoder fully exploit the
larger amounts of agent demonstrations in the replay buffer.
A supervised contrastive loss is then applied to distinguish
agent and expert demonstrations while maintaining consis-
tency with the unsupervised contrastive loss.
Considering
agent demonstrations improving with training, we regard the
agent demonstration as a sample drawn from a mixture of
agent and expert policy distribution and propose a calibrated
supervised contrastive loss accordingly. Empirical results on
DMControl Suite [Tunyasuvunakool et al., 2020] show the
contrastive learning combined AIL framework greatly outper-
forms compared methods.
2
Related Work
Reinforcement Learning (RL) [Sutton and Barto, 2018] aims
to learn a policy for the agent by rewarding its action during
its interaction with the environment. When the state is rep-
resented by raw pixels, it is essential to learn a good state
representation through an image encoder to achieve effec-
tive results. In visual RL, it is common to share the latent
representation between the policy and critic, with only the
critic loss used for updating the encoder. Recent investiga-
tions [Cobbe et al., 2019; Lee et al., 2019] into the effective-
ness of data augmentation in pixel-based RL have shown that
simple techniques, such as cutout and random convolutional,
can enhance the generalization of agents across different vi-
sual RL benchmarks. CURL [Laskin et al., 2020a] leverages
data augmentations to learn a contrastive representation in
the RL setting, which enhances the data-efficiency of pixel-
based RL. Similarly, some other methods [Finn et al., 2015;
Yarats et al., 2021b; Yarats et al., 2021a; Kostrikov et al.,
2020] consider representation learning as an auxiliary task
and train a regularized autoencoder jointly with the RL agent.
RAD [Laskin et al., 2020b] utilizes data augmentation di-
rectly without introducing auxiliary loss, achieving good
data-efficient results in pixel-based RL.
Based on visual reinforcement learning, visual imitation
learning enables the agent to learn a policy from visual
demonstrations. This approach has become increasingly pop-
ular due to its compatibility with real-world tasks such as
autonomous driving and robot learning [Wen et al., 2021;
Ross et al., 2011]. Similar to visual reinforcement learning,
a good image encoder is essential to achieve good results in
visual imitation learning as it is difficult to learn effective rep-
resentations in unstable adversarial training. To address this
problem, T-REX [Brown et al., 2019] and D-REX [Brown et
al., 2020] extrapolate a reward function from visual observa-
tions via ranked trajectories, then learn the agent policy using
the learned reward function. The first work to successfully
apply AIL to video game environments such as Atari games
is [Tucker et al., 2018], which also conducts pre-training on
the encoder first. Some recent works [Rafailov et al., 2021;
Torabi et al., 2018; Cohen et al., 2021; Liu et al., 2023]
have directly modified plain adversarial imitation learning
methods to make them compatible with visual inputs, either
through network architecture or training strategies. However,
there is still a clear gap between visual AIL and plain AIL.
3
Preliminary
Markov Decision Process (MDP). MDP is popular to formu-
late reinforcement learning (RL) [Puterman, 1994] and imi-
tation learning (IL) problems. An MDP normally consists six
basic elements M = (S, A, P, R, γ, µ0), where S is a set
of states, A is a set of actions, P : S × A × S → [0, 1]
is the stochastic transition probability from current state s to
the next state s′, R : S × A → R is the obtained reward of
agent when taking action a in a certain state s, γ ∈ [0, 1] is
the discounted rate and µ0 denotes the initial state distribu-
tion. Given a trajectory τ = {(st, at)}T
t=0, the return R(τ)
is defined as the discounted sum of rewards obtained by the
agent over this episode, R(τ) = PT
t=0 γkr(sk, ak) and T is
the number of steps to reach an absorbing state. The goal of
RL is thus to learn a policy that can maximize the expected
return over all episodes during the interaction. For any policy
π : S → A, there is an one-to-one correspondence between
π and its occupancy measure ρπ : S × A → [0, 1].
Adversarial Imitation Learning (AIL). Generative Adver-
sarial Imitation Learning (GAIL) [Ho and Ermon, 2016] is
the most representative work, which directly applies the gen-
eral GAN framework [Goodfellow et al., 2014] into adver-
sarial imitation learning. Given a set of expert demonstra-
tions (s, a) drawn from the expert policy πe, GAIL aims to
learn an agent policy πθ by minimizing the Jensen-Shannon
divergence between ρπθ and ρπe [Ke et al., 2020]. In the im-
plementation, a discriminator Dϕ is introduced to distinguish
demonstrations from expert and agent policy, yet the agent
policy tries its best to ‘fool’ the discriminator:
min
θ
max
ϕ
E(s,a)∼ρπe [log Dϕ(s, a)]
(1)
+ E(s,a)∼ρπθ [log(1 − Dϕ(s, a))].
The agent is trained to minimize the outer objective function
E(s,a)∼ρπθ [log(1 − Dϕ(s, a))], and therefore the output of
− log(1 − D(s, a)) can be regarded as reward. Regular RL
methods like TRPO [Schulman et al., 2015], PPO [Schulman
et al., 2017] and SAC [Haarnoja et al., 2018] can be thus used
to update the agent policy πθ.
Typically, in the visual imitation learning problem, visual
demonstrations usually do not contain expert actions.
We
therefore replace the state-action pair with the visual state
here and denote it as v to avoid confusion.
With an im-
age encoder f : v → Rdim(r) and an MLP projection head
hd : Rdim(r) → R1, where dim(r) denotes the dimension
of the extracted representation r we can re-formulate the dis-
criminator D → hd ◦f. The objective of training discrimina-
tor can be rewritten as,
L(hd, f) = −Ev∼ρπe [log hd(f(v))]
(2)
− Ev∼ρπθ [log(1 − hd(f(v)))],
The agent policy πθ can then be trained with reward − log(1−
hd(f(v))) via reinforcement learning.
4
Methodology
Applying GAIL (Eq. (2)) directly to visual imitation learning
may result in a decrease in performance, primarily due to the
limited discriminative capacity of the encoder with respect to
visually-similar yet semantically-distinct states. Thus, a more
stringent criterion for the representation ability of the encoder
is necessary. However, the instability associated with adver-
sarial training and random sampling in RL poses obstacles to
effectively learning a robust encoder. In recent times, con-
trastive learning has emerged as a potent methodology for
extracting discriminative representations. The fundamental
concept behind integrating contrastive learning into GAIL in-
volves deriving an informative representation r of states v by
training an encoder function f that brings the representations
r = f(v) of semantically similar states (positives) closer to
each other while moving those of dissimilar states (negatives)
further apart in the embedding space.
To this end, we propose a novel training scheme, named
CAIL, that incorporates contrastive learning as a fundamen-
tal training objective. To begin with, we explore the adop-
tion of an unsupervised contrastive loss function denoted
as LUnSupCon, which serves to fully exploit the agent visual
states. Additionally, we incorporate a supervised contrastive
loss function denoted as LSupCon, to enhance the encoder’s
discriminative ability.
Motivated by the observation that
agent states may learn the expert characteristics to such a high
degree that it is no longer appropriate to treat them as an ab-
solute opposing category to the expert states during training,
we further propose a novel calibrated supervised contrastive
loss function, denoted as LC-SupCon, which considers the agent
states as a mixture of both expert and agent states. The train-
ing framework of CAIL is shown in Figure 2.
4.1
Contrastive Representation on Visual States
Unsupervised Contrastive Loss. Prior research in GAIL has
focused only on the comparison between expert and agent
states while ignoring the significant number of available agent
states in the replay buffer. One potential approach to leverage
the agent states via contrastive learning is to employ unsuper-
vised contrastive learning with the large unlabeled agent state
dataset. In practice, we obtain 2N augmented agent states
va := {va
i }2N
i=1 from a batch of N agent states. Two aug-
mentations from the same source are denoted as (va
i , p(va
i )),
where p(va
i ) is one of the two augmented states other than
va
i . Since the labeling information is unavailable, we con-
sider p(va
i ) as the sole positive example to va
i , and we aim to
minimize the distance between the representations of va
i and
p(va
i ). The remaining 2N − 2 augmented states in the same
batch are regarded as negatives to va
i and their representa-
tions are expected to be far from va
i . We adopt the InfoNCE
loss [Oord et al., 2018], which is widely-accepted in unsu-
pervised contrastive learning to formulate our unsupervised
contrastive loss for agent states va
i :
ℓInfoNCE(va
i , va+
i
, va−, f, h)
= − log
exp
Encoder
500K Timesteps
50K Timesteps
Reward
Timesteps
Augmented States
Representation
Projection
ℒ!""#
ℒ$%&'($)*
𝑟""
+
𝑝(𝑟""
+)
𝑣""
+
𝑝(𝑣""
+)
Contrastive Adversarial Imitation learning
ℒ,*&'($)*
Expert States
Agent States
𝑣""
-
𝑟""
-
Figure 2: An overview of Contrastive Adversarial Imitation Learning. The encoder extracts the representation of the augmented expert state,
and two augmented agent states. The training objective of the encoder consists of a discrimination loss, an unsupervised contrastive loss and
a calibrated supervised contrastive loss.
states together while pulling away expert states with those
agent states. This can be achieved by utilizing supervised
contrastive loss [Khosla et al., 2020]. While the supervised
contrastive loss was designed for supervised settings, we can
treat the expert states and agent states as distinct groups, al-
lowing us to utilize it as a binary classification problem for
training the encoder in an unsupervised setting.
Unlike unsupervised contrastive loss, which employs only
one positive example, supervised contrastive loss generalizes
LUnSupCon to any number of positive examples. Specifically,
in our approach, we consider all augmented expert states
ve = {ve
i }N
i=1 in one batch as states of the same class, while
all augmented agent states va are viewed as representations
of the opposite class. We denote all augmented states in one
batch as v = va ∪ ve. The supervised contrastive loss for
each state ve
i ∈ ve is defined as follows,
ℓSup(ve
i , ve)
= −
1
|ve| − 1
X
v+
j ∈ve\ve
i
ℓInfoNCE(ve
i , v+
j , v \ ve
i , f, hsup)
(5)
where hsup is the projection head dedicated for supervised
contrastive loss and |ve| is the cardinality of ve. While the
expert The final supervised contrastive loss for the CAIL is
the average supervised contrastive loss over all expert states,
denoted as,
LSupCon = 1
N
N
X
i=1
ℓSup(ve
i , ve).
(6)
Through the utilization of the supervised objective LSupCon,
the encoder is capable of extracting a representation that is
not only consistent with LUnSupCon, but also has an enhanced
ability to discriminate between experts and agents.
4.2
Calibrating the Contrastive Representation
In contrast to vanilla supervised contrastive learning, where
label information is pre-determined and fixed, our approach
involves training an agent that progressively acquires ex-
pertise over time, particularly in the latter stages of train-
ing [Wang et al., 2023]. At this point, some of the well-
trained agent states may even be more realistic than the ex-
pert. Thus, continuously treating them as opposite examples
to experts could lead to unstable training and may be deemed
unfair. Instead, it is more reasonable to regard the agent states
as a mixture of positive states and negative states towards ex-
pert states. In this way, we are able to better account for the
varying levels of expertise exhibited by the agent states and
enhance the stability and fairness of the training process.
However, determining which states are well-trained and
which require further training can be a challenging task. To
address this issue, we consider the agent state va
i as a high-
quality sample (e.g., expert state) with a probability of α, and
as a low-quality sample with a probability of 1 − α. When
treating va
i as high-quality sample, we utilize the supervised
contrastive loss ℓSup to train the encoder. In this case, all ex-
pert states ve
i and the corresponding augmented state p(va
i )
are treated as positive pairs for va
i . Conversely, when treating
agent samples as low-quality samples, we change the train-
ing objective to the unsupervised loss ℓInfoNCE. Here, only
the augmented state p(va
i ) is treated as a positive pair for va
i ,
while all other states are considered as negatives. We then
obtain a calibrated supervised contrastive loss, which can be
expressed as:
LC-SupCon = αEva
i ∼ρπθ

ℓSup(va
i , ve ∪ {va
i , p(va
i )})

(7)
+ (1 − α)Eva
i ∼ρπθ

ℓInfoNCE(va
i , p(va
i ), v \ va
i , f, hsup)

Combining Eq. (2), Eq. (4) and Eq. (7), we can obtain the
final training objective of CAIL as:
LCAIL = Ldis + λ1LUnSupCon + λ2LC-SupCon
(8)
where λ1 and λ2 are the hyperparameters to control the
contrastive loss ratio and Ldis =
1
N
PN
i=1 − log(hd(re
i )) −
log(1 − hd(ra
i )) in practice. In this study, we keep both λ1
and λ2 equal to 1.
Algorithm 1 Contrastive Adversarial Imitation Learning
Require:
Empty replay buffer B, Image encoder f, MLP trunk dh,
Policy network πθ, Expert observations De;
1: Initialize f, dh, and πθ;
2: for iter = 0, 1, 2, ... do
3:
Generate an episode {v1, a1, v2, a2, ...} with πθ
4:
Store {(vt, at, vt+1)} to replay buffer B
5:
Sample va ∼ B, ve ∼ De
6:
Update the discriminator formed by image encoder f
and MLP trunk hd with Eq. (8)
7:
Update πθ by off-policy RL method with Eq. (10)
8: end for
4.3
Overall Algorithm
The whole algorithm is summarized in Algorithm 1. The up-
date of the discriminator D → f ◦ hd has been introduced
in the above subsection. The policy learning part in CAIL
is based on the off-policy reinforcement learning method
DDPG [Silver et al., 2014]. In DDPG, the policy network
πθ and the critic Qϕ1 is optimized alternatively. To relieve
the overestimation during the critic learning, another critic
network Qϕ2 is introduced following the idea of double Q-
learning [Van Hasselt et al., 2016].
Value Iteration. The value iteration process in DDPG basi-
cally follows the Deep Q-Learning (DQN), which is a boost-
ing method that updates the critic networks Qϕ based on its
own predicted Q-value at the next state. The parameter ϕ is
learned by minimizing the Temporal-Difference (TD) error,
L(ϕ, B) = Et∼B

(Qϕi(v, a) − (r + γ(1 − d)Γ))2
,
(9)
∀i ∈ {1, 2}
where B denotes the replay buffer, t = (v, a, r, v′, d) is a
tuple with visual state v, action a, next visual state v′ and
done signal d. The reward r is provided by discriminator in
adversarial imitation learning. The estimation of next state’s
Q-value Γ is defined as Γ = (mini=1,2 Qϕi(v′, a′)), where
a′ ∼ πθ(a|s′) and ϕ is an exponential moving average of the
weights.
Policy Iteration. Since the critic Qϕ has been learned to in-
dicate the quality of the action, the policy network πθ thus
should be trained to maximize the expected return as
L(θ) = −E(v,a)∼πθ
h
min
i=1,2 Qϕi(v, a)
i
,
(10)
where action a = πθ(v) + ϵ, ϵ ∼ clip(N(0, σ2), −c, c) and σ
denotes the exploration noise. Following common practice,
we do not use actor gradient to update the parameter of the
image encoder f.
4.4
Convergence Analysis
We provide convergence analysis on the objective function
of CAIL since it adds contrastive regularizer term into the
framework of GAIL. We first show that the objective function
of CAIL has the same convergence point as GAIL in Theorem
1. Then, we provide analysis on the update of encoder f and
how the contrastive loss helps to improve the performance.
Theorem 1. Rewrite the objective function of CAIL as
minθ,f maxhd Jhd,f(θ), where
Jhd,f(θ) =Ev∼ρπe [log hd(f(v))]
(11)
− Ev∼ρπθ [log(1 − hd(f(v)))] + Ψ(f),
where Ψ(f) denotes the contrastive constraint. Regardless of
the encoder f, Lhd,f(θ) can converge with respect to θ, and
the convergence point is reached when ρπθ∗ = ρπe.
The proof of Theorem 1 is available in the supplementary
material. As stated in the theorem, Lhd,f(θ) can converge
to Lhd,f(θ∗) when given a fixed encoder f. Since the con-
trastive constraint Ψ(f) only applies on f, they will not af-
fect the convergence of agent policy πθ. While adversarial IL
often faces the challenge of unstable training and hard conver-
gence, the contrastive constraint helps to improve the ability
of representation in discriminator, which makes the training
more stable and converge fast. Empirical results also identify
the sample-efficient property of CAIL.
Figure 3: Benchmarking domains. Top: Cartpole, Finger, Hopper.
Bottom: Cheetah, Walker, and Quadruped.
5
Experiments
In this section, we conduct experiments to verify the effec-
tiveness of CAIL from different aspects. Evaluations are con-
ducted on pixel-based benchmark tasks on DeepMind Control
Suit (DMControl) [Tunyasuvunakool et al., 2020].
Setups. We evaluate CAIL on 9 MuJoCo [Todorov et al.,
2012] tasks in the DMControl Suite.
These tasks include
Cartpole Swingup, Finger Spin, Cheetah Run, Hopper Hop,
Hopper Stand, Walker Stand, Walker Walk, Walker Run and
Quadruped Run. As for expert demonstrations, we use the
public dataset in ROT [Haldar et al., 2022]. More details can
be found in the supplementary material.
Compared Methods.
We compare CAIL with the other
five methods:
Behavioral Cloning (BC), GAIL, GAIL
with Shared-Encoder (GAIL-SE) [Cohen et al., 2021],
PCIL [Huang et al., 2023] and PatchAIL [Liu et al., 2023].
BC learns a policy that maps visual states to actions via su-
pervised learning. GAIL adopts the policy network and the
discriminator as two independent models, while GAIL-SE
makes both policy network and discriminator network share
a common image encoder.
PCIL also utilizes the idea of
contrastive learning to enhance the representation. PatchAIL
exploits a patch discriminator during training and produces
patch rewards for the agent.
Table 1: Performance of CAIL and compared methods in 9 DeepMind Control tasks at both sample-efficient 500K and 1M steps. The agent
is measured by the average and standard variance of rewards along 10 trajectories (i.e., the higher the better). We repeat the experiments for
5 trials with different random seeds. In contrast to CAIL, CAIL(w/o cal) replaces the calibrated supervised contrastive loss with plain one.
500K Steps
GAIL
GAIL-SE
PCIL
CAIL(w/o cal)
CAIL
BC
Expert
Cartpole Swingup
186±20
734±160
296±52
790±61
838±4
521±120
859±0
Finger Spin
0±0
303±191
408±297
460±193
642±186
284±120
976±9
Cheetah Run
69±27
514±33
461±51
497±44
538±34
185±49
890±19
Hopper Hop
10±7
8±8
38±18
51±23
73±12
109±18
318±7
Hopper Stand
5±3
270±343
451±199
290±306
541±305
386±72
976±9
Walker Stand
272±95
513±286
930±18
910±44
961±9
496±70
939±9
Walker Walk
69±49
268±61
203±19
350±136
463±126
556±110
970±20
Walker Run
24±6
57±17
146±10
148±28
157±9
378±82
778±10
Quadruped Run
151±57
212±5
228±119
238±50
306±110
277±58
547±136
1M Steps
GAIL
GAIL-SE
PCIL
CAIL(w/o cal)
CAIL
BC
Expert
Cartpole Swingup
199±17
801±91
67±36
687±312
837±23
521±120
859±0
Finger Spin
0±0
407±250
534±233
704±122
785±99
284±120
976±9
Cheetah Run
84±30
624±35
662±27
689±37
725±31
185±49
890±19
Hopper Hop
0±0
121±24
158±8
184±25
182±20
109±18
318±7
Hopper Stand
5±3
747±63
733±104
754±106
777±42
386±72
976±9
Walker Stand
275±100
764±251
827±38
859±101
831±154
496±70
939±9
Walker Walk
63±34
953±3
952±0
940±13
938±6
556±110
970±20
Walker Run
28±8
133±28
519±59
468±66
526±48
378±82
778±10
Quadruped Run
115±60
296±82
427±35
322±38
382±27
277±58
547±136
Figure 4: Learning curves of CAIL and PatchAIL on 5 DMC tasks with respect to training time. We scale the training time and ‘1’ denotes
the time that CAIL completed 1M steps. It is obvious that given the same training time, CAIL can outperform PatchAIL.
5.1
Imitation Performance
The results are presented in Table 1. We can observe that
GAIL struggles to achieve desirable results in visual imitation
learning tasks, even in the simplest task Cartpole Swingup.
For tasks like Hopper Stand, the agent is not even able to learn
a better policy than random. These results align with previ-
ous research studies conducted in [Tucker et al., 2018]. By
adopting a shared image encoder for both policy and discrim-
inator, the agent trained by GAIL-SE achieves at least double
the reward as plain GAIL in all nine tasks. However, there
is still a notable gap in comparison to expert performance.
PCIL and CAIL improve the performance over GAIL base-
line greatly, which suggests that incorporating contrastive
representation enables the agent to converge to a better so-
lution. Our proposed method CAIL performs best at 500K
steps in all 9 tasks, which demonstrate the sample-efficient
property of CAIL. At 1M steps, CAIL and CAIL (w/o cal)
demonstrate best performance among 7 out of 9 tasks.
Compare with PatchAIL. PatchAIL is a state-of-the-art vi-
sual IL method that adopts a patch discriminator in the ad-
versarial training. While it exhibits impressive asymptotic
performance, it comes with a substantial increase in train-
ing cost compared to other methods. In Figure 6, we present
the training time and GPU memory usage of PatchAIL in the
Cartpole Swingup task. It shows that PatchAIL incurs a sig-
nificant overhead, with both training time and GPU memory
usage nearly doubling in comparison to GAIL. We provide
the learning curves of CAIL and PatchAIL under the same
training time budget in Figure 4, which shows that CAIL is
significantly computational efficient than PatchAIL. We de-
fer the asymptotic performance of CAIL and PatchAIL to the
supplementary material.
Visualization.
As shown in Figure 5, we visualized the
spatial attention map of the discriminator in two tasks by
Grad-CAM [Selvaraju et al., 2017]. We compare CAIL with
its closest baseline PCIL. The attention map visually high-
lights the areas that the discriminator prioritizes in decision-
making, with the red region indicating a more significant in-
fluence on the decision. From the figure, it is evident that
the attention map of both methods can focus on the joint of
agent’s body, which is intuitively important to distinguish dif-
ferent visual states. Compared to PCIL, the attention map of
CAIL has a better coverage on the joint of agent’s body in its
attention map. This visual interpretation suggests that CAIL
has a better ability in capturing the differences among differ-
ent visual states, thus makes it achieve better performance.
PCIL
CAIL (Ours)
Visual State
cheetah_run
hopper_stand
Figure 5: Spatial attention map of discriminator at 1M steps. The
map shows the region that the discriminators focus on to make the
decision.
Figure 6: Left: Impact of α. ‘HS’, ‘WS’, ‘WW’ and ‘QR’ denote
‘Hopper Stand’, ‘Walker Stand’, ‘Walker Walk’ and ‘Quadruped
Run’ respectively. Right: Training cost of CAIL and compared
methods measured by GPU memory usage and training time.
Impact of α. The parameter α represents prior information
indicating the probability of the agent’s demonstration being
‘positive’ during training. In this study, we examine how dif-
ferent values of α can affect the performance of the calibrated
contrastive loss in the experiment. Intuitively, at the begin-
ning of training, the agent demonstrations are more likely to
be ‘negative’ samples. As training progresses, the probabil-
ity of the agent demonstration being ‘positive’ increases ac-
cordingly. Hence, it is unsuitable to regard the agent demon-
strations as ‘negative’ samples. To address this problem, we
design a dynamic α that increases from 0.3 to 0.5 linearly
during training. The results of dynamic α are presented in
Figure 6. We observe that CAIL is relatively tolerant of dif-
ferent α values. Generally, using a dynamic α leads to better
performance in most cases, which aligns with our hypothe-
sis. We then apply the dynamic α to CAIL, which can also be
considered a method that does not require prior information.
5.2
Empirical Study on Contrastive Loss
Data Augmentation v.s. Contrastive Learning. Some ex-
isting works find that directly using strong augmentation in
model training can cause instability and hurt the performance
of the model [Jeong and Shin, 2021]. To verify this idea,
we apply the same data augmentation in GAIL-SE. The re-
sults are shown in Table 2. Except for the Walker Stand task,
we observe a fact that adding augmentations into GAIL-SE is
not very helpful to improve performance since it leads to an
Table 2: The comparison between data augmentation and contrastive
learning in GAIL-SE.
500K Steps
GAIL-SE
GAIL-SE
CAIL
w/o Aug
w/ Aug
(Ours)
Cartpole Swing
734
729±40(▼0.1%)
838±4 (▲14.2%)
Cheetah Run
514
533±34(▲3.7%)
538±34(▲4.7%)
Hopper Stand
270
206±89(▼23.3%) 541±305(▲100%)
Walker Stand
513
881±15(▲71.7%)
961±9(▲87.3%)
Walker Walk
268
200±5(▼25.4%) 463±126(▲72.8%)
Quadruped Run
212
222±74(▲4.7%) 306±110(▲44.3%)
Table 3: The performance of GAIL-SE and CAIL when using dif-
ferent data augmentation ways in the ‘HS’ task.
Augmentation
GAIL-SE
CAIL
No Aug
602±163
N/A
+Random Shift
676±130
777±42
+Random Crop
649±208
755±71
+Random Cutout
530±260
691±92
+Random Aug
524±165
549±115
average of 40.4% drops compared to the baseline. By con-
trast, we find the CAIL outperforms the baseline in these 5
tasks with an 47.2% improvement. Hence, we conclude that
using our calibrated contrastive loss incorporated with data
augmentation is better than directly applying data augmenta-
tion in adversarial imitation learning.
Different Augmentation Ways. In our experiments, we uti-
lize “Random-Shift” augmentation for producing different
views of demonstrations, which has been shown to be the
most useful data augmentation technique in pixel-based rein-
forcement learning problems [Laskin et al., 2020b]. To ver-
ify how other augmentation ways (e.g., Random Crop, Ran-
dom Cutout and Random Aug) may affect the performance
of adversarial imitation learning, we provide results in the
Hopper Stand task in Table 3. We find that CAIL can out-
perform GAIL-SE with all 4 different augmentations ways,
and using “Random-Shift” augmentation results in the best
performance. We also observe the fact that using some aug-
mentations ways can degrade the performance of AIL (e.g.,
Random Cutout and Random Aug).
6
Conclusion
In this work, we propose a novel solution to address the
challenge of visual adversarial imitation learning using con-
trastive learning. In high-dimensional visual states, it is diffi-
cult to capture changes in the agent’s behavior as easily as in
low-dimensional states. Therefore, a good representation is
essential to ensure the performance of visual imitation learn-
ing. Existing solutions either attempt to separate the process
of learning representation and decision-making or adopt com-
plex architectures to enhance the representation. In contrast,
we introduce contrastive learning to improve AIL’s ability to
discriminate between different demonstrations. Specifically,
three different contrastive losses are adopted to learn a good
image encoder for AIL. We evaluate the effectiveness of the
proposed method on the DMControl suite, and empirical re-
sults demonstrate its superiority from various aspects.
References
[Brantley et al., 2019] Kiante
Brantley,
Wen
Sun,
and
Mikael Henaff. Disagreement-regularized imitation learn-
ing. In International Conference on Learning Representa-
tions, 2019.
[Brown et al., 2019] Daniel Brown, Wonjoon Goo, Prabhat
Nagarajan, and Scott Niekum. Extrapolating beyond sub-
optimal demonstrations via inverse reinforcement learning
from observations.
In International conference on ma-
chine learning, pages 783–792. PMLR, 2019.
[Brown et al., 2020] Daniel S Brown, Wonjoon Goo, and
Scott Niekum. Better-than-demonstrator imitation learn-
ing via automatically-ranked demonstrations. In Confer-
ence on Robot Learning, pages 330–359, 2020.
[Cai et al., 2021] Xin-Qiang Cai, Yao-Xiang Ding, Yuan
Jiang, and Zhi-Hua Zhou. Imitation learning from pixel-
level demonstrations by hashreward. In Proceedings of the
20th International Conference on Autonomous Agents and
MultiAgent Systems, pages 279–287, 2021.
[Chen et al., 2020] Ting Chen, Simon Kornblith, Moham-
mad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In Inter-
national Conference on Machine Learning, 2020.
[Cobbe et al., 2019] Karl Cobbe, Oleg Klimov, Chris Hesse,
Taehoon Kim, and John Schulman. Quantifying general-
ization in reinforcement learning. In International Con-
ference on Machine Learning, pages 1282–1289. PMLR,
2019.
[Cohen et al., 2021] Samuel
Cohen,
Brandon
Amos,
Marc Peter Deisenroth, Mikael Henaff, Eugene Vinit-
sky, and Denis Yarats.
Imitation learning from pixel
observations for continuous control. 2021.
[Espeholt et al., 2018] Lasse Espeholt, Hubert Soyer, Remi
Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al.
Impala:
Scalable distributed deep-rl with importance
weighted actor-learner architectures. In International con-
ference on machine learning, pages 1407–1416. PMLR,
2018.
[Finn et al., 2015] Chelsea Finn, Xin Yu Tan, Yan Duan,
Trevor Darrell,
Sergey Levine,
and Pieter Abbeel.
Learning visual feature spaces for robotic manipula-
tion with deep spatial autoencoders.
arXiv preprint
arXiv:1509.06113, 25:2, 2015.
[Fu et al., 2017] Justin Fu, Katie Luo, and Sergey Levine.
Learning robust rewards with adversarial inverse rein-
forcement learning.
arXiv preprint arXiv:1710.11248,
2017.
[Goodfellow et al., 2014] Ian
Goodfellow,
Jean
Pouget-
Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014.
[Haarnoja et al., 2018] Tuomas
Haarnoja,
Aurick
Zhou,
Pieter Abbeel, and Sergey Levine.
Soft actor-critic:
Off-policy maximum entropy deep reinforcement learning
with a stochastic actor. In International conference on ma-
chine learning, pages 1861–1870. PMLR, 2018.
[Haldar et al., 2022] Siddhant Haldar, Vaibhav Mathur, De-
nis Yarats, and Lerrel Pinto.
Watch and match: Su-
percharging imitation with regularized optimal transport.
arXiv preprint arXiv:2206.15469, 2022.
[Ho and Ermon, 2016] Jonathan Ho and Stefano Ermon.
Generative adversarial imitation learning. In Advances in
neural information processing systems, pages 4565–4573,
2016.
[Huang et al., 2023] Jialei Huang, Zhao-Heng Yin, Ying-
dong Hu, and Yang Gao.
Policy contrastive imitation
learning. In International Conference on Machine Learn-
ing, pages 14007–14022. PMLR, 2023.
[Hussein et al., 2017] Ahmed Hussein, Mohamed Medhat
Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learn-
ing: A survey of learning methods. ACM Computing Sur-
veys (CSUR), 50(2):1–35, 2017.
[Jaderberg et al., 2019] Max Jaderberg, Wojciech M Czar-
necki, Iain Dunning, Luke Marris, Guy Lever, Anto-
nio Gurps Castaneda, Charles Beattie, Neil Rabinowitz,
Ari Morcos, Avraham Ruderman, et al. Human-level per-
formance in 3d multiplayer games with population-based
reinforcement learning. In Conference on Neural Informa-
tion Processing Systems, 2019.
[Jeong and Shin, 2021] Jongheon Jeong and Jinwoo Shin.
Training gans with stronger augmentations via contrastive
discriminator. arXiv preprint arXiv:2103.09742, 2021.
[Ke et al., 2020] Liyiming Ke, Sanjiban Choudhury, Matt
Barnes, Wen Sun, Gilwoo Lee, and Siddhartha Srinivasa.
Imitation learning as f-divergence minimization.
In In-
ternational Workshop on the Algorithmic Foundations of
Robotics, pages 313–329. Springer, 2020.
[Khosla et al., 2020] Prannay Khosla, Piotr Teterwak, Chen
Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised con-
trastive learning. arXiv preprint arXiv:2004.11362, 2020.
[Kostrikov et al., 2020] Ilya Kostrikov, Denis Yarats, and
Rob Fergus. Image augmentation is all you need: Reg-
ularizing deep reinforcement learning from pixels. arXiv
preprint arXiv:2004.13649, 2020.
[Laskin et al., 2020a] Michael Laskin, Aravind Srinivas, and
Pieter Abbeel. Curl: Contrastive unsupervised represen-
tations for reinforcement learning. In International Con-
ference on Machine Learning, pages 5639–5650. PMLR,
2020.
[Laskin et al., 2020b] Misha Laskin,
Kimin Lee,
Adam
Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas.
Reinforcement learning with augmented data. Advances in
neural information processing systems, 33:19884–19895,
2020.
[Lee et al., 2019] Kimin Lee, Kibok Lee, Jinwoo Shin, and
Honglak Lee.
Network randomization: A simple tech-
nique for generalization in deep reinforcement learning.
arXiv preprint arXiv:1910.05396, 2019.
[Liu et al., 2023] Minghuan Liu, Tairan He, Weinan Zhang,
Shuicheng Yan, and Zhongwen Xu. Visual imitation learn-
ing with patch rewards. arXiv preprint arXiv:2302.00965,
2023.
[Oord et al., 2018] Aaron van den Oord, Yazhe Li, and Oriol
Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
[Pomerleau, 1988] Dean A Pomerleau.
Alvinn:
An au-
tonomous land vehicle in a neural network. Advances in
neural information processing systems, 1, 1988.
[Puterman, 1994] Martin L. Puterman.
Markov Decision
Processes: Discrete Stochastic Dynamic Programming.
Wiley Series in Probability and Statistics. Wiley, 1994.
[Rafailov et al., 2021] Rafael Rafailov, Tianhe Yu, Aravind
Rajeswaran, and Chelsea Finn. Visual adversarial imita-
tion learning using variational models. Advances in Neural
Information Processing Systems, 34:3016–3028, 2021.
[Ross et al., 2011] St´ephane Ross, Geoffrey Gordon, and
Drew Bagnell. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Proceed-
ings of the fourteenth international conference on artificial
intelligence and statistics, pages 627–635, 2011.
[Schulman et al., 2015] John
Schulman,
Sergey
Levine,
Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust
region policy optimization. In International conference on
machine learning, pages 1889–1897, 2015.
[Schulman et al., 2017] John Schulman, Filip Wolski, Pra-
fulla Dhariwal, Alec Radford, and Oleg Klimov.
Prox-
imal policy optimization algorithms.
arXiv preprint
arXiv:1707.06347, 2017.
[Selvaraju et al., 2017] Ramprasaath R Selvaraju, Michael
Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi
Parikh, and Dhruv Batra. Grad-cam: Visual explanations
from deep networks via gradient-based localization.
In
Proceedings of the IEEE international conference on com-
puter vision, pages 618–626, 2017.
[Silver et al., 2014] David Silver, Guy Lever, Nicolas Heess,
Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. 2014.
[Silver et al., 2016] David Silver, Aja Huang, Chris J Maddi-
son, Arthur Guez, Laurent Sifre, George Van Den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al.
Mastering the game
of go with deep neural networks and tree search. nature,
529(7587):484–489, 2016.
[Sutton and Barto, 2018] Richard S Sutton and Andrew G
Barto.
Reinforcement learning: An introduction.
MIT
press, 2018.
[Todorov et al., 2012] Emanuel Todorov, Tom Erez, and Yu-
val Tassa.
Mujoco: A physics engine for model-based
control. In 2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems, pages 5026–5033. IEEE,
2012.
[Torabi et al., 2018] Faraz Torabi, Garrett Warnell, and Pe-
ter Stone.
Behavioral cloning from observation.
arXiv
preprint arXiv:1805.01954, 2018.
[Tu et al., 2022] Stephen Tu, Alexander Robey, Tingnan
Zhang, and Nikolai Matni.
On the sample complexity
of stability constrained imitation learning.
In Learning
for Dynamics and Control Conference, pages 180–191.
PMLR, 2022.
[Tucker et al., 2018] Aaron Tucker, Adam Gleave, and Stu-
art Russell.
Inverse reinforcement learning for video
games. arXiv preprint arXiv:1810.10593, 2018.
[Tunyasuvunakool et al., 2020] Saran Tunyasuvunakool, Al-
istair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh
Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and
Yuval Tassa. dm control: Software and tasks for continu-
ous control. Software Impacts, 6:100022, 2020.
[Van Hasselt et al., 2016] Hado Van Hasselt, Arthur Guez,
and David Silver. Deep reinforcement learning with dou-
ble q-learning. In Proceedings of the AAAI conference on
artificial intelligence, volume 30, 2016.
[Wang et al., 2021] Yunke Wang, Chang Xu, Bo Du, and
Honglak Lee. Learning to weight imperfect demonstra-
tions. In International Conference on Machine Learning,
pages 10961–10970. PMLR, 2021.
[Wang et al., 2023] Yunke Wang, Bo Du, and Chang Xu.
Unlabeled imperfect demonstrations in adversarial imita-
tion learning. arXiv preprint arXiv:2302.06271, 2023.
[Wen et al., 2021] Chuan Wen, Jierui Lin, Jianing Qian,
Yang Gao, and Dinesh Jayaraman. Keyframe-focused vi-
sual imitation learning.
In International Conference on
Machine Learning, pages 11123–11133. PMLR, 2021.
[Xu et al., 2020] Tian Xu, Ziniu Li, and Yang Yu.
Error
bounds of imitating policies and environments. Advances
in Neural Information Processing Systems, 33:15737–
15749, 2020.
[Yarats et al., 2021a] Denis Yarats, Rob Fergus, Alessandro
Lazaric, and Lerrel Pinto. Reinforcement learning with
prototypical representations. In International Conference
on Machine Learning, pages 11920–11931. PMLR, 2021.
[Yarats et al., 2021b] Denis
Yarats,
Amy
Zhang,
Ilya
Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus.
Improving sample efficiency in model-free reinforcement
learning from images.
In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 35, pages
10674–10681, 2021.
[Yuan and Kitani, 2018] Ye Yuan and Kris Kitani. 3d ego-
pose estimation via imitation learning.
In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 735–750, 2018.
[Zheng et al., 2022] Boyuan Zheng, Sunny Verma, Jianlong
Zhou, Ivor W Tsang, and Fang Chen. Imitation learning:
Progress, taxonomies and challenges. IEEE Transactions
on Neural Networks and Learning Systems, pages 1–16,
2022.
","nanRecent research in visual imitation learning has focused on enhancing the state representation. Some approaches introduce a reward function or image encoder as a pre-training step. However, these methods often require sophisticated training strategies and prohibit end-to-end training, limiting their applicability to specific tasks. PatchAIL takes a different approach by evaluating the local expertise of image patches, obtaining more discriminative representations. PCIL adds constraints to the discriminator to push expert states together and pull away agent states, improving its ability. Despite these advances, the performance of visual adversarial imitation learning still lags behind proprioceptive-based methods."
"Existing methods for 3D open-vocabulary scene understanding are deficient in their utilization of available information and lack fine-grained feature representation. To address these issues, we introduce UniM-OV3D, a unified multimodal network that aligns point clouds with image, language, and depth information. We propose a hierarchical point cloud extractor that captures local and global features, and a point-semantic caption learning mechanism that generates hierarchical captions from various 3D viewpoints for effective point-language supervision. Extensive experiments validate the superiority of UniM-OV3D on indoor and outdoor datasets, with significant improvements over previous state-of-the-art approaches.","Understanding 3D scenes is crucial for autonomous driving, virtual reality, and robot navigation. Existing models often struggle with novel categories not found in their training data, hindering their applicability in real-world scenarios. In this work, we propose UniM-OV3D, which aligns point clouds with multiple modalities, including image, language, and depth, using a unified framework. To capture comprehensive fine-grained features, we design a hierarchical point cloud extractor. Furthermore, we introduce hierarchical point-semantic caption pairs to generate captions directly from point clouds, enabling coarse-to-fine language supervision. Experiments on multiple datasets demonstrate the effectiveness of UniM-OV3D in open-vocabulary 3D scene understanding, outperforming state-of-the-art methods.","We define the objective of open-vocabulary 3D scene understanding as localizing and recognizing unseen categories without annotation. To achieve this, we propose UniM-OV3D, which consists of several components:

* **Hierarchical Feature Extractor with Local and Global Fusion:** We design a learnable hierarchical point cloud extractor that captures fine-grained local and global features. It uses spatial-aware layers with residual attention modules to fuse multi-scale features, enhancing information aggregation and feature learning.

* **Point-semantic Caption Learning:** To provide direct language supervision and capture fine-grained point-semantic information, we introduce a point-semantic caption generation mechanism. It includes global-, eye-, and sector-view point-semantic captions, offering coarse-to-fine supervision signals. A contrastive point-semantic caption training loss guides the 3D network to learn from vocabulary-rich language captions.

* **Aligning Multimodal Representations:** We align point clouds with image, depth, and text modalities by formulating dense associations across them. The image and text features are extracted using pre-trained encoders, and the depth encoder is learned during training. A contrastive loss function is employed between point clouds and other modalities, allowing them to align in a joint representation space, leveraging their synergistic advantages.","We evaluate UniM-OV3D on four public 3D benchmarks: ScanNet, ScanNet200, S3DIS, and nuScenes. The results are reported in terms of mean intersection over union (mIoU), harmonic mean IoU (hIoU), and mean average precision under 50% IoU threshold (mAP50) for both semantic and instance segmentation tasks. UniM-OV3D consistently outperforms previous state-of-the-art methods on various datasets and partitions. For semantic segmentation, UniM-OV3D achieves significant improvements over existing methods on ScanNet, ScanNet200, and S3DIS datasets, with hIoU improvements of 3.2%-7.8% and mIoU gains of 3.2%-5.4%. On nuScenes, UniM-OV3D also surpasses previous approaches, obtaining gains of 4.8%-6.4% in hIoU and 5%-5.6% in mIoU for novel categories. In instance segmentation, UniM-OV3D outperforms competing methods on ScanNet, S3DIS, nuScenes, and ScanNet200 datasets. These results demonstrate the effectiveness of our proposed approach in open-vocabulary 3D scene understanding tasks.","We introduce UniM-OV3D, a comprehensive multimodal alignment network for open-vocabulary 3D scene understanding. UniM-OV3D addresses the limitations of existing methods by leveraging a hierarchical point cloud extractor, hierarchical point-semantic caption pairs, and a unified multimodal alignment framework. Extensive experiments on various datasets and tasks show that UniM-OV3D outperforms previous state-of-the-art approaches, demonstrating its effectiveness in recognizing unseen categories and capturing fine-grained point-semantic information. Our work highlights the importance of comprehensive data integration and fine-grained feature representation for open-vocabulary 3D scene understanding, opening up new possibilities for addressing real-world 3D understanding challenges.",UniM-OV3D: Uni-Modality Open-Vocabulary 3D Scene Understanding with Fine-Grained Feature Representation,"Qingdong He, Jinlong Peng, Zhengkai Jiang, Kai Wu, Xiaozhong Ji, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Mingang Chen, Yunsheng Wu","UniM-OV3D: Uni-Modality Open-Vocabulary 3D Scene Understanding with
Fine-Grained Feature Representation
Qingdong He1∗ , Jinlong Peng1∗ , Zhengkai Jiang1 , Kai Wu1 , Xiaozhong Ji1 , Jiangning
Zhang1 , Yabiao Wang1 , Chengjie Wang1 , Mingang Chen2 and Yunsheng Wu1
1Tencent YouTu Lab
2Shanghai Development Center of Computer Software Technology
{yingcaihe, jeromepeng, zhengkjiang, lloydwu, xiaozhongji, vtzhang, caseywang,
jasoncjwang,simonwu}@tencent.com, cmg@sscenter.sh.cn
Abstract
3D open-vocabulary scene understanding aims to
recognize arbitrary novel categories beyond the
base label space. However, existing works not only
fail to fully utilize all the available modal informa-
tion in the 3D domain but also lack sufficient gran-
ularity in representing the features of each modal-
ity.
In this paper, we propose a unified mul-
timodal 3D open-vocabulary scene understanding
network, namely UniM-OV3D, which aligns point
clouds with image, language and depth. To bet-
ter integrate global and local features of the point
clouds, we design a hierarchical point cloud feature
extraction module that learns comprehensive fine-
grained feature representations. Further, to facil-
itate the learning of coarse-to-fine point-semantic
representations from captions, we propose the uti-
lization of hierarchical 3D caption pairs, capitaliz-
ing on geometric constraints across various view-
points of 3D scenes. Extensive experimental re-
sults demonstrate the effectiveness and superiority
of our method in open-vocabulary semantic and in-
stance segmentation, which achieves state-of-the-
art performance on both indoor and outdoor bench-
marks such as ScanNet, ScanNet200, S3IDS and
nuScenes. Code is available at https://github.com/
hithqd/UniM-OV3D.
1
Introduction
Accurate and resilient comprehension of 3D scenes plays
a vital role in various practical applications, including au-
tonomous driving, virtual reality and robot navigation. De-
spite significant progress in recognizing closed-set categories
on standard datasets [Graham et al., 2018; Vu et al., 2022;
Misra et al., 2021], existing models frequently struggle to
recognize novel categories that are not present in the training
data label space. The limited scalability of existing models in
open-set scenarios hinders their practical applicability in the
real world and motivates us to explore the open-vocabulary
3D scene understanding capability.
∗Equal contribution.
“device to watch the news”
“home furnishings for 
windows”
Input Point Cloud
Segmentation Results
User-specified Query
Input Point Cloud
“lying on the sofa”
UniM-OV3D(ours)
User-specified Query
OpenScene [Peng et al., 2023]
PLA [Ding et al., 2023]
OpenIns3D [Huang et al., 2023b]
(a)
(b)
Figure 1: Open-vocabulary 3D scene understanding. Different col-
ors represent the confidence of matching the user-specified query.
(a) Comparison of different methods using the same query, (b) Re-
sults of our method in complex reasoning or content that requires
extensive world knowledge.
Benefit from the 2D foundation model, such as CLIP [Rad-
ford et al., 2021] and DINO [Caron et al., 2021], previ-
ous methods [Peng et al., 2023; Chen et al., 2023; Liu et
al., 2023a; Liu et al., 2023b] attempt to distill knowledge
from 2D pixels to 3D features, aligning point clouds with
2D images.
Another line of works [Takmaz et al., 2023;
Huang et al., 2023b] focuses on points-only mechanism. The
existing methods inadequately investigate data from multi-
ple modalities, such as point clouds, images, linguistic in-
put, and depth information, which are crucial for effective
open-vocabulary 3D scene understanding. Despite their ef-
fectiveness, the absence of alignment and learning for other
modalities makes them miss a lot of semantic information and
hinders their ability to effectively handle fine-grained point
arXiv:2401.11395v1  [cs.CV]  21 Jan 2024
cloud object instances, as shown in Figure 1 (a).
Considering the properties of 3D scene, we should not
overlook the inherent characteristics of 3D data itself. Firstly,
depth information serves as a crucial modality for depth-
invariant feature aggregation, which is often neglected. Early
works [Zhang et al., 2022; Zhu et al., 2023; Huang et al.,
2023a] in 3D recognition have attempted to transfer depth
modality to 3D understanding. However, mere projection or
rendering, and merely aligning with a single modality such
as image or text, cannot fully exploit all the characteristics
of depth information, nor can it perform well in the task of
open-vocabulary 3D scene understanding. The second as-
pect to consider is the exploration of point cloud informa-
tion itself. Aiming at the point clouds caption learning, pre-
vious approaches either solely rely on simplistic templates
derived from CLIP [Peng et al., 2023; Liu et al., 2023a;
Zhang et al., 2023] or utilize images as a bridge to gener-
ate corresponding textual descriptions from 2D images [Ding
et al., 2023; Yang et al., 2023]. However, point clouds cap-
tioning can make alignment easier and more comprehensive,
particularly when multiple modalities are incorporated dur-
ing the alignment training. And the point clouds encoder al-
most comes from mature 3D extractors [Choy et al., 2019;
Graham et al., 2018], which often keep frozen during train-
ing. The frozen backbone and bridging alignment impede the
generalization in complex 3D open vocabulary scenarios.
To this end, in order to fully leverage the synergistic ad-
vantages of various modalities, we propose a comprehensive
multimodal alignment approach that co-embeds 3D points,
image pixels, depth, and text strings into a unified latent space
for open-vocabulary 3D scene understanding, namely UniM-
OV3D. We thoroughly explore and utilize the characteristics
of point clouds from two perspectives. First, to extract fine-
grained geometric features of different levels from the origi-
nal irregular 3D point clouds, we design a hierarchical point
cloud feature extraction module. Second, in terms of generat-
ing point-caption pairs, we make the first attempt to generate
corresponding text directly from point clouds instead of using
images as a bridge. And we build hierarchical point-semantic
caption pairs, including global-, eye- and sector-view cap-
tions, which can offer fine-grained language supervisions.
Furthermore, we introduce depth information into the en-
tire network and the modified depth encoder inspired by
CLIP2Point [Huang et al., 2023a] keeps learning during
training . As for image modality, we employ the pre-trained
model PointBIND [Guo et al., 2023] to extract image feature.
Given the multi-modalities and their dense representations,
we finally conduct multimodal contrastive learning for robust
3D understanding. In this way, we are capable of optimiz-
ing their reciprocal benefits by adeptly amalgamating multi-
ple modalities.
Extensive experiments on ScanNet [Dai et al., 2017], Scan-
Net200 [Rozenberszki et al., 2022], S3DIS [Armeni et al.,
2016] and nuScenes [Caesar et al., 2020] show the effective-
ness of our method on 3D open-vocabulary tasks, surpassing
previous state-of-the-art methods by 3.2%-7.8% hIoU on se-
mantic segmentation and 3.8%-10.8% hAP50 on instance seg-
mentation. More interestingly, UniM-OV3D demonstrates a
robust ability to understand complex language queries, even
those that involve intricate reasoning or necessitate extensive
world knowledge, as illustrated in Figure 1 (b).
Our main contributions are summarized as follows:
• Within a joint embedding space, UniM-OV3D firstly
aligns 3D point clouds with multi-modalities, includ-
ing 2D images, language, depth, for robust 3D open-
vocabulary scene understanding.
• In order to acquire more comprehensive and intricate
fine-grained geometric features from point clouds, we
propose a hierarchical point cloud extractor that effec-
tively captures both local and global features.
• We innovatively build hierarchical point-semantic cap-
tion pairs that offer coarse-to-fine supervision signals,
facilitating learning adequate point-caption representa-
tions from various 3D viewpoints directly.
• UniM-OV3D
outperforms
previous
state-of-the-art
methods on 3D open-vocabulary semantic and instance
segmentation tasks by a large margin, covering both
indoor and outdoor scenarios.
2
Related Work
2.1
3D Recognition
To process point clouds directly, PointNet [Qi et al., 2017a]
and PointNet++ [Qi et al., 2017b] are the two pioneering
studies in this field that focus on developing parallel MLPs
for feature extraction from unstructured data which have sig-
nificantly enhanced accuracy. Further, while the VLM [Rad-
ford et al., 2021; Caron et al., 2021] has achieved remarkable
results in zero-shot or few-shot learning of 2D images by uti-
lizing extensive image data, the availability of such internet-
scale data for 3D point clouds is limited. Consequently, ex-
tending the open-vocabulary mechanism to 3D perception
is a nontrivial task due to the difficulty in gathering suffi-
cient data for point-language contrastive training, similar to
the approach employed by CLIP [Radford et al., 2021] in
2D perception. Therefore, PointCLIPs [Zhang et al., 2022;
Zhu et al., 2023] take the first step to convert point clouds
into CLIP-recognizable images and align the point cloud
feature with the language feature by projecting 3D point
cloud into 2D space. Similarly, CLIP2Point [Huang et al.,
2023a] attempts to align the projected depth map with the
images by designing a trainable depth encoder and cross-
modality learning losses.
Different from point projecting,
a series of works [Xue et al., 2023; Zeng et al., 2023;
Liu et al., 2023b] try to collect multi-modalities triplets to
train the 3D backbone. To enable zero-shot capability and im-
prove standard 3D recognition, they align the 3D feature with
the CLIP-aligned visual and textual features. This alignment
facilitates the integration of 3D features with CLIP, enhanc-
ing both the zero-shot capability and the standard 3D recog-
nition capability. However, existing methods cannot integrate
the four modal information types, namely point cloud, image,
depth, and text, which is the focus of this paper. We propose
a unified multimodal network that incorporates fine-grained
feature representation from these four modalities, effectively
leveraging their strengths.
…
…
sector-view
Image 
Encoder
Text
Encoder
𝑓𝑝
𝑓𝑡
𝑓i𝑚𝑔
𝑓𝑑
Depth Maps
Images
Point Clouds
Transform Net
global-view
eye-view
The point cloud is an aerial view of a
small, cluttered apartment, including a
couch, a chair, and a bed. The couch is
located in the middle of the room……
Point-semantic Caption Learning
point-captioning model
Transformation 
Weight
Spatial-aware 
Layer 1
Spatial-aware 
Layer 2
Spatial-aware 
Layer n
…
Attn
C
Attn
Attn
Semantic/Instance 
Head
Depth 
Encoder
Learnable
Frozen
Multiplication
Attn
Attention Block
Uni-modality 
Representation Learning
Concatenation
C
Spatial-aware Layer
Point 
Embedding
Local 
PointBERT
Global
PointMAE
Attn
Attn
C
𝐻𝐿
𝑖
𝐻𝐺
𝑖
𝑓𝐺
𝑖
𝑓𝐿
𝑖
𝑓𝐺
𝑖−1
𝑓𝐿
𝑖−1
Add
Figure 2: Architecture of our proposed UniM-OV3D. The input point clouds are processed by a hierarchical point cloud extraction module
to fuse the local and global features. To fulfill coarse-to-fine text supervision signal, the point-semantic caption learning is designed to acquire
representations from various 3D viewpoints. The overall framework takes point clouds, 2D image, text and depth map as input to establish a
unified multimodal contrastive learning for open-vocabulary 3D scene understanding.
2.2
Open-Vocabulary 3D Scene Understanding
Facing the challenge of the lack of diverse 3D open-
vocabulary datasets for training generalizable models, one
solution is to distill knowledge between pre-trained 2D open-
vocabulary models and 3D models. OpenScene [Peng et al.,
2023] and CLIP-FO3D [Zhang et al., 2023] propose to distill
the knowledge of 2D CLIP into 3D feature without any ex-
tra annotation. Further, OpenMask3D [Takmaz et al., 2023]
first generates class-agnostic instance mask proposals with
the 3D point cloud, which is used to choose the 2D views
and masks.
Meanwhile, 3D-OVS [Liu et al., 2023a] dis-
tills the open-vocabulary multimodal knowledge and object
reasoning capability of CLIP into a neural radiance field.
OpenIns3D [Huang et al., 2023b] employs a “Mask-Snap-
Lookup” scheme to learn class-agnostic mask proposals in
3D point clouds. Another line focuses on point-language con-
trastive training. To obtain 3D backbone-language alignment,
PLA-family [Ding et al., 2023; Yang et al., 2023] uses 2D im-
ages as a bridge to generate descriptions of point cloud data.
Although carefully designed, describing point clouds through
images may not fully capture the details and point-semantic
information due to differences between point clouds and im-
ages. Therefore, methods that describe point cloud data more
directly and accurately need to be further explored. So we
establish a point-semantic caption learning mechanism, in
which captions can be obtained directly from point clouds
with more accurate coarse-to-fine languages supervisions.
3
Method
3.1
Preliminary
The objective of open-vocabulary 3D scene understanding is
to localize and recognize unseen categories without the need
for corresponding manual annotations. To put it formally,
annotations on semantic and instance levels, denoted as Y
= {yB, yN}, are split into base categories CB and novel cat-
egories CN. During the training phase, the 3D model has ac-
cess to all point clouds P = {p}, but only to annotations for
base classes CB. It remains unaware of both annotations yN
and category names related to novel classes CN. However,
during the inference process, the 3D model is required to lo-
calize objects and classify points belonging to both basic and
novel categories CB∪ CN. This inference step is crucial for
achieving comprehensive scene understanding and enabling
the model to generalize to unseen classes. During inference,
the model can predict any desired category by computing the
similarity between point-wise features and the embedding of
queried categories, enabling open-world inference.
3.2
Hierarchical Feature Extractor with Local and
Global Fusion
Taking the sparse point clouds as input, we propose a train-
able hierarchical point cloud extractor to capture fine-grained
local and global features, instead of just utilizing the frozen
3D extractors. As illustrated in Figure 2, the input is chan-
neled into a transformer network, which employs attention-
based layers to regress a 4×4 transformation matrix. This ma-
trix comprises the elements that represent the learned affine
transformation values, which are used for aligning point
clouds. Following alignment, the points are introduced into
multiple stacked spatial-aware layers, which serve to produce
a permutation-invariant embedding of these points. Within
this structure, the residual attention module functions as a
linking bridge between two adjacent layers, facilitating the
transfer of information. After the information has been pro-
cessed through the attention-based layers, the outputs from
all these N-dimensional layers are concatenated. Finally, the
segmentation head can be added to output the global infor-
mation aggregation for the point clouds, providing a compre-
hensive summary of the data.
Spatial-aware Layers.
The architecture of the spatial-
aware layers is shown in Figure 2, where the current i layer
acts as an intermediate layer and the features are not only
transmitted by the mainstream information flow but the resid-
ual attention linking from i − 1 layer. Consider a RD dimen-
sional embedding point clouds set with n points in the i layer
p = {pi,1, pi,2, ..., pi,n}. In each attention-based flow, the
local PointBERT [Yu et al., 2022] layer and the global Point-
MAE [Pang et al., 2022] layer process these points in parallel.
In the branch of the local PointBERT layer, a transformation
is employed on the points to get the feature Hi
L:
Hi
L = {hM
i,1, hM
i,2, ..., hM
i,g} = B(pi,1, pi,2, ..., pi,n)
(1)
where B : RD → RM and g is the local patch. After that,
the attention map from i − 1 layer by the residual linking is
added to the output Hi
L in a point-wise manner which can be
written as:
f i
L = Hi
L + ω1 ⊗ Hi−1
L
(2)
where ⊗ denotes element-wise multiplication. ω1 functions
as a feature filter, dampening the impact of noisy features
while amplifying the beneficial ones. Similarly, global Point-
MAE layer branch applies global transformation E on the
points p and the output from decoder denotes Hi
G. And the
final output after the linking of attention block is:
f i
G = Hi
G + ω2 ⊗ Hi−1
G
(3)
where ω2 is similar to ω1. The outputs f i
L and f i
G from the
two branches are of the same dimension and are transferred to
the attention block of the next layer. Ultimately, they are con-
catenated together, and these combined embedding are used
as the input for the subsequent layer.
The attention block consists of a series of residual units [He
et al., 2016], inserted with interpolation to form a symmetri-
cal top-down architecture. Two consecutive 1×1 convolution
layers are connected at the end to balance the dimensions.
More details are presented in the supplementary.
3.3
Point-semantic Caption Learning
Recent success of open-vocabulary works in 2D [Bangalath
et al., 2022; Wu et al., 2023] and 3D [Ding et al., 2023;
Yang et al., 2023] vision has shown the effectiveness of in-
troducing language supervision to guide the model to ac-
cess abundant semantic concepts with a large vocabulary size.
However, utilizing images as a bridge to associate the points
with language is the core idea in these 3D works. And the
final textual descriptions in the so-called point-caption pairs
are all sourced from image descriptions and do not possess
semantic information specific to the point cloud. To address
this problem, we propose to provide direct language super-
vision and a coarse-to-fine point-semantic caption generation
mechanism is designed.
The point cloud is a top-down view of a bathroom,
showcasing the layout and design of the space. The
bathroom features a toilet, a sink, and a bathtub. The
toilet is located on the right side of the bathroom,
while the sink is positioned on the left side. The
bathtub is situated in the middle of the room.
In addition to the main fixtures, there are several
toilet paper rolls scattered throughout the bathroom.
Some of these rolls are located near the toilet, while
others are placed closer to the sink. The presence of
multiple toilet paper rolls adds a unique touch to the
bathroom's overall design.
This is a 3D model of a room that has been destroyed.
The room is in disarray, with various furniture and
items scattered throughout the space. There are
several chairs, some of which are placed near the
center of the room, while others are closer to the
edges. A couch can be seen in the middle of the room,
and a bed with a trash can is located towards the
right side.
In addition to the furniture, there are several other
objects in the room, including a bottle on the left side
and a cup on the right side, a suitcase sitting on the
top of a wooden table. The overall scene gives the
impression of a catastrophic event that has occurred,
leaving the room in a state of destruction.
A white toilet sitting to a
toilet paper dispenser. a
white towel hanging on the
side of a bathroom door. a
small
bathroom
with
a
toilet
and
a
sink.
a
bathroom stall with a toilet
and a roll of toilet paper.
A black suitcase sitting on a
wooden
table.
a
black
backpack
with
a
black
handle on a wooden floor.
a small room with a bed
and a trash can. a cluttered
room with many items on
the floor. A black and white
photo of a living room.
point-cloud based caption
input point cloud
image-based caption
Figure 3: Point-caption pairs comparison.
The middle column
represents the captions generated by the point-captioning model,
PointLLM [Xu et al., 2023], and the third column represents the
corresponding image captions.
Global-view Point-semantic Caption.
Due to the similar-
ity of point-captioning and image captioning [Mokady et al.,
2021; Wang et al., 2022] as fundamental tasks, various base
models [Xu et al., 2023; Guo et al., 2023] trained on a large
number of 3D point samples are already available for per-
ceiving 3D scenes. Given the ith 3D point cloud scene pi,
the simplest and most rudimentary manner is to link lan-
guage supervision to all points. Specifically, the correspond-
ing global language description tglobal
i
can be generated by
the pre-trained point-captioning model G as follows,
tglobal
i
= G(pi).
(4)
Surprisingly, with the semantic understanding on the overall
points, the entities covered in the generated captions have al-
ready encompassed the entire 3D semantic label space. As
illustrated in Figure 3, the middle column represents the cap-
tions generated by the point-captioning model, and the third
column represents the corresponding image captions. It is ob-
servable that the global point-captions not only offer a more
precise and holistic depiction of the scene, but they also rep-
resent the orientation information of objects more accurately
within the scene and the interrelationships among them. De-
spite the simplicity of global-view caption, we find that
it can significantly improve the whole open vocabulary capa-
bilities of understanding the scene and locating objects.
Hierarchical Point-semantic Caption.
Diving into the ex-
ploration of more fine-grained point-semantic caption learn-
ing, global-view point cloud captions are still too coarse
to capture fine-grained details between objects in point
clouds, making them suboptimal for 3D scene understand-
ing tasks. Hierarchical point captions can further refine the
semantic information of point clouds without ignoring all ob-
ject features in the scene. Therefore, we propose two other
association patterns on point sets at different 3D viewpoints.
For each 3D point cloud scene pi, we initiate from the
x-axis and segment the entire point cloud space into sector-
shaped regions with an angular magnitude of θ degree, where
the intersecting angle between adjacent sectors is ϕ degree.
The segmented 3D scene ps
i can be denoted as:
ps
i = {pθ ϕ
ij
| 0 ≤ ϕ < θ, θ > 0, 0 < j < ⌈360
θ ⌉}.
(5)
For the partitioned sector-shaped regions pθ ϕ
ij , we catego-
rize them into two distinct types of viewpoints based on
the magnitude of the angle, named as sector-view and
eye-view. The corresponding angle size of the two views
is θ1 and θ2, where 0 < θ1 ≤ 90 and 90 < θ2 ≤ 180. This
implies that the sector-view focuses on the minutiae of
local point cloud information, while the eye-view concen-
trates on a broader point cloud area, yet the extent of this area
is within an obtuse angle range, akin to human eyes.
Contrastive Point-semantic Caption Training.
With the
obtained global-,
eye- and sector-view point-
semantic captions from different views, we can conduct
point-language feature contrastive learning to guide the 3D
network to learn from vocabulary-rich language supervisions.
Specifically, we extract the text embeddings f t
i based on
the pre-aligned and fixed text encoder in PointBIND [Guo et
al., 2023]. And the 3D feature f p
i can be acquired by the
trainable 3D point cloud encoder. We formulate the point-to-
text alignment using the contrastive loss as:
Lm
capt = − 1
N
N
X
i=1
log
exp(f p
i · f t
i /τ)
PN
j=0 exp(f p
i · f t
j/τ)
(6)
where m ∈ {global, eye, sector}, N is the total number of
the point-semantic caption pairs and τ is a learnable temper-
ature parameter. With Equation 6, we can easily compute dif-
ferent caption losses on global-view Lglobal
capt , eye-view
Leye
capt and sector-view Lsector
capt . The final caption loss is
a weighted combination between different views as follows:
Ltotal
capt = αLglobal
capt
+ βLeye
capt + γLsector
capt
(7)
where α, β and γ are used to balance the relative importance
of different parts and are set to 1, 0.8, 0.8 by default. The
effect of different views and the comparison of different com-
binations are shown in ablation studies.
3.4
Aligning Multimodal Representations
With the construction of the four modalities, the subsequent
objective of UniM-OV3D is to conduct dense alignment be-
tween the 3D points with their corresponding image, depth
and text.
For the paired image-text (I, t) data, we utilize their cor-
responding encoders from PointBIND [Guo et al., 2023] for
feature extraction, which are kept frozen during training. For
each depth map D, we set the gated aggregator weights to 0 in
CLIP2Point [Huang et al., 2023a] to extract the depth feature,
formulated as
f t, f img = PointBIND(t, I)
f d = Ed(D)
(8)
where f t, f img, f d denote the text, image and depth embed-
dings.
And the depth encoder Ed keeps learnable during
training. For the 3D point clouds, we adopt the designed hi-
erarchical point cloud feature extractor module to extract 3D
features for the point cloud. In order to transform the encoded
3D feature into multi-modal embedding space, we append a
projection network after the 3D encoder Ep. So the final 3D
features f p can be formulate as :
f p = Proj(Ep(p)).
(9)
Dense Associations across Modalities.
As shown in Fig-
ure 2, with a 3D scene and its corresponding features f p, f t,
f img, f d, we employ a contrastive loss between point clouds
and other modalities, which effectively compels the 3D em-
beddings to align with the joint representation space, formu-
lated as:
L(M1,M2) =
X
i,j
−1
2log
exp(f M1
i
· f M2
j
/ϵ)
P
k exp(f M1
i
· f M2
k
/ϵ)
−1
2log
exp(f M1
i
· f M2
j
/ϵ)
P
k exp(f M1
k
· f M2
j
/ϵ)
(10)
where
M1
and
M2
represent
two
modalities
in
(point, image, depth)
and
(i, j)
indicates
a
positive
pair in each training batch. And ϵ is a learnable temperature
parameter, similar to CLIP [Radford et al., 2021].
Finally, the overall objective function to perform unified
multimodal alignment is defined by:
Loverall = L(P,I) + L(P,D) + L(D,I) + Ltotal
capt
(11)
where the language modality delivers comprehensive and
scalable textual descriptions, while the image modality pro-
vides accurate guidance on object edges and contextual data.
Moreover, the depth and 3D modality exposes vital struc-
tural details of objects. By aligning these modalities jointly
in a common space, our approach can maximize the syner-
gistic advantages among them, resulting in superior open-
vocabulary scene understanding performance.
4
Experiments
4.1
Setup
Datasets and Metrics.
To validate the effectiveness of our
proposed UniM-OV3D, we conduct extensive experiments on
four popular public 3D benchmarks: ScanNet [Dai et al.,
2017], ScanNet200 [Rozenberszki et al., 2022], S3DIS [Ar-
meni et al., 2016] and nuScenes [Caesar et al., 2020]. The
first three provide RGBD images and 3D meshes of indoor
scenes, while the last one supplies Lidar scans of outdoor
scenes.
We use all four datasets to compare to alterna-
tive methods, covering both semantic and instance segmen-
tation. For semantic segmentation, we employ the commonly
used 3D segmentation metric mean intersection over union
(mIoUB, mIoUN ) and harmonic mean IoU (hIoU) for eval-
uating base, novel categories and their harmonic mean sep-
arately. For instance segmentation, We apply mean average
precision under 50% IoU threshold (mAPB
50, mAPN
50, hAP50)
as major indicators.
Method
Scannet
S3DIS
B15/N4
B12/N7
B10/N9
B8/N4
B6/N6
hIoU
mIoUB
mIoUN
hIoU
mIoUB
mIoUN
hIoU
mIoUB
mIoUN
hIoU
mIoUB
mIoUN
hIoU
mIoUB
mIoUN
LSeg-3D [Li et al., 2022]
0.0
64.4
0.0
0.9
55.7
0.1
1.8
68.4
0.9
0.1
49.0
0.1
0.0
30.1
0.0
3DGenZ [Michele et al., 2021]
20.6
56.0
12.6
19.8
35.5
13.3
12.0
63.6
6.6
8.8
50.3
4.8
9.4
20.3
6.1
3DTZSL [Cheraghian et al., 2020]
10.5
36.7
6.1
3.8
36.6
2.0
7.8
55.5
4.2
8.4
43.1
4.7
3.5
28.2
1.9
PLA [Ding et al., 2023]
65.3
68.3
62.4
55.3
69.5
45.9
53.1
76.2
40.8
34.6
59.0
24.5
38.5
55.5
29.4
OpenScene [Peng et al., 2023]†
67.1
68.8
62.8
56.8
61.5
51.7
55.7
71.8
43.6
39.1
58.6
33.2
41.2
56.2
36.4
RegionPLC [Yang et al., 2023]
69.4
68.2
70.7
68.2
69.9
66.6
64.3
76.3
55.6
-
-
-
-
-
-
OpenIns3D [Huang et al., 2023b]†
72.6
71.8
73.4
69.3
70.7
68.8
64.5
80.2
49.8
46.8
63.2
38.6
53.6
60.8
47.5
UniM-OV3D (Ours)
75.8
75.3
76.6
74.7
75.2
74.1
69.9
83.5
57.3
54.6
68.3
45.2
59.1
66.7
54.2
Table 1: Results for open-vocabulary 3D semantic segmentation on ScanNet and S3DIS. The evaluation metrics are hIoU, mIoUB and
mIoUN . † denotes results reproduced by us on its official implementation. Best open-vocabulary results are highlighted in bold.
Method
ScanNet200
B170/N30
B150/N50
hIoU
mIoUB
mIoUN
hIoU
mIoUB
mIoUN
3DGenZ [Michele et al., 2021]
2.6
15.8
1.4
3.3
14.1
1.9
3DTZSL [Cheraghian et al., 2020]
0.9
4.0
0.5
0.7
3.8
0.4
LSeg-3D [Li et al., 2022]
1.5
21.1
0.8
3.0
20.6
1.6
PLA [Ding et al., 2023]
11.4
20.9
7.8
10.1
20.9
6.6
OpenScene [Peng et al., 2023]†
15.3
22.5
10.4
16.8
23.5
11.2
RegionPLC [Yang et al., 2023]
16.6
21.6
13.9
14.6
22.4
10.8
OpenIns3D [Huang et al., 2023b]†
17.2
24.6
13.6
18.9
25.8
16.4
UniM-OV3D (Ours)
22.3
29.5
17.1
25.8
30.7
21.6
Table 2: Results for open-vocabulary 3D semantic segmentation on
ScanNet200 on hIoU, mIoUB and mIoUN . † denotes results repro-
duced by us on its official implementation.
Category Partition.
ScanNet densely annotates 20 classes,
ScanNet200 owns 200 classes, S3DIS contains 13 classes and
nuScenes involves 16 classes. Following [Ding et al., 2023;
Yang et al., 2023], we disregard the “otherfurniture” class
in ScanNet and randomly partition the rest 19 classes into
3 base/novel partitions, i.e. B15/N4 (15 base and 4 novel
categories), B12/N7 and B10/N9, for semantic segmentation.
Following SoftGroup [Vu et al., 2022] to exclude two back-
ground classes, we acquire B13/N4, B10/N7, and B8/N9 par-
titions for instance segmentation on ScanNet. Similarly, we
ignore the “clutter” class in S3DIS and get B8/N4, B6/N6
for both semantic and instance segmentation.
For Scan-
Net200, we split 200 classes to B170/N30 and B150/N50. For
nuScenes, we drop the “otherflat” class and obtain B12/N3
and B10/N5.
4.2
Main Results
To validate the effectiveness of our proposed UniM-OV3D,
we compare it with previous state-of-the-art 3D open-
vocabulary methods on semantic and instance segmentation
tasks.
3D Semantic Segmentation.
As shown in Table 1, com-
pared to previous captioning-learning based method Re-
gionPLC [Yang et al., 2023], we obtain 5.1%-13% mIoU
performance gains among different partitions on ScanNet.
Compared with previous zero-shot state-of-the-art method
OpenIns3D [Huang et al., 2023b], our method still obtains
3.2%-5.4% and 5.5%-7.8% improvements in terms of hIoU
across various datasets and partitions. It is noteworthy that
our approach consistently improves performance across dif-
ferent datasets and partitions. Moreover, for partitions with
Method
nuScenes
B12/N3
B10/N5
hIoU mIoUB mIoUN
hIoU mIoUB mIoUN
3DGenZ [Michele et al., 2021]
1.6
53.3
0.8
1.9
44.6
1.0
3DTZSL [Cheraghian et al., 2020]
1.2
21.0
0.6
6.4
17.1
3.9
LSeg-3D [Li et al., 2022]
0.6
74.4
0.3
0.0
71.5
0.0
PLA [Ding et al., 2023]
47.7
73.4
35.4
24.3
73.1
14.5
OpenScene [Peng et al., 2023]†
55.6
75.4
39.6
28.5
75.8
19.2
RegionPLC [Yang et al., 2023]
64.4
75.8
56.0
49.0
75.8
36.3
OpenIns3D [Huang et al., 2023b]†
65.4
77.3
59.6
49.7
78.7
39.2
UniM-OV3D (Ours)
70.2
80.5
64.6
55.1
81.7
44.8
Table 3: Results for open-vocabulary 3D semantic segmentation on
nuScenes in terms of hIoU, mIoUB and mIoUN . † denotes results
reproduced by us on its official implementation.
𝑚𝐴𝑃50
𝑁
ℎ𝐴𝑃
𝑚𝐴𝑃50
𝐵
(a) B13/N14
𝑚𝐴𝑃50
𝑁
ℎ𝐴𝑃
𝑚𝐴𝑃50
𝐵
(b) B10/N7
𝑚𝐴𝑃50
𝑁
ℎ𝐴𝑃
𝑚𝐴𝑃50
𝐵
(c) B8/N9
PLA
OpenIns3D
OpenScene
RegionPLC
UniM-OV3D(ours)
𝑚𝐴𝑃50
𝑁
ℎ𝐴𝑃
𝑚𝐴𝑃50
𝐵
(a) B13/N14
𝑚𝐴𝑃50
𝑁
ℎ𝐴𝑃
𝑚𝐴𝑃50
𝐵
(b) B10/N7
𝑚𝐴𝑃50
𝑁
ℎ𝐴𝑃
𝑚𝐴𝑃50
𝐵
(c) B8/N9
PLA
OpenIns3D
OpenScene
RegionPLC
UniM-OV3D(ours)
𝑚𝐴𝑃50
𝑁
𝑚𝐴𝑃50
𝐵
(a) B13/N4 on ScanNet
𝑚𝐴𝑃50
𝑁
𝑚𝐴𝑃50
𝐵
(b) B8/N4 on S3DIS
𝑚𝐴𝑃50
𝑁
𝑚𝐴𝑃50
𝐵
(c) B12/N3 on nuScenes
𝑚𝐴𝑃50
𝑁
𝑚𝐴𝑃50
𝐵
(d) B170/N30 on Scannet200 
PLA
OpenIns3D
OpenScene
RegionPLC
UniM-OV3D(ours)
ℎ𝐴𝑃50
ℎ𝐴𝑃50
ℎ𝐴𝑃50
ℎ𝐴𝑃50
Figure 4: Comparisons on open-vocabulary 3D instance segmenta-
tion. (a) B13/N4 on ScanNet, (b) B8/N4 on S3DIS, (c) B12/N3 on
nuScenes and (d) B170/N30 on ScanNet200.
more novel categories, our method shows even greater en-
hancement. This further demonstrates the robustness and ef-
fectiveness of our approach in open vocabulary scene under-
standing tasks.
When facing the long-tail problems in ScanNet200, as
shown in Table 2, our method still surpasses correspond-
ing zero-shot method by 5.1%-6.9% hIoU on both splits and
3.5%-5.2% mIoU on novel categories. Furthermore, when
evaluating the performance of our UniM-OV3D on outdoor
setting, our method lifts the hIoU and mIoU of novel cat-
egories by 4.8%-6.4% and 5%-5.6% as shown in Table 3.
In this regard, UniM-OV3D demonstrates superior general-
izability to complex 3D open-world scenarios.
3D Instance Segmentation.
Our pipeline not only offers
point-semantic language descriptions for fine-grained point
Uni
HFE
PCL
B15/N4
B12/N7
hIoU
mIoUB
mIoUN
hIoU
mIoUB
mIoUN
-
-
-
46.2
57.1
42.3
43.5
58.6
41.0
-
-
✓
62.1
66.2
56.4
59.6
67.9
55.3
-
✓
-
60.0
65.8
56.2
59.2
66.5
55.1
✓
-
-
60.3
65.4
57.6
58.7
66.2
56.5
✓
✓
-
72.9
72.8
75.3
72.6
73.5
71.3
✓
-
✓
72.6
72.1
75.1
71.9
73.6
71.8
-
✓
✓
74.8
74.5
76.0
73.9
74.6
73.1
✓
✓
✓
75.8
75.3
76.6
74.7
75.2
74.1
Table 4: Ablations of different components of our network. Uni de-
notes the uni-modalities, HFE is the hierarchical point cloud feature
extractor and PCL represents the point-semantic caption learning.
Fusion Method
B15/N4
B12/N7
hIoU
mIoUB
mIoUN
hIoU
mIoUB
mIoUN
Local PB
72.8
72.4
73.0
71.6
71.8
71.3
Global PM
73.1
72.6
73.2
71.7
72.0
71.4
Local PB + Global PM
75.0
74.8
75.3
73.7
73.8
73.2
Local PB + Attn
74.8
74.6
75.0
73.6
73.8
73.1
Global PM+ Attn
74.9
74.8
75.2
73.8
74.3
73.5
Local PB + Global PM + Attn
75.8
75.3
76.6
74.7
75.2
74.1
Table 5: Ablations of different fusion methods. Local PB denotes
local PointBERT, Global PM is the global PointMAE and Attn rep-
resents the attention block.
clouds, but also encourages points to learn discriminative fea-
tures, which in turn benefits the instance-level localization
task.
As shown in Figure 4, by learning a uni-modalities
feature representations, our method outperforms Region-
PLC [Yang et al., 2023] by 4.1% hAP50, 5.3% mAPB
50, 3%
mAPN
50 on B13/N4 on the ScanNet. On the other datasets, our
UniM-OV3D surpasses OpenIns3D [Huang et al., 2023b] by
5%-10.6% hAP50, 5%-5.3% mAPB
50 and 3.1%-13.2% mAPN
50
on their respective B8/N4, B12/N3 and B170/N30 partitions
(RegionPLC only reports their instance segmentation results
on ScanNet without open-source code). This illustrates the
efficacy of our UniM-OV3D in empowering the model to
identify unseen instances without the need for human annota-
tions. Due to the space limition, specific indicators and more
instance segmentation results are presented in the supplemen-
tary.
4.3
Ablation Studies
In this section, we change components and variants of
our proposed UniM-OV3D by conducting extensive ablation
studies on ScanNet dataset for semantic segmentation by de-
fault. And the partitions are mainly B15/N4 and B12/N7.
Analysis of Different Components.
We illustrate the im-
portance of different components of our network by remov-
ing some parts and keeping all the others unchanged. The
baseline setting is to only use three modalities of image, text
and point cloud. The 3D point cloud encoder uses sparse
convolutional UNet [Choy et al., 2019]. The point captions
come from corresponding image captions, and we use CLIP
as the image and text encoder. As shown in Table 4, using the
depth map to construct the uni-modality learning contributes
much to the whole performance which proves the necessity
of establishing a unified modal architecture.
Without the
point-semantic captions from different view, the performance
Angle
B15/N4
B12/N7
hIoU
mIoUB
mIoUN
hIoU
mIoUB
mIoUN
sector-view
30
66.5
68.4
62.1
67.7
69.3
61.3
45
69.1
69.4
68.6
68.3
70.2
66.3
60
72.3
72.3
74.2
70.2
73.3
69.2
90
72.6
72.3
73.9
70.1
73.2
69.1
eye-view
120
73.1
72.4
74.3
71.3
73.4
70.5
180
71.9
71.3
73.6
71.1
73.1
70.0
global-view
360
72.8
72.7
75.3
72.6
73.5
71.3
combined-view
360+60
73.3
73.2
75.6
73.5
74.2
72.6
360+120
74.2
74.1
75.7
73.6
74.3
72.7
360+60+120
75.8
75.3
76.6
74.7
75.2
74.1
Table 6: Performance of splitting point clouds into different angles
and combining different views.
Encoder
CLIP
ImageBIND
PointBIND
hIoU/mIoUB/mIoUN
73.7/71.5/73.6
74.8/74.6/75.4
75.8/75.3/76.6
Table 7: Performance of different image and text encoder on B15/N4
on ScanNet.
drops dramatically which shows that coarse-to-fine supervi-
sion signal from fine-grained point-semantic caption learning
is indeed a crucial factor. And the performance degradation
caused by the absence of hierarchical point cloud extractor
proves that only the combination of local and global features
can produce the best results.
Comparison of Different Fusion Methods in Hierarchical
Feature Extractor.
To validate the effect of different fusion
methods in the hierarchical point cloud feature extractor, we
design six fusion patterns as shown in Table 5. With the atten-
tion block to transfer features between connected layers, the
fused models override the original ones by 0.8%-2.3% IoU.
And our final fusion method of global and local with attention
block outperforms the alternative by 0.5%-1.4% mIoU.
Effect of Different Point-semantic Caption Views.
In or-
der to explore the effect of dividing the point cloud into dif-
ferent angles in Section 3.3 to obtain the corresponding fine-
grained point-semantic description, we split the point cloud
into six angles for experiments. As shown in Table 6, the
120 degree in eye-view and 60 degree in sector-view
perform relatively well, but they are both slightly inferior to
global-view (360 degree). So, in order to further ob-
tain the coarse-to-fine supervision, we combine 60 degree and
120 degree with 360 degree respectively, and finally adopt the
method of combining the three in the last row.
Text Encoder Selection.
To extract the image and text
embeddings, we experiment with different vision-language
pre-trained encoders, CLIP [Radford et al., 2021], Image-
Bind [Girdhar et al., 2023] and PointBIND [Guo et al., 2023].
As shown in Table 7, PointBIND exceeds the others which
demonstrates that pre-training on point modality can provide
better embeddings for 3D scene understanding.
5
Conclusion
In this paper, we propose UniM-OV3D, a unified muitimodal
network for open-vocabulary 3D scene understanding. We
jointly establish dense alignment between four modalities,
i.e., point clouds, images, depth and text, to leverage their
respective advantages. To fully capture local and global fea-
tures of point clouds, we stack spatial-aware layers to con-
struct a hierarchical point cloud extractor. Furthermore, we
deeply explore the possibility of directly using point clouds
to generate corresponding captions, and the established point-
semantic learning mechanism can provide language supervi-
sion signals more effectively. Extensive experiments on both
indoor and outdoor datasets demonstrate the superiority of
our model in 3D open-vocabulary scene understanding task.
References
[Armeni et al., 2016] Iro Armeni, Ozan Sener, Amir R Za-
mir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Sil-
vio Savarese. 3d semantic parsing of large-scale indoor
spaces. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pages 1534–1543,
2016.
[Bangalath et al., 2022] Hanoona Bangalath,
Muhammad
Maaz, Muhammad Uzair Khattak, Salman H Khan, and
Fahad Shahbaz Khan. Bridging the gap between object
and image-level representations for open-vocabulary de-
tection. Advances in Neural Information Processing Sys-
tems, 35:33781–33794, 2022.
[Caesar et al., 2020] Holger Caesar, Varun Bankiti, Alex H
Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush
Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.
nuscenes: A multimodal dataset for autonomous driving.
In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pages 11621–11631, 2020.
[Caron et al., 2021] Mathilde Caron, Hugo Touvron, Ishan
Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vi-
sion transformers. In Proceedings of the IEEE/CVF in-
ternational conference on computer vision, pages 9650–
9660, 2021.
[Chen et al., 2023] Runnan Chen, Youquan Liu, Lingdong
Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou,
Yu Qiao, and Wenping Wang. Clip2scene: Towards label-
efficient 3d scene understanding by clip. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 7020–7030, 2023.
[Cheraghian et al., 2020] Ali Cheraghian, Shafin Rahman,
Dylan Campbell, and Lars Petersson. Transductive zero-
shot learning for 3d point cloud classification. In Proceed-
ings of the IEEE/CVF winter conference on applications
of computer vision, pages 923–933, 2020.
[Choy et al., 2019] Christopher Choy, JunYoung Gwak, and
Silvio Savarese. 4d spatio-temporal convnets: Minkowski
convolutional neural networks.
In Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition, pages 3075–3084, 2019.
[Dai et al., 2017] Angela Dai, Angel X Chang, Manolis
Savva, Maciej Halber, Thomas Funkhouser, and Matthias
Nießner.
Scannet: Richly-annotated 3d reconstructions
of indoor scenes. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 5828–
5839, 2017.
[Ding et al., 2023] Runyu Ding, Jihan Yang, Chuhui Xue,
Wenqing Zhang, Song Bai, and Xiaojuan Qi.
Pla:
Language-driven open-vocabulary 3d scene understand-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 7010–7019,
2023.
[Girdhar et al., 2023] Rohit Girdhar, Alaaeldin El-Nouby,
Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Ar-
mand Joulin, and Ishan Misra.
Imagebind: One em-
bedding space to bind them all.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 15180–15190, 2023.
[Graham et al., 2018] Benjamin Graham, Martin Engelcke,
and Laurens Van Der Maaten. 3d semantic segmentation
with submanifold sparse convolutional networks. In Pro-
ceedings of the IEEE conference on computer vision and
pattern recognition, pages 9224–9232, 2018.
[Guo et al., 2023] Ziyu Guo, Renrui Zhang, Xiangyang Zhu,
Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen,
Peng Gao, Xianzhi Li, Hongsheng Li, et al. Point-bind
& point-llm: Aligning point cloud with multi-modality for
3d understanding, generation, and instruction following.
arXiv preprint arXiv:2309.00615, 2023.
[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing
Ren, and Jian Sun.
Identity mappings in deep residual
networks. In Computer Vision–ECCV 2016: 14th Euro-
pean Conference, Amsterdam, The Netherlands, October
11–14, 2016, Proceedings, Part IV 14, pages 630–645.
Springer, 2016.
[Huang et al., 2023a] Tianyu Huang, Bowen Dong, Yunhan
Yang, Xiaoshui Huang, Rynson WH Lau, Wanli Ouyang,
and Wangmeng Zuo. Clip2point: Transfer clip to point
cloud classification with image-depth pre-training. In Pro-
ceedings of the IEEE/CVF International Conference on
Computer Vision, pages 22157–22167, 2023.
[Huang et al., 2023b] Zhening
Huang,
Xiaoyang
Wu,
Xi Chen, Hengshuang Zhao, Lei Zhu, and Joan Lasenby.
Openins3d:
Snap and lookup for 3d open-vocabulary
instance segmentation. arXiv preprint arXiv:2309.00616,
2023.
[Li et al., 2022] Boyi Li, Kilian Q Weinberger, Serge Be-
longie, Vladlen Koltun, and Rene Ranftl. Language-driven
semantic segmentation.
In International Conference on
Learning Representations, 2022.
[Liu et al., 2023a] Kunhao Liu,
Fangneng Zhan,
Jiahui
Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El Saddik,
Christian Theobalt, Eric Xing, and Shijian Lu. Weakly
supervised 3d open-vocabulary segmentation. In Thirty-
seventh Conference on Neural Information Processing
Systems, 2023.
[Liu et al., 2023b] Minghua
Liu,
Ruoxi
Shi,
Kaiming
Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong
Cai, Fatih Porikli, and Hao Su. Openshape: Scaling up
3d shape representation towards open-world understand-
ing. arXiv preprint arXiv:2305.10764, 2023.
[Michele et al., 2021] Bj¨orn Michele, Alexandre Boulch,
Gilles Puy, Maxime Bucher, and Renaud Marlet.
Gen-
erative zero-shot learning for semantic segmentation of 3d
point clouds. In 2021 International Conference on 3D Vi-
sion (3DV), pages 992–1002. IEEE, 2021.
[Misra et al., 2021] Ishan Misra, Rohit Girdhar, and Armand
Joulin.
An end-to-end transformer model for 3d object
detection. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 2906–2917, 2021.
[Mokady et al., 2021] Ron
Mokady,
Amir
Hertz,
and
Amit H Bermano.
Clipcap: Clip prefix for image cap-
tioning. arXiv preprint arXiv:2111.09734, 2021.
[Pang et al., 2022] Yatian Pang, Wenxiao Wang, Francis EH
Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked au-
toencoders for point cloud self-supervised learning.
In
European conference on computer vision, pages 604–621.
Springer, 2022.
[Peng et al., 2023] Songyou Peng,
Kyle Genova,
Chiyu
Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas
Funkhouser, et al.
Openscene: 3d scene understanding
with open vocabularies. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 815–824, 2023.
[Qi et al., 2017a] Charles R Qi, Hao Su, Kaichun Mo, and
Leonidas J Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation.
In Proceedings
of the IEEE conference on computer vision and pattern
recognition, pages 652–660, 2017.
[Qi et al., 2017b] Charles Ruizhongtai Qi, Li Yi, Hao Su,
and Leonidas J Guibas. Pointnet++: Deep hierarchical fea-
ture learning on point sets in a metric space. Advances in
neural information processing systems, 30, 2017.
[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris
Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
Clark, et al. Learning transferable visual models from nat-
ural language supervision. In International conference on
machine learning, pages 8748–8763. PMLR, 2021.
[Rozenberszki et al., 2022] David Rozenberszki, Or Litany,
and Angela Dai. Language-grounded indoor 3d seman-
tic segmentation in the wild. In European Conference on
Computer Vision, pages 125–141. Springer, 2022.
[Takmaz et al., 2023] Ayc¸a
Takmaz,
Elisabetta
Fedele,
Robert W Sumner, Marc Pollefeys, Federico Tombari, and
Francis Engelmann. Openmask3d: Open-vocabulary 3d
instance segmentation. arXiv preprint arXiv:2306.13631,
2023.
[Vu et al., 2022] Thang Vu, Kookhoi Kim, Tung M Luu,
Thanh Nguyen, and Chang D Yoo. Softgroup for 3d in-
stance segmentation on point clouds. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 2708–2717, 2022.
[Wang et al., 2022] Peng Wang, An Yang, Rui Men, Junyang
Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jin-
gren Zhou, and Hongxia Yang. Ofa: Unifying architec-
tures, tasks, and modalities through a simple sequence-to-
sequence learning framework. In International Conference
on Machine Learning, pages 23318–23340. PMLR, 2022.
[Wu et al., 2023] Weijia Wu, Yuzhong Zhao, Mike Zheng
Shou, Hong Zhou, and Chunhua Shen. Diffumask: Syn-
thesizing images with pixel-level annotations for seman-
tic segmentation using diffusion models. arXiv preprint
arXiv:2303.11681, 2023.
[Xu et al., 2023] Runsen Xu, Xiaolong Wang, Tai Wang,
Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm:
Empowering large language models to understand point
clouds. arXiv preprint arXiv:2308.16911, 2023.
[Xue et al., 2023] Le
Xue,
Mingfei
Gao,
Chen
Xing,
Roberto Mart´ın-Mart´ın, Jiajun Wu, Caiming Xiong, Ran
Xu, Juan Carlos Niebles, and Silvio Savarese.
Ulip:
Learning a unified representation of language, images, and
point clouds for 3d understanding. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 1179–1189, 2023.
[Yang et al., 2023] Jihan Yang, Runyu Ding, Zhe Wang, and
Xiaojuan Qi.
Regionplc: Regional point-language con-
trastive learning for open-world 3d scene understanding.
arXiv preprint arXiv:2304.00962, 2023.
[Yu et al., 2022] Xumin Yu, Lulu Tang, Yongming Rao,
Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-
training 3d point cloud transformers with masked point
modeling. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 19313–
19322, 2022.
[Zeng et al., 2023] Yihan Zeng, Chenhan Jiang, Jiageng
Mao, Jianhua Han, Chaoqiang Ye, Qingqiu Huang, Dit-
Yan Yeung, Zhen Yang, Xiaodan Liang, and Hang Xu.
Clip2: Contrastive language-image-point pretraining from
real-world point cloud data.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 15244–15253, 2023.
[Zhang et al., 2022] Renrui Zhang, Ziyu Guo, Wei Zhang,
Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao,
and Hongsheng Li. Pointclip: Point cloud understanding
by clip. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 8552–
8562, 2022.
[Zhang et al., 2023] Junbo
Zhang,
Runpei
Dong,
and
Kaisheng Ma.
Clip-fo3d: Learning free open-world 3d
scene representations from 2d dense clip. arXiv preprint
arXiv:2303.04748, 2023.
[Zhu et al., 2023] Xiangyang Zhu, Renrui Zhang, Bowei He,
Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang,
and Peng Gao. Pointclip v2: Prompting clip and gpt for
powerful 3d open-world learning. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 2639–2650, 2023.
","Recent works attempt to transfer knowledge from 2D models like CLIP to 3D features, aligning point clouds with 2D images. Others focus on points-only mechanisms, which suffer from insufficient data for point-language contrastive training. These methods fail to integrate data from multiple modalities, hindering their ability to recognize fine-grained point cloud object instances. To address these limitations, we propose UniM-OV3D, a comprehensive multimodal alignment approach that co-embeds 3D points, image pixels, depth, and text strings into a unified latent space. We also explore and utilize the characteristics of point clouds from two perspectives: a hierarchical point cloud extractor and hierarchical point-semantic caption pairs.nan"
"In this paper, we propose leveraging causal generative learning as an interpretable tool for explaining image classifiers. Specifically, we present a generative counterfactual inference approach to study the influence of visual features (i.e., pixels) as well as causal factors through generative learning. To this end, we first uncover the most influential pixels on a classifier’s decision by varying the value of a causal attribute via counterfactual inference and computing both Shapely and contrastive explanations for counterfactual images with these different attribute values. We then establish a Monte-Carlo mechanism using the generator of a causal generative model in order to adapt Shapley explainers to produce feature importances for the human-interpretable attributes of a causal dataset in the case where a classifier has been trained exclusively on the images of the dataset. Finally, we present optimization methods for creating counterfactual explanations of classifiers by means of counterfactual inference, proposing straightforward approaches for both differentiable and arbitrary classifiers. We exploit the Morpho-MNIST causal dataset as a case study for exploring our proposed methods for generating counterfacutl explantions. We employ visual explanation methods from OmnixAI open source toolkit to compare them with our proposed methods. By employing quantitative metrics to measure the interpretability of counterfactual explanations, we find that our proposed methods of counterfactual explanation offer more interpretable explanations compared to those generated from OmnixAI. This finding suggests that our methods are well-suited for generating highly interpretable counterfactual explanations on causal datasets.","Deep Generative Models (DGMs) are statistical models which aim to generate new synthetic samples following the distribution of training sets using deep neural networks. The recent advancement in DGMs’ architecture and learning techniques have resulted in a new trend in Artificial Intelligence (AI) known as generative AI with a wide variety of applications in different industry sectors such as art, healthcare and marketing. Two popular types of DGMs are Variational Autoencoders (VAEs) [Rezende and Mohamed, 2015] and Generative Adversarial Networks (GANs) [Goodfellow et al., 2020].

Recently, authors have proposed pairing DGMs with causal mechanisms to form structural causal models (SCMs) over high-dimensional datasets [Pawlowski et al., 2020, Dash et al., 2022]. We refer to such models as Causal Deep Generative Models (CGMs) for convenience, as they are deep generative models which preserve assumed or revealed causal constraints between variables of interest. We note that this acronym conflicts with the causal graphical models discussed by Schölkopf and von Kügelgen [2022], though causal graphical models are not otherwise discussed in this work. While CGMs have been applied to data generation and detection of bias in image classifiers [Dash et al., 2022], methods using CGMs to explain image classifiers have thus far been limited to binary classifiers and binary attributes.","The first method we propose exploits generative learning for deriving model explanations, contrasting model behaviour as the value of an attribute varies. Specifically, we use SHAP values [Lundberg and Lee, 2017] and contrastive explanations [Dhurandhar et al., 2018] to visualize the importance of pixel features as a causal generative model varies the attributes of a causal dataset via counterfactual inference.

The second method proposed in this paper uses Monte-carlo sampling in the latent space of conditional generative models to assign classification scores to attribute values rather than image values, allowing feature importance methods such as SHAP for structured data to be applied to image classifiers in settings where causal metadata is available in image datasets.

The third and final method, counterfactual explanation, uses the BiGAN and VAE architectures of ImageCFGen and DeepSCM respectively to generate counterfactual explanations which are realistic and interpretable. We use quantitative metrics to compare our proposed methods to an open-source toolkit for model explanation, demonstrating the viability and interpretability of our counterfactual explainers.","Our results suggest that while slant and thickness have the highest impact in changing digit classification, intensity plays a less evident role. Even though VAE and BiGAN models don’t follow the same levels of attribute importances, they provide a similar order for ranking many of the most important attributes.","This paper presented methods by which causal generative models can be used to explain image classifiers through counterfactual inference. These explanations fell into three main categories. The first method used counterfactuals with varying levels of attributes to analyze the variation in pixel-based explanations, using both SHAP values and contrastive explainers. The second method used a modification to the standard method of extracting SHAP values to extract attribute importances. The third method proposed both gradient-based and model-agnostic counterfactual explanations of image classifiers by searching in the attribute space of a causal generative model.

Our method of analyzing pixel-based explanations for varying levels of a causal attribute (subsection 3.1) helps us identify which levels of an attribute can lead to near-misclassifications or confusion with other classes, as well as determine which levels of an attribute can lead to simpler classifications due to a need for less features to form a pertinent positive.

Our method of identifying attribute importances from subsection 3.2 is able to rank the human-interpretable features of a causal dataset in terms of their importance to an image classifier. This method expands upon the counterfactual importance score from Dash et al. [2022], in that it works for arbitrary classifiers and arbitrary attributes rather than only binary attributes and binary classifiers.

The methods presented in this work for the generation of counterfactual explanations are demonstrably more interpretable than those from OmnixAI. We showed this using quantitative metrics, as well as by displaying examples of visual explanations.",Causal Generative Explainers using Counterfactual Inference: A Case Study on the Morpho-MNIST Dataset,"Will Taylor-Melanson, Zahra Sadeghi, Stan Matwin","CAUSAL GENERATIVE EXPLAINERS USING COUNTERFACTUAL
INFERENCE: A CASE STUDY ON THE MORPHO-MNIST DATASET
A PREPRINT
Will Taylor-Melanson∗
Faculty of Computer Science
Dalhousie University
Halifax, Nova Scotia, B3H 1W5, Canada
Zahra Sadeghi
Faculty of Computer Science
Dalhousie University
Halifax, Nova Scotia, B3H 1W5, Canada
Stan Matwin
Faculty of Computer Science
Dalhousie University
Halifax, Nova Scotia, B3H 1W5, Canada
ABSTRACT
In this paper, we propose leveraging causal generative learning as an interpretable tool for explaining
image classifiers. Specifically, we present a generative counterfactual inference approach to study the
influence of visual features (i.e., pixels) as well as causal factors through generative learning. To this
end, we first uncover the most influential pixels on a classifier’s decision by varying the value of a
causal attribute via counterfactual inference and computing both Shapely and contrastive explanations
for counterfactual images with these different attribute values. We then establish a Monte-Carlo
mechanism using the generator of a causal generative model in order to adapt Shapley explainers to
produce feature importances for the human-interpretable attributes of a causal dataset in the case where
a classifier has been trained exclusively on the images of the dataset. Finally, we present optimization
methods for creating counterfactual explanations of classifiers by means of counterfactual inference,
proposing straightforward approaches for both differentiable and arbitrary classifiers. We exploit the
Morpho-MNIST causal dataset as a case study for exploring our proposed methods for generating
counterfacutl explantions. We employ visual explanation methods from OmnixAI open source toolkit
to compare them with our proposed methods. By employing quantitative metrics to measure the
interpretability of counterfactual explanations, we find that our proposed methods of counterfactual
explanation offer more interpretable explanations compared to those generated from OmnixAI. This
finding suggests that our methods are well-suited for generating highly interpretable counterfactual
explanations on causal datasets.
Keywords Causal Modeling · Explainable AI · Deep Learning · Generative AI · Morpho-MNIST · Counterfactual
Learning · Counterfactual Explanations
1
Introduction
Deep Generative Models (DGMs) are statistical models which aim to generate new synthetic samples following
the distribution of training sets using deep neural networks. The recent advancement in DGMs’ architecture and
learning techniques have resulted in a new trend in Artificial Intelligence (AI) known as generative AI with a wide
variety of applications in different industry sectors such as art, healthcare and marketing. Two popular types of
DGMs are Variational Autoencoders (VAEs) [Rezende and Mohamed, 2015] and Generative Adversarial Networks
(GANs) [Goodfellow et al., 2020].
∗Email: wl647481@dal.ca
arXiv:2401.11394v1  [cs.LG]  21 Jan 2024
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
Recently, authors have proposed pairing DGMs with causal mechanisms to form structural causal models (SCMs)
over high-dimensional datasets [Pawlowski et al., 2020, Dash et al., 2022]. We refer to such models as Causal Deep
Generative Models (CGMs) for convenience, as they are deep generative models which preserve assumed or revealed
causal constraints between variables of interest. We note that this acronym conflicts with the causal graphical models
discussed by Schölkopf and von Kügelgen [2022], though causal graphical models are not otherwise discussed in this
work. While CGMs have been applied to data generation and detection of bias in image classifiers [Dash et al., 2022],
methods using CGMs to explain image classifiers have thus far been limited to binary classifiers and binary attributes.
In this paper, we investigate expanding the applicability of CGMs to Explainable AI (XAI). XAI refers to methods
that offer human understandable interpretations for complex machine learning models. XAI techniques help to enrich
the capacity of AI systems by providing understanding, transparency, fairness, and trust for the decisions made by AI
systems [Dwivedi et al., 2023].
This paper explores explainability methods for image classifiers by focusing on the utilization of two causal deep
generative architectures, namely, the DeepSCM [Pawlowski et al., 2020] and ImageCFGen [Dash et al., 2022] models.
The authors of ImageCFGen defined a counterfactual importance score using their BiGAN-based architecture, which
measures how a classifier’s prediction changes when a given attribute (i.e., cause of the classifier’s input) changes.
For instance, measuring how an “attractive” classifier’s output changes when the attribute “bald” changes. While this
method proved useful in the work by Dash et al., it is not without limitations. Namely, that it is only defined for binary
attributes and binary classifiers. These limitations motivate the methods described in this paper, which aim to provide
image-based and attribute-based classifier explanations on datasets with continuous and categorical variables of interest
representing the causes of an image. The novelty of these methods lies in the fact that they are easily implemented using
any pretrained causal generative model such as DeepSCM and ImageCFGen, rather than requiring explanation-specific
generative models trained with custom loss functions for individual classifiers [Van Looveren et al., 2021, Yang et al.,
2021]. The source code for our experiments is available in a public Github repository.2
The first method we propose exploits generative learning for deriving model explanations, contrasting model behaviour
as the value of an attribute varies. Specifically, we use SHAP values [Lundberg and Lee, 2017] and contrastive
explanations [Dhurandhar et al., 2018] to visualize the importance of pixel features as a causal generative model varies
the attributes of a causal dataset via counterfactual inference.
The second method proposed in this paper uses Monte-carlo sampling in the latent space of conditional generative
models to assign classification scores to attribute values rather than image values, allowing feature importance methods
such as SHAP for structured data to be applied to image classifiers in settings where causal metadata is available in
image datasets.
The third and final method, counterfactual explanation, uses the BiGAN and VAE architectures of ImageCFGen and
DeepSCM respectively to generate counterfactual explanations which are realistic and interpretable. We use quantitative
metrics to compare our proposed methods to an open-source toolkit for model explanation, demonstrating the viability
and interpretability of our counterfactual explainers.
2
Related Work
2.1
Causal Generative Models
Work from Pawlowski et al. [2020] was the first to use deep, non-invertible mechanisms to learn high-dimensional flows
for counterfactual inference. Referred to as DeepSCM, the work proposes three ways to model the data generation
process of an SCM while allowing for counterfactual inference. The mechanism most relevant to this work is the
“amortized, explicit"" strategy, which is most commonly modelled with a VAE. The use of an encoder-decoder model is
to allow for abduction, the recovery of unobserved noise variables from observed data, a necessary part of computing
a counterfactual [Schölkopf and von Kügelgen, 2022]. The low-level attributes of a causal dataset (i.e., non-image
variables) are modelled for the DeepSCM algorithm via the method of normalizing flows [Kobyzev et al., 2020], which
uses a change of variables to make the density of a transformed variable tractable. In their experiments, Pawlowski et
al. perform counterfactual inference on the Morpho-MNIST dataset [de Castro et al., 2018], a modified version of the
famous MNIST handwritten digits dataset3 [Deng, 2012] with added morphological attributes of thickness, intensity,
and slant. Because the Morpho-MNIST dataset has its morphological operations readily available for programmers,
any causal structure can be assumed over its attributes and implemented in a man-made causal graph, as was done
by the authors of DeepSCM. Because of this it is straightforward to evaluate the accuracy of interventions performed
on the Morpho-MNIST attributes, which showed that the DeepSCM model has high accuracy in manipulating the
2https://github.com/wtaylor17/CGMExplainers
3The original MNIST dataset may be downloaded from http://yann.lecun.com/exdb/mnist.
2
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
thickness, intensity, and slant of an image in a causally correct manner. While Pawlowski et al. have success in training
accurate causal generative models using VAEs, they do not directly experiment with using these models to explain
image classifiers.
The ImageCFGen algorithm from Dash et al. [2022] is another recently proposed causal generative model for counter-
factual inference. However, rather than using a VAE, a bidirectional GAN (BiGAN) [Donahue et al., 2016, Dumoulin
et al., 2016] is used to model the data generation process. This has the advantage of a fully-deterministic model, i.e.,
latent vectors do not come from a distribution but are rather computed directly by an encoder both during training and
inference time. However, recent theoretical results have also suggested that models such as deterministic BiGANs
are not universal approximators in the usual sense, and may not be stable during training in practice [Feng et al.,
2021]. Despite this, the ImageCFGen algorithm achieved comparable performance to the DeepSCM algorithm on the
Morpho-MNIST dataset during experimentation in the work by Dash et al. To aid in the reconstruction of images and
the approximation of counterfactuals, Dash et al. also proposed a fine-tuning procedure for their model which was
shown to improve the preservation of image style during counterfactual approximation.
2.2
Explaining Classifiers with Generative Models
While Pawlowski et al. do not directly propose a method of classifier explanation using their causal generative model,
Dash et al. propose a basic method of measuring the importance of a binary attribute to a binary classifier, which
they call their counterfactual importance score. They do this by measuring the difference in a classifiers output
when performing a counterfactual changing the binary attribute from one value to its other value. For instance, when
measuring the importance of the attribute bald to an attractiveness classifier on a dataset of human faces, it was found
by Dash et al. that the classifier associated the presence of baldness with a lower probability of attractiveness. Using a
similar method, Dash et al. were able to measure the bias of a binary classifier with respect to a binary attribute. The
main downside of these methods is that they only work in the very specific setting where both the attribute of interest
and the output of the classifier being explained are binary.
Another approach to explaining classifiers using causal models is the explainer described by Parafita and Vitrià [2019],
who use a causal model to measure the effect of “latent factors"" (what we refer to as attributes or causes of an image)
on a binary classifier. However, their approach does not explicitly perform abduction and thus does not produce
counterfactual data in the standard causal sense [Pearl, 2009] as DeepSCM and ImageCFGen does. In fact, they avoid
explicitly training a generative model in their experiments altogether, and instead simply look for examples in their
synthetic dataset matching the desired attributes. As such, we don’t consider the method from Parafita and Vitria to be a
CGM in the same sense as the deep models used in this work.
A method proposed by Hvilshøj et al. [2021a] uses invertible neural networks (normalizing flows) [Dinh et al., 2014]
to generate counterfactual examples. The invertible network acts as a generative classifier which is able to map input
images to a latent space. This method proved to generate realistic counterfactuals, and requires very little computation
to generate them. However, the method depends on the classifier being explained being an invertible NN, i.e., it cannot
be used on an existing classifier.
Mahajan et al. [2019] propose counterfactual explainers which preserve causal constraints on variables in a dataset
through the use of a structural causal model (SCM). However, the results from this work are limited to structured
datasets, and the generative model used in their method is not directly responsible for generating model explanations.
Van Looveren et al. [2021] propose using conditional generative models for the creation of counterfactual explanations.
While the model is efficient and only requires forward passes to compute the counterfactuals, the trained model is
specific to a certain classifier, meaning that a new generative model needs to be trained for each classifier to be explained.
A similar approach comes from Yang et al. [2021], who train a conditional generative model using an existing classifier
to produce counterfactual explanations, optionally accounting for causal relationships by modifying their generator
structure, but who do not report experiments on image data or otherwise high-dimensional datasets. While either of
these methods may produce believable counterfactuals in an efficient manner, neither of them perform abduction, i.e.,
they do not use a general-purpose CGM to produce counterfactual data and cannot explain arbitrary classifiers without
retraining.
3
Methods
In this section, we propose a novel approach of causal generative explainers. More specifically, we propose three
methods for explaining AI classifiers using causal generative models performing counterfactual inference. In the
first method, we use counterfactuals with different levels of a given attribute for visualizing the evolution of pixel
importances using SHAP [Lundberg and Lee, 2017], as well as the evolution of pertinent positives (PP) and pertinent
3
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
negatives (PN) using a contrastive explainer [Dhurandhar et al., 2018]. In the second method, we define a function over
attributes to provide feature importances in the attribute space for an image classifier using SHAP. In the third method,
we use CGMs to provide both gradient-based and model-agnostic counterfactual explanations of classifiers.
Before we begin, we restate the method of counterfactual approximation using causal generative models as described
by the authors of ImageCFGen [Dash et al., 2022]. To generate a counterfactual using a causal generative model with
encoder E and generator/decoder G, we begin with an observational pair x, a of an image and its attributes. We then
ask the question “what would x have been, if a had instead been a′?"". This requires us to recover the unobserved
variable ϵ ≈ E(x, a) that was used to generate the original data via the encoder, and then to use the counterfactual
attributes a′ to construct a new image. The unobserved variable accounts for the randomness in the data generation
process. For Morpho-MNIST, we interpret this as meaning that ϵ accounts for the writing style of a handwritten digit.
In symbols, the counterfactual x′ is of the form:
x′ = G(E(x, a), a′)
(1)
see the paper from Dash et al. [2022] for a mathematical proof of the causal validity of Equation (1). Intuitively, for
Morpho-MNIST, Equation (1) says to generate an image x′ in the same style as x, but with the attributes a′ instead of a.
When the original observation x, a is fixed and the counterfactual attributes a′ vary, we may denote the counterfactual
from Equation (1) as x′(a′), a function of the counterfactual attributes, rather than writing out the entire right hand side
of Equation (1).
3.1
Pixel Explanations
In our approach, we consider the attributes thickness, intensity, and slant of the Morpho-MNIST [de Castro et al., 2018]
dataset and aim to measure the influence of these attributes on an image classifier f in the pixel space. We denote
SHAP(x; f) as the SHAP values (feature importances) from Lundberg and Lee [2017] of a function f on an input x.
For an input x, SHAP(x; f) consists of K feature importance masks (saliency maps) with the same shape as x, where
K is the number of classes predicted by f.
In the pixel space, we consider the SHAP values for various levels of a given attribute, in order to determine how the
regions highlighted as positively or negatively impacting the output score of a classifier vary along with the attribute in
question. As with any counterfactual, we begin with an observation x, a, where x is an image and a are its corresponding
attributes. To explain the classification f(x) for a classifier f, we can compute SHAP(x; f) as described by Lundberg
and Lee. More specifically, we use the gradient explainer available in the publicly available implementation of SHAP.4
However, in this section we are interested in the consistency of the pixel feature importances as a given attribute ai
(one of thickness, intensity, or slant for Morpho-MNIST) is varied via counterfactual approximation. Denoting ai,v
as the counterfactual attributes under the intervention do(ai = v), the counterfactual image for this intervention is
denoted x′(ai,v) according to our previously established notation. To formalize our visualization technique, we have a
set Si = {v1, v2, . . . , vN} of values for the attribute ai, and wish to visualize the consistency of pixel SHAP values as
we vary ai through this set of values. The explanations given by SHAP are denoted as SHAP(x′(ai,vj); f) for each
vj ∈ Si. We plot these pixel importance values along a vertical axis in order to visualize how the pixel importances
evolve as the value of an attribute varies (see for instance Figure 4). When we generate counterfactual Morpho-MNIST
digits, using the same latent vector E(x, a) for each counterfactual ensures that the style is preserved between images.
Thus, the motivation for using causal generative models in this experiment is to isolate the effect of the attribute in a
causally correct manner, i.e., not also varying the style of the image.
This section presents another perturbation based method for displaying influential pixels for different levels of a given
attribute. However, instead of only displaying pixel importances for the counterfactuals corresponding to each attribute
level, we visualize pertinent negatives (PN) and pertinent positives (PP) using the contrastive explainer from Dhurandhar
et al. [2018]. This contrastive explainer method (CEM) illustrates two minimal sets of features, pertinent positives
(PP) and pertinent negatives (PN) that finds the necessary pixel features to be, respectively, either present to produce
the same classification or changed to produce a different classification. This explainer provides human-interpretable
results and highlights the positively important and negatively important regions of an image to explain a classifier. The
optimization problems corresponding to PN and PP explanations are available in the original contrastive explainer
paper. We also choose to visualize the distribution of class scores fj(x) of the classifier to analyze how they evolve
as a given attribute changes. Unlike with the method using SHAP described previously, this method provides two
visual explanations (a PN and a PP) for any image x, instead of pixel importances for the individual class scores. This
method of visualization is in part meant to detect what types of attribute changes may act as adversarial to a classifier
by changing its decision, or otherwise impacting its class score distribution. Further, this method is used to measure
4https://github.com/shap/shap/blob/master/shap/explainers/_gradient.py
4
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
how consistent the positive and negative regions of the contrastive explanations are across counterfactuals with one
varied attribute, i.e., how specific the explanations are to the original image compared to the style of the image itself.
3.2
Attribute Explanation
Outside of pixel importances, we are also interested in measuring the impact of a given attribute ai on a classifier f.
This problem has previously been briefly investigated by Dash et al. [2022], the authors of the ImageCFGen causal
generative model. However, the method from Dash et al. is limited to only binary classifiers and binary attributes ai.
To make the notion of explanation formal, we again consider an observation x, a of an image and its attributes. We
desire to have attribute importances for the values of a, independent of the specific image x having these attributes
(as there can be several images with the same attributes). Recall a causal generative model has a generator/decoder G,
which can generate an image with the given attributes when a latent vector z ∼ p(z) is supplied. As such, we define an
approximate attribute classifier as the expected classification over all generated images with the given attributes. This is
approximated by the following Monte carlo over a sample z1, z2, . . . , zm of size m drawn from p(z):
ˆf(a) = Ep(z)[f(G(z, a))] ≈ 1
m
m
X
i=1
f(G(zi, a)).
(2)
This process is described in Figure 1. In our experiments, we set m = 4. From this, we can visualize the SHAP values
SHAP(a; ˆf) to determine feature importances in the attribute space. More specifically, we take the mean of absolute
SHAP values over each of the K class score functions fj to determine the importances of the given attribute. Further,
to get a global feature importance, we consider the median value of the local feature importances accross the entire
Morpho-MNIST test set for a given class (see Figure 9).
Figure 1: The proposed method of using a generative causal model (such as the generator G of a BiGAN from
ImageCFGen Dash et al. [2022]) to rank important human-interpretable attributes for an image classifier. Attributes to
be explained are either sampled from an SCM M or from an available test set, and these along with a random sample
of latent features form the input to the image generator G, which feeds inputs to a classifier f to define a classification
of the attributes. From this, any explanation method can be used in the attribute space, though this work has chosen to
use SHAP Lundberg and Lee [2017].
3.3
Counterfactual Explanations with Causal Generative Models
We propose two optimization approaches for creating counterfactual explainers using a generative causal model, which
we refer to as our gradient-based and model-agnostic counterfactual explainers. As before, we denote x′(a′) as a
counterfactual of a given observation x, a with the intervention do(a = a′). When searching over the digit label ℓ, or
generally any categorical attribute in a′, we consider instead a random variable ˆℓ taking possible values of ℓ (the digits
0 through 9 in this case) by representing it as a probability distribution over its values represented by a vector p with
pk = p(ˆℓ = k). We also assume that the categorical variable is passed to the generative models via an embedding
lookup function e(ℓ). When a specific p (i.e., a specific random variable ˆℓ) is considered, it can be passed to a causal
generative model by considering the following linear combination of embeddings:
Ep(ˆℓ)[e(ˆℓ)] =
K
X
k=1
pke(k)
(3)
5
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
which is the expected value of the embedding vector e(·) following a transformation of the chosen distribution p over
the categorical variabl ℓ. This allows us to search over arbitrary mixtures of the categorical variable.
Our first approach uses a loss function similar to that of Watcher et al.’s counterfactual explainer [Wachter et al.,
2018]. OmnixAI modified the equation from Watcher et al., and provides a public-facing implementation based on the
following formula:
min
x′ max
λ
λH

fy(x′) − max
y′̸=y fy′(x′)

+ ||x′ − x||1
(4)
where H is the hinge loss function, y is the predicted class of x, x′ is the counterfactual, λ controls the tradeoff between
the two loss terms, and fj is the classification score function for class j. However, in our gradient-based explainer
approach, we account for a target class yt for the desired counterfactual, and search in the attribute space a′. We propose
the following loss function:
min
a′ max
λ
λ

max
y′̸=yt fy′(x′(a′)) − fyt(x′(a′))

+ ||x′(a′) − x||1.
(5)
The above loss function can be restricted to a subset of attributes to further ensure that counterfactuals are visually
close to the original image. In our experiments, we search only over the digit labels ℓ, such that our counterfactual
explanations lie somewhere between the space of images with the label ℓ = y and those with ℓ = yt. Further, we set
λ = 10 in our experiments rather than performing an explicit search after observing empirically that changing its value
did not significantly impact performance. When searing over distributions p over the label ℓ, we modify the logits of p
by gradient descent on Formula (5), passing these logits through a softmax before computing Equation (3).
The second approach for counterfactual explanation we propose in this work is a model-agnostic optimization, which is
based on a linear search. For a given classifier f, denote C(x) as the most likely class predicted by f, i.e.:
C(x) = arg max
j
fj(x).
(6)
We use a form of Equation (3) to interpolate between images with the label y = C(x) and the target label yt, where
pyt = α, py = 1 − α, and pk = 0 for all other classes. We define an interpolated embedding eα for α ∈ [0, 1] as a
linear combination of the embeddings for y and yt as:
eα = αe(yt) + (1 − α)e(y).
(7)
Such that eα equals e(y) when α = 0 and e(yt) when α = 1. Such an approach is similar to well-known methods of
interpolation in the latent space of generative models [Liu et al., 2018], but occurs instead in the embedding space.
Our goal is to find the smallest α such that, when we replace the embedding for e(y) as the input to the counterfactual
generator with eα to generate a counterfactual x′, we get a classification of C(x′) = yt. This value of α is found in
our experiments by searching over a grid of size 100. Because the proposed method does not use gradients (or even
classification scores), it can be used on any image classifier, regardless of whether it is a neural network. However, in
our experiments, we use a CNN classifier to allow for comparison with gradient-based methods.
4
Experiments
The Morpho-MNIST dataset [de Castro et al., 2018] adds additional causal aspects to the well-known MNIST
handwritten digits dataset [Deng, 2012]. Specifically, attributes of line thickness, image intensity, and slant are added
to the original handwritten digits of MNIST. Because these attributes are added to the digits of MNIST using image
processing techniques, scientists have complete control over the data generation process of Morpho-MNIST data and
hence have full knowledge of the SCM used to generate Morpho-MNIST data. Pawlowski et al. [2020] were the first to
create learned SCMs from the data generated using Morpho-MNIST techniques, and propose a causal graph involving
the following attributes: 1. Thickness, denoted here as t. 2. Intensity, denoted here as i. 3. Digit image output, denoted
here as o. We use in this work the variant of the Morpho-MNIST dataset used by Dash et al. [2022], who added a slant
parameter s to the causal graph. The generative models are also conditioned on the digit label ℓ. Instances from the
Morpho-MNIST dataset are displayed in Figure 2, and the causal relationships between the variables of the dataset is
shown in Figure 3.
In all of our experiments, when training the VAE and BiGAN models of DeepSCM and ImageCFGen, the encoder and
decoder for both the BiGAN and VAE consist of 5 convolutional layers (transposed convolutions in the decoder). The
classifier being explained consists of 4 convolutional layers followed by a single fully-connected classification layer.
Throughout this section, we study the explainability of classifiers trained on the Morpho-MNIST dataset using the three
methods which are described in section 3. Moreover, we evaluate the quality of our counterfactual generation method
from subsection 3.3 using quantitative measurements and provide comparative results with an open-source framework
for counterfactual generation.
6
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
Figure 2: Instances from the Morpho-MNIST training set used in this work. Each digit has a varying class, thickness,
intensity, and slant sampled from the ground-truth SCM.
t
i
s
ℓ
o
Figure 3: Causal graph for Morpho-MNIST, identical to the one used by Dash et al. [2022].
Figure 4: Evolution of SHAP pixel values as the thickness attribute of Morpho-MNIST varies on a digit ‘4’. Positive
attributions (red) grow for the score for class 4 with the thickness, and positive regions for class 4 often correspond to
negative regions for class 9 (and vice-versa). The displayed counterfactuals were computed using a VAE.
4.1
Pixel Explanations
We present the evolution of pixel importances for all three attributes measured by the SHAP explainer approach
described in section subsection 3.1 using both VAE and BiGAN generative models. For both the SHAP and contrastive
7
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
Figure 5: Evolution of SHAP pixel values as the slant attribute of Morpho-MNIST varies on a digit ‘6’.
explainers, we normalize attributes to [−1, 1] in our experiments and set Si = {−0.8, 0.5, 0, 0.5, 0.8} for all attributes.
We plot the image (classifier input) in the first column of our figures, with each subsequent column displaying the
SHAP values for a specific digit class. We can observe in Figure 4 that for lower levels of thickness values, the positive
importance regions determined by SHAP are stronger and clearer. Further, comparing columns for class 4 and 9
in Figure 4, we can see that increasing thickness causes confusion between the two classes (positive and negative regions
swap). We can also observe in Figure 5 that high magnitudes of slant make digits more difficult to classify. Specifically,
on the digit 6 displayed, high clockwise slant leads to confusion with 0, 2, and 8, while high counter-clockwise slant
leads to confusion with 4, 5, and 8.
Contrastive explanations evolving with the values of intensity, thickness, and slant are shown in Figure 6, Figure 7,
and Figure 8, respectively. We can see that with intensity (Figure 6), no change in the attribute causes a change in the
class scores from the classifier. However, smaller values of intensity are associated with larger PP regions, i.e., the high
intensity images require less PP features to produce the same correct classification. The same phenomenon is seen with
an increase in thickness (Figure 7), however, because increases in thickness causes corresponding increases in intensity
due to the causal structure of Morpho-MNIST (see Figure 3), this observation is likely attributed to intensity rather
than thickness. With slant (Figure 8), the highest value of slant causes all non-zero pixels to be part of the PP regions,
suggesting again that slant can cause difficulty in classification as no smaller sub-region of the digit’s features lead to a
correct classification. This is also reflected in the classifiers score distribution, where the 8 with extreme positive slant
is nearly incorrect classified as a 5.
4.2
Attribute Explanation
We also obtained the aggregate influence of each of the attributes on a classifier, which are depicted in Figure 9 for
each of the ten classes of Morpho-MNIST. Our results suggest that while slant and thickness have the highest impact in
changing digit classification, intensity plays a less evident role. Even though VAE and BiGAN models don’t follow the
same levels of attribute importances, they provide a similar order for ranking many of the most important attributes.
4.3
Counterfactual Explanations
To evaluate the interpretability of the counterfactual generation method proposed in subsection 3.3, and to compare the
generated counterfactuals with those produced by OmnixAI explainers, we use the IM1 and IM2 scores from Van Loov-
eren and Klaise [2021] as well as the oracle score from Hvilshøj et al. [2021b]. For an image x from class p and
corresponding counterfactual x′ supposedly from the target class q, IM1 measures whether x′ is closer to class p or q
using autoencoders AEp and AEq trained on data coming from only the respective classes:
IM1 =
||x′ − AEq(x′)||2
2
||x′ − AEp(x′)||2
2 + ε.
(8)
8
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
Method
IM1 95% CI
BiGAN (grad)
0.6489 ± 0.0073
VAE (grad)
0.6497 ± 0.0081
BiGAN (agnostic)
1.1124 ± 0.0104
VAE (agnostic)
1.2399 ± 0.0159
OmnixAI CF
2.8861 ± 0.0341
OmnixAI contrastive
1.5416 ± 0.0102
Table 1: Recorded means and 95% confidence intervals for IM1 scores for the visual explanation methods considered in
this work, computed over the Morpho-MNIST test set. The lowest means are shown in bold.
If IM1 falls below 1, the counterfactual is better reconstructed by a model trained on data from the target class,
suggesting it can be easily interpreted as an instance from the target class. IM2, also proposed by Van Looveren and
Klaise, aims to measure how well the counterfactual follows the overall distribution of data by utilizing an autoencoder
AE trained on the entire training set:
IM2 = ||AEq(x′) − AE(x′)||2
2
||x′||1 + ε
.
(9)
Similarly to IM1, lower values of IM2 are considered better by the authors who proposed the metrics. Both the encoder
and decoder of our autoencoders consist of two convolutional layers and one fully-connected layer.
Figure 11 compares the average distribution of IM1 vs. IM2 in a 2D scatter plot for all the ten classes from the test set
of Morpho-MNIST. Values which are closer to the bottom-left corner indicate higher interpretability. As evident from
the results, our approach achieves lower values of IM1 and IM2 in almost all cases. In addition, we can observe better
consistency in terms of the variation of performance on different classes. It is also observable that both BiGAN and
VAE models lead to similar results. However, as expected, our gradient-based method achieves better IM1 scores due to
its tendency to produce embeddings close to the target class embedding e(yt) when compared with our model-agnostic
approach. We note that although we report both IM1 and IM2 in Figure 11, IM2 has been previously been met
with criticisms by other authors, with Schut et al. [2021] finding that IM2 fails to differentiate in-distribution from
out-of-distribution images. Because of this, we report confidence intervals only for IM1 in Table 1. These results
indicate that the IM1 values of our approaches are significantly better than those of OmnixAI. Overall, the results
suggest that the best performance on these metrics is achieved by the gradient-based BiGAN method.
We also provide a visual example to compare the quality of counterfactually generated images of our methods with that
of OmnixAI in Figure 10. Our approaches produce more visually interpretable images which are accurately classified
as the target class.
Figure 6: Contrastive explanations (PN and PP) from OmnixAI computed on counterfactuals with varying levels of
intensity. Lower levels of intensity cause larger areas of PP to be required for the classification.
9
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
Figure 7: Contrastive explanations (PN and PP) from OmnixAI computed on counterfactuals with varying levels of
thickness. Lower levels of thickness cause larger areas of PP to be required for the classification.
Figure 8: Very high blue PP area means that all features are necessary to avoid a misclassification on the extreme slant
value. This suggests higher values of slant makes classification harder (more confusion).
The metrics proposed by Hvilshøj et al. are based on the presence of an oracle o, which is an additional classifier
trained with a different initial random state than the one used to train the explained classifier f. Letting X ′ denote a set
of counterfactuals explanations from a given method, the oracle score is defined as:
Oracle =
1
|X ′|
X
x′∈X ′
1f(x′)=o(x′)
(10)
where 1f(x′)=o(x′) is an indicator function with a value of 1 if f and o agree on the classification of counterfactual
and 0 otherwise. The idea behind this metric is that if the classifier and oracle do not agree on many counterfactuals,
then the changes made to the original instances are very specific to the weights of f (i.e., they are adversarial and
not interpretable). Figure 12 presents the mean oracle scores over 10 independent runs for all the explainers, using
different oracles on each run. The results are consistent with our previous observation that our proposed explainers
provide superior interpretability in comparison to OmnixAI. In particular, we note that our gradient-based explainer
outperforms all the methods which are discussed in this paper.
10
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
Figure 9: Attribute feature importances computed using the monte-carlo attribute classifier method (see Figure 1),
aggregated over all images from the classes the Morpho-MNIST test set as described in subsection 3.1. Recall that
thickness causes intensity (see Figure 3).
Figure 10: Comparison of visual explanation methods proposed in this work with the counterfactual and contrastive
explainers from OmnixAI. The class of each explanation is shown in the title of each subfigure with the name of the
explanation method, and the class score distribution from the classifier is shown underneath each image. The label
“OmnixAI PN"" refers to the contrastive explainer. The required pixels to remove for converting an image of digit 1 to
digit 2 are shown in red, and added pixels are shown in green. The third row indicates the accuracy of classification of
the counterfactual.
5
Discussion and Conclusion
This paper presented methods by which causal generative models can be used to explain image classifiers through
counterfactual inference. These explanations fell into three main categories. The first method used counterfactuals
with varying levels of attributes to analyze the variation in pixel-based explanations, using both SHAP values and
contrastive explainers. The second method used a modification to the standard method of extracting SHAP values
to extract attribute importances. The third method proposed both gradient-based and model-agnostic counterfactual
explanations of image classifiers by searching in the attribute space of a causal generative model.
Our method of analyzing pixel-based explanations for varying levels of a causal attribute (subsection 3.1) helps us
identify which levels of an attribute can lead to near-misclassifications or confusion with other classes, as well as
determine which levels of an attribute can lead to simpler classifications due to a need for less features to form a
pertinent positive.
Our method of identifying attribute importances from subsection 3.2 is able to rank the human-interpretable features
of a causal dataset in terms of their importance to an image classifier. This method expands upon the counterfactual
11
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
Figure 11: Scatter plots of mean IM1 and IM2 for each visual explanation method, plotted by Morpho-MNIST class
(lower is better). The mean value for each class is shown in a different color.
importance score from Dash et al. [2022], in that it works for arbitrary classifiers and arbitrary attributes rather than
only binary attributes and binary classifiers.
The methods presented in this work for the generation of counterfactual explanations are demonstrably more interpretable
than those from OmnixAI. We showed this using quantitative metrics, as well as by displaying examples of visual
explanations. Further, these methods can be deployed using existing causal generative models, eliminating the need
for additional training of specialized explanation models as is required by current generative model-based explanation
methods.
Figure 12: Oracle scores for each of the explanation methods considered in this work, computed across the Morpho-
MNIST test set. 95% confidence intervals for 10 runs with different oracle initializations are shown. All scores
were significantly different at the 95% confidence level, though the agnostic and contrastive explainers show similar
performance on this metric.
12
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
One possible application of our proposed method is applying our proposed attribute feature explainers to the legibility
of one’s handwriting. The explainability features derived from our methods could in theory be embedded in a
recommendation system to give feedback to the users in order to improve the readability level of handwriting. We
also suggest investigating new loss functions to use with our gradient-based counterfactual explainers in future work,
possibly incorporating more advanced regularization techniques or incorporating a GAN’s discriminator in the loss
function to further ensure the generation of realistic counterfactual examples. Additionally, for our model-agnostic
explainer, more advanced interpolation techniques could be investigated rather than the simple linear interpolation used
in this work.
Acknowledgements
This work was supported by the Natural Sciences and Engineering Research Council of Canada [grant number 550722].
References
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on
machine learning, pages 1530–1538. PMLR, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020.
Nick Pawlowski, Daniel Coelho de Castro, and Ben Glocker. Deep structural causal models for tractable counterfactual
inference. Advances in Neural Information Processing Systems, 33:857–869, 2020.
Saloni Dash, Vineeth N Balasubramanian, and Amit Sharma. Evaluating and mitigating bias in image classifiers: A
causal perspective using counterfactuals. In Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision, pages 915–924, 2022.
Bernhard Schölkopf and Julius von Kügelgen. From statistical to causal learning, 2022. URL https://arxiv.org/
abs/2204.00607.
Rudresh Dwivedi, Devam Dave, Het Naik, Smiti Singhal, Rana Omer, Pankesh Patel, Bin Qian, Zhenyu Wen, Tejal
Shah, Graham Morgan, et al. Explainable ai (xai): Core ideas, techniques, and solutions. ACM Computing Surveys,
55(9):1–33, 2023.
Arnaud Van Looveren, Janis Klaise, Giovanni Vacanti, and Oliver Cobb. Conditional generative models for counterfac-
tual explanations. arXiv preprint arXiv:2101.10123, 2021.
Fan Yang, Sahan Suresh Alva, Jiahao Chen, and Xia Hu. Model-based counterfactual synthesizer for interpretation. In
Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining, pages 1964–1974, 2021.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information
Processing Systems 30, pages 4765–4774. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7062-a-unified-approach-to-interpreting-model-predictions.pdf.
Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, and Payel Das.
Explanations based on the missing: Towards contrastive explanations with pertinent negatives, 2018.
Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker. Normalizing flows: An introduction and review of current
methods. IEEE transactions on pattern analysis and machine intelligence, 43(11):3964–3979, 2020.
Daniel Coelho de Castro, Jeremy Tan, Bernhard Kainz, Ender Konukoglu, and Ben Glocker. Morpho-mnist: Quantitative
assessment and diagnostics for representation learning. CoRR, abs/1809.10780, 2018. URL http://arxiv.org/
abs/1809.10780.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing
Magazine, 29(6):141–142, 2012.
Jeff Donahue, Philipp Krähenbühl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782,
2016.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron
Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Ruili Feng, Zhouchen Lin, Jiapeng Zhu, Deli Zhao, Jingren Zhou, and Zheng-Jun Zha. Uncertainty principles of
encoding gans. In International Conference on Machine Learning, pages 3240–3251. PMLR, 2021.
13
Causal Generative Explainers using Counterfactual Inference
A PREPRINT
Álvaro Parafita and Jordi Vitrià. Explaining visual models by causal attribution. In 2019 IEEE/CVF International
Conference on Computer Vision Workshop (ICCVW), pages 4167–4175. IEEE, 2019.
Judea Pearl. Causality. Cambridge university press, 2009.
Frederik Hvilshøj, Alexandros Iosifidis, and Ira Assent. Ecinn: efficient counterfactuals from invertible neural networks.
arXiv preprint arXiv:2103.13701, 2021a.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. arXiv
preprint arXiv:1410.8516, 2014.
Divyat Mahajan, Chenhao Tan, and Amit Sharma. Preserving causal constraints in counterfactual explanations for
machine learning classifiers. arXiv preprint arXiv:1912.03277, 2019.
Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box:
Automated decisions and the gdpr, 2018.
Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun Wang, Site Li, Ping Jia, and Jane You.
Data augmentation via latent space interpolation for image classification. In 2018 24th International Conference on
Pattern Recognition (ICPR), pages 728–733. IEEE, 2018.
Arnaud Van Looveren and Janis Klaise. Interpretable counterfactual explanations guided by prototypes. In Joint
European Conference on Machine Learning and Knowledge Discovery in Databases, pages 650–665. Springer, 2021.
Frederik Hvilshøj, Alexandros Iosifidis, and Ira Assent. On quantitative evaluations of counterfactuals. arXiv preprint
arXiv:2111.00177, 2021b.
Lisa Schut, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. Generating interpretable
counterfactual explanations by implicit minimisation of epistemic and aleatoric uncertainties. In International
Conference on Artificial Intelligence and Statistics, pages 1756–1764. PMLR, 2021.
14
","nanIn this paper, we investigate expanding the applicability of CGMs to Explainable AI (XAI). XAI refers to methods that offer human understandable interpretations for complex machine learning models. XAI techniques help to enrich the capacity of AI systems by providing understanding, transparency, fairness, and trust for the decisions made by AI systems [Dwivedi et al., 2023].

This paper explores explainability methods for image classifiers by focusing on the utilization of two causal deep generative architectures, namely, the DeepSCM [Pawlowski et al., 2020] and ImageCFGen [Dash et al., 2022] models.

The authors of ImageCFGen defined a counterfactual importance score using their BiGAN-based architecture, which measures how a classifier’s prediction changes when a given attribute (i.e., cause of the classifier’s input) changes. For instance, measuring how an “attractive” classifier’s output changes when the attribute “bald” changes. While this method proved useful in the work by Dash et al., it is not without limitations. Namely, that it is only defined for binary attributes and binary classifiers. These limitations motivate the methods described in this paper, which aim to provide image-based and attribute-based classifier explanations on datasets with continuous and categorical variables of interest representing the causes of an image. The novelty of these methods lies in the fact that they are easily implemented using any pretrained causal generative model such as DeepSCM and ImageCFGen, rather than requiring explanation-specific generative models trained with custom loss functions for individual classifiers [Van Looveren et al., 2021, Yang et al., 2021]. The source code for our experiments is available in a public Github repository.2"
"The advancement of artificial intelligence (AI) marks a significant shift towards artificial general intelligence (AGI). Interactive AI (IAI) addresses the challenges of AGI by emphasizing immediate and direct interaction between AI and users. IAI systems feature capabilities such as retrieval-augmented generation (RAG), which allows them to extract information from vast databases, and LangChain, which extends AI reasoning capabilities. The advantages of IAI over traditional human-in-the-loop (HITL) systems include customizability, personalization, better flexibility, and reduced bias. Wireless networks can particularly benefit from the adaptability of IAI, optimizing resource utilization and improving network performance.","In the evolving landscape of AI, the pursuit of artificial general intelligence (AGI) has gained considerable attention. This journey is characterized by the transition from rule-based algorithms to advanced learning models. During this phase, AI has demonstrated notable achievements, but limitations in adaptability and real-time responsiveness persist. To address these challenges, the concept of interactive AI (IAI) has emerged as a promising approach.","The exploration of IAI in networking is rooted in three main components: user interfaces, intelligent evaluation and adaptability, and advanced generative AI and network optimization. User interfaces in IAI systems incorporate language models (LLMs) and multi-modal techniques, enabling natural language interactions. Intelligent evaluation and adaptability involve real-time performance assessment and network configuration refinement based on user inputs and environmental data. Advanced generative AI and network optimization utilize algorithms like GDM and TBM for network design optimization, facilitating automated generation of network topologies and enhanced resource utilization.","The integration of IAI into networking offers tangible benefits across various layers. In the physical layer, IAI can improve channel estimation and optimization, leading to enhanced signal quality and reduced interference. In the network layer, IAI can optimize network traffic flows, resulting in reduced latency, improved bandwidth usage, and overall network resilience. In the application layer, IAI can enhance user interaction and experience through personalized and responsive interfaces.","IAI has the potential to revolutionize networking by enhancing network functionalities, improving user experience, and facilitating efficient network management. The integration of IAI into wireless networks holds particular promise due to the dynamic nature of these networks and their requirement for continuous adaptation to changing conditions. The seamless interplay between AI and human insights through IAI can pave the way for more intelligent, flexible, and user-centric networks.",Interactive AI with Retrieval-Augmented Generation for Next Generation Networking,"Ruichen Zhang, Hongyang Du, Yinqiu Liu, Dusit Niyato, Jiawen Kang, Sumei Sun, Xuemin Shen, H. Vincent Poor","1
Interactive AI with Retrieval-Augmented Generation
for Next Generation Networking
Ruichen Zhang*, Hongyang Du*, Yinqiu Liu*, Dusit Niyato, Fellow, IEEE, Jiawen Kang,
Sumei Sun, Fellow, IEEE, Xuemin (Sherman) Shen, Fellow, IEEE, and H. Vincent Poor, Life Fellow, IEEE
Abstract—With the advance of artificial intelligence (AI), the
emergence of Google Gemini and OpenAI Q* marks the direction
towards artificial general intelligence (AGI). To implement AGI,
the concept of interactive AI (IAI) has been introduced, which
can interactively understand and respond not only to human
user input but also to dynamic system and network conditions. In
this article, we explore an integration and enhancement of IAI in
networking. We first comprehensively review recent developments
and future perspectives of AI and then introduce the technology
and components of IAI. We then explore the integration of IAI
into the next-generation networks, focusing on how implicit and
explicit interactions can enhance network functionality, improve
user experience, and promote efficient network management.
Subsequently, we propose an IAI-enabled network management
and optimization framework, which consists of environment,
perception, action, and brain units. We also design the pluggable
large language model (LLM) module and retrieval augmented
generation (RAG) module to build the knowledge base and
contextual memory for decision-making in the brain unit. We
demonstrate the effectiveness of the framework through case
studies. Finally, we discuss potential research directions for IAI-
based networks.
Index Terms—IAI, networking, pluggable LLM module, AGI,
RAG, problem formulation.
I. INTRODUCTION
With the current development trajectory, artificial intelli-
gence (AI) is moving towards an important stage of realizing
artificial general intelligence (AGI). The evolution of AI from
its original rule-based algorithms to the adoption of advanced
learning models marks a significant shift in the field of
computing technology. This process is driven by the increasing
growth and complexity of data, requiring more sophisticated
AI solutions. While early AI models provided the foundation
for modern data-driven environments, their limitations in han-
dling dynamic and large-scale data have prompted the devel-
opment of more advanced and novel methods. Among these
R. Zhang, H. Du, Y. Liu, and D. Niyato are with the School of Com-
puter Science and Engineering, Nanyang Technological University, Sin-
gapore (e-mail: ruichen.zhang@ntu.edu.sg, hongyang001@e.ntu.edu.sg, yin-
qiu001@e.ntu.edu.sg, dniyato@ntu.edu.sg).
J. Kang is with the School of Automation, Guangdong University of
Technology, China (e-mail: kavinkang@gdut.edu.cn).
S. Sun is with the Institute for Infocomm Research, Agency for Science,
Technology and Research, Singapore (e-mail: sunsm@i2r.a-star.edu.sg).
X. Shen is with the Department of Electrical and Computer Engineering,
University of Waterloo, Canada (e-mail: sshen@uwaterloo.ca).
H. V. Poor is with the Department of Electrical and Computer Engineering,
Princeton University, Princeton, NJ08544, USA (e-mail: poor@princeton.edu).
* means equal contribution.
advancements, such as Google Gemini1 and OpenAI Q*2,
human-in-the-loop (HITL) systems highlight the importance
of integrating human insights into AI decision-making [1].
These systems combine human feedback with the AI’s learning
process, improving the accuracy and contextual understanding
of AI results. For example, a 2018 Stanford study showed that
AI combined with HITL outperformed human or AI analysis
alone in healthcare3. However, HITL systems have limitations
in adaptability and real-time responsiveness because they rely
on human input, which can be limited, unpredictable, and
erroneous.
To address these challenges and effectively develop AGI,
interactive AI (IAI) has been proposed. The primary difference
between IAI and HITL lies in their interaction modes, i.e.,
IAI emphasizes immediate and direct interaction between AI
and users, whereas HITL focuses on human participation and
supervision in the AI decision-making process [1]. IAI systems
are capable of instantaneously understanding user inputs,
such as voice commands, text messages, or other interactive
commands, and intelligently responding or executing tasks
based on these inputs. This ability not only enhances user
experience but also increases flexibility and effectiveness of
AI applications in dynamic environments. Integrating IAI with
technologies such as retrieval-augmented generation (RAG)
and LangChain can further personalize network operations [2].
For example, RAG enables IAI systems to extract information
from vast databases to enrich their responses and decisions.
Combined with LangChain, which extends AI reasoning ca-
pabilities, IAI can provide more context-aware solutions based
on the existing databases. The main advantages of IAI over
the limitations of HITL systems, especially when augmented
with RAG and LangChain, include:
• Customizabilty and Personalizability: IAI, enhanced
by RAG and LangChain, offers tailored solutions by
aligning AI responses closely with user preferences and
needs, resulting in more personalized and user-centric AI
applications.
• Better Flexibility: IAI’s direct user interaction, supported
by LangChain’s extended reasoning, can offer more
flexibility to adapt to different network scenarios and
users’ requirements, enhancing the system’s versatility
and satisfaction.
1https://deepmind.google/technologies/gemini/#introduction
2https://community.openai.com/t/what-is-q-and-when-we-will-hear-
more/521343
3https://scopeblog.stanford.edu/2018/05/08/artificial-intelligence-in-
medicine-predicting-patient-outcomes-and-beyond/
arXiv:2401.11391v1  [cs.NI]  21 Jan 2024
2
• Less Bias: The combination of IAI with RAG and
LangChain minimizes the reliance on human intervention,
thus reducing the potential for human bias in AI decisions
and leading to more objective outcomes.
It is particularly important to integrate IAI into wire-
less networks, given its dynamic nature and requirement to
continuously adapt the dynamic changes. Fortunately, the
capabilities of IAI are particularly promising in addressing
these challenges. Its interactive adaptive resource manage-
ment can optimize the utilization of network resources and
improve network performance. For example, in a network
with changing user requirements, IAI can dynamically allocate
bandwidth to maintain high performance given instantaneous
user experience feedback. Although the integration of IAI into
networking has several potential advantages, the following
issues need to be addressed:
• Q1: Why is IAI suitable for networking?
• Q2: Which networking challenges can IAI address?
• Q3: How can IAI with RAG be applied in networking?
Therefore, in this article, we attempt to provide forward-
looking research to answer the above “Why, Which, and
How"" questions. To the best of the authors’ knowledge, the
synergy between IAI and networking is still an open issue.
The contributions of this article are summarized as follows.
• A1: We first review some aspects of the development
of AI, then introduce the features, technologies, and
composition of IAI, and finally overview the IAI on
networking.
• A2: We explore the integration of IAI in networking,
focusing on how both implicit and explicit interactions
enhance network functionalities, improve user experi-
ence, and facilitate efficient network management.
• A3: We construct an IAI-enabled network management
and optimization framework. It consists of environment,
action, brain, and perception. More importantly, we de-
sign pluggable LLM and RAG modules to build the
knowledge base and contextual memory for decision-
making. Simulation results based on a real network
optimization case study verify the effectiveness of the
proposed framework.
II. OVERVIEW OF IAI AND NETWORKING
This section provides an overview of IAI, where some
abbreviations are summarized in Table I.
A. The development of AI
Phase 1: Traditional Artificial Intelligence (TAI) TAI
consists of rule-based systems designed for specific tasks
within well-defined parameters [3]. It leverages algorithms
such as support vector machines (SVMs) and other fundamen-
tal methods to excel at pattern recognition and basic predic-
tion tasks. Although TAI systems are effective at processing
structured data, their limited flexibility highlights the need for
more dynamic AI approaches, leading to the development of
discriminative AI/predictive AI.
Phase 2: Predictive AI (PAI) /Discriminative AI (DAI)
With the use of deep neural networks such as convolutional
TABLE I
SUMMARY OF ABBREVIATIONS.
Abbreviation
Full name
Description 
AGI
Artificial General Intelligence
AI capable of understanding, learning, and applying intelligence 
across a broad range of tasks at human-level proficiency
GAI
Generative Artificial Intelligence
AI systems that focus on creating novel data and patterns, often 
used for tasks involving innovation and design
IAI
Interactive Artificial Intelligence
AI that emphasizes immediate, direct interaction with users for 
decision-making and learning
MOE
Mixture of Experts
An approach in AI using multiple expert models, each specialized 
in different aspects of a dataset or problem
RAG
Retrieval Augmented Generation
An approach that enhances generative models by retrieving 
relevant information from database to improve the generation
LLM
Large Language Model
AI models designed to process and interact using human 
language, capable of handling extensive text data
HITL
Human-in-the-Loop
A system where human participation and oversight are 
integrated into the AI decision-making process
PAI
Predictive Artificial Intelligence
AI systems that focus on forecasting future events or trends 
based on historical data and predictive algorithms
TAI
Traditional Artificial Intelligence
Early AI models based on rule-based algorithms and structured 
data analysis
GAN
Generative Adversarial Network
AI models where two neural networks compete, often used for 
generating new data that mimics real data
DRL
Deep Reinforcement Learning
AI models learn to make decisions by trial and error, using deep 
learning to process complex inputs
CNN
Convolutional Neural Network
AI models effective for analyzing visual imagery, known for 
pattern recognition in structured data
GDM
Generative Diffusion Model
AI models that generate data through a process of iterative 
refinement, often used for precise outputs
neural networks (CNNs), PAI and DAI are good at learn-
ing special paradigms from large data sets [4]. PAI/DAI,
with their enhanced learning algorithms, advances beyond the
capabilities of TAI. However, their reliance on extensively
annotated datasets, primarily for classification and prediction,
has practical limitations, leading to the emergence of GAI.
Phase 3: Generative Artificial Intelligence (GAI) GAI
marks a new era where AI systems can create new data
and patterns, showcasing a degree of creativity [5]. With the
advent of generative diffusion models (GDMs) and generative
adversarial networks (GANs), AI’s role expands from analysis
to creation, which can produce innovative outputs and simu-
lations. However, GAI’s reliance on pre-existing data patterns
has limitations such as generating inaccurate results, leading
to the emergence of IAI.
Phase 4: Interactive Artificial Intelligence (IAI) IAI goes
beyond traditional data interaction paradigms by engaging
human users in dynamic and reciprocal information exchanges.
This stage allows the AI system to contextualize interactions
and learn from user input, thus facilitating adaptive feedback
mechanisms [6]. As a result, AI systems can bridge the gap
between AI and human interaction and then move towards
AGI.
For clarity, the evolution of AI development is illustrated in
Fig. 1.
B. Overview of IAI
IAI is based on the principle of dynamic information
exchange to meet the needs of administrators and human end-
users, where the foundation of IAI lies in the following AI
technologies:
Retrieval-Augmented Generation (RAG): RAG plays a
pivotal role in IAI framework by merging retrieval-based and
generative AI techniques. RAG allows IAI systems to access a
vast external knowledge base, retrieving relevant information
to augment the generation process [2].
Large Language Models (LLMs): LLMs are a cornerstone
of IAI, particularly due to their capacity to process and
3
GENERATIVE
Artificial Intelligence
GAN-2014
GAN
can
generate
realistic synthetic data
by a generator and a
discriminator.
DBM-2015
DBM
can
generate
realistic data samples by
starting with noise.
Creativity
Learn data distribution
Diversity
Fidelity
Data augmentation
• Features
Techniques
Generative 
adversarial network
Multi-modal interaction
Large language model
Retrieval-augmented 
generation
Generative diffusion
Mixture of experts
Interactive AI is designed
for user engagement. It
responds to user inputs
and
queries,
making
it
suitable for applications
like
customer
support,
virtual
assistants,
and
recommendation systems.
INTERACTIVE
Artificial Intelligence
User interfaces
Facilitate
communication
between the IAI system and
users, respond to specific user
queries
and
provide
personalized feedback
Evaluation feedback
The evaluation
assesses the
validity of outputs, with feedback
driving iterative improvements in
problem-solving accuracy.
Components:
GAI optimization
Deploy GAI methods to design
solutions
and
enhancements
for interaction scenarios.
DISCRIMIMATIVE
Artificial Intelligence
CNN-1989
LSTM-1997
Discriminative AI mainly
focuses on distinguishing
or distinguishing different
categories in the data set.
SVM-1995
Random forest-2001
The
algorithms
that
predict
or
classify
by
learning
rules
and
patterns from data.
Traditional
Artificial Intelligence
Reinforcement
Learning
Reinforcement
learning
(RL)
is
an
interdisciplinary area of machine learning
and optimal control concerned with how
an intelligent agent ought to take actions
in a dynamic environment to maximize the
cumulative reward.
Supportive
Intertwined
Incorporates
Enables
and
Advantages:
High efficiency
Efficient execution capabilities
can be achieved on various
tasks.
Strong adaptability
Ability to
adapt
to
a
wide
range of work environments
and needs
Innovation ability
Have the potential to innovate
in many fields such as science
and art
Dynamic traffic 
management
Channel estimation 
and optimization
Enhanced user interaction
Anomaly signal identification
Intelligent 
resource allocation
Human-machine interfaces
Advanced network security
Data analysis and 
feature extraction
Potential applications:
Evolutionary branch
Supplements 
and 
extensions
Integrate the characteristics of AI technology and RL technology
Fig. 1. The evolution of AI. For IAI, we highlight the role of the mixture of experts, large language models, deep reinforcement learning, retrieval-augmented
generation, and generative AI in promoting adaptability and interaction with users. Additionally, the components and advantages of IAI are also presented,
emphasizing its efficiency and adaptability in different applications such as optimization and traffic management.
generate human-like text, which facilitates complex interactive
dialogues. This ability is central to IAI’s focus on enhancing
user interaction, as LLMs enable AI systems to comprehend
and respond to natural language inputs in a conversational
manner, thereby significantly improving the interactive and
intuitive nature of AI systems [7].
Multi-modal Interaction: IAI systems are not limited to a
single modal; they are adept at understanding and responding
to various input types, such as text, voice, and images [8].
This multi-modal interaction capability is fundamental to IAI,
embodying the principle of versatility and adaptability in
interactions. It ensures that AI systems can engage with users
in the most natural and intuitive ways possible, adapting to
different interaction modes seamlessly.
Mixture of Experts (MOE): The MOE framework is an
integral component of IAI that embodies its responsiveness to
specialized situation-awareness. MOE models are composed
of multiple expert sub-models, each trained to handle different
aspects or subsets of the data [7]. This specialization enables
IAI systems to leverage the collective expertise of these
individual models to solve complex problems. The gating
mechanism is the core feature of MOE, which dynamically
determines the relevance of each expert to a given input,
thereby directing the input to the most appropriate expert.
Deep Reinforcement Learning (DRL): DRL embodies
the interactivity and adaptability of IAI. It revolves around
learning from interactions with the environment and making
decisions based on feedback, such as rewards and penalties
[9]. This principle aligns well with the idea that IAI learns
and develops through continuous interaction with the user or
environment.
Generative Diffusion Models (GDMs): GDMs improve
iteratively through interactions, consistent with the principles
of IAI. These models generate data by learning to reverse
a diffusion process, which essentially involves a series of
interacting steps [5]. Each iteration in the process presents
an opportunity for the model to adapt and refine its output,
emphasizing the IAI principle of continuous evolution and
response to new information.
Generative Adversarial Networks (GANs): GANs, with
their dueling network structure, inherently align with IAI’s
principles of interactive learning. The continuous learning
process within GANs, where each network learns from the
other, enables the system to adaptively refine its outputs [8].
C. Features of IAI
In the networking domain, IAI systems are characterized by
three main components, each of which is an integral part of
its functionality as follows:
User Interfaces: Modern IAI systems have revolutionized
user interfaces (UIs), extending beyond conventional text and
graphical interfaces. These systems incorporate LLMs and
multi-modal techniques, allowing users to interact with net-
work management systems through conversational language.
4
TABLE II
SUMMARY OF POTENTIAL ISSUES OF IAI.
Tech
Issues
Differences
Layers
Traditional methods
Interactive methods
IAI advantages
Channel 
Estimation and 
Optimization
-Available methods: Fourier-based techniques, CNNs
-Description: Relies on static models for signal estimation
-Cons: Not suited for dynamic environments, high overhead, prone to
model mismatch
-Available methods: MOE, GANs
-Description: Utilizes user feedback and algorithms for channel
optimization
-Pros: Resilient to changes, efficient, higher accuracy
Potential Advantages for Physical layer:
-Enhanced Accuracy: GANs provide refined channel
models 
improving 
signal 
clarity 
and 
reducing
interference.
-Predictive Modeling: GDMs enable precise
channel predictions, aligning real-world conditions.
-Real-Time Adaptation: DRL optimizes beamforming
in real-time for changing network environments.
-Reduced Complexity: Automated processes through
GANs and GDMs simplify the management of physical
layer operations.
Anomaly Signal 
Identification
-Available methods: Statistical analysis
-Description: Detects deviations using predefined criteria
-Cons: Poor adaptability, high false-positive rates, misses subtle anomalies
-Available methods: Multi-modal, LLMs
-Description: Uses LLMs for real-time anomaly detection from diverse
data
-Pros: Accurate, dynamic detection, reduced false alarms
Adaptive 
Beamforming
-Available methods: CVX, ZF/MRT/MMSE.
-Description: Uses mathematical modeling and fixed antenna patterns;
lacks change adaptability
-Cons: Inflexible to fast user movement and fast channel changes
-Available methods: RL, GANs
-Description: Adapts beamforming in real-time environments, GANs for
training scenarios.
-Pros: Adaptable, improves spectral efficiency
Dynamic Traffic 
Management
-Available methods: Static routing algorithms
-Description: Uses preset routes; ignores live network status
-Cons: Unresponsive to real-time conditions, may cause congestion
-Available methods: RL, GDMs
-Description: RL for traffic flow adaptively, GDMs for traffic prediction
-Pros: Better traffic handling, enhances network performance
Potential Advantages for Network layer:
-Optimized Traffic Flow: DRL and GDMs enable
dynamic
management
of
data
traffic,
reducing
congestion.
-Efficient Utilization: Intelligent allocation strategies
via DRL and MOE maximize network resource usage.
-Enhanced Security: GANs provide advanced threat
simulation for robust network security measures.
-Self-Improving
Systems:
DRL
algorithms
continuously learn and improve network performance.
Intelligent 
Resource 
Allocation
-Available methods: Static resource partitioning, heuristic algorithms
-Description: Allocates based on predictions; no real-time adjustments
-Cons: Inefficient under variable loads, risk of resource misallocation
-Available methods: RL, MOE
-Description: Allocates resources smartly, MOE for traffic pattern
analysis
-Pros: More efficient use of network resources, better service quality
Advanced Network 
Security
-Available methods: Signature-based detection systems
-Description: Identifies threats using known signatures
-Cons: May not identify new or variant forms of attacks quickly, leading to a
window of vulnerability
-Available methods: GANs, LLMs
-Description: GANs for attack simulation, LLMs for threat detection
-Pros: Detects new threats, adapts security protocols
Enhanced User 
Interaction
-Available methods: Rule-based command interfaces, GUIs
-Description: Command interfaces need exact syntax; GUIs offer visuals
-Cons: Steep learning curve for commands; GUIs may lack depth in
interaction
-Available methods: RL, RAG
-Description: Personalizes interactions, RAG for natural responses
-Pros: Customized experiences, higher user interaction
Potential Advantages for Application layer:
-Intuitive
Interactions:
LLMs
and
multi-modal
technologies enhance user interface responsiveness.
-Personalized Experiences: Multi-modal capability
allows for services based on user inputs and behaviors.
-In-Depth Analytics: LLMs process vast amounts of
data for analysis and extraction, improving decision-
making.
-User-Centric Design: The LLMs and multi-modal
technologies ensure interfaces cater to diverse user
needs.
Human-Machine 
Interface
-Available methods: Physical controls, voice commands
-Description: Tactile and auditory interaction options
-Cons: Accessibility issues for some users; voice commands fail in noise
-Available methods: LLMs, multi-modal
-Description: Integrates language and sensory data for interaction.
-Pros: Intuitive interfaces, complex data handling
Data Analysis and 
Feature Extraction
-Available methods: Batch processing, rule-based analytics
-Description: Processes large data volumes, uses set rules for analysis.
-Cons: Slow and intensive processing, might overlook new patterns
-Available methods: RAG, MOE
-Description: RAG for unstructured data analysis, MOE for sequential
data
-Pros: Deep analysis, enhanced machine learning inputs
A notable example is Agent GPT4, which employs LLMs
and multi-modal capability for interactive user interfaces in
a dynamic plan-execute pattern. Unlike standard UIs, Agent
GPT engages proactively with users, providing clarity for
ambiguous instructions and continually modifying its approach
based on user feedback.
Intelligent Evaluation and Adaptability: Network en-
vironments benefit from IAI systems that not only assess
performance using real-time analytics and simulation-based
testing but also adapt and evolve over time. These systems
integrate cognitive network operations to create a feedback
loop, continuously refining network configurations based on
user inputs and environmental data. This enables AI models
to develop their understanding strategies and enhance their
decision-making processes. Google’s VirusTotal Code Insight5
is one such system that employs an LLM and MOE for
script analysis, thus enabling human users to identify potential
threats through responsive feedback during use.
Advanced Generative AI and Network Optimization:
The core of IAI systems is the deployment of state-of-the-
art GAI. These systems utilize algorithms like GDM and TBM
for network design optimization. They facilitate the automated
generation of network topologies, simulate traffic for conges-
tion forecasting, and provide routing optimization solutions.
For instance, Google’s B4 network6 employs a combination
of MOE and GAN techniques for capacity planning and
traffic engineering, ensuring optimal data flow and resource
4https://agentgpt.reworkd.ai/de
5https://blog.virustotal.com/2023/04/introducing-virustotal-code-
insight.html
6https://www.b4networks.ca/
utilization. The generative process in these systems is iterative,
leveraging both historical and real-time data to proactively
adapt to evolving network conditions and user behavior trends.
D. IAI for Networking
IAI significantly benefits networking by integrating intelli-
gence across multiple operational layers. It enhances networks
from the physical layer to the application layer.
Physical Layer: In the physical layer, IAI can be used to
improve network efficiency and reliability. A typical example
is the application of channel estimation and optimization.
IAI, particularly DRL approaches, leverage feedback-oriented
neural network interaction to iteratively optimize beamforming
patterns based on real-time environmental feedback, improving
modulation techniques for robust communication [9]. This
leads to more efficient use of the spectrum, reduced inter-
ference, and enhanced signal quality, particularly in dynamic
environments where channel conditions frequently change.
Network Layer: In the network layer, IAI can be used
to streamline network operations and enhance performance.
A typical example is the application of dynamic network
traffic management. IAI, particularly DRL and GDM, can
dynamically manage and optimize network traffic flows by
continuously learning and adapting to traffic conditions [5].
This includes predictive traffic modeling, real-time congestion
management, and intelligent routing strategies, resulting in
reduced latency, optimized bandwidth usage, and improved
overall network resilience.
Application Layer: In the application layer, IAI can be used
to enhance user interaction and experience. A typical example
is the application of human-machine interfaces. IAI, partic-
5
TABLE III
SUMMARY OF IMPLICIT AND EXPLICIT INTERACTION IN NETWORKING.
01
02
03
04
05
06
Implicit Interaction in Networking
VS
Explicit Interaction in Networking
Accurate Information Flow
Real-time Update
Random
Interaction
Periodic
Interaction
Explicit Interaction 
with User Information
Implicit Interaction 
with Algorithm
Accurate Information Flow
Pre-Designed Process
Interaction 
Frequency
Interaction 
Frequency
Interaction 
Feature
Interaction 
Feature
Interaction 
Benefits
Interaction 
Benefits
1) Adaptive Signal Processing:
Utilizing
IAI
can
significantly
enhance
this
process
by
enabling the system to learn
and adapt its signal processing
strategies based on real-time
data and interactions.
2)
Predictive
Resource
Allocation:
Referring
to
the
allocation of network resources like
bandwidth and power based on
predicted
future
demands
and
usage patterns. Implementing IAI in
this domain can result in more
efficient and responsive resource
management.
1) User Feedback Interaction:
involving
users
providing
direct
input
to
influence
the
AI’s
decision-making
or
output
adjustments.
3) Anomaly Detection: Involving
identifying
unusual
patterns
or
behaviors
that
may
indicate
a
problem such as a security breach
or
network
failure.
IAI
can
significantly
improve
anomaly
detection
by
learning
complex
patterns and detecting anomalies in
real-time.
2)
Network
Configuration
and
Management:
Referring
to
the
process
of
setting
up
and
maintaining
network
parameters
and policies. The integration of
explicit IAI in this domain simplifies
complex tasks and enhances the
accuracy of configurations.
3) Human-Computer Interfaces in
Networks:
Encompassing
the
ways users interact with networked
computer systems. Explicit IAI can
revolutionize
HCI
by
facilitating
more interactive and integrated user
experiences.
ularly LLMs, can process and generate natural language for
user interaction, while multi-modal methods integrate visual,
auditory, and other data forms to enrich the interface [10]. This
leads to more personalized and engaging user interactions,
better accessibility for diverse user groups, and an overall more
responsive and intelligent application environment.
For clarity, the potential issues of IAI in different layers are
summarized in Table II.
III. INTERACTIVE APPLICATIONS IN NETWORKING
In this section, we explore the integration of IAI in network-
ing, focusing on how both implicit and explicit interactions.
For clarity, the summary of implicit and explicit interaction is
shown in Table III.
A. Implicit Interaction in Networking
Implicit interaction in networking refers to the process
where AI systems engage and adapt to their environment, in-
cluding network conditions and user behaviors, without direct
external inputs or commands. The benefits of such interactions
include increased system autonomy, improved adaptability to
changing conditions, and enhanced overall network efficiency
[9], [11], [12]. Some major applications of implicit interaction
in networking are as follows:
Adaptive Signal Processing: Adaptive Signal Processing
in wireless communications involves dynamically adjusting
signal processing algorithms in response to changing network
conditions. Utilizing IAI can significantly enhance this pro-
cess by enabling the system to learn and adapt its signal-
processing strategies based on real-time data and interactions.
For example, in [11], the authors proposed an IAI frame-
work for steganographic distortion learning framework. This
framework employs a GAN composing two subnetworks, i.e.,
a steganographic generator and a steganalytic discriminator,
which through adversarial training, learns the probabilities
of embedding changes in pixels to minimize detectability. In
this framework, implicit interactions in the form of adaptive
learning allow the model to evolve from simple random
embedding to advanced content-adaptive embedding, signifi-
cantly improving its security performance with each iteration.
Simulation results showed after 180,000 training iterations, the
IAI model’s system performance steadily improved, achieving
an average embedding rate close to the targeted capacity.
Predictive Resource Allocation: Predictive Resource Al-
location refers to the allocation of network resources like
bandwidth and power based on predicted future demands
and usage patterns. Implementing IAI in this domain can
result in more efficient and responsive resource management.
For example, in [9], the authors proposed a DRL-based IAI
method to optimize system energy efficiency (EE) by adjusting
beamforming vectors, power splitting ratios, and phase shifts,
considering users’ quality of service (QoS) and the transmit-
ter’s power constraints. The proposed IAI method implicitly
interacted with the whole network environment, learning to
fine-tune these parameters without direct human input to sat-
isfy energy harvesting and communication QoS requirements.
6
Simulation results reveal that this approach yields EE close
to an upper bound scheme (i.e., about 2% performance gap)
while significantly reducing computation time (i.e., five orders
of magnitude), particularly in dynamic wireless conditions.
Anomaly Detection: Anomaly Detection in networking
involves identifying unusual patterns or behaviors that may
indicate a problem, such as a security breach or network
failure. IAI can significantly improve anomaly detection by
learning complex patterns and detecting anomalies in real-
time. For example, in [12], the authors proposed an IAI
framework to automatically identify dependencies between
sensors for anomaly detection in multivariate time series data.
The IAI framework employed a Gumbel-Softmax Sampling-
based connection learning policy to automatically learn graph
structures depicting sensor dependencies, and integrates this
with graph convolutions and an IAI architecture for efficient
anomaly detection. The framework’s implicit interaction in-
volved determining bi-directional connections among sensors,
where high probability values imply strong connections, en-
hancing the system’s ability to predict and identify anomalies.
This method is effective as it incorporates user feedback and
network interactions to refine its anomaly detection capability
continually. Simulation results prove that the TBM-based IAI
framework improves precision, recall, and F1-score by at least
6%, 16%, and 11%, respectively, compared with traditional
methods in anomaly detection.
Lesson learned: The exploration of implicit IAI in net-
working underscores the integral role of automated adapt-
ability and self-learning in enhancing network efficiency and
security. A major benefit of implementing implicit IAI is its
proficiency in navigating the complexities of dynamic network
environments. Implicit IAI, through mechanisms like machine
learning algorithms and predictive analytics, excels in dis-
cerning and adjusting to these changes autonomously. For in-
stance, in adaptive signal processing, IAI has shown a notable
improvement in efficiency, with performance enhancements
measurable at approximately 10-15% in complex scenarios.
In resource allocation, IAI has not only achieved near-optimal
energy efficiency but also accelerated convergence speed by
up to five times compared to traditional methods. Additionally,
in anomaly detection, IAI’s continuous learning from network
interactions has led to a substantial increase in accuracy,
outperforming traditional methods by at least 6% to 16% in
key metrics. These improvements highlight IAI’s real-time
adaptability, faster processing, and reduced reliance on large
data sets.
B. Explicit Interaction in Networking
Explicit interaction in networking involves deliberate and
direct engagement between users or network administrators
and the AI system. This type of interaction facilitates pre-
cise control over network functionalities and enhances user
involvement in decision-making processes, leading to more
accurate and user-centric outcomes [13]–[15]. Some major ap-
plications of explicit interaction in networking are as follows:
User Feedback Interaction: User feedback interaction
involves users providing direct input to influence the AI’s
decision-making or output adjustments. IAI enhances this
process by enabling more intuitive and responsive interactions,
where user feedback can dynamically shape AI responses
or network adjustments. For example, in [13], the authors
introduced the Query Generation Assistant, a search interface
that uses the LLM-based IAI method to facilitate interac-
tive and automatic query generation in challenging search
scenarios such as cross-lingual retrieval. This system allows
users to actively interact in refining queries generated by
LLMs, providing feedback and edits throughout the process. It
significantly improves qualitative analysis in complex search
tasks and is a valuable tool for conducting human-in-the-loop
simulations.
Network Configuration and Management: Network con-
figuration and management refers to the process of setting
up and maintaining network parameters and policies. The
integration of explicit IAI in this domain simplifies complex
tasks and enhances the accuracy of configurations. For exam-
ple, in [14], the authors explored the use of the LLM-based
IAI method to manage network configuration tasks. They
introduce NETBUDDY, a system that translates high-level
natural language requirements into low-level network con-
figurations. This method involves explicit interactions where
NETBUDDY decomposes the translation process into multiple
steps, ensuring more accurate and efficient configurations, and
demonstrating considerable improvement in simplifying and
automating network management tasks. The simulation results
showed that the method adopted not only ensures network
accuracy but also increases efficiency by approximately 6
times compared with traditional ones.
Human-Computer Interfaces: Human-computer interfaces
(HCI) in networking encompasses the ways users interact with
networked computer systems. Explicit IAI can revolutionize
HCI by facilitating more interactive and integrated user experi-
ences. For example, in [15], the authors investigated machine-
in-the-loop creative writing using a multi-modal based IAI
method that suggested improvements through sight, sound,
and language, marking an innovative approach in AI-assisted
creativity. This method engaged human users in explicit inter-
actions, where they incorporated AI-generated suggestions into
their writing, a process involving adaptability and cognitive
integration. Simulations effectively demonstrated the potential
of IAI method to enhance the creativity of human-machine
collaboration.
Lesson learned: Explicit IAI interactions in networking
emphasize the critical role of user involvement in shaping AI-
driven functionalities. This approach highlights the integration
of user feedback into AI systems, resulting in more precise
and customized network solutions. Unlike conventional AI,
which relies on predefined algorithms, explicit IAI uses user
input to refine its decision-making. This direct engagement
aligns AI outputs with user-specific needs and preferences.
For instance, digital twins and semantic communications can
use explicit IAI to interpret and prioritize data based on its
meaning and relevance to user requirements. This not only
ensures accuracy but also personalizes network operations to
individual contexts. Furthermore, explicit IAI is able to create
a collaborative environment, merging human expertise with AI
7
Brain
Perception
Multi-modal 
inputs
Action
Analysis
Extraction
Planning Reasoning
Memory
Knowledge
Interaction
IAI 
Agent
Embeddings
Environment
IAI 
Agent
…
Prompt engineering
LLMs
Knowledge base
…
Text Embeddings
Storage
Learn
Retrieve
Summary
Recall
IAI 
Agent
IAI 
Agent
IAI
UAV    RIS  Multi-cell 
CF   WPT   MIMO 
Knowledge
Formulation
Model
Hi,
please
help
me
formulate
an
optimization problem. I have show
you the scenario latter. Based on
your knowledge and do it step-by-
step.
If
clear,
reply
me
ok
and
wait for the following instruction.
Sure, I can help you formulate
an optimization problem. Please
provide me with the scenario or
problem
you
would
like
to
optimize.
The scenario is about optimize the
energy
efficiency
in
RIS-assisted
SWIPT
networks
with
RSMA.
What
information
do
you
need
from
me?
Based on your knowledge and do it
step-by-step.
To
formulate
an
optimization
problem
for
optimizing
energy
efficiency
in
RIS-assisted
SWIPT
networks
with
RSMA,
I
would
need
some
specific
information
about
the
system
and the objectives you want to
achieve.
Here
are
some
key
details that would be helpful:
1. System Model: Please provide
a
description
of
the
RIS-
assisted
SWIPT
network
with
RSMA.
2. Objective Function: Specify
the
objective
you
want
to
optimize.
3.
Variables:
Identify
the
decision variables that can be
adjusted to optimize the system.
4.
Constraints:
Define
any
constraints
that
need
to
be
satisfied
in
the
optimization
problem.
I
want
to
maximize
EE
by
jointly
optimizing the transmit beamforming
vectors,
the
PS
ratios
of
SWIPT,
the common message rates of RSMA,
and
the
phase
shifts
of
RIS.
The
information and EH QoS requirements,
as well as the power budget at the
transmitter
are
considered
as
constraints.
The
optimization
problem
is
formulated as:
Maximize: EE({{w_k},{α_i}, c_i},{Φ}) 
Subject to: 
•
∑c_i ≤ min_i
R_i^{(C)}({{w_k}},{Φ},{α_i})
(Constraint 1)
•
c_i + R_i^{(P)}({{w_k}},{Φ},{α_i})
≥ ξ_i (Constraint 2)
•
Γ_i^{(EH)}({{w_k}},{Φ},{α_i}) ≥
μ_i (Constraint 3)
•
P_T({{w_k}}) ≤ P_max (Constraint
4)
•
Φ_n ∈ Θ (Constraint 5)
•
α_i ∈ (0,1) (Constraint 6)
•
c_i ≥ 0 (Constraint 7)
Great,
help
me
explain
the
above
factors based on your knowledge.
Certainly!
1.
System
Energy
Efficiency:
The
objective
of
the
optimization problem is to maximize
the system energy efficiency.
2. Decision Variables: The decision
variables
in
the
optimization
problem
are
{{w_k},{α_i},
c_i},
which represent the transmit power,
phase
shifts,
and
time
allocation,
respectively.
3.
Power
Budget
Constraint:
Constraint
4
states
that
the
total
transmit power should not exceed the
maximum power budget.
4.
Quality
of
Service
(QoS)
Constraints: Constraints 1, 2, and 3
are related to the QoS requirements
of the system.
5. Phase Shifts and Feasible Range:
Constraint
5
states
that
the
phase
shifts
of
the
reconfigurable
intelligent surface (RIS) elements.
6. Power Splitting Ratio: Constraint
6 specifies that the power splitting
ratio,
7.
Non-Negative
Time
Allocation:
Constraint
7
ensures
that
the
time
allocation is non-negative.
Plug-in Part
Context
History
…
Decision making
Database
RAG
Fig. 2. In the IAI-enabled problem formulation framework, three key components are presented: brain, perception, and action. The brain units with LLM
and RAG modules as the central processor, handling essential tasks such as memory retention, retrieval processing, and making decisions. Meanwhile, the
perception module is responsible for acquiring and interpreting diverse data from the environment. Lastly, the action module implements responses and interacts
with the environment, utilizing various tools for execution.
capabilities (i.e., MOE), thereby boosting the overall efficiency
and effectiveness of network management.
IV. CASE STUDY: IAI-ENABLED PROBLEM FORMULATION
FRAMEWORK
In this section, we propose a framework that utilizes an IAI
agent with RAG to help network users and designers formulate
optimization problems in the network domain.
A. Motivations
In wireless network resource allocation, modeling complex
real-world scenarios as mathematical optimization problems
have traditionally required a deep understanding of com-
plex equations and methods. This task can be challenging,
particularly for newcomers or those with interdisciplinary
backgrounds.
Fortunately, the IAI framework offers a transformative so-
lution to these challenges. By interpreting the network envi-
ronment and goals as defined by designers, the IAI system au-
tomatically formulates the appropriate optimization problem.
This advancement speeds up the problem formulation process
and importantly ensures that the optimization models accu-
rately reflect the intricate details of network scenarios. Acting
as a cognitive intermediary, the IAI framework enhances
human capabilities with AI-driven insights, markedly reducing
the complexity and manpower usually needed for such tasks.
Furthermore, by automating the problem formulation phase
[10], IAI significantly lowers the risk of human error, a crucial
aspect of complex network resource allocation. While this
innovation offers distinct advantages to new network designers
by making it easier to get involved, its main strengths are
in making the optimization process more straightforward,
efficiently managing network resources, and improving the
accuracy and trustworthiness of network models.
B. Proposed Framework
Accordingly, to effectively generate problem modeling, as
shown in Fig. 2, we propose an IAI-enabled problem formu-
lation framework, which consists of the following units.
Perception: The Perception component of the IAI-enabled
problem formulation framework is akin to human sensory
reception, drawing from diverse sources and modalities. This
multi-modal approach enables the IAI to assimilate informa-
tion from text, visuals, and numerical data. Upon receiving this
data, the system employs prompt engineering to transform raw
information into structured embeddings. These embeddings
are designed to encapsulate the complexity of the input data
in a format that is readily interpretable by the IAI agents.
Consequently, the Perception component prepares the IAI to
comprehend the nuances of the environment and facilitates
informed decision-making. In our framework, the Perception
component thus serves as the foundational interface between
multifaceted data inputs and the IAI’s cognitive mechanisms,
ensuring that the system’s responses are grounded in a com-
prehensive understanding of the environmental context.
Brain: The Brain is the central component of the IAI
system, functioning in a three-unit structure: database, storage,
8
and decision-making. The RAG database contains a wealth
of searchable academic texts, such as those pertaining to
unmanned aerial vehicles (UAVs), reconfigurable intelligent
surfaces (RISs), wireless power transfer (WPT), and more,
as its training dataset. This corpus of knowledge is trans-
formed into text embeddings that are stored within the IAI’s
knowledge base. For the decision-making, the system adopts
a plug-in architecture for selecting the appropriate LLM,
such as GPT 4, LLAMA 2, Gemini, and Bloom. When the
database unit and the decision-making unit are combined, they
form the RAG module. RAG allows the brain to infer, learn,
retrieve, and reason, combining its capabilities with database
resources to plan and execute actions to provide appropriate
strategies for generating problems. For the storage, it is a
repository of the agent’s historical observations, thoughts, and
actions. It enables the agent to effectively recall and apply
previous strategies when tackling complex reasoning tasks,
similar to how humans draw on memory to navigate unfamiliar
situations. The Brain’s architecture is integral to the IAI’s
learning process, providing a contextual understanding and
the ability to utilize historical insights for informed decision-
making, thereby enhancing the academic rigor of the system’s
cognitive process.
Action: Within the IAI framework, the Action component
responds to the Brain’s directions and performs tasks in the
network environment. This unit of the system follows a clear
four-step method. It starts by analyzing network designer
inputs, guided by the Brain’s insights. Then, it pulls useful
information from its knowledge base. Next, the module shapes
the network problem into a clear mathematical form. The final
step is to present the formulated optimization model. The
Action module turns plans into real results, making sure that
the IAI’s decisions have a direct and practical impact on the
network.
Environment:
The Environment component of the IAI
framework is pivotal in encapsulating the application domain
for the IAI agent’s operation. It represents the real-world
network scenarios as described by the network designer,
forming the foundation for the IAI’s problem formulation
process. For instance, in a scenario where a network designer
wants to formulate an optimization problem for a RIS-assisted
SWIPT network with RSMA, the interaction begins with
the network designer’s initial request for assistance. The IAI
agent then prompts the network designer for specific details,
such as the system model, the optimization objective, the
decision variables, and any necessary constraints. Through this
iterative dialogue, the network designer provides the required
information step-by-step. Then, the network designer might
describe the network’s configuration for optimizing EE, and
the IAI agent will use its information to develop an appropriate
optimization problem. This interactive process allows the IAI
system to thoroughly understand the network designer-defined
environment and generate an optimized problem model that
aligns with the specific characteristics and requirements of the
network scenario. It showcases how the Environment compo-
nent acts as a collaborative stage, integrating network designer
inputs with IAI functionality to create tailored problem-solving
approaches for complex network environments.
Fig. 3. The chunk size versus the number of interaction rounds required to
solve the network optimization task, where k represents the number of chunks
that the LLM puts into context in each round of interaction.
C. Experiments
Experimental Settings: To explore the effectiveness of the
proposed IAI agent, we apply it to solve a real-world network
optimization problem. In our experiments, the pluggable LLM
module is implemented by OpenAI APIs for calling the GPT-
4 model. The network-oriented knowledge base and context
memory are built on LangChain.
Effectiveness of IAI Agent: As shown in Fig. 2, the user
intends to optimize the energy efficiency in RIS-assisted si-
multaneous wireless information and power transfer (SWIPT)
networks with rate splitting multiple access (RSMA) [9]. Tra-
ditionally, the network designer needs to search numerous arti-
cles, select an appropriate model, and apply it to the problem.
With the help of an IAI agent, such a process can be realized
automatically through four rounds of user-agent interactions.
To be specific, the user first states the requirements, i.e.,
formulating a network optimization problem. Here, the chain-
of-thought prompting is applied to enhance the reasoning
ability of the IAI agent. Then, the user further illustrates the
specific scenario to the IAI agent, which then returns the step-
by-step guidance of the entire optimization process. Afterward,
the users clarify the optimization objective. Empowered by
information retrieval techniques, the IAI agent will jointly
search the local knowledge base and perform the inference.
The resulting problem formulation perfectly aligns with the
ground truth (i.e., the standard problem formulation from
academic journals). Moreover, the user can acquire detailed
explanations of all the factors.
Knowledge Base Settings: With the effectiveness of the
IAI agent being verified, we further evaluate the influence of
knowledge base settings on the interaction quality. Specif-
ically, we adjust the chunk size for organizing knowledge
embeddings in RAG module and test the number of interaction
rounds required for solving the aforementioned task. As shown
in Fig. 3, when k=1 and 3, setting chunk size as 2000 or 3000
can lead to the best interaction quality since the knowledge
chunks can be effectively fetched. In contrast, if the chunk size
decreases to 1000, the problem cannot be completely formu-
lated within 10 rounds of interactions because the knowledge
9
I want to maximize EE by jointly optimizing the transmit
beamforming vectors, the PS ratios of SWIPT, the common
message rates of RSMA, and the phase shifts of RIS. The
information and EH QoS requirements, as well as the power budget
at the transmitter are considered as constraints.
System Description
The optimization problem is formulated as:
Maximize: 𝐸𝐸({𝑤𝑘, 𝛼𝑖, 𝑐𝑖}, Φ)
Subject to: 
•
∑𝑐_𝑖 ≤ min
i
𝑅𝑖
(𝐶)({𝑤𝑘}, Φ, 𝛼𝑖)
•
𝑐𝑖 + 𝑅𝑖
(𝑃)({𝑤𝑘}, Φ, 𝛼𝑖) ≥ 𝜉𝑖
•
Γ𝑖
(𝐸𝐻)({𝑤𝑘}, Φ, 𝛼𝑖) ≥ 𝜇𝑖
•
𝑃𝑇({𝑤𝑘}) ≤ 𝑃𝑚𝑎𝑥
•
Φ𝑛 ∈ Θ
•
𝛼𝑖 ∈ (0,1)
•
𝑐𝑖 ≥ 0
IAI modeling
I want to create an 
IAI model with 
powerful algorithm 
to formualte problem
My model is 
feasible
Interactive 
with model
Doing 
alone
Ideas from 
model
Personal 
idea
How can a network 
designer create a 
model with only 
basic descriptions
The optimization problem is formulated as:
Maximize: 𝐸𝐸({𝑤𝑘, 𝛼𝑖, 𝑐𝑖}, Φ)
Subject to: 
•
∑𝑐_𝑖 ≤ min
i
𝑅𝑖
(𝐶)({𝑤𝑘}, Φ, 𝛼𝑖)
•
𝑐𝑖 + 𝑅𝑖
(𝑃)({𝑤𝑘}, Φ, 𝛼𝑖) ≥ 𝜉𝑖 (Wrong constraint)
•
Γ𝑖
(𝐸𝐻)({𝑤𝑘}, Φ, 𝛼𝑖) ≥ 𝜇𝑖
•
𝑃𝑇({𝑤𝑘}) ≤ 𝑃𝑚𝑎𝑥
•
Φ𝑛 ∈ Θ
•
𝛼𝑖 ∈ (0,1)
•
𝑐𝑖 ≥ 0
Manual modeling
System Solution
The proposed IAI model 
can help network designers 
by capturing essential 
information and generate 
creative ideas
RL framework
 
 
 
 
  
  
                     
 
 
 
 
  
  
  
            
                                
                          
              
Using the same PPO-based
DRL method to solve the
above modeling problems
Environment
Value
Agent Net
Critic Net
Action
Reward
State
I miss essential 
components
The optimization problem is formulated as: 
Maximize: 𝐸𝐸({𝑤𝑘, 𝛼𝑖, 𝑐𝑖}, Φ)
Subject to: 
•
∑𝑐_𝑖 ≤ min
i
𝑅𝑖
(𝐶)({𝑤𝑘}, Φ, 𝛼𝑖)
•
𝑐𝑖 + 𝑅𝑖
(𝑃)({𝑤𝑘}, Φ, 𝛼𝑖) ≥ 𝜉𝑖
•
Γ𝑖
(𝐸𝐻)({𝑤𝑘}, Φ, 𝛼𝑖) ≥ 𝜇𝑖
•
𝑃𝑇({𝑤𝑘}) ≤ 𝑃𝑚𝑎𝑥
•
Φ𝑛 ∈ Θ
•
𝛼𝑖 ∈ (0,1)
•
𝑐𝑖 ≥ 0
•
∀𝑖 ∈ 𝑁, ∀𝑘 ∈ 𝐾
Real modeling
Experiment Result
Fig. 4. Comparison of system performance under various optimization problem generation methods. The figure displays the effectiveness of the proposed
IAI framework modeling against traditional real modeling (upper bound) and manual modeling approaches, where the PPO-based DRL method is set as the
solution method to demonstrate the performance results.
that can be referred to is too limited. Note that extremely large
chunk size is also undesirable. For instance, when the chunk
size equals 4000, the problem formulation also fails since the
LLM can hardly extract useful knowledge from large chunks.
When the chunk size reaches 5000, the IAI agent cannot even
perform inference due to the limited context size. If increasing
k to 10, i.e., allowing the IAI agent to retrieve more chunks in
each round, the interaction quality gets improved when chunk
size equals 1000. However, the context oversize error happens
in all other settings.
Performance Comparison: To demonstrate the efficacy
of our proposed IAI framework, Fig. 4 shows an example
to present a comparison of different ways for generating
optimization problems. It is evident that the first step in
creating an optimization problem involves providing a detailed
problem description. Our IAI framework then interacts with
this description, as illustrated in Fig. 2, to automatically derive
the corresponding optimization problem. In contrast, network
designers often rely on their experience to manually create
optimization problems. However, this manual approach can
be prone to errors, especially when designers are novices or
unfamiliar with certain aspects, such as overlooking or incor-
rectly defining constraints (e.g., decoding RSMA constraints in
our example). Such oversights can lead to flawed optimization
problems. Upon applying the same PPO-based optimization
method for solving, it is observed that the performance of
algorithms under our framework closely matches that of
the original real problem formulation (i.e., in [9]), whereas
manually designed optimization problems yield the lowest
results. This difference is attributed to the strength of the
IAI framework, which leverages LLM capabilities and features
within LangChain to generate precise optimization problems
through an interactive process. Therefore, the IAI framework
ensures a more accurate and efficient problem formulation
compared to traditional manual methods.
Lesson learned: Through the above experiments, we can
observe that the IAI goes beyond GAI, which only considers
the quality of generated content. Instead, the IAI agent is
designed to help users accomplish specific tasks. Therefore,
not only the core LLM but also the settings for other modules
10
that guarantee the smoothness and quality of interaction (e.g.,
the knowledge base and memory) should be well crafted.
V. FUTURE DIRECTIONS
In this section, we outline three main future directions for
the improvement of IAI-enabled networking.
Integration with Emerging Technologies: IAI can enhance
B5G/6G networks, particularly in real-time data processing,
adapting to bandwidth and latency needs. Edge computing
boosts IoT and smart city operations. Blockchain integration
uses IAI for more secure, decentralized networks. IAI also
makes semantic communications more context-sensitive and
improves ISAC systems’ accuracy. These enhancements show
IAI’s pivotal role in modern network technologies.
Security Aspects of IAI-Enabled Networks: Improving
security in IAI networks is crucial. Future research should fo-
cus on IAI-driven protocols for early threat detection and adap-
tive response, automating security management, and evolving
with emerging threats.
Evaluation of IAI systems: It is important to design a
model that can evaluate IAI. In the future, AI evaluation
criteria should be given to evaluate models generated by IAI
instead of human user evaluation.
VI. CONCLUSION
In this article, we have explored the integration and en-
hancement of IAI in networking. We have proposed a problem
formulation framework by using IAI with RAG, where the
effectiveness of the framework was verified through simulation
results. Finally, some potential research directions for IAI-
based networks were outlined.
REFERENCES
[1] W. Xu, M. J. Dainoff, L. Ge, and Z. Gao, “Transitioning to human
interaction with ai systems: New challenges and opportunities for HCI
professionals to enable human-centered AI,” Int. J. Hum-Comput. Int.,
vol. 39, no. 3, pp. 494–518, 2023.
[2] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,
H. Küttler, M. Lewis, W.-T. Yih, T. Rocktäschel et al., “Retrieval-
augmented generation for knowledge-intensive nlp tasks,” NIPS, vol. 33,
pp. 9459–9474, 2020.
[3] Y. Lu, “Artificial intelligence: a survey on evolution, models, applica-
tions and future trends,” J. Manag. Anal., vol. 6, no. 1, pp. 1–29, 2019.
[4] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, “A survey of convolutional
neural networks: analysis, applications, and prospects,” IEEE Trans.
Neural. Netw. Learn. Syst., 2021.
[5] H. Du, R. Zhang, Y. Liu, J. Wang, Y. Lin, Z. Li, D. Niyato, J. Kang,
Z. Xiong, S. Cui et al., “Beyond deep reinforcement learning: A tutorial
on generative diffusion models in network optimization,” 2023, arXiv
preprint arXiv:2308.05384.
[6] S. Teso, Ö. Alkan, W. Stammer, and E. Daly, “Leveraging explanations
in interactive machine learning: An overview,” Front. Robot. AI, vol. 6,
p. 1066049, 2023.
[7] Z. Chen, Z. Wang, Z. Wang, H. Liu, Z. Yin, S. Liu, L. Sheng,
W. Ouyang, Y. Qiao, and J. Shao, “Octavius: Mitigating task interference
in MLLMs via MoE,” 2023, arXiv preprint arXiv:2311.02684.
[8] R. Zhang, K. Xiong, H. Du, D. Niyato, J. Kang, X. Shen, and H. V. Poor,
“Generative AI-enabled vehicular networks: Fundamentals, framework,
and case study,” 2023, arXiv preprint arXiv:2304.11098.
[9] R. Zhang, K. Xiong, Y. Lu, P. Fan, D. W. K. Ng, and K. B. Letaief,
“Energy efficiency maximization in RIS-assisted SWIPT networks with
RSMA: A PPO-based approach,” IEEE J. Sel. Areas Commun., vol. 41,
no. 5, pp. 1413–1430, 2023.
[10] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,
S. Jin, E. Zhou et al., “The rise and potential of large language model
based agents: A survey,” 2023, arXiv preprint arXiv:2309.07864.
[11] W. Tang, S. Tan, B. Li, and J. Huang, “Automatic steganographic
distortion learning using a generative adversarial network,” IEEE Signal
Proces. Lett., vol. 24, no. 10, pp. 1547–1551, 2017.
[12] Z. Chen, D. Chen, X. Zhang, Z. Yuan, and X. Cheng, “Learning
graph structures with transformer for multivariate time-series anomaly
detection in IoT,” IEEE Internet Things J., vol. 9, no. 12, pp. 9179–9189,
2021.
[13] K. D. Dhole, R. Chandradevan, and E. Agichtein, “An interactive query
generation assistant using LLM-based prompt modification and user
feedback,” 2023, arXiv preprint arXiv:2311.11226.
[14] C. Wang, M. Scazzariello, A. Farshin, D. Kostic, and M. Chiesa,
“Making network configuration human friendly,” 2023, arXiv preprint
arXiv:2309.06342.
[15] N. Singh, G. Bernal, D. Savchenko, and E. L. Glassman, “Where to hide
a stolen elephant: Leaps in creative writing with multimodal machine
intelligence,” ACM Trans. Comput. Hum. Interact., vol. 30, no. 5, pp.
1–57, 2023.
","The development of AI has undergone several phases, starting with traditional rule-based systems. Subsequently, generative artificial intelligence (GAI) marked a new era where AI systems could create novel data and patterns, exhibiting a degree of creativity. However, the advent of interactive AI (IAI) goes beyond these traditional data interaction paradigms by engaging human users in dynamic and reciprocal information exchanges.nan"
"This paper addresses the node repair problem of Reed-Solomon (RS) coded distributed storage systems. Specifically, to overcome the challenges of multiple-node failures of RS codes under the rack-aware storage model, we employ good polynomials to guide the placement of the conventional RS codes into racks and then propose a novel repair framework for the resultant rack-aware RS codes, which can transform its repair to that under the homogeneous storage model. As applications of our repair framework, firstly we present the repair scheme of multiple-node failures for some existing constructions, which can only repair a single-node failure before. Secondly, we deduce several new constructions of rack-aware RS codes supporting the repair of multiple-node failures.","Maximum Distance Separable (MDS) codes, particularly RS codes, have been widely employed as a traditional solution in practical storage systems like Facebook f4 [17], Google Colossus [2], and Hadoop Distributed File System [15]. In RS coding, a file is split into k data blocks, each represented by a symbol of a finite field Fqt. In distributed storage systems, node failure is a common problem. To tolerate node failures, the k symbols are encoded into n > k symbols and distributed across different nodes. If a failure occurs in the system, the conventional solution is to download k symbols from any k surviving nodes, thereby repairing the failed symbol. However, this naive repair method scheme is inefﬁcient due to the excessive information downloaded, in fact, the whole file.","Additionally, in a practical setting, the repair mechanism may work when the total amount of failed nodes reaches a given threshold. Therefore, the case of multiple failures is interesting from a practical viewpoint. For multiple-node repair of RS codes, in general, there are two models called centralized model and cooperative model. In the centralized model, a repair center is responsible for the repair of all failed nodes [8], [9], [22]. While in the cooperative model, in addition to downloading data from helper nodes, the replacement nodes are allowed to collaboratively exchange their individual information [19], [20], [24].
For the repair problem of RS codes, most research has primarily concentrated on the homogeneous storage model, in which nodes are distributed uniformly in different locations. However, in modern data centers, storage nodes are often distributed in hierarchical topologies. Therefore, storage nodes are organized into several equally sized groups and stored in different racks, which is referred to as rack-aware storage model. In this model, nodes within the same rack have a signiﬁcantly lower transmission cost compared to the inter-rack transmission. As a result, the intra-rack transmission is naturally considered to be free without counting into the repair bandwidth. The repair of RS codes based on the rack-oriented model has been discussed in [11], [25]. In [11], Jin et al. introduced the techniques of the degree decent method and good polynomials into the design of rack-aware RS codes, enabling a linear alphabet size in the code length. Subsequently, Chen and Barg modiﬁed the code family of [7] for the homogeneous storage model to propose rack-oriented codes achieving the optimal bandwidth for repairing a single failure, but whose alphabet size is exponential with the code length. However, the existing works are devoted to single-node recovery, with limited attention given to the exploration of multiple-node failures in rack-aware RS codes.","In this work, we focus on the repair problem of RS codes with multiple failures under the rack-aware storage model. Our contributions are summarized below:

• A novel repair framework for rack-based RS codes is introduced by leveraging the technique of good polynomials to strategically place nodes within racks. By employing this approach, the multiple-node repair problem of a rack-aware RS code is reduced to the task of respectively repairing multiple codewords of an RS code under the homogeneous storage model.

• The existing rack-aware RS codes in [11], [25] are modiﬁed to repair multiple-node failures through our repair framework. Unlike the previous approach, which only accommodated a single failed node, our framework demonstrates the capability to handle multiple failures within one rack and even the worst case, i.e., the loss of an entire rack.

• By employing different good polynomials, a series of new rack-aware RS codes are constructed, which have good repair properties.","In this paper, we employed good polynomials to arrange nodes of RS codes on racks and propose a generic repair framework for handling multiple failures within racks. Build-ing upon this, the problem of multiple-node recovery in RS codes under the rack-aware storage model is transformed into the repair problem of RS codes under the homogeneous storage model. This transformation leverages the beneﬁts of both rack-aware and homogeneous RS codes, providing a solution for multiple-node repair in the rack-aware storage model.
In this way, we generalized the existing constructions to support multiple-node failures and further designed several new constructions using different good polynomials. These proposed codes can be repaired by means of the known schemes of single-node repair.",A Transformation of Repairing Reed-Solomon Codes from Rack-Aware Storage Model to Homogeneous Storage Model,"Yumeng Yang, Han Cai, Xiaohu Tang","arXiv:2401.11390v1  [cs.IT]  21 Jan 2024
1
A Transformation of Repairing Reed-Solomon
Codes from Rack-Aware Storage Model to
Homogeneous Storage Model
Yumeng Yang, Han Cai, Member, IEEE, and Xiaohu Tang, Senior Member, IEEE
Abstract—In this paper, we address the node repair problem
of Reed-Solomon (RS) coded distributed storage systems. Specif-
ically, to overcome the challenges of multiple-node failures of
RS codes under the rack-aware storage model, we employ good
polynomials to guide the placement of the conventional RS codes
into racks and then propose a novel repair framework for the
resultant rack-aware RS codes, which can transform its repair
to that under the homogeneous storage model. As applications
of our repair framework, ﬁrstly we present the repair scheme
of multiple-node failures for some existing constructions, which
can only repair a single-node failure before. Secondly, we deduce
several new constructions of rack-aware RS codes supporting the
repair of multiple-node failures.
Index Terms—Distributed storage, Reed-Solomon code, rack-
aware model.
I. INTRODUCTION
Maximum Distance Separable (MDS) codes, particularly RS
codes, have been widely employed as a traditional solution
in practical storage systems like Facebook f4 [17], Google
Colossus [2], and Hadoop Distributed File System [15]. In RS
coding, a ﬁle is split into k data blocks, each represented by
a symbol of a ﬁnite ﬁeld Fqt. In distributed storage systems,
node failure is a common problem. To tolerate node failures,
the k symbols are encoded into n > k symbols and distributed
across different nodes. If a failure occurs in the system, the
conventional solution is to download k symbols from any k
surviving nodes, thereby repairing the failed symbol. However,
this naive repair method scheme is inefﬁcient due to the
excessive information downloaded, in fact, the whole ﬁle.
To improve the efﬁciency of node repair in RS codes,
Guruswami and Wootters [21] proposed a linear repair scheme
for a single failure that signiﬁcantly reduces the repair band-
width. Particularly, they introduced the trace function collec-
tion technique to repair RS codes, allowing a smaller subﬁeld
symbol of each helper node to be sufﬁcient for recovering a
single erased symbol. This seminal idea has sparked signiﬁcant
interest in the repair problem of RS codes, leading to extensive
subsequent research for single-node recovery [1], [3], [7],
[14], [18], [22]. On one hand, considerable attention has been
paid to designing non-trivial repair schemes for RS codes
satisfying explicit parameter ranges with a reasonable ﬁeld
size [3], [21], [18], [22]. On the other hand, other research has
Y.
Yang,
H.
Cai,
and
X.
Tang
are
with
the
Information
Se-
curity
and
National
Computing
Grid
Laboratory,
Southwest
Jiaotong
University, Chengdu, China (email: yangyumeng@my.swjtu.edu.cn, han-
cai@swjtu.edu.cn, xhutang@swjtu.edu.cn).
aimed at constructing RS codes that approximate the optimal
repair bandwidth [1], [7], [14], [22], known as the cut-set
bound [4], nevertheless which normally require huge ﬁeld size.
Additionally, in a practical setting, the repair mechanism may
work when the total amount of failed nodes reaches a given
threshold. Therefore, the case of multiple failures is interesting
from a practical viewpoint. For multiple-node repair of RS
codes, in general, there are two models called centralized
model and cooperative model. In the centralized model, a
repair center is responsible for the repair of all failed nodes
[8], [9], [22]. While in the cooperative model, in addition
to downloading data from helper nodes, the replacement
nodes are allowed to collaboratively exchange their individual
information [19], [20], [24].
For the repair problem of RS codes, most research has
primarily concentrated on the homogeneous storage model, in
which nodes are distributed uniformly in different locations.
However, in modern data centers, storage nodes are often
distributed in hierarchical topologies. Therefore, storage nodes
are organized into several equally sized groups and stored in
different racks, which is referred to as rack-aware storage
model. In this model, nodes within the same rack have a
signiﬁcantly lower transmission cost compared to the inter-
rack transmission. As a result, the intra-rack transmission
is naturally considered to be free without counting into the
repair bandwidth. The repair of RS codes based on the rack-
oriented model has been discussed in [11], [25]. In [11],
Jin et al. introduced the techniques of the degree decent
method and good polynomials into the design of rack-aware
RS codes, enabling a linear alphabet size in the code length.
Subsequently, Chen and Barg modiﬁed the code family of [7]
for the homogeneous storage model to propose rack-oriented
codes achieving the optimal bandwidth for repairing a single
failure, but whose alphabet size is exponential with the code
length. However, the existing works are devoted to single-
node recovery, with limited attention given to the exploration
of multiple-node failures in rack-aware RS codes.
In this work, we focus on the repair problem of RS codes
with multiple failures under the rack-aware storage model. Our
contributions are summarized below:
• A novel repair framework for rack-based RS codes
is introduced by leveraging the technique of good
polynomials to strategically place nodes within racks.
By employing this approach, the multiple-node repair
problem of a rack-aware RS code is reduced to the task
of respectively repairing multiple codewords of an RS
2
code under the homogeneous storage model.
• The existing rack-aware RS codes in [11], [25] are
modiﬁed to repair multiple-node failures through our
repair framework. Unlike the previous approach, which
only accommodated a single failed node, our framework
demonstrates the capability to handle multiple failures
within one rack and even the worst case, i.e., the loss of
an entire rack.
• By employing different good polynomials, a series of
new rack-aware RS codes are constructed, which have
good repair properties.
The remainder of this paper is organized as follows. In
Section II, we introduce some basic concepts and recall the
trace repair framework of RS codes under the homogeneous
model. In Section III, we present a repair framework for
multiple-node recovery for rack-aware RS codes. In Section
IV, we interpret and extend the existing constructions by our
repair framework to support multiple-node failures in a single
rack. Moreover, we employ three classes of good polynomials
to design new rack-aware RS codes. Finally, the conclusion is
given in Section V.
II. PRELIMINARIES
In this section, we review linear repair schemes of RS codes
under the homogeneous storage model and rack-aware storage
model, respectively.
A. Deﬁnitions and Notations
For any positive integers a < b, denote by [a] the set
{1, 2, · · · , a} and [a, b] the set {a, · · · , b}. Let q be a prime
power and Fq be a ﬁnite ﬁeld of q elements. Denote by Fqt
the ﬁeld extension of Fq with [Fqt : Fq] = t. Let Fqt[x] be the
ring of polynomials over Fqt. The trace function from Fqt to
Fq is a polynomial deﬁned by
TrFqt/Fq(x) = x + xq + · · · + xqt−1,
which is an Fq-linear function. For simplicity, we denote
TrFqt/Fq as Tr except otherwise speciﬁed.
Let {β1, β2, · · · , βt} be a basis of Fqt
over Fq and
{β⊥
1 , β⊥
2 , · · · , β⊥
t } be its dual basis satisfying
Tr(βiβ⊥
j ) =
(
0,
i ̸= j;
1,
i = j.
Then, any α ∈ Fqt can be represented by {β⊥
1 , · · · , β⊥
t },
whose coefﬁcients are uniquely determined by Tr(βiα), i ∈
[t], i.e.,
α =
t
X
i=1
Tr(βiα)β⊥
i .
(1)
Let U be a subspace of Fqt with the dimension s over Fq,
then a linearized polynomial LU(x) over Fqt is deﬁned by
LU(x) = Πu∈U(x − u).
Clearly, the trace function is a special case of the linearized
polynomial when U is a subﬁeld of Fqt.
Deﬁnition 1 (Reed-Solomon Code). Let A = {α1, · · · , αn}
be the set of distinct elements over Fqt. A Reed-Solomon code
of length n and dimension k with evaluation points A is
deﬁned as
RS(n, k, A) ={(f(α1), f(α2), · · · , f(αn)) :
f ∈ Fqt[x], deg(f) < k}.
Similarly,
a
Generalized
Reed-Solomon
code
GRS(n, k, A, ν) can be deﬁned by the set of vectors
{(ν1f(α1), · · · , νnf(αn)) : f ∈ Fqt[x], deg(f) < k},
where ν = (ν1, · · · , νn) ∈ (F∗
qt)n.
Deﬁnition 2 (Dual Code). Let C be a linear code over Fqt of
length n. The dual code C⊥ of code C is deﬁned as
C⊥ = {(c′
1, c′
2, · · · , c′
n) :
n
X
i=1
c′
ici = 0,
∀(c1, · · · , cn) ∈ C}.
Precisely, (RS(n, k, A))⊥ = GRS(n, n − k, A, ν), where
νi = Πj̸=i(αi − αj)−1, i ∈ [n]. For simplicity, from now on
we omit the multipliers νi for i ∈ [n] in the involved GRS
code because it does not affect our subsequent discussion.
B. Repairing Reed-Solomon Codes under the Homogeneous
Storage Model
Assume that a ﬁle M is divided into k data blocks and
encoded into a codeword using an RS code of length n over
Fqt. Then, the n symbols of the codeword are distributed
across n independent storage nodes. Since each symbol is
represented by an element over Fqt, the contents of a node
can then be regarded as t sub-symbols over Fq.
1) Single Failure: When one single node fails, a new
replacement node contacts d (k ≤ d ≤ n − 1) surviving nodes
and downloads bi sub-symbols from each helper node i ∈ [d]
to recover the desired symbol at the failed node. The repair
bandwidth b is deﬁned as the amount of all the sub-symbols
downloaded by the repair center, i.e., b = Pd
i=1 bi.
In the literature [21], Guruswami and Wootters gave an
exact characterization of linear repair schemes for RS code
with a single failure and d = n − 1 help nodes. Given an
RS code C = RS(n, k, A) deﬁned over the ﬁnite ﬁeld Fqt
such that any polynomial f(x) ∈ Fqt[x] of degree less than
k corresponds to a codeword of C. Suppose that the symbol
f(αi∗) is failed, where αi∗ ∈ A. The repair procedure of
Guruswami and Wootters’ scheme proceeds as follows.
Step 1: Find t polynomials {g1(x), · · · , gt(x)} ∈ Fqt[x]
corresponding to t codewords of C⊥ such that {gl(αi∗) : l ∈
[t]} forms a basis of Fqt over Fq. Assume that gl(αi∗) = βl
for l ∈ [t]. Then, one can deduce the following parity-check
equations:
gl(αi∗)f(αi∗) = βlf(αi∗) = −
X
i̸=i∗
gl(αi)f(αi), l ∈ [t].
(2)
3
Step 2: By applying the trace function to (2), obtain repair
equations
Tr(gl(αi∗)f(αi∗)) = −
X
i̸=i∗
Tr(gl(αi)f(αi))
(3)
for all l
∈
[t]. Thus, node i∗ downloads sub-symbols
{Tr(γi,wf(αi)) : i ∈ [n]\{i∗}; w ∈ [bi]} from the remaining
n − 1 nodes, where {γi,w
:
w
∈
[bi]} is a basis of
SpanFq(gl(αi) : l ∈ [t]) over Fq and bi = RankFq{gl(αi) :
l ∈ [t]}.
Step 3: Node i∗ computes the repair traces {Tr(βlf(αi∗)) :
l ∈ [t]} from (3) and thus recover the failed symbol
f(αi∗) =
t
X
i=1
Tr(βif(αi∗))β⊥
i
by means of (1).
Obviously,
node
i∗
sufﬁces
to
download
bi
=
RankFq{gl(αi) : l ∈ [t]} sub-symbols from node i ∈ [n]\{i∗}.
Therefore, the repair bandwidth is
b =
X
i∈[n]\{i∗}
bi =
X
i∈[n]\{i∗}
RankFq{gl(αi) : l ∈ [t]}.
(4)
In the above approach, the repair of an RS code is converted
to searching for suitable dual codewords to minimize the repair
bandwidth b in (4). This seminal work led to the efﬁcient
single-node repair schemes of RS codes in [3], [7], [14], [18],
[21], [22].
2)
Multiple Failures: The repair of multiple failures is
divided into two cases centralized repair and cooperative
repair, according to whether there exists a repair center.
Roughly speaking, when ǫ (1 < ǫ ≤ n − k) nodes fail, the
centralized repair model assigns a repair center responsible for
downloading all necessary data from the helper nodes and then
repairing all failed nodes. In contrast, under the cooperative
repair model, each replacement node ﬁrstly independently
downloads data from helper nodes and next exchanges data
with each other. It should be noted that concerning the repair
bandwidth, the former model only counts the amount of
information downloaded by the repair center, whereas the latter
considers the total information transmission, including both
downloaded and exchanged data.
Indeed, there are few multiple-node repair schemes of RS
codes under these two models, which are all generalized from
Guruswami and Wootters’ repair framework. Currently, only
one explicit construction for the centralized storage model
exits [8], while the known ones for the cooperative repair
model require that the number of failed nodes is at most 3
[19], [20], [24].
C. Repairing Reed-Solomon Codes under the Rack-Aware
Storage Model
Under the rack-aware model, the n storage nodes are equally
organized into ¯n groups, i.e., ¯n|n. A group is referred to as a
rack, each containing u = n
¯n storage nodes. Once a rack occurs
1 ≤ ǫ ≤ u node failures, a repair operation is performed below.
Let (ci∗,1, · · · , ci∗,u) be the symbols stored in i∗-th rack.
Without loss of generality, assume that the ǫ symbols
ci∗,1, · · · , ci∗,ǫ at ǫ nodes fail respectively. Then, the repair
center in rack i∗ connects to a distinguished node, called
relayer, in each of ⌊ k
u⌋ ≤ ¯d ≤ ¯n − 1 helper racks, to request
¯b1,¯b2, · · · ,¯b ¯d sub-symbols respectively. Next, each relayer
accesses all the symbols within the same rack to form some
linear combinations as the desired sub-symbols. After that,
the repair center recovers the failed ǫ symbols with the help
of surviving u − ǫ nodes within rack i∗. Note that, unlike
the homogeneous storage model, this model is characterized
by heterogeneousness between intra-rack and inter-rack. More
precisely, the nodes inside the same rack share the storage
information for free such that only the cross-rack transmission
is taken into account of the repair bandwidth, i.e.,
b =
¯d
X
i=1
¯bi
(5)
which results in a signiﬁcant beneﬁt compared to the homo-
geneous coding.
Similarly, repair models involving multiple-rack failures
can be derived from Section II-B2. In centralized repair, a
repair center downloads data from helper racks instead of
individual helper nodes. After centrally repairing the failed
nodes, the repaired symbols are transmitted back to each
failed rack. In collaborative repair, racks containing the failed
nodes individually download data from helper racks and then
collaboratively repair the failed racks through information
interaction among themselves.
In [11], Jin et al. generalized the repair framework in [21]
to the single-node recovery of RS codes under the rack-aware
storage model. Up to now, in this model, no result on multiple
failures is known.
III. A REPAIR FRAMEWORK OF RACK-AWARE
REED-SOLOMON CODES
In this section, we transform the repair of a rack-oriented
RS code into a conventional repair problem of an RS code
under the homogeneous storage model.
A. Rack-Aware Reed-Solomon Codes via Good Polynomials
We begin with the formal deﬁnition of RS codes under the
rack-aware storage model.
Deﬁnition 3 (Rack-Aware Reed-Solomon Code). Let u be the
size of rack and A = {αi,j : i ∈ [¯n]; j ∈ [u]} be the set of
distinct elements over Fqt. A rack-aware Reed-Solomon code
of length n = ¯nu and dimension k = ¯ku + v (0 ≤ v ≤ u − 1)
with evaluation points A is deﬁned as
RSrack(n, k, A, u) ={(f(α1,1), f(α1,2), · · · , f(α¯n,1), · · · ,
f(α¯n,u)) : f ∈ Fqt[x], deg(f) < k},
such that the nodes in i-th rack can be represented as
(f(αi,1), f(αi,2), · · · , f(αi,u)), i ∈ [¯n].
Hereafter, we construct rack-aware RS codes by applying
good polynomials to RS codes.
Deﬁnition 4 (Good Polynomial [6]). A polynomial h(x) ∈
Fqt[x] of degree u is called a good polynomial if there exists
4
a partition A = ∪¯n
i=1Ai over Fqt of size n with |Ai| = u, such
that h(x) remains constant on each set Ai. In other words,
h(α) = yi for any α ∈ Ai, where yi is a constant in Fqt for
any i ∈ [¯n].
Construction 1. Let n, k, u, ¯n be positive integers with u | n
and ¯n = n
u. Let f(x) ∈ Fqt[x] be a polynomial with degree at
most k −1 and h(x) be a good polynomial over A = ∪¯n
i=1Ai.
Then, based on the good polynomial h(x), the conventional
RS codeword (f(x), x ∈ A) can be transformed into a rack-
aware RS codeword in the array form of





f1(α1,1)
f1(α1,2)
· · ·
f1(α1,u)
f2(α2,1)
f2(α2,2)
· · ·
f2(α2,u)
...
...
· · ·
...
f¯n(α¯n,1)
f¯n(α¯n,2)
· · ·
f¯n(α¯n,u)




 ,
(6)
where fi(x) ≡ f(x) (mod h(x) − yi) with deg (fi) < u for
i ∈ [¯n] and Ai = {αi,1, αi,2, · · · , αi,u} ⊂ A denote the roots
of h(x) − yi = 0 over Fqt. For i ∈ [¯n], the i-row denotes the
i-th rack.
In the following, we introduce a useful lemma for the repair
of the rack-aware RS codes in (6).
Lemma 1. Let f(x) and h(x) respectively be two polynomials
of degree k−1 and u over Fqt. Set s = ⌈ k
u⌉. For any y ∈ Fqt,
denote the residue polynomial f(x) mod (h(x) − y) by
u−1
X
j=0
Hj(y)xj ≡ f(x) mod (h(x) − y).
Then, Hj(y) is a polynomial of deg(Hj(y)) ≤ s − 1.
Proof: Herein we apply the Euclidean algorithm to f(x)
as follows:
f(x)
=
h(x)u1(x) + v1(x),
u1(x)
=
h(x)u2(x) + v2(x),
...
us−1(x)
=
h(x)us(x) + vs(x),
where the polynomials u1(x), v1(x), · · · , us(x), vs(x) satisfy
0 ≤ deg(v1(x)), · · · , deg(vs(x)) < u and us(x) = 0.
Therefore,
f(x) =hs−1(x)vs(x) + hs−2(x)vs−1(x) + · · ·
+ h(x)v2(x) + v1(x).
which gives
f(x) mod (h(x) − y)
≡ys−1vs(x) + ys−2vs−1(x) + · · · + yv2(x) + v1(x).
This is to say, we can arrange the polynomial in the form
of
f(x) mod (h(x) − y)
≡Hu−1(y)xu−1 + Hu−2(y)xu−2 + · · · + H0(y),
where deg(Hj(x)) ≤ s − 1. This completes the proof.
B. Repair of Rack-Aware Reed-Solomon Codes
In this subsection, we propose a repair framework for rack-
aware RS codes given in (6) capable of handling multiple
failures. To this end, we utilize good polynomials to transform
the problem of multiple-node recovery into the recovery of a
homogeneous RS code.
First of all, we prove the following result by Lemma 1,
which is crucial for our transformation.
Theorem 1. Let y1, · · · , y¯n be ¯n distinct elements over Fqt
such that h(α) = yi for α ∈ Ai, where deg(h(x)) = u and
|Ai| = u for 1 ≤ i ≤ ¯n. Suppose that deg(f(x)) < k and the
residue polynomial fi(x) ≡ f(x) mod (h(x) − yi) is of the
form
fi(x) =
u−1
X
j=0
ei,jxj, 1 ≤ i ≤ ¯n.
(7)
Then, (e1,j, e2,j, · · · , e¯n,j) forms a codeword of an [¯n, ≤ ⌈ k
u⌉]
RS code with evaluation points ¯A = {yi : i ∈ [¯n]} for any
j ∈ {0, · · · , u − 1}.
Proof: By Lemma 1, we have
fi(x) =
u−1
X
j=0
Hj(yi)xj,
together with (7) which implies
ei,j = Hj(yi).
Recall that k = ¯ku + v, 0 ≤ v ≤ u − 1. It then follows
from Lemma 1 that Hj(y) is a polynomial of degree less than
⌈ k
u⌉. Thus, (e1,j, e2,j, · · · , e¯n,j) forms a codeword of an RS
code of dimension at most ⌈ k
u⌉ for any j ∈ {0, · · · , u−1} by
Deﬁnition 1.
Notably, Theorem 1 bridges the repair of RS codes under
the homogeneous storage model and that under the rack-
aware storage model. We brieﬂy illustrate the main idea in
Fig. 1. It follows from (7) that for any i ∈ [¯n], acquiring
the data (f(αi,1), · · · , f(αi,u)) is equivalent to obtaining the
coefﬁcients (ei,0, · · · , ei,u−1) of fi(x). Assume that there are
ǫi∗ failures in rack i∗. Then, in order to repair rack i∗, it is
sufﬁcient to recover {ei∗,j : j ∈ [u − ǫi∗, u − 1]} (or {ei∗,j :
j ∈ [0, ǫi∗−1]}) since after that we can compute the remaining
u − ǫi∗ coefﬁcients from the u − ǫi∗ equations determined by
the u − ǫi∗ surviving nodes within rack i∗. Again according
to Theorem 1, (e1,j, · · · , e¯n,j) forms an RS codeword of
dimension at most ⌈ k
u⌉ for any j ∈ [u − ǫi∗, u − 1]. Note that
an [¯n, ≤ ⌈ k
u⌉] RS code can be regarded as a subcode of an
[¯n, ⌈ k
u⌉] RS code when they share the same evaluation points.
Thus, we can repair it as an [¯n, ⌈ k
u⌉] RS code. As a result, we
can get the desired coefﬁcients {ei∗,j : j ∈ [u − ǫi∗, u − 1]}
(or {ei∗,j : j ∈ [0, ǫi∗ − 1]}) by applying some known repair
schemes under the homogeneous storage model to such ǫi∗
codewords respectively. That is, the repair of a rack-oriented
RS code is therefore transformed into a traditional repair of
an RS code under the homogeneous storage model.
Now, we are going to introduce the repair scheme and
analyze its bandwidth. Assume that there are m racks suf-
fering failures, indexed by the set {i∗
1, · · · , i∗
m} ⊆ [¯n]. For
5
Rack 1
...
Rack 2
Rack ¯n
h(A1) = y1
...
...
...
f(α1,1)
f(α1,2)
f(α1,u)
h(A2) = y2
h(A¯n) = y¯n
f(α2,1)
f(α¯n,1)
f(α2,2)
f(α2,u)
f(α¯n,2)
f(α¯n,u)
...
...
...
e1,0
e1,u−1
e2,0
e¯n,0
e¯n,u−1
e2,u−1
RSrack(n, k, A, u)
RS

¯n, ⌈k
u⌉, ¯A

e1,u−2
e¯n,u−2
e2,u−2
...
...
...
Fig. 1.
The ﬁgure shows the procedure of our repair framework.
1 ≤ τ ≤ m, let ǫτ denote the number of failures in rack
i∗
τ. Let D = {i1, · · · , i ¯d} ⊆ [¯n]\{i∗
1, · · · , i∗
m} be the set of
helper racks of size ¯d. The following repair procedure shows
the concrete repair of rack-aware RS codes in (6).
Repair procedure of rack-aware RS codes organized by
good polynomial h(x) for multiple failures:
Step 1. For i ∈ D, the i-th helper rack computes the residue
polynomials fi(x) through Lagrange interpolation on u
pairs {(αi,j, f(αi,j)) : i ∈ D, j ∈ [u])}, and then gets
(ei,0, ei,1, · · · , ei,u−1).
Step 2. Set ǫ = max1≤τ≤m ǫτ. For 1 ≤ t ≤ ǫ, deﬁne
Rt ≜ {i∗
τ : ǫτ ≥ t, 1 ≤ τ ≤ m}.
Given t ∈ [ǫ], denote
Cu−t = (e1,u−t, e2,u−t, · · · , e¯n,u−t),
which is a codeword of an [¯n, ⌈ k
u⌉] RS code by Theorem
1. Then, for 1 ≤ t ≤ ǫ, respectively invoke the
known repair schemes for homogeneous storage model
to recover the |Rt| symbols
{ei,u−t : i ∈ Rt}
(8)
of the RS codeword Cu−t.
Step 3. Given i∗
τ and τ ∈ [m], let the corresponding eval-
uation points of the u − ǫτ
surviving nodes be
αi∗
τ ,j1, · · · , αi∗
τ ,ju−ǫτ . Then, for each i∗
τ, from (7) one
has the following u − ǫτ equations
u−ǫτ−1
X
j=0
ei∗τ ,jαj
i∗τ ,j1
=
∗,
...
u−ǫτ−1
X
j=0
ei∗τ ,jαj
i∗τ ,ju−ǫτ
=
∗,
where ∗ denotes some certain values determined by the
coefﬁcients {ei∗τ ,u−t : t ∈ [ǫτ]} obtained in (8) and the
values {f(αi∗τ ,j1), · · · , f(αi∗τ ,ju−ǫτ )} of the surviving
nodes in rack i∗
τ. Note that the coefﬁcient matrix is an
invertible Vandermonde matrix. Then, the repair center
is able to obtain the remainder coefﬁcients {ei∗τ ,u−t :
t ∈ [ǫτ + 1, u]} and thus the entire polynomial
fi∗
τ (x) =
u−1
X
j=0
ei∗
τ ,jxj, τ ∈ [m],
which can ﬁgure out the ǫτ failures
{f(αi∗
τ ,ju−ǫτ +1), · · · , f(αi∗
τ ,ju)}.
According to (5), the repair bandwidth is just to count the
data transmission across the rack to repair the symbols in (8)
in Step 2.
Theorem 2. Let c ∈ C be a rack-aware RS codeword in
Construction 1. Assume that the repair bandwidth b|Rt| is
capable to recover the symbols in (8) from |D| helper racks
for 1 ≤ t ≤ ǫ. Then, for the m racks {i∗
τ : τ ∈ [m]} ⊆ [¯n] of
c containing {ǫτ : τ ∈ [m]} failures respectively, the failures
can be recovered with bandwidth
b =
X
1≤t≤ǫ
b|Rt|.
Remark 1. The known repair schemes used in step 2 may be
for single failure or multiple ones depending on |Rt| = 1 or
not, where the latter can be under the centralized model or
cooperative model depending on the application scenarios.
Remark 2. We note that the repair procedure is only related
to the number of failures within the racks, i.e., ǫ1, ǫ2, . . . , ǫm
regardless of which exact ǫτ nodes in the i∗
τ-th rack experience
failure. For example, as shown in Figure 1, when rack 2 con-
tains a failure no matter any one of {f(α2,1), · · · , f(α2,u)},
the coefﬁcient to be repaired is always e2,u−1.
Remark 3. Importantly, the repair procedure can be per-
formed parallelly on repairing ǫ codewords of an [¯n, ⌈ k
u⌉]
RS code from ¯d helper racks to improve the time efﬁciency.
For example, as shown in Figure 1, these two codewords
6
{e1,u−1, · · · , e¯n,u−1} and {e1,u−2, · · · , e¯n,u−2} can be re-
paired in parallel, with the difference being that the former
performs repair on two failures, while the latter performs
repair on a single failure. In contrast, a serial repair is also
feasible, whereby the previously repaired rack participates in
the repair of the next one as a helper rack, with the beneﬁt of
a reduced bandwidth requirement due to the increased number
of connected helper nodes.
IV. SOME EXPLICIT RACK-AWARE REED-SOLOMON
CODES BY MEANS OF GOOD POLYNOMIALS
In this section, we focus on the case that multiple failures
take place within one rack since 1) This paper is mainly
devoted to transforming the repair problem under the rack-
aware model to that under the homogeneous model, whereas
most of the known results on repairing RS codes under the
later model are for a single failure; and 2) It can be easily
generalized to repair multiple-rack failures.
A. The Repair of Single-Rack Failure
We ﬁrst simplify the repair procedure to the case of single-
rack failure.
Repair procedure of rack-aware RS codes organized by
good polynomial h(x) for failures within a single-rack:
Since Steps 1 and 3 are the same as those in the general
procedure, herein we only reﬁne Step 2.
Step 2. Let ǫ be the number of failures in i∗-th rack. In this
case,
R1 = R2 = · · · = Rǫ = {i∗}.
Given t ∈ [ǫ], denote
Cu−t = (e1,u−t, e2,u−t, · · · , e¯n,u−t)
as an [¯n, ⌈ k
u⌉] RS codeword under the homogeneous
storage model. Then, only 1 coordinate ei∗,u−t of Cu−t
needs to be repaired. By employing the known single-
node repair schemes under the homogeneous storage
model (e.g., [3], [7], [14], [18], [21], [22]) to the ǫ
codewords Cu−t, obtain all the desired coefﬁcients
{ei∗,u−1, · · · , ei∗,u−ǫ}.
For the repair scheme, we have the following conclusion,
which is a direct corollary of Theorem 2 and the proof is
omitted.
Corollary 1. Let c ∈ C be a rack-aware RS codeword in
Construction 1. For any i∗ ∈ [¯n] and 1 ≤ ǫ ≤ u, the ǫ failed
nodes of the i∗-th rack in c can be recovered by the above
repair procedure, whose repair bandwidth can be given as
b = ǫb′,
where b′ denotes the repair bandwidth of a single failure with
|D| helper nodes in Step 2.
Then we demonstrate our repair procedure with an illustra-
tive example.
Example 1. Let C = RS(n = 24, k = 7, A = F24, ) be an RS
code over F24. Deﬁne a good polynomial
h(x) = TrF24/F22 (x) = x + x4,
with kernel U = {0, 1, γ5, γ10}, where γ is a root of primitive
polynomial x4 + x + 1 of F24. Obviously, A = F24 = ∪4
i=1Ai
can be partitioned into ¯n = 4 distinct sets A1 = U, A2 =
γ + U, A3 = γ6 + U, A4 = γ3 + U with h(Ai) = 0, 1, γ5, γ10
respectively. Then, according to (6), the n = 24 nodes can be
organized into ¯n = 4 racks based on h(x), each containing
u = 4 nodes, i.e.,




f1(0)
f1(1)
f1(γ5)
f1(γ10)
f2(γ)
f2(γ2)
f2(γ4)
f2(γ8)
f3(γ6)
f3(γ7)
f3(γ9)
f3(γ13)
f4(γ3)
f4(γ11)
f4(γ12)
f4(γ14)



 ,
(9)
where fi(x) ≡ f(x) (mod h(x) − yi) with deg (fi) < 4 for
i ∈ [4],
Note that i-th row in (9) corresponds to the i-th rack. With-
out loss of generality, assume that 3 nodes f(1), f(γ5), f(γ10)
in 1-th rack fail and D = {2, 3, 4} is the set of helper
racks, which is shown in Fig. 2. The repair procedure can
be performed as follows.
Step 1. The relayer in helper rack i ∈ {2, 3, 4} computes the
residue polynomials fi(x) and then obtains its coefﬁ-
cients
(ei,0, ei,1, ei,2, ei,3).
Step 2. For t ∈ [3],
C4−t = (e1,4−t, e2,4−t, e3,4−t, e4,4−t)
forms a codeword of an [¯n = 4, ⌈ k
u⌉ = 2] RS code
over F24 with evaluation points ¯A = F22 according
to Theorem 1. Since ¯n − ⌈ k
u⌉ ≥ 22−1, we employ the
seminal single-node repair scheme in [21]. Let the dual
codewords deﬁned by polynomials
¯gm,w(x) =
ηmTrF22/F2(βw(x − y1))
x − y1
= ηmTrF22/F2(βwx)
x
, m ∈ [2], w ∈ [2],
where {ηm : m ∈ [2]} = {1, γ} is a basis of F24 over
F22, and {βw : w ∈ [2]} = {1, γ5} is a basis of F22
over F2. Then, for any t ∈ [3], the repair center in rack
1 can download data given in Table 1 from the relayers
in helper racks to obtain {TrF24/F2(ηmβwe1,4−t) : m ∈
[2]; w ∈ [2]; t ∈ [3]}. Since {ηmβw : m ∈ [2]; w ∈
[2]} = {1, γ, γ5, γ6} forms a basis of F24 over F2, one
can recover e1,3, e1,2, e1,1 by means of (1).
TABLE I
DOWNLOAD SYMBOLS
Helper Rack
Download
Rack 2
TrF24 /F2(e2,4−t), TrF24 /F2(γe2,4−t)
Rack 3
TrF24 /F2(
e3,4−t
γ5
), TrF24 /F2(
e3,4−t
γ4
)
Rack 4
TrF24 /F2(
e4,4−t
γ10
), TrF24 /F2(
e4,4−t
γ9
)
7
Rack 1
Rack 2
Rack 4
h(A1) = 0
f(0)
f(1)
f(γ10)
h(A2) = 1
h(A4) = γ10
f(γ)
f(γ3)
f(γ2)
f(γ8)
f(γ11)
f(γ14)
e1,0
e1,3
e2,0
e4,0
e4,3
e2,3
RSrack
8
• Single-Rack Repair for Chen and Barg’s Construc-
tion [25]
Let k = u¯k+v, 0 ≤ v ≤ u−1 and q be a power of a prime
with u|(q −1). Let pi be distinct primes such that pi ≡ 1 mod
¯s = ¯d−k′+1 and pi > u for i ∈ [¯n], where k′ = ⌈ k
u⌉. We take
the smallest ¯n primes. For any i ∈ [¯n], let λi be an element of
degree pi over Fq, namely, [Fq(λi), Fq] = pi. Deﬁne the ﬁnite
ﬁeld Fi := Fq(λj : j ∈ [¯n]\{i}) and F := Fq(λ1, · · · , λ¯n),
i.e., F = Fi(λi). Let K be an extension of F with degree ¯s.
It is known that K is also an extension of Fq with degree
t = [K : Fq] = ¯sΠ¯n
i=1pi.
Consider an RS code C = RS(n, k, A) over Fqt with A =
∪¯n
i=1{λij : j ∈ [u]}, where λij = λiλj−1 and λ ∈ Fq is
an element of order u. Since λi is a generator of Fqpi over
Fq, and pi is a prime with pi > u, the element λu
i satisﬁes
Fq(λu
i ) = Fqpi as well .
1) Let
h(x) = xu,
then h(Ai) = λu
i , where Ai = {λij : j ∈ [u]} and
i ∈ [¯n]. Then, place the u nodes corresponding to Ai
into i-th rack, i.e.
(f(λi1), f(λi2), · · · , f(λiu))
are stored in i-th rack, i ∈ [¯n].
2) Suppose that ǫ failures occur in the i∗-th rack. D ⊂
[¯n]\{i∗} represents the set of helper racks of size ¯d.
According to the repair procedure given in Section IV-A,
the repair problem is transformed to repair a single
failure of an [¯n, k′] Reed-Solomon code deﬁned by
¯A = {λu
i : i ∈ [¯n]} under the homogeneous storage
model, which has optimal repair property given in [8].
Let
¯gl(x) = ¯gw,ν(x) = γwxνΠi∈[¯n]\{{i∗}∪D}(x − λu
i ),
where ν ∈ [0, ¯s − 1] and {γw : w ∈ [pi∗]} are elements
of K such that {γwλuν
i∗ : w ∈ [pi∗], ν ∈ [0, ¯s−1]} forms
a basis of K over Fi∗. Therefore, for j ∈ [u − ǫ, u − 1],
rack i∗ downloads
b′ = |{TrK/Fi∗ (γwei,j) : i ∈ D, w ∈ [pi∗]}| = ¯dpi∗
symbols over Fi∗ from the relayers of helper racks,
which can be regarded as downloading
¯dt
¯d−k′+1 sub-
symbols over Fq, then one can repair ǫ coefﬁcients
{ei∗,u−ǫ, · · · , ei∗,u−1}
(11)
of fi∗(x). Then the failed symbols can be repaired from
(11) and the surviving nodes within the rack.
Corollary 3. The rack-aware RS codes given in [25] with
1 ≤ ǫ ≤ u failures located on the same rack can be repaired
with bandwidth
ǫ ¯dt
¯d−⌈ k
u ⌉+1 sub-symbols.
For the case of ǫ = 1, our repair scheme achieves the
same system bandwidth as that in [25] when u|k but slightly
sacriﬁces a bit bandwidth when u ∤ k.
C. New Constructions of Rack-Aware Reed-Solomon Codes
for Single-Rack Failure
In this subsection, we employ the good polynomials pro-
posed by Tamo and Barg [6] to organize nodes of RS codes,
leading to some new constructions of rack-aware RS codes
with efﬁcient repair bandwidth. To begin with, we introduce
three classes of good polynomials in [6].
Suppose that u = mqa, where m|(qt − 1) and 0 ≤ a ≤ t.
Let E be a multiplicative subgroup of F∗
qt with the order m
and U, V be two additive subgroups of Fqt with the size qa.
There are three classes of good polynomials over Fqt.
• When a = 0 and m > 1,
h(x) = xm
(12)
is a good polynomial satisfying h(x) = h(g) for any
x ∈ gE, where g ∈ F∗
qt.
• When a > 0 and m = 1,
h(x) =
a
X
i=0
θixqi
(13)
is a good polynomial satisfying h(x) = h(b) for any
x ∈ b + U, where b ∈ Fqt, θi ∈ Fqt with θ0 ̸= 0, θa ̸= 0.
• When a > 0 and m > 1, e|t and m|(qe − 1),
h(x) = Πm
i=1Πv∈V (x + v + βi)
is a good polynomial satisfying h(x) = h(α) for any
x ∈ ∪m
i=1(αβi + V ), where α ∈ F∗
qt\V , V is an additive
subgroup of Fqt with the size qa that is closed under
the multiplication by the ﬁeld Fqe, and β1, · · · , βm are
the m-th degree roots of unity in Fqt. Assume that V
contains the element 1, then the above polynomial can
be rewritten as
h(x) = (
a/e
X
i=0
θixqei)m,
(14)
where θi ∈ Fqt satisfy Pa/e
i=0 θi = 0, θ0 ̸= 0, and θa/e ̸=
0.
By Construction 1, we employ the good polynomials in
(12)-(14) to organize the nodes of RS codes into different
racks. Consequently, we can transform the repair of the
resultant rack-oriented RS code into repairing a short RS
code under the homogeneous storage model, which has been
pursued in literature [3], [7], [14], [18], [21], [22]. With the
help of those known repair schemes and Corollary 1, we obtain
some rack-aware RS codes with efﬁcient repair properties,
which are presented in Table II. In this table, we provide
explicit formulations of good polynomials denoted as h(x)
for each construction, where the fourth class is the rack-
aware RS code shown in Example 1. As we mentioned before,
Steps 1 and 3 of repairing the resulting rack-oriented RS
code are generally identical. The distinction lies in the explicit
repair scheme adopted in Step 2, which is feasible to utilize
all the known ones for solving single-node failure under the
homogeneous storage model. For the explicit scheme used in
Step 2, refer to the references provided in the last column,
which varies on a case-by-case basis and is therefore omitted
here for simplicity.
9
TABLE II
NEW RACK-AWARE REED-SOLOMON CODES WITH SINGLE-RACK REPAIR PROPERTY
h(x)
Evaluation Points (for i-th rack)
Sub-Packetization
Rack Size
Bandwidth (per failure)
Conditions
References
xm
Ai = α¯ri−1E
t = ¯r¯n
u = m
< (¯n+1)t
¯n−k′
Fq(α) = Fqt, ∀δ|t, δ < t, ord(αm) ∤ (qδ − 1)
[14]
Ai = αiE
t = ( ¯d − k′ + 1)Π¯n
i=1pi
u = m
¯
dt
¯
d−k′+1
Fq(αi) = Fqpi , ord(αm
i ) ∤ (q − 1)
[7]
TrFqt /Fqt−a (x)
Ai = bi + U
t = ¯r¯n + a
u = qa
< (¯n+1)t
¯n−k′
h(bi) = α¯ri−1, Fq(α) = Fqt−a, (t − a)|t
[14]
Ai = bi + U
t = ( ¯d − k′ + 1)Π¯n
i=1pi + a
u = qa
¯
dt
¯
d−k′+1
h(ai) = αi, Fq(αi) = Fqpi , (t − a)|t
[7]
(TrFqt /Fqt−a (x))m
Ai = ∪m
j=1aiβj + V
t = ¯r¯n + a
u = mqa
< (¯n+1)t
¯n−k′
TrFqt /Fqt−a (ai) = α¯ri−1, Fq(α) = Fqt−a,
[14]
m|(qt−a − 1), ∀δ|t, δ < t, ord(αm) ∤ (qδ − 1), (t − a)|t, p|
t
t−a
Ai = ∪m
j=1aiβj + V
t = ( ¯d − k′ + 1)Π¯n
i=1pi + a
u = mqa
¯
dt
¯
d−k′+1
TrFqt /Fqt−a (ai) = αi, Fq(αi) = Fqpi ,
[7]
m|(qt−a − 1), ord(αm
i ) ∤ (q − 1), (t − a)|t, p|
t
t−a
βTrFqt /Fqt−a (x)
Ai = bi + U
t > 0
u = qa
t
t−a (¯n − 1)(t − a − l)
¯n = qt−a, ¯n − k′ ≥ ql, 1 ≤ l < t − a, (t − a)|t
[22]
LU (x)
Ai = bi + U
t > 0
u = qa
(¯n − 1)w
¯n = qt−a, ¯n − k′ = ql, 1 ≤ l < t − a, tl ≥ (t − a)(t − w)
[3], [18]
Denote by k′ = ⌈ k
u ⌉ and ¯r = ¯n − k′. Let p be the characteristic of Fq and p1, p2, · · · , p¯n be coprime satisfying pi ≡ 1 mod ( ¯d − k′ + 1), where i ∈ [¯n]. β is a nonzero element in Fqt.
V. CONCLUSION
In this paper, we employed good polynomials to arrange
nodes of RS codes on racks and propose a generic repair
framework for handling multiple failures within racks. Build-
ing upon this, the problem of multiple-node recovery in RS
codes under the rack-aware storage model is transformed
into the repair problem of RS codes under the homogeneous
storage model. This transformation leverages the beneﬁts of
both rack-aware and homogeneous RS codes, providing a
solution for multiple-node repair in the rack-aware storage
model.
In this way, we generalized the existing constructions to
support multiple-node failures and further designed several
new constructions using different good polynomials. These
proposed codes can be repaired by means of the known
schemes of single-node repair.
REFERENCES
[1] A. Chowdhury and A. Vardy, “Improved schemes for asymptotically
optimal repair of MDS codes,” IEEE Trans. Inf. Theory, vol. 67, no.
8, pp. 5051-5068, 2021.
[2] A.
Fikes,
Colossus,
“Successor
to
Google
File
Sys-
tem,”
2010.
Available:
http:
//static.googleusercontent.co-
m/media/research.google.com/en/us/university/
rela-
tions/facultysummit2010/storage
architecture
and
challenges.pdf,
2010.
[3] A. Berman, S. Buzaglo, A. Dor, Y. Shany, and I. Tamo, “Repairing
Reed-Solomon codes evaluated on subspaces,” IEEE Trans. Inf. Theory,
vol. 68, no. 10, pp. 6505-6515, 2022.
[4] A. G. Dimakis, P. B. Godfrey, Y. Wu, M. J. Wainwright, and K.
Ramchandran, “Network coding for distributed storage systems,” IEEE
Trans. Inf. Theory, vol. 56, no. 9, pp. 4539-4551, 2010.
[5] F. J. MacWilliams and N. J. A. Sloane, “The theory of error-correcting
codes,” North-Holland, 1977.
[6] I. Tamo and A. Barg, “A family of optimal locally recoverable codes,”
IEEE Trans. Inf. Theory, vol. 60, no. 8, pp. 4661-4676, 2014.
[7] I. Tamo, M. Ye, and A. Barg, “Optimal repair of Reed-Solomon codes:
Achieving the cut-set bound,” in Process IEEE 58th Annu. Symp.
Found. Comput. Sci. (FOCS), Oct. 2017, pp. 216-227.
[8] I. Tamo, M. Ye, and A. Barg, “The repair problem for Reed-Solomon
codes: Optimal repair of single and multiple erasures with almost
optimal node size,” IEEE Trans. Inf. Theory, vol. 65, no. 5, pp. 2673-
2695, 2019.
[9] J. Mardia, B. Bartan, and M. Wootters, “Repairing multiple failures
for scalar MDS codes,” IEEE Trans. Inf. Theory, vol. 65, no. 5, pp.
2661-2672, 2018.
[10] K. Shanmugam, D. S. Papailiopoulos, A. G. Dimakis, and G. Caire, “A
repair framework for scalar MDS codes,” IEEE J. Sel. Areas Commu.,
vol. 32, no. 5, pp. 998-1007, 2014.
[11] L. Jin, G. Luo, and C. Xing, “Optimal repairing schemes for Reed-
Solomon codes with alphabet sizes linear in lengths under the rack-
aware model,” Arxiv, 2019.
[12] L. Zhou and Z. Zhang, “Rack-aware regenerating codes with multiple
erasure tolerance,” IEEE Trans. Commun., vol. 70, no. 7, pp. 4316-
4326, 2022.
[13] M. Ye, “New constructions of cooperative MSR codes: Reducing node
size to exp(o(n)),” IEEE Trans. Inf. Theory, vol. 66, no. 12, pp. 7457-
7464, 2020.
[14] M. Ye and A. Barg, “Explicit constructions of MDS array codes and
RS codes with optimal repair bandwidth,” in Proc. IEEE Int. Symp.
Inf. Theory (ISIT), 2016. pp. 1202-1206.
[15] R.
Li,
Z.
Zhang,
K.
Zheng,
and
A.
Wang,
“Progress
report:
Bringing
erasure
coding
to
Apache
Hadoop,”
http://blog.cloudera.com/blog/2016/02/
progress-report-bringing-
erasure-coding-to-apache-hadoop/.
[16] S. Gupta, Bh. R. Devi, and V. Lalitha, “On rack-aware cooperative re-
generating codes and epsilon-MSCR codes,” IEEE Journal on Selected
Areas in Information Theory, 2022.
[17] S. M. et al., “f4: Facebook’s warm BLOB storage system,” in Proc.
11th ACM/USENIX Symp. Oper. Syst. Des. Implementation (OSDI),
2014, pp. 383-398.
[18] S. H. Dau and O. Milenkovic, ”Optimal repair schemes for some
families of full-length Reed-Solomon codes,” in Proc. IEEE Int. Symp.
Inf. Theory (ISIT), 2017, pp. 346-350.
[19] S. H. Dau, I. M. Duursma, H. M. Kiah, and O. Milenkovic, “Repairing
Reed-Solomon codes with multiple erasures,” IEEE Trans. Inf. Theory,
vol. 64, no. 10, pp. 6567-6582, 2018.
[20] S. H. Dau, T. X. Dinh, H. M. Kiah, T. T. Luong, and O. Milenkovic,
“Repairing Reed-Solomon codes via subspace polynomials,” IEEE
Trans. Inf. Theory, vol. 67, no. 10, pp. 6395-6407, Oct. 2021.
[21] V. Guruswami and M. Wootters, “Repairing Reed-Solomon codes,”
IEEE Trans. Inf. Theory, vol. 63, no. 9, pp. 5684-5698, 2017.
[22] W. Li, Z. Wang, and H. Jafarkhani, “On the sub-packetization size
and the repair bandwidth of Reed-Solomon codes,” IEEE Trans. Inf.
Theory, vol. 65, no. 9, pp. 5484-5502, 2019.
[23] Y. Hu, P. P. C. Lee, and X. Zhang, “Double regenerating codes for
hierarchical data centers,” in Proc. IEEE Int. Symp. Inf. Theory (ISIT),
2016, pp. 245-249.
[24] Y. Zhang and Z. Zhang, “An improved cooperative repair scheme for
Reed-Solomon codes,” 2019 19th Int. Symp. Commun. Inf. Technolo-
gies (ISCIT), pp. 525-530, 2019.
[25] Z. Chen and A. Barg, “Explicit constructions of MSR codes for
clustered distributed storage: The rack-aware storage model,” IEEE
Trans. Inf. Theory, vol. 66, no. 2, pp. 886-899, 2019.
[26] Z. Chen, M. Ye, and A. Barg, “Enabling optimal access and error
correction for the repair of Reed-Solomon codes,” IEEE Trans. Inf.
Theory, vol. 66, no. 12, pp. 7439-7456, 2020.
","To improve the efﬁciency of node repair in RS codes, Guruswami and Wootters [21] proposed a linear repair scheme for a single failure that signiﬁcantly reduces the repair bandwidth. Particularly, they introduced the trace function collection technique to repair RS codes, allowing a smaller subﬁeld symbol of each helper node to be sufﬁcient for recovering a single erased symbol. This seminal idea has sparked signiﬁcant interest in the repair problem of RS codes, leading to extensive subsequent research for single-node recovery [1], [3], [7], [14], [18], [22]. On one hand, considerable attention has been paid to designing non-trivial repair schemes for RS codes satisfying explicit parameter ranges with a reasonable ﬁeld size [3], [21], [18], [22]. On the other hand, other research has aimed at constructing RS codes that approximate the optimal repair bandwidth [1], [7], [14], [22], known as the cut-set bound [4], nevertheless which normally require huge ﬁeld size.nan"
"With the rapid expansion of online medical literature, automated question answering systems are becoming increasingly important for healthcare professionals and patients. In this work, we compare the performance of general and medical-specific distilled language models for medical question answering. We evaluate the effectiveness of fine-tuning domain-specific language models and compare the performance of different families of language models. This study aims to provide insights into the suitability of different models for specific applications in the medical domain.","There has been a massive increase in the amount of medical literature and research topics available online, which has made it incredibly challenging to keep up with the latest findings. As a result, there is a need for ubiquitous automated systems to aggregate and summarize essential information for healthcare professionals to more easily apply evidence-based knowledge to their decision-making. Similarly, patients need accurate, reliable medical information to make informed decisions about their health. Generative language models (LLMs) have shown promise in various natural language processing (NLP) tasks, and their potential in the healthcare domain, particularly closed-book generative question answering (QnA), is significant. However, the performance of these models in domain-specific tasks such as medical QnA remains largely unexplored.","To effectively assess various models, we conducted an evaluation based on datasets consisting of questions and answers used by the general public. Our selection process adhered to three specific criteria: dataset task, domain of the questions, and trustworthiness. Following a thorough assessment based on these criteria, we ultimately selected two datasets, namely MedQuAD and Icliniq, to proceed with our analysis. Our research methodology encompassed three key procedures to analyze and improve the performance of large language models (LLMs). The methods employed included testing base LLMs, fine-tuning distilled versions of LLMs, and employing in-context learning via prompting of base LLMs.","In accordance with the above observation, we investigate the impact of data augmentation. To explore this, we further trained the finetuned distilled models on the icliniq dataset and evaluated them on the medquad dataset. Excitingly, the Bloom model performs almost as well as GPT 3.5 did in the previous analysis. This finding suggests that incorporating more data and higher quality data improves the fine-tuning process. In a separate experiment, we exclusively fine-tuned the distilled models on the icliniq dataset and evaluated their performance. As expected, without the strong foundation provided by the medquad dataset, the icliniq performance is significantly poorer compared to GPT 3.5.",The proposed work lays an excellent benchmark for future work on Generative Closed-Book Question Answering. The work explores ten techniques across two datasets to determine what works best for the healthcare domain. The work also exposes the sensitivity of prompting for Medical QnA. The static prompts technique does not perform as well compared to the method where the Base LLM generated the answers without prompting. The work highlights more advanced techniques for Dynamic prompting and how searching for questions of similar topics and contexts from the train set for prompting can drastically improve the metric scores and the output quality.,MedLM: Exploring Language Models for Medical Question Answering Systems,"Niraj Yagnik, Jay Jhaveri, Vivek Sharma, Gabriel Pila, Asma Ben, Jingbo Shang","MedLM: Exploring Language Models for Medical Question Answering
Systems
Niraj Yagnik
UC San Diego
nyagnik@ucsd.edu
Jay Jhaveri
UC San Diego
jjhaveri@ucsd.edu
Vivek Sharma
UC San Diego
v7sharma@ucsd.edu
Gabriel Pila
UC San Diego
ypilahuancachoque
@ucsd.edu
Asma Ben
Microsoft Health AI
abenabacha@microsoft.com
Jingbo Shang
UC San Diego
jshang@ucsd.edu
Abstract
In the face of rapidly expanding online medi-
cal literature, automated systems for aggregat-
ing and summarizing information are becom-
ing increasingly crucial for healthcare profes-
sionals and patients. Large Language Models
(LLMs), with their advanced generative capa-
bilities, have shown promise in various NLP
tasks, and their potential in the healthcare do-
main, particularly for Closed-Book Generative
QnA, is significant. However, the performance
of these models in domain-specific tasks such
as medical Q&A remains largely unexplored.
This study aims to fill this gap by compar-
ing the performance of general and medical-
specific distilled LMs for medical Q&A. We
aim to evaluate the effectiveness of fine-tuning
domain-specific LMs and compare the perfor-
mance of different families of Language Mod-
els. The study will address critical questions
about these models’ reliability, comparative
performance, and effectiveness in the context
of medical Q&A. The findings will provide
valuable insights into the suitability of different
LMs for specific applications in the medical
domain.
1
Introduction
With the plethora of online information on medical
literature and research topics, it becomes incredi-
bly challenging to keep up with the latest findings
. The need for ubiquitous automated systems for
aggregating and summarizing essential information
would make it easier for healthcare professionals to
apply evidence-based knowledge to their decision-
making processes productively. Similarly, from the
patient’s perspective, there is an increasing demand
for easily-accessible accurate, and reliable medical
information for the general population. An auto-
mated system would help patients with the infor-
mation they can trust based on established journals
and research to equip them to make educated and
informed decisions for their health.
The latest advancements in generative models
have made Large Language Models ubiquitous on
the internet (Zhao et al., 2023). The models are be-
coming increasingly large and complex, allowing
them to learn intricate patterns available on the vast
text available online to improve performance on
a wide range of tasks. LLMs have achieved state-
of-the-art results on various NLP tasks, including
sentiment analysis, text summarization, and text
generation. The capabilities of LLMs have been
proven to extend beyond what was once considered
challenging. This includes the creation of unique
forms of text such as poems, code, scripts, musical
compositions, emails, and letters (Sallam, 2023).
The last year has also seen an increased availabil-
ity of these language models to the general public
allowing professionals from all backgrounds to ac-
cess them via their interface or API services for
their specific use-case.
A lot of prior work on automated medical ques-
tion answering is based on information retrieval
systems, which work great for the task at hand but
need to consider the specific context and nuances
of the patients. There is a fundamental need for
more personalization, which many modern auto-
mated medical systems need while trying to address
patients. Large Language Models have presented
themselves as reasons for considering the chain of
thoughts and understanding the user context before
answering the question. This makes it essential
to leverage the strengths of generative language
models and apply their power to the task of Closed-
Book Generative QnA for the healthcare domain.
The recent development in the NLP since the in-
ception of transformers and attention mechanisms
has seen the birth of multiple classes of language
models to address and specialize different down-
stream tasks. There has been massive progress in
the decoder-only family of models, with architec-
tures like GPT-3 and GPT-4 (Radford et al., 2019)
producing outstanding results on a plethora of gen-
arXiv:2401.11389v1  [cs.CL]  21 Jan 2024
erative tasks. Similar improvement has been ob-
served in the encoder-only class of families like the
T5 (Raffel et al., 2020) models have proved to be an
efficient model for a diverse set of problems. This
unprecedented makes it crucial to experiment with
variations of language models from each class to
deduce what works best for the task of generative
question-answering
The plethora of medical information has led to
the growing need for designing and developing
automated systems that can assist laypeople and
health workers find accurate answers to questions
related to diagnosis, medications, treatment, side
effects, etc. Medical Q&A (Question Answering)
systems have great potential to address this need
but are still a massive work in progress. The effi-
cacy of these systems largely relies on the scale and
quality of the underlying language models used to
generate answers. Similarly, large-scale pre-trained
language models like GPT-3 and T5 have achieved
outstanding performance in general language tasks;
their performance on the domain-specific task of
medical Q&A is mainly unexplored.
The proposed work aims to address the gap men-
tioned above in the literation by comparing the per-
formance of general and medical-specific distilled
LMs for the task of medical Q&A. The work aims
to evaluate whether fine-tuning domain-specific
LMs leads to improved performance on the medi-
cal task compared to general LMs. The work also
intends to provide a comparison across different
families of Language Models, i.e., a comparison of
the performance of the decoder-only transformers
model family(GPT-2, GPT-3) and the performance
of the encoder-decoder models (BERT family) thus
providing insights into the suitability of different
LMs for this specific application.
The proposed work attempts to answer the fol-
lowing questions:
1. Reliability Assessment of Generative Lan-
guage Models for Medical QnA: Establish-
ing Benchmark Scores for Closed Generative
Question Answering in the Medical Domain.
2. Comparative Performance Analysis of Dis-
tilled Fine-Tuned Language Models v/s Gen-
eral LLMs for Medical QnA.
3. Performance Evaluation of Decoder-only
Models versus Encoder-Decoder Models for
Medical QnA.
4. Effectiveness Evaluation of Prompt Engineer-
ing in Enhancing the Performance of LLMs
for Medical Question and Answering Tasks.
Making comparisons of the performance of gen-
eral and medical-specific LMs on medical Q&A
tasks is necessary to determine which type is best
suited for the application of medical domain. The
comparisons would provide a deep insight into
the strengths and limitations of different Language
Models and help us highlight areas that need im-
provement. The comparisons would thus assist
developers, researchers, and medical practitioners
decide which model is the best for their specific
task about Medical Q&A.
2
Literature Survey
The task of Automated Question-Answering can
be divided further into three variants based on the
inputs and outputs (hug):
• Extractive QA: The trained model extracts
the answer from the provided context. The
context can be text, table, image, or HTML.
BERT-like models are generally the go-to ap-
proach to solving this problem. These models
extract the span from the context which has
the correct answer.
• Open Generative QA: Given the context, the
model generates free text for the task. These
models take the context as input and develop
a coherent answer in natural language.
• Closed Generative QA: No context is pro-
vided for these models. It doesn’t rely on
external information and generates a response
based solely on pre-trained knowledge.
There has been some research on using Large
Language models for medical question-answering
tasks, detailed below:
BioBERT(Lee et al., 2020) is a pre-trained
biomedical language representation model for
biomedical text mining.
ClinicalBERT(Huang
et al., 2019)talks about modeling Clinical Notes
and Predicting Hospital Readmission"".
Kexin
Huang, Jaan Altosaar, Rajesh Ranganath. This
work develops and evaluates representations of clin-
ical notes using bidirectional transformers (Clini-
calBERT).
Med-BERT(Rasmy et al., 2021) is pretrained
contextualized embeddings on large-scale struc-
tured electronic health records for disease predic-
tion. This paper adapts the BERT framework orig-
inally developed for the text domain to the struc-
tured electronic health records EHR domain.
In ""Can Large Language Models reason about
medical questions?""(Liévin et al., 2022) by Lievin
et al. (2023), the paper evaluates the applicability
of GPT-3.5 on medical questions by using different
prompt strategies and later evaluated them with a
medical expert.
Med-PALM(Singhal et al., 2022): ""Large Lan-
guage Models Encode Clinical Knowledge"" by
Karan Singhal, Shekoofeh Azizi et al. (2022). This
paper proposes a framework to evaluate the large
language model answers along multiple axes in-
cluding factuality, precision, possible harm, and
bias.
A medical question answering system using
large language models and knowledge graphs(Guo
et al., 2022).This study focuses on building a
retrieval-based medical question answering system,
tackling the challenge with large language models
and knowledge extensions via graphs.
3
Datasets
To effectively assess various models, we conducted
an evaluation based on datasets consisting of ques-
tions and answers used by the general public. Our
selection process adhered to three specific criteria:
• Dataset task: Among the plethora of text-
based medical datasets available, we specifi-
cally focused on those centered around ques-
tion and answer interactions, with a focus on
being used for text generation. This ensured
that our evaluation concentrated on datasets
with relevant answers.
• Domain of the questions: While certain
datasets were limited to specific domains like
COVID-related queries, we aimed to identify
datasets that encompassed general questions
that any patient might ask. This allowed for
a comprehensive evaluation that addressed a
broad range of medical inquiries.
• Trustworthiness: We prioritized datasets that
provided the most reliable and credible in-
formation. Accordingly, we favored datasets
sourced from reputable institutions like the
National Institutes of Health (NIH), ensuring
the inclusion of trustworthy data in our evalu-
ation.
Following a thorough assessment based on
these criteria, we ultimately selected two datasets,
namely MedQuAD (Ben Abacha and Demner-
Fushman, 2019) and Icliniq (Regin, 2017), to pro-
ceed with our analysis.
Figure 1: Distribution of token length in the datasets
Note: We consider Icliniq dataset to the combination of
the 4 online sources in blue
3.1
MedQuAD
MedQuAD is a comprehensive dataset comprising
47,457 question-answer pairs sourced from various
NIH websites. The dataset stands out for its high-
quality content, as the answers are supported by the
authoritative backing of the NIH. MedQuAD en-
compasses an extensive range of medical inquiries,
spanning 37 distinct question types. These ques-
tions encompass diverse areas such as treatment,
diagnosis, and other pertinent aspects related to
diseases, drugs, and various medical entities. This
dataset’s answers had a length that varied from 50
words to more than 4,000 words, however, the most
relevant information usually appeared in the first
part of the answer, and the rest of the response was
complimentary.
3.2
Icliniq
The second dataset is a compilation of 29,752
question-answer pairs collected from prominent
websites such as eHealth Forum, iCliniq, Question
Doctors, and WebMD. These sources contribute
to the dataset’s diversity and coverage of medical
information. By including data from multiple rep-
utable platforms, the dataset offers a broader per-
spective on various medical topics and enhances
the overall comprehensiveness of the collection.
4
Methodology
Our research methodology encompassed three key
procedures to analyze and improve the performance
of large language models (LLMs). The methods
employed included testing base LLMs, finetun-
ing distilled versions of LLMs, and employing in-
context learning via prompting of base LLMs.
4.1
Finetuning Distilled Versions of LLMs
The first phase of our methodology involved fine-
tuning the distilled models. In Fine-tuning, pre-
trained LLMs are further trained on specific tasks,
here QnA, to allow the model to adapt its previ-
ously learned knowledge to the new task. This
procedure involved multiple steps shown in Fig. 2.
Figure 2: Finetuning Distil Models
- Concatenate questions and answers: Since
our objective was to develop a generative
model, we concatenated the question and
answer pairs into a single text sequence. This
allowed the model to learn the relationship
between questions and answers, enabling it to
generate appropriate responses when
presented with a question.
- Truncate model input: Due to the varying
lengths of the answers in the datasets, we
standardized the input length by truncating it
to a maximum of 300 tokens for the
MedQuAD dataset and 150 tokens for the
Icliniq dataset. Our decision to select the first
300 tokens was based on the observation that
the most relevant content of each answer was
typically found at the beginning. Considering
that the mean answer length was 197 words,
choosing 300 tokens allowed us to include
the necessary tokens for the question as well.
Additionally, by truncating the answer length,
we reduced the processing time required to
train the models, as longer inputs required
more time for model computations.
- Tokenize and fine-tune: Once the input was
truncated, we proceeded with the specific
tokenization steps required for each model
architecture. After tokenization, the models
were fine-tuned using the information from
the collected datasets. By following this
process, we aimed to optimize the generative
models’ performance by fine-tuning them on
the concatenated question-answer pairs,
standardizing input length, and utilizing
appropriate tokenization techniques for each
model architecture.
Following this, performance testing was con-
ducted on the test set. As part of our methodology,
we also employed a data augmentation technique
where models were trained on both the MedQuad
and iClinic datasets, thus broadening the diver-
sity of the training data and potentially improving
model robustness.
To accommodate the unique response types of
each dataset, we adjusted the training inputs ac-
cordingly. Specifically, inputs were cut off at 300
words for the MedQuad dataset and 150 words for
the iClinic dataset.
4.2
Testing Base LLMs
Following the fine-tuning method, we performed
a comprehensive testing of base LLMs, which in-
volved the pretrained models provided by OpenAI
from the decoder-only family, GPT2 GPT3.5 and
Bloom, and by Google from the Encoder-decoder
family, T5-base. These LLMs were tested on a
diverse range of question-answering inputs present
in the MedQuad and iClinic datasets. The models
were tested to cover various aspects of answer gen-
eration capabilities, ensuring a broad coverage of
topics and complexities.
4.3
Prompting Base LLMs (In-Context
Learning)
To further explore the capabilities of the base
LLMs, we implemented in-context learning, a tech-
nique that uses prompting to guide the models’
responses. The base models were provided with
prompts that included a set of examples indicative
of the desired behavior. These examples provided
the models with contextual information to guide
their generation. We explored a variety of prompt
designs, which included static prompting and dy-
namic prompting as explained below.
4.3.1
Static Prompting
In static prompting, a fixed set of two question-
answer pairs were selected randomly from the train-
ing set and used as prompts before querying the
model. This approach served to give the model a
basic context for its response generation.
As we will see in the result section, this prompt
worked great for some specific questions but per-
formed far worse in other questions. Hence, we
needed a process that will change the given prompt
questions dynamically depending on the input test
question.
4.3.2
Dynamic Prompting
Dynamic prompting, on the other hand, is a more
refined technique. Here, the question-answer pairs
used as prompts were selected based on their
relevance to the query.
Vanilla Dynamic Prompting
The first approach we explored is called Vanilla
Dynamic Prompting. Our plan was to embed all
the questions available in the training set of the
Medquad dataset and then calculate the cosine sim-
ilarity between the embedded vectors and the input
test question, ensuring that the selected prompts are
highly related to the query, potentially improving
the model’s ability to generate a suitable response.
This process 3 helped us identify the top k ques-
tions to be used as prompts.
Figure 3: Dynamic Prompting
To embed these questions, we utilized the In-
structOR Embedder (Su et al., 2022) model. Unlike
classic embedders, InstructOR Embedder accepts a
domain sentence as input, which makes the embed-
ded vectors more suitable for the specific domain
and task at hand. In our case, we used the domain
sentence: ""Represent the Medicine sentence for
retrieval: "".
During the inference phase, we employed the in-
put test question to identify the top k training ques-
tions. Through ablation studies, we determined
that setting k = 2 yielded the best results. We
then utilized these two questions, along with their
corresponding answers, as prompts for perform-
ing in-context learning with large language models
(LLMs). Finally, we incorporated the actual test
question as the third question in the prompt.
Although this approach showed some improve-
ment in scores, the gains were not significant. We
concluded that the vastness of available training
questions played a role in limiting its effectiveness.
Therefore, we needed a way to categorize or filter
the training questions.
Question-Type Specific Dynamic Prompting
In order to address the limitation of Vanilla Dy-
namic Prompting, we developed a more sophis-
ticated technique called Question-Type Specific
Dynamic Prompting. We leveraged the knowledge
that the Medquad training set included question
types associated with each question, such as ""Symp-
toms,"" ""Treatments,"" and ""Information,"" among
others. In total, there were 16 different question
types 4. Note: We did notice the imbalance in ques-
tion types, but we are going to leave handling that
for future work.
Figure 4: Various Question-Types
As a preprocessing step before inference, we
trained a BERT classifier on the training data,
where the input was the question, and the predic-
tion variable was the corresponding question type.
This classifier allowed us to determine the question
type for a given input question.
Similar to the previous approach, we employed
the InstructOR Embedder to create 16 separate em-
bedder blocks, each representing one of the ques-
tion types.
During the inference phase, we first used the pre-
trained BERT classifier to infer the question type of
the incoming question. Next, we accessed the spe-
cific embedding block associated with the inferred
question type and calculated the cosine similarity
to obtain the top k similar questions. Again, we
found that setting k = 2 produced the best results.
As we will discuss in the Results section, this
Question-Type Specific Dynamic Prompting ap-
proach yielded a significant performance boost
compared to the Vanilla Dynamic Prompting ap-
proach.
5
Results
The results of our study are presented in this sec-
tion, where we evaluate the performance of the
model both quantitatively and qualitatively. For
automated evaluation, we employed the BLEU (Pa-
pineni et al., 2002) and ROUGE (Lin, 2004) met-
rics. The BLEU score measures the structural ac-
curacy of generated sentences, while the ROUGE
score assesses the extent to which the generated
answers capture the overall meaning conveyed in
the reference text. However, we found that these
quantitative metrics alone were not entirely reli-
able, necessitating the need for human evaluation.
To this end, we conducted surveys with health pro-
fessionals and potential patients/users of MedLM,
as our research focuses on a Medical QnA dataset.
We begin by examining the worst-performing
questions. One such question is, ""What is fever?""
Our analysis reveals that each model possesses its
own characteristics. The Bloom model, although
factually correct, tends to repeat itself. GPT2 occa-
sionally exhibits hallucinatory behavior by gener-
ating seemingly plausible yet fictional paragraphs.
Moreover, the T5 model sometimes provides incor-
rect information. For instance, for this particular
question, T5 falsely claimed that fever is a rare
condition.
Next, when we explored the performance of
large base language models, we found that these
models exhibit sensitivity to prompts, particularly
concerning hallucinations and answer length.
5.1
Quantitative Results
Moving on to the results obtained from testing the
models on the Medquad dataset’s test set 1, we
assign the highest importance to the ROUGE-1
metric as it signifies the effectiveness of conveying
the expected information accurately. Our analysis
indicates that GPT 3.5 with static prompts yields
the best results. Notably, the fine-tuned models do
not surpass these scores but achieve comparable
performance.
In accordance with the above observation, we
investigate the impact of data augmentation. To ex-
plore this, we further trained the finetuned distilled
models on the icliniq dataset and evaluated them on
the medquad dataset. Excitingly, the Bloom model
performs almost as well as GPT 3.5 did in the pre-
vious analysis Table 2. This finding suggests that
incorporating more data and higher quality data
improves the fine-tuning process.
In a separate experiment, we exclusively fine-
tuned the distilled models on the icliniq dataset
and evaluated their performance Table 3. As ex-
pected, without the strong foundation provided by
the medquad dataset, the icliniq performance is
significantly poorer compared to GPT 3.5.
5.2
Qualitative Results
Shifting our focus to qualitative results, we con-
ducted a survey with potential patients/users to
gauge the comprehensibility of the generated an-
swers in natural language. The survey results, cal-
culated using the Likert scale (Likert, 1932), are
seen in Fig. 5. Surprisingly, some users preferred
the answers generated by GPT models over the
human-written ground truth answers. Additionally,
the Bloom model consistently performs at the level
of the ground truth, reaffirming our earlier observa-
tions.
Figure 5: Heat Map of User Responses.
Note: All survey scores calculated using Likert Scale
(Likert, 1932)
Moreover, we surveyed health professionals to
evaluate the factual accuracy of the generated an-
Model
Bleu1
Bleu4
Rouge-1
Rouge-L
T5
6.186
0.239
0.199
0.178
GPT-2
3.806
0.132
0.196
0.181
Bloom
1.577
0.043
0.193
0.18
GPT-3.5 w/Static Prompt
3.413
0.122
0.232
0.216
GPT-3.5 w/TypeWise Dynamic Prompt
7.132
0.344
0.222
0.211
ChatGPT-4 Dynamic Prompting
3.55
0.107
0.129
0.121
GPT-3.5 w/TypeWise Dynamic Prompt
7.132
0.344
0.222
0.211
ChatGPT-3 Dynamic Prompting
5.5287
0.290
0.2029
0.1897
Table 1: Model Evaluation On Medquad Testset
Model
Bleu1
Bleu4
Rouge-1
Rouge-L
T5
7.117
0.321
0.207
0.186
GPT-2
5.132
0.242
0.190
0.174
Bloom
1.871
0.046
0.226
0.212
Table 2: Model Evaluation after Data Augmentation
swers 6. The top graph 6a in our analysis illustrates
that, similar to the users’ perspective, doctors also
consider large GPT models to be consistently more
factually reliable than the human-generated ground
truth answers. The bottom graph 6b indicates the
propensity of models to hallucinate, with smaller
fine-tuned models exhibiting higher rates of hal-
lucination. Interestingly, doctors also believe that
human-generated answers contain a certain degree
of hallucination.
5.3
Observation
Throughout our research journey, we made several
noteworthy discoveries. This study establishes a
robust benchmark for future researchers working
on medical QnA and other domain-specific tasks.
Contrary to our initial expectations, the metrics
employed for evaluation proved unreliable, and
human evaluation yielded different conclusions.
Decoder-only models, such as GPT, demonstrate
superior performance. Additionally, our findings
indicate that while static prompts yield better re-
sults for certain questions, they can lead to worse
answers for others. Consequently, we introduced
dynamic prompting, which consistently improved
the results.
Overall, our research sheds light on the strengths
and weaknesses of various models in the context of
Medical QnA. The combination of quantitative and
qualitative evaluations provides a comprehensive
understanding of their performance and highlights
avenues for future improvements.
(a) Factual Accuracy
(b) Hallucination
Figure 6: Results of Doctor Survey using Liker Scale
6
Limitations
The work showcases that hallucination is prevalent
in the answers generated, especially in the case
of fine-tuned models of T5, BLOOM, and GPT-2.
The hallucination could be resolved with the proper
augmentation of data. Currently, the dataset has
limited samples to fine-tune and often has varying
answer lengths, with a few questions having incred-
ibly verbose responses and others presenting only
a few sentences.
Getting human evaluation for the responses gen-
erated by each model is challenging to scale. The
current human evaluation of doctors and users was
Model
Bleu1
Bleu4
Rouge-1
Rouge-L
GPT3.5 w/Static Prompt
2.434
0.097
0.221
0.199
T5
9.843
0.807
0.173
0.145
GPT-2
9.989
0.855
0.139
0.123
Table 3: Model Evaluation On icliniq Testset
performed to evaluate the accuracy and understand-
ability of 5 questions on the answers generated by
six models/techniques the work experiments with.
Getting human evaluations would be much more
challenging to scale for higher examples and more
models as a standard technique for evaluation. Par-
ticipants who took the survey for the assessment
expressed concerns about the length of the survey
form.
Resource and computational constraints pre-
vented us from further experiments on the fine-
tuned models, like training with more epochs and
larger batch sizes.
7
Future Work
The research paper proposes several future direc-
tions for further investigation and improvement:
Test DynamicPromting on GPT-4 API: For
this project’s scope, the work has only evaluated
the dynamic prompting technique on the GPT3.5
API. By experimenting with different prompts and
contextual cues on newer and better APIs, we aim
to improve the accuracy and reliability of the gen-
erated responses.
Fine-Tuning on GPT3, GPT-4, and other
larger models: Currently, the work fine-tunes the
distilled versions of GPT-2, BLOOM, and T5 Mod-
els. The lack of availability of the refined open-
source GPT-3 and GPT-4 models prevented the
work from further experimentations. We propose
exploring the impact of fine-tuning on these mod-
els to enhance their performance and reduce hal-
lucination. Fine-tuning newer models may offer
improved contextual understanding and generate
more accurate answers.
Coming up with better metrics for Generative
QnA tasks: As seen in the result section, models
which produced accurate and understandable re-
sults according to human evaluations performed
relatively poorly on the rogue and bleu metrics. Im-
proved evaluation metrics will enable a more com-
prehensive and precise assessment of the models’
performance and help identify and address halluci-
nations.
Enhance the dataset through processing, aug-
mentation, and summarization: As illustrated by
the work, data augmentation is an effective tech-
nique and direction on which future work could
focus. Another approach worth exploring is pro-
cessing the answers to summarize them to make
the dataset more uniform to have more consistent
and cohesive results.
8
Conclusion
The proposed work lays an excellent benchmark
for future work on Generative Closed-Book Ques-
tion Answering. The work explores ten techniques
across two datasets to determine what works best
for the healthcare domain. Future work can use
this as a stepping stone to explore more techniques
mentioned beyond this work.
The work also exposes the sensitivity of prompt-
ing for Medical QnA. The static prompts technique
does not perform as well compared to the method
where the Base LLM generated the answers with-
out prompting. The work highlights more advanced
techniques for Dynamic prompting and how search-
ing for questions of similar topics and contexts
from the train set for prompting can drastically
improve the metric scores and the output quality.
The work also highlights the efficacy of data
augmentation as a technique to compensate for the
relatively low volume of good-quality question-
answer pairs. Last, the work highlights the need for
new metrics beyond Rouge and Bleu to do justice
to the answers given by generative models.
Code: Github
The code for the proposed work can be found on
our Github Repo: MedLM
Ethics Statement
Our research adheres to ethical guidelines and con-
siderations. We have taken steps to ensure privacy
and data protection by obtaining necessary permis-
sions and informed consent. We have also made
efforts to mitigate biases and evaluate the potential
impact on different demographic groups. Trans-
parency and interpretability are prioritized, provid-
ing clear explanations of our approach and limita-
tions. We are committed to a safe and inclusive
environment and consider the broader societal im-
plications of our work. We welcome feedback and
strive for continuous improvement in our ethical
practices.
Acknowledgements
We want to express our sincere gratitude to Dr.
Jingbo Shang, Bill Hogan, and Dr. Asma Ben
Abacha for their invaluable guidance and mentor-
ship throughout this project. Their expertise and
support have been instrumental in our progress.
Without their contributions, our project wouldn’t
have reached its current stage. We are truly thank-
ful for their valuable insights and unwavering sup-
port, which have greatly influenced our journey and
outcomes.
References
Hugging face transformers - question answering.
https://huggingface.co/docs/transformers/
tasks/question_answering.
Accessed on June
11, 2023.
Asma Ben Abacha and Dina Demner-Fushman. 2019. A
question-entailment approach to question answering.
BMC Bioinform., 20(1):511:1–511:23.
Quan Guo, Shuai Cao, and Zhang Yi. 2022. A medi-
cal question answering system using large language
models and knowledge graphs. International Journal
of Intelligent Systems, 37(11):8548–8564.
Kexin Huang, Jaan Altosaar, and Rajesh Ranganath.
2019. Clinicalbert: Modeling clinical notes and pre-
dicting hospital readmission.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon
Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.
2020. Biobert: a pre-trained biomedical language
representation model for biomedical text mining.
Bioinformatics, 36(4):1234–1240.
Rensis Likert. 1932. A technique for the measurement
of attitudes. Archives of psychology.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries.
In Text summarization
branches out, pages 74–81.
Valentin Liévin, Christoffer Egeberg Hother, and Ole
Winther. 2022. Can large language models reason
about medical questions?
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics, pages 311–318.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research,
21(1):5485–5551.
Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and
Degui Zhi. 2021. Med-BERT: pretrained contextual-
ized embeddings on large-scale structured electronic
health records for disease prediction. npj Digital
Medicine, 4(1).
Lasse Regin. 2017.
Medical question answer
data.
https://github.com/LasseRegin/
medical-question-answer-data.
Accessed:
May 15, 2023.
Malik Sallam. 2023. The utility of chatgpt as an exam-
ple of large language models in healthcare education,
research and practice: Systematic review on the fu-
ture perspectives and potential limitations. medRxiv,
pages 2023–02.
Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mah-
davi, Jason Wei, Hyung Won Chung, Nathan Scales,
Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl,
Perry Payne, Martin Seneviratne, Paul Gamble, Chris
Kelly, Nathaneal Scharli, Aakanksha Chowdhery,
Philip Mansfield, Blaise Aguera y Arcas, Dale Web-
ster, Greg S. Corrado, Yossi Matias, Katherine Chou,
Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Ra-
jkomar, Joelle Barral, Christopher Semturs, Alan
Karthikesalingam, and Vivek Natarajan. 2022. Large
language models encode clinical knowledge.
Hongjin Su, Jungo Kasai, Yizhong Wang, Yushi Hu,
Mari Ostendorf, Wen-tau Yih, Noah A Smith, Luke
Zettlemoyer, Tao Yu, et al. 2022. One embedder, any
task: Instruction-finetuned text embeddings. arXiv
preprint arXiv:2212.09741.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
Zhang, Junjie Zhang, Zican Dong, et al. 2023. A
survey of large language models.
arXiv preprint
arXiv:2303.18223.
","nanA lot of the prior work on automated medical question answering is based on information retrieval systems, which are useful but need to consider the context and nuances of the patient. This work looks into the power of using generative language models for the task of Closed-Book Generative QnA for the healthcare domain. Recent advancements in NLP have brought forth multiple classes of language models to address different downstream tasks. LLMs have achieved state-of-the-art results on various NLP tasks, including sentiment analysis, text summarization, and text generation. The possibilities are continually being proven to extend beyond what was once considered challenging, which include the creation of unique forms of text such as poems, code, scripts, musical compositions, emails, and letters."
"In this endeavor, we analyze two distinct architectural choices for integrating speech data into decoder-only and encoder-decoder architectures and meticulously compare their efficacy on ASR and NER missions. We leverage the whisper encoder and ChatGLM3 model, fine-tuning only additional parameters pertinent to LoRA and connectors using the AISHELL dataset. We distill key insights by studying gate values and attention scores and discover that cross-attention architecture outmatches decoder-only architecture for short-context tasks, whereas the decoder-only approach excels in long-context ASR and NER tasks. As an extension, we execute chain-of-thought NER, initially generating long-form ASR transcriptions, which are subsequently employed in NER prediction. This strategy produced a benchmark F1 score of 0.805 on the AISHELL-NER test dataset.","Large language models (LLMs) have emerged as remarkable tools in a multitude of natural language processing endeavors. However, the tantalizing potential of LLMs has been largely untapped in multimodal scenarios. This sparked a renewed fervor in the realm of fusing speech modalities with LLMs, with automatic speech recognition (ASR) garnering the lion's share of attention. In this work, we embark on an exploration of integrating LLMs in Chinese ASR and named entity recognition (NER) tasks, building upon the ChatGLM3 model and Whisper encoder, and meticulously compare two distinct architectural paradigms: decoder-only and encoder-decoder. We seek to elucidate the interplay of architectural choices, task complexity, and historical context in these hybrid models.","Our study leverages the decoder-only and encoder-decoder architectures to seamlessly integrate the Whisper speech encoder with the ChatGLM3 model. To gain deeper insights into the nuances of each approach, we conduct extensive experiments and visualizations, meticulously analyzing gate values and attention scores along the way. Additionally, we introduce chain-of-thought NER, a novel method that employs long-form ASR transcriptions as inputs to NER predictions, yielding superior results on the AISHELL-NER benchmark.","Our results paint a clear picture of the capabilities of the two architectural approaches: the encoder-decoder architecture outperforms its decoder-only counterpart in the short-form ASR setting, while the latter excels in long-context ASR and NER tasks. These findings underscore the decoder-only architecture's prowess in exploiting all LLM layers, making it a prime candidate for longer-context tasks. Furthermore, we demonstrate that chain-of-thought NER significantly reduces the omission error rate and improves entity ASR accuracy compared to traditional methods. By delving into the taxonomy of ASR-NER results, we uncover a remarkable reduction in omission errors and an appreciable improvement in the detection of correct entities.","Through this comprehensive study, we have gained valuable insights into the complexities of integrating LLMs with speech modalities for ASR and NER tasks. The decoder-only and encoder-decoder architectures exhibit distinct strengths and limitations, and the optimal choice hinges on the specific task and context. Our findings illuminate the path towards future research directions, such as exploring the combination of approaches to maximize performance across a wide spectrum of tasks, including audio event detection.",Using Large Language Model for End-to-End Chinese ASR and NER,"Yuang Li, Jiawei Yu, Yanqing Zhao, Min Zhang, Mengxin Ren, Xiaofeng Zhao, Xiaosong Qiao, Chang Su, Miaomiao Ma, Hao Yang","Using Large Language Model for End-to-End Chinese ASR and NER
Yuang Li∗, Jiawei Yu∗, Yanqing Zhao, Min Zhang, Mengxin Ren, Xiaofeng Zhao, Xiaosong Qiao,
Chang Su, Miaomiao Ma, Hao Yang
Huawei Translation Services Center, China
{liyuang3, yanghao30}@huawei.com
Abstract
Mapping speech tokens to the same feature space as text
tokens has become the paradigm for the integration of speech
modality into decoder-only large language models (LLMs). An
alternative approach is to use an encoder-decoder architecture
that incorporates speech features through cross-attention. This
approach, however, has received less attention in the literature.
In this work, we connect the Whisper encoder with ChatGLM3
and provide in-depth comparisons of these two approaches us-
ing Chinese automatic speech recognition (ASR) and name en-
tity recognition (NER) tasks. We evaluate them not only by
conventional metrics like the F1 score but also by a novel fine-
grained taxonomy of ASR-NER errors. Our experiments reveal
that encoder-decoder architecture outperforms decoder-only ar-
chitecture with a short context, while decoder-only architecture
benefits from a long context as it fully exploits all layers of the
LLM. By using LLM, we significantly reduced the entity omis-
sion errors and improved the entity ASR accuracy compared to
the Conformer baseline. Additionally, we obtained a state-of-
the-art (SOTA) F1 score of 0.805 on the AISHELL-NER test set
by using chain-of-thought (CoT) NER which first infers long-
form ASR transcriptions and then predicts NER labels.
Index Terms: speech recognition, name entity recognition,
large language model
1. Introduction
Large language models (LLMs) have been shown to perform re-
markably on a wide range of natural language processing tasks
such as question answering, summarization, machine trans-
lation, etc [1].
To leverage the power of LLMs to multi-
modalities, various approaches have been proposed.
Early
works focus on visual understanding tasks. MiniGPT-4 [2] di-
rectly feeds visual features into the LLM through a projection
layer. LLaMA-Adapter [3] adopts fixed-length trainable vec-
tors as layer-wise prompts which can include visual informa-
tion. MiniGPT-4 and LLaMA-Adapter are decoder-only mod-
els, whereas Flamingo [4] utilizes an encoder-decoder frame-
work where visual representations are merged into the LLM
through cross-attention.
Recently, combining speech encoders with LLMs has at-
tracted increasing attention. Among various applications, the
ASR task has received the most attention [5, 6, 7, 8, 9, 10].
The majority of existing works concentrate on the Adapter
layer, which is responsible for reducing the dimensionality of
the speech features and mapping them to the text embedding
space. Different types of Adapter layers have been proposed,
such as the Attention layer [7], the adaptive CTC downam-
∗ denotes equal contribution to this paper.
pler [6], and the Convolutional layers [9]. Moreover, a range
of speech encoders (e.g., Whisper encoder [11], HuBERT [12],
etc.) and LLMs (e.g., LLaMA [13], Vicuna [14], etc.) have
been explored in this context. Beyond ASR, the potential of
LLMs was further unleashed by applying them to more chal-
lenging tasks such as speech translation [15, 16], ASR error
correction [17] or even multi-task speech and audio event un-
derstanding [18, 19, 20, 21]. However, these methods adopted
a decoder-only architecture that takes speech or audio features
as input to LLMs (similar to miniGPT4), which differs from
the standard encoder-decoder architecture of ASR [22]. The
only exception is [5], where the HuBERT speech encoder was
integrated with the LLaMA model via cross-attention for ASR
domain adaptation. Our study conducted a thorough compari-
son of the two types of architectures on Chinese ASR and NER
tasks, which have received less attention in prior research.
NER from speech is a fundamental task in Spoken Lan-
guage Understanding (SLU), which aims to identify and clas-
sify named entities into predefined categories, such as person
(PER), location (LOC), and organization (ORG). This task can
be performed by either a pipeline system [23] or an End-to-End
(E2E) system [24, 25, 26, 27, 28]. A pipeline system consists of
an ASR module and a text-based NER model, where the input
audio is first transcribed by the ASR module, and then the re-
sulting ASR output is processed by the NER model. In contrast,
an E2E system directly extracts entities from speech without de-
pending on intermediate ASR output, avoiding error propaga-
tion. We chose this task for our experiments because it requires
the LLM to not only learn the mapping from speech features to
text tokens but also to comprehend the semantic meaning of the
ASR transcription. To analyze the ASR-NER results in detail,
we applied the taxonomy of ASR-NER errors proposed in [29]
for the pipeline system to our LLM systems.
In this paper, we combined the frozen Whisper en-
coder with ChatGLM3-6B [30] and only fine-tuned the addi-
tional parameters of LoRA [31] and connectors on AISHELL
datasets [32, 33]. Through extensive experiments and visualiza-
tions of gate values and attention scores, we uncovered that the
encoder-decoder architecture with cross-attention leverages the
deeper layers of the LLM and achieves superior performance on
the short-form ASR task, while the decoder-only architecture
with self-attention exploits all the LLM layers and excels on the
long-context ASR and NER tasks. Additionally, we employed
CoT NER, which first generates long-form ASR transcriptions
and then predicts NER labels. CoT NER attained a SOTA F1
score of 0.805 on the AISHELL-NER test set [33]. According
to the taxonomy of ASR-NER results, CoT NER achieved an
absolute reduction in omission errors by 7% and an absolute
improvement of 9% in correct entities compared to the baseline
Conformer model.
arXiv:2401.11382v1  [cs.CL]  21 Jan 2024
Figure 1: The Speech modality is incorporated into the LLM
through (a) an adapter (decoder-only), and (b) cross-attention
layers (encoder-decoder).
2. Methodology
2.1. Decoder-only model
One simple way to incorporate speech modality into the LLM is
to use an Adapter layer that bridges the gap between the speech
features and the text embeddings. Figure 1 (a) illustrates how
the ChatGLM3 model takes the speech tokens from the Whisper
encoders, which are transformed by an Adapter, as input and
generates ASR transcriptions in an autoregressive manner. In
this setting, the speech tokens act as prompts. Since speech
features usually have much longer lengths than text features, we
downsample the speech features by stacking every five adjacent
frames. Afterwards, the speech features are fed into two linear
layers as shown in Equation 1.
S = Linear(ReLU(Linear(Hwhisper)))
(1)
where Hwhisper is the downsampled speech features and S is
the speech tokens after the Adapter layer.
2.2. Encoder-decoder model
Figure 1 (b) shows the integration of the Whisper encoder into
ChatGLM3, which follows the traditional Transformer [34] ar-
chitecture where the encoder and the decoder are connected
through cross-attention. After each self-attention layer of Chat-
GLM3, a cross-attention layer is added where the hidden states
of text tokens serve as queries and the speech features serve as
keys and values. Similar to the approach in [4, 5], we adopt
gated cross-attention with learnable gates to control the amount
of influence that the speech modality has on the final output.
The gate values are initialized to zeros to stabilize training.
Unlike previous works, we swap the order of the feedforward
and cross-attention layers and apply the gates only to the cross-
attention layers (Equation 2, 3 and 4). In our initial experiments,
this improves the training stability and the performance of our
model, as the speech features are processed by more layers be-
fore being fed into the LLM.
S = ReLU(Linear(Hwhisper))
(2)
S(i) = ReLU(Linear(i)(S))
(3)
H(i) = H(i)+Tanh(g(i)) ⊙ XATT(i)(H(i), S(i), S(i))
(4)
where the downsampled speech features Hwhisper are fed into
a single linear layer, resulting in new features S. Then at the
ith layer, S is processed by a Linear layer followed by a cross-
attention (XATT) layer scaled by a Tanh gate.
The main differences between the Adapter and the cross-
attention architectures can be summarized as follows:
• Principle: The Adapter layer enables the LLM to handle
speech tokens and text tokens uniformly. The cross-attention
layer considers the speech features as a distinct source se-
quence from the text tokens.
• Implementation: The Adapter architecture uses the speech
tokens as inputs whereas the cross-attention architecture in-
corporates the speech features after each layer. Consequently,
the Adapter architecture has the advantage of requiring less
modification to the source code of LLMs.
• Parameters and Computation:
Compared to the naive
Adapter method, the cross-attention approach introduces a
much larger number of parameters, since it involves cross-
attention at every layer.
Nevertheless, we found that the
cross-attention architecture achieves faster training and in-
ference speed because the cross-attention operates at a lower
dimension than the self-attention in the LLM.
2.3. Long-form ASR and CoT NER
We propose a three-phase training schedule to adapt our
Whisper-ChatGLM3 models to Chinese ASR and NER. The
training involves three tasks including short-form ASR, long-
form ASR, and CoT NER. Table 1 illustrates the input formats
of these tasks, and we provide the following details:
• Short-form ASR: We select a variable number of utterances
at random and concatenate their features and their corre-
sponding transcriptions. A special token, denoted by |asr|,
is used to indicate the ASR task. Random concatenation was
shown to be effective for the Whisper model [7], as it uses
30-second inputs that are longer than the typical segments in
the ASR corpus.
• Long-form ASR: The use of historical context from both
speech and text modalities has been shown to enhance ASR
performance [35, 36]. Motivated by this, we investigated the
effect of incorporating both historical speech and text infor-
mation on the recognition of the current utterance. We con-
structed an input sequence by concatenating historical speech
tokens Si−1 and current speech tokens Si, separated by a spe-
cial token γ that marks the beginning of the current utterance.
The model produces a long-form transcription with a special
token |sep| that indicates the start of the current transcription.
• CoT NER: We instruct the LLM to produce long-form tran-
scriptions first and then assign NER labels to the current ut-
terance. This approach can be regarded as a CoT process. It
can be also viewed as the combination of pipeline and E2E
NER systems as the model accesses both ASR transcriptions
and speech features.
Table 1: Different training tasks. S and T denote speech and
text tokens for an utterance respectively. | · | indicates special
text tokens. γ is a special audio token that separates the current
and historical speech tokens.
Short-from ASR
Si, Sj|asr|Ti, Tj
Long-from ASR
Si−1, γ, Si|asr|Ti−1|sep|Ti
CoT NER
Si−1, γ, Si|asr|Ti−1|sep|Ti|ner| ˆTi
In the first two phases of training, the models are optimized
on short-form and long-form ASR tasks respectively. In the
third phase, we perform multitask training of long-form ASR
and CoT NER jointly.
2.4. Categories of NER predictions
Referring to the method in [29], we conducted a fine-grained
analysis of the NER predictions. This allows us to gain a deeper
insight into the sources of errors, the benefits of LLMs, and
the impact of historical information on NER performance. The
categories of NER predictions include:
• Correct Span: Entities that match the gold entity tags, mean-
ing the location of the entity is accurately predicted in the
ASR transcription.
• Correct Entity: A subset of correct span. All the tokens
within the entity are predicted accurately. This is the only
category that is considered correct by the standard F1 score.
• Error Span: Entities that deviate from the gold entity tags.
• Replacement: A subset of error span. The entity type is
predicted incorrectly.
• Omission: A subset of error span. The entity in the gold
transcript is missing.
3. Experimental Setups
3.1. Dataset and metircs
Our experiments used the AISHELL-NER dataset [33], an an-
notated version of the AISHELL-1 [32] dataset, which con-
sists of 170 hours of Chinese speech data.
The training set
of AISHELL-NER contains about 120k sentences, and the test
set has 7,176 sentences. The dataset annotates three types of
named entities: person (PER), location (LOC), and organization
(ORG), using special symbols: “[·]” for PER, “(·)” for LOC,
and “< · >” for ORG. We formulated NER as a sequence gen-
eration task, where the model predicts the entity symbols along
with the ASR transcriptions. We measured the performance of
our model on the test set, which has 900 PER, 1,330 LOC, and
1,165 ORG entities, using character error rate (CER) for ASR
and F1 score for NER.
3.2. Model architecture
This paper employed the Conformer-based [33] E2E system as
a baseline model that did not incorporate the LLM. Our systems
utilized the encoder of Whisper-large-v2 [11] as speech encoder
and ChatGLM3-6B [30] as the base LLM. We froze the Whis-
per encoder and applied efficient finetuning of ChatGLM3 with
LoRA [31], enabling it to adapt to the domain of the AISHELL
dataset and comprehend ASR and NER tasks. The finetuning
of the LLM avoided relying on the parameter in the Adapter
to accomplish the task that the language model should handle.
For LoRA, we set the rank to 32 and applied it to both the at-
tention and feedforward layers, resulting in 15 million parame-
ters. For the decoder-only architecture with the Adapter layer,
the Adapter had 43 million parameters with two linear layers
that mapped the feature from the dimension of 6400 (1280 × 5)
to 4096. For the encoder-decoder architecture with the gated
cross-attention, the speech features were first projected to the
dimension of 4096 and then reduced to 1024 at each layer. The
cross-attention layer had eight heads and the multi-head dot-
product attention was computed at a low dimension of 1024 and
then projected back to 4096 after the attention layer. The linear
layers and the cross-attention layers together had 437 million
parameters.
3.3. Training schedule
The model underwent three phases of training.
In the first
phase, the model was trained on the short-form ASR task for
40 epochs, using a learning rate of 5e-5 and a batch size of 64.
In the second phase, the model was fine-tuned on the long-form
ASR task for 20 epochs, using a learning rate of 3e-5. The
number of historical utterances was randomly sampled for each
training example. The concatenated audio was either padded or
cropped to 30 seconds, depending on whether it was shorter or
longer than that duration. In the third phase, the model was fur-
ther trained for 20 epochs, using a learning rate of 3e-5. To acti-
vate the model’s NER capability without compromising its ASR
capability, we optimized the model for the long-form ASR task
with a probability of 30% and for the NER task with a probabil-
ity of 70%. All training phases were performed on four Nvidia
L40 GPUs.
4. Experimental Results
4.1. ASR results
As Table 2 demonstrates, our systems achieved a substantial
improvement over the Conformer baseline, with a relative CER
reduction of 19.7%. The encoder-decoder architecture outper-
formed the decoder-only architecture when no historical context
was available. Incorporating contextual information led to con-
sistent ASR accuracy enhancement for both architectures. Nev-
ertheless, the decoder-only architecture benefited more from the
history information than the encoder-decoder architecture, as
evidenced by the relative CER reduction of 14.4% versus 7.0%,
respectively. This suggests that the decoder-only architecture
can leverage the history information more effectively.
Table 2: The ASR performance of two systems with or without
history context measured by CER (%).
Short-form
Long-form
Conformer
4.83
/
Decoder-only
4.02
3.44
Encoder-decoder
3.88
3.61
4.2. NER results
As Table 3 demonstrates, the decoder-only and encoder-decoder
models achieved the highest F1 scores of 0.805 and 0.789
respectively when using historical context, which were sig-
nificantly higher than the Conformer model’s score of 0.743.
Table 3: The NER performance of two systems with different
historical context lengths (His.) measured by F1 score.
Method
His.
PER
LOC
ORG
TOTAL
Conformer
0
0.561
0.832
0.778
0.743
Decoder-
0
0.601
0.868
0.812
0.778
only
1
0.631
0.881
0.837
0.799
2
0.635
0.878
0.853
0.805
Encoder-
0
0.596
0.879
0.813
0.782
decoder
1
0.594
0.885
0.832
0.789
2
0.600
0.878
0.831
0.788
Regarding the types of named entities, all models performed
poorly on names. This is mainly because Chinese names have
many variations and homophones that can create confusion and
ambiguity for the ASR system. The trend of NER performance
when using historical context was similar to that of ASR, in that
the decoder-only model’s F1 score increased steadily, while the
encoder-decoder model’s F1 score reached its peak with only
one historical utterance.
4.3. NER taxonomy
Table 4 presents the taxonomy of NER predictions. It is ev-
ident that all models have a low rate of replacement errors,
whereas omission errors are more prevalent.
These errors
mainly stemmed from PER and ORG, which are often rare enti-
ties. One of the main advantages of using LLM is that it reduced
the omission error by almost 50% compared to the Conformer
baseline. With the LLM, more than 90% of entities can be la-
beled accurately, but among them, 10% have erroneous ASR
results which are mostly substitution errors for personal names.
These results prove that with the aid of the LLM, the model can
better identify rare name entities, but it remains difficult to pre-
cisely recognize the tokens within the Chinese names without
prior knowledge.
The impact of historical context was also investigated, and
it can be observed that for the decoder-only model, the addi-
tion of one historical context enhanced the percentage of the
correct span by 1.3% and the percentage of the correct entity by
2.0%. This indicates that a longer context not only improved the
model’s ability to locate the entity but also promoted ASR accu-
racy as it ensured that the predicted entities were more coherent
across utterances. For the encoder-decoder model, adding one
historical context only increased the percentage of the correct
span by 1.1% and the percentage of the correct entity by 0.8%.
Table 4: The classification of the entity predictions into differ-
ent categories: Correct Span (Cor. Span), Correct Entity (Cor.
Ent.), Error Span (Err. Span), Replacement (Rep.), and Omis-
sion (Omi.). The numbers are percentages (%).
Cor.
Cor.
Err.
Method
His.
Span
Ent.
Span
Rep.
Omi.
Conformer
0
82.3
71.0
17.7
1.5
11.5
Decoder-
0
88.7
77.1
11.3
1.3
5.6
only
1
90.0
79.1
10.0
1.1
5.2
2
90.7
79.9
9.3
1.0
4.5
Encoder-
0
88.4
77.4
11.6
1.3
5.8
decoder
1
89.5
78.2
10.5
1.2
5.2
2
88.9
78.0
11.1
1.4
5.5
4.4. Visualizations
Figure 2 provides visualizations to better understand the results
in the previous sections and the differences between the two
architectures. Figure 2 (a) illustrates the gate values across dif-
ferent layers for the cross-attention layer. The gate values re-
mained roughly unchanged across different training phases and
deeper layers were assigned significantly larger gate values than
shallow layers.
For the decoder-only architecture, the influence of speech
modality is controlled by self-attention. We calculated the aver-
age attention scores for the speech tokens on the AISHELL test
Figure 2: (a) Gate values of the cross-attention across differ-
ent layers for the encoder-decoder architecture during differ-
ent training phases. (b) The attention scores correspond to the
speech tokens across different layers for the decoder-only archi-
tecture with different historical (His.) context lengths and tasks
(i.e. ASR or NER).
set (Table 2 (b)). The attention score on the speech tokens de-
creased with longer context as more attention was given to the
text tokens. For the NER task, we first generated ASR transcrip-
tions that provided rich semantic information for NER, resulting
in the lowest attention scores for speech tokens. The attention
scores are consistently above 0.5 across different layers, indi-
cating that the speech features are important for ASR and NER.
Moreover, the attention scores for the deep and shallow layers
are similar, implying that all LLM layers were fully utilized.
Based on the previous observations, we can gain insights
into why the decoder-only model performed better with a longer
context. First, the decoder-only model better utilized the pre-
trained parameters inside the LLM by incorporating speech fea-
tures at shallow layers. Additionally, the decoder-only model
can dynamically adjust the importance of text tokens accord-
ing to the context length and the nature of the task, while the
encoder-decoder architecture treated the speech modality in a
static manner with similar weights under different scenarios.
5. Conclusion
In this paper, we explored combining a speech encoder with
an LLM for Chinese ASR and NER tasks using two differ-
ent architectures: decoder-only and encoder-decoder. Under
utterance-level evaluation, both architectures achieved signif-
icant improvements over the baseline Conformer model. We
further compared these two approaches in terms of their ca-
pability to utilize long-form historical information. The long-
form evaluations indicate that the decoder-only model bene-
fited more from incorporating history context than the encoder-
decoder model. To explain this phenomenon, we conducted a
comprehensive analysis of the gate values and attention scores,
which revealed the superior ability of the decoder-only model
to adjust the importance of speech and text modalities dynami-
cally. For future works, we intend to conduct larger-scale exper-
iments and evaluate our systems on more tasks. We hypothesize
that the encoder-decoder model may have an advantage over the
decoder-only approach in tasks where audio features are crucial,
such as audio event detection. Therefore, we plan to investigate
the potential of combining the two approaches to achieve su-
perior performance on a wide range of tasks that require both
high-level semantic and fine-grained acoustic information.
6. References
[1] OpenAI,
“GPT-4
technical
report,”
arXiv
preprint
arXiv:2303.08774, 2023.
[2] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-
4: Enhancing vision-language understanding with advanced large
language models,” arXiv preprint arXiv:2304.10592, 2023.
[3] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and
Y. Qiao, “LLaMA-adapter: Efficient fine-tuning of language mod-
els with zero-init attention,” arXiv preprint arXiv:2303.16199,
2023.
[4] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson,
K. Lenc, A. Mensch, K. Millican, M. Reynolds et al., “Flamingo:
a visual language model for few-shot learning,” Proc. NeurIPS,
vol. 35, pp. 23 716–23 736, 2022.
[5] Y. Li, Y. Wu, J. Li, and S. Liu, “Prompting large language models
for zero-shot domain adaptation in speech recognition,” in Proc.
ASRU, 2023.
[6] S. Ling, Y. Hu, S. Qian, G. Ye, Y. Qian, Y. Gong, E. Lin,
and M. Zeng, “Adapting large language model with speech for
fully formatted end-to-end speech recognition,” arXiv preprint
arXiv:2307.08234, 2023.
[7] W. Yu, C. Tang, G. Sun, X. Chen, T. Tan, W. Li, L. Lu, Z. Ma, and
C. Zhang, “Connecting speech encoder and large language model
for asr,” arXiv preprint arXiv:2309.13963, 2023.
[8] J. Wu, Y. Gaur, Z. Chen, L. Zhou, Y. Zhu, T. Wang, J. Li,
S. Liu, B. Ren, L. Liu, and Y. Wu, “On decoder-only architecture
for speech-to-text and large language model integration,” arXiv
preprint arXiv:2307.03917, 2023.
[9] Y. Fathullah, C. Wu, E. Lakomkin, J. Jia, Y. Shangguan, K. Li,
J. Guo, W. Xiong, J. Mahadeokar, O. Kalinli, C. Fuegen, and
M. Seltzer, “Prompting large language models with speech recog-
nition abilities,” arXiv preprint arXiv:2307.11795, 2023.
[10] Y. Hono, K. Mitsuda, T. Zhao, K. Mitsui, T. Wakatsuki, and
K. Sawada, “An integration of pre-trained speech and lan-
guage models for end-to-end speech recognition,” arXiv preprint
arXiv:2312.03668, 2023.
[11] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and
I. Sutskever, “Robust speech recognition via large-scale weak su-
pervision,” arXiv preprint arXiv:2212.04356, 2022.
[12] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhut-
dinov, and A. Mohamed, “HuBERT: Self-supervised speech
representation learning by masked prediction of hidden units,”
IEEE/ACM Transactions on Audio, Speech, and Language Pro-
cessing, vol. 29, pp. 3451–3460, 2021.
[13] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al.,
“LLaMA: Open and efficient foundation language models,” arXiv
preprint arXiv:2302.13971, 2023.
[14] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang,
L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and
E. P. Xing, “Vicuna: An open-source chatbot impressing GPT-4
with 90%* ChatGPT quality,” March 2023. [Online]. Available:
https://lmsys.org/blog/2023-03-30-vicuna/
[15] Z. Chen, H. Huang, A. Andrusenko, O. Hrinchuk, K. C. Puvvada,
J. Li, S. Ghosh, J. Balam, and B. Ginsburg, “Salm: Speech-
augmented language model with in-context learning for speech
recognition and translation,” arXiv preprint arXiv:2310.09424,
2023.
[16] Z. Huang, R. Ye, T. Ko, Q. Dong, S. Cheng, M. Wang, and H. Li,
“Speech translation with large language models: An industrial
practice,” arXiv preprint arXiv:2312.13585, 2023.
[17] S. Radhakrishnan, C.-H. Yang, S. Khan, R. Kumar, N. Kiani,
D. Gomez-Cabrero, and J. Tegn´er, “Whispering llama: A cross-
modal generative error correction framework for speech recogni-
tion,” in Proc. EMNLP, 2023.
[18] Y. Gong, A. H. Liu, H. Luo, L. Karlinsky, and J. Glass, “Joint
audio and speech understanding,” in Proc. ASRU, 2023.
[19] J. Liang, X. Liu, W. Wang, M. D. Plumbley, H. Phan, and E. Bene-
tos, “Acoustic prompt tuning: Empowering large language mod-
els with audition capabilities,” arXiv preprint arXiv:2312.00249,
2023.
[20] M. Wang, W. Han, I. Shafran, Z. Wu, C.-C. Chiu, Y. Cao,
Y. Wang, N. Chen, Y. Zhang, H. Soltau, P. Rubenstein, L. Zilka,
D. Yu, Z. Meng, G. Pundak, N. Siddhartha, J. Schalkwyk, and
Y. Wu, “Slm: Bridge the thin gap between speech and text foun-
dation models,” arXiv preprint arXiv:2310.00230, 2023.
[21] Y. Chu, J. Xu, X. Zhou, Q. Yang, S. Zhang, Z. Yan, C. Zhou, and
J. Zhou, “Qwen-audio: Advancing universal audio understand-
ing via unified large-scale audio-language models,” arXiv preprint
arXiv:2311.07919, 2023.
[22] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-
gio, “Attention-based models for speech recognition,” in Proc.
NeurIPS, Dec. 2015, pp. 577–585.
[23] M. A. B. Jannet, O. Galibert, M. Adda-Decker, and S. Rosset, “In-
vestigating the effect of ASR tuning on named entity recognition,”
in Proc. Interspeech, F. Lacerda, Ed., 2017.
[24] M. Gaido, S. Papi, M. Negri, and M. Turchi, “Joint speech trans-
lation and named entity recognition,” CoRR, 2022.
[25] D. Serdyuk, Y. Wang, C. Fuegen, A. Kumar, B. Liu, and Y. Ben-
gio, “Towards end-to-end spoken language understanding,” in
Proc. ICASSP, 2018.
[26] H. Yadav, S. Ghosh, Y. Yu, and R. R. Shah, “End-to-end named
entity recognition from english speech,” in Proc. Interspeech,
H. Meng, B. Xu, and T. F. Zheng, Eds., 2020.
[27] S. Mdhaffar, J. Duret, T. Parcollet, and Y. Est`eve, “End-to-end
model for named entity recognition from speech without paired
training data,” in Proc. Interspeech, H. Ko and J. H. L. Hansen,
Eds., 2022.
[28] S. Arora, S. Dalmia, B. Yan, F. Metze, A. W. Black, and S. Watan-
abe, “Token-level sequence labeling for spoken language un-
derstanding using compositional end-to-end models,” in Proc.
EMNLP Findings, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds.,
2022.
[29] P. Szymanski, L. Augustyniak, M. Morzy, A. Szymczak, K. Sur-
dyk, and P. Zelasko, “Why aren’t we NER yet?
artifacts of
ASR errors in named entity recognition in spontaneous speech
transcripts,” in Proc. ACL, A. Rogers, J. L. Boyd-Graber, and
N. Okazaki, Eds., 2023.
[30] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,
W. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen,
P. Zhang, Y. Dong, and J. Tang, “Glm-130b: An open bilingual
pre-trained model,” Proc. ICLR, 2023.
[31] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,
L. Wang, and W. Chen, “LoRA: Low-rank adaptation of large lan-
guage models,” in Proc. ICLR, 2022.
[32] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “Aishell-1: An open-
source mandarin speech corpus and a speech recognition base-
line,” in Proc. O-COCOSDA, 2017, pp. 1–5.
[33] B.
Chen,
G.
Xu,
X.
Wang,
P.
Xie,
M.
Zhang,
and
F. Huang, “AISHELL-NER: Named entity recognition from chi-
nese speech,” in Proc. ICASSP.
IEEE, 2022.
[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you
need,” in Proc. NeurIPS, I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds.,
vol. 30, 2017.
[35] A. Schwarz, I. Sklyar, and S. Wiesler, “Improving RNN-T ASR
accuracy using context audio,” in Proc. Interspeech, Brno, Czech
Republic, Sep. 2021.
[36] T. Hori, N. Moritz, C. Hori, and J. L. Roux, “Advanced long-
context end-to-end speech recognition using context-expanded
transformers,” in Proc. Interspeech, Brno, Czech Republic, Sep.
2021.
","nanEarly efforts in the marriage of vision and LLMs emerged with MiniGPT-4, which directly fed visual features into the LLM, while LLaMA-Adapter adopted trainable vectors as layer-wise prompts that could incorporate visual cues. On the other hand, Flamingo pioneered an encoder-decoder approach where visual representations were fused via cross-attention."
"Coordinating efforts across multiple Autonomous Underwater Vehicles (multi-AUVs) allows the effective completion of tasks beyond the capacity of individual AUVs. However, designing effective reward functions for multi-AUV control can be challenging, particularly in complex tasks involving high-dimensional spaces. Multi-agent Generative Adversarial Imitation Learning (MAGAIL) circumvents this challenge by learning from expert demonstrations. However, like other imitation learning methods, MAGAIL assumes the optimality of expert demonstrations, limiting its ability to surpass demonstrated performance. In contrast, Generative Adversarial Self-Imitation Learning (GASIL) does not rely on optimal demonstrations but suffers from violating the initial concept of the GAIL framework and requiring pre-defined reward functions. This paper proposes Multi-Agent Generative Adversarial Interactive Self-Imitation Learning (MAGAISIL), an improvement over MAGAIL. In MAGAISIL, sub-optimal expert demonstrations are gradually replaced with good trajectories generated by the agents under the evaluation of a human trainer, avoiding the use of pre-defined reward functions. Our experimental results, conducted in a multi-AUV formation control and obstacle avoidance task, demonstrate that MAGAISIL enables AUVs to not only match but also surpass the performance of MAGAIL trained with optimal demonstrations.","Autonomous Underwater Vehicles (AUVs) are invaluable tools for underwater tasks such as marine resource exploration and scientific research. These tasks are often complex and high-dimensional, demanding the cooperation of multiple AUVs to increase efficiency and reduce time and energy consumption. Multi-Agent Reinforcement Learning (MARL) has been introduced to improve multi-AUV control in uncertain marine environments. However, designing efficient reward functions for MARL, especially for complex and high-dimensional tasks, remains a significant challenge. Imitation learning, an alternative to reinforcement learning, enables robots to learn from expert demonstrations, eliminating the need for intricate reward functions. This research aims to enhance MAGAIL, a multi-agent imitation learning algorithm, by proposing MAGAISIL. Unlike MAGAIL, MAGAISIL does not assume the optimality of expert demonstrations and can surpass their performance through interactive self-imitation.","MAGAISIL improves upon MAGAIL by replacing the provided expert demonstrations with agent-generated good trajectories. A human trainer evaluates the agent-generated trajectories and selects those that are better than the expert demonstrations. These selected trajectories are then used to update the discriminator, which distinguishes between agent and expert trajectories, and the actor and critic networks of the agent. The process continues until the agent learns to generate trajectories that are consistently better than the expert demonstrations. MAGAISIL enables AUVs to learn from sub-optimal expert demonstrations and achieve performance comparable to or even surpassing that of AUVs trained with optimal demonstrations via MAGAIL.","Our experimental results in a multi-AUV formation control and obstacle avoidance task demonstrate the effectiveness of MAGAISIL. Both leader and follower AUVs trained with MAGAISIL surpass the performance of those trained with sub-optimal expert demonstrations via MAGAIL. Moreover, the leader and follower AUVs trained via MAGAISIL achieve performance comparable to or even better than those trained via MAGAIL with optimal demonstrations. These results suggest that MAGAISIL allows AUVs to learn from sub-optimal expert demonstrations, surpassing the provided demonstrations and reaching a level of performance close to or even exceeding that of MAGAIL with optimal demonstrations. Furthermore, our results indicate that the control policies of AUVs trained via MAGAISIL adapt well to complex and different tasks, even with added obstacles or changed angles of walls, demonstrating the adaptability of MAGAISIL to various scenarios.","This research introduces MAGAISIL, a multi-agent imitation learning method that improves upon MAGAIL by allowing AUVs to learn from sub-optimal expert demonstrations and surpassing their performance via interactive self-imitation. MAGAISIL does not require pre-defined reward functions and enables AUVs to learn from solely sub-optimal expert demonstrations. Our experimental results in a multi-AUV formation control and obstacle avoidance task demonstrate that AUVs trained with MAGAISIL can achieve performance close to or even better than those trained via MAGAIL with optimal demonstrations. Furthermore, the control policies of AUVs trained via MAGAISIL adapt well to complex and different tasks, indicating its adaptability to various scenarios. These findings highlight the potential of MAGAISIL for AUV control in uncertain and complex marine environments.",Multi-Agent Generative Adversarial Interactive Self-Imitation Learning for AUV Formation Control and Obstacle Avoidance,"Zheng Fang, Tianhao Chen, Dong Jiang, Zheng Zhang, Guangliang Li","Multi-Agent Generative Adversarial Interactive Self-Imitation Learning
for AUV Formation Control and Obstacle Avoidance
Zheng Fang†∗, Tianhao Chen†∗, Dong Jiang†, Zheng Zhang† and Guangliang Li†∗∗
Abstract— Multiple autonomous underwater vehicles (multi-
AUV) can cooperatively accomplish tasks that a single AUV
cannot complete. Recently, multi-agent reinforcement learning
has been introduced to control of multi-AUV. However, design-
ing efficient reward functions for various tasks of multi-AUV
control is difficult or even impractical. Multi-agent generative
adversarial imitation learning (MAGAIL) allows multi-AUV to
learn from expert demonstration instead of pre-defined reward
functions, but suffers from the deficiency of requiring optimal
demonstrations and not surpassing provided expert demon-
strations. This paper builds upon the MAGAIL algorithm by
proposing multi-agent generative adversarial interactive self-
imitation learning (MAGAISIL), which can facilitate AUVs to
learn policies by gradually replacing the provided sub-optimal
demonstrations with self-generated good trajectories selected
by a human trainer. Our experimental results in a multi-AUV
formation control and obstacle avoidance task on the Gazebo
platform with AUV simulator of our lab show that AUVs
trained via MAGAISIL can surpass the provided sub-optimal
expert demonstrations and reach a performance close to or even
better than MAGAIL with optimal demonstrations. Further
results indicate that AUVs’ policies trained via MAGAISIL
can adapt to complex and different tasks as well as MAGAIL
learning from optimal demonstrations.
I. INTRODUCTION
Autonomous underwater vehicle (AUV) plays an impor-
tant role in underwater tasks of exploring marine resources
and scientific research due to its flexibility [1], [2]. It can
replace humans to perform dangerous underwater tasks such
as survey of ocean topography and landforms, inspection,
maintenance and repair of submarine oil pipelines. Rein-
forcement learning (RL) was introduced and applied to
improve the autonomy and intelligence of AUV control [3],
[4], [5], [6], [7], [8], [9], [10], [11]. Through interactions
with underwater environment, AUV with RL can learn a
control policy adapting to the changes and uncertainty [7],
[12]. However, as the limited detection range and energy
storage of a single AUV, it is necessary to use multi-AUV
to cooperatively complete underwater tasks that single AUV
cannot perform with the increasing complexity of underwater
missions. Multiple AUVs can accomplish underwater detec-
tion, target search, object recognition etc., in a collaborative
way, which can improve the efficiency of task execution and
reduce the time and energy cost. Multi-agent reinforcement
learning (MARL) has been introduced to improve multi-
AUV control in uncertain marine environments [13], [14],
†Zheng Fang, Tianhao Chen, Dong Jiang, Zheng Zhang and Guangliang
Li are with the School of Information Science and Engineering, Ocean Uni-
versity of China, Qingdao, China. {guangliangli}@ouc.edu.cn
∗ Contributing equally
∗∗ Corresponding author
[15]. However, it is difficult to design efficient reward
functions for various tasks, especially those complex and
high-dimensional ones where most robots like AUVs will
be operated in. Moreover, the difficulty of designing reward
functions for MARL increases with the number of agents
and complexity of their relationships [16], [17].
Imitation learning was proposed and successfully applied
to robot control [18], [19] since it is much easier to provide
demonstrations on performing a task than to design a reward
function. There are mainly two kinds of imitation learning:
one is behavior cloning (BC) and the other is inverse
reinforcement learning (inverse RL). BC learns a mapping
from an agent’s states to optimal actions via supervised
learning [20], but requires a large amount of data and cannot
generalize to unseen situations and adapt to different tasks ef-
fectively. Inverse reinforcement learning agents learn control
policies with extracted cost functions from expert demon-
strations via reinforcement learning [21] and can effectively
generalize to unseen states [22]. However, many inverse RL
algorithms need a model to solve a sequence of planning
or reinforcement learning problem in an inner loop and the
performance might decrease if the planning or RL problem
is not optimally solved [23], which prevents applying inverse
RL for robot control to large and complex tasks. Ho et
al. solved this problem by proposing a general model-free
imitation learning method — generative adversarial imitation
learning (GAIL) [23], which allows robots to directly learn
policies from expert demonstrations in large and complex
environments. Higaki et al. applied GAIL to realize ship’s
automatic collision avoidance by mimicking human expert
performance [24]. Jiang et al. [25] implemented GAIL in
AUV path following tasks and further proposed a generative
adversarial interactive imitation learning (GA2IL) method
combining GAIL with interactive RL [26], [27] to improve
AUV’s performance and stability in path following.
GAIL was extended to a multi-agent setting by proposing
multi-agent generative adversarial imitation learning (MAG-
AIL) [28]. Fang et al. [13] successfully applied MAGAIL
to a multi-AUV formation control task with a decentral-
ized training and execution framework. However, MAG-
AIL shares the limitation with GAIL and other imitation
learning methods that they assume the optimality of expert
demonstrations and can seldom surpass the performance
of demonstrations if the provided demonstrations are not
optimal. On the other hand, Guo et al. assumed that op-
timal demonstrations are not available and agents should
imitate “relatively better trajectories” generated by the agent.
They proposed generative adversarial self-imitation learning
arXiv:2401.11378v1  [cs.RO]  21 Jan 2024
(GASIL) [29] by imitating agent’s past good trajectories
measured via pre-defined reward functions, which violates
the initial idea of the GAIL framework learning from solely
demonstrations and avoiding pre-defined reward functions.
In this paper, we proposed multi-agent generative ad-
versarial interactive self-imitation learning (MAGAISIL) by
improving MAGAIL via replacing the provided expert sub-
optimal demonstrations with agent generated good trajecto-
ries. However, different from GASIL, MAGAISIL allows a
human trainer to evaluate whether the agent generated tra-
jectories are better than the provided expert demonstrations
instead of using pre-defined reward functions. We imple-
mented and tested our MAGAISIL method in a multi-AUV
formation control and obstacle avoidance task on the Gazebo
platform with AUV simulator of our lab, and compared to
the MAGAIL method. Our results show that, even provided
with sub-optimal expert demonstrations, AUVs trained with
our MAGAISIL method can learn to reach a performance
close to or even better than those trained via MAGAIL
with optimal demonstrations via gradually replacing the sub-
optimal demonstrations with self-generated good trajectories
selected by a human trainer. In addition, further results indi-
cate that the control policies of AUVs trained via MAGAISIL
can adapt to complex and different tasks even with added
obstacles or changed angles of wall as well as MAGAIL
learning from optimal demonstrations.
II. BACKGROUND
A. Multi-Agent Reinforcement Learning
In single-agent reinforcement learning [30], [31], [3],
the objective of an agent is to learn a policy maximizing
cumulative returns via interacting with the environment. For
example, at time step t, an agent selects an action at with its
policy based on the detected current state st. Then the agent
will transition to a next state and receive a reward rt from
the environment. The goal of the agent is to learn a policy
π which maximizes the discounted accumulated return as
follow:
Vπ(s) = E
 X
h≥0
γhrt
at ∼ π(a|st)

,
(1)
where γ is the discount factor determining the value of future
rewards over immediate ones, Vπ(s) is the value of state s
following policy π thereafter.
In multi-agent reinforcement learning (MARL), there are
multiple agents interacting with the environment [32], [33].
Each agent i has its own policy πi that can be used to select
an action ai,t based on its observed state st at current time
step t. Then the agent transitions to a next state and will
receive a reward ri,t. Similar to single-agent reinforcement
learning, the goal for each agent is to learn a policy maxi-
mizing its discounted accumulated return. However, different
from single-agent reinforcement learning, in MARL, the
policy πi of agent i is affected by other agents’ policies.
The most common concept to solve this problem is Nash
Equilibrium (NE). In NE, agent i will not try to change
its policy πi if other agents do not change their policies,
because its discounted accumulated return cannot continue
to increase. That is to say, if all agents reach the equilibrium
state, each learns a steady optimal policy.
In MARL, the relationship between agents can be divided
into three settings based on the relationship between reward
functions of agents: cooperative, competitive and a mixed
setting [34]. In a fully cooperative setting, all agents perform
the same task and share a same reward function. The
relationship between reward functions of agents is zero-sum
in a competitive setting. In other words, agents maximize
their cumulative rewards by preventing each other from
completing its task. In a mixed setting, each agent has its
own task and reward function, which can be cooperative
or competitive to other agents. In our experiments, the
relationship between leader and follower AUV is in a mixed
setting since they perform different tasks.
B. Generative Adversarial Imitation Learning
Generative adversarial imitation learning (GAIL) [23] al-
lows an agent to learn directly from expert demonstrations
consisting of state-action pairs, avoiding to pre-define re-
ward functions for various tasks. A GAIL agent trained a
discriminator D : S × A → (0, 1) to distinguish expert
state-action pairs (s, a) ∼ τE from agent state-action pairs
(s, a) ∼ τagent, and a generator (i.e., policy π) to “fool” the
discriminator by generating state-action pairs (s, a) ∼ τagent
as close as possible to expert state-action pairs (s, a) ∼ τE
by maximizing Eπ[log(D(s, a))]. The agent generates its
trajectory τagent by interacting with the environment with its
current policy π. That is to say, a GAIL agent learns a policy
directly by generating a distribution of the agent’s state-
action pairs as close as possible to the distribution of state-
action pairs from the expert demonstrations. In summary, the
GAIL algorithm can be summarized as finding a saddle point
(π, D):
−λH(π) + Eπ[log(D(s, a))] + EπE[log(1 − D(s, a))], (2)
where H(π) ≜ Eπ[−logπ(a | s)], is the γ-discounted
causal entropy [35] of the policy π, λ is the weight of
entropy H(π). Song et al. extended GAIL to a multi-
agent setting by proposing multi-agent generative adversarial
imitation learning (MAGAIL) [28]. Guo et al. assumed that
optimal demonstrations are not always available to an agent
in RL, and proposed generative adversarial self-imitation
learning (GASIL) to imitate agent’s past good trajectories
by measuring them via pre-defined reward functions [29].
III. METHODOLOGY
Previous work extending GAIL to multi-agent learning by
proposing MAGAIL [28], which allows multiple agents to
learn from provided expert demonstrations. However, MA-
GAIL shares the limitation with GAIL and other imitation
learning methods that they can seldom surpass the perfor-
mance of demonstrations. On the other hand, generative
adversarial self-imitation learning (GASIL) [29] aims to
imitate agent’s past good trajectories by measuring them via
pre-defined reward functions, but violates the initial idea of
Fig. 1.
Illustration of the mechanism for our multi-agent generative
adversarial interactive self-imitation learning (MAGAISIL) method.
the GAIL framework to allow learning from demonstrations
and avoid pre-defining reward functions. In this paper, we
proposed multi-agent generative adversarial interactive self-
imitation learning (MAGAISIL) by improving MAGAIL via
replacing the provided expert sub-optimal demonstrations
with agent generated good trajectories. However, different
from GASIL, MAGAISIL allows a human trainer to evaluate
whether the agent generated trajectories are better than the
provided expert demonstrations instead of using pre-defined
reward functions. Therefore, we expect and hypothesize that
our MAGAISIL method allow agents to learn from solely
sub-optimal expert demonstrations without pre-defining re-
ward functions and can obtain a much better performance
than the provided expert demonstrations, resolving the limita-
tion of MAGAIL that it can seldom surpass the performance
of demonstrations. Fig. 1 illustrates the mechanism of our
proposed MAGAISIL method.
As shown in Fig. 1, MAGAISIL will take provided sub-
optimal expert demonstrations as input. Then, each agent
will learn an Actor (i.e. control policy) and a Critic (i.e.
value function) via independent proximal policy optimization
(IPPO) [36]. In addition, a discriminator will be trained to
distinguish the state-action pairs of agent’s generated tra-
jectories from provided expert demonstrations. Specifically,
during training, at time step t, Agent i will obtain local
observation oi,t, and select an action ai,t with its current
policy πθi:
ai,t ∼ πθi (ai | oi,t). Then, it will transition
to a new state upon performing the selected action. The
Agent i will repeat the cycle of selecting action and obtain-
ing observation until the end of an episode. The received
state-action pairs during one episode by interacting with
the environment compose the trajectory τi of Agent i. N
state-action pairs (oi, ai) from the agent’s trajectory τi will
be selected and N state-action pairs (oiE, aiE) from the
provided expert trajectory τiE will be selected and used to
train the discriminator Dωi via ADAM [37] with the loss
function as:
Eτi [log (Dωi(o, a))] + EτiE [log (1 − Dωi(o, a))] .
(3)
The updated discriminator Dωi will be used to provide
Fig. 2.
Screenshot of the Gazebo simulation platform with a leader AUV
and a follower AUV in the simulated underwater environment.
rewards ri,D for updating the Actor and Critic of Agent i:
ri,D = − log (1 − Dωi (o, a)) .
(4)
In addition, at the end of an episode, the trajectory τi of
Agent i will be visualized and shown to a human trainer,
who can compare with the provided expert demonstrations
according to her knowledge and experience. If the human
trainer thinks the agent’s generated trajectory is better than
the expert demonstrations, the trajectory τi of Agent i will
be stored in the temporary trajectory pool τiT . Otherwise, the
trajectory τi of Agent i will be disregarded. We set a limit
to the number of state-action pairs in the pool τiT and when
it is full, all stored trajectories in τiT are used to replace the
current expert demonstrations τiE. At the same time, τiT will
be cleared to store new trajectories in the following training
process.
IV. SIMULATION SETUP
We evaluated our method by conducting experiments in
three formation control and obstacle avoidance of multi-
AUV tasks on the Gazebo simulation platform. The simulator
is extended from Unmanned Underwater Vehicle Simulator
[38] with the model of Sailfish 210 developed in our lab.
Fig. 2 shows the screenshot of Gazebo simulation platform
with a leader AUV and a follower AUV in the simulated
underwater environment.
A. Simulation Tasks
We set up three formation control and obstacle avoidance
tasks in our experiments as shown in Fig. 3. Fig. 4 shows the
observed state information of a leader AUV and a follower
AUV in the tasks. In Task I, the objective of the task is to
allow the leader AUV to go through a square pipe and the
follower AUV to follow the leader AUV at a distance of 18
meters, while keeping a safe distance from the wall of both
sides in the pipe. The distance between walls of two sides in
the pipe is 30 meters. The leader AUV and follower AUV
will start from the position (18, 0) and (1, 0), respectively,
and the ending position is at (240, 0). The task is terminated
and a new episode will start in the following situations:
1) Leader AUV or follower AUV is too close to obstacles,
i.e.,|dL| ≤ 2 or |dF | ≤ 2
(a) Task I
(b) Task II
(c) Task III
Fig. 3.
Illustration of the configuration of Task I, II and III in the underwater environment for formation control and obstacle avoidance.
Fig. 4.
The state representation of the leader and follower AUV in the
tasks.
2) The distance between two AUVs is too close or far,
i.e.,|gF | < 3 or |gF | > 33
3) The heading deviation of follower AUV is too large,
i.e.,|aF | ≥ π
3 .
To test the adaptability of our method in complex tasks,
we added rectangular obstacles with a length of 20 meters
and width of 5 meters along the walls on both sides of the
pipe in Task II, and changed the angles of walls and extended
the pipe to be 300 meters long in Task III.
B. State Representation and Action Space
In the tasks, the leader-follower method was adopted and
a leader AUV and a follower AUV were considered for
simplification, which can be easily extended to complex
tasks with more AUVs, as shown in Fig. 2. A decentralized
training and execution framework was used as in [13]. Fig.
4 shows the state representation of AUVs in the task. As
shown in Fig. 4, the black squares represent the walls of pipe
or obstacles, do denotes the distance between walls of the
pipe, which is set to be 30 meters, (xL, yL) is the coordinate
of the leader AUV’s current position, and (xF , yF ) is the
coordinate of the follower AUV’s current position. Both
AUVs need to detect and avoid collision with the wall and/or
obstacles with sonar sensor and go through the pipe as soon
as possilbe. The detection angle range of the sonar sensor
is set to be

− π
3 , π
3

which will be divided into 6 sectors,
as shown by the shaded blue and green area in Fig. 4. The
sonar sensor has 600 beams which are equally distributed
in the 6 sectors. The detection distance is 33 meters at
most. AUV will take the shortest distance detected by all
the sectors as the distance to the obstacle. The detected
shortest distances by the leader AUV and follower AUV
are denoted as dL and dF , respectively. The leader AUV’s
state is represented as oL = {dL1, dL2, dL3, dL4, dL5, dL6}
and the follower AUV’s state is represented as oF
=
{gF , aF , dF 1, dF 2, dF 3, dF 4, dF 5, dF 6}. Here, dLi and dF i
are the detected distance by each sector of the sonar sen-
sor on leader AUV and follower AUV, respectively, i =
1, 2, ..., 6. gF is the directrix distance between two AUVs,
which is computed as gF =
q
(yF − yL)2 + (xF − xL)2,
and aF is the heading deviation of follower AUV and
computed as aF = aF H − atan 2

yF −yL
xF −xL

, where aF H
denotes the current heading of the follower AUV.
TABLE I
ACTION SETUP FOR AUVS TO PERFORM BY SETTING THE ANGLES OF
FOUR RUDDERS (UPPER, RIGHT, LOWER AND LEFT)
Action
Upper
Right
Lower
Left
turn left 1
−14◦
0
14◦
0
turn left 2
−20◦
0
20◦
0
go straight
0
0
0
0
turn right 1
14◦
0
−14◦
0
turn right 2
20◦
0
−20◦
0
We set five discrete actions for both AUVs, which can
be performed by setting the thruster speed and angles of
four rudders, including two actions for turning left, two for
turning right and one for going straight, as shown in Table I.
The upper and lower rudders are set to control the horizontal
direction of AUV. The left and right rudders are used to
control AUV to float and dive in the underwater environment,
which are set to 0 as the tasks are in a 2D space. The thruster
speed of AUV is set to 300 r/s.
C. Evaluation Metrics
Due to the subjectivity of rewards from the learned dis-
criminator with expert demonstrations, we defined reward
functions for both leader and follower AUVs to evaluate
our proposed method. The defined reward functions for
both leader AUV and follower AUV are never used for
learning, but only used for testing the learned policies from
demonstrated trajectories with MAGAIL and our method
MGAISIL. The reward function for the leader AUV is
(a) Learning curve for the leader AUV
(b) Learning curve for the follower AUV
Fig. 5.
Cumulative rewards received by the leader AUV and follower AUV trained via MAGAISIL with suboptimal demonstrations (MAGAILSIL
Optimal), MAGAIL with sub-optimal (MAGAIL Suboptimal) and optimal (MAGAIL Optimal) demonstrations in Task I. The shaded area is the 0.95
confidence interval and the bold line is the mean performance over three experimental trials. Two red lines show the performance of expert optimal and
sub-optimal demonstrations.
(a) Task I
(b) Task II
(c) Task III
Fig. 6.
Tested trajectories of the leader AUV and follower AUV in Task I, II and III using final control policies trained in Task I via MAGAISIL with
sub-optimal expert demonstrations and MAGAIL with optimal expert demonstrations.
defined as:
rL = 1 − |dL − 17.3|
8.65
,
(5)
where 17.3 meters is a safe distance for AUV derived based
on the detection angle range of the sonar sensor and the
distance between walls of the pipe, which can keep AUV in
the middle of the pipe. The reward function for the follower
AUV is defined as:
rF = 0.5 ∗ rc
F + 0.5 ∗ ra
F ,
(6)
where rc
F represents the rewards for tracking the leader AUV,
and is defined as rc
F = − |aF |∗3
π
+
1 − |gF −18|
15
 based on the
distance gL between two AUVs and the heading deviation
aL. Similar to the leader AUV, ra
F = 1− |dF −17.3|
8.65
is defined
based on the distance from the wall of pipe dF to keep the
follower AUV in the middle of the pipe.
In our experiments, we trained both leader and follower
AUV with our MAGAISIL method with sub-optimal expert
demonstrations. In addition, MAGAIL trained with both
optimal and sub-optimal expert demonstrations were also
used as comparisons. We set N = 256, i.e., each time
256 state-action pairs are randomly selected from trajectory
generated by each AUV and provided expert demonstrations
respectively to update the discriminator. During one episode,
the discriminator is updated 3 times and the generator is
updated 9 times for each AUV. The discount factor is set to
be γ = 0.99, λ = 1.0 and the clipping factor ϵ = 0.09 in the
IPPO algorithm. The maximum number of state-action pairs
in the temporary trajectory pool is set to be 2000.
V. RESULTS AND DISCUSSION
This section presents and analyzes our experimental results
by comparing the policy performance trained with our MA-
GAISIL learning from sub-optimal expert demonstrations
to MAGAIL learning from both optimal and sub-optimal
expert demonstrations in Task I as described in Section IV-A.
In addition, to evaluate the adaptability of our MAGAISIL
method, we tested the trained policies of two AUVs in Task
II and III.
A. Learning Curves
Fig. 5 shows the cumulative rewards received by the leader
AUV and follower AUV during training via MAGAISIL with
sub-optimal demonstrations, MAGAIL with sub-optimal and
optimal demonstrations in Task I, measured by the predefined
reward functions for leader and follower AUV in Section
IV-C, respectively. From Fig. 5 we can see that, for both
leader and follower AUVs, at the beginning process, the
speed of our MAGAISIL agent learning from sub-optimal
demonstrations is similar to the MAGAIL agent learning
from sub-optimal demonstrations, which is slower than the
MAGAIL agent learning from optimal demonstrations. This
might be because the good agent-generated trajectories se-
lected by the human trainer in our MAGAISIL method did
not fully replace the sub-optimal expert demonstrations yet.
However, after about 400 episodes’ training, our MAGAISIL
agent learning from sub-optimal demonstrations reached a
performance close to optimal expert demonstrations together
with the MAGAIL agent learning from optimal demonstra-
tions and statblized afterwards. In contrary, the performance
(a) Task I
(b) Task II
(c) Task III
Fig. 7.
The distance to the wall of pipe or obstacles from the leader AUV in Task I, II and III tested using final control policies trained in Task I via
MAGAISIL with sub-optimal expert demonstrations and MAGAIL with optimal expert demonstrations. The red line in (a) shows the distance to the wall
of pipe from the leader AUV during expert sub-optimal demonstration.
(a) Task I
(b) Task II
(c) Task III
Fig. 8.
The distance between the leader AUV and follower AUV in Task I, II and III tested using final control policies trained in Task I via MAGAISIL
with sub-optimal expert demonstrations and MAGAIL with optimal expert demonstrations. The red line in (a) shows distance between the leader AUV
and follower AUV during expert sub-optimal demonstration.
(a) Task I
(b) Task II
(c) Task III
Fig. 9.
The heading deviation of the follower AUV in Task I, II and III tested using final control policies trained in Task I via MAGAISIL with sub-optimal
expert demonstrations and MAGAIL with optimal expert demonstrations. The red line in (a) shows heading deviation of the follower AUV during expert
sub-optimal demonstration.
of the MAGAIL agent learning from sub-optimal demonstra-
tions still fluctuated around the sub-optimal demonstrations
throughout the training process.
In summary, while the performance of MAGAIL agent
learning from sub-optimal demonstrations is limited by sub-
optimal expert demonstrations, our MAGAISIL agent can
learn to reach a performance close to optimal demonstrations
via gradually replacing the sub-optimal demonstrations with
self-generated good trajectories selected by a human trainer.
B. Performance
We also tested and compared the final control policies
of both leader and follower AUVs trained in Task I via
our MAGAISIL with sub-optimal expert demonstrations
and MAGAIL with optimal expert demonstrations from the
perspectives of trajectory, distance to the walls of pipe,
distance between leader and follower AUV, heading deviation
of follower AUV, as shown in Fig. 6(a), 7(a), 8(a), 9(a),
10(a). From Fig. 6(a) we can see that, both leader and
follower AUV trained via our MAGAISIL with sub-optimal
demonstrations and MAGAIL with optimal demonstrations
can successfully complete Task I by generally following the
middle of the pipe. The leader and follower AUV trained via
our MAGAISIL method even performed a bit better than the
one via MAGAIL at the turnings in the pipe. While further
(a) Task I
(b) Task II
(c) Task III
Fig. 10.
The distance to the wall of pipe or obstacles from the follower AUV in Task I, II and III tested using final control policies trained in Task I via
MAGAISIL with sub-optimal expert demonstrations and MAGAIL with optimal expert demonstrations. The red line in (a) shows the distance to the wall
of pipe from the follower AUV during expert sub-optimal demonstration.
examining the observations of both leader and follower AUV
in the testing process, we found that although the leader AUV
in the provided sub-optimal expert demonstrations fluctuated
dramatically around the middle of the pipe after turning,
the leader AUV trained with our MAGAISIL method can
get a performance close to MAGAIL learning from optimal
demonstrations, which can immediately get back to the safe
distance around 17.3 meters (derived based on the detection
angle range of the sonar sensor and the distance between
walls of the pipe, refer to Section IV-C), and keep itself in
the middle of the pipe (Fig. 7(a)).
For the follower AUV in the provided sub-optimal expert
demonstrations, its distance to the leader AUV fluctuated
dramatically between 12 and 25 meters and heading devia-
tion also fluctuated dramatically (Fig. 8(a) and 9(a)). After
training with our MAGAISIL method, the follower AUV can
obtain a performance close to MAGAIL learning from opti-
mal demonstrations, and keep a distance to the leader AUV
at around 18 meters with a heading deviation around 0 (Fig.
8(a) and 9(a)). Moreover, the follower AUV trained with our
MAGAISIL method can keep a safe distance to the walls
of pipe even though it fluctuated largely during and after
turnings in the provided sub-optimal expert demonstrations,
and even performed better than the MAGAIL agent learning
from optimal demonstrations (Fig. 10(a)).
In summary, our results show that both leader and follower
AUV trained with our MAGAISIL method can surpass
the provided sub-optimal expert demonstrations and get a
performance close to or even better than those trained via
MAGAIL with optimal demonstrations.
C. Adaptability to Complex and Different Tasks
We also tested and compared the adaptability of our MA-
GAISIL method to complex and different tasks by running
the saved final control policies of leader and follower AUV
trained in Task I via MAGAISIL with sub-optimal demon-
strations and via MAGAIL with optimal demonstrations in
Task II with extra obstacles and in Task III with changed
angles of the walls and extended length of the pipe. Other
settings are the same as Task I.
Fig. 6(b)(c), 7(b)(c), 8(b)(c), 9(b)(c), 10(b)(c) show the
performance of the leader and follower AUV trained via
MAGAISIL and MAGAIL in terms of trajectory, distance
to the walls of pipe, distance between leader and follower
AUV, heading deviation of follower AUV, respectively. From
these results in Task II and III we can see that, the control
policies of leader and follower AUV trained via MAGAISIL
and MAGAIL can adapt well to complex and different
tasks even with added obstacles or changed angles of wall.
Moreover, the both AUVs trained via MAGAISIL with sub-
optimal demonstrations can obtain a similar performance
to those trained via MAGAIL with optimal demonstrations.
The leader and follower AUV trained via MAGAISIL even
performed a bit better than those via MAGAIL at the turnings
in the pipe (Fig. 6(b)(c)). However, the distance between
follower AUV and leader AUV trained via MAGAISIL
decreased largely after 100 steps compared to those via
MAGAIL, but gradually increased after that and is similar
to those trained via MAGAIL after about 300 steps (Fig.
8(b)). This might be because of the effect of added obstacles,
which were first met after the first turning at about 100
steps. This is consistent with the a bit larger fluctuation
of the heading deviation of the follower AUV trained via
MAGAISIL compared to MAGAIL, which also starts from
about 100 steps (Fig. 9(b)).
VI. CONCLUSION
In this paper, we builds upon the MAGAIL algorithm by
proposing multi-agent generative adversarial interactive self-
imitation learning (MAGAISIL), which can facilitate AUVs
to learn policies by gradually replacing the provided sub-
optimal demonstrations with self-generated good trajectories
selected by a human trainer. We tested and implemented
MAGAISIL in a multi-AUV formation control and obstacle
avoidance task on the Gazebo platform with AUV simulator
of our lab and compared with MAGAIL. Results show that,
AUVs trained with our MAGAISIL method can surpass
the provided sub-optimal expert demonstrations and learn
to reach a performance close to or even better than those
trained via MAGAIL with optimal demonstration. Further
results indicate that the control policies of AUVs trained via
MAGAISIL can adapt to complex and different tasks as well
as MAGAIL learning from optimal demonstrations.
ACKNOWLEDGMENT
This work was supported by Natural Science Foundation
of China (under grant No. 51809246) and Qingdao Municipal
Natural Science Foundation (under grant No. 23-2-1-153-
zyyd-jch).
REFERENCES
[1] L. Paull, S. Saeedi, M. Seto, and H. Li, “Auv navigation and localiza-
tion: A review,” IEEE Journal of Oceanic Engineering, vol. 39, no. 1,
pp. 131–149, 2013.
[2] C. Cheng, Q. Sha, B. He, and G. Li, “Path planning and obstacle
avoidance for auv: A review,” Ocean Engineering, vol. 235, p. 109355,
2021.
[3] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in
robotics: A survey,” The International Journal of Robotics Research,
vol. 32, no. 11, pp. 1238–1274, 2013.
[4] M. Carreras, J. Yuh, J. Batlle, and P. Ridao, “A behavior-based scheme
using reinforcement learning for autonomous underwater vehicles,”
IEEE Journal of Oceanic Engineering, vol. 30, no. 2, pp. 416–427,
2005.
[5] A. El-Fakdi and M. Carreras, “Policy gradient based reinforcement
learning for real autonomous underwater cable tracking,” in 2008
IEEE/RSJ International Conference on Intelligent Robots and Systems.
IEEE, 2008, pp. 3635–3640.
[6] ——, “Two-step gradient-based reinforcement learning for underwa-
ter robotics behavior learning,” Robotics and Autonomous Systems,
vol. 61, no. 3, pp. 271–282, 2013.
[7] R. Yu, Z. Shi, C. Huang, T. Li, and Q. Ma, “Deep reinforcement
learning based optimal trajectory tracking control of autonomous
underwater vehicle,” in Proceedings of 2017 36th Chinese Control
Conference (CCC).
IEEE, 2017, pp. 4958–4965.
[8] Q. Zhang, J. Lin, Q. Sha, B. He, and G. Li, “Deep interactive
reinforcement learning for path following of autonomous underwater
vehicle,” IEEE Access, vol. 8, pp. 24 258–24 268, 2020.
[9] R. B. Grando, J. C. de Jesus, V. A. Kich, A. H. Kolling, N. P.
Bortoluzzi, P. M. Pinheiro, A. A. Neto, and P. L. Drews, “Deep
reinforcement learning for mapless navigation of a hybrid aerial un-
derwater vehicle with medium transition,” in 2021 IEEE International
Conference on Robotics and Automation (ICRA).
IEEE, 2021, pp.
1088–1094.
[10] B. Hadi, A. Khosravi, and P. Sarhadi, “Deep reinforcement learning
for adaptive path planning and control of an autonomous underwater
vehicle,” Applied Ocean Research, vol. 129, p. 103326, 2022.
[11] C. Zhang, P. Cheng, B. Du, B. Dong, and W. Zhang, “Auv path
tracking with real-time obstacle avoidance via reinforcement learning
under adaptive constraints,” Ocean Engineering, vol. 256, p. 111453,
2022.
[12] I. Carlucho, M. De Paula, S. Wang, Y. Petillot, and G. G. Acosta,
“Adaptive low-level control of autonomous underwater vehicles using
deep reinforcement learning,” Robotics and Autonomous Systems, vol.
107, pp. 71–86, 2018.
[13] Z. Fang, D. Jiang, J. Huang, C. Cheng, Q. Sha, B. He, and G. Li,
“Autonomous underwater vehicle formation control and obstacle
avoidance using multi-agent generative adversarial imitation learning,”
Ocean Engineering, vol. 262, p. 112182, 2022.
[14] G. Wang, F. Wei, Y. Jiang, M. Zhao, K. Wang, and H. Qi, “A multi-
auv maritime target search method for moving and invisible objects
based on multi-agent deep reinforcement learning,” Sensors, vol. 22,
no. 21, p. 8562, 2022.
[15] C. Lin, G. Han, T. Zhang, S. B. H. Shah, and Y. Peng, “Smart
underwater pollution detection based on graph-based multi-agent rein-
forcement learning towards auv-based network its,” IEEE Transactions
on Intelligent Transportation Systems, pp. 7494–7505, 2022.
[16] D. Gu and E. Yang, “Multiagent reinforcement learning for multi-robot
systems: A survey,” Technical Report of the Department of Computer
Science, 2004.
[17] L. Bus¸oniu, R. Babuˇska, and B. De Schutter, “Multi-agent reinforce-
ment learning: An overview,” Innovations in Multi-agent Systems and
Applications-1, pp. 183–221, 2010.
[18] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey
of robot learning from demonstration,” Robotics and Autonomous
Systems, vol. 57, no. 5, pp. 469–483, 2009.
[19] H. Ravichandar, A. S. Polydoros, S. Chernova, and A. Billard, “Recent
advances in robot learning from demonstration,” Annual Review of
Control, Robotics, and Autonomous Systems, vol. 3, pp. 297–330,
2020.
[20] S. Ross and D. Bagnell, “Efficient reductions for imitation learning,”
in Proceedings of the Thirteenth International Conference on Artifi-
cial Intelligence and Statistics.
JMLR Workshop and Conference
Proceedings, 2010, pp. 661–668.
[21] A. Y. Ng, S. J. Russell et al., “Algorithms for inverse reinforcement
learning.” in Proceedings of International Conference on Machine
Learning (ICML), vol. 1, 2000, p. 2.
[22] J. Ho, J. Gupta, and S. Ermon, “Model-free imitation learning with
policy optimization,” in Proceedings of International Conference on
Machine Learning (ICML).
PMLR, 2016, pp. 2760–2769.
[23] J. Ho and S. Ermon, “Generative adversarial imitation learning,”
Advances in Neural Information Processing Systems, vol. 29, pp.
4565–4573, 2016.
[24] T. Higaki and H. Hashimoto, “Human-like route planning for au-
tomatic collision avoidance using generative adversarial imitation
learning,” Applied Ocean Research, vol. 138, p. 103620, 2023.
[25] D. Jiang, J. Huang, Z. Fang, C. Cheng, Q. Sha, B. He, and G. Li,
“Generative adversarial interactive imitation learning for path follow-
ing of autonomous underwater vehicle,” Ocean Engineering, vol. 260,
p. 111971, 2022.
[26] G. Li, R. Gomez, K. Nakamura, and B. He, “Human-centered rein-
forcement learning: A survey,” IEEE Transactions on Human-Machine
Systems, vol. 49, no. 4, pp. 337–349, 2019.
[27] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and
D. Amodei, “Deep reinforcement learning from human preferences,”
Advances in Neural Information Processing Systems, vol. 30, pp.
4300–4308, 2017.
[28] J. Song, H. Ren, D. Sadigh, and S. Ermon, “Multi-agent generative
adversarial imitation learning,” Advances in Neural Information Pro-
cessing Systems, vol. 31, 2018.
[29] Y. Guo, J. Oh, S. Singh, and H. Lee, “Generative adversarial self-
imitation learning,” arXiv preprint arXiv:1812.00950, 2018.
[30] R. Sutton and A. Barto, Reinforcement learning: an introduction. MIT
Press, 2018.
[31] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath,
“Deep reinforcement learning: A brief survey,” IEEE Signal Process-
ing Magazine, vol. 34, no. 6, pp. 26–38, 2017.
[32] L. Busoniu, R. Babuska, and B. De Schutter, “A comprehensive survey
of multiagent reinforcement learning,” IEEE Transactions on Systems,
Man, and Cybernetics, Part C (Applications and Reviews), vol. 38,
no. 2, pp. 156–172, 2008.
[33] H. Qie, D. Shi, T. Shen, X. Xu, Y. Li, and L. Wang, “Joint optimization
of multi-uav target assignment and path planning based on multi-agent
reinforcement learning,” IEEE Access, vol. 7, pp. 146 264–146 272,
2019.
[34] K. Zhang, Z. Yang, and T. Bas¸ar, “Multi-agent reinforcement learn-
ing: A selective overview of theories and algorithms,” Handbook of
Reinforcement Learning and Control, pp. 321–384, 2021.
[35] M. Bloem and N. Bambos, “Infinite time horizon maximum causal
entropy inverse reinforcement learning,” in Proceedings of the 53rd
IEEE Conference on Decision and Control.
IEEE, 2014, pp. 4911–
4916.
[36] C. Schroeder de Witt, T. Gupta, D. Makoviichuk, V. Makoviychuk,
P. H. Torr, M. Sun, and S. Whiteson, “Is independent learning all
you need in the starcraft multi-agent challenge?” arXiv e-prints, pp.
arXiv–2011, 2020.
[37] Z. Zhang, “Improved adam optimizer for deep neural networks,” in
2018 IEEE/ACM 26th International Symposium on Quality of Service
(IWQoS).
Ieee, 2018, pp. 1–2.
[38] M. M. M. Manh˜aes, S. A. Scherer, M. Voss, L. R. Douat, and
T. Rauschenbach, “Uuv simulator: A gazebo-based package for un-
derwater intervention and multi-robot simulation,” in OCEANS 2016
MTS/IEEE Monterey.
IEEE, 2016, pp. 1–8.
","nanPrior research has explored various approaches to address the challenges of multi-AUV control. MARL has been implemented to enhance multi-AUV control in uncertain marine environments. However, designing reward functions for MARL can be demanding, particularly for complex tasks. Imitation learning emerged as an alternative, allowing robots to learn from expert demonstrations without the need for reward functions. However, existing imitation learning methods, including GAIL and MAGAIL, assume the optimality of expert demonstrations and struggle to outperform them. GASIL, an imitation learning method, does not rely on optimal demonstrations but requires pre-defined reward functions, contradicting the fundamental concept of GAIL. This research aims to address these limitations by introducing MAGAISIL, a novel imitation learning algorithm for multi-agent systems."
"This paper presents Back-stepping Experience Replay (BER), a technique compatible with arbitrary off-policy reinforcement learning (RL) algorithms. BER enhances learning efficiency in systems with approximate reversibility and reduces the need for complex reward shaping. The method constructs reversed trajectories using back-stepping transitions to reach random or fixed targets. BER is an interpretable bi-directional approach that addresses inaccuracies in back-stepping transitions through a distillation of the replay experience during learning. Given the intricate nature of soft robots and their complex interactions with environments, the paper presents an application of BER in a model-free RL approach for the locomotion and navigation of a soft snake robot, which is capable of serpentine motion enabled by anisotropic friction between the body and ground. A dynamic simulator is also developed to assess the effectiveness and efficiency of the BER algorithm, where the robot demonstrates successful learning (reaching a 100% success rate) and adeptly reaches random targets, achieving an average speed 48% faster than that of the best baseline approach.","Reinforcement learning (RL) has attracted increasing attention for its ability to solve complex control problems and achieve generalization in both virtual and physical tasks. This paper focuses on the application of RL to soft robots, which are characterized by their inherent infinite degrees of freedom and complicated interactions with environments. To address the challenge of learning efficiency in such systems, particularly in tasks involving complicated behaviors, the authors propose Back-stepping Experience Replay (BER), an algorithm compatible with arbitrary off-policy RL algorithms. BER is applicable for systems with approximate reversibility and with fixed or random goal setups.","The Back-stepping Experience Replay (BER) algorithm is proposed to enhance the learning efficiency of off-policy RL algorithms in systems with approximate reversibility. BER constructs reversed trajectories using back-stepping transitions to reach random or fixed targets. The method addresses inaccuracies in back-stepping transitions through a distillation of the replay experience during learning. The key steps of the BER algorithm are as follows:

1. Background: 
   a) Reinforcement Learning: A standard RL formalism is adopted, where an agent interacts with an environment and learns a policy based on perceptions and rewards.
   b) Deep Q-Networks (DQN) and Deep Deterministic Policy Gradient (DDPG): DQN and DDPG are introduced as model-free, off-policy RL approaches suitable for agents operating in discrete and continuous action spaces, respectively.
2. Algorithm for BER: 
   a) The main idea of BER is to navigate backward from the goals to the initial states in the tasks, in addition to relying solely on forward explorations. 
   b) The standard and the back-stepping transitions are collected and stored in separate replay buffers for training. 
   c) A strategy is used to sample the transitions from the standard replay and the back-stepping replay with certain probabilities. 
   d) For systems with imperfect reversibility, the probability of sampling transitions from the back-stepping replay gradually drops to zero to distill the transition set for training.
3. A case study of BER: 
   a) A simple ablation study on a general binary bit flipping game is conducted to illustrate the effectiveness and generality of BER. 
   b) A DQN and a DQN with BER are trained for different state dimensions, and the results show that BER facilitates effective and efficient policy learning, especially when the problem becomes more complex.","The paper presents comprehensive experimental results to evaluate the effectiveness and efficiency of BER in the locomotion and navigation task of a soft snake robot with serpentine locomotion. The results are as follows:

1. Locomotion and Navigation with a Fixed Target: 
   a) Experiments with a challenging fixed target demonstrate that BER outperforms other benchmark algorithms, achieving a faster convergence rate and better stability. 
   b) The evolution of the maximum Q-value at different locations reveals that BER estimates effective Q-values from both the start and target locations, expediting successful explorations and convergence. 
2. Locomotion and Navigation with Random Targets: 
   a) Experiments with random targets show that BER outperforms all other benchmark algorithms tested, achieving the highest return, success rate, and average velocity. 
   b) The robot with the BER controller successfully and smoothly approaches all random targets, demonstrating its efficiency and adaptability. 
   c) Quantitative results confirm the superiority of BER in terms of average velocity, average distance, average deflection, and success rate.","In conclusion, this paper introduces Back-stepping Experience Replay (BER), a novel technique that is compatible with arbitrary off-policy reinforcement learning (RL) algorithms. BER aims to enhance learning efficiency in systems with approximate reversibility and reduces the need for complex reward shaping. The method constructs reversed trajectories using back-stepping transitions to reach random or fixed targets. The application of BER in a model-free RL approach for the locomotion and navigation of a soft snake robot is also presented. A dynamic simulator is developed to assess the effectiveness and efficiency of the BER algorithm, showing that the soft snake robot reaches a 100% success rate and achieves a significant speed improvement compared to other benchmark approaches. Future work involves applying BER to a physical soft snake robot system, studying the influence of approximate reversibility on BER, and analyzing the convergence properties of BER for state-of-the-art off-policy RL algorithms.",Back-stepping Experience Replay with Application to Model-free Reinforcement Learning for a Soft Snake Robot,"Xinda Qi, Dong Chen, Zhaojian Li, Xiaobo Tan","1
Back-stepping Experience Replay with Application to Model-free
Reinforcement Learning for a Soft Snake Robot
Xinda Qi1, Dong Chen1,∗, Member, IEEE, Zhaojian Li2, Senior Member, IEEE, Xiaobo Tan1, Fellow, IEEE
Abstract—In this paper, we propose a novel technique, Back-
stepping Experience Replay (BER), that is compatible with
arbitrary off-policy reinforcement learning (RL) algorithms. BER
aims to enhance learning efficiency in systems with approximate
reversibility, reducing the need for complex reward shaping.
The method constructs reversed trajectories using back-stepping
transitions to reach random or fixed targets. Interpretable
as a bi-directional approach, BER addresses inaccuracies in
back-stepping transitions through a distillation of the replay
experience during learning. Given the intricate nature of soft
robots and their complex interactions with environments, we
present an application of BER in a model-free RL approach
for the locomotion and navigation of a soft snake robot, which
is capable of serpentine motion enabled by anisotropic friction
between the body and ground. In addition, a dynamic simulator
is developed to assess the effectiveness and efficiency of the BER
algorithm, in which the robot demonstrates successful learning
(reaching a 100% success rate) and adeptly reaches random
targets, achieving an average speed 48% faster than that of the
best baseline approach.
Index Terms—Deep reinforcement learning, experience replay,
soft robot, snake robot, locomotion, navigation.
I. INTRODUCTION
As a promising decision-making approach, reinforcement
learning (RL) has drawn increasing attention for its ability
to solve complex control problems and achieve generalization
in both virtual and physical tasks, as evidenced in various
applications, such as chess games [1], quadrupedal locomotion
[2], and autonomous driving [3], [4]. Considering the inherent
infinite degrees of freedom of soft robots and their compli-
cated interactions with environments [5], RL approaches were
adopted for the control of soft robots, such as soft manipulators
[6] and wheeled soft snake robots [7].
As a typical challenge for RL, especially in tasks where
complicated behaviors are involved, the learning efficiency
suffers from the relatively large search space and the inherent
difficulties of the tasks, which usually requires delicate reward
shaping [8] to guide the policy optimization and to constrain
the learning directions or the behavior styles. The RL agents
have to successfully reach their goals for efficient learning
before getting lost in numerous inefficient failure trials. Mul-
tiple strategies were proposed to address the hard exploration
challenge with sparse rewards, including improving the explo-
ration techniques for more versatile trajectories from intrinsic
This work has been submitted to the IEEE for possible publication.
Copyright may be transferred without notice, after which this version may
no longer be accessible. This work was supported by the National Science
Foundation (Grant CNS 2125484).
1Xinda Qi, Dong Chen, and Xiaobo Tan are with the Department of
Electrical and Computer Engineering, Michigan State University, Lansing,
MI, 48824, USA. Email:{qixinda, chendon9, xbtan}@msu.edu.
2Zhaojian Li is with the Department of Mechanical Engineering, Michigan
State University, Lansing, MI, 48824, USA. Email: lizhaoj1@msu.edu.
∗ Corresponding author.
motivations [9]–[12], and exploiting the information acquired
from the undesired trails [13]–[15].
Compatible with these techniques that might improve learn-
ing efficiency, the motivation of BER proposed for off-
policy RL is the human ability to solve problems forward
(from the beginning to goal) and backward (from the goal
to the beginning) simultaneously, which is different from
the standard model-free RL algorithms that mostly rely on
forward exploration. For example, in proving a complicated
mathematical equation, an effective method is to derive the
equation from both sides where the information of both the
left-hand side (beginning) and the right-hand side (goal) is
utilized, to which the reasoning process and the mechanism
of BER are similar.
In this paper, a BER algorithm is introduced that allows the
RL agent to explore bidirectionally, which is compatible with
arbitrary off-policy RL algorithms. It is applicable for systems
with approximate reversibility and with fixed or random goal
setups. After an evaluation of BER with a toy task, it is
applied to the locomotion and navigation task of a soft snake
robot. The developed algorithm is validated on a physics-based
dynamic simulator with a computationally efficient serpentine
locomotion model based on the system characteristic. Com-
prehensive experimental results demonstrate the effectiveness
of the proposed RL framework with BER in learning the
locomotion and navigation skills of the soft snake robot
compared with other state-of-the-art benchmarks, indicating
the potential of BER in general off-policy RL and robot control
applications.
The remainder of the paper is structured as follows. Section
II introduces the BER algorithm with an evaluation of a toy
task. Section III details the BER application in locomotion
and navigation of a soft snake robot with the performance
comparisons with other benchmarks, and Section IV concludes
the paper.
II. BACK-STEPPING EXPERIENCE REPLAY
A. Background
1) Reinforcement Learning:
A standard RL formalism is adopted where an agent (e.g.
a robot) interacts with an environment and learns a policy
according to the perceptions and rewards.
In each episode, the system starts with an initial state s0
with a distribution of p(s0), and the agent observes a current
state st ∈ S ⊆ Rn in the environment at each time step
t. Then, an action at ∈ A ⊆ Rm is generated to control
the agent based on the current policy π and the observations.
Afterward, the system evolves to a new state st+1 based on
the action and transition dynamics p(·|st, at), and a reward
rt = r(st, at, st+1) is collected by the agent for the learning
arXiv:2401.11372v1  [cs.RO]  21 Jan 2024
2
before the termination of the episode. During the training
process, the RL agent learns an optimal policy π∗ : S → A
mapping states to actions that maximize the expected return.
The return is defined as the accumulated discounted reward
Rt = P∞
i=t γi−tri, where γ is a discount factor.
The state value function V π(st) = E(Rt|st) represents the
expected return starting from state st following the current pol-
icy π, and the action value function Qπ(st, at) = E(Rt|st, at)
represents the expected return starting from the state st with
an immediate action at by following the current policy π. All
optimal policies π∗ share the same optimal Q-function Q∗,
according to the Bellman equation [16]:
Q∗(st, at) = Es′∼p(·st,at)

r(st, at, s′) + γ max
a′∈A Q∗(s′, a′)

(1)
2) Deep Q-Networks (DQN) and Deep Deterministic Policy
Gradient (DDPG):
The Deep Q-Network (DQN) is a model-free, off-policy RL
approach suitable for agents operating in discrete action spaces
[16]. It typically employs a neural network Q to approximate
the optimal Q-function Q∗, selecting optimal actions: a∗ =
arg maxa∈A Q(st, a). Exploration is often facilitated by the
ϵ-greedy algorithm. To stabilize training, a replay buffer stores
transition data (st, at, rt, st+1) and is used to optimize Q with
a loss L = E(Q(st, at)−yt), where the target is calculated by
using a periodically updated target network Qtarg: yt = rt +
γ maxa∈A Qtarg(st+1, a), and using transitions in the replay
buffer.
Deep Deterministic Policy Gradient (DDPG) [17] is an off-
policy RL algorithm that simultaneously learns a Q-function
and a policy, tailored specifically for environments with con-
tinuous action spaces. DDPG interweaves the learning process
of an approximator to Q∗, with an approximator to select a∗,
offering a unique adaptation for continuous action scenarios.
B. Algorithm for BER
The above classical off-policy RL algorithms often face
challenges with systems characterized by sparse rewards or
challenging tasks with rewards hard to reshape. In such
scenarios, RL agents rarely achieve informative standard for-
ward explorations due to a low success rate in reaching
goals in complex problems without precise guidance [13]. To
address these challenges, we propose a novel Back-stepping
Experience Replay (BER) algorithm for tasks with different
goals (Alg. 1), designed to enhance the learning efficiency of
off-policy RL algorithms. This is achieved by incorporating
exploration methods in both forward and backward directions.
The BER algorithm requires at least an approximate re-
versibility of the system. This means that from a standard tran-
sition (st, at, st+1), a back-stepping transition (st+1, eat, st)
can be constructed, which is similar to a real transition
(st+1, eat, sb,t) in the environment, i.e., sb,t
≈
st. The
action in the back-stepping transition is calculated as eat =
f(st, at, st+1), where function f is dependent on the en-
vironment. The approximate reversibility is evaluated by a
small upper bound K for all transitions during back-stepping
calculation:
∥sb,t − st∥ ≤ K · ∥st+1 − st∥, K < 1
(2)
There exists a perfect reversibility when K = 0 with a prob-
ably complex function f, while an approximate reversibility
might be achieved with a slightly larger K and a simpler and
solvable function f.
The idea of BER is simple yet effective: instead of solely
relying on forward explorations (navy blue solid line in Fig.
1) from initial states to goals, which depend heavily on
the randomness of forward trajectories to reach these goals,
RL agents also navigate backward from the goals to the
initial states in the tasks (sky blue solid line in Fig. 1). The
standard transitions are sampled from the standard forward
and backward exploration trajectories (solid lines in Fig. 1),
where the initial states of themselves are included. Then, the
back-stepping transitions are calculated based on the standard
transitions to constitute the reversed trajectories (dashed lines
in Fig. 1), where the virtual goals are set to be the original
initial state in their corresponding standard trajectories, such
that the reversed trajectories are guaranteed to reach their
virtual goals and contribute to the learning efficiency.
Figure 1. Illustration of the Back-stepping Experience Replay.
During the explorations, the standard and the back-stepping
transitions are collected and stored in separate replay buffers
for training. A strategy St is used to sample the transitions
from the standard replay Rf with a probability Pt,f and from
the back-stepping replay Rb with a probability Pt,b, where
Pt,f +Pt,b = 1. For a system with imperfect reversibility, Pt,b
gradually drops to zero to distill the transition set for training
because of the inaccurate back-stepping transition. The details
of BER are shown in Alg. 1. It should be noticed that the
operator ⊙ between the states and the goals also indicates
the modification of the sequential data (e.g., the history data)
when the back-stepping transitions are constructed.
The BER accelerates the estimation of Q-functions of the
RL agent by using the reversed successful trajectories to
bootstrap the networks. One interpretation of BER is a bi-
directional search method for standard off-policy RL ap-
proaches, with a higher convergence rate and learning effi-
ciency. The distillation strategy of the transitions for training
needs to be carefully tuned and might be combined with other
exploration tricks, to reach an accurate policy learning in the
end and avoid the limitations brought by the bi-directional
search method, e.g., non-trivial sub-optimum.
In the practical learning tasks, the accuracy and the com-
plexity of the function f
:
eat = f(st, at, st+1), which
calculates the actions
eat in the back-stepping transitions
(st+1, eat, st), need to be balanced. An accurate f yields
3
Algorithm 1: Back-stepping Experience Replay (BER)
Given:
− An off-policy RL algorithm A.
▷ e.g. DDPG
− A probability Pb triggering backward trial.
− A strategy St for sampling transitions in replays
Require:
− Approximate reversibility of the system
1 Initialize A
▷ e.g. initialize networks
2 Initialize replay buffers Rf and Rb
3 for epoch = 1 → M do
4
Sample a goal g with an initial state s0.
5
Forward trial starts
6
for t = 0 → Tend − 1 do
7
Sample an action at using the policy of A:
8
at ← π(st ⊙ g)
▷ e.g. ⊙ → diff, concat
9
Execute action at, observe new state st+1
10
end
11
for t = 0 → Tend − 1 do
12
rt := r(st, at, st+1, g)
13
Store transition (st ⊙ g, at, rt, st+1 ⊙ g) in Rf
▷ standard experience replay
Construct a back-stepping transition:
14
rb,t := r(st+1, eat, st, s0)
15
Store transition (st+1 ⊙ s0, eat, rb,t, st ⊙ s0) in
Rb
▷ BER
16
end
17
Forward trial ends
18
Backward trial starts with Pb
19
Swap the goal g and the initial state s0:
s0, g = g, s0
20
Repeat line 6 - line 16
21
Backward trial ends
22
for t = 1 → N do
23
Sample a mini-batch B from the replay buffers
{Rf, Rb} using Sr
24
Perform one step of optimization using A and
mini-batch B
25
end
26 end
better reversibility (with smaller K in Eq. (2)) with more
accurate back-stepping transitions and brings less bias and
noise, while f itself could be computationally expensive or
even unsolvable. On the other hand, a moderate relaxation of
the accuracy of f might boost the efficiency of the calculation
of back-stepping transitions, when the larger bias and the
noises brought by the approximate reversibility (with larger
K) are managed by the distillation mechanism in BER.
1) A case study of BER:
To illustrate the effectiveness and generality of BER, a general
binary bit flipping game [13] with n bits was considered as an
environment for the RL agent, where the state was the bit value
array s = {si}n
i=1 ∈ S, si ∈ {0, 1}, and the action was the
index of the chosen bit a ∈ {1, ..., n} = A that was flipped.
It was noticed that the game was completely reversible and
eat = f(at) = at for any time step and transition. The initial
state s0 ∈ S and the goal g ∈ S were sampled uniformly
and randomly, with a sparse non-negative reward: rt(s, a) =
−[s ̸= g]. The game is terminated once s = g.
A simple ablation study was designed where a DQN and
a DQN with BER were used for training when n = 4, 6, 8.
The fully activated backward exploration and the use of back-
stepping transitions were stopped after 1k epochs directly.
The experimental result (Fig. 2) showed that BER facilitated
an effective and efficient policy learning for a general DQN
approach, and contributed more when the problem became
more complex (i.e., n was larger).
Figure 2. Training experiments of the bit flip game with different
algorithms and state dimensions. (A) Returns; (B) Success rates.
III. BER IN MODEL-FREE RL FOR A SOFT SNAKE ROBOT
In this section, a locomotion and navigation task for a
compact pneumatic soft snake robot with snake skins in
our previous works [18], [19] is utilized to evaluate the
effectiveness and efficiency of BER with a model-free RL
approach, where the robot learns both movement skills and
efficient strategies to reach different challenging targets.
A. Soft Snake Robot and Serpentine Locomotion
Compared with soft snake robots where each air chamber
was controlled independently [20], in this paper, a more com-
pact soft snake robot with snake skins [18] is considered. There
are only four independent air paths to generate the traveling-
wave deformation of the robot, which enables the robot to
traverse complex environments more easily by reducing the
number of pneumatic tubing. The body of the robot consists
of six bending actuators and each actuator is divided into four
air chambers (Figs. 3A, 3D) that connect to four air paths (Fig.
3B). Four sinusoidal waves with 90-degree phase differences
and the same amplitude can be used as references of pressures
in air paths to generate traveling-wave deformation (Fig. 3C),
when the biases of waves induce unbalance actuation for
steering of the robot.
Serpentine locomotion is adopted for the movement of the
soft snake robot, where the anisotropic friction between the
snake skins and the ground propels the robot during the
traveling-wave deformation [21]. The artificial snake skins are
designed with a soft substrate and embedded rigid scales (Fig.
3E); see [19] for more details.
To describe the serpentine locomotion of the robot, the
dynamic model in [21] is adopted, where the body of the robot
is modeled as an inextensible curve in a 2D plane with a total
length L and a constant density ρ per unit length. The position
of each point on the robot at time t is defined as:
X(s, t) = (x(s, t), y(s, t))
(3)
where s is the curve length measured from the tail of the robot.
By utilizing a mean-zero anti-derivative I0 [22] (I0[f](s, t)
=
R s
0 f(s′, t) ds′ −
1
L
R L
0 ds
R s
0 ds′ f(s′, t)), the position
X(s, t) and the orientation θ(s, t) (the angle between the local
4
Figure 3. The overview of the soft snake robot with skins. (A) The soft snake robot with soft snakeskins; (B) The connection between
air chambers and air paths; (C) The actuation pressures for air paths; (D) The structure of one bending actuator; (E) The structure of soft
snakeskin; (F) The simulation (sim) and experimental (exp) results of the trajectory of the COM of the snake robot on a rough paper surface.
tangent direction and the X-axis of the inertial frame) of each
point are described as a function of the position X(t) and
orientation θ(t) (Fig. 4) of the center of mass (COM) of the
robot:
X(s, t) = X(t) + I0[Xs](s, t)
(4)
θ(s, t) = θ(t) + I0[κ](s, t)
(5)
where Xs = (cos θ, sin θ) and κ(s, t) is the local curvature.
X(t) = 1
L
R L
0 X(s, t) ds, θ(t) = 1
L
R L
0 θ(s, t) ds. The curva-
ture κ(s, t) is related to the local pneumatic pressure via:
κ(s, t) = Kb · ∆p(s, t)
(6)
where Kb is the proportional constant and ∆p(s, t) is the
pressure difference between the two air chambers at point s.
Figure 4. The illustration of the soft snake robot with serpentine
locomotion approaching a target.
The anisotropic friction ffric between the snake skins
and the ground is described as a weighted average of the
independent components in different local directions (forward
ˆf, backward ˆb, transverse ˆt):
(
ffric = −ρg(µt(ˆu · ˆt)ˆt + µl(ˆu · ˆf) ˆf)
µl = µfH(ˆu · ˆf) + µb(1 − H(ˆu · ˆf))
(7)
where ˆu represents the direction of the local velocity, µf,
µb, and µt are the friction coefficients of the snakeskin in
ˆf, backward ˆb, and ˆt directions, respectively. H(x) = (1 +
sgn(x))/2, where sgn is the signum function.
The dynamics of each point of the snake robot is determined
by Newton’s second law:
ρ ¨
X = ffric + finte
(8)
where finte is the internal force in the robot body, which
includes internal air pressure, bending elastic force, et al, with
observations:
R L
0 finte = 0 and
R L
0 (X(s, t)−X(t))×finte =
0.
Finally, the dynamics for the COM of the robot are derived
using the equation (3)-(8) with the observations of finte; see
[22] for more details.
Based on the dynamic model of the robot, which simplifies a
dynamic system for all points of the robot to a single dynamic
system for the COM of the robot, a simulator is designed
with proper discretizations and numerical techniques for RL
training. The simulation results matched the experimental
results [19] of the soft snake robot when different pressure
biases were applied for the robot’s steering (Fig. 3F), where
the wavy trajectories in the experiments were attributed to the
limited number (25) of the tracking markers in the tests.
B. RL formulation of Locomotion and Navigation of the Robot
In this paper, the locomotion and navigation of the soft
snake robot is formulated as a Markov Decision Process
(MDP) M and solved with a model-free RL. The M is defined
as a tuple M = (A, S, R, T , γ):
1) Action space: Compared with a random Central Pattern
Generator (CPG) [7], more constrained sinusoidal waves are
used to generate a smoother traveling-wave deformation of the
robot for better locomotion efficiency. Besides, the learned
controller of the robot is limited to avoid high-frequency
pressure change, i.e., the RL agent is only able to generate
an action to change the parameters of the waveform at the
beginning of each actuation period [0, T] that is same as the
period of the sinusoidal waves, and one episode consists of
multiple connected actuation periods. The sinusoidal pressure
pi for i-th channel of the robot is designed as:
pi = pm sin (c · 2π
T tr + (i − 1) · π
2
) + bi,pre + (bi − bi,pre)tr
T
(9)
where tr
∈ [0, T] is the relative time in one actuation
period. pm and bi ∈ [0, bm] are the fixed magnitude and
bias of the sinusoidal waves for the i-th channel, respectively,
i ∈ {1, 2, 3, 4}. bi,pre is a one-step history of the wave bias
5
bi for the i-th channel with bi,pre = 0 at the initial state.
c ∈ {−1, 1} is a variable to control the propagation direction
of the traveling-wave deformation and thus can change the
movement direction of the robot.
The action space A of the RL agent for locomotion and
navigation of the robot is designed as:
a = {ba,1, ba,2, c} ∈ A
(10)
where bi’s are constructed by ba,1 ∈ [−bm, bm] and ba,2 ∈
[−bm, bm]:
(
b1, b3 = max(0, ba,1), − min(0, ba,1)
b2, b4 = max(0, ba,2), − min(0, ba,2)
(11)
At the beginning of each actuation period, based on the
current policy, the RL agent observes the state and generates
an action, which specifies the waveform of the pressures
in that period to propel the snake robot. The wave design
guarantees the continuity of the pressures across different
actuation periods to avoid impractical sudden changes in the
pressures and the robot’s body shape.
2) State space: A goal-conditioned state is used for the
learning of the RL agent for adapting to different random
targets. Specifically, a relative representation of the snake
robot’s position and orientation with respect to the target is
used as part of the state (Fig. 4):
s = {∆X, ∆Y, ∆θ, ba,1,pre, ba,2,pre} ∈ S
(12)
where ∆X = xg − X, ∆Y = yg − Y denote the relative
position of the target to the COM of the snake robot, ∆θ =
θg − θ ∈ (−π, π] represents the relative direction of the target
to the main direction of the robot, and θg = arctan(∆Y/∆X)
is the angle between the line from the COM of the robot to
the target and the X-axis, ba,1,pre and ba,2,pre are two-step
histories of the action ba,1 and ba,2, respectively, with an initial
setting of {0, 0}.
The velocities of COM of the robot are not included as part
of the state because the value of the Froude number Fr [22] in
serpentine locomotion of the snake robot is small, indicating
that the frictional and gravitational effects dominate the inertial
effect. Two-step histories (longer than one step) are introduced
to compensate for the omission of the velocity state.
3) Reward function: The reward function r is pivotal for
the RL agent to learn the desired behaviors. The training
objective in this work is to drive the COM of the snake robot to
reach a random target as soon as possible, with a preference for
serpentine locomotion where the robot approaches the target
along its main direction. Therefore, the reward assigned to the
agent at time t is designed as:
rt =







w1
∆Lt
∆L0
+ w2
2∆θr,t
π
+ Rg,
∆Lt ≤ ϵ
w1
∆Lt
∆L0
+ w2
2∆θr,t
π
,
else
(13)
where w1 and w2 are non-positive coefficients, Rg is a large
sparse positive success reward once the COM of the robot
enters a neighborhood of the target with a radius of ϵ. ∆Lt =
p
∆X2
t + ∆Y 2
t is the distance between the COM of the robot
and target at time t, and ∆Lt = ∆L0 when t = 0. The
deflection ∆θr,t ∈ [0, π/2] is used in the reward to allow the
robot to approach the target in a backward direction as well:
∆θr,t =
(
|∆θt|,
−π/2 ≤ ∆θt ≤ π/2
π − |∆θt|,
else
(14)
4) Transition probabilities: The transition probability,
T (s′|s, a), characterizes the underlying dynamics of the robot
system in the environment. In this study, we do not assume
any detailed knowledge of this transition probability while
developing our RL algorithm.
C. Experiments of RL algorithms
1) Experimental Setups:
The RL experiments for the locomotion and navigation of
the snake robot were conducted in a customized dynamic
simulator which was developed based on the aforementioned
serpentine locomotion model (Sec. III. A). The soft snake
robot had a length of 0.5 m with a linear density of 1.08
kg/m. The frictional anisotropy between the snake skins and
the ground was set as µf : µb : µt = 1 : 1 : 1.5, and the
maximum of the pressure bias bm was set as the same as
pm = 276 kPa. The proportional constant Kb between the
applied pressure difference and the curvature was set as 0.058
kPa·m. The period of the actuation and the sinusoidal waves
was 1 s.
The serpentine locomotion of the soft snake robot demon-
strated approximate reversibility (Fig. 5A) when the function
f was designed as: eat = f(at) = {ba,1, ba,2, −c} when
at = {ba,1, ba,2, c}. The trajectories in the simulation results
(Fig. 5A) suggested a small K < 1 (in Eq. (2)) for locomotion
and navigation of the soft snake robot when the above function
f was used.
Figure 5. (A). The approximate reversibility of the movement of
the soft snake robot with snake skins. (B). The fixed target and the
sampling range of the random targets.
The soft snake robot was initialized in the simulator by
using a horizontal static curved shape ((X, Y ) = (0, 0), θ = 0)
with zero-value action histories and a target (with neighbor-
hoods: ϵ = 0.03 m), whose control policies was learned by
using BER (with DDPG) and several state-of-art benchmark
algorithms, including DDPG, HER [13], and PPO [23]. The
number of total training epochs was 10k and the strategy
to sample the transitions was Pt,b = 0.5e−0.002i when the
index of epoch i ≤ 2500, Pt,b = 0 when i > 2500, and
Pt,f = 1 − Pt,b, Pb = Pt,b. The coefficients of the reward
were set as ω1 = 0.15, ω2 = 1, and the termination condition
6
Figure 6. Experimental results of the training for locomotion and navigation of the soft snake robot with one fixed target (0, 0.5 m). (A)
Returns; (B) Success rates; (C) Average distances; (D) Average deflections.
Figure 7. Evolution of the maximum Q-value at different locations during the training (from left to right: initial state, epoch 100, 500, 1k,
10k). (A) Training with BER; (B) Training with DDPG.
for one episode was either the COM of the robot entering
a neighborhood of the target and receiving a success reward
(Rg = 50) or the exploration time exceeding 150 s.
The return, success rate, average distance (the averaged
∆Lt/L0 for each time step t), and average deflection (the
averaged ∆θr,t for each time step t) were used to evaluate the
algorithms during the training, with moving-window averaging
for training with different seeds (lwindow = 50 epochs). Three
training experiments with different random seeds (for param-
eter initialization) were conducted to evaluate each algorithm,
where the solid line and the shaded area showed the mean and
the standard deviation, respectively (Figs. 6 and 8). An AMD
9820X processor with 64 GB memory and Ubuntu 18.04 was
used for the training.
2) Locomotion and Navigation with a Fixed Target:
The performance of the algorithms was initially evaluated on
the locomotion and navigation task of the robot, targeting a
challenging fixed point (xg, yg) = (0, 0.5 m) (Fig. 5B). The
experiment results of the training showed that both DDPG
and BER were able to solve the task and learn policies to
reach the fixed target successfully, while HER had worse
stability and PPO was unable to solve the task within the
epoch limitation (Fig. 6). It was also shown that BER had
a faster convergence rate and better stability compared with
other baseline algorithms.
The evolution of the maximum Q-value at different locations
for the algorithms during the training process (with the same
seed) revealed the underlying mechanism and the advantage
of BER (Fig. 7). It was shown that the effective Q-values in
the training with BER were estimated from both the start and
the target locations, expediting the successful explorations and
the convergence of the estimation. The BER learned a more
informative Q-value distribution after 500 epochs than that
of the baseline DDPG after 1000 epochs. The final Q-value
distribution of BER was also more accurate than that of the
baseline DDPG, manifested by their shapes and the positions
of the Q-value’s peaks.
3) Locomotion and Navigation with Random Targets:
A locomotion and navigation task of the soft snake robot
with random targets was then explored by using different RL
algorithms, where a half ring was used to randomly sample
the target because of the system symmetry: g ∈ {(d, α) | d ∈
[0.3, 1], α ∈ [0, π]} (Fig. 5B). Besides, a strategy was designed
where the targets were sampled uniformly from gradually
7
Figure 8. Experimental results of the training for locomotion and navigation of the soft snake robot with random targets. (A) Returns; (B)
Success rates; (C) Average distances; (D) Average deflections.
expanding areas for the i-th training epoch within the total
n epochs: g ∈ {(d, α) | d ∈ [0.3, 1], α ∈ [0, ψ] ∪ ( π
2 − ψ, π
2 +
ψ] ∪ (π − ψ, π]}, ψ =
π
4n2 i2.
The training results revealed that BER outperformed all
other benchmark algorithms tested (Fig. 8). BER achieved the
highest return and success rate during training, exhibiting more
stable behavior and a smaller average deflection. In contrast,
the baseline DDPG’s performance declined when introduced to
a variety of targets, despite its strong early-stage performance.
HER struggled to learn to reach targets in different areas,
whereas PPO gradually learned an effective policy, a process
that benefited from the random-goal training setup involving
progressively changing targets.
The robot’s trajectories further demonstrated BER’s effi-
ciency (Fig. 9), where controllers with median success rates
from each algorithm were used for control. A video for these
experiments can be viewed at https://youtu.be/Z0da6rVu9j8.
Three representative targets were tested: (−0.8 m, 0.1 m) for
moving backward, (0, 0.5 m) for moving towards a lateral
target, (0.8 m, 0.1 m) for moving forward. The BER controller
successfully and smoothly guided the robot to all targets. In
contrast, the DDPG and HER controllers exhibited inefficient
oscillations, possibly due to less accurate Q-function estima-
tion. While the PPO controller managed to reach all targets, it
also displayed inefficient oscillation and adopted a sub-optimal
policy for the forward target (0.8 m, 0.1 m).
The quantitative results of the algorithms (Table I) were
the average values tested by using the controllers trained with
different seeds, and using 50 random targets sampled from
the half-ring area (Fig. 5B). The average velocity (vavg =
∆L0/tep, tep: episode length) indicated the efficiency of the
learned controllers. Notably, the average velocity of the robot
with the BER controller (0.0169 m/s) was approximately 48%
faster than that of the DDPG baseline (0.0114 m/s), and
significantly higher compared to other benchmarks.
Besides, compared to other algorithms, BER not only
learned an efficient controller based on the primary reward
(highest average deflection: 0.2920 rad), but was also able to
sacrifice the secondary reward to some extent (second highest
average distance: 0.4002 m/m) for better performance. The
success rate of BER reached 100% while that of the other
baselines did not exceed 65%, which exhibited the advantage
of BER in the locomotion and navigation learning of the soft
snake robot.
Table I. TESTING PERFORMANCE COMPARISONS OF DIFFERENT
ALGORITHMS.
Metrics
PPO
HER
DDPG
BER
Average velocity (m/s)
0.0061
0.0080
0.0114
0.0169
Average distance (m/m)
0.6241
0.5202
0.3903
0.4002
Average deflection (rad)
0.4049
0.6702
0.3915
0.2920
Success rate (%)
64.44
43.33
61.11
100
IV. CONCLUSIONS AND DISCUSSIONS
A novel technique, Back-stepping Experience Replay, was
proposed in this paper, which exploited the back-stepping
transitions constructed by using the standard transitions in
both forward and backward exploration trajectories, improving
the learning efficiencies in off-policy RL algorithms for the
approximate reversible systems. The BER was compatible with
arbitrary off-policy RL algorithms, demonstrated by combin-
ing with DQN and DDPG in a bit-flip task and locomotion
and navigation task for a soft snake robot, respectively.
A model-free RL framework was proposed for locomotion
and navigation of a soft snake robot as an application of the
proposed BER, where a conventional locomotion model for
real snakes was adopted to describe the serpentine locomotion
of the soft snake robot and to design a simulator for learning.
An RL formulation for locomotion and navigation of the soft
snake robot was built based on the characteristics of the robot.
Extensive experiments showed that the proposed RL approach
was able to learn an efficient controller that drove the soft
snake robot approaching fixed or even random targets by using
serpentine locomotion. For the tasks with random targets, the
controller learned by using BER achieved a 100 % success
rate and the robot’s average speed was 48 % faster than that
of the best baseline RL benchmark.
For future work, we will apply the proposed RL approach
with BER to a physical soft snake robot system, where the data
from the physical system will be used for learning. Besides, we
will also study the influence of the approximate reversibility
of general systems to BER, and analyze the convergence
properties of BER for proper state-of-the-art off-policy RL
algorithms.
REFERENCES
[1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot, et al., “Mastering the game of go with deep neural networks
and tree search,” Nature, vol. 529, no. 7587, pp. 484–489, 2016.
8
Figure 9. Trajectories of the COM of the soft snake robot by using the controllers learned by different algorithms. (A) Trajectories with
a backward target where the relationships between positions and time are shown in (D), (G); (B) Trajectories with a lateral target where
the relationships between positions and time are shown in (E), (H); (C) Trajectories with a forward target where the relationships between
positions and time are shown in (F), (I).
[2] X. Liu, R. Gasoto, Z. Jiang, C. Onal, and J. Fu, “Learning to locomote
with artificial neural-network and cpg-based control in a soft snake
robot,” in 2020 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pp. 7758–7765, IEEE, 2020.
[3] D. Chen, L. Jiang, Y. Wang, and Z. Li, “Autonomous driving using
safe reinforcement learning by incorporating a regret-based human lane-
changing decision model,” in 2020 American Control Conference (ACC),
pp. 4355–4361, IEEE, 2020.
[4] M. Hua, D. Chen, X. Qi, K. Jiang, Z. E. Liu, Q. Zhou, and H. Xu, “Multi-
agent reinforcement learning for connected and automated vehicles
control: Recent advancements and future prospects,” arXiv preprint
arXiv:2312.11084, 2023.
[5] C. Lee, M. Kim, Y. J. Kim, N. Hong, S. Ryu, H. J. Kim, and S. Kim,
“Soft robot review,” International Journal of Control, Automation and
Systems, vol. 15, pp. 3–15, 2017.
[6] A. Gupta, C. Eppner, S. Levine, and P. Abbeel, “Learning dexterous
manipulation for a soft robotic hand from human demonstrations,” in
2016 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS), pp. 3786–3793, IEEE, 2016.
[7] X. Liu, C. D. Onal, and J. Fu, “Reinforcement learning of cpg-regulated
locomotion controller for a soft snake robot,” IEEE Transactions on
Robotics, 2023.
[8] A. Y. Ng, D. Harada, and S. Russell, “Policy invariance under reward
transformations: Theory and application to reward shaping,” in Interna-
tional Conference on Machine Learning, vol. 99, pp. 278–287, Citeseer,
1999.
[9] G. Ostrovski, M. G. Bellemare, A. Oord, and R. Munos, “Count-based
exploration with neural density models,” in International Conference on
Machine Learning, pp. 2721–2730, PMLR, 2017.
[10] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven
exploration by self-supervised prediction,” in International Conference
on Machine Learning, pp. 2778–2787, PMLR, 2017.
[11] L. Choshen, L. Fox, and Y. Loewenstein, “Dora the explorer:
Directed outreaching reinforcement action-selection,” arXiv preprint
arXiv:1804.04012, 2018.
[12] A. P. Badia, P. Sprechmann, A. Vitvitskyi, D. Guo, B. Piot, S. Kap-
turowski, O. Tieleman, M. Arjovsky, A. Pritzel, A. Bolt, et al.,
“Never give up: Learning directed exploration strategies,” arXiv preprint
arXiv:2002.06038, 2020.
[13] M. Andrychowicz et al., “Hindsight experience replay,” Advances in
Neural Information Processing Systems, vol. 30, 2017.
[14] M. Fang, C. Zhou, B. Shi, B. Gong, J. Xu, and T. Zhang, “Dher: Hind-
sight experience replay for dynamic goals,” in International Conference
on Learning Representations, 2018.
[15] Y. Ding, C. Florensa, P. Abbeel, and M. Phielipp, “Goal-conditioned
imitation learning,” Advances in Neural Information Processing Systems,
vol. 32, 2019.
[16] V. Mnih, A. K. others, G. Ostrovski, et al., “Human-level control through
deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533,
2015.
[17] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” arXiv preprint arXiv:1509.02971, 2015.
[18] X. Qi, H. Shi, T. Pinto, and X. Tan, “A novel pneumatic soft snake robot
using traveling-wave locomotion in constrained environments,” IEEE
Robotics and Automation Letters, vol. 5, no. 2, pp. 1610–1617, 2020.
[19] X. Qi, T. Gao, and X. Tan, “Bioinspired 3d-printed snakeskins enable
effective serpentine locomotion of a soft robotic snake,” Soft Robotics,
vol. 10, no. 3, pp. 568–579, 2023.
[20] M. Luo, M. Agheli, and C. D. Onal, “Theoretical modeling and
experimental analysis of a pressure-operated soft robotic snake,” Soft
Robotics, vol. 1, no. 2, pp. 136–146, 2014.
[21] D. L. Hu, J. Nirody, T. Scott, and M. J. Shelley, “The mechanics
of slithering locomotion,” Proceedings of the National Academy of
Sciences, vol. 106, no. 25, pp. 10081–10085, 2009.
[22] D. L. Hu and M. Shelley, “Slithering locomotion,” in Natural locomotion
in fluids and on surfaces: swimming, flying, and sliding, pp. 117–135,
Springer, 2012.
[23] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
2017.
","nanThe literature review highlights the challenges associated with learning efficiency in RL, especially for tasks where complicated behaviors are involved. The authors discuss the need for carefully shaping rewards to guide policy optimization and constrain learning directions or behavior styles. They also mention the use of various strategies, such as improving exploration techniques for more versatile trajectories and exploiting information from undesired trails, to address the hard exploration challenge with sparse rewards. However, existing techniques focus on improving exploration in forward directions, whereas BER introduces a bi-directional approach that is inspired by human ability to solve problems forward and backward simultaneously."
S4 builds knowledge loops between all available knowledge sources that define modern software systems to improve their interpretability and adaptability.,This paper introduces and discusses the S4 concept.,None,S4 considers the deployed systems to be the second knowledge source.,The success of the S4 research agenda depends on the software engineering and systems community embracing the self-sustaining concept.,Self-sustaining Software Systems (S4): Towards Improved Interpretability and Adaptation,"Christian Cabrera, Andrei Paleyes, Neil D. Lawrence","Self-sustaining Software Systems (S4):
Towards Improved Interpretability and Adaptation
Christian Cabrera
Department of Computer Science and
Technology, University of Cambridge
United Kingdom
chc79@cam.ac.uk
Andrei Paleyes
Department of Computer Science and
Technology, University of Cambridge
United Kingdom
ap2169@cam.ac.uk
Neil D. Lawrence
Department of Computer Science and
Technology, University of Cambridge
United Kingdom
ndl21@cam.ac.uk
ABSTRACT
Software systems impact society at different levels as they perva-
sively solve real-world problems. Modern software systems are
often so sophisticated that their complexity exceeds the limits of
human comprehension. These systems must respond to chang-
ing goals, dynamic data, unexpected failures, and security threats,
among other variable factors in real-world environments. Sys-
tems’ complexity challenges their interpretability and requires
autonomous responses to dynamic changes. Two main research
areas explore autonomous systems’ responses: evolutionary com-
puting and autonomic computing. Evolutionary computing focuses
on software improvement based on iterative modifications to the
source code. Autonomic computing focuses on optimising systems’
performance by changing their structure, behaviour, or environ-
ment variables. Approaches from both areas rely on feedback loops
that accumulate knowledge from the system interactions to inform
autonomous decision-making. However, this knowledge is often
limited, constraining the systems’ interpretability and adaptability.
This paper proposes a new concept for interpretable and adaptable
software systems: self-sustaining software systems (S4). S4 builds
knowledge loops between all available knowledge sources that de-
fine modern software systems to improve their interpretability and
adaptability. This paper introduces and discusses the S4 concept.
CCS CONCEPTS
• Software and its engineering → Designing software; Auto-
matic programming; Software evolution.
KEYWORDS
Autonomous Systems, Software Engineering, Knowledge Graphs,
Data-Oriented Architectures, Large Language Models.
1
INTRODUCTION
Software systems have become pervasive, impacting our society
at different levels [28]. They support our daily activities by satis-
fying requirements like recommending the content we consume,
planning our commutes in a city, or enabling scientific discover-
ies. Technological advances, the emergence of challenging data
requirements, and the difference between isolated development
settings and dynamic real-world environments have made systems
more complex over time [3, 12]. This growing complexity and the
dynamic environments challenge systems’ interpretability, mainte-
nance and sustainability [12]. Modern software systems are often
so sophisticated that their complexity exceeds the limits of human
comprehension and control [23]. Deployed systems must respond
to changing goals, variable data, unexpected failures, and secu-
rity threats, among other variables that emerge from demanding
requirements and real-world environments [24]. Such responses
must be autonomous because software developers, systems man-
agers, and end-users do not control all systems components [22].
Systems stakeholders must understand the reasons behind the sys-
tems’ behaviour despite such automation.
Two main research areas explore the development of autonomous
systems that respond to dynamic conditions. Evolutionary comput-
ing develops optimisation and problem-solving techniques inspired
by natural evolution since the 1940s [7]. Genetic improvement op-
timises existing software based on genetic programming. This pro-
cess makes small mutations to the system source code, creating new
system versions. Each version is benchmarked against the previous
one to evaluate the system improvement for a given criteria [17].
Autonomic computing has researched the design and implementa-
tion of self-managing, self-adaptive, self-configuration, self-healing,
and self-optimisation systems in last decades [16]. This field aims
to build systems that respond to variable environments without
human intervention. It proposes to equip systems with sensors for
environment monitoring, decision functions to reason about per-
ceived changes, and actors to modify systems’ structure, behaviour,
and environment variables. These elements constitute feedback
loops that accumulate knowledge from systems’ internal and ex-
ternal interactions. This knowledge drives adaptive decisions [39].
Both research areas are fruitful fields with several successful appli-
cations [26, 40]. They have contributed to developing architectures,
design patterns, methodologies, algorithms, and tools for develop-
ing autonomous systems. However, approaches proposed in these
fields are often limited. They are based on processes that work
well when optimising simple non-functional requirements (e.g., the
number of bugs in the code, the response time of the systems, or the
shortest path for an autonomous vehicle) [2, 17]. Modern systems
have demanding high-level requirements such as accountability,
trustworthiness, transparency, and fairness [6, 31]. Engineers de-
ploy systems in dynamic environments with changing goals and
complex uncertainties. Current autonomous systems are limited
to runtime representations of goals and fail to model real-life com-
plex phenomena [39]. Their optimisation procedures are usually
black-box and hard to explain for sufficiently complex systems [4].
This lack of transparency impacts the end user’s ability to interpret
the software’s behaviour. We argue that the limitations of modern
autonomous systems arise from the intrinsic nature of evolution-
ary and autonomic computing. Current autonomous systems make
adaptive decisions mainly based on knowledge from their internal
and external interactions, which are encoded in black-box models.
SATrends 2024, April 14–20, 2024, Lisbon, Portugal
Cabrera, et al.
This knowledge is insufficient and constrains the systems’ self-
sustaining capabilities. Modern and future end-user requirements
and the current software development trends demand a shift in the
design paradigms of autonomous systems.
This paper introduces the concept of self-sustaining software
systems (S4) (Section 2), which leads to improved interpretability of
systems leveraging the machine capabilities to self-analyse and self-
maintain through shifts in software engineering (SE) practices [3]
and advances in artificial intelligence (AI) [36]. S4 empowers users
such as clinicians, scientists, and manufacturers and enables the de-
velopment of complex systems that operate together with humans
and remain under their control. The idea behind S4 is to replace the
feedback loops that current autonomous systems use with knowl-
edge loops that would enrich both systems’ interpretability and
adaptability. Such loops integrate and exploit the three sources of
knowledge that define modern software systems. The first source
is the developers who encode the users’ requirements and their
expert knowledge into design models. The deployed systems are
the second source of knowledge. Their states and internal and exter-
nal interactions provide knowledge about systems’ behaviour. The
third source is the knowledge produced by the SE community. Such
knowledge provides insights into best practices for systems man-
agement based on empirical evidence. The s4 concept is discussed
in Section 3 and Section 4 concludes the paper.
2
SELF-SUSTAINING SOFTWARE SYSTEMS (S4)
S4 supports the implementation of interpretable software systems
capable of self-analysis and self-maintenance. Interpretability arises
from collecting and managing the knowledge that defines modern
software systems. This extended knowledge informs end-users,
engineers, and the system itself. This section introduces the knowl-
edge sources S4 intends to integrate and provides the first directions
towards self-sustaining systems behaviour.
2.1
S4 Knowledge Sources
S4 integrates knowledge sources that expand the systems’ knowl-
edge base compared to current paradigms that only collect knowl-
edge about systems’ interactions. An extended knowledge base
improves systems’ interpretability and adaptability as more in-
formation is available to explain systems’ behaviour and inform
optimisation processes. Below, we introduce the knowledge sources
and discuss the associated open research challenges.
2.1.1
Systems Requirements and Design Artefacts. The first knowl-
edge source S4 considers comprises the end-user requirements and
the systems’ design artefacts. Engineers build systems according
to the problems they solve. System designers carry out iterative
elicitation processes that translate users’ needs to requirements
representing the stakeholders’ expectations. Requirements drive
the design of artefacts like functional and technical architectures
and structural and behavioural models. These artefacts rule contin-
uous processes of software development, testing, deployment, and
maintenance [9]. Designers use modelling languages like Business
Process Modelling and Notation (BPMN), the Unified Modelling
Language (UML), or Systems Modeling Language (SysML) to rep-
resent end-user requirements and design artefacts. These models
encapsulate knowledge about the problem systems address (i.e., sys-
tems goals) and the expertise of designers in architecting systems
(i.e., designed solutions). S4 proposes to exploit this knowledge to
enhance systems traceability, reusability, and interpretability [1],
which improves the systems’ understanding at deployment and en-
ables adaptive decision processes that align with the systems’ goals.
However, this knowledge is hard to manage and update using cur-
rent software design approaches. Organisations accumulate design
models in architecture documents that are difficult to navigate [20]
and do not have a direct link with the deployed systems [11].
S4 uses Knowledge Graphs (KGs) to model systems’ requirements
and software design decisions (i.e., the designers’ expertise) in dy-
namic artefacts [20]. The idea is to create modular models linked to
the deployed systems through data. KGs are semantic knowledge
bases that describe the physical world [30]. The architecture of a
KG encompasses a schema layer and a data layer [38]. The schema
layer is the core of the knowledge graph that defines the concep-
tual meta-models based on ontologies [34]. This layer provides
the representation capabilities that current software design mod-
elling languages offer. Software design artefacts (e.g., the SysML
requirements diagram) conform to ontologies or meta-models that
define the modelling languages [19]. The data layer complements
the schema layer and stores facts about the modelled entity [38].
We propose to exploit this data layer to close the gap between static
software design artefacts and the dynamic nature of the evolving
environment. A system at deployment produces data all the time,
which can form the data layer of KG-based representations. Such a
process enables the evolution of the design artefacts that reflect the
system’s changes. However, the exact methods behind this process
are an open research challenge because it is hard to nurture KGs
with the data produced by deployed software systems. Deployed
systems usually follow architectures that hide data behind inter-
faces (e.g., microservices), which complicate access to it [3], and the
translation of raw data into knowledge representations (i.e., data
layer facts) is not straightforward. These challenges require further
research to enable KGs as tools for software systems modelling.
2.1.2
Deployed Systems. S4 considers the deployed systems to be
the second knowledge source. Current self-adaptive mechanisms
monitor the state of internal components and input data to make de-
cisions according to perceived changes [39]. However, data access
tends to be localised, limiting the scope of the potential adapta-
tion. Software systems could make better adaptive decision-making
if they consider a holistic view of the entire data model and its
states across multiple internal components. Modern software ar-
chitectures encapsulate systems’ functionalities into services that
communicate through well-defined interfaces (i.e., APIs) [33]. These
interfaces hide the systems’ data and cause a situation known as
‘The Data Dichotomy”: while high-quality data management re-
quires exposing systems’ data, services hide it [32]. The Data Di-
chotomy impedes access to the systems’ data and its posterior use
in monitoring, transparency, traceability, and interpretability tasks.
S4 leverages existing work on Data-Oriented Architecture (DOA)
to overcome these limitations [3]. DOA complements the current
paradigms for building and deploying systems. DOA considers
data as the common denominator between disparate system com-
ponents [15]. Services in DOA are distributed, autonomous, and
Self-sustaining Software Systems (S4):
Towards Improved Interpretability and Adaptation
SATrends 2024, April 14–20, 2024, Lisbon, Portugal
communicate with each other at the data level (i.e., data coupling)
using asynchronous messages [29, 37]. DOA enables systems to
achieve data availability, reusability, and monitoring [15, 29, 37].
S4 pushes forward data-orientation ideas towards systems that
make their data fully available and observable by design, facilitating
systems monitoring and adaptation [3]. For example, a DOA-based
system stores the states of the data that flows through it by design
because of the data coupling between its components. A software
agent or engineer can trace and analyse such records, identify when
the system’s data changes, and update the system’s components. Ex-
isting work shows that building systems following DOA principles
enables reasoning tasks at run-time (e.g., causality analysis [23]). S4
includes software systems designed following the DOA principles
in the knowledge loop (Figure 1). DOA advocates for data-coupling
between components to enable interpretability and better-informed
self-adaptative decisions. Data coupling creates shared data models
between systems’ components, which store the whole systems’ data
and states, including internal and external interactions. Shared data
models are interfaces with external entities like engineers or soft-
ware agents that can monitor the system’s status. This integration
requires S4 to explore research areas for defining effective interfaces
between the knowledge loop components and addressing data secu-
rity and privacy concerns in DOAs. We propose to explore research
efforts on Knowledge Graphs completion and construction [14, 25]
to define effective interfaces between DOA-based systems and their
representations and to include security mechanisms like homo-
morphic encryption [10] in DOA open and decentralised setups to
address security and privacy challenges [3].
2.1.3
Software Engineering Community. Software engineering (SE)
is a discipline that includes principles for documenting, developing,
testing, deploying, and maintaining software systems [13]. The SE
community has built a wealth of knowledge around all these pro-
cesses. This knowledge is a valuable source of software architectural
paradigms, design patterns, common pitfalls, and best practices,
among other crucial information in understanding and maintaining
software systems. This knowledge has been historically available
in written documents (e.g., books, research papers, etc.) and online
(e.g., collaborative knowledge bases such as Stack Overflow, GitHub
or Reddit). Software engineers continuously use this knowledge
to implement modern software systems. Rapid advances in Large
Language Models (LLMs) open new opportunities to include this
domain knowledge across the software systems life cycle. S4 pro-
poses to develop domain-specific LLMs or leverage existing ones
to encode the knowledge produced by the SE community and the
domain knowledge of the problems engineers face and correspond-
ing solutions. These models serve as an interface for engineers,
software agents, and end-users to ask questions about the system
(e.g., adaptation requests, behaviour explanations, etc.).
The use of LLMs in the SE context is growing. The commu-
nity starts to understand the potential of this new technology but
also its limitations [21]. Recent surveys show that most research
efforts are focused on code generation, completion, testing, and
fixing because such tasks benefit from the LLM’s capability to gen-
erate code. Tasks like requirements analysis, systems improvement,
and human-computer interaction require more attention in the
future [8, 13]. S4 envisages a domain-specific LLM for each system,
Shared Data
Model
S1
Si
S3
S2
pub/sub
pub/sub
pub/sub
pub/sub
End
Users
pub/sub
System Designed as a
Knowledge Graph
Data-Oriented
System
SE and Domain-
specific LLM
Software
Engineer
Software Engineer
End Users
Questions
System2KG
Agent
KG2LLM
Agent
Adaptive
Agent
Figure 1: S4 diagram depicting the knowledge sources that
define software systems: Systems requirements and Design
Artefacts, the Deployed Systems, and the SE Community.
A knowledge loop integrates these sources through three
autonomous agents to accomplish self-sustaining behaviour.
which adapts the general SE knowledge to the problems specific to
this system. These domain-specific LLMs are an interface between
systems, stakeholders (e.g., users and engineers) and autonomous
agents. The stakeholders improve their understanding of a system
as they can ask questions and get responses about its structure and
behaviour in natural language. Autonomous agents can query the
LLM to get enriched information and make better informed adaptive
decisions. Building domain-specific LLMs presents significant chal-
lenges. Systems architectures, artefacts and their representations
have unique properties that distinguish them from natural lan-
guage, challenging the LLMs training [8]. LLMs generate different
responses to the same question because of their non-deterministic
nature. Such variability threatens robust, reliable, and stable sys-
tems’ interpretations and adaptive processes [8]. There is also a
risk of injecting bugs and replicating bad practices because of the
LLMs hallucinations [8, 13, 21]. The environmental cost of training
and deploying LLMs is a significant concern [21].
2.2
Self-sustaining Behaviour
S4 aims to create systems with self-sustaining properties that im-
prove their interpretability and adaptation capabilities. Different
parts of S4 improve systems interpretability. A KG model is more
flexible and easier to manage than traditional software design mod-
els. Stakeholders can use these models to understand the system
structure [11, 27]. The shared data model of a DOA-based system
stores the system’s states through time. Such raw data is available
but could be hard to read and understand. The domain-specific
LLM offers a mechanism to address this limitation. The LLM is
an interface for stakeholders to interact with the systems’ design
and data using natural language. S4 requires combining the knowl-
edge sources described above to enable such interfaces. S4 takes
inspiration from the multi-agent systems field [35] to integrate the
knowledge sources (Figure 1) and to enable adaptation capabilities.
We now describe the S4 autonomous agents.
The System2KG Agent connects the deployed system with its
KG representation. This agent updates the KG system model with
the system state by translating the data exposed in the shared data
SATrends 2024, April 14–20, 2024, Lisbon, Portugal
Cabrera, et al.
model to facts in the KG data layer. Traditional KGs provide static
snapshots of knowledge structure, ignoring the evolving nature
of knowledge. Evolving KGs expand their knowledge over time
with continuously generated new facts [18]. This task is challeng-
ing and expensive [5]. The System2KG Agent builds on current
research around evolving KGs by including the temporal dimension
in the models’ representations to perform tasks such as knowledge
graph completion, entity discovery, and relation extraction [14].
The KG2LLM Agent bridges the updated KG system model with
the domain-specific LLM. This agent injects structured information
about the system’s design and states into the LLM to improve re-
sponse accuracy. The LLM provides feedback about the system’s
structure in the KG representation and supports its evolution. The
KG2LLM Agent relies on current research exploring the enhance-
ment of LLMs using KGs, the augmentation of KGs based on LLMs,
and the synergy between the two. KGs enhance LLMs by includ-
ing structured information in the LLM pre-training, inference, or
interpretability stages. LLMs can augment KGs by constructing KG
models from scratch, completing existing KGs, extracting knowl-
edge from text, and supporting question-answering tasks. The syn-
ergic interaction between KGs and LLMs is the most promising,
challenging and least explored research area. It includes using KGs
and LLMs to build unified knowledge representations and applying
them to reasoning in different applications [25].
The Adaptive Agent is responsible for the self-analysis and self-
maintenance capabilities. This agent builds on the previous works
around autonomous software development and systems to make
adaptive decisions using the knowledge available from the com-
bined sources. The agent monitors and analyses the KG system’s
representation and the shared data model to identify changes in the
system’s goals, design, or behaviour. It uses internal models and
reasoning to make decisions that include the knowledge encoded in
the domain-specific LLM. Finally, the agent executes the adaptive
actions in the system and monitors their effect. The Adaptive Agent
builds on existing automatic methods in the software development
and deployment processes rather than considering the LLM an ora-
cle that makes the entire decision-making. For example, software
testing has a significant degree of automation, which S4 extends by
generating unit tests that improve testing coverage [21]. Genetic im-
provement and self-adaptive systems approaches improve systems’
performance regarding simple non-functional requirements (e.g.,
accuracy, response time, etc.) [2, 17]. S4 expands such capabilities
by including more knowledge in the decision-making process.
3
DISCUSSION
S4 proposes a new research agenda to address the limitations of
current paradigms for building interpretable adaptive systems ca-
pable of self-analysis and self-maintenance. The research agenda
includes developing new approaches for formalising and manag-
ing the knowledge sources that define modern software systems:
systems requirements and design artefacts modelled as KGs, the
deployed systems designed following DOA principles, and the SE
community knowledge encoded in LLMs. A knowledge loop com-
bines these sources to improve the systems’ interpretability and
adaptation. Such adaptive processes include the internal systems
structure (i.e., software source code), the system’s behaviour, and
their environments’ configuration. S4 improves interpretability
by offering natural language interfaces between stakeholders and
software systems based on the available knowledge.
The research agenda of S4 is ambitious and aims to transform
SE practices leveraging advances in semantic understanding of
software, data-oriented architectures, and foundational models.
Such transformations are challenging to carry out. For example,
designing software systems with evolving KG models is an open
research area. The main challenges emerge from the technical diffi-
culty of translating raw data into knowledge representations [5].
DOA-based systems also have challenges regarding the privacy and
security of data in the proposed open and decentralised setups [3].
LLMs are non-deterministic and prone to hallucination, which com-
plicates their adoption in tasks that require robust and reliable
responses (e.g., critical systems adaptation) [8, 13, 21]. The S4 re-
search agenda builds on existing research in related fields towards
enabling the required shifts. We believe interdisciplinary research
projects will be crucial to exploiting the expertise developed by
different research communities.
The success of the S4 research agenda depends on the software
engineering and systems community embracing the self-sustaining
concept. We aim to initiate a discussion on current autonomous
computing paradigms and potential alternatives through this po-
sition paper. Our future work includes developing prototypes to
evaluate the presented ideas and translating these prototypes to
real-world systems by working with industry partners and prac-
titioners in different domains. Such development and validation
processes will enable community awareness around the opportuni-
ties, strengths, and limitations of the S4 research agenda.
4
CONCLUSIONS
This position paper introduces the idea of Self-Sustaining Software
Systems (S4) as an alternative paradigm to design and deploy inter-
pretable autonomous systems. S4 integrates the knowledge sources
that define modern software systems: the systems requirements
and design artefacts, the deployed system, and the SE community.
A set of agents integrates these knowledge sources in a knowledge
loop to improve the system’s self-awareness, allow for automated
maintenance, and allow stakeholders to interact with the system
using natural language. We highlight the open research challenges
associated with the S4 idea and discuss the threats towards its vi-
sion realisation. Our future work will focus on developing the S4
idea with incremental prototypes and evaluating them in different
real-world domains.
REFERENCES
[1] Audrey Berquand and Annalisa Riccardi. 2020. From engineering models to
knowledge graph: delivering new insights into models. In ""9th International
Systems & Concurrent Engineering for Space Applications Conference (SECESA
2020)"".
[2] Christian Cabrera and Siobhán Clarke. 2022. A Self-Adaptive Service Discovery
Model for Smart Cities. IEEE Transactions on Services Computing 15, 1 (2022),
386–399. https://doi.org/10.1109/TSC.2019.2944356
[3] Christian Cabrera, Andrei Paleyes, Pierre Thodoroff, and Neil D Lawrence. 2023.
Real-world Machine Learning Systems: A survey from a Data-Oriented Architec-
ture Perspective. arXiv preprint arXiv:2302.04810 (2023).
[4] Matteo Camilli, Raffaela Mirandola, and Patrizia Scandurra. 2023. Enforcing
Resilience in Cyber-Physical Systems via Equilibrium Verification at Runtime.
ACM Trans. Auton. Adapt. Syst. 18, 3 (sep 2023), 32 pages.
https://doi.org/10.
1145/3584364
Self-sustaining Software Systems (S4):
Towards Improved Interpretability and Adaptation
SATrends 2024, April 14–20, 2024, Lisbon, Portugal
[5] Sutanay Choudhury, Khushbu Agarwal, Sumit Purohit, Baichuan Zhang, Meg
Pirrung, Will Smith, and Mathew Thomas. 2017. NOUS: Construction and Query-
ing of Dynamic Knowledge Graphs. In 2017 IEEE 33rd International Conference
on Data Engineering (ICDE). 1563–1565. https://doi.org/10.1109/ICDE.2017.228
[6] Jennifer Cobbe, Michael Veale, and Jatinder Singh. 2023. Understanding Ac-
countability in Algorithmic Supply Chains. In Proceedings of the 2023 ACM Con-
ference on Fairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT
’23). Association for Computing Machinery, New York, NY, USA, 1186–1197.
https://doi.org/10.1145/3593013.3594073
[7] Agoston E Eiben and James E Smith. 2015. Introduction to evolutionary computing.
Springer.
[8] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta,
Shin Yoo, and Jie M. Zhang. 2023. Large Language Models for Software Engi-
neering: Survey and Open Problems. arXiv preprint arXiv:2310.03533 (2023).
[9] Brian Fitzgerald and Klaas-Jan Stol. 2017. Continuous software engineering:
A roadmap and agenda. Journal of Systems and Software 123 (2017), 176–189.
https://doi.org/10.1016/j.jss.2015.06.063
[10] Caroline Fontaine and Fabien Galand. 2007. A survey of homomorphic encryption
for nonspecialists. EURASIP Journal on Information Security 2007 (2007), 1–10.
[11] Chao Fu, Jihong Liu, Longxi Zhang, Yinxuan Mao, and Jie Jin. 2021. Knowledge
graph based System model configuration design. Journal of Physics: Conference
Series 2029, 1 (sep 2021), 012108. https://doi.org/10.1088/1742-6596/2029/1/012108
[12] Yu Gan, Yanqi Zhang, Dailun Cheng, Ankitha Shetty, Priyal Rathi, Nayan Katarki,
Ariana Bruno, Justin Hu, Brian Ritchken, Brendon Jackson, Kelvin Hu, Meghna
Pancholi, Yuan He, Brett Clancy, Chris Colen, Fukang Wen, Catherine Leung,
Siyuan Wang, Leon Zaruvinsky, Mateo Espinosa, Rick Lin, Zhongling Liu, Jake
Padilla, and Christina Delimitrou. 2019. An Open-Source Benchmark Suite for
Microservices and Their Hardware-Software Implications for Cloud & Edge Sys-
tems. In Proceedings of the Twenty-Fourth International Conference on Architectural
Support for Programming Languages and Operating Systems (Providence, RI, USA)
(ASPLOS ’19). Association for Computing Machinery, New York, NY, USA, 3–18.
https://doi.org/10.1145/3297858.3304013
[13] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu
Luo, David Lo, John Grundy, and Haoyu Wang. 2023. Large Language Mod-
els for Software Engineering: A Systematic Literature Review. arXiv preprint
arXiv:2308.10620 (2023).
[14] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. 2022.
A Survey on Knowledge Graphs: Representation, Acquisition, and Applications.
IEEE Transactions on Neural Networks and Learning Systems 33, 2 (2022), 494–514.
https://doi.org/10.1109/TNNLS.2021.3070843
[15] Rajive Joshi. 2007. Data-oriented architecture: A loosely-coupled real-time SOA.
whitepaper, Aug (2007).
[16] Philippe Lalanda, Julie A McCann, and Ada Diaconescu. 2013. Autonomic comput-
ing: principles, design and implementation. Springer Science & Business Media.
[17] William B. Langdon and Mark Harman. 2015. Optimizing Existing Software
With Genetic Programming. IEEE Transactions on Evolutionary Computation 19,
1 (2015), 118–135. https://doi.org/10.1109/TEVC.2013.2281544
[18] Jiaqi Liu, Qin Zhang, Luoyi Fu, Xinbing Wang, and Songwu Lu. 2019. Evolv-
ing Knowledge Graphs. In IEEE INFOCOM 2019 - IEEE Conference on Computer
Communications. 2260–2268. https://doi.org/10.1109/INFOCOM.2019.8737547
[19] Chase McCoy. [n. d.]. Ontology Action Team - MBSE Wiki. https://www.omgwiki.
org/MBSE/doku.php?id=mbse:ontology
[20] Chase McCoy. 2021. Design Systems as Knowledge Graph. https://chasem.co/
2021/08/systems-as-knowledge-graphs
[21] Ipek Ozkaya. 2023. Application of Large Language Models to Software Engineer-
ing Tasks: Opportunities, Risks, and Implications. IEEE Software 40, 3 (2023), 4–8.
https://doi.org/10.1109/MS.2023.3248401
[22] Claus Pahl. 2023. Research challenges for machine learning-constructed software.
Service Oriented Computing and Applications 17, 1 (2023), 1–4.
[23] Andrei Paleyes, Siyuan Guo, Bernhard Scholkopf, and Neil D. Lawrence. 2023.
Dataflow graphs as complete causal graphs. In 2023 IEEE/ACM 2nd International
Conference on AI Engineering – Software Engineering for AI (CAIN). 7–12. https:
//doi.org/10.1109/CAIN58948.2023.00010
[24] Andrei Paleyes, Raoul-Gabriel Urma, and Neil D. Lawrence. 2022. Challenges
in Deploying Machine Learning: A Survey of Case Studies. ACM Comput. Surv.
(apr 2022). https://doi.org/10.1145/3533378
[25] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.
2023. Unifying Large Language Models and Knowledge Graphs: A Roadmap.
arXiv preprint arXiv:2306.08302 (2023).
[26] Justyna Petke, Saemundur O. Haraldsson, Mark Harman, William B. Langdon,
David R. White, and John R. Woodward. 2018. Genetic Improvement of Software:
A Comprehensive Survey. IEEE Transactions on Evolutionary Computation 22, 3
(2018), 415–432. https://doi.org/10.1109/TEVC.2017.2693219
[27] Satrio Adi Rukmono and Michel R.V. Chaudron. 2023. Enabling Analysis and
Reasoning on Software Systems through Knowledge Graph Representation. In
2023 IEEE/ACM 20th International Conference on Mining Software Repositories
(MSR). 120–124. https://doi.org/10.1109/MSR59073.2023.00029
[28] Ina Schieferdecker. 2020. Responsible Software Engineering. Springer International
Publishing, Cham, 137–146. https://doi.org/10.1007/978-3-030-29509-7_11
[29] Robert Schuler, Carl Kesselman, and Karl Czajkowski. 2015. Data Centric Dis-
covery with a Data-Oriented Architecture. In Proceedings of the 1st Workshop on
The Science of Cyberinfrastructure: Research, Experience, Applications and Models
(Portland, Oregon, USA) (SCREAM ’15). Association for Computing Machinery,
New York, NY, USA, 37–44. https://doi.org/10.1145/2753524.2753532
[30] Amit Sheth, Swati Padhee, and Amelie Gyrard. 2019. Knowledge Graphs and
Knowledge Networks: The Story in Brief. IEEE Internet Computing 23, 4 (2019),
67–75. https://doi.org/10.1109/MIC.2019.2928449
[31] Jatinder Singh, Jennifer Cobbe, and Chris Norval. 2019. Decision Provenance:
Harnessing Data Flow for Accountable Systems. IEEE Access 7 (2019), 6562–6574.
https://doi.org/10.1109/ACCESS.2018.2887201
[32] Ben Stopford. 2016. The Data Dichotomy: Rethinking the Way We Treat Data and
Services. Available at https://www.confluent.io/blog/data-dichotomy-rethinking-
the-way-we-treat-data-and-services/.
[33] Davide Taibi, Valentina Lenarduzzi, and Claus Pahl. 2018. Architectural patterns
for microservices: A systematic mapping study. In CLOSER 2018 - Proceedings
of the 8th International Conference on Cloud Computing and Services Science.
SCITEPRESS, 221–232. https://doi.org/10.5220/0006798302210232 International
Conference on Cloud Computing and Services Science ; Conference date: 19-03-
2018 Through 21-03-2018.
[34] Steve Tueno, Régine Laleau, Amel Mammar, and Marc Frappier. 2017. Towards
Using Ontologies for Domain Modeling within the SysML/KAOS Approach. In
2017 IEEE 25th International Requirements Engineering Conference Workshops
(REW). 1–5. https://doi.org/10.1109/REW.2017.22
[35] Adelinde M Uhrmacher and Danny Weyns. 2009. Multi-Agent systems: Simulation
and applications. CRC press.
[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),
Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/
2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
[37] Christian Vorhemus and Erich Schikuta. 2017. A data-oriented architecture
for loosely coupled real-time information systems. In Proceedings of the 19th
International Conference on Information Integration and Web-based Applications &
Services. 472–481.
[38] Lu Wang, Chenhan Sun, Chongyang Zhang, Weikun Nie, and Kaiyuan Huang.
2023. Application of knowledge graph in software engineering field: A systematic
literature review. Information and Software Technology 164 (2023), 107327. https:
//doi.org/10.1016/j.infsof.2023.107327
[39] Danny Weyns. 2019. Software engineering of self-adaptive systems. Handbook
of software engineering (2019), 399–443.
[40] Terence Wong, Markus Wagner, and Christoph Treude. 2022. Self-adaptive sys-
tems: A systematic literature review across categories and domains. Information
and Software Technology 148 (2022), 106934. https://doi.org/10.1016/j.infsof.2022.
106934
",S4 integrates knowledge sources that expand the systems’ knowledge base compared to current paradigms that only collect knowledge about systems’ interactions. An extended knowledge base improves systems’ interpretability and adaptability as more information is available to explain systems’ behaviour and inform optimisation processes.nan
"With the increasing sophistication of social media platforms, a need arose to protect them by detecting and filtering out malicious content. This has been facilitated by employing large-scale deep neural language models for sentiment analysis and content understanding tasks. However, these models, like BERT, are complex, computationally expensive to operate and maintain, and are also prone to large size-induced latency. To overcome these drawbacks, industry experts commonly use a knowledge distillation compression technique which allows them to train distilled models that endeavor to replicate the classification behavior of the original model. This distillation process usually concludes when the distillation loss function reaches a pre-defined stopping criteria. While this function is primarily concerned with ensuring classification accuracy, there are additional properties of the original model that the distilled model should also possess to justify it as an appropriate abstraction of the original model. Here, we investigate whether distilled TinyBERT models conserve the confidence values of the original BERT models. We also explore how this confidence preservation property can dictate tuning hyperparameters of the distillation process.","Deep neural language models such as BERT, ELMo, GPT-3 play pivotal roles in screening social media for fake and harmful content. Attaining high precision in classifying information often comes at the cost of model size, leading to larger models with increased inference latency and deployment issues on mobile devices and embedded systems. The traditional approach taken by the verification community in response has been to reduce large systems while preserving desirable properties through abstraction. This paper explores the possibility of distilled models preserving pairwise confidence property, and if this type of abstraction can serve as an implicit abstraction of a large BERT model.","We introduce a pairwise confidence difference to uncover the confidence-related distillation disagreements between the teacher and the student models. The main goal is to measure the confidence preservation on the same input example for two models B and S. The notion of functional equivalence is commonly used for verification of traditional programs. Thus, we adapt a similar idea of “input-output” equivalence to define the property of confidence preservation for the case of deep neural networks. To measure this property, we introduce a pairwise confidence difference for ∀x ∈ Xtrain as follows:

∆Cnf(x) = 

				( 
CnfS(x) − CnfB(x)
				if IdxB(x) = IdxS(x)
				⊥
				otherwise


Since ∆Cnf can have both positive and negative values, we use the sum of squares to compute the effect of these differences. Thus, we propose the following formula to compute the confidence difference on a training set Xtrain

σ(Xtrain) = 

q
1
|Xtrain|

P
x∈Xtrain ∆2
Cnf(x)


Definition 1. Pairwise Confidence Preservation Property ϕcnf
We say that the confidence preservation property ϕcnf holds if for any input x ∈ Xtrain the spread of the pairwise confidence differences σ between distilled S and original B models is small on Xtrain:

σ(Xtrain) ≤ κ
(4)
where κ > 0 is a small constant, which is determined by users.","Based on our research we found that: 

* Knowledge distillation does not uniformly preserve ϕcnf across all six linguistic tasks.
* S6L model satisfies ϕcnf for three language tasks but fails to do so for the three other tasks.
* We are able to fine-tune all three models to satisfy ϕcnf property without the changes of the distillation architecture and training sets while avoiding significant drop in accuracy.","This paper's main contribution is the exploration of whether distilled TinyBERT models preserve confidence values of the original BERT models, and how this confidence preservation property could guide tuning hyperparameters of the distillation process. The results show that the distilled TinyBERT model maintains the confidence property of the original BERT model in three tasks. For the remaining three tasks, whose models fail to preserve the property, we modify the training hyperparameters so that these models maintain the confidence property without degrading the original accuracy.",Confidence Preservation Property in Knowledge Distillation Abstractions,"Dmitry Vengertsev, Elena Sherman","Confidence Preservation Property in Knowledge
Distillation Abstractions
Dmitry Vengertsev1[0000−0002−6039−0579] and Elena
Sherman1[0000−0003−4522−9725]
Department of Computer Science, Boise State University, Boise, ID 83725, USA
dmitryvengertsev@u.boisestate.edu, elenasherman@boisestate.edu
Abstract. Social media platforms prevent malicious activities by de-
tecting harmful content of posts and comments. To that end, they em-
ploy large-scale deep neural network language models for sentiment anal-
ysis and content understanding. Some models, like BERT, are complex,
and have numerous parameters, which makes them expensive to operate
and maintain. To overcome these deficiencies, industry experts employ a
knowledge distillation compression technique, where a distilled model is
trained to reproduce the classification behavior of the original model.
The distillation processes terminates when the distillation loss function
reaches the stopping criteria. This function is mainly designed to en-
sure that the original and the distilled models exhibit alike classification
behaviors. However, besides classification accuracy, there are additional
properties of the original model that the distilled model should preserve
to be considered as an appropriate abstraction.
In this work, we explore whether distilled TinyBERT models preserve
confidence values of the original BERT models, and investigate how this
confidence preservation property could guide tuning hyperparameters of
the distillation process.
Keywords: Machine Learning Confidence · Knowledge Distillation ·
Model Property Preservation Model Abstraction.
1
Introduction
Deep neural language models such as BERT [5], ELMo [29], GPT-3 [1] play a
crucial role in screening social media for fake and harmful content [13]. To reliably
classify information, those models require high precision, which often comes at
the cost of increased the model’s size [19]. However, larger size models have high
inference latency and are problematic to deploy on mobile devices and embedded
systems. A traditional approach by verification community is to reduce large
systems while preserving desirable properties through abstraction [25]. There
are several examples of constructing explicit abstraction: conversion of neural
network to Boolean combinations of linear arithmetic constraints [30], empirical
extraction of a deterministic finite automaton using clustering of the hidden
states of deep neural networks [37], and leveraged pruning during verification
arXiv:2401.11365v1  [cs.CL]  21 Jan 2024
D. Vengertsev et al.
process [11]. An orthogonal approach to creating smaller models through the
knowledge transfer (training) from a larger model without a significant drop in
accuracy is known as knowledge distillation. However, the majority of current
distillation techniques focuses on the classification correctness while ignoring
other properties of the teacher model.
In this work, we investigate whether the knowledge distillation [15] used
in TinyBERT behaves as an abstraction of a large BERT model and preserves
pairwise confidence property. We call this type of abstraction implicit since unlike
the previously mentioned clustering and pruning abstraction techniques, it has
no direct mappings between BERT and TinyBERT internal architectures.
Our proposed approach determines whether a distilled model preserves prop-
erties of the original model through black-box equivalence checking [4]. In partic-
ular, we investigate preservation of the confidence property, which is important
for high-stake real-world applications such as AI-aided medicine and pedestrian
detection. In the first application, the control should be given to a doctor when
the confidence of the diagnostics model is low [7], and in the second application
when pedestrian detection network is not able to confidently predict the presence
or absence of immediate obstructions, the car should rely more on the output of
other sensors for braking.
In this paper, we outline the concept of confidence preservation property and
establish a method for assessing it. To uncover the confidence-related distilla-
tion disagreements between the teacher and the student models, we introduce a
pairwise confidence difference, Fig. 2. We evaluate the preservation of this prop-
erty on six tasks from the General Language Understanding Evaluation (GLUE)
dataset. Our results show that the distilled TinyBERT model maintains the con-
fidence property of the original BERT model in three tasks. For the remaining
three tasks, whose models fail to preserve the property, we modify the training
hyperparameters so that these models maintain the confidence property without
degrading the original accuracy.
Overall, this work has the following contributions: (1) Considering knowl-
edge distillation as an implicit abstraction with anticipated property preserva-
tion characteristics. (2) A confidence property preservation criterion based on
pair-wise confidence measurements and its empirical evaluations. (3) Identifying
and empirically tuning hyperparameters in the distillation process to ensure the
confidence property preservation.
2
Background and Motivation
2.1
Significance of Knowledge Distillation Models
Recent advancements in the area of machine learning are driven by transformer-
based models. Unfortunately, the size and the efficiency of these models prohibit
their use in resource-constrained and time-sensitive applications. For example,
a baseline transformer model executes a single query in a few tenths of seconds,
which is slow for real time systems. To reduce desirable response latency to
Confidence Preservation Property in Knowledge Distillation Abstractions
Fig. 1. Learning abstract model via distillation. (a) end to end distillation flow; (b)
task specific distillation
milliseconds [21] compressed models that are obtained via knowledge distillation
technique [2,15] are used. The technique essentially trains a compact model,
referred to as the student, to reproduce the behavior of a larger model, known
as the teacher.
In this paper we focus on distillation of BERT (Bidirectional Encoder Rep-
resentations from Transformers) model [34], which has significantly advanced
the state-of-the-art in natural language processing tasks such as language un-
derstanding, text classification, and language translation. Previous work distills
BERT into smaller models such as single layer BiLSTM [33], DistilBERT [31],
and TinyBERT [18]. In this paper, we examine property preservation of Tiny-
BERT models since they outperform other distillation methods and also combine
both response and feature-based knowledge transfers. The reason TinyBERT
achieves superior accuracy is due to its pre-training feature and a specialized
loss function. At the pre-training stage, obtaining a good initialization is cru-
cial for the distillation of transformer-based models. Intermediate loss during
feature-based distillation provides a boost of classification accuracy of at least
10% for most of the GLUE tasks [18].
2.2
TinyBERT Distillation
First, we explain the TinyBERT distillation process using the diagrams shown
in Fig. 1, and then discuss the distillation loss function.
At the general distillation stage (Fig. 1.a), the original BERT model acts
as the teacher B. The student model GeneralTinyBERT mimics the teacher’s
behavior on general-domain corpus from Wikipedia by using the feature-based
D. Vengertsev et al.
distillation, rather than response-based distillation. General distillation helps
TinyBERT learn the rich knowledge embedded in pre-trained BERT, which plays
a major role in improving the generalization capability of the task-specific Tiny-
BERT.
The general distillation produces GeneralTinyBERT that is used as the ini-
tialization of the student model S for the task-specific distillation. This stage
first completes the data augmentation, and then performs distillation on the task-
specific augmented dataset. As shown in the Fig. 1.b, both the teacher B and
the student S models have the same type of the layers, however student model
has m = 1, .., M transformer layers and teacher model has n = 1, .., N > M
such layers. Embedding layers for the student and the teacher are m = 0 and
n = 0 correspondingly, and the prediction layers are labelled m = M + 1 and
n = N + 1.
An input text sequence x enters the embedding layer, and then transformer
layers. A single transformer layer includes the two main sub-layers: multi-head
attention (MHA) and fully connected feed-forward network (FFN). MHA is de-
scribed by attention matrices, and FFN is described using hidden states. The
output of the last transformer is passed to the prediction layer that outputs raw
predictions in the form of logits z. Logits go through the softmax layer for the
final task-specific classification.
There are several ways to map the N layers of teacher model to the M layers
of the student model. We use the uniform layer mapping strategy as it provides
a superior accuracy for the majority of the tasks [18]. This strategy covers the
knowledge transfer from bottom to top layers of BERT to the corresponding
layers of the student model. During the training of a student model, TinyBERT
uses four loss functions: embedding distillation loss for m = 0, attention and
hidden distillation losses for transformer layers 0 ≤ m ≤ M, and finally, the
distillation of the prediction layer. This work focuses on the distillation of the
prediction layer, rather than intermediate layers.
At the prediction layer the loss is defined in Eq. 1, where the student cross
entropy loss LCE is supplemented with the distillation objective Ldist in order
to penalize the loss between the student network’s logits against the teacher’s
logits, which determines distillation training objectives:
L = α · LCE + (1 − α) · Ldistill
= −α
X
i
ti log y(S)
i
+ (1 − α)
X
x∈Xtrain
∥z(B(x)) − z(S(x))∥2
(1)
where z(B) and z(S) are the logits of the teacher B and the student S, corre-
spondingly. The variable t is the one-hot target ground truth label, α ∈ [0, 1] is
a constant, i is the index of the training examples from the augmented dataset.
2.3
Distillation as Implicit Abstraction
Abstraction is a powerful technique that is used extensively in software verifi-
cation. The key to an abstraction is to establish a relationship between a con-
Confidence Preservation Property in Knowledge Distillation Abstractions
crete system P and its model M that hides unnecessary details about P while
preserving some essential P’s properties [26,3,25]. Because the relationship is
established, we refer to this type of abstraction as an explicit abstraction.
Previous research performs such explicit abstraction on a deep neural net-
work using a clustering approach [37], where a set of nodes in the neural network
relates to a single node in the abstracted model. In another approach, researchers
eliminate less relevant nodes in a neural network using pruning process [11]. All
these techniques indeed reduce the size of the original model to make it more
amenable, for example, for verification. However, the drawback of these tech-
niques is that the accuracy of the resulting abstraction suffers, i.e., they fail to
preserve the classification accuracy of the original neural network. This is because
they do not consider accuracy preservation in their abstraction process. Clearly,
considering the size of neural networks, identifying such explicit abstraction is a
daunting task.
On the contrary, the distillation process ensures that the student model has
similar accuracy as the teacher model. Since, by construction, the distilled model
is smaller in terms of layers and dimensions, we can essentially consider it as an
abstraction of the original model. However, since the student model might have
a completely different architecture than its teacher, establishing an explicit map-
ping between those two systems is challenging. Instead, we say that these models
have an implicit abstraction, which is created based on correctness preservation.
Thus, the question is whether this implicit abstraction preserves properties be-
tween two models. One of the properties that we investigate here is confidence,
which we discuss in the next section.
3
Confidence Property Preservation Criterion
In the real-world classification problems, in addition to a model’s accuracy, the
level of confidence with which a model performs classification, (i.e., predictive
probability) is also considered. Confidence value can be used to determine the
model’s properties, such as high-confidence and decisiveness [35]. Therefore, pre-
serving these values of the original model should be an important feature of the
distilled abstraction.
The literature offers different measurements of the confidence for neural net-
works. Traditionally, it is defined as the maximum element of the probability
vector obtained at the final softmax layer of a model M for an input x, i.e.,
CnfM(x) = max(softmax(zM(x)))
(2)
Recent work has shown that using this definition might be inadequate [27]
and general purpose method based on Bayesian interpretation of the neural
networks [8], or confidence calibration [12] should be used. Beyond being un-
calibrated, the main criticisms of the softmax-based confidence evaluation is its
failure to decrease confidence value on inputs far from the training data [27,32].
Therefore, this work evaluates confidence preservation on the training data set
Xtrain, and we monitor ECE of both student and teacher models, as well as
accuracy and the pairwise confidence on the evaluation dataset.
D. Vengertsev et al.
Fig. 2. For two linguistic tasks: SST-2 and MRPC, the individual distributions of
softmax confidence for the teacher and the student do not show significant difference,
under comparable expected calibration error (ECE). However, the distribution of the
pairwise confidence does highlight the issue of poor distillation for the MRPC task.
3.1
Pairwise Confidence Preservation Property
In order to determine how to define and measure the confidence preservation
property, we first examine confidence values for both S and B models. The
confidence values Cnf(x) of a model M over Xtrain represent a distribution in
range [0, 1]. To better understand the data, we first plotted confidence values for
B and S models as density plots. Fig. 2 shows these distribution for SST-2 and
MRPC benchmarks: in the first top row for B and in the second row for S.
From these plots, we can see that the traditional metric of aggregated con-
fidence distributions of S and B models, as well as expected calibrated er-
ror (ECE) [12] are not good candidates for identifying differences in confidence.
Two plots might have points with the same confidence value for different inputs,
which is misleading. That is, such aggregate data loses information on whether
confidence is preserved on the same inputs. Likewise, the ECE values of the
models can be similar as in the case of MRPC, e.g., 2.4 vs. 2.0, but actually fail
to hold the confidence preservation property.
Thus, our main idea is to measure the confidence preservation on the same
input example for two models B and S. In other words, in this work we focus on
Confidence Preservation Property in Knowledge Distillation Abstractions
confidence preservation in the context of functional equivalence where for the
same input B and S should have similar confidence values. The notion of func-
tional equivalence is commonly used for verification of traditional programs [16].
Thus, we adapt a similar idea of “input-output” equivalence to define the prop-
erty of confidence preservation for the case of deep neural networks.
Therefore, to measure this property, we introduce a pairwise confidence dif-
ference to uncover the confidence-related distillation disagreements between B
and S models. The last row in Figure 2 depicts the pair-wise difference distri-
bution and their difference spread σ. We use σ’s value and its threshold of 0.05
as the property preservation criterion. As the data in the graphs show in this
example the SST-2 model satisfies the property preservation criterion, while the
MRPC fails do so since its σ > 0.05.
Here we formally describe the measurement of the differences between CnfB(x)
and CnfS(x). First, let’s define a function IdxM(x) that returns an index in the
probability vector of the maximum element return by Eq. 2. Then we define the
pairwise confidence difference for ∀x ∈ Xtrain as follows:
∆Cnf(x) =
(
CnfS(x) − CnfB(x)
if IdxB(x) = IdxS(x)
⊥
otherwise
(3)
Since ∆Cnf can have both positive and negative values, we use the sum of
squares to compute the effect of these differences. Thus, we propose the fol-
lowing formula to compute the confidence difference on a training set Xtrain
σ(Xtrain) =
q
1
|Xtrain|
P
x∈Xtrain ∆2
Cnf(x)
Definition 1. Pairwise Confidence Preservation Property φcnf We say
that the confidence preservation property φcnf holds if for any input x ∈ Xtrain
the spread of the pairwise confidence differences σ between distilled S and original
B models is small on Xtrain:
σ(Xtrain) ≤ κ
(4)
where κ > 0 is a small constant, which is determined by users.
3.2
Confidence Preservation Property φcnf Dependencies
Since a traditional distillation process does not take into account the confidence
property preservation as in Definition (4), the technique does not guarantee it to
hold for all models. In this section, we establish dependencies between σ(Xtrain)
and the distillation parameters that demonstrate the existence of parameters
that can be tuned for the property (4) to hold.
To determine such relations, we first define a distillation quality condition
on student loss L(S) defined in (1)
L(S) ≤ β
(5)
where β is a small positive scalar.
D. Vengertsev et al.
Next, from the softmax property [9] (Proposition 4) we obtain the following
inequality for any input x: ∥softmax(zS(x))−softmax(zB(x))∥ < γ∥zS(x) −zB(x)∥
, where γ is the inverse temperature constant in the softmax function.
Since the norm of a vector is always greater than or equal to any individual
value of the vector, we have:
∆Cnf(x) ≤ ∥softmax(zS(x)) − softmax(zB(x))∥
(6)
Therefore, from (6) we obtain:
X
x∈Xtrain
∆2
Cnf(x) < γ2
X
x∈Xtrain
∥zS(x) − zB(x)∥2
(7)
σ(Xtrain)2|Xtrain| =
X
x∈Xtrain
∆2
Cnf(x) < γ2
X
x∈Xtrain
∥zS(x)−zB(x)∥2 < γ2
β
(1 − α)
(8)
Ldist ≤
β
1 − α
(9)
Recall in Eq. 1 the student distillation objective L(S) consists of two pos-
itive addends: student’s cross entropy loss LCE and the distillation objective
Ldist. If αLCE + (1 − α)Ldist ≤ β, then Ldist ≤
β
1−α holds because the value
of LCE is positive as the logarithm of a value that is between zero and one
is negative. Therefore, from the distillation quality condition (5) we obtain:
P
x∈Xtrain∥z(S(x)) − z(B(x))∥2 <
β
(1−α)
Finally, using Eq. 7 we produce: σ(Xtrain) < γ
q
β
|Xtrain|(1−α) , which means
(4) is satisfied with κ = γ
q
β
|Xtrain|(1−α). This means that the confidence preser-
vation property depends on several parameters.
However, some of those parameters are predefined for TinyBERT models.
Thus, in the TinyBERT model a regular softmax function is used, which sets the
inverse temperature constant γ = 1. Moreover, due to the multi-stage approach
in TinyBERT, during the prediction layer distillation only Ldistill is used, so
α = 0. As a result, the confidence property preservation mainly depends on the
square root of the distillation quality condition β and the training set size. That
is
σ(Xtrain) <
s
β
|Xtrain|
(10)
We assume that the training set is fixed, due to the cost of the data collection,
and the main focus is on β that depends on the distillation training hyperpa-
rameters. We focus on empirical search and fine-tuning to obtain S models for
which φcnf holds.
Confidence Preservation Property in Knowledge Distillation Abstractions
4
Experiment Setup
To investigate preservation φcnf as defined in Eq. 4 for TinyBERT models and
answer our research questions, we use the training benchmarks for five language
tasks with different TinyBERT settings. In this section, we describe datasets,
knowledge distillation model settings, and the choice of the threshold parameter
κ. In the next section, we state our two research questions and present evaluation
results that answer them.
4.1
GLUE Tasks Benchmarks
The datasets used for training and evaluation during the knowledge distilla-
tion consist of the six benchmarks from the General Language Understanding
Evaluation (GLUE) [36]
The selected benchmark tasks are sentiment similarity and the natural lan-
guage inference for binary classification tasks. Table 1 summarizes each dataset/task,
with the information on both training (Tr) and evaluation (Ev) dataset sizes. For
the smaller datasets such as CoLA, RTE, and MRPC we perform data augmen-
tation. Augmentation is only performed for training datasets, we still perform
evaluation on non-augmented data.
For the data augmentation we use the same approach as in work by Jiao et.
al [18] relying on GLOVE dataset [28]. Specifically, we adopt glove.6B.300d
that maps text sequences to a 300 dimensional dense vector space. As a result
of data augmentation the sizes of the small training datasets - CoLA, RTE
and MRPC - have increased to 211,457, 189,277 and 224,132, respectively; the
original sizes of these datasets are shown in the parentheses in the table.
Table 1. Benchmark datasets GLUE [36]
Task
Description
Tr
Ev
SST-2 Sentiment analysis
67,349
872
RTE
Natural language inference 189,277 (2,490)
277
QQP
Paraphrase similarity
363,846
40,430
QNLI
Natural language inference 104,743
5,463
MRPC Semantic textual similarity 224,132 (3,668)
408
CoLA Semantic correctness
211,457 (8,551)
1,043
4.2
Model Settings
We use a task-specific fine-tuned BERT model B with N = 12 transformer layers.
Specifically, we use an uncased BERT version, meaning the input text has been
changed to lowercase at the input layer. The model B has the dimension of the
hidden layer d
′=768, with the total number of model parameters at 109 million.
D. Vengertsev et al.
For the students, we use two models S4L and S6L. S4L is a smaller distilled
model with only four transformer layers M=4 and relatively small dimension of
a hidden layer d
′=312. In total S4L has 14.5M model parameters. For S6L M=6,
d
′=768, resulting in 67M model parameters.
For general distillation, we use the model by the authors of TinyBERT [18],
which is distilled from BERT using domain agnostic corpus from English Wikipedia
(2,500M words), sequence length of 128 and feature-based distillation. We do not
alter this general distillation model in this work, since the focus is on the task-
specific models and prediction-layer distillation.
We perform task-specific distillation for the above six tasks, which produces
twelve TinyBERT models. We initialize the student model with the parameters
of GeneralTinyBERT, and for the teachers we use BERT that is fine-tuned for
the corresponding task. We use the input sequence length of 128 for task specific
distillation. As for the learning parameters, we use learning rate of 3e−5, batch
size of 32, the number of epochs for intermediate distillation (embedding layer,
attention matrices and hidden layer) is 10 and the number of epochs for pre-
diction layer distillation is 3. The resulting accuracy numbers of the TinyBERT
that we distilled from the BERT are comparable to the ones presented in the
original paper [18], thus making our reproduction of TinyBERT valid.
We perform the knowledge distillation on GPU NVIDIA V100 computation
with 16 GB RAM running on top of Google Cloud Platform (GCP) service.
4.3
Parameters Selection
As described in Section 3, the confidence preservation property φcnf is parame-
terized by the threshold κ in Eq. (4). We select this threshold as κ=0.05 for our
adequacy criterion for φcnf to hold. The final condition we evaluate is:
σ(Xtrain) ≤ 0.05
That is, we say that confidence preservation holds if the value of σ on the
training set Xtrain is less or equal to the threshold 0.05.
To avoid negative effect of hyperparameter fine-tuning on S’s accuracy, we
add a constraint on changes in accuracy value of S that prevents significant
accuracy drop. We consider a drop of accuracy below 1% to be significant, as
according to a Jiao et al. [18] it corresponds to the loss of the “on-par” perfor-
mance. Thus, the accuracy of the fine-tuned ˜S and the original S cannot be less
than 1%.
5
Experimental Evaluations and Results
We conduct our empirical evaluations to answer the following research questions:
– RQ1: Confidence preservation prevalence. Do the distilled models S
from B preserve φcnf property?
– RQ2: Confidence preservation dependencies. Can tuning the distilla-
tion hyperparameters of a failed model S make the property φcnf hold for
its tuned model ˜S?
Confidence Preservation Property in Knowledge Distillation Abstractions
Table 2. Confidence Preservation Property φcnf for GLUE language tasks
Models
φcnf property
Task
Dataset
B
S6L
S4L
B vs S6L B vs S4L
Acc ECC Acc ECC Acc ECC
σ(X)
σ(X)
SST-2
Tr
98.4 3.6 98.0 3.6 97.0 3.5
0.026
0.055
Ev
93.0 1.6 91.4 1.3 89.4 2.9
0.046
0.075
RTE
Tr
94.8 22.1 84.3 14.9 86.5
21
0.062
0.109
Ev
65.3 9.0 66.8 9.9 60.6 2.5
0.100
0.107
QQP
Tr
96.9 7.4 95.2 7.5 92.2 6.0
0.049
0.077
Ev
90.7 2.7 90.9 4.2 89.1 3.5
0.055
0.079
QNLI
Tr
97.6 9.3 96.0 9.3 91.3 4.8
0.049
0.065
Ev
91.4 4.8 91.1 5.8 85.8 1.8
0.079
0.094
MRPC
Tr
81.2 2.4 80.5 2.0 79.9 3.7
0.066
0.054
Ev
78.7 7.5 75.5 2.3 74.2 7.2
0.070
0.075
CoLA
Tr
98.5 5.3 96.6 5.7 94.7 6.4
0.059
0.083
Ev
83.2 7.6 79.6 7.4 72.6 11.5
0.098
0.129
5.1
Confidence Preservation Prevalence (RQ1)
In this section, we evaluate whether φcnf holds for the six language tasks. In
the Table-2, for each task and the dataset we have two multi-column headers
“Models” and “φcnf property”. The former describes individual model perfor-
mance metrics such as accuracy (Acc) and expected calibration error (ECE) for
three models B, S4L, and S6L. The φcnf property columns contain data per-
taining to evaluations of φcnf. Each row corresponds to a task dataset evaluated
on the training (Tr) and evaluation (Ev) sets. As we discussed in the beginning
of Section 3, we examine φcnf on Tr dataset, due to the need to remove the
factor that can affect confidence correctness on the inputs that are far from the
training dataset. We do present results for Ev dataset as well to demonstrate
the consistency of our experiments, i.e., if φcnf does not hold on Tr, then we
expect it to perform the same way on Ev, as well as to observe the same trend
in the confidence value change for ˜S models.
The first observation from the table is that for all language tasks, none of S4L
models maintain φcnf. As a result, we conjecture that distilling BERT to just
four transformer layers with the smaller dimension of hidden states (resulting in
a 7.5X reduction in parameters) is too aggressive to satisfy φcnf.
The second and more important result is that the S6L model satisfies φcnf
for the three language tasks (shaded in light gray ), however it fails to do so for
the three other tasks (shaded in dark gray ).
The answer to RQ1 is that the knowledge distillation does not uniformly
preserve φcnf across all six linguistic tasks. This conclusion supports our intu-
ition that a standard distillation process focuses on classification accuracy and
does not take into account confidence of predicted classes. Therefore, in the next
research question RQ2, we investigate how parameters tuning can help with the
tasks for which φcnf fails.
D. Vengertsev et al.
Table 3. Improving φcnf property
Models
φcnf property
Task
Dataset
B
S6L ˜S6L S4L ˜S4L B vs S6L B vs ˜S6L B vs S4L B vs ˜S4L
Acc Acc Acc Acc Acc
σ(X)
σ(X)
σ(X)
σ(X)
RTE
Tr
94.8 84.3 84.6 86.5 88.5
0.062
0.050
0.109
0.069
Ev
65.3 66.8 66.4 60.6 62.5
0.100
0.084
0.107
0.119
MRPC
Tr
81.2 80.5 80.2 79.9 79.4
0.066
0.047
0.054
0.053
Ev
78.7 75.5 75.9 74.2 73.7
0.070
0.060
0.075
0.074
CoLA
Tr
98.5 96.6 98.1 94.7 95.8
0.059
0.039
0.083
0.071
Ev
83.2 79.6 80.8 72.6 72.1
0.098
0.093
0.129
0.120
5.2
Confidence Preservation Dependencies (RQ2)
We considered several hyperparameters that can improve φcnf adequacy for
failed models S. In particular, we investigated the tasks that have inadequate
values of σ(Xtrain) that fail φcnf - RTE, MRPC, and CoLA with the original
recommended TinyBERT parameters: batch size 32, three epochs, weight decay
1e−4, intermediate layers distillation learning rate 5e−5 and prediction layer
distillation learning rate 3e−5. We performed several exploratory studies where
we changed the learning parameters of distillation.
This preliminary investigation show that changing parameters at interme-
diate layers distillation yields no reductions in σ(Xtrain) values. To no avail,
we varied epochs {3, 6, 9}, learning rate {5e−7, 1e−6, 5e−5, 1e−4, 3e−4, 5e−3},
batch size {28, 32, 36}, and weight decay {1e−4, 1e−3, 1e−2}.
We experimented with the hyperparameters at prediction layers distillation
in the following ranges: epochs {2, 3, 4, 5, 6}, learning rate {3e−6, 1e−5, 3e−5,
7e−5, 4e−4, 5e−4, 8e−4}, batch size {28, 32, 34, 36, 38, 40}, and weight decay
{1e−4, 1e−3, 5e−3, 1e−2, 5e−2}. Changing only the parameters of the prediction
layers distillation reduced the values of σ(Xtrain) so that ˜S models for MRPC
and CoLA hold φcnf. However, in order for the RTE task to satisfy φcnf, it
requires the hyperparameter tuning for both prediction and intermediate layer
distillations.
The hyperparameters of the original knowledge distillation (lrstg1, lrstg2,
batchstg2, epochstg2, wdstg2) are equal to (5e−5, 3e−5, 32, 3, 1e−4), where “stg1”
and “stg2” correspond to intermediate and prediction layer distillation respec-
tively, “lr” and “wd” denote learning rate and weight decay. The resulting prop-
erty improvement is achieved using the fine-tuning with the following parameters
on RTE: (1e−4, 3e−5, 36, 4, 1e−4) for ˜S6L and (5e−5, 3e−5, 32, 4, 1e−4) for
˜S4L; MRPC: (5e−5, 1e−5, 32, 4, 1e−4) for ˜S6L and (5e−5, 3e−5, 32, 2, 1e−4) for
˜S4L and CoLA: (5e−5, 1e−5, 32, 4, 1e−4) for ˜S6L and (5e−5, 1e−5, 32, 4, 1e−4)
for ˜S4L.
Models ˜S6L were fine-tuned for all three tasks to satisfy φcnf property with-
out any significant drop in the accuracy. We can see that 6L RTE model required
Confidence Preservation Property in Knowledge Distillation Abstractions
the most changes to the original parameters, including the changes on the stage
1 of distillation. This can be attributed to the fact that it is the smallest training
dataset. The accuracy of ˜S6L and B on the Ev dataset are comparable, indicating
that the fine-tuning did not change the regime of the distillation significantly.
To answer RQ2, we are able to fine-tune all three models to satisfy φcnf
property without the changes of the distillation architecture and training sets
while avoiding significant drop in accuracy.
5.3
Discussion
Based on the information presented, we can conclude that φcnf is a non-trivial
property, and satisfying the distillation quality condition guarantees its preser-
vation.
The architecture of the distilled model, such as the number of transformer
layers as well as learning hyperparameters at the prediction layer greatly impact
φcnf, while an early intermediate distillation layer has a much lesser effect on
σ(Xtrain) values.
Monitoring φcnf property therefore guides distillation hyperparameters fine-
tuning, so that both accuracy and confidence are preserved. We note that this
does not require any architectural modifications to the existing distillation mod-
els.
6
Related Work
Black-box Equivalency Checking: Similar to our work, black-box equiva-
lence checking determines functional equivalence of two programs using concrete
inputs. This approach is used in verifying correctness of code compilation by es-
tablishing relations between the original and compiled programs [23,4,24]. After
running tests, the technique discovers possible relationship between variables
of two versions of the program. Next, using formal methods, it verifies that the
identified equivalence relation indeed holds for all program inputs. Our approach
only establishes the relationship between S and B confidence values, and does
not generalize it to all possible inputs to those models.
Abstraction for Large-scale Deep Models: The latest research in the
area of formal methods for AI does not perform the verification on the state of
the art large language models. There are two papers exploring relatively large
deep learning models with more than ten million parameters, where one relies on
formula-based approach [17] and another on the abstraction via state transition
system [6]. The main challenge to make formal methods scalable to the sizes
of realistic deep neural networks is the prohibitive size and complexity of a
formula in these approaches [20]. Implicit abstraction on the other hand does
not require exact formula and state transition representation, hence it is a natural
fit for verification of large models. Several papers addressed the verification of
deep neural networks by the development of abstractions. Idealized real-valued
abstraction was proposed in [14] to verify relatively small visual deep neural
D. Vengertsev et al.
networks. The idea is to quantize all the operations in the network to 32-bits,
and then feed into SMT solver. In the recent work [11], the authors present
verification of feed forward neural network using training pipeline based on the
pruning with the goal to make the model amenable to formal analysis. However,
the network under verification has only fully connected layers that were reduced
using pruning for network reduction and then processing by an SMT solver.
Distillation and Property Preservation: There is a substantial research
on measuring and calibrating confidence, including techniques such as temper-
ature scaling [12] and on/off manifold regularization [22]. However, to the best
of our knowledge, the work on confidence property analysis for the knowledge
distillation is limited. Other researchers also note that preserving properties dur-
ing distillation is important, specifically the robustness property preservation.
In [10], the method of the Adversarial Robust Distillation (ARD) was proposed
to mitigate the cases when the robustness of a distilled model suffers compared
to the robustness of the original model. However, the focus of our paper is the
confidence property. Therefore, we did not use robust variants of the distilla-
tion such as ARD or Defensive Distillation in order to fairly evaluate distilled
abstraction of the large-scale language models.
7
Conclusion and Future Work
In this work, we study the confidence preservation property under a knowledge
distillation abstraction. To the best of our knowledge, this is the first work
to view knowledge distillation as an abstraction, as well as to study property
preservation under knowledge distillation and defining the pairwise confidence
preservation criterion.
We evaluate σ(Xtrain) using a black-box equivalence approach on six tasks
from the linguistic benchmark dataset. For the tasks where the property fails, we
modified the hyperparameters of the distillation process to ensure preservation.
Our evaluations demonstrate that preservation of this confidence property could
aid the distillation process by guiding the selection of hyperparameters.
Our future work will involve using formal methods to study more properties
of large deep neural networks. Additionally, due to the complexity of the dis-
tillation schema for TinyBERT, not all the learning parameters were examined.
Therefore, further research is needed to investigate the impact of pre-trained
distillation and the combination of distillation losses.
References
1. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan, A., Shyam, P., Sastry, G., Askell, A.: Language models are few-shot
learners. Advances in neural information processing systems 33, 1877–1901 (2020)
2. Buciluˇa, C., Caruana, R., Niculescu-Mizil, A.: Model compression. 12th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining pp.
535–541 (2006)
Confidence Preservation Property in Knowledge Distillation Abstractions
3. Clarke, E., Grumberg, O., Jha, S., Lu, Y., Veith, H.: Counterexample-guided ab-
straction refinement. International Conference on Computer Aided Verification pp.
154–169 (2000)
4. Dahiya, M., Bansal, S.: Black-box equivalence checking across compiler optimiza-
tions. In: Chang, B.Y.E. (ed.) Programming Languages and Systems. pp. 127–147.
Springer International Publishing, Cham (2017)
5. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-
rectional transformers for language understanding (2018)
6. Du, X., Xie, X., Li, Y., Ma, L., Liu, Y., Zhao, J.: Deepstellar: Model-based quan-
titative analysis of stateful deep learning systems. 27th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations
of Software Engineering pp. 477–487 (2019)
7. Esteva, A., Kuprel, B., Novoa, R.A., Ko, J., Swetter, S.M., Blau, H.M., Thrun, S.:
Dermatologist-level classification of skin cancer with deep neural networks. nature
542(7639), 115–118 (2017)
8. Gal, Y., et al.: Uncertainty in deep learning (2016)
9. Gao, B., Pavel, L.: On the properties of the softmax function with application in
game theory and reinforcement learning. arXiv e-prints pp. arXiv–1704 (2017)
10. Goldblum, M., Fowl, L., Feizi, S., Goldstein, T.: Adversarially robust distillation.
AAAI Conference on Artificial Intelligence 34(04), 3996–4003 (2020)
11. Guidotti, D., Leofante, F., Pulina, L., Tacchella, A.: Verification of neural networks:
enhancing scalability through pruning. arXiv preprint arXiv:2003.07636 (2020)
12. Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On calibration of modern neural
networks. International Conference on Machine Learning pp. 1321–1330 (2017)
13. Heidari, M., Jones, J.H.: Using bert to extract topic-independent sentiment features
for social media bot detection. 11th IEEE Annual Ubiquitous Computing, Elec-
tronics & Mobile Communication Conference (UEMCON) pp. 0542–0547 (2020)
14. Henzinger, T.A., Lechner, M., Zikeli, D.: Scalable verification of quantized neural
networks. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 35,
pp. 3787–3795 (2021)
15. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531 (2015)
16. Huang, S.Y., Cheng, K.T.T.: Formal equivalence checking and design debugging,
vol. 12. Springer Science & Business Media (2012)
17. Huang, X., Kwiatkowska, M., Wang, S., Wu, M.: Safety verification of deep neu-
ral networks. International Conference on Computer Aided Verification pp. 3–29
(2017)
18. Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., Liu, Q.: Tinybert:
Distilling bert for natural language understanding. In: Findings of the Association
for Computational Linguistics: EMNLP 2020. pp. 4163–4174 (2020)
19. Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray,
S., Radford, A., Wu, J., Amodei, D.: Scaling laws for neural language models. arXiv
preprint arXiv:2001.08361 (2020)
20. Katz, G., Barrett, C., Dill, D.L., Julian, K., Kochenderfer, M.J.: Reluplex: An
efficient smt solver for verifying deep neural networks. In: International Conference
on Computer Aided Verification. pp. 97–117. Springer (2017)
21. Kim, Y.J., Awadalla, H.H.: Fastformers: Highly efficient transformer models for
natural language understanding. arXiv preprint arXiv:2010.13382 (2020)
22. Kong, L., Jiang, H., Zhuang, Y., Lyu, J., Zhao, T., Zhang, C.: Calibrated
language model fine-tuning for in-and out-of-distribution data. arXiv preprint
arXiv:2010.11506 (2020)
D. Vengertsev et al.
23. Kurhe, V.K., Karia, P., Gupta, S., Rose, A., Bansal, S.: Automatic generation
of debug headers through blackbox equivalence checking. In: 2022 IEEE/ACM
International Symposium on Code Generation and Optimization (CGO). pp. 144–
154 (2022)
24. Lim, J.P., Nagarakatte, S.: Automatic equivalence checking for assembly implemen-
tations of cryptography libraries. In: 2019 IEEE/ACM International Symposium
on Code Generation and Optimization (CGO). pp. 37–49 (2019)
25. Loiseaux, C., Graf, S., Sifakis, J., Bouajjani, A., Bensalem, S., Probst, D.: Property
preserving abstractions for the verification of concurrent systems. Formal Methods
in System Design 6(1), 11–44 (1995)
26. Min´e, A.: A few graph-based relational numerical abstract domains. International
Static Analysis Symposium pp. 117–132 (2002)
27. Pearce, T., Brintrup, A., Zhu, J.: Understanding softmax confidence and uncer-
tainty. arXiv e-prints pp. arXiv–2106 (2021)
28. Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word repre-
sentation. In: Proceedings of the 2014 conference on empirical methods in natural
language processing (EMNLP). pp. 1532–1543 (2014)
29. Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer,
L.: Deep contextualized word representations. In: NAACL (2018)
30. Pulina, L., Tacchella, A.: An abstraction-refinement approach to verification of
artificial neural networks. pp. 243–257. Springer (2010)
31. Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert, a distilled version of bert:
smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019)
32. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fer-
gus, R.: Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199
(2013)
33. Tang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O., Lin, J.: Distilling task-specific
knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136
(2019)
34. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,
 L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-
cessing systems 30 (2017)
35. Vengertsev, D., Sherman, E.: Recurrent neural network properties and their veri-
fication with monte carlo techniques. AAAI Conference on Artificial Intelligence,
SafeAI@AAAI (2020)
36. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: Glue: A multi-
task benchmark and analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461 (2018)
37. Wang, Q., Zhang, K., Liu, X., Giles, C.L.: Verification of recurrent neural networks
through rule extraction. arXiv preprint arXiv:1811.06029 (2018)
","nanThe distillation process ensures that the student model has similar accuracy as the teacher model. Since, by construction, the distilled model is smaller in terms of layers and dimensions, we can essentially consider it as an abstraction of the original model. However, since the student model might have a completely different architecture than its teacher, establishing an explicit mapping between those two systems is challenging. Instead, we say that these models have an implicit abstraction, which is created based on correctness preservation. Thus, the question is whether this implicit abstraction preserves properties between two models. One of the properties that we investigate here is confidence, which we discuss in the next section."
"This paper presents an extension of the Nova folding scheme for interactive proofs to custom gates and extra rounds of verifier randomness. The authors introduce Origami, the first known folding scheme for lookups. The key insights include:

* Generalization of Nova-style folding to arbitrary polynomial ""custom gates"".
* Extension of the protocol to work with extra rounds of verifier randomness, called custom gates with verifier input.
* Demonstration of how Halo2 lookup folding can be seen as a special case of custom gates with verifier input.","In recent years, interactive proofs (IPs) and zero-knowledge (ZK) have gained significant attention due to new applications arising from blockchain and cryptocurrencies. One exciting development is the concept of folding, which allows multiple instances of a constraint system to be combined into a single instance, reducing the verification task. This paper builds on the Nova folding scheme for R1CS circuits and extends it to custom gates and extra rounds of verifier randomness, presenting Origami, the first known folding scheme for lookups.","The authors present a detailed construction of relaxed AIR instances, which are obtained by converting AIR instances to relaxed AIR instances with additional slack vectors and randomness. Committed relaxed AIR instances are defined, consisting of commitments to the relaxed AIR instance and its witnesses. A single fold for custom gates is described, involving the computation of cross terms and the exchange of commitments and openings between the prover and verifier. Iterative folding is also discussed, allowing multiple relaxed AIR instances to be folded together.","The main result of the paper is the introduction of custom gates with verifier input, which extends the previous work to scenarios where the prover needs information from the verifier (usually in the form of random challenges) to compute certain parts of the trace in the middle of the computation. A full protocol for custom gates with verifier input is provided, along with two remarks on the commitment of verifier input and the generalization to multiple rounds of verifier input.","The paper concludes by discussing an application of the custom gates with verifier input framework to Halo2 lookups, resulting in the Origami folding scheme for lookups. The folding procedure and the knowledge soundness of the protocol are discussed in detail. The authors also compare their work with other protocols such as Nova, Sangria, Moon Moon, and Protostar.",Folding Custom Gates with Verifier Input,"Aard Vark, Yan X Zhang","arXiv:2401.11364v1  [cs.CR]  21 Jan 2024
Folding Custom Gates with Veriﬁer Input
Aard Vark∗
Yan X Zhang†
January 20, 2024
Abstract
In the context of interactive proofs, a folding scheme, popularized by Nova, is a way to
combine multiple instances of a constraint system into a single instance, so the validity of the
multiple instances can statistically be reduced to the validity of a single one. We show how
Nova folding can be generalized to “custom” gates and extra rounds of veriﬁer randomness.
As an application of this extension, we present Origami, the ﬁrst (to our knowledge) known
example of a folding scheme for lookups.
1
Introduction
In recent years, the ﬁeld of interactive proofs (IPs) and zero-knowledge (ZK) have been very active
thanks to new applications arising from blockchain and cryptocurrencies. One of the more exciting
new developments is the concept of folding. A folding scheme is a protocol to aggregate multiple
satisfying instances of an arithmetization (some constraint system, such as a set of equations) into
a single instance. Usually, the constraint systems are encoding some computation done by a prover
to be checked by a veriﬁer. Therefore, a folding scheme then allows multiple computations to be
combined and just veriﬁed once, which can be a signiﬁcant saving for many implementations of
proving systems.
Nova [8] introduced folding and a folding scheme for R1CS circuits (a particular arithmetization).
Under the hood, Nova-style folding works by taking a random linear combination of the instances;
the R1CS equations need to be generalized to relaxed R1CS constraints, which are stable under
linear combination.
Our goals in this work are:
1. Explicitly show how Nova-style folding can be generalized to any polynomial “custom gate”.
The idea is already sketched in Sangria [9, Section 3.3]; we are just ﬁlling in details.
2. Generalize how the protocol works to when we have extra rounds of veriﬁer randomness in
the protocol. We call this custom gates with veriﬁer input.
3. Show that folding Halo2-style lookups can be seen as a special case of custom gates with
veriﬁer input.
We explicitly write out the protocol (Origami).
We did not observe any
previous examples of folding schemes for lookups in literature.
∗tideofwords@gmail.com
†yan.x.zhang@sjsu.edu, Department of Mathematics and Statistics, San Jos`e State University
1
This paper is an expanded version of a note posted on HackMD earlier in April 2023 [11]. Similar
ideas were developed independently in Protostar [3], under the name of “accumulation schemes for
special-sound protocols.”
2
Preliminaries
Generally, we use capital letters as vectors, and the matching lower case letters refer to their
coordinates. For example, a vector S would have its entries labeled (s1, . . . , sn).
2.1
Arithmetization and PAIRs
In interactive proofs, an arithmetization is an embedding of a computation (such as 3 + 4 = 7) into
some algebraic constraint system (such as a polynomial or a linear equation). A preprocessed alge-
braic intermediate representation (PAIR) (as in [1]) is a particular arithmetization of the following
form:
1. P (the structure) is a collection of polynomials f1, . . . , fℓ in 2w variables over a ﬁnite ﬁeld F.
2. An execution trace T for P is an array of elements Tr,c of F, with n rows labeled 0 to n − 1,
each of width w.
3. In order for the trace T to be valid, it is required that, when we substitute into any fi the 2w
elements of any two consecutive rows of T , the result equals zero:
fi(Tj,1, Tj,2, . . . , Tj,w, Tj+1,1, Tj+1,2, . . . , Tj+1,w) = 0,
where j + 1 wraps to 0 if j = n − 1.
4. The ﬁrst t ≤ w of the w columns of T will be predeﬁned and publicly known. These k columns
are known as the PAIR instance.
5. The remaining w − k columns of T are known as the witness.
From a theoretical viewpoint, the requirement of two consecutive rows of the last item is unnec-
essarily restrictive; the main idea is that there is some set of elements Sj “corresponding” to each
row and that Sj+1 is a vertical shift of Sj. In practice, T is a recording of computation, where each
two consecutive rows are embedding one particular computation.
As an example of a PAIR (slightly modiﬁed) from [1], see Table 1. In this example, we have
n = 4, t = 1, w = 2. P has a single polynomial f1, which we can write in shorthand as
f1 = C1(X1[1] − (X1 + X2)) + (1 − C1)(X1[1] − X1 ∗ X2).
Explicitly, this means for all j ∈ {0, . . . , 3}, we have
f1,j = c1,j(x1,j+1 − (x1,j + x2,j) + (1 − c1,j)(x1,j+1 − x1,j ∗ x2,j) = 0,
where:
• the columns correspond to the vectors C1, X1, X2 respectively;
2
1
1
1
1
2
5
0
7
3
0
21
0
Table 1: An example of a PAIR that computes (1 + 1 + 5) ∗ 3 = 21.
• recall that the lower case variables are the parts of the capital letter vectors. For example,
X1 = (x1,1, . . . , x1,4);
• X1[1] means that the variable corresponds to the X1 variable for the next row.
Here, the ﬁrst (t = 1) preprocessed column is acting as a “selector column,” meaning it selects
whether the operation is addition (corresponding to c1,j = 1) or multiplication (corresponding to
c1,j = 0). The information of this column and P = {f1} together encode a circuit that is computing
(1 + 1 + 5) ∗ 3 = 21 step by step, starting with adding the ﬁrst 2 inputs (x1,1 = 1 and x2,1 = 1)
to obtain the ﬁrst intermediate step 2. The “witness to the computation” is then the content of
the second through fourth (w = 3) columns, and the combined table is the execution trace of the
computation.
2.2
Commitments
We assume the basic deﬁnition of a commitment scheme as in [2] (where commitment schemes are
referred to as “blobs”). We assume that we have an additively homomorphic commitment scheme
Com(pp, x, ρ), meaning that
Com(pp, x, ρ) + Com(pp, y, ρ) = Com(pp, x + y, ρ)
. We denote a commitment to a vector V by V = Com(pp, V, ρ).
For clarity of notation, sometimes we will write Com(V ) := Com(pp, V, ρ) when the arguments
are obvious from context.
2.3
Folding
Folding schemes, introduced in [8], reduce the task of checking two instances in a relation R to the
task of checking a single instance R. A formal deﬁnition is given in [8, Section 3].
Given two diﬀerent instance-witness pairs to the same structure, a folding scheme allows one to
combine them into a single instance-witness pair. This reduces the task of checking two traces to
the task of checking a single trace.
In both [8] and the present work, folding is achieved by taking a random linear combination of
the elements Tr,c of the execution trace. The constraint polynomials need to be relaxed so as to be
compatible with linear combinations. A detailed construction is given in Section 3.1.
2.4
Homogenization
We start with a general algebraic identity.
3
Proposition 2.1. Let p(x) be a homogenous polynomial of degree d in n variables (where x =
(x1, x2 . . . , xn)). Then there are d − 1 polynomials ∆1p, ∆2p, . . . , ∆d−1p of degree at most d in 2n
variables such that
p(x + ry) = p(x) + rdp(y) +
d−1
X
k=1
rk∆kp(x, y).
Proof. Write p(x) as a linear combination of monomials xi1xi2 · · · xid:
p(x) =
X
ai1,i2,...,idxi1xi2 · · · xid.
(It’s OK if some of the indices are equal to each other. For example, the monomial x2
1 has i1 = i2 = 1.
Also, this expression is not unique, and that’s OK too. For example, x1x2 could be rewritten as
x2x1 or even 2x1x2 − x2x1.)
The polynomial ∆kp(x) can be computed as follows: for any monomial, ∆k(xi1xi2 · · · xid) is the
sum of the
3.1
Relaxed AIR Instances
Recalling the setting of Section 2.1, we write fi(T ) for the length-n vector whose j-th entry is fi
applied to rows j and j + 1 of T :
fi(Tj,1, Tj,2, . . . , Tj,w, Tj+1,1, Tj+1,2, . . . , Tj+1,w).
(recall that rows wrap around, so row n is the same as row 0.)
We deﬁne a relaxed AIR instance
(P = {fi}i∈{1,...,l}, {Ei ∈ Fn}i∈{1,...,ℓ, u ∈ F)
to be satisﬁed by a trace T (as before, a n-by-w array of elements of F) if
f homog
i
(Tj,1, Tj,2, . . . , Tj,w, Tj+1,1, Tj+1,2, . . . , Tj+1,w, u) = (Ei)j,
for each i and j. We call the Ei the slack vectors and T the witness for the instance.
We’ll write f homog
i
(T, u) for the vector whose j-th entry is
f homog
i
(Tj,1, Tj,2, . . . , Tj,w, Tj+1,1, Tj+1,2, . . . , Tj+1,w, u).
Since f homog
i
is a polynomial in 2w + 1 variables, Proposition 2.1 deﬁnes polynomials ∆kf homog
i
in 2(2w + 1) variables. We’ll write ∆kf homog
i
(T 1, u1; T 2, u2) for the vector whose j-th entry is
∆kf homog
i
(T 1
j,1, . . . , T 1
j+1,w, u1, T 2
j,1, . . . , T 2
j+1,w, u2).
Any AIR instance can be “promoted” to a relaxed AIR instance by setting u = 1 and E = 0.
The main idea is that by converting AIR instances to relaxed AIR instances, the computations can
be more easily combined or “folded.”
3.2
Committed relaxed AIR
A committed relaxed AIR instance-witness pair (T, u, E, ρ) for a structure P = {f1, . . . , fℓ} consists
of:
• a commitment T to an n-by-w matrix of scalars,
• a scalar u,
• a commitment to slack vectors E = (E1, . . . , Eℓ), and
• randomness ρ (to be used by the commitment scheme).
A witness to the committed relaxed AIR instance (fi, Ei, T) is a tuple (E, T ) of
• a slack vector E and
• an n-by-w matrix T ,
such that T = Com(pp, T, ρ), E = Com(pp, E, ρ), and f homog
i
(T, u) = Ei.
5
3.3
Single Fold for Custom Gates
We’ll describe a folding scheme for relaxed AIR instances. Suppose a prover and a veriﬁer have an
AIR P, i.e. they both know the collection of polynomials fi. The prover is given two relaxed AIR
instances (T 1, E1, u1) and (T 2, E2, u2). The veriﬁer knows only the scalars u1 and u2.
Let di be the degree of the polynomial fi, and let ∆kf homog
i
be as deﬁned above.
The prover P and the veriﬁer V carry out the following protocol.
Protocol 3.1 (Custom Gate, Single Fold).
INPUT [to P]: 2 relaxed instance-witness pairs I(i) = (u(i), T (i), E(i)), i ∈ {1, 2}.
OUTPUTS: [from P]: 1 relaxed instance-witness pair I = (u, T, E); [from V]: 1 relaxed
committed instance I = (u, T, E).
1. P computes commitments X for X ∈ {T (1), T (2), E(1), E(2)} and their openings ρX
and sends them to V. (explicitly: for all X, X = Com(pp, X, ρX) holds.)
2. For each constraint polynomial fi, and each degree 1 ≤ k ≤ di − 1, P also computes
the cross term
Bi,k = ∆kf homog
i
(T 1, u1; T 2, u2)
(a vector of length n). P computes commitments and openings for Bi,k and sends
them to V.
3. V samples a random folding challenge r ∈ F, and sends r to P.
4. Both P and V compute u = u1 + ru2. P computes T = T 1 + rT 2 and, for each i,
Ei = E1
i + rdiE2
i +
di−1
X
k=1
rkBi,k.
P returns the “folded” relaxed AIR instance-witness (u, T, E).
5. V computes T = T 1 + rT 2, and, for each i,
E1
i + rdiE2
i +
di−1
X
k=1
rkBi,k.
V returns the folded relaxed committed instance (u, T, E).
Like Nova, this folding process can be iterated. Steps 2-5 allow two committed relaxed AIR
instances to be folded into one, at the cost of sending just 4 + Pℓ
i=1 di − 1 commitments to the
veriﬁer. One can iterate the procedure to fold an arbitrary number of committed relaxed AIR
instances together, one by one. At the end, the prover only has to convince the veriﬁer that the
ﬁnal (E, T ) is a witness to the folded instance. Explicitly:
6
Protocol 3.2 (Custom Gates, Full Protocol).
INPUT: [to P]:
N instance-witness pairs / traces g
I(i) = (T (i)), for i ∈ {1, . . . , N}.
OUTPUTS: [from P]: 1 relaxed instance-witness pair Icml = (u, T, E); [from V]: 1 folded
relaxed committed instance Icml = (u, T, E).
1. To initialize folding, P initializes a “cumulative lookup instance” Icml where u, T, E
are all equal to zero. Similarly, V initializes a cumulative committed instance Icml
where u, T, E are all equal to zero.
2. When we fold in a new lookup instance g
I(i) = (T (i)), P constructs a relaxed lookup
instance I(i) = (u(i), T (i), E(i)) by setting u(i) = 1 and E(i)
j
= (0, . . . , 0) for all j.
3. P and V run Protocol 3.1.
• P overwrites
Icml ← SingleFoldP(Icml, I(i)),
that is: apply Protocol 3.1, using the current Icml as the ﬁrst input and the
new relaxed instance I(i) as the second input. During each step, we need a new
r = r(i) for our “folding randomness.”
• V builds the committed relaxed instance I(i) = (u(i), T (i), E(i)) out of commit-
ments sent from P. Using I(i), V overwrites
Icml ← SingleFoldV(Icml, I(i)).
4. After N steps, the folding is complete. All that remains is for P to convince V that the
ﬁnal folded tuple Icml is a legitimate witness to the committed instance Icml, which
is done as in Halo2.
3.4
Custom Gates with Veriﬁer Input
The work in Section 3.3 is basically making explicit a sketch already outlined in Sangria [9]. To
achieve our main goal of allowing lookups, we need to generalize the context to one where the prover
would need information from the veriﬁer (usually in the form of random challenges) to compute
certain parts of the trace in the middle of the computation (see RAPs [1]). More speciﬁcally, we
assume that:
1. The prover ﬁlls out the partial trace T0, a n × w0 table without additional information.
2. The prover receives some information from the veriﬁer R, which we assume is embedded as a
vector in F v for some constant v ∈ N.
3. With S, the prover is able to compute w1 additional columns to create the full trace T , a
n × w table which can then be checked for validity against the structure P.
We call this setup custom gates with veriﬁer input. The standard context where this workﬂow is
used is lookups, our main goal. Thus, we ﬁrst give the formalism and defer the example to Section 4.
7
Protocol 3.3 (Custom Gates with Veriﬁer Input, Full Protocol).
INPUT: [to P]: N partial instance-witness pairs / traces g
I(i) = (T (i)
0 ), for i ∈ {1, . . . , N}.
OUTPUTS: [from P]: 1 relaxed (full) instance-witness pair Icml = (T, u, E, R); [from V]: 1
folded relaxed committed instance Icml = (T , u, E, R).
1. To initialize folding, P initializes a “cumulative lookup instance” Icml where T, u, E, R
are all equal to zero. Similarly, V initializes a cumulative committed instance Icml
where T, u, E, R are all equal to zero.
2. When we fold in a new partial trace g
I(i) = (T (i)
0 ),
(a) P ﬁrst commits to T (i)
0
for i ∈ {1, 2}, sending the commitments to V.
(b) V then gives P some information R(i).
(c) P uses R(i) to compute T (i), thus obtaining a full relaxed lookup instance I(i) =
(u(i), R(i), T (i), E(i)) by setting u(i) = 1, E(i)
j
= (0, . . . , 0) for all j, and R(i) = R.
(d) P and V run Protocol 3.1.
3.
• P overwrites
Icml ← SingleFoldP(Icml, I(i)),
that is: apply Protocol 3.1, using the current Icml as the ﬁrst input and the
new relaxed instance I(i) as the second input. During each step, we need a new
r = r(i) for our “folding randomness.”
• V builds the committed relaxed instance I(i) = (T (i), u(i), E(i), R(i) = R) out of
commitments sent from P. Using I(i), V overwrites
Icml ← SingleFoldV(Icml, I(i)).
4. After N steps, the folding is complete.
We conclude with two remarks:
• Note that R does not need to be committed; it is, after all, sent by V.
• This protocol can easily be generalized to the framework where we have several rounds of
veriﬁer input. A single step (2) of Protocol 3.3 then involves building up from T (i)
0
to T = T (i)
k
after k steps, each step making the partial commitments to Ti right before asking for a new
veriﬁer input Si. In our work, we do not use this full generality because of the notational
burden and the fact that lookups only require a single round of veriﬁer input.
4
Origami: Folding Lookups
We now come to the main application. In this section, we describe an explicit folding scheme for a
Halo2 lookup argument as a special case of our setup of custom gates with veriﬁer randomness.
8
4.1
Lookups
Given two vectors of numbers (actually, elements of some large ﬁnite ﬁeld F) A = (a1, . . . , am) and
S = (s1, . . . , sn), a lookup argument is an argument that each1 ai is equal to some sj. One common
use case is evaluating a function by lookup table. In this case, the vector S encodes the values of
some function, while A encodes some claimed evaluations of the function at speciﬁc points.
There have been several diﬀerent implementations of lookup arguments, starting with Plookup
[4]. The lookup we are trying to fold is Halo2 lookups [6, 4.1], which we summarize as follows:
1. P ﬁnds a permutation A′ of A and a permutation S′ of S such that, for each j, either a′
j = a′
j−1
or a′
j = s′
j.
2. P and V carry out a grand product protocol to prove that A′ is a permutation of A and S′ is
a permutation of S.
(a) First, V sends P random challenges β and γ.
(b) Based on those random challenges, P creates two new vectors W and Z, the “grand
product vectors”.
3. In order to verify the lookup, V checks a list P of polynomial identities involving the vectors
{A, A′, S, S′, W , Z}, the random challenge scalars β and γ, and constant and public Q∗
vectors (used for zero-knowledge):
(a) (1 − Qblind − Qlast) · (Z[−1](A′ + β) − Z(A + β)) = 0
(b) (1 − Qblind − Qlast) · (W[−1](S′ + γ) − W(S + γ)) = 0
(c) Qlast · (Z2 − Z) = 0
(d) Qlast · (W 2 − W) = 0
(e) (1 − Qblind − Qlast)(A′ − S′)(A′ − A′[1]) = 0
(f) Q0 · (A′ − S′) = 0
(g) Q0 · (Z − 1) = 0
(h) Q0 · (W − 1) = 0.
We call these equations f1 = 0 through f8 = 0 respectively.
1. Recall that each polynomial equation fi = 0 corresponds to n polynomial equations of the
same form
fi,j(a1, . . . , an, a′
1, . . . , a′
n, . . . , w1, . . . , wn, β, γ) = 0,
where each capital letter X ∈ {A, A′, S, S′, Z, W} corresponds to substituting xj for fi,j, xj
being the j-th coordinate for the vector X.
The coordinates of the Q∗ vectors, the q∗
i ’s,
do not appear as arguments since they are constants. As an example, the third equation
f3 = Z2 − Z = 0 is shorthand for n constraints of the form f3,i(· · · ) = z2
i − zi = 0 for each i,
where zi is the i-th coordinate of Z.
1A does not have to contain every element of S. Also, A can contain duplicates (as can S). But every element of
A has to appear somewhere in S.
9
A(1)
S(1)
A(2)
S(2)
3
1
6
2
7
3
4
4
3
5
4
6
5
7
4
8
Table 2: Two lookups that we may want to fold together.
.
2. Recall that e.g. Z[−1] means that when we unpack equation fi,j, instead of substituting zj,
we substitute zj−1. As an example, f1 = 0 corresponds to n constraints of the form
f1,j(· · · ) = (1 − qblind
j
− qlast
j
)
A′(1)
S′(1)
A′(2)
S′(2)
3
3
4
4
3
1
4
8
5
5
4
2
7
7
6
6
Table 3: Example of creating the permuted versions of A and S.
.
A1 + rA(2)
S(1) + rS(2)
A′(1) + rA′(2)
S′(1) + rS′(2)
603
201
403
403
407
403
403
801
403
605
405
205
405
807
607
607
Table 4: The folded trace / witness of the two lookups. Notice that the result is no longer a lookup.
.
• Each entry of A′(1) is either the same as the entry of S′(1) next to it, or a repeat of the previous
entry of A′(1). The ﬁrst entry 3, the 5 and the 7 are all the same as the adjacent entries of
S′(1), while the second 3 is a repeat of the ﬁrst.
• The same holds for A′(2).
The combination of A, S, A′, S′ (for either i) would be considered as our T0 from Section 3.4,
the partial witness computable by P without further information. At this point, as in Protocol 3.3,
the prover sends the veriﬁer commitments to all eight vectors above.
Now, V replies with random challenges R(1) = (β(1), γ(1)) and R(2) = (β(2), γ(2)). P uses them
to create grand products Z(1), W (1), Z(2), W (2). We defer the details of the algebra to later in this
section; what’s important to know is just that the lookup problem can be translated into checking
some polynomial conditions
fi(β(1), γ(1), A(1), S(1), A′(1), S′(1), W (1), Z(1)) = 0
and
fi(β(2), γ(2), A(2), S(2), A′(2), S′(2), W (2), Z(2)) = 0.
So far we have two lookup arguments: one for the odd numbers, and one for the evens. Now
let’s fold them together, as in Protocol 3.1. First, the prover will send the veriﬁer commitments
to some cross-terms B1, B2, B3, B4.
Then the veriﬁer sends the prover a random challenge r.
(In our example, let’s imagine r = 100.) Finally, the prover computes the linear combinations
A(1) + rA(2), S(1) + rS(2) and so forth.
To see the folded example, see Table 4. Something strange happened here: The “lookup” and
“permutation” relations are no longer satisﬁed. The number 603 appears in A(1) + rA(2) but not
in S(1) + rS(2). The vector A′(1) + rA′(2) is not a permutation of A(1) + rA(2). It looks like we’ve
thrown out all the nice properties that made the lookup argument work.
But in fact polynomials save the day! The “folded” vectors will still satisfy a polynomial identity
that we’ll be able to write down, and that’s what the prover will verify.
11
4.3
The Procedure
We now show how our protocol works in the case of lookups. First, we relax fi into the following
homoegenous equations with u and Ei:
1. (1 − Qblind − Qlast) · (Z([−1])(A′ + β) − Z(A + β)) = E1
2. (1 − Qblind − Qlast) · (W([−1])(S′ + γ) − W(S + γ)) = E2
3. Qlast · (Z2 − uZ) = E3
4. Qlast · (W 2 − uW) = E4,
5. (1 − Qblind − Qlast)(A′ − S′)(A′ − A′[1]) = E5,
6. Q0 · (A′ − S′) = 0
7. Q0 · (Z − u) = 0
8. Q0 · (W − u) = 0.
For example, recall that f3 used to encode n constraints of form
f3,i = qlast
i
(z2
i − zi) = 0;
the new relaxed f3 would instead encode n constraints of the form
f3,i = qlast
i
(z2
i − ziu) = e3,i.
Note that each of the ﬁrst 5 quadratic equations give a slack term, and the linear equations do
not. Applying Proposition 2.1 gives the corresponding cross-terms
B1
=
(1 − Qblind − Qlast) ·
Recall that the main idea here is that we really want the fi equations to satisfy constraints of
the form
fi(X1 + rX2) = fi(X1) + rfi(X2),
where X1 and X2 are shorthand meaning “all the arguments,” but for the quadratic fi we have to
settle for
fi(X1 + rX2) = fi(X1) + r2fi(X2) + rBi(X1, X2).
To see this explicitly with f3 = Z2 − uZ, we compute
f3(X1 + rX2) = (Z1 + rZ2)2 − (u1 + ru2)(Z1 + rZ2)
(4.2)
= (Z2
1 + 2rZ1Z2 + r2Z2
2) − u1Z1 − ru1Z2 − ru2Z1 − r2u2Z2
(4.3)
= (Z2
1 − u1Z1) + r2(Z2
2 − u2Z2) + r(2Z1Z2 − u1Z2 − u2Z1)
(4.4)
= f3(X1) + r2f3(X2) + rB3(X1, X2),
(4.5)
as desired.
We can now simply follow Protocol 3.3, with T 0, R, T deﬁned as follows:
1. First, P is given the instance A and S. P then computes A′ and S′ without further help, so
the n×4 partial trace T0 would consist of the information in (A, S, A′, S′), which can be done
via e.g. concatenation.
2. After committing to T0 to obtain T0, P receives the veriﬁer randomness R = (β, γ).
3. P is now able to compute the n × 6 full trace T by making 2 more columns Z and W.
5
Comparison with Other Protocols
5.1
Nova
Nova [8] is a folding scheme for R1CS circuits. The R1CS structure is a special case of an AIR,
where there is a single constraint polynomial f1, of degree 2, having a speciﬁc form.
Because
the polynomial is of degree d = 2, there is only a single cross-term B = B1,1. The folding scheme
described here generalizes Nova: if you apply this scheme to R1CS systems, you recover the original
Nova folding scheme.
The idea that Nova-style folding can be generalized to arbitrary custom gates was introduced
in [9, Section 3.3].
5.2
Sangria
Sangria [9] had outlined an approach to custom gates. Our lookup protocol is almost, but not quite,
a special case of that procedure, mainly because of the roles of β and γ. Instead, what we have here
is a special case of “custom gates with veriﬁer randomness,” a concept that’s a slight generalization
of Sangria’s approach to custom gates.
The argument for knowledge soundness (a cheating prover cannot convince the veriﬁer to accept
a folded proof unless the prover actually knows N satisfying witnesses) is similar to Sangria. We
give a proof of knowledge soundness in Appendix A.
13
In outline, the proof (very similar to as done in Sangria and Nova) is as follows. The idea is to
imagine an extractor that interacts with the prover. The extractor is allowed to rewind the prover
to a previous state. In practice, this means the extractor (playing the role of veriﬁer) can send the
prover diﬀerent challenges r, and see how the prover responds. Like in Nova and Sangria, by testing
enough diﬀerent values r and doing a bit of algebra, the extractor can recover the witnesses N that
were folded together. Once the extractor can recover the N folded witnesses, since Halo2 lookups
themselves are knowledge sound, we know the prover must know N valid lookup witnesses.
Our proof of knowledge soundness in the general scheme is only a bit more complicated than
that of Sangria, partly due to the polynomials of arbitrary degree (although again Sangria had an
outline already in Section 3.3) and partly due to the fact that we have “veriﬁer randomness,” as
stated above. We have to take care that the veriﬁer-provided randomness β and γ does not mess
things up.
5.3
Related Work: Moon Moon and Protostar
In work in progress ([10] and personal communication), Lev Soukhanov has proposed Moon Moon,
a more powerful approach to combine folding with veriﬁer input. In Moon Moon, suppose there
are N instance-witness pairs to be folded. The prover ﬁrst commits to all N partial witnesses, then
receives (once) a piece of veriﬁer randomness R. The prover then uses R to compute the remaining
columns of the full witness.
Moon Moon allows, for example, an extended permutation argument across N execution traces.
The prover commits to all entries of both permutations, spread across N partial witnesses, then
computes a single Fiat–Shamir hash and uses it to construct a grand product polynomial, which
proves the permutation constraint.
Protostar [3] is a proving system that uses a folding scheme for higher-degree polynomial gates.
Additionally, Protostar is a non-uniform IVC: it can prove a sequence of computations of the form
zi+1 = Fki(zi),
where F1, . . . , Fn are n diﬀerent computations encoded by n diﬀerent circuits. This has applications
to proving the output of a virtual machine which allows n diﬀerent operations.
6
Acknowledgments
We thank Nicolas Mohnblatt, Lev Soukhanov, Yi Sun, and Jonathan Wang for valuable discussions.
References
[1] Aztec. From AIRs to RAPs - how PLONK-style arithmetization works.
[2] Gilles Brassard, David Chaum, and Claude Cr´epeau. Minimum disclosure proofs of knowledge.
Journal of Computer and System Sciences, 1987.
[3] Benedikt B¨unz and Binyi Chen.
Protostar:
Generic eﬃcient accumulation/folding
for
special
sound
protocols.
Cryptology
ePrint
Archive,
Paper
2023/620,
2023.
https://eprint.iacr.org/2023/620.
14
[4] Ariel
Gabizon
and
Zachary
J.
Williamson.
plookup:
A
simpliﬁed
polynomial
protocol
for
lookup
tables.
Cryptology
ePrint
Archive,
Paper
2020/315,
2020.
https://eprint.iacr.org/2020/315.
[5] Ashrujit
Ghoshal
and
Stefano
Tessaro.
Tight
state-restoration
soundness
in
the
algebraic
group
model.
Cryptology
ePrint
Archive,
Paper
2020/1351,
2020.
https://eprint.iacr.org/2020/1351.
[6] Halo2. The Halo 2 book. https://zcash.github.io/halo2/index.html.
[7] Aniket Kate, Gregory M. Zaverucha, and Ian Goldberg. Constant-size commitments to polyno-
mials and their applications. In Masayuki Abe, editor, Advances in Cryptology - ASIACRYPT
2010, pages 177–194, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg.
[8] Abhiram Kothapalli, Srinath Setty, and Ioanna Tzialla.
Nova: Recursive zero-knowledge
arguments from folding schemes.
Cryptology ePrint Archive, Paper 2021/370, 2021.
https://eprint.iacr.org/2021/370.
[9] Nicolas Mohnblatt. Sangria: a folding scheme for plonk.
[10] Lev Soukhanov. Folding endgame. https://zkresear.ch/t/folding-endgame/106.
[11] Yan X Zhang and Aard Vark. Folding for arbitrary polynomial custom gates and lookups.
https://hackmd.io/@aardvark/Hk5UtwDl2.
A
Knowledge Soundness
In this section we will prove knowledge soundness of a noninteractive version of Protocol 3.3,
constructed by the Fiat–Shamir transformation. We will work in the algebraic group model, and
assume the commitment scheme used is KZG polynomial commitments. We begin by recalling
some preliminary concepts.
Intuitively, knowledge soundness is the claim that, if the prover convinces the veriﬁer to accept
a proof, then the prover “knows” a satisfying witness.
The claim is generally formalized as follows: a protocol is knowledge sound if, for every (possibly
dishonest) prover P and honest veriﬁer V, there exists an extractor E, such that whenever P con-
vinces V to accept a proof, E can determine a satisfying witness (with all but negligible probability)
by interacting with P. To make this precise, we need to say exactly what sorts of “interaction” are
allowed.
We will work in the algebraic group model. That is, whenever the prover outputs an element g
of a group G (e.g. an element of F), the prover also outputs (but does not send to the veriﬁer) a
representation
g =
n
X
i=1
aiGi
of g in terms of previously-seen elements Gi ∈ G.
We write this representation as [g].
The
representation is not available to the veriﬁer, but it is available to the extractor.
We will assume that the commitment scheme used is KZG polynomial commitments [7].
15
We provide a brief summary of the KZG polynomial commitment scheme.
The KZG commitment scheme allows a prover to commit to polynomials of degree up to
some ﬁxed t ≥ 0. The scheme relies on an trusted setup, which outputs (g, αg, α2g, . . . , αtg),
for some ﬁxed g ∈ G and α ∈ Z; the integer α itself is kept secret.
To commit to a polynomial
p(T ) =
t
X
i=0
aiT i,
the prover sends
p(T ) =
t
X
i=0
ai(αig).
This commitment scheme has the property that, in the algebraic group model, if the prover sends
a commitment to some p(T ), then the extractor has access to p(T ) (except with negligible proba-
bility). Indeed, if the prover has honestly committed some p(T ), then the algebraic representation
of the commitment in terms of the previously-seen group elements αig will take the form
p(T ) =
t
X
i=0
ai(αig),
and the coeﬃcients ai can be directly read oﬀ from this. For future reference, we let [KZG(p(T ))]
denote the algebraic representation
KZG(p(T )) =
t
X
i=0
ai(αig)
of the KZG commitment to the polynomial p(T ).
Protocol 3.3 is a public-coin interactive protocol between a prover P and a veriﬁer V. The
parties P and V send messages to each other in turn, in a total of 2r + 1 rounds, for some r: P
sends some a1, then V sends a challenge c1, then P sends a message a2, and so forth. The veriﬁer’s
messages are randomly chosen challenges. In this setting, the Fiat–Shamir transform replaces V
with a cryptographic hash function: whenever Protocol 3.3 calls for a random challenge from V,
the Fiat–Shamir prover P computes a cryptographic hash of the transcript of all values that have
been sent so far, and uses that hash as the random challenge.
Using Fiat–Shamir gives a dishonest prover a new avenue of attack: the prover can rewind the
veriﬁer to get new challenges. This makes proof of knowledge soundness more diﬃcult. We will use
ideas introduced in [5] to bound the probability that a dishonest prover can produce an accepted
transcript.
[5, Theorem 2] relates knowledge soundness of a Fiat–Shamir protocol to state-restoration wit-
ness extended emulation (sr-wee) soundness of the corresponding interactive protocol. For a formal
deﬁnition of sr-wee soundness, see [5, §4]. Informally, the deﬁnition of sr-wee soundness is as follows:
for every prover P, there exists an emulator E, such that if P can produce an accepted transcript
by repeated interaction with E, then E can produce a satisfying witness with all but negligible
probability. Here E, like the extractor above, interacts with P in the role of the veriﬁer, but is also
given access to the algebraic representation [g] of every group element sent by P. Meanwhile, P is
16
allowed to rewind the emulator, sending diﬀerent group elements to try to get a favorable challenge
back from E. By [5, Theorem 2], if an interactive protocol satisﬁes sr-wee soundness, then the
corresponding noninteractive protocol obtained by Fiat–Shamir is itself sound.
[5, Theorem 1] provides a framework for proving sr-wee soundness. Suppose we have the follow-
ing:
• For every partial transcript τ = ([a1], c1, . . . , [ai]), a small set of bad challenges BadCh(τ),
and
• An extractor function e that takes as input an accepted transcript ([a1], c1, . . . , [ar+1]) and
outputs, with high probability, a valid witness w.
Then [5, Theorem 1] shows that the protocol is sr-wee sound. Precisely, suppose that:
• For every τ, we have |BadCh(τ)| ≤ ǫ |Ch|, where Ch is the set from which challenges are
randomly drawn, and
• The probability in SRS that P can produce an accepted transcript ([a1], c1, . . . , [ar+1]), with
ci ̸∈ BadCh([a1], . . . , [ai]) for every i, is at most pfail. (SRS [5, §3] means that P is allowed
to rewind V.)
Then the protocol has sr-wee soundness error of at most
qǫ + pfail,
where P is allowed to make at most q queries to the (Fiat–Shamir) oracle.
Theorem A.1. Suppose IP0 is an interactive protocol for some relation R, of the following form.
The protocol IP0.
• P sends some T0 to V.
• V sends a random challenge R to P.
• P sends some T1 to V.
• Writing T for the concatenation of T0, R and T1, V veriﬁes the polynomial constraints
fj(T ) = 0
for j = 1, . . . , ℓ. If these constraints are satisﬁed, we say that (T0, R, T1) is an accepted
transcript for IP0.
Assume:
• For each T0, there is a set BadCh0(T0) of bad challenges, such that |BadCh0(T0)| ≤ ǫ |Ch|,
• The degree dj of each constraint polynomial fj satisﬁes dj ≤ ǫ |Ch|, and
• There is an extractor function e such that the probability in SRS that a cheating prover P can
create an accepted transcript (T0, R, T1), such that R ̸∈ BadCh0(T0) but e0([T0], R, [T1]) is not
an accepted transcript, is at most pfail.
17
Let IP be Protocol 3.3, with the following two modiﬁcations.
In place of step 2 (a-c), P and V carry out IP0 (where T is the concatenation T0T1).
At the end of the protocol, P reveals T cml and Ecml. Then V accepts the transcript if the KZG
commitments to T cml and Ecml agree with the committed values T cml and Ecml already calculated,
and the polynomial constraints
f homog
j
(ucml, T cml) = Ecml
are all satisﬁed.
Then IP is sr-wee sound for the relation
Rcml = {(x1, w1, x2, w2, . . . , xN, wN)|(xi, wi) ∈ R for all 1 ≤ i ≤ N} ,
with soundness error at most (2N + 1)ǫ + pfail + Negl(λ).
Proof. We will use [5, Theorem 1]. We need to deﬁne functions BadChi (one for each round of
veriﬁer challenge) and e.
We need to deﬁne bad challenges BadCh for two types of veriﬁer challenge: challenges R(i) in
step 2 of Protocol 3.3, which come from protocol IP0, and folding challenges r(i) in step 3, which
come from Protocol 3.1.
For challenges R(i), we simply let BadCh(T (i)
0 ) be the set of bad challenges for IP0 given by
hypothesis.
For folding challenges r(i), we distinguish two cases.
Recall that the transcript τ of all messages sent before V’s challenge r(i) contains the two
committed instances to be folded Icml = (ucml, T
cml, E
cml) and I(i) = (u(i), T
(i), E
(i)), as well as
P’s commitments Bi,k to cross terms.
Since we are working in the AGM, the function BadCh also has access to representations of
all prover output in terms of previously-seen group elements; these representations are written as
[T
cml], [E
cml)], and so forth.
We say that τ is good if:
1. The algebraic representations of all commitments sent by P take the form of KZG commit-
ments. In other words, there exist T cml, Ecml, T (i), E(i), Bi,k such that
[T
cml] = [KZG(T cml)],
and so forth, and
2. The two instance-witness pairs (ucml, T cml, Ecml) and (u(i), T (i), E(i)) are satisfying instance-
witness pairs, with cross-terms Bi,k.
In other words, we require that
f homog
j
(T cml + rT (i)) = Ecml + rdiE(i) +
dj−1
X
k=1
rkBj,k,
(A.2)
identically as polynomials in r, for each polynomial relation fj.
18
If τ is good, then we take BadCh(τ) to be the empty set.
If τ fails to satisfy condition (1) above (i.e. the prover has not sent KZG commitments), we also
take BadCh(τ) to be the set of r for which the algebraic representations [T cml] + r[T (i)], [Ecml] +
r[E(i)], [Bi,k] all have the form of Kate commitments. By linearity of Kate commitment, |BadCh(τ)| ≤
1.
Otherwise, τ satisﬁes condition (1) but not condition (2): the prover has sent commitments to
invalid witnesses. In this case, choose some j for which Equation A.2 does not hold identically. The
equality can only hold for at most dj values of r; let BadCh(τ) be the set of r for which Equation
A.2 holds.
Next, we deﬁne the extractor function e. At each step i, if the algebraic representation of the
prover’s output T (i) has the form of a KZG commitment
[T (i)] = [KZG(T (i))],
then e applies e0 to the committed value T (i), returning a purported witness w(i) = e0(T (i)). If
not, e simply returns a null value w(i) = ∅. The ﬁnal output of e is the tuple (w(1), . . . , w(n)).
Now we need show that the probability that P can produce some accepted transcript τ, con-
taining no bad challenges, and such that e(τ) is not a valid witness, is negligible in the security
parameter λ.
Since V accepts τ, the ﬁnal (T cml) is of the form (T cml) = KZG(T cml) of a KZG commitment,
and similarly for (Ecml). On the other hand, (T cml) comes with an algebraic representation [(T cml)]
as a linear combination of the representations [T (i)] in the AGM. Since we have assumed the discrete
logarithm problem is hard in G, we have [(T cml)] = [KZG(T cml)] except with probability negligible
in the security parameter λ (because if this equality did not hold, then P would have discovered a
nontrivial linear relation among elements of G).
So now suppose the algebraic representation [(T cml)] has the form of a KZG commitment.
By reverse induction we see that, since there are no bad challenges, all the folded values [(T (i))]
and [(E(i)] are KZG commitments as well, of the form [KZG(T (i))] and [KZG(E(i))], respectively.
Furthermore, at each step, T cml is replaced with T cml + r(i)T (i), and similarly for Ecml.
Again by reverse induction, since τ contains no bad challenges, we see that
f homog
j
(ucml, T cml) = Ecml
at each step, and
f homog
j
(u(i), T (i)) = E(i)
for every i.
Thus, the transcript T (i) is accepted for the protocol IP0. By hypothesis, the probability that
e0(T (i)) is not a satisfying witness is negligible in λ.
19
","nanThe paper begins with an overview of arithmetization, preprocessing algebraic intermediate representations (PAIRs), and commitment schemes. It also introduces the concept of folding schemes and their formal definition. Homogenization of polynomials is discussed, and a proposition is provided for expressing a homogenous polynomial as a linear combination of monomials."
"Quantum circuit simulation plays a pivotal role in various aspects of quantum computing research and development. Existing approaches in this domain often rely on tensor networks (TNs), which offer notable advantages in terms of improved concurrency and reduced computational complexity. However, TNs can suffer from substantial redundancy when represented using arrays. To address this issue, we present fast tensor decision diagram (FTDD), an open-source framework developed primarily in C++. By harnessing tensor decision diagrams (TDDs), FTDD effectively captures and encodes the redundancy inherent in tensor representations. Moreover, FTDD leverages a wealth of optimizations borrowed from both TN and binary decision diagrams (BDDs) to further enhance its performance. Our experiments demonstrate that FTDD achieves significant speedups compared to state-of-the-art array-based and decision diagram-based quantum circuit simulation approaches. For example, FTDD shows a 144x speedup over prior TDD implementations and a 25x speedup over quantum multi-valued decision diagrams on Google random quantum circuits. Additionally, FTDD exhibits an impressive 37x speedup over Google's TensorNetwork library on redundancy-rich circuits, highlighting its remarkable efficiency in exploiting and eliminating redundancies.","Quantum circuit simulation (QCS) is fundamental for diverse aspects of quantum computing research and development, spanning software development and debugging, noise simulation, quantum chip validation, and hybrid quantum-classical algorithms. However, QCS faces inherent scalability limitations due to its exponential complexity, particularly when dealing with deep and random circuits. Researchers have actively pursued solutions to mitigate the otherwise prohibitively long simulation runtimes, resulting in the emergence of Schrödinger-style methods utilizing full vector and matrix representations as well as tensor network (TN) approaches. TNs have gained prominence due to their superior concurrency and reduced complexity achieved through optimizations like TN partitioning and index slicing, tensor decomposition, hyper-edges, and contraction ordering.","Our proposed framework, fast tensor decision diagram (FTDD), aims to fully harness the capabilities of TDDs and overcome the limitations of prior approaches. FTDD comprises two primary steps: TN optimizations as preprocessing followed by TDD optimizations. In the first step, FTDD constructs a TN, applies a novel linear-complexity algorithm called Tetris for rank simplification, and explores near-optimal TN contraction orders. This step effectively reduces the complexity of the simulation by consolidating adjacent tensors and identifying efficient contraction sequences. In the second step, FTDD utilizes information extracted from the previous step to perform near-optimal contraction orders for the rank-simplified TN using TDDs. To optimize TDDs, we propose edge-centric data structures that eliminate overhead during recursive TDD operations and incorporate key optimizations borrowed from binary decision diagrams (BDD). These optimizations improve the efficiency of unique table and computed table operations, which are critical for TDD contraction.","Our experimental evaluation demonstrates the effectiveness of FTDD in quantum circuit simulation. We compare FTDD against state-of-the-art array-based and decision diagram-based QCS approaches, including Google's TensorNetwork library, QMDD, and prior TDD implementation. The results show that FTDD achieves remarkable speedups across various benchmarks, ranging from GHZ and graph state circuits to QFT unitary simulations, entangled-QFT circuits, quantum approximate optimization algorithm (QAOA), and variational quantum eigensolver (VQE). For instance, FTDD delivers a 144x speedup over prior TDD implementations and a 25x speedup over QMDD on Google random quantum circuits. Additionally, FTDD exhibits an impressive 37x speedup over Google's TensorNetwork library on redundancy-rich circuits.","In conclusion, we present fast tensor decision diagram (FTDD), an open-source framework that harnesses TDDs to effectively capture and encode redundancy in tensor representations. FTDD incorporates a range of optimizations drawn from both TN and BDD literature to further enhance its performance. Our experimental results demonstrate that FTDD significantly outperforms state-of-the-art array-based and decision diagram-based QCS approaches, achieving substantial speedups across a variety of quantum circuits. This work highlights the potential of TDDs in QCS and underscores the benefits of combining TN and DD techniques to address the challenges of quantum circuit simulation.",Quantum Circuit Simulation with Fast Tensor Decision Diagram,"Qirui Zhang, Mehdi Saligane, Hun-Seok Kim, David Blaauw, Georgios Tzimpragos, Dennis Sylvester","Quantum Circuit Simulation with
Fast Tensor Decision Diagram
Qirui Zhang, Mehdi Saligane, Hun-Seok Kim, David Blaauw, Georgios Tzimpragos, Dennis Sylvester
Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA
{qiruizh, mehdi, hunseok, blaauw, gtzimpra, dmcs}@umich.edu
Abstract—Quantum circuit simulation is a challenging compu-
tational problem crucial for quantum computing research and
development. The predominant approaches in this area center
on tensor networks, prized for their better concurrency and
less computation than methods using full quantum vectors and
matrices. However, even with the advantages, array-based tensors
can have significant redundancy. We present a novel open-source
framework that harnesses tensor decision diagrams to eliminate
overheads and achieve significant speedups over prior approaches.
On average, it delivers a speedup of 37× over Google’s Tensor-
Network library on redundancy-rich circuits, and 25× and 144×
over quantum multi-valued decision diagram and prior tensor
decision diagram implementation, respectively, on Google random
quantum circuits. To achieve this, we introduce a new linear-
complexity rank simplification algorithm, Tetris, and edge-centric
data structures for recursive tensor decision diagram operations.
Additionally, we explore the efficacy of tensor network contraction
ordering and optimizations from binary decision diagrams.
Index Terms—Quantum circuit simulation, tensor decision di-
agrams, binary decision diagrams, tensor networks.
I. INTRODUCTION
Quantum circuit simulation (QCS) is essential for various
aspects of quantum computing research and development, in-
cluding software development and debugging [1], noise simula-
tion [2], [3], quantum chip validation [4], and hybrid quantum-
classical algorithms [5]. Meanwhile, QCS has inherent scala-
bility limitations due to its exponential complexity, particularly
when dealing with deep and random circuits [4]. To overcome
the obstacle, researchers have been actively seeking solutions
to reduce the otherwise unacceptably long runtime. Among the
solutions are Schr¨odinger-style methods that utilize full vector
and matrix representations [1], [6], [7] as well as tensor network
(TN) approaches [8]–[10], with the latter demonstrating a clear
advantage via a range of optimizations to improve concurrency
and reduce complexity, such as TN partitioning and index
slicing [10], tensor decomposition [11], hyper-edges [10], and
contraction ordering [12].
While significant progress has been made in recent years,
matrix and tensor still contain a considerable amount of redun-
dancy when using arrays [13], [14]. In response to this, a small
but growing number of studies have been looking at decision
diagrams as a promising alternative to arrays. For example,
quantum information decision diagram (QuIDD) [15] and quan-
tum multi-valued decision diagram (QMDD) [13], [16], [17]
compress arrays by encoding them as directed acyclic graphs
(DAGs). This indeed reduces redundancy, which is the primary
objective. However, QuIDD and QMDD are specifically devel-
oped for Schr¨odinger-style methods, and cannot leverage the
numerous TN optimizations that are readily available.
More recently, tensor decision diagram (TDD) has emerged
as a solution that effectively merges the strengths of both TN
and DD approaches. TDD builds upon TN formalism, so TN
optimizations are still applicable, and extends the idea of DD to
compress tensor arrays. Despite its theoretical promise, existing
implementation of TDD [14] (referred to as PyTDD for the
rest of the paper) has not yet provided compelling results to
demonstrate their practical superiority. The reason for this per-
formance gap can be attributed to the limited exploration of the
potential benefits from complexity-reducing TN optimizations
in this specific context, as well as the software’s inefficient
implementation in Python.
QASM file or 
IBM Qiskit 
QuantumCircuit
Circuit-to-TN 
Transpilation 
Tensor 
Network
Rank Simplification 
through Tetris
Contraction Order 
Search
Identifying 
Hyper-edges
Order Finding 
through Cotengra
TDD-based 
Contraction for TN
Optimized TN
PyBind11
TDD-to-Tensor 
Conversion
 Measure Samples
Access QCS Result
PyBind11
Python
C++
Optional
Return Amplitudes
Fig. 1. Overview of the presented FTDD framework.
To fully harness the capabilities of TDD, we present fast
tensor decision diagram (FTDD). FTDD is an open-source
QCS framework that is primarily developed in C++ and in-
corporates a range of optimizations, drawing from both the
novel techniques introduced in this paper and the extensive TN
and DD literature. Fig. 1 provides an overview, distinguishing
Python in blue and C++ in yellow. FTDD comprises two
steps: the first step involves TN optimizations as preprocessing,
and the second step focuses on TDD optimizations. For TN
optimizations, it begins by constructing a TN, then applies
a novel linear-complexity algorithm called Tetris for rank
simplification, followed by an exploration of near-optimal TN
contraction orders. In the second step, the information extracted
from the previous step is utilized to execute near-optimal
contraction orders for the rank-simpilifed TN using TDDs.
arXiv:2401.11362v1  [quant-ph]  21 Jan 2024
To optimize TDD, we propose edge-centric data structures
aimed at eliminating overhead in recursive TDD operations and
incorporate key optimizations borrowed from binary decision
diagram (BDD) [18], [19] for further efficiency.
To test our hypotheses, we compare FTDD to PyTDD,
QMDD, and Google’s TensorNetwork library (GTN) on var-
ious quantum circuits. On Google random quantum circuits
(RQCs) [20], FTDD achieves a 144× speedup over PyTDD and
a 25× speedup over QMDD. This makes FTDD the fastest DD-
based QCS framework to date. On a collection of redundancy-
rich circuits from MQT Bench [21], FTDD achieved an average
speedup of 37× over GTN. FTDD software and test cases are
publicly available at https://github.com/QiruiZhang/FTDD.
II. BACKGROUND
This section provides an overview of the fundamentals of
TN and TDD. Additionally, we introduce an example quantum
circuit, Fig. 3, borrowed from a Google RQC instance [20].
This circuit, or parts of it, will serve as a recurring illustration
throughout the rest of the paper. For quantum computing basics,
we recommend consulting [22].
A. Quantum Circuits as Tensor Networks
A tensor is a multi-dimensional array of complex num-
bers [11]. In visual representation, a tensor takes the form of
a shape with edges, where each edge represents a dimension
identified by an index. Each index can take one of D different
values (0, 1, ..., or D − 1). When simulating a qubit, these
indices adopt two distinct values, 0 or 1, as shown in Fig. 2
(a). The rank of a tensor corresponds to the number of different
dimensions it possesses. Fig. 2 (b) shows how the Hadamard
gate (H) matrix is mapped to a rank-2 tensor, and Fig. 2 (c)
shows how the controlled-Z gate (CZ) matrix is mapped to
a rank-4 tensor. For CZ, i0 and j0 are respectively input and
output for the control qubit, while i1 and j1 are input and output
for the target qubit.
The rank and complexity of tensors can be reduced, as is
shown in Fig. 2 (d), by exploiting diagonality and merging
indices that are always equal into hyper-edges [10]. In other
words, if the indices are equal, the tensor value is non-zero. For
example, CZ is non-zero only when i0 equals j0 and i1 equals
j1. By exploiting hyper-edges, the rank of CZ is reduced from
four to two, decreasing computation during QCS. Hyper-edges
are illustrated with dotted lines.
A tensor network is a graph of tensors connected through in-
dices. Contraction is the fundamental operations for TN-based
QCS; it applies quantum gates to quantum states and calculates
the inner product of tensors over shared indices. Quantum
circuits inherently exhibit a TN structure. When simulating
a quantum circuit as a TN, all the shared indices between
the quantum state and gate tensors are contracted, resulting
in a single tensor indicated by open indices, such as index
k0 in Fig. 3. A tensor network before and after contracting
the tensors highlighted in green is shown in Fig. 3, for which
hyper-edges have been exploited. Hyper-edges that extend to
the open indices (e.g., index k0 of T0) are not considered as
q
j
α 
β 
qvec = 
j
0
1
1
Hmat = 
i
j
j
0
1
0
1
i
1
0
0
1
CZmat = 
j0
1
1
0
1
i1
0 
0
0
0
0 
0
0
0
1
0
0
-1
0
0
i0
0
1
1
1
j1
0
1
0 1
0 0
i0
i1
j0
j1
vector
(a)
(b)
(c)
rank-1 tensor symbol
matrix
matrix
rank-2 tensor symbol 
rank-4 tensor symbol
1
1
-1
H
 1  
√2 
CZ
1
CZmat  = 
i1
0
1
0
1
i0
matrix
1
1
-1
i0
i1
(d)
rank-2 tensor symbol
CZ
i0
i1
Exploit
Hyper-
edges
qj = qvec[ j ]
Hj, i = Hmat[ j ][ i ]
CZj0, j1, i0, i1 = 
CZmat[ j0j1 ][ i0i1 ]
mapping
mapping
mapping
CZi0, i1 = 
CZmat[ i0 ][ i1 ]
mapping
Fig. 2. Qubit and quantum gates represented in matrices and tensor symbols:
(a) single-qubit; (b) H gate; (c) CZ gate; (d) CZ using hyper-edges.
CZ1
CZ0
q0
H0
q1
H1
q2
H2
q3
H3
T2
T3
RX
RY
T0
i0
i1
i2
i3
j3
j2
j1
j0
k0
k1
j2
j3
j2
j1
j0
k0
k1
CZ1
TS1
q0
q1
q2
q3
i0
i1
i2
i3
k0
k1
j2
j3
j2
k1
H2
H3
T2
T3
j3
j2
Fig. 3. The example quantum circuit represented as a tensor network before
and after contracting the tensors highlighted in green. RX, RY and T are
common single-qubit rotation gates.
shared indices. The contraction for the highlighted tensors of
Fig. 3, tensor TS1, is mathematically described in Eq. 1.
TSk0,k1
1,i0,i1 =
X
j0,j1
Hj0
0,i0Hj1
1,i1CZ0,j0,j1Rk0
X,j0Rk1
Y,j1T0,k0
(1)
In practice, tensors are rarely contracted all at once. A usual
approach involves contracting two tensors at a time. When these
tensors are contracted in the sequential order, i.e., qubit by
qubit and layer by layer, TN-based QCS essentially mirrors
the Schr¨odinger-style simulation. However, as will be detailed
in III-B, one of the key advantages of TN lies in its capability to
perform tensor contractions in arbitrary orders, which can lead
to significant reduction in computation. For a more in-depth
understanding of TN, we recommend consulting [10]–[12].
B. Tensors as Tensor Decision Diagrams
TDDs [14] are DAGs that serves as a compact representation
for tensors. They can be considered as a variant of reduced-
ordered binary decision diagrams (ROBDD) [18], with directed
edges weighted in complex numbers, essentially functioning as
a mapping from binary index values to corresponding complex
tensor values. To evaluate the tensor value for certain index
values, we start from the root edge and traverse the TDD. At
each node, we take the left (right) edge to traverse to a child
node if the corresponding index is 0 (1). This process continues
until we reach a leaf node, and the tensor value is the product
of all the edge weights that we have traversed.
i1
1
1
1
-1
j1
i0
j0
i1
j0
0
0
i1
1/2
0
j0
i0
i0
j1
j1
j1
j1
1
2 
j0
i0
1
1
1
1
-1
Normalize 
and Reduce
H1
H0
1/2 -1/2
1/2
1/2
1/2 -1/2
1/2
1/2
1/2 -1/2
1/2
-1/2
-1/2 1/2
-1/2
1
0
0
1
1
1
i0
0
1
1
j1
0
1
0
1
i1
i1
i1
i1
…… ……  
1
2 
1
2 
-1
2 
-1
2 
-1
2 
-1
2 
1
2 
…… ……  
…… ……  
j1
1
2 
(a)
(b)
Fig. 4.
(a) The tensor H ⊗ H and its array; (b) Its TDD before and after
normalization and reduction. A red (blue) edge indicates an index of 0 (1).
Fig. 4 shows how the tensor product of two H gates is
converted to TDD. The array (Fig. 4 (a) right) is first Shannon-
expanded into a binary tree (Fig. 4 (b) left). Then, the nodes are
normalized and reduced level by level, starting from the leaves
and moving up to the root. At each node, the normalization
is performed by dividing the two output edge weights by the
one with the larger magnitude. The resulting divider is then
propagated up to the node’s incoming edge and multiplied
with the corresponding weight. After normalization, reduction
begins by checking if the normalized node already exists in the
unique table, a hash table for storing nodes uniquely. If it does,
the node is linked to its parent node. Otherwise, a new node
is created in the unique table. For example, after normalization
and reduction, circled nodes in Fig. 4 (b) (left) are merged into
the circled node in Fig. 4 (b) (right). This reduces the number
of nodes for H⊗H from 16 to 4. TDD supports all the essential
tensor operations utilized in QCS through recursion, including
contraction and addition. For a comprehensive understanding
of TDD, we recommend consulting [14].
III. TENSOR NETWORK OPTIMIZATIONS
A. Efficient Rank Simplification with Tetris
In the context of TNs, rank simplification is an effective
preprocessing step [12] that reduces the number of gates
to be simulated by consolidating tensors in a local manner.
Specifically, it involves identifying groups of adjacent tensors
in a TN and contracting the tensors in each group, under
the constraint that for each group, the rank of the resulting
tensor does not exceed the ranks of any contracted tensors.
Unfavorably, a TN is typically supplied as a list of tensors
without locality information, and rank simplification therefore
entails a brute-force iteration through the tensor list. For each
tensor, the rest tensors are further iterated and checked against
it one by one, and are contracted with it only if they are
adjacent and follow the constraint. This approach results in a
computational complexity of O(T 2), where T is the number of
tensors. However, DD-based quantum states can exhibit linear
complexity O(n), where n is the number of qubits. With at most
T gates to be applied, simulating the rank-simplified circuit can
results in a complexity of O(nT). Considering that T is usually
much larger than n, the rank simplification preprocessing could
take longer than the simulation itself.
We notice that the brute-force approach mentioned above
assumes that TNs have arbitrary structures, which is not always
the case and can result in unnecessary overhead. We contend
that the sequential layer-by-layer structure of quantum circuits
can be leveraged to minimize rank simplification overheads.
In sequential structures, rank simplification is restricted to
gates that are from adjacent layers and share qubits. Using
this insight, we developed Tetris, an algorithm designed to
achieve rank simplification in quantum circuits with only linear
complexity, O(T). The name of the algorithm is inspired by
the popular game Tetris. In Tetris, various-shaped bricks, known
as tetrominoes, descend and stack on the ground. When rows
of bricks are completely filled, they merge and disappear.
Intriguingly, our algorithm follows a similar pattern.
   
     CZ1
   
     CZ0
H0
H1
H2
H3
T2
T3
RX
RY
T0
q0
q1
q2
q3
Layer-1
Layer-2
Layer-3
Layer-4
   
     CZ1
   
     CZ0
H0
H1
H2
H3
T2
T3
RX
RY
T0
q0
q1
q2
q3
Layer-1
Layer-3
Layer-4
   
     CZ1
   
     CZ0
H0
H1
H2
H3
T2
T3
RX
RY
T0
q0
q1
q2
q3
(c)
Layer-1
Layer-4
   
     CZ1
   
     CZ0
H0
H1
H2
H3
T2
T3
RX
RY
q0
q1
q2
q3
Layer-1
Layer-4
T0
TS1
TS2
TS3
TS2a
TS1a
TS3
TS1b
TS2a
TS3
Tensor
Stacks
TS1a
TS1a
TS2a
TS3
Tensor
Stacks
H0
H1
H2
H3
Tensor
Stacks
TS1b
TS1b
TS2a
TS3
Tensor
Stacks
TS1
TS1
TS2
TS3
TS2
(a)
(b)
(d)
Fig. 5. Applying our Tetris algorithm to the example circuit of Fig. 3. Panels
(a) to (d) depict the transition from the original circuit to the simplified version
as we proceed through a layer-by-layer process, commencing from layer 2.
Fig. 5 illustrates the proposed gradual rank simplification
algorithm for the previously introduced example circuit, with
quantum gates resembling tetrominoes. It initiates by establish-
ing tensor stacks shown in Fig. 5 (a), one for each qubit line,
containing consolidated tensors along their respective qubit
lines. Subsequently, the circuit is systematically examined,
progressing through its layers in a sequential manner. Within
each layer, the gates are handled as if they were descending
along the qubit lines, akin to tetrominoes in the game of
Tetris. Every gate is examined in relation to the top tensors
in the stacks corresponding to its associated qubits. If there is
qubit sharing between the gate and the top tensors, and the
rank simplification constraint is met, the gate is consolidated
with the top tensors via a contraction, and the outcome tensor
replaces the top tensors. Otherwise, the gate is pushed to the
top positions in the stacks. For example, CZ1 and TS2a in
Fig. 5 (c) can be consolidated into TS2 in Fig. 5 (d), while
TS1 and TS2 are not further consolidated, which violates the
constraint. After the last layer is considered, the rank-simplified
circuit is retrieved by popping the bottom tensors from the
stacks layer-by-layer, with duplicated ones discarded. For the
example circuit, Tetris effectively reduces the number of gate
tensors from 11 to 3.
B. Execution of Near-Optimal Contraction Order
In a TN, the order in which tensors are contracted can vary,
but not all ordering choices yield the same level of efficiency.
Fig. 6 shows two different contraction orders for the rank-
simplified example circuit. In Fig. 6 (a), the sequential order
is applied, and immediately after the first layer, intermediate
tensor rank grows to four. On the other hand, Fig. 6 (b) shows
a more efficient order where contractions that results in larger
tensors are delayed as much as possible, and the rank of
intermediate tensor only grows to four at the last step. With the
more efficient order, the number of floating point operations
(FLOP) using arrays to simulate the rank-simplified example
circuit is reduced from 1,120 to only 432.
TS1
q0
q1
q2
q3
i0
i1
i2
i3
k0
k1
j3
j2
k1
TS3
TS2
TS1
i0
i1
i2
i3
k0
k1
j3
j2
k1
TS3
TS2
Ф1 
k0
k1
j3
Ф2 
k1
j2
k1
TS2
i2
k0
j3
Ф3 
j2
k1
TS1
q0
q1
q2
q3
i0
i1
i2
i3
k0
k1
j3
j2
k1
TS3
TS2
TS4
k0
k1
j3
j2
k1
TS6
TS5
k0
k1
j3
j2
k1
TS7
k0
j3
Ф3 
j2
k1
TS6
(a) The sequential contraction order: Qubit-by-qubit and layer-by-layer 
⟹ 1120 FLOP (144 Complex-Mult + 128 Complex-Add) with array-based tensors  
(b) A more efficient contraction order: i0→i1→i2→i3→k1→TS7⊗TS6 
⟹ 432 FLOP (60 Complex-Mult + 36 Complex-Add) with array-based tensors
Fig. 6. Different contraction orders for the rank-simplified example circuit.
As the rank of a tensor aligns with the number of levels
in its TDD representation, we hypothesize that optimizing the
order of contractions can also lead to complexity reduction
in TDD-based TNs. This reduction occurs by minimizing the
number of levels and, consequently, the number of nodes within
intermediate TDDs. Taking advantage of this opportunity, we
introduce the concept of executing near-optimal contraction
orders for TDD-based TNs. This represents the first application
of such optimization techniques to decision diagrams that are
compatible with TN formalism. In our practical implementa-
tion, we rely on Cotengra [12] as the backend tool to search
for these near-optimal TN contraction orders. Additionally,
we combine contraction ordering with TDD and our Tetris
algorithm to synergistically reduce the complexity of quantum
circuit simulation even further. Fig. 7 illustrates how TDDs
decrease the number of FLOPs in array-based QCS from
2,184 to 1,372. Application of the Tetris algorithm goes a
step further, reducing the number of contraction steps from
14 to 6 and decreasing FLOPs from 1,372 to 706. Ultimately,
when employing the contraction order in Fig. 6 (b), the FLOP
count for TDD-based contraction is minimized to just 534. In
summary, the proposed approach results in a 76% reduction in
computation for the example circuit.
706 FLOP
534 FLOP
1372 FLOP
2184 FLOP
Fig. 7. FLOP progressions for the example circuit with different methods.
IV. TENSOR DECISION DIAGRAM OPTIMIZATIONS
To simulate the final quantum state, the TN resulting from
the above optimizations must be contracted globally. To exploit
redundancy and reduce computation, we encode tensors as
TDDs for the global contraction. In this section, we present
our sequence of TDD optimizations, implemented in C++. The
prior TDD work, PyTDD [14], is based on Python. Our results
show that the transition from Python to C++ alone yields a
speedup of ∼2.5× on Google RQCs.
A. Edge-centric Data Structures for TDD
TDD contraction begins with a one-time preparation of con-
trol variables, followed by a recursive process outlined in Fig. 8
(a). During recursion, frequent manipulation of intermediate
TDDs is the primary source of latency. Therefore, optimizing
the underlying data structure designs for TDDs is crucial.
Fig. 8 (b) (left) shows the encoding of TDDs in TDD objects,
as in PyTDD [14]. Each TDD object contains a root node,
incoming edge weight, and auxiliary variables for managing
tensor indices (idx list) and index-to-level correspondences
(idx lvl and lvl idx). We notice that while auxiliary variables
are essential for preparing the control variables, they become
unnecessary during recursion and possess cumbersome sizes
that scale linearly with the number of indices.
i3
1
1
-1
1
j3
√2
2 
i3
1
1
0
1
e 
iπ/4  
contract()
i3
1
1
0
1
1
1
i3
1
1 -1
e 
iπ/4  
q3=|0>
TS3
TS3, j3=0
TS3, j3=1
j3
√2
2 
contract()
i3
1
1
0
1
contract()
The winding phase
The unwinding phase
contract(q3, TS3) called
q3=|0>
q3=|0>
1
1
e 
iπ/4  
1
1
1
e 
iπ/4  
j3
√2
2 
Norm.
j3
Result TDDs 
are inserted 
into computed 
table
Nodes are 
searched in 
unique table
TDDs are 
sliced or copied 
recursively
√2
2 
class TDD:
  list<string> idx_list
  map<int, str> lvl_idx
  map<str, int> idx_lvl
  node* root_node
  complex weight 
class node:
  int lvl
  list<complex> weights
  list<node*> children 
class TDD:
  list<string> idx_list
  map<int, str> lvl_to_idx
  map<str, int> idx_to_lvl
  edge root_edge
class node:
  int lvl
  list<edge> edges
class edge:
  complex weight
  node* node_ptr
Data structure designs in prior TDD work
FTDD: Edge-centric data structure designs
- TDD objects are directly manipulated during recursion
- Unused auxiliary variables induce significant overhead
- Roots extracted from TDD objects to start recursion
- Light-weight edges are manipulated during recursion
(b)
(a)
- Aux. var. unused for recursion 
- Sizes scale linearly with # indices
- E.g., >20× overhead for 20 indices
Fig. 8. (a) Illustration of the recursive process in TDD contraction using q3
and TS3 from the rank-simplified example circuit; (b) Comparison of data
structure designs between the prior TDD work and FTDD.
With that observation, we identify that a major inefficiency
is the direct use of TDD objects during recursion. As Fig. 8
(a) illustrates, the recursion involves two phases: winding and
unwinding. During winding, intermediate TDDs are sliced into
child TDDs to incur downstream calls, where TDDs without
nodes at a level are copied. For example, the TS3 TDD is
split into two children at level j3, while q3 TDD is copied
since it has no nodes at that level. However, the massive
auxiliary variables are also unnecessarily copied, inducing
significant overheads. During unwinding, recursive calls return
level by level from bottom up. For each return, intermediate
result TDDs are inserted into the computed table, which stores
computed TDDs to be reused by later calls with the same
input operands. However, for this purpose, only the root node
and incoming edge weights are required. Using TDD objects,
useless auxiliary variables are also stored in the computed table,
inducing undesired overheads for the access of the computed
table and memory size.
To address the overhead issues, we propose edge-centric data
structures for TDD. Shown in Fig. 8 (b) (right), we introduce
the edge objects in FTDD to replace TDD objects throughout
recursion. To initiate recursion, root edges are extracted from
operand TDD objects to encode TDDs. During recursion, only
lightweight edge objects are manipulated, and computed tables
only store edge objects, completely removing the involvement
of auxiliary variables during recursion. Additionally, a node
object is simplified to store only one list for edges, instead of
two lists for output edge weights and children nodes, reducing
unique table access overhead.
B. Employment of Key BDD Optimizations
While specialized for tensors, TDD still shares common
building blocks with the other decision diagrams. Here, we
explore the connection between TDD and the wealth of prior
DD research for the first time. We find that TDD can effectively
benefit from key BDD [19] optimizations that are essential for
building efficient unique tables and computed tables. However,
it is worth noting that certain BDD optimizations such as
complement edges and standard triple are specific to Boolean
functions, and do not apply to TDD.
FTDD incorporates three BDD optimizations. First, FTDD
adopts merged-DAG unique table, where the memory for stor-
ing all TDD nodes is augmented with hashing support and
can be repurposed as the unique table, eliminating memory
overheads from storing duplicated nodes in a separate unique
table. Second, FTDD supports garbage collection to manage
memory efficiently. It counts references to each node and frees
up unused memory. After each contraction, the reference counts
for all nodes are updated, and when the unique table’s node
count exceeds a certain limit, unreferenced nodes are identified
and removed. Lastly, FTDD employs caching computed table,
which simplifies computed tables from size-varying hash tables
into fixed-size caches addressed by hashes. Upon cache miss,
incoming data replaces existing data instead of being appended.
This approach minimizes computed table overheads by control-
ling its size to match actual needs.
Algorithm 1 FTDD tensor network contraction
Require: unique table, tensor list, order
Ensure: tdd res
tdd list ← empty list of TDD
for tensor in tensor list do
▷ Convert tensors to TDDs
tdd tmp ← tensor to tdd(tensor)
tdd list.append(tdd tmp)
unique table.inc ref cnt(tdd tmp.root edge)
end for
for pair in order do
▷ Contraction with the given order
tdd a ← tdd list[pair.first]
tdd b ← tdd list[pair.second]
tdd c ← contract(tdd a, tdd b)
unique table.dec ref cnt(tdd a.root edge)
unique table.dec ref cnt(tdd b.root edge)
unique table.inc ref cnt(tdd c.root edge)
tdd list.remove(tdd a)
tdd list.remove(tdd b)
tdd list.append(tdd c)
if unique table.size > unique table.size limit then
unique table.garbage collection()
end if
end for
tdd res ← tdd list[0]
Combining aforementioned TN and TDD optimizations, Al-
gorithm 1 illustrates the contraction of a tensor network in
FTDD. The process begins with the initialization of a TDD
list by converting tensors in a TN into TDDs. As the TDDs
are newly created, reference counts of nodes are incremented
for each of them. Then, the TDDs are contracted based on
a specified order presented as a sequence of pairs. Each
pair denotes the locations of tensors to be contracted in the
dynamically updated TDD list. Following each contraction, the
operand TDDs are removed from the list, and the result TDD
is appended to it. Since the removed TDDs are essentially de-
referenced, the reference counts of their nodes can be safely
decremented. For the newly created result TDD, reference
counts of its nodes are incremented. Garbage collection is
triggered if the unique table size exceeds a user-defined limit.
Throughout the process, TDD list length steadily decreases
until reaching one after the final contraction, where the sole
remaining TDD represents the QCS result.
V. EVALUATION
A. Setup
All our experiments were run on a single thread on a
workstation with Intel® Xeon® Gold 6254 CPUs and 240 GiB
DRAM running Ubuntu 20.04 LTS. Cotengra is configured to
conduct 512 searches for each TN.
B. Benchmarks
For comprehensive benchmarking, we consider a wide range
of quantum circuits from MQT Bench [21] and Google [20], in-
cluding both circuits that exhibit rich redundancy, such as GHZ,
graph state, and quantum Fourier transform (QFT), and hard-
to-simulate circuits like entangled-QFT and Google RQCs. For
QFT, we follow [14] and conduct its unitary simulation, which
treats the input qubits as open indices and results in rank-
2n tensors for n qubits. Additionally, we benchmark quantum
approximate optimization algorithm (QAOA) and variational
quantum eigensolver (VQE), as they are representative of noisy
intermediate-scale quantum (NISQ) [23] applications.
C. Unique Table and Computed Table Statistics
The incorporation of garbage collection and caching com-
puted tables in FTDD represents significant modifications to
the original PyTDD design. To ensure that these changes
do not adversely impact the hit rates of unique table and
computed tables, potentially leading to performance degrada-
tion, we verified the table statistics of FTDD against PyTDD
on Google RQCs. Table. I presents the hit rates of unique
table, contraction computed table, and addition computed table
for PyTDD and FTDD across four 16-qubit Google RQCs,
executing TN contractions in sequential orders. The results
indicate a negligible difference in hit rates between PyTDD
and FTDD. Intriguingly, the unique table hit rates of FTDD
are occasionally slightly higher than those of PyTDD.
D. Ranks Simplification Overhead with Tetris
Table II shows Tetris runtime for all the benchmarks. With
O(T) complexity, Tetris only induces 4% runtime overhead on
average when compared to FTDD runtime with TN optimiza-
tions, demonstrating its efficiency. Notably, for 20-qubit Google
RQCs, Tetris has a negligible 0.05% runtime overhead.
E. Comparison Against DD-based QCS Approaches
For DD-based approaches, we first experiment with the
speedup of FTDD over PyTDD [14] using incremental combi-
nations of FTDD optimizations, on four 16-qubit Google RQCs
of different depths. Fig. 9 shows the results of this experiment.
As Section IV mentions, simply transpiling Python to C++
only gives ∼2.5× speedup on average. With edge-centric data
structure designs, the average speedup for the 16-qubit RQCs is
increased to 7.3×. Employing key BDD optimizations further
boosts the average performance to be 19× faster. With Tetris,
75% of the gates can be reduced on average, marching the
speedup to 40×. Finally, executing near-optimal contraction
orders found through Cotengra for the rank-simplified circuits
propells FTDD performance to an astounding average speedup
of 178× compared to PyTDD. We further compare FTDD
against PyTDD for all benchmarks. Table II shows the runtime
of FTDD and PyTDD across the benchmarks. Table III shows
the average speedup of FTDD over PyTDD for each type of
circuits. With TN optimizations, FTDD achieves speedup over
PyTDD on all the benchmarks. Remarkably, on the hardest
benchmark, 20-qubit RQCs, FTDD exhibits 143.7× speedup
over PyTDD with the TN optimizations applied. Optimized
FTDD also shows an impressive 74.8× speedup over PyTDD
on QAOA, a NISQ application. In addition, PyTDD ran into
division-by-zero errors when simulating VQE, indicating that
FTDD may offer improved robustness.
7.3×
19×
40×
178×
Fig. 9. FTDD speedup with successive optimizations on 16-qubit RQCs.
Then, we compare FTDD with QMDD [13], [24], the
state-of-the-art DD for QCS. We configure QMDD to use
its qasm simulator, ensuring that its runtime does not in-
volve the final DD-to-array conversion. Additionally, we allow
QMDD to gain speedup using its quantum state approximation
techniques [25], but with fidelity no lower than 99.9999%
for fairness, since that is the lowest fidelity FTDD achieves
when being verified against IBM Qiskit simulation on the
benchmarks. Table
II presents QMDD runtime across all
benchmarks, while Table III shows the average speedup of
FTDD over QMDD for each type of circuits. FTDD achieves
speedup over QMDD across all the benchmarks when TN
optimizations are applied, establishing FTDD the fastest DD
framework for QCS to date. For 20-qubit Google RQCs, FTDD
exhibits 4.3× speedup over QMDD even without leveraging TN
optimizations. With TN optimizations applied, FTDD achieves
24.5× speedup over QMDD. Furthermore, FTDD shows an
impressive 44.6× speedup over QMDD on VQE. Notably,
QMDD ran into segmentation faults when simulating QAOA,
suggesting that FTDD may also offer better robustness.
TABLE I
UNIQUE TABLE AND COMPUTED TABLE HIT RATES OF PYTDD AND FTDD ON 16-QUBIT GOOGLE RQCS
Circuit
Unique Table Hit Rate
Contraction Computed Table Hit Rate
Addition Computed Table Hit Rate
PyTDD
FTDD
PyTDD
FTDD
PyTDD
FTDD
inst 4x4 10 8
27.52%
27.82%
29.40%
29.38%
5.52%
5.46%
inst 4x4 12 8
8.06%
8.14%
35.69%
35.68%
1.77%
1.72%
inst 4x4 14 8
5.57%
5.62%
30.41%
30.41%
1.17%
1.14%
inst 4x4 16 8
5.52%
5.55%
28.58%
28.58%
0.80%
0.78%
TABLE II
COMPARISON BETWEEN FTDD AND STATE-OF-THE-ART ARRAY-BASED AND DD-BASED QCS METHODS
Benchmarks
Google TensorNetwork
QMDD
PyTDD
FTDD
Circuit
# qubits
# gates
seq. time (s)
opt. time (s)
time (s)
time (s)
seq. time (s)
Tetris time (s)
opt. time (s)
final # nodes
GHZ
25
25
16.2
0.31
0.044
0.032
0.037
0.002
0.03
50
GHZ
26
26
32.86
0.66
0.044
0.034
0.037
0.002
0.03
52
GHZ
27
27
67.54
1.43
0.044
0.036
0.039
0.002
0.03
54
GHZ
28
28
140.95
2.29
0.044
0.038
0.039
0.002
0.031
56
Graph state
25
50
28.02
0.31
0.075
0.42
0.081
0.001
0.038
807
Graph state
26
52
57.53
0.61
0.076
0.47
0.089
0.001
0.037
835
Graph state
27
54
117.6
1.39
0.077
0.3
0.073
0.002
0.036
663
Graph state
28
56
251.47
2.58
0.079
0.48
0.095
0.001
0.041
1359
QFT (unitary)
13
91
56.4
6.0
1.13
6.68
0.67
0.004
0.15
16383
QFT (unitary)
14
105
261.68
22.34
4.21
14.11
1.35
0.005
0.4
32767
QFT (unitary)
15
120
out of mem.
out of mem.
16.51
28.79
3.26
0.007
0.86
65535
QFT (unitary)
16
136
out of mem.
out of mem.
67.88
55.46
9.04
0.008
2.64
131071
Entangled-QFT
14
126
0.049
0.025
1.93
4.01
0.27
0.008
0.074
16384
Entangled-QFT
15
142
0.07
0.034
18.66
7.89
0.49
0.011
0.11
32768
Entangled-QFT
16
160
0.11
0.053
166.98
15.37
0.92
0.014
0.19
65536
Entangled-QFT
17
178
0.21
0.1
1461.96
29.33
1.79
0.017
0.37
131072
QAOA
12
60
0.019
0.005
seg. fault
5.35
0.3
0.002
0.1
3078
QAOA
13
65
0.02
0.006
seg. fault
12.23
0.65
0.002
0.22
6203
QAOA
14
70
0.024
0.005
seg. fault
1.84
0.14
0.002
0.014
512
QAOA
15
75
0.036
0.007
seg. fault
47.88
2.66
0.002
0.8
24709
VQE
16
78
0.05
0.007
1.61
div. zero
0.36
0.002
0.33
36820
VQE
17
83
0.091
0.008
0.43
div. zero
0.033
0.003
0.02
837
VQE
18
88
0.18
0.008
6.85
div. zero
0.062
0.003
0.049
5085
VQE
19
93
0.34
0.011
0.49
div. zero
0.054
0.003
0.036
853
RQC (inst 4x5 10 8)
20
145
1.32
0.014
49.08
828.12
43.7
0.004
3.23
454658
RQC (inst 4x5 12 8)
20
168
1.46
0.023
416.95
2091.5
106.76
0.005
13.58
1048576
RQC (inst 4x5 14 8)
20
192
1.78
0.055
790.89
2863.04
153.08
0.006
37.41
1048576
RQC (inst 4x5 16 8)
20
215
2.13
0.081
1274.46
>3600
183.54
0.008
41.15
1048576
seq. time - Simulating original circuits in sequential contraction orders, opt. time - Best runtime when considering all possible combinations of TN optimizations
TABLE III
AVERAGE SPEEDUP OF FTDD OVER PYTDD AND QMDD
Benchmarks
FTDD seq. time
FTDD opt. time
vs. PyTDD
vs. QMDD
vs. PyTDD
vs. QMDD
GHZ
0.9×
1.2×
1.2×
1.5×
Graph state
4.9×
0.9×
11.0×
2.0×
QFT (unitary)
8.9×
4.4×
33.2×
15.7×
Ent. QFT
16.2×
261.7×
72.0×
1248.5×
QAOA
17.1×
N/A
74.8×
N/A
VQE
N/A
34.0×
N/A
44.6×
RQC
19.2×
4.3×
143.7×
24.5×
For circuits such as GHZ and graph state, FTDD demon-
strates only a moderate speedup over PyTDD and QMDD.
This is primarily due to the substantial redundancy presented in
these circuits. The final number of nodes in Table II indicates
that their quantum states tend to display near-linear complexity.
Consequently, the runtime can be influenced more by the
overhead from auxiliary steps, such as the initial conversion
of gate tensors from arrays to TDDs, rather than the execution
of TDD contractions. Importantly, these auxiliary steps do not
scale with the proposed optimizations.
F. Comparison Against Array-based TN Approach
Finally, we compare FTDD with GTN [26], the state-of-
the-art array-based TN simulator. We configured GTN to use
PyTorch as the backend and limited it to also use only a single
thread for a fair comparison. However, it is important to note
that we are not able to disable PyTorch’s vector execution, as it
is compiled into the software. As Tetris and Cotengra are also
applicable to array-based TNs, we also present GTN results
both with and without the TN optimizations.
In the cases of hard-to-simulate circuits like entangled-QFT
and RQCs, Table II shows that FTDD can have a longer runtime
than GTN. One reason is that GTN takes advantage of vector
execution, while FTDD does not employ any concurrency.
Moreover, the computational cost of manipulating a TDD
node and its edges is higher than simply computing an array
entry. For hard-to-simulate circuits, TDDs cease to exploit
redundancy as circuits reach certain depths and eventually
expand to full binary trees with exponentially many nodes,
the same complexity as arrays, leading to overall performance
degradation. However, FTDD demonstrates superiority for the
redundancy-rich circuits. As Table II shows, the final FTDD
node counts for final quantum states of GHZ and graph state cir-
cuits demonstrate that they only exhibit near-linear complexity
when using FTDD. For the final quantum states of QFT unitary
simulation, array-based simulation imposes O(22n) complexity.
Using FTDD, this complexity can be exponentially reduced
to only O(2n), as is indicated by the final node counts for
QFT unitary simulation. For GHZ, graph state and QFT unitary
simulation, FTDD achieves 37× speedup on average over GTN,
when both approaches exploit TN optimizations.
VI. CONCLUSION
This paper presents FTDD, an open-source QCS framework
that incorporates a novel TN rank simplification algorithm
and near-optimal contraction order search as preprocessing
steps, and TDDs along with BDD optimizations for the global
contraction. FTDD implements TDD operations in C++ for
efficiency, and introduces edge-centric data structures to re-
place the cumbersome structures used in prior TDD approach,
which carries redundant auxiliary variables through recursive
operations. Our results demonstrate the effectiveness of FTDD,
achieving an average speedup of 37× over Google’s TensorNet-
work library on redundancy-rich circuits, and speedups of 25×
and 144× over QMDD and prior TDD approach, respectively,
on Google random quantum circuits.
REFERENCES
[1] X.-C. Wu, S. Di, E. M. Dasgupta, F. Cappello, H. Finkel, Y. Alexeev,
and F. T. Chong, “Full-state quantum circuit simulation by using data
compression,” in Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis (SC), Denver,
Colorado, USA, 2019, pp. 1–24.
[2] G. Li, Y. Ding, and Y. Xie, “Eliminating redundant computation in
noisy quantum computing simulation,” in 2020 57th ACM/IEEE Design
Automation Conference (DAC), San Francisco, CA, USA, 2020, pp. 1–6.
[3] P. Das, E. Kessler, and Y. Shi, “The imitation game: Leveraging CopyCats
for robust native gate selection in NISQ programs,” in 2023 IEEE
International Symposium on High-Performance Computer Architecture
(HPCA), Montreal, QC, Canada, 2023, pp. 787–801.
[4] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin, R. Barends,
R. Biswas, S. Boixo, F. G. Brandao, D. A. Buell et al., “Quantum
supremacy using a programmable superconducting processor,” Nature,
vol. 574, no. 7779, pp. 505–510, 2019.
[5] G. S. Ravi, P. Gokhale, Y. Ding, W. Kirby, K. Smith, J. M. Baker, P. J.
Love, H. Hoffmann, K. R. Brown, and F. T. Chong, “CAFQA: A classical
simulation bootstrap for variational quantum algorithms,” in Proceedings
of the 28th ACM International Conference on Architectural Support for
Programming Languages and Operating Systems (ASPLOS), Vancouver,
BC, Canada, 2022, pp. 15–29.
[6] Y. Zhao, Y. Guo, Y. Yao, A. Dumi, D. M. Mulvey, S. Upadhyay, Y. Zhang,
K. D. Jordan, J. Yang, and X. Tang, “Q-GPU: A recipe of optimizations
for quantum circuit simulation using GPUs,” in 2022 IEEE International
Symposium on High-Performance Computer Architecture (HPCA), Seoul,
Republic of Korea, 2022, pp. 726–740.
[7] I. L. Markov, A. Fatima, S. V. Isakov, and S. Boixo, “Massively
parallel approximate simulation of hard quantum circuits,” in 2020 57th
ACM/IEEE Design Automation Conference (DAC), San Francisco, CA,
USA, 2020, pp. 1–6.
[8] I. L. Markov and Y. Shi, “Simulating quantum computation by contracting
tensor networks,” SIAM Journal on Computing, vol. 38, no. 3, pp. 963–
981, 2008.
[9] B. Villalonga, S. Boixo, B. Nelson, C. Henze, E. Rieffel, R. Biswas,
and S. Mandr`a, “A flexible high-performance simulator for verifying
and benchmarking quantum circuits implemented on real hardware,” npj
Quantum Information, vol. 5, pp. 86–91, 2019.
[10] E. Pednault, J. A. Gunnels, G. Nannicini, L. Horesh, T. Magerlein,
E. Solomonik, E. W. Draeger, E. T. Holland, and R. Wisnieff, “Pareto-
efficient quantum circuit simulation using tensor contraction deferral,”
2020.
[11] R. Or´us, “A practical introduction to tensor networks: Matrix product
states and projected entangled pair states,” Annals of Physics, vol. 349,
pp. 117–158, 2014.
[12] J. Gray and S. Kourtis, “Hyper-optimized tensor network contraction,”
Quantum, vol. 5, pp. 410–431, 2021.
[13] A. Zulehner and R. Wille, “Advanced simulation of quantum compu-
tations,” IEEE Transactions on Computer-Aided Design of Integrated
Circuits and Systems, vol. 38, no. 5, pp. 848–859, 2019.
[14] X. Hong, X. Zhou, S. Li, Y. Feng, and M. Ying, “A tensor network
based decision diagram for representation of quantum circuits,” ACM
Transactions on Design Automation of Electronic Systems, vol. 27, no. 6,
pp. 1–30, 2022.
[15] G. F. Viamontes, I. L. Markov, and J. P. Hayes, “Improving gate-level
simulation of quantum circuits,” Quantum Information Processing, vol. 2,
pp. 347–380, 2003.
[16] D. M. Miller and M. A. Thornton, “QMDD: A decision diagram structure
for reversible and quantum circuits,” in 36th International Symposium on
Multiple-Valued Logic (ISMVL), Singapore, 2006, pp. 30–35.
[17] P. Niemann, R. Wille, D. M. Miller, M. A. Thornton, and R. Drechsler,
“QMDDs: Efficient quantum function representation and manipulation,”
IEEE Transactions on Computer-Aided Design of Integrated Circuits and
Systems, vol. 35, no. 1, pp. 86–99, 2016.
[18] R. E. Bryant, “Graph-based algorithms for boolean function manipula-
tion,” IEEE Trans. Comput., vol. C-35, no. 8, pp. 677–691, 1986.
[19] K. S. Brace, R. L. Rudell, and R. E. Bryant, “Efficient implementation
of a BDD package,” in 27th ACM/IEEE Design Automation Conference
(DAC), Orlando, FL, USA, 1990, pp. 40–45.
[20] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, R. Babbush, N. Ding, Z. Jiang,
M. J. Bremner, J. M. Martinis, and H. Neven, “Characterizing quantum
supremacy in near-term devices,” Nature Physics, vol. 14, no. 6, pp. 595–
600, 2018.
[21] N. Quetschlich, L. Burgholzer, and R. Wille, “MQT Bench: Bench-
marking software and design automation tools for quantum computing,”
Quantum, vol. 7, pp. 1062–1075, 2023.
[22] M. A. Nielsen and I. L. Chuang, Quantum computation and quantum in-
formation: 10th Anniversary Edition. Cambridge: Cambridge University
Press, 2010.
[23] J. Preskill, “Quantum computing in the NISQ era and beyond,” Quantum,
vol. 2, p. 79, aug 2018.
[24] A. Zulehner, S. Hillmich, and R. Wille, “How to efficiently handle com-
plex values? Implementing decision diagrams for quantum computing,”
in 2019 IEEE/ACM International Conference on Computer-Aided Design
(ICCAD), Westminster, CO, USA, 2019, pp. 1–7.
[25] S. Hillmich, A. Zulehner, R. Kueng, I. L. Markov, and R. Wille,
“Approximating decision diagrams for quantum circuit simulation,” ACM
Transactions on Quantum Computing, vol. 3, no. 4, pp. 1–21, 2022.
[26] C. Roberts, A. Milsted, M. Ganahl, A. Zalcman, B. Fontaine, Y. Zou,
J. Hidary, G. Vidal, and S. Leichenauer, “TensorNetwork: A library for
physics and machine learning,” 2019.
","nanPrior research in quantum circuit simulation has primarily focused on TN-based approaches, recognizing their advantages over conventional methods. These TN-based techniques aim to minimize computational overheads and enhance simulation efficiency. However, existing TN implementations often rely on array representations, which can introduce significant redundancy. To address this redundancy, a small but growing body of studies has explored decision diagrams as a promising alternative to arrays. Decision diagrams, such as quantum information decision diagram (QuIDD) and quantum multi-valued decision diagram (QMDD), effectively compress arrays by encoding them as directed acyclic graphs (DAGs), thereby reducing redundancy. While these decision diagram-based methods have demonstrated some success, they are not directly compatible with the extensive TN optimizations that have been developed, limiting their practical utility."
"This study introduces an automated method for generating concise and informative API summaries from informal sources like Stack Overflow. Employing BERTopic for topic modeling and extractive summarization, we extract recurring topics, problems, and potential solutions. Our approach aids developers in comprehending complex API documentation and streamlines their development workflows.","Navigating complex API documentation can be challenging. Developers seek insights from Stack Overflow, highlighting the need for a more user-friendly approach. This research proposes a novel method using BERTopic and extractive summarization to generate API summaries. We investigate three research questions: 1) prevalent topics on Android APIs, 2) identification of common issues, and 3) feasibility of deriving solutions. Our goal is to help developers grasp API methods, leading to practical applications for researchers and developers.","Our methodology entails 3 steps: data collection, topic modeling, and summarization. We gathered Stack Overflow questions related to Android from 2009 to 2022. Data preprocessing included removing code blocks and applying NLP techniques. For topic modeling, we employed BERTopic, powered by BERT, which excels in grasping semantic context. Summarization utilized the extractive version of BERT. We generated summaries in two steps: 1) summarizing topics based on questions to identify problems, and 2) summarizing answers to provide solutions.","Our analysis of Android-related posts in Stack Overflow revealed 81 topics. We highlighted three prevalent topics along with common issues and generated summaries for questions and answers. A user study with 15 developers yielded moderately positive outcomes, indicating the potential of our approach to aid in understanding complex API documentation.","Our method demonstrates the feasibility of generating API summaries from informal sources. By utilizing BERTopic and NLP techniques, we extracted recurring topics, issues, and potential solutions. This approach reduces developers' efforts in navigating API documentation. Future research may expand to diverse domains, explore alternative data sources, investigate various summarization techniques, and integrate API summaries into developer tools. This work has the potential to enhance developers' understanding and usage of APIs, benefiting software engineering practices.",Revolutionizing API Documentation through Summarization,"AmirHossein Naghshzan, Sylvie Ratte","arXiv:2401.11361v1  [cs.SE]  21 Jan 2024
Revolutionizing API Documentation through Summarization
AmirHossein Naghshzan
amirhossein.naghshzan.1@ens.etsmtl.ca
École de Technologie Supérieure
Montreal, Quebec, Canada
Sylvie Ratté
sylvie.ratte@etsmtl.ca
École de Technologie Supérieure
Montreal, Quebec, Canada
ABSTRACT
This study tackles the challenges associated with interpreting Ap-
plication Programming Interface (API) documentation, an integral
aspect of software development. Oﬃcial API documentation, while
essential, can be lengthy and challenging to navigate, prompting
developers to seek unoﬃcial sources such as Stack Overﬂow. Lever-
aging the vast user-generated content on Stack Overﬂow, includ-
ing code snippets and discussions, we employ BERTopic and ex-
tractive summarization to automatically generate concise and in-
formative API summaries. These summaries encompass key insights
like general usage, common developer issues, and potential solu-
tions, sourced from the wealth of knowledge on Stack Overﬂow.
Software developers evaluate these summaries for performance,
coherence, and interoperability, providing valuable feedback on
the practicality of our approach.
KEYWORDS
Topic Modeling, Summarization, Natural Language Processing, BERTopic,
BERT
1
INTRODUCTION
The growing availability of textual data, especially in software de-
velopment, oﬀers both challenges and opportunities for eﬃcient
information extraction. This is particularly relevant in the context
of complex Application Programming Interface (API) documenta-
tion, which is essential for programmers but can be lengthy, intri-
cate, and occasionally incomplete. Oﬃcial documentation, while
valuable, can also be time-consuming to navigate [1].
Developers often turn to unoﬃcial sources like Stack Overﬂow
and GitHub for quicker access to information, highlighting the
need for a more user-friendly approach to extracting insights from
API documentation. To address this, our research introduces a novel
method that utilizes BERTopic [2], a topic modeling technique that
combines BERT embeddings [5] and c-TF-IDF, for topic extraction
and identiﬁes common problems and solutions on Android APIs
as a case study. Furthermore, we use extractive summarization to
generate summaries for these topics and provide insightful docu-
mentation.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE 2023, 03 - 09 December, 2023, San Francisco, USA
© 2024 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
Our approach involves data gathering, preprocessing, employ-
ing BERTopic modeling for topic extraction, and utilizing extrac-
tive summarization. We assess the resulting summaries and topics
for their performance, coherence, and interpretability, all with the
objective of making API and method comprehension easier for de-
velopers.
To guide our research, we aim to answer the following research
questions:
RQ1: What are the prevalent topicsdiscussed on Stack Overﬂow
related to Android APIs?
RQ2: Can summarization methods eﬀectively identify common
issues within these topics?
RQ3: Is it feasible to derive solutions for these common issues
by leveraging information from unoﬃcial documentation sources?
The goal is to assist developers in understanding relevant API
methods for their daily tasks. This research could lead to valuable
tools like IDE plugins or recommender systems for practical use
by researchers and developers.
2
RELATED WORK
The ﬁeld of automatic summarization has gained signiﬁcant atten-
tion, especially in code summarization. However, there’s a gap in
applying summarization techniques to APIs, especially with un-
oﬃcial documentation sources. For instance, Alhaj et al. [3] used
BERTopic to classify cognitive distortions in Arabic Twitter con-
tent, while Egger et al. [4] evaluated topic modeling techniques
for social science research using Twitter data.
Shifting the focus to code summarization, Sridhara et al. [6]
demonstrated the eﬀectiveness of Natural Language Processing
(NLP) in accurately summarizing Java methods, preserving critical
information in the process. Meanwhile, Abdalkareem et al. [10]
devised an approach to extract code examples from Stack Over-
ﬂow posts using HTML encoding and specialized tags ﬁltering, a
method found to be eﬀective and adopted in our study. Hu et al. [7]
introduced DeepCom, a system using NLP to generate code com-
ments from large codebases, eﬀectively handling challenges like
extracting keywords from poorly named methods. Naghshzan et
al. [8, 9] innovatively utilized Stack Overﬂow discussions to pro-
duce natural language summaries, providing developers with a valu-
able additional resource and highlighting the signiﬁcance of unof-
ﬁcial documentation.
3
METHODOLOGY
Our research methodology consists of three main steps: data collec-
tion, topic modeling, and summarization. In the following section,
we delve into a comprehensive examination of these key processes.
ESEC/FSE 2023, 03 - 09 December, 2023, San Francisco, USA
AmirHossein Naghshzan and Sylvie Raté
Table 1: Top 3 topics of Android posts in Stack Ovberﬂow
Topic
Count
Name
Representation
1
14663
project_error_build_gradle
project, proguard, studio, error, build, library, ﬁle, gradle, android, eclipse
2
13471
fragment_viewpager_view
fragment, recyclerview, item, view, listview, scroll, adapter, list, layout, row
3
9947
notiﬁcation_activity_service_gcm
notiﬁcation, activity, service, gcm, app, analytics, push, back, intent, broadcast
3.1
Data Collection and Pre-processing
In the data collection phase, we retrieved all Stack Overﬂow ques-
tions tagged with Android from January 2009 to April 2022, total-
ing 3,698,168 posts. To focus on natural language for topic mod-
eling and summarization, we removed code blocks highlighted by
the HTML tag (<code>). Data preprocessing steps included elimi-
nating stop words, punctuation, tokenization, lemmatization, stem-
ming, and removing special characters and numerals.
3.2
Topic Modeling
To identify topics in Stack Overﬂow discussions, we employed BERTopic
[2], a powerful topic modeling technique based on BERT, a pre-
trained transformer model, for enhanced semantic understanding.
Traditional techniques like LDA [11] struggle with semantic con-
text [4]. BERTopic overcomes this by integrating BERT’s contex-
tual information. We used a pre-trained model available on Hug-
ging Face1 and Google Colab Pro’s T4 GPU for eﬃcient computa-
tions.
3.3
Summarization
Previously, Naghshzan et al. [8] demonstrated that extractive sum-
marization can be employed to extract information from Stack Over-
ﬂow. Therefore, we utilize the extractive version of BERT [12].
Our summarization approach involves two key steps:
Summarizing Topics Based on Questions: In the ﬁrst step, we
generate summaries for each topic using questions. We believe that
summarizing topics through questions can help identify the most
prevalent issues within each topic. By focusing on the top topics
from Stack Overﬂow, we aim to uncover the primary challenges in
Android development discussed on the platform.
Summarizing Topics Based on Answers: The second step in-
volves creating summaries of topics based on the corresponding
answers. These summaries provide potential solutions to the chal-
lenges identiﬁed in the previous step, drawing from the collective
wisdom of the Stack Overﬂow community.
To implement this, we tokenize the input document with the
BERT tokenizer and feed the tokens into the BERT model, which
encodes contextual information and captures semantic meaning.
The summarization algorithm, based on BERT, calculates impor-
tance scores for each sentence or phrase, selecting the highest-
scoring ones for the summary.
In the ﬁrst step, we use BERT to generate summaries of ques-
tions related to identiﬁed topics. The BERTopic algorithm provides
us with relevant posts for each topic, and we use this data to create
summaries.
1https://huggingface.co
Moving to the second step, we seek solutions to the identiﬁed
problems. We locate related questions in the database and note
their IDs. Using these IDs, we ﬁnd answers associated with the
questions and apply the same summarization process to generate
answer summaries. To ensure quality, we select answers marked
as accepted or with a score above the average, typically a score of
2 or higher.
4
RESULTS
We identiﬁed 81 topics for Android discussion in Stack Overﬂow.
Table 1 showcases the three most prevalent topics related to An-
droid posts on Stack Overﬂow. The Count column quantiﬁes the
number of posts associated with each speciﬁc topic. The Name col-
umn contains the names of the topics, as generated by BERTopic.
The Representation column lists the words that best represent each
respective topic. This table oﬀers a concise summary of the data,
providing insights into the topics that most frequently dominate
Android-related discussions on the platform. A comprehensive list
of our ﬁndings can be accessed in the online Appendix2.
Table 2 presents sample summaries for the top two topics, high-
lighting three common problems for each topic. Table 2 presents
generated answer summaries for the previously identiﬁed prob-
lems, oﬀering potential solutions. For example, for the ﬁrst topic,
one common problem is ""jenkins tries to launch tools emulator in-
stead of emulator"" and the generated summary suggests a possible
solution: ""its an issue with android emulator plugin not working with
new command line tools only sdk package"".
5
EVALUATION
We conducted a user study with 15 software developers, including
both experienced and intermediate-level programmers, to evalu-
ate our API documentation summarization approach. Participants
ratedthe summaries for coherence, informativeness, relevance, and
user satisfaction. The results show moderately positive outcomes
with average scores of 3.8 for coherence, 3.7 for informativeness,
3.9 for relevance, and 3.6 for user satisfaction. These results indi-
cate the potential of our approach to assist developers in under-
standing complex API documentation.
6
DISCUSSION
In our research, we tackled the challenges posed by complex API
documentation by introducing an innovative approach for generat-
ing API summaries from informal sources like Stack Overﬂow. Our
methodology involved BERTopic for topic modeling and BERT for
text summarization.
2https://github.com/amirarcane/BERTopic
Revolutionizing API Documentation through Summarization
ESEC/FSE 2023, 03 - 09 December, 2023, San Francisco, USA
Table 2: Generated summaries for questions and answers of topic project_error_build_gradle
Questions
Answers
Jenkins tries to launch tools instead of emulator I’m trying to set
up jenkins ui tests and it fails on running emulator command
I’m answering my own question its an issue with android emulator
plugin not working with new command line tools only sdk package
I am trying to add kotlin sources of an aar in android studio it
doesnt work when I select choose sources and choose the corre-
sponding source jar
add classpath org.jetbrains.kotlin gradle plugin clean and build
your project it worked for me
I have an sdk works with google api and implementing a lot of
dependencies then I implement the lib into my new app which is
also implements dependencies everything goes ﬁne until I try to
run the app
the problem is that you have a duplicated dependency with dif-
ferent version if you can ﬁnd the implementation that needs
com.google.android.gms.location you can put under it exclude
com.google.android.gms or simply in build.gradle
To address our ﬁrst research question (RQ1), we identiﬁed re-
curring topics discussed on Stack Overﬂow concerning Android
APIs. We applied BERTopic to a dataset of 3,698,168 unique An-
droid posts, extracting 80 prominent topics that shed light on fre-
quently discussed issues in Android development.
Moving on to our second research question (RQ2), we employed
text summarization techniques to identify common issues within
these topics. By summarizing questions related to each topic, we
pinpointed prevalent problems gleaned from the Stack Overﬂow
community’s collective knowledge.
Lastly, addressing ourthird research question(RQ3), we harnessed
information from unoﬃcial documentation to generate solutions
to these common issues. Summarizing answers to related ques-
tions, we provided potential solutions sourced from the Stack Over-
ﬂow community’s insights, aiming to address identiﬁed challenges.
To wrap up, our research demonstrated the feasibility of auto-
matically generating API summaries from informal sources, stream-
lining the understanding of APIs and methods for developers. While
this study oﬀers valuable insights and serves as a foundational ex-
ploration, further research and validation are essential to reﬁne
and expand upon these ﬁndings. There is ample room for improve-
ment and future investigations in this area.
7
CONCLUSION AND FUTURE WORK
In this work, we introduce an automated method for generating
API summaries from informal sources like Stack Overﬂow. Uti-
lizing BERTopic and NLP techniques, we extract recurring topics,
condensing common problems and their solutions. This approach
alleviates developers’ challenges in navigating complex API docu-
mentation, streamlining the process. By automatically extracting
and summarizing vital insights, our method saves developers valu-
able time, categorizing common development issues and their res-
olutions.
As we continue to reﬁne and expand our approach, there are sev-
eral avenues for future work that can enhance and expand upon
our ﬁndings. The following are potential areas for future research:
Extension to Diverse Domains: Broaden the applicability and
impact of our work by expanding our approach beyond Android
APIs to encompass various domains and programming languages.
Exploring Multiple Information Sources: Provide a more com-
prehensive and diverse set of data by investigating alternative plat-
forms like GitHub, bug reports, blogs, and forums.
Investigating Summarization Techniques: Exploring various
summarization approaches, including extractive, abstractive, and
hybrid methods to determine their suitability for API summariza-
tion tasks.
Integration with Developer Tools: Seamlessly integrating API
summaries into developer tools and IDEs to enhance productivity.
In conclusion, our method has the potential to assist developers
in comprehending and using APIs more eﬀectively. By pursuing
these future avenues, we aim to advance developer productivity
and software engineering practices.
REFERENCES
[1] Luca Ponzanelli, Gabriele Bavota, Massimiliano Di Penta, Rocco Oliveto, and
Michele Lanza. 2015. Turning the ide into a self-conﬁdent programming assis-
tant.
[2] Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based
TF-IDF procedure. arXiv preprint arXiv:2203.05794.
[3] Alhaj, F., Al-Haj, A., Sharieh, A., & Jabri, R. (2022). Improving Arabic cognitive
distortion classiﬁcation in Twitter using BERTopic. International Journal of Ad-
vanced Computer Science and Applications, 13(1), 854-860.
[4] Egger, R., & Yu, J. (2022). A topic modeling comparison between lda, nmf,
top2vec, and bertopic to demystify twitter posts. Frontiers in sociology, 7,
886498.
[5] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training
of deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805.
[6] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K. Vijay-
Shanker. 2010. Towards automatically generating summary comments for Java
methods. In Proceedings of the 25th IEEE/ACM International Conference on Au-
tomated Software Engineering (ASE ’10). Association for Computing Machinery,
New York, NY, USA, 43–52. https://doi.org/10.1145/1858996.1859006
[7] X. Hu, G. Li, X. Xia, D. Lo and Z. Jin, ""Deep Code Comment Generation,"" 2018
IEEE/ACM 26th International Conference on Program Comprehension (ICPC),
Gothenburg, Sweden, 2018, pp. 200-20010.
[8] A. Naghshzan, L. Guerrouj and O. Baysal, ""Leveraging Unsupervised Learning
to Summarize APIs Discussed in Stack Overﬂow,"" 2021 IEEE 21st International
Working Conference on Source Code Analysis and Manipulation (SCAM), Lux-
embourg, 2021, pp. 142-152, doi: 10.1109/SCAM52516.2021.00026.
[9] Naghshzan, A. (2022). Towards Code Summarization of APIs Based on Unoﬃcial
Documentation Using NLP Techniques. arXiv preprint arXiv:2208.06318.
[10] Abdalkareem, R., Shihab, E. & Rilling, J. (2017). On code reuse from Stack Over-
ﬂow: An exploratory study on android apps. Information and Software Technol-
ogy, 88, 148–158.
[11] Jelodar, H., Wang, Y., Yuan, C., Feng, X., Jiang, X., Li, Y., & Zhao, L. (2019). Latent
Dirichlet allocation (LDA) and topic modeling: models, applications, a survey.
Multimedia Tools and Applications, 78, 15169-15211.
[12] Miller, D. (2019). Leveraging BERT for extractive text summarization on lectures.
arXiv preprint arXiv:1906.04165.
This figure ""acm-jdslogo.png"" is available in ""png""
 format from:
http://arxiv.org/ps/2401.11361v1
This figure ""sample-franklin.png"" is available in ""png""
 format from:
http://arxiv.org/ps/2401.11361v1
","Prior research in automatic summarization focused on code summarization, neglecting API documentation. For topic modeling, BERTopic excels in semantic understanding, overcoming challenges faced by traditional techniques like LDA. NLP-based code summarization has demonstrated effectiveness, aiding comprehension of Java methods. Our study builds upon existing approaches, leveraging unoffical documentation sources, like Stack Overflow, to provide valuable supplementary information.nan"
"Protein language models have significantly impacted peptide sequence representation tasks. However, pre-trained models tailored for peptide-specific requirements remain largely unaddressed due to difficulties in capturing peptide structure. We introduce PepHarmony, a multi-view contrastive learning framework that integrates peptide sequence and structure information using a sequence-level encoding module. Our comprehensive analysis highlights PepHarmony's exceptional capability in capturing the intricate relationship between peptide sequences and structures. Extensive ablation studies emphasize the importance of contrastive loss and data sorting in enhancing predictive performance. PepHarmony serves as a notable contribution to peptide representations and offers valuable insights for peptide drug discovery and engineering.","Peptides, as crucial biological entities, embody an array of functionalities in biological processes, ranging from enzymatic activity to therapeutic applications. They are characterized by their shorter amino acid sequences and increased structural flexibility relative to proteins. This distinctiveness positions peptides as key players in numerous biological and therapeutic contexts. However, their unique attributes require tailored computational models for precise representation, an area not yet fully explored by current protein-centric pre-trained models such as ESM. The inherent relationship between a peptide’s sequence and its structure is a cornerstone in understanding its function and potential utility. However, representing peptides effectively in computational models, a task fundamental to advancing drug discovery and protein engineering, has been a challenging endeavor. Traditional approaches have either focused on peptide sequences or structures in isolation, failing to fully capture the intricate interplay between these two facets. In recent years, the advent of large-scale protein databases like the Protein Data Bank (PDB) and the transformative AlphaFold DB has provided an unprecedented wealth of data, offering new avenues for peptide research. These developments, alongside advancements in machine learning, particularly in protein pre-trained models, have set the stage for innovative approaches in peptide representation.","To enhance sequence representation with structural information, our model adopts a multi-view contrastive learning approach. It treats sequence and structure as two complementary views for each peptide (see Figure 1) and performs self-supervised learning (SSL) between these views. In our investigation of sequence encoding from other models, we find that our sequence representation integrates more structural information.

Our model utilizes two pretrained models, ESM and GearNet, for encoding peptide sequences and structures respectively. ESM is a transformer-based sequence encoder and pre-training model, processing peptide sequences to output residue representations. GearNet, used for structure encoding, learns representations that encode spatial and chemical information of peptide structure. The subsequent sections will delve into detailed introductions of these two pretrained models.

At the training stage, our model conducts self-supervised learning. Following the framework in [27], we employ two losses: a contrastive one and a generative one. These losses focus on different representation learning aspects. The contrastive loss concentrates on mapping relationships between sequence and structure representations of peptides. In contrast, the generative loss emphasizes the feature representation of both sequence and structure, particularly the reconstruction of key data features. From a distribution learning perspective, contrastive SSL and generative SSL learn data distribution in a local and global manner, respectively. Contrastive SSL learns the distribution locally by contrasting inter-data pairwise distances, while generative SSL directly learns the global data density function.","The results presented in Figure 3 indicate a comparative analysis of validation performance using three distinct datasets derived from AlphaFold: af50w, af80, and af90, each corresponding to different confidence levels. The datasets are evaluated on two downstream tasks: CPP (protein-protein interaction prediction) and Affinity(protein-ligand affinity prediction).

For the CPP task, the validation loss and AUC-ROC metrics are considered. As depicted in Figure 3(a) and (b), the af90 dataset demonstrates a marginally lower validation loss and a higher AUC-ROC compared to af50w andaf80. These differences suggest that the af90 dataset enables the model to predict protein-protein interactions with greater accuracy and reliability.

In the context of the Affinity task, Figure 3(c) and Figure 3(d) display the validation loss and Pearson Correlation coefficient, respectively. Consistent with the CPP task results, the af90 dataset exhibits the lowest validation loss and the highest Pearson Correlation coefficient, reinforcing the assertion that higher confidence in the pre-training dataset correlates with superior model performance on downstream tasks.

The experimental outcomes robustly support the conclusion that the af90 dataset, characterized by the highest confidence level, yields the best performance across both downstream tasks. This affirms the hypothesis that the quality of the pre-training dataset is crucial for enhancing model efficacy. The slight but consistent superiority of af90 over the other datasets across all metrics provides empirical evidence that higher confidence scores are significantly beneficial for the model’s predictive capabilities.","In this study, we introduce PepHarmony, a pioneering model in the field of peptide representation learning, notable for its unique identity as a pure sequence-based encoder. Utilizing a novel multi-view contrastive learning approach, PepHarmony successfully integrates sequence and structural information in its training phase. However, it’s crucial to note that during inference, the model exclusively leverages its sequence encoder. This design choice is not merely a technical detail but a strategic innovation, allowing the model to extract features that inherently encapsulate structural information, leading to exceptional results in various analytical tasks.

Through extensive experimentation, we have demonstrated that the quality and preparation of training data play a pivotal role in model performance. Our exploration of datasets from AlphaFold DB and PDB revealed key insights into data distribution, quality, and training strategies. The adept handling of these datasets has been instrumental in harnessing their full potential for training PepHarmony. Our ablation studies further delineated the importance of contrastive learning and the integration of different pretraining strategies. The outcomes suggest that while the incorporation of generative loss provides certain benefits, the model predominantly excels with contrastive loss, especially when coupled with data sorting techniques.

As we move forward, several avenues for future work emerge. First, the exploration of domain-specific adaptations of PepHarmony could be pursued to tailor the model for specific applications, such as targeted drug design or protein engineering. Second, integrating emerging datasets and continual advancements in protein structure prediction methods could further enhance the model’s robustness and accuracy. Finally, exploring the potential of transfer learning and its implications in rapidly evolving fields like synthetic biology could open up new frontiers for PepHarmony.

In conclusion, PepHarmony represents a significant step forward in peptide representation learning. Its ability to effectively capture the intricate relationship between peptide sequences and structures offers a powerful tool for researchers and practitioners alike. We anticipate that this model will serve as a useful tool for future innovations in the field, contributing to advancements in drug discovery, protein engineering, and beyond.",PepHarmony: A Multi-View Contrastive Learning Framework for Integrated Sequence and Structure-Based Peptide Encoding,"Ruochi Zhang, Haoran Wu, Chang Liu, Huaping Li, Yuqian Wu, Kewei Li, Yifan Wang, Yifan Deng, Jiahui Chen, Fengfeng Zhou, Xin Gao","PepHarmony: 
A 
Multi-View 
Contrastive 
Learning 
Framework for Integrated Sequence and Structure-Based 
Peptide Encoding 
Ruochi Zhang1,2,3, Haoran Wu3, Chang Liu4, Huaping Li5, Yuqian Wu1,8, Kewei Li1,9, 
Yifan Wang3, Yifan Deng3, Jiahui Chen3, Fengfeng Zhou1,9,*, and Xin Gao6,7,*. 
1 Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry 
of Education, Jilin University, Changchun, Jilin, China, 130012. 
2 School of Artificial Intelligence, Jilin University, Changchun, Jilin, China, 130012. 
3 Syneron Technology, Guangzhou, China, 510700.  
4 Beijing Life Science Academy, Beijing 102209, China 
5 School of Biomedical Sciences, LKS Faculty of Medicine, the University of Hong 
Kong, Hong Kong SAR, China. 
6 Computational Bioscience Research Center, King Abdullah University of Science and 
Technology (KAUST), Thuwal, Saudi Arabia, 23955. 
7 Computer Science Program, Computer, Electrical and Mathematical Sciences and 
Engeering Division, King Abdullah University of Science and Technology (KAUST), 
Thuwal, Saudi Arabia, 23955.  
8 College of Software, Jilin University, Changchun, Jilin, China, 130012. 
9 College of Computer Science and Technology, Jilin University, Changchun, Jilin, 
China, 130012. 
 
 
* Corresponding authors. 
E-mail addresses: xin.gao@kaust.edu.sa (X. Gao) and FengfengZhou@gmail.com (F. 
Zhou). 
 
 
 
Abstract 
Recent advances in protein language models have catalyzed significant progress in 
peptide sequence representation. Despite extensive exploration in this field, pre-trained 
models tailored for peptide-specific needs remain largely unaddressed due to the 
difficulty in capturing the complex and sometimes unstable structures of peptides. This 
study introduces a novel multi-view contrastive learning framework PepHarmony for 
the sequence-based peptide encoding task. PepHarmony innovatively combines both 
sequence- and structure-level information into a sequence-level encoding module 
through contrastive learning. We carefully select datasets from the Protein Data Bank 
(PDB) and AlphaFold database to encompass a broad spectrum of peptide sequences 
and structures. The experimental data highlights PepHarmony's exceptional capability 
in capturing the intricate relationship between peptide sequences and structures 
compared with the baseline and fine-tuned models. The robustness of our model is 
confirmed through extensive ablation studies, which emphasize the crucial roles of 
contrastive loss and strategic data sorting in enhancing predictive performance. The 
proposed PepHarmony framework serves as a notable contribution to peptide 
representations, and offers valuable insights for future applications in peptide drug 
discovery and peptide engineering. We have made all the source code utilized in this 
study publicly accessible via GitHub at https://github.com/zhangruochi/PepHarmony 
or http://www.healthinformaticslab.org/supp/.  
Keywords: Bioinformatics; Peptide Sequence Representation; Contrastive Learning; 
Sequence-Based Encoding; Multi-View Learning. 
 
1.Introduction 
Peptides, as crucial biological entities, embody an array of functionalities in biological 
processes, ranging from enzymatic activity to therapeutic applications [1, 2]. They are 
characterized by their shorter amino acid sequences and increased structural flexibility 
relative to proteins [3]. This distinctiveness positions peptides as key players in 
numerous biological and therapeutic contexts [4]. However, their unique attributes 
require tailored computational models for precise representation, an area not yet fully 
explored by current protein-centric pre-trained models such as ESM(Evolutionary Scale 
Modeling) [5, 6].  
The inherent relationship between a peptide’s sequence and its structure is a cornerstone 
in understanding its function and potential utility [7, 8]. However, representing peptides 
effectively in computational models, a task fundamental to advancing drug discovery 
and protein engineering, has been a challenging endeavor. Traditional approaches have 
either focused on peptide sequences or structures in isolation [9, 10], failing to fully 
capture the intricate interplay between these two facets. 
In recent years, the advent of large-scale protein databases like the Protein Data Bank 
(PDB) [11, 12] and the transformative AlphaFold DB [13, 14] has provided an 
unprecedented wealth of data, offering new avenues for peptide research. These 
developments, alongside advancements in machine learning, particularly in protein pre-
trained models [5, 15], have set the stage for innovative approaches in peptide 
representation.  
Our study introduces PepHarmony, a pioneering pretrained model that integrates both 
peptide sequence and structure information, leveraging the symbiotic relationship 
between these two aspects. This approach marks a significant departure from traditional 
models that treat sequence and structure as distinct entities. The crux of PepHarmony 
lies in its innovative contrastive learning approach, which, during pre-training, fuses 
structural and sequence information. However, during inference, it exclusively utilizes 
the sequence encoder. This unique strategy ensures that the extracted features from 
sequences inherently include structural information, enhancing the model’s 
effectiveness in various peptide-related tasks.  
Furthermore, we conduct an exhaustive analysis of peptide sequence and structural data 
from AlphaFold DB and PDB. This examination allows us to understand better the 
nuances and intricacies of training data and how they can be effectively harnessed to 
train our model. We delve into various aspects, such as data distribution, quality, and 
the impact of different training strategies, providing a comprehensive view of the data 
landscape.  
Our experimental results demonstrate the effectiveness of PepHarmony in various tasks, 
including peptide-protein interaction prediction and peptide solubility prediction. The 
model’s performance not only attests to its robustness and accuracy but also highlights 
its potential in facilitating advanced research in protein analysis and drug discovery. In 
summary, our work not only presents a novel model in PepHarmony but also offers 
valuable insights into the optimal utilization of peptide data for training.  
The three main contributions of our paper are as follows,  
1. Introduction of PepHarmony, a pioneering pretrained, pure sequence-based encoder 
that effectively encapsulates structural data within peptide sequences. 
2. Detailed exploration and utilization of peptide sequence and structural data from 
AlphaFold DB and PDB, including an in-depth analysis of training methodologies and 
data efficacy.  
3. Demonstrated effectiveness of PepHarmony through extensive experimental 
validation, showcasing superior performance in multiple peptide-related tasks. 
2. Related work 
Understanding the intricate interplay between protein sequences and structures is 
pivotal in computational biology and bioinformatics [16]. Recent advancements have 
led to the development of various computational models, each focusing on different 
aspects of protein analysis. This section reviews the relevant literature, categorized into 
three main areas: sequence-based protein language models, structure-based protein pre-
trained models, and models that infuse both structure and sequence information. 
2.1. Sequence-based protein language models 
Protein language models based on sequence data have made significant strides, 
especially with the adoption of deep learning techniques [17]. Models like BERT [18] 
and GPT [19], originally designed for natural language processing, have been adapted 
to interpret the ’language’ of proteins. These adaptations, such as ESM, leverage the 
sequential nature of amino acids to predict protein functionality and interactions. By 
training on large datasets of known protein sequences, these models have shown 
remarkable proficiency in capturing the underlying biological properties inherent in 
sequence data. However, their reliance solely on sequence information limits their 
ability to fully predict protein behavior, which is also influenced by three-dimensional 
structural conformations. 
2.2. Structure-based protein pretrained models 
Structure-based models focus on understanding proteins from their three-dimensional 
conformations. With the advent of more sophisticated structural prediction algorithms 
and the increasing availability of structural data, these models have gained prominence. 
Notable among them is AlphaFold [13], which predicts protein structures with 
remarkable accuracy. Models like GearNet [20] go a step further by incorporating 
spatial and chemical information into their analyses, providing a more nuanced 
understanding of protein function and interaction. These models have significantly 
advanced the field’s ability to predict protein functionality based on structure, yet they 
often overlook the valuable insights that can be gleaned from sequence data.  
2.3. Structure and sequence infusion models 
Recognizing the limitations of focusing on either sequence or structure alone, recent 
research has seen the emergence of models that integrate both types of information. 
These multi-view models aim to capitalize on the strengths of both sequence-based and 
structure-based approaches [20-23]. They involve advanced algorithms that can 
effectively merge sequence information with structural data, offering a more 
comprehensive understanding of proteins. The fusion of these two data types allows for 
a more accurate prediction of protein functions, interactions, and even the discovery of 
new proteins [24, 25]. This integrated approach represents a significant step forward in 
the field, providing a holistic view of proteins that is more aligned with their 
multifaceted nature in biological systems.  
However, our work presents distinct approaches and focuses when compared to these 
existing models. Unlike the model described in [23] which explores the combination of 
a state-of-the-art Protein Language Model (PLM) and a protein structure encoder, our 
research concentrates specifically on peptides rather than proteins. [23] demonstrates 
the efficiency of ESM-GearNet, a model that combines a PLM (ESM-1b) and a protein 
structure encoder (GearNet) through serial connection and further enhances it with 
pretraining on massive unlabeled protein structures using contrastive learning. Our 
study delves into the intrinsic differences between peptide and protein data. We focus 
on how the unique characteristics of peptides affect the training data and methods used 
in our models, and how these differences influence the final results. This emphasis on 
peptides, which are fundamentally different from proteins in terms of size, complexity, 
and biological roles, requires a tailored approach that differs significantly from the 
protein-centric models previously developed. 
In summary, the field of computational protein analysis is rapidly evolving, with 
significant contributions from sequence-based, structure-based, and integrated models. 
Each approach offers unique insights into protein characterization, and the ongoing 
development of hybrid models promises to unlock even more comprehensive and 
accurate methods for protein analysis in computational biology. 
3. Methodology 
In the following, we first present an overview of our model, and then individually 
introduce the models used for sequence representation learning and structure 
representation learning. Finally, we introduce two learning tasks between sequence and 
structure. 
3.1. Overview of multi-view pretrained model 
To enhance sequence representation with structural information, our model adopts a 
multi-view contrastive learning approach. It treats sequence and structure as two 
complementary views for each peptide (see Figure 1) and performs self-supervised 
learning (SSL) [26] between these views. In our investigation of sequence encoding 
from other models, we find that our sequence representation integrates more structural 
information.  
Our model utilizes two pretrained models, ESM and GearNet, for encoding peptide 
sequences and structures respectively. ESM is a transformer-based sequence encoder 
and pre-training model, processing peptide sequences to output residue representations. 
GearNet, used for structure encoding, learns representations that encode spatial and 
chemical information of peptide structure. The subsequent sections will delve into 
detailed introductions of these two pretrained models.  
At the training stage, our model conducts self-supervised learning. Following the 
framework in [27], we employ two losses: a contrastive one and a generative one. These 
losses focus on different representation learning aspects. The contrastive loss 
concentrates on mapping relationships between sequence and structure representations 
of peptides. In contrast, the generative loss emphasizes the feature representation of 
both sequence and structure, particularly the reconstruction of key data features. From 
a distribution learning perspective, contrastive SSL and generative SSL learn data 
distribution in a local and global manner, respectively. Contrastive SSL learns the 
distribution locally by contrasting inter-data pairwise distances, while generative SSL 
directly learns the global data density function. 
3.2. Sequence model 
ESM, a notable protein language model (PLM), undergoes pre-training using masked 
language modeling (MLM) loss[18]. This process involves predicting the type of a 
hidden residue based on its surrounding context. By leveraging a vast amount of 
unlabeled data, ESM has attained top-tier results in diverse protein understanding tasks. 
Peptides, essentially shorter sequences of proteins, naturally lend themselves to feature 
extraction using PLMs like ESM. Despite the absence of a large-scale peptide-specific 
language model, ESM serves as an effective sequence model encoder. We also utilize 
the ESM-trained parameters on proteins to initialize the sequence encoder, specifically 
employing the ESMt12 version. 
 
Figure 1: Overall architecture of our model. Two encoders, which are sequence encoder and 
structural encoder reprectively, will be trained together by contrastive or generative learning. 
After that, in the downstream tasks, we will just use the sequence coder to extract peptide 
representation.  
3.3. Structure model 
Structures are a direct determinant of peptide functions. We employ a simple yet 
effective structure-based encoder called GeomEtry-Aware Relational Graph Neural 
Network (GearNet), which encodes spatial information by adding different types of 
sequential or structural edges and then performs relational message passing on protein 
residue graphs. Utilized on our peptide data, GearNet constructs a residue graph for 
encoding the structure information. For structure model initialization, we employ two 
versions of pre-trained GearNet, and their performance is extensively compared in our 
ablation study section. The first version, GearNetcons, described in [20], optimizes a 
contrastive loss function following SimCLR [28], aiming to maximize mutual 
information between biologically correlated views. It constructs views reflecting 
protein substructures via two sampling schemes, one being subsequence cropping, 
which samples residues from a protein graph. The second version, GearNetdiff , 
introduced in [29], is inspired by denoising diffusion models’ success in generative 
tasks. This model uses the DiffPreT [29] approach for sequence-structure joint diffusion 
modeling, guiding the encoder to recover native protein sequences and structures from 
perturbed ones along a joint diffusion trajectory. It considers protein conformational 
variations and includes the Siamese Diffusion Trajectory Prediction (SiamDiff) method 
to capture correlations between different protein conformers. 
Graph construction Given peptide structures, the representations encoding their 
spatial and chemical information should be invariant under translations, rotations and 
reflections in 3D space. To achieve this requirement, GearNet constructs peptide graph 
based on spatial features invariant under these transformations. 
GearNet represents the structure of a peptide as a residue-level relational graph 
G= =
V E R
( , ,
) , where V  and E denotes the set of nodes and edges respectively, and  
R  is the set of edge types. We use (i, j, r) to denote the edge from node i to node j with 
type r. We use n and m to denote the number of nodes and edges, respectively. In this 
work, each node in the protein graph represents the alpha carbon of a residue with the 
3D coordinates of all nodes 
×
∈ 
n 3
x
 . We use fi and f(i,j,r) to denote the feature for 
node i and edge (i, j, r), respectively, in which reside types, sequential and spatial 
distances are considered.  
Then, we add three different types of directed edges into our graphs: sequential edges, 
radius edges and K-nearest neighbor edges. Among these, sequential edges will be 
further divided into 5 types of edges based on the relative sequential distance 
∈ −
−
{ 2, 1,0,1,2}
d
 between two end nodes, where we add sequential edges only 
between the nodes within the sequential distance of 2. These edge types reflect different 
geometric properties, which all together yield a comprehensive featurization of proteins. 
Neural message passing architecture Upon the protein graphs defined above, 
GearNet utilizes a graph neural network(GNN) [30, 31] to derive per-residue and 
whole-peptide representations. To balance model capacity and memory cost, GearNet 
use a relational graph convolutional neural network [32] to learn graph representations, 
where a convolutional kernel matrix is shared within each edge type and there are R  
different kernel matrices in total. Formally, the relational graph convolutional layer is 
defined as 
−
=
=
+
( )
(
1)
( )
(0)
,    
l
l
l
i
i
i
i
i
h
f
h
h
u
 
σ
−
∈
∈




=












∑
∑
( )
(
(
1
)
)
r
l
l
i
r
j
r R
j N
i
u
BN
W
h
 
Specifically, we use node features fi as initial representations. Then, given the node 
representation 
ih(0)
  for node i at the l-th layer, we compute updated node 
representation 
( )
i
h l
  by aggregating features from neighboring nodes
r ( )
N r i  ,Where 
{
}
=
∈
∈
( )
|
)
 
( ,
,
r i
j
j i r
N
V
E  denotes the neighborhood of node i with the edge type 
r, and 
r
N r  denotes the learnable convolutional kernel matrix for edge type r. Here BN 
denotes a batch normalization layer and we use a ReLU function as the activation σ(·). 
Finally, we update 
( )
i
h l
 with 
( )
i
u l
 and add a residual connection from the last layer. 
3.4. Learning tasks between sequence and structure  
In this subsection, we introduce two learning tasks between sequence and structure, 
contrastive learning and generative learning.  
Contrastive learning Following the framework of contrastive SSL[28, 33], we first 
define positive and negative pairs of views from an inter-data level, and then align the 
positive pairs and contrast the negative pairs simultaneously. For each peptide, we first 
extract representations from sequence view and structure views, i.e., hx and hy. Then we 
create positive and negative pairs for contrastive learning: the sequence-structure pairs 
(x, y) for the same peptide are treated as positive, and negative otherwise. Finally, we 
align the positive pairs and contrast the negative ones. The pipeline is shown in Figure 
1. In the following, we use InfoNCE objective functions [34] on contrastive graph SSL. 
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
= −




+


+
+




∑
∑
E
L
( ,
)
 
(
1
2
,
 
,
 
,
 
,
,
 
,
(
))
InfoNCE
p x y
x
y
j
j
x
x
y
y
j
j
exp f
x y
exp f y x
log
log
exp f
x y
exp f
x y
exp f
y x
exp f
y x
 
where xj, yj are randomly sampled 2D and 3D views regarding to the anchored pair (x, 
y). fx(x, y) and fy(y, x) are scoring functions for the two corresponding views, with 
flexible formulations. Here we adopt fx(x, y) = fy(y, x) = <hx, hy>. 
Generative learning Generative SSL is another classic track for unsupervised pre-
training[35, 36]. It aims at learning an effective representation by self-reconstructing 
each data point. Specifically to drug discovery, every peptide has a sequence and a 
structure, and our goal is to learn a robust sequence representation that can, to the most 
extent, recover its structure counterparts. By doing so, generative SSL can enhance 
sequence representation to encode the most crucial geometry/topology information, 
which can improve downstream performance.  
GraphMVP [27] proposes a light VAE [37]-like generative SSL, equipped with a crafty 
surrogate loss. They propose a novel surrogate loss by switching the reconstruction 
from data space to representation space, called variational representation reconstruction 
(VRR): 
β






=
=
−
+
−












+
⋅
+






E
E
L
L
2
2
(
| )
(
| )
1
 
 
2
           
(
)
(
)
(
)
(
)
( (
| )
(
))
( (
|
    
.
(
2
)
))
x
y
G
VRR
q z x
x
x
y
q z
y
y
x
y
y
x
x
y
q z
SG h
q z
SG h
KL q z
x
p z
KL q z
y
p z
 
In this formula, SG represents the stop-gradient operation, which is based on the 
assumption that hy is a fixed, learned representation function. The use of SG is a 
common practice in SSL literature, primarily to prevent model collapse, as noted in [38]. 
4. Experiments 
4.1. Dataset Acquisition 
PDB dataset: The Protein Data Bank (PDB) is a globally accessible public database 
that offers detailed information on the three-dimensional structures of various 
biological macromolecules, such as proteins, nucleic acids, and complex assemblies. 
These structures are made available without charge to the international research 
community, supporting a broad spectrum of scientific endeavors including drug 
discovery, protein engineering, and fundamental studies of biological molecules’ 
structure and functionality. In our specialized utilization of the PDB, we extract all 
protein chains from the PDB database and assessed them based on the sequence length 
of their amino acids, identifying chains as peptides if their length was under 50. 
Through this meticulous process, we successfully isolated and cleansed data for 17,300 
peptides, including both their amino acid sequences and corresponding structural 
information.  
AlphaFold dataset: AlphaFold DB, capitalizing on the advanced AlphaFold model, 
provides open access to more than 200 million protein structure predictions, markedly 
accelerating scientific research. It offers an extensive selection of predicted protein 
structures, easily accessible to the scientific community. Drawing from AlphaFold DB, 
we apply a methodology akin to our PDB data cleansing strategy, successfully 
extracting structural information for 3.5 million peptide sequences.  
The AlphaFold DB and PDB datasets both contain structural data for peptides and 
sequences. However, they exhibit distinct data distributions. As shown in Figure 2(a), 
the peptide length distribution in the PDB dataset is significantly shorter compared to 
that in the AlphaFold DB. Acknowledging that the structures predicted by AlphaFold2 
are theoretical rather than actual, we theorized that the precision of these predictions—
essentially the data’s quality—would profoundly influence the training of models. 
AlphaFold computes a per-residue confidence score from 0 to 100, known as pLDDT 
(predicted Local Distance Difference Test), which mirrors the model’s confidence in its 
predictions. In our approach, we averaged these pLDDT scores for each peptide, 
yielding a single confidence score indicative of the overall reliability of each peptide’s 
predicted structure. As depicted in Figure 2(b), we note a trend: shorter sequences 
typically have higher pLDDT score distributions. This led us to an insightful strategy: 
by selecting training data with higher pLDDT scores, we could align the length 
distribution of this subset more closely with that observed in the PDB datasets. 
Consequently, we categorized our training dataset into distinct quality datasets based 
on pLDDT scores for more effective experimental validation:  
af90: About 500,000 data samples were selected, each with a confidence score above 
90.  
af80: Approximately 1.18 million data samples were chosen, all with confidence scores 
above 80.  
af50w: A varied set of 500,000 data samples was randomly sampled from the entire 
dataset, irrespective of confidence scores. 
         
 
(a)                                     (b) 
Figure 2: Statistaics for PDB and AlphaFold DB datasets. (a) Illustrates the differences in 
peptide length distribution between the PDB Dataset and the AlphaFold Dataset. Overall, the 
peptide lengths in the PDB Dataset are shorter than those in the AlphaFold DB. (b) Shows the 
relationship between the averaged pLDDT scores and the peptide lengths in the AlphaFold 
Dataset. As can be seen from the figure, shorter sequences typically exhibit higher distributions 
of pLDDT scores. 
This by pLDDT scores enable a more nuanced and quality-focused selection of data for 
training and research purposes. For our model training, we only utilize data from the 
AlphaFold DB, while for testing, we employed the PDB datasets. Since the PDB 
datasets are derived from actual experimental results, using them for self-contaction 
evaluation ensures a more equitable assessment. We also explored the approach of 
mixing data from both the AlphaFold DB and PDB datasets for combined training and 
testing purposes.  
CPP dataset We first evaluate our models on the dataset of CPP [39-42]. Our CPP 
dataset is the largest known database of cell-penetrating peptides, containing 1162 
positive and negative samples each. We sorted out 22 relevant cell-penetrating peptide 
databases by compiling literature on existing cell-penetrating peptide prediction models. 
To ensure the reliability of the sources, we traced each database and listed the 
relationships between the databases. Besides, we also checked whether there were 
conflicts and duplicates in different databases, and the anomalous data were checked 
artificially again.  
Solubility dataset We utilize PROSO-II dataset [43] for our peptide solubility 
prediction task. Thanks to the restrictive data selection from the pepcDB and PDB 
databases, PROSO-II dataset is one of the largest available databases used for 
solubility-model building and evaluation. A large portion of the data comes from 
pepcDB database, in which each protein is associated with multiple amino acid 
sequences corresponding to different constructs. For each construct, its status history is 
recorded, including the one of solubility. Hence, all constructs that achieved the soluble 
status are considered as positive samples. Besides, additional soluble data originates 
also from PDB entries of heterologous complexes, the proteins with the annotation of 
‘Expression Organism: ESCHERICHIA COLI’ are considered positive. Adapting 
PROSO-II dataset to our needs, we have constructed a subset (about 4 thousand pieces 
of data) which contains only sequences less than 50 in length.  
Affinity dataset We construct our benchmark data set from two sources: first is the 
protein–peptide complex structures from PDB database  and second is the drug-target 
pairs from DrugBank [44-48]. In total, we obtained 7417 positive interacting pairs 
covering 3412 protein sequences and 5399 peptide sequences. Among them, 6581 pairs 
from PDB have residue-level binding labels in peptide sequences. We then constructed 
a negative dataset by randomly shuffling those non-interacting pairs of proteins and 
peptides. More specifically, for each positive interaction, five negatives were generated 
by randomly sampling from all the shuffled pairs of non-interacting proteins and 
peptides. Overall, we obtained 44,502 peptide–protein pairs in our benchmark.  
4.2. Experimental Setup  
Training details In the training phase of our model, we explore the effects of various 
components on the outcomes. We utilized the pre-trained ESM model for sequence 
embedding and two versions of the pre-trained GearNet model for structure embedding. 
The emphasis was on using contrastive learning loss for its intuitive alignment of 
peptide sequences with their structures. Additionally, we assessed the influence of 
generative learning loss on training outcomes. During pretraining, we noted a 
significant drop in validation loss and high accuracy with limited data, raising concerns 
about data leakage. This was attributed to GearNet potentially leaking sequence 
information into structural representations and the model’s ability to match sequence 
and structure pairs based on length. To mitigate these, we masked amino acid 
information in GearNet and altered batch selection, reducing accuracy to about 0.75.  
Downstream tasks This work aims to incorporate structural information into the 
sequence representation of peptides through contrastive learning. To achieve this, we 
extract the sequence encoding model and design the following four downstream tasks 
to validate the learned sequence and structure information of the model.  
• CPP: penetrating peptide prediction (binary classification).  
• Solubility: peptide solubility prediction (binary classification).  
• Affinity: peptide-protein affinity prediction (regression).  
• Self-Contaction: self-contact map prediction (binary matrix prediction). This task 
focuses on peptide structure, predicting the connectivity between amino acids.  
Focus on sequence encoder in downstream tasks Although the contrastive 
learning framework theoretically allows for learning two independent encoders: the 
sequence encoder and the structure encoder, our focus in a downstream task is solely 
on the performance of the sequence encoder. Specifically, we use amino acid sequences 
as input to extract representations and predict the properties of peptides. The reasons 
for this focus are threefold:  
1. Our aim is to effectively integrate structural information into the sequence 
model through the use of contrastive learning or a generative learning 
framework. 
2. The sequence encoder has a high throughput [49], making it particularly useful 
in new drug development scenarios for applications like virtual screening [50]. 
It has a wide range of potential applications.  
3. In most downstream scenarios, especially in the early stages of new drug 
development, when predicting the properties of a peptide, we usually only have 
sequence information and not structural information [51]. Therefore, the 
structure encoder is unable to extract features due to the absence of structural 
data. 
4.3. Dataset efficacy in predictive modeling 
The results presented in Figure 3 indicate a comparative analysis of validation 
performance using three distinct datasets derived from AlphaFold: af50w, af80, and 
af90, each corresponding to different confidence levels. The datasets are evaluated on 
two 
downstream 
tasks: 
CPP 
(protein-protein 
interaction 
prediction) 
and 
Affinity(protein-ligand affinity prediction). 
For the CPP task, the validation loss and AUC-ROC metrics are considered. As depicted 
in Figure 3(a) and (b), the af90 dataset demonstrates a marginally lower validation loss 
and a higher AUC-ROC compared to af50w andaf80. These differences suggest that the 
af90 dataset enables the model to predict protein-protein interactions with greater 
accuracy and reliability. 
In the context of the Affinity task, Figure 3(c) and Figure 3(d) display the validation 
loss and Pearson Correlation coefficient, respectively. Consistent with the CPP task 
results, the af90 dataset exhibits the lowest validation loss and the highest Pearson 
Correlation coefficient, reinforcing the assertion that higher confidence in the pre-
training dataset correlates with superior model performance on downstream tasks. 
The experimental outcomes robustly support the conclusion that the af90 dataset, 
characterized by the highest confidence level, yields the best performance across both 
downstream tasks. This affirms the hypothesis that the quality of the pre-training dataset 
is crucial for enhancing model efficacy. The slight but consistent superiority of af90 
over the other datasets across all metrics provides empirical evidence that higher 
confidence scores are significantly beneficial for the model’s predictive capabilities. 
Our subsequent investigations focused on leveraging the high-quality, experimentally 
measured data available in the PDB dataset. Initially, we hypothesized that a 
combination of PDB’s precision with the extensive volume of af90 might yield optimal 
results. To this end, we implemented two data integration strategies: a direct mix 
strategy and a sequential finetune strategy. 
The mix strategy involved an indiscriminate combination of data from both sources for 
the training process. In contrast, the finetune strategy entailed initial model pre-training 
on the af90 dataset followed by refinement using the PDB dataset. These strategies were 
predicated on the notion that the PDB’s data, despite its smaller size, would infuse the 
model with experimentally validated nuances, thereby enhancing the overall model 
performance. 
Contrary to our expectations, the results depicted in Figure 4 demonstrate that neither 
data mixing strategy provided any significant advantage over the exclusive use of af90. 
The af90 dataset emerged as the clear frontrunner, outperforming the mixed strategies 
on both CPP and Affinity tasks. This suggests that the quantity of data in af90, which 
surpasses that of the PDB by an order of magnitude, plays a more critical role in model 
training than the exclusively high-quality yet limited PDB data. The sheer volume of 
af90 appears to facilitate a more comprehensive learning of peptide representations, 
despite its marginally inferior data quality compared to the PDB. 
Moreover, the underperformance of the mix and finetune strategies may be attributed 
to the heterogeneous nature of the datasets. The PDB and AlphaFold databases exhibit 
significant differences in their data distributions, a factor that potentially undermines 
the efficacy of mixed training sets. These distributional disparities have been 
extensively discussed in earlier sections of this study.  
In light of these findings, we have elected to employ the af90 dataset as the training 
foundation for PepHarmony, our model of choice for subsequent experiments. The 
ensuing chapters will delve into the implications of this decision and the resulting 
performance enhancements in peptide representation learning.  
      
 
(a)                                    (b) 
     
 
(c)                                    (d) 
Figure 3: The impact of AlphaFold DB data with different confidence levels on downstream 
tasks.(a) The model trained with af90 achieved the lowest validation loss in the CPP task. (b) 
Both af90 and af80 models obtained similar validation AUC-ROC scores in the CPP task, 
significantly outperforming af50. (c) The model trained with af90 also achieved the lowest 
validation loss in the affinity task. (d) Moreover, the af90-trained model scored the best Pearson 
correlation in the affinity task. Combining all the results, it indicates that high-quality data 
positively impacts the final model's results. 
      
 
(a)                                     (b) 
Figure 4: Performance of different datasets mix methods on two downstream tasks. (a) In the 
CPP task, the model trained on the af90 dataset achieves the best validation loss, outperforming 
both data mix strategies and the PDB dataset. (b) In the Affinity task, the model trained on the 
af90 dataset not only achieves the best validation loss but also exhibits a very rapid convergence 
rate. 
 
4.4. Overall performance and comparison  
In this section, we present a quantitative analysis of our novel multi-view contrastive 
learning model, PepHarmony, against several baseline models. The evaluation metrics 
include accuracy (ACC), F1 score, receiver operating characteristic area under the 
curve (ROC-AUC), root mean square error (RMSE), and correlation coefficients 
(Pearson and Spearman), across four different tasks: CPP, Solubility, Affinity, and Self-
contact prediction.  
Our PepHarmonycl model, utilizing only contrastive loss, demonstrated superior 
performance in most tasks, outstripping the baseline and fine-tuned models. As can be 
seen in Table 1, PepHarmonycl showed remarkable improvements, particularly in the 
CPP task, with an ACC of 0.79 and an F1 score of 0.766, which are the highest among 
the compared models. Furthermore, it achieved the highest ROC-AUC scores across all 
tasks, highlighting its robust predictive capabilities.  
The comparison between the original ESM2 model and its fine-tuned counterpart 
(ESMfinetune) illustrates the benefits of fine-tuning with peptide sequence data, as 
evidenced by the uplift in performance across all tasks. The ESMfinetune model, for 
instance, shows an increase in ACC from 0.723 to 0.77 in the CPP task. Similarly, the 
GearNetfinetune model demonstrates the advantages of fine-tuning with peptide structural 
data, showing a substantial increase in performance metrics compared to the original 
GearNet model. 
Among the variations of PepHarmony, the VAE model, which incorporates both 
contrastive and generative loss, also shows promising results, often outperforming the 
baseline models and coming second only to the PepHarmonycl model. It suggests that 
the integration of generative loss with contrastive loss can enhance learning peptide 
representations but does not necessarily translate to surpassing the contrastive loss 
model in all tasks.  
In conclusion, our PepHarmonycl model emerges as the leading model for peptide 
representation learning, excelling in capturing the complex interplay between peptide 
sequences and structures. This superiority in learning efficacy makes it the best pre-
trained peptide model to date, as it effectively utilizes high-confidence data and 
advanced dataset mixing methods. 
Table 1. Results for four downstream tasks. 
 
ACC 
F1 
ROC-
 
ACC 
F1 
ROC-
 
ESM2 
0.723 
0.756 
0.864 
0.586 
0.618 
0.610 
ESM_finetune 
0.770 
0.755 
0.839 
0.645 
0.663 
0.666 
GearNet 
0.718 
0.734 
0.822 
0.584 
0.616 
0.608 
GearNet_finetune 
0.764 
0.753 
0.842 
0.612 
0.706 
0.674 
PepHarmony_cl 
0.790 
0.766 
0.874 
0.645 
0.724 
0.692 
PepHarmony_vae 
0.777 
0.764 
0.871 
0.638 
0.721 
0.682 
  
Affinity 
Self-contaction 
  
RMSE 
Pearson 
Spearman 
ACC 
F1 
ROC-
 
ESM2 
1.360 
0.465 
0.428 
0.657 
0.710 
0.790 
ESM_finetune 
1.327 
0.493 
0.473 
0.656 
0.712 
0.765 
GearNet 
1.343 
0.452 
0.426 
0.637 
0.712 
0.723 
GearNet_finetune 
1.336 
0.473 
0.444 
0.661 
0.717 
0.745 
PepHarmony cl 
1.302 
0.499 
0.447 
0.669 
0.711 
0.796  
PepHarmony_vae 
1.306 
0.495 
0.443 
0.661 
0.711 
0.764 
 
 
4.5. Model interpretability  
In the pursuit of assessing the efficacy of our novel algorithm PepHarmony in the 
context of proteomic analysis, we perform comparative experiments to scrutinize the 
capabilities of the model in fusing structural and sequence information within its 
representation space. This exploration aims to substantiate the model’s proficiency in 
delineating distinct biological tissue features across varying scales of complexity. To 
this end, we harness a peptide dataset with less than 50 amino acids in length from the 
SCOP database, resulting in a dataset of 577 sequences spanning 393 protein families. 
To streamline our analysis, we filter out families represented by fewer than 10 peptide 
sequences. Consequently, our final dataset comprises 577 sequences belonging to 27 
distinct protein families. 
In our methodological approach, we conscientiously addressed the influence of 
sequence homogeneity by implementing a clustering algorithm specifically designed to 
group peptides by their sequence similarity. This strategic clustering facilitated the 
selection of a dataset subset characterized by peptides sharing high sequence 
resemblance yet exhibiting pronounced structural diversity. The result of this selection 
process is graphically represented in the accompanying figure, where each color 
corresponds to a unique protein family and each marker shape represent a cluster, 
thereby underscoring the structural variance despite the considerable sequence 
similarity among the peptides.  
Our analysis, particularly evident in Figure 5(b), highlights PepHarmony's enhanced 
capability to discriminate between different protein families. Notably, in the right half 
of Figure 5(b), PepHarmony distinctly separates peptides belonging to different 
families, whereas in the corresponding section of Figure 5(a), these peptides are 
indistinguishably mixed. This observation underscores our model's advanced learning 
and integration of structural information. Similarly, in the upper half of the diagram, 
where all data points are circular, indicating high sequence similarity, PepHarmony still 
successfully distinguishes the blue sequences. This demonstrates that despite high 
sequence similarity, PepHarmony can effectively differentiate sequences based on their 
structural characteristics.  
Through this comparative visualization, we underscore PepHarmony’s superior 
capability in capturing and integrating the multifaceted nuances of peptide structures, 
thereby reinforcing its potential as a useful tool in protein sequence analysis. 
       
 
(a)                                   
 
 (b) 
Figure 5:  The distribution of peptide representations obtained by different algorithms 
after dimensionality reduction with t-SNE in two-dimensional space. (a) Representation 
of ESM in two-dimensional space; (b) Representation of PepHarmony in two-
dimensional space. 
 
4.6. Ablation experiments  
In the ablation study presented in Table 1, we differentiate between several key 
components of our training methodology. The ’Contrastive’ column denotes the 
application of contrastive learning tasks, which are designed to enhance the 
discriminative power of the model by contrasting positive and negative examples. 
The ’Generative’ column indicates the use of generative learning tasks, which aim to 
generate new data instances that are consistent with the training data distribution. 
The ’Contrastive pre-trained GearNet’ and ’Diffusion pre-trained GearNet’ columns 
identify the initialization strategies for our model. ’Contrastive pretrained GearNet’ 
refers to the model being initialized with parameters obtained from a GearNet model 
pre-trained with contrastive learning tasks. Similarly, ’Diffusion pre-trained GearNet’ 
pertains to the model being initialized with parameters from a GearNet model that has 
undergone pretraining with a diffusion-based learning process. Lastly, ’Data sort’ 
specifies our approach to batch construction. By sorting peptide sequences by length 
before batching, we aim to minimize padding and improve computational efficiency. 
Additionally, this approach is intended to prevent data leakage by ensuring that similar 
sequences are processed together, thereby preventing the model from inadvertently 
learning from the batch structure.  
Table 3 presents our comprehensive findings across four key downstream tasks. These 
results allow us to draw several critical conclusions:  
Effectiveness of contrastive loss and Data sorting: The CLcon sort configuration, 
which combines contrastive loss with data sorting, shows superior performance in the 
CPP task, achieving the highest ACC, F1, and ROC-AUC scores (0.79, 0.766, and 0.874, 
respectively). This configuration also maintains consistent performance in the 
Solubility task. The improvement in metrics, especially in the CPP task, underscores 
the efficacy of integrating contrastive loss with a data sorting strategy.  
Comparative analysis of pre-trained models: When comparing the pre-trained 
models, the Contrastive pretrained GearNet (CLcon and CLcon_sort) consistently 
outperforms the Diffusion pre-trained GearNet (CLdiff_sort) across most metrics in 
different tasks. For instance, in the Affinity task, CLcon_sort achieves a lower RMSE 
(1.302) and higher Pearson (0.499) and Spearman (0.447) scores compared to CLdiff_sort. 
This indicates a clear advantage of using a contrastive learning-based pre-training over 
diffusion-based pre-training for GearNet.  
Limited impact of VAE loss: The inclusion of VAE loss (CLvae_con_sort) results in 
marginal improvements in some tasks, such as a slightly higher F1 score in Solubility 
(0.721). However, its overall impact across tasks is limited compared to the other 
configurations, particularly in the Affinity and Self-contaction tasks, where its 
performance is comparable or slightly lower than the other models. 
In summary, our ablation experiments clearly demonstrate the superiority of using 
contrastive loss in combination with a data sorting strategy for training the model. 
Additionally, they reveal that a Contrastive learning pre-trained GearNet is more 
effective than a Diffusion pre-trained GearNet for the tasks examined. The limited 
contribution of VAE loss to overall model performance suggests its lesser relevance in 
the context of our specific application. 
Table 2. Ablation on the pre-train learning tasks and pretraining parameters. 
 
Contrastive 
Generative 
Contrastive pre-trained 
GearNet 
Diffusion 
pre-
trained GearNet 
Data sort 
CL_con 
✓ 
 
✓ 
 
 
CL_con_sort 
✓ 
 
✓ 
 
✓ 
CL_diff_sort 
✓ 
 
 
✓ 
✓ 
CL_vae_con_sort 
✓ 
✓ 
✓ 
 
✓ 
 
Table 3. Results for Ablation experiments on four downstream tasks.  
  
CPP 
Solubility 
  
ACC 
F1 
ROC-AUC 
ACC 
F1 
ROC-AUC 
CL_con 
0.773 
0.762 
0.843 
0.645 
0.693 
0.691 
CL_con_sort 
0.790 
0.766 
0.874 
0.645 
0.672 
0.692 
CL_diff_sort 
0.764 
0.753 
0.855 
0.625 
0.693 
0.644 
CL_vae_con_sort 
0.777 
0.764 
0.871 
0.638 
0.721 
0.682 
  
Affinity 
Self-contaction 
  
RMSE 
Pearson 
Spearman 
ACC 
F1 
ROC-AUC 
CL_con 
1.347 
0.445 
0.409 
0.657 
0.713 
0.788 
CL_con_sort 
1.302 
0.499 
0.447 
0.669 
0.711 
0.796 
CL_diff_sort 
1.299 
0.778 
0.449 
0.655  
0.705 
0.769 
CL_vae_con_sort 
1.306 
0.495 
0.443 
0.661 
0.711 
0.764 
 
5. Conclusions and future work  
In this study, we introduce PepHarmony, a pioneering model in the field of peptide 
representation learning, notable for its unique identity as a pure sequence-based encoder. 
Utilizing a novel multi-view contrastive learning approach, PepHarmony successfully 
integrates sequence and structural information in its training phase. However, it’s 
crucial to note that during inference, the model exclusively leverages its sequence 
encoder. This design choice is not merely a technical detail but a strategic innovation, 
allowing the model to extract features that inherently encapsulate structural information, 
leading to exceptional results in various analytical tasks.  
Through extensive experimentation, we have demonstrated that the quality and 
preparation of training data play a pivotal role in model performance. Our exploration 
of datasets from AlphaFold DB and PDB revealed key insights into data distribution, 
quality, and training strategies. The adept handling of these datasets has been 
instrumental in harnessing their full potential for training PepHarmony. Our ablation 
studies further delineated the importance of contrastive learning and the integration of 
different pretraining strategies. The outcomes suggest that while the incorporation of 
generative loss provides certain benefits, the model predominantly excels with 
contrastive loss, especially when coupled with data sorting techniques.  
As we move forward, several avenues for future work emerge. First, the exploration of 
domain-specific adaptations of PepHarmony could be pursued to tailor the model for 
specific applications, such as targeted drug design or protein engineering. Second, 
integrating emerging datasets and continual advancements in protein structure 
prediction methods could further enhance the model’s robustness and accuracy. Finally, 
exploring the potential of transfer learning and its implications in rapidly evolving fields 
like synthetic biology could open up new frontiers for PepHarmony.  
In conclusion, PepHarmony represents a significant step forward in peptide 
representation learning. Its ability to effectively capture the intricate relationship 
between peptide sequences and structures offers a powerful tool for researchers and 
practitioners alike. We anticipate that this model will serve as a useful tool for future 
innovations in the field, contributing to advancements in drug discovery, protein 
engineering, and beyond. 
 
Acknowledgements 
This publication is based upon work supported by the King Abdullah University of Science and 
Technology (KAUST) Office of Research Administration (ORA) under Award No 
URF/1/4352-01-01, FCC/1/1976-44-01, FCC/1/1976-45-01, REI/1/5234-01-01, REI/1/5414-
01-01, REI/1/5289-01-01, REI/1/5404-01-01. 
This work is also supported by the Senior and Junior Technological Innovation Team 
(20210509055RQ), the Jilin Provincial Key Laboratory of Big Data Intelligent Computing 
(20180622002JC), and the Fundamental Research Funds for the Central Universities, JLU. 
 
References 
1. Apostolopoulos V, Bojarska J, Chai T-T et al. A Global Review on Short Peptides: 
Frontiers and Perspectives, Molecules 2021;26. 
2. Gongora-Benitez M, Tulla-Puche J, Albericio F. Multifaceted Roles of Disulfide 
Bonds. Peptides as Therapeutics, Chemical Reviews 2014;114:901-926. 
3. Wienk HLJ, Czisch M, de Kruijff B. The structural flexibility of the preferredoxin 
transit peptide, Febs Letters 1999;453:318-326. 
4. El Amri C, Bruston F, Joanne P et al. Intrinsic flexibility and structural adaptability 
of Plasticins membrane-damaging peptides as a strategy for functional versatility, 
European Biophysics Journal with Biophysics Letters 2007;36:901-909. 
5. Rives A, Meier J, Sercu T et al. Biological structure and function emerge from 
scaling unsupervised learning to 250 million protein sequences, Proceedings of the 
National Academy of Sciences of the United States of America 2021;118. 
6. Guntuboina C, Das A, Mollaei P et al. PeptideBERT: A Language Model Based on 
Transformers for Peptide Property Prediction, J Phys Chem Lett 2023;14:10427-
10434. 
7. Fjell CD, Hiss JA, Hancock RE et al. Designing antimicrobial peptides: form 
follows function, Nat Rev Drug Discov 2011;11:37-51. 
8. Owji H, Nezafat N, Negandaripour M et al. A comprehensive review of signal 
peptides: Structure, roles, and applications, European Journal of Cell Biology 
2018;97:422-441. 
9. Chen Z, Zhao P, Li F et al. iFeature: a Python package and web server for features 
extraction and selection from protein and peptide sequences, Bioinformatics 
2018;34:2499-2502. 
10. Wei Q, Wang R, Jiang Y et al. ConPep: Prediction of peptide contact maps with 
pre-trained biological language model and multi-view feature extracting strategy, 
Comput Biol Med 2023;167:107631. 
11. Burley SK, Berman HM, Bhikadiya C et al. RCSB Protein Data Bank: biological 
macromolecular structures enabling research and education in fundamental biology, 
biomedicine, biotechnology and energy, Nucleic Acids Res 2019;47:D464-d474. 
12. Berman HM, Battistuz T, Bhat TN et al. The Protein Data Bank, Acta 
Crystallographica Section D 2002;58:899-907. 
13. Jumper J, Evans R, Pritzel A et al. Highly accurate protein structure prediction with 
AlphaFold, Nature 2021;596:583-589. 
14. Varadi M, Anyango S, Deshpande M et al. AlphaFold Protein Structure Database: 
massively expanding the structural coverage of protein-sequence space with high-
accuracy models, Nucleic Acids Res 2022;50:D439-d444. 
15. Brandes N, Ofer D, Peleg Y et al. ProteinBERT: a universal deep-learning model 
of protein sequence and function, Bioinformatics 2022;38:2102-2110. 
16. Whisstock JC, Lesk AM. Prediction of protein function from protein sequence and 
structure, Quarterly Reviews of Biophysics 2003;36:307-340. 
17. Bepler T, Berger B. Learning the protein language: Evolution, structure, and 
function, Cell Syst 2021;12:654-669.e653. 
18. Devlin J, Chang MW, Lee K et al. BERT: Pre-training of Deep Bidirectional 
Transformers for Language Understanding 2018. 
19. Radford A, Narasimhan K. Improving Language Understanding by Generative Pre-
Training. 2018. 
20. Zhang Z, Xu M, Jamasb A et al. Protein Representation Learning by Geometric 
Structure Pretraining 2022. 
21. Wang D, Abbas UL, Shao Q et al. S-PLM: Structure-aware Protein Language 
Model via Contrastive Learning between Sequence and Structure, bioRxiv 2023. 
22. Yan K, Fang X, Xu Y et al. Protein fold recognition based on multi-view modeling, 
Bioinformatics 2019;35:2982-2990. 
23. Zhang Z, Xu M, Chenthamarakshan V et al. Enhancing Protein Language Models 
with Structure-based Encoder and Pre-training. 2023. 
24. Wu F, Wu L, Radev D et al. Integration of pre-trained protein language models into 
geometric deep learning networks, Commun Biol 2023;6:876. 
25. Fang Y, Xu F, Wei L et al. AFP-MFL: accurate identification of antifungal peptides 
using multi-view feature learning, Brief Bioinform 2023;24. 
26. Liu X, Zhang F, Hou Z et al. Self-Supervised Learning: Generative or Contrastive, 
Ieee Transactions on Knowledge and Data Engineering 2023;35:857-876. 
27. Liu S, Wang H, Liu W et al. Pre-training Molecular Graph Representation with 3D 
Geometry. 2021, arXiv:2110.07728. 
28. Chen T, Kornblith S, Norouzi M et al. A Simple Framework for Contrastive 
Learning of Visual Representations 2020. 
29. Zhang Z, Xu M, Lozano A et al. Pre-Training Protein Encoder via Siamese 
Sequence-Structure Diffusion Trajectory Prediction. 2023, arXiv:2301.12068. 
30. Sperduti A, Starita A. Supervised neural networks for the classification of 
structures, IEEE Trans Neural Netw 1997;8:714-735. 
31. Zhou J, Cui G, Hu S et al. Graph neural networks: A review of methods and 
applications, AI Open 2020;1:57-81. 
32. Schlichtkrull M, Kipf TN, Bloem P et al. Modeling Relational Data with Graph 
Convolutional Networks 2017. 
33. Wang T, Isola P. Understanding Contrastive Representation Learning through 
Alignment and Uniformity on the Hypersphere 2020. 
34. Oord AVD, Li Y, Vinyals O. Representation Learning with Contrastive Predictive 
Coding 2018. 
35. Higgins I, Matthey L, Pal A et al. beta-VAE: Learning Basic Visual Concepts with 
a Constrained Variational Framework 2016. 
36. Vincent P, Larochelle H, Bengio Y et al. Extracting and Composing Robust 
Features with Denoising Autoencoders 2008. 
37. Kingma DP, Welling M. Auto-Encoding Variational Bayes, arXiv.org 2014. 
38. Chen X, He K. Exploring Simple Siamese Representation Learning 2020. 
39. Agrawal P, Bhalla S, Usmani SS et al. CPPsite 2.0: a repository of experimentally 
validated cell-penetrating peptides, Nucleic Acids Res 2016;44:D1098-1103. 
40. Gautam A, Singh H, Tyagi A et al. CPPsite: a curated database of cell penetrating 
peptides, Database (Oxford) 2012;2012:bas015. 
41. Fu X, Cai L, Zeng X et al. StackCPPred: a stacking and pairwise energy content-
based prediction of cell-penetrating peptides and their uptake efficiency, 
Bioinformatics 2020;36:3028-3034. 
42. Gautam A, Chaudhary K, Kumar R et al. In silico approaches for designing highly 
effective cell penetrating peptides, J Transl Med 2013;11:74. 
43. Smialowski P, Doose G, Torkler P et al. PROSO II--a new method for protein 
solubility prediction, Febs j 2012;279:2192-2200. 
44. Wishart DS, Knox C, Guo AC et al. DrugBank: a comprehensive resource for in 
silico drug discovery and exploration, Nucleic Acids Res 2006;34:D668-672. 
45. Wishart DS, Knox C, Guo AC et al. DrugBank: a knowledgebase for drugs, drug 
actions and drug targets, Nucleic Acids Res 2008;36:D901-906. 
46. Knox C, Law V, Jewison T et al. DrugBank 3.0: a comprehensive resource for 
'omics' research on drugs, Nucleic Acids Res 2011;39:D1035-1041. 
47. Law V, Knox C, Djoumbou Y et al. DrugBank 4.0: shedding new light on drug 
metabolism, Nucleic Acids Res 2014;42:D1091-1097. 
48. Wishart DS, Feunang YD, Guo AC et al. DrugBank 5.0: a major update to the 
DrugBank database for 2018, Nucleic Acids Res 2018;46:D1074-d1082. 
49. Chowdhury R, Bouatta N, Biswas S et al. Single-sequence protein structure 
prediction using a language model and deep learning, Nat Biotechnol 
2022;40:1617-1623. 
50. Unterthiner T, Mayr A, Wegner JK. Deep Learning as an Opportunity in Virtual 
Screening. 2015. 
51. Shen L, Feng H, Qiu Y et al. SVSBI: sequence-based virtual screening of 
biomolecular interactions, Commun Biol 2023;6:536. 
 
","Understanding the intricate interplay between protein sequences and structures is pivotal in computational biology and bioinformatics. Recent advancements have led to the development of various computational models, each focusing on different aspects of protein analysis. This section reviews the relevant literature, categorized into three main areas: sequence-based protein language models, structure-based protein pre-trained models, and models that infuse both structure and sequence information.

Sequence-based protein language models
Protein language models based on sequence data have made significant strides, especially with the adoption of deep learning techniques. Models like BERT and GPT, originally designed for natural language processing, have been adapted to interpret the ‘language’ of proteins. These adaptations, such as ESM, leverage the sequential nature of amino acids to predict protein functionality and interactions. By training on large datasets of known protein sequences, these models have shown remarkable proficiency in capturing the underlying biological properties inherent in sequence data.

However, their reliance solely on sequence information limits their ability to fully predict protein behavior, which is also influenced by three-dimensional structural conformations.

Structure-based protein pretrained models
Structure-based models focus on understanding proteins from their three-dimensional conformations. With the advent of more sophisticated structural prediction algorithms and the increasing availability of structural data, these models have gained prominence. Notable among them is AlphaFold, which predicts protein structures with remarkable accuracy. Models like GearNet go a step further by incorporating spatial and chemical information into their analyses, providing a more nuanced understanding of protein function and interaction. These models have significantly advanced the field’s ability to predict protein functionality based on structure, yet they often overlook the valuable insights that can be gleaned from sequence data.nan"
"Deep Neural Networks are being utilized to support the operation of semi-autonomous cars through object identification and semantic segmentation. To assess the inadequacy of the current dataset in the context of autonomous and semi-autonomous cars, we created a new dataset named ANNA. The research presented in this paper also emphasizes the importance of developing accurate and efficient object detection algorithms for the advancement of autonomous vehicles.","The advancement of general-purpose computers in the late 1990s made it possible to process large amounts of data quickly. Object detection on a mobile camera for AV necessitates detecting and identifying multiple things in the camera’s field of view using computer vision techniques. Other cars, pedestrians, traffic signs, and other entities that may be significant to the vehicle’s navigation and object avoidance can be included. The autonomous car then uses this information to assess its surroundings, plot a course, and make judgments while driving.","A six-step process was used to construct a multiple object detection model for autonomous vehicles from the perspective of Bangladesh (Fig. 1). To develop autonomous vehicles equipped with mobile cameras, the steps are described in the following subsections:
1) Planning the route: Specific routes were planned for data collection, ensuring coverage of various types of areas including urban and rural roads, freeways, and crossroads.
2) Setting up the vehicle: The vehicle was equipped with a mobile camera capable of recording high-definition video, which was mounted in a position to provide a clear view of the surroundings.
3) Scene selection: The vehicle was operated in a manner that allowed for the recording of a wide range of scenes, including different traffic situations, weather conditions, and lighting conditions.","A comparison was conducted between the custom ANNA dataset and one of the most popular datasets, MS-COCO, for multiple object detection evaluation. Initially, a model trained on the MS-COCO dataset was applied to the dataset collected from Bangladeshi roads to generate predictions. To ensure that only objects from the selected 9 classes were detected by the model, some modifications were made during the prediction process. Subsequently, another model was trained on the ANNA dataset, and using this model, predictions were made on the dataset collected from Bangladesh. Finally, a comparison was made between the two sets of predictions and the ground truth.","The research presented in this paper also emphasizes the importance of developing accurate and efficient object detection algorithms for the advancement of autonomous vehicles. Further work will be done on increasing the number of images by increasing the challenging scenarios; e.g. occluded objects such as a person occluded under an umbrella, the field of views from each object, humans or other animals in vehicles, vehicles in bad weather conditions and by capturing it from a camera within a vehicle itself.",ANNA: A Deep Learning Based Dataset in Heterogeneous Traffic for Autonomous Vehicles,"Mahedi Kamal, Tasnim Fariha, Afrina Kabir Zinia, Md. Abu Syed, Fahim Hasan Khan, Md. Mahbubur Rahman","ANNA: A Deep Learning Based Dataset in
Heterogeneous Traffic for Autonomous Vehicles
Mahedi Kamal*1 Tasnim Fariha*1, Afrina Kabir Zinia1,
Md. Abu Syed 1, Fahim Hasan Khan2, Md. Mahbubur Rahman1
1Department of Computer Science and Engineering, Military Institute of Science and Technology,
Mirpur Cantonment, Dhaka-1216, Bangladesh
2University of California, Santa Cruz, CA 95064, USA
Email: mahedikamal44@gmail.com, nirjonafariha@gmail.com, afrinakabir104@gmail.com,
nihonsyed@gmail.com, fkhan4@ucsc.edu, mahbub@cse.mist.ac.bd
Abstract—Recent breakthroughs in artificial intelligence offer
tremendous promise for the development of self-driving applica-
tions. Deep Neural Networks, in particular, are being utilized
to support the operation of semi-autonomous cars through
object identification and semantic segmentation. To assess the
inadequacy of the current dataset in the context of autonomous
and semi-autonomous cars, we created a new dataset named
ANNA. This study discusses a custom-built dataset that includes
some unidentified vehicles in the perspective of Bangladesh,
which are not included in the existing dataset. A dataset validity
check was performed by evaluating models using the Intersection
Over Union (IOU) metric. The results demonstrated that the
model trained on our custom dataset was more precise and
efficient than the models trained on the KITTI or COCO dataset
concerning Bangladeshi traffic. The research presented in this
paper also emphasizes the importance of developing accurate
and efficient object detection algorithms for the advancement of
autonomous vehicles.
Index Terms—Autonomous Vehicle, Computer Vision, Object
Detection, Dataset, Deep Learning
I. INTRODUCTION
The advancement of general-purpose computers in the late
1990s made it possible to process large amounts of data
quickly. A common approach at that time involved extracting
feature vectors from images and applying machine learning
techniques for image recognition. However, the emergence
of deep learning revolutionized computer vision and greatly
enhanced its capabilities. Computer vision, an area of Ar-
tificial Intelligence (AI), enables computers to perceive and
understand digital images in a manner similar to human
vision. This technology finds applications in various industries,
such as facial identification, augmented reality, driverless cars,
healthcare, and more. For autonomous vehicles (AVs) to
operate effectively, they require accurate perception of their
surroundings, which can be achieved through computer vision
technology.
As Bangladesh is a densely populated country with con-
gested roads, the viability of autonomous cars in such an
environment is uncertain. Therefore, we aim to obtain a fresh
dataset from the roadways of Bangladesh and conduct a
*These authors contributed equally to this work
comparative analysis with existing datasets. Our objective is
to develop a real-time object detection Deep Learning (DL)
model that can run directly on mobile phones without requiring
server-side processing or an internet connection
[1]. This
model is trained on the new dataset, which includes objects and
vehicles specific to Bangladesh and not present in resources
like Microsoft COCO or other AV datasets. By comparing the
MS COCO dataset with our new dataset, we aim to assess
the performance of the AV model on the new dataset and
determine the potential benefits of autonomous vehicles on
Bangladeshi roads.
Our primary focus is on data collection with the detection
of multiple objects on roadways for autonomous vehicles
utilizing real-time object detection modules. The Waymo Open
Dataset Challenges are accepted which contain ”Real-time 2D
Detection”
[2], [3] and ”3D Camera-Only Detection”
[4].
All prior Waymo Open Dataset challenges used data from
multiple cameras, LiDAR, Radar, and other sensors, until Tesla
popularised the notion of camera-only detection for AV [5].
Because Tesla and Waymo are moving in that route, our focus
is solely on vision only using smartphones and all other costly
hardware components are abandoned.
Multiple object detection on a mobile camera for AV
necessitates detecting and identifying multiple things in the
camera’s field of view using computer vision techniques. Other
cars, pedestrians, traffic signs, and other entities that may be
significant to the vehicle’s navigation and object avoidance can
be included. The autonomous car then uses this information
to assess its surroundings, plot a course, and make judgments
while driving. This data is used by the control system of
driverless vehicles to make safe and efficient driving decisions.
YOLOv5 (You Only Look Once version 5) is a cutting-edge
object detection method used in computer vision. Using this
model is intended for speed and accuracy optimization in
real-world object identification applications, and it employs a
single CNN to recognize items inside an image and calculate
the bounding box coordinates and class probabilities for each
identified object [6], [7]. On standard datasets, YOLOv5 has
proven great accuracy and high-speed detection in contrast
to other models, and its compact model size makes it suitable
arXiv:2401.11358v1  [cs.CV]  21 Jan 2024
for usage on devices with limited computational resources [8],
[9]. In this research, we concentrated on collecting image data
from public roads using only a single smartphone camera.
We assembled and annotated a dataset utilizing images of
real-world photographs extracted from videos that were shot
from Bangladeshi urban streets. We have tested the dataset
to evaluate the performance of the CNN-based YOLOv5
model for identifying unique vehicles which are available in
Bangladesh. Then, we compared the ANNA dataset with the
COCO dataset and presented the evaluation of the IOU metrics
for multiple object detection on the roadways. This approach
aims to provide autonomous cars with enough knowledge to
travel safely in excessive traffic conditions in Bangladesh.
II. RELATED WORK
Self-driving technology [10] is a long-term endeavor to im-
prove safety by automating vehicle control. The autonomous
car is a decision-making system that gathers data from nu-
merous sources, such as cameras, LiDAR, RADAR, and other
sensors [11]. With the increasing use of Deep Neural Networks
(DNN) models for object detection, it can be stated that DNN
is capable of processing information in real-time and can
transmit information to cloud storage as well as to automated
vehicles [12]. Deep Learning involves training and testing on
labeled data, which can be labeled in the case of self-driving
cars and annotated using ground-truth bounding boxes [12].
To recognize and classify objects, machine learning (ML)
methods such as Naive Bayes, Support Vector Machines, and
CNN-based DL models are utilized [13]. CNN-based systems
can play a significant role in pedestrian, lane, and redundant
object detection at moderate distances [11], [14]. CNN models
have achieved 100% classification rates on datasets such as
ImageNet [15]. In YOLO, multi-class object detection can be
performed in real-time [14]. YOLO is a one-stage detector
with a high detection speed that directly predicts the bounding
box parameters for the objects in the image and classifies these
objects simultaneously [7], [16].
The camera can be a cost-effective solution compared to
LIDAR for basic detection needs in autonomous driving [17].
Monocular cameras provide detailed information in the form
of pixel intensities, which, at a larger scale, reveal shape
and texture properties [18]. Using a monocular camera, it
is possible to estimate the distance of objects in front of
autonomous vehicles [17], [19]. Monocular image-based 3D
object detection methods utilize RGB images to predict 3D
object bounding boxes [18].
The Cityscapes dataset contains pixel-level semantic la-
beling for 25k images depicting urban scenarios from 50
cities [20]. The ApolloScape dataset [21] provides 140k
labeled images of street views for lane detection, car de-
tection, semantic segmentation, and more. It enables perfor-
mance evaluation across different times of day and weather
conditions. The WoodScape dataset for AV [22] offers 10k
images captured from four dedicated fisheye cameras with
semantic annotations for 40 classes. The nuScenes dataset [23]
addresses the need for multimodal data and includes 23
categories such as vehicles, pedestrians, mobility devices, and
other objects. It comprises 1000 scenes, each 20 seconds
long, with 3D bounding box annotations for 23 classes and
8 attributes, captured using six cameras, five radars, and
one lidar with a full 360-degree field of view. The Oxford
RobotCar dataset [24] route in Oxford, UK and consists of 32
traversals in different traffic, weather, and lighting conditions
totalling 280 km of urban driving resulting 4.7TB of the .
The Lyft dataset [25] evaluates the prediction efficiency of
DL models by calculating the Root Mean Square Error and
provides high-definition scene data with bounding boxes and
class probabilities. It includes over 1000 hours of training data
along a single route. The MS COCO [26] dataset consists of
328k images with 80 labels representing commonly available
objects. The Coyote dataset [27] comprises 894 real-world
photographs with over 1700 ground-truth labels, grouped into
six broad categories. The WildDash [28] dataset contains
1800 frames that address natural risks in images, such as
distortion, overexposure, windscreen, road conditions, weather,
and lighting variations, from diverse geographic locations.
The KITTI dataset, which consists of 13k images of raw
data, was obtained by annotating 400 dynamic scenes using
detailed 3D CAD models for all moving vehicles [29]. The
Virtual KITTI dataset [30] reconstructs the KITTI environment
using a gaming engine and 3D models to create labeled data
with different variations. CARLA [31] is an open-source au-
tonomous driving simulation tool that allows for customizable
environmental setups and sensor configurations. Experimental
results show that simulator-generated datasets yield similar
DNN prediction errors to real-world datasets [32]. However,
it is important to note that simulator-generated datasets may
not fully represent real-life driving environments, and safety
violations were identified [32]. Therefore, in this paper, a
custom-made dataset collected from a real-life environment
was utilized.
Since our dataset includes a diverse range of objects beyond
just vehicles, we leverage the MS COCO dataset [26] as
the foundation for our trained classifiers and for compara-
tive evaluation. While some datasets offer precise pixel-level
semantic segmentations, they may not encompass the wide
range of scenarios present in our ANNA dataset. The use of
the ANNA dataset is intended as an initial step to evaluate
computer vision for autonomous vehicles in challenging traffic
conditions and with uncommon vehicles. Furthermore, this
dataset can be expanded to train computer vision systems that
are more robust and capable of handling adversarial situations,
ultimately enhancing car safety.
III. OVERVIEW OF THE ANNA DATASET
A six-step process was used to construct a multiple object
detection model for autonomous vehicles from the perspective
of Bangladesh (Fig. 1). To develop autonomous vehicles
equipped with mobile cameras, the steps are described in the
following subsections:
Fig. 1. Overview of the ANNA dataset
Fig. 2. Extracted image dataset
A. Data Collection
1) Planning the route: Specific routes were planned for
data collection, ensuring coverage of various types of areas
including urban and rural roads, freeways, and crossroads.
Our data was collected from multiple roads, including those
in Mirpur (Dhaka), Bashundhara Residential Area (Dhaka),
Jhenidah (Khulna), and other locations.
2) Setting up the vehicle: The vehicle was equipped with
a mobile camera capable of recording high-definition video,
which was mounted in a position to provide a clear view of the
surroundings. Our dataset was collected using various devices
including the iPhone 13 Pro, Huawei Nova 3i, and Samsung
Galaxy A12 (48 Megapixels).
3) Scene selection: The vehicle was operated in a manner
that allowed for the recording of a wide range of scenes,
including different traffic situations, weather conditions, and
lighting conditions. Over 20 videos were collected from low-
traffic areas, and 1800 images were manually selected for an-
notation. When choosing scenes for autonomous cars equipped
with mobile cameras, it is important to consider various
scenarios, including high-traffic areas such as intersections and
construction sites, rare vehicle classes, and potentially danger-
ous situations. To ensure that the dataset included a diverse
range of vehicles commonly seen in Bangladeshi neighbor-
hoods, footage of several vehicle types, including cars, buses,
CNGs (compressed natural gas-driven three-wheelers), vans
(rickshaw vans), easy bikes, motorbikes, rickshaws, and others,
was collected.
B. Data Extraction
Image data extraction typically involves reading image files
and converting them into a format that can be utilized by
machine learning models or other computer vision algorithms
(Fig. 2. The following provides a high-level overview of the
process:
1) Read the video file: The first step is to use a video I/O
library or API to read the video file. The video frames are
accessible through the library as a series of images.
2) Extract individual frames: Once the video is loaded,
individual frames are extracted from the video sequence. This
is done by looping over the video frames using a customized
Python script.
3) Convert the frames to a standard format: After extract-
ing the frames, they are converted into a standard picture
format that can be used by machine learning models or other
algorithms.
C. Data Annotation
To provide detailed information about the objects identified
in each frame, the video data collection needs to be annotated.
Computer vision algorithms are trained using this annotated
data to detect objects in video frames. For this project, a
specific set of 10 scenes and 1800 selected images were
manually chosen for annotation (Fig 3). The annotation pro-
cess involved labeling eight classes, including humans, cars,
buses, rickshaws, bikes, CNGs, bicycles, easy bikes, and vans
(rickshaw vans). Expert annotators were employed to annotate
objects continuously throughout each scene. An online AI
software called makesense.ai was utilized for the annotation
process.
By
involving
experts
in
the
annotation
process
and
implementing
multiple
validation
steps,
highly
accurate
annotations were achieved. Labeling objects in the captured
video data is a necessary step in the data annotation process for
an autonomous vehicle implementing the YOLOv5 computer
vision algorithm. The system is trained to identify objects
in each video frame, such as other vehicles, pedestrians,
and more, using the annotated data. The location and class
of each object in every frame are manually marked as part
of the data annotation process. The dataset is available at
https://github.com/MahediKamal/ANNA-A-Deep-Learning-
Based-Dataset-in-Heterogenous-Traffic-for-Autonomous-
Vehicles.
D. Model Training with Data
1) Algorithms and frameworks used: After the data has
been annotated, it can be utilized to train the YOLOv5 object
detection model. The model is then tested on fresh, unlabeled
data to evaluate its accuracy and make any necessary adjust-
ments. YOLOv5, a well-known deep learning (DL) library,
can be employed for training object detection models. Torch,
a DL framework, provides a wide range of tools, libraries,
and support for constructing, refining, and deploying DL
models, including computer vision tasks like object classifi-
cation. When used in conjunction with YOLOv5, the Torch
Fig. 3. Data annotation
framework enables network improvement and reduction of
error between the predicted and actual object positions. Once
the model is trained, it can be applied in autonomous driving
applications.
2) Training the model: While constructing the model, the
data was divided into two sets: (1) test data and (2) training
data. Each set consisted of images along with their correspond-
ing annotations. The model was trained for 100 epochs with a
batch size of 4. In YOLOv5, weight files and pointer files are
utilized to store the learned parameters of the neural network
model during the training process. Pointer files (.yaml) are text
files that specify the structure of the neural network model
and the location of the weight files on disk. These pointer
files contain metadata about the network architecture, such as
the number and size of layers, as well as the location of the
weight files that contain the learned parameters.
Weight files (.pt) contain the actual numerical values of the
trained parameters of the neural network model. These weight
files are binary files that can be loaded into the model during
inference to apply the learned parameters to new input data.
During training, the weights are adjusted to minimize the error
between the predicted and actual object locations and are then
stored in the weight files. Together, the weight files and pointer
files enable the YOLOv5 algorithm to accurately detect objects
in real-world scenarios.
E. Model Deployment
After the model has been trained and tested, it can be
deployed for use in an autonomous car or another system
to detect objects in a live stream. During deployment, the
YOLOv5 model takes a video frame as input and applies a
series of CNN layers to generate a set of bounding boxes
around objects in the frame. Each bounding box is assigned
a class label and a confidence score, indicating the model’s
confidence in the presence of an object of that class within
the bounding box. The model then applies non-maximum
suppression to eliminate overlapping bounding boxes and
outputs the final set of object detections.
Fig. 4. Detection of common and rare vehicles
F. Model Testing and Evaluation
The Mean Average Precision (MAP) metric was utilized
to evaluate the performance of our multiple object detection
model (Fig. 4). A higher MAP value indicates better perfor-
mance. MAP calculates the average precision across all object
categories. We evaluated the model’s performance on the test-
ing set, which consisted of images that were not used during
training. For each image in the testing set, we computed the
average precision for each object category and then averaged
the results across all object categories to obtain the MAP score.
To calculate the average precision for each object category,
we initially computed accuracy and recall values for various
confidence thresholds. True positive, false positive, and false
negative values were determined by comparing the projected
bounding boxes with the ground truth bounding boxes. The
precision and recall data were used to calculate the average
precision for each category.
IV. RESULT ANALYSIS
A comparison was conducted between the custom ANNA
dataset and one of the most popular datasets, MS-COCO, for
multiple object detection evaluation. Initially, a model trained
on the MS-COCO dataset was applied to the dataset collected
from Bangladeshi roads to generate predictions. To ensure
that only objects from the selected 9 classes were detected
by the model, some modifications were made during the
prediction process. Subsequently, another model was trained
on the ANNA dataset, and using this model, predictions were
made on the dataset collected from Bangladesh. Finally, a
comparison was made between the two sets of predictions
Object Class (Ve-
hicle names)
Average
Precision
(AP)
for
ANNA
dataset (%)
Average
Precision
(AP)
for
COCO
dataset (%)
Human (0)
90.73
92.26
Car (1)
96.05
95.05
Bus (2)
87.7
82.25
Rickshaw (3)
96.92
0.00
Bike (4)
95.59
89.50
CNG (5)
95.40
0.00
Bicycle (6)
93.20
82.34
Easy Bike (7)
97.95
0.00
Van (8)
91.11
0.00
TABLE I
COMPARISON BETWEEN ANNA AND COCO DATASET
and the ground truth. For this, the concept of IOU (Inter-
section Over Union) was used. IOU is a measure based on
the Jaccard Index that evaluates the overlap between two
bounding boxes. It requires a ground truth bounding box and
a predicted bounding box. By applying the IOU we can tell
if a detection is valid (True Positive) or not (False Positive).
Based on IOU performance metrics; average precision (AP)
is calculated. Average precision (AP) is a commonly used
evaluation metric in machine learning for object detection or
image segmentation tasks. It measures the average precision
of a machine learning model across all possible values of
the threshold used for making predictions. The AP value
provides an overall assessment of the model’s performance.
The comparison table below presents the results obtained from
the MS COCO and ANNA datasets.
Considering the 9 classes of vehicles specified earlier; the
mean average precision (MAP) for the ANNA dataset is
93.85%, while for the COCO dataset, it is 49.04 %. The
MAP value significantly decreases for the COCO dataset due
to its inability to detect rickshaws, CNGs, easy bikes, and vans
(rickshaw vans). However, since the ANNA dataset includes
data specifically for these vehicles, a model trained on ANNA
can accurately detect them.
V. CONCLUSION
In this paper, the ANNA dataset is presented which was
collected from busy and public roads of Bangladesh and also,
tracking tasks, metrics, baselines, and results are shown. This
is the first dataset where uncommon vehicles of Bangladesh
which can not be found in any of the AV datasets, are
collected to test on public roads. Our dataset is only based
on a mobile camera which completes the Waymo challenge
containing “Real-time 2D Detection” [2], [3] and ”3D Camera-
Only Detection” [4]. The IOU metrics are introduced that
balance all components of detection performance in order
to encourage research on 3D object detection for AVs in
Bangladesh. Further work will be done on increasing the
number of images by increasing the challenging scenarios; e.g.
occluded objects such as a person occluded under an umbrella,
the field of views from each object, humans or other animals in
vehicles, vehicles in bad weather conditions and by capturing
it from a camera within a vehicle itself.
REFERENCES
[1] F. H. Khan, A. de Silva, G. Dusek, J. Davis, and A. Pang, “Authoring
platform for mobile citizen science apps with client-side ml,” in
Companion Publication of the 2021 Conference on Computer Supported
Cooperative Work and Social Computing, ser. CSCW ’21 Companion.
New York, NY, USA: Association for Computing Machinery, 2021, p.
89–94. [Online]. Available: https://doi.org/10.1145/3462204.3481743
[2] Z. Yueming, X. Song, B. Bai, T. Xing, C. Liu, X. Gao, Z. Wang, Y. Wen,
H. Liao, G. Zhang, and P. Xu, “2nd place solution for waymo open
dataset challenge - real-time 2d object detection,” 06 2021.
[3] “Waymo open challenges 2021,” https://waymo.com/open/challenges/
2021/real-time-2d-prediction/, [Accessed 11-May-2023].
[4] “Waymo open challenges 2022,” https://waymo.com/open/challenges/
2022/3d-camera-only-detection/, [Accessed 11-May-2023].
[5] R. Chen and H. Mao, “The impact of autopilot on tesla,” BCP Business
& Management, vol. 31, pp. 89–95, 11 2022.
[6] M. Karthi, V. Muthulakshmi, R. Priscilla, P. Praveen, and K. Vanisri,
“Evolution of yolo-v5 algorithm for object detection: Automated de-
tection of library books and performace validation of dataset,” in
2021 International Conference on Innovative Computing, Intelligent
Communication and Smart Electrical Systems (ICSES), 2021, pp. 1–6.
[7] A. Benjumea, I. Teeti, F. Cuzzolin, and A. Bradley, “Yolo-z: Improving
small object detection in yolov5 for autonomous vehicles,” 11 2021.
[8] D. Dluˇznevskij, P. Stefanoviˇc, and S. Ramanauskait˙e, “Investigation
of yolov5 efficiency in iphone supported systems,” Baltic Journal of
Modern Computing, vol. 9, 01 2021.
[9] J. Wu, J. Dong, W. Nie, and Z. Ye, “A lightweight yolov5 optimization
of coordinate attention,” Applied Sciences, vol. 13, no. 3, 2023.
[Online]. Available: https://www.mdpi.com/2076-3417/13/3/1746
[10] I. Kotseruba and J. Tsotsos, “Behavioral research and practical models
of drivers’ attention,” 04 2021.
[11] E. Khatab, A. Onsy, M. Varley, and A. abouelfarag, “Vulnerable objects
detection for autonomous driving: A review,” Integration the VLSI
Journal, vol. 78, pp. 36–48, 01 2021.
[12] A. Gupta, L. Guan, and A. Khwaja, “Deep learning for object detection
and scene perception in self-driving cars: Survey, challenges, and open
issues,” Array, vol. 10, p. 100057, 02 2021.
[13] A. Juyal, S. Sharma, and P. Matta, “Deep learning methods for object
detection in autonomous vehicles,” in 2021 5th International Conference
on Trends in Electronics and Informatics (ICOEI), 06 2021, pp. 751–
755.
[14] H. Fujiyoshi, T. Hirakawa, and T. Yamashita, “Deep learning-based
image recognition for autonomous driving,” IATSS Research, vol. 43,
12 2019.
[15] Y. Li, F. Cui, X. Xue, and J. C.-W. Chan, “Coarse-to-fine salient
object
detection
based
on
deep
convolutional
neural
networks,”
Signal Processing: Image Communication, vol. 64, pp. 21–32, 2018.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S0923596518300985
[16] H.
Fujiyoshi,
T.
Hirakawa,
and
T.
Yamashita,
“Deep
learning-
based image recognition for autonomous driving,” IATSS Research,
vol.
43,
no.
4,
pp.
244–252,
2019.
[Online].
Available:
https:
//www.sciencedirect.com/science/article/pii/S0386111219301566
[17] A. Pidurkar, R. Sadakale, and A. Prakash, “Monocular camera based
computer vision system for cost effective autonomous vehicle,” in
2019 10th International Conference on Computing, Communication and
Networking Technologies (ICCCNT), 2019, pp. 1–5.
[18] E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby, and
A. Mouzakitis, “A survey on 3d object detection methods for au-
tonomous driving applications,” IEEE Transactions on Intelligent Trans-
portation Systems, vol. 20, no. 10, pp. 3782–3795, 2019.
[19] D. Bao and P. Wang, “Vehicle distance detection based on monocular
vision,” in 2016 International Conference on Progress in Informatics
and Computing (PIC).
IEEE, 2016, pp. 187–191.
[20] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benen-
son, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for
semantic urban scene understanding,” 06 2016.
[21] X. Huang, P. Wang, X. Cheng, D. Zhou, Q. Geng, and R. Yang,
“The apolloscape open dataset for autonomous driving and its applica-
tion,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 42, no. 10, pp. 2702–2719, 2020.
[22] S. Yogamani, C. Hughes, J. Horgan, G. Sistu, S. Chennupati, M. Uricar,
S. Milz, M. Simon, K. Amende, C. Witt, H. Rashed, S. Nayak,
S. Mansoor, P. Varley, X. Perrotton, D. Odea, and P. P´erez, “Woodscape:
A multi-task, multi-camera fisheye dataset for autonomous driving,” in
2019 IEEE/CVF International Conference on Computer Vision (ICCV),
2019, pp. 9307–9317.
[23] H. Caesar, V. K. R. Bankiti, A. Lang, S. Vora, V. E. Liong, Q. Xu, A. Kr-
ishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal
dataset for autonomous driving,” 06 2020, pp. 11 618–11 628.
[24] W. Maddern, G. Pascoe, M. Gadd, D. Barnes, B. Yeomans, and P. New-
man, “Real-time kinematic ground truth for the oxford robotcar dataset,”
arXiv preprint arXiv:2002.10152, 2020.
[25] S. Mandal, S. Biswas, V. E. Balas, R. N. Shaw, and A. Ghosh,
“Motion prediction for autonomous vehicles from lyft dataset using deep
learning,” in 2020 IEEE 5th International Conference on Computing
Communication and Automation (ICCCA), 2020, pp. 768–773.
[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. Zitnick, “Microsoft coco: Common objects in context,”
05 2014.
[27] S. Gupta, I. Ullah, and M. Madden, “Coyote: A dataset of challenging
scenarios in visual perception for autonomous vehicles,” 08 2021.
[28] O. Zendel, K. Honauer, M. Murschitz, D. Steininger, and G. Dom´ınguez,
WildDash - Creating Hazard-Aware Benchmarks: 15th European Con-
ference, Munich, Germany, September 8–14, 2018, Proceedings, Part
VI, 09 2018, pp. 407–421.
[29] M. Menze and A. Geiger, “Object scene flow for autonomous vehicles,”
in 2015 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2015, pp. 3061–3070.
[30] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig, “Virtualworlds as proxy for
multi-object tracking analysis,” in 2016 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2016, pp. 4340–4349.
[31] A. Dosovitskiy, G. Ros, F. Codevilla, A. L´opez, and V. Koltun, “Carla:
An open urban driving simulator,” 11 2017.
[32] F. Ul Haq, D. Shin, S. Nejati, and L. Briand, “Comparing offline and
online testing of deep neural networks: An autonomous car case study,”
11 2019.
","nanDeep Learning involves training and testing on labeled data, which can be labeled in the case of self-driving cars and annotated using ground-truth bounding boxes [12]. To recognize and classify objects, machine learning (ML) methods such as Naive Bayes, Support Vector Machines, and CNN-based DL models are utilized [13]. CNN-based systems can play a significant role in pedestrian, lane, and redundant object detection at moderate distances [11], [14]. CNN models have achieved 100% classification rates on datasets such as ImageNet [15]. In YOLO, multi-class object detection can be performed in real-time [14]. YOLO is a one-stage detector with a high detection speed that directly predicts the bounding box parameters for the objects in the image and classifies these objects simultaneously [7], [16]."
"We provide an analysis of the squared Wasserstein-2 (W2) distance between two probability distributions associated with two stochastic differential equations (SDEs). Based on this analysis, we propose the use of a squared W2 distance-based loss functions in the reconstruction of SDEs from noisy data. To demonstrate the practicality of our Wasserstein distance-based loss functions, we performed numerical experiments that demonstrate the efficiency of our method in reconstructing SDEs that arise across a number of applications.","Stochastic processes are mathematical models of random phenomena that evolve over time or space (Cinlar, 2011). Among stochastic processes, stochastic differential equations (SDE) of the form
dX(t) = f(X(t), t)dt + σ(X(t), t)dB(t), t ∈ [0, T]
(1)
∆are widely used across different fields to model complex systems with continuous variables and noise. Here, f and σ denote deterministic and stochastic components of the SDE, while B(t) represents Brownian motion. In applications such as computational fluid dynamics, cell biology, and genetics, the underlying dynamics are often unknown, partially observed, and subjected to noise. Consequently, it is vital to develop methods capable of reconstructing the governing SDEs from limited data (Sullivan, 2015; Soize, 2017; Mathelin et al., 2005; Bressloff, 2014; Lin and Buchler, 2018). Traditional methods, such as the Kalman filter (Welch et al., 1995; Welch, 2020) and Gaussian process regression (Liu et al., 2020; MacKay et al., 1998) often assume specific forms of noise. These methods may not be suitable for complex or nonlinear systems where noise affects the dynamics in a more complex manner.","Traditional methods for reconstructing SDEs from data usually make assumptions on the specific forms of the underlying SDE and fit unknown parameters.For example, (De Vecchi et al., 2016) uses some polynomials to model f, σ, while (Pereira et al., 2010) assumes linear f and σ in Eq. (1).
Previous attempts at using neural SDEs (nSDEs) have explored different loss functions for reconstruction. For example, Tzen and Raginsky (2019) model the SDE as a continuum limit of latent deep Gaussian models and use a variational likelihood bound for training. Kidger et al. (2021) adopt Wasserstein generative adversarial networks (WGANs) that were proposed in Arjovsky et al. (2017) for reconstructing SDEs. Briol et al. (2019) uses a maximum mean discrepancy (MMD) loss and a generative model for training SDEs.Song et al. (2020) assumes that σ in Eq. (1) depends only on time and uses a score-based generative model for SDE reconstruction.The Wasserstein distance, denoted as W, has gained wide use in statistics and machine learning. Key papers have delved into its analysis (Rüschendorf, 1985) and its utilization in reconstructing discrete-time stochastic processes (Bartl et al., 2021). In the context of SDEs, Bion-Nadal and Talay (2019) introduced a restricted Wasserstein-type distance, while Sanz-Serna and Zygalakis (2021) and Wang (2016); Sanz-Serna and Zygalakis (2021) examined its application in ergodic SDEs, Lévy processes, and Langevin equations, respectively. Calculating the W distance for multidimensional random variables is challenging;hence, approximations such as the sliced W distance and regularized W distance have emerged (Cuturi et al., 2019; Kolouri et al., 2018, 2019; Rowland et al., 2019; Frogner et al., 2015).","We shall follow the definition of the squared W2-distance in Clement and Desch (2008) for two probability measures μm, β associated with two continuous stochastic processes X(t), Ψ(t), t ∈ [0, T].
Definition 1 For two d-dimensional continuous stochastic processes in the separable space
\(C([0, T]; R)\) generated by the solution X(t) to Eq. (1) and Ψ(t) to Eq. (2) respectively, and the probability distribution over the continuous function space C([0, T]; R) generated by the solution X, Ψ at time s, respectively.
Specifically, if X(ti) follows a one-dimensional SDE Eq. (1), then for uniformly spaced time points ti = iT/N, i = 0, ..., N, our proposed time-decoupled loss function is simply
\Delta t\sum\limits\limits^{N-1}_{i=1}Z\frac{1}{0} \Big(F\textsubscript{-1}^\textsubscript{i} (s) - \hat{F}\textsubscript{-1}^\textsubscript{i} (s)\Big)^2ds,
(7)
where \Delta t is the timestep and F\textsubscript{i} and \hat{F}\textsubscript{i} are the empirical cumulative distribution functions for X(ti) and \hat{X}(ti), respectively. This time-decoupled squared W2-distance loss function will be explicitly expressed in Eq. (18).","In this paper, we analyzed the squared W2 distance between two probability distributions associated with two SDEs and proposed a novel method for efficient reconstruction of SDEs from data by minimizing squared W2 distances as loss functions. Upon performing numerical experiments, we found that our proposed finite-time-point time-decoupled squared W2 distance loss function, Eq. (18), is superior than many other recently developed machine-learning and statistical approaches to SDE reconstruction.
A number of extensions are apparent. First, one can further investigate applying the squared W2 loss to the reconstruction of high-dimensional SDEs. Another important direction to develop are approaches to efficiently evaluate the squared W2 loss function Eq. (17) and analyze how well the time-decoupled squared W2 loss function Eq. (18) can approximate Eq. (17). Whether the Wasserstein distance can serve as upper bounds for the errors f - Φ and σ - Φ is also an intriguing question as its resolution will determine is minimizing the squared Wasserstein distance is sufficient for reconstructing SDEs. Finally, another promising area worthy of study is the extension of the squared W2 distance loss function to the reconstruction of general Lévy processes that include jumps in the trajectories.",Squared Wasserstein-2 Distance for Efficient Reconstruction of Stochastic Differential Equations,"Mingtao Xia, Xiangting Li, Qijing Shen, Tom Chou","Squared Wasserstein-2 Distance for Efficient Reconstruction
of Stochastic Differential Equations
Mingtao Xia∗
xiamingtao@nyu.edu
Courant Institute of Mathematical Sciences
New York University
New York, NY 10012, USA
Xiangting Li∗
xiangting.li@ucla.edu
Department of Computational Medicine
University of California Los Angeles
Los Angeles, CA 90095, USA
Qijing Shen
qijing.shen@ndm.oxford.edu
Nuffield Department of Medicine
University of Oxford
Oxford OX2 6HW, UK
Tom Chou
tomchou@ucla.edu
Department of Mathematics,
University of California Los Angeles
Los Angeles, CA 90095, USA
Abstract
We provide an analysis of the squared Wasserstein-2 (W2) distance between two probabil-
ity distributions associated with two stochastic differential equations (SDEs). Based on
this analysis, we propose the use of a squared W2 distance-based loss functions in the re-
construction of SDEs from noisy data. To demonstrate the practicality of our Wasserstein
distance-based loss functions, we performed numerical experiments that demonstrate the
efficiency of our method in reconstructing SDEs that arise across a number of applications.
Keywords:
Wasserstein distance, stochastic differential equation, inverse problem, un-
certainty quantification, optimal transport
1 Introduction
Stochastic processes are mathematical models of random phenomena that evolve over
time or space (Cinlar, 2011). Among stochastic processes, stochastic differential equations
(SDE) of the form
dX(t) = f(X(t), t)dt + σ(X(t), t)dB(t), t ∈ [0, T]
(1)
∗. equal contribution
©2024 Mingtao Xia and Xiangting Li and Qijing Shen and Tom Chou.
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.
arXiv:2401.11354v1  [math.PR]  21 Jan 2024
Xia and Li and Shen and Chou
are widely used across different fields to model complex systems with continuous variables
and noise. Here, f and σ denote deterministic and stochastic components of the SDE,
while B(t) represents Brownian motion. In applications such as computational fluid dy-
namics, cell biology, and genetics, the underlying dynamics are often unknown, partially
observed, and subjected to noise. Consequently, it is vital to develop methods capable of
reconstructing the governing SDEs from limited data (Sullivan, 2015; Soize, 2017; Mathe-
lin et al., 2005; Bressloff, 2014; Lin and Buchler, 2018). Traditional methods, such as the
Kalman filter (Welch et al., 1995; Welch, 2020) and Gaussian process regression (Liu et al.,
2020; MacKay et al., 1998) often assume specific forms of noise. These methods may not
be suitable for complex or nonlinear systems where noise affects the dynamics in a more
complex manner.
Recent advancements leverage machine learning, specifically neural ordinary differential
equations (NODEs) (Chen et al., 2018), to offer a more flexible approach to reconstructing
SDEs in the form of neural SDEs (nSDEs) (Tzen and Raginsky, 2019; Tong et al., 2022;
Jia and Benson, 2019). Despite the promise, challenges remain, particularly in selecting
optimal loss functions (Jia and Benson, 2019).
The Wasserstein distance, a family of
metrics that measures discrepancies between probability measures over a metric space,
has emerged as a potential solution due to its robust properties (Villani et al., 2009; Oh
et al., 2019; Zheng et al., 2020).
In this paper, we introduce bounds on the second-
order Wasserstein W2 distance between two probability distributions over the continuous
function space generated by solutions to two SDEs. Our results motivate the use of this
distance for SDE reconstruction. We test our approach on different examples to showcase
its effectiveness.
Traditional methods for reconstructing SDEs from data usually make assumptions on
the specific forms of the underlying SDE and fit unknown parameters.
For example,
(De Vecchi et al., 2016) uses some polynomials to model f, σ, while (Pereira et al., 2010)
assumes linear f and σ in Eq. (1).
Previous attempts at using neural SDEs (nSDEs) have explored different loss functions
for reconstruction. For example, Tzen and Raginsky (2019) model the SDE as a continuum
limit of latent deep Gaussian models and use a variational likelihood bound for training.
Kidger et al. (2021) adopt Wasserstein generative adversarial networks (WGANs) that
were proposed in Arjovsky et al. (2017) for reconstructing SDEs. Briol et al. (2019) uses
a maximum mean discrepancy (MMD) loss and a generative model for training SDEs.
Song et al. (2020) assumes that σ in Eq. (1) depends only on time and uses a score-based
generative model for SDE reconstruction.
The Wasserstein distance, denoted as W, has gained wide use in statistics and machine
learning. Key papers have delved into its analysis (R¨uschendorf, 1985) and its utilization
in reconstructing discrete-time stochastic processes (Bartl et al., 2021). In the context
of SDEs, Bion-Nadal and Talay (2019) introduced a restricted Wasserstein-type distance,
while Sanz-Serna and Zygalakis (2021) and Wang (2016); Sanz-Serna and Zygalakis (2021)
examined its application in ergodic SDEs, Levy processes, and Langevin equations, respec-
2
Squared Wasserstein Distance for SDE Reconstruction
tively. Calculating the W distance for multidimensional random variables is challenging;
hence, approximations such as the sliced W distance and regularized W distance have
emerged (Cuturi et al., 2019; Kolouri et al., 2018, 2019; Rowland et al., 2019; Frogner
et al., 2015).
The aforementioned WGAN approach in Kidger et al. (2021) uses the first-order Wasser-
stein distance to indirectly reconstruct SDEs via the Kantorovich-Rubinstein duality (Ar-
jovsky et al., 2017). To the best of our knowledge, there has been no published work that
directly applies and analyzes the W distance to the reconstruction of SDEs.
2 Definitions and Outline
We propose a squared W2-distance-based SDE reconstruction method and analyze it under
the following setting. Let µ denote the probability distribution over the continuous function
space C([0, T]; R) generated by the solution X(t) to Eq. (1). In the following approximation
to Eq. (1),
d ˆX(t) = ˆf(X(t), t)dt + ˆσ(X(t), t)d ˆB(t), t ∈ [0, T],
(2)
ˆB(t) is another independent standard Brownian motion and the probability distribution
over the continuous function space C([0, T]; R) generated by the solution ˆX(t) to Eq. (2)
will be denoted ˆµ.
We shall follow the definition of the squared W2-distance in Clement and Desch (2008)
for two probability measures µ, ˆµ associated with two continuous stochastic processes
X(t), ˆX(t), t ∈ [0, T].
Definition 1 For two d-dimensional continuous stochastic processes in the separable space
Xia and Li and Shen and Chou
Our main contributions can be summarized as follows
1. Using Definition 1, we first derive in Section 3 an upper bound for the squared Wasser-
stein distance W 2
2 (µ, ˆµ) between the probability measures associated with solutions to
two 1D SDEs in terms of the errors in the reconstructed drift and diffusion functions
f − ˆf and σ − ˆσ in Eqs. (1) and (2). To be specific, we establish a W2 distance upper
bound which depends explicitly on the difference in the drift and diffusion functions
f − ˆf and σ − ˆσ associated with using Eq. (2) to approximate Eq. (1).
2. In Section 4, we shall prove that the W2 distance between the two SDEs, W2(µ, ˆµ),
can be accurately approximated by estimating the W2 distance between their finite-
dimensional projections. We also develop a time-decoupled version of W 2
2 (µ, ˆµ) de-
fined by
˜W 2
2 (µ, ˆµ) :=
Z T
0
W 2
2 (µ(s), ˆµ(s))ds
(6)
which allows us to define a squared W2-distance-based loss function for reconstructing
SDEs. Here, µ(s), ˆµ(s) are the distributions on Rd generated by projection of the
stochastic processes X, ˆ
X at time s, respectively. Specifically, if X(ti) follows a one-
dimensional SDE Eq. (1), then for uniformly spaced time points ti = iT
N , i = 0, ..., N,
our proposed time-decoupled loss function is simply
∆t
N−1
X
i=1
Z 1
0
(F −1
i
(s) − ˆF −1
i
(s))2ds,
(7)
where ∆t is the timestep and Fi and ˆFi are the empirical cumulative distribution
functions for X(ti) and ˆX(ti), respectively. This time-decoupled squared W2-distance
loss function will be explicitly expressed in Eq. (18).
3. Finally, we carry out numerical experiments to show that our squared W2-distance-
based SDE reconstruction method performs better than recently developed machine-
learning-based methods across many SDE reconstruction problems. In Section 6, we
summarize our proposed squared W2 distance method for SDE reconstruction and
suggest some promising future directions.
Additional numerical experiments and
sensitivity analysis are detailed in the Appendix.
3 Squared W2 distance for reconstructing SDEs
In this section, we prove the bounds for the squared W2 distance of two probability measures
associated with two SDEs. Specifically, we demonstrate that minimizing the squared W2
distance is necessary for the reconstruction of f, σ in Eq. (1).
We shall first prove an upper bound for the W2 distance between the probability mea-
sures µ and ˆµ associated with X(t), ˆX(t), solutions to Eq. (1) and Eq. (2), respectively.
4
Squared Wasserstein Distance for SDE Reconstruction
Theorem 1 If {X(t)}T
t=0, { ˆX(t)}T
t=0 have the same initial condition distribution and they
are solutions to Eq. (1) and Eq. (2) in the univariate case (d = 1 in Eq. (3)), respectively,
and the following conditions hold:
• f, ˆf, σ, ˆσ are continuously differentiable; ∂xσ and ∂xˆσ are uniformly bounded
• there exists two functions η1(x1, x2), η2(x1, x2) such that their values are in (x1, x2)
and
f(X1, t) − f(X2, t) = ∂xf(η1(X1, X2), t)(X1 − X2)
σ(X1, t) − σ(X2, t) = ∂xσ(η2(X1, X2), t)(X1 − X2)
(8)
then,
W 2
2 (µ, ˆµ) ≤3
Z T
0
E
h Z t
0 H2(s, t)ds
i
dt × E
h Z T
0 (f − ˆf)2( ˜X(t), t)dt)
i
+ 3
Z T
0
E
h Z t
0 H2(s, t)ds
i
dt × E
h Z T
0
Xia and Li and Shen and Chou
4 Finite-dimensional and time-decoupled squared W2 loss functions
From Theorem 1 in Section 3, in order to have small errors in the drift and diffusion terms
f − ˆf and σ − ˆσ, a small W2(µ, ˆµ) is necessary. However, W2(µ, ˆµ) cannot be directly used
as a loss function to minimize since we cannot directly evaluate the integration in time
in Eq. (4). In this section, we shall provide a way to estimate the W2(µ, ˆµ) distance by
using finite dimensional projections, leading to squared W2-distance-based loss functions
for minimization.
Consider the two general d-dimensional SDEs defined in Eq. (41). Usually, we only have
finite observations of trajectories for {X(t)} and { ˆ
X(t)} at discrete time points. Thus,
we provide an estimate of the W2 between of the probability measures µ, ˆµ associated
with X(t) and ˆ
X(t), t ∈ [0, T] using their finite-dimensional projections. We assume that
X(t), ˆ
X(t) solve the two SDEs described by Eq. (41). We let 0 = t0 < t1 < ... < tN =
T, ti = i∆t, ∆t :=
T
N be a uniform mesh in time and we define the following projection
operator IN
XN(t) := INX(t) =
(
X(ti), t ∈ [ti, ti+1), i < N − 1,
X(ti), t ∈ [ti, ti+1], i = N − 1.
(13)
As in the previous case, we require X(t) and ˆX(t) to be continuous.
Note that the
projected process is no longer continuous. Thus, we define a new space ˜ΩN containing
all continuous and piecewise constant functions; naturally, µ, ˆµ are allowed to be defined
on ˜ΩN.
Distributions over ˜ΩN generated by XN(t), ˆXN(t) in Eq.
(13) is denoted by
µN and ˆµN, respectively. We will prove the following theorem for estimating W2(µ, ˆµ) by
W2 (µN, ˆµN).
Theorem 2 Suppose {X(t)}T
t=0 and { ˆ
X(t)}T
t=0 are both continuous-time continuous-space
stochastic processes in Rd and µ, ˆµ are their associated probability measures, then W2(µ, ˆµ)
can be bounded by their finite-dimensional projections
W2(µN, ˆµN)−W2(µ, µN)−W2(ˆµ, ˆµN) ≤ W2(µ, ˆµ) ≤ W2(µN, ˆµN)+W2(µ, µN)+W2(ˆµ, ˆµN)
(14)
where µN, ˆµN are the probability distributions associated with XN and ˆ
XN defined in
Eq. (13). Specifically, if X(t) and
ˆ
X(t) solve Eq. (41), and if
F := E
h Z T
0
d
X
i=1
f2
i (X(t), t)dt
i
< ∞, Σ := E
h Z T
0
d
X
ℓ=1
s
X
j=1
σ2
i,j(X(t), t)dt
i
< ∞,
ˆF := E
h Z T
0
d
X
i=1
ˆf2
i ( ˆ
X(t), t)dt
i
< ∞, ˆΣ := E
h Z T
0
d
X
ℓ=1
s
X
j=1
ˆσ2
i,j( ˆ
X(t), t)dt
i
< ∞,
(15)
6
Squared Wasserstein Distance for SDE Reconstruction
then we obtain the following bound
W2(µN, ˆµN)−
p
(s + 1)∆t
√
F∆t + Σ +
q
ˆF∆t + ˆΣ

≤ W2(µ, ˆµ)
≤ W2(µN, ˆµN) +
p
(s + 1)∆t
√
F∆t + Σ +
q
ˆF∆t + ˆΣ

.
(16)
The proof to Theorem 2 relies on the triangular inquality of the Wasserstein distance
and the Itˆo isometry; it is provided in Appendix C. Theorem 2 gives bounds for approxi-
mating the W2 distance between X(t), ˆ
X(t) w.r.t. to their finite dimensional projections
XN(t), ˆ
XN(t). Specifically, if X(t), ˆ
X(t) are solutions to Eq. (1) and Eq. (2), then as
the timestep ∆t → 0, W2(µN, ˆµN) → W2(µ, ˆµ). Theorem 2 indicates that we can use
W 2
2 (µN, ˆµN), which approximates W 2
2 (µ, ˆµ) when ∆t → 0, as a loss function. Further-
more,
W 2
2 (µN, ˆµN) = infπ(µN,ˆµN)
N−1
X
i=1
E(XN, ˆ
XN)∼π(µN,ˆµN)
hX(ti) − ˆ
X(ti)
2
2
i
∆t.
(17)
Here, π(µN, ˆµN) iterates over coupled distributions of XN(t), ˆ
XN(t), whose marginal dis-
tributions coincide with µN and ˆµN. | · |2 denotes the ℓ2 norm of a vector. Note that µN
is fully characterized by values of X(t) at the discrete time points ti.
For a d-dimensional SDE, the trajectories at discrete time points {X(ti)}N−1
i=1
is d ×
(N − 1) dimensional. In Fournier and Guillin (2015), the error bound for |W 2
2 (µN, ˆµN) −
W 2
2 (µe
N, ˆµe
N)|, where µe
N, ˆµe
N are the finite-sample empirical distributions of {X(ti)}N−1
i=1
and { ˆ
X(ti)}N−1
i=1 , will increase as the dimensionality d × (N − 1) becomes large. Alterna-
tively, we can disregard the temporal correlations of values at different times and relax the
constraint on the coupling π(µN, ˆµN) in to minimize the Wasserstein distance between the
marginal distribution of {X(ti)} and the marginal distribution of { ˆ
X(ti)}, as was done in
Chewi et al. (2021). To be more specific, we minimize individual terms in the sum with
respect to the coupling πi of X(ti) and ˆ
X(ti) and define a heuristic loss function
N−1
X
i=1
inf
πi Eπi
hX(ti) − ˆ
X(ti)
2
2
i
∆t =
N−1
X
i=1
W 2
2 (µN(ti), ˆµN(ti))∆t
(18)
where µN(t) and ˆµN(t) are the probability distributions of X(t) and X(t), respectively.
Note that
N−1
X
i=1
inf
πi Eπi
X(ti) − ˆ
X(ti)
2
2

∆t ≤ W 2
2 (µN, ˆµN)
(19)
because the marginal distributions of π(µN, ˆµN) coincide with µN and
ˆ
µN.
Since the
marginal distributions of µN and ˆµN at ti are µN(ti) and ˆµN(ti), respectively, we have
7
Xia and Li and Shen and Chou
N−1
X
i=1
inf
πi Eπi
hX(ti) − ˆ
X(ti)
2
2
i
∆t
≤ infπ(µN,ˆµN)
N−1
X
i=1
E(XN, ˆ
XN)∼π(µN,ˆµN)
hX(ti) − ˆ
X(ti)
2
2
i
∆t.
(20)
The dimensionality of X(ti) and ˆ
X(ti) is d, which is much smaller than (N−1)d for large
N. We denote µe
N(ti) and ˆµe
N(ti) to be the finite-sample empirical distributions of X(ti)
and ˆ
X(ti), respectively. Since the error of estimating the W2 distance using empirical distri-
butions of a random variable increases with the random variable’s dimensionality Fournier
and Guillin (2015), the error
 PN−1
i=1 W 2
2 (µN(ti), ˆµN(ti)) − PN−1
i=1 W 2
2 (µe
N(ti), ˆµe
N(ti))
 can
be smaller than the error
W 2
2 (µN, ˆµN) − W 2
2 (µe
N, ˆµe
N)
. Compared to Eq. (17), the time-
decoupled squared W2 distance Eq. (18) can be better approximated using finite-sample
empirical distributions.
Note that
N−1
X
i=1
W 2
2 (µN(ti), ˆµN(ti))∆t ≤ W 2
2 (µN, ˆµN).
(21)
Thus, from Theorem 1 and Theorem 2, minimizing Eq. (18) when N → ∞ is also necessary
to achieve small f − ˆf and σ − ˆσ when the SDE is univariate. Let µi, ˆµi be the two proba-
bility distributions on the space of continuous functions associated with X(t), t ∈ [ti, ti+1)
and ˆ
X(t), t ∈ [ti, ti+1), respectively. We can then show that Eq. (18) is an approxima-
tion to the partially time-decoupled summation of squared W2 distances PN−1
i=1 W 2
2 (µi, ˆµi)
as N → ∞.
Additionally, we can prove the following theorem that indicates Eq. (18)
approximates a time-decoupled squared Wasserstein distance in the N → ∞ limit.
Theorem 3 We assume the conditions in Theorem 2 hold and for any 0 < t < t′ < T, as
t′ − t → 0, the following conditions are satisfied
E
h Z t′
t
d
X
i=1
f2
i (X(t), t)dt
i
, E
h Z t′
t
d
X
i=1
ˆf2
i ( ˆ
X(t), t)dt
i
→ 0,
E
h Z t′
t
d
X
i=1
s
X
j=1
σ2
i,j(X(t), t)dt
i
, E
h Z t′
t
d
X
i=1
s
X
j=1
ˆσ2
i,j( ˆ
X(t), t)dt
i
→ 0.
(22)
Then,
lim
N→∞
 N−1
X
i=1
inf
πi Eπi
hX(ti) − ˆ
X(ti)
2
2
i
∆t −
N−1
X
i=1
W2(µi, ˆµi)

= 0.
(23)
Furthermore, the limit
8
Squared Wasserstein Distance for SDE Reconstruction
lim
N→∞
N−1
X
i=1
inf
πi Eπi
hX(ti) − ˆ
X(ti)
2
2
i
∆t = lim
N→∞
N−1
X
i=1
W 2
2

µ(ti), ˆµ(ti)

∆t
(24)
exists.
The proof of Theorem 3 will use the result of Theorem 2 and is given in Appendix D.
Specifically, for each N,
N−1
X
i=1
inf
πi Eπi
hX(ti) − ˆ
X(ti)
2
2
i
∆t ≤ W 2
2 (µN, ˆµN),
(25)
so we conclude that
lim
N→∞
N−1
X
i=1
inf
πi Eπi
hX(ti) − ˆ
X(ti)
2
2
i
∆t ≤ lim
N→∞ W 2
2 (µN, ˆµN) = W 2
2 (µ, ˆµ).
(26)
We denote
˜W 2
2 (µ, ˆµ) :=
Z T
0
W 2
2
Xia and Li and Shen and Chou
ˆf ≈ f, ˆσ ≈ σ. Θ1, Θ2 are the parameter sets in the two neural networks for parameterizing
ˆf = ˆfΘ1, ˆσ = ˆσΘ2. We use the sdeint function in the torchsde Python package in Li et al.
(2020) to numerically integrate SDEs. Details of the training hyperparameter setting for
all examples are given in Appendix F. Our code will be made publicly available on Github
upon acceptance of this manuscript.
First, we compare our proposed squared W2-distance-based loss (Eq. (18)) with several
traditional statistical methods for SDE reconstruction.
Example 1 We reconstruct a nonlinear SDE of the form
dX(t) =
Squared Wasserstein Distance for SDE Reconstruction
Figure 1: (a) Ground-truth trajectories. (b) Reconstructed trajectories from nSDE using
MSE loss.
(c) Reconstructed trajectories from nSDE using mean2+variance
loss. (d) Reconstructed trajectories from nSDE using the finite-time-point time-
decoupled W2 loss. (e) Reconstructed trajectories from nSDE using a max-log-
likelihood loss yields the worst approximation.
between the reconstructed ˆf, ˆσ in Eq. (2) and the ground-truth f and σ in Eq. (1). Here,
xj(ti) is the value of the jth ground-truth trajectory at ti.
Example 2 Next, we reconstruct a Cox-Ingersoll-Ross (CIR) model which is a popular
finance model that describes the evolution of interest rates:
dX(t) =
Xia and Li and Shen and Chou
Figure 2: (a) Ground-truth trajectories and reconstructed trajectories by nSDE using the
finite-time-point time-decoupled squared W2 loss with σ0 = 0.5. (b-c) Errors
with respect to the numbers of ground-truth trajectories for σ0 = 0.5.
(d)
Comparison of the reconstructed ˆfΘ1(u), ˆσΘ2(u) to the ground-truth functions
f(u), σ(u) for σ0 = 0.5.
(e-f) Errors with respect to noise level σ0 with 200
training samples. Legends for panels (c, e, f) are the same as the one in (b).
f, σ as defined in Eq. (30). More specifically, we plot the reconstructed ˆfΘ, ˆσΘ by using our
squared W2 loss in Fig. 2(d); these reconstructions also match well with the ground-truth
values f, σ. When we vary σ0 in Eq. (31), our proposed finite-time-point time-decoupled
W2 loss function gives the best performance among all loss functions shown in Fig. 2(e, f).
In Appendix G, instead of using the same initial condition for all trajectories, we sample
the initial condition from different distributions and find that the reconstruction errors
f − ˆf and σ − ˆσ is not sensitive to different initial conditions, implying the robustness
of using our proposed finite-time-point time-decoupled W2 loss function with respect to
different initial conditions. Also, in Appendix H, we change the number of layers and the
number of neurons in each layers for the two neural networks we utilize to parameterize
ˆf := ˆf(X, t; Θ1), ˆσ := ˆσ(X, t; Θ2). We find that wider neural networks can lead to smaller
errors f − ˆf and σ − ˆσ.
12
Squared Wasserstein Distance for SDE Reconstruction
Next, we reconstruct the Ornstein-Uhlenbeck (OU) process given in Kidger et al. (2021)
and in doing so, compare our loss function with the WGAN-SDE method therein and with
another recent MMD method.
Example 3 Consider reconstructing the following time-inhomogeneous OU process
dX(t) =
Xia and Li and Shen and Chou
reconstructed f, σ for all numbers of ground-truth trajectories. Using Eq. (18) as the loss
function achieves better accuracy in a shorter computational time than using Eq. (17).
For Nsample training samples and N total number of timesteps, the memory cost in
using Eq. (18) is O(N × Nsample); however, the number of operations needed is O(N ×
Nsample log Nsample) because we need to reorder the ground-truth X(ti) and predicted ˆX(ti)
data to obtain the empirical cumulative distributions at every ti. The memory cost and
operations needed in using Eq. (17) are both O((N × Nsample)2) because a (N × Nsample) ×
(N ×Nsample) cost matrix must be evaluated. On the other hand, the MMD method needs
to create an Nsample×Nsample matrix for each timestep and thus the corresponding memory
cost and operations needed are at best O(N ×N2
sample). The WGAN-SDE method needs to
create a generator and a discriminator and its training is complex, leading to both a higher
memory cost and a larger runtime than our method. For reconstructing SDEs, a larger
number of ground-truth trajectories leads to higher accuracy (see Appendix I). Overall,
our time-decoupled squared W2 loss, Eq. (18), performs the best in terms of accuracy and
efficiency when reconstructing the 1D SDE Eq. (32).
If we consider using stochastic gradient descent (SDG) to minibatch for training, we
find that the batch size cannot be set too small, especially when we are using the MMD or
Eq. (17) as loss functions, due to the intrinsic noisy nature of trajectories of SDEs. Thus,
using our squared W2 distance loss function given in Eq. (18) can be more efficient overall
than using the MMD or Eq. (17) as the loss function. Additional results using the SGD
with minibatch for training are given in Appendix I.
Finally, we carry out an experiment on reconstructing a 2D correlated geometric Brown-
ian motion. In this 2D reconstruction problem, we will compare the loss functions, Eq. (17)
and Eq. (18), the MMD method, and a sliced squared Wasserstein distance method (Kolouri
et al., 2018).
Example 4 Consider reconstructing the following 2D correlated geometric Brownian mo-
tion that can represent, e.g., values of two correlated stocks (Musiela and Rutkowski, 2006)
dX1(t) =µ1X1(t)dt +
2
X
i=1
σ1,iXi(t)dBi(t),
dX2(t) =µ2X2(t)dt +
2
X
i=1
σ2,iXi(t)dBi(t)
(33)
Here, t ∈ [0, 2], B1(t) and B2(t) are independent Brownian processes, f := (µ1X1, µ2X2)
is a 2D vector, and σ := [σ1,1X1, σ1,2X2; σ2,1X1, σ2,2X2] is a 2 × 2 matrix.
We use
(µ1, µ2) = (0.1, 0.2), σ = [0.2X1, −0.1X2; −0.1X1, 0.1X2], and set the initial condition
(X1(0), X2(0)) = (1, 0.5). In addition to directly minimizing a 2D decorrelated version of
the squared W2 distance Eq. (18) (denoted as W2 in Fig. 4(c)), we consider minimizing
14
Squared Wasserstein Distance for SDE Reconstruction
Figure 4: (a) Black dots and red squares are the ground-truth (X1(2), X2(2)) and the re-
constructed ( ˆX1(2), ˆX2(2)) found using the rotated squared W2 loss function,
respectively. Black and red arrows indicate, respectively, the vectors f(X1, X2)
and ˆf(X1, X2). (b) Relative errors of the reconstructed f and σ. Error bars
indicate the standard deviation across ten reconstructions. (c) Runtime of dif-
ferent loss functions with respect to Nsamples. (d) The decrease of different loss
functions with respect to training epochs. The legend for the panel (d) is the
same as the one in (c).
a sliced squared W2 distance as proposed by Kolouri et al. (2018, 2019). Finally, we nu-
merically estimate the W2 distance Eq. (17) as well as the time-decoupled approximation
Eq. (18) using the ot.emd2 function in the Python Optimal Transport package. Formulae
of the above loss functions are given in Appendix E. We keep the neural network hyperpa-
rameters the same while minimizing all loss functions. Note that since the SDE has two
components, the definition of the relative error in σ is revised to
"" T
X
i=0
PN
j=1 ∥σσT (xj(ti), ti) − ˆσˆσT (xj(ti), ti)∥2
F
(T + 1) PN
j=1 ∥ˆσˆσT (xj(ti), ti)∥2
F
#1/2
,
(34)
15
Xia and Li and Shen and Chou
where ∥ · ∥F is the Frobenius norm for matrices.
Fig. 4(a) shows the ground truth and reconstructed coordinates (X1, X2) (black dots)
and ( ˆX1, ˆX2) (red squares) at time t = 2, along with f(X1, X2) (black) and ˆf(X1, X2)
(red). For reconstructing f and σ in problem, numerically evaluating Eq. (18) (blue curve)
performs better than the MMD method, the loss in Eq. (17), the sliced W2 distance loss, and
the 2D decorrelated squared W2 loss, as shown in Fig. 4(b). Using the sliced W2 distance
yields the poorest performance and least accurate ˆf and ˆσ. Using the 2D decorrelated
squared W2 loss function also gives inaccurate ˆσ. Thus, the sliced W2 distance and the 2D
decorrelated squared W2 loss are not good candidates for reconstructing multivariate SDEs.
Numerically estimating Eq. (17) yields poorer performance than numerically estimating
Eq. (18) because numerically evaluating the W2 distance for higher-dimensional empirical
distributions is generally less accurate.
From Fig. 4(c), we see that the runtime and memory needed to numerically evaluate
the time-decoupled Eq. (18) using ot.emd2 is smaller than those needed for the MMD
method, but larger than those needed to numerically estimate Eq. (17). Yet, as shown
in Fig. 4(d), minimizing Eq. (18) leads to the fastest convergence, potentially requiring
fewer epochs when using Eq. (18) as the loss function. An additional comparison of using
the two loss functions, the finite-time-point squared W2 distance Eq. (17) and the finite-
time-point time-decoupled squared W2 distance Eq. (18) is given in Appendix J. Further
analysis on how the number of samples and the dimensionality of an SDE dimensionality
affects W2-based distances in reconstructing multivariate SDEs will be informative.
6 Summary and Conclusions
In this paper, we analyzed the squared W2 distance between two probability distributions
associated with two SDEs and proposed a novel method for efficient reconstruction of
SDEs from data by minimizing squared W2 distances as loss functions. Upon perform-
ing numerical experiments, we found that our proposed finite-time-point time-decoupled
squared W2 distance loss function, Eq. (18), is superior than many other recently developed
machine-learning and statistical approaches to SDE reconstruction.
A number of extensions are apparent. First, one can further investigate applying the
squared W2 loss to the reconstruction of high-dimensional SDEs. Another important direc-
tion to develop are approaches to efficiently evaluate the squared W2 loss function Eq. (17)
and analyze how well the time-decoupled squared W2 loss function Eq. (18) can approxi-
mate Eq. (17). Whether the Wasserstein distance can serve as upper bounds for the errors
f − ˆf and σ − ˆσ is also an intriguing question as its resolution will determine is minimizing
the squared Wasserstein distance is sufficient for reconstructing SDEs. Finally, another
promising area worthy of study is the extension of the squared W2 distance loss function
to the reconstruction of general L´evy processes that include jumps in the trajectories.
16
Squared Wasserstein Distance for SDE Reconstruction
Acknowledgments and Disclosure of Funding
The authors acknowledge Dr. Hedi Xia for his constructive comments on this manuscript.
TC acknowledges support from the Army Research Office through grant W911NF-18-1-
0345.
References
Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial
networks. In International Conference on Machine Learning, pages 214–223. PMLR,
2017.
Daniel Bartl, Mathias Beiglb¨ock, and Gudmund Pammer.
The Wasserstein space of
stochastic processes. arXiv preprint arXiv:2104.14245, 2021.
Jocelyne Bion-Nadal and Denis Talay. On a Wasserstein-type distance between solutions
to stochastic differential equations. The Annals of Applied Probability, 29(3):1609–1639,
2019. doi: 10.1214/18-aap1423.
Paul C Bressloff. Stochastic Processes in Cell Biology, volume 41. Springer, 2014.
Francois-Xavier Briol, Alessandro Barp, Andrew B Duncan, and Mark Girolami. Statis-
tical inference for generative models with maximum mean discrepancy. arXiv preprint
arXiv:1906.05944, 2019.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud.
Neural
ordinary differential equations. In Advances in Neural Information Processing Systems,
volume 31, 2018.
Sinho Chewi, Julien Clancy, Thibaut Le Gouic, Philippe Rigollet, George Stepaniants, and
Austin Stromme. Fast and smooth interpolation on Wasserstein space. In International
Conference on Artificial Intelligence and Statistics, pages 3061–3069. PMLR, 2021.
Erhan Cinlar. Probability and Stochastics. Springer Science & Business Media, 2011.
Philippe Clement and Wolfgang Desch. An elementary proof of the triangle inequality
for the Wasserstein metric. Proceedings of the American Mathematical Society, 136(1):
333–339, 2008.
Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranks and sorting
using optimal transport. In Proceedings of the 33rd International Conference on Neural
Information Processing Systems, pages 6861–6871, 2019.
17
Xia and Li and Shen and Chou
Francesco C De Vecchi, Paola Morando, and Stefania Ugolini. Reduction and reconstruc-
tion of stochastic differential equations via symmetries. Journal of Mathematical Physics,
57(12), 2016.
R´emi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z Alaya, Aur´elie Boisbunon,
Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier,
et al. Pot: Python optimal transport. The Journal of Machine Learning Research, 22
(1):3571–3578, 2021.
Nicolas Fournier and Arnaud Guillin. On the rate of convergence in Wasserstein distance
of the empirical measure. Probability Theory and Related Fields, 162(3-4):707–738, 2015.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Pog-
gio. Learning with a Wasserstein loss. In Advances in Neural Information Pprocessing
Systems, volume 28, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for
image recognition. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.
Junteng Jia and Austin R Benson. Neural jump stochastic differential equations. Advances
in Neural Information Processing Systems, 32, 2019.
Patrick Kidger, James Foster, Xuechen Li, and Terry J Lyons. Neural SDEs as infinite-
dimensional GANs. In International Conference on Machine Learning, pages 5453–5463.
PMLR, 2021.
Soheil Kolouri, Gustavo K Rohde, and Heiko Hoffmann. Sliced Wasserstein distance for
learning Gaussian mixture models. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 3427–3436, 2018.
Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Gen-
eralized sliced Wasserstein distances.
In Advances in Neural Information Processing
Systems, volume 32, 2019.
Xuechen Li, Ting-Kam Leonard Wong, Ricky TQ Chen, and David Duvenaud. Scalable
gradients for stochastic differential equations. In International Conference on Artificial
Intelligence and Statistics, pages 3870–3882. PMLR, 2020.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In
International Conference on Machine Learning, pages 1718–1727. PMLR, 2015.
Yen Ting Lin and Nicolas E Buchler. Efficient analysis of stochastic gene dynamics in the
non-adiabatic regime using piecewise deterministic Markov processes. Journal of The
Royal Society Interface, 15(138):20170804, 2018.
18
Squared Wasserstein Distance for SDE Reconstruction
Haitao Liu, Yew-Soon Ong, Xiaobo Shen, and Jianfei Cai. When Gaussian process meets
big data: a review of scalable GPs. IEEE Transactions on Neural Networks and Learning
Systems, 31(11):4405–4423, 2020.
David JC MacKay et al. Introduction to Gaussian processes. NATO ASI Series F Computer
and Systems Sciences, 168:133–166, 1998.
Lionel Mathelin, M Yousuff Hussaini, and Thomas A Zang.
Stochastic approaches to
uncertainty quantification in CFD simulations. Numerical Algorithms, 38:209–236, 2005.
Marek Musiela and Marek Rutkowski. Martingale Methods in Financial Modelling, vol-
ume 36. Springer Science & Business Media, 2006.
Jung Hun Oh, Maryam Pouryahya, Aditi Iyer, Aditya P Apte, Allen Tannenbaum, and
Joseph O Deasy. Kernel Wasserstein distance. arXiv preprint arXiv:1905.09314, 2019.
Jos´e Pereira, Morteza Ibrahimi, and Andrea Montanari. Learning networks of stochastic
differential equations. In Advances in Neural Information Processing Systems, volume 23,
2010.
Mark Rowland, Jiri Hron, Yunhao Tang, Krzysztof Choromanski, Tamas Sarlos, and
Adrian Weller. Orthogonal estimation of Wasserstein distances. In The 22nd Interna-
tional Conference on Artificial Intelligence and Statistics, pages 186–195. PMLR, 2019.
Ludger R¨uschendorf. The Wasserstein distance and approximation theorems. Probability
Theory and Related Fields, 70(1):117–129, 1985.
Jesus Maria Sanz-Serna and Konstantinos C Zygalakis. Wasserstein distance estimates for
the distributions of numerical approximations to ergodic stochastic differential equations.
The Journal of Machine Learning Research, 22(1):11006–11042, 2021.
Christian Soize. Uncertainty Quantification. Springer, 2017.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon,
and Ben Poole. Score-based generative modeling through stochastic differential equa-
tions. In International Conference on Learning Representations, 2020.
Timothy John Sullivan. Introduction to Uncertainty Quantification, volume 63. Springer,
2015.
Anh Tong, Thanh Nguyen-Tang, Toan Tran, and Jaesik Choi. Learning fractional white
noises in neural stochastic differential equations. In Advances in Neural Information
Processing Systems, volume 35, pages 37660–37675, 2022.
Belinda Tzen and Maxim Raginsky. Neural stochastic differential equations: deep latent
Gaussian models in the diffusion limit. arXiv preprint arXiv:1905.09883, 2019.
19
Xia and Li and Shen and Chou
C´edric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.
Jian Wang. Lp-Wasserstein distance for stochastic differential equations driven by L´evy
processes. Bernoulli, pages 1598–1616, 2016.
Greg Welch, Gary Bishop, et al. An introduction to the Kalman filter. 1995.
Gregory F Welch. Kalman filter. Computer Vision: A Reference Guide, pages 1–3, 2020.
Wenbo Zheng, Fei-Yue Wang, and Chao Gou. Nonparametric different-feature selection
using Wasserstein distance. In 2020 IEEE 32nd International Conference on Tools with
Artificial Intelligence (ICTAI), pages 982–988. IEEE, 2020.
20
Squared Wasserstein Distance for SDE Reconstruction
Appendix A. Proof to Theorem 1
Here, we shall provide a proof to Theorem 1. First, note that ˜X(t) defined in Eq. (10)
is a specific realization of ˆX(t) defined in Eq. (2), coupled to X(t) in the sense that its
initial values are X(0) almost surely and the Itˆo integral is defined with respect to the
same standard Brownian motion B(t). Therefore, by definition, if we let π in Eq. (4) to
be the joint distribution of (X, ˜X), then
W2(µ, ˆµ) ≤

E
h Z T
0 | ˜X(t) − X(t)|2dt
i1/2
.
(35)
Next, we provide a bound for E
 R T
0 | ˜X(t) − X(t)|2dt
 1
2 by the mean value theorem for f
and g.
d
Xia and Li and Shen and Chou
E
Squared Wasserstein Distance for SDE Reconstruction
for every i = 1, ...d, the ith component dXi(t) = fi(Xi(t), t)dt + σi(Xi(t), t)dBi(t) and
d ˆXi(t) = ˆfi( ˆXi(t), t)dt + ˆσi( ˆXi(t), t)d ˆBi(t), where Bi(t), ˆBi(t) are independent Brownian
motions, then similar conclusions can be derived by calculating the difference Xi − ˆXi.
Developing an upper bound of the W2 distance for general dimensions d requires additional
assumptions to find expressions for X − ˆ
X. We leave this nontrivial derivation as future
work. Although without a formal theoretical analysis, we shall show in Example 4 that
applying the squared W2 distance as the loss function is also effective in reconstructing
multidimensional SDEs.
Appendix B. Single-trajectory MSE and KL divergence
We shall first show that using the single-trajectory MSE tends to fit the mean process
E[X(t)] and make noise diminish, which indicates that the MSE is not a good loss function
when one wishes to fit σ in Eq. (1).
For two independent d-dimensional stochastic processes {X(t)}T
t=0, { ˆ
X(t)}T
t=0 as solu-
tions to Eq. (41) with appropriate f, ˆf and σ, ˆσ, let E[X] represent the trajectory of mean
values of X(t), i.e., E[X] = E[X(t)]. We have
E

∥X − ˆX∥2
=E

∥X − E[X]∥2 
+ E

∥ ˆX − E[X]∥2
− 2E
h Z T
0

X − E[X], ˆX − E[X]

dt
i
,
(42)
where ∥X∥2 :=
R T
0 |X|2
2 dt, | · |2 denotes the ℓ2 norm of a vector, and (·, ·) is the inner
product of two d-dimensional vectors. In view of the independence between X −E[X] and
ˆX − E[X], we have E
Xia and Li and Shen and Chou
Appendix C. Proof to Theorem 2
Here, we shall prove Theorem 2. We denote
ΩN := {Y (t)|Y (t) = Y (ti) t ∈ [ti, ti+1), i < N − 1; Y (t) = Y (ti), t ∈ [ti, ti+1]}
(44)
to be the space of piecewise functions. We also define the space
˜ΩN := {Y1(t) + Y2(t), Y1 ∈ C([0, T]; Rd), Y2 ∈ ΩN}.
(45)
˜ΩN is also a separable metric space because both
Squared Wasserstein Distance for SDE Reconstruction
N
X
i=1
Z ti
ti−1
E
Xia and Li and Shen and Chou
Fi := E
h Z ti+1
ti
d
X
ℓ=1
f2
ℓ (X(t), t)dt
i
< ∞, Σi := E
h Z ti+1
ti
d
X
ℓ=1
s
X
j=1
σ2
ℓ,j(X(t), t)dt
i
< ∞,
ˆFi := E
h Z ti+1
ti
d
X
ℓ=1
ˆf2
ℓ ( ˆ
X(t), t)dt
i
< ∞, ˆΣi := E
h Z ti+1
ti
d
X
ℓ=1
s
X
j=1
ˆσ2
ℓ,j( ˆ
X(t), t)dt
i
< ∞,
(55)
which results from
N−1
X
i=0
Fi = F < ∞,
N−1
X
i=0
ˆFi = ˆF < ∞,
N−1
X
i=0
Σi = Σ < ∞,
N−1
X
i=0
ˆΣi = ˆΣ < ∞,
(56)
where F, ˆF, Σ, ˆΣ are defined in Eq. (15). Squaring the inequality (54), we have
W 2
2 (µi, ˆµi) ≤ inf
πi Eπi
X(ti) − ˆ
X(ti)
2
2

∆t
+ 2 inf
πi
q
Eπi
X(ti) − ˆ
X(ti)
2
2
p
(s + 1)∆t
p
Fi∆t + Σi +
q
ˆF∆t + ˆΣi

+ 2(s + 1)∆t
Squared Wasserstein Distance for SDE Reconstruction
N−1
X
i=1
W 2
2 (µi, ˆµi) ≤
N−1
X
i=1
inf
πi Eπi
hX(ti) − ˆ
X(ti)
2
2
i
∆t
+ 2M∆t
√
s + 1
N−1
X
i=1
p
Fi∆t + Σi +
q
ˆFi∆t + ˆΣi

+ 2∆t(s + 1)
Xia and Li and Shen and Chou
as δt → 0.
First, suppose in the interval (t1
i , t1
i+1), we have t1
i = t3
ℓ < tℓ+1 < ... < t3
ℓ+s = t1
i+1, s ≥ 1,
then for s > 1, since t1
i+1 − t1
i = Pℓ+s−1
k=ℓ
(t3
k+1 − t3
k), we have
W 2
2
Squared Wasserstein Distance for SDE Reconstruction
as δt → 0. Similarly,

N2−1
X
i=0
W 2
2
Xia and Li and Shen and Chou
, where ∆t is the time step and W2 is the Wasserstein-2 distance between two empir-
ical distributions µe
N(ti), ˆµe
N(ti). These distributions are calculated by the samples
of the trajectories of X(t), ˆX(t) at a given time step t = ti, respectively.
3. Mean squared error (MSE) between the trajectories, where M is the total number of
the ground-truth and prediction trajectories. Xi,j and ˆXi,j are the values of the jth
ground-truth and prediction trajectories at time ti, respectively:
MSE(X, b
X) =
N
X
i=1
M
X
j=1
(Xi,j − ˆXi,j)2∆t.
4. The sum of squared distance between mean trajectories and absolute distance be-
tween trajectories, which is a common practice for estimating the parameters of an
SDE. Here M and Xi,j and ˆXi,j have the same meaning as in the MSE definition.
var(Xi) and var( ˆXi) are the variances of the empirical distributions of X(ti), ˆX(ti),
respectively. We shall denote this loss function by
(mean2 + var)(X, ˆX) =
N
X
i=1


 1
n
M
X
j=1
Xi,j − 1
n
N
X
i=1
ˆXi,j
2
+
var(Xi) − var( ˆXi)


 ∆t.
5. Negative approximate log-likelihood of the trajectories:
− log L(X|σ) = −
N−1
X
i=0
M
X
j=1
log ρN
hXi+1,j − Xt,j + f(Xi,j, ti)∆t
σ2(Xi,j, ti)∆t
i
,
where ρN is the probability density function of the standard normal distribution and
f(Xi,j, ti), σ(Xi,j, ti) are the ground-truth drift and diffusion functions in Eq. (1). M
and Xi,j and ˆXi,j have the same meaning as in the MSE definition.
6. MMD (maximum mean discrepancy) (Li et al., 2015):
MMD(X, ˆX) =
N
X
i=1
Squared Wasserstein Distance for SDE Reconstruction
where µN,1(ti) and ˆµN,1(ti) are the empirical distributions of X1, ˆX1 at time ti, re-
spectively. Also, µN,2(ti) and ˆµN,2(ti) are the empirical distributions of X2, ˆX2 at
time ti, respectively.
2. Weighted sliced squared W2 loss
N−1
X
i=1
 m
X
k=1
Nk
Pm
ℓ=1 Nℓ
W 2
2
Xia and Li and Shen and Chou
has entries (Ci)sj = |Xs(ti) − ˆ
Xj(ti)|2
2 for i = 1, ..., N − 1. Xs(ti) is the vector of
the values of the sth ground-truth trajectory at the time point ti, and ˆ
Xj(ti) is the
vector of the values of the jth predicted trajectory at the time point ti.
5. MMD (maximum mean discrepancy) (Li et al., 2015):
MMD(X, ˆ
X) =
N
X
i=1
Squared Wasserstein Distance for SDE Reconstruction
We then train the model by minimizing Eq. (18) to reconstruct Eq. (31) with the same
hyperparameters as in Example 2. The results are shown in Table 2, which indicate our
proposed squared W2 loss function is rather insensitive to the “noise”, i.e., the variance in
the distribution of the initial condition.
Loss
δ
Relative Errors in f
Relative Errors in σ
Nrepeats
W2
0.0
0.072 (± 0.008)
0.071 (± 0.023)
10
W2
0.1
0.053 (± 0.008)
0.043 (± 0.016)
10
W2
0.2
0.099 (± 0.007)
0.056 (± 0.019)
10
W2
0.3
0.070 (± 0.014)
0.083 (± 0.026)
10
W2
0.4
0.070 (± 0.014)
0.078 (± 0.040)
10
W2
0.5
0.075 (± 0.013)
0.138 (± 0.021)
10
W2
0.6
0.037 (± 0.018)
0.069 (± 0.017)
10
W2
0.7
0.075 (± 0.016)
0.043 (± 0.014)
10
W2
0.8
0.041 (± 0.012)
0.079 (± 0.023)
10
W2
0.9
0.082 (± 0.015)
0.108 (± 0.033)
10
W2
1.0
0.058 (± 0.024)
0.049 (± 0.025)
10
Table 2: Reconstructing the CIR model Eq. (31) when u0 ∼ N(2, δ2) with different vari-
ance δ2. The results indicate that the reconstruction results are not sensitive to
the variance in the distribution of the initial value u0.
Appendix H. Neural network structure
We examine how the neural network structure affects the reconstruction of the CIR model
Eq. (31) in Example 2. We vary the number of layers and the number of neurons in each
layer (the number of neurons are set to be the same in each hidden layer), and the results
are shown in Table 3.
The results in Table 3 show that increasing the number of neurons in each layer improves
the reconstruction accuracy in σ. For the reconstructing CIR model in Example 2, using 32
neurons in each layer seems to be sufficient. On the other hand, when each layer contains
32 neurons, the number of hidden layers in the neural network seems does not affect the
reconstruction accuracy of f, σ, and this indicates even 1 or 2 hidden layers are sufficient
for the reconstruction of f, σ. Thus, reconstructing the CIR model in Example 2 using
our proposed squared W2 based loss function does not require using complex deep or wide
neural networks.
We also consider using the ResNet neural network structure (He et al., 2016). However,
the application of the ResNet technique does not improve the reconstruction accuracy of
the CIR model in Example 2. This is because simple feedforward multilayer neural network
33
Xia and Li and Shen and Chou
Table 3: Reconstructing the CIR model when using neuron networks of different widths
and numbers in each hidden layer to parameterize ˆf, ˆσ in Eq. (2).
Loss
Width
Layer
Relative Errors in f
Relative Errors in σ
Nrepeats
W2
16
1
0.131(±0.135)
0.170(±0.102)
10
W2
32
1
0.041(±0.008)
0.109(±0.026)
10
W2
64
1
0.040(±0.008)
0.104(±0.019)
10
W2
128
1
0.040(±0.008)
0.118(±0.019)
10
W2
32
2
0.049(±0.015)
0.123(±0.020)
10
W2
32
3
0.094(±0.013)
0.166(±0.041)
10
W2
32
4
0.124(±0.020)
0.185(±0.035)
10
W2
32
5
0.041(±0.008)
0.122(±0.024)
10
W2
32
6
0.043(±0.013)
0.117(±0.024)
10
W2
32
7
0.044(±0.012)
0.109(±0.017)
10
Table 4: Reconstructing the CIR model Eq. (31) when neuron networks have different
numbers of hidden layers and are equipped with the ResNet technique.
Each
hidden layer contains 32 neurons.
Loss
Layer
Relative Errors in f
Relative Errors in σ
Nrepeats
W2
1
0.045(±0.012)
0.116(±0.025)
10
W2
2
0.053(±0.011)
0.108(±0.024
10
W2
3
0.071(±0.017)
0.117(±0.040)
10
W2
4
0.096(±0.035)
0.149(±0.064)
10
structure can work well for learning Eq. (31) when reconstructing both f and σ so we do
not need deep neural networks. Thus. the ResNet technique is not required. The results
are shown in Table 4.
Appendix I. Using the stochastic gradient descent method for
optimization
Here, we shall reconstruct the OU process Eq. (32) in Example 3 with the initial condition
X(0) = 0 using the MMD and our squared W2 distance loss functions Eqs. (17) and (18)
with different numbers of ground-truth trajectories and different batch sizes for applying the
stochastic gradient descent technique for optimizing the parameters in the neural networks
for reconstructing the SDE.
34
Squared Wasserstein Distance for SDE Reconstruction
Table 5: Errors and runtime for different loss functions and different numbers of ground-
truth trajectories when the training batch size is fixed to 16 and 256. The MMD
and our proposed squared W2 distance Eq. (17) and well as our proposed time-
decoupled squared W2 distance Eq. (18) are used as the loss function.
Loss
Nsample
Batch size
Relative error in f
Relative error in σ
Runtime
Nrepeats
MMD
64
16
0.30 ± 0.12
0.49 ± 0.17
1.19 ± 0.59
10
MMD
128
16
0.30 ± 0.09
0.50 ± 0.20
1.27 ± 0.58
10
MMD
256
16
0.31 ± 0.09
0.44 ± 0.21
1.31 ± 0.59
10
MMD
512
16
0.22 ± 0.12
0.43 ± 0.18
1.22 ± 0.37
10
MMD
1024
16
0.23 ± 0.11
0.37 ± 0.24
1.70 ± 0.47
10
Eq. (17)
64
16
0.28 ± 0.06
0.66 ± 0.11
0.83 ± 0.26
10
Eq. (17)
128
16
0.24 ± 0.07
0.68 ±0.11
0.73 ± 0.18
10
Eq. (17)
256
16
0.25 ± 0.07
0.66 ± 0.09
0.67 ± 0.14
10
Eq. (17)
512
16
0.23 ± 0.06
0.68 ± 0.09
0.75 ± 0.16
10
Eq. (17)
1024
16
0.25 ± 0.07
0.66 ± 0.09
1.02 ± 0.47
10
Eq. (18)
64
16
0.20 ± 0.06
0.42 ± 0.08
0.61 ± 0.14
10
Eq. (18)
128
16
0.22 ± 0.06
0.37 ± 0.14
0.78 ± 0.35
10
Eq. (18)
256
16
0.21 ± 0.07
0.39 ± 0.16
0.88 ± 0.46
10
Eq. (18)
512
16
0.23 ± 0.06
0.43 ± 0.15
0.72 ± 0.11
10
Eq. (18)
1024
16
0.21 ± 0.03
0.36 ± 0.12
1.08 ± 0.52
10
MMD
64
256
0.26 ± 0.12
0.41 ± 0.20
1.54 ± 0.66
10
MMD
128
256
0.25 ± 0.14
0.40 ± 0.23
1.82 ± 0.78
10
MMD
256
256
0.25 ± 0.12
0.35 ± 0.21
3.68 ± 1.31
10
MMD
512
256
0.23 ± 0.14
0.37 ± 0.23
3.45 ± 1.50
10
MMD
1024
256
0.23 ± 0.13
0.35 ± 0.21
3.09 ± 1.35
10
Eq. (17)
64
256
0.28 ± 0.08
0.61 ± 0.04
1.19 ± 0.45
10
Eq. (17)
128
256
0.31 ± 0.07
0.61 ± 0.07
1.04 ± 0.48
10
Eq. (17)
256
256
0.26 ± 0.07
0.53 ± 0.03
0.96 ± 0.43
10
Eq. (17)
512
256
0.26 ± 0.08
0.56 ± 0.05
0.98 ± 0.40
10
Eq. (17)
1024
256
0.27 ± 0.08
0.56 ± 0.05
0.89 ± 0.36
10
Eq. (18)
64
256
0.24 ± 0.08
0.41 ± 0.13
1.39 ± 0.53
10
Eq. (18)
128
256
0.26 ± 0.11
0.37 ± 0.17
1.36 ± 0.61
10
Eq. (18)
256
256
0.20 ± 0.08
0.31 ± 0.16
1.72 ± 0.73
10
Eq. (18)
512
256
0.25 ± 0.11
0.38 ± 0.20
1.67 ± 0.73
10
Eq. (18)
1024
256
0.26 ± 0.10
0.39 ± 0.20
1.64 ± 0.79
10
35
Xia and Li and Shen and Chou
We train 2000 epochs with a learning rate 0.001 for all numerical experiments.
In
all cases, the loss functions converge before 2000 epochs.
From Table 5, for all three
loss function, i.e., the MMD loss, Eq. (17), and Eq. (18), a larger number of training
samples leads to more accurate reconstruction of σ (the noise term). Furthermore, it can
be seen from Table 5 that using a smaller batch size (16) for training tends to lead to less
accurate reconstruction of σ for the MMD and Eq. (17) loss functions even if the number
of trajectories in the training set is large. This feature might arise because the trajectories
are intrinsically noisy and evaluating MMD and Eq. (17) will be inaccurate if the batch size
is small. Therefore, using a smaller batch size does not remedy the high cost of MMD as
the reconstruction error is large and leads to inaccurate reconstruction of the ground-truth
SDE for smaller Nsample. On the other hand, our proposed time-decoupled squared W2
distance loss function Eq. (18) gives similar performance in reconstructing f, σ for both a
batch size of 16 and a batch size of 256. In other words, using Eq. (18) is more robust to
a smaller batch size. From Table 5, using a smaller batch size (16) leads to faster training.
Thus, we can consider using Eq. (18) as the loss function together with a smaller batch
size to boost training efficiency.
From the results in both Example 3 and Table 5, our proposed time-decoupled squared
W2 distance Eq. (18) is faster and more efficient than the MMD method and Eq. (17),
making it potentially most suitable among all three loss functions for reconstructing SDEs.
Appendix J. Additional discussion on the loss functions Eqs. (17)
and (18)
Here, we make an additional comparison between using Eq. (17) and Eq. (18) as loss
functions in Example 4.
We set the number of training samples to be 128 and other
hyperparameters for training to be the same as those in Example 4, as detailed in Table 1.
First, we minimize Eq. (17) and record Eq. (17) and Eq. (18) over training epochs. Next,
we minimize Eq. (18) and record Eq. (17) and Eq. (18) over training epochs. The results
are shown in Fig. 5.
From Fig. 5 (a), we can see that when minimizing Eq. (17), Eq. (17) is almost 100.5
times larger than Eq. (18). However, when minimizing Eq. (18), the values of Eq. (17)
and Eq. (18) are close to each other (Fig. 5 (b)). In both cases, Eq. (18) converges to
approximately 10−1. Interestingly, minimizing Eq. (18) leads to a smaller value of Eq. (17).
This again implies that minimizing Eq. (18) can be more effective than minimizing Eq. (17)
in Example 4. More analysis on Eq. (18) is needed to understand its theoretical properties
and to compare the performances of minimizing Eq. (18) versus minimizing Eq. (17) from
numerical aspects.
36
Squared Wasserstein Distance for SDE Reconstruction
Figure 5: (a) The change in Eq. (17) and Eq. (18) when minimizing Eq. (17) over training
epochs. (b) The change in Eq. (17) and Eq. (18) when minimizing Eq. (18) over
training epochs.
37
","Recent advancements leverage machine learning, specifically neural ordinary differential equations (NODEs) (Chen et al., 2018), to offer a more flexible approach to reconstructing SDEs in the form of neural SDEs (nSDEs) (Tzen and Raginsky, 2019; Tong et al., 2022; Jia and Benson, 2019). Despite the promise, challenges remain, particularly in selecting optimal loss functions (Jia and Benson, 2019).nan"
